% ==================================================

===================================================================================================
\subsubsection{Social}
===================================================================================================

Robots:
little humanoid~\cite{igorevich_behavioral_2011, indrajit_development_2013, lu_research_2020}. Zeno humanoid~\cite{torres_implementation_2012}, 4
Nao~\cite{yang_study_2013, taheri_social_2014, zhu_robust_2017, gong_research_2018} 4
Pepper~\cite{bilac_gaze_2017, sorostinean_activity_2018, cazzato_real-time_2019, jarosz_detecting_2019, augello_towards_2020, lee_visual_2020} 6

Other: 5
humanoid robot receptionist~\cite{li_visual_2015}
Wheeled base, humanoid torso ~\cite{phong_vietnamese_2020}
ROMAN humanoid torso~\cite{saleh_nonverbal_2015}
Harley humanoid~\cite{li_cnn_2019}
iCub~\cite{boucher_i_2012}

5
icat~\cite{castellano_context-sensitive_2014} (with arms),

head robots:  
Eddie~\cite{sosnowski_mirror_2010}, Reeti~\cite{masmoudi_expressive_2011}, and Alice~\cite{meghdari_real-time_2017, taheri_social_2014} robot. 

HSR~\cite{lee_visual_2020}, mobile with manipulator
irobot create~\cite{zhang_optimal_2016}
Kompaï mobile Robot~\cite{vaufreydaz_starting_2016}
Pioneer 3-DX~\cite{chien_navigating_2019}
mobile robot~\cite{li_inferring_2019}
ARIO mobile robot~\cite{tseng_multi-human_2014}
===================================================================================================

Facial expressions were used to control an anthropomorphic social robot head by mimicking human expressions, such as the  Eddie~\cite{sosnowski_mirror_2010}, Reeti~\cite{masmoudi_expressive_2011}, and Alice~\cite{taheri_social_2014, meghdari_real-time_2017} robot. Alice with body~\cite{taheri_social_2014}.


Humanoid robots were required to copy human joint positions~\cite{igorevich_behavioral_2011, csapo_multimodal_2012, torres_implementation_2012, indrajit_development_2013, yang_study_2013, taheri_social_2014, zhu_robust_2017, augello_towards_2020, lu_research_2020}, including head position~\cite{cazzato_real-time_2019}. speaking and face detection for conversation, teleop nao robot through kinect~\cite{csapo_multimodal_2012}. NAO and alice with body robot mimic human pose using kinect. Also haptic device as a control option. Alice can smile, robots can help teach colours.NAO pointing and changing eye colour for children to identify~\cite{taheri_social_2014}. On pepper~\cite{augello_towards_2020}.



Robot uses vision to determine social configurations of people and approaches accordingly, groups of two people, ARIO robot~\cite{tseng_multi-human_2014}



Robot reaches for object human asks for and looks at, iCub + speech~\cite{boucher_i_2012}

Robot bartender James= icat with arms, senses people with vision, interacts and takes orders for drinks, with multiple people.~\cite{foster_two_2012}

Small UAV hovers above the hand of person wearing a blue glove, and can be passed from one person to another~\cite{miyoshi_above_2014}. Uses 2 cameras

Nao robot detects user from vision and sound, and responds accordingly~\cite{cech_active-speaker_2015}.

humanoid robot receptionist conversation with vision, determines the direction of attention of human.~\cite{li_visual_2015}. humanoid robot receptionist

Face emotion detection and response robot head. e.g. if male sad, robot shows surprise then sad, if female sad, robot shows interest then guilt~\cite{ke_vision_2016}.

Molleret et al~\cite{mollaret_multi-modal_2016} use a PR2 robot to approach, and interact with a person when it detects intention for interaction.

detect if robot should engage person~\cite{vaufreydaz_starting_2016}. Kompaï Robot, social robot with head shape, vision + laser + ultrasound + infrared + touchscreen

Gaze detection by mobile robots - uses vicon motion capture~\cite{zhang_optimal_2016}, irobot create

Learning natural social communication through gaze and speech breaks with pepper robot~\cite{bilac_gaze_2017}

Detect person face with nao robot with camera, wave when person waves detected with wearable imu.~\cite{gong_research_2018}.

This is in action rec as well~\cite{sorostinean_activity_2018}. pepper robot detects activity and responds with questions and change in state.CNN + LSTM using skeleton data (SDK)

Pepper mimic head position~\cite{cazzato_real-time_2019}.

Person and gender recognition (discern staff, patient, stranger), obstacle avoidance, and person following~\cite{chien_navigating_2019}. Pioneer 3-DX, plus stand and kinect at the base rgb on top Chien et al perform gender classification and person identification to follow a person~\cite{chien_navigating_2019}. The robot also has a sonar sensor for obstacle avoidance.

pepper robot conducts interview, persons gaze determines actions (peppers speech)~\cite{jarosz_detecting_2019}

mobile robot approaches person if it should. two people. (determined by body pose and several factors including facial recognition (happy, sad, angry etc..)~\cite{li_inferring_2019}

emotion detection triggers pre-defined robot response. Harley humanoid~\cite{li_cnn_2019}

Socialise with people in restaurant (point to seating locations, repeat bar orders), Fetch object and deliver (requires relocation of human)Pepper, and HSR~\cite{lee_visual_2020}.

humand mimics human~\cite{lu_research_2020}

reception robot, conversation, uses vision to detect person is there~\cite{phong_vietnamese_2020}. Wheeled base, humanoid torso


Gestures such as nodding or shaking of the head were used in social interactivity, including to evaluate human engagement~\cite{saleh_nonverbal_2015}. For instance, facial expression and gaze detection were used to determine children engagement in an educational program~\cite{castellano_context-sensitive_2014}. Facial expressions and body pose were used to evaluate if a mobile robot should approach a human~\cite{li_inferring_2019}.


A mobile social robot determines the appropriate time to approach a human after inferring user intent through body gestures and facial expressions~\cite{li_inferring_2019}. For these demonstrations there is often more than one human in the robot field of view, and the robot must at all times maintain appropriate social distance.

\subsubsubsection{Social techniques}
% ==================================================

Joint positions extracted from RGB-D sensor~\cite{igorevich_behavioral_2011, torres_implementation_2012, indrajit_development_2013, yang_study_2013, taheri_social_2014, tseng_multi-human_2014, mollaret_multi-modal_2016, meghdari_real-time_2017, augello_towards_2020, lu_research_2020}

% ==================================================

 iCub gaze controller~\cite{boucher_i_2012}

ms sdk + viola jones~\cite{csapo_multimodal_2012}

colour segmentation + blob detection and tracking ~\cite{foster_two_2012}

two cameras and colour segmentation for little uav~\cite{miyoshi_above_2014}.

moving average and butterworth filter for joints from kinect~\cite{taheri_social_2014}.

approaches people after detecting TODO~\cite{tseng_multi-human_2014}

Face Detector [27], Identity Recognition (identity, age, and gender) [28], [29], Face Tracking [30], Sound Recognition [25], Sound Localization and AV Fusion (described in this paper).~\cite{cech_active-speaker_2015}.

viola-jones haar classifier and cascades, DMP (Direction-Magnitude Pattern) features, SVM. Head gestures from direction and magnitude pattern of depth pixels were classified with a SVM~\cite{saleh_nonverbal_2015}.

Opencv haar features viola-jones and cascades, trained for eye, nose, mouth. Gabor filters, PCA, and SVM~\cite{ke_vision_2016}

Random forests for real time 3D face analysis for line of sight OpenNi for body pose. Detects desire to interact with head/shoulder pose, speech. Voice Activity Detection (VAD). particle swarm optimization inspired tracker, tracking and voice are fused for HMM.~\cite{mollaret_multi-modal_2016}

Minimum Redundancy Maximum Relevance (MRMR)~\cite{vaufreydaz_starting_2016}. 29 interactions with the robot,made by 15 different participants, 5 classes for interact, leave interact etc.. 158000 frames

ROS face recognition package - Viola-jones but using depth to remove false positives based on realistic size of face.~\cite{zhang_optimal_2016}

Train filled pause detector (speech), when detected use peppers gaze detector for 1.5s, make a decision whos turn to speak~\cite{bilac_gaze_2017}

MS SDK + Neural network~\cite{meghdari_real-time_2017}

Velocity (MV) Algorithm and RVM, robust regress-based refining. I think they use an sdk~\cite{zhu_robust_2017}

OpenCV, no details of method used for face detection~\cite{gui_teaching_2018}.

This is in action rec as well~\cite{sorostinean_activity_2018}. pepper robot detects activity and responds with questions and change in state.CNN + LSTM using skeleton data (SDK)

ResNet-10 SSD + OpenFace, pre-trained networks~\cite{cazzato_real-time_2019}.

"Fuzzy combination of depth and sonar. penCV’s
OpenCV’s LPBH (Local Binary Patterns Histogram) face recognizer method"~\cite{chien_navigating_2019}

custom gaze detection  - detection face, detect landmarks on face, eye regions, head pose, pupil centre~\cite{jarosz_detecting_2019}

SSD for face detection, NN- facial recognition model. random forest regression, ~\cite{li_inferring_2019}

HAAr features for detecting face region, CNN and LSTM (custom network, search through parameters)~\cite{li_cnn_2019}

OpenNI~\cite{augello_towards_2020}

From extracted features, classification of gestures was performed with traditional machine learning methods such support vector machines (SVM)~\cite{saleh_nonverbal_2015, ke_vision_2016}. SVM's facial expressions~\cite{sosnowski_mirror_2010, castellano_context-sensitive_2014, ke_vision_2016}. Head gestures from direction and magnitude pattern of depth pixels were classified with a SVM~\cite{saleh_nonverbal_2015}. Other methods use feature extractors such as Gabor filters~\cite{ke_vision_2016}. 
Neural network layers that utilise time-series (such as LSTM) are also used to classify dynamic gestures~\cite{li_cnn_2019}.


Recent works use convolutional neural networks to detect the face region ~\cite{li_inferring_2019, li_cnn_2019}. The face location can be used to then classify facial expressions~\cite{li_cnn_2019},

~\cite{lee_visual_2020}semantic slam

OpenNI~\cite{lu_research_2020},

MS SDK for head pose detection~\cite{phong_vietnamese_2020}

OpenCV 2 cascade face detector, which is based on the Viola and Jones and openCV for locating facial features. Emotion classification ~\cite{masmoudi_expressive_2011}

Deep neural networks have been adopted for gesture classification in recent years. The most common usage was convolutional neural networks (CNN's), classifying gestures directly from pixels. Alternatives perform classification from the extracted skeletal information~\cite{}. architectures and use pre-trained models that are then fine tuned on task specific data~\cite{mazhar_real-time_2019, waskito_wheeled_2020}, or trained with a custom dataset~\cite{gao_hand_2020}. Examples of papers that investigate their own network architecture include~\cite{li_cnn_2019}. Li et al~\cite{li_inferring_2019} TODO: SSD for face detection + other things. Li et al~\cite{li_cnn_2019} use a CNN with LSTM to detect facial expression changes.

\subsubsubection{Cameras Used for Social Classification and Communication:}

From the 41 papers identified that required the robot to communicate with a human in an social interaction, the most common camera type was the RGB-D sensor ($n=24$), followed by monocular RGB cameras ($n=15$). In the 24 papers that used an RGB-D camera, 23 used the Microsoft Kinect. Two papers ($n=2$) used a stereo camera.

\subsubsubsection{Datasets/Models:} 

Papers that used datasets from external sources were most commonly used for facial expression recognition~\cite{sosnowski_mirror_2010, castellano_context-sensitive_2014, ke_vision_2016, li_inferring_2019, li_cnn_2019}. The Cohn-Kanade dataset~\cite{kanade_comprehensive_2000} and its extended form (CK+)~\cite{lucey_extended_2010} was used to train facial expression models~\cite{sosnowski_mirror_2010, ke_vision_2016, li_cnn_2019}. Other datasets include FERET, AffectNet, MMI Face Database, and the Inter-ACT corpus. Several papers built on previous work for gesture recognition models (the details of datasets used in training were not provided)~\cite{saleh_nonverbal_2015}. 

the AffectNet Database contains approx- imately 450,000 + Cohn-Kanade (CK+) dataset includes 123 subjects and 593 image sequences. ~\cite{li_cnn_2019}

Datasets that use video~\cite{meghdari_real-time_2017}
"10 persons, 6 males and 4 females Kinect data output was captured at least for 3 seconds with 30 frame per second rate for each facial expression with different head orientation. Though, each person has a dataset that each frame of data is an array with 17 members."~\cite{meghdari_real-time_2017}

Trained on: Aberdeen Facial Database, GUFD Facial Database, Utrecht ECVP Facial Database + images of laboratory personel using openCV’s LPBH~\cite{chien_navigating_2019}.

WIDER FACE dataset + Kaggle facial expres- sion recognition challenge dataset~\cite{li_inferring_2019}




===================================================================================================
\subsubsection{Learning from demonstrations}
===================================================================================================
Human demonstrates actions observed with vision, robot executes. iCub, actions include Hold, place, take~\cite{saegusa_cognitive_2011}

track hand position, and object, and action, to mirror and perform by robot. Segmentation and blog detection~\cite{saegusa_cognitive_2011}


Industrial arm programmed, and controlled with gestures, Body: lifting the foot to start programming, arm position to encode open/close gripper and position.~\cite{stipancic_programming_2012}. 6-axis FANUC robots

Kinect sdk, joint angle geometry? (not mentioned how gestures are classified)~\cite{stipancic_programming_2012}

Kinect input to update robot behaviour on a humanoid based on periodic dynamic movement primitives~\cite{petric_online_2014}Sarcos humanoid ms SDK?


Little humanoid learns behaviours from demonstration from 2 agents through kinect . ~\cite{potdar_learning_2016}, Supervised learning LSTM for learning how a human responds to another human, then transfered to the robot

Robot follows human with gaze, and follows several aspects of a watering task. Simple statemachine, not sure of the utility. Several vision components~\cite{yoo_gaze_2017} Opencv cascade haar features for face viola jones, CNN to detect objects, 


Teaching robot through gestures and speech for industrial arm~\cite{du_online_2018}. Kuka arm. interval kalman filter, improved particle filter, 


Learning from demonstrations for peg grasping task for humanoid torso with kinect ~\cite{li_learning_2018}. imNeu humanoid torso. OpenNI, Gaussian mixture model and GMM regression


kuka arm learns from demonstrations, with gesture control, and safety factors ~\cite{mazhar_real-time_2019}. OpenPose library, Inception V3 convolutional neural network is adapted and trained to detect the hand gestures + fine tuning 


===================================================================================================
\subsubsection{Handover and collaborative actions}
===================================================================================================

Robotic kitchen, uses vision to interact with disabled person~\cite{aranda_friendly_2010}

ASIMO humanoid robot follows the head pose of an operator for direction while mimicking hand position with its end effector to handover an object (a green marker) to another person~\cite{arumbakkam_multi-modal_2010}. 

Jido robot approaches a person with proxemics in mind to hand over a water bottle to a seated person  "sufficiently far from the human and yet sufficiently near to achieve hand over task" ~\cite{sisbot_synthesizing_2010}. Trajectories are synthesised based on safe and socially acceptable distances.

Industrial robot arm avoids human that has coloured markers attached, detected with stereo vision~\cite{ding_optimizing_2011}

Staubili tx90 robot arm,  multiple people with camera network, industrial arms deactivate when humans get close~\cite{nair_3d_2011}

An ABB industrial 4 actions: warn the operator, stop the robot, move back, and modifying the path ~\cite{wang_vision-guided_2013}.

Multiple kinects create a safe workspace for  Lab-Volt 5150 5 DOF manipulator arm~\cite{morato_toward_2014}. Robot stops when human close. Chassis assembly task.

Kinect sensor is used to track a persons hand in an screwing task with a Kuka arm~\cite{cherubini_unified_2015}. Force sensor is also used.

ABB Industrial arm TCP goes to location of human hand (mimic movement)~\cite{christiernin_interacting_2016}


A human points to an object (See gesture recognition). Once picked up, the hand over process relies on the tactile sensors on the finger tips to know when shear has occurred (the object is in the palm)~\cite{ikai_robot_2016}.

Cooperative folding of a sheet, human hand is detected against fabric corner, robot picks up the opposite corner to folder.~\cite{koustoumpardis_human_2016}. Adept Cobra s800

Safe workspace, robot arm plans to avoid human~\cite{moreno_path_2016}.


Soft robot follows humans back using kinect and ultrasonic sensor for washing robot~\cite{dometios_real-time_2017}.5 DOF Katana arm by Neuronics

FUM SCARA style robot arm, safe workspace, stops if human detected~\cite{shariatee_safe_2017}

Kuka arm stops when human enters shared workspace~\cite{araiza-lllan_dynamic_2018}

Robot arm moves as hand moves in 3D. Can also grasp when two hands out in front. UR10 with hand gripper~\cite{bolano_towards_2018}


Shared workspace with a UR3 arm~\cite{bothe_effective_2018}

Direct control of unknown robotic arm~\cite{hong_interactive_2018}, no gripper.

Safe workspace, detection and avoidance Kawasaky RS005LA~\cite{pasinetti_development_2018}.

Person detection, then gesture from wearable imu, Kuka arm~\cite{haghighi_integration_2019}

Cameras detect surgeon gaze to determine what instrument a UR5 robot should fetch during an operation~\cite{kogkas_free-view_2019}

Robot Fanuc LR Mate 200iD/7L, tracks human hand and changes path if it gets too close~\cite{landi_prediction_2019}

Handover with robot arm and hand tracking using UR3\cite{scimmi_experimental_2019}. Human to robot handover.

Robot picks up show and brings it to human foot~\cite{valle_personalized_2019}. Speech is also used for the state machine. WAM robot arm. Dressing assistant. Robot to human handover.

dual arm panda robot. Velocity at end-effector is reduced when human detected~\cite{zardykhan_collision_2019}. safe workspace

Feeding assistance robot operated with eye tracking glasses~\cite{arai_service_2020}. Unknown robot arm. Eye glasses use near a infrared camera, robot arm has a camera attached, the person can see the camera feed through a hmd.

Safe workspace with kuka arm, also uses touch~\cite{bingol_practical_2020}. 

robot arm avoids collision with human, ~\cite{chan_collision-free_2020}, HIWIN industrial arm

shared workspace, human detection and avoidance~\cite{ferraguti_safety_2020}, 6-DoF Puma 260 robot.

Safe workspace~\cite{liu_dynamic_2020}robot avoids human in safe workspace. ABB industrial arm 

Collision avoidance, safeworkspace, ~\cite{mronga_constraint-based_2020}. Kuka arm


safe workspace with baxter and human~\cite{tarbouriech_bi-objective_2020}

Robot adjusts work height based on human pose~\cite{van_den_broek_ergonomic_2020}. Sawyer robot arm

~\cite{zhang_human_2020}. Collaborative manipulation task, kinect + IMU's to predict human pose. Franka panda arm

Handover robot to human, using force sensors~\cite{bdiwi_handing-over_2013}.Staubili RX90 robot with a JR3 multi-axis force/torque sensor


Dual arm Baxter robot is used to assist with dressing, where the robot helps people put on a jacket with sleeves~\cite{gao_user_2020}.


Human monitoring in safe manufacturing setting (cellular manufacturing)~\cite{tan_safety_2010}. Human wears colour markers on their helmet, and a light curtain indicates to the human the extents of the safe workspace.

Human to robot handover, human is segmented out of the image so the object can be safely grasped, Panda arm, grasping various objects, e.g. banana, lemon, tin of tuna,  ~\cite{rosenberger_object-independent_2020}.


% ==================================================
% ==================================================
% Techniques for handover
% ==================================================
% ==================================================
Unknown~\cite{aranda_friendly_2010}. Consider removing, design paper

Viola-jones for face detecdtion, particle filter for head tracking, joint positions from depth camera~\cite{arumbakkam_multi-modal_2010}.

Unclear vision technique~\cite{sisbot_synthesizing_2010}.

coloured markers tracked with stereo cameras~\cite{ding_optimizing_2011}

segmentation, detection, occulsion checking, tracking~\cite{nair_3d_2011}

Two kinect cameras detect the operator with kinect SDK, a 3D model of the robot is used determine the location of the robot.~\cite{wang_vision-guided_2013}


Multiple kinects provide skeleton sdk, motion model of person, particle filter, and Kalman filter~\cite{morato_toward_2014}

Kinect sdk to get hand position, force sensor is also used in the collaboration. ~\cite{cherubini_unified_2015}

MS sdk is used to get human joints, ~\cite{christiernin_interacting_2016}

largest skin-colored area other than the face area. In addition, the palm’s centroid is obtained ( stereo vision). Tactile sensor knowns when shear force is greater than a threshold (releases object)~\cite{ikai_robot_2016}

Skeleton tracking with OpenNI libraray (using opencv), moving average filter to reduce errors, rgb-d visual servoing, and a neural network force feedback controller~\cite{koustoumpardis_human_2016}.

Kinect sdk used to get joint positions, inverse kinematics used to plan a trajectory that avoids the human~\cite{moreno_path_2016}

uses depth camera for surface shape of human back~\cite{dometios_real-time_2017}

distance threshold, binary image, morphology, sobel edge detection~\cite{shariatee_safe_2017}.

OpenNI to get skeleton info~\cite{araiza-lllan_dynamic_2018}.

Point cloud obtained from depth sensor, hands detected a certain distance out in front. Velocity of TCP controlled with the position of the users hand using a UR10 robot arm. When the user places two hands out in front the gripper closes, for picking up ball task~\cite{bolano_towards_2018}.

Shared workspace using Microsoft SDK for skeleton extraction~\cite{bothe_effective_2018}.

Microsoft SDK for joint positions for direct control. Also uses Savitzky-Golay
smoothing filter~\cite{hong_interactive_2018}.

HOG + svm for detection and avoidance~\cite{pasinetti_development_2018}.

Unknown detection, imu for gesture~\cite{haghighi_integration_2019}.

Gaze and head pose detection (eye tracking with proprietary method)~\cite{kogkas_free-view_2019}

Robot tracks human hand using the Microsoft SDK, and has a motion model of the person (using semi adaptable neural network to model the human hand position with minimium jerk), predicting collision with the robot path~\cite{landi_prediction_2019}.

Microsoft SDK for skeleton (two kinects)~\cite{scimmi_experimental_2019}.

Ms SDK for skeleton~\cite{valle_personalized_2019}.

OpenPose for skeleton, two components of a detection system: Impulse Orb (collision detection), and end-effector trajectory planning ~\cite{zardykhan_collision_2019}

Vison CNN trained in previous work~\cite{bingol_practical_2020}.

Get skeleton from rgb-d~\cite{chan_collision-free_2020}

OpenCV adaptive thresholding for pupil detection, gaze direction determined from distance from the center of the image~\cite{arai_service_2020}.

~\cite{ferraguti_safety_2020} skeletons from two rgb-d 

MS SDK for human pose~\cite{liu_dynamic_2020}.

Kinematic Continuous Colli- sion Detection Library (KCCD) (voxelise stuff in the robot path?) ~\cite{mronga_constraint-based_2020}
 
Bi-directional RRT + 3d occupancy grid, unsure how human detection is performed~\cite{tarbouriech_bi-objective_2020}
 
openPose (with ROS)~\cite{van_den_broek_ergonomic_2020}.
 
~\cite{zhang_human_2020} skeleton from kinect, fused with IMU data to track pose.

Visual servoing, detect and segment objects, handover with force sensors~\cite{bdiwi_handing-over_2013}.
 
OpenNI ROS to get skeleton information. GMM motion model of human joints, with vision and force feedback~\cite{gao_user_2020}. 

Colour segmentation with two cameras to determine where the human is~\cite{tan_safety_2010}.

Pre-trained YOLOv3 to detect objects, RefineNet originally proposed by Lin [22], which was trained on the portion of the PASCAL dataset that belong to the class “Person” to detect person, hand segmentation network (per-pixel, hand or not, ResNet50 trained on custom dataset, GG-CNN to detect grasp pose, visual servoing for moving to object~\cite{rosenberger_object-independent_2020}

===================================================================================================
\subsubsection{Action Recognition}
===================================================================================================

Walking style prediction~\cite{kollmitz_deep_2019, chalvatzaki_learn_2019}, exercise robot~\cite{werner_evaluation_2013, gorer_autonomous_2017, avioz-sarig_robotic_2020}, home service robots~\cite{koppula_anticipating_2016} and robot arms~\cite{lee_learning_2017, wang_collision-free_2017}.  Use cases for visual detection were often actions to directly assist a human, such as with a robotic walker~\cite{chalvatzaki_learn_2019}, playing a game~\cite{efthymiou_multi-_2018}, clearing a table~\cite{lee_learning_2017}, or opening a fridge~\cite{koppula_anticipating_2016}). Additional feedback provided by the robot, such as blinking and face tracking, provided the user with a more pleasant experience~\cite{gorer_autonomous_2017, avioz-sarig_robotic_2020}.

With action recognition the robot should recognise what a human is doing to provide assistance without specific human instruction. In comparison to the other application areas, action recognition is a newer area of research (most papers are from the past few years). Accurate classification of human actions is a difficult vision problem, and requires large datasets to train time series classifiers (such as the use of Long Short Term Memory (LSTM)~\cite{HochSchm97} or Gated Rectified Unit (GRU)~\cite{cho-etal-2014-learning} blocks).

Example methods: Papers that use LSTM:~\cite{potdar_learning_2016, wang_collision-free_2017, sorostinean_activity_2018, chalvatzaki_learn_2019, li_real-time_2019}, papers that use GRU:~\cite{gui_teaching_2018}. Other methods: Hidden Markov Models (HMMs)~\cite{zhuang_learning_2016}, change of position over time (between frames)~\cite{lang_research_2020}, stacked frames (16 images) from multiple kinects fed through a CNN~\cite{efthymiou_multi-_2018}. Not really sure, something to do with segmentation~\cite{saegusa_cognitive_2011}.

==========TODO:
More details about~\cite{efthymiou_multi-_2018}, though I think they are out of scope:
For the first pipeline of our single- view action recognition system, the state-of-the-art Dense Trajecto- ries (DT) [21] features are combined with the Bag of Visual Words (BoVW) encoding framework. In each video frame, dense points are sampled and tracked through time based on a dense optical flow field. The features that are computed along each trajectory are: the Trajectory descriptor [21], Histograms of Oriented Gradients (HOG) [22], Histograms of Optical Flow (HOF) [22], and Motion Boundary Histograms (MBH) [21] computed on both axes (MBHx, MBHy). Encoding of the features using the BoVW and assignment to K=4000 clusters follows in order to form a representation of each video. Videos are classified based on their BoVW representation, using non-linear Support Vector Machines (SVMs)

===================================================================================================
\subsubsection{Robot Movement in Human Spaces}
===================================================================================================


Re-identification using audio~\cite{luo_human_2010}.


Laser for leg detection~\cite{luo_human_2010, alvarez-santos_feature_2012, pereira_human-robot_2013, weinrich_appearance-based_2013, hu_design_2014}, or shoulder detection~\cite{kobayashi_people_2010}.

Laser for obstacle avoidance~\cite{yun_robotic_2013, ali_improved_2015, yang_socially-aware_2019}.

Laser for navigation using SLAM~\cite{do_human-robot_2014, angonese_multiple_2017, yuan_development_2018, miller_self-driving_2019}. Miller et al~\cite{miller_self-driving_2019} follow a human by giving the robot position goals in its occupancy map that are a fixed offset from the tracked human

Ultra-sonic sensor for person following, where the person must hold a coloured marker~\cite{hassan_computationally_2016}. Sonar for navigation~\cite{chien_navigating_2019}.


Multi-human environment~\cite{yang_socially-aware_2019}

Pioneer:~\cite{jia_autonomous_2011, alvarez-santos_feature_2012, pereira_human-robot_2013, yun_robotic_2013, hu_design_2014, munaro_fast_2014, chen_stereovision-only_2014, ali_improved_2015, gupta_robust_2015, angonese_multiple_2017, almonfrey_flexible_2018, chien_navigating_2019}. 


Tracking should be in the techniques section
Face tracking:
~\cite{fahn_real-time_2010, cheng_multiple-robot_2013, bayram_audio-visual_2016, weber_follow_2018, yao_monocular_2017}

body tracking (using extracted skeleton:
~\cite{kahily_real-time_2016, anuradha_human_2020}

Other forms of tracking (e.g. through SURF features, or histogram analysis):
~\cite{chen_person_2012, wu_accompanist_2012,  weinrich_appearance-based_2013 } KCF


Following =============================
==========================================
Following with gestures: 7. 
Papers:

Person following using face tracking~\cite{fahn_real-time_2010}

Person following for wheelchair and museum robot, laser to get shoulder contours, then head position~\cite{kobayashi_people_2010}

Person following with mobile robot using SURF features~\cite{chen_person_2012}.

Other mobile platforms included the SCITOS G5 mobile robot~\cite{weinrich_appearance-based_2013, weber_follow_2018}. Weinrich et al~\cite{weinrich_appearance-based_2013} use lasers to detect a persons legs, and an omni-directional camera to detect, track, and re-identify a person . Weber et al use perform person following using face tracking with a monocular camera~\cite{weber_follow_2018}. 

Bayram et al~\cite{bayram_audio-visual_2016} use audio localisation to allow a robot to find a person when they are not in view. Once the robot can see the person, it performs face tracking and is able to detect multiple people. Nguyen et al~\cite{nguyen_audio-visual_2014} also localise a person that is out of view using sound, and once in view, a target person (from multiple people) is tracked when they wave.

Person following with mobile robot, openi when static, voxelisation and segmentation when moving~\cite{kahily_real-time_2016}.

Yao et al~\cite{yao_monocular_2017} use a monocular camera attached to an autonomous blimp robot to detect and track a persons face.

Medical nurse robot follows with skeleton tracking~\cite{long_kinect-based_2018}

Turtlebot detects, tracks and follows human~\cite{condes_person_2019}. Re-identifies specific target face when they go out of view.

Mecanum mobile robot follows human~\cite{luo_real-time_2019}.

Wheelchair follows person~\cite{wu_improved_2019}.

Person following with vision using target contour bands~\cite{zhang_vision-based_2019}.

Person following with vision using Kinect SDK~\cite{anuradha_human_2020}.

Person following with vision, SSD, facenet and KCF tracking~\cite{hwang_interactions_2020}

Drip stand robot follows person wearing a particular colour pattern~\cite{wu_toward_2020}.

Sound to re-identify lost person, person detection from custom dataset, person following~\cite{luo_human_2010}


Approaching =============================

Line following robot, approaches person when face detected~\cite{budiharto_indoor_2010}.

A PR2 robot approaches a person using a real-time proxemic controller~\cite{mead_probabilistic_2012}.

A mobile R2D2 robot with arms, carries a smaller mobile robot with a gripper~\cite{cheng_multiple-robot_2013}. The mother robot detects a persons face and waves. The robot can also deliver a drink based on the distance to the target persons face. Lastly, the mother robot deploys a smaller robot with a gripper to pick up small objects in its path.

Tibi robot (mobile base with anthropomorphic body and head) approaches people searching for an interaction~\cite{ferrer_robot_2013}.

Chen et al~\cite{chen_stereovision-only_2014} use a pioneer robot to detect and approach a persons face using stereo vision. 

% Avoiding =============================

Person avoiding with mobile robot, HOG features and svm~\cite{talebpour_-board_2016}.

Mobile robot detects people and adjusts velocity~\cite{vasconcelos_socially_2016}

Autonomous people avoidance, people can also use gestures to signal the preferred path of the robot.~\cite{ghandour_human_2017}

Avoid people using a social force model that allows for appropriate social distance when passing~\cite{yang_socially-aware_2019}. Laser for obstacles.

% Avoiding or following:
A vision network (4 cameras) is used to create an intelligent work space, where a mobile robot can localise within a smart space, and follow or avoid people~\cite{almonfrey_flexible_2018}


% Other ===============================

A team of iRobot create platforms navigate through an urban environment detecting human faces and reporting back to a supervisory person~\cite{do_human-robot_2014}. The robots have an omni-directional camera for person detection, and a laser that is used to navigate through the environment (using SLAM).

Multiple people are detected and used as landmarks for integration into a SLAM solution for a mobile pioneer robot following a path~\cite{angonese_multiple_2017}.

% ==============================

RHMS techniques:

% Examples ======================================================
% Face detection with Viola-Jones~\cite{budiharto_indoor_2010, lam_real-time_2011}.

A particle filter is used to track a face~\cite{fahn_real-time_2010}. Face is in the center of the screen at the beginning of the interaction, the colour spectrum of the skin and hair are determined and tracked using a particle filter.

Laser information to get the contours of the shoulder, viola-jones to get face information. Tracked with particle filter.~\cite{kobayashi_people_2010}

Images are first converted to a local binary pattern~\cite{ojala_lbp_2002} to be robust to lighting~\cite{yun_robust_2010}. 

Laser for legs, weighted features for human detection including LBP~\cite{ojala_lbp_2002}, canny edge detector~\cite{canny_computational_1986}, and histogram of gradients (HOG~\cite{dalal_histograms_2005})~\cite{alvarez-santos_feature_2012}

Laser for leg detection, face detection with viola-jones, and then SURF features to match clothing~\cite{wu_accompanist_2012}.

SURF features~\cite{bay_speeded-up_2008} are used to track a person wearing a known pattern~\cite{chen_person_2012}.

Disparity image is taken from a stereo camera, a canny edge detector is used to extract a human shape, the head is segmented using geometry, and compared to a samples from a model image using Hu moments~\cite{jia_autonomous_2011}

Skeleton information is extracted from the Microsoft Kinect SDK, then Bayesian inference is used to sample over all possible robot poses best suited for an interaction~\cite{mead_probabilistic_2012}.

Unknown techniques:
Trajectories are generated based on an ideal interaction area in front of the detected person~\cite{sisbot_synthesizing_2010}.

Unknown face detection method~\cite{cheng_multiple-robot_2013}

Face regions are detected with the Viola-Jones method from OpenCV, then an online random ferns method compares pixel intensities to learn a particular persons face~\cite{ferrer_robot_2013}. This method also utilises human feedback (through a keyboard and touch screen) to improve the classifier when the predictions are uncertain.

Weinrich et al~\cite{weinrich_appearance-based_2013} use a laser for leg detect, then use HOG with a sliding window with a multi-resolution pyramid of the input image to identify, and re-identify, the upper body of a detected person compared with an appearance model (manually constructed model of an upper body).


Yun et al~\cite{yun_robotic_2013} use online multiple instance learning (MIL~\cite{babenko_visual_2009}) to track a person from the back appearance, training a colour histogram score from online images using both positive and negative examples. Chen et al~\cite{chen_stereovision-only_2014} also utilise MIL to perform segmentation with skin colour to detect a person, updating the adaptive appearance model of the person and the local image background. 

Weber et al~\cite{weber_follow_2018} used a CNN (VGG-16~\cite{simonyan_very_2015}) with single shot detection (SSD~\cite{liu_ssd_2016}) for head detection. Detections were then tracked with LSD SLAM~\cite{fleet_lsd-slam_2014} and through the fitting of faces to a template face model (3D morphable face model 3DMM).

Face detection is perform using the Viola-Jones method~\cite{do_human-robot_2014}. 

Hu et al use a model of a person walking to anticipate human movement and more accurately perform person following~\cite{hu_design_2014}.

Munaro et al~\cite{munaro_fast_2014} create a voxel grid filter from an RGB-D image to remove the ground, and cluster groups of voxels based on 3D proximity and shape to detect and track people.


Ali et al~\cite{ali_improved_2015} uses the Viola-Jones method for face detection, and a particle filter for target tracking. Laser for leg obstacle avoidance.

Gupta et al~\cite{gupta_robust_2015} first subtract the background, then perform template matching on the segmented image to detect a person. A motion model and Kalman filter result in smooth tracking.

Bayram et al~\cite{bayram_audio-visual_2016} also use the Viola-Jones for face detection, then perform color-based tracking in YCbCr colour space using Camshift in OpenCV.

Hassan et al~\cite{hassan_computationally_2016} requires a person hold a coloured marker. Colour segmentation is performed and following occurs with the aid of a ultra-sonic range sensor.

Voxelisation, segmentation and 3D clustering is used to detect a person when the sensor is moving~\cite{kahily_real-time_2016}.

HOG features into an SVM to determine if there is a leg. Kalman filter for tracking~\cite{talebpour_-board_2016}

People are detected using HOG features into an SVM, and confirmed using height and distance~\cite{vasconcelos_socially_2016}.

Angonese et al~\cite{angonese_multiple_2017} train a GoogLeNet~\cite{szegedy_going_2015} CNN to detect specific instances of people, such that they can be integrated into a SLAM algorithm for robot navigation. Features from the CNN were used with an SVM. This required a custom dataset of people to be collected. The CNN was pre-trained on ImageNet

Viola-jones for face detection and tracking for blimp~\cite{yao_monocular_2017}.

Laser for slam, skeleton for person tracking (OpenNI)~\cite{yuan_development_2018}

Condes et al~\cite{condes_person_2019} first use a single shot detector (SSD) network~\cite{liu_ssd_2016} to detect a person, then use the Viola-Jones method to determine the region of the persons face. Tracking is performed by determining if a detection in a new frame is near the pixel location in a previous frame. Finally, FaceNet~\cite{schroff_facenet_2015}, a CNN that outputs a vector embedding of a face, is used to determine if the face detected is of a known face, and is used for re-identification when a target person is lost.

Luo et al~\cite{luo_real-time_2019} use a kernalised correlation filter (KCF) to track a target across image frames. An extended kalman filter is used to account for the noise of the predictions.

Skeleton information obtained with Kinect SDK, tracking is performed with optic flow~\cite{wu_improved_2019}

OpenPose for skeleton~\cite{yang_socially-aware_2019}, social force model for navigation adhering to social conventions. Laser for obstacles.

A person is detected and tracked using the contours of the target in the image (target contour band)~\cite{zhang_vision-based_2019}. The robot follows the target using image based visual servo control using a kinematic model of the robot and the bounding corners of the target in the image.

SSD for person, FaceNet to detect specific person, KCF for tracking~\cite{hwang_interactions_2020}.

With gestures:
Laser for leg detection, Viola-Jones for face detection and tracking~\cite{pereira_human-robot_2013}.

Body tracking using skeleton extracted from the Microsoft Kinect SDK~\cite{fujii_gesture_2014}

Prediger et al~\cite{prediger_robot-supported_2014} extract a skeleton tracking with particle filter.

Skeleton extraction, Gesture Builder, and Kalman filter~\cite{long_kinect-based_2018}

OpeNI for joint angles, and motion tracking with human motion prediction~\cite{chen_human-following_2019}.

OpenPose for gestures and skeleton~\cite{miller_self-driving_2019}, goals are given in the robot occupancy map that are a fixed offset from the tracked human.

Kinect SDK for skeleton and following~\cite{anuradha_human_2020}.

Colour segmentation~\cite{wu_toward_2020}.

A vision network is used to create an intelligent work space, where a mobile robot can identify and avoid people~\cite{almonfrey_flexible_2018}. Pedestrian detector models. The pedestrian detectors
(ICCF and ACF) were trained with the INRIA dataset~\cite{dalal_histograms_2005}. Bounding box dimensions are used to determine the 3D location of the people and robot in the scene.

Viola-Jones on custom dataset of people (not just face, body poses)~\cite{luo_human_2010}.

% ==================================

How we are classifying RMHS: 

- Mobile robot and the person is passive in the visual exchange (else it would be gesture or action), (examples: following, approaching, avoiding). Can exist with gesture and action, provided RMHS is separate (e.g. the technique hasn't already been mentioned).

ALL MOBILE ROBOTS:

OTHER

Operator in the loop for surveillance mobile robots~\cite{do_human-robot_2014}

GESTURE CONTROL

Mobile robot goes to location pointed to~\cite{martin_estimation_2010, van_den_bergh_real-time_2011, abidi_human_2013, park_real-time_2011}. In the case of ~\cite{van_den_bergh_real-time_2011} pointing occurs from the hand, and the pointing location is added to the robot map for otherwise autonomous exploration. Abidi uses PR-2 in domestic environment. Park uses social robot in domestic environment.

Approaching with handover: Mobile with manipulator, follows user gestures, face, and voice commands, navigates to person, around person, and to area pointed to (table)~\cite{burger_two-handed_2012}. Mobile manipulator hands over water bottle (R2D2 with small robot inside)~\cite{cheng_multiple-robot_2013}.

Examples of state change include initiating person guiding or following~\cite{pereira_human-robot_2013}, or choosing path direction (left or right) of an otherwise autonomous navigating robot~\cite{lalejini_evaluation_2015, ehlers_human-robot_2016}. Service robot goes into follow-me mode, wait, or stop with waving of arms~\cite{fujii_gesture_2014}

~\cite{fareed_gesture_2015} perform obstacle detection and avoidance while responding to gestures commands.

~\cite{jevtic_comparison_2015} perform person following, and compare to pointing.



% Up to here with movement + gesture ==============
~\cite{prediger_robot-supported_2014}
% =================================================

APPROACHING

~\cite{ferrer_robot_2013}

Robot approaches person with face detection~\cite{budiharto_indoor_2010}, with stereo vision ~\cite{chen_stereovision-only_2014}

Engagement detection~\cite{vaufreydaz_starting_2016}



FOLLOWING 

~\cite{alvarez-santos_feature_2012, mead_probabilistic_2012, ali_human_2013, yun_robotic_2013, munaro_fast_2014, gupta_robust_2015, kahily_real-time_2016, talebpour_-board_2016}

Social considerations~\cite{vasconcelos_socially_2016}

With speech as well~\cite{nguyen_audio-visual_2014, bayram_audio-visual_2016}

Person wears haptic band that signals when the human its leaving the path, multiple following robots that use vision~\cite{scheggi_human-robot_2014}

UAV follows a blue gloved hand~\cite{miyoshi_above_2014}, 


Anticipate human behaviour when following~\cite{hu_design_2014}


Person wearing particlar clothing~\cite{chen_person_2012}, or coloured marker~\cite{hassan_computationally_2016} (low computational cost)

Service robot person following ~\cite{fahn_real-time_2010}

Wheelchair and museum robot following, also has laser~\cite{kobayashi_people_2010}

Wheelchair robot following, also use lasers, vision for re-finding person~\cite{wu_accompanist_2012}

Following and re-identification~\cite{weinrich_appearance-based_2013}


Mobile robot with manipulator approaches human and hands over a bottle~\cite{sisbot_synthesizing_2010}

People following with several sensors in addition to vision (speech, IR, ultrasonic)~\cite{yun_robust_2010}

Human following with stereo vision~\cite{jia_autonomous_2011, ali_improved_2015}

Hand tracking for controlling mobile robot~\cite{paulo_vision-based_2012}



APPLICATION AREAS:
===================================================================================================
ACTION RECOGNITION:
===================================================================================================



===================================================================================================
GESTURE RECOGNITION:
===================================================================================================

Number of control papers:
HAND: 11, 5, 4, 8 = 28
Body gestures: 11 + 20 + 12 = 43
Facial expressions: 3
Total = 74

HAND GESTURES

Control: 11

Control: 5
Hand gestures for controlling robot arm with pre-defined responses~\cite{choudhary_real_2015, deepan_raj_static_2017, song_towards_2017}, lego mindstorm~\cite{song_towards_2017}, or for object handover where the robot arm places an object in the persons hand if it is open~\cite{arenas_deep_2017}, or for object selection and confirmation~\cite{vysocky_interaction_2019}

Control: 4
Coupled with joint control where the hand gesture controls the gripper (open or close)~\cite{fareed_gesture_2015, lima_real-time_2019, martin_real-time_2019, luo_human-robot_2019}. Continuous joint control (following human joint position)~\cite{lima_real-time_2019, martin_real-time_2019}, or with body gestures that controls the arm position~\cite{fareed_gesture_2015}. Lifting gripper~\cite{luo_human-robot_2019}.

Control: 8
Wearable cameras attached to a persons wrist that detect hand gestures to control a wheelchair robot~\cite{yang_novel_2018} and a dual arm robot~\cite{chen_wristcam_2019}.

Mobile robot with gripper, two hand gestures, one to move the robot base, and the other to control the gripper~\cite{fareed_gesture_2015, fareeluo_human-robot_2019}. 

Multimodel fusion~\cite{gao_hand_2020}, astronaut hand mimic, MYO sensor. 

Hand pointing (pointing classified just from hand): ~\cite{luo_tracking_2011, ikai_robot_2016, du_online_2018, vysocky_interaction_2019}. 

Hand gestures (including pointing) can be classified by first getting the human pose, then extracting the region of pixels where the hand would be~\cite{van_den_bergh_real-time_2011}.

% --------------------------------
% Not control:
Hand gestures for playing paper scissors rock~\cite{katsuki_high_speed_2015, yuan_natural_2020}.

Hand gestures for interacting with social robot (humanoid)~\cite{ju_integrative_2017}

Hand gestures can be used to program a robot, such as in the work by Mazhar et al~\cite{mazhar_real-time_2019}, where a pre-determined hand gesture signifies the recording of a task. The robot becomes compliant once the command has been received so the human programmer can manually move the robot end effector to perform a desired action. A second gesture determines the completion of programming, and the robot begins executing the new task. 

An industrial robot arm tracks hand position while learning from online demonstrations. When the robot gets close to the desired position, fine tuning is performed by speech and static hand gesture pointing~\cite{du_online_2018}.

A humanoid robot (Pepper and NAO) detects and tracks a human, then identifies gestures (fingerspelling), then performs a gesture response for communication with deaf-mute people, tested in a public service centre where 9/10 people preferred an embodied robot over a virtual robot on a screen~\cite{kalidolda_towards_2018}.

BODY GESTURES

% Static gesture (pointing, hand gestures), dynamic gesture such as waving (Nao robot on mobile base, waves back, and goes to object pointed to)~\cite{canal_gesture_2015}.

POINTING = 3 hand + 19 from body = 22
WAVING = 7
Body pose control: 11 + 20 + 12

% -------------------------------------
Control: 11
Pointing gestures for controlling mobile robots~\cite{park_real-time_2011, van_den_bergh_real-time_2011, yoshida_evaluation_2011, abidi_human_2013, nishiyama_human_2013, prediger_robot-supported_2014, chen_stereovision-only_2014, jevtic_comparison_2015, tolgyessy_foundations_2017, chen_human-following_2019}. Pointing with two hands~\cite{burger_two-handed_2012}. 

Control: 20
Pointing for robot selection in multi-robot team, UAV:~\cite{lichtenstern_prototyping_2012}, mobile robot ~\cite{milligan_selecting_2011}, multi-robot team (closes robot goes to pointing location)~\cite{nishiyama_human_2013}.

Pointing at an object for mobile robot with manipulator to pick up~\cite{droeschel_towards_2011, moh_gesture_2019, sriram_mobile_2019}, 

robot confirms object by also pointing~\cite{quintero_visual_2015}.

Body gesture control of mobile robot with pre-defined responses~\cite{fujii_gesture_2014,xu_online_2014, cicirelli_kinect-based_2015,fareed_gesture_2015,lalejini_evaluation_2015, ghandour_human_2017,long_kinect-based_2018, pentiuc_drive_2018, zhang_interactive_2018, zhang_automating_2018, chen_human-following_2019,miller_self-driving_2019, kahlouche_human_2019}, (e.g. waving gestures for follow-me, bye-bye)~\cite{fujii_gesture_2014}, turn left/right~\cite{lalejini_evaluation_2015}, and drawing shapes with hands~\cite{xu_online_2014}. Shoulder angle used to control heading angle of a mobile robot with discrete angles (45 degree increments)~\cite{wang_wheeled_2013}. Body gesture control of otherwise autonomous mobile robot (turn, stop)~\cite{ghandour_human_2017}, or when to start or stop following (a nurse in an hospital setting)~\cite{long_kinect-based_2018}, or for an autonomous ottoman that determines if a user would like to put their feet up~\cite{zhang_automating_2018}. Another following robot that changes state with gestures~\cite{chen_human-following_2019}. Tracked robot forward/backwards/left/right from body gestures~\cite{fang_vehicle-mounted_2019}. Following and parking behaviour with body gestures~\cite{miller_self-driving_2019}. Mobile robot approaches human with waving gesture, stops where pointed~\cite{kahlouche_human_2019}

Control: 12
A wheelchair robot velocity is commanded by human shoulder position, and changes configuration (bed mode and sitting mode) with body gesture~\cite{mao_medical_2018}

Mobile robot with manipulator picks up object pointed to~\cite{qian_visually_2013}.

Select mobile robot by looking directly at it in a multi-robot team, with optic-flow to classify waving gesture to determine where the robot should move to~\cite{couture-beil_selecting_2010}. Multiple UAV's detect a persons face direction, robot selection is performed when the person waves to a robot it they are looking at, deselection occurs when they wave with the opposite hand. The selected robots then respond to a command (e.g. land)~\cite{monajjemi_hri_2013}

Direct joint control of humanoid upper body, plus body gestures (rotate the torso, move one foot forward or backward) to control mobile base~\cite{broccia_gestural_2011}. Humanoid (Nao) on mobile base, Nao waves if human waves, mobile base goes to location pointed to~\cite{canal_gesture_2015}.

Body gesture control of humanoid with pre-defined response (e.g. walking)~\cite{shieh_fuzzy_2014,gao_humanoid_2015,han_human_2017}.

Multiple robots, where one robot observes the pointing gesture and communicates with the robot target (UAV's)~\cite{lichtenstern_prototyping_2012}, or to the closest mobile robot to the pointing target~\cite{nishiyama_human_2013}.

Swarm pattern control of multiple robots with gestures. Body pose (arms out, above head), signify different configurations of (little robots make a surprised or happy face)~\cite{alonso-mora_human_2014}.
% -------------------------------------
% Not control:
Programming an industrial robot arm with body gestures~\cite{stipancic_programming_2012}, and hand gestures~\cite{mazhar_real-time_2019}.

Robot provides feedback for human performing pose from exercise program ~\cite{werner_evaluation_2013, gorer_autonomous_2017,  avioz-sarig_robotic_2020} (Nao robot, poppy robot).

Mobile robot approaches human after inferring user intent through body pose and facial expressions~\cite{li_inferring_2019}

HEAD GESTURES
control: 3
Robot head (Eddie) mimic facial expressions~\cite{sosnowski_mirror_2010}, Alice~\cite{meghdari_real-time_2017}, reeti~\cite{masmoudi_expressive_2011}

Interest detection from head gestures (shaking, nodding head), static (head up or down) for social interaction with humanoid upper body~\cite{saleh_nonverbal_2015}.

Robot head performs set of facial expressions for human classification (e.g. if human sad, robot will be surprised, then sad)~\cite{ke_vision_2016}

Social interaction by detection facial expressions (respond depending on expression) HAAR features for detecting face region, then CNN and LSTM. Use fine-tuning from a pre-trained network.~\cite{li_cnn_2019}

Engagement detection from facial expression recognition~\cite{castellano_context-sensitive_2014}.





===================================================================================================
\subsubsection{Object Handover and Collaborative Actions}
===================================================================================================
% Robot counts:
% UR series of robot arm (n=4), others include Kuka (n=5), ABB (n=3), SCARA (n=1), Staubili (n=2), Baxter (n=2), Sawyer (n=1), Frankas Panda arm (n=3), Lab-Volt 5150, ASIMO, Jido, Adept Cobra s800, Kawasaky RS005LA, WAM robot arm = 17 singles/unknowns

Safe workspace ~\cite{nair_3d_2011}

Industrial arm moves Tool Center Point (TCP) to mimic human hand~\cite{christiernin_interacting_2016}, Asimo moves end effectors to mimic human hand position~\cite{arumbakkam_multi-modal_2010}

Approaching with handover: Mobile with manipulator, follows user gestures, face, and voice commands, navigates to person, around person, and to area pointed to (table)~\cite{burger_two-handed_2012}. Mobile manipulator hands over water bottle (R2D2 with small robot inside)~\cite{cheng_multiple-robot_2013}.

===================================================================================================
\subsubsection{Learning from Demonstration}
===================================================================================================

===================================================================================================
\subsubsection{Social Classification and Communication} 
===================================================================================================



===================================================================================================
USER STUDIES: see User Studies spreadsheet
===================================================================================================
Action recognition:
~\cite{lee_learning_2017, efthymiou_multi-_2018, sorostinean_activity_2018, lang_research_2020}

Gesture recognition:
~\cite{martin_estimation_2010, park_real-time_2011, yoshida_evaluation_2011, faudzi_real-time_2012, gori_all_2012, abidi_human_2013,werner_evaluation_2013,castellano_context-sensitive_2014, fujii_gesture_2014, prediger_robot-supported_2014, canal_gesture_2015, gao_humanoid_2015, jevtic_comparison_2015, lalejini_evaluation_2015, saleh_nonverbal_2015, ehlers_human-robot_2016, ghandour_human_2017, gorer_autonomous_2017, han_human_2017, kalidolda_towards_2018, yuan_development_2018, zhang_automating_2018, li_inferring_2019, lima_real-time_2019, avioz-sarig_robotic_2020, yuan_natural_2020}

Robot movement:
~\cite{martin_estimation_2010,park_real-time_2011,yoshida_evaluation_2011,ferrer_robot_2013,abidi_human_2013, scheggi_human-robot_2014,fujii_gesture_2014, prediger_robot-supported_2014,canal_gesture_2015,jevtic_comparison_2015, lalejini_evaluation_2015, ehlers_human-robot_2016, mollaret_multi-modal_2016, ghandour_human_2017, yuan_development_2018, zhang_automating_2018, li_inferring_2019}

Object Handover and Collaborative Actions
~\cite{christiernin_interacting_2016, lee_learning_2017, kogkas_free-view_2019, lima_real-time_2019, valle_personalized_2019, arai_service_2020}

Social:
~\cite{foster_two_2012, gori_all_2012,werner_evaluation_2013, castellano_context-sensitive_2014,taheri_social_2014,  gao_humanoid_2015, li_visual_2015, saleh_nonverbal_2015, mollaret_multi-modal_2016, bilac_gaze_2017, gorer_autonomous_2017, efthymiou_multi-_2018, kalidolda_towards_2018, sorostinean_activity_2018, avioz-sarig_robotic_2020, lang_research_2020, yuan_natural_2020}