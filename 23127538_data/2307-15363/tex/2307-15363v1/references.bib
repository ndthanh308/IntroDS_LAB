@inproceedings{sadigh2016planning,
  title={Planning for autonomous cars that leverage effects on human actions.},
  author={Sadigh, Dorsa and Sastry, Shankar and Seshia, Sanjit A and Dragan, Anca D},
  booktitle={Robotics: Science and systems},
  volume={2},
  pages={1--9},
  year={2016},
  organization={Ann Arbor, MI, USA}
}

@article{driggs2017integrating,
  title={Integrating intuitive driver models in autonomous planning for interactive maneuvers},
  author={Driggs-Campbell, Katherine and Govindarajan, Vijay and Bajcsy, Ruzena},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={18},
  number={12},
  pages={3461--3472},
  year={2017},
  publisher={IEEE}
}

@inproceedings{ognibene2013towards,
  title={Towards Active Event Recognition.},
  author={Ognibene, Dimitri and Demiris, Yiannis},
  booktitle={IJCAI},
  pages={2495--2501},
  year={2013}
}

@article{halin2021survey,
  title={Survey and synthesis of state of the art in driver monitoring},
  author={Halin, Ana{\"\i}s and Verly, Jacques G and Van Droogenbroeck, Marc},
  journal={Sensors},
  volume={21},
  number={16},
  pages={5558},
  year={2021},
  publisher={MDPI}
}

@inproceedings{rosa_integration_2016,
	title = {Integration of {People} {Detection} and {Simultaneous} {Localization} and {Mapping} {Systems} for an {Autonomous} {Robotic} {Platform}},
	doi = {10.1109/LARS-SBR.2016.49},
	abstract = {This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment.},
	booktitle = {2016 {XIII} {Latin} {American} {Robotics} {Symposium} and {IV} {Brazilian} {Robotics} {Symposium} ({LARS}/{SBR})},
	author = {Angonese, Alberto Torres and Ferreira Rosa, Paulo Fernando},
	month = oct,
	year = {2016},
	note = {7},
	keywords = {Feature extraction, Mathematical model, Libraries, Histograms, Simultaneous localization and mapping, MATLAB, CNN, Deep Learning, HOG, People Detection, robot operating system ROS, Simultaneous Localization and Mapping (SLAM), 1XPLORE, 2XPLORE, 1people},
	pages = {251--256},
	file = {Angonese and Ferreira Rosa - 2016 - Integration of People Detection and Simultaneous L.pdf:/home/brendan/Zotero/storage/GIYN6RLV/Angonese and Ferreira Rosa - 2016 - Integration of People Detection and Simultaneous L.pdf:application/pdf},
}

@article{iqbal2017coordination,
  title={Coordination dynamics in multihuman multirobot teams},
  author={Iqbal, Tariq and Riek, Laurel D},
  journal={IEEE Robotics and Automation Letters},
  volume={2},
  number={3},
  pages={1712--1717},
  year={2017},
  publisher={IEEE}
}

@inproceedings{lovon-ramos_people_2016,
	title = {People {Detection} and {Localization} in {Real} {Time} during {Navigation} of {Autonomous} {Robots}},
	doi = {10.1109/LARS-SBR.2016.47},
	abstract = {Currently the navigation involves the interaction of the robot with its environment, this means that the robot has to find the position of obstacles (natural brands and artificial) with respect to its plane. Its environment is time-variant and computer vision can help it to localization and people detection in real time. This article focuses on the detection and localization of people with respect to plane of the robot during the navigation of autonomous robot, for people detection is used Morphological HOG Face Detection algorithm in real-time, where our goal is to localization people in the plane of the robot, obtaining position information relative to the X-Axis (left, right, obstacle) and with the Y-Axis (near, medium, far) with respect robot, to identify the environment in that it's located in the robot is applied the vanishing point detection. Experiments show that people detection and localization is better in the medium region (201 to 600 cm) obtaining 93.13\% of accuracy, this allows the robot has enough time to evade the obstacle during navigation, the navigation getting 97.03\% of accuracy for the vanishing point detection.},
	booktitle = {2016 {XIII} {Latin} {American} {Robotics} {Symposium} and {IV} {Brazilian} {Robotics} {Symposium} ({LARS}/{SBR})},
	author = {Lovon-Ramos, Percy W. and Rosas-Cuevas, Yessica and Cervantes-Jilaja, Claudia and Tejada-Begazo, Maria and Patiño-Escarcina, Raquel E. and Barrios-Aranibar, Dennis},
	month = oct,
	year = {2016},
	note = {6},
	keywords = {Service robots, Face, Navigation, Computer vision, Mathematical model, People detection, Detection algorithms, 1XPLORE, Autonomous Vehicle Navigation, HOG method, Vanishing Point, 2XPLORE, 1people},
	pages = {239--244},
	file = {Lovon-Ramos et al. - 2016 - People Detection and Localization in Real Time dur.pdf:/home/brendan/Zotero/storage/VZ8S6X2N/Lovon-Ramos et al. - 2016 - People Detection and Localization in Real Time dur.pdf:application/pdf},
}

@inproceedings{12_intelligent_2019,
	title = {Intelligent {Interactive} {Robot} {System} for {Agricultural} {Knowledge} {Popularity} and {Achievements} {Display}},
	volume = {1},
	doi = {10.1109/IAEAC47372.2019.8997911},
	abstract = {The development of a guide robot with explanatory features has always been a subject of concern. Most urban populations, especially the youth, have little knowledge about agricultural activities. To address this issue, many exhibition centers featuring the theme of local agriculture were built. These agricultural exhibitions provide opportunities to extend agricultural knowledge to urban communities. However, the exhibition centers also need efficient ways of communication for introducing visitors to the exhibitions' achievements. In this regard, we designed an intelligent interactive robot system. For this purpose, we first collected agricultural knowledge to develop custom database questions and answers and made corresponding images. Then, designed a new interactive way through combining the questions and answers with the image display. Finally, two interactive modules were designed for people of two ages (youth and adults) attending the exhibition. The developed robot system was used in "The first china international smart agriculture achievements exhibition" for the first time. The feedback results showed that the robot system helped the visitors learn more agricultural knowledge. Moreover, this robot system provided a more comprehensive way of understanding the exhibition for the visitors.},
	booktitle = {2019 {IEEE} 4th {Advanced} {Information} {Technology}, {Electronic} and {Automation} {Control} {Conference} ({IAEAC})},
	author = {Sun, Xiaowen and Zhao, Ran and Khattak, Abdul Mateen and Shi, Kaite and Ren, Yanzhao and Gao, Wanlin and Wang, Minjuan},
	month = dec,
	year = {2019},
	note = {0},
	keywords = {Robots, Cameras, Microphone arrays, Speech recognition, Databases, voice recognition, Agriculture, 1XPLORE, interaction design, agricultural knowledge, explanation of the exhibition hall, human-robot interaction (HRI), voice interaction, 2XPLORE, 1people},
	pages = {511--518},
	file = {Sun et al. - 2019 - Intelligent Interactive Robot System for Agricultu.pdf:/home/brendan/Zotero/storage/6UHUYT6M/Sun et al. - 2019 - Intelligent Interactive Robot System for Agricultu.pdf:application/pdf},
}

@inproceedings{gardel_wireless_2016,
	address = {New York, NY, USA},
	series = {{ICDSC} '16},
	title = {Wireless {Camera} {Nodes} on a {Cyber}-{Physical} {System}},
	isbn = {978-1-4503-4786-0},
	url = {https://doi.org/10.1145/2967413.2967423},
	doi = {10.1145/2967413.2967423},
	abstract = {This paper describes the configuration of a networked control system with multiple distributed cameras as edge nodes of a cyber-physical system. The proposed architecture deploys multiple distributed on computer vision edge nodes capturing the motion of mobile robots to interact with people on the environment. The camera nodes, network devices and mobile robots are considered part of the same control loop. Our approach provides a flexible scalable architecture that balances accuracy and reliability of the cyber-physical system, taking advantage of video processing on intermediate networked nodes and providing new functions and capabilities with minor changes. Main contribution of this paper is to configure the computer vision nodes considering the networked control loop with an adjustable sampling period to achieve the desired performance controlling the trajectory of multiple robots. Different tests have been done on a smart space as an end application framework.},
	booktitle = {Proceedings of the 10th {International} {Conference} on {Distributed} {Smart} {Camera}},
	publisher = {Association for Computing Machinery},
	author = {Gardel, A. and Espinosa, F. and Nieto, R. and Lázaro, J. L. and Bravo, I.},
	year = {2016},
	note = {0},
	keywords = {real-time detection, 1ACM, communication channel., Cyber-physical system, trajectory control loop, 2ACM, 1people},
	pages = {31--36},
	file = {Gardel et al. - 2016 - Wireless Camera Nodes on a Cyber-Physical System.pdf:/home/brendan/Zotero/storage/P28QU9VE/Gardel et al. - 2016 - Wireless Camera Nodes on a Cyber-Physical System.pdf:application/pdf},
}

@article{brookshire_person_2010,
	title = {Person following using histograms of oriented gradients},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857629076&doi=10.1007%2fs12369-010-0046-y&partnerID=40&md5=e81d41fcf0d2625a277404663bfb818e},
	doi = {10.1007/s12369-010-0046-y},
	abstract = {In order for robots to effectively interact with people in close proximity, the systems must first be able to detect, track, and follow people. This paper describes results from the development of a mobile robot which will follow a single, unmarked pedestrian using vision. This work demonstrates an improvement over existing pedestrian following applications because (1) it uses sufficiently strong classifiers such that it does not need to adapt to any particular pedestrian, (2) uses only vision and does not rely on any laser range devices, (3) provides a single point benchmark for the level of performance required from a detector to achieve pedestrian following, and (4) its performance is characterized over several kilometers in both rainy and dry weather conditions. The system leverages Histograms of Oriented Gradients (HOG) features for pedestrian detection at over 8 Hz using video from a monochrome camera. The pedestrian's heading is combined with distance from stereo depth data to yield a 3D estimate. A particle filter with some clutter rejection provides a continuous track, and a waypoint follow behavior servos the iRobot PackBot robot chassis to a desired location behind the pedestrian. The final system is able to detect, track, and follow a pedestrian over several kilometers in outdoor environments, demonstrating a level of performance not previously shown on a small unmanned ground vehicle. © Springer Science \& Business Media BV 2010.},
	number = {2},
	journal = {International Journal of Social Robotics},
	author = {Brookshire, J.},
	year = {2010},
	note = {31},
	keywords = {1SCOPUS, 1people},
	pages = {137--146},
	annote = {cited By 16},
	file = {Brookshire - 2010 - Person following using histograms of oriented grad.pdf:/home/brendan/Zotero/storage/LEDIA2E2/Brookshire - 2010 - Person following using histograms of oriented grad.pdf:application/pdf},
}

@article{pennisi_multi-robot_2015,
	title = {Multi-robot surveillance through a distributed sensor network},
	volume = {604},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929630291&doi=10.1007%2f978-3-319-18299-5_4&partnerID=40&md5=e3a4c46074d758eaa300b5abc0c34d62},
	doi = {10.1007/978-3-319-18299-5_4},
	abstract = {Automatic surveillance of public areas, such as airports, train stations, and shopping malls, requires the capacity of detecting and recognizing possible abnormal situations in populated environments. In this book chapter, an architecture for intelligent surveillance in indoor public spaces, based on an integration of interactive and non-interactive heterogeneous sensors, is described. As a difference with respect to traditional, passive and pure vision-based systems, the proposed approach relies on a distributed sensor network combining RFID tags, multiple mobile robots, and fixed RGBD cameras. The presence and the position of people in the scene is detected by suitably combining data coming from the sensor nodes, including those mounted on board of the mobile robots that are in charge of patrolling the environment. The robots can adapt their behavior according to the current situation, on the basis of a Prey-Predator scheme, and can coordinate their actions to fulfill the required tasks. Experimental results have been carried out both on real and on simulated data to show the effectiveness of the proposed approach. © Springer International Publishing Switzerland 2015},
	journal = {Studies in Computational Intelligence},
	author = {Pennisi, A. and Previtali, F. and Gennari, C. and Bloisi, D.D. and Iocchi, L. and Ficarola, F. and Vitaletti, A. and Nardi, D.},
	year = {2015},
	note = {16},
	keywords = {1SCOPUS, 1people},
	pages = {77--98},
	annote = {cited By 8},
	file = {Pennisi et al. - 2015 - Multi-robot surveillance through a distributed sen.pdf:/home/brendan/Zotero/storage/UHMNDKPD/Pennisi et al. - 2015 - Multi-robot surveillance through a distributed sen.pdf:application/pdf},
}

@article{bastos_robot-assisted_2019,
	title = {Robot-{Assisted} {Autism} {Spectrum} {Disorder} {Diagnostic} {Based} on {Artificial} {Reasoning}},
	volume = {96},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064197630&doi=10.1007%2fs10846-018-00975-y&partnerID=40&md5=725381edefec9ab255f1528b77b422c1},
	doi = {10.1007/s10846-018-00975-y},
	abstract = {Autism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are found in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral observation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral observation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior assessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum Disorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when compared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding to summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance the traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable network of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face analysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three children in risk of suffering from ASD. © 2019, Springer Nature B.V.},
	number = {2},
	journal = {Journal of Intelligent and Robotic Systems: Theory and Applications},
	author = {Ramírez-Duque, A.A. and Frizera-Neto, A. and Bastos, T.F.},
	year = {2019},
	note = {14},
	keywords = {1SCOPUS, 1people},
	pages = {267--281},
	annote = {cited By 5},
	file = {Full Text:/home/brendan/Zotero/storage/XRKUPH2N/Ramírez-Duque et al. - 2019 - Robot-Assisted Autism Spectrum Disorder Diagnostic.pdf:application/pdf},
}

@inproceedings{santos_copyrobot_2020,
	title = {{CopyRobot}: {Interactive} {Mirroring} {Robotics} {Game} for {ASD} {Children}},
	volume = {76},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075865820&doi=10.1007%2f978-3-030-31635-8_239&partnerID=40&md5=fb1776efbfde7b1939533f9868cb1964},
	doi = {10.1007/978-3-030-31635-8_239},
	abstract = {The family of disorders commonly known as autism is characterized by a deficit in social interaction and restricted repetitive and stereotyped patterns of behaviours, activities and interests. Motor disturbances are not part of the diagnosis of the children with autism but some studies have estimated that between 80 and 90\% of children with Autism Spectrum Disorder (ASD) demonstrate some degree of motor impairments. Several therapies have been used for the improvement of motor skills, always leading to behavioural improvements as side-effects, demonstrating the importance of motor interaction and stimulation for the case of autism. Recent studies have shown that motor, imitation and social abilities are all related in people with autism. In this work, a humanoid robot is used to create a therapy that unites all these areas. The system involves a robot (NAO), a Kinect camera and Personal Computer, with the goal of facilitating the interaction between therapist and a child with ASD during a physical therapy session. To improve the imitation abilities of the child, the robot was programmed to mirror both the child and the therapist movements. After testing different tracking methodologies, the Kinect sensor was selected as the best compromise of quality and cost. Two protocols were developed, depending on who plays the role of the main actor. In the first protocol, the robot is the master and leads the interaction. It decides the exercise to execute and gives feedback to both the therapist and the child. In the second protocol, the choice of the exercise sequence is the therapist’s responsibility. To promote interaction further during clinical tests, the protocol was changed to include gesture imitation. For the robot master protocol, the space theme was chosen. For the therapist master protocol, the theme of sports, that was already performed by the children in the usual therapy, was adopted. The system was tested in realistic conditions with two different autistic children. The reaction was different in each case but it demonstrated the importance of these imitation games in the treatment of this disease. © 2020, Springer Nature Switzerland AG.},
	booktitle = {{IFMBE} {Proceedings}},
	author = {Santos, L. and Geminiani, A. and Olivieri, I. and Santos-Victor, J. and Pedrocchi, A.},
	year = {2020},
	note = {3},
	keywords = {1SCOPUS, 1people},
	pages = {2014--2027},
	annote = {cited By 3},
	file = {Santos et al. - 2020 - CopyRobot Interactive Mirroring Robotics Game for.pdf:/home/brendan/Zotero/storage/53S3M524/Santos et al. - 2020 - CopyRobot Interactive Mirroring Robotics Game for.pdf:application/pdf},
}

@incollection{agapito_chalearn_2015,
	address = {Cham},
	title = {{ChaLearn} {Looking} at {People} {Challenge} 2014: {Dataset} and {Results}},
	volume = {8925},
	isbn = {978-3-319-16177-8 978-3-319-16178-5},
	shorttitle = {{ChaLearn} {Looking} at {People} {Challenge} 2014},
	url = {http://link.springer.com/10.1007/978-3-319-16178-5_32},
	language = {en},
	urldate = {2021-06-23},
	booktitle = {Computer {Vision} - {ECCV} 2014 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Escalera, Sergio and Baró, Xavier and Gonzàlez, Jordi and Bautista, Miguel A. and Madadi, Meysam and Reyes, Miguel and Ponce-López, Víctor and Escalante, Hugo J. and Shotton, Jamie and Guyon, Isabelle},
	editor = {Agapito, Lourdes and Bronstein, Michael M. and Rother, Carsten},
	year = {2015},
	doi = {10.1007/978-3-319-16178-5_32},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {459--473},
	file = {Escalera et al. - 2015 - ChaLearn Looking at People Challenge 2014 Dataset.pdf:/home/brendan/Zotero/storage/YFXAMAU9/Escalera et al. - 2015 - ChaLearn Looking at People Challenge 2014 Dataset.pdf:application/pdf}
}

@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@INPROCEEDINGS{7451807,  author={Kwon, Minae and Jung, Malte F. and Knepper, Ross A.},  booktitle={2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},   title={Human expectations of social robots},   year={2016},  volume={},  number={},  pages={463-464},  doi={10.1109/HRI.2016.7451807}}

@inproceedings{10.1145/3371382.3378347,
author = {Thellman, Sam and Silvervarg, Annika and Ziemke, Tom},
title = {Anthropocentric Attribution Bias in Human Prediction of Robot Behavior},
year = {2020},
isbn = {9781450370578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371382.3378347},
doi = {10.1145/3371382.3378347},
abstract = {In many types of human-robot interactions, people must track the beliefs of robots
based on uncertain estimates of robots' perceptual and cognitive capabilities. Did
the robot see what happened and did it understand what it saw? In this paper, we present
preliminary experimental evidence that people estimating what a humanoid robot knows
or believes about the environment anthropocentrically assume it to have human-like
perceptual and cognitive capabilities. However, our results also suggest that people
are able to adjust their incorrect assumptions based on observations of the robot.},
booktitle = {Companion of the 2020 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {476–478},
numpages = {3},
keywords = {human-robot interaction, belief-tracking, mental state attribution},
location = {Cambridge, United Kingdom},
series = {HRI '20}
}

@article{duffy2003anthropomorphism,
  title={Anthropomorphism and the social robot},
  author={Duffy, Brian R},
  journal={Robotics and autonomous systems},
  volume={42},
  number={3-4},
  pages={177--190},
  year={2003},
  publisher={Elsevier}
}

@article{clark2019advancing,
  title={Advancing the ethical use of digital data in human research: challenges and strategies to promote ethical practice},
  author={Clark, Karin and Duckham, Matt and Guillemin, Marilys and Hunter, Assunta and McVernon, Jodie and O’Keefe, Christine and Pitkin, Cathy and Prawer, Steven and Sinnott, Richard and Warr, Deborah and others},
  journal={Ethics and Information Technology},
  volume={21},
  number={1},
  pages={59--73},
  year={2019},
  publisher={Springer}
}

@article{freund1997decision,
  title={A decision-theoretic generalization of on-line learning and an application to boosting},
  author={Freund, Yoav and Schapire, Robert E},
  journal={Journal of Computer and System Sciences},
  volume={55},
  number={1},
  pages={119--139},
  year={1997},
  publisher={Elsevier}
}

@inproceedings{butler2012naturalistic,
title = {A naturalistic open source movie for optical flow evaluation},
author = {Butler, D. J. and Wulff, J. and Stanley, G. B. and Black, M. J.},
booktitle = {European Conference on Computer Vision (ECCV)},
editor = {{A. Fitzgibbon et al. (Eds.)}},
publisher = {Springer-Verlag},
series = {Part IV, LNCS 7577},
month = oct,
pages = {611--625},
year = {2012}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@inproceedings{feichtenhofer2019slowfast,
  title={Slowfast networks for video recognition},
  author={Feichtenhofer, Christoph and Fan, Haoqi and Malik, Jitendra and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6202--6211},
  year={2019}
}

@inproceedings{pavlakos2018ordinal,
  title={Ordinal depth supervision for {3D} human pose estimation},
  author={Pavlakos, Georgios and Zhou, Xiaowei and Daniilidis, Kostas},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7307--7316},
  year={2018}
}

@article{loper2015smpl,
  title={{SMPL}: A skinned multi-person linear model},
  author={Loper, Matthew and Mahmood, Naureen and Romero, Javier and Pons-Moll, Gerard and Black, Michael J},
  journal={ACM Transactions on Graphics (TOG)},
  volume={34},
  number={6},
  pages={1--16},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kanazawa2018end,
  title={End-to-end recovery of human shape and pose},
  author={Kanazawa, Angjoo and Black, Michael J and Jacobs, David W and Malik, Jitendra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7122--7131},
  year={2018}
}

@inproceedings{kanazawa2019learning,
  title={Learning {3D} human dynamics from video},
  author={Kanazawa, Angjoo and Zhang, Jason Y and Felsen, Panna and Malik, Jitendra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5614--5623},
  year={2019}
}

@inproceedings{pavllo20193d,
  title={{3D} human pose estimation in video with temporal convolutions and semi-supervised training},
  author={Pavllo, Dario and Feichtenhofer, Christoph and Grangier, David and Auli, Michael},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7753--7762},
  year={2019}
}

@inproceedings{kocabas2020vibe,
  title={{VIBE}: Video inference for human body pose and shape estimation},
  author={Kocabas, Muhammed and Athanasiou, Nikos and Black, Michael J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5253--5263},
  year={2020}
}

@inproceedings{kolotouros2019convolutional,
  title={Convolutional mesh regression for single-image human shape reconstruction},
  author={Kolotouros, Nikos and Pavlakos, Georgios and Daniilidis, Kostas},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4501--4510},
  year={2019}
}

@inproceedings{qi2018learning,
  author          = {Qi, Siyuan and Wang, Wenguan and Jia, Baoxiong and Shen, Jianbing and Zhu, Song-Chun},
  title           = {Learning Human-Object Interactions by Graph Parsing Neural Networks},
  journal         = {Proceedings of the European Conference on Computer Vision},
  year            = 2018
}
@inproceedings{gupta2019nofrills,
  author          = {Gupta, Tanmay and Schwing, Alexander and Hoiem, Derek},
  title           = {No-Frills Human-Object Interaction Detection: Factorization, Layout Encodings, and Training Techniques},
  journal         = {Proceedings of the IEEE International Conference on Computer Vision},
  year            = 2019
}
@inproceedings{chao2018learning,
  author          = {Yu-Wei Chao and Yunfan Liu and Xieyang Liu and Huayi Zeng and Jia Deng},
  title           = {Learning to Detect Human-Object Interactions},
  journal         = {Proceedings of the IEEE Winter Conference on Applications of Computer Vision},
  year            = 2018
}
@inproceedings{zhang2021scg,
  author = {Frederic Z. Zhang, Dylan Campbell and Stephen Gould},
  title = {Spatially Conditioned Graphs for Detecting Human–Object Interactions},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = {October},
  year = {2021},
  pages = {13319-13327}
}
@article{beyer2018deep,
  title={Deep person detection in two-dimensional range data},
  author={Beyer, Lucas and Hermans, Alexander and Linder, Timm and Arras, Kai O and Leibe, Bastian},
  journal={IEEE Robotics and Automation Letters},
  volume={3},
  number={3},
  pages={2726--2733},
  year={2018},
  publisher={IEEE}
}
@inproceedings{vazquez2015parallel,
  title={Parallel detection of conversational groups of free-standing people and tracking of their lower-body orientation},
  author={V{\'a}zquez, Marynel and Steinfeld, Aaron and Hudson, Scott E},
  booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3010--3017},
  year={2015},
  organization={IEEE}
}
@inproceedings{linder2014multi,
  title={Multi-model hypothesis tracking of groups of people in RGB-D data},
  author={Linder, Timm and Arras, Kai O},
  booktitle={17th International Conference on Information Fusion (FUSION)},
  pages={1--7},
  year={2014},
  organization={IEEE}
}
@inproceedings{taylor2016robot,
  title={Robot perception of human groups in the real world: State of the art},
  author={Taylor, Angelique and Riek, Laurel D},
  booktitle={2016 AAAI Fall Symposium Series},
  year={2016}
}
@article{triggs2007scene,
  title={Scene segmentation with CRFs learned from partially labeled images},
  author={Triggs, Bill and Verbeek, Jakob},
  journal={Advances in Neural Information Processing Systems},
  volume={20},
  year={2007}
}
@article{boykov2001fast,
  title={Fast approximate energy minimization via graph cuts},
  author={Boykov, Yuri and Veksler, Olga and Zabih, Ramin},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={23},
  number={11},
  pages={1222--1239},
  year={2001},
  publisher={IEEE}
}
@article{horn1981determining,
  title={Determining optical flow},
  author={Horn, Berthold KP and Schunck, Brian G},
  journal={Artificial Intelligence},
  volume={17},
  number={1-3},
  pages={185--203},
  year={1981},
  publisher={Elsevier}
}
@inproceedings{lucas1981iterative,
  title={An iterative Image Registration Technique with an Application to Stereo Vision},
  author={Lucas, Bruce D and Kanade, Takeo},
  booktitle={Proc 7th Intl Joint Conf on Artificial Intelligence (IJ CAI)},
  year={1981}
}
@article{gould2008multi,
  title={Multi-class segmentation with relative location prior},
  author={Gould, Stephen and Rodgers, Jim and Cohen, David and Elidan, Gal and Koller, Daphne},
  journal={International Journal of Computer Vision},
  volume={80},
  number={3},
  pages={300--316},
  year={2008},
  publisher={Springer}
}
@article{krahenbuhl2011efficient,
  title={Efficient inference in fully connected crfs with gaussian edge potentials},
  author={Kr{\"a}henb{\"u}hl, Philipp and Koltun, Vladlen},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}
@article{felzenszwalb2005pictorial,
  title={Pictorial structures for object recognition},
  author={Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
  journal={International Journal of Computer Vision},
  volume={61},
  number={1},
  pages={55--79},
  year={2005},
  publisher={Springer}
}
@article{felzenszwalb2009object,
  title={Object detection with discriminatively trained part-based models},
  author={Felzenszwalb, Pedro F and Girshick, Ross B and McAllester, David and Ramanan, Deva},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={32},
  number={9},
  pages={1627--1645},
  year={2009},
  publisher={IEEE}
}
@inproceedings{boser1992training,
  title={A training algorithm for optimal margin classifiers},
  author={Boser, Bernhard E and Guyon, Isabelle M and Vapnik, Vladimir N},
  booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
  pages={144--152},
  year={1992}
}

@inproceedings{viola2001rapid,
  title={Rapid object detection using a boosted cascade of simple features},
  author={Viola, Paul and Jones, Michael},
  booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition},
  volume={1},
  year={2001},
  organization={IEEE}
}
@inproceedings{dalal2005histograms,
  title={Histograms of oriented gradients for human detection},
  author={Dalal, Navneet and Triggs, Bill},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={1},
  pages={886--893},
  year={2005},
  organization={IEEE}
}
@inproceedings{lowe1999object,
  title={Object recognition from local scale-invariant features},
  author={Lowe, David G},
  booktitle={Proceedings of the Seventh IEEE International Conference on Computer Vision},
  volume={2},
  pages={1150--1157},
  year={1999},
  organization={Ieee}
}
@inproceedings{agarwal2009building,
  title={Building Rome in a day},
  author={Agarwal, Sameer and Snavely, Noah and Simon, Ian and Seitz, Steven M and Szeliski, Richard},
  booktitle={2009 IEEE 12th International Conference on Computer Vision (ICCV)},
  pages={72--79},
  year={2009},
  organization={IEEE Computer Society}
}
@book{hartley2003multiple,
  title={Multiple view geometry in computer vision},
  author={Hartley, Richard and Zisserman, Andrew},
  year={2003},
  publisher={Cambridge University Press}
}
@inproceedings{izadi2011kinectfusion,
  title={Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera},
  author={Izadi, Shahram and Kim, David and Hilliges, Otmar and Molyneaux, David and Newcombe, Richard and Kohli, Pushmeet and Shotton, Jamie and Hodges, Steve and Freeman, Dustin and Davison, Andrew and others},
  booktitle={Proceedings of the 24th annual ACM symposium on User interface software and technology},
  pages={559--568},
  year={2011}
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}
@article{taylor2020robot,
  title={Robot-centric perception of human groups},
  author={Taylor, Angelique and Chan, Darren M and Riek, Laurel D},
  journal={ACM Transactions on Human-Robot Interaction (THRI)},
  volume={9},
  number={3},
  pages={1--21},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@inproceedings{vazquez2016maintaining,
  title={Maintaining awareness of the focus of attention of a conversation: A robot-centric reinforcement learning approach},
  author={V{\'a}zquez, Marynel and Steinfeld, Aaron and Hudson, Scott E},
  booktitle={2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
  pages={36--43},
  year={2016},
  organization={IEEE}
}
@inproceedings{arras2007using,
  title={Using boosted features for the detection of people in 2D range data},
  author={Arras, Kai O and Mozos, Oscar Martinez and Burgard, Wolfram},
  booktitle={Proceedings 2007 IEEE International Conference on Robotics and Automation},
  pages={3402--3407},
  year={2007},
  organization={IEEE}
}
@inproceedings{jia2020dr,
  title={DR-SPAAM: A spatial-attention and auto-regressive model for person detection in 2D range data},
  author={Jia, Dan and Hermans, Alexander and Leibe, Bastian},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={10270--10277},
  year={2020},
  organization={IEEE}
}
@inproceedings{jafari2014real,
  title={Real-time RGB-D based people detection and tracking for mobile robots and head-worn cameras},
  author={Jafari, Omid Hosseini and Mitzel, Dennis and Leibe, Bastian},
  booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5636--5643},
  year={2014},
  organization={IEEE}
}
@article{lau2010multi,
  title={Multi-model hypothesis group tracking and group size estimation},
  author={Lau, Boris and Arras, Kai O and Burgard, Wolfram},
  journal={International Journal of Social Robotics},
  volume={2},
  number={1},
  pages={19--30},
  year={2010},
  publisher={Springer}
}
@article{munaro2014fast,
  title={Fast RGB-D people tracking for service robots},
  author={Munaro, Matteo and Menegatti, Emanuele},
  journal={Autonomous Robots},
  volume={37},
  number={3},
  pages={227--242},
  year={2014},
  publisher={Springer}
}
@inproceedings{ronneberger2015unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical Image Computing and Computer-assisted Intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}
@inproceedings{he2017mask,
  title={Mask {R-CNN}},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2961--2969},
  year={2017}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European Conference on Computer Vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}
@inproceedings{bertasius2021space,
  title={Is Space-Time Attention All You Need for Video Understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={International Conference on Machine Learning},
  pages={813--824},
  year={2021},
  organization={PMLR}
}
@inproceedings{arnab2021vivit,
  title={ViVit: A video vision transformer},
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lu{\v{c}}i{\'c}, Mario and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6836--6846},
  year={2021}
}
@article{patrick2021keeping,
  title={Keeping your eye on the ball: Trajectory attention in video transformers},
  author={Patrick, Mandela and Campbell, Dylan and Asano, Yuki and Misra, Ishan and Metze, Florian and Feichtenhofer, Christoph and Vedaldi, Andrea and Henriques, Jo{\~a}o F},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{zhang2021upt,
  author = {Frederic Z. Zhang, Dylan Campbell and Stephen Gould},
  title = {Efficient Two-Stage Detection of Human–Object Interactions with a Novel Unary–Pairwise Transformer},
  booktitle = {arXiv preprint arXiv:2112.01838},
  year = {2021}
}
@article{jaegle2021perceiver,
  title={Perceiver io: A general architecture for structured inputs \& outputs},
  author={Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others},
  journal={arXiv preprint arXiv:2107.14795},
  year={2021}
}

@article{doi:10.1177/0018720811417254,
author = {Peter A. Hancock and Deborah R. Billings and Kristin E. Schaefer and Jessie Y. C. Chen and Ewart J. de Visser and Raja Parasuraman},
title ={A Meta-Analysis of Factors Affecting Trust in Human-Robot Interaction},
journal = {Human Factors},
volume = {53},
number = {5},
pages = {517-527},
year = {2011},
doi = {10.1177/0018720811417254},
    note ={PMID: 22046724},

URL = { 
        https://doi.org/10.1177/0018720811417254
    
},
eprint = { 
        https://doi.org/10.1177/0018720811417254
    
}
,
    abstract = { Objective: We evaluate and quantify the effects of human, robot, and environmental factors on perceived trust in human-robot interaction (HRI).Background: To date, reviews of trust in HRI have been qualitative or descriptive. Our quantitative review provides a fundamental empirical foundation to advance both theory and practice.Method: Meta-analytic methods were applied to the available literature on trust and HRI. A total of 29 empirical studies were collected, of which 10 met the selection criteria for correlational analysis and 11 for experimental analysis. These studies provided 69 correlational and 47 experimental effect sizes.Results: The overall correlational effect size for trust was r̄ = +0.26, with an experimental effect size of d̄ = +0.71. The effects of human, robot, and environmental characteristics were examined with an especial evaluation of the robot dimensions of performance and attribute-based factors. The robot performance and attributes were the largest contributors to the development of trust in HRI. Environmental factors played only a moderate role.Conclusion: Factors related to the robot itself, specifically, its performance, had the greatest current association with trust, and environmental factors were moderately associated. There was little evidence for effects of human-related factors.Application: The findings provide quantitative estimates of human, robot, and environmental factors influencing HRI trust. Specifically, the current summary provides effect size estimates that are useful in establishing design and training guidelines with reference to robot-related factors of HRI trust. Furthermore, results indicate that improper trust calibration may be mitigated by the manipulation of robot design. However, many future research needs are identified. }
}




@inproceedings{pena2017benchmarking,
  title={Benchmarking of {CNNs} for low-cost, low-power robotics applications},
  author={Pena, Dexmont and Forembski, Andrew and Xu, Xiaofan and Moloney, David},
  booktitle={RSS 2017 Workshop: New Frontier for Deep Learning in Robotics},
  pages={1--5},
  year={2017}
}

@article{susperregi2013rgb,
  title={{RGB-D}, laser and thermal sensor fusion for people following in a mobile robot},
  author={Susperregi, Loreto and Mart{\'\i}nez-Otzeta, Jose Maria and Ansuategui, Ander and Ibarguren, Aitor and Sierra, Basilio},
  journal={International Journal of Advanced Robotic Systems},
  volume={10},
  number={6},
  pages={271},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{macdonald2019active,
  title={Active sensing for motion planning in uncertain environments via mutual information policies},
  author={MacDonald, Ryan A and Smith, Stephen L},
  journal={The International Journal of Robotics Research},
  volume={38},
  number={2-3},
  pages={146--161},
  year={2019},
  publisher={Sage Publications Sage UK: London, England}
}

@inproceedings{li2020transferring,
  title={Transferring cross-domain knowledge for video sign language recognition},
  author={Li, Dongxu and Yu, Xin and Xu, Chenchen and Petersson, Lars and Li, Hongdong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6205--6214},
  year={2020}
}

@inproceedings{albanie2020bsl,
  title={BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues},
  author={Albanie, Samuel and Varol, G{\"u}l and Momeni, Liliane and Afouras, Triantafyllos and Chung, Joon Son and Fox, Neil and Zisserman, Andrew},
  booktitle={European Conference on Computer Vision},
  pages={35--53},
  year={2020},
  organization={Springer}
}

@InProceedings{pfister2013largescale,
  author       = "Tomas Pfister and James Charles and Andrew Zisserman",
  title        = "Large-scale Learning of Sign Language by Watching {TV} (Using Co-occurrences)",
  booktitle    = "British Machine Vision Conference",
  year         = "2013",
}

@misc{kalashnikov2018qtopt,
      title={QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation}, 
      author={Dmitry Kalashnikov and Alex Irpan and Peter Pastor and Julian Ibarz and Alexander Herzog and Eric Jang and Deirdre Quillen and Ethan Holly and Mrinal Kalakrishnan and Vincent Vanhoucke and Sergey Levine},
      year={2018},
      eprint={1806.10293},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{chen2011kalman,
  title={Kalman filter for robot vision: a survey},
  author={Chen, SY},
  journal={IEEE Transactions on industrial electronics},
  volume={59},
  number={11},
  pages={4409--4420},
  year={2011},
  publisher={IEEE}
}

@article{bonin2008visual,
  title={Visual navigation for mobile robots: A survey},
  author={Bonin-Font, Francisco and Ortiz, Alberto and Oliver, Gabriel},
  journal={Journal of intelligent and robotic systems},
  volume={53},
  number={3},
  pages={263--296},
  year={2008},
  publisher={Springer}
}

@article{doi:10.1080/01691864.2017.1365009,
author = {Harry A. Pierson and Michael S. Gashler},
title = {Deep learning in robotics: a review of recent research},
journal = {Advanced Robotics},
volume = {31},
number = {16},
pages = {821-835},
year  = {2017},
publisher = {Taylor & Francis},
doi = {10.1080/01691864.2017.1365009},

URL = { 
        https://doi.org/10.1080/01691864.2017.1365009
    
},
eprint = { 
        https://doi.org/10.1080/01691864.2017.1365009
    
}

}

@book{corke2011robotics,
  title={Robotics, vision and control: fundamental algorithms in MATLAB},
  author={Corke, Peter I},
  volume={73},
  year={2011},
  publisher={Springer}
}

@article{sunderhauf2018limits,
  title={The limits and potentials of deep learning for robotics},
  author={S{\"u}nderhauf, Niko and Brock, Oliver and Scheirer, Walter and Hadsell, Raia and Fox, Dieter and Leitner, J{\"u}rgen and Upcroft, Ben and Abbeel, Pieter and Burgard, Wolfram and Milford, Michael and others},
  journal={The International journal of robotics research},
  volume={37},
  number={4-5},
  pages={405--420},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}


@article{chen2011active,
  title={Active vision in robotic systems: A survey of recent developments},
  author={Chen, Shengyong and Li, Youfu and Kwok, Ngai Ming},
  journal={The International Journal of Robotics Research},
  volume={30},
  number={11},
  pages={1343--1377},
  year={2011},
  publisher={SAGE Publications Sage UK: London, England}
}

@INPROCEEDINGS{5937403,  author={Regusa, R and Lorenzo, N and Metta, G and Sandini, G},  booktitle={2011 4th International Conference on Human System Interactions, HSI 2011},   title={Cognitive robotics - active perception of the self and others},   year={2011},  volume={},  number={},  pages={419-426},  doi={10.1109/HSI.2011.5937403}}

@article{han2013enhanced,
  title={Enhanced computer vision with microsoft kinect sensor: A review},
  author={Han, Jungong and Shao, Ling and Xu, Dong and Shotton, Jamie},
  journal={IEEE transactions on cybernetics},
  volume={43},
  number={5},
  pages={1318--1334},
  year={2013},
  publisher={IEEE}
}

@book{ForsythDavid2012Cv:a,
publisher = {Pearson},
isbn = {9780136085928},
year = {2012},
title = {Computer vision : a modern approach},
edition = {2nd ed.},
language = {eng},
address = {Boston},
author = {Forsyth, David},
keywords = {Computer vision; Computer vision -- Problems, exercises, etc},
lccn = {2011036341},
}


@article{doi:10.1177/0278364917710318,
author = {Sergey Levine and Peter Pastor and Alex Krizhevsky and Julian Ibarz and Deirdre Quillen},
title ={Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection},
journal = {The International Journal of Robotics Research},
volume = {37},
number = {4-5},
pages = {421-436},
year = {2018},
doi = {10.1177/0278364917710318},

URL = { 
        https://doi.org/10.1177/0278364917710318
    
},
eprint = { 
        https://doi.org/10.1177/0278364917710318
    
}
,
    abstract = { We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images. To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images independent of camera calibration or the current robot pose. This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination. We then use this network to servo the gripper in real time to achieve successful grasps. We describe two large-scale experiments that we conducted on two separate robotic platforms. In the first experiment, about 800,000 grasp attempts were collected over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and gripper wear and tear. In the second experiment, we used a different robotic platform and 8 robots to collect a dataset consisting of over 900,000 grasp attempts. The second robotic platform was used to test transfer between robots, and the degree to which data from a different set of robots can be used to aid learning. Our experimental results demonstrate that our approach achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing. Our transfer experiment also illustrates that data from different robots can be combined to learn more reliable and effective grasping. }
}


@article{doi:10.1177/1529100619832930,
author = {Lisa Feldman Barrett and Ralph Adolphs and Stacy Marsella and Aleix M. Martinez and Seth D. Pollak},
title ={Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements},
journal = {Psychological Science in the Public Interest},
volume = {20},
number = {1},
pages = {1-68},
year = {2019},
doi = {10.1177/1529100619832930},
    note ={PMID: 31313636},

URL = { 
        https://doi.org/10.1177/1529100619832930
    
},
eprint = { 
        https://doi.org/10.1177/1529100619832930
    
}
,
    abstract = { It is commonly assumed that a person’s emotional state can be readily inferred from his or her facial movements, typically called emotional expressions or facial expressions. This assumption influences legal judgments, policy decisions, national security protocols, and educational practices; guides the diagnosis and treatment of psychiatric illness, as well as the development of commercial applications; and pervades everyday social interactions as well as research in other scientific fields such as artificial intelligence, neuroscience, and computer vision. In this article, we survey examples of this widespread assumption, which we refer to as the common view, and we then examine the scientific evidence that tests this view, focusing on the six most popular emotion categories used by consumers of emotion research: anger, disgust, fear, happiness, sadness, and surprise. The available scientific evidence suggests that people do sometimes smile when happy, frown when sad, scowl when angry, and so on, as proposed by the common view, more than what would be expected by chance. Yet how people communicate anger, disgust, fear, happiness, sadness, and surprise varies substantially across cultures, situations, and even across people within a single situation. Furthermore, similar configurations of facial movements variably express instances of more than one emotion category. In fact, a given configuration of facial movements, such as a scowl, often communicates something other than an emotional state. Scientists agree that facial movements convey a range of information and are important for social communication, emotional or otherwise. But our review suggests an urgent need for research that examines how people actually move their faces to express emotions and other social information in the variety of contexts that make up everyday life, as well as careful study of the mechanisms by which people perceive instances of emotion in one another. We make specific research recommendations that will yield a more valid picture of how people move their faces to express emotions and how they infer emotional meaning from facial movements in situations of everyday life. This research is crucial to provide consumers of emotion research with the translational information they require. }
}

@article{phillips1998feret,
  title={The FERET database and evaluation procedure for face-recognition algorithms},
  author={Phillips, P Jonathon and Wechsler, Harry and Huang, Jeffery and Rauss, Patrick J},
  journal={Image and vision computing},
  volume={16},
  number={5},
  pages={295--306},
  year={1998},
  publisher={Elsevier}
}

@inproceedings{castellano2010inter,
  title={Inter-ACT: An affective and contextually rich multimodal video corpus for studying interaction with robots},
  author={Castellano, Ginevra and Leite, Iolanda and Pereira, Andr{\'e} and Martinho, Carlos and Paiva, Ana and McOwan, Peter W},
  booktitle={Proceedings of the 18th ACM international conference on Multimedia},
  pages={1031--1034},
  year={2010}
}

@inproceedings{pantic2005web,
  title={Web-based database for facial expression analysis},
  author={Pantic, Maja and Valstar, Michel and Rademaker, Ron and Maat, Ludo},
  booktitle={2005 IEEE international conference on multimedia and Expo},
  pages={5--pp},
  year={2005},
  organization={IEEE}
}

@inproceedings{kanade2000comprehensive,
  title={Comprehensive database for facial expression analysis},
  author={Kanade, Takeo and Cohn, Jeffrey F and Tian, Yingli},
  booktitle={Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)},
  pages={46--53},
  year={2000},
  organization={IEEE}
}

@misc{noauthor_aberdeen_nodate,
	title = {Aberdeen {Facial} {Database}},
	url = {http://pics.psych.stir.ac.uk/zips/Aberdeen.zip}
}

@article{beer2014toward,
  title={Toward a framework for levels of robot autonomy in human-robot interaction},
  author={Beer, Jenay M and Fisk, Arthur D and Rogers, Wendy A},
  journal={Journal of human-robot interaction},
  volume={3},
  number={2},
  pages={74},
  year={2014},
  publisher={NIH Public Access}
}

@Inbook{Robinette2017,
author="Robinette, Paul
and Howard, Ayanna
and Wagner, Alan R.",
title="Conceptualizing Overtrust in Robots: Why Do People Trust a Robot That Previously Failed?",
bookTitle="Autonomy and Artificial Intelligence: A Threat or Savior?",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="129--155",
abstract="In this chapter, we present work that suggests people tend to be overly trusting and overly forgiving of robots in certain situations. In keeping with the theme of this book where intelligent systems help humans recover from errors, our work so far has focused on robots as guides in emergency situations. Our experiments show that, at best, human participants in our simulated emergencies focus on guidance provided by robots, regardless of a robot's prior performance or other guidance information, and, at worst, believe that the robot is more capable than other sources of information. Even when the robots do break trust, a properly timed statement can convince a participant to follow it. Based on this evidence, we have conceptualized overtrust of robots using our previous framework of situational trust. We define two mechanisms in which people can overtrust robots: misjudging the abilities or intentions of the robot and misjudging the risk in the scenario. We discuss our prior work in light of this new reconceptualization to attempt to explain our previous results and encourage future work.",
isbn="978-3-319-59719-5",
doi="10.1007/978-3-319-59719-5_6",
url="https://doi.org/10.1007/978-3-319-59719-5_6"
}



@article{10.1145/3457607,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in
our everyday lives, accounting for fairness has gained significant importance in designing
and engineering of such systems. AI systems can be used in many sensitive environments
to make important and life-changing decisions; thus, it is crucial to ensure that
these decisions do not reflect discriminatory behavior toward certain groups or populations.
More recently some work has been developed in traditional machine learning and deep
learning that address such challenges in different subdomains. With the commercialization
of these systems, researchers are becoming more aware of the biases that these applications
can contain and are attempting to address them. In this survey, we investigated different
real-world applications that have shown biases in various ways, and we listed different
sources of biases that can affect AI applications. We then created a taxonomy for
fairness definitions that machine learning researchers have defined to avoid the existing
bias in AI systems. In addition to that, we examined different domains and subdomains
in AI showing what researchers have observed with regard to unfair outcomes in the
state-of-the-art methods and ways they have tried to address them. There are still
many future directions and solutions that can be taken to mitigate the problem of
bias in AI systems. We are hoping that this survey will motivate researchers to tackle
these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {115},
numpages = {35},
keywords = {Fairness and bias in artificial intelligence, natural language processing, deep learning, representation learning, machine learning}
}

@INPROCEEDINGS{4415182,  author={Bethel, Cindy L. and Salomon, Kristen and Murphy, Robin R. and Burke, Jennifer L.},  booktitle={RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication},   title={Survey of Psychophysiology Measurements Applied to Human-Robot Interaction},   year={2007},  volume={},  number={},  pages={732-737},  doi={10.1109/ROMAN.2007.4415182}}

@article{halme2018review,
  title={Review of vision-based safety systems for human-robot collaboration},
  author={Halme, Roni-Jussi and Lanz, Minna and K{\"a}m{\"a}r{\"a}inen, Joni and Pieters, Roel and Latokartano, Jyrki and Hietanen, Antti},
  journal={Procedia CIRP},
  volume={72},
  pages={111--116},
  year={2018},
  publisher={Elsevier}
}

@article{hentout2019human,
  title={Human--robot interaction in industrial collaborative robotics: a literature review of the decade 2008--2017},
  author={Hentout, Abdelfetah and Aouache, Mustapha and Maoudj, Abderraouf and Akli, Isma},
  journal={Advanced Robotics},
  volume={33},
  number={15-16},
  pages={764--799},
  year={2019},
  publisher={Taylor \& Francis}
}

@article{ortenzi2021object,
  title={Object handovers: a review for robotics},
  author={Ortenzi, Valerio and Cosgun, Akansel and Pardi, Tommaso and Chan, Wesley P and Croft, Elizabeth and Kuli{\'c}, Dana},
  journal={IEEE Transactions on Robotics},
  year={2021},
  publisher={IEEE}
}

@article{beddiar2020vision,
  title={Vision-based human activity recognition: a survey},
  author={Beddiar, Djamila Romaissa and Nini, Brahim and Sabokrou, Mohammad and Hadid, Abdenour},
  journal={Multimedia Tools and Applications},
  volume={79},
  number={41},
  pages={30509--30555},
  year={2020},
  publisher={Springer}
}

@INPROCEEDINGS{7041588,  author={Roitberg, Alina and Perzylo, Alexander and Somani, Nikhil and Giuliani, Manuel and Rickert, Markus and Knoll, Alois},  booktitle={Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific},   title={Human activity recognition in the context of industrial human-robot interaction},   year={2014},  volume={},  number={},  pages={1-10},  doi={10.1109/APSIPA.2014.7041588}}

@article{doi:10.1177/0278364919881683,
author = {Md Jahidul Islam and Jungseok Hong and Junaed Sattar},
title ={Person-following by autonomous robots: A categorical overview},
journal = {The International Journal of Robotics Research},
volume = {38},
number = {14},
pages = {1581-1618},
year = {2019},
doi = {10.1177/0278364919881683},

URL = { 
        https://doi.org/10.1177/0278364919881683
    
},
eprint = { 
        https://doi.org/10.1177/0278364919881683
    
}
,
    abstract = { A wide range of human–robot collaborative applications in diverse domains, such as manufacturing, health care, the entertainment industry, and social interactions, require an autonomous robot to follow its human companion. Different working environments and applications pose diverse challenges by adding constraints on the choice of sensors, degree of autonomy, and dynamics of a person-following robot. Researchers have addressed these challenges in many ways and contributed to the development of a large body of literature. This paper provides a comprehensive overview of the literature by categorizing different aspects of person-following by autonomous robots. Also, the corresponding operational challenges are identified based on various design choices for ground, underwater, and aerial scenarios. In addition, state-of-the-art methods for perception, planning, control, and interaction are elaborately discussed and their applicability in varied operational scenarios is presented. Then some of the prominent methods are qualitatively compared, corresponding practicalities are illustrated, and their feasibility is analyzed for various use cases. Furthermore, several prospective application areas are identified, and open problems are highlighted for future research. }
}




@article{rios2015proxemics,
  title={From proxemics theory to socially-aware navigation: A survey},
  author={Rios-Martinez, Jorge and Spalanzani, Anne and Laugier, Christian},
  journal={International Journal of Social Robotics},
  volume={7},
  number={2},
  pages={137--153},
  year={2015},
  publisher={Springer}
}

@article{leichtmann2020much,
  title={How much distance do humans keep toward robots? Literature review, meta-analysis, and theoretical considerations on personal space in human-robot interaction},
  author={Leichtmann, Benedikt and Nitsch, Verena},
  journal={Journal of Environmental Psychology},
  volume={68},
  pages={101386},
  year={2020},
  publisher={Elsevier}
}


@article{VILLANI2018248,
title = {Survey on human–robot collaboration in industrial settings: Safety, intuitive interfaces and applications},
journal = {Mechatronics},
volume = {55},
pages = {248-266},
year = {2018},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2018.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0957415818300321},
author = {Valeria Villani and Fabio Pini and Francesco Leali and Cristian Secchi},
keywords = {Human–robot collaboration, Collaborative robots, Safety, User interfaces, Intuitive robot programming, Industrial applications},
abstract = {Easy-to-use collaborative robotics solutions, where human workers and robots share their skills, are entering the market, thus becoming the new frontier in industrial robotics. They allow to combine the advantages of robots, which enjoy high levels of accuracy, speed and repeatability, with the flexibility and cognitive skills of human workers. However, to achieve an efficient human–robot collaboration, several challenges need to be tackled. First, a safe interaction must be guaranteed to prevent harming humans having a direct contact with the moving robot. Additionally, to take full advantage of human skills, it is important that intuitive user interfaces are properly designed, so that human operators can easily program and interact with the robot. In this survey paper, an extensive review on human–robot collaboration in industrial environment is provided, with specific focus on issues related to physical and cognitive interaction. The commercially available solutions are also presented and the main industrial applications where collaborative robotic is advantageous are discussed, highlighting how collaborative solutions are intended to improve the efficiency of the system and which the open issue are.}
}

@article{jaimes2007multimodal,
  title={Multimodal human--computer interaction: A survey},
  author={Jaimes, Alejandro and Sebe, Nicu},
  journal={Computer vision and image understanding},
  volume={108},
  number={1-2},
  pages={116--134},
  year={2007},
  publisher={Elsevier}
}

@article{moeslund2001survey,
  title={A survey of computer vision-based human motion capture},
  author={Moeslund, Thomas B and Granum, Erik},
  journal={Computer vision and image understanding},
  volume={81},
  number={3},
  pages={231--268},
  year={2001},
  publisher={Elsevier}
}

@article{liu2018gesture,
  title={Gesture recognition for human-robot collaboration: A review},
  author={Liu, Hongyi and Wang, Lihui},
  journal={International Journal of Industrial Ergonomics},
  volume={68},
  pages={355--367},
  year={2018},
  publisher={Elsevier}
}

@article{rautaray2015vision,
  title={Vision based hand gesture recognition for human computer interaction: a survey},
  author={Rautaray, Siddharth S and Agrawal, Anupam},
  journal={Artificial intelligence review},
  volume={43},
  number={1},
  pages={1--54},
  year={2015},
  publisher={Springer}
}

@article{zhang2019comprehensive,
  title={A comprehensive survey of vision-based human action recognition methods},
  author={Zhang, Hong-Bo and Zhang, Yi-Xiang and Zhong, Bineng and Lei, Qing and Yang, Lijie and Du, Ji-Xiang and Chen, Duan-Sheng},
  journal={Sensors},
  volume={19},
  number={5},
  pages={1005},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{suma2019computer,
  title={Computer vision for human-machine interaction-review},
  author={Suma, V},
  journal={Journal of trends in Computer Science and Smart technology (TCSST)},
  volume={1},
  number={02},
  pages={131--139},
  year={2019}
}

@inproceedings{khavas2020modeling,
  title={Modeling Trust in Human-Robot Interaction: A Survey},
  author={Khavas, Zahra Rezaei and Ahmadzadeh, S Reza and Robinette, Paul},
  booktitle={International Conference on Social Robotics},
  pages={529--541},
  year={2020},
  organization={Springer}
}

@article{argall2010survey,
  title={A survey of tactile human--robot interactions},
  author={Argall, Brenna D and Billard, Aude G},
  journal={Robotics and autonomous systems},
  volume={58},
  number={10},
  pages={1159--1176},
  year={2010},
  publisher={Elsevier}
}

@article{mi2013human,
  title={Human-robot interaction in UVs swarming: a survey},
  author={Mi, Zhen-Qiang and Yang, Yang},
  journal={International Journal of Computer Science Issues (IJCSI)},
  volume={10},
  number={2 Part 1},
  pages={273},
  year={2013},
  publisher={Citeseer}
}

@article{tezza2019state,
  title={The state-of-the-art of human--drone interaction: A survey},
  author={Tezza, Dante and Andujar, Marvin},
  journal={IEEE Access},
  volume={7},
  pages={167438--167454},
  year={2019},
  publisher={IEEE}
}

@article{saunderson2019robots,
  title={How robots influence humans: A survey of nonverbal communication in social human--robot interaction},
  author={Saunderson, Shane and Nejat, Goldie},
  journal={International Journal of Social Robotics},
  volume={11},
  number={4},
  pages={575--608},
  year={2019},
  publisher={Springer}
}

@article{yan2014survey,
  title={A survey on perception methods for human--robot interaction in social robots},
  author={Yan, Haibin and Ang, Marcelo H and Poo, Aun Neow},
  journal={International Journal of Social Robotics},
  volume={6},
  number={1},
  pages={85--119},
  year={2014},
  publisher={Springer}
}

@article{ZACHARAKI2020104667,
title = {Safety bounds in human robot interaction: A survey},
journal = {Safety Science},
volume = {127},
pages = {104667},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104667},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520300643},
author = {Angeliki Zacharaki and Ioannis Kostavelis and Antonios Gasteratos and Ioannis Dokas},
keywords = {Robot safety, Human-robot interaction, Safe collaboration, Safety standards, Safety techniques, Psychological safety issues},
abstract = {In the era of industrialization and automation, safety is a critical factor that should be considered during the design and realization of each new system that targets operation in close collaboration with humans. Of such systems are considered personal and professional service robots which collaborate and interact with humans at diverse applications environments. In this collaboration, human safety is an important factor in the wider field of human-robot interaction (HRI) since it facilitates their harmonic coexistence. The paper at hand aims to systemize the recent literature by describing the required levels of safety during human-robot interaction, focusing on the core functions of the collaborative robots when performing specific processes. It is also oriented towards the existing methods for psychological safety during human-robot collaboration and its impact at the robot behaviour, while also discusses in depth the psychological parameters of robots incorporation in industrial and social environments. Based on the existing works on safety features that minimize the risk of HRI, a classification of the existing works into five major categories namely, Robot Perceptions for Safe HRI, Cognition-enabled robot control in HRI, Action Planning for safe navigation close to humans, Hardware safety features, and Societal and Psychological factors is also applied. Finally, the current study further discusses the existing risk assessment techniques as methods to offer additional safety in robotic systems presenting thus a holistic analysis of the safety in contemporary robots, and proposes a roadmap for safety compliance features during the development of a robotic system.}
}

@article{socialerrorsHRI,
author = {Tian, Leimin and Oviatt, Sharon},
title = {A Taxonomy of Social Errors in Human-Robot Interaction},
year = {2021},
issue_date = {March 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
url = {https://doi.org/10.1145/3439720},
doi = {10.1145/3439720},
abstract = {Robotic applications have entered various aspects of our lives, such as health care and educational services. In such Human-robot Interaction (HRI), trust and mutual adaption are established and maintained through a positive social relationship between a user and a robot. This social relationship relies on the perceived competence of a robot on the social-emotional dimension. However, because of technical limitations and user heterogeneity, current HRI is far from error-free, especially when a system leaves controlled lab environments and is applied to in-the-wild conditions. Errors in HRI may either degrade a user’s perception of a robot’s capability in achieving a task (defined as performance errors in this work) or degrade a user’s perception of a robot’s socio-affective competence (defined as social errors in this work). The impact of these errors and effective strategies to handle such an impact remains an open question. We focus on social errors in HRI in this work. In particular, we identify the major attributes of perceived socio-affective competence by reviewing human social interaction studies and HRI error studies. This motivates us to propose a taxonomy of social errors in HRI. We then discuss the impact of social errors situated in three representative HRI scenarios. This article provides foundations for a systematic analysis of the social-emotional dimension of HRI. The proposed taxonomy of social errors encourages the development of user-centered HRI systems, designed to offer positive and adaptive interaction experiences and improved interaction outcomes.},
journal = {J. Hum.-Robot Interact.},
month = feb,
articleno = {13},
numpages = {32},
keywords = {human-robot interaction, socio-affective competence, Affective computing, social robotics, social norms}
}

@article{mavrogiannis2021core,
  title={Core Challenges of Social Robot Navigation: A Survey},
  author={Mavrogiannis, Christoforos and Baldini, Francesca and Wang, Allan and Zhao, Dapeng and Steinfeld, Aaron and Trautman, Pete and Oh, Jean},
  journal={arXiv preprint arXiv:2103.05668},
  year={2021}
}

@inproceedings{yanco2004classifying,
  title={Classifying human-robot interaction: an updated taxonomy},
  author={Yanco, Holly A and Drury, Jill},
  booktitle={IEEE International Conference on Systems, Man and Cybernetics},
  volume={3},
  pages={2841--2846},
  year={2004},
  organization={IEEE}
}

@misc{noauthor_utrecht_nodate,
	title = {Utrecht {ECVP} {Facial} {Database}},
	url = {http://pics.psych.stir.ac.uk/zips/utrecht.zip}
}

@misc{noauthor_gufd_nodate,
	title = {{GUFD} {Facial} {Database}},
	url = {http://www.facevar.com/glasgow-unfamiliar-face-database}
}

@inproceedings{yang2016wider,
	Author = {Yang, Shuo and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
	Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	Title = {WIDER FACE: A Face Detection Benchmark},
	Year = {2016}
}

@inproceedings{bambach_lending_2015,
	address = {Santiago, Chile},
	title = {Lending {A} {Hand}: {Detecting} {Hands} and {Recognizing} {Activities} in {Complex} {Egocentric} {Interactions}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Lending {A} {Hand}},
	url = {http://ieeexplore.ieee.org/document/7410583/},
	doi = {10.1109/ICCV.2015.226},
	abstract = {Hands appear very often in egocentric video, and their appearance and pose give important cues about what people are doing and what they are paying attention to. But existing work in hand detection has made strong assumptions that work well in only simple scenarios, such as with limited interaction with other people or in lab settings. We develop methods to locate and distinguish between hands in egocentric video using strong appearance models with Convolutional Neural Networks, and introduce a simple candidate region generation approach that outperforms existing techniques at a fraction of the computational cost. We show how these high-quality bounding boxes can be used to create accurate pixelwise hand regions, and as an application, we investigate the extent to which hand segmentation alone can distinguish between different activities. We evaluate these techniques on a new dataset of 48 ﬁrst-person videos of people interacting in realistic environments, with pixel-level ground truth for over 15,000 hand instances.},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bambach, Sven and Lee, Stefan and Crandall, David J. and Yu, Chen},
	month = dec,
	year = {2015},
	note = {258},
	pages = {1949--1957},
	file = {Bambach et al. - 2015 - Lending A Hand Detecting Hands and Recognizing Ac.pdf:/home/brendan/Zotero/storage/HH9AXJ4I/Bambach et al. - 2015 - Lending A Hand Detecting Hands and Recognizing Ac.pdf:application/pdf}
}

@book{muller2007participatory,
  title={Participatory design: the third space in HCI},
  author={Muller, Michael J},
  year={2007},
  publisher={CRC press}
}

@inproceedings{pittaluga2019revealing,
  title={Revealing scenes by inverting structure from motion reconstructions},
  author={Pittaluga, Francesco and Koppal, Sanjeev J and Kang, Sing Bing and Sinha, Sudipta N},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={145--154},
  year={2019}
}

@InProceedings{Ren_2018_ECCV,
author = {Ren, Zhongzheng and Lee, Yong Jae and Ryoo, Michael S.},
title = {Learning to Anonymize Faces for Privacy Preserving Action Detection},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
month = {September},
year = {2018}
}

@ARTICLE{1200160,  author={Casper, J. and Murphy, R.R.},  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},   title={Human-robot interactions during the robot-assisted urban search and rescue response at the World Trade Center},   year={2003},  volume={33},  number={3},  pages={367-385},  doi={10.1109/TSMCB.2003.811794}}

@ARTICLE{9743954,  author={Yang, Dongfang and Zhang, Haolin and Yurtsever, Ekim and Redmill, Keith A. and Özgüner, \"{U}mit},  journal={IEEE Transactions on Intelligent Vehicles},   title={Predicting Pedestrian Crossing Intention With Feature Fusion and Spatio-Temporal Attention},   year={2022},  volume={7},  number={2},  pages={221-230},  doi={10.1109/TIV.2022.3162719}}

@ARTICLE{8876650,  author={Fang, Zhijie and López, Antonio M.},  journal={IEEE Transactions on Intelligent Transportation Systems},   title={Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation},   year={2020},  volume={21},  number={11},  pages={4773-4783},  doi={10.1109/TITS.2019.2946642}}

@article{liu2016multirobot,
  title={Multirobot cooperative learning for semiautonomous control in urban search and rescue applications},
  author={Liu, Yugang and Nejat, Goldie},
  journal={Journal of Field Robotics},
  volume={33},
  number={4},
  pages={512--536},
  year={2016},
  publisher={Wiley Online Library}
}

@INPROCEEDINGS{8594452,  author={Gui, Liang-Yan and Zhang, Kevin and Wang, Yu-Xiong and Liang, Xiaodan and Moura, José M. F. and Veloso, Manuela},  booktitle={2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},   title={Teaching Robots to Predict Human Motion},   year={2018},  volume={},  number={},  pages={562-567},  doi={10.1109/IROS.2018.8594452}}

@article{ognibene2013contextual,
  title={Contextual action recognition and target localization with an active allocation of attention on a humanoid robot},
  author={Ognibene, Dimitri and Chinellato, Eris and Sarabia, Miguel and Demiris, Yiannis},
  journal={Bioinspiration \& biomimetics},
  volume={8},
  number={3},
  pages={035002},
  year={2013},
  publisher={IOP Publishing}
}

@INPROCEEDINGS{7451737,
  author={Huang, Chien-Ming and Mutlu, Bilge},
  booktitle={2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Anticipatory robot control for efficient human-robot collaboration}, 
  year={2016},
  volume={},
  number={},
  pages={83-90},
  doi={10.1109/HRI.2016.7451737}}

@MISC{EUdataregulations2018,
    title = {2018 Reform of EU Data Protection Rules},
    url = {https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf},
    organization = {European Commission},
    date = {2018-05-25},
    urldate = {2019-06-17}
}

@inproceedings{ognibene2013towards,
  title={Towards Active Event Recognition.},
  author={Ognibene, Dimitri and Demiris, Yiannis},
  booktitle={IJCAI},
  pages={2495--2501},
  year={2013}
}

@InProceedings{10.1007/978-3-030-58542-6_7,
author="Shibuya, Mikiya
and Sumikura, Shinya
and Sakurada, Ken",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Privacy Preserving Visual SLAM",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="102--118",
abstract="This study proposes a privacy-preserving Visual SLAM framework for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Previous studies have proposed localization methods to estimate a camera pose using a line-cloud map for a single image or a reconstructed point cloud. These methods offer a scene privacy protection against the inversion attacks by converting a point cloud to a line cloud, which reconstruct the scene images from the point cloud. However, they are not directly applicable to a video sequence because they do not address computational efficiency. This is a critical issue to solve for estimating camera poses and performing bundle adjustment with mixed line and point clouds in real time. Moreover, there has been no study on a method to optimize a line-cloud map of a server with a point cloud reconstructed from a client video because any observation points on the image coordinates are not available to prevent the inversion attacks, namely the reversibility of the 3D lines. The experimental results with synthetic and real data show that our Visual SLAM framework achieves the intended privacy-preserving formation and real-time performance using a line-cloud map.",
isbn="978-3-030-58542-6"
}



@article{10.1007/s11042-020-09004-3,
author = {Beddiar, Djamila Romaissa and Nini, Brahim and Sabokrou, Mohammad and Hadid, Abdenour},
title = {Vision-Based Human Activity Recognition: A Survey},
year = {2020},
issue_date = {Nov 2020},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {41–42},
issn = {1380-7501},
url = {https://doi.org/10.1007/s11042-020-09004-3},
doi = {10.1007/s11042-020-09004-3},
abstract = {Human activity recognition (HAR) systems attempt to automatically identify and analyze human activities using acquired information from various types of sensors. Although several extensive review papers have already been published in the general HAR topics, the growing technologies in the field as well as the multi-disciplinary nature of HAR prompt the need for constant updates in the field. In this respect, this paper attempts to review and summarize the progress of HAR systems from the computer vision perspective. Indeed, most computer vision applications such as human computer interaction, virtual reality, security, video surveillance and home monitoring are highly correlated to HAR tasks. This establishes new trend and milestone in the development cycle of HAR systems. Therefore, the current survey aims to provide the reader with an up to date analysis of vision-based HAR related literature and recent progress in the field. At the same time, it will highlight the main challenges and future directions.},
journal = {Multimedia Tools Appl.},
month = {nov},
pages = {30509–30555},
numpages = {47},
keywords = {Computer Vision, Human activity recognition, Action detection, Action representation, Behavior understanding, Survey}
}

@article{mollahosseini_affectnet_2019,
	title = {{AffectNet}: {A} {Database} for {Facial} {Expression}, {Valence}, and {Arousal} {Computing} in the {Wild}},
	volume = {10},
	issn = {1949-3045, 2371-9850},
	shorttitle = {{AffectNet}},
	url = {http://arxiv.org/abs/1708.03985},
	doi = {10.1109/TAFFC.2017.2740923},
	abstract = {Automated affective computing in the wild setting is a challenging problem in computer vision. Existing annotated databases of facial expressions in the wild are small and mostly cover discrete emotions (aka the categorical model). There are very limited annotated facial databases for affective computing in the continuous dimensional model (e.g., valence and arousal). To meet this need, we collected, annotated, and prepared for public distribution a new database of facial emotions in the wild (called AffectNet). AffectNet contains more than 1,000,000 facial images from the Internet by querying three major search engines using 1250 emotion related keywords in six different languages. About half of the retrieved images were manually annotated for the presence of seven discrete facial expressions and the intensity of valence and arousal. AffectNet is by far the largest database of facial expression, valence, and arousal in the wild enabling research in automated facial expression recognition in two different emotion models. Two baseline deep neural networks are used to classify images in the categorical model and predict the intensity of valence and arousal. Various evaluation metrics show that our deep neural network baselines can perform better than conventional machine learning methods and off-the-shelf facial expression recognition systems.},
	number = {1},
	urldate = {2021-04-28},
	journal = {IEEE Transactions on Affective Computing},
	author = {Mollahosseini, Ali and Hasani, Behzad and Mahoor, Mohammad H.},
	month = jan,
	year = {2019},
	note = {NoCitationData[s0]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {18--31},
	annote = {Comment: IEEE Transactions on Affective Computing, 2017},
	file = {arXiv Fulltext PDF:/home/brendan/Zotero/storage/TRCN85MV/Mollahosseini et al. - 2019 - AffectNet A Database for Facial Expression, Valen.pdf:application/pdf;arXiv.org Snapshot:/home/brendan/Zotero/storage/MQ9BREGA/1708.html:text/html}
}

@inproceedings{ahmed_improved_2015,
	address = {Boston, MA, USA},
	title = {An improved deep learning architecture for person re-identification},
	isbn = {978-1-4673-6964-0},
	url = {http://ieeexplore.ieee.org/document/7299016/},
	doi = {10.1109/CVPR.2015.7299016},
	abstract = {In this work, we propose a method for simultaneously learning features and a corresponding similarity metric for person re-identiﬁcation. We present a deep convolutional architecture with layers specially designed to address the problem of re-identiﬁcation. Given a pair of images as input, our network outputs a similarity value indicating whether the two input images depict the same person. Novel elements of our architecture include a layer that computes cross-input neighborhood differences, which capture local relationships between the two input images based on midlevel features from each input image. A high-level summary of the outputs of this layer is computed by a layer of patch summary features, which are then spatially integrated in subsequent layers. Our method signiﬁcantly outperforms the state of the art on both a large data set (CUHK03) and a medium-sized data set (CUHK01), and is resistant to overﬁtting. We also demonstrate that by initially training on an unrelated large data set before ﬁne-tuning on a small target data set, our network can achieve results comparable to the state of the art even on a small data set (VIPeR).},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Ahmed, Ejaz and Jones, Michael and Marks, Tim K.},
	month = jun,
	year = {2015},
	note = {1203},
	pages = {3908--3916},
	file = {Ahmed et al. - 2015 - An improved deep learning architecture for person .pdf:/home/brendan/Zotero/storage/6LXF38P2/Ahmed et al. - 2015 - An improved deep learning architecture for person .pdf:application/pdf}
}

@inproceedings{zheng_scalable_2015,
	address = {Santiago, Chile},
	title = {Scalable {Person} {Re}-identification: {A} {Benchmark}},
	isbn = {978-1-4673-8391-2},
	shorttitle = {Scalable {Person} {Re}-identification},
	url = {http://ieeexplore.ieee.org/document/7410490/},
	doi = {10.1109/ICCV.2015.133},
	abstract = {This paper contributes a new high quality dataset for person re-identiﬁcation, named “Market-1501”. Generally, current datasets: 1) are limited in scale; 2) consist of hand-drawn bboxes, which are unavailable under realistic settings; 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated bboxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the Deformable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera.},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zheng, Liang and Shen, Liyue and Tian, Lu and Wang, Shengjin and Wang, Jingdong and Tian, Qi},
	month = dec,
	year = {2015},
	note = {2136},
	pages = {1116--1124},
	file = {Zheng et al. - 2015 - Scalable Person Re-identification A Benchmark.pdf:/home/brendan/Zotero/storage/JQWDEX2H/Zheng et al. - 2015 - Scalable Person Re-identification A Benchmark.pdf:application/pdf}
}

@inproceedings{wei_convolutional_2016,
	address = {Las Vegas, NV, USA},
	title = {Convolutional {Pose} {Machines}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780880/},
	doi = {10.1109/CVPR.2016.511},
	abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly reﬁned estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difﬁculty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
	language = {en},
	urldate = {2021-04-28},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
	month = jun,
	year = {2016},
	note = {2026},
	pages = {4724--4732},
	file = {Wei et al. - 2016 - Convolutional Pose Machines.pdf:/home/brendan/Zotero/storage/4SH4QUS7/Wei et al. - 2016 - Convolutional Pose Machines.pdf:application/pdf}
}

@inproceedings{baltrusaitis_openface_2016,
	title = {{OpenFace}: {An} open source facial behavior analysis toolkit},
	shorttitle = {{OpenFace}},
	doi = {10.1109/WACV.2016.7477553},
	abstract = {Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.},
	booktitle = {2016 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Baltrušaitis, Tadas and Robinson, Peter and Morency, Louis-Philippe},
	month = mar,
	year = {2016},
	note = {946},
	keywords = {Estimation, Face, Magnetic heads, Real-time systems, Training, Videos},
	pages = {1--10},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/9SBCPSZA/7477553.html:text/html;Full Text:/home/brendan/Zotero/storage/7N3Z9I2L/Baltrušaitis et al. - 2016 - OpenFace An open source facial behavior analysis .pdf:application/pdf}
}

@article{ijspeert_dynamical_2013,
	title = {Dynamical {Movement} {Primitives}: {Learning} {Attractor} {Models} for {Motor} {Behaviors}},
	volume = {25},
	abstract = {Nonlinear dynamical systems have been used in many disciplines to model complex behaviors, including biological motor control, robotics, perception, economics, traffic prediction, and neuroscience. While often the unexpected emergent behavior of nonlinear systems is the focus of investigations, it is of equal importance to create goal-directed behavior (e.g., stable locomotion from a system of coupled oscillators under perceptual guidance). Modeling goal-directed behavior with nonlinear systems is, however, rather difficult due to the parameter sensitivity of these systems, their complex phase transitions in response to subtle parameter changes, and the difficulty of analyzing and predicting their long-term behavior; intuition and time-consuming parameter tuning play a major role. This letter presents and reviews dynamical movement primitives, a line of research for modeling attractor behaviors of autonomous nonlinear dynamical systems with the help of statistical learning techniques. The essence of our approach is to start with a simple dynamical system, such as a set of linear differential equations, and transform those into a weakly nonlinear system with prescribed attractor dynamics by means of a learnable autonomous forcing term. Both point attractors and limit cycle attractors of almost arbitrary complexity can be generated. We explain the design principle of our approach and evaluate its properties in several example applications in motor control and robotics.},
	number = {2},
	journal = {Neural Computation},
	author = {Ijspeert, Auke Jan and Nakanishi, Jun and Hoffmann, Heiko and Pastor, Peter and Schaal, Stefan},
	month = feb,
	year = {2013},
	note = {1036},
	pages = {328--373},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/7FMCH7XZ/6797340.html:text/html}
}

@article{ortenzi_object_2020,
	title = {Object {Handovers}: a {Review} for {Robotics}},
	abstract = {This article surveys the literature on human-robot object handovers. A handover is a collaborative joint action where an agent, the giver, gives an object to another agent, the receiver. The physical exchange starts when the receiver first contacts the object held by the giver and ends when the giver fully releases the object to the receiver. However, important cognitive and physical processes begin before the physical exchange, including initiating implicit agreement with respect to the location and timing of the exchange. From this perspective, we structure our review into the two main phases delimited by the aforementioned events: 1) a pre-handover phase, and 2) the physical exchange. We focus our analysis on the two actors (giver and receiver) and report the state of the art of robotic givers (robot-to-human handovers) and the robotic receivers (human-to-robot handovers). We report a comprehensive list of qualitative and quantitative metrics commonly used to assess the interaction. While focusing our review on the cognitive level (e.g., prediction, perception, motion planning, learning) and the physical level (e.g., motion, grasping, grip release) of the handover, we briefly discuss also the concepts of safety, social context, and ergonomics. We compare the behaviours displayed during human-to-human handovers to the state of the art of robotic assistants, and identify the major areas of improvement for robotic assistants to reach performance comparable to human interactions. Finally, we propose a minimal set of metrics that should be used in order to enable a fair comparison among the approaches.},
	journal = {arXiv:2007.12952 [cs, eess]},
	author = {Ortenzi, Valerio and Cosgun, Akansel and Pardi, Tommaso and Chan, Wesley and Croft, Elizabeth and Kulic, Dana},
	month = jul,
	year = {2020},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: Review paper, 19 pages},
	file = {arXiv Fulltext PDF:/Users/tid010/Zotero/storage/3AJJ4C4B/Ortenzi et al. - 2020 - Object Handovers a Review for Robotics.pdf:application/pdf;arXiv.org Snapshot:/Users/tid010/Zotero/storage/SZP42XNG/2007.html:text/html},
}

@article{henriques_high-speed_2015,
	title = {High-{Speed} {Tracking} with {Kernelized} {Correlation} {Filters}},
	volume = {37},
	abstract = {The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies -- any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the Discrete Fourier Transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new Kernelized Correlation Filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call Dual Correlation Filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Henriques, João F. and Caseiro, Rui and Martins, Pedro and Batista, Jorge},
	month = mar,
	year = {2015},
	note = {NoCitationData[s0]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {583--596},
	file = {arXiv Fulltext PDF:/home/brendan/Zotero/storage/AKURILNZ/Henriques et al. - 2015 - High-Speed Tracking with Kernelized Correlation Fi.pdf:application/pdf;arXiv.org Snapshot:/home/brendan/Zotero/storage/ZBPN5332/1404.html:text/html}
}

@inproceedings{schroff_facenet_2015,
	title = {{FaceNet}: {A} unified embedding for face recognition and clustering},
	abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure offace similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings asfeature vectors. Our method uses a deep convolutional network trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. To train, we use triplets of roughly aligned matching / non-matching face patches generated using a novel online triplet mining method. The benefit of our approach is much greater representational efficiency: we achieve state-of-the-artface recognition performance using only 128-bytes perface. On the widely used Labeled Faces in the Wild (LFW) dataset, our system achieves a new record accuracy of 99.63\%. On YouTube Faces DB it achieves 95.12\%. Our system cuts the error rate in comparison to the best published result [15] by 30\% on both datasets.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Schroff, F. and Kalenichenko, D. and Philbin, J.},
	month = jun,
	year = {2015},
	note = {7513},
	keywords = {Accuracy, Artificial neural networks, Face, Face recognition, Principal component analysis, Standards, Training},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/5P3PHK6Y/7298682.html:text/html}
}

@inproceedings{babenko_visual_2009,
	title = {Visual tracking with online {Multiple} {Instance} {Learning}},
	doi = {10.1109/CVPR.2009.5206737},
	abstract = {In this paper, we address the problem of learning an adaptive appearance model for object tracking. In particular, a class of tracking techniques called “tracking by detection” have been shown to give promising results at real-time speeds. These methods train a discriminative classifier in an online manner to separate the object from the background. This classifier bootstraps itself by using the current tracker state to extract positive and negative examples from the current frame. Slight inaccuracies in the tracker can therefore lead to incorrectly labeled training examples, which degrades the classifier and can cause further drift. In this paper we show that using Multiple Instance Learning (MIL) instead of traditional supervised learning avoids these problems, and can therefore lead to a more robust tracker with fewer parameter tweaks. We present a novel online MIL algorithm for object tracking that achieves superior results with real-time performance.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Babenko, B. and Yang, M. and Belongie, S.},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Degradation, Robustness, Supervised learning},
	pages = {983--990},
	file = {IEEE Xplore Abstract Record:/Users/tid010/Zotero/storage/KIT4R4TU/5206737.html:text/html},
}

@inproceedings{vu15heads,
	author = {Vu, Tuan{-}Hung and Osokin, Anton and Laptev, Ivan},
	title = {Context-aware {CNNs} for person head detection},
	booktitle =  {International Conference on Computer Vision (ICCV)},
	year = {2015}}

@article{almonfrey2018flexible,
  title={A flexible human detection service suitable for Intelligent Spaces based on a multi-camera network},
  author={Almonfrey, Douglas and do Carmo, Alexandre Pereira and de Queiroz, Felippe Mendon{\c{c}}a and Picoreti, Rodolfo and Vassallo, Raquel Frizera and Salles, Evandro Ottoni Teatini},
  journal={International Journal of Distributed Sensor Networks},
  volume={14},
  number={3},
  pages={1550147718763550},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{kollmitz2019deep,
  title={Deep 3D perception of people and their mobility aids},
  author={Kollmitz, Marina and Eitel, Andreas and Vasquez, Andres and Burgard, Wolfram},
  journal={Robotics and Autonomous Systems},
  volume={114},
  pages={29--40},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{fleet_lsd-slam_2014,
	title = {{LSD}-{SLAM}: {Large}-{Scale} {Direct} {Monocular} {SLAM}},
	abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by ﬁltering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on sim(3), thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the eﬀect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Engel, Jakob and Schöps, Thomas and Cremers, Daniel},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	note = {NoCitationData[s0]},
	file = {Engel et al. - 2014 - LSD-SLAM Large-Scale Direct Monocular SLAM.pdf:/home/brendan/Zotero/storage/W8KHGHDJ/Engel et al. - 2014 - LSD-SLAM Large-Scale Direct Monocular SLAM.pdf:application/pdf}
}

@inproceedings{liu_ssd_2016,
	title = {{SSD}: {Single} {Shot} {MultiBox} {Detector}},
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets conﬁrm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a uniﬁed framework for both training and inference. For 300 × 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
	booktitle = {European {Conference} on {Computer} {Vision}},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	year = {2016},
	note = {13811},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2016},
	file = {Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:/home/brendan/Zotero/storage/9K7VXY9R/Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf:application/pdf}
}

@article{bay_speeded-up_2008,
	title = {Speeded-{Up} {Robust} {Features} ({SURF})},
	volume = {110},
	abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision.},
	number = {3},
	journal = {Comput. Vis. Image Underst.},
	author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
	month = jun,
	year = {2008},
	note = {11620},
	keywords = {Camera calibration, Feature description, Interest points, Local features, Object recognition},
	pages = {346--359}
}

@article{mendes2017human,
  title={Human behavior and hand gesture classification for smart human-robot interaction},
  author={Mendes, Nuno and Ferrer, Jo{\~a}o and Vitorino, Jo{\~a}o and Safeea, Mohammad and Neto, Pedro},
  journal={Procedia Manufacturing},
  volume={11},
  pages={91--98},
  year={2017},
  publisher={Elsevier}
}

@inproceedings{rehm2013negative,
  title={Negative affect in human robot interaction—impoliteness in unexpected encounters with robots},
  author={Rehm, Matthias and Krogsager, Anders},
  booktitle={2013 IEEE RO-MAN},
  pages={45--50},
  year={2013},
  organization={IEEE}
}

@inproceedings{yu2011eye,
  title={An eye detection and localization system for natural human and robot interaction without face detection},
  author={Yu, Xinguo and Han, Weicheng and Li, Liyuan and Shi, Ji Yu and Wang, Gang},
  booktitle={Conference Towards Autonomous Robotic Systems},
  pages={54--65},
  year={2011},
  organization={Springer}
}

@inproceedings{gonzalez2020audiovisual,
  title={Audiovisual cognitive architecture for autonomous learning of face localisation by a Humanoid Robot},
  author={Gonzalez-Billandon, Jonas and Sciutti, Alessandra and Tata, Matthew and Sandini, Giulio and Rea, Francesco},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={5979--5985},
  year={2020},
  organization={IEEE}
}

@inproceedings{massardi2020parc,
  title={Parc: a plan and activity recognition component for assistive robots},
  author={Massardi, Jean and Gravel, Mathieu and Beaudry, {\'E}ric},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={3025--3031},
  year={2020},
  organization={IEEE}
}

@inproceedings{chen2020human,
  title={Human-robot skill transfer systems for mobile robot based on multi sensor fusion},
  author={Chen, Dingping and He, Jilin and Chen, Guanyu and Yu, Xiaopeng and He, Miaolei and Yang, Youwen and Li, Junsong and Zhou, Xuanyi},
  booktitle={2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)},
  pages={1354--1359},
  year={2020},
  organization={IEEE}
}

@inproceedings{yamakawa2018human,
  title={Human--robot collaborative manipulation using a high-speed robot hand and a high-speed camera},
  author={Yamakawa, Yuji and Matsui, Yutaro and Ishikawa, Masatoshi},
  booktitle={2018 IEEE International Conference on Cyborg and Bionic Systems (CBS)},
  pages={426--429},
  year={2018},
  organization={IEEE}
}

@inproceedings{jiang2018personalize,
  title={Personalize vison-based human following for mobile robots by learning from human-driven demonstrations},
  author={Jiang, Lihua and Wang, Weitian and Chen, Yi and Jia, Yunyi},
  booktitle={2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
  pages={726--731},
  year={2018},
  organization={IEEE}
}

@article{liu2018learning,
  title={Learning proactive behavior for interactive social robots},
  author={Liu, Phoebe and Glas, Dylan F and Kanda, Takayuki and Ishiguro, Hiroshi},
  journal={Autonomous Robots},
  volume={42},
  number={5},
  pages={1067--1085},
  year={2018},
  publisher={Springer}
}

@inproceedings{khatib2017visual,
  title={Visual coordination task for human-robot collaboration},
  author={Khatib, Maram and Al Khudir, Khaled and De Luca, Alessandro},
  booktitle={2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)},
  pages={3762--3768},
  year={2017},
  organization={IEEE}
}

@inproceedings{tornow2013multi,
  title={A multi-agent mobile robot system with environment perception and HMI capabilities},
  author={Tornow, Michael and Al-Hamadi, Ayoub and Borrmann, Vinzenz},
  booktitle={2013 IEEE International Conference on Signal and Image Processing Applications},
  pages={252--257},
  year={2013},
  organization={IEEE}
}

@inproceedings{nguyen2015interaction,
  title={Interaction between humans, NAO robot and multiple cameras for colored objects recognition using information fusion},
  author={Nguyen, Thanh Long and Boukezzoula, Reda and Coquin, Didier and Benoit, Eric and Perrin, St{\'e}phane},
  booktitle={2015 8th International Conference on Human System Interaction (HSI)},
  pages={322--328},
  year={2015},
  organization={IEEE}
}

@article{zhang2013real,
  title={Real-time multiple human perception with color-depth cameras on a mobile robot},
  author={Zhang, Hao and Reardon, Christopher and Parker, Lynne E},
  journal={IEEE Transactions on Cybernetics},
  volume={43},
  number={5},
  pages={1429--1441},
  year={2013},
  publisher={IEEE}
}

@inproceedings{mckeague2013hand,
  title={Hand and body association in crowded environments for human-robot interaction},
  author={McKeague, Stephen and Liu, Jindong and Yang, Guang-Zhong},
  booktitle={2013 IEEE International Conference on Robotics and Automation},
  pages={2161--2168},
  year={2013},
  organization={IEEE}
}

@article{cicconet2013human,
  title={Human-robot percussion ensemble: Anticipation on the basis of visual cues},
  author={Cicconet, Marcelo and Bretan, Mason and Weinberg, Gil},
  journal={IEEE Robotics \& Automation Magazine},
  volume={20},
  number={4},
  pages={105--110},
  year={2013},
  publisher={IEEE}
}

@article{randelli2013knowledge,
  title={Knowledge acquisition through human--robot multimodal interaction},
  author={Randelli, Gabriele and Bonanni, Taigo Maria and Iocchi, Luca and Nardi, Daniele},
  journal={Intelligent Service Robotics},
  volume={6},
  number={1},
  pages={19--31},
  year={2013},
  publisher={Springer}
}

@inproceedings{moladande2019implicit,
  title={Implicit Intention and Activity Recognition of a Human Using Neural Networks for a Service Robot Eye},
  author={Moladande, MWCN and Madhusanka, BGDA},
  booktitle={2019 International Research Conference on Smart Computing and Systems Engineering (SCSE)},
  pages={38--43},
  year={2019},
  organization={IEEE}
}

@inproceedings{del2011interaction,
  title={Interaction in robotics with a combination of vision, tactile and force sensing},
  author={del Pobil, Angel P and Prats, Mario and Sanz, Pedro J},
  booktitle={2011 Fifth International Conference on Sensing Technology},
  pages={21--26},
  year={2011},
  organization={IEEE}
}

@incollection{haddadin2011towards,
  title={Towards the robotic co-worker},
  author={Haddadin, Sami and Suppa, Michael and Fuchs, Stefan and Bodenm{\"u}ller, Tim and Albu-Sch{\"a}ffer, Alin and Hirzinger, Gerd},
  booktitle={Robotics Research},
  pages={261--282},
  year={2011},
  publisher={Springer}
}

@inproceedings{pop2019control,
  title={Control a 6DOF Anthropomorphic Robotic Structure with Computer Vision as MEMS Input},
  author={Pop, Alexandru and Stan, Ovidiu},
  booktitle={2019 22nd International Conference on Control Systems and Computer Science (CSCS)},
  pages={700--706},
  year={2019},
  organization={IEEE}
}

@incollection{papadopoulos2019advanced,
  title={An Advanced Human-Robot Interaction Interface for Collaborative Robotic Assembly Tasks},
  author={Papadopoulos, Christos and Mariolis, Ioannis and Topalidou-Kyniazopoulou, Angeliki and Piperagkas, Grigorios and Ioannidis, Dimosthenis and Tzovaras, Dimitrios},
  booktitle={Rapid Automation: Concepts, Methodologies, Tools, and Applications},
  pages={794--812},
  year={2019},
  publisher={IGI Global}
}

@article{mcfassel2018prototyping,
  title={Prototyping and evaluation of interactive and customized interface and control algorithms for robotic assistive devices using Kinect and infrared sensor},
  author={McFassel, Grace and Hsieh, Sheng-Jen and Peng, Bo},
  journal={International Journal of Advanced Robotic Systems},
  volume={15},
  number={2},
  pages={1729881418769521},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{hacinecipoglu2020pose,
  title={Pose invariant people detection in point clouds for mobile robots},
  author={Hacinecipoglu, Akif and Konukseven, ERHAN and Koku, AHMET},
  journal={International Journal of Mechanical Engineering and Robotics Research},
  volume={9},
  number={5},
  year={2020}
}

@inproceedings{zhao2016intuitive,
  title={Intuitive robot teaching by hand guided demonstration},
  author={Zhao, Lijun and Li, Xiaoyu and Liang, Peidong and Yang, Chenguang and Li, Ruifeng},
  booktitle={2016 IEEE International Conference on Mechatronics and Automation},
  pages={1578--1583},
  year={2016},
  organization={IEEE}
}

@article{ben2016kinect,
  title={Kinect-based sliding mode control for Lynxmotion robotic arm},
  author={Ben Abdallah, Ismail and Bouteraa, Yassine and Rekik, Chokri},
  journal={Advances in Human-Computer Interaction},
  volume={2016},
  year={2016},
  publisher={Hindawi}
}

@inproceedings{mishra2014robot,
  title={Robot arm manipulation using depth-sensing cameras and inverse kinematics},
  author={Mishra, Akhilesh Kumar and Meruvia-Pastor, Oscar},
  booktitle={2014 Oceans-St. John's},
  pages={1--6},
  year={2014},
  organization={IEEE}
}

@inproceedings{ueno2014efficient,
  title={An efficient method for human pointing estimation for robot interaction},
  author={Ueno, Satoshi and Naito, Sei and Chen, Tsuhan},
  booktitle={2014 IEEE International Conference on Image Processing (ICIP)},
  pages={1545--1549},
  year={2014},
  organization={IEEE}
}

@article{du2014markerless,
  title={Markerless human--robot interface for dual robot manipulators using Kinect sensor},
  author={Du, Guanglong and Zhang, Ping},
  journal={Robotics and Computer-Integrated Manufacturing},
  volume={30},
  number={2},
  pages={150--159},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{yamamoto20112d,
  title={A 2D safety vision system for human-robot collaborative work environments based upon the safety preservation design policy},
  author={Yamamoto, Takafumi and Yamada, Yoji and Onishi, Masaki and Nakabo, Yoshihiro},
  booktitle={2011 IEEE International Conference on Robotics and Biomimetics},
  pages={2049--2054},
  year={2011},
  organization={IEEE}
}

@article{canny_computational_1986,
	title = {A {Computational} {Approach} to {Edge} {Detection}},
	volume = {PAMI-8},
	abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
	number = {6},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Canny, J.},
	month = nov,
	year = {1986},
	note = {36945},
	keywords = {Detectors, Edge detection, feature extraction, Feature extraction, Gaussian approximation, Image edge detection, image processing, machine vision, Machine vision, multiscale image analysis, Performance analysis, Shape measurement, Signal synthesis, Signal to noise ratio, Uncertainty},
	pages = {679--698},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/23CNWG6P/4767851.html:text/html}
}

@inproceedings{dalal_histograms_2005,
	title = {Histograms of oriented gradients for human detection},
	abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	author = {Dalal, N. and Triggs, B.},
	month = jun,
	year = {2005},
	note = {35456},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	keywords = {High performance computing, Histograms, Humans, Image databases, Image edge detection, Object detection, Object recognition, Robustness, Support vector machines, Testing},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/9STQYGNC/1467360.html:text/html;Submitted Version:/home/brendan/Zotero/storage/KARLN7WL/Dalal and Triggs - 2005 - Histograms of oriented gradients for human detecti.pdf:application/pdf}
}

@article{ojala_lbp_2002,
author = {Ojala, Timo and Pietik\"{a}inen, Matti and M\"{a}enp\"{a}\"{a}, Topi},
title = {Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns},
year = {2002},
issue_date = {July 2002},
publisher = {IEEE Computer Society},
address = {USA},
volume = {24},
number = {7},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2002.1017623},
doi = {10.1109/TPAMI.2002.1017623},
abstract = {This paper presents a theoretically very simple, yet efficient, multiresolution approach to gray-scale and rotation invariant texture classification based on local binary patterns and nonparametric discrimination of sample and prototype distributions. The method is based on recognizing that certain local binary patterns, termed "uniform" are fundamental properties of local image texture and their occurrence histogram is proven to be a very powerful texture feature. We derive a generalized gray-scale and rotation invariant operator presentation that allows for detecting the "uniform" patterns for any quantization of the angular space and for any spatial resolution and presents a method for combining multiple operators for multiresolution analysis. The proposed approach is very robust in terms of gray-scale variations since the operator is, by definition, invariant against any monotonic transformation of the gray scale. Another advantage is computational simplicity as the operator can be realized with a few operations in a small neighborhood and a lookup table. Excellent experimental results obtained in true problems of rotation invariance, where the classifier is trained at one particular rotation angle and tested with samples from other rotation angles, demonstrate that good discrimination can be achieved with the occurrence statistics of simple rotation invariant local binary patterns. These operators characterize the spatial configuration of local image texture and the performance can be further improved by combining them with rotation invariant variance measures that characterize the contrast of local image texture. The joint distributions of these orthogonal measures are shown to be very powerful tools for rotation invariant texture analysis.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = jul,
pages = {971–987},
numpages = {17},
keywords = {distribution, Outex, histogram, Brodatz, texture analysis, contrast., Nonparametric}
}

@inproceedings{szegedy_going_2015,
	title = {Going {Deeper} with {Convolutions}},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	year = {2015},
	note = {29438}
}

@inproceedings{chatfield_return_2014,
	title = {Return of the {Devil} in the {Details}: {Delving} {Deep} into {Convolutional} {Nets}},
	abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
	booktitle = {British {Machine} {Vision} {Conference}},
	author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = nov,
	year = {2014},
	note = {3236},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published in proceedings of BMVC 2014},
	file = {arXiv Fulltext PDF:/home/brendan/Zotero/storage/56A4B2ZG/Chatfield et al. - 2014 - Return of the Devil in the Details Delving Deep i.pdf:application/pdf;arXiv.org Snapshot:/home/brendan/Zotero/storage/SLA7T2XD/1405.html:text/html}
}

@inproceedings{renNIPS15fasterrcnn,
    Author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
    Title = {Faster {R-CNN}: Towards Real-Time Object Detection
             with Region Proposal Networks},
    Booktitle = {Advances in Neural Information Processing Systems ({NIPS})},
    Year = {2015}
}

@inproceedings{shahroudy_ntu_2016,
	title = {{NTU} {RGB}+{D}: {A} {Large} {Scale} {Dataset} for {3D} {Human} {Activity} {Analysis}},
	abstract = {Recent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classification of action classes. Currently available depth-based and RGB+Dbased action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classification. Experimental results show the advantages of applying deep learning methods over state-of-the-art handcrafted features on the suggested cross-subject and cross-view evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Shahroudy, A. and Liu, J. and Ng, T. and Wang, G.},
	month = jun,
	year = {2016},
	note = {NoCitationData[s0]},
	keywords = {Benchmark testing, Cameras, Feature extraction, Learning systems, Recurrent neural networks, Skeleton, Three-dimensional displays},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/WW9EJSVI/7780484.html:text/html;Submitted Version:/home/brendan/Zotero/storage/XPT89ISJ/Shahroudy et al. - 2016 - NTU RGB+D A Large Scale Dataset for 3D Human Acti.pdf:application/pdf}
}

@article{ionescu_human36m_2014,
	title = {Human3.{6M}: {Large} {Scale} {Datasets} and {Predictive} {Methods} for {3D} {Human} {Sensing} in {Natural} {Environments}},
	volume = {36},
	abstract = {We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20\% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ionescu, C. and Papava, D. and Olaru, V. and Sminchisescu, C.},
	month = jul,
	year = {2014},
	keywords = {3D human pose estimation, articulated body modeling, Cameras, Estimation, Fourier kernel approximations, human motion capture data, Joints, large-scale learning, Modeling and recovery of physical attributes, Motion, optimization, Sensors, Solid modeling, structured prediction, Three-dimensional displays, Training}
}

@inproceedings{wang_action_2011,
	title = {Action recognition by dense trajectories},
	abstract = {Feature trajectories have shown to be efﬁcient for representing videos. Typically, they are extracted using the KLT tracker or matching SIFT descriptors between frames. However, the quality as well as quantity of these trajectories is often not sufﬁcient. Inspired by the recent success of dense sampling in image classiﬁcation, we propose an approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on displacement information from a dense optical ﬂow ﬁeld. Given a state-of-the-art optical ﬂow algorithm, our trajectories are robust to fast irregular motions as well as shot boundaries. Additionally, dense trajectories cover the motion information in videos well.},
	author = {Wang, Heng and Klaser, Alexander and Schmid, Cordelia and Liu, Cheng-Lin},
	month = jun,
	year = {2011},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	file = {Wang et al. - 2011 - Action recognition by dense trajectories.pdf:/home/brendan/Zotero/storage/D33FYVDE/Wang et al. - 2011 - Action recognition by dense trajectories.pdf:application/pdf}
}

@article{fermuller_prediction_2018,
	title = {Prediction of {Manipulation} {Actions}},
	volume = {126},
	abstract = {By looking at a person’s hands, one can often tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor’s intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded videos of subjects performing different manipulation actions on the same object, such as “squeezing”, “flipping”, “washing”, “wiping” and “scratching” with a sponge. In psychophysical experiments, we evaluated human observers’ skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input image patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets show that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithms to predict in real time what and how a dexterous action is performed.},
	number = {2},
	journal = {International Journal of Computer Vision},
	author = {Fermüller, Cornelia and Wang, Fang and Yang, Yezhou and Zampogiannis, Konstantinos and Zhang, Yi and Barranco, Francisco and Pfeiffer, Michael},
	month = apr,
	year = {2018},
	note = {37},
	pages = {358--374},
	file = {Submitted Version:/Users/tid010/Zotero/storage/MU6CUI8H/Fermüller et al. - 2018 - Prediction of Manipulation Actions.pdf:application/pdf},
}

@inproceedings{girshick_fast_rcnn,
author = {Girshick, Ross},
title = {Fast R-CNN},
year = {2015},
isbn = {9781467383912},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCV.2015.169},
doi = {10.1109/ICCV.2015.169},
abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
pages = {1440–1448},
numpages = {9},
series = {ICCV '15}
}

@inproceedings{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {54261},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/brendan/Zotero/storage/5Z27CCKD/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/home/brendan/Zotero/storage/GI8GU393/1409.html:text/html}
}

@article{draper2017ethical,
  title={Ethical values and social care robots for older people: an international qualitative study},
  author={Draper, Heather and Sorell, Tom},
  journal={Ethics and Information Technology},
  volume={19},
  number={1},
  pages={49},
  year={2017},
  publisher={Springer Nature BV}
}

@article{hoffman2019evaluating,
  title={Evaluating fluency in human--robot collaboration},
  author={Hoffman, Guy},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={49},
  number={3},
  pages={209--218},
  year={2019},
  publisher={IEEE}
}

@article{doi:10.1177/2050157919843961,
author = {Christoph Lutz and Maren Schöttler and Christian Pieter Hoffmann},
title ={The privacy implications of social robots: Scoping review and expert interviews},
journal = {Mobile Media \& Communication},
volume = {7},
number = {3},
pages = {412-434},
year = {2019},
doi = {10.1177/2050157919843961},

URL = { 
        https://doi.org/10.1177/2050157919843961
    
},
eprint = { 
        https://doi.org/10.1177/2050157919843961
    
}
,
    abstract = { In this contribution, we investigate the privacy implications of social robots as an emerging mobile technology. Drawing on a scoping literature review and expert interviews, we show how social robots come with privacy implications that go beyond those of established mobile technology. Social robots challenge not only users’ informational privacy but also affect their physical, psychological, and social privacy due to their autonomy and potential for social bonding. These distinctive privacy challenges require study from varied theoretical perspectives, with contextual privacy and human–machine communication emerging as particularly fruitful lenses. Findings also point to an increasing focus on technological privacy solutions, complementing an evolving legal landscape as well as a strengthening of user agency and literacy. }
}


@inproceedings{bilen2016dynamic,
  title={Dynamic image networks for action recognition},
  author={Bilen, Hakan and Fernando, Basura and Gavves, Efstratios and Vedaldi, Andrea and Gould, Stephen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3034--3042},
  year={2016}
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
	month = jun,
	year = {2016},
	note = {73363},
	keywords = {Complexity theory, Degradation, Image recognition, Image segmentation, Neural networks, Training, Visualization},
	pages = {770--778},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/MUQEVVYK/7780459.html:text/html;Submitted Version:/home/brendan/Zotero/storage/S8KHSILZ/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@inproceedings{szegedy_rethinking_2016,
	title = {Rethinking the {Inception} {Architecture} for {Computer} {Vision}},
	abstract = {Convolutional networks are at the core of most stateof-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efﬁciency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efﬁciently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classiﬁcation challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error and 17.3\% top-1 error on the validation set and 3.6\% top-5 error on the ofﬁcial test set.},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
	month = jun,
	year = {2016},
	note = {12616},
	file = {Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf:/home/brendan/Zotero/storage/P9TGRPS6/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer.pdf:application/pdf}
}

@InProceedings{10.1007/978-3-030-62056-1_44,
author="Khavas, Zahra Rezaei
and Ahmadzadeh, S. Reza
and Robinette, Paul",
editor="Wagner, Alan R.
and Feil-Seifer, David
and Haring, Kerstin S.
and Rossi, Silvia
and Williams, Thomas
and He, Hongsheng
and Sam Ge, Shuzhi",
title="Modeling Trust in Human-Robot Interaction: A Survey",
booktitle="Social Robotics",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="529--541",
abstract="As the autonomy and capabilities of robotic systems increase, they are expected to play the role of teammates rather than tools and interact with human collaborators in a more realistic manner, creating a more human-like relationship. Given the impact of trust observed in human-robot interaction (HRI), appropriate trust in robotic collaborators is one of the leading factors influencing the performance of human-robot interaction. Team performance can be diminished if people do not trust robots appropriately by disusing or misusing them based on limited experience. Therefore, trust in HRI needs to be calibrated properly, rather than maximized, to let the formation of an appropriate level of trust in human collaborators. For trust calibration in HRI, trust needs to be modeled first. There are many reviews on factors affecting trust in HRI[22], however, as there are no reviews concentrated on different trust models, in this paper, we review different techniques and methods for trust modeling in HRI. We also present a list of potential directions for further research and some challenges that need to be addressed in future work on human-robot trust modeling.",
isbn="978-3-030-62056-1"
}

@inproceedings{szegedy_incept_2017,
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
title = {Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning},
year = {2017},
publisher = {AAAI Press},
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {4278–4284},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@inproceedings{kanade_comprehensive_2000,
	title = {Comprehensive database for facial expression analysis},
	abstract = {Within the past decade, significant effort has occurred in developing methods of facial expression analysis. Because most investigators have used relatively limited data sets, the generalizability of these various methods remains unknown. We describe the problem space for facial expression analysis, which includes level of description, transitions among expression, eliciting conditions, reliability and validity of training and test data, individual differences in subjects, head orientation and scene complexity, image characteristics, and relation to non-verbal behavior. We then present the CMU-Pittsburgh AU-Coded Face Expression Image Database, which currently includes 2105 digitized image sequences from 182 adult subjects of varying ethnicity, performing multiple tokens of most primary FACS action units. This database is the most comprehensive test-bed to date for comparative studies of facial expression analysis.},
	urldate = {2021-03-23},
	booktitle = {Proceedings {Fourth} {IEEE} {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition} ({Cat}. {No}. {PR00580})},
	author = {Kanade, T. and Cohn, J.F. and {Yingli Tian}},
	year = {2000},
	note = {3124},
	file = {Kanade et al. - 2000 - Comprehensive database for facial expression analy.pdf:/home/brendan/Zotero/storage/7MS6BCZ6/Kanade et al. - 2000 - Comprehensive database for facial expression analy.pdf:application/pdf}
}

@inproceedings{lucey_extended_2010,
	title = {The {Extended} {Cohn}-{Kanade} {Dataset} ({CK}+): {A} complete dataset for action unit and emotion-specified expression},
	abstract = {In 2000, the Cohn-Kanade (CK) database was released for the purpose of promoting research into automatically detecting individual facial expressions. Since then, the CK database has become one of the most widely used test-beds for algorithm development and evaluation. During this period, three limitations have become apparent: 1) While AU codes are well validated, emotion labels are not, as they refer to what was requested rather than what was actually performed, 2) The lack of a common performance metric against which to evaluate new algorithms, and 3) Standard protocols for common databases have not emerged. As a consequence, the CK database has been used for both AU and emotion detection (even though labels for the latter have not been validated), comparison with benchmark algorithms is missing, and use of random subsets of the original database makes meta-analyses difficult. To address these and other concerns, we present the Extended Cohn-Kanade (CK+) database. The number of sequences is increased by 22\% and the number of subjects by 27\%. The target expression for each sequence is fully FACS coded and emotion labels have been revised and validated. In addition to this, non-posed sequences for several types of smiles and their associated metadata have been added. We present baseline results using Active Appearance Models (AAMs) and a linear support vector machine (SVM) classifier using a leave-one-out subject cross-validation for both AU and emotion detection for the posed data. The emotion and AU labels, along with the extended image data and tracked landmarks will be made available July 2010.},
	booktitle = {2010 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} - {Workshops}},
	author = {Lucey, P. and Cohn, J. F. and Kanade, T. and Saragih, J. and Ambadar, Z. and Matthews, I.},
	month = jun,
	year = {2010},
	note = {3019},
	keywords = {action unit, Active appearance model, active appearance models, CK database, Code standards, Databases, emotion detection, emotion recognition, emotion-specified expression, extended Cohn-Kanade dataset, Face detection, face recognition, facial expression detection, Gold, image classification, leave-one-out subject cross-validation, linear support vector machine, Measurement, Performance evaluation, Support vector machine classification, support vector machines, Support vector machines, SVM classifier, Testing},
	file = {IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/FPMJRY2V/5543262.html:text/html;Submitted Version:/home/brendan/Zotero/storage/PJCUZYFC/Lucey et al. - 2010 - The Extended Cohn-Kanade Dataset (CK+) A complete.pdf:application/pdf}
}

@article{viola_robust_2004,
	title = {Robust {Real}-{Time} {Face} {Detection}},
	abstract = {This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The ﬁrst is the introduction of a new image representation called the “Integral Image” which allows the features used by our detector to be computed very quickly. The second is a simple and efﬁcient classiﬁer which is built using the AdaBoost learning algorithm (Freund and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classiﬁers in a “cascade” which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems (Sung and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second.},
	language = {en},
	journal = {International Journal of Computer Vision},
	author = {Viola, Paul and Jones, Michael J},
	year = {2004},
	note = {16072},
	file = {Viola and Jones - Robust Real-Time Face Detection.pdf:/home/brendan/Zotero/storage/C6SC7GG7/Viola and Jones - Robust Real-Time Face Detection.pdf:application/pdf}
}

@InProceedings{Cao_2017_CVPR,
author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
title = {Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@article{ionescu_dataset_2014,
author = {Ionescu, Catalin and Papava, Dragos and Olaru, Vlad and Sminchisescu, Cristian},
title = {Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments},
year = {2014},
issue_date = {July 2014},
publisher = {IEEE Computer Society},
address = {USA},
volume = {36},
number = {7},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2013.248},
doi = {10.1109/TPAMI.2013.248},
abstract = {We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = jul,
pages = {1325–1339},
numpages = {15}
}

@inproceedings{goodfellow_gan_2014,
author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative Adversarial Nets},
year = {2014},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to ½ everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
pages = {2672–2680},
numpages = {9},
location = {Montreal, Canada},
series = {NIPS'14}
}

@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@book{bartneck2020human,
  title={Human-robot interaction: An introduction},
  author={Bartneck, Christoph and Belpaeme, Tony and Eyssel, Friederike and Kanda, Takayuki and Keijsers, Merel and {\v{S}}abanovi{\'c}, Selma},
  year={2020},
  publisher={Cambridge University Press}
}

@InProceedings{Solbach_2017_ICCV,
author = {Solbach, Markus D. and Tsotsos, John K.},
title = {Vision-Based Fallen Person Detection for the Elderly},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops},
month = {Oct},
year = {2017}
}

@ARTICLE{9206048,
  author={Rosenberger, Patrick and Cosgun, Akansel and Newbury, Rhys and Kwan, Jun and Ortenzi, Valerio and Corke, Peter and Grafinger, Manfred},
  journal={IEEE Robotics and Automation Letters}, 
  title={Object-Independent Human-to-Robot Handovers Using Real Time Robotic Vision}, 
  year={2021},
  volume={6},
  number={1},
  pages={17-23},
  doi={10.1109/LRA.2020.3026970}}

@INPROCEEDINGS{8955665,
author={D. {Leal} and Y. {Yihun}},
booktitle={2019 IEEE International Symposium on Measurement and Control in Robotics (ISMCR)}, title={Progress in Human-Robot Collaboration for Object Handover},
year={2019},
volume={},
number={},
pages={C3-2-1-C3-2-6},
doi={10.1109/ISMCR47492.2019.8955665}}

@article{HaddadinSami2009RfSR,
issn = {0278-3649},
abstract = {Physical human—robot interaction and cooperation has become a topic of increasing importance and of major focus in robotics research. An essential requirement of a robot designed for high mobility and direct interaction with human users or uncertain environments is that it must in no case pose a threat to the human. Until recently, quite a few attempts were made to investigate real-world threats via collision tests and use the outcome to considerably improve safety during physical human—robot interaction. In this paper, we give an overview of our systematic evaluation of safety in human—robot interaction, covering various aspects of the most significant injury mechanisms. In order to quantify the potential injury risk emanating from such a manipulator, impact tests with the DLR-Lightweight Robot III were carried out using standard automobile crash test facilities at the German Automobile Club (ADAC). Based on these tests, several industrial robots of different weight have been evaluated and the influence of the robot mass and velocity have been investigated. The evaluated non-constrained impacts would only partially capture the nature of human—robot safety. A possibly constrained environment and its effect on the resulting human injuries are discussed and evaluated from different perspectives. As well as such impact tests and simulations, we have analyzed the problem of the quasi-static constrained impact, which could pose a serious threat to the human even for low-inertia robots under certain circumstances. Finally, possible injuries relevant in robotics are summarized and systematically classified.},
journal = {The International journal of robotics research},
pages = {1507--1527},
volume = {28},
publisher = {SAGE Publications},
number = {11-12},
year = {2009},
title = {Requirements for Safe Robots: Measurements, Analysis and New Insights},
copyright = {The Author(s), 2009.},
language = {eng},
address = {London, England},
author = {Haddadin, Sami and Albu-Schäffer, Alin and Hirzinger, Gerd},
keywords = {Safety and security measures ; Analysis ; Human-computer interaction ; Design and construction ; Research ; Mobile robots ; Robotics},
}



@article{AjoudaniArash2018Papo,
issn = {0929-5593},
abstract = {Recent technological advances in hardware design of the robotic platforms enabled the implementation of various control modalities for improved interactions with humans and unstructured environments. An important application area for the integration of robots with such advanced interaction capabilities is human–robot collaboration. This aspect represents high socio-economic impacts and maintains the sense of purpose of the involved people, as the robots do not completely replace the humans from the work process. The research community’s recent surge of interest in this area has been devoted to the implementation of various methodologies to achieve intuitive and seamless human–robot-environment interactions by incorporating the collaborative partners’ superior capabilities, e.g. human’s cognitive and robot’s physical power generation capacity. In fact, the main purpose of this paper is to review the state-of-the-art on intermediate human–robot interfaces (bi-directional), robot control modalities, system stability, benchmarking and relevant use cases, and to extend views on the required future developments in the realm of human–robot collaboration.},
journal = {Autonomous robots},
pages = {957--975},
volume = {42},
publisher = {Springer US},
number = {5},
year = {2018},
title = {Progress and prospects of the human–robot collaboration},
copyright = {Springer Science+Business Media, LLC 2017},
language = {eng},
address = {New York},
author = {Ajoudani, Arash and Ajoudani, Arash and Zanchettin, Andrea Maria and Zanchettin, Andrea Maria and Ivaldi, Serena and Ivaldi, Serena and Albu-Schäffer, Alin and Albu-Schäffer, Alin and Kosuge, Kazuhiro and Kosuge, Kazuhiro and Khatib, Oussama and Khatib, Oussama},
keywords = {Engineering ; Control, Robotics, Mechatronics ; Human–robot interfaces ; Computer Imaging, Vision, Pattern Recognition and Graphics ; Progress and prospects ; Artificial Intelligence (incl. Robotics) ; Human–robot interaction ; Physical human robot collaboration ; Robotics and Automation ; Human-in-the-loop ; Robotics industry ; Electric power production ; Robots ; Robotics ; Resveratrol ; Electric power generation ; Systems stability ; State-of-the-art reviews ; Interface stability ; Robot control ; Collaboration ; Cooperation ; Control stability ; Economic impact ; Computer Science},
}



@InProceedings{10.1007/978-3-319-47437-3_74,
author="Foster, Mary Ellen
and Alami, Rachid
and Gestranius, Olli
and Lemon, Oliver
and Niemel{\"a}, Marketta
and Odobez, Jean-Marc
and Pandey, Amit Kumar",
editor="Agah, Arvin
and Cabibihan, John-John
and Howard, Ayanna M.
and Salichs, Miguel A.
and He, Hongsheng",
title="The MuMMER Project: Engaging Human-Robot Interaction in Real-World Public Spaces",
booktitle="Social Robotics",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="753--763",
abstract="MuMMER (MultiModal Mall Entertainment Robot) is a four-year, EU-funded project with the overall goal of developing a humanoid robot (SoftBank Robotics' Pepper robot being the primary robot platform) with the social intelligence to interact autonomously and naturally in the dynamic environments of a public shopping mall, providing an engaging and entertaining experience to the general public. Using co-design methods, we will work together with stakeholders including customers, retailers, and business managers to develop truly engaging robot behaviours. Crucially, our robot will exhibit behaviour that is socially appropriate and engaging by combining speech-based interaction with non-verbal communication and human-aware navigation. To support this behaviour, we will develop and integrate new methods from audiovisual scene processing, social-signal processing, high-level action selection, and human-aware robot navigation. Throughout the project, the robot will be regularly deployed in Ideapark, a large public shopping mall in Finland. This position paper describes the MuMMER project: its needs, the objectives, R{\&}D challenges and our approach. It will serve as reference for the robotics community and stakeholders about this ambitious project, demonstrating how a co-design approach can address some of the barriers and help in building follow-up projects.",
isbn="978-3-319-47437-3"
}

@article{sebo2020robots,
  title={Robots in groups and teams: a literature review},
  author={Sebo, Sarah and Stoll, Brett and Scassellati, Brian and Jung, Malte F},
  journal={Proceedings of the ACM on Human-Computer Interaction},
  volume={4},
  number={CSCW2},
  pages={1--36},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{moher2009preferred,
  title={Preferred reporting items for systematic reviews and meta-analyses: the PRISMA statement},
  author={Moher, David and Liberati, Alessandro and Tetzlaff, Jennifer and Altman, Douglas G and Prisma Group and others},
  journal={PLoS medicine},
  volume={6},
  number={7},
  pages={e1000097},
  year={2009},
  publisher={Public Library of Science}
}

@article{robla2017working,
  title={Working together: A review on safe human-robot collaboration in industrial environments},
  author={Robla-G{\'o}mez, Sandra and Becerra, Victor M and Llata, Jos{\'e} Ram{\'o}n and Gonzalez-Sarabia, Esther and Torre-Ferrero, Carlos and Perez-Oria, Juan},
  journal={IEEE Access},
  volume={5},
  pages={26754--26773},
  year={2017},
  publisher={IEEE}
}

@misc{recht2019imagenet,
      title={Do ImageNet Classifiers Generalize to ImageNet?}, 
      author={Benjamin Recht and Rebecca Roelofs and Ludwig Schmidt and Vaishaal Shankar},
      year={2019},
      eprint={1902.10811},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{belpaeme2018social,
  title={Social robots for education: A review},
  author={Belpaeme, Tony and Kennedy, James and Ramachandran, Aditi and Scassellati, Brian and Tanaka, Fumihide},
  journal={Science robotics},
  volume={3},
  number={21},
  year={2018},
  publisher={Science Robotics}
}

@Article{RobinsonSys,
author="Robinson, Nicole Lee
and Cottier, Timothy Vaughan
and Kavanagh, David John",
title="Psychosocial Health Interventions by Social Robots: Systematic Review of Randomized Controlled Trials",
journal="J Med Internet Res",
year="2019",
month="May",
day="10",
volume="21",
number="5",
pages="e13203",
keywords="social robot; healthcare; treatment; therapy; autism spectrum disorder; dementia",
abstract="Background: Social robots that can communicate and interact with people offer exciting opportunities for improved health care access and outcomes. However, evidence from randomized controlled trials (RCTs) on health or well-being outcomes has not yet been clearly synthesized across all health domains where social robots have been tested. Objective: This study aimed to undertake a systematic review examining current evidence from RCTs on the effects of psychosocial interventions by social robots on health or well-being. Methods: Medline, PsycInfo, ScienceDirect, Scopus, and Engineering Village searches across all years in the English language were conducted and supplemented by forward and backward searches. The included papers reported RCTs that assessed changes in health or well-being from interactions with a social robot across at least 2 measurement occasions. Results: Out of 408 extracted records, 27 trials met the inclusion criteria: 6 in child health or well-being, 9 in children with autism spectrum disorder, and 12 with older adults. No trials on adolescents, young adults, or other problem areas were identified, and no studies had interventions where robots spontaneously modified verbal responses based on speech by participants. Most trials were small (total N=5 to 415; median=34), only 6 (22{\%}) reported any follow-up outcomes (2 to 12 weeks; median=3.5) and a single-blind assessment was reported in 8 (31{\%}). More recent trials tended to have greater methodological quality. All papers reported some positive outcomes from robotic interventions, although most trials had some measures that showed no difference or favored alternate treatments. Conclusions: Controlled research on social robots is at an early stage, as is the current range of their applications to health care. Research on social robot interventions in clinical and health settings needs to transition from exploratory investigations to include large-scale controlled trials with sophisticated methodology, to increase confidence in their efficacy. ",
issn="1438-8871",
doi="10.2196/13203",
url="http://www.jmir.org/2019/5/e13203/",
url="https://doi.org/10.2196/13203",
url="http://www.ncbi.nlm.nih.gov/pubmed/31094357"
}

@article{VillaniValeria2018Sohc,
issn = {0957-4158},
abstract = {Easy-to-use collaborative robotics solutions, where human workers and robots share their skills, are entering the market, thus becoming the new frontier in industrial robotics. They allow to combine the advantages of robots, which enjoy high levels of accuracy, speed and repeatability, with the flexibility and cognitive skills of human workers. However, to achieve an efficient human–robot collaboration, several challenges need to be tackled. First, a safe interaction must be guaranteed to prevent harming humans having a direct contact with the moving robot. Additionally, to take full advantage of human skills, it is important that intuitive user interfaces are properly designed, so that human operators can easily program and interact with the robot. In this survey paper, an extensive review on human–robot collaboration in industrial environment is provided, with specific focus on issues related to physical and cognitive interaction. The commercially available solutions are also presented and the main industrial applications where collaborative robotic is advantageous are discussed, highlighting how collaborative solutions are intended to improve the efficiency of the system and which the open issue are.},
journal = {Mechatronics (Oxford)},
pages = {248--266},
volume = {55},
publisher = {Elsevier Ltd},
year = {2018},
title = {Survey on human–robot collaboration in industrial settings: Safety, intuitive interfaces and applications},
copyright = {2018 Elsevier Ltd},
language = {eng},
author = {Villani, Valeria and Pini, Fabio and Leali, Francesco and Secchi, Cristian},
keywords = {User interfaces ; Intuitive robot programming ; Industrial applications ; Safety ; Human–robot collaboration ; Collaborative robots ; Surveys ; Robotics industry ; Robots ; Robotics},
}





@article{doi:10.1177/00187208211037465,
author = {Wendy A. Rogers and Travis Kadylak and Megan A. Bayles},
title ={Maximizing the Benefits of Participatory Design for Human–Robot Interaction Research With Older Adults},
journal = {Human Factors},
volume = {0},
number = {0},
pages = {00187208211037465},
year = {0},
doi = {10.1177/00187208211037465},
    note ={PMID: 34461761},

URL = { 
        https://doi.org/10.1177/00187208211037465
    
},
eprint = { 
        https://doi.org/10.1177/00187208211037465
    
}
,
    abstract = { ObjectiveWe reviewed human–robot interaction (HRI) participatory design (PD) research with older adults. The goal was to identify methods used, determine their value for design of robots with older adults, and provide guidance for best practices.BackgroundAssistive robots may promote aging-in-place and quality of life for older adults. However, the robots must be designed to meet older adults’ specific needs and preferences. PD and other user-centered methods may be used to engage older adults in the robot development process to accommodate their needs and preferences and to assure usability of emergent assistive robots.MethodThis targeted review of HRI PD studies with older adults draws on a detailed review of 26 articles. Our assessment focused on the HRI methods and their utility for use with older adults who have a range of needs and capabilities.ResultsOur review highlighted the importance of using mixed methods and including multiple stakeholders throughout the design process. These approaches can encourage mutual learning (to improve design by developers and to increase acceptance by users). We identified key phases used in HRI PD workshops (e.g., initial interview phase, series of focus groups phase, and presentation phase). These approaches can provide inspiration for future efforts.ConclusionHRI PD strategies can support designers in developing assistive robots that meet older adults’ needs, capabilities, and preferences to promote acceptance. More HRI research is needed to understand potential implications for aging-in-place. PD methods provide a promising approach. }
}





@article{damacharla2018common,
  title={Common metrics to benchmark human-machine teams (HMT): A review},
  author={Damacharla, Praveen and Javaid, Ahmad Y and Gallimore, Jennie J and Devabhaktuni, Vijay K},
  journal={IEEE Access},
  volume={6},
  pages={38637--38655},
  year={2018},
  publisher={IEEE}
}

@article{VASCONEZ201935,
title = "Human–robot interaction in agriculture: A survey and current challenges",
journal = "Biosystems Engineering",
volume = "179",
pages = "35 - 48",
year = "2019",
issn = "1537-5110",
doi = "https://doi.org/10.1016/j.biosystemseng.2018.12.005",
url = "http://www.sciencedirect.com/science/article/pii/S1537511017309625",
author = "Juan P. Vasconez and George A. Kantor and Fernando A. {Auat Cheein}",
keywords = "Human–robot interaction, Agriculture robotics, Collaborative robotics, Precision agriculture",
abstract = "Human–robot interaction (HRI) is an extensive and diverse research topic that has been gaining importance in last years. Different fields of study have used HRI approaches for solving complicated problems, where humans and robots interact in some way to obtain advantages from their collaboration. Many industrial areas benefit by applying HRI strategies in their applications, and agriculture is one of the most challenging of them. Currently, field crops can reach highly autonomous levels whereas speciality crops do not. In particular, crops such as fruits and vegetables are still harvested manually, and also some tasks such as pruning and thinning have long been considered to be too complex to automate completely. In addition, several countries face the problem of farm labour shortages. As a consequence, the production process is affected. In this context, we survey HRI approaches and ap-plications focused on improving the working conditions, agility, efficiency, safety, productivity and profitability of agricultural processes, in cases where manual labour cannot be replaced by but can be complemented with robots."
}

@article{bauer2008human,
  title={Human--robot collaboration: a survey},
  author={Bauer, Andrea and Wollherr, Dirk and Buss, Martin},
  journal={International Journal of Humanoid Robotics},
  volume={5},
  number={01},
  pages={47--66},
  year={2008},
  publisher={World Scientific}
}

@InProceedings{10.1007/978-3-030-28619-4_14,
author="Schmidt, Tanner
and Fox, Dieter",
editor="Amato, Nancy M.
and Hager, Greg
and Thomas, Shawna
and Torres-Torriti, Miguel",
title="Self-directed Lifelong Learning for Robot Vision",
booktitle="Robotics Research",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="109--114",
abstract="Efforts towards robust visual scene understanding tend to rely heavily on manual annotations. When human labels are required, collecting a dataset large enough to train a successful robot vision system is almost certain to be prohibitively expensive. However, we argue that a robot with a vision sensor can learn powerful visual representations in a self-directed manner by relying on fundamental physical priors and bootstrapping techniques. For example, it has been shown that basic visual tracking systems can be used to automatically label short-range correspondences in video that allow one to train a system with capabilities analogous to object permanence in humans. An object permanence system can in turn be used to automatically label long-range correspondences, allowing one to train a system able to compare and contrast objects and scenes. In the end, the agent will develop a representation that encodes persistent material properties, state, lighting, etc. of various parts of a visual scene. Starting with a strong visual representation, the agent can then learn to solve traditional vision tasks such as class and/or instance recognition using only a sparse set of labels that can be found on the Internet or solicited at little cost from humans. More importantly, such a representation would also enable truly robust solutions to challenges in robotics such as global localization, loop closure detection, and object pose estimation.",
isbn="978-3-030-28619-4"
}



@INPROCEEDINGS{5508131,
  author={Liu, Yu-chi and Dai, Qiong-hai},
  booktitle={2010 International Conference on Optics, Photonics and Energy Engineering (OPEE)}, 
  title={A survey of computer vision applied in Aerial robotic Vehicles}, 
  year={2010},
  volume={1},
  number={},
  pages={277-280},
  doi={10.1109/OPEE.2010.5508131}}

@article{liu_gesture_2018,
	title = {Gesture recognition for human-robot collaboration: {A} review},
	volume = {68},
	issn = {0169-8141},
	url = {http://www.sciencedirect.com/science/article/pii/S0169814117300690},
	doi = {https://doi.org/10.1016/j.ergon.2017.02.004},
	abstract = {Recently, the concept of human-robot collaboration has raised many research interests. Instead of robots replacing human workers in workplaces, human-robot collaboration allows human workers and robots working together in a shared manufacturing environment. Human-robot collaboration can release human workers from heavy tasks with assistive robots if effective communication channels between humans and robots are established. Although the communication channels between human workers and robots are still limited, gesture recognition has been effectively applied as the interface between humans and computers for long time. Covering some of the most important technologies and algorithms of gesture recognition, this paper is intended to provide an overview of the gesture recognition research and explore the possibility to apply gesture recognition in human-robot collaborative manufacturing. In this paper, an overall model of gesture recognition for human-robot collaboration is also proposed. There are four essential technical components in the model of gesture recognition for human-robot collaboration: sensor technologies, gesture identification, gesture tracking and gesture classification. Reviewed approaches are classified according to the four essential technical components. Statistical analysis is also presented after technical analysis. Towards the end of this paper, future research trends are outlined.},
	journal = {International Journal of Industrial Ergonomics},
	author = {Liu, Hongyi and Wang, Lihui},
	year = {2018},
	keywords = {Gesture, Gesture recognition, Human-robot collaboration},
	pages = {355 -- 367}
}

@inproceedings{katsuki_high_speed_2015,
	address = {New York, NY, USA},
	series = {{HRI}'15 {Extended} {Abstracts}},
	title = {High-{Speed} {Human} / {Robot} {Hand} {Interaction} {System}},
	isbn = {978-1-4503-3318-4},
	url = {https://doi-org.ezp01.library.qut.edu.au/10.1145/2701973.2701984},
	doi = {10.1145/2701973.2701984},
	abstract = {We propose an entirely new human hand / robot hand interaction system designed with a focus on high speed. The speed of this system, from input via a high-speed vision system to output by a high-speed multifingered robot hand, exceeds the visual recognition speed of humans. Therefore, the motion of the interaction system cannot be recognized by the human eye. As an application, we created a system called “Rock-Paper-Scissors robot system with 100\% winning rate', based on this interaction system. This system always beats human players in the Rock-Paper-Scissors game due to the high speed of our interaction system.We also discuss the future possibilities of this system.},
	booktitle = {Proceedings of the {Tenth} {Annual} {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} {Extended} {Abstracts}},
	publisher = {Association for Computing Machinery},
	author = {Katsuki, Yugo and Yamakawa, Yuji and Ishikawa, Masatoshi},
	year = {2015},
	note = {event-place: Portland, Oregon, USA},
	keywords = {high-speed multifingered robot hand, high-speed vision},
	pages = {117--118}
}

@inproceedings{xia_vision-based_2019,
	title = {Vision-{Based} {Hand} {Gesture} {Recognition} for {Human}-{Robot} {Collaboration}: {A} {Survey}},
	doi = {10.1109/ICCAR.2019.8813509},
	booktitle = {2019 5th {International} {Conference} on {Control}, {Automation} and {Robotics} ({ICCAR})},
	author = {Xia, Z. and Lei, Q. and Yang, Y. and Zhang, H. and He, Y. and Wang, W. and Huang, M.},
	year = {2019},
	pages = {198--205}
}


@article{mitra_gesture_2007,
	title = {Gesture {Recognition}: {A} {Survey}},
	volume = {37},
	issn = {1558-2442},
	shorttitle = {Gesture {Recognition}},
	doi = {10.1109/TSMCC.2007.893280},
	abstract = {Gesture recognition pertains to recognizing meaningful expressions of motion by a human, involving the hands, arms, face, head, and/or body. It is of utmost importance in designing an intelligent and efficient human-computer interface. The applications of gesture recognition are manifold, ranging from sign language through medical rehabilitation to virtual reality. In this paper, we provide a survey on gesture recognition with particular emphasis on hand gestures and facial expressions. Applications involving hidden Markov models, particle filtering and condensation, finite-state machines, optical flow, skin color, and connectionist models are discussed in detail. Existing challenges and future research possibilities are also highlighted},
	number = {3},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Mitra, S. and Acharya, T.},
	month = may,
	year = {2007},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	keywords = {Arm, condensation, connectionist models, face recognition, Face recognition, facial expressions, Filtering, finite state machines, finite-state machines, gesture recognition, hand gestures, Handicapped aids, hidden Markov models, Hidden Markov models, hidden Markov models (HMMs), human computer interaction, Humans, image colour analysis, image sequences, intelligent human-computer interface, Magnetic heads, Manifolds, Optical filters, optical flow, particle filtering, skin color, soft computing, Virtual reality},
	pages = {311--324},
	file = {IEEE Xplore Full Text PDF:/home/brendan/Zotero/storage/K8WD93V5/Mitra and Acharya - 2007 - Gesture Recognition A Survey.pdf:application/pdf;IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/KMALRDKN/4154947.html:text/html}
}

@inproceedings{dallel_inhard_2020,
	title = {{InHARD} - {Industrial} {Human} {Action} {Recognition} {Dataset} in the {Context} of {Industrial} {Collaborative} {Robotics}},
	doi = {10.1109/ICHMS49158.2020.9209531},
	booktitle = {2020 {IEEE} {International} {Conference} on {Human}-{Machine} {Systems} ({ICHMS})},
	author = {DALLEL, M. and HAVARD, V. and BAUDRY, D. and SAVATIER, X.},
	year = {2020},
	pages = {1--6}
}

@inproceedings{cho_human_2017,
	title = {Human gesture recognition performance evaluation for service robots},
	doi = {10.23919/ICACT.2017.7890213},
	booktitle = {2017 19th {International} {Conference} on {Advanced} {Communication} {Technology} ({ICACT})},
	author = {Cho, M. and Jeong, Y.},
	year = {2017},
	pages = {847--851}
}

@inproceedings{codd-downey_human_2019,
	title = {Human {Robot} {Interaction} {Using} {Diver} {Hand} {Signals}},
	doi = {10.1109/HRI.2019.8673133},
	booktitle = {2019 14th {ACM}/{IEEE} {International} {Conference} on {Human}-{Robot} {Interaction} ({HRI})},
	author = {Codd-Downey, R. and Jenkin, M.},
	year = {2019},
	pages = {550--551}
}

@article{gori_multitype_2016,
	title = {Multitype {Activity} {Recognition} in {Robot}-{Centric} {Scenarios}},
	volume = {1},
	issn = {2377-3766},
	doi = {10.1109/LRA.2016.2525002},
	abstract = {Activity recognition is very useful in scenarios where robots interact with, monitor, or assist humans. In the past years many types of activities—single actions, two persons interactions or ego-centric activities, to name a few—have been analyzed. Whereas traditional methods treat such types of activities separately, an autonomous robot should be able to detect and recognize multiple types of activities to effectively fulfill its tasks. We propose a method that is intrinsically able to detect and recognize activities of different types that happen in sequence or concurrently. We present a new unified descriptor, called relation history image (RHI), which can be extracted from all the activity types we are interested in. We then formulate an optimization procedure to detect and recognize activities of different types. We apply our approach to a new dataset recorded from a robot-centric perspective and systematically evaluate its quality compared to multiple baselines. Finally, we show the efficacy of the RHI descriptor on publicly available datasets performing extensive comparisons.},
	number = {1},
	journal = {IEEE Robotics and Automation Letters},
	author = {Gori, I. and Aggarwal, J. K. and Matthies, L. and Ryoo, M. S.},
	month = jan,
	year = {2016},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Activity recognition, Computer vision, Feature extraction, Gesture, Human-robot interaction, Optimization, Physical Human-Robot Interaction, Recognition, Visual Learning},
	pages = {593--600},
	file = {IEEE Xplore Full Text PDF:/home/brendan/Zotero/storage/RFAHH2ZW/Gori et al. - 2016 - Multitype Activity Recognition in Robot-Centric Sc.pdf:application/pdf}
}

@inproceedings{shukla_probabilistic_2015,
	title = {Probabilistic {Detection} of {Pointing} {Directions} for {Human}-{Robot} {Interaction}},
	doi = {10.1109/DICTA.2015.7371296},
	booktitle = {2015 {International} {Conference} on {Digital} {Image} {Computing}: {Techniques} and {Applications} ({DICTA})},
	author = {Shukla, D. and Erkent, O. and Piater, J.},
	year = {2015},
	pages = {1--8}
}


@inproceedings{shukla_supervised_2017,
	title = {Supervised {Learning} of {Gesture}-{Action} {Associations} for {Human}-{Robot} {Collaboration}},
	doi = {10.1109/FG.2017.97},
	booktitle = {2017 12th {IEEE} {International} {Conference} on {Automatic} {Face} {Gesture} {Recognition} ({FG} 2017)},
	author = {Shukla, D. and Erkent, O. and Piater, J.},
	year = {2017},
	pages = {778--783}
}

@inproceedings{shukla_proactive_2017,
	title = {Proactive, incremental learning of gesture-action associations for human-robot collaboration},
	doi = {10.1109/ROMAN.2017.8172325},
	booktitle = {2017 26th {IEEE} {International} {Symposium} on {Robot} and {Human} {Interactive} {Communication} ({RO}-{MAN})},
	author = {Shukla, D. and Erkent, O and Piater, J.},
	year = {2017},
	pages = {346--353}
}

@inproceedings{mead_proxemics_2015,
	title = {Proxemics and performance: {Subjective} human evaluations of autonomous sociable robot distance and social signal understanding},
	doi = {10.1109/IROS.2015.7354229},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Mead, R. and Matarić, M. J.},
	year = {2015},
	pages = {5984--5991}
}

@article{sirithunge_proactive_2019,
	title = {Proactive {Robots} {With} the {Perception} of {Nonverbal} {Human} {Behavior}: {A} {Review}},
	volume = {7},
	doi = {10.1109/ACCESS.2019.2921986},
	journal = {IEEE Access},
	author = {Sirithunge, C. and Jayasekara, A. G. B. P. and Chandima, D. P.},
	year = {2019},
	pages = {77308--77327}
}

@inproceedings{tsiami_multi3_2018,
	title = {Multi3: {Multi}-{Sensory} {Perception} {System} for {Multi}-{Modal} {Child} {Interaction} with {Multiple} {Robots}},
	shorttitle = {Multi3},
	doi = {10.1109/ICRA.2018.8461210},
	abstract = {Child-robot interaction is an interdisciplinary research area that has been attracting growing interest, primarily focusing on edutainment applications. A crucial factor to the successful deployment and wide adoption of such applications remains the robust perception of the child's multi-modal actions, when interacting with the robot in a natural and untethered fashion. Since robotic sensory and perception capabilities are platform-dependent and most often rather limited, we propose a multiple Kinect-based system to perceive the child-robot interaction scene that is robot-independent and suitable for indoors interaction scenarios. The audio-visual input from the Kinect sensors is fed into speech, gesture, and action recognition modules, appropriately developed in this paper to address the challenging nature of child-robot interaction. For this purpose, data from multiple children are collected and used for module training or adaptation. Further, information from the multiple sensors is fused to enhance module performance. The perception system is integrated in a modular multi-robot architecture demonstrating its flexibility and scalability with different robotic platforms. The whole system, called Multi3, is evaluated, both objectively at the module level and subjectively in its entirety, under appropriate child-robot interaction scenarios containing several carefully designed games between children and robots.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Tsiami, A. and Koutras, P. and Efthymiou, N. and Filntisis, P. P. and Potamianos, G. and Maragos, P.},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {control engineering computing, robot vision, mobile robots, Trajectory, Robot sensing systems, human-robot interaction, multi-robot systems, gesture recognition, action recognition modules, child-robot interaction scenarios, child-robot interaction scene, computer games, educational robots, gesture recognition modules, indoors interaction scenarios, Microphone arrays, modular multirobot architecture, Multi3, Multimodal child interaction, multiple Kinect-based system, multiple robots, Multisensory perception system, perception capabilities, robotic platforms, robotic sensory, speech recognition, Speech recognition, speech recognition modules},
	pages = {4585--4592},
	annote = {Task: Speech, gesture and action recognition of a child
Robot: Furhat, Nao, Zeno,
Vision: Multiple kinects, with audio?
What: Robotic perception of action and speech recognition developed for a child
experiments: 28 children 6-10 years old
Real time:
Limitations:
Future:
 
 
 
Task:
Robot:
Vision:
What:
Experiments:
Real time:
Limitations:
Future:
 },
	file = {IEEE Xplore Full Text PDF:/home/brendan/Zotero/storage/5NDIT2W8/Tsiami et al. - 2018 - Multi3 Multi-Sensory Perception System for Multi-.pdf:application/pdf;IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/TQUXQ69P/8461210.html:text/html}
}

@inproceedings{admoni_modeling_2016,
	title = {Modeling communicative behaviors for object references in human-robot interaction},
	doi = {10.1109/ICRA.2016.7487510},
	abstract = {This paper presents a model that uses a robot's verbal and nonverbal behaviors to successfully communicate object references to a human partner. This model, which is informed by computer vision, human-robot interaction, and cognitive psychology, simulates how low-level and high-level features of the scene might draw a user's attention. It then selects the most appropriate robot behavior that maximizes the likelihood that a user will understand the correct object reference while minimizing the cost of the behavior. We present a general computational framework for this model, then describe a specific implementation in a human-robot collaboration. Finally, we analyze the model's performance in two human evaluations-one video-based (75 participants) and one in person (20 participants)-and demonstrate that the system predicts the correct behaviors to perform successful object references.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Admoni, H. and Weng, T. and Scassellati, B.},
	month = may,
	year = {2016},
	keywords = {robot vision, humanoid robots, Robot sensing systems, human-robot interaction, Robot kinematics, Computational modeling, human-robot collaboration, feature extraction, computer vision, object detection, Visualization, cognitive psychology, communicative behavior modeling, Context, high-level scene features, human evaluations, human partner, likelihood maximization, low-level scene features, Mathematical model, maximum likelihood estimation, robot nonverbal behaviors, robot verbal behaviors, user attention},
	pages = {3352--3359},
	annote = {Task: Furniture assembly
Robot: Nao humanoid (though robot agnostic).
Vision used to identify low level salient features. MS kinect
What: robot guides (gaze and pointing) the user toward next target object, that is the most effective, least expensive behaviour
Limitations:
Doesn't operate in real time due to computational requirements of the perception (about 10 seconds)
Future:
More natural and efficient
 
 },
	file = {IEEE Xplore Full Text PDF:/home/brendan/Zotero/storage/IHWUUFVU/Admoni et al. - 2016 - Modeling communicative behaviors for object refere.pdf:application/pdf;IEEE Xplore Abstract Record:/home/brendan/Zotero/storage/9QPBCS4I/7487510.html:text/html}
}