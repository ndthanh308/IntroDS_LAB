
In this section, we evaluate the performance of our scene completion method on a single view RGB-D image and on the out-of-training distribution dataset. We also report ablation studies for understanding the dependence of our method on (1) prompt specificity, (2) viewpoint angles, and (3) consistency filtering.



\subsection{Implementation Details}
The inpainting step of our algorithm is based on OpenAI's \dalle{} API.
% Our inpainting algorithm is done by utilizing OpenAI's API for \dalle{}.
For our implementation of SAM, we choose a $c$ value of 0.01 meters with $m$ = 100 points.  In our method, for choosing new viewpoints we use a $\theta$ value of 20$^\circ$.  For our consistency between viewpoints method, we choose a threshold of 0.01 meters when computing the intersection between points in the viewpoints point clouds.  More information about our implementation can be found in our supplementary materials.

\subsection{Datasets}
We trained our depth completion model using the YCB-V training dataset \cite{xiang2018posecnn}.  For testing, we test on $8$ unseen scenes from the YCB-V test set, and select $5$ RGB-D images from each of the scenes, i.e., we test on a total of $40$ RGB-D images total across $8$ unseen scenes. For ground truth point clouds, and deproject the RGB-D frames of the scene and concatenate them together.  We also place the ground truth meshes in the scene for the objects and convert those to point clouds before concatenating them as well.  This creates a large point cloud covering the majority of the scene with full geometry of the objects in the scene.

To demonstrate our model's capabilities of generalizing to unseen objects and to entirely new datasets, we also compare our method on the HOPE dataset \cite{tyree2022hope}.  HOPE test set only contains individual RGB-D images and is un-usable for generating full scene point cloud. Instead, we use HOPE training dataset for evaluation, as the train set contains RGB-D video and cluttered tabletop scenes with novel objects. The dataset has 10 scenes, and we again sample 5 frames per scene for a total of 50 RGB-D test images.  Ground truth point clouds are obtained in the same manner as the YCB-V dataset.

\subsection{Metrics}
We evaluate our method on standard 3D reconstruction metrics. \textbf{Intersection-over-Union (IoU)}:

We voxelize the ground truth and predicted point clouds at a fixed resolution and compute the IoU score by dividing the number of voxels that intersect to that of their union. In our experiments, we evaluate all the methods at the same grid resolution of $100^3$ after rescaling the predictions and ground truth to fit into the unit cube. \textbf{Chamfer Distance (CD)}: Chamfer distance is commonly used to measure the similarity between two point sets and is defined as:\vspace{-3mm}
\begin{equation}
    CD(X, Y) = \frac{1}{|X|} \sum_{\mathbf{x} \in X} \min_{\mathbf{y} \in Y} ||x - y||_2
\end{equation}
We separately report $CD(S, S^*)$ and $CD(S^*, S)$, as well as their their sum. $CD(S, S^*)$ measures how close the reconstructed points from $S$ are to the ground truth points $S^*$, whereas $CD(S^*, S)$ computes how well the ground truth shape is covered. \textbf{F-Score}: Following ~\cite{tatarchenko2019single}, we also report F-Score$@1\%$ which is a measure for the percentage of the surface points that were reconstructed correctly.


% \subsection{Comparison with State of the Art}
\subsection{Baselines}
We compare our novel scene completion method against four baselines: \textbf{Convolutional Occupancy Networks (CON)} ~\cite{peng2020convolutional} is a 3D scene reconstruction method that inputs a sparse point cloud of a scene for reconstruction.  We use the pretrained model for their \textit{Synthetic Indoor Scene dataset} where similar to our YCBV and HOPE datasets, they place multiple ShapeNet ~\cite{shapenet2015} objects in indoor scenes. \textbf{CoReNet} ~\cite{popov2020corenet} is a multi-object shape estimator that inputs an RGB image and estimates a mesh.  We compare against CoReNet's pretrained model qualitatively, as CoReNet's predictions lack scale information.  \textbf{ShellNet} ~\cite{chavan2022simultaneous} is a shape completion method trained on single objects. ShellNet inputs scene depth image and object instance mask and produces reconstruction for the object instance. We re-implemented ShellNet's architecture and trained it with Mask R-CNN~\cite{DBLP:journals/corr/HeGDG17} as the segmentation network on YCB-V dataset.
% ShellNet input a mask of the object and the depth map of the scene. We reimplemented ShellNet's architecture and trained it with Mask R-CNN~\cite{DBLP:journals/corr/HeGDG17} as the segmentation network on YCB objects.
Finally, we compare against \textbf{CenterSnap} ~\cite{irshad2022centersnap}, a multi-object point cloud prediction method. CenterSnap inputs an RGB-D image and predicts point clouds for each object in the scene. Similar to CenterSnap's original training method, we first train it on YCB-V synthetic dataset \cite{denninger2020blenderproc, hodan2020bop}, then fine-tune it on the YCB-V real training dataset \cite{xiang2018posecnn}.  Since CenterSnap and ShellNet only predict the point clouds for objects and not the rest of the scene, for a fair comparison, we concatenate the input RGB-D image deprojected point cloud to each of the outputs before evaluating.



\begin{table}[t]
\begin{adjustbox}{width=\columnwidth,center}
\centering
\begin{tabular}{l | c c c c c}
\toprule
 \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
\toprule
\multicolumn{6}{c}{YCB-V~\cite{xiang2018posecnn}} \\
\midrule
 CON~\cite{peng2020convolutional} & 0.15 & 0.399 & 0.102 & 0.027 & 0.129  \\ 
 ShellNet~\cite{chavan2022simultaneous} & 0.213 & 0.618 & 0.028 & 0.022 & 0.05 \\
 CenterSnap~\cite{irshad2022centersnap} & 0.255 & 0.683 & 0.030 & \textbf{0.003} & \textbf{0.033} \\
 \ours{} (Ours) & \textbf{0.284} & \textbf{0.688} & \textbf{0.025} & 0.011 & 0.036  \\
\midrule
\multicolumn{6}{c}{HOPE~\cite{lin2021fusion}} \\
\midrule
CON~\cite{peng2020convolutional} & 0.114 & 0.312 & 0.103 & 0.03 & 0.133  \\ 
 ShellNet~\cite{chavan2022simultaneous} & 0.181 & 0.547 & 0.04 & 0.03 & 0.07  \\
 CenterSnap~\cite{irshad2022centersnap} & 0.211  & 0.617 & 0.046 & \textbf{0.004} & 0.050 \\
 \ours{} (Ours) & \textbf{0.298} & \textbf{0.702} & \textbf{0.039} & \textbf{0.004} & \textbf{0.043}  \\
\bottomrule
\end{tabular}
\end{adjustbox}
 \caption{Comparison of methods for the task of 3D scene completion on the YCB-V~\cite{xiang2018posecnn} and HOPE~\cite{lin2021fusion} datasets. Higher numbers for the IoU and F-score metrics, and lower numbers for the Chamfer Distances (CD) indicate better performance.}
    \label{tab:sota-comparison}
\end{table}

% \subsubsection{Qualitative Results}
% Qualitative results
\subsection{Results}
Table~\ref{tab:sota-comparison} shows quantitative evaluations on within-training-distribution YCB-V dataset \cite{xiang2018posecnn} and out-of-training-distribution HOPE dataset \cite{tyree2022hope}. On YCB-V dataset, \ours{} is able to outperform CON and ShellNet on all 3D scene reconstruction metrics. CON takes as input a sparse point-cloud of the scene. When major parts of the input point clouds are missing, as the common case for single-view RGB-D point clouds, CON fails to infer those regions. ShellNet is trained to predict back-side depth image for the detected object. We notice that with varying viewing directions, ShellNet backside depths are either too thin or too thick resulting in low performance. MaskRCNN's failure to detect objects also directly contributed to lower performance for ShellNet. CenterSnap inputs RGB-D image and predicts object shapes via a three-step procedure: (a) detect objects in the image, for each detected object, (b) predict canonical frame point-cloud, and (c) camera frame pose and camera frame scale. This procedure allows CenterSnap to learn strong shape and pose priors for objects within training distribution. Moreover, supervision from ground-truth object poses allowed CenterSnap to perform well within train distribution. \ours{} which is trained without ground-truth object pose or shape supervision is able to perform comparatively with CenterSnap in the overall CD metric.

On out-of-distribution HOPE dataset, \ours{} is able to outperform all baselines. This shows that our normal and occlusion boundary based depth completion method generalizes well to unseen novel scenes. Figure \ref{fig:qualitative-geometry} shows qualitative results for scene completion on the HOPE dataset.


As a byproduct, our method also produces novel views of unseen multi-object scenes from a single RGB-D image. Figure~\ref{fig:qualitative-inpainting} shows our method compared to the ground truth. We show that by combining our masking method with \dalle{}'s inpainting capability, realistic novels views can be generated for multiple unseen objects.

% Figure environment removed

% Figure environment removed




% Figure environment removed



\subsection{Ablation Studies}
\label{sec:ablation}
\textbf{Prompt Specificity}: In our method, we used “household objects on a table” as a generic prompt for all scenes. To investigate whether the performance can be improved by using more specific prompts, we compare our method against a scene specific prompt. Specifically, for a scene containing $n$ objects with object labels $obj_{i}$, we create the prompt as ``a $obj_1$, $obj_{2}$ $\cdots$ $obj_{n-1}$, and $obj_{n}$ on a table''.  This list of objects is limited to the first 10 objects for larger scenes. We call this ablation  \ours{} (S). Table~\ref{tab:prompt} shows that our method does not depend on specific prompt. While image diffusion models generally heavily depend on the input prompt for creating images, we hypothesize that our solution where we maximally retain the information present in the input RGB image, provides enough surrounding context and hence, does not need a very specific prompt.



% Ablation Experiments
\begin{table}[h!]
\centering
 \begin{tabular}{l | c c c} 
 \hline
 \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD $\downarrow$ \\
 \hline
 \ours{} (S) & \textbf{0.298} & \textbf{0.702} & \textbf{0.042}  \\
 \ours{} & \textbf{0.298} & \textbf{0.702} & 0.043  \\
 \hline
 \end{tabular}
 \caption{Prompt specificity  results  on  HOPE  dataset \cite{tyree2022hope}: \ours{}(S) denotes  our  model  with  scene  specific  prompt, as compared to \ours{} which inputs the  general  prompt - ``household objects on a table" for all scenes.}
 \label{tab:prompt}
\end{table}



\textbf{Viewpoint Angles}: Here, we ablate our method for different values for the angle of rotation between viewpoints \(\theta\) (Section~\ref{sec:pcl_rotation}). We experimented with \(\theta\) values of 10, 20, and 30 degrees and chose the best performing \(\theta\) of 20 degrees as our final \(\theta\). Note that even with the larger \(\theta\) of 30 degrees, our method's performance does not change a lot.


\begin{table}[h!]
\begin{adjustbox}{width=\columnwidth,center}
\centering
 \begin{tabular}{l | c c c c c} 
 \hline
 \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
 \hline
 \ours{} (10) & 0.296  & 0.698 & \textbf{0.038} & 0.006 & 0.044  \\
 \ours{} (20) & \textbf{0.298} & \textbf{0.702} & 0.039 & 0.004 & \textbf{0.043}  \\
  \ours{} (30) & 0.296 & 0.699 & 0.04 & \textbf{0.003}  & \textbf{0.043}  \\
 \hline
 \end{tabular}
  \end{adjustbox}
 \caption{Viewpoint angles study on the HOPE~\cite{lin2021fusion} dataset. 10, 20, and 30 denote the angle of rotation \(\theta\) of the point cloud between inpainting steps as described in Section~\ref{sec:pcl_rotation}}
\end{table}

\textbf{Consistency Filtering}: Table~\ref{tab:consistency} shows results for our method without consistency filtering (Section \ref{sec:consistency}). \ours{} (No Filter) has lower performance compared to \ours{} as \dalle{} does not produce consistent in-painting results across different views and often produce objects which are not in the scene. \ours{} with consistency filtering is able to aggregate coherent information from multiple inpainted views.

\begin{table}[h!]
\begin{adjustbox}{width=\columnwidth,center}
\centering
 \begin{tabular}{l | c c c c c} 
 \hline
 \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
 \hline
 \ours{} (No Filter) & 0.290 & 0.667 & \textbf{0.032} & 0.015 & 0.048 \\
  \ours{} (Ours) & \textbf{0.298} & \textbf{0.702} & 0.039 & \textbf{0.004} & \textbf{0.043}  \\
 \hline
 \end{tabular}
 \end{adjustbox}
 \caption{Consistency filtering results on the HOPE~\cite{lin2021fusion} dataset. \ours{} (No Filter) refers to reconstructions without any consistency filtering (Section~\ref{sec:consistency}).}
\label{tab:consistency}
\end{table}

\textbf{Surface-Aware Masking}: Figure~\ref{fig:sam_fig} qualitatively reports the dependence of our method on Surface-Aware Masking step (Section~\ref{sec:bg-mask}). We observe that without SAM, the \dalle{}'s inpainting algorithm confuses background in rotated point cloud as foreground and produces unrealistic artifacts.

