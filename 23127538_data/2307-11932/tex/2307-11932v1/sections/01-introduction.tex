
% Figure environment removed



The understanding of 3D scene geometry is essential for many down-stream applications.  In robotics, it allows for accurate manipulation and motion planning considering the surrounding environment.  In the field of augmented reality, it allows for better mapping and rendering to bridge the virtual world to the real world.  With smartphones and robots that are equipped with high quality depth sensors, the task of 3D scene reconstruction is becoming feasible in these domains. 
%
These depth sensors allow for accurate reconstruction of the observed parts of the scene. However, to reconstruct the unseen parts, we must use prior information conditioned on the observed information. The missing information in the input image combined with the diversity in shapes, sizes, and depth distribution of the household objects presents a major challenge for scene reconstruction in-the-wild. 
%
In this paper, we study this problem in a general setting, where the goal is to reconstruct a complex scene with multiple novel objects, given only one RGB-D image of the scene.  
%



We present our method Rotate-Inpaint-Complete (\ours{}), which predicts both the 3D geometry and the texture of the unseen parts of the scene in the input image by leveraging the inpainting capabilities of large visual-language models.
%
Given an RGB-D image of a scene, first we generate novel views (RGB and depth images) by rotating and then projecting the input scene. Then we use a surface-aware masking method to select regions in the image to allow us to inpaint utilizing the powerful 2D inpainting capabilities of \dalle{}~\cite{ramesh2022hierarchical} for exposing the potential object geometry not visible in the input image. 
%
Finally, we optimize the depth images using the input depth values and occlusion boundaries and normals estimated from the inpainted images. These inpainted and completed novel RGB-D views provide the reconstructed scene geometry as a fused pointcloud with associated textures.
To mitigate the object hallucination and spatial inconsistency of predictions from \dalle{}, we integrate algorithmic features such as filtering inpainting outputs and enforcing consistency across viewpoints into our method that play a crucial role for generalizable, yet accurate and robust scene reconstruction.

%-------------------------------------------------------

We demonstrate our method on cluttered scenes with unseen household objects and categories. Through a series of rigorous quantitative experiments, we show that our approach outperforms baseline methods in settings where no training data is available.

% \subsection{Statement of Contributions}
In short, the contributions of this paper can be summarized as follows. \textit{i)} We present an integrated approach for scene completion of unseen objects under occlusion and clutter, by solving the problem through novel view inpainting and 2D to 3D scene lifting. \textit{ii)} We develop a method for selectively inpainting regions in the novel views of the input scene that enables synthesis of consistent 2D geometry. \textit{iii)} We train a 2D to 3D lifting method on the YCB-V~\cite{xiang2018posecnn} dataset and demonstrate the generalization capability on novel household objects and categories which is crucial for maintaining the generalization capability of our integrated scene reconstruction method.

