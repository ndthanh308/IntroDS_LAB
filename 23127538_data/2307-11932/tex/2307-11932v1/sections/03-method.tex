


% Figure environment removed


% Method overview

In this section, we introduce our method Rotate-Inpaint-Complete, or \ours{}, to generate generalizable reconstructions of 3D scenes containing multiple objects, given a single RGB-D image of the scene.

\ours{} takes in as input an RGB-D image \(\mathcal{I} = (\mathbf{I}, \mathbf{D}) \in \mathbb{R} ^{ H \times W \times 4} \) and outputs a color point cloud \(S \in \mathbb{R} ^{ N \times (3 + 3)} \), where $N$ is the number of predicted points in the scene.
%
Our method consists of three main components: 1) An inpainting step that takes in an RGB-D image \(\mathcal{I}\) and outputs an inpainted RGB image \(\hat{\mathbf{I}}_i\) from a novel viewpoint $\mathbf{T}_i \in SE(3)$. 2) A depth completion component that takes in the inpainted RGB image \(\hat{\mathbf{I}}_i\) as well as an incomplete depth image \(\bar{\mathbf{D}}_i \) rendered from the viewpoint $\mathbf{T}_i$, and outputs a completed depth \(\hat{\mathbf{D}}_i\) at that viewpoint. 3) An iterative method that utilizes the two other components to generate completed RGB-D images at rotated novel views and used them to reconstruct the scene. The following subsections will explain each of these components in detail.



\subsection{Inpainting}

This section describes the inpainting process, as well as the intermediate steps taken before and after to go from the input image \(\mathbf{I}\) to \(\hat{\mathbf{I}}_i\) at a novel viewpoint $\mathbf{T}_i$.

\subsubsection{Rotate and Project RGB-D Image}
Given an RGB-D image of a scene and the camera intrinsics, we deproject the image into a point cloud in the camera frame.  This point cloud is then rotated by an angle \(\theta\) about its center point and rendered at the novel view following masking procedure, which we describe in detail in Section~\ref{sec:bg-mask}. The camera projection of the rotated point cloud creates a new RGB-D image $\bar{\mathbf{I}}_i$ with both missing RGB and depth information as seen in Figure~\ref{fig:method-overview}. Small holes of the missing RGB values are filled with a naive inpainting algorithm ~\cite{telea2004image}.



% \subsubsection{Background Mask Generation}
\subsubsection{Surface-Aware Masking}
\label{sec:bg-mask}

% Figure environment removed

In order for inpainting to work properly, a mask covering the areas to inpaint needs to be generated.  After rotating and projecting to the new camera frame, any 3D space possible to be reconstructed needs to be represented as inpainting mask in the 2D image.  To solve this problem, a 3D frustum is generated from the original camera and depth image.  For every pixel in the original camera frame, a ray is cast from the camera through each point in the projected point cloud from \(\mathcal{I}\).  Once the ray has passed through its respective point, it is used to generate a list of points along the ray from that depth onward with equal spacing $c$.  This is done for every ray, and from this process results a point cloud covering the potential space that the 3D scene could possibly fill.  This point cloud is then converted to a mesh, and when the point cloud from the RGB-D image \(\mathcal{I}\) is rotated to novel views, the mesh is rotated with it.  Finally, when projecting back to the camera frame after rotation, points that are occluded by the mesh are discarded.  Any blank pixels are then used as the 2D inpainting mask to be filled when passed to the inpainting step. This procedure of generating the final image and mask is detailed in Algorithm~\ref{algo:surfawaremask} and its outputs are shown in Figure~\ref{fig:method-overview}, with the green pixels representing the inpainting mask.
Figure~\ref{fig:sam_fig} shows that our surface-aware masking is a crucial step for accurate image inpainting.

%%%%%%%%%%%%%%%%% surface-aware masking pseudocode

\begin{algorithm}
% \begin{adjustbox}{width=\columnwidth,center}
\caption{\textsc{Surface-Aware Masking (SAM)}}

\begin{algorithmic}
\Require{Input RGB-D image $\mathcal{I} = (\mathbf{I}, \mathbf{D})$, intrinsics $\mathbf{K}$, new viewpoint $\mathbf{T}_i$}
\State $U \leftarrow$ Subsample pixels from a uniform grid in $\mathcal{I}$
\State $X \leftarrow \{\}$ \Comment{initialize an empty point set.}
\ForAll {$\mathbf{u} \in U$}
\State $\mathbf{x} \leftarrow \mathbf{D}(\mathbf{u}) \mathbf{K}^{-1} \mathbf{u}$ \Comment{deprojection of $\mathbf{u}$ to 3D point $\mathbf{x}$.}
\For {$i \leftarrow 1$ to $m$}
    \State $\mathbf{p} \leftarrow \mathbf{x} + i \cdot c \cdot \mathbf{K}^{-1} \mathbf{u}$
    \State $X \leftarrow X \cup \{\mathbf{p}\}$ \Comment{set of points with equal spacing.}
\EndFor
\EndFor
\State $\mathcal{M} \leftarrow$ Mesh($X$) \Comment{surface triangulation to create a mesh.}
\State $\bar{\mathbf{I}}_i, \bar{\mathbf{D}}_i \leftarrow$ Reprojection of $\mathbf{I}, \mathbf{D}$ in camera viewpoint $\mathbf{T}_i$, where missing values are set to 0.
\State $\widetilde{\mathbf{D}}_i \leftarrow$ Depth map rendering of $\mathcal{M}$ in camera $\mathbf{T}_i$
\State $M \leftarrow \mathbf{0}_{H \times W}$ \Comment{initialize the mask image as zeros.}
\ForAll {$\mathbf{u} \in M$}
\State $M(\mathbf{u}) \leftarrow 1$ if $\bar{\mathbf{D}}_i(\mathbf{u}) = 0 \vee \bar{\mathbf{D}}_i(\mathbf{u}) > \widetilde{\mathbf{D}}_i(\mathbf{u})$ 
\EndFor


\State \textbf{return} $M, \bar{\mathbf{D}}_i$ 
\end{algorithmic}
\label{algo:surfawaremask}

\end{algorithm}


%%%%%%%%%%%%%%%%%




\subsubsection{Diffusion-based Inpainting}
\label{sec:diff-inpaint}
Once these preprocessing steps have been completed, we pass the processed image and a mask of areas to be filled in to the inpainting algorithm.  We use \dalle{}\cite{ramesh2022hierarchical} for image inpainting since it demonstrates the ability to produce the most realistic results.  This model takes in the incomplete image \(\bar{\mathbf{I}}_i\), the mask generated in the previous step \(M\), and an input prompt \(P\) that describes the context of the image in words.  To demonstrate the generalizability of our method, all our scenes use the generic prompt \textit{``household objects on a table"} when generating inpainted images, although we explore using more specific prompts in our ablation experiments (Section~\ref{sec:ablation}).  The output from this inpainting method is an image  \(\hat{\mathbf{I}}_i\) that now contains estimated areas from the diffusion model.  Figure~\ref{fig:method-overview} shows an example before and after inpainting with \dalle{}.

\subsubsection{Inpainting Ranking Step}
The resulting images from the inpainting method may vary in terms of their perceived realism for every new generation. We use it to our advantage by generating multiple inpainted images for the same incomplete image and mask.  These inpainted images are then compared against the input prompt \(P\) by encoding them to the CLIP embedded space~\cite{DBLP:journals/corr/abs-2103-00020}. The image containing the highest similarity is chosen as the final inpainted image \(\hat{\mathbf{I}}_i\) at viewpoint $\mathbf{T}_i$.

\subsection{Depth Completion}
This section describes the steps taken for generating a complete depth map \(\hat{\mathbf{D}}_i\) from an incomplete depth image \(\bar{\mathbf{D}}_i\) as well as its corresponding RGB image.  For depth completion, we use a method proposed by Zhang et al.~\cite{zhang2018deepdepth}.  This method works by estimating the normals and occlusion boundaries from the RGB image, and then optimizes for the complete depth by utilizing the estimated normals, occlusion boundaries, and incomplete depth.

\subsubsection{Normals and Occlusion Boundaries Prediction}
In order to obtain estimations for the normals and occlusion boundaries, we train Deeplabv3+ with DRN-D-54 on ground truth normals, in the same manner as Sajjan et al.~\cite{sajjan2020clear}. The ground truth normals are obtained using the depth images from the YCB-V training dataset~\cite{xiang2018posecnn}, the YCB-V synthetic dataset \cite{denninger2020blenderproc, hodan2020bop}, and the HomebrewedDB synthetic dataset~\cite{kaskman2019homebreweddb}.  The occlusion boundaries are also obtained by using the ground truth depth from the same datasets, and trained in the same manner as~\cite{sajjan2020clear}.

\subsubsection{Optimize for Depth}
Given the incomplete depth, the estimated normals from the image, and estimated occlusion boundaries, we solve for the completed depth. The main idea behind this method in~\cite{zhang2018deepdepth} is that the areas with missing depth can be computed by tracing along the estimated normals from areas of known depth with the occlusion boundaries acting as barriers where normals should not be traced across.  Formally we solve a system of equations to minimize an error \(E\), where \(E\) is defined as \(E=\lambda_DE_D + \lambda_SE_S + \lambda_NE_NB\).  Here, \(E_D\) is the distance between the ground truth and estimated depth, \(E_S\) influences nearby pixels to have similar depths, and \(E_N\) measures the consistency of estimated depth and estimated normal values. We use the same \( \lambda_D,\lambda_S,\lambda_N \) values from Sajjan et al~\cite{sajjan2020clear}.

\subsection{Scene Completion}
This section describes the complete process we follow to reconstruct a 3D scene from a single RGB-D image.

\subsubsection{Point Cloud Rotation} \label{sec:pcl_rotation}
Our main method consists of first deprojecting the original RGB-D image into a point cloud. The original image will be referred to as \(\mathbf{I}_0\).  Next, we rotate the point cloud around its mean along the world $z$-axis (or perpendicular to the ground plane) by angle \(\theta\).  The rotated point cloud is then projected back into the original camera plane.  This results in an incomplete RGB image as well as an incomplete depth image.  The RGB image is then inpainted, and used to complete the depth image as described in the depth completion section above.  This new viewpoint can be seen in Figure~\ref{fig:figure-3} as \(\mathbf{T}_1\). We denote the completed RGB-D image from this viewpoint as \(\hat{\mathbf{I}}_1\). We then take the image \(\mathbf{I}_0\), and repeat this process, this time rotating by angle \(2\times\theta\) to obtain viewpoint \(\mathbf{T}_2\) and image \(\hat{\mathbf{I}}_2\). This whole process is repeated two more times with \(-\theta\) and \(2\times-\theta\) as rotation values to obtain viewpoints \(\mathbf{T}_3\) and \(\mathbf{T}_4\) respectively.  Resulting from these steps, we obtain four completed novel views of the scene [\(\hat{\mathbf{I}}_1\), \(\hat{\mathbf{I}}_2\), \(\hat{\mathbf{I}}_3\), \(\hat{\mathbf{I}}_4\)].

% as seen in  Figure~\ref{fig:figure-3} and then complete those views with RGB inpainting and depth completion as described in the sections above.

\subsubsection{Enforcing Consistency Across Viewpoints}\label{sec:consistency}
The final step in our method involves combining these generated viewpoints while enforcing consistency across them.  One drawback of utilizing \dalle{} for inpainting real objects, is its inconsistent completion of objects as well as the creation of objects that are not originally in the scene.  To combat this issue, we filter for consistent predictions across viewpoints.  The final prediction is achieved by first deprojecting the RGB-D images \(\hat{\mathbf{I}}_1\) and \(\hat{\mathbf{I}}_2\) from viewpoint \(\mathbf{T}_1\) and \(\mathbf{T}_2\) respectively back into the world frame as point clouds.  The intersection of these point clouds are taken, and this intersection of points is added to our final prediction.  The same process is taken with images \(\hat{\mathbf{I}}_3\) and \(\hat{\mathbf{I}}_4\) from their viewpoints.  With our final prediction being the point cloud from viewpoint \(\mathbf{T}_0\), the intersection between point clouds from \(\mathbf{T}_1\) and \(\mathbf{T}_2\), as well as \(\mathbf{T}_3\) and \(\mathbf{T}_4\).  By taking only the points that both views predict are there, we find that our final output point cloud of the completed scene \({S}\) contains more accurate geometry and color.

% Figure environment removed

% Figure environment removed