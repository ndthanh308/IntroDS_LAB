

\textbf{Scene Reconstruction:}
% Scene reconstruction refers to the problem of estimating the 3D geometry of an environment, usually containing multiple objects, from a single image.
While single-object reconstruction is a well-studied problem, full-scene reconstruction is explored in limited settings. Previously works in scene reconstruction were focused either on room scale~\cite{song2017semantic, dai2018scancomplete} or in autonomous driving settings~\cite{cheng2021s3cnet, rist2021semantic, cao2022monoscene} where the scene geometries are usually more structured. In this work, we focus on an object-level scale, specifically tabletop environments, where objects can be in cluttered configurations. While methods like ~\cite{gkioxari2019mesh, popov2020corenet, irshad2022centersnap} % Mesh R-CNN~\cite{gkioxari2019mesh}, CoReNet~\cite{popov2020corenet}, and CenterSnap~\cite{irshad2022centersnap} 
show an accurate reconstruction of objects at the scene level, they do not generalize to novel category objects. Recently,~\cite{wu2023multiview} introduced a method for reconstructing 3D geometries of objects and scenes of unseen categories, and demonstrated generalization capability to objects in-the-wild. However, different from our setting, they mostly focus on isolated objects and scenes with little to no clutter. In contrast, our method can reconstruct geometries and textures of complex scenes with objects from novel categories under heavy occlusions, as we show in our experiments.

\textbf{Inpainting:}
% Inpainting is the process of filling in missing areas of images.  
%Traditionally, inpainting methods made use of image priors such as self-similarity for tasks like image restoration, where gaps with missing or corrupt values to be filled are usually small holes.
% Traditional inpainting methods made use of image priors to fill small holes.% where gaps with missing or corrupt values to be filled are usually small holes.
% Deep learning methods, on the other hand, using large amounts of training data achieved remarkable success for inpainting images with semantically consistent contents.
% Deep learning methods, on the other hand, achieved remarkable success for inpainting images with semantically consistent contents.
% Deep generative methods like Generative adversarial networks~\cite{goodfellow2020generative}, for example, were shown to handle many challenging inverse problems, including image denoising~\cite{chen2018image}, super-resolution~\cite{ledig2017photo}, and inpainting~\cite{pathak2016context, iizuka2017globally, zhao2021large}.
% \todo{While inpainting was typically done using, ... .  Deep learning methods such as ... also demonstrated improvement with inpainting capabilities in similar images the models had been trained on
% \url{https://openaccess.thecvf.com/content_cvpr_2017/papers/Yeh_Semantic_Image_Inpainting_CVPR_2017_paper.pdf}
% ~\cite{yeh2017semantic}.}
While traditional inpainting methods made use of hand-crafted image priors to fill small gaps for tasks like image restoration \cite{elharrouss2020image}, deep generative methods like Generative Adversarial Networks (GAN) ~\cite{goodfellow2020generative} have shown remarkable success for tasks like image denoising~\cite{chen2018image}, super-resolution~\cite{ledig2017photo}, and inpainting~\cite{pathak2016context, iizuka2017globally, zhao2021large}. However, GAN models are known for potentially unstable training for large datasets \cite{weng2021diffusion}. More recently, resulting from the growth of visual language diffusion models, which can be efficiently trained on internet-scale datasets, inpainting through image diffusion has shown great generalization capabilities to many different objects and scenes~\cite{ramesh2022hierarchical}.
% also taken off~\cite{ramesh2022hierarchical}. These models, having been trained on millions of images, have demonstrated their ability to generalize to many different objects, scenes, and styles.
In this paper, we develop a process to use a visual language diffusion model for inpainting cluttered scenes involving heavy occlusions.


\textbf{Text-to-3D Synthesis: }
% Another area that is currently rapidly being explored is that of shape completion using visual-language models with a combination of neural radiance fields (NeRF)~\cite{mildenhall2021nerf}.
Recent papers such as~\cite{jain2022zero, poole2022dreamfusion, lin2022magic3d} have demonstrated the ability to generate 3D models of very diverse objects from merely a text description. Despite the realistic appearance, these generated objects are not grounded to any real-world geometry.
% , so may not be as useful when estimating the shape of real objects.
To overcome this limitation, ~\cite{xu2022neurallift, melas2023realfusion} extended these methods to reconstruct based on a ground truth reference image. These papers demonstrate high accuracy on individual objects, but do not demonstrate the ability to reconstruct multiple objects in cluttered scenes. Moreover, their runtime is a concern, as optimizing neural radiance fields ~\cite{mildenhall2021nerf} can take up to an hour. ~\cite{nichol2022point} attempts to produce faster results by optimization without NeRF but are still limited to single object reconstruction and do not directly generalize to our setting. % While better optimized for speed, this method is still limited in focus to only single object reconstruction and does not directly generalize to the setting we study in this paper.

% \vspace{-3mm}