
In this section, we evaluate the performance of \ours{} for single view RGB-D scene reconstruction task. % on and on the out-of-training distribution dataset.
 We also report ablation studies for understanding the dependence of our method on (1) prompt specificity, (2) inpainting model, and (3) consistency filtering.



\textbf{Implementation Details: }
\label{sec:implementation}
The inpainting step of our algorithm is based on OpenAI's \dalle{} API.
% Our inpainting algorithm is done by utilizing OpenAI's API for \dalle{}.
For our implementation of SAM, we choose a spacing value $c$ of 0.01 meters with $m$ = 100 points for generating our rays. For choosing the number of viewpoints/viewpoint directions \(V\) as well as the context ratio \(C^*\) described in our method section, we perform a parameter search using 4 held out validation scenes from the YCB-V test set. We test using 6, 8, 10, and 12 viewpoints as well as a value of 0.3, 0.4, 0.5, 0.6, and 0.7 for our context threshold. We found that 10 views and 0.4 as a context threshold gave us the best accuracy on the validation set. 12 views and 0.4 as a context threshold performed similarly, but in the interest of runtime we use 10 for the final method. %Results from this parameter search can be seen in Table~\ref{tab:grid-search} (see Appendix \cite{kasahara2023rico}). 
The full parameter grid search are provided in our appendix. For our module that enforces consistency between synthesized views, we choose a threshold of 0.01 meters when computing the intersection between points in the viewpoints point clouds.




% \begin{table}[t]
% \begin{adjustbox}{width=\columnwidth,center}
% \centering
% % \toprule
% \begin{tabular}{|c|c|c|c|c|c|}
%   \hline
%    & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\
%   \hline
%   6 &  &  &  &  & \\
%   \hline
%   8 &  &  &  &  & \\
%   \hline
%   10 &  &  &  &  & \\
%   \hline
% % \bottomrule
% \end{tabular}
% \end{adjustbox}
%  \caption{Comparison of different parameters for our method on 4 validation scenes of the YCB-V~\cite{xiang2018posecnn} dataset using Chamfer Distances (CD) to indicate better performance.}
%     \label{tab:sota-comparison}
% \end{table}

% \begin{table}[h!]
% \vspace{-2mm}
% % \begin{adjustbox}{width=\columnwidth,center}
% \centering
% \begin{tabular}{l|c c c c c}
% \toprule
% % \diagbox{\(V\)}{\(C^*\)} & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\
% \(V\)~$\vert$~\(C^*\) & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\
% \midrule
% 6 & 0.218 & 0.242 & 0.255 & 0.244 & 0.238 \\
% 8 & 0.247 & 0.268 & 0.269 & 0.251 & 0.236 \\
% 10 & 0.265 & \textbf{0.277} & 0.272 & 0.251 & 0.235 \\
% \bottomrule
% \end{tabular}
% % \end{adjustbox}
%  \caption{Experiment using different values for context \(C^*\) and number of viewpoints \(V\) for our method on 4 validation scenes of the YCB-V~\cite{xiang2018posecnn} dataset using IoU to indicate better performance.}
%     \label{tab:grid-search}
%     \vspace{-5mm}
% \end{table}

\textbf{Datasets:}
We trained our depth completion model using the YCB-V training dataset \cite{xiang2018posecnn}. For testing, we test on $8$ unseen scenes from the YCB-V test set, and select $5$ RGB-D images from each of the scenes, i.e., we test on a total of $40$ RGB-D images in total. For ground truth point clouds, we deproject the RGB-D frames of the scene and concatenate them together. We also place the ground truth meshes in the scene for the objects and convert those to point clouds before concatenating them as well. Finally, we crop this point cloud around the ground truth meshes with a 10cm buffer as the RGB-D frames may contain floors and walls far away that we are not interested in reconstructing. This creates our final ground truth point cloud covering the majority of the scene with full geometry of the objects in the scene.

To demonstrate our model's capabilities of generalizing to unseen objects and to entirely new datasets, we also compare our method on the HOPE dataset \cite{tyree2022hope}. HOPE test set only contains individual RGB-D images and is unusable for generating full scene point cloud. Instead, we use HOPE training dataset for evaluation, as the train set contains RGB-D video and cluttered tabletop scenes with novel objects. The dataset has 10 scenes, and we again sample 5 frames per scene for a total of 50 RGB-D test images. Ground truth point clouds are obtained similarly to the YCB-V dataset.

% \subsection{Metrics}
% We evaluate our method on standard 3D reconstruction metrics: 

% \textbf{Intersection-over-Union (IoU)}:
% We voxelize the ground truth and predicted point clouds at a fixed resolution and compute the IoU score by dividing the number of voxels that intersect to that of their union. In our experiments, we evaluate all the methods at the same grid resolution of $100^3$ after rescaling the predictions and ground truth to fit into the unit cube. \textbf{Chamfer Distance (CD)}: Chamfer distance is commonly used to measure the similarity between two point sets and is defined as:\vspace{-2mm}
% \begin{equation}
%     CD(X, Y) = \frac{1}{|X|} \sum_{\mathbf{x} \in X} \min_{\mathbf{y} \in Y} ||\mathbf{x} - \mathbf{y}||_2
% \end{equation}
% We separately report $CD(S, S^*)$ and $CD(S^*, S)$, as well as their their sum. $CD(S, S^*)$ measures how close the reconstructed points from $S$ are to the ground truth points $S^*$, whereas $CD(S^*, S)$ computes how well the ground truth shape is covered. \textbf{F-Score}: Following ~\cite{tatarchenko2019single}, we also report F-Score$@1\%$ which is a measure for the percentage of the surface points that were reconstructed correctly.


% \subsection{Comparison with State of the Art}
\textbf{Baselines:}
We compare \ours{} against four baselines: Convolutional Occupancy Networks (CON) ~\cite{peng2020convolutional} is a 3D scene reconstruction method that inputs a sparse point cloud. We use their pre-trained model for \textit{Synthetic Indoor Scene dataset} where similar to our YCB-V and HOPE datasets, they place multiple ShapeNet ~\cite{shapenet2015} objects in indoor scenes. CoReNet ~\cite{popov2020corenet} is a multi-object shape estimator that inputs an RGB image and estimates a mesh. We compare against CoReNet's pre-trained model qualitatively since its predictions lack scale information. ShellNet ~\cite{chavan2022simultaneous} is trained for single object reconstruction. Given a scene depth image and object instance mask, ShellNet produces reconstruction for the object instance. We re-implemented ShellNet's architecture and trained it with Mask R-CNN~\cite{DBLP:journals/corr/HeGDG17} as the segmentation network on YCB-V dataset.
% ShellNet input a mask of the object and the depth map of the scene. We reimplemented ShellNet's architecture and trained it with Mask R-CNN~\cite{DBLP:journals/corr/HeGDG17} as the segmentation network on YCB objects.
Finally, we compare against CenterSnap ~\cite{irshad2022centersnap}, a multi-object point cloud prediction method. CenterSnap inputs an RGB-D image and predicts point clouds for each object in the scene. Similar to CenterSnap's original training, we first train it on YCB-V synthetic dataset \cite{denninger2020blenderproc, hodan2020bop}, then fine-tune it on the YCB-V real training dataset \cite{xiang2018posecnn}. Since CenterSnap and ShellNet only predict the point clouds for objects and not the rest of the scene, for a fair evaluation, we concatenate their outputs with deprojected point cloud from the input RGB-D image.


% \subsection{Results}
% % Figure environment removed

% Figure environment removed

% \begin{table}[t]
% \begin{adjustbox}{width=\columnwidth,center}
% \centering
% \begin{tabular}{l | c c c c c}
% \toprule
%  \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
% \toprule
% \multicolumn{6}{c}{YCB-V~\cite{xiang2018posecnn}} \\
% \midrule
%  CON~\cite{peng2020convolutional} & 0.041 & 0.218 & 0.040 & 0.015 & 0.055  \\ 
%  ShellNet~\cite{chavan2022simultaneous} & 0.155 & 0.495 & 0.024 & \textbf{0.009} & 0.033 \\
%  CenterSnap~\cite{irshad2022centersnap} & 0.149 & 0.483 & \textbf{0.023} & 0.013 & 0.036 \\
%  \ours{} (Ours) & \textbf{0.202} & \textbf{0.567} & \textbf{0.023} & \textbf{0.009} & \textbf{0.032} \\
% \midrule
% \multicolumn{6}{c}{HOPE~\cite{lin2021fusion}} \\
% \midrule
% CON~\cite{peng2020convolutional} & 0.051 & 0.224 & 0.078 & 0.019 & 0.097 \\ 
%  ShellNet~\cite{chavan2022simultaneous} & 0.134 & 0.433 & 0.040 & \textbf{0.006} & 0.045  \\
%  CenterSnap~\cite{irshad2022centersnap} & 0.126  & 0.411 & 0.043 & 0.007 & 0.050 \\
%  \ours{} (Ours) & \textbf{0.207} & \textbf{0.55} & \textbf{0.033}  & 0.007 & \textbf{0.040}  \\
% \bottomrule
% \end{tabular}
% \end{adjustbox}
%  \caption{Comparison of methods for the task of 3D scene completion on the YCB-V~\cite{xiang2018posecnn} and HOPE~\cite{lin2021fusion} datasets. Higher numbers for the IoU and F-score metrics, and lower numbers for the Chamfer Distances (CD) indicate better performance.}
%     \label{tab:sota-comparison}
% \end{table}

\begin{table}[t]
\begin{adjustbox}{width=\columnwidth,center}
\centering
\begin{tabular}{l | c c c c c}
\toprule
 \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
\toprule
\multicolumn{6}{c}{YCB-V~\cite{xiang2018posecnn}} \\
\midrule
 CON~\cite{peng2020convolutional} & 0.087 & 0.354 & 0.036 & 0.014 & 0.050 \\ 
 ShellNet~\cite{chavan2022simultaneous} & 0.224 & 0.607 & 0.019 & 0.012 & 0.031 \\
 CenterSnap~\cite{irshad2022centersnap} & 0.225 & 0.622 & 0.019 & \textbf{0.009} & \textbf{0.028} \\
 \ours{} (Ours) & \textbf{0.294} & \textbf{0.661} & \textbf{0.018} & 0.010 & \textbf{0.028} \\
\midrule
\multicolumn{6}{c}{HOPE~\cite{lin2021fusion}} \\
\midrule
CON~\cite{peng2020convolutional} & 0.086 & 0.279 & 0.094 & 0.035 & 0.128 \\ 
 ShellNet~\cite{chavan2022simultaneous} & 0.185 & 0.523 & 0.035 & 0.013 & 0.047  \\
 CenterSnap~\cite{irshad2022centersnap} & 0.180  & 0.526 & 0.037 & 0.006 & 0.042 \\
 \ours{} (Ours) & \textbf{0.290} & \textbf{0.649} & \textbf{0.031}  & \textbf{0.005} & \textbf{0.036}  \\
\bottomrule
\end{tabular}
\end{adjustbox}
 \caption{Comparison of methods for the task of 3D scene completion on YCB-V~\cite{xiang2018posecnn} and HOPE~\cite{lin2021fusion}. Higher numbers for the IoU and F-score metrics, and lower numbers for the Chamfer Distances (CD) indicate better performance.}
    \label{tab:sota-comparison}
    \vspace{-7mm}
\end{table}

% % Figure environment removed

% % \subsection{Results}
% % Figure environment removed

% \subsubsection{Qualitative Results}
% Qualitative results
Table~\ref{tab:sota-comparison} shows quantitative evaluations on within-training-distribution YCB-V dataset \cite{xiang2018posecnn} and out-of-training-distribution HOPE dataset \cite{tyree2022hope}. On YCB-V dataset, \ours{} is able to outperform CON and ShellNet on all 3D scene reconstruction metrics. CON takes as input a sparse point-cloud of the scene. When major parts of the input point clouds are missing, as the common case for single-view RGB-D point clouds, CON fails to infer those regions. ShellNet is trained to predict back-side depth image for the detected object. We notice that with varying viewing directions, ShellNet backside depths are either too thin or too thick resulting in low performance. MaskRCNN's failure to detect objects also directly contributed to lower performance for ShellNet. CenterSnap inputs RGB-D image and predicts object shapes via a multi-step procedure allowing CenterSnap to learn strong shape and pose priors for objects within training distribution. This allowed CenterSnap to perform strongly on YCB-V objects as it was trained on them, but we noticed it struggles with cases of occluded objects. \ours{} which is trained without ground truth object pose or shape supervision is able to match or outperform the baselines in all metrics. Fig. \ref{fig:qualitative-baselines} shows a qualitative comparison with baselines.

On the out-of-distribution HOPE dataset, \ours{} is able to outperform all baselines by an even larger margin. This shows that our normal and occlusion boundary-based depth completion method generalizes well to unseen novel scenes. Figure~\ref{fig:qualitative-geometry} shows qualitative results on these datasets. %while % Figure~\ref{fig:qualitative-geometry} shows qualitative results for our method on the HOPE and YCB-V datasets.

% Figure environment removed

As a byproduct, our method also produces novel views of unseen multi-object scenes from a single RGB-D image. Figure~\ref{fig:qualitative-inpainting} shows our method compared to the ground truth. We show that by combining our masking method with \dalle{}'s inpainting capability, realistic novels views can be generated for multiple unseen objects.

% % Figure environment removed



\subsection{Ablation Studies}


\begin{table}[ht!]
\centering
 \begin{tabular}{l | c c c} 
 \hline
 \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD $\downarrow$ \\
 \hline
 \ours{} (S) & 0.262 & 0.613 & 0.038 \\
 \ours{} (G) & 0.261 & 0.613 & 0.038 \\
 \ours{} (Ours) & \textbf{0.290} & \textbf{0.649} & \textbf{0.036}  \\
 \hline
 \end{tabular}
 \caption{Prompt specificity  results  on  HOPE  dataset \cite{tyree2022hope}: \ours{} (S) denotes  our  model  with  scene  specific  prompt, \ours{} (G) uses ``household objects on a table" as the prompt for all scenes, and \ours{} (Ours) uses an image caption generator~\cite{li2022blip}.}
 \label{tab:prompt}
 \vspace{-3mm}
\end{table}

\textbf{Prompt Reliance:} Image diffusion models tend to heavily rely on the input prompt. To test our methods robustness, we performed an experiment using a general prompt (G), ``a photo of household objects on a table", for every scene to see how much performance degrades. We also use a specific prompt (S) where using the ground truth list of objects we list out every object on the table as the prompt. Table~\ref{tab:prompt} shows that our method does not largely depend on the type of prompt. We hypothesize that our view selection method retains enough surrounding context information in the input RGB image required for the inpainting model to inpaint successfully.


% \label{sec:ablation}
% \textbf{Prompt Specificity}: In our method, we use a deep image captioning method that generated a prompt from the original input image for inpainting. To investigate whether the performance is robust to prompt specificity, we also compare our method using a specific and a general prompt across all scenes. For the specific prompt, with a scene containing $n$ objects with object labels $obj_{i}$, we create the prompt as ``a $obj_1$, $obj_{2}$ $\cdots$ $obj_{n-1}$, and $obj_{n}$ on a table''. This list of objects is limited to the first 10 objects for larger scenes. We call this ablation \ours{} (S). For the general prompt, we use ``household objects on a table" for every scene and call this ablation \ours{} (G). Table~\ref{tab:prompt} shows that our method does not largely depend on the type of prompt. While image diffusion models generally heavily depend on the input prompt for creating images, we hypothesize that our solution where we maximally retain the information present in the input RGB image, provides enough surrounding context and hence, does not require a very specific prompt.





% % \textbf{Viewpoint Angles}: Here, we ablate our method for different values for the angle of rotation between viewpoints \(\theta\) (Section~\ref{sec:pcl_rotation}). We experimented with \(\theta\) values of 10, 20, and 30 degrees and chose the best performing \(\theta\) of 20 degrees as our final \(\theta\). Note that even with the larger \(\theta\) of 30 degrees, our method's performance does not change a lot.


% % \begin{table}[h!]
% % \begin{adjustbox}{width=\columnwidth,center}
% % \centering
% %  \begin{tabular}{l | c c c c c} 
% %  \hline
% %  \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
% %  \hline
% %  \ours{} (10) & 0.296  & 0.698 & \textbf{0.038} & 0.006 & 0.044  \\
% %  \ours{} (20) & \textbf{0.298} & \textbf{0.702} & 0.039 & 0.004 & \textbf{0.043}  \\
% %   \ours{} (30) & 0.296 & 0.699 & 0.04 & \textbf{0.003}  & \textbf{0.043}  \\
% %  \hline
% %  \end{tabular}
% %   \end{adjustbox}
% %  \caption{Viewpoint angles study on the HOPE~\cite{lin2021fusion} dataset. 10, 20, and 30 denote the angle of rotation \(\theta\) of the point cloud between inpainting steps as described in Section~\ref{sec:pcl_rotation}}
% % \end{table}

% \textbf{Inpainting Model}: For our method we use Dalle-2 for the inpainting portion as we found qualitatively it performed very well at completing objects in 2D. We also test out Stable Diffusion 2's~\cite{Rombach_2022_CVPR} inpainting model as an open-source alternative. We found that while Stable Diffusion 2's reconstructions were not as accurate as seen in Table~\ref{tab:inpainting}, it could still be used as a viable alternative if necessary.


% \begin{table}[h!]
% \begin{adjustbox}{width=\columnwidth,center}
% \centering
%  \begin{tabular}{l | c c c c c} 
%  \hline
%  \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
%  \hline
%  \ours{} (SD-2) & 0.265 & 0.620 & 0.033 & \textbf{0.005} & 0.037 \\
%   \ours{} (Ours) & \textbf{0.290} & \textbf{0.649} & \textbf{0.031}  & \textbf{0.005} & \textbf{0.03624}  \\
%  \hline
%  \end{tabular}
%   \end{adjustbox}
%  \caption{Inpainting method study shown on the HOPE~\cite{lin2021fusion} dataset. (Ours) refers to our main method which utilizes OpenAI's Dalle-2 model, and (SD-2) refers to Stable Diffusion 2's inpainting model.}
% \label{tab:inpainting}
% \end{table}

\textbf{Inpainting Model:}
We substitute in Stable Diffusion 2's~\cite{Rombach_2022_CVPR} inpainting model as an open-source alternative to \dalle{} in Table~\ref{tab:ablations}. We find that while accuracy decreases, it is still a viable inpainting substitute for our method.

% Figure environment removed

% Ablation Experiments

\textbf{Consistency Filtering:}
We test our method without applying the consistency filtering step by just combining all predicted viewpoints in Table~\ref{tab:ablations}. This caused a substantial decrease in accuracy as any hallucinated object is kept in.


\begin{table}[t]
\begin{adjustbox}{width=\columnwidth,center}
\centering
 \begin{tabular}{l | c c c c c} 
 \hline
 \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
 \hline
 \ours{} (SD-2) & 0.265 & 0.620 & 0.033 & \textbf{0.005} & 0.037 \\
  \ours{} (No Filter) & 0.271 & 0.574 & \textbf{0.017} & 0.030 & 0.047 \\ 
  \ours{} (Ours) & \textbf{0.290} & \textbf{0.649} & 0.031  & \textbf{0.005} & \textbf{0.036}  \\
 \hline
 \end{tabular}
  \end{adjustbox}
 \caption{Result of swapping out various parts of our method shown on the HOPE~\cite{lin2021fusion} dataset. (Ours) utilizes OpenAI's \dalle{} model and our consistency filtering method, (SD-2) uses Stable Diffusion 2's inpainting model, and (No Filter) refers to our method without filtering.}
\label{tab:ablations}
\vspace{-5mm}
\end{table}

% \textbf{Consistency Filtering}: Table~\ref{tab:consistency} shows results for our method without consistency filtering where we simply concatenate all predicted point clouds (Section \ref{sec:consistency}). \ours{} (No Filter) has lower performance compared to \ours{} (Ours) as \dalle{} does not produce consistent inpainting results across different views and often produce objects which are not in the scene. \ours{} with consistency filtering is able to aggregate coherent information from multiple inpainted views.

% \begin{table}[h!]
% \begin{adjustbox}{width=\columnwidth,center}
% \centering
%  \begin{tabular}{l | c c c c c} 
%  \hline
%  \textbf{Method} & IoU $\uparrow$ & F-Score $\uparrow$ & CD$(S^*, S)$ $\downarrow$ & CD$(S, S^*)$ $\downarrow$ & CD $\downarrow$ \\
%  \hline
%  \ours{} (No Filter) & 0.271 & 0.574 & \textbf{0.017} & 0.030 & 0.047 \\ 
%   \ours{} (Ours) & \textbf{0.290} & \textbf{0.649} & 0.031 & \textbf{0.005} & \textbf{0.036}  \\
%  \hline
%  \end{tabular}
%  \end{adjustbox}
%  \caption{Consistency filtering results on the HOPE~\cite{lin2021fusion} dataset. \ours{} (No Filter) refers to reconstructions without any consistency filtering (Section~\ref{sec:consistency}).}
% \label{tab:consistency}
% \end{table}

% % \textbf{Surface-Aware Masking}: Figure~\ref{fig:sam_fig} 

