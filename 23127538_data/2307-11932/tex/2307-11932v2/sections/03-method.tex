


% % Figure environment removed


% Method overview

In this section, we present our method Rotate-Inpaint-Complete, or \ours{}, for generalizable reconstruction of a 3D scene containing multiple objects, given a single RGB-D image of the scene.

\ours{} takes in as input an RGB-D image \(\mathcal{I} = (\mathbf{I}, \mathbf{D}) \in \mathbb{R} ^{ H \times W \times 4} \) and outputs a color point cloud \(S \in \mathbb{R} ^{ N \times (3 + 3)} \), where $N$ is the number of predicted points in the scene.
%
Our method consists of three main components: 1) An inpainting step that takes in an RGB-D image \(\mathcal{I}\) and outputs an inpainted RGB image \(\hat{\mathbf{I}}_i\) from a novel viewpoint $\mathbf{T}_i \in SE(3)$. 2) A depth completion component that takes in the inpainted RGB image \(\hat{\mathbf{I}}_i\) as well as an incomplete depth image \(\bar{\mathbf{D}}_i \) rendered from the viewpoint $\mathbf{T}_i$, and outputs a completed depth \(\hat{\mathbf{D}}_i\) at that viewpoint. 3) A viewpoint selection and consistency filtering method that utilizes the above two components to generate completed RGB-D images at rotated novel views and uses them to reconstruct the scene. We explain each of these components in detail next.



\subsection{Inpainting}

This section describes the inpainting process, as well as the intermediate steps taken before and after to go from the input image \(\mathbf{I}\) to \(\hat{\mathbf{I}}_i\) at a novel viewpoint $\mathbf{T}_i$.

\subsubsection{Rotate and Project RGB-D Image}
Given an RGB-D image of a scene and the camera intrinsics, we deproject the image into a point cloud in the camera frame. This point cloud is then projected onto a novel viewpoint $\mathbf{T}_i$ and the resulting image is masked using our Surface-Aware Masking method (SAM), which we describe in detail in the following section. The projection from this new viewpoint creates a new RGB-D image $\bar{\mathbf{I}}_i$ with missing RGB and depth information as seen in Figure~\ref{fig:method-overview}. Small holes of the missing RGB values are filled with a naive inpainting algorithm~\cite{telea2004image} by inpainting pixels that are covered after a morphological closing operation of kernel size 5 is applied to the mask. The larger missing areas are left for the deep inpainting module.

% \subsubsection{Background Mask Generation}
\subsubsection{Surface-Aware Masking}
\label{sec:bg-mask}

% Figure environment removed

In order for inpainting to work properly, a mask covering the areas to inpaint needs to be generated. After projecting to the new camera frame, any 3D space possible to be reconstructed needs to be represented as an inpainting mask in the 2D image. This issue can be seen in Figure~\ref{fig:sam_fig} as the table takes up pixels we may want to fill in with the bottle. To solve this problem, a 3D frustum is generated from the original camera and depth image. For every pixel in the original camera frame, a ray is cast from the camera through each point in the projected point cloud from \(\mathcal{I}\). Once the ray has passed through its respective point, it is used to generate a list of points along the ray from that depth onward with $m$ points of equal spacing $c$. This is done for every ray, and from this process results a point cloud covering the potential space that the 3D scene could possibly fill. This point cloud is then converted to a mesh, and when the point cloud from the RGB-D image \(\mathcal{I}\) is rotated to novel views, the mesh is rotated with it. Finally, when projecting back to the camera frame after rotation, points that are occluded by the mesh are discarded. Any blank pixels are then used as the 2D inpainting mask to be filled when passed to the inpainting step. This procedure of generating the final image and mask is detailed in our technical report (see Appendix) and its outputs are shown in Figure~\ref{fig:method-overview}, with the green pixels representing the inpainting mask.

% %%%%%%%%%%%%%%%%% surface-aware masking pseudocode

% \begin{algorithm}
% % \begin{adjustbox}{width=\columnwidth,center}
% \caption{\textsc{Surface-Aware Masking (SAM)}}

% \begin{algorithmic}
% \Require{Input RGB-D image $\mathcal{I} = (\mathbf{I}, \mathbf{D})$, intrinsics $\mathbf{K}$, new viewpoint $\mathbf{T}_i$}
% \State $U \leftarrow$ Subsample pixels from a uniform grid in $\mathcal{I}$
% \State $X \leftarrow \{\}$ \Comment{initialize an empty point set.}
% \ForAll {$\mathbf{u} \in U$}
% \State $\mathbf{x} \leftarrow \mathbf{D}(\mathbf{u}) \mathbf{K}^{-1} \mathbf{u}$ \Comment{deprojection of $\mathbf{u}$ to 3D point $\mathbf{x}$.}
% \For {$i \leftarrow 1$ to $m$}
%     \State $\mathbf{p} \leftarrow \mathbf{x} + i \cdot c \cdot \mathbf{K}^{-1} \mathbf{u}$
%     \State $X \leftarrow X \cup \{\mathbf{p}\}$ \Comment{set of points with equal spacing.}
% \EndFor
% \EndFor
% \State $\mathcal{M} \leftarrow$ Mesh($X$) \Comment{surface triangulation to create a mesh.}
% \State $\bar{\mathbf{I}}_i, \bar{\mathbf{D}}_i \leftarrow$ Reprojection of $\mathbf{I}, \mathbf{D}$ in camera viewpoint $\mathbf{T}_i$, where missing values are set to 0.
% \State $\widetilde{\mathbf{D}}_i \leftarrow$ Depth map rendering of $\mathcal{M}$ in camera $\mathbf{T}_i$
% \State $M \leftarrow \mathbf{0}_{H \times W}$ \Comment{initialize the mask image as zeros.}
% \ForAll {$\mathbf{u} \in M$}
% \State $M(\mathbf{u}) \leftarrow 1$ if $\bar{\mathbf{D}}_i(\mathbf{u}) = 0 \vee \bar{\mathbf{D}}_i(\mathbf{u}) > \widetilde{\mathbf{D}}_i(\mathbf{u})$ 
% \EndFor


% \State \textbf{return} $M, \bar{\mathbf{D}}_i$ 
% \end{algorithmic}
% \label{algo:surfawaremask}

% \end{algorithm}


% %%%%%%%%%%%%%%%%%




\subsubsection{Diffusion-based Inpainting}
\label{sec:diff-inpaint}
Once these preprocessing steps have been completed, we pass the processed image and a mask of areas to be filled in to the inpainting algorithm. We use \dalle{}\cite{ramesh2022hierarchical} for image inpainting since it demonstrates the ability to produce the most realistic results. This model takes in the incomplete image \(\bar{\mathbf{I}}_i\), the mask generated in the previous step \(M\), and an input prompt \(P\) that describes the context of the image in words. 
% When generating inpainted images for all our scenes, 
For prompt, we pass the RGB image $\mathbf{I}$ to a deep captioning model ~\cite{li2022blip} and prefix the generated caption with \textit{``A photo of''}. 
%We use prompts generated from a deep captioning model~\cite{li2022blip} preceded with \textit{``A photo of"}. 
We also explore using a more specific and generic prompt in our ablation experiments (Table~\ref{tab:prompt}).
The output from this inpainting method is an image  \(\hat{\mathbf{I}}_i\) that now contains estimated areas from the diffusion model. Figure~\ref{fig:method-overview} shows an example before and after inpainting with \dalle{}.

% \subsubsection{Inpainting Ranking Step}
% The resulting images from the inpainting method may vary in terms of their perceived realism for every new generation. We use it to our advantage by generating multiple inpainted images for the same incomplete image and mask. These inpainted images are then compared against the input prompt \(P\) by encoding them to the CLIP embedded space~\cite{DBLP:journals/corr/abs-2103-00020}. The image containing the highest similarity is chosen as the final inpainted image \(\hat{\mathbf{I}}_i\) at viewpoint $\mathbf{T}_i$.

\subsection{Depth Completion}
We use a method proposed in~\cite{zhang2018deepdepth} for generating a complete depth image \(\hat{\mathbf{D}}_i\) from an incomplete depth image \(\bar{\mathbf{D}}_i\) and its corresponding RGB image. This method estimates the normals and occlusion boundaries from the RGB image, and optimizes for the complete depth by utilizing the estimated normals, occlusion boundaries, and incomplete depth.

\subsubsection{Normals and Occlusion Boundaries Prediction}
In order to obtain estimations for the normals and occlusion boundaries, we train Deeplabv3+ with DRN-D-54 in the same manner as in~\cite{sajjan2020clear}. 
% Sajjan et al.
The ground truth normals and occlusion boundaries are obtained using the depth images from the YCB-V training dataset~\cite{xiang2018posecnn}, the YCB-V synthetic dataset \cite{denninger2020blenderproc, hodan2020bop}, and the HomebrewedDB synthetic dataset~\cite{kaskman2019homebreweddb}. 
% The occlusion boundaries are also obtained by using the ground truth depth from the same datasets and trained in the same manner as~\cite{sajjan2020clear}.

\subsubsection{Optimize for Depth}
Given the incomplete depth, the estimated normals from the image, and estimated occlusion boundaries, we solve for the completed depth. The main idea behind this method in~\cite{zhang2018deepdepth} is that the areas with missing depth can be computed by tracing along the estimated normals from areas of known depth with the occlusion boundaries acting as barriers where normals should not be traced across. Formally we solve a system of equations to minimize an error \(E\), where \(E\) is defined as \(E=\lambda_DE_D + \lambda_SE_S + \lambda_NE_NB\). Here, \(E_D\) is the distance between the ground truth and estimated depth, \(E_S\) influences nearby pixels to have similar depths, \(E_N\) measures the consistency of estimated depth and estimated normal values, and \(B\) weights the normal values based on the probability that it is a boundary. We use the same \( \lambda_D,\lambda_S,\lambda_N \) values as in~\cite{sajjan2020clear}. % Sajjan et al

\subsection{Scene Completion}
This section describes the complete process we follow to reconstruct a 3D scene from a single RGB-D image.

% \subsubsection{Point Cloud Rotation} \label{sec:pcl_rotation}
% Our main method consists of first deprojecting the original RGB-D image into a point cloud. The original image will be referred to as \(\mathbf{I}_0\). Next, we rotate the point cloud around its mean along the world $z$-axis (or perpendicular to the ground plane) by angle \(\theta\). The rotated point cloud is then projected back into the original camera plane. This results in an incomplete RGB image as well as an incomplete depth image. The RGB image is then inpainted, and used to complete the depth image as described in the depth completion section above. This new viewpoint can be seen in Figure~\ref{fig:figure-3} as \(\mathbf{T}_1\). We denote the completed RGB-D image from this viewpoint as \(\hat{\mathbf{I}}_1\). We then take the image \(\mathbf{I}_0\), and repeat this process, this time rotating by angle \(2\times\theta\) to obtain viewpoint \(\mathbf{T}_2\) and image \(\hat{\mathbf{I}}_2\). This whole process is repeated two more times with \(-\theta\) and \(2\times-\theta\) as rotation values to obtain viewpoints \(\mathbf{T}_3\) and \(\mathbf{T}_4\) respectively. Resulting from these steps, we obtain four completed novel views of the scene [\(\hat{\mathbf{I}}_1\), \(\hat{\mathbf{I}}_2\), \(\hat{\mathbf{I}}_3\), \(\hat{\mathbf{I}}_4\)].

\subsubsection{Viewpoint Selection} \label{sec:pcl_rotation}
% Our main method consists of first deprojecting the original RGB-D image into a point cloud. The original image will be referred to as \(\mathbf{I}_0\). We define a sphere with center


For diffusion-based inpainting, ``known" pixels, i.e., the non-masked areas, guide the prediction of the unknown masked areas. We refer to the known pixels as context pixels and define the context ratio \(C\) for any given image as \(C = (\# context~pixels)\: /\: (\# all\: pixels)\). This ratio gives us some indication about how accurately the inpainting model will be able to fill in the missing areas. With a low \(C\), many areas are unknown and inpainting will struggle, and with a high \(C\) inpainting will do well but only fill in minimal information. An example of different context ratio values can be seen in Figure~\ref{fig:figure-3}.

We then design our viewpoint selection process to search for a context ratio that will allow for accurate inpainting. To do this, we define a sphere with a center as the mean of the input point cloud, and the radius as the distance between the center and the initial camera location. Then from the starting viewing angle, we rotate in various directions along this sphere away from the starting position. At each step in this rotation, we project the input point cloud onto the new camera location. Using this newly projected image, we compute the context \(C\) of the image. If the \(C\) is closest to our chosen context threshold \(C^*\), we use that viewpoint $\mathbf{T}_i$ as next to inpaint. We repeat this process for \(V\) evenly spaced directions we traverse along the sphere as visualized in Figure~\ref{fig:figure-3}, where both \(C^*\) and \(V\) are chosen using the experiment described in Section~\ref{sec:implementation}.

% as seen in  Figure~\ref{fig:figure-3} and then complete those views with RGB inpainting and depth completion as described in the sections above.

% \subsubsection{Enforcing Consistency Across Viewpoints}\label{sec:consistency}
% The final step in our method involves combining these generated viewpoints while enforcing consistency across them. One drawback of utilizing \dalle{} for inpainting real objects, is its inconsistent completion of objects as well as the creation of objects that are not originally in the scene. To combat this issue, we filter for consistent predictions across viewpoints. The final prediction is achieved by first deprojecting the RGB-D images \(\hat{\mathbf{I}}_1\) and \(\hat{\mathbf{I}}_2\) from viewpoint \(\mathbf{T}_1\) and \(\mathbf{T}_2\) respectively back into the world frame as point clouds. The intersection of these point clouds are taken, and this intersection of points is added to our final prediction. The same process is taken with images \(\hat{\mathbf{I}}_3\) and \(\hat{\mathbf{I}}_4\) from their viewpoints. With our final prediction being the point cloud from viewpoint \(\mathbf{T}_0\), the intersection between point clouds from \(\mathbf{T}_1\) and \(\mathbf{T}_2\), as well as \(\mathbf{T}_3\) and \(\mathbf{T}_4\). By taking only the points that both views predict are there, we find that our final output point cloud of the completed scene \({S}\) contains more accurate geometry and color.

\subsubsection{Enforcing Consistency Across Viewpoints}\label{sec:consistency}
{The final step in our method involves combining these generated viewpoints while enforcing consistency across them. One drawback of utilizing \dalle{} for inpainting real objects, is its inconsistent completion of objects as well as the hallucination of objects that are not originally in the scene. To combat this issue, we filter for consistent predictions across viewpoints. The final prediction is achieved by first deprojecting the RGB-D images from each viewpoint \(\mathbf{T}_i\) back into the original camera frame as point clouds. We then apply the following \textit{consistency rule} across all the generated points: If a predicted point from one viewpoint has a predicted point within a 1cm radius from at least two other viewpoints we keep that point, otherwise we remove that point from our final prediction. This rule allows us to keep points that only multiple viewpoints predict. We then combine all filtered points to obtain our final output point cloud of the completed scene \({S}\), which contains more accurate geometry and color than without filtering as seen in Table~\ref{tab:ablations} and Figure~\ref{fig:figure-1}.

% Figure environment removed

% Figure environment removed