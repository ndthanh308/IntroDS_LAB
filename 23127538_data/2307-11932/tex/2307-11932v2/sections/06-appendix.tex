
Here we include further details about implementation and experiments.

\subsection{Surface-Aware Masking Pseudocode}
We include pseudocode to help explain how our Surface-Aware Masking module (SAM) is implemented.
%%%%%%%%%%%%%%%%% surface-aware masking pseudocode

\begin{algorithm}
% \begin{adjustbox}{width=\columnwidth,center}
\caption{\textsc{Surface-Aware Masking (SAM)}}

\begin{algorithmic}
\Require{Input RGB-D image $\mathcal{I} = (\mathbf{I}, \mathbf{D})$, intrinsics $\mathbf{K}$, new viewpoint $\mathbf{T}_i$}
\State $U \leftarrow$ Subsample pixels from a uniform grid in $\mathcal{I}$
\State $X \leftarrow \{\}$ \Comment{initialize an empty point set.}
\ForAll {$\mathbf{u} \in U$}
\State $\mathbf{x} \leftarrow \mathbf{D}(\mathbf{u}) \mathbf{K}^{-1} \mathbf{u}$ \Comment{deprojection of $\mathbf{u}$ to 3D point $\mathbf{x}$.}
\For {$i \leftarrow 1$ to $m$}
    \State $\mathbf{p} \leftarrow \mathbf{x} + i \cdot c \cdot \mathbf{K}^{-1} \mathbf{u}$
    \State $X \leftarrow X \cup \{\mathbf{p}\}$ \Comment{set of points with equal spacing.}
\EndFor
\EndFor
\State $\mathcal{M} \leftarrow$ Mesh($X$) \Comment{surface triangulation to create a mesh.}
\State $\bar{\mathbf{I}}_i, \bar{\mathbf{D}}_i \leftarrow$ Reprojection of $\mathbf{I}, \mathbf{D}$ in camera viewpoint $\mathbf{T}_i$, where missing values are set to 0.
\State $\widetilde{\mathbf{D}}_i \leftarrow$ Depth map rendering of $\mathcal{M}$ in camera $\mathbf{T}_i$
\State $M \leftarrow \mathbf{0}_{H \times W}$ \Comment{initialize the mask image as zeros.}
\ForAll {$\mathbf{u} \in M$}
\State $M(\mathbf{u}) \leftarrow 1$ if $\bar{\mathbf{D}}_i(\mathbf{u}) = 0 \vee \bar{\mathbf{D}}_i(\mathbf{u}) > \widetilde{\mathbf{D}}_i(\mathbf{u})$ 
\EndFor


\State \textbf{return} $M, \bar{\mathbf{D}}_i$ 
\end{algorithmic}
\label{algo:surfawaremask}

\end{algorithm}



\subsection{Metrics}
We also include additional information about the metrics we use for quantitative results in our paper: 

\textbf{Intersection-over-Union (IoU)}:
We voxelize the ground truth and predicted point clouds at a fixed resolution and compute the IoU score by dividing the number of voxels that intersect to that of their union. In our experiments, we evaluate all the methods at the same grid resolution of $100^3$ after rescaling the predictions and ground truth to fit into the unit cube. \textbf{Chamfer Distance (CD)}: Chamfer distance is commonly used to measure the similarity between two point sets and is defined as:\vspace{-2mm}
\begin{equation}
    CD(X, Y) = \frac{1}{|X|} \sum_{\mathbf{x} \in X} \min_{\mathbf{y} \in Y} ||\mathbf{x} - \mathbf{y}||_2
\end{equation}
We separately report $CD(S, S^*)$ and $CD(S^*, S)$, as well as their their sum. $CD(S, S^*)$ measures how close the reconstructed points from $S$ are to the ground truth points $S^*$, whereas $CD(S^*, S)$ computes how well the ground truth shape is covered. \textbf{F-Score}: Following ~\cite{tatarchenko2019single}, we also report F-Score$@1\%$ which is a measure for the percentage of the surface points that were reconstructed correctly.

\subsection{Parameter Grid Search Experiment}
For choosing the number of viewpoints/viewpoint directions \(V\) as well as the context ratio \(C^*\) described in our method section, we perform a parameter search using 4 held out validation scenes from the YCB-V test set. We test using 6, 8, 10, and 12 viewpoints as well as a value of 0.3, 0.4, 0.5, 0.6, and 0.7 for our context threshold. We found that 10 views and 0.4 as a context threshold gave us the best accuracy on the validation set. 12 views and 0.4 as a context threshold performed similarly, but in the interest of runtime we use 10 for the final method. Table~\ref{tab:grid-search} shows the full results from this experiment.

\begin{table}[h!]
\vspace{-2mm}
% \begin{adjustbox}{width=\columnwidth,center}
\centering
\begin{tabular}{l|c c c c c}
\toprule
% \diagbox{\(V\)}{\(C^*\)} & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\
\(V\)~$\vert$~\(C^*\) & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 \\
\midrule
6 & 0.064 & 0.053 & 0.051 & 0.057 & 0.064 \\
8 & 0.057 & 0.048 & 0.050 & 0.059 & 0.066 \\
10 & 0.053 & \textbf{0.047} & 0.052 & 0.059 & 0.070 \\
12 & 0.052 & \textbf{0.047} & 0.052 & 0.063 & 0.071 \\
\bottomrule
\end{tabular}
% \end{adjustbox}
 \caption{Experiment using different values for context \(C^*\) and number of viewpoints \(V\) for our method on 4 validation scenes of the YCB-V~\cite{xiang2018posecnn} dataset using Chamfer Distance to indicate better performance.}
    \label{tab:grid-search}
    \vspace{-5mm}
\end{table}

% \subsection{Qualitative Comparison Results}
% % Figure environment removed