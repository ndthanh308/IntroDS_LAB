\documentclass{EnrichEvent}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}

%%\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.

\begin{document}
\begin{frontmatter}


\title{EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction}

% \author[A]{\fnms{Mohammadali}~\snm{Sefidi Esfahani}\thanks{Corresponding Author. Email: mohammadali.esfahani@aut.ac.ir}}
\author[A]{\fnms{Mohammadali}~\snm{Sefidi Esfahani}\orcid{0009-0007-0285-1545}\thanks{Corresponding Author. Email: mohammadali.esfahani@aut.ac.ir}}
% \author[B]{\fnms{Mohammad}~\snm{Akbari}}
\author[B]{\fnms{Mohammad}~\snm{Akbari}\orcid{0000-0002-3321-5775}}
\address[A]{Department of Mathematics and Computer Science, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran}
\address[B]{Department of Mathematics and Computer Science, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran}
% \author[A]{\fnms{Anonymous}~\snm{Author(s)}}
% \address[A]{Paper ID : 950}

\begin{abstract}
Social platforms have emerged as a crucial platform for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. However, most existing approaches for event detection solely exploit keyword burstiness or network structures to detect hot events. Thus, they often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to detect semantically related tweets in their earliest emergence and enhance the quality of produced clusters. We next produce a cluster chains for each event to show the evolving variation of the event through time. We conducted extensive experiments to evaluate our framework, validating the effectiveness of the proposed framework in detecting and distinguishing social events. 

%Social platforms have emerged as a crucial platforms for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. Most existing approaches for event detection merely exploit keyword burstiness or network structures to detect hot events. They often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, especially tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to detect semantically related tweets as soon as possible and enhance the quality of produced clusters. We conducted several experiments to evaluate our framework, and the experimental results demonstrate the effectiveness of our proposed framework in detecting and distinguishing social events. 

%Social media platforms with billions of active users worldwide have become an integral part of our daily life. As a result, social media has emerged as a crucial platform for discussing and disseminating information about significant events. Real-life events are occurring and evolving in social data streams, and it offers an excellent opportunity for early detection of newsworthy events. However, event detection in social media faces challenges, such as its streaming and evolutionary nature and the variation in language expressions and aspects of opinions. This paper addresses these challenges and discusses the importance of quick and accurate event extraction in social media for various purposes, such as disaster management, marketing, and financial markets. Most existing methods, including those based on community detection, utilize limited knowledge as they don't consider the semantics and contextual aspects of social data streams. In this paper, we make full use of lexical, semantic, and contextual representations of tweets. We propose a novel framework that gradually groups related tweets into clusters, and by connecting them over consecutive time frames, it identifies the events. We conduct experiments, and the results demonstrate the superiority and effectiveness of our proposed framework.
\end{abstract}
\end{frontmatter}

% \begin{keyword}Event detection \sep%
%     Twitter \sep% 
%     Real-time data \sep% 
%     Online social networks \sep% 
% \end{keyword}


\section{Introduction}
% Background one of these two para
% Social media platforms have been incorporated into our daily lifestyle for socializing with friends, disseminating information, discussing events, and learning new concepts, with millions of people from all over the globe are highly active in participating on different social networks. There are currently about $5.4$ billion active users on the internet and among them, around $4.5$ billion of them have more than one account in one of the existing platforms~\cite{find-ref}. As such, when an event occurs in society, both experts and grassroots users start sharing and discussing around different aspects of the event on social media (e.g., Twitter discussions on the Notre-Dame Cathedral fire). This active presence of users has turned social media into one of the most important sources for identifying, analyzing, and investigating events providing live updates on important events. With this sheer volume of live data available on social platforms, early detection of events from social platforms presents great opportunities to gain real-time information around newsworthy events before these are covered by conventional broadcast media channels. Consequently, the development of platforms for discovering social event and understanding the real-time discussions happening has been attracted considerable attention in both academia and the industry.

Social media platforms have become an integral part of our daily routines for communicating with friends, sharing information, exchanging ideas, and acquiring knowledge~\cite{Chen2013EmergingTD}. There is massive participation from people worldwide in social media. Social network statisticians expect the number of active social media users worldwide to increase from 2.86 billion in 2017 to 4.41 billion in 2025~\cite {SocialMedia_Statistics}. As a result, social media has emerged as a crucial platform for discussing and disseminating information about significant events. When a social incident occurs, experts and ordinary users engage in discussions on social media platforms (e.g., Twitter discussions on the Notre-Dame Cathedral fire) from various perspectives~\cite{Aldhaheri_2017, Sun_Xiang_Wu_2015}. The active presence of users on social media has transformed it into a primary source for identifying, analyzing, and investigating events, providing real-time updates on critical incidents. The abundance of live data on social platforms offers an excellent opportunity for early detection of newsworthy events before conventional broadcast media channels cover them. Therefore, academia and industry have demonstrated significant interest in developing platforms that can discover social events and understand real-time discussions happening on social media.
 
%Importance of the research
Traditionally, an \emph{event} is defined as an important happening in a specific time and place~\cite{allan1998topic}. With the emergence of social media, this definition expands to a happening in society that attracts a sudden burst of attention in social media, and similar to the previous explanation, it occurs in a specific time and place~\cite{boettcher2012eventradar}. Intuitively, when an event occurs, quick and accurate event identification is one of the most crucial concerns for the government, aid agencies, relative organizations, and even for journalists and experts~\cite{Governments, HUANG202111}. For instance, when a natural disaster occurs, relief organizations consider event detection as one of the ways to capture different types of information, such as the severity of the incident, the number of victims, and how to provide aid for them during natural disasters. These related organizations are keen on being informed of an event occurrence as soon as possible to take appropriate actions regarding the event~\cite{Bursty_Disaster_Management, nugent2017comparison, pekar2020early, xiao2015understanding, guan2014using, yun2011disaster, sreenivasulu2020comparative, Earthquake_Shakes_Twitter_Users}. Besides, being conscious of event occurrence is vital for other purposes such as marketing, commercial, and even financial markets~\cite{TETLOCK, Mohan, Shah, Engle}. For instance, leading commercial brands always want to discover what's happening all over the market, and this awareness accompanying the right decisions puts them on the path to be ahead of their rivals.

Despite its value and significance, quick and accurate extraction of social events has not been fully investigated due to the following challenges. 1)~\textbf{Streaming and Evolutionary}. Social media services provide a streaming source of information in which people freely create content around real-world events. While processing streaming sources, per se, is a challenging and opening problem, the process of turning a simple happening into a hot event which everyone is going to talk about, is a complex and evolutionary phenomenon comprised of major steps: of a) happening the event in the real-world b) starting a discussion on social platforms by observers, journalists, and people. c) fading out the event or turning on into a hot event for a while~\cite{Chen2013EmergingTD}. How to model evolving characteristics of an event as well as early event detection, is an arduous task. 2) \textbf{Variation in Language Expressions and Aspects of Opinions}. Twitter users produce and consume information in a very informal and irregular way, and the published tweets usually include idioms, abbreviations, misspellings, irregular language, and emojis. Due to the huge and diverse user base, Twitter usually contains different words and word sequences describing the same idea. Also, in most languages, many words have various meanings in different contexts. For example, the word "fair" in a different context has multiple meanings of "carnival", "treating someone right", and "having light skin/hair". This issue makes it so hard for machine learning models to understand users' actual purpose. Besides, users may have different opinions about occurring events and use various contexts to express them. How to detect and aggregate various opinions and aspects of one event is another challenge. 


% According to the text-based structure of Twitter, this social network is one of the main platforms for discussing different types of events. However, some characteristics of Twitter make the problem of event detection more challenging. The first one is the short length of tweets. It's common among users to write short tweets and this shortness makes it harder for the NLP models to consider the contextual aspect of tweets. The second one is the informal use of language. On Twitter, users write tweets in a very different style compared with other media. Misspellings, abbreviations, and slang are common in tweets and when these problems and shortness come together, event detection becomes more challenging. The third challenging aspect of Twitter is its dynamic nature. On Twitter, users generate data streams at every moment and as it's debated in the previous paragraph. So we can't ignore time and detect events statically in a single moment. The last challenging aspect of Twitter is the existence of noise in tweets. Not only there's no guarantee that all the tweets are relevant, but also most of them are irrelevant, and even those which are relevant may contain irrelevant terms.

%But something that's not considered in these definitions is the process of turning simple happenings into hot events which everyone is going to talk about them. In other words, hot events don't emerge in a moment and don't die a while later. At any moment, many happenings occur all over the world, but few of them turn into hot events and this evolutionary process should be considered. Most of the time, this evolutionary process comes in 3 steps. 1)Something happens somewhere in the world. 2)People, journalists, and … start talking about it and sharing with others via different platforms and media based on the importance of the event. The more people share the event, the more trending it will be. 3)At last, some events will disappear after a while, some of them become hot events in a local area and some of them even become real trending events, and people from all over the world, will be hearing about them. 

% should be enhanced
Retrospective studies in social media and event detection have proposed practical approaches for identifying events from social data by leveraging content and network structure. For instance, frequent keyword extraction has been used for detecting bursty keywords corresponding to an event, and hashtags have been leveraged for capturing semantics from tweets to address the short context problem~\cite{Keyword_Volume_Approach, Yang2018AnED}. Similarly, clustering methods have been employed for mitigating short and sparse contexts in social posts~\cite{akbari2017leveraging}. Further, modular decomposition and community detection have been exploited to utilize links and network structures. While a large body of work has been devoted to event detection, limited research tackled the problem of emergent event detection and its evolution over time. In this paper, we design an end-to-end framework, namely \textbf{EnrichEvent}, to address the challenges mentioned in the previous paragraph. We define an event as a chain of clusters where each cluster contains closely related tweets both in time and semantic dimensions. In addition, we leverage contextual knowledge as well as structural aspects of the tweets to enrich their representation and achieve better results. Exploiting contextual knowledge assists the model in comprehending the relation of tweets and producing chains of clusters with high quality, i.e., clusters containing tweets discussing a specific topic. We conducted several experiments to evaluate all the components of our proposed framework. The empirical results validate that EnrichEvent achieves better results compared with various baselines based on the evaluation metrics. In summary, our main contributions are as follows:
\begin{itemize}
    \item We exploit the contextual information alongside the lexical representation of name entities to reach a better representation of an event and enhance aggregation of social posts.  
    \item We design a novel framework, EnrichEvent, that is robust to variation in language expressions and aspects of opinions and considers the temporal variation of context around a given event to better model the evolutionary process of events happenings.
    \item  We provide two real-world datasets for training the trend detection model and performing experiments on our event detection framework. The result empirically demonstrates the effectiveness of EnrichEvent.
\end{itemize}

% In this paper, We consider and add the concept of evolutionary events to the problem of event detection from Twitter. Although some recent research considers these points but context is the key point that is missed in event detection. While designing the event detection models, most of the recent research didn't consider the context and semantic aspects of the tweets and they only focus on the lexical aspects. Event detection models which implemented only based on bursty features or those that focus on frequency and co-occurrence of keywords are examples of models which don't consider the context.

\section{Related Works}
Event detection from Twitter has received noticeable attention in recent years, as it introduced TwitterAPI, and researchers can easily collect real-time data~\cite{Survey_2016, ED_SWE}. We can divide event identification methods into three categories. Identification of \textbf{unspecified events}, \textbf{predetermined events}, and \textbf{specific events}~\cite{atefeh2015survey}. While working on the first category, there's no antecedent description of the events, and they aim to detect general events. In this category, researchers typically extract features from tweets and then try to find trends in them to detect the events~\cite{Long2011TowardsEE, Becker_Naaman_Gravano_2021}. In the second category, researchers aim to detect predefined events with a fixed category, such as earthquake~\cite{Earthquake_Shakes_Twitter_Users}, crime and disaster events~\cite{TEDAS}, etc. In the third category, the event's details are specified, and the goal is to detect events that exactly match an explicit description of the relevant event types~\cite{Planned_Events}. Our work belongs to the first category, and in this section, we investigate these three categories and explain some of the related works that have motivated us to work in this field.


\subsection{Identification of unspecified events}
\textbf{Halterman et al.}~\cite{halterman2021extracting} presented a 2 step method for political event identification. In the first step, a CNN-based classifier was used to identify political events statically, and it didn't consider the evolutionary process of event generation. In other words, the classifier only classifies whether the tweet referred to an important political event or not. In the second step, they extract the detail of happenings based on grammatical aspects of the tweets.

The main idea of~\textbf{Yang et al.}~\cite{Yang2018AnED} is to cluster similar hashtags together, and clusters of hashtags represent occurring events. They focused on the occurrence of hashtags in tweets and considered hashtags as a bag of words and hashtags. Eventually, they used the concatenation of hashtag-hashtag and hashtag-Words co-occurrence vectors as a final feature vector for clustering. The main challenge of this work is that users may not necessarily use hashtags in all tweets. Besides, hashtags used in tweets are not necessarily relevant hashtags.

~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter} considers happening events as chains of clusters. They built a real-time system to identify groups of event-related entities. Then, they linked these clusters together and generated cluster chains that represent the events. Note that this paper only focused on the lexical representation of name entities and used occurrence vectors to cluster them.

\subsection{Identification of predetermined events}
Commonly in this category, researchers leverage datasets such as ACE 2005, Event2012, and Event 2018, which contain predefined events in different event categories. Recently, numerous frameworks have been designed, .i.g., FinEvent~\cite{peng2022reinforced}, PP-GCN~\cite{peng2021streaming}, KP-GNN~\cite{Knowledge_Preserving}, and QS-GNN~\cite{Qsgnn}. Intuitively, these papers model the existing relations between tweets using message graphs in their ways. Eventually, they exploit GNN models to identify the occurring events. Note that most of the existing papers in this category divide the training into two stages. (1) \textbf{Pre-training} stage in which they train models with labeled message graph (obtained message graph from the first time frame). (2) In \textbf{fine-tuning} stage, they utilize unlabeled message graphs and try to generate high-quality pseudo labels.

\subsection{Identification of specific events}
\textbf{Becker et. al}~\cite{Planned_Events} leveraged various user-contributed event features to identify upcoming concerts. They also utilized term frequency-based techniques for selecting representative and descriptive event terms, which serve as complementary queries for retrieving event messages from Twitter. In summary, they have developed a system that returns Twitter messages related to a specific event using our query-building strategies. Note that few papers have been published in this category recently.

\section{Problem Statement}
In this section, first, we present the notation and then formally define the problem of emergent event detection from social media data.
Let $\mathcal{T} = {t_1, t_2, \ldots, t_n }$ denote an order set of $n$ different tweets, i.e., a social stream, where each $t_i$ is associated with a timestamp indicating its posting time. We assume that these tweets are coming in $m$ different blocks of messages $\mathcal{B} = {b_1, b_2, \ldots, b_m }$. A \emph{social event} is formally defined as a set of semantically related tweets where all tweets refer to a real-world event that happened in a specific time point, $e_i = { t_1, t_2, \ldots, t_k}$. Denotes that while an event $e_i$ happened in a specific time point, it usually evolves over longer periods, and includes several message blocks normally. The task addressed in this paper is to detect a set of unspecified events $\mathcal{E} = {e_1, e_2, \ldots, e_z}$ based on available social stream, i.e., $\mathcal{T}$. With the notation above, we formally define the emerging event detection problem as follows: 

Given a social stream of microblog messages $\mathcal{T}$ with their content, we aim to learn a model as follows,
\begin{align}
    \mathrm{M}: \mathcal{T} \rightarrow {e_i}_{i=1}^{z},
\end{align}
where each $e_i$ is a sequence of message clusters $c_i$ each formed by messages in one social block. 

% To solve these challenges and enhance recent works, we tried to implement a real-time model that considers the evolutionary process of event generation. In addition, our focus on contextual features improves the performance of the model and also the quality of detected events. In time frame \(W\), our model tries to extract important and determinant name entities and cluster relative name entities together. Thus, in time frame \(W\), we have some clusters which contain determinant and relative name-entities. We repeat this approach to time frame \(W_1,W_2,…,W_n\). At the moment, we have n time frames and corresponding to each of them, there are some clusters consisting of important and relative name entities. Now it’s time to connect clusters in time frame \(W_i\) to clusters in time frame \(W_{i+1}\) together and create a cluster chain. This chain of clusters demonstrates the evolutionary process of events. Moreover, using a combination of semantic, contextual, and bursty features for detecting determinant and important name entities enhance the quality of the cluster chain and help us to have a better event-detection model.

% Assume that we have finite number of tweets T=$\{t_1,t_2,\dots,t_n\}$ in a specific time frame \(W\) in which finite number of events E=$\{e_1,e_2,\dots,e_m\}$ occurs where \(m \leq n\). Our task is to design a pipeline that detect all the occurring events in the incoming tweets using both structural and contextual aspects. In this pipeline, we want to cluster these tweets in the way that related tweets are clustered together. At the end of the pipeline, we generate cluster chains and each of them represent a happening event.

\section{Methodology}
In this section, we present various components of our proposed framework. The main advantage of the proposed framework is  capability of processing streaming social data and extracting candidate social events in a near online setting. Here, we first go through the different components of the EnrichEvent. Afterward, we define their roles and functionality and then delve into the details of each unit. You can glance at the architecture of EnrichEvent in Figure 1.

% Figure environment removed

\subsection{Framework}
% The proposed framework focuses on the need for an end-to-end pipeline that can handle the streaming nature of social media data by processing incoming data and generating streams of outputs. Note that events start occurring at specific time points and can extend over multiple time windows, requiring a framework capable of processing a stream of tweets and returning cluster chains that represent these events.

% The streaming nature of social media data needs an end-to-end pipeline capable of processing incoming data and forming streams of outputs, as we focused on in this paper. Recall that events are happening at a specific time point, and the discussion can be extended for several time windows. This setting demands a framework with the capability of processing a stream of tweets comes and returning cluster chains that represent the happening events. The proposed framework consists of four components: trending data extraction, contextual knowledge--segment--anything else enhancement, event clustering, and event chain formation. The first step, i.e, trending data extraction distillates newsworthy tweets potentially referring to an event. After extracting the potential pool of tweets, the tweet block sends into the Contextual Feature Enhancement to extract and enrich with semantic features assisting the model to correlate tweets corresponding to an event but from diverging aspects. Event Clustering, as inspired by previous studies in~\cite{ref1, ref2}, clusters semantically related tweets to groups, i.e., perform event detection. The later step, i.e., chain construction, attempts to create the evolving chain of events through various time windows. 

In this paper, we focus on the need for an end-to-end pipeline that can handle the streaming nature of social media data by processing incoming data and generating streams of outputs. Note that events start at a specific time point and can extend over multiple time windows, requiring a framework capable of processing a stream of tweets and returning cluster chains that represent these events. The proposed framework, i.e., \textbf{EnrichEvent}, comprises four main components: \textbf{Trending Data Extraction}, \textbf{Contextual Knowledge Enhancement}, \textbf{Event Clustering}, and \textbf{Event Chain Formation}. First, Trending Data Extraction identifies newsworthy tweets that potentially refer to an occurring event. The tweet block is then sent to Contextual Knowledge Enhancement to extract and enrich semantic features that assist the model in correlating tweets related to an event from different aspects. Inspired by previous studies ~\cite{RealTime_EventDetection_Twitter, Bursty_Event_Detection}, Event Clustering groups the semantically related tweets into groups, i.e., detecting events. Ultimately, Event Chain Formation links the clusters in consecutive time steps and creates an evolving chain of events over various time windows.

% Assume that we want to detect events in time frames \(W_1,W_2,\dots,W_n\). We start from time frame \(W_{1}\) and a stream of tweets comes into the pipeline. First of all, this stream of tweets enters the trend detection component. For this component, we use a CNN-based model to predict the probability of referring to an important event in the incoming tweet. By using this component, we can filter the incoming tweets and reduce the computational cost in the next components. Then in the next component, all the name entities in the filtered tweets are extracted. we use Bert's pre-trained NER model to extract name entities. Besides, we also add hashtags to our extracted name entities. To cluster extracted name entities into different clusters, we have to generate a feature vector for them. To consider both structural and contextual features, we generate occurrence and embedding vectors for each name entity and eventually, we use the concatenation of occurrence and embedding vectors as our final feature matrix. In the next component, We use generated feature vectors to cluster these name entities using HDBSCAN. We repeat the previous steps for all the time frames. At this stage, we have a number of clusters for each time frame and we have to link clusters in time frame \(W_i\) to clusters in time frame \(W_{i+1}\). To do this, we use a maximum bipartite matching algorithm to create cluster chains. At the end of this pipeline, we construct cluster chains each of which represents events that occurred in time frames \(W_1,W_2,\dots,W_n\).

% candidate tweet filtering/gathering, candidate event clusters here we need to represent each tweets, i.e. feature extraction, event chain prediction,

\subsection{Trending Data Extraction}

Social media data is often known as a noisy and sparse information source, and $40\%$ of tweets refer to \emph{pointless babble}. Thus it is crucial to filter out irrelevant data and extract tweets corresponding to events~\cite{SEDTW}. In other words, we had to appropriately filter out noisy data and gather tweets corresponding to a specific event. To do so, we proposed to filter general tweets and accumulate those with a high probability of referring to an event. Here, we leverage a supervised learning model to mark tweets potentially referring to events. 

We start preparing an offline event detection dataset that contains $1.6$ million Persian tweets labeled manually based on key phrases. Intuitively, our model examines certain parts of the tweet to determine whether it refers to an important event. We achieved this through a two-segment architecture. In the first segment, a combination of embedding, convolution, and max pooling layers extract efficient features from the input tweet. In the second segment, we used fully connected layers to classify whether the incoming tweet refers to an important event or not. Eventually, the model returns the probability of the tweet referring to an event.

Inspired by retrospective studies~\cite{Chen2013EmergingTD}, we adopt a supervised learning approach and train a model $\mathcal{M}$ that identifies tweets with a high probability of referring to an event. This model gains features extracted from tweets as input vectors, i.e., $X$, and computes the probability of being general information or a simple status update, i.e., negative class, or referring to a potential event, i.e., positive class. Formally, the prediction of model $\mathcal{M}$ is performed by solving the following optimization problem:

% \begin{equation}
% \begin{split}
% \begin{gathered}
%    f: x \rightarrow y \in \{0, 1\} \quad \text{s.t.} \quad f(x) = y \\
%    \text{Objective:} \quad \min_{\theta} BinaryCrossEntropy(y, f(x)) \\
% \end{gathered}
% \end{split}
% \end{equation}



\begin{align}
   \arg \min_\mathbf{W} \mathcal{L}(\mathbf{X}, \mathbf{W}, \mathbf{Y}),
\end{align}
where $\mathcal{L}(.)$ is the loss function and often is defined as binary cross entropy. $\mathbf{W}$ denotes all parameters that the model learns from the training set, and $\mathbf{Y}$ denotes the label of the tweet where one is used for tweets referring to an event, otherwise zero. 

For learning an effective model, we ought to extract a prominent and discriminative representation from tweets using pre-trained models. As such, Given a tweet, we extract two representations from them: (1)~\textbf{Bert sentence embedding}~\cite{ParsBERT}, to achieve a decent understanding of the context, we used Bert's pre-trained model to obtain the tweets' representation. While training the model, we use this representation in the embedding layer. (2)~\textbf{Word2vec}~\cite{word2vec}, to gain another perspective on tweets, we train a word2vec model $g(token)$ on the whole dataset. Let $ (\mathcal{W})=\{w_1, w_2, \ldots, w_m\}$ denote all the words in one tweet. We consider averages of the word2vec embedding of the words as their representation. Therefore, we apply this model to all the existing tweets in the train, test, and validation dataset, and we use word2vec representation as an input vector for the trend detection model as follows:

\begin{align}
\label{eq:embedding}
    x_i = \frac{1}{m}\sum_{j=1}^{m} g(w_j) \ , w_j \in W_i .
\end{align}

% \begin{align}
% \label{eq:embedding}
%     \forall \ tweet \in dataset \qquad x_i = \frac{1}{m}\sum_{j=1}^{m} g(w_j) \ , w_j \in W_i .
% \end{align}


% Eventually, we train our trend detection model $f(x)$ on the dataset for a specified number of epochs. In the time frame $W_i$, we can use this model to filter incoming tweets based on a minimum threshold $r$. This component helps to identify relevant tweets and filter out tweets that refer to happening events in the time window.

% \begin{equation}
% \begin{split}
% \begin{gathered}
%    f: x \rightarrow y \in \{0, 1\} \quad \text{s.t.} \quad f(x) = y \\
%    \text{Objective:} \quad \min_{\theta} BinaryCrossEntropy(y, f(x)) \\
% \end{gathered}
% \end{split}
% \end{equation}

%As data is available in blocks of messages corresponding to different time windows, i.e, $W_i$. Further, retrospective research studies demonstrate that social media data is noisy and sparse and so many incoming tweets refer to ``pointless babbles''~\cite{SEDTpapeer}. Thus we need to properly filter out noisy data and gather tweets corresponding to events. We proposed to filter general tweets and gather those  which the probability of referring to an event is really high. To handle this issue, we have prepared an offline event detection dataset consist of 1.5 million Persian tweets in different fields that have been labeled manually based on key phrases. Assume that we have a tweet and we want to check whether a user refers to an important happening or not. In most cases, the user talks about an important happening in a certain part of the tweet, and only by examining that certain part we can identify that important happening. In some other cases, the tweet is too short and the user talks about an important happening in the whole tweet. ‌Briefly, important happenings appear in the local part of the tweet and locality is our reason for using CNN. we can divide the architecture into 2 segments. In the first segment, we use a mixture of embedding, convolution, and max pooling layers to extract efficient features from the input. In the second segment, we use fully connected layers to classify whether the input tweet refers to an important happening or not. In summary, The implemented model gets a tweet as input and returns the probability of referring to an event. Then, We have trained our CNN-based classifier on the dataset for xxx epochs. So in time frame \(W\), by using this model, we can filter the incoming tweets based on a minimum threshold \(r\).

% \subsection{Entity Extraction}
% % In this module, we extract all the existing name entities from the filtered tweets. In this stage, we use ParsBert pre-trained NER model which gets all the filtered tweets and it returns all the existing name entities in time frame \(W\). Furthermore, in time frame \(W\), we also extract all the hashtags manually and we consider them as name entities too. We store all the name entities in a dictionary for further experiments and analysis. In the next step, we have to generate feature vectors for all the name entities.

% In this work, we considered entities in a tweet as its representative and we focus on two main types of entities:
% \begin{itemize}
%     \item Named Entity such as "UNICEF"
%     \item Hashtags such as "\#PROTEST"
% \end{itemize}

% In this stage, We use Bert's pre-trained NER model to extract all the existing named entities from all the tweets in the time frame \(W\). Eventually, we add all the posted hashtags to the extracted named entities.

\subsection{Contextual Knowledge Enhancement}
% For all the extracted name entities in time frame \(W\), we generate 2 feature matrices. Assume that we have \(n\) tweets and we have extracted \(k\) name entities from them. First of all, we calculate the occurrence vectors for all the extracted name entities. To calculate the occurrence vectors, we count the frequency and occurrence of name entities in each of the existing tweets. Then, we construct the occurrence matrix in which the element \(e_{ij}\) shows the frequency of name-entity \(k_i\) in tweet \(t_j\). Therefore, the dimensions of the obtained matrix will be \(k*n\). second, to consider the context in which the extracted name entities appear, we define an embedding matrix. To calculate the embedding matrix for all name-entities \(k_i\), we concatenate top k tweets with the highest number of retweet in which name entity \(k_i\) appears. Then, we use ParsBert pre-trained model to calculate the embedding vector of generated text and we consider the output as an embedding vector for name entity \(k_i\). At last, we construct an embedding matrix in which row \(i^{th}\) is the embedding vector of \(i^{th}\) name-entity.
% In the next step, we convert both occurrence and embedding matrices to similarity matrices, respectively, using the cosine similarity function. Each of the similarity matrices represents the similarity of named entities to each other. In other words, element \([ij]\) represents the similarity of name-entity \(k_i\) to name-entity \(k_j\). we should mention that we consider a threshold for the minimum similarity in both generated similarity matrices. Eventually, we use the concatenation of occurrence and embedding matrix as our final feature vector.

Working on lexical and structural aspects of the tweets is a conventional way to design event detection models, but because of the challenging characteristic of the tweets, i.e., variation in language expressions and aspects of opinions, learning a well-performing model is arduous. This is owing to the fact that when users discuss a happening event on social platforms, they debate different aspects and have diverse opinions. Therefore, we should consider different contexts in which users post tweets, and our goal is to generate a high-quality representation for them.

In this work, we considered entities within a tweet as its representative, and we concentrated on two main types of entities: Named Entities and Hashtags. Named entities are critical indicators for detecting events as an event happens at a specific time and/or location and includes various actors. As such, named entities demonstrate one of these attributes of an event. Hashtags, on the other hand, were incorporated into social posts intrinsically to show correlation to an event or a topic, making them invaluable indicators of tweets related to an event. 

For instance, the following text from our dataset contains both named entities and hashtags. The named entities indicate that the attributes of the event are \textbf{Silicon Valley}, \textbf{USA}, and \textbf{2008}. In addition, \textbf{\#bankruptcy} demonstrates that the event corresponds to bankruptcy.

% \textcolor{red}{I think next two should be incorporated into an example}
% \begin{itemize}
%     \item Named Entity such as "UNICEF"
%     \item Hashtags such as "\#PROTEST"
% \end{itemize}

\begin{itemize}
    \item \textbf{Silicon Valley} Bank of the \textbf{United States} collapsed! The largest bank \textbf{\#bankruptcy} in the \textbf{USA} since the crisis of \textbf{2008} !!!
\end{itemize}



In this stage, We use Bert's pre-trained NER model to extract all the existing named entities from all the tweets in the time frame, i.e.,  $(W_i)$. Afterward, we add all the posted hashtags to the extracted named entities. Assume that time frame $(W_i)$ contains $n$ tweets and there exist $k$ unique entities within them. Then, For all the extracted name entities in time frame $(W_i)$, we follow a three-step procedure: 

% \subsubsection{Occurrence Matrix}
% \subsubsection{Lexical aspects}
(1) First, to cover lexical aspects of the tweets, we consider name entity $k$ as a bag of tweets which is an aggregation of all the tweets that contain $k$. Then, we construct the occurrence matrix in which the element $e_{ij}$ shows the frequency of name entity $k_i$ in tweet $t_j$. Therefore, the dimensions of the obtained matrix is $k \times n$. 

% \subsubsection{Sentence Embedding}
% (\subsubsection{Semantic and Contextual aspects}
(2) Sentence embedding is a type of sentence representation that allows sentences with similar meanings to have a close representation. This idea helps us to consider the context in which the extracted name entities appear. To calculate the embedding matrix for all name entities \(k_i\), we concatenate the top k retweeted tweets in which name entity \(k_i\) appears. Then, we use ParsBert pre-trained models~\cite{ParsBERT} to calculate the embedding vector of generated text. At last, we construct an embedding matrix in which row \(i^{th}\) is the contextual representation of \(i^{th}\) name-entity.

% \subsubsection{Feature Generation}
(3) By using the mentioned representations, we simultaneously consider lexical and contextual aspects of the tweets. In this step, we convert occurrence and embedding matrices to similarity matrices using the cosine similarity function. Using similarity matrices enhances clusters' quality and helps us to have a more discriminative event identification. Each of the similarity matrices demonstrates the similarity of name entities to each other. In other words, element \([ij]\) represents the similarity of name-entity \(k_i\) to name-entity \(k_j\). Note that we consider a threshold for the minimum similarity in generated similarity matrices. Eventually, we use the concatenation of lexical and contextual representation as our final feature vector.

\subsection{Event Clustering}
% After generating the feature vector for each name entity in time frame \(W\), we have to cluster name entities. Although we had other options for the clustering model such as KMeans, GMM, \dots but there's a point that the number of clusters may be different in each time frame and we shouldn't use a constant number of clusters for all the time frames. So, in this case, we have 2 options. Our first option is to use algorithms such as elbow method to obtain the optimal number of clusters along with clustering models where the number of clusters should be set. The second option is to use clustering models in which there is no need to specify the number of clusters such as OPTICS, HDBSCAN, \dots . Finally, we select the second option and use HDBSCAN for this component and our goal is to cluster name entities in a way that related name entities are clustered together.

In this work, we defined \emph{event} as linked clusters of related tweets over consecutive time steps. Since name entities are representative of the tweets, at this stage, we have to cluster name entities based on the generated representation. Within this component, we can use a wide range of clustering algorithms such as distance-based clustering algorithms, e.g., K-Means, probability-based clustering algorithms, e.g., GMM, and density-based ones, e.g., DBSCAN and HDBSCAN~\cite{HDBSCAN}. Since we are working on streaming data, and the number of clusters may vary in each time frame, algorithms that don't need to specify the number of clusters are the best option for us. Therefore, we use HDBSCAN for this component, and our goal is to cluster name entities in a way that related name entities are in the same cluster.

\subsection{Event Chain Formation}
Since events occur in an evolutionary process, we couldn't identify the events statically, and we should track the evolution of events over time frames. So far, by utilizing the Event Clustering component in each time frame \(W_i\), we have produced number of clusters consist of related name entities. As a final step, we connect these clusters in time frame \(W_i\) to clusters in time frame \(W_{i-1}\) for \(i\)=$ \{2, 3, \ldots, n\}$ where n denote the total number of time frames. 

Let \(C_i\)=$\{c_1, c_2, \ldots, c_k\}$ and \(C_{i-1}\)=$ \{c_1, c_2, \ldots, c_t\}$ denote produced clusters in time frame \(W_i\) and \(W_{i-1}\) respectively. In this step, we start building a bipartite graph with clusters in time frame \(W_i\) and \(W_{i-1}\), i.e., \(C_i\) and \(C_{i-1}\). We consider clusters as nodes of the graph, and we add $edge_{tk}$ between them if the number of common entities among \(c_k\) and \(c_t\) were more than a threshold. Note that the weight of \(edge_{tk}\) is the number of common entities. Then, we use the maximum weighted bipartite matching algorithm~\cite{kuhn1955hungarian} to link clusters in time frame \(W_i\) to clusters in time frame \(W_{i-1}\). Intuitively, this algorithm tries to build the chains while we have the maximum flow of common entities. As the output of this framework, we construct cluster chains, and each of them represents the events that occurred in time frames \(W_1, W_2,\dots, W_n\).

\section{Experiments and Evaluation Pipeline}
% In this section, we present an evaluation pipeline that appraises different aspects of the proposed method. After running the pipeline, we evaluate the performance of the system from the following aspects.
In this section, we conduct extensive experiments to answer the following research questions:
\textbf{(1)} How is the overall performance of our framework compared with the baselines?
\textbf{(2)} How does the Contextual Knowledge Enhancement component affect the clusters' quality?
\textbf{(3)} How does the Contextual Knowledge Enhancement component help us to identify events with high credibility?
\textbf{(4)} How is the overall performance of the Trending Data Extraction component, and how does it help us to achieve better results?

\subsection{Manual Evaluation}
\subsubsection{Baselines}
We compare our proposed framework to both base algorithms and related works. (1) \textbf{Word2vec}~\cite{word2vec}, which considers averages of the word2vec embedding of words as their representation. (2) \textbf{Bert}~\cite{BERT}, which uses a 768-d embedding vector for the sampled sentences in which the entity occurs as its representative. (3)~\textbf{Yang et al.}~\cite{Yang2018AnED}, which considers hashtags as their main entity and uses the concatenation of hashtag-hashtags and hashtag-words co-occurrence vectors as their final feature. (4)~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter}, which builds a graph using the similarity matrix obtained from the co-occurrence vector of entities. Eventually, they identify the events by applying Louvain community detection to the graph and building cluster chains. Note that we performed all the evaluations in the same setting.

\subsubsection{Evaluation Metrics}
% In our proposed method, events appear in a form of cluster chains, and each cluster chain represents an event. 
% To evaluate cluster chains, we use consolidation, discrimination, and clustering score that has been presented in [baseline]. Consolidation only focuses on detecting all the happening events in the dataset and defined as the following formula where ...\textbf{fig consolidation} In contrast, discrimination measures how well the unique events are separated from each other and clustering score will be obtained by combining these two metrics. In the following formulas ... Our goal is to optimize consolidation and discrimination simultaneously.

We evaluated the Quality of cluster chains by three metrics presented in Fedoryszak et al.~\cite{RealTime_EventDetection_Twitter}. By measuring these metrics, we can figure out how well the events are detected and separated from each other. Note that if the higher value of the metric indicates better results, we put $\big\uparrow$ next to their names, and if the lower value demonstrates better results, we add $\big\downarrow$.

(1) \textbf{Consolidation$\big\uparrow$}: We consider two entities related if they're part of a single event in the ground truth, and both of them are labeled as relevant name entities. Let $A_t$ denote the number of related entity pairs that are part of the system output at timestamp $t$. $a_t$ denotes the number of related entity pairs that share a common cluster in the system output at timestamp $t$, and $T$ denotes the set of all timestamps in the system output. Consolidation is defined as:
\begin{equation}
\begin{gathered}
   \mathbf{C} = \frac{\sum_{t \in t}a_t}{\sum_{t \in T}A_t}
\end{gathered}
\end{equation}

(2) \textbf{Discrimination$\big\uparrow$}: We consider two entities unrelated if they are part of a single event in the ground truth, and one of them is labeled as an irrelevant name entity. Let $B_t$ denote the number of unrelated entity pairs that are part of the system output at timestamp $t$ and $b_t$ denote the number of unrelated entity pairs that are not in a common cluster in the system output at timestamp $t$. Discrimination is defined as:
\begin{equation}
\begin{gathered}
   \mathbf{D} = \frac{\sum_{t \in t}b_t}{\sum_{t \in T}B_t}
\end{gathered}
\end{equation}

(3) \textbf{Clustering score$\big\uparrow$}: The combination of consolidation and discrimination using harmonic mean is known as \emph{Clustering Score}:
\begin{equation}
\begin{gathered}
   \mathbf{CS} = \frac{2CD}{C+D}
\end{gathered}
\end{equation}

\subsubsection{Dataset and Results}

We evaluate the overall performance of EnrichEvent on Twitter streaming data. We consider a one-week time window and crawl the posted tweets from 2023/3/13 to 2023/3/18. We split the time window into daily time frames. As we should manually label the entities, we sample 100 tweets from each time frame with minimal limitations on selection parameters. In other words, while selecting the tweets, we don't gather tweets from specific users, or we don't limit the tweets to those which contain particular hashtags, words, phrases, etc. The only applied constraint is that we pick 80\% of the tweets from the top retweeted tweets, and the remaining 20\% are randomly sampled from the rest. Note that we expand the length of the time window and number of tweets in the following experiments in which there is no need for manual labeling.

\begin{table}
\begin{center}
{\caption{Manual evaluation on streaming social data}\label{table1}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\hline
\rule{0pt}{12pt}
Method &Consolidation &Discrimination & Clustering Score
\\
\hline
\\[-6pt]
\textbf{Word2vec}& \textbf{99.78\%}& 40.80\%& 57.91\% \\
\textbf{BERT}& 93.30\%& 80.00\%& 86.14\% \\
\textbf{Yang et al.~\cite{Yang2018AnED}}& 48.20\%& 39.30\%& 43.29\% \\
\textbf{Fedoryszak et al.~\cite{RealTime_EventDetection_Twitter}}& 96.17\%& 80.03\%& 87.36\% \\
\hline
\\[-6pt]
\textbf{Our Proposed Framework}& 99.05\%& \textbf{80.15\%}& \textbf{88.60\%} \\
\hline
\\[-6pt]
\textbf{Improvement}& 0.73\%$\big\downarrow$& 0.12\%$\big\uparrow$& 1.24\%$\big\uparrow$ \\
\hline
\\[-6pt]
\end{tabular}}
\end{center}
\end{table}

As shown in Table 1, the results indicate that our proposed framework outperforms all the baselines in most cases. Among the baselines,~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter} is the strongest one, and it's impressive that our framework performs better. Based on the evaluation metrics, our framework has performed well in three ways. \textbf{(1)} It detects the majority of the occurring events in the time frames based on the high value of the consolidation metric. In other words, the attending clusters in cluster chains contain a high ratio of related entities. \textbf{(2)} The detected events are highly separated from each other due to value of discrimination metric. \textbf{(3)} It's critical that the framework achieves a high value of the consolidation and discrimination metric simultaneously. For instance, word2vec reaches 99.78\% consolidation and 40.8\% discrimination, which means it couldn't differentiate the events. Compared with the baselines, the clustering score metric verifies the superiority and effectiveness of EnrichEvent in detecting and discriminating the events.

\subsection{Clusters' Quality}
In section $4.3$, we claim that the Contextual Knowledge Enhancement component assists the HDBSCAN model in detecting related entities and grouping them. To prove our claim, we perform the following experiments:

(1) \textbf{Fraction of related entities$\big\uparrow$}: During the construction of cluster chains and event identification, it's crucial that the attending clusters in the chains include related entities to the events. To investigate the effect of the Contextual Knowledge Enhancement component, we calculate the ratio of related entities in clusters when using contextual knowledge compared with other baselines. 

% Figure environment removed

As shown in Figure 2, EnrichEvent yields the highest fraction of related entities. That's because it makes full use of contextual representation as well as lexical representation of the tweets. Compared to word2vec, which utilizes averages of the word2vec embedding of words as their representation, we achieve 17.55\% promotion in the fraction of related entities. It conveys that EnrichEvent gets a good comprehension of the events and clusters them properly. To validate the quality of clusters, we also investigate the clustering metrics. Note that for the following experiments, we extend the number of time frames and tweets into 20 windows consists of 1000 tweets, and we conduct all the experiments in the same setting.

(2) \textbf{Silhouette Score$\big\uparrow$}: The silhouette score measures the similarity of points inside their cluster compared to the others~\cite{Silhouette}. Inspection of Figure 3 demonstrates that using the lexical representation leads to more similar clusters in the first few windows, but exploiting the contextual knowledge has made its impact in the following windows, and the quality of clusters increased.

% Figure environment removed

(3) \textbf{Calinski-Harabasz Index$\big\uparrow$}: Also known as the Variance Ratio Criterion, it measures the sum of between-cluster dispersion against the sum of within-cluster dispersion, where dispersion is the sum of distance squared~\cite{CalinskiHarabasz}. By comparing the value of the Calinski-Harabasz index over consecutive time frames in Figure 4, it signifies the clusters are far away from the others, and they have a higher quality when we utilize the Contextual Knowledge Enhancement component.
% Figure environment removed

(4) \textbf{Davies-Bouldin Index$\big\downarrow$}: This index signifies the average similarity between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves~\cite{DaviesBouldin}. Figure 5 verifies that our framework, i.e., the Contextual Knowledge component, utilizes the knowledge which enhances the quality of the produced clusters.
% Figure environment removed

% \begin{table}
% \begin{center}
% {\caption{caption...}\label{table1}}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{lcccccccc}
% \hline
% \rule{0pt}{12pt}
% &\multicolumn{8}{c}{Window}\\
% \cline{3-9}
% \rule{0pt}{12pt}
% Method &Metric &1 & 2& 3& 4& 5& 6& 7 
% \\
% \hline
% \\[-6pt]
% \quad \textbf{Lexical}& $\Diamond$& 0.67& 0.7& 0.69& 0.66& 0.72& 0.7& 0.74 \\
% \quad \textbf{Contextual Knowledge}& $\Diamond$& 0.59& 0.61& 0.53& 0.56& 0.62& 0.65& 0.67 \\
% \quad \textbf{Lexical + Contextual Knowledge}& $\Diamond$& 0.67& 0.64& 0.56& 0.55& 0.3& 0.71& 0.38 \\

% \hline
% \\[-6pt]
% \quad \textbf{Word2vec}& $\Box$& 16.42& 31.18& 10.59& 22.7& 31.02& 46.83& 29.93 \\
% \quad \textbf{BERT}& $\Box$& 23.55& 16.38& 12.55& 17.45& 16.62& 25.61& 23.23 \\
% \quad \textbf{~\cite{Yang2018AnED}}& $\Box$& 120.3& 878.59& 19.91& 35.71& 1069.93& 172.31& 2534 \\
% \quad \textbf{~\cite{RealTime_EventDetection_Twitter}}& $\Box$& 12.85& 13.34& 14.06& 12.26& 13.44& 14.18& 12.58 \\
% \quad \textbf{Ours}& $\Box$& 6.55& 5.74& 7.07& 6.46& 6.92& 7.21& 6.76 \\
% \hline
% \\[-6pt]

% \quad \textbf{Word2vec}& $\circ$& 1.16& 0.99& 1.27& 1.15& 0.97& 0.77& 0.94 \\
% \quad \textbf{BERT}& $\circ$& 1.21& 1.18& 1.2& 1.06& 1.12& 1.1& 0.99 \\
% \quad \textbf{~\cite{Yang2018AnED}}& $\circ$& 0.22& 0.19& 0.57& 0.42& 0.09& 0.31& 0.07 \\
% \quad \textbf{~\cite{RealTime_EventDetection_Twitter}}& $\circ$& 1.32& 1.42& 1.53& 1.44& 1.41& 1.36& 1.36 \\
% \quad \textbf{Ours}& $\circ$& 1.22& 1.29& 1.31& 1.29& 1.29& 1.23& 1.24 \\
% \hline
% \\[-6pt]

% \multicolumn{8}{l}{$\Diamond$ Silhouette Score $\big\uparrow$ \ \
% $\Box$ Calinski-Harabas index $\big\uparrow$ \ \
% $\circ$ Davies-Bouldin index $\big\downarrow$}
% \end{tabular}}
% \end{center}
% \end{table}

% \begin{table}
% \begin{center}
% {\caption{caption...}\label{table1}}
% \resizebox{\columnwidth}{!}{
% \begin{tabular}{lcccc}

% Method &Silhouette Score $\big\uparrow$ & Calinski-Harabas Score $\big\uparrow$ & Davies-Bouldin score $\big\downarrow$ \\
% \hline
% \\[-6pt]
% \quad \textbf{Lexical Only}& 0.72& 0.7& 0.74 \\
% \quad \textbf{Contextual Knowledge Enhancement}& 0.62& 0.65& 0.67 \\

% \hline
% \\[-6pt]

% \end{tabular}}
% \end{center}
% \end{table}

% \section{Disscussion}
% \textcolor{red}{Pending...}

\subsection{User Diversity Enhancement}
% To evaluate the efficiency of the proposed method, we calculate the minimum number of tweets needed to identify an event. One of the main goals of this research is to detect the trending event as soon as possible and with a minimum number of tweets. Of course, it should be noted that the nature of the events is different from each other, and regarding their nature, the minimum number of required tweets will be different. To consider this, we also check user diversity in a single chain to examine whether the name entities are extracted from a wide range of users or not ?! A cluster chain with very low user diversity seems to be invalid and we should also set a threshold for minimum user diversity in a single chain to consider this issue.

We can define user diversity as the diversity of the users who participate in the evolution of the events. Intuitively, a rise in user diversity leads to growth in the credibility and reliability of an event~\cite{kumar2014tweets}. Let $u_i$ denote the $i^{th}$ user in the produced cluster $C$, and $n_{u_i}$ indicates the number of tweets published by $u_i$. Given an event $e$, its User Diversity $H(e)$ defined as following formula where $n$ is number of tweets in cluster $C$:
\begin{equation}
\begin{gathered}
   H(e) = \sum _{i=1}-\frac{n_{u_i}}{n} \log \frac{n_{u_i}}{n}
\end{gathered}
\end{equation}

In this section, we investigate the impact of our proposed framework on the value of user diversity of events. First, we utilized the proposed framework in~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter} to detect events from 2023/2/24 to 2023/3/15. The average user diversity of the identified events was 4.87. Then, we conducted the same experiment using EnrichEvent, and the average user diversity of the detected events increased to 6.341. The result demonstrates the 29.9\% promotion in user diversity, which leads to more credible events.

\subsection{Evaluation of Trending Data Extraction}
Trend detection is one of the main components of our framework that filters incoming tweets based on the probability of referring to an event. We appraise this component in 2 phases.

\textbf{(1) Efficiency}: To evaluate the performance of the trend detection model, we measure commonly used classification metrics, e.g., precision, recall, and F1-score. We demonstrate the results of the trend detection model in Table 2 and draw some key findings as follows:
\begin{itemize}
    \item Our implemented model is robust in detecting the tweets which don't refer to the occurring events based on its descent performance on tweets with label 0.
    \item Regarding the tweets that referred to the events, it's crucial that the model achieves high recall and acceptable precision simultaneously. Our emphasis on the recall metric is due to the fact that we want to filter tweets with great confidence, and we prefer not to miss any tweets with label one as much as possible. Although learning discriminative patterns is strenuous in imbalanced data, but the results demonstrate that the trend detection model meets our expectations.
\end{itemize}

\begin{table}
\begin{center}
{\caption{Performance of trend detection model}\label{table2}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\hline
\rule{0pt}{12pt}
Label &Precision &Recall & F1-Score & Support
\\
\hline
\\[-6pt]
\textbf{Label 0}& 0.97& 0.90& 0.93 & 312254\\
\textbf{Label 1}& 0.53& 0.78& 0.63 & 45307\\
\hline
\\[-6pt]
\textbf{Macro Average}& 0.75& 0.84& 0.78 & 357561 \\
\textbf{Weighted Average}& 0.91 & 0.89& 0.89 & 357561 \\
\hline
\\[-6pt]
\end{tabular}}
\end{center}
\end{table}

\textbf{(2) Effectiveness}: There is immense participation from people worldwide in social media, and they invariably produce streaming social data. Scalability is one of the vital characteristics of an event detection framework, otherwise, the proposed framework would not be practical. To investigate the effectiveness of the Trending Data Extraction component, we turn it off and calculate pipeline runtime. In comparison to the previous experiments, switching off this component leads to over 50\% increase in runtime, which is a considerable amount.

\section{Conclusion}
Event detection on social media platforms is an active research area, and identification of events can provide beneficial awareness for making crucial decisions in different fields. In this paper, we design a novel event detection framework to identify unspecified events in social media data streams. In this work, we utilize combinations of lexical and contextual representations of tweets. This contextual knowledge enhancement enriches the model's perspective and assists the model in understanding the relationships between tweets more sufficiently. We empirically demonstrate the superiority of our framework in detecting and distinguishing social events compared to the baselines through conducted experiments.

An intriguing future research direction would be adding an event summarization component to our framework and building an intelligible summarization for interested organizations and users.


% \ack We would like to thank the referees for their comments, which
% helped improve this paper considerably

\bibliography{EnrichEvent}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
