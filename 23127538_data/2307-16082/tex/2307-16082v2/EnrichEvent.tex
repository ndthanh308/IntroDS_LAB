\documentclass{EnrichEvent}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{array}
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{tabularray}


%%\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.

\begin{document}
\begin{frontmatter}


\title{EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction}

% \author[A]{\fnms{Mohammadali}~\snm{Sefidi Esfahani}\thanks{Corresponding Author. Email: mohammadali.esfahani@aut.ac.ir}}
\author[A]{\fnms{Mohammadali}~\snm{Sefidi Esfahani}\orcid{0009-0007-0285-1545}\thanks{Corresponding Author. Email: mohammadali.esfahani@aut.ac.ir}}
% \author[B]{\fnms{Mohammad}~\snm{Akbari}}
\author[B]{\fnms{Mohammad}~\snm{Akbari}\orcid{0000-0002-3321-5775}}
\address[A]{Department of Mathematics and Computer Science, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran}
\address[B]{Department of Mathematics and Computer Science, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran}
% \author[A]{\fnms{Anonymous}~\snm{Author(s)}}
% \address[A]{Paper ID : 950}

\begin{abstract}
Social platforms have emerged as crucial platforms for disseminating information and discussing real-life social events, which offers an excellent opportunity for researchers to design and implement novel event detection frameworks. However, most existing approaches merely exploit keyword burstiness or network structures to detect unspecified events. Thus, they often fail to identify unspecified events regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, word sense ambiguation, and irregular language, as well as variation in aspects of opinions. Moreover, extracting discriminative features and patterns for evolving events by exploiting the limited structural knowledge is almost infeasible. To address these challenges, in this thesis, we propose a novel framework, namely EnrichEvent, that leverages the lexical and contextual representations of streaming social data. In particular, we leverage contextual knowledge, as well as lexical knowledge, to detect semantically related tweets and enhance the effectiveness of the event detection approaches. Eventually, our proposed framework produces cluster chains for each event to show the evolving variation of the event through time. We conducted extensive experiments to evaluate our framework, validating its high performance and effectiveness in detecting and distinguishing unspecified social events.

%Social platforms have emerged as a crucial platforms for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. Most existing approaches for event detection merely exploit keyword burstiness or network structures to detect hot events. They often fail to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, especially tweets, is characterized by misspellings, incompleteness, ambiguity, and irregular language, as well as variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, in this paper, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data. In particular, we leverage contextual knowledge to detect semantically related tweets as soon as possible and enhance the quality of produced clusters. We conducted several experiments to evaluate our framework, and the experimental results demonstrate the effectiveness of our proposed framework in detecting and distinguishing social events. 

%Social media platforms with billions of active users worldwide have become an integral part of our daily life. As a result, social media has emerged as a crucial platform for discussing and disseminating information about significant events. Real-life events are occurring and evolving in social data streams, and it offers an excellent opportunity for early detection of newsworthy events. However, event detection in social media faces challenges, such as its streaming and evolutionary nature and the variation in language expressions and aspects of opinions. This paper addresses these challenges and discusses the importance of quick and accurate event extraction in social media for various purposes, such as disaster management, marketing, and financial markets. Most existing methods, including those based on community detection, utilize limited knowledge as they don't consider the semantics and contextual aspects of social data streams. In this paper, we make full use of lexical, semantic, and contextual representations of tweets. We propose a novel framework that gradually groups related tweets into clusters, and by connecting them over consecutive time frames, it identifies the events. We conduct experiments, and the results demonstrate the superiority and effectiveness of our proposed framework.
\end{abstract}
\end{frontmatter}

% \begin{keyword}Event detection \sep%
%     Twitter \sep% 
%     Real-time data \sep% 
%     Online social networks \sep% 
% \end{keyword}


\section{Introduction}
% Background one of these two para
% Social media platforms have been incorporated into our daily lifestyle for socializing with friends, disseminating information, discussing events, and learning new concepts, with millions of people from all over the globe are highly active in participating on different social networks. There are currently about $5.4$ billion active users on the internet and among them, around $4.5$ billion of them have more than one account in one of the existing platforms~\cite{find-ref}. As such, when an event occurs in society, both experts and grassroots users start sharing and discussing around different aspects of the event on social media (e.g., Twitter discussions on the Notre-Dame Cathedral fire). This active presence of users has turned social media into one of the most important sources for identifying, analyzing, and investigating events providing live updates on important events. With this sheer volume of live data available on social platforms, early detection of events from social platforms presents great opportunities to gain real-time information around newsworthy events before these are covered by conventional broadcast media channels. Consequently, the development of platforms for discovering social event and understanding the real-time discussions happening has been attracted considerable attention in both academia and the industry.

Social media platforms have become an integral part of our daily routines for communicating with friends, sharing information, exchanging ideas, and acquiring knowledge~\cite{Chen2013EmergingTD}. There is massive participation from people worldwide in social media. Social network statisticians expect the number of active social media users worldwide to increase from 2.86 billion in 2017 to 4.41 billion in 2025~\cite {SocialMedia_Statistics}. As a result, social media has emerged as a crucial platform for discussing and disseminating information about significant events. When a social incident occurs, experts and ordinary users engage in discussions on social media platforms (e.g., Twitter discussions on the Notre-Dame Cathedral fire) from various perspectives~\cite{Aldhaheri_2017, Sun_Xiang_Wu_2015}. The active presence of users on social media has transformed it into a primary source for identifying, analyzing, and investigating events, providing real-time updates on critical incidents. The abundance of live data on social platforms offers an excellent opportunity for early detection of newsworthy events before conventional broadcast media channels cover them. Therefore, academia and industry have demonstrated significant interest in developing platforms that can discover social events and understand real-time discussions happening on social media.
 
%Importance of the research
Traditionally, an \emph{event} is defined as an important happening in a specific time and place~\cite{allan1998topic}. With the emergence of social media, this definition expands to a happening in society that attracts a sudden burst of attention in social media, and similar to the previous explanation, it occurs in a specific time and place~\cite{boettcher2012eventradar}. Intuitively, when an event occurs, quick and accurate event identification is one of the most crucial concerns for the government, aid agencies, relative organizations, and even for journalists and experts~\cite{Governments, HUANG202111}. For instance, when a natural disaster occurs, relief organizations consider event detection as one of the ways to capture different types of information, such as the severity of the incident, the number of victims, and how to provide aid for them during natural disasters. These related organizations are keen on being informed of an event occurrence as soon as possible to take appropriate actions regarding the event~\cite{Bursty_Disaster_Management, nugent2017comparison, pekar2020early, xiao2015understanding, guan2014using, yun2011disaster, sreenivasulu2020comparative, Earthquake_Shakes_Twitter_Users}. Besides, being conscious of event occurrence is vital for other purposes such as marketing, commercial, and even financial markets~\cite{TETLOCK, Mohan, Shah, Engle}. For instance, leading commercial brands always want to discover what's happening all over the market, and this awareness accompanying the right decisions puts them on the path to be ahead of their rivals.

Despite its value and significance, quick and accurate identification of unspecified social events has not been fully investigated due to the following challenges. 1)~\textbf{Streaming and Evolutionary}. Social media services provide a streaming source of information in which people freely create content around real-world events. While working on streaming sources, you should process the incoming data in a single stage which is a challenging and opening problem. In addition, the process of turning a simple happening into a hot event which everyone will be talking about is a complex and evolutionary phenomenon comprised of three major steps. \textbf{a)} An event happens in the real world. \textbf{b)} Observers, journalists, and ordinary people start a discussion about the occurred event on social platforms. \textbf{c)} The event turns into a hot event for a while or fades out at the beginning~\cite{Chen2013EmergingTD}. How to model evolving characteristics of an event is an arduous task. 2) \textbf{Variation in Language Expressions and Aspects of Opinions}. Twitter users produce and consume information in a very informal and irregular way, and the published tweets usually include idioms, abbreviations, misspellings, irregular language, and emojis. Due to the huge and diverse user base, Twitter usually contains different words and word sequences describing the same idea. Also, in most languages, many words have various meanings in different contexts. For example, the word "fair" in a different context has multiple meanings of "carnival", "treating someone right", and "having light skin/hair". This issue makes it so hard for machine learning models to understand users' actual purpose. Besides, while talking about an event, users may have different opinions and express them in different ways. How to detect and aggregate various opinions and aspects of events is another challenge. 


% According to the text-based structure of Twitter, this social network is one of the main platforms for discussing different types of events. However, some characteristics of Twitter make the problem of event detection more challenging. The first one is the short length of tweets. It's common among users to write short tweets and this shortness makes it harder for the NLP models to consider the contextual aspect of tweets. The second one is the informal use of language. On Twitter, users write tweets in a very different style compared with other media. Misspellings, abbreviations, and slang are common in tweets and when these problems and shortness come together, event detection becomes more challenging. The third challenging aspect of Twitter is its dynamic nature. On Twitter, users generate data streams at every moment and as it's debated in the previous paragraph. So we can't ignore time and detect events statically in a single moment. The last challenging aspect of Twitter is the existence of noise in tweets. Not only there's no guarantee that all the tweets are relevant, but also most of them are irrelevant, and even those which are relevant may contain irrelevant terms.

%But something that's not considered in these definitions is the process of turning simple happenings into hot events which everyone is going to talk about them. In other words, hot events don't emerge in a moment and don't die a while later. At any moment, many happenings occur all over the world, but few of them turn into hot events and this evolutionary process should be considered. Most of the time, this evolutionary process comes in 3 steps. 1)Something happens somewhere in the world. 2)People, journalists, and â€¦ start talking about it and sharing with others via different platforms and media based on the importance of the event. The more people share the event, the more trending it will be. 3)At last, some events will disappear after a while, some of them become hot events in a local area and some of them even become real trending events, and people from all over the world, will be hearing about them. 

% should be enhanced
Retrospective studies in social media and event detection have proposed practical approaches for identifying events from social data by leveraging content and network structure. For instance, frequent keyword extraction has been used for detecting bursty keywords corresponding to an event, and hashtags have been leveraged for capturing semantics from tweets to address the short context problem~\cite{Keyword_Volume_Approach, Yang2018AnED}. Similarly, clustering methods have been employed for mitigating short and sparse contexts in social posts~\cite{akbari2017leveraging}. Further, modular decomposition and community detection have been exploited to utilize links and network structures. While a large body of work has been devoted to event detection, limited research tackled the problem of unspecified event detection and its evolution over time. In this paper, we design an end-to-end framework, namely \textbf{EnrichEvent}, to address the challenges mentioned in the previous paragraph. We define an event as a single chain of clusters where each cluster contains closely related entities both in time and semantic dimensions. In addition, we leverage contextual knowledge as well as structural aspects of the tweets to enrich their representation and achieve better results. Exploiting the contextual knowledge assists the model in comprehending the existing relation among the tweets and generate their representation with higher quality. We conducted several experiments to evaluate all the components of our proposed framework. The empirical results validate that EnrichEvent achieves better results compared with various baselines based on the evaluation metrics. In summary, our main contributions are as follows:
\begin{itemize}
    \item We design a novel framework, EnrichEvent, that is robust to variation in language expressions and aspects of opinions and considers the temporal variation of context around a given event to model the evolutionary process of events occurring sufficiently.
    \item We propose a generative approach for event summarization which is an innovation itself since most of the recent research utilized extractive approaches.
    \item  We provide two real-world datasets for training the trend detection model and performing experiments to evaluate our proposed event detection framework. The result empirically demonstrates the effectiveness of EnrichEvent. Moreover, researchers can exploit these datasets in various tasks such as topic modeling, sentiment analysis, etc.
\end{itemize}

% In this paper, We consider and add the concept of evolutionary events to the problem of event detection from Twitter. Although some recent research considers these points but context is the key point that is missed in event detection. While designing the event detection models, most of the recent research didn't consider the context and semantic aspects of the tweets and they only focus on the lexical aspects. Event detection models which implemented only based on bursty features or those that focus on frequency and co-occurrence of keywords are examples of models which don't consider the context.

\section{Related Works}
% Event detection from Twitter has received noticeable attention in recent years, as it introduced TwitterAPI, and researchers can easily collect real-time data~\cite{Survey_2016, ED_SWE}. We can divide the problem of social event detection into three sub-categories. Identification of \textbf{unspecified events}, \textbf{predetermined events}, and \textbf{specific events}~\cite{atefeh2015survey}. While working on the first category, there's no antecedent description of the events, and they aim to detect general events. In this category, researchers typically extract features from tweets and then try to find trends in them to detect the events~\cite{Long2011TowardsEE, Becker_Naaman_Gravano_2021}. In the second category, researchers aim to detect pre-defined events with a fixed category, such as earthquake~\cite{Earthquake_Shakes_Twitter_Users}, crime and disaster events~\cite{TEDAS}, etc. In the third category, the event's details are specified, and the goal is to detect events that exactly match an explicit description of the relevant event types~\cite{Planned_Events}. Our work belongs to the first category, and in this section, we investigate these three categories and explain some of the related works that have motivated us to work in this field.

Event detection from Twitter has received noticeable attention in recent years, as it introduced TwitterAPI, and researchers can easily collect real-time data~\cite{Survey_2016, ED_SWE}. We can divide the problem of social event detection into three sub-categories. Identification of \textbf{unspecified events}, \textbf{predetermined events}, and \textbf{sub-events}~\cite{atefeh2015survey}. Our work belongs to the first category, and in this section, we investigate these three sub-categories and explain some related works that have motivated us to work in this field.

\subsection{Identification of unspecified events}
While working on this category, there's no antecedent description of the events, and researchers aim to detect general events. In this category, researchers typically investigate different aspects of the tweets to discover the existing relations between them. As mentioned in the previous section, processing streaming data is a challenging task, and \textbf{Comito et al.}~\cite{Bursty_Event_Detection} tried to design an event detection framework that is robust to processing issues. Moreover, they exploited the existing semantics in the tweets by working on the 2-grams of words, hashtags, and mentioned users. Eventually, they identified the events by grouping the related tweets using the incremental clustering approach. The main idea of~\textbf{Yang et al.}~\cite{Yang2018AnED} is to cluster similar hashtags together, and clusters of hashtags represent occurring events. They focused on the occurrence of hashtags in tweets and considered hashtags as a bag of words and hashtags. Eventually, they used the concatenation of hashtag-hashtag and hashtag-words co-occurrence vectors as a final feature vector for clustering. The main challenge of this work is that users may not necessarily use hashtags in all tweets. Besides, hashtags used in tweets are not necessarily relevant to their content. ~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter} considers happening events as chains of clusters. They built a real-time system to identify groups of event-related entities. Then, they linked these clusters together and generated cluster chains that represent the events. Note that this paper only focused on the lexical representation of named entities and used occurrence vectors to cluster them.

\subsection{Identification of predetermined events}
% In the second category, researchers usually leverage datasets such as ACE 2005, Event2012, and Event 2018, which contain predefined events in different event categories. Recently, numerous frameworks have been designed, .i.g., FinEvent~\cite{peng2022reinforced}, PP-GCN~\cite{peng2021streaming}, KP-GNN~\cite{Knowledge_Preserving}, and QS-GNN~\cite{Qsgnn}. Intuitively, these papers tried to model the existing relations between tweets using message graphs in their own ways. Eventually, they exploit GNN models to identify the occurring events. Note that most of the existing papers in this category divide the training into two stages. (1) \textbf{Pre-training} stage in which they train models with labeled message graph (obtained message graph from the first time frame). (2) In \textbf{fine-tuning} stage, they utilize unlabeled message graphs and try to generate high-quality pseudo labels.

In the second category, researchers usually leverage datasets such as Event2012~\cite{Event2012} and Event2018~\cite{Event2018}, which contain predefined events in different event classes. There exist two important challenges in this category, which provide opportunities for researchers to design novel frameworks. \textbf{1) Scalability}, most of the recent papers converted the message blocks to message graphs in their ways to model the dependencies between the tweets. Moreover, they leveraged the GNN models to generate the embedding of the tweets~\cite{Knowledge_Preserving, peng2021streaming, peng2022reinforced}. Although utilizing message graphs and GNN models enhance the messages' embedding, they reduce the scalability of the frameworks. \textbf{2) Usability}, the proposed frameworks should be trained on labeled datasets, which makes their use in real-world problems challenging.~\textbf{Ren et al.}~\cite{Qsgnn} investigated this challenge and designed a framework, namely QS-GNN. While training the proposed model, it only required the labeled messages in the first time frame. Although they have minimized the model's dependence on labeled data, leveraging these frameworks for real-world problems is still challenging.

\subsection{Identification of sub-events}
% \textbf{Becker et. al}~\cite{Planned_Events} leveraged various user-contributed event features to identify upcoming concerts. They also utilized term frequency-based techniques for selecting representative and descriptive event terms, which serve as complementary queries for retrieving event messages from Twitter. In summary, they have developed a system that returns Twitter messages related to a specific event using our query-building strategies. Note that few papers have been published in this category recently.

In this category, researchers consider a specific event, e.g., a football match, and aim to identify the relevant sub-events. Designing a decent approach to find the correlations between the tweets and a given event is the main challenge of this category. ~\textbf{Bekoulis et al.}~\cite{bekoulis-sub} investigated the temporal aspects of the tweets and proposed a two-stage framework that detects the sub-events by analyzing their evolution over time. Moreover, ~\textbf{Lu et al.}~\cite{lu-sub} focused on semantics to generate a better representation for tweets. In addition, they used hashtags and their n-grams to capture the correlations suitably. Although much research has been done in this category, it seems that researchers can enhance the existing frameworks in different ways.

\section{Problem Statement}
% In this section, first, we present the notation and then formally define the problem of emergent event detection from social media data.
% Let $\mathcal{T} = {t_1, t_2, \ldots, t_n }$ denote an order set of $n$ different tweets, i.e., a social stream, where each $t_i$ is associated with a timestamp indicating its posting time. We assume that these tweets are coming in $m$ different blocks of messages $\mathcal{B} = {b_1, b_2, \ldots, b_m }$. A \emph{social event} is formally defined as a set of semantically related tweets where all tweets refer to a real-world event that happened in a specific time point, $e_i = { t_1, t_2, \ldots, t_k}$. Denotes that while an event $e_i$ happened in a specific time point, it usually evolves over longer periods, and includes several message blocks normally. The task addressed in this paper is to detect a set of unspecified events $\mathcal{E} = {e_1, e_2, \ldots, e_z}$ based on available social stream, i.e., $\mathcal{T}$. With the notation above, we formally define the emerging event detection problem as follows: 

% Given a social stream of microblog messages $\mathcal{T}$ with their content, we aim to learn a model as follows,
% \begin{align}
%     \mathrm{M}: \mathcal{T} \rightarrow {e_i}_{i=1}^{z},
% \end{align}
% where each $e_i$ is a sequence of message clusters $c_i$ each formed by messages in one social block. 

In this section, we present the notation and then formally define the problem of emergent event detection from social media data.

\textbf{Definition 2.1.} Social stream $\mathcal{S}=\{\mathcal{M}_1, \ldots, \mathcal{M}_{i-1}, \mathcal{M}_i, \ldots\}$ defined as a consecutive sequence of message blocks. In this definition, we denote message block $\mathcal{M}_i$ as $\mathcal{M}_i=\{m_j | 1 \le j \le |\mathcal{M}_i| \}$, where $|\mathcal{M}_i|$ is total numbers of the tweets in time period $[t_i, t_{i_1})$ and $m_j$ denotes a single tweet.

\textbf{Definition 2.2.} We can group all the tweets in message block $\mathcal{M}_i$ to finite set of clusters $\mathcal{C}_i=\{c_j | 1 \le j \le |\mathcal{C}_i| \}$, where messages in each cluster are contextually related.

\textbf{Definition 2.3.} An \emph{event} $e=\{c_{ij} | 1 \le i \le |\mathcal{M}_i| , 1 \le j \le |\mathcal{C}_i| \}$ is formally defined as a continuous sequence of contextually related clusters where all the clusters refer to a single event. Note that we assume each social message belongs to, at most, one event.

\textbf{Objective:} Given a social stream of message blocks $\mathcal{S}$, we aim to design and implement a framework $F$ as follows,
\begin{align}
    F: \mathcal{S} \rightarrow E=\{e_1, e_2, \ldots\},
\end{align}
where each $E$ is a set of unspecified social events.


\section{Methodology}
In this section, we present various components of our proposed framework. The main advantage of the proposed framework is the capability of processing streaming social data and extracting candidate social events in a near online setting. In addition, our framework is language-independent and can be easily adjusted to different languages. In summary, EnrichEvent get the message blocks as inputs and finally stores the detected events with their attributes in the database. Here, we first go through the different components of the EnrichEvent. Afterward, we define their roles and functionality and then delve into the details of each unit. You can glance at the architecture of EnrichEvent in Figure 1.

% Figure environment removed

\subsection{Framework}
% The proposed framework focuses on the need for an end-to-end pipeline that can handle the streaming nature of social media data by processing incoming data and generating streams of outputs. Note that events start occurring at specific time points and can extend over multiple time windows, requiring a framework capable of processing a stream of tweets and returning cluster chains that represent these events.

% The streaming nature of social media data needs an end-to-end pipeline capable of processing incoming data and forming streams of outputs, as we focused on in this paper. Recall that events are happening at a specific time point, and the discussion can be extended for several time windows. This setting demands a framework with the capability of processing a stream of tweets comes and returning cluster chains that represent the happening events. The proposed framework consists of four components: trending data extraction, contextual knowledge--segment--anything else enhancement, event clustering, and event chain formation. The first step, i.e, trending data extraction distillates newsworthy tweets potentially referring to an event. After extracting the potential pool of tweets, the tweet block sends into the Contextual Feature Enhancement to extract and enrich with semantic features assisting the model to correlate tweets corresponding to an event but from diverging aspects. Event Clustering, as inspired by previous studies in~\cite{ref1, ref2}, clusters semantically related tweets to groups, i.e., perform event detection. The later step, i.e., chain construction, attempts to create the evolving chain of events through various time windows. 

Since events start at a specific time point and can extend over multiple time windows, in this paper, we focus on the need for an end-to-end pipeline that can handle the streaming nature of social media data by processing incoming data and detecting the unspecified events. Our proposed framework, i.e., \textbf{EnrichEvent}, comprises seven components: \textbf{Trending Data Extraction}, \textbf{Contextual Knowledge Enhancement}, \textbf{Event Clustering}, \textbf{Event Chain Formation}, \textbf{Event Summarization}, \textbf{Storage}, and \textbf{Evaluation}. First, Trending Data Extraction identifies newsworthy tweets that potentially refer to an occurring event. The message block is then sent to Contextual Knowledge Enhancement to extract and enrich contextual knowledge, which assists the clustering model in comprehending the existing patterns among the tweets. Inspired by previous studies ~\cite{RealTime_EventDetection_Twitter, Bursty_Event_Detection}, Event Clustering groups the semantically related entities into groups. Then, Event Chain Formation links the clusters in consecutive time steps and generates evolving chains of clusters that present the occurring events. In the following stage, all the identified events are summarized using a generative approach. Eventually, the framework generates a JSON file as the output and stores it in the database. In addition, it evaluates the results at the final stage.

\subsection{Trending Data Extraction}

Social media data is often known as a noisy and sparse information source, and $40\%$ of tweets refer to \emph{pointless babble}. Thus it is crucial to filter out irrelevant data and extract tweets corresponding to the events~\cite{SEDTW}. In other words, we had to appropriately filter out noisy data and gather valuable and informative ones. To do so, we proposed to filter general tweets and accumulate those with a high probability of referring to an event. Here, we leverage a supervised learning model to mark tweets potentially referring to the events. Intuitively, our model examines the tweets to determine whether they refer to an important event. We achieved this through a two-segment architecture. In the first segment, a combination of embedding, convolution, and max pooling layers extract efficient features from the input tweet. In the second segment, we exploited fully connected layers to classify whether the incoming tweet refers to an important event. In summary, the proposed model gets a tweet as input and returns the probability of referring to an event which helps us to filter the tweets based on the minimum threshold $\lambda$.

\textbf{Trend detection dataset}: To train the trend detection model, we started preparing an offline trend detection dataset using SNScrape. We have collected 1.6 million Persian tweets and tried to label them manually based on key phrases. The generated dataset has informative columns such as \textit{timestamp}, \textit{tweet id}, \textit{text}, \textit{username}, \textit{reply count}, \textit{like count}, and \textit{retweet count}. Eventually, we utilized it for training the trend detection model. 
% We add more details about the trend detection dataset in Appendix A.

Inspired by retrospective studies~\cite{Chen2013EmergingTD}, we adopt a supervised learning approach and train a model $\mathcal{T}$ that identifies tweets with a high probability of referring to an event. This model gains extracted features from tweets as input vectors, i.e., $X$, and computes the probability of being general information or a simple status update, i.e., negative class, or referring to a potential event, i.e., positive class. Formally, the prediction of model $\mathcal{T}$ is performed by solving the following optimization problem:


\begin{align}
   \arg \min_\mathbf{W} \mathcal{L}(\mathbf{X}, \mathbf{W}, \mathbf{Y}),
\end{align}
Where $\mathcal{L}(.)$ is the loss function and often is defined as binary cross entropy. $\mathbf{W}$ denotes all parameters that the model learns from the training set, and $\mathbf{Y}$ indicates the label of the tweets where one is used for tweets referring to an event, otherwise zero. 

To learn an effective model, we should extract a prominent and discriminative representation from the tweets using the pre-trained models. Therefore, given a tweet, we extract two representations from them: (1)~\textbf{Bert sentence embedding}~\cite{ParsBERT}, to achieve a decent understanding of the context, we used Parsbert's pre-trained model to obtain the tweets' representation. While training the model, we use this representation in the embedding layer. Note that we didn't fine-tune the Bert model, and the parameters of the embedding layer were not trainable. (2)~\textbf{Word2vec}~\cite{word2vec}, to gain another perspective on tweets, we trained a word2vec model $g(token)$ on the whole dataset. Let $\mathcal{W}=\{w_1, w_2, \ldots, w_m\}$ indicate all the words in a tweet. We consider averages of the word2vec embedding of the tweets' words as their representation. Therefore, we applied this model to all the existing tweets in the train, test, and validation dataset. Finally, we utilized word2vec representation as an input vector for the trend detection model as follows:

\begin{align}
\label{eq:embedding}
    x_i = \frac{1}{m}\sum_{j=1}^{m} g(w_j) \ , w_j \in \mathcal{W}_i
\end{align}


\subsection{Contextual Knowledge Enhancement}
% For all the extracted name entities in time frame \(W\), we generate 2 feature matrices. Assume that we have \(n\) tweets and we have extracted \(k\) name entities from them. First of all, we calculate the occurrence vectors for all the extracted name entities. To calculate the occurrence vectors, we count the frequency and occurrence of name entities in each of the existing tweets. Then, we construct the occurrence matrix in which the element \(e_{ij}\) shows the frequency of name-entity \(k_i\) in tweet \(t_j\). Therefore, the dimensions of the obtained matrix will be \(k*n\). second, to consider the context in which the extracted name entities appear, we define an embedding matrix. To calculate the embedding matrix for all name-entities \(k_i\), we concatenate top k tweets with the highest number of retweet in which name entity \(k_i\) appears. Then, we use ParsBert pre-trained model to calculate the embedding vector of generated text and we consider the output as an embedding vector for name entity \(k_i\). At last, we construct an embedding matrix in which row \(i^{th}\) is the embedding vector of \(i^{th}\) name-entity.
% In the next step, we convert both occurrence and embedding matrices to similarity matrices, respectively, using the cosine similarity function. Each of the similarity matrices represents the similarity of named entities to each other. In other words, element \([ij]\) represents the similarity of name-entity \(k_i\) to name-entity \(k_j\). we should mention that we consider a threshold for the minimum similarity in both generated similarity matrices. Eventually, we use the concatenation of occurrence and embedding matrix as our final feature vector.

Working on lexical and structural aspects of the tweets is a conventional way to design event detection models, but because of the challenging characteristic of the tweets, i.e., variation in language expressions and aspects of opinions, training a well-performing model is arduous. When users debate about an occurring event on social platforms, they discuss different aspects and have diverse opinions. Therefore, we have to consider various contexts in which the users post tweets and aim to generate a high-quality representation for them.

In this work, we considered entities within a tweet as its representative, and we concentrated on two main types of entities: \textbf{Named Entities} and \textbf{Hashtags}. Named entities are critical indicators for detecting events since an event happens at a specific time and/or location and includes various actors. As such, named entities demonstrate one of these attributes about an event. Hashtags, on the other hand, were incorporated into social posts intrinsically to reveal the correlation of the tweets to an event or a topic, making them invaluable indicators of tweets related to an event.

For instance, the following text from our dataset contains both named entities and hashtags. The named entities indicate that the attributes of the event are \textbf{Silicon Valley}, \textbf{USA}, and \textbf{2008}. In addition, \textbf{\#bankruptcy} demonstrates that the event corresponds to bankruptcy.

% \textcolor{red}{I think next two should be incorporated into an example}
% \begin{itemize}
%     \item Named Entity such as "UNICEF"
%     \item Hashtags such as "\#PROTEST"
% \end{itemize}

\begin{itemize}
    \item \textbf{Silicon Valley} Bank of the \textbf{United States} collapsed! The largest bank \textbf{\#bankruptcy} in the \textbf{USA} since the crisis of \textbf{2008} !!!
\end{itemize}

In this stage, We use Bert's pre-trained NER model to extract all the existing named entities from all the tweets in the message blocks, i.e., $\mathcal{M}_i$. Afterward, we add all the posted hashtags to the extracted named entities. Assume that message block $\mathcal{M}_i$ contains $n$ tweets and there exist $k$ unique entities within them. Then, For all the extracted entities in message block $\mathcal{M}_i$, we follow a three-step procedure: 

\textbf{(1)} First, to cover lexical aspects of the tweets, we consider entity $k$ as a bag of tweets which is an aggregation of all the tweets that contain entity $k$. Then, we construct the occurrence matrix in which the element $e_{ij}$ shows the frequency of entity $k_i$ in tweet $t_j$. Therefore, the dimensions of the obtained matrix are $k \times n$. 

\textbf{(2)} Sentence embedding is a type of sentence representation that allows sentences with similar meanings to have a close representation. This idea helps us to consider the context in which the extracted entities appeared. To calculate the embedding vector for a given entity, e.g., entity \(k_i\), we choose a combination of top retweeted and random tweets in which name entity \(k_i\) appears. We defined a hyper-parameter $\beta$ to control the number of selections. Furthermore, we select $\frac{\beta}{2}$ of tweets from the top retweeted tweets and the rest randomly from the other tweets in which name entity \(k_i\) appears. Then, we concatenate the selected tweets and exploit ParsBert pre-trained models~\cite{ParsBERT} to calculate the embedding vector. At last, we repeat this procedure for all the entities and construct an embedding matrix in which row \(i^{th}\) is the contextual representation of \(i^{th}\) entity.

\textbf{(3)} By generating the mentioned matrices, we simultaneously cover lexical and contextual aspects of the tweets. In the next step, we convert occurrence and embedding matrices to entity-to-entity distance matrices separately using the cosine distance function. So far, we have generated two distance matrices representing the cosine distance of entities from each other. Afterward, we combine these distance matrices using weighted average. While averaging the distance matrices, we consider $\alpha$ and $1 - \alpha$ as a weight of embedding and occurrence matrix, respectively. Eventually, we use the generated entity-to-entity distance matrix as input for the Event Clustering component.

\subsection{Event Clustering}
Since entities are representative of the tweets, at this stage, we have to cluster entities based on the generated distance matrix in the previous module. Within this component, we can use a wide range of clustering algorithms such as distance-based clustering algorithms, e.g., K-Means, probability-based clustering algorithms, e.g., GMM, and density-based ones, e.g., DBSCAN and HDBSCAN~\cite{HDBSCAN}. Since we are working on streaming data, and the number of clusters may vary in each message block, algorithms that don't need to specify the number of clusters are the best option. Given an entity-to-entity distance matrix, we leverage HDBSCAN for this component to cluster entities in a way that related entities come together in the same cluster. So far, we clustered the appeared entities within each message block. In the next stage, we have to link these clusters. 

\subsection{Event Chain Formation}
Since events occur in an evolutionary process, we couldn't identify the unspecified events statically, and we have to track the evolution of events over message blocks. Let $\mathcal{C}_i= \{c_1, c_2, \ldots, c_t\}$ and $\mathcal{C}_{i-1}=\{c_1, c_2, \ldots, c_t\}$ denote produced clusters in $\mathcal{M}_i$ and $\mathcal{M}_{i-1}$ respectively. In this step, we start building a bipartite graph using $\mathcal{C}_i$ and $\mathcal{C}_{i-1}$. We consider clusters as nodes of the graph, and we add $edge_{tk}$ between them if the number of common entities among $c_k$ and $c_k$ was more than a minimum threshold $\delta$. Note that the weight corresponding to \(edge_{tk}\) is the number of common entities among $c_k$ and $c_k$. Then, we leverage the \textbf{Hungarian algorithm}~\cite{kuhn1955hungarian} to link produced clusters in message block $\mathcal{M}_i$ to produced clusters in message block $\mathcal{M}_{i-1}$ iteratively for $i=\{2, 3, \ldots, n\}$ where n denote the total number of message blocks. In short, the events appeared by connecting the produced clusters over continuous message blocks and generating the cluster chains. Note that a single chain represents an occurring event.

\subsection{Event Summarization}
In this stage, we try to generate summaries of the detected events by adapting the \textbf{OpinionDigest}~\cite{opiniondigest} framework to the task of event summarization. To do so, we design a 3-step procedure. \textbf{(1)} Assume $t$ is a random tweet, and we could extract key phrases from tweet $t$ by considering a context window with a constant length of $\gamma$ around the appeared entities. \textbf{(2)} In this stage, we require an NLG model which takes a set of key phrases as input and produces a fluent summary as output. Because we don't access the gold-standard summaries for training, we attempt to fine-tune a language model that encodes the extracted key phrases of a single tweet and then try to reconstruct the tweet from them. To do so, we build a synthetic dataset by mapping the existing tweets in our event detection dataset to their corresponding key phrases, similar to the first step. Furthermore, we fine-tune GPT-2 language model on the synthetic dataset. Now, we can utilize the fine-tuned model for generating event summaries. \textbf{(3)} Suppose that we want to generate a summary for event $e$, and $T=\{t_1, t_2, \ldots, t_n\}$ is set of corresponding tweets to event $e$. First, we map all the tweets to their corresponding key phrases. Then, we leverage the DBSCAN model to cluster the key phrases using their BERT representation. In each cluster, we select the closest key phrase to the center of the clusters. Eventually, we utilize the fine-tuned GPT-2 model to generate the summary of event $e$ using the selected key phrases.

\subsection{Storage}
After generating the summaries, we store the details of the identified events as JSON files in the database. While saving the results, we extract informative details of the events such as event id, event period, event summary, details of corresponding entities/hashtags/usernames, tweets with the highest like/reply/retweet count, etc. Concisely, our proposed framework gets the message blocks as input and returns an informative JSON file for each detected event.

\subsection{Evaluation}
In the final stage, we evaluate the performance of our proposed framework from various aspects. We discuss the results and evaluation details in the following section.

\section{Experiments and Evaluation Pipeline}
% In this section, we present an evaluation pipeline that appraises different aspects of the proposed method. After running the pipeline, we evaluate the performance of the system from the following aspects.
In this section, we conduct extensive experiments to answer the following research questions:
\textbf{(1)} How is the overall performance of our framework compared with the baselines?
\textbf{(2)} How does the Contextual Knowledge Enhancement component affect the clusters' quality?
\textbf{(3)} How is the overall performance of the Trending Data Extraction component, and how does it help us to achieve better results?

\subsection{Experimental Setup}

\subsubsection{Dataset}
Similar to the trend detection dataset, we have prepared a dataset for evaluating the overall performance of EnrichEvent on Twitter streaming data using SNScrape. We considered a two-week time window and crawled the posted Persian tweets from 2023/2/28 to 2023/3/14. We collected almost 760000 tweets in this period and divided them into daily message blocks. Since we had to label the entities manually, we sampled 500 tweets in each message block. We selected the tweets based on the words' frequency to preserve the distribution of tweets. In other words, if a tweet contains frequent words, the probability of choosing it will be increased. 

\subsubsection{Baselines}
We compare our proposed framework to both base algorithms and related works. (1) \textbf{Word2vec}~\cite{word2vec}, which considers averages of the word2vec embedding of words as their representation. While implementing this baseline, we only utilized the word2vec embedding in the Contextual Knowledge Enhancement component, and we didn't make use of lexical knowledge. (2) \textbf{Bert}~\cite{BERT}, which generates a 768-d embedding vector for the sampled sentences in which the entity occurs as its representative. Same as Word2vec, we used this representation in the Contextual Knowledge Enhancement component to measure its impact on the final results. (3)~\textbf{Yang et al.}~\cite{Yang2018AnED}, which considered hashtags as their main entity and used the concatenation of hashtag-hashtags and hashtag-words co-occurrence vectors as their final feature. (4)~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter}, which builds an entity graph using the similarity matrix obtained from the co-occurrence vectors. Eventually, they identified the events by applying Louvain community detection to the graph and building cluster chains. Note that we have implemented all the baselines in the same setting.

\subsubsection{Evaluation Metrics}

We have evaluated the performance of our proposed framework by two metrics presented in Fedoryszak et al.~\cite{RealTime_EventDetection_Twitter}. By measuring these metrics, we can figure out how well the events are detected and separated from each other. Suppose that our framework detected event $e$ in a set of consecutive time windows $T$, and we want to measure the quality of the cluster chain corresponding to it. \textit{Consolidation(e)} and \textit{Discrimination(e)} defined as follows. Note that if the higher value of the metric indicates better results, we put $\big\uparrow$ next to their names, and if the lower value demonstrates better results, we use $\big\downarrow$. 

(1) \textbf{Consolidation$\big\uparrow$}: We consider entity pair $(k_1, k_2)$ related if both of them are labeled as relevant entities to event $e$. Let $\mathcal{U}_t$ denote the total number of entity pairs and $\mathcal{A}_t$ indicate the number of related entity pairs that are part of the system output at time window $t$ respectively. So, Consolidation is defined as:
\begin{equation}
\begin{gathered}
   \mathcal{C}(e) = \sum_{t \in T}\frac{\mathcal{A}_t}{\mathcal{U}_t}
\end{gathered}
\end{equation}

(2) \textbf{Discrimination$\big\downarrow$}: We consider entity pair $(k_1, k_2)$ unrelated if only one of them is labeled irrelevant to event $e$. Let $B_t$ denote the number of unrelated entity pairs that are part of the system output at timestamp $t$. Discrimination is defined as:
\begin{equation}
\begin{gathered}
   \mathcal{D}(e) = \sum_{t \in T}\frac{\mathcal{B}_t}{\mathcal{U}_t}
\end{gathered}
\end{equation}

\subsubsection{Implementation Details}
We have implemented our proposed framework, i.e., EnrichEvent, using a machine equipped with a Tesla T4 GPU. While implementing the framework, we have defined some hyper-parameters to control the performance of the components. Details of the hyper-parameters are shown in Table 1.

\begin{table}
\begin{center}
{\caption{Details of the hyper-parameters}\label{table1}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|P{2cm}|P{6cm}|P{0.7cm}|}
\hline
\rule{0pt}{8pt}
Hyper-parameter    & Description &  value  \\
\hline
\rule{0pt}{9pt}
        $\sigma$        & Minimum threshold for the probability of referring to an event in the Trend Detection component            &   $0.3$     \\
\hline
\rule{0pt}{9pt}
        $\alpha$        & Weight of the entity-to-entity distance matrix obtained from the Embedding matrix          &   $0.5$     \\
\hline
\rule{0pt}{9pt}
        $\beta$         & Total number of selected tweets for generating the embedding vectors for entities       &   $4$    \\
\hline
\rule{0pt}{9pt}
        $\gamma$        & Length of the content window for extracting key phrases in the Event Summarization component         &   $2$   \\

\hline
\rule{0pt}{9pt}
       $\delta$        & Minimum number of common entities to add an edge between clusters in the Event Chain Formation component    &   $3$   \\ 
\hline
\end{tabular}}
\end{center}
\end{table}
\medskip
\subsection{Manual Evaluation}

We evaluate the overall performance of EnrichEvent as well as the selected baselines on Twitter streaming message blocks, and we present the results in Table 2. To calculate the evaluation metrics, we manually labeled the entities. Note that Consolidation and Discrimination are calculated for a single event, and the average results obtained from all the detected events are presented as the final result.

\begin{table}
\begin{center}
{\caption{Manual evaluation on streaming social data}\label{table2}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\hline
\rule{0pt}{12pt}
Method &Consolidation &Discrimination
\\
\hline
\\[-6pt]
\textbf{Word2vec}& 63.46\% & 28.94\% \\
\textbf{BERT}& 67.01\%& 15.97\% \\
\textbf{Yang et al.~\cite{Yang2018AnED}}& 38.42\%& 37.64\% \\
\textbf{Fedoryszak et al.~\cite{RealTime_EventDetection_Twitter}}& 65.20\%& 25.90\% \\
\hline
\\[-6pt]
\textbf{EnrichEvent}& \textbf{87.41\%} & \textbf{10.00\%} \\
\hline
\\[-6pt]
\textbf{Improvement}& 20.30\%$\big\uparrow$& 5.97\%$\big\downarrow$ \\
\hline
\end{tabular}}
\end{center}
\end{table}

As shown in Table 2, the results indicate that our proposed framework outperforms all the baselines. We should mention some key findings from the results are as follows:
\textbf{(1)} In section 4.3, we claimed that if we leverage the contextual and lexical knowledge simultaneously, it will empower the event detection frameworks. The obtained results validate the superiority of our proposed framework. \textbf{(2)} Comparing the results of \textbf{Fedoryszak et al.} and \textbf{BERT} demonstrate that exploiting the contextual knowledge reduces the discrimination metric surprisingly, but it doesn't affect the value of the consolidation individually. \textbf{(3)} In summary, utilizing contextual knowledge along with the lexical aspects is the key to achieving the best results.

\subsection{Clusters' Quality}
In section $4.3$, we claim that the Contextual Knowledge Enhancement component assists the HDBSCAN model in detecting related entities and grouping them. To prove our claim, we perform the following experiments:

(1) \textbf{Fraction of Relevant entities$\big\uparrow$}: During the construction of cluster chains and event identification, it's crucial that the attending clusters in the chains include Relevant entities to the events. To investigate the effect of the Contextual Knowledge Enhancement component, we calculate the ratio of Relevant entities in clusters when using contextual knowledge compared with other baselines. 

% Figure environment removed

As shown in Figure 2, EnrichEvent yields the highest fraction of related entities. That's because it makes full use of contextual representation as well as lexical representation of the tweets. Compared to Fedoryszak et al. which only utilized co-occurrence vectors as their representation, we achieve \textbf{13.98\%} promotion in the fraction of related entities. It conveys that EnrichEvent gets a good comprehension of the events and clusters them properly. To validate the quality of clusters, we also investigate the clustering metrics.

(2) \textbf{Silhouette Score$\big\uparrow$}: The silhouette score measures the similarity of points inside their cluster compared to the others~\cite{Silhouette}. Inspection of Figure 3 demonstrates that using the lexical representation leads to more similar clusters in the first two windows, but exploiting the contextual knowledge has made its impact in the following windows, and the quality of clusters increased.

% Figure environment removed

(3) \textbf{Calinski-Harabasz Index$\big\uparrow$}: Also known as the Variance Ratio Criterion, it measures the sum of between-cluster dispersion against the sum of within-cluster dispersion, where dispersion is the sum of distance squared~\cite{CalinskiHarabasz}. By comparing the value of the Calinski-Harabasz index over consecutive time frames in Figure 4, it signifies the clusters are far away from the others, and they have a higher quality when we utilize the Contextual Knowledge Enhancement component.
% Figure environment removed

(4) \textbf{Davies-Bouldin Index$\big\downarrow$}: This index signifies the average similarity between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves~\cite{DaviesBouldin}. Figure 5 verifies that our framework, i.e., the Contextual Knowledge component, utilizes the knowledge which enhances the quality of the produced clusters.
% Figure environment removed

\subsection{Evaluation of Trending Data Extraction}
Trend detection is one of the main components of our framework that filters incoming tweets based on the probability of referring to an event. We appraise this component in 2 phases.

\textbf{(1) Efficiency}: To evaluate the performance of the trend detection model, we measure commonly used classification metrics, e.g., precision, recall, and F1-score. We demonstrate the results of the trend detection model in Table 3 and draw some key findings as follows:
\begin{itemize}
    \item Our implemented model is robust in detecting the tweets which don't refer to the occurring events based on its descent performance on tweets with label 0.
    \item Regarding the tweets that referred to the events, it's crucial that the model achieves high recall and acceptable precision simultaneously. Our emphasis on the recall metric is due to the fact that we want to filter tweets with great confidence, and we prefer not to miss any tweets with label one as much as possible. Although learning discriminative patterns is strenuous in imbalanced data, but the results demonstrate that the trend detection model meets our expectations.
\end{itemize}

\begin{table}
\begin{center}
{\caption{Performance of trend detection model}\label{table3}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\hline
\rule{0pt}{12pt}
Label &Precision &Recall & F1-Score & Support
\\
\hline
\\[-6pt]
\textbf{Label 0}& 0.97& 0.90& 0.93 & 312254\\
\textbf{Label 1}& 0.53& 0.78& 0.63 & 45307\\
\hline
\\[-6pt]
\textbf{Macro Average}& 0.75& 0.84& 0.78 & 357561 \\
\textbf{Weighted Average}& 0.91 & 0.89& 0.89 & 357561 \\
\hline
\\[-6pt]
\end{tabular}}
\end{center}
\end{table}

\textbf{(2) Effectiveness}: There is immense participation from people worldwide in social media, and they invariably produce streaming social data. Scalability is one of the vital characteristics of an event detection framework, otherwise, the proposed framework would not be practical. To investigate the effectiveness of the Trending Data Extraction component, we turn it off and calculate pipeline runtime. In comparison to the previous experiments, switching off this component leads to over 50\% increase in runtime, which is a considerable amount.

\section{Conclusion}
Event detection on social media platforms is an active research area, and identification of unspecified events can provide beneficial awareness for making crucial decisions in different fields. In this paper, we design a novel event detection framework to identify unspecified events from social media data streams. Our proposed framework is not only language-independent, but also can easily be adjusted for different data types, such as news, Wikipedia, etc. In this work, we utilize combinations of lexical and contextual knowledge of tweets. The contextual knowledge enriches the model's perspective and assists the model in understanding the relationships between tweets more sufficiently. We empirically demonstrate the superiority of our framework in detecting and distinguishing social events compared to the baselines through conducted experiments.

An intriguing future research direction would be leveraging LLMs for event summarization and enhancing the process of generating event chains.


% \ack We would like to thank the referees for their comments, which
% helped improve this paper considerably

\bibliography{EnrichEvent}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
