\documentclass{EnrichEvent}
\usepackage{graphicx}
\usepackage{latexsym}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\usepackage{array}
    \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{tabularray}


%%\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use it for camera-ready versions of paper.

\begin{document}
\begin{frontmatter}


\title{EnrichEvent: Enriching Social Data with Contextual Information for Emerging Event Extraction}

% \author[A]{\fnms{Mohammadali}~\snm{Sefidi Esfahani}\thanks{Corresponding Author. Email: mohammadali.esfahani@aut.ac.ir}}
\author[A]{\fnms{Mohammadali}~\snm{Sefidi Esfahani}\orcid{0009-0007-0285-1545}\thanks{Corresponding Author: mohammadali.esfahani@aut.ac.ir}}
% \author[B]{\fnms{Mohammad}~\snm{Akbari}}
\author[B]{\fnms{Mohammad}~\snm{Akbari}\orcid{0000-0002-3321-5775}}
\address[A]{Department of Mathematics and Computer Science, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran}
\address[B]{Department of Mathematics and Computer Science, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran}
% \author[A]{\fnms{Anonymous}~\snm{Author(s)}}
% \address[A]{Paper ID : 950}

\begin{abstract}
Social platforms have emerged as crucial platforms for disseminating information and discussing real-life social events, offering researchers an excellent opportunity to design and implement novel event detection frameworks. However, most existing approaches only exploit keyword burstiness or network structures to detect unspecified events. Thus, they often need help identifying unknown events regarding the challenging nature of events and social data. Social data, e.g., tweets, is characterized by misspellings, incompleteness, word sense ambiguation, irregular language, and variation in aspects of opinions. Moreover, extracting discriminative features and patterns for evolving events by exploiting the limited structural knowledge is almost infeasible. To address these challenges, in this paper, we propose a novel framework, namely EnrichEvent, that leverages the linguistic and contextual representations of streaming social data. In particular, we leverage contextual and linguistic knowledge to detect semantically related tweets and enhance the effectiveness of the event detection approaches. Eventually, our proposed framework produces cluster chains for each event to show the evolving variation of the event through time. We conducted extensive experiments to evaluate our framework, validating its high performance and effectiveness in detecting and distinguishing unspecified social events.

%Social platforms have emerged as crucial platforms for disseminating and discussing information about real-life events, which offers an excellent opportunity for early detection of newsworthy events. Most existing approaches for event detection merely exploit keyword burstiness or network structures to detect hot events. They often need to identify emerging social events before reaching a trending state regarding the challenging nature of events and social data. Social data, especially tweets, is characterized by misspellings, incompleteness, ambiguity, irregular language, and variation in aspects of opinions. Moreover, learning the evolving characteristics of the events utilizing limited contextual knowledge is almost infeasible for machine learning models. To address these problems, we propose a framework that exploits the lexical, semantic, and contextual representations of streaming social data in this paper. In particular, we leverage contextual knowledge to detect semantically related tweets as soon as possible and enhance the quality of produced clusters. We conducted several experiments to evaluate our framework, and the experimental results demonstrate the effectiveness of our proposed framework in detecting and distinguishing social events. 

%Social media platforms with billions of active users worldwide have become an integral part of our daily lives. As a result, social media has emerged as a crucial platform for discussing and disseminating information about significant events. Real-life events are occurring and evolving in social data streams, offering an excellent opportunity for early detection of newsworthy events. However, event detection in social media faces challenges, such as its streaming and evolutionary nature and the variation in language expressions and aspects of opinions. This paper addresses these challenges and discusses the importance of quick and accurate event extraction in social media for various purposes, such as disaster management, marketing, and financial markets. Most existing methods, including those based on community detection, utilize limited knowledge as they need to consider the semantics and contextual aspects of social data streams. In this paper, we use lexical, semantic, and contextual representations of tweets entirely. We propose a novel framework that gradually groups related tweets into clusters, and by connecting them over consecutive time frames, it identifies the events. We conduct experiments, and the results demonstrate the superiority and effectiveness of our proposed framework.
\end{abstract}
\end{frontmatter}

% \begin{keyword}Event detection \sep%
%     Twitter \sep% 
%     Real-time data \sep% 
%     Online social networks \sep% 
% \end{keyword}


\section{Introduction}
% Background one of these two para
% Social media platforms have been incorporated into our daily lifestyle for socializing with friends, disseminating information, discussing events, and learning new concepts, with millions of people from all over the globe being highly active in participating on different social networks. There are currently about $5.4$ billion active users on the internet, and among them, around $4.5$ billion have more than one account in one of the existing platforms~\cite{find-ref}. As such, when an event occurs in society, experts and grassroots users start sharing and discussing different aspects of the event on social media (e.g., Twitter discussions on the Notre Dame Cathedral fire). This active presence of users has turned social media into one of the most important sources for identifying, analyzing, and investigating events and providing live updates on important events. With this sheer volume of live data available on social platforms, early detection of events from social platforms presents great opportunities to gain real-time information about newsworthy events before conventional broadcast media channels cover these. Consequently, the development of platforms for discovering social events and understanding the real-time discussions happening has attracted considerable attention in both academia and the industry.

Social media platforms have become integral to our daily routines for communicating with friends, sharing information, exchanging ideas, and acquiring knowledge~\cite{Chen2013EmergingTD}. There is massive participation from people worldwide in social media. Social network statisticians expect the number of active social media users worldwide to increase from 2.86 billion in 2017 to 4.41 billion in 2025~\cite {SocialMedia_Statistics}. As a result, social media has emerged as a crucial platform for discussing and disseminating information about significant events. When a social incident occurs, experts and ordinary users engage in discussions on social media platforms (e.g., Twitter discussions on the Notre Dame Cathedral fire) from various perspectives~\cite{Aldhaheri_2017, Sun_Xiang_Wu_2015}. The active presence of users on social media has transformed it into a primary source for identifying, analyzing, and investigating events, providing real-time updates on critical incidents. The abundance of live data on social platforms offers an excellent opportunity for early detection of newsworthy events before conventional broadcast media channels cover them. Therefore, academia and industry have demonstrated significant interest in developing platforms that can discover social events and understand real-time discussions happening on social media.
 
%Importance of the research
Traditionally, an \emph{event} is defined as an important happening in a specific time and place~\cite{allan1998topic}. With the emergence of social media, this definition expands to a happening in society that attracts a sudden burst of attention in social media, and similar to the previous explanation, it occurs in a specific time and place~\cite{boettcher2012eventradar}. Intuitively, when an event occurs, quick and accurate event identification is one of the most crucial concerns for the government, aid agencies, related organizations, and even for journalists and experts~\cite{Governments, HUANG202111}. For instance, when a natural disaster occurs, relief organizations consider event detection as one of the ways to capture different types of information, such as the severity of the incident, the number of victims, and how to provide aid for them during natural disasters. These related organizations are keen on being informed of an event occurrence as soon as possible to take appropriate actions regarding the event~\cite{nugent2017comparison, pekar2020early, sreenivasulu2020comparative}. Besides, awareness of event occurrence is vital for other purposes such as marketing, commercial, and even financial markets~\cite{Mohan, Shah}. For instance, leading commercial brands always want to discover what is happening all over the market, and this awareness, accompanying the right decisions, puts them on the path to being ahead of their rivals.

Despite its value and significance, quick and accurate identification of unspecified social events has yet to be fully investigated due to the following challenges. 1)~\textbf{Streaming and Evolutionary}. Social media services provide a streaming source of information in which people freely create content around real-world events. While working on streaming sources, processing the incoming data in a single stage is a challenging and opening problem. In addition, turning a simple happening into a hot event, which everyone will be talking about, is a complex and evolutionary phenomenon comprised of three major steps. \textbf{a)} An event happens in the real world. \textbf{b)} Observers, journalists, and ordinary people start a discussion about the occurred event on social platforms. \textbf{c)} The event turns into a hot event for a while or fades out at the beginning~\cite{Chen2013EmergingTD}. How to model the evolving characteristics of an event is an arduous task. 2) \textbf{Variation in Language Expressions and Aspects of Opinions}. Twitter users produce and consume information in a very informal and irregular way, and the published tweets usually include idioms, abbreviations, misspellings, irregular language, and emojis. Due to the huge and diverse user base, Twitter usually contains different words and word sequences describing the same idea. Also, in most languages, many words have various meanings in different contexts. For example, the word "fair" in a different context has multiple meanings of "carnival", "treating someone right", and "having light skin/hair". This issue makes it hard for machine learning models to understand users' purpose. Besides, while talking about an event, users may have different opinions and express them differently. Detecting and aggregating various opinions and aspects of events is another challenge. 


% According to the text-based structure of Twitter, this social network is one of the leading platforms for discussing different events. However, some characteristics of Twitter make the problem of event detection more challenging. The first one is the short length of tweets. It is common among users to write short tweets, and this shortness makes it harder for the NLP models to consider the contextual aspect of tweets. The second one is the informal use of language. On Twitter, users write tweets in a very different style compared with other media. Misspellings, abbreviations, and slang are common in tweets, and when these problems and shortness come together, event detection becomes more challenging. The third challenging aspect of Twitter is its dynamic nature. On Twitter, users generate data streams at every moment, which is debated in the previous paragraph. So, we cannot ignore time and detect events statically in a single moment. The last challenging aspect of Twitter is the existence of noise in tweets. Not only is there no guarantee that all the tweets are relevant, but most of them are irrelevant, and even those that are relevant may contain irrelevant terms.

%But something that needs to be considered in these definitions is the process of turning simple happenings into hot events, which everyone will talk about. In other words, hot events do not emerge in a moment and do not die a while later. At any moment, many happenings occur all over the world, but few of them turn into hot events, and this evolutionary process should be considered. Most of the time, this evolutionary process comes in 3 steps. 1)Something happens somewhere in the world. 2)People, journalists, and â€¦ start talking about it and sharing with others via different platforms and media based on the importance of the event. The more people share the event, the more trending it will be. 3)At last, some events will disappear after a while. Some become hot events in a local area, and some even become actual trending events, and people worldwide will be hearing about them. 

% should be enhanced
Retrospective studies in social media and event detection have proposed practical approaches for identifying events from social data by leveraging content and network structure. For instance, frequent keyword extraction has been used to detect bursty keywords corresponding to an event, and hashtags have been leveraged to capture semantics from tweets to address the short context problem~\cite{Keyword_Volume_Approach, Yang2018AnED}. Similarly, clustering methods have been employed for mitigating short and sparse contexts in social posts~\cite{akbari2017leveraging}. Further, modular decomposition and community detection have been exploited to utilize links and network structures. While much work has been devoted to event detection, more research has yet to tackle the problem of unspecified event detection and its evolution. In this paper, we design an end-to-end framework, namely \textbf{EnrichEvent}, to address the challenges mentioned in the previous paragraph. We define an event as a single chain of clusters where each cluster contains closely related entities both in time and semantic dimensions. In addition, we leverage contextual knowledge and structural aspects of the tweets to enrich their representation and achieve better results. Exploiting the contextual knowledge assists the model in comprehending the existing relation among the tweets and generating their representation with higher quality. We conducted several experiments to evaluate all the components of our proposed framework. The empirical results validate that EnrichEvent achieves better than various baselines based on the evaluation metrics. In summary, our main contributions are as follows:
\begin{itemize}
    \item We design a novel framework, EnrichEvent, that is robust to variation in language expressions and aspects of opinions and considers the temporal variation of context around a given event to model the evolutionary process of events occurring sufficiently.
    \item We propose a generative approach for event summarization, an innovation since most recent research utilized extractive approaches.
    \item  We provide two real-world datasets for training the trend detection model and performing experiments to evaluate our proposed event detection framework. The result empirically demonstrates the effectiveness of EnrichEvent. Moreover, researchers can exploit these datasets in topic modeling, sentiment analysis, etc.
\end{itemize}

% In this paper, We consider and add the concept of evolutionary events to the problem of event detection from Twitter. Although some recent research considers these points con, text is the critical point missed in event detection. While designing the event detection models, most recent research didn't consider the context and semantic aspects of the tweets and only focused on the lexical aspects. Event detection models implemented only based on bursty features or focusing on frequency and co-occurrence of keywords are examples of models that do not consider the context.

\section{Related Works}
% Event detection from Twitter has received noticeable attention recently, as it introduced TwitterAPI, and researchers can easily collect real-time data~\cite{Survey_2016, ED_SWE}. We can divide the problem of social event detection into three sub-categories. Identification of \textbf{unspecified events}, \textbf{predetermined events}, and \textbf{specific events}~\cite{atefeh2015survey}. While working on the first category, there is no antecedent description of the events, and they aim to detect general events. In this category, researchers typically extract features from tweets and then try to find trends in them to detect the events~\cite{Long2011TowardsEE, Becker_Naaman_Gravano_2021}. In the second category, researchers aim to detect pre-defined events with a fixed category, such as earthquake~\cite{Earthquake_Shakes_Twitter_Users}, crime and disaster events~\cite{TEDAS}, etc. In the third category, the event's details are specified, and the goal is to detect events that exactly match an explicit description of the relevant event types~\cite{Planned_Events}. Our work belongs to the first category, and in this section, we investigate these three categories and explain some related works that have motivated us to work in this field.

Event detection from Twitter has received noticeable attention in recent years, as it introduced TwitterAPI, and researchers can easily collect real-time data~\cite{Survey_2016, ED_SWE}. We can divide the problem of social event detection into three sub-categories. Identification of \textbf{unspecified events}, \textbf{predetermined events}, and \textbf{sub-events}~\cite{atefeh2015survey}. Our work belongs to the first category, and in this section, we investigate these three sub-categories and explain some related works that have motivated us to work in this field.

\subsection{Identification of unspecified events}
While working on this category, there needs to be an antecedent description of the events, and researchers aim to detect general events. Researchers typically investigate different aspects of the tweets in this category to discover their existing relations. As mentioned in the previous section, processing streaming data is a challenging task, and \textbf{Comito et al.}~\cite{Bursty_Event_Detection} tried to design an event detection framework that is robust to processing issues. Moreover, they exploited the existing semantics in the tweets by working on the 2-grams of words, hashtags, and mentioned users. Eventually, they identified the events by grouping the related tweets using the incremental clustering approach. The main idea of~\textbf{Yang et al.}~\cite{Yang2018AnED} is to cluster similar hashtags together, and clusters of hashtags represent occurring events. They focused on the occurrence of hashtags in tweets and considered hashtags as a bag of words and hashtags. Eventually, they used the concatenation of hashtag-hashtag and hashtag-words co-occurrence vectors as a final feature vector for clustering. The main challenge of this work is that users may not necessarily use hashtags in all tweets. Besides, hashtags used in tweets are not necessarily relevant to their content. ~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter} considers happening events as chains of clusters. They built a real-time system to identify groups of event-related entities. Then, they linked these clusters together and generated cluster chains that represent the events. Note that this paper only focused on the lexical representation of named entities and used occurrence vectors to cluster them.

\subsection{Identification of predetermined events}
% In the second category, researchers usually leverage datasets such as ACE 2005, Event2012, and Event 2018, which contain predefined events in different event categories. Recently, numerous frameworks have been designed, .i.g., FinEvent~\cite{peng2022reinforced}, PP-GCN~\cite{peng2021streaming}, KP-GNN~\cite{Knowledge_Preserving}, and QS-GNN~\cite{Qsgnn}. Intuitively, these papers tried to model the existing relations between tweets using message graphs in their ways. Eventually, they exploit GNN models to identify the occurring events. Most existing papers in this category divide the training into two stages. (1) \textbf{Pre-training} stage in which they train models with labeled message graph (obtained message graph from the first time frame). (2) In \textbf{fine-tuning} stage, they utilize unlabeled message graphs and try to generate high-quality pseudo labels.

In the second category, researchers usually leverage datasets such as Event2012~\cite{Event2012} and Event2018~\cite{Event2018}, which contain predefined events in different event classes. Two critical challenges exist in this category, providing researchers with opportunities to design novel frameworks. \textbf{1) Scalability}, most of the recent papers converted the message blocks to message graphs in their ways to model the dependencies between the tweets. Moreover, they leveraged the GNN models to generate the embedding of the tweets~\cite{Knowledge_Preserving, peng2021streaming, peng2022reinforced}. Although utilizing message graphs and GNN models enhances the messages' embedding, they reduce the scalability of the frameworks. \textbf{2) Usability}, the proposed frameworks should be trained on labeled datasets, which makes their use in real-world problems challenging.~\textbf{Ren et al.}~\cite{Qsgnn} investigated this challenge and designed a framework, namely QS-GNN. While training the proposed model, it only required the labeled messages in the first time frame. Although they have minimized the model's dependence on labeled data, leveraging these frameworks for real-world problems is still challenging.

\subsection{Identification of sub-events}
% \textbf{Becker et. al}~\cite{Planned_Events} leveraged various user-contributed event features to identify upcoming concerts. They also utilized term frequency-based techniques for selecting representative and descriptive event terms, which serve as complementary queries for retrieving event messages from Twitter. In summary, using our query-building strategies, they have developed a system that returns Twitter messages related to a specific event. Note that few papers have been published in this category recently.

In this category, researchers consider a specific event, e.g., a football match, and aim to identify the relevant sub-events. Designing a decent approach to find the correlations between the tweets and a given event is the main challenge of this category. ~\textbf{Bekoulis et al.}~\cite{bekoulis-sub} investigated the temporal aspects of the tweets and proposed a two-stage framework that detects the sub-events by analyzing their evolution over time. Moreover, ~\textbf{Lu et al.}~\cite{lu-sub} focused on semantics to generate a better representation for tweets. In addition, they used hashtags and their n-grams to capture the correlations suitably. Though much research has been done in this category, researchers can enhance the existing frameworks differently.

% Figure environment removed

\section{Problem Statement}
% In this section, first, we present the notation and then formally define the problem of emergent event detection from social media data.
% Let $\mathcal{T} = {t_1, t_2, \ldots, t_n }$ denote an order set of $n$ different tweets, i.e., a social stream, where each $t_i$ is associated with a timestamp indicating its posting time. We assume that these tweets are coming in $m$ different blocks of messages $\mathcal{B} = {b_1, b_2, \ldots, b_m }$. A \emph{social event} is formally defined as a set of semantically related tweets where all tweets refer to a real-world event that happened in a specific time point, $e_i = { t_1, t_2, \ldots, t_k}$. Denotes that while an event $e_i$ happened in a specific time point, it usually evolves over longer periods and includes several message blocks normally. The task addressed in this paper is to detect a set of unspecified events $\mathcal{E} = {e_1, e_2, \ldots, e_z}$ based on available social stream, i.e., $\mathcal{T}$. With the notation above, we formally define the emerging event detection problem as follows: 

% Given a social stream of microblog messages $\mathcal{T}$ with their content, we aim to learn a model as follows,
% \begin{align}
%     \mathrm{M}: \mathcal{T} \rightarrow {e_i}_{i=1}^{z},
% \end{align}
% where each $e_i$ is a sequence of message clusters $c_i$, each formed by messages in one social block. 

In this section, we present the notation and then formally define the problem of emergent event detection from social media data.

\noindent \textbf{Definition 2.1.} Social stream $\mathcal{S}=\{\mathcal{M}_1, \ldots, \mathcal{M}_{i-1}, \mathcal{M}_i, \ldots\}$ defined as a consecutive sequence of message blocks. In this definition, we denote message block $\mathcal{M}_i$ as $\mathcal{M}_i=\{m_j | 1 \le j \le |\mathcal{M}_i| \}$, where $|\mathcal{M}_i|$ is total numbers of the tweets in time period $[t_i, t_{i_1})$ and $m_j$ denotes a single tweet.

\noindent\textbf{Definition 2.2.} We can group all the tweets in message block $\mathcal{M}_i$ to finite set of clusters $\mathcal{C}_i=\{c_j | 1 \le j \le |\mathcal{C}_i| \}$, where messages in each cluster are contextually related.

\noindent\textbf{Definition 2.3.} An \emph{event} $e=\{c_{ij} | 1 \le i \le |\mathcal{M}_i| , 1 \le j \le |\mathcal{C}_i| \}$ is formally defined as a continuous sequence of contextually related clusters where all the clusters refer to a single event. Note that we assume each social message belongs to, at most, one event.

\noindent\textbf{Objective:} Given a social stream of message blocks $\mathcal{S}$, we aim to design and implement a framework $F$ as follows where each $E$ is a set of unknown social events.
\begin{align}
    F: \mathcal{S} \rightarrow E=\{e_1, e_2, \ldots\},
\end{align}



\section{Methodology}
In this section, we present various components of our proposed framework. The main advantage of the proposed framework is the capability to process streaming social data and extract candidate social events in a near online setting. In addition, our framework is language-independent and can be easily adjusted to different languages. In summary, EnrichEvent gets the message blocks as inputs and finally stores the detected events with their attributes in the database. Here, we first go through the different components of the EnrichEvent. Afterward, we define their roles and functionality and then delve into the details of each unit. Figure 1 provides an overview of the architecture of EnrichEvent.

% % Figure environment removed



\subsection{Framework}
% The proposed framework focuses on the need for an end-to-end pipeline that can handle the streaming nature of social media data by processing incoming data and generating streams of outputs. Note that events start at specific time points and can extend over multiple time windows, requiring a framework capable of processing a stream of tweets and returning cluster chains that represent these events.

% The streaming nature of social media data needs an end-to-end pipeline capable of processing incoming data and forming streams of outputs, as we focused on in this paper. Recall that events are happening at a specific point in time, and the discussion can be extended for several time windows. This setting demands a framework capable of processing a stream of tweets and returning cluster chains that represent the happening events. The proposed framework consists of four components: trending data extraction, contextual knowledge--segment--anything else enhancement, event clustering, and event chain formation. The first step, i.e., trending data extraction, distillates newsworthy tweets potentially referring to an event. After extracting the potential pool of tweets, the tweet block is sent into the Contextual Feature Enhancement to extract and enrich with semantic features, assisting the model in correlating tweets corresponding to an event but from diverging aspects. As inspired by previous studies in~\cite{ref1, ref2}, Event Clustering clusters semantically related tweets to groups, i.e., perform event detection. The later step, i.e., chain construction, attempts to create the evolving chain of events through various time windows. 

Since events start at a specific time point and can extend over multiple time windows, in this paper, we focus on the need for an end-to-end pipeline that can handle the streaming nature of social media data by processing incoming data and detecting unspecified events. Our proposed framework, i.e., \textbf{EnrichEvent}, comprises seven components: \textbf{Trending Data Extraction}, \textbf{Contextual Knowledge Enhancement}, \textbf{Event Clustering}, \textbf{Event Chain Formation}, \textbf{Event Summarization}, \textbf{Storage}, and \textbf{Evaluation}. First, Trending Data Extraction identifies newsworthy tweets that potentially refer to an occurring event. The message block is then sent to Contextual Knowledge Enhancement to extract and enrich contextual knowledge, which assists the clustering model in comprehending the existing patterns among the tweets. Inspired by previous studies ~\cite{RealTime_EventDetection_Twitter, Bursty_Event_Detection}, Event Clustering groups the semantically related entities into groups. Then, Event Chain Formation links the clusters in consecutive time steps and generates evolving chains of clusters that present the occurring events. In the following stage, all the identified events are summarized using a generative approach. Eventually, the framework generates a JSON file as the output and stores it in the database. In addition, it evaluates the results at the final stage.

\subsection{Trending Data Extraction}

Social media data is often known as a noisy and sparse information source, and $40\%$ of tweets refer to \emph{pointless babble}. Thus, it is crucial to filter out irrelevant data and extract tweets corresponding to the events~\cite{SEDTW}. In other words, we had to appropriately filter out noisy data and gather valuable and informative ones. To do so, we proposed filtering general tweets and accumulating those with a high probability of referring to an event. Here, we leverage a supervised learning model to mark tweets potentially referring to the events. Intuitively, our model examines the tweets to determine whether they refer to an important event. We achieved this through a two-segment architecture. In the first segment, a combination of embedding, convolution, and max pooling layers extract efficient features from the input tweet. In the second segment, we exploited fully connected layers to classify whether the incoming tweet refers to an important event. In summary, the proposed model gets a tweet as input and returns the probability of referring to an event, which helps us to filter the tweets based on the minimum threshold $\lambda$.

\textbf{Trend detection dataset}: To train the trend detection model, we started preparing an offline trend detection dataset using TwitterAPI. We have collected 1.6 million Persian tweets in different categories and tried to label them manually based on key phrases. The generated dataset has informative columns such as \textit{timestamp}, \textit{tweet id}, \textit{text}, \textit{username}, \textit{reply count}, \textit{like count}, and \textit{retweet count}. Eventually, we utilized it for training the trend detection model. We have provided more details about the trend detection dataset in Appendix A.

Inspired by retrospective studies~\cite{Chen2013EmergingTD}, we adopt a supervised learning approach and train a model $\mathcal{T}$ that identifies tweets with a high probability of referring to an event. Given a message block, we converted the existing tweets to sequences of numbers and considered them as the inputs of trend detection models. Additionally, We should extract a prominent and discriminative representation from the tweets to train an effective model using the pre-trained language models. Therefore, we used Parsbert's pre-trained model~\cite{ParsBERT} to obtain the tweets' representation. While training the model, we use this representation in the embedding layer to understand the context better. Note that we did not fine-tune the Bert model, and the parameters of the embedding layer were not trainable. This model gains extracted features from tweets as input vectors, i.e., $X$, and computes the probability of containing general information or a simple status update, i.e., negative class, or referring to a potential event, i.e., positive class. Formally, the prediction of model $\mathcal{T}$ is performed by solving the following optimization problem:


\begin{align}
   \arg \min_\mathbf{W} \mathcal{L}(\mathbf{X}, \mathbf{W}, \mathbf{Y}),
\end{align}
Where $\mathcal{L}(.)$ is the loss function and often is defined as binary cross entropy. $\mathbf{W}$ denotes all parameters that the model learns from the training set, and $\mathbf{Y}$ indicates the label of the tweets where one is used for tweets referring to an event, otherwise zero. 

% (2)~\textbf{Word2vec}~\cite{word2vec}, to gain another perspective on tweets, we trained a word2vec model $g(token)$ on the whole dataset. Let $\mathcal{W}=\{w_1, w_2, \ldots, w_m\}$ indicate all the words in a tweet. We consider averages of the word2vec embedding of the tweets' words as their representation. Therefore, we applied this model to all the existing tweets in the train, test, and validation dataset. Finally, we utilized word2vec representation as an input vector for the trend detection model as follows:

% \begin{align}
% \label{eq:embedding}
%     x_i = \frac{1}{m}\sum_{j=1}^{m} g(w_j) \ , w_j \in \mathcal{W}_i
% \end{align}


\subsection{Contextual Knowledge Enhancement}

Working on lexical and structural aspects of the tweets is a conventional way to design event detection models. However, training a well-performing model is arduous because of the challenging characteristics of the tweets, i.e., variation in language expressions and aspects of opinions. When users debate about an occurring event on social platforms, they discuss different aspects and have diverse opinions. Therefore, we have to consider various contexts where users post tweets and aim to generate a high-quality representation for them.

In this work, we considered entities within a tweet as its representative, and concentrated on two main types of entities: \textbf{Named Entities} and \textbf{Hashtags}. Named entities are critical indicators for detecting events since an event happens at a specific time and location and includes various actors. As such, named entities demonstrate one of these attributes about an event. Hashtags, on the other hand, were incorporated into social posts intrinsically to reveal the correlation of the tweets to an event or a topic, making them invaluable indicators of tweets related to an event. For instance, the following text from our dataset contains both named entities and hashtags. The named entities indicate that the attributes of the event are \textbf{Silicon Valley}, \textbf{USA}, and \textbf{2008}. In addition, \textbf{\#bankruptcy} demonstrates that the event corresponds to bankruptcy.

\begin{itemize}
    \item \textbf{Silicon Valley} Bank of the \textbf{United States} collapsed! The largest bank \textbf{\#bankruptcy} in the \textbf{USA} since the crisis of \textbf{2008} !!!
\end{itemize}

In this stage, We use Bert's pre-trained NER model to extract all the existing named entities from all the tweets in the message blocks, i.e., $\mathcal{M}_i$. Afterward, we add all the posted hashtags to the extracted named entities. Assume that message block $\mathcal{M}_i$ contains $|\mathcal{M}_i|$ tweets, and there exist $k$ unique entities within them. Then, For all the extracted entities in message block $\mathcal{M}_i$, we follow a three-step procedure: 

\noindent\textbf{(1)} To cover lexical aspects of the tweets, we consider entity $k$ as a bag of tweets, which is an aggregation of all the tweets containing entity $k$. Then, we construct the occurrence matrix in which the element $e_{ij}$ shows the frequency of entity $k_i$ in tweet $t_j$. Therefore, the dimension of the obtained occurrence matrix is $k \times |\mathcal{M}_i|$. 

\noindent\textbf{(2)} Sentence embedding is a type of sentence representation that allows sentences with similar meanings to have a close representation. This idea helps us to consider the context in which the extracted entities appeared. To calculate the embedding vector for a given entity, e.g., entity \(k_i\), we choose a combination of top retweeted and random tweets in which entity \(k_i\) appears. We defined a hyper-parameter $\beta$ to control the number of selections. Furthermore, we select $\frac{\beta}{2}$ of tweets from the top retweeted tweets and the rest randomly from the other tweets in which name entity \(k_i\) appears. Then, we concatenate the selected tweets and exploit ParsBert pre-trained models~\cite{ParsBERT} to calculate the embedding vector. At last, we repeat this procedure for all the entities and construct an embedding matrix in which row \(i^{th}\) is the contextual representation of \(i^{th}\) entity.

\noindent\textbf{(3)} By generating the mentioned matrices, we simultaneously cover lexical and contextual aspects of the tweets. In the next step, we convert occurrence and embedding matrices to entity-to-entity distance matrices separately using the cosine distance function. So far, we have generated two distance matrices representing the cosine distance of entities from each other. Afterward, we aggregate these distance matrices using a weighted average. While averaging the distance matrices, we consider $\alpha$ and $1 - \alpha$ as a weight of embedding and occurrence matrix, respectively. Eventually, we use the generated entity-to-entity distance matrix as input for the Event Clustering component.

\subsection{Event Clustering}
Since entities are representative of the tweets, at this stage, we have to cluster entities based on the obtained distance matrix in the previous module. Within this component, we can use a wide range of clustering algorithms such as distance-based clustering algorithms, e.g., K-Means, probability-based clustering algorithms, e.g., GMM, and density-based ones, e.g., DBSCAN and HDBSCAN~\cite{HDBSCAN}. Since we are working on streaming data, and the number of clusters may vary in each message block, algorithms that do not need to specify the number of clusters are the best option. Given an entity-to-entity distance matrix, we leverage HDBSCAN for this component to cluster entities so that related entities come together in the same cluster. So far, we have clustered the entities within each message block. In the next stage, we have to link these clusters. 

\subsection{Event Chain Formation}
Since events occur in an evolutionary process, we cannot identify the unspecified events statically, and we have to track the evolution of events over message blocks. Let $\mathcal{C}_i= \{c_1, c_2, \ldots, c_t\}$ and $\mathcal{C}_{i-1}=\{c_1, c_2, \ldots, c_t\}$ denote produced clusters in $\mathcal{M}_i$ and $\mathcal{M}_{i-1}$ respectively. In this step, we start building a bipartite graph using $\mathcal{C}_i$ and $\mathcal{C}_{i-1}$. We consider clusters as nodes of the graph, and we add $edge_{tk}$ between them if the number of common entities among $c_k$ and $c_k$ was more than a minimum threshold $\delta$. Note that the weight corresponding to \(edge_{tk}\) is the number of common entities among $c_k$ and $c_k$. Then, we leverage the \textbf{Hungarian algorithm}~\cite{kuhn1955hungarian} to link produced clusters in message block $\mathcal{M}_i$ to produced clusters in message block $\mathcal{M}_{i-1}$ iteratively for $i=\{2, 3, \ldots, |\mathcal{S}_i|\}$ where $|\mathcal{S}_i|$ denote the total number of message blocks. In short, the events appeared by connecting the produced clusters over continuous message blocks and generating the cluster chains. Note that a single chain represents an occurring event.

\subsection{Event Summarization}
In this stage, we try to generate summaries of the detected events by adapting the \textbf{OpinionDigest}~\cite{opiniondigest} framework to the task of event summarization. To do so, we design a 3-step procedure. \textbf{(1)} Assume $t$ is a random tweet, and we could extract key phrases from tweet $t$ by considering a context window with a constant length of $\gamma$ around the appeared entities. \textbf{(2)} In this stage, we require an NLG model that takes a set of key phrases as input and produces a fluent summary as output. Because we do not access the gold-standard summaries for training, we attempt to fine-tune a language model that encodes the extracted key phrases of a single tweet and then try to reconstruct the tweet from them. To do so, we build a synthetic dataset by mapping the existing tweets in our event detection dataset to their corresponding key phrases, similar to the first step. Furthermore, we fine-tune the GPT-2 language model on the synthetic dataset. Now, we can utilize the fine-tuned model for generating event summaries. \textbf{(3)} Suppose that we want to generate a summary for event $e$, and $T=\{t_1, t_2, \ldots, t_n\}$ is set of corresponding tweets to event $e$. First, we map all the tweets to their corresponding key phrases. Then, we leverage the DBSCAN model to cluster the key phrases using their BERT representation. In each cluster, we select the closest key phrase to the center of the clusters. Eventually, we utilize the fine-tuned GPT-2 model to generate the summary of event $e$ using the selected key phrases.

\subsection{Storage}
After generating the summaries, we store the details of the identified events as JSON files in the database. While saving the results, we extract informative details of the events, such as event ID, event period, event summary, details of corresponding entities/hashtags/usernames to each event, and tweets with the highest like/reply/retweet count. Concisely, our proposed framework gets the message blocks as input and returns an informative JSON file for each detected event.

\subsection{Evaluation}
In the final stage, we evaluate the performance of our proposed framework from various aspects. We discuss the results and evaluation details in the following section.

\section{Experiments and Evaluation Pipeline}
% In this section, we present an evaluation pipeline that appraises different aspects of the proposed method. After running the pipeline, we evaluate the system's performance from the following aspects.
In this section, we conduct extensive experiments to answer the following research questions:
\textbf{(1)} How is the overall performance of our framework compared with the baselines?
\textbf{(2)} How does the Contextual Knowledge Enhancement component affect the clusters' quality?
\textbf{(3)} How is the overall performance of the Trending Data Extraction component, and how does it help us to achieve better results?

\subsection{Experimental Setup}
\subsubsection{Dataset}
Similar to the trend detection dataset, we have prepared a dataset for evaluating the overall performance of EnrichEvent on Twitter streaming data using TwitterAPI. We considered a two-week time window and crawled the posted Persian tweets from 2023/2/28 to 2023/3/14. We collected almost 760000 tweets in this period and divided them into daily message blocks. Since we had to label the entities manually, we sampled 500 tweets in each message block. We selected the tweets based on the words' frequency to preserve the actual distribution of tweets. In other words, if a tweet contains frequent words, the probability of choosing it will be higher. We provided more details about the event detection dataset in Appendix A.

\subsubsection{Baselines}
We compare our proposed framework to both base algorithms and related works. (1) \textbf{Word2vec}~\cite{word2vec}, which considers averages of the word2vec embedding of words as their representation. While implementing this baseline, we only utilized the word2vec embedding in the Contextual Knowledge Enhancement component and did not use lexical knowledge. (2) \textbf{Bert}~\cite{BERT}, which generates a 768-d embedding vector for the sampled sentences in which the entity occurs as its representative. Like Word2vec, we used this representation in the Contextual Knowledge Enhancement component to measure its impact on the final results. (3)~\textbf{Yang et al.}~\cite{Yang2018AnED}, which considered hashtags as their main entity and used the concatenation of hashtag-hashtags and hashtag-words co-occurrence vectors as their final feature. (4)~\textbf{Fedoryszak et al.}~\cite{RealTime_EventDetection_Twitter}, which builds an entity graph using the similarity matrix obtained from the co-occurrence vectors. Eventually, they identified the events by applying Louvain community detection to the graph and building cluster chains. Note that we have implemented all the baselines in the same setting.

\subsubsection{Evaluation Metrics}

We have evaluated the performance of our proposed framework by two metrics presented in Fedoryszak et al.~\cite{RealTime_EventDetection_Twitter}. By measuring these metrics, we can figure out how well the events are detected and separated from each other. Suppose that our framework detected event $e$ in a set of consecutive time windows $T_e$, and we want to measure the quality of the corresponding cluster chain. \textit{Consolidation(e)} and \textit{Discrimination(e)} defined as follows. Note that if the metric's higher value indicates better results, we use $\big\uparrow$ next to their names; if the lower value demonstrates better results, we use $\big\downarrow$.

\noindent(1) \textbf{Consolidation$\big\uparrow$}: We consider entity pair $(k_1, k_2)$ related if both of them are labeled as relevant entities to event $e$. Let $\mathcal{U}_t$ denote the total number of entity pairs and $\mathcal{A}_t$ indicate the number of related entity pairs that are part of the system output at time window $t$ respectively. Intuitively, the Consolidation metric calculates the average ratio of related entity pairs in cluster chains and is defined as:
\begin{equation}
\begin{gathered}
   \mathcal{C}(e) = \frac{1}{|T_e|}\sum_{t \in T_e}\frac{\mathcal{A}_t}{\mathcal{U}_t}
\end{gathered}
\end{equation}

\noindent(2) \textbf{Discrimination$\big\downarrow$}: We consider entity pair $(k_1, k_2)$ unrelated if only one of them is labeled irrelevant to event $e$. Let $B_t$ denote the number of unrelated entity pairs that are part of the system output at timestamp $t$. The Discrimination is defined as follows and measures the average ratio of unrelated entity pairs in cluster chains:
\begin{equation}
\begin{gathered}
   \mathcal{D}(e) = \frac{1}{|T_e|}\sum_{t \in T_e}\frac{\mathcal{B}_t}{\mathcal{U}_t}
\end{gathered}
\end{equation}

\subsubsection{Implementation Details}
We have implemented our proposed framework, i.e., EnrichEvent, using a machine equipped with a Tesla T4 GPU, and its source code is publicly available on Github. Details of the hyper-parameters are shown in Table 1. 

\begin{table}
\begin{center}
{\caption{Details of the hyper-parameters}\label{table1}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|P{2cm}|P{6cm}|P{0.7cm}|}
\hline
\rule{0pt}{8pt}
Hyper-parameter    & Description &  value  \\
\hline
\rule{0pt}{9pt}
        $\sigma$        & Minimum threshold for the probability of referring to an event in the Trend Detection component            &   $0.3$     \\
\hline
\rule{0pt}{9pt}
        $\alpha$        & Weight of the entity-to-entity distance matrix obtained from the Embedding matrix          &   $0.5$     \\
\hline
\rule{0pt}{9pt}
        $\beta$         & Total number of selected tweets for generating the embedding vectors for entities       &   $4$    \\
\hline
\rule{0pt}{9pt}
        $\gamma$        & length of the content window for extracting key phrases in the Event Summarization component         &   $2$   \\

\hline
\rule{0pt}{9pt}
       $\delta$        & Minimum number of common entities to add an edge between clusters in the Event Chain Formation component    &   $3$   \\ 
\hline
\end{tabular}}
\end{center}
\end{table}
\medskip
\subsection{Manual Evaluation}

We evaluated the overall performance of EnrichEvent and the selected baselines on Twitter streaming message blocks and presented the results in Table 2. To calculate the evaluation metrics, we manually labeled the entities. Note that Consolidation and Discrimination are calculated for a single event, and the average results obtained from all the detected events are presented as the final result.

\begin{table}
\begin{center}
{\caption{Manual evaluation on streaming social data}\label{table2}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\hline
\rule{0pt}{12pt}
Method &Consolidation &Discrimination
\\
\hline
\\[-6pt]
\textbf{Word2vec}& 63.46\% & 28.94\% \\
\textbf{BERT}& 67.01\%& 15.97\% \\
\textbf{Yang et al.~\cite{Yang2018AnED}}& 38.42\%& 37.64\% \\
\textbf{Fedoryszak et al.~\cite{RealTime_EventDetection_Twitter}}& 65.20\%& 25.90\% \\
\hline
\\[-6pt]
\textbf{EnrichEvent}& \textbf{87.41\%} & \textbf{10.00\%} \\
\hline
\\[-6pt]
\textbf{Improvement}& 20.30\%$\big\uparrow$& 5.97\%$\big\downarrow$ \\
\hline
\end{tabular}}
\end{center}
\end{table}

\noindent \textbf{Discussion}: As shown in Table 2, the results indicate that our proposed framework outperforms all the baselines. We should mention some key findings from the results are as follows:
\textbf{(1)} In section 4.3, we claimed that if we leverage the contextual and lexical knowledge simultaneously, it will empower the event detection frameworks. The obtained results validate the superiority of our proposed framework compared to other baselines. \textbf{(2)} Results of \textbf{Yang et al.} reveals that hashtags are not good representatives of tweets. They do not transfer considerable knowledge for event detection since users do not usually use hashtags while posting tweets. \textbf{(3)} Comparing the results of \textbf{Fedoryszak et al.} and \textbf{BERT} demonstrate that exploiting the contextual knowledge reduces the discrimination metric surprisingly by 10\%, but it does not affect the value of the Consolidation individually. \textbf{(4)} Comparing the results of \textbf{BERT} and \textbf{EnrichEvent} reveals that utilizing contextual knowledge along with the lexical aspects increases the ratio of related entity pairs in cluster chains by 20.30\% and is the key to achieving the best results.

\subsection{Clusters' Quality}
In section $4.3$, we claim that the Contextual Knowledge Enhancement component assists the HDBSCAN model in detecting related entities and grouping them. To prove our claim, we perform the following experiments:

\noindent(1) \textbf{Fraction of Relevant entities$\big\uparrow$}: During the construction of cluster chains and event identification, it is crucial that the attending clusters in the chains include Relevant entities to the events. To investigate the effect of the Contextual Knowledge Enhancement component, we calculate the ratio of Relevant entities in clusters when using contextual knowledge compared with other baselines. 

% Figure environment removed

As shown in Figure 2, EnrichEvent yields the highest fraction of related entities. That is because it fully uses contextual and lexical representations of the tweets. Compared to Fedoryszak et al., which only utilized co-occurrence vectors as their representation, we achieve \textbf{13.98\%} promotion in the fraction of related entities. It conveys that EnrichEvent gets a good comprehension of the events and clusters them properly. To validate the quality of clusters, we also investigate the clustering metrics.

\noindent(2) \textbf{Silhouette Score$\big\uparrow$}: The silhouette score measures the similarity of points inside their cluster compared to the others~\cite{Silhouette}. Inspection of Figure 3 demonstrates that using the lexical representation leads to more similar clusters in the first two windows, but exploiting the contextual knowledge has made its impact in the following windows, and the quality of clusters increased.

% Figure environment removed

\noindent(3) \textbf{Calinski-Harabasz Index$\big\uparrow$}: Also known as the Variance Ratio Criterion, it measures the sum of between-cluster dispersion against the sum of within-cluster dispersion, where dispersion is the sum of distance squared~\cite{CalinskiHarabasz}. Comparing the value of the Calinski-Harabasz index over consecutive time frames in Figure 4 signifies that the clusters are far away from the others and have a higher quality when we utilize the Contextual Knowledge Enhancement component.
% Figure environment removed

\noindent(4) \textbf{Davies-Bouldin Index$\big\downarrow$}: This index signifies the average similarity between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves~\cite{DaviesBouldin}. Figure 5 verifies that our framework, i.e., the Contextual Knowledge component, utilizes the knowledge that enhances the quality of the produced clusters.
% Figure environment removed

\subsection{Evaluation of Trending Data Extraction}
Trend detection is one of the main components of our framework that filters incoming tweets based on the probability of referring to an event. We appraise this component in 2 phases.

\noindent \textbf{(1) Efficiency}: There is immense participation from people worldwide in social media, and they invariably produce streaming social data. Scalability is one of the vital characteristics of an event detection framework. Otherwise, the proposed framework would not be practical. To investigate the effectiveness of the Trending Data Extraction component, we turn it off and calculate pipeline runtime. Compared to the previous experiments, switching off this component leads to an over 50\% increase in runtime, which is a considerable amount. In addition, the trend detection module can be used to customize the event detection process. More specifically, we can exploit the domain-specific trend detection models to identify the events in the specific domains. For instance, if we only want to detect sports-related events,  we can easily adjust the trend detection module and leverage a model trained to identify the tweets in which the user referred to an essential event in sports. 

\noindent \textbf{(2) Efficacy}: To evaluate the performance of the trend detection model, we measure commonly used classification metrics, e.g., Precision, Recall, and F1-Score. We demonstrate the results of the trend detection model in Table 3 and draw some key findings as follows:
\begin{itemize}
    \item Our implemented model is robust in detecting the tweets that do not refer to the occurring events based on its descent performance on tweets with label 0.
    \item Regarding the tweets that referred to the events, it is crucial that the model achieves high Recall and acceptable Precision simultaneously. We emphasize the Recall metric since we want to filter tweets with great confidence, and we prefer to see all tweets with label one as much as possible. Although learning discriminative patterns is strenuous in imbalanced data, the results demonstrate that the trend detection model meets our expectations.
\end{itemize}

\begin{table}
\begin{center}
{\caption{Performance of trend detection model}\label{table3}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lcccc}
\hline
\rule{0pt}{12pt}
Label &Precision &Recall & F1-Score & Support
\\
\hline
\\[-6pt]
\textbf{Label 0}& 0.97& 0.90& 0.93 & 312254\\
\textbf{Label 1}& 0.53& 0.78& 0.63 & 45307\\
\hline
\\[-6pt]
\textbf{Macro Average}& 0.75& 0.84& 0.78 & 357561 \\
\textbf{Weighted Average}& 0.91 & 0.89& 0.89 & 357561 \\
\hline
\\[-6pt]
\end{tabular}}
\end{center}
\end{table}

\section{Conclusion}
Event detection on social media platforms is an active research area, and identification of unspecified events can provide beneficial awareness for making crucial decisions in different fields. In this paper, we design a novel event detection framework to identify unspecified events from social media data streams. Our proposed language-independent framework can easily be adjusted for different data types like news, Wikipedia, etc. In this work, we utilize combinations of lexical and contextual knowledge of tweets. The contextual knowledge enriches the model's perspective and assists the model in understanding the relationships between tweets more sufficiently. We empirically demonstrate the superiority of our framework in detecting and distinguishing social events compared to the baselines through conducted experiments. An intriguing future research direction would be leveraging LLMs for event summarization and enhancing the process of generating event chains.


% \ack We would like to thank the referees for their comments, which
% helped improve this paper considerably

\bibliography{EnrichEvent}

\appendix

\section{Appendix}

\subsection{Datasets Details}
\noindent\textbf{Labeling of Trend Detection Dataset}: To prepare a generalized trend detection dataset, we defined 12 categories for tweets' topics. Furthermore, we explored various accounts and collected all the posted tweets from all the experts and active users in each category using TwitterAPI. We leveraged domain-specific key phrases to label the collected tweets in each category. Specifically, we defined a collection of key phrases for each category and checked their occurrence within the tweets to label the dataset. We consider labeling one for the tweets even if one of the key phrases appears in them.

\begin{table}
\begin{center}
{\caption{12 categories for tweets' topics}}
\resizebox{\columnwidth}{!}{
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\rule{0pt}{9pt}
Sports    & Technology &  Art & Industry & Celebrities    & Video Games\\
\hline
\rule{0pt}{9pt}
Crypto & Politics & Health    & Environment &  Economic & Social  \\
\hline
\end{tabular}}
\end{center}
\end{table}

\noindent\textbf{Event Detection Dataset}: In contrast to the Trend Detection Dataset, we did not consider any user specification for the Event Detection Dataset, and we only limited the language of the tweets to Persian.


% \subsection{Event Detection Dataset}
% Event detection on social media platforms is an active research area, and identification of unspecified events can provide beneficial awareness for making crucial decisions in different fields. In this paper, we design a novel event detection framework to identify unspecified events from social media data streams. Our proposed language-independent framework can easily be adjusted for different data types like news, Wikipedia, etc. In this work, we utilize combinations of lexical and contextual knowledge of tweets. The contextual knowledge enriches the model's perspective and assists the model in understanding the relationships between tweets more sufficiently. We empirically demonstrate the superiority of our framework in detecting and distinguishing social events compared to the baselines through conducted experiments. An intriguing future research direction would be leveraging LLMs for event summarization and enhancing the process of generating event chains.
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


