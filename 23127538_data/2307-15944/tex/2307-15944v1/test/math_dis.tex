\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}

 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
 \usepackage{graphicx}
 \usepackage{titling}

%  \title{Advances in Single and Multi-Antenna Technologies for Energy-Efficient IoT
% }
% \author{Gilles Callebaut}
% \date{November 2022}
 
 \usepackage{fancyhdr}
% \fancypagestyle{plain}{%  the preset of fancyhdr 
%     \fancyhf{} % clear all header and footer fields
%     \fancyfoot[R]{% Figure removed}
%     \fancyfoot[L]{\thedate}
%     \fancyhead[L]{Description of Assignment}
%     \fancyhead[R]{\theauthor}
% }
\makeatletter
\def\@maketitle{%
  \newpage
  \null
  \vskip 1em%
  \begin{center}%
  \let \footnote \thanks
    {\LARGE \@title \par}%
    \vskip 1em%
    %{\large \@date}%
  \end{center}%
  \par
  \vskip 1em}
\makeatother

\usepackage{lipsum}  
\usepackage{cmbright}

\begin{document}

% \maketitle

% \noindent\begin{tabular}{@{}ll}
%     Student & \theauthor\\
%      Promotor &  dr. Gilles Callebaut\\
%      Co-promotors & ing. Jarne Van Mulders, ing. Guus Leenders
% \end{tabular}

\section *{social dilemmas problems in cooperative robotics}
Cooperative robotics is the field of robotics that deals with the coordination and cooperation of multiple robots working together to accomplish a common task. One of the main challenges in cooperative robotics is dealing with social dilemmas, which are situations where the optimal behavior for an individual robot may not align with the optimal behavior for the group of robots.\textbf{(LoLA)}\\

One common example of a social dilemma in cooperative robotics is the tragedy of the commons, where individual robots may be motivated to consume a shared resource (such as battery power or communication bandwidth) without considering the impact on the group. This can lead to suboptimal performance or even the failure of the group task.\\

Another example of a social dilemma in cooperative robotics is the free-rider problem, where individual robots may be motivated to avoid performing their fair share of the work, leaving the other robots to do more than their fair share. This can lead to suboptimal performance or even the failure of the group task.\textbf{(fairness idea)}\\

There are different ways to tackle these issues, from enforcing rules and regulations to designing reward functions that encourage cooperation, to implementing communication protocols that facilitate coordination and cooperation.\textbf{(LIO)}
\section*{Objective}  
The goal is to create an adversarial act that may ensure that a certain agent will receive the most reward in comparison to other agents.
This is made feasible by the fact that the LIO's paper figure (Figure\ref{fig:LIO_ER_inc}), illustrates how the cooperator(one of the agents) receive reward by pulling the lever while the winner ends up not receiving any incentives for reaching the door.\\
Incetivized reward based on the game where the "pulling the lever" action has a $-1$ reward, receives an incentive of $ \sim 1$.

% Figure environment removed

We might be able to ensure the winning position for this agent by using partial communication if the adversary can reject any incentive function from other agents while continuing sending incentive functions to other agents. 

\section*{Notation}
$i$ the agent reward giver,\\
$j$ is the reward receiver.\\
$-i$ collection of all indices except i

\section*{Pure PG}
The gradient of the expected return with respect to the policy parameters is given by:

\[\nabla_{\theta} J(\theta) = \mathbb{E} \left[ \nabla_{\theta} \log \pi(a|s; \theta) Q(s,a) \right]\]
Where:\\
$\pi(a|s; \theta)$ is the policy, which is a probability distribution over actions given states, and $\theta$ are the policy parameters
$Q(s,a)$ is the action-value function, which estimates the expected return for taking action $a$ in state $s$

The gradient is calculated by taking the expectation over the states and actions that the agent encounters while following the policy. The agent then updates its policy by moving in the direction of this gradient, using an optimization algorithm such as gradient ascent.
\section*{LIO agent}


% Figure environment removed

for a genuine collaborative agent, we have this reward function :
\begin{equation} \label{eq:3}
r^j\left(s_t, \mathbf{a}_t, \eta^{-j}\right):=r^{j, \mathrm{env}}\left(s_t, \mathbf{a}_t\right)+\sum_{i \neq j} r_{\eta^i}^j\left(o_t^i, a_t^{-i}\right)
\end{equation}

\begin{equation}
\max _{\theta^j} J^{\text {policy }}\left(\theta^j, \eta^{-j}\right):=\mathbb{E}_\pi\left[\sum_{t=0}^T \gamma^t r^j\left(s_t, \mathbf{a}_t, \eta^{-j}\right)\right]
\end{equation}

\begin{equation}
G_t^j\left(\tau^j, \eta^{-j}\right)=\sum_{l=t}^T \gamma^{l-t} r^j\left(s_l, \mathbf{a}_l, \eta^{-j}\right)
\end{equation}

\begin{equation}\label{eq:1}
\hat{\theta}^j \leftarrow \theta^j+\beta \left(  \sum_{t=0}^T \nabla_{\theta^j} \log \pi^j\left(a_t^j \mid o_t^j\right) G_t^j\left(\tau^j ; \eta^{-j}\right)   \right)
\end{equation}


\begin{equation}\label{eq:2}
\max _{\eta^i} J^i\left(\hat{\tau}^i, \tau^i, \hat{\boldsymbol{\theta}}, \eta^i\right):=\mathbb{E}_{\hat{\boldsymbol{\pi}}}\left[\sum_{t=0}^T \gamma^t \hat{r}_t^{i, \text { env }}\right]-\alpha L\left(\eta^i, \tau^i\right)
\end{equation}

\begin{equation}
L\left(\eta^i, \tau^i\right):=\sum_{\left(o_t^i, a_t^{-i}\right) \in \tau^i} \gamma^t\left\|r_{\eta^i}\left(o_t^i, a_t^{-i}\right)\right\|_1
\end{equation}

\begin{equation}
\nabla_{\hat{\theta}^j} J^i\left(\hat{\tau}^i, \hat{\boldsymbol{\theta}}\right)=\mathbb{E}_{\hat{\boldsymbol{\pi}}}\left[\nabla_{\hat{\theta}^j} \log \hat{\pi}^j\left(\hat{a}^j \hat{o}^j\right) Q^{i, \hat{\boldsymbol{\pi}}}(\hat{s}, \hat{\mathbf{a}})\right]
\end{equation}


\begin{algorithm}
\caption{LIO agent}\label{alg:LIO}
\begin{algorithmic}
\State \textbf{Initialize} all agentsâ€™ policy parameters $\theta^i$, incentive function parameters $\eta^i$
\State \textbf{for} each iteration \textbf{do}
\State \quad Generate a trajectory {$\tau^j$ } using $\theta$ and $\eta$
\State \quad For all reward-recipients $j$, update $\hat{\theta ^ j}$ using \ref{eq:1} 
\State \quad Generate a new trajectory {$\hat{\tau ^ i}$} using new $\hat{\theta}$
\State \quad For reward-givers $i$, compute new $\hat{\eta^i}$ by gradient ascent on \ref{eq:2}
\State \quad $\theta ^i \gets \hat{\theta^i}$, $\eta^i \gets \hat{\eta^i}$ for all $i \in [N]$.
\end{algorithmic}
\end{algorithm}

\section*{Partial communication}


adversarial agents have partial communication which is generating the incentivized reward for other agents, on the other hand,  it will ignore the reward function sent to it by other agents. This method will cause this agent to seek the max reward that exists in the environment, not the max reward generated by adding the extrinsic(environment) reward + incentivized reward.\\
To do so from \ref{eq:3} we remove the second part to prevent the agent to receive the reward which leaves the reward function as $r^{j, \mathrm{env}}\left(s_t, \mathbf{a}_t\right)$, then we can calculate the new $\hat{\theta^j}$

\begin{equation} \label{eq:8}
\max _{\theta^j} J^{\text {policy }}\left(\theta^j\right):=\mathbb{E}_\pi\left[\sum_{t=0}^T \gamma^t r^{j, \mathrm{env}}\left(s_t, \mathbf{a}_t \right)\right]
\end{equation}

\begin{equation}
G_t^j\left(\tau^j\right)=\sum_{l=t}^T \gamma^{l-t} r^{j, \mathrm{env}}\left(s_t, \mathbf{a}_t \right)
\end{equation}

\begin{equation}\label{eq:9}
\hat{\theta}^j \leftarrow \theta^j+\beta \left(  \sum_{t=0}^T \nabla_{\theta^j} \log \pi^j\left(a_t^j \mid o_t^j\right) G_t^j\left(\tau^j\right)   \right)
\end{equation}

As we can see from \ref{eq:8} and \ref{eq:9}, the greedy agent is exactly the policy gradient. The only difference is, it calculates and sends the incentivize function. This definition is close to the PG-c which we will use in the baseline. the difference is greedy agents have the second term of \ref{eq:2} as the cost of incentivizing other agents. 

% Figure environment removed

\section*{what are the base lines}
With the same design as the policy component of LIO, the initial baseline is a pure policy gradient, abbreviated PG.
The second option is a policy gradient with discrete "give-reward" actions or PG-d.\\
Additionally, we can define a flexible baseline policy gradient as PG-c that features continuous give-reward.\\Furthermore, we have LoLA and LIO accessible to compare with. \\


\end{document}