To address the problem discussed in this paper, a clear understanding of how reinforcement learning can facilitate effective collaboration among multiple robots is crucial.

\subsection{Multi-Robot RL based on Policy Gradient}

To explain how policy gradient-based reinforcement learning (RL) works, we provide a concise description in an episodic, discrete-action setting. At each time step $t$, the agent observes the environment state $s_t \in S$, selects an action $a_t$ based on a policy $\pi$ from the action space $A$, and receives a reward $r_t$ from the environment. The agent's policy, represented by $\theta$, maps the state representation to a probability distribution over actions. The policy's value, denoted by $J(\pi_\theta)$ or $J(\theta)$, is the expected discounted sum of rewards obtained by the agent when following the policy $\pi_\theta$, and can be expressed as:

\begin{equation}
J(\theta)=E_\theta\left[\sum_{t=0}^{\infty} \gamma^t r_t\right]
\label{eq1}
\end{equation}

The gradient of the value $J$ with respect to the policy parameters $\theta$ can be calculated as follows \cite{NIPS1999_464d828b}: for all time steps $t$ within an episode,

\begin{equation}
\nabla_\theta J(\theta)=E_\theta\left[G\left(s_t, a_t\right) \nabla_\theta \log \pi_\theta\left(a_t \mid s_t\right)\right]
\label{policyGradient}
\end{equation}

Here, $G\left(s_t, a_t\right)=\sum_{i=t}^{\infty} \gamma^{i-t} r_i$ represents the return until termination.


\subsection{Incentivization in Policy Gradient}
Cooperative robotics requires effective incentivization methods to encourage agents to reach the optimal total reward in the environment. In multi-agent reinforcement learning (RL), each agent can have intrinsic and extrinsic sources of reward. While extrinsic rewards come from the environment, intrinsic rewards can be generated based on different policies~\cite{Zheng2018OnLI}. However, in cooperative tasks where the reward distribution varies per task, a self-interested approach may not be effective. In such cases, using an incentive function between robots can help them achieve the optimal total reward.
%
LIO~\cite{10.5555/3495724.3496999} proposes an approach where each agent learns an incentive function by considering its impact on the recipients' behavior and its own objective. LIO describes the general case of $N$ agents, where each agent has its own observation $o^i$, action $a^i$, and incentive function $r_{\eta^i}$ that maps its observation and all other agents' actions to rewards. The notation $i$ is used to refer to the reward-giving part of an agent and $j$ for the part that learns from received rewards. The notations $-i$ and $-j$ refer to all reward-givers/receivers except themselves. At each time step $t$, each recipient $j$ receives a total reward defined as:

\begin{equation} \label{lio_reward_func}
r^j\left(s_t, \mathbf{a}_t, \eta^{-j}\right):= \eqnmarkbox[WildStrawberry]{env}{r^{j, \mathrm{env}}\left(s_t, \mathbf{a}_t\right)}+\eqnmarkbox[Plum]{sigma}{\sum_{i \neq j} r_{\eta^i}^j\left(o_t^i, a_t^{-i}\right)}
\end{equation}
\annotate[yshift=-1em]{below,left}{env}{environment reward}
\annotate[yshift=0.5em]{above,left}{sigma}{Incentive reward}



% TODO: 
% % Figure environment removed




