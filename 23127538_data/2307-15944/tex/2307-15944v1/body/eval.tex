

% Figure environment removed

\subsection{Experimental Setup}
In this section, we will present the evaluation environment used to test our methods for incentive manipulation in multi-robot reinforcement learning within social dilemmas by manipulating the policy and incentive rewards. We present the results of 10 separate runs for each environment to demonstrate the effectiveness of PIMbot's method.





\subsubsection{Escape Room (ER)}
In our evaluation, we utilized Gazebo to implement the Escape Room environment \cite{10.5555/3495724.3496999}. The Escape Room environment ER($N, M$) is a Markov game for N players with individual extrinsic rewards. To successfully exit the environment, an agent must receive a +10 extrinsic reward by exiting a door. However, the door can only be opened when $M$ other agents cooperate to pull the lever ($M<N$), which incurs an extrinsic penalty of -1 for any movement, thus discouraging all agents from taking cooperative action. We conducted experiments with the cases of ($N=2$, $M=1$), ($N=4$, $M=2$), and ($N=4$, $M=3$) to evaluate the effectiveness of our method. 

\subsubsection{Iterated Prisoner's Dilemma (IPD)}
In addition to our evaluation of the Escape Room environment, we conducted tests on the memory-1 Iterated Prisoner's Dilemma environment using our manipulation method \cite{10.5555/3237383.3237408}. In this environment, each agent observes the joint action taken by themselves and the other agent in the previous round. We used the extrinsic reward table for the environment, to evaluate the performance of our method. Table~\ref{table:IPD_reward} specifies the extrinsic rewards for each possible outcome of the environment, including mutual cooperation, mutual defection, and unilateral defection.

\begin{table}[H] \label{IPD_reward}
\centering
\caption{IPD reward table}
\label{table:IPD_reward}
\begin{tabular}{c|cc}
$\mathrm{Agent} 1 / \mathrm{Agent} 2$ & $\mathrm{Cooperate (C)}$ & $\mathrm{Defect (D)}$ \\
\hline $\mathrm{Cooperate (C)}$ & $(-1,-1)$ & $(-3,0)$ \\
$\mathrm{Defect (D)}$ & $(0,-3)$ & $(-2,-2)$
\end{tabular}
\end{table}


\subsection{Implementation Details}
To enable a fair comparison between our approach and LIO~\cite{10.5555/3495724.3496999}, we used the same hyperparameters $\theta$ and $\eta$ for Eq.\ref{newtheta} and Eq.\ref{updateetha}. For our experiments, we deployed the Escape Room (ER) environment in both ROS and Gazebo for two and four-player versions of the environment. We used the TurtleBot3 simulation platform to demonstrate the behavior of agents in the ER environment. Both the simulation and manipulation were run on an Intel Core i7-10700K CPU.

% ROS~\cite{doi:10.1126/scirobotics.abm6074} and Gazebo~\cite{koenig2004design}

\subsection{Experimental Results}
To demonstrate the effectiveness of the manipulation methods discussed in the previous section for multi-robot reinforcement learning in social dilemmas, we present the results of implementing these methods in both environments in this section. Specifically, we will discuss the results of each manipulation method used in the experiments. By evaluating the performance of our approach in comparison to the baseline LIO~\cite{10.5555/3495724.3496999}, we can show the benefits of our proposed manipulation techniques.





% Figure environment removed

\subsubsection{Incentive Reward Manipulation}
In this section, we discuss the methods used for incentive manipulation, namely partial communication and fake incentive rewards, which exploit the incentive channel between agents to achieve their objectives. 

The partial communication method controls the direction of the incentive function and thus maximizes the adversarial agent's extrinsic reward. As demonstrated in Figure~\ref{convergance}, the adversarial agent consistently achieves greater reward values than other benign agents in ER(2,1), ER(4,2), and IPD.

Although the adversarial agent's goal is to maximize its own reward function, our approach enables the entire team of agents to solve the task in a shorter period of time. As shown in the results of ER(2,1) and ER(4,2) in Figure~\ref{convergance}, our approach reaches the global optimum reward faster than LIO in 37.5\% for ER(2,1) and 25\% in ER(4,2), as mentioned in Table~\ref{table:conv}.


The objective of reaching the optimal point in the IPD environment differs from that in the ER environment. In IPD, the global optimal reward is achieved when all agents reach their individual maximum reward function, while in ER, some agents must gain less reward to assist others in reaching their maximum reward and completing the task. Our proposed method enabled an agent to reach the maximum reward in the IPD environment. However, an adversarial agent may only maximize its own extrinsic reward and cannot assist in achieving the total optimal reward for both agents. Therefore, utilizing our method with an adversarial agent may not result in the same outcomes.

In the Fake Incentive Reward manipulation method, the adversarial agent sends out malicious rewards to other agents using the incentive channel. In the Escape Room environment, we set the value of $C^{Adv}$ to 50 in Eq.\ref{fake_reward}. The reason for choosing 50 as the fake reward is that it is greater than the maximum reward that agents can receive for taking any action in the environment.
As shown in the results presented in Figure~\ref{bignum}, the agents explore the environment for 1800 episodes, which is why they achieve some success rate during this period.





\subsubsection{Policy Manipulation}
In the environments discussed previously, we applied the policy manipulation method, which enables the adversarial agent to manipulate the environment by altering the action caused by its policy. The following section presents the results of both the ''Bypass Policy'' and ''Reverse Policy'' methods. 

We evaluated the ''Bypass Policy'' method as the first approach to policy manipulation in the ER environment. In this method, the agent bypasses the policy and as a result, it does not take any action and remains in its initial state. However, since social dilemmas require the cooperation and actions of all agents to reach the optimal result, this method hinders the achievement of the optimal result. The results presented in Figure~\ref{noaction} show that for ER(4,3) and ER(2,1), the agents failed to reach the optimal reward, and the success rate for those environments is zero.

The second method of action manipulation is the ''Reverse Policy'' method, where the adversarial agent uses a different policy than other benign agents. The adversarial agent is minimizing the collective reward by changing its policy from gradient ascent to gradient descent. The adversarial agent takes actions with minimum reward and incentivizes other agents to give it the minimum incentive reward possible. The effectiveness of this method is shown in Figure~\ref{a2d}, where the success rate of task completion drops to zero.



One important observation is that environments can be classified based on whether all agents are required to collaborate to accomplish the task or whether some agents can independently contribute to achieving the goal. In this regard, we introduce the redundancy term in environments to analyze the effect of adversarial manipulation on the task completion process. When the adversarial agent abandons the main task, other agents still try to maximize their rewards. In redundant environments, other agents will ignore the adversarial agent's action and continue to move toward finishing the task. Yet, in non-redundant environments, all agents require the activity of the adversarial agent to finish the task, and they will eventually give up. In the case of our experiments, the environments of ER(4,3) and ER(2,1) are non-redundant, making the policy manipulation method successful in reducing the success rate and preventing the attainment of the total optimal reward. However, in the environment of ER(4,2), where two agents are required to pull the lever and only one agent is needed to reach the door, the adversarial agent's abandonment of other agents does not affect the final task completion process. As shown in Figure~\ref{a2d} and Figure~\ref{noaction}, agents still reach the total optimal global reward.


