In conclusion, this study has revealed that in multi-robot reinforcement learning collaborations, robots can have different intentions when faced with social dilemmas. By introducing an adversarial agent into two social dilemma environments and manipulating its policy and incentive reward channel, we were able to achieve three main goals. Firstly, the adversarial robot successfully obtained the maximum reward, highlighting the effectiveness of its malicious behavior. Secondly, the adversarial robot was able to converge faster to the optimum reward, demonstrating the potential impact of its behavior on the overall performance of the multi-robot system. Finally, the adversarial robot cooperated maliciously, causing issues for the other agents. These findings emphasize the need for further research into developing secure, robust, and equitable methods for multi-robot collaborations in the presence of adversarial agents. Ultimately, this work provides valuable insights into the challenges and opportunities involved in using multi-robot systems to address complex real-world problems.