
% Figure environment removed

In the context of incentivized RL for social dilemmas, we are introducing two main forms of manipulation: incentive manipulation and policy manipulation, as shown in Figure~\ref{bigpic_fig}. A benign agent employs an iterative approach for bilevel optimization, in which the upper level optimizes the incentive function by considering the lower level's receivers' policy optimization. Using Eq.\ref{lio_reward_func} and the policy gradient of Eq.\ref{policyGradient}, we update the policy parameters $\theta^j$ with learning rate $\beta$ as follows:

\begin{equation}\label{newtheta}
\hat{\theta}^j \leftarrow \theta^j+\beta \left(  \nabla _{\theta^j} J^{ \pi }\left(\theta^j, \eta^{-j}\right)   \right)
\end{equation}

We then use the new parameter $\hat{\theta}^j$ to generate a new trajectory $\hat{\tau}^j$. Each reward-giver $i$ updates its individual incentive function parameters $\eta^i$ to maximize the following individual objective:

\begin{equation}\label{updateetha}
\nabla _{\eta^i} J^i\left(\hat{\tau}^i, \tau^i, \hat{\boldsymbol{\theta}}, \eta^i\right):=\mathbb{E}_{\hat{\boldsymbol{\pi}}}\left[\sum_{t=0}^T \gamma^t \hat{r}_t^{i, \text { env }}\right]-\alpha L\left(\eta^i, \tau^i\right)
\end{equation}
The first part of Eq.\ref{updateetha} is the expected extrinsic return of the reward-giver in the new trajectory $\hat{\tau}^i$, and the second term is a cost for giving rewards in the old trajectory $\tau^i$.

Next, we will provide a detailed explanation of the two manipulation methods of PIMbot.

\subsection{Incentive Reward Manipulation}

In multi-agent RL with incentivization setup, agents communicate by sending and receiving incentive rewards, which affects their actions and can lead to efficient cooperation. However, this communication channel is also a weak point that can be manipulated. In this section, we introduce two methods for conducting incentive reward manipulation.


\subsubsection{Partial Communication}

In a multi-agent RL setting, an agent acting in self-interest may choose to ignore incentive rewards and prioritize maximizing its own extrinsic reward. This may lead to the agent behaving differently from other agents, hindering collaboration and potentially resulting in suboptimal outcomes for the group as a whole. However, such behavior could still achieve the best possible outcome for the agent's self-interest.

To influence the behavior of other agents, an adversarial agent can manipulate the incentive communication by altering the second part of Eq.\ref{lio_reward_func}. Nevertheless, it still needs to provide incentive rewards to other agents using the same principle as Eq.\ref{updateetha}. We define self-centered reward $r^{SC}$ as:

\begin{equation} \label{adversaryreward}
r^{SC}\left(s_t, \mathbf{a}_t\right) := \eqnmarkbox[WildStrawberry]{env}{r^{\mathrm{env}}\left(s_t, \mathbf{a}_t\right)}
\end{equation}

This manipulation in the reward function will affect the update of $\theta^{Adv}$ to maximize reward in the environment, as shown in Eq.\ref{newtheta}.
 




\subsubsection{Fake Incentive Reward}
The incentive reward communication can be exploited by the adversarial agent to its advantage. By manipulating the incentive rewards it sends to other agents, the adversarial agent can induce them to behave in ways that benefit the adversarial agent's objectives. This can happen because there is no sanity check for receiving incentive rewards from other agents.

Specifically, the adversarial agent sends a constant positive number $C^{Adv} \in |\mathbb{N}|$ as the incentive reward. as mentioned in Eq.~\ref{Cinq}, $C^{Adv}$ should be much greater than what is available in the environment and provided by other agents to discourage other agents from taking any action.

\begin{equation} \label{Cinq}
C^{Adv} \gg r^{\mathrm{env}}\left(s_t, \mathbf{a}t\right),~ r_{\eta^i}^j\left(o_t^i, a_t^{-i}\right)
\end{equation}

For the reward function of other agents, we introduce this variation of Eq.\ref{lio_reward_func}:

\begin{equation} \label{fake_reward}
r^j\left(s_t, \mathbf{a}_t, \eta^{-j}\right):= \eqnmarkbox[WildStrawberry]{env}{r^{j, \mathrm{env}}\left(s_t, \mathbf{a}_t\right)}+\eqnmarkbox[Plum]{sigma}{\sum_{i \neq j} r_{\eta^i}^j\left(o_t^i, a_t^{-i}\right)} + \eqnmarkbox[RoyalPurple]{fake}{C^{Adv}} 
\end{equation}
\annotate[yshift=1.7em]{above,left}{fake}{Adv incentive reward}
\annotate[yshift=0.5em]{above,left}{sigma}{from all agents except Adv}

Since $C^{Adv}$ is dominating the other two parts of Eq.\ref{fake_reward}, the total reward received by agent $j$ is approximately equal to $C^{Adv}$. In other words, using the reward function in Eq.\ref{fake_reward} will result in no changes to the $J^{ \pi }\left(\theta^j, \eta^{-j}\right)$ of Eq.\ref{newtheta}, which leads to $\hat{\theta}^j = \theta^j$. This means that there will be no policy change and no exploration ($\nabla J^{ \pi } = 0$).

\subsection{Policy Manipulation}
In cooperative multi-agent tasks, such as social dilemma games,  all agents must work together to complete the task. However, an adversarial agent can play a significant role in the overall task by participating and taking appropriate actions based on its policy that can influence the behavior of other agents. Specifically, the adversarial agent may use policy manipulation to induce other agents to perform actions that benefit the adversarial agent's objectives. 

\subsubsection{Bypass Policy}

Social dilemma games are characterized by a group of agents working towards a shared goal, requiring each agent to take actions that benefit the group. However, conflicts may arise between an agent's self-interest and the need to cooperate for the good of the group.
This conflict can be exploited by an adversarial agent who manipulates the environmental dynamics. One way an adversarial agent can achieve this is by circumventing policies that prohibit an agent from taking actions or contributing to the group's effort. Such actions can have a significant impact on the overall outcome, as the final task cannot be completed without the adversarial agent's participation.
The adversarial agent can force other agents to act in ways that serve the adversarial agent's objectives by failing to take any action, thereby bypassing policies and exploiting the tension between individual self-interest and the collective good.


% Figure environment removed


\subsubsection{Reverse Policy} 
In social dilemma games, all agents cooperate to achieve a shared goal using the same policies. However, an adversarial agent can leverage this commonality to manipulate the game by using a different policy to hinder the performance of benign agents. To achieve this, we introduce an agent that aims to minimize its own rewards in the environment. By doing so, the adversarial agent tries to deviate from solutions that benefit all agents and pursues its own objectives. Meanwhile, all benign agents aim to maximize their rewards using Eq. \ref{policyGradient}. They calculate their policy by using Eq. \ref{PGforgeniun}, which maximizes the expected sum of discounted rewards over time.

\begin{equation} \label{PGforgeniun}
\max _{\theta^j} J^{\pi}\left(\theta^j, \eta^{-j}\right):=\mathbb{E}_\theta\left[\sum_{t=0}^T \gamma^t r^j\left(s_t, \mathbf{a}_t, \eta^{-j}\right)\right]
\end{equation}

The adversarial agent, on the other hand, flips the rewards sign in Eq. \ref{PGforgeniun} to calculate $\min_{\theta^j} J^{\pi}\left(\theta^j, \eta^{-j}\right)$ and takes actions that minimize its own rewards. This has two sides of effect based on Eq. \ref{lio_reward_func}. First, the adversarial agent tries to minimize the extrinsic rewards provided in the game. Second, it takes actions that force other agents to send lower and lower incentive rewards. This manipulation by the adversarial agent can cause benign agents to act in ways that benefit the adversarial agent's objectives.





% Figure environment removed

\begin{table}[]
\centering
\caption{Comparison of Convergence Time for Various Methods}
\label{table:conv}
\begin{tabular}{|c|c|c|}
\hline
\multirow{2}{*}{ Method } & \multicolumn{2}{c|}{ Convergence Time (episode$*10^3$ )} \\
\cline { 2 - 3 } & ER(4,2) & ER(2,1) \\
\hline LIO~\cite{10.5555/3495724.3496999} & $14$ & $20$ \\
Partial Communication & $10.5$ & $12.5$ \\
\hline
\end{tabular}
\end{table}








