% work done at NaverLabs Europe www.europe.naverlabs.com
\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{import}
\usepackage[dvipsnames]{xcolor}         %
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage[super]{nth} %
\usepackage{xcolor,colortbl}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{bm}
\usepackage{cases}

\usepackage[breaklinks=true,bookmarks=false]{hyperref}



\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

\iccvfinalcopy %
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\ificcvfinal\pagestyle{empty}\fi



\newcommand{\SCR}{\textit{SCR}}
\newcommand{\SfM}{\textit{SfM}}
\newcommand{\PnP}{\textit{PnP}}
\newcommand{\ours}{SACReg\xspace}
\newcommand{\uline}[1]{\underline{#1}}
\newcommand{\earlyfusion}{EF}
\newcommand{\latefusion}{LF}

\newcommand{\I}{\mathcal{I}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Raug}{\mathcal{R}'}
\newcommand{\V}{\mathcal{V}}
\newcommand{\p}{\mathbf{p}}
\newcommand{\pv}{\mathbf{v}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\hat{\mathbf{y}}}
\newcommand{\bd}[1]{\textbf{#1}}

\newcommand{\PAR}[1]{\noindent{\bf{#1}}}
\newcommand{\hl}[0]{\cellcolor{gray!22}} %

\newlength{\SCRfigwidth}
\newlength{\SCRfigheight}
\newcommand{\SCRraisedgraphics}[1]{\raisebox{-.5\height}[\dimexpr0.5\height+2pt]{% Figure removed}}
\newenvironment{SCRfigtabular}
{
    \setlength{\SCRfigwidth}{0.16\linewidth}
    \setlength{\SCRfigheight}{\SCRfigwidth}
    \setlength{\tabcolsep}{3pt}
    \bf{}
    \small{}
    \begin{tabular}{c@{ }c@{ }cp{0.0cm}cp{0.0cm}c@{ }c}
    \multicolumn{3}{c}{Reference images with sparse 2D/3D annotations} &
    & Query image &
    & Predicted coordinates
    & Confidence \\
}
{
    \end{tabular}
}
\newcommand{\SCRfigline}[1]
{
    \SCRraisedgraphics{#1/refpoints0.jpg} &
    \SCRraisedgraphics{#1/refpoints1.jpg} &
    \SCRraisedgraphics{#1/refpoints2.jpg} & &
    \SCRraisedgraphics{#1/query.jpg} & &
    \SCRraisedgraphics{#1/prediction.jpg} &
    \SCRraisedgraphics{#1/confidence.jpg}
}

\begin{document}

\title{\ours: Scene-Agnostic Coordinate Regression for Visual Localization}

\author{Jerome Revaud$^\dagger$
\and
Yohann Cabon$^\dagger$
\and
Romain Br\'egier$^\dagger$
\and
JongMin Lee$^*$
\and
Philippe Weinzaepfel$^\dagger$\\
\quad\quad$^\dagger$Naver Labs Europe\quad\quad\quad\quad\quad\quad\quad\quad$^*$Seoul University\\
{\tt\small firstname.lastname@naverlabs.com}\quad\quad\quad\quad{\tt\small sdrjseka96@naver.com}
}

\maketitle
\ificcvfinal\thispagestyle{empty}\fi


\begin{abstract}
Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential.
However, existing methods remain mostly scene-specific or limited to small scenes and thus hardly scale to realistic datasets.
In this paper, we propose a new paradigm where 
a single generic SCR model is trained once to be then deployed to new test scenes, regardless of their scale and without further finetuning. 
For a given query image, it collects inputs from off-the-shelf image retrieval techniques and Structure-from-Motion databases: a list of relevant database images with sparse pointwise 2D-3D annotations.
The model is based on the transformer architecture and can take a variable number of images and sparse 2D-3D annotations as input.
It is trained on a few diverse datasets and
significantly outperforms other scene regression approaches on several benchmarks, including scene-specific models, for visual localization.
In particular, we set a new state of the art on the Cambridge localization benchmark, even outperforming feature-matching-based approaches.

\end{abstract}

% Figure environment removed

\section{Introduction}
\label{sec:intro}

Image-based scene coordinate regression (\SCR{}) consists in predicting the coordinates of the 3D point associated to each pixel of a given query image. 
\SCR{} methods have numerous applications in computer vision, and previous work has shown promising potential over the last few years.
Such methods have for instance been proposed for visual localization~\cite{shotton_scene_2013,yang_sanet_2019,tang_learning_2021,dsacstar} in combination with a Perspective-n-Point (\PnP{}) solver~\cite{pnp}. 
Other applications include object pose estimation~\cite{brachmann_learning_2014,zakharov_dpod_2019}, depth completion~\cite{penet, lidar_completion_uncertainty, cspnplus, nonlocal_spn,semattnet}, augmented reality or robotics~\cite{shotton_scene_2013,sc_wls_eccv2022}.



Unfortunately, existing \SCR{} approaches pose significant scalibility issues and end up being rather impractical.
Most of the time, 3D scene coordinates are directly embedded into the parameters of the learned model, being it a random forest~\cite{shotton_scene_2013} or a neural network~\cite{dsac,dsacstar,esac,sc_wls_eccv2022}, hence \emph{de facto} limiting one model to a specific, generally small, scene for which it was trained on. 
Some recent attempts to mitigate this issue, such as training different experts~\cite{esac},
sharing scene-agnostic knowledge between scenes~\cite{neumap}, or heavily relying on dense 3D reconstructions at test time~\cite{tang_learning_2021,yang_sanet_2019}, improve by some aspects but  still require scene-specific finetuning, %
can be limited to small scenes, and do not offer %
scene-agnostic solutions yet.


In this paper, we propose a new paradigm for scene coordinates regression that allows to train a generic model
once, and deploy it to any test dataset. %
As illustrated in Figure~\ref{fig:overview}, %
our scene-agnostic coordinate regression (\ours) model takes as input a query image as well as a set of relevant database images for which 3D scene coordinates are available at sparse 2D locations. 
It then predicts dense 3D coordinates, from which the camera pose can be obtained by solving a Perspective-n-Point (PnP) problem. 
Note that all inputs to \ours can be obtained via off-the-shelf 
methods.
Relevant database images, for instance, are obtained using image retrieval techniques~\cite{fire,kapture,apgem}, while the sparse 2D-3D correspondences are a by-product of the map construction procedure of Structure-from-Motion (SfM) pipelines~\cite{SchonbergerCVPR16StructureFromMotionRevisited}. 


As a first contribution, we introduce a generic model for scene-agnostic coordinate regression, illustrated in Figure~\ref{fig:crocordinate}, implemented using standard transformer and convolutional blocks only.
It uses a Vision Transformer (ViT)~\cite{vit} to encode both the query image and the database images. 
For each database image, input sparse 2D-3D correspondences are injected into the database image features with a so-called \emph{3D mixer} module.
This module consists of a series of transformer decoder blocks, as they were called originally~\cite{attn}, \ie ViT blocks with additional cross-attention modules.
Then, another transformer decoder combines information from these database features with features extracted from the query image, which are further processed by a convolutional head to finally regress pixelwise 3D scene coordinates.

As a second contribution, we propose to regress an encoding of the 3D coordinates rather than the raw 3D coordinates.
Doing so solves a major limitation of existing scene-agnostic approaches~\cite{tang_learning_2021,yang_sanet_2019} which assume small scenes with zero-centered coordinate systems and cannot generalize to unbounded scenes.
To that aim, we introduce an invertible and noise-resistant cosine-based encoding of 3D coordinates.
We show that it can generalize effortlessly to arbitrary coordinate ranges at test time.


To further ensure generalization, we train our model on diverse sources: outdoor buildings with the MegaDepth dataset~\cite{li_megadepth_2018}, indoor environments from the ARKitScenes~\cite{dehghan2021arkitscenes} dataset and synthetic data from the Habitat simulator~\cite{habitat-sim}. 
We additionally pre-train the network %
using cross-view completion~\cite{crocostereo}.
Our third contribution is to evaluate our model \emph{as-is} on various visual localization benchmarks, without any finetuning. 
On the Aachen Day-Night~\cite{aachen} and Cambridge-Landmarks~\cite{KendallICCV15PoseNetCameraRelocalization} benchmarks, \ours{} outperforms current scene-specific and dataset-specific \SCR{} methods, while being competitive with state-of-the-art structure-based methods~\cite{kapture}. 
Even though our model is scene-agnostic, it remains competitive on 7-Scenes~\cite{shotton_scene_2013} compared to scene-specific methods.



% Figure environment removed



\section{Related work}
\label{sec:related}


\PAR{Scene-specific coordinates regression.}
Several methods have been proposed to estimate dense 3D coordinates for a query image in a scene known at training time.
Early approaches~\cite{shotton_scene_2013, random_forest_2014, random_forest_2015} used regression forest models to predict the correspondence of a pixel in a RGB-D frame to its 3D world coordinate. 
However, more recent works~\cite{dsac, brachmann_learning_2018, esac, dsacstar, li2020hscnet, yang_sanet_2019, tang_learning_2021, kfnet, sc_wls_eccv2022,dong2022visual, huang2021vs} have replaced regression forests with CNN-based models that only require an RGB image. 
For example, Brachmann~\etal{}~\cite{dsac, brachmann_learning_2018, esac, dsacstar} train neural networks for this task and combine them with a differentiable RANSAC strategy for camera relocalization. 
Dong~\etal{}~\cite{dong2022visual} and Li~\etal{}~\cite{li2020hscnet} later introduce region classification into their pipelines for effective scene memorization. 
Huang~\etal{}~\cite{huang2021vs} propose to add a segmentation branch to obtain segmentation on scene-specific landmarks, which can then be associated with 3D landmarks in the scene to estimate camera pose. 
These methods are designed to memorize specific scenes,
making them hard to scale and impractical in many scenarios where the test scene is unknown at training time.
In contrast, our method can adjust to any scene or dataset at test time by relying on external image retrieval techniques to find relevant database images.




\PAR{Scene-agnostic coordinates regression with dense database 3D points.}
More related to our work are the scene-agnostic methods of~\cite{yang_sanet_2019,tang_learning_2021}. 
They regress dense scene coordinates given some reference views for which dense coordinates are already available. 
They thus require a dense multi-view reconstruction of the scene. %
Moreover, their methods are limited to small scenes with unit-normalized world coordinates.
In contrast, our approach only requires a sparse multi-view reconstruction of the scene and imposes no restriction on coordinate range, making it better suited to large-scale environments.


\PAR{Image-based localization}
consists in estimating 6-DoF camera pose from a query image.
Different approaches can be used towards that goal, and \SCR{} is one of them.
Recently, learning-based methods in which the pose of a query image is directly regressed with a neural network have been proposed~\cite{KendallICCV15PoseNetCameraRelocalization,KendallCVPR17GeometricLossCameraPoseRegression,BrahmbhattCVPR18GeometryAwareLocalization,WangAAAI20AtLocAttentionGuidedCameraLocalization,lstmposenet,mspn}.
By training the network with database images and their known ground-truth poses as training set, they learn and memorize the relationship between RGB images and associated camera pose. 
These direct approaches however need to be trained specifically for each scene. 
This issue was somehow solved by relative pose regression models ~\cite{balntas2018relocnet,DingICCV19CamNetRetrievalForReLocalization,zhou2020essnet,arnold_map-free_2022}, which train a neural network to predict the relative pose between the query image and similar database image found by image retrieval. 
However, their performance tends to be inferior to structure-based methods~\cite{SchonbergerCVPR16StructureFromMotionRevisited,SnavelyIJCV08ModelingTheWorldFromInternetPhotoCollections,HeinlyCVPR15ReconstructingTheWorldSixDays,kapture}. %
Structure-based visual localization frameworks use sparse feature matching to estimate the pose of a query image relative to a 3D map constructed from database images using SfM techniques, such as those employed in~\cite{SchonbergerCVPR16StructureFromMotionRevisited}. 
This involves extracting 2D features from images using interest point detectors and descriptors~\cite{sift, orb, superpoint, d2net, r2d2, disk, li2022decoupling_posfeat, aslfeat, sosnet, caps, lfnet, lift, geodesc}, and establishing 2D-3D correspondences. 
A %
PnP problem is solved using variants of RANSAC~\cite{fischlerrandom1981}, which then return the estimated pose. 
However, the structure-based methods have to store not only 3D points but also keypoints descriptors, and maintaining the overall localization pipeline is complex.
Our approach in contrast does not require to store keypoints descriptors, and is arguably simpler.















\section{The \ours model}
\label{sec:method}

In this section, we first describe our proposed scene-agnostic coordinate regression model (Section~\ref{sub:model}), then describe in more detail our robust coordinate encoding and associated training loss (Section~\ref{sub:loss}).
We finally describe how the model can be applied to visual localization (Section~\ref{sub:visloc}) %
and present training details (Section~\ref{sub:training}).

\subsection{Model architecture}
\label{sub:model}

Our model takes as input a query image $\I_q$, as well as a set of $K$ database images $\{\I_1,\I_2,\ldots,\I_K\}$, for which some sparse 2D-3D annotations are available. 
We denote by $\V_i = \{(\p^i_j, \pv^i_j)\}$ the set of 2D-3D correspondences for the database image $\I_i$, \ie, $\pv^i_j\in\mathbb{R}^3$ is a 3D point visible at pixel $\p^i_j$ expressed in a world coordinate system. 
Our model predicts 3D coordinates for every pixel in the query image.

\PAR{Overview.}
Figure~\ref{fig:crocordinate} shows an overview of the model architecture. First, the query image is encoded into a set of token features with a Vision Transformer~\cite{vit} (ViT) encoder. Similarly, the same encoder is used to encode each database image. For each database image, the information about the sparse 2D-3D coordinates are injected into the features using a transformer decoder, \ie, a series of blocks, each composed of self-attention between the token features, cross-attention with the 3D coordinates, and a MLP. %
We refer to this module as \emph{3D Mixer}.
The next step consists in combining information from the query features with the database images and the associated 2D-3D coordinates. This is performed once again with a transformer decoder.
Finally, a prediction head outputs dense 3D coordinates for each pixel of the query image. %
We now detail each module: the image encoder, the 3D mixer, the decoder and the prediction head.

\PAR{Image encoder.}
We use a vision transformer~\cite{vit} to encode all query and database images.
In more details, each image is divided into non-overlapping patches, and a linear projection encodes them into patch features. 
A series of transformer blocks is then applied on these features: each block consists of multi-head self-attention and a MLP. 
In practice, we use a ViT-Base model, \ie, $16{\times}16$ patches with $768$-dimensional features, $12$ heads and $12$ blocks.
Following~\cite{deepmatcher,crocostereo}, we use RoPE~\cite{rope} relative position embeddings.
As a result of the ViT encoding, we obtain sets of token features denoted $\R_q$ for the query and $\R_i$ for the database image $\I_i$ respectively:
\begin{equation}
\hspace{-1mm}    \R_q = \text{Encoder} \left( \I_q \right), \R_i = \text{Encoder} \left( \I_i \right), i=1 \ldots K.
\end{equation}

% Figure environment removed

\PAR{3D mixer.}
This step consists in injecting information about the sparse 2D-3D correspondence set $\V_i$ into the features $\R_i$ that represent the corresponding database image $\I_i$. As output of the 3D mixer, we obtain augmented token features $\Raug_i$:
\begin{equation}
    \Raug_i = \text{3Dmixer} \left( \R_i, \V_i \right) .
\end{equation}

As shown in Figure~\ref{fig:3dmixer}, we first embed each sparse 2D-3D correspondences in a 256-dimensional feature space with a MLP and call the result \emph{point token}. 
To that aim, we encode the 3D coordinates using a cosine point encoding $\phi$ described in Section~\ref{sub:loss} before feeding them to the MLP.
We then use a series of transformer decoder blocks, where each block consists of a multi-head self-attention~(SA) between the $768$-dimensional image tokens, %
a multi-head cross-attention~(CA) that injects information from all point tokens into these image tokens, and a MLP.

To improve robustness against potential outliers in the input sparse 3D points (due to \eg triangulation noise), we interleave the image-level decoder blocks with a new set of point-level decoder blocks.
We remove self-attention in these point-level decoder blocks to reduce their complexity. 
To account for the different features sizes between point tokens ($256$) and image tokens ($768$), we use linear projections that are shared across decoder blocks.
In practice, we use $4$ image-level decoder blocks 
interleaved with 3 point-level decoder blocks.


\PAR{Decoder.}
The next step is to inject information from the database images and their sparse 3D coordinates, \ie, from ${\R'_i}$ into the query features $\R_q$. We propose once again to do that with a transformer decoder, \ie, via cross-attention. Specifically, %
we apply multi-head cross-attentions for each ${\R'_i}$ in parallel, followed by an averaging operation, see \cref{fig:crocordinate} (bottom right). Such approach generalizes smoothly from the single-image to the multi-image case.
 

\PAR{Prediction head.}
We reshape the token features from the last transformer decoder block into a dense feature map and apply a convolutional head.
Specifically, we first project the features to 1024 dimensions using 1x1 convolution, then apply a sequence of 6 ResNeXT blocks, with a PixelShuffle~\cite{pixelshuffle_superresol} operation every two blocks to increase the resolution while halving the channel dimension.
For a $224\times 224$ input image, we get a $14^2 \times 1024$ token map after the initial projection, which is gradually expanded to $28^2 \times 512$, $56^2 \times 256$ and finally $224^2 \times d$ where $d$ is the output dimension.


\subsection{Generalization and training loss}
\label{sub:loss}

\newcommand{\hv}[0]{\hat{\textbf{v}}}

\PAR{Output space.}
A naive approach consists in setting $d=3$, \ie, trying to directly regress dense 3D points $\{\hv\}\in\mathbb{R}^3$ from the regression head.
This is possible and could be trained with a standard $\ell_1$ or $\ell_2$ regression loss, but is subject to a major limitation.
At test time, the network is typically unable to regress coordinates outside the range seen during training.
Thus, except for small scenes, it cannot generalize to new datasets (see Section~\ref{sub:ablate}).

Instead,  we propose to regress
a higher-dimensional 3D point encoding $\phi(\pv) \in (\mathbb{S}^1)^{d/2} \subset [-1,1]^d$, with $d \gg 3$.
We design $\phi$ with several desirable properties holding for any given $\pv\in\mathbb{R}^3$:
(i) $\phi$ is an injective mapping, with an inverse projection $\phi^{-1}$ such that $\phi^{-1}(\phi(\pv))=\pv$; 
(ii) the input space of $\phi^{-1}$ is %
the unit-circle product  $(\mathbb{S}^1)^{d/2} \subset [-1,1]^d$, whose high dimension enables error-correcting mechanisms in $\phi^{-1}$.
Thanks to these properties, our method can handle any coordinate at test time.


\PAR{Point encoding}
Assuming uncorrelated $x$, $y$ and $z$ coordinates we can decompose $\phi(\pv) = [\psi(x),\psi(y),\psi(z)]$ and define $\psi(x)$ as
\begin{equation}
    \psi(x) = \left[\cos(f_1 x), \sin(f_1 x), \cos(f_2 x), \sin(f_2 x), \ldots \right] 
\end{equation}
where the $f_i$'s are frequencies defined as $f_i=f_0 \gamma^{i-1}$, $i \in \{1, \ldots, d/6\}$, with $f_0 >0$ and $\gamma > 1$.
In practice, we set $f_0$ and $\gamma$ such that the periods of the lowest and highest frequencies $f_1$ and $f_{d/6}$ approximately corresponds to the maximum scale of a query scene (\eg 300 meters) and the desired spatial resolution (\eg 0.5 meter).
The encoding dimension $d$ then becomes a parameter that controls the level of redundancy.
$d$ must be carefully chosen, as too small encodings are not noise-resistant enough, while too large encodings may demand too much capacity for the decoder.
The inverse mapping $\psi^{-1}$ efficiently solves a least-square problem of the form %
$\psi^{-1}(y) = \text{argmin}_x \left\Vert y-\psi(x)\right\Vert^2$
, see Supplementary and~\cite{cosine}.

\PAR{Regression loss.}
As for the naive regression case, we apply a standard $\ell_1$ regression loss to train the network.
We have
\begin{equation}
    \mathcal{L}_{\text{reg}}(\pv, \y) = \left\vert \phi(\pv) - \y \right\vert,
    \label{eq:loss_reg}
\end{equation}
where $\y \in \mathbb{R}^d$ is the network output and $\pv$ is the corresponding ground-truth 3D point.
We further exploit the correlation between pairs of adjacent values in $\phi(\pv)$, as we have $\cos^2(f_ix) + \sin^2(f_i x) = 1$. 
Before applying the $\mathcal{L}_{\text{reg}}$ loss, we thus $\ell_2$-normalize each pairs of consecutive values in $\y$. 
We find this helps the training significantly.

\PAR{Pixelwise confidence.}
Some parts of the query image are inevitably harder, or even impossible to regress, such as the sky or objects not visible in database images. 
We therefore jointly predict a per-pixel confidence $\tau > 0$ that modulates the regression loss~\eqref{eq:loss_reg}, following~\cite{kendall2018multi}:
\begin{equation}
    \mathcal{L}_{\text{SCR}}({\pv, \y}, \tau) = %
    \tau \mathcal{L}_{\text{reg}}({\pv,\y}) - \log \tau.
    \label{eq:loss}
\end{equation}
$\tau$ can be interpreted as the confidence of the prediction: if $\tau$ is low for a given pixel, the corresponding $\mathcal{L}_{\text{reg}}$ loss at this location will be down-weighted. 
The second term of the loss incites the model to avoid being under-confident.



\subsection{Application to visual localization}
\label{sub:visloc}

We now present how our model can be applied to predict the camera pose of a given query image from a small set of relevant database images with sparse 2D-3D point correspondences.
An overview of our visual localization pipeline is shown in Figure~\ref{fig:overview}. 


\PAR{Image retrieval.}
Given a query image, we first follow the same retrieval step than for standard feature-matching-based localization approaches~\cite{kapture,hfnet,SattlerPriorityLoc2017}. 
Namely, we utilize off-the-shelf image retrieval methods such as HOW~\cite{how}, AP-GeM~\cite{apgem} or FIRe~\cite{fire} to obtain a shortlist of $K$ relevant database images for a a given query image.

\PAR{Sparse 2D-3D annotations.}
Our approach takes as input sparse 2D-3D correspondences for each database image.
To get them, we randomly subsample 2D points from the dense RGB-D data and reproject them in 3D using the known camera poses, when available. If not, we rely on standard Structure-from-Motion pipelines~\cite{SchonbergerCVPR16StructureFromMotionRevisited} during which
2D keypoint matches between images are used to recover the corresponding 3D point locations and the camera poses. 
This process directly yields a set of 2D-3D correspondences for each database image.
Unless specified otherwise, we use the output of COLMAP~\cite{SchonbergerCVPR16StructureFromMotionRevisited} with SIFT~\cite{sift} keypoints.

\PAR{Predicting camera poses.}
Given all images and their corresponding sparse 3D points, our model predicts a dense 3D coordinate map and a corresponding confidence map for the query image, as can be seen in Figure~\ref{fig:crocordinate}.
We first mitigate the noise by sorting all predicted 3D points according to the confidence $\tau$ and retaining the top 50\%.
We then use an off-the-shelf PnP solver to obtain the predicted camera pose.
Specifically, we rely on SQ-PnP~\cite{sqpnp} with 4096 randomly sampled 2D-3D correspondences, 10,000 iterations and a reprojection error threshold of 5 pixels.


\PAR{Multi-image fusion strategies.}
We experiment with two different strategies to handle multiple database images, each one offering a different quality-versus-runtime trade-off.
The first strategy is to let the decoder take care of fusing all $K$ database inputs itself, \ie directly computing $\text{Head}(\text{Decoder}(\R_q,\Raug_1,\ldots,\Raug_K))$, which is denoted as \emph{early fusion} (\earlyfusion) in the following.
The second strategy is to feed database images one at a time to the decoder, gathering each time the dense prediction maps, \ie computing $\{\text{Head}(\text{Decoder}(\R_q,\Raug_1)),\ldots,\text{Head}(\text{Decoder}(\R_q,\Raug_K))\}$.
Then, all resulting dense maps are fused by keeping, for each pixel, the most confident prediction.
This is referred to as \emph{late fusion} (\latefusion) in the following.

\noindent 
On the one hand, \earlyfusion{} is typically more difficult than \latefusion{}, since the decoder has to deal with several images at the same time, including potential outliers, with the same network capacity.
On the other hand, \latefusion{} is much more costly, since both decoder and regression head must be run $K$ times.
Results in Section~\ref{sub:multi_xp} show that, depending on the desired accuracy and computing budget, both might be interesting. %






\subsection{Training details}
\label{sub:training}

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccccc}
    \toprule 
    step & Res & \#img & LR schedule & \#epochs & Loss & Frozen\\
    \midrule
    \textbf{I} & 224 & 1+1 & fixed lr 1e-4 & 150 & $L_{\text{CroCo}}$ & -\\
    \textbf{II} & 224 & 1+1 & fixed lr 1e-4 & 100 & $L_{\text{SCR}}$ & E\\
    \textbf{III} & 512 & 1+1 & cosine decay & 20 & $L_{\text{SCR}}$ & -\\
    \textbf{IV} & 512 & 1+8 & cosine decay & 20 & $L_{\text{SCR}}$ & E,M\\
    \bottomrule
    \end{tabular}
    } \\[-0.25cm]
    \caption{\textbf{Training recipe} for our SCR model, with the 
        image resolution (`Res'), number of query and retrieved images ('\#img'),
        learning rate schedule, number of epochs and training loss.
        The rightmost column shows which modules are frozen during each step
        (E=ViT Encoder, M=3D Mixer).
        }
    \label{tab:train_recipe}
\end{table}

While training the full SCR model from scratch would be possible, we find it prohibitively slow and sub-optimal.
We instead %
train our SCR model in several steps, which are detailed in Table~\ref{tab:train_recipe}, gradually increasing the image resolution and the number of retrieved images.
We first initialize the weights of the ViT encoder and the decoder with cross-view completion pre-training (CroCo)~\cite{crocostereo,croco} (step I). %
From step II onward, we introduce the randomly-initialized 3D mixer and regression head. %
Using the SCR loss from Equation~\ref{eq:loss} and freezing the pre-trained encoder, we jointly train the 3D mixer, decoder and  prediction head, again using pairs of $224{\times}224$ images (a query and one single database image). 
During step III, we switch to a higher image resolution of $512{\times}384$ and finetune all components, still using only one database image. 
We find it important to finetune the encoder when changing the input resolution. %
During step IV, we finally finetune the decoder and head (\ie freezing both encoder and 3D mixer) using 8 database images. 


\PAR{Data.}
We train our model on datasets that cover various scenarios for robustness: MegaDepth~\cite{li_megadepth_2018} contains SfM reconstruction of 275 (mainly) outdoor scenes, ARKitScenes~\cite{dehghan2021arkitscenes} consists of indoor house scenes, and Habitat of synthetic indoor scenes derived from the HM3D~\cite{ramakrishnan2021hm3d}, ScanNet~\cite{dai2017scannet}, Replica~\cite{replica19arxiv} and ReplicaCAD~\cite{szot2021habitat} rendered using Habitat-Sim~\cite{habitat-sim}.
These three datasets provide %
dense depth estimates as well as camera poses, thus allowing to train our SCR model in a fully-supervised manner. 
We use 100K query from each dataset (300K in total). %
For each query, we retrieve beforehand a shortlist of $K$ similar images using FiRE~\cite{fire}.


\PAR{Data augmentation}.
We 
apply standard random crop and color jitter during training.
For robustness to possible triangulation noise, we augment 5\% of the sparse 3D points with simulated depth noise.
We also apply random geometric 3D transformation to scene coordinates for better  generalization. %
Namely, we apply random 3D rotation followed by random scaling in the range $[1/2,2]$ and random translation in $[-1000m,1000m]^3$.



\section{Experiments}

After presenting the test datasets (Section~\ref{sub:datasets}), we present several ablations in Section~\ref{sub:ablate} and study fusion strategies in Section~\ref{sub:multi_xp}. We compare our approach to the state of the art in visual localization (Section~\ref{sub:sota}), and finally evaluate the accuracy of the regressed coordinates (Section~\ref{sub:coordinates_regression}).

\subsection{Datasets and metrics}
\label{sub:datasets}

\PAR{Cambridge-Landmarks}~\cite{KendallICCV15PoseNetCameraRelocalization}
consists of 6 outdoor scenes with RGB images from videos and small-scale landmarks. 

\PAR{7 Scenes\cite{ShottonCVPR13SceneCoordinateRegression}}
consists of 7 indoor scenes with RGB-D images from videos. Each scene has a limited size, and the images contain repeating structures, motion blur, and texture-less surfaces.%
We do not use the depth data of the query image during inference.

\PAR{Aachen Day-Night v1.1\cite{aachen}\cite{ZhangIJCV20ReferencePoseGenerationVisLoc}}
contains 6,697 database images captured at day time, and 1015 query images including 824 taken during daytime (Aachen-Day) and 191 during nighttime (Aachen-Night). 

\PAR{Metrics.} For Cambridge and 7-Scenes, we report the median translation error. For Aachen, we report the percentage of successfully localized images within three thresholds: (0.25m, 2\textdegree), (0.5m, 5\textdegree) and (5m, 10\textdegree).


\subsection{Ablative study}
\label{sub:ablate}

\PAR{Validation sets and metrics.}
For the ablations, we report the visual localization performance on a selected subset of 5 diverse and relatively challenging datasets: 7scenes-stairs, 7scenes-pumpkin, Cambridge-GreatCourt, Cambridge-OldHospital and Aachen-Night. 
For 7scenes and Cambridge-Landmarks, we report the averaged median translation error, while for Aachen-Night we report the localization accuracy for the 3 standard thresholds.

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c|ccc}
    \toprule 
    Point encoding & Aug & Camb.~$\downarrow$ & 7scenes~$\downarrow$ & Aachen-Night~$\uparrow$\\
    \midrule 
    $(x,y,z)\in\mathbb{R}^{3}$ &   & 1.69 & \textbf{0.11} & 0.0 / 0.0 / 0.0 \\
    $(x,y,z)\in\mathbb{R}^{3}$ &  \checkmark & 14.43 & 2.89  & 0.0 / 2.1 / 44.5 \\
    $\phi(\cdot)\in[-1,1]^{24}$ & \checkmark & 0.47 & \textbf{0.11} & 22.0 / 46.6 / 89.5 \\
    \hl $\phi(\cdot)\in[-1,1]^{36}$ & \hl \checkmark & \hl \textbf{0.43} & \hl \textbf{0.11 } & \hl 22.0 / \textbf{47.1} / \textbf{90.6 }\\
    $\phi(\cdot)\in[-1,1]^{48}$ & \checkmark & 0.55  & \textbf{0.11 } & \textbf{23.6} / 40.8 / 87.4 \\
    \bottomrule
    \end{tabular}
    } \\[-0.25cm]
    \caption{\textbf{Ablation on 3D point encoding.} 
        }
    \label{tab:ab_encoding}
\end{table}

\begin{table}[]
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{c|ccc}
    \toprule
    Frozen & Camb.~$\downarrow$ & 7scenes~$\downarrow$ & Aachen-Night~$\uparrow$\\
    \midrule 
    -       & 0.54  & 0.14  & 18.3 / 37.7 / 85.3\\
    \hl Encoder & \hl \textbf{0.43}  & \hl \textbf{0.11} & \hl \textbf{22.0} / \textbf{47.1} / \textbf{90.6} \\
    \bottomrule
    \end{tabular}
    } \\[-0.25cm]
    \caption{\textbf{Ablation on freezing the encoder during step II.}
        }
    \label{tab:ab_freeze_1}
\end{table}

\PAR{Default settings.}
We use the training recipe from Table~\ref{tab:train_recipe} as default setting for the ablations, unless specified otherwise.
For the sake of time, 
we train and test on small images, \ie performing training step III with 224x224 images and completely skipping step IV (hence evaluating with the top-1 database image only).
For each ablation table, we put a gray background color on the row with default settings. %



\PAR{Robust coordinate encoding.}
We first study in Table~\ref{tab:ab_encoding} the impact of different point encoding options.
Direct coordinate regression is only successful when the train and test output distributions are aligned.
This is the case for small scenes with zero-centered spaces like 7-scenes, or Cambridge to a lesser extent. 
For larger scenes with unconstrained coordinates (like Aachen), direct regression utterly fails.
One way to mitigate this issue is to augment 3D coordinates at training time, using random translations for instance (see Section~\ref{sub:training}).
Augmentations somehow improve the situation for Aachen-Night, but the performance overall strongly degrades for Cambridge and 7scenes.
In contrast, the cosine-based encoding $\phi$ proposed in Section~\ref{sub:loss} effectively deals with indoor and outdoor scenes in any coordinate ranges.
For our model, we find optimal to use $6$ frequencies, yielding $d=36$-dimensional outputs. 


\PAR{Pretraining and encoder freeze.}
Table~\ref{tab:ab_freeze_1} shows that freezing the encoder during step II is highly beneficial,
meaning that pretraining with CroCo (step I) effectively learns adequate image representations.
This might be explained by the fact that CroCo essentially learns to compare and match images, and its training set is much larger than our finetuning data (7M versus 300K pairs).


\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|ccc}
    \toprule
    3D-mixer & Camb.~$\downarrow$ & 7scenes~$\downarrow$ & Aachen-Night~$\uparrow$\\
    \midrule 
    Simple, depth=1 & 0.66 & 0.11 & 16.8 / 40.8 / 87.4 \\
    Simple, depth=2 & 0.61 & \textbf{0.08} & 20.4 / 42.4 / 88.0 \\
    Simple, depth=4 & 0.55 & 0.10 & \textbf{22.0} / 39.3 / 89.5 \\
    \midrule 
    Alternating, depth=1 & 0.56 & 0.11 &	18.8 / 41.4 / 88.0 \\
    Alternating, depth=2 & 0.46 & 0.11 &	\textbf{22.0} / 46.1 / 88.5 \\
    \hl Alternating, depth=4 & \hl \textbf{0.43} & \hl 0.11 & \hl \textbf{22.0} / \textbf{47.1} / \textbf{90.6} \\
    \bottomrule
    \end{tabular}
    }\\[-0.25cm]
    \caption{\textbf{Ablation on 3D-mixer variants.}
        }
    \label{tab:ab_mixer3d}
\end{table}

\PAR{Architecture of the 3D mixer.}
We experiment with different architectures for the 3D-mixer module and report results in Table~\ref{tab:ab_mixer3d}.
We compare the interleaved decoder architecture from Figure~\ref{fig:3dmixer} with a simpler architecture composed of a sequence of several image-level decoder blocks (`Simple').
In all cases, increasing the number of decoder blocks leads to improved performance.
However, we observe a substantial gain with the alternating decoder architecture, due to the enhanced denoising ability of this architecture (Section~\ref{sub:model}).

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccc}
    \toprule
    Regression head & Channels & Camb.~$\downarrow$ & 7scenes~$\downarrow$ & Aachen-Night~$\uparrow$\\
    \midrule 
    Linear            & $x,y,z,\tau$  & 0.94 & 0.12 &	11.0 / 31.9 / 84.3 \\
    ConvNeXT & $xyz\tau$     & 0.64 & \textbf{0.11} &	19.9 / 39.8 / 88.0 \\
    ConvNeXT & $xyz,\tau$  & 0.61 & \textbf{0.11} &	15.7 / 41.4 / 87.4 \\
    \hl ConvNeXT & \hl$x,y,z,\tau$  & \hl\textbf{0.43} & \hl\textbf{0.11} &	\hl\textbf{22.0} / \textbf{47.1} / \textbf{90.6} \\
    \bottomrule
    \end{tabular}
    }\\[-0.25cm]
    \caption{\textbf{Ablation on regression head.}}
    \label{tab:ab_head}
\end{table}


\begin{table}[]
    \centering
    \newcommand{\blocks}[0]{b}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cc|ccc}
    \toprule
    Train res. & Test res. & Camb.~$\downarrow$ & 7scenes~$\downarrow$ & Aachen-Night~$\uparrow$\\
    \midrule 
    \hl $224{\times}224$   & \hl $224{\times}224$  & \hl 0.43 & \hl 0.11 &	\hl 22.0 / 47.1 / 90.6 \\
    $512{\times}384$   & $512{\times}384$  & 0.21 & 0.10 &	39.3 / 63.4 / \textbf{94.8}  \\
    $512{\times}384$   & $640{\times}480$  & \textbf{0.20} & \textbf{0.07} & 45.5 / 68.6 / \textbf{94.8}  \\
    $512{\times}384$   & $768{\times}512$  & 0.24 & \textbf{0.07} &	45.5 / \textbf{70.2} / 93.7 \\
    \bottomrule
    \end{tabular}
    } \\[-0.25cm]
    \caption{\textbf{Impact of training and test image resolution.}}
    \label{tab:img_res}

    \vspace{-0.3cm}
    
\end{table}

% Figure environment removed

\PAR{Separate heads.}
We experiment with different architectures for the regression head, this time aiming at exploiting priors of the output space.
Recall that for each pixel, we ultimately predict 4 values: 3 spatial components ($x$, $y$ and $z$) and a confidence $\tau$.
\emph{A priori}, these four components have no reason to be correlated. 
In fact, predicting them jointly could turn detrimental if there is a risk for the network to learn imaginary correlations.
Therefore, we compare: (i) regressing the 4 components jointly using the same head, (ii) regressing the spatial and confidence components separately;
(iii) regressing all 4 components separately, in which case we still use the same prediction head with shared weight for all spatial $x$, $y$ and $z$ components; (iv) as a baseline, a simple linear head, which is the same as 4 independent linear heads (one per component).
From Table~\ref{tab:ab_head}, it is clear that option (iii) yields the best performance, while the linear heads remains the worst option.
This confirms that enforcing decorrelation between all components is strongly beneficial for generalization at test time.




\PAR{Image resolution
} 
can have a strong impact on the test performance.
Table~\ref{tab:img_res} shows that test performance generally increases as a function of image resolution.
Interestingly, the model is able to generalize to higher resolution at test time, as training in $512\times 384$ and testing on higher resolutions consistently yields better results.
In the following, we always test on $640\times 480$ images.



\subsection{Fusion strategies for multiple retrieved images}
\label{sub:multi_xp}

All experiments so far have been performed with a single database image using models trained until step III. 
We now compare the early and late fusion strategies (see Section~\ref{sub:visloc})
in terms of inference time and localization accuracy.
Timings are measured on a A100 GPU and correspond to the query going through the encoder, decoder and regression head, \ie, without the encoding of the database images and their coordinates, as this can be computed offline and saved.
We average the localization accuracy for each dataset, except for Aachen where we display results for Aachen-Day and Aachen-Night separetely.

On Cambridge and Aachen, we observe in Figure~\ref{fig:speed_vs_acc} that late fusion (\latefusion) outperforms early fusion (\earlyfusion), even though \earlyfusion{} has benefited from more training epochs (\ie, reaching step IV instead of stopping at step III).
This is expected as \latefusion{} is a simpler task for the decoder.
On these datasets, \earlyfusion{} performance starts deteriorating beyond 10 database images, which roughly corresponds to the point where most outliers start to appear in the retrieval shortlist.
Since \earlyfusion{} is trained with 8 database images, this may cause a discrepancy and hurt generalization, although results on 7-scenes show this is not always true.
In all cases, \latefusion{} is much slower than \earlyfusion, and as a result, \earlyfusion{} always manages to match or outperform late fusion for an equivalent run-time budget.


\subsection{Visual localization benchmarking}
\label{sub:sota}

\begin{table}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{llcc}
    \toprule
    & Method & Aachen-Day~$\uparrow$ & Aachen-Night~$\uparrow$ \\
    \midrule
    \multirow{2}{*}{\begin{turn}{90}
    Kpts
    \end{turn}} & Active Search~\cite{SattlerPriorityLoc2017}         & 57.3 / 83.7 / 96.6 & 28.6 / 37.8 / 51.0 \\
    & HLoc~\cite{hfnet}       & \textbf{89.6} / \textbf{95.4} / 98.8 & \textbf{86.7} / \textbf{93.9} / \textbf{100} \\
    \midrule
    \multirow{5}{*}{\begin{turn}{90}
    Learning-based
    \end{turn}} & DSAC~\cite{dsac} & 0.4 / 2.4 / 34.0 & - \\
    & ESAC (50 experts)~\cite{esac} & 42.6 / 59.6 / 75.5 & - \\
    & HSCNet~\cite{li2020hscnet} & 65.5 / 77.3 / 88.8 & 22.4 / 38.8 / 54.1 \\
    & NeuMap~\cite{neumap} & 76.2 / 88.5 / 95.5 & 37.8 / 62.2 / 87.8 \\
    \cmidrule{2-4}
    & \textbf{\ours{}}, \earlyfusion, $K=8$ & 79.6 / 89.6 / \uline{98.9} & 51.3 / 75.4 / \uline{96.9} \\ 
    & \textbf{\ours{}}, \latefusion, $K=20$ & \uline{86.2} / \uline{93.8} / \textbf{99.6} & \uline{63.4} / \uline{86.9} / \textbf{100.0} \\ 
    \bottomrule
    \end{tabular}
    } \\[-0.25cm]
    \caption{\textbf{Comparison to the state of the art on Aachen.} %
    }
    \label{tab:sota}

    \vspace{-0.3cm}
    
\end{table}



% Figure environment removed

\begin{table}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cl|ccccc}
    \toprule 
     &  & ShopFacade~$\downarrow$  & OldHospital~$\downarrow$ & College~$\downarrow$ & Church~$\downarrow$ & Court~$\downarrow$\\
    \midrule 
    \multirow{2}{*}{\begin{turn}{90}
    Kpts
    \end{turn}} & Active search \cite{SattlerPriorityLoc2017} & 0.12, 1.12 & 0.52, 1.12 & 0.57, 0.70 & 0.22, 0.62 & 1.20, 0.60\\
     & HLoc \cite{hfnet} & \textbf{0.04,} \textbf{0.20} & \uline{0.15}, 0.3 & \textbf{0.12}, 0.20 & \uline{0.07}, \textbf{0.21} & \uline{0.11}, 0.16\\
    \midrule 
    \multirow{6}{*}{\begin{turn}{90}
    Learning-based
    \end{turn}} & DSAC++ \cite{brachmann_learning_2018} & 0.06, 0.3 & 0.20, \uline{0.3} & 0.18, 0.3 & 0.13, 0.4 & 0.20, 0.4\\
     & DSAC*\cite{dsacstar}  & \uline{0.05}, 0.3 & 0.21, 0.4 & 0.15, 0.3 & 0.13, 0.4 & 0.49, 0.3\\     
     & KFNet\cite{kfnet}  & \uline{0.05}, 0.35 & 0.18, \textbf{0.28} & 0.16, 0.27 & 0.12, 0.35 & 0.42, 0.21\\     
     & HSCNet \cite{li2020hscnet}  & 0.06, 0.3 & 0.19, 0.3 & 0.18, 0.3 & 0.09, 0.3 & 0.28, 0.2\\
     & SANet\cite{yang_sanet_2019}  & 0.1, 0.47 & 0.32, 0.53 & 0.32, 0.54 & 0.16, 0.57 & 3.28, 1.95\\     
     & DSM \cite{tang_learning_2021} & 0.06, 0.3 & 0.23, 0.38 & 0.19, 0.35 & 0.11, 0.34 & 0.19, 0.43\\
     & SC-wLS \cite{sc_wls_eccv2022} & 0.11, 0.7  & 0.42, 1.7 & \uline{0.14}, 0.6 & 0.39, 1.3 & 1.64, 0.9\\
     & NeuMaps \cite{neumap} & 0.06, \uline{0.25} & 0.19, 0.36 & \uline{0.14}, \uline{0.19} & 0.17, 0.53 & \textbf{0.06},  \uline{0.1}\\
     & \textbf{\ours}, \earlyfusion, K=8  & 0.06, 0.32 & 0.18, 0.32 & \uline{0.14}, 0.25 & \uline{0.07}, \uline{0.23} & 0.18, 0.12\\
     & \textbf{\ours}, \latefusion, K=20  & \uline{0.05}, 0.3 & \textbf{0.13}, \uline{0.29} & \textbf{0.12}, \textbf{0.18} & \textbf{0.06}, \textbf{0.21} & 0.13, \textbf{0.09}\\
    \bottomrule 
    \end{tabular} 
    } \\[-0.25cm]
    \caption{\textbf{Comparison to the state of the art on Cambridge} with  the median trans. (m) and angular ($^\circ$) errors.}
    \label{tab:cambridge}

    \vspace{-0.3cm}
    
\end{table}


% Figure environment removed

We compare our approach to the state of the art for visual localization on indoor (7-scenes) and outdoor datasets (Cambridge-Landmarks, Aachen-DayNight).
We compare to learning-based approaches as well as a few representative keypoint-based methods such as Active Search~\cite{SattlerPriorityLoc2017} and HLoc~\cite{hfnet}.
Results are presented in Table~\ref{tab:sota}, Table~\ref{tab:cambridge} and Figure \ref{fig:7-scenes} respectively.
Our late fusion model outperforms the early fusion one on Aachen-Day-Night and Cambridge and perform on par on 7-Scenes.
On the indoor 7-Scenes, our method obtains similar or slightly worse performance compared to other approaches, but overall still performs well with a median error of a few centimeters.
On outdoor datasets, the proposed methods strongly outperforms other learning-based methods, in particular other scene-specific or scene-agnostic coordinate regression approaches like~\cite{dsac,dsacstar,sc_wls_eccv2022,yang_sanet_2019,tang_learning_2021,neumap,esac}.
Our method even sets a new state of the art on the Cambridge dataset.
This is remarkable because, in contrast to any other learning-based approaches, \ours is directly applied to each test dataset without any finetuning.
In other words, the proposed approach works out of the box on test data that were never seen during training.
Interestingly, it even reaches the performance of keypoints-based approaches such as Active Search~\cite{SattlerPriorityLoc2017} or HLoc~\cite{hfnet}. 


\subsection{Scene coordinates regression}
\label{sub:coordinates_regression}

Lastly, to evaluate the regression performances of \ours{}, we apply our model on \emph{7-Scenes}, which provides dense ground truth annotations. Using a shortlist size of $K=8$, we predict the 3D coordinates and corresponding confidence for each pixel of the test images.
We obtain a median and mean error of 4.2cm and 13.2cm respectively.
Results furthermore validate that confidence predictions are meaningful, as errors tends to get smaller when the confidence increases (\cref{fig:7-scenes}, top right). Confidence can thus be used as a proxy to filter out regions where errors are likely to be large (\cref{fig:7-scenes}, bottom right, and black regions in \cref{fig:regression_results}).











{\small
\bibliographystyle{ieee_fullname}
\bibliography{biblio}
}

\clearpage

\appendix

\noindent{\huge\textbf{Appendix}\par}
\vspace{6mm}

% Figure environment removed

In this appendix, we first present additional qualitative results in Section~\ref{sup:more}.
We then elaborate on the offline computation, compression and storage of database images %
 in Section~\ref{sup:compression}.
Afterwards, we give detailed explanations and proofs about the injectivity of the point encoding and its inverse mapping in Section~\ref{sup:inverse}.
Finally, we provide details about online localization timings in Section~\ref{sup:timings}, the architecture of the regression head in Section~\ref{sup:regression_head} and the hyper-parameters used to train the model in Section~\ref{sup:implem_details}.


\section{More qualitative results}
\label{sup:more}

We present in Figure~\ref{fig:additional_regression_results} regression outputs (dense 3D coordinates and confidence map) with their corresponding input images.
From these outputs, we can notice several things.
First, the model can successfully densify sparse inputs incoming from several images while respecting small structures like windows (\eg in row \#4).
Second, the model is robust to occlusions, as can be especially seen in rows \#2 and \#4 where occluded parts in the query \wrt database images are also rated as unreliable in the output confidence map.
Third, the model is typically not confident to extrapolate 3D predictions in query regions for which few or no 2D-3D annotations are available in the corresponding database images, such as the ground region, people or trees.

% Figure environment removed

\PAR{3D reconstruction.}
The 3D coordinates predicted by \ours{} can be used to lift a query image into a 3D colored point-cloud, as we illustrate in the \emph{supplementary video}. 
Point clouds corresponding to different query images of the same scene can then be merged into a single large scale 3D reconstruction, as illustrated in \cref{fig:large_scale_reconstruction}.
In this case, we collected the output point clouds obtained for each query image from the Aachen-Day~\cite{aachen} dataset, removed low-confidence 3D points and simply concatenated all 3D points together before rendering the result using Blender~\cite{blender}.
Note that, because our approach directly predicts 3D coordinates in a metric world coordinate system, no further post-processing of the output is necessary to achieve these reconstructions.


\section{Database compression}
\label{sup:compression}

One potential limitation of the proposed method might be the large volume of the pre-computed database image representations, if stored uncompressed.
Indeed, considering an input resolution $HW \triangleq 640 \times 480$, and a ViT patch size of $p \triangleq 16$, an image representation typically consists of $H W/p^2=1200$ token features of $D \triangleq 768$ dimensions, each encoded using a 4-bytes floating point value. It thus leads to a memory use of $4 D H W/p^2 \approx 3.68\text{MB}$ per image.
We show that these representations can be significantly compressed with very little loss of performance using product quantization.

\PAR{Product quantization (PQ)~\cite{jegou_pq_2011}}
is a lossy data compression technique that works as follows.
Given a vector $\x\in\mathbb{R}^D$ to compress, $\x$ is first split into $N$ sub-vectors denoted as \emph{blocks} $\x_j\in\mathbb{R}^{D/N}$, $j=1\dots N$ with $N$ multiple of $D$ and the block size being $B=D/N$.
Then, each block $\x_j$ is separately vector-quantized~\cite{vector_quantization} using a codebook $\C_j$.
Using codebooks with 256 entries, we can represent each block using a single byte index, effectively decreasing the memory requirement by a factor of $4B$ (\ie assuming 4 bytes per floating point value).
In our specific case, given a 3D-augmented database image representation $\Raug=\{\x_i\}$ composed of token features $\x_i\in\mathbb{R}^D$, we split each token features into $N$ blocks $\x_{i,j}\in\mathbb{R}^B$ and replace each by the closest entry from codebook $\C_j$. 


\PAR{Codebook learning.}
To learn the set of codebooks $\{\C_j\}$, we first select a set of 30,000 database images $\{\I_k\}$ from our universal training set and equally sampled from the \emph{Habitat}, \emph{ARKitScenes} and \emph{MegaDepth} datasets.
We encode them,  along with their associated 2D-3D annotations, and get their 3D-augmented representation $\{\Raug_k\}$, each of which being a set of 1200 token features for a resolution of $640\times 480$. 
Gathering tokens from all images results in a set of $M$ features vectors $\x_i \in \mathbb{R}^{D}, i=1,\dots,M$.
We split each vectors into $N$ blocks $(\x_{i,1}, \dots, \x_{i,N})$ of dimensions $B$, and we cluster features $(\x_{i,j})_{i=1,\dots,M}$ for each block $j\in\{1,\dots,N\}$ into $k=256$ centroids using $k$-means~\cite{kmeans} to form the codebook $\C_j$. 


\PAR{Results.} 
During an offline phase, we compute, compress and store the representations of all database images.
At test time, we reconstruct the full token features from the stored codebook indices and the corresponding codebooks.
Localization results after quantization are reported in \cref{tab:quantization}.
For 7-Scenes and Cambridge-Landmarks, we report the average median translation error over the same challenging subsets than used in the ablative experiments from the main paper (\ie, street+pumpkin for 7-Scenes and greatcourt+oldhospital for Cambridge).
We observe that a quantization with a block size $B=2$ has a limited impact on localization accuracy, but that performance quickly degrades when using more aggressive schemes.

To alleviate the performance drop due to quantization, we slightly finetune the model for one additional epoch using compressed database features as inputs (considered as fixed), with a learning rate of $10^{-4}$ and a cosine-decay scheduler. 
We observe large performance improvements thanks to this fine-tuning. In particular, we observe minimal performance drops compared to the original model when using quantization with blocks of size up to $B=8$, which leads to a database compression factor of 32.
Beyond this point, the performance gracefully degrades, such that for $B=32$ (compression factor of 128) our method is still able to obtain more than 80\% accuracy at 50cm\&5\textdegree{} error threshold on Aachen-Night.

\begin{table*}
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{l|c|c|cccc}
    \toprule
    Quantization & FT & kB/img $\downarrow$ &  Camb.~$\downarrow$ & 7scenes~$\downarrow$ & Aachen-Day~$\uparrow$ & Aachen-Night~$\uparrow$ \\
    \midrule
    Raw & & 3686 & 0.13 & 0.07 & 86.2 / 93.8 / 99.6 & 63.4 / 86.9 / 100.0 \\
    \midrule
    PQ (B=2) & & 461 & 0.13 & 0.07 & 84.5 / 93.8 / 99.6 & 63.9 / 86.9 / 100.0 \\
    PQ (B=4) & & 230 & 0.14 & 0.07 & 84.6 / 93.9 / 99.6 & 58.6 / 82.2 / 100.0   \\
    PQ (B=6) & & 154 & 0.15 & 0.07 & 82.5 / 92.1 / 99.6 & 56.0 / 82.7 / 99.5 \\
    PQ (B=8) & & 115 & 0.18 & 0.08 & 80.6 / 91.4 / 99.6 & 46.1 / 77.0 / 98.4 \\
    PQ (B=16) & & 58 & 0.30 & 0.09 & 48.9 / 74.3 / 96.7 & 8.4 / 24.6 / 67.0 \\
    PQ (B=32) &  & 29 & 24.3 & 1.67 & 0.4 / 1.2 / 8.7 & 0.0 / 0.0 / 3.1 \\
    PQ (B=64) & & 14 & 45.1 & 2.06 & 0.0 / 0.0 / 0.1 & 0.0 / 0.0 / 0.0 \\
    PQ (B=128) & & 7 & 53.8 & 2.23 & 0.0 / 0.0 / 0.0 & 0.0 / 0.0 / 0.0 \\
    \midrule
    PQ (B=2) & \checkmark & 461 & 0.13 & 0.07 & 85.1 / 93.3 / 99.5 &  61.8 / 85.9 / 100.0 \\
    PQ (B=4) & \checkmark & 230 & 0.13 & 0.07 & 84.5 / 93.0 / 99.5 &  63.9 / 89.0 / 100.0 \\
    PQ (B=6) & \checkmark & 154 & 0.13 & 0.07 & 84.5 / 93.7 / 99.5 & 63.9 / 87.4 / 99.5 \\
    PQ (B=8) & \checkmark & 115 & 0.14 & 0.07 & 83.5 / 92.8 / 99.4 & 63.4 / 85.9 / 100.0 \\
    PQ (B=16) & \checkmark & 58 & 0.15 & 0.07 & 81.3 / 91.5 / 99.3 & 59.7 / 84.3 / 100.0 \\
    PQ (B=32) & \checkmark & 29 & 0.20 & 0.08 & 75.6 / 89.6 / 99.5 & 56.5 / 81.2 / 100.0 \\
    PQ (B=64) & \checkmark & 14 & 0.34 & 0.08 & 60.4 / 82.8 / 98.8 & 40.3 / 65.4 / 97.9 \\
    PQ (B=128) & \checkmark & 7 & 2.75 & 0.14 & 24.9 / 51.3 / 91.1 & 17.3 / 36.1 / 90.1 \\
     \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{\textbf{Product quantization (PQ)~\cite{jegou_pq_2011}} enables to compress database tokens with minimal performance losses.
    Results obtained for $640 \times 480$ resolution images, with late fusion (\latefusion) and K=20 retrieved images, for different block sizes $B$. 
    The compression factor with respect to storing raw representations is $4B$. 
    kB/img represents the number of kilobytes required to store 
    the compressed  %
    representations of a single database image.
    FT denotes one more epoch of fine-tuning using the fixed compressed database representations. 
    }
    \label{tab:quantization}
\end{table*}

\begin{table}
    \centering
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{l|c|cc}
    \toprule
    Method & Size $\downarrow$ & Aachen-Day~$\uparrow$ & Aachen-Night~$\uparrow$ \\
    \midrule
    NeuMap (8m, 100)~\cite{neumap} & 1.26 & 80.8 / 90.9 / 95.6 & 48.0 / 67.3 / 87.8 \\
    \textbf{\ours} PQ (B=6) & 0.96 & \bd{84.5} / \bd{93.7} / \bd{99.5} & \bd{63.9} / \bd{87.4} / 99.5 \\
    \midrule
    \textbf{\ours} PQ (B=16) & 0.36 & \bd{81.3} / \bd{91.5} / \bd{99.3} & \bd{59.7} / \bd{84.3} / \bd{100.0} \\
    Squeezer~\cite{yang2022scenesqueezer} & 0.24 & 75.5 / 89.7 / 96.2 & 50.0 / 67.3 / 78.6 \\
    \textbf{\ours} PQ (B=32) & 0.18 & 75.6 / \bd{89.6} / \bd{99.5} & \bd{56.5} / \bd{81.2} / \bd{100.0} \\
    NeuMap (10m, *)~\cite{neumap} & 0.17 & 76.2 / 88.5 / 95.5 & 37.8 / 62.2 / 87.8 \\
    Cascaded~\cite{cheng2019cascaded} & 0.14 & \bd{76.7} / \bd{88.6} / 95.8  & 33.7 / 48.0 / 62.2 \\
    \textbf{\ours} PQ (B=64) & 0.09 & \bd{60.4} / \bd{82.8} / \bd{98.8} & \bd{40.3} / \bd{65.4} / \bd{97.9} \\
     \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{\textbf{Results with compression compared to the state of the art on Aachen Day-Night.} 
    Data sizes in the `Size' column are reported in Gigabytes.
    We highlight in \bd{bold} optimal values lying on an accuracy-versus-compression Pareto front.}
    \label{tab:quantizationsota}
\end{table}

\PAR{Comparison with NeuMap.}
In Table~\ref{tab:quantizationsota}, we compare our approach with other scene-compression methods such as NeuMap~\cite{neumap}, which directly regress the 3D coordinates of a given set of 2D keypoints using learned neural codes, and other scene compression methods such as Cascaded~\cite{cheng2019cascaded} and Squeezer~\cite{yang2022scenesqueezer}, which are based on feature matching.  
Our approach achieves similar or better results compared to all other methods under similar compression settings
(\eg with $B = 32$). 
Compared to NeuMap, which uses a similar learning- and transformer-based approach, our method's inference is also much faster, because NeuMap's decoder must be run multiple times with the retrieved voxels' codes, which takes several seconds. 
Detailed timings of our model are described in Section~\ref{sup:timings}.
Additionally, it is noteworthy that, unlike NeuMap, we did not train our model on the Aachen dataset at all. 


\section{Point encoding}
\label{sup:inverse}
We provide in this section further information regarding the point encoding $\phi$ presented in the paper.
First recall that we have defined $\phi(\boldsymbol{v})=\left[\psi(x),\psi(y),\psi(z)\right]$
for $\bm{v}=(x,y,z) \in \mathbb{R}^3$, \ie simply concatenating the encodings of the $x$,
$y$ and $z$ channels, by a mapping $\psi$.
For the sake of brevity, most of the discussion will thus focus on this mapping:
\begin{equation}
\begin{aligned}
\psi: t \in \mathbb{R} \rightarrow &\left[ \cos(f_{1}x),\sin(f_{1}x), \dots,\cos(f_{F}x),\sin(f_{F}x) \right] \\
&\in (\mathbb{S}^1)^F \subset [-1,1]^{2F},
\end{aligned}
\end{equation}
defined for some positive frequencies $\left\{ f_{i}\right\}_{i=1\ldots F}$, with
$F=d/6\in\mathbb{N}^*$, where $d$ is the dimension of the output point encoding $\phi(\bd{v})$.
We furthermore typically consider harmonically distributed frequencies $f_{i}=f_{1}\gamma^{i-1}$ for $i=1\ldots F$, where $\gamma > 1$.


\subsection{Injectivity / periodicity}

\PAR{Lemma 1.}
$\gamma \in \mathbb{R}\setminus\mathbb{Q} \Rightarrow \psi(x)$ is injective, or equivalently, if $\gamma$ is irrational then there is no $x$ and $y$ with $x\neq y$ such that $\psi(x)=\psi(y)$.

\PAR{Proof by contraposition.}
We will prove that $\psi(x)=\psi(y)\text{ with }x\neq y \Rightarrow \gamma \in \mathbb{Q}$.
For simplicity and without loss of generality, assume $F=2$ frequencies.
Let $x,y \in \mathbb{R}$ two distinct numbers ($x \neq y$).
The proposition $\psi(x)=\psi(y)$ is equivalent to:
\begin{equation}
\begin{cases}
\begin{aligned}
\cos(f_{1}x) & = \cos(f_{1}y) \\
\sin(f_{1}x) & = \sin(f_{1}y) \\
\cos(f_{2}x) & = \cos(f_{2}y) \\
\sin(f_{2}x) & = \sin(f_{2}y),
\end{aligned}
\end{cases}
\end{equation}


which can be rewritten as: 
\begin{equation}
\begin{cases}
\begin{aligned}
e^{if_{1}x} & = e^{if_{1}y}\\
e^{if_{2}x} & = e^{if_{2}y},
\end{aligned}
\end{cases}
\end{equation}


\ie, that there exist $k_{1},k_{2}\in\mathbb{Z}^{*}$ such that:
\begin{equation}
\begin{cases}
\begin{aligned}
f_{1}x & = f_{1}y+2\pi k_{1} \\
f_{2}x & = f_{2}y+2\pi k_{2}. 
\end{aligned}
\end{cases}
\end{equation}
Rearranging the equations, we get:
\begin{numcases}{}
\begin{aligned}
\frac{f_{1}}{2\pi}(x-y) & = k_{1} \label{eq:f1}\\
\frac{f_{2}}{2\pi}(x-y) & = k_{2}. \label{eq:f2}
\end{aligned}
\end{numcases}
Dividing the second term by the first one, which is possible as 
$k_{1},k_{2}\neq0$, yields:
\begin{equation}
\frac{f_{2}}{f_{1}}=\gamma=\frac{k_{2}}{k_{1}}\in\mathbb{Q} \quad\blacksquare
\end{equation}

While any irrational $\gamma$ fulfils the injectivity lemma, we
note that algebraic numbers should ideally be avoided. Indeed they
introduce a risk that $\psi$ would be periodic over a subset of dimensions
$\xi \subseteq \{1,\ldots,F\}$ , with $\left|\xi\right|>1$. 
Take, for instance, an algebraic $\gamma=\sqrt{2}$ and assume $F=3$ frequencies. 
Then we have $f_{2}=\sqrt{2}f_{1}$ and $f_{3}=2f_{1}$.
In this case, one can trivially show that $\psi_\xi = [\psi_i]_{i\in\xi}$ is periodic for $\xi=\{1,3\}$.
In the presence of noise, this can severely impair the robustness of the inverse mapping. 
Hence, in general, we recommend choosing transcendental values for $\gamma$, which is not really a problem as \emph{almost all real }numbers are transcendental, \ie, $p(X\text{ is transcendental})=1$ for a uniformly random real $X$ in any given range $[a,b]$ with $a<b$.

\paragraph{Numerical considerations.}

Modern representations of real numbers on computers are limited by hardware constraints. 
In practice, a real number $x\in\mathbb{R}$ is stored as a fixed-length bit vector (64 bits in our case), 
{which roughly amounts to storing $x$ as a rational $\frac{a}{b},$ with $a,b\in\mathbb{Z}^*$.
Let then denote all frequencies as rational numbers $f_i=\frac{a_i}{b_i}$, with coprime $a_i,b_i\in\mathbb{N}^*$.
In this case, $\psi$ is periodic, %
with a period $P \neq 0$ multiple of all individual periods $2 \pi / f_i$, for $i=1, \ldots, F$. 
Equivalently, there exist some non-zero integers $k_i \in \mathbb{N}^*$ such that:
\begin{equation}
P = k_{i} \cfrac{2 \pi}{f_i}, \quad \forall i=1 \ldots F
\end{equation}
\ie:
\begin{equation}
P = 2 \pi k_{i} \cfrac{b_i}{a_i}, \quad \forall i=1 \ldots F.
\end{equation}
The smallest period $P$ satisfying all these $F$ equations is thus $P=2 \pi ~ \texttt{lcm}\{b_i\} / \texttt{gcd}\{a_i\}$, defined by the least common multiple of all denominators ($\texttt{lcm}$) divided by the greatest common divisor ($\texttt{gcd}$) of all numerators. Such ratio is in general hard to predict for arbitrary frequencies $\{f_i\}$, %
but it can be extremely large when considering irreducible fractions of large terms $a_i,b_i \simeq 10^{15}$.
For our use-case, we want the period to be a large as possible, and in practice, using double floating-point representations, we can find values for $\gamma$ yielding periods in the order of billions of kilometers, which is more than sufficient.


\paragraph{Practical choice.}

We thus adopt an empirical stance and evaluate several random values
for the frequency-generator parameters $f_{1}$ and $\gamma$. 
In particular, we impose that they satisfy the metric scale conditions, \ie, the smallest and largest periods must lie in the following metric ranges: 
\begin{eqnarray}
    P_1 = \frac{2\pi}{f_{1}}\in [200\text{m},500\text{m}] \\
    P_F = \frac{2\pi}{f_F}  \in [20\text{cm},2\text{m}]. \nonumber 
\end{eqnarray}
Note that the 200-500 meters range specified for $P_{1}$ does not relate to the scale of the full dataset, but to the scale of the point-cloud that can be observed through a single query view. 
In the unlikely case that this observable point-cloud is larger than 200 meters, our theoretical results above still guarantees that any encoding admits a unique pre-image in the absence of noise. 
In the presence of noise, however, it could become more likely that large errors would happen during the inverse mapping.

To select the best frequency parameters, we train the network using the step II recipe on a small subset of 20K training pairs for 20 epochs and choose the parameters that yield the maximum accuracy over a small validation set of the same size. We give in Table~\ref{tab:freqs} the resulting values that we have used in our experiments for different number of frequencies $F$.

\begin{table*}
    \centering
    \begin{tabular}{cc|cc|cc}
    \toprule 
    $F$ & $d{=}6F$ & $f_{1}$ & $\gamma$ & $P_{1}$ & $P_F$\\
    \midrule
    4 & 24 & 0.020772487794205544 & 5.7561020938998690 & 302m & 1.59m \\
    6 & 36 & 0.017903170262351338 & 3.7079736887249526 & 351m & 50cm \\
    8 & 48 & 0.031278470093268460 & 2.5735254599557535 & 201m & 27cm \\
    \bottomrule 
    \end{tabular}
    \vspace{-0.3cm}
    \caption{\textbf{%
    Frequency parameters} $f_1$ and $\gamma$ used in our experiments.
            $P_1$ and $P_F$ are respectively the periods of the lowest and highest frequencies.}
    \label{tab:freqs}
\end{table*}



\subsection{Inverse projection}
We formulate the inverse mapping problem as a non-linear least-square projection:
\begin{equation}
\label{eq:inv_def}
\psi^{-1}(\boldsymbol{y})=\text{\ensuremath{\arg\min_{t\in B}}}\left\Vert \psi(t)-\boldsymbol{y}\right\Vert ^{2},
\end{equation}
where $B$ is a search domain constructed as the union of point-clouds
ranges for each input database image $I_{k}$. For instance, and without
loss of generality, when considering the $x$ channel of some 3D coordinates, $B$ is formally
defined 
\begin{equation}
B=\bigcup_{k=1\ldots K}\left[\min\left\{ x_{j}^{k}\right\} ,\max\left\{ x_{j}^{k}\right\} \right],
\end{equation}
where the set $\{x_{j}^{k}\}$ belongs to the sparse 2D-3D annotation
set $\mathcal{V}_{k}=\left\{ (p_{j}^{k},v_{j}^{k})\right\} $ associated
with each database image $\mathcal{I}_{k}$.
While $B$ should ideally be the infinite real space in theory, we found important in practice to narrow down the search range for efficiency reasons as well as noise robustness.

The minimization objective of \cref{eq:inv_def} can be expanded
{
\small
\begin{equation}
\begin{aligned}
S(t) & =\left\Vert \psi(t)-\boldsymbol{y}\right\Vert ^{2}\\
 & =\sum_{i=1\ldots F} \big[ \left(\cos(f_{i}t)-\boldsymbol{y}_{2i-1}\right)^{2} +\left(\sin(f_{i}t)-\boldsymbol{y}_{2i}\right)^{2} \big]\\
 & =\sum_{i=1\ldots F} \big[ \cos^{2}(f_{i}t)-2\boldsymbol{y}_{2i-1}\cos(f_{i}t)+\boldsymbol{y}_{2i-1}^{2} \\
 & \quad\quad\quad\quad\quad\quad\quad+\sin^{2}(f_{i}t)-2\boldsymbol{y}_{2i}\sin(f_{i}t)+\boldsymbol{y}_{2i}^{2} \big]\\
 & =F+\left\Vert \boldsymbol{y}\right\Vert ^{2}-2\sum_{i=1\ldots F} \big[ \boldsymbol{y}_{2i-1}\cos(f_{i}t)+\boldsymbol{y}_{2i}\sin(f_{i}t) \big].
 \label{eq:sum1}
 \end{aligned}
 \end{equation}
 }

Using the trigonometric identity
\begin{equation}
A\cos(x)+B\sin(x)=\sqrt{A^{2}+B^{2}}\cos\left(x-\arctan\frac{B}{A}\right)
\end{equation}
with the convention $\arctan( \pm \infty) = \pm \pi/2$,
we can simplify \cref{eq:sum1} into:
\begin{align}
S(t) & = F + \|\bm{y} \|^2 -2\sum_{i=1\ldots F} \sqrt{\boldsymbol{y}_{2i-1}^2 + \boldsymbol{y}_{2i}^2}\nonumber\\
    &  \quad\quad\quad\quad\quad\quad\quad\quad \times\cos\left(f_{i}t-\arctan\frac{\boldsymbol{y}_{2i}}{\boldsymbol{y}_{2i-1}}\right).\nonumber\\
\label{eq:sumcos}
\end{align}

In other words, the inverse projection consists in finding the minimum of a weighted sum of cosines with different frequencies.
This problem has no analytic solution in general for $F>2$. It can however be approximately solved when considering inputs normalized by pairs (\ie $\bm{y} \in (\mathbb{S}^1)^F$, or $\boldsymbol{y}_{2i-1}^2 + \boldsymbol{y}_{2i}^2$=1).

Inspired by~\cite{cosine}, we efficiently probe the search space at regular intervals $t_{n}=\frac{n\pi}{2f_{F}}\in B$, with $n\in\mathbb{Z}$ a signed integer and where $f_{f}$ is the highest frequency.
A reasoning similar to the one developed in~\cite{cosine} suggests that at most one minima can lie in $]t_{n},t_{n+1}[$, in which case we have $S'(t_{n})<0$ and $S'(t_{n+1})>0$, denoting $S'$ the derivative of $S$.
The location of the corresponding minima can be approximated as
\begin{equation}
t=t_{n}-\left(t_{n+1}-t_{n}\right)\frac{S'(t_{n},\boldsymbol{y})}{S'(t_{n+1},\boldsymbol{y})-S'(t_{n},\boldsymbol{y})}.
\end{equation}

Finally, we return the local minimum approximation $t$ for which $S(t)$ is smallest over all $t_{n}$, which in practice is a good estimate of the global minimum.
Typically, for $f_{F}\simeq10 m^{-1}$, this amounts to probe every $\simeq10$ centimeters, which translates into 200 evaluation of $S$ per channel and per pixel for a typical 20-meter-wide outdoor scene. 
Timings for this inverse mapping step are given in Section~\ref{sup:timings}.






\section{Detailed timings}
\label{sup:timings}

In the main paper, we compare the localization accuracy as a function of the inference time.
We measure the inference time on a A100 GPU for the model to forward one query image through the encoder, decoder and head, given a set of already-computed relevant database image representations.

In Figure~\ref{fig:timings} we plot the detailed timings for each step, this time including the post-processing steps to output the camera pose: (1) inverting the output encoding into an actual 3D dense map with the inverse mapping $\psi^{-1}$ (see Section~\ref{sup:inverse}); (2) computing the pose using PnP-RANSAC~\cite{sqpnp}.
The forward pass in the network overall dominates the total time.
This is especially true considering that the inverse mapping could be much further optimized (\eg using float32 instead of float64, and computing it only for the 50\% most confident pixels). 
Likewise, using 1,000 iterations in PnP-RANSAC instead of 10,000 would result in ${\simeq}10{\times}$ faster computing time for this step without noticeable loss of performance, according to preliminary experiments.
Since the main focus of this paper is not speed optimization, we leave these modifications to future work.

% Figure environment removed



\section{Regression head architecture}
\label{sup:regression_head}

We now give more details about the regression head.
In Section~4.2 of the main paper, we experiment with different architectures and finally retain the one that regresses each of the 4 $x,y,z,\tau$ channels separately.
In this case, we share the prediction head between all spatial x, y and z components as shown in Figure~\ref{fig:head1}, and another head serves to predict the pixel-wise confidence.

All heads (\ie, 3D regression heads and confidence head) share the same architecture, as specified in the main paper and illustrated in Figure~\ref{fig:head2}.
Specifically, the head consists of a sequence of 6 ConvNeXT~\cite{liu2022convnet} blocks interleaved with 3 PixelShuffle~\cite{pixelshuffle_superresol} operations to increase the resolution while simultaneously halving the channel dimension.


% Figure environment removed

% Figure environment removed

\section{Implementation details}
\label{sup:implem_details}

We report the detailed hyperparameter settings we use for our training recipe in Table~\ref{tab:training_step234}. Step I corresponds to the pre-training performed in CroCo~\cite{crocostereo,croco} and we refer to these papers for detailed settings.

   
\begin{table*}[]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l@{\hskip 1.0cm}l@{\hskip 0.8cm}l@{\hskip 0.8cm}l}
    \toprule
    Hyperparameters & Step II & Step III & Step IV \\
    \midrule
    Optimizer & AdamW~\cite{adamw} & AdamW~\cite{adamw} & AdamW~\cite{adamw} \\
    Base learning rate & 1e-4 & 1e-4 & 1e-4 \\
    Weight decay & 0.05 & 0.05 & 0.05 \\
    Adam $\beta$ & (0.9, 0.95) & (0.9, 0.95) & (0.9, 0.95) \\
    Batch size & 128 & 128 & 128 \\
    Learning rate scheduler &  Fixed LR & Cosine decay & Cosine decay \\
    Training epochs & 100 & 20 & 20\\
    Warmup epochs & 10 & 2 & 2 \\
    \midrule
    Frozen modules & Encoder & - & Encoder, 3D mixer \\
    \midrule
    Number of database views & 1 & 1 & 8 \\
    Input resolution & $224{\times}224$ & $512{\times}384$ & $512{\times}384$ \\
    Image Augmentations & Random crop, color jitter & Random crop, color jitter  & Random crop, color jitter \\
    Sampled 3D points & 1024 points per DB image & 1024 points per DB image & 1024 points per DB image \\
    \cmidrule{2-4}
    \multirow{4}{*}{3D Augmentations} & 5\% triangulation noise & 5\% triangulation noise & 5\% triangulation noise\\
                                      & rotations, translations in & rotations, translations in & rotations, translations in\\
                                      & $[-1000,1000]^3$, rescale & $[-1000,1000]^3$, rescale  & $[-1000,1000]^3$, rescale  \\
                                      & in $[0.5,2]$ & in $[0.5,2]$ & in $[0.5,2]$ \\
    \cmidrule{2-4}
    \multirow{2}{*}{Retrieval Augmentations} & Gaussian noise (std=0.05) & Gaussian noise (std=0.05) & Gaussian noise (std=0.05) \\
                                     & on FiRE retrieval scores & on FiRE retrieval scores & on FiRE retrieval scores\\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.3cm}
    \caption{\textbf{Detailed hyper-parameters} for each training step. 
    }
    \label{tab:training_step234}
\end{table*}

\end{document}
