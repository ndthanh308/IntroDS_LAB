\section{Importance Weight Aware Updates}

The IWA updates were motivated by the failure of OGD to deal with arbitrarily large importance weights.
In fact, the standard approach to use importance weights in OGD is to simply multiply the gradient by the importance weight. However, when the importance weight is large, we might have an update that is far beyond what is necessary to attain a small loss on it. \citet{KarampatziakisL11} proposed IWA, a computationally efficient way to use importance weights without damaging the convergence properties of OGD.
In particular, IWA updates are motivated by the following invariance property: an example with importance weight $h \in \Nat$ should be treated as if it is an unweighted example appearing $h$ times in the dataset.

More formally, IWA updates are designed for importance weighted convex losses over linear predictors. So, let $\bq_t\in \mathbb{R}^d$ be the $t^\text{th}$ sample and $h_t\in \R_{+}$ its importance weight. Each loss function $\ell_t:\R^d \to \R$ is defined as $\ell_t(\bx)\triangleq \hat{\ell}_t(\langle \bq_t, \bx\rangle)$, where $\bx$ is the predictor, $\langle \bq_t, \bx\rangle$ is the forecast on sample $\bq_t$ of the linear predictor $\bx$, and $\hat{\ell}_t:\R\to\R$ is the $h_t$-weighted convex loss function. For example $\hat{\ell}_t(p) = \frac{h_t}{2} (p-y_t)^2$ for linear regression with square loss,  $\hat{\ell}_t(p)=h_t \ln (1+e^{-y_tp})$, and $\hat{\ell}_t(p)=h_t \max(1-p y_t,0)$ for linear classification with hinge loss.
%As standard in online learning, the goal is to minimize the regret on the losses $\ell_t$ over $T$ rounds. As we said before, minimizing the regret also maximizes the convergence rate in case the samples are drawn i.i.d. from a fixed distribution.

The key idea of \citet{KarampatziakisL11} is performing a sequence of $N$ updates on each loss function $\ell_t$, each of them with learning rate $\eta/N$, and take $N\to\infty$.
Given the particular shape of the loss functions, all the gradients for a given sample $\bq_t$ points in the same direction: $\nabla \ell_t(\bx) = \hat{\ell}'_t(\langle \bq_t, \bx\rangle) \bq_t$. Therefore, the cumulative effect of performing $N$ consecutive updates in a row on each sample $\bq_t$ amounts to a single update in the direction of $\bq_t$ rescaled by a single scalar. Hence, we just have to find this scalar. More in details, the effect of doing a sequence of infinitesimal updates can be modelled by an ordinary differential equation (ODE), as detailed in the following theorem.
%
\begin{theorem}[{\citep[Theorem~1]{KarampatziakisL11}}]
\label{th:iwaupdate}
Let $\hat{\ell}$ to be continuously differentiable. Then, the limit for $N\to\infty$ of the OGD update with $N$ updates on the same loss function with learning rate $\eta/N$ is equal to the update
\[
\bx_{t+1} = \bx_t - s_t(1)\bq_t,
\]
where the scaling function $s_t:\R\to \R$ satisfies $s_t(0)=0$ and the differential equation
\[
s'_t(h)
= \eta \hat{\ell}'_t(\langle\bq_t, \bx_t - s_t(h)\bq_t\rangle)~.
\]
\end{theorem}

\paragraph{IWA Updates are Generalized Implicit Updates}
As we said above, IWA updates do not have strong theoretical guarantees. Indeed, we do not even know if they give the same performance of the plain gradient updates in the worst case.
Here, we show that IWA is an instantiation of the generalized implicit FTRL. This implies that its regret upper bound is better than the one of online gradient descent. Moreover, this also gives a way to interpret IWA updates as approximate proximal/implicit updates.

Denote by $p_t \triangleq \langle \bq_t, \bx_t\rangle$, $\bx_t(h) \triangleq \bx_t - s_t(h)\bq_t$,  $p_t(h) \triangleq \langle \bq_t, \bx_t(h)\rangle$, and $\bg_t(h) \triangleq \hat{\ell}'_t(p_t(h)) \bq_t$. Consider the generalized implicit FTRL with regularization $\psi_t(\bx) = \frac{1}{2\eta}\|\bx\|^2_2$ and $V=\R^d$.
Set $\bz_t$ as
\begin{align}
\bz_t 
&\triangleq \int_{0}^1 \! \bg_t(h) \, \mathrm{d}h \nonumber \\
&= \frac{1}{\eta} \left(\int_{0}^1 \! \eta \hat{\ell}_t'(\langle\bq_t, \bx_t-s_t(h)\bq_t\rangle) \, \mathrm{d}h \right) \bq_t \nonumber \\
&=  \frac{1}{\eta} s_t(1) \bq_t~. \label{eq:iwa_z}
\end{align}
Then, the iterates of Algorithm~\ref{alg:giftrl} are the same as the iterates of IWA updates 
\[
\bx_{t+1} 
= \frac{\btheta_t-\bz_t}{1/\eta} 
= \bx_t - \frac{\bz_t}{1/\eta} 
= \bx_t - s_t(1) \bq_t~.
\]
In words, we can now analyze IWA updates as an instantiation of generalized implicit updates.

In particular, Theorem~\ref{th:iwa} shows sufficient conditions on the loss $\hat{\ell}_t$ to guarantee that IWA updates are as good as the subgradient $\bg_t$ by proving that $\bz_t$ satisfy $H_t(\bz_t) \leq H_t(\bg_t)$.
\begin{theorem}
\label{th:iwa}
Assume $s'_t(h)$ to be continuous in $[0,1]$. If $\forall h\in[0,1]$, $\hat{\ell}'_t(p_t(h))$ satisfies one of the following requirements:
\begin{itemize}
\item $\hat{\ell}'_t(p_t(h))\geq0$, $\hat{\ell}'''_t(p_t(h)) \geq 0$
\item $\hat{\ell}'_t(p_t(h))\leq0$, $\hat{\ell}'''_t(p_t(h)) \leq 0$
\end{itemize}
then $\bz_t=\int_{0}^1 \! \bg_t(h) \, \mathrm{d}h$ satisfies
\begin{equation}
\label{eq:iwa}
H_t(\bz_t)\leq H_t(\bg_t)~.
%\psi^\star\left(\btheta_t-\bz_t \right) + \ell_t^\star \left(\bz_t\right) 
%\leq \psi^\star(\btheta_t-\bg_t) + \ell_t^\star (\bg_t)~.
\end{equation}
\end{theorem}

Before proving it, we will need the following technical lemmas.

\begin{lemma}
\label{lemma:ell}
Let $\hat{\ell}_t:\R\to \R$ to be three times differentiable.
\begin{itemize}
\item If $\hat{\ell}'(p_t(h))\geq0, \hat{\ell}'''(p_t(h))\geq 0$, then $s'_t(h)$ is non-negative, non-increasing, convex.
\item If $\hat{\ell}_t'(p_t(h))\leq0, \hat{\ell}_t'''(p_t(h)) \leq 0$, then $s'_t(h)$ is non-positive, non-decreasing, concave.
\end{itemize}
\end{lemma}
%
\begin{proof}
First, observe that
\begin{align*}
s'_t(h) &= \eta\hat{\ell}'(\langle\bx_t-s_t(h)\bq_t,\bq_t\rangle)=\eta \hat{\ell}'(p_t (h))\\
s''(h)  &= \eta\hat{\ell}''(p_t (h))(-\| \bq_t\|^2)s'_t(h)\\
s'''(h) &= \eta\hat{\ell}'''(p_t (h))\| \bq_t\|^4 (s'_t(h))^2\\
&\quad +\eta \hat{\ell}''(p_t (h))(-\| \bq_t\|^2_2)s''(h)~.
\end{align*}

\textbf{Case 1:} $\hat{\ell}'(p)\geq0, \hat{\ell}'''(p)\geq 0$.
In this case, $s'_t(h)\geq0$, $s''(h)\leq0$,  $s'''(h)\geq0$. That is, $s'_t(h)$ is non-negative, non-increasing, and convex.

\textbf{Case 2:} $\hat{\ell}'(p)\leq0, \hat{\ell}'''(p) \leq 0$.
In this case, $s'_t(h)\leq0$, $s''(h)\geq0$,  $s'''(h)\leq0$. That is, $s'_t(h)$ is non-positive, non-decreasing, and concave.
\end{proof}

\begin{lemma}
\label{lemma:int}
Let $s'_t(h)$ to be continuous in $[0,1]$.
\begin{itemize}
\item If $s'_t(h)$ is convex and non-negative, then $\forall h \in [0,1]$ we have
\[
\frac{1}{2}(s'_t(0)+s'_t(h)) \geq \frac{h}{2}(s'_t(0)+s'_t(h)) \geq s_t(h)~.
\]
\item If $s'_t(h)$ is concave and non-positive, then $\forall h \in [0,1]$ we have
\[
\frac{1}{2}(s'_t(0)+s'_t(h)) \leq \frac{h}{2}(s'_t(0)+s'_t(h)) \leq s_t(h)~.
\]
\end{itemize}
\end{lemma}
%
\begin{proof}
Given that $s'_t(h)$ is non-negative, we have
$\frac{1}{2}(s'_t(0)+s'_t(h)) \geq \frac{h}{2}(s'_t(0)+s'_t(h))$.
Now, observe that $\frac{h}{2}(s'_t(0)+s'_t(h))$ is the area of the trapezium with first base $s'_t(0)$, second base $s'_t(h)$, and height $h$. Given that the function is convex and non-negative, this area is bigger than the integral of $s'_t$ between 0 and $h$, that is equal to $s_t(h)$, that proves the statement.

We can prove the other case in a similarly way. 
\end{proof}

We can now prove Theorem~\ref{th:iwa}.
\begin{proof}[Proof of Theorem~\ref{th:iwa}]
The left hand side of \eqref{eq:iwa} is equal to
\[
\psi^\star\left(\int_{0}^1 \! \btheta_t- \bg_t(h) \, \mathrm{d}h\right) + \ell_t^\star \left(\int_{0}^1 \! \bg_t(h) \, \mathrm{d}h\right)~.
\]
Since $\psi^\star$ and $\ell_t^\star$ are convex, applying Jensen's inequality, the left hand side of \eqref{eq:iwa} is upper bounded by
\[
\int_{0}^1 \! \psi^\star( \btheta_t- \bg_t(h)) \, \mathrm{d}h + \int_{0}^1 \! \ell_t^\star(\bg_t(h)) \, \mathrm{d}h~.
\]
Moreover, the right hand side of \eqref{eq:iwa} is equal to
\[
\int_{0}^1 \! \psi^\star(\btheta_t- \bg_t) \, \mathrm{d}h + \int_{0}^1 \! \ell_t^\star(\bg_t) \, \mathrm{d}h~.
\]
So, if we can prove that 
\begin{align*}
&\int_{0}^1 \! \psi^\star(\btheta_t- \bg_t(h)) \, \mathrm{d}h + \int_{0}^1 \! \ell_t^\star(\bg_t(h)) \, \mathrm{d}h\\ 
&\quad \leq \int_{0}^1 \! \psi^\star(\btheta_t- \bg_t) \, \mathrm{d}h + \int_{0}^1 \! \ell_t^\star(\bg_t) \, \mathrm{d}h,
\end{align*}
then \eqref{eq:iwa} is proved. 

For this, it is sufficient to prove that $\forall h\in[0,1]$, we have
\[
\psi^\star(\btheta_t- \bg_t(h)) + \ell_t^\star(\bg_t(h))
\leq \psi^\star(\btheta_t- \bg_t) + \ell_t^\star(\bg_t)~.
\]
Given that $\psi_t^\star(\btheta) = \frac{1}{2\lambda}\|\btheta\|^2_2$, where $\lambda = 1/\eta$, and by using the fact that $\langle \bg, \bx \rangle = \ell_t(\bx)+\ell_t^\star(\bg)$, for any pair of $\bx$, $\bg$ satisfying $\bg \in \partial \ell_t(\bx)$, the inequality above can be written as
\begin{align*}
\frac{1}{2\lambda} \| \btheta_t &-\bg_t(h) \|^2_2 + \langle \bg_t(h), \bx_t(h) \rangle - \ell_t(\bx_t(h)) \\
&\quad \leq \frac{1}{2\lambda}\| \btheta_t -\bg_t \|^2_2 + \langle \bg_t,\bx_t \rangle - \ell_t(\bx_t)~.
%\Leftrightarrow &-\frac{1}{\lambda} \langle \btheta_t, \bg_t(h) \rangle + \frac{1}{2\lambda} \| \bg_t(h)\|^2 + \langle \bg_t(h), \bx_t(h) \rangle - \ell_t(\bx_t(h))~.
\end{align*}

Simplifying this inequality, we have
\begin{align*}
&\frac{1}{2\lambda} \| \bg_t(h)\|^2_2 - \frac{1}{2\lambda} \| \bg_t\|^2_2\\
&\quad \leq \ell_t(\bx_t(h)) - \ell_t(\bx_t) -  \langle \bg_t(h), \bx_t(h) -\bx_t \rangle~.
\end{align*}
Since $\ell_t(\bx)$ is convex, we obtain
\begin{align*}
&\ell_t(\bx_t(h)) - \ell_t(\bx_t) -  \langle \bg_t(h), \bx_t(h) -\bx_t \rangle \\
&\quad \geq \langle \bg_t, \bx_t(h) -\bx_t \rangle - \langle \bg_t(h), \bx_t(h) -\bx_t \rangle \\
&\quad = \langle \bg_t(h)-  \bg_t , \bx_t - \bx_t(h) \rangle~.
\end{align*}

So, we just need to prove that 
\[
\frac{1}{2\lambda} \| \bg_t(h)\|^2_2 - \frac{1}{2\lambda} \| \bg_t\|^2_2
\leq \langle \bg_t(h)-  \bg_t , \bx_t - \bx_t(h) \rangle~.
\]
Using $\| \ba\|^2_2 - \|\bb\|^2_2 = \langle \ba-\bb, \ba+\bb\rangle$, the inequality above can be rewritten as
\begin{align*}
\frac{1}{2\lambda} \langle \bg_t(h) -\bg_t, \bg_t(h) + \bg_t \rangle 
&\leq \langle \bg_t(h)-  \bg_t , \bx_t - \bx_t(h) \rangle \\
&= \langle \bg_t(h)-  \bg_t , s_t(h) \bq_t \rangle~.
\end{align*}
That is,
\begin{align*}
\frac{1}{2}& \left( \hat{\ell}'_t(p_t(h)) - \hat{\ell}'_t(p_t)  \right)\left( \frac{1}{\lambda} \hat{\ell}'_t(p_t(h)) + \frac{1}{\lambda} \hat{\ell}'_t(p_t)  \right) \| \bq_t\|^2_2 \\
&\leq \left( \hat{\ell}'_t(p_t(h)) - \hat{\ell}'_t(p_t)  \right) s_t(h) \| \bq_t\|^2_2~.
\end{align*}
Since $s'_t(h) = \frac{1}{\lambda} \hat{\ell}'_t(p_t(h))$, multiplying both side by $1/\lambda$, the above inequality becomes
\begin{align}
&\frac{1}{2}(s'_t(h) - s'_t(0))(s'_t(0)+s'_t(h)) \nonumber \\
&\quad \leq (s'_t(h) - s'_t(0))s_t(h)~. \label{eq:iwa_s}
\end{align}

Now, we consider two cases.

\textbf{Case 1:} $\hat{\ell}'_t(p_t(h))\geq0$ and $\hat{\ell}_t'''(p_t(h))\geq 0$.
By Lemma~\ref{lemma:ell}, $s'_t(h)$ is non-negative, non-increasing, and convex. So, in particular we have $s'_t(h) - s'_t(0)\leq 0$.
In this case, by Lemma~\ref{lemma:int}, we have $\frac{1}{2}(s'_t(0)+s'_t(h)) \geq s_t(h)$.
%So, \eqref{eq:iwa_s} is proved. 

\textbf{Case 2:} $\hat{\ell}'_t(p_t(h))\leq0$, and $\hat{\ell}_t'''(p_t(h)) \leq 0$.
By Lemma~\ref{lemma:ell}, $s'_t(h)$ is non-positive, non-decreasing, concave. So, in particular, we have $s'_t(h) -s'_t(0) \geq 0$.
In this case,  by Lemma~\ref{lemma:int}, we have $\frac{1}{2}(s'_t(0)+s'_t(h)) \leq s_t(h)$.
%Equation~\eqref{eq:iwa_s} is proved. 

Combining the two cases, we conclude that \eqref{eq:iwa_s} is true that implies that \eqref{eq:iwa} is true as well.
\end{proof}



\subsection{Examples of Losses and IWA Updates}

Now, we present some examples of loss functions that satisfy the requirements of Theorem~\ref{th:iwa} and their corresponding IWA updates from \citet{KarampatziakisL11}. In all the following examples, the prediction of the algorithm on sample $\bq$ is $\langle \bq, \bx\rangle$.

\textbf{Logistic loss: $\hat{\ell}(p) = h \ln(1+e^{-y p})$}.

The IWA update is $\frac{W(e^{h \eta \|\bq\|^2_2 + y p +e^{yp}})-h \eta \|\bq\|^2_2-e^{y p}}{y\|\bq\|^2_2}$ for $y \in \{-1, 1\}$, where $W(x)$ is the Lambert function.
We have that
\[
\hat{\ell}' (p) = \frac{-y h}{1+e^{py}}, \quad 
\hat{\ell}''' (p) = h y^3(-e^{-py-1})~.
\]
When $y \geq 0$, $\hat{\ell}'(p) \leq 0$ and $\hat{\ell}''' (p)\leq 0$. When $y\leq 0$, $\hat{\ell}'(p) \geq 0$ and $\hat{\ell}''' (p)\geq 0$. 

\textbf{Exponential loss: $\hat{\ell}(p)=e^{-yp}$}.

The IWA update is $\frac{p y -\ln(h \eta \|\bq\|_2^2+e^{p y})}{\|\bq\|_2^2 y}$ for $y \in \{-1, 1\}$.
We have that
\[
\hat{\ell}' (p) = y(-e^{-py}), \quad 
\hat{\ell}''' (p) = y^3(-e^{-py})~.
\]
When $y \geq 0$, $\hat{\ell}l'(p) \leq 0$ and $\hat{\ell}''' (p)\leq 0$.  
When $y\leq 0$, $\hat{\ell}'(p) \geq 0$ and $\hat{\ell}''' (p)\geq 0$.


\textbf{Logarithmic loss: $\hat{\ell}(p) = y \ln(y/p) + (1-y) \ln((1-y)/(1-p))$}.

The IWA update is $\frac{p -1 + \sqrt{(p-1)^2+2h \eta \|\bq\|^2_2}}{\|\bq\|_2^2}$ for $y=0$, and $\frac{p - \sqrt{p^2+2h \eta \|\bq\|^2_2}}{\|\bq\|_2^2}$ for $y=1$.
\begin{itemize}
\item if y=0
\[
\hat{\ell}' (p)=\frac{1}{1-p},\quad
\hat{\ell}''' (p)= -\frac{2}{(p-1)^3}~.
\]
\item if y=1
\[
\hat{\ell}' (p)=-\frac{1}{p}, \quad
\hat{\ell}''' (p)= -\frac{2}{p^3}~.
\]
\end{itemize}
For both cases, $\hat{\ell}'(p)$ and $\hat{\ell}'''(p)$ will have the same sign.
 
% \textbf{Hinge loss: $\hat{\ell}(p)=\max(0, 1-yp)$}.
% 
% The IWA update in this case coincides with the proximal update: $-y \min(h \eta, \frac{1-y p}{\|\bq\|^2_2})$ for $y\in \{-1,1\}$.
% When $py\leq 1$,
% \[
% \hat{\ell}' (p)=-y,\quad \hat{\ell}''' (p)=0~.
% \]
% Since the IWA updates will not overshoot the hinge corner, the requirements are satisfied. 

\textbf{Squared loss: $\hat{\ell}(p) = \frac{1}{2}(y-p)^2$.}
The IWA update is $\frac{p-y}{\|\bq\|^2_2}(1-e^{-h \eta \|\bq\|^2_2})$.
\[
\hat{\ell}' (p)=p-y,\quad \hat{\ell}''' (p)=0~.
\]
In this case, the sign of the first derivative can change. However, according to Section 4.2 of \citet{KarampatziakisL11}, for the squared loss IWA will not overshoot the minimum. This means that for any $h \in [0,1]$, $p_t(h)-y$ will always have the same sign, so the conditions are verified.
