\section{Empirical Evaluation}
\label{sec:exp}


% Figure environment removed

% Figure environment removed



IWA as an instantiation of generalized implicit updates, guarantees that in the worst case scenario, it will be at least as good as FTRL with linear models.  Generalized implicit FTRL is a flexible framework in that $\bz_t$ has a bunch of choices.  
In this section, we compare the performance of IWA updates with different choices of $\bz_t$ in Algorithm~\ref{alg:giftrl}, when $\psi_t=\frac{\lambda_t}{2}\|\bx\|_2^2$ and $\lambda_t$ is set as in \citet[Corollary 4.3]{ChenO23}.
In particular, we consider:
\begin{itemize}
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\vspace{-0.3cm}
\item FTRL with linearized losses: $\bz_t=\bg_t$;
\item Implicit FTRL with aProx updates: $\bz_t = \min\left\{ 1,\frac{\lambda_t \ell_t(\bx_t)}{\| \bg_t\|^2}\right\}\bg_t$;
\item Implicit FTRL with IWA updates: $\bz_t = \frac{1}{\eta}s_t(1) \bq_t$;
\item Implicit FTRL with proximal updates.
\vspace{-0.3cm}
\end{itemize}

We conduct linear prediction experiments on datasets from LibSVM~\citep{ChangL11}. We show experiments on classification tasks using the logistic loss, and regression tasks with squared loss. We normalize the datasets and added a constant bias term to the features. Given that in the online learning setting, we do not have the training data and validation data to tune the initial learning rate, we will plot the averaged loss, $\frac{1}{t}\sum_{i=1}^t \ell_i(\bx_i)$, versus different choice of initial learning rate $\eta_0$, that at the same time show the algorithms' sensitivity to the hyperparameter $\eta_0$ and their best achievable performance. We consider $\eta_0 \in [10^{-3},10^3]$. Each algorithm is run 10 times with different shuffling of the data and we plot the average of the averaged losses.

Figure~\ref{fig:reg_plot} shows the averaged loss versus a different selection of hyperparameter $\eta_0$ for regression tasks with squared loss. The figure demonstrates that FTRL with linearized updates is very sensitive to the choice of the hyperparameter $\eta_0$, while the implicit FTRL updates are robust to different setting of hyperparameters. The range of learning rate selection is much broader than that of FTRL with linear models.   

Figure~\ref{fig:logistic_plot} shows the averaged loss versus different selections of hyperparameter $\eta_0$ for classification tasks with logistic loss.  In the experiments, implicit FTRL with IWA updates improves upon FTRL with linearized models. Both IWA updates and aProx updates allow broader learning rate selection. This is in line with previous results in \citet{AsiD19} in the offline setting. Note that in this case the proximal operator does not have a closed-form solution. Yet, IWA provided a way to approximate proximal updates efficiently and, in some sense, to enjoy the stability of proximal updates. 


