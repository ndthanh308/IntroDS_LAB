\section{Introduction}
\label{sec:intro}

In this paper, we are interested in studying variants of gradient updates in the Online Convex Optimization (OCO) setting~\citep{Cesa-BianchiL06, Cesa-BianchiO21, Orabona19}.
In the OCO setting, the learner receives an arbitrary sequence of convex loss functions, selects points before knowing the loss functions, and is evaluated on the values of the loss functions on the points it selects. More in detail, at round $t$ the learner outputs a point $\bx_t$ in a convex feasible set $V\subseteq \R^d$. Then, it receives a loss function $\ell_t: V \to \R$ and it pays the value $\ell_t(\bx_t)$. Given the arbitrary nature of the losses, the learner cannot guarantee to have a small cumulative loss, $\sum_{t=1}^T \ell_t(\bx_t)$. On the other hand, it is possible to minimize the \emph{regret}, that is the difference between the cumulative loss of the algorithm and the one of any arbitrary comparator $\bu \in V$:
\[
\Regret_T(\bu)
\triangleq \sum_{t=1}^T \ell_t(\bx_t) -\sum_{t=1}^T \ell_t(\bu)~.
\]
In particular, a successful OCO algorithm must guarantee a regret that grows sublinearly in time for any $\bu \in V$. In this way, its average performance approaches the one of the best comparator in hindsight.

While in the OCO setting we do not assume anything on how the losses are generated, in the case that the losses are i.i.d. from some fixed (but unknown) distribution we can easily convert a regret guarantee into a convergence rate, using the so-called online-to-batch conversion~\citep{Cesa-bianchiCG02}. Moreover, most of the time the convergence guarantee we obtain in this way are optimal. Hence, the OCO framework allows to analyze many algorithms in the stochastic, batch, and adversarial setting with a single analysis.

The simplest OCO algorithm is Online Gradient Descent (OGD), that, starting from a point $\bx_1 \in V$, in each step updates with
\[
\bx_{t+1} = \Pi_V[\bx_t -\eta \bg_t],
\]
where $\Pi_V$ is the Euclidean projection onto $V$, $\bg_t$ is a gradient of $\ell_t$ in $\bx_t$, and $\eta>0$ is the learning rate. Theoretically and practically, the setting of the learning rate is critical to obtain good performance. In fact, while a learning rate $\eta=O(\tfrac{1}{\sqrt{T}})$ will guarantee an optimal $O(\sqrt{T})$ regret on Lipschitz losses, the constant hidden in the big-O notation can be arbitrarily high.
Moreover, things get even worse when each loss function has an \emph{importance weight} $h_t>0$. This is the case, for example, when each loss function is the loss of the predictor on a classification dataset and we have different classification costs. In this case, the update becomes $\bx_{t+1} = \Pi_V[\bx_t -\eta h_t \bg_t]$ and it should be very intuitive that very large $h_t$ will constraint the learning rate to be small, that in turn will hinder the performance of the algorithm.

In this view, a number of algorithms have been proposed to reduce the sensitivity of OGD to the setting of the learning rate. One of the first successful variants of OGD is the Importance Weight Aware (IWA) updates~\citep{KarampatziakisL11}. The basic idea is to make an infinite number of gradient updates on the loss function $\ell_t$, each of them with an infinitesimal learning rate. This update can be written as the solution of an ODE and it has a closed form for linear predictors and common loss functions. In other words, the IWA updates follow the gradient flow on each loss function.
%Moreover, an appealing property of this update is that updating on the same loss function with weight $h_t$ is equivalent to update $N>1$ times the same loss, each time with importance weight $h_t/N$. Note that this is not true for the plain gradient descent update.

%An importance weight represents the relative importance of one sample. The standard approach to model the importance in gradient descent is simply multiplying the gradient by the importance weight. However, when the importance weight is large, such a sample may cause an update that's far beyond what's necessary to attain a small loss on it.  \citet{KarampatziakisL11} proposed IWA that is efficient and enjoys the invariance property that an example with importance weight $h$ should be treated as if it is a regular example that appears $h$ times in the dataset. In the following, we introduce the problem setting and the method. 

While the IWA updates are not so known by the machine learning community, they work extremely well in practice. In fact, they are part of the default optimizer used in the large-scale machine learning library Vowpal Wabbit.\footnote{\url{https://vowpalwabbit.org/}}
However, while the IWA updates are very natural and intuitive, the best theoretical guarantee is that their regret upper bound will not be too much worse than the one of plain online gradient descent.

\paragraph{Contributions} In this paper, for the first time we show that the IWA updates have a regret bound that is \emph{better} than the one of plain OGD. We use the very recently proposed framework of Generalized Implicit Follow-the-Regularized-Leader (FTRL)~\citep{ChenO23} that allows to design and analyze more general updates than the classic gradient one. In particular, we show that IWA updates can be seen as approximate implicit/proximal updates because they approximately minimize a certain dual function.
