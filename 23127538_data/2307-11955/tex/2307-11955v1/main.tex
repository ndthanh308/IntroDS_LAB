

\documentclass{article}


\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 


\usepackage{hyperref}
\usepackage{enumerate}


\newcommand{\theHalgorithm}{\arabic{algorithm}}




% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2023_dp4ml}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\input{symbol.tex}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\defcitealias{ChenO23}{Chen \& Orabona (ICML 2023)}


\usepackage[textsize=tiny]{todonotes}


\icmltitlerunning{ Implicit Interpretation of Importance Weight Aware Updates}

\begin{document}

\twocolumn[
\icmltitle{Implicit Interpretation of Importance Weight Aware Updates}


\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Keyi Chen}{sch}
\icmlauthor{Francesco Orabona}{sch}

\end{icmlauthorlist}


\icmlaffiliation{sch}{Boston University,  Boston, US}

\icmlcorrespondingauthor{Keyi Chen}{keyichen@bu.edu}
\icmlcorrespondingauthor{Francesco Orabona}{francesco@orabona.com}


\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]


\printAffiliationsAndNotice{}  

\begin{abstract}
Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance.
A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory.
In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework by \citetalias{ChenO23} to analyze generalized implicit updates using a \textbf{dual formulation}. In particular, our results imply that IWA updates can be considered as approximate implicit/proximal updates.
\end{abstract}


\input{intro.tex}
\input{relat.tex}
\input{def.tex}
\input{mainresult.tex}
\input{iwa}
\input{exp}
\input{conc}

\section*{Acknowledgements}
Francesco Orabona is supported by the National Science Foundation under the grant no. 2046096 ``CAREER: Parameter-free Optimization Algorithms for Machine Learning''.



\bibliography{../../../../learning}
\bibliographystyle{icml2023}


\end{document}

