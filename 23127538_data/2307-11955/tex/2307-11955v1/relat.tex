\paragraph{Related Work}
While IWA updates~\citep{KarampatziakisL11} were motivated by the use of importance weights, they end up being similar to the implicit~\citep{KivinenW97,KulisB10,CampolongoO20} and proximal~\citep{Rockafellar76} updates. In particular, for some losses like the hinge loss, the IWA update and the implicit/proximal update coincide. In this view, they are also similar to the aProx updates~\citep{AsiD19}, which take an implicit/proximal step on truncated linear models. However, as far as we know, no explicit relationship between implicit/proximal updates and IWA updates was known till now. Moreover, the best guarantee for IWA updates just shows that in some restricted cases the regret upper bound is not too much worse than the one of OGD~\citep{KarampatziakisL11}. Finally, all the previous analysis of implicit updates in online learning are conducted in the primal space, while our analysis is done completely in the dual space.


% While there are many works on implicit mirror descent in both the online and offline setting~\citep[see, e.g.,][]{Moreau65,Martinet70,Rockafellar76,KivinenW97,ParikhB14,CampolongoO20,Shtoff22}, the number of works that deal with implicit updates for FTRL is quite limited. We are only aware of \citet{McMahan10}, which quantifies a gain only for specific regularizers. However, the framework in \citet{McMahan10} is non-constructive in the sense that it is difficult to see how to generalize implicit updates. \citet{JoulaniGS17} extends this last result, but it does not provide a link with the maximization of the dual function that governs the regret upper bound.
% 
% The closest approach to our framework is the one of \citet{Shalev-ShwartzS07,ShalevS07},  which develop a theory of FTRL updates as maximization of a dual function. However, their framework is limited to a specific shape of regularizers and it does not deal with implicit updates.

%For implicit OMD, \citet{CampolongoO20} showed that implicit
%updates give rise to regret guarantees that depend on the temporal variability of the losses, so that constant regret is achievable if the variability of the losses is zero. They suggest that FTRL with full losses can achieve the same guarantee, but they also point out that given its computational complexity it would be ``not worth pursuing.''
%Here, we show how to achieve the same bound of implicit OMD with our generalized implicit FTRL, while retaining the same computational complexity of implicit OMD.

%Proximal updates on surrogate functions were introduced in \citet{AsiD19} for the OMD algorithm.
%\citet{ChenCO22} have tried to incorporate the same idea in an FTRL-based parameter-free algorithm~\cite{OrabonaP16}, however, their approach is ad-hoc is it seems difficult to generalize it.


