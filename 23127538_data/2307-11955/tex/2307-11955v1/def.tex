\section{Definitions and Tools}
We define here some basic concepts and tools of convex analysis, we refer the reader to, e.g., \citet{Rockafellar70,BauschkeC11} for a complete introduction to this topic. We will consider extended value function that can assume infinity values too.
A function $f$ is \emph{proper} if it is nowhere $-\infty$ and finite somewhere.
A function $f:V \subseteq \R^d \rightarrow [-\infty, +\infty]$ is \emph{closed} if $\{\bx: f(\bx) \leq \alpha\}$ is closed for every $\alpha \in \R$.
For a proper function $f:\R^d \rightarrow (-\infty, +\infty]$, we define a \emph{subgradient} of $f$ in $\bx \in \R^d$ as a vector $\bg \in \R^d$ that satisfies $f(\by)\geq f(\bx) + \langle \bg, \by-\bx\rangle, \ \forall \by \in \R^d$.
We denote the set of subgradients of $f$ in $\bx$ by $\partial f(\bx)$.
The \emph{indicator function of the set $V$}, $i_V:\R^d\rightarrow (-\infty, +\infty]$, has value $0$ for
$\bx \in V$ and $+\infty$ otherwise.
We denote the \emph{dual norm} of a norm $\|\cdot\|$ by $\|\cdot\|_\star$.
A proper function $f : \R^d \rightarrow (-\infty, +\infty]$ is \emph{$\mu$-strongly convex} over a convex set $V \subseteq \interior \dom f$ w.r.t. $\|\cdot\|$ if $\forall \bx, \by \in V$ and $\forall \bg \in \partial f(\bx)$, we have $f(\by) \geq f(\bx) + \langle \bg , \by - \bx \rangle + \frac{\mu}{2} \| \bx - \by \|^2$.
%A function $f:V \rightarrow \R$, differentiable in an open set containing $V$, is \emph{$L$-smooth} w.r.t. $\|\cdot\|$ if  $f(\by) \leq f(\bx) + \langle \nabla f(\bx) , \by - \bx \rangle + \frac{M}{2} \| \bx - \by \|^2$ for all $\bx, \by \in V$.
For a function $f: \R^d\rightarrow [-\infty,\infty]$, we define the \emph{Fenchel conjugate} $f^\star:\R^d \rightarrow [-\infty,\infty]$ as $f^\star(\btheta) = \sup_{\bx \in \R^d} \ \langle \btheta, \bx\rangle - f(\bx)$.
From this definition, we immediately have the Fenchel-Young inequality: $f(\bx) + f^\star(\btheta) \geq \langle \btheta, \bx\rangle, \ \forall \bx, \btheta$.
%Let $\psi: X \rightarrow \R$ be strictly convex and continuously differentiable on $\interior X$. The \emph{Bregman Divergence} w.r.t. $\psi$ is $B_\psi : X \times \interior X \rightarrow \R_+$ defined as $B_\psi(\bx, \by) = \psi(\bx) - \psi(\by) - \langle \nabla \psi(\by), \bx - \by\rangle$.
%and $\psi$ is called the \emph{distance generating function}.
%We assume that $\psi$ is strongly convex w.r.t. a norm $\|\cdot\|$ in $\interior X$. We also assume w.l.o.g. the strong convexity constant to be~1, which implies
%\begin{equation} \label{eq:bregman_strongly_convex}
%B_\psi(\bx,\by)\geq \frac{1}{2}\|\bx-\by\|^2, \quad \forall \bx \in X , \by \in \interior X~.
%\end{equation}
%In this paper, we consider the \emph{online learning scenario}, where in each round a learning agent predicts $\bx_t \in V$, where $V\subset \interior X$. Then, he receives a loss $\ell_t:X\rightarrow\R$ and pays $\ell_t(\bx_t)$. The aim of the learner is to minimize the \emph{regret} w.r.t. to any vector $\bu \in V$, defined as
%\[
%R_T(\bu) := \sum_{t=1}^T \left(\ell_t(\bx_t)-\ell_t(\bu)\right)~.
%\]
We will also make use of the following properties of Fenchel conjugates.
\begin{theorem}[{\citep[Theorem~5.7]{Orabona19}}]
\label{thm:props_fenchel}
Let $f:\R^d \rightarrow (-\infty,+\infty]$ be proper. Then, the following conditions are equivalent:
\begin{enumerate}[(a)]
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\vspace{-0.35cm}
\item $\btheta \in \partial f(\bx)$.
\item $\langle \btheta, \by\rangle - f(\by)$ achieves its supremum in $\by$ at $\by=\bx$.
\item $f(\bx)+f^\star(\btheta)=\langle \btheta,\bx\rangle$.
\vspace{-0.35cm}
\end{enumerate}
Moreover, if $f$ is also convex and closed, we have an additional equivalent condition
\begin{enumerate}[(a)]
\setcounter{enumi}{3}
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{0pt}
\vspace{-0.35cm}
\item $\bx \in \partial f^\star(\btheta)$. 
\end{enumerate}
\end{theorem}

%\begin{theorem}[{\citep[Theorem~6.9]{Orabona19}}]
%\label{thm:duality}
%Let $\psi:\R^d \rightarrow (-\infty, +\infty]$ be a proper, closed, convex function. Then, $f$ is $\lambda>0$ strongly convex w.r.t. $\|\cdot\|$ iff $\psi^\star$ is $\frac{1}{\lambda}$-smooth w.r.t. $\|\cdot\|_\star$ on $\R^d$.
%\end{theorem}
