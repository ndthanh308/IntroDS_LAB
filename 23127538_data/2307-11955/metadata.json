{
  "title": "Implicit Interpretation of Importance Weight Aware Updates",
  "authors": [
    "Keyi Chen",
    "Francesco Orabona"
  ],
  "submission_date": "2023-07-22T01:37:52+00:00",
  "revised_dates": [],
  "abstract": "Due to its speed and simplicity, subgradient descent is one of the most used optimization algorithms in convex machine learning algorithms. However, tuning its learning rate is probably its most severe bottleneck to achieve consistent good performance. A common way to reduce the dependency on the learning rate is to use implicit/proximal updates. One such variant is the Importance Weight Aware (IWA) updates, which consist of infinitely many infinitesimal updates on each loss function. However, IWA updates' empirical success is not completely explained by their theory. In this paper, we show for the first time that IWA updates have a strictly better regret upper bound than plain gradient updates in the online learning setting. Our analysis is based on the new framework, generalized implicit Follow-the-Regularized-Leader (FTRL) (Chen and Orabona, 2023), to analyze generalized implicit updates using a dual formulation. In particular, our results imply that IWA updates can be considered as approximate implicit/proximal updates.",
  "categories": [
    "cs.LG"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.11955",
  "pdf_url": null,
  "comment": "arXiv admin note: text overlap with arXiv:2306.00201",
  "num_versions": null,
  "size_before_bytes": 403743,
  "size_after_bytes": 144119
}