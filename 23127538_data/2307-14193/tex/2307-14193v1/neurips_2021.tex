\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[final]{neurips_2021}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage[]{hyperref}       % hyperlinks                
\usepackage[hidelinks]{hyperref}       % hyperlinks            
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage[hypcap=false]{caption}
\usepackage{multirow}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{mathtools}

\usepackage[pdftex]{graphicx}
\graphicspath{{figures/}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\DeclareMathOperator{\GammaDist}{Gamma}
\DeclareMathOperator{\GumbelDist}{Gumbel}
\DeclareMathOperator{\ExpDist}{Exp}
\DeclareMathOperator{\ShortestPath}{SP}


\newcommand{\Lagr}{{\cal L}}
\newcommand{\opt}{\textsc{(Optional) }}
\newcommand{\vx}{x}
\newcommand{\mw}{W}
%newcommand{\vlambda}{{\boldsymbol \lambda}}
\newcommand{\vlambda}{{\lambda}}
\newcommand{\vy}{y}
\newcommand{\vc}{c}
\newcommand{\mc}{C}
\newcommand{\vw}{w}
\newcommand{\vb}{b}

\newcommand{\bc}{\bm{c}}
\newcommand{\bz}{\bm{z}}
\newcommand{\bx}{\bm{x}}
\newcommand{\by}{\bm{y}}
\newcommand{\bw}{\bm{w}}
\newcommand{\bu}{\bm{u}}
\newcommand{\bv}{\bm{v}}
\newcommand{\bs}{\bm{s}}
\newcommand{\br}{\bm{r}}
\newcommand{\bo}{\bm{o}}

\newcommand{\dd}{\mathrm{d}}

\newcommand{\hbx}{\hat{\mathbf{z}}}

\newcommand{\htheta}{\hat{\theta}}
\newcommand{\ttheta}{\tilde{\theta}}

\newcommand{\stat}{t}

\newcommand{\bmu}{\bm{\mu}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\blambda}{\bm{\lambda}}

\newcommand{\lin}{\operatorname{Lin}}
\newcommand{\relu}{\operatorname{ReLU}}

\usepackage{cleveref}

\title{Efficient Learning of Discrete-Continuous Computation Graphs}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  David Friede\textsuperscript{\textbf{1,2}} \\ %\thanks{NEC Laboratories Europe, Heidelberg, Germany. Contact: \texttt{david.friede@neclab.eu}} \\
  \texttt{david@informatik.uni-mannheim.de} \vspace{2mm} \\
  \textsuperscript{\textbf{2}}\text{University of Mannheim} \\
  Mannheim, Germany \\
  % examples of more authors
\And
  Mathias Niepert\textsuperscript{\textbf{1}} \\
  \texttt{mathias.niepert@neclab.eu}  \vspace{2mm} \\
  \textsuperscript{\textbf{1}}\text{NEC Laboratories Europe} \\
  Heidelberg, Germany \\
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle
\begin{abstract}
Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges.
First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Second, we propose dropout residual connections specifically tailored to stochastic, discrete-continuous computation graphs.
With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic softmax tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets.
\end{abstract}


\section{Introduction}


Neuro-symbolic learning systems aim to combine discrete and continuous operations. The majority of recent research has focused on integrating neural network components into probabilistic \mbox{logics~\cite{rocktaschel2017end,manhaeve2018deepproblog}}, that is, making logic-based reasoning approaches more amenable to noisy and high-dimensional input data. On the other end of the spectrum are the data-driven approaches, which aim to learn a modular and discrete program structure in an end-to-end neural network based system.
The broader vision followed by proponents of these approaches are systems capable of assembling the required modular operations to solve a variety of tasks with heterogeneous input data. Reinforcement learning~\cite{sutton2018reinforcement}, neuro-symbolic program synthesis~\cite{parisotto2016neuro}, and neural module networks~\cite{andreas2017neural} are instances of such data-integrated discrete-continuous learning systems.

We focus on learning systems comprised of both symbolic and continuous operations where the symbolic operations are modeled as \emph{discrete} probability distributions. The resulting systems can be described by their \emph{stochastic computation graphs}, a formalism recently introduced to unify several related approaches~\cite{schulman2015gradient}. Figure~\ref{fig:examples-dc} illustrates two instances of such stochastic computation graphs. For a given high-dimensional input such as a set of images or a list of symbols, a computation graph, consisting of stochastic (discrete) and continuous (neural) nodes, is created to solve a particular task. Both, the mechanism to assemble the graphs (if not already provided) and the various operations are learned end-to-end. Discrete nodes in the computation graph are Gibbs distributions modeled at a particular temperature which can also be used to model the argmax operation at zero temperature.

The majority of prior work has focused on graphs with a single discrete probability distribution along each execution path. Examples are the discrete variational autoencoder~\cite{jang2016categorical}, learning to explain~\cite{chen2018learning}, and other applications of stochastic softmax tricks~\cite{paulus2020gradient}. We aim to analyze and improve the training behavior of \emph{complex} computation graphs, that is, graphs with more than one discrete probability distribution in its execution paths. More concretely, we focus on computation graphs where the stochastic nodes are categorical random variables modeled with the Gumbel-softmax trick~\cite{jang2016categorical,maddison2016concrete}.
In Section~\ref{sec:efficient}, we show both analytically and empirically that it is challenging to optimize the parameters of these models, mainly due to insufficient gradient signals often caused by local minima and saturation.
We propose two new methods for mitigating the causes of poor optimization behavior.
First, we show in Section~\ref{sec:tm} that increasing the scale parameter $\beta$ of the Gumbel noise perturbations improves the models' learning behavior. Increasing $\beta$ increases the probability of escaping local minima during training. Second, we propose dropout residual connections for discrete-continuous computation graphs in Section~\ref{sec:dr}. By randomly skipping some discrete distributions, we provide more informative gradients throughout the full computation graph.
We show empirically for several complex discrete-continuous models that the proposed methods are required for training. We also show that the resulting discrete-continuous models generalize better and significantly outperform state of the art approaches on several benchmark datasets.






% Figure environment removed



\section{Efficient Learning of Discrete-Continuous Computation Graphs}\label{sec:efficient}

Standard neural networks compose basic differentiable functions. These networks, therefore, can be fully described by a directed acyclic graph (the computation graph) that determines the operations executed during the forward and backward passes.
\citet{schulman2015gradient} proposed \emph{stochastic computation graphs} as an extension of neural networks that combine deterministic and stochastic operations -- a node in the computation graph can be either a differentiable function or a probability distribution.
A \emph{stochastic node} $X$ is typically a random variable with a parameterized probability distribution $\mathit{p}_{\btheta(x)}$ with parameters $\btheta$. Suppose that $f$ is a smooth function (such as the loss function of a learning problem), then the gradient of $f$ at the stochastic node is $\nabla_{\btheta}\mathbb{E}_{\mathit{p}_{\btheta}(x)}[f(x)]$.
\citet{kingma2013auto} proposed the \emph{reparameterization trick} to overcome the problem of computing gradients with respect to the parameters of a distribution.
The idea is to find a function $g$ and distribution $\rho$ such that one can replace $x \sim p_{\btheta}(x)$ by $x = g(z, \btheta)$ with $z \sim \rho(z)$. If this is possible, then one can write
\begin{equation}%\label{rel={eq:repara_trick}}
  \nabla_{\btheta}\mathbb{E}_{x\sim\mathit{p}_{\btheta}(x)}[f(x)]
%  &= \nabla_\theta\int\mathit{p}_\theta(x)f(x)dx\\
%  &\labelrel={eq:repara_trick} \nabla_\theta\int\mathit{p}(z)f(g(z,\theta))dz\\
  =%\labelrel={eq:repara_trick}
  \mathbb{E}_{z\sim\rho(z)}[\nabla_{\btheta} f(g(z,\btheta))]
  \approx \frac{1}{S} \sum_{i=1}^{S} \nabla_{\btheta} f(g(z_i,\btheta)) \mbox{ with } z_i\sim \rho(z).
\end{equation}
With this paper we address the problem of training stochastic computation graphs where the stochastic nodes are based on categorical (discrete) distributions approximated using stochastic softmax tricks~\citep{jang2016categorical,paulus2020gradient}. More specifically, we consider discrete-continuous components modeling a categorical variable $X$ with $k$ possible values $\bz_1, ..., \bz_k$. Each of the $\bz_i$ is the one-hot encoding of the category $i$. We consider discrete-continuous functions\footnote{For the sake of simplicity we assume the same input and output dimensions.} $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ with $\bv = f(\bu)$ defined as follows


% Figure environment removed

\begin{minipage}{.36\textwidth}
\vspace{-2mm}
\begin{subequations}
\begin{align}
\btheta & = g_{\bw}(\bu)  \label{eqn:dcc-1}\\
p(\bz_i; \btheta) & = \frac{\exp(\btheta_i)}{\sum_{j=1}^{k}\exp(\btheta_j)}  \\
\bz & \sim p(\bz;\btheta)  \\
\bv & = h_{\bw'}(\bz) \label{eqn:dcc-4}
\end{align}
\end{subequations}
\end{minipage}
\hfill
\begin{minipage}{.5\textwidth}
\centering
% Figure removed
\captionof{figure}{Illustration of generic discrete-continuous component (function $f$).} \label{setting-ilustration-main}
\end{minipage}


We assume that the functions $g$ and $h$ (parameterized by $\bw$ and $\bw'$) are expressed using differentiable neural network components. %\footnote{Both $g$ and $h$ can also be the identity.}
In the majority of cases, $h$ is defined as $\bz^{\intercal} \mathbf{w'}$ for a learnable matrix~$\mathbf{w'}$, mapping object $\bz_i$ to its learned vector representation. Figure~\ref{setting-ilustration-main} illustrates a generic discrete-continuous component. %For example, the neural network encoding images as well as the operations such as $\operatorname{ADD}$ each map their input to the parameters $\btheta$ of a discrete probability distribution. $\bz$ represents the (relaxed) one-hot encoding of the categories representing digits.


Let $\GumbelDist(0, \beta)$ be the Gumbel distribution with location $0$ and scale $\beta$.
Using the Gumbel-max trick, we can sample $\bz \sim p(\bz; \btheta)$ as follows:
\begin{equation}
\label{eq:gumbel-max}
\bz := \bz_i \ \ \ \mbox{ with } \ \ \ i = \argmax_{j \in \{1, ..., k\}} \left[ \btheta_j + \bm{\epsilon_j}\right] \mbox{ where } \bm{\epsilon_j} \sim \GumbelDist(0, 1).
\end{equation}
%
The Gumbel-softmax trick is a relaxation of the Gumbel-max trick (relaxing the argmax into a softmax with a scaling parameter $\lambda$) that allows one to perform standard backpropagation:
\begin{equation}
\label{eq:gumbel-softmax}
 \bz_i = \frac{\exp\left( (\btheta_i + \bm{\epsilon_i}) / \tau \right)}{\sum_{j=1}^{k}\exp\left( (\btheta_j + \bm{\epsilon_j}) / \tau \right)} \mbox{ where } \bm{\epsilon_i} \sim \GumbelDist(0, 1).
\end{equation}
Hence, instead of sampling a discrete $\bz$, the Gumbel-Softmax trick computes a relaxed $\bz$ in its place.
%The lower the temperature $\lambda$ the more discrete (the closer to a one-hot encoding) is the vector $\bz$.








We are concerned with the analysis of the behavior of the Gumbel-softmax trick in more complex stochastic computation graphs. In these computation graphs, multiple sequential Gumbel-softmax components occur on a single execution path. %We propose several ways to improve the learning dynamics.
%For instance, Figure~\ref{fig:examples-dc} depicts two such computation graphs.
Throughout the remainder of the paper, we assume that during training, we use the Softmax-trick as outlined above, while at test time, we sample discretely using the Gumbel-max trick. Depending on the use case, we also sometimes compute the argmax at test time instead of samples from the distribution.
%or compute the argmax, depending on the use case.

% Figure environment removed

Let us first take a look at $\partial \bv / \partial \bu$, that is, the partial derivative of the output of the discrete-continuous function $f$ with respect to its input. Using the chain rule (and with a slight abuse of notation to simplify the presentation), we can write
$\partial \bv / \partial \bu = (\partial \bv / \partial \bz) (\partial \bz / \partial \btheta) (\partial \btheta / \partial \bu).$
By assumption $\partial \bv / \partial \bz$ and $\partial \btheta / \partial \bu$ exist and are efficiently computable.
Using the derivative of the softmax function, we have
%
\begin{equation}
\label{eqn-derivative-2}
\frac{\partial z_i}{\partial \theta_j}  = \left\lbrace
\begin{array}{ll}
       \ \ \ \frac{1}{\tau}\ z_i (1 - z_i) & \mbox{ if } i = j
       \vspace{2mm} \\
       -\frac{1}{\tau}\ z_i z_j & \mbox{ if } i \neq j.
\end{array}
\right.
\end{equation}
%
We have empirically observed that during training, there is a tendency of the models to not utilize all existing categories of the distribution and to wrongfully map different input representations to the same category. For instance, for the MNIST addition problem, all encoded images of two digits are mapped to the same category of the distribution.
In these cases, the gradients of the parameters of the unused categories are vanishingly small.
In order to analyze this behavior more closely, we derive an upper bound on the gradient of a parameter $\theta_j$ of the categorical distribution with respect to a loss function $L$. First, we have that
%
\begin{equation}\label{eq:gradient}
     \left\Vert \left(\frac{\partial \bz}{\partial \theta_j}\right) \right\Vert^2_F
    = \sum_i \left\vert \frac{\partial z_i}{\partial \theta_j} \right\vert^2
%    &= \left \vert \frac{1}{\tau} z_j (1 - z_j) \right\vert^2 +
%        \sum_{i\neq j} \left \vert \frac{1}{\tau} z_i z_j \right \vert^2  \\
%    &= \frac{1}{\tau^2} z_j^2 (1 - z_j)^2 +
%        \sum_{i\neq j} \frac{1}{\tau^2} z_i^2 z_j^2  \\
%    &= \frac{1}{\tau^2} \bigl( z_j^2 (1 - z_j)^2 +
%        \sum_{i\neq j} z_i^2 z_j^2 \bigr)  \\
    = \frac{1}{\tau^2}\ z_j^2 \bigl( (1 - z_j)^2 + \sum_{i\neq j} z_i^2 \bigr)
%    &\leq \sqrt{\frac{1}{\tau^2} z_j^2 \left( (1 - z_j)^2 + 1 \right)} \\
%    &\leq \sqrt{2 \frac{1}{\tau^2} z_j^2} \\
    \leq \frac{2}{\tau^2}\ z_j^2,
\end{equation}
%
where $\Vert \cdot \Vert_{F}$ is the Frobenius norm. The full derivation of this inequality can be found in the Appendix. Since the Frobenius norm is a sub-multiplicative matrix norm, we can write
%
\begin{equation}
\label{eqn-ineqaulity}
    \vert \text{grad}(\theta_j) \vert
    = \left\Vert \frac{\partial L}{\partial \theta_j} \right\Vert_F
    = \left\Vert \frac{\partial L}{\partial \bz} \frac{\partial \bz}{\partial \theta_j} \right\Vert_F
    \leq \left\Vert \left(\frac{\partial L}{\partial \bz}\right) \right\Vert_F \left\Vert \left(\frac{\partial \bz}{\partial \theta_j}\right) \right\Vert_F
    \leq \frac{\sqrt{2}}{\tau} \left\Vert \left(\frac{\partial L}{\partial \bz}\right) \right\Vert_F z_j
\end{equation}
where the last inequality follows from \cref{eq:gradient}.
%
%(a sub-multiplicative matrix norm that is \textit{equivalent} to the spectral norm).
%Hence, $\Vert (\frac{\partial \bz}{\partial \theta_j}) \Vert_F \leq \frac{\sqrt{2}}{\tau} z_j$.
Hence, a small value of $\theta_j$ (and consequently $z_j$) leads to a small gradient for $\theta_j$. As a consequence, such models are more prone to fall into poor minima during training.
Figure~\ref{fig:poor-minima-acc} illustrates an example of such a situation, which we encountered in practice. The small value of a specific $z_j$ leads to a small gradient at $\theta_j$ (right) as well as to suboptimal plateauing of the accuracy curve (middle). In other words, the downstream gradients information for $\theta_j$ is neutralized by a small $z_j$. Note that this issue only occurs when $L$ is more complex and not the categorical-cross entropy loss.

Another related problem is saturated probabilities in sequentially connected probabilistic components. Similar to the problem of sigmoid activation units in deep neural networks, which have been replaced by ReLUs and similar modern activation functions, saturated probabilities lead to vanishing gradients. Indeed, we observe in several experiments that the Gumbel-softmax distributions saturate and, as a result, that sufficient gradient information cannot reach the parameters of upstream neural network components.

We propose two strategies to mitigate the vanishing gradient behavior in complex discrete-continuous computation graphs. First, we analyze the interplay between the temperature parameter $\tau$ and the scale parameter $\beta$ of the Gumbel distribution. We make the subtle difference between these parameters explicit, which may also be of interest for improving stochastic softmax tricks~\citep{paulus2020gradient}.
By increasing $\beta$ relative to $\tau$ while keeping $\tau$ fixed, we increase the probability of a gradient flow in the case of saturation. We show that the larger $\beta$, the more uniformly distributed (over the classes) are the discrete samples from the distribution, increasing the chance to escape poor minima.
Second, we introduce \textsc{DropRes} connections allowing us to lower-bound the gradients in expectation and leading to more direct gradient flow to parameters of upstream model components. %We run extensive experiments to validate the hypothesis that our proposed strategies have the desired effect and enable efficient training. We show that in several cases, discrete-continuous models that one cannot train at all with standard stochastic softmax tricks are trainable after adding the proposed strategies.
%
% \begin{align*}
%      \left\Vert \left(\frac{\partial \bz}{\partial \theta_j}\right) \right\Vert_F
%     &= \left\Vert \left(\frac{\partial z_1}{\partial \theta_j} \dots \frac{\partial z_n}{\partial \theta_j} \right) \right\Vert_F \\
%     &= \sqrt{\sum_i \left\vert \frac{\partial z_i}{\partial \theta_j} \right\vert^2} \\
%     &= \sqrt{\left \vert \frac{1}{\tau} z_j (1 - z_j) \right\vert^2 +
%         \sum_{i\neq j} \left \vert \frac{1}{\tau} z_i z_j \right \vert^2}  \\
%     &= \sqrt{\frac{1}{\tau^2} z_j^2 (1 - z_j)^2 +
%         \sum_{i\neq j} \frac{1}{\tau^2} z_i^2 z_j^2}  \\
%     &= \sqrt{\frac{1}{\tau^2} \bigl( z_j^2 (1 - z_j)^2 +
%         \sum_{i\neq j} z_i^2 z_j^2 \bigr)}  \\
%     &= \sqrt{\frac{1}{\tau^2} z_j^2 \bigl( (1 - z_j)^2 + \sum_{i\neq j} z_i^2 \bigr)} \\
%     &\leq \sqrt{\frac{1}{\tau^2} z_j^2 \left( (1 - z_j)^2 + 1 \right)} \\
%     &\leq \sqrt{2 \frac{1}{\tau^2} z_j^2} \\
%     &= \frac{\sqrt{2}}{\tau} z_j
% \end{align*}
%
% Figure environment removed
%
\subsection{\textsc{TempMatch}: Temperature Matching}\label{sec:tm}% and Inverse Scale Annealing}
%
We explore the behavior of two interdependent parameters: the Gumbel-softmax temperature $\tau$ and the Gumbel scale parameter $\beta$. First, we have the temperature parameter~$\tau$~from the Gumbel-softmax trick (see \cref{eq:gumbel-softmax}).
The purpose of this parameter is to make the output of the softmax function more or less discrete, that is, more or less concentrated on one of the categories.
Second, and this is a new insight, we can adjust the scale parameter $\beta$ of the Gumbel distribution. For scale parameter $\beta$, we sample the noise perturbation iid as $\epsilon_i \sim \GumbelDist(0, \beta)$. If we use the Gumbel-max trick of \cref{eq:gumbel-max} we implicitly generate samples from the distribution
$p(\bz_i; \btheta) = \exp(\btheta_i / \beta) / \sum_{j=1}^{k}\exp(\btheta_j / \beta).$
Increasing the scale parameter $\beta$, therefore, makes the Gumbel-max samples more uniform and less concentrated. When using the  Gumbel-softmax trick instead of the Gumbel-max trick, we obtain samples $\bz_i$ that are more uniformly distributed over the categories and more discrete. Indeed, in the limit of $\beta \rightarrow \infty$, we obtain discrete samples uniformly distributed over the categories, independent of the logits. For $\beta \rightarrow 0$, we obtain the standard softmax function. %distribution with temperature $1$.
Figure~\ref{fig:simplex} illustrates the impact of the two parameters on the Gumbel-softmax distribution.
Now, the problem of insufficient gradients caused by local minima can be mitigated by increasing the scale parameter $\beta$ \emph{relative to} the temperature parameter $\tau$. The annealing schedule we follow is the inverse exponential annealing $\beta_t=\tau(1-e^{-t \gamma})$ for some $\gamma > 0$.
Increasing $\beta$ increases the probability of generating samples whose maximum probabilities are more uniformly distributed over the categories. This has two desired effects. First, samples drawn during training have a higher probability of counteracting poor minima caused at an earlier stage of the training. Figure~\ref{fig:poor-minima-acc} (left) shows that higher values for $\beta$ allow the model to find its way out of poor minima. A higher value of $\beta$ makes the model more likely to utilize categories with small $\theta$s, allowing the model to obtain improved minima (Figure~\ref{fig:poor-minima-acc} (middle)).
Second, gradients propagate to parameters of the upstream components even after downstream components are saturating.
%\footnotetext{Both setups, TM and TauAnn, had a single 'bad seed', in which no optimization was possible and the accuracy remained constant at $\sim10\%$. We exclude these runs here for clarity and add them to the Appendix.}
To illustrate the second effect, we conducted the toy experiment depicted in Figure~\ref{fig:temp-matching-example}. The model here  computes Softmax$(\btheta + \bm{\epsilon})$ with parameters $\btheta^{\intercal}=(\theta_1,\theta_2)$ and $\bm{\epsilon} \sim \GumbelDist(0, \beta)$. The learning problem is defined through a cross-entropy loss between the output probabilities and a constant target vector $(1.0, 0.0)^{\intercal}$. We observe that early during training, lower values for $\beta$ work well and exhibit less variance than higher values. However, once the probabilities and, therefore, the gradients start to saturate, a higher value for $\beta$ enables the continued training of the model. While the example is artificial, it is supposed to show that larger values of the scale parameter $\beta$, sustain gradients for upstream components even if downstream components have saturated.
%Indeed, even though the softmax probability is close to 1, we observe a continuous increase of the parameter $\theta_1$ and a high enough gradient signal for~$\beta\geq2$.
%It also shows that annealing the \emph{inverse} scale parameter, that is, increasing the scale parameter relative to $\tau$ over time, should be beneficial.

In summary, increasing the Gumbel scale parameter $\beta$ has positive effects on the training dynamics. We propose an exponential increasing scheme relative to $\tau$. % the two parameters $\tau$ and $\beta$ should be \emph{matched}, that is, optimized relative to each other,
We show empirically that for more realistic learning problems choosing a higher value for $\beta$ %(and/or increasing $\beta$ over time)
is beneficial, especially later during training.






% Figure environment removed

\subsection{\textsc{DropRes}: Residual Dropout Connections }
\label{sec:dr}

Residual (or shortcut) connections are crucial when training deep neural networks~\citep{he2015deep}. A standard residual connection for the discrete-continuous components we consider here would be achieved by replacing Equation~(\ref{eqn:dcc-4}) with
%$\bv = (1-\alpha) \bu + \alpha  h_{\bw'}(\bz).$ At training time, one could use several strategies to balance the residual and model connections by, for instance,  setting $\alpha > 0$ to be a constant or increasing it towards $1$.
$\bv = \bu + h_{\bw'}(\bz).$ By creating a direct connection to the continuous input of the discrete distributions, the optimizing problem simplifies.
Unfortunately, what sets our problem apart from other end-to-end learnable neural networks is that we want to completely remove the residual connections at test time to obtain pure discrete representations and, therefore, the desired generalization properties.
%We show empirically that models with residual connections tend to strongly overfit even if the parameter $\alpha$ is close to $1$, and completely fail to perform well at test time.
To mitigate the problem of overfitting to the existence of residual connections while at the same time reducing vanishing gradients in expectation, we propose \textsc{DropRes} connections. These are residual connections sampled from a Bernoulli distribution with parameter $\alpha$. We replace Equation~(\ref{eqn:dcc-4}) by
\begin{equation}
    \label{eqn-dropres}
\bv  = \left\lbrace
\begin{array}{ll}
        \bu + h_{\bw'}(\bz) & \mbox{ with probability } 1-\alpha  \\
        h_{\bw'}(\bz) & \mbox{ with probability } \alpha,
\end{array}
\right.
\end{equation}
that is, we drop out the residual connection with probability $\alpha$.
With probability $(1-\alpha)$ we obtain a \emph{shortcut connection} between the neural components, effectively bypassing the Gumbel-softmax. With probability $\alpha$ the training path goes exclusively through the Gumbel-softmax distribution.
The expectation of the gradients, taken over a Bernoulli random variable with parameter $\alpha$,  is now
\begin{equation}
    \mathbb{E}_{} \left[ \frac{\partial \bv}{\partial  \bu} \right] = (1-\alpha) \bm{1} + \frac{\partial \bv}{\partial \bz} \frac{\partial \bz}{\partial \btheta} \frac{\partial \btheta}{\partial \bu}.
\end{equation}
We propose a simple linear scheme to modify $\alpha$ from $0\rightarrow 1$ during training. Hence, the model obtains a stronger gradient signal for upstream models \emph{in expectation} in the early phase of training.

To illustrate the impact of dropout residual connections, we analyzed the gradients of message passing steps for the ListOps problem (see experiments) in a discrete-continuous computation graph of the type depicted in Figure~\ref{fig:examples-dc} (bottom). As we can observe in Figure~\ref{fig:gnn_grad}, if we do not use dropout residual connections, the greater the distance of a discrete-continuous operation to the loss function (the lower the value on the x-axis), the smaller is the mean absolute value of the gradients reaching it. This illustrates the vanishing gradient problem. When using dropout residual connections, on the other hand, the mean absolute values of the gradients do not vanish proportional to their distance to the loss and are more evenly distributed.

To summarize, optimizing the parameters of models comprised of discrete distributions and continuous neural network components is challenging, mainly due to local minima and vanishing gradients. By setting the Gumbel scale parameter $\beta=0$ as well as the dropout probability of the residual connections $\alpha=0$ at the beginning of training, we obtain a continuous and deterministic relaxation of the target model. Increasing the \textsc{DropRes} parameter $\alpha$ over time makes the model increasingly use the discrete stochastic nodes. Increasing the Gumbel scale parameter $\beta$ allows the model to escape local minima and small gradients.

\section{Related Work}\label{sec:related}

Various gradient estimators have been proposed for the problem of backpropagating through stochastic nodes. The $\mathrm{REINFORCE}$ algorithm \citep{williams1992simple} utilizes the log derivative trick.
The Straight-Through estimator \citep{bengio2013estimating} back-propagates through hard samples by replacing the threshold function by the identity in the backward pass.
More recent approaches are based on reparameterization tricks \citep{kingma2013auto} that enable the gradient computation by removing the dependence of the density on the input parameters.
\citet{maddison2016concrete} and \citet{jang2016categorical} propose the Gumbel-Softmax trick, a continuous (but biased) reparameterization trick for categorical distributions.
\citet{tucker2017rebar} and \citet{grathwohl2017backpropagation} introduce parameterized control variates to lower the variance  for these gradient estimators. They show that these estimators can, in theory, also be generalized to chains of stochastic nodes but do not discuss any learning behavior.
\citet{shayer2018learning} modify the local reparameterization trick \cite{kingma2015variational} to improve the training of discrete gradient estimators.
Recently, \citet{paulus2020gradient} proposed a framework that generalizes the Gumbel-Softmax trick for various discrete probability distributions.
Our aim is not to improve gradient estimators for a single stochastic node but to analyze the behavior of discrete-continuous computation graphs with multiple sequential discrete components.
\citet{pervez2020low} utilize harmonic analysis for Boolean functions to control the bias and variance for gradient estimates for Boolean latent variable models. We focus on efficiently training models with gradient estimates for categorical distributions.
\citet{huang2016deep} propose to drop out full layers of ResNet to obtain models of stochastic depth during training. In contrast, we use dropout on the residual connections to approximate the associated discrete model.
SparseMAP~\citep{Niculae2018SparseMAPDS} is an approach to structured prediction and latent variables, replacing the exponential distribution (specifically, the softmax) with a sparser distribution.
LP-SparseMAP~\citep{Niculae2020LPSparseMAPDR} is a continuation of SparseMAP using a relaxation of the optimization problem rather than a MAP oracle. In addition, the idea to enforce more sparsity can also be exploited for efficient marginal inference in latent variable models~\citep{correia2020efficient} which can then be used to compute stochastic gradients.


The respective advantages of neural and symbolic methods have led to various attempts to combine these two. Early work like \citet{towell1994knowledge} and \citet{holldobler1999approximating} have explored the possibilities of translating propositional logic programs into neural networks.
\citet{evans2018learning} and \citet{dong2019neural} build upon these ideas by utilizing the framework of inductive logic programming to learn rules from examples.
\citet{kool2019attention} combine an attention model and the $\mathrm{REINFORCE}$ algorithm with a greedy baseline to learn heuristics for combinatorial optimization problems.
A different direction of work focuses on knowledge base reasoning.  \citet{rocktaschel2017end} combine Prolog's backward chaining algorithm with a differentiable unification whereas \citet{yang2017differentiable} learn to compose the inference task into differentiable operations.
\citet{serafini2016logic} propose a differentiable first-order logic language to differentiate deductive reasoning, and \citet{manhaeve2018deepproblog} extend the probabilistic logical language ProbLog to interact with neural networks.
Also related is prior work on automatic program induction. \mbox{\citet{andreas2017neural}} combine neural networks with the compositional linguistic structure of questions, \citet{mao2019neuro} additionally learn visual concepts and \citet{tsamoura2020neural} introduce the concept of abduction to learn the inputs of a symbolic program.
All prior work integrates neural modules into classical or fuzzy logic or separates the neural and the symbolic parts in other ways. In contrast, we propose a method that allows training complex discrete-continuous computation graphs with multiple sequential discrete distributions.
%
\section{Experiments}
%
The aim of the experiments is threefold. First, we want to analyze the behavior of complex stochastic computations graphs arising in typical application domains such as multi-hop reasoning in knowledge graphs and unsupervised parsing of lists of operations. Second, we want to evaluate the behavior of the stochastic computation graphs when incorporating the proposed methods (dropout residual connections and temperature matching) to improve the vanishing gradient problem. Third, we want to compare the resulting discrete-continuous models with state of the art models which do \emph{not} have stochastic components. Here, we are especially interested in analyzing the generalization (extrapolation) behavior of the models under consideration.
The implementations are in PyTorch and can be found at \url{https://github.com/nec-research/dccg}. All experiments were run on a GeForce RTX 2080 Ti GPU.
%
%We compare a model with chained stochastic nodes on the computation graph to a different state-of-the-art gradient estimator for categorical distributions on the latent parse tree task introduced by \citet{nangia2018listops}. In a following ablation study, we display the need of our proposed methods to train these models.
%We further illustrate on a newly created dataset for the path query answering task how incorporating our method into knowledge graph completion models make them more robust towards longer paths. We show how these models improve over state-of-the-art transformer models on the dataset of \citet{guu2015traversing} by changing the priors of the baseline models and without adding any new parameters. Finally, we show how our proposed methods enable a new class of models that achieve all desires of combining discrete and continuous learning while fully acting end-to-end in a differentiable setup.
%
 %For parameters that use annealing, we use an annealing factor of $\gamma=0.008$, e.g., we use $\beta_t = \tau (1 - e^{-t\gamma})$ for the temperature matching. If not mentioned differently, we rise the residual dropout probability $\alpha$ linearly such that it reaches $1$ after half of the training. We change these parameters exactly $10$ times per epoch. Our code and all newly created datasets can be found on GitHub.\footnote{\url{https://github.com/n21-10964/Discrete-Continuous}}
 %
 \begin{table}[t]
 \small
   \caption{The results for unsupervised parsing on ListOps. Results taken from \citet{paulus2020gradient} are marked with an asterisk (*) and are based on a  different (unpublished) dataset.}
   \label{tab:listops}
   \centering
   \begin{tabular}{lccccc}
   \toprule
   & & & & \multicolumn{2}{c}{Task acc. (extrapolation)}  \\
   \cmidrule(r){5-6}
     Model  &Task acc.  &Edge prec.  &Inter. acc.    &$d=8$ &$d=10$\\
     \midrule
     Und.*    &$91.2\pm1.8^*$    &$33.1\pm2.9^*$    &-    &n.a.    &n.a.\\
     Arb.*    &$95.0\pm3.0^*$    &$75.0\pm7.0^*$    &-    &n.a.    &n.a.\\
     \midrule
     LSTM    &$91.5\pm0.3$    &-    &-    &$83.7\pm2.0$    &$76.9\pm3.7$\\
     Arb., $\tau=2$ \hspace{20pt}     &$\mathbf{96.8}\pm0.3$	&$77.4\pm1.8$	&-	&$84.3\pm1.4$	&$75.4\pm1.7$\\
     Ours, $\tau=1$    &$96.1\pm0.4$	&$\mathbf{82.3}\pm1.1$	&$\mathbf{70.9}\pm1.2$	&$\mathbf{92.6}\pm1.1$	&$\mathbf{86.9}\pm4.9$\\
     Ours, $\tau=2$   &$\mathbf{96.3}\pm0.5$	&$76.8\pm2.2$	&$62.9\pm2.4$	&$\mathbf{92.7}\pm1.3$	&$\mathbf{88.7}\pm3.3$	\\
     \midrule
     Arb. (GT) &$98.7\pm0.1$    &$100.0$    &-   &$86.4\pm1.1$    &$71.0\pm2.1$\\
     Ours (GT)  &$99.8\pm0.1$	&$100.0$	&$99.9\pm0.0$	&$99.8\pm0.1$	&$99.9\pm0.1$\\
   \bottomrule
   \end{tabular}
   \caption{The ablation study for the ListOps task. Temperature matching improves the training behavior of the model. The addition of dropout residual connections is  crucial for efficient learning.
   %Using the standard settings for softmax tricks (bottom row) leads to poor results.
   }
   \label{tab:listops_ablation}
   \centering
   \begin{tabular}{llllll}
   \toprule
   & & & & \multicolumn{2}{c}{Task acc. (extrapolation)}  \\
   \cmidrule(r){5-6}
     Model  &Task acc.  &Edge prec.  &Inter. acc.    &\multicolumn{1}{c}{$d=8$} &\multicolumn{1}{c}{$d=10$}\\
     \midrule
     Ours, $\tau=1$ \hspace{20pt}   &$96.1\pm0.4$	&$82.3\pm1.1$	&$70.9\pm1.2$	&$92.6\pm1.1$	&$86.9\pm4.9$\\
     (-) TM           &$95.1\pm1.7$	&$76.9\pm12.1$	&$66.3\pm9.2$	&$90.5\pm2.4$	&$83.7\pm4.0$	\\
     (-) DR           &$74.8\pm11.5$	&$43.9\pm30.3$	&$29.4\pm18.6$	&$70.5\pm9.3$	&$66.7\pm8.8$\\
     (-) DR, TM       &$56.2\pm5.0$	&$15.1\pm7.7$	&$12.7\pm3.2$	&$53.1\pm6.6$	&$49.2\pm7.1$ \\
   \bottomrule
   \end{tabular}
 \end{table}
 %
\paragraph{Unsupervised Parsing on ListOps}
%We investigate how our method compares to using stochastic softmax tricks as introduced by \citet{paulus2020gradient} for estimating gradients on the simplified variant of the ListOps dataset (\citet{nangia2018listops}). %
The Listops dataset contains sequences in prefix arithmetic syntax such as $\mathrm{max[\ 2\ 9\ min[\ 4\ 7\ ]\ 0\ ]}$ and its unique numerical solutions (here: $9$)~\citep{nangia2018listops}. Prior work adapted and used this dataset to evaluate the performance of stochastic softmax tricks~\citep{paulus2020gradient}. Following this prior work, we first encode the sequence into a directed acyclic graph and then run a graph neural network (GNN) on that graph to compute the solution.
Since the resulting dataset was not published, we generated a dataset following their description. In addition to examples of depth $1 \leq d \leq 5$, we further generated test examples of depth $d=8,10$ for the extrapolation experiments. More details on the dataset creation can be found in the supplementary material.
%More precisely, we used the three operators $\mathrm{min}$, $\mathrm{max}$ and $\mathrm{med}$ and capped the maximum length of a sequence at $50$. For each depth $d\in\{1,2,3,4,5\}$ we generated $20,000$ samples for the training set and $2,000$ samples each for the validation set and test set. In order to evaluate the generalization behavior, we further generated $2,000$ test samples for each depth $d\in\{8,10\}$. All other settings are those of the original code~\citep{nangia2018listops}.

The arithmetic syntax of each training example induces a directed rooted in-tree, from now on called arborescence, which is the tree along which the message-passing steps of a GNN operate.  %For instance, in the example mentioned above, there is an edge from each token $\mathrm{2,\ 9,\ min[,\ 0,}$ and the final $\mathrm{]}$ directed to $\mathrm{max[}$ and so on.
We use the same bi-LSTM encoder as \citet{paulus2020gradient} to compute the logits of all possible edges and the same directed GNN architecture. The original model uses a directed version of Kirchoff’s matrix-tree theorem as introduced by \citet{koo2007structured} to induce the arborescence prior. We simplify this idea by taking, for each node, the Gumbel-softmax over all possible parent nodes. Here we make use of the fact that in an arborescece, each non-root node
%(with the exception of the root)
has exactly one parent. Note that this prior is slightly less strict than the arborescence prior.
%since, in theory, it also allows cycles.
As in \citet{paulus2020gradient}, we exclude edges from the final closing bracket~$\mathrm{]}$ since its edge assignment cannot be learned from the task objective.

In contrast to \citet{paulus2020gradient}, where only the edges of the latent graph are modeled with categorical variables, we also modelled the nodes of the latent graphs with discrete-continuous components (see Figure~\ref{fig:examples-dc} (bottom)). Let $\operatorname{Num}\in\mathbb{R}^{10\times\dim}$ be the embedding layer that maps the $10$ numeral tokens to their respective embeddings and let
$\operatorname{Pred}:\mathbb{R}^{\dim}\rightarrow\mathbb{R}^{10}$
be the classification layer that maps the embedding $\bx\in\mathbb{R}^{\dim}$ of the final output to the logits of the $10$ classes: \mbox{$\operatorname{Pred}(\bx)\coloneqq\lin^{10\times\dim}(\relu(\lin_B^{\dim\times\dim}(\bx)))$.}
After each message-passing operation of the GNN, we obtain an embedding $\bu\in\mathbb{R}^{\dim}$ for each node. By choosing $g_{\bw}\coloneqq\operatorname{Pred}$, we obtain the $10$ class logits $\btheta=g_{\bw}(\bu)=\operatorname{Pred}(\bu)\in\mathbb{R}^{10}$ for the numerals  $0,\dots,9$. Using the Gumbel-softmax trick with logits $\btheta$, scale $\beta$, and temperature $\tau$, we obtain $\bz\in\mathbb{R}^{10}$. We then compute $\bv=h_{\bw'}(\bz)= \bz^{\intercal}\operatorname{Num}$. We apply this on all intermediate node embeddings simultaneously and repeat this after each of the first $4$ out of $5$ message-passing rounds of the GNN. We use dropout residual connection with increasing dropout probability $\alpha$.
%In total, we obtain a model with $5$ sets of discrete operations: one set from the graph generation and $4$ from the discrete-continuous message-passing steps.
At test time, the model performs discrete and interpretable operations. Figure~\ref{fig:examples-dc} depicts an example stochastic computation graph for the task.
We used the same LSTM as in \citet{paulus2020gradient} and a re-implemented version of their model with our customized prior. For ablation purposes, we run all our models with and without dropout residuals and temperature matching, respectively.
%In the models that use discretization, we slightly modify the pre-message from node $j$ to node $i$ of the GNN's message-passing from $[x_i,x_j]$ to $[x_{\text{emb}},x_j]$. Since all nodes become intermediate results by construction after the first discretization step, operation nodes would lose the information about their operators otherwise. We also tested this modification on the baselines without discretization but noticed a decline in performance.
All models are run for $100$ epochs with a learning rate of $0.005$ and we select  $\tau\in\{1,2,4\}$. We keep all other hyperparameters as in \citet{paulus2020gradient}. We evaluate the task accuracy, the edge precision (using the correct graphs), the task accuracy on intermediate operations, and two extrapolation tasks. For the evaluation of the extrapolation task with depth $d$, we run the GNN $d$ rounds instead of $5$.

Table~\ref{tab:listops} compares the proposed models with state of the art approaches.  %Table~\ref{tab:listops_ablation} lists the results of the ablation experiments.
The best performing models are those with discrete-continuous components trained with dropout residuals and temperature matching. While our models do not improve the task accuracy itself, they exceed state of the art methods on all other metrics. Most noticeable is the improved generalization behavior: our models extrapolate much better to expressions with a depth not seen during training.
This becomes even more visible when trained on the ground truth graph directly (instead of on the sequence) where the generalization accuracy is close to $100\%$.
%Furthermore, our model is the only one that can read out the intermediate results which enables a form of interpretability. The accuracy of the intermediate results of all operations but the first together is over $70\%$ although the model has not once received a direct supervision signal on these predictions.
Our model also achieves the best performance on the graph structure recovery with a precision of $82.3\%$.
The mismatch between the task accuracy and the edge precision is mainly due to examples for which the correct latent graph is not required to solve the problem, e.g., for the task $\mathrm{max[\ 2\ max[\ 4\ 7\ ]]}$ it does not matter whether the $7$ points to the first or the second $\mathrm{max[}$. Note that our method performs discrete operations at test time which are therefore verifiable and more interpretable.
Our model generates intermediate representations that can be evaluated in exactly the same way as the final outputs, see Column~\textit{Inter. acc.} in Table~\ref{tab:listops}.

The ablation study in Table~\ref{tab:listops_ablation} highlights the challenge of learning discrete-continuous computation graphs with multiple sequential Gumbel-softmax components. It is entirely impossible to train the model without dropout residuals and temperature matching.
The analysis in Section~\ref{sec:efficient} Figure~\ref{fig:gnn_grad} reveal the reason.
Sequential discrete distributions in the computation graph cause vanishing gradients. Temperature matching stabilizes learning somewhat but to avoid vanishing gradients entirely, the use of dropout residuals is necessary.
%Figure~\ref{fig:residual_schema} compares the dropout residual connections with residual connections implemented through a weighted sum and demonstrates their superior performance.

\begin{table}[t]
\small
    \begin{minipage}{.47\textwidth}
    \centering
      \captionof{table}{Results for the path query benchmark~\citep{guu2015traversing}.}% Our model improves over $3$ out of $4$ benchmarks.}
  \label{tb:kg_paths}
    \begin{tabular}{lcccc}
    \toprule
    & \multicolumn{2}{c}{WordNet} & \multicolumn{2}{c}{Freebase} \\
    \cmidrule(r){2-5}
    Model         & MQ   & H@10 & MQ   & H@10 \\
    \midrule
    \vspace{4pt}
    Bilinear-C & $89.4$ & $54.3$ & $83.5$ & $42.1$ \\
    \vspace{4pt}
    DistMult-C & $90.4$ & $31.1$ & $84.8$ & $38.6$ \\
    \vspace{4pt}
    TransE-C   & $93.3$ & $43.5$ & $88.0$ & $50.5$ \\
    \vspace{4pt}
    Path-RNN      & $\mathbf{98.9}$ & –    & –    & –    \\
    \vspace{4pt}
    ROP           & –    & –    & $90.7$ & $56.7$ \\
    CoKE          & $94.2$ & $\mathbf{67.9}$ & $\mathbf{95.0}$ & $\mathbf{77.7}$ \\
    %\midrule
    %ComplEx-COMP  & $92.8$ & $64.9$ & $95.4$ &$82.8$ \\
    \midrule
    \vspace{1pt}
    %DR soft $\lambda=1, \tau=0\rightarrow1$   & $92.9$ & $63.1$ & - & -  \\
    %DR soft $\lambda=2, \tau=0\rightarrow2$   & $93.9$ & $67.0$ & - & -  \\
    %R soft $\lambda=4, \tau=0\rightarrow4$   & $93.6$ & $68.1$ & - & -  \\
    %DR hard $\lambda=1, \tau=0\rightarrow1$   & $67.8$ & $14.7$ & - & -  \\
    %DR hard $\lambda=2, \tau=0\rightarrow2$   & $93.9$ & $67.8$ & - & -  \\
    Ours, $\tau=4$   & $94.4$ & $64.3$ & $89.6$ & $68.7$  \\
    % Ours, $\tau=4$   & $93.9$ & $\mathbf{68.2}$ & $\mathbf{97.9}$ & $\mathbf{89.1}$  \\
    \bottomrule
%    \vspace{10pt}
  \end{tabular}
    \end{minipage}
    \hfill
    \begin{minipage}{.51\textwidth}
    \captionof{table}{Results for the extrapolation benchmark~\citep{wang2019coke} for  the path query dataset~\citep{guu2015traversing}.% Our model achieves best results on almost all setups. %The advantage of discretizing is most visible when trained on short paths.
    }
  \label{tb:kg_extrapolation}
      \centering
    \begin{tabular}{llcccc}
    \toprule
          \small
    & & \multicolumn{2}{c}{WordNet} & \multicolumn{2}{c}{Freebase} \\
    \cmidrule(r){3-6}
    Paths    & Model     & MQ   & H@10 & MQ   & H@10 \\
    \midrule
    \multirow{2}{*}{$\leq4$}
    & CoKE         & $93.6$ & $\mathbf{65.5}$ & $\mathbf{93.1}$ & $\mathbf{71.2}$ \\
    %& ComplEx-COMP  & tba & tba & tba & tba \\
    & Ours  & $\mathbf{94.3}$ & $64.7$ & $88.8$ & $69.4$ \\
    \midrule
    \multirow{2}{*}{$\leq3$}
    & CoKE         & $92.6$ & $\mathbf{65.0}$ & $\mathbf{90.6}$ & $64.6$ \\
    %& ComplEx-COMP  & tba & tba & tba & tba \\
    & Ours  & $\mathbf{93.4}$ & $\mathbf{65.0}$ & $88.3$ & $\mathbf{68.5}$ \\
    \midrule
    \multirow{2}{*}{$\leq2$}
    & CoKE         & $\mathbf{90.8}$ & $49.5$ & $\mathbf{89.4}$ & $59.5$ \\
    %& ComplEx-COMP  & tba & tba & tba & tba \\
    & Ours  & $90.2$ & $\mathbf{62.0}$ & $87.6$ & $\mathbf{68.6}$ \\
    \midrule
    \multirow{2}{*}{$\leq1$}
    & CoKE         & $73.5$ & $16.7$ & $72.7$ & $37.3$ \\
    %& ComplEx-COMP  & tba & tba & tba & tba \\
    & Ours  & $\mathbf{81.1}$ & $\mathbf{52.2}$ & $\mathbf{82.1}$ & $\mathbf{63.9}$ \\
    \bottomrule
  \end{tabular}
\end{minipage}
\end{table}


\paragraph{Multi-Hop Reasoning over Knowledge Graphs}


Here we consider the problem of answering multi-hop (path) queries in knowledge graphs (KGs)~\citep{guu2015traversing}. The objective is to find the correct object given the subject and a sequence of relations (the path) without knowing the intermediate entities. 
We evaluate various approaches on the standard benchmarks for path queries~\citep{guu2015traversing}.\footnote{Revised results in Table~\ref{tb:kg_paths} and Table~\ref{tb:kg_extrapolation} because of a flawed evlauation protocal in the original results.}
%Further experiments on a newly created dataset based on FB15K-237 can be found in the Appendix.

We model each application of a relation type to an (intermediate) entity as a discrete-continuous component that discretely maps to one of the KG entities. The discrete distribution $p(\bz; \btheta)$, therefore, has as many categories as there are entities in the KG.
%Without loss of generality,
Let $\operatorname{score}(\bs,\br, \bo): \mathbb{R}^{3\times\dim}\rightarrow \mathbb{R}$ be a scoring function that maps the embeddings of a triple to the logit (unnormalized probability) of the triple being true. Given the current (intermediate) entity embedding $\bs$ and relation type embedding $\br$ we compute the logits $\btheta$ for all possible entities. Hence, the function $g$ computes the logits for all possible entities. Using the Gumbel-softmax trick with parameter $\btheta$, scale $\beta$, and temperature $\tau$, we obtain the sample $\bz\in\mathbb{R}^n$. The function $h$ now computes $\bz^{\intercal}\mathbf{E}$ where $\mathbf{E}$ be the matrix whose rows are the entity embeddings.
We use dropout residual connection between the input and output vectors of the discrete-continuous component during training. Note that we do not increase the number of parameters compared to the base scoring models.
We use ComplEx \citep{trouillon2016complex} as the scoring function.
%We use the hyperparameters of ComplEx as proposed by \citet{ruffinelli2020you}, i.e.,
%We choose a dimension and batch size of~$512$, a learning rate of $0.001$, and an L3 regularization of $0.01$. We further train $1$vsAll with the cross-entropy loss for $300$ epochs and repeat the experiment for each temperature~$\tau\in\{1,2,4\}$.
We choose a dimension of~$256$, a batch size of~$512$ and learning rate of $0.001$. We further train $1$vsAll with the cross-entropy loss for $200$ epochs and with a temperature of $\tau=4$.
%For these experiments, we use the Gumbel-hard (Gumbel-softmax plus Straight-Through \citep{bengio2013estimating}) setting also during training.
We compare our best performing model on the original dataset from \citet{guu2015traversing} with their proposed evaluation protocol and baselines against RNN models, Path-RNN~\citep{das-etal-2017-chains}, ROP~\citep{yin2018recurrent} and the state-of-the-art transformer model CoKE~\citep{wang2019coke} for the standard task as well as in an extrapolation setting.

Table~\ref{tb:kg_paths} lists the results for the proposed model in comparison to several baselines.
%Our proposed model performs significantly better than other models: we obtain new state of the art results (up to $15\%$ better) on all benchmarks except for the metric MQ on WordNet.
Our proposed model performs significantly better than the baselines KGC models which do not have stochastic components (labeled with -C). On WordNet, our model can even keep pace with the state of the art transformer model CoKE.

% other models: we obtain new state of the art results (up to $15\%$ better) on all benchmarks except for the metric MQ on WordNet.}
Similar to the results on ListOps, the discrete-continuous models have the strongest generalization performance (see Table~\ref{tb:kg_extrapolation}).
Here, we use the same test set as before but reduce the length of the paths seen during training.
%Even when trained only on triples, that is, paths of length $\leq1$,
Even when trained only on paths of length $\leq3$,
our model performs better on the path query task than most baselines trained on paths of all lengths.
%Please note that these performance boosts are based solely on the use of discrete stochastic components while using the exact same standard scoring functions.




\paragraph{End-to-End Learning of MNIST Addition}




\begin{table}
\small
  \caption{A comparison of our proposed method with DeepProbLog and a CNN baseline.}
  \label{tb:mnist_addition}
  \centering
  \begin{tabular}{lcccc}
    \toprule
    %& & \multicolumn{3}{c}{Properties} \\
    %\cmidrule(r){3-5}
    Model        & Task Acc.   & Interpretable & Discrete (test time) & Learned Structure  \\
    \midrule
    Baseline         & $89.14\pm1.22$ & No & No & No \\
    DeepProbLog      & $98.17\pm0.20$ & Yes & Yes & No \\
    Ours, $\tau=8$   & $98.10\pm0.15$ & Yes & Yes & Yes \\

    \bottomrule
  \end{tabular}
\end{table}

The MNIST addition task addresses the learning problem of simultaneously (i) recognizing digits from images and (ii) performing the addition operation on the digit's numerical values~\citep{manhaeve2018deepproblog}.
The dataset was introduced with DeepProbLog, a system that combines neural elements with a probabilistic logic programming language. In DeepProbLog the program that adds two numbers is provided. We adopt their approach of using a convolutional neural network (CNN) to encode the MNIST images. Contrary to their approach, we learn the operation without any prior knowledge about adding numbers in a data-driven manner.

The addition operation is modeled as  $\operatorname{Add}(\bx,\by)=\lin^{\dim\times2\dim}_B(\relu(\lin^{2\dim\times2\dim}_B([\bx;\by])))$
and we use a single linear layer to compute the logits.
Given $\bu\in\mathbb{R}^{\dim}$, the encoding of the digit obained from the CNN, the function $g$ computes $\btheta = \lin(\bu)$, that is, the $19$ logits for all possible numbers. Using the Gumbel-softmax trick with parameter $\btheta$, scale $\beta$, and temperature $\tau$, we obtain $\bz\in\mathbb{R}^{19}$. The function $h$ now computes
$\bv=\bz^{\intercal}\operatorname{P}\in\mathbb{R}^{\dim}$ where $\operatorname{P}$ is the weight matrix of $\lin$. We discretize the two outputs of the CNN, execute the addition layer and use the single linear layer $\operatorname{P}$ for the classification. This results into a model that has in total fewer parameters than the baseline in~\citep{manhaeve2018deepproblog}.
%For the experiments with $3$ summands, see Figure~\ref{fig:examples-dc} and Figure~\ref{fig:annealing_schemas}, we use the same model with $2$ separate addition layers and choose $\operatorname{P}\in\mathbb{R}^{28\times\dim}$. Here, we discretize after each CNN encoding as well as after the first addition.
We use a learning rate of $0.0001$ and a temperature of $\tau=8$.
%
% Instead of using a batchsize of $1$ as DeepProbLog or $2$ as the baseline, we use a batchsize of $16$ and run the training for $30$ epochs instead of $1$ epoch.

Table~\ref{tb:mnist_addition} lists the results on this task. Our proposed model, even though trained without a given logic program and in a data-driven manner, achieves the same accuracy as DeepProbLog.  Since we use hard sampling during test time, we can discretely read out the outputs of the internal CNN layer.
%Furthermore, the learned operation is modular and likely able to extrapolate to multiple sequential addition operations.
%
\section{Conclusion}
%
We introduced two new strategies to overcome the challenges of learning complex discrete-continuous computation graphs. Such graphs can be effective links between neural and symbolic methods but suffer from vanishing gradients and poor local minima. We propose dropout residual connections and a new temperature matching schema for mitigating these problems. Note that even with our proposed methods, learning models with discrete nodes is still considerably more challenging than standard neural networks.
Furthermore, it is not clear how to use dropout residuals for structural stochastic nodes or in such setups in which only hard decisions are feasible.
In experiments across a diverse set of domains, we demonstrate that our methods enable the training of complex discrete-continuous models, which could not be learned before. These models beat prior state of the art models without discrete components on several benchmarks and show a remarkable gain in generalization behavior. Promising future work includes applying the proposed methods to reinforcement learning problems, effectively sidestepping the problem of using the high variance score function estimator.
%
%Future work will mainly concentrate on the automatic assembling of complex computation graphs. The combination of the assembling and the execution of such graphs will enable a fully new class of models. Even with our proposed methods, learning models with discrete nodes is considerably more challenging than models without. A different line of future work will improve on our methods to train these systems even more efficiently. Further work will be based on the application of these models; integrating discrete nodes into computation graphs of existing as well as new models will yield more interpretability as well as robustness of these models.
%
%Note that even with our proposed methods, learning models with discrete nodes is considerably more challenging than models without.
%
%\clearpage
\section{Acknowledgements}
Many thanks to the NeurIPS reviewers for their highly constructive feedback and suggestions. Their input has helped to improve the paper tremendously. Special thanks to reviewer \textsc{RKc5} who provided an impressive review and engaged in a long and fruitful discussion with us.







\bibliographystyle{abbrvnat}
%\begin{small}
\bibliography{neurips_2021}
%\end{small}






















\clearpage


\section{Appendix}



\subsection{Implementation Details}
All our methods were implemented with PyTorch and were run on a GeForce RTX 2080 Ti GPU. For the experiments we picked the best performing learning rate out of $\{1e^{-4}, 3e^{-4}, 5e^{-4}, 1e^{-3}\}$ and the best performing softmax temperature $\tau$ out of $\{1, 2, 4\}$. We found a temperature of $\tau=4$ not sufficiently high enough for the MNIST addition experiments and picked $\tau=8$ for these experiments.
%Our implementations can be found in the supplementary material on OpenReview, and further datasets can be found on DropBox.\footnote{\url{https://www.dropbox.com/s/fsuq673z6uay129/data.tar.gz?dl=0}}

\subsubsection{Unsupervised Parsing on ListOps}
For all ListOps experiments, we use a hidden dimension $\dim=60$, a batch size of $100$, and train the model for a total number of $100$ epochs. We use the Adam optimizer with a learning rate of $0.0005$ to minimize the cross-entropy loss. After every epoch, we evaluate on the validation set and save the best model. We use discretization after the first $4$ out of the $5$ GNN message passes. We always use an exponential function to increase $\beta$ and a linear function to increase $\alpha$ (during experiments for which they are not constant). The temperatures are updated $10$ times per epoch; we use $\beta_t = \tau (1 - e^{-t\gamma})$  for $\gamma=0.008,\ \tau=1$ and $\alpha_t = \max(1, 0.002t)$. We repeat every experiment $8$ times.

Our model is taken from \citet{paulus2020gradient} and has following structure. For the encoder, we start with an embedding layer for the $14$ tokens. We use two independent one-directional LSTMs with a single layer, respectively. We use a token-wise multiplication of the two sequence outputs to obtain a latent graph and use Gumbel-softmax on this latent graph:
\begin{align*}
    x &= E_1[14, \dim](tok) \\
    q &= LSTM_1(x) \\
    k &= LSTM_2(x) \\
    A'_{ij} &= qk^{\intercal} \\
    A_{ij} &= \frac{1}{\lambda}(A'_{ij} + \GumbelDist(0, \tau)).
\end{align*}
To obtain the edge precision, we compare the latent graph $A_{ij}$ with the ground truth adjacency matrix $B_{ij}$. Due to our arborescence prior as described in the main paper, we always have the same number of edges resulting in equality of edge precision and edge recall. For the GNN we use a second, independent embedding layer for the tokens. For each GNN message pass, we use the latent representation after the embedding layer of a token and concatenate it with the representation of the node of the incoming message. In the baseline experiments (Arb.), we use the current state of the token instead of the first embedding during all message passes except the first one. The message is transformed by a message MLP with dropout probability $0.1$ and is summed up regarding the edge weights of the latent graph~$A^{ij}$. The new node state is summed with the one before the message pass:
\begin{align*}
    x_e &= E_2[14, \dim](tok) \\
    p'' &= [x_e; x_j]\quad\text{or}\quad[x_i; x_j] \\
    p'  &= \operatorname{Dropout}(\relu(\lin_{1,Bias}^{\dim\times2\dim}(p''))) \\
    p   &= \relu(\lin_{2,Bias}^{\dim\times\dim}(p')) \\
    m    &= A_{i}p \\
    x'_i &= x_i + m.
\end{align*}
In those experiments that utilize the ground truth edges (GT) instead of the latent graph, we replace the second last line $m = A_{i}p$ by $m = B_{i}p$ with $B$ being the ground truth adjacency matrix. To create Figure~\ref{fig:gnn_grad}, we read out the mean absolute values of the gradient at $x_e$ as well as $x'_i$ for each of the first $4$ GNN massage passes. For the discretization, we use the classification layer that maps the embedding of the final output to the logits of the $10$ classes and the weights of the embedding layer that map the $10$ numeral tokens to their respective embeddings:
\begin{align*}
    \btheta &= \lin_4^{10\times\dim}(\relu(\lin_{3,Bias}^{\dim\times\dim}(x'_i))) \\
    \bz &=  \frac{1}{\lambda}(\btheta + \GumbelDist(0, \tau)) \\
    \bv &= \bz^{\intercal}E_2([0,\dots,9]) \\
    x_i^{\text{next}} &=
    \begin{cases}
        \bv &\text{ with probability } \alpha, \\
        x'_i + \bv &\text{ else}.
    \end{cases}
\end{align*}
To read out the intermediate results, we compare the final (after the last GNN message pass) $\btheta$ of all operation tokens with the ground truth intermediate results obtained by executing the operations. The experiments without dropout residuals are always set to utilize $\alpha=1$.
%For the weighted sum experiment in Figure~\ref{fig:residual_schema}, we replace the last line by $x_i^{\text{next}} = \alpha \bv + (1-\alpha) x'_i$.
To obtain the task class of the full example, we utilize the same MLP from the discretization as the classification layer, i.e., $\lin_4^{10\times\dim}(\relu(\lin_{3,Bias}^{\dim\times\dim}(x^{\text{last}}_0)))$.\\
The baseline LSTM model consists of an one-directional LSTM with a single layer. The hidden state of the final token is fed to a classification MLP, i.e., $\lin^{10\times\dim}(\relu(\lin_{Bias}^{\dim\times\dim}(h_{-1})))$.




\subsubsection{Multi-Hop Reasoning over Knowledge Graphs}
For all Knowledge Graph experiments, we use a hidden dimension $\dim=256$, a batch size of $512$, and train the model for a total of $200$ epochs. We use the Adam optimizer with a learning rate of $0.001$ to minimize the cross-entropy loss.
We use a randomized grid search training on paths of length $1$ and validating hitset $10$ on paths of length $2$ for the L$2$ regularization of the entities and the relations between $1e^{-20}, ..., 1e^{-5}$ and for the dropout probabilities for the subject, object and relations between $0, ..., 0.8$, respectively. This results in an entity regularization of $1e^{-15}$, a relation regularization of $1e^{-9}$, a subject dropout of $0.7\ [0.1]$, an object dropout of $0.1\ [0.6]$ and a relation dropout of $0.5\ [0.2]$ for WordNet [Freebase].
%We initialize our embeddings with the Glorot initialization and use a L$3$ regularization of $0.01$ for the entities and the relations.
After every $10$th epoch, we evaluate on the validation set and save the best model. We use discretization after every single relation. We always use an exponential function to increase $\beta$ and a linear function to increase $\alpha$ (during experiments for which they are not constant). The temperatures are updated $3$ times per epoch; we use $\beta_t = \tau (1 - e^{-t\gamma})$  for $\gamma=0.008,\ \tau=1$ and $\alpha_t = \max(1, 0.005t)$. We repeat every experiment $4$ times.

Our model is based on the ComplEx model from \citet{trouillon2016complex}. Given the current (intermediate) entity embedding $\bs$ and relation type embedding $\br$ we compute the logits $\btheta$ for all possible entities with the ComplEx scoring function $\operatorname{score}=\operatorname{Re}<\bs,\br,\cdot>$. Hence, we obtain the logits for all possible entities. Using the Gumbel-softmax trick with parameter $\btheta$, scale $\beta$, and temperature $\tau$, we obtain the sample $\bz\in\mathbb{R}^n$. The function $h$ now computes $\bz^{\intercal}\mathbf{E}$ where $\mathbf{E}$ be the matrix whose rows are the entity embeddings.
We use dropout residual connections between the input and output vectors of the discrete-continuous component during training.
\citet{guu2015traversing} introduced their own evaluation protocol for multi-hop reasoning that we adopted. On the one hand, we calculate all possible objects that can be reached traversing each path. These are the positives and are filtered during evaluation. On the other hand, we calculate all possible objects that can be reached by the final relation of the path individually. These are the negatives that we rank our prediction against. We compare our best performing model against two RNN models, Path-RNN \citep{das-etal-2017-chains} and ROP \citep{yin2018recurrent} as well as against the state-of-the-art transformer model CoKE \citep{wang2019coke}.

We use a slightly different setup for the FB15K237 experiments in Section~\ref{sec:fb237}. Here, we train the same model for a total number of $100$ epochs and for each path from subject to object as well as from object to subject using reciprocal relations \cite{ruffinelli2020you}. The evaluation is copied from the standard link prediction task \cite{bordes2013translating}, that is, we evaluate all paths in the forward direction from subject to object as well as in the backward direction from object to subject. We also use the filter method for the positives, but we compare against all other possible objects and not only the ones reachable by the last relation.


\subsubsection{End-to-End Learning of MNIST Addition}
For the MNIST addition experiments, we use a batch size of $16$ and train the model for a total number of $30$ epochs. We use the Adam optimizer with a learning rate of $0.0001$ to minimize the cross-entropy loss. We use the dataset from \citet{manhaeve2018deepproblog}. Since they do not offer a validation set, we validate the model twice per epoch on the test set and record the best test accuracy for all models. We use discretization after the CNN encoding layer, i.e., before the addition layer. The temperatures are updated $8$ times per epoch; we increase $\beta$ and $\alpha$ by $\beta_t = \tau (1 - e^{-t\gamma})$  for $\gamma=0.008, \tau=8$ and $\alpha_t = \max(1, 0.002t)$. We repeat every experiment $8$ times.

Our model is based on the baseline model used by \citet{manhaeve2018deepproblog} and has the following structure. The CNN encoder consists of two convolutional layers with kernel size $5$ and filter size $6$ and $16$, respectively. Each convolutional layer is followed by $2$D max-pooling layer of size $2\times2$ and a $\relu$ activation function. An MLP with $3$ layers transforms the output to an embedding size of $84$.
\begin{align*}
    e' &= \relu(\operatorname{maxpool}_{2\times2}(\operatorname{Conv2d}_{6,5}(inp))) \\
    e  &= \relu(\operatorname{maxpool}_{2\times2}(\operatorname{Conv2d}_{16,5}(e'))) \\
    x' &= \lin_{Bias}^{84\times84}(\relu(\lin_{Bias}^{84\times120}(\relu(\lin_{Bias}^{120\times256}(e))))).
\end{align*}
For the discretization we use a single classification matrix $C\in\mathbb{R}^{19\times84}$. This matrix is also used for the classification after the addition layer.
\begin{align*}
    \btheta &= Cx' \\
    \bz &=  \frac{1}{\lambda}(\btheta + \GumbelDist(0, \tau)) \\
    \bv &= \bz^{\intercal}C \\
    x &=
    \begin{cases}
        \bv &\text{ with probability } \alpha, \\
        x' + \bv &\text{ else}.
    \end{cases}
\end{align*}
The addition layer concatenates the final embeddings of both images and transform them through a MLP with $2$ layers into a single representation.
\begin{align*}
    x_{1,2} = \relu(\lin_{Bias}^{84\times168}(\relu(\lin_{Bias}^{168\times168}([x_1;x_2])))).
\end{align*}
We obtain the final class log-probabilities by computing $Cx_{1,2}$. This results in a model with a total of $94,900$ parameters. We use the code offered by \citet{manhaeve2018deepproblog} to run the DeepProbLog experiments as well as the baseline model they compared to. These two models are executed in a single epoch and use a batch size of $1$ and $2$, respectively. The DeepProbLog model uses a similar encoder to get predictions for each of the images individually and has the following structure.
\begin{align*}
    e' &= \relu(\operatorname{maxpool}_{2\times2}(\operatorname{Conv2d}_{6,5}(inp))) \\
    e  &= \relu(\operatorname{maxpool}_{2\times2}(\operatorname{Conv2d}_{16,5}(e'))) \\
    out  &= \lin^{10\times120}(\relu(\lin_{Bias}^{120\times256}(e))),
\end{align*}
The $2$ image outputs are then fed into the probabilistic logic program ProbLog together with the following annotated disjunction, which handles the logic of addition to obtain the final predictions.
\begin{align*}
\operatorname{nn}(\operatorname{mnist\_net},[X],Y,[0,1,2,3,4,5,6,7,8,9]) :: \operatorname{digit}(X,Y);\\
\operatorname{add}(X,Y,Z) :- \operatorname{digit}(X,X2), \operatorname{digit}(Y,Y2), Z \text{ is } X2+Y2.
\end{align*}
Different to DeepProbLog or our model, the baseline model concatenates the images beforehand and uses the following layers.
\begin{align*}
    e' &= \relu(\operatorname{maxpool}_{2\times2}(\operatorname{Conv2d}_{6,5}(inp))) \\
    e  &= \relu(\operatorname{maxpool}_{2\times2}(\operatorname{Conv2d}_{16,5}(e'))) \\
    out  &= \lin^{19\times84}(\relu(\lin_{Bias}^{84\times120}(\relu(\lin_{Bias}^{120\times704}(e))))),
\end{align*}
which results in a model with a total of $98,951$ parameters.

For the experiments in Figure~\ref{fig:poor-minima-acc}, we also use a batch size of $16$, set $\tau=8.0$, use a learning rate of $0.0003$ and train the model for a total number of $30$ epochs. If we update temperatures, we also update them $8$ times per epoch. We do not use dropout residuals. For the experiment Base, we use a constant $\tau=8.0$, $\beta=1.0$ and $\gamma=0.008$. For the experiment TauAnn, we set $\beta=1.0$ constant and anneal $\tau$ by $\tau_t = \max(1.0,\ \tau e^{-t\gamma})$. For the experiment TM, we set $\tau=8.0$ constant and increase $\beta$ by $\beta_t = \tau (1 - e^{-t\gamma})$.






\subsubsection{Temperature Matching}
To illustrate the effect of the parameter $\beta$, we conducted two toy experiments. The first one is depicted in Figure~\ref{fig:simplex}. We use the same setup as in \citet{maddison2016concrete}. We illustrate the Gumbel-softmax densities of the unnormalized probabilities $\theta = (2, 0.5, 1)$. The second toy experiment is depicted in Figure~\ref{fig:temp-matching-example}. The model here  computes Softmax$(\btheta + \bm{\epsilon})$ with parameters $\btheta^{\intercal}=(\theta_1,\theta_2)$ and $\bm{\epsilon} \sim \GumbelDist(0, \beta)$. The learning problem is defined through a cross-entropy loss between the output probabilities of the Gumbel-Softmax and a constant target vector $(1.0, 0.0)^{\intercal}$. We use the initial paramaters $\btheta^{\intercal}=(-0.9442,  0.3893)$ and minimize the loss using stochastic gradient descent with a learning rate of $0.01$ for a total number of $5000$ steps.

\subsection{Datasets}
For the \textbf{Unsupervised Parsing on ListOps} experiment, we followed the data generation description from \citet{paulus2020gradient} applied to the original code from \citet{nangia2018listops}.
More precisely, we used the three operators $\mathrm{min}$, $\mathrm{max}$ and $\mathrm{med}$ and capped the maximum length of a sequence at $50$. For each depth $d\in\{1,2,3,4,5\}$ we generated $20,000$ samples for the training set and $2,000$ samples each for the validation set and test set. In order to evaluate the generalization behavior, we further generated $2,000$ test samples for each depth $d\in\{8,10\}$. All other settings are those of the original code~\citep{nangia2018listops}.


\subsection{Derivation of Equation~\ref{eq:gradient}}
For all $i$, we have $z_i \in [0,1]$ and $\sum_i z_i = 1$. Therefore, we have $\sum_{i\neq j} z_i^2 \leq 1$ and $(1 - z_j)^2 \leq 1$~and thus,
%
\begin{align*}
     \left\Vert \left(\frac{\partial \bz}{\partial \theta_j}\right) \right\Vert_F
    &= \left\Vert \left(\frac{\partial z_1}{\partial \theta_j} \dots \frac{\partial z_n}{\partial \theta_j} \right)^{\intercal} \right\Vert_F \\
    &= \sqrt{\sum_i \left\vert \frac{\partial z_i}{\partial \theta_j} \right\vert^2} \\
    &= \sqrt{\left \vert \frac{1}{\tau} z_j (1 - z_j) \right\vert^2 +
        \sum_{i\neq j} \left \vert \frac{1}{\tau} z_i z_j \right \vert^2}  \\
    &= \sqrt{\frac{1}{\tau^2} z_j^2 (1 - z_j)^2 +
        \sum_{i\neq j} \frac{1}{\tau^2} z_i^2 z_j^2}  \\
    &= \sqrt{\frac{1}{\tau^2} \bigl( z_j^2 (1 - z_j)^2 +
        \sum_{i\neq j} z_i^2 z_j^2 \bigr)}  \\
    &= \sqrt{\frac{1}{\tau^2} z_j^2 \bigl( (1 - z_j)^2 + \sum_{i\neq j} z_i^2 \bigr)} \\
    &\leq \sqrt{\frac{1}{\tau^2} z_j^2 \left( (1 - z_j)^2 + 1 \right)} \\
    &\leq \sqrt{\frac{1}{\tau^2} z_j^2 (1+1)} \\
    &= \frac{\sqrt{2}}{\tau} z_j.
\end{align*}







\subsection{Further Experiments}
In the following, we illustrate further experiments. In particular, we created a new dataset for multi-hop reasoning and compared our model to its base model without discretization.

\subsubsection{Multi-Hop Reasoning over FB15K-237}\label{sec:fb237}

Since the benchmarks in Table~\ref{tb:kg_paths} are based on Freebase and Wordnet, which have several issues, we also created a new dataset based on FB15K-237~\citep{toutanova-chen-2015-observed} using the methodology of \citet{guu2015traversing}.
Particularly, we built the graph consisting of all training triples from the original dataset and sampled a starting entity uniformly. We then sampled an incident relation uniformly and sampled the next entity uniformly from all entities reachable via this relation. Continuing this way, we created a dataset of $272,115$ training paths of length (number of relations) $2,3,4,5$, respectively. We repeated this procedure on the graph consisting of all triples (not only training triples) and removed all duplicates to create $17,535$ validation paths of length $2,3,4,5$, and $20,466$ test paths of length $2,\ldots,10$. Finally, we added all of the FB15K-237 triples to the dataset as paths of length $1$.

We use a slight variation of our model for Freebase and Wordnet to optimize the training in both directions of the path. As baselines, we take the same model without the discretization module and train it first on triples only (ComplEx) and then on all paths (ComplEx-C). We validate the models using filtered MRR (\citet{bordes2013translating}) on all validation paths and report test results on all paths for each length individually. Table~\ref{tb:fb237} lists the results of the experiments on the new FB15K-237 based dataset. The models based on discrete-continuous components achieve improvements of up to $49\%$ compared to the baselines. Even more pronounced is the ability of these models to generalize: the accuracy does not drop even when tested on paths twice the length of those seen during training. The performance gap between $1$~relation and $2$ relations is mainly due to the fact that the test paths of length $1$ purely consist of triples from the test graph while all other paths consist of all triples from the full knowledge graph.

\begin{table}
  \caption{The results of the path query task on the newly created multi-hop reasoning dataset based on FB15K-237. Our model that discretizes the score function of the base model performs much better on the path query task and generalizes perfectly to paths up to length $10$.}
  \label{tb:fb237}
  \centering
  \begin{tabular}{lcccccccccc}
    \toprule
    & \multicolumn{5}{c}{MRR} & \multicolumn{5}{c}{MRR (etrapolation)} \\
    \cmidrule(r){2-6} \cmidrule(r){7-11}
    Model & 1    & 2    & 3    & 4    & 5    & 6    & 7    & 8    & 9    & 10  \\
    \midrule
    ComplEx         & \textbf{32.9} & 30.6 & 24.7 & 20.8 & 16.5 & 15.1 & 12.2 & 10.5 & 8.8 & 7.1 \\
    ComplEx-C    & 31.3 & 35.2 & 36.9 & 37.8 & 35.2 & 31.9 & 25.7 & 19.4 & 14.1 & 9.7 \\
    Ours, $\tau=4$  & 26.8 & \textbf{52.3} & \textbf{48.8} & \textbf{51.4} & \textbf{52.6} & \textbf{54.2} & \textbf{54.8} & \textbf{54.7} & \textbf{54.9} & \textbf{54.7}\\
    \bottomrule
  \end{tabular}
\end{table}



\subsection{Error Bars for Multi-Hop Reasoning over Knowledge Graphs}
In Table~\ref{tab:kg_error_bars} we give results including the error bars for the experiments depicted in Table~\ref{tb:kg_paths} and Table~\ref{tb:kg_extrapolation}. The results of Table~\ref{tb:kg_paths} are can be found in the row Paths~$\leq 5$.
\begin{table}[h!]
    \small
    \centering
    \caption{Error bars for the experiments in Table~\ref{tb:kg_paths} and Table~\ref{tb:kg_extrapolation}.}
    \begin{tabular}{llcccc}
        \toprule
        & & \multicolumn{2}{c}{WordNet} & \multicolumn{2}{c}{Freebase} \\
        \cmidrule(r){3-6}
        Paths    & Model     & MQ   & H@10 & MQ   & H@10 \\
        \midrule
        \multirow{1}{*}{$\leq5$}
        & Ours  & $94.4\pm0.0$ & $64.3\pm0.11$ & $89.6\pm0.46$ & $68.7\pm0.32$  \\
        \midrule
        \multirow{1}{*}{$\leq4$}
        & Ours  & $94.3\pm0.14$ & $64.7\pm0.11$ & $88.8\pm1.29$ & $69.4\pm0.72$ \\
        \midrule
        \multirow{1}{*}{$\leq3$}
        & Ours  & $93.4\pm0.04$ & $65.0\pm0.11$ & $88.3\pm1.85$ & $68.5\pm0.46$ \\
        \midrule
        \multirow{1}{*}{$\leq2$}
        & Ours  & $90.2\pm0.04$ & $62.0\pm0.04$ & $87.6\pm0.62$ & $68.6\pm0.36$ \\
        \midrule
        \multirow{1}{*}{$\leq1$}
        & Ours  & $81.1\pm0.22$ & $52.2\pm0.17$ & $82.1\pm0.54$ & $63.9\pm0.6$ \\
        \bottomrule
        \end{tabular}
    \label{tab:kg_error_bars}
\end{table}

\subsection{Accuracy Curves of the Accuracies in Figure~\ref{fig:poor-minima-acc}}

To obtain the accuracies in Figure~\ref{fig:poor-minima-acc}, we repeated each experiment $8$ times. For both of the experiment setups, TM and TauAnn, we found a single seed each, for which we noticed no training at all. Thus we repeated these experiments a ninth time. We depict the accuracy curves of all $9$ runs of TM in Figure~\ref{fig:tm_acc_9} and of TauAnn in Figure~\ref{fig:tauann_acc_9}.


% Figure environment removed
\vspace{1cm}

% Figure environment removed










\end{document}
