\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreas et~al.(2016)Andreas, Rohrbach, Darrell, and
  Klein]{andreas2017neural}
J.~Andreas, M.~Rohrbach, T.~Darrell, and D.~Klein.
\newblock Neural module networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2016.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Y.~Bengio, N.~L{\'e}onard, and A.~Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Bordes et~al.(2013)Bordes, Usunier, Garcia-Duran, Weston, and
  Yakhnenko]{bordes2013translating}
A.~Bordes, N.~Usunier, A.~Garcia-Duran, J.~Weston, and O.~Yakhnenko.
\newblock Translating embeddings for modeling multi-relational data.
\newblock In \emph{Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting in Lake Tahoe, Nevada, United States, held December 5-8}, pages
  1--9, 2013.

\bibitem[Chen et~al.(2018)Chen, Song, Wainwright, and Jordan]{chen2018learning}
J.~Chen, L.~Song, M.~Wainwright, and M.~Jordan.
\newblock Learning to explain: An information-theoretic perspective on model
  interpretation.
\newblock In \emph{International Conference on Machine Learning}, pages
  883--892. PMLR, 2018.

\bibitem[Correia et~al.(2020)Correia, Niculae, Aziz, and
  Martins]{correia2020efficient}
G.~M. Correia, V.~Niculae, W.~Aziz, and A.~F. Martins.
\newblock Efficient marginalization of discrete and structured latent variables
  via sparsity.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Das et~al.(2017)Das, Neelakantan, Belanger, and
  McCallum]{das-etal-2017-chains}
R.~Das, A.~Neelakantan, D.~Belanger, and A.~McCallum.
\newblock Chains of reasoning over entities, relations, and text using
  recurrent neural networks.
\newblock In \emph{Proceedings of the 15th Conference of the {E}uropean Chapter
  of the Association for Computational Linguistics: Volume 1, Long Papers},
  pages 132--141, Valencia, Spain, Apr. 2017.

\bibitem[Dong et~al.(2019)Dong, Mao, Lin, Wang, Li, and Zhou]{dong2019neural}
H.~Dong, J.~Mao, T.~Lin, C.~Wang, L.~Li, and D.~Zhou.
\newblock Neural logic machines.
\newblock \emph{7th International Conference on Learning Representations, ICLR
  2019, New Orleans, LA, USA, May 6-9}, 2019.

\bibitem[Evans and Grefenstette(2018)]{evans2018learning}
R.~Evans and E.~Grefenstette.
\newblock Learning explanatory rules from noisy data.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  1--64, 2018.

\bibitem[Grathwohl et~al.(2018)Grathwohl, Choi, Wu, Roeder, and
  Duvenaud]{grathwohl2017backpropagation}
W.~Grathwohl, D.~Choi, Y.~Wu, G.~Roeder, and D.~Duvenaud.
\newblock Backpropagation through the void: Optimizing control variates for
  black-box gradient estimation.
\newblock \emph{6th International Conference on Learning Representations, ICLR
  2018, Vancouver, BC, Canada, April 30 - May 3}, 2018.

\bibitem[Guu et~al.(2015)Guu, Miller, and Liang]{guu2015traversing}
K.~Guu, J.~Miller, and P.~Liang.
\newblock Traversing knowledge graphs in vector space.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2015deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[H{\"o}lldobler et~al.(1999)H{\"o}lldobler, Kalinke, and
  St{\"o}rr]{holldobler1999approximating}
S.~H{\"o}lldobler, Y.~Kalinke, and H.-P. St{\"o}rr.
\newblock Approximating the semantics of logic programs by recurrent neural
  networks.
\newblock \emph{Applied Intelligence}, 11\penalty0 (1):\penalty0 45--58, 1999.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
G.~Huang, Y.~Sun, Z.~Liu, D.~Sedra, and K.~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock \emph{European conference on computer vision}, pages 646--661, 2016.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016categorical}
E.~Jang, S.~Gu, and B.~Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{5th International Conference on Learning Representations, ICLR
  2017, Toulon, France, April 24-26}, 2017.

\bibitem[Kingma and Welling(2014)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16}, 2014.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
D.~P. Kingma, T.~Salimans, and M.~Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock \emph{Advances in neural information processing systems},
  28:\penalty0 2575--2583, 2015.

\bibitem[Koo et~al.(2007)Koo, Globerson, Carreras, and
  Collins]{koo2007structured}
T.~Koo, A.~Globerson, X.~Carreras, and M.~Collins.
\newblock Structured prediction models via the matrix-tree theorem.
\newblock In \emph{Proceedings of the 2007 Joint Conference on Empirical
  Methods in Natural Language Processing and Computational Natural Language
  Learning (EMNLP-CoNLL)}, pages 141--150, 2007.

\bibitem[Kool et~al.(2019)Kool, Van~Hoof, and Welling]{kool2019attention}
W.~Kool, H.~Van~Hoof, and M.~Welling.
\newblock Attention, learn to solve routing problems!
\newblock \emph{7th International Conference on Learning Representations, ICLR
  2019, New Orleans, LA, USA, May 6-9}, 2019.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2016concrete}
C.~J. Maddison, A.~Mnih, and Y.~W. Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock \emph{5th International Conference on Learning Representations, ICLR
  2017, Toulon, France, April 24-26}, 2017.

\bibitem[Manhaeve et~al.(2018)Manhaeve, Dumancic, Kimmig, Demeester, and
  De~Raedt]{manhaeve2018deepproblog}
R.~Manhaeve, S.~Dumancic, A.~Kimmig, T.~Demeester, and L.~De~Raedt.
\newblock Deepproblog: Neural probabilistic logic programming.
\newblock \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, Montr√©al, Canada,
  December 3-8, 2018}, 31:\penalty0 3749--3759, 2018.

\bibitem[Mao et~al.(2019)Mao, Gan, Kohli, Tenenbaum, and Wu]{mao2019neuro}
J.~Mao, C.~Gan, P.~Kohli, J.~B. Tenenbaum, and J.~Wu.
\newblock The neuro-symbolic concept learner: Interpreting scenes, words, and
  sentences from natural supervision.
\newblock \emph{7th International Conference on Learning Representations, ICLR
  2019, New Orleans, LA, USA, May 6-9}, 2019.

\bibitem[Nangia and Bowman(2018)]{nangia2018listops}
N.~Nangia and S.~Bowman.
\newblock Listops: A diagnostic dataset for latent tree learning.
\newblock In \emph{Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Student Research
  Workshop}, pages 92--99, 2018.

\bibitem[Niculae and Martins(2020)]{Niculae2020LPSparseMAPDR}
V.~Niculae and A.~F.~T. Martins.
\newblock Lp-sparsemap: Differentiable relaxed optimization for sparse
  structured prediction.
\newblock In \emph{ICML}, 2020.

\bibitem[Niculae et~al.(2018)Niculae, Martins, Blondel, and
  Cardie]{Niculae2018SparseMAPDS}
V.~Niculae, A.~F.~T. Martins, M.~Blondel, and C.~Cardie.
\newblock Sparsemap: Differentiable sparse structured inference.
\newblock In \emph{ICML}, 2018.

\bibitem[Parisotto et~al.(2016)Parisotto, Mohamed, Singh, Li, Zhou, and
  Kohli]{parisotto2016neuro}
E.~Parisotto, A.-r. Mohamed, R.~Singh, L.~Li, D.~Zhou, and P.~Kohli.
\newblock Neuro-symbolic program synthesis.
\newblock \emph{arXiv preprint arXiv:1611.01855}, 2016.

\bibitem[Paulus et~al.(2020)Paulus, Choi, Tarlow, Krause, and
  Maddison]{paulus2020gradient}
M.~B. Paulus, D.~Choi, D.~Tarlow, A.~Krause, and C.~J. Maddison.
\newblock Gradient estimation with stochastic softmax tricks.
\newblock \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, December 6-12},
  2020.

\bibitem[Pervez et~al.(2020)Pervez, Cohen, and Gavves]{pervez2020low}
A.~Pervez, T.~Cohen, and E.~Gavves.
\newblock Low bias low variance gradient estimates for boolean stochastic
  networks.
\newblock \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 119:\penalty0 7632--7640, 13--18 Jul 2020.

\bibitem[Rockt{\"a}schel and Riedel(2017)]{rocktaschel2017end}
T.~Rockt{\"a}schel and S.~Riedel.
\newblock End-to-end differentiable proving.
\newblock \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, Long Beach, CA,
  USA, December 4-9}, 2017.

\bibitem[Ruffinelli et~al.(2020)Ruffinelli, Broscheit, and
  Gemulla]{ruffinelli2020you}
D.~Ruffinelli, S.~Broscheit, and R.~Gemulla.
\newblock You {\{}can{\}} teach an old dog new tricks! on training knowledge
  graph embeddings.
\newblock In \emph{8th International Conference on Learning Representations,
  ICLR 2020, Addis Ababa, Ethiopia, April 26-30}, 2020.

\bibitem[Schulman et~al.(2015)Schulman, Heess, Weber, and
  Abbeel]{schulman2015gradient}
J.~Schulman, N.~Heess, T.~Weber, and P.~Abbeel.
\newblock Gradient estimation using stochastic computation graphs.
\newblock \emph{Advances in Neural Information Processing Systems 28: Annual
  Conference on Neural Information Processing Systems 2015, Montreal, Quebec,
  Canada, December 7-12}, 2015.

\bibitem[Serafini and Garcez(2016)]{serafini2016logic}
L.~Serafini and A.~d. Garcez.
\newblock Logic tensor networks: Deep learning and logical reasoning from data
  and knowledge.
\newblock \emph{arXiv preprint arXiv:1606.04422}, 2016.

\bibitem[Shayer et~al.(2018)Shayer, Levi, and Fetaya]{shayer2018learning}
O.~Shayer, D.~Levi, and E.~Fetaya.
\newblock Learning discrete weights using the local reparameterization trick.
\newblock \emph{6th International Conference on Learning Representations, ICLR
  2018, Vancouver, BC, Canada, April 30 - May 3}, 2018.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Toutanova and Chen(2015)]{toutanova-chen-2015-observed}
K.~Toutanova and D.~Chen.
\newblock Observed versus latent features for knowledge base and text
  inference.
\newblock In \emph{Proceedings of the 3rd Workshop on Continuous Vector Space
  Models and their Compositionality}, pages 57--66, Beijing, China, July 2015.
  Association for Computational Linguistics.

\bibitem[Towell and Shavlik(1994)]{towell1994knowledge}
G.~G. Towell and J.~W. Shavlik.
\newblock Knowledge-based artificial neural networks.
\newblock \emph{Artificial intelligence}, 70\penalty0 (1-2):\penalty0 119--165,
  1994.

\bibitem[Trouillon et~al.(2016)Trouillon, Welbl, Riedel, Gaussier, and
  Bouchard]{trouillon2016complex}
T.~Trouillon, J.~Welbl, S.~Riedel, {\'E}.~Gaussier, and G.~Bouchard.
\newblock Complex embeddings for simple link prediction.
\newblock In \emph{Proceedings of the 33nd International Conference on Machine
  Learning, ICML 2016, New York City, NY, USA, June 19-24}, pages 2071--2080.
  PMLR, 2016.

\bibitem[Tsamoura and Michael(2020)]{tsamoura2020neural}
E.~Tsamoura and L.~Michael.
\newblock Neural-symbolic integration: A compositional perspective.
\newblock \emph{arXiv preprint arXiv:2010.11926}, 2020.

\bibitem[Tucker et~al.(2017)Tucker, Mnih, Maddison, Lawson, and
  Sohl-Dickstein]{tucker2017rebar}
G.~Tucker, A.~Mnih, C.~J. Maddison, D.~Lawson, and J.~Sohl-Dickstein.
\newblock Rebar: Low-variance, unbiased gradient estimates for discrete latent
  variable models.
\newblock \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, Long Beach, CA,
  USA, December 4-9}, 2017.

\bibitem[Wang et~al.(2019)Wang, Huang, Wang, Dai, Jiang, Liu, Lyu, Zhu, and
  Wu]{wang2019coke}
Q.~Wang, P.~Huang, H.~Wang, S.~Dai, W.~Jiang, J.~Liu, Y.~Lyu, Y.~Zhu, and
  H.~Wu.
\newblock Coke: Contextualized knowledge graph embedding.
\newblock \emph{arXiv preprint arXiv:1911.02168}, 2019.

\bibitem[Williams(1992)]{williams1992simple}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Yang et~al.(2017)Yang, Yang, and Cohen]{yang2017differentiable}
F.~Yang, Z.~Yang, and W.~W. Cohen.
\newblock Differentiable learning of logical rules for knowledge base
  reasoning.
\newblock \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, Long Beach, CA,
  USA, December 4-9}, 2017.

\bibitem[Yin et~al.(2018)Yin, Yaghoobzadeh, and Sch{\"u}tze]{yin2018recurrent}
W.~Yin, Y.~Yaghoobzadeh, and H.~Sch{\"u}tze.
\newblock Recurrent one-hop predictions for reasoning over knowledge graphs.
\newblock \emph{arXiv preprint arXiv:1806.04523}, 2018.

\end{thebibliography}
