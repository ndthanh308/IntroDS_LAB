\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsthm}
\usepackage[nodisplayskipstretch]{setspace}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage[shortlabels]{enumitem}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{calc}
\usepackage{float}
\usepackage{tikz}
\usepackage{graphics} 
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{verbatim}
\usepackage{adjustbox}
\usepackage{blindtext}
\usepackage{algorithm}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}

\newcommand{\pink}[1]{\textcolor{magenta}{#1}}
\usepackage{longtable}
\usepackage{setspace}
\usepackage{natbib}
\bibliographystyle{abbrvnat}
\RequirePackage{amsthm,amsmath,amsfonts,amssymb}
\newcommand{\calD}{\mathcal D}
\newcommand{\calV}{\mathcal V}
\newcommand{\calG}{\mathcal G}
\newcommand{\given}{\;|\;}
\newcommand{\ind}{\overset{ind}{\sim}}
\newcommand{\iid}{\overset{i.i.d}{\sim}}

\def\*#1{\mathbf{#1}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\diag}{diag}

\newtheorem{proposition}{Proposition}[section]

%\topmargin=0.05in
%\textheight=8.15in
%\textwidth=5.6in
%\oddsidemargin=0.7in
%\raggedbottom
%\newdimen \jot \jot=5mm
%\brokenpenalty=10000

%\doublespacing
%\setstretch{1.667}
\setstretch{2.1}
\usepackage{authblk}
\begin{document}
\title{Visibility graph-based covariance functions for scalable spatial analysis in nonconvex domains}
\author[1]{Brian Gilbert\thanks{BG and AD were partially supported by the National Science Foundation (NSF) Division of Mathematical Sciences grant DMS-1915803.}}
\author[2]{Abhirup Datta\protect\footnotemark[1]}
\affil[1]{Department of Population Health, NYU Grossman School of Medicine}
\affil[2]{Department of Biostatistics, Johns Hopkins University Bloomberg School of Public Health}
\date{July 2023}




\maketitle

\begin{abstract}
We present a new method for constructing valid covariance functions of Gaussian processes over irregular nonconvex spatial domains such as water bodies, where the geodesic distance agrees with the Euclidean distance only for some pairs of points. Standard covariance functions based on geodesic distances are not positive definite on such domains. Using a visibility graph on the domain, we use the graphical method of ``covariance selection" to propose a class of covariance functions that preserve Euclidean-based covariances between points that are connected through the domain. The proposed method preserves the partially Euclidean nature of the intrinsic geometry on the domain while maintaining validity (positive definiteness) and marginal stationarity over the entire parameter space, properties which are not always fulfilled by existing approaches to construct covariance functions on nonconvex domains. We provide useful approximations to improve computational efficiency, resulting in a scalable algorithm. We evaluate the performance of competing state-of-the-art methods using simulation studies on a contrived nonconvex domain. The method is applied to data regarding acidity levels in the Chesapeake Bay, showing its potential for ecological monitoring in real-world spatial applications on irregular domains.
\end{abstract}

\section{Introduction}
 Much of spatial analysis concerns data collected over domains that are convex and Euclidean (e.g., an agricultural plot of land) or where the effect of irregular boundaries or nonconvexity in parts of the domain can be ignored due to the scale of the analysis. In some scientific contexts, the irregularity and concavity of the spatial domain where the data is collected cannot be ignored. An example is data collection over highly irregularly shaped bodies of water like estuaries, where notions of distance may be better understood with respect to the body itself rather than the larger space it is embedded in. For example, when designating distance in an estuary, it is more appropriate to measure ``as the fish swims" (the length of the shortest path through the water) rather than ``as the crow flies" (the length of the shortest path on Earth's surface), as noted by \citet{little} and \citet{rathbun1998spatial}. Mathematically speaking, in a nonconvex domain, the \textit{geodesic} distances with respect to the \textit{intrinsic} Euclidean metric do not match the ordinary Euclidean metric. However, using non-Euclidean distance measures with common covariance functions like the Mat\'{e}rn family may not always yield positive definite covariance matrices \citep{curriero}. Therefore, it is desirable to construct covariance functions based on distance measures that respect physical geometry while maintaining statistical validity.

Some methods have been designed to address this issue. Multidimensional scaling (MDS) \citep{cox}, is a very general approach that maps the locations from an arbitrary space with some notion of a distance into a Euclidean space while preserving interpoint distances as accurately as possible. For example, for original points $\{s_i\}\in \calD$ where $\calD$ denotes a non-Euclidean domain with pairwise distances $d_{ij}$, we might search for points $\{S^*_i\} \in \mathbb{R}^{k}$ for some $k$ with pairwise distances $\delta_{ij}$ which minimizes a loss function such as $\sum_{ij} \frac{ (\delta_{ij}- d_{ij})^2}{d_{ij}}$. However, this has a necessarily distorting effect, as it is not possible to preserve all the distances through the embedding. This can be seen even in a map from $\mathbb{R}^3$, where four points can be mutually equidistant, to $\mathbb{R}^2$, where they cannot. MDS is the theoretical basis of the popular algorithm ISOMAP \citep{tenenbaum}. ISOMAP maps from non-Euclidean geodesic distances to Euclidean distances. However, the geodesic distances are approximated by summed Euclidean distances of neighboring points, which would be an inaccurate procedure in our context unless the domain is nearly convex.

\cite{davis} propose to input intradomain (i.e., geodesic) distances into any Euclidean covariance function to construct a candidate covariance matrix, which is then passed through an algorithm to find the ``nearest" positive definite matrix to the candidate. Specifically, let $C([d_{ij}])$ denote the matrix of covariance values applying some covariance function to geodesic distances. Then $C([d_{ij}])$ may not be positive definite, but it has eigendecomposition $C([d_{ij}])=V \Lambda V'$. For some tolerance value $\epsilon > 0$, any negative entries of $\Lambda$ are replaced with $\epsilon$ to form a new nonnegative diagonal matrix $\tilde{\lambda}$. Then $\tilde{C} =V \tilde{\Lambda} V'$ is taken as the new covariance matrix for kriging, since it is positive definite and hopefully close to $C$. However, the approximation could be poor if the original matrix has large negative eigenvalues. Additionally, the ascertainment of all pairwise geodesic distances and the eigendecomposition of $C$ are computationally intensive for large data sets.

\cite{niu} present a method that uses the heat kernel as a covariance kernel; the heat kernel in turn is approximated by the transition probabilities of Brownian motion through the domain, where physical boundaries impede the motion of particles. However, these transition probabilities must be estimated by using simulations, which is computationally expensive for large datasets. \cite{dunson} present a related method {\em GLGP} that aims to alleviate the computational burden by appealing to the Graph Laplacian, which ``corresponds to the infinitesimal
generator of a random walk on the sampled data points." Specifically, for training locations $\{s_1, ..., s_m\}$ and test points $\{s_{m+1}, ..., s_{m+n}\}$, they define a kernel $k_\epsilon(s,s') = \exp(- \frac{ |S-S'|^2}{4\epsilon^2})$ and matrix $W_{ij} = \frac{k_\epsilon(s_i, s_j)}{q_\epsilon(s_i) q_\epsilon(s_j)}$ where $q_\epsilon(S) = \sum_{i=1}^{m+n} k_\epsilon(s, s_i)$. Then the ``graph" of interest is the complete weighted graph over all points with weights $W$. Further defining the diagonal matrix $D_{ii} = \sum_{j=1}^{m+n} W_{ij}$, the Graph Laplacian is given by $L = \frac{D^{-1}W-I}{\epsilon^2}$, where $I$ is the identity matrix. The covariance function is then built using finitely many eigenpairs of $L$. However, the approach does not respect domain boundaries as absolute (as reflected in the use of the Euclidean distance in defining the kernel); rather, it relies on a finely-tuned bandwidth parameter $\epsilon$, which can potentially lead to points close in Euclidean space but far in geodesic distance to unduly influence each other. Also, the corresponding likelihood is difficult to optimize, leading to a lack of scalability for large datasets. 


\cite{borovitskiy} generalize Euclidean kriging to the case of Riemannian manifolds without boundaries. They begin with the stochastic partial differential equation solution \citep{whittle} to the Mat\'{e}rn process given by  $
    (\frac{2\nu}{\kappa^2}-\Delta)^{\nu/2+d/4} f = \mathcal{W}
    $, where $f$ is the process and $\mathcal{W}$ is white noise, and the left-hand side contains Mat\'{e}rn parameters. This can be generalized to Riemannian manifolds by replacing the Laplacian $\Delta$ with the Laplace-Beltrami operator. However, this theory has no immediate application to the domains of interest here since we deal with manifolds \textit{with} irregular (sharp) boundaries (e.g., the shorelines). The Barrier Spatial Gaussian Field \citep{bakka} is a way of accounting for physical boundaries similarly using a stochastic partial differential equation model, but it requires boundary conditions (such as fixed function values or derivatives) that may be unrealistic and strongly affect results. In addition, they can lead to anti-conservative inference near the boundaries \citep{bora}.

A recent notable contribution is BORA-GP \citep{bora}, which encodes the geometry of the domain in the form of neighbor-relationships between points and proceeds by fitting a nearest neighbor Gaussian process \citep{vecchia, datta2016} in a Bayesian manner. Points are assumed to be conditionally independent of each other conditional on nearest-neighbor sets; this yields a local low-rank approximation of the likelihood. BORA-GP differs from the usual nearest neighbor approximation in that neighbor sets only include Euclidean neighbors, i.e., points connected by a straight line through the domain. This neighbor scheme is intuitive as it preserves the geometry of the domain, and has similarities to the method we propose here but differs in that BORA-GP requires an ordering of the locations which leads to a lack of stationarity for highly irregular domains. Also, BORA-GP does not attempt to preserve any covariance values relative to the Euclidean model, even though the Euclidean values are partly valid for such domains.

In this manuscript, we propose a general approach to construct valid covariance functions on any irregular nonconvex subset of Euclidean space that preserves the partially Euclidean nature of the domain while respecting the irregular geometry. We also aim to preserve marginal stationarity of the covariance function over the entire domain. These properties ensure that the analysis using our covariance function restricted to any convex subset of the nonconvex domain would agree exactly with standard spatial analysis using Euclidean distances.  
We show that, starting from a covariance function $C$ which is valid on the Euclidean domain $\mathbb R^d$, it is possible to derive a covariance function $C^*$ for a nonconvex subset $\calD \in \mathbb R^d$ such that $C^*$ leaves unchanged the covariances from $C$ among pairs of points whose geodesic distance coincides with Euclidean distances; that is, those pairs of points which are connected by a straight line within the domain. This seems sensible because the ordinary Euclidean metric would presumably be employed were the process restricted to a convex subset of the domain. To our knowledge, none of the aforementioned approaches possess this property. 

Our proposed method is based on creating a visibility graph between points in the domain that incorporates the knowledge of the geometry and barriers, and subsequently applying the graphical method of ``covariance selection" as described in \cite{dempster} to obtain the desired covariance function. The scheme preserves Euclidean covariances between points that are connected in the domain while satisfying a conditional independence (Markov) property for points not connected in the domain. We outline the mathematical theory below, followed by some pragmatic approximations used for implementation with computational tractability. We demonstrate the method and compare it to state-of-the-art alternatives through simulations and analysis of Chesapeake Bay data. 

\section{Valid covariance functions on irregular nonconvex domains}

\subsection{Finite-dimensional construction}\label{sec:finite}

We first present a general approach to construct valid spatial covariance matrices on any arbitrary finite set of locations in a nonconvex domain. Subsequently, we will extend to valid covariance functions of Gaussian processes over the entire domain. 
Consider locations $s_1,...,s_n$ which are points in an irregular nonconvex domain $\mathcal{D}$, a subset of $\mathbb{R}^d$. Define the adjacency matrix $A$ where $a_{ij}$ is 1 if $s_i$ and $s_j$ are connected by a line segment wholly within the space $\mathcal{D}$; let $\mathcal{G} = (\mathcal{V},E)$ be the corresponding formal graph. Here, $E$ are pairs of indices of points in $\mathcal{V}=s_1, ..., s_n$, namely the pairs that denote connected points within $\mathcal{D}$. Let $E$ denote the pairs of points which are \textit{not connected in the domain} in $\mathcal{D}$. In the geometry and artificial intelligence literature, such a graph is often called the ``visibility graph" as, if $s_i$ and $s_j$ are not connected through the domain, the existence of a barrier prohibits seeing $s_i$ from $s_j$ and vice versa.  

We start with any covariance function $C$ that is valid (positive definite) on the Euclidean domain $\mathbb R^d$. Common choices of $C$ in geospatial analysis on Euclidean domains include the Mat\'ern family of covariances, exponential and Gaussian covariances (which are special or limiting case of the Mat\'ern family). Our construction will be agnostic to the specific choice of $C$. Note that when considering spatial analysis within the nonconvex domain $\calD$, using $C$ with Euclidean distance is a valid but inappropriate choice as it ignores the geometry of the domain \citep{little,rathbun1998spatial}. On the other hand, using $C$ with the geodesic distance, although seems reasonable, does not guarantee positive-definiteness \citep{curriero}.   

Observe that for many points within $\calD$, the geodesic distance does coincide with the Euclidean distance. In fact, the covariance function $C$ would be a perfectly valid choice for analyzing data within any convex subset of $\calD$. Hence, we desire a covariance function that both respects the irregular and nonconvex geometry of the domain $\calD$, but also acknowledges this \textit{partly Euclidean} nature of the domain. Formally, given $C$ and the finite set of locations $\mathcal V=\{s_1,\ldots,s_n\}$, we seek a covariance function
$C^*$ with the following properties, letting $L=C^*(\calV,\calV)$ denote the covariance matrix induced by $C^*$ on $V$:
\begin{align}\label{eq:covsel}
    L_{ii} &=C(s_i,s_i) \mbox{ for all } i, \nonumber \\
    L_{ij} &= C(s_i,s_j) \mbox{ for all } i \neq j \mbox{ such that } (i,j) \in E,\\
    (L^{-1})_{ij} &= 0 \mbox{ for all } i \neq j \mbox{ such that } (i,j) \notin E. \nonumber
\end{align}

When using a stationary covariance function $C$, the first condition states that the GP resulting from the new covariance function $C^*$ will also have stationary marginal distributions. The second condition recognizes the partly Euclidean nature of the domain, imposing that the covariance of points connected through the domain is given by a standard Euclidean covariance function. This condition formalizes the belief that the original covariance function is suitable for through-domain distances, since for these connections there is no interference by boundaries. Finally, the third condition posits that two points that are not connected through the domain are conditionally independent, given all other observations. This is reasonable if the boundaries are seen as an impediment to correlation between the points that they separate. We show later in Section \ref{sec:theory} how this Markovian property leads to covariances agreeing with the geodesic covariance on certain special domains. 




\cite{dempster} in the seminal work on \textit{covariance selection} showed that given any positive definite matrix $K$ and a graph $\mathcal G=(\calV,E)$ with nodes indexed on the rows of $K$, there exists an unique positive definite matrix $L$ such that $L_{ij} = K_{ij}$ if $i=j$ or $(i,j) \in E$, and $(L^{-1})_{ij}=0$ if $(i,j) \notin E$. \cite{speed} gives an efficient \textit{iterative proportional scaling} algorithm to obtain $L$ given $K$ and the graph $\calG$. We denote such an $L$ derived from $K$ and $\calG$ using covariance selection as $L=CovSel(K,\calG)$. Hence, that a unique matrix $L=CovSel(C(\calV,\calV),\calG)$ exists satisfying all properties in (\ref{eq:covsel}) follows directly from Dempster's covariance selection using the positive definite matrix $K=C(\calV,\calV)$ and letting $\mathcal G$ be the visibility graph on the domain. 
We then specify a Gaussian process on $\calV$ simply  as 
\begin{equation}\label{eq:finite}
    w(\calV) \sim N(0,L), \mbox{ with } L=CovSel(C(\calV,\calV),\calG).
\end{equation} 
which satisfies $w(s_i) \overset{d}{=} w(s_j)$ (marginally stationary), $Cov(w(s_i),w(s_j)) = C(s_i,s_j)$ if $s_i$ and $s_j$ are connected in the domain (partly Euclidean), and $Cov(w(s_i),w(s_j) \given w(V) \setminus \{w(s_i),w(s_j)\}) = 0$ (Markovian for points not connected in the domain). 


\subsection{Process formulation}\label{sec:process}

The formulation in the previous section only presents a process (or its covariance function) restricted to an arbitrary but finite set of locations. In geostatistics, the goal is typically to be able to predict and make inferences about the outcome at every location in the domain given data at a finite set of locations. One needs the complete specification of the Gaussian process, or its covariance function over the entire domain. Typically, the extension of a GP from a finite set to an entire continuous domain is achieved by specifying the conditional mean and covariance functions, where the conditioning is with respect to the finite set which is often the data locations. With a large number of data points and a large (in fact, infinite) number of prediction locations, evaluating this set of conditional distributions can be computationally expensive. Hence, a few strategies are commonly adopted to improve scalability. One such scheme is positing the conditional distributions to be independent at every new location (i.e., independent kriging) as opposed to modeling joint predictive distributions (i.e., joint kriging) which becomes cumbersome as the number of prediction locations increases. The rationale is that given the information available from the data at a reasonably large set of locations, the residual covariance between the process at two prediction locations is ignorable. Another strategy to improve scalability is 
nearest-neighbor kriging. To specify the conditional distribution of a Gaussian process at a new point, a $k$-nearest neighbor scheme \citep{datta} uses only the $k$ nearest neighbors from the data locations for information. This often makes the accompanying approximate assumption of \textit{screening effect} \citep{stein2002screening}, that distant points can be ignored, conditional on nearby points. 
We leverage both these strategies, but the primary purpose is not to come up with computational approximations of existing covariances, but rather to create a new and valid covariance function $C^*$ on nonconvex domains which extends the properties (stationarity, partial Euclidean and Markov)  established in Section \ref{sec:finite} from a finite set $\calV$ to the entire domain $\calD$. 

In formal terms, for any location $s$ outside of $\calV$, we find a \textit{neighbor set} $N(s)$ of up to $k$ locations in $\calV_n$ that are nearest to $s$. The neighbor set will be used to define the conditional distribution of $w(s) \given w(\calV)$. We enforce two conditions on $N(s)$. First, each location in $N(s)$ is connected in the domain to $s$. This is needed to ensure that we are not including a location in $N(s)$ that is close to $s$ in Euclidean distance but far away in the geodesic distance, as that would distort the geometry of the domain. We also require that the subgraph of $\calG$ restricted to $N(s)$ is complete, or in other words, all pairwise locations in $N(s)$ are connected in the domain to each other. 
This implies that the covariance among $N(s)$ is Euclidean, and in turn, ensures that the resulting process has desirable properties as discussed in Theorem \ref{th:process}. Specific constructions of neighbor sets that satisfy these properties are discussed in Section \ref{sec:prediction}. 

Having defined the neighbor set $N(s)$, for any location $s \notin \calV$, we specify the conditional distributions as follows:
\begin{align}\label{eq:process}
    w(s) \given w(\calV) &\sim N(B(s)w(N(s)),F(s)), \mbox{ where } \nonumber \\
    B(s)&=C(s,N(s)) C(N(s),N(s))^{-1}, \mbox{ and }\\
    F(s) &= C(s,s)- C(s,N(s)) 
    C(N(s),N(s))^{-1}C(N(s),s) \nonumber.
\end{align}

Equations (\ref{eq:finite}) and (\ref{eq:process}) complete the specification of a Gaussian process $w(\cdot)$ on the entire domain $\calD$. It is straightforward to verify (proof omitted) that it is indeed a valid process (in the sense of Kolmogorov's conditions) with a positive definite covariance function $C^*$. The construction only relies on a parent Euclidean covariance function $C$ and the visibility graph $\calG$ encoding the information about the geometry of the irregular nonconvex domain $\calD$. Any valid choice of Euclidean covariance $C$ will yield a valid $C^*$ and the parameters of $C^*$ are the same as the parameters of the Euclidean covariance $C$. We refer to the process as \textit{visGP} due to its reliance on the visibility graph. 

\subsection{Properties}\label{sec:theory}
As discussed in Section \ref{sec:finite}, our visibility graph-based approach is motivated by two principles. The first is that the analysis restricted to any convex subset of the domain should correspond to a traditional geospatial analysis on a convex domain. This translates to preserving all the marginal distributions and pairwise covariances among points connected in the domain. The second is that the conditional covariance of points not connected in the domain is zero. This is intuitive as the domain boundaries can be viewed as preventing any direct information flow between the two points that can result in conditional correlation. The construction of the process (\ref{eq:finite}) on the finite set $\calV$ using covariance selection immediately guarantees these properties hold on $\calV$. The following theorem shows that the extension to a process $w(\cdot)$ on the entire domain $\calD$, achieved via (\ref{eq:process}), retains these properties.

\begin{theorem}\label{th:process} Let $\calD \in \mathbb R^d$ denote an open irregular nonconvex domain. Consider an increasing collection of finite locations $\calV_n$ in $\calD$ such that $\cup_n \calV_n$ is dense in $\calD$, and let $\calG_n$ denote the visibility graph on $\calV_n$ based on $\calD$. Let $C$ denote any valid Euclidean stationary covariance function of $\mathbb R^d$. 

Let $C^*_n$ denote the covariance function of the visGP 
$w(\cdot)$ on $\calD$ defined through (\ref{eq:finite}) and (\ref{eq:process}) using $\calV_n$ and $\calG_n$ and with neighbor sets $N(s)$ described in Section \ref{sec:process} with $\|B(s)\| \leq M$ for some $M$ for all $s \in \calD$. 

Then we have the following:
\begin{enumerate}[(a)]
    \item (Marginal stationarity) $w(s) \overset{d}{=} w(s')$ for any $s,s' \in \calD$. 
    \item (Partly Euclidean) $\lim_n Cov(w(s),w(s')) = C(s,s')$ for any $s,s' \in \calD$ that are connected in $\calD$ (i.e., the straight line connecting $s$ and $s'$ lies entirely in $\calD$).
    \item (Markov) $Cov\Big(w(s),w(s') \given \big\{w(u) \given u \in \calD \setminus \{s,s'\}\big\}\Big) =0$ for large enough $n$ for any $s,s' \in \calD$ that are not connected in $\calD$.
\end{enumerate}
\end{theorem}

The result requires minimal assumptions. It enforces no restriction on the shape of the domain or on the choice of the Euclidean covariance function $C$ beyond stationarity. This can too be relaxed; then the new covariance function $C^*$ would also be marginally non-stationary satisfying $C^*(s,s) = C(s,s)$ for all $s \in \calD$. No restriction is placed on the design of the finite set of locations $\calV_n$ (which in practice is typically the set of data locations). Thus irregular data designs are accommodated, with the asymptotic regime assuming that data locations will become dense in $\calD$. This is the common assumption in infill asymptotics for spatial statistics. The condition $\|B(s)\| \leq M$ puts a bound on the kriging weights $B(s) = C(s,N(s))C(N(s),N(s))^{-1}$. In less technical terms, this essentially prohibits the neighbors to be chosen very close to each other as then the contributions by the different members of $w(N(s)$ in predicting $w(s) \given w(N(s))$ becomes less identifiable, and the kriging weights can diverge. We note that this assumption is purely on the construction of the neighbor sets, which is controlled by the user and can be enforced by sequentially choosing neighbors which are sufficiently distant from the previously chosen neighbors.

Theorem \ref{th:process} proves that visGP with covariance function $C^*$, constructed via covariance selection on the visibility graph (\ref{eq:covsel}) and extended to a process using (\ref{eq:process}), satisfies desirable properties at the process-level on the entire nonconvex domain $\calD$. Properties (a) and (b) ensure that the covariance function $C^*$ restricted to any convex subset $\calD_c \subset \calD$ agrees exactly with $C$, thereby preserving marginal stationarity and Euclidean distances on all of $\calD_c$. It thus ensures coherence with any sub-analysis of the data using $C^*$, restricted to a convex subset $\calD_c$ is equivalent to analysis using the Euclidean distance-based covariance function $C$. The Markov property (c) ensures conditional independence between two points not connected in $\calD$. This encodes the irregular nonconvex geometry of the domain, as correlation between the process realizations at these two points is likely to come from correlations of each of them with realizations at intermediate locations in the domain.

To illustrate the intuitive appeal of these properties more formally and how the resulting covariance between two points not connected in the domain is informed by the geometry of the domain, we now show that for a class of nonconvex domains and our approach yields the same covariance as using covariance directly on the (geodesic) through-domain distances. 

\begin{theorem}\label{thm:convex_union} Let $\calD \subset \mathbb R^d$ denote an irregular simply connected domain equipped with the geodesic distance $d_{geo}$ such that $\calD = \cup_{i=1}^J A_j$ where $A_j$'s are convex and $A_j \cap A_{j'}$ is either empty or contains a single location $s_{jj'}$. Let $C$ denote the exponential covariance on Euclidean distance in $\mathbb R^d$, i.e., $C(s_i,s_j) = \sigma^2 \exp(- \phi \|s_i - s_j\|)$ for $s_i, s_j \in \mathbb R^d$. Let $C^*$ denote a visGP constructed using a finite set of locations $\calV_n \subset \calD$ that contains all $s_{jj'}$. Then $C^*(s,s')=\sigma^2\exp(-\phi\; d_{geo}(s,s'))$ for all $s,s' \in \calV_n$. 
\end{theorem}

Theorem \ref{thm:convex_union} proves that for domains that can be represented as the union of convex domains touching at at-most a single point, a visGP constructed from a parent GP with an exponential covariance function with Euclidean distance has an exponential covariance function with the geodesic distance on the nonconvex domain. Figure \ref{fig:union} provides examples of domains that can be characterized in this way including tree-shaped domains (left), unions of polygons (middle), and curves (right) in a limiting sense. For these domains, the geodesic distances are exactly encoded in the visGP exponential covariance demonstrating explicitly how the Markov property on the visibility graph used to construct visGP encodes the geometry of the domain. 

% Figure environment removed

Note that the exponential covariance with geodesic distances, in general, does not yield valid covariance functions for nonconvex domains. Hence, an immediate corollary of Theorem \ref{thm:convex_union} is that we identify a class of nonconvex domains where it does, which is a result of independent importance. 

\begin{corollary}\label{cor:geodesic} Let $\calD \subset \mathbb R^d$ denote an irregular domain simply connected domain equipped with the geodesic distance $d_{geo}$ such that $\calD = \cup_{i=1}^J A_j$ where $A_j$'s are convex and $A_j \cap A_{j'}$ is either empty or contains a single location $s_{jj'}$. Then the exponential covariance using geodesic distances, i.e., $C_{geo}(s,s')=\sigma^2 \exp(-\phi\; d_{geo}(s,s'))$ is positive definite on $\calD$. 
\end{corollary}

The property from Theorem \ref{thm:convex_union} of $C^*$ being a covariance function on the geodesic distances cannot hold exactly in arbitrary domains; as we have stated, the covariance function based on geodesic distances is not guaranteed to be positive definite. However, Theorem \ref{thm:convex_union} provides a heuristic justification of the intuition that the covariance selection method should yield a covariance function similar to one based on geodesic distances, which would be a natural choice if the positive definite assumption was not violated. Additionally, we show in Section \ref{sec:simcov} that this property holds approximately even in domains excluded from the premise of Theorem \ref{thm:convex_union}. 


\section{Computational strategies}\label{sec:approx}
We now outline the algorithm to analyze geospatial data on nonconvex domains using visGP and provide strategies for improving scalability. Consider data $Y(s_i)$ and $X_i$ observed at locations $s_i$, for $i=1,\ldots,n$ in a nonconvex domain $\calD \subset \mathbb R^d$. Here $Y(s)$ is an univariate response and $X(s)$ is a $p$-dimensional covariate. We consider $\calV$ to be the set of data locations and define the visibility graph $\calG$ on $\calV$ as in Section \ref{sec:finite}. Note that if the data locations leave large gaps in the domain, one can always add more points to $\calV_n$ and define $\calG$ on this augmented set of locations. Let $\Sigma(\cdot,\cdot)$ a parent Euclidean covariance function on $\mathbb R^d$ that combines a spatial GP with Euclidean covariance $C(\cdot,\cdot)$ and a noise (nugget) process $\epsilon(s) \iid N(0,\tau^2)$. 
Let $w(\cdot)$ denote a visGP with covariance function $\Sigma^*$ based on $\Sigma=C + \tau^2 \delta$ where $\delta(s,s')=I(s=s')$ is the Kronecker delta. Then the visGP process model for analysis of the data is given by

    $Y = X(s)'\beta + w(s)$.

Defining $Y=(Y_1,\ldots,Y_n)'$ and $X$ similarly, the data model is 
\begin{equation}\label{eq:model}
    Y = N\Big(X\beta,\Sigma^*(\calV,\calV) \Big) \mbox{ where } \Sigma^*(\calV,\calV)=CovSel\big(C(\calV,\calV) + \tau^2 I),\calG\big).
\end{equation}

The parameters of the visGP covariance $\Sigma^*$ are simply the parameters $\theta$ of the original GP $C=C(\theta)$ and the nugget variance $\sigma^2$. Given these, the matrix $\Sigma^*(\calV,\calV)$ can be calculated using the iterative proportional scaling (IPS) procedure of \cite{speed}. Hence, all parameters $(\beta,\theta,\tau^2)$ can be estimated by maximizing the likelihood corresponding to (\ref{eq:model}).

For moderate to large sample sizes,  the IPS algorithm can be computationally intensive. It involves an iterative procedure which cycles through the cliques; at each cycle step, there is required an inversion of size $k \times k$ where $k$ is the size of the current clique. The cycles must be continued until suitable convergence, and the entire procedure must be repeated for each parameter value at which the likelihood is desired for optimization. We propose a few approximations which preserve the spirit of the method while minimizing computational overhead.

\subsection{Chordal completion}
We first consider computations for the setting where the visibility graph $\calG$ is a \textit{chordal} or \textit{decomposable} graph. A graph $\calG$ is said to be chordal if every one of its cycles of length four or greater has a chord. In graphical statistics, chordal graphs have attractive computational properties. In the present context, we make use of the following from \cite{lauritzen}.
The maximal cliques (i.e., complete subgraphs which are not contained in larger complete subgraphs) of a chordal graph $\calG$ admit a perfect ordering $(K_1, K_2, ..., K_k)$, i.e., one where we can write
\[
H_{j} = K_1 \cup ... \cup K_{j},\,
R_j = K_j \setminus H_{j-1},\,
s_j = H_{j-1} \cap K_j
\]

and $(H_{j-1}, R_j, S_j)$ is a \textit{decomposition}. In this context, this means that $S_j$ \textit{separates} $H_{j-1}$ from $R_j$; i.e., all paths from any vertex in $H_{j-1}$ to any vertex in $R_j$ goes through $S_j$. Hence, $S_j$ are referred to as ``separators," and since they are subgraphs of cliques, they are themselves cliques. For such a perfect ordering for the visibility graph $\calG$, the likelihood for the data model (\ref{eq:model}) is given by 
\begin{equation}\label{eq:chordal}
f(Y \given X, \beta, \theta, \tau^2) = \frac{\prod_{K \in \mathcal{K}} N(Y(K) \given X(K)'\beta, C(K,K) + \tau^2 I)
}{\prod_{S \in \mathcal{S}} N(Y(S) \given X(S)'\beta, C(S,S) + \tau^2 I)}.
\end{equation}
where $\mathcal{K}$ is the set of cliques, $\mathcal{S}$ is the set of separators, and for a set $A \subset \calV$, $Y(K)$ denotes the subset of $Y$ corresponding to locations in $A$; $X(A)$ is defined similarly. Note that this is a slight abuse of notation as it is possible for the same clique to appear multiple times in the sequence of separators, in which case $\mathcal{S}$ should be construed to contain the appropriate number of copies of each separator.

The representation (\ref{eq:chordal}) of the likelihood completely circumvents the IPS algorithm to calculate the covariance matrix $\Sigma(\calV,\calV)$. In fact, the large $n \times n$ matrix $\Sigma(\calV,\calV)$ need not be calculated at all as the likelihood decomposes along the smaller clique and separator likelihoods and of which is simply a likelihood using the original Euclidean GP with covariance $C + \tau^2 \delta$ as the cliques and separators are complete subgraphs. This allows the likelihood to be calculated significantly quicker, as long as the cliques and separators are small relative to the entire graph. 

For certain nonconvex domains, the visibility graph is naturally chordal. Examples include tree-shaped domains (Figure \ref{fig:union}, left), curves (Figure \ref{fig:union}, right), and rectangular ``U"-shaped domains, like the symbol $\bigsqcup$. 
For some others, such as those admitting a decomposition of convex domains as in Theorem \ref{thm:convex_union}, the graph can be pruned to be chordal by removing edges between points lying in different convex components, while exactly preserving the visGP covariance function. For other domains, we use a chordal completion as follows. If $\calG$ is \textit{not} chordal, then $\bar\calG$ is a \textit{chordal completion} of $\calG$ if $\bar\calG$ is a chordal graph of which $\calG$ is a subgraph. Intuitively, $\bar\calG$ contains the edges of $\bar\calG$ plus some edges which are required to serve as necessary subgraph chords. A \textit{minimal} chordal completion is a chordal completion from which no edge may be removed while maintaining a chordal graph; minimal chordal completions are not in general unique.
For computational simplicity, we use a linear-time chordal completion algorithm provided by the \texttt{igraph} software package \citep{csardi}; the graph returned is not guaranteed to be minimal. We replace $\calG$ with the approximate chordal graph $\bar\calG$ for parameter estimation by maximizing the likelihood (\ref{eq:chordal}) based on $\bar\calG$.

\subsection{Graph stochastic gradient descent}\label{sec:sgd}

Note that due to the likelihood decomposition (\ref{eq:model}) over the cliques and separators, we may write the log-likelihood of our model using a chordal graph as above as
\[
\log f(Y) = \sum_{K_i \in \mathcal{K}} \log f(Y(K_i))
-\sum_{S_i \in \mathcal{S}}  \log f(Y(S_i)) = \sum_i[ \log f(Y(K_i)) - \log f(Y(S_i))]
\]
where $f(Y(A))$ is the likelihood for $Y(A)$. Here we use the fact that each clique $K_i$ in the perfect ordering has a corresponding separator $S_i \subset K_i$ (defining $S_1$ to be the empty set). Thus the loss function optimized to obtain parameter estimates is additive over the clique-separator pairs $(K_i,S_i)$ in the perfect ordering of the graph.  The log-likelihood can be written as a sum of (differences of) Gaussian log-likelihoods over these pairs, and the total likelihood is amenable to maximization by stochastic gradient descent \citep{shaley}, a kind of gradient-based optimization in which at each iteration, the total gradient is approximated by a single component and the components are cycled over the iterations. SGD is widely used in the context of machine learning, and in particular in deep neural networks \citep{amari}. However, the core motivation of SGD is the ability to decompose the loss function into small similar components. In most applications, the ``components" are i.i.d. data points (or blocks). In spatial settings, all data are correlated, ruling out naive application of SGD. Instead, we exploit the decomposition above where the component is a clique-separator pair. This enables the evaluation of only a couple of Gaussian likelihoods (corresponding to some $K_i$ and $C_i$) at each iteration of the estimation, thereby massively reducing the computation burden. 

We use a version of stochastic gradient descent called ``Root Mean Square Propagation" or ``RMSProp" which is designed to improve convergence by using an exponentially weighted moving-average gradient \citep{goodfellow}. We describe this algorithm below.

\begin{algorithm}{\linewidth - 0.5 in}
\caption{Stochastic gradient descent for Gaussian likelihood maximization}
\begin{algorithmic}
    \State Set learning rate $\alpha$, decay rate $\beta$, small stability constant $\epsilon$, maximum number of iterations $T$, and initial parameter estimate $\hat\theta$
    \State Initialize accumulation variables $v= \mathbf{0}$
    \For{$t=1$ to $T$}
        \State Randomize the clique and separator sequence corresponding to the perfect ordering
        \For{i = 1 to number of cliques}
            \State For the $i^{th}$ clique $K_i$ and the $i^{th}$ separator $S_i$, compute gradient of log-likelihood:
                \State $g \gets \nabla_{{\theta}} \log L^{(i)} = \nabla_{{\theta}}[\log f(Y_{K_i})- \log f(Y_{S_i})]$
            \State Update accumulation variables:
                \State $v \gets \beta*v+(1-\beta)g*g$
            \State Compute step size and update parameter estimates:
                \State ${\theta} \gets {\theta} + \alpha g / \sqrt{v+\epsilon}$
        \EndFor
    \EndFor
\end{algorithmic}
\end{algorithm}



\subsection{Nearest neighbor clique likelihood}\label{sec:nngp}

The SGD helps improve scalability when the number of cliques in the perfect ordering is large. However, evaluating each clique (or separator) likelihood requires computation time that is cubic in the size of the clique which will be prohibitive if the size of some of the cliques themselves are large. Note that a clique likelihood $N(Y(K) \given X(K)'\beta, C(K,K) + \tau^2 I)$ is simply a standard GP likelihood with an Euclidean covariance over a set of points that are fully connected in the domain $\calD$ (i.e., the clique lies in a convex sub-domain of $\calD$). In Euclidean domains, local low-rank approximations like the nearest neighbor GP \citep{datta,datta2016} which assume independence of responses conditional on nearby neighboring sets of points, offer excellent linear-time approximations to the full GP likelihood within each clique. Note that the neighbor sets used within each clique to create an NNGP approximation to the clique likelihood is different from the neighbor sets created using the visibility graph and used to define the visGP process. 

\subsection{Distance thresholding}

Above, we defined $\calG = (\calV,E)$ be the adjacency graph, where connections indicated whether the line segment connecting a pair of points lay entirely within the nonconvex domain. For large datasets, one can amend this to a {\em distance-thresholded visibility graph} where two points are adjacent if and only if the corresponding line segment lies in the domain \textit{and} the distance between the points is bounded by some user-defined threshold distance $d_{max}$. This introduces sparsity into the adjacency matrix and can be expected not to significantly skew results as long as $d_{max}$-balls around observed points typically contain a fair number of other observations. In other words, as long as there are many neighbors within the threshold, the threshold itself may not be crucial as dependence still flows through the neighbor connections.

\subsection{Prediction strategies}\label{sec:prediction}

In Section \ref{sec:process} we demonstrated how the visGP can be extended from a finite set $\calV$ to a process on the entire domain $\calD$. The construction imposes minimal restrictions on the choice of the neighbor sets except that the neighbors of a location need to be connected through the domain to the location and to each other. This is because, if two neighbors are not connected through the domain, there is no clear distance we should associate with the pair. If the Euclidean distance is used, the geometry of the domain is not respected; if the through-domain distance is used, the corresponding covariance matrix may not be positive definite. Note that this issue was not addressed in \cite{bora}, who assume the Euclidean covariance among all locations in the conditioning set although they may not be connected through the domain when defining the nearest-neighbor kriging weights. 

Selecting neighbor sets that are mutually connected through the domain is critical to visGP possessing the desirable properties, as established in Theorem \ref{th:process}. 
We propose three different algorithms for this.

\begin{enumerate}
    \item Nearest clique (NC): In sequence, add the first nearest neighbor, second nearest neighbor, and so on, until adding the next nearest neighbor would form an incomplete subgraph. (This may create a neighbor set smaller than the prescribed size $k$.)

    Specifically, let $N^m (u_i)$ denote the $m^{th}$ nearest neighbor of $u_i$, where candidate neighbors only include points connected to $u_i$ through the domain, so for an ordinary $k$-nearest neighbors scheme we would have $N(u_i) = \{N^1 (u_i), N^2(u_i) , ..., N^k(u_i)\}$. For an ordered set of locations $A$, let $\mathbf H(A)=1$ if all locations in $A$ are mutually connected, and $0$ otherwise. Then we follow the algorithm below: 

   
    \begin{algorithm}{\linewidth - 0.5 in}
    \caption{Nearest clique algorithm}
    \begin{algorithmic}
\State $N \gets \emptyset$
\For{i = 1, 2, ..., k} 
\If{$\mathbf H( N \cup N^{i}(u_i) ) = 1$}
$N \gets  N \cup N^{i}(u)$
\Else

\textbf{break}
\EndIf
\EndFor

\Return $N$
\end{algorithmic}
\end{algorithm}



    \item Maximum precision (MP): Of all maximal cliques of the graph with $k$ nearest neighbors, we choose the clique whose conditional predictive variance is smallest. Specifically, let $\mathcal{Q}$ denote the set of maximal cliques of the visibility graph associated with $N(u_i)$. Then
    
\begin{minipage}{\linewidth - 0.5 in}
    \[
    N_{MP}(u_i) = \argmin_{Q \in \mathcal{Q}} \mathbf{C}_{u_i, u_i}- \mathbf{C}_{u_i, Q} \mathbf{C}^{-1}_{Q, Q} \mathbf{C}_{Q, u_i}
    \]
\end{minipage}

      
\vspace{1em}
  
    \item Precision-weighted (PW): 
    Instead of considering a single neighbor set, we consider a series of neighbor sets from non-overlapping cliques, starting with the largest nearest clique, then finding the next largest clique, and so on. The cliques are constrained to be non-overlapping by deleting them from the graph after they are selected, before beginning the search for the next clique. We calculate the kriging prediction for each clique and take the average of the predictions weighted by their conditional precision. Specifically, for a set of locations $A$, let $\mathbf{L}(A)$ return the largest clique that can be created from the members of $A$. Then
\[
    N_{PW}^{(1)}(u_i) = \mathbf{L}(N(u_i))
    \]
    \[
    N_{PW}^{(m)}(u_i) = \mathbf{L} (N(u_i) \setminus \cup_{p=1}^{m-1} N_{PW}^{(m)}(u_i) )
    \]
\[K^{(m)} = [\mathbf{C}_{u_i, u_i}- \mathbf{C}_{u_i, N_{PW}^{(m)}(u_i)} \mathbf{C}^{-1}_{ N_{PW}^{(m)}(u_i),  N_{PW}^{(m)}(u_i)} \mathbf{C}_{ N_{PW}^{(m)}(u_i), u_i}]^{-1}\]
\[
 \mu^{(m)} = \mathbf{C}_{u_i, N_{PW}^{(m)}(u_i)} \mathbf{C}^{-1}_{N_{PW}^{(m)}(u_i), N_{PW}^{(m)}(u_i)} \omega_S
\]
\[
E( w(u_i) \given \calV) = \frac{\sum_p K^{(m)} \mu^{(m)}}{\sum_p K^{(m)}}
\]

To illustrate a full model, if we let $w^{(m)}(u_i) \sim N(\mathbf{B}^{(m)}_u w(N_{PW}^{(m)}), \mathbf{F}^{(m)}_u) $ where $\mathbf{B}^{(m)},\mathbf{F}^{(m)}$ are constructed according to $N_{PW}^{(m)}$, then 
\[
w(u_i) | w(\calV) \,{\buildrel d \over =}\, \frac{\sum_p K^{(m)} \omega^{(m)}(u_i)}{\sum_p K^{(m)}}
\]
corresponds to the posterior (conditional) distribution for $w(u_i)$ where the evidence from each of the neighbor sets $N_{PW}^{(m)}$ is considered independently. 
\end{enumerate}


\section{Simulation study}
\subsection{Predictive performance}

We examine the performance of various methods by evaluating their predictive accuracy in a contrived nonconvex domain. There is an underlying fixed spatially-smooth function that generates the expected value of the spatial process at each point; the white-noise error variance beyond this function is varied. To create the fixed function, we use various ``source" points and calculate through-domain distances, as described below. The fixed function is calculated on $25,000$ points which are sampled for each run of the simulations.

We use a fork-shaped domain with four rectangular prongs which are spaced parallel to each other and connected by a base region (Figure \ref{fig:fork}). Using the source points $p_1 = (-5, -5), p_2 = (-3, -5), p_3 = (-1, -5), p_4=(1,-5)$, which lie at the base of each prong, we create a function over the domain as follows:

\vspace{1em}

\begin{minipage}{\linewidth - 0.5 in}
\[
d_i(s) = d_g(p_i, s ); \quad i=1,...,4
\]
\[
f^*(s) = d_1^2/3 + 3*\sin(d_3) - d_2*d_4 
\]
\[
f = \frac{f^* - mean(f^*)}{sd(f^*)}
\]
\end{minipage}


\vspace{1em}

where $d_g$ is the distance metric calculated through the domain, as above. The values of this function can be seen in Figure \ref{fig:fork_vals}.
% Figure environment removed
We then divide the region into test and training data, as shown in Figure \ref{fig:fork_groups}.

For sample sizes of $n=250$, $n=1200$, and $n=10,000$, we divided data into $80 \%$ training and $20\%$ test. To create the values of the spatial process, white noise with standard deviations of $sd=0.1, .25, 1$ was added to the underlying functions $f$, described above. Models were fit to the training data, and the point estimates and confidence/credible intervals were compared to the holdout set.

We fit four classes of models. A nearest-neighbor Gaussian process using Euclidean distances was fit using the \texttt{BRISC} package in \texttt{R} \citep{saha,brisc}, with 10 neighbors and an exponential covariance \citep{saha}. The GLGP model \citep{dunson} was fit with $K = 50$ eigenvectors and a grid search on the other parameters. Due to computational constraints, the GLGP results are omitted for the medium- and large-sample analyses, and interval estimates are not calculated for GLGP. BORA-GP \citep{bora} is fit with diffuse priors, exponential covariance, 10 nearest neighbors, and an ordering based on the first coordinate of $s$; if that ordering fails, the ordering by the second coordinate of $s$ is used. %In the U-shaped domain, the grid size for proxy neighbor sources is set to $0.01$ rather than the length of the barrier crossing, since the barrier has width zero. 
Finally, three versions of our visGP model are fit with exponential covariance, corresponding to the three prediction strategies described in detail above in Section \ref{sec:prediction}. The full likelihood function is used for the $n=250$ and $n=1,200$ scenarios, but the stochastic gradient descent strategy described in Section \ref{sec:sgd} is used for the $n=10,000$ scenario with $5,000$ clique iterations and a distance threshold of 1 unit.

Results are displayed graphically in Figure \ref{fig:sim_all_res} for the visGP and the three competing methods. The comparison between visGP and BORA-GP only is displayed in Figure \ref{fig:sim_zoom_res}. 

% Figure environment removed
 BORA-GP and the visGP methods tend to provide superior predictions; the Euclidean method is associated with significantly higher predictive error in the fork-shaped domain. This is expected. Surprisingly, the GLGP also yields very high prediction errors.
BORA-GP is somewhat less accurate than visGP in the fork-shaped domain. The predictive intervals for visGP are significantly shorter than the intervals for BORA-GP; both methods attain approximately 95\% coverage, though BORA-GP is occasionally slightly over-conservative and visGP is occasionally slightly anti-conservative. It is natural that Bayesian methods may have wider intervals and higher coverage since their estimates integrate uncertainty in the spatial parameter estimation. A Bayesian implementation of visGP, which can also be easily pursued (we do not do it here), would have wider interval estimates.The visGP method presented in Figures \ref{fig:sim_all_res} and \ref{fig:sim_zoom_res} used the maximum-precision prediction strategy. There are small differences in accuracy between the three prediction strategies for visGP, but none are clearly superior or inferior. Tables with full results for all three versions of visGP as well as the competing methods can be found in the appendix, Section \ref{sec:append_sim}. We note that small differences between methods may be idiosyncratic with respect to the domains under consideration as the likelihood function (with variable mean, range, nugget, and marginal variance) is somewhat overparametrized for a domain of fixed diameter (see \cite{zhang}) and many predictions rely on extrapolation due to the checkerboard pattern of the holdout set.

Finally, for a randomly selected simulation run in the fork-shaped domain with $n=10,000$, we compared the runtimes of the BORA-GP and visGP, from the point of the raw data of the domain shape, locations, and observed values to final test-set predictions. We used R version 4.0.3 and a local machine [Intel(R) Core(TM) i5-8265U CPU @ 1.60GHz-1.80 GHz] running Windows 10 x64. The results can be seen in Table \ref{tab:timetest}.

\begin{table}[h]
\centering

\begin{tabular}{lcccc}
\hline
Method & Neighbor-finding & Model-fitting & Prediction & Total\\
\hline
BORA-GP & 11.43 & 90.76 & 10.00 & 112.19\\
visGP   & 68.79 & 16.64 & 0.08 & 85.51\\
\hline
\end{tabular}
\caption{Computation times for the visGP and BORA-GP methods. Units are minutes.}
\end{table}\label{tab:timetest}

In total, the BORA-GP method took 112.19 minutes, while the visGP method took 85.51 minutes. The large majority of the computation time for BORA-GP was in model-fitting, while the large majority of the computation time for visGP was in constructing the adjacency matrix. The latter is a ``one-time" cost; multiple analyses (e.g., at different timepoints) can be accomplished without having to recalculate the adjacency matrix. This step may be more difficult in a less geometrically simple domain; however, the time could be greatly reduced by creating a distance threshold beyond which two points are considered non-adjacent (which we have done only as an approximation at the stage of likelihood optimization). The problem is also embarrassingly parallel, as every pair's adjacency can be calculated independently.

\subsection{Process properties}\label{sec:simcov}

In this Section, we compare empirically the properties of the non-Euclidean covariance functions from BORA-GP and visGP. 
As proved in Section \ref{sec:theory},  visGP preserves entries in the covariance matrix which correspond to points connected through the domain. In the following experiment, we demonstrate the extent to which this property is violated by the BORA-GP approach. We consider a $U$-shaped domain with side-lengths of 12 units and use a parent Euclidean variance-covariance based on the Mat\'{e}rn function with $\sigma^2=1, \nu =1$, and $\phi = 0.1$. From these values, we compute the induced variance-covariances for  BORA-GP and visGP. (A slight modification of the BORA-GP algorithm sets the grid size for proxy neighbor sources to $0.01$ rather than the length of the barrier crossing since the barrier has width zero.)

Figure \ref{fig:covcomp_locs} displays the domain and the locations for which the variance-covariance is computed. 
We first look at the variances. The visGP marginal variances are guaranteed to be $1$, equal to the variance of the parent GP. For BORA-GP, Figure \ref{fig:covcomp_ord} demonstrates that the nearest neighbor ordering strongly influences the marginal variances, with locations appearing later in the ordering having a decrease in the induced marginal variance. This dependence on ordering on process variance is an inherent aspect of the NNGP approximation and has also been observed in convex domains \citep{Datta2021}.  The effect of this can be partly mitigated by the use of random orderings where the decrease in variance does not get confined to one part of the domain. However, in the nonconvex setting, the impact of ordering is exacerbated because standard orderings (e.g, $x$- or $y$-coordinate) will lead to systematic discrepancies in modeled variance in different parts of the domain. A random ordering also cannot be used as it will lead to many points without any neighbors. This is because random ordering leads to the selection of many distant points in the neighbor set, and in a nonconvex domain there will be likely barriers between these points precluding inclusion of them into the neighbor set. 

% Figure environment removed

 We next look at the covariances. Figures \ref{fig:covcomp_vars_water} and \ref{fig:covcomp_vars_land} compare covariance values of the BORA-GP and visGP models with the raw values of a Mat\`ern function on geodesic distances. 
 When points are connected through the domain (Figure \ref{fig:covcomp_vars_water}), the geodesic distance corresponds to the Euclidean distance, and the covariances from visGP for these points are exactly identical
to the Mat\'ern covariances on these Euclidean distances. This property of visGP again, is guaranteed from Theorem \ref{th:process}, and together with preservation of the variances, ensures that a visGP analysis restricted to a convex subdomain coincides with standard GP analysis using Euclidean covariances. For BORA-GP, the deviation from the 45-degree line indicates discrepancy from Euclidean covariances on points connected in the domain and we see that these deviations are often quite large. 

Finally, in Figure \ref{fig:covcomp_vars_land}, we look at covariances for points not connected in the domain. Theorem \ref{thm:convex_union} has shown that for certain domains (as in Figure \ref{fig:union}) and choice of covariance function (exponential), the visGP covariance is exactly the covariance using the geodesic distance. However, this will not hold in general, as covariance functions using geodesic distances are not guaranteed to be positive definite, and visGP covariance is always positive definite. However, we see from Figure \ref{fig:covcomp_vars_land} that visGP retains this property approximately in other domains and other types of covariance functions. The visGP covariance is quite close to the covariance using the geodesic distance, thereby reflecting how the geometry of the domain is embedded into the visGP construction. Once again for BORA-GP, we see the association with the geodesic covariances is considerably weaker, demonstrating a loss of knowledge about the domain geometry to a greater extent. These figures show that, unlike existing methods, the visGP model exactly preserves Euclidean covariances on domain-connected points and roughly preserves geodesic covariances of non-domain-connected pairs. 
 The former is clear by the design of the visGP method; the latter observation is not obvious from construction but is somewhat predicted by the result of Theorem \ref{thm:convex_union}. 

\section{Application: acidity of the Chesapeake Bay}

The Chesapeake Bay is the ``largest, most productive, and most biologically diverse estuary in the United States," according to The Chesapeake Bay Program. Formally founded in 1983, the program has as its mission to protect and restore the Chesapeake Bay and its watershed through ecological monitoring and management in the face of human population growth and environmental degradation \citep{hood}.
One variable tracked by the Project's monitors is pH, which measures local acidity. pH level has been argued to be an important factor in maintaining an estuary system's biological health \citep{ringwood}. In the analysis below we examine average pH levels measured at each of 213 monitoring locations throughout the year 2021, which can be accessed at \texttt{https://data.chesapeakebay.net/}. 

% Figure environment removed

It is apparent that pH levels track the Bay's complicated geometry (see Figure \ref{fig:pH}) with considerable variability in levels from different channels of water that are close in Euclidean distance but far away in the geodesic or water distance. We compare the performance of different GP models for predicting the pH levels in this water body. We split the data into training and test sets in a checkered pattern with 85 training points and 128 test points. We compare four models -- Euclidean GP (fit by BRISC with the exponential covariance function and 15 neighbors) which ignores the water geometry, visGP (with the exponential covariance function and maximum-precision clique prediction with 15 neighbors),  BORA-GP model with 15 neighbors, and GLGP (only point predictions). For the non-Euclidean methods, we face an issue that the monitoring stations are so close to the shoreline (in fact, some of the stations appear to be inland relative to the specified boundary file) that many lack any connections to other points which are connected strictly in the water. To address this, we ``buffer" the Bay's boundary while respecting the broad contours of the Bay's geometry, as shown in Figure \ref{fig:pH_buff}.


The results for the four models are shown in Table \ref{tab:bay}. visGP outperforms the other methods in terms of prediction accuracy (MSE). All three methods producing interval estimates suffer some undercoverage, which may be explained by the data's relatively small sample size and the potential non-Gaussianity of the measurements. The prediction intervals from the visGP method are shorter than those from the other two, as seen in simulations. The poor performance of GLGP may be due to difficulties with parameter optimization by grid search. These results suggest that the assumptions underlying the visGP method fit well with the natural processes governing acidity levels in this domain and can be used to identify or predict areas of concern for protection or intervention, although uncertainty quantification can be problematic for all methods.

%to reproduce these results, make sure blockCV is version 2
\begin{table}[ht]
\centering
\begin{tabular}{lrrr}
  \hline
Method & MSE & Coverage & CI.length \\
  \hline
Euclidean & $7.834 \times 10^{-2}$ & $7.524 \times 10^{-1}$ & $8.605 \times 10^{-1}$ \\
  visGP (maximum precision) & $7.044 \times 10^{-2}$ & $7.089 \times 10^{-1}$ & $8.159 \times 10^{-1}$ \\
  BORA-GP & $7.661 \times 10^{-2}$ & $7.624 \times 10^{-1}$ & $8.861 \times 10^{-1}$ \\
  GLGP & $8.911 \times 10^{-2}$ &  &  \\
   \hline
\end{tabular}
\caption{Results of the Chesapeake Bay pH data analysis.}\label{tab:bay}
\end{table}

\section{Conclusion}\label{sec:}
Inference and prediction for spatial processes in nonconvex domains are often encountered in practice, but methodological contributions have been scarce until relatively recently, and many methods have taken the differential equation perspective to construct Gaussian processes in these domains. We have proposed an alternative that considers the perspective of the covariance function. Using visibility graphs in the domain, we present visGP -- a method that respects domain geometry by encoding it into a graph of adjacency relationships between points and exploiting Dempster's method of covariance selection to simultaneously enforce marginal and conditional covariance (Markovian) constraints. visGP preserves stationary variances and Euclidean covariances on points that are connected via straight lines through the domain of interest. These properties, verified both theoretically and empirically, are unique to visGP among the competing methods, and they ensure that any analysis restricted to a convex sub-domain of the nonconvex domain coincides exactly with traditional GP analysis using Euclidean covariances.

Computationally, we exploit the theory of chordal graphical models to achieve a computationally efficient algorithm for visGP. In all the simulations and the acidity level analysis, visGP performs well against state-of-the-art methods, emerging as the best or competitive with the best consistently. In terms of speed, it is the fastest algorithm even with a demanding one-time computation of the visibility graph. Further research will be dedicated to improving the computing time for this piece by leveraging parallel computing resources. We will also investigate the mathematical properties of parameter estimates in asymptotic regimes. More importantly, we will develop an open-access software for visGP for broader accessibility of the method to practitioners.

\bibliography{bib} 

\newpage
\setcounter{equation}{0}
\setcounter{theorem}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{section}{0}
\renewcommand\thesection{S\arabic{section}}
\renewcommand\theequation{S\arabic{equation}}
\renewcommand\thefigure{S\arabic{figure}}
\renewcommand\thetable{S\arabic{table}}
\renewcommand\thetheorem{S\arabic{theorem}}
\renewcommand\thelemma{S\arabic{lemma}}
\renewcommand\theproposition{S\arabic{proposition}}

\section*{{\Large Supplementary materials for ``Visibility graph-based covariance functions for scalable spatial analysis in nonconvex domains''}}\label{sec:nonconvex_append}

\vskip 0.3in

%\section{Appendix}
\section{Proofs}

\subsection{Proof of Theorem \ref{th:process}}
For any $s \notin \calV_n$, the neighbor sets $N(s)$ are already defined during the construction in Section \ref{sec:process}. For any $s \in \calV_n$, simply define $N(s)=\{s\}$. Then we can represent the process $w(\cdot)$ from (\ref{eq:finite}) and (\ref{eq:process}) as 
\begin{equation}\label{eq:ortho}
    w(s) = v(s)'w(\calV_n) + z(s) 
\end{equation}
where $v(s)$ is a vector encasing $B(s)$ and inserting zeros at locations in $\calV_n$ not corresponding to $N(s)$, and $z(s) \ind N(0,C(s,s)- C(s,N(s)) 
    C(N(s),N(s))^{-1}C(N(s),s))$ and $\{z(s) \given s \in \calD\} \perp w(\calV_n)$. Hence, we immediately have
\begin{align*}
    Var(w(s))&=v(s)'Var(w(\calV_n))v(s) + C(s,s)- C(s,N(s)) 
    C(N(s),N(s))^{-1}C(N(s),s))\\
    &=B(s)Var(w(N(s)))B(s)' + C(s,s)- C(s,N(s)) 
    C(N(s),N(s))^{-1}C(N(s),s))\\
    &=C(s,N(s))   C(N(s),N(s))^{-1}C^*_n(N(s),N(s))C(N(s),N(s))^{-1}C(N(s),s))\, + \\
    & \qquad C(s,s)- C(s,N(s)) 
    C(N(s),N(s))^{-1}C(N(s),s)).
\end{align*}

As the neighbor sets $N(s)$ are constructed to correspond to a clique (complete subgraph) of $\calG_n$ (section \ref{sec:process}), by the property of covariance selection (\ref{eq:covsel}), the covariances from the original covariance $C$ are preserved on the cliques. Hence, we have $C^*_n(N(s),N(s))=C(N(s),N(s))$. This implies $C^*_n(s,s) = Var(w(s)) = C(s,s)$. As $C$ is stationary, $C(s,s)=C(s',s')$ for all $s,s' \in \calD$, we immediately have $C^*_n(s,s)=C^*_n(s',s')$ for all $s,s' \in \calD$ and $w(\cdot)$ is marginally stationary on $\calD$  proving (a). 


To prove (b), for $s,s' \in \calV_n$ and connected in the domain, we exactly have $C^*_n(s,s')=C(s,s')$ for all $n$ from the properties of covariance selection. So the result is exact. We then consider $s,s' \notin \calV_n$ that are connected in $\calD$. Then, as $Z(s) \perp Z(s')$ and both are independent of $w(\calV_n)$, we have
\begin{align}\label{eq:cov}
    C^*_n(s,s') =& Cov(v(s)'w(\calV_n),v(s')'w(\calV_n)) \nonumber \\
    =& C(s,N(s))   C(N(s),N(s))^{-1}C^*_n(N(s),N(s'))C(N(s'),N(s'))^{-1}C(N(s'),s')).
\end{align}

As $\calD$ is open and $s,s'$ are connected in $\calD$,  we can find an open balls $O(s)$ and $O(s')$ around $s$ and $s'$ respectively such that the balls lie entirely within 
 $\calD$ and each point in $O(s)$ is connected to each point in $O(s')$ in $\calD$. As $\cup_n \calV_n$ is dense in $\calD$, for large enough $n$, $N(s) \subset O(s)$ and $N(s') \subset O(s')$ implying that $$C^*_n(N(s),N(s'))=C(N(s),N(s')) \mbox{ for large enough } n $$ due to the property of covariance selection. Additionally, as $\cup_n \calV_n$ is dense in $\calD$, for every $s$, as $n$ increases each member of $N(s)$ converges to $s$. Hence, assuming $C(s,s)=1$ without loss of generality (as $C$ is stationary), we have
 \begin{equation}\label{eq:lim1}
     \lim_n C^*_n(N(s),N(s'))= \lim_n  C(N(s),N(s')) = C(s,s')11' = C(s,s') \lim_n C(N(s),s)C(s',N(s')).
 \end{equation}
 
 Now consider a GP $u(\cdot)$ on $\calD$ equipped with the Euclidean stationary covariance function $C$. Then
 $Var\big(u(s) \given u(N(s)\big) \leq Var\big(u(s) \given u(N(s)[1]\big)$ where $N(s)[1]$ denotes the first member of $N(s)$ implying
 \begin{equation}\label{eq:lim2}
     %1 \geq 
     0 \leq \lim_n 1 - B(s)C(N(s),s) \leq \lim_n 1 - C(s,N(s)[1])^2 = 0, \mbox{ i.e., } \lim_n B(s)C(N(s),s) = 1.
 \end{equation}
 %where the last limit comes from every member of $N(s)$ approaching $s$ as $n \to \infty$. %we have $B(s)'C(N(s),s) \to 1$ for all $s$. 

Combining (\ref{eq:lim1}) and (\ref{eq:lim2}) in (\ref{eq:cov}), we prove part (b) as follows:
\begin{align*}\label{eq:cov2}   
    \lim_n C^*_n(s,s') =& C(s,s') \lim_n B(s)C(N(s),s)C(s',N(s'))B(s')' \\
    &+ \lim_n B(s) \Big[C(N(s),N(s')) -  C(s,s') C(N(s),s) C(N(s'),s') \Big]B(s')' \\
    =& C(s,s') \pm \lim_n \Big\|C(N(s),N(s')) -  C(s,s') C(N(s),s) C(N(s'),s') \Big\|_2\|B(s)\|_2\|_2B(s')\|_2 \\
    =& C(s,s') \pm 
    \lim_n o_p(1) M^2\\
    =& C(s,s'). 
\end{align*}


For part (c) we first consider $s,s' \in \calV_n$. 
Note that for $s,s' \in \calV_n$, the $\sigma$-algebra generated by $\{w(u) \given u \in \calD \setminus \{s,s'\}\}$ is same as the $\sigma$-algebra generated by $\{w(u) \given u \in \calV_n \setminus\{s,s'\}\} \cup \{ Z(u) \given u \in \calD \setminus \{s,s'\}\}$. It is easy to see that the former $\sigma$-algebra is generated by the latter (follows directly from \ref{eq:process}). The converse is true because $z(u)=0$ for all $u \in \calV_n$. So we can write the conditional covariance 
 \begin{align*}
     Cov\Big(w(s),w(s') &\given \big\{w(u) \given u \in \calD \setminus \{s,s'\}\big\}\Big)\\
     =& Cov\Big(w(s),w(s') \given \{w(u) \given u \in \calV_n \setminus \{s,s'\}\} \cup \{ Z(u) \given u \in \calD \setminus \{s,s'\}\} \Big) \\
     =& Cov\Big(w(s),w(s') \given \{w(u) \given u \in \calV_n \setminus \{s,s'\}\} \cup \{ Z(u) \given u \in \calD \setminus \{s,s'\}\} \Big)\\
     =& Cov\Big(w(s),w(s') \given \{w(u) \given u \in \calV_n \setminus \{s,s'\}\} \Big) \\
     =& (L^{-1})_{s,s'} \mbox{ where } L=C^*(\calV_n,\calV_n) \mbox{ from (\ref{eq:covsel})}\\
     =&0 \mbox{ as } s,s' \mbox{ are not connected in } \calD.
 \end{align*}
Here we could drop all $z$ terms from the conditioning sets as $\{z(u)\}$ is a collection of independent random variables. This proves the Markovian property (c) for all $s,s' \in \calV_n$ not connected in $\calD$ where the conditioning set is the $\sigma$-algebra generated by the entire process excluding the realizations at these two points. The result for the case where one of $s$ or $s'$ is not $\calV_n$ is true for any construction of the form (\ref{eq:process}) noting that as $\calV_n$ grows dense, the two points will have no neighbors in common.


\subsection{Proof of Theorem \ref{thm:convex_union}}

Because there is a simple induction step to connect additional convex parts, it suffices to prove the result for a nonconvex domain $\mathcal{D}$ can be decomposed into two smaller convex domains sharing one point in common, like the symbol for the number 8. Label one of the subdomains $\mathcal{A}$ and the other $\mathcal{B}$. Let $O$ denote the point that $\mathcal{A}$ and $\mathcal{B}$ have in common

Let $d_e(.,.)$ denote Euclidean distance and $d_g(.,.)$ denote geodesic distance. 

For $d \in \mathcal{D}$, let $|d| = d_e(d, O)$. Then for $d_1, d_2 \in \mathcal{D}$, 

\[ d_g(d_1, d_2) = \begin{cases} 
      d_e(d_1, d_2) & d_1, d_2 \in \mathbf{A} \quad or \quad d_1, d_2 \in \mathcal{B} \\
      |d_1| + |d_2| & else \\
   \end{cases}
\]

Let $\mathbf{A} = a_1, a_2, ..., a_n \in \mathcal{A}$ and $\mathbf{B} = b_1, b_2, ..., b_m \in \mathcal{B}$. Take $S=(a_1, ..., a_n, O, b_1, ..., b_m)$.

Let $C_e$ denote the exponential covariance function with Euclidean distance and $C_w$ denote the exponential covariance function with water distance. 

Let $C^A = (\Cov(\omega(a_1), \omega(O)),..., \Cov(\omega(a_n), \omega(O))$ and define $C^B$ similarly.

Assume $\omega$ is a mean-zero Gaussian process with exponential $(\phi, \sigma^2)$ covariance function on water distances. 

Let $C^\dagger(s_i, s_j) = C_e(s_i, s_j) -\sigma^{-2}  C_e(s_i, O) C_e(s_j, O)$

Then it suffices to show that
\[
\begin{pmatrix} \omega(\mathbf{A}) \\ \omega(O) \\ \omega(\mathbf{B})
\end{pmatrix} \,{\buildrel d \over =}\, \begin{pmatrix} \sigma^{-2} \omega(O) C^A +\mathbf{z_1}  \\ \omega(O) \\  \sigma^{-2} \omega(O) C^B +\mathbf{z_2}  \end{pmatrix}
\]

where $\mathbf{z_1}, \mathbf{z_2}, \omega(O)$ are mutually independent, $\mathbf{z_1} \sim GP(0, C^\dagger(\mathbf{A}))$, and $\mathbf{z_2} \sim GP(0, C^\dagger(\mathbf{B}))$. This is sufficient for the theorem because in the right-hand-side expression, it is clear that $\omega(\mathbf{A}) \perp \omega(\mathbf{B}) | \omega(O)$, and a pair of points can only be disconnected if one belongs to $\mathbf{A}$ while the other belongs to $\mathbf{B}$.

The $i,j$ entry of $\Var (  \sigma^{-2} \omega(O) C^A +\mathbf{z_1})$ is given by
\[
\sigma^{-4} \Cov(C_e(a_i, O)\omega(O),  C_e(a_j, O) \omega(O)) + C^\dagger(a_i, a_j)\]
\[
= \sigma^{-2} C_e(a_i, O) C_e(a_j, O) + C_e(a_i, a_j) -\sigma^{-2} C_e(a_i, O)  C_e(a_j, O)  
\]
\[
= C_e(a_i, a_j) = C_w(a_i, a_j)
\]

So $\Var(\omega(\mathbf{A}))  = \Var (  \sigma^{-2} \omega(O) C^A +\mathbf{z_1})$, and similarly, $\Var(\omega(\mathbf{B}) ) = \Var (  \sigma^{-2} \omega(O) C^B +\mathbf{z_2})$.

Also,

\[
\Cov(\omega(O), \sigma^{-2} \Cov(\omega(O), \omega(a_i)) \omega(O) + \mathbf{z}_{1i})\]
\[=\sigma^{-2} \Cov(\omega(O), \omega(a_i)) \Cov(\omega(O), \omega(O))
\]
\[
= \Cov(\omega(O), \omega(a_i))
\]

Similarly, \[
\Cov(\omega(O), \sigma^{-2} \Cov(\omega(O), \omega(b_i)) \omega(O) + \mathbf{z}_{2i})=\Cov(\omega(O), \omega(b_i))\]

Finally,

\[
\Cov(\sigma^{-2} \Cov(\omega(O), \omega(a_i)) \omega(O) + \mathbf{z}_{1i}, \sigma^{-2}\Cov(\omega(O), \omega(b_i)) \omega(O) + \mathbf{z}_{2i})
\]
\[
= \Cov(\omega(O), \omega(a_i))\Cov(\omega(O), \omega(b_i))\sigma^{-4}\Cov( \omega(O) ,  \omega(O) )
\]
\[
=\Cov(\omega(O), \omega(a_i))\Cov(\omega(O), \omega(b_i))\sigma^{-2}
\]
\[
= \{\sigma^2\exp(-\phi ||a_i||)\}\{\sigma^2 \exp(-\phi ||b_i||)\} (\sigma^2)^{-1} 
\]
\[
= \sigma^2 \exp(-\phi ( ||a_i|| + ||b_i||))
\]
\[
= C_w(a_i, b_i) = \Cov(\omega(a_i), \omega(b_i))
\]

Thus, the two vectors in question have the same variance. (Since it is clear they are multivariate Gaussian with zero mean, they are equal in distribution.)

\section{Detailed simulation results}\label{sec:append_sim}
\subsection{Simulation results in the fork-shaped domain}
%\addtolength{\oddsidemargin}{-.35in}
	%\addtolength{\evensidemargin}{-.35in}
\begin{longtable}{p{0.05\linewidth} | p{0.045\linewidth} | p{0.315\linewidth} | p{0.165\linewidth}|p{0.045\linewidth} |p{0.165\linewidth} }
\centering
n & $\sigma_{nug}$ & Method & MSE & CP & CI length \\
  \hline
$250$ & $0.1$ & BORA-GP & $5.450 \times 10^{-2}$ & 98\% & $9.806 \times 10^{-1}$ \\ 
  $250$ & $0.1$ & visGP: Maximum precision & $4.084 \times 10^{-2}$ & 92\% & $6.133 \times 10^{-1}$ \\ 
  $250$ & $0.1$ & visGP: Nearest clique & $4.160 \times 10^{-2}$ & 92\% & $6.176 \times 10^{-1}$ \\ 
  $250$ & $0.1$ & visGP: Precision-weighted & $4.046 \times 10^{-2}$ & 92\% & $5.974 \times 10^{-1}$ \\ 
  $250$ & $0.1$ & visGP: Standard kriging & $4.045 \times 10^{-2}$ & 92\% & $6.105 \times 10^{-1}$ \\ 
  $250$ & $0.1$ & Euclidean & $8.833 \times 10^{-1}$ & 75\% & $1.259 \times 10^{0}$ \\ 
  $250$ & $0.1$ & GLGP & $8.784 \times 10^{-1}$ &  &  \\ 
  $250$ & $0.25$ & BORA-GP & $1.464 \times 10^{-1}$ & 97\% & $1.548 \times 10^{0}$ \\ 
  $250$ & $0.25$ & visGP: Maximum precision & $1.108 \times 10^{-1}$ & 94\% & $1.203 \times 10^{0}$ \\ 
  $250$ & $0.25$ & visGP: Nearest clique & $1.115 \times 10^{-1}$ & 94\% & $1.206 \times 10^{0}$ \\ 
  $250$ & $0.25$ & visGP: Precision-weighted & $1.097 \times 10^{-1}$ & 93\% & $1.165 \times 10^{0}$ \\ 
  $250$ & $0.25$ & visGP: Standard kriging & $1.096 \times 10^{-1}$ & 94\% & $1.197 \times 10^{0}$ \\ 
  $250$ & $0.25$ & Euclidean & $9.344 \times 10^{-1}$ & 81\% & $1.813 \times 10^{0}$ \\ 
  $250$ & $0.25$ & GLGP & $8.290 \times 10^{-1}$ &  &  \\ 
  $250$ & $1$ & BORA-GP & $1.287 \times 10^{0}$ & 95\% & $4.398 \times 10^{0}$ \\ 
  $250$ & $1$ & visGP: Maximum precision & $1.183 \times 10^{0}$ & 94\% & $4.189 \times 10^{0}$ \\ 
  $250$ & $1$ & visGP: Nearest clique & $1.184 \times 10^{0}$ & 94\% & $4.191 \times 10^{0}$ \\ 
  $250$ & $1$ & visGP: Precision-weighted & $1.177 \times 10^{0}$ & 93\% & $4.038 \times 10^{0}$ \\ 
  $250$ & $1$ & visGP: Standard kriging & $1.179 \times 10^{0}$ & 94\% & $4.179 \times 10^{0}$ \\ 
  $250$ & $1$ & Euclidean & $2.015 \times 10^{0}$ & 90\% & $4.532 \times 10^{0}$ \\ 
  $250$ & $1$ & GLGP & $2.047 \times 10^{0}$ &  &  \\ 
  $1200$ & $0.1$ & BORA-GP & $4.703 \times 10^{-2}$ & 96\% & $7.688 \times 10^{-1}$ \\ 
  $1200$ & $0.1$ & visGP: Maximum precision & $3.765 \times 10^{-2}$ & 88\% & $5.265 \times 10^{-1}$ \\ 
  $1200$ & $0.1$ & visGP: Nearest clique & $3.790 \times 10^{-2}$ & 88\% & $5.277 \times 10^{-1}$ \\ 
  $1200$ & $0.1$ & visGP: Precision-weighted & $3.762 \times 10^{-2}$ & 88\% & $5.225 \times 10^{-1}$ \\ 
  $1200$ & $0.1$ & visGP: Standard kriging & $3.760 \times 10^{-2}$ & 88\% & $5.261 \times 10^{-1}$ \\ 
  $1200$ & $0.1$ & Euclidean & $1.037 \times 10^{0}$ & 80\% & $9.068 \times 10^{-1}$ \\ 
  $1200$ & $0.25$ & BORA-GP & $1.190 \times 10^{-1}$ & 96\% & $1.342 \times 10^{0}$ \\ 
  $1200$ & $0.25$ & visGP: Maximum precision & $9.795 \times 10^{-2}$ & 93\% & $1.121 \times 10^{0}$ \\ 
  $1200$ & $0.25$ & visGP: Nearest clique & $9.807 \times 10^{-2}$ & 93\% & $1.122 \times 10^{0}$ \\ 
  $1200$ & $0.25$ & visGP: Precision-weighted & $9.785 \times 10^{-2}$ & 93\% & $1.112 \times 10^{0}$ \\ 
  $1200$ & $0.25$ & visGP: Standard kriging & $9.778 \times 10^{-2}$ & 93\% & $1.121 \times 10^{0}$ \\ 
  $1200$ & $0.25$ & Euclidean & $1.098 \times 10^{0}$ & 82\% & $1.463 \times 10^{0}$ \\ 
  $1200$ & $1$ & BORA-GP & $1.212 \times 10^{0}$ & 95\% & $4.269 \times 10^{0}$ \\ 
  $1200$ & $1$ & visGP: Maximum precision & $1.144 \times 10^{0}$ & 95\% & $4.151 \times 10^{0}$ \\ 
  $1200$ & $1$ & visGP: Nearest clique & $1.145 \times 10^{0}$ & 95\% & $4.152 \times 10^{0}$ \\ 
  $1200$ & $1$ & visGP: Precision-weighted & $1.144 \times 10^{0}$ & 94\% & $4.113 \times 10^{0}$ \\ 
  $1200$ & $1$ & visGP: Standard kriging & $1.143 \times 10^{0}$ & 95\% & $4.149 \times 10^{0}$ \\ 
  $1200$ & $1$ & Euclidean & $2.067 \times 10^{0}$ & 89\% & $4.339 \times 10^{0}$ \\ 
  $10000$ & $0.1$ & BORA-GP & $4.596 \times 10^{-2}$ & 95\% & $6.992 \times 10^{-1}$ \\ 
  $10000$ & $0.1$ & visGP: Maximum precision & $3.787 \times 10^{-2}$ & 84\% & $4.657 \times 10^{-1}$ \\ 
  $10000$ & $0.1$ & visGP: Nearest clique & $3.792 \times 10^{-2}$ & 84\% & $4.659 \times 10^{-1}$ \\ 
  $10000$ & $0.1$ & visGP: Precision-weighted & $3.786 \times 10^{-2}$ & 84\% & $4.645 \times 10^{-1}$ \\ 
  $10000$ & $0.1$ & visGP: Standard kriging & $3.786 \times 10^{-2}$ & 84\% & $4.656 \times 10^{-1}$ \\ 
  $10000$ & $0.1$ & Euclidean & $1.131 \times 10^{0}$ & 83\% & $7.021 \times 10^{-1}$ \\ 
  $10000$ & $0.25$ & BORA-GP & $1.157 \times 10^{-1}$ & 96\% & $1.331 \times 10^{0}$ \\ 
  $10000$ & $0.25$ & visGP: Maximum precision & $9.988 \times 10^{-2}$ & 91\% & $1.070 \times 10^{0}$ \\ 
  $10000$ & $0.25$ & visGP: Nearest clique & $9.993 \times 10^{-2}$ & 91\% & $1.071 \times 10^{0}$ \\ 
  $10000$ & $0.25$ & visGP: Precision-weighted & $9.986 \times 10^{-2}$ & 91\% & $1.068 \times 10^{0}$ \\ 
  $10000$ & $0.25$ & visGP: Standard kriging & $9.983 \times 10^{-2}$ & 91\% & $1.070 \times 10^{0}$ \\ 
  $10000$ & $0.25$ & Euclidean & $1.171 \times 10^{0}$ & 84\% & $1.351 \times 10^{0}$ \\ 
  $10000$ & $1$ & BORA-GP & $1.211 \times 10^{0}$ & 95\% & $4.301 \times 10^{0}$ \\ 
  $10000$ & $1$ & visGP: Maximum precision & $1.191 \times 10^{0}$ & 94\% & $4.099 \times 10^{0}$ \\ 
  $10000$ & $1$ & visGP: Nearest clique & $1.192 \times 10^{0}$ & 94\% & $4.099 \times 10^{0}$ \\ 
  $10000$ & $1$ & visGP: Precision-weighted & $1.191 \times 10^{0}$ & 94\% & $4.086 \times 10^{0}$ \\ 
  $10000$ & $1$ & visGP: Standard kriging & $1.191 \times 10^{0}$ & 94\% & $4.098 \times 10^{0}$ \\ 
  $10000$ & $1$ & Euclidean & $2.030 \times 10^{0}$ & 89\% & $4.342 \times 10^{0}$ \\ 
   \hline
\caption{Simulation results in the fork-shaped domain. Columns give the sample size, standard deviation of the nugget, estimation method, mean square prediction error, confidence/credible interval coverage probability, and mean confidence interval length, respectively.}
\label{tab:sim_fork_res}
\end{longtable}

\section{Buffered Chesapeake domain}

% Figure environment removed
% Entries are in the refs.bib file
\end{document}
