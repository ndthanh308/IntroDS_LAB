\begin{thebibliography}{}

\bibitem[Amato et~al., 2022]{amato2022wavelet}
Amato, U., Antoniadis, A., Feis, I.~D., and Gijbels, I. (2022).
\newblock Wavelet-based robust estimation and variable selection in
  nonparametric additive models.
\newblock {\em Statistics and Computing}, 32:1--19.

\bibitem[Aronow, 2016]{aronow2016data}
Aronow, P.~M. (2016).
\newblock Data-adaptive causal effects and superefficiency.
\newblock {\em Journal of Causal Inference}, 4(2).

\bibitem[Barber and Cand{\`e}s, 2015]{barber2015controlling}
Barber, R.~F. and Cand{\`e}s, E.~J. (2015).
\newblock Controlling the false discovery rate via knockoffs.

\bibitem[Bauer et~al., 1988]{bauer1988model}
Bauer, P., P{\"o}tscher, B.~M., and Hackl, P. (1988).
\newblock Model selection by multiple test procedures.
\newblock {\em Statistics}, 19(1):39--44.

\bibitem[Belloni et~al., 2012]{belloni2012sparse}
Belloni, A., Chen, D., Chernozhukov, V., and Hansen, C. (2012).
\newblock Sparse models and methods for optimal instruments with an application
  to eminent domain.
\newblock {\em Econometrica}, 80(6):2369--2429.

\bibitem[Belloni and Chernozhukov, 2013]{belloni2013least}
Belloni, A. and Chernozhukov, V. (2013).
\newblock Least squares after model selection in high-dimensional sparse
  models.

\bibitem[Belloni et~al., 2014]{belloni2014inference}
Belloni, A., Chernozhukov, V., and Hansen, C. (2014).
\newblock Inference on treatment effects after selection among high-dimensional
  controls.
\newblock {\em The Review of Economic Studies}, 81(2):608--650.

\bibitem[Belloni et~al., 2013]{belloni2013honest}
Belloni, A., Chernozhukov, V., and Wei, Y. (2013).
\newblock Honest confidence regions for a regression parameter in logistic
  regression with a large number of controls.
\newblock Technical report, cemmap working paper.

\bibitem[Benkeser et~al., 2020]{ATEsupereff}
Benkeser, D., Cai, W., and Laan, M. (2020).
\newblock A nonparametric super-efficient estimator of the average treatment
  effect.
\newblock {\em Statistical Science}, 35:484--495.

\bibitem[Benkeser and van~der Laan, 2016]{HAL2016}
Benkeser, D. and van~der Laan, M. (2016).
\newblock The highly adaptive lasso estimator.
\newblock {\em International Conference on Data Science and Advanced
  Analytics}, pages 689--696.

\bibitem[Berk et~al., 2013]{modelselectionbias}
Berk, R., Brown, L., Buja, A., Zhang, K., and Zhao, L. (2013).
\newblock Valid post-selection inference.
\newblock {\em The Annals of Statistics}, 41(2).

\bibitem[Bibaut and van~der Laan, 2019]{bibautHAL}
Bibaut, A.~F. and van~der Laan, M.~J. (2019).
\newblock Fast rates for empirical risk minimization over
  c$\backslash$adl$\backslash$ag functions with bounded sectional variation
  norm.
\newblock {\em arXiv preprint arXiv:1907.09244}.

\bibitem[Bickel et~al., 1993]{bickel1993efficient}
Bickel, P.~J., Klaassen, C.~A., Ritov, Y., and Wellner, J. (1993).
\newblock {\em Efficient and adaptive estimation for semiparametric models},
  volume~4.
\newblock Johns Hopkins University Press Baltimore.

\bibitem[Bradic et~al., 2019]{chernoapproxSparse2019}
Bradic, J., Chernozhukov, V., Newey, W.~K., and Zhu, Y. (2019).
\newblock Minimax semiparametric learning with approximate sparsity.
\newblock {\em arXiv preprint arXiv:1912.12213}.

\bibitem[Breiman, 2001]{breiman2001random}
Breiman, L. (2001).
\newblock Random forests.
\newblock {\em Machine learning}, 45:5--32.

\bibitem[B{\"u}hlmann, 1999]{buhlmann1999efficient}
B{\"u}hlmann, P. (1999).
\newblock Efficient and adaptive post-model-selection estimators.
\newblock {\em Journal of Statistical Planning and Inference}, 79(1):1--9.

\bibitem[Bunea, 2004]{bunea2004consistent}
Bunea, F. (2004).
\newblock Consistent covariate selection and post model selection inference in
  semiparametric regression.
\newblock {\em The Annals of Statistics}, 32(3):898--927.

\bibitem[Cai and Guo, 2017]{cai2017confidence}
Cai, T.~T. and Guo, Z. (2017).
\newblock Confidence intervals for high-dimensional linear regression: Minimax
  rates and adaptivity.

\bibitem[Cai and van~der Laan, 2019]{bootstrapHAL}
Cai, W. and van~der Laan, M. (2019).
\newblock Nonparametric bootstrap inference for the targeted highly adaptive
  lasso estimator.

\bibitem[Cand{\`e}s et~al., 2016]{candes2016panning}
Cand{\`e}s, E.~J., Fan, Y., Janson, L., and Lv, J. (2016).
\newblock {\em Panning for gold: Model-free knockoffs for high-dimensional
  controlled variable selection}, volume 1610.
\newblock Department of Statistics, Stanford University Stanford, CA, USA.

\bibitem[Carone et~al., 2019]{MarcoComputerized}
Carone, M., Luedtke, A.~R., and van~der Laan, M.~J. (2019).
\newblock Toward computerized efficient estimation in infinite-dimensional
  models.
\newblock {\em Journal of the American Statistical Association},
  114(527):1174--1190.
\newblock PMID: 32405108.

\bibitem[Chatterjee and Lahiri, 2013]{adaptLassoBadPerf}
Chatterjee, A. and Lahiri, S. (2013).
\newblock Rates of convergence of the adaptive lasso estimators to the oracle
  distribution and higher order refinements by the bootstrap.
\newblock {\em The Annals of Statistics}, 41.

\bibitem[Chen, 2007]{spnpsieve}
Chen, X. (2007).
\newblock Large sample sieve estimation of semi-nonparametric models.
\newblock In Heckman, J. and Leamer, E., editors, {\em Handbook of
  Econometrics}, volume~6B, chapter~76. Elsevier, 1 edition.

\bibitem[Chen and Liao, 2014]{sieveOneStepPlugin}
Chen, X. and Liao, Z. (2014).
\newblock Sieve m inference on irregular parameters.
\newblock {\em Journal of Econometrics}, 182(1):70--86.
\newblock Causality, Prediction, and Specification Analysis: Recent Advances
  and Future Directions.

\bibitem[Chernozhukov et~al., 2018a]{DoubleML}
Chernozhukov, V., D., C., Demirer, M., Duflo, E., Hansen, C., Newey, W., and
  Robins, J. (2018a).
\newblock Double/debiased machine learning for treatment and structural
  parameters.
\newblock {\em The Econometrics Journal}, 21:C1--C68.

\bibitem[Chernozhukov et~al., 2018b]{chernoRegRiesz}
Chernozhukov, V., Newey, W., and Singh, R. (2018b).
\newblock De-biased machine learning of global and local parameters using
  regularized riesz representers.

\bibitem[Chernozhukov et~al., 2022]{chernozhukov2022automatic}
Chernozhukov, V., Newey, W., Singh, R., and Syrgkanis, V. (2022).
\newblock Automatic debiased machine learning for dynamic treatment effects and
  general nested functionals.

\bibitem[Chernozhukov et~al., 2018c]{chernozhukov2018auto}
Chernozhukov, V., Newey, W.~K., and Robins, J. (2018c).
\newblock Double/de-biased machine learning using regularized riesz
  representers.
\newblock Technical report, cemmap working paper.

\bibitem[Claeskens and Carroll, 2007]{claeskens2007asymptotic}
Claeskens, G. and Carroll, R.~J. (2007).
\newblock An asymptotic theory for model selection inference in general
  semiparametric problems.
\newblock {\em Biometrika}, 94(2):249--265.

\bibitem[Crump et~al., 2006]{imbensOverlapEstimand2006}
Crump, R.~K., Hotz, V.~J., Imbens, G., and Mitnik, O. (2006).
\newblock Moving the goalposts: Addressing limited overlap in the estimation of
  average treatment effects by changing the estimand.

\bibitem[Cui and Tchetgen, 2019]{cui2019selective}
Cui, Y. and Tchetgen, E.~T. (2019).
\newblock Selective machine learning of doubly robust functionals.
\newblock {\em arXiv preprint arXiv:1911.02029}.

\bibitem[Donoho et~al., 2005]{donoho2005stable}
Donoho, D.~L., Elad, M., and Temlyakov, V.~N. (2005).
\newblock Stable recovery of sparse overcomplete representations in the
  presence of noise.
\newblock {\em IEEE Transactions on information theory}, 52(1):6--18.

\bibitem[D’Amour et~al., 2021]{d2021overlap}
D’Amour, A., Ding, P., Feller, A., Lei, L., and Sekhon, J. (2021).
\newblock Overlap in observational studies with high-dimensional covariates.
\newblock {\em Journal of Econometrics}, 221(2):644--654.

\bibitem[Efron and Tibshirani, 1994]{efron1994introduction}
Efron, B. and Tibshirani, R.~J. (1994).
\newblock {\em An introduction to the bootstrap}.
\newblock CRC press.

\bibitem[Fan and Li, 2001]{oracleSelectSCAD}
Fan, J. and Li, R. (2001).
\newblock Variable selection via nonconcave penalized likelihood and its oracle
  properties.
\newblock {\em Journal of the American Statistical Association},
  96(456):1348--1360.

\bibitem[Fang et~al., 2021]{fang2021multivariate}
Fang, B., Guntuboyina, A., and Sen, B. (2021).
\newblock Multivariate extensions of isotonic regression and total variation
  denoising via entire monotonicity and hardy--krause variation.
\newblock {\em The Annals of Statistics}, 49(2):769--792.

\bibitem[Fithian et~al., 2015]{fithian2015selective}
Fithian, W., Taylor, J., Tibshirani, R., and Tibshirani, R. (2015).
\newblock Selective sequential model selection.
\newblock {\em arXiv preprint arXiv:1512.02565}.

\bibitem[Freedman, 2006]{freedman2006MLErobust}
Freedman, D.~A. (2006).
\newblock On the so-called “huber sandwich estimator” and “robust
  standard errors”.
\newblock {\em The American Statistician}, 60(4):299--302.

\bibitem[Friedman, 1991a]{Friedman1991}
Friedman, J. (1991a).
\newblock Multivariate adaptive regression splines.
\newblock {\em The Annals of Statistics}, 19:1--67.

\bibitem[Friedman, 1991b]{friedman1991multivariate}
Friedman, J.~H. (1991b).
\newblock Multivariate adaptive regression splines.
\newblock {\em The annals of statistics}, 19(1):1--67.

\bibitem[Goeman and Solari, 2022]{condSelectInf}
Goeman, J. and Solari, A. (2022).
\newblock Conditional versus unconditional approaches to selective inference.

\bibitem[Guo and Shah, 2023]{guo2023rank}
Guo, F.~R. and Shah, R.~D. (2023).
\newblock Rank-transformed subsampling: inference for multiple data splitting
  and exchangeable p-values.
\newblock {\em arXiv preprint arXiv:2301.02739}.

\bibitem[H{\'a}jek, 1972]{hajek1972local}
H{\'a}jek, J. (1972).
\newblock Local asymptotic minimax and admissibility in estimation.
\newblock In {\em Proceedings of the sixth Berkeley symposium on mathematical
  statistics and probability}, volume~1, pages 175--194.

\bibitem[Hastie and Tibshirani, 1987]{hastie1987generalized}
Hastie, T. and Tibshirani, R. (1987).
\newblock Generalized additive models: some applications.
\newblock {\em Journal of the American Statistical Association},
  82(398):371--386.

\bibitem[Hejazi et~al., 2020]{hal2}
Hejazi, N.~S., Coyle, J.~R., and {van der Laan}, M.~J. (2020).
\newblock {hal9001}: Scalable highly adaptive lasso regression in {R}.
\newblock {\em Journal of Open Source Software}.

\bibitem[Hjort and Claeskens, 2003]{hjort2003frequentist}
Hjort, N.~L. and Claeskens, G. (2003).
\newblock Frequentist model average estimators.
\newblock {\em Journal of the American Statistical Association},
  98(464):879--899.

\bibitem[Huang, 2017]{huang2017controlling}
Huang, H. (2017).
\newblock Controlling the false discoveries in lasso.
\newblock {\em Biometrics}, 73(4):1102--1110.

\bibitem[Huang et~al., 2010]{huang2010variable}
Huang, J., Horowitz, J.~L., and Wei, F. (2010).
\newblock Variable selection in nonparametric additive models.

\bibitem[Hubbard et~al., 2016]{dataAdaptTargetParam}
Hubbard, A.~E., Kherad-Pajouh, S., and van~der Laan, M.~J. (2016).
\newblock Statistical inference for data adaptive target parameters.
\newblock {\em The international journal of biostatistics}, 12(1):3--19.

\bibitem[Javanmard and Javadi, 2019]{javanmard2019false}
Javanmard, A. and Javadi, H. (2019).
\newblock False discovery rate control via debiased lasso.

\bibitem[Javanmard and Montanari, 2014]{javanmard2014confidence}
Javanmard, A. and Montanari, A. (2014).
\newblock Confidence intervals and hypothesis testing for high-dimensional
  regression.
\newblock {\em The Journal of Machine Learning Research}, 15(1):2869--2909.

\bibitem[Ju et~al., 2018]{HALoutcomeAdapt}
Ju, C., Benkeser, D.~C., and van~der Laan, M.~J. (2018).
\newblock Robust inference on the average treatment effect using the outcome
  highly adaptive lasso.
\newblock {\em Biometrics}, 76:109 -- 118.

\bibitem[Ju et~al., 2019]{adaptiveTruncationCTMLE}
Ju, C., Schwab, J., and van~der Laan, M.~J. (2019).
\newblock On adaptive propensity score truncation in causal inference.
\newblock {\em Statistical methods in medical research}, 28(6):1741--1760.

\bibitem[Ju et~al., 2017]{CTMLELasso}
Ju, C., Wyss, R., Franklin, J., Schneeweiss, S., Häggström, J., and Laan, M.
  (2017).
\newblock Collaborative-controlled lasso for constructing propensity
  score-based estimators in high-dimensional data.
\newblock {\em Statistical Methods in Medical Research}, 28.

\bibitem[Ki et~al., 2021]{marswithLasso}
Ki, D., Fang, B., and Guntuboyina, A. (2021).
\newblock Mars via lasso.

\bibitem[Kock, 2016]{AdaptLassoOracleStationary}
Kock, A.~B. (2016).
\newblock Consistent and conservative model selection with the adaptive lasso
  in stationary and nonstationary autoregressions.
\newblock {\em Econometric Theory}, 32(1):243--259.

\bibitem[Kuchibhotla et~al., 2022]{PostselectionInference}
Kuchibhotla, A.~K., Kolassa, J.~E., and Kuffner, T.~A. (2022).
\newblock Post-selection inference.
\newblock {\em Annual Review of Statistics and Its Application}, 9(1):505--527.

\bibitem[Laan and Gruber, 2010]{CTMLE}
Laan, M. and Gruber, S. (2010).
\newblock Collaborative double robust targeted maximum likelihood estimation.
\newblock {\em The international journal of biostatistics}, 6:Article 17.

\bibitem[Laan and Rubin, 2006]{laan_rubin_2006}
Laan, M. J. v.~d. and Rubin, D. (2006).
\newblock Targeted maximum likelihood learning.
\newblock {\em The International Journal of Biostatistics}, 2(1).

\bibitem[Lee et~al., 2016]{lee2016exact}
Lee, J.~D., Sun, D.~L., Sun, Y., and Taylor, J.~E. (2016).
\newblock Exact post-selection inference, with application to the lasso.

\bibitem[Leeb and Pötscher, 2005]{LeebModelSelect2005}
Leeb, H. and Pötscher, B.~M. (2005).
\newblock Model selection and inference: Facts and fiction.
\newblock {\em Econometric Theory}, 21(1):21--59.

\bibitem[Li et~al., 2019]{li2019overlapWeights}
Li, F., Thomas, L.~E., and Li, F. (2019).
\newblock Addressing extreme propensity scores via the overlap weights.
\newblock {\em American journal of epidemiology}, 188(1):250--257.

\bibitem[Lumley, 2017]{lumley2017robustness}
Lumley, T. (2017).
\newblock Robustness of semiparametric efficiency in nearly-true models for
  two-phase samples.
\newblock {\em arXiv preprint arXiv:1707.05924}.

\bibitem[Mammen and van~de Geer, 1997]{geerLocalAdapt}
Mammen, E. and van~de Geer, S. (1997).
\newblock {Locally adaptive regression splines}.
\newblock {\em The Annals of Statistics}, 25(1):387 -- 413.

\bibitem[Meinshausen and B{\"u}hlmann, 2010]{meinshausen2010stability}
Meinshausen, N. and B{\"u}hlmann, P. (2010).
\newblock Stability selection.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 72(4):417--473.

\bibitem[Milborrow, 2019]{milborrow2019package}
Milborrow, M.~S. (2019).
\newblock Package ‘earth’.
\newblock {\em R Software package}.

\bibitem[Moosavi et~al., 2023]{moosavi2023costs}
Moosavi, N., H{\"a}ggstr{\"o}m, J., and de~Luna, X. (2023).
\newblock The costs and benefits of uniformly valid causal inference with
  high-dimensional nuisance parameters.
\newblock {\em Statistical Science}, 38(1):1--12.

\bibitem[Morzywolek et~al., 2023]{morzywolek2023general}
Morzywolek, P., Decruyenaere, J., and Vansteelandt, S. (2023).
\newblock On a general class of orthogonal learners for the estimation of
  heterogeneous treatment effects.
\newblock {\em arXiv preprint arXiv:2303.12687}.

\bibitem[Nie and Wager, 2021]{nie2021quasi}
Nie, X. and Wager, S. (2021).
\newblock Quasi-oracle estimation of heterogeneous treatment effects.
\newblock {\em Biometrika}, 108(2):299--319.

\bibitem[Petersen et~al., 2012]{petersen2012diagnosing}
Petersen, M.~L., Porter, K.~E., Gruber, S., Wang, Y., and van~der Laan, M.~J.
  (2012).
\newblock Diagnosing and responding to violations in the positivity assumption.
\newblock {\em Statistical methods in medical research}, 21(1):31--54.

\bibitem[Pfanzagl and Wefelmeyer, 1985]{pfanzagl1985contributions}
Pfanzagl, J. and Wefelmeyer, W. (1985).
\newblock Contributions to a general asymptotic statistical theory.
\newblock {\em Statistics \& Risk Modeling}, 3(3-4):379--388.

\bibitem[P{\"o}tscher, 1991]{potscher1991effects}
P{\"o}tscher, B.~M. (1991).
\newblock Effects of model selection on inference.
\newblock {\em Econometric Theory}, 7(2):163--185.

\bibitem[P{\"o}tscher and Schneider, 2009]{potscherAdaptLasso2009distribution}
P{\"o}tscher, B.~M. and Schneider, U. (2009).
\newblock On the distribution of the adaptive lasso estimator.
\newblock {\em Journal of Statistical Planning and Inference},
  139(8):2775--2790.

\bibitem[Qiu et~al., 2020]{SieveQiu}
Qiu, H., Luedtke, A., and Carone, M. (2020).
\newblock Universal sieve-based strategies for efficient estimation using
  machine learning tools.

\bibitem[Ravikumar et~al., 2009]{ravikumar2009sparse}
Ravikumar, P., Lafferty, J., Liu, H., and Wasserman, L. (2009).
\newblock Sparse additive models.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 71(5):1009--1030.

\bibitem[Rinaldo et~al., 2019]{rinaldo2019bootstrapping}
Rinaldo, A., Wasserman, L., and G’Sell, M. (2019).
\newblock Bootstrapping and sample splitting for high-dimensional,
  assumption-lean inference.

\bibitem[Robins et~al., 1994]{robinsCausal}
Robins, J.~M., Rotnitzky, A., and Zhao, L.~P. (1994).
\newblock Marginal structural models and causal inference in epidemiology.
\newblock {\em Journal of the American statistical Association},
  89(427):846--866.

\bibitem[Robins et~al., 1995]{robins1995analysis}
Robins, J.~M., Rotnitzky, A., and Zhao, L.~P. (1995).
\newblock Analysis of semiparametric regression models for repeated outcomes in
  the presence of missing data.
\newblock {\em Journal of the american statistical association},
  90(429):106--121.

\bibitem[Robinson, 1988]{robinson1988root}
Robinson, P.~M. (1988).
\newblock Root-n-consistent semiparametric regression.
\newblock {\em Econometrica: Journal of the Econometric Society}, pages
  931--954.

\bibitem[Sampson et~al., 2013]{sampson2013controlling}
Sampson, J.~N., Chatterjee, N., Carroll, R.~J., and M{\"u}ller, S. (2013).
\newblock Controlling the local false discovery rate in the adaptive lasso.
\newblock {\em Biostatistics}, 14(4):653--666.

\bibitem[Shen, 1997]{shen1997methods}
Shen, X. (1997).
\newblock On methods of sieves and penalization.
\newblock {\em The Annals of Statistics}, 25(6):2555--2591.

\bibitem[Shortreed and Ertefaie, 2017]{outcomeadaptiveLasso}
Shortreed, S. and Ertefaie, A. (2017).
\newblock Outcome-adaptive lasso: Variable selection for causal inference.
\newblock {\em Biometrics}, 73.

\bibitem[Su and Zhang, 2014]{su2014variable}
Su, L. and Zhang, Y. (2014).
\newblock Variable selection in nonparametric and semiparametric regression
  models.

\bibitem[Tibshirani, 1994]{Tibshirani94regressionshrinkage}
Tibshirani, R. (1994).
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society, Series B},
  58:267--288.

\bibitem[Van~de Geer et~al., 2014]{van2014asymptotically}
Van~de Geer, S., B{\"u}hlmann, P., Ritov, Y., and Dezeure, R. (2014).
\newblock On asymptotically optimal confidence regions and tests for
  high-dimensional models.

\bibitem[van~der Laan, 2015]{vanderlaanGenerlaTMLEFIRST}
van~der Laan, M. (2015).
\newblock A generally efficient targeted minimum loss based estimator.
\newblock {\em Biostatistics Working Paper Series Working Paper 343.}

\bibitem[van~der Laan, 2022]{HALpointwise}
van~der Laan, M. (2022).
\newblock Pointwise asymptotic-normality of the highly adaptive lasso for
  general functions.

\bibitem[van~der Laan et~al., 2022]{undersmoothedHAL}
van~der Laan, M., Benkeser, D., and Cai, W. (2022).
\newblock Efficient estimation of pathwise differentiable target parameters
  with the undersmoothed highly adaptive lasso.
\newblock {\em The International Journal of Biostatistics}.

\bibitem[van~der Laan, 2014]{van2014causal}
van~der Laan, M.~J. (2014).
\newblock Causal inference for a population of causally connected units.
\newblock {\em Journal of Causal Inference}, 2(1):13--74.

\bibitem[van~der Laan and Dudoit, 2005]{DudoitVanderLaan2005}
van~der Laan, M.~J. and Dudoit, S. (2005).
\newblock Asymptotics of cross-validated risk estimation in estimator selection
  and performance assessment.
\newblock {\em Statistical Methodology}, 2:131--154.

\bibitem[van~der Laan et~al., 2013]{van2013AdaptTarget}
van~der Laan, M.~J., Hubbard, A.~E., and Pajouh, S.~K. (2013).
\newblock Statistical inference for data adaptive target parameters.

\bibitem[van~der Laan and Luedtke, 2015]{van2015OptRule}
van~der Laan, M.~J. and Luedtke, A.~R. (2015).
\newblock Targeted learning of the mean outcome under an optimal dynamic
  treatment rule.
\newblock {\em Journal of causal inference}, 3(1):61--95.

\bibitem[van~der Laan and Robins, 2003]{vanderlaanunified}
van~der Laan, M.~J. and Robins, J.~M. (2003).
\newblock {\em Unified Methods for Censored Longitudinal Data and Causality}.
\newblock Springer.

\bibitem[van~der Laan and Rose, 2011]{vanderLaanRose2011}
van~der Laan, M.~J. and Rose, S. (2011).
\newblock {\em Targeted Learning: Causal Inference for Observational and
  Experimental Data}.
\newblock Springer, New York.

\bibitem[van~der Laan and Rose, 2021]{discussionMLEMark}
van~der Laan, M.~J. and Rose, S. (2021).
\newblock Why machine learning cannot ignore maximum likelihood estimation.

\bibitem[van~der Vaart and Wellner, 1996]{vanderVaartWellner}
van~der Vaart, A. and Wellner, J. (1996).
\newblock {\em Weak Convergence and Empirical Processes}.
\newblock Springer.

\bibitem[van~der Vaart, 2000]{vandervaart2000asymptotic}
van~der Vaart, A.~W. (2000).
\newblock {\em Asymptotic statistics}, volume~3.
\newblock Cambridge university press.

\bibitem[Wainwright, 2009]{wainwright2009sharp}
Wainwright, M.~J. (2009).
\newblock Sharp thresholds for high-dimensional and noisy sparsity recovery
  using l1-constrained quadratic programming (lasso).
\newblock {\em IEEE transactions on information theory}, 55(5):2183--2202.

\bibitem[Wasserman and Roeder, 2009]{wasserman2009high}
Wasserman, L. and Roeder, K. (2009).
\newblock High dimensional variable selection.
\newblock {\em Annals of statistics}, 37(5A):2178.

\bibitem[White, 1982]{white1982MLErobust}
White, H. (1982).
\newblock Maximum likelihood estimation of misspecified models.
\newblock {\em Econometrica: Journal of the econometric society}, pages 1--25.

\bibitem[Williamson et~al., 2021]{williamson2021nonparametric}
Williamson, B.~D., Gilbert, P.~B., Carone, M., and Simon, N. (2021).
\newblock Nonparametric variable importance assessment using machine learning
  techniques.
\newblock {\em Biometrics}, 77(1):9--22.

\bibitem[Wood, 2001]{wood2001mgcv}
Wood, S.~N. (2001).
\newblock mgcv: Gams and generalized ridge regression for r.
\newblock {\em R news}, 1(2):20--25.

\bibitem[Wright and Ziegler, 2015]{wright2015ranger}
Wright, M.~N. and Ziegler, A. (2015).
\newblock ranger: A fast implementation of random forests for high dimensional
  data in c++ and r.
\newblock {\em arXiv preprint arXiv:1508.04409}.

\bibitem[Wu and Zhou, 2019]{wu2019hodges}
Wu, X. and Zhou, X. (2019).
\newblock On hodges’ superefficiency and merits of oracle property in model
  selection.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  71(5):1093--1119.

\bibitem[Xu et~al., 2016]{xu2016faithful}
Xu, M., Chen, M., and Lafferty, J. (2016).
\newblock Faithful variable screening for high-dimensional convex regression.

\bibitem[Yang and Yang, 2021]{yang2021rates}
Yang, Y. and Yang, H. (2021).
\newblock Rates of convergence of the adaptive elastic net and the
  post-selection procedure in ultra-high dimensional sparse models.
\newblock {\em Communications in Statistics-Theory and Methods}, 50(1):73--94.

\bibitem[Zhang and Zhang, 2011]{zhang2011confidence}
Zhang, C.-H. and Zhang, S. (2011).
\newblock Confidence intervals for low-dimensional parameters with
  high-dimensional data.
\newblock {\em arXiv preprint arXiv:1110.2563}.

\bibitem[Zhang and Zhang, 2014]{zhang2014confidence}
Zhang, C.-H. and Zhang, S.~S. (2014).
\newblock Confidence intervals for low dimensional parameters in high
  dimensional linear models.
\newblock {\em Journal of the Royal Statistical Society: Series B: Statistical
  Methodology}, pages 217--242.

\bibitem[Zhao and Yu, 2006]{zhao2006model}
Zhao, P. and Yu, B. (2006).
\newblock On model selection consistency of lasso.
\newblock {\em The Journal of Machine Learning Research}, 7:2541--2563.

\bibitem[Zhao et~al., 2017]{zhao2017selective}
Zhao, Q., Small, D.~S., and Ertefaie, A. (2017).
\newblock Selective inference for effect modification via the lasso.
\newblock {\em arXiv preprint arXiv:1705.08020}.

\bibitem[Zhao et~al., 2020]{danielleWittenLassoWorks}
Zhao, S., Witten, D., and Shojaie, A. (2020).
\newblock In defense of the indefensible: A very naive approach to
  high-dimensional inference.
\newblock {\em Statistical Science}.

\bibitem[Zou, 2006]{AdaptLassoOracle}
Zou, H. (2006).
\newblock The adaptive lasso and its oracle properties.
\newblock {\em Journal of the American statistical association},
  101(476):1418--1429.

\end{thebibliography}
