\documentclass[11pt]{article}
\pdfoutput=1
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts,titling,bbm, titlesec, bm, physics, enumitem, accents, xcolor, setspace, fancyhdr, natbib, geometry,
	pdflscape}
\usepackage{graphicx}
\usepackage{float}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{array}
\usepackage[title]{appendix}
\usepackage{afterpage}
\usepackage{listings}
\usepackage{prodint}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage[hidelinks]{hyperref}
\usepackage{subcaption}
\newlength{\continueindent}
\setlength{\continueindent}{2em}
\usepackage{etoolbox}
\makeatletter
\newcommand*{\ALG@customparshape}{\parshape 2 \leftmargin \linewidth \dimexpr\ALG@tlm+\continueindent\relax \dimexpr\linewidth+\leftmargin-\ALG@tlm-\continueindent\relax}
\apptocmd{\ALG@beginblock}{\ALG@customparshape}{}{\errmessage{failed to patch}}
\makeatother

\algnewcommand\algorithmicstack{\textit{Stack:}}
\algnewcommand\Stack{\item[\algorithmicstack]}
\algnewcommand\algorithmicchoosetimegrid{\textit{Choose time grid:}}
\algnewcommand\Choosetimegrid{\item[\algorithmicchoosetimegrid]}
\algnewcommand\algorithmicchoosetimebasis{\textit{Choose time basis:}}
\algnewcommand\Choosetimebasis{\item[\algorithmicchoosetimebasis]}
\algnewcommand\algorithmicbuilddiscretedata{\textit{Build data at time $t_i$:}}
\algnewcommand\Builddiscretedata{\item[\algorithmicbuilddiscretedata]}
\algnewcommand\algorithmicfit{\textit{Fit:}}
\algnewcommand\Fit{\item[\algorithmicfit]}
\algnewcommand\algorithmicpredict{\textit{Predict:}}
\algnewcommand\Predict{\item[\algorithmicpredict]}

\newcommand\Algphase[1]{%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
\Statex\hspace*{-\algorithmicindent}\textbf{#1}%
\vspace*{-.7\baselineskip}\Statex\hspace*{\dimexpr-\algorithmicindent-2pt\relax}\rule{\textwidth}{0.4pt}%
}

\setlength{\droptitle}{-5em}
\bibliographystyle{apalike}

\doublespacing

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\I}{\mathbbm{1}}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Var}{\text{Var}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\prob}{\text{Pr}}
\newcommand{\ut}[1]{\underaccent{\tilde}{#1}}
\renewcommand{\vec}[1]{\ut{#1}}

\providecommand{\keywords}[1]
{
  \small	
  \textbf{\textit{Keywords---}} #1
}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

\allowdisplaybreaks

\newcommand{\cpt}{\stackrel{\text{p}}{\to}}
\newcommand{\haz}{\lambda}
\newcommand{\Haz}{\Lambda}
%\newcommand{\,|\,d}{\,|\,}
\DeclareMathOperator*{\esssup}{ess\,sup}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{condition}{Condition}
\theoremstyle{definition}
\newtheorem{example}{Example} 

%\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
%		\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\titleformat{\section}{\large\scshape\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\normalfont\bfseries}{\thesubsection.}{1em}{}%\itshape
 

 

\title{Adaptive debiased machine learning\\ using data-driven model selection techniques}
 

\author{Lars van der Laan, Marco Carone,  Alex Luedtke, Mark van der Laan}

\begin{document}

\singlespacing
\maketitle

\begin{abstract}
Debiased machine learning estimators for nonparametric inference of smooth functionals of the data-generating distribution can suffer from excessive variability and instability. For this reason, practitioners may resort to simpler models based on parametric or semiparametric assumptions. However, such simplifying assumptions may fail to hold, and estimates may then be biased due to model misspecification. To address this problem, we propose Adaptive Debiased Machine Learning (ADML), a nonparametric framework that combines data-driven model selection and debiased machine learning techniques to construct asymptotically linear, adaptive, and superefficient estimators for pathwise differentiable functionals. By learning model structure directly from data, ADML avoids the bias introduced by model misspecification and remains free from the restrictions of parametric and semiparametric models. While they may exhibit irregular behavior for the target parameter in a nonparametric statistical model, we demonstrate that ADML estimators provides regular and locally uniformly valid inference for a projection-based oracle parameter. Importantly, this oracle parameter agrees with the original target parameter for distributions within an unknown but correctly specified oracle statistical submodel that is learned from the data. This finding implies that there is no penalty, in a local asymptotic sense, for conducting data-driven model selection compared to having prior knowledge of the oracle submodel and oracle parameter. To demonstrate the practical applicability of our theory, we provide a broad class of ADML estimators for estimating the average treatment effect in adaptive partially linear regression models. 


\end{abstract}

 \keywords{Adaptive debiased machine learning, causal inference, superefficient, model selection, selective inference.}

\doublespacing


\section{Introduction}

 



For many scientific applications, including treatment effect estimation and policy learning, it is critical to infer real-valued summaries (i.e., functionals) of probability distributions. For this purpose, several debiased machine learning frameworks are available, including one-step estimation \citep{pfanzagl1985contributions, bickel1993efficient}, estimating equations and double-machine learning \citep{robins1995analysis, robinsCausal, vanderlaanunified, DoubleML}, targeted maximum likelihood estimation \citep{laan_rubin_2006, vanderLaanRose2011}, and sieve-based plug-in estimation \citep{shen1997methods, spnpsieve, sieveOneStepPlugin, SieveQiu, discussionMLEMark,undersmoothedHAL}. These frameworks typically involve two stages: preliminary estimation, wherein flexible machine learning techniques are used to estimate the data-generating distribution; and debiasing, which facilitates valid uncertainty assessment based on a prespecified statistical model. When the model is correctly specified, these methods yield parametric-rate consistent, regular, and asymptotically linear estimators that are efficient among the class of all regular estimators \citep{bickel1993efficient, vandervaart2000asymptotic}. Additionally, such efficient estimators are locally asymptotically minimax  among all estimators, including irregular estimators, with respect to the statistical model \citep{vandervaart2000asymptotic}. In other words, asymptotically, they minimize the maximum mean square estimation error over all local perturbations of the data-generating distribution that fall within the statistical model, that is, over all local alternatives.

 
 


While debiasing approaches have proven effective in generating efficient and locally asymptotically minimax estimators, they do possess a notable limitation: the debiasing step and uncertainty quantification necessitate \textit{a priori} specification of a correct statistical model, and so, are not adaptive to the complexity of the true data-generating distribution. To illustrate this limitation, consider a scenario where the true distribution is sparse, smooth, or otherwise structured, falling within an unknown but potentially learnable submodel of the prespecified statistical model. We refer to this model as an \emph{oracle submodel} as it can generally only be specified under knowledge of the true data-generating distribution. The limiting variance of an estimator obtained by standard debiasing approaches is typically indifferent to the presence of any learnable structure. This lack of adaptivity occurs because such estimators are locally asymptotically minimax, ensuring robustness against all local perturbations, no matter how unrealistic, within the larger prespecified model \citep{vandervaart2000asymptotic}. This raises concerns as such local perturbations may lie outside the oracle submodel and exhibit excessive complexity compared to the true data-generating distribution. In particular, unnecessarily robustifying against these local perturbations  can result in increased estimator variability and wider confidence intervals, even when the data suggests a relatively simple data-generating distribution \citep{moosavi2023costs}. %Unnecessarily robustifying against such local perturbations can lead to needlessly variable estimates and seemingly wide confidence intervals when the data indicates that the true distribution is relatively simple \citep{moosavi2023costs}.
To address this limitation, practitioners may use simpler models incorporating parametric or semiparametric assumptions \citep{imbensOverlapEstimand2006}. However, these assumptions are frequently rooted in subjective beliefs regarding the data-generating distribution's complexity and may result in biased estimates due to model misspecification. Learning such models in a data-driven manner may yield a better balance between model misspecification bias and estimator variance. However, data-driven model selection techniques can invalidate existing theoretical guarantees for inference \citep{LeebModelSelect2005}. 

 
 

 


In this work, we propose a simple nonparametric framework for adaptive and superefficient inference on smooth functionals of the data-generating distribution that can leverage learnable structure in the data-generating distribution. Our framework, which we refer to as \textit{adaptive debiased machine learning} (ADML), integrates data-driven model selection with debiased machine learning techniques to provide asymptotically linear and superefficient estimators of the target parameter. The ADML framework allows us to avoid specifying restrictive parametric or semiparametric assumptions a priori while still benefiting from them when the data suggest they may be valid. In general, to construct an adaptive debiased machine learning estimator (ADMLE), we first use model selection techniques to learn from the data a working model that approximates an oracle submodel of a prespecified statistical model. We then approximate the true distribution by its projection onto the working submodel and finally construct debiased estimators for the corresponding data-adaptive working estimand. In contrast with previous works on inference for data-adaptive parameters \citep{van2013AdaptTarget, rinaldo2019bootstrapping}, we do not require sample-splitting and also obtain valid inference for the actual target parameter. As a special case, ADML encompasses adaptive minimum loss estimators (AMLEs), which are general two-stage plug-in estimators evaluating the parameter at an empirical risk minimizer over a data-dependent working model obtained using model selection techniques. Surprisingly, for AMLEs, we show valid inference can be obtained using the standard model-robust sandwich variance estimator based on the learned working model. We also show that, for general infinite-dimensional models, ADML provides a means to construct adaptive one-step and adaptive targeted maximum likelihood estimators (ATMLEs) \citep{bickel1993efficient, vanderLaanRose2011}.
 
 
 


This paper is organized as follows. In Section \ref{section::AMLEGeneral}, we outline the general ADML framework, state our key results, provide notable examples of ADMLEs for estimating an average treatment effect, and discuss related literature. In Section \ref{section::oracleparameter}, we introduce a class of projection-based oracle parameters and discuss its role in constructing superefficient estimators. Our main theoretical results for ADML are presented in Sections \ref{section::dataAdaptParam} and \ref{section::oracleParamInference}. In Section \ref{section::dataAdaptParam}, we provide results on inference for a data-dependent projection-based working parameter, without requiring sample-splitting. In Section \ref{section::oracleParamInference}, we study the regularity, asymptotic linearity, and (super)efficiency of ADMLEs for an oracle parameter and the original target parameter. Throughout, we illustrate our results by studying a general class of ADMLEs for the average treatment effect (ATE) based on model selection in a partially linear regression model.

 


\section{Adaptive debiased machine learning}
\label{section::AMLEGeneral}

\subsection{Preliminaries}
Let $\mathcal{M}$ be a statistical model, a collection of probability distributions dominated by a common sigma-finite measure $\mu$. For a given $P\in\mathcal{M}$, we denote the $L^2(P)$ norm as $\|\cdot\|_P$. A one-dimensional submodel $\{P_t: t \in \mathbb{R}\} \subseteq \mathcal{M}$ through $P$ at $t=0$ is called \textit{regular} if it is differentiable in quadratic mean at $t=0$. The tangent space $T_\mathcal{M}(P)$ of $\mathcal{M}$ at $P$ is the $L^2(P)$-closure of scores generated by regular one-dimensional submodels of $\mathcal{M}$ through $P$. We assume that $\mathcal{M}$ is smooth in the sense that $T_\mathcal{M}(P)$ is a nonempty linear space for every $P \in \mathcal{M}$. A statistical model $\mathcal{M}_{np}$ is locally nonparametric if $T_{\mathcal{M}_{np}}(P) = L^2_0(P)$ for every $P \in \mathcal{M}_{np}$.

A parameter (or functional) $\Psi: \mathcal{M} \rightarrow \mathbb{R}$ is pathwise differentiable at $P$ if there exists a bounded linear operator $d\Psi(P): T_\mathcal{M}(P) \rightarrow \mathbb{R}$ such that $d\Psi(P)[s] = \frac{d}{dt} \Psi(P_t) \big|_{t=0}$ for all regular submodels with score $s \in T_\mathcal{M}(P)$ at $P$. By the Riesz representation theorem, $d\Psi(P)$ can be expressed in terms of an inner product as $s\mapsto \langle s,  D_P \rangle_{L^2(P)}$ for some element $D_P \in L^2_0(P_0)$ referred to as a gradient. There exists a unique canonical gradient $D_P^*\in T_\mathcal{M}(P)$, referred to as the efficient influence function as its squared $L^2(P)$-norm is the generalized Cramer-Rao (CR) lower bound for estimating $\Psi(P)$ relative to $\mathcal{M}$ \citep{bickel1993efficient}.

For $h \in \mathbb{R}$ and a regular submodel $\{P_t: t \in \mathbb{R}\} \subseteq \mathcal{M}_{np}$ through $P$ at $t=0$, $P_{0,hn^{-1/2}}$ is a local perturbation (or local alternative) of $P_0$. An estimator $\widehat \psi_n$ is regular for a parameter $\Psi$ with respect to the local perturbation $P_{0,hn^{-1/2}}$ if $\sqrt{n}\,\{\widehat \psi_n - \Psi(P_{0,hn^{-1/2}})\}$ converges in distribution when sampling from $P_{0,hn^{-1/2}}$ with a limit that does not depend on  $h$. An estimator $\widehat \psi_n$ is $P_0$--regular for $\Psi$ over $\mathcal{M}$ if it is regular with respect to all local perturbations of $P_0$ in $\mathcal{M}$, and it is regular for $\Psi$ over $\mathcal{M}$ if it is $P$--regular for each $P \in \mathcal{M}$. An estimator $\widehat \psi_n$ is $P_0$--asymptotically linear for a parameter $\Psi$ with influence function $\phi_0$ if $\widehat \psi_n = \psi_0 + P_n \phi_0 + o_p(n^{-1/2})$ under sampling from $P_0$, and it is asymptotically linear for $\Psi$ over $\mathcal{M}$ if it is $P$--asymptotically linear under sampling from each $P\in\mathcal{M}$. A $P_0$--asymptotically linear estimator $\widehat \psi_n$ is $P_0$--efficient for $\Psi$ with respect to model $\mathcal{M}$ if its influence function, under sampling from $P_0$, equals the efficient influence function of $\Psi$ at $P_0$.  An estimator is efficient for $\Psi$ with respect to $\mathcal{M}$ if it is $P$--efficient for each $P \in \mathcal{M}$. Similarly, an estimator $\widehat \psi_n$ is $P_0$--superefficient for $\Psi$ relative to $\mathcal{M}$ if its limiting variance is, under sampling from $P_0$, smaller than the corresponding CR lower bound of $\Psi$ at $P_0$.


 

\subsection{General framework and overview of results}
\label{section::AMLEGeneral1}



Suppose that we have at our disposal a sample of $n$ independent and identically distributed observations, denoted as $O_1, O_2, \dots, O_n$, drawn from a probability distribution $P_0$ known only to belong to a prespecified statistical model ${\cal M}$ contained in some convex and locally nonparametric model ${\cal M}_{np}$. The statistical model ${\cal M}$ is used to incorporate any existing knowledge about the underlying data-generating process. Our objective is to obtain inference for a feature $\psi_0:=\Psi(P_0)$ of $P_0$ arising from a specified real-valued target parameter $\Psi: {\cal M}_{np} \rightarrow \mathbb{R}$ defined on the nonparametric model. For notation convenience, we will denote any summary $S_{P_0}$ of $P_0$ by $S_0$. 

 
 

Suppose that we can employ data-driven model selection techniques to learn a working statistical model $\mathcal{M}_n \subseteq \mathcal{M}$ that sufficiently approximates some unknown submodel $\mathcal{M}_0  \subseteq \mathcal{M}$. Although the working model $\mathcal{M}_n$ may not contain the true data-generating distribution $P_0$ for any $n$, we assume that $\mathcal{M}_0$ is a smooth statistical model containing $P_0$. The smoothness condition on $\mathcal{M}_0$ rules out degenerate models such as $\mathcal{M}_0 = \{P_0\}$. As an example, the submodel $\mathcal{M}_0$ can be defined as $\mathcal{M}_{j_0}$, where $j_0 \in \mathbb{N} \cup \{\infty\}$ denotes the smallest index $j$ such that $P_0$ is contained in a submodel $\mathcal{M}_j$ within a known sequence of nested submodels $\mathcal{M}_1 \subseteq \mathcal{M}_2 \subseteq \dots \subseteq \mathcal{M}_{\infty} := \mathcal{M}$. A working model $\mathcal{M}_n$ can be learned, for instance, via cross-validation from a finite collection of models $\{\mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_{k(n)}\}$, where $k(n) \in \mathbb{N}$ is a sequence that may depend on the sample size. As the submodel $\mathcal{M}_0$ can typically only be specified by an oracle that knows structural properties of $P_0$, such as sparsity or smoothness, we will refer to $\mathcal{M}_0$ as an oracle submodel. 




We aim to construct an adaptive estimator of $\Psi(P_0)$ by leveraging that $P_0$ falls in the unknown oracle submodel $\mathcal{M}_{0}$ of $\mathcal{M}$. To do so, we consider inference on an oracle projection-based parameter $\Psi_0: \mathcal{M}_{np} \rightarrow \mathbb{R}$ defined as the composition 
\begin{equation}
\Psi_0:=\Psi\circ \Pi_0
\label{eqn::oracleParam}
\end{equation}evaluated pointwise as $P \mapsto  \Psi (\Pi_0(P))$
 for an appropriate loss-based projection $ P \mapsto \Pi_0 P \in \argmin_{Q \in \mathcal{M}_0} P \ell(\cdot, Q)$ onto the oracle submodel $\mathcal{M}_0$ --- details are provided in Section \ref{section::oracleparameter}. To illustrate the framework, it is helpful to take $\ell$ to be the negative loglikelihood loss function so that $\Pi_0$ equals the loglikelihood projection $P \mapsto \Pi_0P := \argmax_{Q \in \mathcal{M}_0} P\left(\log \frac{ dQ}{d\mu}\right)$. The loss $\ell$ could also be any working loglikelihood loss, such as the logistic binomial, Poisson or least-squares loss. We will assume that the oracle parameter $\Psi_0$ is pathwise differentiable with nonparametric efficient influence function $D_{0,P_0}$ at $P_0$ and, therefore, amenable to $\sqrt{n}\,$--consistent estimation \citep{bickel1993efficient}. 
 
 A crucial defining property of the oracle parameter $\Psi_0$ is that it satisfies $\Psi_0(P) = \Psi(P)$ for all $P \in \mathcal{M}_0$. As a result, since $P_0 \in \mathcal{M}_0$, it follows that $\Psi_0(P_0) = \Psi(P_0)$, so that the oracle and target parameters yield the same estimand. When the tangent space of $\mathcal{M}_0$ at $P_0$ is smaller than that of $\mathcal{M}_{np}$, the CR lower bound $var_0\{D_{0,P_0}(O)\}$ for $\Psi_0$ at $P_0$ is smaller than that of the parameter $\Psi: \mathcal{M}_{np} \rightarrow \mathbb{R}$ for the prespecified model $\mathcal{M}$. In fact, for $\Pi_0$ equal to the Kullbackâ€“Leibler (KL) or Hellinger projection, the CR lower bound for $\Psi_0$ at $P_0$  is equal to that for $\Psi$ restricted to the oracle submodel $\mathcal{M}_0$. Consequently, a $P_0$--efficient estimator for $\Psi_0$ typically exhibits $P_0$--superefficiency for $\Psi$ relative to both $\mathcal{M}$ and $\mathcal{M}_{np}$, with a limiting variance that depends on the size of the oracle submodel $\mathcal{M}_0$, and thus, adapts to the complexity of $P_0$.
 



 Our proposed ADML framework  suggests obtaining $P_0$--efficient inference for $\Psi_0$ --- and thereby $P_0$--superefficient inference for $\Psi$ --- by constructing debiased estimators for the data-adaptive working parameter $\Psi_n: \mathcal{M}_{np} \rightarrow \mathbb{R}$ defined as the composition 
\begin{equation}
   \Psi_n:= \Psi \circ \Pi_n
   \label{eqn::dataadaptParam}
\end{equation}evaluated pointwise as $P\mapsto \Psi(\Pi_n(P))$, 
where $\Pi_n(P) \in \argmin_{Q \in \mathcal{M}_n} P \ell(\cdot, Q)$ is a projection of $P \in \mathcal{M}_{np}$ onto the data-dependent working submodel $\mathcal{M}_n$. Formally, an ADMLE $\widehat{\psi}_n$ is an estimator that satisfies the asymptotic expansion \[\widehat{\psi}_n = \Psi_n(P_0) + (P_n-P_0) D_{n,P_0} + o_p(n^{-1/2})\ ,\] where $D_{n,P_0}$ is the nonparametric efficient influence function of $\Psi_n$. If $\Psi_n$ were a fixed, deterministic parameter, fulfilling the above asymptotic expansion would imply that $\widehat{\psi}_n$ is as an asymptotically linear and nonparametric efficient estimator for $\Psi_n$ at $P_0$. In view of results in \cite{van2013AdaptTarget} and \cite{dataAdaptTargetParam}, using empirical risk minimization, the method of sieves, or targeted minimum loss-based estimation (TMLE), it is possible to construct an estimator $\widehat{P}_n$ of $P_0$ such that the corresponding plug-in estimator $\widehat\psi_n:=\Psi_n(\widehat P_n)$ satisfies this expansion. Alternatively, adaptive one-step and double machine learning-based estimators that satisfy this property can be used at the cost of the plug-in property.  

 
 
In this manuscript, we establish that, under appropriate conditions, ADMLEs are, with respect to $P_0$, regular, asymptotically linear, and nonparametric efficient for the projection-based oracle parameter $\Psi_0$. Consequently, ADMLEs provide locally uniformly valid inference for $\Psi_0$ in the sense of \cite{buhlmann1999efficient}, even under sampling from least-favorable local perturbations of $P_0$ outside the oracle submodel $\mathcal{M}_0$. This implies, in a local asymptotic sense, that there is no penalty for conducting data-driven model selection compared to having prior knowledge of the oracle submodel or oracle parameter. Furthermore, we show that, under certain conditions, an ADML estimator is:
\begin{enumerate}[label=\roman*., ref=\roman*]
\item asymptotically linear for the original target parameter $\Psi$ with influence function at $P_0$ being the $P_0$--efficient influence function of $\Psi_0$;
\item $P_0$--regular for $\Psi$ with respect to any local perturbation of $P_0$ within the oracle submodel $\mathcal{M}_0$;
\item asymptotically $P_0$--efficient for $\Psi$ relative to the oracle submodel $\mathcal{M}_0$ for suitable $\Pi_0$.
\end{enumerate} 
Consequently, in practice, for an estimator $\widehat P_n$ of $P_0$ with sufficient regularity, the sampling distribution of the ADMLE $\widehat \psi_n$ under $P_0$ can be approximated by the normal distribution with mean $\psi_0$ and variance $\tfrac{1}{n}\sigma_n^2$, where $\sigma_n^2:=\frac{1}{n}\sum_{i=1}^{n}D_{n, \widehat{P}_n}(O_i)^2$ is an influence function-based variance estimator. Notably, when $\widehat \psi_n$ is obtained using empirical risk minimization or M-estimation over $\mathcal{M}_n$, $\sigma_n^2$ reduces to the standard model-robust sandwich variance estimator. An approximate $(1-\alpha)$\% confidence interval for $\psi_0$ can be constructed as $\mathcal{I}_n(\alpha) := (\widehat \psi_n -q_\alpha\sigma_nn^{-1/2},\widehat \psi_n +q_\alpha\sigma_nn^{-1/2})$, where $q_\alpha$ is the $(1-\tfrac{\alpha}{2})$--quantile of the standard normal distribution. This confidence interval is $P_0$--locally uniformly valid and nonparametric $P_0$--efficient for the projection oracle parameter $\Psi_0$. In other words, $\mathcal{I}_n(\alpha)$ is an asymptotically valid and tight confidence interval for $\Psi_0(P_{0,hn^{-1/2}})$ in a uniform sense under sampling from any local perturbation $P_{0,hn^{-1/2}} \in \mathcal{M}_{np}$ of $P_0$. This implies, in particular, that $\mathcal{I}_n(\alpha)$ is asymptotically valid for $\Psi(P_{0,hn^{-1/2}})$ uniformly over every local perturbation $P_{0,hn^{-1/2}} \in \mathcal{M}_0$ within the oracle submodel. Moreover, when $\Pi_0$ is the KL or Hellinger projection, the width of the confidence interval $\mathcal{I}_n(\alpha)$ is also $P_0$--locally asymptotically minimax over $\mathcal{M}_0$ for the actual target parameter $\Psi$.

 
 

 


\subsection{Examples of ADMLE for the ATE parameter}
\label{section::AMLEGeneral2Example}

 

In this section, we illustrate our approach for adaptive inference of the population average treatment effect. Commonly-used nonparametric estimators of the ATE, such as augmented inverse probability weighted (AIPW) estimators \citep{robinsCausal} and TMLEs \citep{vanderLaanRose2011}, can exhibit instability and high variance in settings with limited treatment overlap. Previous studies have suggested using easier-to-estimate target parameters that incorporate prespecified (working) model assumptions, such as treatment effect homogeneity or a known parametric form \citep{imbensOverlapEstimand2006, petersen2012diagnosing, li2019overlapWeights}. However, selecting an appropriate working model is challenging and can lead to compromised inferences due to model misspecification bias. Through our proposed approach, model assumptions are learned from the data, enabling valid inference while mitigating model misspecification bias.

 

 Consider the setup where $O = (W,A,Y)$ with $W \in \mathbb{R}^d$ is a covariate vector, $A \in \{0,1\}$ is a binary treatment assignment, and $Y \in \mathbb{R}$ is a bounded outcome. Let $P$ be a given distribution for $O$ and write $(a,w)$ as a realization of $(A,W)$. We denote by $\mu_P(a,w)$ the outcome regression $E_P(Y \,|\, A = a, W = w)$, and by $\tau_P(w)$  the conditional average treatment effect (CATE) $E_P(Y \,|\, A = 1, W = w) - E_P(Y \,|\, A = 0, W = w)$. Additionally, we denote by $\pi_P(w)$ the propensity score $ P(A=1 \,|\, W=w)$ and by $m_P(w)$ the conditional mean outcome $E_P(Y \,|\, W =w)$. We denote by  $P_{0,W}$ and $P_{0,W,A}$ the marginal distributions of $W$ and $(W,A)$, respectively, under $P_0$. We assume that $\mu_0$ lies in a known nonparametric regression model $\Theta \subseteq L^2(P_{0,A,W})$ corresponding to a prespecified statistical model $\mathcal{M}$. We also assume the equivalence of $L^2$-norms $\|\cdot\|_P$ and $\|\cdot \|_{P_0}$ for each distribution $P \in \mathcal{M}_{np}$.

  




 
 
  Our target of interest is the average treatment effect (ATE) parameter denoted as $\Psi: \mathcal{M}_{np} \rightarrow \mathbb{R}$ and given by the mapping
$$P \mapsto \Psi(P) := E_P \left\{E_P \left(Y \,|\, A = 1, W   \right)  - E_P\left(Y \,|\, A = 0, W \right) \right\}.$$ 
To construct an ADMLE of the ATE, we adopt the formulation presented in the previous section. Let $\Theta_n \subseteq \Theta$ be a working linear regression submodel obtained through data-driven model selection techniques. We view the working model $\Theta_n$ as an approximation of an oracle linear submodel $\Theta_0 \subseteq \Theta$ that contains $\mu_0$. We take the oracle and working submodels $\mathcal{M}_0$ and $\mathcal{M}_n$ to be submodels of  $\mathcal{M}$ compatible with the regression models $\Theta_0$ and $\Theta_n$, respectively. We use $\mathcal{T}_n$ and $\mathcal{T}_0$ to denote the implied working and oracle linear models for the CATE $\tau_0$. All linear models are assumed to be closed subspaces of $L^2(P_0)$.
 



\begin{example}[Sparse basis selection using the Lasso]
   Let $\Theta$ be the linear closure of a countable basis $\Phi=\{\varphi_1,\varphi_2,\dots\}$ for the outcome regression $\mu_0$. The oracle submodel $\Theta_0$ is the linear closure of a sub-basis $\Phi_0 \subseteq \Phi$ that corresponds to the nonzero coefficients in the basis expansion of $\mu_0$. For the working model $\Theta_n$, we can use a linear span of data-adaptive basis functions selected through sparsity-driven methods like the Lasso. The literature extensively covers support recovery under sparsity constraints, which we review in Section \ref{section::dataAdaptParam}.
\end{example}




 

To construct an ADMLE of the ATE, we consider inference for the oracle ATE projection parameter $\Psi_0: \mathcal{M}_{np} \rightarrow \mathbb{R}$ defined as the map
\begin{equation}
    P \mapsto \Psi_0(P) := E_P \left\{\Pi_0 \mu_P(1,W) -  \Pi_0 \mu_P(0,W)\right\}, \label{eqn::ATEintroExampOracle}
\end{equation}
where $\Pi_0 \mu_P:=\arg\min_{\theta\in \Theta_0}E_P\left\{\mu_P(A,W)-\theta(A,W)\right\}^2$ is the best approximation of $\mu_P$ within $\Theta_0$. The corresponding data-adaptive working parameter $\Psi_n: \mathcal{M}_{np} \rightarrow \mathbb{R}$ is then given by the mapping $P \mapsto \Psi_n(P) := E_P \left\{\Pi_n \mu_P(1,W) -  \Pi_n \mu_P(0,W)\right\}$, where $\Pi_n \mu_P:=\arg\min_{\theta\in \Theta_n}E_P\left\{Y-\theta(A,W)\right\}^2$ is the projection of $\mu_P$ onto the working model $\Theta_n$. The following examples illustrate general classes of ADMLEs for the ATE based on parametric and semiparametric working regression models.  


\begin{example}[ADML for finite-dimensional working models]
\label{example::regress::intro}
    When $\Theta_n$ is a finite-dimensional linear space, the least-squares plug-in estimator $\widehat \psi_n := \frac{1}{n}\sum_{i=1}^n \{\mu_n(1,W_i) - \mu_n(0,W_i)\}$ with $\mu_n := \argmin_{\theta \in \Theta_n} \sum_{i=1}^n \left\{Y_i - \theta(A_i, W_i) \right\}^2$ is an ADMLE of the ATE. This ADMLE encompasses many two-stage plug-in estimators that involve model selection before evaluation of the parameter $\Psi$, including post-Lasso OLS \citep{Tibshirani94regressionshrinkage, belloni2012sparse, belloni2013least, belloni2014inference, moosavi2023costs}, step-wise regression procedures such as MARS \citep{Friedman1991}, and cross-validated and penalized sieve estimators \citep{spnpsieve, belloni2014inference}. 
\end{example}

 

\begin{example}[ADML for partially linear working models]
    \label{example::partially::intro}
   Suppose $\Theta_n$ and $\Theta_0$ are partially linear regression models corresponding to the CATE models $ \mathcal{T}_n$ and $\mathcal{T}_0$, respectively \citep{robinson1988root}. The partially linear regression model enables direct modelling of the conditional average treatment effect. Given user-supplied estimators $m_n$ and $\pi_n$ of $m_0$ and $\pi_0$, a semiparametric ADMLE for the ATE is given by $\widehat\psi_n := \frac{1}{n}\sum_{i=1}^n \tau_n(X_i)$, where
$$\tau_n := \argmin_{\tau\in {\mathcal{T}}_n} \sum_{i=1}^n \left[Y_i - m_n(X_i) - \left\{A_i - \pi_n(X_i)\right\}\tau(X_i)\right]^2.$$
This partially linear ADMLE encompasses various data-adaptive CATE estimators, including the post-Lasso R-learner \citep{belloni2014inference, zhao2017selective, nie2021quasi}.
\end{example}
 
The partially linear ADMLE of Example \ref{example::partially::intro} serves as a working example throughout this paper and is therefore studied in subsequent sections. Results for the plug-in ADMLE of Example \ref{example::regress::intro} can instead be found in Appendix \ref{appendix::pluginAMLE}.

\subsection{Related work}
 \label{section::AMLEGeneral3Lit}
 


The impact of data-adaptive model selection on inference has been studied extensively in the literature --- see, e.g., \cite{bauer1988model, potscher1991effects, buhlmann1999efficient, hjort2003frequentist, bunea2004consistent, LeebModelSelect2005} and \cite{ claeskens2007asymptotic}. Some studies focus on superefficient estimators based upon consistent model selection procedures aiming to select a correctly specified model with probability tending to one, a feature commonly referred to as the `oracle property' \citep{buhlmann1999efficient, oracleSelectSCAD, LeebModelSelect2005, AdaptLassoOracle, AdaptLassoOracleStationary}. However, reliance on the oracle property has been criticized due to poor performance when an incorrect or approximately correct model is selected and due to the need for large sample sizes to achieve a high selection probability \citep{LeebModelSelect2005}. ADML relaxes the oracle property by only requiring the selected working submodel to approximate a fixed oracle submodel at a given sample size. We note that similar relaxations have been made in the context of post-Lasso-based estimators in high-dimensional linear regression models under approximate sparsity \citep{belloni2012sparse, belloni2013honest, belloni2014inference}. Another criticism of superefficient estimators is that resulting inferences may not hold uniformly over all local perturbations within a prespecified statistical model \citep{LeebModelSelect2005, adaptLassoBadPerf, wu2019hodges}. Although ADML does not provide locally uniformly valid inference for the original target parameter $\Psi$ within the nonparametric model, we demonstrate in Section \ref{section::oracleParamInference} that they do provide such inference for the projection-based oracle parameter $\Psi_0$. Moreover, for the original parameter $\Psi$, we establish that ADML provides locally uniformly valid inference for local perturbations within the oracle submodel $\mathcal{M}_0$. Such criticisms of superefficient estimators may not be as applicable in situations in which regular nonparametric estimators do not exist or are too variable for reliable inference, such as when estimating the ATE with limited or no overlap \citep{moosavi2023costs}.

Selective inference involves conducting inference after examining the data, particularly in the context of high-dimensional regression models with data-driven model selection \citep{ modelselectionbias,zhang2014confidence, lee2016exact, zhao2017selective,  danielleWittenLassoWorks, PostselectionInference}. Previous works have addressed this topic by focusing on inference for infinite-dimensional coefficient vectors identified through model selection techniques, but this poses challenges due to a lack of pathwise differentiability, resulting in irregular behavior and nonstandard estimator convergence rates \citep{potscherAdaptLasso2009distribution, adaptLassoBadPerf, cai2017confidence, yang2021rates}. Conditional selective inference \citep{lee2016exact, condSelectInf} offers one solution by constructing valid p-values and confidence intervals conditioned on the selected model, but it typically relies on strong distributional assumptions and a case-by-case analysis. It has been shown that selective inference is more attainable for smooth functionals of a coefficent vector in a high-dimensional linear model \citep{zhang2011confidence, belloni2012sparse, belloni2013honest, belloni2014inference, van2014asymptotically, javanmard2014confidence}. We build upon these contributions by considering inference after model selection for pathwise differentiable functionals in general statistical models. By leveraging the smoothness of these functionals, we derive $\sqrt{n}\,$--consistent and asymptotically linear estimators within a flexible framework that imposes only high-level conditions on black-box model selection procedures. In contrast to several selective inference works, we demonstrate the validity of seemingly naive model-based inference methods that ignore variation due to model selection. Notably, our general theorems recover existing results for both single-selection and double-selection estimators \citep{belloni2012sparse, belloni2013honest, belloni2014inference} in the special case of a smooth functional of an approximately sparse high-dimensional linear model.

 
 Our work contributes to the literature on obtaining inference for data-adaptive target parameters by providing asymptotically normal estimators for a broad class of parameters defined through projections onto data-dependent working models. In contrast to previous works \citep{ van2013AdaptTarget, dataAdaptTargetParam,aronow2016data, rinaldo2019bootstrapping}, we achieve valid inference for these data-adaptive parameters by constructing asymptotically linear estimators without sample-splitting, thereby improving efficiency --- see Section \ref{section::dataAdaptParam}. There are several examples in the literature where inference for a data-adaptive target parameter happens to also provide valid inference for a fixed population parameter due to negligible bias of the data-dependent estimand with respect to the fixed target estimand. For example, this occurs in the context of estimating the causal effect of the optimal dynamic treatment \citep{van2015OptRule} and for certain measures of  variable importance \citep{williamson2021nonparametric}. Our work contributes to this literature by establishing general conditions under which inference for smooth functionals of a projection onto a data-dependent working model provides valid inference for that of a fixed oracle model.

  
 
Several superefficient estimators based on debiased machine learning and adaptive nuisance estimator selection have been proposed in the literature. One notable approach is collaborative TMLE (CTMLE) \citep{CTMLE, CTMLELasso, adaptiveTruncationCTMLE}, which adjusts the level of aggressiveness in the debiasing step by adaptively selecting from a range of increasingly complex models for orthogonal nuisance parameters. Similarly, the outcome-adaptive Lasso \citep{outcomeadaptiveLasso}, the outcome-adaptive HAL-TMLE based on the highly adaptive Lasso (HAL) \citep{vanderlaanGenerlaTMLEFIRST, HALoutcomeAdapt}, and the super-efficient ATE estimator proposed by \cite{ATEsupereff} all employ a model selection strategy for an orthogonal nuisance parameter based on the goodness-of-fit to the relevant portion of the data-generating distribution. \cite{cui2019selective} propose a cross-validation technique for selecting among various ATE estimators based on different machine learning estimators that provides valid selective inference. In this work, we contribute to this literature by presenting a unified framework for constructing adaptive superefficient estimators of smooth parameters in a general statistical model. 


 


\section{Defining a projection-based oracle parameter}
\label{section::oracleparameter}

 

\subsection{Definition of oracle parameter and superefficiency considerations} 
 

  

Let $\ell:\mathbb{R}^d \times \mathcal{M}_{np}\rightarrow\mathbb{R}$ be a loss function satisfying $P \in \argmin_{ Q \in \mathcal{M}_{np}} P \ell(\cdot, Q)$ for each $P \in \mathcal{M}_{np}$. We define the (possibly non-unique) loss-based projection operator $\Pi_0: \mathcal{M}_{np}\rightarrow \mathcal{M}_0$ as any map $P \mapsto \Pi_0P$ whose range is contained in the solution set $\argmin_{Q \in \mathcal{M}_0} P \ell(\cdot, Q)$. Informally, the projection $\Pi_0  $ maps a given distribution $P \in \mathcal{M}_{np}$ to one of its best approximations in $\mathcal{M}_0$ under the risk $Q \mapsto P \ell(\cdot, Q)$. The oracle projection parameter $\Psi_0: \mathcal{M}_{np} \rightarrow \mathbb{R}$, formally defined as $\Psi_0:=\Psi \circ \Pi_0$, applies the oracle projection operator $\Pi_0$ before evaluating the target parameter mapping $\Psi$.  If $\Pi_0$ is the loglikelihood projection and $\mathcal{M}_0$ is a fixed parametric model, $\Psi_0(P)$ corresponds to the $P$-limit that a maximum likelihood estimator (MLE) would converge to, even if the MLE is computed under an incorrectly specified model \citep{white1982MLErobust, freedman2006MLErobust}. 


In general, the efficiency bound for $\Psi_0$ depends not only on the oracle model ${\cal M}_0$ but also on the choice of loss-based projection $\Pi_0$. The following theorem provides a characterization of the efficient influence function $D_{0,P_{0}}$ in terms of the oracle model and the loss function. We require that the loss function $\ell$ and oracle parameter $\Psi_0$ satisfy the following conditions:
 

 
 
\begin{enumerate}[label=(A\arabic*), ref=A\arabic*,series=cond]
\item \textit{Invariance of $\Psi$ over solution set:} For all $P \in \mathcal{M}_{np}$, $\argmin_{Q \in \mathcal{M}_0} P\ell(\cdot, Q)$ is nonempty and $\Psi(Q) = \Psi(Q')$ for all $Q, Q' \in \argmin_{Q \in \mathcal{M}_0} P\ell(\cdot, Q) $.   \label{cond::lossProjIdent}
\item  \textit{Pathwise differentiability of $\Psi_0$ at $P_0$:} The oracle projection parameter $\Psi_0:\mathcal{M}_{np}\rightarrow\mathbb{R}$  is pathwise differentiable at $P_0$ with efficient influence function $o \mapsto D_0(o; P_0)$.
\label{cond::oracleParamPathwise}
\item  \textit{Loss function is smooth:} For all $P \in \mathcal{M}_{np}$ and regular paths $\{Q_{t}: t \in \mathbb{R}\} \subset \mathcal{M}_0$ through $\Pi_0 P$, there exists a G\^{a}teaux derivative $\frac{d}{dt} \ell(\cdot, Q_t) \big|_{t=0} \in L^2(P)$ such that $\frac{d}{dt} P  \ell(\cdot, Q_t)   \big|_{t=0}  = P \left\{ \frac{d}{dt} \ell(\cdot, Q_t) \big|_{t=0} \right\}$.  \label{cond::lossSmooth}
\item \textit{Risk minimizer determined by score equations:} For each $P \in \mathcal{M}_{np}$, $Q_P \in \argmin_{Q \in \mathcal{M}_0} P \ell(\cdot, Q)$ if and only if $\frac{d}{dt} P \ell(\cdot,Q_t)\big |_{t=0} = 0$ for each regular path $\{Q_t: t \in \mathbb{R}\} \subseteq \mathcal{M}_0$ with $Q_t = Q_P$ at $t=0$.   \label{cond::lossProjDeriv}
\end{enumerate}
 

In the following theorem, we define the loss-based tangent space $\mathcal{S}_{\mathcal{M}_0}(P)\subseteq L^2_0(P_0)$ of the oracle submodel $\mathcal{M}_0$ at any $P\in {\cal M}$ as the closure of the linear span of $P$--weak G\^{a}teaux derivatives (i.e., $\ell$-scores) of the form $\frac{d}{dt}  \ell(\cdot, Q_t)  \big|_{t = 0}$, where $\{Q_{t}: t \in \mathbb{R}\} \subseteq \mathcal{M}_0$ is a regular path with $Q_{t} = \Pi_0P$ at $t=0$.  


 \begin{theorem}[Efficient influence function of oracle parameter]
 Under Conditions \ref{cond::lossProjIdent}-\ref{cond::lossProjDeriv},  the efficient influence function $D_{0,P_{0}}$ of the oracle projection parameter $\Psi_0 :{\cal M}_{np}\rightarrow\mathbb{R}$ at $P_0$ is an element of $\mathcal{S}_{\mathcal{M}_0}(P_0)$.
As a consequence, if $\mathcal{S}_{{\cal M}_0}(P_0)$ is a subspace of the tangent space $T_{{\cal M}_0}(P_0)$ at $P_0$ for model ${\cal M}_0$, then $D_{0,P_0}$ equals the $P_0$--efficient influence function of $\Psi:{\cal M}_0\rightarrow\mathbb{R}$. 
\label{theorem::lossbasedEIF}
\end{theorem}




 The loss-based tangent space $\mathcal{S}_{\mathcal{M}_0}(P)$ consists of loss-based scores of paths through $\Pi_0P$ that remain in the oracle model, and so, it is a subspace of $L^2_0(P)$. For the loglikelihood loss $\ell(\cdot, Q)=-\log\left(\frac{dQ}{d\mu}\right)$, the loss-based tangent space $\mathcal{S}_{{\cal M}_0}(P_0)$ equals the tangent space $T_{P_0}({\cal M}_0)$ at $P_0$ for the model ${\cal M}_0$. Thus, as a consequence of Theorem \ref{theorem::lossbasedEIF}, the efficient influence function of the oracle parameter $\Psi_0$ for the loglikelihood loss at $P_0 \in \mathcal{M}_0$ is equal to the efficient influence function of the parameter $\Psi: \mathcal{M}_0 \rightarrow \mathbb{R}$ for the oracle model $\mathcal{M}_0$. In such cases, an efficient estimator for $\Psi_0$ at $P_0$ performs as well in a local asymptotic minimax sense as an efficient estimator that knew the oracle model $\mathcal{M}_0$ beforehand. 



  
  Conditions \ref{cond::lossSmooth} and \ref{cond::lossProjDeriv}  are imposed to ensure the smoothness of the loss function and are generally satisfied by most practical loss functions. When the minimizing set $\argmin_{Q \in \mathcal{M}_{0}} P \ell(\cdot, Q)$ is either empty or contains more than one element, the definition of the oracle  projection parameter can be complicated. This is indeed the case for the oracle parameter given in the next section, which depends solely on the outcome regression and covariate distribution. Condition \ref{cond::lossProjIdent} alleviates this concern by enforcing, for a given loss $\ell$, that the choice of loss-based projection operator $\Pi_0$ does not affect the definition of the oracle parameter $\Psi_0$. Condition \ref{cond::oracleParamPathwise} assumes that $\Psi_0:{\cal M}_{np}\rightarrow\mathbb{R}$ is pathwise differentiable at $P_0$, allowing for efficient estimation of $\Psi_0(P_0)$ at $\sqrt{n}$--rate \citep{bickel1993efficient}, even if $\Psi$ itself is not pathwise differentiable at $P_0$.  In most cases, the smoothness of $\Psi_0$ at $P_0$ follows when $\Psi$ is pathwise differentiable at $P_0$ under $\mathcal{M}_0$ and the risk functional $(P,Q) \mapsto P\ell(\cdot, Q)$ used to define the loss-based projection operator $\Pi_0$ is smooth in a suitable sense.
  
  
 



 


\subsection{Example: working ATE for overlap-weighted projection of CATE}
\label{section::oracleparam::example}





We now revisit Example \ref{example::partially::intro}. In this example, the oracle statistical model $\mathcal{M}_0$ is such that $P \in \mathcal{M}_0$ if and only if $\mu_P$ is in the partially linear regression model \[\Theta_0 := \left\{(w,a) \mapsto \mu(w,0) + a \tau(w): \mu \in L^2(P_{0,A,W}), \tau \in \mathcal{T}_0 \right\},\] where $\mathcal{T}_0$ is an unknown but learnable linear CATE model for $\tau_0$ and $P_{0,A,W}$ refers to the distribution of $(A,W)$ implied by $P_0$. Using Robinson's transformation \citep{robinson1988root} of the outcome regression, given $P_0$--almost everywhere by $\mu_0:(a,w)\mapsto m_0(w) + \{a - \pi_0(w)\}\tau_0(w)$ with $m_0(w):=E_0(Y \,|\, W=w)$, the oracle parameter defined in \eqref{eqn::ATEintroExampOracle} can be expressed as $P \mapsto  \Psi_0(P) := E_P\left\{\Pi_0\tau_P(W)\right\}$, where $\Pi_0\tau_P := \argmin_{\tau \in \mathcal{T}_0}E_P \left[Y -  m_P(W) - \{A - \pi_P(W)\} \tau(W)\right]^2$.  Interestingly, it can be shown that $\Pi_0 \tau_P$ is the overlap-weighted projection of the CATE \citep{imbensOverlapEstimand2006,li2019overlapWeights, d2021overlap, morzywolek2023general}.



The oracle parameter $\Psi_0$ corresponds to the composite least-squares loss defined pointwise as $\ell(o, Q) := \left\{y- \mu_Q(a,w)\right\}^2 - \log \left\{\frac{dQ_W}{d\mu}(w)\right\}$, where $Q_W$ is the distribution of $W$ under $Q$. The negative loglikelihood term ensures that the induced projection $\Pi_0 P$ leaves the covariate distribution of $P$ unchanged. While the minimizer of the risk $Q \mapsto P \ell(\cdot, Q)$ over any submodel of $\mathcal{M}_{np}$ is typically nonunique, the loss function $\ell$ satisfies the conditions of Theorem \ref{theorem::lossbasedEIF}. In particular, when $\Psi_0$ is pathwise differentiable, the efficient influence function of $\Psi_0$ lies in the loss-based tangent space and is given by the following theorem. For the statement of this theorem, we introduce the following condition:

\begin{enumerate}[label=(E\arabic*), ref=E\arabic*,series=condE]
 \item $\gamma_P(W) := \argmin_{\gamma \in \mathcal{T}_0} E_P\left[\pi_P(W)\{1-\pi_P(W)\}\gamma(W)^2 -  2 \gamma(W) \right]$ exists. \label{cond::CATE::boundedFunc}
\end{enumerate}
  

  
\begin{theorem}[Efficient influence function under partially linear model]
    Under Condition \ref{cond::CATE::boundedFunc}, the oracle parameter $\Psi_0:\mathcal{M}_{np}\rightarrow\mathbb{R}$ is pathwise differentiable at $P$ with efficient influence function
    $$D_{0,P}(o) = \Pi_0 \tau_P(w) - E_P\left\{\Pi_0\tau_P(W)\right\} +  \gamma_P(w)\left\{a - \pi_P(w)\right\}\left\{y - \Pi_0 \mu_P(a,w) \right\},$$ 
    which is an element of $\mathcal{S}_{\mathcal{M}_0}(P_0) = L^2_0(P_{0,W}) \oplus \left\{o \mapsto h(a,w)\{y - \Pi_0 \mu_P(a,w)\} : h \in L^2(P_{0,A,W})\right\}$.\label{example::theorem::RlearnerEIF}
\end{theorem}
Condition \ref{cond::CATE::boundedFunc} holds if and only if the linear functional $\mu \mapsto E_P\left\{\mu(1,W) - \mu(0,W)\right\}$ is bounded on $\Theta_0$. When $\pi_P(W)\{1-\pi_P(W)\} > 0$ almost surely and its reciprocal has finite variance, $\gamma_P$ equals the overlap-weighted $L^2(P)$-projection of $\{\pi_P(1-\pi_P)\}^{-1}$ onto the linear working model $\mathcal{T}_0$. If $\mathcal{T}_0 := L^2(P_{0,W})$, then $\Psi_0 = \Psi$ and $\gamma_P = \{\pi_P(1-\pi_P)\}^{-1}$ so that Theorem \ref{example::theorem::RlearnerEIF} recovers the nonparametric efficient influence function of the ATE. 
 

 
\section{Inference for data-adaptive projection-based working parameter}

\label{section::dataAdaptParam}

 



 


 

\subsection{Asymptotic linearity for the data-adaptive working parameter}




In this section, we outline the conditions under which the ADMLE $\widehat{\psi}_n$ is $\sqrt{n}\,$--consistent and asymptotically normal as an estimator of the data-adaptive working estimand $\Psi_n(P_0)$. We will demonstrate that if $\Psi_n$ locally converges in an appropriate sense to $\Psi$ around $P_0$, the ADMLE exhibits not only $P_0$--asymptotic normality but also $P_0$--asymptotic linearity, with the influence function being the $P_0$--efficient influence function of the oracle parameter $\Psi_0$. In contrast to the work of \cite{rinaldo2019bootstrapping}, we allow for arbitrary dependence between $\mathcal{M}_n$ and the data; in particular, we do not require that sample-splitting be used to compute $\mathcal{M}_n$ and $\widehat{\psi}_n$. We will refer to the following conditions in the theorem below:

 \begin{enumerate}[label=(B\arabic*), ref=B\arabic*,series=cond2]

\item \textit{First order expansion for $\widehat{\psi}_n$:}  $\widehat{\psi}_n = \Psi_n(P_0) + (P_n-P_0) D_{n,P_0} + o_p(n^{-1/2})$ with $D_{n,P_0}$ the efficient influence function of $\Psi_n:\mathcal{M}_{np}\rightarrow\mathbb{R}$;\label{cond::debiased}%$\norm{ D_{n}(\cdot\, ;\widehat P_n) - D_{n,P_0}}_{P_0} = o_p(1)$. }
\end{enumerate}


 
\begin{enumerate}[label=(B\arabic*), ref=B\arabic*,resume=cond2]


\item \textit{Local consistency of $\Psi_n$ for $\Psi_0$:}  $ \norm{D_{n,P_0}- D_{0,P_{0}}}_{P_0} = o_p(1)$;\label{cond::consDn}
\item \textit{Negligible empirical process remainder:}  $(P_n - P_0) \left(D_{n,P_0} - D_{0,P_0}\right) =  o_p(n^{-1/2})$. 
\label{cond::Donsker}

 
\end{enumerate}

\begin{theorem}[Asymptotic linearity for data-adaptive working parameter]
    Under Conditions \ref{cond::lossProjIdent}--\ref{cond::lossProjDeriv} and \ref{cond::debiased}--\ref{cond::Donsker}, the ADMLE $\widehat{\psi}_n$ is a $P_0$--asymptotically linear estimator of $\Psi_n(P_0)$ with influence function equal to the $P_0$--efficient influence function of $\Psi_0$ relative to $\mathcal{M}_{np}$.
    \label{theorem::limitDataAdapt}
\end{theorem}


Condition \ref{cond::debiased} is the defining property of the ADMLE and can be guaranteed to hold using debiased machine learning techniques for the working parameter $\Psi_n$. Condition \ref{cond::consDn} is equivalent to requiring that the pathwise derivative operator $d\Psi_n(P_0): L^2_0(P_0)\rightarrow \mathbb{R}$ is consistent in operator norm for $d\Psi_0(P_0) : L^2_0(P_0)\rightarrow \mathbb{R}$. Condition \ref{cond::Donsker} is implied by \ref{cond::consDn} as long as $D_{n,P_0}$ falls in a $P_0$--Donsker class.




When $\ell$ is the negative loglikelihood loss, the following lemma establishes sufficient conditions for \ref{cond::consDn}. An analogous result can often be established for losses based on working loglikelihoods --- an example is provided in Section \ref{section::DataAdaptexampleATEPartially}. Let $P_{n,0} := \Pi_n P_0\in {\cal M}_n$ denote the projection of $P_0$ onto the working model ${\mathcal{M}}_n$, and for the loss-based score $D_{0,P_{n,0}} \in \mathcal{S}_{\mathcal{M}_0}(P_{n,0})$, let $\bar{D}_{0,P_{n,0}} := \argmin_{s \in \mathcal{S}_{\mathcal{M}_n}(P_{n,0})} \|D_{0,P_{n,0}} - s \|_{L^2(P_{n,0})}$ denote the $L^2(P_{n,0})$--projection of $D_{0,P_{n,0}}$ onto the working loss-based tangent space $\mathcal{S}_{{\mathcal{M}}_n}(P_{n,0})$. The lemma below involves the following condition:

\begin{enumerate}[label=\ref{cond::consDn}*), ref={\ref{cond::consDn}*}]
\item \begin{enumerate}[label={\bf \alph*.}, ref=\alph*,leftmargin=15pt]
\item \textit{Loglikelihood-like loss:}  $Q \mapsto \ell(\cdot, Q)$ is either the negative loglikelihood loss or is such that $\mathcal{S}_{\mathcal{M}_n}(P_{n,0}) \subseteq T_{\mathcal{M}_n}(P_{n,0})$ and $\mathcal{S}_{\mathcal{M}_0}(P_{n,0}) \subseteq T_{\mathcal{M}_0}(P_{n,0})$.\label{cond::loglikloss}
\item \textit{Weak consistency: } $\norm{D_{n, P_{n,0}} - D_{n, P_0}}_{P_0} + \norm{D_{0,P_{n,0}} - D_{0,P_0}}_{P_0} = o_p(1)$. \label{cond::weakconstencyOracle}
\item \textit{Negligible tangent space approximation error:}   \label{cond::consistentTangentSpace}  $ \norm{D_{0,P_{n,0}} - \bar{D}_{0,P_{n,0}}}_{P_0} = o_p(1)$. 
\item \textit{Locally nested working model:} $P_{n,0} \in \mathcal{M}_0$ and $T_{\mathcal{M}_n}(P_{n,0}) \subseteq T_{\mathcal{M}_0}(P_{n,0})$ with probability tending to one. \label{cond::nestedModel}
\end{enumerate}
\label{cond::loglikEquivCond}
\end{enumerate}

\begin{lemma}[Sufficient conditions for B2]
    Condition \ref{cond::loglikEquivCond} implies Condition \ref{cond::consDn}. \label{lemma::stableLogLik}
\end{lemma}

 Condition \ref{cond::loglikEquivCond}\ref{cond::weakconstencyOracle} imposes a mild consistency assumption on the working model projection $P_{n,0}$. Condition \ref{cond::loglikEquivCond}\ref{cond::consistentTangentSpace} requires that the working tangent space $T_{\mathcal{M}_n}(P_{n,0})$ can sufficiently approximate elements of the tangent space $T_{\mathcal{M}_0}(P_{n,0})$. To illustrate this, suppose that $\mathcal{M}_0$ and $\mathcal{M}_n$ are indexed by the linear span of basis functions that are learned using the Lasso or cross-validation. To satisfy this condition, the model selection procedure must include basis functions of the oracle submodel that are important for approximating the efficient influence function $D_{0,P_{n,0}}$ of $\Psi_0$ at a sufficiently fast rate. Condition \ref{cond::loglikEquivCond}\ref{cond::nestedModel} is, in our view, the strongest assumption and restricts the possible model selection procedures used to obtain $\mathcal{M}_n$. A sufficient condition is that $\mathcal{M}_n \subseteq \mathcal{M}_0$ with probability tending to one. 
 

Condition \ref{cond::loglikEquivCond}\ref{cond::nestedModel}  is plausibly satisfied in discrete optimization settings, where $\mathcal{M}_0$ or a sufficient approximation of $\mathcal{M}_0$ is known to be contained in a finite (potentially growing) collection of candidate models. In particular, this holds if the model selection procedure used to obtain ${\cal M}_n$ is able to learn the exact support of $P_0$ (in terms of basis functions) with probability tending to one. For example, cross-validation oracle inequalities \citep{DudoitVanderLaan2005, wasserman2009high} establish that, under general conditions, the cross-validation model selector selects $\mathcal{M}_0$ or a submodel thereof with probability tending to one, even when the number of candidate models to grow polynomially with sample size. Additionally, a number of popular model selection methods based on sparsity constraints can satisfy this condition. The Lasso \citep{Tibshirani94regressionshrinkage} is a sparsity-driven variable selection procedure that can satisfy the stronger property of exact support recovery, namely that $P({\cal  M}_n= {\cal M}_0)\rightarrow 1$, in sparse high-dimensional settings \citep{zhao2006model, wainwright2009sharp}. The adaptive Lasso and SCAD are related methods that can achieve exact support recovery under potentially weaker conditions \citep{oracleSelectSCAD,AdaptLassoOracle, AdaptLassoOracleStationary}. Several variable and model selection methods for nonparametric and semiparametric models have also been shown to satisfy the exact support recovery under conditions \citep{ravikumar2009sparse, huang2010variable, su2014variable, xu2016faithful, amato2022wavelet}. There are numerous methods for controlling the false discovery rate of variable selection methods that can satisfy the weaker condition $P({\cal  M}_n\subseteq {\cal M}_0) \rightarrow 1$ \citep{donoho2005stable, meinshausen2010stability,sampson2013controlling, zhang2014confidence,   fithian2015selective, barber2015controlling, candes2016panning, huang2017controlling, javanmard2019false}. 
  




 

 


 
\subsection{Example: asymptotic linearity for data-adaptive working ATE}
\label{section::DataAdaptexampleATEPartially}



We now apply the theory of this section to the partially linear ADMLE of the ATE introduced in Example \ref{example::partially::intro} and the overlap-weighted projection-based oracle parameter $\Psi_0$ of \eqref{eqn::ATEintroExampOracle}.

The corresponding data-adaptive working parameter $\Psi_n:\mathcal{M}\rightarrow \mathbb{R}$ is defined pointwise as
$\Psi_n(P) :=  E_P\left\{\Pi_n\tau_P(W)\right\}$ with
\[\Pi_n\tau_P := \argmin_{\tau \in \mathcal{T}_n}E_P \left[Y -  m_P(W) - \left\{A - \pi_P(W)\right\} \tau(W)\right]^2.\] Hence, the partially linear ADMLE $\widehat{\psi}_n$ is simply a plug-in estimator of $\Psi_n(P_0)$. The first-order equations that characterize the empirical risk minimizer $\tau_n$ imply that $\frac{1}{n}\sum_{i=1}^{n}D_{n,\widehat{P}_n}(O_i) = 0$ so that $\widehat{\psi}_n$ is in fact an ATMLE and satisfies \ref{cond::debiased} under mild conditions.

We now state our main result. To this end, we denote the overlap-weighted $L^2(P_0)$--norm of a function $f \in L^2(P_{0,W})$ by $\|f\|_{w_0P_0} :=   \|w_0^{1/2}f\|_{P_0}$ with $w_0 := \pi_0(1-\pi_0)$, and introduce the following conditions:



\begin{enumerate}[label=E\arabic*), ref={E\arabic*}, resume=condE]
    \item \textit{Donsker condition: } $\tau_n$, $\pi_n$, $m_n$ and $\Pi_n \gamma_0$ are uniformly bounded and fall in a fixed $P_0$--Donsker class with probability tending to one;
     \label{cond::CATE::DonskerMLE}
     \item  \textit{Nested working model:} $\mathcal{T}_n \subseteq \mathcal{T}_0$ with probability tending to one;\label{cond::CATE::nestedModel}
     \item \textit{Consistency of nuisance estimators:} $\norm{\pi_n - \pi_0}_{P_0}+ \norm{\tau_n - \Pi_n \tau_0}_{P_0} +  \|m_n - m_0\|_{P_0} = o_p(1)$;\label{cond::CATE::consistentNuis}
     \item  \textit{Consistency of working model:} $\norm{\gamma_0 - \Pi_n \gamma_0}_{w_0P_0} + \norm{\Pi_n \tau_0 - \tau_0}_{P_0} = o_p(1)$;\label{cond::CATE::consistentWorking}
     \item   \textit{Sufficient nuisance rates:}  $\norm{\pi_n - \pi_0}_{P_0} = o_p(n^{-1/4})$ and $\norm{\pi_n - \pi_0}_{P_0}\norm{m_n - m_0}_{P_0}= o_p(n^{-1/2})$.\label{cond::CATE::DRterm}
\end{enumerate}

 

 
\begin{theorem}[Inference for data-adaptive working ATE]
     Under Conditions \ref{cond::CATE::boundedFunc}-\ref{cond::CATE::DRterm}, the partially linear ADMLE $\widehat{\psi}_n$ is a $P_0$--asymptotically linear estimator of $\Psi_n(P_0)$ with influence function given by the $P_0$--efficient influence function $D_{0,P_0}$ of $\Psi_0$ relative to $\mathcal{M}_{np}$.
    \label{example::theorem::RlearnerLimitDist}
\end{theorem}



In particular, Theorem \ref{example::theorem::RlearnerLimitDist} implies that  $\sqrt{n}\,\{\widehat{\psi}_n - \Psi_n(P_0)\}$ tends in distribution to a mean-zero random variable with variance $\sigma_0^2 := var_{P_0}\left\{D_{0,P_0}(O)\right\}$. Conditions \ref{cond::CATE::boundedFunc} and \ref{cond::CATE::nestedModel} together ensure that the parameters $\Psi_n$ and $\Psi_0$ are pathwise differentiable. Condition \ref{cond::CATE::DonskerMLE} restricts the complexity of nuisance estimators $\pi_n$ and $m_n$ and can be relaxed to allow for the use of generic machine learning tools using cross-fitting \citep{vanderLaanRose2011, DoubleML}. The requirement that $\tau_n$ fall in a Donsker class is satisfied by various estimators, including the highly adaptive Lasso and Lasso-regularized regression over reproducing kernel Hilbert spaces. However, without strong sparsity conditions, this condition may be violated in high-dimensional settings \citep{DoubleML,chernoapproxSparse2019}. Condition \ref{cond::CATE::nestedModel} ensures that ${\mathcal{M}}_n \subseteq \mathcal{M}_0$ with probability approaching one, which, as discussed in Section \ref{section::dataAdaptParam}, can hold for various model selection algorithms. Conditions \ref{cond::CATE::consistentNuis} and \ref{cond::CATE::consistentWorking} typically require that $\mathcal{T}_n$ be finite-dimensional and impose mild consistency requirements on the nuisance estimators and projections. Finally, Condition \ref{cond::CATE::DRterm} is a standard nuisance rate condition for partially linear regression, and is trivially satisfied when the propensity score $\pi_0$ is known and $\pi_n = \pi_0$. 


\section{Adaptive and superefficient inference for the target parameter}
\label{section::oracleParamInference}

 



 
\subsection{Oracle model approximation bias is second-order}
\label{section::oracleParamInference::oraclebias}

We now establish results on the regularity, asymptotic linearity, and (super)efficiency of $\widehat{\psi}_n$ at $P_0$ for the parameters $\Psi_0$ and $\Psi$ relative to $\mathcal{M}_{np}$. Previously, we established conditions under which it holds that $\widehat{\psi}_n$ is a $P_0$--asymptotically linear estimator for $\Psi_n$ with influence function equal to the $P_0$--efficient influence function of $\Psi_0$. Consequently, to establish the $P_0$--asymptotic linearity of $\widehat{\psi}_n$ as an estimator of $\psi_0$, it suffices to show that the oracle bias $\Psi_n(P_0) - \psi_0=\Psi_n(P_0)-\Psi_0(P_0)$ is $o_p(n^{-1/2})$ and thus asymptotically negligible.

The following lemma establishes that this oracle bias is second-order and tends to zero at a rate determined by how well the working model $\mathcal{M}_n$ approximates the oracle model $\mathcal{M}_0$. We recall that $P_{n,0} := \Pi_n P_0$ is the loss-based projection of $P_0$ onto the working model $\mathcal{M}_n$, and that $\bar{D}_{0,P_{n,0}} \in S_{{\cal M}_n}(P_{n,0})$ is the $L^2(P_{n,0})$--projection of the efficient influence function $D_{0,P_{n,0}} \in S_{{\cal M}_0}(P_{n,0})$ of $\Psi_0$ onto the working loss-based tangent space $S_{{\cal M}_n}(P_{n,0})$. 


 
  
\begin{lemma}[Representation for oracle bias]
  
    Suppose that Conditions \ref{cond::lossProjIdent}-\ref{cond::lossProjDeriv} hold. On the event $\{P_{n,0} \in \mathcal{M}_0\}$, which in particular holds if ${\cal M}_n\subseteq {\cal M}_0$, the oracle bias can be decomposed as 
    \begin{align*}
        \Psi_n(P_0) - \Psi_0(P_0) = B_{n,0} + R_{n,0} 
    \end{align*}with $B_{n,0}:=(P_{n,0} - P_0)\left( D_{0,P_{n,0}} - \bar{D}_{0,P_{n,0}} \right)$ and 
   $R_{n,0} :=  \Psi_0(P_{n,0}) - \Psi_0(P_0) +  P_0 D_{0,P_{n,0}}$.
      \label{theorem::exactBiasOracle}
\end{lemma}
The critical term in the bias expansion of Lemma \ref{theorem::exactBiasOracle} is $B_{n,0}$, which can be upper bounded by \[\norm{\frac{dP_{n,0}}{d\mu}-\frac{dP_0}{d\mu}}_{\mu}\norm {\bar{D}_{0,P_{n,0}}-D_{0,P_{n,0}}}_{\mu}\] in view of the Cauchy-Schwarz inequality.  Typically, the remainder $R_{n,0}$ is second-order in how well $P_{n,0}$ approximates $P_0$ due to the pathwise differentiability of $\Psi_0$. We note that $P_{n,0}$ is the optimal loss-based approximation of $P_0 \in \mathcal{M}_0$ in $\mathcal{M}_n$, and $\bar{D}_{0,P_{n,0}}$ is the optimal $L^2_0(P_{n,0})$-approximation of $D_{0,P_{n,0}} \in \mathcal{S}_{\mathcal{M}_0}(P_{n,0})$ in $\mathcal{S}_{{\mathcal{M}}_n}(P_{n,0})$. As such, for the critical bias term to vanish asymptotically, $P_{n,0}$ should be consistent for the true distribution $P_0$ and the working model should locally approximate the oracle model near $P_{n,0}$ at sufficient rates. The requirement that ${P_{n,0} \in \mathcal{M}_0}$ with probability tending to one is weaker than the requirement that $\lim_{n\rightarrow \infty} P_0({\cal M}_n\subseteq {\cal M}_0) = 1$, which was sufficient for Condition \ref{cond::loglikEquivCond}\ref{cond::nestedModel}. In the event that ${P_{n,0} \not\in \mathcal{M}_0}$, the result of Lemma \ref{theorem::exactBiasOracle} still holds, up to negligible error, if $\Psi(P_{n,0})-\Psi(\Pi_0 P_{n,0})=o_p(n^{-1/2})$. Nonetheless, ensuring second-order behavior of $\Psi_n(P_0)-\Psi_0(P_0)$ may impose constraints on the model selection procedure used to obtain $\mathcal{M}_n$.

 
 
\begin{example}
In Appendix \ref{appendix::pluginAMLE}, we demonstrate that for the oracle ATE parameter \eqref{eqn::ATEintroExampOracle}, the critical bias term $B_{n,0}$ of Lemma \ref{theorem::exactBiasOracle} can be expressed as $-P_0\left\{(\alpha_0 - \Pi_n \alpha_0 )(\mu_0 - \Pi_n \mu_0) \right\}$, where $\alpha_0 \in \Theta_0$ is the Riesz representer of the linear functional $\alpha \mapsto E_0\left\{\alpha(1,W) - \alpha(0,W)\right\}$ for the oracle regression model $\Theta_0$ \citep{chernoRegRiesz, chernozhukov2018auto}, and $\Pi_n$ is the $L^2(P_0)$--projection onto the linear working model $\Theta_n$. This term depends on the approximation of $\mu_0$ and $\alpha_0$ by elements of $\Theta_n$. If $\Theta_n$ is selected using Lasso regression over a basis, the oracle bias is typically negligible when $\alpha_0$ and $\mu_0$ are approximately sparse under the basis functions that span $\Theta_0$ \citep{chernoapproxSparse2019}.
 
\end{example}
 



 
 \subsection{Regularity, asymptotically linearity, and efficiency for oracle parameter}

\label{section::oracleParamInference::oracle}
 
We now establish that an ADMLE is a regular, asymptotically linear, and nonparametric efficient estimator for the oracle parameter $\Psi_0$ at $P_0$ with respect to the nonparametric statistical model $\mathcal{M}_{np}$. The following theorem involves additional conditions:

\begin{enumerate}[label=C\arabic*), ref=C\arabic*, series = cond3]%,resume=cond]
\item  \textit{Projection of $P_0$ onto ${\cal M}_n$ is nearly in $\mathcal{M}_0$:}  $\Psi(\Pi_nP_0)-\Psi(\Pi_0(\Pi_n P_0))=o_p(n^{-1/2})$;\label{cond::oracleProjInModelFinal}
\item \textit{Negligible oracle bias:} $B_{n,0} + R_{n,0} =o_p(n^{-1/2})$.\label{cond::doubleRemrootnFinal}
\end{enumerate}




\begin{theorem}[Nonparametric regularity and efficiency for oracle parameter]
       Suppose that the conditions of Theorem \ref{theorem::limitDataAdapt} hold. Suppose also that 
 Conditions \ref{cond::oracleProjInModelFinal}--\ref{cond::doubleRemrootnFinal} hold for a fixed oracle submodel $\mathcal{M}_0\subseteq \mathcal{M}_{np}$ with $P_0\in {\cal M}_0$ and a data-dependent working model $\mathcal{M}_n$. Then, the ADMLE $\widehat{\psi}_n$ is a $P_0$--asymptotically linear estimator for $\Psi_0$ with influence function equal to the efficient influence function of $\Psi_0:\mathcal{M}_{np}\rightarrow \mathbb{R}$ at $P_0$ relative to $\mathcal{M}_{np}$.
      
   
      \label{theorem::oracleEff}
\end{theorem}

 We note that in the special case $\mathcal{M}_0 := \mathcal{M}_{np}$, wherein  $\Psi_0 = \Psi$, Theorem \ref{theorem::oracleEff} recovers known results for efficient plug-in estimation using TMLE \citep{vanderLaanRose2011}, undersmoothed empirical risk minimizers \citep{undersmoothedHAL}, or the method of sieves \citep{spnpsieve}. In contrast, when the efficiency bound of the oracle parameter $\Psi_0$ is smaller than that of $\Psi$, Theorem \ref{theorem::oracleEff} implies that an ADMLE is a $P_0$--asymptotically linear and $P_0$--superefficient estimator for $\Psi$ in the model $\mathcal{M}_{np}$.  An important consequence of Theorem \ref{theorem::oracleEff} is that an ADMLE is a $P_0$--regular estimator for $\Psi_0$ relative to the nonparametric model $\mathcal{M}_{np}$. Hence, even under sampling from a worst-case local perturbation of $P_0$, an ADMLE allows locally uniformly valid nonparametric inference on the oracle parameter $\Psi_0$. This implies that, at least in a local asymptotic sense, there is no loss in performance of the ADMLE from empirically learning $\mathcal{M}_0$ compared to the oracle that knows $\mathcal{M}_0$ or $\Psi_0$.
 
 
   The limiting variance $\sigma_{0}^2$ can typically be estimated consistently by the empirical plug-in  estimator $\sigma_n^2:=\frac{1}{n}\sum_{i=1}^{n}D_{n,\widehat P_n}(O_i)^2$  for some consistent estimator $\widehat{P}_n$ of $P_0$. For a maximum likelihood estimator over a data-adaptive parametric working model ${\mathcal{M}}_n$, $\sigma_n^2$ corresponds to the model-robust sandwich variance estimator and offers a simple way to estimate the limiting variance $\sigma^2_0$ of such superefficient estimators. This result is particularly useful for parameters whose efficient influence function does not admit a closed form \citep{MarcoComputerized}.



We note that the ADMLE $\widehat{\psi}_n$ of $\psi_0$ has the potential to achieve $\sqrt{n}$--consistency and asymptotic normality under weaker conditions, without assuming Condition \ref{cond::consDn}. Specifically, if we can show that $\sqrt{n}\,\{\widehat{\psi}_n - \Psi_n(P_0)\}/\sigma_n\rightarrow_d N(0,1)$ for a suitable, potentially random scaling constant $\sigma_n^2 > 0$, then Lemma \ref{theorem::exactBiasOracle} and Condition \ref{cond::doubleRemrootnFinal} imply that $\sqrt{n}\,(\widehat{\psi}_n - \psi_0)/\sigma_n \rightarrow_d N(0,1)$ under regularity conditions. The distributional convergence result for $\Psi_n(P_0)$ can be achieved under virtually no conditions on the model selection procedure using sample-splitting, although this may come at the cost of efficiency \citep{dataAdaptTargetParam, rinaldo2019bootstrapping}. Alternatively, without sacrificing efficiency, we can establish this convergence if the working model $\mathcal{M}_n$ is deterministic with probability tending to one. Notably, in the context of selective inference in high-dimensional regression models, \cite{danielleWittenLassoWorks} establish general conditions under which a Lasso-selected working model is equivalent to a nonrandom model $\mathcal{M}_n$ derived from a noiseless Lasso. While \cite{danielleWittenLassoWorks} focuses on establishing $\sqrt{n}$--consistency and asymptotic normality for Lasso-based estimators of the noiseless Lasso coefficients, our results extend this to the plug-in Lasso estimator for suitably smooth functionals of the true coefficient vector, assuming similar conditions and Condition \ref{cond::doubleRemrootnFinal}.








 

 


  

 


 
 
\subsection{Regularity, asymptotic linearity, and superefficiency for the original target parameter}

\label{section::oracleParamInference::original}
 

Theorem \ref{theorem::oracleEff} establishes that an ADMLE $\widehat{\psi}_n$ is a regular, asymptotically linear, and nonparametric efficient estimator for the oracle parameter $\Psi_0:\mathcal{M}_{np} \rightarrow \mathbb{R}$ at $P_0$, treating the oracle model $\mathcal{M}_0$ as given. Using that $\Psi(P) = \Psi_0(P)$ for all $P \in \mathcal{M}_0$, the following theorem establishes that the ADMLE $\widehat{\psi}_n$ is asymptotically linear and nonparametric superefficient for the original target parameter $\Psi$ at $P_0$. In addition, the ADMLE $\widehat{\psi}_n$ is regular, asymptotically linear, and potentially efficient for $\Psi$ at $P_0$ relative to the oracle submodel $\mathcal{M}_0$. 


A consequence of the following theorem is that plug-in maximum likelihood estimators based on data-dependent parametric working models are, under the stated conditions, $P_0$--asymptotically linear and achieve the $P_0$--efficiency bound of $\Psi$ under the oracle model $\mathcal{M}_0$. Notably, this theorem recovers existing results for both single-selection and double-selection estimators \citep{belloni2012sparse, belloni2013honest, belloni2014inference} in the special case of a smooth functional of an approximately sparse high-dimensional linear model.

 




 

 

 

\begin{theorem}[Regularity, asymptotic linearity, and efficiency for oracle model]
    Under the conditions of Theorem \ref{theorem::oracleEff}, the ADMLE $\widehat{\psi}_n$ satisfies the asymptotically linear expansion \[\widehat{\psi}_n = \psi_0 + (P_n - P_0) D_{0,P_0} + o_p(n^{-1/2})\] at $P_0$ where $D_{0,P_0}$ is the $P_0$--efficient influence function of $\Psi_0$. Moreover, $\widehat{\psi}_n $ is $P_0$--regular for $\Psi$ over all local alternatives $P_{0,hn^{-1/2}}$ in the oracle submodel $\mathcal{M}_0$.  Consequently, $\sqrt{n}\,(\widehat{\psi}_n - \psi_0) \rightarrow_d N(0, var_0\{D_{0,P_0}(O)\})$, even under sampling from local perturbations of $P_0$ remaining in $\mathcal{M}_0$. If, in addition, $\ell$ is the negative loglikelihood loss, or more generally, $S_{{\cal M}_0}(P_0) \subseteq T_{{\cal M}_0}(P_0)$, then the ADMLE $\widehat{\psi}_n$ is asymptotically $P_0$--efficient for $\Psi$ with respect to the oracle submodel $\mathcal{M}_0$.  \label{theorem::oracleRegularity}
\end{theorem}

 When the tangent space $T_{\mathcal{M}_0}(P_0)$ is smaller than $T_{\mathcal{M}}(P_0)$, Theorem \ref{theorem::oracleRegularity} typically implies that the ADMLE $\widehat{\psi}_n$ is $P_0$--superefficient for $\Psi$, with limiting variance smaller than the efficiency bound of $\Psi$ at $P_0$ for the model $\mathcal{M}$. While a $P_0$--superefficient ADMLE is necessarily irregular for $\Psi$ at $P_0$ relative to $\mathcal{M}$, this result establishes that it is nevertheless $P_0$--regular for $\Psi$ with respect to the oracle submodel $\mathcal{M}_0$.  Heuristically, the irregularity under sampling from local perturbations of $P_0$ outside $\mathcal{M}_0$ occurs because model selection procedures can become unstable \citep{LeebModelSelect2005}. Regardless, Theorem \ref{theorem::oracleRegularity} shows that the regularity and superefficiency of ADMLEs fall in a continuous spectrum driven by the size of the oracle model. Sacrificing some regularity can be justifiable to achieve efficiency gains, especially when nonparametric regular estimators for $\Psi$ are unavailable, such as when the ATE is nonparametrically unidentifiable. 

To understand the impact of irregularity on inference for $\Psi$, the following theorem characterizes the limiting bias of the ADMLE under sampling from any local perturbation of $P_0$ in the prespecified statistical model $\mathcal{M}$.  

 


\begin{theorem}[Limiting distribution under local perturbations]
    Suppose that the conditions of Theorem \ref{theorem::oracleEff} hold and that $\Psi$ is pathwise differentiable at $P_0$ relative to the prespecified statistical model $\mathcal{M}$ with efficient influence function $D_{\mathcal{M}, P_0} \in T_{\mathcal M} (P_0)$. Then, under sampling from any local perturbation $P_{0,hn^{-1/2}} \in  \mathcal{M}$ of $P_0$ with $h \in \mathbb{R}$ and score $s \in T_{\mathcal M} (P_0)$, the ADMLE $\widehat{\psi}_n$ satisfies that
    $$\sqrt{n}\,\{\widehat{\psi}_n - \Psi(P_{0,hn^{-1/2}})\}\xrightarrow[ ]{\;\;d\;\;}  N(b_0(h;s), \sigma_0^2)\ ,$$ 
    where $b_{0}(h;s) := h\langle s, D_{0,P_0} -  D_{\mathcal{M}, P_0}\rangle_{P_0} $ and $\sigma_0^2 := var_0\{D_{0,P_0}(O)\}$.  
    \label{theorem::irreg}
    
\end{theorem}

 



By Theorem \ref{theorem::oracleRegularity}, $b_{0}(h;s) = 0$ for each score $s \in T_{\mathcal{M}_0}(P_0)$, which correspond to local perturbations of $P_0$ that, in first order, remain in $\mathcal{M}_0$. To interpret $h$ as a local distance, we note that the scaled Hellinger distance between the local perturbation $P_{0,h n^{-1/2}}$ and $P_0$ satisfies $n^{-1/2} \|\sqrt{p_{0,h n^{-1/2}}} - \sqrt{p_0}\|_{\mu} = h \|s\|_{P_0} + o(1)$ as $n\rightarrow\infty$ with $p_0 := \frac{dP_0}{d\mu}$ denoting the $\mu$-density of $P_0$. By the Cauchy-Schwarz inequality, the asymptotic bias $b_{0}(h;s)$ of the ADMLE is maximized, subject to the constraint that $\|s\|_{P_0} \leq 1$, by any local perturbation $P_{0,hn^{-1/2}}$ with score $s$ at $P_0$ in the direction of the difference $D_{0,P_0} -  D_{\mathcal{M}, P_0}$. The maximal absolute bias corresponding to such least-favorable local perturbation is given by $ h\|D_{0,P_0} -  D_{\mathcal{M}, P_0}\|_{P_0}$. Interestingly, a $P_0$--efficient prespecified estimator for $\Psi$ constructed under a known model $\mathcal{M}' \subseteq \mathcal{M}_0$ contained in the oracle submodel generally exhibits worst-case asymptotic bias $h\|D_{\mathcal{M}', P_0} - D_{\mathcal{M}, P_0}\|_{P_0}$ not exceeding that of the ADMLE based on $\mathcal{M}_0$ using the negative loglikelihood loss function $\ell$. This suggests that by learning the working model $\mathcal{M}_n$ subject to the constraint $\mathcal{M}' \subseteq \mathcal{M}_n$, we can ensure that the ADMLE is, under sampling from any distribution in $\mathcal{M}_{np}$, asymptotically no more biased than a given prespecified estimator based on $\mathcal{M}'$.



 

It is interesting to contrast the worst-case asymptotic mean squared error of the ADMLE for a fixed $h$, as implied by Theorem \ref{theorem::irreg}, with the local asymptotic minimax bounds of \cite{hajek1972local} obtained as $h \rightarrow \infty$. When $S_{{\cal M}_0}(P_0) \subseteq T_{{\cal M}_0}(P_0)$, we have that $\|D_{0,P_0} -  D_{\mathcal{M}, P_0}\|^2$ equals the absolute efficiency gain $\Delta_0^2 := \sigma^2(\mathcal{M}) - \sigma_0^2$ for $\Psi$ from assuming $\mathcal{M}_0$ instead of $\mathcal{M}$, where $\sigma^2(\mathcal{M})$ denotes the efficiency bound $var_0\{D_{\mathcal{M},P_0}(O)\}$ at $P_0$ relative to $\mathcal{M}$. In this case, Theorem \ref{theorem::irreg} shows that the asymptotic mean squared error of the ADMLE under a least-favorable local perturbation in $\mathcal{M}$ with unit score is given by $h^2 \Delta_0^2 +  \sigma_0^2$. Importantly, this least-favorable mean squared error of the ADMLE is strictly better than the local asymptotic minimax bound over $\mathcal{M}$ when $|h| < 1$, and equals this bound when $h = 1$, as $\sigma_0^2(\mathcal{M}) = \Delta_0^2 +  \sigma_0^2$. Thus, for local perturbations near the oracle submodel $\mathcal{M}_0$, in the sense that $|h| \approx 1$, an ADMLE exhibits comparable or better mean squared error performance relative to a prespecified efficient estimator for $\mathcal{M}$, even despite potentially being more biased. However, while remaining locally minimax optimal over $\mathcal{M}_0$, an ADMLE is suboptimal for any strongly misspecified local perturbation in $\mathcal{M}$ for $h > 1$, with mean squared error tending to infinity as $ h \rightarrow \infty$. These findings build upon the research by \cite{lumley2017robustness} on model misspecification in estimating the ATE in nearly-true models. Furthermore, they are consistent with the experimental observations in \cite{ATEsupereff} and \cite{moosavi2023costs}, which found superefficient estimators to exhibit superior performance in terms of mean squared error, albeit at the potential cost of increased bias.


 
 
 

 
 
 
\subsection{Example: adaptive inference for the ATE}
\label{section::exampleATEPartially}


 
In this section, we return to the setup of Section \ref{section::AMLEGeneral2Example} and expand upon the results of Section \ref{section::DataAdaptexampleATEPartially} for the partially linear ADMLE of the ATE. Under high-level conditions on the model selection algorithm, the following theorem characterizes the asymptotic behavior of the partially linear ADMLE. To this end, we introduce the following condition, which constrains how quickly the working model $\mathcal{T}_n$ approximates certain elements of the oracle model $\mathcal{T}_0$

 

\begin{enumerate}[label=E\arabic*), ref={E\arabic*}, resume=condE]
     \item    \textit{Negligible critical bias term:} $\norm{\gamma_0 - \Pi_n \gamma_0}_{w_0P_0}\norm{\tau_0 - \Pi_n \tau_0}_{w_0P_0} = o_p(n^{-1/2})$
     \label{cond::CATE::critBias}
\end{enumerate}

 
   Under mild smoothness conditions on $\tau_0$ and $\gamma_0$, this condition is satisfied for a wide range of model selection algorithms, including the highly adaptive Lasso \citep{undersmoothedHAL,HALpointwise} and Lasso regression in reproducing kernel Hilbert spaces \citep{belloni2012sparse, chernoapproxSparse2019}.


 
\begin{theorem}[Limiting behavior of partially linear ADMLE of ATE]
     Suppose that the conditions of Theorem \ref{example::theorem::RlearnerLimitDist} and Condition \ref{cond::CATE::critBias} hold. Then, the partially linear ADMLE $\widehat{\psi}_n$ is $P_0$--asymptotically linear, regular, and efficient for the oracle parameter $\Psi_0: \mathcal{M}_{np} \rightarrow \mathbb{R}$, with \[\widehat{\psi}_n  - \Psi(P_0) = (P_n - P_0)D_{0,P_0} + o_p(n^{-1/2})\ .\] If, in addition, the conditional variance of $Y$ given $(A,W)$ is almost surely constant, then $\widehat \psi_n$ is $P_0$--efficient for $\Psi: \mathcal{M}_0 \rightarrow \mathbb{R}$ with respect to the oracle submodel $\mathcal{M}_0$.
    \label{example::theorem::RlearnerLimitDistORACLE}
\end{theorem}
\noindent Theorem \ref{example::theorem::RlearnerLimitDistORACLE} implies that $\sqrt{n}\,( \widehat{\psi}_n - \psi_0)/\sigma_0 \rightarrow_d N(0, 1)$, where $\sigma^2_0$ equals the efficiency bound $var_{0}\left\{D_{0,P_0}(O)\right\}$. Under general conditions, Theorem \ref{example::theorem::RlearnerLimitDist} implies that the ADMLE $\widehat{\psi}_n$ is $P_0$--superefficient for the ATE parameter $\Psi$ with limiting variance adaptive to the complexity of the CATE $\tau_0$.  


The following corollary establishes that the ADMLE is regular over each local perturbation of $P_0$ with corresponding CATE in the oracle submodel $\mathcal{T}_0$. It is important to note that when the learned oracle submodel $\mathcal{M}_0$ is only approximately correct for given sample size $n$, the ADMLE may suffer from asymptotic bias. Nevertheless, the following corollary demonstrates that even when sampling from a least-favorable local perturbation that lies outside the oracle submodel, the ADMLE still yields valid inference for an oracle projection-based ATE estimand.
 
 

\begin{corollary}[Limiting behavior under local perturbations]
   The ADMLE is $P_0$--regular for the ATE parameter $\Psi$ with respect to local perturbations of $P_0$ in the oracle submodel $\mathcal{M}_0$. Moreover, under sampling from any local perturbation $P_{0,hn^{-1/2}} \in \mathcal{M}_{np}$ not in the oracle submodel $\mathcal{M}_0$, it holds that $\sqrt{n}\,\{\widehat{\psi}_n-\Psi_0(P_{0,hn^{-1/2}})\}/\sigma_0\rightarrow_d N(0, 1)$. \label{cor::CATEinf}
\end{corollary}

   
  To further highlight the advantages of ADMLEs, we may consider the semiparametric estimator of the ATE based on the partially linear intercept model \citep{robinson1988root,imbensOverlapEstimand2006, li2019overlapWeights, d2021overlap} corresponding to $\mathcal{T} := \{w \mapsto c: c \in \mathbb{R}\}$. This estimator is known to exhibit irregular behavior and asymptotic bias under local perturbations that deviate from the semiparametric model. In contrast, when $\mathcal{T}_0$ contains the intercept CATE model, the partially linear ADMLE achieves regularity and asymptotic unbiasedness under a broader range of local perturbations. Moreover, in view of Corollary \ref{cor::CATEinf} and Theorem \ref{theorem::irreg}, the ADMLE is typically less biased than the semiparametric estimator when data are sampled from local perturbations outside the oracle submodel. It is interesting to note that if $\mathcal{T}_0$ corresponds to an intercept model, then the ADMLE and the semiparametric estimator are asymptotically equivalent under sampling from $P_0$ or any local perturbation of $P_0$ in $\mathcal{M}_{np}$.


\section{Numerical experiments}

 
 
  


\subsection{Data-generating distributions and nuisance estimation}

 We conducted a simulation study to evaluate the performance of the plug-in and partially linear ADMLEs defined in Examples \ref{example::regress::intro} and \ref{example::partially::intro} for estimating the ATE. Both ADMLEs employ the relaxed highly adaptive Lasso estimator (HAL) \citep{vanderlaanGenerlaTMLEFIRST,HAL2016, bibautHAL} for the outcome regression and CATE. The HAL estimator is based on the sectional variation norm penalty, which extends first-order total variation denoising to nonparametric settings \citep{geerLocalAdapt, fang2021multivariate, marswithLasso}, and performs variable selection and adapts to sparse functions using a tensor product basis of piecewise linear hinge functions of the form $x \mapsto (x-u)1(x \geq u)$ with knot point $u\in\mathbb{R}$ \citep{marswithLasso}. We implemented the HAL estimator using the R package \texttt{hal9001} \citep{hal2} and selected the sectional variation norm tuning parameter via cross-validation. The R package \texttt{causalHAL} provides code for implementing both ADMLEs. As non-adaptive benchmarks, we included  in our experiments a semiparametric ATE estimator based on a partially linear intercept model \citep{robinson1988root, imbensOverlapEstimand2006}, and the nonparametric efficient augmented inverse probability-weighted (AIPW) estimator \citep{robinsCausal,robins1995analysis}. 

For the simulation studies, we considered sample sizes $n \in \{500, 1000, 2000, 3000, 4000, 5000\}$ and independent covariates $W_1, W_2, W_3, W_4$ each drawn from the uniform distribution on $(-1,+1)$. Given $W=w:=(w_1,w_2,w_3,w_4)$, the treatment assignment $A$ was generated from a Bernoulli distribution with conditional mean $\pi_0(w)$ defined by $\text{logit}\{\pi_0(w)\}=\gamma \sum_{j=1}^4 \{w_j + \sin(4w_j)\}$, where $\gamma \in \{0.5, 1, 2\}$ controls the degree of treatment overlap. Given $(W,A)=(w,a)$, the outcome variable was generated from a normal distribution with mean $\mu_0(0,w)+a\tau_0(w)$ and variance $\sigma^2=0.5$, where $\mu_0(0,w)$ is the control conditional mean and $\tau_0(w) = 1 + w_1 + |w_2| + \cos(4w_3) + w_4$ is the CATE. We note that $\tau_0$ is approximately sparse under the HAL basis, implying potential superefficiency of the HAL-ADMLEs. Two choices of the control conditional mean were considered: the piecewise linear form  $\mu_0(0,w) =  w_1 + |w_2|  + w_3 + |w_4|$ and the nonlinear form $\mu_0(0,w) = \cos(4w_2) + \sum_{j=1}^4\sin(4w_j)$.  

To ensure comparability, we employed identical nuisance estimators for $\pi_0$, $\mu_0$ and $m_0$ across all four estimators. The outcome regression $\mu_0$ was estimated using the relaxed HAL least-squares estimator, with separate additive models and regularization parameters for $\mu_0(0,\cdot)$ and $\tau_0$. The number of prespecified basis functions included in the Lasso regression for $\mu_0$ were, respectively, $k=80, 400, 608, 608, 800,800$ for sample sizes $n=500, 1000, 2000, 3000, 4000, 5000$. To estimate the propensity score $\pi_0$, we used least-squares regression with 10-fold cross-validation employed to select among three candidate algorithms: generalized additive models implemented in R by the \texttt{mgcv} package \citep{hastie1987generalized, wood2001mgcv}, multivariate adaptive regression splines implemented by the \texttt{earth} package \citep{friedman1991multivariate, milborrow2019package}, and random forests implemented by the \texttt{ranger} package \citep{breiman2001random,wright2015ranger}. To ensure that the estimated propensity scores are bounded away from $0$ and $1$, we truncated estimates to fall within the range $(c_n,1-c_n)$, where $c_n$ is a data-adaptive cutoff selected by minimizing a loss function for the inverse propensity score \citep{chernozhukov2022automatic}. Finally, we estimated $m_0$ using the plug-in estimator $\pi_n \mu_n(1,\cdot) + (1-\pi_n) \mu_n(0,\cdot)$, where $\mu_n$ and $\pi_n$ are the estimators of $\mu_0$ and $\pi_0$ described above. %For more details, we refer to Appendix \sect



\subsection{Experimental findings}
\subsubsection{Demonstrating superefficiency: sampling under true distribution}

To quantify the level of overlap in each scenario, we report the overlap constant $c_0 := \inf_{w}\{\pi_0(w), 1- \pi_0(w)\}$, which depends on the choice of $\gamma$ in each simulation setting. The bias, variance, mean squared error, and confidence interval coverage for all estimators considered are estimated through Monte Carlo simulations and presented in Figure \ref{fig:simsEasyMain} and Appendix \ref{appendix::figures}. Figure \ref{fig:simsEasyMain} presents results for the scenario in which the control conditional mean exhibits a linear relationship with covariates for settings with both weak ($c_0 \approx 0.04$) and moderate overlap ($c_0 \approx 10^{-6}$). Results for the remaining scenarios are presented in Appendix \ref{appendix::figures} and are qualitatively similar. Overall, these experimental results provide strong evidence that ADMLEs based on the highly adaptive Lasso exhibit asymptotic normality and superefficiency, corroborating our theoretical results. In all settings considered, we observe that both ADMLEs significantly outperform the prespecified semiparametric estimator and AIPW estimator in terms of bias, variance, and confidence interval coverage. Remarkably, the prespecified semiparametric estimator based on incorrectly assuming a constant CATE is both more biased and more variable than the two ADMLEs considered.  

 % Figure environment removed

 % Figure environment removed
 
Based on our theory, in the case of a simple linear relationship that is sparse under the HAL basis, the plug-in ADMLE is expected to demonstrate greater efficiency than the partially linear ADMLE. In the nonlinear scenario, we anticipate comparable efficiency between the two estimators. Our experimental results align with these expectations, as we observe that the standard error of the plug-in ADMLE is generally smaller in the linear case with limited overlap. Moreover, in the nonlinear case, the two estimators appear to have the same large-sample variance. Although the plug-in ADMLE is generally more efficient than the partially linear ADMLE, it is important to note that it is typically irregular under a larger class of local alternatives. Additionally, the plug-in ADMLE lacks quasi double-robustness in the sense outlined in Condition \ref{cond::CATE::DRterm}, which limits its ability to take advantage of the smoothness properties of the propensity score.

\subsubsection{Demonstrating irregularity: sampling under a least-favorable local alternative}

In this experiment, we evaluate the performance of the estimators considered under a least-favorable local perturbation $P_{0,h n^{-1/2}}$ to $P_0$ for the ATE within a nonparametric statistical model. To achieve this, we introduce a local perturbation to the outcome regression components $\mu_0(0,\cdot)$ and $\tau$. Specifically, we define the control conditional mean corresponding to $P_{0,h n^{-1/2}}$ pointwise as $\mu_{n,0}(0,w) := \mu_0(0,w) - n^{-1/2}/\{1-\pi_0(w)\}$, and also define the CATE pointwise as $\Pi_n \tau_0 (w) := 1 + n^{-1/2}/[\pi_0(w)\{1-\pi_0(w)\}]$, where $n$ represents the sample size in a given simulation. The remaining components of the data-generating distribution remain unchanged. It is important to note that, apart from the local perturbation, the prespecified semiparametric estimator based on the intercept CATE model is correctly specified. Moreover, the oracle submodel $\mathcal{M}_0$ corresponding to the unperturbed distribution $P_0$ is given by the partially linear intercept model. Therefore, we expect from Corollary \ref{cor::CATEinf} that the partially linear ADMLE is asymptotically equivalent under sampling from $P_{0,h n^{-1/2}}$ to the prespecified semiparametric estimator. The experimental results under linearity with moderate and limited treatment overlap are displayed in Figure \ref{fig:simsEasyMain}, while results for other settings exhibit similar qualitative patterns and can be found in Appendix \ref{appendix::figures}. 


 % Figure environment removed

We find empirical support for the theoretical predictions implied by our results about the behavior of the estimators when sampling from a least-favorable local perturbation. Specifically, the prespecified semiparametric estimator and ADMLEs exhibit irregularity relative to the nonparametric model, leading to nonvanishing asymptotic bias. However, all estimators demonstrate $\sqrt{n}\,$--consistency as expected. However, consistent with Corollary \ref{cor::CATEinf}, the prespecified semiparametric estimator based on the intercept CATE model appears to be asymptotically equivalent to the partially linear ADMLE. This suggests that there is no asymptotic loss in learning the oracle submodel from data compared to knowing it in advance, even under the least-favorable local alternative. 

Notably, confidence intervals based on the AIPW estimator achieve 95\% coverage under the least-favorable local perturbation due to its regularity, but at the cost of significantly increased variance. Moreover, the AIPW estimator performs worse in terms of mean squared error in the moderate overlap setting, with only marginal improvement in confidence interval coverage. We note that for overlap constant $c_0 \approx 10^{-6}$ the AIPW estimator is biased and highly variable, likely due to the lack of identifiability of the ATE in the nonparametric model. In the linear case, we observe higher asymptotic bias in the plug-in ADMLE compared to the partially linear ADMLE, in line with expectations given that the former is regular under a smaller oracle submodel.


 


\section{Conclusion}

 


 
Adaptive debiased machine learning provides a general approach for constructing asymptotically linear and superefficient estimators of pathwise differentiable parameters using data-driven model selection techniques. In this work, we showed that ADMLEs are regular and provide locally uniformly valid inference for an oracle projection-based parameter that agrees with the target parameter for distributions contained in the oracle statistical submodel. Our findings establish that, in a local asymptotic sense over a nonparametric model, there is no disadvantage in performing data-driven model selection compared to having prior knowledge of the oracle submodel. In addition, we demonstrated how to construct ADMLEs that exhibit, in a local asymptotic sense, comparable or better performance compared to any predefined estimator relying on parametric or semiparametric model assumptions, while also providing robustness against model misspecification. While we focused on the \emph{iid} data setting, our results can be easily adapted to dependent data settings by applying suitable central limit theorems, e.g., as in \cite{van2014causal}.
 
While our results indicate that data-driven model selection does not impact the asymptotic bias or variance of the ADMLE, it has the potential to cause finite-sample variance inflation, which can lead to suboptimal confidence interval coverage. To address this issue, sample-splitting could be used to reduce the dependence between the estimator and the working model. This technique involves dividing the available data into two halves, computing the working model and estimator separately on each half. To fully restore efficiency, this process could then be repeated by exchanging the roles of the data halves and taking the final ADMLE to be the average of the split-specific ADMLEs. Our theory can be readily applied to establish the asymptotic linearity and efficiency of this split-averaged ADMLE for the oracle target parameter. This approach can be extended to multi-fold splits using cross-fitting techniques \citep{vanderLaanRose2011, DoubleML}. Alternatively, to address the additional finite sample variation introduced by data-driven model selection, bootstrap techniques \citep{efron1994introduction, bootstrapHAL, rinaldo2019bootstrapping} or subsampling techniques \citep{guo2023rank} could be considered for variance estimation. 

 
 


\vspace{0.3in}
\singlespacing

\section*{Acknowledgments}
Research reported in this publication was supported in part by grants DP2-LM013340 (AL), R01-HL137808 (MC) and R01-AI074345 (MvdL) from the National Institutes of Health.


 
 

\vspace{.3in}
 
 \bibliography{ref}

\newpage 



\appendix
\renewcommand{\thesection}{\Alph{section}}
\renewcommand{\thesubsection}{\Alph{section}.\roman{subsection}}

 \doublespacing
   
\section{Supplementary experimental results}

 
\label{appendix::figures}
\subsection*{Sampling from true distribution}


 % Figure environment removed


\subsection*{Sampling from least-favorable local alternative}


  % Figure environment removed

 
 

  
 
\section{Proofs of theoretical results}
 
 \begin{proof}[Proof of Theorem \ref{theorem::lossbasedEIF}]
 
Let $P \in \mathcal{M}_{np}$ be arbitrary. By  \ref{cond::lossSmooth} and \ref{cond::lossProjDeriv}, a minimizing solution $Q_P \in  \argmin_{Q \in \mathcal{M}_0} P \ell (\cdot, Q)$ satisfies
$$ \frac{d}{dt} P  \ell(\cdot, Q_t)   \big|_{t=0}  = P \left\{ \frac{d}{dt} \ell(\cdot, Q_t) \big|_{t=0} \right\} = 0,$$
for all regular paths $(Q_t: t \in (-\varepsilon, \varepsilon)) \subseteq \mathcal{M}_0$ with $Q_t = Q_P$ at $t=0$. 

For some $\delta > 0$, let $(P_t : t \in (-\delta, \delta)) \subseteq \mathcal{M}_{np}$ be a regular path through $P$ such that $dP_t = (1+ ts) dP$ for a bounded score $s \in L^2_0(P)$ orthogonal to the loss-based tangent space $\mathcal{S}_{\mathcal{M}_0}(P)$. Since ${\cal M}_{np}$ is a convex nonparametric model and the score $s$ is bounded, such a path necessarily exists for sufficiently small $\delta > 0$. By Condition \ref{cond::lossProjDeriv}, for all regular paths $(Q_u: u \in (-\varepsilon, \varepsilon)) \subseteq \mathcal{M}_0$ through $Q_P$ at $u = 0$, we have
\begin{align*}
P_t \left\{ \frac{d}{du} \ell(\cdot, Q_u) \big|_{u=0} \right\}& = \int \left\{ \frac{d}{du} \ell(o, Q_u) \big|_{u=0} \right\}\{ 1+ t s(o)\} P(do)\\
&= \int \left\{ \frac{d}{du} \ell(o, Q_u) \big|_{u=0} \right\} P(do)  + t \int \left\{ \frac{d}{du} \ell(o, Q_u) \big|_{u=0} \right\} s(o) P(do)\ .
\end{align*}
 The first term on the right-hand side is $0$ since $Q_P$ is a minimizer of $Q \mapsto P \ell(\cdot, Q)$ over $Q \in \mathcal{M}_0$. The second term on the right-hand side is also $0$ since $s$ is centered under $P$ and, by construction, orthogonal to $\frac{d}{du} \ell(\cdot, Q_u) \big|_{u=0}  \in \mathcal{S}_{\mathcal{M}_0}(P)$. It follows that $P_t \left\{ \frac{d}{du} \ell(\cdot, Q_u) \big|_{u=0} \right\} = 0$ for all such paths $(Q_u: u \in (-\varepsilon, \varepsilon)) \subseteq \mathcal{M}_0$ and $t$ sufficiently small. By Condition \ref{cond::lossProjDeriv}, this can only occur if $Q_P \in \argmin_{Q \in \mathcal{M}_0} P_t \ell (\cdot, Q)$ for all $t$ sufficiently small. 
 
 
 By Condition \ref{cond::oracleParamPathwise}, $\Psi_0 = \Psi \circ \Pi_0$ is pathwise differentiable at $P_0$; thus, its efficient influence function $D_{0,P_0}$ exists and is contained in $ L^2_0(P_0)$. For some sufficiently small $\delta > 0$, let $(P_t  : t \in (-\delta, \delta)) \subseteq \mathcal{M}_{np}$ be a regular path through $P_0$ such that $dP_t = (1+ ts) dP_0$ with score $s \in L^2_0(P_0)$ orthogonal to the loss-based tangent space $\mathcal{S}_{\mathcal{M}_0}(P_0)$. Then, by the above and \ref{cond::lossProjIdent}, our chosen path $(P_t: t \in (-\varepsilon, \varepsilon)$ is such that
 $\Psi_0(P_t) = \Psi(\Pi_0 P_t) = \Psi(Q_{P_0})$
 for all $t$ sufficiently small. Thus, upon differentiation, we find that
 \begin{align*}
   0 =  \frac{d}{dt} \Psi_0(P_t) \big |_{t=0} = \langle D_{0,P_0}, s \rangle_{L^2(P_0)}.
 \end{align*}
 Thus, $D_{0,P_0}$ is necessarily orthogonal to the score $s$ in $L^2_0(P_0)$. However, $s$ was an arbitrary bounded score taken to be orthogonal to the loss-based tangent space $\mathcal{S}_{\mathcal{M}_0}(P_0)$. Since $\mathcal{S}_{\mathcal{M}_0}(P)$ is a closed linear space and bounded scores are dense in $L^2_0(P_0)$, we must have that $D_{0,P_0} \in \mathcal{S}_{\mathcal{M}_0}(P)$. The result then follows.\end{proof}

 

\begin{proof}[Proof of Theorem \ref{theorem::limitDataAdapt}]
By \ref{cond::debiased}--\ref{cond::Donsker}, we have
\begin{align*}
    \widehat{\psi}_n - \Psi_n(P_0)\ &=\ (P_n-P_0 )D_{n, P_0} + o_p(n^{-1/2}) \\  
    & =\  (P_n-P_0 )D_{0,P_0} + (P_n-P_0 )(D_{n, P_0} - D_{0,P_0}) + o_p(n^{-1/2}) \\
     & =\  (P_n-P_0 )D_{0,P_0} + o_p(n^{-1/2})\ ,
\end{align*}
where the final equality uses,  by \ref{cond::Donsker}, that $(P_n-P_0 )(D_{n, P_0} - D_{0,P_0}) = o_p(n^{-1/2})$. The result now follows. We note that \ref{cond::consDn}, while not used in this proof, is typically required to establish \ref{cond::Donsker}.\end{proof}

\begin{proof}[Proof of Lemma \ref{lemma::stableLogLik}]
    Under \ref{cond::loglikEquivCond}\ref{cond::loglikloss} and by Theorem \ref{theorem::lossbasedEIF}, $D_{n,P_{n,0}} \in T_{\mathcal{M}_n}(P_{n,0})$ and $D_{0,P_{n,0}} \in T_{\mathcal{M}_0}(P_{n,0})$ are the $P_{n,0}$--efficient influence functions of $\Psi$ relative to the models $\mathcal{M}_n$ and $\mathcal{M}_0$, respectively. Under \ref{cond::loglikEquivCond}\ref{cond::nestedModel}, we have $T_{\mathcal{M}_n}(P_{n,0}) \subseteq T_{\mathcal{M}_0}(P_{n,0})$, with probability tending to one. On this event, we claim that $D_{n,P_{n,0}} = \bar{D}_{0,P_{n,0}} := \bar{\Pi}_n D_{0,P_{n,0}}$, where $\bar{\Pi}_n: T_{\mathcal{M}_0}(P_{n,0}) \rightarrow T_{\mathcal{M}_n}(P_{n,0})$ is the $L^2(P_{n,0})$--projection onto $T_{\mathcal{M}_0}(P_{n,0})$. 
    First, on this event, since $D_{0,P_{n,0}}$ is a gradient for $d\Psi(P_{n,0})$ relative to $T_{\mathcal{M}_0}(P_{n,0})$, we have  that $\bar{D}_{0,P_{n,0}}$ is a gradient for $d\Psi(P_{n,0})$ relative to $T_{\mathcal{M}_n}(P_{n,0})$. Since $\Psi = \Psi_n$ on $\mathcal{M}_n$, this implies, for all $s \in T_{\mathcal{M}_n}(P_{n,0})$, that 
    $d\Psi_n(P_{n,0})[s] = \langle s, \bar{D}_{0,P_{n,0}} \rangle_{P_{n,0}}$.
    However, we also know that $d\Psi_n(P_{n,0})[s] = \langle s, D_{n,P_{n,0}} \rangle_{P_{n,0}}$, and thus, for all $s \in T_{\mathcal{M}_n}(P_{n,0})$,
    $$\langle s, \bar{D}_{0,P_{n,0}} - D_{n,P_{n,0}}\rangle_{P_{n,0}} = 0\ .$$
    Since both $\bar{D}_{0,P_{n,0}}$ and $D_{n,P_{n,0}}$ are elements of $T_{\mathcal{M}_n}(P_{n,0})$, we must have that  $D_{n,P_{n,0}} = \bar{D}_{0,P_{n,0}}$ on this event. Finally, by the triangle inequality, \ref{cond::loglikEquivCond}\ref{cond::weakconstencyOracle} and \ref{cond::loglikEquivCond}\ref{cond::consistentTangentSpace}, we have that
    \begin{align*}
        \norm{D_{n,P_0} - D_{0,P_{0}}}_{P_0}\ &\leq\ \norm{D_{n, P_{n,0}} - D_{n, P_0}}_{P_0} + \norm{D_{0,P_{n,0}} - D_{0,P_0}}_{P_0}+ \norm{D_{0,P_{n,0}} - \bar{D}_{0,P_{n,0}}}_{P_0} + o_p(1)\\ &=\ o_p(1)\ ,
    \end{align*}
    where we used that $T_{\mathcal{M}_n}(P_{n,0}) \subseteq T_{\mathcal{M}_0}(P_{n,0})$, and so, $D_{n,P_{n,0}} = \bar{D}_{0,P_{n,0}}$ occurs with probability tending to one. The result then follows.
\end{proof}

 

\begin{proof}[Proof of Lemma \ref{theorem::exactBiasOracle}]
By Condition \ref{cond::oracleParamPathwise}, the data-dependent efficient influence function $D_{0,P_{n,0}} \in T_{\mathcal{M}_0}(P_{n,0})$ at $P_{n,0} = \Pi_n P_0$ exists. Using that $\Psi(P_{n,0})-\Psi(\Pi_0P_{n,0})=o_p(n^{-1/2})$, we have the exact expansion
\begin{align}
        \Psi_n(P_0) - \Psi_0(P_0)\ &=\
        \Psi(P_{n,0}) -\Psi(\Pi_0 P_{n,0}) +  \Psi(\Pi_0P_{n,0}) -\Psi_0(P_0)\nonumber\\
        &=\
       o_p(n^{-1/2}) +  \Psi_0(P_{n,0}) -\Psi_0(P_0)\nonumber\\
        &=\  -P_0  D_{0,P_{n,0}}  + R_{n,0} + o_p(n^{-1/2})  \nonumber\\
        &=\  (P_{n,0}- P_0)  D_{0,P_{n,0}}  + R_{n,0}     + o_p(n^{-1/2}) \ ,\label{proof::eqn::exactBiasOracleExpansion}
\end{align}
where $R_{n,0}  := \Psi_0(P_{n,0}) - \Psi_0(P_0) +  P_0 D_{0,P_{n,0}}$ is the exact second-order remainder. Now, by  \ref{cond::lossProjDeriv}, we have that the minimizer $P_{n,0} = \Pi_n P_0$ satisfies
\begin{equation}
     P_0  \left\{\frac{d}{dt} \ell(\cdot,Q_t) \big|_{t=0} \right\}= 0
     \label{proof::eqn::exactBiasOracle1}
\end{equation}
for all regular paths $(Q_t: t \in (-\delta, \delta)) \subseteq \mathcal{M}_n$ such that $Q_t = \Pi_n P_0$ at $t=0$. By definition, we have that $ \bar{D}_{0,P_{n,0}}$ is contained in the loss-based tangent space $\mathcal{S}_{\mathcal{M}_n}(P_{n,0}) \subseteq L^2_0(P_{n,0})$. Thus, there exists some regular path $(Q_t: t \in (-\delta, \delta)) \subseteq \mathcal{M}_n$ such that $Q_t = P_{n,0}$ at $t=0$ and $\frac{d}{dt} \ell(\cdot,Q_t) \big|_{t=0} =  \bar{D}_{0,P_{n,0}}$. Moreover, by Equation \eqref{proof::eqn::exactBiasOracle1}, we must have that
$$P_0 ( \bar{D}_{0,P_{n,0}} ) = P_0 \left\{ \frac{d}{dt} \ell(\cdot,Q_t) \big|_{t=0}\right\}  = 0\ .$$
Since $\bar{D}_{0,P_{n,0}} \in L^2_0(P_{n,0})$ is centered under $P_{n,0}$, we also have $(P_{n,0} - P_0) \bar{D}_{0,P_{n,0}} = 0$. Combining this with Equation \eqref{proof::eqn::exactBiasOracleExpansion}, we obtain the expansion
\begin{align*}
        \Psi_n(P_0) - \Psi_0(P_0)=  (P_{n,0}- P_0) ( D_{0,P_{n,0}} - \bar{D}_{0,P_{n,0}} ) + R_{n,0}  + o_p(n^{-1/2})\ ,     
\end{align*}
as desired.\end{proof}

 


\begin{proof}[Proof of Theorem  \ref{theorem::oracleEff}]

Under \ref{cond::oracleProjInModelFinal}, the proof of Lemma \ref{theorem::exactBiasOracle} establishes the bound
\begin{align*}
        \Psi_n(P_0) - \Psi_0(P_0)=  (P_{n,0}- P_0) ( D_{0,P_{n,0}} - \bar{D}_{0,P_{n,0}}) + R_{n,0}  + o_p(n^{-1/2})\ .   
\end{align*}
 Combining this with Theorem \ref{theorem::limitDataAdapt}, we obtain the expansion
\begin{align*}
    \widehat{\psi}_n - \Psi_0(P_0)\ & =\ (P_n-P_0 )D_{0,P_0}  + o_p(n^{-1/2})\\
    & \qquad + (P_{n,0} - P_0)( D_{0,P_{n,0}} - \bar{D}_{0,P_{n,0}}) + R_{n,0} + o_p(n^{-1/2})\ .
\end{align*}
By \ref{cond::doubleRemrootnFinal}, we have that $ (P_{n,0} - P_0)( D_{0,P_{n,0}} - \bar{D}_{0,P_{n,0}}) + R_{n,0} = o_p(n^{-1/2})$, and so, $  \widehat{\psi}_n - \Psi_0(P_0)  = (P_n-P_0 )D_{0,P_0}  + o_p(n^{-1/2})$ as desired. It follows that $\widehat{\psi}_n $ is asymptotically linear at $P_0$ for $\Psi_0:\mathcal{M}_{np} \rightarrow \mathbb{R}$ with influence function being the efficient influence function of $\Psi_0$ at $P_0$. Hence, $\widehat{\psi}_n $ is $P_0$--efficient for $\Psi_0$ relative to $\mathcal{M}_{np}$ \citep{bickel1993efficient}. Moreover, since  $\widehat{\psi}_n $ is efficient, it is necessarily regular for $\Psi_0$ relative to $\mathcal{M}_{np}$ \citep{vandervaart2000asymptotic}. The limiting distribution result follows immediately from the central limit theorem and Slutsky's lemma.\end{proof}
 
 


\begin{proof}[Proof of Theorem \ref{theorem::oracleRegularity}]
    By Theorem \ref{theorem::oracleEff} and that $\Psi_0(P_0) = \Psi(P_0)$, the ADMLE $\widehat{\psi}_n$ is asymptotically linear for $\Psi$ at $P_0$ with influence function being the efficient influence function of $\Psi_0$. Since $\Psi_0(P) = \Psi(P)$ for all $P \in \mathcal{M}_0$, regularity of the ADMLE for $\Psi_0$ over $\mathcal{M}_{np}$ implies that the ADMLE is regular for $\Psi$ over the oracle submodel $\mathcal{M}_0$. Under the loss-based tangent space conditions, we have by Theorem \ref{theorem::lossbasedEIF} that $D_{0,P_0}$ is the efficient influence function of $\Psi$ relative to $\mathcal{M}_0$. Thus, the ADMLE is asymptotically linear with influence function equal to the efficient influence function of $\Psi$ relative to $\mathcal{M}_0$. It follows that the ADMLE is asymptotically efficient \citep{vandervaart2000asymptotic}.\end{proof}

     
 \begin{proof}[Proof of Theorem \ref{theorem::irreg}]
 
      

     We assumed that $\Psi$ is pathwise differentiable at $P_0$ relative to $\mathcal{M}$ with efficient influence function $D_{\mathcal{M},P_0}$. Using that $\Psi_0(P_0) = \Psi(P_0)$, we first observe that 
     \begin{align*}
        \Psi_0(P_{0,hn^{-1/2}}) - \Psi(P_{0,hn^{-1/2}})\ &=\ \Psi_0(P_{0,hn^{-1/2}}) - \Psi_0(P_0)  + \Psi(P_0)- \Psi(P_{0,hn^{-1/2}})\\
         &=\ hn^{-1/2}\{ d\Psi_0(P_0)(S) - d\Psi(P_0)(S)\} + o(n^{-1/2})\\
          &=\ hn^{-1/2}\langle S, D_{0,P_0}- D_{\mathcal{M},P_0} \rangle_{P_0} + o(n^{-1/2})\ ,
     \end{align*}
where the final two equalities use pathwise differentiability of $\Psi:\mathcal{M} \rightarrow \mathbb{R}$ and $\Psi_0:\mathcal{M}_{np} \rightarrow \mathbb{R}$. Hence, we find that
\begin{align*}
    \sqrt{n}\left\{\widehat{\psi}_n - \Psi(P_{0,hn^{-1/2}}) \right\}\ &=\ \sqrt{n}\left\{\widehat{\psi}_n  - \Psi_0(P_{0,hn^{-1/2}})\right\} + \sqrt{n}\left\{ \Psi_0(P_{0,hn^{-1/2}}) - \Psi(P_{0,hn^{-1/2}}) \right\} \\
   & =\ \sqrt{n}\,(P_n - P_0)D_{0,P_0} +  h\langle S, D_{0,P_0}- D_{\mathcal{M},P_0} \rangle_{P_0} + o(1)\ .
\end{align*} 
The result then follows from Slutsky's theorem.\end{proof}
 


 


\section{Theory and proofs for plug-in ADMLE of ATE} 
\label{appendix::pluginAMLE}
 In this section, we provide the efficient influence function for the oracle ATE parameter provided by \eqref{eqn::ATEintroExampOracle}. Moreover, we analyze the estimation and oracle bias of the plug-in ADMLE for the ATE considered in Example \ref{example::regress::intro} and establish its efficiency for the oracle parameter.
 
Recall the setup of Section \ref{section::AMLEGeneral2Example}. Let $\alpha_0 := \argmin_{\alpha \in \Theta_0} E_0\left[\alpha(A,W)^2 - 2\{\alpha(1,W) - \alpha(0,W)\} \right]$ be the Riesz representer of the ATE functional with respect to $\Theta_0$.   Let $\Theta_n$ be a data-dependent working linear regression model of finite dimension and consider the plug-in ADMLE of the ATE
$$\widehat{\psi}_n := \frac{1}{n}\sum_{i=1}^n \left\{ \mu_n(1,W_i) - \mu_n(0,W_i)\right\}$$
with $\mu_n := \argmin_{\theta \in \Theta_n} \sum_{i=1}^n \{Y_i - \mu_n(A_,W_i)\}^2$.  


The following lemma is a direct corollary of the efficient influence function derivations provided in the proofs of Theorem 4.1 and 4.2 of \cite{chernoRegRiesz} --- see also \cite{chernozhukov2018auto} for additional details.
\begin{lemma}
    Let $\Theta \subseteq L^2(P_{A,W})$ be a closed linear space and  $\Pi_P \mu_P := \argmin_{\theta \in \Theta} \|\mu_P - \theta \|_{L^2(P)}$ denote the projection of $\mu_P$ onto $\Theta$. Suppose that $\theta \mapsto E_P\left\{\theta(1,W) - \theta(0,W)\right\}$ is a bounded linear functional over $\Theta$. Then, the projection parameter $\Psi_{\Theta}: P \mapsto E_P \left\{\Pi_P \mu_P(1,W) - \Pi_P \mu_P(0,W)  \right\}$ is pathwise differentiable at $P$ under a locally nonparametric statistical model with efficient influence function given almost everywhere by
    $$D_P: o\mapsto \alpha_P(a,w)\left\{y - \Pi_P \mu_P(a,w) \right\}+\Pi_P \mu_P(1,w) - \Pi_P \mu_P(0,w)  - \Psi(P)\ ,$$
    where $\alpha_P := \argmin_{\alpha \in \Theta} E_P\left[\alpha(A,W)^2 - 2\{\alpha(1,W) - \alpha(0,W)\} \right]$ is the Riesz representer.
     \label{example::lemma:Riesz}\end{lemma}

The following lemmas provide bounds for the estimation bias $\widehat{\psi}_n - \Psi_n(P_0)$ and oracle bias $\Psi_n(P_0) - \Psi_0(P_0)$.

\begin{lemma}[Oracle bias is second-order]\label{appendix::lemmaRieszOracle}
On the event $\{\Pi_n \mu_0 \in \Theta_0\}$, which necessarily occurs if $\Theta_n \subseteq \Theta_0$, we have that
     $$\Psi_n(P_0) - \Psi_0(P_0) = E_0 \left[\left\{\alpha_0(A,W) - \Pi_n \alpha_0(A,W)\right\}\{\Pi_n \mu_0(A,W) - \mu_0(A,W)\} \right].$$\end{lemma}
\begin{proof}
We have that
    $\Psi_n(P_0) = E_0\left\{\Pi_n \mu_0(1,W) - \Pi_n \mu_0(0,W)\right\}$, where $\Pi_n \mu_0$ denotes the projection $\argmin_{\theta \in \Theta_n} E_0\left\{\mu_0(A,W) - \theta(A,W) \right\}^2$.
    Let $\alpha_0 \in \Theta_0$ be the Riesz representer of the ATE functional with respect to $\Theta_0$, so that $E_0\left\{\mu(1,W) - \mu(0,W) \right\} = E_0 \left\{\alpha_0(A,W)\mu(A,W) \right\}$ for all $\mu \in \Theta_0$. Then, since $\Pi_n \mu_0, \mu_0 \in \Theta_0$, we have that
    $$\Psi_n(P_0) - \Psi_0(P_0) = E_0 \left[\alpha_0(A,W)\{\Pi_n \mu_0(A,W) - \mu_0(A,W)\} \right].$$
    Moreover, we have that $E_0 \left[\Pi_n \alpha_0(A,W)\{\Pi_n \mu_0(A,W) - \mu_0(A,W)\} \right] = 0$ since the orthogonal projection $\Pi_n \mu_0$ has the property that $\Pi_n \mu_0  - \mu_0 $ is orthogonal to $\Theta_n$ in $L^2(P_0)$. Hence, the previous display gives that
    $\Psi_n(P_0) - \Psi_0(P_0) = E_0 \left[\left\{\alpha_0(A,W) - \Pi_n \alpha_0(A,W)\right\}\{\Pi_n \mu_0(A,W) - \mu_0(A,W)\} \right]$.\end{proof}

     
\begin{lemma}\label{appendix::lemmaRieszDataAdapt}
Suppose $\Theta_n \subseteq \Theta_0$ with probability tending to one. Then, with  $D_{n,\widehat P_n}$ denoting the map $ o \mapsto \Pi_n \alpha_0(a,w)\left\{y - \mu_n(a,w)\right\}+ \mu_n(1, w) - \mu_n(0,w)$, we have that $\widehat{\psi}_n - \Psi_n(P_0)  =  (P_n - P_0)D_{n,\widehat P_n}$.

\end{lemma}
\begin{proof}
The first-order equations characterizing the empirical risk minimizer $\mu_n$ imply that
$$\frac{1}{n}\sum_{i=1}^n \Pi_n \alpha_0(A_i,W_i)\left\{Y_i - \mu_n(A_i,W_i)\right\} = 0\ .$$
Since $\Theta_n \subseteq \Theta_0$, we have that
\begin{align*}
    \widehat{\psi}_n   = \frac{1}{n}\sum_{i=1}^n \left[\mu_n(1,W_i) - \mu_n(0,W_i) +  \Pi_n \alpha_0(A_i,W_i)\left\{Y_i - \mu_n(A_i,W_i)\right\} \right]  = P_n D_{n,\widehat P_n}\ .
\end{align*}
Next, since  $\Theta_n \subseteq \Theta_0$, we also have that $\Pi_n \alpha_0$ is the Riesz representer of the ATE linear functional relative to $\Theta_n$ \citep{chernoRegRiesz}.  Hence, we have that $E_0\left\{\mu_n(1,W) - \mu_n(0,W) \right\} = E_0\left\{\Pi_n \alpha_0(A,W) \mu_n(A,W) \right\}$,  and consequently, that
\begin{align*}
    &E_0\left[\mu_n(1,W) - \mu_n(0,W) +  \Pi_n \alpha_0(A,W)\left\{Y - \mu_n(A,W)\right\} \right]\\
    &=\ E_0\left[\Pi_n \alpha_0 \mu_n +  \Pi_n \alpha_0(A,W)\left\{Y - \mu_n(A,W)\right\} \right]  \\
    &=\ E_0\left\{ \Pi_n \alpha_0(A,W) Y \right\}\ =\ E_0\left\{ \Pi_n \alpha_0(A,W) \mu_0(A,W) \right\} \ =\ \Psi_n(P_0)\ .
\end{align*}
Combining the previous two displays, it follows that $\widehat{\psi}_n - \Psi_n(P_0)  =  (P_n - P_0)D_{n,\widehat P_n}$.\end{proof}


Finally, we are ready to give our main theorem on the asymptotic behavior of the plug-in ADMLE of the ATE.

\begin{enumerate}[label=\bf{(S\arabic*)}, ref={S\arabic*}, series=condS]
  
    \item \textit{Donsker condition: } $\Pi_n \alpha_0$ and $\mu_n$ are uniformly bounded and fall in a fixed Donsker class with probability tending to one.
     \label{cond::CATE::suppDonskerMLE}
     \item  \textit{Nested working model:} $\liminf_{n \rightarrow \infty} P({\Theta}_n \subseteq {\Theta}_0) = 1$. \label{cond::suppCATE::nestedModel}
     \item \textit{Consistency:} $\norm{\Pi_n \alpha_0 - \alpha_0}_{P_0} = o_p(1)$ and $\max_{a \in \{0,1\}}\norm{\mu_n(a, \cdot) - \mu_0(a, \cdot)}_{P_0} = o_p(1)$. \label{cond::suppCATE::consistentNuis}
     \item  \textit{Negligible oracle bias:} $\norm{\Pi_n \alpha_0 - \alpha_0}_{P_0}\norm{\Pi_n \mu_0 - \mu_0}_{P_0} = o_p(n^{-1/2})$.\label{cond::suppCATE::oraclebias}
    
\end{enumerate}

\begin{theorem}
Under \ref{cond::CATE::suppDonskerMLE}--\ref{cond::suppCATE::oraclebias}, the plug-in ADMLE $\widehat{\psi}_n$ is regular, asymptotically linear, and asymptotically efficient for $\Psi_0$ with
    $\widehat{\psi}_n - \Psi_0(P_0) = (P_n - P_0)D_{0,P_{0}} + o_p(n^{-1/2})$, where $D_{0,P_0}$ is the efficient influence function of $\Psi_0: \mathcal{M}_{np} \rightarrow \mathbb{R}$.
\end{theorem}
\begin{proof}
Applying \ref{cond::suppCATE::nestedModel}, Lemma \ref{appendix::lemmaRieszOracle}, and Lemma \ref{appendix::lemmaRieszDataAdapt}, we find that
$$ \widehat{\psi}_n - \Psi_0(P_0)  =  (P_n - P_0)D_{n,\widehat P_n} + E_0 \left[\left\{\alpha_0(A,W) - \Pi_n \alpha_0(A,W)\right\}\{\Pi_n \mu_0(A,W) - \mu_0(A,W)\} \right].$$
By the Cauchy-Schwarz inequality and \ref{cond::suppCATE::oraclebias}, we have that 
\begin{align*}
&E_0 \left[\left\{\alpha_0(A,W) - \Pi_n \alpha_0(A,W)\right\}\{\Pi_n \mu_0(A,W) - \mu_0(A,W)\} \right]\\
&\leq\  \norm{\Pi_n \alpha_0 - \alpha_0}_{P_0}\norm{\Pi_n \mu_0 - \mu_0}_{P_0}\ =\ o_p(n^{-1/2})\ .\end{align*}
Additionally, since $\Pi_n \alpha_0$ and $\mu_n$ are bounded in view of \ref{cond::CATE::suppDonskerMLE}, we have that $$\|D_{n,\widehat P_n}  - D_{0,P_{0}}  \|_{P_0}\ \lessapprox\ \max_{a \in \{0,1\}}\|\mu_n(a, \cdot)  - \mu_0(a, \cdot)\|_{P_0} + \|\Pi_n \alpha_0 - \alpha_0 \|_{P_0}\ ,$$ which is $o_P(1)$ by \ref{cond::suppCATE::consistentNuis}. Finally, since $D_{n,\widehat P_n}$ is a Lipschitz transformation of $\mu_n$ and $\Pi_n \alpha_0$, we have by \ref{cond::CATE::suppDonskerMLE} that $D_{n, \widehat P_n}$ falls with probability tending to one in a Donsker function class \citep{vanderVaartWellner}. Therefore, by stochastic asymptotic equicontinuity of empirical processes on Donsker classes,  $\|D_{n,\widehat P_n}  - D_{0,P_{0}}  \|_{P_0} = o_P(1)$ implies that
$$ (P_n - P_0)D_{n,\widehat P_n} =  (P_n - P_0)D_{0,P_{0}} + o_p(n^{-1/2})\ .$$
Consequently, we have that $\widehat{\psi}_n - \Psi_0(P_0) = (P_n - P_0)D_{0,P_{0}} + o_p(n^{-1/2})$, as desired.\end{proof}


 

 


\section{Proofs for partially linear ADMLE of ATE}

\begin{proof}[Proof of Theorem  \ref{example::theorem::RlearnerEIF}]
    
    Let $\Theta_0 := \{(a,w) \mapsto m(w) + a \tau(w): m \in L^2(P_0), \tau \in \mathcal{T}_0\}$ be the corresponding oracle model for the outcome regression. By Lemma \ref{example::lemma:Riesz}, the efficient influence function $D_{0,P}$ for $\Psi_0$ at $P$ under the prespecified statistical model $\mathcal{M}$ is given by $o\mapsto \Pi_0 \tau_P(w) - E_0\{\Pi_0 \tau_P(W)\} + \alpha_P(w)\{a - \pi_P(w)\}\{y - \mu_P(w)\}$, where $\alpha_P := \argmin_{\alpha \in \Theta_0} E_P\left[\alpha(A,W)^2 - 2\{\alpha(1,W)-\alpha(0,W)\}\right]$ is the Riesz representer of the linear functional $\mu \mapsto E_P\{\mu(1,W) - \mu(0,W)\}$ \citep{chernoRegRiesz, chernozhukov2018auto}. We claim that $\alpha_P(A,W) = \gamma_P(W)\{A-\pi_P(W)\}$  $P$--almost surely. To see this, we observe that $\argmin_{\alpha \in \Theta_0} E_P\left[\alpha(A,W)^2 - 2\{\alpha(1,W)-\alpha(0,W)\}\right]$ coincides with 
    \begin{equation}\argmin_{m \in L_2(P_0),\gamma \in \mathcal{T}_0} E_P\left[\{m(W) + A \gamma(W) \}^2 - 2\gamma(W)\right].\label{eq:proofthm2}\end{equation} Next, we expand the objective function in \eqref{eq:proofthm2} as
    $$(m,\gamma)\mapsto E_P\left\{m(W)^2 - 2\pi_P(W)m(W)\gamma(W) + A^2 \gamma(W) - 2\gamma(W)  \right\},$$
    and observe that $m$ and $\gamma$ are able to vary freely in the minimization problem. Holding $\gamma$ fixed, we find that the above is minimized by $m_{P,\gamma}:w\mapsto -\pi_P(w)\gamma(w)$. With the choice $m=m_{P,\gamma}$, we obtain the profiled objective function 
    $\gamma\mapsto E_P\left[\{A-\pi_P(W)\}^2 \gamma(W)^2 - 2\gamma(W)\right]$, which is exactly minimized  over $\gamma \in \mathcal{T}_0$ by $\gamma_P$. Thus, plugging in these optimizers, we conclude that $\alpha_P(a,w) = \{a-\pi_P(w)\}\gamma_P(w)$. Plugging this expression for $\alpha_P$ in $D_{0, P}$, we obtain the efficient influence function given in the theorem.\end{proof}



\begin{proof}[Proof of Theorem \ref{example::theorem::RlearnerLimitDist}]

Due to Condition \ref{cond::CATE::nestedModel}, we can assume, without loss of generality, that the event $\mathcal{T}_n \subseteq \mathcal{T}_0$ occurs. All our results remain valid since any convergence results derived conditionally on this event must also hold unconditionally, as the event occurs with a probability approaching one. In such case, we have that
\begin{align*}
    \Pi_n \gamma_0\ :=&\ \ \argmin_{\gamma \in \mathcal{T}_n} E_0\left[\{A-\pi_0(W)\}^2\gamma(W)^2 -  2 \gamma(W) \right]\\
    =&\ \ \argmin_{\gamma \in \mathcal{T}_n} E_0\left( \{A-\pi_0(W)\}^2 \left[ \gamma(W) -  \{A-\pi_0(W)\}^{-2}\right]\right).
\end{align*}  
It follows that $\Pi_n \gamma_0$ is the overlap-weighted projection of $(a,w) \mapsto \{a-\pi_0( w)\}^{-2}$ onto $\mathcal{T}_n$. Since $\mathcal{T}_n \subseteq \mathcal{T}_0$, we also have that
\begin{align*}
    \Pi_n \gamma_0 := \argmin_{\gamma \in \mathcal{T}_n} E_0\left[ (A-\pi_0(W))^2 \left\{ \gamma(W) - \gamma_0(W) \right\}\right],
\end{align*} 
 where we note that $\Pi_n \gamma_0$ appears in the efficient influence function $D_{n,P_0}$ of $\Psi_n$ as indicated in Theorem \ref{example::theorem::RlearnerEIF}. 
 
 
 Let $o \mapsto \widehat D_{n,0}(o) := \tau_n(w) - P_n \tau_n + \Pi_n\gamma_{0}(w)\{a-\pi_n(w)\}\{y - \mu_n(a,w)\}$ be an estimator of the efficient influence function $D_{0,P_0}$ of $\Psi_0$ provided in Theorem \ref{example::theorem::RlearnerEIF}. %Let $\widehat P_n$ be any distribution compatible with the empirical distribution of $W$ and $(a,w) \mapsto \mu_n(a,w) := m_n(w) + (a-\pi_n(w))\tau_n(w)$. 
 By the first-order conditions characterizing the minimizer $\tau_n$,  we have that
\begin{align*}
     \widehat{\psi}_n \ &=\ \frac{1}{n}\sum_{i=1}^n \tau_n(X_i)\ =\  \frac{1}{n}\sum_{i=1}^n \tau_n(X_i)   + 
    \frac{1}{n}\sum_{i=1}^n \Pi_n \gamma_0(X_i)\left\{A-\pi_n(X_i)\right\} \left\{Y_i -\mu_n(A_i,X_i) \right\} \\
    & =\  \widehat{\psi}_n + P_n \widehat D_{n,0}\ .
\end{align*}
As a consequence, we have the bias expansion
\begin{align*}
    \widehat{\psi}_n - \Psi_n(P_0)\ &=\  \widehat{\psi}_n + P_n \widehat D_{n,0}  - \Psi_n(P_0)\\
    &=\ (P_n-P_0)D_{0,P_{0}}  + (P_n-P_0)(\widehat D_{n,0} - D_{0,P_{0}}) + R_{n,0}
\end{align*} 
with
$R_{n,0} :=  \widehat{\psi}_n - \Psi_n(P_0) + P_0 \widehat D_{n,0}$. 

We first show that $ (P_n-P_0)( \widehat D_{n,0}- D_{0,P_{0}} ) = o_p(n^{-1/2})$. By \ref{cond::CATE::DonskerMLE} and preservation of the Donsker property under Lipschitz transformations  \citep{vanderVaartWellner}, $\widehat D_{n,0}- D_{0,P_{0}}$ falls in a Donsker class with probability tending to one. Hence, it suffices to show that $\|  \widehat D_{n,0}- D_{0,P_{0}} \|_{P_0} = o_p(1)$. To this end, we define $\alpha_n$ and $\alpha_0$ pointwise as $\alpha_n(a,w):= \Pi_n\gamma_0(w) \{a- \pi_n(w)\}$ and $\alpha_0(a,w):= \gamma_0(w) \{a- \pi_0(w)\}$. Then, we can write $\widehat D_{n,0}(o) = \tau_n(w) - P_n \tau_n + \alpha_n(a,w)\{y - \mu_n(a,w)\}$ and $D_{0,P_0}(o) = \tau_0(w) - P_0 \tau_0 + \alpha_0(a,w)\{y - \mu_0(a,w)\}$. To show that $\|\widehat D_{n,0} - D_{0,P_0}\|_{P_0} = o_p(1)$, we show that $\|\tau_n - \tau_0 - P_n \tau_n + P_0 \tau_0 \|_{P_0} = o_P(1)$ and $\|\alpha_n(\mathcal{I}_Y - \mu_n) -\alpha_0(\mathcal{I}_Y - \mu_0) \|_{P_0} = o_p(1)$ with $\mathcal{I}_Y: o \mapsto y$. By \ref{cond::CATE::consistentNuis}, \ref{cond::CATE::consistentWorking}, and the triangle inequality, we have that $\|\tau_n -   \tau_0 \|_{P_0} \leq \|\tau_n -   \Pi_n \tau_0 \|_{P_0} + \|\Pi_n \tau_0 -   \tau_0 \|_{P_0} = o_p(1)$. Moreover, $P_n \tau_n - P_0 \tau_0 = (P_n - P_0) \tau_n + P_0(\tau_n - \tau_0) = o_p(1)$ since $P_0(\tau_n - \tau_0) \leq \|\tau_n - \tau_0 \|_{P_0}  = o_p(1)$ and $(P_n - P_0) \tau_n = \mathcal{O}_p(n^{-1/2})$ given that $\tau_n$ falls in a Donsker class by \ref{cond::CATE::DonskerMLE}. Hence, $\|\tau_n - \tau_0 - P_n \tau_n + P_0 \tau_0 \|_{P_0} = o_P(1)$ by the triangle inequality. Next, we note that 
\begin{align*}
    \alpha_n(\mathcal{I}_Y - \mu_n) -\alpha_0(\mathcal{I}_Y - \mu_0)   = \alpha_n(\mu_0 - \mu_n)  + (\alpha_n - \alpha_0)(\mathcal{I}_Y - \mu_0)\ .
\end{align*} 
By \ref{cond::CATE::DonskerMLE}, $\mathcal{I}_Y - \mu_0$ and $\alpha_n$ are uniformly bounded so that, by the triangle inequality, the norm of the right-hand side is upper bounded by $\|\alpha_n - \alpha_0 \|_{P_0} + \|\mu_n - \mu_0\|_{P_0}$ up to a constant. We first show that $\|\alpha_n - \alpha_0 \|_{P_0} = o_p(1)$. We note that
\begin{align*}
    \alpha_n - \alpha_0\ =\ \Pi_n \gamma_0(\mathcal{I}_A - \pi_n) - \gamma_0(\mathcal{I}_A - \pi_0)\  =\  \Pi_n \gamma_0(\pi_0 - \pi_n) - (\Pi_n \gamma_n - \gamma_0)(\mathcal{I}_A- \pi_0)
\end{align*} 
with $\mathcal{I}_A: o \mapsto a$, and that $\|(\Pi_n \gamma_n - \gamma_0)(\mathcal{I}_A- \pi_0)\|_{P_0} = \|\Pi_n \gamma_n - \gamma_0\|_{w_0 P_0} = o_p(1)$ by \ref{cond::CATE::consistentNuis}. Moreover, since $\Pi_n \gamma_0$ is bounded with probability tending to one by \ref{cond::CATE::DonskerMLE}, we have that $\| \Pi_n \gamma_0(\pi_0 - \pi_n)  \|_{P_0} =\mathcal{O}_p\left( \|\pi_0 - \pi_n  \|_{P_0}\right) = o_p(1)$ by \ref{cond::CATE::consistentNuis}. We now show that $\|\mu_n - \mu_0\|_{P_0} = o_p(1)$. We note that, by the triangle inequality, \begin{align*}
    \|\mu_n - \mu_0\|_{P_0}\ &\leq\ \|m_n - m_0\|_{P_0} + \| (\mathcal{I}_A-\pi_n)\tau_n +  (\mathcal{I}_A - \pi_0)\tau_0 \|_{P_0}\\
    &\leq\ \|m_n - m_0\|_{P_0} +\| (\mathcal{I}_A-\pi_n)(\tau_n-\tau_0) +  (\pi_n - \pi_0)\tau_0 \|_{P_0}\\
    &=\ \mathcal{O}_p\left(\|m_n - m_0\|_{P_0} +\|\tau_n - \tau_0\|_{P_0} + \| \pi_n - \pi_0\|_{P_0}\right)\ ,
\end{align*} where we use that $\mathcal{I}_A$, $\pi_n$ and $\tau_0$ are bounded with probability tending to one by \ref{cond::CATE::DonskerMLE}.  Hence, by \ref{cond::CATE::consistentNuis}, we have that $\|\mu_n - \mu_0\|_{P_0}= o_p(1)$, and thus,  $\|\widehat D_{n,0} - D_{0,P_0}\|_{P_0} = o_p(1)$, as desired. 

It remains to show that $  R_{n,0} = o_p(n^{-1/2})$. First, we observe that
\begin{align*}
   R_{n,0}\ & =\  \widehat{\psi}_n - \Psi_n(P_{0}) + P_0 \widehat D_{n,0} \\
    &=\  E_0\left[\Pi_n \gamma_0(W)\{A-\pi_n(W)\}\{Y - \mu_n(A,W) \} \right]   + E_0\{\tau_n(W) - \Pi_n \tau_0(W)\}\\
     &=\  E_0\left[\Pi_n \gamma_0(W)\{A-\pi_n(W)\}\{\mu_0(A,W) - \mu_n(A,W) \} \right]   + E_0\{\tau_n(W) - \Pi_n \tau_0(W)\}\ ,\end{align*}
where we used the law of iterated expectations. Next, substituting $\mu_n := m_n + (\mathcal{I}_A - \pi_n)\tau_n$ and $\mu_0 := m_0 + (\mathcal{I}_A - \pi_0)\tau_0$, we find that $R_{n,0}=\text{(I)}+\text{(II)}+\text{(III)}$ with \begin{align*}
    \text{(I)}\ &:=\ E_0\left[\Pi_n \gamma_0(W)\{A-\pi_n(W)\}\{m_0(W) - m_n(W)\}   \right]\\ 
    \text{(II)}\ &:=\ E_0\left(\Pi_n \gamma_0(W)\{A-\pi_n(W)\}\left[ \{A-\pi_0(W)\}\tau_0(W) - \{A - \pi_n(W)\}\tau_n(W) \right] \right)\\ 
    \text{(III)}\ &:=\ E_0\{\tau_n(W) - \Pi_n \tau_0(W)\}\ .
\end{align*}
First, we note that $\text{(I)}=E_0\left[\Pi_n \gamma_0(W)\{\pi_0(W)-\pi_n(W)\}\{m_0(W) - m_n(W)\}\right]$ by the law of iterated expectations,  and so,
since $\Pi_n \gamma_0$ is bounded by \ref{cond::CATE::DRterm}, the Cauchy-Schwarz inequality implies that $\text{(I)}$ is of order $\mathcal{O}_p \left(\|\pi_n - \pi_0\|_{P_0}\|m_n - m_0\|_{P_0}\right)$. Next, we can write $\text{(II)}=\text{(IIa)}+\text{(IIb)}$ with \begin{align*}
    \text{(IIa)}\ &:=\ E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}[  \{A-\pi_0(W)\}  \tau_0(W) - \{A - \pi_n(W)\}\tau_n(W) ] \right]\\
    \text{(IIb)}\ &:=\ E_0\left[\Pi_n \gamma_0(W)\{\pi_0(W)-\pi_n(W)\}[  \{A-\pi_0(W)\}  \tau_0(W) - \{A - \pi_n(W)\}\tau_n(W) ] \right].
\end{align*}On one hand, we can write
 \begin{align*} 
 \text{(IIa)}\ &=\  E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2  \tau_0(W)\right]  - E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}\{A - \pi_n(W)\}\tau_n(W) ] \right]  \\
 &=\  E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2  \Pi_n \tau_0(W)\right]  - E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}\{A - \pi_n(W)\}\tau_n(W) ] \right]\\
 &=\ E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2 \{\Pi_n\tau_0(W)  - \tau_n(W)\}\right]\\
 &\hspace{.5in}- E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}\{\pi_0(W) - \pi_n(W)\}\Pi_n\tau_0(W)\right]\\
 &=\ E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2 \{\Pi_n\tau_0(W)  - \tau_n(W)\}\right]  ,
 \end{align*}where, in particular, we have used the definition of the overlap-weighted projection $\Pi_n$ to replace $\tau_0$ by $\Pi_n \tau_0$,
On the other hand, using that $E_0\left\{A - \pi_0(W)\,|\, W\right\} = 0$ almost surely, we can write
 \begin{align*}
 \text{(IIb)}\ &=\  E_0\left[\Pi_n \gamma_0(W)\{\pi_0(W)-\pi_n(W)\}[  \{A-\pi_0(W)\}  \tau_0(W) - \{A - \pi_n(W)\}\tau_n(W) ] \right]\\
 &=\ -E_0\left[\Pi_n \gamma_0(W)\{\pi_0(W)-\pi_n(W)\}^2\tau_n(W) \right].
 \end{align*}
Hence, by the Cauchy-Schwarz inequality, we find that $\text{(II)}$ can be written as
 \begin{align*}
   &E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2 \{\Pi_n\tau_0(W)  - \tau_n(W)\}\right]-E_0\left[\Pi_n \gamma_0(W)\{\pi_0(W)-\pi_n(W)\}^2\tau_n(W) \right]\\
    &=\ E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2 \{\Pi_n\tau_0(W)  - \tau_n(W)\}\right]+\mathcal{O}_p\left(\| \pi_n-  \pi_0 \|_{P_0}^2\right).
 \end{align*}
Combining \text{(III)} with our bounds for \text{(I)} and \text{(II)}, we finally find that 
\begin{align*}
     R_{n,0}\ &=\ E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2 \{\Pi_n\tau_0(W)  - \tau_n(W)\}\right] + E_0\{\tau_n(W) - \Pi_n \tau_0(W)\}\\
     &\qquad +\mathcal{O}_p\left(\norm{\pi_n-\pi_0}_{P_0}\norm{m_n- m_0}_{P_0}\right) + \mathcal{O}_p\left(\| \pi_n-  \pi_0 \|_{P_0}^2 \right).
\end{align*}
Since $\tau_n -\Pi_n \tau_0 \in \mathcal{T}_n$ by \ref{cond::CATE::nestedModel}, and in view of the proof of Theorem \ref{example::theorem::RlearnerEIF}, we have that
\begin{align*}
    E_0\left\{\tau_n(W) - \Pi_n \tau_0(W)\right\}\ &=\ E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2\{\tau_n(W) - \Pi_n \tau_0(W) \}\right]\\
    &=\  -E_0\left[\Pi_n \gamma_0(W)\{A-\pi_0(W)\}^2\{\Pi_n \tau_0(W) - \tau_n(W) \}\right].
\end{align*}
Thus, by Condition \ref{cond::CATE::DRterm}, we conclude that $R_{n,0} = \mathcal{O}_p\left(\norm{\pi_0-\pi_n}_{P_0}\norm{m_n- m_0}_{P_0}+\norm{\pi_0-\pi_n}_{P_0}^2\right)$ is of order $o_p(n^{-1/2})$.\end{proof}


 \begin{proof}[Proof of Theorem   \ref{example::theorem::RlearnerLimitDistORACLE}]
     
Under the stated conditions, Theorem \ref{example::theorem::RlearnerLimitDist} implies that the ADMLE is asymptotically linear for $\Psi_n(P_0)$ with $\widehat{\psi}_n - \Psi_n(P_0) = (P_n - P_0)D_{0,P_0} + o_p(n^{-1/2})$, where $D_{0,P_0}$ is the efficient influence function of the oracle parameter $\Psi_0$. We will verify  that $\Psi_n(P_0) - \Psi_0(P_0)$ is $o_p(n^{-1/2})$ under the stated conditions. The result then follows from the proof of Theorem \ref{theorem::oracleEff}. 

 
 Let $(a,w) \mapsto \mu_{n,0}(a,w) := m_0(w) + \{a - \pi_0(w)\}\Pi_n \tau_0(w)$ be an oracle approximation of $\mu_0$ compatible with $\Pi_n \tau_0$. Since $\Pi_n \tau_0 \in \mathcal{T}_0$ by \ref{cond::CATE::nestedModel}, we have that $\mu_{n,0} \in \Theta_0$. In view of \ref{cond::CATE::boundedFunc}, the parameter $\Psi_0$ can be viewed as a bounded linear functional of $\mu_0 \in \Theta_0$. Therefore, by the Riesz representation theorem, we have that $\Psi_n(P_0)=E_0\left\{\Pi_n \tau_0(W)\right\}=E_0\left\{\mu_{n,0}(1,W) - \mu_{n,0}(0,W)\right\}=E_0\left\{\alpha_0(W) \mu_{n,0}(A,W)\right\}$,
where $\alpha_0 \in \Theta_0$ is a Riesz representer. In view of the proof of Theorem \ref{example::theorem::RlearnerEIF}, we have that $\alpha_0(A,W) = \gamma_0(W)\{A - \pi_0(W)\}$ almost surely, so that we can write
\begin{align*}
    \Psi_n(P_0)\ &=\ E_0\left[\gamma_0(W)\{A - \pi_0(W)\}\mu_{n,0}(A,W)\right] \\
    &=\  E_0\left[\gamma_0(W)\{A - \pi_0(W)\}[m_0(W) + \{A - \pi_0(W)\}\Pi_n \tau_0(W)]\right]\\
    & =\  E_0\left[\gamma_0(W)\{A - \pi_0(W)\}^2 \Pi_n \tau_0(W)\right].
\end{align*}
Similarly, since $\tau_0 \in \mathcal{T}_0$, we have that $ \Psi_0(P_0) =E_0\left[\gamma_0(W)\{A - \pi_0(W)\}^2 \tau_{0}(W)\right] $. Therefore, we can write 
$\Psi_n(P_0) - \Psi_0(P_0)  =   E_0\left[\gamma_0(W)\{A - \pi_0(W)\}^2 \{\Pi_n \tau_0(W) - \tau_0(W)\}\right]$.
Since $\Pi_n \tau_0$ is the $\pi_0(1-\pi_0)$--weighted projection of $\tau_0$ onto $\mathcal{T}_n$, the orthogonality condition \[E_0\left[\gamma(W)\{A - \pi_0(W)\}^2 \{\Pi_n \tau_0(W) - \tau_0(W)\}\right] = 0\] holds for each $\gamma \in \mathcal{T}_n$. In particular, for the choice $\gamma = \Pi_n \gamma_0 $, we find that
\begin{align*}
    \Psi_n(P_0) - \Psi_0(P_0)\ &=\ E_0\left[\{\gamma_0(W) - \Pi_n \gamma_0 (W)\}\{A - \pi_0(W)\}^2 \{\Pi_n \tau_0(W) - \tau_0(W)\}\right]\\
    &=\ \mathcal{O}_p\left(\norm{\Pi_n \gamma_0  - \gamma_0}_{w_0P_0}\norm{\Pi_n \tau_0 - \tau_0}_{w_0P_0}\right)
\end{align*}
by the Cauchy-Schwarz inequality. Thus, as desired, $\Psi_n(P_0)-\Psi_0(P_0)$ is of order $o_p(n^{-1/2})$ by \ref{cond::CATE::critBias}.

We have shown that $  \widehat{\psi}_n - \Psi_0(P_0) = (P_n - P_0) D_{0,P_0} + o_p(n^{-1/2}) $, so that $\widehat{\psi}_n$ is an asymptotically linear estimator with influence function $D_{0,P_0}$ equal to the efficient influence function of $\Psi_0$ under $\mathcal{M}_{np}$. Thus, $\widehat{\psi}_n$ is efficient for $\Psi_0(P)$. Moreover, since efficient estimators are necessarily regular \citep{vanderVaartWellner}, we also have that $\widehat{\psi}_n$ is regular for $\Psi_0$. Finally, if the conditional variance of the outcome is almost surely constant, then $D_{0,P_0}$ is also the efficient influence function of $\Psi$ relative to the oracle model $\mathcal{M}_0$ \citep{chernoRegRiesz}. The result then follows.\end{proof}
  


 

\begin{proof}[Proof of Corollary \ref{cor::CATEinf}]
    This is a consequence of Theorem \ref{theorem::oracleRegularity} and regularity of the ADMLE for $\Psi_0$.\end{proof}

 


 

 

\end{document}