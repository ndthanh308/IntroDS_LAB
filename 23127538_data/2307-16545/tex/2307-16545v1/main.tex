\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{afterpage}
% \usepackage{wraptable}
% \usepackage{amsmath,amssymb,amsfonts}
\floatname{algorithm}{Algorithm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Init:}}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

% \def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Towards General Visual-Linguistic Face Forgery Detection}
\author{
   \large Ke Sun $^{1}$, Shen Chen $^{2}$, Taiping Yao $^{2}$, Xiaoshuai Sun $^{1}$, Shouhong Ding$^{2}$, Rongrong Ji$^{1}$\thanks{Corresponding author} \\
   \normalsize $^1$ Key Laboratory of Multimedia Trusted Perception and Efficient Computing, \\
   \normalsize Ministry of Education of China, Xiamen University, China \\
   \normalsize $^2$ Youtu Lab, Tencent, China
}
\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
   Deepfakes are realistic face manipulations that can pose serious threats to security, privacy, and trust.
Existing methods mostly treat this task as binary classification, which uses digital labels or mask signals to train the detection model. We argue that such supervisions lack semantic information and interpretability.
To address this issues, in this paper, we propose a novel paradigm named
Visual-Linguistic Face Forgery Detection(VLFFD), which uses fine-grained sentence-level prompts as the annotation.
Since text annotations are not available in current deepfakes datasets, VLFFD first generates the mixed forgery image with corresponding
fine-grained prompts via Prompt Forgery Image Generator
(PFIG). Then, the fine-grained mixed data and coarse-grained original data and is jointly trained with the Coarse-and-Fine Co-training framework (C2F), enabling the model to gain more generalization and interpretability. The experiments show the proposed method improves the existing detection models on several challenging benchmarks.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Recently, face forgery methods have achieved significant success with the growth of computer vision techniques. Such methods can manipulate facial attributes~\cite{gonzalez2018facial}, expressions~\cite{liu2019stgan}, and even identity~\cite{korshunov2018deepfakes} with high quality, which can be easily abused by malicious users and further cause severe trust issues or societal problems~\cite{tolosana2020deepfakes}. Thus, to address these issues, it is significant to develop the face forgery detection model to identify authenticity. 
% Figure environment removed


The early methods leverage the convolutional neural network (CNN) as the backbone to distinguish the authenticity, which treats the face forgery detection as a binary classification problem~\cite{afchar2018mesonet,jain2018detecting,dolhansky2020deepfake}. 
However, the forgery faces in real-world scenarios often have a considerable domain gap compared to the training distribution~\cite{sun2021dual}, resulting in a significant drop in the performance of the detector.
Therefore, more and more works are devoted to \textit{general face forgery detection}~\cite{sun2021domain}, which aims to improve the discrimination under unseen forgery attacks.
Major solutions can be divided into two categories: \textit{forgery simulation} and \textit{framework engineering}. The former manually produces forgery samples to enhance the generalization forgery features, such as boundary~\cite{li2020face,shiohara2022detecting} and self-inconsistency~\cite{zhao2021learning}. The latter is devoted to specially designed frameworks that boost the generalization of backbones~\cite{zhao2021multi,sun2021dual}.
% For example, GFF~\cite{sun2021dual} use high-frequency stream to relieve the textures bias, and DCL~\cite{sun2021dual} leverage supervised contrastive learning framework to preserve the variance among forgery instances. 
However, the aforementioned methods are basically trained with binary labels $\{0, 1\}$ under a unimodal setting. Such a coarse-grained binary classification paradigm lacks fine-grained semantic information, which contains the essential forgery clues, leading to sub-optimal performance when transferred to unseen forgery types.
Furthermore, only outputting a single confidence of real or fake lacks interpretability, especially under some strict situations such as forensics. 

Driven by the above challenges, we can not help raising the following question: \textit{Can we find another supervisory signal instead of number labels that can provide more comprehensive and understandable information?}
We believe that natural language will be the perfect answer. The recent multi-modality visual language pretrain models, as represented by the CLIP~\cite{radford2021learning}, are trained based on a large amount of multimodal paired data on the Web, and have the ability to align semantic information hidden in natural language with the visual signal with the impressive accuracy in wild scenarios.
Compared with the digital label, the language signal, which now can be well understood by pretrained models, contains more fine-grained semantic information which can provide more accurate guidance to extract generalization features. 

To this end, in this paper, we attempt to introduce natural language as fine-grained supervision into the face forgery detection task and design a paradigm to utilize a pre-trained multimodal model to improve the generalization and interpretability. We call this new paradigm Visual-Linguistic Face Forgery Detection(VLFFD), the difference between the traditional method and ours is shown in Fig.~\ref{fig:intro}. Specifically, since VLFFD requires sentence-level annotations which is not available in the original forgery dataset, we first generate mixed forgery images with fine-grained annotations by disentangling and taxonomizing
the dataset of real and fake pairs via quantitative criterion, named Prompt Forgery Image Generator (PFIG).
Such annotations can further form the sentence-level prompts, which can provide fine-grained semantic annotations for the mixed forgery image. 
Subsequently, we design a new contrastive based multimodal mechanism to jointly train the coarse-grained original data and fine-grained mixed forgery data in a visual-linguistic manner, named Coarse-and-Fine Co-training framework (C2F). 
Compared with the traditional framework, Our VLFFD has three main advantages 1) the fine-grained prompt can provide more precise guidance to encourage the image encoder to extract generalized forgery clues. 2) Semantic information hidden in the pretrained model can be exploited in the general face forgery detection task.
3)  traditional methods can only output the confidence of real or fake, while our framework can indicate the sentence about the forgery regions and forgery types, enabling us to understand the basis of the discrimination and have more interpretability.

Our main contributions can be summarized as follows:
\begin{itemize}
    \item We are the first to introduce the language modality into general face forgery detection task as a supplementary signal instead of binary numbers label.
    \item We propose a novel multimodal learning paradigm named VLFFD to improve the generalization and interpretability in a visual-language manner.
    % \item We propose a pipeline to generate the mixed forgery image with fine-grained annotation within the existing dataset and provide a quantitative criterion for classifying types of forgery.
    \item Extensive experiments and visualizations demonstrate the effectiveness of our method against state-of-the-art competitors.
    
\end{itemize}

% Since traditional face forgery dataset only contains category-level labels (real or fake) while lacks sentence-level annotations. Collecting manual annotation not only
% requires huge labor-intensive but also faces the problem of mislabeling, especially when the forgery face is too lifelike to distinguish for human eyes. Thus, in this work, we first propose a pipeline to produce forgery faces with fine-grained annotation using an existing paired face forgery dataset--as represented by the Faceforensics++~\cite{rossler2019faceforensics++}, including four steps: 
% 1) generate a forgery mask by subtracting the real and corresponding forgery face. 2) randomly determine the forged region through the key points and forgery mask. 3) decide the forgery types for such regions via specially designed evaluation metrics. 4) blend the forgery region into the corresponding real faces and save the forgery region and types as fine-grained labels. 
% Then, we design the prompt for both the original dataset and the
% generated forgery samples through such fine-grained labels in terms of forgery regions and forgery types. Finally,
% we design a multimodal framework follows the coarse-and-fine co-trianing mechanism, 
% which combine the original dataset with coarse-grained prompts and generated forgery dataset with fine-grained prompts. 


\section{Related Work}
\label{sec:related}
\subsection{General Face Forgery Detection}
Since the performance of the traditional face forgery detection method drops significantly when tested on unseen attacks, some works are devoted to relieving the generalizing problem from architecture engineering or forgery simulation. The former manually produces forgery samples to enhance the generalization forgery features that are easily overlooked by networks, such as SLADD\cite{chen2022self} and SBI~\cite{shiohara2022detecting}. The latter works are devoted to a specially designed framework that can boost the generalization of backbones. 
For example, GFF~\cite{sun2021dual} uses high-frequency stream to relieve the textures bias, and DCL~\cite{sun2021dual} leverage a supervised contrastive learning framework to preserve the variance among forgery instances. Another line of work introduces modalities other than RGB images to mine for forgery traces. For example, some works leverage frequency information as an auxiliary modal to obtain subtle forgery clues, such as DCT~\cite{qian2020thinking}, spatial-phase~\cite{liu2021spatial}, and SRM~\cite{luo2021generalizing}.
Other works exploit the motion of lips as additional supervisory signals~\cite{haliassos2021lips,haliassos2022leveraging}. Besides, some methods also leverage mask as auxiliary supervision signal\cite{chen2021local,stehouwer2019detection}.
However, these methods ignore the fine-grained semantic information, which can help the model obtain more generalization features.
% Figure environment removed
\subsection{Visual-Language Multimodal Leaning}
Vision and language are the two important signals for human perception, the visual-language multimodal learning thus has drawn lots of attention in the AI community.
Some works are devoted to using language information as a supervisory signal to guide vision tasks. Such as Visual Grounding~\cite{karpathy2014deep,kamath2021mdetr,zhu2022seqtr},
Vision-and-Language Navigation\cite{anderson2018vision,fried2018speaker},  
and Image Generation from Text~\cite{xu2018attngan,ramesh2021zero}. The success of these tasks demonstrate that language can help the vision models learn more fine-grained representations. Recently, another important multimodal learning paradigm \textit{i.e.} visual-language pretraining, has achieved great success. Specifically, CLIP~\cite{radford2021learning} first uses multimodal contrastive learning to train text and image encoders with 
4 million paired visual-language web data. Subsequently, many works have been put forward to fine-tune the VL pretrain model to adapt downstream tasks such as action recognition
\cite{wang2021actionclip}, and ReId\cite{yan2022clip}.
% Inspired by these works, we first introduce the language information into a general face forgery detection task with the help of the pretrain model, enabling the model to achieve more generalizability and interpretability.

\section{Method}
\label{sec:method}
% Previous general face forgery detection methods treat this task as a binary classification problem under the binary numbers labels supervision. This pipeline lacks fine-grained guidance and ignores the semantic information contained in the natural language.
% Figure environment removed
% Figure environment removed
% To provide such knowledge, 
In this section, we introduce our Visual-Linguistic Face Forgery Detection (VLFFD) framework, which 
explores the coarse-grained and
fine-grained natural language supervision with a pretrained multimodal framework to strengthen the generalization and interpretability. As shown in Fig \ref{fig:main}, VLFFD contains two unimodal encoders $E_i$ and $E_t$ for images and text prompts via dual stream framework. The $E_i$ extract image feature for the visual modality could be any architecture. The text encoder $E_t$ is used to extract language features of the input prompt. The output dimensions of $E_i$ and $E_t$ should be consistent in order to calculate the similarities.
The beginning of the VLFFD is
to generate the mixed forgery images with their corresponding fine-grained prompts using forgery dataset of paired real and fake faces, named Prompt Forgery Image Generator (PFIG). 
Subsequently, the Coarse-and-Fine Co-training Framework (C2F) is used to jointly train the coarse and fine data via multimodal contrastive learning, enabling encoders to potentially perform the coarse-to-fine learning process.
% During test time, the VLFFD can provide not only simple real or fake classification results but also give the similarity of sentences contained
% forgery region and types.


\subsection{Prompt Forgery Image Generator}
The existing face forgery dataset contains category-level labels (real or fake) while lacking sentence-level annotations. Collecting manual annotation not only
requires huge labor-intensive but also faces the problem of mislabeling.
Thus, in this work, we first propose \textit{Prompt Forgery Image Generator (PFIG)} to produce mixed forgery images with fine-grained annotation using an existing paired face forgery dataset. 
The PFIG disentangles the forgery artifacts from the existing dataset into \textbf{regions} and \textbf{types}, which has been argued that is two key elements of the forgery traces~\cite{chen2022self,shiohara2022detecting}. 
As shown in Fig~\ref{fig:pfig}, 
given a real image $i_r \in R^{ 3\times H \times W}$ with its corresponding forgery image $i_f \in R^{ 3\times H \times W}$, the PFIG include following steps:
% \begin{itemize}
    % \item  
    
    \textbf{Mask Generation}. To locate the forgery region, similar to~\cite{chen2021local}, we first construct manipulated mask $M$ by computing the absolute pixel-wise difference in the RGB channels, and normalized it to the range of $[0,1]$:
    \begin{equation}
        M = |i_r-i_f| / 255.
        \label{equation:1}
    \end{equation}
    
    % \item 
    \textbf{Forgery Region Extraction}. 
    This step aims to choose one forgery region that contains the $i_f$.
    Specially, we divide the facial image into four regions based on landmarks, including mouth, nose, eyes, and face. 
    Then, we calculate the average value of $M$ within each region area, and empirically determine the threshold $\theta$ to obtain the forgery regions list $L_f$. That is:
    \begin{equation}
        \frac{1}{|R_t|}\sum_{j\in R_t}M(j) > \theta, R_t \rightarrow L_f,
        \label{equation:2}
    \end{equation}
    where $R_t$ is one of the regional areas of the predefined four regions, $|R_t|$ represented the sum of pixels within the region $t$. If the value is greater than the $\theta$, the corresponding region is appended into regions list $L_f$.
    After looping the Eq.~\ref{equation:2} with four regions, we randomly select one region $R_s$ from $L_f$ for the next step. The $R_s(i_r)$ and $R_s(i_f)$ represent the forgery region of the real and fake pixels, respectively.
    % \item  
    
    \textbf{Forgery Type Decision}. The goal of this step is to determine the type of forgery via a specially designed criterion.
    According to the previous work and our observation, we categorize the existing forgery types as color difference, blur, structure abnormal, texture abnormal, and blend boundary. We detail each forgery type and corresponding evaluation standard as follows:
    1) \textbf{Color Difference}. 
    This phenomenon occurs in the face swap when the color of the source and target face has a drastic difference, as shown in Fig~\ref{fig:type}(a). Inspired by the color transfer~\cite{reinhard2001color},
    we leverage the distance of the average channel-wise mean and variance of the real and fake region in the $Lab$ color space to determine whether there exists a color difference. 2) \textbf{Blur}. To quantify the local blurring of forgery faces as shown in Fig~\ref{fig:type}(b), we exploit the Laplacian operator to reflect the sharpness of image edges. Specifically, we compute the variance of the real and fake images of the selected region after the Laplacian operator to determine whether this part is blurred.
    3) \textbf{Structure Abnormal}. We observed that some organs of fake faces will be obviously deformed, such as the left eye in Fig~\ref{fig:type}(c). To metric such a phenomenon, we use the Structural Similarity (SSIM) index difference between real and fake images of the selected region $R_s$ to decide whether the chosen region has a structure abnormal or not. 
    4) \textbf{Texture Abnormal}. It has been proved that the generator cannot generate as strong texture contrast as real data~\cite{liu2020global}, leading to texture difference as shown in Fig~\ref{fig:type}(d). Inspired by Gram-Net~\cite{liu2020global}, we leverage the contrast of Gray-Level Co-occurrence Matrix (GLCM)~\cite{haralick1973textural}, formed as $C_d$, to reflect the clarity of texture.
    We define a forgery region as texture abnormal when the $C_d$ of the real is larger than the fake one beyond the threshold.
    5) \textbf{Blend Boundary}. Existing face manipulation methods
    conduct blending operation to transfer an altered face into an existing background, which leaves intrinsic cues across the blending boundaries~\cite{li2020face}, such as the red circle of Fig~\ref{fig:type}(e). However, we find it difficult to directly detect the blend boundaries, especially if the forged image is of high quality. We thus enhance this trace by adjusting the blend methods and ratio in the next step.

    % \item  
    \textbf{Forgery Blending}. 
    To increase the variety of mixed forgery image $i_m$, we exploit Poisson blending or alpha blending according to a certain probability, formulated as follows:
    \begin{equation}
        \label{eq6}
        \left\{
        \begin{aligned}
            i_m =  & \alpha * R_s(i_f) + (1-\alpha)*i_r, &p<\theta_b\\
            i_m =  & Possion(R_s(i_f),i_r),& p>\theta_b,
        \end{aligned}
        \right.
    \end{equation}
    where $\alpha$ is the ratio used to control the degree of the blending, $Possion$ represents the standard Possion blending method, $p$ is a random variable sampled from a uniform distribution, $\theta_p$ is a threshold control the probability of two blending mechanisms. Compared with Poisson blending which leverages gradient regularization to smooth the boundary, alpha blending will bring obvious blending cues. Thus, we define the forgery type as Blend Boundary when choosing the alpha blending method.

% \end{itemize}
Supplementary materials provide pseudocodes for each forgery type criterion in detail. 
Finally, to combine the fine-grained annotation and adjust the up-stream pre-trained model, we generate sentence-level
fine-grained prompt $P$ for each mixed forgery image with template \textit{"this is a fake person, the forgery region is \_\_ , the forgery type is \_\_"}. The former blank fills with the selected forgery region $R_s$, the latter blank fills with the corresponding forgery types. Different from regular multimodal tasks that require raw descriptions of the image, recent works demonstrate that CLIP can achieve SOTA performance by simply filling templated text.
Following this paradigm, we use a specially designed cloze form as our prompt to fine-tune CLIP, which can better perceive the face regions and forgery types.



\subsection{Coarse-and-Fine Co-training Framework}
In this section, we introduce our learning framework adopted by the multimodal co-training mechanism. Specifically, the original data with coarse-grained prompts are trained by Coarse-grained Multimodal Learning (CML) to let $E_i$ and $E_t$ obtain the basic discrimination.
Meanwhile, Fine-grained Multimodal Learning (FML) further guides and refine the encoders to learn more generalized cues by learning from rich supervision semantic information. 

\noindent\textbf{Coarse-grained Multimodal Learning.}
To adapt the multimodal learning framework, we first convert the digitized binary labels (0 for real and 1 for forgery) into prompts. After trying several templates,
we choose \textit{"this is a fake person"} for the forgery image and \textit{"this is a real person"} for the real one as the coarse-grained language prompts. 
Then, the real and forgery prompts are fed into the text encoder $E_t$ and concentrated together to form a coarse-grained language feature, represents as $l_c \in R^{2 \times D}$. Given the input data $x \in R^{B\times 3 \times H \times W}$ (B is the batch size)
with label $y \in \{0,1\}$, the visual feature $v_c \in R^{B\times D} $ (D is the feature dimension) is obtained by the image encoder $E_i$. We use normalized cosine similarity as the metric of two modalities features denoted as $s(u,v) = \frac{u.v^T}{||u|||v||}$.
The coarse-grained loss $L_c$ based on cross-entropy loss can be defined as:
\begin{equation}
    L_{c} = -\frac{1}{B}\sum y log(s(v_c,l_c)).
\end{equation}


\noindent\textbf{Fine-grained Multimodal Learning.}  
To efficiently exploit the sentence-level knowledge, inspired by the visual-language pre-trained methods~\cite{radford2021learning}, we use the multimodal contrastive learning framework, which pulls the paired image and text representations close to each other.
Specifically, a batch of mixed forgery images $x_f \in R^{N \times 3 \times H \times W}$ ($N$ is the batch size) with prompt $p_f \in P$, are fed into the image encoder $E_i$ and text encoder $E_t$ to extract visual and language features $v_f \in R^{B \times D}$ and $l_f\in R^{B \times D}$, respectively, where $D$ is the dimension of the feature. Then, we compute the symmetric normalized cosine similarities between two modalities 
$s(v_f,l_f)$ and $s(l_f,v_f) \in R^{B\times B}$. The diagonal elements of the two symmetric matrices represent the similarity of paired features, while others are mismatched. Thus, we define the symmetric cross-entropy loss as the fine-grained loss $L_{f}$, which maximizes the diagonal of the two similarity matrices as follows:
\begin{equation}
    L_{f} = -\frac{1}{2N}(\sum log(s(v_f,l_f) \odot I) + \sum log(s(l_f,v_f) \odot I)),
\end{equation}
where $I \in R^{N \times N}$ represents the identity matrix. Since the beginning of the fine-grained prompt is the forgery coarse-fined prompt, the $L_f$ can not only 
help encoders extract more discriminative representations but also encourage the image encoder to extract more refined features that promote generalization.

\noindent\textbf{Overall Loss Function.} Considering both the
coarse-grained loss and fine-grained loss, the overall loss for our proposed framework is:
\begin{equation}
    L = L_c + \phi L_f,
\end{equation}
where $\phi$ is the hyperparameter used to scale the fine-grained loss. 

\section{Experiment}
\label{sec:experiment}
\subsection{Experimental Setting}
\input{table/tab1.tex}

\noindent\textbf{Dataset.}
To evaluate the generalization of our proposed VLFFD, we 
conduct our experiments on several challenging datasets:
1) FaceForensics++~\cite{rossler2019faceforensics++}: a widely-used forgery dataset contains 1000 videos with four different manipulated approaches, including two deep learning based \textit{DeepFakes} and \textit{NeuralTextures} and two graphics-based methods \textit{Face2Face} and \textit{FaceSwap}. This dataset provides  pairwise real and forgery data, enabling us to generate mixed forgery images with PFIG. 2) DFDC-P~\cite{dolhansky2020deepfake} dataset is a challenging dataset with $1133$ real videos and $4080$ fake videos, containing various manipulated methods and backgrounds.
3) DFD is a forgery dataset containing $363$ real videos and $3068$ fake videos, which is mostly generated by the Deepfake method. 4) Celeb-DF~\cite{li2019celeb}
is another high-quality Deepfake dataset that contains various scenarios. 5) Wild-Deepfake~\cite{zi2020wilddeepfake} is a forgery face dataset
obtained from the internet, leading to a diversified distribution of scenarios. We use DSFD~\cite{li2019dsfd} to extract faces from each video.

% \noindent\textbf{Evaluation Metrics}. We use the area under the receiver operating characteristic curve (AUC) and equal error rate (EER) as our basic evaluation metrics.

\noindent\textbf{PFIG details.}
We use the open-source dlib algorithm as the face landmark detector. For the forgery type decision, the threshold of mean and variance is $1.0$ and $0.5$. 
For the blur, the threshold is set to $100$. If the difference of SSIM is larger than $0.97$, we determine the forgery part is structure abnormal. The texture abnormal threshold is set to $0.7$. The blending ratio $\alpha$ is set to $0.9$.


\noindent\textbf{Training details.}
To leverage the visual-language correspondence information,
we use CLIP\cite{radford2021learning} as the pretrain model of $E_i$ and $E_t$. The feature dimension $D$ is $768$. 
We resize the input into $224\times 224$, and the ViT-L is used as the image encoder. We use Adam optimizer to train the framework and the learning rate is set to $1e-6$. The batch size of Coarse-grained Multimodal Leaning $B$ is set to $32$ and the batch size of Fine-grained Multimodal Leaning $N$ is set to $24$. The hyperparameters $\phi$ is set to $0.1$. The total training epoch is 40 and the overall framework is implemented in PyTorch on one NVIDIA A-100 GPU.

\noindent\textbf{Testing details.}
During testing, Following CLIP~\cite{radford2021learning}, all text features corresponding to coarse-grained and fine-grained prompts can be extracted in advance, and the image feature of the test face is matched with the most similar text feature by cosine similarity to obtain the binary label as well as the text description. The test period can be divided into coarse-grained classification and fine-grained matching. The former aims to identify the real or fake of the input face, while the latter output the sentence-level description in terms of forgery region and types. All the quantization results use the coarse-grained classification.
\subsection{Experimental Results}
\noindent\textbf{Cross-dataset evaluation.} To demonstrate the generalization of our VLFFD, we first evaluate performance on the unseen datasets against the recent SOTA methods.
Specifically, we train our model on the high-quality version of FF++ and test on the other datasets, which have large domain gaps with FF++ in terms of forgery types, human ids, video backgrounds, and image quality \textit{e.t.c}. 
The quantitative frame-level results are shown in Tab~\ref{table:1}. We can observe that our method can obtain significant improvement compared with the other methods in terms of AUC and EER.
Concretely, our methods outperform about 8\% on DFDC-P and 12\% on Wild-Deepfake compared with framework engineering based methods DCL.
Compared with forgery simulation based SBI and PCL+I2G, VLFFD can achieve better performance on Celeb-DF.
Furthermore, our method outperforms the recent transformer-based methods UIA-ViT by 9\% and 2\% on DFDC-P and Celeb-DF, respectively.
The results demonstrate that fine-grained language information and a powerful visual-language pre-training model can greatly improve the generalization ability.

\noindent\textbf{Cross-manipulation evaluation.}
The aforementioned experiments show the effectiveness of VLFFD under large domain gap situations. To further demonstrate the generalization among different manipulated methods, we train a model on one method within the high-quality of FF++ dataset and test on the four methods. We compare with the recent four methods including the Multi-attentional, GFF and DCL, results for these methods are from~\cite{sun2021dual}.
As shown in Tab~\ref{table:2}, our proposed method achieve the SOTA performance in most situations. In particular, in several cases, the VLFFD outperforms the compared methods significantly. For example, when training on the FaceSwap and testing on the DeepFakes, the performance improved by over 18\% in terms of AUC against DCL. The performance gain because the fine-grained supervisory information guides the model to capture more refined forgery patterns, which are shared across the different forgery types, such as the texture abnormal face in Deepfakes.

\input{table/tab2.tex}

% \noindent\textbf{Multi-source manipulation evaluation.}
% As mentioned in~\cite{sun2021domain}, the multisource cross-manipulation has great practical significance. Therefore, we also test performance when train on three methods and test on the unknown method. Furthermore, to demonstrate robustness of our method against different the image quality. We conduct this experiments on both high-quality and low-quality datasets. The results are reported in Tab~\ref{table:4}. Our method achieves the SOTA performance on all protocols and quality, which shows the robustness and generalization of our proposed framework.
% \input{table/tab3.tex}

\subsection{Ablation Study}
In this section, we perform an extensive ablation study to explore the impact of each component of the VLFFD. 
% For the following ablation experiments, we uniformly train on ff++ (HQ) dataset and perform generalization testing on Celeb-DF and DFDC-P in terms of AUC and EER.

\noindent\textbf{Impact of different components.}
To analyze the impact of the Multimodal learning and PFIG module, we adapt the image-encoder of CLIP to the binary classification task and remove the text encoder, which has the same backbone and pretrain weight as our multimodal version. 
The results are shown in Tab~\ref{table:ab1}. The PFIG represents whether use the generated mixed forgery images to train the model.
We can observe a consistent improvement in performance when using the multimodal learning framework, with or without PFIG, which demonstrates the advantage of language information with the visual-linguistic pre-training model.
Furthermore, the introduction of PFIG leading a 3\% increase in AUC, which demonstrates the effectiveness of fine-grained data generated by PFIG.

\input{table/ablation1.tex}
\input{table/ablation2.tex}

% Figure environment removed

\noindent\textbf{Impact of pretrain weight and backbone.}
To analyze the impact of pretrain weight and backbone, we explore pretrain weight from ImageNet as well as different backbones, including CNN-based backbones Xception and EN-B4 and Transformer-based backbones ViT-B and ViT-L.
To adapt the CNN-based backbone into our framework, we use MLP to align the feature dimension with the CLIP pretrain text encoder.
The quantitative results are shown in Tab~\ref{table:ab2}, we can observe that 1) the performance is consistently improved after adopting our VLFFD into different backbones, which demonstrates that our method can boost generalization even without pretrain knowledge.
2) The CLIP-based image encoder outperforms ImageNet-based by 4\% and 5\% on Celeb-DF and DFDC-P datasets, respectively. That may prove that the semantic knowledge hidden in the VL pretrain model can better unleash the potential of our approach.


\input{table/ablation_text.tex}
\noindent\textbf{Impact of language information.}
% To demonstrate the importance of language information quantitatively, we contrast our text-based supervisory signals with masks and multiple numerical labels. 
To quantify the importance of language information, we compare our text-based supervisory signals with masks and digital labels. We use a four-layer upsampling convolution as a decoder after the last feature of the image-encoder to regress on the forgery mask and compute the mean squared error loss with the mask obtained from Eq.~\ref{equation:1}. For the digital labels, we replace the text-encoder with multiple classifiers (MC) that have three levels of granularity (Region only, Type only, and both Region and Type) for the generated mixed forgery samples. As shown in Tab~\ref{table:ablation}, our text-based method surpasses both mask and multiple classifiers for all levels of granularity. The mask-supervised method exhibits weak generalization, suggesting that mask supervision alone can cause overfitting to the training data. Moreover, unlike the text-encoder that improves steadily with more fine-grained supervision, the multiple classifiers are affected by the number of classes and class imbalance problem, which further confirms the scalability and versatility of language over multi-classification.
% To quantitatively prove the necessity of language information, we compare our text based supervisory signals with mask and multiple digital labels. Specifically, to regress with the forgery mask, we apply a four-layer upsampling convolution as decoder after the last feature of the image-encoder and perform mean squared error loss with the mask calculated with E.q.\ref{equation:1}. For the digital labels, we replace the text-encoder with multiple classifiers (MC) with three granularities (only Region, only Type, and both Region with Type.) of the generated mixed forgery samples. The results reported in Tab~\ref{table:ablation} show that text based method outperforms both mask and multiple classifiers under all granularities. The mask-supervised method suffers from poor generalization, indicating that directly using mask supervision may lead to overfitting of the training data. Furthermore, different from the text-encoder improved consistently with more fine-grained supervision, the multiple classifiers are influenced by the number of categories and class imbalance problem, which further proves the scalability and versatility of language compared to multi-classification. 

% To quantitatively compare language information with other supervisory signals in our method, we 
% Mask, Digital, and Prompt (Ours) as the supervisory signals with 
% with different granularities

% The main advantage of utilizing text-encoder lies in two aspects: \textit{1) to exploit and simultaneously update the semantic information hidden in the large-scale multi-modal pretrain model; 2) to enable future studies on unlimited combinations of forgery types.}
% To quantitatively prove the necessity of text-encoder, we 
% utilize various 
% replace the text-encoder with multiple classifiers (MC) with three granularities (Region, Type, Region+Type), and show the results in Tab~\ref{table:ablation}. Text based method outperforms multiple classifiers under all granularities.
% Furthermore, different from the text-encoder improved consistently with more fine-grained supervision, the multiple classifiers are influenced by the number of categories and class imbalance problem, which further proves the scalability and versatility of language compared to multi-classification. Besides 


% \noindent\textbf{Impact of the different training strategy.}
% In table \ref{table:ab3}, we conduct several training strategies to demonstrate the effectiveness of our C2F. Specifically, Coarse-Only means using a single coarse-grained label to train the encoders; Fine-Pretrain represents that we first leverage fine-grained multimodal learning framework as the pretrain model to learn the forgery-related semantic information, then we adapt with the coarse-grained multimodal learning to obtain binary classification ability. The Fine-and-Fine mechanism uses the PFIG to relabel the original dataset and leverage the fine-grained prompt instead of coarse-grained labels. The result demonstrates our Coarse-and-Fine co-training strategy achieves the best generalization compared to others, which shows the importance of coarse and fine-grained coordination. The Fine-and-fine scheme also obtains suboptimal performance demonstrating that too fine granularity will affect the basic classification ability and generalization.

% \input{table/ablation3.tex}

% \noindent\textbf{Impact of different prompt.}
% We try several coarse-grained sentences to explore the effectiveness of prompt template. Specifically, we use
% "A photo of fake/real person" as \textit{Prompt1}; "A photo of real/Deepfakes" as \textit{Prompt2}; "This is a normal/fake person" as \textit{Prompt3}; "This is a real/fake person" represents the \textit{Prompt4}. The result are shown in Tab~\ref{table:ab4} report the quantitative results, we can observe that the Prompt5 obtain the best generalization, probably because the social media data based CLIP is more sensitive to the word "real" and "fake".
% We will try to automatically learn the appropriate prompt like CoOp~\cite{zhou2022learning} in future work to eliminate the effect of prompt chosen.




\subsection{Visualizations}

% % Figure environment removed

\noindent\textbf{Visualizations of classification decision.}
To understand the decision-making mechanism of our method, we visualize the attention heatmap of the baseline method (binary classification with CLIP pretrained image-encoder) and our VLFFD for the unseen test forgery image, respectively. Furthermore, we also show the Top-1 similarity
score of fine-grained prompt as the linguistic interpretation of our model. As shown in Fig~\ref{fig:vis1}, we can observe that the attention is more clearly directed to the fine-grained
forgery regions compared with the baseline. In addition, the highlighted attention area basically corresponds to the region in the matched semantic sentence. For example, the attention of the baseline method ignores the abnormal region of the right eye of the first face. After training with the VLFFD framework, the network clearly focuses on this region due to the supervision of fine-grained semantic information. The prompt can further provide a detailed classification basis for regions and types.
This visualization demonstrates that our VLFFD can effectively capture more refined and accurate forgery features and improve the interpretability of the model. More visualizations are provided in the supplementary material.
% \input{table/ablation4.tex}

% \noindent\textbf{Visualizations of the mixed forgery image.}
% In this section, we provide more examples of the mixed forgery image generated by the PFIG module in Fig~\ref{fig:vis2}. Note that there may exist more than one forgery types of each image, here we show the most obvious features.
% From the figure, we observe that each forgery type corresponds to a specific characteristic. For example, the \textit{blend boundary} have obvious fusion traces in the designated area, and if the shape is deformed, it will be judged as the \textit{Structure abnormal}. 


% We also conduct human scoring experiments. Specifically, we sample 1000 mixed forgery images with their fine-grained prompt and found 10 people to score the matching degree, the score range is 1-5, the higher the score, the more accurate the fine-grained annotation. The average score is xx points, which demonstrate the classification of forgery types is human-intuitive.

\section{Conclusion}
\label{sec:conclusion}
This paper focuses on improving the generalization and interpretability of the face forgery detection task via visual-linguistic manner. Traditional methods usually use binary numbers label as the supervisory signal which lacks fine-grained guidance and semantic information. In this paper, we tackle this issue by proposing a novel multimodal learning paradigm named VLFFD, which introduces the language modality as fine-grained semantic supervisory signals. Specifically, we first leverage PFIG module to automatically generates mixed forgery images with fine-grained prompts by analyzing the original forgery dataset. Then the C2F is designed to learn the multi-grained semantic information. Extensive experiments demonstrate the significant superiority of our method over state-of-the-art methods.

\section{Acknowledgments}
\label{sec:ack}
This work was supported by National Key R\&D Program of China (No.2022ZD0118202), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No.2021J01002,  No.2022J06001).

{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}
\clearpage
\appendix


\section*{\centering \Large Appendix \label{appendix}}

\section{Details of PFIG.}
\noindent\textbf{Color Difference.}
This phenomenon occurs in the face swap when the color of the source and target face has a drastic difference. Inspired by the color transfer~\cite{reinhard2001color},
we leverage the distance of the average channel-wise mean and variance of the real and fake regions in the $Lab$ color space to determine whether there exists a color difference. The $Lab$ color space minimizes correlation between channels, which helps reduce the impact of changes in a certain channel on the overall color.
The pseudocode is shown in Alg.~\ref{alg:cd}, $split$ represents dividing the channel of the image, $Lab$ denotes converting the RGB color space into $Lab$ space.
\input{table/alg_cd.tex}

\noindent\textbf{Blur.}
There exists local blurring in forgery faces due to the instability of the generated model or blending operation. To quantify such phenomena, we make use of the Laplacian image, which can reflect the sharpness of image edges.
Specifically, as shown in Alg.~\ref{alg:blur}, we compute the variance of the real and fake images of the selected region after the Laplacian operator, and if the value of the real is larger than the fake one and their difference is greater than a certain threshold, we define this part as blurred. The $Laplacian(.)$ represents the Laplacian operator, $var(.)$ means calculating the variance of the input image.
\input{table/alg_blur.tex}

\noindent\textbf{Structure Abnormal.}
We observed that compared with normal faces, some organs of fake faces will be obviously deformed. To metric such structure deformable, we use the Structural Similarity (SSIM) index difference between real and fake images of the selected region $R_s$ to decide whether the chosen region has a structure abnormal or not, which details in Alg.~\ref{alg:sab}.
\input{table/alg_sab.tex}

\noindent\textbf{Texture Abnormal.}
It has been proved that the generator typically correlates the values of nearby pixels and cannot generate as strong texture contrast as real data~\cite{liu2020global}, leading to texture differences in some forgery regions. Similar to the Gram-Net~\cite{liu2020global}, we leverage a texture analysis tool--the contrast of Gray-Level Co-occurrence Matrix (GLCM)~\cite{haralick1973textural}, formed as $C_d$.
Larger $C_d$ reflects stronger texture contrast, sharper and clearer visual effects. Inversely, a low value $C_d$ means the texture is blurred and unclear.
We define a forgery region as texture abnormal when the $C_d$ of the real is larger than the fake one beyond the threshold. The algorithm is shown in Alg.~\ref{alg:tab}, where $GLCM$ represents the average Gray-Level Co-occurrence Matrix of the input from right, down, left, and upper four orthogonal directions.
\input{table/alg_tab.tex}

\section{Additional Experimental Results.}
\noindent\textbf{Multi-source manipulation evaluation.}
As mentioned in~\cite{sun2021domain}, the multisource cross-manipulation has great practical significance. Therefore, we also test performance when training on three forgery methods and test on the unknown forgery method. Furthermore, to demonstrate the robustness of our method against different image qualities, we conduct these experiments on both high-quality and low-quality datasets. The results are reported in Tab.~\ref{table:4}. Our method achieves the SOTA performance on all protocols and quality. Specifically, our method outperforms UIA-ViT by 4\% in the high-quality version of DF.
Furthermore, our method achieves significant improvement compared with recent DCL in low-quality settings,
which shows the robustness and generalization of our proposed framework.
\input{table/tab3.tex}

\noindent\textbf{Impact of the different training strategy.}
In Tab.~\ref{table:ab3}, we conduct several training strategies to demonstrate the effectiveness of our C2F. Specifically, Coarse-Only means using a single coarse-grained label to train the encoders; Fine-Pretrain represents that we first leverage fine-grained multimodal learning framework as the pretrain model to learn the forgery-related semantic information, then we adapt with the coarse-grained multimodal learning to obtain binary classification ability. The Fine-and-Fine mechanism uses the PFIG to relabel the original dataset and leverage the fine-grained prompt instead of coarse-grained labels. The result demonstrates our Coarse-and-Fine co-training strategy achieves the best generalization compared to others, which shows the importance of coarse and fine-grained coordination. The Fine-and-fine scheme also obtains suboptimal performance demonstrating that too fine granularity will affect the basic classification ability and generalization.

\input{table/ablation3.tex}

% \noindent\textbf{Impact of the different prompt.}
% We try several coarse-grained sentences to explore the effectiveness of the prompt template. Specifically, we use
% "A photo of fake/real person" as \textit{Prompt1}; "A photo of real/Deepfakes" as \textit{Prompt2}; "This is a normal/fake person" as \textit{Prompt3}; "This is a real/fake person" represents the \textit{Prompt4}. The result shown in Tab.~\ref{table:ab4} report the quantitative results, we can observe that Prompt5 obtain the best generalization, probably because the social media data based CLIP is more sensitive to the word "real" and "fake".
% We will try to automatically learn the appropriate prompt like CoOp~\cite{zhou2022learning} in future work to eliminate the effect of the prompt chosen.
% \input{table/ablation4.tex}



\section{Additional Visualization Results.}

% \noindent\textbf{Additional visualizations on the unseen dataset.}
% To further illustrate the superiority of our method intuitively, we have supplemented Figure 5 of the main paper with a more detailed comparison with the state-of-the-art method UIA-VIT~\cite{zhuang2022uia}. Specifically, we use the official code with the well-trained model on FFpp (HQ) and visualize the attention layer of UIA-VIT. The visualization results are shown in Fig.~\ref{fig:vis1}. We can observe that compared with baseline and UIA-VIT, our method can better capture forged features, such as in the first column of images, where both baseline method and UIA-VIT are distracted by the interfering microphone, while our method can focus on the fake eyes and nose of the face. Moreover, for some high-quality forgery faces, such as in the fourth column, our method can highlight more discriminative features (eye color differences). Furthermore, the fine-grained cues provided by our method can also enhance the interpretability of the model, which is beneficial for applying the model in various scenarios.
% % Figure environment removed
% Figure environment removed
% Figure environment removed
\noindent\textbf{Additional visualizations on FFpp dataset.}
To further validate the interpretability of our method, we visualized the attention heatmap of the baseline method and the state-of-the-art method UIA-VIT~\cite{zhuang2022uia} compared with ours on the test set of training data (FFpp HQ), including four methods namely DeepFake, FaceSwap, Face2Face and NeuralTextures, and ground truth mask for comparison. Specifically, we use the official code with the well-trained model on FFpp (HQ) and visualize the attention layer of UIA-VIT.
We can see from Fig.~\ref{fig:vis1} that our method can detect more precise forgery traces compared with baseline and UIA-VIT. For example, our method clearly focuses on the subtle mouth area in the NeuralTextures method, which is consistent with the ground truth, while other methods are misplaced. This demonstrates that fine-grained linguistic information can provide more precise guidance to our detection method.


\noindent\textbf{Visualizations of the mixed forgery image.}
In this section, we provide more examples of the mixed forgery image generated by the PFIG in Fig.~\ref{fig:vis2}. Note that there may exist more than one forgery type for each image, here we show the most obvious features.
From the figure, we observe that each forgery type corresponds to a specific characteristic. For example, the \textit{Blend boundary} has obvious fusion traces in the designated area, and if the shape is deformed, it will be judged as the \textit{Structure abnormal}. 


\section{Details of the testing period.}
During testing, all text features corresponding to coarse-grained and fine-grained prompts can be extracted in advance, and the image feature of the test face is matched with the most similar text feature by cosine similarity to obtain the binary label as well as the text description. The test period can be divided into coarse-grained classification and fine-grained matching. The former aims to identify the real or fake of the input face, while the latter output the sentence-level description in terms of forgery region and types. All the quantization results use the coarse-grained classification.
% \input{iccv2023AuthorKit/table/alg_test}


\end{document}
