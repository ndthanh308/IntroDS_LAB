\section{Introduction}

Video Object Segmentation (VOS)~\cite{VOS_survey} is a widely performed vision task with applications ranging from object recognition, scene understanding, medical imaging, to filter effects in video chats. While fully automated approaches based on pre-trained models for object segmentation are often desired, interactive user guidance is commonly practiced to annotate new training data or when precise rotoscoping is required for highly complex footages such as those found in visual effects. This is particularly the case when the videos have challenging lighting conditions and dynamic scenes, or when partial region segmentation is required. While automatic VOS methods are designed to segment complete objects with clear semantic outlines, interactive video object segmentation (IVOS) and semi-supervised video object segmentation (SSVOS) techniques~\cite{VOS_survey} are more flexible, and typically use a scribble or contour drawing interface for manual refinement such as those found in commercial software solutions such as Adobe After Effects and Nuke. Despite advancements in IVOS and SSVOS techniques, rotoscoping in film production is still a highly labor-intensive task, and often requires nearly every frame of a shot to be annotated and refined \cite{adoberotoscope2017}.

State-of-the-art IVOS and SSVOS techniques use memory-based models~\cite{STM} and have shown impressive segmentation results on complex scenes based on user-provided mask annotations, but they are often designed to improve single annotation performances~\cite{STM, AOT, DeAOT, STCN, cheng2022xmem}  and are still not suitable for production use cases. In particular, they tend to over-segment known semantic outlines (person, hair, faces, entire objects) and fail on partial regions (e.g., half of a person's face, a dog's tail), harsh lighting conditions such as shadows, and extreme object poses. As a result, inconsistent segmentations are obtained when only a single annotated frame is provided, due to the inherent ambiguity of the object's appearance, Especially when it varies too much due to large viewing angles and complex lighting conditions, which limits the scalability of these techniques, and it is often unclear which frame annotations to prioritize, especially for long sequences.

%Thus a common variation of VOS is semi-supervised video object segmentation (SSVOS) \cite{VOS_survey}. In this setting, the end user provides annotations for one or more frames of the video that capture the target object, and the model then segments the rest. This is \textbf{particularly useful in the industry setting} because it allows having \textbf{one generic segmentation model} that can segment any given object with just a few annotations. Interactive Video Object Segmentation (IVOS) is an extension of SSVOS that aims to simplify the task of annotating the reference frames by allowing the user to provide \textbf{sparse annotations} (usually in the form of scribbles \todo{cite scribbles}), that are used instead of dense pixel-level annotations masks \todo{citeteaser
%Existing works on SSVOS and IVOS have shown impressive results, particularly after the introduction of memory-based models in \cite{STM}. \todo{Write more}. However, there are certain limitations they possess - they are not yet suitable for production-level segmentation. Any professional solution (\todo{for movies only or for other applications as well?}) requires 99\% of the frames to be manually annotated \todo{cite to back up the claim}, which usually means using rotoscoping \todo{cite examples} on every frame of the video. Existing works primarily focus on improving single-annotation performance \cite{STM, AOT, DeAOT, STCN, XMem}, with limited multi-frame annotation scalability analysis, focusing on user interaction duration \cite{MIVOS} or picking the optimal frame to annotate \cite{IVOS-W, GISRAmap}. However, the achievable level of segmentation quality obtained using only 1 annotated frame per video or per object is limited by inevitable changes of the object's appearance with variations in pose, lighting, and camera viewpoint. Only a certain subset of existing SSVOS and IVOS architectures supports using multiple annotations by design, and there has been a lack of performance scaling analysis for them. Furthermore, they often fall short on difficult usecases like \textbf{partial segmentation} (Fig. \ref{fig:pumavosdataset}) - when only a certain part of the object needs to be segmented - e.g. left part of the actor's face or a dog's tail, due to the lack of a relevant data.

We propose a new SSVOS framework, \modelname{}, which uses a permanent memory module that stores all annotated frames and makes them available as references for all the frames in the video. While most SSVOS methods focus on single-frame mask annotations, our approach is designed to handle multiple frames that can be updated by the user iteratively with optimal frames being recommended by our system. While we adopt the cutting-edge architecture of XMem \cite{cheng2022xmem} as backbone, we show that our important modification enables accurate segmentation of challenging video objects (including partial regions) in complex scenes with significantly fewer annotated frames than existing methods. Our modification does not require any re-training or calibration and additionally shows improved temporal coherence in challenging scenarios (Fig. \ref{fig:teaser}, \ref{fig:xmempp_results}, \ref{fig:xmemppcomparison}). Our attention-based frame suggestion method predicts the next candidate frame for annotation based on previous labels while maximizing the diversity of frames being selected. Our system supports both sparse (scribbles) and dense (full masks) annotations and yields better quality scaling with more annotations provided than existing methods. The video segmentation performs in real-time, and frame annotations are instantly taken into account with a pre-trained network.

%We address these problems by introducing an important modification to a SOTA architecture XMem \cite{XMem} - \modelname{}, making it use additional annotated frames more efficiently, and thus result in \textbf{better segmentation accuracy for the same amount of work} by the end user. As a result we obtain an IVOS model that requires very few annotations per video (getting similar or better quality with 2x fewer annotation, see Table \ref{tab:LVOS_scaling}), works with both sparse (scribbles) and dense (full masks) annotations and yields better quality scaling with more annotations provided. Our modification does not require any re-training or calibration and additionally shows improved temporal coherence in challenging scenes (Fig. \ref{fig:frontal}). 

% introduce dataset

% say what evaluations we have done

% summarize findings

% summarize contributions as bullets
We further introduce a new dataset, \datasetname{} for benchmarking purposes, which includes new challenging scenes and use cases, including occlusion, partial segmentation, and object parts segmentation, where the annotation mask boundaries may not  correspond to visual cues.

We evaluate the performance of our algorithm both qualitatively and quantitatively on a wide range of complex video footages as well as existing datasets, and demonstrate SOTA segmentation results on complex scenes. In particular, we show examples where our method achieves higher accuracy and temporal coherence than existing methods with up to $5 \times$ fewer frame annotations and on $2 \times$ twice less on existing benchmarks (Section \ref{sec:results}).
We further demonstrate the effectiveness of our frame annotation candidate selection method by showing that it selects semantically meaningful frames, similar to those chosen by expert users. 
%Furthermore, we used \modelname{} to collect and annotate a dataset (\datasetname{}) of new challenging scenes and annotation use cases (including occlusion, partial segmentation, and object parts segmentation) - in particular those, where the annotation mask boundaries do not necessarily correspond to low-level visual cues.
We make the following contributions: 
\begin{itemize}[noitemsep,topsep=0pt]
    \item We have introduced a new VOS model, \modelname{}, that uses a permanent memory module that effectively utilizes multiple frame annotations and produces temporally-smooth segmentation results without overfitting to common object cues. %during video segmentation without overfitting to pre-trained object cues. \todo{add temporal smoothness}
    \item We further propose the use of an attention-based similarity scoring algorithm that can take into account previously predicted frame annotations to suggest the next best frame for annotation.
    \item We present a new dataset, \datasetname{}, which contains long video sequences of complex scenes and non object-level segmentation cues, which cannot be found in existing datasets.
    \item We achieve SOTA performance on major benchmarks, with significantly fewer annotations, and showcase successful examples of complex multi-class and partial region segmentations that fail for existing techniques.
\end{itemize}