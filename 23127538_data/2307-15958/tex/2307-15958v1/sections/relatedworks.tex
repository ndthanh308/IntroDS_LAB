\section{Related Works}
\label{sec:related_works}
\paragraph{Video Object Segmentation.} 
A wide range of video object segmentation (VOS) methods have been introduced in the past decade~\cite{VOS_survey}, spanning a broad spectrum of computer vision applications, including visual effects. Many solutions have been deployed in established commercial video editing software such as Adobe After Effects and Nuke. While early VOS techniques were often based on classic optimization methods and graph representations~\cite{SeamSeg2014, VOSpredp2012}, recent ones are typically using deep neural networks.

Semi-supervised video object segmentation (SSVOS) aims at segmenting objects in a video using a frame of reference (usually the first \cite{OSVOS2017} but some models also support multiple annotations). To help users create the annotation, interactive VOS methods (IVOS) were introduced \cite{joon2019, MIVOS}, providing users a convenient way to create annotation masks commonly using scribbles and dots selection interface.  

To facilitate user annotations, some IVOS methods suggest a fine-tuning approach, which makes any iterative user interaction slow during inference as retraining is required~\cite{OSVOS2017, xiao2018monet}. Although more efficient alternatives like online adaptation were introduced, their output quality is generally poorer~\cite{NEURIPS2020_eosvos, Robinson_2020_CVPR, park2021learning, bhat2020learning}. 

Attention-based methods use different techniques such as similarity or template matching algorithms to dictate which frames need to be focussed on from a set of available frames (referred to as memories)~\cite{STM, duarte2019capsulevos, zhang2020transductive,huang2020fast, ge2021video}. 
Multiple authors have focused on facilitating the model to use local/pixel-to-pixel information which improves the quality of the masks using either kernels\cite{KMN2020}, optical flow\cite{xie2021efficient, yu2022batman}, transformers \cite{mao2021joint, lan2022learning, AOT, DeAOT, yu2022batman} or improvements to the spatial-temporal memory \cite{ wang2021swiftnet, xu2022reliable, MIVOS, STCN, AFB_UBR2020, li2020fast, QDMN2022, SWEM2022, li2022recurrent}. 

Most recently, XMem~\cite{cheng2022xmem} has proposed a resource-efficient method that compresses the feature memory and supports the usage of multiple annotated frames references. We base our approach on it, as the architecture is resource-efficient, quick, extendable, and demonstrates SOTA results on modern benchmarks.
% and allows us easily extent to support multiple frame annotations \cite{AOT, DeAOT} and incorporate a permanent memory module. 

%check this ones: QDMN2022, SWEM2022, li2022recurrent


\paragraph{Frame Annotation Candidate Selection.}
% \begin{itemize}
%     \item BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames. \textbf{Only selects the first frame to annotate,s }
%     \item Guided Interactive Video Object Segmentation Using Reliability-Based Attention Maps
%     \item Learning to Recommend Frame for Interactive Video Object Segmentation in the Wild
%     \item some classic methods listed in my cvpr 2019 paper, Fast User-Guided Video Object Segmentation
% \end{itemize}

The task of annotation candidate prediction is finding a specific frame or set of frames for the user to annotate, in the first or consecutive interaction rounds that maximize the overall video segmentation quality (defined with metrics like \textit{Intersection over Union} (IoU) and \textit{F-score}). The authors of BubbleNets \cite{BubbleNets} propose a VOS architecture that simultaneously learns to predict the optimal candidate frame to annotate, by using uses a bubble-sort \cite{bubblesort} style algorithm to find the close-to-best candidate. The authors of IVOS-W \cite{IVOS-W} claim that annotating the frame with the lowest quality is suboptimal and introduce a Reinforcement Learning Agent to intelligently select the candidates based on the assessed quality of all frames. GIS-RAmap \cite{GISRAmap} introduces an end-2-end deep neural network, that operates on sparse user input (scribbles), and uses the R-attention mechanism to segment frames and directly predicts the best annotation candidates.  
% % Figure environment removed
% Figure environment removed
% \vspace{-1em}
% Figure environment removed

These works exhibit the following limitations: 
\cite{BubbleNets,IVOS-W} work under the assumption that it is possible to directly estimate the segmentation quality or frame importance without explicit information of the target object, which makes them highly domain-dependent and does not hold for partial segmentation, as illustrated by Fig. \ref{fig:broken_quality_assessement}. \cite{BubbleNets} and \cite{GISRAmap} do not use annotation information when selecting annotation candidates, which limits their usefulness for partial segmentation, occlusions, or multiple similar objects in the scene (use-case illustrated in rows 1-2 in Fig. \ref{fig:frame_selector}).

\paragraph{Video Segmentation Datasets.}
Earlier datasets had annotations for videos without heavy occlusions or appearance changes \cite{ochs2013segmentation, jain2014supervoxel, fan2015jumpcut}. DAVIS \cite{DAVIS, DAVIS2017} became the benchmark dataset for the previous decade and is still being used because of the high resolution of the videos and the quality of the annotations. The benchmark released in 2016 had only single-object annotations, and the extended one in 2017 incorporated multiple-object annotations. For both datasets, their clips are 2-4 seconds long. Youtube-VOS \cite{YOUTUBE-VOS} presented a large dataset of around 4000 videos where each is 3-6 seconds long. Given the number of videos, they have a variety of categories and every 5-th frame is annotated. OVIS \cite{OVIS} is a dataset of severe occlusions, and is very challenging for the current VOS methods. MOSE \cite{MOSE} uses a subset of OVIS \cite{OVIS} as well as other videos, for a total of 2149 clips, from 5 to 60 seconds long, 12 on average. LVOS \cite{LVOS} dataset consists of 220 long videos, on average 115 seconds each. Most recently, BURST \cite{BURST} introduced a large dataset with almost 3000 videos, that can be used for tasks like VOS and Multi-Object Tracking and Segmentation. Even though there are resources with long videos and high-quality masks, many use cases from the industry are not reflected, such as partial objects, reflections, and segmentation targets without clear boundaries. 
