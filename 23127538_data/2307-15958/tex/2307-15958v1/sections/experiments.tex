% 5 Results
    % some text describing the overall results
    % describe each example (what do we see in the scene, what are we segmenting? how many frames were used? why is it challenging and not possible before) in a setense
    % We refer to the accompanying video for 
    % highlights:
        % partial segmentation (show figure)
        % multi-region segmentation (show example)
        % robustness to occlusions, cropped frames, lighting changes (show in figures)
        % temporal coherence (refer to video)
        % -> production worthy

    % paragraph - Evaluation
        % how does the number of annotated frames affect performance?
        % how does the frame selection affect the results (with and without) what are the length of the videoss? what are the challenges in the video? how is a different manual segmentation affecting the frame prediction?
        % add quatitative evaluations 

    % paragraph - Comparison
        % we compare with 3 SOTA methods in Figure bla, and table bla shows blabbla... with VOS models        
        % qualitative and quantitative
        % discussions: how does our method perform on new vs old benchmark, and why that is so? make the point that even though other public datasets aren't optimal ours is still better, but for the examples in our dataset we are significantly better.
        
    % paragraph - Limitations
        %break our system!

        % extreme cases: 
            % highly detailed shapes
            % extreme deformations or appearance changes
            % ambiguous textures and repetive objects
            % it's in the appendix
       
        % frame selection: blurry frame, everything moves, non distiguishable scenes (if tracking was available it could help)
    
    % paragraph - Performance
        % half the length

\section{Results}
\label{sec:results}

We test our segmentation framework on a wide range of complex scenes (varying poses and deformations of human subjects, faces, rigid objects, t-shirts) and annotation tasks (object, partial, multi-class segmentation) in the presence of occlusions, lighting variations, and cropped views. In particular, we showcase rotoscoping examples that occur frequently in labor-intensive real production settings, where over 50\% of frames would need to be annotated or refined \cite{adoberotoscope2017}. Qualitative results are presented in Fig. \ref{fig:teaser}, Fig. \ref{fig:xmempp_results}, and Fig. \ref{fig:xmemppcomparison}. Our videos are recorded at 30 fps and $1920 \times 1080$ resolution, and the input is resized to $854 \times 480$ when processed by our model. We also refer to the accompanying video and supplemental material to view the results.


% Figure environment removed

In row a) of Fig. \ref{fig:xmempp_results}, we show a partial segmentation result where only the front part of the guitar body is annotated, not the side or back. Only 6 out of 924 total frames were annotated (0.6\% total) in order to successfully produce reliable segmentations through a challenging sequence with frequent occlusions, pose variations, and rapid movement.
Rows 6 in Fig. \ref{fig:xmemppcomparison} depicts an example where the goal is to composite the reflection of an object (e.g., a chair) into a different scene, which is a common use case in visual effects production. Our chair rotates throughout the video, which changes its projected outline significantly, but a consistent segmentation can be performed by only annotating 6 frames out of 411 total frames (1.5\%). This use-case is especially challenging since the scene contains a larger, more prominent object with the exact same appearance and movement. A challenging multi-class segmentation example for a long and diverse sequence is illustrated in row 3 of Fig. \ref{fig:xmemppcomparison}, where the subject's face is segmented into 3 different regions, one of which consists of two separate parts. All regions are segmented simultaneously, which prevents them from overlapping with each other which can happen when each region is segmented independently. The additional challenge in this scenario is that the regions are often not separated by prominent visual cues such as boundaries, corners, edges, etc. Highly consistent results can be extracted even in the presence of extreme lighting variations, moving background and head poses, where again only 6 out of 951 (0.6\%) total frames have been annotated. 

%  part of the face is annotated. Only X frames need to be annotated from a video with total number of X frames in order to successfully produce reliable segmentations through a challenging sequence with extreme head poses and facial expressions. Notice how the segmentation can reliably extract temporally coherent masks even when the region of interest on the face completly disappearas and reappears.
\paragraph{Evaluation.}
We evaluate our framework on a diverse set of complex videos from MOSE \cite{MOSE} dataset, as well as on long videos provided by LVOS \cite{LVOS}. We use the training subset of MOSE (since it does not provide annotation for the test subset), consisting of 1507 videos, and a validation subset of LVOS consisting of 50 videos\footnotemark. We compare the performance of multiple SSVOS models when given $1$, $5$, and $10$ uniformly-sampled annotated frames. For comparison, we picked existing works in SSVOS and IVOS, that support the usage of multiple annotation frames by design as well as support dense annotations, since MOSE and LVOS do not provide scribbles information.

The performance of \modelname{} with only one annotation given is equivalent to XMem. We see that on both datasets \modelname{} demonstrates noticeably better performance scaling in terms of $\mathcal{J}$ and $\mathcal{F}$ metrics, for 5 and 10 annotated frames provided at input. Moreover, it can be seen that \modelname{} achieves comparable or higher segmentation quality with fewer annotations provided (5) than the competition (10), thus making it on average $2\times$. Furthermore, we evaluate our model and XMem on a subset of \datasetname{} dataset and observe that on some videos the efficiency of \modelname{} is up to $5 \times$ higher. 

% Figure environment removed

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[t]
\setlength\tabcolsep{1.75pt}
\renewcommand{\arraystretch}{1.25} 
\centering
\small
\begin{tabular}{c|cc|cc|cc|cc}
\cline{2-9}
\multicolumn{1}{l}{}            & \multicolumn{6}{c|}{Number of annotations provided}   & \multicolumn{2}{c}{\multirow{2}{*}{$|\mathcal{D}_{1 \rightarrow 10}|$}}   \\  \cline{2-7}
\multicolumn{1}{l}{}            & \multicolumn{2}{c|}{1 frame}   & \multicolumn{2}{c|}{5 frames}  & \multicolumn{2}{c|}{10 frames} & & \\  \hline

\textbf{Method}                 & $\mathcal{J}$             &$ \mathcal{F}$             & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$  & $\mathcal{J}$ & $\mathcal{F}$ \\  \hline

$\text{TBD}_{\textit{DAVIS}}$   & 42.72                     & \textbf{53.38}            & 52.17         & 63.64         & 60.04         & 71.65          & \good{+17.3}         & \good{+18.3}         \\
$\text{TBD}_{\textit{YT}}$      & 41.15                     & 50.79                     & 57.19         & 67.83         & 65.07         & 75.72          & \good{+23.9}         & \good{+24.9}         \\
XMem                            & \multirow{2}{*}{\textbf{44.06}} & \multirow{2}{*}{52.33}& 56.30       & 66.05         & 62.62         & 73.18          & \good{+18.5}         & \good{+20.9}        \\
\modelname                      &                           &                           & \textbf{67.36}& \textbf{78.11}& \textbf{75.74}& \textbf{86.35} & \textbf{\good{+31.7}}& \textbf{\good{+34.0}} \\ \hline
\end{tabular}

\caption{Quantitative results on LVOS \cite{LVOS} validation dataset. $\mathcal{J}$ and $\mathcal{F}$ mean Jaccard index and boundary F-score correspondingly, as defined in \cite{DAVIS}. $\text{TBD}_{\textit{DAVIS}}$ and $\text{TBD}_{\textit{YT}}$ stand for TBD \cite{TBD2022} model trained on DAVIS \cite{DAVIS} and YouTube-VOS \cite{YOUTUBE-VOS} datasets accordingly. \textit{At $k=5$ annotation frames \modelname{} achieves higher quality ($\mathcal{J}$ and $\mathcal{F}$) then all other models at $k=10$ frames.} $|\mathcal{D}_{1 \rightarrow 10}|$ denotes the increase in segmentation quality from $1$ to $10$ annotated frames. }
\label{tab:LVOS_scaling}
\end{table}

\footnotetext{One of the videos provided does not have any target objects on frame \#0, which an unsupported use-case for some of the models used, so only 49 out of 50 videos were included in the evaluation. }

% Figure environment removed

\begin{table}[h]
\setlength\tabcolsep{1.75pt}
\renewcommand{\arraystretch}{1.25} 
\centering
\small
\begin{tabular}{c|cc|cc|cc|cc}
\cline{2-9}
\multicolumn{1}{l}{}            & \multicolumn{6}{c|}{Number of annotations provided}   & \multicolumn{2}{c}{\multirow{2}{*}{$|\mathcal{D}_{1 \rightarrow 10}|$}}   \\  \cline{2-7}
\multicolumn{1}{l}{}            & \multicolumn{2}{c|}{1 frame}   & \multicolumn{2}{c|}{5 frames}  & \multicolumn{2}{c|}{10 frames} & & \\  \hline

\textbf{Method}                 & $\mathcal{J}$             &$ \mathcal{F}$             & $\mathcal{J}$ & $\mathcal{F}$ & $\mathcal{J}$ & $\mathcal{F}$  & $\mathcal{J}$ & $\mathcal{F}$ \\  \hline

$\text{TBD}_{\textit{DAVIS}}$   & 43.34                     & 49.70            & 56.57         & 63.00         & 63.15         & 69.15          & \good{+19.8}         & \good{+19.5}          \\
$\text{TBD}_{\textit{YT}}$      & 48.28                     & 54.05                     & 62.71         & 68.94         & 68.24         & 74.23          & \good{+20.0}         & \good{+20.2}          \\
STCN                            & 54.51 &                   60.69                       & 58.73         & 65.86         & 62.78         & 70.11          & \good{+8.3}         & \good{+9.4}          \\
XMem                            & \multirow{2}{*}{\textbf{57.21}} & \multirow{2}{*}{\textbf{63.98}}& 67.95 & 76.41      & 77.78         & 85.26          & \good{+20.6}         & \good{+21.3}          \\
\modelname                      &                           &                           & \textbf{77.11}& \textbf{84.56}& \textbf{82.87}& \textbf{90.20} & \textbf{\good{+27.7}}& \textbf{\good{+26.5}} \\ \hline
\end{tabular}
\caption{Quantitative results on MOSE \cite{MOSE} training dataset. $\mathcal{J}$ and $\mathcal{F}$ mean Jaccard index and boundary F-score correspondingly, as defined in \cite{DAVIS}. $\text{TBD}_{\textit{DAVIS}}$ and $\text{TBD}_{\textit{YT}}$ stand for TBD \cite{TBD2022} model trained on DAVIS \cite{DAVIS} and YouTube-VOS \cite{YOUTUBE-VOS} datasets accordingly. Since the training subset of MOSE dataset includes some very short videos, we only considered videos with $>= 50$ frames each for comparison. Results from a total of $722$ videos are presented. $|\mathcal{D}_{1 \rightarrow 10}|$ denotes the increase in segmentation quality from $1$ to $10$ annotated frames.}
\label{tab:MOSE_scaling}
\end{table}

\begin{table}[h]
\setlength\tabcolsep{1.75pt}
\renewcommand{\arraystretch}{1.25} 
\centering
\small
\begin{tabular}{c|cc|cc|cc|c}
\cline{2-8}
% \multicolumn{1}{l}{}            & \multicolumn{6}{c|}{Number of annotations provided}   & \multicolumn{2}{c}{\multirow{2}{*}{Efficiency}}   \\  \cline{2-7}
\multicolumn{1}{l}{}            & \multicolumn{2}{c|}{XMem}   & \multicolumn{2}{c|}{\modelname{}}  & \multicolumn{2}{c|}{\# Annotations} & \multirow{2}{*}{$\mathbb{E}$} \\  \cline{2-7}

\textbf{Sequence}                 & $\mathcal{J}$             &$ \mathcal{F}$             & $\mathcal{J}$ & $\mathcal{F}$ & XMem & \modelname{}  \\  \hline

Vlog 3-part     & 85.43          & 89.70  & \textbf{85.45} & \textbf{89.89} & 45             & 10         & $4.5 \times$             \\
Lips            & \textbf{87.24} & \textbf{94.79} & 86.93          & 94.43          & 45         & 10          & $4.5 \times$                  \\
Half face       & 93.26          & 98.13          & \textbf{93.31} & \textbf{98.35} & 50         & 10          & $5 \times$             \\  \hline
% \modelname                      &                           &                           & \textbf{67.36}& \textbf{78.11}& \textbf{75.74}& \textbf{86.35} & \textbf{\good{+31.7}}& \textbf{\good{+34.0}} \\
\end{tabular}

\caption{Quantitative results on a subset of \datasetname{} dataset. $\mathcal{J}$ and $\mathcal{F}$ mean Jaccard index and boundary F-score correspondingly, as defined in \cite{DAVIS}. $\mathbb{E}$ is the frame usage efficiency of \modelname{} compared to XMem.}
\label{tab:pumavos_scaling}
\end{table}
The behavior of our annotation candidate selection module is demonstrated in Fig. \ref{fig:frame_selector}. Often there is more than one possible target in the video, so selecting frames for the right one is important. A video is presented in rows 1 and 2 of Fig. \ref{fig:frame_selector}, where two people change their head pose, but one at a time. Our algorithm successfully adapts the recommended frames based on which person is being segmented. Rows 3 and 4 depict a more complicated video sequence, where the target object has extreme lighting variations, rapid movement, disappearances, pose and expression variations (whenever applicable). The algorithm selects frames that capture these varieties without repeating or selecting frames with no target object. Our experiments indicate that the selected annotation candidates are very similar to those chosen by expert users, which makes it practical in real-life scenarios where an end user is likely to work with multiple videos at the same time, thousands of frames long, and rewatching them for multiple rounds to select the frames manually is often infeasible. 


% \todo{START READING FROM HERE}

% \label{sec:FACS_results}
% We evaluate both \todo{Algorithm 1} on MOSE and LVOS datasets. Since MOSE contains some very short videos ($< 10$ frames long), we only select the top-100 longest ones (where number of frames $N$ lies within the range $[232, 500]$) for quantitative comparison (since our frame selector performance on short videos is not representative of real-life use-cases). We also choose a simple baseline approach, that samples $k$ frame annotation candidates $F_{A}$ uniformly throughout the video, shown in equation \ref{eq:uniform_baseline} 

% \begin{equation}
% \label{eq:uniform_baseline}
%     F_A = \lfloor linspace(0, N - 1, k) \rfloor
% \end{equation}


% % % Figure environment removed

% % Figure environment removed

% Fig. \ref{fig:facs_kde_regular} shows that both algorithms tend to select different frames from all parts of the video, however, the resulting segmentation quality (Fig. \ref{fig:lvos_mose_box_plots}) is very similar for both methods, which only considers frames from the same hardcoded parts of every video, thus missing a lot. This analysis suggests that the videos in these datasets are "balanced" due to their nature, without any "dominating" scenes, thus a \textbf{uniformly sampled set of frames is sufficient to provide adequate scene representation}. To demonstrate the usefulness of the "maximum representation" approach, we modified existing LVOS dataset, by taking the first 100 frames of each video, looping them and then repeating the resulting 200 frames \textbf{8} times, and then adding the first 200 frames at the end \todo{maybe adding noise so that the frames are not exactly the same}, thus obtaining LVOS-"looped" dataset. The resulting videos have $1000$ frames each, with the frames arranged as follows: 
% \[
% \left\{ \{1, 2, ..., 99, 100, 99, ..., 2, 1 | \times{\mathbf{8}}\}, 1, 2, ..., 199, 200 \right\}
% \]

% This simulates having a "dominating" scene, which has limited object appearance variety but occupies most of the video duration (the first $90\%$ in this case), while still preserving temporal smoothness. We also evaluate the algorithms on \todo{three} of the videos from our collected dataset, which present a challenge in the form of partial segmentation as well as naturally occurring "dominating" scenes. 

% % Figure environment removed

% % % Figure environment removed


% % Figure environment removed

% \todo{Qualitative result figure}

% In this use-case it can be observed that our algorithm tends to pick frame annotation candidates from \textbf{both} parts of the artificially enlarged video (the first $90\%$ and the last $10\%$), shown in Fig. \ref{fig:facs_kde_looped}. The average quality for the whole video is still similar (Fig \todo{ref}), however, the quality of the second half is noticeable better for frames chosen by our algorithm \todo{ref}.




\paragraph{Comparison.}
We compare our results with 3 SOTA methods in Table \ref{tab:MOSE_scaling}, and with 2 in Table \ref{tab:LVOS_scaling}, and Fig. \ref{fig:xmemppcomparison}. We demonstrate that our model produces smooth and temporally continuous segmentation in rows 3-6 of Fig. \ref{fig:xmemppcomparison}, where both other methods produce incorrect masks missing a part of the target object. In rows 1-3 of Fig. \ref{fig:xmemppcomparison} our method successfully segments challenging multi-part regions of the face, that are mostly not visually aligned with the low-level image cues, and the masks produced by TBD and XMem are ``bleeding" into the neighbouring regions, as well as have sharp, ``torn"-looking edges. In Tables \ref{tab:MOSE_scaling} and \ref{tab:LVOS_scaling} we demonstrate that our method results in higher segmentation quality given the same frame annotations, and is at least $2 \times$ as efficient in the number of annotations necessary on generic videos. We additionally show in Table \ref{tab:pumavos_scaling} that in more challenging practical sequences \modelname{} can be up to $5 \times$ more efficient.
    % paragraph - Comparison
        % we compare with 3 SOTA methods in Figure bla, and table bla shows blabbla... with VOS models        
        % qualitative and quantitative
        % discussions: how does our method perform on new vs old benchmark, and why that is so? make the point that even though other public datasets aren't optimal ours is still better, but for the examples in our dataset we are significantly better.
        

\paragraph{Limitations.}
% \todo{APPENDIX: Speed and memory usage; IF have time - show that basically == XMem, realtime for 480p videos, extra memory usage is static, practical for long-term video segmentation.}
% \todo{Describe in 1-2 paragraphs the problem, "example frames are in the Appendix"}

The segmentation quality of our method sometimes suffers on blurry frames with quickly moving objects, and the similarity measure for the annotation candidate selection is not well-defined on such data either. With multiple similar/identical objects in the frame, the method can sometimes switch to the wrong target if they occlude each other. Use cases of extreme deformation (clothing) and high-detail objects (hair) remain an active challenge. Visual illustrations are provided in the Appendix.

\paragraph{Performance.}

In the original XMem, given $n$ frames, the processing time is bound by $O \left( \frac{n^2k}{z} + \frac{n}{q} \right)$, where $k$ is the maximum size the working memory (typically $k=100$), $q$ is the memory insertion frequency (typically $q=5$), and $z$ is a compression rate for long-term memory ($z > 600$) \cite{cheng2022xmem}. 
%The scaling law is ultimately quadratic, but for short- and medium-length videos ($n < 2000$), the $z$ is large enough (up to $600 \times$) to ignore the infinite long-term memory growth, thus the inference speed is not affected in a significant way. 

Given $m$ annotated frames, \modelname{} loads them into the permanent memory (static $+m$ factor), with the working memory size $=k+m$, thus processing time is $O\left( \frac{n^2(k+m)}{z} + \frac{n}{q} + m \right)$. In practice $m$ is likely to be small, $m \leq 20$, thus $m \leq 0.2 k$, having a slowdown of $<1.2\times$ on memory readout, and an even smaller effect on the overall segmentation process. On RTX-3090 GPU with a $500$-frame video and $5$ annotations provided, at $854 \times 480$ resolution, \modelname{} yields 32 FPS ({35} FPS excluding loading the frames into permanent memory), and XMem yields {39} FPS. Total memory usage only increases by a static factor of $+m$, as we store $m$ additional annotations.

