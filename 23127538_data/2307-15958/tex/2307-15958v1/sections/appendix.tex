\appendix
\section{\datasetname{} Dataset}
\datasetname{} is a dataset that covers visually challenging segmentation scenarios, including, but not limited to high appearance variability due to changes in lighting, viewing angles, deformation, and scale changes; partial segmentation (where only a part of the object is being segmented), often with limited to no low-level image cues (e.g. half of a person's face) and occlusion. It consists of 23 videos, of 18 to 35 seconds long, recorded in 1080p resolution with different aspect ratios (both vertical and horizontal). 

Examples of the videos from the dataset are illustrated in Fig. \ref{fig:dataset_overview}. Sequences ``Half Face", ``Guitar" and ``Dog Tail" depict challenging partial segmentation use-case where only the left part of a person's face, the dog's tail or just the body of the guitar body is segmented, without following any image cues such as edges or corners. ``Royal car" and ``Full Face" contain changes in scale and heavy occlusion throughout the sequence, while ``Pimples" and ``Turkish Ice Cream" both contain very small objects that also move a lot throughout the scene. ``Pimples" and ``Cap" are also examples of ambiguity - there are multiple objects with similar appearances present in the video, but only some of them are supposed to be segmented. ``Vlog" is one of the most challenging sequences in the dataset, as it not only contains partial segmentation with arbitrarily-defined boundaries (irrespective of the low-level image cues) but also has multiple target regions to segment, which are located on the same physical object, as well as having a lot of variability in lighting, static and dynamic background scenes, and frequent deformations, which are also present in ``Shirt" and ``Tattoo" sequences. 

\textit{\datasetname{} and the source code of \modelname{} and the annotation algorithm are going to be released to the public after the paper is accepted. We also provide pseudocode for our frame selection algorithm.}

% Figure environment removed

% Summarize what's in the main paper (stats)
% Describe use-cases (what does this dataset contain that others don't)
% Show a few examples, describe why is it difficult (because long sequences, static and dynamic background, deformations, complex occlusions, ambiguity), refer to Figure 1.
% "Motion blur", "Partial segmentation", "..."
% Mention the length of the videos, different poses, angles, ambiguity, highly-deformable, selecting just the front (guitar)

\section{Frame Selection User Study}
A user study was performed to analyze the effectiveness of the automatic frame selection module. A total of seven participants were selected - two experienced and two novice users. The users were given three videos each and asked to select 5 most representative frames (besides frame \#0) that capture the variation in the appearance of the target object. The videos chosen from \datasetname{} are: sequences ``Half Face" (shortened), ``Turkish Ice Cream", and ``Vlog". Videos last from 22s (673 frames) to 32s (960 frames). 

The experienced users were explained how \modelname{} works internally in detail, as well as shown its predictions on a few sample videos, while novice users treated it as a blackbox. The participants were shown the full video and the annotation of the target object(s) for the first frame and were free to rewatch it as many times as they deemed necessary, then asked to pick the frames. They were told that the relative position of the frame does not matter, only its content. We recorded the time it took the users to select the frames (excluding the initial video viewing time) and compared it to the running time of the frame selection algorithm. We then took the chosen annotations and performed a quantitative comparison of speed and final segmentation accuracy using \modelname{}, presented in Table \ref{tab:user_study}. 

\begin{table}[ht]
\centering
\begin{tabular}{l|ccc}
\hline
\textbf{Group/Method} & \textbf{IoU} & \textbf{F-score} & Average time, s \\
\hline
Algorithm & 0.777 & 0.828 & \textbf{1.7} \\
Experts & \textbf{0.805} & \textbf{0.855} & 47.1 \\
Non-experts & 0.779 & 0.825 & 54.8 \\
\hline
\end{tabular}
\caption{Comparison of performance metrics across expert and non-expert with the frame annotation candidate selection algorithm.}
\label{tab:user_study}
\end{table}

Our algorithm is $27 \times$ faster than the expert users, and $32 \times$ faster than non-expert ones, while providing comparable performance to non-expert users' results. This makes it a practical tool for large-scale environments where it is infeasible to have a lot of trained experts to work on videos, while also having a practical application for expert users, who can use the algorithm to very quickly obtain a lot of potentially valuable annotation candidates and then select the best ones with their expertise, saving a lot of time in the process.
% 
\section{Additional Results}
Please refer to the accompanying video to see all the video results and method comparisons. We highlight a number of highly complex scenes, where existing methods are challenged with varying scales, appearance, occlusions, and ambiguous objects. For example, in Fig. \ref{fig:good_results} row 1 the subject's face is segmented (without ears or hair), while they are going through a variety of poses, rotate around, occlude the target region, and move both closer to and farther from the camera. 

Our method successfully segments the target across multiple scales and poses, resulting in a smooth, temporally coherent, and accurate segmentation. Rows 4 and 5 depict scenes with similar challenges - a multitude of objects present, that have a similar appearance, frequently occlude each other and move, both around the scene and with their individual body parts. In both cases, our method successfully segments all of the targets, without confusing them, ``bleeding" the mask into neighboring objects, or merging multiple targets into one. 

Moreover, in the last picture of row 5, Fig. \ref{fig:good_results} it can be observed that the flower the person marked with the blue mask is holding was correctly not segmented, since in the provided annotations, it is not included, as not being a part of the target object. This illustrates that \modelname{} can work correctly with a large number of targets in the scene while preserving a high level of detail about their appearance.

\paragraph{Comparisons.}
We provide additional comparisons between our method and the current SOTA interactive segmentation model XMem \cite{cheng2022xmem}. XMem is a resource-efficient and fast memory-based segmentation method introduced in 2022 by Ho Kei Cheng and Alexander G. Schwing. Additional comparison results are provided in Fig. \ref{fig:comparison} and Fig. \ref{fig:interpolation}. For each video 6 frames were selected for annotation by uniformly sampling the video (refer to Eq. \ref{eq:uniform_baseline}), starting from frame 0. Row 1 shows the same video as in ``Full Face" sequence from \datasetname{}, but now with only half of the face being segmented, thus providing the same challenges as discussed earlier, but with even more difficult segmentation. Row 2 is a ``guitar" example from \datasetname{}, where only the frontal part of the guitar's body is the target. In both cases we see \modelname{} resulting in a noticeably better segmentation, in particular with the ``Half face" sequence, where it manages to produce the correct segmentation mask throughout the extreme variations in pose, expression, and scale, while XMem often ``bleeds" the mask into neighboring regions, sticking more to the visual cues of the object. The results for the ``Guitar" sequence demonstrate a similar outcome - \modelname{} correctly segmenting the front of the guitar, but not the sides, while XMem segments the whole object, again overfitting to visual cues instead of correct segmentation boundaries.

% Figure environment removed

% Figure environment removed

% it can handle movement,s scale variations and deformatinos with just 5  frames
% In sequence ..., XXX is happening. 
% Compare with original XMem. 
% 
%
\paragraph{Limitations.}
Some examples are challenging even for our method. For example, while some motion blur is fine, with extreme motion blur it can't, which is a common challenge for all existing methods. Furthermore, memory-based models generally struggle when provided ``negative masks" - empty annotations where the target object is not present, but the model has a false-positive segmentation prediction, illustrated in Fig. \ref{fig:limitations}

Our frame selection algorithm does not work well when there is too much dynamics in the scene, with a lot of objects moving chaotically at the same time. Equivalently, if there is too little movement, no clear scene boundaries, or very little variation in the target object's appearance. In both of these cases, the importance of selecting the right candidates for annotations is significantly reduced, as most of the frames result in a similar accuracy improvement. In this case, our algorithm wouldn't necessarily perform better than randomly/uniformly selected frames. To prove this, we evaluate our frame selection algorithm with \modelname{}, and a uniform baseline, calculated by the formula in the Eq. \ref{eq:uniform_baseline} on LVOS dataset \cite{LVOS}, in which the videos typically have one of the aforementioned traits, and we show that the performance of the uniform baseline and our algorithm is very similar (Fig. \ref{fig:lvos_candidates}). 

% Figure environment removed

\begin{equation}
\label{eq:uniform_baseline}
    F_A = \lfloor linspace(0, N - 1, k) \rfloor
\end{equation}

% Figure environment removed

Given a video with $N$ total frames, we select $k$ candidates for both uniform and our frame annotation candidate selection algorithm. We then run inference on LVOS validation set with 49 videos, described in the Results section in the main paper, and illustrate the distribution of both F-score ($\mathcal{F}$) and Intersection-over-Union ($\mathcal{J}$) metrics in Fig. \ref{fig:lvos_candidates}.

\section{Additional Evaluation}
We analyze the performance increase of XMem and \modelname{} with the number of annotations available, by providing both qualitative (Fig. \ref{fig:1_5_10}) and quantitative results (Fig. \ref{fig:scaling}). In Rows 1-3 of the ``Caps" sequence in Fig. \ref{fig:1_5_10}, we see that providing just 5 frames is enough to completely resolve the ambiguity problem when segmenting one of the two identical caps in the frame. Providing 10 annotated frames further improves segmentation quality in challenging scenes, such as in Columns 4-5, where there is a lot of motion blur on the target object, as well as lighting variation. \modelname{} demonstrates similar results for ``Royal car" sequence where just 5 provided annotated frames drastically improve the quality in the scenes with extreme occlusion, where the car is hardly visible behind lots of people (Columns 2-4).

We furthermore evaluate our method's quality scaling performance on LVOS validation dataset, from 1 to 10 annotated frames provided, illustrated in Fig. \ref{fig:scaling}. As expected, given only 1 frame both models yield equivalent results, however \modelname{} (drawn with orange line) demonstrates significantly higher scalability potential and efficiency starting at 2 annotated frames and keeps the advantage throughout the whole comparison, up to \textbf+{13\%} difference at 10 frames ($0.63$ XMem vs $0.76$ \modelname{})

% Figure environment removed

\paragraph{In-Memory Augmentations.}
We address a possible use-case where annotations could be very sparsely available or too expensive to produce, by exploring in-memory augmentations for provided annotated frames. Through rigorous testing, we select the \textbf{11} best augmentations that, when combined, lead to the highest possible segmentation quality improvement for \modelname{}, shown in Fig. \ref{tab:augs}. When processing and adding provided frames and their annotations to the permanent memory, each of the augmentations is applied to every frame (and their corresponding mask, where necessary) and stored in the permanent memory as well. This can be a practical way of increasing the segmentation accuracy without any extra work done by the end-user, at the cost of higher memory usage and potentially slower inference speed.

\begin{table}[ht]
\centering
\begin{tabular}{l|c|c}
\hline
\textbf{Increase brightness} & 0.721 & +0.009 \\ \hline
\textbf{Decrease brightness} & 0.725 & +0.013 \\ \hline
Grayscale & 0.707 & -0.005 \\ \hline
\textbf{Reduce bits to $3$} & 0.717 & +0.005 \\ \hline
\textbf{Make sharp} & 0.718 & +0.006 \\ \hline
\textbf{Gaussian blur} & 0.731 & +0.019 \\ \hline
\textbf{Rotate right $45\deg$}\dag & 0.723 & +0.011 \\ \hline
Translate right +100 px & 0.675 & -0.037 \\ \hline
\textbf{Zoom out $0.5 \times$} & 0.715 & +0.003 \\ \hline
\textbf{Zoom in $1.5 \times$} & 0.727 & +0.015 \\ \hline
\textbf{Shear right by $20$}\dag & 0.730 & +0.018 \\ \hline
Crop mask region & 0.704 & -0.008 \\ \hline
\end{tabular}
\caption{In-memory augmentations in their individual effect on the overall segmentation quality on LVOS dataset. Only transformations named in \textbf{bold} were considered for experiments. For transformation with \dag the equivalent symmetrical transform was used as well. A total of 11 augmentations were used for the experiments in Fig. \ref{fig:scaling}.}
\label{tab:augs}
\end{table}

\paragraph{Utility of Permanent Memory.}
We further demonstrate the capabilities of our introduced permanent memory module by disabling updates to the temporary memory in \modelname{}, effectively keeping it empty and frozen throughout the inference. We observe that for LVOS dataset (Fig. \ref{fig:scaling}) this results in an \textit{increase} in the overall segmentation quality, both with (red) and without (purple) using in-memory augmentations. This shows that for certain types of videos, especially when predicted masks are prone to have errors, the best strategy is to not use them at all, and very few high-quality references in the memory result in higher segmentation quality than dozens, potentially hundreds, but containing errors. Temporary memory plays has an important role in \modelname{} architecture, allowing it to adapt better to changes in the target appearance, but through our experiments we show that for some videos it can be safely disabled (for example, if the target object's appearance stays relatively consistent throughout the video), leading to higher inference speed and lower memory footprint.
% 

% Figure environment removed

% % Figure environment removed

% Figure environment removed

% % Figure environment removed

\clearpage
% \section{Annotation Candidate Selection Algorithm}
\label{sec:alg}


\newcommand{\keys}{\mathbf{K}}
\newcommand{\masks}{\mathbf{M}}
\newcommand{\prevcand}{\mathbf{PC}}
\newcommand{\composite}{\mathbb{K}}
\newcommand{\compositeKey}{\hat{k}}
\newcommand{\norm}[1]{\lvert #1 \rvert}
\newcommand{\chosen}{\mathbf{CC}}
\newcommand{\chosenKeys}{\mathbf{C\hat{K}}}
\newcommand{\dis}{\neg\mathbf{S}}
\newcommand{\disMin}{\neg S_{min}}
\newcommand{\disMem}{\neg \mathbf{S}_{\keys}}
%

\begin{algorithm*}[!htb]
Here is the pseudo-code for the annotation candidate selection algorithm. The $\odot$ is a pointwise multiplication operation. Symbol $[\:]$ denotes an empty list, and symbols $\mathbf{S}$ and $\dis$ are used to denote similarity and dissimilarity correspondingly (the negation symbol $\neg$ is used as a visual cue) 
    \caption{select-next-candidates($\keys$, $\masks$, k, $\prevcand$, $\alpha=0.5$, $\beta=9$)}
    \begin{algorithmic}[1]
    \Require $\keys$: list of ``key" feature maps for all frames of the video
    \Require $\masks$: list of masks for each frame (predicted or user-provided)
    \Require $k$: number of candidate frames to select
    \Require $\prevcand$: list of previously chosen candidate indices (default is [0])
    \Require $\alpha$: weight of mask regions (default is 0.5), $\alpha \in [0..1]$
    \Require $\beta$: minimum number of pixels for a valid mask, to explicitly filter out frames without the target object or where it is too small (default is 9px)
    
    \Ensure $\prevcand$ not empty, $0 \leq \alpha \leq 1.0$, $k > 0$
    
    \Function{select-next-candidates}{$\keys$, $\masks$, $k$, $\prevcand$, $\alpha$, $\beta$}
    \State $\composite \gets [\:],$  $N \gets$ $\norm{\keys} $\Comment{Composite keys, Number of frames}

    \For{$i$ in $[0, N-1]$}
        \State $\compositeKey \gets \keys[i] \odot \masks[i] \cdot \alpha + \keys[i] \cdot (1 - \alpha)$ \Comment{$\compositeKey$ is a ``composite" key}
        \State \Comment{Equivalent to alpha-blending operation}
        \State $\composite$.add\_to\_end($\compositeKey$) 
    \EndFor
    
    \State $\chosen \gets \prevcand$ \Comment{Chosen candidates, initialize with previous candidates}
    \State $\chosenKeys \gets \left[\composite[i] \mid i \in \prevcand \right]$ \Comment{Chosen candidates composite keys}
    
    \For{$i$ in [0, $k$]}
        \State $\dis \gets [\:]$ \Comment{Dissimilarities between candidates and other frames}
        \For{$j$ in [0, $N$]}
            \If{$\norm{\masks[i] > 0} < \beta$}
                \Comment{Mask empty or too small, ignore}
                \State $\disMin \gets 0$ \Comment{Minimum dissimilarity of frame $i$ to all in $\chosen$}
            \Else
                \State $\disMem \gets [\:]$ \Comment{Dissimilarities of $i \rightarrow j, \forall j \in \chosen$}
                \For{$j$ in $\chosen$}
                    \State $S_{j \rightarrow i} \gets$ similarity($\chosenKeys[j]$, $\composite[i]$)
                    \State $S_{i \rightarrow j} \gets$ similarity($\composite[i]$, $\chosenKeys[j]$)
    
                    % \Comment{Mapping of pixels $A \rightarrow B$ would be very similar to $B \rightarrow A$ if the images are similar and very different if the images are different}
                    \State $\dis_{cycle} \gets (S_{j \rightarrow i} - S_{i \rightarrow j})$ \Comment{Pixel-wise cycle dissimilarity}
                    \State $\neg S_{cycle} \gets \frac{\sum{\max(0, \dis_{cycle})}}{\norm{\dis_{cycle}}}$ \Comment{Only non-negative mappings}
                    \State $\disMem$.add\_to\_end($\neg S_{cycle}$) 
                \EndFor
                \State $\disMin \gets \min(\disMem)$
            \EndIf
            \State $\dis$ .add\_to\_end($\disMin$)
        \EndFor
        \State $c \gets \text{argmax}(\dis)$ \Comment{New selected candidate}
        \State $\chosen$.add\_to\_end($c$)
        \State $\chosenKeys$.add\_to\_end($\composite[c]$)
    \EndFor
    \State \Return $\left[\chosen[i] \mid i \geq \norm{\prevcand}\right]$ \Comment{Return new candidates, from index $\norm{\prevcand}$}
    \EndFunction
    \end{algorithmic}
\label{alg:alg_candidates}
\end{algorithm*}