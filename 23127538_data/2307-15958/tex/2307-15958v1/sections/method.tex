\section{\modelname{}}
\subsection{Overview}
%XMem \cite{cheng2022xmem} is an interactive video object segmentation model released by Ho Kei Cheng and Alexander Schwing in 2022 (published at ECCV). It is a memory-based segmentation model, a direct successor to MiVOS \cite{MIVOS}. 

Fig. \ref{fig:xxxmem_flow} provides an overview of XMem++. Given a video, the end user first selects a frame $f_i$ they want to annotate, to give the model the template of the object(s) that will be segmented. The user then provides the annotations in the form of scribbles (that are then converted to a dense segmentation mask $m_i$) or the mask $m_i$ directly. The segmentation model (Section \ref{sec:permanent_memory}) then processes $m_i$, puts it into the permanent memory and segments the given target(s) in all other frames by predicting segmentation masks $\hat{\mathbf{m}}$. The annotation candidate selection algorithm (Section \ref{sec:frame_selection}) takes $m_i$ and $\hat{\mathbf{m}}$ and predicts the $k$ next best frames to annotate, in order of importance. The user then provides the annotations for some or all of them, and the process is repeated until the segmentation quality is satisfactory. The annotation candidate selection module takes into account all of the previously annotated frames, so it avoids selecting the frames that are similar to those already annotated.

The segmentation module is described in Fig. \ref{fig:xxxmem_architecture}. It is based on XMem architecture \cite{cheng2022xmem} and consists of a convolutional neural network with multiple parts and three types of separate memory modules. Given a sequence of frames $\mathbf{f}$ and at least one segmentation mask $m_i$ containing the target object(s), the mask is processed together with the corresponding frame $f_i$ by the model and stored in the permanent working memory as a reference for segmenting other frames. For every frame in the memory, two feature maps are extracted and saved - a smaller ``key", containing information about the whole frame, used for matching similar frames, and a corresponding larger ``value" with target-specific information, used in predicting the segmentation mask. When predicting the segmentation for a frame $f_j$, the model searches for similar frames in all three memory modules by calculating pixel-wise attention across stored ``keys" using a scaled $L_2$ similarity measure, aggregates the information from them and uses it to predict the segmentation for the frame $f_j$. The model also stores its own predictions in the temporary working memory modules and uses them together, usually every $n$-th frame. The long-term memory module was introduced in XMem. It limits memory usage and allows processing longer videos by frequently compressing and removing outdated features from temporary working memory. Sensory memory is a small module that captures motion information by processing differences between consecutive frames, also unchanged from \cite{cheng2022xmem}.   
% XMem \cite{cheng2022xmem} is the latest video object segmcentation model based on the memory-based approach \cite{STM}, a direct successor to MiVOS \cite{MIVOS} and STCN \cite{STCN}. 
% Besides achieving state-of-the-art results on existing datasets such as DAVIS \cite{DAVIS} and YouTube-VOS \cite{YOUTUBE-VOS}, it is also memory-efficient and performs well on longer videos (1+ minutes), while having close-to-real-time speed (20$+$ FPS on $480\times 854$ videos).

% The main part of memory-based models like XMem is the \textit{memory mechanism}. In a typical architecture, a backbone model extracts feature maps from the video frames in succession, then a decoder predicts the segmentation mask for each. There are two main operations involving memory in this process: \textit{adding entries to the memory} and \textit{memory readout}. 

% During the former, two feature maps are created - a \textit{key} (target-agnostic) and a \textit{value} (target-specific). The \textit{key} incorporates information about the whole frame, and is then used to match against other \textit{keys} during \textit{memory readout} operation, computing the similarity measure, typically in the form of pixel-wise dot-product attention. The \textit{value} is a target-aware feature map, containing information about the target object and a corresponding segmentation mask (either given by the user as annotation or predicted by the model). For a given frame, the \textit{keys} are used to find \textit{similar looking frames} from those stored in the memory, and aggregate all their corresponding \textit{values} with corresponding attention weights.
% \textit{Values} are only computed when adding them to the memory, typically every $k$ frames. Common values of $k$ are 5 and 10. \textit{This algorithm is well-illustrated in the original memory-based model, STM \cite{STM}.}

% The main contribution of XMem is the introduction of a \textit{long-term memory} in addition to regular working memory. This allows the model efficiently compress ($> 6000\%$) \cite{cheng2022xmem} unimportant or duplicate frames, freeing the working memory and keeping overall memory usage low, allowing to process videos of 1000-s frames in length.


% % Figure environment removed

% Figure environment removed
\subsection{SSVOS with Permanent Memory Module}
\label{sec:permanent_memory}
% In \modelname{}, we address the following two challenges in SSVOS and IVOS: 

% \begin{itemize}
%     \item Improving scaling of performance with the number of annotated frames provided (or, equivalently, increasing the efficiency of the segmentation).
%     \item Increasing temporal coherence in difficult scenes due to target ambiguity and inefficient usage of extra annotations available (Fig. \ref{fig:frontal}).
% \end{itemize}

Our main contribution is the introduction of a ``permanent" working memory module, that changes how the annotated frames are treated before and during model inference. In the original work, there was only temporary working memory, so the first frame was always annotated and permanently kept there for predicting segmentation masks for new frames. We define this frame as a ``ground-truth reference", meaning that the segmentation mask associated with it is 100\% accurate because it is provided by the user. All the other frames, which would be added to the working memory, were only added there temporarily and are likely to be moved to long-term memory eventually, especially for longer videos. We define these frames as ``imperfect references", as the associated segmentation mask is predicted by the model, thus it is likely to have errors. This approach works great with only 1 annotation provided, however, two problems arise when using multiple annotations. First, visible ``jumps" in quality appear when the model encounters a new annotated frame during the segmentation process, and corrects its predictions for the \textit{following} frames, but not for the \textit{previous} ones. Second, additional annotated frames were treated like ``imperfect references" - thus only having an impact on a limited part of the video, and likely to be compressed and moved to long-term memory, reducing their impact even further. 
% \begin{itemize}
%     \item Lack of temporal coherence results in ``jumps" in quality along the temporal axis, illustrated in Fig. \todo{Figure?} \ref{fig:frontal}.This happens when the model encounters a new annotated frame in the video, adds it to the working memory and immediately corrects its following predictions. It becomes available as a ``perfect reference" for the consecutive frames, but \textit{not for the previous ones}, which leads to a perceivable jump in segmentation quality due to high temporal correlation among neighbouring frames.
%     \item In XMem additional frames with annotations (besides frame \#0) are still treated as "imperfect references", thus only having an impact on a limited part of the video (the frames \textit{after} the annotation, but not \textit{before}), after which they are likely to be moved to long-term memory. However, given that videos are temporally smooth in both forward and backward directions, we argue that such frames should be treated the same way as frame \#0.
%     % - making them available as references for all the frames in the video, not just a subset.
% \end{itemize}

To address these issues, we propose to add another, ``permanent" working memory module (labeled dark green in Fig.~\ref{fig:xxxmem_architecture}), implemented to store only ``ground-truth references" - i.e., annotated frames for the duration of the whole video. During inference, the annotated frames are processed separately and added to the new permanent working memory before the segmentation process starts. The ``permanent" working memory module stays unchanged throughout the whole inference, its contents are never compressed or moved to the long-term memory. During memory readout, the keys and values in the permanent memory are simply concatenated to those of the temporary memory.

This allows the model to produce smooth transitions when the target object changes between two scenes, as the model now has access to references from both (refer to Fig. \ref{fig:interpolation} from Appendix for illustration). The module also decouples the frame position from frame content, helping the model to find matches across frames regardless of their positions in the video. This results in an increased scaling of segmentation quality and efficiency, as well as fixes the ``jumping" quality issue (Section \ref{sec:results}).

\subsection{Attention-Based Frame Selection}
\label{sec:frame_selection}
% After making the model more efficient at utilizing additional annotations, the next logical problem is to predict \textit{which} frames should be annotated. In real-world datasets, not all data points are equally important to train a machine learning model. Likewise, not all frames in a video are equally important for an interactive segmentation algorithm, and 

Choosing the right frames to annotate leads to higher overall segmentation accuracy, which was demonstrated by previous works \cite{IVOS-W, GISRAmap, BubbleNets}. We designed an algorithm for this based on an empirical idea - to select the most \textit{diverse subset of frames} that capture the target object in different illumination conditions, pose and camera viewpoint, inspired by \cite{GISRAmap}. Given the task to select $b$ frames from a video, we assume there are $b$ different ``scenes" in it, and sample the most representative frame from each.

Given $a$ previous annotations ($a\ge 1$ since there is at least one mask provided by the user) and the predictions of the model for the rest of the frames in the video, we extract the ``key" features $k_i$ of size $(h, w)$ with our segmentation module, weighted by corresponding mask $m_i$, obtaining a region-weighted mask $r_i$. This allows the algorithm to focus on target-specific region similarity, while still having information about surrounding regions. The influence of the mask over the holistic frame features is controlled by parameter $\alpha, \alpha \in [0..1]$. With $\alpha = 0$ the algorithm ignores the annotation masks, predicting the candidates only based on the overall frame appearance, with $\alpha = 1$ it only looks at the masked regions, ignoring the rest of the pixels.
%
\[
    r_a = \alpha k_a \odot m_a + (1 - \alpha) k_a
\]
%
We then iteratively pick $b-a$ candidates with the highest dissimilarity $\mathcal{D}$, using negative pixelwise $L$-2 similarity $S$ from \cite{cheng2022xmem} with added cycle consistency. 
%
\[
    \mathcal{D}_{r_a, r_b} = \frac{\sum_{i, j}^{h \times w}{\max{\left(0, \left( S_{i, j}(r_a, r_b) - S_{i, j}(r_b, r_a)  \right) \right)}}}{h \cdot w}
\] 
%
Given frames $\mathbf{f}$ and previous annotations $a$, we compute the dissimilarity across all the frames $\mathbf{f}$ and all previous annotations $\mathbf{a}$: $\mathcal{D}_{r_ai, r_b} \forall a_i \in \mathbf{a}, b \in \mathbf{f}$. We then select $\argmax{\left( \argmin{\mathcal{D}_{r_ai, r_b}}\right) \forall a_i \in \mathbf{a}, b \in \mathbf{f}}$, the frame with the \textit{largest minimal distance} to all the existing annotations, as the next candidate. This process is repeated $b-a$ times. Due to ambiguity in pixel-to-pixel mapping, often a lot of pixels from $f_a$ map to other pixels from $f_a$, thus average self-dissimilarity is $> 0$. Cycle consistency ($S_{i, j}(r_a, r_b) - S_{i, j}(r_b, r_a)$) ensures that frames are only dissimilar if pixels in $f_i$ map to different positions in $f_j$, then from $f_j$ back to $f_i$. This guarantees that self-dissimilarity $\mathcal{D}_{r_i, r_i}=0$. Refer to the Appendix for a step-by-step explanation of the algorithm.

This allows our algorithm to demonstrate the desirable properties: it does not select candidates similar to already chosen ones, is generic and applicable to any memory-based segmentation model, and does not assume that the mask should match the visual object cues, which is violated in the case of partial segmentation, shown in Sec. \ref{sec:related_works}.
 % There are multiple challenges associated with the problem, the most notable being:
% \begin{itemize}
% % Due to the nature of a video, very often the frames that are neighbors on the time axis have high spatial correlation between pixel values.
%     \item High \textit{spatial correlation} between consecutive frames. If annotating frame $i$ leads to great segmentation quality improvements, annotating frame $i+1$ or $i-1$ is often likely to result in a similar improvement as well.
%     \item Duration and position imbalance of different scenes in the video. A 1-minute video could contain the following 50 seconds of an almost static scene and then 10 seconds of active movement, and high-quality segmentation is required for \textit{both}. 
%     %This is why simply picking every $i$-th or a random frame for annotation is a sub-optimal strategy for some real-life videos because it results in redundant frames being selected. 
% \end{itemize}

% We also impose the following constraints to make selecting frames a practical problem:

% \begin{itemize}
%     \item Depending on the target object, the best subset of frames to annotate (of the same video) might vary. The algorithm must be \textit{target-aware} - able to adapt the recommendations based on what is being segmented.
%     \item Must be applicable to most segmentation models, preferably as a \textit{generic decoupled module}, not requiring code/architecture modifications and/or retraining.
%     \item We define a ``scene" as a contiguous subset of frames in a video, with a relatively consistent combination of the visual appearance of the target object, its location, and movement. The algorithm must \textit{represent each scene equally}, regardless of their relative durations. 
%     %(i.e. one long scene should not overshadow a few shorter scenes). 
%     \item Should not use the \textit{assumption of the quality} of predicted segmentation masks being correlated with \textit{how well the mask matches the visual cues} (such as object boundaries), explicitly or implicitly, as it is violated in the case of dealing with partial segmentation. 
% \end{itemize}

% With the aforementioned challenges in mind, we designed an algorithm based on an empirical idea - to select the most \textit{diverse subset of frames} that capture the target object in different illumination conditions, pose and camera viewpoints: ``Iterative Similarity through Attention Algorithm with Cycle Consistency" \textit{our method} - an algorithm with an idea similar to R-score computation proposed in \cite{GISRAmap}. Given the task to select $b$ frames from a video, we assume there are $b$ different ``scenes" in it, and sample the most representative frame from each.
%\textcolor{red}{JL: Can we elaborate on this algorithm with additional details? then, do we use [key feat, mask], channel dimension of dim(key)+1, as a feature in this case? do we somehow balance the weights of key and mask features?  what similarity metric do we use, cosine/L2? We could have one equation for this.}



% \begin{algorithm}
% \todo{CHANGE PSEUDO CODE}
% \caption{Our algorithm}\label{alg:ISAACC}
% \begin{algorithmic}
% \State $c_{1} \gets (\frac{I_1.width}{2}, \frac{I_1.height}{2})$ \Comment{center of $I_1$}
% \State $c_{2} \gets (\frac{I_2.width}{2}, \frac{I_2.height}{2})$ \Comment{center of $I_2$}
% \State $\phi \gets |\cos{(\arctantwo{(c_2.y - c_1.y, c_2.x - c_1.x)})}|$
% \State $is\_left \gets c_1.x < c_2.x$
% \State $is\_top \gets c_1.y < c_2.y$

% \For{$i$ in $[0..m-1]$}
%     \For{$j$ in $[0..n-1]$}
%         \State $\alpha_i^j \gets \phi (\alpha_h)_i^j + (1 - \phi) (\alpha_v)_i^j$
%     \EndFor
% \EndFor

% \Comment{Account for the images' relative position}
% \If{$c_1.x < c_2.x$} \Comment{$I_1$ is to the left of $I_2$}
%     \State $\alpha_i^j = \alpha_i^{n - j - 1}$ \Comment{swap columns}
% \EndIf

% \If{$c_1.y < c_2.y$} \Comment{$I_1$ is to the top of $I_2$}
%     \State $\alpha_i^j = \alpha_{m - i -1}^j$ \Comment{swap rows}
% \EndIf

% \end{algorithmic}
% \end{algorithm}

\section{Dataset and Benchmark}
 
% \begin{itemize}
%     \item \todo{Show examples of use-cases from the dataset (tattoo, moving chair, half-face, ..)}
%     \item \todo{Provide info about the dataset (number of videos, number of objects, FPS, how many annotations, etc. (how if it's still not created???)}
%     \item \todo{Explain how the annotations were created (refer to BURST \cite{BURST} section 4 for reference}
% \end{itemize}
We provide a new benchmark that covers use cases for multipart partial segmentation with visually challenging situations (segments of parts of the scene with little-to-no pixel-level visual cues) called Partial and Unusual MAsk Video Object Segmentation (\datasetname). To the best of our knowledge, there are no currently available datasets like this. We contemplate scenarios from the video production industry that still conventional VOS methods struggle to tackle. We focus on partial objects such as half faces, neck, tattoos, and pimples, which are frequently retouched in film production as shown in Fig \ref{fig:pumavosdataset}. Our dataset consists of 23 clips of 18 to 35 seconds \footnotemark. 
\footnotetext{Subject to change, the dataset will be available after acceptance.}
To generate the annotations, we adopted a similar approach to MOSE \cite{MOSE} that used a framework with XMem \cite{cheng2022xmem} to create masks for each frame, but instead we used our method, \modelname. In MOSE the videos were annotated every 5th frame (20\% of the video), while in our case we noticed that complex scenes require 8\% to 10\% and simple scenes required 4\% to 6\% of total frames to be annotated. 

% % Figure environment removed
% the gifs need to be available, if not it won't compile 
% \includemovie{1cm}{1cm}{figures/dataset/vlog_78_fr_8th_iter.gif}

 % Figure environment removed
% % Figure environment removed