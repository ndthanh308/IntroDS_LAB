\section{Discussion}

We introduced a highly robust semi-supervised and interactive video segmentation framework, \modelname, with automatic next best frame prediction for user annotation. We have shown that by introducing a permanent memory module to XMem~\cite{cheng2022xmem}, efficient usage of multiple annotated frames is possible, for segmenting a particular object or region, even with drastic changes in the appearance of the object. Our approach achieves better segmentation results than current SOTA VOS methods with significantly fewer frame annotations (in our experiments, up to 5$\times$ fewer annotations in highly challenging cases). Our approach further demonstrates the ability to reliably segment partial regions of an object (e.g., the left half of a face) with only a few frame annotations, which is a notoriously difficult task for any existing segmentation methods. As highlighted in our accompanying video, even for highly challenging and long scenes, our masks are temporally smooth without the need for additional post-processing. Hence, our method is suitable for production use cases, such as rotoscoping, where accurate region segmentation and minimal user input is needed.

Our proposed solution is also suitable for non-expert users, as it suggests the next best frame for the user to annotate using an effective yet simple attention-based algorithm. Our experiments indicate that the predicted frames are often very similar to those chosen by expert users, which is always superior to randomly chosen ones. We also show that our framework can be conveniently used to collect and annotate a new dataset, \datasetname{}, covering challenging practical segmentation use-cases, such as partial segmentations, multi-object-part segmentation, complex lighting conditions, which cannot be found in existing datasets. 

\paragraph{Future Work.}

While our framework significantly improves the current SOTA within the context of IVOS, we believe that further reduction in frame annotations and complex shape segmentations is possible. In particular, we plan to investigate methods that incorporate dense scene correspondences and on-the-fly generative data augmentation of the segmented regions, which can even be used to improve the robustness of the frame prediction further. 

 % difficult to give a negative mask -> future work


%We present a new framework \modelname for semi-supervised and interactive video object segmentation, which allows efficient usage of multiple annotated frames, produces temporaly smooth segmentations for complex scenes, such as partial segmentation, and has a frame annotation candidates selection module, that automatically suggests the best frames for annotation, based on the previously segmented frames, which handles difficult use-cases such as partial segmentation and duration-imbalanced scenes. It is \todo{realtime on 480p footage, proof}, is memory-efficient, is capable of processing long videos (1000+ frames), works with both sparse (scribbles) and dense (segmentation masks). We use this framework to collect and annotate a dataset covering challengind practical segmentation use-cases, such as partial segmentations, multi-object-part segmentation, complex lighting conditions and more. 
% What have we tried so far:
% \todo{Maybe write about use-cases? Show how XMem fails on them, but we don't?}

% \begin{itemize}
%     \item Multiscale memory - CUDA OOM errors, maybe have time to try and fix?
%     \item Swapping backbone - no improvements? Ari
%     \item A lot of different FACS approaches - why they don't work?
%     \item On the usefulness of memory at all - if we froze it with K permanent annotations, is it better or worse? 
%     \item \todo{Apply permanent memory to STM, show that it also improves quality; same with maybe FACS?}
%     \item \todo{Run multiple models (at least 2 more) with SAME FRAMES as XMem\_preloading, UNIFORM BASELINE (FAIR), on }?
% \end{itemize}
% Both the architecture modification and the ASAC module can be applied to any existing memory-based SSVOS or IVOS model. 