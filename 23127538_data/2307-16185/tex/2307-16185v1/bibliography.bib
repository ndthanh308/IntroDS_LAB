@manual{aniche-ck,
  title={Java code metrics calculator (CK)},
  author={Maurício Aniche},
  year={2015},
  note={Available in https://github.com/mauricioaniche/ck/}
}

@misc{InternationalStandardOrganization(ISO).2001,
  added-at = {2013-10-18T15:41:04.000+0200},
  author = {{International Standard Organization (ISO)}},
  biburl = {https://www.bibsonomy.org/bibtex/23897d5d6e39fc4567a0a4ad58fbf2ee5/hlackner},
  institution = {{International Standard Organization (ISO)}},
  interhash = {cb33656a36c06ad92e469d0f1e44d523},
  intrahash = {3897d5d6e39fc4567a0a4ad58fbf2ee5},
  keywords = {Informatik},
  timestamp = {2013-10-18T15:41:04.000+0200},
  title = {International Standard ISO/IEC 9126, Information technology ‐ Product  Quality ‐ Part1: Quality Model},
  year = 2001
}

@misc{IEEE1990,
  added-at = {2013-10-18T17:16:24.000+0200},
  author = {IEEE},
  biburl = {https://www.bibsonomy.org/bibtex/2fe0466bf1128a3b4dd9361415f13d1e0/junkerm},
  doi = {10.1109/IEEESTD.1990.101064},
  groups = {public},
  institution = {IEEE},
  interhash = {7d3dc24efccff2d3336f506d4f1063af},
  intrahash = {168c4ff010eecef4cce9d5b0030060ce},
  journal = {IEEE Std 610.12-1990},
  keywords = {glossary;terminology;definition;ieee},
  organization = {IEEE},
  pages = {1-84},
  timestamp = {2013-10-28T09:22:14.000+0100},
  title = {IEEE Standard Glossary of Software Engineering Terminology},
  username = {junkerm},
  year = 1990
}


@inproceedings{terragni_measuring_2020,
	address = {Seoul Republic of Korea},
	title = {Measuring {Software} {Testability} {Modulo} {Test} {Quality}},
	isbn = {978-1-4503-7958-8},
	url = {https://dl.acm.org/doi/10.1145/3387904.3389273},
	doi = {10.1145/3387904.3389273},
	abstract = {Comprehending the degree to which software components support testing is important to accurately schedule testing activities, train developers, and plan effective refactoring actions. Software testability estimates such property by relating code characteristics to the test effort. The main studies of testability reported in the literature investigate the relation between class metrics and test effort in terms of the size and complexity of the associated test suites. They report a moderate correlation of some class metrics to test-effort metrics, but suffer from two main limitations: (i) the results hardly generalize due to the small empirical evidence (datasets with no more than eight software projects); and (ii) mostly ignore the quality of the tests. However, considering the quality of the tests is important. Indeed, a class may have a low test effort because the associated tests are of poor quality, and not because the class is easier to test. In this paper, we propose an approach to measure testability that normalizes the test effort with respect to the test quality, which we quantify in terms of code coverage and mutation score. We present the results of a set of experiments on a dataset of 9,861 Java classes, belonging to 1,186 open source projects, with around 1.5 million of lines of code overall. The results confirm that normalizing the test effort with respect to the test quality largely improves the correlation between class metrics and the test effort. Better correlations result in better prediction power and thus better prediction of the test effort.},
	language = {english},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Program} {Comprehension}},
	publisher = {ACM},
	author = {Terragni, Valerio and Salza, Pasquale and Pezzè, Mauro},
	month = jul,
	year = {2020},
	pages = {241--251},
}

@article{garousi_survey_2019,
	title = {A survey on software testability},
	volume = {108},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584918302490},
	doi = {10.1016/j.infsof.2018.12.003},
	abstract = {Objective: Our objective is to summarize the body of knowledge in this area and to beneﬁt the readers (both practitioners and researchers) in preparing, measuring and improving software testability.
Method: To address the above need, the authors conducted a survey in the form of a systematic literature mapping (classiﬁcation) to ﬁnd out what we as a community know about this topic. After compiling an initial pool of 303 papers, and applying a set of inclusion/exclusion criteria, our ﬁnal pool included 208 papers (published between 1982 and 2017).
Results: The area of software testability has been comprehensively studied by researchers and practitioners. Approaches for measurement of testability and improvement of testability are the most-frequently addressed in the papers. The two most often mentioned factors aﬀecting testability are observability and controllability. Common ways to improve testability are testability transformation, improving observability, adding assertions, and improving controllability.
Conclusion: This paper serves for both researchers and practitioners as an “index” to the vast body of knowledge in the area of testability. The results could help practitioners measure and improve software testability in their projects. To assess potential beneﬁts of this review paper, we shared its draft version with two of our industrial collaborators. They stated that they found the review useful and beneﬁcial in their testing activities. Our results can also beneﬁt researchers in observing the trends in this area and identify the topics that require further investigation.},
	language = {english},
	urldate = {2022-06-21},
	journal = {Information and Software Technology},
	author = {Garousi, Vahid and Felderer, Michael and Kılıçaslan, Feyza Nur},
	month = apr,
	year = {2019},
	pages = {35--64},
}

@article{khan_metric_2009,
	title = {Metric based testability model for object oriented design ({MTMOOD})},
	volume = {34},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1507195.1507204},
	doi = {10.1145/1507195.1507204},
	abstract = {This paper does an extensive review on testability of object oriented software, and put forth some relevant information about class-level testability. Testability has been identified as a key factor to software quality, and emphasis is being drawn to predict class testability early in the software development life cycle. A Metrics Based Model for Object Oriented Design Testability (MTMOOD) has been proposed. The relationship from design properties to testability is weighted in accordance with its anticipated influence and importance. A suit of adequate object-oriented metrics useful in determining testability of a system has been proposed, which may be used to locate parts of design that could be error prone. Identification of changes in theses parts early could significantly improve the quality of the final product and hence decrease the testing effort. The proposed model has been further empirically validated and contextual interpretation has been drawn using industrial software projects.},
	language = {english},
	number = {2},
	urldate = {2022-06-21},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Khan, R. A. and Mustafa, K.},
	month = feb,
	year = {2009},
	pages = {1--6},
}

@article{kout_empirical_2011,
	title = {An empirical analysis of a testability model for object-oriented programs},
	volume = {36},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/1988997.1989020},
	doi = {10.1145/1988997.1989020},
	abstract = {We present, in this paper, a metric based testability model for objectoriented programs. The model is, in fact, an adaptation of a model proposed in literature for assessing the testability of object-oriented design. The study presented in this paper aims at exploring empirically the capability of the model to assess testability of classes at the code level. We investigate testability from the perspective of unit testing and required testing effort. We designed an empirical study using data collected from two Java software systems for which JUnit test cases exist. To capture testability of classes in terms of required testing effort, we used different metrics to quantify the corresponding JUnit test cases. In order to evaluate the capability of the model to predict testability of classes (characteristics of corresponding test classes), we used statistical tests using correlation.},
	language = {english},
	number = {4},
	urldate = {2022-06-21},
	journal = {ACM SIGSOFT Software Engineering Notes},
	author = {Kout, Aymen and Toure, Fadel and Badri, Mourad},
	month = aug,
	year = {2011},
	pages = {1--5},
}

@article{voas_software_1995,
	title = {Software testability: the new verification},
	volume = {12},
	issn = {07407459},
	shorttitle = {Software testability},
	url = {http://ieeexplore.ieee.org/document/382180/},
	doi = {10.1109/52.382180},
	language = {english},
	number = {3},
	urldate = {2022-06-21},
	journal = {IEEE Software},
	author = {Voas, J.M. and Miller, K.W.},
	month = may,
	year = {1995},
	pages = {17--28},
}

@incollection{bologna_object-oriented_1996,
	address = {Boston, MA},
	title = {Object-{Oriented} {Software} {Testability}},
	isbn = {978-1-4757-4392-0 978-0-387-34869-8},
	url = {http://link.springer.com/10.1007/978-0-387-34869-8_23},
	abstract = {This paper studies whether object-oriented systems are more likely to hide faults during system level testing. When this project began, we suspected that although object-oriented designs have features particularly supportive of software reuse, this paradigm is detrimental to system level testing for fault detection. After comparing the testability of both 00 and procedural systems, this project studied ways to engender reusability, while maintaining an acceptable level of testability. By studying whether this family of programming languages and design paradigm are harmful to testability, and evaluating methods to increase testability, we provide information that should be considered before 00 languages are endorsed as tools for developing safety-critical software.},
	language = {english},
	urldate = {2022-06-21},
	booktitle = {Achieving {Quality} in {Software}},
	publisher = {Springer US},
	author = {Voas, Jeffrey M.},
	editor = {Bologna, Sandro and Bucci, Giacomo},
	year = {1996},
	doi = {10.1007/978-0-387-34869-8_23},
	pages = {279--290},
}

@article{bertolino_use_1996,
	title = {On the use of testability measures for dependability assessment},
	volume = {22},
	issn = {00985589},
	url = {http://ieeexplore.ieee.org/document/485220/},
	doi = {10.1109/32.485220},
	abstract = {Program “testability” is, informally, the probabilitythat a program will fail under test if it contains at least one fault. When a dependability assessment has to be derived from the observationof a series of failure-free test executions (a common need for software subject to “ultra-high reliability” requirements), measures of testability can-in theoty-be used to draw inferences on program correctness (and hence on its probabilityof failure in operation). In this paper, we rigorously investigatethe concept of testability and its use in dependability assessment, criticizing, and improving on, previously published results.},
	language = {english},
	number = {2},
	urldate = {2022-06-21},
	journal = {IEEE Transactions on Software Engineering},
	author = {Bertolino, A. and Strigini, L.},
	month = feb,
	year = {1996},
	pages = {97--108},
}

@inproceedings{zhao_new_2006,
	address = {Shanghai China},
	title = {A new approach for software testability analysis},
	isbn = {978-1-59593-375-1},
	url = {https://dl.acm.org/doi/10.1145/1134285.1134469},
	doi = {10.1145/1134285.1134469},
	abstract = {Software testability analysis has been an important research direction since 1990s and becomes more pervasive when entering 21st century. In this paper, we summarize problems in existing research work. We propose to use beta distribution to indicate software testability. When incorporating testing eﬀectiveness information, we theoretically prove that the distribution can express testing eﬀort and test value at the same time. We conduct experiment and validate our results on Siemens programs. Future work concentrate on deducing a prior estimation of the distribution for given software and testing criterion pair from program slicing and semantic analysis.},
	language = {english},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the 28th international conference on {Software} engineering},
	publisher = {ACM},
	author = {Zhao, Liang},
	month = may,
	year = {2006},
	pages = {985--988},
}

@inproceedings{khoshgoftaar_predicting_2000,
	address = {Richardson, TX, USA},
	title = {Predicting testability of program modules using a neural network},
	isbn = {978-0-7695-0559-6},
	url = {http://ieeexplore.ieee.org/document/888032/},
	doi = {10.1109/ASSET.2000.888032},
	abstract = {Voas de nes testability as the probability that a test case will fail if the program has a fault. It is de ned in the context of an oracle for the test, and a distribution of test cases, usually emulating operations. Because testability is a dynamic attribute of software, it is very computation-intensive to measure directly.},
	language = {english},
	urldate = {2022-06-21},
	booktitle = {Proceedings 3rd {IEEE} {Symposium} on {Application}-{Specific} {Systems} and {Software} {Engineering} {Technology}},
	publisher = {IEEE Comput. Soc},
	author = {Khoshgoftaar, T.M. and Allen, E.B. and Xu, Z.},
	year = {2000},
	pages = {57--62},
}

@article{badri_empirical_2011,
	title = {An {Empirical} {Analysis} of {Lack} of {Cohesion} {Metrics} for {Predicting} {Testability} of {Classes}},
	volume = {5},
	abstract = {The aim of this work is to explore empirically the relationship between lack of cohesion metrics and testability of classes in object-oriented systems. We addressed testability from the perspective of unit testing. We performed an empirical analysis using data collected from two Java software systems for which JUnit test cases exist. To capture testability of classes, we used different metrics to measure some characteristics of the corresponding JUnit test cases. In order to evaluate the capability of lack of cohesion metrics to predict testability, we used statistical analysis techniques using correlation and logistic regression. The performance of the predicted model was evaluated using Receiver Operating Characteristic (ROC) analysis. The achieved results provide evidence that there exist a relationship between lack of cohesion and testability.},
	language = {english},
	number = {2},
	journal = {International Journal of Software Engineering and Its Applications},
	author = {Badri, Linda and Badri, Mourad and Toure, Fadel},
	year = {2011},
	pages = {18},
}

@inproceedings{bruntink_predicting_2004,
	address = {Chicago, IL, USA},
	title = {Predicting class testability using object-oriented metrics},
	isbn = {978-0-7695-2144-2},
	url = {http://ieeexplore.ieee.org/document/1386167/},
	doi = {10.1109/SCAM.2004.16},
	abstract = {In this paper we investigate factors of the testability of object-oriented software systems. The starting point is given by a study of the literature to obtain an initial model of testability, and related software metrics. Subsequently, the metrics are evaluated by means of two case studies of large Java systems for which JUnit test cases exist. The goal of this paper is to deﬁne and evaluate a set of metrics that can be used to assess the testability of the classes of a Java system.},
	language = {english},
	urldate = {2022-06-21},
	booktitle = {Source {Code} {Analysis} and {Manipulation}, {Fourth} {IEEE} {International} {Workshop} on},
	publisher = {IEEE Comput. Soc},
	author = {Bruntink, M. and van Deursen, A.},
	year = {2004},
	pages = {136--145},
}

@article{bruntink_empirical_2006,
	title = {An empirical study into class testability},
	volume = {79},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121206000586},
	doi = {10.1016/j.jss.2006.02.036},
	abstract = {In this paper we investigate factors of the testability of object-oriented software systems. The starting point is given by a study of the literature to obtain both an initial model of testability and existing object-oriented metrics related to testability. Subsequently, these metrics are evaluated by means of ﬁve case studies of commercial and open source Java systems for which JUnit test cases exist. The goal of this paper is to identify and evaluate a set of metrics that can be used to assess the testability of the classes of a Java system.},
	language = {english},
	number = {9},
	urldate = {2022-06-21},
	journal = {Journal of Systems and Software},
	author = {Bruntink, Magiel and van Deursen, Arie},
	month = sep,
	year = {2006},
	pages = {1219--1232},
}

@inproceedings{da_cruz_empirical_2017,
	address = {Porto, Portugal},
	title = {An {Empirical} {Analysis} of the {Correlation} between {CK} {Metrics}, {Test} {Coverage} and {Mutation} {Score}},
	isbn = {978-989-758-247-9 978-989-758-248-6 978-989-758-249-3},
	shorttitle = {An {Empirical} {Analysis} of the {Correlation} between {CK} {Metrics}, {Test} {Coverage} and {Mutation} {Score}},
	url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006312703410350},
	doi = {10.5220/0006312703410350},
	abstract = {Testability, Testing, Metrics, CK, Code Coverage, Mutation Score.},
	language = {english},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Enterprise} {Information} {Systems}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {da Cruz, Robinson Crusoé and Medeiros Eler, Marcelo},
	year = {2017},
	pages = {341--350},
}

@article{badri_empirical_2012,
	title = {Empirical {Analysis} of {Object}-{Oriented} {Design} {Metrics} for {Predicting} {Unit} {Testing} {Effort} of {Classes}},
	volume = {05},
	issn = {1945-3116, 1945-3124},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/jsea.2012.57060},
	doi = {10.4236/jsea.2012.57060},
	abstract = {In this paper, we investigate empirically the relationship between object-oriented design metrics and testability of classes. We address testability from the point of view of unit testing effort. We collected data from three open source Java software systems for which JUnit test cases exist. To capture the testing effort of classes, we used metrics to quantify the corresponding JUnit test cases. Classes were classified, according to the required unit testing effort, in two categories: high and low. In order to evaluate the relationship between object-oriented design metrics and unit testing effort of classes, we used logistic regression methods. We used the univariate logistic regression analysis to evaluate the individual effect of each metric on the unit testing effort of classes. The multivariate logistic regression analysis was used to explore the combined effect of the metrics. The performance of the prediction models was evaluated using Receiver Operating Characteristic analysis. The results indicate that: 1) complexity, size, cohesion and (to some extent) coupling were found significant predictors of the unit testing effort of classes and 2) multivariate regression models based on object-oriented design metrics are able to accurately predict the unit testing effort of classes.},
	language = {english},
	number = {07},
	urldate = {2022-06-21},
	journal = {Journal of Software Engineering and Applications},
	author = {Badri, Mourad and Toure, Fadel},
	year = {2012},
	pages = {513--526},
}

@article{toure_predicting_2018,
	title = {Predicting different levels of the unit testing effort of classes using source code metrics: a multiple case study on open-source software},
	volume = {14},
	issn = {1614-5046, 1614-5054},
	shorttitle = {Predicting different levels of the unit testing effort of classes using source code metrics},
	url = {http://link.springer.com/10.1007/s11334-017-0306-1},
	doi = {10.1007/s11334-017-0306-1},
	language = {english},
	number = {1},
	urldate = {2022-06-21},
	journal = {Innovations in Systems and Software Engineering},
	author = {Toure, Fadel and Badri, Mourad and Lamontagne, Luc},
	month = mar,
	year = {2018},
	pages = {15--46},
}

@article{freedman_testability_1991,
	title = {Testability of software components},
	volume = {17},
	issn = {0098-5589},
	doi = {10.1109/32.87281},
	number = {6},
	journal = {IEEE transactions on Software Engineering},
	author = {Freedman, Roy S},
	year = {1991},
	pages = {553--564},
}

@article{singh_predicting_2010,
	title = {Predicting testability of eclipse: a case study},
	volume = {4},
	doi = {10.3923/jse.2010.122.136},
	number = {2},
	journal = {Journal of Software Engineering},
	author = {Singh, Y and Saha, A},
	year = {2010},
	pages = {122--136},
}

@inproceedings{badri_exploring_2010,
	title = {Exploring empirically the relationship between lack of cohesion and testability in object-oriented systems},
	publisher = {Springer},
	author = {Badri, Linda and Badri, Mourad and Toure, Fadel},
	year = {2010},
	pages = {78--92},
}

@article{gupta_fuzzy_2005,
	title = {A fuzzy approach for integrated measure of object-oriented software testability},
	volume = {1},
	doi = {10.3844/jcssp.2005.276.282},
	number = {2},
	journal = {Journal of Computer Science},
	author = {Gupta, Vandana and Aggarwal, KK and Singh, Yogesh},
	year = {2005},
	pages = {276--282},
}

@inproceedings{alshahwan_improving_2009,
	title = {Improving web application testing using testability measures},
	isbn = {1-4244-5126-4},
	publisher = {IEEE},
	author = {Alshahwan, Nadia and Harman, Mark and Marchetto, Alessandro and Tonella, Paolo},
	year = {2009},
	pages = {49--58},
}

@inproceedings{yu_predicting_2016,
	title = {Predicting testability of concurrent programs},
	isbn = {1-5090-1827-1},
	publisher = {IEEE},
	author = {Yu, Tingting and Wen, Wei and Han, Xue and Hayes, Jane Huffman},
	year = {2016},
	pages = {168--179},
}

@inproceedings{lin_estimated_1997,
	title = {An estimated method for software testability measurement},
	isbn = {0-8186-7840-2},
	publisher = {IEEE},
	author = {Lin, Jin-Cherng and Lin, Szu-Wen},
	year = {1997},
	pages = {116--123},
}

@article{zhou_-depth_2012,
	title = {An in-depth investigation into the relationships between structural metrics and unit testability in object-oriented systems},
	volume = {55},
	issn = {1869-1919},
	doi = {10.1007/s11432-012-4745-x},
	number = {12},
	journal = {Science china information sciences},
	author = {Zhou, YuMing and Leung, Hareton and Song, QinBao and Zhao, JianJun and Lu, HongMin and Chen, Lin and Xu, BaoWen},
	year = {2012},
	note = {Publisher: Springer},
	pages = {2800--2815},
}

@inproceedings{khalid_analysis_2010,
	title = {Analysis of object oriented complexity and testability using object oriented design metrics},
	author = {Khalid, Sadaf and Zehra, Saima and Arif, Fahim},
	year = {2010},
	pages = {1--8},
}

@article{voas_improving_1991,
	title = {Improving the software development process using testability research},
	author = {Voas, Jeffrey M and Miller, Keith W},
	year = {1991},
	keywords = {⛔ No DOI found},
}

@article{voas_pie_1992,
	title = {{PIE}: {A} dynamic failure-based technique},
	volume = {18},
	issn = {0098-5589},
	doi = {10.1109/32.153381},
	number = {8},
	journal = {IEEE Transactions on software Engineering},
	author = {Voas, Jeffrey M.},
	year = {1992},
	note = {Publisher: IEEE Computer Society},
	pages = {717},
}

@article{voas_predicting_1991,
	title = {Predicting where faults can hide from testing},
	volume = {8},
	issn = {0740-7459},
	doi = {10.1109/52.73748},
	number = {2},
	journal = {IEEE Software},
	author = {Voas, Jeffrey and Morell, Larry and Miller, Keith},
	year = {1991},
	note = {Publisher: IEEE},
	pages = {41--48},
}

@article{chidamber_metrics_1994,
	title = {A metrics suite for object oriented design},
	volume = {20},
	issn = {0098-5589},
	doi = {10.1109/32.295895},
	number = {6},
	journal = {IEEE Transactions on software engineering},
	author = {Chidamber, Shyam R and Kemerer, Chris F},
	year = {1994},
	note = {Publisher: IEEE},
	pages = {476--493},
}

@inproceedings{singh_predicting_2008,
	title = {Predicting testing effort using artificial neural network},
	publisher = {Citeseer},
	author = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
	year = {2008},
	pages = {1012--1017},
}

@article{binder_design_1994,
	title = {Design for testability in object-oriented systems},
	volume = {37},
	issn = {0001-0782},
	doi = {10.1145/182987.184077},
	number = {9},
	journal = {Communications of the ACM},
	author = {Binder, Robert V},
	year = {1994},
	note = {Publisher: ACM New York, NY, USA},
	pages = {87--101},
}

@book{kalman_topics_1969,
	title = {Topics in mathematical system theory},
	volume = {33},
	publisher = {McGraw-Hill New York},
	author = {Kalman, Rudolf Emil and Falb, Peter L and Arbib, Michael A},
	year = {1969},
}

@book{mccluskey_logic_1986,
	title = {Logic design principles with emphasis on testable semicustom circuits},
	isbn = {0-13-539784-7},
	publisher = {Prentice-Hall, Inc.},
	author = {McCluskey, Edward J},
	year = {1986},
}

@article{krejcie_determining_1970,
	title = {Determining sample size for research activities},
	volume = {30},
	issn = {0013-1644},
	doi = {10.1177/001316447003000308},
	number = {3},
	journal = {Educational and psychological measurement},
	author = {Krejcie, Robert V and Morgan, Daryle W},
	year = {1970},
	note = {Publisher: Sage publications Sage CA: Los Angeles, CA},
	pages = {607--610},
}

@inproceedings{coles_pit_2016,
	title = {Pit: a practical mutation testing tool for java},
	author = {Coles, Henry and Laurent, Thomas and Henard, Christopher and Papadakis, Mike and Ventresque, Anthony},
	year = {2016},
	pages = {449--452},
}

@article{tufano_methods2test_2022,
	title = {{Methods2Test}: {A} dataset of focal methods mapped to test cases},
	journal = {arXiv preprint arXiv:2203.12776},
	author = {Tufano, Michele and Deng, Shao Kun and Sundaresan, Neel and Svyatkovskiy, Alexey},
	year = {2022},
	keywords = {⛔ No DOI found},
}

@article{demillo_hints_1978,
	title = {Hints on test data selection: {Help} for the practicing programmer},
	volume = {11},
	issn = {0018-9162},
	doi = {10.1109/C-M.1978.218136},
	number = {4},
	journal = {Computer},
	author = {DeMillo, Richard A and Lipton, Richard J and Sayward, Frederick G},
	year = {1978},
	note = {Publisher: IEEE},
	pages = {34--41},
}

@inproceedings{fraser_evosuite_2011,
	title = {Evosuite: automatic test suite generation for object-oriented software},
	author = {Fraser, Gordon and Arcuri, Andrea},
	year = {2011},
	pages = {416--419},
}

@book{pezze_software_2008,
	title = {Software testing and analysis: process, principles, and techniques},
	isbn = {81-265-1773-5},
	publisher = {John Wiley \& Sons},
	author = {Pezzè, Mauro and Young, Michal},
	year = {2008},
}

@inproceedings{voas1993software,
  title={Software testability and its application to avionic software},
  author={Voas, Jeffrey and Miller, Keith and Payne, Jeffery},
  booktitle={9th Computing in Aerospace Conference},
  pages={4542},
  year={1993}
}

@inproceedings{voas1993empirical,
  title={An empirical comparison of a dynamic software testability metric to static cyclomatic complexity},
  author={Voas, Jeffrey M and Miller, Keith W and Payne, Jeffrey E},
  booktitle={NASA. Goddard Space Flight Center, Proceedings of the Eighteenth Annual Software Engineering Workshop},
  year={1993}
}

@inproceedings{jianping2010present,
  title={Present and future of software testability analysis},
  author={Jianping, Fu and Bin, Liu and Minyan, Lu},
  booktitle={2010 International Conference on Computer Application and System Modeling (ICCASM 2010)},
  volume={15},
  pages={V15--279},
  year={2010},
  organization={IEEE}
}

@article{demillo1991constraint,
  title={Constraint-based automatic test data generation},
  author={DeMillo, Richard A and Offutt, A Jefferson and others},
  journal={IEEE Transactions on Software Engineering},
  volume={17},
  number={9},
  pages={900--910},
  year={1991},
  publisher={Citeseer}
}