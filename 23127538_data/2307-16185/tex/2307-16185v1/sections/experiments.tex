We investigated to which extent our estimates of software testability for the methods that belong to a Java program can capture the actual complexity of developing test cases for those methods, in a set of experiments with many class methods and test cases out of three large Java projects.


We remark upfront the foundational nature of our current experiments.  
In particular we do not make any strong claim on the efficiency of either our current implementation, or the test generator and the mutation analyzer that the current implementation depends on. 
The main goal of the experiments reported in this section is to explore if there is merit in our idea of estimating testability by relying on empirical observations made with automatically generated test cases, and the possible complementarity between this new approach and the traditional approach of relying on the correlation with static software metrics.

\subsection{Research questions}
Our experiments were driven by the following research questions:

\begin{inparaenum}[RQ1:]
\item How large is the portion of inconclusive estimates with our current implementation of the technique? 

\item To what extent do our (conclusive) testability estimates correlate with the development complexity of the test cases that were designed for the considered methods, and how do they compare with traditional software metrics in this respect? 

\item Does combining our testability estimates with other static metrics improve over using only the static metrics as predictors of testability?
\end{inparaenum}

\subsection{Subjects}
We selected from GitHub three open-source Java projects that
\begin{inparaenum}[(i)]
\item use Maven as build tool, as this is a requirement of our current implementation of the technique, 
\item are representative of large projects comprised of at least 500 classes,
\item include at least 300 methods that can be associated (with the procedure that we describe in Section~\ref{testMethodAssociation}) with  test cases available in the projects,  
\item are representative of different types of software developments, namely, a programming library, a software engineering tool and a business oriented application. 
\end{inparaenum}
The three projects are: 
\begin{itemize}
\item JFreeChart, a \emph{programming library} that supports the display of charts,
\item Closure Compiler, a \emph{software engineering tool} that parses and optimizes programs in Javascript, and
\item OpenMRS-Core, a \emph{business-oriented application} for the healthcare domain.
\end{itemize}

\begin{table*}[htbp]
\small
\centering
\caption{\label{tab:overallStats}Descriptive statistics of the subject methods in the three considered Java projects}
\begin{tabular}{r|ccc|ccc|ccc|}
 & \multicolumn{3}{c|}{\textbf{JFreeChart}} & \multicolumn{3}{c|}{\textbf{Closure Compiler}} & \multicolumn{3}{c|}{\textbf{OpenMRS}} \\
& All & Tested & Subjects & All & Tested & Subjects & All & Tested & Subjects \\
\hline
\textbf{Number of methods}& 8552 & 613 & 246 & 14723 & 300 & 111 & 9166 & 493 & 241 \\
\textbf{Total lines of code (LOC)}& 71703 & 6751 & 5162 & 110553 & 3195 & 2266 & 51412 & 6527 & 5402 \\
\textbf{Average LOC per method}& 8.38 & 11.01 & 20.98 & 7.51 & 10.65 & 20.41 & 5.61 & 13.21 & 22.41 \\
\textbf{Mininum LOC per method}& 1 & 3 & 5 & 1 & 1 & 1 & 1 & 3 & 6 \\
\textbf{Maximum LOC per method}& 288 & 188 & 188 & 433 & 246 & 246 & 221 & 121 & 121 \\
\textbf{Average mutants per method}& 2.45 & 3.69 & 7.42 & 2.85 & 3.64 & 7.59 & 1.33 & 4.70 & 8.28 \\
\textbf{Mininum mutants per method}& 0 & 1 & 3 & 0 & 1 & 3 & 0 & 1 & 3 \\
\textbf{Maximum mutants per method}& 104 & 74 & 74 & 617 & 66 & 66 & 74 & 46 & 46 \\
\end{tabular}
\end{table*}

Table~\ref{tab:overallStats} summarizes descriptive statistics about the Java methods that belong to each project, namely, the number of methods (first row), their total and individual size (from the second to the fifth row), and the number of mutants in the methods (from the sixth to the eighth row).
The columns \textit{All} refer to all methods in the projects, while the columns \textit{Tested} and \textit{Subjects} refer, respectively, to the subset of methods that we were able to successfully associate with some test cases, and to the further subset that we selected as actual subjects for our study. We describe the procedure by which we selected these two latter subsets in the next section.
The data in the table indicate that we selected methods with increasing size and increasing number of mutants at each selection step.

\subsection{Ground Truth} \label{testMethodAssociation}
Out of the Java methods in the considered projects (Table~\ref{tab:overallStats}, columns \textit{All}), we excluded all methods \textit{hashCode} and \textit{equals} that are usually generated automatically, and further selected only the methods that we could associate with a reference ground-truth, that is, available test cases that the programmers developed for those methods.
This because we aimed at investigating the correlation between our testability estimates for the methods and the development complexity of the corresponding test cases, for methods and test cases designed by human programmers.
We built on the \textit{methods2test} tool~\cite{tufano_methods2test_2022} to associate the methods with the test cases available in the projects, and selected only the methods for which we identified at least an associated test case (Table~\ref{tab:overallStats}, columns \textit{Tested}).

Methods2test heuristically infers the associations between the available test cases and the methods that are their main testing target. It originally relies on two heuristics, \emph{name matching} and \emph{unique method call}, but we extended it with three additional heuristics, \emph{stemming-based name matching}, \emph{contains-based name matching} and \emph{non-helper unique method call},
which generalize the two original ones with the aim to increase the set of identified associations. 

For each test case, which in the considered projects is a test method within a test class, \emph{name matching} searches for a target method that both exactly matches with the name of the test case  and belongs to a class that exactly matches with the name as the test class.
\emph{Stemming-based name matching} and \emph{contains-based name matching} address the name matching with respect to either the stemmed names of methods and test cases, or  whether the test name contains the method name, respectively. For example \textit{testCloning}  and \textit{testCloneSecondCase} will match with method \textit{clone} after name stemming or by name containment, respectively.  
\emph{Unique method call} further exploits the name-based association between a test class and a target class, by searching for test cases that call a single method of the target class. \emph{Non-helper unique method call} re-evaluates the unique-method check after excluding the calls to possible helper methods, such as setter methods, getter methods and the method \textit{equals}.

After the association with the test cases, we further refined the set of subject methods by excluding the methods for which PIT computed mutants for at most two lines of code. For these methods our technique could  distill the unitary testability evidences out of a too squeezed population of seeded faults, which results in yielding unbalanced estimates in most cases. 
We see this as a drawback of the fault models that we are currently able to consider by relying on PIT, rather than as a limitation of our idea of estimating testability based on automatically generated test cases, and we thus dismissed these methods from the current experiments on this basis. We ended with selecting the set of subject methods summarized in the columns \textit{Subjects} of Table~\ref{tab:overallStats}.

We quantified the development complexity of the test cases associated with each subject method as the number of unique method invocations made within the ensemble of those test cases (counted with the tool CK) . We refer to this values as \textit{RfcTest}, i.e., the \textit{Rfc} values of the test cases.
Since the test cases are often sheer sequences of method calls (no decisions, no loops) other complexity metrics (like the cyclomatic complexity) are scarcely representative, while size metrics (like LOC, number of test cases or number of assertions) are more sensitive than RfcTest to arbitrary choices of testers.
\textit{RfcTest} represents more consistently than other metrics the effort that testers spent for understanding methods of other classes, as also considered in several testability studies \cite{badri_exploring_2010, badri_empirical_2011, kout_empirical_2011}.

Furthermore, we assessed the reliability of our  ground-truth with respect to possible errors in the method-test associations returned by Methods2test, by manually crosschecking 10\% of the methods (randomly sampled with R's function \textit{sample}) for which our technique produced a conclusive estimation (cfr.\ Section~\ref{sec:RQ1}, Table~\ref{tab:nomCovered}). 
Out of 42 subject methods that we crosschecked, we detected need for corrections for 7 methods, i.e., 5 methods for which Methods2test reported a wrongly matched test case (false positives), and 2 other methods for which Methods2test missed 3 and 2 associations, respectively (false negatives). For 3 of these 7 methods, correcting the errors of Methods2test did not affect the \textit{RfcTest} value, thus the corrections impacted only 4 out 42 methods. This datum suggests mild impact of the possible errors of Methods2test.  

\subsection{Experimental setting}
We instantiated our technique with EvoSuite, version 1.2.0, and PIT, version 1.8.1. In Section~\ref{sec:technique} we have already described the configuration of EvoSuite with respect to the fitness functions, the time budget and the generation of assertions, and  the mutation operators used with PIT. 

We discriminated inconclusive testability estimates by determining, for each subject method, the threshold for the minimal number of lines of code that we must sample with testability evidences out of the lines for which PIT identified at least a mutant. We computed the thresholds by referring to the classic approximation to the hypergeometric distribution~\cite{krejcie_determining_1970}, setting the confidence level set to 95\%, the population portions to 0.5 and the corresponding accuracy to 15\%.

To compare the performance of our testability estimates with the performance of the  estimates that can be done with traditional software metrics we used the tool CK\footnote{the tool CK is available at \url{https://github.com/mauricioaniche/ck}}~\cite{aniche-ck} to collect the 7 metrics \textit{Loc}, \textit{Rfc}, \textit{Cbo}, \textit{Fan-out}, \textit{Fan-in}, \textit{Cbo-modified} and \textit{Wmc}, for each subject method.
\textit{Loc} is the number of lines of code in the method. \textit{Rfc} is the number of unique method invocations done within the method. \textit{Cbo} is the number of non-primitive data types used in the method.  \textit{Fan-out} is the number of unique classes on which the method depends via method calls.
\textit{Fan-in} is the number of other methods that call the method within the same class. \textit{Cbo-modified} is the sum of \textit{Fan-out} and \textit{Fan-in}.  \textit{Wmc} is the number of branch instructions within the method or 1 for no branch.

\subsection{Results}
\subsubsection{Conclusiveness (RQ1)}\label{sec:RQ1}
Table \ref{tab:nomCovered} reports, for each of the three Java projects (column \textit{Project}) and set of subject methods (column \textit{Subjects}), 
the number of methods for which we achieved conclusive estimations (column \textit{Conclusive}) and the corresponding portion (column \textit{Portion}).  

\begin{table}[htbp]
\caption{\label{tab:nomCovered}Conclusive testability estimations}
\begin{center}
\begin{tabular}{crrr}
\scriptsize
\textbf{Project}& \textbf{Subjects}& \textbf{Conclusive} & \textbf{Portion}\\
\hline
\textit{JFreeChart} & 246 & 206 & 84\%\\
\textit{Closure Compiler} & 111 & 68 & 61\%\\
\textit{OpenMRS} & 241 & 141 & 59\%\\
\end{tabular}
\end{center}
\end{table}

The portion of inconclusive estimations is evidently not negligible, ranging between 16\% and 41\% across the three Java projects. The inspection of the methods with inconclusive estimations revealed that, as we expected, many subject methods were not hit with any test case from EvoSuite since they depended on inputs that EvoSuite cannot generate due to limitations of its current implementation. For example, we identified several methods that take files and streams as inputs (e.g., parameters of type \textit{ObjectInputStream}) that EvoSuite does not currently handle.

We remark that EvoSuite is a research prototype, though very popular in the community of researchers that work on test generation, and we did not expect it to be perfect.
Tuning our technique with further test generators or even ensembles of test generator (as well as experiencing with further mutation analysis tools other than PIT) is an important milestone for our technique to make its way to practice, and definitely the most relevant next goal in our research agenda.
But we also underline the importance of studying the merit of our novel proposal for the cases in which we could indeed achieve conclusive results with the current implementation, which admittedly is our main objective in this paper.

\subsubsection{Correlation with Test Case Complexity (RQ2)}
For the research questions RQ2 and RQ3 we focused on the subject methods for which our technique yielded conclusive results. 

\begin{table*}
\caption{Correlations between testability estimates, static metrics and test case complexity}
\label{tab:comparisonCorr}
\scriptsize\begin{center}
\setlength{\tabcolsep}{4pt}
\begin{small}Legenda: T=Testability, L=Loc, R=Rfc, C=Cbo, CM=CboModified, FI=FanIn, FO=FanOut, W=Wmc, RfcTest=Rfc on test cases.\end{small}\\~\\
\begin{tabular}{r|
cccccccc|
cccccccc|
cccccccc}

&\multicolumn{8}{c|}{\textbf{JFreeChart}}&\multicolumn{8}{c|}{\textbf{Closure Compiler}}&\multicolumn{8}{c}{\textbf{OpenMRS}} \\ 
 & T & L & R & C & CM & FI & FO & W & T & L & R & C & CM & FI & FO & W & T & L & R & C & CM & FI & FO & W \\
\hline 
Loc & -0.16 & & & & & & & & -0.25 & & & & & & & & -0.28 & & & & & & & \\ 
Rfc & -0.44 & 0.64 & & & & & & & - & 0.57 & & & & & & & -0.42 & 0.72 & & & & & & \\ 
Cbo & -0.49 & 0.41 & 0.66 & & & & & & - & 0.33 & 0.69 & & & & & & -0.33 & 0.40 & 0.49 & & & & & \\
CboModified & -0.40 & 0.51 & 0.71 & 0.66 & & & & & -0.31 & 0.24 & 0.43 & 0.36 & & & & & -0.27 & 0.43 & 0.60 & 0.28 & & & & \\ 
FanIn & - & - & -0.20 & -0.28 & 0.15 & & & & - & - & - & - & 0.56 & & & & 0.20 & - & - & -0.30 & 0.38 & & & \\ 
FanOut & -0.43 & 0.50 & 0.76 & 0.77 & 0.93 & -0.16 & & & -0.29 & 0.39 & 0.85 & 0.72 & 0.55 & - & & & -0.44 & 0.51 & 0.78 & 0.57 & 0.74 & -0.19 & & \\ 
Wmc & -0.18 & 0.85 & 0.51 & 0.34 & 0.48 & - & 0.45 & & - & 0.82 & 0.45 & - & - & - & 0.26 & & - & 0.77 & 0.51 & 0.32 & 0.36 & - & 0.43 & \\ 
\hline 
RfcTest & -0.51 & 0.21 & 0.50 & 0.45 & 0.44 & - & 0.49 & 0.16 & -0.41 & - & - & - & 0.23 & - & 0.30 & - & -0.45 & 0.33 & 0.47 & 0.48 & 0.33 & -0.17 & 0.53 & -\\ 
\end{tabular}
\end{center}
\end{table*}

Table \ref{tab:comparisonCorr} reports the correlation (as the Spearman rank correlation coefficient\footnote{The Spearman rank correlation coefficient indicates the extent to which the ranking of the subjects with respect to an indicator produces a good approximation of the ranking  with respect to the other indicator. 
The table also reports the correlation between the testability estimations and the 7 static software metrics that we measured with the tool CK, and the correlation of those 7 metrics between them and with \textit{RfcTest}. The correlation value ranges between -1 and 1, being 1 an indication of perfect correlation (same ranking), -1 and indication of perfect anti-correlation (same inverse ranking) and 0 an indication of no correlation (completely different ranking).})
between our testability estimations, the 7 static software metrics that we measured with the tool CK, and the development complexity of the test cases (measured as the metric \textit{RfcTest}) for the subjects methods in each considered Java project. 
Each cell in the table represents the correlation between the metrics indicated in the titles of the corresponding column and row, respectively. For example, the column \textit{T} represents the correlations between our testability estimates and all other metrics, and the row \textit{RfcTest} represents the correlation of all possible metrics (including our testability estimates) with the development complexity of the cases.
All reported correlation values were computed with R. The missing correlation values (indicated as \emph{dash} symbols in the table) refer to cases for which we did not find support for statistical significance (p-values greater  than 0.05).

We observe that:
\begin{itemize}
\item our testability estimations have a moderate correlation with \textit{RfcTest} for the sets of subject methods of all the considered projects (\textit{JFreeChart}: 0.51,  \textit{Closure Compiler}: 0.41 and \textit{OpenMRS}: 0.45). 
\item Our testability estimates yielded the best correlation with \textit{RfcTest} for the methods of \textit{JFreeChart} and \textit{Closure Compiler}, and the fourth best correlation  for the methods of \textit{OpenMRS}.
\item Our testability estimates have weak correlation with the size of the methods measured as the lines of code (top-left correlation value, row \textit{Loc}, in the three value sets in the table). 
\item The other static metrics resulted in significantly higher correlations with \textit{Loc} (columns \textit{L} in the table) than \textit{Testability}, with the only exceptions of 
\textit{CboModified} in project \textit{Closure Compiler} (where however \textit{CboModified} has only a weak correlation with \textit{RfcTest}).
\end{itemize}

In summary the findings confirm that our testability estimates may contribute to explain the variability in the complexity of the test cases,
while capturing a different phenomenon than the size of the software.
The other software metrics also correlate with the test complexity, sometimes with comparable strength as our testability  estimates, but their independence from  \textit{Loc} is questionable.
Overall, these findings motivate us to explore the possible synergies between our testability estimates and the static metrics.

\subsubsection{Synergy with Static Software Metrics (RQ3)}
We evaluated the performance of the 7 testability indicators obtained by combining each static software metrics with our testability estimates. For each static metric, we obtained the combined  indicator as the average ranking of the two rankings yielded by the static metric and our testability estimates, respectively, for the subject methods.
For the static metrics that are anti-correlated with the testability estimates (all but \textit{FanIn}, see Table \ref{tab:comparisonCorr}) we reversed the testability rankings before computing the combined indicators.

In this study we considered also the methods for which our technique resulted in inconclusive estimates. Since the static metrics are generally available for all methods, and we aim to evaluate if we can benefit from the static metrics in combination with the testability estimates, it makes sense to include those methods as well. For the methods with inconclusive testability estimates, we obtained the combined indicators as just the ranking value yielded by the static metrics (that is, without any additional benefit from testability estimates).

\begin{table}[htbp]
\caption{\label{tab:combinedCorr}Correlation with the combined testability indicators}
\begin{center}
\scriptsize
\begin{tabular}{r|rr|rr|rr}
&\multicolumn{2}{c|}{\textbf{JFreeChart}}&\multicolumn{2}{c|}{\textbf{Closure Compiler}}&\multicolumn{2}{c}{\textbf{OpenMRS}} \\
& base & combined  & base & combined  & base & combined \\
\hline
Loc & 0.17 & 0.34 & - & - & 0.39 & 0.47\\ 
Rfc & 0.46 & 0.53 & 0.23 & 0.32 & 0.52 & 0.55\\ 
Cbo & 0.45 & 0.53 & 0.26 & 0.33 & 0.46 & 0.51\\ 
CboModified & 0.46 & 0.54 & - & 0.35 & 0.39 & 0.52\\ 
FanIn & - & -0.38 & - & -0.31 & -0.20 & -0.32 \\ 
FanOut & 0.50 & 0.58 & 0.23 & 0.30 & 0.51 & 0.53\\ 
Wmc & 0.14 & 0.32 & - & - & 0.28 & 0.43\\ 
\end{tabular}
\end{center}
\end{table}


Table \ref{tab:combinedCorr} reports the correlation between \textit{RfcTest} and the 7 combined testability indicators (columns \textit{combined}) in comparison with the correlation obtained with respect to the base static metrics alone (columns \textit{base})  for the subjects methods in each considered Java project. We report only the correlation values supported with statistical significance (p-value less than 0.05).
The data in the table  show that the correlation yielded with the combined indicators  consistently outperformed the correlation yielded with the corresponding static metrics alone, in most cases with relevant deltas.
This confirms our main research hypothesis that our testability estimates capture a complementary dimension of testability with respect to the traditional software metrics, and can be synergistically combined with those metrics for the purpose of predicting software testability.

\subsection{Threats to validity}
The main threats to the internal validity of our experiments depend on our current choices about the test generation and mutation analysis tools (EvoSuite and PIT) embraced in our current prototype. 
On one hand, our results directly depend on the effectiveness of those tools in sampling the execution space and the fault space of the programs under test, respectively, and thus we might have observed different results if we had experienced with different tools. On the other hand, our experiments suffered of several subjects for which PIT failed to identify  sufficient sets of mutants (the subjects that belonged to the subsets \textit{Tests} in Table~\ref{tab:overallStats}, but that we excluded from the considered subsets \textit{Subjects}) and EvoSuite failed to provide sufficient test cases (the subjects that resulted in inconclusive estimates, see Table~\ref{tab:nomCovered}).

We mitigated the possible threats by focusing our analysis only on the methods that could be reasonably handled with PIT, and by explicitly pinpointing the methods for which EvoSuite allowed us to compute conclusive results. We studied the performance of our technique both as the extent of correlation of our conclusive estimates with the development complexity of the test cases, and by looking into how well our estimates can combine with traditional software metrics also with consideration of our inconclusive results.
But we are aware that we cannot make any strong claim on the efficiency of our current implementation of the technique that we propose in the paper, and  in particular on its specific characteristics of being based on EvoSuite and PIT. Our current claims are only on having provided initial empirical evidence that  
\begin{inparaenum}[(i)]
\item our approach captures a different testability dimension than the size of the software, and
\item it can complement traditional software metrics to reason on software testability in synergistic fashion.
\end{inparaenum}

As for the external validity, our findings may not generalize to other software projects other than the ones that we considered or to programming languages other than Java. In the future, we aim to replicate our experiments on further projects and implement our technique for additional programming languages.