The notion of software testability has been first introduced by Freedman~\cite{freedman_testability_1991} along with the related concepts of \emph{observability} and \emph{controllability}.
In turn, these two concepts were inherited from the fields of dynamic systems~\cite{kalman_topics_1969} and hardware testing~\cite{mccluskey_logic_1986}.
Then over time the problem of measuring software testability has been addressed with two classes of approaches, based on either \emph{fault-sensitivity}, which addresses testability by estimating the probability of revealing faults, or \emph{software metrics}, which estimate testability by conjecturing the correlations between software metrics and the testing effort. 

\emph{Fault-sensitivity approaches} were popular in the 90s, with the work about the PIE (or RIP) model~\cite{voas_improving_1991,voas_predicting_1991,voas_pie_1992,voas_software_1995,bologna_object-oriented_1996,demillo1991constraint}. PIE stands for propagate, inject and execute, which are the three main stages of the fault-revealing executions that must be considered to estimate testability. In practice, sensitivity analysis injects simulated faults into the code and evaluates their effect on the outputs.
Bertolino and Strigini exploited this notion of testability to study the relation between testability and reliability~\cite{bertolino_use_1996}.
Lin et al.\ proposed to use a modified version of the PIE technique, which analyzes the structure of the code instead of executing test cases~\cite{lin_estimated_1997}.
Zhao proposed a metric that quantifies the portion of a test suite that can detect specific faults under given test criteria~\cite{zhao_new_2006}.

The strength of the fault-sensitivity approaches is to refer to actual faults, the weakness is on the performance side, since the number of input data that need to be provided for sensitivity analysis is high even for small programs. For these reasons, researchers progressively moved to software metrics, which are considered more cost-effective to compute. 
Our work however shows that dynamic measures derived from observability and controllability  are important factors not subsumed by static software metrics, which should be rather considered in combination with them.  

\emph{Software metrics} derive testability indexes from metrics that capture information about the static structure of the code. These studies aim at finding a correlation between the static metrics and the testability of the analyzed software, to identify which metrics are best predictors of testability. Different research efforts studied different combination of metrics. 
Khalid et al.\ proposed static metrics that aim at  estimating the complexity of an object and evaluate their performance for testability prediction~\cite{khalid_analysis_2010}. Alshawan et al.\ proposed a set of static metrics specific to web applications~\cite{alshahwan_improving_2009}. A large body of papers refer to the so called CK metrics for object oriented software~\cite{chidamber_metrics_1994}.
Gupta et al.\ propose a fuzzy approach to integrate the CK metrics in a unique metric that should represent the testability~\cite{gupta_fuzzy_2005}.  Singh et al.\ and Zhou et al.\ used neural networks and linear regression, respectively, to predict testability to combine several software static metrics to predict testability~\cite{zhou_-depth_2012,singh_predicting_2008}. 

All research efforts share the challenge of deriving a ground truth for evaluating the goodness of the proposed techniques. 
Typically researchers referred their experiments to metrics that quantify the testing effort as the size or the complexity of test suites available in software repositories. Possible metrics include: the number of test cases, the number of lines of test code, the number of assertions, the number of all or unique method calls in test cases, and the average cyclomatic complexity of the test cases. For instance, Bruntik and van Deursen studied the correlation between object-oriented metrics and the testing effort estimated as above~\cite{bruntink_predicting_2004,bruntink_empirical_2006}. Other studies measured the test effort as the time required for completing the testing tasks~\cite{gupta_fuzzy_2005}. This datum is however seldom available as historical data in software repositories. Others referred to code coverage to evaluate testability indicators with respect the quality of the test suite.
Terragni et al.\ referred to coverage data normalized with respect to the size of the test cases~\cite{terragni_measuring_2020}. In line with previous studies, we consider the complexity of the test cases (measured as RfcTest) as ground truth of testability.