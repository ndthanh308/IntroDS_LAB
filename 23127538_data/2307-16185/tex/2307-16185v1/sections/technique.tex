 Providing testability measurements amounts to estimating \emph{the degree to which a software component facilitates its own testing}~\cite{freedman_testability_1991,voas_pie_1992,IEEE1990,InternationalStandardOrganization(ISO).2001}.
In this section we elaborate on both the intuitions and the  methodological choices that underlie our novel proposal to make these estimates.

\subsection{Intuitions and  Approach Overview}

\subsubsection{Exploiting automated test generation}
The main intuition that inspires our approach is to experience with the testability of a given piece of software by simulating the activity of crafting test cases for that software. Namely, we rely on automatically generating test cases with a test generator (in this paper we used the test generator Evosuite~\cite{fraser_evosuite_2011}).
Looking into the results from the test generator, we aim to judge the extent to which the current design is making it hard (or easy) for the test generator to accomplish test objectives against the considered software.

\subsubsection{Exploiting mutation analysis} 
We sample  possible test objectives in the form of synthetic faults injected in the target software.
We rely on \emph{mutation-based fault seeding}~\cite{demillo_hints_1978,pezze_software_2008}. 

Mutation-based fault seeding injects possible faults by referring to so-called mutation operators, each describing a class of code-level modifications that may simulate faults in the program.
For instance, the mutation operator \emph{replace-arithmetic-operators} creates faulty versions (called mutants) of a program by exchanging an arithmetic operator used in the code with a compatible arithmetic operator: it can produce a faulty version for each possible legal replacement.
In the sample Java program in Figure~\ref{fig:sample_program} we indicated some possible mutants in the comments included in the code: For example, we can create a faulty version of the program by replacing the statement at line~\ref{fig:sample_program:mut_easy} with the statement indicated in the comment at the same line. This is a possible instance of \emph{replace-arithmetic-operators}; another one is the mutant indicated in the comment at line~\ref{fig:sample_program:mut_not_observable}. The mutants indicated at lines~\ref{fig:sample_program:mut_not_baseline} and~\ref{fig:sample_program:mut_hard} refer to another possible mutation operator, \emph{replace-expressions-with-literals}, which consists in replacing a numeric expression with a compatible constant mentioned somewhere else in the code.
In this paper we used the mutation analysis tool PIT~\cite{coles_pit_2016}.

% Figure environment removed

To judge testability, we focus on each seeded fault separately, and we evaluate whether the current program, by virtue of its design, makes it hard (or easy) for the test generator to reveal that fault: If the test generator succeeds to reveal the fault, 
we infer a piece of testability evidence, under the intuition that a human could succeed as well with controlled effort; Otherwise, we might infer a non-testability evidence (though this  requires the further analysis described below,~$\S$~\ref{sec:custom_setters}).

For instance, our prototype based on Evosuite easily reveals the mutant at line~\ref{fig:sample_program:mut_easy} of  Figure~\ref{fig:sample_program}, e.g., with a test case like
\begin{lstlisting}[language=Java,numbers=none]
SampleProg p = new SampleProg();
p.setScale(0);
assertEquals(0, p.getScale();
\end{lstlisting}
whose execution fails against the mutant, but not for the original program. We thus infer 
a piece of testability evidence after observing that Evosuite easily reveals this mutant. 
Conversely, revealing the mutant at line~\ref{fig:sample_program:mut_hard} requires  a test case carefully tuned on several interdependent class methods, which Evosuite consistently fails to generate. A test case that could reveal this mutant would be resemblant to the following one,
\begin{lstlisting}[language=Java,numbers=none]
SampleProg p = new SampleProg();
p.setMode(10); // Hit mutant iff this.state = 10...
p.setScale(20000); // ...and this.scale > 1000000...
p.setSensor(-5); // ...and this.sensor < 0 
p.updtState(); // ...when executing updtState().
assertEquals(-1, p.currState();
\end{lstlisting}
which requires an arguably non-negligible amount of mental and manual effort also for a human tester. We infer a piece of non-testability evidence after observing that the test generator is unable to reveal this latter mutant.

Eventually, we aggregate the testability and non-testability evidence across the seeded faults of the piece of software of interest, to estimate the degree of testability of that software: The larger the amount of testability (resp. non-testability) evidence, i.e., many mutants are easy (resp. hard) for the test generator to reveal, the higher (resp. lower) the estimated degree of testability.

\subsubsection{Exploiting testability-facilitated APIs as baseline}
\label{sec:custom_setters}

When inferring non-testability evidences as above, we must pay attention that the quality of our estimations could be jeopardized by intrinsic limitations of the approaches (and ultimately the tools) to which we refer for generating the test cases.
In particular, we aim to \emph{avoid non-testability judgements that can derive from intrinsic limitations of the  test generator}, rather than testability issues.

For example, a test generator that is not able to construct some types of data structures, or does not handle test data from files or network streams, will systematically miss test cases for any fault that depends on those types of test data, regardless of actual testability issues of the software under test.
Evosuite consistently fails to hit the mutant at line~\ref{fig:sample_program:mut_not_baseline} of Figure~\ref{fig:sample_program}, simply because manipulating the file system (to set a proper file \texttt{/config.conf}) is not part of its functionality.

To acknowledge cases of this type, our approach constructively discriminates the subset of seeded faults (out of the ones provided by the mutation-analysis tool) for which we can acquire a \emph{sufficient evidence} (not necessarily a proof) that they are not out of the scope of the considered test generator.
We refer to the resulting subset of mutants as the \emph{baseline mutants}, since they provide the actual baseline for us to judge the testability evidences.

In our approach, the baseline mutants are those that the test generator either can already reveal or could reveal if it has the freedom to both directly assign any state variable in the program, and directly inspect the infected execution states. 
We enable this capability by 
\begin{inparaenum}[(i)]
    \item augmenting the program under test with custom setters for all the state variables of any module in the program, and
    \item recording the mutants as revealed-mutants as soon as they get executed, even if there is not a failing assertion in the test cases.
\end{inparaenum}
We refer to this simplified setting of the testing problem that we submit to the test generator as \emph{testing the program with testability-facilitated APIs}.

For example, in the case of the program in Figure~\ref{fig:sample_program}, we equip the program with the following set of custom setters:
\begin{lstlisting}[language=Java,numbers=none]
public void _custom_1__(int i) {this.state = i;}
public void _custom_2__(int i) {this.scale = i;}
public void _custom_3__(int i) {this.mode = i;}
public void _custom_4__(int i) {this.sensor = i;}
\end{lstlisting}

In the simplified setting, Evosuite easily executes 
\begin{inparaenum}[(i)]
    \item the mutant at line~\ref{fig:sample_program:mut_easy}, e.g., with the test case that we already discussed above, 
    \item the mutant at line~\ref{fig:sample_program:mut_hard}, e.g., with a test case like
%
\begin{lstlisting}[language=Java,numbers=none]
SampleProg p = new SampleProg();
p._custom_1__(-3); // Sets state with custom setter
assertEquals(-1, p.currState();
\end{lstlisting}
%
that suitably exploits the custom setter \texttt{custom\_1\_\_} to workaround the \emph{hard-for-testing API} of the original program for controlling a negative value of \texttt{this.state},
and \item  the mutant at line~\ref{fig:sample_program:mut_not_observable}, e.g., with a test case like
%
\begin{lstlisting}[language=Java,numbers=none]
SampleProg p = new SampleProg();
p.getScale();
\end{lstlisting}
%
that makes the mutant generate an infected execution state at line~\ref{fig:sample_program:mut_not_observable}, even if Evosuite cannot generate a proper assertion for it.
\end{inparaenum}
However Evosuite cannot anyway hit the mutant at line~\ref{fig:sample_program:mut_not_baseline} that remains out of the scope of the test generator, regardless of the availability of the custom setters.
Thus, we eventually consider as baseline mutants only the mutants at lines~\ref{fig:sample_program:mut_easy}, \ref{fig:sample_program:mut_not_observable} and~\ref{fig:sample_program:mut_hard}, but not the one at line~\ref{fig:sample_program:mut_not_baseline}. 

In summary, our testability judgements are made by generating test cases for both the original program and the program augmented with the custom setters, and mutually crosschecking both sets of test results. We infer testability evidences upon observing that the test generator successfully generates test cases that reveal mutants in the original program, e.g., the mutant at Figure~\ref{fig:sample_program}, line~\ref{fig:sample_program:mut_easy}. We infer non-testability evidences out of the inability of the test generator to reveal baseline mutants, e.g., the mutants at lines~\ref{fig:sample_program:mut_hard} and~\ref{fig:sample_program:mut_not_observable}. But our estimations dismiss the information about the non-baseline mutants (as the one at line~\ref{fig:sample_program:mut_not_baseline}) conjecturing that the test generator could not address those mutants regardless of the testability of the program.

We are aware that, technically speaking, using the testability-facilitated APIs may lead us to generate some input states that are illegal for the original program. Nonetheless, we embrace this approach heuristically: observing faults that the test generator can hit only with the testability-facilitated APIs suggests restrictive designs of the program APIs, which may pinpoint testability issues.

\subsection{The Technique}\label{sec:technique}
Figure~\ref{fig:technique} illustrates the workflow by which our technique exploits automated test generation (left part of the figure) and mutation analysis (middle part of the figure) in order to judge testability evidences (right part of the figure). 

% Figure environment removed

The input is a given program under test, which is indicated at the top-left corner in the figure, and the result is a set of testability evidences, classified as either controllability evidences or observability evidences, as indicated at the rightmost side of the figure. The blocks named \textit{Test Generation} indicate test generation activities.
The blocks named \textit{Mutation Analysis} indicate mutation analysis activities.
The block named \textit{Enrich APIs in program} augments the program under test with the testability-facilitated APIs, as we introduced in the previous section. The block \emph{Prune non original API from tests} removes the calls to the testability-facilitated APIs from the test cases generated for the program augmented with the testability-facilitated APIs, to obtain additional test cases (and thus further testability evidence) for the original version of the program, as explained below.
The circles that contain the \textit{+} symbol indicate post-processing for merging the generated test suites into a single test suite. The circles that contain the \textit{$\times$} symbol indicate post-processing of the data derived from mutation analysis to derive controllability and observability evidences.
The arrows specify the inputs and the outputs of each activity.

We have currently implemented the entire workflow of Figure~\ref{fig:technique} for programs in Java as a fully automated process scripted in Bash and Java.
Below we explain all details of our approach for the three phases of the workflow.

\subsubsection{Test Generation}
Our current implementation generates test cases with the test generator EvoSuite that exploits a \emph{search-based test generation algorithm} to generate test cases for Java classes~\cite{fraser_evosuite_2011}. 
Given a Java class and a set of code coverage criteria, EvoSuite starts with randomly sampling a first set of possible test cases for the class, and then iterates through evolving the test cases multiple times by applying random changes, while searching for sets of test cases that optimize the given code coverage criteria.
Furthermore, it generates assertion-style oracles on the observed outputs.
 
With reference to the \textit{Test Generation} blocks in Figure~\ref{fig:technique}, our technique runs EvoSuite against both the program under test and its augmented version \textit{P'}. 
Our implementation of the block \textit{Enrich APIs in program} obtains the augmented program \textit{P'} by enriching the interfaces of all classes with custom setters for any class variable declared in the code.
In Figure~\ref{fig:technique} we denoted as \textit{TestsP} and \textit{TestsP'} the test suites generated as result of those EvoSuite runs, respectively.
The test suite \textit{TestsP'} generated against \textit{P'} indicates  program behaviors that EvoSuite could provably exercise, possibly with the help of facilitated APIs. At the same time, the test suite \textit{TestsP'} implicitly captures the program behaviors that the test generation algorithm of EvoSuite is unlikely to exercise, since it failed even when facilitated by the capability to set the input state independently from the constraints encoded in the program APIs.
The comparison between the test suites \textit{TestsP'} and \textit{TestsP} indicates program behaviors that arguably were hard to exercise due specifically to the constraints encoded in the APIs, that is, behaviors that do not belong to  \textit{TestsP} while being in \textit{TestsP'}.

For each of the \textit{Test Generation} blocks in Figure~\ref{fig:technique}, our technique runs EvoSuite for a maximum time budget that depends on the size of the class, considering a minimum time budget of two minutes for the smallest classes in the considered project and a maximum time budget of 20 minutes for the largest classes, while linearly scaling the time budget for the classes of intermediate size.

Furthermore, our technique acknowledges the dependency of the search-based algorithm of EvoSuite from the different code coverage criteria that the tool allows as possible fitness functions, and from the intrinsic randomness that can naturally make EvoSuite generate different sets of test cases at different runs. Aiming to exercise as many program behaviors as possible, we set EvoSuite to address  all available fitness functions, i.e, line coverage, branch coverage, output coverage, exception coverage, and mutation coverage.

To get rid of the confounding effect of the differences between the test suites \textit{TestsP} and \textit{TestsP'} that might be just due to randomness, we constructively merge those test suites as follows.
We test the original program with both the test cases from \textit{TestsP} and the ones from \textit{TestsP'$_{noapi}$}, i.e., the test cases that either were generated in \textit{TestsP'} but still did not use any custom setter, or could be adapted from test cases in \textit{TestsP'} by commenting the calls to the custom setters (Figure~\ref{fig:technique}, block \textit{Prune non original APIs from tests})\footnote{\textit{TestsP'$_{noapi}$} represents test cases that Evosuite could generate also for the original program. Note that the custom setters can be safely commented without breaking the syntactic validity of the test cases.}. 
For similar reasons, all the available test cases must be accounted among the ones that EvoSuite could generate for \textit{P'}, that is, \textit{TestsP}, \textit{TestsP'} and \textit{TestsP'$_{noapi}$}.

\subsubsection{Mutation Analysis}
We use the mutation analysis tool PIT to both seed possible faults of the program under test~\cite{coles_pit_2016}, and characterize the generated test suites according to their ability to execute and reveal those seeded faults. 
This is the information that we will use in the next phase to judge the testability evidences that the generated test suites provided for the program under test.

PIT seeds faults in the program under test according to the mutation operators described in the documentation of the tool.\footnote{\url{https://pitest.org/quickstart/mutators/}} We specifically considered the set of mutation operators that PIT advises as the "stronger" group, which includes 13 mutation operators that address several types of  mutations at the level of the arithmetic operators, the comparison operators, the logic operators, the return values and the if and switch statements in the programs\footnote{We did not consider the larger set of mutation operators that PIT refers to as the "all" group because either they are marked as \emph{experimental} in the documentation, or our initial experiments showed that they result most often in duplicating mutants that we already obtain with the operators of the "stronger" group.}. 

PIT monitors the execution of the test suites against the mutants that it computes according to the selected mutation operators, and classifies the mutants as either \emph{revealed}, \emph{executed} or \emph{missed}. PIT classifies a mutant as revealed, if at least a test case produces a different result when executed against the original program or the mutant program, respectively. That is, (i) the test case executes with no exception and raises an exception for either program, or (ii) it raises different exceptions for either program, or (iii) it passes all test oracles and fails for at least a test oracle for either program, or (iv) it fails with respect to different test oracles  for either program.
Our technique considers the  assertion-style oracles that EvoSuite generated in the test cases.\footnote{We instructed EvoSuite to generate assertions for all program outputs encompassed in the test cases (that is with the option \texttt{assertion\_strategy=ALL}), since we aim to reveal as many mutants as possible, even if the test cases could become large.} PIT classifies a mutant as executed, if it could not classify the mutant as revealed, but there is at least a test case that executes the code in which the corresponding fault was injected.
PIT classifies a mutant as missed if it could not classify it neither as revealed nor as executed. 

In Figure~\ref{fig:technique}, the two blocks \textit{MA} indicate that our technique executes PIT for the test suites that we generated with EvoSuite for both the program under test and its augmented version\footnote{When running PIT on \textit{P'}, we do not inject mutations in the API methods that we artificially added to obtain exactly the same set of mutants for both programs \textit{P} and \textit{P'}.}. As result we collect:
\begin{itemize}
    \item the \textit{baseline mutants}, i.e., the mutants that are executed with the test cases run against for \textit{P'}. 
    \item the \textit{executed mutants}, i.e., the mutants that are executed with the test suite generated for the original program. These mutants were provably executed with EvoSuite with the original program APIs.
    \item the \textit{revealed mutants}, i.e., the mutants that are revealed with the test suite generated for \textit{P'}. These mutants were provably revealed with actual assertions within at least a test case in which they could be successfully executed.
\end{itemize}

\subsubsection{Testability Evidences}
Based on the results of mutation analysis, we look for indications of the testability of the program under test. Specifically, we first judge the testability of the program with respect to the testing goal that each mutant represents: do the results provide evidence that \emph{the program under test facilitates its own testing} with respect to the goal of revealing the seeded fault that each mutant represents?
By answering yes or no to this question we infer a testability evidence or an  evidence of non-testability, respectively, for each specific mutant that belongs to the set of baseline mutants computed as above.

We further split the testability verdicts into controllability verdicts and observability verdicts. Controllability refers to whether or not the results of mutation analysis provide evidence that the program under test facilitates the execution of the seeded faults. We annotate a controllability evidence for each mutant that mutation analysis marks as \textit{executed} for the original program under test, i.e., the set of executed mutants computed as above. 
With respect to these mutants, the test cases that we generated with EvoSuite provide empirical evidence that the program under test, with its original APIs, provides sufficient means of controlling the assignment of the program inputs and the program states for test cases to achieve the execution of those seeded faults. 
On the other hand, we annotate a non-controllability evidence for each baseline mutant not marked as executed for the original program. 

Observability refers to whether or not the results of mutation analysis provide evidence that the program under test facilitates to reveal the seeded faults. We annotate an observability or non-observability evidence for each baseline mutant that mutation analysis marks or does not mark, respectively, as \textit{revealed}.
The observability evidences correspond to empirical evidence that the program under test provides sufficient means for the seeded faults to be observed from the test cases.  

We aggregate the testability evidences, i.e., both the controllability and the observability evidences, for the mutants that correspond to faults seeded at the same line of code, to prevent the unbalanced skewing of our results towards those instructions that were associated with higher numbers of mutants than other instructions.
For each line of code associated with at least a baseline mutant, we infer a unitary controllability (resp. observability) evidence if more than half of the associated baseline mutants vote as controllability (resp. observability) evidences; or we infer unitary non-controllability (resp. non-observability) evidence otherwise. 

\subsection{Estimating Testability}
We refer to the collected testability and non-testability evidences to reason on the testability of given parts (e.g., software components) of the program under test.
For instance, in the experiments of this paper, we aimed to estimate testability values that represent the  testability of the methods that belong to a Java program. 

To this end, we first map each target piece of software (e.g., each method) to the subset of testability evidences that relate with that software, and then aggregate those testability evidences into a testability value measured in the interval $[0, 1]$, where 0 and 1 correspond the minimum and the maximum testability values that we can estimate for a component, respectively. 
 
Let $C$ be a software component that belongs to the program under test, and let $contr^+(C)$, $contr^-(C)$, $obs^+(C)$ and $obs^-(C)$ be the subsets of positive and negative controllability and observability evidences, respectively, that we mapped to the component $C$, out of the unitary evidences collected with the technique that we described in the previous section. Then, by referring to the size of those sets, we estimate the controllability and the  observability of the component $C$ as:

\[Controllability(C) = \frac{|contr^+(C)|}{|contr^+(C)| + |contr^-(C)|},\]

\[Observability(C) = \frac{|obs^+(C)|}{|obs^+(C)| + |obs^-(C)|}.\]

\noindent Finally we estimate the  testability of the component $C$ as the combination of its controllability and its observability, namely, as the arithmetic product of the two:

\[Testability(C) = Controllability(C) \times Observability(C).\]

Furthermore, we acknowledge that the testability evidences collected with our technique can be sometimes insufficient to calculate reliable estimates for some program components. In particular, we reckon this to be the case if our technique was unable to significantly sample the execution space of the component.
When reasoning on the testability of a piece of software, we mark our estimates as \emph{inconclusive} if the portion of lines of code for which we successfully computed testability evidences was not a representative sample out of the component's lines of code that were associated with some mutants.
We ground on the classic theory of small sample techniques~\cite{krejcie_determining_1970}. 
As a consequence, the possibility of producing inconclusive results for some components is a possible limitation of our technique. Depending on the actual implementations of the technique, the concrete manifestation of this limitation boils down to the characteristics of the tools with which we instantiate the test generation tool and mutation analysis phases. Explicitly pinpointing the conclusiveness of the estimates aims to alleviate the impact of such limitation.