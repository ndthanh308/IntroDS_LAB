Software testing is a key activity of the software life-cycle that requires time and resources to be effective.
In this paper we focus on the \emph{testability} of the software, which is defined as \emph{the degree to which the design of software artifacts supports or hardens their own testing}~\cite{InternationalStandardOrganization(ISO).2001,IEEE1990}, and which can correlate in many relevant ways with the cost of the testing activities and ultimately with the effectiveness of those activities for revealing the possible faults.
For example, the availability of estimates on the testability of the software under test and the components therein can support test analysts in
anticipating the cost of testing, tuning the test plans, or pinpointing components that should undergo refactoring before testing.

At the state of the art the problem of estimating software testability has been addressed with two main classes of approaches: \emph{fault-sensitivity approaches}, which estimate testability by focusing on the probability of executing and revealing possible faults, and approaches based on \emph{software metrics}, which conjecture the correlation between the testing effort and the static structures of the code  characterized with software metrics as, for example, the cyclomatic complexity or the lines of code.

The \emph{fault-sensitivity approach} grounds on the seminal work of Voas and colleagues on the \textit{execute-infect-propagate} (PIE) model of fault sensitivity~\cite{voas_improving_1991,voas_predicting_1991,voas_pie_1992,voas_software_1995,bologna_object-oriented_1996}. The PIE model defines fault sensitivity as the combined probability of executing faulty locations, infecting the execution state and propagating the effects of the infection to some observable output. 
High fault sensitivity can be a proxy of high testability, and vice-versa.
However, doing actual estimates requires to observe the frequency of execution of the program locations with very thorough test suites, hardly available before testing~\cite{voas1993empirical}. As a matter of fact, after the initial momentum in the nineties, this approach has never made its way to established testability estimation tools and has been progressively abandoned.

The \emph{software metrics approach} is the main focus of most past and recent research on software testability~\cite{bruntink_predicting_2004,gupta_fuzzy_2005,bruntink_empirical_2006,singh_predicting_2008,badri_exploring_2010, khalid_analysis_2010,singh_predicting_2010,badri_empirical_2011,badri_empirical_2012,zhou_-depth_2012,da_cruz_empirical_2017,toure_predicting_2018,terragni_measuring_2020,alshahwan_improving_2009,khan_metric_2009,kout_empirical_2011}.

Most research effort focuses on object-oriented programs, by using metrics that capture information about the static structure of the code at the class-level or method-level (as for example the Chidamber and  Kemerer's metrics~\cite{chidamber_metrics_1994}). The software metrics that have the potential of being good testability predictors are derived by investigating the correlation between the metrics and the amount, the complexity and the thoroughness of the associated test cases.

We observe that a potential threat to the way these software metrics have been investigated is the fact that many of these studies are performed only on a, oftentimes small, sample of projects, and this could lead to generalization problems.
For instance, several studies report contrasting results: Bruntink et al. \cite{bruntink_predicting_2004, bruntink_empirical_2006} do not identify WMC and LCOM as good predictors differently to other studies \cite{badri_empirical_2011, badri_empirical_2012, gupta_fuzzy_2005, singh_predicting_2008, da_cruz_empirical_2017} and NOC is identified as a good predictor only by Singh et al. \cite{singh_predicting_2008}, while others have not found such correlation \cite{bruntink_predicting_2004, bruntink_empirical_2006,badri_empirical_2012,terragni_measuring_2020}.

In this paper we introduce and discuss the novel idea of not relying on the possible correlation between static metrics and testability, but to \emph{directly estimate the testability degree of a software by sampling the test space and the fault space} of the software, and therein collect empirical \emph{evidence of the easiness or hardness} to accomplish effective testing.
According to our approach, the stronger the evidence that we can collect about hard-to-test faults in a software component, the higher the probability that its design is not facilitating testing enough.
Drawing on this idea, we rely on a search-based test generation tool to automatically generate test cases~\cite{fraser_evosuite_2011}, and refer to mutation-based fault seeding to sample possible faults~\cite{coles_pit_2016}. We then refer to the generated test cases and the seeded faults to  \emph{extrapolate the testability evidence}.

We empirically studied the effectiveness of our testability estimates with respect to 598 class methods of three large software projects in Java. In particular we analyzed to what extent our estimates correlate with the development complexity of the test cases that were available in the considered projects, and we compared the correlation yielded by our estimates with the one yielded by a selection of popular software metrics for object-oriented programs. 
Our main findings were that our testability estimates contribute to explain the variability in the development complexity of the test cases by capturing a different phenomenon than the metrics on the size and the structure of the software.
Furthermore, motivated by such findings, we explored the combination of our metric with the software metrics, revealing synergies to improve the testability estimates.
Thus, our findings support the research hypothesis that it is viable and useful to estimate testability based on empirical observations collected with automatically generated test cases. We remark that we do not claim that  testability estimates based on software metrics must be avoided and replaced with our testability estimates, but rather the two approaches could be used synergically to improve the accuracy of the estimates.

The paper is organized as follows. 
Section \ref{sec:approach} presents our novel approach to estimate testability. Section \ref{sec:experiments} presents our experiments.
Section \ref{sec:related_works} surveys the relevant related work. Section \ref{sec:conclusions} summarizes the main contributions of this paper.