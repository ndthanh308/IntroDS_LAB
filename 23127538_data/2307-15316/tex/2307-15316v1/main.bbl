% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Whitepaper_Huawei}
W.~Tong and P.~Zhu, Eds., \emph{6G: The Next Horizon: From Connected People and
  Things to Connected Intelligence}.\hskip 1em plus 0.5em minus 0.4em\relax
  Cambridge University Press, 2021.

\bibitem{6G_Letaief}
K.~B. Letaief, Y.~Shi, J.~Lu, and J.~Lu, ``Edge artificial intelligence for
  {6G}: Vision, enabling technologies, and applications,'' \emph{IEEE J. Sel.
  Areas Commun.}, vol.~40, no.~1, pp. 5--36, 2021.

\bibitem{Dis_learning_Chen}
M.~Chen, D.~Gündüz, K.~Huang, W.~Saad, M.~Bennis, A.~V. Feljan, and H.~V.
  Poor, ``Distributed learning in wireless networks: Recent progress and future
  challenges,'' \emph{IEEE J. Sel. Areas Commun.}, vol.~39, no.~12, pp.
  3579--3605, 2021.

\bibitem{Split_inf_Shao}
J.~Shao and J.~Zhang, ``Communication-computation trade-off in
  resource-constrained edge inference,'' \emph{IEEE Commun. Mag.}, vol.~58,
  no.~12, pp. 20--26, 2020.

\bibitem{Model_downloading_KB}
K.~Huang, H.~Wu, Z.~Liu, and X.~Qi, ``In-situ model downloading to realize
  versatile edge {AI} in {6G} mobile networks,'' \emph{to appear in IEEE
  Wireless Commun.}, 2023.

\bibitem{3GPP}
\BIBentryALTinterwordspacing
J.~Shen, ``{5G System (5GS); Study on traffic characteristics and performance
  requirements for AI/ML model transfer},'' {3rd Generation Partnership Project
  (3GPP)}, Technical Specification (TS) 22.874, June~21 2021. [Online].
  Available:
  \url{https://portal.3gpp.org/desktopmodules/Specifications/SpecificationDetails.aspx?specificationId=3721}
\BIBentrySTDinterwordspacing

\bibitem{JSCC_Deniz}
T.-Y. Tung, D.~B. Kurka, M.~Jankowski, and D.~Gündüz, ``{DeepJSCC-Q}:
  Constellation constrained deep joint source-channel coding,'' \emph{IEEE J.
  Sel. Areas Inf. Theory}, 2022.

\bibitem{Device_edge_Zhou}
W.~Shi, Y.~Hou, S.~Zhou, Z.~Niu, Y.~Zhang, and L.~Geng, ``Improving device-edge
  cooperative inference of deep learning via 2-step pruning,'' in \emph{Proc.
  IEEE Conf. Comput. Commun. Workshops (INFOCOM WKSHPS)}, Paris, France,
  April~29 -- May~2 2019, pp. 1--6.

\bibitem{Edge_AI_Li}
E.~Li, L.~Zeng, Z.~Zhou, and X.~Chen, ``Edge {AI}: On-demand accelerating deep
  neural network inference via edge computing,'' \emph{IEEE Trans. Wireless
  Commun.}, vol.~19, no.~1, pp. 447--457, 2019.

\bibitem{Cooper_inference_Shao}
J.~Shao, Y.~Mao, and J.~Zhang, ``Task-oriented communication for multidevice
  cooperative edge inference,'' \emph{IEEE Trans. Wireless Commun.}, vol.~22,
  no.~1, pp. 73--87, 2023.

\bibitem{Bottleneck++_Shao}
J.~Shao and J.~Zhang, ``Bottlenet++: An end-to-end approach for feature
  compression in device-edge co-inference systems,'' in \emph{Proc. IEEE Int.
  Conf. Commun. Workshops (ICC Workshops)}, Virtual Event, June~7--11 2020, pp.
  1--6.

\bibitem{Wireless_AI_Park}
J.~Park, S.~Samarakoon, M.~Bennis, and M.~Debbah, ``Wireless network
  intelligence at the edge,'' \emph{Proc. IEEE}, vol. 107, no.~11, pp.
  2204--2239, 2019.

\bibitem{PFTX_Lan}
Q.~Lan, Q.~Zeng, P.~Popovski, D.~Gündüz, and K.~Huang, ``Progressive feature
  transmission for split classification at the wireless edge,'' \emph{IEEE
  Trans. Wireless Commun.}, 2022.

\bibitem{Batch_Liu}
Z.~Liu, Q.~Lan, and K.~Huang, ``Resource allocation for multiuser edge
  inference with batching and early exiting,'' \emph{IEEE J. Sel. Areas
  Commun.}, vol.~41, no.~4, pp. 1186--1200, 2023.

\bibitem{Compression_NN_Li}
J.~Guo, J.~Wang, C.-K. Wen, S.~Jin, and G.~Y. Li, ``Compression and
  acceleration of neural networks for communications,'' \emph{IEEE Commun.
  Mag.}, vol.~27, no.~4, pp. 110--117, 2020.

\bibitem{Model_placement_Yan}
J.~Yan, S.~Bi, and Y.-J.~A. Zhang, ``Optimal model placement and online model
  splitting for device-edge co-inference,'' \emph{IEEE Trans. Wireless
  Commun.}, vol.~21, no.~10, pp. 8354--8367, 2022.

\bibitem{Foundation_Model_2021}
\BIBentryALTinterwordspacing
R.~Bommasani, D.~A. Hudson, E.~Adeli \emph{et~al.}, ``On the opportunities and
  risks of foundation models,'' Standford University, Tech. Rep., 2022.
  [Online]. Available: \url{https://crfm.stanford.edu/assets/report.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Pathway}
\BIBentryALTinterwordspacing
D.~Jeff. (2021) Introducing pathways: A next-generation {AI} architecture.
  [Online]. Available:
  \url{https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/}
\BIBentrySTDinterwordspacing

\bibitem{TL_survey}
F.~Zhuang, Z.~Qi, K.~Duan, D.~Xi, Y.~Zhu, H.~Zhu, H.~Xiong, and Q.~He, ``A
  comprehensive survey on transfer learning,'' \emph{Proc. IEEE}, vol. 109, pp.
  43--76, 2020.

\bibitem{Reassembly_2022}
X.~Yang, Z.~Daquan, S.~Liu, J.~Ye, and X.~Wang, ``Deep model reassembly,'' in
  \emph{Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)}, New Orleans, LA,
  USA, Nov~28--Dec~9 2022.

\bibitem{SNNet_2023}
Z.~Pan, J.~Cai, and B.~Zhuang, ``Stitchable neural networks,'' in \emph{Proc.
  IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)}, Vancouver B.C. Canada,
  Jun~18--22 2023.

\bibitem{muNet_2022}
A.~Gesmundo and J.~Dean, ``An evolutionary approach to dynamic introduction of
  tasks in large-scale multitask learning systems,'' \emph{arXiv preprint
  arXiv:2205.12755}, 2022.

\bibitem{Shapley_Game_Theory_2002}
E.~Winter, ``The {Shapley} value,'' \emph{Handbook of game theory with economic
  applications}, vol.~3, pp. 2025--2054, 2002.

\bibitem{ResNet_2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proc. IEEE Conf. Comput. Vis. Pattern Recognit.
  (CVPR)}, Las Vegas, NV, USA, June~26--July~1 2016.

\bibitem{ViT_2021}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image
  recognition at scale,'' in \emph{Proc. Int. Conf. Learn. Represent. (ICLR)},
  Vienna, Austria, May~3--7 2021.

\bibitem{XAI_2019}
D.~Gunning, M.~Stefik, J.~Choi, T.~Miller, S.~Stumpf, and G.-Z. Yang, ``{XAI}
  — explainable artificial intelligence,'' \emph{Science robotics}, vol.~4,
  no.~37, p. eaay7120, 2019.

\bibitem{Explain_Shapley_2019}
M.~Ancona, C.~Oztireli, and M.~Gross, ``Explaining deep neural networks with a
  polynomial time algorithm for {Shapley} value approximation,'' in \emph{Proc.
  Int. Conf. Mach. Learn. (ICML)}, Long Beach, CA, USA, Jun~9--15 2019.

\bibitem{Neuron_Shapley_2020}
A.~Ghorbani and J.~Y. Zou, ``Neuron {Shapley}: Discovering the responsible
  neurons,'' in \emph{Proc. Adv. Neural Inform. Process. Syst. (NeurIPS)},
  Vancouver, BC, Canada, Dec~6--12 2020.

\bibitem{Shapley_Value_2020}
M.~Sundararajan and A.~Najmi, ``The many {Shapley} values for model
  explanation,'' in \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, Virtual Event,
  Jul~13--18 2020.

\bibitem{power_control}
M.~Chiang, P.~Hande, T.~Lan, C.~W. Tan \emph{et~al.}, ``Power control in
  wireless cellular networks,'' \emph{Foundations and Trends{\textregistered}
  in Networking}, vol.~2, no.~4, pp. 381--533, 2008.

\bibitem{Relaxation}
C.~Y. Wong, R.~Cheng, K.~Lataief, and R.~Murch, ``Multiuser {OFDM} with
  adaptive subcarrier, bit, and power allocation,'' \emph{IEEE J. Sel. Areas
  Commun.}, vol.~17, no.~10, pp. 1747--1758, 1999.

\bibitem{cvx}
M.~Grant and S.~Boyd, ``{CVX}: Matlab software for disciplined convex
  programming, version 2.1,'' \url{http://cvxr.com/cvx}, Mar. 2014.

\bibitem{ImageNet_Russakovsky}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein \emph{et~al.}, ``{ImageNet} large scale
  visual recognition challenge,'' \emph{Int. J. Comput. Vis.}, vol. 115, no.~3,
  pp. 211--252, 2015.

\bibitem{CIFAR_Krizhevsky}
A.~Krizhevsky, G.~Hinton \emph{et~al.}, ``Learning multiple layers of features
  from tiny images,'' 2009.

\bibitem{SVHN}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng, ``Reading
  digits in natural images with unsupervised feature learning,'' in \emph{Proc.
  of Int. Conf. Neural Inform. Process. Syst. Workshops (NIPS Workshops)},
  2011.

\bibitem{Max-Min}
G.~Yu, ``On the max-min 0-1 knapsack problem with robust optimization
  applications,'' \emph{Operations Research}, vol.~44, no.~2, pp. 407--415,
  1996.

\end{thebibliography}
