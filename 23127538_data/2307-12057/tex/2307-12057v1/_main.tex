\input{_constants}
\arxiv % \review OR \arxiv OR \cameraready

\pdfoutput=1
\documentclass[10pt,twocolumn,letterpaper]{article}
\input{cvpr_header}
\begin{document}
%% TITLE
\title{\paperTitle}
\author{\authorBlock}

\twocolumn[{
\renewcommand\twocolumn[1][]{#1}%
\maketitle
\begin{center}
 \centering
 \captionsetup{type=figure}
 % Figure removed
% \vspace{-34pt}

% \caption{Despite their remarkable ability to generate plausible images from text descriptions, diffusion models fail to be faithful to multiple concepts in the input text. We identify the issues causing this pitfall, and propose a training-free method to fix them. We propose two new loss functions, attention segregation loss and attention retention loss, that only require test time optimization to drive the diffusion process and produce substantially improved generation results. We can note from these results that our method captures all key concepts in the input prompt as opposed to baseline Stable Diffusion \cite{rombach2022high}.}
	\caption{
	Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback System Overall Framework.
In \textit{Section A)}, we elucidate the operational workflow of the Chat Research Paper System. The process commences when a user submits a query concerning a specific research paper. The system initially retrieves the PDF of the paper from an online repository or, alternatively, from local storage if available. The document is then segmented into multiple portions which undergo an embedding process using OpenAI Embeddings, generating distinct query and document embeddings. Following this, a k-nearest neighbors algorithm is employed to match these embeddings, facilitating the extraction of the most relevant segments of the document in relation to the userâ€™s query. These pertinent segments, along with instructions and the user's query, are then submitted to the Large Language Model (LLM) for processing.
In \textit{Section B)}, we expound on the policy-oriented aspect of the system, which dynamically adjusts its approach based on human feedback, particularly when ambiguous queries or unsatisfactory responses from the LLM are encountered. Users have the option to escalate the level of assistance rendered by the system. At the entry level, the system employs an efficient matching algorithm and integrates a local summarization layer for document embeddings. Ascending to the intermediate level, GPT-3 is engaged to perform chunk-level filtering and summarization. At the most advanced stage, the system relaxes token window limitations and utilizes GPT-3-16k to conduct multi-page retrieval and refinement. During this phase, the embeddings are generated using the cutting-edge Davini003 model, and a robust recommendation system is employed to craft a comprehensive prompt that queries GPT-4 for sophisticated reasoning. Additionally, all intermediate outputs are retained in a memory buffer for potential utility in addressing subsequent queries.
	}
 \label{fig:overall}
\end{center}
}]

\maketitle
%%



\input{00_abstract}
\input{01_intro}
%\input{02_related}

\input{03_method}

\input{10_conclusion}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{11_references}
}
\newpage

\input{12_appendix.tex}

%\ifarxiv \clearpage \input{12_appendix} \fi

\end{document}
