\section{Method}
\label{sec:method}


\begin{algorithm}
\caption{Structure of a Scientific Paper}
\begin{algorithmic}[1]
\State \textbf{Structure} Paper
\State \Comment{Title of the paper}
\State $\text{Paper.title} \gets \text{string}$
\State \Comment{Abstract of the paper}
\State $\text{Paper.abstract} \gets \text{string}$
\State \Comment{Sections of the paper, each with heading and text}
\For{section in Paper.sections}
    \State $\text{section.heading} \gets \text{string}$
    \State $\text{section.text} \gets \text{string}$
\EndFor
\State \Comment{References cited in the paper}
\For{reference in Paper.references}
    \State $\text{reference.title} \gets \text{string}$
    \State $\text{reference.year} \gets \text{string}$
    \State $\text{reference.journal} \gets \text{string}$
    \State $\text{reference.author} \gets \text{string}$
\EndFor
\State \Comment{Figures included in the paper}
\For{figure in Paper.figures}
    \State $\text{figure.figure\_label} \gets \text{string}$
    \State $\text{figure.figure\_type} \gets \text{string}$
    \State $\text{figure.figure\_id} \gets \text{string}$
    \State $\text{figure.figure\_caption} \gets \text{string}$
    \State $\text{figure.figure\_data} \gets \text{string}$
\EndFor
\State \Comment{Digital Object Identifier of the paper}
\State $\text{Paper.doi} \gets \text{string}$
\end{algorithmic}
\label{alg:ds}
\end{algorithm}



\subsection{PDF parsing and Preprocessing}

In this section, we delve into the utilization of a robust PDF parser, specifically tailored for academic research, named `scipdf\_parser`. This parser is adept at dissecting scholarly articles and categorizing the extracted content into distinct segments, namely: 'title', 'authors', 'abstract', 'sections', and 'references' shows in \cref{alg:ds}.
Through the course of our experiments, an intriguing observation emerged regarding the inclusion of the 'references' segment. We discerned that omitting the 'references' section from the parsed content led to a discernible enhancement in matching accuracy for the majority of queries. The rationale behind this improvement is that the 'references' section is replete with titles from cited papers, which often bear a resemblance to the terminology utilized in the main article. However, these citations tend not to contribute substantially to the reasoning process, and, as such, their inclusion can inadvertently result in less meaningful matches.
Following the parsing process, the next step involves segmenting the content of the PDF into multiple chunks. This segmentation is instrumental for facilitating more granular processing of the text. By default, each chunk encompasses 150 tokens, which has been determined to be a judicious size for ensuring both computational efficiency and the preservation of contextual information necessary for subsequent processing stages.


\begin{table}[htbp]
  \centering
  \caption{Comparison of Embedding models on Linear probe classification over 7 datasets.}
  \label{tab:models}
  \begin{tabular}{lc}
    \toprule
    \textbf{Model} & \textbf{Accuracy (\%)} \\
    \midrule
    Prevs SOTA & 90.20 \\
    text-embedding-ada & 89.30 \\
    text-embedding-babbage & 91.10 \\
    text-embedding-curie & 91.50 \\
    text-embedding-davinci & 92.20 \\
    \bottomrule
  \end{tabular}
  \label{tab:embedding_acc}
\end{table}


\subsection{Context embeddings}

In our research, a critical component of our methodology is the utilization of OpenAI's embedding models to generate embeddings for queries and documents. The objective of this process is to enable efficient and accurate retrieval of sections that are highly pertinent to the queries at hand. 
Among the OpenAI text embedding models we employ are: text-embedding-ada, text-embedding-babbage, text-embedding-curie, and text-embedding-davinci. These models exhibit a hierarchical nature in terms of their capabilities with text-embedding-ada at the basic end of the spectrum, and text-embedding-davinci being the most sophisticated among them. As we progress along this hierarchy, the quality of the embeddings improves significantly, which can result in more accurate section retrieval show in \cref{tab:embedding_acc}
However, it is imperative to recognize that the enhanced capabilities of the more sophisticated models come at the expense of increased computational costs. Consequently, there is an inherent trade-off between the quality of embeddings and the computational overhead involved.
In light of this trade-off, judicious selection of embedding models is essential. In our system, we adopt text-embedding-ada as the default model for generating embeddings. This choice is rooted in its ability to offer a balance between performance and computational efficiency for a broad range of queries. Nevertheless, when the system is engaged in the 'extreme level assistance' mode - a setting that is triggered when faced with particularly challenging queries we opt to maximize all system components to their highest capacity. In this mode, we employ text-embedding-davinci owing to its superior capability to create high-quality embeddings. It is in this context that the system is willing to incur the additional computational costs to ensure optimal performance in addressing the complex nature of the queries under consideration.


\subsection{Context Retrieval}

We introduce two distinct methodologies for context retrieval, namely cosine similarity matching and k-nearest neighbors (KNN) based matching.
Cosine similarity matching operates by computing the cosine of the angle between two vectors, representing the document embeddings and the query embeddings. This measure quantifies the similarity between the embeddings, where a value of 1 denotes perfect alignment and 0 indicates orthogonality.
Let $d$ represent the document embedding vector and $q$ represent the query embedding vector. The cosine similarity $S_{cos}$ is calculated as follows:

\begin{equation}
S_{cos}(d, q) = \frac{d \cdot q}{\|d\| \|q\|}
\end{equation}

Where $\cdot$ denotes the dot product, and $\|\cdot\|$ represents the norm of a vector. The system ranks the chunks according to the cosine similarity scores and retrieves the chunks with the highest scores as the most relevant context.

KNN-based matching involves locating the k-nearest neighbors of the query embedding in the space of document embeddings. This is based on the assumption that similar content will have nearby embeddings.
Let $D = \{d_1, d_2, ..., d_n\}$ be the set of document embedding vectors and $q$ be the query embedding vector. We define a distance metric, commonly the Euclidean distance, between the query embedding and each document embedding. For any two vectors $a$ and $b$, the Euclidean distance $D_{E}$ is defined as:
\begin{equation}
D_{E}(a, b) = \sqrt{\sum_{i=1}^{n} (a_i - b_i)^2}
\end{equation}
The system retrieves the k document embeddings that have the smallest Euclidean distances to the query embedding. These are considered as the k-nearest neighbors and are regarded as the most relevant context for the given query.
In practice, selecting the appropriate value for k is crucial as it can impact the quality of the retrieved context. Additionally, while cosine similarity is often used for high-dimensional data, KNN can be more suitable for data where the intrinsic geometry is meaningful. The choice between these methods should consider the characteristics of the data and the application requirements.

\subsection{Prompt Engineering}

Prompt engineering is a critical area of research that directly influences the performance of Large Language Models (LLMs) and Question-Answering (QA) systems. The construction of effective prompt templates is essential for eliciting accurate and relevant responses from LLMs. We propose the following structured template for posing questions to the LLM:

\begin{enumerate}
  \item \textbf{Document Context}: We define the document context with the notation [docs], followed by a supporting prompt that presents the relevant document chunks to the LLM. The template for this part is as follows:
  
  \begin{quote}
  I will provide the document chunks as follows: [docs]
  \end{quote}
  \item \textbf{Instructional Context}: We denote the instructions for response composition with the notation [Instructions]. This part of the template guides the LLM on how to structure the response, use citations, distinguish between subjects, and ensure the accuracy and relevance of the content. The template for this part is as follows:
  \begin{quote}
  Instructions: Compose a comprehensive reply to the query using the provided document chunks. Cite each reference using [Page Number] notation (each document chunk begins with this number). Ensure citations are placed at the end of each sentence. If the document chunks mention multiple subjects sharing the same name, create separate responses for each. Include only information found in the document chunks, and refrain from adding extraneous details. Ensure the accuracy of the response and avoid disseminating false content. Exclude search results that are not pertinent to the question. Respond concisely and in a step-by-step manner. 
  \end{quote}
  \item \textbf{Query Context}: Finally, we denote the actual query with the notation [query]. This part of the template specifies the question and asks the LLM for a detailed response based on its findings. The template for this part is as follows:
  \begin{quote}
  Query: [query]. Please provide detailed findings in response to the query:
  \end{quote}
\end{enumerate}
This structured approach ensures that the LLM is provided with clear context, guided instructions, and the specific query, which collectively contribute to eliciting a well-formed, accurate, and informative response. Prompt engineering in this manner can substantially enhance the efficacy and applicability of LLMs in QA systems.

\subsection{Summarization}

\textbf{Performance Enhancement.} During our experiments, we observed that raw text chunks occasionally struggled to encapsulate semantic meanings or exhibit relevance, especially in the context of Type 2 questions, which often embody implicit meanings in conjunction with user queries. In light of this, we carried out a series of experiments to devise a mechanism that could distill the essential ideas from each text chunk through summarization.
In our approach, we leveraged summarization algorithms to condense text chunks, thereby capturing the crux of the content. For the entry-level assistance, we employed a pre-trained summarization model, specifically the "facebook/bart-large-cnn" model. This model was chosen for its proficiency in generating concise summaries and its relatively low computational overhead, making it suitable for initial, low-resource processing.
For intermediate-level assistance, we further elevated the summarization process by utilizing GPT-3.5. The incorporation of GPT-3.5 allowed for not only summarization but also refinement of the text chunks. The refinement process aimed to sharpen the focus of the summary, ensuring that the essential elements align more closely with the user's query.
By summarizing and refining text chunks before they are processed by the Large Language Model, our approach enhances the relevance and accuracy of the responses to Type 2 questions, which are characterized by implicit meanings. This method simultaneously alleviates the challenge of representing semantic meanings with raw text chunks, paving the way for more effective and efficient question-answering systems.

\textbf{Efficiency Improvement.} Another salient aspect of employing summarization in the processing pipeline is the notable enhancement in efficiency and reduction in computational costs. Large language models (LLMs), by design, are often constrained by token limits, and processing extensive text chunks may incur high computational expenses and protracted processing times. Summarization serves as a linchpin to circumvent these constraints.
By distilling text chunks into condensed summaries, the volume of data fed into the LLM is considerably reduced. This contraction in data not only ensures that the input remains within the token limits of the LLM but also decreases the computation required for processing. Consequently, this leads to faster response times, making the system more agile and efficient.
Furthermore, the reduction in computational requirements translates to diminished resource utilization, which in turn leads to lower operating costs. This is especially vital in scenarios where scalability and cost-effectiveness are essential attributes.
In addition, by focusing on the essence of the content through summarization, the LLM can allocate more computational resources to in-depth analysis and generating higher quality responses. This results in not just a cost-effective and efficient system, but one that is also more effective in addressing complex queries.
In conclusion, the integration of summarization into the processing pipeline is a multi-faceted strategy that boosts efficiency, reduces computational expenses, and fosters an environment conducive to generating higher quality responses in LLM-based question-answering systems.

\subsection{Policy Oriented Muti-LLMs assistance}

As delineated in Section B of Figure \ref{fig:overall}, a distinguishing innovation in our approach lies in the incorporation of a policy system which empowers users with the flexibility to choose between different levels of assistance, in order to address their inquiries more effectively. This feature is particularly germane in instances where the responses generated by the system may not meet the userâ€™s expectations or requirements.
Our system, cognizant of varying complexity in user queries, operates across three tiers of assistance, each progressively employing more sophisticated strategies to render higher quality responses. The initial two tiers primarily engage summarization techniques to distil and represent text chunks succinctly. While this approach is adept at handling queries of moderate complexity, it may falter when faced with queries demanding a deeper understanding or requiring the synthesis of information scattered across extensive text. 
For such intricate scenarios, users are accorded the option through the user interface to escalate their query to a higher level of assistance. This triggers the activation of the policy system, which navigates through the available assistance levels.
In the event that a user remains dissatisfied with the outputs of the first two tiers, our system elevates the query to the 'extreme' level of assistance. At this juncture, the system harnesses the prowess of GPT-3-16K, deploying it to perform multi-page level refinement. Significantly, at this stage, there is no imposition of context length reduction, especially if the contextual information bears high relevance to the query. This ensures a thorough examination of the content, facilitating a more nuanced and comprehensive response.
This policy-driven, multi-tiered approach is instrumental in adapting the system to the diverse range of user queries, efficiently marshalling resources, and optimizing response quality in alignment with the complexity of the inquiry at hand. The system, therefore, maintains a judicious equilibrium between efficiency and depth, tailoring its approach to meet the evolving demands of the user.


\subsection{Key Reference Matching}

The process of identifying and obtaining key references within academic papers is crucial for understanding the foundational work or significant advancements in a given field. This section presents an innovative approach, termed "Key Reference Matching," which facilitates this process through memory augmentation and advanced information retrieval techniques. Our system, predicated on the hypothesis that a user engages in multiple interactions with the system, employs a memory-enhanced technique to refine the selection of key references. It operates on the premise that users may have already engaged with the system multiple times before requesting key references. The memory component of our system plays a pivotal role in augmenting the retrieval of key references. As the user interacts with the system, the responses and document summaries generated by LLMs are cached. When the user queries for key references, the system recalls the cached summaries and combines them with the raw abstract section of the paper. For the classification of references, the system integrates the recalled summary with the abstract section, and feeds the concatenated information to an LLM. The objective is to classify and identify references that are central to the research paper. This is grounded in the assumption that well-crafted research papers typically delineate the challenges in the field, proposed solutions, performance evaluation, and the impact of the field, often citing seminal works. This memory-enhanced approach yields multiple benefits. It reduces the redundancy in processing, as the cached summaries are reused, and ensures a more comprehensive understanding by combining different segments of the paper. Furthermore, by focusing on the essential components of the paper, it prioritizes the references that are most likely to be of key significance.


\section{Ablation Study}

% Figure environment removed

\subsection{Impact of embedding methods.} Our experiment involved a comparative analysis of Sentence-BERT (SBERT) embeddings and several OpenAI embeddings, namely, text-embedding-ada, text-embedding-babbage, text-embedding-curie, and text-embedding-davinci shows in \cref{fig:embedded_comp} . We aimed to gauge the performance of these embedding techniques across various types of questions, with particular emphasis on their capacity to accurately capture semantic relationships. The results of our experiments exhibited a clear hierarchy in performance among the assessed embedding models. It was observed that, across the spectrum of question types, the SBERT embeddings underperformed relative to their OpenAI counterparts. We attribute this disparity in performance to the smaller model size and lower dimensionality of the SBERT embeddings. Furthermore, within the cohort of OpenAI embeddings, text-embedding-curie and text-embedding-davinci stood out as superior in terms of their matching accuracy, particularly for Type 2 questions, which require a deeper understanding of semantic relationships. Conversely, text-embedding-ada and text-embedding-babbage delivered comparatively weaker performance on Type 2 questions. The choice of embedding model plays a pivotal role in determining the performance of natural language processing tasks, particularly in context retrieval. Our findings underscore the superiority of text-embedding-curie and text-embedding-davinci in tasks requiring sophisticated semantic understanding. However, it is crucial to balance the demands of the task with the computational efficiency and resource considerations.



\begin{table}
\centering
\begin{tabular}{l|cccccc} 
\toprule
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Q0} & \multicolumn{1}{l}{Q1} & \multicolumn{1}{l}{Q2} & \multicolumn{1}{l}{Q3} & \multicolumn{1}{l}{Q4~} & \multicolumn{1}{l}{Q5} \\ 
\hline\hline
Cosine S=150 k=3 & 30 & 70 & 65 & 30 & 40 & 70 \\
Cosine S=150 k=5 & 85 & 75 & 80 & 50 & 30 & 80 \\
Cosine S=300 k=5 & 85 & 70 & 75 & 55 & 50 & 70 \\ 
\midrule
KNN S=150 K=3 & 85 & 80 & 75 & 60 & 70 & 80 \\
KNN S=300 K=3 & 85 & 90 & 85 & 70 & 60 & 80 \\
KNN S=300 K=5 & \textbf{85} & \textbf{95} & 70 & 90 & \textbf{95} & \textbf{90} \\
KNN S=512 K=6 & 85 & 90 & 85 & \textbf{95} & 95 & 90 \\
\bottomrule
\end{tabular}
\caption{Impact of retrieval methods. We conduct experiments with Cosine similarly match and KNN match algorithm, we evaluate the score by expert system,  Claude+ 100K. S is the size of segment chunk and K is top k answers in the algorithms.}
\label{tab:retrieve}
\end{table}


\subsection{Impact of matching algorithms.} 
In this section, we analyze the effectiveness of different matching algorithms in the retrieval of relevant document sections shows in \cref{tab:retrieve} . The primary algorithms under scrutiny are Cosine Similarity Matching and k-Nearest Neighbors (KNN) Matching. This analysis serves to inform the selection of the most effective algorithm, and the corresponding parameter configuration, for the retrieval task. As can be seen from the table, the KNN matching algorithm consistently outperformed the Cosine Similarity Matching in retrieving relevant sections. Particularly, KNN with segment size 300 and k=5 exhibited the highest performance across multiple queries. The superior performance of the KNN matching algorithm, especially with specific parameter configurations, can be attributed to its inherent ability to capture the semantic relationships more effectively than the Cosine Similarity Matching. Furthermore, by allowing flexibility in the number of nearest neighbors, KNN enables consideration of a broader context, which is particularly beneficial for complex queries.



















