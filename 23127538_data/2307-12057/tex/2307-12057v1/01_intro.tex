\section{Introduction}
\label{sec:intro}

% To insert a figure: \input{figs/template}
% Or table: \input{tables/template}


% Figure environment removed



Large Language Models (LLMs) \cite{brown2020language, Ouyang2022TrainingLM, Chowdhery2022PaLMSL, Zhang2022OPTOP, Zeng2022GLM130BAO, Touvron2023LLaMAOA} such as ChatGPT have garnered widespread interest from both scholarly and commercial arenas, attributable to their extraordinary efficacy across a gamut of Natural Language Processing (NLP) tasks. These LLMs are architected upon an edifice of extensive pre-training on colossal text corpora, complemented by the augmentation of Reinforcement Learning from Human Feedback (RLHF) \cite{Ouyang2022TrainingLM} . This dual-pronged approach imbues LLMs with an unparalleled capacity for language comprehension, generation, interaction, and reasoning.
The formidable capabilities of LLMs have catalyzed a proliferation of nascent research domains that are poised to explore and harness the immense potential intrinsic to these models. Notably, these burgeoning areas of exploration include in-context learning \cite{brown2020language, Xie2021AnEO, Min2022RethinkingTR}, instruction-based learning  \cite{Wei2021FinetunedLM, Wang2022SuperNaturalInstructionsGV, Iyer2022OPTIMLSL, Chung2022ScalingIL} , and chain-of-thought prompting \cite{Wei2022ChainOT, Kojima2022LargeLM, Gao2022PALPL, Wang2022SelfConsistencyIC} , amongst others. These emergent research directions exemplify the seemingly boundless prospects that LLMs afford in the quest to architect sophisticated artificial intelligence systems.
This work, in particular, ventures into External Reasoning as an avenue to exploit the strengths of LLMs through a system that not only incorporates the interchangeability of various LLMs but also judiciously integrates human feedback, fostering a symbiosis between human intelligence and computational language models in the pursuit of advanced reasoning capabilities.


Despite the remarkable achievements attributed to Large Language Models (LLMs), these technologies harbor intrinsic limitations that pose substantial challenges in the development of advanced AI systems. We analyze these challenges from three distinct vantage points:
1) Static Knowledge Post-Training: LLMs acquire their knowledge reservoir through unsupervised pre-training stages. However, subsequent to this phase, the knowledge base of LLMs becomes immutable. Incorporating or updating the knowledge is computationally prohibitive due to the extensive resources required, which stifles the models' adaptability and responsiveness to evolving information landscapes.
2) Inadequacies in Zero-Shot Learning: LLMs may exhibit suboptimal performance in zero-shot learning settings, even when they have encountered relevant information during the pre-training stage. This is attributed to the disproportionate weighting of data during training, which can hinder the models’ ability to effectively recall and apply pertinent knowledge when required.
3) Context Window Limitations in Few-Shot Settings: In scenarios that utilize few-shot learning, LLMs are constrained by the finite context window length, which restricts the amount of information that can be processed in a single interaction. Furthermore, the efficient retrieval of highly relevant text sources from a local context poses a formidable challenge, as the models must discern and extract the most germane information from a constrained dataset.
Addressing these challenges necessitates innovative approaches that enhance the flexibility and adaptability of LLMs, optimize data weighting strategies for improved zero-shot performance, and develop efficient retrieval mechanisms that can operate within the context window constraints. This would pave the way for more robust and versatile AI systems. 

In this paper, we underscore the challenge of incorporating ongoing knowledge and external data, sourced from external retrieval systems, into the processing pipeline of chat-based PDF document interaction systems, such as ChatPDF. The crux of this challenge lies in the development of an efficacious external filter system capable of extracting the most pertinent information to address the queries at hand. 
As an initial step, we establish a baseline methodology that entails the matching of query and document embeddings to identify the most relevant sections within the document. Subsequently, a prompt template is formulated, incorporating the document, instructions, and query, to solicit reasoning from GPT-3.5, as depicted in Section A of Figure (refer to \cref{fig:overall}). However, a comprehensive evaluation reveals that the baseline system is adept at addressing Type 1 level questions, which involve explicit keyword matching, but is inept at handling the more nuanced Type 2 level questions that demand the extraction of implicit information such as main insights.

To surmount this limitation, we introduce a novel, policy-oriented algorithm titled "\textbf{Multi-Large-Language-Models Interchangeable Assistance with Human Feedback}". This innovative system fosters interaction between localized specialized models and LLMs, executing a preliminary stage of processing that encompasses summarization, refinement, and document similarity retrieval shows in \cref{fig:overall} secion B. The system is stratified into three tiers: entry, intermediate, and extreme. These tiers offer escalating levels of support to cater to the complexity of user queries. Specifically, in instances where users encounter challenging questions, the system offers the flexibility to elevate the level of assistance, thereby adapting to the users’ requirements.
It is imperative to highlight that our approach endeavors to strike a judicious balance between response quality and resource utilization. At the entry-level, the system is designed to efficiently address less complex tasks using a minimal token count. Conversely, at the extreme level, the system adopts a more resource-intensive approach, transcending cost constraints in a bid to deliver the highest quality responses that are commensurate with user expectations. This tiered approach ensures agility and adaptability while optimizing resource allocation. 

Additionally, we propose a solution to a compelling task (Type 3) - identifying the key references within a paper. Since key references often denote seminal or influential works in a given field, ascertaining these references empowers users with insights into the genesis and evolution of the field. In addressing this task, we present a memory-enhanced approach.
Elaborating further, when a user seeks to uncover key references, it is presumed that the user has engaged in multiple interactions with the system. Throughout this course of interactions, our system caches the summarized results derived from the user's queries and the responses generated by the LLM. Upon receiving a query for key references, the system retrieves the paper's summary from the cache, amalgamates it with the raw abstract section of the paper, and submits this combined text to the LLM. The LLM is then tasked with classifying and identifying the references based on their titles.
We posit that in a well-constructed research paper, the author elucidates several core components, such as challenges within the field, the proposed solutions, performance evaluations, and the impact on the field. By utilizing these components as reference points, the LLM is better equipped to pinpoint key references that are indicative of groundbreaking or consequential works in the respective field. This memory-enhanced approach not only streamlines the process but also fosters a deeper understanding of the research landscape.


We assess the efficacy of our system through a comparative analysis with results generated by a fully-equipped expert system. For this purpose, we employ an examiner, an advanced super-large window LLM named Claude+ (capable of handling 100K token lengths). We input the entire PDF text into the examiner to generate responses, which we consider as the ground truth. Our rationale is that with comprehensive in-context information, the results derived from the examiner should exhibit greater stability and accuracy compared to those obtained from our system, which operates under constrained conditions involving few-shot scenarios.
It is crucial to emphasize that our evaluation is not centered on assessing the reasoning capabilities of LLMs. Rather, the focus is directed towards understanding the performance of the retrieval system and analyzing how LLMs respond when provided with limited or potentially unstable references.
In addition to evaluating our system, we also scrutinize responses from several widely-used open-source LLMs under the same challenging conditions. This enables us to gauge the comparative efficacy and robustness of different LLMs in scenarios where retrieval systems must operate with constraints.