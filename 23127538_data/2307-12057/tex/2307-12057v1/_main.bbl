\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{Chowdhery2022PaLMSL}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark D{\'i}az, Orhan Firat, Michele Catasta, Jason Wei,
  Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah
  Fiedel.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em ArXiv}, abs/2204.02311, 2022.

\bibitem{Chung2022ScalingIL}
Hyung~Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric
  Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams~Wei Yu, Vincent
  Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~Huai hsin
  Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason
  Wei.
\newblock Scaling instruction-finetuned language models.
\newblock {\em ArXiv}, abs/2210.11416, 2022.

\bibitem{Gao2022PALPL}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie
  Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock {\em ArXiv}, abs/2211.10435, 2022.

\bibitem{Iyer2022OPTIMLSL}
Srinivas Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig,
  Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh Koura, Xian Li,
  Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli
  Celikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Opt-iml: Scaling language model instruction meta learning through the
  lens of generalization.
\newblock {\em ArXiv}, abs/2212.12017, 2022.

\bibitem{Kojima2022LargeLM}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em ArXiv}, abs/2205.11916, 2022.

\bibitem{leiter2023chatgpt}
Christoph Leiter, Ran Zhang, Yanran Chen, Jonas Belouadi, Daniil Larionov,
  Vivian Fresen, and Steffen Eger.
\newblock Chatgpt: A meta-analysis after 2.5 months.
\newblock {\em arXiv preprint arXiv:2302.13795}, 2023.

\bibitem{Min2022RethinkingTR}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock In {\em Conference on Empirical Methods in Natural Language
  Processing}, 2022.

\bibitem{OpenAI2023GPT4TR}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em ArXiv}, abs/2303.08774, 2023.

\bibitem{Ouyang2022TrainingLM}
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em ArXiv}, abs/2203.02155, 2022.

\bibitem{Touvron2023LLaMAOA}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and
  Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em ArXiv}, abs/2302.13971, 2023.

\bibitem{Wang2022SelfConsistencyIC}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Huai hsin Chi, and Denny
  Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock {\em ArXiv}, abs/2203.11171, 2022.

\bibitem{Wang2022SuperNaturalInstructionsGV}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva
  Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi~Gary Lai,
  Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
  Maitreya Patel, Kuntal~Kumar Pal, M. Moradshahi, Mihir Parmar, Mirali
  Purohit, Neeraj Varshney, Phani~Rohitha Kaza, Pulkit Verma, Ravsehaj~Singh
  Puri, Rushang Karia, Shailaja~Keyur Sampat, Savan Doshi, Siddharth~Deepak
  Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral,
  Yejin Choi, Noah~A. Smith, Hanna Hajishirzi, and Daniel Khashabi.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock In {\em Conference on Empirical Methods in Natural Language
  Processing}, 2022.

\bibitem{Wei2021FinetunedLM}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em ArXiv}, abs/2109.01652, 2021.

\bibitem{Wei2022ChainOT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Huai hsin Chi, F.
  Xia, Quoc Le, and Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock {\em ArXiv}, abs/2201.11903, 2022.

\bibitem{Xie2021AnEO}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock {\em ArXiv}, abs/2111.02080, 2021.

\bibitem{zaitsu2023distinguishing}
Wataru Zaitsu and Mingzhe Jin.
\newblock Distinguishing chatgpt (-3.5,-4)-generated and human-written papers
  through japanese stylometric analysis.
\newblock {\em arXiv preprint arXiv:2304.05534}, 2023.

\bibitem{Zeng2022GLM130BAO}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng~Lam Tam, Zixuan Ma, Yufei Xue,
  Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong, and Jie Tang.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock {\em ArXiv}, abs/2210.02414, 2022.

\bibitem{Zhang2022OPTOP}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em ArXiv}, abs/2205.01068, 2022.

\end{thebibliography}
