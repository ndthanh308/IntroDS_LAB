\section{Preliminaries}
\label{sec: Preliminaries}
\noindent{This section introduces some basic definitions and descriptions necessary to understand the progression of this paper and the necessary evaluations on time series data.}
% \todo{add an overview sentence of what this section is about}
 
\subsection{Neural Network Verification Tool and Star Sets}
The Neural Network Verification (NNV) tool is a framework for verifying the safety and robustness of neural networks \cite{tran2020nnv, lopez2023nnv}. It analyzes neural network behavior under various input conditions, ensuring safe and correct operation in all cases. NNV supports reachability algorithms like the over-approximate star set approach \cite{tran2020verification,tran2019fm}, calculating reachable sets for each network layer. These sets represent all possible network states for a given input, enabling the verification of specific safety properties. NNV is particularly valuable for safety-critical applications, such as autonomous vehicles and medical devices, ensuring neural networks are trustworthy and reliable under all conditions, maintaining public confidence. 
For this paper, we have implemented the work as an extension of NNV tool and used the star [Def. \ref{def:star}] based reachability analysis to get the reachable sets of the neural networks at the outputs.
\newblue{\begin{definition}\label{def:star} \emph{\textbf{A generalized star set}} (or simply star) $\Theta$ is a tuple $\langle c, V, P \rangle$ where $c \in \mathbb{R}^n$ is the center, $V = \{v_1, v_2, \cdots, v_m\}$ is a set of m vectors in $\mathbb{R}^n$ called basis vectors, and $P: \mathbb{R}^m \to \{ \top, \bot\}$ is a predicate. The basis vectors are arranged to form the star's $n \times m$ basis matrix. The set of states represented by the star is given as:
\begin{equation}
 \llbracket \Theta \rrbracket = \{x~|~x = c + \Sigma_{i=1}^m(\alpha_iv_i)~\text{and}~P(\alpha_1, \cdots, \alpha_m) = \top \}.
\end{equation}
In this work, we restrict the predicates to be a conjunction of linear constraints, $P(\alpha) \triangleq C\alpha \leq d$ where, for $p$ linear constraints, $C \in \mathbb{R}^{p \times m}$, $\alpha$ is the vector of $m$-variables, i.e., $\alpha = [\alpha_1, \cdots, \alpha_m]^T$, and $d \in \mathbb{R}^{p \times 1}$.% A star is an empty set, i.e., $\Theta = \emptyset$ if and only if the predicate $P(\alpha)$ is infeasible. In other words, we can say the predicate polyhedron $P(\alpha)$ is an empty set, i.e., $P(\alpha) = \emptyset$.
\end{definition}
\vspace*{-\baselineskip}
\vspace*{-\baselineskip}
% Figure environment removed
 \vspace*{-\baselineskip}
% \end{definition}
An alternative approach to defining a Star set for time series data involves using the upper and lower bounds of the noisy input while centering the actual input. These bounds on each input parameter, along with the predicates, create the complete set of constraints the optimizer will solve to generate the initial set of states.}

A 4 Ã— 4 time series data with a bounded disturbance
$b \in [-2, 2]$ applied on the time instance 2 of feature 1, i.e., position (1, 2) can be described as a Star depicted in Fig. \ref{fig:Signalstar}.
\subsection{Time Series and Regression Neural Network}
\textbf{Signal.} The definition of a `signal' varies depending on the applicable fields. In the area of signal processing, a \emph{\textbf{signal}} $S$ can be defined as some physical quantity that varies with respect to (w.r.t.) some independent dimension (e.g., space or time) \cite{priemer1991introductory}. 
%
In other words, a signal can also be thought of as a function that carries information about the behavior of a system or properties of some physical process \cite{priemer1991signals}. 
\begin{equation}
    \label{equ: Signal}
     S = g(q) 
\end{equation}
where $q$ is space, time, etc.
%
Depending on the nature of the spaces signals are defined over, they can be categorized as discrete or continuous. Discrete time signals are also known as time series data.
% \end{definition}

\noindent{We next define the specific class of signals considered in this paper, namely time series.}

\begin{definition} \label{def:Timeseries}
A \textbf{time series signal} $S_T$ is defined as an ordered sequence of values of a variable (or variables) at different time steps. In other words, a time series signal is an ordered sequence of discrete-time data of one or multiple features\footnote{Each \textbf{feature} is a measurable piece of data that is used for analysis.}.
\begin{equation}\label{equ: Timeseries}
\begin{split}
    S_T = s_{t_1}, s_{t_2}, s_{t_3}, ... \\
    T = t_1, t_2, t_3, ... 
\end{split}
\end{equation}
where, $t_1,t_2,t_3,\hdots$ is an ordered sequence of instances in time $T$ and $S_T = s_{t_1},s_{t_2},s_{t_3},\hdots$ are the signal values at those time instances for each $t = t_i$.

Here, sometimes we have used `signal' to refer to the `time series signal.'
\end{definition}
\noindent{Next, we define the specific types of neural networks considered in this paper, namely regression neural networks (specifically time series regression neural networks).}

\begin{definition} A \textbf{time series regression neural network (TSRegNN)} $f$ is a nonlinear/partially-linear function that maps each time-stamped value $x(i, j)$ (for $i^{th}$ feature and $j^{th}$ timestamp) of a single or multifeatured time series input $\textbf{x}$ to the output $\textbf{y}$.
% = \{y_1, y_2,\ldots,y_p\}$, $p$ is the number of expected values (either single or multiple (application dependent)) at the output.
\begin{equation}
    \label{equ: TSNN}
    f:~\textbf{x} \in \mathbb{R}^{\newblue{n_f}\times t_s} \rightarrow \textbf{y} \in {\mathbb{R}}^{p \times q} \\
\end{equation}
where $t_s, \newblue{n_f}$ are the time-sequence length and the number of features of the input data, respectively, $(j, i) \in \{ 1,\ldots,t_s \} \times \{ 1,\ldots,\newblue{n_f} \}$ are the time steps and corresponding feature indices, respectively, and $p$ is the number of values present in the output, while $q$ is the length of each of the output values; \newblue{it can either be equal to $t_s$ or not, depending on the network design.} 

Here, each row of $\textbf{x}$ represents a timestamped feature variable.
\end{definition}

\subsection{Reachability of a Time Series Regression Network}\label{Sec:Reachability}
In this section, we provide a description of how the reachability of a NN layer and the NN as a whole is computed for this study.
\begin{definition}\label{Def: Layer} A \textbf{layer} $L$ of a TSRegNN is a function $h:~u \in \mathbb{R}^{j} \rightarrow v \in {\mathbb{R}}^{p},$ with input $u \in {R}^{j}$ and output $v \in {R}^{p}$ defined as
follows 
\begin{equation}
    \label{equ: NNlayer}
    v = h(u)
\end{equation}
where the function $h$ is determined by parameters $\theta$, typically defined as a tuple $\theta = \langle \sigma, W, b \rangle$ for fully-connected
layers, where $ W \in {R}^{j\times p}, b \in {R}^{p}$, and activation function $\sigma :
{R}^{j} \rightarrow {R}^{p}$. Thus, the fully connected NN layer is described
as
\begin{equation}
    \label{equ: FC}
    v = h(u) = \sigma(\textbf{W}\times u + \textbf{b})
\end{equation}

For convolutional NN-Layers, $\theta$ may include parameters like the filter size, padding, or dilation factor, and the function in Eq. \ref{equ: FC} may need alterations.

\end{definition}

\begin{definition} Let $h:~u \in \mathbb{R}^{j} \rightarrow v \in {\mathbb{R}}^{p}$, be a NN layer as described in Eq. \ref{equ: NNlayer}. The
\textbf{reachable set} ${R_h}$, with input, $I \in {R}^{n}$ is defined as
\begin{equation}
    \label{equ: ReachLayer}
    \mathcal{R}_{h} \triangleq \{v~|~v = h(u),~u \in \mathcal{I}\}
    \end{equation}
\end{definition}

\noindent \textbf{Reachability analysis (or shortly, reach) of a TSRegNN} $f$ on Star input set $I$ is similar to the reachable set calculations for CNN\cite{tran2020verification} or FFNN\cite{tran2019star}, the only difference being both the previous works had been done for classification networks. 
\begin{equation}
\begin{split}
	Reach(f, I):~&I \rightarrow \mathcal{R}_{ts} %\\
				 %&x \rightarrow y = f(x). 
\end{split}
\end{equation}
We call $\mathcal{R}_{ts}(I)$ the \emph{output reachable set} of the TSRegNN corresponding to the input set $I$.

For a regression type NN, the output reachable set can be calculated as a step-by-step process of constructing the reachable sets for each network layer. 
 \begin{equation*}
    \begin{split}
      \mathcal{R}_{L_{1}} &\triangleq \{v_1~|~v_1 = h_1(x),~ x \in \mathcal{I}\}, \\
      \mathcal{R}_{L_{2}} &\triangleq \{v_2~|~v_2 = h_2(v_1),~ v_1 \in \mathcal{R}_{L_1}\}, \\
     &\vdots \\
    \mathcal{R}_{ts} = \mathcal{R}_{L_k} &\triangleq \{v_k ~|~ v_k = h_k(v_{k-1}),~v_{k-1} \in \mathcal{R}_{L_{k-1}}\}, \\
    \end{split}
  \end{equation*}
  where $h_k$ is the function represented by the $k^{th}$ layer $L_k$. The reachable set $\mathcal{R}_{L_k}$ contains all outputs of the neural network corresponding to all input vectors $x$ in the input set $\mathcal{I}$.
\section{Adversarial Noise}\label{Sec: Advnoise} In the case of time series samples, while the sensor transmits the sampled data, sensor noises might get added to the original data. One example of such noise is sensor vibration, but sometimes the actual sources are not even known by the sensor providers\cite{martinez2018ihorizon}.
 
\begin{definition}
A \textbf{noise} can be defined as some unintentional, usually  small-scaled signal which, when added to the primary signal, can cause malfunctioning of the equipment in an industrial premise. Mathematically, a noisy signal $s^{noise}$ 
%$=$ $[s_1^{noise}$, $\dots$, $s_n^{noise}]$ 
can be produced by a linear parameterized function $g_{\epsilon,s^{noise}}(\cdot)$ that takes an input signal and produces the corresponding noisy signal. 
% \setlength{\belowdisplayskip}{0pt}
\begin{equation}
	s^{noise} = g_{\epsilon, s^{noise}}(s) = s + \Sigma_{i=1}^n\epsilon_i \cdot s_i^{noise}
\end{equation} 
% where $s_i^{noise}$ is the 
For time series data, we can also assume the noise as a set of unit vectors associated with a coefficient vector $\epsilon$ at each time step $i$, where the value of the coefficient vector $\epsilon$ is unknown but bounded within a range $[\underline{\epsilon}, \overline{\epsilon}]$, i.e., $\underline{\epsilon_i} \leq \epsilon_i \leq \overline{\epsilon_i}$.
\end{definition}
% \diego{Should we add to the definition something that shows that the noise/perturbation is some value $\epsilon$ $\leq$ $\epsilon_{max}$ such that the value of the data $x_i$ in set $X$, |$x_i$ + $\epsilon$| $\leq$ |$x_i$ + $\epsilon_{max}$|}
\vspace*{-\baselineskip}
\subsubsection{Types of Possible Noises.}\label{Noise Types} For an input sequence with $t_s$ number of time instances and $n_f$ number of features, there can be four types of noises ($l_\infty$ norm) [\ref{l_inf norm}], based on its spread on the signal. They can be categorized as below:
\begin{enumerate}
    

    \item \textbf{Single Feature Single-instance Noise (SFSI)} i.e., perturbing a feature value only at a particular instance ($t$) by a certain percentage around the actual value.
    \begin{equation}\label{equ: SFSI}
	s^{noise} = g_{\epsilon, s^{noise}}(s) = s + \epsilon_t \cdot s_t^{noise}
\end{equation}
    \item \textbf{Single Feature All-instances Noise (SFAI)} i.e., perturbing a specific feature throughout all the time instances by a certain percentage around the actual values of a particular feature.
        \begin{equation}\label{equ: SFAI}
	s^{noise} = g_{\epsilon, s^{noise}}(s) = s + \Sigma_{i=1}^n\epsilon_i \cdot s_i^{noise}
\end{equation}
    \item \textbf{Multifeature Single-instance Noise (MFSI)} i.e., perturbing all feature values but only at a particular instance (t), following Eq.~\ref{equ: SFSI} for all features.

    \item \textbf{Multifeature All-instance Noise (MFAI)} i.e., perturbing all feature values throughout all the instances, following Eq.~\ref{equ: SFAI} for all features.  
 \end{enumerate}

A sample plot for all four types of noises is shown in [\ref{AppendixNoise}].

\section{Verification Properties}\label{Sec: VerProp} Verification properties can be categorized into two types: local properties and global properties. A local property is defined for a specific input \newblue{$x$ at time-instance $t$ or a set of points $X$} in the input space \textbf{\newblue{$R^{n_f \times t_s}$}}. In other words, a local property must hold for certain specific inputs. On the other hand, a global property \cite{wang2022tool} is defined over the entire input space \newblue{$R^{n_f \times t_s}$} of the network model and must hold for all inputs without any exceptions.
\subsubsection{Robustness.}
Robustness refers to the ability of a system or a model to maintain its performance and functionality under various challenging conditions, uncertainties, or perturbations. It is a desirable quality that ensures the system's reliability, resilience, and adaptability in the face of changing or adverse circumstances. For an input perturbation measured by $\delta$ and admissible output deviation $\epsilon$, the `delta-epsilon' formulation for the desired robustness property can be written as:
\begin{equation}
    ||x' - x||_{\infty} < \delta \implies || f(x') - f(x) ||_{\infty} < \epsilon
\end{equation}
where $x$ is the original input belonging to the input space \newblue{$R^{n_f \times t_s}$}, $x'$ is the noisy input, $f(x')$ and $f(x)$ are NN model outputs for, respectively, $x'$ and $x$, $\delta$ is the max measure of the noise added, $\epsilon$ is the max deviation in the output because of the presence of noise ($\delta, \epsilon \in \textbf{R} >0 )$.
\paragraph*{\textbf{Local Robustness.}}\label{Robust} Given a TSRegNN $f$ and an input time series signal $S$, the network is called \textbf{locally robust} to any noise $\mathcal{A}$ if and only if: the estimated output reachable bounds for a particular time-step corresponding to the noisy input lie between predefined allowable bounds w.r.t to the actual signal.

\textbf{Robustness Value (RV)} of a time series signal $S$ is a binary variable, which indicates the local robustness of the system. RV is $1$ when the estimated output range for a particular time instance (t) lies within the allowable range, making it locally robust at t; otherwise, RV is 0. 

$RV = 1 \iff {LB_t}^{est} \geq {LB_t}^{allow} \newblue{\land} {UB_t}^{est} \leq {UB_t}^{allow}$
else, RV = 0 

where ${LB_t}^{est}$ and ${UB_t}^{est}$  are estimated bounds and ${LB_t}^{allow}$ and ${UB_t}^{allow}$ are allowable bounds.

\begin{definition} \textbf{Percentage Sample Robustness (PR)}\label{def:PSR} of a TSRegNN corresponding to any noisy input is defined as
\begin{equation}
PR = \frac{N_{robust}}{N_{total}}\times 100\%,
\end{equation}
where $N_{robust}$ is the total number of robust time instances, and $N_{total}$ = the total number of time steps in the time series signal. Percentage robustness can be used as a measure of \textbf{global robustness \newblue{\cite{wang2022tool}}} of a TSRegNN w.r.t any noise.

\newblue{In this study, we adapt the concept of Percentage Robustness (PR) previously used in image-based classification or segmentation neural networks \cite{tran2021robustness} to time-series inputs. PR in those cases assessed the network's ability to correctly classify/segment inputs even with input perturbations for a given number of images/pixels. We extend this concept to analyze the robustness of time-series inputs in our research.}

\end{definition}

\begin{definition} \textbf{Percentage Overlap Robustness (POR)}\label{def:POR} of a TSRegNN corresponding to any noisy input is defined as
\begin{equation}
POR = \frac{\Sigma_{i=1}^{N_{total}}(PO_i)}{N_{total}}\times 100\%,
\end{equation}
where $N_{total}$ = total number of time instances in the time series signal, and $PO_i$ is the percentage overlap between estimated and allowed ranges at each time step w.r.t the estimated range
\begin{equation}
 PO = \frac{Overlapped~Range}{Estimated~Range}
\end{equation}
Here $Overlapped~Range$ is the overlap between the estimated range and the allowable range for a particular time step, whereas $Estimated~Range$ is the output estimation given by the TSRegNN for that time step. 
Percentage overlap robustness can also be used as a measure of \textbf{global robustness \newblue{\cite{wang2022tool}}} of TSRegNN.
\end{definition}
\newblue{When selecting robustness properties, it is crucial to consider the specific application area. If the application allows for some flexibility in terms of performance, POR can be utilized. On the other hand, if the application requires a more conservative approach, PR should be considered.}
An example showing calculations for the robustness measures is shown in [\ref{Robustness Measures Example}].
\vspace*{-\baselineskip}
\subsubsection{Monotonicity.} In PHM applications, the monotonicity property refers to the system's health indicator, i.e., the degradation parameter exhibiting a consistent increase or decrease as the system approaches failure. PHM involves monitoring a system's health condition and predicting its Remaining Useful Life (RUL) to enable informed maintenance decisions and prevent unforeseen failures. For detailed mathematical modeling of the monotonicity property, please refer to \cite{sivaraman2020counterexample} and the latest report on formal methods at \cite{ForMuLA}. In general, for a TSRegNN $f:~\textbf{x} \in \mathbb{R} \rightarrow \textbf{y} \in {\mathbb{R}}$ with single-featured input and output spaces, at any time instance $t$, the property for monotonically decreasing output can be written as:
 \begin{equation}\label{equ: monotonicity}
     \begin{split}
         \forall x' \exists \delta : x \leq x' \leq x+\delta \implies f(x') \leq f(x) \\
         \forall x' \exists \delta : x-\delta \leq x' \leq x \implies f(x') \geq f(x)\\
     \end{split}
 \end{equation}
 This is a local monotonicity property. If this holds true for the entire time range, then the property can be considered as a global property \cite{wang2022tool}. In this paper, the monotonicity property is only valid for the PHM examples for RUL estimation. 
\section{Reachability of Specific Layers to Allow Variable-Length Time Series Input}\label{Sec:ReachabilityLayers}

\subsubsection{Reachability Of A Fully-connected Layer.}
We consider a fully-connected layer with the following parameters: the weights $W_{fc} \in {R}^{op \times ip}$ and the bias $b_{fc} \in {R}^{op \times 1}$, where $op$ and $ip$ are, respectively, the output and input sizes of the layer. The output of this fully connected layer w.r.t an input $i \in {R}^{ip \times T_s}$ will be
\begin{equation*}
    \begin{split}
        o = W_{fc} \times i + b_{fc} \\
where~output~o \in {R}^{op \times T_s}
    \end{split}    
\end{equation*}
Thus, we can see that the layer functionality does not alter the output size for a variable length of time sequence, making the functionality of this layer independent of the time series length.

\noindent The reachability of  a fully-connected layer will be given by the following lemma.
\vspace*{-\baselineskip}
\begin{lemma} \label{lem:fc}
The reachable set of a fully-connected layer with 
 a Star input set $I = \langle c, V, P \rangle$ is another Star $I' = \langle c', V', P' \rangle$ where $c' = W_{fc} \times c + b_{fc}$, the matrix multiplication of $c$ with Weight matrix $W_{fc}$,$V' = \{v'_1,...,v'_m\}$, where $v'_i = W_{fc} \times v_i$, the matrix multiplication of the weight matrix and the $i^{th}$ basis vector, \newblue{and $P' = P$}. 
\end{lemma}
\vspace*{-\baselineskip}
\subsubsection{Reachability of a 1D Convolutional Layer.} We consider a 1d convolution layer with the following parameters: the weights $W_{conv1d} \in {R}^{w_f \times nc \times fl}$ and the bias $b_{conv1d} \in {R}^{1 \times fl}$, the padding size $P$, the stride $S$, and the dilation factor $D$; where $w_f, nc~and~fl$ are the filter size, number of channels and number of filters respectively.

The output of this 1d convolution layer w.r.t an input $i \in {R}^{ip \times T_s}$ will be
\begin{equation*}
    \begin{split}
        o = W'_{conv1d} \cdot i' + b_{conv1d} ~~dot~product~along~time~dimesion~for~each~filter\\
where~output~o \in {R}^{fl \times T'_s}
    \end{split}    
\end{equation*}
where $T'_s = T_s + T_d - T_{fl} $ is the new time series length at the output and $T_d, T_{fl}$ are the time lengths contributed by the dilation factor and the 1d convolution function, respectively. $w'_{conv1d}$ is the modified weight matrix after adding dilation, and $i'$ is the modified input after padding. We can see when $T_d$ becomes equal to $T_{fl}$ for any convolution layer, the layer functionality becomes independent of the length of the time series.

\noindent The reachability of  a 1d convolution layer will be given by the following lemma.
\vspace*{-\baselineskip}
\begin{lemma} \label{lem:conv1d}
The reachable set of a 1d convolution layer with 
 a Star input set $I = \langle c, V, P \rangle$ is another Star $I' = \langle c', V', P' \rangle$ where $c' = W_{conv1d} \cdot c $, 1d convolution applied to the \newblue{basis} vector $c$ with Weight matrix $W_{conv1d}$,$V' = \{v'_1,...,v'_m\}$, where $v'_i = W_{conv1d} \cdot v_i$, is the 1d convolution operation with zero bias applied to the generator vectors, i.e., only using the weights of the layer, \newblue{and $P' = P$}.
\end{lemma}
