\documentclass{article}

%
%
%

%
\PassOptionsToPackage{numbers}{natbib}
\usepackage[preprint]{neurips_data_2023}

%
%
%

%
%

%
%

%
%
%
%
%
%

\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{listings}
\usepackage[group-separator={,},group-minimum-digits=4]{siunitx}
\usepackage{graphicx}
\usepackage{graphbox}
\usepackage[capitalize]{cleveref}
\newcommand{\algname}{\texttt{trajdata}}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\title{\algname{}: A Unified Interface to Multiple\\Human Trajectory Datasets}

%
%
%
%
%
%
%

\author{%
  Boris Ivanovic$^1$ \hspace{0.75cm} Guanyu Song$^2$ \hspace{0.75cm} Igor Gilitschenski$^2$ \hspace{0.75cm} Marco Pavone$^{1,3}$\\$^1$NVIDIA Research \hspace{0.75cm} $^2$University of Toronto \hspace{0.75cm} $^3$Stanford University
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
  %
}

\begin{document}

\maketitle

\begin{abstract}
  The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets.
  %
  To remedy this, we present \algname{}: a unified interface to multiple human trajectory datasets. At its core, \algname{} provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. \algname{} is permissively licensed (Apache 2.0) and can be accessed online at \url{https://github.com/NVlabs/trajdata}.
  
  %
\end{abstract}

\section{Introduction}
\label{sec:introduction}
Research in trajectory forecasting (i.e., predicting where an agent will be in the future) has grown significantly in recent years, partially owing to the success of deep learning methods on the task~\cite{RudenkoPalmieriEtAl2019}; availability of new large-scale, real-world datasets (see~\cref{fig:hero}); and investment in its deployment within domains such as autonomous vehicles (AVs)~\cite{GMSafety2018,UberATGSafety2020,LyftSafety2020,WaymoSafety2021,ArgoSafety2021,MotionalSafety2021,ZooxSafety2021,NVIDIASafety2021} and social robots~\cite{KrusePandeyEtAl2013,ChikYeongEtAl2016,LasotaFongEtAl2017}.

In addition, recent dataset releases have held associated prediction challenges which have periodically benchmarked the field and spurned new developments~\cite{nuTonomy2020,Lyft2020,Waymo2021,Yandex2022}. While this has been a boon for research progress, each dataset has a unique data format and development API, making it cumbersome for researchers to train and evaluate methods across multiple datasets. For instance, the recent Waymo Open Motion dataset employs binary TFRecords~\cite{EttingerChengEtAl2021} which differ significantly from nuScenes' foreign-key format~\cite{CaesarBankitiEtAl2019} and Woven Planet (Lyft) Level 5's compressed zarr files~\cite{HoustonZuidhofEtAl2020}. 
%
The variety of data formats has also hindered research on topics which either require or greatly benefit from multi-dataset comparisons, such as prediction model generalization (e.g.,~\cite{GillesSabatiniEtAl2022c,IvanovicHarrisonEtAl2023}). To remedy this, we present \algname{}: a unified interface to multiple human trajectory datasets. %

\textbf{Contributions.} Our key contributions are threefold. First, we introduce a standard and simple data format for trajectory and map data, as well as an extensible API to access and transform such data for research use. Second, we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a richer understanding of the data underpinning much of pedestrian and AV motion forecasting research. Finally, we leverage insights from these analyses to provide suggestions for future dataset releases.

% Figure environment removed

%

\section{Related Work}
\label{sec:related_work}

\textbf{Human Trajectory Datasets.} Initial trajectory forecasting research employed video motion tracking datasets for benchmarking, primarily due to the availability of annotated agent positions
%
over time. Of these, the ETH~\cite{PellegriniEssEtAl2009} and UCY~\cite{LernerChrysanthouEtAl2007} pedestrian datasets were among the most widely-used~\cite{RudenkoPalmieriEtAl2019}, containing a total of 1536 pedestrians and challenging behaviors such as couples walking together, groups crossing each other, and groups forming and dispersing. Soon after the successful application of deep learning models to pedestrian trajectory forecasting~\cite{AlahiGoelEtAl2016}, and as data needs grew in autonomous driving research and industry, numerous large-scale datasets have emerged containing significantly more heterogeneous-agent interactive scenarios (e.g., between vehicles and pedestrians) in urban environments. \cref{fig:hero} visualizes the scale, collection, and annotation strategy of such datasets, with a comprehensive review of earlier human motion datasets available in~\cite{RudenkoPalmieriEtAl2019,AmirianZhangEtAl2020}. In particular, the gradual shift from human annotation to autolabeling can be seen, with the recent large-scale Yandex Shifts~\cite{MalininBandEtAl2021}, Waymo Open Motion~\cite{EttingerChengEtAl2021}, and nuPlan~\cite{CaesarKabzanEtAl2021} datasets employing powerful autolabeling pipelines to accurately label sensor data collected by vehicle fleets at scale.

%

%

%

\textbf{Multi-Dataset Benchmarking.} While the increase in datasets and associated challenges has bolstered research, their unique formats increase the complexity of evaluating methods across datasets, complicating efforts to analyze, e.g., prediction model generalization.
%
%
To address this issue for pedestrian motion data, OpenTraj~\cite{AmirianZhangEtAl2020} created dataloaders for different pedestrian motion datasets as part of its effort to evaluate and compare motion complexity across pedestrian datasets. More recently, TrajNet++~\cite{KothariKreissEtAl2022} and Atlas~\cite{RudenkoPalmieriEtAl2022} present multi-dataset benchmarks to systematically evaluate human motion trajectory prediction algorithms in a unified framework. While these efforts have provided the community with multi-dataset benchmarks, they are primarily focused on pedestrian data. In contrast, \algname{} tackles the standardization of both pedestrian \emph{and} autonomous vehicle datasets, including additional data modalities such as maps.
%
%

%

\section{\algname{}: A Unified Interface to Multiple Human Trajectory Datasets}
\label{sec:trajdata}

\algname{} is a software package that efficiently compiles multiple disparate dataset formats into one canonical format, with an API to access and transform that data for use in downstream frameworks (e.g., PyTorch~\cite{PaszkeGrossEtAl2017}, which is natively supported).
%
Currently, \algname{} supports 8 diverse datasets, comprising \num{3216} hours of data, 200+ million unique agents, and 10+ locations across 7 countries (see~\cref{tab:datasets}).
To date, \algname{} has been extensively used in research on trajectory forecasting~\cite{IvanovicHarrisonEtAl2023}, pedestrian~\cite{RempeLuoEtAl2023} and vehicle~\cite{XuChenEtAl2023,ZhongRempeEtAl2023} simulation, and AV motion planning~\cite{ChristianosKarkusEtAl2023,ChenKarkusEtAl2023}.

\subsection{Standardized Trajectory and Map Formats}
\textbf{Trajectories.} For each dataset, \algname{} extracts position, velocity, acceleration, heading, and extent (length, width, height) information for all agents in standard SI units (see~\cref{fig:format}). In order to support a variety of dataset formats, \algname{} has minimal base data requirements: As long as agent positions (i.e., $x, y$ coordinates) are provided, all other dynamic information can be derived automatically.
%
If entire dynamic quantities (e.g., velocity) are not captured in the original dataset, \algname{} uses finite differences to compute derivatives by default. Further, missing data between timesteps is imputed via linear interpolation. \algname{} internally represents and stores trajectory data as tabular data frames, allowing for advanced indexing and data grouping depending on user queries and the use of efficient open-source tabular data storage frameworks such as Apache Arrow~\cite{ApacheArrow2023}. Note that each of these default choices (finite differences, linear interpolation, and tabular data frames) can be changed by the end user.
%

\textbf{Maps.} To retain the most information from high-definition (HD) dataset maps, \algname{} adopts a polyline representation for map data. This choice matches the vast majority of modern trajectory datasets which provide vector map data and makes them immediately compatible with our format. 
%
Currently, there are four core map elements: \texttt{RoadLane}, \texttt{RoadArea}, \texttt{PedCrosswalk}, and \texttt{PedWalkway}. As illustrated in \cref{fig:format}, a \texttt{RoadLane} represents a driveable road lane with a centerline and optional left and right boundaries. A \texttt{RoadArea} represents other driveable areas of roads which are not part of lanes, e.g., parking lots or shoulders. A \texttt{PedCrosswalk} denotes a marked area where pedestrians can cross the road. Finally, a \texttt{PedWalkway} marks sidewalks adjacent to roads. Of these, only \texttt{RoadLane} elements are required to be extracted, other elements are optional (they are absent in some datasets). Our map format additionally supports lane connectivity information in the form of left/right adjacent lanes (i.e., lanes accessible by left/right lane changes) and successor/predecessor lanes (i.e., lanes that continue from / lead into the current lane following the road direction).

Each map element is designed to be compatible with popular computational geometry packages, such as Shapely~\cite{Shapely2023},
%
enabling efficient set-theoretic queries to calculate, e.g., road boundary violations. By default, \algname{} serializes map data using Google protocol buffers~\cite{GoogleProtobuf2023}, and, in particular, only stores neighboring position \emph{differences} for efficiency,
%
similar to the implementation used in~\cite{HoustonZuidhofEtAl2020}.
Dynamic traffic light information is also supported, and \algname{} makes use of a separate data frame to link the traffic signal shown per timestep with the lane ID being controlled.

\setlength{\tabcolsep}{0.5em}
\begin{table}[t]
  \caption{
Datasets currently supported by \algname{}. More details can be found in the appendix.
%
}
  \label{tab:datasets}
  \centering
  \begin{tabular}{lrcl|lrcl}
    \toprule
    Dataset & Size & Locations & Maps? & Dataset & Size & Locations & Maps? \\
    \midrule
    ETH~\cite{PellegriniEssEtAl2009} & 0.4h & 2 & No & INTERACTION~\cite{ZhanSunEtAl2019} & 16.5h & 4 & Yes \\
    UCY~\cite{LernerChrysanthouEtAl2007} & 0.3h & 2 & No & Lyft Level 5~\cite{HoustonZuidhofEtAl2020} & 1118h & 1 & Yes \\
    SDD~\cite{RobicquetSadeghianEtAl2016} & 5h & 1 & No & Waymo Open~\cite{EttingerChengEtAl2021} & 570h & 6 & Yes \\
    nuScenes~\cite{CaesarBankitiEtAl2019} & 5.5h & 2 & Yes & nuPlan~\cite{CaesarKabzanEtAl2021} & 1500h & 4 & Yes \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Core \algname{} Functionalities}

% Figure environment removed

% Figure environment removed

\textbf{Multi-dataset training and evaluation.} One of \algname{}'s core functionalities\footnote{Detailed demonstrations of \algname{}'s capabilities can be found in our repository's \texttt{examples/} folder.} is compiling data from multiple datasets in a \texttt{UnifiedDataset} object (a PyTorch~\cite{PaszkeGrossEtAl2017} \texttt{Dataset} subclass by default).%
%

\begin{lstlisting}[language=Python]
from trajdata import UnifiedDataset
dataset = UnifiedDataset( 
  desired_data=["nusc_mini-boston", "sdd-train"], desired_dt=0.1,
  centric="agent", history_sec=(1.0, 3.0), future_sec=(4.0, 4.0)
) # These settings were used to create Figure 2.
\end{lstlisting}

In this example, a dataset is created that 
%
%
%
%
provides agent-centric data batches (i.e., each batch element contains data for one agent at one timestep, see \cref{fig:batch_format}) sourced from only Boston in the nuScenes mini dataset (\texttt{"nusc\_mini-boston"}) as well as the Stanford Drone Dataset's entire training split (\texttt{"sdd-train"}), with time upsampling ensuring all data is at 10Hz (\texttt{desired\_dt=0.1}). \texttt{history\_sec=(1.0, 3.0)} specifies that the predicted agent's trajectory must have at least $1.0s$ of history available, with padding for any missing data up to $3.0s$ (see~\cref{fig:batch_format}). Similarly, \texttt{future\_sec=(4.0, 4.0)} requires that the predicted agent's trajectory have $4.0s$ of future available.
%

%

%

\algname{} provides many other capabilities in addition to the above, including scene-centric batches (i.e., data for all agents in a scene at the same timestep), semantic search (e.g., nuScenes~\cite{CaesarBankitiEtAl2019} provides text descriptions for each scene), agent filtering (e.g., only vehicles), coordinate frame standardization (i.e., making trajectories relative to the predicted agent's frame at the current timestep), map rasterization (e.g., if encoding scene context with a convolutional architecture), data augmentations (e.g., additive Gaussian noise to past trajctories), and general data transforms via custom functions.

\textbf{Map API.} \algname{}'s standardized vector map object is \texttt{VectorMap}. In addition to providing access to individual map elements (e.g., lanes, sidewalks), it also leverages precomputed spatial indices to make nearest neighbor queries very efficient.

\begin{lstlisting}[language=Python]
from trajdata import MapAPI, VectorMap
vec_map: VectorMap = MapAPI(<=>).get_map("nusc_mini:boston-seaport")
lane = vec_map.get_closest_lane(np.array([50.0, 100.0, 0.0]))
\end{lstlisting}

In the example above, the polyline map of Boston's seaport neighborhood (from nuScenes~\cite{CaesarBankitiEtAl2019}) is loaded from the user's \algname{} cache (its path would be specified instead of \texttt{<=>}) and queried for the closest \texttt{RoadLane} to a given $x, y, z$ position.
%

\textbf{Simulation Interface.} \algname{} also provides a simulation interface that enables users to initialize a scene from real-world data and simulate agents from a specific timestep onwards. Simulated agent motion is recorded by \algname{} and can be analyzed with a library of evaluation metrics (e.g., collision and offroad rates, statistical differences to real-world data distributions) or exported to disk. This functionality was extensively used to benchmark learning-based traffic models in~\cite{XuChenEtAl2023,ZhongRempeEtAl2023}.
\begin{lstlisting}[language=Python]
from trajdata.simulation import SimulationScene
sim_scene = SimulationScene(<=>) # Specify initial scene to use.
obs = sim_scene.reset() # Initialized from real agent states in data.
for t in range(10): # Simulating 10 timesteps in this example.
    new_state_dict = ... # Compute the new state of sim agents.
    obs = sim_scene.step(new_state_dict)
\end{lstlisting}
In this example, a \texttt{SimulationScene} is initialized from a scene in an existing dataset (specified with the \texttt{<=>} arguments), after which it can be accessed similarly to an OpenAI Gym~\cite{BrockmanCheungEtAl2016} reinforcement learning environment, using methods like \texttt{reset} and \texttt{step}.

\section{Dataset Comparisons and Analyses}
\label{sec:analyses}

In this section, we leverage \algname{}'s standardized trajectory and map representations to directly compare many popular AV and pedestrian trajectory datasets along a variety of metrics. Our goal is to provide a deeper understanding of the datasets underpinning much of human motion research by analyzing their data distributions, motion complexity, and annotation quality.

Note that we only analyze dataset training and validation splits,
%
since these are the splits predominantly used by methods for development.
We explicitly do not analyze test splits since they are either not available publicly or because doing so may harm existing benchmark validity.
Further, while \algname{} supports data frequency up- and down-scaling via interpolation and down-sampling, all of the following analyses were conducted in each dataset's native data resolution.
All analyses were performed using the latest version of \algname{} at the time of writing (v1.3.2) on a desktop computer with $64$ GB of RAM and an AMD Ryzen Threadripper PRO 3975WX 32-core CPU. For larger datasets, an NVIDIA DGX-1 server with $400$ GB of RAM and 64 CPU cores was used.

\subsection{Agent Distributions}
\label{sec:agent_dist}

\textbf{Population.} To build a fundamental understanding of the considered datasets, we first analyze and compare agent populations.
\cref{fig:agent_counts} visualizes overall agent counts and proportions per dataset. As can be expected, modern large-scale AV datasets such as Waymo~\cite{EttingerChengEtAl2021} and Lyft Level 5~\cite{HoustonZuidhofEtAl2020} contain multiple orders of magnitude more agents than earlier pedestrian datasets SDD~\cite{RobicquetSadeghianEtAl2016}, ETH~\cite{PellegriniEssEtAl2009}, or UCY~\cite{LernerChrysanthouEtAl2007}. However, as we will show later, pedestrian datasets still provide value in terms of agent diversity, density, and motion complexity in popular social robotics settings such as college campuses. 
%

As can be seen in~\cref{fig:agent_counts} (right), the vast majority of agents in AV datasets are vehicles or pedestrians, with the exception of Lyft Level 5~\cite{HoustonZuidhofEtAl2020} where $71.8\%$ of agents have unknown types.
In contrast, bicycles (a relatively niche category in many datasets) account for $41\%$ of all agents in SDD~\cite{RobicquetSadeghianEtAl2016} (indeed, biking is a popular method of transportation around Stanford's large campus). 
%
Such imbalances in agent populations are indicative of real-world distributions, e.g., motorcycles make up only $3.5\%$ of vehicles in the USA~\cite{BTS2023}, similar to their proportion in nuScenes~\cite{CaesarBankitiEtAl2019} ($1.6\%$).

%
%

% Figure environment removed

\textbf{Density and Observation Duration.}
In addition to which agent types are captured in scenes, the amount and density of agents can be an important desiderata (e.g., for research on crowd behavior) or computational consideration (e.g., for methods whose runtime scales with the number of agents). \cref{fig:simultaneous} visualizes the distribution of the number of agents observed per scene per timestep (left), as well as the \emph{maximum} number of simultaneous agents per scene (right). As can be seen, urban scenarios captured in modern AV datasets frequently contain $100+$ detected agents (with a long tail extending to $250+$ agents). In this respect, ETH~\cite{PellegriniEssEtAl2009}, UCY~\cite{LernerChrysanthouEtAl2007}, and INTERACTION~\cite{ZhanSunEtAl2019} are limited by their fixed-camera and drone-based data-collection strategies compared to the comprehensive on-vehicle sensors used in nuScenes~\cite{CaesarBankitiEtAl2019}, Waymo~\cite{EttingerChengEtAl2021}, Lyft~\cite{HoustonZuidhofEtAl2020}, and nuPlan~\cite{CaesarKabzanEtAl2021}. 
However, while ETH~\cite{PellegriniEssEtAl2009}, UCY~\cite{LernerChrysanthouEtAl2007}, and INTERACTION~\cite{ZhanSunEtAl2019} do not contain as many agents, they consistently provide the highest-density scenes (see~\cref{fig:density}), especially for pedestrians and bicycles. We compute agent density by dividing the number of agents in a scene by their overall bounding rectangle area, as in~\cite{AmirianZhangEtAl2020}.

Each dataset supported by \algname{} adopts different scenario lengths and corresponding agent observation durations. As can be seen in~\cref{fig:length}, AV datasets are comprised of scenarios with lengths ranging from $4s$ in INTERACTION~\cite{ZhanSunEtAl2019} to $25s$ in Lyft Level 5~\cite{HoustonZuidhofEtAl2020}.
%
The peaks at the right of each AV dataset duration distribution are caused by the always-present ego-vehicle (for Vehicles) as well as other agents detected throughout the scene (common in steady traffic, parking lots, or at an intersection with stopped traffic and pedestrians waiting to cross). One can also see that
%
Lyft Level 5~\cite{HoustonZuidhofEtAl2020} agent detections are much shorter-lived compared to other AV datasets' relatively uniform distributions (Waymo~\cite{EttingerChengEtAl2021}, nuScenes~\cite{CaesarBankitiEtAl2019}, and nuPlan~\cite{CaesarKabzanEtAl2021}).
%
This could be caused by Lyft's annotations being collected from an onboard perception system~\cite{HoustonZuidhofEtAl2020} (which are affected by noise and occlusions) vs human annotators~\cite{CaesarBankitiEtAl2019} or autolabeling~\cite{CaesarKabzanEtAl2021,EttingerChengEtAl2021} which can leverage data from past and future timesteps be more robust to such errors. We conduct additional comparisons between data collection methodologies in~\cref{sec:annot_quality}.

% Figure environment removed

% Figure environment removed

% Figure environment removed

\textbf{Ego-Agent Distances.} When developing AV perception systems, an important consideration is the sensor range(s) necessary to facilitate the desired prediction and planning horizons as well as provide advanced warning of critical situations (e.g., stopped traffic on a highway). In~\cref{fig:ae_dist}, we compare the distribution of ego-agent distances and find that, while nuScenes~\cite{CaesarBankitiEtAl2019} and Lyft Level 5~\cite{HoustonZuidhofEtAl2020} have long-tailed distributions extending past $200m$, Waymo~\cite{EttingerChengEtAl2021} and nuPlan~\cite{CaesarKabzanEtAl2021} appear to have artificial cut-offs at $75$-$80m$, potentially to maintain data quality by avoiding poor data from distant agents. However, it would be more useful to maintain distant detections and add uncertainty outputs from the autolabeler to support uncertain long-range detection research in addition to improving autolabeling.

%

% Figure environment removed

\textbf{Mapped Areas.} HD maps are a core component of many AV datasets, frequently leveraged in trajectory forecasting and motion planning research to provide scene context and geometric lane information (e.g., for global search-based planning and trajectory optimization). Current AV dataset maps are very large (see~\cref{tab:map_area} in the appendix) and comprehensive, spanning multiple neighborhoods in different cities.
%
However, not all HD maps are created equal, commonly differing along three axes: Area completeness, lane definitions, and traffic lights.
While most AV datasets provide complete HD maps of neighborhoods, Waymo~\cite{EttingerChengEtAl2021} differs by only providing local map crops per scenario without a common reference frame across scenarios\footnote{See \url{https://github.com/waymo-research/waymo-open-dataset/issues/394} for visualizations.}. This also significantly increases the storage requirements of Waymo~\cite{EttingerChengEtAl2021} maps compared to other datasets.

Lane definitions can also differ significantly between datasets, with intersections being a notable differentiator.
For instance, the nuScenes dataset~\cite{CaesarBankitiEtAl2019} does not annotate intersections fully, opting for only lane centerlines without associated edges (\cref{fig:format} shows an example). Lyft Level 5~\cite{HoustonZuidhofEtAl2020} and nuPlan~\cite{CaesarKabzanEtAl2021} both include full lane center and edge information for all possible motion paths through an intersection. Waymo~\cite{EttingerChengEtAl2021} maps are unique in that they provide full lane center and boundary information, but
%
%
%
there are many gaps in the associations between lane centerlines and boundaries, making it difficult to construct lane edge polylines or lane area polygons\footnote{See \url{https://github.com/waymo-research/waymo-open-dataset/issues/389} for visualizations.}. As a result, we exclude Waymo maps from map-based analyses in this work.

%

\subsection{Motion Complexity}
\label{sec:motion_complexity}

Measuring the complexity of driving scenarios is an important open problem in the AV domain, with a variety of proposed approaches ranging from heuristic methods~\cite{AmirianZhangEtAl2020} to powerful conditional behavior prediction models~\cite{TolstayaMahjourianEtAl2021}. To avoid potential biases in analyzing datasets with an externally-trained model, we employ simple and interpretable heuristics similar to~\cite{AmirianZhangEtAl2020}.
%
%

\textbf{Motion Diversity.} We first analyze distributions of dynamic agent quantities (e.g., speed, acceleration, jerk).
%
As can be seen in \cref{fig:speed}, the majority of speed distributions have high peaks at zero (no motion). This is corroborated by~\cref{tab:stationary} in the appendix, which shows that a significant portion of agents are stationary in many datasets, especially for nuScenes~\cite{CaesarBankitiEtAl2019} ($17.5\%$) and Waymo~\cite{EttingerChengEtAl2021} ($53.6\%$). After the initial peak, agent speed distributions drop sharply to a roughly uniform plateau (up to $20 m/s$ for vehicles) before dropping completely around $30 m/s$ (a common highway speed around the world).

While SDD~\cite{RobicquetSadeghianEtAl2016} and INTERACTION~\cite{ZhanSunEtAl2019} have sensible vehicle speeds, their pedestrian speeds can be too high. Such high speeds may be caused by annotations near the edge of drone camera view or by rectification artifacts near the image border. Additionally, the very long-tailed distribution of Lyft~\cite{HoustonZuidhofEtAl2020}) and Waymo~\cite{EttingerChengEtAl2021}) vehicle, pedestrian, and bicycle speeds (exceeding $60 m/s$) show a remaining area of improvement for state-of-the-art AV perception systems and autolabeling pipelines. Comparisons of acceleration and jerk can be found in the appendix. Overall, from dynamic quantities alone, Waymo~\cite{EttingerChengEtAl2021}) and Lyft~\cite{HoustonZuidhofEtAl2020} provide the most diversity in agent motion. If such long-tailed data is undesirable, the INTERACTION~\cite{ZhanSunEtAl2019} dataset provides the most realistic set of vehicle speeds. %

%

% Figure environment removed 

%
%
%
%
%
%

\textbf{Trajectory Nonlinearity.} To analyze the spatial diversity of agent trajectories, we first compare each agent's heading to their initial timestep. As can be seen in~\cref{fig:dh}, and reiterating earlier analyses, the vast majority of human movement is straight and linear ($\Delta h = 0$). Moving away from the center, we also see repeated symmetric peaks at $\pm \frac{\pi}{2}$ (capturing left and right turns) and $\pm k\pi$ in some datasets. One possible reason for these periodic peaks in the distribution is an artifact of the autolabeling methods used in the datasets (since only datasets that autolabel sensor data are affected), another is that their respective scene geometries contain more roundabouts, cul-de-sacs, and repeated turns than other datasets (more detailed heading distributions can be found in the appendix).
%
We can also see that pedestrians' distributions are more uniform as they do not have to adhere to rigid road geometry.
%

% Figure environment removed

\textbf{Path Efficiency.} Lastly, we also measure agent path efficiencies, defined as the ratio of the distance between trajectory endpoints to the trajectory length~\cite{AmirianZhangEtAl2020}. Intuitively, the closer to $100\%$, the closer the trajectory is to a straight line. As can be seen in~\cref{fig:path_eff} in the appendix, most path efficiency distributions are uniformly distributed, with peaks near $100\%$, echoing earlier straight-line findings. However, the INTERACTION~\cite{ZhanSunEtAl2019} dataset is an outlier in that its agent trajectories are predominantly straight lines with much less curved motion than other AV and pedestrian datasets.

\subsection{Annotation Quality}
\label{sec:annot_quality}

While analyzing datasets' true annotation accuracy would be best, neither we nor the original data annotators have access to the underlying real-world ground truth.
%
As a proxy, we instead analyze the \emph{self-consistency} of annotations in the form of incidence rates of collisions between agents, off-road driving, and uncomfortable high-acceleration events (using $0.4g$ as a standard threshold~\cite{Simons-MortonOuimetEtAl2009,KlauerDingusEtAl2009}).
%
%

%

% Figure environment removed

Virtually all observed agent data is free of collisions and off-road driving, save for rare one-offs (e.g., the INTERACTION dataset contains a minor car accident~\cite{ZhanSunEtAl2019}). We denote bounding box intersections between agents as collisions, and agent center-of-mass exiting the road boundary as off-road driving. Collisions typically indicate errors in bounding box annotations, whereas off-road driving can indicate erroneous bounding box dimensions, missing map coverage, or harsh driving that, e.g., cuts corners during a right turn.
%

As can be seen in~\cref{fig:failure_rates} (left), most vehicles in datasets experience collision rates below $5\%$. Of particular note is the fact that state-of-the-art autolabeling systems (e.g., used in Waymo~\cite{EttingerChengEtAl2021}) are nearly matching the accuracy of human annotations (e.g., used in nuscenes~\cite{CaesarBankitiEtAl2019}) in terms of resulting collision rates. However, detecting agents from a near-ground perspective (even with 3D LiDAR) is a very challenging task, and current performance still lags behind high altitude viewpoints. In particular, The INTERACTION~\cite{ZhanSunEtAl2019} dataset achieves orders of magnitude lower vehicle collision, off-road, and harsh acceleration rates owing to its drone-based data collection strategy. In theory, SDD~\cite{RobicquetSadeghianEtAl2016} should enjoy a similar advantage, but it only provides axis-aligned bounding box annotations (which overestimate agent extents) and Stanford's college campus contains much more interactive agents than other urban environments. More generally, the notion of bounding box intersections as collisions does not transfer exactly to pedestrians as they can enter/exit cars and walk in close groups, and further study is needed to robustly distinguish between errant motion and normal interactive motion.

In~\cref{fig:failure_rates} (middle), we find that vehicles in general experience very few ($<1\%$) harsh acceleration events, with Waymo~\cite{EttingerChengEtAl2021}, Lyft~\cite{HoustonZuidhofEtAl2020}, and nuScenes~\cite{CaesarBankitiEtAl2019} all having the highest incidence, commensurate with their earlier-discussed long-tail acceleration distributions. Lastly, we find in~\cref{fig:failure_rates} (right) that the INTERACTION~\cite{ZhanSunEtAl2019} and nuPlan~\cite{CaesarKabzanEtAl2021} agent annotations are well-aligned onto their maps, whereas nuScenes~\cite{CaesarBankitiEtAl2019} suffers from poor map coverage away from main roads (there are many annotated parked cars next to the main road) and Lyft~\cite{HoustonZuidhofEtAl2020} suffers from high false positive detections next to the main road (the majority of which take the Unknown class).

\section{Conclusions and Recommendations}
\label{sec:conclusion}
The recent releases of large-scale human trajectory datasets have significantly accelerated the field of AV research. However, their unique data formats and custom developer APIs have complicated multi-dataset research efforts (e.g.,~\cite{GillesSabatiniEtAl2022c,IvanovicHarrisonEtAl2023}). In this work, we present~\algname{}, a unified trajectory data loader that aims to harmonize data formats, standardize data access APIs, and simplify the process of using multiple AV datasets within the AV research community with a simple, uniform, and efficient data representation and development API.
We used \algname{} to comprehensively compare existing trajectory datasets, finding that, in terms of annotation self-consistency, drone-based data collection methods yield significantly more accurate birds-eye view bounding box annotations than even state-of-the-art AV perception stacks with LiDAR (albeit with much less spatial coverage), modern autolabeling pipelines are nearing human annotation performance, and smaller-scale pedestrian datasets can still be useful for investigations requiring high-agent-density scenarios.

As concrete recommendations, we saw that some datasets artificially limit the distance agents are autolabeled. Instead, it would be more useful to the long-range detection community to remove such restrictions, but add autolabeler-output uncertainties to long-range detections, supporting uncertain perception research along the way. Further, incorporating explicit self-consistency checks within autolabeling pipelines and catching, e.g., collisions, prior to release can both improve the autolabeling method as well as the resulting data labels. 

More broadly, providing researchers with access to more data comprised of various agent types from diverse geographies should help in modeling rare agent types and behaviors, in addition to aiding in the generalization of methods to multiple geographies. However, as we have seen in prior sections, there is an \emph{overwhelming} bias towards straight line driving, and one capability missing from \algname{} is the ability to (re)balance data on a semantic (behavioral) level. Finally, even if lower-level trajectory classes (e.g., driving straight, turning left/right, slowing down, speeding up, etc) are balanced, an important higher-level consideration during original dataset curation time is to ensure that AV datasets explore \emph{all} geographic regions within an environment, and not only those of certain socioeconomic statuses or transportation access.

Future work will address the current limitations of \algname{} (e.g., expanding the number of supported datasets and new capabilities such as geometric map element associations to support Waymo-like map formats~\cite{EttingerChengEtAl2021}). Further, incorporating sensor data would also enable perception research as well as joint perception-prediction-planning research, an exciting emerging AV research field.

\begin{ack}
We thank all past and present members of the NVIDIA Autonomous Vehicle Research Group for their code contributions to \algname{} and feedback after using it in projects.
\end{ack}

\bibliographystyle{IEEEtran}
\bibliography{main}

\clearpage
\newpage

\appendix

\section{Additional Dataset Details}
\label{supp:dataset_details}

\textbf{Licenses.} The ETH~\cite{PellegriniEssEtAl2009} and UCY~\cite{LernerChrysanthouEtAl2007} datasets are provided for research purposes\footnote{See the statement at the top of \url{https://icu.ee.ethz.ch/research/datsets.html} and in the ``Crowds Data" card of \url{https://graphics.cs.ucy.ac.cy/portfolio}.}. SDD~\cite{RobicquetSadeghianEtAl2016} is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 (CC BY-NC-SA 3.0) License. nuScenes~\cite{CaesarBankitiEtAl2019} and nuPlan~\cite{CaesarKabzanEtAl2021} are mostly licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License (CC BY-NC-SA 4.0), with modifications outlined in \url{https://www.nuscenes.org/terms-of-use}. The INTERACTION~\cite{ZhanSunEtAl2019} dataset is provided for non-commercial teaching and research use\footnote{See \url{http://interaction-dataset.com/terms-for-non-commercial-use} for full terms.}. The Lyft Level 5~\cite{HoustonZuidhofEtAl2020} dataset is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license (CC-BY-NC-SA-4.0).The Waymo Open Motion Dataset~\cite{EttingerChengEtAl2021} is licensed under its own Waymo Dataset License Agreement for Non-Commercial Use\footnote{Full terms can be found at \url{https://waymo.com/open/terms/}.}.

\textbf{Protecting Personal Privacy.} All datasets supported by \algname{} are captured in public spaces. Further, each of the ETH~\cite{PellegriniEssEtAl2009}, UCY~\cite{LernerChrysanthouEtAl2007}, SDD~\cite{RobicquetSadeghianEtAl2016}, and INTERACTION~\cite{ZhanSunEtAl2019} datasets capture data in public spaces from elevated fixed traffic cameras or drones, whose birds-eye viewpoints shield the faces of pedestrians and drivers from being collected. The nuScenes~\cite{CaesarBankitiEtAl2019}, Lyft Level 5~\cite{HoustonZuidhofEtAl2020}, Waymo Open Motion~\cite{EttingerChengEtAl2021}, and nuPlan~\cite{CaesarKabzanEtAl2021} datasets each preserve privacy by leveraging state-of-the-art object detection techniques to detect and blur license plates and faces.

\section{Map Statistics}
\label{supp:map_stats}

As shown in~\cref{tab:map_area}, current AV dataset maps are very large, spanning multiple neighborhoods in different cities. The INTERACTION~\cite{ZhanSunEtAl2019} dataset is a notable exception in magnitude, however, due to a drone camera's limited spatial observation range.

\setlength{\tabcolsep}{0.5em}
\begin{table}[t]
  \caption{Map statistics for maps currently supported by \algname{}. Note that the INTERACTION dataset does not provide pedestrian walkway or crosswalk information~\cite{ZhanSunEtAl2019}.}
  \label{tab:map_area}
  \centering
  \begin{tabular}{lccc}
    \toprule
    Dataset & Lane Length ($km$) & Road Area ($m^2$) & Pedestrian Area ($m^2$) \\
    \midrule
    nuScenes~\cite{CaesarBankitiEtAl2019} & \num{212.85} & \num{946275} & \num{250164} \\
    INTERACTION~\cite{ZhanSunEtAl2019} & \num{18.78} & \num{76502} & -- \\
    Lyft Level 5~\cite{HoustonZuidhofEtAl2020} & \num{185.42} & \num{591333} & \num{17359} \\
    nuPlan~\cite{CaesarKabzanEtAl2021} & \num{325.95} & \num{1327965} & \num{271277}\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Stationary Agents}
\label{supp:stationary}

\cref{tab:stationary} summarizes the amount of stationary agents per dataset. As can be seen, nuScenes ($17.5\%$) and Waymo Open ($53.6\%$) are comprised of many parked vehicles.

\setlength{\tabcolsep}{0.5em}
\begin{table}[t]
  \caption{Proportion of stationary agents per dataset. S/M denotes Single/Multi.}
  \label{tab:stationary}
  \centering
  \begin{tabular}{lr|lr}
    \toprule
    Dataset & Proportion & Dataset & Proportion \\
    \midrule
    ETH~\cite{PellegriniEssEtAl2009} & $4.0\%$ & INTERACTION S/M~\cite{ZhanSunEtAl2019} & $5.2\% / 4.5\%$ \\
    UCY~\cite{LernerChrysanthouEtAl2007} & $0.0\%$ & Lyft Level 5~\cite{HoustonZuidhofEtAl2020} & $0.1\%$ \\
    SDD~\cite{RobicquetSadeghianEtAl2016} & $5.1\%$ & Waymo Open~\cite{EttingerChengEtAl2021} & $53.6\%$ \\
    nuScenes~\cite{CaesarBankitiEtAl2019} & $17.5\%$ & nuPlan~\cite{CaesarKabzanEtAl2021} & $0.1\%$ \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Acceleration and Jerk}
\label{supp:acc_jerk}

Similar to the speed distributions in the main text, \cref{fig:acc} shows that Waymo~\cite{EttingerChengEtAl2021}, Lyft~\cite{HoustonZuidhofEtAl2020}, and SDD~\cite{RobicquetSadeghianEtAl2016} have long-tailed acceleration and jerk distributions. Further, as seen in the main text, the fixed-camera-based ETH~\cite{PellegriniEssEtAl2009} and UCY~\cite{LernerChrysanthouEtAl2007} datasets, as well as the drone-based INTERACTION dataset~\cite{ZhanSunEtAl2019}, yield tightly-clustered distributions around smooth motion (generally having small acceleration and jerk magnitudes).
Note that these values are derived by \algname{} via finite differencing. Accordingly, some overestimation of the acceleration and jerk distribution supports are to be expected.

% Figure environment removed

\section{Heading Distributions}
\label{supp:heading}

The distributions of (unnormalized) agent headings are shown in~\cref{fig:heading}. As can be seen, most distributions contain peaks around 0 and $\pm \pi/2$ radians, as north-south and east-west roads are very common in many cities. As a particular example, \cref{fig:polar_heading} visualizes heading distributions for vehicles and pedestrians in the Waymo Open Motion Dataset~\cite{EttingerChengEtAl2021}, showing that pedestrians have much more varied heading values than road-following vehicles.

% Figure environment removed

% Figure environment removed


\section{Path Efficiency}

As can be seen in~\cref{fig:path_eff}, most path efficiency distributions are uniformly distributed, with peaks near $100\%$ (shown as brightly-colored regions), echoing earlier straight-line findings. Further, the INTERACTION~\cite{ZhanSunEtAl2019} dataset is an outlier in that its vehicle and pedestrian trajectories are virtually all straight lines with much less curved motion than other AV and pedestrian datasets.

% Figure environment removed

%
%

%

\end{document}
