\documentclass[11pt,letterpaper]{article}
\pdfoutput=1 % for arXiv
\usepackage[height=8in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx, color}
\usepackage{array, multirow}
\usepackage[font=small,labelfont=bf]{caption} 
\usepackage{lipsum}
\usepackage{amsfonts}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{mathtools}
\usepackage{tikz}
\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi

% FIX MARGINS FOR arXiv
\AtBeginDocument{%
	\setlength{\oddsidemargin}{\dimexpr(\paperwidth-\textwidth)/2-1in}%
	\setlength{\evensidemargin}{\oddsidemargin}%
	\setlength{\topmargin}{%
		\dimexpr(\paperheight-\textheight)/2-\headheight-\headsep-1in}%
}

% Lists
\usepackage[shortlabels]{enumitem}

%\input{EDMDopt-macros}

% No orphans/widows
\clubpenalty10000
\widowpenalty10000

% Add a serial/Oxford comma by default.
\newcommand{\creflastconjunction}{, and~}

% Bibliography
\usepackage[sort]{natbib}
\def\bibfont{\small}
\setcitestyle{numbers,square,comma}
\setlength{\bibsep}{4pt plus 8pt}

% hyperlinks
\usepackage{hyperref}
\hypersetup{colorlinks=true,
			urlcolor=blue,
			linkcolor=blue,
			citecolor=blue,
			bookmarksdepth=paragraph}
\usepackage[nameinlink]{cleveref}
\crefname{equation}{}{}
\crefname{section}{section}{sections}
\crefname{figure}{figure}{figures}
\crefname{table}{table}{tables}
\crefname{example}{example}{examples}
\crefname{proposition}{proposition}{propositions}
\Crefname{section}{Section}{Sections}
\Crefname{figure}{Figure}{Figures}
\Crefname{table}{Table}{Tables}
\Crefname{definition}{Definition}{Definitions}
\Crefname{theorem}{Theorem}{Theorems}
\Crefname{remark}{Remark}{Remarks}
\Crefname{example}{Example}{Examples}
\Crefname{proposition}{Proposition}{Propositions}
\numberwithin{equation}{section}

% Theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{exmp}{Example}[section]

% Commands
\newcommand{\alert}[1]{{\color{red}[#1]}}
% fix for the QED in equation    
%\newenvironment{equation}{\[}{\]\ignorespacesafterend}

\title{Transferability of Graph Neural Networks using Graphon and Sampling Theories}

\author{A. Martina Neuman$^1$, Jason J.\ Bramburger$^2$}
\date{$^1$Department of Mathematics and Statistics, University of Vienna, Vienna, Austria\\
$^2$Department of Mathematics and Statistics, Concordia University, Montr\'eal, QC, Canada}


\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
Graph neural networks (GNNs) have become powerful tools for processing graph-based information in various domains. A desirable property of GNNs is transferability, where a trained network can swap in information from a different graph without retraining and retain its accuracy. A recent method of capturing transferability of GNNs is through the use of graphons, which are symmetric, measurable functions representing the limit of large dense graphs. In this work, we contribute to the application of graphons to GNNs by presenting an explicit two-layer graphon neural network (WNN) architecture. We prove its ability to approximate bandlimited signals within a specified error tolerance using a minimal number of network weights. We then leverage this result, to establish the transferability of an explicit two-layer GNN over all sufficiently large graphs in a sequence converging to a graphon. Our work addresses transferability between both deterministic weighted graphs and simple random graphs and overcomes issues related to the curse of dimensionality that arise in other GNN results. The proposed WNN and GNN architectures offer practical solutions for handling graph data of varying sizes while maintaining performance guarantees without extensive retraining.
\end{abstract}

\paragraph{Key words:} graphon, regularized sampling, graph neural network, transferability, random graph, curse of dimensionality

%% Table of contents
\tableofcontents


%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Falling under the category of geometric deep learning models \cite{bronstein2021geometric}, graph neural networks (GNNs) have become a powerful tool for processing information that can be represented on graphs \cite{wu2022graph,scarselli2008graph,micheli2009neural}. GNNs are a class of neural networks that incorporate an underlying graph topology to facilitate information exchange among neighboring vertices; they have been applied to a variety of different domains including natural language processing \cite{wu2023graph}, chemistry \cite{jiang2021could,fung2021benchmarking,gilmer2017neural}, citation networks \cite{bhagavatula2018content,cummings2020structured,hamilton2017inductive}, and recommender systems \cite{ruiz2020graphon,huang2021mixgcf,gao2022graph,wu2022graph,ricci2021recommender,resnick1997recommender,gao2023survey}. GNNs have been shown to outperform the predictive power of traditional neural networks \cite{ala2020improving,ma2021deep} and can generalize bandlimited functions using far fewer network weights than the best-known results for deep neural networks \cite{neuman2022superiority}. For a more complete introduction to the theory and application of GNNs see the review \cite{zhou2020graph}.  
 
When it comes to GNNs, one of the most desirable properties is that of {\em transferability}. In this context transferability means that a GNN can be transported between different graphs without re-training, with performance guarantees. A simple example of transferability would be providing robustness guarantees on a GNN when there are alterations in the underlying edge or weight structure, measured by a suitable metric. Beyond just manipulating the edge structure of a graph, one can similarly ask for transferability between graphs of different sizes. An important example of when one would desire such transferability comes from recommender systems. In this case the vertices of the network represent users/subscribers, while edge weights could represent a similarity measure between them. The goal is to leverage the knowledge of one user's taste or interests to curate recommendations for users deemed similar. In this situation one should expect the number of users to vary frequently as new ones sign up for the platform and others end their subscriptions. Certainly, one does not want to re-train a recommender neural network each time a user is added or lost, especially since all of the structure in the graph remains the same between users who continue on the platform. Thus, one seeks to identify a single GNN architecture that exhibits transferability between these similar user networks with varying numbers of vertices.     

To achieve transferability of GNNs over similar networks of different sizes, \cite{ruiz2020graphon} introduced the concept of a {\em graphon neural network} (WNN). The key ingredient is a graphon - a symmetric, measurable function $W:[0,1]^2 \to [0,1]$ that can be used to represent the limit of large dense graphs as the number of vertices tends to infinity \cite{lovasz2006limits,Janson,borgs2017sparse,glasscock2015graphon}. Members of a sequence of graphs converging to a graphon $W$ can be seen as sharing structural characteristics, despite not being of the same size, much along the line of our motivating example of recommender systems. The ability for graphons to capture similarity of graphs of different sizes was employed in \cite{ruiz2020graphon} to prove transferability of GNNs for sufficiently large graphs coming from the same sequence that converges to a graphon. 

Graphons continue to be employed to capture transferability and other desirable properties of GNNs. The work \cite{maskey2023transferability} uses graphons to prove transferability of spectral graph neural networks, while GNN stability with respect to graphon and graph perturbations has also been investigated \cite{ruiz2021graphon,keriven2020convergence}. Transferability results are examined experimentally in \cite{ruiz2021transferability} where GNNs are trained on moderately sized graphs and then transferred to large graphs to evaluate theoretical performance guarantees. Graphons have also been estimated from data to train GNNs \cite{hu2021training}, and used to perform pooling and sampling in GNNs, with the latter application demonstrating that graphon pooling GNNs are less prone to overfitting \cite{parada2021graphon}. While the concept of a WNN primarily exists in theory to describe transferability, \cite{cervino2023learning} proposes a technique for learning a WNN by training GNNs on growing networks sampled from the graphon. Finally, there is also an emerging literature of signal processing using graphons \cite{ruiz2021graphon,morency2021graphon,ruiz2021graphonsignal,ruiz2021graphonprocessing} that is particularly useful in describing structurally-similar graph signals over varying network sizes that are used to train GNNs.    

In this work we contribute to the emerging application of graphons to data science by providing an explicit two layer WNN architecture and proving that it can approximate a bandlimited signal within an $\varepsilon > 0$ error tolerance using $\mathcal{O}(\varepsilon^{-10/9})$ network weights. This result is then leveraged to prove transferability of an explicit two layer GNN among all sufficiently large graphs in a sequence converging to a graphon. In particular, transferability is accounted for between both simple deterministic weighted graphs and simple random graphs that belong to the same graphon family, of which the latter is often overlooked in the WNN literature. Importantly, graphons allow us to circumvent any issues related to the curse of dimensionality, and in particular, keep the number of layers bounded independently of $\varepsilon$. This is because graphons embed data into the one-dimensional interval $[0,1]$. The reader should compare with similar results on approximating bandlimited functions on high-dimensional domains \cite{montanelli2021deep,chen2019note}, where one requires the numbers of weights and layers to become unbounded as $\varepsilon \to 0^+$. 

Our restriction to bandlimited functions enables a sampling result that motivates the WNN architecture we present herein, and builds upon a growing literature on the ability of neural networks to approximate bandlimited functions \cite{neuman2022superiority,chen2019note,montanelli2021deep,opschoor2022exponential,wang2018exponential,dziedzic2019band,wang2022convolutional}. Further, we consider bandlimited functions in the traditional sense that they are a linear combination of only finitely many Fourier modes. This is different from some emerging related literature where bandlimited signals replace the Fourier modes with eigenfunctions of the underlying graphon or manifold \cite{ruiz2021graphonprocessing,wang2022convolutional}. In practice one rarely has access to the graphon and graphon signals beyond finitely-many samples of them and so we view working with Fourier bandlimited functions as potentially more practical. Nonetheless, we note that in the case of graphons that only depend on the distance $|x - y|$ (so-called ring graphons) the eigenfunctions are exactly the Fourier modes \cite{bramburger2023pattern}, causing the bandlimited definitions to coincide.   

We summarize our results in this paper as follows:
\begin{enumerate}
    \item We prove a sampling theorem that gives an explicit reconstruction of bandlimited functions on $[0,1]$, with a subgeometric convergence rate, and provides a foundation for our WNN and GNN architectures.
    \item Informed by our sampling theorem, we provide an explicit shallow WNN architecture comprised of only two layers. We prove a generalization theorem that shows that this architecture can reproduce bandlimited graphon signals with an $L^2$ error $\varepsilon > 0$ using only $2N = \mathcal{O}(\varepsilon^{-10/9})$ evenly spaced samples from $[0,1]$. 
    \item Through a discretization of the interval $[0,1]$, the WNN architecture leads to a transferable GNN architecture for which graph adjacency matrices are simply swapped into the filter computational unit. We show performance guarantees on the transferability of these GNNs for all sufficiently large simple deterministic weighted graphs and simple random graphs belonging to the same graphon family.   
\end{enumerate}

This paper is organized as follows. We begin in \S\ref{sec:Preliminaries} with a review of all of the important concepts, definitions, and auxiliary results that we will use throughout the remainder of the paper. Precisely, \S\ref{sec:GNNIntro} reviews GNNs, \S\ref{sec:graphons} introduces graphons and \S\ref{sec:WNN} defines WNNs as a generalization of GNNs, bandlimited functions are discussed in \S\ref{sec:Fsamptheory}, and finally \S\ref{sec:WNNframework} introduces our specific WNN and GNN architectures. Our main results are presented in \S\ref{sec:Results}, organized into subsections according to the summarized contributions above, followed by a concluding discussion in \S\ref{sec:Ramifications} on their ramifications.  The proof of our sampling result is left to \S\ref{sec:Tsamplingthm}, the proof of WNN generalization is in \S\ref{sec:WNNthm}, and GNN proofs can be found in Sections~\ref{sec:GNNdet} and \ref{sec:GNNran}. We conclude in \S\ref{sec:Discussion} with a discussion of our results and some important avenues for future investigation.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Preliminaries}\label{sec:Preliminaries}

We introduce here the symbols, notations, and conventions that will be used consistently throughout this paper. The symbol $\mathbb{N}$ will denote the set of natural numbers. We preserve the letters $j, k, l, n$ to represent integers, while $\mathfrak{m}, L, M, N, d, m$ specifically represent natural numbers, and the latter two indicating dimensions. For $N\in\mathbb{N}$, we define an integer interval $B_N$ by
\begin{equation*}
    B_N := \{-N, -N+1, \cdots, 0, \cdots, N-2, N-1\}. 
\end{equation*}
%Given $N,\mathfrak{m}\in\mathbb{N}$, by ``$N>\mathfrak{m}$", we mean that $N>2\mathfrak{m}$ at least. 
Given a set $X$ of objects, we denote its cardinality by the notation $\texttt{\#} X$.

We employ a conventional abuse of notation for the symbol $|\cdot|$. When $I$ is an interval, then $|I|$ means the length of $I$. When $x$ is an element in a Euclidean space, $|x|$ denotes its Euclidean norm.  

For $p=1,2$, we will consider the Banach spaces $L^p(X;\mathbb{C}^d)$ consisting of all functions $f:X \to \mathbb{C}^d$ for which
\begin{equation*}
    \|f\|_{L^p(X;\mathbb{C}^d)} := \bigg(\int_X |f(x)|^p\,\mathrm{d}x\bigg)^{1/p}
\end{equation*}
is finite. Here, the integral is interpreted in the Bochner sense. When $X$ is discrete the integral reduces to a summation over the individual elements of $X$, i.e. 
\begin{equation*}
    \|f\|_{L^p(X;\mathbb{C}^d)} = \bigg(\sum_{x \in X} |f(x)|^p \bigg)^{1/p}.
\end{equation*}
We preserve the symbols $\ell^1$ and $\ell^2$ for when $X=\mathbb{Z}$, giving
\begin{equation*}
    \|f\|_{\ell^p(\mathbb{Z};\mathbb{C}^d)} := \bigg(\sum_{j \in\mathbb{Z}} |f(j)|^p \bigg)^{1/p}, \quad p=1,2.
\end{equation*}
We will futher consider the case of $p = \infty$, which leads to the $L^\infty$-norm of a function $f$:
\begin{equation*}
    \|f\|_{L^\infty(X;\mathbb{C}^d)} := \sup_{x\in X} |f(x)|.
\end{equation*}
For convenience, when $d = 1$ we simply write $L^p(X)$ and $\ell^p(\mathbb{Z})$. 

We also employ another abuse of notation when referring to the congruence symbol $\cong$. When $X$, $Y$ are two sets, then $X\cong Y$ means $X, Y$ are isomorphic as sets. For example, the torus $\mathbb{T}\cong [0,1)$ or $\mathbb{T}\cong [-1/2,1/2)$ as sets. When $X, Y$ are two Lebesgue measure sets, then $X\cong Y$ means that the set difference $X\Delta Y$ has Lebesgue measure zero. For example, $[0,1]\cong [0,1)$. Lastly, when $X,Y$ are both groups, then $X\cong Y$ signifies that they are isomorphic as groups, however this usage only appears in Appendix~\ref{sec:Tsamp}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graph Neural Networks}\label{sec:GNNIntro}

We represent a graph by the triple $G = (V,E,w)$. Here $V$ denotes the set of graph vertices and $E\subset V\times V$ the set of graph edges. Suppose $\texttt{\#}V =n$, for some $n\in\mathbb{N}$. By adopting a predetermined labeling for the vertices, we simply express $V$ as $V = \{v_1,v_2,\dots,v_n\}$. The edge weight function $w: V\times V\to [0,1]$ describes the connections among the vertices. That is, $w(v_k,v_l) > 0$ if and only if there exists an edge connecting $v_k, v_l$, i.e. $(v_k,v_l) \in E$. In this paper, we restrict ourselves to the case of simple, undirected graphs, meaning that $w$ meets two conditions: symmetry, $w(v_k,v_l) = w(v_l,v_k)$ and absence of self-loops, $w(v_k,v_k)=0$. Associated with the edge weight function is the $n\times n$ graph adjacency matrix, symbolized by ${\bf A}$, whose $kl$-entry satisfies $[{\bf A}]_{kl}=w(v_k,v_l)$. 

A {\em graph signal} is a function $f\in L^2(V;\mathbb{C}^m)$, for some $m \in \mathbb{N}$, giving each vertex $v_k \in V$, a set of features $f(v_k)$. Often in many applications of GNNs, vertices are assigned coordinate vectors that depict their locations in space, say $v_k$ is given as $X_k \in \mathbb{R}^d$. 
In this case, the graph signal $f$ can be interpreted as a function in $L^2(\mathbb{R}^d;\mathbb{C}^m)$. From an analytical standpoint, a GNN $\Psi_G$ on $G$ operates as a graph signal, which means, in this context, $\Psi_G\in L^2(\mathbb{R}^d;\mathbb{C}^m)$. The input of the GNN is the collection of all of the feature vectors $X_k$, called the (input) {\it feature matrix} and denoted by
\begin{equation}\label{def:featuremat}
    {\bf X} := \begin{bmatrix} 
        X_1 & X_2 & \cdots & X_n
    \end{bmatrix}\in \mathbb{R}^{d\times n}.
\end{equation} 
The output feature of $\Psi_G$ at each vertex is a vector in $\mathbb{C}^m$; the collection of such vectors is the (output) feature matrix
\begin{equation*}
    {\bf Y} := \begin{bmatrix} 
        \Psi_G(X_1) & \Psi_G(X_2) & \cdots & \Psi_G(X_n)
    \end{bmatrix}  \in \mathbb{C}^{m\times n}.
\end{equation*}
The input and output dimensions, respectively, $d, m$, are problem dependent. In terms of structure, the defining characteristic of a GNN as a neural network lies in its architecture. Specifically, GNNs are built by combining multiple iterations of two distinct computational units, given as follows:
\begin{itemize}
    \item {\bf Multilayer Perceptron (MLP)}: An MLP is a two-step process of first applying an affine transformation and then a nonlinear activation function. For example, for a weight matrix ${\bf W} \in\mathbb{C}^{m\times d}$, a {\it bias vector} ${\bf b}\in\mathbb{C}^{m}$, and a nonlinear activation function $\rho$, an MLP unit acts symbolically on ${\bf Z}\in\mathbb{C}^{d\times n}$ as 
    \begin{equation*} 
        {\bf Z} \mapsto \rho({\bf W}{\bf Z} + {\bf b}),
    \end{equation*}
    where $\rho$ is applied componentwise. These units are the fundamental building blocks of feedforward neural networks. Throughout this work we consider ReLU (rectified linear unit) activation functions, $ReLU(x) = \max\{x,0\}$, for which the resulting GNN will be called a {\it ReLU GNN}.
    \item {\bf Filter}: A graph filter unit $\mathfrak{F}$ is what distinguishes a GNN from a standard feedforward NN architecture. Generally, a graph filtering process is any process that takes in nodal features (e.g. {\bf X} in \eqref{def:featuremat}) {\it and} graphical structure $(V,E,w)$ and outputs a new set of nodal features. There are two types of graph filters: spatial-based and spectral-based. The spatial filters explicitly leverage the graph connections to perform a feature refining process, whereas the spectral ones utilize spectral graph theory to design filtering in the spectral domain. Moreover, some well-known spectral filters can be considered spatial filters \cite{ma2021deep}.
\end{itemize}

With the above, the operation of a GNN can be expanded as:
\begin{equation}\label{gnnex}
    {\bf Y}\equiv \rho_{L}({\bf W}^{(L)} \psi_{L-1}\circ\psi_{L-2} \circ\cdots\circ \psi_1({\bf X})+{\bf b}^{(L)}).
\end{equation}
Here, $L\in\mathbb{N}$, and each $\psi_{j}$ in \eqref{gnnex} is a (intermediate) {\it network layer}, which is one composition of an MLP and a filter units, e.g.
\begin{equation}\label{GNNlayer}
    \begin{split}
        &{\bf H}_{j} = \psi_{j}({\bf H}_{j-1}) = \mathfrak{F} (\rho({\bf W}^{(j)}{\bf H}_{j-1} + {\bf b}^{(j)})), \quad  j=1,\cdots, L-1,\\
        &{\bf H}_0 \equiv {\bf X}.
    \end{split}
\end{equation}
This says that at the $j$th layer, the output of the previous layer ${\bf H}_{j-1}$, is fed to an MLP (with weight matrix ${\bf W}^{(j)}$, bias vector ${\bf b}^{(j)}$ and activation $\rho$) and then to a graph filter $\mathfrak{F}$. A common alternative to \eqref{GNNlayer} is
\begin{equation*}
    {\bf H}_{j} = \psi_{j}({\bf H}_{j-1}) = \rho(\mathfrak{F}({\bf W}^{(j)}{\bf H}_{j-1} + {\bf b}^{(j)})), 
\end{equation*}
where there is a switch between the filter and activation applications. In this work, we will specifically employ the configuration \eqref{GNNlayer}. Note from \eqref{gnnex} that $\mathfrak{F}$ is not performed at the last $L$th layer and that the last activation $\rho_{L}$ is optional - for example, one can take $\rho_{L}$ to be a softmax function to ensure that the output is bounded. Furthermore, $\mathfrak{F}$ should be enabled at a minimum of one - but not necessarily all - intermediate layers. The GNN {\it network parameter}, often denoted by $\theta$, is the set of all entries of all the weight matrices, all the bias vectors, and the number of layers, i.e.
\begin{equation*}
    \theta=\{\{{\bf W}^{(j)}\}_{kl},{\bf b}^{(j)}, j=1,\cdots,L\}.
\end{equation*}

Throughout this work we will focus on (linear) spatial-based graph filters. Such filtering exchanges information between vertices of the graph. This is achieved by a {\it graph filter kernel} (sometimes referred to as a {\em graph shift operator} \cite{ruiz2020graphon}) ${\bf K} \in \mathbb{C}^{n \times n}$ which encodes the topology of the %related 
graph $G$. Namely, if ${\bf Z}\in\mathbb{C}^{d\times n}$, then the filter layer takes the form 
\begin{equation}\label{GSO}
    \mathfrak{F}: \quad {\bf X} \mapsto {\bf K}{\bf Z}^{T}.
\end{equation} 
The choice of ${\bf K}$ can be determined by the user and is often problem dependent. As will become apparent in the forthcoming subsections, we explore graph filters of the form
\begin{equation}\label{GSOchoice}
    {\bf K} = {\bf G} \circ {\bf A},
\end{equation}
where $\circ$ is the Hadamard product, and ${\bf A}$ is again the adjacency matrix of $G$. The matrix ${\bf G}\in\mathbb{C}^{n\times n}$ in \eqref{GSOchoice} is used to localize information spread by introducing a near-sparsity pattern to ${\bf K}$. This sparsification effect is particularly helpful when $G$ is a dense graph, since then ${\bf A}$ is a dense matrix, and so information ${\bf Z}$ in \eqref{GSO} would be spread globally over $G$ simply through matrix multiplication by ${\bf K}$. 

%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graphons} \label{sec:graphons}

A {\em graphon} is a symmetric, Lebesgue-measurable function $W:[0,1]^2 \to [0,1]$. The terminology graphon is a combining graph and function and is a continuum generalization of a graph adjacency matrix. This interpretation comes from envisioning $[0,1]$ as a graph of an uncountable number of vertices, where the values $W(x,y)\in [0,1]$ represents the weight of an edge between vertices $x,y \in [0,1]$ - as is common with weighted graphs, one interprets $W(x,y) = 0$ as no edge being present. 

\paragraph{Graphons as limit objects.} Graphons arise as limits of sequences of graphs on increasing numbers of vertices. To give a meaningful interpretation of graph convergence, we introduce the concept of ``homomorphism density" from a graph $G$ into another graph $H$ as follows:
\begin{equation*}
    t(G,H) := \frac{\texttt{\#} \mathrm{Hom}(G,H)}{ (\texttt{\#} V_H)^{\texttt{\#} V_G}},
\end{equation*}
where $V_H$, $V_G$ denote the number of vertices in graphs $G$, $H$, respectively, and $\texttt{\#} \mathrm{Hom}(G,H)$ is the number of adjacency preserving maps ({\it graph homomorphisms}) from $G$ to $H$. Thus, the ratio that defines $t(G,H)$ can be interpreted as the probability that a map from the vertices in $G$ to those in $H$ will be a homomorphism, with $(\texttt{\#} V_H)^{\texttt{\#} V_G}$ being the total number of all possible maps. A sequence of graphs $\{G_n\}_{n = 1}^\infty$ is said to converge if for all finite simple graphs $F$ the limit of the real-valued sequences $\{t(F,G_n)\}_{n = 1}^\infty$ exist. On the other hand, the homomorphism density of a graph $F$ into a graphon $W$ can be defined in an analogous manner \cite[Chapter~7.2]{lovasz2012large}, resulting in a ratio denoted as $t(F,W)$. Then, as is shown in \cite[Chapter~11]{lovasz2012large}, for every convergent sequence of graphs $\{G_n\}_{n = 1}^\infty$ there exists a graphon $W$ so that
\begin{equation}\label{graphlim}
    \lim_{n \to \infty} t(F,G_n) = t(F,W),
\end{equation}
for every finite simple graph $F$. We observe that \eqref{graphlim} resembles the conventional notion of pointwise convergence in analysis. Therefore, through \eqref{graphlim} we may think of the graphs $\{G_n\}_{n=1}^\infty$ as belonging to the same graphon family. Next, we present two important sequences of graphs from the same family that are particularly relevant to our discussion. 

\paragraph{Generating graphs from graphons.} A major application of graphons is to generate graphs on finitely many vertices. Typically this is done by discretizing the unit interval $[0,1]$ into $n\in\mathbb{N}$ points as
\begin{equation}\label{xj}
    x_k := \frac{k - 1}{n}, \quad k = 1,\cdots,n,
\end{equation}
which represent graph vertices. Denote $\mathcal{X}_n:=\{x_1, \cdots, x_n\}$, and the resulting intervals of length $1/n$ by $I_k := [(k-1)/n, k/n)$, $k=1,\cdots, n$. 

One can use $W$ to prescribe a simple {\it deterministic weighted graph} $G^{\mathrm{det}}_n$ on $n$ vertices $v_1, \cdots, v_n$, identified with $x_1, \cdots, x_n$, respectively. To do so, associate with $G^{\mathrm{det}}_n$ the symmetric adjacency matrix ${\bf A}^{\mathrm{det}}_n$, defined by
\begin{equation*}
    [{\bf A}^{\mathrm{det}}_n]_{k,l} := \begin{cases}
        W(x_k,x_l) & k \neq l\\
        0 & k = l
    \end{cases}, 
\end{equation*}
for $k,l=1,\cdots, n$. Hence the edge weight $[{\bf A}^{\mathrm{det}}_n]_{k,l}$ between $v_k$, $v_l$ is given by the value of $W$ in the bottom left corner of the square $I_k\times I_l = [x_k, x_{k+1})\times [x_l, x_{l+1})$. Furthermore, we assert that an edge exists between $v_k$, $v_l$ if and only if $[{\bf A}^{\mathrm{det}}_n]_{k,l}>0$. %Thus, all pertinent information for a weighted graph has been encoded.

Likewise, one can also use $W$ to generate a simple {\it random graph} $G^{\mathrm{ran}}_n$, denoted in analogy with the deterministic graph presentation, on $n$ vertices $v_1, \cdots, v_n$. In this case, the associated (random) adjacency matrix ${\bf A}^{\mathrm{ran}}_n$ is given by
\begin{equation*}
    [{\bf A}^{\mathrm{ran}}_n]_{k,l} := \begin{cases}
        \xi_{k,l} & k \neq l\\
        0 & k = l
    \end{cases}, 
\end{equation*}
for $k,l=1,\cdots, n$. Here, $\xi_{k,l} = \xi_{l,k}$ are independent Bernoulli random variables whose probability distribution is given by
\begin{equation*}
    \mathbb{P}(\xi_{k,l} = 1) = 1 - \mathbb{P}(\xi_{k,l} = 0) = W(x_k,x_l), \qquad \forall k > l.
\end{equation*}
Again, all relevant graph information is captured by the adjacency matrix ${\bf A}^{\mathrm{ran}}_n$: an edge appears between $v_k, v_l$ if $\xi_{k,l}=\xi_{l,k}=1$. 

The graphs in both sequences $\{G^{\mathrm{det}}_n\}_{n = 1}^\infty$ and $\{G^{\mathrm{ran}}_n\}_{n = 1}^\infty$ belong to the graphon family of $W$. Indeed, for all finite simple graphs $F$, one has 
\begin{equation}\label{detGraphConv}
    \lim_{n \to \infty} t(F,G^{\mathrm{det}}_n) = t(F,W), 
\end{equation}
and 
\begin{equation}\label{randGraphConv}
    \lim_{n \to \infty} t(F,G^{\mathrm{ran}}_n) = t(F,W), 
\end{equation}
with probability 1 \cite[Chapter~11]{lovasz2012large}.

\paragraph{Embedding graphs into the graphon space.}
Graphs can be lifted into the space of graphons. Let $G = (V,E,w)$ be a finite simple graph on $\texttt{\#} V = n$ vertices $v_1, \cdots, v_n$. We define the step graphon $W_n:[0,1]^2 \to [0,1]$ by
\begin{equation}\label{def:tildeW_n}
    \overline{W}_n(x,y) := \sum_{k,l = 1}^n w(v_k,v_l)\chi_{I_k\times I_l}(x,y).
\end{equation}
That is, the vertices $v_k$ of $G$ are identified with the points $x_k \in [0,1]$ as in \eqref{xj}, while $\overline{W}_n$ is a simple function taking on the $n^2$ edge weight values of $w$ on each of the squares $I_k\times I_l$. In particular, when $G$ is a simple deterministic graph generated from a graphon $W$, the formula \eqref{def:tildeW_n} becomes
\begin{equation*}
    \overline{W}_n(x,y) = \sum_{k,l = 1}^n w(v_k,v_l)\chi_{I_k\times I_l}(x,y) = \sum_{\substack{k,l=1\\ k\not= l}}^n W(x_k, x_l)\chi_{I_k\times I_l}(x,y).
\end{equation*}
Note that $\overline{W}_n(x_k,x_l)=w(v_k,v_l)=W(x_k, x_l)$, $k\not= l$. 

As a point of completion, we mention that the space of graphons is typically endowed with the so-called {\em cut norm}, a metric given by
\begin{equation}\label{cutNorm}
    \|W\|_\square := \sup_{S,T} \bigg|\int_{S \times T} W(x,y)\mathrm{d}x\mathrm{d}y\bigg|.
\end{equation}
Importantly, the convergence \eqref{detGraphConv} and \eqref{randGraphConv} in the homomorphism density is equivalent to the convergence of the respective step graphons to $W$ in the cut norm \cite[Theorem~11.3]{lovasz2012large}. 

\paragraph{Graph and graphon signals.} Similar to the representation of graph signals, a {\it graphon signal} is a vector-valued function assumed to belong to $L^2([0,1];\mathbb{C}^m)$. An $m$-dimensional graph signal $g$ on a graph $G$ of $n$ vertices has a corresponding graphon signal representative, defined as follows. As usual, identify the vertices $v_k$ of $G$ with $x_k\in\mathcal{X}_n$ as in \eqref{xj}. In this context, $g$ can be viewed as a function defined on $\mathcal{X}_n$: $g(v_k) = g(x_k)$. We then embed $g$ into $L^2([0,1];\mathbb{C}^m)$ as the step function
\begin{equation}\label{def:tildef}
    \overline{g}(x) := \sum_{k = 1}^n g(x_k)\chi_{I_k}(x) = \sum_{k = 1}^n g(v_k)\chi_{I_k}(x), \quad\forall x\in [0,1].
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Graphon neural networks} \label{sec:WNN}

Drawing upon the analogy between graphs and graphons, we can define a WNN as an extension of the GNN framework. Specifically, a WNN can be expressed as a function $\Psi: [0,1]\to \mathbb{C}^{m}$, and just like a GNN, is constructed through a finite number of iterated compositions involving two distinct computational units:
\begin{itemize}
    \item {\bf MLP}: An MLP layer for a WNN is again a two-step process. Let $b\in \mathbb{C}^{m}$ be the bias vector, ${\bf W} \in\mathbb{C}^{m\times d}$ a weight matrix, and $\rho$ be a nonlinear activation function. Then, for $g\in L^2([0,1];\mathbb{C}^d)$, an MLP in a WNN takes the form
    \begin{equation*} 
        g(x) \mapsto \rho({\bf W}g(x) + b), \quad \forall x\in [0,1],
    \end{equation*}
    where $\rho$ is applied componentwise. We will again consider $\rho = ReLU$ and refer to such WNN as a {\it ReLU WNN}.
    \item {\bf Graphon filter}: A graphon filter is a generalization of a graph filter. Namely, let $\mathcal{K}: [0,1]^2\to\mathbb{C}$ such that $\mathcal{K}\in L^{\infty}([0,1]^2)$. Then the graphon filter is an operator $T_\mathcal{K}$ acting on $g = (g_1,\cdots,g_m)\in L^2([0,1];\mathbb{C}^{m})$ as
    \begin{equation}\label{def:TK}
        \begin{split}
            T_\mathcal{K}g(x) &= (T_\mathcal{K}g_1(x), \cdots, T_\mathcal{K}g_m(x))\\
            &:= \bigg(\int_0^1 \mathcal{K}(x,y)g_1(y)\,\mathrm{d}y, \cdots, \int_0^1 \mathcal{K}(x,y)g_m(y)\,\mathrm{d}y\bigg), \quad\forall x\in [0,1].
        \end{split}
    \end{equation} 
\end{itemize}

The choice of $\mathcal{K}$ can be either user or problem determined. For example, by taking $\mathcal{K}$ to be a graphon $W$, we obtain the operator $T_W$ which is well-studied in the literature \cite{Janson,lovasz2012large,ruiz2021graphonsignal}. For arbitrary $\mathcal{K}$, the action of a WNN closely mirrors that of a GNN, which if we adopt the configuration \eqref{GNNlayer}, can be expressed symbolically as follows:
\begin{equation*}
    \Psi(g_0)(x) \equiv \rho_{L}({\bf W}^{(L)} \psi_{L-1}\circ\psi_{L-2} \circ\cdots\circ \psi_1(g_0)(x) +b^{(L)}), \quad\forall x\in [0,1].
\end{equation*}
Here, $L\in\mathbb{N}$, with the input graphon signal $g_0\in L^2([0,1];\mathbb{C}^{d})$, and each intermediate layer $\psi_{j}$ is presented as
\begin{equation*}
    g_{j}(x) = \psi_{j}(g_{j-1})(x) = T_{\mathcal{K}} (\rho({\bf W}^{(j)}g_{j-1}(x) + b^{(j)})), \quad  j=1,\cdots, L-1.
\end{equation*}
The last activation $\rho_{L}$ is optional, and filtering is performed at a minimum of one - but not necessarily all - intermediate layer. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bandlimited graphon signals and sampling} \label{sec:Fsamptheory}

Let $f=(f_1,\cdots, f_m): [0,1]\to\mathbb{C}^m$ be a graphon signal. We assume $f\in L^2([0,1];\mathbb{C}^{m})$, and therefore $f$ can be expressed as a Fourier series,
\begin{equation*}
    f(x) = \sum_{k\in\mathbb{Z}} \hat{f}(k)e^{i2\pi nx},
\end{equation*}
where $\hat{f}(k)=(\hat{f}_1(k),\cdots,\hat{f}_{m}(k))$, and the equal sign equates two members of $L^2([0,1];\mathbb{C}^m)$.  We are interested in profiles $f$ whose Fourier coefficients 
\begin{equation}\label{Fcoef}
    \hat{f}(k)=(\hat{f}_1(k),\cdots,\hat{f}_{m}(k)):=\int_0^1 f(x)e^{-i2\pi kx}\,\mathrm{d}x 
\end{equation} 
are zero for all but a finite number of modes $k\in\mathbb{Z}$. To avoid confusion with another notion of bandlimitedness circulated in the current graphon literature \cite{ruiz2020graphon}, we call such $f$ {\it Fourier bandlimited}.

\begin{definition} \label{def:Fblmtd}
Let $\mathfrak{m}\in\mathbb{N}$ and $f=(f_1,\cdots,f_{m})\in L^2([0,1];\mathbb{C}^{m})$. Then $f$ is said to be $\mathfrak{m}$-Fourier bandlimited, or $f\in\mathcal{B}_\mathfrak{m}$, if
\begin{equation*}
    \hat{f}(k) = 0, \quad \forall k\not\in B_\mathfrak{m} = \{-\mathfrak{m},\cdots,0,\cdots,\mathfrak{m}-1\}.
\end{equation*}
or equivalently, 
\begin{equation*}
    \hat{f}_j(k) = 0, \quad \forall k\not\in B_\mathfrak{m} = \{-\mathfrak{m},\cdots,0,\cdots,\mathfrak{m}-1\}
\end{equation*}
for every $j=1,\cdots,m$.
\end{definition}

It is well-known that a Fourier bandlimited $f$ is almost everywhere equal to a continuous function $f_{c}$ on $[0,1]$ where $f_{c}(0)=f_{c}(1)$ and
\begin{equation}\label{contf}
    f_{c}(x) = \sum_{k=-\mathfrak{m}}^{\mathfrak{m}-1} \hat{f}(k) e^{i2\pi kx}. 
\end{equation}
Therefore, we can and will identify $f$ with this continuous version on $[0,1]$ and on $\mathbb{T}\cong [0,1)$. A function that is Fourier bandlimited is significant in the sense that all its information is stored in its spatial sampling, as encapsulated by the following proposition.

\begin{proposition}\label{prop:gpsampling} 
Let $f\in L^2([0,1];\mathbb{C}^m)\cap\mathcal{B}_\mathfrak{m}$ and $N\geq\mathfrak{m}$. Then, 
\begin{equation}\label{gpsampcited}
    f(x) = \sum_{j=0}^{2N-1} f(j/2N)s_N(x-j/2N), \quad \forall x\in [0,1],
\end{equation}
where 
\begin{equation}\label{def:sampf}
    s_N(x):=\frac{1}{2N}\sum_{k=-N}^{N-1} e^{i2\pi kx}, \quad \forall x\in [0,1].
\end{equation}
\end{proposition}

Proposition~\ref{prop:gpsampling} is a straightforward consequence of the Kluvan\'ek's sampling theorem~\cite{kluvanek1965sampling}, with the proof left to Appendix~\ref{sec:Tsamp}. The function $s_N$ 
is referred to as a {\it sampling function}. The reasoning behind the choice of $s_N$, as well as a derivation of \eqref{gpsampcited}, is fully detailed in Appendix~\ref{sec:Tsamp} - see in particular \eqref{sfunc} and \eqref{sampseries}. One notable observation from the requirement $N\geq\mathfrak{m}$ is that, when $N=\mathfrak{m}$, the sampled values \eqref{sampledrate} are said to be sampled at the Nyquist rate \cite{benedetto2001modern}.

In terms of information, \eqref{gpsampcited} represents a remarkable trade-off, in that, to acquire the values of the signal $f$ at every point on the interval $[0,1]$, one only needs to know its values on a discrete lattice of $[0,1]$. Therefore, the gains achieved substantially outweigh the supply cost. In the following subsection, we will develop a systematic approach to implement the sampling series on the right-hand-side of \eqref{gpsampcited} (but more specifically, its regularized variants) into a WNN architecture, which then will be used to generate GNNs capable of generalizing effectively on both deterministic and random graphs.

We conclude this subsection with the following useful lemma that serves as a fundamental connection between sampling theory and Fourier theory on the torus group $\mathbb{T}$. The second conclusion of the lemma, in particular, provides a practical means for accessing the total graphon signal energy through its spatial samples. This crucial point will be a recurring element in our analysis. The proof is again left to Appendix~\ref{sec:Tsamp}. 

\begin{lemma} \label{lem:tech} 
Let $f\in L^2([0,1];\mathbb{C}^m)\cap\mathcal{B}_\mathfrak{m}$ and let $N\geq\mathfrak{m}$. Then, for $\hat{f}$ be as in \eqref{Fcoef}, we have
\begin{equation}\label{claim_finv}
    \hat{f}(k) = \begin{cases} 
                    \frac{1}{2N}\sum_{j=0}^{2N-1}f(j/2N)e^{-i2\pi kj/2N} &k\in B_\mathfrak{m}\\
                    0 &\mathrm{ otherwise}
                 \end{cases},
\end{equation}
and
\begin{align}\label{GquadforT}
    \nonumber \|f\|_{L^2([0,1];\mathbb{C}^m)}^2 = \|f\|_{L^2(\mathbb{T};\mathbb{C}^m)}^2 &= \|\hat{f}\|_{\ell^2(\mathbb{Z};\mathbb{C}^m)}^2 \\
    &= \sum_{k=-\mathfrak{m}}^{\mathfrak{m}-1} |\hat{f}(k)|^2 = \sum_{k=-N}^{N-1} |\hat{f}(k)|^2 = \sum_{j=0}^{2N-1} |f(j/2N)|^2.
\end{align}   
\end{lemma}


%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{WNN and GNN architectures enabled by the sampling theory} \label{sec:WNNframework}

With the groundwork laid in the previous subsections, we are now in a position to describe the specific network architectures that are used to achieve our main results. 

\paragraph{WNN architecture.} Let $f: [0,1]\to\mathbb{C}^m$ and suppose that for some $N\geq 1$ we have access to the vector of values
\begin{equation}\label{sampledrate}
    f_\mathrm{samp} = \begin{bmatrix} f(0) & f(1/2N) & \cdots & f((2N-1)/2N)\end{bmatrix} \in \mathbb{C}^{m \times 2N}.
\end{equation}
The objective is to design a WNN that can predict the value of $f$ at some $x\in[0,1]$ using only the knowledge of $f_\mathrm{samp}$. Our WNN architecture, using ReLU activations and a graphon filter
\begin{equation}\label{TcalK}
    T_\mathcal{K}g(x) = \int_0^1 \mathcal{K}(x,y)g(y) \,\mathrm{d}y, %\quad \forall x \in [0,1],
\end{equation}
will be comprised of two hidden layers: a filter layer and an NN layer.
Precisely, it is built according to the following steps:

\begin{enumerate}
    \item {\bf Input:} The input for our WNN is the function $g_0(x)= x$ for all $x\in [0,1]$.
    \item {\bf First hidden layer:} Given the input $g_0$, the output function $g_1$ of the first layer is given by an application of componentwise activation followed by a graphon filtering,
    \begin{equation*} 
        g_0(x)= x \overset{\rho}{\mapsto} \begin{bmatrix} ReLU(x) \\ ReLU(x-\frac{1}{2N}) \\ \vdots \\ ReLU(x - \frac{2N-1}{2N}) \end{bmatrix} \overset{T_\mathcal{K}}{\mapsto} \begin{bmatrix} T_\mathcal{K} ReLU(x) \\ T_\mathcal{K} ReLU(x-\frac{1}{2N}) \\ \vdots \\ T_\mathcal{K} ReLU(x - \frac{2N-1}{2N}) \end{bmatrix} = g_1(x), \quad \forall x\in [0,1].
    \end{equation*}
    \item{\bf Second hidden layer:} Taking $g_1$ as the input for the second layer, the output $g_2$ is given by
    \begin{equation*}
         g_1(x) \mapsto f_\mathrm{samp}\cdot g_1(x) = \sum_{j=0}^{2N-1} f(j/2N)T_\mathcal{K}ReLU(x-j/2N)= g_2(x), \quad \forall x\in [0,1]
    \end{equation*}
    where `$\cdot$' denotes matrix multiplication between the matrix $f_\mathrm{samp}$ and the vector-valued function $g_1$.
    \item{\bf Output:} The output of our WNN is the function $g_2(x)$, for each $x\in [0,1]$.
\end{enumerate}

The formula for the output WNN,
\begin{equation}\label{WNNsamplingform}
    \sum_{j=0}^{2N-1} f(j/2N)T_\mathcal{K}ReLU(x-j/2N),
\end{equation}
resembles the sampling form \eqref{gpsampcited} on $[0,1]$. As we progress further in this paper, it will become apparent that \eqref{WNNsamplingform} nearly approximates \eqref{gpsampcited} (hence in turn, $f$). This realization is a compelling testament to the fact that networks can effectively harness sampling principles. Precisely, this will be achieved by taking the graphon kernel $\mathcal{K}$ of the form 
\begin{equation}\label{Kchoice}
    \mathcal{K}(x,y) = \mathcal{G}^*(x,y)W(x,y),
\end{equation}
for a ``Gaussian-like'' function $\mathcal{G}^*$ whose specifics will be presented along with our main results. For now, we note the similarity of \eqref{Kchoice} with \eqref{GSOchoice}. The function $\mathcal{G}^*$ is used to localize the integration in \eqref{TcalK} to put more preference on ``vertices" close to each $x \in [0,1]$. We therefore assume that $W$ possesses the following ``local" regularity, for \eqref{Kchoice} to be useful in our work.  

\begin{assumption} \label{assump:regular} There exist $\kappa, \eta\in (0,1)$, $K>0$, such that for every $x\in [0,1]$ and almost every pair $(x,y)$ and $(x,z)$ in the diagonal region
\begin{equation}\label{def:diagregion}
    \mathcal{D}_\kappa := \{(x,y)\in [0,1]^2: |x-y|\leq \kappa\}
\end{equation}
we have
\begin{equation}\label{regularcond}
    W(x,y) \geq \eta \quad \text{ and }\quad |W(x,y)-W(x,z)|\leq K|y-z|.
\end{equation}
\end{assumption}

%Moving forward, we will refer to graphons that satisfy Assumption~\ref{assump:regular} as {\it regular}. 
We emphasize that we do not believe this assumption to be overly restrictive in practice. The second condition in \eqref{regularcond} is a (one-dimensional) {\it local Lipschitz} condition, while the first, the {\it nonvanishing} condition $W(x,y) \geq \eta$, simply guarantees that ``vertices" $x,y$ that are sufficiently close together have some minimal level of influence upon each other in the network. On the other hand, if the contrary holds true, i.e., $W(x,y)=0$ for almost every $y\in [-\kappa + x, x+\kappa]$, then any WNN using $W$ will fail to predict the value $f(x)$ for all graphon signals $f$ that are essentially supported in a neighborhood of $x$. For a simple demonstration of this, note that an application of $T_W$ to such an $f$ would lead to
\begin{equation*}
    \bigg|\int_0^1 W(x,y)f(y)\,\mathrm{d}y \bigg| = \text{ small }\not= |f(x)|, 
\end{equation*}
which indicates that the filtering loses information regarding $f$ at $x$.

\paragraph{GNN architecture.} 
Running parallel to the presented WNN architecture is our GNN architecture. Let $G_n$ be a graph on $n$ vertices generated from a graphon $W$, in the sense presented in \S\ref{sec:graphons}. We identify a vertex $v_k$ of $G_n$ with $x_k = \frac{k-1}{n}$, where $k=1,\cdots,n$. Let $f_n$ be an associated graph signal, identified as a function on $\mathcal{X}_n = \{x_1, \cdots, x_n\}$ via $f_n(x_k)=f_n(v_k)$. Suppose further that $f_n(x_k) = f(x_k)$ for all $k$, for some graphon signal $f$, and that, for a fixed $N \geq 1$, we have sampled 
\begin{equation*} 
    f_\mathrm{samp} = \begin{bmatrix} f(0) & f(1/2N) & \cdots & f((2N-1)/2N)\end{bmatrix} \in \mathbb{C}^{m \times 2N}.
\end{equation*}
We design a GNN that uses only the knowledge of $f_\mathrm{samp}$ to predict the value of the graph signal $f_n$ at some $x_k$. Our GNN architecture uses ReLU activations and a graph filter
\begin{equation}\label{frakF}
    \mathfrak{F}(g)(x_k) = \frac{1}{n}\sum_{l\not=k, l=1}^{n}\mathcal{K}(x_k,x_l)g(x_l),
\end{equation}
where $\mathcal{K}$ is the graphon kernel \eqref{Kchoice} above. Notably, \eqref{frakF} is a discretization of \eqref{TcalK} to the ``vertices" $\{x_k=(k - 1)/n\}_{k = 1}^n$. The architecture is comprised of two hidden layers: a filter layer and an NN layer, described as follows.

\begin{enumerate}
    \item {\bf Input:} The input for our GNN is the function $g_0(x_k)= x_k = \frac{k-1}{n}$ for each $k = 1,\dots, n$.
    \item {\bf First hidden layer:} Given the input $g_0$, the output function $g_1$ of the first layer is given by an application of component-wise activation followed by a graph filtering:
    \begin{equation*} 
        g_0(x_k) = x_k \overset{\rho}{\mapsto} \begin{bmatrix} ReLU(x_k) \\ ReLU(x_k-\frac{1}{2N}) \\ \vdots \\ ReLU(x_k - \frac{2N-1}{2N}) \end{bmatrix} \overset{\mathfrak{F}}{\mapsto} \begin{bmatrix} \mathfrak{F}(ReLU)(x_k) \\ \mathfrak{F}(ReLU)(x_k-\frac{1}{2N}) \\ \vdots \\ \mathfrak{F}(ReLU)(x_k - \frac{2N-1}{2N}) \end{bmatrix} \equiv g_1(x_k),
    \end{equation*}
    for all $k = 1,\dots, n$.
    \item{\bf Second hidden layer:} Taking $g_1$ as the input for the second layer, the output $g_2$ is given by
    \begin{equation*} 
         g_1(x_k) \mapsto f_\mathrm{samp}\cdot g_1(x_k) = \sum_{j=0}^{2N-1} f(j/2N)\mathfrak{F}(ReLU)(x_k-j/2N)\equiv g_2(x_k),
    \end{equation*}
    for all $k = 1,\dots, n$.
    \item{\bf Output:} The output of our GNN is the function $g_2(x_k)$ for each $k = 1,\dots, n$.
\end{enumerate}

We make three observations. Firstly, as our GNN only requires two hidden layers, the parameter of the network consists of the nonunital linear weights that are the $2N$ entries of $f_\mathrm{samp}$. Secondly, the formula for the GNN output %of the GNN we provide above, given by
\begin{equation}\label{GNNoutput}
    \sum_{j=0}^{2N-1} f(j/2N)\mathfrak{F}(ReLU)(x_k-j/2N),
\end{equation}
like the one in \eqref{WNNsamplingform}, carries the form of a sampling principle. Thirdly, it's evident from \eqref{frakF} that our choice for graph filter kernel assumes the form
\begin{equation*} 
    \mathcal{K}(x_k,x_l) =  \mathcal{G}^*(x_k,x_l)W(x_k,x_l), \quad \forall k\not= l.
\end{equation*}
We again note the similarity with \eqref{GSOchoice}. 

\begin{remark} \label{rem:inputfeat} {\it ($d$-dimensional input features versus one-dimensional input features)}\\
Our GNN construction exclusively operates on the input features $x_k \in [0,1]$ assigned to the vertices $v_k$ during the embedding of $G_n$ into the graphon space. While the graph vertices may have initially been associated with Euclidean feature vectors in $\mathbb{R}^d$, we disregard them and instead adopt the vertex locations on the interval $[0,1]$ as the input for our GNN. This distinguishes our approach from the conventional GNN framework, which typically relies on vectors in $\mathbb{R}^d$. By simplifying the input to single scalar values, we enable the construction of our GNN with two hidden layers and $2N$ nonunital linear weights corresponding to the entries of $f_\mathrm{samp}$. The network output value $g_2(x_k)$ represents an estimate of the target function $f_n$ at the vertex $v_k$.
\end{remark}

\begin{remark} \label{rem:sampling} {\it (embedded sampling principle)} As demonstrated, we explicitly employ the order \eqref{GNNlayer} for both of our networks. This deliberate choice serves a purpose that is to allow a seamless integration of sampling theory into the network architectures. The emerging sampling principle will provide all the required network parameters for achieving effective generalization, thereby removing the need for a learning processes. A demonstration of this link in a similar context can be found in \cite{neuman2022superiority}.
\end{remark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Main Results}\label{sec:Results}

In this section we present the main contributions of this paper. We present four theorems. First, in \S\ref{sec:SampResult} we provide a uniform sampling theorem on $[0,1]$. Then, in \S\ref{sec:WNNResult} we provide a theorem that establishes WNNs' capacity to generalize graphon signals from evenly spaced samples. We leverage these two theoretical results to derive transferability results for GNNs in \S\ref{sec:GNNthms}, applying to both deterministic and random graph sequences generated from a single graphon. We conclude this section with a discussion of the ramifications of our results to practical applications in \S\ref{sec:Ramifications}. All proofs are left to the sections that follow this one.

As an aside, through the remainder of this paper we adhere to the analysis practice where universal constants denoted by $C$ may vary in value from one instance to another. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A uniform sampling theorem}\label{sec:SampResult}

To begin, let $r \in (0,1/2)$ and define the 1-periodic function $\mathcal{G}_{r,\sigma}$ on $\mathbb{R}$ to be
\begin{equation}\label{def:Grsigma}
    \mathcal{G}_{r,\sigma}(x) := \begin{cases}
                            \mathcal{G}_{\sigma}(x) := c(\sigma)\sum_{n\in\mathbb{Z}} e^{-n^2\sigma^{-2}/2}e^{i2\pi n x}, &  -r \leq x\leq r\\
                            0, &\mathrm{otherwise}
                          \end{cases}
\end{equation}
where $r$ is a truncation parameter, $\sigma$ a variance parameter, and $c(\sigma)>0$ is a normalization constant such that $\mathcal{G}_{r,\sigma}(0)=\mathcal{G}_{\sigma}(0)=1$. Due to its periodicity, $\mathcal{G}_{r,\sigma}$ can equivalently be viewed as a function on $\mathbb{T}$. Now supposing $N>\mathfrak{m}$, for $f\in L^2([0,1];\mathbb{C}^{m})\cap \mathcal{B}_\mathfrak{m}$ and $\mathcal{G}_{r,\sigma}$, we consider the reconstruction scheme of $f$ from its uniform samples $f_\mathrm{samp} = \begin{bmatrix} f(0) & f(1/2N) & \cdots & f((2N-1)/2N)\end{bmatrix}$ given by
\begin{equation}\label{reconstruction}
    \mathcal{R}_{r,\sigma}f(x) := \sum_{j=0}^{2N-1} f(j/2N)s_N(y-j/2N)\mathcal{G}_{r,\sigma}(x-j/2N),
\end{equation}
where $s_N$ is as in \eqref{def:sampf}. The following theorem is our first result of this section, an interesting sampling theorem in its own right. It shows that, with the right choice of $r,\sigma$ and $N$, the reconstruction scheme in \eqref{reconstruction} produces a subgeometric convergence rate in $L^2$ norm. It will enable our results on WNNs and GNNs that follow. The proof is left to \S\ref{sec:Tsamplingthm}.

\begin{theorem} \label{thm:TsamplingregwG}
Let $f\in L^2([0,1];\mathbb{C}^{m})\cap\mathcal{B}_\mathfrak{m}$ and $\mathcal{R}_{r,\sigma}f$ be as in \eqref{def:truncregsampseries}. Let $0<\alpha<\beta<1$, $N>\mathfrak{m}$, and set
\begin{equation}\label{rsigmapair}
    \sigma = \frac{(N-\mathfrak{m})^{\beta}}{\sqrt{6}\pi} \quad\text{ and }\quad r = \frac{3\pi}{(N-\mathfrak{m})^{\alpha}}.
\end{equation}
Then there exists $C>0$ such that the followings hold:
\begin{equation}\label{TsamplingregwG}
    \|f-\mathcal{R}_{r,\sigma}f\|_{L^2([0,1];\mathbb{C}^m)} \leq C\widetilde{\mathcal{E}}(N,\mathfrak{m},\alpha,\beta)\|f\|_{L^2([0,1];\mathbb{C}^m)}
    %\|f-\mathcal{R}_{r,\sigma}f\|_{L^\infty([0,1];\mathbb{C}^m)} \leq C\sqrt{\mathfrak{m}}\,\widetilde{\mathcal{E}}(N,\mathfrak{m},\alpha,\beta)\|f\|_{L^2([0,1];\mathbb{C}^m)},
\end{equation}
for sufficiently large $N$, where
\begin{equation}\label{def:tildE}
    \widetilde{\mathcal{E}}(N,\mathfrak{m},\alpha,\beta) := e^{-3\pi^2(N-\mathfrak{m})^{2(1-\beta)}}\max\bigg\{1,(N-\mathfrak{m})^{2\beta-1}\bigg\} + (N-\mathfrak{m})^{\alpha+\beta} e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}}.
\end{equation}
\end{theorem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization using WNNs}\label{sec:WNNResult} 

The sampling result in Theorem~\ref{thm:TsamplingregwG} will play a crucial role in enabling us to define the kernel \eqref{Kchoice} used in constructing the filter layer of our WNN architecture in \S\ref{sec:WNNframework}. We begin by considering a graphon $W:[0,1]^2 \to [0,1]$, the function $\mathcal{G}_{r,\sigma}$ as given in \eqref{def:Grsigma}, and, for some $N \geq 1$, $s_N$ as given in \eqref{def:sampf}. Then, suppressing the dependence on $(r,\sigma,N)$ for the ease of presentation, we define an (asymmetric) kernel $\mathcal{K}$ (see \eqref{TcalK}) by
\begin{equation}\label{WNNkernel}
    \mathcal{K}(x,y) = \bigg(\frac{\,\mathrm{d}^2}{\,\mathrm{d} y^2}\, (s_N\,\mathcal{G}_{r,\sigma})(x-y)\bigg)\bigg(\frac{W(x,y)}{\mathcal{W}_x}\bigg),
\end{equation}
where, for each $x\in [0,1]$ we set 
\begin{equation}\label{Wxbar}
    \mathcal{W}_x := \frac{1}{|\mathfrak{I}_x|}\int_{\mathfrak{I}_x} W(x,y)\,\mathrm{d}y \quad\text{ and }\quad \mathfrak{I}_x:= [0,1]\cap [-r+x, x+r],
\end{equation}
and for every $j\in\mathbb{N}$,
\begin{equation}\label{def:Gtruncder}
    \frac{\,\mathrm{d}^j}{\,\mathrm{d} x^j}\mathcal{G}_{r,\sigma}(x) = \mathcal{G}^{(j)}_{r,\sigma}(x) := \begin{cases}
                                    \mathcal{G}^{(j)}_{\sigma}(x) & -r\leq x\leq r\\
                                    0 &\mathrm{otherwise}
                                            \end{cases}.
\end{equation}
We now state our result on the generalization capabilities of WNNs for Fourier bandlimited graphon signals. The proof can be found in \S\ref{sec:WNNthm}. 

\begin{theorem} \label{thm:WNN}
Let $f\in L^2([0,1];\mathbb{C}^{m})\cap\mathcal{B}_\mathfrak{m}$ and $W$ a graphon satisfying Assumption~\ref{assump:regular}. Let $\alpha=0.96$, $\beta=0.98$, and $\varepsilon\in (0,1)$. Then there exist $C>0$ and a ReLU WNN $\Psi_f$, %instantiated by $f$, 
using the graphon kernel $\mathcal{K}$ in \eqref{WNNkernel}, with $r,\sigma$ specified by
\begin{equation*} 
    r = \frac{3\pi}{(N-\mathfrak{m})^{\alpha}} \quad\text{ and }\quad \sigma = \frac{(N-\mathfrak{m})^{\beta}}{\sqrt{6}\pi},
\end{equation*}
where $N=\lceil \varepsilon^{-10/9}\rceil\gg\mathfrak{m}$, such that
\begin{equation}\label{WNNthmconc}
    \|\Psi_f-f\|_{L^2([r,1-r];\mathbb{C}^m)} \leq C\eta^{-1}K\,\varepsilon\,\|f\|_{L^2([0,1];\mathbb{C}^m)}.
\end{equation}

More precisely, $\Psi_f$ assumes the structure described in \S\ref{sec:WNNframework}, with two layers, including one filter layer, and $2N=2\lceil \varepsilon^{-10/9}\rceil$ linear weights that are supplied by the sampled functional values of $f$.
\end{theorem}

\begin{remark} \label{rem:alphabeta}
The specific choices of $\alpha,\beta$ are made for the sake of concreteness. In fact, it is possible to choose $\alpha$ in such a way that we obtain $N=\lceil \varepsilon^{-\delta}\rceil$, where $\delta$ can be as close to $1$ as possible. The choices we make are motivated by the following considerations:
\begin{equation*}
    1-\beta = \beta-\alpha\leq 1/4 \quad\text{ and }\quad \alpha>3/5.
\end{equation*}
The last condition will become evident during the proof of the theorem (see \eqref{sys}), while the first condition arises from our effort to equate the two geometric rates in \eqref{def:tildE}.
\end{remark}

The foundation of Theorem~\ref{thm:WNN} relies on the result established in Theorem~\ref{thm:TsamplingregwG}. However, the two differ in the scope of their predictions. While our sampling result guarantees predictions on the entire interval $[0,1]$, Theorem~\ref{thm:WNN} provides guarantees for the specific {\it predictable} zone $[r,1-r]$, which, since $r=C(N-\mathfrak{m})^{-\alpha}$ and $N=\lceil\varepsilon^{-10/9}\rceil$, widens to $[0,1]$ as $\varepsilon\to 0^+$. This distinction arises from a crucial implementation of an integration-by-parts result in Theorem~\ref{thm:WNN} (see Proposition~\ref{prop:WNNthmcont}), which secures its conclusion but in the process, inherently restricts the prediction range. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization on graphs and GNN-transferability}\label{sec:GNNthms}

Now, consider a graphon $W:[0,1]^2 \to [0,1]$ and let $\{G_n\}_{n = 1}^\infty$ be a sequence of simple graphs on $n$ vertices, generated from $W$ as in \S\ref{sec:graphons}. Further, we denote ${\bf A}_n \in \mathbb{R}^{n \times n}$ as the adjacency matrix associated to each $G_n$. Taking $\mathcal{G}_{r,\sigma}$ to be as in \eqref{def:Grsigma} and $s_N$ as in \eqref{def:sampf} for some fixed $N \geq 1$, for each $n \geq 1$ we define the graph filter kernel (see \eqref{frakF})
\begin{equation}\label{graphker}
    \mathcal{K}_n(x_k,x_l) = \frac{1}{n}\bigg(\frac{\,\mathrm{d}^2}{\,\mathrm{d} x^2}\, (s_N\,\mathcal{G}_{r,\sigma})(x_k-x_l)\bigg)\bigg(\frac{[{\bf A}_n]_{k,l}}{\mathcal{W}_{x_k}}\bigg).
\end{equation}
Here $\mathcal{W}_{x_k}$ is defined from the graphon $W$ as in \eqref{Wxbar} for $x=x_k$. 

The sequence $\{G_n\}_{n=1}^\infty$ can be comprised of either deterministic or random graphs. When we need to differentiate between the two cases, we adopt specific notations. When $G_n$ is deterministic, we denote it as $G^{\mathrm{det}}_n$, and in this scenario, $[{\bf A}_n]_{k,l}=W(x_k,x_l)$, for $k\not=l$. When $G_n$ is random, we represent it as $G^{\mathrm{ran}}_n$. In this case, ${\bf A}_n$ becomes a random matrix, and each $\mathcal{K}_n(x_k,x_l)$ is a random variable for $k \neq l$. A realization of $G^{\mathrm{ran}}_n$ yields a graph with a binary adjacency matrix ${\bf A}_n$ whose entries $[{\bf A}_n]_{k,l}\in\{0,1\}$. In short, regardless of whether $G_n$ is deterministic or random or realized, we only need to substitute the corresponding adjacency matrix into \eqref{graphker}.

We now state our results on the generalization capabilities of GNNs for graphs belonging to the same graphon family. Their proofs can be found in \S\ref{sec:GNNdet}, \S\ref{sec:GNNran}, with respect to the order in which the theorems are presented.

\begin{theorem} (Deterministic GNNs) \label{thm:GNNdet}
Let $f\in L^2([0,1];\mathbb{C}^m)\cap\mathcal{B}_\mathfrak{m}$ and $W$ a graphon satisfying Assumption~\ref{assump:regular}. Let $\varepsilon\in (0,1)$, $n\geq 1$, $G^{\mathrm{det}}_n$ be a deterministic graph generated from $W$, and $f_n$ an associated graph signal such that $f_n(x_k)=f(x_k)$ for all $x_k$. Then there exist $C>0$ and a ReLU GNN $\Psi_{n,f}$, using the graph kernel $\mathcal{K}_n$ in \eqref{graphker} with $r,\sigma$ specified by
\begin{equation*} 
    r = \frac{3\pi}{(N-\mathfrak{m})^\alpha} \quad\text{ and }\quad \sigma = \frac{(N-\mathfrak{m})^\beta}{\sqrt{6}\pi},
\end{equation*}
$\alpha=0.96$, $\beta=0.98$, $N=\lceil \varepsilon^{-10/9}\rceil\gg\mathfrak{m}$, such that for all $n\geq\varepsilon^{-13/3}$, it holds that
\begin{equation}\label{GNNdetconc}
    \|\overline{\Psi}_{n,f} - f\|_{L^2([r,1-r];\mathbb{C}^m)}\leq C\eta^{-2}(1+K)\,\varepsilon\,\|f\|_{L^2([0,1];\mathbb{C}^m)},
\end{equation}
where
\begin{equation}\label{def:barPsinf}
    \overline{\Psi}_{n,f}(x):=\sum_{k=1}^n \Psi_{n,f}(x_k)\chi_{I_k}(x), \quad\forall x\in [0,1]. 
\end{equation}

More precisely, $\Psi_{n,f}$ assumes the structure described in \S\ref{sec:WNNframework}, with two layers, including one filter layer, and $2N=2\lceil \varepsilon^{-10/9}\rceil$ linear weights that are supplied by the sampled functional values of the graphon signal $f$.
\end{theorem} % end of deterministic GNN theorem

%\textcolor{blue}{A noteworthy aspect of the $L^2$ convergence in \eqref{GNNdetconc} is its applicability to the extension of the GNN output $\Psi_{n,f}$ as a graphon signal $\overline{\Psi}_{n,f}$ on $[0,1]$. This allows one to infer the values of $f$ on the predictable zones $[r,1-r]$, and consequently, the values of $f_n$ for large $n$, as $f_n = f$ at all $x_k$.} %Further, by Egoroff's theorem \cite[Theorem 2.33]{folland1999real}, it ensures that the construction/prediction $\Psi$ is also closely aligned with the value of $f$ on the predictable zone.

\begin{theorem} (Random GNNs) \label{thm:GNNran}
Let $f\in L^2([0,1];\mathbb{C}^m)\cap\mathcal{B}_\mathfrak{m}$ and $W$ a graphon satisfying Assumption~\ref{assump:regular}. Let $\varepsilon\in (0,1)$, $n\geq 1$, $G^{\mathrm{ran}}_n$ be a simple random graph generated from $W$, and $f_n$ an associated graph signal such that $f_n(x_k)=f(x_k)$ for all $x_k$. Then there exist $C>0$ and a ReLU GNN $\Psi_{n,f}$, using the graph kernel $\mathcal{K}_n$ in \eqref{graphker} with $r,\sigma$ specified by
\begin{equation*} 
    r = \frac{3\pi}{(N-\mathfrak{m})^\alpha} \quad\text{ and }\quad \sigma = \frac{(N-\mathfrak{m})^\beta}{\sqrt{6}\pi},
\end{equation*}
$\alpha=0.96$, $\beta=0.98$, $N=\lceil \varepsilon^{-10/9}\rceil\gg\mathfrak{m}$, such that for all $n\geq\varepsilon^{-13/3}$, it holds that
\begin{equation}\label{GNNranconc}
    \|\overline{\Psi}_{n,f} - f\|_{L^2([r,1-r];\mathbb{C}^m)} \leq C\eta^{-2}(1+K)\,\varepsilon\,\|f\|_{L^2([0,1];\mathbb{C}^m)},
\end{equation}
where $\overline{\Psi}_{n,f}$ is defined in \eqref{def:barPsinf}, with a probability at least
\begin{equation}\label{probability}
    1 - 2n\varepsilon^{10(1-\alpha)/9}\exp\bigg(-\frac{C\eta^2 n}{\varepsilon^{4(15\beta-5\alpha-2)/9}}\bigg).
\end{equation}

More precisely, $\Psi_{n,f}$ assumes the structure described in \S\ref{sec:WNNframework}, with two layers, including one filter layer, and $2N=2\lceil \varepsilon^{-10/9}\rceil$ linear weights that are supplied by the sampled functional values of the graphon signal $f$.
\end{theorem} % end of random GNN theorem

It should be apparent in \eqref{probability} that as the number of graph vertices $n$ increases, the likelihood of \eqref{GNNranconc} holding also increases. We make a brief remark that the lower bound of $\varepsilon^{-13/3}$ for $n$ can be improved to be as near $\varepsilon^{-4}$ as possible (see also Remark~\ref{rem:optimal_n}). The lower bound we obtain for $n$ in both theorems is a direct consequence of the choices we make for $\alpha, \beta$.

\begin{remark}
We would like to highlight an alternative generation of simple random graphs using a specific graphon $W$, as follows. Given $n \geq 1$ vertices, labeled as $v_1, \cdots, v_n$, one can independently sample them as random variables $\xi_1, \cdots, \xi_n$ from the uniform distribution on the interval $[0,1]$. The vertices $v_k$ and $v_l$, $k>l$, are connected by an edge with an edge-weight of $W(\xi_k,\xi_l)$. This results in a distinct type of random graph, different from the primary ones considered in this paper. Nevertheless, a variant of Theorem~\ref{thm:GNNran} can be adapted to analyze these types of random graphs and provide similar estimations. To derive this version, we anticipate utilizing the Bernstein inequality \cite{boucheron2003concentration}. For a practical demonstration of how this inequality is applied in this context, interested readers are referred to \cite[Lemma~1]{calder2018game}. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ramifications of the results}\label{sec:Ramifications}

Having now presented our main results, we now highlight their most important ramifications.

\begin{enumerate}
\item The two major contributions of this work are the GNN results presented in Theorems~\ref{thm:GNNdet} and \ref{thm:GNNran}. What they provide is the production of a shallow GNN structure that uses a small number of generated samples from the graphon signal and gives a reliable prediction on any large graph within the graphon family. As the GNNs are constructed with predetermined network parameters, our approach does not involve any learning process.

\item Our approach yields a dimensionless result for the network structure, which is the consequence of our taking in of one-dimensional input features. This lack of dimensional-dependence, as explained in Remark~\ref{rem:inputfeat}, is made possible by the graphon presence. The existence of a macroscopic graphon structure is crucial since it enables us to leverage the transition from a multi-dimensional generalization process to a one-dimensional one, hence circumventing the curse of dimensionality. As a simple demonstration of this efficacy, note that the generalization depicted in Theorem~\ref{thm:GNNdet} provides accurate predictions within, after some appropriate scaling by universal and intrinsic constants, an error of approximately $0.1$, for at least $19700$ vertices in a graph containing at least $21500$ vertices, while using a low-cost network with only $26$ linear weights, therefore resulting in a cost ratio of approximately $0.0006$.

\item Additionally, our approach offers a seamless transferability of the network architecture across graphs of different sizes. This transferability, which is of paramount importance, involves a straightforward substititon of the relevant adjacency matrix into the kernel \eqref{graphker} while ensuring comparable signal generalization estimates. The subsequent corollary, which we give without proof, serves as a validation of this notable outcome.

\begin{corollary} (GNN transferability) \label{cor:transfer}
Let $f\in L^2([0,1];\mathbb{C}^m) \cap \mathcal{B}_m$ and $W$ be a graphon satisfying Assumption~\ref{assump:regular}. Let $\varepsilon\in (0,1)$ and let $G_{n_1}$, $G_{n_2}$ be two graphs generated from $W$ (random or deterministic) where $n_1\leq n_2$. Denote $\Psi_{n_1,f}$ as the ReLU GNN associated with $G_{n_1}$, assuming the structure described in \S\ref{sec:GNNthms}, and $\Psi_{n_2,f}$ the similar ReLU GNN associated with $G_{n_2}$. Suppose that \eqref{GNNdetconc} holds for $\Psi_{n_1,f}$, in the case that $G_{n_1}$ is deterministic, or with a probability at least 
\begin{equation*}
    1 - 2n_1\varepsilon^{10(1-\alpha)/9}\exp\bigg(-\frac{C\eta^2 n_1}{\varepsilon^{4(15\beta-5\alpha-2)/9}}\bigg).
\end{equation*}
in the case that $G_{n_1}$ is random. Then \eqref{GNNdetconc} also holds for $\Psi_{n_2,f}$, in the case that $G_{n_2}$ is deterministic, or with a probability at least 
\begin{equation*}
    1 - 2n_2\varepsilon^{10(1-\alpha)/9}\exp\bigg(-\frac{C\eta^2 n_2}{\varepsilon^{4(15\beta-5\alpha-2)/9}}\bigg).
\end{equation*}
in the case that $G_{n_2}$ is random.
\end{corollary}

As an expository application, take the recommender systems that were used as motivation in the introduction and provided the first application of WNNs in \cite{ruiz2020graphon}. An $m$-dimensional graph signal encodes the ratings given by an individual user, represented as a vertex, for each of the $m$ products that the service recommends. Our approach permits a GNN to be constructed from the ratings obtained from $2N$ users, which can then predict the ratings that $n \gg 2N$ users would assign to each product based on the sampled information. This avoids having to retrain the GNN structure as users are added or subtracted from the service, allowing for a more efficient and scalable recommender system that can adapt to changes in the user base without the burden of frequent model updates. Furthermore, our approach extends the methodology of \cite{ruiz2020graphon}, which constructs deterministic graphs based on pairwise correlations of user rankings, by interpreting these correlations as the likelihood of two users having overlapping tastes, therefore enabling the construction of random graphs. Our GNNs then provide the same level of prediction accuracy with high probability on these random graphs. This probabilistic framework enhances the robustness and reliability of the predictions generated by the GNN-based recommender system.

\item Our results to some degree strengthen the findings presented in \cite{neuman2022superiority}, where a similar strategy of incorporating the sampling principle into a GNN structure was employed. A limitation of said work is the unavoidable growth in the number of layers with respect to the input feature dimension, attributed to its use of one-dimensional numerical integration methods. In contrast, as our approach utilizes input features in $[0,1]$, it eliminates the dependency on the input dimension and results in a streamlined GNN consisting of only two layers. In the graphon context, the grid lattice employed in \cite{neuman2022superiority} falls within the family of the graphons that are almost everywhere constant in a neighborhood of the diagonal. Consequently, with our approach, as detailed in Remark~\ref{rem:flat}, the number of weights would only accumulate to $N=C\lceil (\log(1/\varepsilon))^{1/(2(\beta-\alpha))}\rceil$. This further serves as a testament to the potential and the unique advantage that graphons offer in GNN applications.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem~\ref{thm:TsamplingregwG}} \label{sec:Tsamplingthm}

In this section, we make the necessary steps toward establishing the validity of Theorem~\ref{thm:TsamplingregwG} in \S\ref{sec:truncregwG} below. The main idea for the proof centers around the use of a truncated and regularized variant of Kluv\'anek's sampling formulation \cite{kluvanek1965sampling}. More precisely, let $\mathfrak{m}, N\in\mathbb{N}$ and suppose $N\geq\mathfrak{m}$. For any $f\in \mathcal{B}_\mathfrak{m}$, by Proposition~\ref{prop:gpsampling} we have
\begin{equation}\label{sampseriesrecall}
    f(x) = \sum_{j=0}^{2N-1} f(j/2N)s_N(x-j/2N), \quad \forall x\in [0,1],
\end{equation}
where $s_N$ is as in \eqref{def:sampf}. Our goal is to replace the series on the right-hand-side of \eqref{sampseriesrecall} with 
\begin{equation*}
    \sum_{j=0}^{2N-1} f(j/2N)\mathcal{G}(x-j/2N)
\end{equation*}
where $\mathcal{G}$ is some function exhibiting a ``Gaussian-like'' behavior, but only supported in a vicinity of $x$ of some appropriate radius $r$. To begin, we need to establish a general result for regularized sampling. In this section as well as others that follow, we will make repeated use of the McLaurin-Cauchy integral test \cite[\S 3.3, Theorem 3]{knopp1956infinite} and the Mills' ratio formula \cite{boyd1959inequalities}, which we cite here for the reader's convenience:
\begin{equation}\label{Mill}
    %\frac{\pi e^{-x^2/2}}{\sqrt{x^2+2\pi} + (\pi-1)x} \leq 
    \int_x^\infty e^{-t^2/2}\,\mathrm{d}t \leq \frac{\pi e^{-x^2/2}}{\sqrt{(\pi-2)^2x^2+2\pi} + 2x},
\end{equation}
for every $x>0$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A regularized sampling result}

Let $\phi:[0,1] \to\mathbb{C}$ be a continuous function such that $\phi(0)=\phi(1) = 1$ and its Fourier coefficients are absolutely summable, i.e. $\hat{\phi}\in \ell^1(\mathbb{Z})$. Then $\hat{\phi}\in \ell^2(\mathbb{Z})$ as well, and from the Fourier inversion theorem \cite{folland1999real} we have $\phi\in L^2([0,1])$ with
\begin{equation}\label{phi}
    \sum_{l\in\mathbb{Z}} \hat{\phi}(l) = \phi(0) = 1.
\end{equation}
For any $N\in\mathbb{N}$, let $\chi_{B_N}$ denote a function that is one on $B_N = \{-N,\dots,N-1\} \subset \mathbb{Z}$ and zero on $\mathbb{Z}\setminus B_N$. For each $k\in\mathbb{Z}$ we define
\begin{equation}\label{def:mu}
    \mu(k) : =\chi_{B_N}\ast\hat{\phi}(k) = \sum_{l\in\mathbb{Z}} \chi_{B_N}(k-l)\hat{\phi}(l) = \sum_{l\in k-B_N}\hat{\phi}(l),
\end{equation}
and
\begin{equation}\label{def:nu}
    \nu(k) := \sum_{l\not\in k-B_N}\hat{\phi}(l).
\end{equation}
We gather from \eqref{phi}, \eqref{def:mu}, \eqref{def:nu} that
\begin{equation}\label{nu&mu1}
   \sum_{l\in\mathbb{Z}} \mu(k+l2N) = \sum_{l\in\mathbb{Z}} \hat{\phi}(l)= 1 = \nu(k) + \mu(k), \quad\forall k\in\mathbb{Z},
\end{equation}
and so we deduce
\begin{equation}\label{nu&mu2}
    \nu(k) = \sum_{l\in\mathbb{Z}\setminus\{0\}} \mu(k+l2N), \quad\forall k\in\mathbb{Z}.
\end{equation}

Now, let $f\in\mathcal{B}_\mathfrak{m}$ and take $N\geq\mathfrak{m}$. We define
\begin{equation}\label{def:reg}
    \mathcal{R}_\phi f(x) := \sum_{j=0}^{2N-1} f(j/2N)s_N(x-j/2N)\phi(x-j/2N), \quad \forall x\in [0,1],
\end{equation}
where, from the definition \eqref{def:sampf} of $s_N$, we can write 
\begin{equation*} 
    \hat{s}(k) =\frac{1}{2N}\chi_{B_N}(k), \quad\forall k\in\mathbb{Z}.
\end{equation*}
Since all the functions involved in \eqref{def:reg} are continuous on $\mathbb{T}\cong [0,1)$, we treat the argument addition in \eqref{def:reg} as a group addition on $\mathbb{T}$, i.e. it is taken modulo 1. (Alternatively, we can consider treating them as $1$-periodic functions on $\mathbb{R}$. This perspective will be used in \S\ref{sec:WNNthm}.) Reasonably, we can no longer expect $f=\mathcal{R}_\phi f$. However, we claim that the regularized sampling error, i.e. $f-\mathcal{R}_\phi f$, can be understood in terms of $\mu, \nu$ in \eqref{def:mu} and \eqref{def:nu}, respectively.

\begin{lemma} \label{lem:regerr}
Let $f\in L^2([0,1];\mathbb{C}^m)\cap\mathcal{B}_\mathfrak{m}$ and $N\geq\mathfrak{m}$. Then
\begin{equation*} 
    \hat{f}(k)-\widehat{\mathcal{R}_\phi f}(k) = \begin{cases}
                                \hat{f}(k)\nu(k), & k\in B_\mathfrak{m}\\
                                -\hat{f}(k-2lN)\mu(k), & \exists l\neq 0\ \mathrm{s.t}\ k-2lN\in B_\mathfrak{m}\\
                                0, &\mathrm{ otherwise}
                             \end{cases}.
\end{equation*}
\end{lemma}

\begin{proof}
Recall from Lemma~\ref{lem:tech} that
\begin{equation}\label{2FTrecall}
    \hat{f}(k) = \begin{cases} 
                    \frac{1}{2N}\sum_{j=0}^{2N-1}f(j/2N)e^{-i2\pi kj/2N} &k\in B_\mathfrak{m}\\
                    0 &\mathrm{ otherwise}
                 \end{cases}.
\end{equation}
Let $k\in B_N$. By combining \eqref{2FTrecall} with \eqref{def:mu}, \eqref{def:reg}, and the definition of $s_N$, we further obtain
\begin{equation*}
    \widehat{\mathcal{R}_\phi f}(k) = \frac{1}{2N}\sum_{j=0}^{2N-1} f(j/N)e^{-i2\pi kj/2N}\mu(k) = \hat{f}(k)\mu(k).
\end{equation*}
Therefore, from \eqref{nu&mu1}
\begin{equation}\label{regerrlem1}
    \hat{f}(k) - \widehat{\mathcal{R}_\phi f}(k)= \hat{f}(k)\nu(k),
\end{equation}
which from \eqref{2FTrecall}, in turn implies $\hat{f}(k) - \widehat{\mathcal{R}_\phi f}(k)= 0$, if $k\in B_N\setminus B_\mathfrak{m}$. 

Now suppose $k\not\in B_N$ but $k-2lN\in B_N$ for some $l\neq 0$. Then
\begin{equation}\label{regerrlem2}
    \begin{split}
        \hat{f}(k) - \widehat{\mathcal{R}_\phi f}(k) &= - \widehat{\mathcal{R}_\phi f}(k) \\
        &= -\frac{1}{2N}\sum_{j=0}^{2N-1} f(j/N)e^{-i2\pi (k-2lN)j/2N}\mu(k) = -\hat{f}(k-2lN)\mu(k),   
    \end{split}
\end{equation}
where we have again used \eqref{2FTrecall}. In this case, if $k-l2N\in B_N\setminus B_\mathfrak{m}$ then it follows from \eqref{2FTrecall} and \eqref{regerrlem2} that
\begin{equation}\label{regerrlem3}
    \hat{f}(k)-\widehat{\mathcal{R}_\phi f}(k) = 0.
\end{equation}
Combining \eqref{regerrlem1}, \eqref{regerrlem2}, \eqref{regerrlem3}, we conclude the lemma. 
\end{proof} % end of proof of lemma

We now provide the bounds on the difference between $f$ and $\mathcal{R}_\phi f$ in both the $L^2$ and $L^\infty$ norms, which will eventually be employed to prove Theorem~\ref{thm:TsamplingregwG} in the following subsection \S\ref{sec:truncregwG}.

\begin{lemma} \label{lem:regsamp} 
Let $f\in L^2([0,1];\mathbb{C}^m)\cap\mathcal{B}_\mathfrak{m}$. Then 
\begin{equation}\label{l2replace}
    \overline{\varepsilon}_1\|f\|_{L^2([0,1];\mathbb{C}^m)}\leq \|f-\mathcal{R}_\phi f\|_{L^2([0,1];\mathbb{C}^m)} \leq\,\overline{\varepsilon}_2\|f\|_{L^2([0,1];\mathbb{C}^m)},
\end{equation}
where
\begin{equation*}
    \begin{split}
        \overline{\varepsilon}_1 &:= \min_{k\in B_\mathfrak{m}}\bigg\{|\nu(k)|^2 +  \sum_{l\in\mathbb{Z}\setminus\{0\}} |\mu(k+l2N)|^2 \bigg\}^{1/2}\\
        \overline{\varepsilon}_2 &:= \max_{k\in B_\mathfrak{m}}\bigg\{|\nu(k)|^2 +  \sum_{l\in\mathbb{Z}\setminus\{0\}} |\mu(k+l2N)|^2 \bigg\}^{1/2}.
    \end{split}
\end{equation*}
\end{lemma}

\begin{proof}
By applying Lemma~\ref{lem:regerr} and utilizing Plancherel's theorem, we obtain
\begin{equation}\label{L2bdequal}
    \|f-\mathcal{R}_\phi f\|_{L^2([0,1];\mathbb{C}^m)}^2 = \sum_{k\in B_\mathfrak{m}} |\hat{f}(k)\nu(k)|^2 + \sum_{l\in\mathbb{Z}\setminus\{0\}} \sum_{k\in B_\mathfrak{m}} |\hat{f}(k)\mu(k+2l\pi)|^2,
\end{equation}
from which \eqref{l2replace} automatically follows. We note that the $L^2$-bounds cannot be improved, due to the equality \eqref{L2bdequal}. 
\end{proof} % end of proof

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sampling with $\mathcal{G}_{r,\sigma}$} \label{sec:truncregwG}

In this subsection we will apply Lemma~\ref{lem:regsamp} to specific regularizers $\phi$, resulting in the proof of Theorem~\ref{thm:TsamplingregwG}.

Let $\sigma>0$ and recall the regularizer
\begin{equation}\label{def:G}
    \mathcal{G}_{\sigma}(x) = c(\sigma)\sum_{k\in\mathbb{Z}} e^{-k^2\sigma^{-2}/2}e^{i2\pi kx} = 2c(\sigma)\sum_{k = 0}^\infty e^{-k^2\sigma^{-2}/2}\cos (2\pi kx), \quad\forall x\in [0,1].
\end{equation}
presented previously in \eqref{def:Grsigma}. Since $\mathcal{G}_\sigma(0)=\mathcal{G}_\sigma(1)$, we consider $\mathcal{G}_\sigma$ as a function on $\mathbb{T}\cong [0,1)\cong [-1/2,1/2)$. The parameter $\sigma$ is a variance parameter, and $c(\sigma)$ is a normalization constant guaranteeing that
\begin{equation*}
    \mathcal{G}_\sigma(0) = \|\hat{\mathcal{G}_\sigma}\|_{\ell^1(\mathbb{Z})} = c(\sigma)\sum_{k\in\mathbb{Z}} e^{-k^2\sigma^{-2}/2}=1,
\end{equation*}
where $\hat{\mathcal{G}}_\sigma(k)=c(\sigma)e^{-k^2\sigma^{-2}/2}$ according to \eqref{def:G}. Note that $c(\sigma)\leq 1$. 

The motivation for the definition \eqref{def:G} stems from the fact that $\mathcal{G}_\sigma$ mimics ``Gaussian''-like behavior. We highlight this with the following lemma. 

\begin{lemma}\label{lem:GaussianLike}
    The function $\mathcal{G}_\sigma$, as defined in \eqref{def:G}, satisfies
\begin{equation}\label{posclaim}
    \mathcal{G}_\sigma(x)> 0, \quad \forall x\in\mathbb{T},
\end{equation}
is even, and 
\begin{equation}\label{Poisson_app}
    \mathcal{G}_\sigma(x)= \sum_{l\in\mathbb{Z}} \mathfrak{g}(x+l), \quad \forall x\in\mathbb{T}.
\end{equation}
where $\mathfrak{g}(\xi):= d(\sigma)e^{-2\xi^2\pi^2\sigma^2}$, with $d(\sigma):=\sigma c(\sigma)\sqrt{2\pi}$. 
\end{lemma}

\begin{proof}
    For any $g\in L^2(\mathbb{R})$ we denote the Fourier transform and its inverse by
\begin{equation*}
    \mathcal{F}_{\mathbb{R}}g(\xi) := \int_{\mathbb{R}} g(u)e^{-i2\pi u\xi}\,\mathrm{d}u \quad\text{ and }\quad \mathcal{F}^{-1}_{\mathbb{R}}g(\xi) := \int_{\mathbb{R}} g(u)e^{i2\pi u\xi}\,\mathrm{d}u.
\end{equation*}
Precisely, it is known that $\mathcal{F}_{\mathbb{R}}\circ\mathcal{F}^{-1}_{\mathbb{R}}=Id_{L^2(\mathbb{R})} = \mathcal{F}^{-1}_{\mathbb{R}}\circ\mathcal{F}_{\mathbb{R}}$ \cite{folland1999real}. Now, the function $\mathfrak{g}^*(\xi) := c(\sigma)e^{-\xi^2\sigma^{-2}/2}$ is such that $\mathfrak{g}^*(l) = \hat{\mathcal{G}_\sigma}(l)$ for all $l\in\mathbb{Z}$, and it's the Fourier transform of the function
\begin{equation*}
    \mathcal{F}^{-1}_{\mathbb{R}}\mathfrak{g}^*(\xi)=\mathfrak{g}(\xi)=d(\sigma)e^{-2\xi^2\pi^2\sigma^2}, \quad \forall\xi\in\mathbb{R},
\end{equation*}
where $d(\sigma)=\sigma c(\sigma)\sqrt{2\pi}$. Hence, by invoking the Poisson summation formula \cite{folland1999real}, for each $x\in\mathbb{T}$ we obtain
\begin{equation*}
    \mathcal{G}_\sigma(x)=\sum_{l\in\mathbb{Z}} \hat{\mathcal{G}_\sigma}(l) e^{i2\pi lx} = \sum_{l\in\mathbb{Z}} \mathfrak{g}^*(l)e^{i2\pi lx} = \sum_{l\in\mathbb{Z}} \mathfrak{g}(x+l)>0,
\end{equation*} 
which proves both \eqref{posclaim} and \eqref{Poisson_app}. Finally, that $\mathcal{G}_\sigma$ is even can be seen from definition \eqref{def:G}. 
\end{proof} % end of proof

Next, we introduce a truncated version of $\mathcal{G}_\sigma$, also presented earlier in \eqref{def:Grsigma}. Let $r\in (0,1/2)$ and define, for $x\in [-1/2, 1/2)\cong\mathbb{T}$, the function
\begin{equation*} 
    \mathcal{G}_{r,\sigma}(x) = \begin{cases}
                        \mathcal{G}_\sigma(x) = c(\sigma)\sum_{k\in \mathbb{Z}} e^{-k^2\sigma^{-2}/2}e^{i2\pi kx}, & -r \leq x\leq r\\
                        0, &\text{ otherwise}
                      \end{cases}.
\end{equation*}
Note that $\mathcal{G}_{r,\sigma} = \mathcal{G}_\sigma\chi_{I_r}$, where $\chi_{I_r}$ is the indicator function on the interval $I_r = [-r,r]$. Since $\mathcal{G}_\sigma$ is even and real, so is $\hat{\mathcal{G}}_{r,\sigma}$ on $\mathbb{Z}$. 
Recall the following truncated regularized sampling series of $f$ introduced in \eqref{reconstruction}:
\begin{equation}\label{def:truncregsampseries}
    \mathcal{R}_{r,\sigma}f(x) = \sum_{j=0}^{2N-1} f(j/2N)s(x-j/2N)\mathcal{G}_{r,\sigma}(x-j/2N).
\end{equation}
We compute that
\begin{equation*}
    \widehat{\mathcal{R}_{r,\sigma}f}(k) = \frac{1}{2N}\sum_{j=0}^{2N-1}f(j/2N)e^{-i2\pi kj/2N}\mu_{r,\sigma}(k)
\end{equation*}
where, we have defined 
\begin{equation}\label{def:mur}
    \mu_{r,\sigma}(k) := \chi_{B_N}\ast\hat{\mathcal{G}}_{r,\sigma}(k) = \sum_{l\in k-B_N} \hat{\mathcal{G}}_{r,\sigma}(l).
\end{equation}
If we let
\begin{equation}\label{def:nursigma}
    \nu_{r,\sigma}(k) := \sum_{l\not\in k-B_N} \hat{\mathcal{G}}_{r,\sigma}(l),
\end{equation}
then we gather that
\begin{equation}\label{nur&mur}
    \nu_{r,\sigma}(k) + \mu_{r,\sigma}(k) = \sum_{l\in\mathbb{Z}} \hat{\mathcal{G}}_{r,\sigma}(l) = 1 \quad\text{ and }\quad \nu_{r,\sigma}(k) = \sum_{l\in\mathbb{Z}\setminus\{0\}} \mu_{r,\sigma}(k+l2N), \quad\forall k\in\mathbb{Z}.
\end{equation}
Both $\nu_{r,\sigma}, \mu_{r,\sigma}$ are even on $\mathbb{Z}$. It should be clear from the past analysis that $\mu_{r,\sigma}$ will play the role of $\mu$ in \eqref{def:mu}. Therefore, to access the upper bound in \eqref{l2replace}, Lemma~\ref{lem:regsamp}, we would need an estimate for
\begin{equation*}
    \max_{k\in B_\mathfrak{m}}\bigg\{|\nu_{r,\sigma}(k)|^2 + \sum_{l\in\mathbb{Z}\setminus\{0\}} |\mu_{r,\sigma}(k+l2N)|^2\bigg\}^{1/2}.
\end{equation*}
For ease of computation, we will replace this quantity with
\begin{equation}\label{l2replace*}
    \max_{k\in B_\mathfrak{m}}|\nu_{r,\sigma}(k)| +  \max_{k\in B_\mathfrak{m}}\bigg(\sum_{l\in\mathbb{Z}\setminus\{0\}} |\mu_{r,\sigma}(k+l2N)|^2\bigg)^{1/2}.
\end{equation}
We first handle $\max_{k\in B_\mathfrak{m}}|\nu_{r,\sigma}(k)|$. Referring to definition \eqref{def:nursigma}, observe that for any $k\in\mathbb{Z}$, 
\begin{align}
    \nonumber \nu_{r,\sigma}(k) &= 1 - \sum_{l\in k-B_N} \hat{\mathcal{G}}_{r,\sigma}(l)\\ 
    \nonumber &= 1 - \sum_{l\in k-B_N} \bigg(\hat{\mathcal{G}}_\sigma(l) - \int_{u\not\in [-r,r]\cap [-1/2,1/2)} \mathcal{G}_\sigma (u)e^{-i2\pi lu}\,\mathrm{d}u\bigg)\\
    \nonumber &= \sum_{l\not\in k-B_N}\hat{\mathcal{G}}_\sigma(l) + \int_{u\not\in [-r,r]\cap [-1/2,1/2)} \mathcal{G}_\sigma(u)\sum_{l\in k-B_N} e^{-i2\pi lu}\,\mathrm{d}u\\
    \label{explicitnur} &= \sum_{l\not\in k-B_N}\hat{\mathcal{G}}_\sigma(l) + \int_{u\not\in [-r,r]\cap [-1/2,1/2)} \mathcal{G}_\sigma(u) \cos(2\pi (k-N+1)u)\bigg(\frac{1-e^{-i4\pi Nu}}{1-e^{-i2\pi u}}\bigg)\,\mathrm{d}u.
\end{align}
Note that the first sum on the right-hand-side above is $\nu(k)$ in \eqref{def:nu} with $\phi=\mathcal{G}_\sigma$. This is the {\it regularization} error. The integral on the right-hand-side, carrying an $r$ in its expression, is the {\it truncation} error. 

A quick calculation from \eqref{explicitnur} gives
\begin{equation}\label{maxerr}
    \max_{k\in B_{\mathfrak{m}}} |\nu_{r,\sigma}(k)| \leq \sum_{l\not\in \mathfrak{m}-1-B_N}\hat{\mathcal{G}_\sigma}(l) + \frac{2\mathcal{G}_\sigma(r)}{|1-e^{-i2\pi r}|}.
\end{equation}
Therefore, a bound on $\max_{k\in B_{\mathfrak{m}}} |\nu_{r,\sigma}(k)|$ can be derived from an upper bound for the right-hand-side above.

\begin{proposition}\label{prop:nu}
Let $\nu_{r,\sigma}$ be as in \eqref{def:nursigma}. Let $r,\sigma$ be as in \eqref{rsigmapair}. Then 
\begin{equation*}
    \max_{k\in B_{\mathfrak{m}}} |\nu_{r,\sigma}(k)| \leq C\widetilde{\mathcal{E}}_1(N,\mathfrak{m},\alpha,\beta),
\end{equation*}
for some $C>0$, and
\begin{equation*}
    \widetilde{\mathcal{E}}_1(N,\mathfrak{m},\alpha,\beta) := e^{-3\pi^2(N-\mathfrak{m})^{2(1-\beta)}} \max\bigg\{1,(N-\mathfrak{m})^{2\beta-1}\bigg\} + (N-\mathfrak{m})^{\alpha+\beta} e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}}.
\end{equation*}
\end{proposition}

\begin{proof}
Since $e^{-x^2\sigma^{-2}/2}$ is even on $\mathbb{R}$ and strictly decreasing on $[0,\infty)$, we have
\begin{equation}\label{truncregerrbound1}
    \sum_{l\not\in \mathfrak{m}-1-B_N} \hat{\mathcal{G}_\sigma}(l) \leq 2\sum_{l\geq N-\mathfrak{m}}\hat{\mathcal{G}_\sigma} \leq 2\hat{\mathcal{G}_\sigma}(N-\mathfrak{m}) + 2\int_{N-\mathfrak{m}}^{\infty} e^{-u^2\sigma^{-2}/2}\,\mathrm{d}u,
\end{equation}   
and so using Mills' ratio \eqref{Mill} we obtain 
\begin{equation}\label{Mills}
    \int_{N-\mathfrak{m}}^{\infty} e^{-u^2\sigma^{-2}/2}\,\mathrm{d}u < \frac{\pi\sigma e^{-(N-\mathfrak{m})^2\sigma^{-2}/2}}{\sqrt{(\pi-2)^2 (N-\mathfrak{m})^2\sigma^{-2}+2\pi} + 2(N-\mathfrak{m})\sigma^{-1}}.
\end{equation}
By combining \eqref{truncregerrbound1} and \eqref{Mills}, and performing the necessary simplifications, we arrive at
\begin{equation*}
    \sum_{l\not\in \mathfrak{m}-1-B_N} \hat{\mathcal{G}}_{\sigma}(l)\leq 4e^{-(N-\mathfrak{m})^2\sigma^{-2}/2}\max\bigg\{1,\frac{\sigma^2}{N-\mathfrak{m}}\bigg\}.
\end{equation*}
Keeping in mind that $\sigma = (N-\mathfrak{m})^{\beta}/(\sqrt{6}\pi)$ in \eqref{rsigmapair} allows for the estimate
\begin{equation}\label{truncregerrbound2}
    \sum_{l\not\in \mathfrak{m}-1-B_N} \hat{\mathcal{G}}_{\sigma}(l)\leq 4e^{-3\pi^2(N-\mathfrak{m})^{2(1-\beta)}}\max\bigg\{1,(N-\mathfrak{m})^{2\beta-1}\bigg\}.
\end{equation}
On the other hand, since $r\in (0,1/2)$, the function $e^{-2(x+r)^2\pi^2\sigma^{2}}$ is strictly decreasing on $[0,\infty)$ and increasing on $(-\infty,-1]$. Thus, we deduce from \eqref{Poisson_app} and Mills' ratio that
\begin{equation*}
    \begin{split}
        \mathcal{G}_\sigma(r) &= \sum_{l\in\mathbb{Z}} \mathfrak{g}(r+l) \leq 3d(\sigma)e^{-2r^2\pi^2\sigma^2} + 2d(\sigma)\int_{1-r}^\infty e^{-2\pi^2\sigma^2u^2} \,\mathrm{d}u \\
        &\leq 3\sigma\sqrt{2\pi} e^{-2r^2\pi^2\sigma^2} + \frac{\sqrt{\pi} e^{-2\pi^2\sigma^2(1-r)^2}}{\sqrt{(\pi-2)^2 \pi^2\sigma^2(1-r)^2+\pi} + 2\pi\sigma(1-r)},
    \end{split}
\end{equation*}
which, by the selections of $r, \sigma$ in \eqref{rsigmapair}, simplifies to
\begin{equation}\label{G(r)}
    \mathcal{G}_\sigma(r) \leq C(N-\mathfrak{m})^{\beta} e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}}
\end{equation}
for some $C>0$. Since $r=3\pi(N-\mathfrak{m})^{-\alpha}$, when $N$ is sufficiently large, we can ensure that $1-2\pi r\geq 1/2$. Hence
\begin{equation}\label{secondbd}
    \frac{\mathcal{G}_\sigma(r)}{|1-e^{-i2\pi r}|} \leq \frac{C(N-\mathfrak{m})^{\beta}}{r(1-2\pi r)} \bigg(e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}}\bigg)\leq C(N-\mathfrak{m})^{\alpha+\beta} e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}}.
\end{equation}
By integrating \eqref{maxerr}, \eqref{truncregerrbound2} and \eqref{secondbd}, we acquire the conclusion of the proposition.
\end{proof}

We will now shift our focus to the components involving $\mu_{r,\sigma}$ in \eqref{l2replace*}.

\begin{proposition} \label{prop:mu}
Let $\mu_{r,\sigma}$ be as in \eqref{def:mur}. Let $r,\sigma$ be as in \eqref{rsigmapair}. Then there exists $C>0$ such that
\begin{equation}\label{mul2}
    \max_{k\in B_\mathfrak{m}} \bigg(\sum_{l\in\mathbb{Z}\setminus\{0\}} |\mu_{r,\sigma}(k+l2N)|^2\bigg)^{1/2} \leq C\widetilde{\mathcal{E}}_2(N,\mathfrak{m},\alpha,\beta),
\end{equation}
where
\begin{equation*}
    \widetilde{\mathcal{E}}_2(N,\mathfrak{m},\alpha,\beta) := (N-\mathfrak{m})^\beta e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}} + (N-\mathfrak{m})^{2\beta-\alpha-1} e^{-3\pi^2(N-\mathfrak{m})^{2(1-\beta)}}.
\end{equation*}
\end{proposition}

\begin{proof}
Since $\mu_{r,\sigma}$ is even on $\mathbb{Z}$ and $N>\mathfrak{m}$, it is enough to prove the convergences in \eqref{mul2} for all integers of the form $k'=k+2l'N\geq 0$, where $l'\in\mathbb{N}$, and so $k'-B_N$ consists of positive integers. Fix one such $k'$. Then, by definition \eqref{def:mur}
\begin{equation}\label{murint}
    \mu_{r,\sigma}(k+2l'N) = \mu_{r,\sigma}(k') = \sum_{l\in k'-B_N} \hat{\mathcal{G}}_{r,\sigma}(l) = \sum_{l\in k'-B_N} \int_{-r}^{r} \mathcal{G}_\sigma e^{-i2\pi lu}\,\mathrm{d}u.
\end{equation}
From the Poisson formula \eqref{Poisson_app}, we obtain for each $l\in k'-B_N$ in \eqref{murint} that
\begin{equation}\label{Ijl}
    \begin{split}
        \int_{-r}^{r} \mathcal{G}_\sigma e^{-i2\pi lu}\,\mathrm{d}u &= d(\sigma) \sum_{j\in\mathbb{Z}} \int_{-r}^{r} e^{-2(u+j)^2\pi^2\sigma^2}e^{-i2\pi lu}\,\mathrm{d}u\\
        &= d(\sigma) \sum_{j\in\mathbb{Z}} \int_{j-r}^{j+r} e^{-2u^2\pi^2\sigma^2} e^{-i2\pi lu}\,\mathrm{d}u\\
        &= d(\sigma) e^{-l^2/(2\sigma^2)} \sum_{j\in\mathbb{Z}} \int_{j-r}^{j+r} e^{-2\pi^2\sigma^2(u + il/(2\pi\sigma^2))^2}\,\mathrm{d}u \\
        &=: d(\sigma) \sum_{j\in\mathbb{Z}} \mathcal{I}_{j,l}.
    \end{split}
\end{equation}
Fix one $j\in\mathbb{Z}$ and one $l\in k'-B_N$. We consider the contour integration of the holomorphic function $e^{-l^2/(2\sigma^2)}e^{-2\pi^2\sigma^2 z^2}$ over a rectangle consisting of the following four lines on the complex plane:
\begin{equation}\label{rlines}
    \begin{split}
        \text{vertical:}\quad \{(j+r,y): 0\leq y\leq l/(2\pi\sigma^2)\}, \quad &\{(j-r,y): 0\leq y\leq l/(2\pi\sigma^2)\}\\
        \text{horizontal:}\quad \{(x,0): j-r\leq x\leq j+r\}, \quad &\{(x,l/(2\pi\sigma^2)): j-r\leq x\leq j+r\}.
    \end{split}
\end{equation}
Cauchy's integral theorem \cite[Theorem 2.2]{stein2010complex} then gives that
\begin{equation}\label{contint}
    \begin{split}
        0 = & -\mathcal{I}_{j,l} + e^{-l^2/(2\sigma^2)} \int_{j-r}^{j+r} e^{-2\pi^2\sigma^2(x+i(l/(2\pi\sigma^2)-1/(2\pi\sigma^2)))^2} \,\mathrm{d}x \\
        & + e^{-l^2/(2\sigma^2)}\bigg(\int_{l/(2\pi\sigma^2)}^0 e^{-2\pi^2\sigma^2(j-r +iy)^2}\,i\mathrm{d}y + \int_0^{l/(2\pi\sigma^2)} e^{-2\pi^2\sigma^2(j+r+iy)^2} \,i\mathrm{d}y\bigg).
    \end{split}
\end{equation}
The contribution of the first vertical line in \eqref{rlines} is
\begin{equation}\label{vertical}
    \begin{split}
        e^{-l^2/(2\sigma^2)}\bigg| \int_0^{l/(2\pi\sigma^2)} e^{-2\pi^2\sigma^2(j+r+iy)^2}\,i\mathrm{d}y\bigg| \leq &\frac{e^{-2\pi^2\sigma^2(j+r)^2}}{\sqrt{2}\pi\sigma}\int_0^{l/(\sqrt{2}\sigma)} e^{y^2-l^2/(2\sigma^2)}\,\mathrm{d}y\\
        &=\frac{e^{-2\pi^2\sigma^2(j+r)^2}}{\sqrt{2}\pi\sigma}\int_0^{l/(\sqrt{2}\sigma)} e^{(y-l/(\sqrt{2}\sigma))(y+l/(\sqrt{2}\sigma))}\,\mathrm{d}y.
    \end{split}
\end{equation}
We note that the second and third integrals appearing above take the form $e^{-a^2}\int_0^a e^{-u^2}\,\mathrm{d}u$, which is a {\it Dawson's integral} \cite{carneiro2013bandlimited}. To handle it, we make a convenient change of variables as follows. Let $u\geq 0$ be such that $-u=y-l/(\sqrt{2}\sigma)$. Then since $0\leq y\leq l/(\sqrt{2}\sigma)$,
\begin{equation*}
    -\frac{2l}{\sqrt{2}\sigma} u\leq y^2-\frac{l^2}{2\sigma^2} = -u(y+\frac{l}{\sqrt{2}\sigma})\leq -\frac{l}{\sqrt{2}\sigma} u.
\end{equation*}
Putting this back into \eqref{vertical} gives us
\begin{equation*}
    \begin{split}
        e^{-l^2/(2\sigma^2)}\bigg| \int_0^{l/(2\pi\sigma^2)} e^{-2\pi^2\sigma^2(j+r+iy)^2}\,i\mathrm{d}y\bigg| &\leq \frac{e^{-2\pi^2\sigma^2(j+r)^2}}{\sqrt{2}\pi\sigma}\int_0^{l/(\sqrt{2}\sigma)} e^{-ul/(\sqrt{2}\sigma)}\,\mathrm{d}u\\
        &\leq \frac{e^{-2\pi^2\sigma^2(j+r)^2}}{l\pi}\int_0^{l^2/(2\sigma^2)} e^{-u}\,\mathrm{d}u\\
        &=: \frac{e^{-2\pi^2\sigma^2(j+r)^2}\mathcal{J}_l}{l\pi},
    \end{split}
\end{equation*}
where, $\mathcal{J}_l\to 1$ as $l\to\infty$. By the Poisson summation formula \eqref{Poisson_app} again,
\begin{equation*}
        \sum_{l\in k'-B_N} \frac{\mathcal{J}_l}{l\pi}\sum_{j\in\mathbb{Z}} d(\sigma) e^{-2\pi^2\sigma^2(j+r)^2} = 
        \sum_{l\in k+2l'N-B_N} \frac{\mathcal{G}_\sigma(r)\mathcal{J}_l}{l\pi}.
\end{equation*}
We square-sum the final term above over $l' \in \mathbb{N}$, acquiring for any $k\in B_\mathfrak{m}$
\begin{equation}\label{sideint}
    \begin{split}
        \sum_{l'\in\mathbb{N}} \bigg| \sum_{l=k+(2l'-1)N+1}^{k+(2l'+1)N} \frac{\mathcal{G}_\sigma(r)\mathcal{J}_l}{l\pi}\bigg|^2 &\leq \frac{2N\mathcal{G}_\sigma(r)^2}{\pi^2} \sum_{l'\in\mathbb{N}} \quad \sum_{l=k+(2l'-1)N+1}^{k+(2l'+1)N} \frac{1}{l^2}\\
        &= \frac{2N\mathcal{G}_\sigma(r)^2}{\pi^2} \sum_{l=k+N+1}^\infty \frac{1}{l^2}\\
        &\leq \frac{2N\mathcal{G}_\sigma(r)^2}{\pi^2}\int_{k+N}^\infty \frac{1}{l^2}\\
        &\leq \frac{2N\mathcal{G}_\sigma(r)^2}{\pi^2(N-\mathfrak{m})}.
    \end{split}
\end{equation}
Employing the same method for the contribution of the second vertical line in \eqref{rlines}, and keeping in mind that $\mathcal{G}_\sigma(r) = \mathcal{G}_\sigma(-r)$, we also arrive at an analogous bound. On the other hand, consideration of the bottom horizontal integral in \eqref{contint} leads to:
\begin{equation*}
    e^{-l^2/(2\sigma^2)} d(\sigma) \sum_{j\in\mathbb{Z}} \int_{j-r}^{j+r} e^{-2\pi^2\sigma^2x^2}\,\mathrm{d}x = e^{-l^2/(2\sigma^2)}\int_{-r}^r \mathcal{G}_\sigma(x) \,\mathrm{d}x \leq \frac{2e^{-l^2/(2\sigma^2)}}{(N-\mathfrak{m})^\alpha}.
\end{equation*}
Similarly as before, since $l \in k' - B_N$, where $k' = k + 2l'N$, $k \in B_\mathfrak{m}$ and $l' \in \mathbb{N}$, summing over $l' \in \mathbb{N}$ for an arbitrary $k \in B_\mathfrak{m}$ results in
\begin{equation*}
    \begin{split}
        \sum_{l'=1}^\infty \quad \sum_{l\in k+2l'N-B_N} e^{-l^2/(2\sigma^2)} = \sum_{l=k+N+1}^\infty e^{-l^2/(2\sigma^2)}
        &\leq\int_{N+k}^\infty e^{-x^2/(2\sigma^2)}\,\mathrm{d}x\\
        &\leq \frac{C\sigma^2e^{-(N+k)^2/(2\sigma^2)}}{N+k}\\
        &\leq \frac{C\sigma^2e^{-(N-\mathfrak{m})^2/(2\sigma^2)}}{N-\mathfrak{m}},
    \end{split}
\end{equation*}
where Mills' ratio \eqref{Mill} has entered at the second to last inequality. Then an application of the embedding of the $\ell^p$ spaces leads to
\begin{equation}\label{botint}
    \begin{split}
        \frac{1}{(N-\mathfrak{m})^\alpha}\bigg(\sum_{l'=1}^\infty \bigg| \sum_{l\in k+2l'N-B_N} e^{-l^2/(2\sigma^2)}\bigg|^2\bigg)^{1/2} &\leq \frac{1}{(N-\mathfrak{m})^\alpha} \sum_{l=k+N+1}^\infty e^{-l^2/(2\sigma^2)} \\
        &\leq \frac{C\sigma^2e^{-(N-\mathfrak{m})^2/(2\sigma^2)}}{(N-\mathfrak{m})^{\alpha+1}}.
    \end{split}    
\end{equation}
Therefore, by combining \eqref{murint}, \eqref{Ijl}, \eqref{contint} \eqref{sideint}, \eqref{botint}, along with \eqref{G(r)}, we obtain 
\begin{equation*}
    \begin{split}
        \bigg(\sum_{l'\in\mathbb{N}} |\mu_{r,\sigma}(k+2l'N)|^2\bigg)^{1/2} &\leq \frac{C\sigma^2e^{-(N-\mathfrak{m})^2/(2\sigma^2)}}{(N-\mathfrak{m})^{\alpha+1}} + C\mathcal{G}_\sigma(r) \\
        &\leq C(N-\mathfrak{m})^{2\beta-\alpha-1}e^{-3\pi^2(N-\mathfrak{m})^{2(1-\beta)}} + C(N-\mathfrak{m})^{\beta} e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}}\\
        &= \widetilde{\mathcal{E}}_2(N,\mathfrak{m},\alpha,\beta),
    \end{split} 
\end{equation*}
from which and the symmetry mentioned at the start of the proof, \eqref{mul2} follows. 
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:TsamplingregwG}]
The proof is a combination of the observation made in \eqref{l2replace*}, the proof of Lemma~\ref{lem:regsamp} and Propositions~\ref{prop:nu},~\ref{prop:mu}.
\end{proof}
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem~\ref{thm:WNN}} \label{sec:WNNthm}

We begin by reminding the reader of some important points that will be useful for this section. Firstly, let $f \in \mathcal{B}_\mathfrak{m}$ and assume that $N > \mathfrak{m}$. We have asserted that 
\begin{equation}\label{gpapprox}
    \mathcal{R}_{r,\sigma}f(x) = \sum_{j=0}^{2N-1} f(j/2N)(s_N\,\mathcal{G}_{r,\sigma})(x-j/2N) \approx f(x), 
\end{equation}
in the $L^2$ norm on $[0,1]$, with precise error bounds provided in Theorem~\ref{thm:TsamplingregwG}. Importantly, we require in $\mathcal{G}_{r,\sigma}$,
\begin{equation*}
    \sigma = \frac{(N-\mathfrak{m})^{\beta}}{\sqrt{6}\pi} \quad\text{ and }\quad r = \frac{3\pi}{(N-\mathfrak{m})^{\alpha}},
\end{equation*}
where $0<\alpha<\beta<1$. To facilitate convenience in future calculations, we now identify $s_N$, $\mathcal{G}_{r,\sigma}$ with their $1$-periodic counterparts, respectively. Then $s_N\in C^{\infty}(\mathbb{R})$, and for every $j\in\mathbb{N}$, $\mathcal{G}_{r,\sigma}^{(j)}= d^{j}\mathcal{G}_{r,\sigma}/dx^{j}$ exists almost everywhere. Recall that we have defined in \eqref{def:Gtruncder}
\begin{equation}\label{def:Grprime}
    \mathcal{G}_{r,\sigma}^{(j)}(x)=\begin{cases}
                                \mathcal{G}_{\sigma}^{(j)}(x),  &\exists k\in\mathbb{Z}\ \mathrm{s.t.}\ k - r \leq x \leq k + r \\
                                0, &\mathrm{otherwise}.
                              \end{cases}
\end{equation}
This extension allows us to interpret the argument addition in
\begin{equation*} 
    \sum_{j=0}^{2N-1} f(j/2N)(s_N\,\mathcal{G}_{r,\sigma})(x-j/2N), 
\end{equation*}
as taking place on $\mathbb{R}$. 

Secondly, we have established in \S\ref{sec:WNNframework} that the output of our WNN will take the form
\begin{equation}\label{WNNoutputrecall}
    \sum_{j=0}^{2N-1} f(j/2N)T_\mathcal{K}ReLU(x-j/2N), \quad\forall x\in [0,1].
\end{equation}
Therefore, by juxtaposing \eqref{gpapprox} and \eqref{WNNoutputrecall}, it becomes evident that we would complete most of the proof if we can demonstrate that:
\begin{equation}\label{filterstep}
        T_\mathcal{K}ReLU(x-j/2N) = (s_N\,\mathcal{G}_{r,\sigma})(x - j/2N) + \mathcal{E},
\end{equation}
for a small $\mathcal{E}$ error term and $x$ in an appropriately large subset of $[0,1]$. If $\mathcal{E}$ can be bounded and shown to vanish as $N \to \infty$, then the overall, accumulated, discrepancy between the network output and $\mathcal{R}_{r,\sigma}f$ can be made small uniformly for $x$ in the same subset. The proof is then completed by recalling \eqref{gpapprox}.

We proceed as planned with the task of bounding the error in \eqref{filterstep} with the following proposition. 

\begin{proposition} \label{prop:WNNthmcont} Let $\mathfrak{m}\in\mathbb{N}$ and $\varepsilon>0$. Suppose $W$ satisfies Assumption~\ref{assump:regular} and $N>\mathfrak{m}$ is large enough so that 
\begin{equation}\label{rkappa}
    r = \frac{3\pi}{(N-\mathfrak{m})^{\alpha}} \leq \kappa
\end{equation}
where $\kappa$ is as in \eqref{def:diagregion}. Fix $x\in [r,1-r]$ and $j=0,\cdots, 2N-1$. If $x-j/2N\in [-r,r)$, then there exists $C>0$ such that the following holds
\begin{multline}\label{eq:WNNthmcont}
    |T_\mathcal{K}ReLU(x-j/2N)-(s_N\,\mathcal{G}_{r,\sigma})(x-j/2N)| \\ 
    \leq C\eta^{-1} \bigg(KNr^2 + (N-\mathfrak{m})^{\beta}e^{-(N-\mathfrak{m})^{2\beta}/24}\bigg).
\end{multline}
\end{proposition}

The proof of Proposition~\ref{prop:WNNthmcont} requires the knowledge of two auxiliary lemmas below. One is an integration-by-parts result, and the other gives estimates of $s_N\,\mathcal{G}_{r,\sigma}$ that are not only essential for the present section but also for all others. Their proofs are given in \S\ref{sec:intbypartslem} and \S\ref{sec:Gsizelem}, respectively.

\begin{lemma} \label{lem:integrationbyparts}
Fix $r\in (0,1)$, $x\in [r,1-r]$ and $j=0,\cdots, 2N-1$. Suppose $x-j/2N\in [-r,r)$. Then
\begin{multline}
    (s_N\,\mathcal{G}_{r,\sigma})(x-j/2N)=
    (x+r-j/2N)(s_N\,\mathcal{G}_{r,\sigma})'(-r) + (s_N\,\mathcal{G}_{r,\sigma})(-r) \\
    + \int_{x-r}^{x+r} ReLU(y-j/2N)(d^2/dy^2)
    (s_N\,\mathcal{G}_{r,\sigma})(x-y)\,\mathrm{d}y.
\end{multline}
\end{lemma}

\begin{lemma} \label{lem:Gsize} 
Given a pair of $r,\sigma$ as in \eqref{rsigmapair}, with $0<\alpha<\beta<1$. There exists $C> 0$ such that for all $N>\mathfrak{m}$ we have
\begin{enumerate}
    \item[(a)] $|(s_N\,\mathcal{G}_{r,\sigma})'(r)| \leq C(N-\mathfrak{m})^{\beta} e^{-(N-\mathfrak{m})^{2\beta}/24}$,
    \item[(b)] $\|(s_N\,\mathcal{G}_{r,\sigma})'\|_{L^{\infty}([0,1])} \leq CN$
    \item[(c)] $\|(s_N\,\mathcal{G}_{r,\sigma})''\|_{L^{\infty}([0,1])} \leq C\max\{N^2, N^{3\beta}\}$, 
    \item[(d)] $\|(s_N\,\mathcal{G}_{r,\sigma})'''\|_{L^{\infty}([0,1])} \leq CN\max\{N^2, N^{3\beta}\}$.
\end{enumerate}
\end{lemma}

Equipped with these lemmas, we now delve into the proof of Proposition~\ref{prop:WNNthmcont}.

\begin{proof}[Proof of Proposition~\ref{prop:WNNthmcont}]
We introduce abbreviations that will be used in this proof as well as in the proofs of Theorems~\ref{thm:GNNdet},~\ref{thm:GNNran} to enhance a compact presentation. Set
\begin{equation*}
    \mathcal{G}^*(x,y):=\frac{\mathrm{d}^2}{\mathrm{d} y^2}(s_N\,\mathcal{G}_{r,\sigma})(x-y)
\end{equation*}
and
\begin{equation*}
    \rho_j(x) := ReLU(x-j/2N)=ReLU_{j/2N}(x)
\end{equation*}
for each for $j=0,\cdots, 2N-1$. Begin by noting that if $x\in [r,1-r]$, then from definition \eqref{Wxbar}, $\mathfrak{I}_x = [-r+x,x+r]$. Now if $x-j/2N\in [-r,r)$, then from Lemma~\ref{lem:integrationbyparts} 
\begin{equation}\label{intbpclaim}
    \begin{split}
        (s_N\,\mathcal{G}_{r,\sigma})(x-j/2N) &= \int_{x-r}^{x+r} \rho_j(y)\mathcal{G}^*(x,y)\,\mathrm{d}y \\ 
        &\quad + (x+r-j/2N)(s_N\,\mathcal{G}_{r,\sigma})'(-r) + (s_N\,\mathcal{G}_{r,\sigma})(-r).
    \end{split}
\end{equation}
I.e.,
\begin{equation}\label{E1}
    \int_{\mathfrak{I}_x} \rho_j(y)\mathcal{G}^*(x,y)\,\mathrm{d}y
    =(s_N\,\mathcal{G}_{r,\sigma})(x-j/2N) + \mathcal{E}_1,
\end{equation}
and
\begin{equation*} 
    \mathcal{E}_1 :=-(x+r-j/2N)(s_N\,\mathcal{G}_{r,\sigma})'(-r) - (s_N\,\mathcal{G}_{r,\sigma})(-r).
\end{equation*}
From Lemma~\ref{lem:Gsize} and \eqref{G(r)}, we obtain
\begin{equation}\label{E1bd}
    |\mathcal{E}_1|\leq C(N-\mathfrak{m})^{\beta} e^{-(N-\mathfrak{m})^{2\beta}/24}.
\end{equation}
Next, we compare $T_\mathcal{K}ReLU(x - j/2N)=T_\mathcal{K}\rho_j(x)$ with $\int_{\mathfrak{I}_x} \rho_j(y)\mathcal{G}^*(x,y)\,\mathrm{d}y$. Due to \eqref{rkappa}, $\{(x,y): y\in \mathfrak{I}_x\} \subset\mathcal{D}_\kappa$. Therefore, it follows from Assumption~\ref{assump:regular} on $W$ that $\mathcal{W}_{x}\geq\eta$ and 
\begin{equation}\label{Lip}
\begin{split}
    |\mathcal{W}_{x} - W(x,y)| &= \bigg|\frac{1}{|\mathfrak{I}_x|}\int_{\mathfrak{I}_x} (W(x,u)- W(x,y))\,\mathrm{d}u \bigg| \\ 
    &\leq \frac{1}{|\mathfrak{I}_x|}\int_{\mathfrak{I}_x} |W(x,u)-W(x,y)|\,\mathrm{d}u \\ 
    &\leq 2 Kr
\end{split}
\end{equation}
for almost every $y\in \mathfrak{I}_x$. Moreover, by the Lipschitz continuity assumption, $(\mathrm{d}/\mathrm{d}y)W(x,y)$ exists almost everywhere on $I_{x}$ (Radamacher's theorem) \cite[\S 5.8.3, Theorem 5]{evans1998partial} and that
\begin{equation}\label{Kbd}
    \bigg|\frac{\mathrm{d}}{\mathrm{d}y}W(x,y)\bigg|\leq K;
\end{equation}
therefore
\begin{equation}\label{tighter}
    \begin{split}
        &\int_{\mathfrak{I}_x} \rho_{j}(y)\mathcal{G}^*(x,y)\bigg(W(x,y)-\mathcal{W}_{x}\bigg)\,\mathrm{d}y = \int_{j/2N}^{x+r}\rho_{j}(y)\mathcal{G}^*(x,y)\bigg(W(x,y)-\mathcal{W}_{x}\bigg)\,\mathrm{d}y \\
        &= -\int_{j/2N}^{x+r} (\mathrm{d}/\mathrm{d}y)(s_N\,\mathcal{G}_{r,\sigma})(x-y)\bigg\{\rho_{j}(y)(\mathrm{d}/\mathrm{d}y)W(x,y)+W(x,y)-\mathcal{W}_{x}\bigg\}\,\mathrm{d}y + \mathcal{E}_2.
    \end{split}
\end{equation}
Here
\begin{equation*}
    \mathcal{E}_2:= (\mathrm{d}/\mathrm{d}y)(s_N\,\mathcal{G}_{r,\sigma})(x-y)\rho_{j}(y)\bigg(W(x,y)-\mathcal{W}_{x}\bigg)\bigg|_{y=j/2N}^{y=x+r}
\end{equation*}
and so as a result of Lemma~\ref{lem:Gsize} again
\begin{equation}\label{E2bd}
    |\mathcal{E}_2|\leq C(N-\mathfrak{m})^{\beta} e^{-(N-\mathfrak{m})^{2\beta}/24}.
\end{equation}
Going back to the third integral in \eqref{tighter}, we see that
\begin{equation}\label{tighter2}
    \begin{split}
        &\bigg|\int_{j/2N}^{x+r} (\mathrm{d}/\mathrm{d}y)(s_N\,\mathcal{G}_{r,\sigma})(x-y)\bigg\{\rho_{j}(y)(\mathrm{d}/\mathrm{d}y)W(x,y)+W(x,y)-\mathcal{W}_{x}\bigg\}\,\mathrm{d}y\bigg|\\
        & \leq CN\bigg(K\int_0^{2r} y\,\mathrm{d}y + Kr^2 \bigg) \leq CKNr^2
    \end{split}
\end{equation}
where the first bound comes from the definition of $\rho_{j}=ReLU_{j/2N}$ for $j/2N\in [x-r,x+r)$, Lemma~\ref{lem:Gsize}, \eqref{Lip} and \eqref{Kbd}. Then, by synthesizing the findings of \eqref{Kbd}, \eqref{tighter}, \eqref{E2bd}, \eqref{tighter2}, we acquire
\begin{equation}\label{oneend}
    \begin{split}
        \bigg|T_\mathcal{K}\rho_{j}(x) - \int_{\mathfrak{I}_x} \rho_{j}(y)\mathcal{G}^*(x,y)\,\mathrm{d}y\bigg| &= \frac{1}{\mathcal{W}_x}\bigg|\int_{\mathfrak{I}_x} \rho_{j}(y)\mathcal{G}^*(x,y)\bigg(W(x,y)-\mathcal{W}_x\bigg)\,\mathrm{d}y\bigg|\\
        &\leq C\eta^{-1}\bigg(KNr^2 + (N-\mathfrak{m})^{\beta} e^{-(N-\mathfrak{m})^{2\beta}/24}\bigg).
    \end{split}
\end{equation}
As the final step, we combine \eqref{E1}, \eqref{E1bd}, \eqref{oneend} and arrive at the inequality \eqref{eq:WNNthmcont}, proving the proposition. 
\end{proof} % end of proof of proposition

Having now successfully established the proof of Proposition~\ref{prop:WNNthmcont}, we proceed to the proof of Theorem~\ref{thm:WNN}. 

% WNN proof
\begin{proof}[Proof of Theorem~\ref{thm:WNN}]
Without loss of generality, we may assume $\|f\|_{L^2([0,1];\mathbb{C}^m)} = 1$. From Lemma~\ref{lem:tech} we further have
\begin{equation}\label{norm1}
    \|f\|_{L^2([0,1];\mathbb{C}^m)}^2 = \sum_{j=0}^{2N-1} |f(j/2N)|^2 =1.
\end{equation}
At the outset, we will make a temporary assumption that \eqref{rkappa} holds. Fix $x\in [r,1-r]$ and suppose that 
\begin{equation}\label{exclcond}
    x-j/2N\not=r, \quad\forall j=0,\cdots,2N-1.
\end{equation}
Note that there are only at most $\lceil rN\rceil$ indices $j$ such that $x-j/2N\in [-r,r)$. Define
\begin{equation}\label{def:fsharp}
    f_{\sharp}(y) := \sum_{j=0}^{2N-1} f(j/2N)\rho_j(y), \quad \forall y\in [0,1].
\end{equation} 
Then $T_\mathcal{K}f_{\sharp}(x) = \sum_{j=0}^{2N-1} f(j/2N)T_\mathcal{K}\rho_j(x) = \Psi_f(x)$ (see \S\ref{sec:WNNframework}). On the other hand, recall the regularlized sampling series
\begin{equation*} 
    \mathcal{R}_{r,\sigma}f(x) = \sum_{j=0}^{2N-1} f(j/2N)(s_N\,\mathcal{G}_{r,\sigma})(x-j/2N).
\end{equation*}
Therefore, as a direct consequence of \eqref{norm1}, Proposition~\ref{prop:WNNthmcont} and the $L^p$-embedding property for finite measure spaces, 
\begin{equation*}
    \begin{split}
        |\Psi_f(x)-\mathcal{R}_{r,\sigma}f(x)| &=|T_\mathcal{K}f_{\sharp}(x)-\mathcal{R}_{r,\sigma}f(x)|\\
        &\leq C\eta^{-1} r^{1/2}N^{1/2}\bigg(KNr^2 + (N-\mathfrak{m})^{\beta} e^{-(N-\mathfrak{m})^{2\beta}/24}\bigg).
    \end{split}
\end{equation*}
Combining this and the result of Theorem~\ref{thm:TsamplingregwG} - and ignoring all the exponential error parts - we obtain
\begin{equation}\label{laststep}
    \|\Psi_f-f\|_{L^2([r,1-r];\mathbb{C}^m)}\leq C\eta^{-1}K N^{3/2-\alpha 5/2}.
\end{equation}
We now set
\begin{equation}\label{sys}
    3/2 - \alpha 5/2 = 0.9 \quad\text{ and }\quad \beta - \alpha = 0.02,
\end{equation}
hence, $\alpha=0.96$, $\beta=0.98$. Set
\begin{equation*}
    N=\lceil \varepsilon^{-10/9}\rceil;
\end{equation*}
then it follows from \eqref{laststep}, \eqref{sys} that
\begin{equation*}
    \|\Psi_f-f\|_{L^2([r,1-r];\mathbb{C}^m)} \leq C\eta^{-1}K\varepsilon
\end{equation*}
for every $x\in [r,1-r]$ that satisfies \eqref{exclcond}. We now note that \eqref{rkappa} can be guaranteed by taking
\begin{equation*}
    \frac{3\pi}{\kappa}\leq \varepsilon^{-1}, 
\end{equation*}
which can be accomplished by taking $\varepsilon\in (0,1)$ to be sufficiently small. The proof is now concluded.
\end{proof}

\begin{remark} \label{rem:choices}
It should be noted that the choices made for $r,\sigma$ in \eqref{rsigmapair} are made specifically to ensure that, when $\alpha = 0.96$, $\beta = 0.98$, all exponential error terms that should appear in \eqref{laststep} are dominated by the rational term on its right-hand side even for small $(N-\mathfrak{m})$. In general, for given $0<\alpha<\beta<1$, one can find $c_1(\alpha,\beta), c_2(\alpha,\beta)>0$ such that, if
\begin{equation*}
    r = \frac{c_1(\alpha,\beta)}{(N-\mathfrak{m})^{\alpha}} \quad\text{ and }\quad \sigma = \frac{(N-\mathfrak{m})^{\beta}}{c_2(\alpha,\beta)},
\end{equation*} 
then a similar effect of keeping the exponential terms small, as observed in \eqref{laststep}, is achieved.
\end{remark}

\begin{remark}\label{rem:flat}
It is important to note that the favorable subgeometric sampling convergence rate established Theorem~\ref{thm:TsamplingregwG} has been forfeited in the proof of Theorem~\ref{thm:WNN} due to the implementation of the integration by parts given in Lemma~\ref{lem:integrationbyparts}. Let us suppose for a moment that $W$ is constant over the diagonal neighborhood $\mathcal{D}_\kappa$, in which case, a diligent study of the proof presented would lead to the conclusion that 
\begin{equation*}
    \|\Psi_f-f\|_{L^2([r,1-r];\mathbb{C}^m)}=\mathcal{O}(e^{-CN^{2(\beta-\alpha)}}),
\end{equation*}
and it would have been enough to select $N=C\lceil (\log(1/\varepsilon))^{1/(2(\beta-\alpha))}\rceil$. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:integrationbyparts}} \label{sec:intbypartslem}

The lemma is a direct consequence of the following known result which we provide for the reader.

\begin{theorem} (Abel's identity) \cite[Theorem 4.2]{apostol1998introduction}\label{thm:Abel}
Let $\{a_n\}_{n\geq 0}$ be a real-valued sequence. Define, for $t\geq 0$
\begin{equation}\label{A} 
    A(t) :=\sum_{n\leq t}a_n,
\end{equation}
and $A(t)=0$ if $t<0$. Then for every $g\in C^1[u,v]$, one has,
\begin{equation}\label{abel}
    \sum_{u<n\leq v}a_ng(n)=A(v)g(v)-A(u)g(u)-\int_{u}^{v}A(y)g'(y)\,\mathrm{d}y.
\end{equation}
\end{theorem}

To apply Theorem~\ref{thm:Abel}, we define $\{a_n\}_{n\geq 0}$ be such that $a_0 = 1$ and $a_n=0$ if $n\not= 0$. Let $A$ be as in \eqref{A}, so
\begin{equation}\label{ourA} 
    A(t)=\chi_{\{t\geq 0\}}(t).
\end{equation}
For $x$ and $j$ as stated in the lemma, we define 
\begin{equation*}
    \tilde{\mathcal{G}}(y):= (s_N\,\mathcal{G}_{r,\sigma})(x-j/2N-y).
\end{equation*}
Observe that $\tilde{\mathcal{G}}\in C^1([x-r-j/2N, x+r-j/2N])$. Therefore it follows from \eqref{abel} and \eqref{ourA} that
\begin{align} 
    \nonumber \sum_{x-r-j/2N<n\leq x+r-j/2N}a_n\tilde{\mathcal{G}}(n) &=A(x+r-j/2N)\tilde{\mathcal{G}}(x+r-j/2N)-\int_0^{x+r-j/2N} (\mathrm{d}/\mathrm{d}y)\tilde{\mathcal{G}}(y)\,\mathrm{d}y \\
    \label{1st} \Rightarrow (s_N\,\mathcal{G}_{r,\sigma})(x-j/2N) &= (s_N\,\mathcal{G}_{r,\sigma})(-r)+\int_{-r}^{x-j/2N} (\mathrm{d}/\mathrm{d}y)(s_N\,\mathcal{G}_{r,\sigma})(y)\,\mathrm{d}y.
\end{align}
On the other hand,
\begin{align}
    \nonumber \int_{x-r}^{x+r} &ReLU(y-j/2N)(d^2/dy^2)
    (s_N\,\mathcal{G}_{r,\sigma})(x-y)\,\mathrm{d}y \\
    \nonumber &= \int_{-r}^{x-j/2N} (x-y-j/2N)(d^2/dy^2)(s_N\,\mathcal{G}_{r,\sigma})(y)\,\mathrm{d}y\\
    \label{2nd} &= -(x+r-j/2N)(s_N\,\mathcal{G}_{r,\sigma})'(-r) + \int_{-r}^{x-j/2N} (\mathrm{d}/\mathrm{d}y)(s_N\,\mathcal{G}_{r,\sigma})(y)\,\mathrm{d}y,
\end{align}
and so combining \eqref{1st} and \eqref{2nd} yields 
\begin{multline*}
    \int_{x-r}^{x+r} ReLU(y-j/2N)(d^2/dy^2)
    (s_N\,\mathcal{G}_{r,\sigma})(x-y)\,\mathrm{d}y \\
    = -(x+r-j/2N)(s_N\,\mathcal{G}_{r,\sigma})'(-r) - s_N\,\mathcal{G}_{r,\sigma}(-r) + (s_N\,\mathcal{G}_{r,\sigma})(x-j/2N), 
\end{multline*}
which is what we want. \qed 
% end of proof of lemma

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Lemma~\ref{lem:Gsize}} \label{sec:Gsizelem}

For $k=1,2,3$ we have, from \eqref{def:Grprime}, that
\begin{equation}\label{ders}
    (s_N\,\mathcal{G}_{r,\sigma})^{(k)}(x) = \sum_{l=0}^k {k \choose l} (s^{(l)}\,\mathcal{G}_\sigma^{(k-l)})(x),\quad \forall x\in [-r,r].
\end{equation}
We first prove (a), so we take $k=1$. We utilize the Poisson formulation \eqref{Poisson_app}, which states
\begin{equation*} 
    \mathcal{G}_{\sigma}(x)= \sum_{l\in\mathbb{Z}} \mathfrak{g}(x+l), \quad \forall x\in [0,1].
\end{equation*}
where $\mathfrak{g}(x) = d(\sigma)e^{-2x^2\pi^2\sigma^2}$ on $\mathbb{R}$ and $d(\sigma) = \sigma c(\sigma)\sqrt{2\pi}$. Therefore,
\begin{equation*}
    \mathcal{G}_\sigma'(r) = \sum_{l\in\mathbb{Z}} \mathfrak{g}'(r+l) = -4\pi^2\sigma^2 d(\sigma)\sum_{l\in\mathbb{Z}} (r+l)e^{-2(r+l)^2\pi^2\sigma^2}.
\end{equation*}
Since $r\in (0,1/2)$, simple dominations and an application of Mills' ratio yield
\begin{equation}\label{Gprimer}
    \begin{split}
        |\mathcal{G}_\sigma'(r)| &\leq C\sigma^2 d(\sigma)\bigg( e^{-2\pi^2\sigma^2(1-r)^2} + \int_{1-r}^{\infty} e^{-\pi^2y^2\sigma^{2}} \,\mathrm{d}y\bigg)\\
        &\leq C\sigma^3 e^{-2\pi^2\sigma^2(1-r)^2} + C\sigma e^{-\pi^2\sigma^2(1-r)^2}\\
        &\leq C(N-\mathfrak{m})^{\beta} e^{-(N-\mathfrak{m})^{2\beta}/24}.
    \end{split}
\end{equation}
Now, it's evident that
\begin{equation}\label{sampfsize}
    \|s'\|_{L^{\infty}([0,1])}\leq CN, \quad \|s''\|_{L^{\infty}([0,1])}\leq CN^2, \quad \|s'''\|_{L^{\infty},([0,1])}\leq CN^3,
\end{equation}
and that
\begin{equation}\label{Grsize}
    |\mathcal{G}_\sigma(r)|\leq C(N-\mathfrak{m})^{\beta}e^{-3\pi^2(N-\mathfrak{m})^{2(\beta-\alpha)}}
\end{equation}
was demonstrated in \eqref{G(r)}. Thus, combining \eqref{ders}, \eqref{Gprimer}, \eqref{sampfsize}, \eqref{Grsize} with that $|\mathcal{G}_\sigma(r)|=|\mathcal{G}_{r,\sigma}(r)|$, $|\mathcal{G}'_\sigma(r)|=|\mathcal{G}'_{r,\sigma}(r)|$, we have the proof of statement (a).

We will now prove statements (b), (c), (d) simultaneously. If $x\in [-r,r]$, then similar to the calculation done in \eqref{Gprimer}, we can obtain
\begin{equation}\label{shortroute}
    |\mathcal{G}'_{r,\sigma}(x)|=|\mathcal{G}'_\sigma(x)|\leq C(N-\mathfrak{m})^{\beta} e^{-(N-\mathfrak{m})^{2\beta}/24},
\end{equation}
which is of order $\mathcal{O}(1)$ whenever $N$ is sufficiently large. Moreover, by definition \eqref{def:G}
\begin{equation*}
    \mathcal{G}_{r,\sigma}^{(k)}(x)=\mathcal{G}_\sigma^{(k)}(x) = (i2\pi)^k c(\sigma)\sum_{l\in\mathbb{Z}} l^ke^{-l^2\sigma^{-2}/2}e^{i2\pi lx}
\end{equation*}
for $k=1,2,3$. Hence, 
\begin{equation*}
    |\mathcal{G}_{r,\sigma}^{(k)}(x)|=|\mathcal{G}_\sigma^{(k)}(x)|\leq C\sigma^{k+1}\sum_{l\in\mathbb{N}} 
    \bigg(\frac{l^k}{\sigma^{k}} e^{-l^2\sigma^{-2}/2}\bigg)\frac{1}{\sigma} \leq C\sigma^{k+1}\bigg( 1 + \int_0^{\infty} x^{k} e^{-x^2}\,\mathrm{d}x\bigg)
\end{equation*}
which, together with \eqref{sampfsize}, \eqref{shortroute}, yields (b), (c), (d), as desired. \qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem~\ref{thm:GNNdet}} \label{sec:GNNdet}

We start out with some preparation. We continue to assume \eqref{rkappa} holds with $\alpha=0.96$. Let the graph $G^{\mathrm{det}}_n$ and its graph signal $f_n$ be as in the premise of Theorem~\ref{thm:GNNdet}. As illustrated by \S\ref{sec:WNN}, \S\ref{sec:WNNframework}, we can identify the graph signal $f_n$ with a function on $\mathcal{X}_n = \{x_1,\cdots,x_n\}$, where
\begin{equation*}
    x_k = \frac{k-1}{n}\in [0,1].
\end{equation*}
Recall that $f_n(x_k)=f(x_k)$, for every $x_k$, and the following abbreviations set in the proof of Theorem~\ref{thm:WNN}:
\begin{equation*}
    \mathcal{G}^*(x,y)=\frac{\mathrm{d}^2}{\mathrm{d} y^2}(s_N\,\mathcal{G}_{r,\sigma})(x-y) \quad\text{ and }\quad \rho_j(x) = ReLU_{j/2N}(x).
\end{equation*}
Then, from \S\ref{sec:WNNframework}, the GNN instantiated by $\Psi_{n,f}$ is expressed as
\begin{equation*}
    \Psi_{n,f}(x_k) = \sum_{j=0}^{2N-1} f(j/2N)\mathfrak{F}(\rho_j)(x_k).
\end{equation*}
Since $f\in\mathcal{B}_\mathfrak{m}$, we infer from Theorem~\ref{thm:WNN} that the WNN machinery described therein yields the following outcome
\begin{equation*}
    \Psi_f(x) = \sum_{j=0}^{2N-1} f(j/2N)T_\mathcal{K}\rho_j(x) \approx f(x)
\end{equation*}
in the $L^2$ sense on the predictable zone $[r,1-r]$. Therefore, by establishing that
\begin{equation}\label{handwaving}
    \mathfrak{F}(\rho_j)(x)\approx T_\mathcal{K}(\rho_j)(x)
\end{equation}
in some appropriate sense, we would complete a substantial portion of our proof. However, as $\mathfrak{F}(\rho_j)$ is a graph signal and $T_\mathcal{K}(\rho_j)$ a graphon signal, to make sense of \eqref{handwaving}, we must extend the former to a graphon signal. We proceed with the necessary steps below.

We write our graph filter (see \eqref{frakF}, \eqref{graphker}) as
\begin{equation*}
    \begin{split}
        \mathfrak{F}(g_n)(x_k) &= \frac{1}{n\mathcal{W}_{x_k}}\sum_{l=1}^n\mathcal{G}^*(x_k,x_l)[{\bf A}_n]_{k,l}g_n(x_l)\\
        &= \frac{1}{n\mathcal{W}_{x_k}}\sum_{l\not=k, l=1}^n\mathcal{G}^*(x_k,x_l)W(x_k,x_l)g_n(x_l),   
    \end{split}
\end{equation*}
for a graph signal $g_n$. For each $j=0,\cdots, N-1$, we define a step graphon signal
\begin{equation}\label{def:extendFrho}
    \overline{\mathfrak{F}(\rho_j)}(x) := \sum_{k=1}^n \mathfrak{F}(\rho_j)(x_k)\chi_{I_k}(x) \quad\forall x\in [0,1],
\end{equation}
where, recall that $I_k= [(k-1)/n, k/n)$. Note that, $\overline{\mathfrak{F}(\rho_j)}(x_{k'})=\mathfrak{F}(\rho_j)(x_{k'})$ for any $k'=1,\cdots,n$, and 
\begin{equation}\label{GNNextend}
    \begin{split}
        \overline{\Psi}_{n,f}(x) &= \sum_{k=1}^{n} \Psi_{n,f}(x_k)\chi_{I_k}(x) = \sum_{k=1}^{n}\bigg(\sum_{j=0}^{2N-1}f(j/2N)\mathfrak{F}(\rho_j)(x_k)\bigg)\chi_{I_k}(x)\\
        &= \sum_{j=0}^{2N-1}f(j/2N)\overline{\mathfrak{F}(\rho_j)}(x)
    \end{split}
\end{equation}
by definition \eqref{def:barPsinf}. With that, we can now formalize \eqref{handwaving} as follows.

\begin{lemma} \label{lem:kerneldiff} Let $\alpha, \beta$ be as prescribed by Theorem~\ref{thm:GNNdet}. Then there exists $C>0$ such that, for each $k=1,\cdots,n$ and $j=0,\cdots, 2N-1$, we have
\begin{equation}\label{kerneldifferr}
    |T_\mathcal{K}\rho_j(x_k) - \overline{\mathfrak{F}(\rho_j)}(x_k)|\leq C\eta^{-1}(1+K)\bigg(\frac{N^{3\beta+1}}{n^2} + \frac{N^{3\beta-\alpha+1}}{n}\bigg).
\end{equation}
\end{lemma}

The proof of Lemma~\ref{lem:kerneldiff} is given in \S\ref{sec:kerneldiff}, at the end of this section, so as for us to proceed with the proof of Theorem~\ref{thm:GNNdet}.

\begin{proof}[Proof of Theorem~\ref{thm:GNNdet}] Without loss of generality, we assume $\|f\|_{L^2([0,1];\mathbb{C}^m)}=1$. Recall the following function \eqref{def:fsharp} from \S\ref{sec:WNNthm}:
\begin{equation*}
    f_{\sharp}(x) = \sum_{j=0}^{2N-1} f(j/2N)\rho_j(x), \quad\forall x\in [0,1].
\end{equation*} 
As previously demonstrated, $T_\mathcal{K}f_{\sharp}(x) =\sum_{j=0}^{2N-1} f(j/2N)T_\mathcal{K}\rho_j(x) = \Psi_f(x)$. Define
\begin{equation}\label{def:tildefsharp}
    \overline{\mathfrak{F}(f_{\sharp})}(x) := \sum_{j=0}^{2N-1} f(j/2N)\overline{\mathfrak{F}(\rho_j)}(x), \quad\forall x\in [0,1]. 
\end{equation} 
Then it's ready from \eqref{GNNextend} that $\overline{\mathfrak{F}(f_{\sharp})}=\overline{\Psi}_{n,f}$ and $\overline{\mathfrak{F}(f_{\sharp})}(x_k)=\overline{\Psi}_{n,f}(x_k)=\Psi_{n,f}(x_k)$, for every $k=1,\cdots,n$. 

Let us examine the difference $|T_\mathcal{K}f_{\sharp}(x_k) - \overline{\mathfrak{F}(f_{\sharp})}(x_k)| = |\Psi_f(x_k) - \Psi_{n,f}(x_k)|$. By definition, $T_\mathcal{K}\rho_j(x_k)=0=\mathfrak{F}(\rho_j)(x_k)$ if $|x_k-j/2N|>r$. Therefore, 
\begin{equation}\label{filtererror}
    \begin{split} 
        |\Psi_f(x_k) - \Psi_{n,f}(x_k)| &=|T_\mathcal{K}f_{\sharp}(x_k) - \overline{\mathfrak{F}(f_{\sharp})}(x_k)| \\
        &\leq \sum_{j=0}^{2N-1}|f(j/2N)||T_\mathcal{K}\rho_j(x_k) - \overline{\mathfrak{F}(\rho_j)}(x_k)| \\
        &\leq C\eta^{-1}(1+K)N^{1/2-\alpha/2}\bigg(\frac{N^{3\beta+1}}{n^2} + \frac{N^{3\beta-\alpha+1}}{n}\bigg),
    \end{split}
\end{equation}
where the last line follows from Lemmas~\ref{lem:tech} and \ref{lem:kerneldiff} and from the fact that there are only $\lceil rN\rceil$ indices $j$ partaking in \eqref{filtererror}. 

On the other hand, it can be readily assumed that 
\begin{equation}\label{1stthresholdforn}
    n>2N>N^{\alpha}. 
\end{equation}
Recall that $\alpha=0.96$ and we have selected $N\geq \lceil\varepsilon^{-10/9}\rceil$, this condition implies $n\geq\varepsilon^{-1}$. It also follows that, if $x\in [(k-1)/n,k/n)=I_k$ then $x\in \mathfrak{I}_{x_k}$, and moreover, there can only be one $j/2N$ in $(\mathfrak{J}_{x_k}\setminus\mathfrak{J}_x)\cup (\mathfrak{J}_{x}\setminus\mathfrak{J}_{x_k})$. Let $j=0,\cdots, 2N-1$ be such that $T_\mathcal{K}\rho_j(x_k)\not=0$ and $T_\mathcal{K}\rho_j(x)\not=0$, i.e., $j/2N\in \mathfrak{I}_{x_k}\cap \mathfrak{I}_x$. Then for almost every $x\in I_k$
\begin{align}
    \nonumber |T_\mathcal{K}\rho_j(x_k) - T_\mathcal{K}\rho_j(x)| &= \bigg| \int_0^1 \bigg(\frac{\mathcal{G}^*(x_k,y)W(x_k,y)}{\mathcal{W}_{x_k}} - \frac{\mathcal{G}^*(x,y)W(x,y)}{\mathcal{W}_{x}}\bigg)\rho_j(y)\,\mathrm{d}y\bigg|\\
    \nonumber &=\bigg| \int_{\mathfrak{J}_{x_k}\cap\mathfrak{J}_x} \bigg(\frac{\mathcal{G}^*(x_k,y)W(x_k,y)}{\mathcal{W}_{x_k}} - \frac{\mathcal{G}^*(x,y)W(x,y)}{\mathcal{W}_{x}}\bigg)\rho_j(y)\,\mathrm{d}y\bigg|\\
    \label{bulk} &\leq \frac{C\eta^{-2} KN^{3\beta-\alpha}}{n} + \frac{C\eta^{-1} N^{3\beta-\alpha+1}}{n} + \frac{C\eta^{-1}KN^{3\beta-\alpha}}{n},
\end{align}
since, by Assumption~\ref{assump:regular}, $|W(x_k,y)-W(x,y)|\leq K/n$ for every $y\in \mathfrak{I}_{x_k}\cap \mathfrak{I}_x$, and since
\begin{equation*}
    |\mathcal{G}^*(x_k,y)-\mathcal{G}^*(x,y)|\leq \frac{CN^{3\beta+1}}{n},
\end{equation*}
by Lemma~\ref{lem:Gsize}, and lastly, similar to what was done in \eqref{Lip}
\begin{equation*}
    \bigg| \frac{1}{\mathcal{W}_{x_k}} - \frac{1}{\mathcal{W}_{x_k}} \bigg| \leq \eta^{-2}\bigg(|\mathcal{W}_{x_k}-W(x_k,x)|+|W(x_k,x)-\mathcal{W}_x|\bigg)\leq \frac{2K\eta^{-2}}{n}.
\end{equation*}
If $j=0,\cdots, 2N-1$ is such that $T_\mathcal{K}\rho_j(x_k)=0$ and $T_\mathcal{K}\rho_j(x)\not=0$ or $T_\mathcal{K}\rho_j(x_k)\not=0$ and $T_\mathcal{K}\rho_j(x)=0$, then $j/2N\in [x_k+r,x+r]\cap [0,1]$ or $j/2N\in [x_k-r,x-r]\cap [0,1]$, respectively. In either scenario, we obtain
\begin{equation}\label{end}
    |T_\mathcal{K}\rho_j(x_k) - T_\mathcal{K}\rho_j(x)| \leq C\eta^{-1} N^{3\beta} \int_0^{1/n} y\,\mathrm{d}y\leq \frac{C\eta^{-1}N^{3\beta}}{n^2}.
\end{equation}
With \eqref{filtererror}, \eqref{bulk}, \eqref{end} at hand, we can now estimate $|T_\mathcal{K}f_{\sharp}-\overline{\mathfrak{F}({f}_{\sharp})}|=|\Psi_{f}-\overline{\Psi}_{n,f}|$ on $[r,1-r]$. Let $x_{k^*}$ denote the first $x_k\in [r,1-r]$. Then 
\begin{equation*}
    [r,x_{k^*})\subset [x_{k^*-1},x_{k^*})=\mathfrak{I}_{x_{k^*-1}} \quad\text{ and }\quad [x_{k^*},1-r]\subset\bigcup_{x_k\in [r,1-r]} [x_k, x_{k+1}).
\end{equation*}
Keeping this in mind, and combining \eqref{filtererror}, \eqref{bulk}, \eqref{end}, we deduce that for almost every $x\in [(k-1)/n,k/n)$, where either $x_{k}=x_{k^*-1}$ or $x_k\in [r,1-r]$
\begin{equation*}
    \begin{split}
        |T_\mathcal{K}f_{\sharp}(x)-\overline{\mathfrak{F}(f_{\sharp})}(x)| &\leq \sum_{j=0}^{2N-1}|f(j/2N)|\bigg(|T_\mathcal{K}\rho_j(x)-T_\mathcal{K}\rho_j(x_k)| + |T_\mathcal{K}\rho_j(x_k)-\overline{\mathfrak{F}(\rho_j)}(x_k)|\bigg)\\
        &\leq C\sum_{j=0}^{2N-1}|f(j/2N)|\bigg(\frac{\eta^{-2} (1+K)N^{3\beta-\alpha+1}}{n} + \frac{\eta^{-1}N^{3\beta}}{n^2} \bigg)\\
        &\leq C\eta^{-2}N^{1/2-\alpha/2}\bigg(\frac{(1+K)N^{3\beta-\alpha+1}}{n} + \frac{N^{3\beta}}{n^2} \bigg),
    \end{split}
\end{equation*}
since, by the arguments presented above, there are at most $\lceil rN\rceil+1$ indices $j$ participating in the sum. Therefore,
\begin{equation*}
    \|T_\mathcal{K}f_{\sharp}-\overline{\mathfrak{F}(f_{\sharp})}\|_{L^2([r,1-r];\mathbb{C}^m)} \leq C\eta^{-2}N^{1/2-\alpha/2}\bigg(\frac{(1+K)N^{3\beta-\alpha+1}}{n} + \frac{N^{3\beta}}{n^2} \bigg),
\end{equation*}
which together with Theorem~\ref{thm:WNN}, yields
\begin{equation}\label{nearfinGNNdet}
    \begin{split}
    \|\overline{\Psi}_{n,f}-f\|_{L^2([r,1-r];\mathbb{C}^m)} &\leq \|\overline{\mathfrak{F}(f_{\sharp})}-T_\mathcal{K}f_{\sharp}\|_{L^2([r,1-r];\mathbb{C}^m)} + \|T_\mathcal{K}f_{\sharp} - f\|_{L^2([r,1-r];\mathbb{C}^m)}\\
    &\leq C\eta^{-2}N^{1/2-\alpha/2}\bigg(\frac{(1+K)N^{3\beta-\alpha+1}}{n} + \frac{N^{3\beta}}{n^2} \bigg) + C\eta^{-1}K\varepsilon.
    \end{split}
\end{equation}
Since $\beta=0.98$, we only need to impose
\begin{equation}\label{powern}
    n\geq \frac{N^{3\beta-3\alpha/2+3/2}}{\varepsilon}\geq \varepsilon^{-13/3}.
\end{equation}
Note that \eqref{powern} implies \eqref{1stthresholdforn}. Putting \eqref{powern} back in \eqref{nearfinGNNdet}, we obtain
\begin{equation*}
    \|\overline{\Psi}_{n,f}-f\|_{L^2([r,1-r];\mathbb{C}^m)} \leq C\eta^{-2}(1+K)\varepsilon
\end{equation*}
thereby concluding the proof.
\end{proof} % end of GNNdet proof

\begin{remark}\label{rem:optimal_n}
A quick calculation shows that the smallest possible power of $\varepsilon$ in \eqref{powern} that could be achieved for $n$ is $4$. Indeed, let $\Delta=\beta-\alpha$. Then $3\beta-3\alpha/2+3/2=3\alpha/2+3\Delta+3/2$. Getting the power $\delta$ in $N^{-\delta}\sim\varepsilon$ as close to $1$ as possible requires $\alpha$ to be also close to $1$, and $\Delta$ close to $0$, giving the smallest possible value of $(3\beta-3\alpha/2+3/2)/\delta$ to be $3$. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proof of Lemma~\ref{lem:kerneldiff}} \label{sec:kerneldiff}

Fix one $x_k$, and let $j=0,\cdots N-1$. Since $|x_k-j/2N|>r$ implies $T_\mathcal{K}\rho_j(x_k)=0=\mathfrak{F}\rho_j(x_k)$. Therefore, we only consider those $j$ where $|x_k-j/2N|\leq r$. Take one such $j$. Write
\begin{equation}\label{splitdiff}
    T_\mathcal{K}\rho_j - \overline{\mathfrak{F}(\rho_j)} = T_\mathcal{K}\rho_j - T_\mathcal{K}\overline{\rho}_j + T_\mathcal{K}\overline{\rho}_j - \overline{\mathfrak{F}(\rho_j)}
\end{equation}
where
\begin{equation*}
    \overline{\rho}_j(x) :=\sum_{l=1}^n \rho_j(x_l)\chi_{I_l}(x), \quad\forall x\in [0,1].
\end{equation*}
The difference $T_\mathcal{K}\rho_j - T_\mathcal{K}\overline{\rho}_j$ is straightforward to address. In fact, from Lemma~\ref{lem:Gsize} we can manage
\begin{equation}\label{1stdiff}
    \begin{split}
    \|T_\mathcal{K}(\rho_j-\overline{\rho}_j)\|_{L^{\infty}([0,1])} 
    &\leq \eta^{-1}\|\mathcal{G}^*\|_{L^{\infty}([0,1])}\|\rho_j-\overline{\rho}_j\|_{L^1([0,1])}\\ 
    &\leq \frac{C\eta^{-1}\max\{N^2, N^{3\beta}\}}{n} \\
    &\leq \frac{C\eta^{-1}N^{3\beta}}{n},
    \end{split}
\end{equation}
since $\beta=0.98$. The second to last bound above is due to
\begin{equation*}
    \|\rho_j-\overline{\rho}_j\|_{L^1([0,1])} \leq \sum_{k=1}^n \frac{1}{n^2} = \frac{1}{n},
\end{equation*}
as the functions $\rho_j$ are ReLU functions, the maximum difference $|\rho_j(x) - \rho_j(x_k)| \leq 1/n$ for every $x$ belonging to the interval $I_k$ of length $1/n$.

The second difference $T_\mathcal{K}\overline{\rho}_j - \overline{\mathfrak{F}(\rho_j)}$ on the right-hand-side of \eqref{splitdiff} requires a more delicate treatment. Using the definition \eqref{def:extendFrho} introduced at the beginning of this section, we get
\begin{multline} \label{2nddiff}
    |T_\mathcal{K}\overline{\rho}_j(x_k) - \overline{\mathfrak{F}(\rho_j)}(x_k)| \\
    \leq \frac{1}{\mathcal{W}_{x_k}}\sum_{l=1}^n \bigg|\int_{(l-1)/n}^{l/n} \{\mathcal{G}^*(x_k,y)W(x_k,y)-\mathcal{G}^*(x_k,x_l)[{\bf A}_n]_{k,l}\}\rho_j(x_l)\,\mathrm{d}y\bigg|.
\end{multline}
Observe that at most $\lceil 2rn\rceil$ indices $l$ participate in the sum above; namely those that satisfy
\begin{equation}\label{relevantind}
    I_l=\bigg[\frac{l-1}{n}, \frac{l}{n}\bigg) \cap \mathfrak{I}_{x_k} \not= \emptyset.
\end{equation}
Among all the indices that satisfy \eqref{relevantind}, there will be at most two ``boundary" $l$ such that
\begin{equation}\label{boundaryl}
    I_l = \bigg[\frac{l-1}{n}, \frac{l}{n}\bigg) \not\subset \mathfrak{I}_{x_k},
\end{equation}
on at most one of which,
\begin{equation}\label{leftboundary1}
    \mathcal{G}^*(x_k,x_l) = 0.
\end{equation}
Let $l^*$ denote this index and $l_*$ denote the other index that satisfies \eqref{boundaryl}. Let $S$ be the set of the remaining, ``interior", indices, i.e., those $l$ that satisfy \eqref{relevantind} but not \eqref{boundaryl}. Among these include $l=k$, in which case it follows directly from definition that
\begin{equation}\label{leftboundary2}
    [{\bf A}_n]_{k,l} = [{\bf A}_n]_{k,k} = 0.
\end{equation}
Consider
\begin{equation*} 
    \begin{split}
        D_1 &:=\frac{1}{\mathcal{W}_{x_k}} \bigg|\int_{(l^*-1)/n}^{l^*/n} (\mathcal{G}^*(x_k,y)W(x_k,y)-\mathcal{G}^*(x_k,x_{l^*})[{\bf A}_n]_{k,l^*})\rho_j(x_{l^*})\,\mathrm{d}y\bigg|\\
        &= \frac{1}{\mathcal{W}_{x_k}} \bigg|\int_{(l^*-1)/n}^{l^*/n} \mathcal{G}^*(x_k,y)W(x_k,y)\rho_j(x_{l^*})\,\mathrm{d}y\bigg|,
    \end{split}
\end{equation*}
due to \eqref{leftboundary1}. We find, as a consequence of Lemma~\ref{lem:Gsize}, that
\begin{equation}\label{D1bd}
    D_1 \leq \frac{1}{\mathcal{W}_{x_k}}\int_{(l^*-1)/n}^{l^*/n} |\mathcal{G}^*(x_k,y)W(x_k,y)||\rho_j(x_{l^*})|\,\mathrm{d}y \leq \frac{C\eta^{-1}N^{3\beta}}{n}.
\end{equation}
Likewise, by virtue of \eqref{leftboundary2},
\begin{equation}\label{def:D2bd}
    \begin{split}
        D_2 &:=\frac{1}{\mathcal{W}_{x_k}} \bigg|\int_{(k-1)/n}^{k/n} (\mathcal{G}^*(x_k,y)W(x_k,y)-\mathcal{G}^*(x_k,x_k)[{\bf A}_n]_{k,k})\rho_j(x_k)\,\mathrm{d}y\bigg|\\
        &= \frac{1}{\mathcal{W}_{x_k}} \bigg|\int_{(k-1)/n}^{k/n} \mathcal{G}^*(x_k,y)W(x_k,y)\rho_j(x_k)\,\mathrm{d}y\bigg|\\
        &\leq \frac{C\eta^{-1}N^{3\beta}}{n}.
    \end{split}
\end{equation}
Next, let
\begin{equation}\label{def:D3}
    D_3:=\frac{1}{\mathcal{W}_{x_k}}\sum_{l\not=k, l\in S} \bigg|\int_{(l-1)/n}^{l/n} (\mathcal{G}^*(x_k,y)W(x_k,y)-\mathcal{G}^*(x_k,x_l)[{\bf A}_n]_{k,l})\rho_j(x_l)\,\mathrm{d}y\bigg|.
\end{equation}
We estimate, on one of these intervals $I_l$, that
\begin{equation}\label{splitcal}
    \begin{split}
        &|\mathcal{G}^*(x_k,x_l)[{\bf A}_n]_{k,l}-\mathcal{G}^*(x_k,y)W(x_k,y)| \\
        &\leq |\mathcal{G}^*(x_k,x_l)-\mathcal{G}^*(x_k,y)| + |[{\bf A}_n]_{k,l}-W(x_k,y)||\mathcal{G}^*(x_k,y)| \\
        &= |\mathcal{G}^*(x_k,x_l)-\mathcal{G}^*(x_k,y)| + |W(x_k,_l)-W(x_k,y)||\mathcal{G}^*(x_k,y)| \leq \frac{CN^{3\beta+1}}{n} + \frac{CKN^{3\beta}}{n}
    \end{split}
\end{equation}
where the last inequality follows from Lemma~\ref{lem:Gsize} and Assumption~\ref{assump:regular} on $W$. By plugging this bound back in \eqref{def:D3}, we gain
\begin{equation}\label{D3finbd}
    D_3 \leq C\eta^{-1}(1+K)\bigg(\sum_{l\not=k, l\in S}\frac{1}{n}\bigg(\frac{N^{3\beta+1}}{n}\bigg)\bigg) \leq C\eta^{-1}(1+K)\bigg(\frac{N^{3\beta -\alpha+1}}{n}\bigg),
\end{equation}
since $\texttt{\#} S\leq\lceil 2rn \rceil$. Finally, let
\begin{equation*}
    D_4:=\frac{1}{\mathcal{W}_{x_k}}\bigg|\int_{(l_*-1)/n}^{l_*/n} (\mathcal{G}^*(x_k,y)W(x_k,y)-\mathcal{G}^*(x_k,x_{l_*})[{\bf A}_n]_{k,l_*})\rho_j(x_{l_*})\,\mathrm{d}y\bigg|.
\end{equation*}
Then following a similar calculation as in \eqref{splitcal}, we derive
\begin{equation*}
    |\mathcal{G}^*(x_k,x_{l_*})[{\bf A}_n]_{k,l_*}-\mathcal{G}^*(x_k,y)W(x_k,y)| \leq \frac{CN^{3\beta+1}}{n} + \frac{CKN^{3\beta}}{n}
\end{equation*}
and so
\begin{equation}\label{D4bd}
    D_4\leq C\eta^{-1}(1+K)\bigg(\frac{N^{3\beta+1}}{n^2}\bigg).
\end{equation}
Taking into account \eqref{splitdiff}, \eqref{1stdiff}, \eqref{2nddiff}, \eqref{D1bd}, \eqref{def:D2bd}, \eqref{D3finbd}, and \eqref{D4bd}, we arrive at \eqref{kerneldifferr}, as desired. \qed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proof of Theorem~\ref{thm:GNNran}} \label{sec:GNNran}

Throughout this section we continue to assume \eqref{rkappa} holds. We continue to work with the shorthand abbreviations established in the previous sections, which are
\begin{equation*}
    \mathcal{G}^*(x,y)=\frac{\mathrm{d}^2}{\mathrm{d} y^2}(s_N\,\mathcal{G}_{r,\sigma})(x-y) \quad\text{ and }\quad \rho_j(x) = ReLU_{j/2N}(x).
\end{equation*}
The proof will be a straightforward application of the bounded-difference inequality and Theorem~\ref{thm:WNN}. As usual, we begin with some preliminary arrangements. 

First, recall that in the case of a simple random graph $G^{\mathrm{ran}}_n$ generated from a graphon $W$, the event of two vertices $v_k, v_l$ being connected by an edge is a random variable 
\begin{equation}\label{Ber}
    \xi_{k,l}\sim\mathrm{Bernoulli}(W(x_k, x_l)), \quad k > l.
\end{equation}
Since $W(x_k, x_l)=W(x_l,x_k)$, we have the undirected symmetry $\xi_{lk} = \xi_{k,l}$ of the graph; nevertheless, each of the sets
\begin{equation*}
    \{\xi_{k,l}: k,l=1,\cdots,n \text{ s.t } k>l\} \quad\text{ and }\quad \{\xi_{k,l}: k,l=1,\cdots,n \text{ s.t } k<l\}
\end{equation*}
is an independent set of random variables. A realization of the random graph $G^{\mathrm{ran}}_n$ is then a simple graph whose associated adjacency matrix ${\bf A}_n$ satisfies
\begin{equation*}
    [{\bf A}_n]_{k,l}\in\{0,1\}.
\end{equation*}
Using this matrix, the graph filter $\mathfrak{F}$'s application on a graph signal $g_n$ has the value
\begin{equation*}
    \mathfrak{F}(g_n)(x_k) = \frac{1}{n\mathcal{W}_{x_k}}\sum_{ l=1}^n\mathcal{G}^*(x_k,x_l)[{\bf A}_n]_{k,l}g_n(x_l).
\end{equation*}
Until $\xi_{k,l}$ are all realized, the quantity
\begin{equation}\label{ranfrakF}
    \mathfrak{F}^{\mathrm{ran}}(g_n)(x_k) := \frac{1}{n\mathcal{W}_{x_k}}\sum_{l\not=k, l=1}^n\mathcal{G}^*(x_k,x_l)\xi_{k,l} g_n(x_l)
\end{equation}
is a random variable, for each $k=1,\cdots,n$, as indicated by the superscript `$\mathrm{ran}$'. Fix a $k$ and $j=0,\cdots, 2N-1$. Set $g_n = \rho_j$ in \eqref{ranfrakF}, suppressing the dependence on $j$ for simplicity. We define the random variables
\begin{align*}
    Y_{\mathcal{L}_k} &:= \frac{1}{n\mathcal{W}_{x_k}}\sum_{l\in\mathcal{L}_k}\mathcal{G}^*(x_k,x_l)\xi_{k,l}\rho_j(x_l)\\
    Y_{\mathcal{U}_k} &:= \frac{1}{n\mathcal{W}_{x_k}}\sum_{l\in\mathcal{U}_k}\mathcal{G}^*(x_k,x_l)\xi_{k,l}\rho_j(x_l),
\end{align*}
where 
\begin{align*}
    \mathcal{L}_k &:=\{l<k: \mathcal{G}^*(x_k,x_l)>0, \rho_j(x_l)>0\}\\
    \mathcal{U}_k &:=\{l>k: \mathcal{G}^*(x_k,x_l)>0, \rho_j(x_l)>0\};
\end{align*}
note that 
\begin{equation}\label{size}
    \texttt{\#}\mathcal{L}_k, \texttt{\#}\mathcal{U}_k \leq \lceil nr\rceil. 
\end{equation}
We first focus on $Y_{\mathcal{L}_k}$. Observe from \eqref{Ber} that
\begin{equation*} 
    \mathbb{E}[Y_{\mathcal{L}_k}] = \frac{1}{n\mathcal{W}_{x_k}}\sum_{l\in\mathcal{L}_k}\mathcal{G}^*(x_k,x_l)W(x_k,x_l)\rho_j(x_l).
\end{equation*}
Moreover, $Y_{\mathcal{L}_k}$ has the form of a function $g_{\mathcal{L}_k}$ of $L_k := \texttt{\#}\mathcal{L}_k$ one-dimensional variables in $[0,1]$, applied to $\{\xi_{k,l}: l\in\mathcal{L}_k\}$, where
\begin{equation*}
    g_{\mathcal{L}_k}((z_l)_{l\in\mathcal{L}_k}):= \frac{1}{n\mathcal{W}_{x_k}}\sum_{l\in\mathcal{L}_k}\mathcal{G}^*(x_k,x_l) \rho_j(x_l)z_l.
\end{equation*}
Such a function $g_{\mathcal{L}_k}$ is of bounded difference. Indeed, let $z, z'$ be two $L_k$-tuples that differ at only one coordinate entry. It's due to \eqref{rkappa} that $\mathcal{W}_{x_k}\geq\eta$, and Lemma~\ref{lem:Gsize} further provides
\begin{equation}\label{gL}
    |g_{\mathcal{L}_k}(z)-g_{\mathcal{L}_k}(z')|\leq \frac{C\eta^{-1}N^{3\beta}}{n}.
\end{equation}
As mentioned earlier, the central element of our proof is a bounded-difference inequality, presented as follows.

\begin{theorem} (adapted from \cite[Theorem 2.9.1]{vershynin2018high}) \label{thm:vershynin} Let $X_1, \cdots, X_L$ be independent random variables. Let $g:\mathbb{R}^L\to\mathbb{R}$. Assume that the value of $g(x)$ can be changed by at most $c_j>0$ under an arbitrary change of a single coordinate of $x\in\mathbb{R}^L$. Then for any $\delta>0$, we have
\begin{equation*}
    \mathbb{P}(|g(X)-\mathbb{E}[g(X)]|>\delta)\leq 2\exp\bigg(-\frac{2\delta^2}{\sum_{j=1}^L c_j^2}\bigg)
\end{equation*}
where $X=(X_1,\cdots,X_L)$.
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:GNNran}]
Applying Theorem~\ref{thm:vershynin} to $Y_{\mathcal{L}_k}$, and using \eqref{size}, \eqref{gL}, we acquire
\begin{equation*}
    \mathbb{P}(|Y_{\mathcal{L}_k}-\mathbb{E}[Y_{\mathcal{L}_k}]|>\varepsilon)\leq 2\exp\bigg(-\frac{C\varepsilon^2\eta^2 n^2}{N^{6\beta}\lceil nr\rceil}\bigg)\leq 2\exp\bigg(-\frac{C\varepsilon^2\eta^2 n}{N^{6\beta-\alpha}}\bigg).
\end{equation*}
A similar argument in the case of $\mathcal{U}_k$ would also result in
\begin{equation*}
    \mathbb{P}(|Y_{\mathcal{U}_k}-\mathbb{E}[Y_{\mathcal{U}_k}]|>\varepsilon)\leq 2\exp\bigg(-\frac{C\varepsilon^2\eta^2 n}{N^{6\beta-\alpha}}\bigg).
\end{equation*}
Since $Y_{\mathcal{L}_k}+Y_{\mathcal{U}_k}=\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)$, we conclude that
\begin{equation}\label{basicReLU}
    \mathbb{P}(|\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)-\mathbb{E}[\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)]|>\varepsilon)\leq 2\exp\bigg(-\frac{C\varepsilon^2\eta^2 n}{N^{6\beta-\alpha}}\bigg),
\end{equation}
for every $x_k\in \mathcal{X}_n$ and each fixed $j = 0,\dots,2N - 1$. 

Now, allowing $j$ to vary gives
\begin{align}
    \nonumber &\mathbb{P}\bigg(\bigg|\sum_{j=0}^{2N-1}f(j/2N)\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)-\sum_{j=0}^{2N-1}f(j/2N)\mathbb{E}[\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)]\bigg|>\varepsilon\|f\|_{L^2([0,1];\mathbb{C}^m)}\bigg)\\
    \nonumber \leq &\mathbb{P}\bigg(\sum_{j=0}^{2N-1} |f(j/2N)||\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)-\mathbb{E}[\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)]|>\varepsilon\|f\|_{L^2([0,1];\mathbb{C}^m)}\bigg)\\
    \nonumber \leq &\mathbb{P}\bigg(\bigg(\sum_{j=0}^{2N-1} |\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)-\mathbb{E}[\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)]|^2\bigg)^{1/2}>\varepsilon\bigg)\\
    \nonumber \leq &\sum_{j=0}^{2N-1} \mathbb{P}\bigg(|\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)-\mathbb{E}[\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)]|>\varepsilon (rN)^{-1/2}\bigg)\\
    \label{chainofprobs} \leq & 2N^{1-\alpha}\exp\bigg(-\frac{C\varepsilon^2\eta^2 n}{N^{6\beta-2\alpha+1}}\bigg).
\end{align}
The number of indices $j$ taking part in these sums above is again at most $\lceil rN\rceil$. Therefore, the second to last inequality follows from the $L^p$-embedding for finite measures and the last from \eqref{basicReLU}. Note that, by design
\begin{equation*}
    \sum_{j=0}^{2N-1}f(j/2N)\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)
\end{equation*}
is the GNN network $\Psi_{n,f}$ applying to the random graph $G^{\mathrm{ran}}_n$ (see \eqref{GNNoutput}). In this case
\begin{equation*}
    \overline{\Psi}_{n,f}(x) = \sum_{k=1}^n \bigg(\sum_{j=0}^{2N-1}f(j/2N)\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)\bigg)\chi_{I_k}(x),
\end{equation*}
and that from \eqref{def:extendFrho}, \eqref{def:tildefsharp}
\begin{equation*}
    \begin{split}
        \mathbb{E}[\overline{\Psi}_{n,f}(x)] &= 
         \sum_{j=0}^{2N-1} f(j/2N) \bigg(\sum_{k=1}^n \mathbb{E}[\mathfrak{F}^{\mathrm{ran}}(\rho_j)(x_k)]\chi_{I_k}(x)\bigg) \\
        &= \sum_{j=0}^{2N-1} \frac{f(j/2N)}{n\mathcal{W}_{x_k}}\sum_{k=1}^{n} \bigg(\sum_{l\not=k,l=1}^n \mathcal{G}^*(x_k,x_l)W(x_k,x_l)\rho_j(x_l)\bigg)\chi_{I_k}(x) \\
        &= \sum_{j=0}^{2N-1}f(j/2N)\overline{\mathfrak{F}(\rho_j)}(x) = \overline{\mathfrak{F}(f_{\sharp})}(x).
    \end{split} 
\end{equation*}
Therefore, based on \eqref{chainofprobs}, we deduce that for every $x\in [r,1-r]$,
\begin{equation}\label{probconsequence}
    |\overline{\Psi}_{n,f}(x)-\overline{\mathfrak{F}(f_{\sharp})}(x)| \leq \varepsilon\|f\|_{L^2([0,1];\mathbb{C}^m)}
\end{equation}
with probability at least $1-2nN^{1-\alpha}\exp(-\frac{C\varepsilon^2\eta^2 n}{N^{6\beta-2\alpha+1}})$. On the other hand, we obtain from the proof of Theorem~\ref{thm:GNNdet} that for almost every $x\in [r,1-r]$
\begin{equation}\label{WNNrecall}
    \bigg|\overline{\mathfrak{F}(f_{\sharp})}(x)-T_\mathcal{K}f_{\sharp}(x)\bigg| \leq C\eta^{-2}(1+K)\,\varepsilon\,\|f\|_{L^2([0,1];\mathbb{C}^m)},
\end{equation}
whenever $n\geq \lceil\varepsilon^{-13/3}\rceil$. By combining \eqref{probconsequence}, \eqref{WNNrecall}, and recalling that $N=\lceil\varepsilon^{-10/9}\rceil$, we infer that
\begin{equation*}
    |\Psi_{n,f}(x_k)-f(x_k)|\leq C\eta^{-2}(1+K)\,\varepsilon\,\|f\|_{L^2([0,1];\mathbb{C}^m)}
\end{equation*}
holds for all $x_k\in [r,1-r]$, with a probability at least
\begin{equation*}
    1 - 2n\varepsilon^{10(1-\alpha)/9}\exp\bigg(-\frac{C\eta^2 n}{\varepsilon^{4(15\beta-5\alpha-2)/9}}\bigg),
\end{equation*}
where $\alpha=0.96$, $\beta=0.98$, so as long as $n\geq \varepsilon^{-13/3}$, leading us to the conclusion of the proof.  
\end{proof} % end of random GNN proof

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}\label{sec:Discussion}

In this work we have provided powerful novel results regarding the generalizability of WNNs and the transferability of GNNs. Our results were enabled first through the sampling routine in Theorem~\ref{thm:TsamplingregwG}, which was used to inform our network architecture for both WNNs and GNNs. In the case of WNNs we provide bounds on the number of samples required to achieve the approximation of graphon signals to arbitrary accuracy using our proposed WNN architecture. These results were then leveraged to provide transferability results for GNNs across graphs belonging to the same graphon family. This included both deterministic weighted graphs and simple random graphs. Together these results constitute a significant advancement in the budding theory of graphons applied to neural networks and the expressive capabilities of WNNs. 

We emphasize that these results mark only an initial step towards a unified theory of graphons and their application to GNNs. Moving forward we wish to expand the class of graphons for which our results apply beyond those that satisfy Assumption~\ref{assump:regular}. For example, popular graphs/graphons in application are stochastic block models \cite{airoldi2013stochastic,sischka2022stochastic} which do not satisfy our assumption. However, one may subdivide the domain of a stochastic block model according to each of the blocks, in which case our results can be applied piecewise to the distinct blocks. Hence, it is important that a follow-up work generalize our results to as wide a class of graphons as possible to provide the greatest applicability of our results. One direction in which such a generalization could be achieved is by replacing the graphon domain $[0,1]^2$ with $\Omega^2$, for an arbitrary compact set $\Omega$. In the case of a stochastic block model $\Omega$ would take the form of a union over the finitely-many (compact) blocks. A presentation of graphons with arbitrary $\Omega$ is done in \cite{Janson} where much of the theory laid out in \S\ref{sec:graphons} can still be applied, but the presentation becomes more delicate at points. Therefore, such a generalization marks an immediate point for a follow-up investigation of our work herein.    

Given that our work describes an explicit network architecture, we have therefore provided a simple GNN/WNN architecture that can be readily applied in experiments. As with all statements in mathematical analysis, our results are given in the worse case scenario and so it remains to benchmark our network's performance on real-world data. In particular, it would be interesting to compare the performance of our network architecture against some of the other architectures for both WNNs and GNNs that have been provided throughout the literature. Such experiments may provide key insights into better architectures and tighter bounds than those which are reported here. 

As a final aside, we note that graphons can only describe {\em dense} graphs. To fully generalize our results to accommodate a more diverse range of graph structures one should hope to be able to apply our results to {\em sparse} graphs as well. There is theory for capturing sparse graph structures using graphons \cite{klopp2017oracle,borgs2017sparse,lunde2023subsampling,fabian2023learning} which could form the basis for an extension of our work in this paper. We expect such an extension to be highly nontrivial and leave it for future work.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}

JJB was supported by an NSERC Discovery grant. 



%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{abbrv}
%\bibliography{samplingGNN.bib}

\begin{thebibliography}{10}

\bibitem{airoldi2013stochastic}
Edo~M Airoldi, Thiago~B Costa, and Stanley~H Chan.
\newblock Stochastic blockmodel approximation of a graphon: Theory and
  consistent estimation.
\newblock {\em Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem{ala2020improving}
Maher Alaraj, Munir Majdalawieh, and Maysam~F Abbod.
\newblock Improving binary classification using filtering based on k-nn
  proximity graphs.
\newblock {\em Journal of Big Data}, 7(1):1--18, 2020.

\bibitem{apostol1998introduction}
Tom~M Apostol.
\newblock {\em Introduction to analytic number theory}.
\newblock Springer Science \& Business Media, 1998.

\bibitem{benedetto2001modern}
John~J Benedetto and Paulo~JSG Ferreira.
\newblock {\em Modern sampling theory: mathematics and applications}.
\newblock Springer Science \& Business Media, 2001.

\bibitem{bhagavatula2018content}
Chandra Bhagavatula, Sergey Feldman, Russell Power, and Waleed Ammar.
\newblock Content-based citation recommendation.
\newblock In {\em Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 238--251, 2018.

\bibitem{borgs2017sparse}
Christian Borgs, Jennifer~T Chayes, Henry Cohn, and Nina Holden.
\newblock Sparse exchangeable graphs and their limits via graphon processes.
\newblock {\em The Journal of Machine Learning Research}, 18(1):7740--7810,
  2017.

\bibitem{boucheron2003concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Olivier Bousquet.
\newblock Concentration inequalities.
\newblock In {\em Summer school on machine learning}, pages 208--240. Springer,
  2003.

\bibitem{boyd1959inequalities}
AV~Boyd.
\newblock Inequalities for mills ratio.
\newblock {\em Rep. Statist. Appl. Res. Un. Japan. Sci. Engrs}, 6:44--46, 1959.

\bibitem{bramburger2023pattern}
Jason Bramburger and Matt Holzer.
\newblock Pattern formation in random networks using graphons.
\newblock {\em SIAM Journal on Mathematical Analysis}, 55(3):2150--2185, 2023.

\bibitem{bronstein2021geometric}
Michael~M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli{\v{c}}kovi{\'c}.
\newblock Geometric deep learning: Grids, groups, graphs, geodesics, and
  gauges.
\newblock {\em arXiv preprint arXiv:2104.13478}, 2021.

\bibitem{calder2018game}
Jeff Calder.
\newblock The game theoretic p-laplacian and semi-supervised learning with few
  labels.
\newblock {\em Nonlinearity}, 32(1):301, 2018.

\bibitem{carneiro2013bandlimited}
Emanuel Carneiro and Friedrich Littmann.
\newblock Bandlimited approximations to the truncated gaussian and
  applications.
\newblock {\em Constructive Approximation}, 38:19--57, 2013.

\bibitem{cervino2023learning}
Juan Cervino, Luana Ruiz, and Alejandro Ribeiro.
\newblock Learning by transference: Training graph neural networks on growing
  graphs.
\newblock {\em IEEE Transactions on Signal Processing}, 71:233--247, 2023.

\bibitem{chen2019note}
Liang Chen and Congwei Wu.
\newblock A note on the expressive power of deep rectified linear unit networks
  in high-dimensional spaces.
\newblock {\em Mathematical Methods in the Applied Sciences}, 42(9):3400--3404,
  2019.

\bibitem{cummings2020structured}
Daniel Cummings and Marcel Nassar.
\newblock Structured citation trend prediction using graph neural networks.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3897--3901. IEEE, 2020.

\bibitem{dummit2004abstract}
David~Steven Dummit and Richard~M Foote.
\newblock {\em Abstract algebra}, volume~3.
\newblock Wiley Hoboken, 2004.

\bibitem{dziedzic2019band}
Adam Dziedzic, John Paparrizos, Sanjay Krishnan, Aaron Elmore, and Michael
  Franklin.
\newblock Band-limited training and inference for convolutional neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1745--1754. PMLR, 2019.

\bibitem{evans1998partial}
Lawrence~C Evans.
\newblock Partial differential equations.
\newblock {\em Graduate studies in mathematics}, 19(4):7, 1998.

\bibitem{fabian2023learning}
Christian Fabian, Kai Cui, and Heinz Koeppl.
\newblock Learning sparse graphon mean field games.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4486--4514. PMLR, 2023.

\bibitem{folland1999real}
Gerald~B Folland.
\newblock {\em Real analysis: modern techniques and their applications},
  volume~40.
\newblock John Wiley \& Sons, 1999.

\bibitem{fung2021benchmarking}
Victor Fung, Jiaxin Zhang, Eric Juarez, and Bobby~G Sumpter.
\newblock Benchmarking graph neural networks for materials chemistry.
\newblock {\em npj Computational Materials}, 7(1):1--8, 2021.

\bibitem{gao2022graph}
Chen Gao, Xiang Wang, Xiangnan He, and Yong Li.
\newblock Graph neural networks for recommender system.
\newblock In {\em Proceedings of the Fifteenth ACM International Conference on
  Web Search and Data Mining}, pages 1623--1625, 2022.

\bibitem{gao2023survey}
Chen Gao, Yu~Zheng, Nian Li, Yinfeng Li, Yingrong Qin, Jinghua Piao, Yuhan
  Quan, Jianxin Chang, Depeng Jin, Xiangnan He, et~al.
\newblock A survey of graph neural networks for recommender systems:
  challenges, methods, and directions.
\newblock {\em ACM Transactions on Recommender Systems}, 1(1):1--51, 2023.

\bibitem{gilmer2017neural}
Justin Gilmer, Samuel~S Schoenholz, Patrick~F Riley, Oriol Vinyals, and
  George~E Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In {\em International conference on machine learning}, pages
  1263--1272. PMLR, 2017.

\bibitem{glasscock2015graphon}
Daniel Glasscock.
\newblock What is... a graphon.
\newblock {\em Notices of the AMS}, 62(1):46--48, 2015.

\bibitem{hamilton2017inductive}
Will Hamilton, Zhitao Ying, and Jure Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{hu2021training}
Ziqing Hu, Yihao Fang, and Lizhen Lin.
\newblock Training graph neural networks by graphon estimation.
\newblock In {\em 2021 IEEE International Conference on Big Data (Big Data)},
  pages 5153--5162. IEEE, 2021.

\bibitem{huang2021mixgcf}
Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang,
  and Jie Tang.
\newblock Mixgcf: An improved training method for graph neural network-based
  recommender systems.
\newblock In {\em Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 665--674, 2021.

\bibitem{Janson}
Svante Janson.
\newblock {\em Graphons, cut norm and distance, couplings and rearrangements},
  volume~4 of {\em New York Journal of Mathematics. NYJM Monographs}.
\newblock State University of New York, University at Albany, Albany, NY, 2013.

\bibitem{jiang2021could}
Dejun Jiang, Zhenxing Wu, Chang-Yu Hsieh, Guangyong Chen, Ben Liao, Zhe Wang,
  Chao Shen, Dongsheng Cao, Jian Wu, and Tingjun Hou.
\newblock Could graph neural networks learn better molecular representation for
  drug discovery? a comparison study of descriptor-based and graph-based
  models.
\newblock {\em Journal of cheminformatics}, 13(1):1--23, 2021.

\bibitem{keriven2020convergence}
Nicolas Keriven, Alberto Bietti, and Samuel Vaiter.
\newblock Convergence and stability of graph convolutional networks on large
  random graphs.
\newblock {\em Advances in Neural Information Processing Systems},
  33:21512--21523, 2020.

\bibitem{klopp2017oracle}
Olga Klopp, Alexandre~B Tsybakov, and Nicolas Verzelen.
\newblock Oracle inequalities for network models and sparse graphon estimation.
\newblock {\em Annals of Statistics}, 45(1):316--354, 2017.

\bibitem{kluvanek1965sampling}
Igor Kluv{\'a}nek.
\newblock Sampling theorem in abstract harmonic analysis.
\newblock {\em Matematicko-fyzik{\'a}lny {\v{c}}asopis}, 15(1):43--48, 1965.

\bibitem{knopp1956infinite}
Konrad Knopp.
\newblock {\em Infinite sequences and series}.
\newblock Courier Corporation, 1956.

\bibitem{lovasz2012large}
L{\'a}szl{\'o} Lov{\'a}sz.
\newblock {\em Large networks and graph limits}, volume~60.
\newblock American Mathematical Soc., 2012.

\bibitem{lovasz2006limits}
L{\'a}szl{\'o} Lov{\'a}sz and Bal{\'a}zs Szegedy.
\newblock Limits of dense graph sequences.
\newblock {\em Journal of Combinatorial Theory, Series B}, 96(6):933--957,
  2006.

\bibitem{lunde2023subsampling}
Robert Lunde and Purnamrita Sarkar.
\newblock Subsampling sparse graphons under minimal assumptions.
\newblock {\em Biometrika}, 110(1):15--32, 2023.

\bibitem{ma2021deep}
Yao Ma and Jiliang Tang.
\newblock {\em Deep learning on graphs}.
\newblock Cambridge University Press, 2021.

\bibitem{maskey2023transferability}
Sohir Maskey, Ron Levie, and Gitta Kutyniok.
\newblock Transferability of graph neural networks: an extended graphon
  approach.
\newblock {\em Applied and Computational Harmonic Analysis}, 63:48--83, 2023.

\bibitem{micheli2009neural}
Alessio Micheli.
\newblock Neural network for graphs: A contextual constructive approach.
\newblock {\em IEEE Transactions on Neural Networks}, 20(3):498--511, 2009.

\bibitem{montanelli2021deep}
Hadrien Montanelli, Haizhao Yang, and Qiang Du.
\newblock Deep relu networks overcome the curse of dimensionality for
  generalized bandlimited functions.
\newblock {\em Journal of Computational Mathematics}, 39(6):801--815, 2021.

\bibitem{morency2021graphon}
Matthew~W Morency and Geert Leus.
\newblock Graphon filters: Graph signal processing in the limit.
\newblock {\em IEEE Transactions on Signal Processing}, 69:1740--1754, 2021.

\bibitem{neuman2022superiority}
A~Martina Neuman, Rongrong Wang, and Yuying Xie.
\newblock Superiority of {GNN} over {NN} in generalizing bandlimited functions.
\newblock {\em arXiv preprint arXiv:2206.05904}, 2022.

\bibitem{opschoor2022exponential}
Joost~AA Opschoor, Ch~Schwab, and Jakob Zech.
\newblock Exponential relu dnn expression of holomorphic maps in high
  dimension.
\newblock {\em Constructive Approximation}, 55(1):537--582, 2022.

\bibitem{parada2021graphon}
Alejandro Parada-Mayorga, Luana Ruiz, and Alejandro Ribeiro.
\newblock Graphon pooling in graph neural networks.
\newblock In {\em 2020 28th European Signal Processing Conference (EUSIPCO)},
  pages 860--864. IEEE, 2021.

\bibitem{resnick1997recommender}
Paul Resnick and Hal~R Varian.
\newblock Recommender systems.
\newblock {\em Communications of the ACM}, 40(3):56--58, 1997.

\bibitem{ricci2021recommender}
Francesco Ricci, Lior Rokach, and Bracha Shapira.
\newblock Recommender systems: Techniques, applications, and challenges.
\newblock {\em Recommender Systems Handbook}, pages 1--35, 2021.

\bibitem{ruiz2020graphon}
Luana Ruiz, Luiz Chamon, and Alejandro Ribeiro.
\newblock Graphon neural networks and the transferability of graph neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1702--1712, 2020.

\bibitem{ruiz2021graphonsignal}
Luana Ruiz, Luiz~FO Chamon, and Alejandro Ribeiro.
\newblock Graphon filters: Signal processing in very large graphs.
\newblock In {\em 2020 28th European Signal Processing Conference (EUSIPCO)},
  pages 1050--1054. IEEE, 2021.

\bibitem{ruiz2021graphonprocessing}
Luana Ruiz, Luiz~FO Chamon, and Alejandro Ribeiro.
\newblock Graphon signal processing.
\newblock {\em IEEE Transactions on Signal Processing}, 69:4961--4976, 2021.

\bibitem{ruiz2021transferability}
Luana Ruiz, Luiz~FO Chamon, and Alejandro Ribeiro.
\newblock Transferability properties of graph neural networks.
\newblock {\em arXiv preprint arXiv:2112.04629}, 2021.

\bibitem{ruiz2021graphon}
Luana Ruiz, Zhiyang Wang, and Alejandro Ribeiro.
\newblock Graphon and graph neural network stability.
\newblock In {\em ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 5255--5259. IEEE, 2021.

\bibitem{scarselli2008graph}
Franco Scarselli, Marco Gori, Ah~Chung Tsoi, Markus Hagenbuchner, and Gabriele
  Monfardini.
\newblock The graph neural network model.
\newblock {\em IEEE transactions on neural networks}, 20(1):61--80, 2008.

\bibitem{sischka2022stochastic}
Benjamin Sischka and G{\"o}ran Kauermann.
\newblock Stochastic block smooth graphon model.
\newblock {\em arXiv preprint arXiv:2203.13304}, 2022.

\bibitem{stein2010complex}
Elias~M Stein and Rami Shakarchi.
\newblock {\em Complex analysis}, volume~2.
\newblock Princeton University Press, 2010.

\bibitem{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem{wang2018exponential}
Qingcan Wang et~al.
\newblock Exponential convergence of the deep neural network approximation for
  analytic functions.
\newblock {\em arXiv preprint arXiv:1807.00297}, 2018.

\bibitem{wang2022convolutional}
Zhiyang Wang, Luana Ruiz, and Alejandro Ribeiro.
\newblock Convolutional neural networks on manifolds: From graphs and back.
\newblock In {\em NeurIPS 2022 Workshop: New Frontiers in Graph Learning},
  2022.

\bibitem{wu2023graph}
Lingfei Wu, Yu~Chen, Kai Shen, Xiaojie Guo, Hanning Gao, Shucheng Li, Jian Pei,
  Bo~Long, et~al.
\newblock Graph neural networks for natural language processing: A survey.
\newblock {\em Foundations and Trends in Machine Learning}, 16(2):119--328,
  2023.

\bibitem{wu2022graph}
Lingfei Wu, Peng Cui, Jian Pei, Liang Zhao, and Le~Song.
\newblock {\em Graph neural networks}.
\newblock Springer, 2022.

\bibitem{zhou2020graph}
Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu,
  Lifeng Wang, Changcheng Li, and Maosong Sun.
\newblock Graph neural networks: A review of methods and applications.
\newblock {\em AI open}, 1:57--81, 2020.

\end{thebibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix %\label{appenx}

\section{Proofs of Proposition~\ref{prop:gpsampling} and Lemma~\ref{lem:tech}} \label{sec:Tsamp}

Let us begin by identifying $\mathbb{T}\cong [0,1)$, in the sense that points in $\mathbb{T}$ are identified as orbits of points in $[0,1)$ under a shift by $1$ unit:
\begin{equation*}
    \mathbb{T}\ni [x] :=\{x+m: m\in\mathbb{Z}\} \quad\text{ where }\quad x\in [0,1).
\end{equation*}
To reduce the amount of notation, we will simply write $x\in\mathbb{T}$, with an understanding that either $x$ is an abstract point in $\mathbb{T}$ or $x$ is the unique real-valued representative of $[x]$ in $[0,1)$. (We sometimes identify $x$ with its unique representative in $[-1/2,1/2)$; for example, see \S\ref{sec:Tsamplingthm}.) A continuous function $\phi$ on $\mathbb{T}$, therefore, one-to-one corresponds to a continuous function $g$ on $[0,1]$ such that $g(0)=g(1)$. We associate with $\mathbb{T}$ the following modulo addition, $+: \mathbb{T}\times\mathbb{T}\to\mathbb{T}$:
\begin{equation}\label{def:gpadd}
    (x,y) \overset{+}{\mapsto} x+y \mod 1,
\end{equation}
where $x,y\in [0,1)$, and, by an abuse of notation, the second $+$ denotes the usual addition on $\mathbb{R}$. It's known that $(\mathbb{T},+)$ is a compact Abelian group \cite{dummit2004abstract}.

\begin{definition} \label{def:Pontryagindual}
A character $\phi$ of $\mathbb{T}$ is any group homomorphism from $\mathbb{T}$ to the multiplicative group $(\mathbb{C}^*,\times)$, where $\mathbb{C}^*:=\mathbb{C}\setminus\{0\}$. The Pontryagin dual $\widehat{\mathbb{T}}$ of $\mathbb{T}$ is the set of all these characters. 
\end{definition}

It is known that $\widehat{\mathbb{T}}\cong\mathbb{Z}$, as additive groups, and that a character of $\mathbb{T}$ acts on $\mathbb{T}$ as follows
\begin{equation*}
    \phi: x\mapsto e^{i2\pi kx}, \quad\text{ for some }\quad k\in\mathbb{Z}.
\end{equation*}
Clearly, $|\phi_x|=1$ for all $\phi\in\widehat{\mathbb{T}}$, $x\in\mathbb{T}$. The concept of group characters allows one to define the duality between $\mathbb{T}$ and $\mathbb{Z}$ to be
\begin{equation}\label{dual}
    \langle x,k\rangle := e^{i2\pi kx}.
\end{equation}
It follows from Fourier theory that if $f\in L^2(\mathbb{T})$, then
\begin{equation*}
    \hat{f}(k) = \int_0^1 f(x)e^{-i2\pi kx}\,\mathrm{d}x = \int_0^1 f(x)\langle x,-k\rangle\,\mathrm{d}x \quad\text{ for }\quad k\in\mathbb{Z},
\end{equation*}
and that,
\begin{equation}\label{TFinv}
    f(x) = \sum_{k\in\mathbb{Z}} \hat{f}(k)e^{i2\pi kx}
\end{equation}
for almost every $x \in \mathbb{T}$.

If $H$ is a nontrivial discrete subgroup of $\mathbb{T}$, then $H$ must take the form \cite{dummit2004abstract}
\begin{equation}\label{groupH}
    H=\{x\in [0,1): Nx = 0 \mod 1 \}=\{0, 1/N,\cdots, (N-1)/N\}
\end{equation}
for some integer $N \geq 1$. Equivalently, $H$ can be identified as the group of $N$th roots of unity, $H=\{z\in\mathbb{C}: z^{N}=1\}$. Let $\Lambda$ be a discrete subgroup lattice of $\mathbb{Z}=\widehat{\mathbb{T}}$ that is the {\it annihilator} of $H$, defined by 
\begin{equation*}
    \Lambda=H^{\perp}:=\{k\in\mathbb{Z}: \langle z,k\rangle = 1, \forall\, z\in H\}.
\end{equation*}
Suppose $\Lambda=M\mathbb{Z}$ for some positive integer $M$. Then by \eqref{dual} and \eqref{groupH}
\begin{equation*}
    e^{\frac{i2\pi Mk}{N}} = 1 = \cdots = e^{\frac{i2\pi(N-1)Mk}{N}}, \quad \forall\, k\in\mathbb{Z}
\end{equation*}
which implies $M=N$ and $\Lambda=H^{\perp}=N\mathbb{Z}$.\\

Next, we equip $\mathbb{T}$ with the Haar measure $\lambda$ that is the usual Lebesgue measure on $[0,1)$; so $\lambda(\mathbb{T})=1$. Thus, we can identify $L^2(\mathbb{T})=L^2([0,1))= L^2([0,1])$. Suppose $f\in L^2([0,1];\mathbb{C}^m)\cap B_N$, according to Definition~\ref{def:Fblmtd}. We present the following Kluvan\'ek sampling theorem which states that $f$ can be determined by its sampled functional values on a discrete subgroup of $\mathbb{T}$. 

\begin{theorem}\cite[Theorem~1.5]{benedetto2001modern} \label{thm:Kluvanek}
Let $H\subset G$ be a discrete subgroup of a locally compact Abelian (LCA) group $G$, with a discrete annihilator subgroup $H^{\perp}\subset\hat{G}$. Let $E\subset\hat{G}$ be any subset of finite Haar measure for which the cannonical surjective map
\begin{equation*}
    h: \hat{G} \to \hat{G}/H^{\perp}
\end{equation*}
restricted to $E$ is a bijection, and define the sampling function
\begin{equation*} 
    s_{E}(x):=\int_{E} \langle x,\gamma\rangle\,\mathrm{d}\hat{\lambda}(\gamma), \quad\forall\,x\in G,
\end{equation*}
where $\hat{\lambda}$ is a normalized Haar measure  on $\hat{G}$ such that $\hat{\lambda}(E)=1$. Let $f\in L^2(G)$ and assume $\hat{f}=0$ almost everywhere off $E$. Then the followings hold.
\begin{enumerate}
    \item There exists a continuous function $f_{c}$ on $G$ such that $f=f_{c}$ almost everywhere.
    \item If $f$ is continuous on $G$, then
    \begin{equation}\label{Kluvaneksamp}
        f(x) = \sum_{y\in H} f(y)s_{E}(x-y)
    \end{equation}
    where the convergence of the sums is in $L^2$-norm and uniformly on $G$. Furthermore, the Gaussian quadrature formula
    \begin{equation}\label{Gquad}
        \|f\|^2_{L^2(G)} = \sum_{y\in H}|f(y)|^2
    \end{equation}
    is valid.
\end{enumerate}
\end{theorem}

With Theorem~\ref{thm:Kluvanek} at our disposal, we provide the proof of Proposition~\ref{prop:gpsampling}.

\begin{proof}[Proof of Proposition~\ref{prop:gpsampling}]
Let $f \in L^2([0,1];\mathbb{C}^m)\cap B_\mathfrak{m}$ and fix an $N \geq \mathfrak{m}$. We now specify Theorem~\ref{thm:Kluvanek} to the case of $G=\mathbb{T}$, 
\begin{equation*}
    H = \{0, 1/2N, \cdots, (2N-1)/2N\} \subset\mathbb{T},
\end{equation*}
and the continuous $f$ on $\mathbb{T}$ (see \eqref{contf}) such that $\hat{f}=0$ off $E=H^{\perp}=B_N$. Let $\hat{\lambda}$ be the scaled counting measure on finite subsets $A \subset \mathbb{Z}$ given by 
\begin{equation*}
    \hat{\lambda}(A) := \frac{\texttt{\#} A}{2N}.
\end{equation*}

Clearly, $\hat{\lambda}(B_N)=1$, and $s_{E}=s_N$ in \eqref{def:sampf}, so that
\begin{equation}\label{sfunc}
    s_N(x) = \frac{1}{2N}\sum_{k=-N}^{N-1} e^{i2\pi kx}.
\end{equation}
%as first introduced in \S\ref{sec:Fsamptheory}. 
Then by \eqref{Kluvaneksamp}, for every $x\in\mathbb{T}$, 
\begin{equation}\label{sampseries}
    f(x) = \sum_{j=0}^{2N-1} f(j/2N)s_N(x-j/2N) \\
    = \frac{1}{2N}\sum_{j=0}^{2N-1} f(j/2N) \bigg(\sum_{k=-N}^{N-1} e^{i2\pi (x-j/2N)k}\bigg),
\end{equation}
as was to be shown. This completes the proof of the proposition.
\end{proof} % End of proposition proof

More can be inferred from \eqref{Gquad} and \eqref{sampseries}, which leads us toward the statement of Lemma~\ref{lem:tech} in \S\ref{sec:Fsamptheory}. However, we first need to revisit some concepts in Fourier theory on finite groups. Let $G$ be a finite abelian group and set $M = \texttt{\#} G$. Then, $\hat{G}\cong G\cong\mathbb{Z}_M$ \cite{dummit2004abstract}, and so to simplify our presentation we will treat both $G$ and $\hat{G}$ as $\mathbb{Z}_M$. We define the Fourier transform on $L^2(G)$ by
\begin{equation*} 
    \mathcal{F}_{G}(f)(v) := \frac{1}{M}\sum_{u\in G} f(u)e^{-i2\pi uv}, \quad \forall v\in \hat{G}=\mathbb{Z}_M,
\end{equation*}
where $f:\mathbb{Z}_M\to\mathbb{C}$ and $uv$ denotes usual scalar multiplication. The inverse Fourier transform $\mathcal{F}^{-1}_{G}$ on $L^2(\hat{G})$ is then
\begin{equation*} 
   \mathcal{F}^{-1}_{G}(f)(u) := \sum_{v\in\hat{G}} f(v)e^{i2\pi uv}, \quad \forall u\in G=\mathbb{Z}_M,
\end{equation*}
where $f: \hat{G}=\mathbb{Z}_M\to\mathbb{C}$. Together these transformations satisfy 
\begin{equation}\label{id}
    \mathcal{F}^{-1}_{G}\circ\mathcal{F}_{G} = Id_{L^2(G)} \quad\text{ and }\quad \mathcal{F}_{G}\circ\mathcal{F}^{-1}_{G} = Id_{L^2(\hat{G})}.
\end{equation}
We now provide the proof of Lemma~\ref{lem:tech}.

\begin{proof}[Proof of Lemma~\ref{lem:tech}]
First, \eqref{GquadforT} is a direct consequence of \eqref{Gquad} and Plancherel's theorem. Hence, we need only prove \eqref{claim_finv}. Setting $\hat{f}(k)=0$ if $k\in\mathbb{Z}\setminus B_N$ in \eqref{TFinv}%, and identifying $f$ with its continuous version
, we obtain
\begin{equation}\label{FTfx}
    f(x) = \sum_{k=-N}^{N-1} \hat{f}(k)e^{i2\pi kx}, \quad \forall x\in\mathbb{T}.
\end{equation}
Letting $H=\{0,1/2N,\cdots, (2N-1)/2N\}\subset\mathbb{T}\cong [0,1)$ inherit the group addition \eqref{def:gpadd}, it follows that $H\cong\mathbb{Z}_{2N}$ and so $\hat{H}\cong H\cong\mathbb{Z}_{2N}$. We let
\begin{equation*}
    \hat{H} = B_N = \{-N, \cdots, 0, \cdots, N-1\},
\end{equation*}
with the group addition $k+l \equiv k+l \mod 2N$. 

Define $\tilde{g}: \hat{H}\to\mathbb{C}$ to be such that for each $k\in B_N$ we have $\tilde{g}(k) := \hat{f}(k)$. Then, from \eqref{id} we have $\tilde{g}=\mathcal{F}_{H}(g^*)$ for some $g^*: H\to\mathbb{C}$, where
\begin{equation}\label{FTgp1}
    \mathcal{F}_{H}(g^*)(k) = \frac{1}{2N}\sum_{x\in H}g^*(x)e^{-i2\pi kx} = \tilde{g}(k) = \hat{f}(k),
\end{equation}
and so
\begin{equation}\label{FTgp2}
    g^*(x) = \sum_{k=-N}^{N-1}\tilde{g}(k)e^{i2\pi kx} = \sum_{k=-N}^{N-1}\hat{f}(k)e^{i2\pi kx}, \quad \forall x\in H. 
\end{equation}
Comparing \eqref{FTfx} with \eqref{FTgp2} we conclude that $g^*(j/2N) = f(j/2N)$ for every $j=0,\cdots, 2N-1$. Therefore, from \eqref{FTgp1},
\begin{equation*} 
    \hat{f}(k) = \frac{1}{2N}\sum_{j=0}^{2N-1}f(j/2N)e^{-i2\pi kj/2N},
\end{equation*}
for $k\in B_N$. Since it's clear from definition that $\hat{f}(k)=0$ for $k\in B_N\setminus B_\mathfrak{m}$, we complete the proof.    
\end{proof} %end of proof of technical lemma


\end{document}