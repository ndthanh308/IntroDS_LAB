% \documentclass{article}
\documentclass[letterpaper, 10 pt, conference]{ieeeconf} 
\usepackage[utf8]{inputenc}
% \usepackage{float}  % can use H to force figures
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}



\makeatletter
\def\bstctlcite{\@ifnextchar[{\@bstctlcite}{\@bstctlcite[@auxout]}}
\def\@bstctlcite[#1]#2{\@bsphack
  \@for\@citeb:=#2\do{%
    \edef\@citeb{\expandafter\@firstofone\@citeb}%
    \if@filesw\immediate\write\csname #1\endcsname{\string\citation{\@citeb}}\fi}%
  \@esphack}
\makeatother 

\title{CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for  Robots
}
\author{Nikhil Kakodkar, Dmitriy Rivkin, Bobak H. Baghi, Francois Hogan, Gregory Dudek}
% \date{September 2022}

\newcommand{\bnfdef}{::=}
\newcommand{\bnfistypeof}{$\in$}
\newcommand{\bnftype}[1]{$\mathtt{#1}$}
\newcommand{\bnfalt}{$\mid \;$}
\newcommand{\bnfnonterm}[1]{$\mathit{#1}$}
\newcommand{\bnfterm}[1]{$\mathrm{#1}$}
\newcommand{\bnfspace}{$\;$}
\newcommand{\bnfdescr}[1]{ #1  }
\newcommand{\bnflabel}[1]{ #1 }




\begin{document}
% Set bibliography parameters. Must be before any citation! --gd
\bstctlcite{BSTcontrol}
\maketitle


\begin{abstract}
This work explores the capacity of large language models (LLMs) to 
address problems at the intersection of spatial planning and natural language interfaces for navigation.
Our focus is on following relatively complex instructions that are more akin to natural conversation than
traditional explicit procedural directives seen in robotics. Unlike 
most prior work,
where navigation directives are provided as imperative commands (e.g., ``go to the fridge"), 
we examine implicit directives within conversational interactions.
% the instructions examined in this work are structured as conversational interactions with directives that are often only implicit. 
We leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types.
We demonstrate that a robot can  better parse descriptive language queries than existing methods by using an LLM to interpret the user interaction in the context of a list of the objects in the scene. 
%Finally, we confirm our simulation results with a real-world experiment, 
% but we don't report it to you since we are so 
% stingy and mean-spirited.  
% That's why we named our full system JERK CARTIER !
% Also, why are you wasting time reading a mere comment, 
% and why am I wasting time writing this?
\end{abstract}

\section{Introduction}
This paper explores the extent to which natural interaction is possible between human and robot in the context of a navigation task. Unlike current voice assistants, can a robot infer its task in a navigational context without receiving an explicit command?

Household robotic tasks can often be formulated using imperative commands whose template has a structure that can be abstracted as go-do (go somewhere, then do something).
In this work, we focus primarily on complex variants of the first step  and measure the capacity of the robot to recognize the physical location associated with the intent of the query. While existing voice assistants or robot
systems use very simple variants of such specifications, we must recall that natural language between humans often uses complex descriptors.
Endowing  robots with the ability to  infer a goal from a natural specification is desirable as it provides a  more natural mode of interaction for most users and it allows various navigation trade-offs to be left in the hands of the system (for example path length vs traction vs landmark visibility).


% Figure environment removed

For a robot, planning the navigation target requires a sophisticated understanding of the world it inhabits. For example, consider the simple-seeming instruction ``Get a photo of the sunset." Assuming the robot is confined to stay within the home, this implies the need to navigate to a western-facing window. The robot must know that the sun sets in the west, and can only be seen through a window. Now imagine the user says something like: ``Oh Robot, I am so sad. I just feel like there is nothing beautiful left in this world anymore, you know? I know I probably shouldn't have stayed in bed till 5:30 pm, but what is the point of getting up when the world is just so ugly?" As little as a year or two ago, the idea that a robot might be able to address such an interaction by navigating to the western window and snapping a picture of the sunset to cheer up the user would have been unrealistic -- but by leveraging recent progress on several fronts, notably large language models (LLMs), this has become plausible. Significant challenges remain, however, in grounding the the reasoning capabilities of LLMs in the reality of the physical world. The aim of this work is to examine how such challenges can be overcome in order to allow LLMs to be used to support such sophisticated interactions.




To study this question, we augment the AI2Thor \cite{ai2thor} simulation environment with implicit  conversational queries for $40$ object types. 
We then introduce CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots), a pipeline which first uses an LLM to infer which object the user is referring to in the query, then uses a ``spatial language index" to associate that object with a location in the scene. To tackle the problem of grounding the LLM in the robot's perception of its environment, we propose a novel method to caption the scene based on the objects contained within it. We then evaluate multiple approaches, of varying complexity, to construct and query the spatial language index.

The contributions of this paper are:
\begin{itemize}
    \item The augmentation of the AI2Thor simulator with three types of queries for $40$ object types: 1) Short explicit queries. 2) Short implicit queries where objects are referred to by their use rather than by their name in one sentence. 3) Long, implicit, and conversational queries that span several sentences, and include a significant amount of distracting content.
    \item The introduction of our method, CARTIER, that can handle all three query types. CARTIER consists of an offline stage where the robot explores the environment to build a spatial language index, and an online stage which processes user queries and relates them to physical points in space.
    \item The evaluation of our method's performance against a baseline and an ablation study.
\end{itemize}

\section{Related Work}

\subsection{Foundation Models for Open-Vocabulary Language Navigation}
In exploiting the association between images and language, a key tool that we use is CLIP. CLIP is an acronym for Contrastive Language-Image Pre-Training and it refers to an approach for training large-scale models that can encode the relationship between text and images~\cite{clip}.  The approach uses a transformer architecture trained on a large data of paired images and text and can subsequently predict whether 
an image and associated caption  are related or not. 
Since the release of CLIP~\cite{clip},  several authors have explored how to make use of this powerful vision-language model to enable language-driven navigation instructions using open vocabularies (i.e. not constrained to a fixed set of objects). Several works, including  VLMaps~\cite{vlmaps}, ConceptFusion~\cite{conceptfusion}, and Clip Fields~\cite{clipfields} generate per-pixel or per-voxel embeddings in the CLIP embedding space, which can then be queried using the CLIP language encoder. As a consequence of the related embedding spaces, however, these models have the same limitations as CLIP in terms of the sophistication of the query that they can successfully interpret. Some work, including VLMaps~\cite{vlmaps} and LMNav~\cite{lmnav}, use an LLM such as GPT-3~\cite{gpt3} in an effort to parse longer navigation instructions into a sequence of moves which can be executed by the robot. While these authors also make  use of an LLM to parse instructions, the nature of the instructions differs from  those considered in this paper as they   focus on a detailed description of the robot's path  rather than interpreting complex humans queries describing the desired location of the robot.
CoW (CLIP on Wheels)~\cite{cows} presents a general framework for CLIP-based object navigation, where a robot navigates using some policy while attempting to localize the object in question using CLIP. As such, some of the papers mentioned earlier in this sections can also be considered CoWs, although the precise problem formulation taken by the different authors differ. CARTIER is fundamentally different from these results due to its use of an LLM to interpret the user query. However, most of the work mentioned in this section could be used to implement the navigation component of CARTIER, once the correct object has been identified.
Finally, ANSEL Photobot~\cite{ansel} used an LLM to break down enigmatic user queries into ensembles of concepts that can be detected using CLIP, however the focus was on open-vocabulary photo selection rather than navigation. 


\input{table.tex}

\subsection{Captioning for Question Answering}
A key component of the CARTIER pipeline is to leverage an LLM to inform the robot on the contents of the scene. We do this by enumerating the objects present in the scene, having first detected them with pre-trained object detector. Other work has also explored captioning images \cite{captionvqa} and $3$D scenes \cite{scan2cap}, \cite{sqa3d} in order to support visual question answering (VQA) and situated question answering (SQA), respectively. With all of these approaches, the way the scene is captioned constrains the types of questions that can be answered. SQA3D \cite{sqa3d} proposes to solve a situated question answering task using scene captioning and an LLM, but the captioning style is different from that used in CARTIER, necessitated by the difference in the target task. The task of interest in SQA3D has to do with answering queries related to the spatial arrangement of objects (e.g. ``what is behind the couch?").

There is good evidence that simple queries often yield unsatisfactory results and these can be enhanced via a more natural or conversational paradigm~\cite{keyvan2022approach,lopez2018alexa}.
A more complete coverage of the long history and relationship between natural and expressive query languages, and data collecting agents, most of which relates to Internet search is outside the scope of what we can cover here~\cite{lieberman2001exploring,hu2016natural,allen1983recognizing,lieberman1995letizia}.


\section{Dataset}
\label{sec:dataset}
The AI2Thor simulation environment contains $189$ different object types. We select a subset of $40$ objects and create three different types of queries (see Section \ref{sec:three_query_types} for details). Since the objects are distributed across many different scenes in different combinations, we can evaluate the same query across many different scenes. In this paper, we consider $7$ scenes over which we evaluate our approach. Certain queries may plausibly be answered by multiple objects, so the alternative answers for each query are also included in our dataset. Using ground-truth object bounding boxes from the simulator, we compute the minimum distance between the predicted position and any of the bounding boxes of objects that are considered to be plausible answers to the query. Using the distance as a performance metric rather than the object identity is convenient because it supports comparison to methods which do not explicitly output the identity of the object being searched for.
The environment exploration phase is achieved by teleoperating a mobile robot, which collects RGBD frames from a camera mounted on the robot at approximately $1.5$ meters from the ground. 

\subsection{Three query types}
\label{sec:three_query_types}
The queries are split into $3$ categories with increasing complexity: explicit, implicit, and conversational, as detailed in Table~\ref{table:query_types}.  Explicit queries are straightforward requests to go to some explicitly-named object (e.g. get the scissors), and can be successfully answered using an object detector or CLIP-based methods in the open vocabulary case. Implicit queries refer to an object implicitly by expressing some need of the user which could be addressed with that object, e.g. ``I need something to cut this paper with." The Implicit queries in our dataset are short (one or two short sentences) and to the point, focused entirely on discussing the need which is to be serviced. These cannot be answered with an object detector, since there is no straightforward way to extract the object of interest from the utterance, but in some cases may still be answered satisfactorily by CLIP, which is likely to learn to embed ``scissors" nearer to the phrase ``cut paper" than any other object. Our third class of queries, Conversational, are comprised of several sentences and may contain a significant amount of information that is irrelevant to the need expressed by the user. For example, consider the utterance ``Wow, my mom will really love this card I'm making her for her birthday. It has all of her favorite things on it -- lilies, rabbits, birds. I think I'll make it a little smaller than a full page though, so I guess I'll need something to cut the cardstock with. Oh, I do hope she likes it", as depicted in Fig.~\ref{fig:query_types} There are several challenges associated with using CLIP to answer this query directly. First, the length of CLIP's sentence embeddings is limited to $77$ tokens, meaning it may not even be possible to encode the entire text of the query. Second, CLIP will attempt to incorporate all of the concepts expressed in the utterance into the embedding (mom, birthday, lilies, rabbits, birds, etc), the majority of which are irrelevant to the need of the user. In order to interpret the query, and estimate  what the user is asking for, an LLM capable of processing lengthy pieces of text is required. These conversational queries are quite different from the manner in which people must speak to today's voice assistants. Based on the evidence from information retrieval, natural queries should enhance usability and effectiveness~\cite{keyvan2022approach}.
The full list of queries can be found in the Appendix (Sec.~\ref{app1}). Table \ref{table:query_types} summarizes the three types of query and the methods that may be able to answer them.


% Figure environment removed

\section{Method}
In this Section, we introduce CARTIER, a framework that can interpret complex user language instructions to generate navigation targets. CARTIER is composed of six parts, summarized graphically in Figure \ref{fig:pipeline}:
\begin{enumerate}
    \item Environment exploration
    \item Object detection and spatial language index construction
    \item LLM query construction
    \item Querying the LLM
    \item LLM response processing
    \item Querying the spatial language index.
\end{enumerate}

The rest of the this section explores each of these parts in more detail.
\subsection{Environment exploration}
The robot navigates the environment collecting RGBD video, as well as the associated pose in which each video frame was collected. In the simulated environment, the robot pose is obtained from the simulator directly.
Depth information and 6-DOF pose is required in order to maximize the accuracy of object localization, however certain versions of CARTIER, presented in the following sections, can work using only a monocular camera and 2-DOF pose estimates.

\subsection{Object detection and spatial language index construction}
\label{sec:object_detection_and_spatial_index}
CARTIER populates the LLM query with a list of objects in the scene, so we use an off-the-shelf object detector (EVA \cite{EVA}) trained on the LVIS dataset \cite{lvis} that contains $1204$ object categories. We run the detector on each of the frames in the video and  compile a list of all LVIS objects encountered by the robot during the exploration phase. Though we elect to use an object detector with a fixed vocabulary, CARTIER could potentially handle open vocabulary queries. See Section \ref{sec:query_the_llm} for more discussion of this point.


A ``spatial language index" is a map that allows  to look up positions given an object name. We explore three such indices in this work. In order of descending order of complexity, the first is VLMap \cite{vlmaps} which creates a 2-D top-down grid of the environment, where each grid cell represents a CLIP embedding (calculated via LSeg \cite{lseg}) averaged over each point projected into the grid. The second involves selecting the frame with the largest bounding box for the given object (a heuristic for proximity to the object), then using the depth signal in combination with the object bounding box to determine the position of the object. The third methods returns the position of the robot at the time that the frame with the largest object bounding box was observed. We improve the accuracy of our proximity heuristic (maximize bounding box size) by partially compensating for perspective distortion using a transform based on the weak perspective camera model. With $x_t$ being the position of a pixel in the transformed image, $x$ being its position in the original image, and $d$ being the focal distance, the transform is given by:
\begin{equation}
    x_t = sin(tan^{-1}(\frac{x}{d}))
\end{equation}

We refer to these methods as LSeg, ObjectDepth, and ObjectViewpoint, respectively.


\subsection{LLM query construction}
\label{sec:query_construction}
We wish for the LLM to select the most relevant object in the scene that fits the user query. The LLM is informed of the contents of the scene via the inclusion of a comma separated list of the objects that were detected. Each object type is only listed once, even if multiple instances were detected. In order to condition the correct behavior, the LLM is also presented with two examples of correct questions/answer pairs in the prompt. The Backusâ€“Naur form  (BNF)~\cite{bnf} completely describing the construction of the LLM query is given in Figure \ref{fig:bnf}.

% Figure environment removed

\subsection{Query the LLM}
\label{sec:query_the_llm}
We wish for the LLM to select a single object from the list, following the examples provided in the query as described in Section \ref{sec:query_construction}. In practice, we find that the LLM sometimes deviates in two ways: by selecting multiple objects in the form of a comma separated list and by selecting an object that is not in the list. We remedy the former by setting the comma as a ``stop token," which means the LLM will stop generating text if it generates a comma. We solve the latter by generating five different answers, and selecting the highest probability answer (as determined by the LLM) which contains an object from the list. Empirically, we find that $5$ different answers is sufficient to ensure that at least one contains an object from the list.

Because CARTIER forces the LLM to select from a fixed set of objects known from the object detecto, one could erroneously assume that CARTIER is a closed-vocabulary method. For an illustrative example of why this is not true, consider a query in which the user wants a tamper (an object used to compress coffee when making espresso), but ``tamper" is not contained in the vocabulary of the object detector. An LLM can use its significant world-knowledge to infer that the tamper is likely to be found near the coffee machine, and direct the robot to the correct location. In fact, even if the tamper had been hidden from the view of the robot during the exploration phase, it would still likely find the right answer, as long as a reasonable guess can be made about where to find the object in question.

\subsection{LLM response processing}
In some (relatively rare) cases, none of the $5$ answers generated by the LLM include an object from the list of detected objects. In these cases, we retry querying the LLM up to five times. If an object from the list is not found, we select the highest-probability LLM result and find the most similar object in the list as measured by CLIP text embeddings.

\subsection{Query the spatial language index}
In order to query the VLMaps index, we embed the object name using CLIP and then select the pixel with the highest similarity, after first dropping the large embedding dimensions following \cite{ansel}. The ObjectDepth and ObjectViewpoint indices are queried as described in Section \ref{sec:object_detection_and_spatial_index}.


\section{Results}
We compare three versions of our method on the dataset described in Section \ref{sec:dataset}, as well as a baseline. The three versions correspond to the three spatial language indices described in Section \ref{sec:object_detection_and_spatial_index}, and are referred to as CARTIER-VLMap, CARTIER-ObjectViewpoint, and CARTIER-ObjectDepth. The baseline method involves embedding the query with CLIP and then using this embedding to search the VLMap spatial index. When the query is longer than the maximum length supported by CLIP, it is truncated.  This baseline is referred to as ``VLMap."

Table \ref{tab:mean_results} shows the mean errors (in meters) across the four methods described above and the three query types. We also present the success rates (defined as error $<$ 1 meter) in Table \ref{tab:success_results}. The VLMap baseline is behaving as expected: performance decreases with increasing query complexity (explicit $<$ implicit $<$ conversational). In contrast, the three CARTIER methods perform consistently across the three query types. This indicates that the LLM is doing an excellent job of inferring the queried object, which explains why performance on the complex query, where the object identity must be inferred from a complex utterance, is the same as the explicit case where the object is given directly.

We were surprised to discover that the ObjectViewpoint and ObjectDepth spatial language indices, while simpler, outperformed VLMap in terms of mean error. One hypothesis that might explain this phenomenon is the loss of information that can occur when mapping CLIP embeddings from full images to pixels, an issue which is described in more detail in \cite{conceptfusion}. Less surprising was the significant improvement attained using ObjectDepth over ObjectViewpoint. The poor performance of CARTIER-ObjectViewpoint in Table \ref{tab:success_results} is also expected: most teleop trajectories avoid navigating the robot too close to any objects.

Finally, the VLMap baseline did outperform CARTIER-VLMap for the Explicit queries, indicating that some performance degradation due to the object detector is occurring. As CARTIER forces the LLM to select from a list of objects detected by the detector, it will fail to select the correct object if the object detector does not detect it. However, it is also clear that as the complexity of the query increases, the added value of the LLM more than compensates for the challenges associated with object detection.


\begin{table}
\begin{center}
\begin{tabular}{||c|c|c|c||} 
 \hline
  & Explicit & Implicit & Conversational\\ [0.5ex]
 \hline\hline
 CARTIER-VLMap & 2.14 & 2.13 & 2.08\\ [0.5ex]
 CARTIER-ObjectViewpoint & 1.8 & 1.77 & 1.75 \\ [0.5ex]
 CARTIER-ObjectDepth & \textbf{1.34} & \textbf{1.3} & \textbf{1.35} \\ [0.5ex]
 \hline
 VLMap & 1.86 & 2.3 & 2.43 \\ 
 \hline
\end{tabular}
\caption{\label{tab:mean_results}Mean error in meters across three spatial language index variations of CARTIER, plus VLMap baseline, for each of the Query Types.}
\end{center}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{||c|c|c|c||} 
 \hline
  & Explicit & Implicit & Conversational\\ [0.5ex]
 \hline\hline     	      	     
 CARTIER-VLMap           &29.92   & 28.35 & 29.92\\ [0.5ex]
 CARTIER-ObjectViewpoint   &16.54 & 18.90 & 14.96 \\ [0.5ex]
 CARTIER-ObjectDepth        & \textbf{59.84}  & \textbf{57.48} & \textbf{53.54}\\ [0.5ex]
 \hline
 VLMap & 44.09 & 20.47 & 18.90 \\ 
 \hline
\end{tabular}
\caption{\label{tab:success_results}  Success rates (percent) across three spatial language index variations of CARTIER, plus VLMap baseline, for each of the Query Types. Success is defined as selecting a location within one meter of the target object.}
\end{center}
\end{table}

\section{Conclusion}
The existing work on the value and impact of conversation search for the Internet~\cite{keyvan2022approach} suggests that 
users are also likely to desire more natural, conversational interactions with robot assistants in the near future. In this work, we took a step towards enabling those types of interactions by introducing CARTIER, a method which fuses power of modern LLMs and state of the art object detectors. 

The use of a fixed-vocabulary object detector remains something of a limitation, as was demonstrated by reduced performance of CARTIER-VLMap as compared to baseline VLMap on the Explicit query set. This issue is actually more fundamental than it seems on the surface, as it is related to the question of how to best describe the scene in order to allow the LLM to reason about it. Despite the relative simplicity of CARTIER's textual scene representation however, it still appears to work well in many cases.   

By allowing users to specify ``go-do tasks'' in more natural ways, we hope that the utility and impact of robot navigation agents will be greatly enhanced.



\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,main}

\section{Appendix}
\label{app1}
All Conversational queries, the intended object, and alternate acceptable objects.
\begin{tiny}


\begin{itemize}
\item \textbf{AlarmClock:} What is that sound? Sounds like some kind of ringing. Oh looks like it's time for Ben to wake up. Could you please go turn it off?. \textbf{Alternate objects: } CellPhone
\item \textbf{Apple:} Looks like I am almost out of groceries. But it's too late to go today. Okay I should schedule some time to go to the grocery store tomorrow. Probably after work. Could you tell me what I do I already have?. \textbf{Alternate objects: } Apple, Tomato, Egg, Bread, Potato
\item \textbf{BaseballBat:} Do you hear that ... what's that sound? It's the middle of the night. Is that the floor creaking? That definitely sounds like the squeaky step. Oh my god.... \textbf{Alternate objects: } Knife
\item \textbf{BasketBall:} Time to go shoot some hoops. But I am running so late. I hope the bus is on time. If so, I think I can still make it. I think I have everything... am I missing anything?. \textbf{Alternate objects: } 
\item \textbf{Bathtub:} I had quite the day at work today. You would not believe what Linda said to me. She said I was high strung. Can you imagine that? Me? High strung? I'll show her high strung... Well, maybe I could use a little relaxation. I'm gonna take a soak, make sure everything is set up for me in there.. \textbf{Alternate objects: } 
\item \textbf{Bed:} I told Bobby a million times to clean up his room. And those sheets! Can you believe those sheets? Somebody has got to change those, they smell like a pigsty!. \textbf{Alternate objects: } 
\item \textbf{Blinds:} Uggggg, I have the worst headache! I'm not sure I'll even be able to get out of bed today, it's so bad. Why is it so damn bright in here?. \textbf{Alternate objects: } LightSwitch
\item \textbf{Book:} Ahh that was a long day! Was looking forward to pick up where I left off. The previous chapter was a cliff-hanger. Can't wait to read further. Could you bring it to me?. \textbf{Alternate objects: } 
\item \textbf{Bowl:} Quick, quick, there guests will be here any minute. Ohhh, where can put these grapes? I don't see anything to put them in.. \textbf{Alternate objects: } 
\item \textbf{Bread:} I am so tired of eating nothing but dry toast for breakfast, but, of course, we must all abide by the doctor's orders. It's better than a heart attack, anyway. Now, let's see... the toaster looks clean, its plugged in. Now, I just need to find that loaf and I'll be ready for business!. \textbf{Alternate objects: } 
\item \textbf{ButterKnife:} Ohhh, honey, it's ok. Don't be sad, a little PB and J should cheer you right up. Here, we have a beautiful fresh loaf of bread, and some homemade jam. Just need to find something to spread it with.... \textbf{Alternate objects: } 
\item \textbf{CD:} I'm a creeeeeeep, I'm a weeeeiirrrrdoooooo, what the hell am I doing here? I don't belong here... Man, I love song, and I sure do miss the 90s. Streaming just totally lacks the sense of occasion you get when you pop in your favorite album. You know what, I think it's the perfect time to relive my youth.. \textbf{Alternate objects: } 
\item \textbf{Candle:} Oh no! The lights just went out. This is the worst winter storm I have witnessed in years. It probably blew a transformer. I am guessing it'll take a while until it's fixed. Anyways, it's too dark in here. Is there anything around I could use?. \textbf{Alternate objects: } 
\item \textbf{CellPhone:} Mom just told me that Aunt Sarah is tryng to call me. I wonder what it is about. What should I look for?. \textbf{Alternate objects: } 
\item \textbf{CoffeeMachine:} I have so much work to do. Looks like I'll be working late into the night. I could definitely use a cup of joe. I am guessing the presentation would take me another two hours or so to finish. But I am getting really sleepy. Anything around I could use?. \textbf{Alternate objects: } 
\item \textbf{CreditCard:} Oh my god, I am so so so exicted to see Michael Buble in concert. That's why I've been sitting here hitting refresh on my browser every two minutes, waiting for the tour to be announced those tickets are going to go fast! Oh, oh they're out! They're out! Just gotta put in my payment info ... damn! What's my CVC? I can't believe I forgot it.. \textbf{Alternate objects: } 
\item \textbf{Drawer:} Oh my god, Mom and Dad are home early! If they find out I smoke I'll be grounded for weeks. Quick, hide my smokes. And this lighter. They're gonna kill me!. \textbf{Alternate objects: } SideTable, Cabinet
\item \textbf{Fridge:} I am thinking of making some cocktails for the party tonight. I think I have all the ingredients except a few. But my brother said he'll get the rest. But yeah I probably need to some ice.. \textbf{Alternate objects: } 
\item \textbf{GarbageCan:} Does it smell weird in here? Smells like something might be rotting. That is so disgusting. I wonder what it is.. \textbf{Alternate objects: } GarbageBag
\item \textbf{Kettle:} Hmm I am feeling a little sick today. I know, some lemon ginger tea would be perfect. Let me check the if I have any. Could you boil some water?. \textbf{Alternate objects: } 
\item \textbf{Laptop:} I can't believe they messed up my cabinets! These cost more than my car, and they messed them up! Look, they're all wrong. They didn't even get the right number of door handles. I am going to email that Jessica right now and give her a piece of my mind!. \textbf{Alternate objects: } CellPhone
\item \textbf{LightSwitch:} I wonder if I need glasses? I can't seem to make out the words on this page. Am I getting old? Or is it just dark?. \textbf{Alternate objects: } Candle, Blinds
\item \textbf{Pencil:} Yes, I can hear you. Yes, that will do fine, I'll call you back on Thursday. What's your number? Wait hold on sec, let me get find something to write that down with.. \textbf{Alternate objects: } Pen
\item \textbf{PepperShaker:} George! Geoooorrrgeee! This steak is wonderfully done honey, but it's a little bland, don't you think?. \textbf{Alternate objects: } SaltShaker, PepperShaker
\item \textbf{Pillow:} Oh my god, I'm sooo drunk. No, you don't get it. Like, sooooo drunk. I don't think I can make it up to bed, I can't even get my pants off! I think I'll just take a little nap right here.. \textbf{Alternate objects: } 
\item \textbf{Potato:} Mmmmm, steak tonight. Can't wait. I guess I should probably have some veggies too... oven fries are a vegetable, right?. \textbf{Alternate objects: } 
\item \textbf{RemoteControl:} Oh oh, I think Conan is on now. I love that guy, he's so funny? Hmmm, where's that clicker? I love that Conan, he's so perky and positive all the time!. \textbf{Alternate objects: } 
\item \textbf{Shelf:} Scrappy no! No scrappy, that's not for you. I said NO! I better put this somewhere you can't reach it... down Scrappy, bad dog!. \textbf{Alternate objects: } 
\item \textbf{Sink:} Oh wow, that plant is looking very droopy. I think it needs some water. Hmmm, I think I have a pail here somewhere. Ah, here it is. Now go fill it up, will you?. \textbf{Alternate objects: } Bathtub
\item \textbf{SoapBottle:} Greg, look at you, you're all sticky. Come, come, wash your hands. No, you come right now mister. Don't you touch anything, now! I said no! Come over here to the sink. This oil on your hands is just not coming off, I wonder what we could use that might dissolve it better?. \textbf{Alternate objects: } SoapBar
\item \textbf{Sofa:} Is that the alarm already? I guess that means dinner is ready. Smells pretty good, I hope Chad will eat some. He's such a skinny boy, he could really stand to put on a few pounds, maybe go outside once in a while. He's always playing that Xbox. Anyway, could you fetch him for dinner?. \textbf{Alternate objects: } Television
\item \textbf{Spoon:} OK, here's your soup, there's the bread, and here's a napkin. I made this soup specially for you, I hope you like it. No, I hope you love it. Oh wait, silly me, I haven't given you anything to eat it with!. \textbf{Alternate objects: } 
\item \textbf{StoveKnob:} So then I told her, I'm not paying 4 dollars a pound for tomatoes. No way no how, I said ... wait, hold on a sec. My pasta is about to boil over! Quick, turn it down! Hurry!. \textbf{Alternate objects: } Stove
\item \textbf{Television:} Ooooh, this movie is disgusting and violent! Where's the clicker when you need it? Oh, shoot, I forgot, it's broken, just like my leg. What am I going to do? I can't watch this any more, it makes me sick.. \textbf{Alternate objects: } 
\item \textbf{TissueBox:} No, no, he can't be dead! You must be wrong. My Johnny can't be dead, he was so young and full of life. Oh how will I go on? Oh, I'm so sorry for getting carried away like this, I understand, there are papers to sign. Let me just get something to dry my eyes with and I'll be right with you.. \textbf{Alternate objects: } 
\item \textbf{Toilet:} Flush this goldfish.. \textbf{Alternate objects: } 
\item \textbf{Towel:} My hair is wet, I need something to dry it off with.. \textbf{Alternate objects: } 
\item \textbf{Vase:} Oh my, Johan, these flowers are beautiful! What a nice suprise, my birthday isn't for weeks! Aren't you just the sweetest fellow? Let me just try to find somewhere to put these, and then we can sit down and catch up.. \textbf{Alternate objects: } 
\item \textbf{Window:} Aunt Annie's coming to visit, I hope nobody takes that parking spot out in front of our house. You know she can't walk very well. Can you just make sure it's still free? It will give me some peace of mind.. \textbf{Alternate objects: } 
\item \textbf{WineBottle:} Oooh, I love a nice, stinky, blue cheese. I think I'll crack this puppy open and watch some shark tank. But what shall I have to go with it?. \textbf{Alternate objects: } 

\end{itemize}
\end{tiny}

\end{document}
