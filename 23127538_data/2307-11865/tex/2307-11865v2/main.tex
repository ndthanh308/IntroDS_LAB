% \documentclass{article}
\documentclass[letterpaper, 10 pt, conference]{ieeeconf} 
\usepackage[utf8]{inputenc}
% \usepackage{float}  % can use H to force figures
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{xcolor}




\makeatletter
\def\bstctlcite{\@ifnextchar[{\@bstctlcite}{\@bstctlcite[@auxout]}}
\def\@bstctlcite[#1]#2{\@bsphack
  \@for\@citeb:=#2\do{%
    \edef\@citeb{\expandafter\@firstofone\@citeb}%
    \if@filesw\immediate\write\csname #1\endcsname{\string\citation{\@citeb}}\fi}%
  \@esphack}
\makeatother 

\title{CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for  Robots
}
\author{Dmitriy Rivkin, Nikhil Kakodkar, Francois Hogan, Bobak H. Baghi, Gregory Dudek}

\newcommand{\bnfdef}{::=}
\newcommand{\bnfistypeof}{$\in$}
\newcommand{\bnftype}[1]{$\mathtt{#1}$}
\newcommand{\bnfalt}{$\mid \;$}
\newcommand{\bnfnonterm}[1]{$\mathit{#1}$}
\newcommand{\bnfterm}[1]{$\mathrm{#1}$}
\newcommand{\bnfspace}{$\;$}
\newcommand{\bnfdescr}[1]{ #1  }
\newcommand{\bnflabel}[1]{ #1 }





\begin{document}
% Set bibliography parameters. Must be before any citation! --gd
\bstctlcite{BSTcontrol}
\maketitle


\begin{abstract}
This work explores the capacity of large language models (LLMs) to 
address problems at the intersection of spatial planning and natural language interfaces for navigation. We focus on following complex instructions that are more akin to natural conversation than traditional explicit procedural directives typically seen in robotics. Unlike most prior work where navigation directives are provided as simple imperative commands (e.g., ``go to the fridge"), we examine implicit directives obtained through conversational interactions.We leverage the 3D simulator AI2Thor to create household query scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot using our method CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots) can parse descriptive language queries up to 42\% more reliably than existing LLM-enabled methods by exploiting the ability of LLMs to interpret the user interaction in the context of the objects in the scenario.
%Finally, we confirm our simulation results with a real-world experiment, 
% but we don't report it to you since we are so 
% stingy and mean-spirited.  
% That's why we named our full system JERK CARTIER !
% Also, why are you wasting time reading a mere comment, 
% and why am I wasting time writing this?
\end{abstract}

\section{Introduction}
This paper explores the extent to which natural interaction is possible between human and robot in the context of a navigation task. We seek to answer the question: ``Can a robot infer its task in a navigational context without receiving an explicit command?'' 
Household robotic tasks are often  formulated using imperative commands with a template structure that can be abstracted as ``go-do commands'' (go somewhere, then do something), which are a narrow and
atypical subset of the traditional conversational interactions between humans~\cite{Amazonconvversational,karppi2019non}.
We explore to what extent a robotic assistant can execute 
commands based on conversational instructions akin to those used between humans?  Our existing voice assistants 
to date are so limited in this respect that the notion itself may seem astounding.


In this work, we focus primarily on unstructured variants of the first step (go somewhere) and measure the capacity of the robot to recognize the physical location associated with the intent of the query. While existing voice assistants and robotic
systems are capable of interpreting simple, straightforward specifications (e.g. ``get me a shirt"),  natural language between humans often uses complex descriptors that fall outside of the scope of current systems.
Endowing robots with the ability to  infer a goal from a natural specification is desirable as it provides a more effective mode of interaction for most users and it allows various navigation trade-offs to be optimized by the system (for example path length vs traction vs landmark visibility).



% Figure environment removed

For a robot, planning the navigation target requires a sophisticated understanding of the world which it inhabits. For example, consider the simple-seeming instruction ``Get a healthy snack." To effectively address this request, the robot must know what food the user has available (or at least where it can go to search for food), whether each food item qualifies as a snack, and how healthy each item is. Now imagine the user says something like: ``Am I hungry? I mean, am I really hungry or am I just bored? I don't know! I'm supposed to be on diet. And we just never have anything good to eat. How is it possible that we just got groceries delivered yesterday and we still have nothing good to eat? I think I'm hungry, and dinner is in like an hour."  As little as a year or two ago, the idea that a robot might be able to address such an interaction by navigating to an apple and bringing it to the user seemed unrealistic -- but by leveraging recent progress on several fronts, notably large language models (LLMs), this has become plausible. Significant challenges remain, however, in grounding the reasoning capabilities of LLMs in the reality of the physical world. The aim of this work is to examine how such challenges can be overcome in order to allow LLMs to be used to support such sophisticated interactions.

To study this question, we augment the AI2Thor \cite{ai2thor} simulation environment with implicit  conversational queries for $40$ object types. 
We then introduce CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots), a pipeline that first uses an LLM to infer which object the user is referring to in the query, then uses a ``spatial language index" to associate that object with a location in the scene. To tackle the problem of grounding the LLM in the robot's perception of its environment, we propose a novel method to caption the scene based on the objects the robot has detected within it by exploiting a pre-trained object detector. We then evaluate multiple approaches, of varying complexity, to construct and query the spatial language index.

The contributions of this paper are:
\begin{itemize}
    \item The augmentation of the AI2Thor simulator with three types of queries for $40$ object types: 1) Short explicit queries. 2) Short implicit queries where objects are referred to by their use rather than by their name in one sentence. 3) Long, implicit, and conversational queries that span several sentences, and include a significant amount of distracting content.
    \item The introduction of our method, CARTIER, that can handle all three query types. CARTIER consists of an offline stage where the robot explores the environment to build a spatial language index, and an online stage which processes user queries and relates them to physical points in space. The CARTIER reasoning pipeline is the first to address robot navigation by querying LLMs with holistic, object-centric representations.
    \item The evaluation of our method's performance in conjunction with several spatial language indices.
    \item A proof-of-concept experiment demonstrating the feasibility of CARTIER deployment in a physical robotic system.
\end{itemize}

\section{Related Work}

\subsection{Foundation Models for Open-Vocabulary Language Navigation}
To exploit the association between images and language, we make use of the  visual language model  CLIP~\cite{clip}. CLIP is an acronym for Contrastive Language-Image Pre-Training and it refers to an approach for training large-scale models that can encode the relationship between text and images.  The approach uses a transformer architecture trained on a large dataset of paired images and text and can subsequently predict whether an image and associated caption  are related or not. 

Finding and localizing objects and image features, especially in response to queries, is a  fundamental robotics problem which is often handicapped by a lack of prior knowledge~\cite{pateras1995understanding,sim2001learning,shridhar2018interactive,shridhar2020ingress}. Since the release of CLIP,  several authors have explored how to make use of this powerful vision-language model to enable language-driven navigation instructions using open vocabularies (i.e. not constrained to a fixed set of objects). Several works, including  VLMaps~\cite{vlmaps}, ConceptFusion~\cite{conceptfusion}, and Clip Fields~\cite{clipfields} generate per-pixel or per-voxel embeddings in the CLIP embedding space, which can then be queried using the CLIP language encoder. 
As a consequence of the related embedding spaces, however, these models have the same limitations as CLIP in terms of the sophistication of the query that they can successfully interpret. Some work, including VLMaps~\cite{vlmaps} and LMNav~\cite{lmnav}, use an LLM such as GPT-3~\cite{gpt3} in an effort to parse longer navigation instructions into a sequence of moves which can be executed by the robot. While these authors also make  use of an LLM to parse instructions, the nature of the instructions differs from  those considered in this paper as they   focus on a detailed description of the robot's path  rather than interpreting complex humans queries describing the desired location of the robot.
Clip on Wheels (CoW)~\cite{cows} presents a general framework for CLIP-based object navigation, where a robot navigates using some policy while attempting to localize the object in question using CLIP. As such, some of the papers mentioned earlier in this section can also be considered CoWs, although the precise problem formulation taken by the different authors differs. CARTIER is fundamentally different from these results due to its use of an LLM to interpret the user query. However, most of the work mentioned in this section could be used to implement the navigation component of CARTIER, once the correct object has been identified.

NLMap \cite{chen2023open} augments Say-Can \cite{brohan2023can} with mapping and navigation capabilities, using an open-vocabulary CLIP-based map similar to VLMaps. An LLM is used to make object proposals based on the user query, then the map is searched for the presence of these proposed objects. If any of the objects are located in the map, they are used in the construction of a planning prompt. CARTIER differs in that it uses an object detector to compile a list of all objects in the scene, thereby providing more information to the planner.

Finally, ANSEL Photobot~\cite{ansel} used an LLM to break down enigmatic user queries into ensembles of concepts that can be detected using CLIP, however the focus was on open-vocabulary photo selection rather than navigation. 


\subsection{Captioning for Question Answering}
A key component of the CARTIER pipeline is to leverage an LLM to inform the robot on the contents of the scene. We do this by enumerating the objects present in the scene, having first detected them with a pre-trained object detector. Other work has also explored captioning images \cite{captionvqa} and $3$D scenes \cite{scan2cap}, \cite{sqa3d} in order to support visual question answering (VQA) and situated question answering (SQA), respectively. With all of these approaches, the way the scene is captioned constrains the types of questions that can be answered. SQA3D \cite{sqa3d} proposes to solve a situated question answering task using scene captioning and an LLM, but the captioning style is different from that used in CARTIER, necessitated by the difference in the target task. The task of interest in SQA3D has to do with answering queries related to the spatial arrangement of objects (e.g. ``what is behind the couch?").

There is good evidence that simple queries often yield unsatisfactory results and these can be enhanced via a more natural or conversational paradigm~\cite{keyvan2022approach,lopez2018alexa}.
A more complete coverage of the long history and relationship between natural and expressive query languages, and data collecting agents, most of which relates to Internet search is outside the scope of what we can cover here~\cite{lieberman2001exploring,hu2016natural,allen1983recognizing,lieberman1995letizia}.


\section{Dataset}
\label{sec:dataset}
The AI2Thor simulation environment contains $189$ different object types. We select a subset of $40$ objects and create three different types of queries (see Section \ref{sec:three_query_types} for details). Since the objects are distributed across many different scenes in different combinations, we can evaluate the same query across many different scenes. In this paper, we consider $7$ scenes over which we evaluate our approach. Certain queries may plausibly be answered by multiple objects, so the alternative answers for each query are also included in our dataset. Using ground-truth object bounding boxes from the simulator, we compute the minimum distance between the predicted position and any of the bounding boxes of objects that are considered to be plausible answers to the query. Using the distance as a performance metric rather than the object identity is convenient because it supports comparison to methods which do not explicitly output the identity of the object being searched for.
The environment exploration phase is achieved by teleoperating a mobile robot, which collects RGBD frames from a camera mounted on the robot at approximately $1.5$ meters from the ground. 

\subsection{Query Types}
\label{sec:three_query_types}
The queries are split into $3$ categories with increasing complexity: explicit, implicit, and conversational.  Explicit queries are straightforward requests to go to some explicitly-named object (e.g. get the scissors), and can be successfully answered using an object detector or CLIP-based methods in the open vocabulary case. Implicit queries refer to an object implicitly by expressing some need of the user which could be addressed with that object, e.g. ``I need something to cut this paper with." The Implicit queries in our dataset are short (one or two short sentences) and to the point, focused entirely on discussing the need which is to be serviced. These cannot be answered with an object detector, since there is no straightforward way to extract the object of interest from the utterance, but in some cases may still be answered satisfactorily by CLIP, which is likely to learn to embed ``scissors" nearer to the phrase ``cut paper" than any other object. Our third class of queries, Conversational, are comprised of several sentences and may contain a significant amount of information that is irrelevant to the need expressed by the user. For example, consider the utterance ``Wow, my mom will really love this card I'm making her for her birthday. It has all of her favorite things on it -- lilies, rabbits, birds. I think I'll make it a little smaller than a full page though, so I guess I'll need something to cut the cardstock with. Oh, I do hope she likes it", as depicted in Fig.~\ref{fig:query_types}. There are several challenges associated with using CLIP to answer this query directly. First, the length of CLIP's sentence embeddings is limited to $77$ tokens, meaning it may not even be possible to encode the entire text of the query. Second, CLIP will attempt to incorporate all of the concepts expressed in the utterance into the embedding (mom, birthday, lilies, rabbits, birds, etc), the majority of which are irrelevant to the need of the user. In order to interpret the query, and estimate  what the user is asking for, an LLM capable of processing lengthy pieces of text is required. These conversational queries are quite different from the manner in which people must speak to today's voice assistants. Based on the evidence from information retrieval, natural queries should enhance usability and effectiveness~\cite{keyvan2022approach}.

% Figure environment removed

\section{Method}
In this Section, we introduce CARTIER, a framework that can interpret complex user language instructions to generate navigation targets. CARTIER is composed of six parts, summarized graphically in Figure \ref{fig:pipeline}:
\begin{enumerate}
    \item Environment exploration
    \item Object detection and spatial language index construction
    \item LLM query construction
    \item Querying the LLM
    \item Querying the spatial language index
\end{enumerate}

The rest of the this section explores each of these parts in more detail.
\subsection{Environment exploration}
The robot navigates the environment collecting RGBD video, as well as the associated pose in which each video frame was collected. In the simulated environment, the robot pose is obtained from the simulator directly. %In the real environment, we use DROID-SLAM \cite{droidslam} in order to estimate the poses. 
Depth information and 6-DOF pose is required in order to maximize the accuracy of object localization, however certain versions of CARTIER, presented in the following sections, can work using only a monocular camera and 2-DOF pose estimates.
%In our experiments, we control the robot using human teleoperation for data collection, but this could be substituted with any other method, as long as sufficient coverage of the scene is obtained.

\subsection{Object detection and spatial language index construction}
\label{sec:object_detection_and_spatial_index}
CARTIER populates the LLM query with a list of objects in the scene, so we use an off-the-shelf object detector (EVA \cite{EVA}) trained on the LVIS dataset \cite{lvis} that contains $1204$ object categories. We run the detector on each of the frames in the video and compile a list of all LVIS objects encountered by the robot (with confidence score $> 0.8$) during the exploration phase. Though we elect to use an object detector with a fixed vocabulary, CARTIER could potentially handle open vocabulary queries. See Section \ref{sec:query_the_llm} for more discussion of this point.


A ``spatial language index" is a map that allows for a lookup of positions given an object name. We explore three such indices in this work. In order of descending order of complexity, the first is VLMaps \cite{vlmaps} which creates a 2-D top-down grid of the environment, where each grid cell represents a CLIP embedding (calculated via LSeg \cite{lseg}) averaged over each point projected into the grid. The second involves selecting the frame with the largest bounding box for the given object (a heuristic for proximity to the object), then using the depth signal in combination with the object bounding box to determine the position of the object. The third method returns the position of the robot at the time that the frame with the largest object bounding box was observed. We improve the accuracy of our proximity heuristic (maximize bounding box size) by partially compensating for perspective distortion using a transform based on the weak perspective camera model. With $x_t$ being the position of a pixel in the transformed image, $x$ being its position in the original image, and $d$ being the focal distance, the transform is given by:
\begin{equation}
    x_t = sin(tan^{-1}(\frac{x}{d}))
 \end{equation}

% \footnote{I am sure I did a bad job on the description of the transform and the equation. FIXIT after submission.}
We refer to these methods as VLMaps, ObjectDepth, and ObjectViewpoint, respectively.

\subsection{LLM query construction}
\label{sec:query_construction}
We wish for the LLM to select the most relevant object in the scene that fits the user query. The LLM is informed of the contents of the scene via the inclusion of a comma separated list of the objects that were detected. Each object type is only listed once, even if multiple instances were detected. In order to condition the correct behavior, the LLM is prompted with a short description of the task as follows:


% Figure environment removed


This prompt is targeted at LLMs fine-tuned for instruction following using RLHF (Reinforcement Learning from Human Feedback), such as ChatGPT and GPT-4. Unlike prompts for earlier text completion models (such as GPT-3) which often require several examples of the desired behavior in order to achieve performance \cite{ansel}, this prompt is terse, containing only a description of the desired behavior. By keeping the prompt
short and high-level, we avoid the risk of over-engineering the model's behavior to a particular case, leaving space for the model to apply its own best judgement to answer the query.


\subsection{Query the LLM}
\label{sec:query_the_llm}
We query the LLM with temperature set to 0 in order to sample maximum likelihood tokens and minimize the stochasticity of our results. In our experiments, we use ChatGPT and GPT-4. We do not specify stop tokens. Empirically we found that both models were reliable in terms of answering each query with a single object. In some cases, some minimal parsing was required to extract the identity of the objects due to responses such as ``In order to fulfil the user request, the robot should navigate to the object called `Bed'".

Because CARTIER forces the LLM to select from a fixed set of objects known from the object detector, one could erroneously assume that CARTIER is a closed-vocabulary method. For an illustrative example of why this is not true, consider a query in which the user wants a tamper (an object used to compress coffee when making espresso), but ``tamper" is not contained in the vocabulary of the object detector. An LLM can use its significant world-knowledge to infer that the tamper is likely to be found near the coffee machine, and direct the robot to the correct location. In fact, even if the tamper had been hidden from the view of the robot during the exploration phase, it would still likely find the right answer, as long as a reasonable guess can be made about where to find the object in question.


\subsection{Query the spatial language index}
In order to query the VLMaps index, we embed the object name using CLIP and then select the pixel with the highest similarity, after first dropping the large embedding dimensions following \cite{ansel}. The ObjectDepth and ObjectViewpoint indices are queried as described in Section \ref{sec:object_detection_and_spatial_index}.


\section{Evaluation}
We evaluate CARTIER by running it on the dataset described in Sec \ref{sec:dataset} using both ChatGPT and GPT-4 as the LLM. We measure both the correctness of the LLM output and the distance to the target object achieved by the spatial language index. We compare against two recently published baselines. Details are provided below.


\subsection{Baselines}
We consider two baselines, which we refer to VLMaps-baseline and NLMap. VLMaps-baseline consists of querying the VLMaps spatial language index with the user query directly. If the user query is longer than 77 tokens, it is truncated. The NLMap baseline uses the prompt from \cite{chen2023open} in order to generate object proposals. The generated proposals are then queried against the VLMaps spatial language index, and the similarity value to the nearest-match grid square for each proposal is thresholded (threshold value of 12.05) to generate the list of object proposals found in the scene. These proposals passed to the LLM along with the CARTIER prompt described in Section \ref{sec:query_construction} in order to select the final object. This method can only be used with the VLMaps spatial language index, as it produces open vocabulary outputs which are not guaranteed to align with the vocabulary of the object detector used to implement ObjectDepth and ObjectViewpoint.

\subsection{Metrics}
\label{sec:metrics}
Each query in the dataset has an associated list of objects in the scene which may be reasonable answers to the query. The first metric (referred to as object-match) measures the correctness of the text response produced by the LLM, i.e. whether the object it has responded with is in the list. This evaluation is performed manually, as the automation of evaluation presents two challenges. The first is the presence of synonyms (e.g. ``trash can", ``trash bin", ``garbage can", etc.) in natural language. This is not such an issue for CARTIER, which selects object from a fixed vocabulary, but presents more of a challenge for evaluating NLMap, an open-vocabulary method. The second is co-location of multiple objects. For example, the target object may be a "sink" but the LLM selects "faucet." This should be counted a success, since the mapping between sinks and faucets is (usually) 1:1. On the other hand, if the target object is ``bed" but the LLM selects ``sheets" this a failure, since sheets are sometimes co-located with beds, but are also sometimes found in closets. Manual evaluation allows for a case-by-case approach to these matters. Note that metric is not applied to the VLMaps baseline as it does not produce a text-based answer.

The second metric (referred to Euclidian-distance) measures the distance (in meters) between the inferred object location and the nearest ground-truth object in the scene. This metric avoids any issues related to object co-location -- if the sheets are indeed on the bed, both answers are equally good. The downside is that it conflates the performance of the LLM and the spatial language index, therefore the two metrics are complementary.



\section{Results}

\begin{table*}[htbp]
\vspace{3mm}
    \centering
    \renewcommand{\arraystretch}{1.5}
    % \begin{tabularx}{\textwidth}{|X|X|X|X|X|X|X|}
    \begin{tabular}{|>{\centering\arraybackslash}m{1.8 cm}|>{\centering\arraybackslash}m{1.8cm}|>{\centering\arraybackslash}m{1.8cm}|>{\centering\arraybackslash}m{2.2cm}|>{\centering\arraybackslash}m{1.8cm}|>{\centering\arraybackslash}m{1.8cm}|>{\centering\arraybackslash}m{1.8cm}|}
        \hline
        \rowcolor{lightgray}
        \textbf{Model} & \textbf{Type} & \textbf{CARTIER}  & \textbf{CARTIER} & \textbf{CARTIER} & \textbf{NLMap} & \textbf{VLMaps}\\
        \rowcolor{lightgray}
                \textbf{} & \textbf{} & \textbf{ObjectDepth} & \textbf{ObjectViewpoint} & \textbf{VLMaps}& \textbf{} & \textbf{Baseline}  \\
        \hline
gpt4 & conversational& \textbf{1.20} & 1.94 & 1.70  & 2.11 & 2.35 \\ \hline
chatgpt & conversational & \textbf{1.39} & 2.04 & 1.78 &  2.28 & 2.35 \\ \hline
gpt4 & implicit & \textbf{1.17} & 1.89 & 1.66  & 2.09 & 2.27 \\
chatgpt & implicit & \textbf{1.20}& 1.80 & 1.69  & 2.22 & 2.27 \\ \hline
gpt4 & explicit & \textbf{1.09} & 1.91 & 1.70  & 2.02 & 1.93 \\ \hline
chatgpt & explicit & \textbf{1.21}& 1.96 & 1.71  & 1.89 & 1.93 \\ \hline
 \hline
    \end{tabular}
\caption{\label{tab:mean_results} 
Mean (over all scenes and tasks) distances, in meters, between predicted object location and ground truth object location for a collection of different methods, question types, and language models.
}
\end{table*}

% Figure environment removed

The results according to the Euclidian-distance metric are presented in Table \ref{tab:mean_results}, while Figure \ref{fig:object_matching} summarizes results for the object-match metric. 
We observe that CARTIER is significantly stronger than other baselines according to the euclidean-distance metric, and also that this gap is more pronounced for the conversational queries, which are the most challenging. The ObjectDepth spatial language index appears to be significantly better at localizing objects than VLMaps, suggesting that open-vocabulary mapping is not always necessary. ObjectViewpoint was the weakest spatial language index, but this is not surprising because it is only expected to be accurate when the robot has passed very near the object on its exploration trajectory.

Figure \ref{fig:object_matching} also confirms the superiority of CARTIER on the challenging conversational queries. CARTIER's performance using GPT4 on all three query types is comparable, suggesting that the limiting factor is the object detector, rather than the reasoning capabilities of the LLM. NLMap beats CARTIER on explicit queries in terms of the object-matching metric, but loses on euclidean-distance. This suggests that CARTIER is able to exploit object co-location (as described in Sections \ref{sec:query_the_llm}  \ref{sec:metrics}) and/or more robustly localize the objects it does find.

On the whole, these results help to confirm the underlying CARTIER hypothesis -- as queries become more complex and nuanced, contextualizing the LLM-based decision making process withe details about the contents of the scene increases the LLM's ability to correctly resolve the query.






\section{Real-world deployment}

In this section, we deploy  CARTIER in a real-world scenario where a telepresence robot (Ohmni by Ohmnilabs) is given human queries and must navigate to the appropriate location. In Fig.~\ref{fig:real_world}, a robot is given the conversational query ``I have so much work to do. Looks like I'll be working late into the night. I am guessing the presentation would take me another two hours or so to finish. But I am getting really sleepy. We better do something about that.'' Using CARTIER, the robot determines that within all the objects detected in the environment, the coffee machine best matches the human's intent and can navigate to the coffee machine's position using CARTIER's perception pipeline. In this proof-of-concept, we generate the exploration trajectory by teleoperating the robot during which we record $394$ images using a Samsung S23 Ultra mobile phone. The pose trajectory of the robot and the depth images are generated from the sequences of RGB frames using the software package ARCore by Google. 


% Figure environment removed

\section{Conclusion}
The existing work on the value and impact of conversation search for the Internet~\cite{keyvan2022approach} suggests that 
users are also likely to desire more natural, conversational interactions with robot assistants in the near future. In this work, we took a step towards enabling those types of interactions by introducing CARTIER, a method which fuses power of modern LLMs and state of the art object detectors. 

CARTIER's approach of providing the LLM decision maker with a detailed description of the scene contents represents an alternative approach to those described in work such as NLMap, where the LLM must first hypothesise which objects are useful given the input query. Our results show that this approach provides more relative benefit as the complexity of the queries increases.

By allowing users to specify ``go-do tasks'' in more natural ways, we hope that the utility and impact of robot navigation agents will be greatly enhanced.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,biblio}


\end{document}
