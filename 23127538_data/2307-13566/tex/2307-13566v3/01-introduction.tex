\section{Introduction}

With the deployment of imperfect artificial intelligence (AI) in high-stakes decision-making scenarios, decision-makers struggle with knowing when they should and should not rely on AI advice, causing frustration and resulting in potentially harmful decisions. As a result, designing and developing human-centered explanations has become a core theme in computer-supported cooperative work (CSCW) and human-AI collaboration research~\cite{wang2019designing, liao2021human,ehsan2022human}. Tangentially, recent work has proposed new explanation techniques that leverage machine learning models to explain the prediction of another machine learning model~\cite{bertaglia2023closing,kayser2022explaining,hendricks2021generating,tanida2023interactive,warburg2021bayesian,barata2021improving,sadeghi2018users,yang2023harnessing,tursun2023towards,kroeger2023large,sovrano2023toward}. A subset of these studies propose to exploit language models to generate natural language explanations for image classifications with the rationalization that natural language is more ``human-friendly''~\cite{hendricks2021generating,kayser2022explaining,tanida2023interactive}. Aside from natural language explanations, another subset of recent work proposes advanced content-based image recognition techniques to generate example-based explanations~\cite{warburg2021bayesian,barata2021improving,sadeghi2018users}. These types of approaches to explainability introduce another level of uncertainty in the collaboration between the decision-maker and the AI, as explanation models are imperfect.

% \niklas{Good summary. We could additionally make a 2x2 matrix with AI correct / incorrect and XAI correct / incorrect and then list existing work and draw a rectangle around everything and label it "this work"}

% \begin{table}[htbp!]
% \captionsetup{justification=centering}
% \caption{XAI research fields that are under-explored. \katelyn{come back to the design of this? Think about excluding it if we leave out related work. Replace table with explicit point in the text}}
% \begin{threeparttable}
% \begin{tabular}{P{1.5cm} P{3cm} P{3cm}}\hline
%  &  \multicolumn{2}{c}{AI explanation}  \\ \hline 
%  \multicolumn{1}{P{1.5cm}}{} & & \\ \cline{3-3}
% \multicolumn{1}{P{1.5cm}}{\multirow{5}{*}{AI advice}}  & \textit{Correct advice Correct explanation}  & \multicolumn{1}{|P{3.1cm}|}{\textit{Correct advice Incorrect explanation}}    \\  
%  \multicolumn{1}{P{1.5cm}}{}&&\multicolumn{1}{|P{3cm}|}{} \\ 
%  \multicolumn{1}{P{1.5cm}}{}& \textit{Incorrect advice  Correct explanation}   & \multicolumn{1}{|P{3cm}|}{\textit{Incorrect advice  Incorrect explanation}} \\ \cline{3-3}
%  \multicolumn{1}{P{1.5cm}}{}& & \multicolumn{1}{R{3cm}}{*} \\\hline
% \end{tabular}
%     \begin{tablenotes}
%         \item[*] Our research focus
%     \end{tablenotes}    
% \end{threeparttable}

% \label{research_focus}

% \end{table}


% \begin{table}[htbp!]
% \captionsetup{justification=centering}
% \caption{XAI research fields that are under-explored.}

% \begin{tabular}{m{2cm} P{2cm} P{2cm} P{2cm}}\hline
%  & &  \multicolumn{2}{c}{XAI advice}  \\ \cline{3-4} 
%  & & Correct explanation & Incorrect explanation \\ \hline \\
% \cline{4-4}
% \multirow{2}{c}{AI advice}  & Correct advice   & bla   & bla    \\
% \multicolumn{1}{|c|}{bla}   & bla   & bla   & bla \\ 
% \multicolumn{1}{|c|}{ble} & ble & ble & bla \\ \cline{1-1}
% bla   & bla   & bla   & bla   \\ \hline
% \end{tabular}

% \label{mod_anal_nle_rair}

% \end{table}

CSCW, and more specifically human-AI collaboration, is prevalent across high-stakes scenarios~\cite{cai2019hello, wang2019human, lindvall2021rapid, tschandl2020human}. For instance, radiologists collaborate with AI to identify abnormalities in medical imagery~\cite{tschandl2020human}; conservationists use AI to help monitor biodiversity~\cite{berger2017wildbook}, and humanitarian aids use AI to help identify damaged buildings after natural disasters or armed conflicts from satellite imagery \cite{zhang2019crowdlearn, morrison-xai-23}. While some of these human-AI collaboration scenarios require the human decision-maker to have several years of experience in the given domain, such as radiology, monitoring biodiversity with the help of AI doesn't necessarily require domain expertise~\cite{berger2017wildbook,paxton2019citizen}. Platforms, such as iNaturalist~\cite{iNaturalist}, Merlin Bird App~\cite{merlin}, and Wildbooks from WildMe.org~\cite{Wildme}, have allowed non-experts (\emph{i.e.}, citizen scientists, hobbyists, or students) to partake in monitoring biodiversity alongside domain experts (\emph{i.e.}, ornithologists and conservationists). While these platforms are valuable for non-experts to use, the AI models backing these platforms are imperfect: they do not always provide correct predictions~\cite{kocielnik2019will}. 
% \katelyn{maybe there is a better paper to cite for defining imperfect AI, but this is the closest, most relevant one I could find.}

Experts and non-experts interacting with the same imperfect AI and the same type of explanations in human-AI collaboration scenarios, such as decision-making \cite{schemmer2022meta} or learning systems \cite{spitzer2023ml}, could result in some users misunderstanding or inappropriately relying on/overriding the AI advice. Experts may have more context outside of the AI's classification and confidence that a non-expert may not have. This might result in experts using their context information to appropriately rely on the AI when advice is provided, such as correctly overriding when wrong AI advice is presented and correctly using AI advice when it is correct. Non-experts, on the other hand, might not be able to judge the correctness of the AI advice appropriately as they are missing this context information. For example, for bird species identification, ornithologists tend to be more aware of information related to the visual differences between the male and female birds for a given species, the bird's habitat, and migration patterns, whereas non-experts may not know some or all of that information. This same situation can arise in radiology where residents (``non-experts'') may initially be less familiar with certain diseases than an attending radiologist (``experts''). However, both experts and non-experts can struggle to identify certain instances. In this case, collaborating with AI can result in complementary team performance (CTP), leveraging the unique knowledge of both humans and AI, resulting in the human-AI team's task performance being better than the human or AI alone~\cite{hemmer2021human}.

Inappropriate reliance can also occur in the presence of \textit{imperfect XAI}, a term that we introduce to represent the phenomenon where an explanation reveals evidence that does not necessarily comply with the prediction. ~\citet{papenmeier2019model} use the term `explanation fidelity' while ~\citet{kroeger2023large} use the term `faithfulness' to measure how ``truthful'' an explanation is. We use imperfect XAI to align with existing terms, such as imperfect AI, in the CSCW and human-computer interaction (HCI) communities. Specifically, we define imperfect XAI as explanation techniques that can potentially provide explanations that do not fit with the AI's predictions. We view explanation fidelity or faithfulness as a term that can be used under the umbrella term of imperfect XAI; we view explanation fidelity as referring to the continuum of explanation correctness, such as when explanations are partially correct.
% \footnote{\kmdelete{~\citet{papenmeier2019model} use the term `explanation fidelity' to measure how ``truthful'' an explanation is. We use imperfect XAI to align with terms that already exist, such as imperfect AI, in the CSCW and human-computer interaction (HCI) communities. Furthermore, we view imperfect XAI to generally refer to when an explanation is incorrect or correct. We view explanation fidelity to refer to the in-between cases, such as when an explanation is partially incorrect.}} 
Imperfect XAI can exist regardless of whether the AI's advice is correct or not. AI explanations may oversimplify or improperly estimate complex models in order to make them more interpretable, deceiving and misleading the human decision-maker. As a result, non-experts may be more prone to under- or over-relying on AI advice in the presence of incorrect explanations. Within knowledge transfer scenarios, this could cause the non-expert to learn incorrect information about a given class. Furthermore, previous research shows that the language tone within natural language explanations can impact decision-makers \cite{calisto2023assertiveness}. However, the effect of language tone on humans' appropriate reliance when interacting with imperfect explanations is underexplored. Therefore, it is necessary to investigate how the communication style of explanations (\emph{e.g.}, the language tone\kmedit{, the information provided, and their representations}) impacts human-AI collaborations across levels of expertise ~\cite{kim2023communicating,calisto2023assertiveness}.

Collaborating with imperfect AI is not a new concept to the CSCW and HCI communities~\cite{hemmer2021human,lindvall2021rapid,kocielnik2019will}. Despite numerous user studies over the years investigating human-AI collaborations and XAI, few have formally acknowledged the existence of imperfect XAI in their studies~\cite{papenmeier2019model,hemmer2021human}. By formally acknowledging the existence of imperfect XAI in human-AI collaboration, research has several new interesting dimensions to explore. Although numerous user studies seek to understand how humans align, perceive, and interact with different types of explanations in various human-AI collaboration scenarios (\emph{e.g.}, ~\cite{chen2023understanding,kim2023help}), few studies explore the impact that incorrect or ``noisy'' explanations have on human-AI collaboration~\cite{vasconcelos2023generation,jang2023know,papenmeier2019model,hemmer2021human}. Recent work has used technical approaches to mitigate ``noisy'' or incorrect natural language explanations~\cite{jang2023know}, and ~\citet{kroeger2023large} propose metrics to algorithmically evaluate the effectiveness of the generated post-hoc natural language explanations. However, to our knowledge, no studies investigate how the interaction between the correctness of explanations and the decision-maker's level of expertise impact appropriate reliance on AI, human-AI team performance, and the extent to which AI explanations deceive decision-makers. 

% Natural language explanations can involve a lot more than just rationalizing a prediction, though. For example, the assertiveness of a message (or in our case a natural language explanation) can potentially impact how that message is interpreted and perceived~\cite{baek2015environmental}. Furthermore, the information within the natural language explanation can be full of jargon from the perception of a non-expert which could make the explanation misleading or confusing. Another explanation technique that has been under-explored is example-based explanations~\cite{cai2019effects}. These explanations may be less informative than the information presented in a natural language explanation. However, they may be more natural to non-experts providing the ``right'' information for the non-expert to appropriately agree or disagree with the AI. 

With the growing use of machine learning models to explain other machine learning models in high-stakes decision-making scenarios, we argue that it is necessary to understand how humans interact with explanations that are incorrect, even when the AI's advice is correct. We also argue that it is important to understand the relationship that the level of expertise and the tone of explanations (\emph{i.e.}, assertive, non-assertive, or neutral) have on a decision-maker's reliance on AI. Understanding these dimensions of human-AI collaboration will provide insight to XAI and CSCW researchers. With these topics under-explored in current literature, we present the following research questions:

\begin{req}[leftmargin=1.06cm, labelindent=0pt, labelwidth=0em, label=\textbf{RQ\arabic*}:, ref=\arabic*]
    \item How does the correctness of explanations affect appropriate reliance on AI, and to what extent do the decision-maker's level of expertise and the explanation's assertiveness moderate this effect? \label{rq1}
    \item How is complementary team performance impacted by the correctness of explanations and the decision-maker's level of expertise? \label{rq2} 
    \item How do different types of explanations change the effect that the correctness of explanations has on appropriate reliance and complementary team performance? \label{rq3} 
    \item To what extent do incorrect and correct explanations deceive decision-makers with different levels of expertise? \label{rq4}
\end{req}

To address our research questions, we employ an imperfect AI model for a bird species identification task~\cite{hendricks2021generating}. We focus on bird species identification because the use of AI for wildlife conservation efforts among experts and non-experts is a rapidly growing field in research and practice~\cite{tuia2022perspectives}. \kmedit{Furthermore, it is less difficult to find people with varying levels of expertise in birding than in radiology who will have time to participate in our study.}

Through a mixed-methods study, we answer our research questions by asking participants to classify bird images in two phases: without any advice from AI (phase 1) and then showing the AI's advice and explanation (phase 2). To answer \textbf{RQ} \ref{rq1}, we design a research model based on phenomena from relevant research in CSCW and conduct rigorous moderation analyses based on \cite{hayes2017introduction}. Our analyses leverage the appropriate reliance metrics defined by ~\citet{schemmer2023appropriate}. We design our study to be within-subjects for the correctness of the explanation and the assertiveness of explanations allowing us to answer \textbf{RQ} \ref{rq2} and between-subjects for the explanation modality allowing us to answer \textbf{RQ} \ref{rq3}. Moreover, we calculate the magnitude of deception caused by incorrect explanations compared to correct explanations across both explanation modalities and levels of expertise to account for the impact of imperfect XAI on humans' decision-making behavior. This measurement gives us insight into \textbf{RQ} \ref{rq4}.
Lastly, we conduct an inductive content analysis based on \citet{gioia2013seeking} to assess the open-ended responses from participants to gain insight into designing for imperfect XAI in human-AI collaborations.

As a result of our study, we contribute the following to the CSCW community:
\begin{itemize}
    \item \textit{Research Model for Human-AI Collaboration with Imperfect XAI}: We propose a research model for the moderating roles of the decision-maker's level of expertise and the explanation's assertiveness on the effect of the correctness of explanations on appropriate reliance.
    \item \textit{Novel Empirical Study}: To validate our proposed research model, we conduct the first empirical investigation that explores the moderation of assertiveness of explanations and the level of expertise on the impact that the correctness of explanations has on appropriate reliance. We do this on a human-AI collaboration scenario across two different modalities of explanations: natural language explanations and visual, example-based explanations. We also investigate the impact on complementary team performance. Our findings inform designers of human-AI collaboration systems on how to deploy imperfect XAI from a user-centric perspective.
    \item \textit{Novel Metric for Impact of Imperfect XAI:} We contribute a novel metric to the human-AI decision-making field accounting for the impact of incorrect explanations on humans' decision-making behavior when collaborating with AI. Specifically, we propose the Deception of Reliance (DoR) caused by imperfect XAI. With DoR, we investigate to what extent imperfect XAI deceives humans.
    \item \textit{Qualitative Insights}: We provide insights on how decision-makers, regardless of expertise, prefer the tone of explanations to align with factors related to the AI's behavior and the impact on decision-makers. These insights can inform future works to conduct new evaluations.
\end{itemize}

The remainder of this article is structured as follows: We present related literature on imperfect AI systems, human-AI collaboration, and explainability (\Cref{related_section}, p. \pageref{related_section}) before outlining our theoretical development (\Cref{theoretical_section}, p. \pageref{theoretical_section}) and methodology for conducting an empirical study (\Cref{methodology}, p. \pageref{methodology}). We then present the results of our work for two different types of explanations (\Cref{results_section}, p. \pageref{results_section}). After that, we discuss the implications (\Cref{discussion_section}, p. \pageref{discussion_section}) and limitations (\Cref{limitations}, p. \pageref{limitations}) of our findings. Finally, we end our article with a brief conclusion (\Cref{conclusion_section}, p. \pageref{conclusion_section}).