\section{Related Work} 
\label{related_section}

 We situate our contributions in relation to past work about decision-making with imperfect AI/XAI, the impacts of end-user expertise on human-AI collaboration, and the impact of explanation type on human-AI collaboration. 

\subsection{Decision-Making with Imperfect AI/XAI}

% \katelyn{Shared interest work} 
Numerous studies in CSCW and HCI have investigated the impact that imperfect AI has on human-AI collaboration (\emph{e.g.}, ~\cite{kocielnik2019will, bansal2021does,vasconcelos2023generation}). 
~\citet{kocielnik2019will} offer three techniques for setting user expectations about the performance of an imperfect AI system, including an accuracy indicator, example-based explanations, and performance control. Through a user study with an AI-powered scheduling assistant, the authors demonstrate the efficacy of their techniques in maintaining user satisfaction and acceptance. The authors also demonstrate that the nature of system errors can impact user perception.

Several recent studies investigate how programmers collaborate with Copilot, an imperfect AI programming assistant (\emph{e.g.}, ~\cite{vasconcelos2023generation,barke2023grounded,dakhel2023github}). One of those studies specifically looks at how to convey the uncertainty of outputs from Copilot~\cite{vasconcelos2023generation}. By highlighting code that is most likely going to be edited by the programmer instead of highlighting based on the probability of the code being generated, they observe that programmers arrive at solutions faster. Furthermore, ~\citet{dakhel2023github} conclude that GitHub Copilot is valuable for expert programmers but something non-expert programmers should be cautious about.

Previous studies explore the impact that revealing the confidence of the model's prediction has on the human-AI team (\emph{e.g.}, \cite{kim2020effect,tejeda2023displaying, bansal2021does}). For example, \citet{kim2020effect} investigate the effect of various framings and timings for presenting the performance of an AI system on user acceptance. Through their user study, the authors reveal that users find AI advice to be more reasonable when it is not accompanied by information about AI system performance than when it is. In the case that AI system performance is shown, users consider AI advice to be more reasonable when system performance is displayed before they make a decision rather than afterward. However, communicating uncertainty for image classification in a visual format is under-explored. Recent work conducts a user study to see how showing the confidence of an AI prediction through a green hue on an image impacts reliance on AI~\cite{tejeda2023displaying}.

Fewer studies investigate the impact that imperfect XAI has on human-AI collaboration~\cite{papenmeier2019model}.
Similar to our contributions, ~\citet{papenmeier2019model} investigate the impact that explanation fidelity has on user trust. They present a user study where participants collaborate with AI of different accuracies and XAI with different levels of correctness to determine if a Tweet should be published or not based on its content. While ~\citet{papenmeier2019model} investigate how an explanation's level of correctness impacts trust, they do not explore the role that a user's level of expertise plays.  


\subsection{Domain Expertise \& Human-AI Complementarity}

There has been a growing interest in understanding the impact that the decision-maker's domain expertise has on human-AI collaborations~\cite{nourani2020role,calisto2023assertiveness,szymanski2021visual,dikmen2022effects,tschandl2020human,ford2022explaining,bayer2021role,zhao2023exploring,ooge2021trust}. One recent study investigates the impact of decision-makers' domain expertise on task accuracy in a high-stakes human-AI collaboration task~\cite{calisto2023assertiveness}. ~\citet{calisto2023assertiveness} also look at the impact that the assertiveness of natural language explanations has on human-AI collaboration. In their study, they present natural language explanations with varying levels of assertiveness to radiologists with different years of experience on a mammogram classification task. Their main analysis consists of the radiologists' task performance. Unlike ~\citet{calisto2023assertiveness}, our experiment collects the human's initial decision before showing the AI's advice to the human, allowing us to measure appropriate reliance and assess for complementary team performance.

One study investigates how the level of expertise for Arabic or Indian Numerals from various versions of MNIST impacts task accuracy and model perception~\cite{ford2022explaining}. A similar study shows clinicians with various levels of expertise four different types of explanations, including visual, example-based explanations~\cite{tschandl2020human}. Similar to our study design, they show participants the three most similar example images for the example-base explanations. Another study investigates how practitioners with different levels of expertise perceive explanations that were implemented in a manufacturing industry context~\cite{zhao2023exploring}. They observe that practitioners with a higher level of expertise are more accepting of the explanations. Recent work proposes a research model to identify the impact decision-maker's level of expertise has on trust in XAI~\cite{bayer2021role}. Their research model does not consider the correctness or tone of explanations. Through their online, AI-supported chess experiment, they observe that expertise negatively affects trust. 

While numerous previous works investigate the impact of domain expertise, to the best of our knowledge, none explore the impact of the correctness of explanations and the level of expertise on the decision-maker's reliance behavior together.


\subsection{Explanation Modality}

In human-AI collaboration scenarios, the human decision-making behavior depends on the type of explanation provided (\emph{e.g.}, ~\cite{humer2022comparing}).
To validate why we evaluate our research model for two different types of explanations (\emph{i.e.}, natural language and visual, example-based), we synthesize previous work that compares multiple different modes of XAI. 

% Previous literature across different fields of machine learning has proposed generating natural language explanations. For example, Hendricks et al. propose an inherently interpretable model that generates visual explanations for fine-grained image classification where the visual explanations are natural language explanations that are class- and image-relevant~\cite{hendricks2021generating}. The authors evaluate their method on the CUB-200-2011 dataset~\cite{WahCUB_200_2011}. \katelyn{mention the small user study they performed and what they specifically looked at in their user study.} Similarly, Kayser et al. propose a model that classifies chest x-ray images and generates a natural language explanation for each classification~\cite{kayser2022explaining}. The authors showed a subset of the generated natural language explanations to a clinician and asked them to determine how accurate they are on a Likert scale. 

Several studies investigate the use of example-based explanations in human-AI decision-making~\cite{cai2019effects,humer2022comparing,chen2023understanding,du2022role,yang2020visual}. ~\citet{cai2019effects} propose normative and comparative explanations, different types of example-based explanations. They evaluate how these explanations impact end-users' understandability and perception of the AI model in a drawing guessing game. The authors find the normative explanations to help users better understand how the AI makes decisions when the model prediction is incorrect. Another paper investigates example-based explanations in a slightly different format from Cai~\cite{yang2020visual}. They similarly found the example-based explanations improve the users' appropriate trust in the classifier. 

In a different study, \citet{du2022role} examines the effect of different explanation modalities on clinical practitioners' reliance behavior. The authors show no significant differences between example-based explanations and feature-based explanations. However, they find that different types of practitioners prefer different modalities from a user-centric perspective. 
% moved this paragraph from below -- nothing else has changed within the paragraph
More recent work compares example-based explanations to feature importance through a think-aloud study~\cite{chen2023understanding}. From their mixed-methods study, ~\citet{chen2023understanding} outline three types of intuition that are employed when decision-makers reason about AI predictions and explanations, including task outcomes, features, and AI limitations. The authors use these three intuition types to explain study results in which feature-based explanations lead to overreliance on AI while example-based explanations improve human-AI performance.

Several recent works have compared text-based explanations to visual explanations (\emph{e.g.}. ~\cite{kim2023should,robbemond2022understanding,szymanski2021visual}). For example, \citet{kim2023should} analyze a unified explanation technique from a human-centric point of view. In their work, \citet{kim2023should} explore visual and text explanations in a user study. They investigate users' preferences for different AI interfaces. The authors conclude that users prefer local visual explanations in such interfaces over text-based ones. Another study compares six different types of explanation modalities in an extensive user study \cite{robbemond2022understanding}. \citet{robbemond2022understanding} look into the impact of text, audio, graphics, and combinations of the previous modalities on decision-makers' reliance on decision support systems. Their results show that combinations of different explanation modalities lead to higher user performance. ~\citet{szymanski2021visual} conduct a similar study, only evaluating visual and textual explanations. 

Based on findings from previous studies that evaluate the impact of various explanation modalities on human-AI collaboration, we choose to explore visual, example-based explanations and natural language explanations.