\section{Discussion}
\label{discussion_section}

% this paper might be valuable to cite or pull into discussion: https://www.nature.com/articles/s41746-022-00737-z

We investigate how imperfect XAI impacts humans' decision-making when collaborating with an AI. More precisely, we assess how imperfect explanations affect humans' reliance behavior and investigate the effects on the human-AI team performance. To answer RQ \ref{rq1} and RQ \ref{rq2}, we assess the validity of our research model for two different types of explanations: natural language explanations and example-based explanations. Previous research emphasizes the need to consider imperfect AI when designing for human-AI collaboration~\cite{kocielnik2019will}. With recent research looking into how humans and AI can achieve complementary team performance~\cite{bansal2021does},  \citet{schemmer2023appropriate} conceptualize the role of appropriate reliance in human-AI collaboration. We extend \citet{schemmer2023appropriate}'s framework by adding another dimension: XAI advice. Given that an explanation can be incorrect even if the AI advice is correct, it is crucial to understand the impact of incorrect XAI advice on decision-making. Furthermore, it is necessary to understand the impact of imperfect XAI for different types of explanations. Below, we discuss how our contributions are situated in current literature and the implications for CSCW. 

% \textbf{Our research model can be used as a guideline to thoroughly assess imperfect XAI for human-AI decision-making.}
% To answer RQ \ref{rq1} and RQ \ref{rq2}, we assess the validity of our research model for two different types of explanations: natural language explanations and example-based explanations. 
In our study, we observe a significant moderation of humans' level of expertise on the effect of explanations' correctness on RAIR for both explanation modalities. However, we do not see this moderation for RSR. When humans are being provided wrong AI advice, their level of expertise does not moderate the impact of imperfect explanations on humans' RSR. We identify a direct effect of the level of expertise on RSR in both explanation modalities. Additionally, the correctness of explanations impacts RSR negatively for example-based explanations. Overall, our work synthesizes how humans' level of expertise impacts their reliance on AI when provided with imperfect explanations. Non-experts rely more on AI than experts, whereas experts rely more on their initial decisions. Especially for example-based explanations, imperfect XAI deceives experts' self-reliance and experts'/non-experts' relative AI reliance, fostering inappropriate reliance on the AI. Thus, this study sets a starting point to investigate the effect of imperfect XAI for different explanation modalities.

% We also investigate how imperfect XAI and humans' level of expertise affect complementary team performance to answer RQ \ref{rq3}. 
\textbf{Our findings show that imperfect explanations impact human-AI decision-making}. We observe that experts reach complementary team performance when imperfect explanations are provided. This holds true for natural language and example-based explanations. While non-experts do not reach complementary team performance, our analyses reveal that their performance can be improved to be similar to that of the AI performance. 
% Thus, our findings contribute to a more integrated understanding of the impact of human-AI decision-making on different user groups. 
Moreover, there is a difference between reaching complementary team performance when the correctness, or fidelity, of the explanation changes (see \Cref{hai-correct} and \Cref{hai-incorrect} in \Cref{hai-appendix}). Previous research discusses the impact of explanations' fidelity on humans' reliance on AI and hypothesizes that fidelity has a positive impact on humans' reliance behavior on AI \cite{hemmer2021human}. With our results, we confirm this hypothesis. Furthermore, ~\citet{papenmeier2019model} observe that low-fidelity explanations (or incorrect explanations) impact user trust in AI when the global model performance is around $75\%$ accurate, which helps validate our findings.
% The level of complementary team performance for experts drops by 3.26\% for example-based explanations and 2.78\% for natural language explanations when provided with incorrect explanations. \katelyn{I think this is too detailed, maybe for the discussion? so I'm commenting it out for now}
We also observe that the lack of expertise among non-experts impacts their task performance when shown incorrect explanations regardless of the AI advice being correct (\cref{hai-incorrect} in \cref{hai-appendix}). Similar to our findings, \citet{nourani2020role} observe that non-experts tend to over-rely on AI advice, attributing this to their inability to identify when the AI is incorrect because of their lack of expertise. These findings contribute to a more integrated understanding of the impact of human-AI decision-making on different user groups in the presence of imperfect XAI. For example, this can inform managers on how to assign tasks to humans with different levels of expertise and provide them with explanations in different modalities. \kmedit{It could also lead to organizations modifying their human-AI collaboration workflows. From informal conversations with the product team of an AI decision-support tool\footnote{WildMe.org} for biologists and conservationists to classify species and identify individuals from camera trap imagery, we learned that organizations using their tool have modified their workflow to incorporate ``checks-and-balances''. For example, intro-level biologists will collaborate with the AI to match individuals and then request a review of their ``human-AI team'' decision from a higher-up. In this unique human-human-AI collaboration scenario, the expert biologist could potentially correct situations when an intro-level biologist over-relies on AI advice because of an incorrect explanation.}
% Currently, we are only at the starting point to understand and explore the impact of incorrect explanations on humans' decision-making behavior. What is the effect of model performance on complementary team performance? What is the threshold for model performance that results in a positive teaming experience? Future works can look into these questions and use our results as a starting point to further research those topics.

\textbf{Visual, example-based explanations are more deceptive than natural language explanations.} 
To account for the impact of imperfect XAI on humans' appropriate reliance, we establish a novel metric \textbf{DoR}, (Deception of Reliance), to measure the difference in RAIR and RSR for correct and incorrect explanations. Our results indicate that people are more deceived by example-based explanations than by natural language explanations. In terms of RAIR (note that in RAIR cases, the AI advice is correct), experts and non-experts are deceived by incorrect explanations. In terms of RSR (note that in RSR cases, the AI advice is incorrect), experts are deceived by correct explanations. This is an interesting observation that may be explained by the consistency of shown visual examples. For \textit{correct advice, incorrect explanations} cases, the XAI is providing three visual examples that show a different bird species than the bird species on the image to be classified (see \Cref{explanationformat}). Moreover, these three visual examples can belong to different bird species since the XAI is choosing the top three most similar bird images (in our study, this is in 90\% of all \textit{correct advice, incorrect explanation} cases). This inconsistency in example-based explanations might deceive experts and non-experts to not rely on the AI anymore when they identify visual differences in the images provided as explanations, disregarding the correct AI advice. We discover the same behavior for experts for \textit{incorrect advice, correct explanation} cases. In those cases, the explanations consist of three images of the same bird species as the AI predicted. The incorrect explanations consist of three images that can be inconsistent in the bird species shown (in our study, this in 40\% of all \textit{incorrect advice, incorrect explanation} cases). Thus, this inconsistency in examples might deceive experts into no longer relying on themselves anymore when they identify three consistent examples shown, disregarding the incorrect AI advice. Hence, the DoR of experts and non-experts is positive for RAIR cases as they are deceived by incorrect explanations, while experts additionally have a negative DoR and are deceived by correct explanations. Note that the overall RSR for experts is still higher than non-experts' RSR; the impact on deception caused by imperfect XAI is higher.
% Interestingly, correct explanations for incorrect AI advice deceive experts into under-relying on themselves. 
As participants mentioned in the survey, it is more convincing that there is less uncertainty in the AI advice when three images that are similar to each other are shown than when three different images are shown. This corroborates our findings. This trend is not present in natural language explanations. 
% \katelyn{Remove this since we didn't do this analysis -> we could check what \% of these incorrect are consistent --- suggest the future work to better understand the inconsistencies and deception.} While uncertainty can be revealed through the language tone, we suspect that visually seeing three different images is a stronger hint of uncertainty due to the visual format. 
% One reason for this phenomenon could be that when being provided correct explanations, all three visual examples present the same bird species, whereas, for incorrect explanations, those three examples can differ in the bird species shown. Hence, when being provided with correct explanations for cases in which the AI is wrong, three examples of the same bird species are displayed. This leads to the fact that experts are being deceived in their initial decision and falsely follow the AI advice. Beyond, in all cases in which the AI gives correct advice, correct explanations correspond to showing three examples of the same bird species as shown, whereas for incorrect explanations, three images of different bird specie(s) are displayed. Non-experts are being deceived by this situation. 
% Thus, with $DoR$, we are able to measure this level of deception caused by incorrect explanations and contribute with a quantitative metric to assess the impact of imperfect XAI on humans' decision-making behavior. 

\textbf{The language tone of explanations does not impact humans' decision-making behavior. } Calisto et al. conclude that the level of expertise influences whether the framing of the explanation should be assertive or non-assertive~\cite{calisto2023assertiveness}. Based on their observations, they specifically suggest that natural language explanations should be designed such that the tone of the explanation is appropriate for the end user's level of expertise.
Despite the numerous previous studies finding that the framing of the explanations has a significant impact on human-AI collaboration~\cite{kim2020effect,calisto2023assertiveness,kim2023communicating}, our findings do not show an impact on appropriate reliance. Quantitatively, we do not find a direct effect of assertiveness on appropriate reliance. Qualitatively, we observe that the tone of the explanation should depend on certain situations, such as the AI's behaviors, instead of the human's level of domain expertise. We observe that $31\%$ of the participants prefer assertiveness to align with the confidence of the model's prediction, while only $10\%$ of participants prefer assertiveness to align with their confidence in the domain. This finding could be attributed to the fact that participants are collaborating with imperfect AI and are able to acknowledge that the AI advice was occasionally incorrect. However, given these qualitative suggestions \kmedit{and the potential for natural language explanations to present irrelevant or incorrect information~\cite{sovrano2023toward}}, we encourage future work to explore various ways to alter the tone of an explanation based on the themes we identified in \cref{qual-data}. 
% \kmedit{\citet{kayser2022explaining} learned from clinicians that some natural langauge explanations for chest X-rays were not clinically relevant. Future work could focus on this high-stakes domain when investigating explanation tone.}

% \textbf{Prior domain knowledge influences humans' decision-making.} 
% \katelyn{I feel like this is already known throughout the literature so I wonder if we can combine sentences from this with the first paragraph of the discussion? } 
\textbf{Our findings can guide researchers and practitioners on how to assess and design for imperfect XAI in human-AI collaborations.}
Regardless of the explanation modality, it is important to understand how humans interact with imperfect XAI. Visual explanations, such as example-based explanations and saliency maps, have been shown in the past to be of high educational value to the end-user (\emph{e.g.}, \cite{kim2023help,mac2018teaching}), making it even more important to understand how to design for and mitigate imperfect XAI. This need is intensified with the role AI takes in organizational learning \cite{spitzer2023ml}. Especially in the workplace, AI can facilitate knowledge transfer and support organizations in retaining and distributing expert knowledge~\cite{spitzer2022training, jarrahi2023artificial, wilkens2020artificial}. Similarly, it is also crucial to understand how imperfect XAI affects the learning of novices through AI-based learning systems \cite{spitzer2023ml} or through collaboration with AI \cite{schemmer2023towards}.
Our findings can guide knowledge managers within organizations on how to make use of explanations for employees with different levels of domain knowledge. More precisely, knowledge managers should be aware of the impact of exposing humans with different levels of expertise to imperfect XAI.
In addition, our findings contribute to a more integrated understanding of the impact that incorrect explanations can have on human-AI decision-making and inform different stakeholders in organizations. As non-experts are more affected by imperfect XAI, their performance drops more than experts' performance when incorrect explanations are provided in comparison to correct explanations. With this finding, knowledge managers can adjust their knowledge retention activities when training new employees; designers can adjust the development of human-AI collaboration systems to successfully facilitate explanations in decision-making and support humans in their work setting. Thus, we encourage practitioners designing human-AI collaboration systems to apply our findings to structure their design approach. This can aid organizations in laying out the strategic direction of human resource development by matching the use of explanations to humans' prior knowledge. Overall, these findings shed light on the ongoing discussion in CSCW on how to make use of explanations within work settings.

\section{Limitations \& Future Work}
\label{limitations}

We elaborate on various limitations of our study, how they could impact the interpretation of our results, and identify opportunities for future work.

\textbf{Lack of Information to Properly Identify Birds.} Expert birders usually rely on more information than just the visual characteristics of a bird when determining the bird species, especially for ambiguous cases. For example, the location and habitat in which the bird was spotted can be imperative to determine the exact bird species within a family. It's unclear to what extent the lack of this information influences our results. Future technical work could consider using this information to help build more transparent bird classification models.

\textbf{Correctness of Explanations versus Explanation Fidelity.} Throughout our study, we consider explanations to either be incorrect or correct. However, as we mention, some explanations that we classify as incorrect can contain evidence that is correct, making it difficult to only have a binary categorization for the correctness of explanations. While we use a binary scale for our analyses, explanation correctness, or fidelity, can be quantitatively measured on a continuous scale and categorized as low fidelity and high fidelity~\cite{papenmeier2019model}. We encourage future work to explore explanation fidelity using multiple categories instead of two to understand the differences between low- and medium-fidelity explanations when it comes to task performance and appropriate reliance. This will provide insight into the impact that noisy explanations have on decision-making, such as when an explanation reveals some information that is aligned with the ground truth class and some information that is aligned with the predicted class.

\textbf{Simplistic Natural Language Explanations.} 
The natural language explanations are very limited and simplistic, although this is a fault of the model that we are using to generate those natural language explanations~\cite{hendricks2021generating}. Compared to a field guide such as All About Birds~\cite{birdsguide}, these natural language explanations could be correct for several species given their lack of detail and their short length. This is potentially a side effect of the text descriptions in the CUB-200-2011 dataset being sourced from crowd workers instead of experts. We encourage technical researchers who are developing explanation methods that explain image classifications using natural language to pay careful attention to the text descriptions used for training the language model. For example, researchers could consider using text data from a field guide as their training data. We also encourage CSCW researchers to conduct more studies on how the explanation's length and level of detail impact experts' and non-experts' appropriate reliance. 
% exlimit the number of tokens in the generated output and the specificity of the generated output depending on whether the user is an expert or non-expert. 

\textbf{Different Explanations Convey Different Information.} 
While we do not intend to directly compare the two explanation modalities throughout our analyses, they are discussed in terms of similarities and differences throughout the article. Our main intention is to investigate how our research model holds across different modalities of explanations. While several previous studies compare multiple different types of explanation modalities qualitatively and quantitatively (\emph{e.g.}, ~\cite{kim2023help,chen2023understanding,kim2023should,du2022role,szymanski2021visual}), we encourage readers to avoid directly comparing the two explanations because they present different information. Previous research reveals that different explanation techniques can result in disagreements for the same dataset \cite{roy2022don}. For example, the natural language explanations from \citet{hendricks2021generating} are feature-based, providing descriptions of features present in the image~\cite{hendricks2021generating}. However, the example-based explanations present three similar images, which is very different information from the natural language description of features. \psedit{On top of that, the incorrectness in both explanation modalities is represented in different ways. While there are factual errors in natural language explanations that previous research addresses (e.g., hallucination effects of natural language models~\cite{sovrano2023toward}), there are logical errors (e.g., inconsistencies) within example-based explanations. This opens avenues for future research to investigate how different human cognitive abilities (i.e., cognitive styles) impact the perception of these imperfect explanations in AI-assisted decision-making scenarios.}

\textbf{Visualizing Assertiveness for Example-Based Explanations.} While there is previous research on how to visualize the confidence of a prediction for image classification~\cite{tejeda2023displaying}, to the best of our knowledge, there is no work visualizing the assertiveness of example-based explanations. Given this, our example-based explanations use natural language to convey assertiveness. Visualizing the assertiveness for each example in an example-based explanation would provide the decision-maker with another queue about whether they should rely on the model. 
% \katelyn{notes on finding a way to visually show assertiveness}

\textbf{AI Expertise versus Domain Expertise.} Our analyses are based on the participant's expertise in bird species identification. We do not ask participants about their knowledge of AI. Thus, we do not analyze their expertise related to AI. It is unclear to what extent the participant's expertise with AI would impact the effect of imperfect XAI on appropriate reliance and task accuracy for bird species classification. Future work should consider looking at the effect that AI knowledge combined with domain expertise has on appropriate reliance, taking into account an imperfect XAI.