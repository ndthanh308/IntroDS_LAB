\section{Theoretical Development}
\label{theoretical_section}

The increasing use of explanations to reveal the rationale behind AI predictions has led to a rise in research examining the impact of explanations on decision-makers' behavior \cite{de2022perils, schemmer2022meta, leichtmann2023explainable}. As imperfect AI is utilized more within high-stakes contexts, such as decision-making in the medical sector, research has focused on the impacts of potentially inaccurate AI advice on humans' decision-making \cite{kocielnik2019will, lee2019egoistic, robbemond2022understanding}. 
% However, to the best of our knowledge, there is no research on how \textbf{imperfect XAI} impacts humans' appropriate reliance on AI advice.
However, there are very few works investigating how \textbf{imperfect XAI} impacts humans' appropriate reliance on AI advice~\cite{papenmeier2019model}.


% Figure environment removed

Thus, in this work, we draw from the conceptualization on appropriate reliance previous research has established \cite{bansal2021does, schemmer2022meta, schemmer2023appropriate}. We specifically build on the conceptualization presented by ~\citet{schemmer2023appropriate} in \cref{reliance-model} by adding a new dimension to consider when investigating appropriate reliance in human-AI collaboration: The correctness of XAI advice. We simplified the correctness of XAI advice to be a binary case of correct or incorrect in \cref{reliance-model}.




The introduction of this new dimension unveils previously unexplored avenues within the realm of human-AI collaboration in CSCW, thereby offering a conceptual framework to delve into a more profound comprehension of human decision-making behavior with an AI collaborator. As a result, researchers can calculate more specific metrics regarding human decisions after receiving the AI and XAI advice. For example, the ratio of under-reliance based on a correct prediction and incorrect explanation could be different than when based on a correct prediction and correct explanation. These types of scenarios should not be overlooked when investigating human-AI collaborations in work settings. In \Cref{formulas}, we provide additional details of the newly introduced metrics.

With this new dimension for analyzing human-AI collaborations, we investigate the effect that imperfect explanations have on humans' decision-making; we investigate this relation in a sequential decision-making scenario. Based on the constructs of relative AI reliance (RAIR) and relative self-reliance (RSR), we account for the appropriateness of reliance \cite{schemmer2023appropriate}\footnote{Appropriateness of reliance is the quantitative measurement for appropriate reliance. These terms will be used interchangeably throughout the article.}. RAIR comprises the cases in which the human corrects their initial incorrect decision by overriding it with the correct AI advice. On the other hand, RSR comprises all cases in which the human makes an initially correct decision, the AI system gives incorrect advice,  and the human rightly dismisses this advice. Thus, we use appropriateness of reliance as the dependent variable in our research model (see \cref{research model}). In the recent work of \citet{schoeffer2022explanations} the authors investigate how explanations affect distributed fairness in AI-assisted decision-making. Their study shows that task-relevant explanations impact humans' reliance behavior into increasing stereotype-based errors. We apply these findings to our research model and assume that for the cases in which the AI provides correct advice, explanations will affect RAIR. Accordingly, we hypothesize:





\begin{hyp}[resume, wide, leftmargin=0cm, labelindent=0pt, labelwidth=0em]
\item The correctness of explanations impacts humans' relative AI reliance in human-AI decision-making. 
\label{hyp1}
\end{hyp}

\begin{hyp}[resume, wide, leftmargin=0cm, labelindent=0pt, labelwidth=0em]
\item The correctness of explanations impacts humans' relative self-reliance in human-AI decision-making. 
\label{hyp2}
\end{hyp}

One crucial factor in this interrelation between imperfect explanations and humans' appropriate reliance is the level of domain knowledge that humans possess. Previous work shows that humans' level of expertise can influence their decision-making \cite{calisto2023assertiveness, bayer2021role}. Related research in information systems investigates the role of domain knowledge in decision-making. \citet{erjavec2016impact} show in their behavioral experiment in online supply chains that domain knowledge positively impacts humans' confidence in decision-making. Similarly, \citet{dikmen2022effects} analyze humans' reliance on AI when possessing different levels of domain knowledge. In their study, the authors provide an imperfect AI and argue that higher domain knowledge leads to less trust in AI. With this impact of domain knowledge on human-AI decision-making, we intend to examine how the effect of imperfect explanations on appropriate reliance is influenced by humans' level of expertise. Humans with high domain-specific knowledge demonstrate an enhanced ability to discriminate between erroneous explanations and accurate ones with greater rigor \cite{levy2021assessing}. This discernment is facilitated by their extensive expertise, which empowers them to readily identify and discern false information \cite{barfield1986expert}. Thus, in our study, we hypothesize:

\begin{hyp}[resume, wide, leftmargin=0cm, labelindent=0pt, labelwidth=0em]
\item Humans' level of expertise moderates the effect of the correctness of explanations on RAIR.
\label{hyp3}
\end{hyp}

\begin{hyp}[resume, wide, leftmargin=0cm, labelindent=0pt, labelwidth=0em]
\item Humans' level of expertise moderates the effect of the correctness of explanations on RSR.
\label{hyp4}
\end{hyp}

Humans' information processing is not only influenced by what they are provided but also by the way this information is provided. Previous research demonstrates that language style can impact humans' decision-making behavior~\cite{huang2022and,kronrod2022think,calisto2023assertiveness}. \citet{huang2022and} show that the level of assertiveness in reviews affects humans' online review persuasion. Similarly, \citet{kronrod2022think} investigates the effect of the level of assertiveness on the tone of language. They find that there is a relationship between the tone of language and its level of assertiveness. Next to the psychological research field, recent research in HCI examines how assertiveness in explanations affects humans' performance when interacting with AI \cite{calisto2023assertiveness}. They show that the level of assertiveness does impact humans' trust when collaborating with AI. In their work, they reveal that humans are more likely to follow AI advice when the explanation is presented in their own communication style. With those findings and related research on the impact of assertiveness on humans' decision-making, it is reasonable to hypothesize that assertive explanations will impact humans' reliance on AI. Hence, we hypothesize:

\begin{hyp}[resume, wide, leftmargin=0cm, labelindent=0pt, labelwidth=0em]
\item Explanations' level of assertiveness moderates the effect of the correctness of explanations on RAIR. 
\label{hyp5}
\end{hyp}

\begin{hyp}[resume, wide, leftmargin=0cm, labelindent=0pt, labelwidth=0em]
\item Explanations' level of assertiveness moderates the effect of the correctness of explanations on RSR. 
\label{hyp6}
\end{hyp}

Our research model shown in \cref{research model} summarizes the hypotheses that we test in our mixed-methods study. Overall, with this research model, we test for moderating effects of the level of expertise and level of assertiveness on the effect of the correctness of explanations on appropriate reliance.

% Figure environment removed
