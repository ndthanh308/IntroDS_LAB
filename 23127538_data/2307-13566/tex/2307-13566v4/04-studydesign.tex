
\section{Methodology}
\label{methodology}
% \katelyn{Let's think about the order that makes the most sense for these sections and then update this paragraph accordingly.}
In this section, we describe the task of bird species identification, the experiment design, the recruitment process of participants, and the development of the explanations we use in the study. Finally, we end this section by outlining the data we select to use for the study and the metrics we use to analyze the results.

\subsection{Task Domain: Bird Species Identification}

% Birding is a very popular hobby around the world. For example, over $121,000$ birders submit content to Macaulay Library at Cornell University~\cite{MerlinLibrary}. 
Computer-supported cooperative work is becoming core to wildlife conservation efforts~\cite{tuia2022perspectives,bondi2022role,green2020innovations}.
With mobile devices becoming increasingly powerful, non-experts and experts alike can use AI-powered applications like the Merlin Bird ID app~\cite{merlin} to identify bird species for monitoring biodiversity and learning about birds. 
The popularity of both birding and AI-based image classification techniques suggests that bird species identification would be a sensible domain to investigate our research techniques.
Furthermore, this is a task for people with a wide range of expertise. 

While identifying bird species from images may not be posed as a high-stakes task in our study, this task is imperative to conserving and managing species and biodiversity~\cite{akccay2020automated}. Furthermore, the task of fine-grained image classification, such as bird species identification, is comparable to higher-stakes tasks, such as identifying diseases from medical imagery~\cite{tschandl2020human}. For example, radiologists collaborating with an imperfect AI and imperfect XAI to diagnose diseases present in chest X-rays would go through a similar visual decision-making process as if they were trying to classify an image of a Bewick Wren in our study interface. ~\citet{kayser2022explaining,hou2021ratchet} propose an imperfect natural language explanation for chest x-rays, similar to the explanation we implement in our study, which helps bridge our findings between bird species identification and higher-stakes tasks.

Previous studies that focused on human-centered XAI and human-AI collaboration also use the domain of bird species identification to understand better human-AI collaboration (\emph{e.g.}, ~\cite{kim2023help,nguyen2022visual,cabrera2023improving}). However, few previous works focus on human-AI collaboration for decision-making in the wildlife conservation domain overall. Yet, the field of AI for wildlife conservation is rapidly growing~\cite{tuia2022perspectives} and could benefit from research related to CSCW and HCI.

\subsection{Study Design}
\label{study-design-section}

To answer our research questions, we design a mixed between- and within-subjects study to examine various effects of explanations on appropriate reliance. Our study was carefully reviewed by two experienced birders\kmedit{: one experienced birder is a migration counter for a bird sanctuary, and the other experienced birder holds a graduate degree in environmental science, conducted research at a nature center, and worked at a nature conservancy}. The procedure of the study follows the design outlined in \Cref{experimentdesign} and is divided into four different parts ($A-D$), which we explain in further detail below. 

% Figure environment removed

The study begins with part A in \cref{experimentdesign}: In a bird identification test, we assess participants' expertise in classifying six different species of birds\footnote{Specific birds used for the bird identification test are reported in \cref{bird-test-details}, p. \pageref{bird-test-details}.}. We distinguish the six bird images based on their level of difficulty. This level of difficulty is derived from discussions with an experienced migration counter from a bird sanctuary. \kmedit{While previous work has collected participants' self-perception of expertise~\cite{kim2023help}, this method is subjective, and participants may self-perceive their skills differently. ~\citet{kazemitabaar2023novices} measure participants ``experience-level'' through log data instead of subjective measures. Similarly, we try to avoid defining expertise subjectively.}
% The level of difficulty was further validated during data collection by comparing participants' perceptions of their expertise with their performance on the test (\cref{test-scores} in \cref{bird-test-details}). 
For the purpose of our analyses, we identify two different levels of expertise: \textit{non-experts} and \textit{experts}.
% \textit{citizen scientists and hobbyists} and \textit{conservationists and ornithologists}. 

In the next section of the study (part B in \cref{experimentdesign}), participants are randomly assigned to one of two explanation types. Similar to previous studies \cite{riefle2022influence, robbemond2022understanding, szymanski2021visual, chandrasekaran2017takes}, the treatments differ in the explanation modality participants receive: \textit{Natural language} explanations or \textit{visual, example-based} explanations. We use natural language explanations because the AI model that we used for the study was specifically designed to generate natural language explanations based on fine-grained image classifications~\cite{hendricks2021generating}. We choose to also look at example-based explanations as recent studies focus on this modality in human-AI collaboration~\cite{chen2023understanding,kim2023help,cai2019effects,humer2022comparing}. \psedit{However, recent studies show that example-based explanations have potential benefits. \citet{chen2023understanding} show that example-based explanations improve humans' performance, so much so that it leads to complimentary team performance. With promising results from previous research and numerous clinical decision-support tools proposing to incorporate example-based explanations (\emph{e.g.}, ~\cite{sadeghi2018users,barata2021improving}), we find it necessary to investigate the effect of example-based explanations on humans' appropriate reliance in the context of imperfect XAI.}


The human-AI bird identification task (part C of \cref{experimentdesign}) consists of two phases. For each treatment condition, the participants are asked to initially identify the bird species from an image (phase 1 in \cref{experimentphases}). After submitting an initial identification, they are shown the AI's prediction along with the explanation, and again, they have to submit an identification for the bird species in the image (phase 2 in \cref{experimentphases}). The structure of phases one and two are corroborated with previous work~\cite{green2019principles}. Initially, participants must click on a button that shows ``Show AI Explanation''. Without revealing the explanation, the participant cannot proceed to the next question. This is one way for us to ensure that the participant acknowledges the presence of an explanation. Overall, participants do this process for twelve different random bird images.

% Figure environment removed

% They are made aware before starting the task that after submitting their initial response they are provided AI advice in the form of the AI's prediction and explanations. 
% The twelve samples that participants have to classify deviate in the type of prediction and explanations that are shown to them after submitting their initial response. 

As the AI that we are utilizing for identifying the bird species is not perfect~\cite{hendricks2021generating}, the predictions and explanations provided can be incorrect. In order to understand how this affects participants' appropriate reliance, we ensure that each participant is shown three samples of the following four categories in random order: 
\begin{itemize}
    \item \textbf{CC}: correct prediction and correct explanation
    \item \textbf{CI}: correct prediction and incorrect explanation
    \item \textbf{IC}: incorrect prediction and correct explanation
    \item \textbf{II}: incorrect prediction and incorrect explanation
\end{itemize}
Overall, participants are shown twelve different bird species. For each of the four categories we identified, we show three samples where each explanation is framed with a different level of assertiveness: \textit{assertive}, \textit{non-assertive}, or \textit{neutral}\footnote{\Cref{explanationformat} provides examples of \textit{assertive}, \textit{non-assertive}, and \textit{neutral} explanations.}. As a result, each participant is shown one \textit{assertive}, one \textit{non-assertive}, and one \textit{neutral} explanation for each category. We ensure that the order is randomized for each participant. Moreover, we also vary the samples shown, meaning that not every participant sees the same bird images. This is to ensure that our results are not dependent on the difficulty of the bird species.

After finishing the task, participants must fill out an additional questionnaire (part D of \cref{experimentdesign}). Here, we qualitatively assess participants' ability to properly rely on the AI based on the explanations that they were shown. Thus we ask them: ``\textit{Under what circumstances would you prefer assertive (e.g., “definitely”, “clearly”) versus non-assertive (e.g., “might be”, “appears to be”) versus neutral explanations and why?}''. Aside from this open-text question, we also ask participants about their occupations and the regions in North America that they are most familiar with in terms of bird species.

% Let's not mention this in this paper to not confuse people why we didn't address it in the results?
% Next to these qualitative open-text questions, we also ask participants to indicate their cognitive load on a five-point Likert scale, similar to \cite{ouwehand2021measuring}. We do this to investigate whether different types of explanations have distinct effects on participants' cognitive decision-making processes.



\subsection{Data Selection}

We create a dataset of bird images and explanations to show participants by manually curating bird images from the well-established CUB-200-2011~\cite{WahCUB_200_2011} dataset and explanations from the Generating Visual Explanations model~\cite{hendricks2021generating}. The original dataset consists of 11,788 images of 200 different bird species and is split into 5,994 training and 5,794 test images. Each bird species is represented with around 60 images of the respective bird class. When curating birds, we first filtered for bird families with several species in the CUB dataset. We specifically filtered out every bird class that is not a part of the Warblers, Wrens, Swallows, Sparrows, or Finches/Grosbeaks families. After applying this filter, we had $1,864$ out of $5,794$ images from the test set of CUB-200-2011. Of those $1,864$ images, $1,609$ were predicted correctly by the AI, and $255$ images were predicted incorrectly by the model. 

After filtering the bird species, multiple researchers on our team separately classified the natural language explanations and the visual, example-based explanations for a subset of the $1,864$ birds as incorrect or correct. Cases of doubt were discussed by a subset of the research team and excluded from consideration if an agreement was not met. In total, we identified ten examples for each category\footnote{The four categories are identified in Section \ref{study-design-section}.} and explanation type. In some cases, the example-based and natural language explanations for a single bird are used. As a result, the dataset represents 66 different images and 43 different bird species from the CUB-200-2011 dataset. 

We define a correct natural language explanation to be when the explanation aligns with the description of the predicted bird class. We define an incorrect natural language explanation to misalign with all or part of the description of the predicted bird class. \psedit{Thus, an incorrect natural language explanation contains a factual error. This type of incorrectness is present in different natural language techniques and is a focus of current research \citep{liu2022improving, xie2023faithful}.} We use descriptions from the Cornell Lab of Ornithology All About Birds Guide~\cite{birdsguide} to corroborate our classification for each explanation. Examples of incorrect and correct natural language explanations are provided in \cref{explanationexamples}. 

% Figure environment removed

For the example-based explanations, we define a correct explanation as the three most similar images belonging to the predicted class (as shown in phase 2 of \cref{experimentphases}). We define an incorrect explanation to be most similar to at least one image that is not of the predicted class. \psedit{This means that incorrect example-based explanations incorporate logical errors as the examples shown are dissimilar from the predicted class. Moreover, such incorrect explanations can have an inconsistency as the examples shown might differ in the classes shown.} However, we only choose to show participants incorrect explanations that have at least two images that are not of the predicted class. For example, in \cref{explanationexamples}, the AI correctly predicts a Nashville Warbler; however, the three most similar examples are a Painted Bunting, a Yellow-Breasted Chat, and an American Redstart.  In some cases where the advice is correct and the explanation is incorrect\footnote{The explanation does not align with the predicted class.}, the explanation may align with the ground truth class. For example, for a Tennessee Warbler, the AI predicts an Orange-crowned Warbler (incorrect advice since the wrong bird species is predicted), but the three most similar examples are all of Tennessee Warblers (incorrect explanation since the examples' bird species do not align with prediction). It's possible that a model could be relying on spurious patterns to make classifications~\cite{plumb2021finding}. Since we are dealing with an imperfect AI, we do not choose to exclude such cases from our dataset.



\subsection{Explanation Modalities}
 

\paragraph{Natural Language Explanations}
The natural language explanations were generated by the model proposed by Hendricks et al.~\cite{hendricks2021generating}. We followed the PyTorch implementation~\cite{salanizpytorch-gve-lrcn} of Hendricks et al.'s model to obtain the natural language explanations since the original model from Hendricks et al. was unavailable. 
% Their fine-grained image classification model has an accuracy of approximately $84\%$ on the test set~\cite{hendricks2021generating}. 
After running the test images through the model, a natural language explanation is generated for each classification. For example, the natural language explanation for the Magnolia Warbler in \cref{explanationexamples} is: ``\texttt{this is a bird with a yellow belly black stripes on its breast and a grey head}''.  

\paragraph{Example-Based Explanations}
Previous work creates example-based explanations, specifically normative explanations, by calculating the Euclidean distance between the given image and the images in the dataset~\cite{cai2019effects}. Another study generates the example-based explanation by calculating the $L_{2}$ distance of the embedded features~\cite{nguyen2021effectiveness}. 
% Ultimately, there are several different ways to calculate image similarity for example-based explanations. 
We generated the example-based explanations by following methods used in previous works~\cite{barata2021improving,tschandl2020human}. As done by \citet{tschandl2020human} and \citet{barata2021improving}, we calculate the cosine similarity between the extracted feature vector of the given image and the rest of the extracted feature vectors of the images in the training set. Unlike Barata and Santiago~\cite{barata2021improving}, we choose not to take the example's ground truth class into consideration. The extracted features from the images were provided by Hendricks et al.~\cite{hendricks2021generating}. Because the model is not perfect, the example-based explanations are also not perfect. For example, even though the model correctly predicted an image of a Nashville Warbler in \cref{explanationexamples}, the three most similar images are of three different birds. For this study, we consider an example-based explanation to be incorrect if two of the three examples are of a different class than the predicted class.
Inspired by Ford et al., we choose to show participants the three most similar examples~\cite{ford2022explaining}.

% Figure environment removed

\paragraph{Assertiveness of Explanations} Following previous studies~\cite{winter2015don,calisto2023assertiveness,pacheco2019alignment}, we define assertive explanations to include words and adjectives such as ``definitely'' and ``clearly''. We define the non-assertive explanations to include words and adjectives such as ``might be'' and ``appears to be''. For the neutral condition, we omit the adjectives to maintain the same structure of the information being presented. For the natural language explanations, we append the assertiveness to the beginning of the explanation generated by the model to read like a sentence. For the non-assertive and assertive conditions, we removed the text ``this is a'' from the generated explanation in order to incorporate it into the sentence structure we designed. The three versions of assertiveness for both explanation modalities are shown in \cref{explanationformat}. To our knowledge, there is no literature to rationalize how to appropriately present assertiveness visually for example-based explanations, so we opt to use natural language in combination with the example-based explanations. 

\subsection{Recruitment}

We recruit the participants through several communication channels that are related to the environment and conservation, such as the AI for Conservation Slack, Birding International Discord, Climate Change AI community forum, WildLabs.net community forum, and Audubon Society mailing lists. Additionally, we use Prolific as previous research has indicated that this platform is a reliable source of research data \cite{peer2017beyond, palan2018prolific}. We apply a custom filter on Prolific to target individuals who currently work in a field related to nature, science, the environment, or animals. Participants receive compensation that is above minimum wage.
% Moreover, we also share our study on several community forums or personally email experts through shared connections and interest groups. 
Overall, we try to limit recruitment to only address people with prior knowledge of birding to minimize the prevalence of novices' randomly guessing bird species identification. 
After excluding participants who provide incomplete and fake responses (i.e., lorem ipsum response to our survey question), we have $136$ people complete our study. In order to determine if a participant is familiar with birding, participants take a bird identification test (phase A in \cref{experimentdesign}). The participant's score on the bird identification test is used to determine whether they are considered a non-expert or an expert. Details related to clustering participants based on their test scores are provided in \cref{results_section}.

\subsection{Quantitative and Qualitative Metrics}

\paragraph{Quantitative Metrics} We quantitatively calculate appropriate reliance across the four dimensions defined by \citet{schemmer2023appropriate}: correct AI reliance, correct self-reliance, under-reliance, and over-reliance. Accordingly, correct AI reliance measures the number of correct decisions when the human's initial decision is incorrect, and the human is rightly taking over the correct AI advice. Correct self-reliance is when the human initially makes the correct decision and does not overwrite their decision with the incorrect AI advice. On the other hand, under-reliance reflects the case in which the human initially makes an incorrect decision and does not adhere to the correct AI advice. On the other hand, over-reliance represents the scenario in which the human makes an initial correct decision but overrides her own decision with incorrect AI advice. Following the appropriate reliance metrics defined by ~\citet{schemmer2023appropriate}, we calculate RSR and RAIR to account for the appropriateness of reliance. 

With the new dimension for XAI advice, we can separately measure RAIR and RSR for correct and incorrect explanations and derive its impact on appropriate reliance. In order to measure this impact, we look at the Deception of Reliance (DoR) caused by imperfect XAI. For RAIR, we can apply the following: 
\begin{equation}
    DoR_{RAIR} = RAIR_{C} - RAIR_{I}.
\end{equation}
In this equation, the subscript $I$ represents incorrect explanations, whereas the subscript $C$ represents correct explanations. We can compute the same for RSR: 
\begin{equation}
    DoR_{RSR} = RSR_{C} - RSR_{I}.
\end{equation}
In order to measure the overall deception impact of explanations on humans' decision-making behavior, we compute the deception on appropriate reliance by calculating the Gaussian distance in the RAIR-RSR space between incorrect and correct explanations:
\begin{equation}\label{DAIR_aor_eq}
    DoR(RAIR, RSR) = \sqrt{{(RAIR_{C} - RAIR_{I})}^2 + {(RSR_{C} - RSR_{I})}^2 }.
\end{equation}
According to Schemmer et al.'s conceptualization of Appropriateness of Reliance \cite{schemmer2023appropriate}, this results in the following:
\begin{equation} 
    DoR_{AoR} = AoR_{C}(RAIR, RSR) - AoR_{I}(RAIR, RSR).
\end{equation}

This difference represents the deception between the correct and incorrect explanations. If the deception is a positive value, then incorrect explanations are more deceptive; if the difference is a negative value, then correct explanations are more deceptive. 

Lastly, as defined by previous work (\emph{e.g.}, ~\cite{hemmer2021human,bansal2021does,green2019principles}), we can calculate the human-AI team performance to determine if CTP exists. Following the constructs defined in those previous works, we determine if CTP exists by calculating the participants' performance in identifying the bird species \textbf{before} and \textbf{after} they see the AI advice and compare this to the performance of the model on the twelve birds images shown to the participant. We utilize accuracy as a performance metric. Since every participant is shown six birds that the AI correctly classifies and six that the AI incorrectly classifies, the model performance is $50\%$.  

\paragraph{Qualitative Metrics} We also conduct an inductive content analysis of the open-ended responses to better understand the participants' preferences regarding assertiveness. As a reminder, in the end-survey, we ask participants specifically ``\textit{Under what circumstances would you prefer assertive (e.g.,
``definitely'', ``clearly'') versus non-assertive (e.g., ``might be'', ``appears to be'') versus neutral explanations
and why?}''. To analyze these responses, we follow the established procedure of \citet{gioia2013seeking} and screen the answers in three coding workshops. In the first workshop, the first and second authors of this article initially screen the answers and applied open coding \cite{holton2007coding} to extract and aggregate core constructs of participants' answers. In this procedure, both authors discuss their findings and align their understanding of relevant constructs. Through a second coding workshop, we apply axial coding to derive subcategories of these constructs and align those with the data. In a final workshop, we distill those emerging themes and derive aggregated dimensions \cite{wolfswinkel2013using}. 