\section{Results}
\label{results_section}


To answer our research questions and confirm or reject our hypotheses, we conduct rigorous statistical and qualitative analyses. We measure appropriate reliance based on metrics defined in previous work: Relative AI reliance (RAIR) and relative self-reliance (RSR)~\cite{schemmer2023appropriate}. By doing so, we answer \textbf{RQ} \ref{rq1} and \textbf{RQ} \ref{rq2} in \Cref{section_ar}. We also calculate the participant's task accuracy before and after receiving AI and XAI advice to answer \textbf{RQ} \ref{rq3} (see \Cref{hai_performance}). This way, we can determine whether complementary team performance exists~\cite{hemmer2021human}. Additionally, based on the new metric, Deception of Reliance (DoR), we measure to what extent explanations deceive humans, answering \textbf{RQ} \ref{rq4} in \Cref{deception_section}. Lastly, we qualitatively analyze open-ended responses through rigorous inductive content analysis in \Cref{qual-res}. For all of our research questions, we look at two different types of explanations: natural language explanations that are focused on specific features present in the image and visual, example-based explanations showing the top three most similar example images from the training set. While our analyses look at both modalities, we do not intend to compare them directly. Therefore, we do not conclude one modality is better or worse than the other.

\subsection{Participant Statistics}
On average, the study takes $24$ minutes to complete.
In order to distinguish experts from non-experts, we perform K-means clustering ($k=2$) based on a principal component analysis with two components for four features from the bird species identification test (part A of \cref{experimentdesign}). These four features represent participants' scores in correctly identifying the family and species of the easy and the difficult bird images. By clustering the $136$ participants into the expert and non-expert group, we end up with $83$ experts and $53$ non-experts. With this clustering, the average bird identification test score (summing up all four scores in the identification test) for non-experts is $38.99\% (STD = 11.42\%)$ while the average test score for experts is $83.84\% (STD = 12.30\%)$\footnote{Participants performance on the bird identification test is shown in \cref{test-scores}, Appendix Section \ref{bird-test-details}.}. Of the $83$ experts, $42$ see example-based explanations, and $41$ see natural language explanations. Of the $53$ non-experts, $25$ see example-based explanations, and $28$ see natural language explanations. In terms of the fields that the $136$ participants represent, $45$ participants have an occupation primarily related to biology, conservation, and/or the environment. $26$ have an occupation primarily related to engineering and/or technology; $30$ are either researchers, students, or affiliated with education in some other way; $24$ have occupations in miscellaneous industries; and $11$ are retired. 


\subsection{Moderating Effects in Imperfect XAI Research Model}
\label{section_ar}

In order to test whether humans' level of expertise and the explanations' assertiveness moderate the relation of the correctness of explanations on humans' appropriate reliance, we conduct several moderation analyses utilizing the process macro model of \citet{hayes2017introduction}. 
An overview of the regression analyses is presented in \Cref{mod_anal}.
% on p. \pageref{mod_anal}.

\begin{table}[htbp!]

\caption{Moderation analyses of the correctness of natural language and example-based explanations on RAIR and RSR with the level of expertise and assertiveness as moderators. The coding of assertiveness used for the moderation analyses is provided. }

\begin{tabular}{P{2cm} P{1cm} P{1cm}}
\multicolumn{3}{c}{Coding of assertiveness} \\

\hline

assertiveness &  Z1 & Z2 \\ \hline \hline

\textit{neutral} & 0 & 0 \\  \hline
\textit{non-assertive} & 1 & 0 \\  \hline
\textit{assertive} & 0 & 1 \\  \hline
\\\\ \end{tabular}


\begin{threeparttable}


\begin{tabular}{m{1.5cm}R{0.9cm} R{0.9cm} R{0.002cm} R{0.9cm} R{0.9cm} R{0.002cm} R{0.9cm} R{0.9cm} R{0.002cm} R{0.9cm} R{0.9cm}} \hline
& \multicolumn{5}{c}{RAIR} && \multicolumn{5}{c}{RSR}\\
\cmidrule{2-6} \cmidrule{8-12}
& \multicolumn{2}{c}{Natural Language} &&  \multicolumn{2}{c}{Example-Based} & & \multicolumn{2}{c}{Natural Language} & & \multicolumn{2}{c}{Example-Based}  \\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9} \cmidrule{11-12}
& coeff & p&  & coeff & p & & coeff & p&  & coeff & p \\
\hline \hline
const   & 1.26   & .00 & & .43 & .17 & & -17.16   & .98  & & -3.60  & .00    \\\hline
corr   & .57   & .29 & & 1.02 & .04 & & 13.25   & .98  & & -.4.36  & .74    \\\hline
exp   & 2.12  & .00 &&  -1.25 & .00 &  & 15.89   & .98 &  & 3.25  & .00    \\\hline
Z1   & -.46 & .24 &&  .26 & .47 &&  .14   & .26  & & -.09  & .83    \\\hline
Z2   & .00  & 1.00 &&  .00 & 1.00 & & -.31   & .58  & & .09  & .83    \\\hline 
exp x corr   & -1.00  & \best{.05} & & -1.04 & \best{.03} & & -13.33  & .98  & & -.73  & .57    \\\hline
Z1 x corr   & .46  & .44 &&  -.03 & .95 & & -.28   & .71  & & -.45  & .55    \\\hline
Z2 x corr   & .19  & .74 &&  -.16 & .77 & & .90   & .23  & & -.43  & .55   \\\hline
\end{tabular}
    \begin{tablenotes}
        \item[1] \textit{corr} --- \textit{correctness}; \textit{exp} --- \textit{level of expertise}
    \end{tablenotes}
    \end{threeparttable}

\label{mod_anal}

\end{table}


\subsubsection{Participants' level of expertise moderates the effect of the correctness of explanations on RAIR for natural-language explanations}
\label{mod_analysis_nle_rair}
As theoretically developed in Section \ref{theoretical_section}, we model the correctness of explanations as an independent variable. Accordingly, we model RAIR as the dependent variable. To account for the moderation effect of the level of expertise and assertiveness, we examine each variable as a moderator and report the interaction effects with the correctness of explanations. The results of this moderation analysis are shown in \Cref{mod_anal} (a detailed view is shown in \Cref{mod_anal_nle_rair} on p. \pageref{mod_anal_nle_rair} in the \Cref{mod_appendix}).

The moderation analysis shows that the interaction of the level of expertise with the correctness of explanations is significant (coeff $= -1.00$, p-value $= .05$). We observe a negative coefficient. Accordingly, the moderation effect on the relation of correctness on RAIR is higher for non-experts than for experts. \kmedit{In other words, non-experts change their initially incorrect decision to align with the correct AI advice more often than experts do when the natural language explanation is correct.} However, there is no significant effect in the interaction of assertiveness and the correctness of explanations. Thus, we conduct a regression analysis with the moderators as independent variables to evaluate for a direct effect of assertiveness as recommended by \citet{hayes2017introduction} and \citet{warner2012applied}. The results of the regression analysis show that there is no direct effect between assertiveness and RAIR (coeff $= .04$, p-value $= .77$). Thus, we confirm hypotheses \ref{hyp1} and \ref{hyp3} and reject hypothesis \ref{hyp5} for natural language explanations.


\subsubsection{Participants' level of expertise moderates the effect of the correctness of explanations on RAIR for example-based explanations}

Next, we present the moderation analysis of example-based explanations on RAIR. We set up the analysis for example-based explanations the same as the analyses for natural language explanations (see \Cref{mod_analysis_nle_rair}). As seen in \Cref{mod_anal}, there is a significant moderation effect of the level of expertise (coeff $= -1.04$, p-value $= .03$). The negative coefficient signals that this moderation is higher for non-experts than for experts. The correctness of the example-based explanations has a positive coefficient (coeff $= 1.02$, p-value $= .04$), and thus, correct explanations have a positive impact on RAIR. \psedit{Thus, if participants are provided with a correct explanation, they more often correctly follow the AI advice.} \kmedit{Overall, correct explanations result in humans changing their initially incorrect decisions to align with the correct AI advice more often, and this is especially prevalent among non-experts.}

Furthermore, the analysis reveals that assertiveness does not moderate the effect between correctness and RAIR. According to \citet{hayes2017introduction} and \citet{warner2012applied}, we drop the interaction term and conduct a regression analysis with assertiveness set as the independent variable. The result shows that there is no significant direct effect of assertiveness on RAIR (coeff $= -.04$, p-value $= .79$).
Hence, we confirm hypotheses \ref{hyp1} and \ref{hyp3} and reject hypothesis \ref{hyp5} for example-based explanations.

\subsubsection{Participant's level of expertise has a direct effect on RSR for natural language explanations}
\label{mod_analysis_nle_rsr}
In addition to analyzing whether the level of expertise and assertiveness moderate the effect of explanations' correctness on RAIR, we conduct the same analyses for the effect of explanations' correctness on RSR. For RSR, we look at all cases in which the AI prediction is giving incorrect advice (i.e., the prediction is wrong) and the initial human decision is correct \cite{schemmer2023appropriate}. 
% This next sentence seems to be out of place so I'm commenting it out for now.
% As outlined in \Cref{methodology}, correct explanations reflect the scenario in which the explanation is describing the AI predicted class.

% In this subsection, we report the moderation analysis of natural language explanations on RSR. The results can be seen . 
The moderation analysis in \Cref{mod_anal} for the natural language explanation shows that there is no significant effect of correctness on RSR moderated by level of expertise (coeff $= -13.33$, p-value $= .98$) and assertiveness (Z1 x corr.: coeff $= -.28$, p-value $= .71$; Z2 x corr.: coeff $= .90$, p-value $= .23$). 

Thus, we perform a regression analysis with the level of expertise and assertiveness as independent variables and drop the interaction terms. We observe that there is no significant effect of assertiveness on RSR (coeff $= .10$, p-value $= .58$). However, the level of expertise (coeff $= 3.18$, p-value $= .00$) has a significant effect on RSR. 
% \psedit{Thus, expert participants correctly follow the AI advice more often than non-expert participants.} 
\kmedit{With a positive coefficient, this means that experts dismiss incorrect AI advice more than non-experts when shown natural language explanations.}
\psdelete{The trend} \psedit{This can also be seen} in \cref{rsr-rair}\psedit{, which} tells us that experts have a higher RSR than non-experts. 
% This can also be seen in \Cref{rsr-rair}. 
Therefore, we reject hypothesis \ref{hyp2}, and additionally, hypothesis \ref{hyp4} as the level of expertise does not have a moderating role but has a direct effect on RSR for natural language explanations. On top of that, we reject hypothesis \ref{hyp6}.


\subsubsection{The correctness of explanations and participants' level of expertise have a direct effect on RSR for example-based explanations}

Lastly, we report the results of the moderation analysis for example-based explanations on RSR. The analysis is set up the same as it is in \Cref{mod_analysis_nle_rsr} but for the example-based explanations. 
% \Cref{mod_anal_ex_rsr} displays the results of this regression analysis.

We can see in \Cref{mod_anal} that the level of expertise does not significantly moderate the effect of correctness on RSR (coeff = -.73, p-value = .57).
Additionally, there is no significant moderation of assertiveness (Z1 x corr.: coeff $= -.45$, p-value $= .55$; Z2 x corr.: coeff $= -.43$, p-value $= .55$). Thus, we conduct a regression analysis without the interaction terms. The results of this analysis show no direct effect of assertiveness on RSR (coeff $= -.03$, p-value $= .86$). However, there is a significant direct effect of explanations' correctness on RSR (coeff $= -1.40$, p-value $= .00$) and a direct effect of level of expertise on RSR (coeff $= 3.05$, p-value $= 0.00$). \psedit{This means that experts more often correctly override the wrong AI advice and stick to their correct initial decision compared to non-experts for example-based explanations. Additionally, when the explanations are correct, participants more often correctly override wrong AI advice and stick to their correct initial decision.} Hence, experts have a higher RSR than non-experts, which can also be seen in \Cref{rsr-rair}. Moreover, as incorrect explanations have a higher impact on RSR, experts are able to identify false AI advice for incorrect explanations better. This also shows that experts are able to identify incorrect AI advice to a greater extent than non-experts; experts, in this case, rely more heavily on their own judgment.

Thus, we confirm hypothesis \ref{hyp2} and reject hypothesis \ref{hyp4} as the level of expertise does not take in a moderating role but has a direct effect on RSR for example-based explanations. On top of that, we reject hypothesis \ref{hyp6}.
\\
\\
Overall, the moderation analyses reveal that the level of expertise moderates the effect of the correctness of explanations on RAIR for both explanation modalities. Additionally, the analyses show that the level of expertise has a direct effect on RSR for both explanation modalities, and the correctness of explanations has a direct effect on RSR for example-based explanations.


\subsection{Human-AI Team Performance}
\label{hai_performance}
Hemmer et al. argue that interpretability is a key component of human-AI complementarity~\cite{hemmer2021human}. Several previous user studies have failed to show that incorporating XAI into AI systems can lead to CTP~\cite{fok2023search}. However, with a new dimension of XAI advice in \cref{reliance-model}, we can contribute to the current literature by investigating how the correctness of explanations affects CTP. By calculating the participants' performance before and after seeing the AI and XAI advice, we can determine whether CTP exists in the presence of imperfect XAI. 
As the analyses in \Cref{section_ar} reveal, the level of expertise impacts appropriate reliance in terms of RSR and RAIR. Thus, in comparing the human-AI team performance, we distinguish by participants' level of expertise. We use accuracy as the performance metric. \Cref{reliance-counts} presents the performance of AI and humans for each treatment.

% Figure environment removed

The AI's performance is always $50\%$ because the study was designed to show participants six birds that the model correctly classified and six that the model incorrectly classified. In \Cref{reliance-counts}, we see that when experts are paired with the AI, their performance improves by $8.74\%$ for the natural language modality and $9.53\%$ for the example-based modality. When experts are paired with AI, they perform $6.91\%$ better than the AI alone for natural language explanations and $5.36\%$ for example-based explanations.

While experts reach CTP, we do not see this for non-experts. However, we do see that the non-experts greatly improve their performance and nearly match the AI's performance when paired with the AI. Specifically, non-expert participants who see the natural language explanations improve their performance by $39.58\%$ (task accuracy of $45.83\%$), while non-expert participants who see the example-based explanations improve their performance by $34.67\%$ (task accuracy of $43.00\%$) when paired with the AI.

We can separate \cref{reliance-counts} into correct and incorrect explanations. When we only consider cases with correct explanations (\cref{hai-correct} in \cref{hai-appendix}), the non-experts' task accuracy is approximately the same as the AI alone: $48.81\%$ for natural language explanations and $49.33\%$ for example-based explanations. Experts reach CTP in both modalities. When only considering incorrect explanations (\cref{hai-incorrect} in \cref{hai-appendix}), we still see complementary team performance for the experts. However, the non-experts' task accuracy suffers more when shown incorrect explanations. Non-experts' task accuracy for natural language explanations is $42.86\%$ and $36.67\%$ for example-based explanations.

Additionally, we calculate two-sample t-tests to assess whether the trends in \cref{reliance-counts} are significant. The team performance of experts and AI is significantly higher than the team performance of non-experts and AI in both explanation modalities (natural language: p-value $= 0.00$, example-based: p-value $= 0.00$). Furthermore, we also compare the performance for correct and incorrect explanations. Here, we see the same results: experts achieve a significantly higher team performance than non-experts (correct explanations --- natural language: p-value = 0.00, example-based: p-value $= 0.01$; incorrect explanations --- natural language: p-value $= 0.00$, example-based: p-value $= 0.00$). 


\subsection{Deception caused by Imperfect XAI}
\label{deception_section}


In \Cref{rsr-rair}, we compare RAIR to RSR for both levels of expertise and the correctness of explanations. We show this comparison for example-based explanations (the graph on the left side of \Cref{rsr-rair}) and natural language explanations (the graph on the right side of \Cref{rsr-rair}). By measuring RAIR and RSR for incorrect and correct explanations separately, we can calculate the deception caused by imperfect XAI (refer to \cref{DAIR_aor_eq} on p. \pageref{DAIR_aor_eq}). We do not visualize assertiveness since we do not see any significant direct or moderation effects. 

%- for experts in nle: explanation is helping experts to determine why it is actually wrong
% - for example-based: incorrect ones are differing (potentially) but for correct ones its always the same thing so that's why people are switching and not based on the information content
% - we see the same thing for non-experts: the gap for RAIR is way higher 
% - being interpreted as cofnidence score (add this thought to the discussion section for this)

% when experts see nle - they are able to distinguish when the AI prediction is wrong for correct explanations because it is clear that the characteristics identified don't match 

% for ex -- RSR is dropping -- experts are being misled by the visuals for correct explanations, which is just reinforcing the AI advice  --- but showing visual explanations that are incorrect it is obvious to experts for when the AI is wrong
% Even non-experts can tell if it is three of the same birds or not 

% Assertiveness is playing out visually more than natural language explanations (add this point to the discussion section)


% Figure environment removed

The figure shows that experts have a higher RSR than non-experts for both incorrect and correct explanations across both explanation modalities, validating that experts rely more on their own initial decisions when AI advice is given. The most striking result that emerges from the data is that for example-based explanations, we observe that experts have a significantly higher RSR for incorrect explanations (RSR $= 0.57$) than correct explanations (RSR $= 0.29$), resulting in a negative $DoR_{RSR}$ of $-0.28$ (p-value $= 0.00$).
% A negative difference in deception means that correct example-based explanations are more deceptive than incorrect explanations when the AI advice is incorrect. 
As a result, experts are falsely relying on the AI advice when provided with correct example-based explanations\footnote{Note that correct example-based explanations are consistent in showing three images of the predicted class. Incorrect example-based explanations represent three images that do not correspond to the predicted class of the AI. Moreover, the examples shown are not consistent with the bird species displayed in 90\% of the \textit{correct advice, incorrect explanation} cases and in 40\% of the \textit{incorrect advice, incorrect explanation} cases in our study.}. This means that experts are prone to being misled by correct explanations when the AI advice is incorrect.
However, we do not see this trend for natural language explanations. Here, there is a positive $DoR_{RSR}$ of 0.09, which is not significant (p-value $= 0.41$).
For example-based explanations, the $DoR_{RAIR}$ is positive, meaning that experts rightly follow correct AI advice more often when provided with correct explanations than with incorrect explanations. The data shows a weak, significant positive deception of reliance for example-based explanations ($DoR_{RAIR} = 0.16$, p-value $= 0.10$). Similarly to the RSR cases, for the RAIR cases, the experts are provided with three consistent examples for correct explanations that represent the AI's correctly predicted bird species. The incorrectly provided explanations represent three images that can be inconsistent in the bird species. Thus, experts are deceived by such incorrect explanations even though the AI advice is correct.

Non-experts have, in both modalities, a similar $DoR_{RSR}$ indicating no significant difference in their RSR between correct and incorrect explanations. However, non-experts follow the correct AI advice for correct example-based explanations more often than for incorrect example-based explanations (significant with p-value $= 0.01$). For the latter, the three examples can show inconsistent bird specie(s) that are different from the ground truth of the shown image. Thus, the $DoR_{RAIR}$ for non-experts is at $0.26$. Interestingly, for natural language explanations, the incorrect explanations are not misleading as much ($DoR_{RAIR} = 0.03$, not significant, with a p-value $= 0.68$). This means that non-experts are not misled by incorrect explanations in natural language as much as by visual, example-based explanations. In general, non-experts have a higher RAIR than experts.

Overall, participants have a higher $DoR(RAIR, RSR)$  for example-based explanations (experts: $DoR(RAIR, RSR) = 0.32$; non-experts: $DoR(RAIR, RSR) = 0.26$) than for natural language explanations(experts: $DoR(RAIR, RSR) = 0.11$; non-experts: $DoR(RAIR, RSR) = 0.06$). This means that especially the correctness of example-based explanations has an impact on humans' decision-making behavior.

\subsection{Designing for Imperfect XAI} \label{qual-res}

At the end of the bird identification task, we ask participants: ``\textit{Under what circumstances would you prefer assertive (e.g.,
``definitely'', ``clearly'') versus non-assertive (e.g., ``might be'', ``appears to be'') versus neutral explanations
and why?}''. With imperfect XAI existing in human-AI collaborations, it is necessary not only to understand quantitatively how it impacts decision-makers but also qualitatively. Even though we do not observe the level of assertiveness to have a direct effect or a moderation effect on appropriate reliance, it is still valuable to analyze participants' preferences when it comes to the tone of explanations.

% Figure environment removed

Through inductive content analysis of participants' responses to this question, we derive two dimensions that researchers and designers should consider when developing and evaluating imperfect XAI in human-AI collaborations: \textit{AI Behaviors} and the \textit{Impact on Human-AI Teams}. Each dimension is made up of four themes that are derived from concepts that emerge in the responses, shown in \cref{qual-data}. We highlight those themes in bold. $25\%$ ($34$ participants) of the responses either do not provide reasoning for their opinion or do not answer the question such that it could be grouped into one of the eight themes we identify. We provide quotes from participants for each theme to structure the aggregated dimensions and shape our insights on designing for imperfect XAI. 


\subsubsection{Aggregated Dimension: AI Behaviors}

$54$ out of the $136$ participants answer the survey question with comments relating to the first aggregated dimension: AI Behavior. Participants rationalize that the AI's behavior determines when they prefer assertive, non-assertive, and neutral explanations. Within the AI Behavior dimension, four themes emerge from the participants' comments, such as the model's overall performance, whether the model's prediction is correct or not, the model's confidence in individual predictions, and the correctness/quality of the explanation for a given prediction. 

While only $6$ out of the $136$ participants make comments about the \textbf{model's performance}, it still provides interesting insight that should be considered. Instead of looking at the individual prediction level, these participants focus on the global performance of the model. One participant states that if developers find their model ``\textit{... to be 90\% accurate in your testing, use more definite language, but if it’s not there yet, consider making the tone more neutral and put more responsibility with the end user to interpret the field marks ...}''.

Looking at the individual prediction level, $8$ out of $136$ participants comment on the \textbf{correctness of the AI's prediction}. For example, one participant says that ``\textit{... a non-assertive response would be preferred since the AI selections were incorrect ...}'' while another participant says, ``\textit{I would prefer the assertive language to be accompanied by correct identifications}''.

Considering individual predictions on a more granular level, many participants ($31$ out of $136$) make comments related to \textbf{the confidence of the AI's prediction}. These participants comment on how this factor could be used to determine the tone of the explanation. Specifically, participants, ``\textit{... would prefer the level of assertiveness to depend on the level of confidence of the answer given by the AI}''. One participant expands upon that sentiment by specifying when non-assertive versus assertive tones should be used: ``\textit{I would prefer assertive sentences when the probability of the AI model is very high, while I would prefer non-assertive when the probability is very close to other classes of the model}''. 

$9$ out of $136$ participants make comments related to the \textbf{correctness and quality of the AI explanations}. One participant who sees example-based explanations comments on how some of the examples are incorrect and do not show the correct species. They use this specific situation to rationalize when they would prefer the tone of explanations to be assertive versus non-assertive:  
``\textit{I would prefer assertive explanations if the `similar' photos were actually of the correct species and if the explanation confirmed this. Otherwise, non-assertive explanations are more helpful}''. Another participant who makes a comment related to this theme agrees that assertive language should be used, ``\textit{when all of the reference pictures match up and there are no other similar-looking species}''. 
% Another participant shared a similar sentiment on using a non-assertive tone when the explanations are incorrect, ``\textit{I would prefer non-assertive explanations nearly every time because I didn't have confidence in the AI's explanations (and a few were incorrect)}''.
% When all of the reference pictures match up and there are no other similar looking species it should be assertive.

One participant who sees the natural language explanations comments on the quality and detail of the explanation being a factor to use when determining the tone of the explanation, ``\textit{If the bird description [natural language explanation] is very generic, i.e., brown wings, gray body, or yellow beak (traits that correspond to many birds), I'd rather the AI appear more cautious in its judgment and use non-assertive explanations. However, if the bird has some standout characteristic that the AI correctly identifies [through the natural language explanation], i.e., bright yellow body or red-tipped wings, etc., then assertive explanations seem more convincing}''.


\subsubsection{Aggregated Dimension: Impact on Human-AI Team} 

$48$ out of the $136$ participants answer this survey question with comments related to the human-AI team, such as the confidence and knowledge of the decision-maker, impact on the decision-maker, unambiguous input data, and characteristics of the input data. 

$10$ of $136$ prefer the level of assertiveness to align with their \textbf{own confidence level} and knowledge of the domain. For example, one participant says, ``\textit{I would prefer more assertive explanations when I don't feel very sure about my choice}''. Another participant adds that they would prefer assertive explanations if they ``\textit{... didn't know anything about the topic in question ...}''. However, one participant says, ``\textit{I would prefer neutral explanations if I'm unsure of the species and assertive if I'm confident in my identification}''.

% ``\textit{I would prefer assertive answers if I didn't know anything about the topic in question but would be satisfied with non-assertive if the answer was to back up any doubts of my own answer}''.

% ``\textit{In situations where I am not confident in my knowledge at all, I would prefer assertive explanations so that it would help me definitely understand the subject...I would like non-assertive and neutral explanations when I am somewhat confident, and I must make a decision based on my own knowledge mixed with some additional feedback}''.

$15$ out of the $136$ make comments related to the \textbf{impacts on the decision-maker}. Some comments consist of concerns related to being misled and over- or under-relying on the AI, while other comments motivate the benefits of having assertive explanations. For example, one participant states that ``\textit{Assertive words create more security while changing your opinion or trying to gain knowledge}''. Another participant who shares the same sentiment said, ``\textit{I would prefer assertive explanations because it would make me feel more secure about the answer}''.
However, some participants do not share the same sentiment about assertive explanations and pointed out the potential consequences of them: ``\textit{Assertive AI explanations were given for incorrect identifications, which would mislead users}''. Given that potential to be misled, another participant rationalizes they ``\textit{... would prefer non-assertive explanations because I [they] do not fully trust AI with bird ID just yet}''.

$17$ out of the $136$ participants rationalize that the assertiveness of explanations should be based on how \textbf{ambiguous the input} is for a given prediction. For example, one participant brings up the quality of the input image and the difficulty of the bird ID as a way to determine whether explanations should be assertive or not: ``\textit{When it comes to less distinctive IDs like most sparrows, or harder to ID circumstances like winter or females or juveniles, or situations with weird lighting or harder angles it makes sense to use non-assertive.}''. On a similar line of thought, another participant says, ``\textit{I would prefer assertive explanations for birds that have distinctive traits over similar bird species, and non-assertive or neutral explanations for birds that are similar with characteristics that are more difficult to tell apart}''.
% ``\textit{I would prefer assertive if the photo is clear and able to easily recognize most of the markings of the bird.  I would prefer non-assertive if the markings are not clear. The same applies to neutral explanations}''.

% ``Assertive makes most sense when used for very clear examples of distinctive birds, like the Magnolia Warbler or Cerulean Warbler I had in this set. When it comes to less distinctive ID’s like most sparrows, or harder to ID circumstances like winter or females or juveniles, or situations with weird lighting or harder angles it makes sense to use non-assertive. Many beginning and casual birders take ID’s from Merlin to be absolute so we need more birding tools with nuance.'' 

$6$ out of the $136$ participants commented on how the use of assertive explanations helped them realize various \textbf{characteristics of the birds}. For example, one participant values the assertive tone because it is ``\textit{... helpful for pointing out distinctive features that would help ID the bird.}''. They also think that ``\textit{... the non-assertive language was helpful for species that share similar characteristics (aka clear, non-streaked breast) with other species that share that characteristic}''.