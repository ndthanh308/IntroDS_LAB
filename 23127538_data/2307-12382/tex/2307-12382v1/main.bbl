\begin{thebibliography}{10}

\bibitem{adadi2018xaisurvey}
A.~Adadi and M.~Berrada.
\newblock Peeking inside the black-box: A survey on explainable artificial
  intelligence (xai).
\newblock {\em IEEE Access}, 6:52138--52160, 2018.
  \href{https://doi.org/10.1109/ACCESS.2018.2870052}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}ACCESS\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2018\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2870052}}}


\bibitem{amershi2015modeltracker}
S.~Amershi, M.~Chickering, S.~M. Drucker, B.~Lee, P.~Simard, and J.~Suh.
\newblock Modeltracker: Redesigning performance analysis tools for machine
  learning.
\newblock In {\em Proc. CHI}, pp. 337--346. ACM, New York, 2015.
  \href{https://doi.org/10.1145/2702123.2702509}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1145\discretionary{/}{%
}{/}2702123\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2702509}}}


\bibitem{ayoub2021combat}
J.~Ayoub, X.~J. Yang, and F.~Zhou.
\newblock Combat covid-19 infodemic using explainable natural language
  processing models.
\newblock {\em Inf. Process. Manage.}, 58(4):102569, 2021.
  \href{https://doi.org/10.1016/j.ipm.2021.102569}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1016\discretionary{/}{%
}{/}j\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}ipm\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}102569}}}


\bibitem{xai_survey}
A.~{Barredo Arrieta}, N.~DÃ­az-RodrÃ­guez, J.~{Del Ser}, A.~Bennetot,
  S.~Tabik, A.~Barbado, S.~Garcia, S.~Gil-Lopez, D.~Molina, R.~Benjamins,
  R.~Chatila, and F.~Herrera.
\newblock Explainable artificial intelligence (xai): Concepts, taxonomies,
  opportunities and challenges toward responsible ai.
\newblock {\em Inf. Fusion}, 58:82--115, 2020.
  \href{https://doi.org/10.1016/j.inffus.2019.12.012}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1016\discretionary{/}{%
}{/}j\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}inffus\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2019\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}12\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}012}}}


\bibitem{bhagavatula2019abductive}
C.~Bhagavatula, R.~L. Bras, C.~Malaviya, K.~Sakaguchi, A.~Holtzman, H.~Rashkin,
  D.~Downey, S.~W.-t. Yih, and Y.~Choi.
\newblock Abductive commonsense reasoning.
\newblock In {\em ICLR}, 2020.

\bibitem{bian2023chatgpt}
N.~Bian, X.~Han, L.~Sun, H.~Lin, Y.~Lu, and B.~He.
\newblock Chatgpt is a knowledgeable but inexperienced solver: An investigation
  of commonsense problem in large language models.
\newblock {\em arXiv preprint arXiv:2303.16421}, 2023.
  \href{https://doi.org/10.48550/arXiv.2303.16421}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}48550\discretionary{/}{%
}{/}arXiv\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2303\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}16421}}}


\bibitem{bisk2020piqa}
Y.~Bisk, R.~Zellers, R.~L. Bras, J.~Gao, and Y.~Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In {\em AAAI}, 2020. \href{https://doi.org/10.1609/aaai.v34i05.6239}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1609\discretionary{/}{%
}{/}aaai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}v34i05\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}6239}}}


\bibitem{sharedinterest}
A.~Boggust, B.~Hoover, A.~Satyanarayan, and H.~Strobelt.
\newblock Shared interest: Measuring human-ai alignment to identify recurring
  patterns in model behavior.
\newblock In {\em Proc. CHI}, pp. 1--17. ACM, New York, 2022.
  \href{https://doi.org/10.1145/3491102.3501965}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1145\discretionary{/}{%
}{/}3491102\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}3501965}}}


\bibitem{boratko2020protoqa}
M.~Boratko, X.~L. Li, R.~Das, T.~O'Gorman, D.~Le, and A.~McCallum.
\newblock Protoqa: A question answering dataset for prototypical common-sense
  reasoning.
\newblock In {\em Proc. EMNLP}, pp. 1122--1136. ACL, Online, 2020.
  \href{https://doi.org/10.18653/v1/2020.emnlp-main.85}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}emnlp\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}85}}}


\bibitem{bordes2013translating}
A.~Bordes, N.~Usunier, A.~Garcia-Duran, J.~Weston, and O.~Yakhnenko.
\newblock Translating embeddings for modeling multi-relational data.
\newblock In {\em NeurIPS}, vol.~26, pp. 2787--2795, 2013.

\bibitem{gpt3}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,
  G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu,
  C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess,
  J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em NeurIPS}, vol.~33, pp. 1877--1901, 2020.

\bibitem{chen2020generating}
H.~Chen, G.~Zheng, and Y.~Ji.
\newblock Generating hierarchical explanations on text classification via
  feature interaction detection.
\newblock In {\em Proc. ACL}, pp. 5578--5593. ACL, Online, 2020.
  \href{https://doi.org/10.18653/v1/2020.acl-main.494}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}acl\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}494}}}


\bibitem{cui2020commonsense}
L.~Cui, S.~Cheng, Y.~Wu, and Y.~Zhang.
\newblock On commonsense cues in bert for solving commonsense tasks.
\newblock In {\em Findings of ACL: ACL/IJCNLP}, pp. 683--693. ACL, online,
  2021. \href{https://doi.org/10.18653/v1/2021.findings-acl.61}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}findings\discretionary{%
}{-}{-}acl\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}61}}}


\bibitem{dai-etal-2022-knowledge}
D.~Dai, L.~Dong, Y.~Hao, Z.~Sui, B.~Chang, and F.~Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock In {\em Proc. ACL}, pp. 8493--8502. ACL, Dublin, Ireland, 2022.
  \href{https://doi.org/10.18653/v1/2022.acl-long.581}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2022\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}acl\discretionary{%
}{-}{-}long\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}581}}}


\bibitem{de2021editing}
N.~De~Cao, W.~Aziz, and I.~Titov.
\newblock Editing factual knowledge in language models.
\newblock In {\em Proc. EMNLP}, pp. 6491--6506. ACL, Online and Punta Cana,
  Dominican Republic, 2021.
  \href{https://doi.org/10.18653/v1/2021.emnlp-main.522}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}emnlp\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}522}}}


\bibitem{dinu2014improving}
G.~Dinu and M.~Baroni.
\newblock Improving zero-shot learning by mitigating the hubness problem.
\newblock In {\em ICLR}, 2015.

\bibitem{mediators}
N.~Feldhus, A.~M. Ravichandran, and S.~M{\"o}ller.
\newblock Mediators: Conversational agents explaining nlp model behavior.
\newblock {\em arXiv preprint arXiv:2206.06029}, 2022.
  \href{https://doi.org/10.48550/arXiv.2206.06029}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}48550\discretionary{/}{%
}{/}arXiv\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2206\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}06029}}}


\bibitem{feng2020scalable}
Y.~Feng, X.~Chen, B.~Y. Lin, P.~Wang, J.~Yan, and X.~Ren.
\newblock Scalable multi-hop relational reasoning for knowledge-aware question
  answering.
\newblock In {\em Proc. EMNLP}, pp. 1295--1309. ACL, Online, 2020.
  \href{https://doi.org/10.18653/v1/2020.emnlp-main.99}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}emnlp\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}99}}}


\bibitem{feng2023xnli}
Y.~Feng, X.~Wang, B.~Pan, K.~K. Wong, Y.~Ren, S.~Liu, Z.~Yan, Y.~Ma, H.~Qu, and
  W.~Chen.
\newblock Xnli: Explaining and diagnosing nli-based visual data analysis.
\newblock {\em IEEE Trans. Visual Comput. Graphics}, 2023.
  \href{https://doi.org/10.1109/TVCG.2023.3240003}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}TVCG\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2023\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}3240003}}}


\bibitem{hoover2019exbert}
B.~Hoover, H.~Strobelt, and S.~Gehrmann.
\newblock ex{BERT}: {A} visual analysis tool to explore learned representations
  in transformer models.
\newblock In {\em Proc. ACL: System Demonstrations}, pp. 187--196, 2020.
  \href{https://doi.org/10.18653/v1/2020.acl-demos.22}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}acl\discretionary{%
}{-}{-}demos\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}22}}}


\bibitem{huang2019cosmos}
L.~Huang, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Cosmos qa: Machine reading comprehension with contextual commonsense
  reasoning.
\newblock In {\em Proc. EMNLP}, pp. 2391--2401. ACL, Hong Kong, 2019.
  \href{https://doi.org/10.18653/v1/D19-1243}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}D19\discretionary{%
}{-}{-}1243}}}


\bibitem{atomic2020}
J.~D. Hwang, C.~Bhagavatula, R.~L. Bras, J.~Da, K.~Sakaguchi, A.~Bosselut, and
  Y.~Choi.
\newblock Comet-atomic 2020: On symbolic and neural commonsense knowledge
  graphs.
\newblock In {\em AAAI}, pp. 6384--6392, 2021.
  \href{http://dx.doi.org/10.1609/aaai.v35i7.16792}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1609\discretionary{/}{%
}{/}aaai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}v35i7\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}16792}}}


\bibitem{jin2023shortcutlens}
Z.~Jin, X.~Wang, F.~Cheng, C.~Sun, Q.~Liu, and H.~Qu.
\newblock Shortcutlens: A visual analytics approach for exploring shortcuts in
  natural language understanding dataset.
\newblock {\em IEEE Trans. Visual Comput. Graphics}, 2023.
  \href{https://doi.org/10.1109/TVCG.2023.3236380}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}TVCG\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2023\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}3236380}}}


\bibitem{jin2022gnnlens}
Z.~Jin, Y.~Wang, Q.~Wang, Y.~Ming, T.~Ma, and H.~Qu.
\newblock Gnnlens: A visual analytics approach for prediction error diagnosis
  of graph neural networks.
\newblock {\em IEEE Trans. Visual Comput. Graphics}, 2022.
  \href{https://doi.org/10.1109/TVCG.2022.3148107}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}TVCG\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2022\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}3148107}}}


\bibitem{kaushik2019learning}
D.~Kaushik, E.~Hovy, and Z.~C. Lipton.
\newblock Learning the difference that makes a difference with
  counterfactually-augmented data.
\newblock In {\em ICLR}, 2020.

\bibitem{khashabi2022unifiedqa}
D.~Khashabi, Y.~Kordi, and H.~Hajishirzi.
\newblock Unifiedqa-v2: Stronger generalization via broader cross-format
  training.
\newblock {\em arXiv preprint arXiv:2202.12359}, 2022.
  \href{https://doi.org/10.48550/arXiv.2202.12359}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}48550\discretionary{/}{%
}{/}arXiv\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2202\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}12359}}}


\bibitem{2020unifiedqa}
D.~Khashabi, S.~Min, T.~Khot, A.~Sabhwaral, O.~Tafjord, P.~Clark, and
  H.~Hajishirzi.
\newblock Unifiedqa: Crossing format boundaries with a single qa system.
\newblock In {\em Findings of ACL: EMNLP}, pp. 1896--1907. ACL, online, 2020.
  \href{https://doi.org/10.18653/v1/2020.findings-emnlp.171}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}findings\discretionary{%
}{-}{-}emnlp\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}171}}}


\bibitem{li-etal-2022-systematic}
X.~L. Li, A.~Kuncoro, J.~Hoffmann, C.~de~Masson~d{'}Autume, P.~Blunsom, and
  A.~Nematzadeh.
\newblock A systematic investigation of commonsense knowledge in large language
  models.
\newblock In {\em Proc. EMNLP}, pp. 11838--11855. ACL, Abu Dhabi, 2022.

\bibitem{li2022unified}
Z.~Li, X.~Wang, W.~Yang, J.~Wu, Z.~Zhang, Z.~Liu, M.~Sun, H.~Zhang, and S.~Liu.
\newblock A unified understanding of deep nlp models for text classification.
\newblock {\em IEEE Trans. Visual Comput. Graphics}, 28(12):4980--4994, 2022.
  \href{https://doi.org/10.1109/TVCG.2022.3184186}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}TVCG\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2022\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}3184186}}}


\bibitem{liang2022multiviz}
P.~P. Liang, Y.~Lyu, G.~Chhablani, N.~Jain, Z.~Deng, X.~Wang, L.-P. Morency,
  and R.~Salakhutdinov.
\newblock Multiviz: Towards visualizing and understanding multimodal models.
\newblock In {\em ICLR}, 2022.

\bibitem{lin2019kagnet}
B.~Y. Lin, X.~Chen, J.~Chen, and X.~Ren.
\newblock Kagnet: Knowledge-aware graph networks for commonsense reasoning.
\newblock In {\em Proc. EMNLP}, pp. 2829--2839. ACL, Hong Kong, 2019.
  \href{https://doi.org/10.18653/v1/D19-1282}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}D19\discretionary{%
}{-}{-}1282}}}


\bibitem{lin2020differentiable}
B.~Y. Lin, H.~Sun, B.~Dhingra, M.~Zaheer, X.~Ren, and W.~W. Cohen.
\newblock Differentiable open-ended commonsense reasoning.
\newblock In {\em Proc. NAACL}, pp. 4611--4625. ACL, Online, 2021.
  \href{https://doi.org/10.18653/v1/2021.naacl-main.366}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}naacl\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}366}}}


\bibitem{lin2021riddlesense}
B.~Y. Lin, Z.~Wu, Y.~Yang, D.-H. Lee, and X.~Ren.
\newblock Riddlesense: Reasoning about riddle questions featuring linguistic
  creativity and commonsense knowledge.
\newblock In {\em Findings of ACL: ACL/IJCNLP}, pp. 1504--1515. ACL, Online,
  2021. \href{https://doi.org/10.18653/v1/2021.findings-acl.131}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}findings\discretionary{%
}{-}{-}acl\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}131}}}


\bibitem{liu2021generated}
J.~Liu, A.~Liu, X.~Lu, S.~Welleck, P.~West, R.~L. Bras, Y.~Choi, and
  H.~Hajishirzi.
\newblock Generated knowledge prompting for commonsense reasoning.
\newblock In {\em Proc. ACL}, pp. 3154--3169. ACL, Dublin, Ireland, 2022.
  \href{https://doi.org/10.18653/v1/2022.acl-long.225}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2022\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}acl\discretionary{%
}{-}{-}long\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}225}}}


\bibitem{unicorn}
N.~Lourie, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Unicorn on rainbow: A universal commonsense reasoning model on a new
  multitask benchmark.
\newblock In {\em AAAI}, pp. 13480--13488, 2021.
  \href{https://doi.org/10.1609/aaai.v35i15.17590}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1609\discretionary{/}{%
}{/}aaai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}v35i15\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}17590}}}


\bibitem{lundberg2017unified}
S.~M. Lundberg and S.-I. Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In {\em NeurIPS}, vol.~30, pp. 4765--4774, 2017.

\bibitem{ma2021knowledge}
K.~Ma, F.~Ilievski, J.~Francis, Y.~Bisk, E.~Nyberg, and A.~Oltramari.
\newblock Knowledge-driven data construction for zero-shot evaluation in
  commonsense question answering.
\newblock In {\em AAAI}, vol.~35, pp. 13507--13515, 2021.
  \href{https://doi.org/10.1609/aaai.v35i15.17593}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1609\discretionary{/}{%
}{/}aaai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}v35i15\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}17593}}}


\bibitem{manning1999foundations}
C.~Manning and H.~Schutze.
\newblock {\em Foundations of statistical natural language processing}.
\newblock MIT press, 1999.

\bibitem{mcinnes2018umap}
L.~McInnes, J.~Healy, and J.~Melville.
\newblock Umap: Uniform manifold approximation and projection for dimension
  reduction.
\newblock {\em arXiv preprint arXiv:1802.03426}, 2018.
  \href{https://doi.org/10.48550/arXiv.1802.03426}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}48550\discretionary{/}{%
}{/}arXiv\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1802\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}03426}}}


\bibitem{meng2022locating}
K.~Meng, D.~Bau, A.~Andonian, and Y.~Belinkov.
\newblock Locating and editing factual knowledge in gpt.
\newblock In {\em ICLR}, 2022.

\bibitem{meng2022mass}
K.~Meng, A.~S. Sharma, A.~Andonian, Y.~Belinkov, and D.~Bau.
\newblock Mass-editing memory in a transformer.
\newblock In {\em ICLR}, 2023.

\bibitem{ming2017understanding}
Y.~Ming, S.~Cao, R.~Zhang, Z.~Li, Y.~Chen, Y.~Song, and H.~Qu.
\newblock Understanding hidden memories of recurrent neural networks.
\newblock In {\em Proc. VAST}, pp. 13--24. IEEE, 2017.
  \href{https://doi.org/10.1109/VAST.2017.8585721}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}VAST\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2017\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}8585721}}}


\bibitem{mitchell2022fast}
E.~Mitchell, C.~Lin, A.~Bosselut, C.~Finn, and C.~D. Manning.
\newblock Fast model editing at scale.
\newblock In {\em ICLR}, 2022.

\bibitem{glucose}
N.~Mostafazadeh, A.~Kalyanpur, L.~Moon, D.~Buchanan, L.~Berkowitz, O.~Biran,
  and J.~Chu-Carroll.
\newblock {GLUCOSE}: {G}enera{L}ized and {CO}ntextualized story explanations.
\newblock In {\em Proc. EMNLP}, pp. 4569--4586. ACL, Online, 2020.
  \href{https://doi.org/10.18653/v1/2020.emnlp-main.370}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}emnlp\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}370}}}


\bibitem{DBLP:journals/corr/abs-2203-02155}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~L. Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray, J.~Schulman, J.~Hilton, F.~Kelton, L.~Miller,
  M.~Simens, A.~Askell, P.~Welinder, P.~F. Christiano, J.~Leike, and R.~Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In {\em NeurIPS}, vol.~35, pp. 27730--27744, 2022.

\bibitem{petroni2019language}
F.~Petroni, T.~Rockt{\"a}schel, P.~Lewis, A.~Bakhtin, Y.~Wu, A.~H. Miller, and
  S.~Riedel.
\newblock Language models as knowledge bases?
\newblock In {\em Proc. EMNLP}, pp. 2463--2473. ACL, Hong Kong, 2019.
  \href{https://doi.org/10.18653/v1/D19-1250}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}D19\discretionary{%
}{-}{-}1250}}}


\bibitem{poyiadzi2020face}
R.~Poyiadzi, K.~Sokol, R.~Santos-Rodriguez, T.~De~Bie, and P.~Flach.
\newblock Face: Feasible and actionable counterfactual explanations.
\newblock In {\em Proc. AIES}, pp. 344--350, 2020.
  \href{https://doi.org/10.1145/3375627.3375850}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1145\discretionary{/}{%
}{/}3375627\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}3375850}}}


\bibitem{t5model}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em J. Mach. Learn. Res.}, 21(140):1--67, 2020.

\bibitem{rajani2019explain}
N.~F. Rajani, B.~McCann, C.~Xiong, and R.~Socher.
\newblock Explain yourself! leveraging language models for commonsense
  reasoning.
\newblock In {\em Proc. ACL}, pp. 4932--4942. ACL, Florence, Italy, 2019.
  \href{https://doi.org/10.18653/v1/p19-1487}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}p19\discretionary{%
}{-}{-}1487}}}


\bibitem{reif2019visualizing}
E.~Reif, A.~Yuan, M.~Wattenberg, F.~B. Viegas, A.~Coenen, A.~Pearce, and
  B.~Kim.
\newblock Visualizing and measuring the geometry of bert.
\newblock In {\em NeurIPS}, vol.~32, pp. 8592--8600, 2019.

\bibitem{ribeiro2016should}
M.~T. Ribeiro, S.~Singh, and C.~Guestrin.
\newblock "{Why} should i trust you?": Explaining the predictions of any
  classifier.
\newblock In {\em Proc. KDD}, pp. 1135--1144. ACM, New York, 2016.
  \href{https://doi.org/10.1145/2939672.2939778}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1145\discretionary{/}{%
}{/}2939672\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2939778}}}


\bibitem{sakaguchi2021winogrande}
K.~Sakaguchi, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock {\em Communications of the ACM}, 64(9):99--106, 2021.
  \href{https://doi.org/10.1145/3474381}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1145\discretionary{/}{%
}{/}3474381}}}


\bibitem{atomic2019}
M.~Sap, R.~Le~Bras, E.~Allaway, C.~Bhagavatula, N.~Lourie, H.~Rashkin, B.~Roof,
  N.~A. Smith, and Y.~Choi.
\newblock Atomic: An atlas of machine commonsense for if-then reasoning.
\newblock In {\em AAAI}, vol.~33, pp. 3027--3035, 2019.
  \href{https://doi.org/10.1609/aaai.v33i01.33013027}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1609\discretionary{/}{%
}{/}aaai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}v33i01\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}33013027}}}


\bibitem{socialiqa}
M.~Sap, H.~Rashkin, D.~Chen, R.~L. Bras, and Y.~Choi.
\newblock Social iqa: Commonsense reasoning about social interactions.
\newblock In {\em Proc. EMNLP}, pp. 4462--4472. ACL, Hong Kong, 2019.
  \href{https://doi.org/10.18653/v1/D19-1454}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}D19\discretionary{%
}{-}{-}1454}}}


\bibitem{schwab2019cxplain}
P.~Schwab and W.~Karlen.
\newblock Cxplain: Causal explanations for model interpretation under
  uncertainty.
\newblock In {\em NeurIPS}, vol.~32, pp. 10220--10230, 2019.

\bibitem{shwartz2020unsupervised}
V.~Shwartz, P.~West, R.~L. Bras, C.~Bhagavatula, and Y.~Choi.
\newblock Unsupervised commonsense question answering with self-talk.
\newblock In {\em Proc. EMNLP}, pp. 4615--4629. ACL, Online, 2020.
  \href{https://doi.org/10.18653/v1/2020.emnlp-main.373}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}emnlp\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}373}}}


\bibitem{singh2021com2sense}
S.~Singh, N.~Wen, Y.~Hou, P.~Alipoormolabashi, T.-L. Wu, X.~Ma, and N.~Peng.
\newblock Com2sense: A commonsense reasoning benchmark with complementary
  sentences.
\newblock In {\em Findings of ACL: ACL/IJCNLP}, pp. 883--898. ACL, online,
  2021. \href{https://doi.org/10.18653/v1/2021.findings-acl.78}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}findings\discretionary{%
}{-}{-}acl\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}78}}}


\bibitem{slack2021counterfactual}
D.~Slack, A.~Hilgard, H.~Lakkaraju, and S.~Singh.
\newblock Counterfactual explanations can be manipulated.
\newblock In {\em NeurIPS}, vol.~34, pp. 62--75, 2021.

\bibitem{conceptnet}
R.~Speer, J.~Chin, and C.~Havasi.
\newblock Conceptnet 5.5: An open multilingual graph of general knowledge.
\newblock In {\em AAAI}, pp. 4444--4451, 2017.

\bibitem{strobelt2017lstmvis}
H.~Strobelt, S.~Gehrmann, H.~Pfister, and A.~M. Rush.
\newblock Lstmvis: A tool for visual analysis of hidden state dynamics in
  recurrent neural networks.
\newblock {\em IEEE Trans. Visual Comput. Graphics}, 24(1):667--676, 2018.
  \href{https://doi.org/10.1109/TVCG.2017.2744158}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}TVCG\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2017\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2744158}}}


\bibitem{sundararajan2017axiomatic}
M.~Sundararajan, A.~Taly, and Q.~Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In {\em Proc. ICML}, pp. 3319--3328. PMLR, 2017.

\bibitem{CSQA1}
A.~Talmor, J.~Herzig, N.~Lourie, and J.~Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense
  knowledge.
\newblock In {\em Proc. NAACL}, pp. 4149--4158. ACL, Minneapolis, Minnesota,
  2019. \href{https://doi.org/10.18653/v1/n19-1421}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}n19\discretionary{%
}{-}{-}1421}}}


\bibitem{CSQA2}
A.~Talmor, O.~Yoran, R.~L. Bras, C.~Bhagavatula, Y.~Goldberg, Y.~Choi, and
  J.~Berant.
\newblock Commonsenseqa 2.0: Exposing the limits of {AI} through gamification.
\newblock In {\em NeurIPS Datasets and Benchmarks Track}, vol.~1, 2021.

\bibitem{webchild}
N.~Tandon, G.~de~Melo, F.~Suchanek, and G.~Weikum.
\newblock Webchild: Harvesting and organizing commonsense knowledge from the
  web.
\newblock In {\em Proc. WSDM}, p. 523–532. ACM, New York, 2014.
  \href{https://doi.org/10.1145/2556195.2556245}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1145\discretionary{/}{%
}{/}2556195\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2556245}}}


\bibitem{tenney2019bert}
I.~Tenney, D.~Das, and E.~Pavlick.
\newblock Bert rediscovers the classical nlp pipeline.
\newblock In {\em Proc. ACL}, pp. 4593--4601. ACL, Florence, Italy, 2019.
  \href{https://doi.org/10.18653/v1/p19-1452}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}p19\discretionary{%
}{-}{-}1452}}}


\bibitem{tenney2020language}
I.~Tenney, J.~Wexler, J.~Bastings, T.~Bolukbasi, A.~Coenen, S.~Gehrmann,
  E.~Jiang, M.~Pushkarna, C.~Radebaugh, E.~Reif, et~al.
\newblock The language interpretability tool: Extensible, interactive
  visualizations and analysis for nlp models.
\newblock In {\em Proc. EMNLP: System Demonstrations}, pp. 107--118, 2020.
  \href{https://doi.org/10.18653/v1/2020.emnlp-demos.15}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}emnlp\discretionary{%
}{-}{-}demos\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}15}}}


\bibitem{vig2019bertviz}
J.~Vig.
\newblock Bertviz: A tool for visualizing multihead self-attention in the bert
  model.
\newblock In {\em ICLR Workshop: Debugging Machine Learning Models}, 2019.

\bibitem{voita-titov-2020-information}
E.~Voita and I.~Titov.
\newblock Information-theoretic probing with minimum description length.
\newblock In {\em Proc. EMNLP}, pp. 183--196. ACL, Online, Nov. 2020.
  \href{https://doi.org/10.18653/v1/2020.emnlp-main.14}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2020\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}emnlp\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}14}}}


\bibitem{wang2021m2lens}
X.~Wang, J.~He, Z.~Jin, M.~Yang, Y.~Wang, and H.~Qu.
\newblock M2lens: Visualizing and explaining multimodal models for sentiment
  analysis.
\newblock {\em IEEE Trans. Visual Comput. Graphics}, 28(1):802--812, 2021.
  \href{https://doi.org/10.1109/TVCG.2021.3114794}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}TVCG\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}3114794}}}


\bibitem{DBLP:conf/emnlp/WeiZ19}
J.~W. Wei and K.~Zou.
\newblock {EDA:} easy data augmentation techniques for boosting performance on
  text classification tasks.
\newblock In {\em Proc. EMNLP}, pp. 6381--6387. ACL, Hong Kong, 2019.
  \href{https://doi.org/10.18653/v1/D19-1670}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}D19\discretionary{%
}{-}{-}1670}}}


\bibitem{whatiftool}
J.~Wexler, M.~Pushkarna, T.~Bolukbasi, M.~Wattenberg, F.~Vi{\'e}gas, and
  J.~Wilson.
\newblock The what-if tool: Interactive probing of machine learning models.
\newblock {\em IEEE Trans. Visual Comput. Graphics}, 26(1):56--65, 2019.
  \href{https://doi.org/10.1109/TVCG.2019.2934619}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1109\discretionary{/}{%
}{/}TVCG\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2019\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2934619}}}


\bibitem{wu2021polyjuice}
T.~Wu, M.~T. Ribeiro, J.~Heer, and D.~S. Weld.
\newblock Polyjuice: Generating counterfactuals for explaining, evaluating, and
  improving models.
\newblock In {\em Proc. ACL}, pp. 6707--6723. ACL, Online, 2021.
  \href{https://doi.org/10.18653/v1/2021.acl-long.523}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}acl\discretionary{%
}{-}{-}long\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}523}}}


\bibitem{humanparity}
Y.~Xu, C.~Zhu, S.~Wang, S.~Sun, H.~Cheng, X.~Liu, J.~Gao, P.~He, M.~Zeng, and
  X.~Huang.
\newblock Human parity on commonsenseqa: Augmenting self-attention with
  external attention.
\newblock In {\em Proc. IJCAI}, pp. 2762--2768, 2022.
  \href{https://doi.org/10.24963/ijcai.2022/383}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}24963\discretionary{/}{%
}{/}ijcai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2022\discretionary{/}{%
}{/}383}}}


\bibitem{yasunaga2021qagnn}
M.~Yasunaga, H.~Ren, A.~Bosselut, P.~Liang, and J.~Leskovec.
\newblock Qa-gnn: Reasoning with language models and knowledge graphs for
  question answering.
\newblock In {\em Proc. NAACL}, pp. 535--546. ACL, online, 2021.
  \href{https://doi.org/10.18653/v1/2021.naacl-main.45}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}2021\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}naacl\discretionary{%
}{-}{-}main\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}45}}}


\bibitem{zellers2019hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In {\em Proc. ACL}, pp. 4791--4800. ACL, Florence, Italy, 2019.
  \href{https://doi.org/10.18653/v1/p19-1472}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}18653\discretionary{/}{%
}{/}v1\discretionary{/}{%
}{/}p19\discretionary{%
}{-}{-}1472}}}


\bibitem{transomcs}
H.~Zhang, D.~Khashabi, Y.~Song, and D.~Roth.
\newblock Transomcs: From linguistic graphs to commonsense knowledge.
\newblock In {\em Proc. IJCAI}, pp. 4004--4010, 2020.
  \href{https://doi.org/10.24963/ijcai.2020/554}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}24963\discretionary{/}{%
}{/}ijcai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}2020\discretionary{/}{%
}{/}554}}}


\bibitem{zhou2020evaluating}
X.~Zhou, Y.~Zhang, L.~Cui, and D.~Huang.
\newblock Evaluating commonsense in pre-trained language models.
\newblock In {\em AAAI}, vol.~34, pp. 9733--9740, 2020.
  \href{https://doi.org/10.1609/aaai.v34i05.6523}
{doi: {{%
10\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}1609\discretionary{/}{%
}{/}aaai\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}v34i05\hspace{.1pt}\discretionary{.}{%
}{.}\hspace{.4pt}6523}}}


\end{thebibliography}
