\section{System \& Methods}
\label{sec.system}
\begin{comment}
% system introduction
Based on the design requirements, we design and implement \name{}, 
\rev{a visual analytics system that can contextualize and visualize the commonsense reasoning capabilities of NLP models in a systematic and scalable manner.
% Particularly, \name{} adopts an external commonsense knowledge base to derive and summarize the implicit commonsense knowledge in input data. 
% Then, based on the extracted knowledge, the system contextualizes and visualizes the model behaviors on different concepts and their implicit relations to guide multi-faceted and multi-level exploration of model.
% aligns the model behavior with human commonsense knowledge
Particularly, \name{} leverages an external \textbf{knowledge graph} to 1) derive and summarize commonsense knowledge in input data with concepts and their relations, and  2) facilitate contextualized multi-level exploration and diagnosis of model behaviors on different concepts and their implicit relations.}
% based on feature attribution methods and external commonsense knowledge bases.

Question answering (QA) is a common way to evaluate model understanding and reasoning over natural language~\cite{sap2020commonsense}. And most commonsense reasoning benchmark datasets adopt the QA form~\cite{socialiqa, bisk2020piqa, sakaguchi2021winogrande, 2020unifiedqa, CSQA1, CSQA2}.
% , which is designed so that the answers require commonsense knowledge and reasoning and cannot be directly derived from the question contexts. 
Therefore, \rev{without loss of generality, in this paper, we target commonsense QA tasks to showcase our system.}
% , in this paper, we target commonsense QA and choose one representative benchmark, CSQA~\cite{CSQA1}, to showcase our system.

In this section, we describe the system framework, input data and model, methods of extracting commonsense knowledge and  contextualizing model behaviors, and the interface designs of \name{}.
\end{comment}

\vis{
% Motivated by the design requirements, 
% We design \name{}, a visual analytics system that enables scalable and systematic analysis of NLP models' commonsense reasoning capabilities. 
Our system, \name{}, leverages an external knowledge graph to summarize and derive commonsense knowledge and facilitate multi-level exploration and diagnosis of model behaviors in commonsense question-answering tasks. We focus on question-answering tasks because they are a common evaluation method for natural language understanding, and most commonsense reasoning benchmarks adopt the QA format~\cite{socialiqa, bisk2020piqa, sakaguchi2021winogrande, 2020unifiedqa, CSQA1, CSQA2}.}
% We describe our system framework, input data and model, methods for extracting commonsense knowledge and contextualizing model behaviors, and the interface design of \name{}.}

% \vis{Based on the design requirements, we design and implement \name{}, a visual analytics system that enables systematic and scalable analysis of NLP models' commonsense reasoning capabilities. Our system leverages an external knowledge graph to 1) derive and summarize commonsense knowledge, and 2) facilitate contextualized multi-level exploration and diagnosis of model behaviors. Specifically, we focus on commonsense question-answering (QA) tasks to showcase our system's capabilities, given that QA is a common evaluation method for natural language understanding~\cite{sap2020commonsense} and most commonsense reasoning benchmarks adopt the QA form~\cite{socialiqa, bisk2020piqa, sakaguchi2021winogrande, 2020unifiedqa, CSQA1, CSQA2}. In this section, we describe our system framework, input data and model, methods for extracting commonsense knowledge and contextualizing model behaviors, and the interface design of CommonsenseVIS.}

% In this section, we first describe the system framework (\autoref{subsec.system_framework}). 
% \rev{Then, we describe the system input data and model (\autoref{subsec.system_data}), followed by the methods of extracting commonsense knowledge from data (\autoref{subsec.extract_commonsense}) and contextualizing model behaviors (\autoref{subsec.align_model}). Finally, we introduce the interface designs of \name{} (\autoref{sec.user_interface}).}

% Then, we describe the methods of model behavior contextualization. After that, we introduce the visualization and interaction designs. 

% % Figure environment removed 

% Figure environment removed 

\revv{\subsection{System Overview}}
\label{subsec.system_framework}
% \vis{\autoref{fig:system_framework}} summarizes the system framework. 
\revv{\autoref{fig:system_framework}} provides an overview of our system. \name{} takes in QA instances and an NLP model to compute model answers. Then, it identifies important concepts (\ie, words) in questions using feature attribution methods and extracts relevant commonsense knowledge from input data using an external knowledge base. This knowledge helps align the model behavior with \cpn{}. The user interface enables multi-level exploration, interactive probing, and editing.
% This knowledge serves as a reference for aligning model behavior with ConceptNet knowledge. The user interface facilitates multi-level exploration of model behavior, interactive model probing and editing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \autoref{fig:system_framework} summarizes the system framework. Given the QA instances and an NLP model, the system computes the model answers to the questions.
% % (in \autoref{fig:system_framework}A). 
% Then, the system identifies the concepts (\ie, words) in the questions that are important to the model decisions based on feature attribution methods. Meanwhile, the system extracts relevant commonsense knowledge contained in input data  (represented as graphs with words in questions and answers being nodes and their relations being links) based on a large external commonsense knowledge base.
% % ---ConceptNet~\cite{conceptnet}. 
% This knowledge is set as contextual references for 
% % the model behavior.
% % Then, the system generates model explanations by 
% aligning the model behavior with ConceptNet knowledge about different concepts and relations.
% % with the extracted knowledge regarding the input question concepts and their underlying relations.
% % (in \autoref{fig:system_framework}C).
% % as well as the concepts in the questions that are important to the model decisions based on feature attribution methods (\ie, SHAP in our case).
% % Meanwhile, the system extracts relevant commonsense knowledge contained in data instances (represented as graphs with question concepts being nodes and their relations being links) based on a large external commonsense knowledge base---ConceptNet~\cite{conceptnet}.
% % This knowledge is set as contextual references for the model behavior.
% % Then, the system generated model explanations by aligning the model behavior with the extracted knowledge regarding the question concepts and their underlying relations between each other.
% Finally, the user interface 
% % (\autoref{fig:teaser}) 
% guides the multi-level and multi-faceted exploration of the model explanations and enables interactive model probing and editing. 
% % probing of models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{System Data \& Model}\label{subsec.system_data}
\rev{Here, we introduce the system input, including the QA data, model, and external knowledge base for contextualizing model behavior.
% Before diving into the contextualization and visualization of the model's commonsense reasoning capabilities, we describe the system input, including the QA data, model, and external commonsense knowledge base.

\textbf{QA data.} Each QA instance contains a \emph{question concept}, a \emph{target concept (\ie, answer)}, \emph{alternative answers} (if any), and a \emph{question stem}.
Following the previous commonsense QA benchmarks~\cite{CSQA1,CSQA2,atomic2019}, \revv{\textbf{concepts} are defined as words, and question stems provides \textbf{contexts} for the commonsense relations between the question and target concepts.}
\revv{As shown in \autoref{fig:system_framework}A, the question concept is air conditioning, the target concept is house, and air conditioning is located at the house. And the context in the question stem is: ``A man...watches the game on Saturday...''.}
% the question concept (QC)/prompt, the target concept (TC)/answer, alternative answers (if any), and the question stem (QS). 
% The QS probes the commonsense relations between QC and TC.
% The QS suggests the context and commonsense relation between QC and TC, while the alternative choices are set as distractors.
% \\\textbf{clarify the relations between QC, TC, and QS.}\\
% This data format is adopted by many commonsense QA benchmarks~\cite{CSQA1,CSQA2,atomic2019}.
% And if a question concept is not presented, 
If a question concept is absent,
knowledge graph embedding methods~\cite{bordes2013translating} can be used to determine the relation strength between the target concept and words in the question stem. The word with the highest score becomes the question concept~\cite{lin2019kagnet}.
% If a question concept is absent,
% users can apply knowledge graph embedding methods~\cite{bordes2013translating} to score the relation strength between the target concept and words in the question stem and find the one with the highest score as the question concept~\cite{lin2019kagnet}.
% and the concepts in the question stem and find the one with the highest score as the QC~\cite{lin2019kagnet}. 
% Then, users can prepare the desired QA input structure.

% For the demonstration purpose, we choose one representative commonsense QA benchmark---CSQA~\cite{CSQA1}.
We utilize one representative commonsense QA benchmark, CSQA~\cite{CSQA1}, for demonstration.
The dataset has 12,102 multiple-choice questions, covering diverse topics and various forms of commonsense.
Each human-authored question contextualizes relations between a question concept and a target concept (\ie, the correct answer among five candidates). 
Triplets of these concepts and relations are drawn from \cpn{}~\cite{conceptnet}.
% incorporating a wide range of commonsense, such as spatial, social, and casual. 
% Each question has one correct answer and four distractors and is authored by humans to reflect the context and commonsense relation between a question concept and a target concept. The triplets of question concepts, target concepts and their relation are drawn from \cpn{}~\cite{conceptnet}.
The most frequent question concepts are about people, water, and animals, probing various relations such as spatial (41\%) and causal (23\%).
Questions are formulated in diverse forms (\eg, wh-questions, statements, and hypotheses) with 13 words on average.

\textbf{QA model.}
Our system is designed to accommodate various NLP models that select answers to given questions, as it focuses on the input-output model behavior and we can adopt model-agnostic feature attribution methods to quantify this behavior.

\revv{For the purpose of system demonstration, we have chosen UnifiedQA\footnote{\small \url{https://huggingface.co/allenai/unifiedqa-v2-t5-large-1363200}}~\cite{2020unifiedqa, khashabi2022unifiedqa} as an example for model analysis. It is an open-source, general QA model that has been pre-trained across various QA datasets, showing great generalization capabilities.
We use SHAP to compute the importance scores of model inputs because of its strong theoretical foundation and widespread adoption in various domains.}

% The system can accept various NLP models that select answers to the given questions, as it focuses on the input-output behavior of the models and can adopt model-agnostic feature attribution methods (e.g., SHAP, LIME) to quantify such behavior. In this paper, for the system demonstration, 
% we use UnifiedQA~\cite{2020unifiedqa, khashabi2022unifiedqa}--- an open-sourced general QA model pretrained across different QA datasets---as an example for model analysis. 
% \revv{We choose SHAP to compute the importance scores of model inputs because of its widespread recognition and adoption across various domain applications.}


% Given the QA data, the system can accept various QA NLP models since the system focuses on the input-output behavior of the models and can adopt model-agnostic feature attribution methods (\eg, SHAP, LIME) to quantify such behavior.

% In this paper, we use UnifiedQA~\cite{2020unifiedqa}, a state-of-the-art QA model, as an example for model analysis. 
% Similar to other modern natural language models, it is built upon a large base model~\cite{t5model} and trained across different QA datasets. 
% Given a question with multiple choices from CSQA, the model selects one as the answer.
\textbf{Commonsense knowledge base.}
We utilize an external knowledge base to capture the commonsense knowledge in the QA data, which provides context for inferring the model's implicit reasoning.  To ensure meaningful and helpful context for model analysis, the knowledge base must \textit{sufficiently cover relevant commonsense }reflected in the QA data. 

We adopt \cpn{}~\cite{conceptnet} as an external resource, a large-scale commonsense knowledge graph connecting \textbf{concepts} (\ie, words) with \textbf{relations}.
% (\eg, \hl{be capable of} links words \hl{people} to \hl{driving}). 
The graph integrates diverse knowledge sources with over 8 million nodes and over 21 million links.
Particularly, it uses 36 general relations (\eg, \hl{IsA}, \hl{UsedFor}) to connect words, mostly covering taxonomic, lexical knowledge, and physical commonsense knowledge.
% (e.g., \hl{IsA})
% (\eg, \hl{Synonym})
% (\eg, \hl{MadeOf}, \hl{PartOf}).
ConceptNet is widely used to enhance NLP models with commonsense capabilities~\cite{lin2019kagnet, feng2020scalable} and build reasoning benchmarks~\cite{CSQA1, CSQA2, lin2021riddlesense}. 
For example, \revv{the questions and answers in CSQA are based on word-relation triplets (\texttt{A}, \texttt{Relation}, \texttt{B}) from ConceptNet.
The prevalent relations include \texttt{AtLocation} (A is typically located at B), \texttt{Causes} (A is the typical cause for B), and \texttt{CapableOf} (A can typically do B).}
% \texttt{AtLocation} means that B is typically the location for A.
% \texttt{Causes} means that A is typically the cause for B.
% \texttt{CapableOf} means that something that A can typically do is B.
Moreover, over 98\% of words in CSQA questions are covered in ConceptNet. Therefore, ConceptNet is a suitable resource for contextualizing model behaviors on CSQA and other commonsense QA datasets~\cite{feng2020scalable}.

% Here, we adopt \cpn{}~\cite{conceptnet} as the external resource. It is a large-scale commonsense knowledge graph, where concepts (\ie, \textbf{words}) are connected with \textbf{relations} (\eg, relation \hl{be capable of} presents the directed link from \hl{people} to \hl{driving}). 
% It has over 8 million nodes (1.5 English nodes) and over 21 million links in the graph, integrating diverse knowledge sources.
% Particularly, it uses 36 general relations to connect words, mostly covering taxonomic (e.g., \hl{IsA}), lexical knowledge (e.g., \hl{RelatedTo}, \hl{Synonym}), and physical commonsense knowledge (e.g., \hl{MadeOf}, \hl{PartOf}).
% It has good generality and is broadly used to inject commonsense capabilities into NLP models~\cite{lin2019kagnet, feng2020scalable} and build reasoning benchmarks~\cite{CSQA1, CSQA2, lin2021riddlesense}.
% For example, all questions and answers in CSQA dataset are authored based on word-relation triplets drawn from \cpn{}. And over 98\% of words in CSQA questions are covered in \cpn{}.
% Thus, it is a suitable resource for contextualizing model behaviors on CSQA and many other commonsense QA datasets~\cite{feng2020scalable}. 

% \textbf{Commonsense knowledge base.} We resort to an external knowledge base to capture the commonsense knowledge in the QA data, which provides the context for inferring the model's implicit commonsense reasoning.
% To ensure such context is meaningful and helpful for the model analysis, the knowledge base should have \textit{sufficient coverage of the relevant commonsense} reflected in the corresponding QA data.

% Here, we adopt \cpn{}~\cite{conceptnet} as the external resource. It is a large-scale commonsense knowledge graph, where concepts (\ie, \textbf{words}) are connected with \textbf{relations} (\eg, relation \hl{be capable of} presents the directed link from \hl{people} to \hl{driving}). 
% It has over 8 million nodes (1.5 English nodes) and over 21 million links in the graph, integrating diverse knowledge sources.
% Particularly, it uses 36 general relations to connect words, mostly covering taxonomic (e.g., \hl{IsA}), lexical knowledge (e.g., \hl{RelatedTo}, \hl{Synonym}), and physical commonsense knowledge (e.g., \hl{MadeOf}, \hl{PartOf}).
% It has good generality and is broadly used to inject commonsense capabilities into NLP models~\cite{lin2019kagnet, feng2020scalable} and build reasoning benchmarks~\cite{CSQA1, CSQA2, lin2021riddlesense}.
% For example, all questions and answers in CSQA dataset are authored based on word-relation triplets drawn from \cpn{}. And over 98\% of words in CSQA questions are covered in \cpn{}.
% Thus, it is a suitable resource for contextualizing model behaviors on CSQA and many other commonsense QA datasets~\cite{feng2020scalable}. 
}


\subsection{Extract Relevant Commonsense Knowledge}\label{subsec.extract_commonsense}
% introduce motivation (why we do it and what commonsense knowledge base we use)
To help users build a concrete understanding of commonsense questions and their connections with model behavior, we distill relevant commonsense knowledge in data instances based on \cpn{} (\imp{R1}).
% In our work, 
% we introduce ConceptNet~\cite{conceptnet} as an external resource to find underlying relations between the relevant concepts of the questions. ConceptNet is one of the most comprehensive commonsense knowledge graph, covering millions of concepts and relations (\eg, \hl{antonym}, \hl{at location}, \hl{related to}).
% Besides, many existing commonsense reasoning benchmarks (including the CSQA dataset used in our paper)~\cite{CSQA1, CSQA2, lin2021riddlesense} are built upon or extended from ConceptNet.

% introduce extraction methods
The commonsense knowledge extraction consists of two major steps \renfei{(\autoref{fig:system_framework}B)}, including recognizing mentioned concepts in the questions and constructing sub-graphs on the concepts.
% First, each question instance of CSQA contains a question concept (QC), a question stem (QS), and a target concept (TC, also the ground truth). 
% Readers can refer to \autoref{subsec.datasets} for more details on the CSQA dataset.
\rev{To reflect the reasoning paths from the question concept to the target concept/answer, we perform tokenization of the question stem by n-gram (\rev{$n=1,2,3$\footnote{\rev{To balance the coverage of meaningful phrases with varying lengths and computational complexity, we limit maximum gram size to be three~\cite{manning1999foundations}.}}}) and match the tokens (\ie, words of length $n$) with the concepts in ConceptNet to identify a set of candidate concepts for commonsense reasoning. 
Since the matched concepts (with different lengths) may have overlaps, we reduce the redundancy by keeping the longest matched concepts in \cpn{}.}
Moreover, to enhance the robustness of matching, we conduct soft matching by lemmatization and removal of stop words and punctuations.
For example, after token matching, the candidate concepts in a question \hl{A man wants air conditioning, ...} will be \{man, want, air conditioning, ...\}.
Next, those tokens are used to construct a knowledge graph that contains the question concept and the target concept
% QC and TC 
to describe the reasoning process.
By leveraging the connections among the candidate concepts, question concept, and target concept in \cpn{}, we establish relational paths, employing a two-hop relation search. 
% Specifically, we employ two-hop relation search to find the relation connections among the candidate concepts, question concept, and target concept in \cpn{}.
% QC, and TC in ConceptNet. 
% If concept A is within a two-hop relation search of concept B using \cpn{}, then A and B will be connected and their relational paths will be stored.
We set the hop size to two to balance the computation scalability and coverage of reasoning paths, following the prior work~\cite{lin2019kagnet,yasunaga2021qagnn}.
% because the number of the possible paths between two concepts is \textit{exponential} to the hop size. And we keep the relevant reasoning paths by compromising between computation scalability and coverage of reasoning paths, following the prior work~\cite{lin2019kagnet,yasunaga2021qagnn}.
% \renfei{For example, since the relation connects ``ship'' with ``metal'' and ``metal'' with ``corrosion'', ``ship'' and ``corrosion'' will be connected in the extracted graph. }
% \xingbo{TODO: consider adding an example here if necessary}
% And two concepts are connected if they are within N-hop search. 
% Here, to balance the scalability and relevance of the knowledge extraction, we set the hop to two by following the prior work~\cite{lin2019kagnet}. 
Thereafter, the resulting graph of concepts and relations (in~\renfei{\autoref{fig:system_framework}B}) describes the relevant commonsense knowledge for the question. This graph is referred as ConceptNet knowledge.


\subsection{Align Model Behavior with ConceptNet Knowledge}\label{subsec.align_model}
\rev{To help users build mental models about the model's relational reasoning over concepts,
% the model's commonsense knowledge and implicit reasoning, 
we align the model input-output behavior with ConceptNet knowledge regarding different concepts and relations (\imp{R3}).}
% The alignment provides contexts for users to decide whether the model captures proper words for reasoning and learns the implicit commonsense relations among them.
% concept alignment
For concept alignment \renfei{(\autoref{fig:system_framework}C)}, SHAP is used to calculate the importance scores of the input concepts to the model outputs.
And we call those with large positive influences on the model predictions as model concepts.
\rev{Then, we compute the differences between the set of model concepts and the set of \cpn{} concepts (\ie, question concepts and concepts in question stems derived in \autoref{subsec.extract_commonsense}).}
% relation alignment
% \rev{Since commonsense relations are not explicit in the input, we introduce}
\rev{For relation alignment, 
% For relation alignment , we contextualize model behavior with commonsense relations in \cpn{}. Particularly, 
we mainly consider the key relations (\ie, the relations between question concepts and target concepts) for correctly answering the questions (\autoref{fig:system_framework}C). Noticing that \textit{question concepts} are included in question stems as \textit{model inputs} and \textit{target concepts are ground truths for model outputs}, we surface the model learning of 
% the \textit{QC-TC relations} 
their relations
by investigating the relationships of model inputs and outputs.}
% To surface the model learning of the underlying QC-TC relations,
% relations between QCs and TCs, 
% we investigate input-output relationships 
\rev{Specifically, the inputs and outputs are high-dimensional embeddings that the model operates on. And we compute the linear transformation matrix $W \in \mathbb{R}^{d \times d'}$ between model input embeddings $X \in \mathbb{R}^{N \times d}$ and output embeddings $Y \in \mathbb{R}^{N \times d'}$.
Particularly, to reflect relations between question concepts and target concepts encoded in $W$, we use those correctly-predicted instances (\ie, model predictions $P$ are equal to target concepts) as the anchor points for the transformation. And we adopt a least-square error objective to compute the linear matrix $W$: $\mathop{\mathrm{argmin}}_{W \in \mathbb{R}^{d \times d'}}~||XW - Y||_2$, where $(X, Y) = \{(x_i, y_i)~|~TC_i = P_i\},~i=1,...,N $.
% given the ground truth answers $TC$ and model predictions $P$, we consider the correctly-predicted instances as the anchor points for alignment and a least-squares error objective:
}

\begin{comment}
\begin{equation}
\mathop{\mathrm{argmin}}_{W \in \mathbb{R}^{d \times d'}}~||XW - Y||_2
\end{equation}
\begin{equation}
(X, Y) = \{(x_i, y_i)~|~TC_i = P_i\},~i=1,...,N 
\end{equation}
\end{comment}


% Where $X = \{x_i | x_i \in \}$, $Y = \{\}$, and $W$ is the linear mapping between $X$ and $Y$.
% Where $X$ and $Y$ correspond to the pairs of input-output model embeddings in high-dimensional vector spaces $\{ (x_i, y_i) | \}$.

\rev{The general idea is that the input-output relationships can be modeled by translations in the model embedding space~\cite{bordes2013translating, dinu2014improving}: if a model can capture the relations between question concepts and target concepts,  then question concept embeddings transformed with the matrix $W$ should be close to target concept embeddings.}
% each other after applying the transformation matrix $W$ to QC embeddings.


\subsection{Model Editing}\label{subsec.model_editing}

\vis{After identifying model deficits in specific commonsense knowledge, we present \textit{editor networks} to modify model parameters that can correct problematic model answers (\imp{``reliability''}), as well as other semantically-equivalent questions (\imp{``generality''}) without affecting unrelated knowledge much (\imp{``locality''}).
Particularly, editor networks are neural networks trained to modify model parameters (from $\theta$ to $\theta'$) with the objectives that maximize editing accuracy on both editing targets ($x_e, y_e$) and their equivalence ($x_e', y_e'$) while minimizing differences (KL divergence) in model predictions on locality examples ($x_{loc}, y_{loc}$) before and after the edits: $L_e = -log p_{\theta}'(y_{e}'|x_{e}'), L_{loc} = \texttt{KL}(p_{\theta}(\cdot | x_{loc})||p_{\theta}'(\cdot | x_{loc}))$.
% \begin{equation}
% L_e = -log p_{\theta}'(y_{e}'|x_{e}'), L_{loc} = \texttt{KL}(p_{\theta}(\cdot | x_{loc})||p_{\theta}'(\cdot | x_{loc}))
% \end{equation}
The total loss is $L_{total} = - w_{e} \cdot L_{e} + L_{loc}$, where $w_e$ is a weight factor. The editing examples come from QA pairs in CSQA train/val set, 
where their equivalences are generated by popular 
data augmentation techniques, i.e., back-translation and EDA~\cite{DBLP:conf/emnlp/WeiZ19}.
% backtranslation and \todo{data augmentation} techniques, 
Locality examples are independently sampled. We adopt gradient decomposition techniques~\cite{mitchell2022fast} to train editor networks on the last two transformer layers of the model. More technical details are included in 
\referappendix{Suppl. A.}
% \referappendix{Suppl. \ref{sec.model_editing}}.
}
% 


\begin{comment}
The model behavior is described by input feature importance and output accuracy. 
The feature importance is measured by SHAP values of associated pieces. We consider positive SHAP values and connect the pieces into tokens (concepts). 
Then we align model behavior with ConceptNet knowledge. 
To align concepts, we compare the differences between concepts extracted from SHAP values and from ConceptNet. The concepts that are covered by both indicate the concepts that are considered by the model. The concepts that are only considered by SHAP values are those stop words or concepts that are rarely seen. The ConceptNet-only concepts indicate the contexts that the model ignored and may lead to wrong answers. 

For relation alignment, we mainly consider the relations from QCs to TCs which are the commonsense knowledge explicitly tested. 
To surface the patterns of the relation learning, we learn a translation matrix between QS embeddings and TC embeddings from the model. After the translation, the sufficiently learned questions within a relation should form a cluster. By inspecting the instances away from the clusters, we are able to identify relations not learned instances.  

In summary, we use the data and computation metrics mentioned in this section for visualizations. 
\end{comment}

% Compute Feature Importance and model performance. 
% - We consider data with positive SHAP values.

% Align model behavior with ConceptNet Knowledge (concept and relations):
% - Concept-driven alignment (compare SHAP with relevant concepts from ConceptNet)
% - Surface patterns of relation learning. (compute translation matrix between question stem embeddings and target concept/answer embeddings)

% Summarize our data and computation metrics for visualization
% - data: csqa instances, model (and embeddings)
% - computation metrics. (accuracy, SHAP coverage on QC/TC/QS)

