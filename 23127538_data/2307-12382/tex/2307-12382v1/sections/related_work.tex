\section{Related Work}\label{sec.related_work}
% We introduce the related work in the following paragraphs, which includes commonsense reasoning, explainable AI methods for natural language processing, and visualization for understanding natural language processing models.

We discuss related work in commonsense reasoning, explainable AI methods, and visualization for NLP models.

\subsection{Commonsense Reasoning}\label{subsec.cs_reasoning}

% Commonsense reasoning has become an important NLP research topic. 
Here, we introduce the most relevant work, including large knowledge graphs, benchmark datasets, and commonsense reasoning methods.
% The related work of commonsense reasoning can be categorized into large knowledge graphs, benchmark datasets, and commonsense reasoning methods. 

% Maybe better to introduce resources first
% \subsubsection{Knowledge Resources (Knowledge Bases and Graphs, etc)}
% \subsubsection{Large Knowledge Graphs}\label{subsec.cskg}

\textbf{Large-scale knowledge graphs} act as the representation of commonsense knowledge for NLP models to access and exploit. 
The commonsense knowledge graphs (CSKGs) can be divided into two categories, which are human-annotated CSKGs (\eg, ConceptNet \cite{conceptnet}, ATOMIC \cite{atomic2019, atomic2020}, and GLUCOSE \cite{glucose}) and web content extracted CSKGs \cite{webchild, transomcs}. 
% Human annotated CSKGs. 
% Representative human-annotated CSKGs include
\cpn~\cite{conceptnet} is a comprehensive large-scale knowledge graph with over 3.4M entity-relation tuples to connect concepts (words and phrases) by 36 types of relations. It primarily focuses on taxonomic, lexical, and physical knowledge. 
It is collected by crowdsourcing and merged with high-quality knowledge databases.
Here, we use \cpn{} to reveal commonsense knowledge in data instances and contextualize model behavior.
% from DBPedia \cite{dbpedia}, WordNet \cite{wordnet_book, wordnet_api, wordnet_interface}, Wiktionary, and OpenCyc \cite{opencyc_cyc}. 
% \xingbo{Besides, the WebChild \cite{webchild} and TransOMCS \cite{transomcs} are two example CSKGs constructed from web content.}
% The ATOMIC (2019)~\cite{atomic2019} is another knowledge graph collected through crowdsourcing. It consists of 9 relations of inferential knowledge, mostly about social commonsense. Later in 2020, ATOMIC 2020~\cite{atomic2020} is developed with an increased amount of everyday inferential knowledge tuples about entities and events. It consists of 23 relations types, including social, physical, and eventive commonsense relations. 
% GLUCOSE~\cite{glucose} is another human-annotated dataset featuring implicit commonsense causal knowledge. It employs cognitive psychology to identify causal explanations, including story-specific causal statements paired with inference rules generalized from the statement. 
% Web based CSKGs
% Besides, Constructing CSKGs from web content is also a popular approach. The WebChild \cite{webchild} is a large commonsense knowledge base constructed automatically from web contents. It consists of triples that connect nouns with adjectives via fine-grained relations. 
% The TransOMCS \cite{transomcs} knowledge graph consists of tuples converted from syntactic parses of sentences from various web sources, including Wikipedia, Yelp, and Reddit.

% In our work, we extract relevant concepts and relations in ConceptNet to reveal commonsense knowledge in data instances and contextualize language model behavior.

% leverage ConceptNet to contextualize model behavior by extracting relevant concepts and relations from ConceptNet. 
 



% \subsubsection{Benchmark Datasets}\label{subsec.datasets}
% \paragraph{Datasets for testing commonsense reasoning ability} 

% Commonsense reasoning ability has been acknowledged as an important component of natural language processing. 
% However, how to evaluate this ability possessed by a machine remains difficult. 
\textbf{Benchmark datasets} for evaluating NLP models' commonsense reasoning abilities typically involve question-answering tasks, reading comprehension~\cite{huang2019cosmos, zellers2019hellaswag},  open-ended question answering~\cite{boratko2020protoqa, lin2020differentiable}, and multiple-choice questions~\cite{socialiqa, bisk2020piqa, sakaguchi2021winogrande, CSQA2,   singh2021com2sense}.
One example is ComonsenseQA (CSQA) dataset, which consists of 12k commonsense questions authored by crowd workers in a 5-way multiple-choice format. Among the five answer choices, three are directly extracted from ConceptNet, with one being the correct answer. Then the crowd workers create two additional distractors, one from ConceptNet and another authored by themselves. 
% CSQA tests models mostly about factual and physical commonsense knowledge, with the largest part of relation as \texttt{atlocation}. 
CSQA evaluates models mainly on factual and physical commonsense relations (\eg, \texttt{atlocation}) between entities. And we use CSQA to showcase how our system enables scalable and systematic analysis of NLP models' commonsense reasoning abilities.

% Researchers have developed many benchmark datasets, typically formulated as question answering tasks, to evaluate commonsense reasoning abilities of NLP models, for example, reading comprehension~\cite{huang2019cosmos, zellers2019hellaswag},  open-ended question answering~\cite{boratko2020protoqa, lin2020differentiable}, and multiple-choice questions~\cite{socialiqa, bisk2020piqa, sakaguchi2021winogrande, CSQA2,   singh2021com2sense}.
%, etc \cite{commonsenserun}. 
% In this paper, we focus on benchmarks using multiple-choice questions, a major form of testing commonsense knowledge in NLP models. 
% Among them, we introduce the datasets that are closely related to our work. 
%%%%%%%%%%%%%% OLD
% In this work, we use CommonsenseQA (CSQA)~\cite{CSQA1} as an example to demonstrate how our system supports systematic and scalable analysis of commonsense reasoning abilities. 

% Specifically, we contextualize the analysis of model behavior using commonsense knowledge from ConceptNet and visualize what commonsense knowledge the model does (not) learn. 
%%%%%%%%%%%%%% OLD
% CSQA \cite{CSQA1} is a 5-way multiple-choice question answering dataset containing 12k commonsense questions authored by crowd-workers. Among the five choices, three are directly extracted from ConceptNet with one of them being the correct answer. Then the crowd workers choose two additional distractors, one from ConceptNet and another authored by themselves. CSQA tests models mostly about factual and physical commonsense knowledge, with the largest part of relation as \texttt{atlocation}. 

% 我们已经说了focus QA 了 csqa 2 是不是可以不提了。
% CommonsenseQA 2.0 (CSQA 2.0) \cite{CSQA2} is a dataset consisting of 14k yes/no questions collected by crowdsourcing with gamification to make it more challenging. 
% CommonsenseQA 2.0 (CSQA 2.0) \cite{CSQA2} is a relatively more difficult dataset consisting of 14k yes/no questions. It is collected by crowdsourcing using gamification: the goal of players is to compose questions that can beat the rival AI. Each question has a topic prompt and a relation prompt to give the game designer better control over topics and reasoning skills to test. 
% The underlying commonsense knowledge in CSQA 2.0 is more implicit than CSQA, and even state-of-the-art models struggle to reach a reasonable performance. 
% Besides, it is not in multiple-choice form and our domain experts suggested it does not fit into our analytical scenario very well. 

% Social IQA \cite{socialiqa} is a large-scale dataset targeting social commonsense. 
% The social commonsense knowledge of Social IQA is drawn from ATOMIC to seed the contexts and question types. The WinoGrad Schema Challenge (WSC)~\cite{wscchallenge} tests the physical and social commonsense reasoning, and the Choice Of Plausible Alternatives (COPA)~\cite{copa} dataset tests the knowledge of causes and effects surrounding social situations. 
% There are also many other datasets that test models of different commonsense abilities, such as social \cite{socialiqa, wscchallenge, copa}, physical \cite{bisk2020piqa, wscchallenge}, temporal \cite{zhou2019temporal}, and numerical \cite{lin2020numersense} commonsense.

% 这里我想说还有很多其他数据集由于和我们工作相关性不大就不介绍了，不知道如何措辞。或者其他 datasets 就不提了。。。
% There are also many other benchmarks \cite{commonsenserun} whose targets are not closely related to our work. 
% 志华建议介绍我们工作用了哪个benchmark，用这个benchmark 来做什么，为什么不用别的 benchmark。 



% Reference: https://commonsense.run/datasets/


% \subsubsection{Commonsense Reasoning Methods}\label{subsec.cs_reasoning_methods}
% NLP models for CSR
% % Reference: https://commonsense.run/methods/#neural-methods 

% 1) Large-scale language models, both zero-shot/fe-shot (GPT3) and fully finetuned general/multitask models (UNICORN, UNIFIEDQA).
\vis{\textbf{Commonsense reasoning models} include
\revv{large language models (LLMs)~\cite{t5model,gpt3,unicorn, 2020unifiedqa, khashabi2022unifiedqa} pretrained on large text corpora.
% with transformer-based architectures~\cite{wolf2019transformers} 
They achieve impressive performance on commonsense benchmarks. Nevertheless, these models exhibit limitations in their capacity to possess and effectively utilize commonsense knowledge for reasoning tasks~\cite{bian2023chatgpt,ma2021knowledge}.}}
% , have been widely adopted to solve commonsense reasoning problems \cite{commonsenserun}. With careful pretraining strategies, these models~\cite{unicorn, 2020unifiedqa, khashabi2022unifiedqa} have shown strong performance across different benchmarks.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD
% Large-scale language models have been widely adopted to solve commonsense reasoning problems \cite{commonsenserun}. The success of Transformers \cite{wolf2019transformers} inspired many state-of-the-art NLP models. Recent progresses for zero-shot/few-shot models include T5 \cite{t5model} and GPT-3 \cite{gpt3}. Fully finetuned multitask models also make huge progress, such as UNICORN \cite{unicorn} and UnifiedQA \cite{2020unifiedqa, khashabi2022unifiedqa}. 
% \renfei{UnifiedQA is a state-of-the-art pre-trained QA model with a unified text-to-text style. It can generalize well across different QA tasks.} 
%  2) Incorporating external knowledge resources, e.g., KagNet, QAGNN, Xu et al (human parity).
\vis{To enhance models' commonsense knowledge, some methods~\cite{lin2019kagnet,yasunaga2021qagnn, humanparity}
% models like KagNet \cite{lin2019kagnet}, QAGNN \cite{yasunaga2021qagnn}, KEAR \cite{humanparity}, and PU-GEN \cite{seo2022pu} 
integrate external knowledge bases and/or linguistic theories into the models to provide more contexts and facts for improving model accuracy.}
\vis{In addition, \revv{pretrained} language models can be used as knowledge bases to generate clarification questions~\cite{shwartz2020unsupervised}, commonsense explanations~\cite{rajani2019explain}, and prompts~\cite{liu2021generated} to enhance commonsense reasoning.}
\vis{However, they do not explain what commonsense knowledge is injected (un)successfully. 
\revv{And we present a model-agnostic explanation system to systematically evaluate commonsense knowledge that these NLP models possess and utilize for reasoning tasks.}
% In this paper, we propose a model-agnostic visual analytics system to analyze and enhance the model's commonsense reasoning abilities in a systematic, scalable, and transparent way.
}

% Methods like KagNet \cite{lin2019kagnet} and QAGNN \cite{yasunaga2021qagnn} use external knowledge graphs for commonsense reasoning tasks.  
% Recent work further addresses the challenge of effectively integrating relevant knowledge and performing joint reasoning with knowledge graphs. 
% QAGNN \cite{yasunaga2021qagnn} identifies important graph nodes by relevance scoring and connects context with knowledge graph into a joint graph to perform joint reasoning.  
% Besides, Xu et al. \cite{humanparity} proposed to consider more external knowledge resources, including knowledge graphs and dictionaries, to further improve model performance. Assisted by a huge amount of external knowledge, they are able to reach human-level reasoning performance with CSQA. 
% Liu et al. proposed Generated Knowledge Prompting (GKP;  \cite{gkp}) to decompose the reasoning process into knowledge generation and integration, which improves the model accuracy on multiple datasets such as CSQA 2.0. 
% While these methods have significantly improved the commonsense reasoning ability of models, a major reason for the improvement is the use of external knowledge resources without improving the models' intrinsic commonsense reasoning ability. 
% Besides, these methods do not provide explainability and transparency of what commonsense knowledge the models do (not) learn. 
% % 这里要不要加上我们的 methodology 的简介？
% With systematical visualizations, our system helps experts to understand the deficits of current models and pinpoint directions to improve model performance. 

% In this paper, we take UnifiedQA as an example and propose a model-agnostic visual analytics system to support systematic and scalable analysis of the model's commonsense reasoning abilities. 



\subsection{Explainable AI Techniques for NLP} 
\label{subsec.xai}
Explainable AI (XAI) is critical to promote model transparency and reliability~\cite{xai_survey}. We focus on post-hoc model explanations via a model-agnostic approach. 
One popular \revv{post-hoc} explainability technique is feature attribution~\cite{ribeiro2016should, lundberg2017unified, sundararajan2017axiomatic}, which quantifies the contribution of input features to the model output. 
We use a model-agnostic method, SHAP~\cite{lundberg2017unified}.
% , a model-agnostic method, to identify concepts (\ie, words) that are important to model predictions.
% the models deem important to answers.
Another related direction is counterfactual analysis~\cite{kaushik2019learning, wu2021polyjuice}, which uses examples to reverse the target label, helping understand model decision boundaries.
% which uses examples that can flip the target label to help experts understand the decision boundary of models.
Our system enables question manipulation to probe model behavior regarding specific concepts or relations.

While previous work~\cite{li-etal-2022-systematic, zhou2020evaluating} has explained Natural Language Processing (NLP) models via zero-shot or few-shot accuracy evaluations on pre-trained language models, our research conducts a detailed, systematic analysis of model behavior on diverse commonsense concepts and relations. 
Compared to other methods~\cite{petroni2019language, tenney2019bert, reif2019visualizing, cui2020commonsense} that design auxiliary classification tasks to understand linguistic knowledge in NLP models, we aim to reveal the role of commonsense knowledge in the model's reasoning process. 
\revv{Directly prompting large language models helps probe the simple facts embedded in them. However, many NLP models~\cite{2020unifiedqa, unicorn, lin2019kagnet} cannot be easily prompted due to their inherent designs.
Finally, some studies~\cite{meng2022locating, meng2022mass, dai-etal-2022-knowledge} conduct causal analysis that attributes a piece of knowledge to specific neurons in the models. However, these methods do not efficiently summarize how a model learns different commonsense knowledge.} Our system adopts the model-agnostic feature attribution method to quantify model behavior and contextualizes it with a knowledge graph. It then employs multi-level visualizations to facilitate systematic exploration of model behavior across various commonsense concepts and relations.
% By using SHAP to quantify model behavior and contextualizing it with an external knowledge graph, our system supports a systematic and scalable understanding of models' commonsense reasoning abilities.
\begin{comment}
In addition, there are methods specifically designed to explain NLP models. Some work~\cite{li-etal-2022-systematic, zhou2020evaluating} evaluates and compares the zero-shot or few-shot accuracy of pre-trained language models on various benchmark datasets to reflect different commonsense reasoning abilities. 
However, we conduct a fine-grained and systematic analysis of model behavior on different commonsense concepts and relations.
Other methods~\cite{petroni2019language, tenney2019bert, reif2019visualizing, cui2020commonsense} adopt auxiliary classification tasks to explain linguistic knowledge in NLP models~\cite{xai_nlp_survey}. 
However, these low-level linguistic features (\eg, part-of-speech) are not that helpful to reveal the role of commonsense knowledge in the model's reasoning process.
Some methods direct prompt large language models~\cite{gkp, wei2022chain, zelikman2022star, kojima2022large} to probe simple facts embedded within them. However, some models~\cite{2020unifiedqa,khashabi2022unifiedqa} cannot be easily prompted due to their inherent designs.
% explain what linguistic information or facts are captured by NLP models~\cite{xai_nlp_survey}. 
% They focus on linguistic and factual probing by constructing auxiliary classification tasks~\cite{petroni2019language, tenney2019bert, reif2019visualizing, cui2020commonsense} and prompting language models~\cite{gkp, wei2022chain, zelikman2022star, kojima2022large}. 

However, these methods target low-level linguistic features (\eg, part-of-speech) or simple facts and cannot help infer the role of commonsense knowledge in the reasoning process. 
\end{comment}

\begin{comment}

Explainable AI (XAI) is acknowledged as a crucial aspect of deploying AI models to improve model transparency and reliability \cite{xai_survey}. 
% General Intro
Our paper falls into post-hoc model explanations, and we use a model-agnostic approach. 
% In this section, we introduce general model-agnostic methods for post-hoc model explanations. 
% Within the category of model-agnostic post-hoc explanations, simplification methods using rule-based learners or decision trees and feature relevance explanations such as saliency methods are widely adopted \cite{xai_survey}. 
Feature attributions~\cite{ribeiro2016should, lundberg2017unified, sundararajan2017axiomatic} are popular post-hoc explainability techniques, which quantify the contribution of input features to the model output.
% relevance explanations, such as saliency methods, are a popular kind of post-hoc model explanations.
% \subsubsection{General XAI methods}
% For example, Local Interpretable Model-Agnostic Explanations (LIME) \cite{ribeiro2016should} is a popular simplification approach to demystify opaque models. 
% It provides model explanations on predictions by learning a locally linear model around the prediction. 
SHAP \cite{lundberg2017unified}, one representative model-agnostic feature attribution method, computes additive feature importance scores for predictions with helpful properties, such as local accuracy, missingness, and consistency. 
In our system, we use SHAP to identify the concepts (\ie, words) in questions that model attach importance to when deciding the answers.
% compute importance scores of words in questions and
% for different words in each question to identify the important parts the model uses for its reasoning process. 
% We also collectively summarize these important tokens in groups and clusters to support subset-level inspection of what words are treated as important for the CSR process. 
% counterfactual explanation (our model editting)
Another direction that is closely related to our work is counterfactual analysis~\cite{kaushik2019learning, wu2021polyjuice}. Through crafting the counterfactual examples which can flip the target label, experts can understand the decision boundary of models.
Our system also enables users to manipulate question stems and choices to probe model behavior regarding specific concepts or relations.
% supports instance editing of question stems and choices to probe model behavior around the selected instance. 


\vis{Besides, some work~\cite{li-etal-2022-systematic, zhou2020evaluating} evaluates and compares zero-shot or few-shot accuracy of PLMs on various benchmark datasets to reflect different commonsense reasoning abilities. However, we conduct a more fine-grained and systematic analysis of model behavior on data instances that involve a large number of commonsense concepts and relations.}
\end{comment}

\begin{comment}
% Besides the generally applicable methods, 
There are also methods that are specifically designed to explain what linguistic information or facts are captured by NLP models \cite{xai_nlp_survey}.
% NLP tasks to improve explainability and transparency. 
% They are interested in what linguistic information is captured by the models, which phenomena are successfully captured, and where these NLP models may fail \cite{xai_nlp_survey}. 
% \subsubsection{XAI methods for NLP}
% linguistic probing, low level auxiliary task. Prompting. 
Widely applied methods include linguistic and factual probing by constructing auxiliary classification tasks~\cite{petroni2019language, tenney2019bert, reif2019visualizing, cui2020commonsense} and prompting language models~\cite{gkp, wei2022chain, zelikman2022star, kojima2022large}. 
% For example, Cui et al. \cite{cui2020commonsense} investigated the structural commonsense cues in BERT and how these cues relate to the model prediction to understand the models' commonsense reasoning ability. 
% Should I include more detail here? But I don't know the details.
However, these methods focus on either low-level linguistic features (\eg, part-of-speech) or verifying simple facts. 
They cannot help infer the role commonsense knowledge plays in the process of commonsense reasoning.

% While these methods can be applied to commonsense reasoning tasks, the underlying commonsense knowledge is not reflected or exploited.  
In this paper, to explore the commonsense reasoning capabilities of NLP models, we utilize the model-agnostic feature attribution method (\ie, SHAP) to quantify model behavior. And we contextualize it with an external knowledge graph (\ie, ConceptNet) to shed light on what commonsense knowledge the models may learn and use.
Besides, we adopt multi-level visualization designs to facilitate systematic and scalable model analysis.
% We utilize large-scale knowledge bases (\ie, ConceptNet) to provide contextual reference to model behavior and shed light on what commonsense knowledge the models may learn and how they use it for reasoning.
% We facilitate large-scale knowledge bases such as ConceptNet to provide contexts of what commonsense knowledge the language model may learn. 
% Besides, our system features a three-level design to help experts analyze at different levels to gain a high level and scalable understanding of the models' commonsense reasoning abilities.  
\end{comment}

\subsection{Visualization for Understanding NLP Models}
\label{subsec.vis4ai}
% There is a growing need for better understanding of NLP models due to their wide use in real-world applications. 
Visualizations can effectively help understand NLP models~\cite{adadi2018xaisurvey}.
% how NLP models work~\cite{adadi2018xaisurvey}.
% , and there are two main types: model-specific and model-agnostic~\cite{adadi2018xaisurvey}. 
Model-specific visualizations~\cite{strobelt2017lstmvis,ming2017understanding,vig2019bertviz,hoover2019exbert,jin2022gnnlens} reveal the model's inner workings, such as the behavior of neurons, layers, and attention maps. 
By examining these visualizations, users can gain insights into hidden state dynamics~\cite{strobelt2017lstmvis}, the relationships between hidden states and words~\cite{ming2017understanding}, and diagnose model bias~\cite{vig2019bertviz}.
% For example, LSTMVis \cite{strobelt2017lstmvis} helps understand hidden state dynamics in recurrent neural networks, while RNNVis \cite{ming2017understanding} visualizes the relationship between hidden states and words. Recently, transformers have become popular, and methods have been developed to visualize their layers and attention maps \cite{vig2019bertviz, hoover2019exbert, wang2021visqa, vskrlj2020attviz, bracsoveanu2020visualizing, how_bert_answer_questions}, which help diagnose model bias.

\begin{comment}
Because NLP models have been widely adopted to solve real-world problems, there is a huge demand for a better understanding of NLP models. 
% Add taxonomy, why model agnostic and model specific. 
Overall, methods for using visualizations to understand NLP models include model-specific visualizations and model-agnostic visualizations \cite{adadi2018xaisurvey}. 
% 要不要提两者的优缺点？
Model-specific methods focus on revealing inner working mechanisms, while model agnostic methods have better generalizability. 


% 工作的顺序是否需要调整，有逻辑链条。
Model-specific visualizations help users understand the inner working mechanism of models, such as neurons and layers.
% layers and attention heads. 
% Visualizing recurrent neural networks attracted great research interest before transformers~\cite{vaswani2017attention} became popular. 
For example, LSTMVis \cite{strobelt2017lstmvis} was developed to understand the hidden state dynamics in recurrent neural networks. RNNVis \cite{ming2017understanding} applies the co-clustering method to visualize the relationship between hidden states and words. 
After the emergence of transformers~\cite{vaswani2017attention}, visualizing the layers and attention maps of transformers has been a popular line of work \cite{vig2019bertviz, hoover2019exbert, wang2021visqa, vskrlj2020attviz, bracsoveanu2020visualizing, how_bert_answer_questions} that promote understanding and helps diagnose the model bias and shortcuts exploitation. 
% such as BertViz \cite{vig2019bertviz}, ExBERT \cite{hoover2019exbert}, VisQA \cite{wang2021visqa} and AttViz \cite{vskrlj2020attviz}, etc \cite{bracsoveanu2020visualizing, how_bert_answer_questions}. 
% BertViz \cite{vig2019bertviz} is an interactive tool to visualize attention in transformers. 
%  combine different views of the corpus, embeddings, and attention maps to provide a better understanding of Transformers. 
% AKen et al. \cite{} presented layer-wise visualizations of hidden states to better understand transformer-based models' reasoning processes. 
% VisQA is a visual analytics tool that helps users understand the attention maps in transformers and diagnose the models about bias and shortcuts exploitation.  
\end{comment}

\revv{Many model-agnostic visualizations}~\cite{whatiftool, sharedinterest, mediators, amershi2015modeltracker, wang2021m2lens, liang2022multiviz} focus on input-output model behavior and are generally applicable to different models.
% and visualize inputs and outputs to help users understand model behavior. 
For example, What-If Tool \cite{whatiftool} allows users to understand model behavior concerning feature importance, different inputs, and other hypothetical situations.
M\textsuperscript{2}lens~\cite{wang2021m2lens} characterizes intra- and inter-modal interactions learned by multimodal models.
Shared Interest \cite{sharedinterest} compares the reasoning of models and humans using saliency results and ground truths. 
% It helps users identify recurring patterns to better understand model behavior. 
% Mediators \cite{mediators} helps users understand the behavior of NLP models through conversational agents.
\revv{DeepNLPVis \cite{li2022unified} and MultiViz~\cite{liang2022multiviz} present multi-level visualizations to explore both the behavior and working mechanisms of different NLP models across different tasks.} 
% by multi-level visualizations of word-related information across different model layers.}
% GAMUT help users understand the models and predictions better with design probes. 
% Prospector and GAMUT not decided to add here, not so relevant to NLP. 

% 可能要加上我们的工作属于 model specific 还是 agnostic。
% While these visualizations have improved understanding of NLP models in various aspects, they do not provide an understanding of what commonsense knowledge the models may or may not learn. 
% To improve understanding of the commonsense reasoning process, we propose a model-agnostic approach leveraging large knowledge graphs to help demystify the implicit reasoning of language models over concepts and relations for solving commonsense questions. 

However, these studies do not provide insights into the commonsense knowledge that models may (not) learn. To fill the gap, we propose a model-agnostic approach that leverages multi-level visualization and an external knowledge base to contextualize
% leverages large knowledge graphs to explain 
the implicit reasoning of models over concepts and relations in commonsense questions.

