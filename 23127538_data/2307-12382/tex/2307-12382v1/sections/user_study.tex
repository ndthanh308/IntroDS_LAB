\subsection{User Study}\label{subsec.user_study}
% We conducted a user study to evaluate the helpfulness and usability of \name{} for understanding and diagnosing the commonsense reasoning capabilities of natural language models.

% \rev{After introducing the cases of using \name{} for understanding and diagnosing the model's commonsense reasoning capabilities, we formally describe the user study experiment design and investigate user behavior patterns in utilizing different system components. In addition, we summarize their feedback on our system workflow and designs.}

We describe a user study that investigates how NLP experts utilize different components of \name{} to understand and diagnose models' commonsense reasoning capabilities. We also summarize their feedback on our system workflow and designs.

\subsubsection{Experiment Design}
% The visual analysis and exploration of what commonsense knowledge the language models do (not) learn is rather a new problem. 
% % To the best of our knowledge, we do not find a similar explanation system for comparison.
% Although we found some general explainability techniques available~\cite{sharedinterest,lundberg2017unified, ribeiro2016should}, they do not reveal commonsense knowledge contained in the data instances and help users infer the model's implicit reasoning over concepts and their relations.
% Therefore, we decided to investigate how the experts use our system to reason about the model behavior contextualized by the external knowledge base.

\textbf{Participants}
We recruited 10 postgraduate students and alumni (eight males and two females, age: 20-30, referred as \imp{E4}-\imp{E13}) from the computer science department of a local university through emails and word-of-mouth. 
They had at least two years of experience in developing and evaluating natural language understanding models in academia or industry. 
% They are well-experienced (\xingbo{at least two years}) in developing and evaluating natural language understanding models in academia or industry.
% \xingbo{Exx is ... Exx is ... what you want to add here?}
None of the participants had prior involvement in our system's design or usage. Each participant received a cash compensation of \$13.
% All of them neither participated in our design process nor used our system before the experiment.
% Each participant was compensated with \$13 in cash. 
% for their time and effort.


\textbf{User tasks}
Participants were required to use \name{} to analyze 
UnifiedQA~\cite{khashabi2022unifiedqa} on the CSQA validation set.
% a language model on the commonsense question-answering dataset.
They needed to finish the following tasks: 1) gain an overview of model performance for different concepts and relations;
2) Find a relation of interest and assess the overall model learning of that relation;
3) Find a cluster of instances in the question stem/target concept scatter plots with large/small errors;
4) Summarize the model behavior on the cluster of instances;
5) Explore individual instances and infer if the model learns some commonsense to reason about concepts and their underlying relations.
% \begin{compactitem}
% \item Gain an overview of model performance regarding different QSs, TCs, and QC-TC relations;
% \item Find a relation of interest and assess the overall model learning of that relation;
% \item Find a cluster of instances in the QS/TC scatter plots with large/small errors;
% \item Summarize the model behavior on the cluster of instances;
% \item Explore individual instances and infer if the model learns some commonsense to reason about concepts and their underlying relations.
% \end{compactitem}
% We use the UnifiedQA model~\cite{2020unifiedqa} and the valid set of the CSQA dataset~\cite{CSQA1} for the system evaluation.
% The details of the model and dataset are introduced in \autoref{subsec.cs_reasoning}.


\textbf{Procedures}
The whole study lasted about one hour. It started with a \xingbo{20-minute} tutorial, where we collected participants' demographics, asked for their permission to use their log data generated during the study, and introduced the background and the system usage. 
% Participants could raise their questions to interrupt the process and we would answer them accordingly.
Afterward, participants could freely explore the system and get familiar with it (15 minutes). Then, they were asked to use our system to finish the aforementioned tasks. They were encouraged to speak out their hypotheses and findings about the model following a think-aloud protocol (20 minutes). 
During the task exploration, all their user interaction activities (\eg, lasso, clicking, and hovering), together with the timestamps, were automatically recorded.
% The whole system exploration took about \xingbo{35} minutes, where all their user interaction activities (\eg, lasso, clicking, and hovering) during the task, together with the timestamps were automatically recorded.
Finally, participants needed to finish a questionnaire about system workflow, designs, and usability \revv{on a 5-point Likert scale}. And we collected their post-study feedback on the experience of using \name{}.

\subsubsection{Results and Analysis}

%% Log analysis
% We analyze the user interaction activities through the following processes:
% 1. We exclude the hovering interactions first.
% 1. For each individual users, we first sort their interactions by their timestamp in an increasing order. The data are formulated as event sequence data. 
% 2. The interactions on the same view during one time period will be grouped together. The start timestamp of this group of interactions is the timestamp of the first interaction in this group and the end timestamp is the timestamp of the last interaction in this group. The duration of this group of interactions is calculated as the difference between the end timestamp and the start timestamp. The frequency of this group of interactions is defined as the number of interactions in this group.
% 3. Then the frequency of the usage of view for one user is calculated as the summation of all groups of the interactions on that view. The durations are also similarly calculated.
% 4. For each component in the one view, the frequency of using this component is calculated as the number of interactions on that component. For each interaction of using this component, the duration is calculated as the gap time between this interaction and the previous interaction. If the previous interaction is not on the same view, we will not consider this duration in the following calculation. We take the average of the duration in the interactions as the average durations on using this component. 
% 5. We calculate the average and standard derivation of frequency and durations for each view and component. The average and standard derivation of frequency and duration for each view is displayed at \autoref{tab:interaction-statistics}. The average and standard derivation of frequency and duration for each component is displayed at \autoref{fig:interaction_freq}.

We report the analysis of user log data, the questionnaire, and participants' feedback.
For user logs, we extracted the frequency and duration of individual interactions (\eg, clicking) and aggregated them to derive the total usage frequency and duration of corresponding views.

\textbf{Model behavior contextualization and alignment}
% Participants found it reasonable and helpful to use ConceptNet as a contextual reference to understand the model's commonsense reasoning abilities on the CSQA dataset.
Participants found it reasonable and helpful to use ConceptNet to contextualize the model's commonsense reasoning abilities on the CSQA dataset. 

% as the contextual reference to build a concrete understanding of the model's commonsense reasoning abilities on the CSQA dataset. 

Most participants agreed that \name{} helped them understand the data ($Mean_{Q1}=4.70$, $SD_{Q1}=0.67$), model performance  ($Mean_{Q2}=4.60$, $SD_{Q2}=0.70$) regarding different types of commonsense knowledge, and infer the model's implicit reasoning over concepts and their latent relations ($Mean_{Q3}=4.40$, $SD_{Q3}=0.70$). 
For example, 
\imp{E6} appreciated the intuitive assessment of overall relation learning by comparing transformed embeddings.
% appreciated that it is intuitive to visually assess the overall relation learning by comparing the transformed QS embeddings with the TC embeddings.
\imp{E7} mentioned, 
\hl{... getting overlap between the set of entities mapped in ConceptNet and model entities (concepts) can explain whether LMs are focusing on the right things.}
\imp{E8} added, \hl{Aggregating the concept hit ratio and model performance over the whole dataset and organizing them by relation is a great way to understand the model from a global scale.}
Moreover, participants were fairly confident in their findings ($Mean_{Q3}=4.40$, $SD_{Q3}=0.70$).
% in their findings about the model.
Besides, the extra context information provided by the ConceptNet helped develop hypotheses about model learning and probe the models' behavior on specific concepts and relations (\imp{E6, E9, E10}).

Nevertheless, participants also raised some concerns 
over using \cpn{}.
% some relations are not important components for reasoning 
\imp{E10} said that sometimes the retrieved relations from ConceptNet are not guaranteed to be true reasoning paths for solving the questions.
% \hl{QC-TC relations are sometimes not perfect rationale for solving the questions.}
% \imp{E8} noticed that in some questions, the answer choices share the same relations with the question concepts. In such cases, the context words play an important role in deciding the true answers.
% some concepts are also not important
Similarly, the extracted ConceptNet concepts were considered generic or not informative for some questions (\imp{E8}).



\begin{comment}
\hl{Aggregating the concept hit ratio and model performance over the whole dataset and organizing them by relations is a great way to understand the model’s commonsense reasoning ability for given relation on a more global scale, which can be viewed as a way of aggregating local explanation (SHAP) into global behavior explanation.}

\hl{I think finding related entities from ConceptNet is helpful to find reasonable explanations. Getting overlap between the set of entities mapped in ConceptNet and the set of model entities (concepts) can explain whether Language models are focusing on the right things. We can find a lot word bias (or artifacts), which the language models can cheat with.}

\hl{That seems reasonable for me at least for CSQA dataset, since the dataset construction refers to ConceptNet.}

\hl{I think showing the words that the models focus on the most may not be helpful to analyze whether one knowledge triple is stored in the models or not. For instance, the models usually focus on some words like “what”. It’s actually really important for the model to answer the question, but it does not indicate any knowledge at all. I would be curious about changing a question into natural language and checking the performance.}

\hl{It is reasonable, since ConceptNet has broad coverage and is of high quality. }

\hl{I think conceptnet can be a good source of knowledge. But it should be assumed to be well aligned to the qa data.}

\hl{Explaining the commonsense reasoning behavior with a specific concept and SHAP explanation to locate a few impact words is very interesting. I think this can even be broaden to explaining eventuality conceptualization linking, as proposed in https://arxiv.org/pdf/2206.01532.pdf. In existing commonsense reasoning we see that the model is applying commonsense knowledge because of some keywords. This means that existing "reasoning" styles do remember and apply commonsense knowledge.}
\end{comment}

\begin{table}[t]
\centering
\caption{The frequencies and durations of user interactions.}
\resizebox{0.7\linewidth}{!}{
\begin{tabular}{ccc} 
\hline
View          & Frequency                          & Duration (s)                             \\ 
\hline
Global View   & 59.63 $\pm$ 32.32 & 468.48 $\pm$ 289.01  \\
Subset View   & 22.50 $\pm$ 17.12 & 66.82 $\pm$ 60.15    \\
Instance View & 33.38 $\pm$ 23.65 & 212.15 $\pm$ 204.15  \\
\hline
\end{tabular}
}
\label{tab:interaction-statistics}
\vspace{-5mm}
\end{table}

% \begin{table}[h]
% \caption{The frequencies and durations of interactions in the three views.}
% \centering
% \begin{tabular}{ccc} 
% \hline
% View          & Frequency                          & Duration (s)                             \\ 
% \hline
% Global View   & 59.63 $\pm$ 32.32 & 468.48 $\pm$ 289.01  \\
% Subset View   & 22.50 $\pm$ 17.12 & 66.82 $\pm$ 60.15    \\
% Instance View & 33.38 $\pm$ 23.65 & 212.15 $\pm$ 204.15  \\
% \hline
% \end{tabular}
% \label{tab:interaction-statistics}
% \end{table}


% Figure environment removed


\textbf{System usage analysis}
Participants thought that \name{} supports a more systematic and scalable analysis of model behavior, compared to conventional analysis of ad-hoc instances.
They appreciated the multi-level model explanations, especially the global understanding of model behavior.
% global view
As shown in \autoref{tab:interaction-statistics}, the \gv{} which provides an overview of model behavior clearly dominates the user interactions.
It takes up 51.62\% and 62.68\% of total system interaction frequency and duration respectively, exceeding those of the \sv{} and \iv{} by a large margin.
% is the most frequently used view. And participants interacted with it for the longest time.
Moreover, as shown in \autoref{fig:interaction_freq},
% As indicated by \autoref{fig:interaction_freq}A, 
participants showed great interest in examining model performance for different relations and projection modes. 
They spent much time selecting and exploring instances in the scatter plots (indicated by lasso and zoom interactions).
% subset view
The \sv{} has the least user interactions regarding frequency and duration.
Participants usually used it to quickly preview the details of different cluster glyphs (indicated by ``cluster\_glyph'' interactions).
% instance view
The \iv{} was considered to represent the traditional analysis of model behavior.
Participants generally used it to check different instances (indicated by ``pagination'' interaction) and probe the model (indicated by ``run\_model'' interaction). Also, they spent the longest time searching words and phrases in the \iv{} (indicated by ``search'' interaction).

% global level summary ==> model performance from different aspects (relation types, correctness)
% helping them quickly narrow down to instances of interest.
% subset-level === 
% local explanations ===

% \xingbo{\textbf{TODO}}

\textbf{Patterns and insights}
Participants also reported many interesting findings about the model behavior and dataset issues.
For example, the model was found to rely on spurious correlations to solve many questions.
For the question \hl{Minerals can be obtained in what way for a person who avoids leafy greens? (answer: multivitamin)}, the model attached importance to ``minerals obtained'' and selects ``ore'', ignoring the important contexts ``person'' and ``leafy green''. 
Also in many instances, the model just focuses on some irrelevant words 
% (\eg, modifiers) 
like ``what'' and ``at'' (\imp{E9, E11}).
\imp{E10} suggested that we can convert the questions into statements and check the model behavior change.
\hl{We can eliminate those word biases by using counterfactuals.} (\imp{E11}). 

\begin{comment}
\renfei{Here the expert misunderstood, the ground truth is fairgrounds. Model result is ``zoos''. }
\renfei{\hl{As many explanation works have stated, the LMs very often rely on spurious correlations to solve problems. For instance, for the question "Minerals can be obtained in what way for a person who avoids leafy greens?" (correct answer: "multivitamin"), the model focuses on "minerals obtained" and selects "ore", ignoring the important contexts "person" and "leafy green". 
However, substituting "Minerals" with "Micro minerals" and deleting "who avoids leafy greens" can make the model output the correct answer "multivitamin".}}
I have another example, if the mineral example doesn't make sense. Where would you hear a trumpet along with other instruments made from the same material?
\hl{As many explanation works have stated, the LMs very often rely on spurious correlations to solve problems. For instance, for question "Where can children play with animals?", the model focuses on "children can play" and selects "fairgrounds". However, substituting "children" with "people" can make the model to output correct answer "zoos" (although it cannot be checked the model focuses on animals this time using the system).}

\hl{I found that, in many instances, the model just focuses on some general words, like "what," "at," "how," and so on. We can eliminate those word biases by using counterfactual examples. \url{https://aclanthology.org/2021.acl-long.422/}}

\hl{I am not familiar with the explanation methods, but the highlights in the instance view often fall on irrelevant words like modifiers. Though the model made right predictions, coud we say the model made the commonsense prediction based on wrong reasons?}

\hl{The model still fails on rare occasions, (making the wrong predictions), which is totally normal as tricky situations such as challenges in WSC is too hard. I do suggest that if the authors are digging into commonsense reasoning insights, we can test some WSC examples to see why the model is predicting the wrong concept to two entities, like which word causes this to happen. At the same time, we can also merge Probase as the concept corpus. My personal experience is that Probase is more like a concept corpus. (if we are focusing on concept)}

\end{comment}

Besides, participants noticed that some CSQA questions are poorly designed. 
These questions have multiple plausible answers or typos that entirely invalidate the whole question instances.
For example, \imp{E11} mentioned, for the question ``Where is seaweed from?'', the model outputs ``sea'', while the true answer is defined as ``ocean''.
However, both ``sea'' and ``ocean'' seem correct.
And \hl{`what can a person with a what can do?' should’ve been `...with a watch...'} (\imp{E7}).


\textbf{Visual designs and interactions}
Participants generally agreed that our system is easy to use, but it required some effort to learn. 
They found the \iv{} to be the most intuitive, followed by the \gv{}. 
% For the \iv{}, 
participants found the \iv{} the most helpful in diagnosing if the model uses proper information and learns relations between question and target concepts.
% QC-TC relations for reasoning. 
They thought that SHAP explanations and model probing complement each other to deepen the model understanding.
The \gv{} was thought useful in summarizing the learning of relations and concepts, though it was sometimes a bit hard to visually align dot clusters between the projection plots due to the embedding rotation effect and scarcity of instances. 
The \sv{} was considered the most difficult to understand.
But it was considered helpful to compare the model concepts and ConceptNet concepts across different subsets.
% and summarize model behavior on different QC/QS/TC. 
Their ratings and detailed feedback are in 
% \referappendix{Suppl.~\ref{app.user_rating_feedback}}. 
\referappendix{Suppl. E.2}.
% \todo{\textbf{TODO}: add back more feedback}
% The \gv{} was thought helpful for finding relations/concepts with large/small prediction errors, and the question and answer scatter plots helped them infer if the relations are generally learned well. However, some participants reported that it was sometimes hard to visually align and match clusters of dots in the scatter plots due to the embedding rotation effect and scarcity of instances. 
% For the \sv{}, participants thought it was a bit complex but considered it helpful to compare the model concepts and ConceptNet concepts and summarize model behavior on different QC/QS/TC. 

\begin{comment}
% overall usability and visualization, interaction designs
As shown in \autoref{fig:system-user-scores},  participants generally agreed that our system is easy to use ($Mean_{Q15} = 4.20$, $SD_{Q15} = 1.03$) while it required some efforts for learning ($Mean_{Q14} = 3.80$, $SD_{Q14} = 0.79$).
They were willing to use ($Mean_{Q16} = 4.50$, $SD_{Q16} = 0.85$) and recommend our system ($Mean_{Q17} = 4.70$, $SD_{Q17} = 0.67$) for understanding and diagnosing commonsense reasoning capabilities of language models.
The most intuitive view of \name{} is the \iv{}, then the \gv{}. And the \sv{} was thought to be the most difficult to understand. 
We summarize participants' feedback (as shown in \autoref{fig:three-view-user-scores}) on our visual designs as follows.

For the \gv{}, participants found it quite useful for finding relations/concepts with large/small prediction errors ($Mean_{Q5} = 4.30$, $SD_{Q5} = 0.95$). \hl{I can quickly observe the correctness distribution among instances} (\imp{E6, E8, E12}) and \hl{narrow down to specific cluster of instances} (\imp{E7, E9}). 
And the question and answer scatter plots helped them infer if the relations are generally learned well ($Mean_{Q6} = 4.40$, $SD_{Q6} = 1.26$). 
Furthermore, \imp{E6} and \imp{E8} added that the correctness coloring of the dots (\ie, model accuracy) is really helpful when they analyze relation learning and were not sure about the quality of the alignment between questions and answers. 
However, some participants reported that sometimes it is a bit hard to visually align and match clusters of dots in the question and answer scatter plots due to the embedding rotation effect (\imp{E10}) and scarcity of instances (\imp{E11}).

For the \sv{}, participants thought it was a bit complex ($Mean_{10} = 3.20$, $SD_{Q10} = 0.63$). And after they got familiar with it, they considered it helpful to compare the model concepts and ConceptNet concepts ($Mean_{Q8} = 4.20$, $SD_{Q8} = 0.63$) and summarize model behavior on different QC/QS/TC ($Mean_{Q9} = 4.50$, $SD_{Q9} = 0.53$).
For example, \imp{E9} commented \hl{The hit ratio and top missed concepts are very useful in understanding what types of concepts the model focuses on.}
Some participants felt \sv{} could be a bit too informative sometimes. 
\imp{E10} suggested, \hl{Choosing the number of clusters and viewing the concepts and linkers are a bit messy and too informative, can we simplify the design and only the most impactful one?}

The \iv{} was the most favored by participants for its intuitiveness and helpfulness in diagnosing if the model uses proper information ($Mean_{Q11} = 4.80$, $SD_{Q11} = 0.42$) and learns QC-TC relations for reasoning ($Mean_{Q12} = 4.50$, $SD_{Q12} = 0.97$). 
Participants generally thought that SHAP explanations and model probing complement each other to deepen the model understanding.
\hl{I tested many examples and found that the explanation results were very satisfactory. And the instance probing also helped me to do some further investigation and testing on model behavior.} (\imp{E7})
\hl{The model probing is a great tool to change the input to the model and check the behavioral change of the model. This can be used to do some causal analysis of concepts.} (\imp{E8})
\end{comment}

\textbf{Suggestions for improvement}
% Participants also provided some valuable suggestions.
\imp{E10} desired an automatic zoom-in function when lassoing dots in the \gv{}.
\imp{E11} suggested showing the neighboring dots when hovering over a dot. 
\imp{E8} proposed to summarize concepts that are not covered by both ConceptNet and the model.
\imp{E7} recommended integrating other knowledge bases (\eg, ATOMIC~\cite{atomic2019,atomic2020}) to broaden the commonsense coverage.
% , which can empower a much richer set of commonsense inferences.

% \hl{It would be great to see SHAP output after probing the model also.}
% \hl{Also, I think you also can provide what entities from ConceptNet are missing to help users better understand what is happening.}
% \hl{show the neighboring dots when hovering on dots in global view}
% \hl{However, as ConceptNet only covers a certain type of commonsense relation, incorporating other commonsense knowledge bases (such as ATOMIC, GLUCOSE, ASER, ...) will provide a much richer set of commonsense inference. Besides, the fact that we examine natural-language sentence linked with triples (qc, rel, answer) instead of only triples in simple form let us test the generalizability of models.}


% \hl{interesting to see projections of the embedding in 2D space. However, kind of hard to understand the correlation between question and answer space. Great tool to focus on correct / wrong instances.}

% \hl{In the global view, I can get a general matching of point clouds. It is a good way to narrow down problems in current Language models.}

% \hl{ I really like the Global view as it is quite user-friendly and i can easily access to cluster of instances and see the correctness distribution.}

% \hl{The global view shows a clear connection between instances entities by larger nodes. }

% \sv{}:
% \hl{Probably the hardest to understand; mainly because it is difficult to know how each cluster is generated, and what data points are included in each cluster. Still, the hit ratio and missed concept bar are very useful in understanding the model’s ability to focus on concepts.}
% \begin{compactitem}
%     \item \hl{On the other hand, it can be a little misleading, because higher hit ratio does not mean that the model is doing better; not all conceptNet concepts are useful for solving the question. Moreover, some conceptNet concepts are very generic, like "thing".}
% \end{compactitem}

% \hl{But the subset view is less related to the instances.}

% \hl{However, I think the cluster glyphs need to be somehow improved, I doubt that choosing the number of clusters and viewing the concepts and linkers are a bit messy and too informative, I guess maybe we can simplify and only demonstrate the most impactful clusters?}


% \iv{}:
% \hl{the most intuitive one to use. All sections are easy to understand. Also, when browsing through instances here, the global view highlighting the corresponding point is also very useful. }
% \begin{itemize}
%     \item \hl{SHAP explanations and model probing can help users to have better idea of why the model was WRONG, especially. However, Model Concept Frequency doesn’t help much, as they are just all stop words. For this to have some meaning, I think the ground truth Question Concept Frequency graph should also be included.}
% \end{itemize}

% \hl{It must take me more time to get familiar to the system. Currently, from my point of view, the most useful component is Instance View. It helps us to know if the model focus on the right concept or not.}

% \hl{I think the instance view is the most useful, we can see what words the language models focus on now. Also, I think you also can provide what entities from ConceptNet are missing to help users better understand what is happening. }

% \hl{For me, I prefer instance view and model probing part. I think it’s important to provide the interactive tool with models so that the users can try possible solutions directly.}

% \hl{The Instance view is intuitive to me and super helpful for diagnosing model reaction to single instance.}

% \hl{I think the instance level information is the easiest one for me to understand.}

% \hl{I think the most interesting part is the instance explanations and model probing. I tested a lot of examples and found that the explanation result was very satisfactory. And the instance explanation also helped me to do some further investigation and testing on model probing. }


% \hl{The extracted conceptNet concepts are not perfect rationale for solving the question; some of them are very generic, or do not actually contribute to solving the question. Therefore, hit ratio alone is not an ideal measure of model’s commonsense reasoning ability.
% Still, if hit ratio and model performance shows strong correlation, it could be argued to be a valid metric.
% TC hit ratio should be more or less directly connected to the performance. QC hit ratio may have weaker correlation. It would be great to see some statistics for this?}

% \hl{1. It would be great to see SHAP output after probing the model also.
% The "model concept frequency" in the Instance View is not very informative, because the top words are mostly stop words like "what, where, the, a, you, in, of, ..."
% 2. Better visualization of the clusters. It seems that currently, when I click on a cluster in the Subset View, the Global View shows a large circle, which I assume is the centroid of the cluster. It would be great to see which points are included in this cluster too - maybe by lowering opacity of other points that are not included in this cluster.}

% \hl{However, as ConceptNet only covers a certain type of commonsense relation, incorporating other commonsense knowledge bases (such as ATOMIC, GLUCOSE, ASER, ...) will provide a much richer set of commonsense inference. Besides, the fact that we examine natural-language sentence linked with triples (qc, rel, answer) instead of only triples in simple form let us test the generalizability of models.}

% \hl{According to the idea in slides p11, Translated QS embedding should be close to TC embedding, which hint that they should be embedded into the same space. However, in the current implementation, Transformed QS and TC are in different spaces. That somehow makes it hard to understand the meaning of the global view and to study if a relation is well learned.}

% \hl{However, in the \gv{}, the definition of "Original," Relation," and "Correctness" are not given. After hearing your explanation of these three modes, I think other NLP researchers may also be not familiar with different ways to map data into a two-dimensional space. So, maybe more explanation.} - \hl{I think more explanation about the page, like "Original," Relation," "Correctness," and how you get the clusters in the subset view.}


% \hl{In the Instance view, when I put my cursor over an option and try to edit it, there will be a box of related concepts of the option, which blocks my cursor. I think you can change the location of this box of related concepts.}

% \hl{Besides the one I mentioned during zoom meeting, I feel this tool is not really intuitive when showing the analysis about relations.}


% \hl{The subset view is probably not in its best condition right now, the bars and concepts are a little bit confusing at the beginning.}

% \hl{For the global graph, it is not clear how the transformed head entities are aligned to the tail entities. i am also expecting that, whether there are some animation for top k closest embeddings when one hover on one embedding on both sides.}

% \hl{I think it should be more careful when two models are used. The first one is the lm, the other one is the "transformation" between two set of embeddings.}

% \hl{I have two suggestions in total. First, we can add the relation's name into the green bar located between two projection maps if possible, and maybe we can consider to set a larger length for rare relations, because those are worth focusing but they are hard to click on. On the other hand, I wonder if it's possible to rotate the projection map to a similar distribution so that it's more visually predictable whether they are related or not?}

% \hl{I think the clusters are hard to understand, and from my understanding, only a few clusters helped my judgement, (maybe I was wrong about this), so I would suggest that we can rank the clusters according to the concept + instances and demo topK. At the same time, automatic rotation of the projection map is worth trying. And we can have some tricky case studies on WSC if necessary.}

\begin{comment}
All participants agreed that our system was easy to use and intuitive and improved their efficiency of evaluating the models' commonsense reasoning abilities. 
They described our system design as clean, intuitive, and user-friendly. The experts highly praised the rich and intuitive interactions. 
Participants also appreciated our efforts to make their top-down or bottom-up analytical approaches more smooth. 
E4 added that the smoothness makes their hypothesis-driven analysis much more efficient. 
More specifically, E2 commented that the interactions and designs make the transitions between the global level and local level analysis seamless. 
E3 said that it felt interesting to play with our system, especially editing instances to probe the model behavior. The model often surprised E3 with unprecedented results demonstrating the subtleness and complexity of the model's behavior (the commonsense reasoning process).


\textbf{Design philosophy and lessons learned}

We value simple and effective static designs that are friendly to our users. The philosophy behind our design of interactions is intuitiveness. Almost all of our designed components support hovering and clicking, flattening the learning curve of using our system efficiently which is agreed among the experts. 
Besides, we consider only universally understood shapes such as rectangles 
\end{comment} 