\section{Discussion}
\label{sec.discussion}
% In this section, we discuss the lessons learned, model and data diagnosis through explanations, commonsense knowledge bases, generalizability and scalability, and technical limitations.

% \subsection{Lessons Learned}
% \textbf{Concretize model behavior}
% The relevant commonsense knowledge mined from ConceptNet helps experts concretize model behavior efficiently. 
% We allow users to evaluate model behavior on different scales. We also support instance-level editing to directly probe the model behavior. 
% All our efforts are greatly appreciated by our interviewed experts. 
% They agreed that concretizing model behavior on related commonsense knowledge improves the efficiency of understanding and probing model behavior. 

% \textbf{Priortize simple charts and intuitive interactions}
% We value simple and effective visual designs that are friendly to our users. The philosophy behind our design of interactions is intuitiveness. Almost all of our designed components support hovering and clicking, flattening the learning curve of using our system efficiently which is agreed among the experts. 

\subsection{Human-AI Alignment with Contextualization}
% We design \name{}, a visual analytics system that builds the alignment between the commonsense knowledge possessed by humans and NLP models.
Considering that commonsense knowledge is implicit and not explicitly stated (in model input), many interpretability techniques~\cite{sharedinterest, lundberg2017unified}, which rely on existing model input and output, cannot explain models for commonsense reasoning tasks.
We introduce an external knowledge graph, \cpn{}, to characterize the commonsense knowledge in the model input with a group of concepts connected by different relations. 
This external knowledge is set as contextual information to align model behavior with human commonsense knowledge and reasoning.
Given the large space of commonsense knowledge, achieving human-AI alignment on commonsense reasoning tasks with additional contexts is challenging.
Our multi-level visualizations enable the exploration of model behavior on different concepts and their underlying relations in a scalable and systematic way.
% In the future, explainable AI systems can consider what contexts are understandable and valuable to humans for building human-AI alignment. Meanwhile, they should reduce users' mental burden of comprehending the additional context information.
 \vis{Moreover, visualizations produce actionable insights into what specific knowledge the model underperforms and guides the model probing and editing.
With pre-trained language models becoming so large and powerful (\eg, ChatGPT), it poses significant challenges to understand, diagnose, and adjust model behavior after deployment. \name{} presents \textit{``exploration-explanation-editing''} posthoc analysis pipeline to contextualize and align model behavior with users' expectations.
}

\begin{comment}
Moreover, the complex concepts and relations 
ompared with many existing black-box explainability techniques~\cite{sharedinterest, lundberg2017unified} rely on existing model input and output
However, they are not efficient in explaining models' commonsense knowledge 

Commonsense knowledge is not explicitly stated in natural language and is assumed to be known by most humans. 
It is challenging to build an alignment between the commonsense knowledge of humans and NLP models. 
Many black-box explainability techniques cannot explain models for commonsense reasoning tasks since they rely on existing model input and output, but commonsense knowledge is not explicitly stated in the input. 
Building an alignment between the commonsense knowledge of humans and NLP models faces several challenges.
First, such knowledge is assumed to be known by most humans and not explicit is and thus many black-box explainability techniques may fail since they 
apply existing black-box explainability techniques to evaluate and infer what commonsense knowledge is possessed by NLP models  
since commonsense knowledge is considered well-known to humans and not explicitly stated in the input.
to build an alignment between the commonsense knowledge possessed by humans and NLP models.
since many black-box explainability techniques rely on existing input and output data.
such human capabilities humans to evaluate and infer what commonsense knowledge the NLP models learn and use in commonsense reasoning tasks.
however, it is challenging to investigate such human abilities 
unlike many black-box explainability techniques that focus on the existing input and output data for model behavior analysis.
0. human-ai alignment ==> significance
1. commonsense knowledge implicit (background knowledge). compared to prior work
2. use the external knowledge base to contextualize model behavior
2.1. multi-faceted analysis: from concepts, relations
2.2. multi-level?
The system should consider what contexts are understandable and valuable to humans for building human-AI alignment.
\end{comment}

\begin{comment}
\subsection{Diagnose Model and Data Simultaneously Through Explanations}
Our work uses ConceptNet to extract relevant commonsense knowledge contained in data instances, which helps to contextualize the implicit reasoning of language models and helps to explain what commonsense knowledge the models learn. 
In the user study, our participants discovered model deficits and dataset problems based on the explanations provided by \name{}. 
For example, the language model was found to exploit spurious correlations between specific word pairs, while ignoring the important contexts. Also, the models frequently focus on some irrelevant modifiers, such as ``what'', ``a''.
Besides the model issues, 
CSQA has biased relation distribution, where \texttt{atlocation} makes up the largest proportion of the dataset. And the model learning of relations with small proportions (\eg, \texttt{partof}) is not satisfying. 
More importantly, many QA instances of CSQA datasets contain controversial or ambiguous statements, and multiple plausible answers (such as ``sea'' and ``ocean''), which even humans find hard to decide the correct answer. 
And after resolving the data issues (\eg, changing ambiguous answer choices), the model could answer them correctly.
Our participants also raised concerns over the dataset while exploring model behavior.
Therefore, besides offering model explanations, the system should also attach importance to diagnosing and even improving data quality since data can largely impact model learning as well.
\end{comment}


% \textbf{Model diagnosis}
% In our study, we find that the model results are frequently influenced severely by some words that have strong correlations with certain choices. 
% When we perform question editing, as long as those words exist, they will strongly influence the model decision to the wrong choices. 
% Our study may indicate that current large language models can learn strongly correlated patterns of concepts which makes the contexts less considered. 

% \textbf{Dataset diagnosis}
% During our study, we frequently encounter controversial statements, ambiguous descriptions, and choices being synonyms (such as ``sea'' and ``ocean'') or even the same, which even humans find hard to decide which is the correct answer. After editing the question stems or choices to make the question less ambiguous or less controversial, the models are able to answer correctly. The experts thought that it is important to reduce the ambiguity and controversy within commonsense benchmarks to test models with better quality. 

% Another aspect is the semantic space distribution of the benchmark dataset. We find CSQA to be well designed to have a relatively uniform distribution of concepts over the semantic space, where the performance of the model is not biased in certain local areas. But its distribution of relations is not satisfying, where \texttt{atlocation} makes up the largest proportion of the dataset. And the performance of smaller proportion relations is not satisfying and not learned very well. Recent knowledge bases and benchmarks have put more attention to the distribution of relations and the diversity of the relations to include more commonsense knowledge. We agree on this point that datasets need to be better balanced to help the model to be less biased regarding relation and semantic space. 

\subsection{Commonsense Knowledge Bases for Contextualization}
% Commonsense reasoning is implicit and rarely explicitly stated since it is assumed that this background knowledge is shared by everyone.
% \rev{Many black-box explainability techniques rely on existing model input and output. Therefore, they are not sufficient to explain }
To reflect implicit commonsense knowledge in models, we use an external knowledge graph (\cpn{}) as the contextual reference of commonsense knowledge and align the model behavior with the \cpn{} knowledge. 
% \rev{Specifically, to ensure meaningful contextualization, 
% we first select \cpn{} to
% the selected knowledge base should have sufficient coverage of the commonsense knowledge in the target dataset. Moreover, 
% In this paper, we use \cpn{} to contextualize model behavior on the CSQA dataset since it is directly built on the knowledge in \cpn{}.
Given that ConceptNet is large and comprehensive with good generality and CSQA is built upon \cpn{}, it is reasonable and sufficient to use ConceptNet to cover the commonsense knowledge in CSQA.
% \cpn{} is a comprehensive knowledge graph that covers taxonomic, lexical, and physical commonsense knowledge, making it suitable for grounding commonsense knowledge in the CSQA dataset. 
\revv{Our quantitative evaluation with CSQA examples and qualitative users' feedback also show that most commonsense knowledge in CSQA questions can be grounded in ConceptNet, justifying its use in our study.}
% participants in the evaluation confirmed that such model behavior contextualization is reasonable and helpful. 
% In this paper, we utilize ConceptNet for understanding the model's commonsense reasoning on the CSQA dataset. Since ConceptNet is large and comprehensive and the CSQA dataset is built upon ConceptNet, it is reasonable and sufficient to use ConceptNet to cover the commonsense knowledge in CSQA.
However, \cpn{} mainly contains taxonomic, lexical, and physical commonsense. 
\revv{It has limitations in covering other commonsense knowledge, such as temporal and inferential commonsense, thus impacting the effectiveness of model behavior contextualization and visualization for understanding models' true reasoning capabilities in these knowledge areas.}
% It has limited capabilities to cover commonsense knowledge in datasets like Social IQA, which targets inferential knowledge about the causes and effects of social interactions.
To support model analysis for more commonsense reasoning benchmarks, we can integrate diverse commonsense knowledge bases, such as ATOMIC~\cite{atomic2019,atomic2020}, GLUCOSE~\cite{glucose}, or large pretrained language models~\cite{shwartz2020unsupervised}, to contextualize model behavior.
% to increase the coverage of commonsense.
\rev{It is worth noting that to ensure meaningful model contextualization,  we should choose a knowledge base that has sufficient and relevant coverage of the commonsense knowledge in the target benchmark dataset.
}
Besides knowledge graphs, other types of commonsense knowledge representations (\eg, arithmetic and logical operations) can be used to improve the expressiveness of model behavior contextualization.

% \textbf{Commonsense knowledge representation}
% Our work supports analyzing commonsense knowledge which is generally expressed as knowledge graphs. However, there are also other types of commonsense knowledge that cannot be represented as graphs (\eg, arithmetic and logical operations), since knowledge graphs are typically finite graphs that consist of a finite number of entities and relations. 
% For instance, since numbers are continuous and infinite, it is not possible to use each entity to represent each number and each relationship represents the arithmetic relationship between the numbers (\eg, comparison or addition) in knowledge graphs   . 
% It is also not possible to represent all exclusive relations, one kind of logical operations, for the entity. For example, if we would like to represent the knowledge on what ways people cannot obtain minerals, the answers can contain infinite ways like ore, oil, etc. Those relations cannot be fully represented in the knowledge graph. 
% Therefore, we need to consider other designs to represent commonsense knowledge such as numerical commonsense.
% the coverage of knowledge graph can be limited and not fully represent commonsense knowledge. 


% \textbf{Coverage of commonsense knowledge bases}
% In this paper, we utilize ConceptNet for understanding the model's commonsense reasoning on the CSQA dataset. Since ConceptNet is large and comprehensive and the CSQA dataset is built upon ConceptNet, it is reasonable and sufficient to use ConceptNet to cover the commonsense knowledge in CSQA.
% To support model analysis on more commonsense reasoning benchmarks, we can integrate diverse commonsense knowledge bases (\eg, ATOMIC~\cite{atomic2019,atomic2020}, GLUCOSE~\cite{glucose}) to enlarge the coverage of commonsense.
% \textbf{Reporting bias} in the knowledge bases/model training data. The training data can have different distributions different from real-world distributions. For example, the grass is green but may be under-reported in web corpora (as it is assumed to be true).


\subsection{Generalizability and Scalability}
% Besides UnifiedQA and CSQA dataset introduced in the paper, 
We showcase \name{} through a state-of-the-art QA language model (\ie, UnifiedQA) and CSQA dataset.
% the UnifiedQA model and CSQA dataset,
% \rev{
Our system can be immediately used to \textbf{explain any other language models} for commonsense question answering (CQA) since our explanations center around the model's input-output behavior with a model-agnostic approach.
% (\ie, SHAP).}
% Moreover, our system can be easily extended to support model analysis for \textbf{other commonsense reasoning tasks}. 
\rev{Moreover, our system can be used for \textbf{other CQA datasets}. 
For example, Social IQA~\cite{socialiqa} is a multiple-choice QA dataset about social interactions in everyday events.
It is built upon ATOMIC~\cite{atomic2019,atomic2020}---a commonsense knowledge graph about the causes and effects of different events.
Therefore, our system can integrate ATOMIC to retrieve relevant causes or events for a given event extracted from questions in Social IQA to contextualize the model's social commonsense reasoning.}
% Our model contextualization method can be 
% extended to support model analysis for \textbf{other commonsense reasoning benchmarks}.
% For example, we can contextualize model behavior on Social IQA (a multiple-choice CQA dataset) by using ATOMIC as the external knowledge base.
% , we can use ATOMIC as external knowledge base. ATOMIC
% since Social IQA (a multiple-choice ) is built upon the knowledge in ATOMIC, we can contextualize model behavior on Social IQA dataset by using ATOMIC as external knowledge base.
% that can be transformed into similar QA formats (\eg, natural language inference). 
% and are grounded on corresponding knowledge bases, such as Social IQA~\cite{socialiqa} with ATOMIC~\cite{atomic2019,atomic2020}. 
\rev{Furthermore, our model contextualization method has the potential to support \textbf{other commonsense reasoning tasks}.
For instance, for visual question-answering (VQA) tasks, we can extract concepts (\eg, person, dog) in the images. Then, by combining the concepts in the images and text questions, we can utilize an external knowledge base to build a relevant knowledge graph that covers these concepts. The resulting knowledge graph can be used to contextualize models' reasoning.}
Our \textbf{multi-level visual designs} facilitate NLP model analysis for tasks like machine translation. The scatter plots of the \gv{} can summarize frequent associations and translation errors between source and target concepts. Aligning embeddings helps assess translation quality. Then, the \iv{} shows correlations between source and target sentences, enabling users to evaluate translation robustness.
% In addition, our multi-level \textbf{visual designs} can be used to analyze NLP models for other tasks, such as machine translation.
% \rev{For example, given source and target language text pairs, we can use the scatter plots in the \gv{} to summarize what pairs of source and target concepts are frequently associated and translated wrongly at a global level.
% In addition, aligning source embeddings with target embeddings in the scatter plots helps diagnose the overall machine translation quality.
% Besides, at the local level, the \iv{} can show the pairwise correlation between a source and a target sentence. Users can manipulate words in input and output to evaluate the robustness of translation.}
% Our system design and the three-level analytics approach can be generalized to other CQA datasets grounded on corresponding knowledge bases, such as Social IQA with ATOMIC. 
% The datasets that are in the form of multiple choice questions can be transformed into the multiple-choice question answering format. 
% We believe that our methodology can inspire the following work to support a deeper understanding of the commonsense reasoning abilities of large language models. 

\revv{Our approach faces \textbf{scalability} issues due to the computation cost of feature attribution methods, which can take several hours to compute SHAP for thousands of instances. 
To mitigate this impact, we have precomputed and integrated SHAP values into the system to enable seamless interactions for posthoc model analysis.
To further speed up the process, we can adopt faster feature attribution methods (\eg, CXplain~\cite{schwab2019cxplain}) and techniques like data sampling, caching, and parallel computing.}
% To reduce computation time, we can use data sampling, caching techniques, and parallel computing.
Regarding visual designs, the links between cluster glyphs in the \sv{} can be cluttered when the cluster number exceeds five,
% can suffer from the visual clutter of links between cluster glyphs when the cluster number exceeds five, 
requiring horizontal scrolling to examine different clusters.

% Our approach also has some \textbf{scalability issues} in algorithms and visualization.
% The overhead of computation cost is determined by the feature attribution methods and commonsense knowledge extraction from data, which could take several hours to derive for thousands of instances.
% We can further use data sampling, caching techniques, and parallel computing to reduce the computation time.
% For the visual designs, visual clutter of links between cluster glyphs exists in the \sv{}. If the cluster number is large than five, users need to scroll horizontally to examine different clusters.
% Although the \gv{} can support thousands of instances, the \sv{}
% The scalability of subset view when cluster number is larger than 5, the user has to scroll to examine different clusters and the links can be cluttered. 
% Computing time of building commonsense knowledge extracting from ConceptNet, clusters, and searching interactions.

\subsection{Limitations and Future Work}
\name{} also has some limitations:
1) To extract relevant commonsense knowledge for answering a question, 
we build a sub-graph containing the
words in the question stem that are within two hops of the question concept and answer using ConceptNet. 
However, some concepts in the sub-graph may not so relevant for solving the question. 
Also, some important concepts could be connected with the question concept through multiple hops.
\rev{2) We perform n-gram tokenization to match the words in a question with the words in \cpn{}. However, this may exclude some longer phrases or sentences (in question stems and answers), which affects the relation extraction between the question and answer.}
3) To reflect the model's overall learning of relations, we apply the translation to the input embedding and align it with the output embedding.
However, it is also possible that after linear transformation, question and target concepts are not close to each other, but the models still capture the relations between question and target concepts through non-linear transformation. 
\revv{4) Model behavior probing may lead to incorrect model understanding~\cite{slack2021counterfactual}. To mitigate this, the system can improve the reliability of probing~\cite{voita-titov-2020-information,poyiadzi2020face} or integrate multiple explanation methods to cross-validate the model insights discovered by model probing. Moreover, larger-scale evaluation across different datasets and longer-term user studies with our experts can further validate the model understanding facilitated by our system.}

\revv{In the future, we can improve the system designs to handle more complex questions with multiple plausible answers and explanations, often influenced by diverse arguments and opinions.}
% where uncertainties exist in questions with multiple possible answers and explanations by being based on arguments and opinions.} 
Moreover, we can improve the usability of \name{} by displaying prediction scores for answer choices in the \iv{} and enhancing our model editing methods for larger-scale editing.

% \revv{5) uncertainty in answers and model accuracies;}
% In future work, we can improve the usability of \name{} by displaying prediction scores for answer choices in the \iv{} and enhancing our model editing methods for larger-scale editing. 



\begin{comment}
Suggested Discussion Points:

1. 
Introduction:
Generally, the commonsense knowledge is represented as graphs, where nodes denote the conceptual entities (e.g., cars, people, traffic jams) and links describe the relations between different concepts (e.g., people “is capable of” driving cars which can “cause” traffic jam).

For this sentence, we may need to discuss:
Many commonsense knowledge cannot be represented as graphs, e.g, arithmetic operation, negation, etc. Discussion part may include the discussion of this limitation.

2. 
Introduction:
In this paper, we consider the problem of commonsense question answering (CQA). And we use ConceptNet [27]—a large commonsense knowledge graph that connects words and phrases with labeled relations—as the external knowledge base. 

For this sentence, we may need to discuss:
Discussion may include generalizability of the system to other kinds of commonsense knowledge and reasoning tasks.

3. Relation alignment.

The general idea is that the input-output relationships can be modeled by translations in the model embedding space [8, 12]: if a model can capture the relations between QCs and TCs, their embeddings should be close to each other after the transformation.

"Input-output relationships can be modeled by translations in the model embedding space" is an assumption and not guranteed as correct. For example, it is also possible that after linear transformation, QCs and TCs are not close to each other but the models still capture the relations between QCs and TCs through other non-linear transformation. 

4. Instance View editing

Checking instance one by one and edit them one by one may be tedious things. We can extend the system to automatically provide some edited sentences for users to have a quick look of results.

Appendix Suggestions.
1. Instance View search box.

Users can search linguistic patterns of interest, for example, instances containing specific words or
structure (e.g., “many NOUN”).

Readers don't know what it can be supported. Some details about the specific controls can be introduced in appendix for further illustration.

\end{comment}
 




