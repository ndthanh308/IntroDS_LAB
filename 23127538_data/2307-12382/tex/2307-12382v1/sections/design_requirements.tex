\section{Design Requirements}
\label{sec.design_requirements}
% background, (goal and target users)
We aim to develop a visual analytics system to help NLP experts understand and diagnose commonsense reasoning capabilities of NLP models in a systematic and scalable manner.
% our tool benefits
Explaining such model abilities helps users determine whether models are suitable and trustworthy for downstream applications and enhance specific knowledge that models do not learn well.
However, it is challenging to depict and summarize the vast and complex space of commonsense knowledge that models learn, as it is not directly presented in the input, and concepts are entangled with various relations and contexts.
% recognize what types of concepts and relations models learn well or poorly and whether they rely on some superficial word correlations for reasoning.
% \vis{After that, our users can determine whether models are suitable and trustworthy for downstream applications and enhance specific knowledge that models do not learn well.}
%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%
% Furthermore, they can enhance models by encoding required commonsense knowledge from knowledge bases as additional inputs~\cite{gkp, wang2020connecting, yao2022nlp, dalvi2022towards} or into the model parameters~\cite{de2021editing, meng2022locating}.
%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%
% challenges - our tool improvement over prior work (consider commonsense knowledge which may not explicitly be specified in the statement)
% However, it is challenging to interpret what commonsense models know, since it is not directly mentioned in statements and does not rely much on linguistic contexts for inference tasks.
% \rev{However, it is challenging to reveal and summarize what commonsense models know since it is not directly mentioned in the input. Moreover, the space of commonsense knowledge is vast and complex, where concepts are entangled with different relations and contexts.}

% how we formulate the design requirements
% To develop a concrete understanding of commonsense reasoning and methods of explaining NLP models, 
We first conducted a literature review on explainability techniques \renfei{\cite{ribeiro2016should, lundberg2017unified, kaushik2019learning, wu2021polyjuice}} and visual analytics~\renfei{\cite{sharedinterest, whatiftool, feng2023xnli,jin2023shortcutlens}} for NLP, and commonsense reasoning~\renfei{\cite{conceptnet, cui2020commonsense}}. 
To further characterize users' common practices and needs, we collaborated with three NLP experts (\imp{E1-E3}, \imp{E1} is the coauthor) through regular weekly meetings for about six months.
\imp{E1} is a Ph.D. candidate who investigates commonsense knowledge acquisition and reasoning. 
\imp{E2} has obtained a Ph.D. degree in HCI and has rich experience in building human-centered interactive NLP models.
And \imp{E3} is a research scientist from an international media company whose expertise is in explainable AI and visualization for NLP. 
% All of them have several publications in their research fields.
%% How we collaborate
During the meetings, we asked them about 
% \rev{1) current practice of evaluating NLP models;}
1) the general methods of NLP model evaluation; 
2) what types of explanations for models' commonsense reasoning capabilities;
and 3) the desired system task support.
% functions for systematic and scalable model analysis for commonsense reasoning.
Meanwhile, we developed our system prototypes iteratively and collected their feedback for further improvement.

% general methods of evaluating models and what do they care about during evaluation

\rev{\textbf{Current practice and limitations.}}
Our users usually start with performance metrics (\eg, accuracy) to locate data instances (\esp, wrong predictions) and manually summarize what commonsense knowledge is needed for inference. 
Specifically, users identify important relations and concepts for commonsense reasoning and combine performance metrics with feature attribution methods to determine whether models capture important concepts or superficial word associations. Moreover, they can probe the models by modifying the data instances to verify their hypotheses.
However, this analysis process is tedious, mentally demanding, and difficult to generalize to larger data subsets. They desire a visual analytics tool to analyze what commonsense knowledge is contained in data instances and (not) learned by NLP
models.
% To support the systematic and scalable exploration of commonsense knowledge, a visual analytics tool is necessary.

% To understand models' commonsense reasoning abilities, our users usually start with performance metrics (\eg, accuracy).
% Then, they sample some data instances, especially those with wrong predictions, for further examination.
% For each instance, they manually summarize what commonsense knowledge is needed for inference. 
% \rev{Specifically, our users often build the mental model about \textit{relations} between mentioned \textit{concepts} in data instances. And they identify important relations and concepts for commonsense reasoning.}
% Afterward, they combine performance metrics with feature attribution methods (\eg, SHAP) to see whether models capture those important concepts or superficial word associations for commonsense reasoning. Furthermore, they generate hypotheses about models' reasoning over relations between concepts.
% And they probe the models by modifying the data instances to verify their hypotheses.

%% Limitations
% However, the analysis method above focuses on individual instances. It is tedious and mentally demanding to go through all of them, identify the important commonsense knowledge in the instances, and reason about model behavior on concepts and relations.
% Besides, it is hard to generalize their findings to larger data subsets, 
% \rev{for example, how a LM performs on the instances containing different groups of concepts connected by the same latent relations.}
% They value our effort in building such a visual analytics tool to support the systematic and scalable exploration of what commonsense knowledge \rev{(\esp, concepts and relations) is contained in data instances and (not) learned by NLP models.}
% % summarize design requirements
% % Finally, we summarize our derived design requirements as follows.
% Our derived design requirements are as follows.

% Expert background 
%  - expert one: PhD candidate whose research interests are commonsense knowledge acquisition and reasoning.
%  - expert two: a research scientist from a media company whose expertise is in explainable AI and visualization for NLP
%  - expert three: a graduated PhD who builds human-centered interactive NLP models

\textbf{R1. Reveal commonsense knowledge in data instances.}
Our users need to distill the external commonsense knowledge from data instances, which helps verify if model behavior aligns well with human knowledge~\cite{sharedinterest,jin2023shortcutlens}. Since concepts and relations are critical components of commonsense knowledge~\cite{conceptnet, cui2020commonsense}, the system should extract relevant concepts and their relations in questions as references to understand data itself and model behavior:

\begin{compactdesc}
\setlength\itemindent{-1em}
  \item[Q1:] What concepts (\eg, entities) are mentioned in the instances?
  \item[Q2:] What are the latent relations between the mentioned concepts?
\end{compactdesc}

% \hspace{\parindent}\textit{Q1. What concepts (\eg, entities) are mentioned in the instances?}
% \hspace{\parindent}\textit{Q2. What are the latent relations between the mentioned concepts?}

\textbf{R2. Summarize model performance on different concepts and relations.}
% Performance metrics are crucial quantitative indicators of model evaluation, which help users gain a concrete understanding of model behavior.
Our users usually depend on accuracy scores to pinpoint cases where models fail and prioritize exploring them.
% summarization importance
To scale up the analysis of individual instances to large datasets, it is necessary to summarize model performance from multiple aspects~\cite{whatiftool, sharedinterest, feng2023xnli}.

% In commonsense reasoning, concepts, relations, and contexts are essential for model understanding and evaluation. 
\imp{E3} said that a concept-driven summary can reveal what topics models perform well. \imp{E1} mentioned that compared to the vast concept space, relations have more summative power and connect concepts meaningfully. \imp{E3} suggested relating model performance to linguistic contexts to assess their ability to use commonsense knowledge in different situations. For instance, testing models on instances where adults and children use staplers helps understand whether models can distinguish between them.
Therefore, the system should answer:
% Therefore, the system should help answer the following questions:

% In commonsense reasoning, concepts, relations, and their contexts are three essential for model understanding and evaluation.
% \imp{E3} said that a concept-driven summary of model performance can help reveal what topics (\eg, sports, music) models perform well.
% And \imp{E1} mentioned that compared to enormous concept space, relations are more compact and have more summative power that connects concepts in a meaningful way.
% Besides, \imp{E3} added that relating the model performance to linguistic contexts 
% provides evidence of whether models can correctly utilize commonsense knowledge in different situations.
% % how models capture important cues to find latent relations between concepts in different situations.
% For example, both adults and children are capable of using staplers. However, adults usually associate with office and children are likely to use staplers in school.
% And testing models on the cases aid the understanding of whether models can distinguish between adults and children.
% To summarize, the system should help users answer the following questions about model performance:

\begin{compactdesc}
\setlength\itemindent{-1em}
  \item[Q3:] What concepts, relations, and their combinations are predicted right or wrong by the models?
  \item[Q4:] What are the contexts of the relations and concepts? What is the model performance?
\end{compactdesc}

% \hspace{\parindent}\textit{Q3. What concepts, relations, and their combinations are predicted right or wrong by the models?}

% \hspace{\parindent}\textit{Q4. What are the contexts of the relations and concepts? What is the model performance?}

\textbf{R3. Infer model relational reasoning over concepts based on relevant commonsense knowledge.}
% Besides model performance, our users usually need to identify important input features (\eg, words, phrases) for model predictions and then judge the rationality of the model reasoning by aligning these features with their prior knowledge~\cite{ribeiro2016should, lundberg2017unified, sharedinterest}.
To develop a mental model about models' commonsense knowledge and reasoning, users need to first use their own prior knowledge to build the relevant reasoning paths that connect important words in statements. Then, they need to check whether models capture these meaningful concepts in statements based on their importance to model predictions. Although sometimes models are correct, they may rely on task-unrelated linguistic features (\eg, stop words) to make decisions. 
Moreover, \imp{E2} and \imp{E3} thought that to better surface the patterns of how models regard unmentioned relations, it is necessary to show whether models attach importance to the mentioned words in statements connected by those relations.
By concept-driven comparison between important concepts recognized by models and humans, users can generate hypotheses about models' relational reasoning over concepts:
% since some relations between concepts are not explicit in statements, it is necessary to reveal whether 
% Moreover, to develop a mental model about models' commonsense, users need to 
% To generate hypothesis about models' commonsense reasoning abilities, our users are interested in 

\begin{compactdesc}
\setlength\itemindent{-1em}
  \item[Q5:] What concepts are important for model predictions? Are they reasonable?
  \item[Q6:] What unexpressed relations are necessary for inference? Do models cover the concepts connected by these relations?
  \item[Q7:] What are the differences between the important concepts for commonsense reasoning and for model predictions?
\end{compactdesc}


% \hspace{\parindent}\textit{Q5. What concepts are important for model predictions? Are they reasonable?}

% \hspace{\parindent}\textit{Q6. What unexpressed relations are necessary for inference? Do models cover the concepts connected by these relations?}

% \hspace{\parindent}\textit{Q7. What are the differences between the important concepts for commonsense reasoning and for model predictions?}


\textbf{R4. Allow interactive probing and editing of NLP models.}
One straightforward and useful way to understand and debug models is interactively interrogating them~\cite{whatiftool, kaushik2019learning, wu2021polyjuice}. To generate and verify the what-if hypothesis about model behavior, users can conduct counterfactual analysis by manipulating specific input components and seeing how models react to these changes.
% Through interactions with models, users can inject their own knowledge and probe models based on their interests.
% Specifically, what-if analysis 
This helps disentangle influences of individual concepts in statements for model predictions and check whether models are biased towards some concepts. Moreover, modifying the input components can test the robustness of models against noisy concepts and probe the underlying relations of interest that link the mentioned concepts in the input.
% - manipulate and modify data instances based on human knowledge and model behavior to probe relations and concepts
% - conduct counterfactual analysis through interactions with models
\vis{After model probing, users may desire to conduct posthoc editing of model behavior to inject their desired knowledge and make a flexible localized update about specific knowledge areas that models do not learn well~\cite{de2021editing,mitchell2022fast}.}

\revvv{Given the requirements, we consolidated a series of system tasks that guides the systematic exploration of models' commonsense reasoning capabilities: Initially, commonsense knowledge from data is extracted as concept-relation triplets (\imp{R1}). 
Then, the system summarizes model performance across these concepts and relations
% Then, the system summarizes model performance on different relations and concepts 
(\imp{R2}), and further assesses the overall relation learning (\imp{R3}). 
Next, it enables users to pinpoint error instances related to specific concepts or relations (\imp{R2}). 
For these instances, the system summarizes the concepts considered important by models in varying contexts, aligning them with those in the extracted triplets (\imp{R3}). 
Moreover, visualization is utilized in conjunction with model probing (\imp{R4}) to comprehensively explain models' input-output behavior on these instances (\imp{R3}).
% Furthermore, visualizations of models' input-output behavior on these instances (\imp{R3}) and probe models (\imp{R4}) for a comprehensive model understanding.
Finally, users can bookmark instances for targeted model refinement (\imp{R4}).
}

