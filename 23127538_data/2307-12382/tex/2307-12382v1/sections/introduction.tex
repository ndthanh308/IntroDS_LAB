\label{sec.intro}
Commonsense knowledge describes the general facts and beliefs about the world that are obvious and intuitive to most humans. It allows people to smoothly explore and reason over everyday events and situations.
For example, \hl{my parents are older than me} and \hl{take an umbrella when it rains}.
% \renfei{\hl{Where do cars run at super high speeds? (answer: race track)}}.
Equipping machines with humanlike commonsense reasoning abilities can benefit the development of social robots and intelligent personal agents to assist humans in daily tasks.

Commonsense knowledge and reasoning have been important and long-standing challenging topics in the natural language processing (NLP) community.
Many researchers have devoted their efforts to building commonsense knowledge bases by extracting information from existing data sources (\eg, Wikipedia) or acquiring it from domain experts or crowd workers.
Generally, the commonsense knowledge is represented as graphs, where nodes denote the conceptual entities (\eg, cars, people) and links describe the relations between different concepts (\eg, people \hl{is capable of} driving cars).
Building upon the knowledge bases, a few benchmark datasets have been designed to evaluate and improve NLP models for automated commonsense reasoning. Particularly, question answering (QA) is the primary and popular form of benchmarks~\cite{CSQA1, CSQA2, zellers2019hellaswag, bisk2020piqa, bhagavatula2019abductive, singh2021com2sense, huang2019cosmos}.
% , natural language inference (NLI)~\cite{bhagavatula2019abductive, singh2021com2sense}, and machine comprehension (MC)~\cite{huang2019cosmos}.

Recent advances in large pre-trained language models (PLMs) of NLP (\eg, BERT \cite{tenney2019bert}, GPT \cite{gpt3}, and T5 \cite{t5model}) have yielded impressive and even human-level performance \cite{humanparity} on commonsense benchmarks.
However, these models lack interpretability and transparency, 
which hinders model debugging and development for real-world applications. 
It is unclear what commonsense knowledge the models have learned and used in the process of reasoning,
% learned, how they use the knowledge for reasoning tasks, 
and whether they merely explore the spurious correlation in the datasets.
This issue has led to a rallying call for explaining NLP models to reflect their real commonsense reasoning capabilities and to build more robust benchmarks and models.

To help NLP experts understand the NLP model's reasoning process, feature attribution methods (\eg, LIME~\cite{ribeiro2016should}, SHAP~\cite{lundberg2017unified}) are popular explainability techniques, which quantify the importance of input features (\eg, words and phrases) to the model outputs. 
Therefore, NLP experts can identify critical concepts for model predictions and determine whether they are aligned with human knowledge.
We have witnessed the success of these methods for various applications (\eg, sentiment analysis~\cite{chen2020generating} and fake news detection~\cite{ayoub2021combat}). 

However, feature attribution methods cannot be directly applied to explain models for commonsense reasoning tasks. 
First, \revv{\textit{they are incapable of 
% facilitating a high-level understanding of 
revealing models' relational reasoning over concepts (\eg, entities) in different contexts}} since their relations may require background knowledge and not be explicitly presented in the input.
For example, in \hl{take an umbrella when it rains}, the inherent commonsense is that the umbrella \hl{is used for} \revv{protection from the rain}, which is not mentioned in the original statement.
\revv{Moreover, contexts significantly influence the reasoning over these implicit relations between concepts.}
For instance, depending on the weather, the umbrella can \hl{be used for} \revv{protection from the rain or sun.}
% Second, \textit{commonsense knowledge is highly context-dependent}, where different knowledge (\ie, combinations of concepts and their relations) needs to be utilized to draw inferences about different situations.
% For instance, depending on the weather, the umbrella can \hl{be used for} \revv{protection from the rain or sun.}
% And the feature attribution methods are insufficient to support contextual analysis of the implicit relations between the concepts and how models use them to make decisions.
% the concepts and relations, and how machines combine them to make decisions.
Furthermore, feature attributions often focus on individual instances. 
\revv{Given the complexity and vastness of the commonsense knowledge space the models operate on, where concepts are intertwined with various relations and contexts, it is challenging to scale up these methods to efficiently 
build high-level abstractions of model behavior (\eg, under what contexts a relation is well learned) and generalize model understanding to large datasets.}
% build high-level abstractions of model behavior and generalize model understanding to large datasets.
% They are not scalable and efficient enough to facilitate a global understanding of the model's commonsense reasoning capabilities due to the vast space of concepts and their relations in commonsense knowledge.
% \revv{Consequently, }
\vis{Furthermore, to better align the model with human knowledge and expectations, it is crucial to not only understand its reasoning but also actively inject and update the desired knowledge within the model, such as using human feedback to finetune ChatGPT~\cite{DBLP:journals/corr/abs-2203-02155}.}
% Can cite ChatGPT/InstructGPT paper
% \textit{It is challenging for NLP experts to conduct a scalable and efficient analysis of a large set of instances} 
% \rev{to derive commonsense knowledge in data instances and generate hypotheses about whether it is learned by models.}
% generalize their hypothesis about what commonsense knowledge the models learn.

\revv{Visual analytics~\cite{sharedinterest, amershi2015modeltracker, whatiftool} have been an effective approach for summarizing complex data characteristics and facilitating data-driven model understanding at scale.}
% To tackle the challenges above, 
Motivated by this, we design and develop a visual analytics system, 
\name{}, which enables \revv{NLP experts (\eg, model developers)} to conduct a systematic and scalable analysis of the commonsense reasoning capabilities of NLP models (outlined in \autoref{fig:workflow_teaser}). 
% Our system goes beyond existing visual explanation tools~\cite{tenney2020language,whatiftool,amershi2015modeltracker,sharedinterest} by integrating an external knowledge base to derive implicit commonsense knowledge from the input for model behavior analysis. This knowledge is used as additional context to align model behavior with human reasoning through interactive visualizations. 
Going beyond many existing visual explanation tools~\cite{tenney2020language,whatiftool,amershi2015modeltracker,sharedinterest} that focus on input-output behaviors of models, our system integrates an external knowledge base to derive implicit commonsense knowledge from the input and uses them as additional contexts to align model behavior with human reasoning through interactive visualizations.
%%%%%%%%%% OLD %%%%%%%%%%
% our system introduces an external knowledge base to derive the commonsense knowledge implied in the input. Then, with the extracted knowledge as the additional context, the system visualizes model behaviors on explicit input concepts (\ie, words) and the implicit relations among them in commonsense reasoning tasks.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Our system goes beyond the existing interpretability tools~\cite{tenney2020language,whatiftool} that focus on the input-output behavior of models. 
% Particularly, building upon the external knowledge base, the system derives the commonsense knowledge implied in input instances and contextualizes and visualizes model behaviors on explicit input concepts and their implicit relations in commonsense reasoning tasks.}
% \textit{Building upon feature attribution methods and external commonsense knowledge bases, the system contextualizes model behaviors on different concepts and relations in commonsense reasoning tasks.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
Here, we focus on commonsense question answering (CQA), a common and popular way to evaluate commonsense reasoning abilities.
And we showcase our system on the CSQA dataset~\cite{CSQA1}, a representative CQA benchmark.
Moreover, we use \cpn{}~\cite{conceptnet}---a large and comprehensive commonsense knowledge graph---to support the contextualized analysis of model reasoning in CQA, as CSQA is built upon ConceptNet. 
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We focus on commonsense question answering (CQA), a popular task for evaluating commonsense reasoning abilities, and showcase our system on the representative CSQA benchmark dataset~\cite{CSQA1}. 
\cpn{}~\cite{conceptnet}, a commonsense knowledge graph, is used to extract commonsense knowledge from data as concept-relation triplets for model contextualizations. 
Our system provides multi-level visualizations of model behavior by comparing important input features for model decisions with the extracted triplets from ConceptNet.
% that connects words and phrases with labeled relations---as the external knowledge base 
% And we use ConceptNet~\cite{conceptnet}---a large commonsense knowledge graph that connects words and phrases with labeled relations---as the external knowledge base. 
\begin{comment}
% Specifically, we extract relevant concepts and relations from the ConceptNet as the references of commonsense knowledge contained in the question instances.
% Then, the system provides multi-level visual explanations of model behavior by comparing important input features for model decisions with the extracted commonsense knowledge in \cpn{}.
\end{comment}
At the global level, the system adopts data transformation and projection strategies to summarize model performance on questions and relations and assesses the overall relation learning.
At the subset level, the system presents a contextual summary of the alignment between model behavior and related \cpn{} knowledge for different subsets.
% regarding question concepts, question stems, and answers.
And at the local level, the system provides visual explanations for individual instances and allows for model probing and editing to identify and enhance specific knowledge areas where models underperform.
% the system highlights the important question concepts (\ie, words) for the model predictions and related commonsense knowledge.
% % the commonsense knowledge related to the instances.
% Moreover, the system allows users to manipulate questions and answer choices and conduct counterfactual analysis of model behavior on individual instances. 
% \vis{Moreover, the system supports interactive model probing and editing, allowing users to manipulate input questions, conduct counterfactual analysis, and identify and enhance specific knowledge areas where models underperform.}
% Through a user study with 10 NLP experts, we demonstrate 
% Through case studies and \renfei{user studies} 
\vis{Through a user study using CommonsenseQA (CSQA)~\cite{CSQA1} dataset, we show that \name{} can help NLP experts effectively understand, diagnose, and edit model knowledge
% the models' commonsense reasoning over 
on concepts and their implicit relations in different contexts.}
% abilities effectively. 
% \xingbo{\textbf{TODO}: summarize the cases and evaluation results}.

The major contributions of this paper are summarized as follows:
\begin{compactitem}
    \item \vis{{\name}, a visual analytics system that supports a systematic and scalable analysis of the modelâ€™s reasoning on commonsense tasks involving a large number of concepts and their relations. Particularly, it helps align model behavior with human reasoning through model contextualization, multi-level visualizations, and interactive model probing and editing.}
    % that facilitates a systematic and scalable analysis of what commonsense knowledge the NLP models do (not) learn. Particularly, it contextualizes model behavior on different concepts and the implicit relations among them with an external commonsense knowledge base.
    \item A user study with cases that shows the effectiveness and usability of our system in revealing, diagnosing, and editing underlying commonsense knowledge the language model does not learn.
\end{compactitem}




