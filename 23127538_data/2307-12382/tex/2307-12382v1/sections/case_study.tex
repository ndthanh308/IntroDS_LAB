\section{Evaluation}
\label{sec.evaluation}

% In this section, we demonstrate how {\name{}} helps experts analyze the commonsense reasoning abilities of language models through three case studies and a user study.
% We invite \xingbo{10} experts (\imp{E4-E13}) to evaluate our system on a commonsense reasoning benchmark.
We conducted a user study to evaluate how {\name{}} helps experts analyze the commonsense reasoning abilities of language models.
% through three case studies and a user study.
Specifically, we invite \xingbo{10} experts (\imp{E4-E13}) to evaluate our system on a commonsense reasoning benchmark.
% experts' background
% \xingbo{Ex is ... Ex is ... (experts' background goes here)}
% And \imp{Ex} is the expert involved in our design process. 
The experts are NLP researchers or practitioners, and all of them have rich experience in natural language understanding topics.
We delay the introduction of their backgrounds in \autoref{subsec.user_study}.
% QA model and benchmark
We evaluate the system by using a state-of-the-art QA model UnifiedQA~\cite{2020unifiedqa} 
% \footnote{\href{https://github.com/allenai/unifiedqa}{UnifiedQA} is used as an example model for evaluation. It adopts a typical pretraining paradigm like other modern NLP models.} 
and the CSQA~\cite{CSQA1} validation set.
% for the system evaluation.
% UnifiedQA is a state-of-the-art pre-trained QA model which adopts a text-to-text paradigm and achieves good performance across different seen or unseen QA datasets.
The CSQA validation set contains 1,221 multiple-choice commonsense QA instances, and the model performance is \xingbo{71.00\%}. \revv{We also randomly sampled 100 instances from the validation set to evaluate the commonsense coverage of \cpn{} for CSQA. For each instance, an NLP expert (\imp{E14}, not a co-author) from a tech company assessed if the relational paths extracted from \cpn{} accurately covered the necessary commonsense knowledge to answer the questions. 
The results show that ConceptNet knowledge covered the necessary commonsense in 91 out of 100 instances. More details are in Suppl. C.}
% The results show that the extracted ConceptNet knowledge covered the necessary commonsense knowledge in 91 out of 100 instances. The details are in \todo{Suppl. C}.}
% The details about the model and dataset are introduced in \autoref{subsec.cs_reasoning}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
% The cases are found by \imp{E4} and \imp{E5} during their system exploration of model behavior in the user studies. And we summarize their feedback and our findings in \autoref{subsec.user_study}. 
\rev{In this section, we first \renfeivis{present three cases} of using \name{} to systematically derive insights into the model's relational reasoning over different concepts in different situations.
% Specifically, the case introduces when a relation is well-learned or not sufficiently learned and when the model fails to use proper contexts for commonsense reasoning.
These cases were found by \imp{E4} and \imp{E5} during their system exploration of model behavior in the user study. 
Afterward, we will summarize the user behavior under the characterized workflow supported by \name{}. And we report users' ratings and qualitative feedback on the system designs and workflow.} 
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vis{Next, we present cases of using \name{} for model analysis. The cases were found by \imp{E4} and \imp{E5} during their system exploration of model behavior in the user study. Afterward, we summarize the user behaviors under the characterized system workflow. 
% supported by \name{}. 
And we report users' ratings and feedback on the system designs and workflow.}
% We present cases of using \name{} to systematically analyze the model's relational reasoning over different concepts in different situations. These cases were found by \imp{E4} and \imp{E5} during their system exploration of model behavior in the user study. 
% Afterward, we summarize the user behaviors under the characterized workflow supported by \name{}. And we report users' ratings and feedback on the system designs and workflow.}


\rev{\subsection{Cases of Using \systemname{}}}
\label{subsec.case_studies}

\vis{
% When analyzing UnifiedQA using \name{}, 
Using \name{}, experts discovered that the model has learned the relation \texttt{atlocation} in the context of ``office'' and ``room'' properly (details are in \referappendix{
% Suppl. \ref{app.case1}).}
Suppl. D.1).}
However, it has limitations in \texttt{cause} relation learning and ``movie'' context understanding.}
% However, it struggles with the cases about \texttt{causes} relation and does not understand the context around ``movie'' well. These insights motivated experts to probe and edit the model to enhance model knowledge on these cases.}

\begin{comment}
\subsubsection{Case one: Relation of \texttt{atlocation} regarding room and office is relatively well-learned}\label{subsec.case1}

\textbf{Global Summary}
(\imp{R1, R2}) 
% overview of the relation
After loading the system and dataset, the expert \imp{E4} first referred to the \gv{} to explore the model performance regarding different relations. 
After hovering over the green bars between the scatter plots, he was able to quickly observe that although there is an imbalanced relation distribution (as indicated by varied bar height), accuracies for most relations are about 0.70 (\renfei{\autoref{fig:case1 full}A}). It indicates that the model may have learned a fair amount of relations between different concepts.
Then, \imp{E4} felt curious about what QC-TC relations are and under what contexts the model learns well. 
He started with the relation \texttt{atlocation} with the highest green bar at the top. After clicking the bar, he selected the ``\textit{Correctness X Tranformed}'' projection mode and ``\textit{Correctness}'' coloring scheme to explore the distribution of the correctly-answered instances of \texttt{atlocation} in the question and answer (TC) scatter plots (\renfei{\autoref{fig:case1 full}A}). 
He noticed that there is a large cluster with green dots in the answer scatter plot. He wondered whether the models have really learned \texttt{atlocation} between QCs and TCs in these instances. Therefore, he switched to ``\textit{Relation X Tranformed}'' projection mode to see how the relation is learned by examining the correspondence between QSs and TCs after transformation (\renfei{\autoref{fig:case1 full}A}). And he discovered two well-formed and well-aligned clusters in the two scatter plots, which provides support for a good learning of \texttt{atlocation} relation.

\textbf{Subset Exploration}
(\imp{R1, R2, R3}) 
% subset view.
To further explore the contexts of selected instances, \imp{E4} looked at the QS cluster glyphs in the \sv{} (\renfei{\autoref{fig:case1 full}B}), where \xingbo{the green and blue bars} nearly occupy the two stacked bars at the top. It indicates a high model accuracy and overlap between the model concepts and ConceptNet concepts.
Moreover, he observed the yellow rectangles on the left of QS clusters are much shorter than the dark blue ones (\renfei{\autoref{fig:case1 full}B}), confirming that very few ConceptNet concepts are not covered by the model. 
He then hovered over the first cluster glyph to see the details of those concepts, where words like ``man'' and ``want'' appear. He thought that these concepts, not important to model predictions, might not affect the reasoning about \texttt{atlocation}.
Therefore, he hypothesized that the question contexts are properly considered by the model.
% clicked the first cluster glyph with the most instances to verify his hypothesis.
And he clicked this glyph to explore detailed instances and their explanations in the \iv{} to verify his hypothesis.
% Then, the \iv{} displayed the model statistics at the top. 
By scanning the top frequent model concepts in the histogram (\renfei{\autoref{fig:case1 full}C}) (\eg, ``where'', ``what'', ``store'', ``office'', ``room'', ``building''), he reasoned that many of these instances of \texttt{atlocation} are ``what'', ``where'' questions and associate with ``office'' and ``room''.

% Figure environment removed

\textbf{Instance Exploration and Searching}
(\imp{R1, R4}) 
Finally, through exploration of individual questions in the \iv{}, \imp{E4} found that the model truly captures important words for answering commonsense questions.
For example, in \renfei{\autoref{fig:case1 instances}}, SHAP values show ``office'' and ``put'' as important contexts for where the ``check'' can be located, which is ``desk drawer''. 
Another example in \renfei{\autoref{fig:case1 instances}} shows that the model properly considered contexts like ``room'' and ``contemplation'' for where the ``bookcases'' should be located in a ``study room'', which aligns with human knowledge. 
Then, \imp{E4} reasoned that the model has a good sense of \texttt{atlocation} in the situations of ``office'' and ``room''. 
And he lassoed all the instances of \texttt{atlocation} in the \gv{} and typed ``office'' and ``room'' to search relevant instances in the \iv{}, where the model achieves 90.00\% and 88.89\% accuracy, respectively (much higher than the overall 71.00\% accuracy). 
\imp{E4} was convinced that the model has learned the relation \texttt{atlocation} in the context of ``office'' and ``room'' properly. 

% Figure environment removed


\end{comment}

% % Figure environment removed



% \subsubsection{Relation of \texttt{causes} is not learned sufficiently}
\revvv{\subsubsection{Reveal Model Deficiencies in \texttt{Cause} Relation Learning via Multi-level Exploration and Instance Editing}}
\label{subsec.case2}

\textbf{Global Summary}
(\imp{R1, R2}) 
\renfeivis{E4 first diagnosed the model learning of \texttt{causes} relation by clicking the second largest bar between the scatter plots (\renfei{\autoref{fig:teaser}A}).
% , which shares similar performance with \texttt{atlocation} he explored earlier (in \referappendix{\autoref{app.case1}}).
}
% After exploring the largest relation group \texttt{atlocation} (in \autoref{app.case1}), \imp{E4} clicked the second largest green bars (\ie, \texttt{causes} relation) between the scatter plots (\renfei{\autoref{fig:case2 full}A}), which shares a very similar performance with \texttt{atlocation}. 
% He wanted to know how well this relation is learned by the model. Thus, he selected the 
He selected \textit{``Relation X Transformed''} projection scheme with the \textit{``correctness''} coloring to examine the alignment between question stem and target concept clusters considering model performance (dashed area in \renfei{\autoref{fig:teaser}A}).
In the left scatter, \imp{E4} observed that the transformed question stem projection does not form a neat cluster and does not align well with the target concept projection. It suggests insufficient learning of this relation.
% that this relation may not be sufficiently learned. 
In addition, there is a group of outliers at the top. Almost all of them have red color, which indicates a low model accuracy.
He wondered if it is the poor learning of \texttt{causes} relation that causes such high errors.
Afterward, \imp{E4} lassoed these instances to inspect their details further in the \sv{}.

\textbf{Subset Exploration}
(\imp{R1, R2, R3}) 
In the \sv{}, 
\imp{E4} noticed low model accuracy across all clusters, indicated by short green bars at the top of glyphs in \renfei{\autoref{fig:teaser}B}. 
% \imp{E4} noticed that all the clusters have low model accuracy (as indicated by the short green bars at the top of glyphs in \renfei{\autoref{fig:teaser}B}).
Moreover, he spotted that question stem clusters have long blue bars compared to short green bars, implying that the model considers \cpn{} concepts in question stems but still fails to answer correctly. 
% Moreover, he spotted that all question stem clusters have long blue bars in contrast to short green bars at the top.
% It implies that though the model considers most ConceptNet concepts in the question stems, it still cannot answer the questions correctly. 
This strengthened \imp{E4}'s concerns over the model learning of the \texttt{cause} relation.
% \texttt{cuses} between those question and target concepts. 
He clicked on the first question stem cluster (\renfei{\autoref{fig:teaser}B}) to explore its instances in the \iv{}.


\textbf{Instance Exploration and Editing}
(\imp{R1, R4}) 
\vis{
\imp{E4} first clicked the gray part of the accuracy bars to focus on the incorrect instances.
When scanning the model concepts (with stop words filtered) in the histogram, \imp{E4} found that the model usually attaches importance to words related to mood and emotion, such as ``happiness'', ``exhaustion'', and ``boredom''. 
Given the low model accuracy, \imp{E4} surmised that the model is not aware of the \texttt{causes} for human emotion.
To verify his thought, he explored and edited the detailed instances below. He found that most of these questions are very short, directly asking QC-TC relations. For example, as shown in \todo{\autoref{fig:teaser}C}, even after simplifying the original questions to ask straight at the relation between ``releasing built-up energy'' and ``wonderful'', the model still chose the wrong answer ``exhaustion''. He bookmarked these instances for model editing. 
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \imp{E4} observed that the average QC hit ratio is relatively low (as shown by the short blue bar) in \renfei{\autoref{fig:case2 full}C}. It implies that the model does not attach importance to QC in many cases for answering questions. 
% Meanwhile, from the histogram of the model concepts in the \iv{} (\renfei{\autoref{fig:case2 full}C}), \imp{E4} observed that the model normally attaches importance to common stop words (\eg, ``it'', ``a'', ``to'', ``that'') for answering questions.
% He surmised that the model is not aware of the QC-TC relation (\ie, \texttt{causes}) in these instances.
% To verify his thought, he explored and edited the detailed instances below. He found that most of these questions are very short, directly asking QC-TC relations under simple contexts. 
% However, the model fails to answer them.
% For example, as shown in \renfei{\autoref{fig:case2 full}D}, even after simplifying the original questions to ask straight at the relation between ``stabbing to death'' and ``being arrested'', the model still chose the wrong answer.
%%%%%%%%%%%%%%%%%%%%%%%%%%%% OLD %%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vis{\textbf{Model Editing} (\imp{R4}) 
\imp{E4} concluded that emotion-related \texttt{causes} is not sufficiently learned. And he applied model edits on the previously saved instances in the Model Editing Panel (\renfeivis{\autoref{fig:teaser}D}). He found that the editing accuracy is 100\% and model performance decrement is small (\ie, 71.01\%-70.93\%=0.08\%). He was satisfied with the edits.
}

% \textbf{Conclusion}
% \rev{Therefore, \imp{E4} concluded that \texttt{causes} relation is not sufficiently learned by the model. And he thought that we can apply data augmentation techniques~\cite{yang2020generative} to deliberately generate instances with the \texttt{cause} relation and further remove some common and unimportant stopwords in the questions that distract the model. With the training on augmented data, the \texttt{cause}-related performance could be improved. 
% In addition, he suggested re-weighting the underfit relations with simple rules~\cite{yi2021reweighting}. For \texttt{cause} relations, he proposed up-weighting the loss of \texttt{cause}-related QA pairs with a factor proportional to its loss.}

% Case 5.1.2, relation of “cause” not well learned. For specific relations that are not learnt well, we can use generative data augmentation (https://arxiv.org/abs/2004.11546) to explicitly generate augmented instance with that specific relation. With training on augmented data on under-fit relations, the overall performance can be accordingly improved. We can also do re-weighting on the under-fit relations with simple rules. E.g., for the “cause” relation, up-weight the loss of ‘cause’-related QA pairs with a factor proportional to its loss (Similar with the ideas in https://openreview.net/forum?id=9G5MIc-goqB).

% Therefore, \imp{E4} concluded that \texttt{causes} relation is not sufficiently learned by the model. And he thought that we can create augmented data or adversarial data for further training or fine-tuning. For example, by removing stopwords that distract the model, we can form some more high-quality training examples for models to focus specifically on commonsense relations, thus boosting the model performance.

% \xingbo{\textbf{TODO}: add actionable insights here. for example, data augmentation stuff.}
% \xingbo{The workflow in this case to can be well used to help NLP researchers to manually create augmented data or adversarial data for further training or finetuning. For example, by removing stopwords that distract the model, we can form some more high quality training examples for models to focus specifically on commonsense relations. Re-weighting on those augmented data may further help boost the model performance.}

% % Figure environment removed


% \subsubsection{Learning the relations is not enough, contexts are also important}
\revvv{\subsubsection{Probe Model Limitations in Understanding Relation Contexts via Instance Exploration, Editing, and Querying}}
\label{subsec.case3}
% Case three: model learns relations but the model is wrong and SHAP results do not attach important words (i.e., not learn contexts)
\vspace{-5mm}
% Figure environment removed

% % Figure environment removed

\textbf{Global Summary}
(\imp{R1, R2}) 
Another expert \imp{E5} used our system to investigate
% understand and diagnose the model's commonsense reasoning.
% He was interested in 
under what circumstances the model might fail to use contexts for relational reasoning over concepts.
He first chose the largest relation group (\ie, \texttt{atlocation}, the first green bar) in the \gv{} (\renfei{\autoref{fig:case2_new}A}). 
And he found good correspondence between question stem and target concept clusters under the \textit{``Relation X Transformed''} projection scheme. 
It implies good learning of \texttt{atlocation} in general.
% \vis{(similar to the case in \autoref{app.case1}).}
\imp{E5} wondered when the model might fail to reason about contexts. Then, he noticed that a group of dense red dots appear at the bottom left (\renfei{\autoref{fig:case2_new}A}). 
The accuracy of this group is low, and \imp{E5} decided to lasso the group for further exploration.

\textbf{Subset Exploration}
(\imp{R1, R2, R3}) 
In the \sv{}, \imp{E5} noticed that QSs fall into three clusters with varied accuracies (as suggested by the green bars) (\renfei{\autoref{fig:case2_new}B}).
Particularly, he was interested in the leftmost cluster since it has the lowest accuracy yet a similarly high question stem hit ratio (also indicated by tall dark blue bars at the left), compared to the other two.
As he hovered over the cluster glyph (\renfei{\autoref{fig:case2_new}B}), he discovered that the top model concepts are not so meaningful (\eg, a, the, to, you). 
He speculated that the model frequently relies on superficial information in question stems for answering the questions. 
\imp{E5} then clicked the cluster to explore the instances and model explanations. 

% % Figure environment removed

\textbf{Instance Manipulation}
(\imp{R1, R4}) 
In the \iv{}, \imp{E5} was curious about the incorrect instances and thus clicked the gray parts in the bar of model accuracy at the top to filter them.
When exploring the cases below, he found several interesting ones whose contexts associate with ``movie''.
For example, in one question (\renfei{\autoref{fig:case2_new}C}), 
\imp{E5} found that although ``air conditioning'' can also locate at ``movie theatre'', in this case, the model ignores the important context ``watch the game'' (without a green background), which normally happens in ``house''. 
Then, he further modified this instance to verify his finding.
Specifically, through several edits around ``watches'' in the original question (\renfei{\autoref{fig:case2_new}C}), the model still chooses ``movie theatre'' even though those contents such as television or live shows usually do not happen at ``movie theatre''. 
Therefore, \imp{E5} thought that the model attaches superficial information of ``watch'' to ``movie'' and does not understand the contexts.
In addition, other similar cases were observed where the model chooses ``movie'' without understanding what normally does not occur when watching movies, such as ``curtains drawing back'' or ``audiences clapping''.

\textbf{Instance query \& model editing} (\imp{R4}) 
\imp{E5} concluded that the model probably does not understand the contexts around ``movie'' well. He then located the related instances by searching keywords ``movie'' in the search input of \iv{}. He added those incorrect instances for model editing in the Model Editing Panel (\renfei{\autoref{fig:case2_new}D}). Finally, he saw that the editing accuracy is 100\% and the model maintains nearly the same performance as the original version (\ie, 70.84\% v.s. 71.01\%).

% \textbf{Conclusion}
% \imp{E5} concluded that the model probably does not understand the contexts around ``movie'' well and has some bias towards it when making decisions. 
% For such cases where context matters, 
% \rev{\imp{E5} suggested that we can perturb the context by word substitution or paraphrasing 
% % that contributes to the answers 
% to create several augmented examples under the same target commonsense relation.
% For example, in the case of the air conditioning located at the house (not the movie theater) while watching TV, we can replace the air conditioning with some other furniture located in the house to remind the model to focus on the context.}
