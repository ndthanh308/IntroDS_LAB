
%Version 2.1 April 2023
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%======================================================%%
%% to compile with pdflatex/xelatex use pdflatex option %%
%%======================================================%%

%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style


%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-vancouver.bst, sn-chicago.bst, sn-mathphys.bst. %  
 
%%\documentclass[sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
\documentclass[sn-basic]{sn-jnl}
% Basic Springer ]Nature Reference Style/Chemistry Reference Style
%%\documentclass[sn-mathphys,Numbered]{sn-jnl}% Math and Physical Sciences Reference Style
%%\documentclass[sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[sn-vancouver,Numbered]{sn-jnl}% Vancouver Reference Style
%%\documentclass[sn-apa]{sn-jnl}% APA Reference Style 
%%\documentclass[sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style
%%\documentclass[default]{sn-jnl}% Default
%%\documentclass[default,iicol]{sn-jnl}% Default with double column layout

%%%% Standard Packages
%%<additional latex packages if required can be included here>
\usepackage{graphics}
\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%

\usepackage{hyperref}

\usepackage[utf8]{inputenc}
%\usepackage[matrix,frame,arrow]{xypic}
\usepackage[braket, qm]{qcircuit}

% \usepackage{subcaption}
\usepackage[caption=false]{subfig}

\usepackage{listings}

\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%

%\jyear{2021}%

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{Quantum Convolutional Neural Networks with Interaction Layers for Classification of Classical Data}

%%=============================================================%%
%% Prefix	-> \pfx{Dr}
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% NatureName	-> \tanm{Poet Laureate} -> Title after name
%% Degrees	-> \dgr{MSc, PhD}
%% \author*[1,2]{\pfx{Dr} \fnm{Joergen W.} \spfx{van der} \sur{Ploeg} \sfx{IV} \tanm{Poet Laureate} 
%%                 \dgr{MSc, PhD}}\email{iauthor@gmail.com}
%%=============================================================%%

\author[1]{\fnm{Jishnu} \sur{Mahmud}}\email{jishnu.mahmud@gmail.com}
%\equalcont{These authors contributed equally to this work.}

\author[2]{\fnm{Raisa} \sur{Mashtura}}\email{raisamashtura@gmail.com}
%\equalcont{These authors contributed equally to this work.}

\author*[3]{\fnm{Shaikh Anowarul} \sur{Fattah}}\email{fattah@eee.buet.ac.bd}
%\equalcont{These authors contributed equally to this work.}

% \affil[1,2]{\orgdiv{Computer Science and Engineering}, \orgname{BRAC University}, \orgaddress{\city{Dhaka}, \postcode{1212}, \country{Bangladesh}}}

\affil[1,2,3]{\orgdiv{Electrical \& Electronics Engineering}, \orgname{Bangladesh University of Engineering \& Technology}, \orgaddress{\city{Dhaka}, \postcode{1000}, \country{Bangladesh}}}

%%==================================%%
%% sample for unstructured abstract %%
%%==================================%%

\abstract{Quantum Machine Learning (QML) has come into the limelight due to the exceptional computational abilities of quantum computers. With the promises of near error-free quantum computers in the not-so-distant future, it is important that the effect of multi-qubit interactions on quantum neural networks is studied extensively. This paper introduces a Quantum Convolutional Network with novel Interaction layers exploiting three-qubit interactions increasing the network's expressibility and entangling capability, for classifying both image and one-dimensional data. The proposed approach is tested on three publicly available datasets namely \textit{MNIST}, \textit{Fashion MNIST}, and \textit{Iris} datasets, to perform binary and multiclass classifications and is found to supersede the performance of the existing state-of-the-art methods. }

%%================================%%
%% Sample for structured abstract %%
%%================================%%

\keywords{Quantum Machine Learning, classification, entanglement, quantum gates, qubits.}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec1}

{I}{n} this era of artificial intelligence a constant improvement in computation speed, accuracy, and precision is a necessity. This widespread success in the world of computing over the last decade can be attributed to both the development of efficient software algorithms and the advancements in computational hardware. However, the physical limits of semiconductor fabrication in the post-Moore's Law era raise concerns about the extrapolation of its effectiveness in the future. On the other hand, significant advancements have been made in the field of quantum computing, which has shown promise as a potential solution for modern computing problems. Quantum computing exploits the laws of quantum mechanics to store and process information in quantum devices, using qubits instead of classical bits, which enables them to solve problems intractable for classical computers (\cite{r1}). Qubits have several properties that make them superior to the classical bit (\cite{yanofsky2008quantum}). Firstly, they are the superposition of the two fundamental bit states. This property enables a collective number of qubits to have exponentially higher processing power than the same number of classical bits. The second noteworthy property is entanglement, which creates interdependencies among the qubits, suggesting that the change of state of one will affect the state of the qubit it is entangled with. Superposition, Entanglement along with Interference, which is a method of controlling the qubit wavefunctions to reach their desired states, are the three properties that give qubits their formidable potential.

The era of quantum computing, currently referred to as the Noisy Intermediate Scale Quantum (NISQ) era, is characterized by the lack of absolute control over the qubits due to errors arising from quantum decoherence, crosstalk, and imperfect calibration, thereby limiting the number of qubits used on quantum computers. However, the revelation in January 2022 that quantum computing in silicon hit 99\% fidelity (\cite{mkadzik2022precision}) indicates a greater similarity between the desired and actual quantum states. This result promises near-error-free quantum computing and indicates that they are close to being utilized in large-scale applications, which further motivates the development of various machine learning algorithms to be implemented on quantum devices.

\section{Related Works}
Quantum machine learning (QML) involves constructing a sequential circuit of different types of parameterized quantum gates that act on a specific number of qubits. The main tasks of these QML algorithms are to logically place these parameterized gates and train their parameters to minimize the objective cost function. QML has already been implemented to address one of the most fundamental machine learning problems, i.e., classification. It is shown in \cite{QSVM_rebentrost} and \cite{mengoni_kernel} that kernel-based algorithms, such as Quantum-enhanced Support Vector Machine (QSVM) can classify data efficiently and accurately. The search for devising various convolutional networks in the quantum domain is introduced in \cite{cong_quantum_org} in which the concept of quantum convolutional neural networks (QCNNs) is proposed. Their architecture also claims to be able to tackle the exponential complexity of many-body problems, making them suitable for use in quantum physics problems. Advancing the field of QCNNs, a parameterized quantum neural network (QNN) with an enhanced feature mapping process has been designed in \cite{liu_the2nd}. Their proposed network is called a quantum-classical CNN (QCCNN) which is suitable for NISQ devices. It appears that several quantum counterparts of a large variety of classical machine learning models have been proposed over the recent years, all of which claim superior performances in various categories, such as accuracy and speed. Therefore, it is of no surprise that quantum networks have been shown to have a wide range of applications in medicine (\cite{lung_cancer}), weather forecasting (\cite{weather}), quantum chemistry (\cite{chemical}), and many more.

Ever since the proposal of QCNNs and the availability of quantum simulators and quantum computers, much attention has been drawn to devising various methods to improve the performance of classification problems.  This is driven by the fact that QCNN models are immune to barren plateau problems (\cite{pesah2021absence}) contrary to other structures. The architecture proposed in \cite{tak_hur_boss} has been benchmarked for binary image classification on the \textit{MNIST} and \textit{Fashion MNIST} datasets. A multiclass classification method using a quantum network is also reported in \cite{chalumuri}, which has been proven to perform well on 1D data such as the \textit{Iris dataset}. All these prior studies confirm that a QNN aids speed with a significantly lower number of parameters with better accuracy than their existing classical counterparts using a comparable number of training parameters. However, a crucial aspect of designing parameterized quantum circuits is to maintain sufficient expressibility and entangling capability, while keeping it cost-effective \cite{sim2019expressibility}. Expressibility is the ability of a quantum circuit, comprising quantum gates to explore the Hilbert space. The cost of a quantum circuit is judged by the number of layers and hence its parameters, as well as the number of two-qubit operations. This paper proposes a new QCNN structure with the additional incorporation of novel Interaction Layers and an ancilla-qubit-based classifier layer, with a minimal number of circuit depth and trainable parameters, which helps the proposed model outperform those mentioned above.

% The architecture proposed in this paper addresses all these issues. However, to this date, a shallow QCNN model with a quantum activation function as well as using a feasible number of qubits, and a minimal number of parameters to perform multiclass classification has not been devised to the best of our knowledge.
\

The major contributions of this work are summarized as follows: 

\

1. A new QCNN architecture is proposed which uses \textit{Amplitude} and \textit{Angle Encoding} schemes separately, considering two different data reduction and encoding techniques. 

\

2. Novel Interaction Layers are introduced, which exhibit sufficient expressibility and exploit the entanglement property of qubits further to help the quantum network learn more nuanced information from the data.

\ 

3. A classifier layer involving ancilla qubits and \textit{CNOT} gates are cascaded with the quantum convolutional structure to accommodate both binary and multiclass classifications. 

The use of ancilla qubits in the classifier layer cascaded with the Interaction Layers in a QCNN structure is a first to the best of our knowledge. 

\ 


4. The proposed network is tested on three publicly available datasets for binary and multiclass classification, and it is seen that the performance supersedes that of the existing state-of-the-art models using a similar number of parameters. The versatility of the network is further demonstrated in its ability to perform equally well in both image and 1-dimensional data.


\section{Proposed Architecture}
A simplified block diagram representation of the proposed architecture is depicted in Fig.\ref{fig:overall_network}. A quantum circuit, designed in the spirit of QCNN structure and possessing minimal trainable parameters, has been proposed. The robustness of QCNNs against the ``barren plateau'' issue is expected to be exhibited by the proposed network (\cite{pesah2021absence}). 
% Figure environment removed

% In classical convolutional neural networks, activation functions such as ReLU, along with convolutional and pooling layers, are utilized to introduce non-linearity in the network. This non-linearity results in the further separation of classes, leading to improved classifier performance. The success of activation functions in classical classifiers motivates the incorporation of similar ideas in the proposed quantum architecture. Entanglement, as a form of non-linear operation in quantum networks, has been proposed as Interaction Layers to simulate non-linear functions. Inspired by the use of activation functions in classical CNNs, these 
Interaction Layers are introduced in various stages of the proposed quantum architecture and are designed to leverage three-qubit interactions through the use of \textit{Toffoli} and parameterized rotational gates. The implementation of these  Layers in various stages of the network can be observed in Fig.\ref{fig:overall_network}. It differs from the earlier quantum convolutional methods, which relied on the reduction of qubits through sequences of convolutional and pooling layers alone. The rapid development of quantum hardware promises the ability to perform much more complex operations on qubits in the near future. Although three-qubit interactions are practically difficult to implement on existing hardware, this paper explores the comparative advantage in the performance of a network upon the addition of such layers. It must be noted that the number of trainable parameters and the total number of qubits have been kept minimum such that they can be implemented on NISQ devices for the purpose of comparison with other methods.  

It is expected that the incorporation of these novel Interaction Layers will enable the network to have the ability to extensively span the Hilbert space as well as exploit the entanglement property further for improved classification performance. The use of Toffoli gates, enabling three-qubit interactions in QCNN networks is a first to the best of our knowledge.

\subsection{Data Preprocessing}
The number of qubits and, therefore, the size of a QNN is bound by the current limitations of NISQ computing technology, in contrast to classical models, which often possess many trainable parameters due to their substantial size and depth. In the next stage where quantum feature encoding is performed, the features of the data to be classified are inserted as parameters of quantum gates, which perform various operations on these qubits. Therefore, a limited number of qubits also sets a bar on the total number of gate parameters; thus, the dimensionality reduction of classical data prior to its utilization within a quantum network is deemed imperative. 

Standard classical techniques, such as the autoencoder and simple resizing, are chosen as they allow for efficient compression of high-dimensional input data, which is important for reducing the computational complexity of quantum machine learning models. The autoencoder is particularly useful in this regard, as it can learn to represent the input data of dimensions $p\times p$, to a lower $q$-dimensional space, $q$ $<$ $p$, extracting a reduced set of features of size $q\times 1$, while still preserving important features and minimizing information loss. As an alternative, the simple resizing operation can also be effective in reducing the dimensionality of input data. It converts input data of dimensions $p\times p$ to a desired dimension of $q\times q$, $q$ $<$ $p$.
\subsection{Quantum Feature Encoding}
The projection of the reduced classical values, received as output from the previous layer, into quantum states is referred to as quantum feature encoding. Mathematically, the mapping of the classical input data, \textit{X}, into higher dimensional quantum states, represented in the Hilbert space and denoted by \textit{H}, is represented as \[\phi : X\mapsto H\]
% This operation is implemented in the subsequent layer of the proposed quantum machine learning architecture, as illustrated in Figure \ref{fig:overall_network}. 
where \(\phi\) is the feature map. In this stage of the network, $n$ qubits initialized to the state of\(\ \big|0\rangle\), are fed. %Compromising between the number of these input qubits and classical data dimension, a choice of eight qubits has been deemed optimum for error-free computations for NISQ computing. 
The qubits are then subjected to state operations via quantum gates, parameterized by the classical data $X$ which is the output of the block performing classical data reduction. This results in the mapping of the classical data to the Hilbert space and the resulting state is represented by \(\big|\psi\rangle\). This process of quantum state preparation encodes the classical values into the input qubits which can then exploit the unique properties of superposition, entanglement, and interference to achieve superior performance.  Two widely employed state preparation techniques, known as amplitude and \textit{Angle Encoding}, have aided in achieving significant results and are discussed in subsequent sections. 

\subsubsection{Amplitude Encoding}
In this encoding scheme, the normalized classical vectors from the Data Preprocessing Layer are represented as amplitudes of the \(n\)  input qubits in the Quantum Feature Encoding Layer. This displays a particular quantum advantage as normalized feature vectors of size \(2^n\) can be encoded into only $n$-qubits (\cite{schuld2018supervised}). The following equation shows the states prepared after performing \textit{Amplitude Encoding} on the input qubits. 
\begin{equation}
 \big|\psi_x\rangle\ = \sum_{i=1}^{N} x_i\big|i\rangle
\end{equation}
Here \(\big|\psi_x\rangle\) is the quantum state corresponding to the $N$-dimensional classical datapoint \(X\) after reduction, where \(N=2^n\), \(x_i\) is the \textit{i}-th element of the datapoint \(X\) and\(\ \big|i\rangle\) is the \textit{i}-th computational basis state. 

In a classical neural network, each binary value necessitates a distinct trainable weight or bias, resulting in a considerable number of parameters. In contrast, \textit{Amplitude Encoding} permits the representation of data through the amplitudes of a limited number of quantum states, thereby enabling a more compact representation. This has been demonstrated to result in a significant decrease in the number of trainable parameters, contributing to the simplification of the model and enhancement of its performance. While this method provides this benefit, it also increases the depth of the quantum circuit as \textit{O(poly(n))} or as \textit{O(n)} if the number of qubits fed in this layer is increased (\cite{div_conq}).

\subsubsection{Angle Encoding}
\textit{Angle Encoding} is another technique employed in quantum machine learning for the representation of data, which utilizes the rotation of quantum gates ($R_x, R_y$ and $R_z$) to encode classical information. This method involves encoding the $N$ features of classical data as the angles of \(n\) input qubits between quantum states (\cite{schuld2021supervised}). In this method, $N$ has been kept equal to $n$ to allow us to use the maximum size of classical features possible. The advantage of this approach lies in its ability to represent continuous data more naturally and efficiently compared to \textit{Amplitude Encoding} (\cite{schuld2021supervised}). The states resulting from performing \textit{Angle Encoding} on the input qubits are:
\begin{equation}
 \big|\psi_x\rangle\ = \otimes_{i=1}^{n} R(x_i)\big|0^n\rangle  
\end{equation}
Here \(R(.)\) can be either of the rotation gates \(R_x\), \(R_y\), or \(R_z\).
In \textit{Angle Encoding}, the angles between the quantum states can be varied continuously to capture the intricacies of the data. This leads to a more precise and nuanced representation of the data and can result in improved performance for certain types of quantum machine learning models. Although, unlike \textit{Amplitude Encoding}, it can only encode one qubit with one feature value, resulting in the reduction of noise, which makes it particularly advantageous in NISQ computing. 

 The selection of encoding techniques for this design is contingent upon the classical dimensionality reduction technique employed in the first layer. It can be recalled from the previous section that the \textit{Amplitude Encoding} method, which uses $n$ input qubits, can accommodate a maximum of $2^n$ data points. This requires the use of simple resizing to $2^{n/2} \times 2^{n/2}$ dimension followed by flattening, which is essential according to this state preparation method. Conversely, the \textit{Angle Encoding} technique encodes the flattened $N$ data points into $n$ qubits and thus relies on the use of an autoencoder to reduce the dimensions accordingly.

 \subsection{Proposed Layers}

\subsubsection{The Quantum Convolutional Layer}
% Figure environment removed
The proposed model for the classification problem is comprised of two Convolutional Layers with one Pooling Layer in between.  As shown in Fig.\ref{fig:conv1}, each of these layers is constructed of blocks of quantum gates called ansatzes, which are parameterized quantum circuits. In this study, an ansatz is composed of different configurations of single and multi-gate operations as illustrated in Fig.\ref{fig:ansatz1_FINAL} and \ref{fig:ansatz2_FINAL}. 

The first ansatz in Fig.\ref{fig:ansatz1_FINAL} consists of a relatively large number of parameters, i.e., 15, which helps increase flexibility, and the controlled \textit{R} gates help increase expressibility. The ansatz in Fig.\ref{fig:ansatz2_FINAL} has five fewer parameters, which is a parametrized form of a reduced version of the circuit that recorded the best expressibility in a study carried out by \cite{sim2019expressibility}. The ansatzes consist of the $R_x$, $R_y$, $R_z$ gates, which cause qubit rotations about the $x$, $y$, and $z$ axes, respectively. Ansatz 1 additionally has the $U3$ gate, which can be decomposed to rotation and phase shift gates and is represented by the matrix as follows:
$$U{3}(\theta_1,\theta_2,\theta_3) = \begin{bmatrix} \cos(\frac{\theta_1}{2}) & -\text{e}^{i\theta_3}\sin(\frac{\theta_1}{2}) \ \\
\text{e}^{i\theta_2}\sin(\frac{\theta_1}{2}) & \text{e}^{i(\theta_2+\theta_3)}\cos(\frac{\theta_1}{2}) \end{bmatrix}$$

Here, $\theta_1$, $\theta_2$, and $\theta_3$ are the parameters of the $U3$ gate, and the matrix represents a unitary operation on a qubit. The reason for the selection of two different ansatzes is to inspect the flexibility of the proposed network performance on slight changes in the structure of the ansatz and the number of trainable parameters.
% Figure environment removed
% Figure environment removed
\subsubsection{The Quantum Pooling Layer}
The main purpose of the Pooling Layer in any convolutional neural network is to reduce the spatial size of the data representation and to maintain the most important information. In the process, the layer helps reduce the computational cost of the network and improve its generalization capabilities by decreasing overfitting, making it robust to translations, rotations, and other minute changes. 

The quantum Pooling Layer in Fig.\ref{fig:pooling_ansatz} traces out one qubit from the two qubits it is fed and thus reduces the two-qubit states to a one-qubit state. The Pooling Layer uses two controlled rotation gates and a $Pauli-X$ gate. The Pooling Layers, along with the Convolutional Layers, extend qubit interactions beyond nearest neighbors and hence establishes further dependencies. The output qubits are further processed by an Interaction Layer to aid the learning process. 

% Figure environment removed

\subsubsection{The Interaction Layers}
The novelty brought forward in the proposed quantum architecture involves making variational quantum layers designed to introduce extensive entanglement and expressibility in the overall quantum network.
In order to bring forward this special quantum phenomenon, in this proposed \textit{"Interaction layer"}, \textit{Toffoli} gates are cascaded with the convolutional and rotational gates and are expected to establish three-qubit interactions, as shown in Fig.\ref{fig:entang_layer1} and \ref{fig:entang_layer-2}.
In order to bring forward this special quantum phenomenon in this proposed layer, \textit{Toffoli} and \textit{CNOT} gates are cascaded with the convolutional and rotational gates and are expected to establish three and two-qubit interactions, as shown in Fig.\ref{fig:entang_layer1} and \ref{fig:entang_layer-2}. 

% EDITED Two-qubit gates are more difficult to implement than single-qubit gates. Thus, the use of three-qubit interactions is rare in the design of QCNNs and other QNNs as they comprise multiple two-qubit gates. However, the rapid development of quantum hardware which promises to enable better control of qubit states makes the experimentation with a higher number of qubits to build quantum ansatzes important. The impact of \textit{Toffoli} gates in a QCNN structure has not been studied yet. \cite{sim2019expressibility} has shown that two-qubit \textit{CNOT} gates display high entangling capability along with a fair amount of expressibility. Therefore, it is expected that \textit{Toffoli} gates will show these characteristics even further.  Hence, if there is a significant improvement in results due to multi-qubit gate interactions, it can only be concluded that such gates will be used in real quantum hardware in the not-so-distant future.

In this era of NISQ computing, the implementation of two-qubit gates is more difficult than single-qubit ones. However, with the recent developments in quantum hardware and the promise of near error-free quantum computers in the not-so-distant future, the use of multi-qubit gates in quantum computers is expected to be more common. It must be noted that the use of two-qubit gates, such as the \textit{CNOT} gate, in quantum machine learning models is motivated by the increase in entanglement and expressibility of the network, which helps it to learn more complex features of classical data \cite{sim2019expressibility}. It is therefore imperative that studies are conducted exploring the effectiveness of three-qubit gates in various quantum networks. 

This paper explores the comparative advantage of three-qubit \textit{Toffoli} gates with the introduction of Interaction layers in the conventional QCNN structure. The difference in performance upon this addition is expected to indicate the extent of effectiveness which may result from the successful implementation of such gates in quantum hardware.  


In the first Interaction Layer, the four \textit{Toffoli} gates entangle the four qubits in a circuit-block interaction configuration, as illustrated in Fig.\ref{fig:entang_layer1}, which means that interdependency has been established between these quantum states, so the measured value of one state will depend on the others. This particular configuration is chosen over a nearest-neighbor or all-to-all configuration, as it tends to display favorable expressibility and less qubit connectivity requirements. The placement of these \textit{Toffoli} gates after the previous layers enable the network to span all the basis states more strongly (i.e., with a higher probability for the basis states that previously had near-zero probabilities) than without them. 
  % Figure environment removed

  % Figure environment removed

  % Figure environment removed
After the second Convolutional Layer, the qubits are passed through Interaction Layer 2. This Interaction Layer differs from the first in the inclusion of \(R_x\), \(R_y\), and \(R_z\) rotation gates with trainable parameters between its \textit{Toffoli} gates as shown in Fig.\ref{fig:entang_layer-2}. The parameterized gates in between the \textit{Toffoli} gates increase the degree of freedom of the quantum states, increasing the flexibility of the learning process and therefore has the potential to learn more nuanced features of the training data.

The last Interaction Layer in Fig.\ref{fig:entang_layer-3} acts as a classifier utilizing \textit{CNOT} gates to entangle the remaining qubits with the ancilla qubits, which are used to store the entangled states. It must be noted that the number of ancilla qubits is equal to the number of classes that are to be classified using the network and have been set to \ket{0} initially. The ancilla qubits interact with the remaining qubits of the network through the \textit{CNOT} gates as shown in Fig.\ref{fig:entang_layer-3} and are passed through the three rotational gates at the terminal of the quantum network. Finally, the measurement operation is performed on the ancilla qubits which causes the wavefunction to collapse into deterministic values. The expectation values of the ancilla qubits are measured and fed into the \textit{Softmax} function.


% the expectation values of ancilla qubits are measured collapsing the quantum states to classical values which are then passed through a softmax function EDIT?. 

The introduction of these Interaction Layers in the middle of the conventional QCNN along with an ancilla-based classifier (Interaction Layer 3) is expected to provide promising results and is inspected in detail later in the next section \ref{sec:sim_results}. The proposed Interaction Layers consisting of a combination of \textit{Toffoli} and \textit{CNOT} gates along with trainable parameters in between the Convolutional Layers, and the use of ancilla qubit-\textit{CNOT} classifier in QCNNs is a first to the best of our knowledge. 

\subsubsection{Cost Function and Softmax}

Following the measurement of the ancilla qubits, the classical values are sent to the \textit{softmax} function to calculate the probability vectors for each class. Then the losses are calculated using the classical categorical cross-entropy loss function which can be expressed as:
\begin{equation}
loss = \sum_{i=1}^{output\ size} y_i.log(\bar{y_i})
\end{equation}
where \(y_i\) is the true-label and \(\bar{y_i}\) is the predicted probability of the corresponding class. The parameters of the quantum gates are optimized through gradient descent using classical computational techniques, after which the parameters are updated accordingly through back-propagation.

\section{Simulation and Results}\label{sec:sim_results}

\subsection{Dataset}
The widely utilized standard datasets, namely \textit{MNIST} \cite{deng2012mnist} and \textit{Fashion MNIST} \cite{xiao2017fashion} are employed to benchmark the proposed QCNN model. Binary classification involving classes (0, 1) and three-class classification involving classes (0, 1, 2) are performed. In the \textit{MNIST dataset}, the number of training (test) images, for class 0 is 5,923 (980); for class 1 it is 6,742 (1,135); for class 2 it is 5,958 (1,032). The \textit{Fashion MNIST dataset} consists of 6,000 training and 1,000 test images per class. The original size of the images from either dataset is $28\times28$, and a reduction in dimension is accomplished through a classical autoencoder or simple resize to the desired shape. A third dataset known as the \textit{Iris dataset} \cite{de2015mobile} is used solely for the purpose of multiclass classification. It consists of feature data of three classes of iris species with 50 samples per class. The features include 4 attributes per sample namely sepal length, sepal width, petal length \& petal width. The dataset is such that one flower class is linearly separable from others but the other two classes are not linearly separable from each other.

% Figure environment removed

% Figure environment removed

\subsection{Simulation}
The simulation of the proposed QCNN model is conducted using Pennylane (\cite{bergholm2018pennylane}). The variational circuit is trained through the use of the Nesterov Moment Optimization algorithm (\cite{nesterov}). A loop is executed through the training process where a batch of randomly selected images is fed into the network in each iteration, reducing run time and preventing the gradient from becoming trapped in a local minimum. The optimization of the learning process is further facilitated through the use of an adaptive learning rate, where the learning rate is decreased as the rate of change of the output of the cost function is decreased. 

\subsection{Performance evaluation}
\subsubsection{Binary classification}

For the binary classification problem, classes 0 and 1 are chosen for both datasets in order to be able to compare with previous works. A total of eight input qubits along with two ancilla qubits for the two classes are used in the proposed model. The Convolutional and the Pooling Layers are arranged as illustrated in Fig.\ref{fig:overall_network}. 
During training, the batch size is kept at 50 images, which are randomly selected in each iteration. The learning rate in the Nesterov Optimizer is tuned to be 0.05 at the beginning of the learning process and in the later stages, it is reduced in accordance with the decrease in the cost and the improvement of test accuracy. The trainable parameters are initialized randomly using the normal distribution and the average classification accuracy is calculated over five random initializations.
\begin{table*}[!ht]
    \caption{Results of the proposed binary classification model}
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccc}
    \hline
    \textbf{Dataset}               & \textbf{Ansatz}    & \textbf{Encoding Method} & \textbf{Trainable parameters} & \textbf{Accuracy (\%)} \\ \hline
    \multirow{4}{*}{Fashion MNIST} & \multirow{2}{*}{1} & Angle & \multirow{2}{*}{50}      & 96.60                  \\ 
     &                    & Amplitude &                     & 92.50 \\ 
     & \multirow{2}{*}{2} & Angle       & \multirow{2}{*}{40} & 94.70 \\  
     &                    & Amplitude &                     & 91.30 \\ \hline
    \multirow{4}{*}{MNIST}         & \multirow{2}{*}{1} & Angle  & \multirow{2}{*}{50}      & 98.22                  \\ 
     &                    & Amplitude &                     & 99.24 \\  
     & \multirow{2}{*}{2} & Angle       & \multirow{2}{*}{40} &   93.52    \\ 
     &                    & Amplitude &                     &    94.28   \\ \hline
    \end{tabular}%
    }
    \label{tab:1}
\end{table*}
\begin{table}[ht]
    \caption{Table showing a comparison of the results of our proposed model to that of existing models for different datasets} 
    \begin{tabular}{ccc}
    \hline
    \textbf{Dataset}               & \textbf{Model used}  & \textbf{Accuracy (\%)} \\ \hline
    \multirow{3}{*}{Fashion MNIST} & Proposed    & 96.60                  \\  
                                   & QCNNFCDC (\cite{tak_hur_boss})       & 94.30    
                \\      & Proposed without E & 92.00 \\ 
                                   \hline
    \multirow{2}{*}{MNIST}         & Proposed            & 99.24                  \\  
                                   & QCNNFCDC (\cite{tak_hur_boss})            & 98.70                  \\
                                   \hline
    \multirow{2}{*}{Iris}         & Proposed            & 94.74                  \\  
                                   & HCQAMC (\cite{chalumuri})            & 92.10                 \\ 
     
    \end{tabular}
\footnotetext{Note: `E' refers to Interaction Layers 1 and 2.}
\label{tab:2}
\end{table}

% Figure environment removed

% Figure environment removed

The effect of the following different approaches on overall performance is investigated:

1. Quantum encoding by either \textit{Amplitude Encoding} or \textit{Angle Encoding}.

2. The two parameterized ansatzes given in Fig.\ref{fig:ansatz1_FINAL} and \ref{fig:ansatz2_FINAL} used to construct the Convolutional Layers in Fig.\ref{fig:conv1}. 

 % Figure environment removed
  % \hfill
  % Figure environment removed
  
The classification of the \textit{Fashion MNIST} is benchmarked to an accuracy of 96.60\% using the combination of autoencoder with \textit{Angle Encoding} and ansatz 1 as the Convolutional Layer filters, for which the total number of trainable parameters in the quantum network is 50. In order to demonstrate the convergence of the cost function of the training stage, Fig.\ref{fig:auto_cost} and Fig.\ref{fig:res_cost} are shown. It can be observed from Fig.\ref{fig:auto_acc} that over 90\% accuracy is reached within only 50 iterations for both the ansatzes which suggests that the network converges rapidly with respect to the number of iterations. The peak accuracy attained for the \textit{MNIST} dataset is 99.24\%. The number of trainable parameters, in this case, is also 50, and simple resizing with ansatz 1 is used. The accuracies for the different combinations are summarized in Table \ref{tab:1}.

The superiority in performance of ansatz 1 over ansatz 2, due to its additional parameters is demonstrated in Fig.\ref{fig:res_cost} and \ref{fig:res_acc}, where the performances are compared with respect to the same Quantum Feature Encoding methods.

It can be observed from table \ref{tab:1}, that the type of Quantum Encoding method used to bear the best accuracies is dependent on the dataset that is used. The peak accuracy for the \textit{Fashion MNIST} dataset results from reducing the classical data by an autoencoder followed by \textit{Angle Encoding} whereas, for \textit{MNIST}, it is simple resizing followed by \textit{Amplitude Encoding}. Comparison of the results with other existing quantum machine learning models for binary classification, such as that proposed in \cite{tak_hur_boss}, shows that our model, surpasses their accuracy, as shown in the first two rows of table \ref{tab:2}. The accuracy for the binary classification of classes 0 and 1 for \textit{Fashion MNIST} is 2.6\% and for \textit{MNIST} it is 0.5\% more than that found in \cite{tak_hur_boss}. This increase in accuracy can be attributed to the incorporation of the Interaction Layers and the use of the ancilla-based final classifier (Interaction Layer 3). \cite{tak_hur_boss} have further shown that their quantum network outperforms classical counterparts using a similar number of trainable parameters for the binary classification problem. %A comparison in \ref{tab:2}, shows the network proposed in our paper outperforms the one presented in \cite{tak_hur_boss}. 
It can therefore be concluded that the results of the study in our paper exhibit a clear superiority in performance compared to the classical networks with a similar number of trainable parameters. 

The peak accuracy for ansatz 1 used to construct the Convolutional Layers is 96.60\% compared to 94.70\% when ansatz 2 is used. This increase can be related to the number of trainable parameters available for each ansatz which is more in the case of ansatz 1. Additionally, the effect of the proposed Interaction Layers 1 and 2 on the performance of the network is demonstrated by comparing the performances with and without their presence. As evident in Fig.\ref{fig:entang_cost} and Fig.\ref{fig:entang_acc}, it can be concluded that these layers help reduce cost and increase accuracy by creating further dependencies between quantum states and making them more capable of spanning the \textit{Hilbert Space} adding only $12$ more trainable parameters. In both cases, the data reduction and quantum encoding technique used is autoencoder and \textit{Angle Encoding} respectively on the \textit{Fashion MNIST} dataset with ansatz 1 in Fig.\ref{fig:ansatz1_FINAL} used as the convolutional filter.

\subsubsection{Multiclass classification} \label{sec:multiclass_results}
Multiclass classification is performed on the \textit{MNIST} and \textit{Fashion MNIST} datasets with the network slightly modified to include two Convolutional Layers cascaded together in each convolutional stage. It must be noted that these cascaded Convolutional Layers share the same weight and therefore the number of trainable parameters in the circuit does not significantly increase. The number and placement of the Interaction Layers remain unchanged from the network for Binary classification. Classes 0, 1, and 2 are selected for both datasets and the batch size is kept at 100 with the learning rate set at 0.05 in the beginning and adapted to 0.01 after 50 iterations. 
The peak classification accuracy obtained is 91.76\% for the \textit{Fashion MNIST} dataset and 85.11\% for the \textit{MNIST} dataset using ansatz 1. The total number of trainable parameters in the network is only 53.

It is noticed that the combination of ansatz 1 and \textit{Angle Encoding} as the Quantum Encoding Method provides the highest accuracy for the \textit{Fashion MNIST} dataset but the combination of ansatz 1 with the \textit{Amplitude Encoding} dataset performs better for \textit{MNIST} dataset. 

\begin{table}[]
\centering
\caption{Results of the Multiclass Classification problems.}
\label{tab:my-table}
\begin{tabular}{cccc}
\hline
\textbf{Dataset}       & \textbf{Encoding Method} & \textbf{Trainable Parameters} & \textbf{Accuracy} \\ \hline
Fashion MNIST & Angle           & 53                   & 91.76\%  \\ \hline
MNIST         & Amplitude       & 53                   & 85.11\%  \\ \hline
Iris          & Angle           & 31                   & 94.74\%  \\ \hline
\end{tabular}
\footnotetext{Note: In all of the cases, ansatz 1 is used to construct the Convolutional Layers.}
\label{tab:3}
\end{table}

To demonstrate the flexibility of the proposed circuit, performance on the \textit{Iris} dataset is also tested. In order to accommodate data of smaller dimensions, a cut-down version of the proposed circuit, with only 4 qubits was sufficient. The test accuracy with a batch size of 50 and a learning rate of 0.005 is found to be 94.74\%. A three-class classifier with a variational quantum circuit has been proposed in \cite{chalumuri}, where classification was performed on classical one-dimensional feature data. The accuracy of 94.74\% supersedes the accuracy of the network proposed in \cite{chalumuri} (92.10\%) as shown in the third row of table \ref{tab:2}. It must also be noted that the network used for benchmarking the \textit{Iris} dataset has only 31 parameters and is much shallower than the one in \cite{chalumuri}. It is therefore understood that the network is not only limited to image classification but performs equally well in one-dimensional feature data.
The results of Multiclass classification problems are summarized in \ref{tab:3}.

\section{Conclusion}
In this work, a shallow entangled QCNN with a minimal number of trainable parameters is proposed, which provides very satisfactory performance in binary and multiclass classification problems. The incorporation of weighted Interaction Layers, consisting of trainable parameters and utilizing three-qubit interactions between the quantum Convolutional Layers, has played a major role in enhancing the performance of the network. This result indicates the significance of increased qubit interaction on the substantial increase in the ability of a quantum network to learn more complex information from the training data whilst only using a few parameters.

This approach constitutes a novel way towards the development of a generalized parameterized QNN that performs equally well for binary and multiclass classification on both image data and one-dimensional feature data. It further explores the possibilities of performance enhancement of quantum networks upon the use of increased qubit interaction which is expected to be a reality in the not-so-distant future.

The simulation results indicate a quantum advantage of such networks, showing a clear superiority in performance compared to their classical counterparts using a similar number of parameters. Further research could be conducted to gain a more comprehensive understanding of the quantum advantage of these networks. An extensive investigation of the underlying causes of data dependencies on the proposed feature encoding methods can be done. Other future milestones may also include an extension of the work for big data analysis and the solution of more complex problems utilizing more resources and power on real quantum computers.

\section*{Statements and Declaration}
The authors declare no competing interest in any other work or publication.
\section*{Data Availability}
The simulation code used in this paper can be found at the following link: \href{https://github.com/chacconed/Quantum-Convolutional-Neural-Networks-with-Interaction-Layers-for-Classification-of-Classical-Data}{Simulation and Code}.

The datasets used in this paper are publicly available and can be found in the works of \cite{deng2012mnist}, \cite{xiao2017fashion}, and \cite{de2015mobile}.







\bibliography{sn-bibliography}% common bib file
%% if required, the content of .bbl file can be included here once bbl is generated
%%\input sn-article.bbl


\end{document}
