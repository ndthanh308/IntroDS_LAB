% Figure environment removed

\section{Preliminaries: Model-based Planning under Uncertainty via Learning over Subgoals}\label{sec:lsp}
% \subsection{Learned Subgoal Planner (LSP)}
As Eq.~\eqref{eq:POMDP} cannot be solved directly, our robot instead relies on the recent Learning over Subgoals Planning (LSP) approach~\cite{pmlr-v87-stein18a} to determine the robot's behavior.
LSP introduces a model-based planning abstraction that alleviates the computational requirements of POMDP planning, affording both reliability and good performance informed by predictions about unseen space from learning.

For LSP planning, actions available to the robot correspond to navigation to \emph{subgoals}---each associated with a boundary between free and unknown space---and then exploration beyond in an effort to reach the unseen goal.
Consistent with this action abstraction, planning under the LSP model is done over an abstract belief state: a tuple $b_t = \{m_t, q_t\}$, where $m_t$ is the current map of the environment, and $q_t$ is the robot pose.
Each high-level action $a_t \in \mathcal{A}(\{m_t, q_t\})$ has a binary outcome: with probability $P_S(a_t)$, the robot \emph{succeeds} in reaching the goal or (with the inverse probability $1 - P_S(a_t)$) fails to reach the goal.
Upon selecting an action $a_t$, the robot must first move through known space to the boundary, accumulating a cost $D(m_t, q_t, a_t)$.
If the robot succeeds in reaching the goal, it accumulates a \emph{success cost} $R_S(a_t)$, the expected cost for the robot to reach the goal, and no further navigation is necessary.
Otherwise, the robot accumulates an \emph{exploration cost} $R_E(a_t)$, the expected cost of exploring the region beyond the subgoal of interest and needing to turn back, and must subsequently choose another action $a_{t+1} \in A_{t+1} \equiv \mathcal{A}(\{m_t, q(a_t)\})\setminus \{ a_t \}$.

Under this LSP planning model, the expected cost of taking an action $a_t$ from belief state $b_t = \{ m_t, q_t\}$ is
\begin{equation}\label{eq:lsp-planning}
\begin{split}
    Q(&\{m_t, q_t\}, a_t\in  \mathcal{A}) = D(m_t, q_t, a_t) + P_S(a_t) R_S(a_t) \\
    & + (1-P_S(a_t)) \left[R_E(a_t) + \min_{a_{t+1}}Q(\{m_t, q(a_t)\},a_{t+1}) \right]
    \end{split}
\end{equation}
While the known-space distance $D(m_t, q_t, a_t)$ can be calculated directly from the observed map using A$^{\!*}$ or RRT$^*$, the \emph{subgoal properties} $P_S(a_t)$, $R_S(a_t)$, and $R_E(a_t)$ for each subgoal are estimated via learning from information collected during navigation.\footnote{The terms $P_S$, $R_S$, and $R_E$ are implicitly functions of the belief, but shown here only as functions of the chosen action for notational simplicity.}

In the LSP approach~\cite{pmlr-v87-stein18a} and in other LSP-derived planners so far~\cite{NEURIPS2021_926ec030, bradley2021}, learning has relied only on \emph{local} information---e.g., semantic information, images, or local structure.
However, locally-accessible information alone cannot inform effective predictions about unseen space in general; information revealed elsewhere in the environment may determine where a robot should navigate next.
As such, the learned models upon which existing LSP approaches rely perform poorly in even simple environments where non-locally available information is required.
We show one example of this limitation in Sec.~\ref{sec:example-case} and discuss how we use Graph Neural Networks to overcome it in Sec.~\ref{sec:eq-theory-non-local}.