
\section{Training Hyperparameters} \label{sec:hyperparams}

We provide here detailed hyperparameters for all training settings included in the main paper. Note however that, for pre-training and fine-tuning, we follow the recipes of the original works. We therefore \textbf{perform no hyperparameter tuning}.
%
This highlights a positive feature of \ours: transferability. As \ours's training objective is straightforward (pseudo-)class aware detection, which is already what detectors are typically designed for, it can be adapted to any detector architecture with minimal effort.
%
The specific hyperparameters for each architecture are presented below:\\

\noindent \textbf{Def. DETR:} We follow~\cite{bar2022detreg} and pretrain for 5 epochs per stage on ImageNet with a batch size of 192 and a fixed learning rate of 0.0002. For finetuning, we train on COCO for 50 epochs and PASCAL VOC for 100 epochs, with a batch size of 32. The learning rate is set to 0.0002, and is decreased by a factor of 10 at epoch 40 and 100 for COCO and PASCAL VOC respectively.

\noindent \textbf{ViDT+:} We use the training hyperparameters proposed in~\cite{vidt}. Specifically, unless stated otherwise, ViDT+ is pretrained for 10 epochs per stage on ImageNet and Open Images, and for 50 epochs per stage on COCO, with batch size 128.
In all cases, the learning rate is set to 0.0001 and follows a cosine decay schedule.

\noindent \textbf{Cascade Mask R-CNN:} We use the pretraining and fine-tuning hyperparameters proposed in~\cite{wang2023cut}. Specifically, unless stated otherwise, Cascade Mask R-CNN is pretrained for 160,000 steps per stage on ImageNet with batch size 16. The learning rate is set to 0.01 and decreased by a factor of 10 at after 80,000 training steps.

\noindent \textbf{Mask R-CNN:} We use the pretraining and fine-tuning hyperparameters proposed in~\cite{wang2023cut}. 
This experiment is designed to enable a direct comparison with AlignDet~\cite{li2023aligndet} \textit{under the AlignDet setting}. We note however that it involves limited pretraining (12 epochs) on a small dataset (MS COCO) with a shallow detector architecture (Mask R-CNN), each of these being a disadvantageous regime for \ours, which was designed for pretraining on large datasets with the goal of learning strong representations through object detection.
Accordingly, we modify our approach \textbf{only for this setting} by
a) freezing the backbone (due to the combination of shallow detector, low data regime, and short schedule), and b) avoiding self-training (to restrict training to 12 epochs to match AlignDet).
Even with this setting, which plays against the strengths of \ours, we outperform AlignDet by a significant margin, demonstrating the effectiveness of our proposed aligned pretext task.

Unless stated otherwise, we pretrain with 2048 pseudo-classes (i.e. we set the number of clusters for the global clustering step to 2048), and apply one round of self-training, following our findings in~\cref{tab:abl_stages}.

For experiments other than the full setting (i.e. semi-supervised, few-shot, and self-supervised representation learning), we apply the following changes to the training parameters described above:

\noindent \textbf{Semi-supervised:} For Def. DETR we follow DETReg~\cite{bar2022detreg} and finetune on COCO for 2,000 epochs for 1\% of samples annotated, 1,000 epochs for 2\% of samples, 500 epochs for 5\% of samples, and 400 epochs for 10\% of samples. The learning rate is kept fixed at 0.0002. Results in Table 3 are measured over 5 runs, with different, randomly sampled annotated samples. For Cascade Mask R-CNN, we closely follow the training setting and evaluation protocol used in~\cite{wang2023cut}.

\noindent \textbf{Few-shot:} We finetune on COCO's base classes using the splits proposed in~\cite{wang2020frustratingly}. For the standard few-shot setting we a) finetune on the base classes following the COCO finetuning settings outlined above, and b) finetune on the 10- and 30-shot sets for 30 and 50 epochs respectively, with a fixed learning rate of 0.0002 and 0.00004. For the extreme setting, we directly finetune on the 10- and 30-shot sets for 400 epochs with a learning rate of 0.0002 which is decreased by a factor of 10 after 320 epochs. Results in Table 4 correspond to the best validation score of each run during training, averaged over 5 runs, with k-shot samples corresponding to seeds 1-5 of~\cite{wang2020frustratingly}. When finetuning on the k-shot instances, the backbone is kept frozen in both settings.

\noindent \textbf{Self-supervised representation learning on scene-centric data:} For these experiments, where the entire architecture is initialized from scratch (backbone \& detector), we train for 1,000 epochs on COCO, 100 epochs on ImageNet, and 70 epochs on Open Images. This allows for a fair comparison, with approximately the same number of training steps across datasets.


\section{Datasets} \label{sec:datasets}

We use the training sets of ImageNet~\cite{imagenet2015}, Open Images~\cite{OpenImages2} and MS COCO~\cite{coco2014} for unsupervised pretraining. For supervised finetuning we use the training sets of MS COCO and PASCAL VOC~\cite{everingham2010pascal}. Results are reported for the corresponding validation sets, using Average Precision (AP) and Average Recall (AR).
ImageNet includes 1.2M object-centric images, classified with 1,000 labels and without object-level annotations. Open Images includes 1.7M scene-centric images and a total of 14.6M bounding boxes with 600 object classes. COCO is a scene-centric dataset with 120K training images and 5K validation images containing 80 classes. PASCAL VOC is scene-centric and contains 20K images with object annotations covering 21 classes.



\section{Algorithm}

In this section, we present~\ours as an algorithm.

\begin{algorithm}
\footnotesize{
\caption{\ours Pretraining}
\label{alg:method}
\begin{algorithmic}[1]
\Require $\{X_i\}_{i=1}^I$, Net $g=(g_b,g_h)$, initial params. $\Theta_0$
\State \Comment{Unsup. train set gen. (\cref{sec:initialization})}
\For{$i=1:N$}  
\State ${\mathbf{F}_l} \gets g_b(X_i)$
\State $\mathbb{M}_i \gets \bigcup \text{Cluster}( F_l, K )$ \Comment{$\;K \in \mathcal{K}, l \in \mathcal{L}$}
\State $\mathbb{R}_i \gets $ Connected Components($\mathbb{M}_i$) 
\State $\{b_n^i, f_n^i\}_{N(i)} \gets$ Filter$( \mathbb{R}_i )$ 
\EndFor
\State $\{c_n^i\} \gets $ K-Means$( \{f_n^i\}, K=C)$ \Comment{Pseudo-classes}
\State $\mathcal{T}_0 \gets \left\{ X_i, \{(b_n,c_n)\}_{n=1}^{N(i)} \right\}_{i=1}^I$
\State \Comment{Self-training (\cref{sec:self-training})}
\For{$j$ stages}  
\State $g(-; \Theta_{j+1} ) \gets $ Train $(\mathcal{T}_j, g)$  \Comment{Using eq.~\ref{eq:detr_loss}}
\State $\mathcal{T}_{j+1} \gets $ Filter( $\{g(X_i;\Theta_j)\}_{i=1}^{I}$ )
\EndFor
\end{algorithmic}
}
\end{algorithm}


\section{Convergence \& Alignment Analysis}\label{sec:fsanalysis}

In this section, we discuss the convergence and alignment properties of \ours by analyzing the results of the "extreme" few-shot experiments. As discussed in paper Sec. 5, in this setting we pretrain Def. DETR on ImageNet, and then finetune directly on COCO {\tt train2014}, using $k\in \{10, 30\}$ instances from all classes.


\begin{table}[t]
    \caption{Results of "extreme" few-shot training for 50 epochs and 400 epochs.}
  \label{tab:few_shot_ap}
  \begin{center}
  \begin{footnotesize}
  \begin{tabular}{ c c c c c c c}
    \toprule
    \multirow{2}{*}{Method} & \multirow{2}{*}{Epochs} & \multicolumn{2}{c}{Novel Class AP} & \multicolumn{2}{c}{Novel Class AP$_{75}$} \\
     &  & 10 & 30 & 10 & 30 \\
     \midrule
     DETReg & \multirow{2}{*}{50} & 1.9 & 3.4 & 1.8 & 3.52 \\
     \textbf{\ours} &  & \textbf{8.32} & \textbf{13.9} & \textbf{8.06} & \textbf{14.4} \\
      \midrule
     DETReg & \multirow{2}{*}{400} & 5.6 & 10.3 & 6.0 & 10.9 \\
     \textbf{\ours} &  & \textbf{10.3} & \textbf{14.5} & \textbf{10.9} & \textbf{15.1} \\
    \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
\end{table}

% Figure environment removed


% Figure environment removed

In~\cref{fig:extreme_k10,fig:extreme_k30} we present the AP scores for \ours and DETReg during training, averaged over 5 runs, and measured over the validation set's novel classes. As was noted in paper Sec. 5, \ours outperforms DETReg by large margins. Notably, however, it is also shown to converge much faster. More specifically, in~\cref{tab:few_shot_ap} we present results for 50 epochs of k-shot finetuning against the performance reached after 400 epochs. In both cases, we average the best validation score across 5 runs. We see that, at 50 epochs, \ours has already reached near-peak performance, while DETReg converges at a much slower rate.

This means \ours effectively alleviates the sample inefficiency and slow convergence of DETR architectures and makes our method particularly useful when annotations and/or computational resources are extremely scarce. These results provide further support for our conclusions in paper Sec. 5, namely that \ours is much better aligned with the downstream task, with learned object representations that are well suited for class-aware object detection, so that minimal training and supervision can lead to strong performance. 

\section{Analysis and ablations}
\label{sec:ablations}

Throughout this section we use ViDT+ and, unless stated otherwise, pretrain on ImageNet for 10 epochs per stage.

\paragraph{Impact of object proposals:} We evaluate our object proposal method in two ways: a) we examine how well it localizes objects by computing the Average Recall (AR) score on COCO {\tt val2017} (see~\cref{tab:rpm}), and b) we investigate its impact on \ours by replacing it with Selective Search and present the outcomes (see~\cref{tab:abl_proposals_tr}). 


\begin{table}[t]
    \caption{\textbf{Quality of proposals:} AR results on COCO {\tt val2017}. The first section presents results for the initial extraction of object proposals, while the lower two sections present results for proposals generated by detection/segmentation architectures trained on the initial proposals.}
  \label{tab:rpm}
  \begin{center}
  \begin{footnotesize}
  \begin{tabular}{ c c c c }
    \toprule
     Object proposals & Detection Architecture & AR$^{100}$ \\
     \midrule
     Sel. Search & - & 10.9 \\
      \textbf{\ours}-St. 0 & - & \textbf{13.4} \\
      \midrule
     DETReg & \multirow{3}{*}{ViDT+} & 21.5 \\
     \textbf{\ours}-St. 1 &  & 25.9\\
     \textbf{\ours}-St. 2 &  &\textbf{ 27.1} \\
      \midrule
     CutLER & \multirow{3}{*}{Cascade Mask R-CNN} & \textbf{32.7} \\
     \textbf{\ours}-St. 1 &  & 24.5\\
     \textbf{\ours}-St. 2 &  & 24.6 \\
    \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
\end{table}

\begin{table}[t]
  \begin{center}
      \caption{\textbf{Impact of initial proposals:} AP results on COCO {\tt val2017}, using different initial object proposal methods.}
  \label{tab:abl_proposals_tr}
  \begin{footnotesize}
  \begin{tabular}{ c c c c c }
    \toprule
     Method & Proposals & AP & AP$_{50}$ & AP$_{75}$ \\
     \midrule
     MoBY & - & 48.3 & 66.9 & 52.4 \\
     \midrule
     \ours-St. 1 & \multirow{2}{*}{Sel. Search} & 48.7 & 67.3 & 52.7 \\
     \ours-St. 2 &  & 48.6 & 67.1 & 52.2 \\
     \midrule
     \ours-St. 1 & \multirow{2}{*}{Our Anns.} & 48.9 & 67.4 & 52.9 \\
     \ours-St. 2 &  & \textbf{49.6} & \textbf{68.2} & \textbf{53.8}  \\
    \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
\end{table}

\begin{table}[t]
  \begin{center}
      \caption{\textbf{Number of classes}. Pretraining and finetuning on COCO, evaluation in terms of training accuracy, AR of the pretrained detector, and AP of the finetuned model. 1 class implies class-unaware pretraining.}
  \label{tab:abl_clusters}
  \begin{footnotesize}
  \begin{tabular}{ c c c c }
    \toprule
     Classes & ACC & AR & AP \\
     \midrule
     1 & - & \textbf{25.2} & 41.2 \\
    256 & \textbf{80.01} & 23.9 & 43.8 \\
    512 & 75.13 & 24.0 & 43.9 \\
    2048 & 53.75 & 23.9 & \textbf{44.1} \\
    \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
\end{table}


\cref{tab:rpm} includes results both for our initial proposals (noted as \ours-St. 0), and the proposals generated by pretrained detectors. Results show that our approach is superior to Selective Search and that detector pretraining significantly improves over our initial proposals, supporting our decision to self-train. We observe also that our framework leads to better localization results than DETReg. Most interestingly, we observe that CutLER performs better in terms of localization than~\ours, even though~\ours consistently outperforms CutLER in terms of object detection pretraining. This reinforces our claim in paper Sec. 2, that unsupervised localization methods generate annotations and follow training processes that are not necessarily good for detector pretraining.

In~\cref{tab:abl_proposals_tr} we find that using Selective Search proposals, \ours still outperforms the MoBY baseline, but we observe a performance drop relative to our object proposal method. We attribute this to two reasons: a) our method likely produces more discriminative descriptors $f$ by aggregating representations over a mask of semantically related pixels, rather than over a box, which is the case for Selective Search. This, in turn, leads to better pseudo-labels. b) Our proposals are more robust (see~\cref{tab:rpm}), and therefore provide better supervision. In summary, we conclude that \ours is robust to different object proposal methods, but greatly benefits from an appropriate method choice.

\paragraph{Number of classes:} We ablate the number of pseudo-classes produced by the global clustering of object proposals. For this set of experiments, we pretrain and finetune on COCO {\tt train2017} for 25 epochs each. Note this is a simplified (and cheaper) setting for the purpose of ablating.
We find that, during pretraining, increasing the number of clusters/pseudo-labels leads to decreased training accuracy (ACC) and class-unaware AR (measured on the validation set), which is expected, since increasing the number of classes makes the task harder. However, the AP score after finetuning increases, indicating that the pretrained detector is more powerful. Overall, results indicate that our method is fairly robust to the number of pseudo-labels chosen. We do not increase the number of classes beyond 2048 as training becomes less stable and more computationally expensive.

\paragraph{Self-training stages:} We examine the impact of self-training in \cref{tab:abl_stages}, and find that it produces meaningful gains. We explore additional self-training with ViDT+~\cite{vidt}, but observe no benefits, and therefore limit self-training to one round throughout the paper.


\paragraph{Schedule length:} In~\cref{tab:epochs} we examine the impact of a longer training schedule on our method for both training stages by extending training from 10 to 25 epochs per stage. The results show that a longer training schedule can have some benefits, albeit marginal. Interestingly, \cref{tab:epochs} highlights the importance of self-training, as two training stages totaling a combined 20 epochs (10 per stage) clearly outperform a single training round of 25 epochs.

\paragraph{Time \& VRAM requirements:} We present in~\cref{tab:runtime} runtimes and VRAM usage for \ours and DETReg~
\cite{bar2022detreg}, for pretraining on ImageNet using a VIDT+ detector. We note here that DETReg is among the most efficient methods in the relevant literature~\cite{huang2023siamese}. As seen in~\cref{tab:runtime}, \ours requires slightly more GPU memory than DETReg, which is to be expected given that \ours trains the backbone as well as the detector. However, despite training more parameters and using more proposals \ours has the same training time as DETReg. This demonstrates the increased training efficiency of \ours's class-aware detection framework compared with the contrastive student-teacher pipelines utilized by most previous works on this subject~\cite{li2023aligndet,bar2022detreg,huang2023siamese,joindet}.

Regarding the proposal extraction processes, extracting the initial proposals $\mathcal{T}_0$ is a CPU-intensive process that, in our hardware, requires 24 hours for ImageNet. 
The subsequent proposals $\mathcal{T}_1$ used for self-training are acquired through straightforward inference with the pretrained detector, and require approximately 2 hours to extract.

\begin{table}[t]
  \begin{center}
  \footnotesize{
  \begin{tabular}{ c c c}
    \toprule
      & DETReg & AptDet \\
    \midrule
    Training time (h) & 164 & 165 \\
    Avg. proposals per img. & 28 & 36 / 53 \\
    Memory usage per GPU (GB) & 55.6 & 73.6 \\
    \bottomrule
  \end{tabular}
  }
  \end{center}
    \caption{Runtimes on ImageNet with ViDT+ and 8 V100 GPUs. For \ours, we present the avg. proposals per image for both stages (stage 1/stage 2).} \label{tab:runtime}
\end{table}

\begin{table}[t]
    \caption{\textbf{Self-training rounds.} AP results for ViDT+ pretrained with \ours on ImageNet and finetuned on COCO. Avg. proposals per image are measured during training.}
  \label{tab:abl_stages}
  \begin{center}
  \begin{footnotesize}
  \begin{tabular}{ c c c c c }
    \toprule
     Detector & Stage & AP & AP$_{50}$ & AP$_{75}$ \\
     \midrule
      \multirow{3}{*}{ViDT+~\cite{vidt}} & 1 & 48.9 & 67.4 & 52.9 \\
      & \textbf{2} & \textbf{49.6} & \textbf{68.2} & 53.8 \\
      & 3 & 49.6 & 68.0 & \textbf{53.9} \\
      \midrule
      \multirow{2}{*}{Def. DETR~\cite{def_detr}} & 1 & 46.1 & 64.6 & 50.3 \\
      & \textbf{2} & \textbf{46.7} & \textbf{65.4} & \textbf{50.9} \\
    \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
\end{table}


\begin{table}[t]
\parbox[b]{.45\linewidth}{
  \begin{center}
      \caption{\textbf{Scheduler length.} AP results for varying training epochs. 10 and 25 epoch Stage 2 models are initialized from 10 and 25 epoch Stage 1 models respectively.}
  \label{tab:epochs}
  \begin{footnotesize}
  \begin{tabular}{ c c c c c }
    \toprule
     Stage & Epochs & AP & AP$_{50}$ & AP$_{75}$ \\
     \midrule
      1 & 10 & 48.9 & 67.4 & 52.9 \\
      1 & 25 & 49.2 & 67.7 & 53.6 \\
      \midrule
      2 & 10 & 49.6 & 68.2 & 53.8 \\
      2 & 25 & 49.7 & 68.1 & 54.2 \\
    \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
  }
\hfill
\parbox[b]{.45\linewidth}{
  \begin{center}
      \caption{\textbf{Data augmentations.} \ours's performance with/without mosaic transforms, pretrained on ImageNet and evaluated on MS COCO.}
  \label{tab:abl_mosaic}
  \begin{footnotesize}
  \begin{tabular}{ c c c}
    \toprule
     Method & Augmentations & AP \\
     \midrule
      CutLER~\cite{wang2023cut} & Copy-paste~\cite{dwibedi2017cut} & 44.7 \\
      \ours & \xmark & 44.8 \\
      \ours & Mosaic~\cite{bochkovskiy2020yolov4} & \textbf{45.0} \\
    \bottomrule
  \end{tabular}
  \end{footnotesize}
  \end{center}
    }
\end{table}

\paragraph{Impact of augmentations:} Unlike most previous works, \ours does not use contrastive learning. Therefore we do not use the color and cropping augmentations that are standard in contrastive learning. Since our task is better aligned with detection, we use mosaic~\cite{bochkovskiy2020yolov4} transforms, a standard object detection augmentation. We evaluate the impact of this choice in~\cref{tab:abl_mosaic} with a Cascade Mask R-CNN detector. While using augmentations helps, we observe that even \textit{without augmentations}, \ours achieves state-of-the-art performance. These findings provide strong evidence that the core performance differentiator is our framework.

\newpage

\section{Visualization} \label{sec:viz}

  % Figure environment removed

In~\cref{fig:viz} we provide examples visual examples of bounding boxes produced by Selective Search, our pseudo-labeled object proposal method, and \ours, specifically a ViDT+ detector trained for two stages on ImageNet. To avoid clutter, for all three methods we only include objects whose predicted bounding boxes have an IOU higher than 0.5 with an object in the ground truth set.

The images illustrate that self-training significantly improves the object discovery performance of \ours over the original region proposals. Notably, those include much smaller items, and much better performance in cluttered scenes. As stated in the main paper, this contributes to the performance of our framework and specifically the performance gains between stages.