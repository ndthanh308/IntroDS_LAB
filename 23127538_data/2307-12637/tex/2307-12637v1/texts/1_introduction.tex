
\section{Introduction}

%3D object detection using LiDAR point clouds is a fundamental perception task in autonomous driving and has received significant attention in recent years.
%Similar to the Faster R-CNN method \cite{fasterrcnn_ren} in the 2D domain, two-stage detectors \cite{pointrcnn_shi,pvrcnn_shi, lidarrcnn_li, voxelrcnn_deng, ct3d_sheng, pdv_hu, pcrgnn_zhang, sienet_li} have also proven highly effective in this problem.
%These R-CNN variants consist of two stages: 1) region proposal, where the network encodes the scene and generates initial bounding box proposals, and 2) proposal refinement, where features from the regions of interest (RoIs) are utilized to refine the proposals.

%Early works on LiDAR-based 3D object detection \cite{pointnet_qi, pointnet++_qi, 3dssd_yang, voxelnet_zhou, second_yan} have mainly focused on the first stage, exploring different backbones for scene representation.
%Point-based methods \cite{pointrcnn_shi, lidarrcnn_li, std_ma} use PointNet series \cite{pointnet_qi, pointnet++_qi} backbone to extract point-level features directly from the raw point cloud.
%On the other hand, voxel-based methods \cite{, second_yan} divide the point cloud into a grid of voxels and use convolutional networks for feature extraction, sacrificing some fine-grained point information in the process.
%Despite this, voxel-based approaches are widely adopted in recent works due to their high efficiency and accuracy.


%Meanwhile, the investigation for an effective RoI feature extraction method in the proposal refinement stage continued.
%A widely popular technique is to grid-pool scene representations aggregated at voxel centers or sparse keypoints \cite{voxelrcnn_deng, pvrcnn_shi}.
%This technique may work well in regularly structured image data, but is not well-suited for LiDAR point clouds, which are sparse and unevenly distributed. 
%More recent works \cite{ct3d_sheng, graphrcnn_yang, pdv_hu} argue to take into account the precise location and density of internal points, leveraging different architectures to exploit point information within the RoIs.
%However, RoI points are often sparse and incomplete due to the inherent nature of LiDAR data.
%This led to a line of work that involves point completion \cite{sienet_li, pcrgnn_zhang}, aiming to add more points to RoIs. 
%These approaches use a pre-trained point completion network to enhance the resolution of point clouds, but they do not consider the semantic information available from the region proposal stage during the completion process.

3D object detection using LiDAR point clouds is a fundamental perception task in autonomous driving that has received significant attention in recent years.
LiDAR sensors are frequently used in many 3D applications, such as odometry and mapping \cite{loam_zhang, floam_wang, legoloam_shan}, object tracking \cite{pointtracknet_wang, tpn_wu, 3dmot_wu}, and detection \cite{second_yan, pvrcnn_shi, voxelrcnn_deng}, due to their ability to provide accurate distance information in various conditions.

LiDAR-based 3D object detectors use either a point-based \cite{pointnet_qi, pointnet++_qi, pointgnn_shi, pointformer_pan} or a voxel-based \cite{voxelnet_zhou, second_yan, votr_mao} network to generate bounding boxes for foreground objects. 
%Voxel-based methods are generally preferred due to their efficiency and effectiveness, despite sacrificing some fine-grained point information.
The two-stage framework with a proposal refinement stage is often adopted in many detectors to enhance the detection accuracy \cite{pointrcnn_shi, graphrcnn_yang}.
While researchers have explored different methods \cite{pvrcnn_shi, voxelrcnn_deng} to extract effective refinement features for the regions of interest (RoIs), some of the most recent works with voxel-based backbones \cite{ct3d_sheng, pdv_hu} revisit the point information within the RoIs at the refinement stage, considering the precise coordinates and density of internal points.

% Figure environment removed

Nevertheless, the inherent sparsity of LiDAR point clouds poses a challenge in 3D object detection, particularly for distant and occluded objects.
These objects have fewer collected points, making them difficult to detect and degrading the overall performance of detectors \cite{sienet_li, btc_xu}.
To address this problem, point cloud completion methods have been explored to assist proposal refinement by adding more points to the RoIs.
% The methods in \cite{sienet_li, pcrgnn_zhang} utilize a point cloud completion network pre-trained from an external dataset \cite{shapenet} to enhance the resolution of point clouds, but they only take point coordinates pooled from the proposal bounding box into account during the generation process.
The methods in \cite{sienet_li, pcrgnn_zhang} enhance the resolution of point clouds by utilizing a point cloud completion network pre-trained from an external dataset \cite{shapenet}, but they only take point coordinates pooled from the proposal bounding box into account during the generation process.
As a result, they fail to capture the contextual information of the surroundings and indiscriminately produce dense point clouds for all proposals, including incorrect proposals.

Motivated by this, we propose the Point Generation R-CNN (PG-RCNN), an end-to-end two-stage 3D object detection method that can extract geometrically and semantically rich proposal refinement features via semantic surface point generation.
Our method includes the RoI point generation (RPG) module that estimates the actual shape and displacement of foreground objects, using primitive RoI features aggregated from the backbone as input.
Note that we jointly train the RPG module using auxiliary supervision from given data without introducing any external dataset.
While previous point cloud completion methods only output sets of spatial coordinates, our method goes beyond that by assigning a semantic feature to each generated point, which represents its estimated probability of belonging to the foreground.
%Point completion networks in previous methods only produce a set of spatial coordinates.
%Instead, our method assigns a semantic feature for each generated point, indicating its estimated foreground probability.
%Moreover, our RoI point generation (RPG) module produces point clouds using primitive RoI features aggregated from the backbone as input.
%The purpose of the RPG module is not just to generate plausible spatial point clouds of the objects, but to estimate the actual shape and displacement of foreground objects.
%We jointly train the RPG module using auxiliary supervision from given data without introducing any external dataset. 
%We use auxiliary supervision available from the detection dataset that encourages the  module not just to generate a spatial point cloud of an object, but to detect and estimate the shape and displacement of foreground objects.
These characteristics allow our novel point generation method to produce more informative point clouds for object detection.
Figure \ref{fig:1_overview} shows the intermediate outputs of our method.
PG-RCNN generates points with different foreground scores, presenting high-confidence foreground points for true positive proposals.
The generation points intuitively express the predicted location and shape of the objects, visualizing the reasoning process of PG-RCNN.
%Also note that our RoI point generation module is jointly trained with the rest of the detector, 
%For each region proposal, PG-RCNN first attains primitive RoI features by grid-pooling voxel features of the backbone network.
%Then, our jointly trained point generation module process grid point features with a Transformer \cite{transformer_vaswani} encoder and outputs surface point coordinates, estimating the complete shape and displacement of the object. 
%Moreover, our RoI point generation module also produces a semantic feature for each point that indicates its foreground probability. 
%The generated point clouds are processed with a lightweight PointNet++ \cite{pointnet++_qi} encoder and produce rich features for subsequent proposal refinement.

We demonstrate the effectiveness of our method with extensive experiments on the KITTI dataset \cite{kitti}. 
PG-RCNN achieves competitive performance with state-of-the-art models while significantly reducing the computational cost.
%In particular, our model shows a similar inference speed with first-stage detectors \cite{second_yan, pointpillars_lang}  
Qualitative results show that our approach better serves the purpose of refining false-positive or misaligned proposals compared to previous point cloud completion methods.

In summary, our main contributions are:
\begin{itemize}
    \item We present PG-RCNN, a novel two-stage 3D object detection method for LiDAR point clouds.
    In the proposal refinement stage, our method generates semantic surface points with foreground probabilities to extract shape-aware, semantically rich refinement features.
    % In the proposal refinement stage, PG-RCNN tackles the challenging problem of foreground point generation to achieve a rich shape-aware refinement feature.
    \item We compare the point generation results of PG-RCNN to a previous point cloud completion approach and show that our method generates more effective points for object detection.
    %We analyze the point generation results of PG-RCNN and their role in proposal refinement in comparison to a previous point cloud completion approach.
    %As a result, it is shown that our method generates points that are substantially more helpful for object detection.
    %As a result, it is shown that our model generates points that are substantially helpful for proposal refinement.
    % \item To the best of our knowledge, PG-RCNN is the first work to generate object point clouds with foreground probabilities. We demonstrate the advantage of our approach compared to the previous point cloud completion method with extensive experiments.
    \item PG-RCNN achieves competitive performance on the KITTI benchmark, with a significantly smaller number of parameters and inference time than the state-of-the-art models. 
\end{itemize}