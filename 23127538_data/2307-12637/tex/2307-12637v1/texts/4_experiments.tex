\section{Experiments}

In this section, we conduct a comprehensive analysis on the KITTI dataset \cite{kitti} to verify the effectiveness of PG-RCNN and its components.
In Sec\onedot \ref{sec:4.2}, we evaluate PG-RCNN on the competitive benchmark and compare the performance with the state-of-the-art methods.
Furthermore, we qualitatively compare our point generation results with a prior point cloud completion approach \cite{sienet_li} in Sec\onedot \ref{sec:4.3}. Extensive ablation studies in Sec\onedot \ref{sec:4.4} validate our design. 
We also conducted experiments on the Waymo Open Dataset \cite{waymo}, another popular autonomous driving dataset.
Please refer to the supplementary materials for  experiments on Waymo Open Dataset. 

\subsection{Experimental Setup}\label{sec:4.1}
\paragraph{KITTI Dataset.}
The KITTI dataset provides 7,481 annotated training samples and 7,518 testing samples.
Following \cite{kitti_split_chen}, we split the original training data into 3,712 and 3,769 samples for training and validation, respectively.
We detect three object classes: cars, cyclists, and pedestrians.

%To obtain the target point cloud of each object, we use the approximation method proposed in \cite{btc_xu}.
%We first search for other objects of the same class that have similar bounding boxes and point distributions.
%Then we combine the point sets of two best-matching objects with the original points and produce a dense point cloud.
%For cars and cyclists, we assume symmetry along the object's heading axis and mirror the points accordingly.

%\paragraph{Waymo Open Dataset.} %The Waymo Open Dataset is a large-scale autonomous driving dataset containing 798 training sequences (around 158k point cloud samples) and 202 validation sequences (around 40k point cloud samples).To obtain the generation target, we use the same object appearing in other frames in the sequence to augment the point cloud.Here we also mirror the vehicles and cyclists along the object's heading axis. 

%\subsection{Implementation Details}
\paragraph{Network Architecture.}
We limit the detection range as [0m, 70.4m] for the X-axis, [-40m, 40m] for the Y-axis, and [-3m, 1m] for the Z-axis.
To process this data, the raw point clouds are divided into voxels of size (0.05m, 0.05m, 0.1m) along each axis.
% For the Waymo Open Dataset, the detection range is [-75.2m, 75.2m], [-75.2m, 75.2m], and [-2m, 4m] for the X, Y, and Z-axis respectively, and a voxel size of (0.1m, 0.1m, 0.15m) is used.
The feature dimensions of the 3D backbone are (16, 32, 48, 64) across four stages, while the proposal layer's feature dimensions are (64, 128).
%for the KITTI and (128, 256) for the Waymo dataset.
In the RoI grid pooling step, the dimension of each grid's feature $\mathbf{f}_{\mathbf{g}_{i}}$ is set to 96 with a grid size $G$ of 6.
% In the RoI grid pooling step, the dimension of each grid's feature $\mathbf{f}_{\mathbf{g}_{i}}$ is set to 96 and 192 for the KITTI and Waymo datasets, respectively, with a grid size $G$ of 6.
We use a single-layer Transformer encoder with a hidden feature dimension of 384.
The semantic feature vector $\mathbf{f}^{se}_{\mathbf{p}_{i}}$ and local spatial feature vector $\mathbf{f}^{sp}_{\mathbf{p}_{i}}$ of generated points have 32 and 64 dimensions, respectively.
For each proposal, the point cloud encoder in the detection head extracts an RoI feature vector $\mathbf{f}^{r}$ of dimension 256.
Overall, PG-RCNN use lighter MLP layers than the motivational works \cite{second_yan, voxelrcnn_deng, ct3d_sheng, pointrcnn_shi}, allowing our model to be significantly more efficient than the previous methods (please refer to Table \ref{table:kitti_val}).

\input{texts/table_kitti_val.tex}
\input{texts/table2.tex}
\paragraph{Training Details.}
For data augmentation, we apply widely employed strategies, including random flipping along the X-axis, global scaling, global rotation around the Z-axis, and ground truth sampling.
Please refer to OpenPCDet \cite{openpcdet} for detailed configurations since we used the toolbox for all our experiments.
PG-RCNN is trained using the Adam optimizer \cite{adam_kingma} with a one-cycle policy for 80 epochs with an initial learning rate of 0.01.
We used 4 NVIDIA RTX 3090 GPUs to train our network with a batch size of 16, and the training time was less than 5 hours.
% PG-RCNN is trained using the ADAM optimizer with a one-cycle policy for 80 epochs with an initial learning rate of 0.01 on the KITTI dataset, and for 30 epochs with an initial learning rate of 0.001 on the Waymo dataset.
$N_p$, the number of points used to calculate $\mathcal{L}_{score}$, is set to 2,048.
% The network is trained with a batch size of 16 on 4 NVIDIA RTX 3090 GPUs for the KITTI, and 1 NVIDIA A100 GPU for waymo.


\subsection{Comparison with State-of-the-Arts}\label{sec:4.2}

We trained our model on the \textit{train} set and tuned the hyperparameters based on the \textit{val} set evaluation results.
To submit the detection results on KITTI official test server, we trained the model using all annotated \textit{train}+\textit{val} samples.
All results are evaluated by the mean average precision (AP) calculated with 40 recall positions (R40), using IoU thresholds of 0.7 for cars, and 0.5 for pedestrians and cyclists.
The evaluation results are reported on three levels of difficulties: easy, moderate, and hard.

% Figure environment removed

Table \ref{table:kitti_val} summarizes the performance comparison of PG-RCNN on KITTI \textit{val} set with the state-of-the-art models that officially released the trained weights.
PG-RCNN shows the best or second-best performance for all classes and difficulties, except for the car class on a hard difficulty, where we achieved the third-best performance.
The previous point cloud completion approach, SIENet \cite{sienet_li} surpasses our model on car class for moderate and hard difficulties.
 % We believe this is due to the advanced region proposal network, and thus we re-implemented SIENet with our RPN to compare the effect of point generation on the refinement stage.
We believe this is due to its advanced region proposal network, and we re-implemented SIENet with general RPN from SECOND \cite{second_yan} as our model to fairly compare the effect of point generation on the refinement stage.
In this case, it can be observed that PG-RCNN outperforms the model (denoted as SIENet$^\dag$ in Table \ref{table:kitti_val}) at all levels of the car class, implying that our refinement method is more effective.
% We believe this is due to the advanced region proposal network, and thus we re-implemented SIENet with our RPN to compare the effect of point generation on the refinement stage.
% We denote the re-implemented version as SIENet$^\ddag$ on Table \ref{table:kitti_val}, and we can observe that our model outperforms SIENet in that case.
Moreover, PG-RCNN exhibits remarkably superior efficiency compared to recent methods.
Notably, our model has over 9 times fewer parameters in to previous point cloud completion approaches.
PG-RCNN also has a low inference time demand, comparable to a single-stage detector \cite{second_yan}.

PG-RCNN also obtains competitive detection performance on the KITTI \textit{test} set, as summarized in Table \ref{table:kitti_test}.
We ranked second or third place on the 3D detection results except on the easy level for the car class.
In comparison to previous point cloud completion approaches \cite{pcrgnn_zhang, sienet_li}, PG-RCNN consistently outperforms the competitors on 3D detection performances for cars on all levels.
Although our method's performance falls short on certain metrics when compared to the most recent publications, PG-RCNN still exhibits a remarkable trade-off in terms of high efficiency. 
However, we hypothesize that our model's lack of scalability in \textit{test} set evaluation results from its lightweight nature. 
In our future work, we plan to explore the use of a more sophisticated detection head to improve detection performance on larger datasets.
%The models that surpass our car-easy performance \cite{pvrcnn_shi, voxelrcnn_deng, btc_xu, pdv_hu} have one thing in common; they extract RoI features at grid points and concatenate them to get the refinement feature.
%We hypothesize that with abundant information, flattening grid features is particularly powerful for detecting car objects because it preserves dense local features.
%However, the operation would produce a refinement feature of great size (20736D vector in case of \cite{voxelrcnn_deng}), whereas our method introduces a 256-dimensional vector as a refinement feature.
%However, flattening grid features cost an overwhelming number of parameters, sacrificing efficiency to a far extent.
%Considering our high efficiency, our method shows a remarkable trade-off.
%In comparison to previous point completion approaches \cite{pcrgnn_zhang, sienet_li}, PG-RCNN shows the highest 3D AP$_{R40}$ values except for cyclist easy level which shows that semantic surface points generated by our method are particularly effective for object detection.
% PG-RCNN has advantages over these two models, in that, unlike them, it does not require auxiliary networks and end-to-end learning is possible.
%Considering the superior efficiency of PG-RCNN in comparison to other contemporary state-of-the-art models, the obtained results are particularly impressive.



\subsection{Analysis on Point Generation Results}\label{sec:4.3}
Here, we compare the qualitative results of the proposed method on KITTI \textit{val} data with a previous point cloud completion method, SIENet \cite{sienet_li}.
%SIENet uses a pre-trained spatial shape prediction network to generate a set of point coordinates of complete object shape, given the existing points within the RoI as input. 
%SIENet then refines its initial proposals by fusing spatial information acquired from generated point cloud to pooled voxel features.
To fully focus on the effect of point generation in the refinement stage, we compare our model with SIENet$^\dag$ we presented in Table \ref{table:kitti_val}.%, where we replaced the first stage detector with RPN of ours.
%The model used in this comparison is the same as the one presented in Table \ref{table:kitti_val}.

% Figure environment removed
Figure \ref{fig:4} illustrates some of the point generation and detection results of SIENet and PG-RCNN.
The foreground score of each point is expressed with its opacity in the figure.
Since the point cloud completion network of SIENet only produces spatial coordinates, we set the foreground score of all its generated points to 1.
The top two rows of Fig\onedot \ref{fig:4} display the outputs in a bird's-eye-view.
Observations show that SIENet indiscriminately generates point clouds for all region proposals, and results more false positive predictions than ours.
In contrast, our method presents high-confidence foreground points only at true positive bounding boxes.
This suggests that considering foreground probabilities of generated points can effectively regularize producing false positive detection results.
The third row of Fig\onedot \ref{fig:4} exhibits the projections of the outputs of our model onto the images, showing that the generated points are well-aligned with the foreground objects.
The results suggest that PG-RCNN can not only detect objects but also successfully estimate their actual shape.
% Moreover, our method provides better explainability.
%3D object detection is a task with the objective to predict the location, size, and orientation of objects. PG-RCNN aims for a more challenging task of even predicting the actual shape of the objects, by introducing point generation.
%Also, when we project the generated points to the image, we were able to observe that the points lie nicely on the foreground objects.
% Our detector predicts not only the location, size, and heading orientation of an object, but also its actual shape.
% We can better understand the detector's decision process and easily figure out important hyperparameters (\eg, detection threshold) or the causes of failure cases.




To further investigate the effectiveness of our point generation method, we compare how the point cloud completion network of SIENet and our RPG module behave in the same situation.
We artificially composed misaligned proposals by slightly distorting ground truth bounding boxes, and provided them to both models.
Figure \ref{fig:5} illustrates some of the refinement stage outputs of SIENet and ours.
Although SIENet creates a denser point cloud than ours, it does not align with existing foreground points nor get outside of the proposal bounding box.
In the top example of Fig\onedot \ref{fig:5}, SIENet refined the proposal towards the ground truth bounding box.
However, this prediction did not align with the point generation result.
Moreover, in the bottom example, SIENet was unable to make a confident final prediction with the generated point cloud.
This indicates the generated points are \textit{pointless} for proposal refinement.
On the contrary, points generated with our method actively move outside the initial proposal, attempting to capture the actual foreground object surface.
The generated points mostly fit within the ground truth bounding box, and the final detection bounding boxes are predicted accordingly.
The intuitive comparisons show that our point generation results better serve a purpose for addressing misaligned proposals.

%We also want to note that our method provides an excellent explainability of the detector's decision process.
%Point generation serves as an intermediate stage of detection that moderates incorrect proposals and express the estimated shape of objects.


%\input{texts/table3.tex}

% \subsection{Evaluation on the Waymo Open Dataset}
% We also validate the effectiveness of PG-RCNN on the Waymo Open Dataset. 
% The model trained on the training set is evaluated on the validation set using mean average precision (mAP) and mean average precision weighted by heading (mAPH), using IoU thresholds of 0.7 for Vehicles and 0.5 for Pedestrians and Cyclists.
% Objects are categorized into two groups based on the number of points within their bounding boxes: Level\_1 includes more than five points, while Level\_2 includes fewer than five points.

% \paragraph{Performance Comparisons.}





%Ablation

%\begin{table*}[ht!]
 % \caption{Comparison with state-of-the-art methods on the KITTI validation set. The results are evaluated by the 3D Average Precision (AP) under 40 recall thresholds (R40).}
 % \centering
 % \label{table:1}
 % \begin{tabular}{l|c|c c c|c c c|c c c}
 %   \hline
 %   \multirow{2}{*}{Method} &
 %   Param.&
 %   \multicolumn{3}{c|}{Car 3D AP_{R40}} &
 %   \multicolumn{3}{c|}{Ped. 3D AP_{R40}} &
 %   \multicolumn{3}{c}{Cyc. 3D AP_{R40}} \\
 %   % \cline{2-10}
 %   &(M)& Easy & Mod. & Hard & Easy & Mod. & Hard & Easy & Mod. & Hard \\
 %   \hline
 %   SECOND \cite{second_yan}&20& 90.55 & 81.61 & 78.56 & 55.94 & 51.15 & 46.17 & 82.97 & 66.74 & 62.78 \\ % OpenPCDet
 %   PointPillars \cite{pointpillars_lang}&18& 87.75 & 78.41 & 75.19 &  57.30 & 51.42 & 46.87 & 81.57 & 62.93 & 58.98 \\ % OpenPCDet
 %   PointRCNN \cite{pointrcnn_shi}&16& 91.39 & 80.53 & 78.05 & 62.41 & 55.70 & 49.01 & 92.56 & 73.13 & 68.81\\ % OpenPCDet
 %   PartA$^2$ \cite{parta2_shi} &226& 91.66 & 80.29 & 78.08 & 72.32  & 66.37 & 60.07 & 91.89 & 75.33 & 70.68 \\ % OpenPCDet
 %   PV-RCNN \cite{pvrcnn_shi}&50& 92.11&84.39&82.50	&62.69&54.51&49.84&89.10&70.38&66.02 \\
%    Voxel-RCNN \cite{voxelrcnn_deng}&28& 91.72 & 83.19 & 78.60 &$-$&$-$&$-$&$-$&$-$&$-$\\
%    CT3D \cite{ct3d_sheng}& 92.85 & 85.82 & 83.46 & 65.73 & 58.56 & 53.04 & 91.99 & 71.60 & 67.34 \\
%    CT3D \cite{ct3d_sheng}&30& 92.34 & 84.97 & 82.91 & 61.05 & 55.57 & 51.10 & 89.01 & 71.88 & 67.91 \\
%    PDV \cite{pdv_hu}&147&  & 85.05 & & & 57.41 & & & 75.95 & \\
    % BtcDet$^{\dag}$ \cite{btc_xu}& 93.15 & 86.28 & 83.86 & 69.39 & 61.19 & 55.86 &91.45 & 74.70 & 70.08 \\
    % BtcDet$^{\dag}$ \cite{btc_xu}&&&&&&&& \\
 %   \hline
%    PG-RCNN (Ours) &26&92.42&85.09&82.65&68.87&60.88&55.67&95.22&76.05&72.16\\
%    \hline
%\end{tabular}
%\end{table*}


\subsection{Ablation Studies}
\label{sec:4.4}
To verify the effectiveness of the proposed method, we conduct extensive ablation studies on the KITTI \textit{val} set.
\vspace{-2ex}
\paragraph{Point Generation Loss.} 
Our RPG module is trained with the supervision of two losses, $\mathcal{L}_{score}$ and $\mathcal{L}_{offset}$, which assign semantic and geometric attributes to the generated points, respectively.
To investigate the impact of these supervisions on object detection performance, we ablated each loss term and compared the 3D detection performances on car objects.
In the absence of $\mathcal{L}_{score}$, a uniform foreground score of $s_i = 1$ is allocated for all generated points.
Similarly, a fixed offset $\mathbf{o}_i = (0,0,0)$ is used for the absence of $\mathcal{L}_{offset}$, indicating that all generated points were located at the grid centers.
Table \ref{table:ablation1} summarizes the results of the experiments.
Our model showed a consistent performance drop when we did not employ $\mathcal{L}_{score}$, resulting in a decline of 0.82\% in mAP.
This reveals that assigning semantic information to generated points is an important feature of our method.
Similarly, when $L_{offset}$ was not utilized, the mAP decreased by 0.61\%. This result demonstrates the necessity of spatial supervision, which provides a beneficial trait of shape-awareness for refinement.
%Likewise, when comparing the $2^{nd}$ and $3^{rd}$ rows, a notable improvement in performance is observed in the presence of $\mathcal{L}_{offset}$, signifying the crucial role of object surface prediction in enhancing detection performance.

\vspace{-2ex}
\paragraph{RoI Point Generation Module.}
In this ablation study, we justify the decision choices regarding the components of the RPG module. Here we compare the 3D detection performances of three classes on the moderate level.
First, we examined the performance gain brought by the Transformer \cite{transformer_vaswani} encoder.
Second, we verified the method for deriving generate points' coordinates, comparing the case of using the RoI center and grid points as the reference point for predicting offsets.
Table \ref{table:ablation2} summarizes the results of the experiment.
The use of the Transformer encoder resulted in a gain of over 1\% in AP for all classes, highlighting the advantage of accessing RoI-level contextual information by employing this component.
The results demonstrate that using grid points as the offset center can significantly improve detection performance for all classes, as compared to using the RoI center as the offset center.

\input{texts/ablation1.tex}
\input{texts/ablation2.tex}
%In Table \ref{table:ablation2}, we investigated the effect of our proposed methods for the RPG module on both point generation and object detection performance.
%We employed the AUROC (Area Under the ROC Curve) metric to evaluate the point generation performance of the RPG module.
% The ROC curve plots the true positive rate against the false positive rate for varying threshold values between 0 and 1.
% AUROC measures the area under the curve and its value ranges from 0 to 1.
%In this experiment, a higher AUROC value of the generated points indicates better point generation performance, as it implies that a larger number of confident points were created within the true positive bounding boxes.
%The $1^{st}$ and the $3^{rd}$ row of Table \ref{table:ablation2} show that a model incorporating the Transformer encoder enables more accurate point generation, thereby enhancing the object detection performance for all classes compared to the model without it.
%Furthermore, an additional experiment was conducted to determine whether the coordinates of the generated points are derived as an offset from the grid points or from the center of the proposals.
%The results showed that utilizing the offset from the grid point yielded better performance in both point generation and object detection, as observed in the $2^{nd}$ and $3^{rd}$ row.


