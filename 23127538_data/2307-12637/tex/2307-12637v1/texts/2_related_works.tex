
\section{Related works}

\subsection{LiDAR-Based 3D Object Detection}

LiDAR-based 3D object detection methods can be categorized into two streams: point-based and voxel-based.
% LiDAR-based 3D object detection methods can be broadly categorized into two streams: point-based and voxel-based.
%Point-based methods \cite{3dssd_yang, pointrcnn_shi, lidarrcnn_li, std_ma, pointgnn_shi, pointformer_pan} directly learn point features for detection by sampling raw point clouds and employing permutation-invariant operations.
Point-based methods directly learn point features for detection by sampling raw point clouds and employing permutation-invariant operations.
The majority of point-based methods \cite{3dssd_yang, pointrcnn_shi, lidarrcnn_li, std_yang} use PointNet-like backbones \cite{pointnet_qi, pointnet++_qi}, while methods like \cite{pointgnn_shi, pointformer_pan} adopt other architectures to process the sampled points.
%Point-based methods can retain fine-grained point information, but the sampling and grouping operations are computationally expensive and cause inference time bottlenecks.

On the contrary, voxel-based detectors \cite{voxelnet_zhou, second_yan, votr_mao, voxelrcnn_deng} convert point clouds into regular 3D voxels and extract features with convolution operations.
VoxelNet \cite{voxelnet_zhou} first proposed a voxel feature encoding method for point clouds, and SECOND \cite{second_yan} reduced computational cost by introducing efficient sparse convolution \cite{spconv_graham}.
Voxel R-CNN \cite{voxelrcnn_deng} is a typical two-stage detector that takes advantage of voxel representations in the proposal refinement stage via voxel RoI pooling.

To mitigate information loss due to data quantization, voxel-based approaches are often combined with point-level supervision and representations.
Part-A$^{2}$ Net \cite{parta2_shi} and SA-SSD \cite{sassd_he} exhibited remarkable performance using a point-level auxiliary task.
PV-RCNN \cite{pvrcnn_shi} aggregates voxel features at a set of keypoints obtained from the scene using the farthest point sampling (FPS) algorithm, and exploits the keypoint features during the proposal refinement.
Some methods use the points within proposal bounding boxes instead of sampling the points from the entire scene.
%For example, CT3D \cite{ct3d_sheng} utilizes a voxel-based backbone but relies solely on the raw point coordinates within the proposal during refinement.
For example, CT3D \cite{ct3d_sheng} utilizes a voxel-based backbone, but for refining a proposal, it relies solely on the raw point coordinates within the proposal.
% For instance, CT3D \cite{ct3d_sheng} uses a voxel-based backbone for region proposal, but encodes the refinement feature using raw point coordinates inside the RoI.
Others \cite{fastpointrcnn_chen, pdv_hu} exploit internal point information alongside the voxel features for RoI feature pooling.
However, the limited number of collected points for distant or occluded objects still poses a challenge to detection performance.


% Figure environment removed

\subsection{Point Generation for 3D Object Detection}

Several recent works have aimed to overcome the sparsity and incompleteness of LiDAR points by augmenting additional points to the scene.
For example, \cite{mvp_yin, sfd_wu, vpfnet_zhu} incorporate RGB images to generate dense pseudo-LiDAR and virtual points.
In terms of single-modal approaches, PC-RGNN \cite{pcrgnn_zhang} and SIENet \cite{sienet_li} generate dense point clouds of the objects to capture rich spatial information of RoIs.
These methods utilize a pre-trained point cloud completion network that takes raw point coordinates within a initial bounding box proposal as input and outputs a set of point coordinates that form a plausible object shape.
PC-RGNN adds supplementary points to the original points using a GCN-based \cite{gcn_wang} point cloud completion network and encodes their coordinates with a graph neural network.
Meanwhile, SIENet generates dense point clouds with an existing framework called PCN \cite{pcn_yuan}.
Then SIENet extracts features from the generated points using the PointNet++  \cite{pointnet++_qi} encoder and fuses the features with the grid-pooled voxel features to enhance spatial information.
 % In terms of single-modal approaches, PC-RGNN \cite{pcrgnn_zhang} and SIENet \cite{sienet_li} generate dense point clouds of the objects to capture rich spatial information using a GCN \cite{gcn_wang} and PointNet-based point completion network, respectively.
% However, these methods only use raw point coordinates within the initial bounding box proposals as input, and output sets of point coordinates that construct plausible object shapes.
In contrast, our point generation method takes RoI-pooled features as input, capturing the contextual information of the surroundings.
Our approach is also distinguished from SPG \cite{spg_xu}, the unsupervised domain adaptation method that assigns a semantic point to every estimated foreground voxel before feeding into a detector.
SPG does not recover the actual shape of the objects and can be used in parallel with our method as a data pre-processing technique.
% It is not interested in recovering the actual shape of the objects and can be used with our model in parallel.


%One of the main challenges in LiDAR-based 3D object detection is the sparsity of the data. Distant objects or occluded objects have a relatively small number of points, and
%The issue of missing LiDAR points owing to occlusion and signal miss limits the performance of 3D object detection.
%Recently, approaches have been introduced that solve this problem by generating virtual points and using them to refine region proposals.
%SPG \cite{spg_xu} enhanced detection performance by generating semantic points in locations that are likely to belong in the foreground bounding box. This model, however, does not learn the complete shape of objects that provide contextual information.
%In contrast, PC-RGNN \cite{pcrgnn_zhang} and SIENet \cite{sienet_li} generated dense points with the object's complete shape in each RoI employing graph-based and PointNet-based point completion network, respectively. The point completion network produced virtual points by inserting the raw points within the candidate boxes obtained by stage-1.
%However, the points generated by these models have no semantic features except relative coordinate values. In other words, there is no indication that these points actually belong in the foreground. Since this is undesirable for detector performance, we generate semantic points with the complete shape of objects. These semantic points provide information about whether the points are in the foreground.

