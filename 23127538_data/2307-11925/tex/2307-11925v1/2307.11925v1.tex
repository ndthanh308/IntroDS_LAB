\documentclass[12pt]{amsart}

%\usepackage[notref,notcite]{showkeys}
%\usepackage[backref]{hyperref}
\usepackage{fullpage}
\usepackage{amsfonts,amssymb}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage[all]{xy}
\usepackage{tikz-cd}
\usetikzlibrary{matrix}
\usepackage{comment}

%\usepackage{musicography}

%Marcin's commands

\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sp}{\mathbb{S}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\NN}{\mathcal {N}}

\newcommand{\calD}{{\mathcal {D}}}
\newcommand{\calF}{{\mathcal {F}}}
\newcommand{\calB}{{\mathcal {B}}}
\newcommand{\calw}{{\mathcal {W}}}
\newcommand{\calW}{{\mathcal {W}}}
\newcommand{\calG}{{\mathcal {G}}}
\newcommand{\calS}{{\mathcal {S}}}
\newcommand{\calR}{{\mathcal {R}}}
\newcommand{\calH}{{\mathcal {H}}}
\newcommand{\calM}{{\mathcal {M}}}
\newcommand{\calP}{{\mathcal {P}}}
\newcommand{\calC}{{\mathcal {C}}}
\newcommand{\calL}{{\mathcal L}}

\newcommand{\F}{\mathbf F}
\newcommand{\B}{\mathbf B}

\newcommand{\supp}{\operatorname{supp}}
\newcommand{\spa}{\operatorname{span}}
%\newcommand{\span}{\operatorname{span}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\ov}{\overline}
\newcommand{\ch}{\mathbf 1}
\newcommand{\al}{\alpha}
\newcommand{\ga}{\gamma}
\newcommand{\Ga}{\Gamma}
\newcommand{\ve}{\varepsilon}
\newcommand{\vp}{\varphi}
\newcommand{\la}{\lambda}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}
\newcommand{\chip}{{\rho}}


\newcommand{\kk}{{\musSharp{}}}
\newcommand{\bb}{{\musFlat{} }}


\def\Koniec{\hbox to\hsize{\hfil$\diamond$}}
\def\th{\vartheta}
\def\1{\mathbf 1}

% SET UP THE THEOREM ENVIRONMENTS

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{question}[theorem]{Question}
\newtheorem{conjecture}[theorem]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

\numberwithin{equation}{section}

\usepackage[normalem]{ulem}
\usepackage[pagewise]{lineno}%\linenumbers

\definecolor{blue-violet}{rgb}{0.54,0.17,0.89}
\definecolor{amethyst}{rgb}{0.6,0.4,0.8}
\definecolor{darkviolet}{rgb}{0.58, 0.0, 0.83}
\definecolor{darkgreen}{rgb}{0,.4,0}
\definecolor{mixedgreen}{rgb}{0.3,0.6,00}
\definecolor{bananayellow}{rgb}{1.0, 0.88, 0.21}
\definecolor{arylideyellow}{rgb}{0.91, 0.84, 0.42}
\definecolor{bananamania}{rgb}{0.98, 0.91, 0.71}

\newcommand{\REMOVEsk}[1]%
           {{\color{magenta}\sout{#1}}}
\newcommand{\ADDsk}[1]{{\color{blue}{#1}}}
\newcommand{\CHANGEsk}[1]{{\color{cyan}{#1}}}
\newcommand{\REPLACEsk}[2]{\REMOVEsk{#1} {\color{red}{#2}}}
\newcommand{\COMMENTsk}[1]
           {{\color{mixedgreen}\textbf{\textit{SK:}}
              \textit{#1}
             }
           }
%\renewcommand{\REMOVEva}[1]{}
%\renewcommand{\COMMENTva}[1]{}

\allowdisplaybreaks

%\voffset=-3truecm \hoffset=-1.5 truecm
%\linespread{1}
%\addtolength{\headheight}{1.5pt}

%%%        %%%%%%%%%%%%%%%%%      %%%

 \begin{document}

%\title{Mercer Large-Scale Kernel Machines}
\title{Mercer Large-Scale Kernel Machines from Ridge Function Perspective}

\author{Karol Dziedziul}
\address{
Faculty of Applied Mathematics,
The Gda\'nsk University of Technology,
ul. G. Narutowicza 11/12,
80-952 Gda\'nsk, Poland}

\email{karol.dziedziul@pg.edu.pl}

\author{Sergey Kryzhevich}
\address{
Faculty of Applied Mathematics,
The Gda\'nsk University of Technology,
ul. G. Narutowicza 11/12,
80-952 Gda\'nsk, Poland}

\email{sergey.kryzhevich@pg.edu.pl}

\begin{abstract}
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from {\it Fundamentality of ridge functions}. We consider the main theorem of the recent paper by Rachimi and Recht, 2008, {\it Random features for large-scale kernel machines} in terms of the Approximation Theory. We study which kernels can be approximated by a sum of cosine function products with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article may have various applications in Deep Learning, especially in problems related to Image Processing.
\end{abstract}

\maketitle

\begin{quote}
{\small {\bf Mathematics subject classifications:} 42A10, 68T07, 26B40}
\end{quote}

\begin{quote}
{\small {\bf Key words and phrases:}
approximation on compact sets; deep learning; image processing; kernel methods, ridge functions}
\end{quote}

\bigskip

\section{Introduction}

Convolution neural networks play an important role in Deep Learning Theory.  Inspired by \cite{Rahimi}, we know that a continuous kernel ${\mathbf k}(x,y)={\mathbf k}(y,x)=\tilde{{\mathbf k}}(x-y)$, $x,y\in \R^n$, i.e.
{\it a shift-invariant kernel} can be approximated by a feature map, 
\begin{equation}\label{approx}
{\mathbf k}(x,y)\approx \sum_{j=1}^n c_j \cos(\lan w_j,x\ran-\theta_j)\cos(\lan w_j,y\ran-\theta_j),\quad c_j\geq 0.
\end{equation}
where $w_j\in \R^n$, $\theta_j\in \R$, $\lan \cdot, \cdot\ran$ stands for scalar product. Such kernels are examples of Mercer kernels.

\begin{definition} We say that a kernel is \emph{Mercer} if it is continuous, symmetric, and positive definite (see \cite{mercer}).  
\end{definition}

We understand Eq.\, \eqref{approx} in the following sense. Given an $\varepsilon>0$ and a compact set $K\in {\mathbb R}^n$, there exists an $n\in{\mathbb N}$ and parameters $\omega_j\in \R^n$, $c_j>0$ and $\theta_j\in \R$ such that
\begin{equation}\label{rahimi1}
\sup_{x,y\in K} \left| {\mathbf k}(x,y)-\sum_{j=1}^n c_j \cos(\lan w_j,x\ran-\theta_j)\cos(\lan w_j,y \ran-\theta_j)\right|<\varepsilon.
\end{equation}

Precisely, it is demonstrated in \cite{Rahimi}, using Bochner's Theorem, that any continuous shift-invariant basis function kernel can be represented as
\[
{\mathbf k}(x,y)=E (\xi_\omega(x)\xi_\omega(y)), \quad \xi_\omega(x)=\sqrt{2}\cos(\lan\omega,x\ran+b),
\]
where $b$ is uniformly distributed on $[0,2\pi]$ and $\omega$ is distributed according to a probability measure $p$ on $\R^n$. In this case, Claim 2 of \cite{Rahimi} states that the probability of inequality Eq.\, \eqref{rahimi1} holding, tends to one as $n$ goes to infinity. Moreover, the corresponding asymptotic estimate is given.
This theorem gained various responses (see \cite{Belkin} or \cite{2021}). In \cite{2021}, authors propose to examine kernels given by a kind of Bochner's approach. Namely, the kernel ${\mathbf k}$ admits an integral representation:
\[
{\mathbf k} (x, x' ) =
\int_\Xi
\psi(x, \omega)\psi(x', \omega)d\pi(\omega) \qquad \mbox{for all}\qquad x, x'\in X,
\]
where $(\Xi, \pi )$ is a probability space and $\psi \in {\mathbb L}^2 (X\times \Xi \to \R)$. The random feature approach
consists in approximating ${\mathbf k}$ via Monte Carlo sampling of its integral representation.
We show a limitation of such an approach if we would like to expect more. 

More precisely, in Section 6 we study which kernels represented as
\[
{\mathbf k}(x, x' ) =
\int_\Xi
\psi(\lan x, \omega \ran +b )\psi(\lan x', \omega\ran+b )d\pi(\omega,b) \qquad \mbox{for all}\qquad x, x'\in X\subset \R^n,
\]
where $\psi \in {\mathbb L}^2 (\R \to \R)$, 
$b$ is distributed on $\R$ and $\omega$ is distributed on $\R^n$ according to some probability measures, can be approximated by feature maps of form 
\begin{equation}
\label{propose}
{\mathbf k}(x,y|c,w,g)= \sum_{j=1}^{M} c_j g(\lan x, w_j\ran+b_j) g(\lan y, w_j\ran+b_j), \quad x,y,w_j\in \R^n, c_j>0,b_j\in \R.
\end{equation}

In Section 2, we present the results of Lin and Pinkus \cite{Pinkus1}. We state our contribution and discuss how we use their approach to attack some problems related to Machine Learning. 

However, one cannot expect to obtain all the Mercer kernels by the above approach. We show this in Section 3. A detailed description of this space of the so-called ridge functions \cite{Pinkus1} is given in Section 4. In Section 5, we demonstrate that every continuous function 
${\mathbf k}: \R^{2n}\to \R^{2n}$ such that 
\begin{equation}\label{tildek}
{\mathbf k}(y,x)={\mathbf k}(x,y)=\tilde{{\mathbf k}}(x-y), \qquad \mbox{for all} \qquad x,y\in \R^n
\end{equation}
for some $\tilde{\mathbf k}\in C(\R^n,\R)$ can be represented as a sum of products of cosine functions, see \eqref{rahimi1}. Such a representation is a consequence of the properties of the space of ridge functions. Also, we show that it is worth analyzing a proper subspace of ridge functions if we know additional information about the data.

\section{Preliminaries}
In this paper, we explore the results from \cite{Pinkus1}, especially, Theorem 2.1 of that paper. Let $\Omega$ be a subset of the set of real matrices of dimension $d\times \tau$. Consider the functional space
\[
\calM(\Omega)=\mbox{\rm span}\,\{ g(Az): A\in \Omega, g\in C(\R^d,\R)\}
\]
where $z\in \R^\tau$. Let $L(A)$ denote the span of rows of $A$ and
\[
L(\Omega)=\bigcup_{A\in \Omega} L(A).
\]

\begin{theorem}\label{Pinkus}
The linear space  $\calM(\Omega)$ is dense in $C(\R^{\tau},\R)$ endowed with the topology of uniform
convergence on compact sets if and only if the only homogeneous polynomial of $m$ variables which vanishes identically on $L(\Omega)$ is the zero polynomial.
\end{theorem}

Following \cite{Pinkus1}, we denote by $\calP_\Omega$ the set of all polynomials, vanishing identically on $L(\Omega)$, i.e.
\[
\calP_\Omega =\{p\in \Pi^{\tau} : p|_{L(\Omega)}=0 \},
\]
where $\Pi^{\tau}$ is the set of all algebraic polynomials of $\tau$ variables. By $\Pi^\tau_k\subset \Pi^\tau$, we denote the set of polynomials of total degree $k$. 

According to \cite[Theorem 4.1]{Pinkus1}, in order to find the set $\overline{\calM(\Omega)}$, it is sufficient to find the polynomials $q$ such that for all polynomials $p$ vanishing on $L(\Omega)$ we have 
\begin{equation}
\label{poly}
p(D) q=0.
\end{equation}
Here $D$ is the gradient vector; for any multiindex $m$ we can define $$D^m=\dfrac{\partial^{|m|}}{\partial z^m}.$$
Speaking of the closure of a set of functions (for instance, $\overline{\calM(\Omega)}$), we always mean the topology of uniform convergence on compact sets. Introduce the set of all polynomials satisfying \eqref{poly}:
\[
\calC_1=\mbox{\rm span}\,\{q\in \Pi^\tau: p(D) q=0 \quad \hbox{for all}\quad p\in \calP_\Omega\}.
\]
Moreover, let
\[
\calC_2=\mbox{\rm span}\,\{ q(z)=(\lan z, a\ran)^l:  l\in \Z_+, z\in \R^{\tau}, a\in \mathcal N \}
\]
where
\[
 \NN:=\mathrm{ker}\, \calP_\Omega:=\bigcap_{p\in \calP_\Omega} \mathrm{ker}\, p.
\]
Let 
\[
\calC_3=\mbox{\rm span}\,\{ f(\lan z, a\ran):  f\in C(\R,\R), a\in \mathcal N \}
\]
Now we can reformulate \cite[Theorem 4.1]{Pinkus1}.

\begin{theorem}\label{Momega}
In the topology of uniform convergence on compact sets, we have
\[
\overline{\calM(\Omega)}=\overline{\calC_1}=\overline{\calC_2}=\overline{\calC_3}.
\]
\end{theorem}
\begin{proof} The Weierstrass Theorem implies that
$\overline{\calC_2}= \overline{\calC_3}$.
It follows from the proof of \cite[Theorem 4.1]{Pinkus1} that 
\[
\overline{\calM(\Omega)}=\overline{\calC_1}=\overline{\calC_3}.
\]
\end{proof}
Consider a specific set $\Omega_S$ of $2\times 2n$ matrices of the form
\begin{equation}\label{matrixa}
A=\left[
\begin{matrix}
w & 0\cr
0 & -w
\end{matrix}
\right],\qquad 0,w\in \R^n.
\end{equation}
We will show that
\[
\calM(\Omega_S)=  \spa\,\{ g(Az): A\in \Omega_S, \quad g\in C(\R^2,\R) \},
\]
is not dense in $C(\R^{2n},\R)$ endowed with the topology of uniform convergence on compact sets.

Let the subspace $L(A)\subset \R^{2n}$ be the span of two rows of $A$ and, as we defined above,
\begin{equation}
\label{def1}
L(\Omega_S):=\bigcup_{A\in \Omega_S} L(A).
\end{equation}
Denote
\begin{equation}
\label{def2}
 \NN_S=\mathrm{ker}\, \calP_{\Omega_S}:=\bigcap_{p\in \calP_{\Omega_S}} \mathrm{ker}\, p,
\end{equation}
where
\begin{equation}
\label{def3}
\calP_{\Omega_S} =\{p\in \Pi^{2n} : p|_{L(\Omega_S)}=0 \}
\end{equation}
and $\Pi^{2n}$ is a space of polynomials of $2n$ variables. 
We prove in Theorem  \ref{almost} that 
\[
L(\Omega_S)=\overline{L(\Omega_S)}=\NN_S.
\]

This gives a more explicit characterization of $\overline{\calM(\Omega_S)}$ by polynomials $\calC_2$, i.e.
\[
\overline{\calM(\Omega_S)}=\overline{\calC_2}
\]
where
\[
\calC_2=\mbox{\rm span}\,\{ q(z)=(\lan z, a\ran)^l:  l\in \Z_+, z\in \R^{\tau}, a\in L(\Omega_S)\}.
\]

Since functions $g=g_1\otimes g_2$ are dense in $C(\R^{2},\R)$ with the topology of uniform convergence on compact sets, we get that in this topology, 
\begin{equation}
\label{tensor}
\overline{\calM(\Omega_S)}=\overline{\spa\,\{ g_1(\lan w, x\ran)g_2(\lan w, y\ran): w\in \R^n, g_1,g_2\in C(\R,\R) \}}.
\end{equation}

Consequently, if we approximate a Mercer kernel ${\mathbf k}(x,y)$, $(x,y)\in X\subset \R^{2n}$ on a compact set $X$ by a function
\begin{equation}
\label{ridge}
 f_M(x,y)=\sum_{j=1}^{M} c_j g_j(\lan x, w_j\ran) g_j(\lan y, w_j\ran), \quad x,y,w_j\in \R^n, c_j\in \R, g_{j}\in C(\R,\R)
\end{equation} 
we have, by definition, $f_M\in \calM(\Omega_S)$. This formula explains the importance of the space $\calM(\Omega_S)$ and its properties.

The closure $\overline{\calM(\Omega_S)}$ appears in the characterization of Mercer kernels as a sum of products of cosine functions. Our proof shows that, in this case, we can consider matrices as vectors $A=[w,-w]$.
Hence, the Mercer kernel as a sum of products of cosine functions is less general than \eqref{ridge}.

\section{Polynomials vanishing on $L(\Omega_S)$}

Following the approach proposed by Lin and Pinkus \cite{Pinkus1}, we demonstrate that the closure $\overline{\calM(\Omega_S)}$ is not $C(\R^2,\R)$.

Let $H^{2n}_k$ denote the set of homogeneous polynomials of $2n$ variables of total degree $k$ i.e.
\[
H^{2n}_k=\left\{ \sum_{|m|=k} a_m z^m, \quad z\in \R^{2n}\right\}.
\]
For any $s\in \Z_+^n$, $|s|=k$ and $0\leq l\leq k$ we define the set
\[
\Delta_{s,l}=\big\{\overline{m} \in [0,s_1]\times \cdots \times [0,s_n]: |\overline{m}| =l\big\}.
\]
For any $0<l<k$, let
\[
\calH(\Delta_{s,l})=\{p\in H^{2n}_k : p=P_{s,l},\quad |s|=k \},
\]
and, taking $(x,y)\in \R^{2n}$, we get
\[
P_{s,l}(x,y)= \sum_{\overline{m}\in \Delta_{s,l}} a_{\overline{m},s-\overline{m}} x^{\overline{m}}
y^{s-\overline{m}}, \qquad \sum_{\overline{m}\in \Delta_{s,l}} a_{\overline{m},s-\overline{m}}=0.
\]
Let
\[
\calH_{\Omega_S,k} =\{p\in H^{2n}_k : p|_{L(\Omega_S)}=0 \}.
\]
Applying Theorem \ref{Pinkus}, we obtain the following result.

\begin{theorem}\label{l1.2}
Let $\Omega_S$ be a family of $2\times 2n$ matrices of the form \eqref{matrixa}, $n>1$. Then 
\begin{equation}
\label{equality}
\bigcup_{|s|=k,0<l<k} \calH(\Delta_{s,l}) = \calH_{\Omega_S,k}
\end{equation}
Consequently, $\calM(\Omega_S)$ is not dense in $C(\R^{2n},\R)$ with the topology of uniform convergence on compact sets.
\end{theorem}

\begin{proof}

Let $P$ be any homogeneous polynomial of $2n$ variables of total degree $k$, i.e.
for $m=(\overline{m},\underline{m})$, $\overline{m},\underline{m} \in \Z_+^n$, $x,y\in \R^n$
\[
P(z)=P(x,y)=\sum_{|m|=k} a_m x^{\overline{m}} y^{\underline{m}}=\sum_{|m|=k} a_m z^m.
\]
Here $\Z_+$ stands for the set of all non-negative integers. We use the multivariate notation. 
Let $A\in \Omega_S$. Then 
\[
L(A)=\{(x,y)=\alpha (w,0)+\beta (0,w), \alpha,\beta\in \R \}.
\]
Hence for any vector $(x,y)\in L(A)$,
\[
P(x,y)=\sum_{|m|=k} a_m
 x^{\overline{m}} y^{\underline{m}}=\sum_{|m|=k} a_m \alpha^{|\overline{m}|} \beta^{|\underline{m}|} w^{\overline{m}+\underline{m}}.
\]
A linear operator $E:\Z^{2n} \to \Z^{2n}$
\begin{equation}
\label{linear}
E(m)=E(\overline{m},\underline{m})=(\overline{m},\overline{m}+\underline{m})=(\overline{m},s), \quad s(m)=s=\overline{m}+\underline{m}
\end{equation}
is isomorphism. Let $B=E^{-1}$. Hence $B(\overline{m},s)= (\overline{m},s-\overline{m})$ then, using $\overline{m}=(m_1,\ldots,m_n)$, $(x,y)=\alpha (w,0)+\beta (0,w)$
\[
P(x,y)=\sum_{|s|=k,s\in \Z_+^n} \sum_{m_1=0}^{s_1}\cdots \sum_{m_n=0}^{s_n}
a_{B(\overline{m},s)}\alpha^{|\overline{m}|} \beta^{k-|\overline{m}|} w^{s}.
\]
Observe that $\# \Delta_{s,l}=1$ if $|s|=l$ or $l=0$ or there is only one non-zero entry $s_j$. In all other cases, $\# \Delta_{s,l} >1$. Here $\#$ stands for the cardinality of a set. Then, for all $w\in {\R^n}$, $\alpha,\beta\in \R$,
$(x,y)=\alpha (w,0)+\beta (0,w)$,
\begin{equation}
\label{cond0}
P(x,y)=\sum_{|s|=k,s\in \Z_+^n} \sum_{l=0}^k \sum_{\overline{m}\in \Delta_{s,l}}
a_{B(\overline{m},s)}\alpha^{l} \beta^{k-l} w^{s}=0
\end{equation}
if and only if for all $s,l\leq |s|$
\begin{equation}
\label{cond}
\sum_{\overline{m}\in \Delta_{s,l}}
a_{B(\overline{m},s)}=0.
\end{equation}
This is $(k+1)\binom{n+k-1}{k}$ equations. Observe that $\binom{n+k-1}{k}$  is a dimension of space of homogeneous polynomials of $n$ variables of degree $k$. We see that if $\#\Delta_{s,l}>1$ then there exist non-zero polynomials that vanish $L(\Omega_S)$. 

Taking to account Eq.\,\eqref{cond}, we see that both
$\calH_{\Omega,k} $ and $\calH(\Delta_{s,l})$ are linear spaces, $\calH(\Delta_{s,l}) \subset
\calH_{\Omega,k}$. If $\#\Delta_{s,l}>1$ then $\dim(\calH(\Delta_{s,l}))=\#\Delta_{s,l}-1$. To see this, fix one coefficient $a_{\overline{m_0},s-\overline{m_0}}=-1$ and change the others to zero, but one equals one.
The equality \eqref{equality} is a consequence of an equivalence of \eqref{cond0} and \eqref{cond}.
\end{proof}

\section{Characterization of polynomials in $\overline{\calM(\Omega_S)}$}

Let $H^{2n}$ be a set of all  homogeneous polynomials of $2n$ variables and
\[
\calH_{\Omega_S} =\{p\in H^{2n} : p|_{L(\Omega_S)}=0 \},
\]
\[
\calP_{\Omega_S} =\{p\in \Pi^{2n} : p|_{L(\Omega_S)}=0 \},
\]
\[
 \NN_S=\mathrm{ker}\, \calP_{\Omega_S}:=\bigcap_{p\in \calP_{\Omega_S}} \mathrm{ker}\, p.
\]

\begin{lemma}\label{NS}
Then
\[
\NN_S=\mathrm{ker}\, \calH_{\Omega_S}:=\bigcap_{p\in \calH_{\Omega_S}} \mathrm{ker}\, p.
\]
\end{lemma}

\begin{proof}
Inclusion $\NN_S\subset \hbox{ker}\, \calH_{\Omega_S}$ is obvious. Let $q\in \calP_{\Omega_S}$,
\[
q(z)=\sum_{|m|\leq \rho} a_m z^m=\sum_{k=0}^\rho q_k(z),\qquad z\in \R^{2n},
\]
where $q_k$ is a homogeneous polynomial
\[
q_k(z)=\sum_{|m|=k} a_m z^m.
\]
Then for all $t\in \R$ and all $z\in L(\Omega_S)$, $tz\in L(\Omega_S)$
\[
q(tz)=\sum_{k=0}^\rho t^k q_k(z)=0.
\]
Hence for all $k=0,\ldots,\rho$   
\[
q_k(z)=0, \qquad z\in L(\Omega_S),\qquad q_k\in \calH_{\Omega_S}.
\]
 Consequently,
 \[
  \bigcap_{k=1}^\rho \hbox{ker}\, q_k= \hbox{ker}\, q.
 \]
 %\bigcap_{p\in \calH_{\Omega_S}} \hbox{ker}\, p \subset
 This statement finishes the proof. 
\end{proof}

\begin{theorem}\label{almost}
Let $n>1$ and $\Omega_S$ be a family of matrices such as in Lemma \ref{l1.2}. Then
\[
L(\Omega_S)=\overline{L(\Omega_S)}=\NN_S.
\]
\end{theorem}
\begin{proof}
It follows from definitions \eqref{def1}-\eqref{def3} that
\begin{equation}\label{lns}
L(\Omega_S) \subset \overline{L(\Omega_S)}\subset \NN_S.
\end{equation}
By Lemma \ref{NS}, definition of $\calH_{\Omega_S,k}$ and Theorem \ref{l1.2}, we obtain
\[
\NN_S=\mathrm{ker}\,\calH_{\Omega_S}\subset \bigcap_{|s|=k, 0<l<k} \mathrm{ker}\, \calH(\Delta_{s,l})=\mathrm{ker}\,\calH_{\Omega_S,k} 
\]
Now it suffices to prove that for sets $\calH(\Delta_{s,l})$ large enough
\[
\bigcap \mathrm{ker}\, \calH(\Delta_{s,l}) \subset  L(\Omega_S).
\]
Here the polynomials identifying $\NN_S$ are given below in Step 1- Step 3.

Let us fix $s\in \Z_+^n$, $|s|=k$. Let $\#\Delta_{s,l}>1$. From Theorem \ref{l1.2}, $\calH(\Delta_{s,l}) \subset\calH_{\Omega_S,k}$. 
For a fixed multiindex $\kappa \in \Delta_{s,l}$ the test set of polynomials is constructed as follows: $b_{s,l,\kappa,m}$ is for
\[\begin{array}{c}
m=(m_1,m_2,\ldots, m_n)\neq \kappa=(\kappa_1,\ldots,\kappa_n), \quad |m|=l, m\in \Delta_{s,l}, 
\\ s=(s_1,s_2,\ldots, s_n), |s|=k, \quad  \quad \mbox{for all} \quad 0\le j\le n,
\end{array}\]
\[
b_{s,l,\kappa,m}(x,y)=x^{\kappa}y^{s-\kappa}- x^{m}y^{s-m},
\]
where
\[
x=(x_1,\ldots,x_n),\quad y=(y_1,\ldots,y_n).
\]
Let $(x,y)\in \NN_S$. Then $b_{s,l,\kappa,m}(x,y)=0$ for all $ b_{s,l,\kappa,m} \in \calH(\Delta_{s,l}) $
if and only if for all $s$, $m$ as above
\begin{equation}\label{bslm}
x_1^{\kappa_1} x_2^{\kappa_2}\cdots x_n^{\kappa_n}  y_1^{s_1-\kappa_1} y_2^{s_2-\kappa_2}\cdots y_n^{s_n-\kappa_n}=x_1^{m_1}x_2^{m_2}\cdots x_n^{m_n} y_1^{s_1-m_1} y_2^{s_2-m_2}\cdots y_n^{s_n-m_n}.
\end{equation}
We would like to show that $(x,y)\in L(\Omega_S)$, i.e. there are $\alpha,\beta\in \R$ and a vector $w\in \R^n$ such that
\begin{equation}\label{LO}
(x,y)=\alpha(w,0,\ldots,0)+\beta (0,\ldots,0,w).
\end{equation}

\textbf{Step 1.} We show that if $x_1\neq 0,\ldots, x_n\neq 0$ then either all $y_i$ are zero, all all $y_j$ are non-zero. Take  $\kappa=(1,0,\ldots,0)$, $s=(1,1,0,\ldots,0)$,
$m=(0,1,0,\ldots,0)$. Then
\[
b_{s,l,\kappa,m}(x,y)=x_1y_2-x_2y_1=0.
\]
Hence $y_2=\frac{x_2}{x_1} y_1$. By a similar way we can show that $y_j=\frac{x_j}{x_1} y_1$ for $j=3,\ldots,n$.
Hence we prove the desired statement.

\textbf{Step 2.} Thus if all coefficients $x$ and $y$ are non-zero then
\[
x_1^{m_1-\kappa_1} x_2^{m_2-\kappa_2}\cdots x_n^{m_n-\kappa_n}  = y_1^{m_1-\kappa_1} y_2^{m_2-\kappa_2}\cdots y_n^{m_n-\kappa_n}.
\]
\[
1=\Big(\frac{x_1}{y_1}\Big)^{m_1-\kappa_1} \Big(\frac{x_2}{y_2}\Big)^{m_2-\kappa_2}\cdots \Big(\frac{x_n}{y_n}\Big)^{m_n-\kappa_n}.
\]
Without loss of generality, we can assume that $m_1<s_1$. Otherwise, we can increase $s_1$.
Take $m_1=\kappa_1+1$ and $m_i=\kappa_i-1$ and $m_j=\kappa_j$ for $j\neq 1$ and $j\neq i$. Then 
\[
1=\Big(\frac{x_1}{y_1}\Big)^{-1} \Big(\frac{x_i}{y_i}\Big).
\]
Since $i$ is selected arbitrarily, we get 
\begin{equation}\label{xty}
x_i=ty_i, \qquad i=1,2,\ldots,n.
\end{equation}
Consequently, we get \eqref{LO}. 

\textbf{Step 3.} Now let us assume that $(x,y)\in \NN_S$ is such that  $x_1=\cdots =x_i=0$ and $x_{i+1}\neq 0, \ldots, x_n\neq 0$. We want to show that $y_1=\cdots =y_i=0$.
Take $\kappa_1=\cdots=\kappa_i=0,\kappa_{i+1}=\ldots=\kappa_n=1$,
$s_1=1,s_2=\cdots = s_i=0,s_{i+1}=\ldots=s_n=1$, $m_1=1,m_2=\cdots =m_i=m_{i+1}=0$, $m_{i+2}=\cdots =m_n=1$.
Since $b_{s,l,\kappa,m}(x,y)=0$ then $y_1=0$ in similar way we get $y_2=\cdots =y_i=0$. Now by Step 1, 
$y_j=\frac{x_j}{x_1} y_1$ for $j=2,\ldots,n$. Hence, by Step 2, there is $t$ such that
\[
t x_i=y_i, \qquad i=1,\ldots,n.
\]
Then \eqref{LO} is satisfied. 

\end{proof}

\section{Sum of products of cosine functions}

\begin{theorem}\label{th_appr} Let ${\mathbf k}: \R^{2n}\to \R$ be a continuous function
satisfying \eqref{tildek} for some continuous $\tilde{\mathbf k}\in C(\R^n,\R)$.
Then $k\in \overline{\calM(\Omega_S)}$.

In addition, for any compact set $K\subset \R^{2n}$, and for every $\epsilon>0$ there exist $M_1,M_2$, coefficients $c_j\in \R$ and
parameters $t_k\in \R$, $w_j\in \R^n$ such that for all $(x,y)\in K $
\begin{equation}
\label{calka2}
\left|{\mathbf k}(x,y)- \sum_{j=1}^{M_1} \sum_{k=1}^{M_2} c_j/M_2 \cos(\lan w_j, x\ran+t_k)\cos(\lan w_j, y\ran+t_k)\right|<\epsilon.
\end{equation}
\end{theorem}

\begin{proof}
Let $\mathbf k$ be a symmetric shift-invariant kernel (as in the statement of the theorem). Since $K\subset \R^{2n}$ is the compact set then there is a compact set $\tilde{K}\subset \R^n$ such that if $(x,y)\in K$ then $x-y\in \tilde{K}$ and  $y-x\in \tilde{K}$.       
Let  $\sigma:\R\to \R$ be a so-called activating function, which is continuous and not a polynomial.  By \cite[Proposition 3.7]{Pinkus2}, the space
\[
\spa\{\sigma(\lambda t- \theta): \lambda,\theta\in \R  \},\qquad t\in \R,
\]
is dense in $C(\R,\R)$ endowed with the topology of uniform convergence on compact sets.  Then by \cite[Proposition 3.3]{Pinkus2}, 
\[
\spa\{\sigma( \lan w,x\ran-\theta): w\in \R^n, \theta\in \R \},
\]
is dense in $C(\R^n)$ with the topology of uniform convergence on compact sets. Hence, we have
 \[
\tilde{{\mathbf k}}\in \overline{\spa}\{\sigma( \lan w,x\ran-\theta): w\in \R^n, \theta\in \R \}
\]
Consequently, for all $\epsilon>0$ and $u\in \tilde{K}$, there exist constants $M$, $c_j\in \R$, $\theta_j\in \R$ 
\begin{equation}
 \label{rep0}
\left|\tilde{{\mathbf k}}(u)- \sum_{j=1}^M c_j \sigma (\lan w_j, u\ran-\theta_j)\right|<\epsilon.
 \end{equation}
Let  $a_j=(w_j,-w_j)\in \R^{2n}$. Then  for $(x,y)\in K\subset \R^{2n}$
 \begin{equation}
 \label{rep}
\left|{\mathbf k}(x,y)- \sum_{j=1}^M c_j \sigma (\lan a_j, (x,y)\ran-\theta_j)\right|<\epsilon.
 \end{equation}
Note that $a_j=(w_j,-w_j)\in L(\Omega_S)$.
By \cite[Theorem 4.1 (2)]{Pinkus1} (see  also Theorem \ref{Momega}) and taking into account the inclusion \eqref{lns}, we get ${\mathbf k}\in \overline{\calM(\Omega_S)}$.

By Eq.\, \eqref{rep}, we get that for $\sigma=\cos$ and any $\epsilon>0$ there exist constants $M$, $c_j\in \R$, $\theta_j\in \R$ and vectors $a_j=(w_j,-w_j)\in \R^{2n}$ such that
\[
\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(\theta_j)+\sin (\lan w_j,x-y\ran)\sin(\theta_j)\Big)\right|<\epsilon,\qquad (x,y)\in K.
\]
Now we show that also
\begin{equation}
\label{claim}
\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(\theta_j)\Big)\right|<\epsilon,\qquad (x,y)\in K.
\end{equation}
Indeed,
\[
2\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(\theta_j)\Big)\right|
\]
\[
\leq \Big|2{\mathbf k}(x,y)-\]
\[ \left.\sum_{j=1}^M c_j \Big(2\cos (\lan w_j,x-y\ran)\cos(\theta_j)-\sin (\lan w_j,x-y\ran)\sin(\theta_j)
+\sin (\lan w_j,x-y\ran)\sin(\theta_j)\Big)\right|
\]
\[
\leq \left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(\theta_j)-\sin (\lan w_j,x-y\ran)\sin(\theta_j)\Big)\right|
\]
\[
+\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(\theta_j)+\sin (\lan w_j,x-y\ran)\sin(\theta_j)\Big)\right|
\]
Take $(x,y)\in K$. Then, by definition of $\tilde K$, we have $x-y\in \tilde{K}$ and $y-x\in \tilde{K}$. Since ${\mathbf k}(x,y)={\mathbf k}(y,x)$, and since the function $\cos$ is even and $\sin$ is odd, we get the claim \eqref{claim}.  It follows from trigonometric formulas, that  
\[
\int_0^{2\pi} 2\cos(\alpha+t)\cos(\beta+t) dt=
\int_0^{2\pi} \left( \cos(\alpha-\beta)+\cos(\alpha+\beta+2t)\right) dt=2\pi \cos(\alpha-\beta).
\]
Putting $\alpha=\lan w_j, x\ran, \beta=\lan w_j, y\ran$, we obtain that for $\epsilon>0$ there are $M$, $\tilde{c}_j\in \R$, $\theta_j\in \R$ and vectors $w_j\in \R^n$
\begin{equation}
\label{calka}
\left|{\mathbf k}(x,y)- \sum_{j=1}^M \tilde{c}_j \int_0^{2\pi} \cos(\lan w_j, x\ran+t)\cos(\lan w_j, y\ran+t) dt
\right|<\epsilon, \qquad (x,y)\in K.
\end{equation}
Now we use the uniform approximation of the integral 
\[
\int_0^{2\pi} \cos(a+t)\cos(b+t) dt,
\] 
on the interval $[0,2\pi]$. Namely, for any $a,b\in \R$ and any $\epsilon>0$ there is $M_2>0$ such that for 
$\{t_k= 2\pi k/M_2,\quad k=1,\ldots, M_2\}$ we have
\[
\left| \int_0^{2\pi} \cos(a+t)\cos(b+t) dt-\frac{1}{M_2}\sum_{k=1}^{M_2} \cos(a+t_k)\cos(b+t_k)\right|<\epsilon.
\]
Thus we prove the theorem.
\end{proof}
 
\section{Min-Max criteria of choosing Mercer kernels in learning theory }

Now we study how to select the optimal Reproducing Kernel Hilbert Space $\calH_k$ (RKHS) for a learning process, see \cite{Cucker}, \cite{Smale}. Let ${\mathbf k}$ be a Mercer kernel corresponding to $\calH_k$.  In this perspective, previous sections show that Mercer kernels ${\mathbf k}$ from $\overline{\calM(\Omega_S)}$ provide a good choice. More precisely, due to \eqref{tensor}, we can take a feature map
\[
{\mathbf k}\in \overline{\spa\,\{ g(\lan w, x\ran)g(\lan w, y\ran): w\in \R^n, g\in C(\R,\R) \}}\subset \overline{\calM(\Omega_S)}.
\]
so that for any fixed $M\in {\mathbb N}$ there exist parameters $c=\{c_j\}, w=\{w_j\}, b=\{b_j\},\sigma\in C(\R,\R)$,
defining the approximation \eqref{propose}. Let $\theta=(c,w,g)$ denote the parameters, i.e.
\[
{\mathbf k}(x,y)={\mathbf k}(x,y|\theta)={\mathbf k}(x,y|c,w,g),\qquad x,y\in \R^n.
\]

Now we present a simplified version of problems of learning theory.
Let $X$ be a compact subset of $\R^n$ and $Z = X \times Y$. Let $\rho$ be a probability measure on $Z$ and $\rho_X, \rho_{Y |x}$  be the induced marginal probability measure on $X$ and conditional probability measure on $\mathbb R$ conditioned on $x\in X$, respectively. Define $f_\rho : X \to {\mathbb R}$ as follows:
\[
f_\rho(x)=\int_Y y d\rho_{y|x}.
\]
This function is the regression function of $\rho$. We assume that $f_\rho \in L^2(\rho_X)$. 
As a possible method of finding $f_\rho$ one can minimize the regularized least square problem (variation problem) in $\calH_k$
$ \inf_{f \in \calH_k} \calL(f)$
where
\begin{equation}
\label{functional}
\calL(f)=
\int_Z (f (x) -y)^2 d\rho(x,y) + \lambda \|f\|_{\calH_k}^2,\qquad \lambda >0.
\end{equation}
\cite[Proposition 7 in Chapter III]{Cucker} garantees the existence and uniqueness of a minimizer.
Usually, the measure $\rho$ is unknown. Now we consider the sampling, let
\[
z = (z_1,\ldots,z_N)=((x_1 , y_1),\ldots , (x_N, y_N))
\]
be a sample in $Z^N$, i.e. $n$ examples independently drawn according to $\rho$.
In the context of RKHS, given a sample $z$, 
discrete version of \eqref{functional} 
\begin{equation}
\label{functional1}
\calL_N(f)=\frac{1}{N}\sum_{i=1}^N(f (x_i ) -y_i )^2 + \lambda \lan f, f \ran_{\calH_k},
\end{equation}
and “batch learning”\ means
solving the regularized least square problem 
\begin{equation}
\label{variation}
f_{\lambda,z} = \mbox{arg\,min}_{f\in \calH_k}\,
\calL_N(f),
\quad
\lambda > 0,
\end{equation}
see \cite[Comparison with “Batch Learning” Results]{Smale}
and \cite{2021}.
The existence and uniqueness of $f_{\lambda,z}$ given as in \cite [Section 6]{Cucker} says
\[
f_{\lambda,z} =
\sum_{i=1}^N
a_i {\mathbf k}(x,x_i).
\]
where $a = (a_1 ,\ldots, a_N)$ is the unique solution of the well-posed linear system in $\R^n$,
where 
\[
a=(K_N+\lambda N I)^{-1}y,\qquad y=(y_1,\ldots,y_N),\qquad K_N=[{\mathbf k}(x_i,x_j)]_{1\leq i,j\leq N}.
\]
\cite [Section 6]{Cucker}.
To emphasize  the connection with parameters $\theta$, we use the notion:
\[
 K_{N,\theta}=[{\mathbf k}(x_i,x_j|\theta)]_{1\leq i,j\leq N}
 \]
\begin{lemma} Let $z \in  Z^N$. For $\lambda>0$
\[
\calL_N(f_{\lambda,z})= y'  \left(\frac{1}{N}K_N+ I\right)^{-1}y
\]
\end{lemma}
\begin{proof}
Now let put $f_{\lambda,z}$ and calculate
\[
\calL_N(f_{\lambda,z})=\frac{1}{N}\sum_{j=1}^N(f_{\lambda,z} (x_j ) -y_j )^2 + \lambda \lan f_{\lambda,z}, f_{\lambda,z} \ran_{\calH_k},
\]
Note that
\[
\frac{1}{N}\sum_{i=1}^N(f_{\lambda,z} (x_i ) -y_i )^2= \frac{1}{N}\|K_N(K_N+\lambda N I)^{-1} y-y\|^2.
\]
But
\[
K_N(K_N+\lambda N I)^{-1}-I=-\lambda N(K_N+\lambda N I)^{-1}.
\]
From definition of ${\mathbf k}$ and $a$ we derive
\[
\lan f_{\lambda,z}, f_{\lambda,z} \ran_{\calH_k}=a'K_N a=y'(K_N+\lambda N I)^{-1} K_N (K_N+\lambda N I)^{-1}y.
\]
Hence
\[
\calL_N(f_{\lambda,z})=N\lambda^2\|(K_N+\lambda N I)^{-1} y \|^2 + \lambda y'(K_N+\lambda N I)^{-1} K_N (K_N+\lambda N I)^{-1}y.
\]
Consequently,
\[
\calL_N(f_{\lambda,z})=N\lambda^2 y' (K_N+\lambda N I)^{-1} (K_N+\lambda N I)^{-1} y  + \lambda y'(K_N+\lambda N I)^{-1} K_N (K_N+\lambda N I)^{-1}y.
\]
\[
\calL_N(f_{\lambda,z})=\lambda\big( y' (K_N+\lambda N I)^{-1} (\lambda N I  +  K_N) (K_N+\lambda N I)^{-1}y\big)
\]
\[
\calL_N(f_{\lambda,z})=\lambda\big( y'  (K_N+\lambda N I)^{-1}y\big)
\]
\end{proof}
The Von Neumann series gives that
\[
\left(\frac{1}{\lambda N}K_N+ I\right)^{-1}=\sum_{j=0}^\infty (-1)^j \left( \frac{1}{\lambda N}\right)^j K^j_N
\]
The series converge in norm topology if ${\lambda}N > \|K_N\|$.  So for any $\epsilon>0$ there is $L>0$ such that
\[
\left\|\left(\frac{1}{N \lambda}K_N+ I\right)^{-1}-\sum_{j=0}^L (-1)^j \left( \frac{1}{\lambda N}\right)^j K^j_N\right\|<\epsilon
\]
Now, by Spectral Theorem, we can decompose  matrix $K_N$ as
\[
K_N=K_{N,\theta}=U' \Lambda  U=U_\theta' \Lambda_\theta  U_\theta,
\]
where $U$ is an orthonormal  matrix consisting of eigenvectors of of $K_N$ and $\Lambda$ is a diagonal matrix
consisting of eigenvalues of of $K_N$. Hence $K^j_{N,\theta}=U_\theta' \Lambda_\theta^j  U_\theta$.

Our first proposition on the estimation of the best parameters is the following. Given a number $M$, a sample $z$, and $\lambda>0$
we find parameters $\theta$ such that
\[
\min_{\theta} \calL_N(f_{\lambda,z})=\min_{\theta}
\lambda\big( y'  (K_{N,\theta}+\lambda N I)^{-1}y\big).
\]
A similar approach is possible for given  $L$ and $\epsilon$. Let $\lambda_{k,\theta}, k=1,\ldots,N$ be a sequence of eigenvalues of 
$K_{N,\theta}$, $\lambda_{k,\theta}\geq  \lambda_{k+1,\theta}$. Let ${\lambda}N> \lambda_{1,\theta}$. A criterion 
for optimal $\theta$
\[
\min_{\theta} \frac{1}{N} y'U_\theta' \left( \sum_{j=0}^L (-1)^j \left( \frac{1}{\lambda N}\right)^j \Lambda^j_{n,\theta} \right) U_\theta y.
\]
it is assumed in \cite{Binev} that this probability measure $\rho_{y|x}$ is supported on an interval.  We can assume that $|y|\leq 1$.
We propose to chose  parameters $\widehat{\theta}$ of Mercer kernel which satisfy $\epsilon$-min-max criteria namely those where
\begin{equation}
\label{min-max}
 \min_{\theta} \max_{y\in \Sp^n} \frac{1}{N} y' \left( \sum_{j=0}^L (-1)^j \left( \frac{1}{\lambda N}\right)^j \Lambda^j_{\theta} \right) y.
\end{equation}
is attained. Finally, we can change $\sigma$ used in \eqref{propose}. Taking to account Theorem \ref{th_appr} we propose
$\sigma=\cos$. It will be interesting to compare with $\sigma=\mbox{Re}\,LU$.

\section{Conclusion} 

\subsection{Criteria}
 
Note that the criterion \eqref{min-max} does not take into account the value $M$. Perhaps, it would be better to consider  
Akaike type criteria \cite{akaike} with the penalty being the number of parameters
\begin{equation}
\label{variation1}
\min_{\theta}
\lambda\big( y'  (K_{N,\theta}+\lambda N I)^{-1}y\big)+3M,
\quad
\lambda > 0.
\end{equation}

\subsection{ A choice of the general formula of Mercer kernel.}

Our selection of kernels by Eq.\, \eqref{propose}, involves a further discussion. The properties of the kernel ${\mathbf k}$ depend on the choice of the domain $X\subset \R^{2n}$. So, selecting the parameters $c_j\in \R$, we can obtain the Mercer kernel given by formula
\[
{\mathbf k}(x,y)= \sum_{j=1}^{L}  f_j(\lan x, w_j\ran) f_j(\lan y, w_j\ran)-\sum_{j=L+1}^{M}  f_j(\lan x, w_j\ran) f_j(\lan y, w_j\ran),
\] 
where $w_j\in \R^n, x,y\in X$, $f_{j}\in C(\R,\R)$ for all $j=1,\ldots, M$. 
Indeed, ${\mathbf k}$ is a Mercer kernel on a finite set $X=\{x_1,\ldots,x_n\}\subset \R^n$ as we show in the following example.  Let
$v_j=(f_j(\lan x_1, w_j\ran),\ldots, f_j(\lan x_n, w_j\ran))$, $j=1,\ldots, M$. 
\begin{example}
Let $X=\{x_1,\ldots,x_n\}$, 
\[
{\mathbf k}(x,y)= \sum_{j=1}^{L}  f_j(\lan x, w_j\ran) f_j(\lan y, w_j \ran)-\sum_{j=L+1}^{M}  f_j(\lan x, w_j \ran) f_j(\lan y, w_j \ran),
\] 
$f_{j}\in C(\R,\R)$ for all $j$. Then ${\mathbf k}$ is a Mercer kernel on $X$ iff for all $a\in \Sp^n$
 \begin{equation}\label{other}
 \sum_{j=1}^{L} \lan v_j, a\ran^2 \geq  \sum_{j=L+1}^{M} \lan v_j,  a\ran^2.
 \end{equation}
Thus if $\{v_j, j=1,\ldots,L\}$ is a frame with lower frame bound $A$ (for definition of frame see \cite{waldron}) and $\{v_j, j=L+1,\ldots,M\}$  is a frame with and upper frame bound $B$ and $A\geq B$ then ${\mathbf k}$ is Mercer kernel.
\end{example}
 
\begin{proof}
 Note that the kernel
 \[
{\mathbf k}_j(x,y)=f_j(\lan x, w_j \ran)f_j(\lan y, w_j \ran), x,y\in X\subset \R^n
 \]
is a Mercer kernel. Indeed, ${\mathbf k}_j$ is symmetric, continuous, and positive definite since for all $a\in \R^N$ and $x_1,\ldots,x_n\in X$,
\[
a' K_j a=\lan v_j'  a, v_j'  a\ran =\lan v_j, a \ran^2  \geq 0,
\]
$a'$ stands for transposition of the vector $a$ where  $K_j =[{\mathbf k}_j(x_i,x_k)] _{1\leq i,k\leq n}$. Hence 
 \[
 {\mathbf k}=\sum_{j=1}^{L} {\mathbf k}_j-\sum_{j=L+1}^{M} {\mathbf k}_j
 \]
is a Mercer kernel if and only if for all $a\in \R^N$ and $x_1,\ldots,x_N\in \R^n$ and $K =[{\mathbf k}(x_i,x_k)] _{1\leq i,k\leq n}$ we have
  \[
 a' K a=\sum_{j=1}^{L} \lan v_j, a\ran^2 -\sum_{j=L+1}^{M} \lan v_j,  a\ran^2\geq 0.
 \]
In other words, for all $a\in \Sp^n$
 \[
 \sum_{j=1}^{L} \lan f_j, a\ran^2 \geq  \sum_{j=L+1}^{M} \lan f_j,  a\ran^2.
\]
The sufficient condition follows from the definition of a frame.
 \end{proof}

\section*{Acknowledgements}

 The research of the second coauthor was supported by Gda\'nsk University of Technology by the DEC 14/2021/IDUB/I.1 grant under the Nobelium - 'Excellence Initiative - Research University' program.

\bibliographystyle{amsplain}

\begin{thebibliography}{99}

\bibitem{Belkin} 
M.\, Belkin, {\it Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation}. Acta Numerica, 30, 203-248, 2021.

\bibitem{Binev}
P. Binev, A. Cohen, W. Dahmen, R. DeVore and V. Temlyakov, Universal algorithms
for learning theory. Part I : piecewise constant functions, J. Machine Learning Res.
6 (2005), 1297–1321.

\bibitem{SVM}
A.\, Christmann and I.\, Steinwart, {\it Support Vector Machines}. Springer, Berlin, 2008.

\bibitem{Cucker}
Cucker, F. and S. Smale {\it On the mathematical foundations of learning}. Bull. of the
Amer. Math. Soc. 29 (1), 1–49.

\bibitem{2021}
E.\, De Vito, L.\, Rosasco, and A.\, Rudi, {\it Regularization: From Inverse Problems
to Large-Scale Machine Learning} In: De Mari, F., De Vito, E. (eds) Harmonic and Applied Analysis. Applied and Numerical Harmonic Analysis. Birkh\"auser, Cham, 2021, 245-296.

\bibitem{LeCun}
Y.\, LeCun, B.\, Boser, J.\, S.\, Denker, D.\, Henderson, R.\, E.\, Howard, W.\, Hubbard and L.\, D.\, Jackel: \emph{Backpropagation Applied to Handwritten Zip Code Recognition}, Neural Computation, 1(4):541-551, Winter 1989.

\bibitem{akaike} S.\, Konishi, G.\, Kitagawa, {\it 
Information Criteria and Statistical Modeling},  Springer 2008.

\bibitem{mercer} J.\, Mercer, \emph{Functions of positive and negative type and their connection with the theory of integral equations}, Philosophical Transactions of the Royal Society A, 209 (441–458): 415–446, 1909.

\bibitem{Pinkus1}
  V.\, Ya.\, Lin, A.\, Pinkus, {\it Fundamentality of ridge functions.} J. Approx. Theory 75 (1993), no. 3, 295-311.

\bibitem{Pinkus2}
  A.\, Pinkus, {\it Approximation theory of the MLP model in neural networks}. Acta numerica, 1999, 143-195, Acta Numer., 8, Cambridge Univ. Press, Cambridge, 1999. 
  
\bibitem{Rahimi}
A.\, Rahimi and B.\, Recht, {\it Random features for large-scale kernel machines},  Advances in Neural Information Processing Systems, 2008, 
1177-1184.

\bibitem{Smale} S.\,Smale; Y. Yao {\it Online learning algorithms},
Found. Comput. Math. 6 (2006), no. 2, 145-170.  

\bibitem{waldron} S.\,F.\,D.\, Waldron,
{\it An introduction to finite tight frames,} 
Applied and Numerical Harmonic Analysis. Birkhäuser/Springer, New York, 2018. xx+587 pp.
  
\end{thebibliography}

\end{document}

where $\Lambda_\theta^{1/2}$ is a square root of $\Lambda_\theta$. 
Let
\[
h(x)=1-x+x^2-x^3+\cdots=\frac{1-x^{N+1}}{1+x},\qquad x\geq 0.
\]
 We obtain min-max criteria to obtain the best estimation of $\theta$, i.e.
\[
\widehat{\theta} =argmin_{\theta}  \sqrt{h^2\left(\sqrt{\lambda_{1,\theta}/\lambda}\right)+\cdots h^2\left(\sqrt{\lambda_{n,\theta}/\lambda}\right)}\]

\begin{lemma} Let $\Omega_S$ be given, $n>1$.
Let $s\in \Z_+^n$. Let $m=(0,s)$ or $m=(s,0)$. Then a polynomial $q(z)=z^m$, $z\in \R^{2n}$
belongs to $\calC_1$.
\end{lemma}

\begin{proof}
Note that if we have a monomial $p_\tau(z)=z^\tau$, $\tau\in \Z_+^{2n}$  then 
 $p_\tau(D)q\neq 0$ if and only if $\tau_j\leq m_j$ for all $j=1,\ldots, 2n$.
 By \eqref{cond} we see that such $p_\tau \notin \calP_\Omega$ since 
 $\#\Delta_{s(\tau),l}=1$ for both $l=0$ and $l=|\tau|$. Recall $s(\tau)=\overline{\tau}+\underline{\tau}$. In our case $\underline{\tau}=0$ or $\overline{\tau}=0$ for fixed $m$.
 Hence, $p(D)q=0$ for all $p\in \calP_\Omega$.
\end{proof}
