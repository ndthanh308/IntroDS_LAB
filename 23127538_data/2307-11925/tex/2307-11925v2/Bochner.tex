\documentclass[12pt]{amsart}

%\usepackage[notref,notcite]{showkeys}
%\usepackage[backref]{hyperref}
\usepackage{fullpage}
\usepackage{amsfonts,amssymb}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage[all]{xy}
\usepackage{tikz-cd}
\usetikzlibrary{matrix}
\usepackage{comment}

%\usepackage{musicography}

%Marcin's commands

\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Sp}{\mathbb{S}}
\newcommand{\T}{\mathbb{T}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\NN}{\mathcal {N}}

\newcommand{\calD}{{\mathcal {D}}}
\newcommand{\calF}{{\mathcal {F}}}
\newcommand{\calB}{{\mathcal {B}}}
\newcommand{\calw}{{\mathcal {W}}}
\newcommand{\calW}{{\mathcal {W}}}
\newcommand{\calG}{{\mathcal {G}}}
\newcommand{\calS}{{\mathcal {S}}}
\newcommand{\calR}{{\mathcal {R}}}
\newcommand{\calH}{{\mathcal {H}}}
\newcommand{\calM}{{\mathcal {M}}}
\newcommand{\calP}{{\mathcal {P}}}
\newcommand{\calC}{{\mathcal {C}}}
\newcommand{\calL}{{\mathcal L}}

\newcommand{\F}{\mathbf F}
\newcommand{\B}{\mathbf B}

\newcommand{\supp}{\operatorname{supp}}
\newcommand{\spa}{\operatorname{span}}
%\newcommand{\span}{\operatorname{span}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\ov}{\overline}
\newcommand{\ch}{\mathbf 1}
\newcommand{\al}{\alpha}
\newcommand{\ga}{\gamma}
\newcommand{\Ga}{\Gamma}
\newcommand{\ve}{\varepsilon}
\newcommand{\vp}{\varphi}
\newcommand{\la}{\lambda}
\newcommand{\lan}{\langle}
\newcommand{\ran}{\rangle}
\newcommand{\chip}{{\rho}}


%\newcommand{\kk}{{\musSharp{}}}
\newcommand{\bb}{{\musFlat{} }}
\newcommand{\kk}{{\mathfrak {K}}}


\def\Koniec{\hbox to\hsize{\hfil$\diamond$}}
\def\th{\vartheta}
\def\1{\mathbf 1}

% SET UP THE THEOREM ENVIRONMENTS

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{question}[theorem]{Question}
\newtheorem{conjecture}[theorem]{Conjecture}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

\numberwithin{equation}{section}

\usepackage[normalem]{ulem}
\usepackage[pagewise]{lineno}%\linenumbers

\definecolor{blue-violet}{rgb}{0.54,0.17,0.89}
\definecolor{amethyst}{rgb}{0.6,0.4,0.8}
\definecolor{darkviolet}{rgb}{0.58, 0.0, 0.83}
\definecolor{darkgreen}{rgb}{0,.4,0}
\definecolor{mixedgreen}{rgb}{0.3,0.6,00}
\definecolor{bananayellow}{rgb}{1.0, 0.88, 0.21}
\definecolor{arylideyellow}{rgb}{0.91, 0.84, 0.42}
\definecolor{bananamania}{rgb}{0.98, 0.91, 0.71}

\newcommand{\REMOVEsk}[1]%
           {{\color{magenta}\sout{#1}}}
\newcommand{\ADDsk}[1]{{\color{blue}{#1}}}
\newcommand{\CHANGEsk}[1]{{\color{cyan}{#1}}}
\newcommand{\REPLACEsk}[2]{\REMOVEsk{#1} {\color{red}{#2}}}
\newcommand{\COMMENTsk}[1]
           {{\color{mixedgreen}\textbf{\textit{SK:}}
              \textit{#1}
             }
           }
%\renewcommand{\REMOVEva}[1]{}
%\renewcommand{\COMMENTva}[1]{}

\allowdisplaybreaks

%\voffset=-3truecm \hoffset=-1.5 truecm
%\linespread{1}
%\addtolength{\headheight}{1.5pt}

%%%        %%%%%%%%%%%%%%%%%      %%%

 \begin{document}

%\title{Mercer Large-Scale Kernel Machines}
\title{Mercer Large-Scale Kernel Machines from Ridge Function Perspective}

\author{Karol Dziedziul}
\address{
Faculty of Applied Mathematics,
The Gda\'nsk University of Technology,
ul. G. Narutowicza 11/12,
80-952 Gda\'nsk, Poland}

\email{karol.dziedziul@pg.edu.pl}

\author{Sergey Kryzhevich}
\address{
Faculty of Applied Mathematics,
The Gda\'nsk University of Technology,
ul. G. Narutowicza 11/12,
80-952 Gda\'nsk, Poland}

\email{sergey.kryzhevich@pg.edu.pl}


\author{Paweł Wieczyński}

\begin{abstract}
To present Mercer large-scale kernel machines from a ridge function perspective, we recall the results by Lin and Pinkus from {\it Fundamentality of ridge functions}. We consider  the main result of the recent paper by Rachimi and Recht, 2008, {\it Random features for large-scale kernel machines} from the Approximation Theory point of view. We study which kernels could be approximated by a sum of products of cosine functions with arguments depending on $x$ and $y$ and present the obstacles of such an approach. The results of this article are applied to Image Processing by procedure "one-vs-rest".
\end{abstract}

\maketitle

\begin{quote}
{\small {\bf Mathematics subject classifications:} 42A10, 68T07, 26B40}
\end{quote}

\begin{quote}
{\small {\bf Key words and phrases:}
approximation on compact sets; Deep Learning; image processing; Mercer kernels, ridge functions}
\end{quote}

\bigskip

\section{Introduction}


In the context of Reproducing Kernel Hilbert Spaces (RKHS), a \textbf{kernel} is a function \( k: X \times X \to \mathbb{R} \) (or \(\mathbb{C}\)) that maps pairs of elements from a set \( X\subset \R^d \) into the real (or complex) numbers. Here, we consider only real-valued functions. The function must satisfy the following properties:

1. \textbf{Symmetry}: \( k(x, y) = k(y, x) \) for all \( x, y \in X \).

2. \textbf{Positive definiteness}: For any finite set of points \( \{x_1, x_2, \ldots, x_n\} \subset X \), the Gram matrix \( G_n \) defined by \( G_n =[ k(x_i, x_j)]_{1\leq i,j\leq n} \) is positive definite. This means that for any vector \( \mathbf{c} = (c_1, c_2, \ldots, c_n)^\top \in \mathbb{R}^n \),
   \[
   \mathbf{c}^\top G_n \mathbf{c} = \sum_{i=1}^n \sum_{j=1}^n c_i c_j k(x_i, x_j) \geq 0.
   \]
  In our approach, we mainly assume that  $X=\R^d$. If $k$ is continuous,  it is referred to as a Mercer kernel \cite{mercer}.  
   
     If there exists a function $\kk$ such that  $k(x,y)=\kk(x-y)$ we say that the kernel $k$ is {\bf shift invariant} or {\bf translation invariant kernel.} In applications of machine learning, we encounter universal kernels, which are studied by Ch. A. Micchelli, Y. Xu, and H. Zhang \cite{Micc}. An example of such 
kernel is the Gaussian kernel, $k(x,y)=\kk_G(x-y)$ , where $\kk_G$ is
the multivariate Gaussian density function (up to the constant)  given by:
\[ \kk_G(x) \asymp  \exp\left(-\gamma {x}^\top {x}\right), x\in \R^d,
 \]
 where $\gamma>0$. 
 A universal kernel is a continuous function  for which, for any compact subset $K\subset \R^d$, the set
 $\spa \{k(\cdot,y) : y \in K\}$ is dense in $C(K)$ under the maximum norm.
   
Different kernels can be represented by random feature maps, as introduced by Rahimi and Recht \cite{Rahimi}.
 Such a kernel depends on a set of parameters  $\theta=\{(c_j,b_j,w_j)_{j=1}^m\}$, $c_j>0$, $w_j\in \R^d$, $b_j\in \R$
\begin{equation}
\label{kRR}
k_{RR}(x,y|\theta)=\sum_{j=1}^m c_j \cos(\lan w_j,x\ran+b_j)\cos(\lan w_j,y\ran+b_j).
\end{equation}
It is evident that  $k_{RR}$ is not a universal kernel. 
Nevertheless, such kernels approximate any continuous, shift-invariant Mercer kernel
in the topology of uniform convergence on compact sets.
  Rahimi-Recht result is proved in \cite{Rahimi} using Bochner's theorem.  

In this paper, we also introduce ridge kernels depending on 
 $\theta$  which includes both the set of parameters $(\{c_j,b_j,w_j\}_{j=1}^m)$, $c_j>0$, $b_j\in\R$, $w_j\in \R^d$ and a set of functions $G\subset C(\R,\R)$ 
\begin{equation}
\label{RK}
k_{RK}(x,y|\theta)=\sum_{j=1}^m c_j g_j(\lan w_j,x\ran+b_j)g_j(\lan w_j,y\ran+b_j),\quad c_j\geq 0,
\end{equation}
such that $g_j\in G, j=1,\ldots,m$. Usually $G=\{\sigma\}$.
Such kernels also are not universal kernels.  Why might we still need such kernels? Sometimes we require universal tools, but other times, we need particular tools  to do a better job, i.e. to approximate functions of some specific types. This motivates the following definition.
\begin{definition}
We call a family $\F$ of kernels  {\bf universal}  if, for any continuous function $f$, any $\epsilon>0$ 
and any compact set $K\subset \R^d$, there exists $m\in {\mathbb N}$, $k\in \F$ and coefficients $a_j\in \R$ and $x_j\in \R^d$ such that
\[
\sup_{x\in K}|f(x)-\sum_{j=1}^m a_j k(x,x_j)|<\epsilon.
\]
Members of $\F$ are called particular kernels. 
\end{definition}
Note  that in case $k$ is a universal kernel then $\F=\{k\}$ is a universal family.
In Section \ref{UAT} we prove that the both  families  of kernels given by \eqref{RK} and \eqref{kRR}, i.e.
\[
\F_{RR}=\{k_{RR}(\cdot|\theta):\theta=\{(c_j,b_j,w_j)_{j=1}^m, c_j>0,b_j\in\R,w_j\in \R^n\}\}\]
\[
\F_{RK}=\{k_{RK}(\cdot|\theta):\theta=\{(c_j,b_j,w_j,g_j)_{j=1}^m\}, c_j>0, b_j,w_j\in\R^n,g_j\in G\}
\] are universal. 
It is a  consequence of universal approximation theorem assuming that  the family  $G$ contains at least one function that is not a polynomial.  We use arguments similar to ones from proof of \cite[Theorem 5.1]{Pinkus2}.


The contribution of this paper is as follows.   For a class of functions we get that in the topology of uniform convergence on compact sets  
\[
\overline{\calM(\Omega_S)}=\overline{\spa\,\{ g_1(\lan w, x\ran)g_2(\lan w, y\ran): w\in \R^n, g_1,g_2\in C(\R,\R) \}}\neq
C(\R^{2n},\R),
\]
see Theorem  \ref{l1.2}.
In Theorem \ref{almost} we demonstrate the geometric structure of space of polynomials contained in
$\overline{\calM(\Omega_S)}$.
It is small progress comparing \cite{Pinkus1}.
It seems more difficult to characterize spaces $\overline{\F_{RR}}$ and $\overline{\F_{RK}}$
where both class belong to $\overline{\calM(\Omega_S)}$. Nerveless, our results throw light on an limitation 
of an approximation by these classes. Moreover
Rahimi-Recht result throws some light on $\overline{\F_{RR}}$. 
Using Universal Approximation Theorem \ref{th_appr} we obtain, two results. First
parallel result to Rahimi-Recht theorem, i.e.  
 for  continuous functions $f:\R^{2n}\to \R$  symmetric and shift invariant, i.e. $f(x,y)=g(x-y)$. Secondly
 we show that both classes $\F_{RR}$ and $\F_{RK}$ are universal. This motivates to our numerical simulation. From general perspective this neural networks with machine learning. 


Both types of ridge kernels use a representation with positive coefficients
 $c_j$. Since data (samples) are usually from a compact set $X$, 
we show that theoretically we can expect also  kernels having  a representation similar to \eqref{kRR} or \eqref{RK}  with some negative coefficients $c_j$. 
In our simulation we try to find such kernels which are  more efficient than kernels with positive $c_j$ but  without success.  
 
Our numerical results compare the effectiveness of estimating a regression function using ridge kernels, Rahimi-Recht kernels. We  show how to use particular kernels as a set of filters in the spirit of  "one-vs-rest".

\section{Preliminaries}
In this paper we explore the results from \cite{Pinkus1}, especially, Theorem 2.1 of that paper. Let $\Omega$ be a subset of the set of real matrices of dimension $d\times \tau$. Consider the functional space
\[
\calM(\Omega)=\mbox{\rm span}\,\{ g(Az): A\in \Omega, g\in C(\R^d,\R)\}
\]
where $z\in \R^\tau$. Let $L(A)$ denote the span of rows of $A$ and
\[
L(\Omega)=\bigcup_{A\in \Omega} L(A).
\]

\begin{theorem}\label{Pinkus}
The linear space  $\calM(\Omega)$ is dense in $C(\R^{\tau},\R)$ endowed with the topology of uniform
convergence on compact sets if and only if the only homogeneous polynomial of $m$ variables which vanishes identically on $L(\Omega)$ is the zero polynomial.
\end{theorem}

Following \cite{Pinkus1}, we denote by $\calP_\Omega$ the set of all polynomials, vanishing identically on $L(\Omega)$, i.e.
\[
\calP_\Omega =\{p\in \Pi^{\tau} : p|_{L(\Omega)}=0 \},
\]
where $\Pi^{\tau}$ is the set of all algebraic polynomials of $\tau$ variables. By $\Pi^\tau_k\subset \Pi^\tau$, we denote the set of polynomials of total degree $k$. 

According to \cite[Theorem 4.1]{Pinkus1}, in order to find the set $\overline{\calM(\Omega)}$, it is sufficient to find the polynomials $q$ such that for all polynomials $p$ vanishing on $L(\Omega)$ we have 
\begin{equation}
\label{poly}
p(D) q=0.
\end{equation}
Here $D$ is the gradient vector; for any multiindex $m$ we can define $$D^m=\dfrac{\partial^{|m|}}{\partial z^m}.$$
Speaking of the closure of a set of functions (for instance, $\overline{\calM(\Omega)}$), we always mean the topology of uniform convergence on compact sets. Introduce the set of all polynomials satisfying \eqref{poly}:
\[
\calC_1=\mbox{\rm span}\,\{q\in \Pi^\tau: p(D) q=0 \quad \hbox{for all}\quad p\in \calP_\Omega\}.
\]
Moreover, let
\[
\calC_2=\mbox{\rm span}\,\{ q(z)=(\lan z, a\ran)^l:  l\in \Z_+, z\in \R^{\tau}, a\in \mathcal N \}
\]
where
\[
 \NN:=\mathrm{ker}\, \calP_\Omega:=\bigcap_{p\in \calP_\Omega} \mathrm{ker}\, p.
\]
Let 
\[
\calC_3=\mbox{\rm span}\,\{ f(\lan z, a\ran):  f\in C(\R,\R), a\in \mathcal N \}
\]
Now we can reformulate \cite[Theorem 4.1]{Pinkus1}.

\begin{theorem}\label{Momega}
In the topology of uniform convergence on compact sets, we have
\[
\overline{\calM(\Omega)}=\overline{\calC_1}=\overline{\calC_2}=\overline{\calC_3}.
\]
\end{theorem}
\begin{proof} The Weierstrass Theorem implies that
$\overline{\calC_2}= \overline{\calC_3}$.
It follows from the proof of \cite[Theorem 4.1]{Pinkus1} that 
\[
\overline{\calM(\Omega)}=\overline{\calC_1}=\overline{\calC_3}.
\]
\end{proof}
Consider a specific set $\Omega_S$ of $2\times 2n$ matrices of the form
\begin{equation}\label{matrixa}
A=\left[
\begin{matrix}
w & 0\cr
0 & -w
\end{matrix}
\right],\qquad 0,w\in \R^n.
\end{equation}
We will show that
\[
\calM(\Omega_S)=  \spa\,\{ g(Az): A\in \Omega_S, \quad g\in C(\R^2,\R) \},
\]
is not dense in $C(\R^{2n},\R)$ endowed with the topology of uniform convergence on compact sets.

Let the subspace $L(A)\subset \R^{2n}$ be the span of two rows of $A$ and, as we defined above,
\begin{equation}
\label{def1}
L(\Omega_S):=\bigcup_{A\in \Omega_S} L(A).
\end{equation}
Denote
\begin{equation}
\label{def2}
 \NN_S=\mathrm{ker}\, \calP_{\Omega_S}:=\bigcap_{p\in \calP_{\Omega_S}} \mathrm{ker}\, p,
\end{equation}
where
\begin{equation}
\label{def3}
\calP_{\Omega_S} =\{p\in \Pi^{2n} : p|_{L(\Omega_S)}=0 \}
\end{equation}
and $\Pi^{2n}$ is a space of polynomials of $2n$ variables. 
We prove, Theorem  \ref{almost}, that 
\[
L(\Omega_S)=\overline{L(\Omega_S)}=\NN_S.
\]

This gives a more explicit characterization of $\overline{\calM(\Omega_S)}$ by polynomials $\calC_2$, i.e.
\[
\overline{\calM(\Omega_S)}=\overline{\calC_2}
\]
where
\[
\calC_2=\mbox{\rm span}\,\{ q(z)=(\lan z, a\ran)^l:  l\in \Z_+, z\in \R^{\tau}, a\in L(\Omega_S)\}.
\]
This shows immediately that for $d=2$
such kernels
$x=(x_1,x_2), y=(y_1,y_2)$
\begin{equation}
\label{not1}
k(x,y)=(x_1^2+x_2^2)(y_1^2+y_2^2)
\end{equation}
or
\begin{equation}
\label{not2}
k(x,y)=(x_1^2-x_2^2)(y_1^2-y_2^2).
\end{equation}
do not are contained in $\calC_2$.
Since functions $g=g_1\otimes g_2$ are dense in $C(\R^{2},\R)$ with the topology of uniform convergence on compact sets, we get that in the topology of uniform convergence on compact sets, 
\begin{equation}
\label{tensor}
\overline{\calM(\Omega_S)}=\overline{\spa\,\{ g_1(\lan w, x\ran)g_2(\lan w, y\ran): w\in \R^n, g_1,g_2\in C(\R,\R) \}}.
\end{equation}
Note that all consider families $\F_{RR}, \F_{RK}\subset \overline{\calM(\Omega_S)}$.


\section{Polynomials vanishing on $L(\Omega_S)$}

Following the approach proposed by Lin and Pinkus \cite{Pinkus1}, we demonstrate that the closure $\overline{\calM(\Omega_S)}$ is not $C(\R^2,\R)$.

Let $H^{2n}_k$ denote the set of homogeneous polynomials of $2n$ variables of total degree $k$ i.e.
\[
H^{2n}_k=\left\{ \sum_{|m|=k} a_m z^m, \quad z\in \R^{2n}\right\}.
\]
For any $s\in \Z_+^n$, $|s|=k$ and $0\leq l\leq k$ we define the set
\[
\Delta_{s,l}=\big\{\overline{m} \in [0,s_1]\times \cdots \times [0,s_n]: |\overline{m}| =l\big\}.
\]
For any $0<l<k$, let
\[
\calH(\Delta_{s,l})=\{p\in H^{2n}_k : p=P_{s,l},\quad |s|=k \},
\]
and, taking $(x,y)\in \R^{2n}$, we get
\[
P_{s,l}(x,y)= \sum_{\overline{m}\in \Delta_{s,l}} a_{\overline{m},s-\overline{m}} x^{\overline{m}}
y^{s-\overline{m}}, \qquad \sum_{\overline{m}\in \Delta_{s,l}} a_{\overline{m},s-\overline{m}}=0.
\]
Let
\[
\calH_{\Omega_S,k} =\{p\in H^{2n}_k : p|_{L(\Omega_S)}=0 \}.
\]
Applying Theorem \ref{Pinkus}, we obtain the following result.

\begin{theorem}\label{l1.2}
Let $\Omega_S$ be a family of $2\times 2n$ matrices of the form \eqref{matrixa}, $n>1$. Then 
\begin{equation}
\label{equality}
\bigcup_{|s|=k,0<l<k} \calH(\Delta_{s,l}) = \calH_{\Omega_S,k}
\end{equation}
Consequently, $\calM(\Omega_S)$ is not dense in $C(\R^{2n},\R)$ with the topology of uniform convergence on compact sets.
\end{theorem}

\begin{proof}

Let $P$ be any homogeneous polynomial of $2n$ variables of total degree $k$, i.e.
for $m=(\overline{m},\underline{m})$, $\overline{m},\underline{m} \in \Z_+^n$, $x,y\in \R^n$
\[
P(z)=P(x,y)=\sum_{|m|=k} a_m x^{\overline{m}} y^{\underline{m}}=\sum_{|m|=k} a_m z^m.
\]
Here $\Z_+$ stands for the set of all non-negative integers. We use the multivariate notation. 
Let $A\in \Omega_S$. Then 
\[
L(A)=\{(x,y)=\alpha (w,0)+\beta (0,w), \alpha,-\beta\in \R \}.
\]
Hence for any vector $(x,y)\in L(A)$,
\[
P(x,y)=\sum_{|m|=k} a_m
 x^{\overline{m}} y^{\underline{m}}=\sum_{|m|=k} a_m \alpha^{|\overline{m}|} \beta^{|\underline{m}|} w^{\overline{m}+\underline{m}}.
\]
A linear operator $E:\Z^{2n} \to \Z^{2n}$
\begin{equation}
\label{linear}
E(m)=E(\overline{m},\underline{m})=(\overline{m},\overline{m}+\underline{m})=(\overline{m},s), \quad s(m)=s=\overline{m}+\underline{m}
\end{equation}
is isomorphism. Let $B=E^{-1}$. Hence $B(\overline{m},s)= (\overline{m},s-\overline{m})$ then, using $\overline{m}=(m_1,\ldots,m_n)$, $(x,y)=\alpha (w,0)+\beta (0,w)$
\[
P(x,y)=\sum_{|s|=k,s\in \Z_+^n} \sum_{m_1=0}^{s_1}\cdots \sum_{m_n=0}^{s_n}
a_{B(\overline{m},s)}\alpha^{|\overline{m}|} \beta^{k-|\overline{m}|} w^{s}.
\]
Observe that $\# \Delta_{s,l}=1$ if $|s|=l$ or $l=0$ or there is only one non-zero entry $s_j$. In all other cases, $\# \Delta_{s,l} >1$. Here $\#$ stands for the cardinality of a set. Then, for all $w\in {\R^n}$, $\alpha,\beta\in \R$,
$(x,y)=\alpha (w,0)+\beta (0,w)$,
\begin{equation}
\label{cond0}
P(x,y)=\sum_{|s|=k,s\in \Z_+^n} \sum_{l=0}^k \sum_{\overline{m}\in \Delta_{s,l}}
a_{B(\overline{m},s)}\alpha^{l} \beta^{k-l} w^{s}=0
\end{equation}
if and only if for all $s,l\leq |s|$
\begin{equation}
\label{cond}
\sum_{\overline{m}\in \Delta_{s,l}}
a_{B(\overline{m},s)}=0.
\end{equation}
This is $(k+1)\binom{n+k-1}{k}$ equations. Observe that $\binom{n+k-1}{k}$  is a dimension of space of homogeneous polynomials of $n$ variables of degree $k$. We see that if $\#\Delta_{s,l}>1$ then there exist non-zero polynomials that vanish $L(\Omega_S)$. 

Taking to account Eq.\,\eqref{cond}, we see that both
$\calH_{\Omega,k} $ and $\calH(\Delta_{s,l})$ are linear spaces, $\calH(\Delta_{s,l}) \subset
\calH_{\Omega,k}$. If $\#\Delta_{s,l}>1$ then $\dim(\calH(\Delta_{s,l}))=\#\Delta_{s,l}-1$. To see this, fix one coefficient $a_{\overline{m_0},s-\overline{m_0}}=-1$ and change the others to zero, but one equals one.
The equality \eqref{equality} is a consequence of an equivalence of \eqref{cond0} and \eqref{cond}.
\end{proof}

\section{Characterization of polynomials in $\overline{\calM(\Omega_S)}$}

Let $H^{2n}$ be a set of all  homogeneous polynomials of $2n$ variables and
\[
\calH_{\Omega_S} =\{p\in H^{2n} : p|_{L(\Omega_S)}=0 \},
\]
\[
\calP_{\Omega_S} =\{p\in \Pi^{2n} : p|_{L(\Omega_S)}=0 \},
\]
\[
 \NN_S=\mathrm{ker}\, \calP_{\Omega_S}:=\bigcap_{p\in \calP_{\Omega_S}} \mathrm{ker}\, p.
\]

\begin{lemma}\label{NS}
Then
\[
\NN_S=\mathrm{ker}\, \calH_{\Omega_S}:=\bigcap_{p\in \calH_{\Omega_S}} \mathrm{ker}\, p.
\]
\end{lemma}

\begin{proof}
Inclusion $\NN_S\subset \hbox{ker}\, \calH_{\Omega_S}$ is obvious. Let $q\in \calP_{\Omega_S}$,
\[
q(z)=\sum_{|m|\leq \rho} a_m z^m=\sum_{k=0}^\rho q_k(z),\qquad z\in \R^{2n},
\]
where $q_k$ is a homogeneous polynomial
\[
q_k(z)=\sum_{|m|=k} a_m z^m.
\]
Then for all $t\in \R$ and all $z\in L(\Omega_S)$, $tz\in L(\Omega_S)$
\[
q(tz)=\sum_{k=0}^\rho t^k q_k(z)=0.
\]
Hence for all $k=0,\ldots,\rho$   
\[
q_k(z)=0, z\in L(\Omega_S),\qquad q_k\in \calH_{\Omega_S}.
\]
 Hence
 \[
  \bigcap_{k=1}^\rho \hbox{ker}\, q_k= \hbox{ker}\, q.
 \]
 %\bigcap_{p\in \calH_{\Omega_S}} \hbox{ker}\, p \subset
 This statement finishes the proof. 
\end{proof}

\begin{theorem}\label{almost}
Let $n>1$ and $\Omega_S$ be a family of matrices such as in Lemma \ref{l1.2}. Then
\[
L(\Omega_S)=\overline{L(\Omega_S)}=\NN_S.
\]
\end{theorem}
\begin{proof}
It follows from definitions \eqref{def1}-\eqref{def3} that
\begin{equation}\label{lns}
L(\Omega_S) \subset \overline{L(\Omega_S)}\subset \NN_S.
\end{equation}
By Lemma \ref{NS}, definition of $\calH_{\Omega_S,k}$ and Theorem \ref{l1.2}, we obtain
\[
\NN_S=\mathrm{ker}\,\calH_{\Omega_S}\subset \bigcap_{|s|=k, 0<l<k} \mathrm{ker}\, \calH(\Delta_{s,l})=\mathrm{ker}\,\calH_{\Omega_S,k} 
\]
Now it suffices to prove that for sets $\calH(\Delta_{s,l})$ large enough
\[
\bigcap \mathrm{ker}\, \calH(\Delta_{s,l}) \subset  L(\Omega_S).
\]
Here the polynomials identifying $\NN_S$ are given below in Step 1- Step 3.

Let us fix $s\in \Z_+^n$, $|s|=k$. Let $\#\Delta_{s,l}>1$. From Theorem \ref{l1.2}, $\calH(\Delta_{s,l}) \subset\calH_{\Omega_S,k}$. 
For a fixed multiindex $\kappa \in \Delta_{s,l}$ the test set of polynomials is constructed as follows: $b_{s,l,\kappa,m}$ is for
\[\begin{array}{c}
m=(m_1,m_2,\ldots, m_n)\neq \kappa=(\kappa_1,\ldots,\kappa_n), \quad |m|=l, m\in \Delta_{s,l}, 
\\ s=(s_1,s_2,\ldots, s_n), |s|=k, \quad  \quad \mbox{for all} \quad 0\le j\le n,
\end{array}\]
\[
b_{s,l,\kappa,m}(x,y)=x^{\kappa}y^{s-\kappa}- x^{m}y^{s-m},
\]
where
\[
x=(x_1,\ldots,x_n),\quad y=(y_1,\ldots,y_n).
\]
Let $(x,y)\in \NN_S$. Then $b_{s,l,\kappa,m}(x,y)=0$ for all $ b_{s,l,\kappa,m} \in \calH(\Delta_{s,l}) $
if and only if for all $s$, $m$ as above
\begin{equation}\label{bslm}
x_1^{\kappa_1} x_2^{\kappa_2}\cdots x_n^{\kappa_n}  y_1^{s_1-\kappa_1} y_2^{s_2-\kappa_2}\cdots y_n^{s_n-\kappa_n}=x_1^{m_1}x_2^{m_2}\cdots x_n^{m_n} y_1^{s_1-m_1} y_2^{s_2-m_2}\cdots y_n^{s_n-m_n}.
\end{equation}
We would like to show that $(x,y)\in L(\Omega_S)$, i.e. there are $\alpha,\beta\in \R$ and a vector $w\in \R^n$ such that
\begin{equation}\label{LO}
(x,y)=\alpha(w,0,\ldots,0)+\beta (0,\ldots,0,w).
\end{equation}

\textbf{Step 1.} We show that if $x_1\neq 0,\ldots, x_n\neq 0$ then either all $y_i$ are zero, all all $y_j$ are non-zero. Take  $\kappa=(1,0,\ldots,0)$, $s=(1,1,0,\ldots,0)$,
$m=(0,1,0,\ldots,0)$. Then
\[
b_{s,l,\kappa,m}(x,y)=x_1y_2-x_2y_1=0.
\]
Hence $y_2=\frac{x_2}{x_1} y_1$. By a similar way we can show that $y_j=\frac{x_j}{x_1} y_1$ for $j=3,\ldots,n$.
Hence we prove the desired statement.

\textbf{Step 2.} Thus if all coefficients $x$ and $y$ are non-zero then
\[
x_1^{m_1-\kappa_1} x_2^{m_2-\kappa_2}\cdots x_n^{m_n-\kappa_n}  = y_1^{m_1-\kappa_1} y_2^{m_2-\kappa_2}\cdots y_n^{m_n-\kappa_n}.
\]
\[
1=\Big(\frac{x_1}{y_1}\Big)^{m_1-\kappa_1} \Big(\frac{x_2}{y_2}\Big)^{m_2-\kappa_2}\cdots \Big(\frac{x_n}{y_n}\Big)^{m_n-\kappa_n}.
\]
Without loss of generality, we can assume that $m_1<s_1$. Otherwise, we can increase $s_1$.
Take $m_1=\kappa_1+1$ and $m_i=\kappa_i-1$ and $m_j=\kappa_j$ for $j\neq 1$ and $j\neq i$. Then 
\[
1=\Big(\frac{x_1}{y_1}\Big)^{-1} \Big(\frac{x_i}{y_i}\Big).
\]
Since $i$ is selected arbitrarily, we get 
\begin{equation}\label{xty}
x_i=ty_i, \qquad i=1,2,\ldots,n.
\end{equation}
Consequently, we get \eqref{LO}. 

\textbf{Step 3.} Now let us assume that $(x,y)\in \NN_S$ is such that  $x_1=\cdots =x_i=0$ and $x_{i+1}\neq 0, \ldots, x_n\neq 0$. We want to show that $y_1=\cdots =y_i=0$.
Take $\kappa_1=\cdots=\kappa_i=0,\kappa_{i+1}=1,\ldots,\kappa_n=1$,
$s_1=1,s_2=\cdots = s_i=0,s_{i+1}=1,\ldots,s_n=1$, $m_1=1,m_2=\cdots =m_i=m_{i+1}=0$, $m_{i+2}=\cdots =m_n=1$.
Since $b_{s,l,\kappa,m}(x,y)=0$ then $y_1=0$ in similar way we get $y_2=\cdots =y_i=0$. Now by Step 1, 
$y_j=\frac{x_j}{x_1} y_1$ for $j=2,\ldots,n$. Hence, by Step 2, there is $t$ such that
\[
t x_i=y_i, \qquad i=1,\ldots,n.
\]
Then \eqref{LO} is satisfied. 

\end{proof}

\section{Two application of Universal Approximation Theorem}\label{UAT}

The following theorem is similar to the RR result, but there are some differences. The theorem below states that a symmetric function
  $ k: \R^{2n}\to \R$ has an approximant given by \eqref{kRR} 
 but without any restrictions on the coefficients $\{c_j\}$. 
If we add one assumption, that 
 for any finite set of points \( \{x_1, x_2, \ldots, x_n\} \subset X \), the Gram matrix \( G_n \) defined by \( G_n =[ k(x_i, x_j)]_{1\leq i,j\leq n} \) is positive definite,and  hence $k$ is Mercer kernel, RR result  assures that  an approximant given by \eqref{kRR} can be chosen such that  $c_j>0$.
 
However, even an approximant with some negative coefficients can still be a Mercer kernel, as shown below. Moreover, we cannot claim that an approximant with positive coefficients is necessarily better than one with arbitrary coefficients.

\begin{theorem}\label{th_appr} Let $ {\mathbf k}: \R^{2n}\to \R$ be a continuous function such that there exists a real valued continuous function $\tilde{{\mathbf k}}:\R^d\to \R $ such that  ${\mathbf k}(x,y)=\tilde{{\mathbf k}}(x-y)$ (we called such kernels shift invariant).
Then $k\in \overline{\calM(\Omega_S)}$.

In addition, for any compact set $K\subset \R^{2n}$, and for every $\epsilon>0$ there exist $M_1,M_2$, coefficients $c_j\in \R$ and
parameters $t_k\in \R$, $w_j\in \R^n$ such that for all $(x,y)\in K $
\begin{equation}
\label{calka2}
\left|{\mathbf k}(x,y)- \sum_{j=1}^{M_1} \sum_{k=1}^{M_2} c_j/M_2 \cos(\lan w_j, x\ran+t_k)\cos(\lan w_j, y\ran+t_k)\right|<\epsilon.
\end{equation}
\end{theorem}

\begin{proof}
Let $\mathbf k$ be a symmetric function, such that ${\mathbf k}(x,y)=\tilde{{\mathbf k}}(x-y)$ (as in the statement of the theorem). Since $K\subset \R^{2n}$ is the compact set then there is a compact set $\tilde{K}\subset \R^n$ such that if $(x,y)\in K$ then $x-y\in \tilde{K}$ and  $y-x\in \tilde{K}$.       
Let  $\sigma:\R\to \R$ be a so-called activating function, which is continuous and not a polynomial.  By \cite[Proposition 3.7]{Pinkus2}, the space
\[
\spa\{\sigma(\lambda t- \theta): \lambda,b\in \R  \},\qquad t\in \R,
\]
is dense in $C(\R,\R)$ endowed with the topology of uniform convergence on compact sets.  Then by \cite[Proposition 3.3]{Pinkus2}, 
\[
\spa\{\sigma( \lan w,x\ran-b): w\in \R^n, b\in \R \},
\]
is dense in $C(\R^n)$ with the topology of uniform convergence on compact sets. Hence, we have
 \[
\tilde{{\mathbf k}}\in \overline{\spa}\{\sigma( \lan w,x\ran-b): w\in \R^n, b\in \R \}
\]
Consequently, for all $\epsilon>0$ and $u\in \tilde{K}$, there exist constants $M$, $c_j\in \R$, $b_j\in \R$ 
\begin{equation}
 \label{rep0}
\left|\tilde{{\mathbf k}}(u)- \sum_{j=1}^M c_j \sigma (\lan w_j, u\ran-b_j)\right|<\epsilon.
 \end{equation}
Let  $a_j=(w_j,-w_j)\in \R^{2n}$. Then  for $(x,y)\in K\subset \R^{2n}$
 \begin{equation}
 \label{rep}
\left|{\mathbf k}(x,y)- \sum_{j=1}^M c_j \sigma (\lan a_j, (x,y)\ran-b_j)\right|<\epsilon.
 \end{equation}
Note that $a_j=(w_j,-w_j)\in L(\Omega_S)$.
By \cite[Theorem 4.1 (2)]{Pinkus1} (see  also Theorem \ref{Momega}) and taking into account the inclusion \eqref{lns}, we get ${\mathbf k}\in \overline{\calM(\Omega_S)}$.

By Eq.\, \eqref{rep}, we get that for $\sigma=\cos$ and any $\epsilon>0$ there exist constants $M$, $c_j\in \R$, $b_j\in \R$ and vectors $a_j=(w_j,-w_j)\in \R^{2n}$ such that
\[
\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(b_j)+\sin (\lan w_j,x-y\ran)\sin(b_j)\Big)\right|<\epsilon,\qquad (x,y)\in K.
\]
Now we show that also
\begin{equation}
\label{claim}
\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(b_j)\Big)\right|<\epsilon,\qquad (x,y)\in K.
\end{equation}
Indeed,
\[
2\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(b_j)\Big)\right|
\]
\[
\leq \left|2{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(2\cos (\lan w_j,x-y\ran)\cos(b_j)-\sin (\lan w_j,x-y\ran)\sin(b_j)
+\sin (\lan w_j,x-y\ran)\sin(b_j\Big)\right|
\]
\[
\leq \left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(b_j)-\sin (\lan w_j,x-y\ran)\sin(b_j)\Big)\right|
\]
\[
+\left|{\mathbf k}(x,y)-  \sum_{j=1}^M c_j \Big(\cos (\lan w_j,x-y\ran)\cos(b_j)+\sin (\lan w_j,x-y\ran)\sin(b_j)\Big)\right|
\]
Take $(x,y)\in K$. Then, by definition of $\tilde K$, we have $x-y\in \tilde{K}$ and $y-x\in \tilde{K}$. Since ${\mathbf k}(x,y)={\mathbf k}(y,x)$, and since the function $\cos$ is even and $\sin$ is odd, we get the claim \eqref{claim}.  It follows from trigonometric formulas, that  
\[
\int_0^{2\pi} 2\cos(\alpha+t)\cos(\beta+t) dt=
\int_0^{2\pi} \left( \cos(\alpha-\beta)+\cos(\alpha+\beta+2t)\right) dt=2\pi \cos(\alpha-\beta).
\]
Putting $\alpha=\lan w_j, x\ran, \beta=\lan w_j, y\ran$, we obtain that for $\epsilon>0$ there are $M$, $\tilde{c}_j\in \R$, $\theta_j\in \R$ and vectors $w_j\in \R^n$
\begin{equation}
\label{calka}
\left|{\mathbf k}(x,y)- \sum_{j=1}^M \tilde{c}_j \int_0^{2\pi} \cos(\lan w_j, x\ran+t)\cos(\lan w_j, y\ran+t) dt
\right|<\epsilon, \qquad (x,y)\in K.
\end{equation}
Now we use the uniform approximation of the integral 
\[
\int_0^{2\pi} \cos(a+t)\cos(b+t) dt,
\] 
on the interval $[0,2\pi]$. Namely, for any $a,b\in \R$ and any $\epsilon>0$ there is $M_2>0$ such that for 
$\{t_k= 2\pi k/M_2,\quad k=1,\ldots, M_2\}$ we have
\[
\left| \int_0^{2\pi} \cos(a+t)\cos(b+t) dt-\frac{1}{M_2}\sum_{k=1}^{M_2} \cos(a+t_k)\cos(b+t_k)\right|<\epsilon.
\]
Thus we prove the theorem.
\end{proof}

Our selection of an approximant of  symmetric functions involves a further discussion. The properties of the approximant of  ${\mathbf k}$ depend on the choice of the domain $X\subset \R^{2n}$. So, selecting the parameters $c_j\in \R$ in \eqref{calka2}, the approximant can be  the Mercer kernel. Indeed let us consider symmetric function  given by formula
\[
{\mathbf k}(x,y)= \sum_{j=1}^{L}  f_j(\lan x, w_j\ran) f_j(\lan y, w_j\ran)-\sum_{j=L+1}^{M}  f_j(\lan x, w_j\ran) f_j(\lan y, w_j\ran),
\] 
where $w_j\in \R^n, x,y\in X$, $f_{j}\in C(\R,\R)$ for all $j=1,\ldots, M$. 
   
\begin{theorem}
Let $X=\{x_1,\ldots,x_n\}$, 
\[
{\mathbf k}(x,y)= \sum_{j=1}^{L}  f_j(\lan x, w_j\ran) f_j(\lan y, w_j \ran)-\sum_{j=L+1}^{M}  f_j(\lan x, w_j \ran) f_j(\lan y, w_j \ran),
\] 
$f_{j}\in C(\R,\R)$ for all $j$.
Let
$v_j=(f_j(\lan x_1, w_j\ran),\ldots, f_j(\lan x_n, w_j\ran))$, $j=1,\ldots, M$.
 Then ${\mathbf k}$ is a Mercer kernel on $X$ iff for all $a\in \Sp^n$
 \begin{equation}\label{other}
 \sum_{j=1}^{L} \lan v_j, a\ran^2 \geq  \sum_{j=L+1}^{M} \lan v_j,  a\ran^2.
 \end{equation}
Thus if $\{v_j, j=1,\ldots,L\}$ is a frame with lower frame bound $A$ and $\{v_j, j=L+1,\ldots,M\}$  is a frame with and upper frame bound $B$ and $A\geq B$ then ${\mathbf k}$ is Mercer kernel.
\end{theorem}
 
\begin{proof}
 Note that the kernel
 \[
{\mathbf k}_j(x,y)=f_j(\lan x, w_j \ran)f_j(\lan y, w_j \ran), x,y\in X\subset \R^n
 \]
is a Mercer kernel. Indeed, ${\mathbf k}_j$ is symmetric, continuous, and positive definite since for all $a\in \R^N$ and $x_1,\ldots,x_n\in X$,
\[
a' K_j a=\lan v_j'  a, v_j'  a\ran =\lan v_j, a \ran^2  \geq 0,
\]
$a'$ stands for transposition of the vector $a$ where  $K_j =[{\mathbf k}_j(x_i,x_k)] _{1\leq i,k\leq n}$. Hence 
 \[
 {\mathbf k}=\sum_{j=1}^{L} {\mathbf k}_j-\sum_{j=L+1}^{M} {\mathbf k}_j
 \]
is a Mercer kernel if and only if for all $a\in \R^N$ and $x_1,\ldots,x_N\in \R^n$ and $K =[{\mathbf k}(x_i,x_k)] _{1\leq i,k\leq n}$ we have
  \[
 a' K a=\sum_{j=1}^{L} \lan v_j, a\ran^2 -\sum_{j=L+1}^{M} \lan v_j,  a\ran^2\geq 0.
 \]
In other words, for all $a\in \Sp^n$
 \[
 \sum_{j=1}^{L} \lan f_j, a\ran^2 \geq  \sum_{j=L+1}^{M} \lan f_j,  a\ran^2.
\]
The sufficient condition follows from the definition of a frame.
 \end{proof}
 
\begin{theorem}
Consider the family of functions depending on parameters $\theta=(c,w,b,\sigma)$ such that let
 $m\in {\mathbb N}$ and $c=\{c_j\}, w=\{w_j\}, b=\{b_j\},\sigma\in C(\R,\R)$
\[
{\mathbf k}(x,y|\theta)= \sum_{j=1}^{m} c_j \sigma(\lan x, w_j\ran+b_j) \sigma(\lan y, w_j\ran+b_j), \quad x,y,w_j\in \R^n, c_j>0,b_j\in \R,
\]
where $\sigma$ is not polynomial. Then such family is universal.
\end{theorem} 

\begin{proof}
We need to prove that for any $f\in C(\R^n,R)$ any compact subset $K\subset \R^n$ and any $\epsilon>0$ there is 
 a kernel
\[
{\mathbf k}(x,y)= \sum_{j=1}^{m} c_j \sigma(\lan x, w_j\ran+b_j) \sigma(\lan y, w_j\ran+b_j), \quad x,y,w_j\in \R^n, c_j>0,b_j\in \R,
\]
such that there are points $x_l\in K$ such that
\[
\sup_{x\in K}|f(x)-\sum_{l=1}^s d_l {\mathbf k}(x,x_l)|<\epsilon.
\]
By Universal Approximation Theorem we find for $f$, $K$ and $\epsilon>0$   a function
\[
h(x)=\sum_{j=1}^s a_j \sigma(\lan x,v_j\ran+\beta_j)
\]
such that
\[
\sup_{x\in K}|f(x)-h(x)|<\epsilon.
\]
Taking another representation of $h$ we  we can assume that we take functions
\[
\phi_j(x)=\sigma(\lan x,v_j\ran+\beta_j)
\]
to be linear independent over $x\in K$.
Hence to prove the theorem it is sufficient find coefficients $d_l$ and $x_l\in K$, $l=1,\ldots,s$
such that
\[
h(x)=\sum_{l=1}^s d_l {\mathbf k}(x,x_l),
\]
where the kernel ${\mathbf k}$ is taken such that $m=s$ and the parameters $w_l=v_l$ and $b_l=\beta_l$.
Then
\[
\sum_{l=1}^s d_l {\mathbf k}(x,x_l)= \sum_{l=1}^s d_l \sum_{j=1}^{s} c_j \sigma(\lan x, v_j\ran+\beta_j) \sigma(\lan x_l, v_j\ran+\beta_j).
\]
Consequently
\[
\sum_{l=1}^s d_l {\mathbf k}(x,x_l)= \sum_{j=1}^{s} c_j \Big(\sum_{l=1}^s d_l   \sigma(\lan x_l, v_j\ran+\beta_j)
\Big)
\sigma(\lan x, v_j\ran+\beta_j).
\]
Hence we need to prove that 
we can  chose $d_l,c_l$ and  $x_l\in K$, $l=1,\ldots,s$ such that for all $j=1,\ldots, s$
\[
a_j=c_j \Big(\sum_{l=1}^s d_l   \sigma(\lan x_l, v_j\ran+\beta_j)
\Big)=c_j \Big(\sum_{l=1}^s d_l  \phi_j(x_l)\Big)
\]
or
that the matrix $[\sigma(\lan x_l, v_j\ran+\beta_j)]_{l,j=1}^s=[\phi_j(x_l)]_{l,j=1}^s$ is inverible.
Since we assume that $\{\phi_j\}_{j=1}^s$ are 
linear independent over $x\in K$  we will find $x_l$ such that the matrix is inverible.

\end{proof}
 
\section{Criteria of choosing Mercer kernels in learning theory }

Now we study how to select the optimal Reproducing Kernel Hilbert Space $\calH_k$ (RKHS) for a learning process, see \cite{Cucker}, \cite{Smale}. Let ${\mathbf k}$ be a Mercer kernel corresponding to $\calH_k$. We take
a  feature map of the following  form. Let $\theta=(c,w,b,\sigma)$ denote the parameters,
such that for any  $m\in {\mathbb N}$ we take parameters  $c=\{c_j\}, w=\{w_j\}, b=\{b_j\},\sigma\in C(\R,\R)$
\begin{equation}
\label{propose}
{\mathbf k}(x,y|\theta)= \sum_{j=1}^{m} c_j \sigma(\lan x, w_j\ran+b_j) \sigma(\lan y, w_j\ran+b_j), \quad x,y,w_j\in \R^n, c_j>0,b_j\in \R.
\end{equation}


Now we present a simplified version of problems of learning theory.
Let $X$ be a compact subset of $\R^n$ and $Z = X \times Y$. Let $\rho$ be a probability measure on $Z$ and $\rho_X, \rho_{Y |x}$  be the induced marginal probability measure on $X$ and conditional probability measure on $\mathbb R$ conditioned on $x\in X$, respectively. Define $f_\rho : X \to {\mathbb R}$ as follows:
\[
f_\rho(x)=\int_Y y d\rho_{y|x}.
\]
This is the regression function of $\rho$. We assume that $f_\rho \in L^2(\rho_X)$. 
As a possible method of finding $f_\rho$ one can minimize the regularized least square problem (variation problem) in $\calH_k$
$ \inf_{f \in \calH_k} \calL(f)$
where
\begin{equation}
\label{functional}
\calL(f)=
\int_Z (f (x) -y)^2 d\rho(x,y) + \lambda \|f\|_{\calH_k}^2,\qquad \lambda >0.
\end{equation}
\cite[Proposition 7 in Chapter III]{Cucker} guarantees the existence and uniqueness of a minimizer.
Usually, the measure $\rho$ is unknown. Now we consider the sampling, let
\[
z = (z_1,\ldots,z_N)=((x_1 , y_1),\ldots , (x_N, y_N))
\]
be a sample in $Z^N$, i.e. $n$ examples independently drawn according to $\rho$.
In the context of RKHS, given a sample $z$, 
discrete version of \eqref{functional} 
\begin{equation}
\label{functional1}
\calL_N(f)=\frac{1}{N}\sum_{i=1}^N(f (x_i ) -y_i )^2 + \lambda \lan f, f \ran_{\calH_k},
\end{equation}
and “batch learning”\ means
solving the regularized least square problem 
\begin{equation}
\label{variation}
f_{\lambda,z} = \mbox{arg\,min}_{f\in \calH_k}\,
\calL_N(f),
\quad
\lambda > 0,
\end{equation}
see \cite[Comparison with “Batch Learning” Results]{Smale}
and \cite{2021}.
The existence and uniqueness of $f_{\lambda,z}$ given as in \cite [Section 6]{Cucker} says
\[
f_{\lambda,z} =
\sum_{i=1}^N
a_i {\mathbf k}(x,x_i).
\]
where $a = (a_1 ,\ldots, a_N)$ is the unique solution of the well-posed linear system in $\R^n$,
where 
\[
a=(K_N+\lambda N I)^{-1}y,\qquad y=(y_1,\ldots,y_N),\qquad K_N=[{\mathbf k}(x_i,x_j)]_{1\leq i,j\leq N}.
\]
\cite [Section 6]{Cucker}.
In order to emphasize  the connection kernels $k$ with parameters $\theta$,  we use the notion:
\[
 K_{N,\theta}=[{\mathbf k}(x_i,x_j|\theta)]_{1\leq i,j\leq N}
 \]
\begin{lemma} 
\label{61}
 Let $z \in  Z^N$. For $\lambda>0$
\[
\calL_N(f_{\lambda,z})= \lambda y'  \left(K_N+ \lambda N I\right)^{-1}y
\]
\end{lemma}
\begin{proof}
Now let put $f_{\lambda,z}$ and calculate
\[
\calL_N(f_{\lambda,z})=\frac{1}{N}\sum_{j=1}^N(f_{\lambda,z} (x_j ) -y_j )^2 + \lambda \lan f_{\lambda,z}, f_{\lambda,z} \ran_{\calH_k},
\]
Note that
\[
\frac{1}{N}\sum_{i=1}^N(f_{\lambda,z} (x_i ) -y_i )^2= \frac{1}{N}\|K_N(K_N+\lambda N I)^{-1} y-y\|^2.
\]
But
\[
K_N(K_N+\lambda N I)^{-1}-I=-\lambda N(K_N+\lambda N I)^{-1}.
\]
From definition of ${\mathbf k}$ and $a$
\[
\lan f_{\lambda,z}, f_{\lambda,z} \ran_{\calH_k}=a'K_N a=y'(K_N+\lambda N I)^{-1} K_N (K_N+\lambda N I)^{-1}y.
\]
Hence
\[
\calL_N(f_{\lambda,z})=N\lambda^2\|(K_N+\lambda N I)^{-1} y \|^2 + \lambda y'(K_N+\lambda N I)^{-1} K_N (K_N+\lambda N I)^{-1}y.
\]
Consequently,
\[
\calL_N(f_{\lambda,z})=N\lambda^2 y' (K_N+\lambda N I)^{-1} (K_N+\lambda N I)^{-1} y  + \lambda y'(K_N+\lambda N I)^{-1} K_N (K_N+\lambda N I)^{-1}y.
\]
\[
\calL_N(f_{\lambda,z})=\lambda\big( y' (K_N+\lambda N I)^{-1} (\lambda N I  +  K_N) (K_N+\lambda N I)^{-1}y\big)
\]
\[
\calL_N(f_{\lambda,z})=\lambda\big( y'  (K_N+\lambda N I)^{-1}y\big)
\]
\end{proof}
%\section{Numerical simulation and conclusions} 
The Von Neumann series gives that
\[
\left(\frac{1}{\lambda N}K_N+ I\right)^{-1}=\sum_{j=0}^\infty (-1)^j \left( \frac{1}{\lambda N}\right)^j K^j_N
\]
The series converge in norm topology if ${\lambda}N > \|K_N\|$.  So for any $\epsilon>0$ there is $L>0$ such that
\[
\left\|\left(\frac{1}{N \lambda}K_N+ I\right)^{-1}-\sum_{j=0}^L (-1)^j \left( \frac{1}{\lambda N}\right)^j K^j_N\right\|<\epsilon
\]
Now by Spectral Theorem, we can decompose  matrix $K_N$ as
\[
K_N=K_{N,\theta}=U' \Lambda  U=U_\theta' \Lambda_\theta  U_\theta,
\]
where $U$ is an orthonormal  matrix consisting of eigenvectors of of $K_N$ and $\Lambda$ is a diagonal matrix
consisting of eigenvalues of of $K_N$. Hence $K^j_{N,\theta}=U_\theta' \Lambda_\theta^j  U_\theta$.

Our first proposition on estimation of the best parameters is the following. Given a number $m$, a sample $z$, and $\lambda>0$
we find parameters $\theta$ such that
\[
\min_{\theta} \calL_N(f_{\lambda,z})=\min_{\theta}
\lambda\big( y'  (K_{N,\theta}+\lambda N I)^{-1}y\big).
\]
A similar approach is possible for given  $L$. Let $\lambda_{k,\theta}, k=1,\ldots,N$ be a sequence of eigenvalues of 
$K_{N,\theta}$, $\lambda_{k,\theta}\geq  \lambda_{k+1,\theta}$. Let ${\lambda}N> \lambda_{1,\theta}$. A criterion 
for optimal $\theta$
\begin{equation}
\label{Newman}
\min_{\theta} \frac{1}{N} y'U_\theta' \left( \sum_{j=0}^L (-1)^j \left( \frac{1}{\lambda N}\right)^j \Lambda^j_{n,\theta} \right) U_\theta y.
\end{equation}
it is assumed in \cite{Binev} that this probability measure $\rho_{y|x}$ is supported on an interval.  We can assume that $|y|\leq 1$.

 Taking to account Theorem \ref{th_appr} we propose
$g=\cos$ and $g=\mbox{Re}\,LU$.



\section{Experimental results}

Iris is a labeled dataset of $n = 150$ flowers of $k=3$ species, namely \textit{setosa}, \textit{virginica} and \textit{versicolor} (50 each). For each observation we have 4 measurements of length and width, so original feature space is $\mathcal{X} = \mathbb{R}^4$ and $\mathcal{Y} = \lbrace 1, 2, 3 \rbrace$. As a preprocessing step we shifted and scaled each feature to have zero mean and unit variance across whole sample.

In order to classify single observation into one of three available labels we use one-vs-all strategy, i.e. for each $k=1,2,3$ we create new labels $y_i^* = \mathbf{1}_{\lbrace y_i = k \rbrace}$ and then fit binary classifier $f_k$. Then $i$-th observation is classified by evaluating predictions from all $k$ classifiers and assigning the label corresponding to the classifier with the highest $f_k(x_i)$ value.

In our experiment we choose the following parameters:

\begin{enumerate}
	\item $m = 2$ - number of terms in Mercer kernel approximation, which gives us $m(2+d) = 12$ parameters $\theta = (c, b, w)$ to be estimated
	\item $\lambda = 0.01$ - regularization parameter.
\end{enumerate}

Due to relatively small dataset it was possible to apply Lemma \ref{61} by inverting relevant matrix and minimizing loss function directly. Additionally, we approximated inverse matrix with Neumann series by minimizing \eqref{Newman}. We performed numerical optimization using L-BFGS-B algorithm.

During the experiment we encountered problems with numerical stability of two kinds:

\begin{enumerate}
	\item signularities when inverting matrix $K_{\lambda} = K_N + \lambda n I$, which appears in Lemma \ref{61} and during solving linear system $a = K_{\lambda}^{-1} y$
	\item infinity in Neuman approximation of inverse matrix.
\end{enumerate}

We overcame the first problem by QR decomposition of $K_{\lambda}$ matrix. For Neumann approximation we used $L=5$. Here we observed high variance of accuracy when conducting this experiment repeatedly, suggesting that L-BFGS-B optimization with Neumann approximation is sensitive to random initialization of parameters $\theta$. In all simulations we drew initial parameters independently from the uniform and normal distributions:

$$
\begin{cases}
c_j \sim \mathcal{U}(0,1) \\
b_j \sim \mathcal{N}(0,1) \\
w_{ij} \sim \mathcal{N}(0,1)
\end{cases}
$$

where $i=1,2,3,4$ is dimension of input space $\mathcal{X}$ and $j = 1,2$ is number of terms in Mercer kernel approximation of form 1.1.

% Figure environment removed

% Figure environment removed

% Figure environment removed

Numerical results are presented in Table 1. True labels and predicted labels are presented on Figures 1-3. Shifted and scaled dataset is projected onto spaced spanned by first two principal components. $PC_1$ and $PC_2$ together explain $95.81\%$ of dataset's variance.

\begin{table}[h!]
\begin{tabular}{lll}
\hline
\textbf{Method}         & \textbf{Function} & \textbf{Accuracy}   \\ \hline
Inverted QR matrix      & ReLU              & $0.9$               \\ \hline
Inverted QR matrix      & $\cos$             & $0.98$              \\ \hline
Neumann approx. ($L=5$) & ReLU              & optimization failed \\ \hline
Neumann approx. ($L=5$) & $\cos$             & $0.8267$            \\ \hline
\end{tabular}
\end{table}




\bibliographystyle{amsplain}

\begin{thebibliography}{99}

\bibitem{Belkin} 
M.\, Belkin, {\it Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation}. Acta Numerica, 30, 203-248, 2021.

\bibitem{Binev}
P. Binev, A. Cohen, W. Dahmen, R. DeVore and V. Temlyakov, Universal algorithms
for learning theory. Part I : piecewise constant functions, J. Machine Learning Res.
6 (2005), 1297–1321



\bibitem{SVM}
A.\, Christmann and I.\, Steinwart, {\it Support Vector Machines}. Springer, Berlin, 2008.

\bibitem{Cucker}
Cucker, F. and S. Smale {\it On the mathematical foundations of learning}. Bull. of the
Amer. Math. Soc. 29 (1), 1–49.

\bibitem{2021}
E.\, De Vito, L.\, Rosasco, and A.\, Rudi {\it Regularization: From Inverse Problems
to Large-Scale Machine Learning} 245-296.
Harmonic and Applied Analysis
From Radon Transforms to Machine Learning
Book Series: Applied and Numerical Harmonic Analysis
Springer International Publishing 2021



\bibitem{LeCun}
Y.\, LeCun, B.\, Boser, J.\, S.\, Denker, D.\, Henderson, R.\, E.\, Howard, W.\, Hubbard and L.\, D.\, Jackel: \emph{Backpropagation Applied to Handwritten Zip Code Recognition}, Neural Computation, 1(4):541-551, Winter 1989.

\bibitem{Micc}
Micchelli, Charles A.; Xu, Yuesheng; Zhang, Haizhang Universal kernels. J. Mach. Learn. Res. 7 (2006), 2651-2667. 

\bibitem{mercer} J.\, Mercer, \emph{Functions of positive and negative type and their connection with the theory of integral equations}, Philosophical Transactions of the Royal Society A, 209 (441–458): 415–446, 1909.

\bibitem{Pinkus1}
  V.\, Ya.\, Lin, A.\, Pinkus, {\it Fundamentality of ridge functions.} J. Approx. Theory 75 (1993), no. 3, 295-311.

\bibitem{Pinkus2}
  A.\, Pinkus, {\it Approximation theory of the MLP model in neural networks}. Acta numerica, 1999, 143-195, Acta Numer., 8, Cambridge Univ. Press, Cambridge, 1999. 
  
\bibitem{Rahimi}
A.\, Rahimi and B.\, Recht, {\it Random features for large-scale kernel machines},  Advances in Neural Information Processing Systems, pages
1177-1184, 2008  
  
 \bibitem{Smale}

Smale, Steve; Yao, Yuan {\it Online learning algorithms},
Found. Comput. Math. 6 (2006), no. 2, 145-170.  
  
\end{thebibliography}

\end{document}

where $\Lambda_\theta^{1/2}$ is a square root of $\Lambda_\theta$. 
Let
\[
h(x)=1-x+x^2-x^3+\cdots=\frac{1-x^{N+1}}{1+x},\qquad x\geq 0.
\]
 We obtain min-max criteria to obtain the best estimation of $\theta$, i.e.
\[
\widehat{\theta} =argmin_{\theta}  \sqrt{h^2\left(\sqrt{\lambda_{1,\theta}/\lambda}\right)+\cdots h^2\left(\sqrt{\lambda_{n,\theta}/\lambda}\right)}\]

\begin{lemma} Let $\Omega_S$ be given, $n>1$.
Let $s\in \Z_+^n$. Let $m=(0,s)$ or $m=(s,0)$. Then a polynomial $q(z)=z^m$, $z\in \R^{2n}$
belongs to $\calC_1$.
\end{lemma}

\begin{proof}
Note that if we have a monomial $p_\tau(z)=z^\tau$, $\tau\in \Z_+^{2n}$  then 
 $p_\tau(D)q\neq 0$ if and only if $\tau_j\leq m_j$ for all $j=1,\ldots, 2n$.
 By \eqref{cond} we see that such $p_\tau \notin \calP_\Omega$ since 
 $\#\Delta_{s(\tau),l}=1$ for both $l=0$ and $l=|\tau|$. Recall $s(\tau)=\overline{\tau}+\underline{\tau}$. In our case $\underline{\tau}=0$ or $\overline{\tau}=0$ for fixed $m$.
 Hence, $p(D)q=0$ for all $p\in \calP_\Omega$.
\end{proof}
