%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Introduction}
\label{introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% New order
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Discrete variational autoencoders based on categorical distributions \cite{jang2017categorical,maddison2017concrete} or vector quantization \cite{van2017neural} have enabled recent success in large-scale image generation \cite{van2017neural,razavi2019generating}, model-based reinforcement learning \cite{hafner2020mastering,ozair2021vector,hafner2023mastering}, and perhaps most notably, in text-to-image generation models like Dall-E \cite{ramesh2021zero} and Stable Diffusion \cite{rombach2022high}. Prior work has argued that discrete representations are a natural fit for complex reasoning or planning \cite{jang2017categorical,ramesh2021zero,ozair2021vector} and has shown empirically that a discrete latent space yields better generalization behavior \cite{hafner2020mastering,friede2021efficient,rombach2022high}. Hafner et al. \cite{hafner2020mastering} hypothesize that the sparsity enforced by a vector of discrete latent variables could encourage generalization behavior. However, they admit that "we do not know the reason why the categorical variables are beneficial."

We focus on an extensive study of the \emph{structural impact} of discrete representations on the latent space. 
The disentanglement literature \cite{bengio2013representation,higgins2018towards,locatello2019challenging} provides a common approach to analyzing the structure of latent spaces. Disentangled representations \cite{bengio2013representation} recover the low-dimensional and independent ground-truth factors of variation of high-dimensional observations. Such representations promise interpretability \cite{higgins2018towards,adel2018discovering}, fairness \cite{locatello2019fairness,creager2019flexibly,trauble2021disentangled}, and better sample complexity for learning \cite{scholkopf2012causal,bengio2013representation,peters2017elements,van2019disentangled}. State-of-the-art unsupervised disentanglement methods enrich \emph{Gaussian} variational autoencoders \cite{kingma2013auto} with regularizers encouraging disentangling properties \cite{higgins2017beta,kumar2017variational,burgess2018understanding,kim2018disentangling,chen2018isolating}. Locatello et al.~\cite{locatello2019challenging} showed that unsupervised disentanglement without inductive priors is theoretically impossible. Thus, a recent line of work has shifted to weakly-supervised disentanglement \cite{locatello2019disentangling,shu2019weakly,locatello2020weakly,klindt2021towards}.

%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
%
%
%
We focus on the impact on disentanglement of replacing the standard variational autoencoder with a slightly tailored \emph{categorical} variational autoencoder \cite{jang2017categorical,maddison2017concrete}.
Most disentanglement metrics assume an ordered latent space, which can be traversed and visualized by fixing all but one latent variable \cite{higgins2017beta,chen2018isolating,eastwood2018framework}. Conventional categorical variational autoencoders lack sortability since there is generally no order between the categories.
For direct comparison via established disentanglement metrics, we modify the categorical variational autoencoder to represent each category with a \emph{one-dimensional} representation.
While regularization and supervision have been discussed extensively in the disentanglement literature, the variational autoencoder is a component that has mainly remained constant.
At the same time, Watters et. al \cite{watters2019spatial} have observed that Gaussian VAEs might suffer from rotations in the latent space, which can harm disentangling properties.
We analyze the rotational invariance of multivariate Gaussian distributions in more detail and show that the underlying grid structure of categorical distributions mitigates this problem and acts as an efficient inductive prior for disentangled representations.
We first show that the observation from \cite{burgess2018understanding} still holds in the discrete case, in that neighboring points in the data space are encouraged to be also represented close together in the latent space. Second, the categorical latent space is less rotation-prone than its Gaussian counterpart and thus, constitutes a stronger inductive prior for disentanglement as illustrated in Figure~\ref{fig:latent_distance}. Third, the categorical variational autoencoder admits an unsupervised disentangling score that is correlated with several disentanglement metrics. Hence, to the best of our knowledge, we present the first disentangling model selection based on unsupervised scores.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Preliminaries
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Disentangled Representations} \label{sct:discrete}
The disentanglement literature is usually premised on the assumption that a high-dimensional observation $\bm{x}$ from the data space $\mathcal{X}$ is generated from a low-dimensional latent variable $\bm{z}$ whose entries correspond to the dataset's ground-truth factors of variation such as position, color, or shape \cite{bengio2013representation,tschannen2018recent}. First, the \emph{independent} ground-truth factors are sampled from some distribution $\bm{z}\sim p(\bm{z})=\prod{p(z_i)}$. The observation is then a sample from the conditional probability $\bm{x}\sim p(\bm{x}|\bm{z})$. The goal of disentanglement learning is to find a representation $r(\bm{x})$ such that each ground-truth factor $z_i$ is recovered in one and only one dimension of the representation. The formalism of variational autoencoders \cite{kingma2013auto} enables an estimation of these distributions. Assuming a known prior $p(\bm{z})$, we can depict the conditional probability $p_{\theta}(\bm{x}|\bm{z})$ as a parameterized probabilistic decoder. In general, the posterior $p_{\theta}(\bm{z}|\bm{x})$ is intractable. Thus, we turn to variational inference and approximate the posterior by a parameterized probabilistic encoder $q_{\phi}(\bm{z}|\bm{x})$ and minimize the Kullback-Leibler (KL) divergence
$\kl{q_{\phi}(\bm{z}|\bm{x})}{p_{\theta}(\bm{z}|\bm{x})}$.
This term, too, is intractable but can be minimized by maximizing the evidence lower bound (ELBO)
\begin{equation}\label{eq:elbo}
    \mathcal{L}_{\theta,\phi}(\bm{x}) =
    \E{q_{\phi}(\bm{z}|\bm{x})}{\log p_{\theta}(\bm{x}|\bm{z})} -
    \kl{q_{\phi}(\bm{z}|\bm{x})}{p(\bm{z})}.
\end{equation}
State-of-the-art unsupervised disentanglement methods assume a Normal prior %distribution
$p(\bm{z}) = \mathcal{N}\bigl(\bm{0},\bm{I}\bigr)$
as well as an amortized diagonal Gaussian for the approximated posterior distribution
$q_{\phi}(\bm{z}|\bm{x})=\mathcal{N}\bigl(\bm{z}\;|\;\bm{\mu}_{\phi}(\bm{x}), \bm{\sigma}_{\phi}(\bm{x})\bm{I}\bigr)$.
They enrich the ELBO with regularizers encouraging disentangling \cite{higgins2017beta,kumar2017variational,burgess2018understanding,kim2018disentangling,chen2018isolating} and choose the representation as the mean of the approximated posterior $r(\bm{x})=\bm{\mu}_{\phi}(\bm{x})$ \cite{locatello2019challenging}.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
%
%
%
\subsubsection{Discrete VAE.}
We propose a variant of the categorical VAE modeling a joint distribution of $n$ \emph{Gumbel-Softmax} random variables \cite{jang2017categorical,maddison2017concrete}.
Let $n$ be the dimension of $\bm{z}$, $m$ be the number of categories,
$\alpha_i^j\in(0,\infty)$ be the unnormalized probabilities of the categories and $g_i^j \sim\operatorname{Gumbel}(0, 1)$ be i.i.d. samples drawn from the Gumbel distribution for $i\in[n], j\in[m]$.
For each dimension $i\in[n]$, we sample a Gumbel-softmax random variable $\bm{z}_i\sim \operatorname{GS}(\bm{\alpha}_i)$ over the simplex
$\Delta^{m-1} = \{\bm{y}\in\mathbb{R}^n\;|\;y^j\in[0,1],\sum_{j=1}^m y^j = 1\}$
by setting
\begin{equation}\label{eq:gum_softmax}
  z_i^j = \frac{\exp(\log \alpha_i^j + g_i^j)}{\sum_{k=1}^m \exp(\log \alpha_i^k + g_i^k)}
\end{equation}
for $j\in[m]$.
We set the approximated posterior distribution to be a joint distribution of $n$ Gumbel-softmax distributions, i.e., 
$q_{\phi}(\bm{z}|\bm{x})=\operatorname{GS}^n\bigl(\bm{z}\;|\;\bm{\alpha}_{\phi}(\bm{x})\bigr)$
% to be a joint distribution of $n$ Gumbel-softmax distributions
and assume a joint discrete uniform prior distribution
$p(\bm{z})=\mathcal{U}^n\{1,m\}$.
Note that $\bm{z}$ is of dimension $n\times m$.
To obtain the final $n$-dimensional latent variable $\bar{\bm{z}}$, we define a function
$f:\Delta^{m-1}\rightarrow [0,1]$
as the dot product of $\bm{z}_i$ with the vector $\bm{v}_m=(v_m^1, \dots, v_m^m)$ of $m$ equidistant entries $v_m^j = \tfrac{j-1}{m-1}$ of the interval\footnote{The choice of the unit interval is arbitrary.} $[0,1]$, i.e.,
\begin{equation}\label{eq:fn_f}
  \bar{z}_i = f(\bm{z}_i) = \bm{z}_i \cdot \bm{v}_m
  = \tfrac{1}{m-1} \textstyle\sum_{j=1}^m j z_i^j
\end{equation}
as illustrated in Figure~\ref{fig:d-vae}. We will show in Section~\ref{sct:disent_prop} that this choice of the latent variable $\bar{\bm{z}}$ has favorable disentangling properties.
The representation is obtained by the standard softmax function
$r(\bm{x})_i = f\bigl(\operatorname{softmax}(\log\bm{\alpha}_\phi(\bm{x})_i)\bigr)$.
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Learning disentangled discrete representations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
\section{Learning Disentangled Discrete Representations} \label{sct:learning}
%
Using a discrete distribution in the latent space is a strong inductive bias for disentanglement. 
In this section, we introduce some properties of the discrete latent space and compare it to the latent space of a Gaussian VAE.
First, we show that mapping the discrete categories into a shared unit interval as in Eq.~\ref{eq:fn_f} causes an ordering of the discrete categories and, in turn, enable a definition of neighborhoods in the latent space.
Second, we derive that, %%the main argument from \cite{burgess2018understanding} still holds 
in the discrete case, neighboring points in the data space are encouraged to be represented close together in the latent space.
Third, we show that the categorical latent space is less rotation-prone than its Gaussian counterpart and thus, constituting a stronger inductive prior for disentanglement.
Finally, we describe how to select models with better disentanglement using the straight-through gap.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Neighborhoods in the data space
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Moved to Appendix (app:subs:neighX)
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Neighborhoods in the latent space
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Neighborhoods in the latent space}\label{sec:latent_space}
In the Gaussian case, neighboring points in the observable space correspond to neighboring points in the latent space.
The ELBO Loss Eq.~\ref{eq:elbo}, more precisely the reconstruction loss as part of the ELBO, implies a topology of the observable space.
For more details on this topology, see Appendix~\ref{app:subs:neighX}.
In the case, where the approximated posterior distribution, $q_{\phi}(\bm{z}|\bm{x})$, is Gaussian and the covariance matrix, $\Sigma(\bm{x})$, is diagonal, the topology of the latent space can be defined in a similar way:
The negative log-probability is the weighted Euclidean distance to the mean $\bm{\mu}(\bm{x})$ of the distribution 
\begin{equation}
    \begin{split}
        C -\log q_{\phi}(\bm{z}|\bm{x})  
        &= \frac{1}{2}\left[ (\bm{z}-\bm{\mu}(\bm{x}))^{\intercal} \bm{\Sigma}(\bm{x})  (\bm{z}-\bm{\mu}(\bm{x}))\right]^2
        = \sum_{i=1}^n \frac{(z_i - \mu_i(\bm{x}))^2}{2\sigma_i(\bm{x})}
    \end{split}
\end{equation}
where $C$ denotes the logarithm of the normalization factor in the Gaussian density function. 
Neighboring points in the observable space will be mapped to neighboring points in the latent space to reduce the log-likelihood cost of sampling in the latent space \cite{burgess2018understanding}.

In the case of categorical latent distributions, the induced topology is not related to the euclidean distance and, hence, it does not encourage that points that are close in the observable space will be mapped to points that are close in the latent space.
The problem becomes explicit if we consider a single categorical distribution.
In the latent space, neighbourhoods entirely depend on the shared representation of the $m$ classes.
The canonical representation maps a class $j$ into the one-hot vector $\bm{e}^j = (e_1,e_2,\dots,e_m)$ with $e_k=1$ for $k=j$ and $e_k=0$ otherwise.
The representation space consists of the $m$-dimensional units vectors, and all classes have the same pairwise distance between each other.

To overcome this problem, we inherit the canonical order of $\mathbb{R}$ by depicting a $1$-dimensional representation space.
We consider the representation $\bar{z}_i=f(\bm{z}_i)$ from Eq.~\ref{eq:fn_f} that maps a class $j$ on the value $\frac{j-1}{m-1}$ inside the unit interval.
In this way, we create an ordering on the classes $1 < 2 < \dots < m$ and define the distance between two classes by $d(j,k) = \frac{1}{m-1}\vert j - k \vert$.
In the following, we discuss properties of a VAE using this representation space.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Latent space properties
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Disentangling properties of the discrete VAE} \label{sct:disent_prop}
In this section, we show that neighboring points in the observable space are represented close together in the latent space and that each data point is represented discretely by a single category $j$ for each dimension $i\in\{1,\dots,n\}$.
First, we show that reconstructing under the latent variable $\bar{z}_i=f(\bm{z}_i)$ encourages each data point to utilize neighboring categories rather than categories with a larger distance.
Second, we discuss how the Gumbel-softmax distribution is encouraged to approximate the discrete categorical distribution.
For the Gaussian case, this property was shown by \cite{burgess2018understanding}.
Here, the ELBO (Eq.~\ref{eq:elbo}) depicts an inductive prior that encourages disentanglement by encouraging neighboring points in the data space to be represented close together in the latent space \cite{burgess2018understanding}.
To show these properties for the D-VAE, we use the following proposition. The proof can be found in Appendix~\ref{app:proof}.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proposition 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{proposition}\label{prop:fn_f}
  Let $\bm{\alpha}_i \in [0, \infty)^m$, $\bm{z}_i\sim \operatorname{GS}(\bm{\alpha}_i)$ be as in Eq.~\ref{eq:gum_softmax} and $\bar{z}_i=f(\bm{z}_i)$ be as in Eq.~\ref{eq:fn_f}.
  Define $j_{\text{min}}=\argmin_j\{\alpha_i^j > 0\}$ and $j_{\text{max}}=\argmax_j\{\alpha_i^j > 0\}$. Then it holds that
  \begin{enumerate}
      \item[(a)]
      $\operatorname{supp}(f)=(\frac{j_{\text{min}}}{m-1}, \tfrac{j_{\text{max}}}{m-1})$ \label{prop:fn_f_a}
      \item[(b)] $\frac{\alpha_i^j}{\sum_{k=1}^m \alpha_i^k} \rightarrow 1 \Rightarrow
      \mathbb{P}(z_i^j=1)=1 \land f(\bm{z}_i) = \mathbbm{1}_{\{\frac{j}{m-1}\}}$. \label{prop:fn_f_b} %
  \end{enumerate}%
\end{proposition}%
Prop.~\ref{prop:fn_f} has multiple consequences.
First, a class $j$ might have a high density regarding $\bar{z}_i=f(\bm{z}_i)$ although $\alpha_i^j \approx 0$. For example, if $j$ is positioned between two other classes with large $\alpha_i^k$ $\bigl($e.g. $j = 3$ in Figure~\ref{fig:d-vae}(a)$\bigr)$
Second, if there is a class $j$ such that $\alpha_i^k \approx 0$ for all $k \geq j$ or $k \leq j$, then the density of these classes is also almost zero $\bigl($Figure~\ref{fig:d-vae}(a-c)$\bigr)$.
Note that a small support benefits a small reconstruction loss since it reduces the probability of sampling a wrong class.
The probabilities of Figure~\ref{fig:d-vae} (a) and (b) are the same with the only exception that $\alpha_i^3 \leftrightarrow \alpha_i^5$ are swapped.
Since the probability distribution in (b) yields a smaller support and consequently a smaller reconstruction loss while the KL divergence is the same for both probabilities,\footnote{The KL divergence is invariant under permutation.} the model is encouraged to utilize probability (b) over (a).
This encourages the representation of similar inputs in neighboring classes rather than classes with a larger distance.

Consequently, we can apply the same argument as in \cite{burgess2018understanding}~Section~4.2 about the connection of the posterior overlap with minimizing the ELBO.
Since the posterior overlap is highest between neighboring classes, confusions caused by sampling are more likely in neighboring classes than those with a larger distance.
To minimize the penalization of the reconstruction loss caused by these confusions, neighboring points in the data space are encouraged to be represented close together in the latent space.
Similar to the Gaussian case \cite{burgess2018understanding}, we observe an increase in the KL divergence loss during training while the reconstruction loss continually decreases.
The probability of sampling confusion and, therefore, the posterior overlap must be reduced as much as possible to reduce the reconstruction loss.
Thus, later in training, data points are encouraged to utilize exactly one category while accepting some penalization in the form of KL loss,
meaning that
$\alpha_i^j/(\sum_{k=1}^m \alpha_i^k) \rightarrow 1$.
Consequently, the Gumbel-softmax distribution approximates the discrete categorical distribution, see Prop.~\ref{prop:fn_f} (b). 
An example is shown in Figure~\ref{fig:d-vae}(c).
This training behavior results in the unique situation in which the latent space approximates a discrete representation while its classes maintain the discussed order and the property of having neighborhoods.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Advantages of discrete disentanglement
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Structural advantages of the discrete VAE}\label{sct:discrete_disent}
In this section, we demonstrate that the properties discussed in Section~\ref{sct:disent_prop} aid disentanglement.
So far, we have only considered a single factor $\bm{z}_i$ of the approximated posterior $q_{\phi}(\bm{z}|\bm{x})$.
To understand the disentangling properties regarding the full latent variable $\bm{z}$, we first highlight the differences between the continuous and the discrete approach.

In the continuous case, neighboring points in the observable space are represented close together in the latent space. 
However, this does not imply disentanglement, since the first property is invariant under rotations over $\mathbb{R}^n$ while disentanglement is not.
Even when utilizing a diagonal covariance matrix for the approximated posterior $q(\bm{z}|\bm{x})=\mathcal{N}\bigl(\bm{z}\;|\;\bm{\mu}(\bm{x}), \bm{\sigma}(\bm{x})\bm{I}\bigr)$, which, in general, is not invariant under rotation,
there are cases where rotations are problematic, as the following proposition shows. We provide the proof in Appendix~\ref{app:proof}.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proposition 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{proposition}[Rotational Equivariance] \label{prop:rotation}
    Let $\alpha\in[0,2\pi)$ and let $\bm{z}\sim\mathcal{N}\bigl(\bm{\mu}, \Sigma\bigr)$ with $\Sigma=\bm{\sigma}\bm{I},\; \bm{\sigma}=(\sigma_0, \dots, \sigma_n)$.
    If $\sigma_i = \sigma_j$ for some $i\neq j\in[n]$,
    then $\bm{z}$ is equivariant under any $i,j$-rotation, i.e.,
    $R_{ij}^\alpha\bm{z} \overset{d}{=} \bm{y}$ with
    $\bm{y}\sim\mathcal{N}\bigl(R_{ij}^\alpha\bm{\mu}, \Sigma\bigr)$.%
\end{proposition}%
%
%
%
Since, in the Gaussian VAE, the KL-divergence term in Eq.~\ref{eq:elbo} is invariant under rotations, Prop.~\ref{prop:rotation} implies that its latent space can be arbitrarily rotated in dimensions $i,j$ that hold equal variances $\sigma_i = \sigma_j$.
Equal variances can occur, for example, when different factors exert a similar influence on the data space, e.g., X-position and Y-position or
for factors where high log-likelihood costs of potential confusion causes lead to variances close to zero.
In contrast, the discrete latent space is invariant only under rotations that are axially aligned.

We illustrate this with an example in Figure~\ref{fig:circles}. 
Here we illustrate the $2$-dimensional latent space of a Gaussian VAE model trained on a dataset generated from the two ground-truth factors, X-position and Y-position.
We train $50$ copies of the model and depicted the best, the $5$th best, and the $10$th best latent space regarding 
the Mutual Information Gap (MIG) \cite{chen2018isolating}.
All three latent spaces exhibit rotation, while the disentanglement score is strongly correlated with the angle of the rotation.
In the discrete case, the latent space is, according to Prop.~\ref{prop:fn_f} (b), a subset of the regular grid
$\mathbb{G}^n$ with $\mathbb{G}=\{\tfrac{j}{m-1}\}_{j=0}^{m-1}$
as illustrated in Figure~\ref{fig:latent_distance} (right).
Distances and rotations exhibit different geometric properties on $\mathbb{G}^n$ than on $\mathbb{R}^n$.
First, the closest neighbors are axially aligned.
Non-aligned points have a distance at least $\sqrt{2}$ times larger.
Consequently, representing neighboring points in the data space close together in the latent space encourages disentanglement.
Secondly, $\mathbb{G}^n$ is invariant only under exactly those rotations that are axially aligned.
Figure~\ref{fig:circles} (bottom right) illustrates the $2$-dimensional latent space of a D-VAE model trained on the same dataset and with the same random seeds as the Gaussian VAE model.
Contrary to the Gaussian latent spaces, the discrete latent spaces are sensible of the axes and generally yield better disentanglement scores.
The set of all $100$ latent spaces is available in Figures \ref{fig:50_vae} and \ref{fig:50_d-vae} in Appendix~\ref{app:exp}.
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Model selection by an unspervised score
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{The straight-through gap} \label{sec:gap_st}
We have observed that sometimes the models approach local minima, for which $\bm{z}$ is not entirely discrete.
As per the previous discussion, those models have inferior disentangling properties.
We leverage this property by selecting models that yield discrete latent spaces.
Similar to the Straight-Through Estimator \cite{bengio2013estimating}, we round $\bm{z}$ off using $\argmax$ and measure the difference between the rounded and original ELBO, i.e.,
  $\operatorname{Gap}_{ST}(\bm{x}) = \lvert\mathcal{L}_{\theta,\phi}^{ST}(\bm{x}) - \mathcal{L}_{\theta,\phi}(\bm{x})\rvert$,
which equals zero if $\bm{z}$ is discrete.
Figure~\ref{fig:st-gap_downstream} (left) illustrates the Spearman rank correlation between $\operatorname{Gap}_{ST}$ and various disentangling metrics on different datasets.
A smaller $\operatorname{Gap}_{ST}$ value indicates high disentangling scores for most datasets and metrics.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MIG Comparison Table
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{table*}[t]
\setlength{\tabcolsep}{3.5pt}
\caption{%
The median MIG scores in \% for state-of-the-art unsupervised methods compared to the discrete methods.
Results taken from \cite{locatello2019challenging} are marked with an asterisk~(*).
We have re-implemented all other results with the same architecture as in \cite{locatello2019challenging} for the sake of fairness.
The last row depicts the scores of the models selected by the smallest $\operatorname{Gap}_{ST}$.
The $25\%$ and the $75\%$ quantiles can be found in Table~\ref{tb:mig_25} in Appendix~\ref{app:exp}.}
\label{tb:mig}
\begin{center}
\begin{tabular}{lcccccc}
\toprule
Model & dSprites & C-dSprites & SmallNORB & Cars3D & Shapes3D & MPI3D\\
\midrule
$\beta$-VAE \cite{higgins2017beta}    &11.3$^*$& 12.5$^*$& 20.2$^*$& 9.5$^*$& n.a.& n.a.\\
$\beta$-TCVAE \cite{chen2018isolating}  &17.6$^*$& 14.6$^*$& 21.5$^*$& 12.0$^*$& n.a.& n.a.\\
DIP-VAE-I \cite{kumar2017variational}      &3.6$^*$& 4.7$^*$& 16.7$^*$& 5.3$^*$& n.a.& n.a.         \\
DIP-VAE-II \cite{kumar2017variational}     &6.2$^*$& 4.9$^*$& 24.1$^*$& 4.2$^*$& n.a.& n.a.\\
AnnealedVAE \cite{burgess2018understanding}    &7.8$^*$& 10.7$^*$& 4.6$^*$& 6.7$^*$& n.a.& n.a.\\
FactorVAE \cite{kim2018disentangling}      &17.4& 14.3& \textbf{25.3}& 9.0& 34.7& 11.1\\
\midrule
D-VAE          &17.4& 9.4& 19.0& 8.5& 28.8& 12.8\\
FactorDVAE    &\textbf{21.7}& \textbf{15.5}& 23.2& \textbf{14.9}& \textbf{42.4}& \textbf{30.5}\\
\midrule
Selection      &39.5 &20.0 &22.7 &19.1 &40.1 &32.3\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Related Work}
Previous studies have proposed various methods for utilizing discrete latent spaces.
The REINFORCE algorithm \cite{williams1992simple} utilizes the log derivative trick.
The Straight-Through estimator \cite{bengio2013estimating} back-propagates through hard samples by replacing the threshold function with the identity in the backward pass.
Additional prior work employed the nearest neighbor look-up called vector quantization \cite{van2017neural} to discretize the latent space.
Other approaches use reparameterization tricks \cite{kingma2013auto} that enable the gradient computation by removing the dependence of the density on the input parameters.
Maddison et al. \cite{maddison2017concrete} and Jang et al. \cite{jang2017categorical} propose the Gumbel-Softmax trick, a continuous %(but biased) 
reparameterization trick for categorical distributions.
Extensions of the Gumbel-Softmax trick discussed control variates \cite{tucker2017rebar,grathwohl2018backpropagation}, the local reparameterization trick \cite{shayer2018learning}, or the behavior of multiple sequential discrete components \cite{friede2021efficient}.
In this work, we focus on the structural impact of discrete representations on the latent space from the viewpoint of disentanglement.

\noindent State-of-the-art unsupervised disentanglement methods enhance Gaussian VAEs with various regularizers that encourage disentangling properties.
The $\beta$-VAE model \cite{higgins2017beta} introduces a hyperparameter to control the trade-off between the reconstruction loss and the KL-divergence term, promoting disentangled latent representations.
The annealedVAE \cite{burgess2018understanding} adapts to the $\beta$-VAE by annealing the $\beta$ hyperparameter during training.
FactorVAE \cite{kim2018disentangling} and $\beta$-TCVAE \cite{chen2018isolating} promote independence among latent variables by controlling the total correlation between them.
DIP-VAE-I and DIP-VAE-II \cite{kumar2017variational} are two variants that enforce disentangled latent factors by matching the covariance of the aggregated posterior to that of the prior.
Previous research has focused on augmenting the standard variational autoencoder with discrete factors \cite{makhzani2015adversarial,dupont2018learning,jeong2019learning} to improve disentangling properties.
In contrast, our goal is to replace the variational autoencoder with a categorical one, treating every ground-truth factor as a discrete representation.
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Figure environment removed%
% %
% %
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experimental setup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Experimental Setup}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methods
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\noindent\textbf{Methods.}
The experiments aim to compare the Gaussian VAE with the discrete VAE.
We consider the unregularized version and the total correlation penalizing method, VAE, D-VAE, FactorVAE \cite{kim2018disentangling} and FactorDVAE a version of FactorVAE for the D-VAE. We provide a detailed discussion of FactorDVAE in Appendix~\ref{app:improv}.
For the semi-supervised experiments, we augment each loss function with the supervised regularizer $R_s$ as in Appendix~\ref{app:improv}.
For the Gaussian VAE, we choose the BCE and the $L_2$ loss for $R_s$, respectively.
For the discrete VAE, we select the cross-entropy loss, once without and once with masked attention where we incorporate the knowledge about the number of unique variations.
We discuss the corresponding learning objectives in more detail in Appendix~\ref{app:improv}.

%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Datasets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\noindent\textbf{Datasets.}
We consider six commonly used disentanglement datasets which offer explicit access to the ground-truth factors of variation: \emph{dSprites} \cite{higgins2017beta}, \emph{C-dSprites} \cite{locatello2019challenging}, \emph{SmallNORB} \cite{lecun2004learning}, \emph{Cars3D} \cite{reed2015deep}, \emph{Shapes3D} \cite{kim2018disentangling} and \emph{MPI3D} \cite{gondal2019transfer}.
We provide a more detailed description of the datasets in Table~\ref{tb:data} in Appendix~\ref{app:data}.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Metrics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\noindent\textbf{Metrics.}
We consider the commonly used disentanglement metrics that have been discussed in detail in \cite{locatello2019challenging} to evaluate the representations: \emph{BetaVAE} metric \cite{higgins2017beta}, \emph{FactorVAE} metric \cite{kim2018disentangling}, \emph{Mutual Information Gap} (MIG) \cite{chen2018isolating}, \emph{DCI Disentanglement} (DCI) \cite{eastwood2018framework}, \emph{Modularity} \cite{ridgeway2018learning} and \emph{SAP score} (SAP) \cite{kumar2017variational}.
As illustrated on the right side of Figure~\ref{fig:st-gap_downstream}, the MIG score seems to be the most reliable indicator of sample efficiency across different datasets.
Therefore, we primarily focus on the MIG disentanglement score.
We discuss this in more detail in Appendix~\ref{app:further}.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experimental protocol
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\noindent\textbf{Experimental protocol.}
We adopt the experimental setup of prior work (\cite{locatello2019challenging} and \cite{locatello2019disentangling}) for the unsupervised and for the semi-supervised experiments, respectively.
Specifically, we utilize the same neural architecture for all methods so that all differences solely emerge from the distribution of the type of VAE.
For the unsupervised case, we run each considered method on each dataset for $50$ different random seeds.
Since the two unregularized methods do not have any extra hyperparameters, we run them for $300$ different random seeds instead.
For the semi-supervised case, we consider two numbers ($100$/$1000$) of perfectly labeled examples and split the labeled examples ($90$\%/$10$\%) into a training and validation set.
We choose $6$ values for the correlation penalizing hyperparameter $\gamma$ and for the semi-supervising hyperparameter $\omega$ from Equation~\ref{eq:factor_dvae} and~\ref{eq:semi-sup} in Appendix~\ref{app:improv}, respectively.
We present the full implementation details in Appendix~\ref{app:impl}.
%
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 6
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Figure environment removed
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Experimental Results}
First, we investigate whether a discrete VAE offers advantages over Gaussian VAEs in terms of disentanglement properties, finding that the discrete model generally outperforms its Gaussian counterpart and showing that the FactorDVAE achieves new state-of-the-art MIG scores on most datasets.
Additionally, we propose a model selection criterion based on $\operatorname{Gap}_{ST}$ to find good discrete models solely using unsupervised scores.
Lastly, we examine how incorporating label information can further enhance discrete representations.
The implementations are in JAX and Haiku and were run on a RTX A6000 GPU.\footnote{The implementations and Appendix are at \url{https://github.com/david-friede/lddr}.}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Backbone Comparison
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Improvement in unsupervised disentanglement properties}\label{ex:backbone}
\noindent\textbf{Comparison of the unregularized models.}
In the first experiment, we aim to answer our main research question of whether discrete latent spaces yield structural advantages over their Gaussian counterparts.
Figure~\ref{fig:unreg_mpi3d} depicts the comparison regarding the disentanglement scores (left) and the datasets (right).
The discrete model achieves a better score on the MPI3D dataset for each metric with median improvements ranging from $2$\% for Modularity to $104$\% for MIG.
Furthermore, the discrete model yields a better score for all datasets but SmallNORB with median improvements ranging from $50$\% on C-dSprites to $336$\% on dSprites.
More detailed results can be found in Table~\ref{tb:quant}, Figure~\ref{fig:app_unreg_all_1}, and Figure~\ref{fig:app_unreg_all_2} in Appendix~\ref{app:exp}.
Taking into account all datasets and metrics, the discrete VAE improves over its Gaussian counterpart in $31$ out of $36$ cases.

\noindent\textbf{Comparison of the total correlation regularizing models.}
For each VAE, we choose the same $6$ values of hyperparameter $\gamma$ for the total correlation penalizing method and train $50$ copies, respectively.
The right side of Figure~\ref{fig:st-gap_factor} depicts the comparison of FactorVAE and FactorDVAE w.r.t. the MIG metric.
The discrete model achieves a better score for all datasets but SmallNORB with median improvements ranging from $8$\% on C-dSprites to $175$\% on MPI3D.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 7
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Figure environment removed
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Model Improvements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Match state-of-the-art unsupervised disentanglement methods}
Current state-of-the-art unsupervised disentanglement methods enrich Gaussian VAEs with various regularizers encouraging disentangling properties.
Table~\ref{tb:mig} depicts the MIG scores of all methods as reported in \cite{locatello2019challenging} utilizing the same architecture as us.
FactorDVAE achieves new state-of-the-art MIG scores on all datasets but SmallNORB, improving the previous best scores by over $17$\% on average.
These findings suggest that incorporating results from the disentanglement literature might lead to even stronger models based on discrete representations.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Unsupervised Model Selection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Unsupervised selection of models with strong disentanglement} 
A remaining challenge in the disentanglement literature is selecting the hyperparameters and random seeds that lead to good disentanglement scores \cite{locatello2019disentangling}.
We propose a model selection based on an unsupervised score measuring the discreteness of the latent space utilizing $\operatorname{Gap}_{ST}$ from Section~\ref{sec:gap_st}.
The left side of Figure~\ref{fig:st-gap_factor} depicts the Spearman rank correlation between various disentangling metrics and $\operatorname{Gap}_{ST}$ of D-VAE and FactorDVAE combined.
Note that the unregularized D-VAE model can be identified as a FactorDVAE model with $\gamma=0$.
A small Straight-Through Gap corresponds to high disentangling scores for most datasets regarding the MIG, DCI, and SAP metrics.
This correlation is most vital for the MIG metric.
We anticipate finding good hyperparameters by selecting those models yielding the smallest $\operatorname{Gap}_{ST}$.
The last row of Table~\ref{tb:mig} confirms this finding.
This model selection yields MIG scores that are, on average, $22$\% better than the median score and not worse than $6$\%.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Unsupervised Model Selection
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Utilize label information to improve discrete representations}
Locatello et al. \cite{locatello2019disentangling} employ the semi-supervised regularizer $R_s$ by including $90$\% of the label information during training and utilizing the remaining $10$\% for a model selection.
We also experiment with a model selection based on the $\operatorname{Gap}_{ST}$ value.
Figure~\ref{fig:semisupervised} depicts the percentage of each semi-supervised method being the best over all datasets and disentanglement metrics.
The unregularized discrete method surpasses the other methods on the semi-supervised disentanglement task.
The advantage of the discrete models is more significant for the median values than for the model selection.
Utilizing $\operatorname{Gap}_{ST}$ for selecting the discrete models only partially mitigates this problem.
Incorporating the number of unique variations by utilizing the masked regularizer improves the disentangling properties significantly, showcasing another advantage of the discrete latent space.
The quantiles of the discrete models can be found in Table~\ref{tb:quant_semi} in Appendix~\ref{app:exp}.
%
%
%
\subsection{Visualization of the latent categories}
Prior work uses latent space traversals for qualitative analysis of representations \cite{higgins2017beta,burgess2018understanding,kim2018disentangling,watters2019spatial}. 
A latent vector $\bm{z} \sim q_{\phi}(\bm{z}|\bm{x})$ is sampled, and each dimension $z_i$ is traversed while keeping the other dimensions constant.
The traversals are then reconstructed and visualized.
Unlike the Gaussian case, the D-VAE's latent space is known beforehand, allowing straightforward traversal along the categories.
Knowing the number of unique variations lets us use masked attention to determine the number of each factor's categories, improving latent space interpretability.
Figure~\ref{fig:recon} illustrates the reconstructions of four random inputs and latent space traversals of the semi-supervised D-VAE utilizing masked attentions.
While the reconstructions are easily recognizable, their details can be partially blurry, particularly concerning the object shape.
The object color, object size, camera angle, and background color are visually disentangled, and their categories can be selected straightforwardly to create targeted observations.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure environment removed
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Conclusion}
In this study, we investigated the benefits of discrete latent spaces in the context of learning disentangled representations by examining the effects of substituting the standard Gaussian VAE with a categorical VAE. Our findings revealed that the underlying grid structure of categorical distributions mitigates the rotational invariance issue associated with multivariate Gaussian distributions, thus serving as an efficient inductive prior for disentangled representations.

In multiple experiments, we demonstrated that categorical VAEs outperform their Gaussian counterparts in disentanglement.
We also determined that the categorical VAE provides an unsupervised score, the Straight-Through Gap, which correlates with some disentanglement metrics, providing, to the best of our knowledge, the first unsupervised  model selection score for disentanglement.

However, our study has limitations.
We focused on discrete latent spaces, without investigating the impact of vector quantization on disentanglement.
Furthermore, the Straight-Through Gap does not show strong correlation with disentanglement scores, affecting model selection accuracy.
Additionally, our reconstructions can be somewhat blurry and may lack quality.

Our results offer a promising direction for future research in developing more powerful models with discrete latent spaces. Such future research could incorporate findings from the disentanglement literature and potentially develop novel regularizations tailored to discrete latent spaces.