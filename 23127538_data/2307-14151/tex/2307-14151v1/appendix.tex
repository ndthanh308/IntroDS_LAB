%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
%
\newpage
\begin{subappendices}
\renewcommand{\thesection}{\arabic{section}}%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proofs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Proofs} \label{app:proof}
\subsubsection{Proof of Proposition~\ref{prop:fn_f}}
\begin{proof}
  For the sake of clarity, we ignore the $i$-index in our notation and write the $j$-index as a subscript.\\
  Part (a):
  Let $J$ be the set of all indices of $\bm{\alpha}$ with $\alpha_j=0$ and let $m^\prime=m-|J|$ be the number of elements of $\bm{\alpha}$ that are non-zero.
  We will first show that
  \begin{equation*}
    \operatorname{supp}\bigl(\operatorname{GS}(\bm{\alpha})\bigr)=\operatorname{int}\{\bm{y}\in\mathbb{R}^n\;|\;y_j\in[0,1],\sum_{j=1}^m y_j = 1, y_k = 0 \text{ for } k\in J\}.
  \end{equation*}
  Let $P_{\bm{\alpha}}:\mathbb{R}^m\rightarrow\mathbb{R}^{m^\prime}$ be the projection that maps $\bm{\alpha}$ on its non-zero elements $\bm{\alpha}^\prime = P_{\bm{\alpha}}(\bm{\alpha})$ with $\alpha^{\prime}_j\neq0$ for all $j\in[m^\prime]$.
  We write $ P_{\bm{\alpha}}^{-1}(\bm{\alpha}^\prime) = \bm{\alpha}$ for the inverse of the projection.
  Sampling  $z \sim \operatorname{GS}(\bm{\alpha})$ is then defined by $P_{\bm{\alpha}}^{-1}(\bm{z}^\prime)$ for $\bm{z}^\prime \sim \operatorname{GS}(\bm{\alpha}^\prime)$.
  By Maddison et. al \cite{maddison2017concrete}, Proposition 1a, we know that the density of $\operatorname{GS}(\bm{\alpha}^\prime)$ is
  \begin{equation*}
    p_{\bm{\alpha}^\prime}(\bm{x}) = \frac{(m^\prime - 1)!}{(\sum_{j=1}^{m^\prime} \alpha^{\prime}_j x_j^{-1})^{m^\prime}}
    \prod_{k=1}^{m^\prime} \frac{\alpha^{\prime}_k}{x_k^2},
  \end{equation*}
  which is defined for all $x\in \Delta^{m^\prime - 1}$ with $x_j > 0$ for all $j\in [m^\prime]$.
  Furthermore, we have $p_{\bm{\alpha}^\prime}(\bm{x}) > 0$ for all $x\in \operatorname{int}\Delta^{m^\prime - 1}$
  since, in this case, $p_{\bm{\alpha}^\prime}(\bm{x})$ consists of a sum and products of a finite number of positive elements.
  By definition of $z \sim \operatorname{GS}(\bm{\alpha})$ we reverse the projection $P_{\bm{\alpha}}$ to obtain
  $\operatorname{supp}\bigl(\operatorname{GS}(\bm{\alpha})\bigr)=\operatorname{int}\{\bm{y}\in\mathbb{R}^n\;|\;y_j\in[0,1],\sum_{j=1}^m y_j = 1, y_k = 0 \text{ for } k\in J\}$.\\
  We will now show that $\operatorname{supp}(f)=(\frac{j_{\text{min}}}{m-1}, \tfrac{j_{\text{max}}}{m-1})$.
  First, let $\bm{z}\in\operatorname{supp}\bigl(\operatorname{GS}(\bm{\alpha})\bigr)$, then it holds that
  \begin{equation*}
    f(z) = \frac{1}{m-1} \sum_{j=1}^m j z_j = \frac{1}{m-1} \sum_{j=j_{\text{min}}}^m j z_j > \frac{1}{m-1} j_{\text{min}}.
  \end{equation*}
  With the same argument, we can show that $f(z) < \frac{j_{\text{max}}}{m-1}$.
  Conclusively, we will show that
  \begin{equation*}
    \forall \tilde{z} \in (\frac{j_{\text{min}}}{m-1}, \frac{j_{\text{max}}}{m-1})\; \exists \bm{z} \in \operatorname{supp}\bigl(\operatorname{GS}(\bm{\alpha})\bigr) \text{ with } \tilde{z} = f(\bm{z}).
  \end{equation*}
  Let $\tilde{z} \in (\frac{j_{\text{min}}}{m-1}, \frac{j_{\text{max}}}{m-1})$, then there exists $\delta \in (0,1)$ with
  $\tilde{z} = \delta \frac{j_{\text{min}}}{m-1} + (1-\delta) \frac{j_{\text{max}}}{m-1}$.
  Choose $\bm{z}$ with
  \begin{equation*}
    z_j =
    \begin{cases}
      \delta, & \text{if}\ j=j_{\text{min}}, \\
      1-\delta, & \text{if}\ j=j_{\text{max}}, \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation*}
  to conclude the proof of Part (a).\\

  Part (b):
  Let $c>0$.
  We will first show that $\operatorname{GS}(\bm{\alpha}) = \operatorname{GS}(c\bm{\alpha})$.
  It holds that
  \begin{equation*}
    p_{c\bm{\alpha}}(\bm{x})
    = \frac{(m - 1)!}{(\sum_{j=1}^{m} c\alpha_j x_j^{-1})^{m}}
    \prod_{l=1}^{m} \frac{c\alpha_l}{x_l^2}
    = \frac{(m - 1)!}{(c\sum_{j=1}^{m} \alpha_j x_j^{-1})^{m}}
    c^m \prod_{l=1}^{m} \frac{\alpha_l}{x_l^2}
    % = \frac{c^m(m - 1)!}{c^m(\sum_{j=1}^{m} \alpha_j x_j^{-1})^{m}}
    % \prod_{l=1}^{m} \frac{\alpha_l}{x_l^2}
    = p_{\bm{\alpha}}(\bm{x}).
  \end{equation*}
  We will now show that $\frac{\alpha_k}{\alpha_j} \rightarrow 0$ for all $j\neq k$ and thus, $\frac{\bm{\alpha}}{\alpha_j}\rightarrow \bm{e}^j$ with
  \begin{equation*}
    e^j_k =
    \begin{cases}
      1, & \text{if}\ k=j, \\
      0, & \text{otherwise}
    \end{cases}
  \end{equation*}
  and therefore, $\operatorname{GS}(\bm{\alpha}) = \operatorname{GS}(\frac{1}{\alpha_j}\bm{\alpha}) \rightarrow \operatorname{GS}(\bm{e}^j)$ to conclude the proof.
  By assumption, we have
  \begin{equation*}
    \frac{1}{\sum_{k=1}^m \frac{\alpha_k}{\alpha_j}}
    = \frac{\frac{\alpha_j}{\alpha_j}}{\frac{1}{\alpha_j}\sum_{k=1}^m \alpha_k}
    = \frac{\alpha_j^{-1}}{\alpha_j^{-1}} \frac{\alpha_j}{\sum_{k=1}^m \alpha_k}
    = \frac{\alpha_j}{\sum_{k=1}^m \alpha_k}
    \rightarrow 1
  \end{equation*}
and thus, $\sum_{k=1}^m \frac{\alpha_k}{\alpha_j} \rightarrow 1$.
It holds that $\sum_{k=1}^m \frac{\alpha_k}{\alpha_j} = 1 + \sum_{j\neq k} \frac{\alpha_k}{\alpha_j}$ and thus, $\sum_{j\neq k} \frac{\alpha_k}{\alpha_j} \rightarrow 0$.
Since $\frac{\alpha_k}{\alpha_j} \geq 0$ for all $j\neq k$, we have $\frac{\alpha_k}{\alpha_j} \rightarrow 0$ and the proof follows.
\end{proof}



\subsubsection{Proof of Proposition~\ref{prop:rotation}}
\begin{proof}
    For the sake of clarity, we write $R \coloneqq R_{ij}^\alpha$.
    We know that $R\bm{z} \overset{d}{=} \bm{y}^{\prime}$ with
    $\bm{y}^{\prime}\sim\mathcal{N}\bigl(R\bm{\mu}, R\Sigma R^{\intercal}\bigr)$.
    Thus, we need to show that $R\Sigma R^{\intercal} = \Sigma$.
    Let $\hat{\sigma}\coloneqq\sigma_i=\sigma_j$.
    In the case of $n=2$, we have that
    $\bm{\sigma}=(\hat{\sigma}, \hat{\sigma})$ and
    \begin{equation*}
        R\Sigma R^{\intercal}
         = R\bm{\sigma}\bm{I}R^{\intercal}
         = R\hat{\sigma}\bm{I}R^{\intercal}
         = \hat{\sigma} R R^{\intercal}
         = \bm{\sigma}\bm{I}
         = \Sigma.
    \end{equation*}
    In the case of $n>2$, we use a change of basis to rotate over the first two axes.
    Let $P$ be the permutation matrix that swaps
    $\bm{e}_1 \leftrightarrow \bm{e}_i$ and $\bm{e}_2 \leftrightarrow \bm{e}_j$.
    Then $P = P^{-1} = P^{\intercal}$ and
    \begin{align*}
        R\Sigma R^{\intercal}
        &= P^{\intercal}PRP^{\intercal}P\Sigma P^{\intercal}PR^{\intercal}P^{\intercal}P\\
        &= P^{\intercal}R_P\Sigma_P R_P^{\intercal}P\\
        &= P^{\intercal} \left[
            \begin{array}{cc}
                R_{12} &0 \\
                % 0 & \bar{\bm{I}}
                0 & \bm{I}_{n-2}
            \end{array}\right]
            \left[
            \begin{array}{cc}
                \hat{\sigma}\bm{I}_2 &0 \\
                % 0 & \bar{\bm{\sigma}} \bar{\bm{I}}
                0 & \bar{\bm{\sigma}} \bm{I}_{n-2}
            \end{array}\right]
            \left[
                \begin{array}{cc}
                    R_{12}^{\intercal} &0 \\
                    % 0 & \bar{\bm{I}}
                    0 & \bm{I}_{n-2}
                \end{array}\right] P\\
        &= P^{\intercal} \left[
            \begin{array}{cc}
                R_{12}\hat{\sigma}\bm{I}_2 R_{12}^{\intercal} &0 \\
                % 0 & \bar{\bm{\sigma}} \bar{\bm{I}}
                0 & \bar{\bm{\sigma}} \bm{I}_{n-2}
            \end{array}\right] P\\
        &= P^{\intercal} \left[
            \begin{array}{cc}
                \hat{\sigma}\bm{I}_2 &0 \\
                % 0 & \bar{\bm{\sigma}} \bar{\bm{I}}
                0 & \bar{\bm{\sigma}} \bm{I}_{n-2}
            \end{array}\right] P\\
        &= P^{\intercal}\Sigma_P P\\
        &= \Sigma.
    \end{align*}
\end{proof}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Further theoretical considerations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Further theoretical considerations} \label{app:subs:neighX}
%
Without any inductive biases, unsupervised disentanglement is theoretically impossible \cite{locatello2019challenging}.
Fortunately, the negative ELBO loss function from Eq.~\ref{eq:elbo} imposes an inductive prior that encourages disentanglement \cite{burgess2018understanding}.
In this subsection, we discuss the concept of defining neighborhoods in the observable space.
We hypothesize that the closest neighbors of an observation typically differ in only a single dimension of the ground-truth factors.
%
\subsubsection{Defining neighborhoods in the observable space.}
Locatello et al.~\cite{locatello2019challenging} showed that there is an infinite number of transformations of the ground truth factors $\bm{z}\sim p(\bm{z})=\prod{p(z_i)}$ that lead to the same data distribution.
A representation $r(\bm{x})$ that is fully disentangled with respect to $\bm{z}$ might be fully entangled with respect to such a transformation $\hat{\bm{z}}$.
Without any inductive biases, unsupervised disentanglement is theoretically impossible.
We will make use of two properties to mitigate this impossibility result.
First, we can utilize the reconstruction loss to define \emph{neighboring observations}
\begin{equation}
  U_{\epsilon}(\bm{x}) = \left\{\bm{y}\;|\;-\E{q_{\phi}(\bm{z}|\bm{x})}{\log p_{\theta}(\bm{y}|\bm{z})} \leq \log\epsilon\right\}.  
\end{equation}
Intuitively, the neighborhood $U_{\epsilon}(\bm{x})$ of some observation $\bm{x}$ are those observations/reconstructions $\bm{y}$ that have a high log-likelihood when encoding $\bm{x}$.
This intuition becomes especially clear in the case of the mean squared error reconstruction loss since this loss function fulfills the properties of a metric.
In this case, the neighborhood simplifies to $U_{\epsilon}(\bm{x}) = \left\{\bm{y}\;|\;\frac{1}{d}\Vert \bm{x}-\bm{y} \Vert_2^2 \leq \epsilon\right\}$, and neighboring observations are those with similar pixel values.
We utilize a second property to associate neighboring observations with small changes in the ground truth factors.
Many datasets in the disentanglement literature consist of \emph{discrete} ground truth factors
\cite{lecun2004learning,reed2015deep,higgins2017beta,kim2018disentangling,locatello2019challenging,gondal2019transfer}.
We argue that because of the discrete nature of many datasets, e.g., pixels, even continuous ground truth factors often convert into discrete changes in the data space.
For instance, although we sample the X-position in the Circles dataset \cite{watters2019spatial} from a random uniform distribution, we only obtain $\sim40$ distinct observations regarding the X-position, see Figure~\ref{fig:circles} (left).
As a consequence, we mostly observe incremental changes in the ground truth factors $\bm{z}$, that is, a small change in a single dimension $z_i$ or $z_j$ or both, but never \emph{half} a change in $z_i$ and $z_j$ as illustrated in Figure~\ref{fig:latent_distance} (left).
We hypothesize that, consequently, the closest neighbors $\bm{x}^\prime$ of $\bm{x}$ are generally those observations whose ground-truth factors $\bm{z}^\prime$ differ in only a \emph{single} dimension compared to the ground-truth factor $\bm{z}$ of $\bm{x}$.
In the following, we will discuss neighborhoods in the latent space and eventually show that neighboring points in the data space are encouraged to be represented close together in the latent space enabling disentangling properties.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Improving discrete representations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Improving Discrete Representations} \label{app:improv}
%
Regularization and supervision encouraging disentangling properties play little to no role in models based on discrete latent spaces.
In this section, we demonstrate how to utilize some of the main results from the disentanglement literature to further improve the discrete representations of categorical VAEs.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Total correlation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Regularizing the total correlation.}
State-of-the-art unsupervised disentanglement methods enrich the Gaussian ELBO with various regularizers encouraging disentangling properties.
Kim \& Mnih \cite{kim2018disentangling} and Chen et al.~\cite{chen2018isolating} penalize the \emph{total correlation} \cite{watanabe1960information}
\begin{equation*}
  \operatorname{TC}(\bm{z}) =
  \kl{q(\bm{z})}{\hat{q}(\bm{z})} =
  \E{q(\bm{z})}{\log\frac{q(\bm{z})}{\hat{q}(\bm{z})}}
\end{equation*}
where $\hat{q}(\bm{z}) \coloneqq \prod_{i=1}^n q(z_i)$ to reduce the dependencies between the dimensions of the representation.
Kim \& Mnih \cite{kim2018disentangling} first sample from $\hat{q}(\bm{z})$ by randomly shuffling samples from $q(\bm{z})$ across the batch for each latent dimension \cite{arcones1992bootstrap}.
They then utilize the density-ratio trick \cite{nguyen2010estimating,sugiyama2012density} to estimate the total correlation by training a discriminator $D$ to classify between samples from $q(\bm{z})$ and $\hat{q}(\bm{z})$.
Fortunately, we can adopt the same procedure to estimate the total correlation of $q(\bar{\bm{z}})$ of the D-VAE latent variable.
We augment the ELBO of the D-VAE with a total correlation regularizer to obtain the learning objective
\begin{equation}\label{eq:factor_dvae}
    \mathcal{L}_{\theta,\phi}(\bm{x}) - \gamma \E{q(\bm{z})}{\log\frac{D(\bm{\bar{z}})}{1 - D(\bm{\bar{z}})}}
\end{equation}
for $\gamma>0$ and name the corresponding model \emph{FactorDVAE}.
Finding new regularizers of the total correlation, which are tailored to the D-VAE could be interesting future work.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Semi-supervised training
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsubsection{Semi-supervised training.}
The idea of semi-supervised disentanglement is that incorporating label information of a limited amount of annotated data points during training encourages a latent space with desirable structure w.r.t. the ground-truth factors of variation \cite{locatello2019disentangling}.
The supervision is incorporated by enriching the ELBO with a regularizer $R_s(r(\bm{x}), \bm{z})$, where $R_s$ is a function of the annotated observation-label pairs.
Locatello et al.~\cite{locatello2019disentangling} normalize the targets $z_i$ to $[0,1]$ and propose the binary cross-entropy loss (BCE) or the $L_2$ loss for $R_s$.
In contrast, we discretize $\bm{z}$ by binning each dimension $z_i$ into $m$ bins and utilize the cross-entropy loss for $R_s$ obtaining the learning objective
\begin{equation}\label{eq:semi-sup}
    \mathcal{L}_{\theta,\phi}(\bm{x}) + \omega \sum_{i=1}^n z_i^j \log \frac{\alpha_i^j}{\sum_{k=1}^m \alpha_i^k}
\end{equation}
where $\omega>0$ and $z_i^j = 1$ if $z_i$ is in bin $j$ and $z_i^j = 0$ otherwise.

In order to utilize semi-supervised training, a set of data points needs to be annotated beforehand.
Different ground-truth factors of variation usually have a specific finite number of unique values they can take on, see Table~\ref{tb:data} in Appendix~\ref{app:data}.
It is unclear how to incorporate the knowledge about the number of unique variations in the Gaussian VAE.
Thus, previous work dismisses this information entirely \cite{locatello2019disentangling}.
In contrast, it is straightforward to implement this information in the D-VAE using masked attention as introduced for the transformer architecture \cite{vaswani2017attention}.
If we know that factor $z_i$ can assume a total of $m^\prime < m$ distinct values, we set the set of the $m^\prime$ active categories to be
$J_i = \{1 + \lfloor j \tfrac{m-1}{m^\prime-1} \rceil\}_{j=0}^{m^\prime-1} \subseteq [m]$ and set $\alpha_i^j = 0$ for all $j \not\in J_i$.
We experiment with both the masked and the unmasked semi-supervision.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Further experiments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Further experiments} \label{app:further}
%
We explore the usefulness of different disentanglement metrics for downstream tasks, revealing that the MIG score is the most reliable indicator of sample efficiency across different datasets.
%
\subsubsection{Which disentanglement metric is useful for downstream tasks regarding the sample complexity of learning?}
In this experiment, we want to determine which disentanglement metric indicates a sound discrete latent space with respect to downstream tasks.
We follow the simple downstream classification task from \cite{locatello2019challenging} of recovering the true factors of variations from the learned representation using either multi-class logistic regression (LR) or gradient-boosted trees (GBT).
More precisely, we sample training sets of two different sizes, $100$ and $10\;000$, and evaluate the average test accuracy across factors on a test set of size $5\;000$, respectively.
To analyze the sample complexity, we measure the Spearman rank correlation between the different disentanglement metrics and the statistical efficiency that is, the test accuracy based on $100$ training samples divided by the accuracy based on $10\;000$ samples.
The right side of Figure~\ref{fig:st-gap_downstream} depicts this correlation regarding the LR task for all six datasets.
We can observe a high variance of the correlation depending on the selected disentanglement metric.
The correlation with the DCI, Modularity, and SAP scores depends on the data, while a high BetaVAE or FactorVAE score even negatively impacts the statistical efficiency.
Only a high MIG score reliably leads to a higher sample efficiency over all six datasets.
The experiments regarding the GBT task in Figure~\ref{fig:app_eff_gbt} mostly confirm this finding.
Consequently, we are mainly interested in the structural behavior of discrete representations regarding the MIG disentanglement score.
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Implementation Details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Implementation details} \label{app:impl}
Locatello et al. \cite{locatello2019challenging} unified the choice of architecture, batch size, and optimizer to guarantee a fair comparison among the different methods.
We adopt these unifications and describe them here for the sake of completeness.
The only differences emerge from the Gumbel-softmax distribution from Equation~\ref{eq:gum_softmax}.
For all experiments, we choose the same number of $m=64$ categories.
If not mentioned differently, we utilize the symmetric interval $[-1,1]$ for the latent variable.
As proposed in \cite{friede2021efficient}, we utilize a constant Gumbel-softmax temperature of $\lambda=1.0$ and, instead, increase the scale parameter of the Gumbel distribution from $0.5$ to $2.0$ w.r.t. a cosine annealing and set the scale parameter to $0.0$ at test time.
We found this annealing scheme to improve training stability while encouraging discrete representations.
The implementation of the architectures is depicted in Table~\ref{tb:arch}, all hyperparameters can be found in Table~\ref{tb:hyper}.
We utilize the spatial broadcast decoder \cite{watters2019spatial} for the Circles experiments with a latent space dimension of $n=2$.
The implementations for the Circles experiments can be found in Table~\ref{tb:circle}.
If not mentioned differently, we utilize the ReLU activation function.
%
%
%
\begin{table*}[ht]
\caption{%
The architectures of the encoders and the decoder for the main experiments.}
\label{tb:arch}
\begin{center}
\begin{tabular}{l@{\hskip .3in}l@{\hskip .3in}l}
\toprule
\textbf{Encoder} (Gaussian) & \textbf{Encoder} (Discrete)   & \textbf{Decoder}\\
\midrule
Input: $64\times64\times C$ & Input: $64\times64\times C$   & Input: $10$ \\
Conv($4\times4,\;32,\;s=2$) & Conv($4\times4,\;32,\;s=2$)   & FC($256$) \\
Conv($4\times4,\;32,\;s=2$) & Conv($4\times4,\;32,\;s=2$)   & FC($4\times4\times64$) \\
Conv($4\times4,\;64,\;s=2$) & Conv($4\times4,\;64,\;s=2$)   & DeConv($4\times4,\;64,\;s=2$) \\
Conv($4\times4,\;64,\;s=2$) & Conv($4\times4,\;64,\;s=2$)   & DeConv($4\times4,\;32,\;s=2$) \\
FC($256$)                   & FC($256$)                     & DeConv($4\times4,\;32,\;s=2$) \\
FC($2\times10$)             & FC($10\times64$)              & DeConv($4\times4,\;C,\;s=2$) \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
%
\begin{table*}[ht]
\caption{%
The architectures of the discriminator for the TC regularizing experiments and the spatial broadcast decoder \cite{watters2019spatial} for the Circles experiments.}
\label{tb:circle}
\begin{center}
\begin{tabular}{l@{\hskip .5in}l}
\toprule
\textbf{Discriminator} & \textbf{Decoder} (Circles) \\
\midrule
FC($1000$), leaky ReLU      & Input: $2$    \\
FC($1000$), leaky ReLU      & Tile($64\times64\times10$)    \\
FC($1000$), leaky ReLU      & Concat. coordinate channels    \\
FC($1000$), leaky ReLU      & Conv($4\times4,\;64,\;s=1$)    \\
FC($1000$), leaky ReLU      & Conv($4\times4,\;64,\;s=1$)    \\
FC($1000$), leaky ReLU      & Conv($4\times4,\;C,\;s=1$)    \\
FC($2$)                     & \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
%
\begin{table*}[ht]
\caption{%
The model's hyperparameters.}
\label{tb:hyper}
\begin{center}
\begin{tabular}{l@{\hskip .3in}l@{\hskip .3in}l}
\toprule
\textbf{Parameter}              & \textbf{Model}            & \textbf{Values}\\
\midrule
Decoder type                &                   & Bernoulli  \\
Batch size                  &                   & $64$ \\
Latent space dim.      &                   & $10$ \\
Optimizer                   &                   & Adam \\
Adam: $\beta_1$             &                   & $0.9$ \\
Adam: $\beta_2$             &                   & $0.999$ \\
% Adam: $\epsilon$             &                   & $1e^{-8}$ \\
Learning rate               &                   & $1e^{-4}$ \\
Training steps              &                   & $300\;000$ \\
Latent space dim. (Circles) & Circles                  & $2$ \\
Number of categories        & discrete          & $64$ \\
Gumbel scale: init          & discrete          & $0.5$ \\
Gumbel scale: final         & discrete          & $2.0$ \\
Disc. Adam: $\beta_1$       & TC regularizing   & $0.5$ \\
Disc. Adam: $\beta_2$       & TC regularizing   & $0.9$ \\
% Disc. Adam: $\epsilon$             & TC regularizing                  & $1e^{-8}$ \\
$\gamma$                    & TC regularizing   & $[10,20,30,40,50,100]$ \\
$\omega$                    & semi-supervised   & $[1,2,4,6,8,16]$ \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
%
%
\begin{table*}[t]
\caption{%
The $25\%$ and the $75\%$ quantile MIG scores in \% for state-of-the-art unsupervised methods compared to the discrete methods.
Results taken from \cite{locatello2019challenging} are marked with an asterisk~(*).
We have re-implemented all other results with the same architecture as in \cite{locatello2019challenging} for the sake of fairness.}
\label{tb:mig_25}
\begin{center}
\begin{tabular}{lcccccc}
\toprule
Model & dSprites & C-dSprites & SmallNORB & Cars3D & Shapes3D & MPI3D\\
\midrule
$\beta$-VAE \cite{higgins2017beta}    &\scriptsize{[7.5,15.8]}$^*$&\scriptsize{[9.7,14.6]}$^*$& \scriptsize{[19.1,22.8]}$^*$&\scriptsize{[5.6,11.7]}$^*$& n.a.& n.a.\\
$\beta$-TCVAE \cite{chen2018isolating}  &\scriptsize{[13.6,22.2]}$^*$&\scriptsize{[10.4,18.0]}$^*$& \scriptsize{[18.3,24.5]}$^*$&\scriptsize{[7.3,14.0]}$^*$& n.a.& n.a.\\
DIP-VAE-I \cite{kumar2017variational}      &\scriptsize{[1.9,9.4]}$^*$&\scriptsize{[2.4,9.0]}$^*$& \scriptsize{[8.5,20.9]}$^*$&\scriptsize{[3.4,7.2]}$^*$& n.a.& n.a.         \\
DIP-VAE-II \cite{kumar2017variational}     &\scriptsize{[3.6,8.6]}$^*$&\scriptsize{[3.2,7.9]}$^*$& \scriptsize{[22.4,25.4]}$^*$&\scriptsize{[2.7,6.4]}$^*$& n.a.& n.a.\\
AnnealedVAE \cite{burgess2018understanding}    &\scriptsize{[2.9,20.9]}$^*$&\scriptsize{[4.8,25.7]}$^*$& \scriptsize{[1.5,8.1]}$^*$&\scriptsize{[4.6,7.7]}$^*$& n.a.& n.a.\\
FactorVAE \cite{kim2018disentangling}      &\scriptsize{[12.6,26.3]}&\scriptsize{[11.7,20.9]}& \scriptsize{[24.0,26.4]}&\scriptsize{[7.2,10.6]}&\scriptsize{[27.0,44.3]}&\scriptsize{[6.9,31.3]}\\
\midrule
D-VAE          &\scriptsize{[13.2,20.0]}&\scriptsize{[5.5,13.4]}&\scriptsize{[16.3,21.8]}& \scriptsize{[5.8,11.1]}& \scriptsize{[21.8,34.2]}&\scriptsize{[8.9,16.5]}\\
FactorDVAE    &\scriptsize{[14.5,35.7]}&\scriptsize{[11.3,20.3]}&\scriptsize{[20.6,24.8]}&\scriptsize{[12.8,16.3]}& \scriptsize{[34.8,48.3]}&\scriptsize{[26.0,32.1]}\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
% Figure environment removed
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Median Table
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{table*}[ht]
\setlength{\tabcolsep}{3.5pt}
\caption{%
The $50\%$ (median), $25\%$, and $75\%$ quantiles in \% of the unsupervised D-VAE over all metrics.}
\label{tb:quant}
\begin{center}
\begin{tabular}{lcccccc}
\toprule
Metric & dSprites & C-dSprites & SmallNORB & Cars3D & Shapes3D & MPI3D\\
\midrule
BetaVAE&86.2&83.6&88.1&100.0&100.0&72.3\\
&\scriptsize{[85.4,86.6]}&\scriptsize{[81.9,85.0]}&\scriptsize{[85.6,90.2]}&\scriptsize{[100.0,100.0]}&\scriptsize{[99.5,100.0]}&\scriptsize{[67.9,76.5]}\\
FactorVAE&67.4&67.5&70.0&91.6&94.0&49.6\\
&\scriptsize{[61.9,71.9]}&\scriptsize{[60.3,71.3]}&\scriptsize{[66.9,73.3]}&\scriptsize{[89.0,94.0]}&\scriptsize{[88.3,98.2]}&\scriptsize{[46.7,53.4]}\\
MIG&17.4&9.4&19.0&8.5&28.8&12.8\\
&\scriptsize{[13.2,20.0]}&\scriptsize{[5.5,13.4]}&\scriptsize{[16.3,21.8]}&\scriptsize{[5.8,11.1]}&\scriptsize{[21.8,34.2]}&\scriptsize{[8.9,16.5]}\\
DCI&25.6&16.7&31.5&25.1&72.8&29.9\\
&\scriptsize{[19.6,28.0]}&\scriptsize{[12.7,20.4]}&\scriptsize{[29.5,32.7]}&\scriptsize{[21.0,29.4]}&\scriptsize{[68.1,78.2]}&\scriptsize{[27.6,31.7]}\\
Modularity&86.7&89.4&79.0&87.7&96.1&88.7\\
&\scriptsize{[84.5,88.6]}&\scriptsize{[87.0,91.2]}&\scriptsize{[76.5,81.3]}&\scriptsize{[85.8,89.3]}&\scriptsize{[94.9,97.2]}&\scriptsize{[87.2,89.9]}\\
SAP&6.6&2.5&8.6&1.4&7.4&5.5\\
&\scriptsize{[5.1,7.2]}&\scriptsize{[1.5,3.7]}&\scriptsize{[7.4,9.6]}&\scriptsize{[0.8,2.2]}&\scriptsize{[5.6,9.9]}&\scriptsize{[4.3,7.8]}\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Median Table Semi-sup
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\begin{table*}[ht]
\setlength{\tabcolsep}{3.5pt}
\caption{%
The $50\%$ (median), $25\%$, and $75\%$ quantiles in \% of the MIG score for the discrete semi-supervised models D-VAE, D-VAE (Masked) (M), FactorDVAE, FactorDVAE (Masked) (M) for $1000$ labels.}
\label{tb:quant_semi}
\begin{center}
\begin{tabular}{lcccccc}
\toprule
Model & dSprites & C-dSprites & SmallNORB & Cars3D & Shapes3D & MPI3D\\
\midrule
D-VAE&32.0&28.4&15.8&17.7&45.8&39.2\\
&\scriptsize{[28.9,36.2]}&\scriptsize{[27.3,31.3]}&\scriptsize{[14.7,22.5]}&\scriptsize{[10.6,23.3]}&\scriptsize{[38.9,49.6]}&\scriptsize{[36.7,44.9]}\\
D-VAE (M)&32.0&27.2&25.3&22.0&46.0&52.1\\
&\scriptsize{[29.9,33.9]}&\scriptsize{[24.5,32.0]}&\scriptsize{[23.3,28.4]}&\scriptsize{[14.5,28.6]}&\scriptsize{[40.7,51.6]}&\scriptsize{[43.7,55.2]}\\
F-DVAE&37.6&37.4&27.2&11.4&38.8&36.5\\
&\scriptsize{[35.4,39.4]}&\scriptsize{[30.9,38.9]}&\scriptsize{[23.2,32.1]}&\scriptsize{[9.3,13.2]}&\scriptsize{[34.6,49.6]}&\scriptsize{[32.0,50.1]}\\
F-DVAE (M)&37.3&34.1&33.6&8.8&29.3&48.1\\
&\scriptsize{[29.6,38.4]}&\scriptsize{[23.2,37.0]}&\scriptsize{[26.8,37.3]}&\scriptsize{[5.9,10.1]}&\scriptsize{[23.0,42.3]}&\scriptsize{[35.2,52.7]}\\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dataset Detail
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Dataset details} \label{app:data}
All datasets are rendered in images of size $64\times64$ and normalized to $[0,1]$.
As in \cite{locatello2019challenging}, we directly sample from the generative model, effectively avoiding overfitting.
We consider gray-scale datasets dSprites, SmallNORB, and Circles, as well as datasets with three color channels C-dSprites, Cars3D, Shapes3D, and MPI3D.
We followed the instructions from \cite{watters2019spatial} to create the Circles dataset utilizing the Spriteworld environment \cite{watters2019spriteworld}, setting the size to $0.2$.
Table~\ref{tb:data} contains a set of all ground-truth factors of variation for each dataset.
%
%
%
%
%
\begin{table*}[ht!]
\caption{%
The ground-truth factors of the datasets.}
\label{tb:data}
\begin{center}
\begin{tabular}{l@{\hskip .3in}l@{\hskip .3in}c}
\toprule
\textbf{Dataset}            & \textbf{Ground-truth factor}        & \textbf{Number of values}\\
\midrule
dSprites               &  Shape                 & $3$  \\
                       &  Scale                 & $6$  \\
                       &  Orientation                 & $40$  \\
                       &  X-Position                 & $32$  \\
                       &  Y-Position                 & $32$  \\
\midrule
C-dSprites               &  Shape                 & $3$  \\
                       &  Scale                 & $6$  \\
                       &  Orientation                 & $40$  \\
                       &  X-Position                 & $32$  \\
                       &  Y-Position                 & $32$  \\
                       &  Color                 & Uniform$(0.5, 1.0)^3$  \\
\midrule
SmallNORB               &  Category                 & $5$  \\
                       &  Elevation               & $9$  \\
                       &  Azimuth                 & $18$  \\
                       &  Lighting condition                 & $6$  \\
\midrule
Cars3D                 &  Elevation               & $4$  \\
                       &  Azimuth                 & $24$  \\
                       &  Object type                 & $183$  \\
\midrule
Shapes3D               &  Floor color                 & $10$  \\
                       &  Wall color               & $10$  \\
                       &  Object color               & $10$  \\
                       &  Object size               & $8$  \\
                       &  Object type               & $4$  \\
                       &  Azimuth                 & $15$  \\
\midrule
MPI3D               &  Object color                 & $4$  \\
                       &  Object shape               & $4$  \\
                       &  Object size               & $2$  \\
                       &  Camera height               & $3$  \\
                       &  Background colors               & $3$  \\
                       &  First DOF                 & $40$  \\                       
                       &  Second DOF                 & $40$  \\
\midrule
Circles                & X-Position                 & Uniform($0.2, 0.8$)  \\
                       &  Y-Position                 & Uniform($0.2, 0.8$)  \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Detailed experimental results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\section{Detailed experimental results} \label{app:exp}
%
\subsubsection{Quantiles of the experimental results}
The $25\%$ and the $75\%$ quantile MIG scores in \% for state-of-the-art unsupervised methods compared to the discrete methods can be found in Table~\ref{tb:mig_25}.
The $50\%$ (median), $25\%$, and $75\%$ quantiles in \% of D-VAE over all metrics can be found in Table~\ref{tb:quant}.
The quantiles of the MIG score for the semi-supervised models with $1000$ labels can be found in Table~\ref{tb:quant_semi}.
%
\subsubsection{Circles experiment.}
The latent space visualizations of the circles experiment \cite{watters2019spatial}, sorted by the MIG score of all $50$ models of the Gaussian VAE and the discrete VAE, respectively.
Figure~\ref{fig:50_vae} depicts the Gaussian latent spaces.
Even the latent spaces yielding the best MIG scores are affected by rotation.
Figure~\ref{fig:50_d-vae} depicts the discrete latent spaces.
More than $25$\% of the latent spaces lie parallel to the axes.
%
% Figure environment removed
%
% Figure environment removed
%
%
%
%
%
\subsubsection{Comparison of the unregularized models.}
Figure~\ref{fig:app_unreg_all_1} and Figure~\ref{fig:app_unreg_all_2} depict the comparison of the unregularized models as violin plots for all datasets and metrics.
The discrete VAE improves over its Gaussian counterpart in $31$ out of $36$ cases.
%
% Figure environment removed
%
% Figure environment removed


%
\end{subappendices}
