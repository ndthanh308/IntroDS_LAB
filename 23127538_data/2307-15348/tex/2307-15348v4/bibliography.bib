@book{jolliffe_principal_2002,
	author = {Jolliffe, Ian T.},
	title = {Principal {Component} {Analysis}},
	language = {en},
	publisher = {Springer},
	year = {2002},
	keywords = {statistics, principal component analysis, Factor analysis, Regression analysis, time series},
}

@article{jolliffe_principal_2016,
author = {Jolliffe, Ian T.  and Cadima, Jorge },
title = {Principal component analysis: a review and recent developments},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
volume = {374},
number = {2065},
pages = {20150202},
year = {2016},
doi = {10.1098/rsta.2015.0202},
}

@article{jolliffe_rotation_1989,
	title = {Rotation of {Ill}-{Defined} {Principal} {Components}},
	volume = {38},
	issn = {0035-9254},
	doi = {10.2307/2347688},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
	author = {Jolliffe, I. T.},
	year = {1989},
	pages = {139--147},
}

@article{sirovich_low-dimensional_1987,
	title = {Low-dimensional procedure for the characterization of human faces},
	volume = {4},
	copyright = {© 1987 Optical Society of America},
	issn = {1520-8532},
	doi = {10.1364/JOSAA.4.000519},
	number = {3},
	journal = {JOSA A},
	author = {Sirovich, L. and Kirby, M.},
	year = {1987},
	keywords = {Copper, Crosstalk, Fourier transforms, Illumination, Information processing, Video},
	pages = {519--524},
}

@book{bellman_dynamic_1984,
	title = {Dynamic programming},
	isbn = {978-0-691-07951-6},
	language = {en},
	publisher = {Princeton Univ. Pr},
	author = {Bellman, Richard},
	year = {1984},
}

@article{arnold_modes_1972,
	title = {Modes and quasimodes},
	volume = {6},
	issn = {1573-8485},
	doi = {10.1007/BF01077511},
	language = {en},
	number = {2},
	urldate = {2023-11-07},
	journal = {Functional Analysis and Its Applications},
	author = {Arnold, V. I.},
	year = {1972},
	keywords = {Functional Analysis},
	pages = {94--101},
}

@article{north_sampling_1982,
	title = {Sampling {Errors} in the {Estimation} of {Empirical} {Orthogonal} {Functions}},
	volume = {110},
	issn = {1520-0493, 0027-0644},
	doi = {10.1175/1520-0493(1982)110<0699:SEITEO>2.0.CO;2},
	language = {EN},
	number = {7},
	journal = {Monthly Weather Review},
	author = {North, Gerald R. and Bell, Thomas L. and Cahalan, Robert F. and Moeng, Fanthune J.},
	year = {1982},
	pages = {699--706},
}

@article{tipping_probabilistic_1999,
	title = {Probabilistic {Principal} {Component} {Analysis}},
	volume = {61},
	doi = {10.1111/1467-9868.00196},
	abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
	number = {3},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Tipping, Michael E. and Bishop, Christopher M.},
	year = {1999},
	pages = {611--622},
}

@article{tipping_mixtures_1999,
	title = {Mixtures of {Probabilistic} {Principal} {Component} {Analyzers}},
	volume = {11},
	issn = {0899-7667},
	doi = {10.1162/089976699300016728},
	abstract = {Principal component analysis (PCA) is one of the most popular techniques for processing, compressing, and visualizing data, although its effectiveness is limited by its global linearity. While nonlinear variants of PCA have been proposed, an alternative paradigm is to capture data complexity by a combination of local linear PCA projections. However, conventional PCA does not correspond to a probability density, and so there is no unique way to combine PCA models. Therefore, previous attempts to formulate mixture models for PCA have been ad hoc to some extent. In this article, PCA is formulated within a maximum likelihood framework, based on a specific form of gaussian latent variable model. This leads to a well-defined mixture model for probabilistic principal component analyzers, whose parameters can be determined using an expectation-maximization algorithm. We discuss the advantages of this model in the context of clustering, density modeling, and local dimensionality reduction, and we demonstrate its application to image compression and handwritten digit recognition.},
	number = {2},
	journal = {Neural Computation},
	author = {Tipping, Michael E. and Bishop, Christopher M.},
	year = {1999},
	pages = {443--482},
}

@article{bouveyron_intrinsic_2011,
	title = {Intrinsic dimension estimation by maximum likelihood in isotropic probabilistic {PCA}},
	volume = {32},
	number = {14},
	journal = {Pattern Recognition Letters},
	author = {Bouveyron, Charles and Celeux, Gilles and Girard, Stéphane},
	year = {2011},
    doi = {10.1016/j.patrec.2011.07.017},
	pages = {1706--1713},
}

@article{bouveyron_high-dimensional_2007,
	title = {High-dimensional data clustering},
	volume = {52},
	issn = {0167-9473},
	doi = {10.1016/j.csda.2007.02.009},
	language = {en},
	number = {1},
	urldate = {2022-11-14},
	journal = {Computational Statistics \& Data Analysis},
	author = {Bouveyron, C. and Girard, S. and Schmid, C.},
	year = {2007},
	keywords = {Model-based clustering, Subspace clustering, Gaussian mixture models, High-dimensional data, Parsimonious models},
	pages = {502--519},
}

@article{schwarz_estimating_1978,
	title = {Estimating the {Dimension} of a {Model}},
	volume = {6},
	abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
	number = {2},
	urldate = {2022-09-27},
	journal = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	year = {1978},
	pages = {461--464},
}

@article{tyler_asymptotic_1981,
	title = {Asymptotic {Inference} for {Eigenvectors}},
	volume = {9},
	abstract = {Asymptotic procedures are given for testing certain hypotheses concerning eigenvectors and for constructing confidence regions for eigenvectors. These asymptotic procedures are derived under fairly general conditions on the estimates of the matrix whose eigenvectors are of interest. Applications of the general results to principal components analysis and canonical variate analysis are given.},
	number = {4},
	urldate = {2023-05-18},
	journal = {The Annals of Statistics},
	author = {Tyler, David E.},
	year = {1981},
	pages = {725--736},
    doi = {10.1214/aos/1176345514}
}

@article{anderson_asymptotic_1963,
	title = {Asymptotic {Theory} for {Principal} {Component} {Analysis}},
	volume = {34},
	issn = {0003-4851, 2168-8990},
	doi = {10.1214/aoms/1177704248},
	abstract = {The Annals of Mathematical Statistics},
	number = {1},
	urldate = {2024-04-26},
	journal = {The Annals of Mathematical Statistics},
	author = {Anderson, T. W.},
	year = {1963},
	pages = {122--148},
}

@book{bishop_pattern_2006,
	title = {Pattern recognition and machine learning},
	isbn = {978-0-387-31073-2},
	language = {en},
	publisher = {Springer},
	author = {Bishop, Christopher M.},
	year = {2006},
	keywords = {Machine learning, Pattern perception},
}

@article{myung_counting_2000,
	title = {Counting probability distributions: {Differential} geometry and model selection},
	volume = {97},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Counting probability distributions},
	doi = {10.1073/pnas.170283897},
	language = {en},
	number = {21},
	urldate = {2023-02-24},
	journal = {PNAS},
	author = {Myung, In Jae and Balasubramanian, Vijay and Pitt, Mark A.},
	year = {2000},
	pages = {11170--11175},
}

@article{breiding_geometry_2018,
	title = {On the {Geometry} of the {Set} of {Symmetric} {Matrices} with {Repeated} {Eigenvalues}},
	volume = {4},
	abstract = {We investigate some geometric properties of the real algebraic variety \$\${\textbackslash}Delta \$\$of symmetric matrices with repeated eigenvalues. We explicitly compute the volume of its intersection with the sphere and prove a Eckart–Young–Mirsky-type theorem for the distance function from a generic matrix to points in \$\${\textbackslash}Delta \$\$. We exhibit connections of our study to real algebraic geometry (computing the Euclidean distance degree of \$\${\textbackslash}Delta \$\$) and random matrix theory.},
	language = {en},
	number = {3},
	urldate = {2022-06-22},
	journal = {Arnold Mathematical Journal},
	author = {Breiding, Paul and Kozhasov, Khazhgali and Lerario, Antonio},
	year = {2018},
	keywords = {Euclidean distance degree theory, Integral geometry, Random matrices},
	pages = {423--443},
    doi = {10.1007/s40598-018-0095-0}
}

@inproceedings{ma_flag_2021,
	title = {The {Flag} {Manifold} as a {Tool} for {Analyzing} and {Comparing} {Sets} of {Data} {Sets}},
	doi = {10.1109/ICCVW54120.2021.00465},
	language = {English},
	urldate = {2023-07-07},
	booktitle = {ICCV Workshops},
	author = {Ma, Xiaofeng and Kirby, Michael and Peterson, Chris},
	year = {2021},
	pages = {4168--4177},
}

@inproceedings{mankovich_chordal_2023,
	title = {Chordal {Averaging} on {Flag} {Manifolds} and {Its} {Applications}},
	language = {en},
	urldate = {2024-06-21},
	booktitle = {ICCV},
	author = {Mankovich, Nathan and Birdal, Tolga},
	year = {2023},
	pages = {3881--3890},
}

@inproceedings{mankovich_fun_2024,
	title = {Fun with {Flags}: {Robust} {Principal} {Directions} via {Flag} {Manifolds}},
	shorttitle = {Fun with {Flags}},
	booktitle = {CVPR},
	author = {Mankovich, Nathan and Camps-Valls, Gustau and Birdal, Tolga},
	year = {2024},
	pages = {330--340},
}

@inproceedings{lefevre_perturbation_2023,
	series = {LNCS},
	title = {Perturbation of {Fiedler} {Vector}: {Interest} for {Graph} {Measures} and {Shape} {Analysis}},
	shorttitle = {Perturbation of {Fiedler} {Vector}},
	language = {en},
	booktitle = {GSI},
	publisher = {Springer},
	author = {Lefevre, Julien and Fraize, Justine and Germanaud, David},
	year = {2023},
	keywords = {Shape analysis, Fiedler vector, Graph Laplacian},
	pages = {593--601},
    doi = {10.1007/978-3-031-38299-4_61}
}

    
@inproceedings{nishimori_riemannian_2006,
	series = {LNCS},
	title = {Riemannian {Optimization} {Method} on the {Flag} {Manifold} for {Independent} {Subspace} {Analysis}},
	language = {en},
	booktitle = {ICA},
	publisher = {Springer},
	author = {Nishimori, Yasunori and Akaho, Shotaro and Plumbley, Mark D.},
	year = {2006},
	keywords = {Gradient Descent Method, Image Patch, Independent Component Analysis, Orthogonal Group},
	pages = {295--302},
    doi = {10.1007/11679363_37}
}

@article{ye_optimization_2022,
	title = {Optimization on flag manifolds},
	volume = {194},
	issn = {1436-4646},
	doi = {10.1007/s10107-021-01640-3},
	language = {en},
	number = {1},
	urldate = {2024-01-19},
	journal = {Mathematical Programming},
	author = {Ye, Ke and Wong, Ken Sze-Wai and Lim, Lek-Heng},
	year = {2022},
	keywords = {14M15, 62H10, 62H12, 68T10, 90C30, Flag manifold, Manifold optimization, Multiresolution, Multiscale, Riemannian optimization},
	pages = {621--660},
}

@article{zhu_practical_2024,
	title = {Practical gradient and conjugate gradient methods on flag manifolds},
	volume = {88},
	issn = {1573-2894},
	doi = {10.1007/s10589-024-00568-6},
	abstract = {Flag manifolds, sets of nested sequences of linear subspaces with fixed dimensions, are rising in numerical analysis and statistics. The current optimization algorithms on flag manifolds are based on the exponential map and parallel transport, which are expensive to compute. In this paper we propose practical optimization methods on flag manifolds without the exponential map and parallel transport. Observing that flag manifolds have a similar homogeneous structure with Grassmann and Stiefel manifolds, we generalize some typical retractions and vector transports to flag manifolds, including the Cayley-type retraction and vector transport, the QR-based and polar-based retractions, the projection-type vector transport and the projection of the differentiated polar-based retraction as a vector transport. Theoretical properties and efficient implementations of the proposed retractions and vector transports are discussed. Then we establish Riemannian gradient and Riemannian conjugate gradient algorithms based on these retractions and vector transports. Numerical results on the problem of nonlinear eigenflags demonstrate that our algorithms have a great advantage in efficiency over the existing ones.},
	language = {en},
	number = {2},
	urldate = {2024-06-21},
	journal = {Computational Optimization and Applications},
	author = {Zhu, Xiaojing and Shen, Chungen},
	year = {2024},
	keywords = {65K05, 90C06, 90C30, 90C48, Flag manifold, Manifold optimization, Riemannian conjugate gradient method, Riemannian gradient method},
	pages = {491--524},
}

@article{pennec_barycentric_2018,
	title = {Barycentric {Subspace} {Analysis} on {Manifolds}},
	volume = {46},
	abstract = {This paper investigates the generalization of Principal Component Analysis (PCA) to Riemannian manifolds. We first propose a new and general type of family of subspaces in manifolds that we call barycentric subspaces. They are implicitly defined as the locus of points which are weighted means of k + 1 reference points. As this definition relies on points and not on tangent vectors, it can also be extended to geodesic spaces which are not Riemannian. For instance, in stratified spaces, it naturally allows principal subspaces that span several strata, which is impossible in previous generalizations of PCA. We show that barycentric subspaces locally define a submanifold of dimension k which generalizes geodesic subspaces. Second, we rephrase PCA in Euclidean spaces as an optimization on flags of linear subspaces (a hierarchy of properly embedded linear subspaces of increasing dimension). We show that the Euclidean PCA minimizes the Accumulated Unexplained Variances by all the subspaces of the flag (AUV). Barycentric subspaces are naturally nested, allowing the construction of hierarchically nested subspaces. Optimizing the AUV criterion to optimally approximate data points with flags of affine spans in Riemannian manifolds lead to a particularly appealing generalization of PCA on manifolds called Barycentric Subspace Analysis (BSA).},
	number = {6A},
	urldate = {2023-07-07},
	journal = {The Annals of Statistics},
	author = {Pennec, Xavier},
	year = {2018},
	pages = {2711--2746},
    doi = {10.1214/17-AOS1636}
}


@article{groisser_geometric_2017,
	title = {Geometric foundations for scaling-rotation statistics on symmetric positive definite matrices: {Minimal} smooth scaling-rotation curves in low dimensions},
	volume = {11},
	issn = {1935-7524, 1935-7524},
	shorttitle = {Geometric foundations for scaling-rotation statistics on symmetric positive definite matrices},
	doi = {10.1214/17-EJS1250},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Groisser, David and Jung, Sungkyu and Schwartzman, Armin},
	year = {2017},
	pages = {1092--1159},
}


@inproceedings{jenatton_structured_2010,
	title = {Structured {Sparse} {Principal} {Component} {Analysis}},
	booktitle = {AISTATS},
	publisher = {PMLR},
	author = {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
	year = {2010},
	pages = {366--373},
}

@article{olshausen_emergence_1996,
	title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
	volume = {381},
	copyright = {1996 Springer Nature Limited},
	issn = {1476-4687},
	doi = {10.1038/381607a0},
	abstract = {THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1–4 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7–12. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13–18, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
	language = {en},
	number = {6583},
	urldate = {2023-06-09},
	journal = {Nature},
	author = {Olshausen, Bruno A. and Field, David J.},
	year = {1996},
	keywords = {Humanities and Social Sciences, multidisciplinary, Science},
	pages = {607--609},
}

@article{field_relations_1987,
	title = {Relations between the statistics of natural images and the response properties of cortical cells},
	volume = {4},
	copyright = {© 1987 Optical Society of America},
	issn = {1520-8532},
	doi = {10.1364/JOSAA.4.002379},
	abstract = {The relative efficiency of any particular image-coding scheme should be defined only in relation to the class of images that the code is likely to encounter. To understand the representation of images by the mammalian visual system, it might therefore be useful to consider the statistics of images from the natural environment (i.e., images with trees, rocks, bushes, etc). In this study, various coding schemes are compared in relation to how they represent the information in such natural images. The coefficients of such codes are represented by arrays of mechanisms that respond to local regions of space, spatial frequency, and orientation (Gabor-like transforms). For many classes of image, such codes will not be an efficient means of representing information. However, the results obtained with six natural images suggest that the orientation and the spatial-frequency tuning of mammalian simple cells are well suited for coding the information in such images if the goal of the code is to convert higher-order redundancy (e.g., correlation between the intensities of neighboring pixels) into first-order redundancy (i.e., the response distribution of the coefficients). Such coding produces a relatively high signal-to-noise ratio and permits information to be transmitted with only a subset of the total number of cells. These results support Barlow’s theory that the goal of natural vision is to represent the information in the natural environment with minimal redundancy.},
	language = {EN},
	number = {12},
	urldate = {2024-05-24},
	journal = {JOSA A},
	author = {Field, David J.},
	year = {1987},
	keywords = {Edge detection, Fourier transforms, Image sensors, Information processing, Physiology, Position sensors},
	pages = {2379--2394},
}

@article{basri_lambertian_2003,
	title = {Lambertian reflectance and linear subspaces},
	volume = {25},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2003.1177153},
	abstract = {We prove that the set of all Lambertian reflectance functions (the mapping from surface normals to intensities) obtained with arbitrary distant light sources lies close to a 9D linear subspace. This implies that, in general, the set of images of a convex Lambertian object obtained under a wide variety of lighting conditions can be approximated accurately by a low-dimensional linear subspace, explaining prior empirical results. We also provide a simple analytic characterization of this linear space. We obtain these results by representing lighting using spherical harmonics and describing the effects of Lambertian materials as the analog of a convolution. These results allow us to construct algorithms for object recognition based on linear methods as well as algorithms that use convex optimization to enforce nonnegative lighting functions. We also show a simple way to enforce nonnegative lighting when the images of an object lie near a 4D linear space. We apply these algorithms to perform face recognition by finding the 3D model that best matches a 2D query image.},
	number = {2},
	urldate = {2023-10-18},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Basri, R. and Jacobs, D.W.},
	year = {2003},
	pages = {218--233},
}


@inproceedings{szwagier_rethinking_2023,
	series = {LNCS},
	title = {Rethinking the {Riemannian} {Logarithm} on {Flag} {Manifolds} as an {Orthogonal} {Alignment} {Problem}},
	doi = {10.1007/978-3-031-38271-0_37},
	booktitle = {GSI},
	publisher = {Springer},
	author = {Szwagier, Tom and Pennec, Xavier},
	year = {2023},
	keywords = {Flag manifolds, Riemannian logarithm, Orthogonal alignment, Procrustes analysis},
	pages = {375--383},
}

@book{murphy_probabilistic_2023,
	title = {Probabilistic machine learning: advanced topics},
	isbn = {978-0-262-04843-9},
	shorttitle = {Probabilistic machine learning},
	abstract = {"An advanced book for researchers and graduate students working in machine learning and statistics that reflects the influence of deep learning"--},
	language = {en},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	year = {2023},
	keywords = {Machine learning, Probabilities},
}


@article{rohe_vintage_2023,
	title = {Vintage factor analysis with {Varimax} performs statistical inference},
	volume = {85},
	issn = {1369-7412},
	doi = {10.1093/jrsssb/qkad029},
	number = {4},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Rohe, Karl and Zeng, Muzhe},
	year = {2023},
	pages = {1037--1060},
}


@inproceedings{sommer_anisotropic_2015,
	series = {{LNCS}},
	title = {Anisotropic {Distributions} on {Manifolds}: {Template} {Estimation} and {Most} {Probable} {Paths}},
	volume = {9123},
	isbn = {978-3-319-19991-7 978-3-319-19992-4},
	shorttitle = {Anisotropic {Distributions} on {Manifolds}},
	language = {en},
	booktitle = {IPMI},
	publisher = {Springer},
	author = {Sommer, Stefan},
	year = {2015},
	pages = {193--204},
}


@article{lombaert_focusr_2013,
	title = {{FOCUSR}: feature oriented correspondence using spectral regularization--a method for precise surface matching},
	volume = {35},
	issn = {1939-3539},
	shorttitle = {{FOCUSR}},
	doi = {10.1109/TPAMI.2012.276},
	language = {eng},
	number = {9},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Lombaert, Herve and Grady, Leo and Polimeni, Jonathan R. and Cheriet, Farida},
	year = {2013},
	pmid = {23868776},
	pmcid = {PMC3707975},
	pages = {2143--2160},
}


@inproceedings{ng_spectral_2001,
	title = {On {Spectral} {Clustering}: {Analysis} and an algorithm},
	volume = {14},
	shorttitle = {On {Spectral} {Clustering}},
	booktitle = {NeurIPS},
	publisher = {MIT Press},
	author = {Ng, Andrew and Jordan, Michael and Weiss, Yair},
	year = {2001},
}

@article{belkin_laplacian_2003,
	title = {Laplacian {Eigenmaps} for {Dimensionality} {Reduction} and {Data} {Representation}},
	volume = {15},
	issn = {0899-7667},
	doi = {10.1162/089976603321780317},
	number = {6},
	urldate = {2023-12-22},
	journal = {Neural Computation},
	author = {Belkin, Mikhail and Niyogi, Partha},
	year = {2003},
	pages = {1373--1396},
}


@article{lambert_limited-memory_2023,
	title = {The limited-memory recursive variational {Gaussian} approximation ({L}-{RVGA})},
	volume = {33},
	language = {en},
	number = {70},
	urldate = {2023-09-22},
	journal = {Statistics and Computing},
	author = {Lambert, Marc and Bonnabel, Silvère and Bach, Francis},
	year = {2023},
	keywords = {Factor analysis, Bayesian logistic regression, Expectation-maximization, Kalman filter, Variational approximation},
    doi = {10.1007/s11222-023-10239-x}
}

@article{celeux_gaussian_1995,
	title = {Gaussian parsimonious clustering models},
	volume = {28},
	issn = {0031-3203},
	doi = {10.1016/0031-3203(94)00125-6},
	abstract = {Gaussian clustering models are useful both for understanding and suggesting powerful criteria. Banfield and Raftery, Biometriks 49, 803–821 (1993), have considered a parameterization of the variance matrix Σk of a cluster Pk in terms of its eigenvalue decomposition, Σk = λkDkAkDk′ where λk defines the volume of Pk, Dk is an orthogonal matrix which defines its orientation and Ak is a diagonal matrix with determinant 1 which defines its shape. This parametrization allows us to propose many general clustering criteria from the simplest one (spherical clusters with equal volumes which leads to the classical k-means criterion) to the most complex one (unknown and different volumes, orientations and shapes for all clusters). Methods of optimization to derive the maximum likelihood estimates as well as the practical usefulness of these models are discussed. We especially analyse the influence of the volumes of clusters. We report Monte Carlo simulations and an application on stellar data which dramatically illustrated the relevance of allowing clusters to have different volumes.},
	language = {en},
	number = {5},
	urldate = {2022-11-14},
	journal = {Pattern Recognition},
	author = {Celeux, Gilles and Govaert, Gérard},
	year = {1995},
	keywords = {Cluster volumes, Eigenvalue decomposition, Gaussian mixture},
	pages = {781--793},
}

@misc{olikier_first-order_2023,
	title = {First-order optimization on stratified sets},
	language = {en},
	publisher = {arXiv},
	author = {Olikier, Guillaume and Gallivan, Kyle A. and Absil, P.-A.},
	year = {2023},
	doi = {10.48550/arXiv.2303.16040},
	keywords = {Mathematics - Optimization and Control, Mathematics - Numerical Analysis, 65K10, 49J53, 90C26, 90C46, 58A35, 14M12, 15B99},
    note = {arXiv:2303.16040}
}


@book{absil_optimization_2009,
	title = {Optimization {Algorithms} on {Matrix} {Manifolds}},
	isbn = {978-1-4008-3024-4},
	language = {en},
	urldate = {2022-05-03},
	publisher = {Princeton University Press},
	author = {Absil, P.-A. and Mahony, R. and Sepulchre, Rodolphe},
	year = {2009},
}

@article{leygonie_gradient_2023,
	title = {A gradient sampling algorithm for stratified maps with applications to topological data analysis},
	volume = {202},
	issn = {1436-4646},
	doi = {10.1007/s10107-023-01931-x},
	language = {en},
	number = {1},
	journal = {Mathematical Programming},
	author = {Leygonie, Jacob and Carrière, Mathieu and Lacombe, Théo and Oudot, Steve},
	year = {2023},
	keywords = {49J52, 55N31, 57N80, 62R40, 90C26, Nonsmooth analysis, Optimization, Persistent homology},
	pages = {199--239},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {0035-9246},
	number = {1},
	urldate = {2023-08-09},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	pages = {267--288},
}


@article{geiger_stratified_2001,
	title = {Stratified exponential families: {Graphical} models and model selection},
	volume = {29},
	shorttitle = {Stratified exponential families},
	number = {2},
	journal = {The Annals of Statistics},
	author = {Geiger, Dan and Heckerman, David and King, Henry and Meek, Christopher},
	year = {2001},
	keywords = {60E05, 62H05, Bayesian networks, curved exponential families, graphical models, hidden variables, Model selection, semialgebraic sets, stratified exponential families},
	pages = {505--529},
    doi = {10.1214/aos/1009210550}
}

@article{kaiser_varimax_1958,
	title = {The varimax criterion for analytic rotation in factor analysis},
	volume = {23},
	issn = {1860-0980},
	doi = {10.1007/BF02289233},
	abstract = {An analytic criterion for rotation is defined. The scientific advantage of analytic criteria over subjective (graphical) rotational procedures is discussed. Carroll's criterion and the quartimax criterion are briefly reviewed; the varimax criterion is outlined in detail and contrasted both logically and numerically with the quartimax criterion. It is shown that thenormal varimax solution probably coincides closely to the application of the principle of simple structure. However, it is proposed that the ultimate criterion of a rotational procedure is factorial invariance, not simple structure—although the two notions appear to be highly related. The normal varimax criterion is shown to be a two-dimensional generalization of the classic Spearman case, i.e., it shows perfect factorial invariance for two pure clusters. An example is given of the invariance of a normal varimax solution for more than two factors. The oblique normal varimax criterion is stated. A computational outline for the orthogonal normal varimax is appended.},
	language = {en},
	number = {3},
	urldate = {2024-02-01},
	journal = {Psychometrika},
	author = {Kaiser, Henry F.},
	year = {1958},
	keywords = {Public Policy, Statistical Theory, Analytic Criterion, Factorial Invariance, Simple Structure},
	pages = {187--200},
}

@article{hyvarinen_emergence_2000,
	title = {Emergence of {Phase}- and {Shift}-{Invariant} {Features} by {Decomposition} of {Natural} {Images} into {Independent} {Feature} {Subspaces}},
	volume = {12},
	abstract = {Olshausen and Field (1996) applied the principle of independence maximization by sparse coding to extract features from natural images. This leads to the emergence of oriented linear filters that have simultaneous localization in space and in frequency, thus resembling Gabor functions and simple cell receptive fields. In this article, we show that the same principle of independence maximization can explain the emergence of phase- and shift-invariant features, similar to those found in complex cells. This new kind of emergence is obtained by maximizing the independence between norms of projections on linear subspaces (instead of the independence of simple linear filter outputs). The norms of the projections on such “independent feature subspaces” then indicate the values of invariant features.},
	language = {en},
	number = {7},
	urldate = {2022-12-21},
	journal = {Neural Computation},
	author = {Hyvärinen, Aapo and Hoyer, Patrik},
	year = {2000},
	pages = {1705--1720},
    doi = {10.1162/089976600300015312}
}

@article{hyvarinen_independent_2000,
	title = {Independent component analysis: algorithms and applications},
	volume = {13},
	issn = {0893-6080},
	shorttitle = {Independent component analysis},
	doi = {10.1016/S0893-6080(00)00026-5},
	abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
	language = {en},
	number = {4},
	urldate = {2023-06-09},
	journal = {Neural Networks},
	author = {Hyvärinen, A. and Oja, E.},
	year = {2000},
	keywords = {Factor analysis, Independent component analysis, Blind signal separation, Projection pursuit, Representation, Source separation},
	pages = {411--430},
}

@article{taeb_model_2024,
	title = {Model selection over partially ordered sets},
	volume = {121},
	doi = {10.1073/pnas.2314228121},
	number = {8},
	urldate = {2024-06-05},
	journal = {PNAS},
	author = {Taeb, Armeen and Bühlmann, Peter and Chandrasekaran, Venkat},
	year = {2024},
	pages = {e2314228121},
}

@misc{rabenoro_geometric_2024,
	title = {A geometric framework for asymptotic inference of principal subspaces in {PCA}},
	abstract = {In this article, we develop an asymptotic method for constructing confidence regions for the set of all linear subspaces arising from PCA, from which we derive hypothesis tests on this set. Our method is based on the geometry of Riemannian manifolds with which some sets of linear subspaces are endowed.},
	language = {en},
	urldate = {2024-06-19},
	publisher = {arXiv},
	author = {Rabenoro, Dimbihery and Pennec, Xavier},
	year = {2024},
    doi = {10.48550/arXiv.2209.02025},
	keywords = {Mathematics - Statistics Theory, 62R30, 60F05},
    note = {arXiv:2209.02025}
}


@article{pennec_intrinsic_2006,
	title = {Intrinsic {Statistics} on {Riemannian} {Manifolds}: {Basic} {Tools} for {Geometric} {Measurements}},
	volume = {25},
	issn = {1573-7683},
	shorttitle = {Intrinsic {Statistics} on {Riemannian} {Manifolds}},
	doi = {10.1007/s10851-006-6228-4},
	abstract = {In medical image analysis and high level computer vision, there is an intensive use of geometric features like orientations, lines, and geometric transformations ranging from simple ones (orientations, lines, rigid body or affine transformations, etc.) to very complex ones like curves, surfaces, or general diffeomorphic transformations. The measurement of such geometric primitives is generally noisy in real applications and we need to use statistics either to reduce the uncertainty (estimation), to compare observations, or to test hypotheses. Unfortunately, even simple geometric primitives often belong to manifolds that are not vector spaces. In previous works [1, 2], we investigated invariance requirements to build some statistical tools on transformation groups and homogeneous manifolds that avoids paradoxes. In this paper, we consider finite dimensional manifolds with a Riemannian metric as the basic structure. Based on this metric, we develop the notions of mean value and covariance matrix of a random element, normal law, Mahalanobis distance and χ2 law. We provide a new proof of the characterization of Riemannian centers of mass and an original gradient descent algorithm to efficiently compute them. The notion of Normal law we propose is based on the maximization of the entropy knowing the mean and covariance of the distribution. The resulting family of pdfs spans the whole range from uniform (on compact manifolds) to the point mass distribution. Moreover, we were able to provide tractable approximations (with their limits) for small variances which show that we can effectively implement and work with these definitions.},
	language = {en},
	number = {1},
	urldate = {2023-05-23},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Pennec, Xavier},
	year = {2006},
	keywords = {statistics, Frechet mean, geometry, computing on manifolds, covariance, Riemannian manifolds},
	pages = {127--154},
}

@inproceedings{minka_automatic_2000,
	title = {Automatic {Choice} of {Dimensionality} for {PCA}},
	volume = {13},
	booktitle = {NeurIPS},
	publisher = {MIT Press},
	author = {Minka, Thomas},
	year = {2000},
}

@article{gershkovich_problem_2004,
	title = {Problem of close eigenvalues in the vibration testing of structures},
	volume = {46},
	copyright = {Copyright (c)},
	issn = {1445-8810},
	doi = {10.21914/anziamj.v46i0.982},
	abstract = {We outline our vibration based testing approach towards reconstruction of structural properties and damage detection of large structures. Our emphasis is on developing algorithms for the detection of close or coinciding eigenvalues and their calculation --- a commonly encountered situation that has not been adequately addressed in the modal analysis literature nor in commercial software.},
	language = {en},
	urldate = {2024-06-12},
	journal = {ANZIAM Journal},
	author = {Gershkovich, V. and Haritos, N.},
	year = {2004},
	pages = {C658--C671},
}


@inproceedings{besson_multiplicy_1988,
	title = {On the multiplicy of the eigenvalues of the {Laplacian}},
	isbn = {978-3-540-45930-9},
	doi = {10.1007/BFb0083045},
	abstract = {The recent progress in the study of multiplicities of the eigenvalues of the Laplacian has brought through new ideas, in particular the link between the operator theory on graphs and operator theory on manifolds. This has lead to a complete solution of the original problem in dimension greater than 2. The case of Dimension 2 is still open, but the conjecture mentioned above is an aim to reach.},
	language = {en},
	booktitle = {Geometry and {Analysis} on {Manifolds}},
	publisher = {Springer},
	author = {Besson, Gérard},
	year = {1988},
	keywords = {Banach Manifold, Complete Graph, Multiple Eigenvalue, Riemann Surface, Riemannian Manifold},
	pages = {32--53},
}

@book{lazutkin_kam_1993,
	title = {{KAM} {Theory} and {Semiclassical} {Approximations} to {Eigenfunctions}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-76249-9 978-3-642-76247-5},
	publisher = {Springer},
	author = {Lazutkin, Vladimir F.},
	year = {1993},
	doi = {10.1007/978-3-642-76247-5},
	keywords = {convergence, deduction, derivative, dynamical systems, Eigenvalue, Hamilton-Systeme, hamiltonian system, Hamiltonian systems, KAM Theorie, KAM theory, manifold, measure, operator, schrödinger equation, schrödinger operator},
}

@article{kohonen_emergence_1996,
	title = {Emergence of invariant-feature detectors in the adaptive-subspace self-organizing map},
	volume = {75},
	issn = {1432-0770},
	doi = {10.1007/s004220050295},
	language = {en},
	number = {4},
	journal = {Biological Cybernetics},
	author = {Kohonen, Teuvo},
	year = {1996},
	keywords = {Functional Form, Input Pattern, Sensory Environment, Training Data, Transformation Group},
	pages = {281--291},
}

@inproceedings{watanabe_subspace_1973,
  title={Subspace method of pattern recognition},
  author={Watanabe, Satosi and Pakvasa, Nikhil},
  booktitle={Proc. 1st. IJCPR},
  pages={25--32},
  year={1973}
}

@article{akaike_new_1974,
	title = {A new look at the statistical model identification},
	volume = {19},
	issn = {1558-2523},
	doi = {10.1109/TAC.1974.1100705},
	number = {6},
	journal = {IEEE Transactions on Automatic Control},
	author = {Akaike, H.},
	year = {1974},
	keywords = {Maximum likelihood estimation, Testing, Linear systems, Stochastic processes, Art, Estimation theory, History, Roundoff errors, Sampling methods, Time series analysis},
	pages = {716--723},
}

@article{edelman_geometry_1998,
	title = {The {Geometry} of {Algorithms} with {Orthogonality} {Constraints}},
	volume = {20},
	number = {2},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Edelman, Alan and Arias, Tomás A. and Smith, Steven T.},
	year = {1998},
	pages = {303--353}
}

@inproceedings{bishop_bayesian_1998,
	title = {Bayesian {PCA}},
	volume = {11},
	booktitle = {NeurIPS},
	author = {Bishop, Christopher},
	year = {1998}
}




@book{burnham_model_2004,
	title = {Model {Selection} and {Multimodel} {Inference}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-95364-9},
	language = {en},
	urldate = {2024-07-25},
	publisher = {Springer},
	author = {Burnham, Kenneth P. and Anderson, David R.},
	year = {2004},
	doi = {10.1007/b97636},
	keywords = {information theory, data analysis, Estimator, Inference, Likelihood, Model Selection},
}

@Manual{sinkr,
    title = {sinkr: Collection of functions with emphasis in multivariate data analysis},
    author = {Marc Taylor},
    year = {2022},
    note = {R package version 0.7},
}

@article{hurvich_regression_1989,
  title={Regression and time series model selection in small samples},
  author={Hurvich, Clifford M and Tsai, Chih-Ling},
  journal={Biometrika},
  volume={76},
  number={2},
  pages={297--307},
  year={1989},
  publisher={Oxford University Press},
  doi = {10.1093/biomet/76.2.297}
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	urldate = {2024-07-26},
	booktitle = {CVPR},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@misc{mitchell_cmu_1997,
	title = {{CMU} {Face} {Images}},
	doi = {10.24432/C5JC79},
	urldate = {2024-06-06},
	publisher = {UCI Machine Learning Repository},
	author = {Mitchell, Tom},
	year = {1997},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{blackard_covertype_1998,
	title = {Covertype},
	doi = {10.24432/C50K5N},
	urldate = {2024-07-26},
	publisher = {UCI Machine Learning Repository},
	author = {Blackard, Jock},
	year = {1998},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{alpaydin_optical_1998,
	title = {Optical {Recognition} of {Handwritten} {Digits}},
	doi = {10.24432/C50P49},
	urldate = {2024-01-11},
	publisher = {UCI Machine Learning Repository},
	author = {Alpaydin, E. and Kaynak, C.},
	year = {1998},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{hopkins_spambase_1999,
	title = {Spambase},
	doi = {10.24432/C53G6X},
	urldate = {2024-07-26},
	publisher = {UCI Machine Learning Repository},
	author = {Hopkins, Mark and Reeber, Erik and Forman, George and Suermondt, Jaap},
	year = {1999},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{fisher_iris_1936,
	title = {Iris},
	doi = {10.24432/C56C76},
	urldate = {2024-07-26},
	publisher = {UCI Machine Learning Repository},
	author = {Fisher, R. A.},
	year = {1936},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{german_glass_1987,
	title = {Glass {Identification}},
	doi = {10.24432/C5WW2P},
	publisher = {UCI Machine Learning Repository},
	author = {German, B.},
	year = {1987},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{wolberg_breast_1995,
	title = {Breast {Cancer} {Wisconsin} ({Diagnostic})},
	doi = {10.24432/C5DW2B},
	publisher = {UCI Machine Learning Repository},
	author = {Wolberg, W. H. and Mangasarian, O. L. and Street, W. N.},
	year = {1995},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{aeberhard_wine_1991,
	title = {Wine},
	doi = {10.24432/C5PC7J},
	publisher = {UCI Machine Learning Repository},
	author = {Aeberhard, Stefan and Forina, M.},
	year = {1991},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{sigillito_ionosphere_1989,
	title = {Ionosphere},
	doi = {10.24432/C5W01B},
	publisher = {UCI Machine Learning Repository},
	author = {Sigillito, V. G. and Wing, S. P. and Hutton, L. V. and Baker, K. B.},
	year = {1989},
	note = {\textit{UCI Machine Learning Repository}}
}

@misc{szwagier_parsimonious_2025,
	title = {Parsimonious {Gaussian} mixture models with piecewise-constant eigenvalue profiles},
	doi = {10.48550/arXiv.2507.01542},
	urldate = {2025-07-04},
	publisher = {arXiv},
	author = {Szwagier, Tom and Mattei, Pierre-Alexandre and Bouveyron, Charles and Pennec, Xavier},
	year = {2025},
    note = {arXiv:2507.01542}
}

@misc{ennes_liedetect_2025,
	title = {{LieDetect}: {Detection} of representation orbits of compact {Lie} groups from point clouds},
	shorttitle = {{LieDetect}},
	doi = {10.48550/arXiv.2309.03086},
	urldate = {2025-07-16},
	publisher = {arXiv},
	author = {Ennes, Henrique and Tinarrage, Raphaël},
	year = {2025},
	note = {To appear in Foundations of Computational Mathematics},
}

@article{schwartzman_inference_2008,
	title = {Inference for eigenvalues and eigenvectors of {Gaussian} symmetric matrices},
	volume = {36},
	issn = {0090-5364},
	doi = {10.1214/08-AOS628},
	language = {en},
	number = {6},
	urldate = {2024-10-25},
	journal = {The Annals of Statistics},
	author = {Schwartzman, Armin and Mascarenhas, Walter F. and Taylor, Jonathan E.},
	year = {2008},
}

@article{pourahmadi_covariance_2011,
	title = {Covariance {Estimation}: {The} {GLM} and {Regularization} {Perspectives}},
	volume = {26},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Covariance {Estimation}},
	doi = {10.1214/11-STS358},
	number = {3},
	urldate = {2025-02-24},
	journal = {Statistical Science},
	author = {Pourahmadi, Mohsen},
	year = {2011},
	pages = {369--387},
}


@inproceedings{mankovich_flag_2025,
	title = {A {Flag} {Decomposition} for {Hierarchical} {Datasets}},
	url = {https://openaccess.thecvf.com/content/CVPR2025/html/Mankovich_A_Flag_Decomposition_for_Hierarchical_Datasets_CVPR_2025_paper.html},
	language = {en},
	urldate = {2025-07-16},
	booktitle = {Proceedings of the {Computer} {Vision} and {Pattern} {Recognition} {Conference}},
	author = {Mankovich, Nathan and Santamaria, Ignacio and Camps-Valls, Gustau and Birdal, Tolga},
	year = {2025},
	pages = {18738--18748},
}

@misc{szwagier_eigengap_2025,
	title = {Eigengap {Sparsity} for {Covariance} {Parsimony}},
	doi = {10.48550/arXiv.2504.10110},
	urldate = {2025-04-15},
	publisher = {arXiv},
	author = {Szwagier, Tom and Olikier, Guillaume and Pennec, Xavier},
	year = {2025},
	keywords = {Statistics - Methodology},
	note = {To appear in Geometric Science of Information Conference Proceedings}
}


@misc{szwagier_nested_2025,
	title = {Nested subspace learning with flags},
	doi = {10.48550/arXiv.2502.06022},
	urldate = {2025-02-11},
	publisher = {arXiv},
	author = {Szwagier, Tom and Pennec, Xavier},
	year = {2025},
    note = {arXiv:2502.06022}
}

@article{mairal_online_2010,
	title = {Online {Learning} for {Matrix} {Factorization} and {Sparse} {Coding}},
	volume = {11},
	number = {2},
	journal = {Journal of Machine Learning Research},
	author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
	year = {2010},
	pages = {19--60},
}

@inproceedings{basiri_fusing_2019,
	title = {Fusing {Eigenvalues}},
	doi = {10.1109/ICASSP.2019.8682906},
	urldate = {2025-04-17},
    issn = {2379-190X},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Basiri, Shahab and Ollila, Esa and Drašković, Gordana and Pascal, Frédéric},
	year = {2019},
	pages = {4968--4972},
}

@article{tyler_lassoing_2020,
	title = {Lassoing eigenvalues},
	volume = {107},
	issn = {0006-3444},
	doi = {10.1093/biomet/asz076},
	number = {2},
	urldate = {2025-04-17},
	journal = {Biometrika},
	author = {Tyler, David E and Yi, Mengxi},
	year = {2020},
	pages = {397--414},
}

@article{ledoit_quadratic_2022,
	title = {Quadratic shrinkage for large covariance matrices},
	volume = {28},
	issn = {1350-7265},
	doi = {10.3150/20-BEJ1315},
	number = {3},
	journal = {Bernoulli},
	author = {Ledoit, Olivier and Wolf, Michael},
	year = {2022},
	keywords = {Inverse shrinkage, Kernel estimation, Large-dimensional asymptotics, signal amplitude, Stein shrinkage},
	pages = {1519--1547},
}

@article{bai_consistency_2018,
	title = {Consistency of {AIC} and {BIC} in estimating the number of significant components in high-dimensional principal component analysis},
	volume = {46},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-46/issue-3/Consistency-of-AIC-and-BIC-in-estimating-the-number-of/10.1214/17-AOS1577.full},
	doi = {10.1214/17-AOS1577},
	language = {en},
	number = {3},
	urldate = {2025-06-20},
	journal = {The Annals of Statistics},
	author = {Bai, Zhidong and Choi, Kwok Pui and Fujikoshi, Yasunori},
	year = {2018},
}

@article{kass_bayes_1995,
	title = {Bayes {Factors}},
	volume = {90},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572},
	doi = {10.1080/01621459.1995.10476572},
	language = {en},
	number = {430},
	urldate = {2024-11-19},
	journal = {Journal of the American Statistical Association},
	author = {Kass, Robert E. and Raftery, Adrian E.},
	year = {1995},
	pages = {773--795},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	url = {http://jmlr.org/papers/v12/pedregosa11a.html},
	number = {85},
	urldate = {2024-10-15},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Edouard},
	year = {2011},
	pages = {2825--2830},
}

@article{boya_volumes_2003,
	title = {Volumes of compact manifolds},
	volume = {52},
	issn = {0034-4877},
	doi = {10.1016/S0034-4877(03)80038-1},
	number = {3},
	urldate = {2024-09-17},
	journal = {Reports on Mathematical Physics},
	author = {Boya, Luis J. and Sudarshan, E. C. G. and Tilma, Todd},
	year = {2003},
	keywords = {flag manifolds, Measures on manifolds},
	pages = {401--422},
}
@article{khatri_von_1977,
	title = {The von {Mises}–{Fisher} {Matrix} {Distribution} in {Orientation} {Statistics}},
	volume = {39},
	issn = {1369-7412, 1467-9868},
	doi = {10.1111/j.2517-6161.1977.tb01610.x},
	language = {en},
	number = {1},
	urldate = {2024-10-30},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Khatri, C. G. and Mardia, K. V.},
	year = {1977},
	pages = {95--106},
}

@article{pal_conjugate_2020,
	title = {Conjugate {Priors} and {Posterior} {Inference} for the {Matrix} {Langevin} {Distribution} on the {Stiefel} {Manifold}},
	volume = {15},
	issn = {1936-0975, 1931-6690},
	doi = {10.1214/19-BA1176},
	number = {3},
	urldate = {2025-05-01},
	journal = {Bayesian Analysis},
	author = {Pal, Subhadip and Sengupta, Subhajit and Mitra, Riten and Banerjee, Arunava},
	year = {2020},
	keywords = {Bayesian inference, conjugate prior, hypergeometric function of matrix argument, matrix Langevin distribution, Stiefel manifold, vectorcardiography},
	pages = {871--908},
}

@article{hoff_hierarchical_2009,
	title = {A {Hierarchical} {Eigenmodel} for {Pooled} {Covariance} {Estimation}},
	volume = {71},
	issn = {1369-7412},
	number = {5},
	urldate = {2023-12-04},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Hoff, Peter D.},
	year = {2009},
	pages = {971--992},
}

@article{jupp_maximum_1979,
	title = {Maximum {Likelihood} {Estimators} for the {Matrix} {Von} {Mises}-{Fisher} and {Bingham} {Distributions}},
	volume = {7},
	issn = {0090-5364, 2168-8966},
	doi = {10.1214/aos/1176344681},
	number = {3},
	urldate = {2025-05-01},
	journal = {The Annals of Statistics},
	author = {Jupp, P. E. and Mardia, K. V.},
	year = {1979},
	keywords = {62F05, 62F10, Bingham matrix distribution, exponential family, maximum likelihood estimator, von Mises-Fisher matrix distribution},
	pages = {599--606},
}

@article{james_normal_1954,
	title = {Normal {Multivariate} {Analysis} and the {Orthogonal} {Group}},
	volume = {25},
	issn = {0003-4851, 2168-8990},
	doi = {10.1214/aoms/1177728846},
	number = {1},
	urldate = {2024-10-30},
	journal = {The Annals of Mathematical Statistics},
	author = {James, A. T.},
	year = {1954},
	pages = {40--75},
}

@article{yang_estimation_1994,
	title = {Estimation of a {Covariance} {Matrix} {Using} the {Reference} {Prior}},
	volume = {22},
	issn = {0090-5364, 2168-8966},
	doi = {10.1214/aos/1176325625},
	number = {3},
	urldate = {2025-03-12},
	journal = {The Annals of Statistics},
	author = {Yang, Ruoyong and Berger, James O.},
	year = {1994},
	pages = {1195--1211},
}

@article{ye_schubert_2016,
	title = {Schubert {Varieties} and {Distances} between {Subspaces} of {Different} {Dimensions}},
	volume = {37},
	issn = {0895-4798, 1095-7162},
	doi = {10.1137/15M1054201},
	language = {en},
	number = {3},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Ye, Ke and Lim, Lek-Heng},
	year = {2016},
	pages = {1176--1197},
}