\section{Model selection}\label{sec:MS}
As discussed in Appendix~\ref{subsec:proof_Q}, sample covariance matrices almost surely have distinct eigenvalues. This makes the full covariance model the most likely to have generated some observed data.
However, it does not mean that all the parameters---that are the eigenvectors and the eigenvalues---can be accurately identified, especially in the small-data regime.
Hence, one can wonder if a covariance model with repeated eigenvalues and multidimensional eigenspaces would not be more robust.
The results of the previous section enable to provide a possible answer, through SPCA model selection. 
First, we study the identifiability of two adjacent sample eigenvalues and deduce that one rarely has enough samples to distinguish them. We conclude that when the eigenvalue gap is small and the number of samples is limited, one should rather equalise the eigenvalues and gather the associated eigenvectors in a multidimensional eigenspace.
Second, to extend this result to more than two eigenvalues, we develop a general model selection framework based on the stratified structure of SPCA.. 

\subsection{Bayesian Information Criterion}\label{subsec:BIC}
In this work, we focus on one simple model selection criterion to set up the ideas. The \emph{Bayesian Information Criterion (BIC)} is defined as 
\begin{equation}\label{eq:BIC}
    \operatorname{BIC} := \kappa \ln n - 2 \ln \hat{\mathcal{L}},
\end{equation}
where $\kappa$ is the number of free parameters---computed in  Proposition~\ref{prop:SPCA_nparam}---and $\ln \hat{\mathcal{L}}$ is the maximum log-likelihood \eqref{eq:SPCA_ML}.
By removing the constant variables within model selection (like $p$ and $n$), we get the following proposition.

\begin{proposition}
\label{prop:SPCE_BIC}
The SPCA model minimizing the BIC is
\begin{equation}\label{eq:BIC_simpl}
\hat {\boldsymbol{\gamma}} = \argmin_{\substack{\boldsymbol{\gamma} \in \mathcal{C}(p)}} \lrp{d - \sum_{k=1}^d \frac{\gamma_k (\gamma_k - 1)} {2}} \frac{\ln{n}}{n} + \sum_{k=1}^d \gamma_k \ln{\overline{\boldsymbol{\lambda}^k}}.
\end{equation}
\end{proposition}

\noindent From now on, we remove the shift parameter $\boldsymbol{\mu} \in \R^p$ because it has the same complexity across models, and rather consider SPCA as a covariance model, like done in \cite{tipping_probabilistic_1999}.



\subsection{Eigenvalue equalisation}\label{subsec:exp_averaging}
Willing to better apprehend the dynamics of SPCA model selection, we lead the experiment of quantifying the variation of the BIC induced by the equalisation of two adjacent eigenvalues. More precisely and without loss of generality, we compare the BIC of a \emph{full covariance model} $\boldsymbol{\gamma} = \lrp{1, \dots, 1}$ to the one of an \emph{equalised covariance model} $\boldsymbol{\gamma}' = \lrp{1, \dots, 1, 2, 1, \dots 1}$.


\begin{proposition}
\label{prop:eigenvalue_averaging}
Let $\lrp{x_i}_{i=1}^n$ be a p-dimensional dataset with $n$ samples, $\lambda_j \geq \lambda_{j+1}$ two adjacent sample eigenvalues and $\delta_j := \frac{\lambda_{j} - \lambda_{j+1}}{\lambda_j}$ their \emph{relative eigengap}. 
If
\begin{equation}
    \delta_j < 2 - 2 e^{2 \frac{\ln n} n} + 2\sqrt{e^{4 \frac{\ln n} n} - e^{2 \frac{\ln n} n}} := \delta(n),
\end{equation} 
then the equalised covariance model has a lower BIC than the full one.
\end{proposition}
\begin{proof} --
The proof is given in Appendix~\ref{subsec:appendix_eigval_averaging}.
\end{proof}

\noindent A few values of the \emph{threshold function} $\delta(n)$ are reported in Figure~\ref{fig:threshold_function}.
% Figure environment removed
\noindent The table can be read in the following way: if a pair of sample eigenvalues
has a relative eigengap lower than $21\%$, then we need at least $1000$ data points to statistically distinguish them. This is an important result, as many real datasets do not fulfill this condition, as we will see in the next section.

As far as we know, this is the first time that a study on the parsimony induced by the equalisation of two adjacent sample eigenvalues is performed. This is enabled by the very design of SPCA and the geometric interpretation of its parameter space, involving flag manifolds.
We could extend this study to the equalisation of more than two eigenvalues, but it would not necessarily yield a condition as simple as the one of Proposition~\ref{prop:eigenvalue_averaging}. Hence, in the following, we establish a general framework for SPCA model selection. We study the structure of the family of models and design efficient model selection heuristics.



\subsection{Structure of the Stratified PCA family}
Given a dimension $p$, PPCA has $p$ models, ranging from the isotropic Gaussian ($q=0$) to the full covariance model ($q=p-1$). We can naturally equip the set of PPCA models with the  \emph{less-than-or-equal} relation $\leq$ on the latent variable dimension $q$, which makes it a totally ordered set. The complexity of the model then increases with $q$ (cf. Subsection~\ref{subsec:SPCA_param}).
The characterization of the SPCA family structure is a bit more technical, as it requires to study the hierarchy of types, involving the concept of integer composition. Fortunately, the structure of such sets has already been well studied in combinatorics \citep{bergeron_standard_1995}. 
Moreover, several works have shown and exploited the stratification of symmetric matrices by eigenvalue multiplicity \citep{arnold_remarks_1995,groisser_geometric_2017, breiding_geometry_2018}. Hence, without proof, we can state the following result.

\begin{proposition}\label{prop:pos}
The family of $p$-dimensional SPCA models induces a stratification of the space of full-rank $p \times p$ covariance matrices by eigenvalue multiplicity. The refinement relation $\preceq$ \eqref{subsec:type} makes it a partially ordered set of cardinal $2^{p-1}$.
% And is a stratified space?
\end{proposition}

\noindent Hence the set of SPCA models at a given data dimension can be represented using a Hasse diagram, as done in Figure~\ref{fig:hasse_complexity}.
% Figure environment removed
We can see that SPCA contains PPCA, IPPCA, and many new models. 
SPCA therefore has the advantage of possibly providing more adapted models than PPCA and IPPCA, but also the drawback of requiring more comparisons for model selection. 
In high dimension this becomes quickly computationally heavy, so we need to define heuristics for selecting only a few number of models to compare. The previously derived partial order $\preceq$ on the set of SPCA models allows simple non-greedy heuristics for model selection.


\subsection{Heuristics}\label{subsec:heuristic}
In this subsection, we 
develop two simple heuristics for model selection. Their common idea is to a priori choose a subfamily of candidate models based on the shape of the eigenvalue profile, and then restrict the model selection process to this smaller subset.

\subsubsection{Hierarchical clustering of eigenvalues}\label{subsubsec:hierarchical_heuristic}
In this heuristic, the subset of candidate models is generated by the \emph{hierarchical clustering} \citep{ward_hierarchical_1963} of the sample eigenvalues. The general principle of hierarchical clustering is to agglomerate one by one the eigenvalues into clusters, thanks to a so-called \emph{cluster-linkage criterion}, which is a measure of dissimilarity between clusters.
Here, given two clusters of sample eigenvalues $A$, $B$ and any continuous distance $\Delta$ (such as the relative eigengap defined in Proposition~\ref{prop:eigenvalue_averaging}), we take as a cluster-linkage criterion the distance between the average eigenvalue in each cluster, $\Delta\lrp{\overline A, \overline B}$. % / centroid
The method is detailed in Algorithm~\ref{alg:hierarchical} and illustrated in Figure~\ref{fig:hierarchical_clustering}.
The hierarchical clustering heuristic creates a \emph{trajectory} in the Hasse diagram of SPCA types $\lrp{\boldsymbol{\gamma}^t}_{t=1}^p$. The sequence starts from $\boldsymbol{\gamma}^1 = \lrp{1, \dots, 1}$, the full covariance model, in which each eigenvalue is in its own cluster. Then, one by one, the eigenvalues that are the closest in terms of distance $\Delta$ are agglomerated, and the inter-cluster distances are updated. The algorithm ends when we reach the isotropic covariance model, $\boldsymbol{\gamma}^p = \lrp{p}$, in which all the eigenvalues are in the same cluster.
\begin{algorithm}
\caption{Hierarchical clustering heuristic for SPCA model selection}\label{alg:hierarchical}
\begin{algorithmic}
\Require $\lambda_1 \geq \dots \geq \lambda_p, \Delta$ \Comment{sample eigenvalues and distance}
\Ensure $\lrp{\boldsymbol{\gamma}^{t}}_{t=1}^p$ \Comment{subfamily of SPCA models}
\State $\boldsymbol{\gamma}^{1} \Leftarrow \lrp{1, \dots, 1}, \quad {\boldsymbol{\lambda}}^{1} \Leftarrow {\boldsymbol{\lambda}} := \lrp{\lambda_1, \dots, \lambda_p}$ \Comment{full covariance model}
\For{$t = 1 \twodots p-1$}
    \State $\Delta^{t} \Leftarrow \Delta\lrp{\lambda^{t}_k, \lambda^{t}_{k+1}}_{k=1}^{p-t}$ \Comment{distance between adjacent clusters}
    \State $k^{t} = \argmin \Delta^{t}$ \Comment{minimal distance}
    \State $\boldsymbol{\gamma}^{t+1} = \lrp{\gamma^t_1, \dots, \gamma^t_{{k^t}-1}, \gamma^t_{k^t} + \gamma^t_{{k^t}+1}, \gamma^t_{{k^t}+2}, \dots, \gamma_d}$ \Comment{type agglomeration}
    \State ${\boldsymbol{\lambda}}^{t+1} = \overline{{\boldsymbol{\lambda}}^{\boldsymbol{\gamma}^{t+1}}}$ \Comment{$\boldsymbol{\gamma}$-averaging}
\EndFor
\end{algorithmic}
\end{algorithm}
% Figure environment removed


The hierarchical clustering heuristic hence generates a subfamily of $p$ models that can be then compared within a classical model selection framework. In order to assess the quality of such a heuristic, we show the following consistency result.

\begin{proposition}\label{prop:hierarchical_heuristic}
If the true generative model belongs to SPCA, then the hierarchical clustering heuristic~\eqref{subsubsec:hierarchical_heuristic} will asymptotically consistently select it.
\end{proposition}
\begin{proof} --
The proof is given in Appendix~\ref{subsec:appendix_heuristic_hierarchical}.
\end{proof}


Hence, the hierarchical clustering heuristic generates a hierarchical family of models of different complexities, and provided enough data, the true model will be included. Using asymptotic model selection criteria making a tradeoff between goodness-of-fit and complexity like the BIC will then allow to select the true model. %%XP Now we will 
We now propose a second heuristic that is not hierarchical but instead makes a prior assumption on the model complexity and then selects the one that has the maximum likelihood among all the candidates.

\subsubsection{Prior on the length of the type}\label{subsubsec:type_length_heuristic}
In this heuristic, we perform model selection at a given floor of the Hasse diagram (cf. Figure~\ref{fig:hasse_complexity}). More precisely, we consider for selection only the models that have a given type length $d$, like done in IPPCA with $d=2$.
Similarly as for the hierarchical clustering heuristic, the type-length prior heuristic drastically reduces the search space, this time to $\binom{p-1}{d-1}$ models.

Similarly as in the hierarchical clustering heuristic \eqref{subsubsec:hierarchical_heuristic}, we could then use the BIC to choose the best model among this reduced family. 
We provide an additional criterion that is nothing but the maximum likelihood itself. 
We indeed manage to extend to SPCA the surprising result from \cite{bouveyron_intrinsic_2011} that the maximum likelihood criterion alone asymptotically consistently finds the true intrinsic dimension within the IPPCA setting. 
As this criterion empirically yields competitive results with respect to other classical model selection criteria in the large sample, low signal-to-noise ratio regime, we expect it to be of interest in SPCA as well.
\begin{proposition}\label{prop:fixed_length_heuristic}
If the true generative model belongs to SPCA, then the maximum likelihood criterion alone will asymptotically consistently select it within the type-length prior heuristic~\eqref{subsubsec:type_length_heuristic}.
\end{proposition}
\begin{proof} --
The proof is given in Appendix~\ref{subsec:appendix_heuristic_length}. We emphasize the use of Jensen's inequality, which elegantly generalizes the proof of \cite{bouveyron_intrinsic_2011}.
\end{proof}

Hence we derived two simple heuristics for model selection, taking into account the structure of the SPCA models family.
We now have all the tools needed for inference and model selection using SPCA.