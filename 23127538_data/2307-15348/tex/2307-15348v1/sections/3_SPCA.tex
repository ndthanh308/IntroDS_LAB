\section{Stratified Principal Component Analysis} \label{sec:SPCA}
Inspired by the complexity drop induced by the isotropy in the noise space in PPCA, we aim at investigating more general isotropy constraints on the full data space.
In this section, we introduce \emph{Stratified PCA (SPCA)}, a covariance model with a general constraint on the sequence of eigenvalue multiplicities. SPCA generalizes PPCA and IPPCA and unifies them in a new family of models parameterized with flag manifolds \citep{monk_geometry_1959}. Flag manifolds are themselves generalizations of Stiefel manifolds and Grassmannians \citep{edelman_geometry_1998}, hence the link between PPCA, IPPCA and SPCA that is detailed in this section.


\subsection{Model}\label{subsec:SPCA_model}
We recall that in combinatorics, a \emph{composition} of an integer $p$ is an ordered sequence of positive integers that sums up to $p$. 
It has to be distinguished from
a \emph{partition} of an integer, which doesn't take into account the ordering of the parts. 

Let $\boldsymbol{\gamma} := (\gamma_1, \gamma_2, \dots, \gamma_d) \in \mathcal{C}(p)$ be a composition of a positive integer $p$.
We define the \emph{Stratified PCA} model of \emph{type} $\boldsymbol{\gamma}$, noted $\boldsymbol{\gamma}$-SPCA as
\begin{equation}\label{eq:SPCA_model}
\boldsymbol{x} = \sum_{k=1}^{d-1} \sigma_k \mathit{U}_k \boldsymbol{z}_k + \boldsymbol{\mu} + \boldsymbol{\epsilon}.
\end{equation}
In this formula, $~{\sigma_1 > \dots > \sigma_{d-1} > 0}$ are decreasing scaling factors,
$~{\mathit{U}_k \in \St{p, \gamma_k}}$ are mutually orthogonal frames (belonging to Stiefel manifolds) and
$~{\boldsymbol{z}_k \sim \N{0_{\gamma_k}, \mathit{I}_{\gamma_k}}}$ are independent latent variables.
$\boldsymbol{\mu} \in \R^p$, $\sigma^2 > 0$ and $~{\boldsymbol{\epsilon} \sim \N{0_{p}, \sigma^2 \mathit{I}_{p}}}$ are the classical shift and isotropic noise present in PPCA. An illustration is provided in Figure~\ref{fig:SPCA}.
% Figure environment removed


Similarly as for PPCA, we can compute the population density
\begin{equation}
    \boldsymbol{x} \sim \N{\boldsymbol{\mu}, \hspace{1mm} \sum_{k=1}^{d-1} \sigma_k^2 \mathit{U}_k \mathit{U}_k\T + \sigma^2 \mathit{I}_p}.
\end{equation}
The expression of the covariance matrix $~{\mathit{\Sigma} := \sum_k \sigma_k^2 \mathit{U}_k \mathit{U}_k\T + \sigma^2 \mathit{I}_p \in \R^{p\times p}}$ can be simplified by gathering all the orthonormal frames into one orthogonal matrix $~{\mathit{Q} := \lrb{\mathit{U}_1 | \dots | \mathit{U}_{d-1} | \mathit{U}_d} \in \O(p)}$ where $\mathit{U}_d \in \St{p, \gamma_d}$ is an orthogonal completion of the previous frames. 
Writing $\mathit{L} := \diag{\ell_1 \mathit{I}_{\gamma_1}, \dots, \ell_d \mathit{I}_{\gamma_{d}}}$, with 
$\ell_k := \sigma_k^2 + \sigma^2$ for $k\in \lrb{1 \twodots d-1}$ and $\ell_d := \sigma^2$, one gets
\begin{equation}\label{eq:cov_simpl}
    \mathit{\Sigma} = \mathit{Q} \mathit{L} \mathit{Q}\T.
\end{equation}
Hence,
the fitted density 
of $\boldsymbol{\gamma}$-SPCA
is a multivariate Gaussian with repeated eigenvalues $~{\ell_1 > \dots > \ell_d > 0}$ of respective multiplicity $~{\gamma_1, \dots, \gamma_d}$. Therefore, PPCA and IPPCA can be seen as SPCA models, with respective types $~{\boldsymbol{\gamma} = (1, \dots, 1, p - q)}$ and $\boldsymbol{\gamma} = (q, p - q)$.
From a geometric point of view, the fitted density is isotropic on 
the  eigenspaces of $\mathit{\Sigma}$, which constitute 
a sequence of mutually orthogonal subspaces of respective dimension $\gamma_1, \dots, \gamma_d$, whose direct sum generates the data space.
Such a sequence is called a \emph{flag} of linear subspaces of \emph{type} $\boldsymbol{\gamma}$ \citep{monk_geometry_1959}.
Hence flags are natural objects to geometrically interprete SPCA, and so a fortiori PPCA and IPPCA. We detail this point later in Subsection~\ref{subsec:SPCA_param}.



\subsection{Type}\label{subsec:type}
Just like the latent variable dimension $q \in \lrb{1 \twodots p-1}$ is a central notion in PPCA, the type $\boldsymbol{\gamma} \in \mathcal{C}(p)$ is a central notion in SPCA. In this subsection, we introduce the concepts of \emph{refinement} and \emph{$\boldsymbol{\gamma}$-composition} to make its analysis more convenient.

Let $\boldsymbol{\gamma} := (\gamma_1, \gamma_2, \dots, \gamma_d) \in \mathcal{C}(p)$. 
We say that $\boldsymbol{\gamma}' \in \mathcal{C}(p)$ is a \emph{refinement} of $\boldsymbol{\gamma}$, and note $\boldsymbol{\gamma} \preceq \boldsymbol{\gamma}'$, if we can write $\boldsymbol{\gamma}' := (\boldsymbol{\gamma}'_1, \boldsymbol{\gamma}'_2, \dots, \boldsymbol{\gamma}'_d)$, with $\boldsymbol{\gamma}'_k \in \mathcal{C}(\gamma_k), \forall k \in \lrb{1\twodots d}$. For instance, one has $(2, 3) \preceq (1, 1, 2, 1)$.

Let $\boldsymbol{\gamma} := (\gamma_1, \gamma_2, \dots, \gamma_d) \in \mathcal{C}(p)$. 
Then each integer between $1$ and $p$ can be uniquely assigned a \emph{part} of the composition, indexed between $1$ and $d$. 
We define the \emph{$\gamma$-composition function} $\phi_{\boldsymbol{\gamma}}\colon \lrb{1, p} \to \lrb{1, d}$ to be this surjective map, such that $\phi_{\boldsymbol{\gamma}}(j)$ is the index $k$ of the part the integer $j$ belongs to. 
For instance, one has $~{\phi_{(2, 3)}(1) = \phi_{(2, 3)}(2) = 1}$ and $~{\phi_{(2, 3)}(3) =  \phi_{(2, 3)}(4) = \phi_{(2, 3)}(5) = 2}$.
Then, intuitively and with slight abuse of notation, each object of size $p$ can be partitioned into $d$ sub-objects of respective size $\gamma_k$, for $k \in \lrb{1\twodots d}$. 
We will call it the \emph{$\boldsymbol{\gamma}$-composition} of an object. We give two examples. 
Let $\mathit{Q} \in \O(p)$. The {$\boldsymbol{\gamma}$-composition} of $\mathit{Q}$ is the sequence $\mathit{Q}^{\boldsymbol{\gamma}} := \lrp{\mathit{Q}_1, \dots, \mathit{Q}_d}$ such that $~{\mathit{Q}_k \in \R^{p\times\gamma_k}, \forall k \in \lrb{1\twodots d}}$ and $~{\mathit{Q} = \lrb{\mathit{Q}_1 | \dots | \mathit{Q}_d}}$.
Let $\boldsymbol{\lambda} := \lrp{\lambda_1, \dots, \lambda_p}$ be a sequence of decreasing eigenvalues. The {$\boldsymbol{\gamma}$-composition} of $\boldsymbol{\lambda}$ is the sequence  ${\boldsymbol{\lambda}}^{\boldsymbol{\gamma}} := \lrp{{\boldsymbol{\lambda}}^1, \dots, {\boldsymbol{\lambda}}^d}$ such that $~{{\boldsymbol{\lambda}}^k \in \R^{\gamma_k}}, ~{\forall k \in \lrb{1\twodots d}}$ and $\boldsymbol{\lambda} = \lrb{{\boldsymbol{\lambda}}^1 |  \dots | {\boldsymbol{\lambda}}^d}$. 
We will call \emph{$\boldsymbol{\gamma}$-averaging} of $\boldsymbol{\lambda}$ the sequence $\overline{{\boldsymbol{\lambda}}^{\boldsymbol{\gamma}}} := \lrp{\overline{{\boldsymbol{\lambda}}^1},  \dots, \overline{{\boldsymbol{\lambda}}^d}}\in \R^d$ of average eigenvalues in ${\boldsymbol{\lambda}}^{\boldsymbol{\gamma}}$.


\subsection{Maximum Likelihood}\label{subsec:SPCA_ML}
Similarly as for PPCA, the log-likelihood of the model can be easily computed
\begin{equation}\label{eq:SPCA_LL}
\ln {\mathit{\mathcal{L}}} \lrp{\boldsymbol{\mu}, \mathit{\Sigma}} = -\frac n 2 \lrp{p \ln(2\pi) + \ln |\mathit{\Sigma}| + \tr{\mathit{\Sigma}^{-1} {\mathit{C}}}},
\end{equation}
with
${\mathit{C}} = \frac 1 n \sum\limits_{i=1}^n (\boldsymbol{x}_i - \boldsymbol{\mu}) (\boldsymbol{x}_i - \boldsymbol{\mu})\T$.
We will now show that the maximum likelihood estimate for $\boldsymbol{\gamma}$-SPCA consists in the eigenvalue decomposition of the sample covariance matrix followed by a block-averaging of adjacent eigenvalues such that the imposed type $\boldsymbol{\gamma}$ is respected; in other words, a $\boldsymbol{\gamma}$-averaging of the eigenvalues.
Just before, we naturally extend the notion of \emph{type} to symmetric matrices, as the sequence of geometric multiplicities of its ordered-descending-eigenvalues.
\begin{theorem}
\label{thm:SPCA}
Let $\lrp{x_i}_{i=1}^n$ be a p-dimensional dataset, $\overline {\boldsymbol{x}} := \frac 1 n \sum_{i=1}^n x_i$ its mean and $~{\mathit{S} := \sum_{j=1}^p \lambda_j \boldsymbol{v}_j \boldsymbol{v}_j\T}$ its sample covariance matrix, with $~{\lambda_1 \geq \dots \geq \lambda_p \geq 0}$ its eigenvalues and $\lrb{\boldsymbol{v}_1 | \dots | \boldsymbol{v}_p} := \mathit{V} \in \O(p)$ some associated eigenvectors. 

\noindent The maximum likelihood parameters of $\gamma$-SPCA are
\begin{equation}
    \hat {\boldsymbol{\mu}} = \overline {\boldsymbol{x}}, \hspace{15mm}
    \hat {\mathit{Q}} = {\mathit{V}}, \hspace{15mm}
    \lrp{\hat{\ell}_1, \dots, \hat{\ell}_d} = \overline{{\boldsymbol{\lambda}}^{\boldsymbol{\gamma}}}.
\end{equation}

\noindent The parameters $\hat {\boldsymbol{\mu}}$ and $\hat{\ell}_1, \dots, \hat{\ell}_d$ are unique. $\hat {\mathit{Q}}$ is not unique but the flag of linear subspaces generated by its $\boldsymbol{\gamma}$-composition almost surely is---more precisely, the flag is unique if and only if the type of $S$ is a refinement of $\gamma$, which is almost sure.
\end{theorem}

\begin{proof} --
The proof is given in Appendix~\ref{sec:appendix_sPCA}. It relies on optimization and linear algebra. We emphasize that the almost-sure uniqueness of the solution comes from the null Lebesgue measure of the set of symmetric matrices with repeated eigenvalues. 
\end{proof}



\noindent One can then easily express the maximum log-likelihood of $\boldsymbol{\gamma}$-SPCA:
\begin{equation}\label{eq:SPCA_ML}
    \ln \hat{\mathcal{L}} = -\frac n 2 \lrp{p \ln(2\pi) + \sum_{k=1}^d \gamma_k \ln{\overline{\boldsymbol{\lambda}^k}} + p}.
\end{equation}




\subsection{Geometric interpretation with flag manifolds}
\label{subsec:SPCA_param}
As intuited in Subsection~\ref{subsec:SPCA_model} and then proven in Theorem~\ref{thm:SPCA}, 
the accurate parameter space for $\mathit{Q}$ in $\boldsymbol{\gamma}$-SPCA is the space of flags of type $\boldsymbol{\gamma}$, noted $\operatorname{Flag}(\boldsymbol{\gamma})$.
The geometry of such a set is well known \citep{monk_geometry_1959}. $\operatorname{Flag}(\boldsymbol{\gamma})$ is a smooth quotient manifold, consisting in equivalence classes of orthogonal matrices
\begin{equation}
    \operatorname{Flag}(\boldsymbol{\gamma}) \cong \O(p) / \lrp{\O(\gamma_1) \times \dots \times \O(\gamma_d)}.
\end{equation}
This result enables the accurate computation of the number of parameters in SPCA.
Let us just before note that the other parameters are $\boldsymbol{\mu} \in \R^p$ and $~{\mathit{L} \in \operatorname{D}(\boldsymbol{\gamma}) := \lrs{\diag{\ell_1 \mathit{I}_{\gamma_1}, \dots, \ell_d \mathit{I}_{\gamma_d}} \in \R^{p \times p} \colon \ell_1 > \dots > \ell_d > 0}}$, which can be seen as a convex cone of $\R^d$.
\begin{proposition}
\label{prop:SPCA_nparam}
The number of free parameters in $\boldsymbol{\gamma}$-SPCA is
\begin{equation}
    \kappa := p + d + \frac{p(p-1)}{2} - \sum_{k=1}^{d} \frac {\gamma_k (\gamma_k - 1)} {2}.
\end{equation}
\end{proposition}

This geometric interpretation sheds light on PPCA, which---we remind---is a special case of SPCA with $\boldsymbol{\gamma} = \lrp{1, \dots, 1, p-q}$. First, as flags of type $(1, \dots, 1, p-q)$ are nothing but Stiefel manifolds (up to changes of signs), we can naturally parameterize PPCA models with those spaces, which is already commonly done in the literature. Second, we can now see PPCA as removing $~{(p - q - 1) + \frac{(p-q)(p-q-1)}{2}}$ parameters with respect to the full covariance model by imposing an isotropy constraint on the noise space. SPCA then goes beyond the noise space and results in even more parsimonious models.

We can extend this analysis to the IPPCA model, which---we remind---is a special case of SPCA with $\boldsymbol{\gamma} = \lrp{q, p-q}$. Hence we can parameterize it with flags of type $\lrp{q, p-q}$, which are nothing but Grassmannians.