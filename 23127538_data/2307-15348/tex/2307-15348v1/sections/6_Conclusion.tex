\section{Discussion}\label{sec:Ccl}
We introduced in this paper a generative covariance model with repeated eigenvalues called \emph{Stratified PCA (SPCA)}, which generalizes \emph{Probabilistic PCA (PPCA)} \citep{tipping_probabilistic_1999} and \emph{Isotropic PPCA (IPPCA)} \citep{bouveyron_intrinsic_2011}.
The geometric interpretation of its parameter space shed light on the parsimony of PPCA and raised the natural question of extending its eigenvalue-equalisation principle to the signal space.
We indeed argued that assuming all the eigenvalues and eigenvectors in the signal space to be identifiable is not justified in many settings. Hence, SPCA could circumvent this issue by equalising the adjacent eigenvalues with small gaps and gathering the associated eigenvectors into a multidimensional eigenspace.
We confirmed our expectations on synthetic and real datasets, showing how SPCA models achieve a better complexity/goodness-of-fit tradeoff than PPCA.



SPCA is at an early stage of research and its development has been requiring several limiting choices that could be relaxed and improved in future works.
A first limit is the choice of the BIC for model selection. Indeed, the BIC is known to favor under-parameterized models and not work very well in the small-data regime. However, this does not prevent it from being widely used due to its simplicity. Therefore, it provides an elementary way to highlight the interest of SPCA, similarly as \cite{tipping_probabilistic_1999} used a simple model selection criterion when introducing PPCA. One could later investigate extensions of \cite{minka_automatic_2000} and \cite{drton_bayesian_2017} to SPCA models.
A second limit is the linear-Gaussian nature of SPCA which is not suited to real data. Some nonlinear and non-Gaussian extensions could therefore be considered in the future. The probable lack of analytic solution would involve optimization on flag manifolds \citep{ye_optimization_2021}. Due to the cost of inference for each model, we might need to replace discrete model selection with a global optimization scheme on the space of all SPCA models. The latter being stratified by eigenvalue multiplicities, we could benefit from recent works on stratified optimization \citep{leygonie_gradient_2023, olikier_first-order_2023}.



SPCA also comes with several exciting perspectives.
First, it unleashes a whole new family of parsimonious linear-Gaussian models interpolating between the isotropic model and the full covariance one. Hence when a PPCA model overfits and the associated IPPCA model underfits, the perfect model might lie in the SPCA family.
Second, the multidimensional eigenspaces obtained by gathering eigenvectors associated to distinct sample eigenvalues could provide robust, invariant and interpretable feature subspaces \citep{hyvarinen_emergence_2000}. Indeed, just like the first eigenvectors can be interpreted as modes of variation \citep{castro_principal_1986}, the eigenspaces inferred from SPCA could be interpreted as multidimensional attributes, and the norms of projection onto them as their level of expressiveness.
Third, SPCA brings a statistical framework to the flag-based multiscale modeling of datasets. Indeed, several works use flags to represent datasets, be it in an independent \citep{nishimori_riemannian_2006} or principal \citep{ma_flag_2021} component analysis context, enriching the already well developed literature on Grassmannians and Stiefel manifolds for dimension reduction \citep{edelman_geometry_1998}.
In this paper, by introducing a generative model whose maximum likelihood estimate coincides with the minimizer of the \emph{accumulated unexplained variance} criterion \citep{pennec_barycentric_2018}, we enrich the previous works and enable for instance to perform flag-type selection.