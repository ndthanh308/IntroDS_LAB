\section{Experiments}\label{sec:Exp}
As seen in the previous sections, given a dataset and its sample covariance matrix, SPCA equalises the eigenvalues and gives rise to new multidimensional eigenspaces. This causes an additional drop of complexity with respect to PPCA which, according to %Proposition~\ref{prop:eigenvalue_averaging} and
Figure~\ref{fig:threshold_function}, seems justified when the eigenvalue gaps are small in view of the number of available samples. 
%and the goodness-of-fit of Isotropic PPCA.
In this section, we confirm experimentally this hypothesis on some synthetic and real datasets.


\subsection{Simpler models for all sample size}
A key result in the previous section is that we rarely have enough available samples to confidently assert that two adjacent sample eigenvalues are distinct.
Consequently, PPCA models could be made more parsimonious by equalising the adjacent sample eigenvalues with small gaps in the signal space as well.

Willing to better understand how this result applies in practice, we make the following SPCA model selection experiment.
We consider a given multivariate Gaussian population density, with covariance matrix eigenvalues $\lrp{10, 9, 7, 4, 0.5}$, and sample $n \in \lrb{20, 50000}$ data points from it. We fit all the SPCA models to this data distribution and select the one with the lowest BIC. The experiment is repeated several times independently  for each $n$, and the results are reported on Figure~\ref{fig:BIC_traj}, where we plot only a few models among the $16$ for readability.
% Figure environment removed
First, on the BIC plots, we can see that for $n \leq 6000$, the best models do not belong to PPCA. This shows that even for a very large number of samples with respect to the dimension, distinguishing the first eigenvalues is not justified.
Second, on the complexity plots, we can see that PPCA mostly selects the full covariance model for any sample size, while SPCA finds less complex models along the whole trajectory.
Moreover, interestingly, we note the consistent increase of model complexity with the number of samples. As the sample size increases, SPCA can more confidently distinguish the sample eigenvalues.
Third, on the Hasse diagram, we can see that SPCA follows a trajectory, going up with the number of available samples, which recalls the kind of subfamily generated by the hierarchical clustering heuristic (cf. Figure~\ref{fig:hierarchical_clustering}).
To conclude, we see on this synthetic example that SPCA achieves a better complexity/goodness-of-fit tradeoff than PPCA in a wide range of sample sizes by equalising the highest eigenvalues.


\subsection{Parsimony on real data}
As the previous experiment was synthetic, we naturally wonder whether the same conclusions can be made out of real data.
Indeed, as real datasets follow rather non-linear and multimodal distributions, the application of a simple linear-Gaussian model like SPCA to real datasets seems limited. 
However, PPCA has the same limits and remains quite used as a simple representation.

In this experiment, we compare PPCA to SPCA on several classical real datasets extracted from the open source \href{https://archive.ics.uci.edu}{UCI Machine Learning Repository}: \textit{Glass Identification}, \textit{Ionosphere}, \textit{Wine} and \textit{Breast Cancer Wisconsin (WDBC)}.
Due to the high dimensionality of some datasets, we cannot apply greedy SPCA model selection techniqes, therefore we use the hierarchical clustering heuristic introduced in Subsection~\ref{subsec:heuristic}. As those datasets are made for classification problems, we keep only one class in order to make the data distribution more unimodal. For each dataset, we compare the best SPCA model to the best PPCA model (in terms of BIC). The results are reported in Table~\ref{tab:real_life_BIC}.
\begin{table}
\centering
\caption{Comparison of PPCA and SPCA best models on several real datasets. We can see that for any dataset, SPCA finds new models that have a lower BIC. For instance, on the Wine dataset, PPCA finds a principal subspace of dimension $3$ with distinct eigenvalues, while SPCA finds a principal subspace of dimension $8$ with isotropic variability. %no leading mode of variation. 
To shrink long types, we use the power notation to indicate repetition of elements; for instance $(1, 1, 1, 2, 2, 3) := (1^3, 2^2, 3)$.}
\label{tab:real_life_BIC}
\begin{tabular}{|c c c | l  c | l  c|}
    \hline
    \multicolumn{3}{|c}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{PPCA}} & \multicolumn{2}{c|}{\textbf{SPCA}}\\
    Name & $n$ & $p$ & $\boldsymbol{\gamma}$ & $\BIC$ & $\boldsymbol{\gamma}$ & $\BIC$\\ 
    \hline\hline
    Glass & $17$ & $9$ & $(1^9)$ & $-16.77$ & $(1, 2, 3, 1^3)$ & $-17.49$\\
    Ion & $224$ & $32$ & $(1^{30}, 2)$ & $-26.59$ & $(1^5, 2, 13, 6, 4, 2)$ & $-28.50$\\
    Wine & $48$ & $13$ & $(1^3, 10)$ & $+36.35$ & $(8, 5)$ & $+35.57$\\
    WDBC & $357$ & $30$ & $(1^{30})$ & $+25.12$ & $(2, 1, 2, 1, 2, 5, 1, 2, 1, 3^2, 4, 1^3)$ & $+24.72$\\
    \hline
\end{tabular}
\end{table}
We see that SPCA achieves again a better complexity/goodness-of-fit tradeoff than PPCA by equalising some eigenvalues with small gaps. 
For conciseness, we do not report the sample eigenvalue profiles of those datasets, but we can check that none of them satisfies the relative eigengap condition of Proposition~\ref{prop:eigenvalue_averaging}. Hence, the use of SPCA to model real datasets is justified.

In addition to the previous experiment, we also perform a floor-by-floor model comparison on the Glass dataset. More precisely, for each type length, we compare the unique associated PPCA model to the best SPCA one using the type-length prior heuristic introduced in Subsection~\ref{subsec:heuristic}. The results are reported on Table~\ref{tab:glass_BIC}.
\begin{table}
\centering
\caption{Comparison  of  PPCA  and  SPCA best models in the fixed-type-length setting \eqref{subsubsec:type_length_heuristic} on the Glass dataset.
For a given $d \in \lrb{1 \twodots p}$, PPCA contains only one model ($q = d-1$), while SPCA contains $\binom{p-1}{d-1}$, which increases the modelling power. We can see for instance that the SPCA model of type $(8, 1)$ outperforms the PPCA model of type $(1, 8)$ on this dataset, therefore assuming a principal subspace of dimension $8$ with isotropic variability is more likely than assuming a principal subspace of dimension $1$.}
\label{tab:glass_BIC}
\begin{tabular}{|c  c | c  c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{PPCA}}& \multicolumn{2}{c|}{\textbf{SPCA}}\\
    $\boldsymbol{\gamma}$ & $\BIC$ & $\BIC$ & $\boldsymbol{\gamma}$ \\ 
    \hline\hline
    $(9,)$ & $+4.20$ & $+4.20$ & $(9,)$ \\
    $(1, 8)$ & $-0.78$ & $-8.21$ & $(8, 1)$\\
    $(1, 1, 7)$ & $-3.45$ & $-15.92$ & $(3, 5, 1)$\\
    $(1, 1, 1, 6)$ & $-5.97$ & $-16.93$ & $(3, 3, 2, 1)$\\
    $(1, 1, 1, 1, 5)$ & $-6.36$ & $-17.38$ & $(1, 2, 3, 2, 1)$\\
    $(1, 1, 1, 1, 1, 4)$ & $-6.55$ & $-17.49$ & ${(1, 2, 3, 1, 1, 1)}$\\
    $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
    ${(1, \dots \dots \dots, 1)}$ & ${-16.77}$ & $-16.77$ & $(1, \dots \dots \dots, 1)$ \\
    \hline
\end{tabular}
\end{table}
We can see that the rich family of SPCA models with a prespecified number of distinct eigenvalues  $d \in \lrb{1 \twodots p}$, which is of cardinal  $\binom{p-1}{d-1}$, importantly increases the modelling power of PPCA, which only contains one model for each $d$.