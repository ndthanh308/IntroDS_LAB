\section{Introduction}
\emph{Principal Component Analysis (PCA)} \citep{pearson_lines_1901} is a well-known dimension reduction method that is based on the eigenvalue decomposition of the sample covariance matrix.
Usually, after the decomposition, one plots the eigenvalue profile in decreasing order and decomposes it into two parts: the signal on the left and the noise on the right. The position of the separation relates to the so-called \emph{intrinsic dimension} of the dataset \citep{shepard_analysis_1962}.
Such a decomposition can be done with simple rules relying on the shape of the profile, like the elbow method \citep{thorndike_who_1953} or the percentage of explained variance. However, those heuristics lack statistical foundation and do not depend on the size of the dataset.
Some more statistically grounded dimension selection methods rely on a generative modelling formulation of PCA, called \emph{Probabilistic PCA (PPCA)} \citep{tipping_probabilistic_1999}. PPCA can be seen as a covariance model where the lowest eigenvalues, representing the noise, are all equal. %, which models the data as a multivariate Gaussian whose last eigenvalues are equal, and correspond to an isotropic noise.
The choice of the cutoff dimension is then rather based on the \emph{principle of parsimony}: the selected model is the one that has the lowest number of parameters, while still well representing the data distribution. Such a tradeoff can be achieved with model selection criteria such as the \emph{Bayesian~Information~Criterion~(BIC)} \citep{schwarz_estimating_1978}, which depends on the dataset size and favors \emph{low-complexity} over \emph{goodness-of-fit} when the number of available samples is limited (\emph{small-data} regime). % it is this kind of criterion that is used in the official sklearn PCA implementation \cite{Minka}.
%%XP More in details,
As the eigenvalues of full-rank sample covariance matrices are almost surely all distinct (see discussion in Appendix~\ref{subsec:proof_Q}), PPCA makes an error when modelling the sample covariance matrix with equal noise eigenvalues, but this error is balanced by the complexity drop.
% This results in a parsimonious version of the full covariance model with repeated noise eigenvalues.
%%XP:
One may wonder however if such a complexity drop is enough, especially in the small-data regime.
The eigenvalue-equalisation principle could indeed naturally be extended to the signal space by equalising adjacent sample eigenvalues with small gaps, achieving a better complexity/goodness-of-fit tradeoff. 


This motivates us to investigate a more general family of covariance models with repeated eigenvalues that is stratified by eigenvalue multiplicity \citep{arnold_remarks_1995}. 
Those models, 
coined 
\emph{Stratified PCA (SPCA)}, enjoy an explicit maximum likelihood estimate and a unifying geometric characterization relying on flag manifolds.
SPCA enables to answer a first key question on the identifiability of two adjacent sample eigenvalues.
Among the outcomes, we get that a pair of adjacent eigenvalues with a relative gap lower than $21\%$ needs at least $1000$ data points to be distinguished, which is rarely satisfied by real datasets.
To extend this result to more than two eigenvalues, we must perform model selection among the whole family of SPCA models, which contains PPCA.
As the number of candidate models grows exponentially with the data dimension, we are encouraged to design non-greedy model selection heuristics.
Fortunately, the stratification of the family of SPCA models and the induced partial order on the sequence of eigenvalue multiplicities enables to
design computationally efficient model selection heuristics, whose asymptotic consistency is moreover proven.
The application of our model to synthetic and real datasets successfully shows that equalising groups of adjacent eigenvalues with small gaps is indeed justified when the number of available samples is limited. The experiments notably show that SPCA models achieve a better complexity/goodness-of-fit tradeoff than PPCA.


The paper is organized in the following way. 
In Section~\ref{sec:PPCA}, we present the PPCA model, its maximum likelihood estimate and number of free parameters, as well as a parsimonious version of PPCA called \emph{Isotropic PPCA (IPPCA)} \citep{bouveyron_intrinsic_2011}.
In Section~\ref{sec:SPCA}, we introduce the SPCA model. We derive an explicit maximum likelihood estimate that boils down to an eigenvalue decomposition of the sample covariance matrix followed by a block-averaging of groups of adjacent eigenvalues. We show that SPCA extends PPCA and IPPCA and comes with an insightful geometric interpretation relying on flag manifolds. This enables the accurate computation of the number of free parameters.
In Section~\ref{sec:MS}, we develop a model selection framework for SPCA. This one first enables to answer a key question on the distinguishability of two adjacent sample eigenvalues and second to go beyond two eigenvalues using heuristics based on the structure of the SPCA family.
In Section~\ref{sec:Exp}, we compare PPCA and SPCA models on synthetic and real datasets and show the improvement brought by equalising adjacent eigenvalues with small gaps.