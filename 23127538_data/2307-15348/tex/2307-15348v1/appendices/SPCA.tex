\section{Proof of Theorem~\ref{thm:SPCA} (Maximum likelihood of SPCA)}
\label{sec:appendix_sPCA}

% to which extent is the proof necessary, and needs to be detailed? Because in \citep{bouveyron_intrinsic_2011}, they almost do not give a proof...

% Rk: maybe could add some references to EigenGAN, PPCA, Xavier's BSA, my Master's thesis, and finally Breiding et. al. I guess it's not the first time such a kind of proof is done. So either I redo it fully, or maybe I can cite another work. I don't know which one is the most elegant.fr


% We showed in equation \eqref{eq:cov_simpl} that we can obtain a more compact expression of the SPCA population covariance matrix using orthogonal matrices. For conciseness, we will use that parameterization of the maximum likelihood, and the results with the original parameters $\hat{\boldsymbol{\mu}}, \hat {U_k^2}, \hat{\sigma^2}, \hat {\sigma_k^2}$ will be easily deduced.

We successively find the optimal $\hat {\boldsymbol{\mu}} \in \R^p$, $\hat {\mathit{Q}} \in \O(p)$ and $\hat{\ell}_k \in \R$.


\subsection{Expression of $\hat {\boldsymbol{\mu}}$}

The log-likelihood expresses as a function of ${\boldsymbol{\mu}} \in  \R^p$ in the following way
\begin{equation}\ln \mathcal{L}({\boldsymbol{\mu}}) = -\frac n 2 \tr{{\mathit{\Sigma}}^{-1} {\mathit{C}}} + \operatorname{constant}
\end{equation}
with ${\mathit{C}} = \frac 1 n \sum_{i=1}^n ({\boldsymbol{x}}_i - {\boldsymbol{\mu}}) ({\boldsymbol{x}}_i - {\boldsymbol{\mu}})\T$.
The optimal shift $\hat{{\boldsymbol{\mu}}}$ is thus
\begin{equation}
\hat{{\boldsymbol{\mu}}} = \argmin_{\substack{{\boldsymbol{\mu}} \in \R^p}}
\sum_{i=1}^n ({\boldsymbol{x}}_i - {\boldsymbol{\mu}})\T {\mathit{\Sigma}}^{-1} ({\boldsymbol{x}}_i - {\boldsymbol{\mu}}) := f({\boldsymbol{\mu}}).
%\tr{{\mathit{\Sigma}}^{-1} \sum_{i=1}^n ({\boldsymbol{x}}_i - {\boldsymbol{\mu}}) ({\boldsymbol{x}}_i - {\boldsymbol{\mu}})\T} = 
\end{equation}
The gradient of ${\boldsymbol{x}} \mapsto ({\boldsymbol{x}} - {\boldsymbol{\mu}})\T {\mathit{\Sigma}}^{-1} ({\boldsymbol{x}} - {\boldsymbol{\mu}})$ is ${\boldsymbol{x}} \mapsto 2 {\mathit{\Sigma}}^{-1} ({\boldsymbol{x}} - {\boldsymbol{\mu}})$.
Hence, setting the gradient of $f$ to $0$ at $\hat {\boldsymbol{\mu}}$, one gets
$~{
\sum_{i} 2 {\mathit{\Sigma}}^{-1} ({\boldsymbol{x}}_i - \hat {\boldsymbol{\mu}}) = 0
}$,
whose solution is \underline{$\hat {\boldsymbol{\mu}} = \bar {\boldsymbol{x}}$}.

Hence $\hat {\mathit{C}}$ is actually the sample covariance matrix of the dataset, which will be denoted ${\mathit{S}}$ (as in the theorem statement) from now on.




\subsection{Expression of $\hat {\mathit{Q}}$}\label{subsec:proof_Q}

The log-likelihood expresses as a function of ${\mathit{Q}}$ in the following way
\begin{equation}\ln \mathcal{L}({\mathit{Q}}) = -\frac n 2 \lrp{\ln |{\mathit{\Sigma}}| + \tr{{\mathit{\Sigma}}^{-1} {\mathit{S}}}} + \operatorname{constant}
\end{equation}
with ${\mathit{\Sigma}} = {\mathit{Q}} {\mathit{L}} {\mathit{Q}}\T$.
Hence $|{\mathit{\Sigma}}|$ is independent of ${\mathit{Q}}$ and the optimal orthogonal transformation $\hat{{\mathit{Q}}}$ is
\begin{equation}
\hat{{\mathit{Q}}} = \argmin_{\substack{{\mathit{Q}} \in \O(p)}} \tr{{\mathit{\Sigma}}^{-1} {\mathit{S}}} = \tr{{\mathit{Q}} {\mathit{L}}^{-1} {\mathit{Q}}\T {\mathit{S}}}  :=  g({\mathit{Q}}).
\end{equation}

% We will look for minimizers in the full space of $\O(p)$, and as the function is constant on the fibers of $\flag \simeq \O(p)/\O({\boldsymbol{\gamma}})$, we will get the optimal flags.

\noindent As $g$ is a smooth function on $\O(p)$ which is a compact manifold, $\hat{{\mathit{Q}}}$ exists and 
$~{
dg_{\hat{{\mathit{Q}}}}\colon \mathcal{T}_{\hat {\mathit{Q}}}(\O(p)) \ni {\mathit{\delta}} \mapsto \tr{\lrp{{\mathit{\delta}} {\mathit{L}}^{-1} {\hat {\mathit{Q}}}\T + {\hat {\mathit{Q}}} {\mathit{L}}^{-1} {\mathit{\delta}}\T} {\mathit{S}}} \in \R
}$ vanishes.
It is known that $~{\mathcal{T}_{\hat {\mathit{Q}}}(\O(p)) = \Skew_p {\hat {\mathit{Q}}}}$, therefore one has for all ${\mathit{A}} \in \Skew_p$
\begin{equation}
dg_{\hat{{\mathit{Q}}}}({\mathit{A}} {\hat {\mathit{Q}}}) = 
\tr{\lrp{({\mathit{A}} {\hat {\mathit{Q}}}) {\mathit{L}}^{-1} {\hat {\mathit{Q}}}\T + {\hat {\mathit{Q}}} {\mathit{L}}^{-1} ({\mathit{A}} {\hat {\mathit{Q}}})\T} {\mathit{S}}} = \tr{{\mathit{A}} ({\mathit{\Sigma}}^{-1} {\mathit{S}} - {\mathit{S}} {\mathit{\Sigma}}^{-1})} = 0.    
\end{equation}
Therefore ${\mathit{\Sigma}}^{-1} {\mathit{S}} - {\mathit{S}} {\mathit{\Sigma}}^{-1} = 0$. Hence, ${\mathit{S}}$ and  ${\mathit{\Sigma}}^{-1}$ are two symmetric matrices that commute, so they must be simultaneously diagonalizable in an orthonormal basis.
Since the trace is basis-invariant, $g$ simply rewrites as a function of the eigenvalues
\begin{equation}\label{eq:g_psi}
    g({\mathit{Q}}) = %\tr{{\mathit{L}}^{-1} \diag{\lambda_{\psi(1)}, \dots, \lambda_{\psi(p)}}} = 
    \sum_{k=1}^d \ell_k^{-1} \lrp{\sum_{j \in \phi_{\boldsymbol{\gamma}}^{-1} (\lrs{k})} \lambda_{\psi(j)}},
\end{equation}
where $\psi \in S_p$ is a permutation and $\phi_{\boldsymbol{\gamma}}^{-1} (\lrs{k})$ is the set of indexes in the $k$-th part of the composition $\boldsymbol{\gamma}$ (cf. Subsection~\ref{subsec:type}).
We now need to find the permutation $\hat \psi \in S_p$ that minimizes $g$.
First, since $~{\ell_1 > \dots > \ell_d > 0}$ by assumption, then $~{\lrp{\ell_k^{-1}}_{k=1}^d}$ is an increasing sequence.
Therefore, $\lrp{\lambda_{\hat{\psi}\lrp{\phi_{\boldsymbol{\gamma}}^{-1} \lrs{k}}}}_{k=1}^d$ must be a non-increasing sequence, in that for $k_1 < k_2$, the eigenvalues in the $k_1$-th part of $\boldsymbol{\gamma}$ must
be greater than or equal to the eigenvalues in the $k_2$-th part.
Indeed, for $\ell < \ell'$, if $\lambda < \lambda'$, then $~{\ell \lambda' + \ell' \lambda < \ell \lambda + \ell' \lambda'}$.
Second, for such a $\hat{\psi}$ sorting the eigenvalues in non-increasing order in between parts, we can easily check that the inequality between eigenvalues of distinct parts is strict if and only if the type of ${\mathit{\Sigma}}$ is a refinement of ${\boldsymbol{\gamma}}$. 
If so, the minimizing $\hat{\psi}$ is unique up to permutations within each part of $\boldsymbol{\gamma}$. 
Therefore, it is not $\hat {\mathit{Q}}$ itself but the sequence of eigenspaces of $\hat {\mathit{Q}}$ generated by its $\boldsymbol{\gamma}$-composition (cf. Subsection~\ref{subsec:type}) that is unique, and we have $\underline{(\operatorname{Im}(\hat{{\mathit{Q}}}_1), \dots, \operatorname{Im}(\hat{{\mathit{Q}}}_d)) = (\operatorname{Im}(V_1), \dots, \operatorname{Im}(V_d))}$. Hence, the accurate space to describe the parameter $\hat {\mathit{Q}}$ is actually the space of flags of type $\gamma$.

%Therefore, $\underline{\hat {\mathit{Q}} = V} \in \O(p)$ is the unique maximum likelihood estimate up to permutations of the columns of $V$ within each part of $\boldsymbol{\gamma}$.
%\textbf{(in writing...)}


%Hence there exists a permutation $\psi \in S_p$ such that
%\begin{equation}
%    \begin{cases}
%        {\mathit{\Sigma}}^{-1} &= {\hat {\mathit{Q}}} \diag{\ell_1^{-1} I_{\gamma_1}, \dots, \ell_d^{-1} I_{\gamma_d}} {\hat {\mathit{Q}}}\T\\
%        {\mathit{S}} &= {\hat {\mathit{Q}}} \diag{\lambda_{\psi(1)}, \dots, \lambda_{\psi(p)}} {\hat {\mathit{Q}}}\T
%    \end{cases}
%\end{equation}
%and we now need to find the permutation $\hat \psi \in S_p$ that minimizes the objective 
%\begin{equation}\label{eq:g_psi}
%    g(\psi) = \tr{{\mathit{L}}^{-1} \diag{\lambda_{\psi(1)}, \dots, \lambda_{\psi(p)}}} = \sum_{k=1}^d \ell_k^{-1} \lrp{\sum_{j \in \phi_{\boldsymbol{\gamma}}^{-1} (\lrs{k})} \lambda_{\psi(j)}},
%\end{equation}
%where $\phi_{{\boldsymbol{\gamma}}}$ is the ${\boldsymbol{\gamma}}$-composition function.

%Let us now prove that the identity (up to permutations within each part of ${\boldsymbol{\gamma}}$) is an optimal permutation, and that it is unique if and only if the type of ${\mathit{\Sigma}}$ is a refinement of ${\boldsymbol{\gamma}}$.
%First, we show that the optimal permutation $\hat \psi$ must make all the sample eigenvalues in a part of $\boldsymbol{{\boldsymbol{\gamma}}}$ greater than or equal to the ones in the next parts.
%\textit{Ad absurdum}, let us assume that for a given couple of integers ${j_1} < {j_2}$ located in two different parts of indexes ${k_1} < {k_2}$, one has $~{\lambda_{\hat{\psi}({j_1})} < \lambda_{\hat{\psi}({j_2})}}$. Then 
%$~{(\ell^{-1}_{k_1} - \ell^{-1}_{k_2}) (\lambda_{\hat{\psi}({j_1})} - \lambda_{\hat{\psi}({j_2})}) > 0}$.
%Therefore 
%$~{\ell_{k_1}^{-1} \lambda_{\hat{\psi}({j_1})} + \ell_{k_2}^{-1} \lambda_{\hat{\psi}({j_2})} > \ell_{k_1}^{-1} \lambda_{\hat{\psi}({j_2})} + \ell_{k_2}^{-1} \lambda_{\hat{\psi}({j_1})}}$ 
%which proves that $\hat{\psi}$ doesn't minimize $g$.
% Hence, \textit{ad absurdum}, \underline{$\hat{{\mathit{Q}}} = V$} is a maximum likelihood solution.
%Second, we show that such a permutation $\hat{\psi}$ (up to permutations within each part of ${\boldsymbol{\gamma}}$) is unique if and only if the type of ${\mathit{\Sigma}}$ is a refinement of ${\boldsymbol{\gamma}}$.
%On the one hand, if the type of ${\mathit{\Sigma}}$ is a refinement of ${\boldsymbol{\gamma}}$, then the sample eigenvalues in a part $k_1$ will be all strictly greater than the ones in a part ${k_2} > {k_1}$, so a permutation of indexes between two different parts will contradict the first claim. On the other hand, if the type of ${\mathit{\Sigma}}$ is not a refinement of ${\boldsymbol{\gamma}}$, then we can find two consecutive parts $k$ and $k+1$ which contain the same sample eigenvalues. Therefore the permutation of the associated indexes, belonging to different parts, won't change $g$.
%Hence, for any couple of indexes ${j_1} < {j_2}$ located in two different parts, one must have $\lambda_{\hat{\psi}({j_1})} > \lambda_{\hat{\psi}({j_2})}$, which yields by product of transpositions that an optimal permutation $\hat{\psi}$ is the identity, up to permutations within each part.
%To conclude, the optimal permutation $\hat{\psi}$ is unique, up to permutations within each part of ${\boldsymbol{\gamma}}$, if and only if the type of ${\mathit{\Sigma}}$ is a refinement of ${\boldsymbol{\gamma}}$. This means that $\hat {\mathit{Q}}$ is not unique, but the flag of linear subspaces generated by its $\boldsymbol{\gamma}$-composition is. Hence, \underline{$\hat{{\mathit{Q}}} \sim V$}.

% Must really improve the last part, by more rigorously defining the notion of "part", and also being more rigorous in the implications.

%We take back the proof of Theorem~\ref{thm:SPCA} in subsection~\ref{subsec:proof_Q}, where we want to find a permutation minimizing the objective function~\eqref{eq:g_psi}.
%This time, the permutation isn't necessarily the identity up to permutations within parts, except when the type of ${\mathit{S}}$ is finer than ${\boldsymbol{\gamma}}$.
%It essentially comes from the fact that the inequality \eqref{eq:permutation_energy} cannot be strict anymore in the general setting.
%With the same arguments as in the previous proof, we can show that the minimizing permutations must sort the eigenvalues of ${\mathit{S}}$ in the decreasing order by part, in the sense that for any two parts $k_1 < k_2 \in \lrb{1 \twodots d}$, all the eigenvalues of ${\mathit{S}}$ in $k_1$ must be greater or equal than the ones in $k_2$.
%One then has two cases. 
%If the type of ${\mathit{S}}$ is finer than ${\boldsymbol{\gamma}}$, then for any two parts $k_1 < k_2 \in \lrb{1 \twodots d}$, all the eigenvalues of ${\mathit{S}}$ in $k_1$ are strictly greater than the ones in $k_2$, so the associated eigenspaces are uniquely determined. 
% Otherwise, two adjacent parts may contain the same eigenvalue of ${\mathit{S}}$ and thus interverting some associated eigenvectors may not change the minimum but change the associated eigenspaces.

An important remark is that the uniqueness condition will almost surely be met.
Indeed, the set of $p \times p$ symmetric matrices with repeated eigenvalues has null Lebesgue measure (it is a consequence of Sard's theorem applied to the discriminant polynomial function \citep{breiding_geometry_2018}). % look for older references?
% In a few words: a matrix has repeated eigenvalues if and only if the discriminant of its characteristic polynomial is zero (https://math.stackexchange.com/questions/4612973/conditions-for-distinct-eigenvalues-of-a-matrix). The characteristic polynomial of a matrix is polynomial in the coefficients of the matrix, and the discriminant as well. Hence by composition, the set of matrices with repeated eigenvalues is the preimage of zero by a non-trivial polynomial function. Therefore it is a non-trivial algebraic variety and in particular it has zero measure (https://mathoverflow.net/questions/76245/measure-of-subspace-of-matrices-with-repeated-singular-values).
% To avoid the part I don't master well (algebraic variety), an alternative end using Sard's theorem is the following: symm. matrices with repeated eigenvalues are critical points of the discriminant function (indeed one can shift the entry by the identity and the type of the symm matrix won't change, so the differential along that direction is zero, so the Jacobian is not full rank).
Therefore, for $n \geq p$ and any density with respect to Lebesgue measure on the set of sample covariance matrices, a randomly drawn matrix ${\mathit{S}}$ almost surely has distinct eigenvalues.
%%XP
%for $n > p+1$, 
Consequently, its type is $\lrp{1, \dots, 1}$, which is a refinement of any possible type in $\mathcal{C}(p)$.
% Hence we almost surely have the uniqueness.


\subsection{Expression of $\hat {{\mathit{L}}}$}

The log-likelihood expresses as a function of ${\mathit{L}}$ in the following way
\begin{equation}\ln \mathcal{L}({\mathit{L}}) = -\frac n 2 \lrp{\ln |{\mathit{\Sigma}}| + \tr{{\mathit{\Sigma}}^{-1} {\mathit{S}}}} + \operatorname{constant}
\end{equation}
with ${\mathit{\Sigma}} = \hat {\mathit{Q}} {\mathit{L}} {\hat {\mathit{Q}}}\T$. 
First, one has $\ln|{\mathit{\Sigma}}| = \sum_{k=1}^d \gamma_k \ln \ell_k$. 
Second, according to the previous results, one has $\tr{{\mathit{\Sigma}}^{-1} {\mathit{S}}} = \sum_{k=1}^d \ell_k^{-1} \lrp{\sum_{j \in \phi_{\boldsymbol{\gamma}}^{-1} \lrs{k}} \lambda_j}$.
The optimal eigenvalues $\lrp{\hat{\ell}_1, \dots, \hat{\ell}_d}$ are thus
\begin{equation}
\lrp{\hat{\ell}_1, \dots, \hat{\ell}_d} = \argmin_{\substack{\ell_1, \dots, \ell_d \in \R}} \sum_{k=1}^d \gamma_k \ln \ell_k + \ell_k^{-1} \lrp{\sum_{j \in \phi_{\boldsymbol{\gamma}}^{-1} \lrs{k}} \lambda_j}  :=  h(\ell_1, \dots, \ell_d).
\end{equation}

\noindent As 
$\frac {\partial h} {\partial \ell_k} = \frac{\gamma_k}{\ell_k} - \ell_k^{-2} \lrp{\sum_{j \in \phi_{\boldsymbol{\gamma}}^{-1} \lrs{k}} \lambda_j}$, we get that \underline{$\hat {\ell}_k = \frac 1 {\gamma_k} \lrp{\sum_{j \in \phi_{\boldsymbol{\gamma}}^{-1} \lrs{k}} \lambda_j}$}.