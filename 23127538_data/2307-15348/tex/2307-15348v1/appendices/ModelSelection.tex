\section{Other proofs}
\label{sec:appendix_heuristics}


\subsection{Proof of Proposition~\ref{prop:eigenvalue_averaging} (Eigenvalue equalisation)}\label{subsec:appendix_eigval_averaging}
We compare the BIC of the full covariance model $\boldsymbol{\gamma} = \lrp{1, \dots, 1}$ to the one of the equalised covariance model $\boldsymbol{\gamma}' = \lrp{1, \dots, 1, 2, 1, \dots 1}$ where the $j$-th eigenvalue has been equalised with the $j+1$-th. This boils down to studying the sign of the function $\Delta \BIC := \BIC({\boldsymbol{\gamma}}) - \BIC({\boldsymbol{\gamma}}')$. One gets
\begin{align}
\Delta \BIC &= p \frac{\ln n}{n} + \sum_{k=1}^p \ln \lambda_k - \lrp{p - 2} \frac{\ln n}{n} - \sum_{k \notin \lrs{j, j+1}} \ln \lambda_k - 2 \ln\lrp{\frac{\lambda_j + \lambda_{j+1}} 2}\\
&= 2\frac{\ln n}{n} + \ln \lambda_j + \ln \lambda_{j+1} - 2 \ln\lrp{\frac{\lambda_j + \lambda_{j+1}} 2}\\
&= 2\frac{\ln n}{n} + \ln \lambda_j + \ln \lrp{\lambda_j \lrp{1 - \delta_j}} - 2 \ln\lrp{\frac{\lambda_j \lrp{2 - \delta_j}} 2}\\
&= 2\frac{\ln n}{n} + \ln{\lrp{1 - \delta_j}} - 2 \ln\lrp{1 - \frac{\delta_j} 2}\\
&= 2\frac{\ln n}{n} - \ln\lrp{\frac{\lrp{1 - \frac{\delta_j} 2}^2}{1 - \delta_j}}.
\end{align}

\noindent Hence, one has
$$\Delta \BIC = 0 \iff e^{2\frac{\ln n}{n}} = \frac{\lrp{1 - \frac{\delta_j} 2}^2}{1 - \delta_j} \iff \frac {\delta_j^2} 4 - \lrp{1 - e^{2 \frac{\ln n} n}} \delta_j + 1 - e^{2 \frac{\ln n} n} = 0.$$
It is a polynomial equation whose positive solution is unique when $n \geq 1$ and is
\begin{equation}
\delta(n) :=  2 - 2 e^{2 \frac{\ln n} n} + 2\sqrt{e^{4 \frac{\ln n} n} - e^{2 \frac{\ln n} n}}. 
\end{equation}



\subsection{Proof of Proposition~\ref{prop:hierarchical_heuristic} (Asymptotic consistency of the hierarchical clustering)}\label{subsec:appendix_heuristic_hierarchical}
Let us assume that the true generative model is stratified with type ${\boldsymbol{\gamma}} \in \mathcal{C}(p)$. 
We can then write the population covariance matrix as ${\mathit{\Sigma}} = \sum_{k=1}^{d} \ell_k {\mathit{Q}}_k {\mathit{Q}}_k\T$ with $~{\ell_1 > \dots > \ell_{d} > 0}$ and ${\mathit{Q}} := \lrb{{\mathit{Q}}_1|\dots|{\mathit{Q}}_{d}} \in \O(p)$. 
Let $n$ be the number of independent samples and ${\mathit{S}}_n := \sum_{j=1}^{p} \lambda_j({\mathit{S}}_n) {\boldsymbol{v}}_j({\mathit{S}}_n) {\boldsymbol{v}}_j({\mathit{S}}_n)\T$ with $\lambda_1 \geq \dots \geq \lambda_p$ and ${\mathit{V}} := \lrb{{\boldsymbol{v}}_1|\dots|{\boldsymbol{v}}_p} \in \O(p)$. 
According to \cite[Proposition~1]{bouveyron_intrinsic_2011} and  \cite[Lemma~2.1~(i)]{tyler_asymptotic_1981}, one then has almost surely, as $n$ goes to infinity, $\lambda_j({\mathit{S}}_n) \to \ell_{\phi_{{\boldsymbol{\gamma}}}(j)}$, where $\phi_{{\boldsymbol{\gamma}}}$ is the ${\boldsymbol{\gamma}}$-composition function (cf. Subsection~\ref{subsec:type}). 
Hence for $n$ large enough, the gaps between eigenvalues in the same part of the ${\boldsymbol{\gamma}}$-composition will be arbitrarily close to $0$, while the other will be arbitrarily close to the true values $\lrs{\Delta\lrp{\ell_k, \ell_{k+1}}, k \in \lrb{1\twodots d-1}}$, which are all positive.
% I could detail the ``large enough'' and ``arbitrarily close'' but I don't think that's needed... the idea is that we can find an n such that above it, all the sample eigenvalues are very close to the population eigenvalues. And if this ``very close'' is such that the largest eigengap between the largest eigenvalue and the lowest one of each cluster is for eg twice smaller that the smallest of the eigengaps between clusters, then, although the progressive merging modifies the clusters and the average eigenvalues, they will still yield smaller eigengaps than the smallest of the eigengaps between clusters.
Hence the hierarchical clustering method will first agglomerate the eigenvalues that are in the same part of ${\boldsymbol{\gamma}}$, and second the distinct blocks, by increasing order of pairwise distance. The last model of the first phase will be exactly the true model. Asymptotic criteria like the BIC will thus consistently choose the true model among this reduced subfamily of cardinal $p$. %, because it commits almost no error due to the averaging of eigenvalues that are roughly equal, while being less complex than the full covariance model.



\subsection{Proof of Proposition~\ref{prop:fixed_length_heuristic} (Asymptotic consistency of the type-length prior)}\label{subsec:appendix_heuristic_length}
% some better notations on the number of the batches are to introduce...
Let us assume that the true generative model is stratified with type $~{{\boldsymbol{\gamma}}^*:=\lrp{\gamma_1^*, \dots, \gamma_d^*}}$, of length $d$, and let $\ell_1 > \dots > \ell_{d} > 0$ be the eigenvalues of the associated population covariance matrix.
%In this proof, by comparing the maximum likelihood of all Stratified PCA models with type of cardinal $c+1$, we will show that the one of ${\boldsymbol{\gamma}}^*$ is maximal, and thus it will be selected.
Then, similarly as in the last proof, almost surely, asymptotically, the sample covariance matrix eigenvalues are the ones of the population covariance matrix.
Hence, for any SPCA model of type ${\boldsymbol{\gamma}} := \lrp{\gamma_1, \dots, \gamma_d}$, the maximum likelihood writes
\begin{equation}
\ln{\hat{\mathcal{L}}} \sim -\frac n 2 \lrp{p \ln 2\pi + \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{j \in \phi_{{\boldsymbol{\gamma}}}^{-1} \lrs{k}} \ell_{\phi_{{\boldsymbol{\gamma}}^*}(j)}}}.
\end{equation}
As $n$ and $p$ are fixed when we compare the models, they do not intervene in the model selection. Hence, the search of the optimal model in terms of maximum likelihood boils down to the following problem 
\begin{equation}
    \argmin_{\substack{{\boldsymbol{\gamma}} \in \mathcal{C}(p)\\ \#{{\boldsymbol{\gamma}}}=d}}
    \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{j \in \phi_{{\boldsymbol{\gamma}}}^{-1} \lrs{k}} \ell_{\phi_{{\boldsymbol{\gamma}}^*}(j)}} := f({\boldsymbol{\gamma}}).
\end{equation}

\noindent One has $f({\boldsymbol{\gamma}}) = \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{k'=1}^{d} %\#{\gamma(k) \cap \gamma^*(k')}
c_{kk'} \ell_{k'}}$, where $c_{kk'}$ is the cardinal of the intersection of the $k$-th part of ${\boldsymbol{\gamma}}$ with the $k'$-th part of ${\boldsymbol{\gamma}}^*$.
Then, by definition, one has $\sum_{k'=1}^{d} c_{kk'} = \gamma_k$ and $\sum_{k=1}^{d} c_{kk'} = {\boldsymbol{\gamma}}^*_{k'}$. Hence, using Jensen's inequality,
\begin{equation}
f({\boldsymbol{\gamma}}) \geq \sum_{k=1}^{d} \gamma_k \lrp{\sum_{k'=1}^{d} \frac{c_{kk'}}{\gamma_k}\ln \ell_{k'}} = \sum_{k,k'=1}^{d} c_{kk'} \ln \ell_{k'} = \sum_{k'=1}^{d} {\boldsymbol{\gamma}}^*_{k'} \ln \ell_{k'} = f({\boldsymbol{\gamma}}^*).
\end{equation}
To conclude, asymptotically, ${\boldsymbol{\gamma}}^*$-SPCA is the most likely model. Hence, the maximum likelihood criterion alone finds the true model among the family of SPCA models with the same type length.