\section{Discussion}\label{sec:Ccl}
We introduced in this paper a generative covariance model with repeated eigenvalues called SPCA, which generalises PPCA~\citep{tipping_probabilistic_1999} and IPPCA~\citep{bouveyron_intrinsic_2011} under a unique geometric framework relying on flag manifolds.
We noticed that the parsimony of PPCA comes from the low-rank model and the emergence of a multidimensional isotropic noise eigenspace. This raised the natural question of extending the isotropy constraint to the signal space.
The SPCA model showed that assuming distinct eigenvalues in the signal space---as PPCA does---is not justified in practice. Hence, SPCA could circumvent this issue by equalising the adjacent eigenvalues with small gaps and gathering the associated eigenvectors into multidimensional eigenspaces.
We confirmed our expectations on synthetic and real datasets, showing how SPCA models achieve a better complexity/goodness-of-fit tradeoff than PPCA.
The code is available on GitHub\footnote{\url{https://github.com/tomszwagier/stratified-pca}}.


SPCA is at an early stage of research and its development has been requiring several limiting choices that could be relaxed and improved in future works.
A first limit is the choice of the BIC for model selection. Indeed, the BIC is known to favor under-parameterised models and not work very well in the small-data regime. However, this does not prevent it from being widely used due to its simplicity. Therefore, it provides an elementary way to highlight the interest of SPCA, similarly as~\cite{tipping_probabilistic_1999} used a simple model selection criterion when introducing PPCA. One could later investigate extensions of~\cite{minka_automatic_2000} (which is relying on a geometric interpretation of PPCA with Stiefel manifolds) and~\cite{drton_bayesian_2017} to SPCA models.
A second limit is the linear-Gaussian nature of SPCA which is not suited to real data. Some nonlinear and non-Gaussian extensions could therefore be considered in the future. The probable lack of analytic solution would involve optimisation on flag manifolds~\citep{ye_optimization_2021}. Due to the cost of inference for each model, we might need to replace discrete model selection with a global optimisation scheme on the space of all SPCA models. The latter being stratified by eigenvalue multiplicity, we could benefit from recent works on stratified optimisation~\citep{leygonie_gradient_2023, olikier_first-order_2023}.



SPCA also comes with several interesting perspectives.
First, it unleashes a whole new family of parsimonious linear-Gaussian models interpolating between the isotropic model and the full covariance one. Hence when a PPCA model overfits and the associated IPPCA model underfits, a better model might lie in the SPCA family.
Second, the multidimensional eigenspaces obtained by gathering eigenvectors associated with distinct sample eigenvalues could provide robust, invariant and interpretable feature subspaces~\citep{hyvarinen_emergence_2000}. Indeed, just like the first eigenvectors can be interpreted as modes of variation~\citep{castro_principal_1986}, the eigenspaces inferred from SPCA could be interpreted as multidimensional attributes, and the norms of projection onto them as their level of expressiveness.
Third, SPCA brings a statistical framework to the flag-based multiscale modeling of datasets. Indeed, several works use flags to represent datasets, be it in an independent component analysis~\citep{nishimori_riemannian_2006} or principal component analysis~\citep{ma_flag_2021} context, enriching the already well developed literature on Grassmannians and Stiefel manifolds for dimension reduction~\citep{edelman_geometry_1998}.
In this paper, by introducing a generative model whose maximum likelihood estimate coincides with the minimiser of the \emph{accumulated unexplained variance} criterion~\citep{pennec_barycentric_2018}, we enrich the previous works and enable for instance to perform flag-type selection.
Fourth, beyond statistical modelling, SPCA provides a low-dimensional approximation of any symmetric matrix. Applications could therefore be investigated in spectral clustering~\citep{ng_spectral_2001} and shape analysis~\citep{lefevre_perturbation_2023}, where repeated eigenvalues in the graph Laplacian are prone to occur, as well as in variational Bayesian methods, where parsimonious Gaussian models can be used to approximate posterior distributions of parameters~\citep{lambert_limited-memory_2023}.