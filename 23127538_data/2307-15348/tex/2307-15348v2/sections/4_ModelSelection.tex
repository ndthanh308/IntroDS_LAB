\section{Model selection}\label{sec:MS}
As discussed in Appendix~\ref{subsec:proof_Q}, sample covariance matrices almost surely have distinct eigenvalues. This makes the full covariance model the most likely to have generated some observed data.
However, it does not mean that the true parameters---that are the eigenvectors and the eigenvalues---can be individually precisely inferred, especially in the small-data regime.
Hence, one can wonder if a covariance model with repeated eigenvalues and multidimensional eigenspaces would not be more robust.
The results of the previous section enable us to provide a possible answer, through SPCA model selection. 
First, we study the inference of two adjacent eigenvalues and their associated eigenvectors. We show that when the eigenvalue gap is small and the number of samples is limited, one should rather equalise the eigenvalues and gather the associated eigenvectors in a multidimensional eigenspace.
Second, to extend this result to more than two eigenvalues, we develop a general model selection framework based on the stratified structure of SPCA.


\subsection{Bayesian information criterion}\label{subsec:BIC}
In this work, we focus on one simple model selection criterion to set up the ideas. The Bayesian information criterion is defined as 
\begin{equation}\label{eq:BIC}
    \operatorname{BIC} := \kappa \ln n - 2 \ln \hat{\mathcal{L}},
\end{equation}
where $\kappa$ is the number of free parameters---computed in  Proposition~\ref{prop:SPCA_nparam}---and $\ln \hat{\mathcal{L}}$ is the maximum log-likelihood \eqref{eq:SPCA_ML}.
By removing the constant variables within model selection (like $p$ and $n$), we get the following proposition.

\begin{proposition}
\label{prop:SPCE_BIC}
The SPCA model minimising the BIC is
\begin{equation}\label{eq:BIC_simpl}
\hat {\boldsymbol{\gamma}} = \argmin_{\substack{\boldsymbol{\gamma} \in \mathcal{C}(p)}} \lrp{d - \sum_{k=1}^d \frac{\gamma_k (\gamma_k - 1)} {2}} \frac{\ln{n}}{n} + \sum_{k=1}^d \gamma_k \ln{\overline{\boldsymbol{\lambda}^k}}.
\end{equation}
\end{proposition}

\noindent From now on, we remove the shift parameter $\boldsymbol{\mu} \in \R^p$ because it has the same complexity across models, and rather consider SPCA as a covariance model, like done in~\cite{tipping_probabilistic_1999}.



\subsection{Eigenvalue equalisation}\label{subsec:exp_averaging}
Willing to better apprehend the dynamics of SPCA model selection, we lead the experiment of quantifying the BIC variation induced by the equalisation of two adjacent eigenvalues. More precisely and without loss of generality, we compare the BIC of a \emph{full covariance model} $\boldsymbol{\gamma} = \lrp{1, \dots, 1}$ to the one of an \emph{equalised covariance model} $\boldsymbol{\gamma}' = \lrp{1 \dots 1, 2, 1 \dots 1}$, where the eigenvalues $\lambda_j$ and $\lambda_{j+1}$ have been equalised.


\begin{proposition}
\label{prop:eigenvalue_averaging}
Let $\lrp{x_i}_{i=1}^n$ be a p-dimensional dataset with $n$ samples, $\lambda_j \geq \lambda_{j+1}$ two adjacent sample eigenvalues and $\delta_j := \frac{\lambda_{j} - \lambda_{j+1}}{\lambda_j}$ their \emph{relative eigengap}. 
If
\begin{equation}
    \delta_j < 2 - 2 \exp\lrp{2 \frac{\ln n} n} + 2\sqrt{\exp\lrp{4 \frac{\ln n} n} - \exp\lrp{2 \frac{\ln n} n}} := \delta(n),
\end{equation} 
then the equalised covariance model has a lower BIC than the full one.
\end{proposition}
\begin{proof} --
The proof is given in Appendix~\ref{subsec:appendix_eigval_averaging}.
\end{proof}

\noindent The \emph{threshold function} $\delta(n)$ is represented in Figure~\ref{fig:threshold_function}.
% Figure environment removed
One can deduce for instance that if a pair of adjacent sample eigenvalues has a relative eigengap lower than $21\%$, then one needs at least $1000$ samples to justify the use of a model with distinct eigenvalues. This is an important result since many real datasets do not fulfill this condition, as we will see in the next section.
As far as we know, this is the first time that a study on the parsimony induced by the equalisation of two adjacent sample eigenvalues is performed. This is enabled by the very design of SPCA and the geometric interpretation of its parameter space, involving flag manifolds.
We could extend this study to the equalisation of more than two eigenvalues, but it would not necessarily yield a condition as simple as the one of Proposition~\ref{prop:eigenvalue_averaging}. Hence, in the following, we establish a general framework for SPCA model selection. We study the structure of the family of models and design efficient model selection heuristics.


\subsection{Structure of the SPCA family}
Given a dimension $p$, PPCA has $p$ models, ranging from the isotropic Gaussian ($q=0$) to the full covariance model ($q=p-1$). We can naturally equip the set of PPCA models with the  \emph{less-than-or-equal} relation $\leq$ on the latent variable dimension $q$, which makes it a totally ordered set. The complexity of the model then increases with $q$ (cf. Subsection~\ref{subsec:SPCA_param}).
The characterisation of the SPCA family structure is a bit more technical, as it requires to study the hierarchy of types, involving the concept of integer composition. Fortunately, the structure of such sets has already been well studied in combinatorics~\citep{bergeron_standard_1995}. 
Moreover, several works have shown and exploited the stratification of symmetric matrices according to the multiplicities of the eigenvalues~\citep{arnold_remarks_1995,groisser_geometric_2017, breiding_geometry_2018}. Hence, without proof, we can state the following result.

\begin{proposition}\label{prop:pos}
The family of $p$-dimensional SPCA models induces a stratification of the space of full-rank $p \times p$ covariance matrices according to the type $\boldsymbol{\gamma}$.

\noindent The refinement relation $\preceq$ \eqref{subsec:type} makes it a partially ordered set of cardinal $2^{p-1}$.
\end{proposition}

\noindent Hence the set of SPCA models at a given data dimension can be represented using a Hasse diagram, as done in Figure~\ref{fig:hasse_complexity}.
% Figure environment removed
We can see that SPCA contains PPCA, IPPCA, and many new models. 
SPCA therefore has the advantage of possibly providing more adapted models than PPCA and IPPCA, but also the drawback of requiring more comparisons for model selection. 
In high dimension this becomes quickly computationally heavy, so we need to define heuristics for selecting only a few number of models to compare. The previously derived partial order $\preceq$ on the set of SPCA models allows simple efficient heuristics for model selection.


\subsection{Heuristics}\label{subsec:heuristic}
In this subsection, we develop two simple heuristics for model selection. Their common idea is to a priori choose a subfamily of candidate models based on the shape of the eigenvalue profile, and then restrict the model selection process to this smaller subset.

\subsubsection{Hierarchical clustering of eigenvalues}\label{subsubsec:hierarchical_heuristic}
In this heuristic, the subset of candidate models is generated by the \emph{hierarchical clustering}~\citep{ward_hierarchical_1963} of the sample eigenvalues. The general principle of hierarchical clustering is to agglomerate one by one the eigenvalues into clusters, thanks to a so-called \emph{cluster-linkage criterion}, which is a measure of dissimilarity between clusters. 
Here, given two clusters of sample eigenvalues $A$, $B$ and any \emph{continuous} distance $\Delta$ (such as the relative eigengap defined in Proposition~\ref{prop:eigenvalue_averaging}), we take as a cluster-linkage criterion the distance between the average eigenvalue in each cluster, $\Delta\lrp{\overline A, \overline B}$. 
The method is detailed in Algorithm~\ref{alg:hierarchical} and illustrated in Figure~\ref{fig:hierarchical_clustering}.
The hierarchical clustering heuristic creates a \emph{trajectory} $\lrp{\boldsymbol{\gamma}^t}_{t=1}^p$ in the Hasse diagram of SPCA models. The sequence starts from $\boldsymbol{\gamma}^1 = \lrp{1, \dots, 1}$, the full covariance model, in which each eigenvalue is in its own cluster. Then, one by one, the eigenvalues that are the closest in terms of distance $\Delta$ are agglomerated, and the inter-cluster distances are updated. The algorithm ends when one reaches the isotropic covariance model, $\boldsymbol{\gamma}^p = \lrp{p}$, in which all the eigenvalues are in the same cluster. This corresponds to an \textit{agglomerative} approach in the hierarchical clustering vocabulary, in opposition to a \textit{divisive} approach, that we could similarly develop for this heuristic.
\begin{algorithm}
\caption{Hierarchical clustering heuristic for SPCA model selection}\label{alg:hierarchical}
\begin{algorithmic}
\Require $\lambda_1 \geq \dots \geq \lambda_p, \Delta$ \Comment{sample eigenvalues and distance}
\Ensure $\lrp{\boldsymbol{\gamma}^{t}}_{t=1}^p$ \Comment{subfamily of SPCA models}
\State $\boldsymbol{\gamma}^{1} \gets \lrp{1, \dots, 1}, \quad {\boldsymbol{\lambda}}^{1} \gets \lrp{\lambda_1, \dots, \lambda_p} := {\boldsymbol{\lambda}}$ \Comment{full covariance model}
\For{$t = 1 \twodots p-1$}
    \State $\Delta^{t} \gets (\Delta({\lambda}^{t}_k, {\lambda}^{t}_{k+1}))_{k=1}^{p-t}$ \Comment{distance between adjacent clusters}
    \State $k^{t} = \argmin \Delta^{t}$ \Comment{minimal distance}
    \State $\boldsymbol{\gamma}^{t+1} = ({\gamma}^t_1, \dots, {\gamma}^t_{{k^t}-1}, {\gamma}^t_{k^t} + {\gamma}^t_{{k^t}+1}, {\gamma}^t_{{k^t}+2}, \dots, {\gamma}^t_d)$ \Comment{type agglomeration}
    \State ${\boldsymbol{\lambda}}^{t+1} = \overline{{\boldsymbol{\lambda}}^{\boldsymbol{\gamma}^{t+1}}}$ \Comment{$\boldsymbol{\gamma}$-averaging}
\EndFor
\end{algorithmic}
\end{algorithm}
% Figure environment removed

The hierarchical clustering heuristic hence generates a subfamily of $p$ models that can be then compared within a classical model selection framework. In order to assess the quality of such a heuristic, we show the following consistency result.

\begin{proposition}\label{prop:hierarchical_heuristic}
The hierarchical clustering heuristic~\eqref{subsubsec:hierarchical_heuristic} is consistent, in the sense that almost surely, given enough samples, it generates a subfamily of SPCA models of size $p$ that contains the true model.
\end{proposition}
\begin{proof} --
The proof is given in Appendix~\ref{subsec:appendix_heuristic_hierarchical}.
\end{proof}


Hence, the hierarchical clustering heuristic generates a hierarchical subfamily of models of decreasing complexities, and provided enough data, the true model will be included. Thereafter, using consistent model selection criteria on this reduced subfamily, one can asymptotically recover the true model. 
We now propose a second heuristic that is not hierarchical but instead makes a prior assumption on the model complexity and then selects the one that has the maximum likelihood among all the candidates.

\subsubsection{Prior on the length of the type}\label{subsubsec:type_length_heuristic}
In this heuristic, we perform model selection at a given floor of the Hasse diagram (cf. Figure~\ref{fig:hasse_complexity}). More precisely, we consider as candidates only the models that have a given type length $d$, like done in IPPCA with $d=2$.
The type-length prior heuristic reduces the search space like the previous heuristic, this time to $\binom{p-1}{d-1}$ models. In contrast to the hierarchical clustering heuristic which creates a hierarchy of models with decreasing complexity, we here rather fix the complexity range of the candidate models, by working on one floor of the Hasse diagram, and then try to find the model of best fit.

Just like in the hierarchical clustering heuristic \eqref{subsubsec:hierarchical_heuristic}, we could use the BIC to choose the best model among this reduced family (as done in the second experiment of Subsection~\ref{subsec:real_exp}).
For completeness, we provide an additional criterion that is nothing but the maximum likelihood itself. 
We indeed manage to extend to SPCA the surprising result from~\cite{bouveyron_intrinsic_2011} that the maximum likelihood criterion alone asymptotically consistently finds the true intrinsic dimension within the IPPCA setting. 
Intuitively, this can be explained by the fact that we a priori fix the complexity of the candidate models and therefore we can focus on the other side of the weighing scale that is the goodness of fit.
As this criterion empirically yields competitive results with respect to other classical model selection criteria in the large sample, low signal-to-noise ratio regime, we expect it to be of interest in SPCA as well.
\begin{proposition}\label{prop:fixed_length_heuristic}
The maximum likelihood is asymptotically consistent within the subfamily of SPCA models with a given type length $d$.
\end{proposition}
\begin{proof} --
The proof is given in Appendix~\ref{subsec:appendix_heuristic_length}. We emphasize the use of Jensen's inequality, which elegantly generalises the proof of~\cite{bouveyron_intrinsic_2011}.
\end{proof}

Hence we derived two simple heuristics for model selection, taking into account the structure of the SPCA models family.
We now have all the tools needed for inference and model selection using SPCA.