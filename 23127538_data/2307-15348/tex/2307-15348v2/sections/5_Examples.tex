\section{Experiments}\label{sec:Exp}
As seen in the previous sections, given a dataset and its sample covariance matrix, SPCA equalises the eigenvalues and gives rise to new multidimensional eigenspaces. This causes an additional drop of complexity with respect to PPCA which, according to Figure~\ref{fig:threshold_function}, seems justified when the eigenvalue gaps are small in view of the number of available samples.
In this section, we confirm experimentally this hypothesis on some synthetic and real datasets.


\subsection{Simpler models for all sample size}
A key result in the previous section is that we rarely have enough available samples to confidently assert that two adjacent sample eigenvalues are distinct.
Consequently, PPCA models could be made more parsimonious by equalising the adjacent sample eigenvalues with small gaps in the signal space as well.

Willing to better understand how this result applies in practice, we make the following SPCA model selection experiment.
We consider a given multivariate Gaussian population density, with covariance matrix eigenvalues $\lrp{10, 9, 7, 4, 0.5}$, and sample $n \in \lrb{20, 50000}$ data points from it. We fit all the SPCA models to this data distribution and select the one with the lowest BIC. The experiment is repeated several times independently  for each $n$, and the results are reported on Figure~\ref{fig:BIC_traj}, where we plot only a few models among the $16$ for readability.
% Figure environment removed
First, on the BIC plots, we can see that for $n \leq 6000$, SPCA discloses a whole family of models that better explain the observed data than PPCA.
This shows that even for a very large number of samples with respect to the dimension, distinguishing the first eigenvalues and eigenvectors like PPCA does is not justified.
Second, on the complexity plots, we can see that PPCA mostly selects the full covariance model for any sample size, while SPCA finds less complex models along the whole trajectory.
Moreover, interestingly, we note the consistent increase of model complexity with the number of samples. We deduce that as the sample size increases, SPCA can more confidently distinguish the sample eigenvalues.
Third, on the Hasse diagram, we can see that SPCA follows a trajectory as the number of available samples increases, which recalls the kind of subfamily generated by the hierarchical clustering heuristic (cf. Figure~\ref{fig:hierarchical_clustering}).
To conclude, we see on this synthetic example that SPCA achieves a better complexity/goodness-of-fit tradeoff than PPCA in a wide range of sample sizes by equalising the highest eigenvalues.


\subsection{Parsimony on real data}
\label{subsec:real_exp}
As the previous experiment was synthetic, we naturally wonder whether the same conclusions can be made out of real data.
Indeed, as real datasets follow rather non-linear and multimodal distributions, the application of a simple linear-Gaussian model like SPCA to real datasets seems limited. 
However, PPCA has the same limits and remains quite used as a simple representation.

In this experiment, we compare PPCA to SPCA on several classical real datasets extracted from the open source \href{https://archive.ics.uci.edu}{UCI Machine Learning Repository}: \textit{Glass Identification} \citep{german_glass_1987}, \textit{Ionosphere} \citep{sigillito_ionosphere_1989}, \textit{Wine} \citep{aeberhard_wine_1991} and \textit{Breast Cancer Wisconsin (WDBC)} \citep{wolberg_breast_1995}.
Due to the high dimensionality of some datasets, we cannot perform an exhaustive comparison between all the SPCA models, therefore we use the hierarchical clustering heuristic introduced in Subsection~\ref{subsec:heuristic} with the relative eigengap distance. As those datasets are made for classification problems, we keep only one class in order to make the data distribution more unimodal. For each dataset, we compare the best SPCA model to the best PPCA model (in terms of BIC). The results are reported in Table~\ref{tab:real_life_BIC}.
\begin{table}
\centering
\caption{Comparison of PPCA and SPCA best models on several real datasets.
To shrink long types, we use the power notation to indicate repetition of elements; for instance $(1, 1, 1, 2, 2, 3) := (1^3, 2^2, 3)$.}
\label{tab:real_life_BIC}
\begin{tabular}{|c c c | l  c | l  c|}
    \hline
    \multicolumn{3}{|c}{\textbf{Dataset}} & \multicolumn{2}{c}{\textbf{PPCA}} & \multicolumn{2}{c|}{\textbf{SPCA}}\\
    Name & $n$ & $p$ & $\boldsymbol{\gamma}$ & $\BIC$ & $\boldsymbol{\gamma}$ & $\BIC$\\ 
    \hline\hline
    Wine & $48$ & $13$ & $(1^3, 10)$ & $+36.35$ & $(8, 5)$ & $+35.57$\\
    Glass & $17$ & $9$ & $(1^9)$ & $-16.77$ & $(1, 2, 3, 1^3)$ & $-17.49$\\
    Ion & $224$ & $32$ & $(1^{30}, 2)$ & $-26.59$ & $(1^5, 2, 13, 6, 4, 2)$ & $-28.50$\\
    WDBC & $357$ & $30$ & $(1^{30})$ & $+25.12$ & $(2, 1, 2, 1, 2, 5, 1, 2, 1, 3^2, 4, 1^3)$ & $+24.72$\\
    \hline
\end{tabular}
\end{table}
We can see that for any dataset, SPCA achieves again a better complexity/goodness-of-fit tradeoff than PPCA. 
For instance, on the Wine dataset, PPCA finds a principal subspace of dimension $3$ with distinct eigenvalues, while SPCA finds a principal subspace of dimension $8$ with isotropic variability.
For conciseness, we do not report the sample eigenvalue profiles of those datasets, but we can check that none of them satisfies the relative eigengap condition of Proposition~\ref{prop:eigenvalue_averaging} given the number of available samples. Hence, the SPCA model is indeed justified for modelling real datasets.

In addition to the previous experiment, we also perform a floor-by-floor model comparison on the Glass dataset. More precisely, for a given type length $d \in \lrb{1 \twodots p}$, we compare the unique associated PPCA model ($q = d-1$) to the best SPCA model among the $\binom{p-1}{d-1}$. The results are reported on Table~\ref{tab:glass_BIC}.
\begin{table}
\centering
\caption{Floor-by-floor comparison of PPCA and SPCA best models on the Glass dataset.}
\label{tab:glass_BIC}
\begin{tabular}{|c  c | c  c|}
    \hline
    \multicolumn{2}{|c|}{\textbf{PPCA}}& \multicolumn{2}{c|}{\textbf{SPCA}}\\
    $\boldsymbol{\gamma}$ & $\BIC$ & $\BIC$ & $\boldsymbol{\gamma}$ \\ 
    \hline\hline
    $(9,)$ & $+4.20$ & $+4.20$ & $(9,)$ \\
    $(1, 8)$ & $-0.78$ & $-8.21$ & $(8, 1)$\\
    $(1, 1, 7)$ & $-3.45$ & $-15.92$ & $(3, 5, 1)$\\
    $(1, 1, 1, 6)$ & $-5.97$ & $-16.93$ & $(3, 3, 2, 1)$\\
    $(1, 1, 1, 1, 5)$ & $-6.36$ & $-17.38$ & $(1, 2, 3, 2, 1)$\\
    $(1, 1, 1, 1, 1, 4)$ & $-6.55$ & $-17.49$ & ${(1, 2, 3, 1, 1, 1)}$\\
    $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$\\
    ${(1, \dots \dots \dots, 1)}$ & ${-16.77}$ & $-16.77$ & $(1, \dots \dots \dots, 1)$ \\
    \hline
\end{tabular}
\end{table}
We can see that the rich family of SPCA models with a prespecified number of distinct eigenvalues $d$ importantly increases the modelling power of PPCA. For instance, the SPCA model of type $(8, 1)$ has a lower BIC than the PPCA model of type $(1, 8)$. This suggests that a principal subspace of dimension $8$ with isotropic variability better models the data distribution than a principal subspace of dimension $1$.