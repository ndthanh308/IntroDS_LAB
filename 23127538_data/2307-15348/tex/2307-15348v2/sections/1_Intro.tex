\section{Introduction}
\emph{Principal component analysis (PCA)}~\citep{jolliffe_principal_2002} is a well-known dimension reduction method that is based on the eigenvalue decomposition of the sample covariance matrix.
Usually, after the decomposition, one plots the eigenvalue profile in decreasing order and decomposes it into two parts: the signal on the left and the noise on the right. The position of the separation relates to the so-called \emph{intrinsic dimension} of the dataset~\citep{shepard_analysis_1962}.
Such a decomposition can be done with simple rules relying on the shape of the profile, like the elbow method or the percentage of explained variance~\citep{jolliffe_principal_2002}. 
Another large family of dimension selection methods relies on a generative modelling formulation of PCA, called \emph{probabilistic principal component analysis (PPCA)}~\citep{tipping_probabilistic_1999}, which can be interpreted as a low-dimensional Gaussian model of the data up to an isotropic Gaussian noise. 
In such a framework, the choice of the intrinsic dimension is based on the \emph{principle of parsimony} (also known as \emph{Occam's razor}): the selected model is the one that has the lowest number of parameters, while still well representing the data distribution. Such a tradeoff can be achieved with model selection criteria such as the \emph{Bayesian information criterion (BIC)}~\citep{schwarz_estimating_1978}, which depends on the dataset size and promotes \emph{low-complexity} beyond \emph{goodness-of-fit} when the number of available samples is limited (\emph{small-data}).

Due to the isotropic noise assumption, PPCA can be reinterpreted as a covariance model where the lowest eigenvalues are constrained to be all equal (cf. Section~\ref{sec:PPCA}). This constraint greatly reduces the number of parameters with respect to the full covariance model, while not excessively lowering the approximation quality of the sample covariance matrix, whose eigenvalues are almost-surely all distinct (see discussion in Appendix~\ref{subsec:proof_Q}).
One may wonder however if such a complexity drop is enough, especially in the small-data regime.
The eigenvalue-equality constraint could indeed naturally be extended to the signal space by equalising adjacent sample eigenvalues with small gaps, achieving a better complexity/goodness-of-fit tradeoff. 
 
This motivates us to investigate a more general family of covariance models with repeated eigenvalues, which contains in particular PPCA.
Those models, coined \emph{stratified principal component analysis (SPCA)}, enjoy an explicit maximum likelihood estimate and a unifying geometric characterisation relying on flag manifolds.
Such a geometric interpretation of SPCA enables us to answer a first key question on the inference of two adjacent eigenvalues and their associated eigenvectors.
Among the outcomes, we get that a pair of adjacent eigenvalues with a relative gap lower than $21\%$ needs at least $1000$ data points to be distinguished. More precisely, if this condition is not met (which is often the  case in real datasets), then a model with two equal eigenvalues and a two-dimensional eigenspace is more optimal in terms of BIC.
To extend this result to more than two eigenvalues, we must perform model selection among the whole family of SPCA models.
Since the number of candidate models grows exponentially with the data dimension, we are encouraged to design efficient model selection heuristics.
This leads us to study the structure of the SPCA models, which are shown to have a particular hierarchy that is the one of a stratified family~\citep{geiger_stratified_2001}, according to the multiplicities of the eigenvalues~\citep{arnold_remarks_1995,groisser_geometric_2017, breiding_geometry_2018}.
The partial order induced by such a stratification enables us to design computationally efficient model selection heuristics, whose asymptotic consistency is moreover proven.
The application of our model to synthetic and real datasets successfully shows that equalising groups of adjacent eigenvalues with small gaps is indeed relevant, especially when the number of available samples is limited. The experiments notably show that SPCA models achieve a better complexity/goodness-of-fit tradeoff than PPCA.

The paper is organised in the following way. 
In Section~\ref{sec:PPCA}, we present the PPCA model, its maximum likelihood estimate and number of free parameters, as well as a parsimonious version of PPCA called \emph{isotropic probabilistic principal component analysis (IPPCA)}~\citep{bouveyron_intrinsic_2011}.
In Section~\ref{sec:SPCA}, we introduce the SPCA model. We derive an explicit maximum likelihood estimate that boils down to an eigenvalue decomposition of the sample covariance matrix followed by a block-averaging of adjacent eigenvalues. We show that SPCA extends PPCA and IPPCA and comes with an insightful geometric unification relying on flag manifolds. This enables the accurate computation of the number of free parameters.
In Section~\ref{sec:MS}, we develop a model selection framework for SPCA, taking advantage of the partial order on the family of models induced by the stratification. This notably allows us to answer a key question on the distinguishability of adjacent sample eigenvalues.
In Section~\ref{sec:Exp}, we compare PPCA and SPCA models on synthetic and real datasets and show the improvement brought by equalising adjacent eigenvalues with small gaps.