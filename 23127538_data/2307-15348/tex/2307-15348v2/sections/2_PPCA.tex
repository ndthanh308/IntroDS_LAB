\section{Probabilistic Principal Component Analysis}\label{sec:PPCA}
PCA is a ubiquitous tool in statistics, which however used to lack a statistical model formulation.
\cite{tipping_probabilistic_1999} circumvented this issue by introducing PPCA that we describe in this section.

\subsection{Model}
Let $\lrp{\boldsymbol{x}_i}_{i=1}^n$ be a $p$-dimensional dataset and $q \in \lrb{0 \twodots p-1}$ a lower dimension. In PPCA, the observed data is assumed to stem from a $q$-dimensional latent variable via a linear-Gaussian model

\begin{equation}\label{eq:PPCA_model}
    \boldsymbol{x} = \mathit{W} \boldsymbol{z} + \boldsymbol{\mu} + \boldsymbol{\epsilon}
\end{equation}
with $\boldsymbol{z} \sim \N{0, \mathit{I}_q}$, $\mathit{W} \in \R^{p \times q}$, $\boldsymbol{\mu} \in \R^p$, $\boldsymbol{\epsilon} \sim \N{0, \sigma^2 \mathit{I}_p}$ and $\sigma^2 > 0$. 

Through classical probability theory, one can show that the observed data is modeled as following a multivariate Gaussian distribution:
\begin{equation}
    \boldsymbol{x} \sim \N{\boldsymbol{\mu}, \mathit{W} \mathit{W}\T + \sigma^2 \mathit{I}_p}.
\end{equation}
An analysis of the covariance matrix reveals that the distribution is actually anisotropic on the first $q$ dimensions and isotropic on the remaining $p - q$ ones. Hence there is an implicit constraint on the covariance model of the data, which is that the lowest $p - q$ eigenvalues are assumed to be all equal.


\subsection{Maximum Likelihood}
The PPCA model parameters are the shift $\boldsymbol{\mu}$, the linear map $\mathit{W}$ and the noise factor $\sigma^2$. Let some observed dataset $\lrp{\boldsymbol{x}_i}_{i=1}^n$, $\overline {\boldsymbol{x}} := \frac 1 n \sum_{i=1}^n \boldsymbol{x}_i$ its mean and $~{\mathit{S} := \sum_{j=1}^p \lambda_j \boldsymbol{v}_j {\boldsymbol{v}_j}\T}$ its sample covariance matrix, with $\lambda_1 \geq \dots \geq \lambda_p \geq 0$ its eigenvalues and $\boldsymbol{v}_1 \perp \dots \perp \boldsymbol{v}_p$ some associated eigenvectors. One can explicitly infer the parameters that are the most likely to have generated these data using maximum likelihood estimation.
\cite{tipping_probabilistic_1999} show that the most likely shift is the empirical mean, the most likely linear map is the composition of a scaling by the $q$ highest eigenvalues $\mathit{\Lambda}_q:=\diag{\lambda_1, \dots, \lambda_q}$ (up to the noise) and an orthogonal transformation by the associated $q$ eigenvectors $\mathit{V}_q:=\lrb{{\boldsymbol{v}}_1|\dots|{\boldsymbol{v}}_q}$, and finally the most likely noise factor is the average of the $p - q$ discarded eigenvalues
\begin{equation}\label{eq:PPCA_ML}
\hat{\boldsymbol{\mu}} = \overline {\boldsymbol{x}} , \hspace*{15mm}
\hat{\mathit{W}} = \mathit{V}_q \lrp{\mathit{\Lambda}_q - \hat{\sigma}^2 \mathit{I}_q}^{\frac 1 2} , \hspace*{15mm}
\hat{\sigma}^2 = \frac 1 {p - q} \sum_{j=q+1}^p \lambda_j .
\end{equation}
One can then easily express the maximum log-likelihood
\begin{equation}
    \ln \hat{\mathcal{L}} := -\frac n 2 \lrp{p \ln(2\pi) + \sum_{j=1}^q \ln{\lambda_j} + (p - q) \ln\lrp{\frac 1 {p - q} \sum_{j=q+1}^p \lambda_j} + p}.
\end{equation}

\subsection{Parsimony and model selection}\label{subsec:PPCA_param}
The previously described PPCA is already a somewhat parsimonious statistical model. Indeed, it not only makes the assumption that the observed data follows a multivariate Gaussian distribution, which is the entropy-maximising distribution at a fixed mean and covariance, but it also reduces the number of covariance parameters by constraining the last $p-q$ eigenvalues to be equal.
The covariance matrix $\Sigma := \mathit{W} \mathit{W}\T + \sigma^2 \mathit{I}_p$ is parameterised by $\mathit{W} \in \R^{p\times q}$ and $\sigma^2$. It is shown in~\cite{tipping_probabilistic_1999} to have $\kappa := p q - \frac{q (q-1)}{2} + 1$ free parameters---the removal of $\frac{q (q-1)}{2}$ parameters being due to the invariance of the latent variable distribution to a rotation. Although not evident at first sight with this expression of $\kappa$, we have a drop of complexity---with respect to the full covariance model which is of dimension $\frac {p(p+1)}{2}$---due to the equality constraint on the low eigenvalues, and the number of parameters decreases along with $q$.
As shown in Subsection~\ref{subsec:SPCA_param}, we can give an insightful geometric interpretation to the number of free parameters in the PPCA model using Stiefel manifolds~\citep{edelman_geometry_1998}.

For a given data dimension $p$, a PPCA model is indexed by its latent variable dimension $q \in \lrb{0 \twodots p-1}$. The process of model selection then consists in comparing different PPCA models and choosing the one that optimises a criterion, like the BIC~\citep{schwarz_estimating_1978} or more PPCA-oriented ones~\citep{bishop_bayesian_1998, minka_automatic_2000}. They often rely on a tradeoff between goodness-of-fit (via maximum likelihood) and complexity (via the number of parameters), ponderated by the number of samples.

\subsection{Isotropic Probabilistic Principal Component Analysis}
IPPCA~\citep{bouveyron_intrinsic_2011} is an even more constrained covariance model with only two distinct eigenvalues. For $a > b$ and $\mathit{U} \in \R^{p \times q}$ such that $\mathit{U}\T \mathit{U} = \mathit{I}_q$, one defines it as
\begin{equation}
    \Sigma := \lrp{a - b} \mathit{U} \mathit{U}\T + b \mathit{I}_p.
\end{equation}
Such a parsimonious model is shown to be efficient in high-dimensional classification problems~\citep{bouveyron_robust_2009}.
The authors derive the maximum likelihood of such a model, which is highly related to the one of PPCA, where this time the $q$ first sample covariance eigenvalues are also averaged to fit the model. They also show that the maximum likelihood criterion alone is surprisingly asymptotically consistent for selecting the true intrinsic dimension under the assumptions of IPPCA.