\section{Other proofs}
\label{sec:appendix_heuristics}


\subsection{Proof of Proposition~\ref{prop:eigenvalue_averaging} (Eigenvalue equalisation)}\label{subsec:appendix_eigval_averaging}
We compare the BIC of the full covariance model $\boldsymbol{\gamma} = \lrp{1, \dots, 1}$ to the one of the equalised covariance model $\boldsymbol{\gamma}' = \lrp{1, \dots, 1, 2, 1, \dots 1}$ where the $j$-th eigenvalue has been equalised with the $j+1$-th. This boils down to studying the sign of the function $\Delta \BIC := \BIC({\boldsymbol{\gamma}}) - \BIC({\boldsymbol{\gamma}}')$. One gets
\begin{align}
\Delta \BIC &= p \frac{\ln n}{n} + \sum_{k=1}^p \ln \lambda_k - \lrp{p - 2} \frac{\ln n}{n} - \sum_{k \notin \lrs{j, j+1}} \ln \lambda_k - 2 \ln\lrp{\frac{\lambda_j + \lambda_{j+1}} 2}\\
&= 2\frac{\ln n}{n} + \ln \lambda_j + \ln \lambda_{j+1} - 2 \ln\lrp{\frac{\lambda_j + \lambda_{j+1}} 2}\\
&= 2\frac{\ln n}{n} + \ln \lambda_j + \ln \lrp{\lambda_j \lrp{1 - \delta_j}} - 2 \ln\lrp{\frac{\lambda_j \lrp{2 - \delta_j}} 2}\\
&= 2\frac{\ln n}{n} + \ln{\lrp{1 - \delta_j}} - 2 \ln\lrp{1 - \frac{\delta_j} 2}\\
&= 2\frac{\ln n}{n} - \ln\lrp{\frac{\lrp{1 - \frac{\delta_j} 2}^2}{1 - \delta_j}}.
\end{align}

\noindent Hence, one has
\begin{align}
\Delta \BIC = 0 &\iff \exp\lrp{2\frac{\ln n}{n}} = \frac{\lrp{1 - \frac{\delta_j} 2}^2}{1 - \delta_j}\\
&\iff \frac {\delta_j^2} 4 - \lrp{1 - \exp\lrp{2 \frac{\ln n} n}} \delta_j + 1 - \exp\lrp{2 \frac{\ln n} n} = 0.    
\end{align}
It is a polynomial equation whose positive solution is unique when $n \geq 1$ and is
\begin{equation}
\delta(n) :=  2 - 2 \exp\lrp{2 \frac{\ln n} n} + 2\sqrt{\exp\lrp{4 \frac{\ln n} n} - \exp\lrp{2 \frac{\ln n} n}}. 
\end{equation}



\subsection{Proof of Proposition~\ref{prop:hierarchical_heuristic} (Consistency of hierarchical clustering)}\label{subsec:appendix_heuristic_hierarchical}
Let us assume that the true generative model is stratified with type ${\boldsymbol{\gamma}} \in \mathcal{C}(p)$. 
We can then write the population covariance matrix as ${\mathit{\Sigma}} = \sum_{k=1}^{d} \ell_k {\mathit{Q}}_k {{\mathit{Q}}_k}\T$ with $~{\ell_1 > \dots > \ell_{d} > 0}$ and ${\mathit{Q}} := \lrb{{\mathit{Q}}_1|\dots|{\mathit{Q}}_{d}} \in \O(p)$. 
Let $n$ be the number of independent samples and ${\mathit{S}}_n := \sum_{j=1}^{p} \lambda_j({\mathit{S}}_n) {\boldsymbol{v}}_j({\mathit{S}}_n) {{\boldsymbol{v}}_j({\mathit{S}}_n)}\T$ with $\lambda_1 \geq \dots \geq \lambda_p$ and ${\mathit{V}} := \lrb{{\boldsymbol{v}}_1|\dots|{\boldsymbol{v}}_p} \in \O(p)$. 
According to~\cite[Proposition~1]{bouveyron_intrinsic_2011} and~\cite[Lemma~2.1~(i)]{tyler_asymptotic_1981}, one then has almost surely, as $n$ goes to infinity, $\lambda_j({\mathit{S}}_n) \to \ell_{\phi_{{\boldsymbol{\gamma}}}(j)}$, where $\phi_{{\boldsymbol{\gamma}}}$ is the ${\boldsymbol{\gamma}}$-composition function (cf. Subsection~\ref{subsec:type}). 
Hence for $n$ large enough, by continuity of the distance function $\Delta$, the gaps between eigenvalues in the same part of the ${\boldsymbol{\gamma}}$-composition will be arbitrarily close to $0$, while the other will be arbitrarily close to the true values $\lrs{\Delta\lrp{\ell_k, \ell_{k+1}}, k \in \lrb{1\twodots d-1}}$, which are all positive.
Hence the hierarchical clustering method will first agglomerate the eigenvalues that are in the same part of ${\boldsymbol{\gamma}}$, and second the distinct blocks, by increasing order of pairwise distance. The last model of the first phase will be exactly the true model. 

Note that one can thereafter perform model selection within the reduced subfamily of SPCA models obtained by the hierarchical clustering heuristic and asymptotically recover the true model using a consistent criterion.


\subsection{Proof of Proposition~\ref{prop:fixed_length_heuristic} (Consistency of maximum likelihood)}\label{subsec:appendix_heuristic_length}
Let us assume that the true generative model is stratified with type $~{{\boldsymbol{\gamma}}^*:=\lrp{\gamma_1^*, \dots, \gamma_d^*}}$, of length $d$, and let $\ell_1 > \dots > \ell_{d} > 0$ be the eigenvalues of the associated population covariance matrix.
Then, similarly as in the previous proof, almost surely, asymptotically, the sample covariance matrix eigenvalues are the ones of the population covariance matrix.
Hence, for any SPCA model of type ${\boldsymbol{\gamma}} := \lrp{\gamma_1, \dots, \gamma_d}$, the maximum likelihood writes
\begin{equation}
\ln{\hat{\mathcal{L}}} \sim -\frac n 2 \lrp{p \ln 2\pi + \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{j \in \phi_{{\boldsymbol{\gamma}}}^{-1} \lrs{k}} \ell_{\phi_{{\boldsymbol{\gamma}}^*}(j)}}}.
\end{equation}
As $n$ and $p$ are fixed when we compare the models, they do not intervene in the model selection. Hence, the search of the optimal model in terms of maximum likelihood boils down to the following problem 
\begin{equation}
    \argmin_{\substack{{\boldsymbol{\gamma}} \in \mathcal{C}(p)\\ \#{{\boldsymbol{\gamma}}}=d}}
    \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{j \in \phi_{{\boldsymbol{\gamma}}}^{-1} \lrs{k}} \ell_{\phi_{{\boldsymbol{\gamma}}^*}(j)}} := f({\boldsymbol{\gamma}}).
\end{equation}

\noindent One has $f({\boldsymbol{\gamma}}) = \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{k'=1}^{d}
c_{kk'} \ell_{k'}}$, where $c_{kk'}$ is the cardinal of the intersection of the $k$-th part of ${\boldsymbol{\gamma}}$ with the $k'$-th part of ${\boldsymbol{\gamma}}^*$.
Then, by definition, one has $\sum_{k'=1}^{d} c_{kk'} = \gamma_k$ and $\sum_{k=1}^{d} c_{kk'} = {\boldsymbol{\gamma}}^*_{k'}$. Hence, using Jensen's inequality,
\begin{equation}
f({\boldsymbol{\gamma}}) \geq \sum_{k=1}^{d} \gamma_k \lrp{\sum_{k'=1}^{d} \frac{c_{kk'}}{\gamma_k}\ln \ell_{k'}} = \sum_{k,k'=1}^{d} c_{kk'} \ln \ell_{k'} = \sum_{k'=1}^{d} {\boldsymbol{\gamma}}^*_{k'} \ln \ell_{k'} = f({\boldsymbol{\gamma}}^*).
\end{equation}
To conclude, asymptotically, ${\boldsymbol{\gamma}}^*$-SPCA is the most likely model. Hence, the maximum likelihood criterion alone finds the true model among the family of SPCA models with the same type length.