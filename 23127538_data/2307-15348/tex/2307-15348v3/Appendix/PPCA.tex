\section{Reminders on probabilistic principal component analysis}
Principal component analysis (PCA) is a ubiquitous tool in statistics, which however lacks a probabilistic formulation.
Such a framework can indeed be useful in a variety of contexts like decision-making, generative modeling, missing data and model selection.
The Probabilistic PCA model of~\citet{tipping_probabilistic_1999} circumvents this issue, and we describe it in this section.

\subsection{Model}
Let $\lrp{{x}_i}_{i=1}^n$ be a $p$-dimensional dataset and $q \in \lrb{0, p-1}$ a lower dimension. In PPCA, the observed data is assumed to stem from a $q$-dimensional latent variable via a linear-Gaussian model

\begin{equation}\label{appeq:PPCA_model}
    {x} = {W} {z} + {\mu} + {\epsilon},
\end{equation}
with ${z} \sim \N{0, {I}_q}$, ${W} \in \R^{p \times q}$, ${\mu} \in \R^p$, ${\epsilon} \sim \N{0, \sigma^2 {I}_p}$ and $\sigma^2 > 0$. 

Through classical probability theory, one can show that the observed data is modeled as following a multivariate Gaussian distribution
\begin{equation}
    {x} \sim \N{{\mu}, {W} {W}\T + \sigma^2 {I}_p}.
\end{equation}
An analysis of the covariance matrix reveals that the distribution is actually anisotropic on the first $q$ dimensions and isotropic on the remaining $p - q$ ones. Hence there is an implicit constraint on the covariance model of the data, which is that the lowest $p - q$ eigenvalues are assumed to be all equal.


\subsection{Maximum likelihood}
The PPCA model parameters are the shift ${\mu}$, the linear map ${W}$ and the noise factor $\sigma^2$. Let some observed dataset $\lrp{{x}_i}_{i=1}^n$, $\overline {{x}} := \frac 1 n \sum_{i=1}^n {x}_i$ its mean and $~{{S} := \sum_{j=1}^p \ell_j {v}_j {{v}_j}\T}$ its sample covariance matrix, with its eigenvalues $\ell_1 \geq \dots \geq \ell_p \geq 0$ and associated eigenvectors ${v}_1 \perp \dots \perp {v}_p$. One can explicitly infer the parameters that are the most likely to have generated these data using maximum likelihood estimation.
It is shown in the original PPCA paper that the most likely shift is the empirical mean, the most likely linear map is the composition of a scaling by the $q$ highest eigenvalues ${L}_q:=\diag{\ell_1, \dots, \ell_q}$ (up to the noise) and an orthogonal transformation by the associated $q$ eigenvectors ${V}_q:=\lrb{{{v}}_1|\dots|{{v}}_q}$, and finally the most likely noise factor is the average of the $p - q$ discarded eigenvalues
\begin{equation}\label{appeq:PPCA_ML}
\hat{{\mu}} = \overline {{x}} , \hspace*{15mm}
\hat{{W}} = {V}_q \lrp{{L}_q - \hat{\sigma}^2 {I}_q}^{\frac 1 2} , \hspace*{15mm}
\hat{\sigma}^2 = \frac 1 {p - q} \sum_{j=q+1}^p \ell_j.
\end{equation}
One can then easily express the maximum log-likelihood
\begin{equation}
    \ln \hat{\mathcal{L}}(q) := -\frac n 2 \lrp{p \ln(2\pi) + \sum_{j=1}^q \ln{\ell_j} + (p - q) \ln\lrp{\frac 1 {p - q} \sum_{j=q+1}^p \ell_j} + p}.
\end{equation}

\subsection{Parsimony and model selection}
The previously described PPCA is already a parsimonious statistical model. Indeed, it not only makes the assumption that the observed data follows a multivariate Gaussian distribution, which is the entropy-maximizing distribution at a fixed mean and covariance, but it also reduces the number of covariance parameters by constraining the last $p-q$ eigenvalues to be equal.
The covariance matrix $\Sigma := {W} {W}\T + \sigma^2 {I}_p$ is parameterized by ${W} \in \R^{p\times q}$ and $\sigma^2$. It is shown in the original PPCA paper to have $\kappa(q) := p q - \frac{q (q-1)}{2} + 1$ free parameters---the removal of $\frac{q (q-1)}{2}$ parameters being due to the rotational-invariance of the latent variable $z\in\R^q$. Although not evident at first sight with this expression of $\kappa$, we have a drop of complexity---with respect to the full covariance model which is of dimension $\frac {p(p+1)}{2}$---due to the equality constraint on the low eigenvalues, and the number of parameters decreases along with $q$.
As discussed in the next section, we can give an insightful geometric interpretation to the number of free parameters in the PPCA model using Stiefel manifolds.

For a given data dimension $p$, a PPCA model is indexed by its latent variable dimension $q \in \lrb{0, p-1}$. The process of model selection then consists in comparing different PPCA models and choosing the one that optimizes a criterion, like the Bayesian information criterion (BIC) or more PPCA-oriented ones like Bayesian PCA~\citep{bishop_bayesian_1998} or Minka's criterion~\cite{minka_automatic_2000}. They often rely on a tradeoff between goodness-of-fit (via maximum likelihood) and complexity (via the number of parameters), weighted by the number of samples.

\subsection{Isotropic PPCA}
Isotropic PPCA (IPPCA) is an even more constrained covariance model with only two distinct eigenvalues. For $a > b$ and ${U} \in \R^{p \times q}$ such that ${U}\T {U} = {I}_q$, one defines it as
\begin{equation}
    \Sigma := \lrp{a - b} {U} {U}\T + b {I}_p.
\end{equation}
Such a parsimonious model is shown to be efficient in high-dimensional classification problems~\cite{bouveyron_high-dimensional_2007}.
The authors derive the maximum likelihood of such a model, which is highly related to the one of PPCA, where this time the $q$ first sample covariance eigenvalues are also averaged to fit the model. They also show that the maximum likelihood criterion alone is surprisingly asymptotically consistent for selecting the true intrinsic dimension under the assumptions of IPPCA.