\section{Model selection}\label{appsec:MS}
As discussed previously, sample covariance matrices almost surely have distinct eigenvalues. This makes the full covariance model the most likely to have generated some observed data.
However, it does not mean that the true parameters---that are the eigenvectors and the eigenvalues---can be individually precisely inferred, especially in the small-data regime.
Hence, one can wonder if a covariance model with repeated eigenvalues and multidimensional eigenspaces would not be more robust.
The results of the previous section enable us to provide a possible answer, through PSA model selection. 
First, we study the inference of two adjacent eigenvalues and their associated eigenvectors. We show that when the relative eigengap is small and the number of samples is limited, one should prefer a PSA model with repeated eigenvalues---i.e. block-average the eigenvalues and gather the associated eigenvectors in a multidimensional eigenspace.
Second, to extend this result to more than two eigenvalues, we develop a general model selection framework based on the stratified structure of PSA models.


\subsection{Bayesian information criterion}
The Bayesian information criterion (BIC) is defined as 
\begin{equation}\label{appeq:BIC}
    \operatorname{BIC}(\gamma) = \kappa(\gamma) \ln n - 2 \ln \hat{\mathcal{L}}(\gamma),
\end{equation}
where $\kappa$ is the number of free parameters~\eqref{appeq:PSA_kappa} and $\ln \hat{\mathcal{L}}$ is the maximum log-likelihood~\eqref{appeq:PSA_ML}.
It is a widely-used model selection criterion, making a tradeoff between model complexity $\kappa$ and goodness-of-fit $\hat{\mathcal{L}}$. The formula results from an asymptotic approximation of the model evidence.
In this section, we use the BIC for PSA model selection. The model with lowest BIC is considered as the best model. In the two-eigenvalue case, we get an explicit criterion based on eigenvalue gaps to decide if we must assume that they are equal, and in the more general case, we propose efficient model comparison strategies. We also investigate other model selection criteria than the BIC for completeness in this section, and get similar conclusions.

\subsection{The two-eigenvalue case}
In order to better understand the dynamics of PSA model selection, we lead the experiment of quantifying the BIC variation induced by the equalization of two adjacent eigenvalues. More precisely and without loss of generality, we compare the BIC of a \emph{full covariance model} ${\gamma} = \lrp{1, \dots, 1}$ to the one of an \emph{equalized covariance model} ${\gamma}' = \lrp{1 \dots 1, 2, 1 \dots 1}$, where the eigenvalue $\lambda_j$ has multiplicity $2$.


\begin{theorem}
\label{appthm:releigengap}
Let $\lrp{x_i}_{i=1}^n$ be a $p$-dimensional dataset with $n$ samples, $\ell_j \geq \ell_{j+1}$ two adjacent sample eigenvalues and $\delta_j = \frac{\ell_{j} - \ell_{j+1}}{\ell_j}$ be their \emph{relative eigengap}. 
If
\begin{equation}\label{appeq:releigengap_BIC}
    \delta_j < 2\lrp{1 - n^{\frac 2 n} + n^{\frac 1 n}\sqrt{n^{\frac 2 n} - 1}},
\end{equation}
then the equalized covariance model has a lower BIC than the full one.
\end{theorem}
\begin{proof}
Since $n$ and $p$ are constant within model selection, the BIC can be rewritten (up to constant terms and factors) as
\begin{equation}\label{appeq:BIC_simpl}
\operatorname{BIC} (\gamma) := \lrp{d - \sum_{k=1}^d \frac{\gamma_k (\gamma_k - 1)} {2}} \frac{\ln{n}}{n} + \sum_{k=1}^d \gamma_k \ln{\overline{L_k}}.
\end{equation}
We compare the BIC of the full covariance model ${\gamma} = \lrp{1, \dots, 1}$ to the one of the equalized covariance model ${\gamma}' = \lrp{1, \dots, 1, 2, 1, \dots 1}$ where the $j$-th eigenvalue has been equalized with the $j+1$-th. This boils down to studying the sign of the function $\Delta \BIC = \BIC({{\gamma}}) - \BIC({{\gamma}}')$. One gets  % may
\begin{align}
\Delta \BIC &= p \frac{\ln n}{n} + \sum_{k=1}^p \ln \ell_k - \lrp{p - 2} \frac{\ln n}{n} - \sum_{k \notin \lrs{j, j+1}} \ln \ell_k - 2 \ln\lrp{\frac{\ell_j + \ell_{j+1}} 2},\\
&= 2\frac{\ln n}{n} + \ln \ell_j + \ln \ell_{j+1} - 2 \ln\lrp{\frac{\ell_j + \ell_{j+1}} 2},\\
&= 2\frac{\ln n}{n} + \ln \ell_j + \ln \lrp{\ell_j \lrp{1 - \delta_j}} - 2 \ln\lrp{\frac{\ell_j \lrp{2 - \delta_j}} 2},\\
&= 2\frac{\ln n}{n} + \ln{\lrp{1 - \delta_j}} - 2 \ln\lrp{1 - \frac{\delta_j} 2},\\
&= 2\frac{\ln n}{n} - \ln\lrp{\frac{\lrp{1 - \frac{\delta_j} 2}^2}{1 - \delta_j}}.
\end{align}
Hence, one has
\begin{equation}
\Delta \BIC = 0 \iff \exp\lrp{2\frac{\ln n}{n}} = \frac{\lrp{1 - \frac{\delta_j} 2}^2}{1 - \delta_j} \iff \frac {\delta_j^2} 4 - \lrp{1 - \exp\lrp{2 \frac{\ln n} n}} \delta_j + 1 - \exp\lrp{2 \frac{\ln n} n} = 0.    
\end{equation}
It is a polynomial equation whose positive solution is unique when $n \geq 1$ and is
\begin{equation}
\delta(n) =  2 - 2 \exp\lrp{2 \frac{\ln n} n} + 2\sqrt{\exp\lrp{4 \frac{\ln n} n} - \exp\lrp{2 \frac{\ln n} n}}.
\end{equation}
\end{proof}

\subsection{Comparison with North's rule-of-thumb}
A rule-of-thumb for determining which sample eigenvalue pairs might lead to large PC sampling error is proposed in~\citet{north_sampling_1982}.
The authors show that the asymptotic sampling error of a population eigenvalue $\lambda$ is $\Delta\lambda := \lambda(\frac2n)^{\frac12}$ in the Gaussian setting. North's rule-of-thumb (NRT) states that when one population eigenvalue's sampling error is comparable to or larger than its distance to an adjacent eigenvalue, then the PC's sampling error is comparable to the associated adjacent PC.
Note that this is not an explicit rule (compared to our relative eigengap threshold~\eqref{appeq:releigengap_BIC}) since one has to choose the level of uncertainty, and---most of all---it is based on the \textit{true} eigenvalues (on which the confidence intervals are based) which are unknown.
However, this rule has been applied in many contexts and it is commonly implemented in the following way~\cite{sinkr}. 
For each sample eigenvalue pair $\ell_{j} \geq \ell_{j+1}$, compute the 1 sigma error intervals $I_{j} = [\ell_{j} - \ell_{j}\sqrt{\frac2n}, \ell_{j} + \ell_{j}\sqrt{\frac2n}]$ and $I_{j+1} = [\ell_{j+1} - \ell_{j+1}\sqrt{\frac2n}, \ell_{j+1} + \ell_{j+1}\sqrt{\frac2n}]$. If $I_j\cap I_{j+1} \neq \emptyset$, then the associated principal components suffer from large sampling errors and might be random mixtures of the true eigenvectors. We reformulate it as a relative eigengap threshold.
\begin{proposition}\label{appprop:releigengap_North}
North's rule-of-thumb (as implemented in practice) boils down to the relative eigengap threshold
\begin{equation}\label{appeq:releigengap_North}
    \delta_j \leq \frac{2\sqrt{\frac2n}}{1+\sqrt{\frac2n}}.
\end{equation}
\end{proposition}
\begin{proof}
The sampling error interval overlap condition writes as
\begin{align}
    \ell_{j} - \sqrt{\frac2n} \ell_{j} \leq \ell_{j+1} + \sqrt{\frac2n} \ell_{j+1} & \iff \frac{\ell_j - \ell_{j+1}}{\ell_j} \leq \sqrt{\frac2n} \lrp{1 + \frac{\ell_{j+1}}{\ell_j}},\\
    & \iff \frac{\ell_j - \ell_{j+1}}{\ell_j} \leq \sqrt{\frac2n} \lrp{2 - \frac{\ell_j - \ell_{j+1}}{\ell_j}},\\
    & \iff \frac{\ell_j - \ell_{j+1}}{\ell_j} \leq \frac{2 \sqrt{\frac2n}}{1 + \sqrt{\frac2n}}.
\end{align}
\end{proof}
\noindent This threshold is reported in Fig.~\ref{appfig:releigengap_curves}, under the name NRT-1 (for 1 sigma sampling errors). We also report North's rule-of-thumb for 2 sigma sampling errors (NRT-2), yielding a relative eigengap threshold of $\frac{4 \sqrt{\frac2n}}{1 + 2\sqrt{\frac2n}}$.
% Figure environment removed
We see that the relative eigengap NRT-1 is much smaller than ours (e.g. $8.6\%$ instead of $21\%$ for $1000$ samples). Therefore, although warning scientists about close sample eigenvalues in principal component analysis, North's rule-of-thumb largely overlooks the curse of isotropy compared to our method.
To see the practical effect of this lower threshold, we test this condition on the same real datasets as in Fig.~\ref{fig:releigengap_UCI}. The results are in Fig.~\ref{appfig:releigengaps_real}.
% Figure environment removed
We can see that the curse of isotropy remains a nonnegligible phenomenon with North's rule, even though it is less marked than with the BIC.
We think that North's rule (as implemented in practice) underestimates the phenomenon, notably because it uses 1 sigma uncertainties and since it is based on sample eigenvalues instead of true eigenvalues in the implementations. We recall that 1 sigma uncertainties (NRT-1) correspond to $68\%$ error bars while 2 sigma uncertainties (NRT-2) correspond to $95\%$ error bars and yield a relative eigengap threshold of $16\%$, which is much closer to our results with the BIC.
An interesting perspective would be to consider our guideline instead of the less-impactful North's rule in seminal climate science papers which made some conclusions out of possibly degenerate principal components.


\subsection{Comparison with other model selection criteria}
Although being widely used in model selection, the BIC is well-known for its heavy complexity penalization, tending to select over-parsimonious models~\cite{burnham_model_2004}. Another widely-used criterion is the Akaike information criterion~\cite{akaike_new_1974}. It is defined as
\begin{equation}\label{appeq:AIC}
    \mathrm{AIC}(\gamma) = 2 \kappa(\gamma) - 2 \ln \hat{\mathcal{L}}(\gamma)
\end{equation}
where $\kappa$ is the number of free parameters~\eqref{appeq:PSA_kappa} and $\ln \hat{\mathcal{L}}$ is the maximum log-likelihood~\eqref{appeq:PSA_ML}.
Comparing an equalized covariance model to one with distinct eigenvalues like in Thm.~\ref{appthm:releigengap} but this time using the AIC yields another relative eigengap condition.
\begin{proposition}\label{appprop:releigengap_AIC}
Let $\lrp{x_i}_{i=1}^n$ be a $p$-dimensional dataset with $n$ samples, $\ell_j \geq \ell_{j+1}$ two adjacent sample eigenvalues and $\delta_j = \frac{\ell_{j} - \ell_{j+1}}{\ell_j}$ their \emph{relative eigengap}. 
If
\begin{equation}\label{appeq:releigengap_AIC}
    \delta_j < 2\lrp{1 - e^{\frac4n} + e^{\frac2n}\sqrt{e^{\frac4n} - 1}}
\end{equation} 
then the equalized covariance model has a lower AIC than the full one.
\end{proposition}
\begin{proof}
The proof is essentially the same as the one of Thm.~\ref{appthm:releigengap}.
Since $n$ and $p$ are constant within model selection, the AIC can be rewritten (up to constant terms and factors) as
\begin{equation}\label{appeq:AIC_simpl}
\operatorname{AIC} (\gamma) := \lrp{d - \sum_{k=1}^d \frac{\gamma_k (\gamma_k - 1)} {2}} \frac{2}{n} + \sum_{k=1}^d \gamma_k \ln{\overline{L_k}}
\end{equation}
Replacing $\frac{\ln n}{n}$ with $\frac{2}{n}$ in the proof of Thm.~\ref{appthm:releigengap}, we finally get the result that $\delta(n) =  2 - 2 \exp\lrp{\frac{4} n} + 2\sqrt{\exp\lrp{\frac{8} n} - \exp\lrp{\frac{4} n}}$.
\end{proof}
\noindent This threshold is reported in Fig.~\ref{appfig:releigengap_curves}. We see that this relative eigengap is smaller than ours~\eqref{appeq:releigengap_BIC} (e.g. $12\%$ instead of $21\%$ for $1000$ samples), but higher than North's rule~\eqref{appeq:releigengap_North}. This result is interesting since AIC is known for tending to select overparameterized models, especially for small sample sizes~\cite{burnham_model_2004} (cf. next paragraph). 
Despite this, the relative eigengap condition with AIC is more impactful than North's rule.
To see the practical effect of the AIC threshold of~\eqref{appeq:releigengap_AIC}, we also report the relative eigengap condition on real datasets in Fig.~\ref{appfig:releigengaps_real}.
We see that many eigenvalue pairs should be assumed equal---slightly less than with BIC. Therefore, even with another model selection criterion, the curse of isotropy is still a nonnegligible phenomenon in real datasets, and the principal subspace analysis methodology enables to leverage it to improve interpretability.


Additionally, we provide a relative eigengap condition for the AICc~\cite{hurvich_regression_1989}, which is a small-sample correction to the AIC. In practice, the AICc is advised over the AIC for $n/\kappa < 40$~\cite{burnham_model_2004}.
The AICc is defined as
\begin{equation}\label{appeq:AICc}
    \mathrm{AICc}(\gamma) = 2 \kappa(\gamma) {\frac{n}{n-\kappa(\gamma)-1}} - 2 \ln \hat{\mathcal{L}}(\gamma)
\end{equation}
where $\kappa$ is the number of free parameters~\eqref{appeq:PSA_kappa} and $\ln \hat{\mathcal{L}}$ is the maximum log-likelihood~\eqref{appeq:PSA_ML}.
One can see that this corrected criterion converges asymptotically to the AIC. 
Comparing an equalized covariance model to one with distinct eigenvalues like in Thm.~\ref{appthm:releigengap} but this time with the AICc yields the following relative eigengap condition.
\begin{proposition}\label{appprop:releigengap_AICc}
Let $\lrp{x_i}_{i=1}^n$ be a $p$-dimensional dataset with $n > \frac{p(p+3)}{2} + 1$ samples, $\ell_j \geq \ell_{j+1}$ two adjacent sample eigenvalues, $\delta_j = \frac{\ell_{j} - \ell_{j+1}}{\ell_j}$ their \emph{relative eigengap} and $\varphi = \frac{4n-4}{\lrp{n - \frac{p(p+3)}{2}}^2 - 1}$.
If
\begin{equation}\label{appeq:releigengap_AICc}
    \delta_j < 2\lrp{1 - e^{\varphi} + e^{\frac{\varphi}{2}}\sqrt{e^{\varphi} - 1}}
\end{equation} 
then the equalized covariance model has a lower AICc than the full one.
\end{proposition}
\begin{proof}
The proof is essentially the same as in Thm~\ref{appthm:releigengap} and Prop.~\ref{appprop:releigengap_AIC}.
Since $n$ and $p$ are constant within model selection, the AICc can be rewritten (up to constant terms and factors) as
\begin{equation}\label{appeq:AICc_simpl}
\operatorname{AICc} (\gamma) := \frac{2\kappa(\gamma)}{n - \kappa(\gamma) - 1} + \sum_{k=1}^d \gamma_k \ln{\overline{L_k}}
\end{equation}
We compare the AICc of the full covariance model ${\gamma} = \lrp{1, \dots, 1}$ to the one of the equalized covariance model ${\gamma}' = \lrp{1, \dots, 1, 2, 1, \dots 1}$ where the $j$-th eigenvalue has been equalized with the $j+1$-th. This boils down to studying the sign of the function $\Delta \operatorname{AICc} = \operatorname{AICc}({{\gamma}}) - \operatorname{AICc}({{\gamma}}')$. One gets
\begin{align}
\Delta \operatorname{AICc} &= \frac{p(p+3)}{n - \frac{p(p+3)}{2} - 1} - \frac{p(p+3) - 4}{n - \lrp{\frac{p(p+3)}{2} - 2} - 1} + \ln \ell_j + \ln \ell_{j+1} - 2 \ln\lrp{\frac{\ell_j + \ell_{j+1}} 2}\\
&= \frac{4n-4}{\lrp{n-\frac{p(p+3)}{2}}^2 - 1} + \ln \ell_j + \ln \ell_{j+1} - 2 \ln\lrp{\frac{\ell_j + \ell_{j+1}} 2}
\end{align}
Replacing $2\frac{\ln n}{n}$ with $\varphi = \frac{4n-4}{\lrp{n-\frac{p(p+3)}{2}}^2 - 1}$ in the proof of Thm.~\ref{appthm:releigengap}, we finally get the result that $\delta(n) =  2 - 2 \exp\lrp{\varphi} + 2\sqrt{\exp\lrp{2\varphi} - \exp\lrp{\varphi}}$.
\end{proof}
\noindent Contrary to the other criteria (Thm~\ref{appthm:releigengap}, Prop.~\ref{appprop:releigengap_North} and Prop.~\ref{appprop:releigengap_AIC}), this threshold depends on the dimension $p$. Therefore, we plot it for several $p$ in Fig.~\ref{appfig:releigengap_curves}. 
We can see that this relative eigengap converges to the AIC for large $n$, but is higher than the one with the BIC~\eqref{appeq:releigengap_BIC} when the number of samples is close to the number of model parameters.
We also test this condition on the same real datasets as in Fig.~\ref{fig:releigengap_UCI} and report the results in Fig.~\ref{appfig:releigengaps_real}.
We see that many eigenvalue pairs are ill-defined, especially in high-dimensional datasets where those are even more numerous than with the BIC.


\subsection{Efficient model selection}
Given a dimension $p$, PPCA has $p$ models, ranging from the isotropic Gaussian ($q=0$) to the full covariance model ($q=p-1$). We can naturally equip the set of PPCA models with the  \emph{less-than-or-equal} relation $\leq$ on the latent variable dimension $q$, which makes it a totally ordered set. The complexity of the model then increases with $q$.

The characterization of the PSA family structure is a bit more technical, as it requires to study the hierarchy of types, involving the concept of integer composition. Fortunately, this analysis can be lifted to the stratification of symmetric matrices according to the multiplicities of the eigenvalues, which is already well-known~\cite{arnold_modes_1972,groisser_geometric_2017,breiding_geometry_2018}. Therefore, without proof, we can state the following result.

\begin{proposition}\label{appprop:pos}
The family of $p$-dimensional PSA models induces a stratification of the space of symmetric positive-definite (SPD) matrices $S_p^{++}$ according to the type ${\gamma}$.
The refinement relation $\preceq$ makes it a partially ordered set of cardinal $2^{p-1}$.
\end{proposition}

\noindent Hence the set of PSA models at a given data dimension can be represented using a Hasse diagram, as done in Fig.~\ref{fig:hasse_complexity}.
We see that PSA contains PPCA, IPPCA, and many new models. 
PSA therefore has the advantage of possibly providing more adapted models than PPCA and IPPCA, but also the drawback of requiring more comparisons for model selection. 
In high dimension this becomes quickly computationally heavy, therefore we need to define strategies for selecting only a few number of models to compare. The previously derived partial order $\preceq$ on the set of PSA models allows simple efficient strategies for model selection. In the following subsubsections, we detail those strategies and prove additional properties.

\subsubsection{Relative eigengap threshold clustering of eigenvalues}
The \textit{relative eigengap threshold strategy} consists in clustering the eigenvalues whose relative eigengap $\delta_j := \frac{\ell_{j} - \ell_{j+1}}{\ell_j}$ is below a given threshold, e.g. the one of Thm.~\ref{appthm:releigengap}. This clustering uniquely determines a PSA type $\gamma$, from which we apply maximum likelihood estimation, i.e. we block-average the corresponding eigenvalue clusters.
This rule is extremely simple but it may select overly parsimonious models, since distant eigenvalues may end up in the same cluster by propagation. Therefore, we provide a more-advanced strategy in the following subsubsection.


\subsubsection{Hierarchical clustering of eigenvalues}
In this strategy, the subset of candidate models is generated by the \emph{hierarchical clustering} of the sample eigenvalues. The general principle of hierarchical clustering is to agglomerate one by one the eigenvalues into clusters, thanks to a so-called \emph{cluster-linkage criterion}, which is a measure of dissimilarity between clusters.
More precisely, here we choose a \textit{continuous} pairwise distance $\delta$ between adjacent eigenvalues (such as the relative eigengap defined in Thm.~\ref{appthm:releigengap}), and a linkage criterion $\Delta$ between eigenvalue clusters, making sense with respect to our model selection problem (such as the single-linkage criterion $\Delta(\Lambda_1, \Lambda_2) = \min_{\ell_1, \ell_2 \in \Lambda_1 \times \Lambda_2} \delta(\ell_1, \ell_2)$ or the centroid-linkage criterion $\Delta(\Lambda_1, \Lambda_2) = \delta(\overline{\Lambda_1}, \overline{\Lambda_2})$). 
The method is detailed in Algorithm~\ref{appalg:hierarchical} and illustrated in Figure~\ref{appfig:hierarchical_clustering}.
\begin{algorithm}[H]
   \caption{Hierarchical clustering strategy for PSA model selection}
   \label{appalg:hierarchical}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\ell_1 \geq \dots \geq \ell_p, \Delta$ \hfill sample eigenvalues and distance
   \STATE {\bfseries Output:} $\lrp{{\gamma}^{t}}_{t=1}^p$ \hfill hierarchical subfamily of PSA models
   \STATE ${\gamma}^{1} \gets \lrp{1, \dots, 1}, \quad {{\Lambda}}^{1} \gets \lrp{\lrs{\ell_1}, \dots, \lrs{\ell_p}}$ \hfill initialize with full covariance model
   \FOR{$t = 1 \dots p-1$}
    \STATE $\Delta^{t} \gets \lrp{\Delta({\Lambda}^{t}_1, {\Lambda}^{t}_{2}), \dots, \Delta({\Lambda}^{t}_{p-t}, {\Lambda}^{t}_{p-t+1})}$ \hfill {compute distances between adjacent clusters}
    \STATE $k^{t} \gets \argmin \Delta^{t}$ \hfill {find clusters with minimal distance}
    \STATE ${{\Lambda}}^{t+1} \gets ({\Lambda}^t_1, \dots, {\Lambda}^t_{{k^t}-1}, {\Lambda}^t_{k^t} \cup {\Lambda}^t_{{k^t}+1}, {\Lambda}^t_{{k^t}+2}, \dots, {\Lambda}^t_d)$ % \overline{{{\lambda}}^{{\gamma}^{t+1}}}$
    \hfill {merge the two clusters of eigenvalues}
    \STATE ${\gamma}^{t+1} \gets ({\gamma}^t_1, \dots, {\gamma}^t_{{k^t}-1}, {\gamma}^t_{k^t} + {\gamma}^t_{{k^t}+1}, {\gamma}^t_{{k^t}+2}, \dots, {\gamma}^t_d)$ \hfill {update the model type}
\ENDFOR
\end{algorithmic}
\end{algorithm}
% Figure environment removed
\noindent The hierarchical clustering strategy creates a \emph{trajectory} $({\gamma}^t)_{t=1}^p$ in the Hasse diagram of PSA models (cf. Fig.~\ref{fig:hasse_complexity}). The sequence starts from ${\gamma}^1 = \lrp{1, \dots, 1}$, the full covariance model, in which each eigenvalue is in its own cluster. Then, one by one, the eigenvalues that are the closest in terms of distance $\Delta$ are agglomerated, and the inter-cluster distances are updated. The algorithm ends when one reaches the isotropic covariance model, ${\gamma}^p = \lrp{p}$, in which all the eigenvalues are in the same cluster. This corresponds to an \textit{agglomerative} approach in the hierarchical clustering vocabulary, in opposition to a \textit{divisive} approach, that we could similarly develop for this strategy.

The hierarchical clustering strategy hence generates a subfamily of $p$ models that can be then compared within a classical model selection framework. In order to assess the quality of such a strategy, we show the following consistency result.

\begin{proposition}[Asymptotic consistency of the hierarchical clustering strategy]\label{appprop:hierarchical_heuristic}
The hierarchical clustering strategy generates a subfamily of PSA models that almost surely contains the true PSA model for $n$ large enough.
\end{proposition}
\begin{proof}
Let us assume that the true generative model is stratified with type ${{\gamma}} \in \mathcal{C}(p)$. 
We can then write the population covariance matrix as ${{\Sigma}} = \sum_{k=1}^{d} \lambda_k {{Q}}_k {{{Q}}_k}\T$ with $~{\lambda_1 > \dots > \lambda_{d} > 0}$ and ${{Q}} := \lrb{{{Q}}_1|\dots|{{Q}}_{d}} \in \O(p)$. 
Let $n$ be the number of independent samples and ${{S}}_n := \sum_{j=1}^{p} \ell_j({{S}}_n) {{v}}_j({{S}}_n) {{{v}}_j({{S}}_n)}\T$ with $\ell_1 \geq \dots \geq \ell_p$ and ${{V}} := \lrb{{{v}}_1|\dots|{{v}}_p} \in \O(p)$. 
According to Tyler (1981), Lemma~2.1~(i), one then has almost surely, as $n$ goes to infinity, $\ell_j({{S}}_n) \to \lambda_{\phi_{{{\gamma}}}(j)}$, where $\phi_{{{\gamma}}}$ is the ${{\gamma}}$-composition function.
Hence for $n$ large enough, by continuity of the distance function $\Delta$, the gaps between eigenvalues in the same part of the ${{\gamma}}$-composition will be arbitrarily close to $0$, while the other will be arbitrarily close to the true values $\lrs{\Delta\lrp{\lambda_k, \lambda_{k+1}}, k \in \lrb{1, d-1}}$, which are all positive.
Hence the hierarchical clustering method will first agglomerate the eigenvalues that are in the same part of ${{\gamma}}$, and second the distinct blocks, by increasing order of pairwise distance. The last model of the first phase will be exactly the true model.
\end{proof}


\noindent Hence, the hierarchical clustering strategy generates a hierarchical subfamily of models of decreasing complexities, including the true PSA model for $n$ large enough. The true model can be then recovered using asymptotically consistent model selection criteria on the subfamily.
We now propose a second strategy that is not hierarchical but instead makes a prior assumption on the model complexity and then selects the one that has the maximum likelihood among all the candidates.

\subsubsection{Prior on the number of distinct eigenvalues}
In this strategy, we perform model selection at a given level of the Hasse diagram (cf. Fig.~\ref{fig:hasse_complexity}). More precisely, we consider as candidates only the models that have a given type length $d$, like done in IPPCA with $d=2$.
The type-length prior strategy reduces the search space like the previous strategy, this time to $\binom{p-1}{d-1}$ models. In contrast to the hierarchical clustering strategy which creates a hierarchy of models with decreasing complexity, we here rather fix the complexity range of the candidate models, by working on one floor of the Hasse diagram, and then try to find the model of best fit.

Just like in the hierarchical clustering strategy, we could use the BIC to choose the best model among this reduced family.
For completeness, we provide an additional criterion that is nothing but the maximum likelihood itself. 
We indeed manage to extend to PSA the surprising result from~\citet{bouveyron_intrinsic_2011} stating that the maximum likelihood criterion alone asymptotically consistently finds the true intrinsic dimension within the IPPCA setting. 
Intuitively, this can be explained by the fact that we a priori fix the complexity of the candidate models and therefore we can focus on the other side of the weighing scale that is the goodness of fit.
As this criterion empirically yields competitive results with respect to other classical model selection criteria in the large sample, low signal-to-noise ratio regime, we expect it to be of interest in PSA as well.
\begin{proposition}[Asymptotic consistency of the maximum likelihood for fixed $d$]\label{appprop:fixed_length_heuristic}
If the true PSA model has $d$ distinct eigenvalues, then maximum likelihood model selection within the subfamily of PSA models of type-length $d$ almost surely recovers the true model for $n$ large enough.
\end{proposition}
\begin{proof}
Let us assume that the true generative model is stratified with type $~{{{\gamma}}^*:=\lrp{\gamma_1^*, \dots, \gamma_d^*}}$, of length $d$, and let $\lambda_1 > \dots > \lambda_{d} > 0$ be the eigenvalues of the associated population covariance matrix.
Then, similarly as in the previous proof, almost surely, asymptotically, the sample covariance matrix eigenvalues are the ones of the population covariance matrix.
Hence, for any PSA model of type ${{\gamma}} := \lrp{\gamma_1, \dots, \gamma_d}$, the maximum likelihood writes
\begin{equation}
\ln{\hat{\mathcal{L}}} \sim -\frac n 2 \lrp{p \ln 2\pi + \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{j \in \phi_{{{\gamma}}}^{-1} \lrs{k}} \lambda_{\phi_{{{\gamma}}^*}(j)}}}.
\end{equation}
As $n$ and $p$ are fixed when we compare the models, they do not intervene in the model selection. Hence, the search of the optimal model in terms of maximum likelihood boils down to the following problem 
\begin{equation}
    \argmin_{\substack{{{\gamma}} \in \mathcal{C}(p)\\ \#{{{\gamma}}}=d}}
    \sum_{k=1}^{d} \gamma_k \ln \lrp{\frac{1}{\gamma_k}\sum_{j \in \phi_{{{\gamma}}}^{-1} \lrs{k}} \lambda_{\phi_{{{\gamma}}^*}(j)}} := f({{\gamma}}).
\end{equation}
One has $f({{\gamma}}) = \sum_{k=1}^{d} \gamma_k \ln (\frac{1}{\gamma_k}\sum_{k'=1}^{d}
c_{kk'} \lambda_{k'})$, where $c_{kk'}$ is the cardinal of the intersection of the $k$-th part of ${{\gamma}}$ with the $k'$-th part of ${{\gamma}}^*$.
Then, by definition, one has $\sum_{k'=1}^{d} c_{kk'} = \gamma_k$ and $\sum_{k=1}^{d} c_{kk'} = {{\gamma}}^*_{k'}$. Hence, using Jensen's inequality,
\begin{equation}
f({{\gamma}}) \geq \sum_{k=1}^{d} \gamma_k \lrp{\sum_{k'=1}^{d} \frac{c_{kk'}}{\gamma_k}\ln \lambda_{k'}} = \sum_{k,k'=1}^{d} c_{kk'} \ln \lambda_{k'} = \sum_{k'=1}^{d} {{\gamma}}^*_{k'} \ln \lambda_{k'} = f({{\gamma}}^*).
\end{equation}
To conclude, asymptotically, ${{\gamma}}^*$-PSA is the most likely model. Hence, the maximum likelihood criterion alone finds the true model among the family of PSA models with the same type length.
\end{proof}


\noindent Hence we derived three simple strategies for model selection, taking into account the structure of the PSA models family. 
\begin{remark}
Many variants can be adopted depending on the problem at hand. For instance if the noise is known, or assumed with some explained variance ratio rules, one can first search for the associated intrinsic dimension $q$ like in classical PCA, and then try to equalize some of the $q$ first eigenvalues by optimizing the model selection criterion over the subfamily of models whose $p - q$ last eigenvalues are all equal.
\end{remark}
\begin{remark}
In high dimensions, some eigenvalues might be very small or even null. The case of small positive eigenvalues may yield high relative eigengaps in the last eigenvalue pairs---therefore PSA model selection tends to separate those eigenvalues---whereas those are traditionally considered as noise. The case of null eigenvalues yields undefined PSA models. To circumvent those two issues, a classical trick is the one of \textit{covariance regularization}, consisting in adding a small constant to all the covariance eigenvalues. This somewhat boils down to adding an isotropic Gaussian noise to the data. This has notably the effect of diminishing the relative eigengaps, especially for the small positive or null eigenvalues. Another possibility would be to constrain the model types to have at least the last $p - q$ eigenvalues equal, where $q$ is chosen sufficiently small such that the first $q$ eigenvalues are large enough. This is left for future research.
\end{remark}