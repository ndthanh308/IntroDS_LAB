\section{Principal subspace analysis}\label{appsec:PSA}
Inspired by the complexity drop induced by the isotropy in the noise space in PPCA, we aim at investigating more general isotropy constraints on the full data space.
In this section, we introduce PSA, a covariance generative model with a general constraint on the sequence of eigenvalue multiplicities. PSA generalizes PPCA and IPPCA and unifies them in a new family of models parameterized by flag manifolds. Flag manifolds are themselves generalizations of Stiefel manifolds and Grassmannians, hence the link between PPCA, IPPCA and PSA that is detailed in this section.


\subsection{Model}
We recall that in combinatorics, a \emph{composition} of an integer $p$ is an ordered sequence of positive integers that sums up to $p$.
It has to be distinguished from
a \emph{partition} of an integer, which doesn't take into account the ordering of the parts. 

Let ${\gamma} := (\gamma_1, \gamma_2, \dots, \gamma_d) \in \mathcal{C}(p)$ be a composition of a positive integer $p$.
We define the PSA model of \emph{type} ${\gamma}$ as
\begin{equation}\label{appeq:PSA_model}
{x} = \sum_{k=1}^{d-1} \sigma_k {Q}_k {z}_k + {\mu} + {\epsilon}.
\end{equation}
In this formula, $~{\sigma_1 > \dots > \sigma_{d-1} > 0}$ are decreasing scaling factors,
${Q}_k \in \R^{p \times \gamma_k}$ are mutually-orthogonal frames (i.e. they verify ${{Q}_k}\T {Q}_{k'} = \delta_{k k'} {I}$ in Kronecker notation)
and $~{{z}_k \sim \N{{0}, {I}_{\gamma_k}}}$ are independent latent variables.
${\mu} \in \R^p$, $\sigma^2 > 0$ and $~{{\epsilon} \sim \N{{0}, \sigma^2 {I}_{p}}}$ are the classical shift, variance and isotropic noise present in PPCA.

Similarly as in PPCA, we can compute the population density
\begin{equation}
    {x} \sim \N{{\mu}, \hspace{1mm} \sum_{k=1}^{d-1} {\sigma_k}^2 {Q}_k {{Q}_k}\T + \sigma^2 {I}_p}.
\end{equation}
The expression of the covariance matrix $~{{\Sigma} := \sum_k {\sigma_k}^2 {Q}_k {{Q}_k}\T + \sigma^2 {I}_p \in \R^{p\times p}}$ can be simplified by gathering all the orthonormal frames into one orthogonal matrix $~{{Q} := \lrb{{Q}_1 | \dots | {Q}_{d-1} | {Q}_d} \in \O(p)}$ where ${Q}_d \in \R^{p \times \gamma_d}$ is an orthogonal completion of the previous frames. 
Writing ${\Lambda} := \diag{\lambda_1 {I}_{\gamma_1}, \dots, \lambda_d {I}_{\gamma_{d}}}$, with 
$\lambda_k := {\sigma_k}^2 + \sigma^2$ for $k\in \lrb{1, d-1}$ and $\lambda_d := \sigma^2$, one gets
\begin{equation}\label{appeq:cov_simpl}
    {\Sigma} = {Q} {\Lambda} {Q}\T.
\end{equation}
Hence, the fitted density of PSA is a multivariate Gaussian with repeated eigenvalues $~{\lambda_1 > \dots > \lambda_d > 0}$ of respective multiplicity $~{\gamma_1, \dots, \gamma_d}$. 
An illustration of the generative model is provided in Fig.~\ref{fig:PSA}.
Therefore, PPCA and IPPCA can be seen as PSA models, with respective types $~{{\gamma} = (1, \dots, 1, p - q)}$ and ${\gamma} = (q, p - q)$.
From a geometric point of view, the fitted density is isotropic on the  eigenspaces of ${\Sigma}$, which constitute 
a sequence of mutually-orthogonal subspaces of respective dimension $\gamma_1, \dots, \gamma_d$, whose direct sum generates the data space.
Such a sequence is called a \emph{flag} of linear subspaces of \emph{type} ${\gamma}$.
Hence flags are natural objects to geometrically interpret PSA, and so a fortiori PPCA and IPPCA. We detail this point in the next section.


\subsection{Type}
Just like the latent variable dimension $q \in \lrb{0, p-1}$ is a central notion in PPCA, the type ${\gamma} \in \mathcal{C}(p)$ is a central notion in PSA. In this subsection, we introduce the concepts of \emph{refinement} and \emph{${\gamma}$-composition} to make its analysis more convenient.

Let ${\gamma} := (\gamma_1, \gamma_2, \dots, \gamma_d) \in \mathcal{C}(p)$. 
We say that ${\gamma}' \in \mathcal{C}(p)$ is a \emph{refinement} of ${\gamma}$, and note ${\gamma} \preceq {\gamma}'$, if we can write ${\gamma}' := ({\gamma}'_1, {\gamma}'_2, \dots, {\gamma}'_d)$, with ${\gamma}'_k \in \mathcal{C}(\gamma_k), \forall k \in \lrb{1, d}$. For instance, one has $(2, 3) \preceq (1, 1, 2, 1)$, while $(2, 3) \npreceq (3, 2)$ and $(3, 2) \npreceq (2, 3)$.

Let ${\gamma} := (\gamma_1, \gamma_2, \dots, \gamma_d) \in \mathcal{C}(p)$. 
Then each integer between $1$ and $p$ can be uniquely assigned a \emph{part} of the composition, indexed between $1$ and $d$. 
We define the \emph{$\gamma$-composition function} $\phi_{{\gamma}}\colon \lrb{1, p} \to \lrb{1, d}$ to be this surjective map, such that $\phi_{{\gamma}}(j)$ is the index $k$ of the part the integer $j$ belongs to. 
For instance, one has $~{\phi_{(2, 3)}(1) = \phi_{(2, 3)}(2) = 1}$ and $~{\phi_{(2, 3)}(3) =  \phi_{(2, 3)}(4) = \phi_{(2, 3)}(5) = 2}$.
Then, intuitively and with slight abuse of notation, each object of size $p$ can be partitioned into $d$ sub-objects of respective size $\gamma_k$, for $k \in \lrb{1, d}$. 
We call it the \emph{${\gamma}$-composition} of an object. We give two examples. 
Let ${Q} \in \O(p)$. The {${\gamma}$-composition} of ${Q}$ is the sequence ${Q}^{{\gamma}} := \lrp{{Q}_1, \dots, {Q}_d}$ such that $~{{Q}_k \in \R^{p\times\gamma_k}, \forall k \in \lrb{1, d}}$ and $~{{Q} = \lrb{{Q}_1 | \dots | {Q}_d}}$.
Let ${L} := \lrp{\ell_1, \dots, \ell_p}$ be a sequence of decreasing eigenvalues. The {${\gamma}$-composition} of ${L}$ is the sequence  ${{L}}^{{\gamma}} := \lrp{{{L}}_1, \dots, {{L}}_d}$ such that $~{{{L}}_k \in \R^{\gamma_k}}, ~{\forall k \in \lrb{1, d}}$ and ${L} = \lrb{{{L}}_1 |  \dots | {{L}}_d}$. 
We call \emph{${\gamma}$-averaging} of ${L}$ the sequence $\overline{{{L}}^{{\gamma}}} := \lrp{\overline{{{L}}_1},  \dots, \overline{{{L}}_d}}\in \R^d$ of average eigenvalues in ${{L}}^{{\gamma}}$.


\subsection{Maximum likelihood}
Similarly as for PPCA, the log-likelihood of the model can be easily computed
\begin{equation}\label{appeq:PSA_LL}
\ln {{\mathcal{L}}} \lrp{{\mu}, {\Sigma}} = -\frac n 2 \lrp{p \ln(2\pi) + \ln |{\Sigma}| + \tr{{\Sigma}^{-1} {{C}}}},
\end{equation}
with
${{C}} = \frac 1 n \sum_{i=1}^n ({x}_i - {\mu}) {({x}_i - {\mu})}\T$.
We now show that the maximum likelihood estimate for PSA consists in the eigenvalue decomposition of the sample covariance matrix followed by a block-averaging of adjacent eigenvalues such that the imposed type ${\gamma}$ is respected; in other words, a ${\gamma}$-averaging of the eigenvalues.
Before that, let us naturally extend the notion of \emph{type} to symmetric matrices, as the sequence of multiplicities of its ordered-descending-eigenvalues.
\begin{theorem}
\label{appthm:PSA}
Let $\lrp{x_i}_{i=1}^n$ be a p-dimensional dataset, $\overline {{x}} := \frac 1 n \sum_{i=1}^n x_i$ its mean and $~{{S} := \sum_{j=1}^p \ell_j {v}_j {{v}_j}\T}$ its sample covariance matrix, with $~{\ell_1 \geq \dots \geq \ell_p \geq 0}$ its eigenvalues and $\lrb{{v}_1 | \dots | {v}_p} := {V} \in \O(p)$ some associated eigenvectors. 

\noindent The maximum likelihood parameters of PSA are
\begin{equation}
    \hat {{\mu}} = \overline {{x}}, \hspace{15mm}
    \hat {{Q}} = {{V}}, \hspace{15mm}
    \lrp{\hat{\lambda}_1, \dots, \hat{\lambda}_d} = \overline{{\lrp{\ell_1, \dots, \ell_p}}^{{\gamma}}}.
\end{equation}

\noindent The parameters $\hat {{\mu}}$ and $\hat{\lambda}_1, \dots, \hat{\lambda}_d$ are unique. $\hat {{Q}}$ is not unique but the flag of linear subspaces generated by its ${\gamma}$-composition almost surely is when $S$ is full-rank---more precisely, the flag is unique if and only if the type of $S$ is a refinement of $\gamma$, which is almost sure when $S$ is full-rank.
\end{theorem}

\begin{proof}
Original results about the maximum likelihood estimation of covariance eigenvalues and eigenvectors from multivariate Gaussian distributions with repeated covariance eigenvalues date back from the celebrated paper of~\cite{anderson_asymptotic_1963}.
We provide an independent proof for completeness with a particular emphasis on geometry, flags of linear subspaces, and uniqueness.
We successively find the optimal $\hat {{\mu}} \in \R^p$, $\hat {{Q}} \in \O(p)$ and $\hat{\lambda}_k \in \R$.

The log-likelihood expresses as a function of ${{\mu}} \in  \R^p$ in the following way
\begin{equation}\ln \mathcal{L}({{\mu}}) = -\frac n 2 \tr{{{\Sigma}}^{-1} {{C}}} + \operatorname{constant},
\end{equation}
with ${{C}} = \frac 1 n \sum_{i=1}^n ({{x}}_i - {{\mu}}) ({{x}}_i - {{\mu}})\T$.
The optimal shift $\hat{{{\mu}}}$ is thus
\begin{equation}
\hat{{{\mu}}} = \argmin_{\substack{{{\mu}} \in \R^p}}
\sum_{i=1}^n ({{x}}_i - {{\mu}})\T {{\Sigma}}^{-1} ({{x}}_i - {{\mu}}) := f({{\mu}}).
\end{equation}
The gradient of ${{x}} \mapsto ({{x}} - {{\mu}})\T {{\Sigma}}^{-1} ({{x}} - {{\mu}})$ is ${{x}} \mapsto 2 {{\Sigma}}^{-1} ({{x}} - {{\mu}})$.
Hence, setting the gradient of $f$ to $0$ at $\hat {{\mu}}$, one gets
$~{
\sum_{i} 2 {{\Sigma}}^{-1} ({{x}}_i - \hat {{\mu}}) = 0
}$,
whose solution is $\hat {{\mu}} = \bar {{x}}$.
Hence $\hat {{C}}$ is actually the sample covariance matrix of the dataset, which will be denoted ${{S}}$ (as in the theorem statement) from now on.

The log-likelihood expresses as a function of ${{Q}}$ in the following way
\begin{equation}\ln \mathcal{L}({{Q}}) = -\frac n 2 \lrp{\ln |{{\Sigma}}| + \tr{{{\Sigma}}^{-1} {{S}}}} + \operatorname{constant},
\end{equation}
with ${{\Sigma}} = {{Q}} {{\Lambda}} {{Q}}\T$.
Hence $|{{\Sigma}}|$ is independent of ${{Q}}$ and the optimal orthogonal transformation $\hat{{{Q}}}$ is
\begin{equation}
\hat{{{Q}}} = \argmin_{\substack{{{Q}} \in \O(p)}} \tr{{{\Sigma}}^{-1} {{S}}} = \tr{{{Q}} {{\Lambda}}^{-1} {{Q}}\T {{S}}}  :=  g({{Q}}).
\end{equation}
As $g$ is a smooth function on $\O(p)$ which is a compact manifold, $\hat{{{Q}}}$ exists and 
$~{
dg_{\hat{{{Q}}}}\colon \mathcal{T}_{\hat {{Q}}}(\O(p)) \ni {{\delta}} \mapsto \tr{\lrp{{{\delta}} {{\Lambda}}^{-1} {\hat {{Q}}}\T + {\hat {{Q}}} {{\Lambda}}^{-1} {{\delta}}\T} {{S}}} \in \R
}$ vanishes.
It is known that $~{\mathcal{T}_{\hat {{Q}}}(\O(p)) = \Skew_p {\hat {{Q}}}}$, therefore one has for all ${{A}} \in \Skew_p$
\begin{equation}
dg_{\hat{{{Q}}}}({{A}} {\hat {{Q}}}) = 
\tr{\lrp{({{A}} {\hat {{Q}}}) {{\Lambda}}^{-1} {\hat {{Q}}}\T + {\hat {{Q}}} {{\Lambda}}^{-1} ({{A}} {\hat {{Q}}})\T} {{S}}} = \tr{{{A}} ({{\Sigma}}^{-1} {{S}} - {{S}} {{\Sigma}}^{-1})} = 0.
\end{equation}
Therefore ${{\Sigma}}^{-1} {{S}} - {{S}} {{\Sigma}}^{-1} = 0$. Hence, ${{S}}$ and  ${{\Sigma}}^{-1}$ are two symmetric matrices that commute, so they must be simultaneously diagonalizable in an orthonormal basis.
Since the trace is basis-invariant, $g$ simply rewrites as a function of the eigenvalues
\begin{equation}\label{appeq:g_psi}
    g({{Q}}) = \sum_{k=1}^d \lambda_k^{-1} \lrp{\sum_{j \in \phi_{{\gamma}}^{-1} (\lrs{k})} \ell_{\psi(j)}},
\end{equation}
where $\psi \in S_p$ is a permutation and $\phi_{{\gamma}}^{-1} (\lrs{k})$ is the set of indexes in the $k$-th part of the composition ${\gamma}$.
We now need to find the permutation $\hat \psi \in S_p$ that minimizes $g$.
First, since $~{\lambda_1 > \dots > \lambda_d > 0}$ by assumption, then $~{(\lambda_1^{-1}, \dots, \lambda_d^{-1}})$ is an increasing sequence.
Therefore, $(\ell_{\hat{\psi}(\phi_{{\gamma}}^{-1} \lrs{1})}, \dots, \ell_{\hat{\psi}(\phi_{{\gamma}}^{-1} \lrs{d})})$ must be a non-increasing sequence, in that for $k_1 < k_2$, the eigenvalues in the $k_1$-th part of ${\gamma}$ must
be greater than or equal to the eigenvalues in the $k_2$-th part.
Indeed, for $\lambda < \lambda'$, if $\ell < \ell'$, then $~{\lambda \ell' + \lambda' \ell < \lambda \ell + \lambda' \ell'}$.
Second, for such a $\hat{\psi}$ sorting the eigenvalues in non-increasing order in between parts, we can easily check that the inequality between eigenvalues of distinct parts is strict if and only if the type of ${{\Sigma}}$ is a refinement of ${{\gamma}}$. 
If so, the minimizing $\hat{\psi}$ is unique up to permutations within each part of ${\gamma}$. 
Therefore, it is not $\hat {{Q}}$ itself but the sequence of eigenspaces of $\hat {{Q}}$ generated by its ${\gamma}$-composition that is unique, and we have ${(\operatorname{Im}(\hat{{{Q}}}_1), \dots, \operatorname{Im}(\hat{{{Q}}}_d)) = (\operatorname{Im}(V_1), \dots, \operatorname{Im}(V_d))}$. Hence, the accurate space to describe the parameter $\hat {{Q}}$ is actually the space of flags of type $\gamma$.

An important remark is that the uniqueness condition will almost surely be met when $S$ is full-rank.
Indeed, the set of $p \times p$ symmetric matrices with repeated eigenvalues has null Lebesgue measure (it is a consequence of Sard's theorem applied to the discriminant polynomial function (as defined in~\citet{breiding_geometry_2018}).
Therefore, since sample covariance matrices are measurable functions with absolutely continuous (Gaussian) densities with respect to Lebesgue measure, a randomly drawn matrix ${{S}}$ almost surely has distinct eigenvalues. 
Consequently, its type is $\lrp{1, \dots, 1}$, which is a refinement of any possible type in $\mathcal{C}(p)$.
Note that the full-rank assumption avoids having multiple null eigenvalues with nonzero measure.

The log-likelihood expresses as a function of ${{\Lambda}}$ in the following way
\begin{equation}\ln \mathcal{L}({{\Lambda}}) = -\frac n 2 \lrp{\ln |{{\Sigma}}| + \tr{{{\Sigma}}^{-1} {{S}}}} + \operatorname{constant},
\end{equation}
with ${{\Sigma}} = \hat {{Q}} {{\Lambda}} {\hat {{Q}}}\T$. 
First, one has $\ln|{{\Sigma}}| = \sum_{k=1}^d \gamma_k \ln \lambda_k$. 
Second, according to the previous results, one has $\tr{{{\Sigma}}^{-1} {{S}}} = \sum_{k=1}^d \lambda_k^{-1} \lrp{\sum_{j \in \phi_{{\gamma}}^{-1} \lrs{k}} \ell_j}$.
The optimal eigenvalues $\lrp{\hat{\lambda}_1, \dots, \hat{\lambda}_d}$ are thus
\begin{equation}
\lrp{\hat{\lambda}_1, \dots, \hat{\lambda}_d} = \argmin_{\substack{\lambda_1, \dots, \lambda_d \in \R}} \sum_{k=1}^d \gamma_k \ln \lambda_k + \lambda_k^{-1} \lrp{\sum_{j \in \phi_{{\gamma}}^{-1} \lrs{k}} \ell_j}  :=  h(\lambda_1, \dots, \lambda_d).
\end{equation}
As 
$\frac {\partial h} {\partial \lambda_k} = \frac{\gamma_k}{\lambda_k} - \lambda_k^{-2} \lrp{\sum_{j \in \phi_{{\gamma}}^{-1} \lrs{k}} \ell_j}$, we get that $\hat {\lambda}_k = \frac 1 {\gamma_k} \lrp{\sum_{j \in \phi_{{\gamma}}^{-1} \lrs{k}} \ell_j} = \overline{L_k}$.
\end{proof}



\noindent One can then easily express the maximum log-likelihood of PSA
\begin{equation}\label{appeq:PSA_ML}
    \ln \hat{\mathcal{L}}(\gamma) = -\frac n 2 \lrp{p \ln(2\pi) + \sum_{k=1}^d \gamma_k \ln{\overline{L_k}} + p}.
\end{equation}




\subsection{Geometric interpretation with flag manifolds}
As discussed in the previous subsections, the appropriate parameter space for ${Q}$ in PSA is the space of flags of type ${\gamma}$, noted $\operatorname{Flag}({\gamma})$.
The geometry of such a space is well known~\cite{ye_optimization_2022}.
In a few words, each subspace $\mathcal{V}_k$ of dimension $\gamma_k$ can be endowed with an orthonormal basis $Q_k := [q_k^1|\dots|q_k^{\gamma_k}]\in \R^{p\times\gamma_k}$. This basis is invariant to rotations within the subspace---i.e. for  $R_k\in\O(\gamma_k)$, $Q'_k := Q_k R_k$ is still an orthonormal basis of $\mathcal{V}_k$. Concatenating such orthonormal frames for all the mutually-orthogonal subspaces of a flag creates an orthogonal matrix $Q := [Q_1|\dots|Q_d]\in\O(p)$.
Eventually, $\operatorname{Flag}({\gamma})$ is a smooth quotient manifold, consisting in equivalence classes of orthogonal matrices:
\begin{equation}
    \operatorname{Flag}({\gamma}) \cong \O(p) / \lrp{\O(\gamma_1) \times \dots \times \O(\gamma_d)}.
\end{equation}
This result enables the accurate computation of the number of parameters in PSA.
Before that, let us note that the other parameters are ${\mu} \in \R^p$ and $~{{\Lambda} \in \operatorname{D}({\gamma}) := \lrs{\diag{\lambda_1 {I}_{\gamma_1}, \dots, \lambda_d {I}_{\gamma_d}} \in \R^{p \times p} \colon \lambda_1 > \dots > \lambda_d > 0}}$, which can be seen as a convex cone of $\R^d$.
\begin{proposition}
\label{appprop:PSA_nparam}
The number of free parameters in PSA is
\begin{equation}\label{appeq:PSA_kappa}
    \kappa(\gamma) := p + d + \frac{p(p-1)}{2} - \sum_{k=1}^{d} \frac {\gamma_k (\gamma_k - 1)} {2}.
\end{equation}
\end{proposition}
This geometric interpretation sheds light on PPCA, which---we remind---is a special case of PSA with ${\gamma} = \lrp{1, \dots, 1, p-q}$. First, as flags of type $(1, \dots, 1, p-q)$ belong to Stiefel manifolds (up to changes of signs), we can naturally parameterize PPCA models with those spaces, which is already commonly done in the literature~\cite{minka_automatic_2000}. Second, we can now see PPCA as removing $~{(p - q - 1) + \frac{(p-q)(p-q-1)}{2}}$ parameters with respect to the full covariance model by imposing an isotropy constraint on the noise space. PSA then goes beyond the noise space and results in even more parsimonious models.

We can extend this analysis to the IPPCA model, which---we remind---is a special case of PSA with ${\gamma} = \lrp{q, p-q}$. Hence we can parameterize it with flags of type $\lrp{q, p-q}$, which belong to Grassmannians.
With that in mind, we notice that our formula~\eqref{appeq:PSA_kappa} differs from the one given in~\citet{bouveyron_intrinsic_2011}. We think that this paper overestimates the number of free parameters by implicitly assuming eigenvectors living on Stiefel manifolds like in PPCA, whereas the isotropy in the signal space yields an additional rotational invariance which makes them actually live on Grassmannians. Therefore IPPCA is even more parsimonious than originally considered.
