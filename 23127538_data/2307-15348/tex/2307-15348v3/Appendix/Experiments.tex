\section{Additional experiment --- Dynamics of model selection}
A key result in the previous section is that we rarely have enough samples to confidently assert that two adjacent sample eigenvalues are distinct.
Consequently, PPCA models could be made more parsimonious by equalizing the adjacent sample eigenvalues with small gaps in the signal space as well.

In order to better understand how this result applies in practice, we make the following PSA model selection experiment.
We consider a given multivariate Gaussian population density, with covariance matrix eigenvalues $\lrp{10, 9, 7, 4, 0.5}$, and sample $n \in \lrb{20, 50000}$ data points from it. We fit all the PSA models to this data distribution and select the one with the lowest BIC. The experiment is repeated several times independently  for each $n$, and the results are reported in Figure~\ref{appfig:BIC_traj}, where we plot only a few models among the $16$ for readability.
% Figure environment removed
First, on the BIC plots, we can see that for $n \leq 6000$, PSA discloses a whole family of models that better explain the observed data than PPCA.
This shows that even for a very large number of samples with respect to the dimension, distinguishing the first eigenvalues and eigenvectors like PPCA does is not justified.
Second, on the complexity plots, we can see that PPCA mostly selects the full covariance model for any sample size, while PSA finds less complex models along the whole trajectory.
Moreover, interestingly, we note the consistent increase of model complexity with the number of samples. We deduce that as the sample size increases, PSA can more confidently distinguish the sample eigenvalues.
Third, on the Hasse diagram, we can see that PSA follows a trajectory as the number of available samples increases, which recalls the kind of subfamily generated by the hierarchical clustering strategy (cf. Figure~\ref{appfig:hierarchical_clustering}).
To conclude, we see on this synthetic example that PSA achieves a better complexity/goodness-of-fit tradeoff than PPCA in a wide range of sample sizes by equalizing the highest eigenvalues.