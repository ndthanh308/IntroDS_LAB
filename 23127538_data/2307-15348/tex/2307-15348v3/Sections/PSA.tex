\section{Identifying the curse of isotropy}
In order to spot the curse of isotropy, we go through the lens of statistical modeling and introduce the PSA generative model. This model assumes a Gaussian distribution with repeated covariance eigenvalues. 
It enjoys an explicit maximum likelihood estimate with a rich geometry enabling effective model selection.


\subsection{PSA model}
Let ${\gamma} := (\gamma_1, \dots, \gamma_d)$ be a \textit{composition} of a positive integer $p$---i.e. a sequence of positive integers that sums up to $p$.
We define the PSA model of \emph{type} ${\gamma}$ as the family of Gaussian distributions $~{p(x | \mu, \Sigma) := \mathcal{N}(x | \mu, \Sigma)}$, where $~{\mu \in \R^p}$ is a mean vector and $~{\Sigma = \sum_{k=1}^d \lambda_k Q_k {Q_k}\T\in S_p^{++}}$ is a covariance matrix with repeated eigenvalues $~{\lambda_1 > \dots > \lambda_d > 0}$ of respective multiplicity $\gamma_1, \dots, \gamma_d$ and associated eigenspaces $\mathrm{Im}(Q_1), \dots, \mathrm{Im}(Q_d)$.
These distributions can be rewritten as a (linear-Gaussian) latent variable generative model
\begin{equation}\label{eq:PSA_model}
{x} = \sum_{k=1}^{d-1} \sigma_k {Q}_k {z}_k + {\mu} + {\epsilon},
\end{equation}
where $~{\sigma_1 > \dots > \sigma_{d-1} > 0}$ are decreasing scaling factors,
${Q}_k \in \R^{p \times \gamma_k}$ are mutually-orthogonal orthonormal $\gamma_k$-frames, $~{{z}_k \sim \N{{0}, {I}_{\gamma_k}}}$ are independent latent variables and $~{{\epsilon} \sim \N{{0}, \sigma^2 {I}_{p}}}$ is an isotropic Gaussian noise. 
An illustration of the generative model is provided in Fig.~\ref{fig:PSA}. 
PPCA and IPPCA models can then be reinterpreted as PSA models, of respective types $~{{\gamma} = (1, \dots, 1, p - q)}$ and ${\gamma} = (q, p - q)$, where $q < p$ is the intrinsic dimension (cf. Sec.~\ref{appsec:PSA}).
% Figure environment removed



\subsection{Geometry and inference}
From a geometric point of view, the fitted density is isotropic on a sequence of mutually-orthogonal subspaces $\operatorname{Im}(Q_1) \perp \dots \perp \operatorname{Im}(Q_{d})$ of respective dimensions $\gamma_1, \dots, \gamma_d$.
Such a sequence is called a \emph{flag} of linear subspaces of \emph{type} ${\gamma}$.
Therefore, flags of type $\gamma$---which are diffeomorphic to $\O(p) / (\O(\gamma_1) \times \dots \times \O(\gamma_d))$~\cite{arnold_modes_1972, ye_optimization_2022}---naturally parameterize PSA models. 
Consequently, Stiefel manifolds and Grassmannians---which are particular cases of flag manifolds---respectively parameterize PPCA and IPPCA models (cf. Sec.~\ref{appsec:PSA}).
The remaining model parameters are the subspace variances $(\lambda_1, \dots, \lambda_d) \in \R^{d}$ and the mean ${\mu} \in \R^p$.
Thus, the \textit{complexity} (dimension of the parameter space) of the PSA model of type ${\gamma}$ is
\begin{equation}\label{eq:kappa}
    \kappa({\gamma}) := p + d + \frac{p(p-1)}{2} - \sum_{k=1}^{d} \frac {\gamma_k (\gamma_k - 1)} {2}.
\end{equation}
We can notably see that the decrease in model complexity is quadratic in the number of equalized eigenvalues.

One of the strength of the PSA models is that their maximum likelihood estimate is \textit{explicit}, similarly to PPCA and IPPCA. In a nutshell, we show in Thm.~\ref{appthm:PSA} that the most likely mean vector $\mu$ is the \textit{empirical mean}, the most likely variances $\lambda_1, \dots, \lambda_d$ are the \textit{block-averaged sample eigenvalues} according to the type $\gamma$, and the most likely flag $(\operatorname{Im}(Q_1), \dots, \operatorname{Im}(Q_{d}))$ is the sequence of mutually-orthogonal subspaces spanned by the associated eigenvectors. This yields the following expression for the maximum likelihood
\begin{equation}\label{eq:PSA_ML}
    \ln \hat{\mathcal{L}} (\gamma) = -\frac n 2 \left(p \ln(2\pi) + \sum_{k=1}^d \gamma_k \ln{\overline{L_k}} + p\right).
\end{equation}

\subsection{Identifying the curse of isotropy in practice}
The Bayesian information criterion~\cite{schwarz_estimating_1978} is defined as 
\begin{equation}\label{eq:BIC}
    \operatorname{BIC} (\gamma) := \kappa (\gamma) \ln n - 2 \ln \hat{\mathcal{L}} (\gamma).
\end{equation}
It is a widely-used model selection criterion, making a tradeoff between model complexity and goodness-of-fit, to prevent from overfitting given the number of observed samples. The formula results from an asymptotic approximation of the Bayesian model evidence. Given a dataset, one can compare the BIC of a PSA model with repeated eigenvalues to the BIC of a PSA model with distinct eigenvalues. The model with the lowest BIC is selected over the other one. 


As discussed previously, two adjacent sample eigenvalues with a relatively small gap may be prone to isotropic PC variability. 
To identify such situations where the curse of isotropy may arise, we compare a \textit{full} covariance model $\gamma = (1, \dots, 1)$ with an \textit{equalized} covariance model $~{\gamma' = (1, \dots, 1, 2, 1, \dots, 1)}$ where eigenvalues $j$ and $j+1$ are assumed equal.
Denoting $\delta_j := \frac{\ell_{j} - \ell_{j+1}}{\ell_j}$ the \emph{relative eigengap} between the two sample eigenvalues, we show in Sec.~\ref{appsec:MS} that
\begin{equation}\label{eq:releigengap_threshold}
    \mathrm{BIC}(\gamma') < \mathrm{BIC}(\gamma) \iff \frac{\delta_j}{2} < 1 - n^{\frac2n} + n^{\frac1n}\sqrt{n^{\frac2n} - 1}.
\end{equation}
This condition---independent of $p$---is illustrated in Fig.~\ref{fig:BIC_eigengap}. 
% Figure environment removed
We notably deduce by substitution that for $n = 1000$ samples, all the adjacent sample eigenvalues with a relative eigengap lower than $\delta = 21\%$ should be assumed equal. In other words, given two sample eigenvalues of respective magnitude $1$ and $0.8$, one needs \textit{at least} $1000$ samples to overcome the curse of isotropy. \textit{This is rarely the case in practice.} To illustrate this, we test the condition~\eqref{eq:releigengap_threshold} on many classical datasets from the UCI Machine Learning Repository (cf. Sec.~\ref{appsec:data}), with $n/p$ ratios ranging from $10$ to $10^4$.
For each dataset, we report the pairs of adjacent eigenvalues that are below the relative eigengap threshold in Fig.~\ref{fig:releigengap_UCI}.
% Figure environment removed
The outcomes are striking: all datasets but one have some eigenvalue pairs below the threshold. This does not only concern the smallest eigenvalues---which are usually tossed away because considered as noise---but also the highest ones---which are usually interpreted by applied scientists.
This shows that the curse of isotropy is not a negligible phenomenon at all and that particular care should be taken before interpreting the principal components.
Note that~\eqref{eq:releigengap_threshold} involves the \textit{relative} eigengap between adjacent eigenvalues and not the \textit{absolute} one, meaning that an exponentially-decreasing sample eigenvalue profile can actually highly suffer from the curse of isotropy. In other words, PSA models are not just suited to piecewise-constant-like sample covariance profiles.

The BIC is known for its tendency to select underparameterized models~\cite{bishop_pattern_2006}. Therefore, we also investigate in Sec.~\ref{appsec:MS} the eigenvalue-equalization guideline under other model selection criteria like the Akaike information criterion (AIC)~\cite{akaike_new_1974} and under empirical models~\cite{north_sampling_1982}. We get relative eigengaps around $10-20\%$ for $n=1000$, and experimental results substantiating the curse of isotropy's importance.


\subsection{Stratification and efficient model selection}
We now explicit the stratified structure of PSA models and show how it enables to design efficient model selection strategies to choose which groups of eigenvalues to equalize. More details are given in Sec.~\ref{appsec:MS}.


The space of symmetric matrices can be stratified according to the sequence of eigenvalue multiplicities \cite{arnold_modes_1972,groisser_geometric_2017,breiding_geometry_2018}. This implies that the PSA models in dimension $p$ form a stratified exponential family~\cite{geiger_stratified_2001} of cardinal $2^{p-1}$, partially-ordered~\cite{taeb_model_2024} by the stratum-inclusion relation.
We illustrate the family of $5$-dimensional PSA models in Fig.~\ref{fig:hasse_complexity}.
% Figure environment removed

In order to prevent from greedily exploring the whole family for model selection, we propose a simple yet efficient model selection technique based on the stratified structure of this family.
The \textit{hierarchical clustering strategy} consists in performing a hierarchical clustering of the sample eigenvalues, based on chosen \textit{pairwise distance} (e.g. the relative eigengap $\delta_j = \frac{\ell_{j} - \ell_{j+1}}{\ell_j}$) and \textit{cluster-linkage criterion} (e.g. single-linkage). This strategy yields a hierarchical subfamily of $p$ models with decreasing complexity, from which we can more efficiently select the model minimizing the BIC. We prove the \textit{asymptotic consistency} of the hierarchical clustering strategy in Prop~\ref{appprop:hierarchical_heuristic}, as well as introduce other strategies.