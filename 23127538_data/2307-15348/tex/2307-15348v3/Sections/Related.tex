\section{Related works}
In the climate research community, a celebrated work~\cite{north_sampling_1982}---often cited as \textit{North's rule-of-thumb}---warns scientists against close eigenvalues in the Karhunen-Lo√®ve expansion of a meteorological field. Indeed, the associated principal components---referred to as \textit{empirical orthogonal functions} (EOF)---suffer from large sampling errors, which is very problematic due to the key role EOF's play in this field for exploratory data analysis. The authors provide a perturbation-theoretical rule-of-thumb to decide which eigenvalues form \textit{degenerate multiplets}. The rule as stated in the paper is quite vague, however we are able (cf. Sec.~\ref{appsec:MS}) to reformulate its practical software implementation as a relative eigengap threshold and to compare it to our criterion~\eqref{eq:releigengap_threshold}. We show that this threshold is much lower than ours (e.g. $8.6\%$ instead of $21\%$ for $1000$ samples), therefore our result has a much larger impact on the practical methodology of PCA.

More broadly, several works have mentioned close-eigenvalues in PCA or in general symmetric matrices. 
A paper from Jolliffe~\cite{jolliffe_rotation_1989} shows the advantages of factor rotation inside subspaces spanned by principal components with close eigenvalues for structured data. 
Eigenvalue equality has also been studied formally in the context of oscillatory systems~\cite{arnold_modes_1972,lazutkin_kam_1993,gershkovich_problem_2004} diffusion tensor imaging~\cite{groisser_geometric_2017}, spectral geometry~\cite{besson_multiplicy_1988}, spectral shape analysis~\cite{lombaert_diffeomorphic_2013}, statistical tests~\cite{anderson_asymptotic_1963,tyler_asymptotic_1981,rabenoro_geometric_2024} etc.

Finally, the use of flags for statistical analysis has been particularly well illustrated with the example of \textit{independent subspace analysis}~\cite{hyvarinen_emergence_2000}, from which the name of our model is drawn. The authors notice the emergence of phase and shift-invariant features by maximizing the independence between the norms of projections of samples into so-called \textit{independent feature subspaces}. The learning algorithm is later recast as an optimization problem on flag manifolds~\cite{nishimori_riemannian_2006}. 
Flags also implicitly arise in general subspace methods under the name \textit{mutually orthogonal subspaces}, like in the mutually-orthogonal class-subspaces of Watanabe and Pakvasa~\cite{watanabe_subspace_1973} and the adaptive-subspace self-organizing maps of Kohonen~\cite{kohonen_emergence_1996}.
More recently, PCA was also reformulated as an optimization problem on flag manifolds~\cite{pennec_barycentric_2018}, raising perspectives for multilevel data analysis on manifolds.