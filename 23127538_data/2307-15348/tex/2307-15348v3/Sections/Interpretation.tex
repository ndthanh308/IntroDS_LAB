\section{From principal components to principal subspaces}

To summarize the previous section, parsimonious considerations invite us to block-average eigenvalues whose relative gaps are close---given the number of observed samples. The associated PSA model is now parameterized with \textit{eigenspaces} instead of individual \textit{eigenvectors} and we are therefore facing the curse of isotropy.
In this section, we propose to actually take advantage of the curse of isotropy and improve data interpretability by transitioning from \textit{principal components} to \textit{principal subspaces}.

A first idea, rather \textit{qualitative}, is to work at the subspace level and generate samples from the multidimensional principal subspaces via Eq.~\eqref{eq:PSA_model} (cf. Fig.~\ref{fig:PSA}). Those samples might have common characteristics like low-frequencies or invariances for images~\cite{hyvarinen_emergence_2000}.
Since there is an isotropic Gaussian variability, one can also uniformly discretize the unit sphere included in the principal subspace---especially in 2D and 3D---to help visualization. Other ideas involve orthogonal projections of explainable variability modes and correlation between subspace-projected dataset and co-variables.

A second idea, rather \textit{quantitative}, is to look for rotations of principal components inside their principal subspace in order to increase interpretability. 
Indeed, as explained previously, the curse of isotropy might cause principal components to be rotated versions of more interpretable components.
\textit{Varimax} rotation~\cite{kaiser_varimax_1958} (on the components and not the projected data as sometimes done~\cite{rohe_vintage_2023}) enables for instance to get rotated components with sparse loadings.
Many other criteria can be considered depending on the data type, notably entropy, structured sparsity~\cite{jenatton_structured_2010} or total variation for images.