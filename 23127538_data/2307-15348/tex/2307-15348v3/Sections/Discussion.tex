\section{Discussion}
We raised an important issue---the \textit{curse of isotropy}---about the isotropic variability of principal components under Gaussian models with repeated covariance eigenvalues, and showed that these models should often be assumed in practice according to the principle of parsimony. We developed a simple methodology---\textit{principal subspace analysis}---based on generative modeling and flags of subspaces to spot this curse in practice and transition from fuzzy principal components to much-more-interpretable principal subspaces.\footnote{The code is available on GitHub: \url{https://github.com/tomszwagier/principal-subspace-analysis}.}

Principal subspace analysis paves the way to numerous extensions.
First, one could deal with non-Gaussian data (exponential families, Gaussians on manifolds~\cite{pennec_intrinsic_2006}, deep generative models etc.). 
In that case, the maximum likelihood estimate might not be explicit and one might require tools from optimization on Riemannian manifolds~\cite{absil_optimization_2009} (flag manifolds~\cite{ye_optimization_2022, zhu_practical_2024} or symmetric positive-definite (SPD) matrices~\cite{groisser_geometric_2017}).
Second, one should investigate more advanced model selection strategies from Bayesian statistics, like an extension of Minka's criterion~\cite{minka_automatic_2000}. 
If in addition the maximum likelihood estimate is not explicit, then model selection could be embedded in a shrinkage method~\cite{tibshirani_regression_1996} or stratified optimization~\cite{leygonie_gradient_2023, olikier_first-order_2023} framework.
Third, since PSA models are nothing but parsimonious Gaussian models, one could simply extend them into parsimonious Gaussian mixture models~\cite{tipping_mixtures_1999}. The eigenvalue-equalization principle could actually be applied to any problem relying on symmetric matrices, like 
variational Bayesian methods~\cite{lambert_limited-memory_2023} or spectral geometry problems.
Notably, we think that the PSA methodology could extend to spectral graph theory and applications~\cite{ng_spectral_2001,belkin_laplacian_2003,lefevre_perturbation_2023}, where relatively-close Laplacian eigenvalues are common (related to shape symmetries) and might be especially problematic for spectral embedding and spectral matching~\cite{lombaert_diffeomorphic_2013}.
Fourth, any method relying on flags of subspaces~\cite{nishimori_riemannian_2006, ma_flag_2021, szwagier_rethinking_2023, mankovich_chordal_2023, mankovich_fun_2024} could benefit from our framework to select an adapted flag type, whose choice has been canonical or heuristic up to now.