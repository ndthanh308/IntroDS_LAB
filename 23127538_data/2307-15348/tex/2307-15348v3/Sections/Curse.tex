\section{The curse of isotropy}

Let us consider a dataset sampled independently from a two-dimensional \textit{isotropic} Gaussian distribution. This implies that the eigenvalues of the population covariance matrix are equal. The sample covariance matrix, however, is an approximation of the population covariance matrix, whose accuracy improves with the number of observed samples~\cite{tyler_asymptotic_1981}. Notably, the empirical eigenvalues are almost surely distinct (cf. Thm~\ref{appthm:PSA}). 
Therefore, PCA outputs the unique eigenvectors (up to sign) associated with each eigenvalue.
If we repeat this experiment several times independently and plot the principal components, we get Fig.~\ref{fig:isotropy}.
As we can see, the principal components are evenly spread in all directions---i.e. isotropically.  We call this phenomenon \textit{the curse of isotropy}. It is a curse since it yields principal components with high intersample variability and without any preferred direction. The observed components could therefore be random combinations of \textit{actually} interpretable components.

A legitimate question might then be: \textit{why (and when) should we assume that a given dataset has been sampled from a Gaussian distribution with repeated eigenvalues?} 
The Gaussian assumption is notably justified by the central limit theorem, the entropy maximization and the attractive computational properties that make Gaussian distributions the cornerstone of machine learning generative models~\cite{bishop_pattern_2006}.
Now, regarding the multiple-eigenvalue assumption, we have to go back to one of the founding principles of modeling that is the \textit{law of parsimony}, also known as \textit{Occam's razor}: ``The simplest explanation is usually the best one''. This principle is particularly applied in statistical modeling, where the limited number of observed samples makes overparameterized models overfitting~\cite{myung_counting_2000}. Notably, covariance matrices (which have $\mathcal{O}(p^2)$ parameters) can almost never be correctly estimated in practice, especially in high dimensions. Therefore, more parsimonious models have to be considered, like isotropic Gaussians (which have $1$ parameter---the variance), where all the covariance eigenvalues are equal. 
In the following, we show that a Gaussian model with \textit{repeated eigenvalues}, i.e. isotropic in some multidimensional eigenspaces, has less parameters than one with \textit{distinct eigenvalues} and therefore provides a simpler explanation of the data. Then, using parsimonious model selection criteria such as the BIC, we are able to decide which eigenvalues should be assumed equal.