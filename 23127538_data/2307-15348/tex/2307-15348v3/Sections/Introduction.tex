\section{Introduction}

Principal component analysis (PCA) \cite{jolliffe_principal_2002} is a universal method in data analysis. It gives the main modes of variation in the data by diagonalizing the empirical covariance matrix. The eigenvectors associated with the largest eigenvalues are the \textit{principal components}, and the subspace they span is used for dimension reduction and visualization.
Additionally, principal components can be used for exploratory data analysis and interpretability purposes. It has been extensively used on structured anatomical data (with components related to morphological features), in atmospheric sciences (with components related to climate patterns), computer vision (with so-called \textit{eigenfaces~\cite{sirovich_low-dimensional_1987}}) and many other fields. 
We refer to the chapters 4 and 11 of \citet{jolliffe_principal_2002}  for detailed examples of principal component interpretation.

Let us assume that a dataset has been sampled from a multivariate Gaussian distribution.
If all the population covariance eigenvalues are \textit{simple} (i.e. distinct), then we can associate to each eigenvalue a unique eigenvector (up to sign). Now, if some eigenvalues are \textit{multiple}, then those are associated with multidimensional eigenspaces, i.e. an infinite number of eigenvectors. This implies that the principal components associated with those multiple eigenvalues exhibit a large \textit{intersample variability}. More specifically, for any dataset size $n$, each independent $n$-sample from the distribution can yield totally different principal components, with a full rotational uncertainty within the eigenspaces. Therefore, under this multiple-eigenvalue assumption, principal components are unstable---regardless of $n$---which is fatal to data interpretability. We call this issue the \textit{curse~of~isotropy}.
% Figure environment removed

In real datasets, empirical covariance eigenvalues are never exactly equal (they are almost surely different from a measure-theoretical point of view, cf. proof of Thm.~\ref{appthm:PSA}), but some may be relatively close. In this case, it might be wiser to assume that \textit{close} eigenvalues are actually \textit{equal}---especially for small $n$---in order to avoid overfitting some spurious patterns caused by \textit{sampling errors}~\cite{north_sampling_1982}. Under this assumption, the dataset suffers from the curse of isotropy and one must be careful about interpreting the associated principal components. Therefore, identifying the curse of isotropy in practice boils down to answering the following question: \textit{when should we assume that the dataset has been sampled from a multivariate Gaussian distribution with repeated covariance eigenvalues?}

In this paper, we answer the question with an \textit{explicit} guideline, derived from two key concepts: \textit{parsimonious Gaussian modeling} and \textit{flags of subspaces}.
More specifically, we introduce a latent variable generative model called \textit{principal subspace analysis} (PSA). This model assumes a Gaussian density with repeated eigenvalues, where the sequence of eigenvalue multiplicities is specified by the so-called \textit{type} of the model. We show that PSA generalizes the celebrated Probabilistic PCA (PPCA) of Tipping and Bishop~\cite{tipping_probabilistic_1999} and unifies it with Isotropic PPCA (IPPCA)~\cite{bouveyron_high-dimensional_2007, bouveyron_intrinsic_2011}---a parsimonious version of PPCA suited to high dimensions. PSA models have a rich geometry relying on flag manifolds and stratify the space of covariance matrices. This enables us to assess the drop of model complexity caused by equalizing some eigenvalues and to perform efficient model selection based on parsimony-inducing criteria such as the Bayesian information criterion (BIC)---other criteria are investigated in Sec.~\ref{appsec:MS} with similar conclusions.
We show that two adjacent sample eigenvalues should be assumed equal when their relative eigengap is lower than a given threshold. This threshold depends on $n$ but is independent of the dimension $p$.

The results are striking: in almost all the datasets that we analyze, the curse of isotropy arises.
This questions the numerous scientific works relying on the \textit{interpretation} of principal components.
While this could sound fatal to exploratory data analysis, we show that the curse of isotropy can actually be leveraged to improve data interpretability.
Indeed, in such a situation, we suggest to give up \textit{principal components} and transition to more-interpretable \textit{principal subspaces}. Taking advantage of our generative model and factor rotation methods, we propose several qualitative and quantitative methods to increase the interpretability of principal components. We test the resulting PSA methodology on synthetic and real datasets and get promising results. More precisely, while principal components associated with close eigenvalues may be fuzzy---as arbitrary linear combinations of more interpretable factors---principal subspaces show the emergence of \textit{multidimensional attributes}.