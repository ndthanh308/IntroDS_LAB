\section{Experiments}
Eventually, PSA is grounded in a generative model with a rich geometry, yet the methodology is very simple and can be summarized in the three following steps:
\textit{eigendecomposition} of the sample covariance matrix, \textit{block-averaging} of the eigenvalues with small relative eigengaps (or more formally, PSA model selection), \textit{interpretation} of the resulting principal subspaces via factor rotation or latent subspace sampling. 
In this section we apply the PSA methodology to several synthetic and real datasets in a variety of fields. The experiments show that principal components associated with relatively-close eigenvalues are generally {fuzzy} due to the curse of isotropy. Therefore, equalizing the problematic eigenvalues and lifting the analysis to principal subspaces dramatically enhances exploratory data analysis.


\subsection{Natural image patches}
In this experiment, we consider patches extracted from natural images, as done in many seminal works investigating biological vision via unsupervised machine learning methods~\cite{field_relations_1987,olshausen_emergence_1996, hyvarinen_emergence_2000}.
We consider 10 flower images from the Natural Images database (cf. Sec.~\ref{appsec:data}) and randomly extract $n=500$ $(8, 8)$-pixel patches from those. After removing the DC component (i.e. the mean value) to each patch, like usually done in such studies~\cite{hyvarinen_emergence_2000} and looking at the sample eigenvalue profile, we decide to fit a PSA model of type $\gamma' = (2, 3, 59)$ and compare it to the associated PPCA model $\gamma = (1, 1, 1, 1, 1, 59)$. We get a lower BIC with the PSA model. Then, we uniformly sample from the unit spheres included in the first and second feature subspaces, which are respectively 2D and 3D. We report the results in Fig.~\ref{fig:exp_patch}.
% Figure environment removed
While principal components do not look particularly interpretable individually, grouping them into principal subspaces with isotropic variability brings out low-frequency subspaces with rotational invariance.
From a curse-of-isotropy point of view, the observed principal components are random samples from the illustrated multidimensional subspaces.


\subsection{Laplacian eigenfunctions}
In this experiment, we generate a synthetic dataset consisting in linear combinations of Laplacian eigenfunctions---also known as \textit{quasimodes}~\cite{arnold_modes_1972}---with variance being a decreasing function of the Laplacian eigenvalue. This kind of generative model has been extensively used in many different areas, notably climate sciences~\cite{north_sampling_1982} (for modeling atmospheric fields on the earth) and computer vision (for modeling shadows on faces under varying illumination conditions~\cite{basri_lambertian_2003} or low-frequency patches in natural images~\cite{field_relations_1987}). The global idea behind those models is that natural symmetries are present in the shapes under study (face, earth, square domain etc.) and lead to multiple eigenvalues in their Laplacian matrix, and therefore to multiple eigenvalues in the covariance matrix of homogeneous stochastic processes on those shapes.

We generate $n=500$ points on a square grid with 64 pixels on each side. We take a combination of the 	$q=9$ leading eigenmodes with variance scaling like $\exp(-2\lambda)$ (where $\lambda$ is the Laplacian eigenvalue) and add an isotropic noise.
We fit a PSA model of type $\gamma' = (1, 2, 1, 2, 2, 1, 4087)$ (corresponding to the expected Laplacian eigenvalue multiplicities on a square domain) and compare it to the associated PPCA model $\gamma = (1, \dots, 1, 4087)$. We get a lower BIC for the PSA model, and then perform factor rotation in the multidimensional eigenspaces. The results are shown in Fig.~\ref{fig:synthetic_exp}.
% Figure environment removed
We can see that the principal components are linear combinations of the original eigenmodes, i.e. quasimodes. With factor rotation in the associated subspaces, we better recover the original modes.



\subsection{Eigenfaces~\cite{sirovich_low-dimensional_1987}}
In this experiment, we consider the Carnegie Mellon University (CMU) Face Image database (cf. Sec.~\ref{appsec:data}). It consists in 640 grayscale pictures of people from varying pose, expression, and eye conditions. We extract $n=31$ $(60, 64)$-pixel images of the subject \textit{Choon}. Inspired by the seminal paper~\cite{basri_lambertian_2003} (which establishes a link between face shadowing under varying illumination conditions and spherical harmonics), we fit a PSA model of type $\gamma' = (1, 3, 5, 3831)$ and compare it to the associated PPCA model $\gamma = (1, \dots, 1, 3831)$. We get a lower BIC and perform entropy-guided subspace sampling in the second feature subspace, which is 3-dimensional. The results are illustrated in Fig.~\ref{fig:exp_CMU}.
% Figure environment removed
While principal components 2, 3, and 4 are fuzzy and uninterpretable, we can see that they actually correspond to linear combinations of three \textit{much more interpretable} factors.


\subsection{Structured data}
In this experiment, we consider a structured dataset taken from the UCI ML repository. The \textit{Glass identification} dataset (cf. Sec.~\ref{appsec:data}) from the USA Forensic Science Service contains chemical features about different types of glasses, with applications in criminology. We fit a PSA model of type $\gamma' = (5, 4)$ and compare it to the associated PPCA model $\gamma = (1, \dots, 1, 4)$. We get a lower BIC and perform varimax rotation in the first feature subspace, of dimension 5. We report the loadings of the sample eigenvectors and compare them to the PSA factors after rotation in Fig.~\ref{fig:exp_rotation}.
We see that the PSA factors are more interpretable than the principal components, in the sense that they express as sparser combinations of the original variables. Moreover, contrary to classical factor rotation methods done after PCA (cf. Chapter~11 of \citet{jolliffe_principal_2002}), we here do not loose any hierarchy in the principal components in terms of explained variance, since under the PSA model of type $\gamma' = (5, 4)$, the five components have equal variance.
% Figure environment removed
