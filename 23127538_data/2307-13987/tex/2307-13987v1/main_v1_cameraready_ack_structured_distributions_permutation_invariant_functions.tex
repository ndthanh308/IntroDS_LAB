\documentclass[conference,letterpaper]{IEEEtran}%[conference,a4paper]
%\thispagestyle{plain}
%\pagestyle{plain}
%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}
%\pagenumbering{arabic}


\IEEEoverridecommandlockouts % Don't forget this command!


\usepackage{amssymb, cite}
\usepackage{bm,bbm}
%\usepackage[small]{caption}
\usepackage{multirow,color,amsfonts}
\usepackage{tabulary}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumerate}
\usepackage{algorithm,algpseudocode}
%\usepackage[]{algorithm2e}
\usepackage{comment}


\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{cite}
%\usepackage[cmex]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)


\usepackage{mathtools} % for 'bsmallmatrix' environment

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
\newcommand{\infdiv}{D\infdivx}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}

\usepackage{amsthm}
\usepackage{amsmath}

\newtheorem{theo}{Theorem}
\newtheorem{defi}{Definition}
\newtheorem{assu}{Assumption}
\newtheorem{prop}{Proposition}
\newtheorem{remark}{Remark}
\newtheorem{note}{Note}
\newtheorem{cor}{Corollary}
\newtheorem{ques}{Questions}
\newtheorem{lem}{Lemma}
\newtheorem{ex}{Example}


\newenvironment{brsm}{% % short for 'bracketed small matrix'
  \left[ \begin{smallmatrix} }{%
  \end{smallmatrix} \right]}

\DeclareMathOperator\erf{erf}
\DeclareMathOperator*{\Q}{\rm Q}

\newcommand{\chr}[1]{{\em \color{red} #1}}

\newcommand{\derya}[1]{\textit{\textcolor{blue}{#1}}}
\newcommand{\deryam}[1]{\textit{\textcolor{red}{#1}}}

\newcommand{\E}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\Pro}[1]{\mathbb{P}\left[#1\right]}
  \newcommand*{\eleminvalid}{\ensuremath{\text{\scriptsize{\ding{54}}}}}
  
  
\newcommand{\Kgkw}{K}%{K_{X_1,X_2}}
\newcommand{\Kf}{K}%{K_{{\rm c}}}%{K_{G_{X_1},G_{X_2}}}
\newcommand{\Kmatch}{K_{\rm M}}
\newcommand{\RJ}{R_{{\rm Comp}}}  
\newcommand{\graphweight}{{q}}


\interdisplaylinepenalty=2500 % As explained in bare_conf.tex


% Reduces space around equations and figures 
%\setlength\abovedisplayskip{3pt plus 2pt minus 2pt} 	% Reduce space before equation
%\setlength\belowdisplayskip{3pt plus 2pt minus 2pt}	% Reduce space after equation
%\setlength\textfloatsep{10pt plus 2pt minus 2pt}		% Reduce space between figure caption and text




%%%%%%
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}






\usepackage{enumitem}







  
  


%Chromatic entropy related definitions

\newcommand{\CGE}{{graph entropy}}
\newcommand{\FCGE}{{fractional graph entropy}}

\newcommand{\FCG}{{fractional characteristic graph}}
\newcommand{\FCN}{{fractional chromatic number}}
\newcommand{\CN}{{chromatic number}}
\newcommand{\FCE}{{fractional chromatic entropy}}
\newcommand{\CE}{{chromatic entropy}}

\newcommand{\EWCG}{{edge-weighted characteristic graph}}
\newcommand{\EWCGs}{{edge-weighted characteristic graphs}}


\newcommand{\HG}{{H_{G_{X_1}}(X_1\vert X_2)}}
\newcommand{\HGfrac}{{H^{f}_{G_{X_1}}(X_1\vert X_2)}}

%chromatic entropy
\newcommand{\HGchi}{{H^{\chi}_{G_{X_1}}(X_1\vert X_2)}}
\newcommand{\HGchifrac}{{H^{\chi_f}_{G_{X_1}}(X_1\vert X_2)}}

\newcommand{\HGchifracweighted}{{H^{\chi_f}_{G^w_{X_1}}(X_1\vert X_2)}}
\newcommand{\HGchipowertwofracweighted}{{H^{\chi_f}_{G^{2,w}_{{\bf X}_1}}({\bf X}_1\vert {\bf X}_2)}}


\newcommand{\GXone}{G_{X_1}^w}
\newcommand{\GXtwo}{G_{X_2}^w}

\newcommand{\GXonen}{G_{{\bf X}_1}^{n,w}}
\newcommand{\GXtwon}{G_{{\bf X}_2}^{n,w}}

%n=2
\newcommand{\GXonetwo}{G_{{\bf X}_1}^{2,w}}
\newcommand{\GXtwotwo}{G_{{\bf X}_2}^{2,w}}




\newcommand{\Graph}{G_{X_1}}
\newcommand{\twoPowerGraph}{G^2_{{\bf X_1}}}
\newcommand{\threePowerGraph}{G^3_{{\bf X_1}}}
\newcommand{\nPowerGraph}{G^n_{{\bf X_1}}}
\newcommand{\infPowerGraph}{G^{\infty}_{{\bf X_1}}}
\newcommand{\HGchipowertwo}{{H^{\chi}_{G^2_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}
\newcommand{\HGchipowertwofrac}{{H^{\chi_f}_{G^2_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}
\newcommand{\HGchipowern}{{H^{\chi}_{G^n_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}
\newcommand{\HGchipowerk}{{H^{\chi}_{G^k_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}
\newcommand{\HGchipowerkminusone}{{H^{\chi}_{G^{k-1}_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}
\newcommand{\HGchipowernfrac}{{H^{\chi_f}_{G^n_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}
\newcommand{\HGchipowerkfrac}{{H^{\chi_f}_{G^k_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}
\newcommand{\HGchipowerkminusonefrac}{{H^{\chi_f}_{G^{k-1}_{{\bf X_1}}}({\bf X_1}\vert {\bf X_2})}}

\newcommand{\coloring}{c_{{G_{X_1}}}(X_1)}
\newcommand{\coloringx}{c_{{G_{X_1}}}}
\newcommand{\coloringpowern}{c_{{G^n_{{\bf X_1}}}}({\bf X_1})}
\newcommand{\coloringpowernplusone}{c_{{G^{n+1}_{{\bf X_1}}}}({\bf X_1})}
\newcommand{\coloringpowernx}{c_{{G^n_{{\bf X_1}}}}}
\newcommand{\coloringpowertwox}{c_{{G^2_{{\bf X_1}}}}}

\newcommand{\coloringf}{c^f_{{G_{X_1}}}(X_1)}
\newcommand{\coloringxf}{c^f_{{G_{X_1}}}}
\newcommand{\coloringpowernf}{c^f_{{G^n_{{\bf X_1}}}}({\bf X_1})}
\newcommand{\coloringpowernb}{c^b_{{G^n_{{\bf X_1}}}}({\bf X_1})}
\newcommand{\coloringpowernplusonef}{c^f_{{G^{n+1}_{{\bf X_1}}}}({\bf X_1})}
\newcommand{\coloringpowernxf}{c^f_{{G^n_{{\bf X_1}}}}}
\newcommand{\coloringpowertwoxf}{c^f_{{G^2_{{\bf X_1}}}}}









\interdisplaylinepenalty=2500 % As explained in bare_conf.tex







% ------------------------------------------------------------

%https://susanfitzell.com/strategies-for-effective-group-processes-establish-ground-rules/

%http://employees.oneonta.edu/vomsaaw/w/psy220/files/GroupWorkGuidelines.htm



% ------------------------------------------------------------
\begin{document}
\title{Distributed Computing of Functions of\\ Structured Sources with Helper Side Information}
\author{
  \IEEEauthorblockN{Derya Malak}
  \IEEEauthorblockA{Communication Systems Department,   EURECOM, France,\\
derya.malak@eurecom.fr
}
\thanks{Funded by the European Union (ERC, SENSIBILITÉ, 101077361). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.}
%\thanks{Manuscript last revised: {\today}.}
}

\maketitle




\begin{abstract}
%We consider the problem of low-complexity computing of a permutation invariant function over a network with two sources, one user that seeks the outcome of the function, and one low-rate helper that builds a bridge between the sources and the user. The sources are partially distributed, meaning that they are highly structured such that their joint distribution is described according to a mixture model. Mixture distributions provide a flexible, parametric framework for statistical modeling and analysis, and arise in Bayesian inference. We provide an achievable coding scheme by exploiting the joint source information,  the symmetry induced by the function, and accounting for the matching information extracted through a low-rate helper from the mixture distribution. The role is to reduce the communication cost, taking not only advantage of the correlations between the data, but also the special nature of the computing function. The proposed approach attains gains up to $\% 40$ versus distributed schemes that do not exploit the structure, and nicely captures a variety of real-life scenarios like learning, combinatorics, and graph neural network applications, and substitute high complexity joint decoding techniques.

In this work, we consider the problem of distributed computing of functions of structured sources, focusing on the classical setting of two correlated sources and one user that seeks the outcome of the function while benefiting from low-rate side information provided by a helper node. %We also consider the case where the sources are jointly distributed according to a very general mixture model that can be calibrated to capture various inference scenarios. 
Focusing on the case where the sources are jointly distributed according to a very general mixture model, 
we here provide an achievable coding scheme that manages to substantially reduce the communication cost of distributed computing by exploiting the nature of the joint distribution of the sources, the side information, as well as %by exploiting 
the symmetry enjoyed by the desired functions. Our scheme --- which can readily apply in a variety of real-life scenarios including learning, combinatorics, and graph neural network applications --- is here shown to provide substantial reductions in the communication costs, while simultaneously providing computational savings by reducing the exponential complexity of joint decoding techniques to a complexity that is merely linear. 
\end{abstract}

%\begin{IEEEkeywords}
%Distributed computation, distributed source coding, structured sources.
%\end{IEEEkeywords}




%%%
\section{Introduction}
\label{sec:intro}

The past few years have seen a rising need to speed up computationally-intensive tasks, as well as have witnessed an ever increasing necessity for new parallel processing techniques that efficiently distribute computations across groups of servers. This necessary transition to distributed computing though, has also introduced a variety of challenges that involve accuracy \cite{wang2021price}, computing scalability \cite{soleymani2021analog}, and straggler mitigation \cite{li2021flexible}. Key among these challenges comes in the form of the crippling communication bottleneck of distributed computing, brought about by the astronomical communication costs often required for implementing computing in a distributed manner. This bottleneck is central to our work here.

Following the seminal work in \cite{yao1979some} on the communication complexity of distributed computing, several %other 
works have sought to minimize the associated communication cost, with some of these works including the recent breakthroughs in distributed linearly separable computation %\cite{wan2021distributed}, 
\cite{khalesi2022multi}, distributed matrix multiplication%product computation
~\cite{jia2021capacity}, %distributed matrix multiplication with stragglers \cite{li2021flexible}, secure matrix multiplication \cite{jia2021capacity}, and linearly separable distributed computation \cite{khalesi2022multi}, 
and others that extended the seminal work in \cite{SlepWolf1973} %followed in the footsteps of seminal works on 
%distributed source compression 
to %for 
function computation, %. Such seminal works 
including~%\cite{OR01}, 
\cite{Kor73} and \cite{feizi2014network}, that consider %the case of 
function computation over networks, as well as %\cite{feizi2014network} and 
\cite{doshi2007source} 
that study functional rate distortion. 
%
This same communication cost was naturally also affected by the nature of the computed function. With this in mind, 
we studied distributed computation using structured distributions, see e.g., \cite{Malak2022hyperbin}% and \cite{malakmedard2020}
.%the work in \cite{korner1979encode}, \cite{sefidgaran2015korner} studied the encoding problem focusing on computing modulo-two sums of binary sources, while the work in \cite{nair2020optimal} extended this problem to account for a larger class of binary distributions, and the work in \cite{adikari2022two} subsequently considered the case of two-terminals and common sum reconstruction. Furthermore, in the same context, we can also find the general techniques of compression of graph data \cite{delgosha2020universal}, which is again geared toward capturing the structure of the computed function (see also the related work in \cite{zhou2007bipartite}).

%Our work here draws from the common theme that indeed, when it comes to distributed computing, the rate of joint compression can be substantially reduced by exploiting not only the structure in the data but also the structure in the desired functions to be computed, as well as the corresponding function behavior with respect to each source. Motivated by the above, we here consider the class of functions %\emph{permutation invariant functions}, 
%which will also take as inputs data from sources that themselves enjoy a strong joint structure.  

The above motivate us to explore the joint effect of having structure in data and in sources, and to explore how this structure can help us reduce the aforementioned communication bottleneck of distributed computing. Toward this, we here consider a partially distributed computation problem with two sources of jointly distributed data, with one user that seeks the outcome of a function of the sources, and with one low-rate helper that mediates the computation of the function by providing a small amount of side information on the instantaneous matching of the sources. The sources are here modeled by a mixture distribution. From the perspective of Bayesian inference, such models can accurately capture the behavior of data distributed according to a mixing distribution.

Our aim will be to exploit the structure of the data and of the desired functions, in order to reduce communication (as well as computational/decoding) costs. To do that, while capturing the structure of the partially distributed sources, we will seek to decompose the joint source distribution into a convex combination of integral matchings, and we will do so by exploiting the well-known {\emph{Birkhoff-von Neumann Theorem}} in \cite{caron1996nonsquare}, as well as {\emph{Sinkhorn’s Theorem}} in \cite{sinkhorn1964relationship}.  %and \cite{marshall1968scaling}. 
We will here focus on the case where the nonmatched distributions correspond to low-probability events, and the case where the side information captures this matching behavior. 

We next describe generalizations of the Birkhoff-von Neumann statistical multiplexing approaches that have been successful in signal processing, wireless and networking applications, including switching theory, e.g., the study of the rate region of flows to compute a switch schedule using a graph-theoretic formulation \cite{4285330}, and best-effort switching services \cite{chang2000birkhoff}. 
Fast converging algorithms for delay-sensitive applications with sparse switching configurations by leveraging Frank-Wolfe methods %and block-coordinate descent 
were also studied \cite{valls2021birkhoff}. In \cite{dimakis2010gossip}, %and \cite{kar2010gossip}, 
the Birkhoff-von Neumann theorem was used for the analysis of gossip algorithms. There exist applications in multi-sensor data in signal processing \cite{uccar2019partitioning}, matrix analysis \cite{tropp2022acm}, and stochastic matrix optimization for power amplifiers \cite{safa2022low}. 
%Related work also includes game theory models to reduce response time and information loss rate \cite{somasundaram2015game}. 
For instance, in latency-constrained applications, it is possible to precompute an approximated Birkhoff-von Neumann decomposition offline and then select a permutation matrix at random with probability proportional to its coefficient \cite{hoyos2020approximate}.


%\derya{highlight challenges accomplished to overcome, new tools and ideas, the improvements over literature. Optimality of the proposed schemes and if the helper can be exploited further }

All the above will allow us to propose an achievable coding scheme that captures the aforementioned structure in data and functions as well as exploits the side information, %in order 
to reduce the overall communication cost. It is worth noting that the proposed scheme is different than the existing zero-error distributed coding schemes %\cite{koulgi2003zero}, 
\cite{witsenhausen1976zero} whose operating rates are often limited by the network topology. The proposed scheme, as we will see, can attain gains of approximately 40\% over distributed schemes that do not exploit the above structure. In addition, the helper-based approach will be shown to dramatically decrease the decoding complexity of joint distributions versus minimum-entropy decoding (which is NP-hard in general) used in Slepian-Wolf source coding \cite{coleman2005towards}. %, at the expense of a possibly increased communication cost.

%%% 
The rest of the paper is structured as follows. In Sect.~\ref{sec:model_defns}, we give a primer on %permutation invariant functions and 
graph compression for modeling the computation rate region for two sources jointly distributed according to a mixture. 
In Sect.~\ref{sec:problem_statement}, we detail %our main results, namely 
the rate region %for computation 
with a helper that can extract the %side information on the matching 
matching information 
of the sources for (i) perfectly matched, and (ii) maximally matched sources. 
We conclude in Sect.~\ref{sec:examples} with an example to numerically evaluate the complexity of our technique.

%%%%
{\bf Notation.} For a random variable $X$ with a finite alphabet $\mathcal{X}$, then $P_X$ will denote its probability mass function (PMF). Similarly, for variables $X_1$ and $X_2$, then $P_{X_1,X_2}$ will denote the joint PMF.  %of finite alphabet $X_1$ and $X_2$. 
%We denote the probability of an event $A$ by $\mathbb{P}(A)$. 
Let the entropy function of a PMF ${\bf p}$ be $h({\bf p})=-\sum\nolimits_i p_i\log p_i$ where the logarithm is in base $2$, $h(p)$ be the binary entropy function with parameter $p$, %not sure if you clarified if this is for GF2 or reals, or general finite field
and $H(X)=\mathbb{E}[-\log P_{X}(X)]$ be the Shannon entropy of $X$ drawn from  $P_{X}$. 
We denote by ${\bf X}_1^n=X_{11},X_{12},\dots, X_{1n}\in \mathcal{X}_1^n$ the length $n$ sequence of $X_1$ sampled from an $n$-fold finite alphabet $\mathcal{X}_1^n$. 
We let $[N]=\{1,2,\dots,N\}$, $N\in \mathbb{Z}^+$. %and $[a:b]=\{a,a+1,\dots,b\}$ for $a<b$, $a,b\in\mathbb{Z}^+$. 




%%%%
\section{Model, Definitions, \& Related Existing Results}
\label{sec:model_defns}
We consider the problem of computing functions of two sources of partially distributed data according to a mixture distribution, and one user that aims to recover the function outcome. 
% 
%As suggested, this is a pertinent problem in modern distributed computing where indeed communication overheads remain massive, and where datasets exhibit large correlations. 
%
Our model relies on the availability of the side information that captures the matching information extracted from the mixture, which we will elaborate later on, just below, at the source sites, which we leverage via one helper possibly with a limited rate. %We note that %our focus is mainly on lossless source compression, and our encoding scheme does not concern the losses pertaining to the channels. 
The user can recover both sources in an error-free manner.
%
Our main purpose is, for sources and functions with described structures, to characterize an achievable rate region for asymptotically lossless computation.
Given the existence of schemes, e.g., \cite{SlepWolf1973} and \cite{korner1979encode}, %\cite{korner1998zero}, 
that aim to recover data or a function %the modulo $2$ sum 
of data in a similar %an asymptotically and losslessly
manner and have higher sum rates than our scheme, the two advantages of our model over these pertinent ones are (i) a tighter rate region, which is %indeed 
critical in modern applications because of symmetries, and (ii) complexity of decoding, not only alleviating the distributed computing systems from performing calculations beyond what the user seeks to compute %we actually want to compute 
but also eliminating the prohibitive complexity of minimum-entropy decoding.

%In our effort to characterize the achievable rate region for asymptotically lossless compression, we will employ and combine various fundamental approaches, each dealing with different aspects of the problem at hand. Before doing so, 
In what follows, we describe some of the fundamental ingredients that we will use, first with respect to some basic principles behind the graph entropy approach that will allow us to capture some of the interdependencies between the sources and the computed function, and then we will discuss the mixture model that captures the joint source distribution. 






\begin{comment}
From the data processing inequality, the rate region to compute $X_1+X_2$ is encompassed by the rate region for computing $f(X_1,\, X_2)=\Phi(X_1+X_2)$.  %if $X_1+X_2$ can be computed, then any $f(X_1,\, X_2)$ in the form of (\ref{SymmetricFunction}) can be computed.%is upper bounded by the sum rate of $X_1+X_2$. 
The entropy of $f(X_1,\, X_2)$ is $H(\Phi(X_1+X_2))$, which gives a trivial rate lower bound, which is achievable when sources are centralized, or when the data is structured, e.g., a perfect matching of the sources.
\end{comment}

%%%%%
\subsection{Representation of Source Characteristic Graphs} 
\label{sec:CharacteristicGraphs}

In this section, we review the fundamental limits of asymptotically lossless compression for computation, which can be realized using the concept of \emph{characteristic graphs}. 
%
Source one builds a characteristic graph $G_{X_1}=(V_{X_1},E_{X_1})$ \cite{Kor73} for computing $f(X_1,X_2)$ to distinguish the source outcomes that yield a different output for any value of $X_2$, and similarly for source two. Vertices are the sample values, i.e., $V_{X_1}=\mathcal{X}_1$, and the edges are determined as follows. Given two arbitrary vertices $(x_1^{k_1}, x_1^{k_2}) \in \mathcal{X}_1^2$ in $G_{X_1}$ such that $k_1\neq k_2$, if $\exists$ a vertex $x_{2}^{l}\in \mathcal{X}_{2}$ in $G_{X_{2}}$ such that $P_{X_1,X_2}(x_1^{k_1},x_2^{l})P_{X_1,X_2}(x_1^{k_2},x_2^{l})>0$ and the function satisfies $f(x_1^{k_1},x_{2}^{l})\neq f(x_1^{k_2},x_{2}^{l})$, %$\Phi(x_1^{k_1}+x_{2}^{l})\neq \Phi(x_1^{k_2}+x_{2}^{l})$, 
then $(x_1^{k_1},x_1^{k_2})\in E_{X_1}$. Otherwise, $(x_1^{k_1},x_1^{k_2})\notin E_{X_1}$.


We let $c_{G_{X_1}}(X_1)$ be a valid coloring of $G_{X_1}$, where a valid coloring %\footnote{In \cite{feizi2014network}, the authors have devised a necessary and sufficient condition for any achievable coding model that relies on colorings, known as the coloring connectivity condition.} 
is such that any two vertices of $G_{X_1}$ that share an edge are assigned distinct colors. As one would expect, the characteristic graphs built by the sources %each source, i.e., $G_{X_1}$ and $G_{X_2}$, 
are correlated.

For simultaneous encoding of multiple instances of a source, source $m\in [2]$ similarly builds the $n$-th power of $G_{X_m}$, i.e., $G_{{\bf X}_m}^n$. We note that $G_{{\bf X}_m}^n=(V_{X_m}^n,E_{X_m}^n)$ is an OR graph such that $V_{X_m}^n=\mathcal{X}_m^n$ and if $(x_{mi}^{k_1},x_{mi}^{k_2})\in E_{X_m}$ for some coordinate $i\in [n]$, then $({\bf x}_{m}^{k_1},{\bf x}_{m}^{k_2})\in E_{X_m}^n$. 

The characteristic graph entropy of $X_1$ is given by \cite{Kor73} $$H_{G_{X_1}}(X_1)=\lim_{n\to\infty}\, \min\limits_{c_{G_{{\bf X}_1}^n}}\,\frac{1}{n} H(c_{G_{{\bf X}_1}^n}({\bf X}_1))\ ,$$ %\\
%H_{G_{X_1}}(X_1\vert X_2)&=\lim_{n\to\infty} \min\frac{1}{n} H(c_{G_{X_1}^n}(X_1)\vert X_2).\nonumber
where the minimization is over the set of all valid colorings $c_{G_{{\bf X}_1}^n}({\bf X}_1)$ of $G_{{\bf X}_1}^n$. 
%
%{\bf A sum rate model.} 
Similarly, conditional graph entropy \cite{OR01} and joint graph entropy \cite{feizi2014network} can be determined.

%%%%%
\subsection{Mixture Distribution in the Discrete Parameter Domain}
\label{sec:mixture}


As we outlined above, the two sources are partially distributed based on a mixture distribution. 
Generally, such mixture distribution results from assuming that a random variable $Y$ is distributed according to some parametrized distribution $P_{Y|\Theta}$ with an unknown parameter $\Theta$  with %distributed according to 
the  latent distribution $P_{\Theta}$ \cite{rover2017discrete}. %The resulting distribution $P_Y$ is said to be the distribution that results from compounding $P_{Y|\Theta}$ with $P_{\Theta}$. 
In this context, the unconditional distribution $P_Y$ results from marginalizing over $P_{\Theta}$: 
\begin{align}
\label{mixture_distribution}
P_Y(y)= \sum\limits_{\theta} P_{\Theta}(\theta )\,P_{Y|\Theta}(y|\theta )  = \sum\limits_{l\in[L]} q_l\,P_{Y|\Theta}(y|\theta_l )\ ,
\end{align}
where $\Theta\sim (q_1,\dots,q_L)$ is distributed over $ (\theta_1,\dots,\theta_L)$. 

%%%%
Our interest in mixture distributions stems from two main properties. The first has to do with the fact that such mixture distributions span a broad range of scenarios that are of particular interest in modern science.  
Mixture distributions arise in Bayesian inference where a hypothesis is updated as more information becomes available. 
The second aspect relates to the fact that albeit broad, this mixture structure is endowed with properties that are easier to analyze and expand. One such property relates to the renowned Birkhoff's algorithm. %which we briefly discuss below. 

%To model the distributed computation of mixture distributions, we next detail a decomposition for bistochastic matrices. 

%%%%%
Birkhoff's algorithm is a greedy algorithm that receives as input a bistochastic matrix %\footnote{A bistochastic matrix is an $N\times N$ matrix $W=(w_{ij})$ of nonnegative real numbers, each of whose rows and columns sums to $1$.} 
$W=(w_{ij})\in\mathbb{R}_{\geq 0}^{N\times N}$ (meaning that each of whose rows and columns sums to $1$), and returns %as output 
a Birkhoff-von Neumann decomposition of $W$, instantiated by the Birkhoff–von Neumann Theorem \cite{caron1996nonsquare}, where $W$ is %a presentation of it as 
a sum of permutation matrices with non-negative weights \cite{aziz2020simultaneously}. 
The run-time complexity of Birkhoff's algorithm is $O(N^2)$.  %\cite{johnson1960algorithm}. 
This algorithm is useful in our particular case with partially distributed sources, in providing a decomposition for the mixture distribution, which we will detail in Sect. \ref{sec:representation_matchings}.

\begin{comment}
We next give Birkhoff–von Neumann Theorem, which can be proved using Hall's Marriage Theorem \cite{caron1996nonsquare}. %\cite{stanley1986enumerative}.

\begin{theo}\label{BirkoffTheorem}
({\bf Birkhoff–von Neumann Theorem \cite{caron1996nonsquare}.}) %https://webpages.charlotte.edu/ghetyei/courses/old/F07.3116/birkhofft.pdf
%the polytope $\mathcal{B}_{n}$ is the convex hull of the set of $n\times n$ permutation matrices, and furthermore that the vertices of $\mathcal{B}_{n}$ are precisely the permutation matrices. 
If $W\in\mathbb{R}^{N\times N}$ is a bistochastic matrix, then there exist ${\bf q}=(q _{1},\ldots ,q _{L})\in\mathbb{R}^L_{>0}$, an integer $L\in O(N^2)$ such that $\sum _{l\in[L]}q _{l}=1$, and a set of $N\times N$ permutation %\footnote{The number of possible permutations is $n !$%which gives an upper bound to $L$ due to sampling without replacement
%.} 
matrices $ B_{1},\ldots, B_{L}$ %which constitute a support of the matching (there may be multiple supports) 
such that $W$ has a decomposition given as a convex combination of integral matchings
%\begin{align}
$W=\sum\nolimits_{l\in[L]}q _{l}B_{l}$.
%\end{align}
\end{theo} 
%https://theory.stanford.edu/~jvondrak/MATH233B-2017/lec2.pdf %Applying the LP duality theorem to the bipartite matching problem on $\mathcal{B}_{X_1,X_2}$, we have K\H{o}nig's theorem: Theorem 8 (K\H{o}nig) For any bipartite graph G = (V, E), the cardinality of the maximum matching max-matching(G) and the cardinality of the minimum vertex cover min-vertex-cover(G) are the same.
\end{comment}

Birkhoff's algorithm has applications in fair random assignment,  alleviates the problem of solving challenging linear systems with unstructured and indefinite coefficient matrices, and
%Splitting of a matrix according to its Birkhoff-von Neumann decomposition \cite{benzi2017preconditioning} 
improves efficiency and scalability of parallel computing %\cite{aziz2020simultaneously. 
\cite{benzi2000preconditioning}. 
%Fast converging algorithms for delay-sensitive applications use a sparse collection of switching configurations by leveraging Frank-Wolfe methods and block-coordinate descent \cite{valls2021birkhoff}. 
It also helps us in our goal to derive the rate region in Sect. \ref{sec:problem_statement} for distributed computing of %permutation invariant 
functions.

\begin{comment}
In the following, we represent each of the integral matchings $B_l$ using column vectors ${\bf e}_i$'s. 
We next demonstrate a Birkhoff-von Neumann decomposition of $W$.
%https://en.wikipedia.org/wiki/Birkhoff_algorithm

\begin{ex}\label{Birkhoff_example}
Let $W$ be a bistochastic matrix given as %$w_{ij}=0.2$ for $ij\in\{(1,1),(2,2),(2,3),(3,1)\}$, $w_{ij}=0.3$ for $ij\in\{(1,2),(3,3)\}$, $w_{ij}=0.5$ for $ij\in\{(1,3),(3,2)\}$, and $w_{21}=0.6$. 
$$W=\begin{bmatrix}
0.2 & 0.3 & 0.5\\
0.6 & 0.2 & 0.2\\
0.2 & 0.5 & 0.3
\end{bmatrix},$$
which can be decomposed as follows:
\begin{align}
W%=\sum\limits_{l\in[4]} q_l B_l
%=0.2 B_1+0.2 B_2+0.1 B_3 + 0.5 B_4\nonumber\\
&=0.2 \begin{bmatrix}
{\bf e}_3 & {\bf e}_1 & {\bf e}_2
\end{bmatrix}
+0.2 
\begin{bmatrix}
{\bf e}_1 & {\bf e}_2 & {\bf e}_3
\end{bmatrix}\nonumber\\
&+0.1
\begin{bmatrix}
{\bf e}_2 & {\bf e}_1 & {\bf e}_3
\end{bmatrix}
+0.5
\begin{bmatrix}
{\bf e}_2 & {\bf e}_3 & {\bf e}_1
\end{bmatrix},\nonumber
\end{align}
where  %$\theta_1=0.2$, $\theta_2=0.2$, $\theta_3=0.1$, and $\theta_4=0.5$, and  
${\bf e}_i$'s form the standard basis of $\mathbb{R}^3$.
\end{ex}
\end{comment}

%%%%%
\section{Problem Statement and Results}
\label{sec:problem_statement}

We here consider a pair of partially distributed sources $X_1,X_2$ that accept a joint PMF $P_{X_1,X_2}(x_1,x_2)>0$ for all $(x_1,x_2)$. 
Our goal is to derive the rate region for the distributed computation of $f(X_1,X_2)$ %in (\ref{SymmetricFunction}), 
exploiting %the symmetry incurred by the permutation invariance of $f(\cdot)$, and 
the fact that $(X_1,X_2)$ is distributed according to a mixture PMF (cf.~\ref{mixture_distribution}). Such mixture assumption implies that the sources are matched with a high probability. Equivalently, we can see that for $\mathcal{B}_{X_1,X_2}=(\mathcal{X}_1,\mathcal{X}_2,E)$ being the bipartite graph representation of $P_{X_1,X_2}$ --- with vertex sets $\mathcal{X}_1$ and $\mathcal{X}_2$ representing the individual source outcomes, and edge set $E$ whose weights capture $P_{X_1,X_2}$ --- then the vertices of $\mathcal{B}_{X_1,X_2}$ are non-matched with a vanishing probability. 
To derive the sought rate region, we next detail how to represent and then exploit this structural decomposition of mixture distributions.





%%%
\subsection{Representation of Matchings}
\label{sec:representation_matchings}

To now explore and exploit the implications of having integral matchings, let us consider $X_2$. 
%The possibility of representing a bistochastic matrix $W$ as a convex combination of integral matchings has implications in distributed computing. 
Let $X_2=\pi(X_1,\Theta)$ such that the permutation index $l\in [L]$ leads to a matching configuration described by the mixture parameter $\Theta=\theta_l$, as introduced in (\ref{mixture_distribution}). 
%
We let ${\bf q}=(q _{1},\ldots ,q _{L})\in\mathbb{R}^L_{>0}$ where $q_l=P_{\Theta}(\theta_l)$ such that $\sum _{l\in[L]}q _{l}=1$, for an integer $L\in O(N^2)$  \cite{caron1996nonsquare}, and a set of $N\times N$ permutation matrices $\{B_l,\, l\in[L]\}$.  
%
Let $\pi_l: \mathcal{X}_1\to \mathcal{X}_1$ be a one-to-one and onto function that permutes the elements of $\mathcal{X}_1$ to provide a perfect matching where every vertex is adjacent to exactly one edge % the vertices on one side are $n$ rows and the vertices on the other side are the $n$ columns
in $\mathcal{B}_{X_1,X_2}$, which simply means that $X_2=\pi_l(X_1)$. 
%
Given the perfect integral matching structure in $B_l$ given $l\in [L]$ and the value of $X_1$, say $X_1=x_{1}^k$, the source $X_2$ takes the value $\pi_l(x_{1}^k)$ and the function $f(X_1,X_2)$ in takes the form 
\begin{align}
\label{general_function_mixture}
f(x_{1}^k,\pi_l(x_{1}^k)) \ ,\quad \forall k\in [N]\ ,
\end{align}
with probability $q_l \mathbb{P}(X_1=x_{1}^k\, \vert \theta_l)$. 
We note that the outcome of $f(X_1,X_2)$ is completely determined by $B_l$ and $X_1$.


We next state our lemma, which allows a standard form for any joint PMF with positive entries.
%%%%%%%%%%%%
\begin{comment}
We next state Sinkhorn's theorem, which allows a standard form for any square matrix with positive entries.

\begin{theo}\label{Sinkhorn}
{\bf (Sinkhorn's theorem \cite{sinkhorn1964relationship}.%, \cite{marshall1968scaling}
).}
If $A\in \mathbb{R}^{N\times N}$ is a matrix with strictly positive elements, then there exist diagonal matrices $D_1$ and $D_2$ with strictly positive diagonals such that $W=D_1 A D_2$ is doubly stochastic. The matrices $D_1$ and $D_2$ are unique up to a scalar factor. %are unique modulo multiplying the first matrix by a positive number and dividing the second one by the same number.
%
Note also that $W$ is the only doubly stochastic matrix expressible in the form $W=D_1 A D_2$ for the given %strictly positive 
$A$, where %the diagonal matrices 
$D_1$ and $D_2$ are with positive diagonals. 
\end{theo}
\end{comment}

\begin{lem}
\label{lemma:weighted_sum}
Provided that $P_{X_1,X_2}(x_1,x_2)>0$ for all $(x_1,x_2)$, the joint PMF $P_{X_1,X_2}$ can be written as a weighted sum of generalized permutation matrices:
\begin{align}
\label{joint_distribution_mixture}
P_{X_1,X_2}(x_1,x_2)=\sum\limits_{l\in[L]}q_{l}\,P_{X_1,X_2\vert \Theta}(x_1,x_2\vert \theta_l)\ .
\end{align}
\end{lem}


\begin{proof}
Under our assumption that $P_{X_1,X_2}(x_1,x_2)>0$ for all $(x_1,x_2)$, then we first employ Sinkhorn’s Theorem \cite{sinkhorn1964relationship} to note that $P_{X_1,X_2}$ can be mapped into a doubly stochastic $W=D_1 P_{X_1,X_2} D_2$ for unique diagonal matrices $D_1=(d_{1,ij})$ and $D_2=(d_{2,ij})$ in $ \mathbb{R}_{>0}^{N\times N}$. 
Then we proceed to employ the famous Birkhoff–von Neumann Theorem \cite{caron1996nonsquare} to note that $W=\sum\nolimits_{l\in[L]}q _{l}B_{l}$, and $P_{X_1,X_2}$ is a weighted sum of generalized permutation matrices, thus taking the %stated 
form in (\ref{joint_distribution_mixture}), 
%\begin{align}
%P_{X_1,X_2}(x_1,x_2)=\sum\limits_{l\in[L]}q_{l}\,P_{X_1,X_2\vert \Theta}(x_1,x_2\vert \theta_l)\ ,\nonumber
%\end{align}
where $P_{X_1,X_2\vert \Theta}(\cdot,\cdot \vert \theta_l)=D_1^{-1}B_{l}D_2^{-1}$ is a generalized permutation matrix obtained from $\{B_l,\, l\in[L]\}$ where $L\in O(N^2)$, and the matrices $D_1^{-1}$ and $D_2^{-1}$ are such that $D_1^{-1}=(d^{-1}_{1,ij})$ and $D_2^{-1}=(d^{-1}_{2,ij})$ in $ \mathbb{R}_{>0}^{N\times N}$, and the mapping $\pi_l: \mathcal{X}_1\to \mathcal{X}_1$ from $X_1$ to $X_2$ captures $P_{X_1,X_2\vert \Theta}(x_1,x_2\vert \theta_l)$.
\end{proof}

%%%%




%%%%
\subsection{Identifying Structures of $\mathcal{B}_{X_1,X_2}$ via Matchings}
\label{sec:identification_structures}

There exist techniques to exploit the combinatorial structure of $\mathcal{B}_{X_1,X_2}$ when it has multiple connected components, see e.g., the G{\'a}cs-K{\"o}rner-Witsenhausen common information (GKW-CI)  \cite{gacs1973common%,witsenhausen1975sequences
}. 
\begin{comment}
This CI variable leverages the bipartitions %information 
and represents the index of the connected component 
in $\mathcal{B}_{X_1,X_2}$. 
The CI can be extracted %achieve 
by employing a helper, possibly with a limited rate. 
%
Exploiting the structural information in $\mathcal{B}_{X_1,X_2}$ can facilitate low-complexity \derya{efficient}  computation \derya{as the helper needs to be informed at a low rate....the helper does not need to know the value of $X_1$ and $X_2$ if they are not part of the cut.}. 
\end{comment}
However, if $\mathcal{B}_{X_1,X_2}$ has only one bipartition, GKW-CI cannot be extracted, i.e., GKW-CI is zero. %because the maximal number of connected components in $\mathcal{B}_{X_1,X_2}$ is 1, 
 

A helper-based scheme can still provide a low-complexity distributed coding technique even when $\mathcal{B}_{X_1,X_2}$ has one bipartition, despite being sub-optimal in terms of its operating rate.  
We are motivated to contemplate a matching-based helper as maximum matching in %general
graphs can be determined using polynomial time algorithms   \cite{edmonds1965paths}, which can be exploited to facilitate the Birkhoff-von Neumann decomposition. 
%
%Motivated by mixture distributions, a relevant problem is finding a (perfect) matching in $\mathcal{B}_{X_1,X_2}$. 
When $P_{X_1,X_2}$ is a mixture distribution satisfying (\ref{joint_distribution_mixture}) that accepts a matching-based decomposition, a helper can be used to distinguish the matched and non-matched vertices of $\mathcal{B}_{X_1,X_2}$. 
%A maximal matching can be found with a simple greedy algorithm (A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage. In many problems, a greedy strategy does not produce an optimal solution, but a greedy heuristic can yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.). A maximum matching is also a maximal matching, and hence it is possible to find a largest maximal matching in polynomial time. 
%
In the following, we devise a helper-based model to extract the matching information from $\mathcal{B}_{X_1,X_2}$, and denote this helper variable by $\Kmatch$. 
For the described partial distributed setting, Theorem \ref{RateRegion_matching} provides the rate region for computing  (\ref{general_function_mixture}). 
%It is also unclear why permutation invariance is needed in the results (e.g. Theorem 2). It seems that the theorem is valid for general function f.
%\derya{We note that Theorem \ref{RateRegion_matching} can easily be extended to more general functions $f(X_1,X_2)$ beyond (\ref{general_function_mixture}) where $X_1$ and $X_2$ are not necessarily over the same $\mathcal{X}$.}

%Identifying matchings, chooses to use a greedy approximation algorithm for maximal matching. Why? Note that polynomial time algorithms are known for maximum matchings in general graphs; indeed, this was the main result of the classic paper by Edmonds that defined notions of computational efficiency in terms of polynomial running time.
%https://www.cambridge.org/core/services/aop-cambridge-core/content/view/08B492B72322C4130AE800C0610E0E21/S0008414X00039419a.pdf/paths_trees_and_flowers.pdf





\begin{comment}For any given $l$, ${\bm X}_2=B_l{\bm X}_1$ can be determined as function of ${\bm X}_1=\begin{bmatrix}
X_1 &
X_2 &
X_3
\end{bmatrix}^{\intercal}$ as follows:
\begin{align}
\begin{bmatrix}
X_2 &
X_3 &
X_1
\end{bmatrix}^{\intercal}&=B_1{\bm X}_1,\quad 
\begin{bmatrix}
X_1 &
X_2 &
X_3
\end{bmatrix}^{\intercal}=B_2{\bm X}_1 \ ,\nonumber\\
\begin{bmatrix}
X_2 &
X_1 &
X_3
\end{bmatrix}^{\intercal}&=B_3{\bm X}_1,\quad
\begin{bmatrix}
X_3 &
X_1 &
X_2
\end{bmatrix}^{\intercal}=B_4{\bm X}_1 \ .\nonumber
\end{align}
\end{comment}




%%%%
\begin{theo}\label{RateRegion_matching}
({\bf A matching-based computation sum rate.}) 
For computing $f(X_1,X_2)$, where $P_{X_1,X_2}$ is given by the mixture PMF in (\ref{joint_distribution_mixture}), with a helper that extracts the perfect matching between the sources, there exists a low-complexity zero-error encoding and decoding of $X_1$ and $\Theta$ that operates at rates
\begin{align}
R_{H}\geq H(\Kmatch)=H(\Theta),\quad
R_1\geq H_{G_{X_1}}(X_1\vert \Theta)\ ,\nonumber    
\end{align}
yielding the following total rate to compute $f(X_1,X_2)$:
\begin{align}
R_M \geq H(\Kmatch)+H_{G_{X_1}}(X_1\vert \Theta) \ ,
\end{align}
where $H_{G_{X_1}}(X_1\vert \Theta)$ denotes the conditional characteristic graph entropy of $X_1$ for computing $f(X_1,X_2)$ given $\Theta$.  
\end{theo}

\begin{proof}
In the partially distributed setting, to be able to compute $f({\bf X}_1^n,{\bf X}_2^n)$ accurately, the perfect matching variable ${\bm \Theta}^n$ where $\Theta_i\sim {\bf q}$ for all $i\in [n]$ should be made available to the user (or via side information), which requires an asymptotic rate $R_H\geq H(K_M)=H(\Theta)$. Knowledge of ${\bm \Theta}^n$ determines the jointly typical coloring sequence pairs $(c_{G_{X_1}}({\bf X}_1^n),\,c_{G_{X_2}}({\bf X}_2^n))$ to be compressed asymptotically. %in (\ref{color_class_distribution}).
%A source builds a characteristic graph $G_{X_m}=(V_{X_m},E_{X_m})$ for computing $f(X_1,X_2)$ to distinguish the source outcomes that yield a different output for any value of $X_{m'}$ for $m=1,2$ and $m'\neq m$. Vertices are the sample values, i.e., $V_{X_m}=\mathcal{X}_m$, and the edges are determined as follows. Given two arbitrary vertices $(x_m^{k_1}, x_m^{k_2}) \in \mathcal{X}_m^2$ in $G_{X_m}$ such that $k_1\neq k_2$, if $\exists$ a vertex $x_{m'}^{l}\in \mathcal{X}_{m'}$ in $G_{X_{m'}}$ where $m'\neq m$ such that $P_{X_m,X_{m'}}(x_m^{k_1},x_{m'}^{l})P_{X_m,X_{m'}}(x_m^{k_2},x_{m'}^{l})>0$ and the function satisfies $\Phi(x_m^{k_1}+x_{m'}^{l})\neq \Phi(x_m^{k_2}+x_{m'}^{l})$, then $(x_m^{k_1},x_m^{k_2})\in E_{X_m}$. Otherwise, $(x_m^{k_1},x_m^{k_2})\notin E_{X_m}$.

Given that the perfect matching variable $\Theta\sim {\bf q}$ is known at the user, 
such that $X_2=\pi(X_1,\Theta)$, it is sufficient for the user if only one source transmits. Let us assume source one is selected. %Due to perfect matching, the rates do not change when source 2 is selected instead. 
%
Source one builds the characteristic graph $G_{X_1}$ to compute $f({\bf X}_1^n,{\bf X}_2^n)=f({\bf X}_1^n,\pi({\bf X}_1^n,{\bf\Theta}^n))$ given ${\bf\Theta}^n$. This requires, following the notion of the conditional graph entropy, as detailed in \cite{OR01}, an asymptotic rate of $H_{G_{X_1}}(X_1\vert \Theta)$.

The rate needed from source one to compute $f(X_1,X_2)$ is %By averaging over the distribution ${\bf q}$,
\begin{align}
\label{rate_one_source_permutation_invariant}
R_1%\geq H(\Phi(X_1+X_2)\,\vert \,\Theta)\nonumber\\
\geq\sum\limits_{l\in[L]}  q_l H(f(X_1,X_2)\,\vert \, X_2=\pi_l(X_1)) \ ,
\end{align} 
%which is less than $H(f(X_1,X_2))=H(\Phi(X_1+X_2))$ as conditioning reduces the entropy. %$H(\Phi(X_1+X_2))\geq H(\Phi(X_1+X_2)\, \vert\, X_2=\pi_l(X_1))$ for all $l\in[L]$
enabling the partial distributed computation of $f(X_1,X_2)$ at an asymptotic rate upper bounded by $H(f(X_1,X_2)\, , \,  \Theta)$. 
\end{proof}

\begin{comment}
In the special case when $f(X_1,X_2)=(X_1,X_2)$ (permutation variant), where $P_{X_1,X_2}$ satisfies (\ref{joint_distribution_mixture}), Prop. \ref{RateRegion_matching} gives
\begin{align}
R_H\geq H(\Theta),\quad
R_1\geq H(X_1\vert \Theta)\ .\nonumber    
\end{align}
\end{comment}



The rate region in Theorem \ref{RateRegion_matching} is encompassed by that of optimal distributed functional compression given in \cite{feizi2014network}. 
On the other hand, when $P_{X_1,X_2}$ accepts the decomposition in (\ref{joint_distribution_mixture}), a helper-based model to extract the matching information has advantages over Slepian-Wolf coding. Not only it eliminates the complexity associated with joint typicality decoding \cite{cover2012elements} with exponential complexity, but also provides an almost lossless compression asymptotically, given by Theorem \ref{RateRegion_matching}. 

We next impose additional structure on the distributed sources by exploiting the maximal coupling construction. %to maximize the probability that the sources are maximally matched 


%%%%%
\subsection{Extracting Matched versus Non-Matched Vertices}
\label{sec:matching_nonmatching}

In this section, we capture the matchings between the pair of partially distributed sources $X_1$ and $X_2$, which are maximally coupled. 
%
A maximal coupling between %two PMFs ${\bf p}_1$ and ${\bf p}_2$ over the same $\mathcal{X}$ is a PMF of 
a pair $(X_1,X_2)$ %that 
maximizes $\mathbb{P}(X_1 = X_2)$ subject to the marginal PMFs $X_1\sim {\bf p}_1%=(p_{11},\hdots, p_{1|\mathcal{X}_1|})
$ and $X_2\sim {\bf p}_2%=(p_{21},\hdots, p_{2|\mathcal{X}_2|})
$. %on the same $\mathcal{X}$. 
%
%where $p_{1k}$ and $p_{2k}$ denote their respective masses at the points in $\mathcal{X}$. 
Let $C({\bf p}_1,\,{\bf p}_2)$ be the set of all joint PMFs of $X_1\sim {\bf p}_1$ and $X_2\sim {\bf p}_2$. %A coupling of a pair of random variables is a joint distribution with the known marginals. 
Elements $M=[m_{ij}]~\in\mathbb{R}_{\geq 0}^{|\mathcal{X}|\times |\mathcal{X}|}$ of $C({\bf p}_1,\,{\bf p}_2)$ are couplings of ${\bf p}_1$ and ${\bf p}_2$:
\begin{align}
C({\bf p}_1,\,{\bf p}_2)\triangleq\Big\{%M=
\big[m_{ij}\,:\sum_{j\in |\mathcal{X}|} m_{ij}=p_{1i},\,\,%\nonumber\\ 
\sum_{i\in |\mathcal{X}|} m_{ij}=p_{2j}\big]\Big\} \ .\nonumber
\end{align}
%Each element is a matrix $M=[m_{ij}]\in\mathbb{R}^{|\mathcal{X}_1|\times |\mathcal{X}_2|}$ such that row sums give elements of ${\bf p}_2$ and column sums give elements of ${\bf p}_1$.
%For simplicity, we assume $\mathcal{X}=\mathcal{X}_1=\mathcal{X}_2$. If the alphabets were different, we can fill each $M\in C({\bf p}_1,\,{\bf p}_2)$ in with $m_{ij}=0$ when $p_{1i}=0$ or $p_{2j}=0$. 
%
\begin{comment}
Let $X_1$ and $X_2$ be discrete random variables with alphabets $\mathcal{X}_1$ and $\mathcal{X}_2$, and marginals $X_1\sim {\bf p}_1=(p_{11},\hdots, p_{1n_1})$ and $X_2\sim {\bf p}_2=(p_{21},\hdots, q_{2n_2})$,  respectively, where $p_{1k}$ and $p_{2k}$ denote their respective masses at the points $x_k$. Let $C({\bf p}_1,\,{\bf p}_2)$ be the set of all joint distributions of $X_1$ and $X_2$ with marginals ${\bf p}_1$ and ${\bf p}_2$, respectively. %A coupling of a pair of random variables is a joint distribution with the known marginals. 
Hence, elements of $C({\bf p}_1,\,{\bf p}_2)$ are couplings of ${\bf p}_1$ and ${\bf p}_2$. Each element is a matrix $M=[m_{ij}]\in\mathbb{R}^{n_1\times n_2}$ such that row sums give elements of ${\bf p}_2$ and column sums give elements of ${\bf p}_1$:
\begin{align}
    C({\bf p}_1,\,{\bf p}_2)\triangleq\Big\{M=[m_{ij}:\sum_{j=1}^{n_2} m_{ij}=p_{1i},\,\, \sum_{i=1}^{n_1} m_{ij}=p_{2j}]\Big\}.
\end{align}
\end{comment}
%
\begin{comment}
%\begin{prop} %Grimmett 4.12-Exercises 4
The coupling inequality states that \cite[Ch. 4.12]{grimmett2001probability}
\begin{align}
\label{CouplingInequality}
\mathbb{P}(X_1\neq X_2)\geq \norm{{\bf p}_1-{\bf p}_2}_{\rm{TV}}\triangleq \frac{1}{2}\sum\limits_{k\in [|\mathcal{X}|]} |p_{1k}-p_{2k}| \ ,
\end{align}
where the measure $\norm{{\bf p}_1-{\bf p}_2}_{\rm{TV}}$ is the total variation distance between the probability distributions of $X_1$ and $X_2$.
%\end{prop}
%Proof: https://www.math.wisc.edu/~roch/mdp/roch-mdp-chap4.pdf
\end{comment}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
%https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures

%Problem 7.11.16. from Grimmett Parts a and b

The total variation distance $d_{\rm TV}(X,\,Y)$ between two random variables $X$ and $Y$ is defined by
\begin{align}
d_{\rm TV}(X,\,Y) = \sup\limits_{u:\norm{u}_{\infty}=1} |\mathbb{E}[u(X)]-\mathbb{E}[u(Y)]|     
\end{align}
where the supremum is over all (measurable) functions $u: \mathbb{R} \to \mathbb{R}$ such that $\norm{u}_{\infty}=\sup_x |u(x)|$ satisfies $\norm{u}_{\infty}=1$.



(a) The total variation distance $d_{\rm TV}(X_1,\,X_2)$ between $X_1$ and $X_2$ is defined by
\begin{align}
d_{\rm TV}(X_1,\,X_2) = \sum\limits_k |p_k-q_k|.\nonumber\\
&=2\sup\limits_{A\subseteq\mathbb{R}} |\mathbb{P}(X_1\in A)-\mathbb{P}(X_2\in A)|.\nonumber
\end{align}
 
(b) If $X$ and $Y$ are continuous with respective density functions $f$ and $g$,
\begin{align}
d_{\rm TV}(X,\,Y) &=\int\nolimits_{-\infty}^{\infty} |f(x)-g(x)|{\rm d}x\nonumber\\
&= 2\sup\limits_{A\subseteq\mathbb{R}} |\mathbb{P}(X\in A)-\mathbb{P}(Y\in A)|.\nonumber
\end{align}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





 



A coupling of $(X_1,\,X_2)$ that maximizes $\mathbb{P}(X_1=X_2)$ %, i.e., achieves equality in (\ref{CouplingInequality}) 
is called a maximal coupling which is formally stated next.
%https://colcarroll.github.io/couplings/static/maximal_couplings.html
\begin{lem}\label{maximumcoupling}
Maximal coupling $(X_1, X_2 )$ subject to the marginal distributions $X_1\sim {\bf p}_1$ and $X_2\sim {\bf p}_2$ satisfies %https://www.math.wisc.edu/~roch/mdp/roch-mdp-chap4.pdf
\begin{align}
\label{MaximalCoupling}
%\norm{{\bf p}_1-{\bf p}_2}_{\rm{TV}}= \inf[\mathbb{P}(X_1 \neq X_2): \mbox{coupling }(X_1, X_2 )\, \mbox{of}\, {\bf p}_1\, \mbox{and} \,{\bf p}_2].
\norm{{\bf p}_1-{\bf p}_2}_{\rm{TV}}= \inf\ [\mathbb{P}(X_1 \neq X_2): \,C({\bf p}_1,\,{\bf p}_2)] \ ,%: (X_1,\,X_2) a coupling of distributions {\bf p}_1,{\bf p}_2}
\end{align}
where the measure $\norm{{\bf p}_1-{\bf p}_2}_{\rm{TV}}$ is the total variation distance between the PMFs %probability distributions 
of $X_1$ and $X_2$.
\end{lem} 

From Lemma \ref{maximumcoupling}, if the sources $X_1$ and $X_2$ are maximally matched, the total variation distance between them is minimum.

Maximal coupling of the pair $(X_1,X_2)$ becomes relevant when the discrepancy between ${\bf p}_1$ and ${\bf p}_2$ is bounded above, e.g., the PMFs coincide %(with large probability) 
in the first two decimal points. Intuitively, this coupling %between the variables 
can be exploited to further reduce the communication cost for distributed computing.

%comment on the differences between Theorems 2 and 3. 
Recall that Theorem \ref{RateRegion_matching} derives a schedule-based lower bound on the sum rate given by Lemma \ref{lemma:weighted_sum}, irrespective of the coupling of $(X_1,X_2)$. 
%On the other hand, 
Next, Theorem \ref{RateRegionPermutationInvariant_maximal_coupling} 
provides a lower bound on the sum rate by assuming an additional structural correlation between $X_1$ and $X_2$ through their maximal coupling.

% Figure environment removed


\begin{theo}\label{RateRegionPermutationInvariant_maximal_coupling}
({\bf A maximally-coupled computation sum rate.}) 
Assume that the sources $X_1$ and $X_2$ are maximally coupled. %equality in (\ref{CouplingInequality}), 
Then, there exists a low-complexity zero-error encoding and decoding of $f(X_1,X_2)$ and $\Theta$ with a helper that extracts the perfect matching between the sources and operates at rates
\begin{align}
R_H+\sum\limits_{m\in[2]}R_m\geq H(\Theta)+\sum\limits_{l\in [L]}q_l \big(h(\delta_l)+\delta_l^c H(T_l)\nonumber
\end{align}
\begin{align}
\label{rate_maximal_coupling}
+\delta_l [H(V_l)+H(W_l)]\big) \ ,
\end{align}
where $V_l$, $W_l$, and $T_l$ are independent integer-valued %random 
variables with distributions given according to (\ref{Coupling_distributions}) with $\delta$ is substituted with $\delta_l=\mathbb{P}(\pi_l(X_1)\neq X_2)$ for given $l\in [L]$, and $\delta_l^c=1-\delta_l$.
\end{theo}

\begin{proof}
%Grimmett 4.12-Exercise 5. It also refers to Problem 7.11.16d.
To prove this result, we provide a construction for maximal coupling from %for discrete random variables 
\cite[Ch. 4.12]{grimmett2001probability}.  
There exists a pair $X_1'$ and $X_2'$ having the same marginals as $X_1$ and $X_2$ such that $\mathbb{P}(X_1'=X_2')= 1-\norm{{\bf p}_1-{\bf p}_2}_{\rm{TV}}=1-\delta$. %, i.e., equality in (\ref{MaximalCoupling}) is possible.
%We denote a Bernoulli distributed random variable $X$ with success probability $a$ by $X\sim$Bern$(a)$.
Let $U\sim$Bern$(1-\delta)$ %be a Bernoulli distributed variable, 
and $V$, $W$, $T$ be independent integer-valued variables with respective masses at $k=1,\dots,|\mathcal{X}|$:
\begin{align}
\label{Coupling_distributions}
\mathbb{P}(T=k)&\triangleq \frac{\min[p_{1k},\,p_{2k}]}{(1-\delta)},\\
\mathbb{P}(V=k)\triangleq \frac{[p_{1k}-p_{2k}]^+}{\delta},&\quad
\mathbb{P}(W=k)\triangleq \frac{[p_{2k}-p_{1k}]^+}{\delta} \ .\nonumber
\end{align}
Then, the random variables $X_1'$ and $X_2'$ defined as
\begin{align}
\label{maximally_coupled_discrete_RVs}
X_1' \triangleq U T + (1 - U) V,  \quad% \nonumber\\%
X_2' \triangleq U T + (1 - U) W
\end{align}
have the required marginals, and are maximally coupled such that $\mathbb{P}(X_1'=X_2')=\mathbb{P}(U=1)=1-\delta$. Furthermore, $X'_1$ and $X'_2$ are independent when $X'_1\neq X'_2$ as their sets are disjoint. %https://arxiv.org/pdf/2010.08573.pdf
%https://people.math.wisc.edu/~roch/mdp/roch-mdp-chap4.pdf
For the proof, we refer the reader to \cite[Ch. 4.12]{grimmett2001probability}. %See Exercise 5 

%%%%%
%We next provide a construction for maximal coupling of $X_1$ and $X_2$ using \cite[Ch. 4.12]{grimmett2001probability}. 
%
Let us consider the possible matchings described by $\Theta\sim {\bf q}$, where the matched vertices are invariant up to %certain 
permutations. For each $l\in[L]$ with probability $q_l=P_{\Theta}(\theta_l)$, we have
\begin{align}
\pi(X_1,\theta_l)=\pi_l(X_1)&= U_l T_l + (1 - U_l) V_l \ ,\nonumber\\
X_2&= U_l T_l + (1-U_l) W_l \ , \quad l\in [L] \ ,
\end{align}
where $U_l\sim$Bern$(1-\delta_l)$, and $V_l$, $W_l$, and $T_l$ are independent integer-valued variables with respective masses at $k\in [|\mathcal{X}|]$ according to (\ref{Coupling_distributions}) where $\delta$ being replaced by $\delta_l$.

A maximal coupling of $X_1$ and $X_2$ results in $$\mathbb{P}(X_2=\pi(X_1,\Theta))=\sum\limits_{l\in[L]} q_l \mathbb{P}(U_l=1) =\sum\limits_{l\in[L]} q_l \cdot \delta_l^c\ ,$$
where $\delta_l^c=1-\delta_l$. 
The parts corresponding to $U_l=1$ versus $U_l=0$  denote the matched and the non-matched components, respectively. This yields the sum rate needed from the helper and both sources to compute $f(X_1,X_2)$, as given in (\ref{rate_maximal_coupling}).
\end{proof}

To contrast our model with the state-of-the-art, we next consider an example. 
Due to limited space, we deferred several %results, proofs, and 
examples to the extended version of the draft \cite{Malak2022Structured}.

%%%%
\section{Example with Structured Sources}
\label{sec:examples}
%\vspace{-0.1cm}
We next study an example where a helper leverages the structure of $\mathcal{B}_{X_1,X_2}$, %(with one bipartition or more) 
and provides the necessary rate to distinguish the matching information, $K_M$, in $\mathcal{B}_{X_1,X_2}$. 
%
%This idea becomes relevant when extraction of CI via a GKW-CI-based helper to bipartition $\mathcal{B}_{X_1,X_2}$ is not possible.
%
Extracting this information alleviates the complexity of distributed computing provided that the non-matched distributions correspond to low-probability events. Given $K_M$, one source (or both) needs to send a refinement to identify the function outcome. 



In the following, we denote by $F$ the table of function outcomes, where the coordinates match the coordinates of $P_{X_1,X_2}$ with entries ordered in an increasing fashion. %However, $F$ might not be necessarily symmetric as $P_{X_1}\neq P_{X_2}$ and $P_{X_1,X_2}$ is not necessarily symmetric, i.e.,  $P_{X_1,X_2}(x_1,x_2)\neq P_{X_1,X_2}(x_2,x_1)$ for some given $(x_1,x_2)\in\mathcal{X}_1\times \mathcal{X}_2$. 
%We note that in some examples, $P_{X_1,X_2}(x_1,x_2)=0$ for some $(x_1,x_2)$ to be able to exploit the GKW-CI.



%%%%
\begin{ex}\label{SymmetricHighError}Consider the probability matrix and the table of %permutation invariant 
function outcomes given as follows:
\begin{align}
P_{X_1,X_2} = \frac{1}{3} \begin{bmatrix} 0 & 1-\delta & \delta \\ 1-\delta & 0 & \delta \\ \delta & \delta & 1-2\delta\end{bmatrix},\quad F= \begin{bmatrix} X & 1 & 4 \\ 1 & X & 3\\ 4 & 3 & 2\end{bmatrix},  \nonumber 
\end{align}
where $\delta\in (0,0.5)$. The bipartite graph $\mathcal{B}_{X_1,X_2}$ is shown in Fig. \ref{BipartiteGraphs_HighDelta} where the vertices are listed in the presented order. 






{\bf Fully distributed coding.} The entropy of the function for given $P_{X_1,X_2}$ and $F$ is $H(f(X_1,X_2)) = h\left(\frac{2-2\delta}{3},\frac{1-2\delta}{3},\frac{2\delta}{3},\frac{2\delta}{3}\right)$. A trivial rate upper bound is $H_{G_{X_1}}(X_1)+H_{G_{X_2}}(X_2) = 2\log 3$. The sum rate required to compute the function in the case of no helper is $H_{G_{X_1}}(X_1)+H_{G_{X_2}}(X_2\vert X_1) = \log(3)+\frac{2}{3} h(\delta)+\frac{1}{3} h(\delta,\delta,1-2\delta)$.   

%%%%%%
{\bf Partially distributed coding via extracting matchings.} The helper decomposes $\mathcal{B}_{X_1,X_2}$ into a perfect matching and a non-matched graph. More specifically, $P_{X_1,X_2}$ can be described by the following mixture distribution: 
\begin{align}
P_{X_1,X_2} %= \frac{1}{3} \begin{bmatrix} 0 & 1-\delta & 0 \\ 1-\delta & 0 & 0\\ 0 & 0 & 1-2\delta\end{bmatrix}+\frac{\delta}{3} \begin{bmatrix} 0 & 0 & 1 \\ 0 & 0 & 1\\ 1 & 1 & 0\end{bmatrix},\nonumber\\
=\graphweight_1 \begin{bmatrix} 0 & \frac{1-\delta}{3-4\delta} & 0 \\ \frac{1-\delta}{3-4\delta} & 0 & 0\\ 0 & 0 & \frac{1-2\delta}{3-4\delta}\end{bmatrix}+\graphweight_2 \begin{bmatrix} 0 & 0 & \frac{1}{4} \\ 0 & 0 & \frac{1}{4}\\ \frac{1}{4} & \frac{1}{4} & 0\end{bmatrix},\nonumber
\end{align}
where $\delta\in(0,0.5)$, the first matrix describes a perfect matching, denoted by $\Kmatch=0$ with probability $\graphweight_1=1-\frac{4\delta}{3}$, and the second matrix describes a low-probability event, denoted by $\Kmatch=1$ with  $\graphweight_2=\frac{4\delta}{3}$. Hence, the rate required from the helper to distinguish between these two is $H(\Kmatch)=h\left(\graphweight_1\right)$. 
Given $\Kmatch=0$, only 1 source needs to transmit, which requires a rate of $h\Big(\frac{2-2\delta}{3-4\delta}\Big)$ to determine the function outcome. 
Given $\Kmatch=1$, source 1, $X_1$, needs 2 colors for $u_1$ and $u_2$ to distinguish the outcomes $4$ and $3$ %that are equally likely 
each with a probability $\frac{1}{4}$, and $u_3$, which has a probability $\frac{1}{2}$, does not need to be distinguished from $\{u_1,u_2\}$ (no shared edges between $\{u_1,u_2\}$ and $u_3$). Similarly for $X_2$. Hence, given $\Kmatch=1$, each source is required to send at an asymptotic rate of $h\left(\frac{3}{4}\right)$ bits per use. Hence, the sum rate required to compute $F$ with a helper that exploits the matching information of $\mathcal{B}_{X_1,X_2}$ is 
\begin{multline}
R_M\geq H(\Kmatch)+\mathbb{P}(\Kmatch=0)H_{G_{X_1}}(X_1\vert \Kmatch=0) \nonumber\\ 
+\mathbb{P}(\Kmatch=1)(H_{G_{X_1}}(X_1\vert \Kmatch=1)+(H_{G_{X_2}}(X_2\vert \Kmatch=1))\nonumber\\
=h\left(\graphweight_1\right)+\graphweight_1 h\Big(\frac{1-2\delta}{3-4\delta}\Big)+\graphweight_2\Big(h\left(\frac{3}{4}\right)+h\left(\frac{3}{4}\right)\Big)\ .\nonumber
\end{multline}
\end{ex}

\begin{comment}
We compare the rates of entropy coding, coloring, and helper-based schemes in Fig. \ref{fig:simulations_all}-(b). %\ref{fig:BipartiteGraph_HighDelta}. 
The benefit of the helper is eminent when $\delta$ is low. The helper needs to identify whether the edge between the bipartitions exists as a function of $\delta$, which requires $2h\left(\frac{\delta}{3},\frac{\delta}{3},1-\frac{2\delta}{3}\right)$ bits per use asymptotically. Note that compared to Example \ref{SymmetricLowError} in Fig. \ref{fig:simulations_all}-(a),  
the rate required from the helper to identify the right bipartition is higher because in this example the graph has a higher likelihood to be complete and it is preferred that the sources jointly compress their characteristic graphs. As $\delta$ increases, the graph becomes %almost fully 
connected with high probability, and the gap between the rates of the helper-based scheme and entropy coding increases.
\end{comment}

From above, our matching-based approach has the best approximation for small $\delta$, where the gain of our model over the fully distributed coding setting that exploits the structure of the function but not the source is $\% 42$ and the loss versus 
the fundamental limit $H(f(X_1,X_2))$ is at most $\% 26$.



\begin{comment}
We compare the rates of entropy coding, coloring, and helper-based schemes in Fig. \ref{fig:simulations_all}-(b). %\ref{fig:BipartiteGraph_HighDelta}. 
The benefit of the helper is eminent when $\delta$ is low. The helper needs to identify whether the edge between the bipartitions exists as a function of $\delta$, which requires $2h\left(\frac{\delta}{3},\frac{\delta}{3},1-\frac{2\delta}{3}\right)$ bits per use asymptotically. Note that compared to Example \ref{SymmetricLowError} in Fig. \ref{fig:simulations_all}-(a),  
the rate required from the helper to identify the right bipartition is higher because in this example the graph has a higher likelihood to be complete and it is preferred that the sources jointly compress their characteristic graphs. As $\delta$ increases, the graph becomes %almost fully 
connected with high probability, and the gap between the rates of the helper-based scheme and entropy coding increases.
\end{comment}





\begin{comment}
We next consider an example where $(X_1,X_2)$ is maximally coupled for the helper to fully exploit the structural matchings.

%%%%
\begin{ex}({\bf Low probability non-matchings.})
Consider a joint distribution $P_{X_1,X_2}$ with a decomposition %in Example \ref{Example_joint_distribution_X1_skewed} where modifying $P_{X_1,X_2}$ in (\ref{Example_joint_distribution_X1_skewed_formula}) 
given as
\begin{align}
\label{Example_joint_distribution_X1_skewed_formula_nonmatched}
P_{X_1,X_2}=\begin{bmatrix}
0.05 & 0.1-\frac{2\delta_2}{5}+\frac{7\delta_3}{20} & 0.05\\
0.1+\frac{2\delta_2}{5} & 0.2 & 0.2\\
0.1+\frac{\delta_1}{4} & 0.1-\frac{\delta_1}{4} & 0.1-\frac{7\delta_3}{20}
\end{bmatrix}\hspace{0.6cm}&\\
=q_1\begin{brsm}
0 & 0 & \frac{1}{5}\\
\frac{2}{5} & 0 & 0\\
\delta_1 & \frac{2}{5}-\delta_1 & 0
\end{brsm}
+q_2\begin{brsm}
0 & \frac{1}{4}-\delta_2 & 0\\
\delta_2 & 0 & \frac{1}{2}\\
\frac{1}{4} & 0 & 0
\end{brsm}
+q_3\begin{brsm}
\frac{1}{7} & \delta_3 & 0\\
0 & \frac{4}{7} & 0\\
0 & 0 & \frac{2}{7}-\delta_3
\end{brsm}&\ , \hspace{1.3cm} \nonumber
\end{align}
where ${\bf q}=(0.25,0.4,0.35)$, and $\delta_l$'s model the deviation of the joint PMF from the matching (i.e., $\delta_l>0$, $l\in [3]$).

Letting $\delta_l^c=1-\delta_l$, the sum rate for evaluating $P_{X_1,X_2}$ can be determined using (\ref{rate_maximal_coupling}), 
%\begin{comment}
\begin{align}
R_M&\geq h({\bf q})+q_1\Big(h(\delta_1)+\delta_1^c H(\frac{2}{5\delta_1^c},\frac{2}{5\delta_1^c}-\frac{\delta_1}{\delta_1^c},\frac{1}{5(\delta_1^c})\Big)\nonumber\\
&+q_2\Big(h(\delta_2)+\delta_2^c H(\frac{1}{4\delta_2^c}-\frac{\delta_2}{\delta_2^c},\frac{1}{2\delta_2^c},\frac{1}{4\delta_2^c})\Big)\nonumber\\
&+q_3\Big(h(\delta_3)+\delta_3^c H(\frac{1}{7\delta_3^c},\frac{4}{7\delta_3^c},\frac{2}{7\delta_3^c}-\frac{\delta_3}{\delta_3^c})\Big) \ ,\nonumber
\end{align}
%\end{comment}
where $h({\bf q})=1.559$. 
Let $\delta_1=\frac{1}{5}$, $\delta_2=\frac{1}{8}$, and $\delta_3=\frac{1}{7}$. Hence, $h(\delta_1)=0.722$, $h(\delta_2)=0.544$, and $h(\delta_2)=0.592$. Then, evaluating the sum rate for these numbers, the required rate is $R_M\geq %1.559+0.25\Big(0.722+\frac{4}{5}h(\frac{2}{4},\frac{1}{4},\frac{1}{4})\Big)+0.4\Big(0.544+\frac{7}{8}h(\frac{1}{7},\frac{4}{7},\frac{2}{7})\Big)+0.35\Big(0.592+\frac{6}{7}h(\frac{1}{6},\frac{4}{6},\frac{1}{6})\Big)=
3.323$ bits.

%Based on Prop. \ref{RateRegionPermutationInvariant} (and its proof) why is the helper rate not $0.25\delta_1$? is it because of deterministic schedules?

Provided that the joint PMF %(\ref{Example_joint_distribution_X1_skewed_formula_nonmatched}) 
has the decomposition given by Lemma~\ref{lemma:weighted_sum}, %Theorems \ref{BirkoffTheorem} and \ref{Sinkhorn}, 
plugging in the $\delta_l$ values above, $P_{X_1,X_2}$ satisfies 
\begin{align}
\label{non-matchings}
P_{X_1,X_2}=\begin{bmatrix}
0.05 & 0.1 & 0.05\\
0.15 & 0.2 & 0.2\\
0.15 & 0.05 & 0.05
\end{bmatrix},\hspace{0.5cm}
\end{align}
which has the equivalent representation given as
\begin{align}
&\tilde{q}_1\begin{bmatrix}
0 & 0 & \frac{1}{5}\\
\frac{3}{5} & 0 & 0\\
0 & \frac{1}{5} & 0
\end{bmatrix}+\tilde{q}_2\begin{bmatrix}
0 & \frac{2}{9} & 0\\
0 & 0 & \frac{4}{9}\\
\frac{3}{9} & 0 & 0
\end{bmatrix}+\tilde{q}_3\begin{bmatrix}
\frac{1}{6} & 0 & 0\\
0 & \frac{4}{6} & 0\\
0 & 0 & \frac{1}{6}
\end{bmatrix}\ ,\nonumber
\end{align}
where ${\bf \tilde{q}}=(0.25,0.45,0.3)$, which yields $\mathbb{P}(X_2=\pi(X_1,\Theta))=0.25(1-\frac{1}{5})+0.4(1-\frac{1}{8})+0.35(1-\frac{1}{7})%=0.2+0.35+0.3
=0.85$.


To specify the mixing distribution with ${\bf \tilde{q}}=(0.25,0.45,0.3)$ %, i.e., to identify the weights of the mixture, 
the helper requires a rate of $R_H\geq H(\Theta)=h({\bf \tilde{q}})=1.54$. 
Furthermore, source $1$ needs to send at a rate $H(X_1\vert \Theta)=0.25h(\frac{1}{5},\frac{3}{5},\frac{1}{5})+0.45h(\frac{2}{9},\frac{4}{9},\frac{3}{9})+0.3h(\frac{1}{6},\frac{4}{6},\frac{1}{6})=1.407$, yielding %a sum rate of 
$R_M\geq 2.947$ bits. Hence, perfectly matched sources require a lower sum rate versus maximally matched ones. 



Using $P_{X_1,X_2}$ in (\ref{non-matchings}), the achievable sum rate %using the distributed source coding scheme 
of the Slepian-Wolf scheme is $H(X_1,X_2)%=h(P_{X_1,X_2})
=2.946$ bits \cite{SlepWolf1973}.

\end{ex}
\end{comment}

%\derya{summarize the takeaway messages from these examples.}





%\begin{spacing}{0.83}
\bibliographystyle{IEEEtran}
\bibliography{references}
%\end{spacing}



\begin{comment}
%%%%%
\subsection{Representation of Permutation Invariance} 
\label{sec:CompressionRates}

While our model focuses on distributed computing of general functions, as a special practical case, we will exploit the structure of permutation invariant functions, via using the decomposition principle devised by Zaheer {\em et al.} for such class of functions \cite{zaheer2017deep}. Indeed, many of the functions accept the main principle of permutation invariance.

Motivated by the above, we here consider the class of functions %\emph{permutation invariant functions}, 
which will also take as inputs data from sources that themselves enjoy a strong joint structure.  This same class of permutation invariant functions enjoys applicability in a variety of modern practical scenarios that include multi-agent reinforcement learning, supervised and unsupervised learning including in estimation of population statistics, various combinatorial
problems, as well as problems in detection of symmetry \cite{zaheer2017deep}. Lately this same permutation invariance property has become key in enabling the design of a deep network architectures that can be deployed in learning scenarios \cite{zaheer2017deep}. %In particular, in this latter case, in order to save computing time, equivariant neural network architectures learn the equivalent states and symmetry in networks (for games, e.g., go and chess) have been devised in \cite{carroll2020finite}.

The structure of permutation invariant functions is characterized by  \cite{zaheer2017deep}, where the function outcome does not change by permuting the sources of the data. The class of such functions includes the summation, the mean, the majority, and the parity functions, among others \cite{Benso2003}.  


\begin{defi}\label{PermInv}
({\bf Permutation invariant functions \cite[Theorem 2]{zaheer2017deep}.}) 
A function $f(S)$ %operating 
on a set $S= \{S_1, \dots , S_M\}$ with elements $S_m\in S$ from a countable space, is a %valid 
set function\footnote{A set function associates an affinely extended real number, i.e., the set $\mathbb {R} \cup \{\pm \infty \}$, to each set in a collection of sets.} %, i.e., invariant to the permutation of elements, 
if and only if it can be decomposed as $\Phi\big(\sum\nolimits_{S_m\in S} \psi(S_m)\big)$, for suitable mappings $\psi$ and $\Phi$.
\end{defi}

%It is unclear what is meant by saying that $f(X_1,X_2)$ is permutation invariant, when $X_1$ and $X_2$ are real numbers instead of sets.
We let $X_m=\psi(S_m)$, $m\in[2]$ be suitable inner transformations implemented at each source associating a finite value from $\mathcal{X}$ to each set $S_m$. %independently, 
%where the range $\mathcal{X}$ of $\psi$ is a discrete space. %, e.g., in classification. %A continuous space can be quantized as well, e.g., in the case of regression, for a generalized rate-distortion scenario. 
We call the transformed variables $X_1$ and $X_2$ the source random variables. Hence, a permutation invariant function of two variables can be represented as 
\begin{align}
\label{SymmetricFunction}
\Phi(\psi(S_1)+\psi(S_2))=\Phi(X_1+X_2) = f(X_1,X_2)\ ,
\end{align}
which generalizes the summation problem through a univariate outer mapping $\Phi(\cdot)$, where the transformed variables $X_1$ and $X_2$ model two statistically dependent i.i.d. finite alphabet source sequences with alphabets $\mathcal{X}_1$ and $\mathcal{X}_2$, and are jointly distributed according to $P_{X_1,X_2}$. We assume that these sequences are independently encoded. 
%
We illustrate the block diagram that describes the proposed system model in Fig. \ref{Permutation_block_diagram_helper}.

% Figure environment removed

We note that under permutation invariance, 
the function $f(X_1,X_2)$ in (\ref{SymmetricFunction}) takes the form 
\begin{align}
f(x_{1}^k,\pi_l(x_{1}^k))=\Phi(x_{1}^k+\pi_l(x_{1}^k)) \ ,\quad \forall k\in [N]\ ,\nonumber
\end{align}
with probability $q_l \mathbb{P}(X_1=x_{1}^k\, \vert \theta_l)$. 
We note that the outcome of $f(X_1,X_2)$ is completely determined by $B_l$ and $X_1$.


\derya{Exploiting permutation invariance in Theorem \ref{RateRegion_matching}, where we will derive the rate region, see e.g., (\ref{rate_one_source_permutation_invariant}). We note that while we do not need to restrict ourselves to permutation invariant functions, this scenario is a nice application of our results.}

\begin{proof}
In the partially distributed setting, to be able to compute $\Phi({\bf X}_1^n+{\bf X}_2^n)$ accurately, the perfect matching variable ${\bm \Theta}^n$ where $\Theta_i\sim {\bf q}$ for all $i\in [n]$ should be made available to the user (or via side information), which requires an asymptotic rate $R_H\geq H(K_M)=H(\Theta)$. Knowledge of ${\bm \Theta}^n$ determines the jointly typical coloring sequence pairs $(c_{G_{X_1}}({\bf X}_1^n),\,c_{G_{X_2}}({\bf X}_2^n))$ to be compressed asymptotically. %in (\ref{color_class_distribution}).
%A source builds a characteristic graph $G_{X_m}=(V_{X_m},E_{X_m})$ for computing $f(X_1,X_2)$ to distinguish the source outcomes that yield a different output for any value of $X_{m'}$ for $m=1,2$ and $m'\neq m$. Vertices are the sample values, i.e., $V_{X_m}=\mathcal{X}_m$, and the edges are determined as follows. Given two arbitrary vertices $(x_m^{k_1}, x_m^{k_2}) \in \mathcal{X}_m^2$ in $G_{X_m}$ such that $k_1\neq k_2$, if $\exists$ a vertex $x_{m'}^{l}\in \mathcal{X}_{m'}$ in $G_{X_{m'}}$ where $m'\neq m$ such that $P_{X_m,X_{m'}}(x_m^{k_1},x_{m'}^{l})P_{X_m,X_{m'}}(x_m^{k_2},x_{m'}^{l})>0$ and the function satisfies $\Phi(x_m^{k_1}+x_{m'}^{l})\neq \Phi(x_m^{k_2}+x_{m'}^{l})$, then $(x_m^{k_1},x_m^{k_2})\in E_{X_m}$. Otherwise, $(x_m^{k_1},x_m^{k_2})\notin E_{X_m}$.

Given that the perfect matching variable $\Theta\sim {\bf q}$ is known at the user, 
such that $X_2=\pi(X_1,\Theta)$, it is sufficient for the user if only one source transmits. Let us assume source one is selected. %Due to perfect matching, the rates do not change when source 2 is selected instead. 
%
Source one builds the characteristic graph $G_{X_1}$ to compute $\Phi({\bf X}_1^n+{\bf X}_2^n)=\Phi({\bf X}_1^n+\pi({\bf X}_1^n,{\bf\Theta}^n))$ given ${\bf\Theta}^n$. This requires, following the notion of the conditional graph entropy, as detailed in \cite{OR01}, an asymptotic rate of $H_{G_{X_1}}(X_1\vert \Theta)$.

The rate needed from source one to compute $f(X_1,X_2)$ is %By averaging over the distribution ${\bf q}$,
\begin{align}
\label{rate_one_source_permutation_invariant}
R_1%\geq H(\Phi(X_1+X_2)\,\vert \,\Theta)\nonumber\\
\geq\sum\limits_{l\in[L]}  q_l H(\Phi(X_1+X_2)\,\vert \, X_2=\pi_l(X_1)) \ ,
\end{align} 
%which is less than $H(f(X_1,X_2))=H(\Phi(X_1+X_2))$ as conditioning reduces the entropy. %$H(\Phi(X_1+X_2))\geq H(\Phi(X_1+X_2)\, \vert\, X_2=\pi_l(X_1))$ for all $l\in[L]$
enabling the partial distributed computation of $f(X_1,X_2)$ at an asymptotic rate upper bounded by $H(\Phi(X_1+X_2)\, , \,  \Theta)$. 
\end{proof}

%\derya{It also seems that Definition 1 is not quite relevant to this paper. While the author is free to focus only on permutation invariant set functions, this seems to be an unnecessary restriction when none of the results depend on it. Or perhaps permutation invariant set functions can be included as merely an example of application of the theorem.}
\end{comment}


\end{document}