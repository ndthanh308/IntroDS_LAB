%\documentclass[fleqn,a4paper,twocolumn,9pt,twoside]{jarticle}     %fleqn,twocolumn,leqno
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% \usepackage[subrefformat=parens]{subcaption}
%\usepackage[dvips]{graphicx}
% \usepackage{listings}
\usepackage{float}
  \pagestyle{empty}
\usepackage[dvipdfmx]{graphicx}
\usepackage{latexsym}
%\usepackage{slashbox}
\usepackage{url}
\usepackage{cite}
\usepackage{comment}
\usepackage{algorithm}
\usepackage{algpseudocode}

\begin{comment}
\setlength{\topmargin}{-15mm}        	   %defoult 25mm {-5mm}
\setlength{\footskip}{1000mm}
\setlength{\textwidth}{178mm}              %印刷幅　175mm
\setlength{\textheight}{260mm}       	   %印刷高　250mm
\setlength{\oddsidemargin}{-10mm}     	   %サイドマージン  -17
\setlength{\evensidemargin}{-10mm}
\setlength{\columnsep}{10mm}         	   %中央分離幅　10mm
\setlength{\mathindent}{3mm}               %数式の左からのズレ
\markright{}
\pagestyle{empty}           	           %empty,headings
\pagestyle{empty}\markboth{}{}
\date{
\begin{list}{}{
\setlength{\leftmargin}{20mm}
\setlength{\rightmargin}{20mm}
}
\setlength{\baselineskip}{2.0mm} %4.5mm
\item {\normalsize
\vspace{-13mm}
\hspace{-1.5mm}
\vspace{-1mm}\\
\hspace{5mm}
}
\end{list}
\vspace{-7mm}
}
%%
\begin{document}
\maketitle
\setlength{\baselineskip}{4.0mm}%%行間
\thispagestyle{empty}
\renewcommand{\figurename}{Fig.}
\renewcommand{\tablename}{Table}
\end{comment}






\begin{document}


\newcommand{\FIG}[3]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed\\
{\scriptsize #3}
\end{center}
\end{minipage}
}

\newcommand{\FIGU}[3]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed\\
{\scriptsize #3}
\end{center}
\end{minipage}
}

\newcommand{\FIGm}[3]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed\\
{\scriptsize #3}
\end{center}
\end{minipage}
}

\newcommand{\FIGR}[3]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed
\\
{\scriptsize #3}
\vspace*{1mm}
\end{center}
\end{minipage}
}

\newcommand{\FIGRpng}[5]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed\vspace*{1mm}
\\
{\scriptsize #3}
\vspace*{1mm}
\end{center}
\end{minipage}
}

\newcommand{\FIGpng}[5]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed\vspace*{-1mm}\\
{\scriptsize #3}
\vspace*{1mm}
\end{center}
\end{minipage}
}

\newcommand{\FIGtpng}[5]{
\begin{minipage}[t]{#1cm}
\begin{center}
% Figure removed\vspace*{1mm}
\\
{\scriptsize #3}
\vspace*{1mm}
\end{center}
\end{minipage}
}

\newcommand{\FIGRt}[3]{
\begin{minipage}[t]{#1cm}
\begin{center}
% Figure removed\vspace*{1mm}
\\
{\scriptsize #3}
\vspace*{1mm}
\end{center}
\end{minipage}
}

\newcommand{\FIGRm}[3]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed\vspace*{0mm}
\\
{\scriptsize #3}
\vspace*{1mm}
\end{center}
\end{minipage}
}

\newcommand{\FIGC}[5]{
\begin{minipage}[b]{#1cm}
\begin{center}
% Figure removed~$\Longrightarrow$\vspace*{0mm}
\\
{\scriptsize #5}
\vspace*{8mm}
\end{center}
\end{minipage}
}

\newcommand{\FIGf}[3]{
\begin{minipage}[b]{#1cm}
\begin{center}
\fbox{% Figure removed}\vspace*{0.5mm}\\
{\scriptsize #3}
\end{center}
\end{minipage}
}





\title{\Large \bf
Active Robot Vision for Distant Object Change Detection: \\
A Lightweight Training Simulator Inspired by Multi-Armed Bandits
}

\author{Terashima Kouki$^{1}$, Tanaka Kanji$^{1}$, Yamamoto Ryogo$^{1}$, and Tay Yu Liang Jonathan$^{1}$
%\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{$^{1}$K. Terashima, K. Tanaka, R. Yamamoto, and J. Tay Yu Liang are with Robotics course, Faculty of Engineering, University of Fukui, Japan. {\tt\small \{ha209059,tnkknj,mf220362,mf228029\}@g.u-fukui.ac.jp}}%
}


\date{}

\maketitle



\newcommand{\figA}{
% Figure environment removed
}

\newcommand{\figB}{
% Figure environment removed
}

\newcommand{\figC}{
% Figure environment removed
}

\newcommand{\figD}{
% Figure environment removed
}

\newcommand{\figE}{
% Figure environment removed
}

\newcommand{\figF}{
% Figure environment removed
}

\newcommand{\figG}{
% Figure environment removed
}


\begin{abstract}
In ground-view object change detection, the recently emerging map-less navigation has great potential as a means of navigating a robot to distantly detected objects and identifying their changing states (appear/disappear/no-change) with high resolution imagery. However, the brute-force naive action strategy of navigating to every distant object requires huge sense/plan/action costs proportional to the number of objects. In this work, we study this new problem of ``Which distant objects should be prioritized for map-less navigation?" and in order to speed up the R{\&}D cycle, propose a highly-simplified approach that is easy to implement and easy to extend. In our approach, a new layer called map-based navigation is added on top of the map-less navigation, which constitutes a hierarchical planner. First, a dataset consisting of $N$ view sequences is acquired by a real robot via map-less navigation. Then, an environment simulator was built to simulate a simple action planning problem: ``Which view sequence should the robot select next?". Then, a solver was built inspired by the analogy to the multi-armed bandit problem: ``Which arm should the player select next?". Finally, the effectiveness of the proposed framework was verified using the semantically non-trivial scenario ``sofa as bookshelf".
\end{abstract}

\section{
Introduction
}




Ground-view change detection plays an important role 
for a robot 
to detect 
inconsistencies between 
the environment model (``map")
and live scenes, 
in emerging application domains such as 
lifelong learning \cite{l3}
and 
long-term autonomy \cite{lta}. Unlike classical vision setups such as parallel projection satellite imagery \cite{satellite}, the problem is complicated by non-linear perspective projection and visual uncertainties (e.g., depth ambiguity, occlusion) \cite{gvcd}. One of the open issues in ground view change detection is distantly detected object changes (Fig. \ref{fig:A}). Due to poor spatial resolution, distant objects look significantly different than they were seen during training, and they are often incorrectly categorized as change objects even with state-of-the-art deep learning models. Navigating to objects is a typical means for robots to obtain sharp, high-resolution object images, although a brute-force naive action strategy of navigating to every object exhaustively would require a huge sense/plan/action cost proportional to the number of candidate objects. To address this issue, in this work, we explore an active vision problem (e.g., \cite{tanaka2003viewpoint}), which aims to determine the priority of visiting objects, given such distantly-detected uncertain objects, in order of degree of similarity to the query object.



Our active visual approach is motivated by recent advances in point-goal navigation \cite{pgn} and other map-less navigation techniques \cite{ogn,vln,ans}. Safe and efficient navigation based solely on vision has traditionally been considered difficult, but advances in map-less navigation, represented by point goal navigation, have provided significantly improved planners. Given a point-goal (i.e., a distantly-detected object) as input, the next-best-view action is planned by these planners. There are two special notes on this matter. First, these planners are black-box state-action maps and do not provide the entire state-action sequence in advance. Second, these planners rely on deep learning and require a significant amount of time/space/dataset to train/retrain. Due to the trade-off between domain adaptation cost and retraining cost, we decided to avoid retraining these planners. Our action planning is therefore reduced to the problem of selecting one out of the distantly detected objects which the robot should navigate to.


\figA


Nevertheless, the feasibility of this active vision framework remains unclear. Although distant object detection has made great progress in recent years \cite{dod}, distant object change detection remains largely unsolved. As a result, it is often the case that an object is detected far away but not recognized. Detected objects at a distance may have low spatial resolution and are blurry, providing few cues to the planning problem ``Which of the detected potential object change should the robot navigate to?''. If the available cues were useless, the planning task would be a significantly ill-posed one, so it would degenerate into a costly exploration task such as frontier exploration \cite{yamauchi1997frontier}. To test its feasibility, we explore an action-selection problem inspired by the classic ``multi-armed bandit (MAB)" problem: The question of ``which object should the robot navigate to?" in our case is analogous to the question of ``which arm should the player choose?" in the MAB case.







Our proposed framework consists of two layers, called map-less navigation and map-based navigation (Fig. \ref{fig:D}). The low-level planner, map-less navigation, is domain-invariant, responsible for navigating to a given object- or point- goal of interest (i.e., an uncertain object), and trained offline with end-to-end machine learning, via such as reinforcement learning. A high-level planner, map-based navigation, is domain-adaptive, responsible for long-term tasks such as object map maintenance \cite{lta} 
and landmark object-based navigation \cite{huang2023visual}. Note that the two hierarchies are complementary and modular. Of these, in this work we are interested in a high-level navigation task, distant object change detection. A lightweight map-based planner inspired by the classic ``multi-armed bandit" problem is developed by employing a modern high-capacity deep learning neural networks \cite{vgg16} for image embedding, for change prediction, and for action planning. The effectiveness of the proposed method was verified using the semantically non-trivial scenario ``sofa as bookshelf".




\section{
Related Work
}

In the field of computer vision, various formulations have been studied for image change detection. One popular formulation is for remote sensing applications \cite{satellite}, such as change detection from satellite images. Detecting changes across different domains (e.g., across seasons) based on comparisons between reference scenes (i.e., map) and input scenes is a topic of on-going research \cite{gvcd}. However, these studies rely on simplified vision setups such as parallel projection view. This is not the case for ground-view change detection, where the nonlinear perspective projection further complicated the problem. 
This complex and ill-posed scenario has recently been investigated with support from high-capacity deep learning models \cite{2d2d}. 
This line of researches can be further categorized into 
2D-3D comparison \cite{2d3d}, % icra2022 20230527 Diff-Net 
3D-3D comparison \cite{3d3d}, and % icra2022 ref
2D-2D comparison \cite{2d2d}, % icra2022 ref
according to the format of the live scene used for comparison. 
Our method falls into the 2D-2D comparison, which is the most challenging setting without assuming the availability of the 3D model in both the training and test domains. 

The most relevant to our approach is ground-view object change detection, 
which aims to maintain an object map that records the objects detected in the workspace at some point in the past.
In \cite{he2022diff}, a framework was presented to detect significant changes such as the appearance/disappearance of portable traffic lights in high-definition HD maps. Specifically, it first constructs a rasterized image by projecting the map elements from the camera's perspective. Pyramid features of different resolutions are then extracted by two parallel CNN-based backbones from both the rasterized and camera images. This approach has been shown to be effective for change detection in large-scale high-definition HD maps. 

Compared with these existing studies, our approach has two key novelties. First, rather than assuming passive vision as in most change detection approaches, we consider active vision, including the problem of next-best-view action planning, to determine distant object changes. Second, we do not assume the availability of high-definition 3D models, and use only a list or bag of objects appearing in the workspace as the map.


Map-less navigation module is a workspace-specific state-action map. It is an invariant model, under the realistic assumption that the workspace structure does not change significantly. Such a model is suitable for our envisioned map maintenance scenario, an application of daily navigation, the workspace rarely changes, and the purpose of the robot is to detect rare changing objects. Since distantly detected objects often occupy only a small portion of the input image, we can assume that they do not significantly affect the navigation ability of this map-less navigation. Map-less navigation has various variants such as 
point-goal navigation \cite{pgn}, 
object-goal navigation \cite{ogn}, 
vision-language navigation \cite{vln}, 
and point-goal navigation is the most relevant for our application. 
Given the coordinate of a distantly detected object
as the goal point, 
it should guide the robot to the goal safely and efficiently based on vision alone. Note that a map-less navigation model usually requires a training phase that is data-, time-, and space-intensive, which decreases the flexibility required for domain adaptation of the model. In contrast, our hierarchical planner has the potential to combine domain-invariance of the map-less navigation with the domain-adaptability of map-based navigation.


\figC

\figD

\figB




\section{Approach}



\subsection{Task}

Figure \ref{fig:C} shows an overview of the robot. 
A front-facing camera 
is equipped on the robot 
and can be used for change detection and collision avoidance.

The robot aims to maintain a bag-of-object map that records the objects detected in the workspace at some point in the past without spatial information. This is consistent with the typical objectives of change detection in terms of lifelong learning and long-term autonomy. In the experiment, the semantically non-trivial ``sofa as bookshelf" dataset is considered. 
In this dataset scenario, books with different colors, textures, and sizes without the spatial information are considered as objects in the map. The goal of active change detection is to determine if each book on the sofas has changed. In the robot's start viewpoint, all books are detected at distant and are erroneously identified as non-book objects (such as pillows). An example of such misrecognition is shown in Fig. \ref{fig:A}. The object category prediction attached to each bounding box was obtained by faster RCNN, a popular object detector widely used for scene understanding. Despite receiving a high confidence score, the recognition result is incorrect. The robot needs to get closer and observe the object. By doing so, it is expected that the changing state of an object (appearance/disappearance/no-change) can be identified with higher accuracy.

A dataset was acquired for the experiment. For each $i$-th object in a group of distantly-detected $N=10$ objects, a view-sequence consisting of $L=100$ images is acquired, by navigating the robot from the same start viewpoint, whose view image is shown in Fig. \ref{fig:B}, to the closest viewpoint to that object. In this way, the dataset consisting of $N$ view-sequences of length $L$ was obtained. Figure \ref{fig:F} shows samples images from the dataset.


\figF

A simplified environment simulator was built on top of this dataset. Although real-world navigation costs consist of travel and observation costs, in this simulator, the travel cost was ignored and the observation cost was fixed at 1. The internal state of the simulator at time step $i$ is represented by a state vector $s^i=$$(s^i[1], \cdots, s^i[N])$, where 
$s^i[n]$
represents 
the unseen sub-sequence
$V_n(s^i[n]), \cdots, V_n(L)$
of each $n$-th view-sequence $V_n$.
At simulator startup $i=0$. 
the state vector $s^0$
is initialized to 
$s^0=(1, \cdots, 1)$.
At each time step, the robot can send an ``object navigation" action request $n^*$ to the environment simulator once per time to navigate to the $n^*$-th view-sequence out of the $N$ view-sequences ($n^*\in [1,N]$). Then, the simulator returns the first image $V_{n^*}(s^i[n^*])$ in the unseen part of $n^*$-th view-sequence, then removes that image ID from the unseen part (i.e., $s^{i+1}[n^*]$$\leftarrow$$s^{i}[n^*]+1$). Note that since each object-specific view-sequence is initialized to length $L$, the robot is not allowed to make more than $L$ action requests for a single view-sequence. If the top-priority action request by the robot is such an unacceptable action, the next-priority action request will be executed instead.




\subsection{Active Vision}


Our planning problem can be understood by analogy with the classic ``multi-armed bandit (MAB)" problem. 
As an analogy to requiring the player to select one out of $N$ arms in a multi-armed bandit, our problem requires the robot to select one out of $N$ view-sequences at each time step.
The purpose of the exploration phase is to learn the expected reward for each arm in a multi-armed bandit, whereas in our problem it is to learn the expectation that the query object will be detected in each view-sequence. 
The purpose of the exploitation phase is to focus on the arm with the highest potential rewards in a multi-armed bandit, whereas in our problem it is to focus on the view-sequence where the query object is most likely to be detected.

A simple algorithm that runs two distinctive phases, exploration and exploitation, in sequence is considered. 
%
In this framework, the key hyperparameter is $N'$, which controls the balance between exploitation and exploration.
%
Given the hyperparameter $N'$, the exploration phase is simply reduced to the task of performing the $N'$ actions in a breadth-first manner for every view-sequence. This will result in obtaining $NN'$ images.
%
By contrast, the exploitation phase is the task of visiting the view-sequences in a depth-first manner, in an order  prioritized by expected reward. 
At the beginning of the exploitation phase, the top-priority view-sequence $n(n\in[1,N])$ with the highest expected reward is selected. Note that the expected reward depends not only on the $NN'$ images obtained in the exploration phase, but also on all images obtained so far in the exploitation phase. Nevertheless, once the robot has chosen a view-sequence in the exploitation phase, it is not allowed to change it until the robot has completed the chosen view-sequence. In other words, when the robot completes one view-sequence to the end, it is allowed to use all the images up to that point in deciding which view-sequence to select next. Note that in that case, at least one view-sequence has finished (i.e., $s^i[n]=L$), and finished view-sequences can no longer be executed.


\subsection{Passive Vision}



The passive vision module has two roles.

One is to keep an estimate up to date for the given problem: ``Which of the distantly detected objects is most likely to be the query object?". This is done by using the image embedding of the query object and the observed object 
to measure similarity between them. For image embedding, the signal of the 13-th layer of the convolutional neural network in vgg16 of \cite{vgg16} 
is used. Cosine-similarity is used as the similarity measure. 

Another role is for prioritizing the view-sequence in the ``object navigation" action plan, ``which object should the robot navigate to?" in the aforementioned active vision. Out of the objects detected at a distance, the one with the highest likelihood of being the query object is chosen and given the highest priority to the associated view-sequence.



\section{
Experiments
}

\subsection{Setup}

The performance of the proposed method for active change detection in a map maintenance scenario has been experimentally evaluated. Figure 
\ref{fig:B} 
shows an image taken by the robot at its start viewpoint. 


The robot is given a map that was learned at some point in the past. This map consists of a bag of object-specific images of objects detected in the environment at that time. In the evaluation experiment, one is randomly selected from this bag of object images and used as the query image. Given a query image of an object, the task is to identify whether the query object still exists in the workspace (that is, has not changed). As shown in figure \ref{fig:B}, $N=10$ objects were observed in the view from the robot's starting viewpoint.

Nonetheless, none of the objects were similar to the query image, nor were they even correctly recognized as books. One reason might be that in this workspace, sofas were customarily used as bookshelves, so books might not be recognized correctly and categorized in the wrong category, such as pillows. Also, sofas were often not categorized as sofas. Moreover, it is known that chairs, a superordinate category of sofas, are a difficult object class to recognize even with the help of affordances \cite{grabner2011makes}, which may be the reason.

The active vision here aims to navigate to objects, acquire higher spatial resolution images, and investigate which one out of the $N$ objects correspond to the query image. It is clear that there is a trade-off between cost and performance. As the robot iterates through the sense-plan-action cycle, 
we can expect to infer the correct object ID with a higher success rate, 
at a cost that monotonically increases with the travel distance.

\figE

\subsection{Implementation Details}

% In order to facilitate the analysis of the problem, the following simplified problem setting was considered. Suppose the candidate objects are $N=10$ books. In other words, background objects and stuff for indoor workspaces like windows and trees were ignored. For each candidate object, we collect a view-sequence of length $L=100$ while performing simple collision avoidance and the shortest path by the robot, as shown in Fig. \ref{fig:B}, while navigating to each candidate object. 
$NL$ images are provided, they are used as the dataset. The action space is a candidate object ID $n\in [1,N]$ that represents ``to which of the $N$ candidate objects should the robot navigate to?''. 

The state vector was defined as a summary of the cost paid for a view-sequence over all view-sequences: $s^i=$$(s^i[1], \cdots, s^i[N])$.
When the robot performs the $n^*$-th action, 
the state transition is expressed as 
\begin{equation}
s^{i+1}[n] \leftarrow 
\left\{
\begin{array}{ll}
s^{i}[n]+1 & (n=n^*) \\
s^{i}[n] & (\mbox{otherwise})
\end{array}
\right.
.
\end{equation}
This means that 
as the result of this action, the robot newly acquired the image $V_{n^*}(s^{i}[n^*])$ as new knowledge.
The cost of a sense-plan-action cycle is simply defined to be constant, 1, 
independent of the executed action $n^*$, as mentioned earlier.


A simplified active vision task consisting of two phases,
exploration and exploitation,
were considered.
The exploration phase aims to acquire knowledge about all candidate objects by performing a predefined number of $N'$ ``object navigation" actions on all candidate objects. Note that the knowledge acquired here consists of 
$NN'$ images. We repeat the sense-plan-action cycle to navigate to the most likely candidate object $n^*\in[1,N]$.
The likelihood of candidate objects is measured using convolutional neural network embeddings. 
The acquired image 
$V(s^i[n^*])$ 
is input to a convolutional neural network $C(\cdot)$, and a high-dimensional discriminative feature vector $C(V(s^i[n^*]))$ is obtained in the form of signals in the 13-th layer of vgg16 $C(\cdot)$. 
Given a state vector $s^i=$
$(s^i[1]$, 
$\cdots$, 
$s^i[N])$,
the likelihood of 
a reference object $V_q$
being
exist in the view-sequence 
$V_n$ 
is evaluated 
by max-pooling of cos-similarity over the already-seen part of the view-sequence $[1, s^i[n]-1]$, as follows: 
\begin{equation}
\max_{j\in[1, s^i[n]-1]}
\cos(C(V_q), C(V_n(j))).
\end{equation}



\subsection{Results}

One interesting issue in learning-based planners is how to establish a good balance between exploitation and exploration. For the robot to make good decisions, the exploration phase is important to gather prior knowledge.
On the other hand, if the cost of collecting prior knowledge increases too much, the total cost performance of the entire task will decrease.

Recall that in our setup this tradeoff is controlled by the hyperparameter $N'$. Increasing $N'$ means giving more weight to exploration. Reducing $N'$ means putting more weight on exploitation.

The performance was investigated for various settings of the hyperparameter $N'$. This performance is measured by examining whether the candidate object with the top-1 likelihood is successfully determined to be the correct candidate object based on comparison with the ground truth. We then computed the average performance over all the test samples as a function of the possible costs 
1, 
$\cdots$, 
$NL$.


Figure \ref{fig:E} shows the experimental results. 
When only a small exploration cost is paid, such as $N'=1$, the prior knowledge was often insufficient and actions could be chosen almost at random. A large amount of prior knowledge can be collected by paying a large exploration cost, such as $N'=50$, but paying an excessive exploration cost increases the total cost. 
Note that the cost performance for a user robot depends on the robot's available budget. Given a large budget, it may be possible to pay a large amount for the exploration phase. Taking this fact into account, we divided the supposed users into different user groups by the maximum budget available (1, 2, $\cdots$, 999). Then, the time average of performance was examined independently for each group. The findings are summarized in Fig. \ref{fig:G}. According to this research, $N'=20$ paid the appropriate cost to exploration in our experiment. 


In the current experiment,
the low-level tasks of the robot were simplified and the focus was on the high-level tasks. We believe that our findings can be generalized to more realistic low-level tasks as well. In the direction of this research, we are currently researching and developing a realistic map-less navigation planner that combines the frontier exploration and reinforcement learning. However, whether the results of this research can be generalized to complex workspaces is left as a future issue. For example, in a maze-like workspace, more exploration cost should be paid for the robot to learn unseen traversable regions. Developing advanced path planning algorithms for such tasks is our current research topic. 


\section{Conclusions and Future Works}

This manuscript deals with a new and challenging active vision problem, distant object change detection. 
The effectiveness of the proposed method was demonstrated on the semantically non-trivial data set ``sofa as bookshelf". 
Our approach simplifies the sense-plan-action cycles of the high-level planner to the extreme by simplifying the low-level planner. 
We believe that the simplicity of the proposed method plays an important role in making it computationally feasible 
to run the huge number of training episodes (i.e., sense-plan-action cycles) required in typical sim-to-real frameworks.

There are several possible directions for future research in this research. (1) Implementation of low-level planners is one of the most important issues. We expect to implement more realistic planners by combining existing algorithms such as frontier methods and reinforcement learning. (2) It is desired to realize an integrated framework in which high-level and low-level planners interact with each other to improve total cost performance,
via such as a hierarchical planning framework in \cite{ans}. 
(3) The high-level planner introduced in this paper is based on an approximated simplified problem formulation, which naturally introduces approximation errors. Knowledge transfer from this high-level planner to other planners is expected to 
achieve improved planning performance \cite{ktrl}. (4) In this paper, we considered the most naive implementation of the multi-armed bandit (MAB) framework as a solution to active change detection. There are many evolutions of the MAB solution. It is also an interesting future topic to investigate whether the performance of the proposed method can be significantly improved by using these advanced solvers.


\figG


\bibliography{reference} %hoge.bibから拡張子を外した名前
\bibliographystyle{unsrt} %参考文献出力スタイル




\end{document}

