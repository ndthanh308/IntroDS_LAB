%  cross-window attention between the average-pooled keys and values from each window and each query position in the input feature map

% It may introduce some artifacts due to window partitioning[10].
% It may require more hyperparameter tuning for different tasks and datasets[10].
% It may not be compatible with some existing vision transformer architectures or modules

% To address this issue, recent studies have proposed various methods to reduce the computational complexity of the self-attention operation, such as using sparse attention or compressing the attention mechanism.

% problem: 下面这里的讨论是否应该就只限于 token mixing? 有的论文, 比如mobilevit v2, 他的O(N) 复杂度的attention 本质上不能算是一个token mixer @Separable self-attention 

% This paper introduces Reformer~\cite{kitaev2020reformer}, a Transformer model that uses reversible layers and locality-sensitive hashing to improve efficiency on long sequences.

% Its attention algorithm which complexity is O(NlogN) has some possible shortcomings, such as:

% It may lose some accuracy because it only considers the attentions within the same bucket and ignores the attentions between different buckets.
% It may be affected by hash collisions, causing some Q and K with low similarity to be assigned to the same bucket, and some Q and K with high similarity to be assigned to different buckets.
% It may not be efficient for short sequence lengths because it requires multiple hash and sort operations.
%  One drawback of Longformer-LSH is that it relies on random projections for hashing, which may introduce noise and collisions in the attention matrix. Another drawback is that it requires careful tuning of hyperparameters such as bucket size and number of hash tables

% DNA~\cite{hassani2022neighborhood, hassani2022dilated}, a new type of local attention that allows each token to attend to tokens within a dilated neighborhood. DNA can be applied at different scales and resolutions, enabling efficient and flexible hierarchical vision transformers. The paper also proposes DNAT-Net, a network architecture that combines DNAT with convolutional layers and residual connections.

% mobilevit~\cite{mehta2021mobilevit} 使用fold 和 unfold 的方式, 高效的实现了一种dilated attention 的机制


% This means that the model divides the image into overlapping windows and computes self-attention within each window1. Then, it shifts the windows by half their size and repeats the process1. This way, the model can capture both local and global information from different parts of the image at different scales21.
%  it may introduce border artefacts around each patch when restoring images1. It may also ignore some contextual information of the input data2. It may also have difficulties in handling visual elements with various scales, especially in object detection3.
%  Swin Transformer may not capture the relationship between windows and windows very well. However, it tries to address this issue by using a hierarchical structure and shifted windows12. This means that it merges image patches into larger ones in deeper layers and shifts the windows by half their size in each layer12. This way, it can learn and reason about different image regions at different scales12.

% The paper you linked is titled “Twins: Revisiting Spatial Attention Design in Vision Transformers” and it was published on arXiv.org1. It proposes a new vision Transformer model, called Twins, that uses a simple yet effective spatial attention mechanism1. The model consists of two components: a local spatial attention (LSA) module that captures fine-grained details and a global spatial attention (GSA) module that captures long-range dependencies1. The paper claims that the model achieves competitive results on several benchmarks, such as image classification, object detection and semantic segmentation1.

% The GSA module is a global spatial attention module that captures long-range dependencies among image regions1. It works by first applying a strided convolution function to sub-sample the local tokens into global tokens2. Then, it applies a multi-head self-attention (MSA) layer to compute the attention weights between global tokens2. Finally, it applies another strided convolution function to restore the original resolution of the tokens2.

%  LSA stands for local self-attention and it divides the input feature map into non-overlapping windows and applies self-attention within each window. GSA stands for global sub-sampled attention and it applies self-attention on a sub-sampled feature map to capture global information1.

% The advantage of LSA is that it can reduce the complexity of self-attention by limiting the scope of attention to local regions. The disadvantage is that it may introduce boundary artifacts due to the hard partitioning of windows1. The advantage of GSA is that it can capture long-range dependencies across the whole feature map. The disadvantage is that it may lose some spatial resolution due to the sub-sampling step1.

% E-MHSA aims to reduce the computational complexity and memory consumption of the standard multi-head self-attention (MHSA) mechanism3.

% E-MHSA does this by using two types of queries: one for local features (LQ) and another for global features (GQ). LQ attends to a small region around each token, while GQ attends to all tokens. E-MHSA also uses two types of key-value pairs: one for spatial information (SKV) and another for semantic information (CKV). SKV encodes the relative position of each token, while CKV encodes the content of each token123.

% E-MHSA computes four attention matrices: LQ-SKV, LQ-CKV, GQ-SKV, and GQ-CKV. Then it applies average pooling to reduce the size of LQ-SKV and GQ-SKV. Finally, it concatenates all four matrices and applies a linear projection to obtain the output123.

% E-MHSA has some advantages and disadvantages compared to other attention mechanisms. Some of the advantages are:

% - It reduces the computational complexity and memory consumption of MHSA by using average pooling and two types of queries and key-values⁴⁵⁶.
% - It captures both local and global dependencies in an image by using spatial and semantic information⁴⁵⁶.
% - It improves Next-ViT's performance on various vision tasks such as classification, detection, and segmentation⁴⁵⁶.

% Some of the disadvantages are:

% - It may lose some information due to average pooling.
% - It may introduce some redundancy due to concatenating four attention matrices.
% - It may require more fine-tuning for different tasks and datasets⁶.

% HiLo Attention is based on the observation that high frequencies in an image capture local fine details and low frequencies focus on global structures, whereas a multi-head self-attention layer neglects the characteristic of different frequencies .

% HiLo Attention divides the attention heads into two groups: one group encodes high frequencies via self-attention within each local window, and another group encodes low frequencies via cross-window attention between the average-pooled keys and values from each window and each query position in the input feature map .
% Some of the disadvantages of HiLo Attention are:
% It may introduce some artifacts due to window partitioning[10].
% It may require more hyperparameter tuning for different tasks and datasets[10].
% It may not be compatible with some existing vision transformer architectures or modules

Nyströmformer~\cite{xiong2021nystromformer}, a Nyström-based algorithm for approximating self-attention. It reduces the quadratic complexity of self-attention to linear complexity and achieves competitive results on various natural language processing tasks.
One drawback of Nyström-based attention is that it requires a fixed number of landmarks, which may not be optimal for different input lengths or tasks. Another drawback is that it may introduce approximation errors due to random sampling of landmarks23.

\cite{ma2021luna} proposes a new attention algorithm called Luna, which can compute the similarity between any two elements in the input sequence in linear time. Luna algorithm is based on the idea of low-rank approximation and nested attention, which can effectively reduce the computational complexity and memory consumption while maintaining high-quality representation.
The drawbacks of the attention algorithm proposed in this paper are:
Luna algorithm requires chunking the input sequence, which may cause some boundary effects or information loss.
Luna algorithm requires performing low-rank approximation on each chunk, which may cause some detail information to be ignored or blurred.
Luna algorithm requires performing nested attention on each chunk, which may cause some redundant computation or overfitting problems.

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[htbp]
%   \centering
%     \begin{tabular}{l|c|c}
%     \toprule
%     \multicolumn{1}{c|}{Model} & Param (M) & mAP(\%) \\
%     \midrule
%     MViT-XXS~\cite{mehta2021mobilevit} & 1.5   & 18.5 \\
%     MViTv2-0.5~\cite{mobilevitv2} & 2.0   & 21.2 \\
%     \ours-XXS &   1.7    & 21.8 \\
%     \midrule
%     MViT-XS~\cite{mehta2021mobilevit} & 2.7   & 24.8 \\
%     MViTv2-0.75~\cite{mobilevitv2} & 3.6   & 24.6 \\
%     \ours-XS &  2.9    &  25.3 \\
%     \midrule
%     ResNet-50~\cite{he2016deep} & 22.9  & 25.2 \\
%     MobileNetv1~\cite{howard2017mobilenets} & 5.1   & 22.2 \\
%     MobileNetv2~\cite{sandler2018mobilenetv2} & 4.3   & 22.1 \\
%     MobileNetv3~\cite{howard2019searching} & 5.0   & 22.0 \\
%     MOne-S2~\cite{vasu2022improved} & 7.8  & 26.6 \\
%     MixNet~\cite{tan2019mixconv} & 4.5   & 22.3 \\
%     MNASNet~\cite{tan2019mnasnet} & 4.9   & 23.0 \\
%     MViT-S~\cite{mehta2021mobilevit} & 5.7   & 27.7 \\
%     EdgeNext~\cite{maaz2023edgenext} & 6.2 & 27.9 \\
%     MViTv2-1.0~\cite{mobilevitv2} & 5.8   & 26.5 \\
%     \ours-S & 5.4   & 28.5 \\
%     \bottomrule
%     \end{tabular}%
%     \vspace{+2mm}
%     \caption{Comparison of our method with other state-of-the-art models on the xx\_small, x\_small and small levels for object detection on COCO dataset. Params denotes the number of parameters in millions of the en-coder/backbone plus the ssd network. Abbreviated MOne and MViT: MobileOneand MobileViT. Comparison of our method with other state-of-the-art models on the xx\_small, x\_small and small levels for segmentation on ADE20k and VOC dataset. Params denotes the number of parameters in millions of the en-coder/backbone architecture only. Abbreviated MOne and MViT: MobileOneand MobileViT.}
%   \label{tab:sota_detection}%
% \end{table}%

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[htbp]
%   \centering
%     \begin{tabular}{l|c|cc}
%     \toprule
%     \multirow{2}[0]{*}{\quad\quad Model} & \multirow{2}[0]{*}{Param (M)} & \multicolumn{2}{c}{mIOU(\%)} \\
%           &       & ADE20k & VOC \\
%     \midrule
%     MViT-XXS~\cite{mehta2021mobilevit} & 1.9   &    -   & 73.6 \\
%     MViTv2-0.5~\cite{mobilevitv2} & 2.8   &   31.2    & 75.1 \\
%     MobileNetV2-0.5~\cite{sandler2018mobilenetv2} & 2.3 & - & 70.2 \\
%     \ours-XXS &  1.9    &  33.1    & 75.4 \\
%     \midrule
%     MViT-XS~\cite{mehta2021mobilevit} &  2.9  &   -    & 77.3 \\
%     MViTv2-0.75~\cite{mobilevitv2} & 3.4   & 34.7  & 77.1 \\
%     \ours-XS &  2.9    &   36.2    & 78.3 \\
%     \midrule
%     MobileNetv2-1.0~\cite{sandler2018mobilenetv2} & 6.9 & 34.1 & 75.7 \\
%     MOne-S2~\cite{vasu2022improved} & 7.8  & 35.7 & 77.9 \\
%     ResNet-50~\cite{he2016deep} & 23.5 & 36.2 & 76.8 \\
%     MViT-S~\cite{mehta2021mobilevit} & 6.4  &   -    & 79.1 \\
%     MViTv2-1.0~\cite{mobilevitv2} & 7.9  & 37.0  & 78.9 \\
%     \ours-S &  6.3   &   39.1    & 80.6 \\
%     \bottomrule
%     \end{tabular}%
%     \vspace{+2mm}
%   \caption{Comparison of our method with other state-of-the-art models on the xx\_small, x\_small and small levels for segmentation on ADE20k and VOC dataset. Params denotes the number of parameters in millions of the en-coder/backbone architecture only. Abbreviated MOne and MViT: MobileOneand MobileViT.}
%   \label{tab:addlabel}%
% \end{table}%

In this subsection, we will discuss what type of operation in the frequency domain can serve as a global operation. Previous works such as Fnet, FFC, and AFNO insert one or two linear layer with some nonlinear layer operations. 

Surprisingly, we discover that a linear operation in the frequency domain is equivalent to a spatial convolution. This equivalence is illustrated in Fig.\ref{fig:fftorder}, where we can see that the positions of the linear and the FFT operation can be interchanged, as shown in Fig.\ref{fig:fftorder}(a) and Fig.\ref{fig:fftorder}(b). When these operations are swapped, the FFT and iFFT operations cancel each other out, resulting in an ordinary linear, as shown in Fig.\ref{fig:fftorder}(c) which is a local operation and not a global operation.


% Figure environment removed 


Specifically, we assume that the linear operation in the frequency domain acts on the real and imaginary parts of the feature after FFT and sharing a real value weight convolution operation, the calculation process can be simplified as follows:

\begin{equation}
\begin{split}
    &\mathcal{F} ^{-1} ( Linear(\mathcal{F} (\mathbf{X}))) \\
           = &\mathcal{F} ^{-1} ( Linear(\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w, c) e^{-2\pi i(uh+vw)}) \\
\end{split}
\label{eq: fft}
\end{equation}

Since the linear layer is a real weight and acts on the real and imaginary parts simultaneously, it can be expressed as an ordinary multiplication:


\begin{equation}
\begin{split}
      &\mathcal{F} ^{-1} ( Linear(\mathcal{F} (\mathbf{X}))) \\
    = &\mathcal{F} ^{-1} ( \sum_{c=0}^{C-1} w_{c} (\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w, c) e^{-2\pi i(uh+vw)}) \\
\end{split}
\label{eq: fft}
\end{equation}

% problem: 这里要不要把c表达出来, 如果把c表达出来, 则有点啰嗦, 不把c 表达出来, 1x1 卷积好像看的不是那么明显?

Since the Linear operation acts on the channel dimension and the FFT operation acts on the "$H\times W$" dimension, which are orthogonal. We can use the exchange summation to change the formula into the following form:

\begin{equation}
\begin{split}
              & \mathcal{F} ^{-1} ( Linear(\mathcal{F} (\mathbf{X}))) \\
            = &\mathcal{F} ^{-1} ( Linear(\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w, c) e^{-2\pi i(uh+vw)}) \\
            = &\mathcal{F} ^{-1} ( \sum_{c=0}^{C-1} w_{c} (\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w, c) e^{-2\pi i(uh+vw)}) \\
            = &\mathcal{F} ^{-1} ( \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} (\sum_{c=0}^{C-1} w_{c} \mathbf{X}(h, w, c)) e^{-2\pi i(uh+vw)} \\
            = &\mathcal{F} ^{-1}(\mathcal{F} (Linear (\mathbf{X}))) \\
            = &Linear (\mathbf{X})
\end{split}
\label{eq: fft}
\end{equation}

As a result, the FFT and inverse FFT operations cancel each other out, and the formula becomes an ordinary linear layer.


Generally, the implementation of the linear operation in the frequency domain can be divided into four cases: (1) real part and imaginary part share the same real weight  which is discussed above, (2) real part and imaginary part use separate real weight, (3) linear with complex weight, and (4) real weight linear after concatenating the real and imaginary parts in the channel dimension. We demonstrate the first case in the above discussion. And In the supplementary, we prove that the other three cases can be decomposed into the superposition of the first case with the same conclusion.


Based on this finding, we can conclude that the FFT-Linear-ReLU-iFFT structure in the FFC paper is equivalent to the Linear-\textbf{FFT-ReLU-iFFT} structure, while the FFT-Linear-ReLU-Linear-softshink-iFFT structure in the AFNO paper is equivalent to the Linear-\textbf{FFT-ReLU-iFFT}-Linear-\textbf{FFT-softshink-iFFT} structure. Interestingly, we discovered that the core global operation is not convolution in FFT, but rather ReLU or softshink, the determine nonlinear operation.

We analyzed that after the FFT operation is applied to the feature, each position corresponds to a distinct frequency component, and they should not share weights for projection. The ReLU operation is essentially a position-specific binary mask that restricts the output by setting negative values to zero, allowing different frequency components to be treated differently. In contrast, our method makes this mask learnable and more adaptable, ensuring the network's expressiveness while reducing computation.

Additionally, we noticed that GFNet also takes the operation of multiplying the mask in the frequency domain. However, GFNet is expensive for higher solution images as sequence length increases, and is not adaptive to different image resolution inputs, limiting its application.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{l|c|cc|c}
    \toprule
    \multicolumn{1}{c|}{Adaptivity} & mask shape  & Param (M) & FLOPs (G) & top1 \\
    \midrule
    Task & C$\times$H$\times$W & 6.5   & 1.5   & 79.1 \\
    Instance  & C$\times$H$\times$W & 5.5   & 1.5   & 79.8 \\
    \midrule
    Instance  & 1$\times$H$\times$W & 5.5   & 1.5   & 79.3 \\
    Instance  & C$\times$H$\times$W & 5.5   & 1.5   & 79.8 \\
    \bottomrule
    \end{tabular}%
    }
  \caption{Ablation study on the Adaptiveness of the frequency filtering operation. ``Task.'' refers to the filtering operation using a task-level attention mask where the mask is implemented with network parameters and is shared over different instances.  “Ins.” denotes the filtering operation using learned instance-adaptive mask. ``1$\times$H$\times$W" refers to the output shape of the filtering mask, pooling from C$\times$H$\times$W and then multiplied on the frequency features with channel-wise sharing. Compare to that, `Instance-wise filtering with shape of ``C$\times$H$\times$W'' is our proposed \ourop, where the filtering mask is multiplied on the frequency features with channel adaptivity.}
  \label{tab:adaptive}%
\end{table}%