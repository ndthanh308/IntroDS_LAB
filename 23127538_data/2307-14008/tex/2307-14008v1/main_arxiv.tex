\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{1854} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

%  packets add by huangzp
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{xcolor} 
\usepackage{pifont}
\usepackage{colortbl}
\usepackage{hyperref}
\usepackage{diagbox}
%  packets add by zhizzhang
\usepackage{enumitem}
\usepackage{bm}

\newcommand{\tco}{\textcolor{orange}}
\newcommand{\tcb}{\textcolor{blue}}
\newcommand{\tcr}{\textcolor{red}}
\newcommand{\tcp}{\textcolor{purple}}

\newcommand{\ieno}{\textit{i}.\textit{e}.}
\newcommand{\egno}{\textit{e}.\textit{g}.}
\newcommand{\etcno}{\textit{etc.}}
\newcommand{\ours}{AFFNet}
\newcommand{\ourop}{AFF}
\newcommand{\ouropfull}{Adaptive Frequency Filters}
\newcommand{\ourblock}{AFF Block}


\begin{document}

%%%%%%%%% TITLE
\title{Adaptive Frequency Filters As Efficient Global Token Mixers}
% \title{Adaptive Frequency Filters Are Efficient Global Dynamic Token Mixers}
% \title{Learned Frequency Filters Are Efficient Global Adaptive Token Mixers}

\author{Zhipeng Huang\textsuperscript{\rm 1,2}\thanks{This work was done when Zhipeng Huang was an intern at Microsoft Research Asia.} \quad 
Zhizheng Zhang\textsuperscript{\rm 2}\quad 
Cuiling Lan\textsuperscript{\rm 2}\quad 
Zheng-Jun Zha\textsuperscript{\rm 1}\quad 
Yan Lu\textsuperscript{\rm 2}\quad 
Baining Guo\textsuperscript{\rm 2}\\
\textsuperscript{\rm 1}{University of Science and Technology of China}\quad
\textsuperscript{\rm 2}{Microsoft Research Asia}\\
\tt\small \{zhizzhang,\ culan,\ yanlu,\ bainguo\}@microsoft.com \\ 
\tt\small hzp1104@mail.ustc.edu.cn \quad
\tt\small zhazj@ustc.edu.cn}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}

Recent vision transformers, large-kernel CNNs and MLPs have attained remarkable successes in broad vision tasks thanks to their effective information fusion in the global scope. However, their efficient deployments, especially on mobile devices, still suffer from noteworthy challenges due to the heavy computational costs of self-attention mechanisms, large kernels, or fully connected layers. In this work, we apply conventional convolution theorem to deep learning for addressing this and reveal that adaptive frequency filters can serve as efficient global token mixers. With this insight, we propose Adaptive Frequency Filtering (AFF) token mixer. This neural operator transfers a latent representation to the frequency domain via a Fourier transform and performs semantic-adaptive frequency filtering via an elementwise multiplication, which mathematically equals to a token mixing operation in the original latent space with a dynamic convolution kernel as large as the spatial resolution of this latent representation. We take AFF token mixers as primary neural operators to build a lightweight neural network, dubbed AFFNet. Extensive experiments demonstrate the effectiveness of our proposed AFF token mixer and show that AFFNet achieve superior accuracy and efficiency trade-offs compared to other lightweight network designs on broad visual tasks, including visual recognition and dense prediction tasks.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Remarkable progress has been made in ever-changing vision network designs to date, wherein effective token mixing in the global scope is constantly highlighted. Three existing dominant network families, \ieno, Transformers, CNNs and MLPs achieve global token mixing with their respective ways. Transformers \cite{dosovitskiy2020image,touvron2021training,liu2021swin,chu2021twins,yu2022metaformer} mix tokens with self-attention mechanisms where pairwise correlations between query-key pairs are taken as mixing weights. CNNs achieve competitive performance with transformers by scaling up their kernel sizes \cite{peng2017large,ding2022scaling,liu2022more,chen2022scaling}. MLPs \cite{tolstikhin2021mlp,hou2022vision,lian2021mlp} provide another powerful paradigm via fully connections across all tokens. All of them are effective but computationally expensive, imposing remarkable challenges in practical deployments, especially on edge devices.

% Figure environment removed 

Recently, there is increased attention on improving the efficiency of token mixing in transformers. Some works ~\cite{kitaev2020reformer,liu2021swin,chu2021twins,hassani2022dilated,mehta2021mobilevit,pan2022fast,li2022next} squeeze the scope of token mixing in different ways to compromise the representation capacities of neural networks for their efficiencies. Other works reduce the complexity of the matrix operations in self-attention by making use of the associativity property of matrix products \cite{katharopoulos2020transformers} or low-rank approximation methods \cite{guo2019low,xiong2021nystromformer}. These methods all sacrifice the expressiveness of neural networks and lead to unsatisfactory performance of efficient network designs. A general-purpose global token mixing for lightweight networks is still less explored. Better trade-off between accuracy and efficiency for global-scope token mixing is worthy of further study. 

% [ToDO: Zhizheng, whether we should give specific complexity analysis here?]
In this work, we reveal that \textit{adaptive frequency filters can serve as efficient global token mixers}, inspired by the convolution theorem \cite{mcgillem1991continuous,rabiner1975theory,oppenheim1999discrete} widely used in conventional signal processing. 
% The convolution theorem \cite{mcgillem1991continuous,rabiner1975theory,oppenheim1999discrete}
This theorem states that a convolution in one domain mathematically equals the Hadamard product (also known as elementwise product) in its corresponding Fourier domain. This equivalence allows us to frame global token mixing as a large-kernel convolution in the latent space and efficiently implement this convolution with a Hadamard product operation in the frequency domain by performing Fourier transforms on tokens in the latent space.


Besides large scopes, the adaptability to semantics also matters for token mixing as studied in \cite{dai2017deformable,chen2020dynamic,activemlp,bau2020understanding,Wu_2021_CVPR}. This means that the weights for token mixing should be instance-adaptive. Moreover, different semantic attributes of the learned latent representations distribute in different channels \cite{bau2020understanding,wu2021stylespace}. This property poses requirements for channel-specific token mixing wherein the weights of token mixing vary across different channels. From the perspective of framing global adaptive token mixing as a convolution, the kernel of this convolution operation should be not only large but also spatially dynamic. However, it is well known that dynamic convolutions are computationally expensive in common. Large-kernel dynamic convolutions seem extremely prohibitive for efficient/lightweight network designs. In this paper, we propose to adopt frequency filtering in the Fourier domain with learned instance-adaptive masks as a mathematical equivalent of token mixing using large-kernel dynamic convolutions by making use of the aforementioned convolution theorem. This equivalent could reduce the complexity of token mixing from $\mathcal{O}(N^2)$ to $\mathcal{O}(N \log N)$ thanks to adopting Fast Fourier Transforms (FFT), which is more computationally efficient.


With the key insight above, we propose Adaptive Frequency Filtering (\ourop) token mixer. In this neural operator, the latent representations (\ieno, a set of tokens) are transferred from its original latent space to a frequency space via a 2D discrete Fourier transform applied spatially. In this way, we get the frequency representations whose spatial positions correspond to different frequency components. We adopt an extremely lightweight network to learn instance-adaptive masks from these frequency representations, and then calculate the Hadamard product between the learned masks and the frequency representations for adaptive frequency filtering. The filtered representations are transferred back to the original latent space via an inverse Fourier transform. The features after this inverse transform could be viewed as the results of token mixing with depthwise convolution kernels whose spatial dimensions are as large as those of latent representations (\ieno, the token set). According to the convolution theorem \cite{mcgillem1991continuous}, our proposed operation mathematically equals to taking the tensors of applying an inverse Fourier transform to the learned masks in the Fourier domain as the corresponding kernel weights and perform convolution with this kernel in the original domain. Detailed introduction, demonstration and analysis are given in subsequent sections.

Furthermore, we take the proposed \ourop~token mixer as the primary neural operator and assemble it into an \ourop~block together with a plain channel mixer. \ourop~blocks serve as the basic units for constructing efficient vision backbone, dubbed \ours. We evaluate the effectiveness and efficiency of our proposed \ourop~token mixer by conducting extensive ablation study and comparison across diverse vision tasks and model scales.

Our contributions can be summarized in the following:

\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
\item We reveal that adaptive frequency filtering in the latent space can serve as efficient global token mixing with large dynamic kernels, and propose Adaptive Frequency Filtering (\ourop) token mixer.
\item We conduct theoretical analysis and empirical study to compare our proposed \ourop~token mixer with other related frequency-domain neural operators from the perspective of information fusion for figuring out what really matters for the effects of token mixing.
\item We take \ourop~token mixer as the primary neural operator to build a lightweight vision backbone \ours. \ours~achieves the state-of-the-art accuracy and efficiency trade-offs compared to other lightweight network designs across a broad range of vision tasks. An experimental evidence is provided in Fig.\ref{fig:paramflops-acc}.
% \item We have discovered that a global adaptive token mixer can be efficiently implemented as an adaptive frequency domain filter using the convolution theorem.
% \item This insight has led us to design a novel module called \ourop~that harnesses this principle to achieve significant benefits in performance.
% \item By incorporating our module into an efficient transformer backbone \ours, we achieve state-of-the-art results on challenging computer vision tasks such as ImageNet classification, object detection, and segmentation.
\end{itemize}


\section{Related Work}

\subsection{Token Mixing in Deep Learning}

Mainstream neural network families, \ieno, CNNs, Transformers, MLPs, differ in their ways of token mixing, as detailed in \cite{wei2022activemlp}. CNNs \cite{o2015introduction} mix tokens with the learned weights of convolution kernels where the spatial kernel size determines the mixing scope. Commonly, the weights are deterministic and the scope is commonly a local one. Transformers \cite{vaswani2017attention,dosovitskiy2020image} mix tokens with pairwise correlations between query and key tokens in a local \cite{liu2021swin,chu2021twins} or global\cite{dosovitskiy2020image,touvron2021training} range. These weights are semantic-adaptive but computationally expensive due to the $\mathcal{O}(N^2)$ complexity. MLPs commonly mix tokens with deterministic weights in manually designed scopes \cite{chen2022cyclemlp,touvron2021resmlp,tolstikhin2021mlpmixer,zhang2021morphmlp} wherein the weights are the network parameters. This work aims to design a generally applicable token mixer for lightweight neural networks with three merits: computation-efficient, semantic-adaptive and effective in the global scope.


\subsection{Lightweight Neural Networks}
Lightweight neural network designs have been of high values for practical deployments.
% of deep learning technologies, especially on mobile devices. 
CNNs, Transformers, and MLPs have their own efficient designs. MobileNets series \cite{howard2017mobilenets,sandler2018mobilenetv2,howard2019searching} introduce depthwise and pointwise convolutions as well as modified architectures for improving the efficiency. Shufflenet series \cite{zhang2018shufflenet,ma2018shufflenet} further improve pointwise convolution via shuffle operations.  MobileViT \cite{mehta2021mobilevit} combines lightweight MobileNet block and multi-head self-attention blocks. Its follow-up versions further improve it with a linear-complexity self-attention method \cite{mobilevitv2}. Besides, there are many works reducing the complexity of self-attention via reducing the region of token mixing \cite{liu2021swin,chu2021twins,pan2022fast,li2022next} or various mathematical approximations \cite{guo2019low,xiong2021nystromformer,ma2021luna}. Many efficient MLPs limit the scope of token mixing to horizontal and vertical stripes \cite{zhang2022morphmlp,hou2022vision,tang2022sparse} or a manually designed region \cite{chen2021cyclemlp}.

\subsection{Frequency-domain Deep Learning}
\label{sec: related-freq}
Frequency-domain analysis has been a classical tool for conventional signal processing \cite{baxes1994digital,pitas2000digital} for a long time. Recently, frequency-domain methods begin to be introduced to the field of deep learning for analyzing the optimization \cite{xu2019training,yin2019fourier} and generalization \cite{wang2020high,xu2018understanding} capabilities of Deep Neural Networks (DNNs). Besides these, frequency-domain methods are integrated into DNNs to learn non-local \cite{chi2020fast,rao2021global,li2020fourier,guibas2021adaptive} or domain-generalizable \cite{lin2022deep} representations. Our proposed method might be similar to them at first glance but actually differs from them in both modelling perspectives and architecture designs. These five works propose different frequency-domain operations by introducing convolutions \cite{chi2020fast}, elementwise multiplication with trainable network parameters \cite{rao2021global}, matrix multiplication with trainable parameters \cite{li2020fourier}, groupwise MLP layers \cite{guibas2021adaptive} and elementwise multiplication with spatial instance-adaptive masks \cite{lin2022deep} to frequency-domain representations, respectively. All of them are not designed for the same purpose with ours. We provide detailed mathematical analysis on their shortcomings as token mixers and conduct extensive experimental comparisons in the following sections.
% \ieno, FFC \cite{chi2020fast}, GFNet \cite{rao2021global}, FNO \cite{li2020fourier}, AFNO \cite{guibas2021adaptive} and DFF \cite{lin2022deep}
% \subsection{Frequency in Deep Learning}
% Frequency analysis has long been used in conventional digital image processing \cite{pitas2000digital,baxes1994digital}. Recently, frequency-based operations, such as Fourier transform, have been incorporated into deep learning for various purposes, including: 1) accelerating training or optimizing deep neural networks (DNNs) \cite{mathieu2013fast,pratt2017fcnn,li2020falcon,prabhu2020butterfly,nair2020fast,chitsaz2020acceleration}; 2) achieving effective data augmentation \cite{yang2020fda,xu2021fourier,liu2021feddg,huang2021fsdr}; 3) learning informative representations of non-local receptive fields \cite{chi2020fast,rao2021global,yi2021contrastive,suvorov2022resolution,mao2021deep}; and 4) analyzing and understanding DNN behaviors \cite{yin2019fourier,xu2019training,xu2018understanding,wang2020high}. Previous theoretical studies have shown that different frequency components have different priorities during training and contribute differently to feature robustness. This motivates us to enhance the generalization capability of DNNs by modulating the feature components of different frequencies.

% In \cite{chi2020fast}, a 1$\times$1 convolution in the frequency domain is used to learn global representations of intermediate features after FFT. However, such representations capture global characteristics while losing local ones, making them complementary with the features learned in the original latent space. To address this, a two-branch architecture is proposed in \cite{chi2020fast} to fuse these two types of features. While our work also encounters this problem, it is not our focus. Therefore, we adopt our proposed frequency filtering operation in the spectral transformer branch of the two-branch architecture proposed in \cite{chi2020fast} for our effectiveness evaluation. Additionally, in \cite{rao2021global}, a learnable filter layer is used for self-attention (i.e., transformer) to mix tokens at different spatial locations. Although it may appear similar to our approach at first glance, the learnable filter in \cite{rao2021global} is implemented by network parameters, whereas ours is the network output, making it instance-adaptive. We provide a theoretical analysis and experimental comparison between the two approaches in the following sections. Finally, our goal is to improve the generalization capability of DNNs, which differs from the purpose of token mixing.

% \subsection{Vision Transformer for mobile device}
% Although ViT variants have been designed with efficiency in mind, they are not suitable for on-device applications, except for MobileViTs~\cite{mehta2021mobilevit}. However, compared to the best light-weight CNNs such as MobileNets~\cite{howard2017mobilenets,sandler2018mobilenetv2} and EfficientNets~\cite{tan2019efficientnet}, MobileViTs still have lower on-device accuracy and efficiency. To address this, we propose a new family of efficient ViTs that can achieve comparable or even better tradeoffs in terms of accuracy and efficiency compared to both CNNs and ViTs. % In addition, we conduct extensive on-device evaluations and provide energy consumption analysis, which is critical but often missing in previous work.

% Figure environment removed

% \section{Adaptive Frequency Filtering Token Mixer}
\section{Method}

We first describe a unified formulation of token mixing, then introduce our proposed Adaptive Frequency Filtering (\ourop) token mixer. We further analyze what properties matter for a frequency-domain operation in terms of its effects on token mixing. We finally introduce \ours~which is a lightweight backbone with \ourop~token mixer as its core.


\subsection{Unified Formulation of Token Mixing}

Token mixing is of high importance since learning non-local representations is critical for visual understanding \cite{wang2018non,dosovitskiy2020image,touvron2021training}. In most mainstream neural networks, the input image is firstly patchified into a feature tensor $\mathbf{X}\in \mathbb{R}^{H \times W \times C}$ whose spatial resolution is $H\times W$ and the number of channels is $C$. This feature tensor could be viewed as a set of tokens, in which each token can be denoted as $\mathbf{x}\in \mathbb{R}^{1 \times 1 \times C}$. The updated token for a query $\mathbf{x}^q$ after token mixing in its contextual region $\mathcal{N}(\mathbf{x}^q)$ can be formulated in a unified form:
% As we discussed above, one of the major challenges in designing efficient vision transformers is to reduce the computational cost of their global operations, which are based on attention mechanisms. Attention can be formulated as follows:
\begin{equation}
    \hat{\mathbf{x}^q} = \sum_{i\in\mathcal{N}(\mathbf{x}^q)} 
    \bm{\omega}^{i\rightarrow q} \times \phi(\mathbf{x}^i), 
    \label{eq: token-mix}
\end{equation}
% $$ \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $$
where $\hat{\mathbf{x}^q}$ refers to the updated $\mathbf{x}^q$ and $\mathbf{x}^i$ refers to the tokens in $\mathcal{N}(\mathbf{x}^q)$. $\phi(\cdot)$ denotes the embeding functions. $\bm{\omega}^{i\rightarrow q}$ represents the weights of information fusion from token $\mathbf{x}^i$ to the updated $\mathbf{x}^q$. The symbol $\times$ could be Hadamard product or matrix multiplication.

We revisit the prevailing token mixing methods in different types of network architectures in terms of their effectiveness and efficiency. For CNNs, tokens are mixed by matrix multiplication with deterministic network parameters as the mixing weights. Here, the kernel sizes of convolutions determine the scopes of token mixing. This makes mixing in a global scope quite costly due to the quadratically increased parameters and FLOPs as the kernel size increases. Transformers mix tokens with pairwise correlations between query and key tokens. Its computational complexity is $\mathcal{O}(N^2)$ ($N$ is the total number of tokens), limiting its applications in lightweight networks. Like CNNs, MLPs also mix tokens with deterministic network parameters. The scope of token mixing in advanced MLPs \cite{chen2022cyclemlp,touvron2021resmlp,tolstikhin2021mlpmixer,zhang2021morphmlp} are commonly manually design, where the globality comes at the cost of huge computational complexity. They are all not specifically designed for lightweight neural networks.


This work aims to deign a \textit{computationally efficient}, \textit{semantically adaptive} and \textit{global-scope} token mixer for lightweight networks. This requires a large $\mathcal{N}(\mathbf{x}^q)$ and instance-adaptive $\bm{\omega}^{i\rightarrow q}$ with less network parameters and low computation costs as possible.


\subsection{Adaptive Frequency Filtering Token Mixer}
\label{sec:AFF-token-mixer}
% \subsection{Model Design and equivalence}
% \subsection{\ourop}

We apply the convolution theorem \cite{mcgillem1991continuous,rabiner1975theory,oppenheim1999discrete} to deep learning for designing a token mixer with aforementioned merits for lightweight neural networks. 
% This theorem states that a convolution in one domain mathematically equals the Hadamard product in its corresponding Fourier domain.
Based on this theorem, we reveal that \textit{adaptive frequency filters can serve as efficient global token mixers}. In the following, we introduce its mathematical modelling, architecture design and the equivalence between them for our proposed token mixer.
% for our proposed \ourop~token mixer.


\paragraph{Modelling.} To simplify understanding, we frame token mixing in the form of global convolution, succinctly denoted by $\mathbf{\hat{X}}=\mathcal{K}*\mathbf{X}$. For the query token at position $(h, w)$, \ieno, $\mathbf{X}(h, w)$, Eq.(\ref{eq: token-mix}) can be reformulated as:
\begin{equation}
    \mathbf{\hat{X}}(h, w) =\!\sum_{h'=-\lfloor \frac{H}{2}  \rfloor}^{\lfloor \frac{H}{2} \rfloor} \sum_{w'=-\lfloor \frac{W}{2} \rfloor}^{\lfloor \frac{W}{2} \rfloor} \mathcal{K}(h', w') \mathbf{X}(h-h', w-w'),
    % \mathbf{\hat{X}}(h, w) = \sum_{h'=0}^{H-1}\sum_{w'=0}^{W-1} \mathcal{K}(h', w') \mathbf{X}(h-h', w-w'),
    \label{eq: modelling-1}
\end{equation}
where $\mathbf{\hat{X}}(h, w)$ represents the updated token for $\mathbf{X}(h, w)$ after token mixing. $H$ and $W$ are the height and weight of the input tensor, respectively. $\mathcal{K}(h', w')$ denotes the weights for token mixing, implemented by a global convolution kernel which has the same spatial size with $\mathbf{X}$. The padding operation for $\mathbf{X}$ is omitted here for simplicity and the specific padding method is introduced in the subsequent parts.


With the expectation for our proposed token mixer as a \textit{semantic-adaptive} and \textit{global-scope} one, the weights $\mathcal{K}$ for token mixing should be adaptive to $\mathbf{X}$ and of large spatial size. As illustrated by the lower right subfigure in Fig.\ref{fig:framework}, a straightforward way for enabling $\mathcal{K}$ adaptive to $\mathbf{X}$ is to implement it with a dynamic convolution kernel \cite{jia2016dynamic,chen2020dynamic,he2019dynamic,zhang2021accurate}, \ieno, inferring weights of $\mathcal{K}$ with $\mathbf{X}$ as the inputs of a sub-network. However, adopting dynamic convolutions is usually computational costly, even more so, when using large-kernel ones. This thus imposes big challenges in designing an efficient token mixer for lightweight networks along this way. Next, we introduce an efficient method as its equivalent implementation by making use of the convolution theorem \cite{mcgillem1991continuous}.

\paragraph{Architecture.} The convolution theorem \cite{mcgillem1991continuous,oppenheim1999discrete,rabiner1975theory} for inverse Fourier transform  states that a convolution in one domain mathematically equals the Hadamard product in its corresponding Fourier domain. This inspires us to propose a lightweight and fast architecture (illustrated by the lower left part of Fig.\ref{fig:framework}) as an extremely efficient implementation of our modelling above.

Given feature $\mathbf{X} \in \mathbb{R}^{H\times W \times C}$, \ieno, a set of tokens in the latent space, we adopt Fast Fourier Transform (FFT) to obtain the corresponding frequency representations $\mathbf{X}_{F}$ by $\mathbf{X}_{F} = \mathcal{F}(\mathbf{X})$. The detailed formulation of $\mathcal{F}(\cdot)$ is:
% Specifically, to operate on input feature $\mathbf{X} \in \mathcal{R}^{C\times H\times W}$ in frequency space, we use the Fast Fourier Transform (FFT) to obtain the transformed feature $\mathbf{X}_{F}$:
\begin{equation}
    \mathbf{X}_{F} (u, v) = \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X} (h, w) e^{-2\pi i(uh+vw)}.
    \label{eq: fft}
\end{equation}
As indicated by Eq.(\ref{eq: fft}), features of different spatial positions in $\mathbf{X}_{F}$ correspond to different frequency components of $\mathbf{X}$. They incorporate global information from $\mathbf{X}$ with a transform of $\mathcal{O}(N\log N)$ complexity.
% Here, $\mathcal{F} (\cdot)$ represents the FFT operator. By using the FFT algorithm, we can perform the transformation efficiently with a computational complexity of O(Nlog(N)).

% We generate an attention mask with the same size as the input feature in the frequency domain, and multiply it with the frequency feature.

% $\mathbf{X}_{F} \in \mathbb{R}^{H \times (\left\lfloor\frac{W}{2}\right\rfloor+1) \times 2C}$
We apply the aforementioned convolution theorem to achieve efficient global token mixing for $\mathbf{X}$ by filtering its frequency representation $\mathbf{X}_{F}$ with a learnable instance-adaptive mask. We further adopt inverse FFT to the filtered $\mathbf{X}_{F}$ for getting the updated feature representations $\mathbf{\hat{X}}$ in the original latent space. This process can be formulated as:
\begin{equation}
    \mathbf{\hat{X}} = \mathcal{F} ^{-1} [\mathcal{M}(\mathcal{F} (\mathbf{X})) \odot \mathcal{F} (\mathbf{X})],
    % \mathbf{\hat{X}} = \mathcal{F} ^{-1} \left(\mathcal{M}(\mathcal{F} (\mathbf{X})) \odot \mathcal{F} (\mathbf{X})\right),
    \label{eq: aff}
\end{equation}
% where $\mathcal{M}(\mathcal{F}(\mathbf{X})) \in \mathbb{R}^{H \times (\left\lfloor\frac{W}{2}\right\rfloor+1) \times 2C}$ 
where $\mathcal{M}(\mathcal{F}(\mathbf{X}))$ is the mask tensor learned from $\mathbf{X}_{F}$, which has the same shape with $\mathbf{X}_{F}$. As shown in the lower left subfigure in Fig.\ref{fig:framework}, to make the network lightweight as possible, $\mathcal{M}(\cdot)$ is efficiently implemented by a group $1\times 1$ convolution (linear) layer, followed by a ReLU function and another group linear layer. $\odot$ denotes Hadamard product, also known as elementwise multiplication, and $\mathcal{F}^{-1}(\cdot)$ denotes inverse Fourier transform. Here, $\mathbf{\hat{X}}$ can be viewed as the results of global adaptive token mixing for $\mathbf{X}$, which is mathematically equivalent to adopting a large-size dynamic convolution kernel as the weights for token mixing. The equivalence is introduced in the following.

\paragraph{Equivalence.}
The convolution theorem still applies to the latent representations of neural networks. The multiplication of two signals in the Fourier domain equals to the Fourier transform of a convolution of these two signals in their original domain. When applying this to the frequency-domain multiplication in Fig.(\ref{fig:framework}), we know that:
\begin{equation}
    \mathcal{M}(\mathcal{F}(\mathbf{X})) \odot \mathcal{F} (\mathbf{X}) = \mathcal{F}\{\mathcal{F} ^{-1}[\mathcal{M}(\mathcal{F} (\mathbf{X}))] \ast \mathbf{X}\}.
    \label{eq: conv-theorem}
\end{equation}
% \begin{equation}
%     \mathcal{M}(\mathcal{F}(\mathbf{X})) \odot \mathcal{F} (\mathbf{X}) = \mathcal{F}\left(\mathcal{F} ^{-1}(\mathcal{M}(\mathcal{F} (\mathbf{X}))) \ast \mathbf{X}\right).
%     \label{eq: conv-theorem}
% \end{equation}
Combining Eq.(\ref{eq: aff}) and Eq.(\ref{eq: conv-theorem}), it is easy to get that:
\begin{equation}
    \mathbf{\hat{X}} = \mathcal{F}^{-1}[\mathcal{M}(\mathcal{F} (\mathbf{X}))] \ast \mathbf{X},
    % \mathbf{\hat{X}} = \mathcal{F}^{-1}(\mathcal{M}(\mathcal{F} (\mathbf{X}))) \ast \mathbf{X},
    \label{eq: equivalent-token-mix}
\end{equation}
where $\mathcal{F}^{-1}(\mathcal{M}(\mathcal{F} (\mathbf{X})))$ is a tensor of the same shape with $\mathbf{X}$, which could be viewed as a dynamic depthwise convolution kernel as large as $\mathbf{X}$ in spatial. This kernel is adaptive to the contents of $\mathbf{X}$. Due to the property of Fourier transform \cite{mcgillem1991continuous}, a circular padding is adopted to $\mathbf{X}$ here as shown in Fig.\ref{fig:framework}. So far, we understand why the operation in Eq.(\ref{eq: aff}) mathematically equals to a global-scope token mixing operation with semantic-adaptive weights.

\subsection{Analysis}
\label{sec:analysis}
% \subsection{Discussion about operation design in Frequency domain}

As introduced in Sec.\ref{sec: related-freq}, there have been some studies applying frequency-domain methods to DNN for learning non-local or domain-generalizable representations in previous works \cite{chi2020fast,rao2021global,li2020fourier,guibas2021adaptive,lin2022deep}. They are all designed for different purposes with ours. In this section, we revisit the frequency-domain operations in these works from the perspective of token mixing and compare our design with them.
% In this section, we compare the proposed frequency-domain design in our \ourop~token mixer with other frequecy-domain operations in these related works from the perspective of token mixing.

% In this section, we will analyze the essence of previous frequency operations, which is filtering instead of linear operation. We will also examine the limitations of previous frequency operations, specifically the lack of adaptability in filtering other frequencies. The lack of adaptability of the filtering mas is threefold: instance-adaptablity, rule-based vs learnable, channel-shared vs channel adaptive.

FFC~\cite{chi2020fast} and AFNO~\cite{guibas2021adaptive} adopt linear (also known as $1\!\times\!1$ convolution) layers with non-linear activation functions to the representations in the frequency domain. Specifically, AFNO~\cite{guibas2021adaptive} adopt a linear layer followed by a ReLU function, another linear layer and a SoftShrink\footnote{\url{https://pytorch.org/docs/stable/generated/torch.nn.Softshrink.html}} function to the frequency representations after Fourier transforms, which can be briefly described as \textit{FFT$\to$Linear$\to$ReLU$\to$Linear$\to$SoftShrink$\to$iFFT}. Here, linear layer and Fourier transform are in fact commutative, \ieno, $\operatorname{Linear}(\mathcal{F}(\mathbf{X}))\!=\!\mathcal{F}(\operatorname{Linear}(\mathbf{X}))$, which can be proved with the distributive property of matrix multiplication by:
\begin{equation}
\begin{split}
    &\ \mathbf{W}_{Linear} \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w) e^{-2\pi i(uh+vw)} \\
    = &\ \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \left(\mathbf{W}_{Linear} \mathbf{X}(h, w)\right) e^{-2\pi i(uh+vw)},
\end{split}
\label{eq: commutative}
\end{equation}
where $\mathbf{W}_{Linear}$ denotes the parameters of a linear layer. We know that successive Fourier transform and its inverse transform equal to an identity function. Thus, the architecture of AFNO could be rewrote as: \textit{FFT$\to$Linear$\to$ReLU$\to$\\(iFFT$\to$FFT)$\to$Linear$\to$SoftShrink$\to$iFFT}. Upon the commutative law proved in Eq.(\ref{eq: commutative}), we can know this architecture is in fact equivalent to \textit{Linear$\to$\textbf{FFT$\to$ReLU$\to$iFFT}\\$\to$Linear$\to$\textbf{FFT$\to$SoftShrink$\to$iFFT}}. Now, it is easy to find that only ReLU and SoftShrink functions remain in the Fourier domain. These two deterministic functions cannot achieve semantic-adaptive filtering as our proposed \ourop~token mixer does. The same problem also exists in FFC~\cite{chi2020fast}.

GFNet~\cite{rao2021global} and FNO~\cite{li2020fourier} multiply the representations after Fourier transforms with trainable network parameters. GFNet~\cite{rao2021global} adopts elementwise multiplication while FNO~\cite{li2020fourier} uses matrix multiplication. Both of them are not semantic-adaptive since the masks implemented by network parameters are shared over different instances and fixed after training. Besides, they cannot support for variable-size inputs since the shapes of these masks are fixed, leading to the lack of flexibility in their practical using.

% \label{lemma1}
% Lemma1: The linear layer in the frequency domain is equivalent to the linear layer in the original space. In other words, ``FFT-Linear-iFFT = Linear''.

% Since the Linear operation acts on the channel dimension and the FFT operation acts on the "$H\times W$" dimension, which are orthogonal, we can swap the order of these operations. This results in cancelling out the FFT and inverse FFT operations, and the formula simplifies to an ordinary linear layer. The formula derivation is shown below:

% \begin{equation}
% \begin{split}
%               & \mathcal{F} ^{-1} ( Linear(\mathcal{F} (\mathbf{X}))) \\
%             = &\mathcal{F} ^{-1} ( \sum_{c=0}^{C-1} \mathbf{w}_{c} (\sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w, c) e^{-2\pi i(uh+vw)}) \\
%             = &\mathcal{F} ^{-1} ( \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} (\sum_{c=0}^{C-1} \mathbf{w}_{c} \mathbf{X}(h, w, c)) e^{-2\pi i(uh+vw)} \\
%             = &\mathcal{F} ^{-1}(\mathcal{F} (Linear (\mathbf{X}))) \\
%             = &Linear (\mathbf{X})
% \end{split}
% \label{eq: fft}
% \end{equation}

% \begin{equation}
% \begin{split}
%               &\ \mathcal{F} ^{-1} ( Conv(\mathcal{F} (\mathbf{X}))) \\
%             = &\ \mathcal{F} ^{-1} ( \mathbf{W}_{Conv} \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w) e^{-2\pi i(uh+vw)}) \\
%             = &\ \mathcal{F} ^{-1} ( \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \left(\mathbf{W}_{Conv} \mathbf{X}(h, w)\right) e^{-2\pi i(uh+vw)}) \\
%             = &\ \mathcal{F} ^{-1}(\mathcal{F} ( Conv(\mathbf{X}))) \\
%             = &\ Conv (\mathbf{X})
% \end{split}
% \label{eq: fft}
% \end{equation}

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cccccc}
    \toprule
    \multicolumn{1}{c|}{Properties} & FFC & AFNO & \!GFNet\! & FNO & DFF & Ours \\
    % \multicolumn{1}{c|}{Properties} & FFC~\cite{chi2020fast}   & AFNO~\cite{guibas2021adaptive}  & GFNet~\cite{rao2021global}   & FNO~\cite{li2020fourier}   & DFF~\cite{lin2022deep}   & \ourop~(Ours) \\
    \midrule
    % Learnable &   \ding{55}    &    \ding{55}   &    \ding{51}   &   \ding{51}    &    \ding{51}   &  \ding{51}\\
    % Instance-adaptive &   \ding{51}     & \ding{51}   &    \ding{55}   &   \ding{55}    &   \ding{51}    & \ding{51} \\
    Semantic-adaptive &   \ding{55}     & \ding{55}   &    \ding{55}   &   \ding{55}    &   \ding{51}    & \ding{51} \\
    Variable-size input & \ding{51} & \ding{51} & \ding{55}  & \ding{55}  & \ding{51} & \ding{51} \\
    Channel-wise mix &  \ding{51}      &  \ding{51}     &   \ding{51}    &   \ding{51}    &  \ding{55}     & \ding{51} \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{+1.6mm}
    \caption{Comparisons of our proposed \ourop~token mixer with other frequency-domain neural operators in terms of three important properties for token mixing.}
  % \caption{Comparison of our \ourop~with other frequency-domain neural operators based on four criteria: learnability of the filtering, instance-adaptivity of the filtering, adaptivity to variable input resolution, and channel adaptivity of the filtering.}
  \label{tab:compare-operations}%
\end{table}%

DFF~\cite{lin2022deep} learns a spatial mask to filter out frequency components that are not conductive to domain generalization. It is proposed for domain generalization problems in which only spatial mask is needed as studied in~\cite{lin2022deep} since different spatial position of the features after a Fourier transform correspond to different frequency components. However, it is not competent as a token mixer since the learned mask is shared along the channel dimension. This means that the weights for its equivalent token mixing are shared for different channels. However, different channels commonly represent different semantic attributes \cite{bau2020understanding,wu2021stylespace}, thus requiring adaptive weights in token mixing.
 % We will experimentally demonstrate the importance of this in the following parts.

We summarize the comparisons of different frequency-domain designs in terms of three important properties for token mixing in Table \ref{tab:compare-operations}. The results of experimental verification are in Table \ref{tab:freq_operations} as follows.


\subsection{Network Architectures}

With our \ourop~token mixer as the core neural operator, we introduce its corresponding module and network design.
% In this section, we first introduce how we use \ourop~to build an efficient Transformer block (\ourblock), and then describe how we stack multiple \ourblock s and a few convolution layers to build the efficient Transformer backbone (\ours).

\paragraph{\ourblock}
For the output $\mathbf{X}^{l-1}$ of the ($l\!-\!1$)-th \ourblock, we adopt the commonly used module MBConv~\cite{mobilevitv2,tan2019efficientnet,tan2021efficientnetv2,mehta2021mobilevit,tu2022maxvit} with Layer Normalization (LN) for channel mixing, then feed it to our proposed \ourop~token mixer for global token mixing to get the output of $l$-th \ourop~block. Skip-connections for channel mixing and token mixing are adopted to facilitate model training. The entire architecture of \ourblock~can be formulated as:

\begin{equation}
\begin{split}
    & \hat{\mathbf{X}}^l=\operatorname{MBConv}^l\left(\operatorname{LN}\left(\mathbf{X}^{l-l}\right)\right)+\mathbf{X}^{l-l} \\
    & \mathbf{X}^l=\operatorname{\ourop}^l\left(\operatorname{LN}\left(\hat{\mathbf{X}}^l\right)\right)+\hat{\mathbf{X}}^l
\end{split}
\label{eq: block}
\end{equation}

% Following previous works, the MLP in our architecture consists of a Linear layer, followed by a depth-wise 3X3 Convolution, and finally another Linear layer. Compare to traditional transformer block, we swap the token Mixer and the Channel Mixer.
% Moving the Channel Mixer, a local operation, to the front of the Token Mixer, a global operation, can make the overall model change more smoothly from local operations at the head to global operations at the tail. This smooth transformation is achieved by increasing the depth of the model in general transformers, but increasing the depth of the model will slow down the model speed. Our design can achieve this effect while ensuring model efficiency.

\paragraph{\ours} 
We stack multiple \ourop~blocks for constructing a lightweight backbone network, namely \ours, as shown in Fig.\ref{fig:framework}. Following the common practices \cite{mehta2021mobilevit,mobilevitv2}, we employ a convolution stem for tokenization and a plain fusion for combining local and global features at each stage. We build three versions of \ours~with different numbers of channels, yielding different parameter scales. \ours~and its tiny (\ours-T) and extremely tiny (\ours-ET) versions have 5.5M, 2.6M and 1.4M parameters, respectively. Their detailed configurations are in the Supplementary.
% We propose three hierarchical backbone variants with different channel dimensions, namely \ours-ET, \ours-T, and \ours. Following previous works, we adopt a convolution stem to cpature local structures, such as lines and edges among the neighboring pixels. the Conv Stem consists of a 3x3 convolutional layer with a downsampling stride of 2, followed by four layers of MBConv. MBConv is the InvertedResidualBlock proposed by MNetv2. As shown in Fig~\ref{fig:framework}. After the Conv Stem, there are three stages with different feature resolutions. Each stage is composed of a layer of MBConv with stride 2 as the downsample layer and $N_i$ cascaded \ourblock. At the end of each stage.

% Note that we add a fusion structure which is similar to a skip connection, and swap the location of channel mixer and token mixer which can smooth the model transition from local to global operations, especially when the model depth is not too deep. 
% We place more details discussion and ablation studies about it in supplementary, and more details about \ours-ET, \ours-T, and \ours~are also shown in supplementary material.

\section{Experiments}

We evaluate our proposed \ourop~token mixer by conducting comparisons with the state-of-the-art lightweight networks and extensive ablation studies for its design.
% In this section, we conducted a comprehensive comparison of our proposed model with state-of-the-art models on three different tasks, namely image classification, object detection, and segmentation. Our comparison highlighted both the superior performance and efficiency of our model. Furthermore, we performed a thorough analysis and ablation studies of the Adaptive Frequency Filtering (AFF) module, and comparing it with existing frequency domain operations to demonstrate its effectiveness. Then we visualize the instance-adaptive filtering masks and its equavvlents global convolution kernels. We place the ablation studies on architecture design and latency analysis in the supplementary material.



% \subsection{Effectiveness for Lightweight Networks}
% % \subsection{Comparisons with SOTA Lightweight Networks}
% \tcr{We evaluate the effectiveness of the proposed \ourop~token mixer on building lightweight networks in this section.}

\subsection{Image Classification}

\paragraph{Settings.} We train different versions of our proposed lightweight networks \ours~as backbones on ImageNet-1k dataset~\cite{russakovsky2015imagenet} from scratch. All models are trained for 300 epochs on 8 NVIDIA V100 GPUs with a batch size of 1024. More implementation details are in the Supplementary.

% [ToDO: zhizheng]: move details to supplementary.
% We train our \ours~on the ImageNet-1k dataset~\cite{russakovsky2015imagenet} which has 1.28 million training samples and 50,000 validation samples.  The resolution of input images is $256\times 256$. The loss function used is cross-entropy with label smoothing. The optimizer used is AdamW with weight decay of 0.05. The learning rate is linearly increased from 1.0e-06 to 0.002 for the first 20k iterations and then decayed using a cosine annealing scheduler till the end of training. The model is trained for 300 epochs on 8 NVIDIA GPUs with a batch size of 1024 images. The model weights are initialized using kaiming normal and an exponential moving average of model weights is used for inference. We place more implementation details in the supplementary material.


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[h]
  \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|c|c|cc|c}
    \toprule
    \multicolumn{1}{c|}{Model} & Pub.  & Res.  & \makecell[c]{Param.  \\ (M)} & \makecell[c]{FLOPs \\ (G)} & Top-1 \\
    \midrule
    MNetv1-0.5~\cite{howard2017mobilenets} & arXiv17 & $224^2$ & 1.3 & 0.2 & 63.7 \\
    MViT-XXS~\cite{mehta2021mobilevit} & ICLR22 & $256^2$ & 1.3   & 0.4   & 69.0 \\
    EdgeNext-XXS~\cite{maaz2023edgenext} & ECCV22 & $256^2$ & 1.3 & 0.3 & 71.2 \\
    MViTv2-0.5~\cite{mobilevitv2} & TMLR23 & $256^2$ & 1.4   & 0.5   & 70.2 \\
    % EMO-1M~\cite{zhang2023rethinking} & -     & $224^2$ & 1.3   & 0.3   & 71.5 \\
    % \midrule
    \rowcolor[gray]{0.9}\ours-ET  & -     & $256^2$ & 1.4   & 0.4   & 73.0 \\
    \midrule
    MNetv3-L-0.5~\cite{howard2019searching} & ICCV19 & $224^2$ & 2.6 & 0.1 & 68.8 \\
    MFormer-52~\cite{chen2022mobile}  & CVPR22 & $224^2$ & 3.6   & 0.1   & 68.7 \\
    PVTv2-B0~\cite{wang2022pvt} & CVM22 & $224^2$ & 3.7 & 0.6 & 70.5 \\
    MViT-XS~\cite{mehta2021mobilevit} & ICLR22 & $256^2$ & 2.3   & 1.0   & 74.8 \\
    EdgeNext-XS~\cite{maaz2023edgenext} & ECCV22 & $256^2$ & 2.3 & 0.5 & 75.0 \\
    EFormer-S0~\cite{li2022rethinking} & arXiv22 & $224^2$ & 3.5 & 0.4 & 75.7  \\
    MViTv2-0.75~\cite{mobilevitv2} & TMLR23 & $256^2$ & 2.9   & 1.0   & 75.6 \\
    % EMO-2M~\cite{zhang2023rethinking} & -     & $224^2$ & 2.3   & 0.4   & 75.1 \\
    % \midrule
    \rowcolor[gray]{0.9}\ours-T  & -     & $256^2$ & 2.6   & 0.8   & 77.0 \\
    \midrule
    MNetv2~\cite{sandler2018mobilenetv2} & CVPR18 & $224^2$ & 6.9   & 0.6   & 74.7 \\
    ShuffleNetV2~\cite{ma2018shufflenet} & ECCV18 & $224^2$ & 5.5   & 0.6   & 74.5 \\
    MNetv3~\cite{howard2019searching} & ICCV19 & $224^2$ & 5.4   & 0.2   & 75.2 \\
    T2T-ViT~\cite{yuan2021tokens} & ICCV21 & $224^2$ & 6.9   & 1.8   & 76.5 \\
    DeiT-T~\cite{touvron2021training} & ICML21 & $224^2$ & 5.7   & 1.3   & 72.2 \\
    CoaT-Lite-T~\cite{dai2021coatnet} & ICCV21 & $224^2$ & 5.7   & 1.6   & 77.5 \\
    LeViT-128~\cite{graham2021levit} & ICCV21 & $224^2$ & 9.2   & 0.4   & 78.6 \\
    GFNet-Ti~\cite{rao2021global} & NeurIPS21 & $224^2$ & 7.0   & 1.3   & 74.6 \\
    EFormer-L1~\cite{li2022efficientformer} & NeurIPS22    & $224^2$ & 12.3  & 1.3   & 79.2 \\
    EFormer-S1~\cite{li2022rethinking} & arXiv22 & $224^2$ & 6.1 & 0.7 & 79.0 \\
    Mformer~\cite{chen2022mobile} & CVPR22 & $224^2$ & 9.4   & 0.2   & 76.7 \\
    EfficientViT~\cite{cai2022efficientvit}  & arXiv22 & $224^2$ & 7.9   & 0.4   & 78.6 \\
    EdgeViT-XS~\cite{chen2022edgevit} & ECCV22 & $256^2$ & 6.7   & 1.1   & 77.5 \\
    MOne-S3~\cite{vasu2022improved} & arXiv22 & $224^2$ & 10.1  & 1.9   & 78.1 \\
    MViT-S~\cite{mehta2021mobilevit} & ICLR22 & $256^2$ & 5.6   & 2.0   & 78.4 \\
    EdgeNext-S~\cite{maaz2023edgenext} & ECCV22 & $256^2$ & 5.6 & 1.3 & 79.4 \\
    MViTv2-1.0~\cite{mobilevitv2} & TMLR23 & $256^2$ & 4.9   & 1.8   & 78.1 \\
    % EMO-5M~\cite{zhang2023rethinking} & -     & $224^2$ & 5.1   & 0.9   & 78.4 \\
    % \midrule
    \rowcolor[gray]{0.9}\ours  &   -    & $256^2$ & 5.5   & 1.5   & 79.8 \\
    \bottomrule
    \end{tabular}}%
    \vspace{+1.6mm}
  \caption{Comparisons of our proposed \ours~with other state-of-the-art lightweight networks on ImageNet-1K classification over different model scales (\ieno, $<$2M, 2M $\sim$ 4M and $>$ 4M). For conciseness, Pub., Res., Param., MNet, MOne, MFormer, EFormer and MViT are short for Publication, Resolution, Parameters, MobileNet, MobileOne, MobileFormer, EfficientFormer and MobileViT, respectively.}
  % \caption{Comparison of our method with other state-of-the-art models on the three parameter size levels ( $<$2M, 2M $\sim$ 4M and $>$ 4M) for image classification on Imagenet-1K dataset. Abbreviated Res., MNet, MOne, MFormer, EFormer and MViT: Resolution, MobileNet, MobileOne, MobileFormer, EfficientFormer and MobileViT.}
  \label{tab:sota_imagenet}%
\end{table}%

\vspace{-2mm}
\paragraph{Results.}  We report the comparison results between our proposed \ours~and other SOTA lightweight models in Table \ref{tab:sota_imagenet}. We observe that our \ours~outperforms other lightweight networks with comparable model sizes in Top-1 accuracy. The \ours~reaches 79.8\% Top-1 accuracy with 5.5M parameters and 1.5G FLOPs. Our extremely tiny model \ours-ET attains 73\% Top-1 accuracy with sorely 1.4M and 0.4G FLOPs. As a result, \ours~achieves the best trade-offs between accuracy and efficiency. To show the comparison results more intuitively, we illustrate the accuracy and efficiency trade-offs of our \ours~and some advanced lightweight models with global token mixers in Fig.~\ref{fig:paramflops-acc}. Thanks to \ourop~token mixer, \ours~is superior to them by a clear margin across different model scales. Its superiority is especially significant when the model is extremely tiny, which demonstrates the effectiveness of \ourop~token mixer on information fusion at very low costs.
\ours, \ours-T, and \ours-ET models achieve 4202, 5304, and 7470 images/s thoughtput on ImageNet-1K tested with one NVIDIA A100 GPU, respectively, which is 13.5\%, 8.2\%, and 14.9\% faster than MobileViT-S/XS/XXS. More detailed results are in the Supplementary.

% \noindent\textbf{Compare with SOTA works.}  Table \ref{tab:sota_imagenet} provides a comparison between our proposed model, \ours, and state-of-the-art light-weight models for image classification on the Imagenet-1K~\cite{russakovsky2015imagenet} dataset. We observe that our \ours~outperforms most existing models in three different parameter size levels. Specifically, 'xx\_small' refers to models with 1M to 2M parameters, 'x\_small' refers to models with 2M to 3M parameters, and 'small' refers to models with 5M to 6M parameters. For the xx\_small level, \ours-ET achieves 73.0\%  top-1 accuracy, which is 1.8\% higher than the second-best model EdgeNext-XXS~\cite{maaz2023edgenext}. At the x\_small level, \ours-T achieves a top-1 accuracy of 77.0\%, outperforming the second-best model MOne~\cite{vasu2022improved} by 1.1\% accuracy, with parameters size decreasing by a large margin. Finally, at the small level, our \ours~ achieves a top-1 accuracy of 79.8\%, surpassing the second-best model EdgeNext-S~\cite{maaz2023edgenext} by 0.4\% accuracy. In summary, our proposed \ours~model achieves state-of-the-art performance across all three parameter size levels, demonstrating its effectiveness and competitiveness with existing approaches. For a more intuitive comparison, in Fig.~\ref{fig:paramflops-acc}, we displays the trade-off between the number of parameters, FLOPs and the top-1 accuracy, with the x-axis indicating the number of parameters, the y-axis showing the accuracy and the bubble size corresponds to the number of FLOPs. Our \ours~models demonstrate a superior compute cost (Parameters and FLOPs) versus accuracy trade-off compared to recent approaches. This suggests that our \ours~models are more computationally efficient and can achieve state-of-the-art performance with less computational resources.

% \subsection{Downstream Tasks}

% We compare \ours~with recent state-of-the-art models for object detection and segmentatin tasks, to highlight the effectiveness and efficiency of our method.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|ccc}
    \toprule
    \multirow{3}[0]{*}{\quad\quad Model} & \multicolumn{2}{c|}{Detection} & \multicolumn{3}{c}{Segmentation} \\
    & \multirow{2}[0]{*}{Param.}\!& mAP(\%) & \multirow{2}[0]{*}{Param.} & \multicolumn{2}{c}{mIOU(\%)} \\
    &      & COCO   &      & ADE20K & VOC \\
    \midrule
    MViT-XXS~\cite{mehta2021mobilevit} & 1.9   & 18.5  & 1.9   &    -   & 73.6 \\
    MViTv2-0.5~\cite{mobilevitv2} & 2.0   & 21.2  & 3.6   & 31.2  & 75.1 \\
    \rowcolor[gray]{0.9}\ours-ET & 1.9   & 21.8  &  2.2    & 33.0    & 76.1 \\
    \midrule
    MViT-XS~\cite{mehta2021mobilevit} & 2.7   & 24.8  & 2.9   &   -    & 77.1 \\
    MViTv2-0.75~\cite{mobilevitv2} & 3.6   & 24.6  & 6.2   & 34.7  & 75.1 \\
    \rowcolor[gray]{0.9}\ours-T & 3.0   & 25.3  &  3.5    & 36.9  & 77.8 \\
    \midrule
    ResNet-50~\cite{he2016deep} & 22.9  & 25.2  & 68.2  & 36.2  & 76.8 \\
    \hline
    MNetv1~\cite{howard2017mobilenets} & 5.1   & 22.2  & 11.2  &  -     & 75.3 \\
    MNetv2~\cite{sandler2018mobilenetv2} & 4.3   & 22.1  & 18.7   & 34.1  & 75.7 \\
    MViT-S~\cite{mehta2021mobilevit} & 5.7   & 27.7  & 6.4   &   -    & 79.1 \\
    MViTv2-1.0~\cite{mobilevitv2} & 5.6   & 26.5  & 9.4  & 37.0  & 78.9 \\
    EdgeNext~\cite{maaz2023edgenext} & 6.2   & 27.9  & 6.5   &  -     & 80.2 \\
    \rowcolor[gray]{0.9}\ours~ & 5.6   & 28.4  & 6.9   & 38.4  & 80.5 \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{+1.6mm}
  \caption{Comparisons of our \ours~with other state-of-the-art models for object detection on COCO dataset, and segmentation on ADE20k and VOC dataset. Here, Param., MNet and MViT are short for Paramters, MobileNet and MobileViT, respectively.}
  \label{tab:sota_downstream}%
\end{table}%


\subsection{Object Detection}

% \noindent\textbf{Implementation Details} 
\paragraph{Settings.}
We conduct object detection experiments on MS-COCO dataset~\cite{lin2014microsoft}, Following the common practices in~\cite{howard2017mobilenets,sandler2018mobilenetv2,mehta2021mobilevit,mobilevitv2, maaz2023edgenext}, we compare different lightweight backbones upon the Single Shot Detection (SSD)~\cite{liu2016ssd} framework wherein separable convolutions are adopted to replace the standard convolutions in the detection head for evaluation in the lightweight setting. In the training, we load ImageNet-1K pre-trained weights as the initialization of the backbone network, and fine-tune the entire model on the training set of MS-COCO with the AdamW optimizer for 200 epochs. The input resolution of the images is 320$\times$320. Detailed introduction for the used dataset and more implementation details are in the Supplementary.

\vspace{-4mm}
\paragraph{Results.} As shown in Table \ref{tab:sota_downstream}, the detection models equipped with \ours~consistently outperforms other lightweight CNNs or transformers based detectors in mAP across different model scales. Specifically, \ours~surpasses the second-best EdgeNext~\cite{maaz2023edgenext} by 0.5\% in mAP with 0.6M fewer parameters, and surpasses the model with ResNet-50 backbone by 3.2\% in mAP using about 1/4 of parameters. Our smallest model \ours-ET outperforms the second-best model with comparable parameters MobileViTv2-0.5~\cite{mobilevitv2} by 0.6\% in mAP with fewer parameters. These results demonstrate the effectiveness of our proposed method on capturing spatial location information required by the task of object detection at low costs.


\subsection{Semantic Segmentation}
% \subsubsection{Segmentation}

\paragraph{Settings.}
We conduct semantic segmentation experiments on two benchmarks datasets ADE20k~\cite{zhou2019semantic} and PASCAL VOC 2012~\cite{everingham2015pascal} (abbreviated as VOC). For the experiments on VOC dataset, we follow the common practices in \cite{chen2017rethinking,mehta2019espnetv2} to extend the training data with 
more annotations and data from~\cite{hariharan2011semantic} and \cite{lin2014microsoft}, respectively. The widely used semantic segmentation framework DeepLabv3~\cite{chen2017rethinking} is employed for experiments with different backbones. The input resolution of the images is set to 512$\times$512 and the ImageNet-1K pretrained weights are loaded as model initialization. All models were trained for 120 and 50 epochs on ADE20K and VOC dataset, respectively. Please see our Supplementary for more detailed introduction.

\vspace{-4mm}
\paragraph{Results.}
As the results shown in Table ~\ref{tab:sota_downstream}, \ours~performs clearly better than other lightweight networks on these two datasets. Our \ours~outperforms the second-best lightweight network MobileViTv2-1.0~\cite{mobilevitv2} by 1.4\% in mIOU on ADE20K, and outperforms the second-best lightweight model EdgeNext~\cite{maaz2023edgenext} by 0.3\% in mIOU on VOC. Besides, it achieves large improvements (2.2\% mIOU on ADE20K, 3.7\% mIOU on VOC) relative to the representative CNN model (\ieno, ResNet-50) with about 10\% of the parameters of ResNet-50. These exhibit the effectiveness of our proposed method on dense prediction tasks.
% \noindent\textbf{Compare with SOTA works. } As shown in Table ~\ref{tab:sota_downstream}, DeepLabv3 with \ours~constantly outperforms DeepLabv3 with other light-weight CNN models and transformer models on both dataset. For example, \ours~ achieves 38.4\% mIOU, and outperforms the second-best MobileViTv2-1.0~\cite{mobilevitv2} by 1.4\% mIOU. \ours~ achieves 80.5\% mIOU, outperforms the second-best EdgeNext~\cite{maaz2023edgenext} by 0.3\% mIOU. The results demonstrate \ours~s superiority and efficiency on dense prediction task.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|c}
    \toprule
    \multicolumn{1}{c|}{Method} & Param (M) & FLOPs (G) & Top-1 \\
    \midrule
    Base. & 5.2  & 1.3   & 77.9 \\
    Base. + Conv-mixer (3$\times$3) & 10.7   & 2.7   & 78.6 \\
    % \rowcolor[gray]{0.9}Base. + \ourop~(Ours)& 5.5   & 1.5   & 79.8 \\
    % \midrule
    Base. + AFF w/o FFT  & 5.5   & 1.5   & 78.4 \\
    \rowcolor[gray]{0.9}Base. + \ourop~ (Our \ours)& 5.5   & 1.5   & 79.8 \\
    \bottomrule
    \end{tabular}%
    } 
    \vspace{+1.6mm}
  \caption{Comparisons of our proposed model with baseline (no spatial token mixer) and models with other token mixers in the original domain on ImageNet-1K classification. ``Base.'' denotes the baseline model discarding all \ourop~token mixers. ``Conv-Mixer (3$\times$3)'' refers to adopting token mixers implemented by 3$\times$3 convolutions in the original space. ``AFF w/o FFT'' denotes performing adaptive filtering in the original space with the same networks by discarding the Fourier transforms where ``w/o'' and ``AFF'' are short for ``without'' and ``\ourop~token mixer'', respectively.}
  \label{tab:aff2baseline}
\end{table}%
% \vspace{-1.6mm}

\subsection{Ablation Study}

\paragraph{Effectiveness and complexity of \ourop~token mixer.}
% \subsubsection{The effectiveness of \ourop~token mixer}
We analyze the effectiveness and complexity of our proposed \ourop~token mixer by comparing \ours~with the \textit{Base.} model in which all \ourop~token mixers are replaced with identity functions. As shown in Table \ref{tab:aff2baseline}, all \ourop~token mixers in \ours~only requires 0.3M parameter increase ($<6\%$) and 0.2G FLOPs increase ($\sim15\%$) relative to the baseline and improves the Top-1 accuracy on ImageNet-1K by 1.9\%. Comparing to the model with one 3$\times$3 convolution layer as the token mixer, \ieno, \textit{Base.+Conv-Mixer (3$\times$3)}, \ours~delivers 1.2\% Top-1 accuracy improvements with about only half of parameters and FLOPs. This strongly demonstrates the effectiveness and efficiency of our proposed method for token mixing in lightweight networks.

% To demonstrate the effectiveness and efficiency of our \ourop~token mixer as a global token mixer, 
% we compare our \ours~model with two baselines that remove all \ourop~token mixer in Table~ \ref{tab:aff2baseline}: ``Base.'' replaces all \ourop~token mixer with an identity function, while ``Base.(Conv3$\times$3)'' replaces all \ourop~token mixer with a normal 3$\times$3 convolution as a token mixer. Our results show that our \ours~outperforms both baselines. 
% Compared to ``Base.'', our \ours~model achieves 1.9\% higher accuracy with only a slight increase of 0.3M parameters and 0.2G FLOPs, demonstrating that our \ourop~token mixer only occupies a small amount of parameters and FLOPs, yet it can achieve global scope to obtain good performance. 
% Compared to ``Base. + Conv-Mixer(3$\times$3)'', our \ourop~outperforms it by 1.2\%, while significantly reducing 5.2M parameters and 1.2G FLOPs. This comparison further highlights the effectiveness and efficiency of our \ourop~token mixer as a global token mixer.

\vspace{-2mm}
\paragraph{Original vs. frequency domain.}
We compare applying the same adaptive filtering operations in original domain and in frequency domain. We discard the all Fourier and inverse Fourier transforms and remain others the same as \ours, \ieno, \textit{Base.+AFF w/o FFT} in Table \ref{tab:aff2baseline}. Our \ours~clearly outperforms it by 1.4\% Top-1 accuracy with the same model complexity. Applying adaptive filtering in the original domain is even weaker than convolutional token mixer, which indicates that only adaptive \textit{\textbf{frequency}} filters can serve as effeicient global token mixers.
% \subsubsection{Frequency-domain v.s. Original-domain filtering} 
% Feature filtering operations can be implemented in either the original feature domain or the frequency domain. Feature filtering operations in frequency domain can serve as a global token mixer, while filtering in original space cannot. We conduct experiments to provide proof in Table \ref{tab:aff2baseline}. Our \ourop~outperforms ``Base. + AOF'' by a clear margin.


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|c}
    \toprule
    \multicolumn{1}{c|}{Method} & Param (M) & FLOPs (G) & Top-1 \\
    \midrule
    % Base. & 5.2  & 1.3   & 77.9 \\
    Base. & 5.2  & 1.3   & 77.9 \\
    Base. + AFNO~\cite{guibas2021adaptive} & 5.5 & 1.5   & 78.8 \\
    Base. + GFN~\cite{rao2021global} & 6.5 & 1.5   & 79.1 \\
    Base. + FFC~\cite{chi2020fast} & 7.7 & 1.7   & 79.1 \\
    Base. + DFF~\cite{lin2022deep} & 7.7 & 1.7   & 79.3 \\
    Base. + FNO~\cite{li2020fourier} & 141.0 & 1.5   & 79.7 \\
    % \hline
    Base. + AFF w. SUM & 5.5 & 1.5  & 78.8 \\
    \rowcolor[gray]{0.9}Base. + \ourop~(\ours) & 5.5 & 1.5   & 79.8 \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{+1mm}
  \caption{Comparisons of our design for \ourop~token mixer and other frequency-domain operations in previous works \cite{li2020fourier, rao2021global, chi2020fast, lin2022deep, guibas2021adaptive} in terms of their roles for token mixing on ImageNet-1K. ``AFF w. SUM'' denotes replacing the Hadamard product with a summation operation, ``w.'' is short for ``with''. }
  \label{tab:freq_operations}
\end{table}%

% \vspace{-4mm}
\vspace{-2mm}
\paragraph{Comparisons of different frequency operations.} 
We compare the frequency operation design in \ourop~token mixer with those in previous works~\cite{li2020fourier,rao2021global,chi2020fast,lin2022deep,guibas2021adaptive} in terms of their effects as token mixers. The results are in Table~\ref{tab:freq_operations}. As analyzed in Sec.\ref{sec:analysis}, FFC~\cite{chi2020fast} and AFNO~\cite{guibas2021adaptive} actually perform filtering with deterministic functions, resulting in the lack of the adaptivity to semantics. The frequency-domain operations in them are both obviously inferior to ours. Moreover, our operation design is also clearly better than those in GFN~\cite{rao2021global} and FNO~\cite{li2020fourier} since they perform filtering with network parameters implemented masks. These masks are fixed after training and lead to a large increase in parameters (\textit{Base.+FNO} has more than 25$\times$ parameters as ours). Note that the implementation of FNO~\cite{li2020fourier} with unshared fully connected layers for each frequency component results in a significant increase in the number of parameters. DFF~\cite{lin2022deep} is designed for filtering out the frequency components adverse to domain generalization, thus requiring a spatial mask only. Our \ours~is superior to \textit{Base.+DFF} by 0.5\% with fewer parameters and FLOPs, demonstrating the importance of channel-wise mixing. This will be further verified with a fairer comparison. These existing frequency-domain operations might be similar with our proposed one at the first glance, but they are designed for different purposes and perform worse than ours as token mixers. When replacing the Hadamard product in our method with a summation operation, the Top-1 accuracy drops by 1.0\% since the equivalence introduced in Sec.\ref{sec:AFF-token-mixer} no longer holds.
% although they are originally designed for different purposes
% We compare our \ourop~with other frequency-domain neural operators~\cite{li2020fourier,rao2021global,chi2020fast,lin2022deep,guibas2021adaptive} as well as AFO in Table~\ref{tab:freq_operations}. Our method outperforms FFC~\cite{chi2020fast} and AFNO~\cite{guibas2021adaptive} as we use a learnable filtering mask instead of rule-based filtering which can achieve semantic-adaptive filtering. In contrast to GFN~\cite{rao2021global} and FNO~\cite{li2020fourier}, our filtering masks are semantic-adaptive and parameter-efficient, achieve higher performance with lower computing cost. Note that the implementation of FNO~\cite{li2020fourier} with unshared fully connected layers for each frequency component results in a significant increase in the number of parameters. Furthermore, our method outperforms DFF~\cite{lin2022deep}, our method is channel-specific and can capture semantic information for better filtering, which DFF~\cite{lin2022deep} lacks.


% \vspace{-4mm}
\vspace{-2mm}
\paragraph{The importance of channel-specific token mixing.}
% \subsubsection{Adaptability of frequency filtering}
We have preliminarily demonstrated this by comparing the frequency-domain operations in DFF \cite{lin2022deep} and ours. Considering their masks are learned with different networks, here, we conduct a fairer comparison by adopting an average pooling along the channels of the learned masks in \ourop~token mixer. As shown in Table \ref{tab:channel-wise}, frequency filtering with the masks of a shape of 1$\times$H$\times$W lead to 0.5\% accuracy drop with the same model complexities, verifying the importance of channel-specific token mixing. This is because different semantic attributes of the learned latent representations distribute in different channels \cite{bau2020understanding,wu2021stylespace}, thus requiring channel-specific weights for token mixing. 
Besides, it delivers the same accuracy with \textit{Base.+DFF} in Table~\ref{tab:freq_operations}. This indicates that the network architectures here are in fact not dominating factors for the effects of token mixing, allowing us to use a lightweight one.
% \vspace{-4mm}
% As we discussed earlier, channel-wise varying weights are crucial for effective token mixing, as the adaptability to semantics is essential and different semantic attributes of learned representations distributed across different channels. We compare the performance of channel-sharing and channel-specific filtering mask in Table \ref{tab:adaptive}. We observe that channel-specific filtering mask (ours) outperforms the channel-sharing filtering mask and the task-level filtering mask by a large margin, which supports our claims about the importance of channel-specific token mixing.


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{l|cc|c}
    \toprule
    \multicolumn{1}{c|}{Mask Shape}  & Param (M) & FLOPs (G) & Top-1 \\
    \midrule
     1$\times$H$\times$W & 5.5   & 1.5   & 79.3 \\
     \rowcolor[gray]{0.9}C$\times$H$\times$W & 5.5   & 1.5   & 79.8 \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{+1mm}
    \caption{Experiments of verifying the importance of channel-specific token mixing on ImageNet-1K. Here, we adopt an average pooling operation along the channel dimension of the masks learned in \ours, yielding the mask with a shape of 1$\times$H$\times$W. This mask is shared across channels.}
  % \caption{Ablation study on the Channel Adaptability of the frequency filtering operation.  ``1$\times$H$\times$W" refers to the output shape of the filtering mask, pooling from C$\times$H$\times$W and then multiplied on the frequency features with channel-wise sharing. ``C$\times$H$\times$W'' is our proposed \ourop, where the filtering mask is multiplied on the frequency features with channel adaptivity.}
  \label{tab:channel-wise}%
\end{table}%
% \vspace{-4mm}

\vspace{-2mm}
\paragraph{Comparisons of hyper-parameter choices.}
% \subsubsection{Operation to capture the semantic for instance-adaptive masks}
As shown in Fig.\ref{fig:framework}, we adopt two group linear layers (also known as 1$\times$1 convolution layers) with ReLU to learn the masks for our proposed adaptive frequency filtering. As shown in Table \ref{tab:mask_gen}, improving the kernel size cannot further improve the performance but leads to larger model complexities. Moreover, we keep the spatial kernel size as 1$\times$1 while using different group numbers. When $N_{group}$=$C$, the Top-1 accuracy drops by 0.4\%, in which depthwise convolutions are used so that the contexts among different channels are under-exploited for inferring the weights of token mixing. When $N_{group}$=1, it means that regular convolution/linear layers are used, which slightly improve the Top-1 accuracy by 0.1\% at the expense of 40\% parameters increase and 33.3\% FLOPs increase. This setting explores more contexts but results in a worse accuracy and efficiency trade-off.
% We use a lightweight network in our \ourop~token mixer to learn semantic-adaptive masks from frequency representations. The group linear layers in this network can capture semantic information across channels while maintaining low computational cost. In Tab~\ref{tab:mask_gen}, Our \ours~outperforms "Linear($N_{group}=1$)" by 0.4\% top-1 accuracy, highlighting the ability of our design to capture semantic information for generating semantic-adaptive masks. Moreover, our design achieves similar performance to "Linear($N_{group}=1)$" while saving 2.2M parameters and 0.5G FLOPs which demonstratesits efficiency. In addition, our \ours~achieve same performance with ``Conv 3$\times$3'' indicating that frequency component mixing is unnecessary.


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|c|cc|c}
    \toprule
    \multicolumn{1}{c|}{Spatial K-Size} & $N_{group}$ & Param. (M) & FLOPs (G) & Top-1 \\
    \midrule
    $1 \times 1$ & $C$        & 5.3   & 1.4   & 79.4 \\
    $1 \times 1$ & 1        & 7.7   & 2.0   & 79.9 \\
    % \rowcolor[gray]{0.9}Linear & 8        & 5.5   & 1.5   & 79.8 \\
    % \midrule
    $3 \times 3$ & 8  &  7.9     &  2.0     & 79.8 \\
    \rowcolor[gray]{0.9}$1 \times 1$ & 8        & 5.5   & 1.5   & 79.8 \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{+1mm}
  \caption{Comparisons of different hyper-parameter choices in the sub-network for learning the filtering masks in \ours~on ImageNet-1K. ``Spatial K-Size'' refers to the spatial size of convolution kernels. $N_{group}$ denotes the number of groups for group linear or convolution layers. $C$ is the total number of channels.}
  % \caption{Comparison of different operations in \ours~for capturing semantic information used in filtering. $N_{group}$ denotes the number of groups for linear or convolution layer.}
  \label{tab:mask_gen}%
\end{table}%

% For example, we used the notation ``Group Linear (g=C)'' to denote a group linear operation where the number of groups equals the number of input channels, while ``Group Conv 3$\times$3 (g=8)'' referred to a group convolution operation with 8 groups.

\section{Conclusion}
In this work, we reveal that \textit{adaptive frequency filters can serve as efficient global token mixers} in a mathematically equivalent manner. Upon this, we propose Adaptive Frequency Filtering (\ourop) token mixer to achieve low-cost adaptive token mixing in the global scope. Moreover, we take \ourop~token mixers as primary neural operators to build a lightweight backbone network, dubbed \ours. \ours~achieves SOTA accuracy and efficiency trade-offs compared to other lightweight network designs across multiple vision tasks. Besides, we revisit the existing frequency-domain neural operations for figuring out what matters in their designs for token mixing. We hope this work could inspire more interplay between conventional signal processing and deep learning technologies.
% In this paper, we have introduced Adaptive Frequency Filtering (\ourop), which can serve as an efficient global token mixing method with large dynamic kernels.  We take \ourop~token mixer as the primary neural operator to build a lightweight vision backbone \ours, which achieves state-of-the-art accuracy and efficiency trade-offs across a broad range of vision tasks compared to other lightweight network designs. Our results demonstrate the potential of combining conventional signal processing and deep learning technologies to achieve superior performance. We encourage more research into exploring the interplay between these fields and further optimization of conventional signal processing operations within various neural network inference frameworks.

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[htbp]
%   \centering
%     \resizebox{0.8\linewidth}{!}{
%     \begin{tabular}{l|cc|c|c}
%     \toprule
%     \multicolumn{1}{c|}{Model} & Param (M) & FLOPs (G) & \makecell[c]{Throughput  \\ (images/s)} & Top1 \\
%     % \midrule
%     %  MViT-XXS & 1.3 &  0.4    & 6503  & 69.0 \\
%     % MViTv2-0.5 & 1.4 & 0.5    & 6800  & 70.2 \\
%     % % MFormer-52 & 3.6     & 6673  & 68.7 \\
%     % \rowcolor[gray]{0.9}Ours-XXS & 1.4  & 0.4     & 7470  & 73.0 \\
%     % \midrule
%     %  MViT-XS & 2.3    & 1.0   & 4903  & 74.8 \\
%     %  MViTv2-0.75 & 2.6  & 1.0   & 5043  & 75.6 \\
%     % \rowcolor[gray]{0.9}Ours-XS & 2.6   & 0.8     & 5304  & 77.0 \\
%     % \midrule
%      % DeiT-Tiny & 5.5 & 1.3 & 
%      % MFormer-294  & 11.8  & 0.3  & 2790  & 77.9 \\
%      MViT-S & 5.6  & 2.0      & 3703  & 78.4 \\
%      MViTv2-1.0 & 5.1  & 1.8   & 4023  & 78.1 \\
%      EdgeNext & 
%      \rowcolor[gray]{0.9}Ours-X & 5.5  & 1.5       & 4202  & 79.8 \\
%     % MFormer-294  & 11.8  & 2790  & 77.9 \\
%     \bottom
%     \end{tabular}%
%     }
%   \caption{Add caption}
%   \label{tab:addlabel}%
% \end{table}%




{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

%%%%%%%%% Supplementary
\clearpage
\section*{\Large{\textbf{Supplementary Material}}}

\section{Detailed Network Architectures}

As introduced in our manuscript, we build three versions of our proposed hierarchical backbone \ours~with different channel dimensions, namely \ours, \ours-T and \ours-ET, respectively. Here, we provide the detailed model configurations of them in Table~\ref{tab:model_detail}. Specifically, following commonly used designs~\cite{mehta2021mobilevit, mobilevitv2}, we adopt a convolution stem for tokenization, which consists of a 3$\times$3 convolution layer with a stride of 2, followed by four MBConv layers. MBConv is short for the Mobile Convolution Block in ~\cite{sandler2018mobilenetv2} with a kernel size of 3. After tokenization, three stages are cascaded as the main body of \ours, where each stage is composed of a MBConv layer with stride 2 for down-sampling in spatial and $N_i$ \ourblock. Specifically, we set $N_1=2$, $N_2=4$ and $N_3=3$.

% \tcb{As illustrated in Table~\ref{tab:model_detail}, we propose three hierarchical backbones variant with different channel dimensions, namely \ours, \ours-T and \ours-ET. Following previous works~\cite{mehta2021mobilevit, mobilevitv2}, we adopt a convolution stem to capture local structures, such as lines and edges among the neighboring pixels. the convolution stem consists of a 3$\times$3 convolution layer with a downsampling stride of 2, followed by four MBConv~\cite{sandler2018mobilenetv2} layers. MBConv refers to Mobile Convolution Block with kernel size 3. After the convolution stem, there are three stages with different feature resolutions. Each stage is composed of a MBConv layer with stride 2 as the downsampling layer and $N_i$ cascaded \ourblock. Specifically, $N_1 = 2$, $N_2=4$ and $N_3=3$.}

% \tcb{Following previous works~\cite{huang2017densely,mehta2021mobilevit}, at the end of each stage, we incorporate a fusion structure that resembles a skip connection. This structure concatenates the feature maps from the start and end of the stage along the channel dimension and applies a 1$\times$1 convolution (linear layer) to it. In \ourblock, inspired by previous works~\cite{wang2021pyramid,wang2022pvt,chen2021regionvit,tu2022maxvit,yang2022moat}, we directly use MBConv as our channer mixer, which involves low-cost Depth-Wise (DW) 3$\times$3 convolution layers into regular channel mixer, Feed-Forward Networks (FFN). Additionally, following previous works~\cite{tu2022maxvit,yang2022moat}, we swap the position of channel mixers and token mixers compared to regular transformer blocks. Notably, these modifications incur only minor increases in parameters and FLOPs. Further ablation studies are presented in Table~\ref{tab:fuse_block}, \ref{tab:aff_block}.}


% This fusion structure plays a similar role as the token and channel mixer swapping mentioned above, and can smooth the model transition from local to global operations, especially when the model depth is not too deep. 

% Following previous works, the MLP in our architecture consists of a Linear layer, followed by a depth-wise 3X3 Convolution, and finally another Linear layer. Compare to traditional transformer block, we swap the token Mixer and the Channel Mixer.

% Moving the Channel Mixer, a local operation, to the front of the Token Mixer, a global operation, can make the overall model change more smoothly from local operations at the head to global operations at the tail. This smooth transformation is achieved by increasing the depth of the model in general transformers, but increasing the depth of the model will slow down the model speed. Our design can achieve this effect while ensuring model efficiency.

% This structure concatenates the input feature at the beginning of the stage with the output feature of the stage along the channel dimension, and then passes through a linearal layer before outputting the final feature. This fusion structure plays a similar role as the token and channel mixer swapping mentioned above, and can smooth the model transition from local to global operations, especially when the model depth is not too deep.


\begin{table*}[t]
  \centering
    \begin{tabular}{l|c|c|c|ccc}
    \toprule
    \multicolumn{1}{c|}{\multirow{2}[0]{*}{Layer / Block}} & \multirow{2}[0]{*}{Resolution} & \multirow{2}[0]{*}{Down-sample Ratio} & \multirow{2}[0]{*}{Number of Blocks} & \multicolumn{3}{c}{Number of Channels} \\
          &       &       &       & \ours-ET   & \ours-T    & \ours \\
    \midrule
    Image & $256^2$ &   -    & 1     & 16    & 16    & 16 \\
    \midrule
    \multirow{2}[0]{*}{Conv Stem} & $128^2$ & $\downarrow$ 2     & 1     & 32    & 32    & 32 \\
     & $64^2$ & $\downarrow$ 2     & 4     & 48    & 48    & 64 \\
    \midrule
    Down-sampling & $32^2$ & $\downarrow$ 2     & 1     & 64    & 96    & 128 \\
    \ourblock & $32^2$ & -     & 2     & 64    & 96    & 128 \\
    \midrule
    Down-sampling & $16^2$ & $\downarrow$ 2     & 1     & 104   & 160   & 256 \\
    \ourblock & $16^2$ & -     & 4     & 104   & 160   & 256 \\
    \midrule
    Down-sampling & $8^2$   & $\downarrow$ 2     & 1     & 144   & 192   & 320 \\
    \ourblock & $8^2$   & -     & 3     & 144   & 192   & 320 \\
    % \midrule
    % Global Avg Pool &   -    &   -    &   -    & 144   & 192   & 320 \\
    % Linear &   -    &   -    &   -    & 1000  & 1000  & 1000 \\
    \midrule
    Parameters &   -    &   -    &  -     & 1.4M   & 2.6M   & 5.5M \\
    FLOPs &  -     &   -    &   -    & 0.4G   & 0.8G   & 1.5G \\
    \bottomrule
    \end{tabular}%
  \vspace{+1.6mm}
  \caption{Detailed model configurations. The resolution and the number of channels in above table correspond to the output representations for each layer/block.}
  % \caption{\tcb{\ours~architecture. Description of the model's layers with respect to output size, downsampling, block number, output channels and model FLOPs and parameters. The number of the output channels for \ours, \ours-T, and \ours-ET models are chosen to match the number of parameters with the counterpart MobileViT~\cite{mehta2021mobilevit} model. MBConv refers to Mobile Convolution Block~\cite{sandler2018mobilenetv2} with kernel size 3. ``Param.'' refers to number of parameters.}}
  \label{tab:model_detail}%
\end{table*}%

% \section{More Details of Experiment Settings}
\section{Detailed Introduction for Dataset}

\textbf{ImageNet~\cite{russakovsky2015imagenet}} is a large-scale dataset with over 1.2 million images and 1000 object categories for the visual recognition challenge. It serves as the most widely used dataset for image classification. The images in this dataset are of varying sizes and resolutions, and include various objects in diverse backgrounds. We train our models on Imagenet-1k dataset from scratch to illustrate the effectiveness and efficiency of our proposed models on image classification.

% \paragraph{ImageNet.}
% \tcb{The ImageNet dataset~\cite{russakovsky2015imagenet} is a large-scale visual recognition challenge dataset with over 1.2 million images and 1000 object categories. It is widely used in computer vision research and has been instrumental in advancing the state-of-the-art in image classification. The images in this dataset are of varying sizes and resolutions, and depict objects in diverse settings and contexts. We train our models on Imagenet-1k dataset from scratch to illustrate our models' effectiv eness and efficiency on image classificaiton task.}

\textbf{MS-COCO~\cite{lin2014microsoft}} (abbreviated as COCO) is a widely used benchmark dataset for object detection, instance segmentation, and keypoint detection tasks. It contains more than 200,000 images and 80 object categories, annotated with bounding boxes, masks, and keypoints. The objects in this dataset are diverse and challenging, including people, animals, vehicles, household items, \etcno.

% \paragraph{MS-COCO.}
% \tcb{The MS-COCO (abbreviated as COCO) dataset~\cite{lin2014microsoft} is a widely-used benchmark for object detection, instance segmentation, and keypoint detection tasks. It contains over 330,000 images with more than 2.5 million object instances, annotated with bounding boxes, masks, and keypoints. The objects in this dataset are diverse and challenging, including people, animals, vehicles, and household items.}

\textbf{ADE20k~\cite{zhou2019semantic}} is a dataset consisting of 20,210 images covering a wide range of indoor and outdoor scenes. The images in this dataset are annotated with pixel-level labels for 150 semantic categories, such as sky, road, person and so on. This dataset is widely used for evaluating the performance of deep models on semantic segmentation and scene understanding.

% \paragraph{ADE20k.}
% \tcb{The ADE20k dataset~\cite{zhou2019semantic} is a scene parsing dataset consisting of 20,210 images covering a wide range of indoor and outdoor scenes. The images are annotated with pixel-level labels for 150 semantic categories, such as sky, road, and person. This dataset is often used for evaluating models for semantic segmentation and scene understanding.}

\textbf{PASCAL VOC 2012~\cite{everingham2015pascal}} (abbreviated as VOC) is a widely used benchmark for object recognition, object detection, and semantic segmentation. It consists of 20 object categories and contains more than 11,000 images with pixel-level annotations for object boundaries and semantic categories. This dataset is challenging due to the large variability in object appearances and the presence of occlusions and clutter within it. 
% It has been used as a standard benchmark for evaluating deep learning models for object recognition and segmentation.

% \paragraph{VOC.}
% \tcb{The PASCAL VOC 2012~\cite{everingham2015pascal} (abbreviated as VOC) is a widely-used benchmark for object recognition, object detection, and semantic segmentation. It consists of 20 object categories and contains over 11,000 images with pixel-level annotations for object boundaries and semantic categories. This dataset is challenging due to the large variability in object appearances and the presence of occlusions and clutter. It has been used as a standard benchmark for evaluating deep learning models for object recognition and segmentation.}

\section{Detailed Experiment Settings}

We provide detailed experiment settings for different tasks in Table~\ref{tab:train_config}, including the detailed configurations for model, data and training.
% \subsection{Experiment Setting for Image Classification}

% % TODO: , mobilevit  edgenext batcher sampler  multi-scale batch sampler.

% We detail the experiment setting for image classifcation task in Table~\ref{tab:train_config}. for image classification, We train our models from scratch on the ImageNet-1k dataset~\cite{russakovsky2015imagenet}. We use kaiming normalization to initialize the model weights. And we employed an exponential moving average of model weights for inference. The resolution of input images is $256 \times 256$. For data augmentation strategies, we follow previous works~\cite{mehta2021mobilevit,mobilevitv2,maaz2023edgenext} use only Random resized crop and Random horizontal flip for \ours-ET and \ours-T and use RandAugment, CutMix, MixUp, Random resized crop, Random horizontal flip and Random erase for \ours. For training recipes, the base learning rates for \ours, \ours-T and \ours-ET are 0.002, 0,0049 and 0.009, respectively and the weight decays for \ours, \ours-T and \ours-ET are 0.05, 0.02 and 0.008, respectively. The loss function used is cross-entropy with label smoothing. The optimizer used is AdamW. The learning rate is linearly increased from 1.00E-06 to base learning rate for the first 20k iterations and then decayed using a cosine annealing scheduler till the end of training. The model is trained for 300 epochs on 8 NVIDIA V100 GPUs with a batch size of 1024.

% \tcb{As illustrated on Table~\ref{tab:train_config}, for image classification, We train our models from scratch on the ImageNet-1k dataset~\cite{russakovsky2015imagenet}. We use kaiming normalization to initialize the model weights. And we employed an exponential moving average of model weights for inference. The resolution of input images is $256 \times 256$. For data augmentation strategies, we follow previous works~\cite{mehta2021mobilevit,mobilevitv2,maaz2023edgenext} use only Random resized crop and Random horizontal flip for \ours-ET and \ours-T and use RandAugment, CutMix, MixUp, Random resized crop, Random horizontal flip and Random erase for \ours. For training recipes, the base learning rates for \ours, \ours-T and \ours-ET are 0.002, 0,0049 and 0.009, respectively and the weight decays for \ours, \ours-T and \ours-ET are 0.05, 0.02 and 0.008, respectively. The loss function used is cross-entropy with label smoothing. The optimizer used is AdamW. The learning rate is linearly increased from 1.00E-06 to base learning rate for the first 20k iterations and then decayed using a cosine annealing scheduler till the end of training. The model is trained for 300 epochs on 8 NVIDIA V100 GPUs with a batch size of 1024.}

% \subsection{Experimental setting for object detection}

% % TODO: ssd 
% % TODO: augmentation
% \tcb{As shown in Table~\ref{tab:train_config}, we performed object detection experiments on the MS-COCO dataset~\cite{lin2014microsoft} using the Single Shot Detection (SSD)~\cite{liu2016ssd} framework with separable convolutions replacing the standard convolutions in the detection head, following common practices in the literature~\cite{howard2017mobilenets,sandler2018mobilenetv2,mehta2021mobilevit,mobilevitv2, maaz2023edgenext}. In our training process, we utilized pre-trained weights on the ImageNet-1K dataset to initialize the backbone network, and fine-tuned the entire model using the AdamW optimizer for 200 epochs on the COCO training set.  We adopted the ssd multibox loss as the training loss, which consists of the confidence loss and the location loss. The confidence loss measures the network's confidence of the computed bounding box with the categorical cross-entropy loss, and the location loss that evaluates L2-Norm distance between the network's predicted bounding with the ground truth ones. The input resolution of the images was set to 320$\times$320, and we used a batch size of 128. The learning rate was set to 0.0007, and the weight decay was set to 0.05.}


% \subsection{Experimental setting for semantic segmentation}
% \tcb{As shown in Table~\ref{tab:train_config}, we conduct semantic segmentation experiments on two benchmarks datasets ADE20k~\cite{zhou2019semantic} and PASCAL VOC 2012~\cite{everingham2015pascal} (abbreviated as VOC). For the experiments on VOC dataset, we follow the common practices in \cite{chen2017rethinking,mehta2019espnetv2} to extend the training data with more annotations and data from~\cite{hariharan2011semantic} and \cite{lin2014microsoft}, respectively. The DeepLabv3~\cite{chen2017rethinking} framework is used for semantic segmentation experiments with different backbones, where aspp out channels was set to 256 and aspp rates were 6, 12, and 18. The input resolution of the images was set to 512$\times$512, and the ImageNet-1K pretrained weights were loaded as model initialization. All models were trained for 120 epochs and 50 epochs on the ADE20K and VOC datasets. In the training process, Random resized crop, Random horizontal flip, and Gaussian Noise were used as data augmentation strategies. We employed AdamW as the optimizer and segmentation cross-entropy loss as the loss function. The learning rate and weight decay were set to be 0.0005 and 0.05, respectively. The batch size was 16 and 128 for ADE20k and VOC, respectively.}


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table*}[t]
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|ccc|c|cc}
    \toprule
    \multicolumn{1}{c}{Task}  & \multicolumn{3}{|c}{Image Classification} & \multicolumn{1}{|c}{Object Detection} & \multicolumn{2}{|c}{Semantic Segmentation} \\
    \midrule
    Model & \ours-ET & \ours-T & \ours   & \ours   & \ours   & \ours \\
        \midrule
    EMA & \ding{51}     & \ding{51}     & \ding{51}     & \ding{51}     & \ding{51}     & \ding{51} \\
    % \midrule
    Weight Initialization & \makecell[c]{Kaiming \\ normal} & \makecell[c]{Kaiming \\ normal} & \makecell[c]{Kaiming \\ normal} & \makecell[c]{ImageNet-1k\\ pretrain} & \makecell[c]{ImageNet-1k\\ pretrain} & \makecell[c]{ImageNet-1k\\ pretrain} \\
    \midrule
    Dataset & ImageNet-1k & ImageNet-1k & ImageNet-1k & COCO  & ADE20k & PASCAL VOC \\
    % Sampler & batch\_sampler & batch\_sampler & batch\_sampler & batch\_sampler & batch\_sampler & batch\_sampler \\
    Resolution & $256^2$ & $256^2$ & $256^2$ & $320^2$ & $512^2$ & $512^2$ \\
    % Val Resolution & $256^2$ & $256^2$ & $256^2$ & $320^2$ & $512^2$ & $512^2$ \\
    \midrule
    RandAug & \ding{55}     & \ding{55}     & \ding{51}     & \ding{55}     & \ding{55}     & \ding{55} \\
    CutMix & \ding{55}     & \ding{55}     & \ding{51}     & \ding{55}     & \ding{55}     & \ding{55} \\
    MixUp & \ding{55}     & \ding{55}     & \ding{51}     & \ding{55}     & \ding{55}     & \ding{55} \\
    Random Resized Crop & \ding{51}     & \ding{51}     & \ding{51}     & \ding{55}     & \ding{51}     & \ding{51} \\
    Random Horizontal Flip & \ding{51}     & \ding{51}     & \ding{51}     & \ding{55}     & \ding{51}     & \ding{51} \\
    Random Erase & \ding{55}     & \ding{55}     & \ding{51}     & \ding{55}     & \ding{55}     & \ding{55} \\
    Gaussian Noise & \ding{55}     & \ding{55}     & \ding{55}     & \ding{55}     & \ding{51}     & \ding{51} \\
    \midrule
    % Stochastic depth & \ding{55}     & \ding{55}     & \ding{55}     & \ding{55}     & \ding{55}     & \ding{55} \\
    Label Smoothing & \ding{51}     & \ding{51}     & \ding{51}     & \ding{55}     & \ding{55}     & \ding{55} \\
    Loss  & CE    & CE    & CE    & Ssd Multibox    & CE    & CE \\
    Optimizer & AdamW & AdamW & AdamW & AdamW & AdamW & AdamW \\
    Weight Decay & 0.008 & 0.02  & 0.05  & 0.05  & 0.05  & 0.05 \\
    % no\_decay\_bn\_filter\_bias & \ding{51}     & \ding{55}     & \ding{51}     & \ding{51}     & \ding{51}     & \ding{51} \\
    Warm-up Iterations & 20 k  & 20 K  & 20 k  & 500   & 500   & 500 \\
    % Warm-up init LR & 1.00E-06 & 1.00E-06 & 1.00E-06 & 1.00E-06 & 5.00E-05 & 5.00E-05 \\
    % Warm-up scheduler & Linear & Linear & Linear & Linear & Linear & Linear \\
    LR Scheduler & Cosine & Cosine & Cosine & Cosine & Cosine & Cosine \\
    Base LR & 0.009 & 0.0049 & 0.002 & 0.0007 & 0.0005 & 0.0005 \\
    Minimal LR & 0.0009 & 0.00049 & 0.0002 & 0.00007 & 1.00E-06 & 1.00E-06 \\
    Number of Epochs & 300   & 300   & 300   & 200   & 120   & 50 \\
    Batch Size & 1024  & 1024  & 1024  & 128   & 16    & 128 \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{+0.6mm}
    \caption{Detailed training configurations of \ours, \ours-T, and \ours-ET models on different tasks. ``LR'' denotes the learning rate and ``EMA'' is short for Exponential Moving Average. For object detection and semantic segmentation tasks, \ours-T and \ours-ET use the same training configuration as \ours.}
  % \caption{\tcb{Detailed training configurations of \ours, \ours-T, and \ours-ET models for various tasks and datasets, including parameter initialization, input dataset and resolution, data augmentation, and optimization recipe. ``LR'' denotes the learning rate, ``EMA'' means Exponential Moving Average. For detection and segmentation tasks, \ours-T and \ours-ET use the same training configuration as \ours.}}
  \label{tab:train_config}%
\end{table*}%

\section{More Experiment Results}
% \section{More ablation studies}

\subsection{Quantitative Results}

\paragraph{Running speed evaluation.}
% \section{Comparison of the model speed}
We report the model speeds of our proposed \ours~models on mobile devices (iPhone) and GPUs, and compare them with other advanced lightweight models that incorporate global token mixers in Table~\ref{tab:lantency}. Models with similar Top-1 accuracy are grouped together for clear comparison. The latency results are equivalently measured by CoreML\protect\footnote{\url{https://github.com/apple/coremltools}} on an iPhone with a batch size of 1. The throughput results are measured with TorchScript\protect\footnote{\url{https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md}} on an A100 GPU (batch size = 128). As shown in Table~\ref{tab:lantency}, thanks to the \ourop~token mixer, \ours~outperforms other network designs by a clear margin across different model scales. On GPUs (NVIDIA A100), \ours~achieves 0.4\% Top-1 accuracy improvement with 179 image/s lager throughput compared to the second fastest model EdgeNext-S. On the mobile device (iPhone), \ours~also surpasses the second fastest model mobilevitv2 by 1.7\% Top-1 accuracy with 0.3 ms less latency. These results reflect high effectiveness and efficiency of our proposed method.
% \tcb{In Table~\ref{tab:lantency}, we present a comparison of the speed performance of \ours~and several advanced lightweight models that incorporate global token mixers. Models with similar Top-1 accuracy are grouped together. We measured the latency using CoreML\protect\footnote{\url{https://github.com/apple/coremltools}} on an iPhone (batch size = 1), and the throughput using TorchScript\protect\footnote{\url{https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/OVERVIEW.md}} on an A100 GPU (batch size = 128). As shown in Table~\ref{tab:lantency}, thanks to the \ourop~token mixer, \ours~outperforms them by a clear margin across different model scales. For instance, compared to the second fastest model on A100, EdgeNext-S, \ours~achieves a 0.4\% Top-1 accuracy improvement with 179 image/s more throughput on A100 GPU. On iPhone, \ours~also surpasses the second fastest model, mobilevitv2, by 1.7\% Top-1 accuracy while reducing the latency by 0.3 ms. These results highlight the effectiveness and efficiency of our proposed method.}


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|cc|c}
    \toprule
     \multicolumn{1}{c|}{Model} & \makecell[c]{Param.  \\ (M)} & \makecell[c]{FLOPs  \\ (G)} & \makecell[c]{Latency  \\ (ms)} & \makecell[c]{Throughput  \\ (images/s)} & Top-1 \\
    \midrule
    % MFormer-52 & 3.6 & 0.1   & 7.1   & 6673  & 68.7 \\
    MViT-XXS~\cite{mehta2021mobilevit} & 1.3 & 0.4   & 4.8   & 6803  & 69.0 \\
    MViTv2-0.5~\cite{mobilevitv2} & 1.4 & 0.5   & 1.6   & 7021  & 70.2 \\
    EdgeNext-XXS~\cite{maaz2023edgenext} & 1.3 & 0.3 & 1.7 & 7768 & 71.2 \\
    \rowcolor[gray]{0.9}\ours-ET & 1.4 & 0.4  &  1.4     & 8196  & 73.0 \\
    \midrule
    MViT-XS~\cite{mehta2021mobilevit} & 2.3 & 1.0 &  7.0   & 4966  & 74.8 \\
    MViTv2-0.75~\cite{mobilevitv2} & 2.9  & 1.0 &   2.4    & 5150  & 75.6 \\
    EdgeNext-XS~\cite{maaz2023edgenext} & 2.3 & 0.5 & 2.6 & 5307 & 75.0 \\
    \rowcolor[gray]{0.9}\ours-T & 2.6 & 0.8   &  2.1     & 5412  & 77.0 \\
    \midrule
    CycleMLP-B1~\cite{chen2021cyclemlp} & 15.2 & 2.1 & 15.2 & 3073 & 79.1 \\
    PoolFormer-S12~\cite{yu2022metaformer} & 11.9 & 1.8 & 5.3 & 3922 & 77.2 \\
    MFormer-294~\cite{chen2022mobile}  & 11.8 & 0.3  & 40.7  & 2790  & 77.9 \\
    MViT-S~\cite{mehta2021mobilevit} & 5.6 & 2.0  & 9.9   & 3703  & 78.4 \\
    MViTv2-1.0~\cite{mobilevitv2} & 4.9 & 1.8   & 3.4   & 3973  & 78.1 \\
    EdgeNext-S~\cite{maaz2023edgenext}  & 5.6 & 1.3 & 6.4 & 4023 & 79.4 \\
    \rowcolor[gray]{0.9}\ours & 5.5 & 1.5   &  3.1    & 4202  & 79.8 \\
    \bottomrule
    \end{tabular}}%
    \vspace{+1.6mm}
  \caption{Results of model speed evaluation. Here, the latency results are equivalently measured using CoreML on an iPhone with a batch size of 1. The throughput results are measured using TorchScript on an A100 GPU with a batch size of 128.}
  % \caption{\tcb{Comparison of model speed. Models with similar Top-1 accuracy are grouped together, with latency measured using CoreML on an iPhone (batch size  = 1) and throughput measured using TorchScript on an A100 GPU (batchsize = 128).}}
  \label{tab:lantency}%
\end{table}%

\paragraph{Evaluation on more downstream task frameworks.}
For the experiments reported in our main paper (\egno, Table~\ref{tab:sota_downstream}), we adopt the most commonly used task frameworks, \ieno, SSD and Deeplabv3, in accordance with recent studies~\cite{sandler2018mobilenetv2,mehta2021mobilevit,mobilevitv2,maaz2023edgenext} on general-purpose lightweight backbone design to ensure a fair comparison. Moreover, to evaluate the compatibility of \ours~with more downstream task frameworks, we incorporated AFFNet into more downstream task frameworks ~\cite{ge2021yolox,tan2020efficientdet,guo2022visual,yang2022moat} as their encoders. These frameworks involve multi-stage/scale feature interactions via some task-specific architecture designs. By utilizing AFFNet as the encoders, these models perform consistently better compared to their vanilla versions in mAP@COCO and mIOU@ADE20K, as presented in Table~\ref{tab:downstream2}. There results further demonstrate that our proposed \ours~is compatible with diverse downstream task frameworks and generally applicable.
% \tcb{In Table~\ref{tab:sota_downstream}, we conduct experiments upon plain task frameworks, \ieno, SSD and Deeplabv3, in accordance with recent studies~\cite{sandler2018mobilenetv2,mehta2021mobilevit,mobilevitv2,maaz2023edgenext} that focus on general-purpose lightweight backbone design to ensure a fair comparison. Moreover, to demonstrate the versatility and effectiveness of \ours~ in various advanced downstream task frameworks, we incorporated AFFNet as the backbone into the frameworks of task-specific state-of-the-art works~\cite{ge2021yolox,tan2020efficientdet,guo2022visual,yang2022moat}. These works employ more advanced task-tailored structures compared to SSD and Deeplabv3, which involve multi-stage/scale feature interactions. By utilizing AFFNet as the encoder, we observed significant improvements in performance metrics (mAP@COCO, mIOU@ADE20K), as presented in Table~\ref{tab:downstream2}. These findings underscore the generic effectiveness of \ours~ when integrated with various task-specific frameworks.}


\begin{table}[!h]
\small
\centering

\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cc}
\toprule
Task    & \multicolumn{2}{c|}{Detection(mAP)} & \multicolumn{2}{c}{Segmentation(mIOU)}  \\
Framework From &  yolox~\cite{ge2021yolox} & efficientdet~\cite{tan2020efficientdet}  & van~\cite{guo2022visual} & moat~\cite{yang2022moat} \\ 
% framework & yolovx [a] & Efficientdet [d] & Sem FPN [b] & Deeplabv3+ [c]\\ \hline
% backbone & \makecell[c]{Origin\\ /AFFNet} & \makecell[c]{Origin\\ /AFFNet} & \makecell[c]{Origin\\ /AFFNet} & \makecell[c]{Origin\\ /AFFNet}  \\ \hline
\midrule
w. Origin Encoder  & 32.8  & 40.2  & 38.5 & 41.2  \\
\rowcolor[gray]{0.9} w. \ours~Encoder & 35.9  & 41.6  & 43.2 & 41.5  \\
\bottomrule
\end{tabular}
}
\vspace{+0.8mm}
\caption{Performance evaluation on more downstream task frameworks. Our proposed \ours~are integrated into them as their encoders to compare with their original ones.}

\label{tab:downstream2}
\end{table}

\paragraph{Comparisons of different frequency transforms.} 
% \paragraph{Comparison of different frequency transformations} 
We investigate the effectiveness of adopting different frequency transforms in implementing our proposed \ourop~token mixer. Specifically, we compare using FFT and using wavelet transform or Discrete Cosine Transform (DCT). The comparison results are in Table~\ref{tab:fourier_process}. We observe that adopting the wavelet transform also attains improvements compared to the baseline model without any frequency transforms, but it is clearly inferior to adopting FFT as we recommend. This is because the wavelet transform is a low-frequency transformation that performs our proposed filtering operation in a local space, which limits the benefits of our \ourop~token mixer as a global token mixer. Moreover, DCT is slightly inferior to FFT since that DCT is a Fourier-related transform with coarser transform basis. It thus leads to more information loss when mixing tokens. Besides, DCT only performs transformation only on real numbers.
% \tcb{We conducted an investigation to explore the effectiveness of using alternative frequency transforms in implementing our proposed \ourop~token mixer. Specifically, we replaced the Fast Fourier Transform (FFT) in our proposed scheme with the wavelet transform using commonly used Haar filter and DCT (Discrete Cosine transform). The experiment results presented in Table~\ref{tab:fourier_process} indicate that adopting the wavelet transform also provides improvements compared to the baseline "w/o FFT", but it is inferior to adopting FFT. This is because the wavelet transform is a low-frequency transformation that performs our proposed filtering operation in a local space, which limits the benefits of our \ourop~token mixer as a global token mixer. Furthermore, we also investigate the use of the Discrete Cosine Transform (DCT) as an alternative to FFT in our \ourop~token mixer. Our experimental results in Table~\ref{tab:fourier_process} show that DCT is slightly inferior to FFT in terms of performance. DCT is a Fourier-related transform that operates only on real numbers, and thus can also be integrated as a global token mixer using convolution theory. However, due to its coarser granularity of transformation basis, DCT leads to more information loss, resulting in slightly lower performance compared to FFT.} 


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[t]
  \centering
  \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{l|cc|c}
    \toprule
    \multicolumn{1}{c|}{\makecell[c]{Frequency  \\ Transformations}} & Param (M) & FLOPs (G) & Top-1 \\
    \midrule
    Baseline &   5.5 & 1.5    &  78.4 \\
    Wavelet &   5.5 & 1.5    &  78.6 \\
    DCT     &   5.5 & 1.5    &  79.6 \\
    % FFT   &       &  \\
    % \midrule
    \rowcolor[gray]{0.9}FFT (Ours)  &  5.5 & 1.5    &  79.8 \\
    \bottomrule
    \end{tabular}%
    }
  \vspace{+2.3mm}
  \caption{Comparisons of adopting different frequency transforms in implementating our proposed method. ``Baseline'' denotes the model without any frequency transforms, ``Wavelet'' denotes the wavelet transforms with the Haar filters, and ``DCT'' is short for Discrete Cosine transform.}
  % \caption{\tcb{Performance comparisons of different frequency transformations. ``w/o fft'' is the baseline, ``Wavelet'' denotes the wavelet transforms with the Haar filters, ``DCT'' denotes the Discrete Cosine transform.}}
  \label{tab:fourier_process}%
\end{table}%


% TODO: ? 
% \paragraph{Design choices of performing FFT} 
% In our proposed scheme, we perform FFT across the height and width dimensions of the given intermediate feature $\mathbf{X} \in \mathcal{R}^{C\times H\times W}$. Table~\ref{tab:fft_dimension} shows that performing FFT across all dimensions (H, W, and C) results in a performance drop compared to our approach. This is because performing filtering on frequcency space with FFT in all dimensions is equivalent to conducting a global 3D convolution of C, H, and W in the original space with kernel shifting across the channel dimension, which may disrupt the distribution of semantic information along the channel dimension.

% In our proposed scheme, for the given intermediate feature X  RCHW , we perform FFT for each channel independently to obtain the latent frequency representations, as described in the Sec. 3.2 of the main paper. Here, we investigate other design choices of perform FFT. In the Table 4, we find that performing FFT across H, W, C dimensions leads to performance drop compared to Baseline. For the intermediate feature X  RCHW , its different channels correspond to the outputs of different convolution kernels, which are independent in fact. Thus, we perform FFT on each channel of X independently.

% \begin{table}[t]
%   \centering
%   \resizebox{0.8\linewidth}{!}{
%     \begin{tabular}{l|cc|c}
%     \toprule
%     \multicolumn{1}{c|}{\makecell[c]{Frequency  \\ Transformations}} & Param (M) & FLOPs (G) & Top-1 \\
%     \midrule
%     % FFT   &       &  \\
%     FFT(CHW) &  5.5 & 1.5    &  78.5 \\
%     % \midrule
%     \rowcolor[gray]{0.9}FFT(HW)  &  5.5 & 1.5    &  79.8 \\
%     \bottomrule
%     \end{tabular}%
%     }
%   \vspace{+1.6mm}
%   \caption{ FFT (CHW) refers to the models in which FFT is performed across the height (H), width (W) and channel (C) dimensions. In FFT (HW), we just perform FFT across the height and width dimensions, i.e., for each feature map independently, which is the default setting in this paper.  }
%   \label{tab:fft_dimension}%
% \end{table}%

% \paragraph{Ablation of the fusion structure} 
% \tcb{As we describe on Sec. 3.4 in the main paper, we employ a plain fusion for combining local and global features at each stage. In Table~\ref{tab:fuse_block}, we presents the results of an ablation study on the fusion structure and its location in terms of model parameter, FLOPs, and Top-1 accuracy. As shown in Table~\ref{tab:fuse_block}, ``Fusion (Concat)'' outperforms ``Fusion (Sum)'' by 0.4\% accuracy with only 0.3M parameters and 0.1G FLOPs increase, which indicates that the fusion structure is an efficient way to fuse global and local information in light-weight models. In addition, as we increase the number of stages in the fusion block, namely from ``w/o Fusion'', ``Fusion@($S_1$'', ``Fusion@($S_1$,$S_2$)'', to ``Fusion@($S_1, S_2, S_3$)'', we observe a consistent improvement in the Top-1 performance of the model. In particular, "Fusion@($S_1$)" shows the most significant improvement, with an increase of 0.2\% in Top-1 accuracy with negligible increase in parameters and FLOPs, which suggests that the fusion block integrates local and global features in the shallow stages.}



% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[t]
%   \centering
%   \resizebox{0.8\linewidth}{!}{
%     \begin{tabular}{l|cc|c}
%     \toprule
%     \multicolumn{1}{c|}{Fusion} & Param (M) & FLOPs (G) & Top-1 \\
%     \midrule
%     Fusion (Sum) &   5.2    &  1.4  &  79.4\\
%     \rowcolor[gray]{0.9}Fusion (Concat) &  5.5     & 1.5  & 79.8 \\
%     \midrule
%     w/o Fusion &   5.2    & 1.4   & 79.4 \\
%     Fusion@($S_1$)  &   5.2    &  1.4 & 79.6 \\
%     Fusion@($S_1$,$S_2$)  & 5.4   & 1.5  & 79.7 \\
%     % \midrule
%     \rowcolor[gray]{0.9}Fusion@($S_1, S_2, S_3$) &  5.5     & 1.5  & 79.8 \\
%     \bottomrule
%     \end{tabular}%
%     }
%   \vspace{+1.6mm}
%   \caption{\tcb{Ablation on the fusion structure, "Fusion (Sum)" uses residual skip connection to fuse features from the beginning and end of each stage, while "Fusion (Concat)" which we adopt concatenates them with a linear layer. Additionally, We also evaluate the impact of the location of the fusion structure by comparing different configurations, including no fusion structure (W/o Fusion), fusion structure only in stage 1 (Fusion@$S_1$), in stage 1 and 2 (Fusion@($S_1$,$S_2$), and in stage 1, 2, 3 (Fusion@($S_1, S_2, S_3$)), $S_i$ denotes the i-th stage.}}
%   \label{tab:fuse_block}%
% \end{table}%

% \subsection{Ablation of the architecture design}
% \paragraph{Token - Channel v.s. Channel - Token} 
\paragraph{The order of token-mixing and channel-mixing.}
We study the effect of the order of token mixing and channel mixing in backbone design. As shown in Table~\ref{tab:aff_block}, \textit{channel-mixing first} design is slightly superior to the \textit{token-mixing first} design, indicating it would be better to perform within-token refinement before token mixing. Overall, they deliver very close results.
% \tcb{In contrast to the traditional order used in transformer design (token mixer followed by channel mixer), we adopt a channel mixer followed by a token mixer in \ourblock, following the approach in previous works~\cite{tu2022maxvit,yang2022moat}. As shown in Table~\ref{tab:aff_block}, this design choice leads to a 0.1\% increase in Top-1 accuracy without any additional computing cost.}


\begin{table}[t]
  \centering
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|cc|c}
    \toprule
    \multicolumn{1}{c|}{Order} & Param (M) & FLOPs (G) & Top-1 \\
    \midrule
    Token-mixing first &   5.5    & 1.5 &  79.7 \\
    \rowcolor[gray]{0.9}Channel-mixing first (Ours) &   5.5    & 1.5 &  79.8 \\
    % \midrule
    % channel mixer (FFN) &   5.5    & 1.5 &  79.5 \\
    % \rowcolor[gray]{0.9}channel mixer (MBConv) &   5.5    & 1.5 &  79.8 \\
    \bottomrule
    \end{tabular}%
    }
  \vspace{+1.6mm}
  \caption{Investigation results of the effects of the order of token-mixing and channel-mixing in \ourblock. ``Token-mixing first'' denotes performing token mixing before channel mixing while ``Channel-mixing first'' is an opposite order.}
  % \caption{\tcb{Comparison of different order of token mixer and channel mixer in \ourblock. ``Token-Channel'' refers to the use of a token mixer followed by a channel mixer in \ourblock, while ``Channel-Token'' is the opposite, with a channel mixer followed by a token mixer.}}
  \label{tab:aff_block}%
\end{table}%

\paragraph{The design of channel mixer.}
% \paragraph{FFN v.s. MBConv} 
In this paper, we focus on the design of token mixer while the channel mixer is not the main point of this work. Thus, we employ a plain channel mixer implemented by Mobilenet Convolution Block (MBConv) \cite{sandler2018mobilenetv2} following prior works~\cite{wang2022pvt,chen2021regionvit,tu2022maxvit,yang2022moat}. Here, we compare two dominated designs of the channel mixer in Table~\ref{tab:channel_mixer} for a detailed empirical study. 
Feed-Forward Network (FFN)~\cite{vaswani2017attention,dosovitskiy2020image} adopts two cascaded linear layers while MBConv adds a depth-wise 3$\times$3 convolution layer between two linear layers. We find MBConv is more powerful as the channel mixer in lightweight neural network design than FFN, in which their computational costs are almost the same.
% \tcb{Following previous works~\cite{wang2022pvt,chen2021regionvit,tu2022maxvit,yang2022moat}, we adopt MBConv as the channel mixer of \ourblock, which inserts a low computing cost DW 3$\times$3 convolution layer into the regular channel mixer FFN. Table~\ref{tab:channel_mixer} illustrates that this approach results in a 0.3\% increase in Top-1 accuracy with almost no additional parameters or FLOPs.}

\begin{table}[t]
  \centering
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{l|cc|c}
    \toprule
    \multicolumn{1}{c|}{Channel-mixing Design} & Param (M) & FLOPs (G) & Top-1 \\
    \midrule
    FFN &   5.5    & 1.5 &  79.5 \\
    \rowcolor[gray]{0.9}MBConv (Ours) &   5.5    & 1.5 &  79.8 \\
    \bottomrule
    \end{tabular}%
    }
  \vspace{+1.6mm}
  \caption{Comparisons of two mainstream designs for channel mixers. They are  FFN (Feed-Forward Network) and MBConv (Mobilenet Convolution Block) as channel mixer. Note that the design of channel mixers is not the focus of our work, and we adopt MBConv as token mixers in our proposed method.}
  % \caption{\tcb{Comparison between model using FFN (Feed-Forward Network) as channel mixer and model using MBConv (Mobilenet Convolution Block) as channel mixer.}}
  \label{tab:channel_mixer}%
\end{table}%


% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[t]
%   \centering
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{cc|ccc|c}
%     \toprule
%     Channel & Block number & Throughput & Param (M) & FLOPs (G) & Top-1 \\
%     \midrule
%           & [3, 4, 2] &       &       &     & \\
%           & [3, 8, 2] &       &       &     & \\
%     \toprule
%     \end{tabular}%
%   \label{tab:addlabel}%
%   }
%   \caption{Add caption}
% \end{table}%

\section{Visualization Results}
We present the qualitative results of \ours~on object detection and semantic segmentation in Fig.~\ref{fig:qualitative_results_detection} and Fig.~\ref{fig:qualitative_results_segmentation}, respectively. These qualitative results demonstrate that our proposed \ours~is capable of precisely localizing and classifying objects in the dense prediction tasks with diverse object scales and complex backgrounds as a lightweight network design. And this demonstrates the effectiveness of our proposed \ourop~token mixer in preserving the spatial structure information during token mixing. 

\section{Limitations}
Although we show the superiority of \ours~in the running speed, We have to point out that there is still a gap between the current running speed and the theoretical upper limit of the speed it can achieve, as the speed optimization in engineering implementation of frequency transformations such as FFT/iFFT has not been fully considered yet. Besides, this work only focuses on the vision domain currently. We are looking forwards to its further extension to other research fields.
% \tcb{First of all, The theoretical computational complexity of FFT is notably low; however, in real-world computing hardware, there are only limited low-level optimizations specifically designed for FFT. In contrast, convolutional networks have undergone continuous optimization for decades, allowing their operational speed and latency to approach theoretical limits on general computing hardware, thus providing substantial advantages. We encourage more low-level optimization for FFT in the future. Secondly, the primary focus of our current work is on general-purpose lightweight backbone design, without extensively investigating lightweight designs specifically tailored for downstream tasks, such as object detection, person re-identification, pose estimation, among others. We recognize this limitation and intend to explore these specialized areas in future research. }

% \tcb{Fig.~\ref{fig:qualitative_results_detection}, \ref{fig:qualitative_results_segmentation} show the qualitative results of \ours~on detection and segmentation tasks, respectively.}  The results demonstrate that our \ours~model is capable of effectively localizing and classifying objects in diverse scenes, while also providing high-quality segmentation masks and generalizable representations of the objects. These capabilities enable our model to perform well in real-world scenarios, indicating its potential for practical applications.

% \section{More details about Eq.(7) in the main paper}
% As we describe in Sec. 3.3 and Eq.(7) in the main paper, the linear layer and Fourier transform are in fact commutative, \ieno, $\operatorname{Linear}(\mathcal{F}(\mathbf{X}))\!=\!\mathcal{F}(\operatorname{Linear}(\mathbf{X}))$, which can be proved with the distributive property of matrix multiplication by:
% \begin{equation}
% \begin{split}
%     &\ \mathbf{W}_{Linear} \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \mathbf{X}(h, w) e^{-2\pi i(uh+vw)} \\
%     = &\ \sum_{h=0}^{H-1} \sum_{w=0}^{W-1} \left(\mathbf{W}_{Linear} \mathbf{X}(h, w)\right) e^{-2\pi i(uh+vw)},
% \end{split}
% \label{eq: commutative}
% \end{equation}
% where $\mathbf{W}_{Linear}$ denotes the parameters of a linear layer.
% The weights can be expressed in either real numbers or complex numbers, and the commutative property in Eq. (7) of the main paper holds for both cases. Furthermore, when applying the Fast Fourier Transform (FFT), the real FFT (rFFT) is used, resulting in the elimination of the imaginary part of the weights. As a result, this modification does not affect the validity of the commutative property.





% Figure environment removed

% Figure environment removed

\end{document}