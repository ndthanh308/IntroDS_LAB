@article{megiddo_complexity_1988,
	title = {On the complexity of polyhedral separability},
	volume = {3},
	issn = {0179-5376, 1432-0444},
	url = {http://link.springer.com/10.1007/BF02187916},
	doi = {10.1007/BF02187916},
	abstract = {It is NP-complete to recognize whether two sets of points in general space can be separated by two hyperplanes. It is NP-complete to recognize whether two sets of points in the plane can be separated with k lines. For every fixed k in any fixed dimension, it takes polynomial time to recognize whether two sets of points can be separated with k hyperplanes.},
	language = {en},
	number = {4},
	urldate = {2022-02-28},
	journal = {Discrete \& Computational Geometry},
	author = {Megiddo, Nimrod},
	year = {1988},
	pages = {325--337},
	file = {Megiddo - 1988 - On the complexity of polyhedral separability.pdf:C\:\\Users\\mana_\\Zotero\\storage\\592HBX46\\Megiddo - 1988 - On the complexity of polyhedral separability.pdf:application/pdf},
}

@article{astorino_polyhedral_2002,
	title = {Polyhedral Separability Through Successive {LP}},
	volume = {112},
	issn = {1573-2878},
	url = {https://doi.org/10.1023/A:1013649822153},
	doi = {10.1023/A:1013649822153},
	abstract = {We address the problem of discriminating between two finite point sets \$\${\textbackslash}mathcal\{A\}\{{\textbackslash}text\{ and \}\}{\textbackslash}mathcal\{B\}\$\$in the n-dimensional space by h hyperplanes generating a convex polyhedron. If the intersection of the convex hull of \$\${\textbackslash}mathcal\{A\}\{{\textbackslash}text\{ with \}\}{\textbackslash}mathcal\{B\}\$\$is empty, the two sets can be strictly separated (polyhedral separability). We introduce an error function which is piecewise linear, but not convex nor concave, and define a descent procedure based on the iterative solution of the LP descent direction finding subproblems.},
	language = {en},
	number = {2},
	urldate = {2022-02-28},
	journal = {Journal of Optimization Theory and Applications},
	author = {Astorino, A. and Gaudioso, M.},
	year = {2002},
	pages = {265--293},
	file = {Springer Full Text PDF:C\:\\Users\\mana_\\Zotero\\storage\\L6CVY3PM\\Astorino and Gaudioso - 2002 - Polyhedral Separability Through Successive LP.pdf:application/pdf},
}


@article{roijers_multi-objective_2017,
	title = {Multi-{Objective} {Decision} {Making}},
	volume = {11},
	issn = {1939-4608},
	url = {https://www.morganclaypool.com/doi/abs/10.2200/S00765ED1V01Y201704AIM034},
	doi = {10.2200/S00765ED1V01Y201704AIM034},
	number = {1},
	urldate = {2022-02-20},
	journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	author = {Roijers, Diederik M. and Whiteson, Shimon},
	year = {2017},
	pages = {1--129},
	file = {Full Text PDF:C\:\\Users\\mana_\\Zotero\\storage\\YH5CN6DQ\\Roijers and Whiteson - 2017 - Multi-Objective Decision Making.pdf:application/pdf},
}

@book{altman_constrained_1999,
	title = {Constrained {Markov} decision processes},
	volume = {7},
	publisher = {CRC Press},
	author = {Altman, Eitan},
	year = {1999},
	file = {608fd77b9b65f5bd378e8797b2ab1b8acde7.pdf:C\:\\Users\\mana_\\Zotero\\storage\\5LDLGBIR\\608fd77b9b65f5bd378e8797b2ab1b8acde7.pdf:application/pdf},
}

@inproceedings{abbeel_apprenticeship_2004,
	title = {Apprenticeship learning via inverse reinforcement learning},
	isbn = {978-1-58113-838-2},
	url = {https://doi.org/10.1145/1015330.1015430},
	doi = {10.1145/1015330.1015430},
	abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
	urldate = {2022-04-27},
	booktitle = {Proceedings of the twenty-first international conference on {Machine} learning},
	author = {Abbeel, Pieter and Ng, Andrew Y.},
	year = {2004},
	pages = {1-8},
	file = {Full Text PDF:C\:\\Users\\mana_\\Zotero\\storage\\B2R49PLR\\Abbeel and Ng - 2004 - Apprenticeship learning via inverse reinforcement .pdf:application/pdf},
}


@article{wirth_survey_2017,
	title = {A survey of preference-based reinforcement learning methods},
	volume = {18},
	issn = {1532-4435},
	abstract = {Reinforcement learning (RL) techniques optimize the accumulated long-term reward of a suitably chosen reward function. However, designing such a reward function often requires a lot of task-specific prior knowledge. The designer needs to consider different objectives that do not only influence the learned behavior but also the learning progress. To alleviate these issues, preference-based reinforcement learning algorithms (PbRL) have been proposed that can directly learn from an expert's preferences instead of a hand-designed numeric reward. PbRL has gained traction in recent years due to its ability to resolve the reward shaping problem, its ability to learn from non numeric rewards and the possibility to reduce the dependence on expert knowledge. We provide a unified framework for PbRL that describes the task formally and points out the different design principles that affect the evaluation task for the human as well as the computational complexity. The design principles include the type of feedback that is assumed, the representation that is learned to capture the preferences, the optimization problem that has to be solved as well as how the exploration/exploitation problem is tackled. Furthermore, we point out shortcomings of current algorithms, propose open research questions and briefly survey practical tasks that have been solved using PbRL.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Wirth, Christian and Akrour, Riad and Neumann, Gerhard and Fürnkranz, Johannes},
	year = {2017},
	keywords = {Markov decision process, policy search, preference learning, preference-based reinforcement learning, qualitative feedback, reinforcement learning, temporal difference learning},
	pages = {4945--4990},
	file = {Full Text PDF:C\:\\Users\\mana_\\Zotero\\storage\\NRECV2VP\\Wirth et al. - 2017 - A survey of preference-based reinforcement learnin.pdf:application/pdf},
}

@misc{sutton_reward_2004,
	title = {The Reward Hypothesis},
	url = {http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html},
	urldate = {2022-04-24},
	author = {Sutton, Richard S.},
	year = {2004},
	note = {\url{http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html}},
	file = {The reward hypothesis:C\:\\Users\\mana_\\Zotero\\storage\\SVXR7HGB\\rewardhypothesis.html:text/html},
}


@article{mangasarian_linear_1965,
	title = {Linear and {Nonlinear} {Separation} of {Patterns} by {Linear} {Programming}},
	volume = {13},
	issn = {0030-364X},
	url = {https://doi.org/10.1287/opre.13.3.444},
	doi = {10.1287/opre.13.3.444},
	abstract = {A pattern separation problem is basically a problem of obtaining a criterion for distinguishing between the elements of two disjoint sets of patterns. The patterns are usually represented by points in a Euclidean space. One way to achieve separation is to construct a plane or a nonlinear surface such that one set of patterns lies on one side of the plane or the surface, and the other set of patterns on the other side. Recently, it has been shown that linear and ellipsoidal separation may be achieved by nonlinear programming. In this work it is shown that both linear and nonlinear separation may be achieved by linear programming.},
	number = {3},
	urldate = {2022-04-26},
	journal = {Operations Research},
	author = {Mangasarian, O. L.},
	year = {1965},
	pages = {444--452},
}

@book{von_neumann_theory_1944,
	title = {Theory of Games and Economic Behavior},
	isbn = {978-0-691-13061-3},
	url = {https://www.jstor.org/stable/j.ctt1r2gkx},
	abstract = {This is the classic work upon which modern-day game theory is based. What began more than sixty years ago as a modest proposal that a mathematician and an economist write a short paper together blossomed, in 1944, when Princeton University Press published \textit{Theory of Games and Economic Behavior} . In it, John von Neumann and Oskar Morgenstern conceived a groundbreaking mathematical theory of economic and social organization, based on a theory of games of strategy. Not only would this revolutionize economics, but the entirely new field of scientific inquiry it yielded--game theory--has since been widely used to analyze a host of real-world phenomena from arms races to optimal policy choices of presidential candidates, from vaccination policy to major league baseball salary negotiations. And it is today established throughout both the social sciences and a wide range of other sciences.  This sixtieth anniversary edition includes not only the original text but also an introduction by Harold Kuhn, an afterword by Ariel Rubinstein, and reviews and articles on the book that appeared at the time of its original publication in the \textit{New York Times} , tthe \textit{American Economic Review} , and a variety of other publications. Together, these writings provide readers a matchless opportunity to more fully appreciate a work whose influence will yet resound for generations to come.},
	urldate = {2022-04-27},
	publisher = {Princeton University Press},
	author = {von Neumann, John and Morgenstern, Oskar and Rubinstein, Ariel},
	year = {1944},
}

@inproceedings{sunehag_axioms_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Axioms for Rational Reinforcement Learning},
	isbn = {978-3-642-24412-4},
	doi = {10.1007/978-3-642-24412-4_27},
	abstract = {We provide a formal, simple and intuitive theory of rational decision making including sequential decisions that affect the environment. The theory has a geometric flavor, which makes the arguments easy to visualize and understand. Our theory is for complete decision makers, which means that they have a complete set of preferences. Our main result shows that a complete rational decision maker implicitly has a probabilistic model of the environment. We have a countable version of this result that brings light on the issue of countable vs finite additivity by showing how it depends on the geometry of the space which we have preferences over. This is achieved through fruitfully connecting rationality with the Hahn-Banach Theorem. The theory presented here can be viewed as a formalization and extension of the betting odds approach to probability of Ramsey and De Finetti [Ram31, deF37].},
	language = {en},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer},
	author = {Sunehag, Peter and Hutter, Marcus},
	year = {2011},
	keywords = {Banach Space, Linear Functional, Probability, Rationality, Utility},
	pages = {338--352},
	file = {Submitted Version:C\:\\Users\\mana_\\Zotero\\storage\\IMWUMDHQ\\Sunehag and Hutter - 2011 - Axioms for Rational Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{brown_extrapolating_2019,
	title = {Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations},
	url = {https://proceedings.mlr.press/v97/brown19a.html},
	abstract = {A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.},
	language = {en},
	urldate = {2022-04-27},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	author = {Brown, Daniel and Goo, Wonjoon and Nagarajan, Prabhat and Niekum, Scott},
	year = {2019},
	pages = {783--792},
	file = {Full Text PDF:C\:\\Users\\mana_\\Zotero\\storage\\M8GLHB7Q\\Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf:application/pdf;Supplementary PDF:C\:\\Users\\mana_\\Zotero\\storage\\TKKT5HJT\\Brown et al. - 2019 - Extrapolating Beyond Suboptimal Demonstrations via.pdf:application/pdf},
}


@inproceedings{abel_expressivity_2021,
	title = {On the Expressivity of {Markov} Reward},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/4079016d940210b4ae9ae7d41c4a2065-Abstract.html},
	abstract = {Reward is the driving force for reinforcement-learning agents. This paper is dedicated to understanding the expressivity of reward as a way to capture tasks that we would want an agent to perform. We frame this study around three new abstract notions of “task” that might be desirable: (1) a set of acceptable behaviors, (2) a partial ordering over behaviors, or (3) a partial ordering over trajectories. Our main results prove that while reward can express many of these tasks, there exist instances of each task type that no Markov reward function can capture. We then provide a set of polynomial-time algorithms that construct a Markov reward function that allows an agent to optimize tasks of each of these three types, and correctly determine when no such reward function exists. We conclude with an empirical study that corroborates and illustrates our theoretical findings.},
	urldate = {2022-04-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Abel, David and Dabney, Will and Harutyunyan, Anna and Ho, Mark K and Littman, Michael and Precup, Doina and Singh, Satinder},
	year = {2021},
	pages = {7799--7812},
	file = {Full Text PDF:C\:\\Users\\mana_\\Zotero\\storage\\5F2WNQAN\\Abel et al. - 2021 - On the Expressivity of Markov Reward.pdf:application/pdf},
}

@inproceedings{pitis_rethinking_2019,
	title = {Rethinking the discount factor in reinforcement learning: {A} decision theoretic approach},
	shorttitle = {Rethinking the discount factor in reinforcement learning},
	booktitle = {Proceedings of {theThirty}-third {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Pitis, Silviu},
	year = {2019},
	pages = {7949--7956},
	file = {Full Text PDF:C\:\\Users\\mana_\\Zotero\\storage\\KRMQEPWS\\Pitis - 2019 - Rethinking the discount factor in reinforcement le.pdf:application/pdf},
}
