\begin{thebibliography}{10}

\bibitem{Roy2019towards}
Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.
\newblock Towards spike-based machine intelligence with neuromorphic computing.
\newblock {\em Nature}, 575:607--617, 2019.

\bibitem{Guo2021neural}
Wenzhe Guo, Mohammed~E. Fouda, Ahmed~M. Eltawil, and Khaled~Nabil Salama.
\newblock Neural coding in spiking neural networks: A comparative study for
  robust neuromorphic systems.
\newblock {\em Frontiers in Neuroscience}, 15:638474, 2021.

\bibitem{Auge2021survey}
Daniel Auge, Julian Hille, Etienne Mueller, and Alois Knoll.
\newblock A survey of encoding techniques for signal processing in spiking
  neural networks.
\newblock {\em Neural Processing Letters}, 53:4693--4710, 2021.

\bibitem{Diehl2015fast}
P.~U. {Diehl}, D.~{Neil}, J.~{Binas}, M.~{Cook}, S.~{Liu}, and M.~{Pfeiffer}.
\newblock Fast-classifying, high-accuracy spiking deep networks through weight
  and threshold balancing.
\newblock In {\em 2015 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8, July 2015.

\bibitem{Ruechauer2017conversion}
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and
  Shih-Chii Liu.
\newblock Conversion of continuous-valued deep networks to efficient
  event-driven networks for image classification.
\newblock {\em Frontiers in Neuroscience}, 11:682, 2017.

\bibitem{Kim2020spiking}
Seijoon Kim, Seongsik Park, Byunggook Na, and Sungroh Yoon.
\newblock Spiking-{YOLO}: spiking neural network for energy-efficient object
  detection.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 11270--11277, 2020.

\bibitem{Davies2018loihi}
M.~Davies et~al.
\newblock Loihi: A neuromorphic manycore processor with on-chip learning.
\newblock {\em IEEE Micro}, 38(1):82--99, January 2018.

\bibitem{Barth2012experimental}
Alison~L. Barth and James~F.A. Poulet.
\newblock Experimental evidence for sparse firing in the neocortex.
\newblock {\em Trends in Neurosciences}, 35(6):345--355, 2012.

\bibitem{Fujiidynamical1996}
Hiroshi Fujii, Hiroyuki Ito, Kazuyuki Aihara, Natsuhiro Ichinose, and Minoru
  Tsukada.
\newblock Dynamical cell assembly hypothesis â€• {T}heoretical possibility of
  spatio-temporal coding in the cortex.
\newblock {\em Neural Networks}, 9(8):1303--1350, 1996.

\bibitem{Gollisch2008rapid}
Tim Gollisch and Markus Meister.
\newblock Rapid neural coding in the retina with relative spike latencies.
\newblock {\em Science}, 319(5866):1108--1111, 2008.

\bibitem{Portelli2016rank}
Geoffrey Portelli, John~M Barrett, Gerrit Hilgen, Timoth{\'e}e Masquelier,
  Alessandro Maccione, Stefano Di~Marco, Luca Berdondini, Pierre Kornprobst,
  and Evelyne Sernagor.
\newblock Rank order coding: a retinal information decoding strategy revealed
  by large-scale multielectrode array retinal recordings.
\newblock {\em e{N}euro}, 3(3):e0134, 2016.

\bibitem{Jaramillo2017phase}
Phase precession: a neural code underlying episodic memory?
\newblock {\em Current Opinion in Neurobiology}, 43:130--138, 2017.

\bibitem{Tavanaei2019Deep}
Amirhossein Tavanaei, Masoud Ghodrati, Saeed~Reza Kheradpisheh, Timoth^^c3^^a9e
  Masquelier, and Anthony Maida.
\newblock Deep learning in spiking neural networks.
\newblock {\em Neural Networks}, 111:47--63, 2019.

\bibitem{Pfeiffer2018Deep}
Michael Pfeiffer and Thomas Pfeil.
\newblock Deep learning with spiking neurons: Opportunities and challenges.
\newblock {\em Frontiers in Neuroscience}, 12(774):1--18, 2018.

\bibitem{Dampfhoffer2023backpropagation}
Manon Dampfhoffer, Thomas Mesquida, Alexandre Valentian, and Lorena Anghel.
\newblock Backpropagation-based learning techniques for deep spiking neural
  networks: A survey.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  pages 1--16, 2023.

\bibitem{Eshraghian2023training}
Jason~K. Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish
  Dwivedi, Mohammed Bennamoun, Doo~Seok Jeong, and Wei~D. Lu.
\newblock Training spiking neural networks using lessons from deep learning.
\newblock {\em arXiv:2109.12894}, 2023.

\bibitem{Neftci2019surrogate}
Emre~O. Neftci, Hesham Mostafa, and Friedemann Zenke.
\newblock Surrogate gradient learning in spiking neural networks: Bringing the
  power of gradient-based optimization to spiking neural networks.
\newblock {\em IEEE Signal Processing Magazine}, 36(6):51--63, 2019.

\bibitem{Neftci2017event}
Emre~O. Neftci, Charles Augustine, Somnath Paul, and Georgios Detorakis.
\newblock Event-driven random back-propagation: Enabling neuromorphic deep
  learning machines.
\newblock {\em Frontiers in Neuroscience}, 11:324, 2017.

\bibitem{Huh2018gradient}
Dongsung Huh and Terrence~J. Sejnowski.
\newblock Gradient descent for spiking neural networks.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 1440^^e2^^80^^93--1450, 2018.

\bibitem{Zenke2018super}
Friedemann Zenke and Surya Ganguli.
\newblock {SuperSpike: Supervised Learning in Multilayer Spiking Neural
  Networks}.
\newblock {\em Neural Computation}, 30(6):1514--1541, 2018.

\bibitem{Wu2018spatio}
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi.
\newblock Spatio-temporal backpropagation for training high-performance spiking
  neural networks.
\newblock {\em Frontiers in Neuroscience}, 12:331, 2018.

\bibitem{Zenke2021remarkable}
Friedemann Zenke and Tim~P. Vogels.
\newblock {The Remarkable Robustness of Surrogate Gradient Learning for
  Instilling Complex Function in Spiking Neural Networks}.
\newblock {\em Neural Computation}, 33(4):899--925, 2021.

\bibitem{Yin2021accurate}
Bojian Yin, Federico Corradi, and Sander~M Boht{\'e}.
\newblock Accurate and efficient time-domain classification with adaptive
  spiking recurrent neural networks.
\newblock {\em Nature Machine Intelligence}, 3(10):905--913, 2021.

\bibitem{Bellec2020solution}
Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj,
  Robert Legenstein, and Wolfgang Maass.
\newblock A solution to the learning dilemma for recurrent networks of spiking
  neurons.
\newblock {\em Nature communications}, 11(1):1--15, 2020.

\bibitem{Kim2021revisiting}
Youngeun Kim and Priyadarshini Panda.
\newblock Revisiting batch normalization for training low-latency deep spiking
  neural networks from scratch.
\newblock {\em Frontiers in Neuroscience}, 15:773954, 2021.

\bibitem{Cramer2022surrogate}
Benjamin Cramer, Sebastian Billaudelle, Simeon Kanya, Aron Leibfried, Andreas
  Gr^^c3^^bcbl, Vitali Karasenko, Christian Pehle, Korbinian Schreiber, Yannik
  Stradmann, Johannes Weis, Johannes Schemmel, and Friedemann Zenke.
\newblock Surrogate gradients for analog neuromorphic computing.
\newblock {\em Proceedings of the National Academy of Sciences},
  119(4):e2109194119, 2022.

\bibitem{Yan2022backpropagation}
Yulong Yan, Haoming Chu, Yi~Jin, Yuxiang Huan, Zhuo Zou, and Lirong Zheng.
\newblock Backpropagation with sparsity regularization for spiking neural
  network learning.
\newblock {\em Frontiers in Neuroscience}, 16:760298, 2022.

\bibitem{Bohte2002error}
Sander~M. Bohte, Joost~N. Kok, and Han~La Poutr^^c3^^a9.
\newblock Error-backpropagation in temporally encoded networks of spiking
  neurons.
\newblock {\em Neurocomputing}, 48(1):17--37, 2002.

\bibitem{Thorpe2001spike}
Simon Thorpe, Arnaud Delorme, and Rufin {Van Rullen}.
\newblock Spike-based strategies for rapid processing.
\newblock {\em Neural Networks}, 14(6):715--725, 2001.

\bibitem{Bonilla2022analyzing}
Lina Bonilla, Jacques Gautrais, Simon Thorpe, and Timoth^^c3^^a9e Masquelier.
\newblock Analyzing time-to-first-spike coding schemes: A theoretical approach.
\newblock {\em Frontiers in Neuroscience}, 16:971937, 2022.

\bibitem{Mostafa2018supervised}
H.~{Mostafa}.
\newblock Supervised learning based on temporal coding in spiking neural
  networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  29(7):3227--3235, 2018.

\bibitem{Kheradpisheh2019s4nn}
S.~R. Kheradpisheh and Timoth^^c3^^a9e Masquelier.
\newblock Temporal backpropagation for spiking neural networks with one spike
  per neuron.
\newblock {\em International Journal of Neural Systems}, 30(6):2050027, 2020.

\bibitem{Comsa2021temporal}
Iulia-Maria Com^^c5^^9fa, Krzysztof Potempa, Luca Versari, Thomas Fischbacher,
  Andrea Gesmundo, and Jyrki Alakuijala.
\newblock Temporal coding in spiking neural networks with alpha synaptic
  function: Learning with backpropagation.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(10):5939--5952, 2022.

\bibitem{Sakemi2023supervised}
Yusuke Sakemi, Kai Morino, Takashi Morie, and Kazuyuki Aihara.
\newblock A supervised learning algorithm for multilayer spiking neural
  networks based on temporal coding toward energy-efficient vlsi processor
  design.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  34(1):394--408, 2023.

\bibitem{Sakemi2021effects}
Yusuke Sakemi, Takashi Morie, Takeo Hosomi, and Kazuyuki Aihara.
\newblock Effects of {VLSI} circuit constraints on temporal-coding multilayer
  spiking neural networks.
\newblock {\em arXiv:2106.10382}, 2021.

\bibitem{Zhang2021rectified}
Malu Zhang, Jiadong Wang, Jibin Wu, Ammar Belatreche, Burin Amornpaisannon,
  Zhixuan Zhang, Venkata Pavan~Kumar Miriyala, Hong Qu, Yansong Chua, Trevor~E.
  Carlson, and Haizhou Li.
\newblock Rectified linear postsynaptic potential function for backpropagation
  in deep spiking neural networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(5):1947--1958, 2022.

\bibitem{Goltz2021fast}
J~G{\"o}ltz et~al.
\newblock Fast and energy-efficient neuromorphic deep learning with first-spike
  times.
\newblock {\em Nature Machine Intelligence}, 3:823--835, 2021.

\bibitem{Oh2022neuron}
Seongbin Oh, Dongseok Kwon, Gyuho Yeom, Won-Mook Kang, Soochang Lee, Sung~Yun
  Woo, Jaehyeon Kim, and Jong-Ho Lee.
\newblock Neuron circuits for low-power spiking neural networks using
  time-to-first-spike encoding.
\newblock {\em IEEE Access}, 10:24444--24455, 2022.

\bibitem{Murshed2021machine}
M.~G.~Sarwar Murshed, Christopher Murphy, Daqing Hou, Nazar Khan, Ganesh
  Ananthanarayanan, and Faraz Hussain.
\newblock Machine learning at the network edge: A survey.
\newblock {\em ACM Comput. Surv.}, 54(8):1--37, oct 2021.

\bibitem{Lecun1998gradient}
Y.~{LeCun}, L.~{Bottou}, Y.~{Bengio}, and P.~{Haffner}.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{Xiao2017fashion}
H.~Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv:1708.07747}, 2017.

\bibitem{CIFAR10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.

\bibitem{Zhou2021temporal}
Shibo Zhou, Xiaohua Li, Ying Chen, Sanjeev~T. Chandrasekaran, and Arindam
  Sanyal.
\newblock Temporal-coded deep spiking neural network with easy training and
  robust performance.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  35(12):11143--11151, 2021.

\bibitem{Yamamoto2022timing}
Kakei Yamamoto, Yusuke Sakemi, and Kazuyuki Aihara.
\newblock Timing-based backpropagation in spiking neural networks without
  single-spike restrictions.
\newblock {\em arXiv:2211.16113}, 2022.

\bibitem{Moradi2018scalable}
S.~{Moradi}, N.~{Qiao}, F.~{Stefanini}, and G.~{Indiveri}.
\newblock A scalable multicore architecture with heterogeneous memory
  structures for dynamic neuromorphic asynchronous processors ({DYNAP}s).
\newblock {\em IEEE Transactions on Biomedical Circuits and Systems},
  12(1):106--122, 2018.

\bibitem{Sakemi2022spiking}
Yusuke Sakemi, Kai Morino, Takashi Morie, Takeo Hosomi, and Kazuyuki Aihara.
\newblock A spiking neural network with resistively coupled synapses using
  time-to-first-spike coding towards efficient charge-domain computing.
\newblock In {\em 2022 IEEE International Symposium on Circuits and Systems
  (ISCAS)}, pages 2152--2156, 2022.

\bibitem{Oh2020hardware}
Seongbin Oh, Dongseok Kwon, Gyuho Yeom, Won-Mook Kang, Soochang Lee, Sung~Yun
  Woo, Jang~Saeng Kim, Min~Kyu Park, and Jong-Ho Lee.
\newblock Hardware implementation of spiking neural networks using
  time-to-first-spike encoding.
\newblock {\em arXiv:2006.05033}, 2020.

\bibitem{Kheradpisheh2022bs4nn}
Saeed~Reza Kheradpisheh, Maryam Mirsadeghi, and Timoth{\'e}e Masquelier.
\newblock {BS4NN}: Binarized spiking neural networks with temporal coding and
  learning.
\newblock {\em Neural Processing Letters}, 54(2):1255--1273, 2022.

\bibitem{Faghihi2022synaptic}
Faramarz Faghihi, Hany Alashwal, and Ahmed~A Moustafa.
\newblock A synaptic pruning-based spiking neural network for hand-written
  digits classification.
\newblock {\em Frontiers in Artificial Intelligence}, 5:680165, 2022.

\bibitem{Han2022adaptive}
Bing Han, Feifei Zhao, Yi~Zeng, and Wenxuan Pan.
\newblock Adaptive sparse structure development with pruning and regeneration
  for spiking neural networks.
\newblock {\em arXiv preprint arXiv:2211.12219}, 2022.

\bibitem{Kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv}, 1412.6980, 2014.

\bibitem{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\end{thebibliography}
