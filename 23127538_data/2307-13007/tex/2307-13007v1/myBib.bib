

@article{Eshraghian2023training,
      title={Training Spiking Neural Networks Using Lessons From Deep Learning}, 
      author={Jason K. Eshraghian and Max Ward and Emre Neftci and Xinxin Wang and Gregor Lenz and Girish Dwivedi and Mohammed Bennamoun and Doo Seok Jeong and Wei D. Lu},
      year={2023},
      eprint={2109.12894},
      journal={arXiv:2109.12894},
      primaryClass={cs.NE}
}

@article{Jaramillo2017phase,
title = {Phase precession: a neural code underlying episodic memory?},
journal = {Current Opinion in Neurobiology},
volume = {43},
pages = {130-138},
year = {2017},
%%note = {Neurobiology of Learning and Plasticity},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2017.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0959438817300533},
author = {Jorge Jaramillo and Richard Kempter},
abstract = {In the hippocampal formation, the sequential activation of place-specific cells represents a conceptual model for the spatio-temporal events that assemble episodic memories. The imprinting of behavioral sequences in hippocampal networks might be achieved via spike-timing-dependent plasticity and phase precession of the spiking activity of neurons. It is unclear, however, whether phase precession plays an active role by enabling sequence learning via synaptic plasticity or whether phase precession passively reflects retrieval dynamics. Here we examine these possibilities in the context of potential mechanisms generating phase precession. Knowledge of these mechanisms would allow to selectively alter phase precession and test its role in episodic memory. We finally review the few successful approaches to degrade phase precession and the resulting impact on behavior.}
}


@article{Thorpe2001spike,
title = {Spike-based strategies for rapid processing},
journal = {Neural Networks},
volume = {14},
number = {6},
pages = {715-725},
year = {2001},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(01)00083-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608001000831},
author = {Simon Thorpe and Arnaud Delorme and Rufin {Van Rullen}},
keywords = {Rank Order Coding, Latency, Rapid visual processing, Spikes, Retina, Information},
abstract = {Most experimental and theoretical studies of brain function assume that neurons transmit information as a rate code, but recent studies on the speed of visual processing impose temporal constraints that appear incompatible with such a coding scheme. Other coding schemes that use the pattern of spikes across a population a neurons may be much more efficient. For example, since strongly activated neurons tend to fire first, one can use the order of firing as a code. We argue that Rank Order Coding is not only very efficient, but also easy to implement in biological hardware: neurons can be made sensitive to the order of activation of their inputs by including a feed-forward shunting inhibition mechanism that progressively desensitizes the neuronal population during a wave of afferent activity. In such a case, maximum activation will only be produced when the afferent inputs are activated in the order of their synaptic weights.}
}

@article{Murshed2021machine,
author = {Murshed, M. G. Sarwar and Murphy, Christopher and Hou, Daqing and Khan, Nazar and Ananthanarayanan, Ganesh and Hussain, Faraz},
title = {Machine Learning at the Network Edge: A Survey},
year = {2021},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {8},
issn = {0360-0300},
url = {https://doi.org/10.1145/3469029},
doi = {10.1145/3469029},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {170},
pages = {1-37},
keywords = {mobile edge computing, machine learning, embedded, distributed computing, low-power, resource-constrained, deep learning, Edge intelligence, IoT}
}



@article{Fujiidynamical1996,
title = {Dynamical Cell Assembly Hypothesis — {T}heoretical Possibility of Spatio-temporal Coding in the Cortex},
journal = {Neural Networks},
volume = {9},
number = {8},
pages = {1303-1350},
year = {1996},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(96)00054-8},
url = {https://www.sciencedirect.com/science/article/pii/S0893608096000548},
author = {Hiroshi Fujii and Hiroyuki Ito and Kazuyuki Aihara and Natsuhiro Ichinose and Minoru Tsukada},
keywords = {Carrier of information}
}


@article{Kingma2014adam,
author = {Kingma, Diederik P. and Ba, Jimmy},
title = {Adam: A Method for Stochastic Optimization},
journal = {arXiv},
volume = {1412.6980},
year = {2014},
}

@misc{CIFAR10,
 title={Learning multiple layers of features from tiny images},
 author={Krizhevsky, Alex},
 URL = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
 }

@article{Han2022adaptive,
  title={Adaptive Sparse Structure Development with Pruning and Regeneration for Spiking Neural Networks},
  author={Han, Bing and Zhao, Feifei and Zeng, Yi and Pan, Wenxuan},
  journal={arXiv preprint arXiv:2211.12219},
  year={2022}
}


@ARTICLE{Dampfhoffer2023backpropagation,
  author={Dampfhoffer, Manon and Mesquida, Thomas and Valentian, Alexandre and Anghel, Lorena},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Backpropagation-Based Learning Techniques for Deep Spiking Neural Networks: A Survey}, 
  year={2023},
  volume={},
  number={},
  pages={1-16},
  doi={10.1109/TNNLS.2023.3263008}}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{Shimbo2021scalable,
author = {Akihiro Shimbo  and Ei-Ichi Izawa  and Shigeyoshi Fujisawa },
title = {Scalable representation of time in the hippocampus},
journal = {Science Advances},
volume = {7},
number = {6},
pages = {eabd7013},
year = {2021},
doi = {10.1126/sciadv.abd7013},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.abd7013},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abd7013},
abstract = {Temporal information represented in the hippocampus is scalable. Hippocampal “time cells” encode specific moments of temporally organized experiences that may support hippocampal functions for episodic memory. However, little is known about the reorganization of the temporal representation of time cells during changes in temporal structures of episodes. We investigated CA1 neuronal activity during temporal bisection tasks, in which the sets of time intervals to be discriminated were designed to be extended or contracted across the blocks of trials. Assemblies of neurons encoded elapsed time during the interval, and the representation was scaled when the set of interval times was varied. Theta phase precession and theta sequences of time cells were also scalable, and the fine temporal relationships were preserved between pairs in theta cycles. Moreover, theta sequences reflected the rats’ decisions on the basis of their time estimation. These findings demonstrate that scalable features of time cells may support the capability of flexible temporal representation for memory formation.}}



@article{Barth2012experimental,
title = {Experimental evidence for sparse firing in the neocortex},
journal = {Trends in Neurosciences},
volume = {35},
number = {6},
pages = {345-355},
year = {2012},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2012.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0166223612000513},
author = {Alison L. Barth and James F.A. Poulet},
keywords = {silent neurons, optimality, kurtosis, pyramidal neuron, coding},
abstract = {The advent of unbiased recording and imaging techniques to evaluate firing activity across neocortical neurons has revealed substantial heterogeneity in response properties in vivo, and that a minority of neurons are responsible for the majority of spikes. Despite the computational advantages to sparsely firing populations, experimental data defining the fraction of responsive neurons and the range of firing rates have not been synthesized. Here we review data about the distribution of activity across neuronal populations in primary sensory cortex. Overall, the firing output of granular and infragranular layers is highest. Although subthreshold activity across supragranular neurons is decidedly non-sparse, spikes are much less frequent and some cells are silent. Superficial layers of the cortex may employ specific cell and circuit mechanisms to increase sparseness.}
}


@article{Yamamoto2022timing,
doi = {10.48550/ARXIV.2211.16113},
author = {Yamamoto, Kakei and Sakemi, Yusuke and Aihara, Kazuyuki},
title = {Timing-Based Backpropagation in Spiking Neural Networks Without Single-Spike Restrictions},
journal = {arXiv:2211.16113},
year = {2022},
copyright = {Creative Commons Attribution 4.0 International}
}



@INPROCEEDINGS{Sakemi2022spiking,
  author={Sakemi, Yusuke and Morino, Kai and Morie, Takashi and Hosomi, Takeo and Aihara, Kazuyuki},
  booktitle={2022 IEEE International Symposium on Circuits and Systems (ISCAS)}, 
  title={A Spiking Neural Network with Resistively Coupled Synapses Using Time-to-First-Spike Coding Towards Efficient Charge-Domain Computing}, 
  year={2022},
  volume={},
  number={},
  pages={2152-2156},
  doi={10.1109/ISCAS48785.2022.9937662}
}


@article{Kheradpisheh2022bs4nn,
title={{BS4NN}: Binarized spiking neural networks with temporal coding and learning},
author={Kheradpisheh, Saeed Reza and Mirsadeghi, Maryam and Masquelier, Timoth{\'e}e},
journal={Neural Processing Letters},
volume={54},
number={2},
pages={1255--1273},
year={2022},
publisher={Springer}
}

@article{Faghihi2022synaptic,
title={A Synaptic Pruning-Based Spiking Neural Network for Hand-Written Digits Classification},
author={Faghihi, Faramarz and Alashwal, Hany and Moustafa, Ahmed A},
journal={Frontiers in Artificial Intelligence},
volume={5},
pages={680165},
year={2022},
publisher={Frontiers Media SA}
}


@article{Zhou2021temporal, 
title={Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance}, 
volume={35}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/17329}, 
DOI={10.1609/aaai.v35i12.17329}, 
number={12}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Zhou, Shibo and Li, Xiaohua and Chen, Ying and Chandrasekaran, Sanjeev T. and Sanyal, Arindam}, 
year={2021}, 
pages={11143-11151} 
}

@article{Bellec2020solution,
title={A solution to the learning dilemma for recurrent networks of spiking neurons},
author={Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
journal={Nature communications},
volume={11},
number={1},
pages={1--15},
year={2020},
publisher={Nature Publishing Group}
}


@article{Yan2022backpropagation,
AUTHOR={Yan, Yulong and Chu, Haoming and Jin, Yi and Huan, Yuxiang and Zou, Zhuo and Zheng, Lirong},   
TITLE={Backpropagation With Sparsity Regularization for Spiking Neural Network Learning},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={16},    
pages={760298},
YEAR={2022},      
URL={https://www.frontiersin.org/articles/10.3389/fnins.2022.760298},       
DOI={10.3389/fnins.2022.760298},      
ISSN={1662-453X},   
}




@artiCLE{Neftci2019surrogate,
author={Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
journal={IEEE Signal Processing Magazine}, 
title={Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks}, 
year={2019},
volume={36},
number={6},
pages={51-63},
doi={10.1109/MSP.2019.2931595}}



@article{Zenke2021remarkable,
author = {Zenke, Friedemann and Vogels, Tim P.},
title = "{The Remarkable Robustness of Surrogate Gradient Learning for Instilling Complex Function in Spiking Neural Networks}",
journal = {Neural Computation},
volume = {33},
number = {4},
pages = {899-925},
year = {2021},
issn = {0899-7667},
doi = {10.1162/neco_a_01367},
url = {https://doi.org/10.1162/neco\_a\_01367},
eprint = {https://direct.mit.edu/neco/article-pdf/33/4/899/1902294/neco\_a\_01367.pdf},
}






@article{Cramer2022surrogate,
author = {Benjamin Cramer  and Sebastian Billaudelle  and Simeon Kanya  and Aron Leibfried  and Andreas Grübl  and Vitali Karasenko  and Christian Pehle  and Korbinian Schreiber  and Yannik Stradmann  and Johannes Weis  and Johannes Schemmel  and Friedemann Zenke },
title = {Surrogate gradients for analog neuromorphic computing},
journal = {Proceedings of the National Academy of Sciences},
volume = {119},
number = {4},
pages = {e2109194119},
year = {2022},
doi = {10.1073/pnas.2109194119},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2109194119},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2109194119},
}


@article{Kim2020unifying,
author = {Kim, Jinseok and Kim, Kyungsu and Kim, Jae-Joon},
journal = {Advances in Neural Information Processing Systems},
pages = {19534--19544},
title = {Unifying Activation- and Timing-based Learning Rules for Spiking Neural Networks},
url = {https://proceedings.neurips.cc/paper/2020/file/e2e5096d574976e8f115a8f1e0ffb52b-Paper.pdf},
volume = {33},
year = {2020}
}




@article{Tavanaei2019Deep,
title = "Deep learning in spiking neural networks",
journal = "Neural Networks",
volume = "111",
pages = {47--63},
year = "2019",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2018.12.002",
author = "Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timothée Masquelier and Anthony Maida",
keywords = "Deep learning, Spiking neural network, Biological plausibility, Machine learning, Power-efficient architecture",
abstract = "In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.",
}

@ARTICLE{Pfeiffer2018Deep,
AUTHOR={Pfeiffer, Michael and Pfeil, Thomas},   
TITLE={Deep Learning With Spiking Neurons: Opportunities and Challenges},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={12},      
number={774},     
pages={1-18},
YEAR={2018},      
DOI={10.3389/fnins.2018.00774},      
ISSN={1662-453X},   
ABSTRACT={Spiking neural networks (SNNs) are inspired by information processing in biology, where sparse and asynchronous binary signals are communicated and processed in a massively parallel fashion. SNNs on neuromorphic hardware exhibit favorable properties such as low power consumption, fast inference, and event-driven information processing. This makes them interesting candidates for the efficient implementation of deep neural networks, the method of choice for many machine learning tasks. In this review, we address the opportunities that deep spiking networks offer and investigate in detail the challenges associated with training SNNs in a way that makes them competitive with conventional deep learning, but simultaneously allows for efficient mapping to hardware. A wide range of training methods for SNNs is presented, ranging from the conversion of conventional deep networks into SNNs, constrained training before conversion, spiking variants of backpropagation, and biologically motivated variants of STDP. The goal of our review is to define a categorization of SNN training methods, and summarize their advantages and drawbacks. We further discuss relationships between SNNs and binary networks, which are becoming popular for efficient digital hardware implementation. Neuromorphic hardware platforms have great potential to enable deep spiking networks in real-world applications. We compare the suitability of various neuromorphic systems that have been developed over the past years, and investigate potential use cases. Neuromorphic approaches and conventional machine learning should not be considered simply two solutions to the same classes of problems, instead it is possible to identify and exploit their task-specific advantages. Deep SNNs offer great opportunities to work with new types of event-based sensors, exploit temporal codes and local on-chip learning, and we have so far just scratched the surface of realizing these advantages in practical applications.}
}

@article{Roy2019towards,
  title={Towards spike-based machine intelligence with neuromorphic computing},
  author={Roy, Kaushik and Jaiswal, Akhilesh and Panda, Priyadarshini},
  journal={Nature},
  volume={575},
  pages={607--617},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{Schuman2017survey,
  author    = {Catherine D. Schuman,  and others},
  title     = {A Survey of Neuromorphic Computing and Neural Networks in Hardware},
  journal   = {arXiv:1705.06963},
  year      = {2017},
  archivePrefix = {arXiv},
  eprint    = {1705.06963},

  timestamp = {Mon, 13 Aug 2018 16:48:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchumanPPBDRP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Bouvier2019spiking,
 author = {Bouvier, Maxence and others},
 title = {Spiking Neural Networks Hardware Implementations and Challenges: A Survey},
 journal = {J. Emerg. Technol. Comput. Syst.},
 issue_date = {April 2019},
 volume = {15},
 number = {2},
 month = apr,
 year = {2019},
 issn = {1550-4832},
 pages = {22:1--22:35},
 articleno = {22},
 numpages = {35},
 doi = {10.1145/3304103},
 acmid = {3304103},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Neuromorphic computing, event driven, hardware, hardware implementation, machine learning, neural network, neuromorphic computing, spiking, spiking neural networks},
}

@article {Merolla2014,
	author = {Merolla, Paul A. and others},
	title = {A million spiking-neuron integrated circuit with a scalable communication network and interface},
	volume = {345},
	number = {6197},
	pages = {668--673},
	year = {2014},
	doi = {10.1126/science.1254642},
	publisher = {American Association for the Advancement of Science},
	journal = {Science},
	abstract = {Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power.Science, this issue p. 668 Inspired by the brain{\textquoteright}s structure, we have developed an efficient, scalable, and flexible non{\textendash}von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
	issn = {0036-8075},
	eprint = {http://science.sciencemag.org/content/345/6197/668.full.pdf},
}

@ARTICLE{Davies2018loihi,
author={M. Davies and others},
journal={IEEE Micro},
title={Loihi: A Neuromorphic Manycore Processor with On-Chip Learning},
year={2018},
volume={38},
number={1},
pages={82-99},
keywords={circuit optimisation;integrated circuit modelling;learning (artificial intelligence);microprocessor chips;multiprocessing systems;neural chips;spike-based computation;CPU iso-process-voltage-area;magnitude superior energy-delay-product;LASSO optimization problems;locally competitive algorithm;hierarchical connectivity;dendritic compartments;synaptic delays;programmable synaptic learning rules;spiking neural networks;Intels process;on-chip learning;neuromorphic manycore processor;Loihi;size 14 nm;Neurons;Computer architecture;Computational modeling;Neuromorphics;Biological neural networks;Algorithm design and analysis;neuromorphic computing;machine learning;artificial intelligence},
doi={10.1109/MM.2018.112130359},
ISSN={0272-1732},
month={January},
}

@ARTICLE{Moradi2018scalable,
author={S. {Moradi} and N. {Qiao} and F. {Stefanini} and G. {Indiveri}},
journal={IEEE Transactions on Biomedical Circuits and Systems},
title={A Scalable Multicore Architecture With Heterogeneous Memory Structures for Dynamic Neuromorphic Asynchronous Processors ({DYNAP}s)},
year={2018},
volume={12},
number={1},
pages={106-122},
keywords={asynchronous circuits;image sensors;medical computing;neural nets;neurophysiology;real-time systems;heterogeneous memory structures;scalable multicore architecture;routing methodology;event-based neural network architectures;multicore neuromorphic processor chip;hybrid analog-digital circuits;neuron dynamics;asynchronous digital circuits;address-event traffic;convolutional neural network;real-time classification;dynamic vision sensor;dynamic neuromorphic asynchronous processors;Neurons;Routing;Memory management;Neuromorphics;Asynchronous;circuits and systems;neuromorphic computing;routing architectures},
doi={10.1109/TBCAS.2017.2759700},
ISSN={1932-4545},
}

@ARTICLE{Qiao2015reconfigurable,
AUTHOR={Qiao, Ning and others},
TITLE={A reconfigurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128K synapses},
JOURNAL={Frontiers in Neuroscience},
VOLUME={9},
number={141},
pages={1-17},
YEAR={2015},
DOI={10.3389/fnins.2015.00141},
ISSN={1662-453X},
ABSTRACT={Implementing compact, low-power artificial neural processing systems with real-time on-line learning abilities is still an open  challenge. In this paper we present a full-custom mixed-signal VLSI device with neuromorphic learning circuits that emulate the biophysics of real spiking neurons and dynamic synapses for exploring the properties of computational neuroscience models and for building brain-inspired computing systems. The proposed architecture allows the on-chip configuration of a wide range of network connectivities, including recurrent and deep networks with short-term and long-term plasticity. The device comprises 128 K analog synapse and 256 neuron circuits with biologically plausible dynamics and bi-stable spike-based plasticity mechanisms that endow it with on-line learning abilities. In addition to the analog circuits, the device comprises also asynchronous digital logic circuits for setting different synapse and neuron properties as well as different network configurations. This prototype device, fabricated using a 180 nm 1P6M CMOS process, occupies an area of 51.4 mm 2 , and consumes approximately 4 mW for typical experiments, for example involving attractor networks. Here we describe the details of the overall architecture and of the individual circuits and present experimental results that showcase its potential. By supporting a wide range of cortical-like computational modules comprising plasticity mechanisms, this device will enable the realization of intelligent autonomous systems with on-line learning capabilities.}
}

@inproceedings{Huh2018gradient_old,
author = {Huh, Dongsung and Sejnowski, Terrence J},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1433--1443},
title = {Gradient Descent for Spiking Neural Networks},
volume = {31},
year = {2018}
}


@inproceedings{Huh2018gradient,
author = {Huh, Dongsung and Sejnowski, Terrence J.},
title = {Gradient Descent for Spiking Neural Networks},
year = {2018},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {1440–-1450},
numpages = {11},
location = {Montr\'{e}al, Canada},
}

@inproceedings{Jin2018hybrid_old,
author = {Jin, Yingyezhe and Zhang, Wenrui and Li, Peng},
booktitle = {Advances in Neural Information Processing Systems},
pages = {7005--7015},
title = {Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks},
volume = {31},
year = {2018}
}


@inproceedings{Jin2018hybrid,
author = {Jin, Yingyezhe and Zhang, Wenrui and Li, Peng},
title = {Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks},
year = {2018},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7005–-7015},
numpages = {11},
}


@article{Zhang2020temporal,
title={Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks},
author={W. Zhang and Li, Peng},
journal={arXiv:2002.10085},
year={2020}
}

@article{Bohte2002error,
title = "Error-backpropagation in temporally encoded networks of spiking neurons",
journal = "Neurocomputing",
volume = "48",
number = "1",
pages = "17-37",
year = "2002",
issn = "0925-2312",
doi = "https://doi.org/10.1016/S0925-2312(01)00658-0",
author = "Sander M. Bohte and Joost N. Kok and Han La Poutré",
keywords = "Spiking neurons, Temporal coding, Error-backpropagation",
abstract = "For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional error-backpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we find that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations."
}

@ARTICLE{Mostafa2018supervised, 
author={H. {Mostafa}}, 
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title={Supervised Learning Based on Temporal Coding in Spiking Neural Networks}, 
year={2018}, 
volume={29}, 
number={7}, 
pages={3227-3235}, 
keywords={feedforward neural nets;gradient methods;learning (artificial intelligence);network input-output relation;gradient descent training techniques;artificial neural networks;spike generation hard nonlinearity;feedforward spiking network;spiking neural networks;temporal coding;ANN;MNIST;Neurons;Training;Encoding;Biological neural networks;Feedforward neural networks;Nonhomogeneous media;Backpropagation;Backpropagation;spiking networks;supervised learning}, 
doi={10.1109/TNNLS.2017.2726060}, 
ISSN={2162-237X}, 
}

@article{Kheradpisheh2019s4nn,
author = {S. R. Kheradpisheh and  Masquelier, Timothée},
title = {Temporal Backpropagation for Spiking Neural Networks with One Spike per Neuron},
journal = {International Journal of Neural Systems},
volume = {30},
number = {6},
pages = {2050027},
year = {2020},
}


@INPROCEEDINGS{Comsa2020temporal,
author={I. M. Comsa and others },
booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function}, 
year={2020},
volume={},
number={},
pages={8529-8533},}



@ARTICLE{Comsa2021temporal,
  author={Comşa, Iulia-Maria and Potempa, Krzysztof and Versari, Luca and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala, Jyrki},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Temporal Coding in Spiking Neural Networks With Alpha Synaptic Function: Learning With Backpropagation}, 
  year={2022},
  volume={33},
  number={10},
  pages={5939-5952},
  doi={10.1109/TNNLS.2021.3071976}}


@ARTICLE{Sakemi2020supervised,
  author={Sakemi, Yusuke and Morino, Kai and Morie, Takashi and Aihara, Kazuyuki},
  journal={IEEE Transactions on Neural Networks and Learning Systems (Early Access)}, 
  title={A Supervised Learning Algorithm for Multilayer Spiking Neural Networks Based on Temporal Coding Toward Energy-Efficient {VLSI} Processor Design}, 
  year={2021},
  volume={},
  number={},
  pages={1-15},
  doi={10.1109/TNNLS.2021.3095068}
}

@ARTICLE{Sakemi2023supervised,
  author={Sakemi, Yusuke and Morino, Kai and Morie, Takashi and Aihara, Kazuyuki},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Supervised Learning Algorithm for Multilayer Spiking Neural Networks Based on Temporal Coding Toward Energy-Efficient VLSI Processor Design}, 
  year={2023},
  volume={34},
  number={1},
  pages={394-408},
  doi={10.1109/TNNLS.2021.3095068}
}




@article{Sakemi2021effects,
      title={Effects of {VLSI} Circuit Constraints on Temporal-Coding Multilayer Spiking Neural Networks}, 
      author={Yusuke Sakemi and Takashi Morie and Takeo Hosomi and Kazuyuki Aihara},
      year={2021},
      journal={arXiv:2106.10382},
}



@ARTICLE{Zhang2021rectified,
  author={Zhang, Malu and Wang, Jiadong and Wu, Jibin and Belatreche, Ammar and Amornpaisannon, Burin and Zhang, Zhixuan and Miriyala, Venkata Pavan Kumar and Qu, Hong and Chua, Yansong and Carlson, Trevor E. and Li, Haizhou},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Rectified Linear Postsynaptic Potential Function for Backpropagation in Deep Spiking Neural Networks}, 
  year={2022},
  volume={33},
  number={5},
  pages={1947-1958},
  doi={10.1109/TNNLS.2021.3110991}}




@article{Goltz2021fast,
  title={Fast and energy-efficient neuromorphic deep learning with first-spike times},
  author={G{\"o}ltz, J and others},
  journal={Nature Machine Intelligence},
  volume={3},
  pages={823--835},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{Frenkel2021bottomup,
      title={Bottom-Up and Top-Down Neural Processing Systems Design: Neuromorphic Intelligence as the Convergence of Natural and Artificial Intelligence}, 
      author={Charlotte Frenkel and David Bol and Giacomo Indiveri},
      year={2021},
      journal={arXiv:2106.01288},
}

@article{Cai2019fully,
title={A fully integrated reprogrammable memristor--{CMOS} system for efficient multiply--accumulate operations},
author={Cai, Fuxi  and others},
journal={Nature Electronics},
volume={2},
pages={290-299},
year={2019},
month={July},
publisher={Nature Publishing Group}
}

@ARTICLE{Bavandpour2019energy,
author={M. {Bavandpour} and M. R. {Mahmoodi} and D. B. {Strukov}},
journal={IEEE Transactions on Circuits and Systems II: Express Briefs},
title={Energy-Efficient Time-Domain Vector-by-Matrix Multiplier for Neurocomputing and Beyond},
year={2019},
volume={66},
number={9},
pages={1512-1516},
keywords={Time-Domain Computing;Floating Gate Memory;Vector Matrix Multiplication;Neuromorphic Computing.},
doi={10.1109/TCSII.2019.2891688},
ISSN={1549-7747},
}

@ARTICLE{Saito2020igzo,  
author={Saito, D. and others},  
journal={IEEE Transactions on Electron Devices},   
title={{IGZO}-Based Compute Cell for Analog In-Memory Computing—DTCO Analysis to Enable Ultralow-Power {AI} at Edge},   
year={2020},  
volume={67},  
number={11},  
pages={4616-4620},  
doi={10.1109/TED.2020.3025986}}


@ARTICLE{Yamaguchi2021energy,
author={M. {Yamaguchi} and G. {Iwamoto} and Y. {Nishimura} and H. {Tamukoh} and T. {Morie}},
journal={IEEE Access}, 
title={An Energy-Efficient Time-Domain Analog {CMOS} {BinaryConnect} Neural Network Processor Based on a Pulse-Width Modulation Approach}, 
year={2021},
volume={9},
number={},
pages={2644-2654},
doi={10.1109/ACCESS.2020.3047619}}

@ARTICLE{Verma2019inmemory, 
author={N. Verma and others}, 
journal={IEEE Solid-State Circuits Magazine}, 
title={In-Memory Computing: Advances and prospects}, 
year={2019}, 
volume={11}, 
number={3}, 
pages={43-55}, 
keywords={electronic engineering computing;SRAM chips;potential gains IMC;computing stack;integrative research;disruptive approach;moving data;in-memory computing;Signal to noise ratio;Arrays;Memory management;Throughput;Measurement;Energy efficiency}, 
doi={10.1109/MSSC.2019.2922889}, 
ISSN={}, 
}

@INPROCEEDINGS{Roy2020inmemory,  
author={Roy, Kaushik and Chakraborty, Indranil and Ali, Mustafa and Ankit, Aayush and Agrawal, Amogh},  
booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},   
title={In-Memory Computing in Emerging Memory Technologies for Machine Learning: An Overview},   
year={2020},  
volume={},  
number={},  
pages={1-6},  
doi={10.1109/DAC18072.2020.9218505}
}

@ARTICLE{Valavi2019tile,
  author={Valavi, Hossein and Ramadge, Peter J. and Nestler, Eric and Verma, Naveen},
  journal={IEEE Journal of Solid-State Circuits}, 
  title={A 64-Tile 2.4-Mb In-Memory-Computing CNN Accelerator Employing Charge-Domain Compute}, 
  year={2019},
  volume={54},
  number={6},
  pages={1789-1799},
  doi={10.1109/JSSC.2019.2899730}}


@ARTICLE{Liu2014ReRAM,  
author={Liu, Tz-yi and others},  
journal={IEEE Journal of Solid-State Circuits},   
title={A 130.7-$\hbox{mm}^{2}$  2-Layer 32-Gb {ReRAM} Memory Device in 24-nm Technology},   
year={2014},  
volume={49},  
number={1},  
pages={140-153},  
doi={10.1109/JSSC.2013.2280296}
}

@ARTICLE{Zhou2021high,  
author={Zhou, Keji and others},  
journal={IEEE Journal of Solid-State Circuits},   
title={High-Density 3-{D} Stackable Crossbar {2D2R} nv{TCAM} With Low-Power Intelligent Search for Fast Packet Forwarding in 5{G} Applications},   
year={2021},  
volume={56},  
number={3},  
pages={988-1000},  
doi={10.1109/JSSC.2020.3025756}
}

@ARTICLE{Marinella2018multi, 
author={M. J. Marinella and others}, 
journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems}, 
title={Multiscale Co-Design Analysis of Energy, Latency, Area, and Accuracy of a {ReRAM} Analog Neural Training Accelerator}, 
year={2018}, 
volume={8}, 
number={1}, 
pages={86-101}, 
keywords={integrated circuit design;neural nets;resistive RAM;SRAM chips;ReRAM analog neural training accelerator;neural networks;natural language processing;pattern recognition;deep networks;CMOS scaling;analog resistive memory;device-level analysis;static random access memory operations;SRAM;Training;Biological neural networks;Kernel;Laboratories;Phase change random access memory;Algorithm design and analysis;Neural network training;ReRAM;accelerators}, 
doi={10.1109/JETCAS.2018.2796379}, 
ISSN={2156-3357}, 
}

@ARTICLE{Lecun1998gradient, 
author={Y. {LeCun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}}, 
journal={Proceedings of the IEEE}, 
title={Gradient-based learning applied to document recognition}, 
year={1998}, 
volume={86}, 
number={11}, 
pages={2278-2324}, 
keywords={optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis}, 
doi={10.1109/5.726791}, 
ISSN={0018-9219}, 
}

@article{Xiao2017fashion,
author       = {H. Xiao and Kashif Rasul and Roland Vollgraf},
title        = {Fashion-{MNIST}: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
date         = {2017-08-28},
year         = {2017},
journal={arXiv:1708.07747},
}

@article{Zhang2019tdsnn, 
title={{TDSNN}: From Deep Neural Networks to Deep Spike Neural Networks with Temporal-Coding}, 
volume={33}, 
DOI={10.1609/aaai.v33i01.33011319}, 
number={01}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Zhang, Lei and Zhou, Shengyuan and Zhi, Tian and Du, Zidong and Chen, Yunji}, 
year={2019}, 
pages={1319-1326} }


@INPROCEEDINGS{Park2020t2fsnn,
  author={Park, Seongsik and Kim, Seijoon and Na, Byunggook and Yoon, Sungroh},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)}, 
  title={{T2FSNN}: Deep Spiking Neural Networks with Time-to-first-spike Coding}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/DAC18072.2020.9218689}}

@article{Park2021training,
      title={Training Energy-Efficient Deep Spiking Neural Networks with Time-to-First-Spike Coding}, 
      author={Seongsik Park and Sungroh Yoon},
      year={2021},
      journal={arXiv:2106.02568},
}


@article{Stockl2021optimized,
  title={Optimized spiking neurons can classify images with high accuracy through temporal coding with two spikes},
  author={St{\"o}ckl, Christoph and Maass, Wolfgang},
  journal={Nature Machine Intelligence},
  volume={3},
  pages={230--238},
  year={2021},
}

@article {Sun2019solving,
	author = {Sun, Zhong and others},
	title = {Solving matrix equations in one step with cross-point resistive arrays},
	volume = {116},
	number = {10},
	pages = {4123--4128},
	year = {2019},
	doi = {10.1073/pnas.1815682116},
	publisher = {National Academy of Sciences},
	abstract = {Linear algebra is involved in virtually all scientific and engineering disciplines, e.g., physics, statistics, machine learning, and signal processing. Solving matrix equations such as a linear system or an eigenvector equation is accomplished by matrix factorizations or iterative matrix multiplications in conventional computers, which is computationally expensive. In-memory computing with analog resistive memories has shown high efficiencies of time and energy, through realizing matrix-vector multiplication in one step with Ohm{\textquoteright}s law and Kirchhoff{\textquoteright}s law. However, solving matrix equations in a single operation remains an open challenge. Here, we show that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations, matrix eigenvectors, and differential equations in just one step.Conventional digital computers can execute advanced operations by a sequence of elementary Boolean functions of 2 or more bits. As a result, complicated tasks such as solving a linear system or solving a differential equation require a large number of computing steps and an extensive use of memory units to store individual bits. To accelerate the execution of such advanced tasks, in-memory computing with resistive memories provides a promising avenue, thanks to analog data storage and physical computation in the memory. Here, we show that a cross-point array of resistive memory devices can directly solve a system of linear equations, or find the matrix eigenvectors. These operations are completed in just one single step, thanks to the physical computing with Ohm{\textquoteright}s and Kirchhoff{\textquoteright}s laws, and thanks to the negative feedback connection in the cross-point circuit. Algebraic problems are demonstrated in hardware and applied to classical computing tasks, such as ranking webpages and solving the Schr{\"o}dinger equation in one step.},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{Ambrogio2018equivalent,
  title={Equivalent-accuracy accelerated neural-network training using analogue memory},
  author={Ambrogio, Stefano and others},
  journal={Nature},
  volume={558},
  pages={60--67},
  year={2018},
  publisher={Nature Publishing Group}
}

@INPROCEEDINGS{Chakraborty2020geniex,  
author={Chakraborty, Indranil and others},  
booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},   
title={{GENIEx}: A Generalized Approach to Emulating Non-Ideality in Memristive Xbars using Neural Networks},   
year={2020},  
volume={},  
number={},  
pages={1-6},  
doi={10.1109/DAC18072.2020.9218688}
}

@ARTICLE{Bhattacharjee2021neat,
  author={Bhattacharjee, Abhiroop and Bhatnagar, Lakshya and Kim, Youngeun and Panda, Priyadarshini},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (Early Access)}, 
  title={{NEAT}: Non-linearity Aware Training for Accurate, Energy-Efficient and Robust Implementation of Neural Networks on 1{T}-1{R} Crossbars}, 
  year={2021},
  volume={},
  number={},
  doi={10.1109/TCAD.2021.3109857}}

@INPROCEEDINGS{Hayakawa2015highly,
  author={Hayakawa, Y. and others},
  booktitle={2015 Symposium on VLSI Technology (VLSI Technology)}, 
  title={Highly reliable {TaOx} {ReRAM} with centralized filament for 28-nm embedded application}, 
  year={2015},
  volume={},
  number={},
  pages={T14-T15},
  doi={10.1109/VLSIT.2015.7223684}}


@inproceedings{Ankit2019puma,
author = {Ankiti, Aayush and others},
title = {PUMA: A Programmable Ultra-Efficient Memristor-Based Accelerator for Machine Learning Inference},
year = {2019},
isbn = {9781450362405},
doi = {10.1145/3297858.3304049},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {715–-731},
numpages = {17},
}

@article{Yamaguchi2019energy,
  author    = {Masatoshi Yamaguchi and
               Goki Iwamoto and
               Hakaru Tamukoh and
               Takashi Morie},
  title     = {An Energy-efficient Time-domain Analog {VLSI} Neural Network Processor
               Based on a Pulse-width Modulation Approach},
  journal   = {arXiv:1902.07707},
  year      = {2019},
  archivePrefix = {arXiv},
  eprint    = {1902.07707},
  timestamp = {Mon, 04 Mar 2019 15:54:33 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-07707},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{Long2019design,
  author={Long, Yun and She, Xueyuan and Mukhopadhyay, Saibal},
  booktitle={2019 Design, Automation   Test in Europe Conference   Exhibition (DATE)}, 
  title={Design of Reliable {DNN} Accelerator with Un-reliable {ReRAM}}, 
  year={2019},
  volume={},
  number={},
  pages={1769-1774},
  doi={10.23919/DATE.2019.8715178}}

@article{Sebastian2020memory,
  title={Memory devices and applications for in-memory computing},
  author={Sebastian, Abu and Le Gallo, Manuel and Khaddam-Aljameh, Riduan and Eleftheriou, Evangelos},
  journal={Nature Nanotechnology},
  volume={15},
  pages={529--544},
  year={2020},
  publisher={Nature Publishing Group}
}





NOT CHECKED



























@InProceedings{Wang2016,
author={Wang, Quan
and Tamukoh, Hakaru
and Morie, Takashi},
title={Time-Domain Weighted-Sum Calculation for Ultimately Low Power {VLSI} Neural Networks},
booktitle={Neural Information Processing},
year={2016},
month={September},
volume={9947},
pages={240--247},
abstract={Time-domain weighted-sum operation based on a spiking neuron model is discussed and evaluated from a VLSI implementation point of view. This calculation model is useful for extremely low-power operation because transition states in resistance and capacitance (RC) circuits can be used. Weighted summation is achieved with energy dissipation on the order of 1 fJ using the current CMOS VLSI technology if 1 G{\$}{\$}{\backslash}varOmega {\$}{\$}order resistance can be used, where the number of inputs can be more than a hundred. This amount of energy is several orders of magnitude lower than that in conventional digital processors. In this paper, we show the software simulation results that verify the proposed calculation method for a 500-input neuron in a three-layer perceptron for digit character recognition.},
isbn={978-3-319-46687-3}
}

@article{Portelli2016rank,
  title={Rank order coding: a retinal information decoding strategy revealed by large-scale multielectrode array retinal recordings},
  author={Portelli, Geoffrey and Barrett, John M and Hilgen, Gerrit and Masquelier, Timoth{\'e}e and Maccione, Alessandro and Di Marco, Stefano and Berdondini, Luca and Kornprobst, Pierre and Sernagor, Evelyne},
  journal={e{N}euro},
  volume={3},
  number={3},
  pages={e0134},
  year={2016},
  publisher={Society for Neuroscience}
}

@article{Gollisch2008rapid,
author = {Tim Gollisch  and Markus Meister },
title = {Rapid Neural Coding in the Retina with Relative Spike Latencies},
journal = {Science},
volume = {319},
number = {5866},
pages = {1108-1111},
year = {2008},
doi = {10.1126/science.1149639},
URL = {https://www.science.org/doi/abs/10.1126/science.1149639},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1149639},
}


@article{Auge2021survey,
  title={A survey of encoding techniques for signal processing in spiking neural networks},
  author={Auge, Daniel and Hille, Julian and Mueller, Etienne and Knoll, Alois},
  journal={Neural Processing Letters},
  volume={53},
  pages={4693--4710},
  year={2021},
  publisher={Springer}
}


@INPROCEEDINGS{Hassan2017hybrid,
author={A. M. {Hassan} and C. {Yang} and C. {Liu} and H. H. {Li} and Y. {Chen}},
booktitle={Design, Automation   Test in Europe Conference   Exhibition (DATE)}, 
title={Hybrid spiking-based multi-layered self-learning neuromorphic system based on memristor crossbar arrays}, 
year={2017},
volume={},
number={},
pages={776-781}
}


@INPROCEEDINGS{Narayanan2017inxs,
author={S. {Narayanan} and A. {Shafiee} and R. {Balasubramonian}},
booktitle={International Joint Conference on Neural Networks (IJCNN)}, 
title={{INXS}: Bridging the throughput and energy gap for spiking neural networks}, 
year={2017},
volume={},
number={},
 pages={2451-2459},
 }

@article{Nandakumar2019supervised,
  title={Experimental demonstration of supervised learning in spiking neural networks with phase-change memory synapses},
  author={S. R. Nandakumar and Boybat, Irem and Le Gallo, Manuel and Eleftheriou, Evangelos and Sebastian, Abu and Rajendran, Bipin},
  journal={Scientific Reports},
  volume={10},
  number={8080},
  year={2020},
}

@INPROCEEDINGS{Liu2015spiking,
author={C. {Liu, \textit{et al.}}},
booktitle={52nd ACM/EDAC/IEEE Design Automation Conference (DAC)}, 
title={A spiking neuromorphic design with resistive crossbar}, 
year={2015},
volume={},
number={},
pages={1-6}
}





@ARTICLE{Sun20192019,  
author={Sun, Xiaoyu and Yu, Shimeng},  
journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},   
title={Impact of Non-Ideal Characteristics of Resistive Synaptic Devices on Implementing Convolutional Neural Networks},   
year={2019},  
volume={9},  
number={3},  
pages={570-579},  
doi={10.1109/JETCAS.2019.2933148}
}





@article{Sebastian2020memory,
  title={Memory devices and applications for in-memory computing},
  author={Sebastian, Abu and Le Gallo, Manuel and Khaddam-Aljameh, Riduan and Eleftheriou, Evangelos},
  journal={Nature nanotechnology},
  volume={15},
  number={7},
  pages={529--544},
  year={2020},
  publisher={Nature Publishing Group}
}


@article{Berdan2020low,
  title={Low-power linear computation using nonlinear ferroelectric tunnel junction memristors},
  author={Berdan, Radu and Marukame, Takao and Ota, Kensuke and Yamaguchi, Marina and Saitoh, Masumi and Fujii, Shosuke and Deguchi, Jun and Nishi, Yoshifumi},
  journal={Nature Electronics},
  volume={3},
  number={5},
  pages={259--266},
  year={2020},
  publisher={Nature Publishing Group}
}




@article{Nandakumar2020experimental,
  title={Experimental demonstration of supervised learning in spiking neural networks with phase-change memory synapses},
  author={Nandakumar, SR and Boybat, Irem and Le Gallo, Manuel and Eleftheriou, Evangelos and Sebastian, Abu and Rajendran, Bipin},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--11},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{Wang2018time,
  title={A Time-domain Analog Weighted-sum Calculation Model for Extremely Low Power {VLSI} Implementation of Multi-layer Neural Networks},
  author={Wang, Quan and Tamukoh, Hakaru and {Morie}, T},
  journal={arXiv:1810.06819},
  year={2018},
  month={October}
}
@ARTICLE{Kim2021revisiting,
AUTHOR={Kim, Youngeun and Panda, Priyadarshini},   
TITLE={Revisiting Batch Normalization for Training Low-Latency Deep Spiking Neural Networks From Scratch},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={15},       
pages={773954},
YEAR={2021},          
DOI={10.3389/fnins.2021.773954},      
ISSN={1662-453X}
}

@ARTICLE{Bonilla2022analyzing,
AUTHOR={Bonilla, Lina and Gautrais, Jacques and Thorpe, Simon and Masquelier, Timothée},   
TITLE={Analyzing time-to-first-spike coding schemes: A theoretical approach},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={16},      
pages={971937},
YEAR={2022},      
}

@article{Kheradpisheh2018stdp,
title = {STDP-based spiking deep convolutional neural networks for object recognition},
journal = {Neural Networks},
volume = {99},
pages = {56-67},
year = {2018},
author = {Saeed Reza Kheradpisheh and Mohammad Ganjtabesh and Simon J. Thorpe and Timothée Masquelier},
keywords = {Spiking neural network, STDP, Deep learning, Object recognition, Temporal coding},
}

@article{Yin2021accurate,
  title={Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks},
  author={Yin, Bojian and Corradi, Federico and Boht{\'e}, Sander M},
  journal={Nature Machine Intelligence},
  volume={3},
  number={10},
  pages={905--913},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@ARTICLE{Neftci2017event,
  AUTHOR={Neftci, Emre O. and Augustine, Charles and Paul, Somnath and Detorakis, Georgios},   
	 TITLE={Event-Driven Random Back-Propagation: Enabling Neuromorphic Deep Learning Machines},      
  JOURNAL={Frontiers in Neuroscience},      
  VOLUME={11},           
pages={324},
  YEAR={2017},      
}

@ARTICLE{Wu2018spatio,
 AUTHOR={Wu, Yujie and Deng, Lei and Li, Guoqi and Zhu, Jun and Shi, Luping},   
 TITLE={Spatio-Temporal Backpropagation for Training High-Performance Spiking Neural Networks},      
 JOURNAL={Frontiers in Neuroscience},      
 VOLUME={12},           
 YEAR={2018},      
 pages={331},
 URL={https://www.frontiersin.org/articles/10.3389/fnins.2018.00331},       
 DOI={10.3389/fnins.2018.00331},      
 ISSN={1662-453X},   
}

@article{Zenke2018super,
    author = {Zenke, Friedemann and Ganguli, Surya},
    title = "{SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks}",
    journal = {Neural Computation},
    volume = {30},
    number = {6},
    pages = {1514-1541},
    year = {2018},
    issn = {0899-7667},
    doi = {10.1162/neco_a_01086},
    url = {https://doi.org/10.1162/neco\_a\_01086},
    eprint = {https://direct.mit.edu/neco/article-pdf/30/6/1514/1039264/neco\_a\_01086.pdf},
}





@ARTICLE{Guo2021neural,
AUTHOR={Guo, Wenzhe and Fouda, Mohammed E. and Eltawil, Ahmed M. and Salama, Khaled Nabil},   
TITLE={Neural Coding in Spiking Neural Networks: A Comparative Study for Robust Neuromorphic Systems},    
JOURNAL={Frontiers in Neuroscience},      
VOLUME={15},           
YEAR={2021},      
pages={638474},
}


@article{Oh2022neuron,
  title={Neuron Circuits for Low-Power Spiking Neural Networks Using Time-To-First-Spike Encoding},
  author={Oh, Seongbin and Kwon, Dongseok and Yeom, Gyuho and Kang, Won-Mook and Lee, Soochang and Woo, Sung Yun and Kim, Jaehyeon and Lee, Jong-Ho},
  journal={IEEE Access},
  volume={10},
  pages={24444--24455},
  year={2022},
  publisher={IEEE}
}













@ARTICLE{Zhou2019edge,
author={Z. {Zhou} and X. {Chen} and E. {Li} and L. {Zeng} and K. {Luo} and J. {Zhang}},
journal={Proceedings of the IEEE}, 
title={Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing}, 
year={2019},
volume={107},
number={8},
pages={1738-1762},}




@ARTICLE{Shi2016edge,
author={W. {Shi} and J. {Cao} and Q. {Zhang} and Y. {Li} and L. {Xu}},
journal={IEEE Internet of Things Journal}, 
title={Edge Computing: Vision and Challenges}, 
year={2016},
volume={3},
number={5},
pages={637-646},}





@INPROCEEDINGS{Ruechauer2018conversion,  
author={Rueckauer, Bodo and Liu, Shih-Chii},  
booktitle={2018 IEEE International Symposium on Circuits and Systems (ISCAS)},   
title={Conversion of analog to spiking neural networks using sparse temporal coding},   
year={2018},  
volume={},  
number={},  
pages={1-5},  
doi={10.1109/ISCAS.2018.8351295}}














@article{Oh2020hardware,
title={Hardware Implementation of Spiking Neural Networks Using Time-To-First-Spike Encoding}, 
author={Seongbin Oh and Dongseok Kwon and Gyuho Yeom and Won-Mook Kang and Soochang Lee and Sung Yun Woo and Jang Saeng Kim and Min Kyu Park and Jong-Ho Lee},
year={2020},
journal={arXiv:2006.05033},
}





























@ARTICLE{Ruechauer2017conversion,
AUTHOR={Rueckauer, Bodo and Lungu, Iulia-Alexandra and Hu, Yuhuang and Pfeiffer, Michael and Liu, Shih-Chii},   
TITLE={Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven Networks for Image Classification},      
JOURNAL={Frontiers in Neuroscience},      
VOLUME={11},      
PAGES={682},     
YEAR={2017},      
URL={https://www.frontiersin.org/article/10.3389/fnins.2017.00682},       
DOI={10.3389/fnins.2017.00682},      
ISSN={1662-453X},   
ABSTRACT={Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and Inception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.}
}


@inproceedings{Esser2015energy,
author = {Esser, Steve K and Appuswamy, Rathinakumar and Merolla, Paul and Arthur, John V. and Modha, Dharmendra S},
booktitle = {Advances in Neural Information Processing Systems},
pages = {1117--1125},
title = {Backpropagation for Energy-Efficient Neuromorphic Computing},
volume = {28},
year = {2015}
}



@INPROCEEDINGS{Diehl2015fast,
author={P. U. {Diehl} and D. {Neil} and J. {Binas} and M. {Cook} and S. {Liu} and M. {Pfeiffer}},
booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
title={Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing},
year={2015},
volume={},
number={},
pages={1-8},
keywords={neural nets;pattern classification;threshold balancing;weight balancing;spiking neural networks;deep ANN;spiking neurons;optimization techniques;ConvNets;fully connected deep networks;MNIST database;rectified linear units;ReLU;weight normalization method;firing rates;SNN;low-latency classification;spiking deep networks;convolutional neural networks;Neuromorphics;Handheld computers;Neurons;Robustness;Accuracy},
doi={10.1109/IJCNN.2015.7280696},
ISSN={2161-4407},
month={July},}


@inproceedings{Kim2020spiking,
title={Spiking-{YOLO}: spiking neural network for energy-efficient object detection},
author={Kim, Seijoon and Park, Seongsik and Na, Byunggook and Yoon, Sungroh},
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
volume={34},
number={07},
pages={11270--11277},
year={2020}
}





















@ARTICLE{Sze2017, 
author={V. {Sze} and Y. {Chen} and T. {Yang} and J. S. {Emer}}, 
journal={Proceedings of the IEEE}, 
title={Efficient Processing of Deep Neural Networks: A Tutorial and Survey}, 
year={2017}, 
volume={105}, 
number={12}, 
pages={2295-2329}, 
keywords={artificial intelligence;computational complexity;neural nets;energy efficiency;hardware design changes;DNN hardware designs;deep neural networks;hardware cost;computation cost reduction;artificial intelligence;computational complexity;hardware platforms;hardware architecture;DNN hardware implementations;Neurons;Biological neural networks;Artificial intelligence;Machine learning;Neural networks;Tutorials;Convolutional neural networks;Artificial intelligence;Benchmark testing;Computer architecture;ASIC;computer architecture;convolutional neural networks;dataflow processing;deep learning;deep neural networks;energy-efficient accelerators;low power;machine learning;spatial architectures;VLSI}, 
doi={10.1109/JPROC.2017.2761740}, 
ISSN={0018-9219}, 
month={Dec},}
























@ARTICLE{Taherkhani2018supervised,
author={A. {Taherkhani} and A. {Belatreche} and Y. {Li} and L. P. {Maguire}},
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title={A Supervised Learning Algorithm for Learning Precise Timing of Multiple Spikes in Multilayer Spiking Neural Networks}, 
year={2018},
volume={29},
number={11},
pages={5394-5407},
}



