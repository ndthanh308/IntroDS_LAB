\section{Extension: Leader is not Perfectly Informed}
\subsection{Alternative Information Structure} \label{sect_noisy_signaling}


Suppose, now, that instead of perfectly learning the state $\theta$, the leader observes a noisy signal $x_L = \theta + \sigma_L \varepsilon_L$ with $\varepsilon_L$ being a standard Gaussian noise independent of $\theta$ and $\varepsilon_j$ for any $j \in F$. We deem this extension important for three reasons. First, from an applied perspective, it is more natural to assume that the leader observes a noisy signal about the state rather than its true value. Second, we document that multiplicity of rationalizable behavior is not a consequence of the one-sided dominance in the subgames. That is, the reason for multiplicity is not that the action of the leader when she is perfectly informed introduces ``too much'' common knowledge. Rather, the main result of the paper continues to hold with the (noisy) information structure that brings back the standard two-sided dominance. Finally, this extension will show that while it must be the case that the leader is fully informed to obtain efficiency, it is not enough for the leader to be better informed than the followers to obtain unique rationalizable behavior. The requirement that the noise in leader's information is smaller than the one in the followers' (i.e., $\sigma_L < \sigma_F$), is neither necessary nor sufficient for uniqueness.  

\subsection{Analysis and Rationalizable Behavior}


Call $x_L \in X_L = \RR$ the leader's type. A strategy for the leader is now a mapping $s_L: X_L \to \Action_L$. Let $S_L$ denote the strategy space for the leader. The actions and strategies of the followers remain the same as defined in Section \ref{sect_signaling_game}.
Let $\mu_L(\cdot\vl x_L) \in \Delta \left(\Theta \times X \times S \right)$ be leader $x_L$'s belief about the state and the type-strategy pairs of the followers, where $X \times S = \prod_{j\in F} (X_j \times S_j)$. Since there is no learning for the leader, the marginal of $\mu_L(\cdot \vl x_L)$ about $\theta$ has a Gaussian distribution with mean $x_L$ and variance $\sigma_L^2$.
Let $\mu_j(\cdot \vl x_j, h) \in \Delta \left(\Theta \times X_L \times X_{-j} \times S_{-j} \right)$ be follower $j$'s belief about 
the state and the type-strategy pairs of his opponents given type $x_j$ and history $h$, where $X_{-j} \times S_{-j} = \prod_{k \neq j \in F} (X_k \times S_k)$.


For type $x_L$ of the leader, exerting effort is the best response to a belief $\mu_L(\cdot \vl x_L)$ if
\begin{equation*}
    \int_{(\theta, x, s)} u(\theta, A_{-L}(s)) \dd \mu_L(\theta, x, s \vl x_L) > 0,
\end{equation*}
where $A_{-L}(s(\invest)) = \sum_{j \in F} \one(s_j(\invest) = \invest)$.
For a follower with type $x_j$, $s_j(h) = \invest$ is the best response to $\mu_j(\cdot \vl x_j, h)$ when
\begin{equation*}
    \int_{(\theta, x_L, x_{-j}, s_{-j})} u\left(\theta, A_{-j}(h, s_{-j}) \right) \dd \mu(\theta, x_L, x_{-j}, s_{-j} \vl x_j, h) > 0
\end{equation*}
where $A_{-j}(h, s_{-j}(h)) = \chi_\invest + \sum_{k \in F, \,k \neq j} \one(s_k(h) = \invest)$. The initial set of type-strategy pairs for the leader in the definition of $\Delta$-rationalizability is now given by $R_L^0 =X_L \times S_L$.


Suppose that a follower type $x$ believes that the leader uses a monotone strategy with threshold $z$; i.e., $a_L = \invest$ if and only if $x_L > z$. Then, 
upon observing $h = \invest$, type $x$'s interim belief has CDF
\begin{equation} \label{posterior_invest}
    G^\invest(\theta; \,x, z) = \frac{1}{ \Phi\left( \frac{x - z}{\sigma}\right) } \int_{-\infty}^{\theta} \frac{1}{\sigma_F} \phi\left(\frac{t- x}{\sigma_F}\right) \Phi\left(\frac{t - z}{\sigma_L}\right)\dd t,
\end{equation}
where $\sigma^2 = \sigma_F^2 + \sigma_L^2$.
Similarly, type $x$'s interim CDF under history $h = \notinvest$ is
\begin{equation} \label{posterior_not_invest}
    G^\notinvest(\theta; \,x, z) = \frac{1}{\Phi \left(\frac{z - x}{\sigma}\right) } \int_{-\infty}^{\theta}
    \frac{1}{\sigma_F} \phi\left(\frac{t-x}{\sigma_F}\right) \Phi\left(\frac{z - t}{\sigma_L}\right)\dd t.
\end{equation}
It is worth noting that $G^h(\cdot; \,x, z)$, unlike the interim beliefs $\Psi^h(\cdot; \,x, z)$ in the main model, has support over the entire real line. Thus, the subgames no longer feature one-sided dominance in the $\Delta$-rationalizability procedure.

If type $x$ believes further that other followers use monotone strategies with threshold $x_h$ under history $h$, then his payoff to choosing $\invest$ yields
\begin{equation*}
    \pi_F^h(x_j; z, x_h) = \EE_{\theta \sim G^h(\cdot; x, z) } \left[\theta - \frac{n-1}{n} \Phi\left(\frac{x_h - \theta}{\sigma_F}\right) \right]  -\frac{\chi_\notinvest}{n}.
\end{equation*}
We show in Appendix A that $\pi_F^h(x; z, x_h)$ is strictly increasing in $x$ and crosses zero once from below. Furthermore, it is strictly increasing in $z$ and strictly decreasing in $x_h$.
Type $x$'s 
conditional rank belief is given by
\begin{equation} \label{ext_rank_belief}
\begin{cases}
    R^\invest(x; z) = \prob(x_k \leq x_j \vl x_j = x, x_L > z) = \frac{1}{2} - \frac{T\left(\frac{x-z}{\sigma},  
 ~\alpha \right)}{\Phi\left(\frac{x - z}{\sigma}\right)} & \\
    ~ & \\
    R^\notinvest(x; z)  = \prob(x_k \leq x_j \vl x_j = x, x_L \leq z) = \frac{1}{2} + \frac{T\left(\frac{z - x}{\sigma}, ~\alpha \right)}{\Phi\left(\frac{ z - x}{\sigma}\right)} 
\end{cases},
\end{equation}
where $\alpha = \sigma_F/(2\sigma_L^2 + \sigma_F^2)^{1/2}$ and $T(y, a)$ is \textit{Owen's T-function}.\footnote{\label{owen_t}~Owen's T-function, first introduced by \cite{owen_1956}, is defined by 
\[
T(y,a) = \frac{1}{2\pi}\int_0^a \frac{\ee^{-(1+t^2)y^2/2}}{1+t^2} \dd t.
\]
It gives the probability of the event $\{X > y, ~0 < Y < a X \}$ when $X$ and $Y$ are independent standard Gaussian random variables. 
See \cite{savischenko_2014} and \cite{brychkov_savischenko_2016} for an overview of the function.} The derivation of (\ref{ext_rank_belief}) is given in Appendix A. When $x = x_h$, it can be shown that
\begin{equation} \label{ext_follower_fp_eqn}
    \pi_F^h(x_h; z, x_h) = \EE_{\theta \sim G^h(\cdot; \,x_h, z)}[\theta] - \frac{n-1}{n}R^h(x_h; z) - \frac{\chi_\notinvest}{n}.
\end{equation}


Now consider type $x_L$ of the leader. Suppose that leader $x_L$ believes that
followers use monotone strategies with threshold $x_\invest$ under history $h = \invest$. Then her payoff to choosing $\invest$ is
\begin{equation*}
    \pi_L(x_L; y_\invest) = x_L - \Phi\left(\frac{x_\invest -x_L}{\sigma}\right).
\end{equation*}
which is strictly increasing in $x_L$ and strictly decreasing in $x_\invest$.


The $\Delta$-rationalizability procedure again yields six sequences. We prove in Appendix A that $(\xlow^k)_{k=0}^\infty$, $(\xilow^k)_{k=0}^\infty$, and $(\xnlow^k)_{k=0}^\infty$ are increasing and bounded above, and $(\xup^k)_{k=0}^\infty$, $(\xiup^k)_{k=0}^\infty$, and $(\xnup^k)_{k=0}^\infty$ are decreasing and bounded below.
The monotone convergence theorem therefore guarantees that they converge to $\xlow$, $\xilow$, $\xnlow$, $\xup$, $\xiup$, and $\xnup$, respectively. In addition, the limits together solve the following system of equations:
\begin{equation} \label{ext_sys_eqs}
    \begin{cases}
        \pi_L(\xlow; \xilow) = 0 \\
        \pi_L(\xup; \xiup) = 0 \\
        \pi_F^\invest(\xilow; \xup, \xilow) = 0 \\ \pi_F^\invest(\xiup; \xlow, \xiup) = 0 \\
        \pi_F^\notinvest(\xnlow; \xup, \xnlow) = 0 \\
        \pi_F^\notinvest(\xnup; \xlow, \xnup) = 0
    \end{cases}.
\end{equation}

For a given pair of $(\sigma_L, \sigma_F)$, note that we can view it as a point on the ray from the origin with slope $\sigma_F/\sigma_L$. To understand which pair of $(\sigma_L, \sigma_F)$ induces unique $\Delta$-rationalizable behavior, we establish a sufficient condition on each fixed ray $\sigma_F = \gamma \sigma_L$, $\gamma \geq 0$, along which the slope parameter of Owen's T-function is given by $\alpha = \gamma/\sqrt{2 + \gamma^2}$.



\begin{proposition} \label{prop_unique_rat_ext}

The $\Delta$-rationalizable sets are $R_L^\infty = R_L^0 \setminus \overline{R}_L^\infty$ and $R_{F, \,j}^\infty = R_{F, \,j}^0 \setminus \overline{R}_{F, \,j}^\infty$, where
\begin{equation*}
     \overline{R}_L^{\infty} = \left\{(x_L, a_L) \vl~ a_L = \invest ~\text{if}~ x_L \leq \xlow ~\text{and}~ a_L = \notinvest ~\text{if}~ x_L > \xup \right\}
\end{equation*}
and
\begin{equation*}
\overline{R}_{F, \,j}^\infty =  \left\{(x_j, s_j) \vl
\text{$s_j(h) = \invest$ if $x_j \leq \underline{x}_h$ and  $s_j(h) = \notinvest$ if $x_j > \overline{x}_h$, \text{for all}~$h \in \Action_L$} \right\}
\end{equation*} 
Moreover, \\
(i) there exists $\widehat{\sigma}_L(\gamma)$ such that the game has unique $\Delta$-rationalizable behavior if $\sigma_L > \widehat{\sigma}_L(\gamma)$; \\
(ii) in the limit as $\sigma_L \to 0$ (while keeping $\sigma_F/\sigma_L = \gamma$ fixed), the game features multiplicity of $\Delta$-rationalizable behavior.
\end{proposition}


Proposition \ref{prop_unique_rat_ext} is an analog of Proposition \ref{prop_unique_rat}. It establishes that the game generally features multiplicity of rationalizable behavior. In particular, this is necessarily the case, given the ray $\gamma$, in the limit where both noises approach zero in a way that their ratio is always given by $\gamma$. Moreover, for each $\gamma$, one can increase the noises $(\sigma_F,\sigma_L)$ in a way that their ratio is given by $\gamma$ and the game features unique rationalizable behavior. Note, however, that now, even when unique rationalizable play is obtained, the thresholds the agents use depend on the noises $(\sigma_F,\sigma_L)$. This was not the case in our main model. There, as long as $\sigma_F >\widehat{\sigma}_F$, the unique rationalizable play was always the fully efficient one. Finally, the leader having more accurate information than the followers is neither necessary nor sufficient to obtain unique rationalizable behavior, since this can happen irrespective of whether $\gamma$ is greater, equal, or less than one. On the other hand, the leader being arbitrarily better informed than followers is necessary to obtain the efficient outcome.


\subsection{Discussion}

\subsubsection{Signaling Effect, Miscoordination Effect, and Multiplicity}

In a similar spirit to the analysis of the main model, one can analyze the subgame after history $h$ and consider the rationalizable profiles of followers' type-strategy pairs.  Let $z=x_L^* $, that is, $z$ be equal to the leader's threshold in the case where unique rationalizable behavior obtains. Assume that $\widehat{x}$ is the type of follower who is indifferent between choosing $\invest$ and $\notinvest$. One can rewrite Equation \ref{eq_eqm_follower} as
\begin{equation} \label{eq_f_ext}
    \EE_{\underbrace{{\theta \sim G^h(\widehat{x}; z, \widehat{x})}}_\text{signaling effect}} \left[ \theta \right] -  \underbrace{\frac{\chi_\notinvest}{n}}_{\text{externality from leader's action}}= \underbrace{\frac{n-1}{n}R^h(\widehat{x}; z)}_{\text{miss-coordination effect}} 
\end{equation} 

As we stated, in this case, Equation (\ref{eq_f_ext}) has at least one solution. To get the uniqueness of rationalizable play, it must be the case that the derivative of the conditional rank belief function is sufficiently bounded. This is not generally the case, since around $z$, for certain values of $\sigma_L $ and $\sigma_F$\footnote{~In particular, this is necessarily the case in the limit as $\sigma_L\rightarrow 0$ and $\sigma_F \rightarrow 0$ with $\sigma_F/\sigma_L= \gamma$.}, the rank belief function abruptly changes, which means that the expected payoff of the indifferent type of follower $j$ changes sign more than once. The condition of Proposition \ref{prop_unique_rat_ext} ensures that this rapid change is not enough to make the expected payoff of follower $j$ cross the $x$-axis multiple times. Similarly to the main model, if the sign change occurred more than once, the subgame would feature at least two Bayesian Nash equilibria that would correspond to the solutions of Equation \ref{eq_f_ext}. In this case, multiplicity of rationalizable behavior immediately obtains.  Such a case is given in Figure \ref{fig_invest_noisy_multiple}.

% Figure environment removed


\begin{comment}
Notice that the subgame after the leader chooses action $\invest$ has three Bayesian Nash equilibria, in which the followers use thresholds $\xilow<x_{\invest}^* < \xiup$. The leader's best responses are given by $\xlow<x_L^* <\xup$ respectively. Now, every type of leader in $[\xlow,\xup]$ finds both actions rationalizable and the same holds for every type of a follower in $[\xilow,\xiup]$\footnote{The values $(x_L^*,x_{\invest}^*)$ together with the value $x_{\notinvest}^*$ (which is not shown in the graph) correspond to the PBE of the game.}.
\end{comment}



\subsubsection{Inefficiency of the Unique Outcome}
Contrary to the main model, the extension features an inefficient outcome irrespective of the uniqueness of rationalizable play. This result obtains whenever $\sigma_L$ is bounded away from zero. In the limiting case where $\sigma_L \to 0^+$ and for $\sigma_F$ sufficiently large, we recover the unique efficient $\Delta$-rationalizable profile of the main model. It is worth noting, though, that in all cases, the extensive form game leads to outcomes at least as efficient as the ones that would obtain if the game was a simultaneous move game, a prediction consistent with existing literature. This means that the presence of the leader is always helpful, even if her information is very imprecise. This is not surprising, since the leader's action apart from information carries a benefit that spills over to followers.

% \subsubsection{Equilibrium Behavior}

% If the sufficient condition derived in Proposition \ref{prop_unique_rat_ext} is satisfied, the game features unique rationalizable behavior. This immediately implies that the game features unique equilibrium behavior. In particular, the unique rationalizable strategy profile corresponds to the unique Perfect Bayesian Equilibrium of the game, which is in monotone strategies, with thresholds for the leader and followers given by $x_L^*$, $\xistar$ and $\xnstar$. 



\subsection{A Synthesis of the Results}
One can think of the results established in Propositions 1 and 3 in the following way: In the $(\sigma_L , \sigma_F)$ space, when $\sigma_L=0$, Proposition 1 derives a necessary and sufficient condition under which the game features unique rationalizable behavior which delivers the fully efficient outcome. When $\sigma_F=0$, multiplicity immediately obtains since the followers are perfectly informed about the state. In the limit where both noises vanish and $\sigma_F/\sigma_L\to 0$,\footnote{~ This means that followers are arbitrarily better informed than the leader.} the subgame becomes a standard global game, where the leader's action carries only the positive spillover and no information. In this case, the unique rationalizable behavior features a monotone strategy for the leader and the followers with threshold $(n-1)/2n$.  When both $\sigma_L$ and $\sigma_F$ are nonzero and vanish at a rate such that their ratio is given by $\gamma$ for any $\gamma>0$, Proposition 3 establishes the multiplicity of rationalizable behavior in the limit when we move towards the origin along the fixed ray $\gamma$ and the existence of a value $\widehat{\sigma}_L(\gamma)$ such that when one is sufficiently away from the origin the game features unique rationalizable behavior. Thus, in general, the leader cannot ``discipline'' the followers on imitating her behavior, and thus, she may choose the inefficient action in the first place. 












