
\section*{Appendix B: Unique Monotone Equilibrium}
In this appendix, we show that the game has a unique equilibrium when we restrict attention to monotone strategies. Although the result is standard in the literature, it is included for completeness.

Let $s_L: \Theta \to \Action_L$ be the leader's strategy. 
The leader is said to follow a \emph{monotone strategy} if her strategy takes the form:
\begin{equation*}
    s_L(\theta) = \begin{cases}
        \invest & \text{if $\theta > \thetazerohat$} \\
        \notinvest & \text{if $\theta \leq \thetazerohat$}
    \end{cases}.
\end{equation*}
A strategy for any follower $j$ is a mapping $s_j: X_j \times \Action_L \to \Action_j$. Follower $j$'s strategy is monotone if
\begin{equation*}
    s_j(x_j, h) = \begin{cases}
        \invest & \text{if $x_j > \xhhat$} \\
        \notinvest & \text{if $x_j \leq \xhhat$}
    \end{cases}.
\end{equation*}
A \emph{monotone equilibrium} is a symmetric perfect Bayesian equilibrium in monotone strategies with thresholds $(\theta_L^*, x_\invest^*, x_\notinvest^*)$.




\begin{lemma} \label{lemma_monotone_b}
There exists a monotone equilibrium with thresholds $\thetazerostar = 0$, $\xistar = -\infty$, and $\xnstar = \infty$.
\end{lemma}

\begin{proof}
    Fix a follower type $x$. Suppose that the leader uses threshold $\thetazerostar = 0$ and other followers use thresholds $\xistar = -\infty$ and $\xnstar = \infty$. If the leader exerts effort, then type $x$'s payoff yields
\begin{equation*}
    \pi_F^\invest(x; \thetazerostar, \xistar) = \EE_{\theta \sim \Psi^\invest(\cdot; \, x, 0)} \left[\theta\right] > 0.
\end{equation*}
This means that all types $x$ will exert effort under history $h = \invest$. Thus, follower $j$'s best response is a monotone strategy with threshold $\xistar = -\infty$. In contrast, if the leader does not exert effort, then the payoff for type $x$ is
\begin{equation*}
    \pi_F^\notinvest(x; \thetazerostar, \xnstar) = \EE_{\theta \sim \Psi^\notinvest(\cdot; \,x, 0)} [\theta] - 1< 0.
\end{equation*}
Thus, under history, $h = \notinvest$, follower $j$ will best respond by using a monotone strategy with threshold $\xnstar = \infty$.

Consider now type $\theta$ of the leader. Since all followers will invest if they see the leader invests, investing generates a payoff of $\theta$ for type $\theta$. Therefore, type $\theta$ invests if and only if $\theta > 0$. In other words, the leader will best respond by choosing threshold $\thetazerostar = 0$. The proof is complete.
\end{proof}


\begin{proposition}
There is no monotone equilibrium other than the one given in Lemma \ref{lemma_monotone_b}.

\end{proposition}
\begin{proof}
By way of contradiction, suppose that $\thetazerostar$ and $\xistar$ are the equilibrium thresholds. Then they must solve the indifference conditions
\begin{equation} \label{app_b_leader_eq}
    \pi_L(\thetazerostar; \xistar) = \thetazerostar - \Phi\left(\frac{\xistar - \thetazerostar}{\sigma_F}\right) = 0 \tag{B.1}
\end{equation}
and
\begin{equation} \label{app_b_follower_eq}
    \pi_F^\invest(x_\invest^*; \theta_L^*, x_\invest^*) =  \EE_{\theta \sim \Psi^\invest(\cdot; \,\xistar, \thetazerostar)} \left[ \theta - \frac{n-1}{n}\Phi\left(\frac{\xistar - \thetazerostar}{\sigma_F}\right) \right] = 0. \tag{B.2}
\end{equation}
By Equations (\ref{rank_belief}) and (\ref{eq_eqm_follower}) we can write (\ref{app_b_follower_eq}) as
\begin{equation} \label{app_b_follower_eq_2}
    \xistar + \sigma_F \lambda\left(\frac{\xistar - \thetazerostar}{\sigma_F}\right) = \frac{n-1}{2n}\Phi\left(\frac{\xistar - \thetazerostar}{\sigma_F}\right). \tag{B.3}
\end{equation}
Subtracting Equation (\ref{app_b_leader_eq}) from Equation (\ref{app_b_follower_eq_2}) yields
\begin{equation} \label{app_eq_diff}
    \xistar - \thetazerostar + \sigma_F \lambda\left( \frac{\xistar - \thetazerostar}{\sigma_F} \right) = - \frac{n+1}{2n} \Phi\left(\frac{\xistar - \thetazerostar}{\sigma_F}\right). \tag{B.4}
\end{equation}
Note that $x + \lambda(x)$ is increasing in $x$ with $\lim_{x \to -\infty} x + \lambda(x) = 0$, and hence $x + \lambda(x) > 0$ for all $x$. This implies that the left-hand side of (\ref{app_eq_diff}) is positive. But since the right-hand side of (\ref{app_eq_diff}) is negative, this leads to a contradiction.
\end{proof}
