\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em arXiv preprint arXiv:2204.14198}, 2022.

\bibitem{bao2021beit}
Hangbo Bao, Li Dong, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock {\em arXiv preprint arXiv:2106.08254}, 2021.

\bibitem{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3558--3568, 2021.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em IEEE conference on computer vision and pattern recognition},
  pages 248--255. IEEE, 2009.

\bibitem{desai2021virtex}
Karan Desai and Justin Johnson.
\newblock Virtex: Learning visual representations from textual annotations.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 11162--11173, 2021.

\bibitem{dong2022maskclip}
Xiaoyi Dong, Yinglin Zheng, Jianmin Bao, Ting Zhang, Dongdong Chen, Hao Yang,
  Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et~al.
\newblock Maskclip: Masked self-distillation advances contrastive
  language-image pretraining.
\newblock {\em arXiv preprint arXiv:2208.12262}, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fang2022eva}
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun
  Huang, Xinlong Wang, and Yue Cao.
\newblock Eva: Exploring the limits of masked visual representation learning at
  scale.
\newblock {\em arXiv preprint arXiv:2211.07636}, 2022.

\bibitem{fang2021compressing}
Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, and Zicheng
  Liu.
\newblock Compressing visual-linguistic model via knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1428--1438, 2021.

\bibitem{fang2021seed}
Zhiyuan Fang, Jianfeng Wang, Lijuan Wang, Lei Zhang, Yezhou Yang, and Zicheng
  Liu.
\newblock Seed: Self-supervised distillation for visual representation.
\newblock {\em ICLR}, 2021.

\bibitem{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 16000--16009, 2022.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8340--8349, 2021.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{howard2019searching}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 1314--1324, 2019.

\bibitem{jia2021scaling}
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em International Conference on Machine Learning}, pages
  4904--4916. PMLR, 2021.

\bibitem{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock {\em arXiv preprint arXiv:1909.10351}, 2019.

\bibitem{kornblith2019similarity}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In {\em International conference on machine learning}, pages
  3519--3529. PMLR, 2019.

\bibitem{li2022scaling}
Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He.
\newblock Scaling language-image pre-training via masking.
\newblock {\em arXiv preprint arXiv:2212.00794}, 2022.

\bibitem{li2021supervision}
Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao,
  Fengwei Yu, and Junjie Yan.
\newblock Supervision exists everywhere: A data efficient contrastive
  language-image pre-training paradigm.
\newblock {\em arXiv preprint arXiv:2110.05208}, 2021.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{liu2021swin}
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{mehta2021mobilevit}
Sachin Mehta and Mohammad Rastegari.
\newblock Mobilevit: light-weight, general-purpose, and mobile-friendly vision
  transformer.
\newblock {\em arXiv preprint arXiv:2110.02178}, 2021.

\bibitem{mu2022slip}
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
\newblock Slip: Self-supervision meets language-image pre-training.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI}, pages 529--544.
  Springer, 2022.

\bibitem{peng2022beit}
Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei.
\newblock Beit v2: Masked image modeling with vector-quantized visual
  tokenizers.
\newblock {\em arXiv preprint arXiv:2208.06366}, 2022.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In {\em International Conference on Machine Learning}, pages
  5389--5400. PMLR, 2019.

\bibitem{romero2014fitnets}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock {\em ICLR}, 2015.

\bibitem{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565, 2018.

\bibitem{srinivas2018knowledge}
Suraj Srinivas and Fran{\c{c}}ois Fleuret.
\newblock Knowledge transfer with jacobian matching.
\newblock In {\em International Conference on Machine Learning}, pages
  4723--4731. PMLR, 2018.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em International conference on machine learning}, pages
  6105--6114. PMLR, 2019.

\bibitem{tian2019contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive representation distillation.
\newblock {\em arXiv preprint arXiv:1910.10699}, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{wang2022gradient}
Lean Wang, Lei Li, and Xu Sun.
\newblock Gradient knowledge distillation for pre-trained language models.
\newblock {\em arXiv preprint arXiv:2211.01071}, 2022.

\bibitem{wang2022multimodal}
Zhecan Wang, Noel Codella, Yen-Chun Chen, Luowei Zhou, Xiyang Dai, Bin Xiao,
  Jianwei Yang, Haoxuan You, Kai-Wei Chang, Shih-fu Chang, et~al.
\newblock Multimodal adaptive distillation for leveraging unimodal encoders for
  vision-language tasks.
\newblock {\em arXiv preprint arXiv:2204.10496}, 2022.

\bibitem{wang2021simvlm}
Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock {\em arXiv preprint arXiv:2108.10904}, 2021.

\bibitem{wei2022mvp}
Longhui Wei, Lingxi Xie, Wengang Zhou, Houqiang Li, and Qi Tian.
\newblock Mvp: Multimodality-guided visual pre-training.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXX}, pages 337--353.
  Springer, 2022.

\bibitem{wei2022icar}
Yixuan Wei, Yue Cao, Zheng Zhang, Zhuliang Yao, Zhenda Xie, Han Hu, and Baining
  Guo.
\newblock icar: Bridging image classification and image-text alignment for
  visual recognition.
\newblock {\em arXiv preprint arXiv:2204.10760}, 2022.

\bibitem{xie2022simmim}
Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi
  Dai, and Han Hu.
\newblock Simmim: A simple framework for masked image modeling.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9653--9663, 2022.

\bibitem{yang2022mutual}
Chuanguang Yang, Zhulin An, Linhang Cai, and Yongjun Xu.
\newblock Mutual contrastive learning for visual representation learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2022.

\bibitem{yang2023online}
Chuanguang Yang, Zhulin An, Helong Zhou, Fuzhen Zhuang, Yongjun Xu, and Qian
  Zhang.
\newblock Online knowledge distillation via mutual contrastive learning for
  visual recognition.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2023.

\bibitem{yang2022cross}
Chuanguang Yang, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, and Qian Zhang.
\newblock Cross-image relational knowledge distillation for semantic
  segmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12319--12328, 2022.

\bibitem{yang2021knowledge}
Jing Yang, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos, et~al.
\newblock Knowledge distillation via softmax regression representation
  learning.
\newblock International Conference on Learning Representations (ICLR), 2021.

\bibitem{yang2022attentive}
Yifan Yang, Weiquan Huang, Yixuan Wei, Houwen Peng, Xinyang Jiang, Huiqiang
  Jiang, Fangyun Wei, Yin Wang, Han Hu, Lili Qiu, et~al.
\newblock Attentive mask clip.
\newblock {\em arXiv preprint arXiv:2212.08653}, 2022.

\bibitem{yao2021g}
Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li, and Tong Zhang.
\newblock G-detkd: towards general distillation framework for object detectors
  via contrastive and semantic-guided feature imitation.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 3591--3600, 2021.

\bibitem{young2014image}
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
\newblock From image descriptions to visual denotations: New similarity metrics
  for semantic inference over event descriptions.
\newblock {\em Transactions of the Association for Computational Linguistics},
  2:67--78, 2014.

\bibitem{yu2022coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and
  Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock {\em arXiv preprint arXiv:2205.01917}, 2022.

\bibitem{yuan2021multimodal}
Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire,
  Ajinkya Kale, and Baldo Faieta.
\newblock Multimodal contrastive training for visual representation learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 6995--7004, 2021.

\end{thebibliography}
