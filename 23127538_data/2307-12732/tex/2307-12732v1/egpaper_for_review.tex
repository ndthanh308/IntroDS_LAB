\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{multirow} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{siunitx}
\usepackage{arydshln}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{2625} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{CLIP-KD: An Empirical Study of Distilling CLIP Models}

\author{Chuanguang Yang$^1$ \qquad Zhulin An$^1$\thanks{Corresponding author.} \qquad Libo Huang$^1$ \qquad Junyu Bi$^{1,2}$  \qquad Xinqiang Yu$^{1,2}$ \\ 
	Han Yang$^{1,2}$   \qquad Yongjun Xu$^1$ \\
	$^1$Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China  \\ $^2$University of Chinese Academy of Sciences, Beijing, China \\
	% Institution1 address\\
	{\tt\small yangchuanguang@ict.ac.cn}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
	\begin{abstract}		
	CLIP has become a promising language-supervised visual pre-training framework and achieves excellent performance over a wide range of tasks. This paper aims to distill small CLIP models supervised by a large teacher CLIP model. We propose several distillation strategies, including relation, feature, gradient and contrastive paradigm, to examine the impact on CLIP distillation. We show that the simplest feature mimicry with MSE loss performs best. Moreover, interactive contrastive learning and relation-based distillation are also critical in performance improvement. We apply the unified method to distill several student networks trained on 15 million (image, text) pairs. Distillation improves the student CLIP models consistently over zero-shot ImageNet classification and cross-modal retrieval benchmarks. We hope our empirical study will become an important baseline for future CLIP distillation research. The code is available at \url{https://github.com/winycg/CLIP-KD}. 
\end{abstract}

%%%%%%%%% BODY TEXT   
\section{Introduction}
Language-supervised image pre-training has attracted widespread attention for visual representation learning. As a representative work, CLIP (Contrastive Language-Image Pre-training)~\cite{radford2021learning} applies contrastive learning to (image, text) pairs. It guides the model to predict the correct (image, text) pair among the candidate image and text samples. Pre-trained CLIP models show excellent versatility in zero-shot multimodal and unimodal visual tasks.

Some recent works aim to improve CLIP using an extra visual self-supervision task~\cite{mu2022slip,li2021supervision} or mask images~\cite{li2022scaling,yang2022attentive}. The pre-trained CLIP model is also introduced as a remarkable teacher to provide guidance for downstream visual pre-training~\cite{wei2022mvp,fang2022eva,peng2022beit}. However, to our knowledge, none of previous works explores improving the small CLIP models that are valuable in real-world applications. The teacher-student-based Knowledge Distillation (KD) provides a reasonable solution to achieve this goal. This paper proposes several distillation strategies in the context of CLIP and provides an empirical study on distilling small CLIP models. 

Given the teacher and student CLIP models, we design distillation strategies from the view of mimicry and interaction. For mimicry learning, we guide the student to align the corresponding knowledge generated from the teacher, which is a basic framework in KD~\cite{hinton2015distilling,romero2014fitnets}. The core question is how to construct meaningful knowledge. Under CLIP, we build contrastive image-to-text relationships, (image, text) features and gradients for mimicry. For interactive learning, we combine the teacher and student for joint contrastive learning, letting the student learn teacher implicitly. For example, the student is regarded as an anchor to contrast the teacher embeddings. We also attempt to aggregate the student and teacher features for CLIP training.

In this paper, we train CLIP over Conceptual dataset~\cite{sharma2018conceptual,changpinyo2021conceptual} including 15 million (image, text) pairs and evaluate pre-trained models over zero-shot ImageNet classification and cross-modal retrieval on MSCOCO and Flickr. All distillation methods improve the student CLIP model but with various margins. The simplest feature mimicry with MSE loss performs best. Interactive contrastive learning and relation mimicry achieve the second-best and third-best performance, respectively. We apply the unified method to distill a series of student networks with different architectures and achieve consistent improvements. For example, on zero-shot ImageNet-1K, CLIP-KD improves MobileViT-S from 32.60\% to 35.96\%, significantly reducing the gap with the teacher ViT-B/16 (36.99\%).



\section{Related Works}
\textbf{Language-supervised learning.} Some previous multi-modal works explore visual representations supervised by language. A critical problem is how to create meaningful interaction between visual and linguistic. CLIP~\cite{radford2021learning} is a representative approach using contrastive learning over image-text pairs. ALIGN~\cite{jia2021scaling} utilizes larger-scale  contrastive pairs with noisy text supervision. Contrastive multi-modal learning~\cite{yuan2021multimodal,yu2022coca} has popularized exploring cross-modal correlation. Beyond the contrastive paradigm, generative approaches~\cite{desai2021virtex,wang2021simvlm,alayrac2022flamingo} have been examined for visual-linguistic learning. Our method focuses on CLIP distillation that improves the performance of the small CLIP models.

\textbf{CLIP variants.} Some recent works attempt to improve CLIP with better performance and efficiency. SLIP~\cite{mu2022slip} combines CILP and visual self-supervised learning as a multi-task framework. MaskCLIP~\cite{dong2022maskclip} introduces mask self-distillation to train an image EMA encoder for CLIP. DeCLIP~\cite{li2021supervision} performs data-efficient pre-training through multi-dimension supervision signals. Beyond auxiliary supervision, FLIP~\cite{li2022scaling} and A-CLIP~\cite{yang2022attentive} conduct image masking over the input to accelerate CLIP training and achieve a better trade-off between performance and efficiency. In contrast, our paper focuses on CLIP compression using KD instead of a new CLIP method.

\textbf{Multi-modal knowledge distillation.} Knowledge Distillation (KD)~\cite{hinton2015distilling} has been applied to a broad range of tasks, such as visual recognition~\cite{romero2014fitnets,yang2022cross}, language model compression~\cite{jiao2019tinybert} and multi-modal representation learning~\cite{fang2021compressing,wang2022multimodal}. DistillVLM~\cite{fang2021compressing} performs transformer distillation using hidden attention distribution and feature maps with MSE loss. MAD~\cite{wang2022multimodal} aligns visual and text token features between teacher and student with token selection and confidence weighting. Guiding the student to mimic teacher features is a straightforward yet effective method for multi-modal distillation. Our empirical study also demonstrates this claim in CLIP-KD. Beyond the direct feature mimicry, we also explore some strategies for CLIP-KD.

% Figure environment removed


\section{Methodology}
\subsection{A Brief Review of CLIP}
\textbf{CLIP (Contrastive Language-Image Pre-Training).} Given a set of (image, text) pairs denoted as $\mathcal{D}=\{(I_{k},T_{k})\}_{k=1}^{|\mathcal{D}|}$, CLIP performs an image-text alignment task to push the paired image-text close and unpaired ones apart in the feature embedding space. The CLIP framework includes a visual encoder $f_{i}$ and a text encoder $f_{t}$ to transform the image $I_{k}$ and the text $T_{k}$ into feature embeddings $v_{k}$ and $s_{k}$ respectively, \emph{i.e.} $v_{k}=f_{i}(I_{k})$, $s_{k}=f_{t}(T_{k})$. Here, all embeddings are post-processed by $l_2$ normalization.  CLIP adopts InfoNCE-based contrastive loss to maximize the similarity between $v_{k}$ and $s_{k}$ against other negative samples. Given the image embedding $v_{k}$ as the anchor, the image-to-text contrastive loss is formulated as:
\begin{equation}
\mathcal{L}_{I\to T}=-\log\frac{\exp(v_{k}\cdot s_{k}/\tau)}{\sum_{b=1}^{|\mathcal{B}|}\exp(v_{k}\cdot s_{b}/\tau)}.
\end{equation}
CLIP conducts a symmetric image-text alignment contrastive loss. Given the text embedding $v^{T}_{k}$ as the anchor, the text-to-image contrastive loss is formulated as:
\begin{equation}
\mathcal{L}_{T\to I}=-\log\frac{\exp(s_{k}\cdot v_{k}/\tau)}{\sum_{b=1}^{|\mathcal{B}|}\exp(s_{k}\cdot v_{b}/\tau)}.
\end{equation}
Here, $\cdot$ denotes the dot product to measure the similarity, $\tau$ is a learnable temperature to scale the distribution. In practice, the negative samples are retrieved from the mini-batch $\mathcal{B}$. The total loss of CLIP is formulated as:
\begin{equation}
\mathcal{L}_{CLIP}=\frac{1}{2}(\mathcal{L}_{I\to T}+\mathcal{L}_{T\to I})
\label{clip}
\end{equation}




\subsection{CLIP Knowledge Distillation}
In this section, we propose several CLIP distillation methods and illustrate the overview of details in Fig.~\ref{clip_kd}.
\label{CLIP_KD}
\subsubsection{Contrastive Relational Distillation}
The core idea of CLIP is to maximize the similarity between the paired image-text embeddings over the contrastive similarity distribution. Therefore, the straightforward knowledge type is output-oriented contrastive distribution for Contrastive Relational Distillation (CRD). This idea of CRD is also used by some previous works for image classification~\cite{tian2019contrastive,yang2022mutual}, self-supervision~\cite{fang2021seed}, object detection~\cite{yao2021g} and semantic segmenation~\cite{yang2022cross}. The contrastive distribution captures the structured relationships among feature embeddings. A good teacher often constructs a well-structured feature space. CRD makes the student mimic better structured semantic relations from the teacher, further improving the quality of feature representations.

Given a mini-batch $\mathcal{B}=\{(I_{k},T_{k})\}_{k=1}^{|\mathcal{B}|}$, the generated (visual, text) embeddings from teacher and student are $\{(v_{k}^{\mathbf{T}},s_{k}^{\mathbf{T}})\}_{k=1}^{|\mathcal{B}|}$ and  $\{(v_{k}^{\mathbf{S}},s_{k}^{\mathbf{S}})\}_{k=1}^{|\mathcal{B}|}$, respectively. Given the $k$-th image embedding $v_{k}$ as an anchor, the teacher and student image-to-text contrastive distributions $p_{k}^{\mathbf{T}}\in \mathbb{R}^{|\mathcal{B}|}$ and $p_{k}^{\mathbf{S}}\in \mathbb{R}^{|\mathcal{B}|}$ are formulated as:
\begin{equation}
p_{k}^{\mathbf{T}}[j]=\frac{\exp(v_{k}^{\mathbf{T}}\cdot s_{j}^{\mathbf{T}}/\tau^{\mathbf{T}})}{\sum_{b=1}^{|\mathcal{B}|}\exp(v_{k}^{\mathbf{T}}\cdot s_{b}^{\mathbf{T}}/\tau^{\mathbf{T}})},
\label{p}
\end{equation}
\begin{equation}
p_{k}^{\mathbf{S}}[j]=\frac{\exp(v_{k}^{\mathbf{S}}\cdot s_{j}^{\mathbf{S}}/\tau^{\mathbf{S}})}{\sum_{b=1}^{|\mathcal{B}|}\exp(v_{k}^{\mathbf{S}}\cdot s_{b}^{\mathbf{S}}/\tau^{\mathbf{S}})}.
\end{equation}
Here, $j\in [1,2,\cdots,|\mathcal{B}|]$ denotes the index of the contrastive distribution. Symmetrically, given the text embedding $s_{k}$ as an anchor, the teacher and student text-to-image contrastive distributions $q_{k}^{\mathbf{T}}\in \mathbb{R}^{|\mathcal{B}|}$ and $q_{k}^{\mathbf{S}}\in \mathbb{R}^{|\mathcal{B}|}$ are formulated as:
\begin{equation}
q_{k}^{\mathbf{T}}[j]=\frac{\exp(s_{k}^{\mathbf{T}}\cdot v_{j}^{\mathbf{T}}/\tau^{\mathbf{T}})}{\sum_{b=1}^{|\mathcal{B}|}\exp(s_{k}^{\mathbf{T}}\cdot v_{b}^{\mathbf{T}}/\tau^{\mathbf{T}})},
\end{equation}
\begin{equation}
q_{k}^{\mathbf{S}}[j]=\frac{\exp(s_{k}^{\mathbf{S}}\cdot v_{j}^{\mathbf{S}}/\tau^{\mathbf{S}})}{\sum_{b=1}^{|\mathcal{B}|}\exp(s_{k}^{\mathbf{S}}\cdot v_{b}^{\mathbf{S}}/\tau^{\mathbf{S}})}.
\end{equation}

We align the contrastive distributions between teacher and student via KL-divergence loss. For image-to-text and text-to-image, the distillation losses are formulated as Eq.(\ref{CRD_I}) and Eq.(\ref{CRD_T}):
\begin{equation}
\mathcal{L}_{CRD\_I\to T}=\frac{1}{|\mathcal{B}|}\sum_{k=1}^{|\mathcal{B}|}\sum_{j=1}^{|\mathcal{B}|}p_{k}^{\mathbf{T}}[j]\log{\frac{p_{k}^{\mathbf{T}}[j]}{p_{k}^{\mathbf{S}}[j]}},
\label{CRD_I}
\end{equation}
\begin{equation}
\mathcal{L}_{CRD\_T\to I}=\frac{1}{|\mathcal{B}|}\sum_{k=1}^{|\mathcal{B}|}\sum_{j=1}^{|\mathcal{B}|}q_{k}^{\mathbf{T}}[j]\log{\frac{q_{k}^{\mathbf{T}}[j]}{q_{k}^{\mathbf{S}}[j]}}.
\label{CRD_T}
\end{equation}
The total CRD loss for CLIP distillation is summarized as:
\begin{equation}
\mathcal{L}_{CRD}=\mathcal{L}_{CRD\_I\to T}+\mathcal{L}_{CRD\_T\to I}.
\end{equation}

\subsubsection{Feature Distillation}
A simple yet effective way is to align feature embeddings between teacher and student to reduce the knowledge gap directly. This idea has also been widely applied to previous distillation works~\cite{fang2021compressing,yang2021knowledge}. Here, we guide the student to mimic the teacher's visual and text embeddings via Mean Squared Error (MSE) loss:
\begin{equation}
\mathcal{L}_{FD}=\frac{1}{|\mathcal{B}|}\sum_{k=1}^{|\mathcal{B}|}(\left \| v_{k}^{\mathbf{T}}-v_{k}^{\mathbf{S}} \right \|  ^{2}_{2}+\left \| s_{k}^{\mathbf{T}}-s_{k}^{\mathbf{S}} \right \|  ^{2}_{2}).
\end{equation}
Here, when the embedding sizes between teacher and student are different, we apply a linear projection head to student embeddings to match the dimension.
\subsubsection{Masked Feature Distillation}
Masked image modeling~\cite{bao2021beit,xie2022simmim,he2022masked} achieves excellent performance for self-supervised representation learning. The core idea is to recover the masked regions using contextual information modeling by a vision transformer. Recently, FLIP~\cite{li2022scaling} introduces image patch masking into the CLIP framework for improving both accuracy and
speed over the no-masking baseline. In the scenario of distillation, the teacher is a good supervisor that could provide valuable information to help the student recover the visual semantics given the masked image as input. Similar to FD, we utilize MSE loss to align the student's and teacher's visual and text embeddings. The difference is that Masked Feature Distillation (MFD) uses masked images as the input to a student. The patch masking algorithm is followed by MAE~\cite{he2022masked}. The total loss of MFD is formulated as:
\begin{equation}
\mathcal{L}_{MFD}=\frac{1}{|\mathcal{B}|}\sum_{k=1}^{|\mathcal{B}|}(\left \| v_{k}^{\mathbf{T}}-\tilde{v}_{k}^{\mathbf{S}} \right \|  ^{2}_{2}+\left \| s_{k}^{\mathbf{T}}-s_{k}^{\mathbf{S}} \right \|  ^{2}_{2}),
\end{equation}
where $\tilde{v}_{k}^{\mathbf{S}}$ is the visual embedding based on the masked input image.

\subsubsection{Gradient Distillation}
The gradient information often shows
how the model responds to changes according to inputs. Using gradient as the knowledge source has also been explored by previous KD studies~\cite{srinivas2018knowledge,wang2022gradient}. We propose to force the gradient consistency between teacher and student using the derivative w.r.t the visual and text embeddings. By this means, the student could better understand how the output should change according to the input. This helps the student behave more similarly to the teacher.

Given the image-to-text contrastive loss $\mathcal{L}_{I\to T}$, the visual embedding $v_k$ is the anchor, and the text embeddings $\{s_{b}\}_{b=1}^{|\mathcal{B}|}$ are contrastive samples. The gradient w.r.t visual and text embeddings are calculated as $\frac{\partial \mathcal{L}_{I\to T}}{\partial v_k}$ and $\frac{\partial \mathcal{L}_{I\to T}}{\partial s_b}$:
\begin{equation}
\frac{\partial \mathcal{L}_{I\to T}}{\partial v_k}=\sum_{b=1}^{|\mathcal{B}|}(p_k[b]-\bm{1}_{[k=b]})s_b/\tau,
\end{equation}
\begin{equation}
\frac{\partial \mathcal{L}_{I\to T}}{\partial s_b}=(p_k[b]-\bm{1}_{[k=b]})v_k/\tau.
\end{equation}
Here, $p_k$ is the contrastive distribution from $v_k$ to $\{s_{b}\}_{b=1}^{|\mathcal{B}|}$, corresponding to the definition in Eq.(\ref{p}). $\bm{1}$ is an indicator function that equals to 1 when $k=b$ else returns 0. Similarly, the gradient of text-to-image contrastive loss $\mathcal{L}_{T\to I}$ w.r.t the text embedding $s_k$ and visual embeddings $\{v_{b}\}_{b=1}^{|\mathcal{B}|}$ are calculated as $\frac{\partial \mathcal{L}_{I\to T}}{\partial s_k}$ and $\frac{\partial \mathcal{L}_{I\to T}}{\partial v_b}$:
\begin{equation}
\frac{\partial \mathcal{L}_{T\to I}}{\partial s_k}=\sum_{b=1}^{|\mathcal{B}|}(q_k[b]-\bm{1}_{[k=b]})v_b/\tau,
\end{equation}
\begin{equation}
\frac{\partial \mathcal{L}_{T\to I}}{\partial v_b}=(q_k[b]-\bm{1}_{[k=b]})s_k/\tau.
\end{equation}
As a result, the gradient of CLIP contrastive loss $\mathcal{L}_{CLIP}$ w.r.t each visual embedding $v_k$ and text embedding $s_k$ are formulated as:
\begin{equation}
\frac{\partial \mathcal{L}_{CLIP}}{\partial v_k}=\frac{1}{2}(\frac{\partial \mathcal{L}_{I\to T}}{\partial v_k}+\frac{\partial \mathcal{L}_{T\to I}}{\partial v_k}),
\end{equation}
\begin{equation}
\frac{\partial \mathcal{L}_{CLIP}}{\partial s_k}=\frac{1}{2}(\frac{\partial \mathcal{L}_{I\to T}}{\partial s_k}+\frac{\partial \mathcal{L}_{T\to I}}{\partial s_k}).
\end{equation}
We align the gradient information w.r.t each visual and text embedding between teacher and student via MSE loss:
\begin{align}
\mathcal{L}_{GD}=\frac{1}{|\mathcal{B}|}\sum_{k=1}^{|\mathcal{B}|}&(\left \| \frac{\partial \mathcal{L}_{CLIP}^{\mathbf{T}}}{\partial v_k^{\mathbf{T}}}-\frac{\partial \mathcal{L}_{CLIP}^{\mathbf{S}}}{\partial v_k^{\mathbf{S}}} \right \|  ^{2}_{2} \notag\\
&+\left \| \frac{\partial \mathcal{L}_{CLIP}^{\mathbf{T}}}{\partial s_k^{\mathbf{T}}}-\frac{\partial \mathcal{L}_{CLIP}^{\mathbf{S}}}{\partial s_k^{\mathbf{S}}} \right \|  ^{2}_{2})
\end{align}


\begin{table*}[tbp]
	\centering
	\caption{Comparison of CLIP distillation losses on zero-shot ImageNet-related classification and cross-modal retrieval on CC3M Val, MSCOCO and Flickr. We report top-1 accuracy (\%) for image classification and R@1 (\%) of Image-to-Text (I2T) and Text-to-Image (T2I) retrieval. The numbers in \textbf{bold} denote the best results for individual methods (the third block) and unified methods (the fourth block), respectively. The 'T' and 'S' tags represent the teacher and student roles, respectively.}
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|c|cccc|cc|cc|cc}  
			\hline
			\multirow{2}{*}{Method}&\multirow{2}{*}{Params}& IN&INV2&IN-R&IN-S&\multicolumn{2}{c|}{CC3M Val}&\multicolumn{2}{c|}{MSCOCO}&\multicolumn{2}{c}{Flickr}\\ 
			&&Top-1&Top-1&Top-1&Top-1&I2T&T2I&I2T&T2I&I2T&T2I\\
			\hline
			T: ViT-B/16 & 86.2M
			&36.99  &32.08&48.36&25.96&40.16&39.54&24.98&24.66&54.60&56.60\\
			\hline
			S: ViT-T/16 &\multirow{7}{*}{5.6M}	&30.55&25.55&35.68&17.42&33.34&33.25&20.70&20.30&46.40&47.70\\
			+CRD& &31.94&27.58&38.81&19.56&35.34&34.90&21.35&20.72&48.80&49.90\\
			+FD&  &\textbf{34.23}&\textbf{29.51}&\textbf{42.68}&\textbf{21.38}&37.13&36.85&22.52&\textbf{22.16}&\textbf{51.10}&\textbf{51.30}\\
			+MFD& &34.09&29.46&42.32&21.17&\textbf{37.37}&\textbf{36.87}&\textbf{22.88}&22.14&50.90&51.10\\
			+GD& &31.54&27.04&37.85&19.12&34.52&34.02&21.28&20.88&47.50&48.30\\
			+ICL& &33.11&28.24&40.62&20.79&36.12&35.83&21.79&21.70&50.50&50.40\\
			+AFD& &31.42&26.86&37.76&18.62&34.55&34.72&20.86&20.54&47.30&48.70\\
			\hdashline
			+FD+ICL&\multirow{4}{*}{5.6M} &34.64&30.04&43.21&21.95&37.92&37.58&22.98&22.45&51.70&51.90\\
			
			+FD+ICL+CRD& &\textbf{34.90}&\textbf{30.12}&43.49&21.91&\textbf{38.21}&\textbf{37.94}&23.14&\textbf{22.62}&52.30&\textbf{52.40}\\
			+FD+ICL+CRD+GD& &34.85&29.87&42.75&\textbf{21.96}&38.14&37.69&\textbf{23.26}&22.52&\textbf{52.40}&52.30\\
			+FD+ICL+CRD+AFD& &34.76&30.09&\textbf{43.56}&21.56&38.16&37.72&23.04&22.48&52.20&\textbf{52.40}\\
			\hline
	\end{tabular}}
	\label{comparison_clip} 
\end{table*}

\subsubsection{Interactive Contrastive Learning}
The conventional CLIP performs contrastive learning using the student visual and text encoders. To facilitate the interaction between teacher and student, we perform Interactive Contrastive Learning (ICL)~\cite{yang2022mutual,yang2023online} between the student and teacher feature encoders. ICL regards the student as an anchor to contrast the teacher's embeddings. Given the student image embedding $v_k^{\mathbf{S}}$, the contrastive text embeddings denoted as $\{s_{b}^{\mathbf{T}}\}_{b=1}^{|\mathcal{B}|}$ are from the teacher text encoder. The image-to-text ICL loss is formulated as:
\begin{equation}
\mathcal{L}_{ICL\_I\to T}=-\log\frac{\exp(v_{k}^{\mathbf{S}}\cdot s_{k}^{\mathbf{T}}/\tau)}{\sum_{b=1}^{|\mathcal{B}|}\exp(v_{k}^{\mathbf{S}}\cdot s_{b}^{\mathbf{T}}/\tau)}.
\end{equation}
Symmetrically, given the student text embedding $s_k^{\mathbf{S}}$, the contrastive image embeddings denoted as $\{v_{b}^{\mathbf{T}}\}_{b=1}^{|\mathcal{B}|}$ are from the teacher visual encoder. The text-to-image ICL loss is formulated as:
\begin{equation}
\mathcal{L}_{ICL\_T\to I}=-\log\frac{\exp(s_{k}^{\mathbf{S}}\cdot v_{k}^{\mathbf{T}}/\tau)}{\sum_{b=1}^{|\mathcal{B}|}\exp(s_{k}^{\mathbf{S}}\cdot v_{b}^{\mathbf{T}}/\tau)}.
\end{equation}
The total loss of ICL is summarized as:
\begin{equation}
\mathcal{L}_{ICL}=\frac{1}{2}(\mathcal{L}_{ICL\_I\to T}+\mathcal{L}_{ICL\_T\to I})
\end{equation}

\subsubsection{Augmented Feature Distillation}
To help the interaction between teacher and student, we propose to augment the student embeddings using the teacher embeddings by a fusion encoder. We hope the teacher could guide the student to optimize a meaningful visual-text embedding space. We introduce a visual fusion encoder $\phi_i$ and a text fusion encoder $\phi_t$ to aggregate the student and teacher embeddings:
\begin{equation}
v_{k}^{\mathbf{A}}=\phi_i(v_{k}^{\mathbf{S}}||v_{k}^{\mathbf{T}}), 	s_{k}^{\mathbf{A}}=\phi_t(s_{k}^{\mathbf{S}}||s_{k}^{\mathbf{T}}).
\end{equation}
Here, $||$ is the concatenation operator, and the fusion encoder is a simple linear projection layer. The augmented feature embeddings $(v_{k}^{\mathbf{A}},s_{k}^{\mathbf{A}})$ are applied to the general CLIP contrastive loss in Eq.(\ref{clip}).

\subsubsection{Overall Loss of CLIP Distillation}
We summarize the original CLIP task loss and distillation loss together to jointly train a student model:
\begin{equation}
\mathcal{L}_{CLIP\_KD}=\mathcal{L}_{CLIP}+\lambda \mathcal{L}_{KD},
\end{equation}
Here, $\mathcal{L}_{KD}\in \{\mathcal{L}_{CRD},\mathcal{L}_{FD},\mathcal{L}_{MFD},\mathcal{L}_{GD},\mathcal{L}_{ICL},\mathcal{L}_{AFD}\}$ represents a distillation loss. $\lambda$ is a distillation loss weight to scale the magnitude.

\begin{table*}[tbp]
	\centering
	\caption{Distillation performance compared to baseline for cross-modal retrieval on CC3M validation set. }
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|c|cccccc||l|c|cccccc}  
			\hline
			\multirow{3}{*}{Method}&\multirow{3}{*}{Params}& \multicolumn{6}{c||}{CC3M Val}&\multirow{3}{*}{Method}&\multirow{3}{*}{Params}& \multicolumn{6}{c}{CC3M Val}\\ 
			&&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c||}{Text2Image Retrieval} &&&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c}{Text2Image Retrieval} \\
			&&R@1&R@5&R@10&R@1&R@5&R@10&&&R@1&R@5&R@10&R@1&R@5&R@10\\
			\hline
			T: ViT-B/16 & 86.2M
			&40.16&65.71&74.19&39.54&65.43&73.64&T: ResNet-101 & 56.3M
			&41.37&66.89&74.99&40.49&65.97&73.95\\
			\hline
			S: MobileViT-S &\multirow{2}{*}{5.6M}	&35.99&61.46&70.57&35.59&60.94&69.76&S: MobileViT-S &\multirow{2}{*}{5.6M}	&35.99&61.46&70.57&35.59&60.94&69.76\\
			+CLIP-KD& &39.39&65.40&74.48&38.58&64.54&73.65&
			+CLIP-KD& &39.89&65.38&74.10&38.55&64.12&73.15\\
			\hline
			S: Swin-T &\multirow{2}{*}{27.9M}	&39.80&64.90&73.24&39.16&64.53&72.94&Swin-T &\multirow{2}{*}{27.9M}	&39.80&64.90&73.24&39.16&64.53&72.94\\
			+CLIP-KD& &43.68&69.68&78.00&42.45&68.87&77.09&
			+CLIP-KD& &44.15&70.11&78.04&43.04&68.81&76.93 \\
			\hline
			S: MobileNetV3 &\multirow{2}{*}{2.0M}	&28.06&51.25&61.37&27.51&50.83&60.74&S: MobileNetV3 &\multirow{2}{*}{2.0M}	&28.06&51.25&61.37&27.51&50.83&60.74\\
			+CLIP-KD& &30.12&55.00&64.56&28.58&53.51&63.66&+KD& &30.18&54.84&64.26&29.38&53.78&63.61\\
			\hline
			S: EfficientNet-B0 &\multirow{2}{*}{4.7M}	&35.37&60.80&70.24&34.92&60.51&69.61&S: EfficientNet-B0 &\multirow{2}{*}{4.7M}	&35.37&60.80&70.24&34.92&60.51&69.61\\
			+CLIP-KD& &39.04&65.33&74.76&38.00&64.37&73.54&+KD& &37.43&63.75&72.73&36.75&62.76&71.70\\
			\hline
			S: ResNet-18 &\multirow{2}{*}{11.4M}	&31.10&55.50&65.15&30.35&54.89&64.25&S: ResNet-18 &\multirow{2}{*}{11.4M}	&31.10&55.50&65.15&30.35&54.89&64.25\\
			+CLIP-KD& &34.22&59.83&69.35&32.95&58.21&68.23&+KD& &34.70&60.51&69.79&33.65&59.49&68.96\\
			\hline
	\end{tabular}}
	
	\label{vit_b_kd_cc3m_val} 
\end{table*}

\begin{table*}[tbp]
	\centering
	\caption{Distillation performance compared to baseline for zero-shot cross-modal retrieval on MSCOCO and Flickr.}
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|c|cccccc|cccccc}  
			\hline
			\multirow{3}{*}{Method}&\multirow{3}{*}{Params}&  \multicolumn{6}{c|}{MSCOCO}& \multicolumn{6}{c}{Flickr}\\ 
			&&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c|}{Text2Image Retrieval}&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c}{Text2Image Retrieval} \\
			&&R@1&R@5&R@10&R@1&R@5&R@10&R@1&R@5&R@10&R@1&R@5&R@10 \\
			\hline
			T: ViT-B/16 & 86.2M
			&24.98&50.18&61.82&24.66&49.26&60.90&54.60&81.00&88.20&56.60&80.80&88.60 \\
			\hline
			S: MobileViT-S &\multirow{2}{*}{5.6M} 
			&22.30&45.62&56.68&22.92&46.20&57.72&50.10&76.40&84.80&53.00&79.80&86.90\\
			+CLIP-KD&&26.12&51.14&63.30&24.86&50.44&61.80&55.00&81.10&88.20&56.20&81.60&88.60 
			\\
			\hline
			S: Swin-T &\multirow{2}{*}{27.9M} &24.72&48.86&60.58 &25.32 &48.76 &60.32 &53.40 &79.40 &87.10 &54.40&81.10&87.90 
			
			\\
			+CLIP-KD&&28.46&54.52 &66.58&28.56&54.46 &65.58 &62.20 &85.10 &90.90&60.90 &86.00&90.50   
			
			\\
			\hline
			S: MobileNetV3 &\multirow{2}{*}{2.0M} &
			15.26&35.58 &46.66&14.96 &35.62&47.32&36.90&64.70&73.60 &38.00 &63.50&74.10 
			
			\\
			+CLIP-KD&&17.90&38.70&51.12&15.98&37.24 &48.56&42.40&68.90&77.90 &42.30  &68.40 &77.60 
			
			\\
			\hline
			S: EfficientNet-B0 &\multirow{2}{*}{4.7M} &
			21.66&44.58 &55.82 &21.10 &45.20 &56.46&48.30 &75.70 
			
			&83.70 &50.10 &76.50 &84.30 
			
			\\
			+CLIP-KD&&26.00&49.54 &61.64 &23.90 &48.72 &60.62 &55.50&82.80 &88.00  &54.20 &81.10&88.70  
			
			\\
			\hline
			S: ResNet-18 &\multirow{2}{*}{11.4M} &19.20&41.78&53.48 &18.60&40.20 &52.34&41.00 &71.10&78.10 &43.30&70.40 &79.50 
			
			\\
			+CLIP-KD&&21.28 &44.92 &57.26  &19.78  &42.70  &54.74 &47.80 &77.00  &85.10  &47.10  &75.40 &84.70 
			
			\\
			\hline
	\end{tabular}}
	
	\label{vit_kd_coco} 
\end{table*}


\section{Experiments}
\subsection{Experimental Setup}
\textbf{Dataset.} We use Conceptual Captions 3M (CC3M)~\cite{sharma2018conceptual} and Conceptual 12M (CC12M)~\cite{changpinyo2021conceptual} for large-scale vision-and-language pre-training. The joint training dataset CC3M+12M contains around 15 million image-text pairs. We follow the consistent evaluation protocol with CLIP-related works~\cite{wei2022icar,li2022scaling}. The CC3M validation set, including 13K image-text pairs, is used for cross-modal retrieval evaluation. For zero-shot classification, we utilize the ImageNet (IN)~\cite{deng2009imagenet} validation set and its several variants, such as ImageNet-V2 (IN-V2)~\cite{recht2019imagenet}, ImageNet-Rendition (IN-R)~\cite{hendrycks2021many} and ImageNet-Sketch (IN-S)~\cite{wang2019learning} for evaluation. For zero-shot cross-modal image/text retrieval, we adopt MSCOCO~\cite{lin2014microsoft} and Flickr~\cite{young2014image} for evaluation.


\textbf{Evaluation metrics.} Following the standard setting, we employ Recall@$K$ (R@$K$) to measure the cross-modal retrieval performance in $K$ nearest neighbours. We use top-1 and top-5 accuracy for image classification.

\textbf{Image encoders.} We examine two different architectures, \emph{i.e.} Convolutional Neural Networks (CNN) and Vision Transformer (ViT), for the image encoder. For teacher image encoders, we use ResNet-101~\cite{he2016deep} and ViT-B/16~\cite{dosovitskiy2020image} due to their
widespread adoption with superior performance.  For student image encoders, we introduce several well-known light-weight networks, such as ViT-T/16~\cite{dosovitskiy2020image}, MobileViT-S~\cite{mehta2021mobilevit}, Swin-T~\cite{liu2021swin}, ResNet-18~\cite{he2016deep}, EfficientNet-B0~\cite{tan2019efficientnet} and MobileNetV3~\cite{howard2019searching}. The first four networks are ViTs, and the last three ones are CNNs. 

 \textbf{Text encoders.} Followed by CLIP, the text encoder is a Transformer~\cite{vaswani2017attention}. For the teacher text encoder, we use a 12-layer 512-width 8-attention-heads model (Transformer-12-512-8) with 63M parameters in total. For the student text encoder, we utilize a 12-layer 384-width 6-attention-heads model (Transformer-12-384-6) with 40M parameters in total.

\textbf{Training details.} We follow the basic training setup with open\_clip~\footnote{https://github.com/mlfoundations/open\_clip}. We adopt an AdamW optimizer~\cite{loshchilov2017decoupled} with an initial learning rate of 0.001 and a weight decay of 0.1. A cosine learning rate schedule is applied with a linear warm-up for 10K iterations in 32 epochs. All experiments are ran over 8 V100 GPUs. The batch size is 1024, where each GPU holds 128 samples. For the weight of each distillation loss, we set $\lambda_{CRD}=1$, $\lambda_{FD}=\lambda_{MFD}=2000$, $\lambda_{GD}=10^{8}$ and $\lambda_{ICL}=1$. The learnable temperature $\tau$ is initialized from 0.07. Other training settings are followed by the original CLIP~\cite{radford2021learning}.



\subsection{Ablation Study of Distillation Losses}
In this section, we examine the effectiveness of various CLIP distillation approaches proposed in Section~\ref{CLIP_KD}. As shown in Table~\ref{comparison_clip}, we conduct a comprehensive comparison of zero-shot ImageNet-related classification and cross-modal retrieval. The teacher image encoder is selected as ViT-B/16, and the student is ViT-T/16. Any individual distillation loss could boost the student performance over the baseline. Feature Distillation (FD) with a simple MSE mimicry loss achieves the best distillation performance among them. It improves the student by 4.18\% top-1 accuracy on ImageNet, 4.70\%, 2.24\% and 5.15\% R@1 values on CC3M, MSCOCO and Flickr, respectively. We further evaluate MFD by applying image patch masking into FD. MFD shows similar performance with FD, therefore we do not introduce this technique for CLIP-KD. Beyond MFD, ICL and CRD become the second- and third-best approaches for overall zero-shot performance. GD and AFD lead to relatively moderate performance gains compared to the baseline. 

We further combine loss terms to investigate the unified distillation approach. The combination of FD+ICL outperforms the single FD or ICL, indicating that FD and ICL are complementary to each other.  We further apply CRD to FD+ICL, and the performance is improved consistently. Moreover, we find adding GD or AFD to FD+ICL+CRD may not lead to performance gains. As a summary, the combination FD+CRD+ICL achieves the best performance in 6 out of 11 cases. As the default configuration, we utilize this unified method for distilling various CLIP models in this paper. 



\begin{table*}[tbp]
	\centering
	\caption{Distillation performance of zero-shot cross-modal retrieval on MSCOCO and Flickr.}
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|c|cccccc|cccccc}  
			\hline
			\multirow{3}{*}{Method}&\multirow{3}{*}{Params}&  \multicolumn{6}{c|}{MSCOCO}& \multicolumn{6}{c}{Flickr}\\ 
			&&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c|}{Text2Image Retrieval}&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c}{Text2Image Retrieval} \\
			&&R@1&R@5&R@10&R@1&R@5&R@10&R@1&R@5&R@10&R@1&R@5&R@10 \\
			\hline
			T: ResNet-101 & 56.3M
			&25.18&49.72&62.10 &25.74 &50.60 &61.44&57.00&82.90 &88.30  &55.50 &82.70 &89.20 
			
			\\
			\hline
			S: MobileViT-S &\multirow{2}{*}{5.6M} 
			&22.30&45.62&56.68&22.92&46.20&57.72&50.10&76.40&84.80&53.00&79.80&86.90\\
			+CLIP-KD&&25.98&50.54&62.04&25.34 &49.82 &61.28 &57.60 &82.50 &88.10 &56.10 &82.10 &88.50 
			
			
			\\
			\hline
			S: Swin-T &\multirow{2}{*}{27.9M} &24.72&48.86&60.58 &25.32 &48.76 &60.32 &53.40 &79.40 &87.10 &54.40&81.10&87.90 
			
			\\
			+CLIP-KD&&27.80 &53.80&65.06&28.92&53.34 &64.98&60.80 &83.70 &90.00& 61.50 &84.80 &90.90 
			
			
			
			\\
			\hline
			S: MobileNetV3 &\multirow{2}{*}{2.0M} &
			15.26&35.58 &46.66&14.96 &35.62&47.32&36.90&64.70&73.60 &38.00 &63.50&74.10 
			
			\\
			+CLIP-KD&&17.16&38.22&49.86&16.58&37.10&48.76 &40.20 &68.90 &78.30&42.20&68.70 &77.70 
			\\
			\hline
			S: EfficientNet-B0 &\multirow{2}{*}{4.7M} &
			21.66&44.58 &55.82 &21.10 &45.20 &56.46&48.30 &75.70 
			
			&83.70 &50.10 &76.50 &84.30 
			
			\\
			+CLIP-KD&&24.68 &49.88 &62.04&24.58&48.46 &60.50&55.80&80.40&87.80 &56.20 &80.30 &87.70 
			\\
			\hline
			S: ResNet-18 &\multirow{2}{*}{11.4M} &19.20&41.78&53.48 &18.60&40.20 &52.34&41.00 &71.10&78.10 &43.30&70.40 &79.50 
			
			\\
			+CLIP-KD&&21.04 &44.66&56.78&20.88&44.10 &55.64  &48.80&77.30 &84.40 &48.40 &77.10 &84.50  
			
			
			
			\\
			\hline
	\end{tabular}}
	
	\label{res101_kd_coco} 
\end{table*}

\begin{table*}[tbp]
	\centering
	\caption{Distillation performance of zero-shot ImageNet classification and its variants on top-1 classification accuracy (\%).}
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|c|c|c|c|c||l|c|c|c|c|c}  
			\hline
			Method&Params&IN-1K&INV2& IN-R&IN-S&Method&Params&IN-1K&INV2& IN-R&IN-S\\ 
			\hline
			T: ViT-B/16 & 86.2M
			&36.99 &32.08&48.36&25.96 &T: ResNet-101 & 56.3M
			&36.76 &31.91&49.16&26.65\\
			\hline
			S: MobileViT-S &\multirow{2}{*}{5.6M}  &32.60  & 27.57&39.45&20.11&S: MobileViT-S &\multirow{2}{*}{5.6M}  &32.60 & 27.57&39.45&20.11 \\
			+CLIP-KD& &35.96 &31.06&44.47&23.46&+CLIP-KD& &34.97 & 30.05&43.72&22.66\\
			\hline
			S: Swin-T& \multirow{2}{*}{27.9M}&36.38 & 31.11&45.91&24.44&S: Swin-T& \multirow{2}{*}{27.9M}&36.38 & 31.11&45.91&24.44\\
			+CLIP-KD&&40.18&34.88&51.38&28.20&+CLIP-KD&&39.51&34.19 &51.87 &28.12 \\ 
			\hline
			S: MobileNetV3& \multirow{2}{*}{2.0M}&25.11 & 20.67&29.15&13.44&S:MobileNetV3& \multirow{2}{*}{2.0M}&25.11& 20.67&29.15&13.44\\
			+CLIP-KD&&26.95&22.95&30.60&14.10&+CLIP-KD&&26.15&22.16&29.32&13.73\\ 
			\hline
			S: EfficientNet-B0& \multirow{2}{*}{4.7M}&32.55  & 27.76&40.92&20.67&S:EfficientNet-B0& \multirow{2}{*}{4.7M}&32.55 & 27.76&40.92&20.67\\
			+CLIP-KD&&35.44&30.57&44.68&23.65&+CLIP-KD&&34.64&29.37&44.39&23.14\\ 
			\hline
			S: ResNet-18& \multirow{2}{*}{11.4M}&28.55 & 24.01&35.29&18.08 &S:ResNet-18& \multirow{2}{*}{11.4M}&28.55 & 24.01&35.29&18.08\\
			+CLIP-KD&&31.36&26.91&39.18&19.96&+CLIP-KD&&30.88&25.90&38.02&19.45\\ 
			\hline
	\end{tabular}}
	
	\label{vit_b_imagenet} 
\end{table*}

% Figure environment removed

\subsection{Distilling CLIP Models}
Given the pretrained teacher CLIP model, we distill several light-weight student CLIP models with various architectures. The results are evaluated on cross-modal retrieval and zero-shot ImageNet classification compared to the CLIP baseline.

\textbf{Cross-modal retrieval on CC3M.} Table~\ref{vit_b_kd_cc3m_val} reports distillation performance supervised by ViT-B/16 and ResNet-101 as teachers. The proposed KD approach improves student performance over various network architectures consistently. Supervised by ViT-B/16 for image$\to $text retrieval, KD leads to 3.40\%, 3.88\%, 2.06\%, 3.67\% and 3.12\% R@1 gains on MobileViT-S, Swin-T, MobileNetV3, EfficientNet-B0 and ResNet-18, respectively. For text$\to $image retrieval, KD results in 2.99\%, 3.29\%, 1.07\%, 3.08\% and 2.60\% R@1 gains on these networks. Supervised by ResNet-101, KD boosts the baseline by 3.90\%, 4.35\%, 2.12\%, 2.06\% and 3.60\% R@1 for image$\to $text retrieval, as well as 2.96\%, 3.88\%, 1.87\%, 1.83\% and 3.30\% R@1 for text$\to $image retrieval, over these five student networks, respectively. The results demonstrate the effectiveness of CLIP-KD over a series of networks. Moreover, the architectural difference between ViT and CNN does not affect distillation performance. This is because our CLIP-KD only considers the final output embeddings for distillation instead of information from hidden layers.

\textbf{Zero-shot cross-modal retrieval on MSCOCO and Flickr.} We further transfer student CLIP models to zero-shot cross-modal retrieval on MSCOCO and Flickr. Table~\ref{vit_kd_coco} reports the results supervised from the teacher ViT-B-16. On MSCOCO, KD outperforms the baseline by 3.82\%, 3.74\%, 2.64\%, 4.34\% and 2.08\% R@1 margins for image$\to$text retrieval, as well as 1.94\%, 3.24\%, 1.02\%, 2.80\% and 1.18\% R@1 margins for text$\to$image retrieval on MobileViT-S, Swin-T, MobileNetV3, EfficientNet-B0 and ResNet-18, respectively. On Flickr, the R@1 gains are 4.9\%, 8.8\%, 5.5\%, 7.2\% and 6.8\% for image$\to$text retrieval, as well as 3.2\%, 6.5\%, 4.3\%, 4.1\% and 3.8\% for text$\to$image retrieval. Table~\ref{res101_kd_coco} reports the results supervised from the teacher ResNet-101. On MSCOCO, KD leads to 3.68\%, 3.08\%, 1.90\%, 3.02\% and 1.84\% R@1 improvements for image$\to$text retrieval, as well as 2.42\%, 3.60\%, 1.62\%, 3.48\% and 2.22\% R@1 improvements for text$\to$image retrieval, over five student networks, respectively. On Flickr, KD results in 7.5\%, 7.4\%, 3.3\%, 7.5\% and 7.8\% R@1 gains for image$\to$text retrieval, as well as 3.2\%, 6.5\%, 4.3\%, 4.1\% and 3.8\% for text$\to$image retrieval. The results reveal the transfer ability to zero-shot cross-modal retrieval using CLIP-KD. 


\textbf{Zero-shot ImageNet-related classification.} We transfer the student CLIP models to zero-shot ImageNet classification for visual recognition and ImageNet-variants for robustness evaluation. Table~\ref{vit_b_imagenet} reports distillation performance supervised by ViT-B-16 and ResNet-101 as teachers, respectively. For ImageNet classification supervised by ViT-B/16, KD improves 3.36\%, 3.80\%, 1.84\%, 2.89\% and 2.81\% top-1 accuracy gains over MobileViT-S, Swin-T, MobileNetV3, EfficientNet-B0 and ResNet-18, respectively. Supervised by ResNet-101, KD achieves 2.37\%, 3.13\%, 1.04\%, 2.19\% and 2.33\% top-1 accuracy improvements over five networks, respectively. The results show that CLIP-KD can help
 downstream visual recognition effectively. Extensive experiments over ImageNet variants indicate that CLIP-KD can lead to clear accuracy gains over baseline.




%	\begin{table}[tbp]
%		\centering
%		\caption{Comparison of various loss terms for cross-modal retrieval on CC3M validation set. The numbers in \textbf{bold} denote the best results. The 'T' and 'S' tags represent the teacher and student roles, respectively.}
%		\resizebox{1.\linewidth}{!}{
%			\begin{tabular}{l|c|cccccc}  
%				\hline
%				\multirow{3}{*}{Method}&\multirow{3}{*}{Params}& \multicolumn{6}{c}{CC3M Val}\\ 
%				&&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c}{Text2Image Retrieval} \\
%				&&R@1&R@5&R@10&R@1&R@5&R@10\\
%				\hline
%				T: ViT-B/16 & 86.2M
%				&40.16&65.71&74.19&39.54&65.43&73.64\\
%				\hline
%				S: ViT-T/16 &\multirow{7}{*}{5.6M}	&32.39&56.87&66.07&32.29&57.01&66.18\\
%				+CRD& &34.34&59.58&68.87&33.90&58.80&68.15\\
%				+FD&  &37.90&63.84&72.56&37.24&62.95&71.71\\
%				+MFD& &37.50&63.70&72.92&37.16&62.64&72.11\\
%				+GD& &32.84&58.10&67.16&33.02&58.08&67.52\\
%				+ICL& &34.72&59.56&69.03&34.73&60.08&69.16\\
%				+AFD& &33.55&58.46&67.82&33.72&58.32&67.57\\
%				\hline
%			+FD+CRD&\multirow{2}{*}{5.6M} &37.94&63.87&\textbf{72.97}&37.32&62.92&72.15\\
%			+FD+CRD+ICL& &\textbf{38.27}&\textbf{64.03}&72.86&\textbf{37.48}&\textbf{63.02}&\textbf{72.18}\\
%			\hline
%			\end{tabular}}
%		
%		\label{cc3m_val} 
%	\end{table}








%
%	\begin{table*}[tbp]
%		\centering
%		\caption{Comparison of various loss terms for zero-shot cross-modal retrieval on MSCOCO and Flickr. The numbers in \textbf{bold} denote the best results. The 'T' and 'S' tags represent the teacher and student roles, respectively.}
%		\resizebox{1.\linewidth}{!}{
%			\begin{tabular}{l|c|cccccc|cccccc}  
%				\hline
%				\multirow{3}{*}{Method}&\multirow{3}{*}{Params}&  \multicolumn{6}{c|}{MSCOCO}& \multicolumn{6}{c}{Flickr}\\ 
%				&&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c|}{Text2Image Retrieval}&\multicolumn{3}{c}{Image2Text Retrieval}&\multicolumn{3}{c}{Text2Image Retrieval} \\
%				&&R@1&R@5&R@10&R@1&R@5&R@10&R@1&R@5&R@10&R@1&R@5&R@10 \\
%				\hline
%				T: ViT-B/16 & 86.2M
%				&24.98&50.18&61.82&24.66&49.26&60.90&54.60&81.00&88.20&56.60&80.80&88.60 \\
%				\hline
%				S: ViT-T/16 &\multirow{7}{*}{5.6M} &18.95&40.12&53.14&19.04&42.16&53.14&43.90&71.90&81.20&46.10&72.80&81.20\\
%				+CRD&&19.66&43.34&55.92&20.72&43.78&55.42&46.00&72.80&81.50&47.30&74.80&82.00\\
%				+FD& &23.22&48.42&60.20&22.60&47.62&59.24&52.30&79.20&87.10&\textbf{53.60}&78.90&86.10\\
%				+MFD&  &23.18&48.30&60.16&23.14&47.06&59.30&\textbf{53.20}&77.00&86.50&52.50&79.00&85.90\\
%				+GD& &19.28&42.14&54.08&19.88&42.60&54.46 &44.50&73.40&81.40&46.30&74.60&83.40\\
%				+ICL& &20.28&43.86&56.02&20.70&43.62&55.50&46.40&74.20&83.40&51.90&75.90&84.70\\
%				+AFD& &19.06&41.26&53.78&19.62&42.28&53.92&44.30&73.20&81.60&46.70&73.80&82.30\\
%				\hline
%				+FD+CRD&\multirow{2}{*}{5.6M} &23.44&48.46&60.24&23.10&47.56&59.38 &52.90&78.70&87.10&52.60&79.00&86.10\\
%				+FD+CRD+ICL& &\textbf{23.62}&\textbf{48.82}&\textbf{60.90}&\textbf{23.28}&\textbf{47.84}&\textbf{59.48}&53.10&\textbf{80.20}&\textbf{87.20}&53.50&\textbf{79.20}&\textbf{86.40}\\
%				\hline
%		\end{tabular}}
%		
%		\label{ablation_coco_flickr} 
%	\end{table*}



%\begin{table*}[tbp]
%	\centering
%	\caption{Comparison of various loss terms for zero-shot ImageNet and its variants on classification performance. The numbers in \textbf{bold} denote the best results. The 'T' and 'S' tags represent the teacher and student roles, respectively.}
%	
%	\begin{tabular}{l|c|cc|cc|cc|cc|cc}  
%		\hline
%		\multirow{2}{*}{Method}&\multirow{2}{*}{Params}& \multicolumn{2}{c|}{ImageNet}& \multicolumn{2}{c|}{ImageNetV2}& \multicolumn{2}{c|}{ImageNet-R}&
%		\multicolumn{2}{c|}{ImageNet-A}&
%		\multicolumn{2}{c}{ImageNet-Sketch}\\ 
%		&&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5&Top-1&Top-5 \\
%		\hline
%		T: ViT-B/16 & 86.2M
%		&36.99 &64.58 &32.08&58.33&48.36&74.30&9.84&31.21&25.96&50.43\\
%		\hline
%		S: ViT-T/16 &\multirow{7}{*}{5.6M}  &30.20 & 56.29 & 25.62& 50.56&33.10&58.95&5.48&22.27&15.54&34.85\\
%		+CRD& &30.94 & 57.63&26.49&51.69&35.92&62.15&6.44&23.84&17.42&37.82\\
%		+FD&  & 34.14& 61.94&29.72&56.04&42.33&68.29&7.41&\textbf{26.95}&21.30&44.63\\
%		+MFD&  & 34.11& 61.83&29.15&56.02&42.04&68.02&6.96&25.93&21.39&44.71\\
%		+GD& &30.93 & 57.24 & 26.28&51.46&35.45&61.64&6.04&23.08&17.22&37.08\\
%		+ICL& &32.07 &59.22 & 27.59&53.15&38.62&64.89&6.71&24.84&19.39&40.84\\
%		+AFD& &30.92&56.85&26.13&50.77&35.76&62.17&6.21&22.92&17.62&38.13\\
%		\hline
%		+FD+CRD& \multirow{2}{*}{5.6M}&34.28 & 61.60 & 29.25&55.72&42.56&68.67&7.25&26.56&21.44&44.43\\
%		+FD+CRD+ICL& &\textbf{34.56}&\textbf{62.18}&\textbf{30.31}&\textbf{56.37}&\textbf{42.90}&\textbf{69.23}&\textbf{7.55}&26.77&\textbf{21.96}&\textbf{45.30}\\
%		\hline
%	\end{tabular}
%	
%	\label{ablation_b_imagenet} 
%\end{table*}







\subsection{Analysis}
In this section, we conduct thorough analysis and ablation experiments to investigate  CLIP-KD. Unless otherwise specified, the teacher and student visual encoders are
ViT-B/16 and ViT-T/16, respectively. The teacher and student text encoders are Transformer-12-512-8 and Transformer-12-384-6, respectively.


\subsubsection{Training curve of CLIP-KD} As shown in Fig.~\ref{train_curve}, we illustrate some statistics and  analyses of CLIP-KD during the training procedure:

(1) \textbf{Training loss analysis.} Fig.~\ref{loss_curve} shows training curves of various loss terms. All loss values decrease as the training goes on and are converged until the end of training. CLIP-KD has lower task loss than that of the baseline during the training since it is supervised by a pretrained CLIP teacher. The task loss is often larger than the ICL loss, because the teacher provides converged contrastive embeddings to the student in ICL, helping the student optimize feature space readily. 

(2) \textbf{Sample similarity analysis.} Fig.~\ref{sim_curve} shows the similarity curve of positive minus negative pairs, which represents the relative distance between positive and negative pairs. Contrastive learning expects that positive pairs have higher similarities, while negative pairs have lower similarities. Both the baseline and CLIP-KD increase (positive-negative) pair similarity during the training stage, indicating a discriminative embedding space is gradually learned. CLIP-KD has higher similarity values than the baseline, manifesting that it guides the student to learn more discriminative features, further beneficial to downstream tasks. 

(3) \textbf{Performance analysis.} Fig.~\ref{recall_curve} and Fig.~\ref{acc_curve} show performance curves of cross-modal retrieval and ImageNet classification, respectively. CLIP-KD outperforms the baseline consistently during the training process.

\subsubsection{Analyses of various KD methods } In Table~\ref{sim_methods}, we analyse various KD methods in different performance from the view of cosine and CKA~\cite{kornblith2019similarity} similarities between student and teacher features after distillation. We find student accuracy is in line with feature similarity. The larger similarity means that the student learns more similar teacher features, reducing the performance gap with the teacher. The simplest FD performs the best because it forces the student to increase the similarity with teacher features directly. 

However, FD does not consider informative
contrastive image-text relations. ICL is proposed to promote contrastive distillation and increase the mutual information~\cite{yang2022mutual,yang2023online} between teacher and student, resulting in high similarity. By contrast, CRD, GD, and AFD are relatively weaker in enhancing similarity with the teacher, thus achieving limited gains above baseline. Overall, the combination of FD+ICL is capable of feature alignment and contrastive distillation, which is the major source of performance improvement.


\begin{table}[tbp]
	\centering
	\caption{Similarity statistics between teacher and student features after distillation.  $v_{k}^{\mathbf{T}}$ and $v_{k}^{\mathbf{S}}$ denote the teacher and student image features, respectively. $s_{k}^{\mathbf{T}}$ and $s_{k}^{\mathbf{S}}$ denote the teacher and student text features, respectively. We also attach the rank in '()' in each column for clear illustration. }
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|cc|cc|c}  
			\hline
				\multirow{2}{*}{Method}&\multicolumn{2}{c|}{Image feature}&\multicolumn{2}{c|}{Text feature}&ImageNet\\ 
			&$\cos(v_{k}^{\mathbf{T}},v_{k}^{\mathbf{S}})$&$\rm{CKA}$$(v_{k}^{\mathbf{T}},v_{k}^{\mathbf{S}})$&$\cos(s_{k}^{\mathbf{T}},s_{k}^{\mathbf{S}})$&$\rm{CKA}$$(s_{k}^{\mathbf{T}},s_{k}^{\mathbf{S}})$&Top-1 Acc \\
			\hline
			Baseline & 0.0162 (6th)  & 0.7918 (6th) & 0.0193 (6th) &0.8076 (6th)&30.55 (6th) \\
			+CRD & 0.8172 (3rd) & 0.8524 (3rd)  &0.9056 (3rd) &0.8362 (4th) &31.94 (3rd) \\
			+FD & \textbf{0.9272} (1st) & \textbf{0.9430} (1st) & \textbf{0.9392} (1st) & \textbf{0.9364} (1st) & \textbf{34.23} (1st) \\
			+GD & 0.8041 (4th) & 0.8315 (4th)& 0.9032 (4th)&0.8378  (3rd)&31.54 (4th) \\
			+ICL & 0.8584 (2nd) & 0.8969 (2nd) & 0.9139 (2nd)& 0.8454 (2nd)&33.11 (2nd) \\
			+AFD & 0.7841 (5th) & 0.8219 (5th) & 0.8917 (5th)&0.8080 (5th)&31.42 (5th)
			  \\
			\hline
	\end{tabular}}
	\vspace{-0.2cm}
	\label{sim_methods} 
\end{table}

\subsubsection{Analyses of hyper-parameters}
In this section, we investigate the impact of hyper-parameters on distillation performance.

\textbf{Loss weight of FD.} As shown in Table~\ref{FD_lambda}, we examine the impact of FD's loss weight $\lambda_{FD}$. The performance is gradually improved as $\lambda_{FD}$ increases but saturates at $\lambda_{FD}=2000$.

\textbf{Loss weight of CRD.} As shown in Table~\ref{CRD_lambda}, we examine the impact of CRD's loss weight $\lambda_{CRD}$. Overall, the performance is robust to the weight change, where $\lambda_{CRD}=1$ is a suitable choice. This is because CRD loss is entropy-based KL-divergence loss, and the magnitude is consistent with cross-entropy-based task loss. 

\textbf{Loss weight of GD.} As shown in Table~\ref{GD_lambda}, we examine the impact of GD's loss weight $\lambda_{GD}$. The performance is gradually improved as $\lambda_{GD}$ increases but saturates at $\lambda_{GD}=10^8$.

\textbf{Loss weight of ICL.} As shown in Table~\ref{ICL_lambda}, we examine the impact of ICL's loss weight $\lambda_{ICL}$. Overall, the performance is robust to the weight change, where $\lambda_{ICL}=1$ achieves the best performance. ICL has the same contrastive loss function as CLIP task loss, so $\lambda_{ICL}=1$ leads to the same magnitude as CLIP task loss.

\textbf{Mask ratio.} As shown in Table~\ref{mask_ratio}, we examine the impact of mask ratio. Using various mask ratios does not result in more performance gains than the no-masking baseline. 

\begin{table}[tbp]
	\centering
	\caption{Analysis of FD loss weight $\lambda_{FD}$. 'scratch$\to $converge' denotes the change of loss value from scratch to convergence. }
	\resizebox{1.\linewidth}{!}{
		\begin{tabular}{l|c|c|cc}  
			\hline
			\multirow{2}{*}{$\lambda_{FD}$}&Loss&ImageNet& \multicolumn{2}{c}{CC3M Val}\\ 
			&scratch$\to $converge&Acc&I2T&T2I \\
			\hline
			10 & 0.079$\to $0.013 & 31.10 & 33.57&33.46 \\
			100 & 0.794$\to $0.089 & 32.31  & 34.59&34.40 \\
			1000 & 7.721$\to $0.538 & 33.67 & 36.70&36.44 \\
			2000 & 15.880$\to $0.902 & \textbf{34.23} & \textbf{37.13}&\textbf{36.85} \\
			3000 & 30.452$\to $1.651 & 34.12 & 37.05&36.61 \\
			\hline
	\end{tabular}}
	\vspace{-0.2cm}
	\label{FD_lambda} 
\end{table}

\begin{table}[tbp]
	\centering
	\caption{Analysis of CRD loss weight $\lambda_{CRD}$. }
	
	\begin{tabular}{l|c|cc}  
		\hline
		\multirow{2}{*}{$\lambda_{CRD}$}&ImageNet& \multicolumn{2}{c}{CC3M Val}\\ 
		&Acc&I2T&T2I \\
		\hline
		0.5  & 31.56 &  34.86 & 34.64 \\
		1  & \textbf{31.94} &  \textbf{35.34} & \textbf{34.90} \\
		2 & 31.67 & 35.22&34.85 \\
		10& 31.24 & 34.85&34.63 \\
		\hline
	\end{tabular}
	
	\label{CRD_lambda} 
\end{table}


\begin{table}[tbp]
	\centering
	\caption{Analysis of GD loss weight $\lambda_{GD}$.}
	
	\begin{tabular}{l|c|cc}  
		\hline
		\multirow{2}{*}{$\lambda_{GD}$}&ImageNet& \multicolumn{2}{c}{CC3M Val}\\ 
		&Acc&I2T&T2I \\
		\hline
		$10^6$ & 30.64 &  33.69 & 33.11 \\
		$10^7$  & 30.76 &  33.86 & 33.25 \\
		$10^8$ &  \textbf{31.54} & \textbf{34.52} & \textbf{34.02} \\
		$10^9$& 31.35 & 34.24 & 33.73 \\
		\hline
	\end{tabular}
	
	\label{GD_lambda} 
\end{table}


\begin{table}[tbp]
	\centering
	\caption{Analysis of ICL loss weight $\lambda_{ICL}$. }
	
	\begin{tabular}{l|c|cc}  
		\hline
		\multirow{2}{*}{$\lambda_{ICL}$}&ImageNet& \multicolumn{2}{c}{CC3M Val}\\ 
		&Acc&I2T&T2I \\
		\hline
		0.5  & 33.66 &  36.95 & 36.77 \\
		1  & \textbf{34.23} &  \textbf{37.13} & \textbf{36.85} \\
		2 & 33.87 & 36.75&36.76 \\
		10& 33.56 & 36.34&36.26 \\
		\hline
	\end{tabular}
	
	\label{ICL_lambda} 
\end{table}

\begin{table}[tbp]
	\centering
	\caption{Analysis of mask ratio for MFD. }
	\begin{tabular}{l|c|cc}  
		\hline
		\multirow{2}{*}{Mask ratio}&ImageNet& \multicolumn{2}{c}{CC3M Val}\\ 
		&Acc&I2T&T2I \\
		\hline
		0 &  \textbf{34.23} & \textbf{37.13}&36.85 \\
		0.25 &  34.09 & 37.30&36.82  \\
		0.5 &  33.75 & 37.26 &36.74 \\
		0.75 &  33.79 & 37.07 &\textbf{36.87} \\
		\hline
	\end{tabular}
	
	\label{mask_ratio} 
\end{table}

%\begin{table}[tbp]
%	\centering
%	\caption{Analysis of mask ratio for MFD on T:ViT-B/16 and S:ViT-T/16. We report accuracy (Acc) on zero-shot ImageNet classification and R@1 of Image-to-Text (I2T) and Text-to-Image (T2I) retrieval on CC3M Val. The numbers in \textbf{bold} denote the best results. }
%		\begin{tabular}{l|c|cc}  
%			\hline
%			\multirow{2}{*}{Mask ratio}&ImageNet& \multicolumn{2}{c}{CC3M Val}\\ 
%			&Acc&I2T&T2I \\
%			\hline
%			0 &  34.73 & 38.13&37.85 \\
%			0.25 &  \textbf{34.77} & 38.30&37.82 \\
%			0.5 &  34.75 & 38.26 &37.74 \\
%			0.75 &  34.59 & \textbf{38.37} &\textbf{37.87} \\
%			\hline
%	\end{tabular}
%	
%	\label{lambda} 
%\end{table}

\section{Conclusion}
This paper provides a comprehensive empirical study on CLIP-KD by examining several distillation strategies, including relation, feature, gradient and contrastive paradigm. We evaluate the pre-trained CLIP models on zero-shot ImageNet classification and cross-modal retrieval. Experimental results show that the proposed distillation methods can lead to improvements on the student network, where the traditional feature mimicry with MSE loss still behaves the best on CLIP-KD. We hope our study could attract more attention to CLIP compression and provide a solid baseline for future research.

\textbf{Limitations.} Due to the resource limitation, we did not conduct CLIP experiments on larger-scale image-text dataset LAION-400M~\cite{schuhmann2021laion}, therefore the CLIP performance of zero-shot ImageNet classification or cross-modal retrieval may be lower than some previous works~\cite{li2022scaling,radford2021learning}. Moreover, using larger image encoder networks, such as ViT-L/14, ViT-H/14 and ViT-G/14~\cite{dosovitskiy2020image}, as the teacher is also valuable to be explored in CLIP-KD.

{\small
	\bibliographystyle{ieee_fullname}
	\bibliography{egbib}
}

\end{document}