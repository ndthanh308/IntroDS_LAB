\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx,psfrag,epsf}
\usepackage{enumerate}
%\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 


%\usepackage[utf8]{inputenc}
\usepackage{caption}
%\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{setspace}
%\usepackage{color}
\usepackage{graphics}
\usepackage{epstopdf}
\usepackage{lipsum}
%\usepackage{mwe}
\usepackage{bm}
%\usepackage[margin=1in]{geometry}
%\usepackage[]{mdframed}
\usepackage{bbm}
%\usepackage{comment}
\usepackage{placeins}
%\usepackage{soul}

%\usepackage{float}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[style=authoryear-ibid,backend=biber]{biblatex}
\addbibresource{PSIBiblio.bib}
 
\usepackage{arydshln}
\usepackage[usestackEOL]{stackengine} 

\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{Hypothesis}{Hypothesis}

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\usepackage{setspace}
\doublespacing
\begin{document}

%\bibliographystyle{natbib}

%\def\spacingset#1{\renewcommand{\baselinestretch}%
%{#1}\small\normalsize} \spacingset{1.45}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  \title{\bf The Population Resemblance Statistic: A Chi-Square Measure of Fit for Banking}
  \author{CJ Potgieter\thanks{Department of Mathematics, Texas Christian University}, C van Zyl\thanks{Absa Bank Limited; Centre for Business Mathematics and Informatics, North-West University}, WD Schutte\thanks{Absa Bank Limited; Centre for Business Mathematics and Informatics, North-West University}, F Lombard\thanks{Posthumous, Department of Statistics, University of Johannesburg }}
  \date{}
  \maketitle
  
\begin{abstract}
\onehalfspacing
The Population Stability Index (PSI) is a widely used measure in credit risk modeling and monitoring within the banking industry. Its purpose is to monitor for changes in the population underlying a model, such as a scorecard, to ensure that the current population closely resembles the one used during model development. If substantial differences between populations are detected, model reconstruction may be necessary. Despite its widespread use, the origins and properties of the PSI are not well documented. Previous literature has suggested using arbitrary constants as a rule-of-thumb to assess resemblance (or ``stability''), regardless of sample size. However, this approach too often calls for model reconstruction in small sample sizes while not detecting the need often enough in large sample sizes.

This paper introduces an alternative discrepancy measure, the Population Resemblance statistic (PRS), based on the Pearson chi-square statistic. Properties of the PRS follow from the non-central chi-square distribution. Specifically, the PRS allows for sample-size configured critical values, also dependent on the number of risk categories, and enables the specification of risk tolerances and power. The PRS is demonstrated to be universally competent in a simulation study and with real-world examples.  
\end{abstract}

\noindent%
{\it Keywords:}  credit risk monitoring, discrete goodness-of-fit, non-central chi-square, population stability index (PSI), information value
\vfill

\newpage
%\spacingset{1.45} % DON'T change the spacing!

\section{Introduction}
\label{sec:intro}

The Population Stability Index (PSI) is a commonly used measure for assessing the degree of discrepancy, conversely similarity, between two discrete probability distributions. The PSI is used in diverse fields including banking and credit risk modeling {(see \textcite[pp. 155 ff.]{thomas2002credit} and \textcite[pp. 368 ff.]{siddiqi2017intelligent}), insurance, healthcare, and engineering {(see \textcite{huang}, \textcite{li}, \textcite{sahu}, \textcite{dong2022prediction}, \textcite{wu2010enterprise}, \textcite{mcadams2022risk}, \textcite{chou2022expert} and \textcite{karakoulas2004empirical})}.} The PSI measure is based on the Kullback-Leibler divergence, measuring difference between two probability distributions {\parencite[eq. (2.6)]{kullback1951information}}. Its primary use in the banking sector is to monitor the evolution of the population underlying a model, for example scores of applicants in a scorecard, and to determine whether the current population \textit{resembles} the one used during model development, as required by prudential authorities (see \textcite{ecb2019internalmodels, ecb2023validation}, \textcite{federalreserve2023sr1107a1}, \textcite{resbank2022credit}; and \textcite{pruitt2010applied} for an application of the PSI in $SAS^\text{\textregistered}$). If there is a significant \textit{discrepancy}, the model may be at risk of misstating the outcome of interest. Here, the word \textit{resembles} is used in the sense of difference by no more than a specified small deviation - see Definition \ref{deltaresemblance} in Section \ref{sec: measuring resemblance}. {\textcite[p. 106]{lewis1994introduction} omits formulating a hypothesis in the statistical sense, but also describes the problem as ``If a user finds the distribution of scores \textit{close together}, [they] can be confident that the population has not changed.''}

Despite its widespread use, the origins and properties of the PSI are not widely understood. The earliest reference to this measure can be found in \textcite{lewis1994introduction}, who also coined the term ``Population Stability Index'', used therein as a quantification of the degree of \textit{resemblance} {(conversely, \textit{discrepancy}) }between the two distributions. Lewis suggests {in his example} that a PSI value under 0.10 indicates that the current population \textit{resembles} the original and no action is required. A value between 0.10 and 0.25 suggests that some investigation should be undertaken to identify the source of the difference. A value over 0.25 suggests that there has been a substantial change in either the incoming population or the policies of the user and may prompt a model reconstruction. The thresholds 0.10 and 0.25 will henceforth be termed \textit{Lewis constants}. {The use of these constants as a ``rule of thumb'' {are} popular in the literature and practice (see the popular credit risk modeling texts of \textcite[p. 155 ff.]{thomas2002credit} and \textcite[p. 368 ff.]{siddiqi2017intelligent}). These thresholds, however, are not universally accepted and their arbitrary nature has been acknowledged by several authors {(for example, \textcite{yurdakul2020statistical} and \textcite{dupisanievisagie2020})} and practitioners \parencite{moodys2021email}. Therefore, it's crucial for practitioners to be aware of the limitations of this measure and to interpret the results with caution.}

{In this paper, we present an easy-to-use and novel alternative discrepancy measure - the {Population Resemblance statistic (PRS)} - based on the Pearson chi-square statistic and the non-central chi-square distribution. In Section \ref{sec: measuring resemblance} we formalize the problem and propose the PRS as solution, while listing some additional measures available in the literature. The PRS offers statistically well-founded properties, see Section \ref{sec: PRS properties}, resulting in sample-size dependent critical values (Section \ref{sec:Critical Values}). We demonstrate the competency of the PRS measure in Section \ref{sec: PRS simulation} by means of a simulation study, followed by a real-world application (Section \ref{Application}), comparing the PRS with the popular PSI and some other measures. These examples serve to illustrate the competency of the PRS in small sample sizes often encountered in low default portfolios. }


\section{Measuring population resemblance}
\label{sec: measuring resemblance}
{Consider a set of observed ordinal scores, denoted $x_1, x_2, \ldots, x_n$, assumed to be a random sample from a discrete population with distribution function $F$ taking values in the set $\{1,2,\ldots,B\}$. In practice, the ordinal scores are typically realized by discretizing numerical values into pre-defined categories. Thus, each score can be thought of as indicating membership in one of $B$ possible categories. Let $n_i$ represent the number of scores in category $i$, i.e. $n_i = \sum_{j=1}^{n} \mathbb{I}(x_j = i)$. Here $\mathbb{I}(A)$ is the indicator function that equals $1$ when $A$ is true and $0$ otherwise. The total number of observed scores can be recovered through $n = \sum_{i=1}^{B} n_i$. The true category probabilities are $p_{i} = F(i)-F(i-1)$ for $i=1,\ldots,B$, while the estimated probabilities based on the observed scores are $\hat{\mathbf{p}} = (\hat{p}_1, \hat{p}_2, \ldots, \hat{p}_B)$ where $\hat{p}_i = n_i/n$ for $i = 1, 2, \ldots, B$.  }

The aim is to determine whether the current population $\mathbf{p}$ resembles the population used to construct the model, say $\mathbf{p}_0 = (p_{01}, p_{02}, \ldots, p_{0B})^\top$. The nature of this model is not germane. Rather, the question is whether there is evidence that $\mathbf{p}$ has changed ``substantively'' from, or still ``resembles'', the model construction $\mathbf{p}_0$. In practice, the current population $\mathbf{p}$ is unknown, but $\hat{\mathbf{p}}$ represents an unbiased estimator of this quantity. In fact, then the random variable $n\hat{\mathbf{p}}$ follows a multinomial distribution with $\mathrm{E}[n\hat{\mathbf{p}}] = n\mathbf{p}$ and $\mathrm{Var}[n\hat{\mathbf{p}}] = n\big[\mathrm{diag}(\mathbf{p})-\mathbf{p}\mathbf{p}^\top\big].$

A ubiquitously used measure for comparing model and current populations is the Population Stability Index (PSI) of \textcite{lewis1994introduction}, defined as
\begin{equation}
\mathrm{PSI} = \sum_{j=1}^{B} (\hat{p}_{j}-p_{0j})(\log{\hat{p}_j}-\log{p_{0j}})\mathbb{I}(\hat{p}_{j}>0),
\label{PSI_eqn}
\end{equation}
which is a consistent estimator of the symmetric Kullback-Leibler divergence
\begin{equation}
	\label{Kullback J2}
	J := J(\mathbf{p},\mathbf{p}_0) = \sum_{j=1}^{B} ({p}_j-{p}_{0j})(\log{{p}_j}-\log{{p}_{0j}}).
\end{equation}
Similar measures used in risk modeling that are estimators of $J$ include the information value (IV) \parencite[p. 184]{siddiqi2017intelligent} and characteristic (or system) stability index (CSI) \parencite[p. 369]{siddiqi2017intelligent}. Current implementation of the PSI does not rely on any considerations of the underlying statistical properties, instead making use of arbitrary threshold values proposed by \textcite{lewis1994introduction} to evaluate the evidence for or against population stability. Suggested thresholds, also arbitrary, for the IV can be found in \textcite[p. 185]{siddiqi2017intelligent}, and are different from the Lewis constants.

An alternative measure comparing two distributions is the chi-square divergence, 
\begin{equation}
    \label{ChiSq}
	\chi^2 := \chi^2(\mathbf{p},\mathbf{p}_0) = \sum_{i=1}^{B} \frac{(p_i-p_{0i})^2}{p_{0i}}.
\end{equation}
Based on the $\chi^2$ divergence, we introduce the Population Resemblance Statistic
\begin{equation}
\mathrm{PRS} = \sum_{j=1}^{B} \frac{(\hat{p}_{j}-p_{0j})^2}{p_{0j}},
\label{PR_eqn}
\end{equation}
noting that $\mathrm{PRS}$ is a consistent estimator of $\chi^2$. The interested reader is referred to Appendix \ref{sec: info theory} for an in-depth discussion of the information-theoretic perspective on the Kullback-Leibler and chi-square divergence measures. 

Another measure of fit is the Kolmogorov-Smirnov (KS) statistic \parencite{dagostino1986goodness}, in this context defined as
$$\mathrm{KS} = \max_{j=1,\ldots,B} |\hat{F}(j) -  F_0(j)| = \max_{j=1,\ldots,B} \Big|\sum_{i=1}^{j}\hat{p}_i -  \sum_{i=1}^{j} p_{0i}\Big| $$
where $F_0$ denotes the distribution function of the model construction population. In discrete populations, unlike the continuous setting where the KS statistic is asymptotically distribution-free, the distribution depends upon $\mathbf{p}_0$, see \textcite{conover1972kolmogorov}. Furthermore, in smaller samples, the distribution only has a few realizable values, creating a challenge when trying to perform inference at specific significance levels.

Other measures that have been considered in the literature include the coefficient of overlap,  $\mathrm{OVL} = \sum_{j=1}^{B} \min(\hat{p}_j,p_{0j})$, see \textcite{inman1984behavior}. It is easily verified that $\mathrm{OVL}\geq \mathrm{KS}/2 - 1$ and \textcite{komaba2022novel} prove the equivalence of $KS$ and $OVL$. More recently, \textcite{dupisanievisagie2020} suggested use of $\mathrm{DPV}=\max_{j=1,...,B^*} |p_j-p_{0j}|/p_{0j},$
based on only the first $B^*\leq B$ categories and using a Monte Carlo simulation to determine thresholds. Only simulation studies are available for the properties of $\mathrm{DPV}$ and the authors restrict their discussions to sample sizes in excess of $10\ 000$. Similar to the discrete $KS$, the $\mathrm{DPV}$ thresholds are dependent upon $\mathbf{p_0}$. Finally, a general class of power-divergence statistics, denoted by $\mathrm{I}_{\gamma}$ is given in \textcite{cressie1984multinomial}. When appropriately normed, the PSI, PRS, and I$_\gamma$ measures all converge to a chi-square distribution for $\mathbf{p}=\mathbf{p}_0$.

This paper does not aim to provide a comprehensive review and comparison of these different measures. Rather, the interested reader is referred to \textcite{agresti2012categorical} for an exposition of the discrete goodness-of-fit problem. We focus on discussing the deficiencies of the PSI, both as currently implemented and as a measure of fit. We then show how the PRS alleviates these concerns. Furthermore, we will establish statistical properties of the PRS not only when the two populations are equal, but also under an assumption of population drift where the current population has shifted relative to the model construction population. To this end, we introduce a notion of two populations being $\delta$-resemblant, and consider the behavior of the PRS within this framework.

\begin{definition}\label{deltaresemblance}
Let $\delta>0$. For the probability vector $\mathbf{p}_0=(p_{01},\ldots,p_{0B})$, define the region  $$\mathcal{P}(\delta|\mathbf{p}_0) = \Bigg\{\tilde{\mathbf{p}} = (\tilde{p}_1,\ldots,\tilde{p}_B) : \max_{j=1,\ldots,B} |p_{0j} - \tilde{p}_j| \leq \delta,\ \sum_{j=1}^{B}\tilde{p}_j = 1 \Bigg\}.$$ The probability vector $\mathbf{p}=(p_{1},\ldots,p_{B})$ is said to be $\delta$-resemblant of $\mathbf{p}_0$ whenever $\mathbf{p}\in \mathcal{P}(\delta|\mathbf{p}_0)$.
\end{definition}

Intuitively, $\mathcal{P}(\delta|\mathbf{p}_0)$ represents the set of all valid probability vectors where the largest deviation from any element in $\mathbf{p}_0$ is no greater than $\delta$, a tolerance constant. To illustrate, say we have $\mathbf{p}_0 = (0.25,0.25,0.25,0.25)$. Consider $\delta=0.02$. For probabilities $\mathbf{p}_1 = (0.23,0.25,0.27,0.25)$, the maximum deviation between any two corresponding categories is $0.02$, meaning $\mathbf{p}_1$ is inside $\mathcal{P}(0.02|\mathbf{p}_0)$ and is therefore said to $\delta$-resemble $\mathbf{p}_0$ at $\delta=0.02$. On the other hand, $\mathbf{p}_2 = (0.20,0.25,0.30,0.25)$ is not inside $\mathcal{P}(0.02|\mathbf{p}_0)$ as some probabilities differ from $\mathbf{p}_0$ by more than $0.02$. Thus, $\mathbf{p}_2$ does not $\delta$-resemble $\mathbf{p}_0$ at $\delta=0.02$.

We formulate the hypothesis for a population change as follows: \textit{$H_0 : \mathbf{p} \in \mathcal{P}(\delta|\mathbf{p}_0)$ against $H_A : \mathbf{p} \not\in \mathcal{P}(\delta|\mathbf{p}_0)$ for $\mathbf{p_0}$ fixed and known.} In the context of credit risk model monitoring, such a composite null hypothesis connecting to Lewis' \parencite{lewis1994introduction} statement of ``closeness'' has to date not been formulated. To our knowledge, the point null hypothesis involving the equality $\mathbf{p}=\mathbf{p}_0$ is referenced universally (see, for example, \textcite{pisanie2023critical}). 

We conclude this section by noting that a two-sample formulation of the problem, i.e. treating $\mathbf{p_0}$ as obtained through random sampling from the model constriction population, may have some appeal. In practice, however, this approach faces several challenges including dependence between the model construction and current sample, which may stem from at least temporal evolution and overlapping data. To address the issue of non-independence between the samples, we have thus resorted to a one-sample problem formulation, treating the model construction probabilities $\mathbf{p_0}$ as fixed and known. Even in situations as described where the model construction probabilities were established using sampling tools, the one-sample approach can still be employed, providing a conditional inference solution. The statistical properties of the PSI and PRS under the one-sample framework are now explored both under an assumption of equality as well as an assumption of $\delta$-resemblance. 


\section{Statistical properties of Population Resemblance}
\label{sec: PRS properties}

\subsection{Comparing the PSI and PRS}
\label{sec: comp PSI PRS}

In this subsection, we consider properties of the PSI as in \eqref{PSI_eqn} and the PRS as in \eqref{PR_eqn} under the assumption that the current population $\mathbf{p}$ is equal to the model construction population $\mathbf{p}_0$. Define $T_n = n\cdot \mathrm{PSI}$ and $Q_n = n\cdot \mathrm{PRS}$. Both $T_n$ and $Q_n$ have limiting $\chi^2$ distributions with $B-1$ degrees of freedom. Despite the asymptotic distribution of $T_n$ being well-established \parencite[Chapter 6]{kullback1978information}, in his example \textcite{lewis1994introduction} suggests PSI thresholds of $0.1$ and $0.25$, with $\mathrm{PSI}< 0.1$ indicating the current population is similar to the original and no action is necessary, and $\mathrm{PSI}\geq 0.25$ indicating model reconstruction is required. Henceforth, we refer to such unmotivated constants as ``thresholds'', whereas the term ``critical values'' will allude to decision boundaries based on probabilistic assumptions.

A simulation study presented below illustrates concerns around decision-making reliant on the Lewis constants. Data were generated under two scenarios, both having equal model construction category probabilities, $\mathbf{p}_0 = (1/B,\ldots,1/B)$. The first simulation scenario assumed no population shift, so that $\mathbf{p}=\mathbf{p}_0$ and $J(\mathbf{p},\mathbf{p}_0) = 0$. The second scenario assumed the current probabilities $\mathbf{p}$ differ from the model construction probabilities $\mathbf{p}_0$ so that $J(\mathbf{p},\mathbf{p}_0) = 0.1$ to be consistent with reference of ``closeness'' by \textcite[p. 106]{lewis1994introduction}. Of interest is evaluating the probability that model reconstruction is recommended, $P(\mathrm{PSI}\geq 0.25).$ These estimated probabilities summarized in Table \ref{TABLE1} were calculated from $K=1\ 000\ 000$ simulated data sets under each configuration, giving a maximum standard error of simulation close to $0.0005$.

\begin{table}[h!]
  \centering
\caption{Monte Carlo estimates of $P(\mathrm{PSI}\geq 0.25)$ under two scenarios.}
\begin{tabular}{|c|c|c|c|c|}
 \cline{1-5} 
\multicolumn{1}{|c|}{} & \multicolumn{2}{|c|}{$J=0$} & \multicolumn{2}{|c|}{$J=0.1$} \\  
 \cline{2-5} 
\multicolumn{1}{|c|}{$n$} & $B=5$ & $B=10$ & $B=5$ & $B=10$ \\ \hline
$50$ & 0.0226 & \textbf{0.2356} & \textbf{0.2542} & \textbf{0.5459}\\
$100$ &  0.0001 & 0.0086 & 0.0872 & \textbf{0.2508} \\ 
$200$ & \textit{0.0000} & \textit{0.0000} & 0.0131 & 0.0434 \\  
$500$ &  \textit{0.0000} & \textit{0.0000} & \textit{0.0001} & \textit{0.0003} \\ \hline 
\end{tabular}
\label{TABLE1}
\end{table}

Table \ref{TABLE1} clearly indicates that the probability of mandating a model reconstruction is unacceptably high in several cases where $n$ is small (indicated in boldface). Moreover, where $n$ is larger, this probability is negligibly small - almost equal to zero (indicated in italics) even when moderate population shift has taken place. This simulation study highlights the need to find more appropriate measures and/or critical values as the universal application of the Lewis constants disregards both the number of categories $B$ and the sample size $n$. 

A plausible motivation for the Lewis constants could have been that these values correspond to specific levels of discrepancy between the model construction population and the current population. Appendix \ref{sec: model deviance} examines this possible link by setting the PSI equal to one of the Lewis constants and then evaluating the implication for the underlying population, thereby gaining insight into the degree of resemblance between the two populations. We conclude that this was unlikely to be the case: the PSI is unbounded and the maximum and minimum deviance given a specified PSI is rapidly increasing; and a small deviation can result in a large PSI value.

Given the limiting $\chi^2_{B-1}$ distributions of $T_n$ and $Q_n$ under an assumption of no population shift, a naive approach to assessing population resemblance would be to consider two discrete populations to resemble one another if and only if their distribution functions are strictly equal. This can be formulated as the simple null hypothesis $H_0: \mathbf{p} = \mathbf{p}_0$ with alternative a negation of the null, $H_a: \mathbf{p} \neq \mathbf{p}_0$. Under this naive approach (based on the same principle as in \textcite{yurdakul2020statistical}), PSI and/or PRS critical values to assess population resemblance can be calculated as  $c_{\mathrm{l}}=F_{B-1}^{-1}(1-\alpha_l)/n$ and $c_{\mathrm{u}}=F_{B-1}^{-1}(1-\alpha_u)/n$ where $F_{m}^{-1}(\cdot)$ denotes the inverse $\chi^2$ distribution function with $m$ degrees of freedom. The significance levels $\alpha_l > \alpha_u$ are pre-specified based on the specific context and requirements of the population comparison being carried out.

However, this naive approach does not alleviate other concerns around the PSI. Under the assumption of population equality, we have $\mathrm{E}[T_n] \xrightarrow[\infty]{n} B-1$ and $\mathrm{E}[Q_n] \xrightarrow[\infty]{n} B-1$. Similarly, we have
$\mathrm{Var}[T_n] \xrightarrow[\infty]{n} 2(B-1)$ and $\mathrm{Var}[Q_n] \xrightarrow[\infty]{n} 2(B-1)$. The small-sample stability of the PSI and PRS can be evaluated using the mean stability and variance stability ratios, respectively,
$$R_{T_n}^{(1)} = \frac{\mathrm{E}[T_n]}{B-1} \qquad \mathrm{and} \qquad R_{T_n}^{(2)} = \frac{\mathrm{Var}[T_n]}{2(B-1)}$$ with similar definitions holding for $R_{Q_n}^{(1)}$ and $R_{Q_n}^{(2)}$. The required finite-sample expectations and variances can be calculated to arbitrary accuracy using Monte Carlo sampling. This was done for sample sizes $n$ ranging from $20$ to $1\ 000$, and a total of $K=2\ 000\ 000$ Monte Carlo realizations of $T_n$ and $Q_n$ at each $n$. The estimated ratios $R^{(1)}$ and $R^{(2)}$ are shown in Figures \ref{fig: Mean Stab Ratios} and \ref{fig: Var Stab Ratios} for $B=5$.

% Figure environment removed

The figures make it evident that the normalized PRS ($Q_n$) demonstrates a pattern of mean stability even at the smallest sample size considered. Moreover, it exhibits variance stability for sample sizes surpassing 50, with the stability ratio being close to 1. On the other hand, the normed PSI ($T_n$) has small-sample behavior that depends greatly on the underlying sample size and the empirical mean (variance) at times exceeds the asymptotic mean by more than $8\%$ ($30\%$). Of course, close empirical means and variances to their asymptotic counterparts do not establish convergence. Conversely, disparities signal a lack of convergence, raising doubts about the utility of the asymptotic distribution. Although we're not obligated to depend on asymptotic results, it certainly simplifies matters when these can be relied upon as reasonable approximations in finite sample settings. To this end, we argue that the PRS is a more natural choice than the currently used PSI.


\subsection{PRS properties under $\delta$-resemblance} \label{sec: PRS properties1}

In this subsection, we consider the behavior of the PRS based on the assumption that the two populations are $\delta$-resemblant and $\delta \leq \min_{i=1,\ldots,B} p_{0i}$. Specifically, we only consider the case where the tolerated deviation is no greater than the smallest category probability. To this end, it is important to understand the behavior of $Q_n = n\cdot \mathrm{PRS}$ not just when $\mathbf{p}_0$ represents the true value, but for all distributions in $\mathcal{P}(\delta|\mathbf{p}_0)$. For each $j=1,\ldots,B$, let $\delta_j = \xi_j/\sqrt{n}$ with $\xi_j$ a fixed real number, and assume the true current probabilities satisfy $p_j = p_{0j} + \delta_j = p_{0j} + \xi_j/\sqrt{n}$. Then, $Q_n  \xrightarrow[\infty]{n} \chi_{B-1}^{2}(\lambda)$ with $$\lambda = \sum_j \frac{\xi_j^2}{p_{0j}} = \sum_j \frac{n\ (p_j - p_{0j})^2}{p_{0j}},$$ that is, the normed PRS converges to a non-central chi-square distribution with $B-1$ degrees of freedom and non-centrality parameter $\lambda$. 

Since the true value of the current probability vector $\mathbf{p}$ is unknown, the most conservative approach is to base a statistic on the largest possible value of the non-centrality parameter when the populations are $\delta$-resemblant. This requires calculating
\begin{equation}
    \lambda_{\mathrm{sup}} = \sup_{\mathbf{p}\in\mathcal{P}(\delta|\mathbf{p}_0)} \sum_j  \frac{n\ (p_j - p_{0j})^2}{p_{0j}}.
    \label{eq:noncentral parm}
\end{equation} Details of simplifying this optimization problem in a general context are presented in Appendix \ref{sec: Appendix 1}. Letting $\mathbf{a}=(a_1,\ldots,a_B)$ be a vector with entries $a_j\in\{-1,0,1\}$ and defining $$h(\mathbf{a},\mathbf{p}) = \frac{\sum_j a_j}{\sum_j p_j(1-x_j^2)} \ \mathrm{for}\ \sum_j a_j^2 < B$$ with $h(\mathbf{a},\mathbf{p})$ taking the value $0$ otherwise, we have $$\lambda_{\mathrm{sup}} = n\delta^2 \max_{\mathbf{a}:a_j\in\{- 1,0,1\}} \sum_{j=1}^{B}\frac{a_j^2-h^2(\mathbf{a},\mathbf{p}_0)(1-a_j^2)p_{0j}}{p_{0j}}.$$
Thus, the non-centrality parameter is proportional to $n$, the sample size, and $\delta^2$, the square of the specified tolerance constant. The remaining maximization problem can be found through an exhaustive search of the $3^B$ possible vectors $\mathbf{a}$ with entries restricted to $\{-1,0,1\}$. In the special case where the reference distribution specifies equal model construction probabilities, $\mathbf{p}_{0}=(1/B,\ldots,1/B)$, we have $$\lambda_{\mathrm{sup}} =
\left\{
	\begin{array}{ll}
		nB^2\delta^2  & \mbox{if } B\text{ is even,} \\
		nB(B-1)\delta^2 & \mbox{if } B\text{ is odd.}
	\end{array}
\right.$$

\subsection{Calculating critical values for the PRS}
\label{sec:Critical Values}

There are two distinct approaches one can take to determine critical values for the PRS criterion. The first, a \textit{direct} approach, requires the specification of the tolerance constant $\delta$ for assessing when two distributions are considered $\delta$-resemblant. In contrast, the \textit{indirect} approach treats $\delta$ as an implicit parameter and instead requires the specification of a multiplicative factor $M > 1$. This factor signifies the magnitude increase in population shift that is deemed unacceptable, hence mandating reconstruction if the populations do not exhibit $M\delta$-resemblance. Furthermore, the indirect approach mandates the specification of a {probability at which model reconstruction is deemed necessary when the populations fail to meet the $M\delta$-resemblance criterion, hereafter called the power level.}

While the \textit{direct} approach may initially seem simpler, note that $\delta$ should be chosen inversely proportional to the sample size, i.e., $\delta = C\cdot n^{-1/2} + o(n^{-1/2})$ for a constant $C>0$, to ensure a valid approximation with respect to the non-central $\chi^2$ limiting distribution. Ensuring objective selection of such a $\delta$ can be a challenge. On the other hand, the indirect approach not only provides critical values but also results in a tolerance constant $\delta$ that is dependent on the sample size and adjusted for the required rejection power, offering a more comprehensive solution. We advocate for the indirect approach, but implementation of both solutions is outlined here.

For the direct approach, assume that a tolerance level $\delta$ has been specified along with significance levels $\alpha_{l}>\alpha_{u}$. This approach sets critical values $c_l = F^{-1}(1-\alpha_l|B-1,\lambda)/n$ and $c_u = F^{-1}(1-\alpha_u|B-1,\lambda)/n$ where $F^{-1}(q|m,\lambda)$ denotes the inverse distribution function of the non-central $\chi^2$ distribution with $m$ degrees of freedom and non-centrality parameter $\lambda$. When the current population has shifted maximally within the $\delta$-resemblance region, we have $Q_n = n\cdot \mathrm{PRS}\rightarrow \chi^2_{B-1}(\lambda_{\mathrm{sup}})$ so that $P(\mathrm{PRS} < c_l) \approx 1-\alpha_l$ and $P(\mathrm{PRS} \geq c_u) \approx \alpha_u$. Thus, when $\mathrm{PRS} < c_l$,  we deem the current population to be $\delta$-resemblant, when $c_l \leq \mathrm{PRS} < c_u$ further investigation is warranted, while when $\mathrm{PRS} \geq c_u$ the current population is deemed to no longer be $\delta$-resemblant and model reconstruction is required. 

For the indirect approach, we specify a level $M>1$ indicating the relative increase in population shift that is deemed unacceptable. We also specify the probability (power) $1-\beta$ with which model reconstruction will be required if the current population is, in fact, $M\delta$-resemblant. The tolerance level $\delta$ is then found relative to the specified values of $M$ and $1-\beta$. The values $\alpha_l > \alpha_u$ are the same as before, and the corresponding critical values are calculated for a $\delta$-resemblant population. Specifically, the critical value $c_u$ as well as the implied tolerance level $\delta$ are found by simultaneously solving the two equations (in terms of $c_u$ and $\lambda$),
\begin{equation}
    1 - F(c_u|B-1,\lambda) = \alpha_u\quad \mathrm{and}\quad F(c_u|B-1,M^2\lambda) = \beta. \notag 
\end{equation}
The tolerance $\delta$ is subsequently recovered from the non-centrality parameter $\lambda$ as per Section \ref{sec: PRS properties}. It's worth taking a closer look at the above two equations. Note that the upper critical value $c_u$ still suggests model reconstruction in a $\delta$-resemblant population with an approximate probability of $\alpha_u$. However, if the current population exhibits $M\delta$-resemblance, model reconstruction is now recommended with approximate probability of $1-\beta$. Finally, the lower critical value is found as with the direct method, $c_l = F^{-1}(1-\alpha_l|B-1,\lambda).$

The approach described above may appear unconventional. In many statistical applications, the desired power under a specific alternative hypothesis is used to calculate the required sample size. However, in the current context, the sample size is predetermined. For example, in banking, a specific portfolio has a fixed number of clients. Thus, we manage the statistical power of the procedure by allowing the acceptable and unacceptable tolerance levels, respectively $\delta$ and $M\delta$, to be determined based on the required power.

To illustrate, Figure \ref{fig:Mdelta} shows the implied tolerance and the corresponding unacceptable deviation levels as $M$ increases. In this illustration, for $n=500$ and $B=10$, we determine $\delta$ if the desired power at $M\delta$ is $0.9$ and the desired significance level at a deviation of $\delta$ is $\alpha_u=0.01$. Table \ref{tab:Critical values} shows the critical values according to the indirect method for a range of $n,\ B$ and $M \in (5,7.5)$, using significance levels of $\alpha=1\%(10\%)$ to obtain $c_u(c_l)$.


% Figure environment removed

\begin{table}[h!]
  \centering
  \caption{ Critical values according to the indirect method and implied tolerance values $\delta_{tol}$ for the PRS with $M=5$ and $M=7.5$.}
    \begin{tabular}{|c|ccc|ccc|}
    \hline
          & \multicolumn{3}{c|}{$M=5$} & \multicolumn{3}{c|}{$M=7.5$}\\
    $(n,B)$ & $\delta_{\mathrm{tol}}$ & $c_l$ & $c_u$ & $\delta_{\mathrm{tol}}$ & $c_l$ & $c_u$ \\
    \hline
    $(50, 3)$ & 0.05416 & 0.13052 & 0.24958 & 0.03397 & 0.10776 & 0.21304 \\
    $(50, 5)$ & 0.03141 & 0.19252 & 0.32364 & 0.01998 & 0.17087 & 0.29060 \\
    $(500, 10)$ & 0.00487 & 0.03315 & 0.04873 & 0.00313 & 0.03096 & 0.04568 \\
    $(2\ 000, 10)$ & 0.00243 & 0.00829 & 0.01218 & 0.00157 & 0.00774 & 0.01141 \\
    $(10\ 000, 20)$ & 0.00061 & 0.00293 & 0.00389 & 0.00039 & 0.00281 & 0.00374 \\
    \hline
    \end{tabular}
  \label{tab:Critical values}
\end{table}


\section{Competency of the PRS method: a simulation study}
\label{sec: PRS simulation}

A comprehensive simulation study was conducted to evaluate the performance of the proposed PRS procedure when applying the indirect calibration method to obtain critical values (see Section \ref{sec:Critical Values}). We assessed the effectiveness of the PRS under a range of sample sizes ($n$), numbers of categories ($B$), and the relative deviation from the model construction probabilities. The true model construction probabilities were characterized by equi-probable categories, $\bm{p}_0 = \left(1/B, \ldots, 1/B\right)$. We set the significance levels at $(\alpha_l, \alpha_u) = (0.1, 0.01)$ with a specified power of $0.9$ and utilizing a multiplier factor $M=5$. Let $\delta_{\mathrm{tol}}$ denote the implied tolerance associated with this approach, and let $(c_l,c_u)$ denote the associated lower and upper critical values.

We used a Monte Carlo approach to assess the performance of the PRS under different population models that deviated from $\bm{p}_0$. For a given tolerance deviation $\delta_v$, we constructed a success probability vector $\bm{p}_v$ as follows: For categories $j \leq B/2$, we have $p_{v,j} = 1/B - \delta_v$, for categories $j \geq B/2 + 1$, we have $p_{v,j} = 1/B + \delta_v$, and when $B$ is odd the central category has $p_{v,(B+1)/2} = 1/B$. {This corresponds to maximal $\delta_v$-resemblant deviation from $\bm{p}_0$.}


We consider three populations that are $\delta_{\mathrm{tol}}$-resemblant with $\delta_v \leq \delta_{\mathrm{tol}}$ signifying that the simulated deviation from the original model does not exceed the tolerance level. For these simulations, the current population should be deemed ``\textit{resemblant}'' (indicating insufficient change to warrant reconstruction).  The \textit{acceptable} deviation levels considered were $\delta_v \in \{0, \delta_{\mathrm{tol}}/2, \delta_{\mathrm{tol}}\}$. For the case where $\delta_v = \delta_{\mathrm{tol}}$, we expect $P(\mathrm{PRS}_j < c_l) = 1-\alpha_l = 0.9$ and $P(\mathrm{PRS}_j \geq c_u) = \alpha_u = 0.01$.

Next, we consider three populations that are not $\delta_{\mathrm{tol}}$-resemblant with \textit{unacceptable} deviation levels, $\delta_v \in \left\{(1+\Delta)\delta_{\mathrm{tol}},(1+2\Delta)\delta_{\mathrm{tol}},(1+3\Delta)\delta_{\mathrm{tol}}\right\}$ and $\Delta = (M-1)/3$. In these scenarios, $\delta_v > \delta_{\mathrm{tol}}$, signifying the simulated deviation exceeds the tolerance level, and therefore the current population should be deemed ``\textit{discrepant}'', and reconstruction mandated.  Furthermore, the largest deviation considered is $M\delta_{\mathrm{tol}}$, indicating the point where the procedure is calibrated to have power $P(\mathrm{PRS}_j \geq c_u) = 0.9$. 

For each unique configuration of $(n,B,\bm{p}_v)$, we generated $J=10^6$
independent random vectors $\bm{X}_j$, each distributed according to $\mathrm{Multinomial}(\bm{p}_v)$, $j=1,\ldots,J$. We subsequently computed estimated model probabilities $\hat{\bm{p}}_j$ and used this information to evaluate PRS$_j$ for the $j$th sample and using $\bm{p}_0$ as the reference. We finally calculated the proportion of samples where PRS$_j$ falls into the three categories, $\{\mathrm{PRS}_j < c_l\}$, $\{c_l \leq \mathrm{PRS}_j < c_u\}$, and $\{\mathrm{PRS}_j \geq c_u\}$. Selected results for $n\in\{50,100\}$ are presented in Tables \ref{tab:EmpClassB5n50 - 1} through \ref{tab:EmpClassB10n100 - 1}. These are representative of the broader trends observed through our extensive simulation study.


\begin{table}[h!]
    \centering
        \caption{Empirical classification probabilities for indirect method with $(B,n)=(5,50)$ with $M=5$, $(\alpha_l,\alpha_u)=(0.1,0.01)$, and $\text{power}=0.9$ corresponding to $\delta_{\mathrm{tol}}=0.031$.
    }
    \label{tab:EmpClassB5n50 - 1}

    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Status & $\delta_v$ & $\hat{P}(\mathrm{PRS} < c_l)$ & $\hat{P}(c_l \leq \mathrm{PRS} < c_u)$ & $\hat{P}(\mathrm{PRS} \geq c_u)$ \\
    \hline
Acceptable & 0.000  & 0.956  & 0.041  & 0.003 \\
  & 0.016  & 0.945  & 0.050  & 0.004 \\
  & 0.031  & \textbf{0.907}  & 0.083  & \textbf{0.010} \\
  \hline
 Unacceptable & 0.073  & 0.599  & 0.229  & 0.102 \\
  & 0.115  & 0.133  & 0.352  & 0.514 \\
  & 0.157  & 0.002  & 0.038  & 0.961 \\
 \hline
    \end{tabular}
\end{table}

\begin{table}[h!]
    \centering
        \caption{Empirical classification probabilities for indirect method with $(B,n)=(5,100)$ with $M=5$, $(\alpha_l,\alpha_u)=(0.1,0.01)$, and $\text{power}=0.9$ corresponding to $\delta_{\mathrm{tol}}=0.022$.
    }
    \label{tab:EmpClassB5n100 - 1}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Status & $\delta_v$ & $\hat{P}(\mathrm{PRS} < c_l)$ & $\hat{P}(c_l \leq \mathrm{PRS} < c_u)$ & $\hat{P}(\mathrm{PRS} \geq c_u)$ \\
    \hline
Acceptable & 0.000 & 0.953 & 0.044 & 0.003 \\
  & 0.011 & 0.941 & 0.055 & 0.004 \\
  & 0.022 & \textbf{0.902} & 0.089 & \textbf{0.010} \\
  \hline
 Unacceptable & 0.052 & 0.586 & 0.307 & 0.107 \\
  & 0.081 & 0.145 & 0.345 & 0.510 \\
  & 0.111 & 0.006 & 0.067 & 0.927 \\
 \hline
    \end{tabular}

\end{table}

\begin{table}[h!]
    \centering
        \caption{Empirical classification probabilities for indirect method with $(B,n)=(10,100)$ with $M=5$, $(\alpha_l,\alpha_u)=(0.1,0.01)$, and $\text{power}=0.9$ corresponding to $\delta_{\mathrm{tol}}=0.011$.
    }
    \label{tab:EmpClassB10n100 - 1}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    Status & $\delta_v$ & $\hat{P}(\mathrm{PRS} < c_l)$ & $\hat{P}(c_l \leq \mathrm{PRS} < c_u)$ & $\hat{P}(\mathrm{PRS} \geq c_u)$ \\
    \hline
Acceptable & 0.000 & 0.944 & 0.052 & 0.004 \\
  & 0.005 & 0.934 & 0.061 & 0.005 \\
  & 0.011 & \textbf{0.900} & 0.090 & \textbf{0.010} \\
  \hline
 Unacceptable & 0.025 & 0.617 & 0.287 & 0.096 \\
  & 0.040 & 0.164 & 0.353 & 0.483 \\
  & 0.054 & 0.006 & 0.066 & 0.927 \\
 \hline
    \end{tabular}

\end{table}

The competence of the PRS method is clear in the cases presented in Tables \ref{tab:EmpClassB5n50 - 1} to \ref{tab:EmpClassB10n100 - 1}. Specifically, when $\delta_v = \delta_{\mathrm{tol}}$ (the last ``Acceptable'' row in each table), the Monte Carlo probability of falling below the lower critical value $c_l$ is consistently close to $1-\alpha_l = 0.9$, while the equivalent estimate of falling above the upper critical value $c_u$ is close to $\alpha_u = 0.01$. This consistency underscores the accuracy of the asymptotic non-central chi-square distribution in determining the critical values. Similarly, when $\delta_v = M\delta_{\mathrm{tol}}$ (the last ``Unacceptable'' row in each table), the empirical probability of exceeding the upper critical value $c_u$ is consistently greater than the target power of $0.9$. Furthermore, when $\delta_v$ exceeds $\delta_{\mathrm{tol}}$, the empirical probability of exceeding $c_u$ escalates rapidly, underscoring prompt identification of deviations surpassing the acceptable tolerance level. 

It's worth noting that the present approach can face some limitations when the magnitude of deviance $\delta_{\mathrm{tol}}$ is large relative to the number of categories $B$, equivalently corresponding to small values for the multiplier $M$. In such cases, not presented in this paper, we have observed instances where the exceedance probabilities $\hat{P}(\mathrm{PRS}\geq c_u)$ are smaller than would be anticipated under the limiting distribution. These cases are not ones encountered in practice. For example, when $B=10$ with equi-probable $p_{0j}=0.1$, this occurs when the target tolerable deviance is $0.05$ or larger, a rather large deviation when compared relatively to the reference $p_{0j}$.


\section{Real-world applications}
\label{Application}

In this section, we present (anonymized) real-world examples from a number of credit risk models. These examples utilize data sets with equi-probable construction categories (``risk buckets'') ${p_{0i}}=1/B, \ i=1,...,B$. The suite of models considered comprises a range of sample sizes and risk bucket configurations to demonstrate the behavior of the PRS method in practically plausible scenarios. We compare the PRS outcomes with the ubiquitously used PSI applying both the Lewis constants and critical values of \textcite{yurdakul2020statistical}, denoted by ``L'' and ``YN'' throughout. We acknowledge these don't cover cases where probabilities are based on segment features (e.g. a product type). Nonetheless, the comparison between the PSI and the PRS generalizes well and these are available from the authors. Of course, this is expected because the PRS is distribution-free with respect to $\mathbf{p_0}$. A further comparison is made with the well-known discrete Kolmogorov-Smirnov (KS) test. 

Tables \ref{tab: n=50 example} through \ref{tab:n=10000 B=20 example} report the PSI and PRS statistics as well as the p-value for the discrete Kolmogorov-Smirnov. In these tables we also assign the commonly used red-amber-green ($\mathbbm{rag}$) status used in banks ($\mathbbm{r}$ indicating exceedance of $c_u$, $\mathbbm{g}$ indicating non-exceedance of $c_l$, and $\mathbbm{a}$ is assigned otherwise). Given the discrete nature of the KS statistic, particularly in small samples, it is not feasible to match critical values at the $\alpha_l$ lower and $\alpha_u$ upper levels and we rather report the p-value, $p(\mathrm{KS})$ from the \texttt{dgof} package in \texttt{R} \parencite{RDGOF} and assign $\mathbbm{r}$ when $p<\alpha_u$, $\mathbbm{g}$ when $p\geq\alpha_l$ and $\mathbbm{a}$ otherwise. The designs considered are $(n,B)\in\{(50,3),\, (50,5),\, (500,10),\, (2\ 000,10),\, (10\ 000,20)\}$, and we universally apply significance levels $(\alpha_l,\alpha_u)=(10\%,1\%)$ for comparability. The YN-normed critical values are $c=2n^{-1}F^{-1}_{B-1}(1-\alpha)$ (in the notation of \textcite{yurdakul2020statistical} and assuming $m=n$), where $F_{\nu}^{-1}$ denotes the chi-square quantile function with $\nu$ degrees of freedom. For the PRS, we calculate critical values using the \textit{indirect} method, specifying $M \in(5,7.5)$ and a power of 90\%. These critical values appear in Table \ref{tab:Critical values} alongside the implied values of $\delta_{\mathrm{tol}}$. The choice of $M$, and indirectly $\delta_{\mathrm{tol}}$, is steered by a level of risk appetite acceptable to the bank. One can potentially use analogues of Figure \ref{fig:Mdelta} to guide this choice: for example, given $(n.B)=(500,10)$ the choices $M=5$ and $M=7.5$ is reasonable because $\delta_{\mathrm{tol}}<1/B$ is practically sensible and the decline in shift is diminishing with respect to $M$. 

\begin{table}[htbp]
  \centering
  \caption{Population resemblance comparison using the current population of size $n=50$ and observed category sample sizes $n_i$, $i=1,...,B$ for $B=3$ and 5.}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & ${\mathbf{n_i}}$ & PSI (L,YN) & \multicolumn{1}{l|}{PRS (5,7.5)} & \multicolumn{1}{l|}{p(KS)} \\
    \hline
    \multicolumn{5}{|c|}{$n=50,\ B=5,\ {n_{0i}}=10 \ i=1,...,5$} \\
    \hline
    $t_1$ & (6, 9, 10, 11, 14)& 0.072 ($\mathbbm{g,g}$) & 0.068 ($\mathbbm{g,g}$)& 0.699 ($\mathbbm{g}$)\\
    $t_2$ & (4, 10, 11, 11, 14)& 0.141  ($\mathbbm{a,g}$) & 0.108 ($\mathbbm{g,g}$)& 0.468 ($\mathbbm{g}$)\\
    $t_3$ & (7, 8, 8, 10, 17)& 0.114 ($\mathbbm{a,g}$) &  0.114 ($\mathbbm{g,g}$)& 0.281 ($\mathbbm{g}$)\\
    $t_4$ & (3, 8, 12, 13, 14)& 0.227  ($\mathbbm{a,g}$) & 0.164 ($\mathbbm{g,g}$)& 0.078 ($\mathbbm{a}$)\\
    $t_5$ & (2, 9, 12, 13, 14)& 0.310  ($\mathbbm{r,g}$) & 0.188 ($\mathbbm{g,a}$)& 0.078 ($\mathbbm{a}$)\\
    $t_6$ & (2, 5, 13, 14, 16)& 0.423  ($\mathbbm{r,a}$) & 0.300 ($\mathbbm{a,r}$)& 0.002 ($\mathbbm{r}$)\\
    \hline
    \multicolumn{5}{|c|}{$n=50,\ B=3,\ \mathbf{n_0}=(16,17,17)$}\\
    \hline
    $t_1$ & (12, 13, 25)& 0.109  ($\mathbbm{a,g}$) & 0.116 ($\mathbbm{g,a}$)& 0.124 ($\mathbbm{g}$)\\
    $t_2$ & (10, 20, 20)& 0.076  ($\mathbbm{g,g}$) & 0.066 ($\mathbbm{g,g}$)& 0.336 ($\mathbbm{g}$)\\
    $t_3$ & (10, 10, 30)& 0.278  ($\mathbbm{r,a}$) & 0.301 ($\mathbbm{r,r}$)& 0.002 ($\mathbbm{r}$)\\

    \hline
    \end{tabular}
  

  \label{tab: n=50 example}
\end{table}

\begin{table}[htbp]
  \centering
  \caption{Population resemblance comparison using the current population of size $n=500$ or $n=2\ 000$ and observed category sample sizes $n_i$, $i=1,...,B$ for $i=1,...,B$, $B=10$.}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
     & ${\mathbf{n_i}}$ & \multicolumn{1}{l|}{PSI (L,YN)} & \multicolumn{1}{l|}{PRS (5,7.5)} & \multicolumn{1}{l|}{p(KS)} \\
    \hline
    \multicolumn{5}{|c|}{$n=500,\ B=10,\ {n_{0i}}=50 \ i=1,...,10$} \\
    \hline
    $t_1$ & (35, 40, 45, 45, 47, 50, 55, 58, 60, 65) & 0.032 ($\mathbbm{g,g}$) & 0.032 ($\mathbbm{g,a}$)& 0.760 ($\mathbbm{g}$)\\
    $t_2$ & (40, 45, 45, 45, 47, 48, 55, 55, 60, 60) & 0.017 ($\mathbbm{g,g}$) & 0.018 ($\mathbbm{g,g}$)& 0.760 ($\mathbbm{g}$)\\
    $t_3$ & (35, 36, 42, 43, 44, 44, 60, 60, 61, 75) & 0.060 ($\mathbbm{g,a}$) & 0.062 ($\mathbbm{r,r}$)& 0.164 ($\mathbbm{g}$)\\
    $t_4$ & (20, 35, 35, 40, 40, 62, 65, 65, 65, 73) & 0.131 ($\mathbbm{a,r}$)& 0.116 ($\mathbbm{r,r}$)& 0.002 ($\mathbbm{r}$)\\
    \hline
    \multicolumn{5}{|c|}{$n=2\ 000,\ B=10,\ {n_{0i}}=200 \ i=1,...,10$} \\
    \hline
    $t_1$ & (160, 170, 180, 180, 190, 200, 210, 220, 240, 250) & 0.020 ($\mathbbm{g,a}$) & 0.020 ($\mathbbm{r,r}$)& 0.164 ($\mathbbm{g}$)\\
    $t_2$ & (180, 180, 184, 190, 194, 200, 200, 210, 222, 240) & 0.008 ($\mathbbm{g,g}$) & 0.008 ($\mathbbm{g,a}$)& 0.087 ($\mathbbm{a}$)\\
    $t_3$ & (180, 180, 190, 194, 200, 200, 204, 210, 220, 222) & 0.005 ($\mathbbm{g,g}$) & 0.005 ($\mathbbm{g,g}$)& 0.241 ($\mathbbm{g}$)\\
    $t_4$ & (160, 170, 170, 178, 180, 210, 210, 220, 242, 260) & 0.025 ($\mathbbm{g,r}$) & 0.026 ($\mathbbm{r,r}$)& 0.005 ($\mathbbm{r}$)\\

    \hline
    \end{tabular}
  \label{tab:n=500,2000 example}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Population resemblance comparison using the current population of size $n=10\ 000$ and observed category sample sizes $n_i$, $i=1,...,B$ for $i=1,...,B$, $B=20$.}
    \begin{tabular}{|c|c|c|c|}
    \hline
     & ${\mathbf{n_i}}$ & \multicolumn{1}{l|}{PSI (L,YN)} & \multicolumn{1}{l|}{PRS (5,7.5)}  \\
    \hline
    \multicolumn{4}{|c|}{$n=10\ 000,\ B=20,\ n_{0i}=500,\ i=1,\dots ,20$}\\
    \hline
    $t_1$ & \Centerstack{(445, 455, 480, 480, 480, 480, 485, 491, 495, 495,\\500, 502, 502, 502, 502, 520, 540, 546, 550, 550)} & 0.0032 ($\mathbbm{g,g}$) & 0.0032 ($\mathbbm{a,a}$)\\ \hdashline
    $t_2$ & \Centerstack{(150, 170, 400, 400, 450, 450, 460, 460, 525, 525,\\545, 545, 550, 550, 600, 620, 650, 650, 650, 650)} & 0.1060 ($\mathbbm{a,r}$) & 0.0769 ($\mathbbm{r,r}$)\\ \hdashline
    $t_3$ & \Centerstack{(445, 455, 480, 480, 485, 485, 490, 495, 500, 500,\\501, 502, 502, 510, 510, 520, 520, 530, 540, 550)} & 0.0025 ($\mathbbm{g,g}$) & 0.0025 ($\mathbbm{g,g}$)\\ \hdashline
    $t_4$ & \Centerstack{(425, 425, 440, 440, 445, 445, 460, 460, 475, 475,\\490, 490, 525, 525, 555, 555, 585, 585, 600, 600)} & 0.0139 ($\mathbbm{g,r}$) & 0.0142 ($\mathbbm{r,r}$)\\ \hdashline
    $t_5$ & \Centerstack{(390, 390, 450, 450, 450, 450, 460, 460, 475, 475,\\525, 525, 545, 545, 550, 550, 555, 555, 600, 600)} & 0.0153 ($\mathbbm{g,r}$) & 0.0150 ($\mathbbm{r,r}$)\\ \hdashline
    $t_6$ & \Centerstack{(29, 300, 360, 380, 400, 400, 400, 400, 400, 400,\\420, 420, 510, 510, 511, 520, 520, 520, 600, 2000)} & 0.3785 ($\mathbbm{r,r}$) & 0.5260 ($\mathbbm{r,r}$)\\
    \hline
        \multicolumn{4}{|l|}{Note: In all of $t_1,\dots,t_6$, $p(KS)<0.0001$ ($\mathbbm{r}$).}\\
        \hline
    \end{tabular}  \label{tab:n=10000 B=20 example}
\end{table}

In Tables \ref{tab: n=50 example} through \ref{tab:n=10000 B=20 example}, when comparing the PSI(L) with the PRS, the results are commensurate with the conclusions from Table \ref{TABLE1}: the PRS procedure less frequently indicates a ``\textit{discrepancy}'' in small samples ($n=50$), while doing so more frequently in larger samples ($n\geq 500$). The same conclusion holds when comparing the PSI(L) with the PSI(YN). Recall that the PSI(L) has an inflated {probability of indicating a shift when none has occurred} in small samples and, conversely, a too-close-to-zero {probability of indicating \textit{discrepancy}} in large samples. Notably, in the cases where $n$ exceeds $500$, PSI(L) seems insensitive to \textit{discrepancy}. Observe in all cases considered that the PRS at $M=7.5$ indicates \textit{discrepancy} more frequently than the PSI(YN). Of course, the PRS and PSI(YN) are not directly comparable because the critical values of the PRS guarantee a specified level of statistical power, while the PSI(YN) is known to have weak power at small sample sizes (Table 4 in \textcite{yurdakul2020statistical}), a limitation of PSI(YN). Moreover, the null hypotheses under which the critical values are calculated, are dissimilar {between PSI(YN) and the PRS,} with the $\mathrm{PRS}$ critical values calculated under a region of $\delta$-resemblance and not exact equality. In all tables, it is clear that the PRS indicates the need for reconstruction (\textit{discrepancy}) more often as $M$ increases. There exists an inverse relationship between $M$ and $\delta_{\mathrm{tol}}$, hence, for larger values of $M$, the tolerated deviance is smaller at the same level of power. 

A comparison with the discrete KS remains. As with the PSI(YN), the KS is based on a null hypothesis of exact equality, while the PRS includes a risk tolerance through $\delta_{\mathrm{tol}}$. This results in a challenging direct comparison. A further challenge is that the KS depends a great deal on the category where the maximum deviance occurs; and, in small samples the KS has limited realizable values - see cases $t_4$ and $t_5$ of $(n,B)=(50,5)$, and $t_1$ and $t_2$ of $(n,B)=(500,10)$ with equal values of $p(\mathrm{KS})$. The KS is not distribution-free with respect to $\mathbf{p_0}$ and obtaining critical values/p-values for the KS is computationally costly. Appendix \ref{App:KSvsPRS} is a useful reference to the discussion to follow - it contains a simulation study when comparing the KS and the PRS under differing ``null hypothesis'' cases. As a first departure, observe Table \ref{tab:n=500,2000 example}. The conclusions are commensurate with those found in Appendix \ref{App:KSvsPRS}: the PRS is more likely to indicate \textit{discrepancy} ($\mathbbm{r}$) than the KS, even when $M=5$. See cases $t_1$, $t_3$ of $(n,B)=(500,10)$ and $t_1$ of $(n,B)=(2\ 000,10)$. Further to this, it is well known that the power of the KS diminishes as the sample size decreases (not unlike the PSI(YN)), possibly explaining the more frequent $\mathbbm{g}$ status where $n=500$, compared to $n=2\ 000$. Next, consider the $(n,B)=(10\ 000,20)$ design in Table \ref{tab:n=10000 B=20 example} where the KS indicates \textit{discrepancy} in all cases. This is a result of the substantial power of the KS at larger sample sizes, and by its design, that any shift away from the null will swiftly be detected. Perhaps, detecting shifts so often might not be practically ideal to a risk practitioner, while the construction of the PRS allows for a range of detection capabilities (see case $t_3$ and $t_1$ with an $\mathbbm{r}$ and $\mathbbm{a}$ status).

We conclude from this comparative real-world study that among the measures considered here, the PRS is the only universally competent procedure at a range of sample sizes, including small samples. Clearly, the tuning of the PRS through $M$ and $\delta_{\mathrm{tol}}$ allows for a level of risk acceptance by the practitioner while indicating \text{discrepancy} sufficiently in small samples and often enough in larger samples. Unlike the KS, the PRS is sensitive to ranges of shifts over multiple risk buckets and has easily obtainable critical values that are unique in small sample sizes. 


\section{Conclusion}

Monitoring for changes in the population underlying a developed model is a frequent practice, especially in credit risk models. Several measures, most prominently the PSI, have been proposed over the years. Limitations in these methods have spurred research into alternative approaches for assessing population resemblance. Our contribution introduces the PRS, utilizing the Pearson chi-square statistic and non-central chi-square distribution. 

The main advantageous features are that the PRS accommodates sample-size dependent critical values and enables the specification of risk tolerances. To date, the PRS is \textit{the only} universally competent procedure at a range of sample sizes, especially at small sample sizes. The PRS has statistically well-founded properties and is distribution-free with respect to $\mathrm{p_0}$ (unlike the well-known Kolmogorov-Smirnov test), implying a set of once-off critical values. Further, the power of the PRS procedure can be guaranteed through the choice of critical values and is associated with a risk tolerance $\delta$ under {an assumption of no or limited population shift} - a feature that is {neither} present in the KS, nor the PSI of Lewis and of \textcite{yurdakul2020statistical}. Moreover, the inclusion of the concept of $\delta$-resemblance and {the} {tuning} {parameter} $M$ allows for {the practitioner to calibrate the procedure to align with their acceptable level of risk appetite}, an advantage of the PRS over both the PSI and the KS tests. These competency characteristics of the PRS were demonstrated through Monte Carlo simulations and real-world applications. In the applications, the suitability of the PRS was showcased, measuring the resemblance between populations given both small sample sizes (often encountered in low default portfolios) or larger samples frequently encountered in retail portfolios of a bank.


%% Figure environment removed
%
%\begin{table}
%\caption{D-optimality values for design $X$ under five different scenarios.  \label{tab:tabone}}
%\begin{center}
%\begin{tabular}{rrrrr}
%one & two & three & four & five\\\hline
%1.23 & 3.45 & 5.00 & 1.21 & 3.41 \\
%1.23 & 3.45 & 5.00 & 1.21 & 3.42 \\
%1.23 & 3.45 & 5.00 & 1.21 & 3.43 \\
%\end{tabular}
%\end{center}
%\end{table}
%
%\begin{itemize}
%\item Note that figures and tables (such as Figure~\ref{fig:first} and
%Table~\ref{tab:tabone}) should appear in the paper, not at the end or
%in separate files.
%\item In the latex source, near the top of the file the command
%\verb+\newcommand{\blind}{1}+ can be used to hide the authors and
%acknowledgements, producing the required blinded version.
%\item Remember that in the blind version, you should not identify authors
%indirectly in the text.  That is, don't say ``In Smith et. al.  (2009) we
%showed that ...''.  Instead, say ``Smith et. al. (2009) showed that ...''.
%\item These points are only intended to remind you of some requirements.
%Please refer to the instructions for authors
%at \url{http://amstat.tandfonline.com/action/authorSubmission?journalCode=ubes20&page=instructions#}
%\item For more about ASA\ style, please see \url{http://journals.taylorandfrancis.com/amstat/asa-style-guide/}
%\item If you have supplementary material (e.g., software, data, technical
%proofs), identify them in the section below.  In early stages of the
%submission process, you may be unsure what to include as supplementary
%material.  Don't worry---this is something that can be worked out at later stages.
%\end{itemize}
%
%\section{Methods}
%\label{sec:meth}
%Don't take any of these section titles seriously.  They're just for
%illustration.
%
%\section{Verifications}
%\label{sec:verify}
%This section will be just long enough to illustrate what a full page of
%text looks like, for margins and spacing.
%
%
%\cite{Campbell02}, \cite{Schubert13}, \cite{Chi81}
%
%
%The quick brown fox jumped over the lazy dog.

%\section{Conclusion}
%\label{sec:conc}
%



\bigskip
\begin{center}
{\large\bf}
\end{center}
\newpage
\section{Appendix}

\subsection{Non-centrality parameter calculation}
\label{sec: Appendix 1}

Consider the calculation of the non-centrality parameter $\lambda_{\mathrm{sup}}$ in \eqref{eq:noncentral parm}. To this end, for $\boldsymbol{\xi} = (\xi_1,\ldots,\xi_B)$ and $\mathrm{p}_0 = (p_{01},\ldots,p_{0B})$, define the function $$f(\boldsymbol{\xi})=\sum_{j} \frac{\xi_j^2}{p_{0j}}.$$ The goal is to maximize $f(\boldsymbol{\xi})$ subject to the constraints $\sum_j \xi_j = 0$ and $\xi_j^2 \leq n\delta^2,\quad j=1,\ldots,B$. The latter set of inequality constraints can be re-formulated as equality constraints by introducing proxy variables $\epsilon_j$, $j=1,\ldots,B$, so that $\xi_j^2 + \epsilon_j^2 = n\delta^2,\quad j=1,\ldots,B$.

This permits the formulation of the problem using the method of Lagrange multipliers. The objective function is given by
$$g(\bm{\theta}) = \sum_{j=1}^{B} \frac{\xi_j^2}{p_{0j}} + \kappa \bigg( \sum_{j=1}^{B} \xi_j\bigg)+\sum_{j=1}^{B} \gamma_j \bigg(\xi_j^2 + \epsilon_j^2 - z^2 \bigg)$$
where  $\bm{\theta}=(\boldsymbol{\xi},\kappa,\bm{\gamma},\bm{\epsilon})$ denotes the set of parameters being optimized over and $\kappa$ and $\bm{\gamma}=(\gamma_1,\ldots,\gamma_B)$ are the necessary Lagrange multiplier coefficients. To find the maximum of this function, we evaluate the set of partial derivatives and set these equal to $0$. Thus, we have $3B+1$ equations
\begin{eqnarray}
    \dfrac{\partial g}{\partial \xi_k} &=& \frac{2\xi_k}{p_{0k}}+\kappa+2\gamma_k\xi_k = 0, \ k = 1,\ldots,B, \label{eq:partial c_k}\\
    \notag \\ 
    \dfrac{\partial g}{\partial \kappa} &=& \sum_{j=1}^{B} \xi_j = 0, \label{eq:partial kappa}\\
    \notag \\
    \dfrac{\partial g}{\partial \epsilon_k} &=& 2\gamma_k\epsilon_k=0, \ k=1,\ldots,B, \label{eq:partial epsilon_k}\\
    \notag \\ 
    \dfrac{\partial g}{\partial \gamma_k} &=& \xi_k^2 + \epsilon_k^2 - n\delta^2=0, \ k=1,\ldots,B.\label{eq:partial gamma_k}
\end{eqnarray}

From \eqref{eq:partial epsilon_k}, for each category $k$, either $\gamma_k=0$ or $\epsilon_k=0$. When $\epsilon_k=0$, it follows from \eqref{eq:partial gamma_k} that $\xi_k^2 = n\delta^2$. Conversely, when $\gamma_k=0$, it follows from \eqref{eq:partial c_k} that $\xi_k = -\kappa p_{0k}/2$. Introducing variables $a_k \in \{-1,0,1\}$ allow us to express $\xi_k$ as $$\xi_k = n^{1/2}\delta a_k -\kappa p_{0k}(1-a_k^2)/2, \ k=1,\ldots,B,$$ where $\kappa$ can be found using \eqref{eq:partial kappa}. Specifically, we have
$$\sum_{j=1}^{B} \xi_j = n^{1/2}\delta \sum_{j=1}^{B} a_j - \frac{\kappa}{2}\sum_{j=1}^{B} p_{0j}(1-a_j^2) = 0$$ so that $$\kappa = \frac{2n^{1/2}\delta \sum_j a_j}{\sum_j p_{0j}(1-a_j^2)}.$$ When $\sum a_j^2 = B$, we can set $\kappa=0$ without loss of generality, as the $\xi_j$ will not depend on this value. Defining $$h(\mathbf{a},\mathbf{p}) = \frac{\sum_j a_j}{\sum_j p_j(1-a_j^2)} \ \mathrm{for}\ \sum_j a_j^2 < B,$$ with $h(\mathbf{a},\mathbf{p})$ taking the value $0$ otherwise, it is possible to write $$\xi_k = n^{1/2}\delta \big[a_k - h(\mathbf{a},\mathbf{p}_0)(1-a_k^2)p_{0k}\big],  \ k=1,\ldots,B.$$
Subsequently, we have
\begin{equation}
\lambda_{\mathrm{sup}} = n\delta^2 \max_{\mathbf{a}:a_j\in\{-1,0,1\}} \sum_{j=1}^{B}\frac{a_j^2-h^2(\mathbf{a},\mathbf{p}_0)(1-a_j^2)p_{0j}}{p_{0j}}. \label{eq: delta to lambda}
\end{equation}
Notice in \eqref{eq: delta to lambda} that the non-centrality parameter is proportional to the sample size $n$ and the squared tolerance level $\delta^2$. The remaining maximization problem can be solved by exhaustively searching through the $3^B$ possible vectors $\mathbf{a}$ with entries in $\{-1,0,1\}$.

\subsection{Quantifying model deviance}
\label{sec: model deviance}
One plausible motivation for the Lewis constants is that they signify different levels of deviation between the model construction population $\bm{p}_0$ and the true current population $\bm{p}$. By equating the KL divergence in \eqref{Kullback J2} to a Lewis constant and examining the consequences for the populations, we can gain insight into the implied divergence between the populations. As per \textcite[p. 62]{vaart2000asymptotic}, we understand divergence to be a pseudo-distance. This will be further explored through the formulation of an optimization problem. 

For illustration, assume uniform model construction probabilities $\mathbf{p}_0=(1/B,\ldots,1/B)$ across the $B$ categories. We aim to find the minimum and maximum values of $J$ with respect to $\mathbf{p}$ while ensuring ``closeness'' to $\mathbf{p}_0$. We measure this closeness using the Euclidean distance, where $\delta$ represents the specified deviation level. For example, with $\delta=0.01$, the typical deviation between model construction and current probabilities for a category is approximately 1\%, while individual deviations can vary within this limit.

We adopt the notation $ \mathrm{opt}_{\mathbf{p}} J(\mathbf{p},\mathbf{p}_0)$ to represents that both minimization and maximization over $\mathbf{p}$ is considered. This is done subject to $\sum p_j = 1$, $\min_j p_j \geq 0$ and $B^{-1}\sum_{j=1}^{B} (p_j - p_{0j})^2 = \delta^2$. The first two constraints ensure the optimization is performed over valid probability distributions, while the third constraint ensures ``closeness'' to $\mathbf{p}_0$. The set of distributions satisfying these constraints is a subset of the $\delta$-resemblant class from Definition 1.

This problem was solved using a stochastic optimization routine for the cases $B=5$ and $B=10$. Figure \ref{fig:PSI B5} shows the minimum and maximum divergence ($J$) values as a function of $\delta$. Since the upper Lewis constant is $0.25$, we extend the plot range to cover twice this value for clarity.

% Figure environment removed

To ascertain the meaning of a specific Lewis constant in relation to the underlying populations, we now interpolate specific $J$ values on the divergence boundaries to the range of $\delta$ values. For example, when $(B,J)=(5,0.1)$, we have $\delta \in (0.055,0.070)$, meaning that if the true PSI is 0.1, the typical deviation between current and model construction probabilities ranges from $5.5\%$ to $7\%$. When $(B,J)=(10,0.1)$, this corresponds to $\delta \in (0.025,0.036)$, meaning the typical deviation is between $2.5\%$ and $3.6\%$. Similarly, for $(B,J)=(5,0.25)$ we have $\delta \in (0.077,0.114)$, and for $(B,J)=(10,0.25)$ we have $\delta \in (0.039,0.062)$.

\subsection{A simulation comparing the discrete Kolmogorov-Smirnov with the PRS}
\label{App:KSvsPRS}

We simulate $10^6$ values of the discrete KS statistic under the null hypothesis conditions and determine critical values that correspond to $(\alpha_l,\alpha_u)=(10\%,1\%)$ significance levels. Of course obtaining exact significance levels is problematic due to the discrete nature calling for a slight departure: when $(n,B)=(500,10)$ the closest realized $\alpha_l$ is $12.7\%$ or $8.6\%$ (we choose $12.7\%$). The simulation that follows is an analogue of Section \ref{sec: PRS simulation}. When the KS is properly calibrated, we should expect a $\mathbbm{g}$ ($\mathbbm{r}$) status $90\%$ ($1\%$) of the time given $\delta_\nu=0$. Conversely, when the PRS is calibrated, we expect a $\mathbbm{g}$ ($\mathbbm{r}$) status $90\%$ of the time given $\delta_\nu=\delta_\mathrm{tol}$ ($\delta_\nu=M\delta_{\mathrm{tol}}$). Table \ref{Tab: KS_PRS sim} show some results. It is insightful to observe that when $\delta_\nu=\delta_\mathrm{tol}$, the KS is significantly less (slightly more) likely to yield a $\mathbbm{g}$ ($\mathbbm{r}$) status compared to the PRS. This observation is expected because the KS is designed to detect deviation from the null while the PRS is calibrated to accept the simulated level $\delta_\mathrm{tol}$ of deviation. When $\delta_\nu=M\delta_\mathrm{tol}$ the observation is different: in principle, the KS should detect any deviation from the null, but here the KS is \textit{more likely} than the PRS to result in a $\mathbbm{g}$ status and substantially \textit{less likely} an $\mathbbm{r}$ status. Here, an $\mathbbm{r}$ status (thus, PRS) is desirable. More extensive simulation results are available from the authors.
\begin{table}[htbp]
  \centering
  \caption{Simulated $\mathbbm{rag}$ statuses for the KS and PRS statistics.}
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
    \hline
          & \multicolumn{4}{c|}{$(n,B)=(50,5)$} & \multicolumn{4}{c|}{$(n,B)=(500,10)$} \\
\cline{2-9}          & \multicolumn{2}{c|}{PRS} & \multicolumn{2}{c|}{KS} & \multicolumn{2}{c|}{PRS} & \multicolumn{2}{c|}{KS} \\
    \hline
    $\delta_\nu$ & \multicolumn{1}{c|}{$\mathbbm{g}$} & \multicolumn{1}{c|}{$\mathbbm{r}$} & \multicolumn{1}{c|}{$\mathbbm{g}$} & \multicolumn{1}{c|}{$\mathbbm{r}$} & \multicolumn{1}{c|}{$\mathbbm{g}$} & \multicolumn{1}{c|}{$\mathbbm{r}$} & \multicolumn{1}{c|}{$\mathbbm{g}$} & \multicolumn{1}{c|}{$\mathbbm{r}$} \\
    \hline
    0     & 0.9566 & 0.0029 & 0.9065 & 0.0136 & 0.9561 & 0.0037 & 0.8714 & 0.0141 \\
    $\delta_\mathrm{tol}$ & 0.9084 & 0.0092 & 0.8394 & 0.0311 & 0.9001 & 0.0105 & 0.8170 & 0.0264 \\
    $M\delta_\mathrm{tol}$ & 0.0015 & 0.9609 & 0.0051 & 0.8618 & 0.0116 & 0.9070 & 0.0581 & 0.5588 \\
    \hline
    \end{tabular}%
\label{Tab: KS_PRS sim}
\end{table}
\FloatBarrier

\subsection{An information-theoretic view of resemblance} \label{sec: info theory}

The PSI is based on the symmetric KullbackLeibler divergence in \eqref{Kullback J2}, which originates in the entropy work by \textcite{shannon1948mathematical}. For a discrete random variable $X$ with probability mass function (pmf) $p(x),\ x\in\mathcal{X}$, the Shannon entropy is 
$$H(X)=-\sum_{x\in\mathcal{X}} p(x)\log p(x)=-\mathrm{E}[\log p(X)].$$ 
Shannon entropy measures the expected uncertainty associated with an outcome; higher entropy indicates greater uncertainty. Maximum entropy occurs when the distribution is uniform across $\mathcal{X}$, signifying the highest level of uncertainty. The Kullback-Leibler information, a measure of relative entropy introduced by \textcite{kullback1951information}, measures the ``expected per observation information.'' Here,
$$I(p,q)=\sum_{x\in\mathcal{X}} p(x) \log \bigg[\frac{p(x)}{q(x)}\bigg],$$
for two pmf's $p(x)$ and $q(x)$ defined on the same set $\mathcal{X}$. It quantifies how much information is needed to distinguish between two distributions. The measure is fundamentally asymmetric, denoting the divergence of $p(x)$ \textit{from} $q(x)$. The symmetrized version $J(p,q)=I(p,q)+I(q,p)$, termed the ``divergence'', was first defined by \textcite{jeffreys1948theory} and formed the basis for the PSI measure of \textcite{lewis1994introduction}.

The measures $I(p,q)$ and $J(p,q)$ belong to the class of $f$-divergences pioneered by \textcite{renyi1961measures} and \textcite{csiszar1967information}. For pmf's $p(x)$ and $q(x)$, the $f$-divergence is defined as $$D_f(p,q) = \sum_{x\in\mathcal{X}}q(x)f\bigg[\frac{p(x)}{q(x)}\bigg],$$ where $f(t)$ is a convex function satisfying $f(1)=0$.  The choice $f(t)=(t-1)\log(t)$ results in the symmetric divergence $J(p,q)$, while the choice $f(t)=(1-t)^2/t$ results in the classic chi-square divergence metric $$\chi^2(p,q) = \sum_{x\in\mathcal{X}}\frac{\big[q(x)-p(x)\big]^2}{p(x)}.$$ In fact, the chi-square divergence plays an important role with respect to $f$-divergences in general. By Theorem 4.1 of \textcite{csiszar2004information}, any $f$-divergence can be approximated by the chi-square divergence when $p(x)$ and $q(x)$ are close. By their theorem, if $f(t)$ is twice differentiable at $t=1$ and $f''(1)>0$, then $$\frac{D_f(p,q)}{\chi^2(p,q)} \rightarrow \frac{f''(1)}{2} \text{  as  } p(x)\rightarrow q(x).$$

In general, $f$-divergences are not true distances; for many choices of $f(t)$ the resulting measure is not symmetric. Also, when a specific $f(t)$ results in a symmetric divergence measure, it still may not satisfy the triangle inequality. Nonetheless, the class of $f$-divergences does satisfy an important property of interest in the present context: when $f(t)$ is strictly convex at $t=1$, $D_f(p,q)=0$ if and only if $p(x)=q(x)$ for all $x\in\mathcal{X}$, see Section 4 of \textcite{csiszar2004information}. For the interested reader, the latter reference includes further discussion on the geometric properties of $f$-divergences.
\newpage
%\bibliographystyle{chicago}

\printbibliography

%\bibliography{PSIBiblio}
\end{document}
