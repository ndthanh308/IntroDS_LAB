\section{Introduction}\label{sec:intro}
Sparsity-induced optimization has achieved great success in many data analyses, and low-rank regularization is a powerful tool to impose sparsity. 
In this paper, we study the class of non-convex and non-smooth sparse learning problems in discrete space presented as follows:
\begin{equation}\label{eq:fx}
    \min F(\xm) = g(\xm) + h(\xm),
\end{equation}
where $\xm \in \mathbb{R}^{n \times k}, h(\xm) = \lambda\cdot rank (\xm) \ (\lambda > 0)$.
Countless problems in machine learning~\cite{shang2017nonnegative, su2020low}, computer vision~\cite{haeffele2014structured, he2015total}, and signal processing~\cite{dong2014compressive, zhou2014regularized} naturally fall into this template, including but not limited to rank regularized matrix factorization for recommendation, image restoration and clustering, compressive sensing, and multi-task learning. %Due to the $rank$-regularization term, which is non-smooth, non-convex, the objective is a discrete optimization problem.

Besides the efforts trying to relax the $rank$ regularization with the nuclear norm, which is the tightest convex envelop, a class of non-convex optimization
algorithms has been developed to solve rank-regularized problems with vanilla objectives. The benefit is obvious that it will bring strict sparse solution instead of low energy of singular values. For example, the cardinality constraint is studied for $M$-estimation problems by Iterative Hard-Thresholding (IHT) method~\cite{blumensath2008iterative}. In addition, the alternating direction method of multipliers is also explored to apply on the rank-regularized problem~\cite{qu2023efficient}. Moreover, general iterative shrinkage and thresholding algorithm has been proposed to solve non-convex sparse regularization problems~\cite{gong2013multiple}. We refer the readers to the papers aforementioned and the references therein. In this paper, we use the proximal gradient descent method to obtain a sparse sub-optimal solution for Eq.~(\ref{eq:fx}). Also, with a \textit{support set projection} operation on the singular values of the matrix $\xm$, our proposed algorithm can be accelerated with a faster convergence rate. We explicitly list our contributions as following: 
\begin{itemize}
	\item We show that with the proximal gradient descent method, the sequence obtained converges to a critical point with \textit{zero} gradient, at a convergence rate of $F(\xm^{t+1}) - F(\xm^*) \leq O(\frac{1}{t})$.
	\item We propose two accelerated proximal gradient descent methods (monotone and non-monotone decreasing) which can \textit{asymptotically} achieve Nesterov Accelerated Gradient (NAG) convergence rate ($O(\frac{1}{t^2})$) which originally is supposed for convex problems~\cite{nesterov2003introductory} and we give the rigorous proof.
	\item The proposed algorithms can indeed admit strict sparsity, as the support set of singular values in each update is a subset of the initialized set and keeps shrinking during the update. 
\end{itemize}
