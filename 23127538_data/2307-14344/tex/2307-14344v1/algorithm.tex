\section{Algorithms and Convergence Analysis}\label{sec:alg}
 Throughout this paper, we use uppercase bold letters for matrices, lowercase bold letters for vectors, and lowercase letters for scalars. We use superscript to represent the current iteration. $\sigma_i (\cdot)$ is  the $i$-th largest singular value of a matrix. $\sigma_{min} (\cdot)$ and $\sigma_{max} (\cdot)$ represent the smallest and the largest singular value of a matrix, respectively. $\sigma(\cdot)$ indicates the vector formed by the singular values of a matrix, and $\dg(\sigma(\cdot))$ denotes the diagonal matrix formed by the vector. $|\cdot|$ denotes the cardinality of a set, and supp$(\cdot)$ denotes the support of a vector. For a matrix $\xm \in \mathbb{R}^{n \times k}$, let $\sigma(\xm) = (\sigma_{max} (\xm), \sigma_2 (\xm), \dots, \sigma_{min} (\xm))$, then we can see rank$(\xm) = \|\sigma(\xm)\|_0$. 
 $S = \sp(\sigma(\xm^0))$, $\xm^0$ is the initialization. 
 The proximal mapping associated with $h(\cdot)$ is defined as $\textrm{prox}_h(\um) = \argmin_{\vm} h(\vm) + \frac{1}{2} \|\um-\vm\|^2_F$. 
We make some mild assumptions regarding $g(\xm)$:
1) g($\cdot)$ is convex, and $\nabla g(\cdot)$ has bounded singular value $\sigma_{max}(\nabla g(\xm))) \leq G$ for any $\xm$;
2) g($\cdot)$ has L-Lipschitz continuous gradient, $\| \nabla g(\xm_1) - \nabla g (\xm_2) \|_F \leq L\|\xm_1 - \xm_2\|_F$.
Regarding $g(\cdot)$, we set the squared loss $\|\ym - \dm \xm\|_F^2$ to demonstrate the algorithm for simplicity, where $\ym \in \mathbb{R}^{d \times k}, \dm \in \mathbb{R}^{d \times n},\xm \in \mathbb{R}^{n \times k}$. It is worth noting that the objective can be extended to other loss functions.
\subsection{Proximal Gradient Descent for Low-Rank Approximation}\label{subsec:PGD}
In the $t$-th iteration of the proximal gradient descent (PGD) method, gradient descent is applied on the squared loss function $g(\xm)$ to get
$\xm^t - s\nabla g(\xm^t)$, where $s \geq 0$ is the step size in gradient descent and $\frac{1}{s}$ is typically larger than the Lipschitz continuous constant $L$ of $g(\xm)$. After applying the proximal mapping on $\xm^t - s\nabla g(\xm^t)$ we can get $\xm^{t+1}$:
\begin{equation}\begin{split}\label{eq:proximal}
    \xm^{t+1} =  \prox_h (\xm^t - s\nabla g(\xm^t))  
    % =  \argmin_{\tm} \frac{1}{2s} \|\tm - (\xm^t - s\nabla g(\xm^t))\|_F^2 + \lambda \rk(\tm) 
    =  T_{\sqrt{2\lambda s}} (\xm^t - s\nabla g(\xm^t)).
\end{split}\end{equation}
$T_{\theta} (\cdot)$ is the singular value hard thresholding operator defined as 
$T_{\theta}(\qm) = \um \Sigma_{\theta} \vm^T$, where
$\qm = \um \Sigma \vm^T$ is the singular value decomposition and 
$ \Sigma_{\theta} (i,i) = \begin{cases}
      0,  &|\Sigma(i,i)| \leq \theta, \\
      \Sigma(i,i),  &\textrm{otherwise.} \\
     \end{cases}$

The optimization algorithm to minimize problem Eq.~(\ref{eq:fx}) by PGD is summarized in Algorithm~\ref{alg:1}.
\begin{algorithm}[h]
	\caption{Proximal Gradient Descent for the Rank-Regularized Problem}
	\label{alg:1}
	\begin{algorithmic}
		\STATE {\bfseries Input:} Initialization $\xm^0$, step size $s$, regularization parameter $\lambda$.
		\FOR{$t=0, \dots $} 
		\STATE Update $\xm^{t+1}$ according to Eq.~(\ref{eq:proximal})
		\ENDFOR
	\end{algorithmic}
\end{algorithm}
%
%
%\begin{multicols}{2}
%	% Figure environment removed
%	\begin{algorithm}[H]
%		\caption{The algorithm to solve the problem (\ref{opfp}).}
%		\label{alg:general}
%		\begin{algorithmic}
%			\STATE {\bfseries Initialization:} ${v} \in \mathcal{C}$
%			\REPEAT
%			%		\STATE Initialize $noChange = true$.
%			%		\FOR{$i=1$ {\bfseries to} $m-1$}
%			%		\IF{$x_i > x_{i+1}$}
%			\STATE 1) Calculate $\lambda = \frac{f(v)} {g(v)}$
%			\STATE 2) Update $v$ by solving the following problem:
%			\begin{equation}\label{opdiff}
%				{v} = \arg \max_{{v}\in \mathcal{C}} f({v}) - \lambda g({v})
%			\end{equation}
%			%		\ENDIF
%			%		\ENDFOR
%			\UNTIL{convergence}
%		\end{algorithmic}
%	\end{algorithm}
%\end{multicols}

We present the analysis of the convergence rate of Algorithm~\ref{alg:1}. Before that, we first show the support set of the singular value vector shrinks, and then the rank of obtained solutions decreases, also the objective has a sufficient decrease during each update.
\begin{lemma}\label{lem:1}
If $s \leq \min \{\frac{2\lambda}{G^2}, \frac{1}{L}\}$, then 
$\sp (\sigma(\xm^{t+1})) \subseteq \sp (\sigma(\xm^{t})), \textrm{for } t \geq 0$,
and
$\rk(\xm^{t+1}) \leq  \rk(\xm^{t}), \textrm{for } t \geq 0$,
which means the support of the singular value vectors of the sequence $\{\xm^t\}_t$ shrinks, also the rank of the sequence $\{\xm^t\}_t$ decreases.
\end{lemma}

\begin{lemma}\label{lem:2}
With $s \leq \min \{\frac{2\lambda}{G^2}, \frac{1}{L}\}$, the sequence of the objective $\{F(\xm^t)\}_t$ is monotonically non-increasing, and the following inequality holds for all $t \geq 0$:
\begin{equation}
    F(\xm^{t+1}) \leq F(\xm^t) - (\frac{1}{2s} - \frac{L}{2})\|\xm^{t+1} - \xm^t\|^2_F.
\end{equation}
\end{lemma}
To eliminate the concern that very small step size is required to ensure Lemma~\ref{lem:1} with the prerequisite $s \leq \min \{\frac{2\lambda}{G^2}, \frac{1}{L}\}$, we show that $\frac{1}{L}$ is no larger than $\frac{2\lambda}{G^2}$ with high probability, thus the choice of step size $s$ in Lemma~\ref{lem:1} would not be smaller than $ \frac{1}{L}$ with high probability. 
\begin{theorem}\label{thm:1}
\cite{yang2020fast} \ Suppose $\dm \in \mathbb{R}^{d \times n} (n \geq d)$ is a random matrix with elements i.i.d. sampled from the standard Gaussian distribution \textit{N}(0,1), then 
\begin{equation}
    \textrm{Probability}(\frac{1}{L} \leq \frac{2\lambda}{G^2}) \geq 1-e^{-\frac{a^2}{2}} - ne^{-a},
\end{equation}
if 
$n \geq (\sqrt{d} + a + \sqrt{\frac{(d+2\sqrt{da}+2a)(x_0 + \lambda |S|))}{\lambda}})^2$,
where $x_0 = \|\ym - \dm \xm^0\|^2_F$, $S = \sp(\sigma(\xm^0))$, and $a$ can be chosen as $a_0 \textrm{log}n$ for $a_0 > 0$.
% to ensure that Eq.~(\ref{eq:un}) holds with high probability. 
\end{theorem}


The sequence $\{\xm^t\}_t$ can be segmented into the following $|S|+1$ subsequences $\{\xt^{k'}\}_{k'=0}^{|S|}$ with the definition as follows:
\begin{equation}
    \xt^{k'} = \{\xm^t: |supp(\sigma(\xm^t))| = k', t \geq 0\}, 0 \leq k' \leq |S|. 
\end{equation}

With the definition defined in Definition~\ref{def:1}, the nonempty subsequences in $\{\xt^{k'}\}_{k'=0}^{|S|}$ form a disjoint cover of $\{\xt^{t}\}_t$ and they are in descending order of rank.

\begin{definition}\label{def:1}
Subsequences with shrinking support:
All the $K \leq |S|+1$ nonempty subsequences among $\{\xt^{k'}\}_{k'=0}^{|S|}$ are defined to be subsequences with shrinking support, denoted by $\{\xt^{k}\}_{k=1}^{K}$. The subsequences with shrinking support are ordered with decreasing support size of singular value vectors, i.e. $|\sp(\sigma(\xm^{t_2}))| < |\sp(\sigma(\xm^{t_1}))|$ for any $\xm^{t_1} \in \xt^{k_1}$ and $\xm^{t_2} \in \xt^{k_2}$ with any $1\leq k_1 < k_2 \leq K$.
\end{definition}

Based on the above definition, we have the following lemma about the property of subsequences with shrinking support:
\begin{lemma}\label{lem:3}
(a) All the elements of each subsequence $\xt^k$ ($k = 1, \dots, K)$ in the subsequences with shrinking support have the same support. In addition, for any $1 \leq k_1 < k_2 \leq K$ and any $\xm^{t_1} \in \xt^{k_1}$, and $\xm^{t_2} \in \xt^{k_2}$, we have $t_1 < t_2$ and $\sp(\sigma(\xm^{t_2})) \subset \sp(\sigma(\xm^{t_1}))$. 

(b) All the subsequences except for the last one, $\xt^k$ ($k = 1, \dots, K-1)$ have finite size, and $\xt^K$ have an infinite number of elements, and there exists some $t_0 \geq 0$ such that $\{\xm^t\}_{t=t_0}^\infty \subseteq \xt^K$.
\end{lemma}

The following theorem shows the convergence property of the sequence $\{\xm^{t}\}_t$:
\begin{theorem}\label{thm:2}
    Suppose $s \leq \min \{\frac{2\lambda}{G^2}, \frac{1}{L}\}$. and $\xm^*$ is a limit point of $\{\xm^{t}\}_{t=0}^{\infty}$, 
    and $\sigma(\xm^*)$ is a limit point of $\{\sigma(\xm^{t})\}_{t=0}^{\infty}$, 
    then the sequence $\{\xm^{t}\}_{t=0}^{\infty}$ generated by Algorithm~\ref{alg:1} converges to $\xm^*$,
    % and the sequence $\{\sigma(\xm^{t})\}_{t=0}^{\infty}$ converges to $\sigma(\xm^*)$, 
    $\xm^*$ is a critical point of F($\cdot$), and $\sp(\sigma(\xm^*)) = S^*$, where $S^*$ is the support of any element in $\xt^K$.
    Moreover, there exists $t_1 \geq 0$ such that for all $m \geq t_1$, we have
    \begin{equation}
        F(\xm^{m+1}) - F(\xm^*) \leq \frac{1}{2s(m-t_1+1)}\|\xm^{t_1} - \xm^*\|_F^2.
    \end{equation}
\end{theorem}


 \begin{multicols}{2}
	% Figure environment removed
	\begin{algorithm}[H]
		\caption{Non-monotone Accelerated Proximal Gradient Descent}
		\label{alg:2}
		\begin{algorithmic}
			\STATE {\bfseries Input:} Initialize $\xm^0$,
			$\xm^1 = \xm^0$, $\alpha^0 = 1, s, \lambda$.
			\FOR{$t=1, \dots $} 
			% \STATE Update $\um^{t}, \vm^{t}, \xm^{t+1}, \alpha^{t+1}$ according to Eq.~(\ref{eq:non1}), Eq.~(\ref{eq:non2}), Eq.~(\ref{eq:non3}), Eq.~(\ref{eq:non4}), respectively.
			\STATE Update $\um^{t}, \vm^{t}, \xm^{t+1}, \alpha^{t+1}$ by Eq.~(\ref{eq:non1}).
			%    Update $\um^{t}$ according to Eq.~(\ref{eq:non1}),
			% Update $\vm^{t}$ according to Eq.~(\ref{eq:non2}),
			%  Update $\xm^{t+1}$ according to Eq.~(\ref{eq:non3}),
			%  Update $\alpha^{t+1}$ according to Eq.~(\ref{eq:non4}).
			\ENDFOR
		\end{algorithmic}
	\end{algorithm}
	\begin{algorithm}[H]
		\caption{Monotone Accelerated Proximal Gradient Descent}
		\label{alg:3}
		\begin{algorithmic}
			\STATE {\bfseries Input:} Initialization 
			$\zm^1 = \xm^1 = \xm^0$, $\alpha^0, s, \lambda$.
			\FOR{$t=1, \dots $} 
			% \STATE Update $\um^{t}, \vm^t, \zm^{t+1}, \alpha^{t+1}, \xm^{t+1}$ according to Eq.~(\ref{eq:mo1}), Eq.~(\ref{eq:mo2}), Eq.~(\ref{eq:mo3}), Eq.~(\ref{eq:mo4}), and Eq.~(\ref{eq:mo5}) respectively.
			\STATE Update $\um^{t}, \vm^t, \zm^{t+1}, \alpha^{t+1}, \xm^{t+1}$ by (\ref{eq:mo1}).
			\ENDFOR
		\end{algorithmic}
	\end{algorithm}
\end{multicols}
\subsection{Non-monotone Accelerated Proximal Gradient Descent with Support Projection}\label{subsec:acc}
% Figure environment removed
In the non-monotone accelerated proximal gradient descent with support projection, the update process in the $t$-th iteration is as follows:
\begin{equation}\begin{split}\label{eq:non1}
    \um^t = \xm^t + \frac{\alpha^{t-1}-1}{\alpha^t}(\xm^t - \xm^{t-1}),
     \vm^t = P_{\sp(\sigma(\xm^t))} (\um^t), \\
    \xm^{t+1} = \prox_h (\vm^t - s\nabla g(\vm^t)) , 
     \alpha^{t+1} = \frac{\sqrt{1+4(\alpha^{t}) ^2}+1}{2},
\end{split}\end{equation}
where $P_{\sp(\sigma(\xm^t))}(\cdot)$ is the support projection operator which projects the singular value vector of a matrix to the support of the singular value vector of $\xm^t$ as $P_{\sp(\sigma(\xm^t))}(\tm) = \am \Sigma_{\textrm{projected}} \bm^T,$
with $\tm = \am \Sigma \bm^T$ being the singular value decomposition, and 
$\Sigma_{\textrm{projected}} (i,i) = \Sigma(i,i) $ if $i$ is in the support of $\sigma(\xm^t)$, otherwise $=0$. Here the support projection is employed to enforce the support shrinkage property mentioned in Lemma~\ref{lem:3}. The algorithm for the non-monotone accelerated proximal gradient descent with support projection is summarized in Algorithm~\ref{alg:2}. Lemma~\ref{lem:4} shows the support shrinkage property for Algorithm~\ref{alg:2}, and Theorem~\ref{thm:3} presents the convergence rate. 
%\begin{algorithm}[tb]
%   \caption{Non-monotone Accelerated Proximal Gradient Descent with Support Projection for the Rank-Regularized Problem}
%   \label{alg:2}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} Initialization $\xm^0$,
%   $\xm^1 = \xm^0$, $\alpha^0 = 1$
%   step size $s$, regularization parameter $\lambda$.
%   \FOR{$t=1, \dots $} 
%   % \STATE Update $\um^{t}, \vm^{t}, \xm^{t+1}, \alpha^{t+1}$ according to Eq.~(\ref{eq:non1}), Eq.~(\ref{eq:non2}), Eq.~(\ref{eq:non3}), Eq.~(\ref{eq:non4}), respectively.
%   \STATE Update $\um^{t}, \vm^{t}, \xm^{t+1}, \alpha^{t+1}$ according to Eq.~(\ref{eq:non1}).
%%    Update $\um^{t}$ according to Eq.~(\ref{eq:non1}),
%% Update $\vm^{t}$ according to Eq.~(\ref{eq:non2}),
%%  Update $\xm^{t+1}$ according to Eq.~(\ref{eq:non3}),
%%  Update $\alpha^{t+1}$ according to Eq.~(\ref{eq:non4}).
%   \ENDFOR
%\end{algorithmic}
%\end{algorithm}
\begin{lemma}\label{lem:4}
    The sequence $\{\xm^t\}_{t=1}^{\infty}$ generated by Algorithm~\ref{alg:2} satisfies $\sp(\sigma(\xm^{t+1})) \subseteq \sp(\sigma(\xm^{t})), t \geq 1$.
\end{lemma}
\begin{theorem}\label{thm:3}
      Suppose $s \leq \min \{\frac{2\lambda}{G^2}, \frac{1}{L}\}$. and $\xm^*$ is a limit point of $\{\xm^{t}\}_{t=0}^{\infty}$ generated by Algorithm~\ref{alg:2}, 
  then there exists $t_0 \geq 1$ such that for all $m \geq t_0$, we have
    \begin{equation}
        F(\xm^{m+1}) - F(\xm^*) \leq \frac{4}{(m+1)^2}[\frac{1}{2s} \|(\alpha^{t_0-1}-1)\xm^{t_0-1} - \alpha^{t_0-1}\xm^{t_0} +\xm^*\|_F^2 + (\alpha^{t_0-1})^2(F(\xm^{t_0})-F(\xm^*))].
    \end{equation}
%    where $V^{t_0}$ is a value defined as
%    $ V^{t_0} =  \frac{1}{2s} \|(\alpha^{t_0-1}-1)\xm^{t_0-1} - \alpha^{t_0-1}\xm^{t_0} +\xm^*\|_F^2 + (\alpha^{t_0-1})^2(F(\xm^{t_0})-F(\xm^*))$.
\end{theorem}
\subsection{Monotone Accelerated Proximal Gradient Descent with Support Projection}\label{subsec:mon}
To ensure the objective is non-increasing, we introduce the following  algorithm which is summarized in Algorithm~\ref{alg:3}~\cite{beck2017first}:
\begin{equation}\begin{split}\label{eq:mo1}
    \um^t = \xm^t + \frac{\alpha^{t-1}-1}{\alpha^t}(\xm^t - \xm^{t-1}) 
    + \frac{\alpha^t-1}{\alpha^t}(\zm^t - \xm^t),
     \vm^t &= P_{\sp(\sigma(\zm^t))} (\um^t), \\
    \zm^{t+1} = \prox_h (\vm^t - s\nabla g(\vm^t)) , 
     \alpha^{t+1} = \frac{\sqrt{1+4(\alpha^{t})^ 2}+1}{2},
     \xm^{t+1} &= \begin{cases}
         \zm^{t+1} & \text{if } F(\zm^{t+1}) \leq F(\xm^t), \\
         \xm^t & \text{otherwise.} \\
     \end{cases}
\end{split}\end{equation}
%\begin{algorithm}[tb]
%   \caption{Monotone Accelerated Proximal Gradient Descent with Support Projection for the Rank-Regularized Problem}
%   \label{alg:3}
%\begin{algorithmic}
%   \STATE {\bfseries Input:} Initialization $\xm^0$,
%   $\zm^1 = \xm^1 = \xm^0$, $\alpha^0 = 1$
%   step size $s$, regularization parameter $\lambda$.
%   \FOR{$t=1, \dots $} 
%   % \STATE Update $\um^{t}, \vm^t, \zm^{t+1}, \alpha^{t+1}, \xm^{t+1}$ according to Eq.~(\ref{eq:mo1}), Eq.~(\ref{eq:mo2}), Eq.~(\ref{eq:mo3}), Eq.~(\ref{eq:mo4}), and Eq.~(\ref{eq:mo5}) respectively.
%   \STATE Update $\um^{t}, \vm^t, \zm^{t+1}, \alpha^{t+1}, \xm^{t+1}$ according to Eq.~(\ref{eq:mo1}).
%   \ENDFOR
%\end{algorithmic}
%\end{algorithm}
Lemma~\ref{lem:5} shows the support shrinkage property and Theorem~\ref{thm:4} presents the convergence rate. 
\begin{lemma}\label{lem:5}
    The sequence $\{\zm^t\}_{t=1}^{\infty}$ and $\{\xm^t\}_{t=1}^{\infty}$ generated by Algorithm~\ref{alg:3} satisfies
$\sp(\sigma(\zm^{t+1})) \subseteq \sp(\sigma(\zm^{t})), t \geq 1,
        \sp(\sigma(\xm^{t+1})) \subseteq \sp(\sigma(\xm^{t})), t \geq 1$.
\end{lemma}
\begin{theorem}\label{thm:4}
      Suppose $s \leq \min \{\frac{2\lambda}{G^2}, \frac{1}{L}\}$. and $\xm^*$ is a limit point of $\{\xm^{t}\}_{t=0}^{\infty}$ generated by Algorithm~\ref{alg:3}, 
  then there exists $t_0 \geq 1$ such that for all $m \geq t_0$, we have
    \begin{equation}
        F(\xm^{m+1}) - F(\xm^*) \leq \frac{4}{(m+1)^2}[\frac{1}{2s} \|(\alpha^{t_0-1}-1)\xm^{t_0-1} - \alpha^{t_0-1}\zm^{t_0} +\xm^*\|_F^2  + (\alpha^{t_0-1})^2(F(\xm^{t_0})-F(\xm^*))].
    \end{equation}
    %where $W^{t_0}$ is a value defined as 
%$W^{t_0} = \frac{1}{2s} \|(\alpha^{t_0-1}-1)\xm^{t_0-1} - \alpha^{t_0-1}\zm^{t_0} +\xm^*\|_F^2  + (\alpha^{t_0-1})^2(F(\xm^{t_0})-F(\xm^*))$.
\end{theorem}
We leave all the detailed proof in the Appendix due to space limit.
%\section{Experiments}\label{sec:exp}
%We demonstrate the efficiency of the proposed algorithms and compare them with the monotone accelerated proximal gradient (Mon-APG) method for nonconvex problems from~\cite{li2015accelerated} without support projection. Fig.~(\ref{fig:1}) shows the convergence plots for Algorithm~\ref{alg:1} PGD, Algorithm~\ref{alg:3} Mon-PGD, and Mon-APG, the plots of Algorithm~\ref{alg:2} Non-PGD shares a similar one with Mon-PGD. We can see that our proposed accelerated algorithm has better convergence performance. We also show the support shrinkage property of the singular values in Algorithm~\ref{alg:1} PGD in Fig.~(\ref{fig:2}). We can see that the support set of the singular value vector keeps shrinking in the update process, and this improves the interpretability of the algorithm. 