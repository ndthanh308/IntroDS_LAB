\section{Preliminaries}\label{sec:pre}

\subsection{Notations}
Throughout this paper, we use uppercase bold letters for matrices, lowercase bold letters for vectors, and lowercase letters for scalars. We use superscript to represent the current iteration. $\sigma_i (\cdot)$ is  the $i-$th largest singular value of a matrix, $\sigma_{min} (\cdot)$ and $\sigma_{max} (\cdot)$ represent the smallest and the largest singular value of a matrix, respectively, $\sigma(\cdot)$ indicates the vector formed by the singular values of a matrix, and $\dg(\sigma(\cdot))$ denotes the diagonal matrix formed by the vector. $|\cdot|$ denotes the cardinality of a set, supp$(\cdot)$ denotes the support of a vector. For a matrix $\xm \in \mathbb{R}^{n \times k}$, let $\sigma(\xm) = (\sigma_{max} (\xm), \sigma_2 (\xm), \dots, \sigma_{min} (\xm))$, then we can see rank$(\xm) = \|\sigma(\xm)\|_0$. 
$S = \sp(\sigma(\xm^0))$, $\xm^0$ is the initialization. 
The proximal mapping associated with $h(\cdot)$ is defined as $\textrm{prox}_h(\um) = \argmin_{\vm} h(\vm) + \frac{1}{2} \|\um-\vm\|^2_F$. 



\subsection{Assumptions}
In this paper, 
$g(\xm)$ in Eq.~(\ref{eq:fx}) is assumed to satisfy the following assumptions:
\begin{assumption}
1) g($\cdot)$ is convex, and $\nabla g(\cdot)$ has bounded singular value $\sigma_{max}(\nabla g(\xm))) \leq G$ for any $\xm$;

2) g($\cdot)$ has L-Lipschitz continuous gradient, $\| \nabla g(\xm_1) - \nabla g (\xm_2) \|_F \leq L\|\xm_1 - \xm_2\|_F$.
\label{ass:gx}
\end{assumption}

For the choice of $g(\cdot)$ in this paper, we let it be the squared loss $\|\ym - \dm \xm\|_F^2$, where $\ym \in \mathbb{R}^{d \times k}, \dm \in \mathbb{R}^{d \times n},\xm \in \mathbb{R}^{n \times k}$. 