{
  "title": "Strictly Low Rank Constraint Optimization -- An Asymptotically $\\mathcal{O}(\\frac{1}{t^2})$ Method",
  "authors": [
    "Mengyuan Zhang",
    "Kai Liu"
  ],
  "submission_date": "2023-07-04T16:55:41+00:00",
  "revised_dates": [],
  "abstract": "We study a class of non-convex and non-smooth problems with \\textit{rank} regularization to promote sparsity in optimal solution. We propose to apply the proximal gradient descent method to solve the problem and accelerate the process with a novel support set projection operation on the singular values of the intermediate update. We show that our algorithms are able to achieve a convergence rate of $O(\\frac{1}{t^2})$, which is exactly same as Nesterov's optimal convergence rate for first-order methods on smooth and convex problems. Strict sparsity can be expected and the support set of singular values during each update is monotonically shrinking, which to our best knowledge, is novel in momentum-based algorithms.",
  "categories": [
    "math.OC",
    "cs.AI"
  ],
  "primary_category": "math.OC",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14344",
  "pdf_url": null,
  "comment": "Accepted by Workshop SODS on ICML'23",
  "num_versions": null,
  "size_before_bytes": 1094115,
  "size_after_bytes": 912717
}