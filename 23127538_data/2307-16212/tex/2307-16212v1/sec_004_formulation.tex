In this section, to solve the robust multi-agent reinforcement learning problem with state uncertainty, we first introduce the framework of the Markov game with state perturbation adversaries. We then provide characterization results for the proposed framework: Markov and history-dependent policies, the definition of a solution concept called robust equilibrium based on value functions, derivation of Bellman equations, as well as certain conditions for the existence of a robust equilibrium and the optimal value function. 

\subsection{Markov Game with State Perturbation Adversaries}

We use a tuple $\tilde{G}:= (\mathcal{N}, \mathcal{M}, S, \{A^i\}_{i \in \mathcal{N}},  \{B^{\tilde{i}}\}_{\tilde{i} \in \mathcal{M}}, \{r^i\}_{i \in \mathcal{N}}, p, f, \gamma)$ to denote a Markov game with state perturbation adversaries (MG-SPA). In an MG-SPA, we introduce an additional set of adversaries   {$\mathcal{M} = \{\tilde{1}, \cdots, \tilde{N}\}$} to a Markov game (MG) with an agent set $\mathcal{N}$. Each agent $i$ is associated with an adversary $\tilde{i}$ and can observe the true state $s \in {S}$ if without adversarial perturbation. Each adversary $\tilde{i}$ is associated with an action $ b^{\tilde{i}} \in {B}^{\tilde{i}}$ and the same state $s \in {S}$ that agent $i$ has. We define the adversaries' joint action as   {$ b = (b^{\tilde{1}}, ..., b^{\tilde{N}}) \in {B}$, ${B} = {B}^{\tilde{1}} \times \cdots \times {B}^{\tilde{N}}$}. At time $t$, adversary $\tilde{i}$ can manipulate the corresponding agent $i$'s state information. Once   {adversary $\tilde{i}$} gets state $s_t$, it chooses an action $ b^{\tilde{i}}_t$ according to a policy $ \rho^{\tilde{i}}: {S} \rightarrow  \Delta({B}^{\tilde{i}})$. According to a perturbation function $f$,   {adversary $\tilde{i}$} perturbs state $s_t$ to $\tilde{s}^i_t = f(s_t,  b^{\tilde{i}}_t) \in {S}$. We use $\tilde{s}_t = (\tilde{s}_t^1, \cdots, \tilde{s}_t^N)$ to denote a joint perturbed state and use the notation $f(s_t, b_t) = \tilde{s}_t$. We denote the adversaries' joint policy as 
$\rho(b|s) = \prod_{\tilde{i} \in \mathcal{M}}\rho^{\tilde{i}}(b^{\tilde{i}}|s)$. 
The definitions of agent action and agents' joint action are the same as their definitions in an MG. Agent $i$ chooses its action $a^i_t$ with $\tilde{s}^i_t$ according to a policy $\pi^i (a^i_t|\tilde{s}_t^i)$, $\pi^i: {S} \rightarrow \Delta(A^i)$. We denote the agents' joint policy as 
$\pi(a|\tilde{s}) = \prod_{i \in \mathcal{N}}\pi(a^i|\tilde{s}^i)$. 
Agents execute the agents' joint action $a_t$, then at time $t+1$, the joint state $s_t$ turns to the next state $s_{t+1}$ according to a transition probability function $p: {S} \times {A} \times B \rightarrow \Delta({S})$. Each agent $i$ gets a reward according to a state-wise reward function $r^i_t: {S} \times {A} \times B \rightarrow {\mathbb{R}}$. Each   {adversary $\tilde{i}$} gets an opposite reward $-r^i_t$. In an MG, the transition probability function and reward function are considered as the model of the game. In an MG-SPA, the perturbation function $f$ is also considered as a part of the model, i.e., the model of an MG-SPA consists of $f,p$ and $\{r^i\}_{i \in \mathcal{N}}$.
\\

\begin{definition}[Value Functions] 
\label{def_value_function}
\textbf{}

$v^{\pi,\rho} = (v^{\pi,\rho,1}, \cdots, v^{\pi,\rho,N}), q^{\pi, \rho} = (q^{\pi,\rho,1}, \cdots, q^{\pi,\rho,N})$ are defined as the state-value function or value function for short, and the action-value function, respectively. The $i$th element $v^{\pi,\rho,i}$ and $q^{\pi,\rho,i}$ are defined as following:
\begin{align}
\label{def_action_value_function}
            q^{\pi,\rho,i}(s, a, b) &= \mathbb{E} \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r_t^i |
            s_1 = s, a_1 = a, b_1 = b, a_t \sim \pi(\cdot | \tilde{s}_t), b_t \sim \rho(\cdot | s_t), \tilde{s}_t = f(s_t,  b_t) \right],\\
\label{def_state_value_function}
            v^{\pi,\rho,i}(s) &= \mathbb{E} \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r_t^i |
            s_1 = s, a_t \sim \pi(\cdot | \tilde{s}_t), b_t \sim \rho(\cdot | s_t), \tilde{s}_t = f(s_t,  b_t)\right].
\end{align}
\end{definition}

To incorporate realistic settings into our analysis, we restrict the power of each adversary, which is a common assumption for state perturbation adversaries in the RL literature~\citep{zhang2020robust, zhang2021robust,everett2021certifiable}. We define perturbation constraints $\tilde{s}^i \in \mathcal{B}_{dist}(\epsilon, s) \subset S$ to restrict the   {adversary $\tilde{i}$} to perturb a state only to a predefined set of states. $\mathcal{B}_{dist}(\epsilon, s)$ is a $\epsilon$-radius ball measured in metric $dist(\cdot, \cdot)$, which is often chosen to be the $l$-norm distance: $dist(s, \tilde{s}^i) = \|s - \tilde{s}^i\|_l$. We omit the subscript $dist$ in the following context. 
For each agent $i$, it attempts to maximize its expected sum of discounted rewards, i.e. its objective function $J^i(s, \pi, \rho) = \mathbb{E} \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r_t^i(s_t,a_t) | s_1 = s, a_t \sim \pi(\cdot | \tilde{s}_t), \tilde{s}_t = f(s_t, b_t), b_t \sim \rho(\cdot | s_t) \right]$.
Each adversary $\tilde{i}$ aims to minimize the objective function of agent $i$ and is considered as receiving an opposite reward of agent $i$, which also leads to a value function $-J^i(s, \pi, \rho)$ for   {adversary $\tilde{i}$}. We further define the value functions in an MG-SPA as in Definition \ref{def_value_function}. Then we propose robust equilibrium (RE), a NE-structured solution as our solution concept for the proposed MG-SPA framework. We formally define RE in Definition \ref{def_re}.\\

\begin{definition}[Robust Equilibrium] 
\label{def_re}
\textbf{}

Given a Markov game with state perturbation adversaries $\tilde{G}$, a joint policy $d_* = (\pi_*, \rho_*)$ where $\pi_* = (\pi^1_*, \cdots, \pi^N_*)$ and   {$\rho_* = (\rho^{\tilde{1}}_*, \cdots, \rho^{\tilde{N}}_*)$} is said to be in robust equilibrium, or a robust equilibrium, if and only if, for any $i \in \mathcal{N}$, $\tilde{i} \in \mathcal{M}$, $s \in S$, 
% $v^{(\pi^{-i}_*,\pi_*^i,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}), i}(s) \geq v^{(\pi^{-i}_*, \pi^{i}_*,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}_*), i}(s) \geq v^{(\pi^{-i}_*, \pi^{i},  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}_*), i}(s)$
\begin{align}
    v^{(\pi^{-i}_*,\pi_*^i,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}), i}(s) \geq v^{(\pi^{-i}_*, \pi^{i}_*,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}_*), i}(s) \geq v^{(\pi^{-i}_*, \pi^{i},  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}_*), i}(s), 
\end{align}
% for all $\pi^i$ and $ \rho^{\tilde{i}}$, 
where   {$-i/-\tilde{i}$} represents the indices of all agents/adversaries except agent  $i$/  {adversary $\tilde{i}$}. 
\end{definition} 
As the maximin solution is also a popular solution concept in robust RL problems \citep{zhang2020robust}, here, we discuss why we choose a NE-structured solution other than a maximin solution. Firstly, we aim to propose a framework that can describe and model the interactions among agents when each agent has its own interest or reward function under state perturbations, and maximin solution may not be a general solution concept for robust MARL problems. A maximin solution is natural to use when considering robustness in single-agent RL problems and identical-interest MARL problems, but it fails to handle those MARL problems where each agent has its own interest or reward function. Secondly, the Nash equilibrium is also a commonly used robust solution concept in both single-agent RL and MARL problems \citep{tessler2019action, zhang2020robust_nips}. Many papers have used NE as their solution concept to investigate robustness in RL problems, and to the best of our knowledge, the NE-structured solution is the only one used in robust non-identical-interest MARL problems \citep{zhang2020robust_nips}. Lastly, for finite two-agent zero-sum games, it is known that NE, minimax, and maximin solution concepts all give the same answer \citep{yin2010stackelberg, owen2013game}. 

After defining RE, we seek to characterize the optimal value $v_*(s) = (v^{1}_*(s), \cdots, v^{N}_*(s))$ defined by 
%\begin{align}
$\label{def_optimal_v}
v^{i}_*(s) = \max_{\pi^i}\min_{ \rho^{\tilde{i}}}v^{(\pi^{-i}_*,\pi^i,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}), i}(s)$.
%\end{align}
For notation convenience, we use $v^i(s)$ to denote $v^{(\pi^{-i}_*,\pi^i,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}), i}(s)$. 
% i.e.
% \begin{align}
% \label{notation_vi}
%     v^i(s) = v^{(\pi^{-i}_*,\pi^i,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}}), i}(s).
% \end{align}
The Bellman equations of an MG-SPA are in the forms of \eqref{def_bellman_q} and \eqref{def_bellman_optimality}.
The Bellman equation is a recursion for expected rewards, which helps us identify or find an RE. 
\begin{align}
\label{def_bellman_q}
    q^{i}_*(s, a, b) &= r^i(s, a, b) + \gamma \sum_{s^\prime \in S} p(s^\prime | s, a, b) \max_{\pi^i}\min_{ \rho^{\tilde{i}}} \mathbb{E}\left[ q^{i}_*(s^\prime, a^\prime, b^\prime) | a^\prime \sim \pi(\cdot | \tilde{s}), b^\prime \sim \rho(\cdot | s) \right], \\
\label{def_bellman_optimality}
    v^{i}_*(s) &= \max_{\pi^i} \min_{ \rho^{\tilde{i}}} \mathbb{E}\left[\sum_{s^{\prime} \in {S}}p(s^{\prime}| s, a, b)[r^i(s,a,b)+\gamma v^{i}_*(s^{\prime})] |  a \sim \pi(\cdot | \tilde{s}), b \sim \rho(\cdot | s)\right],
\end{align}
for all $i \in \mathcal{N}$, $\tilde{i} \in \mathcal{M}$, where $\pi = (\pi^i, \pi^{-i}_*), \rho = ( \rho^{\tilde{i}},  \rho^{-\tilde{i}}_*)$, and $(\pi^i_*, \pi^{-i}_*, \rho^{\tilde{i}}_*, \rho^{-\tilde{i}}_*)$ is a robust equilibrium for $\tilde{G}$. We prove them in the following subsection. The policies in\eqref{def_bellman_q} and \eqref{def_bellman_optimality} are defined to be Markov policies which only input the current state. The robust equilibrium is also based on Markov policies. History-dependent policies for MARL under state perturbations may improve agents' ability to adapt to adversarial state perturbations and random sensor noise, by allowing agents to take into account past observations when making decisions. Therefore, we further discuss how the current MG-SPA frame and solution concept adapt to history-dependent policies in subsection \ref{sec_history}.


\subsection{Theoretical Analysis of MG-SPA}
\label{sec_Theoretical_Analysis_of_MG_SPA}

In this subsection, we first introduce the vector notations we used in the theoretical analysis and define a minimax operator in Definition \ref{definition_minimax}. We then introduce Assumption \ref{assumption} which is considered in our theoretical analysis. Later, we prove two propositions about the minimax operator and Theorem \ref{new_theorem} which shows a series of fundamental characteristics of an MG-SPA under Assumption \ref{assumption}, e.g., the derivation of Bellman equations, the existence of optimal value functions and robust equilibrium.

\textbf{Vector Notations:} To make the analysis easy to read, we follow and extend the vector notations in \cite{puterman2014markov}. Let $V$ denote the set of bounded real valued functions on ${S}$ with component-wise partial order and norm $\|v^i\| := \sup_{s \in {S}} |v^i(s)|$. Let $V_M$ denote the subspace of $V$ of Borel measurable functions. For discrete state space, all real-valued functions are measurable so that $V = V_M$. But when ${S}$ is a continuum, $V_M$ is a proper subset of $V$. Let $v = (v^1, \cdots, v^N) \in \mathbb{V}$ be the set of bounded real valued functions on $S \times \cdots \times S$, i.e. the across product of $N$ state set and norm $\|v\|:= \sup_{j} \|v^j\|$.
% Let $e \in V$ denote the function with all components equal to $1$, that is $e(s) = 1$ for all $s \in \mathcal{S}$. 
% For discrete ${S}$, we often refer to elements of $V$ as vectors and linear operators on $V$ as metrices. When using the above norm on $V$, the corresponding matrix norm is given by $||H|| = \sup_{s \in {S}} \sum_{s^{\prime} \in {S}} |H(s^{\prime} | s)|$, where $H(s^\prime | s)$ denotes the $(s,s^{\prime})$th component of $H$.
For discrete ${S}$, let $|{S}|$ denote the number of elements in ${S}$. Let $r^i$ denote a $|S|$-vector, with $s$th component $r^i(s)$ which is the expected reward for agent $i$ under state $s$. And $P$ the $|{S}| \times |{S}|$ matrix with $(s, s^\prime)$th entry given by $p(s^\prime | s)$. We refer to $r_d^i$ as the reward vector of agent $i$, and $P_d$ as the probability transition matrix corresponding to a joint policy $d = (\pi, \rho)$. $r^i_d + \gamma P_d v^i$ is the expected total one-period discounted reward of agent $i$, obtained using the joint policy $d = (\pi, \rho)$. Let $z$ as a list of joint policy $\{d_1, d_2, \cdots\}$ and $P^0_z = I$, we denote the expected total discounted reward of agent $i$ using $z$ as $v^{i}_z = \sum_{t = 1}^{\infty}\gamma^{t-1}P^{t-1}_z r_{d_t}^i = r_{d_1}^i + \gamma P_{d_1}r_{d_2}^i + \cdots + \gamma^{n-1}P_{d_1}\cdots P_{d_{n-1}}r^i_{t_n} + \cdots$. Now, we define the following minimax operator which is used in the rest of the paper.\\







\begin{definition}[Minimax Operator]
\label{definition_minimax}
\textbf{}

For $v^i \in V, s \in S$, we define the nonlinear operator ${L^i}$ on $v^i(s)$ by ${L^i} v^i(s) := \max_{\pi^i}  \min_{ \rho^{\tilde{i}}} [ r_d^i + \gamma P_d v^i](s)$, 
% \begin{align}
%     {L_d} v^i := \max_{\pi^i}  \min_{ \rho^{\tilde{i}}} [ r_d^i + \gamma P_d v^i ]
% \end{align}
where $d := (\pi^{-i}_*,\pi^i,  \rho^{-\tilde{i}}_*,  \rho^{\tilde{i}})$. We also define the operator $L v(s) = L (v^1(s), \cdots, v^N(s)) = (L^1 v^1(s), \cdots, L^N v^N(s))$. Then $L^iv^i$ is a $|S|$-vector, with $s$th component $L^iv^i(s)$. 
\label{def_minimax_operator}
\end{definition}

For discrete ${S}$ and bounded $r^i$, it follows from Lemma 5.6.1 in \cite{puterman2014markov} that ${L^i} v^i \in V$ for all $v^i \in V$. Therefore $L v \in \mathbb{V}$ for all $v \in \mathbb{V}$. 
% For convenience, without of generality, we omit the superscript $i$ on $v^i,r^i_d, \pi^i,  \rho^{\tilde{i}}, v^{i}_z$. 
And in this paper, we consider the following assumptions in Markov games with state perturbation adversaries.\\
\begin{assumption}{}
\textbf{}

(1) Bounded rewards; $|r^i(s,a,b)| \leq M^i < M <\infty$ for all $i \in \mathcal{N}$, $a \in {A}$, $b \in B$, and $s \in {S}$.

(2) Finite state  and action spaces:  all ${S}, A^i,  B^{\tilde{i}}$ are finite.

(3) Stationary transition probability and reward functions.

(4) $f(s, \cdot)$ is a bijection for any fixed $s \in S$.

(5) All agents share one common reward function.

\label{assumption}
\end{assumption}
Finite state and action spaces, bounded rewards, stationary transition kernels, and stationary reward functions are common assumptions in both reinforcement learning and multi-agent reinforcement learning literature~\citep{puterman2014markov, bacsar1998dynamic}. Additionally, the bijection property of perturbation functions implies that in a finite MG-SPA, adversaries that adopt deterministic policies provide a permutation on the state space. Collaboration and coordination among agents are often required in real-world scenarios to achieve a common goal. In such cases, a shared reward function can motivate agents to work together effectively. Moreover, the assumption of a shared reward function is necessary to transform an MG-SPA into a zero-sum two-agent extensive-form game in our proof. Although these assumptions do not always hold true in real-world applications, they provide good properties for an MG-SPA and enable the first attempt of theoretical analysis on an MG-SPA.
The next two propositions characterize the properties of the minimax operator $L$ and space $\mathbb{V}$. We provide the proof in Appendix \ref{appendix_propositions}. These contraction mapping and complete space results are used in the proof of RE existence for an MG-SPA.\\

\begin{proposition}[Contraction mapping]
\label{proposition_contraction}
\textbf{}

Suppose $0 \leq \gamma < 1$,   {and Assumption \ref{assumption} holds.} Then ${L}$ is a contraction mapping on $\mathbb{V}$.
\end{proposition}
\begin{proposition}[Complete Space]
\label{proposition_complete}
The space $\mathbb{V}$ is a complete normed linear space.
\end{proposition}

In Theorem \ref{new_theorem}, we show some fundamental characteristics of an MG-SPA. In (1), we show that an optimal value function of an MG-SPA satisfies the Bellman equations by applying the Squeeze theorem [Theorem 3.3.6, \cite{sohrab2003basic}]. Theorem \ref{new_theorem}-(2) shows that the unique solution of the Bellman equation exists, a consequence of the fixed-point theorem \citep{smart1980fixed}. Therefore, the optimal value function of an MG-SPA exists under Assumption \ref{assumption}. By introducing (3), we characterize the relationship between the optimal value function and a robust equilibrium. However, (3) does not imply the existence of an RE. To this end, in (4), we formally establish the existence of RE when the optimal value function exists. We formulate a $2N$-player Extensive-form game (EFG)~\citep{osborne1994course, von2007theory} based on the optimal value function such that its Nash equilibrium (NE) policy is equivalent to an RE policy of the MG-SPA.\\

\begin{theorem}{}
\textbf{}

Suppose $0 \leq \gamma < 1$ and Assumption \ref{assumption} holds. 

(1) (Solution of Bellman equation)
A value function $v_* \in \mathbb{V}$ is an optimal value function if for all $i \in \mathcal{N}$, the point-wise value function $v^i_{*} \in V$ satisfies the corresponding Bellman Equation \eqref{def_bellman_optimality},
i.e. $v_* = Lv_*$.

(2) (Existence and uniqueness of optimal value function)
There exists a unique $v_* \in \mathbb{V}$ satisfying $Lv_* = v_*$, i.e. for all $i \in \mathcal{N}$, $L^i v^i_* = v^i_*$. %Thus the optimal value function exists. 

(3) (Robust equilibrium and optimal value function)
A joint policy $d_* = (\pi_*, \rho_*)$, where $\pi_* = (\pi^1_*, \cdots, \pi^N_*)$ and $\rho_* = (\rho^{\tilde{1}}_*, \cdots, \rho^{\tilde{N}}_*)$, is a robust equilibrium if and only if $v^{d_*}$ is the optimal value function. 

(4) (Existence of robust equilibrium)
There exists a mixed RE for an MG-SPA.
\label{new_theorem}

\end{theorem}

\begin{proof}
    The full proof of Theorem \ref{new_theorem} is presented in Appendix \ref{appendix_theory}, specifically in \ref{appendix_new_theorem}. We provide a high-level proof sketch here. Our proof consists of two main parts: 1. Constructing an extensive-form game that is connected to an MG-SPA. 2. Proof of Theorem \ref{new_theorem}. 
    In the first part, we begin by constructing an extensive-form game (EFG) whose payoff function is related to the value functions of an MG-SPA (Appendix \ref{sec_coop_efg}). Using the EFG as a tool, we can analyze the properties of an MG-SPA. We provide insights into solving an MG-SPA by solving a constructed EFG: a robust equilibrium (RE) of an MG-SPA can be derived from a Nash equilibrium of an EFG when the EFG's payoff function is related to the optimal value function of the MG-SPA (Lemma \ref{lemma_efg_ne_re} in Appendix). Thus, by providing conditions under which a Nash equilibrium of a well-constructed EFG exists (Appendix \ref{appendix_efg_ne_exist}), we can prove the existence of an RE of the MG-SPA (Theorem \ref{new_theorem}-(4)). The existence of an optimal value function is not yet proven and is left for the second part. 
    In the second part, we prove Theorem \ref{new_theorem}-(1) by showing that for all $i$, there exists a $v^i \in V$ such that $v^i \geq Lv^i$, then $v^i \geq v^i_*$, and there also exists a $v^i \in V$ such that $v^i \leq Lv^i$, then $v^i \leq v^i_*$. Propositions \ref{proposition_contraction} and \ref{proposition_complete} enable us to use Banach Fixed-Point Theorem \citep{smart1980fixed} to prove Theorem \ref{new_theorem}-(2). The proof of Theorem \ref{new_theorem}-(3) benefits from the definitions of the optimal value function and robust equilibrium, Theorem \ref{new_theorem}-(1) and (2). Finally, given the existence of the optimal value function and the results from the first part, we prove the existence of an RE.
\end{proof}

\iffalse
In the following theorems, we show the fundamental theoretical analysis of an MG-SPA. In Theorem \ref{theorem_solution_bellman}, 
we show that the optimal value function of an MG-SPA satisfies the Bellman Equations by applying the Squeeze theorem [Theorem 3.3.6, \cite{sohrab2003basic}]. The proof is provided in Appendix \ref{proof_theorem_solution_bellman}. Then we show that the unique solution of the Bellman Equation exists using the fixed-point theorem \citep{smart1980fixed} and contraction mapping \citep{reich1971some} in Theorem \ref{theorem_optimal_v_exist} that is proved in Appendix \ref{proof_theorem_optimal_v_exist}. To construct the relationship between the Bellman Equation and Robust Equilibrium, we prove Theorem \ref{theorem_robust_eq_eq_optimal_v} in Appendix \ref{proof_theorem_robust_eq_eq_optimal_v}. Thus, we can establish the existence of RE when the optimal value function exists.

\begin{theorem}
\label{theorem_solution_bellman} (Solution of Bellman Equation)
A value function $v_* \in \mathbb{V}$ is an optimal value function if for all $i \in \mathcal{N}$, the point-wise value function $v^i_{*} \in V$ satisfies the corresponding Bellman Equation \eqref{def_bellman_optimality},
i.e. $v^i_* = L^i v^i_*$ for all $i \in \mathcal{N}$.
\iffalse
\begin{align}
    v_{*}^i(s) = \max_{\pi^i} \min_{ \rho^{\tilde{i}}} \sum_{a \in {A}} \sum_{b \in {B}}\rho(*,i) \pi(*,i) 
            \sum_{s^{\prime} \in {S}}p(s^{\prime}| s, a, b)[r^i(s,a,b)+\gamma v^i_{*}(s^{\prime})] \nonumber
\end{align}
for all $s \in S$, where $\pi(*,i) = \pi^i(a^i|\tilde{s}^i) \prod_{j \neq i} \pi_*^j(a^j | \tilde{s}^j)$ and $\rho(*,i) =  \rho^{\tilde{i}}( b^{\tilde{i}}|{s}^i) \prod_{j \neq i} \rho_*^j(a^j | {s}^j)$.
\fi
\end{theorem}


\begin{theorem}
\label{theorem_optimal_v_exist}
(Existence of optimal value function)
Suppose $0 \leq \gamma  < 1$, $S$ is finite or countable, and $r(s,a,b)$ is bounded. There exists a unique $v_* \in \mathbb{V}$ satisfying $Lv_* = v_*$, i.e. for all $i \in \mathcal{N}$, $L^i v^i_* = v^i_*$. Thus the optimal value function exists.
\end{theorem}


\begin{theorem}
\label{theorem_robust_eq_eq_optimal_v}
(Robust Equilibrium and Bellman Equation)
A joint policy $d_* = (\pi_*, \rho_*)$ where $\pi_* = (\pi^1_*, \cdots, \pi^N_*)$ and $\rho_* = ( \rho^{\tilde{1}}_*, \cdots, \rho^N_*)$ is a robust equilibrium if and only if $v^{d_*}$ satisfies all Bellman Equations.
\end{theorem}

We then prove conditions under which a Robust Equilibrium (RE) exists for MG-SPA in Theorem \ref{theorem_conditions_ne_stochastic}. We formulate a $2N$-player Extensive-form game (EFG)~\citep{osborne1994course, von2007theory} based on the optimal value function such that its Nash Equilibrium (NE) is equivalent to the RE of the MG-SPA. By proving the existence of the NE under certain conditions, the existence conditions of the RE are found. Though the existence of NE in a stochastic game with perfect information has been investigated \citep{nash1951non, wald1945generalization}, it is still an open and challenging problem when players have no global state or partially observable information~\citep{hansen2004dynamic, Yang2020gameMARL}.  There is a bunch of literature developing algorithms trying to find the NE in Dec-POMDP or partially observable stochastic game (POSG), and conducting algorithm analysis assuming that NE exists~\citep{chades2002heuristic, hansen2004dynamic, nair2002towards} without proving the conditions for the existence of NE. 
\fi
Though the existence of NE in a stochastic game with perfect information has been investigated \citep{shapley1953stochastic, fink1964equilibrium}, it is still an open and challenging problem when players have partially observable information~\citep{hansen2004dynamic, Yang2020gameMARL}. There is a bunch of literature developing algorithms trying to find the NE in Dec-POMDP or partially observable stochastic game (POSG), and conducting algorithm analysis assuming that NE exists~\citep{chades2002heuristic, hansen2004dynamic, nair2002towards} without proving the conditions for the existence of NE. Once established the existence of RE, we design algorithms to find it. In Section \ref{sec_algorithm}, we first develop a robust multi-agent Q-learning (RMAQ) algorithm with a convergence guarantee, then propose a robust multi-agent actor-critic (RMAAC) algorithm to handle the case with high-dimensional state-action spaces. 
\\
\begin{remark}[Heterogeneous agents and adversaries]

    In the above problem formulation, we assume all agents have the same type of state perturbations (share one $f$), and all adversaries have the same level of perturbation power (share one $\epsilon$), which made the notation more concise and the analysis more tractable. However, these assumptions are sometimes unrealistic in practice since agents/adversaries may have different capabilities. To introduce heterogeneous agents and adversaries to an MG-SPA, we let each agent $i$ has its own perturbation function $f^i$, and each adversary $\tilde{i}$ has its own perturbation power constraint $\epsilon^i$, such that $\tilde{s}^i = f^i(s, b^i) \in \mathcal{B}(\epsilon^i, s)$. We use $f(s,b) = (f^1(s,b^1), \cdots, f^N(s,b^N)) = \tilde{s}$ to denote the joint perturbation function. When all perturbation functions $f^i(s, \cdot)$ are bijective for any fixed $s \in S$, the joint perturbation function $f(s, \cdot)$ is also a bijection. Assumption \ref{assumption}-(4) still holds. When constructing an extensive-form game, the action set for player $P1$ is defined as $\tilde{S} = \mathcal{B}^1(\epsilon^1,s) \times \mathcal{B}^2(\epsilon^2,s) \times \cdots \mathcal{B}^N(\epsilon^N,s)$ instead of $\tilde{S} = \mathcal{B}(\epsilon, s) \times \cdots \times \mathcal{B}(\epsilon, s)$. The subsequent proofs still hold and they are not affected by the introduction of heterogeneous agents and adversaries. After extending our MG-SPA framework to handle heterogeneous agents and adversaries, we can model more complex and realistic multi-agent systems.
\end{remark}

\begin{remark}[A reduced case of MG-SPA: a single-agent system]   
When there is only one agent in the system, the MG-SPA problem reduces to a single-agent robust RL problem with state uncertainty, which has been studied in the literature \citep{zhang2020robust,zhang2021robust}. In this case, single-agent robust RL with state uncertainty can be seen as a specific and special instance of MG-SPA presented in this paper. However, the proposed analysis and algorithm in this paper provide a new perspective and approach to single-agent robust reinforcement learning, by explicitly modeling the adversary's perturbations and optimizing the agent's policy against them in a game-theoretic framework. Moreover, the presence of multiple agents and adversaries in the system can result in more complex and challenging interactions, joint actions and policies that do not present in single-agent RL problems. Our proposed MG-SPA framework allows for the modeling of a wide range of agent interactions, including cooperative, competitive, and mixed interactions. 
\end{remark}

\subsection{History-dependent-policy-based Robust Equilibrium}
\label{sec_history}
It is natural and desirable to consider history-dependent policies in robust MARL with state perturbations, since the agents may not fully capture the state uncertainty from the current state information, and a policy that only depends on the current state may not be sufficient for ensuring robustness. The history-dependent policy allows agents to take into account past observations when making decisions, which helps agents better reason about the adversaries' possible strategies and intentions. This is particularly true in the case of Dec-POMDPs and POSGs, where the agent cannot fully observe the state. Therefore, we further extend the above Markov-policy-based RE to a history-dependent-policy-based robust equilibrium and discuss Theorem \ref{new_theorem} under history-dependent policies in this subsection. In Section \ref{sec_algorithm}, we also discuss how the proposed algorithms can adapt to historical state input. We further validate that a history-dependent-policy-based RE outperforms a Markov-policy-based RE in Section \ref{sec_exp}.
% since from the perspective of the agent, it is dealing with a partially observable Markov decision process (POMDP). Its optimal policy is history-dependent instead of Markov.

In this subsection, we clarify the generalization steps of extending Markov-policy-based RE to history-dependent-policy-based RE. We first introduce the definition of history-dependent policy with a finite time horizon $h$ in an MG-SPA. We then give the formal definition of a history-dependent-policy-based robust equilibrium. Finally, we show that Theorem \ref{new_theorem} still holds when agents and adversaries adopt history-dependent policies.

We consider an MG-SPA with a time horizon $h$, in which adversaries and agents respectively observe the states and perturbed states in the latest $h$ time steps and adopt history-dependent policies. More concretely, adversary $\tilde{i}$ can manipulate the corresponding agent $i$'s state at time $t$ by using a history-dependent policy $\rho_h^{\tilde{i}}(\cdot|s_{h,t})$ and agent $i$ chooses its actions using a history-dependent policy $\pi_h^i(\cdot|\tilde{s}^i_{h,t})$. Specifically, once adversary $\tilde{i}$ gets the true state $s_t$ at time $t$, it chooses an action $b^{\tilde{i}}_t$ according to a history-dependent policy $\rho^{\tilde{i}}_h: S_h \rightarrow \Delta(B^{\tilde{i}})$, where $s_{h,t} = (s_t, \cdots, s_{t-h+1}) \in S_h $ is a concatenated state consists of the latest $h$ time steps of states. According to a perturbation function $f$, adversary $\tilde{i}$ perturbs state $s_t$ to $\tilde{s}^i_t = f(s_t, b^{\tilde{i}}_t) \in {S}$. The adversaries' joint policy is defined as $\rho_h(b|s_h) = \prod_{\tilde{i} \in \mathcal{M}} \rho_h^{\tilde{i}}(b^{\tilde{i}}|s_h)$. Agent $i$ chooses its action $a^i_t$ for $\tilde{s}^i_{h,t} = (\tilde{s}_t^i, \cdots, \tilde{s}_{t-h+1}^i) \in S_h$ with probability $\pi_h^i (a^i_t|\tilde{s}^i_{h,t})$ according to a history dependent policy $\pi^i_h: {S}_h \rightarrow \Delta(A^i)$. The agents' joint policy is defined as $\pi_h(a|\tilde{s}_h) = \prod_{i \in \mathcal{N}}\pi_h^i(a^i|\tilde{s}_h^i)$. Then a joint history-dependent policy $d_{h,*} = (\pi_{h,*}, \rho_{h,*})$ where $\pi_{h,*} = (\pi^1_{h,*}, \cdots, \pi^N_{h,*})$ and   {$\rho_{h,*} = (\rho^{\tilde{1}}_{h,*}, \cdots, \rho^{\tilde{N}}_{h,*})$} is said to be in a history-dependent-policy-based robust equilibrium if and only if, for any $i \in \mathcal{N}, \tilde{i} \in \mathcal{M}, s \in S$, 
$$v^{(\pi^{-i}_{h,*},\pi_{h,*}^i,  \rho^{-\tilde{i}}_{h,*},  \rho^{\tilde{i}}_h), i}(s) \geq v^{(\pi^{-i}_{h,*}, \pi^{i}_{h,*},  \rho^{-\tilde{i}}_{h,*},  \rho^{\tilde{i}}_{h,*}), i}(s) \geq v^{(\pi^{-i}_{h,*}, \pi^{i}_h,  \rho^{-\tilde{i}}_{h,*},  \rho^{\tilde{i}}_{h,*}), i}(s).$$ 
It is worth noting that the main differences between history-dependent-policy-based RE and Markov-policy-based RE are the definition and notation of policies and states. A Markov-policy-based RE is a special case of a history-dependent-policy-based RE by adopting the time horizon $h=1$. We also notice that these two REs' definitions are the same if we remove the subscript $h$ from the concatenated state and history-dependent policies. Therefore, in this paper, we use notations without the time horizon subscripts, i.e. Markov policy and Markov-policy-based RE, to avoid redundant and complicated notations. While in this subsection, we clarify the definitions of history-dependent policy and history-dependent-policy-based RE and show that Theorem \ref{new_theorem} still holds when agents and adversaries use history-dependent policies in the following corollary.\\

\begin{corollary}
\label{corollary_history}
Theorem \ref{new_theorem} still holds when all agents and adversaries in an MG-SPA use history-dependent policies with a finite time horizon.
\end{corollary}

\begin{proof}
    See Appendix \ref{appendix_history}.
\end{proof}

\begin{remark}[MG-SPA, Dec-POMDP, and POSG]

Decentralized Partially Observable Markov Decision Process (Dec-POMDP) enables a team of agents to optimize policies with partial observable states \citep{dec-pomdp2016,nair2002towards}, while a Partially Observable Stochastic Game (POSG) \citep{hansen2004dynamic,emery2004approximate} is an extension of stochastic games with imperfect information that can handle partial observable states. We are inspired by them to consider history-dependent policies for our proposed MG-SPA problem. However, there are several differences between MG-SPA, Dec-POMDP and POSG. First, unlike Dec-POMDP, an MG-SPA does not restrict all agents to share the same interest or reward function. The proposed MG-SPA framework is applicable for modeling different relationships between agents, including cooperative, competitive, or mixed interactions. Second, neither Dec-POMDP nor POSG considers the worst-case state perturbation scenarios. In contrast, in an MG-SPA, state perturbation adversaries receive opposite rewards to the agents, which motivates them to find the worst-case state perturbations to minimize the agentsâ€™ returns. As we explained in the introduction, considering worst-case state perturbations is important for MARL. Third, while in a Dec-POMDP or a POSG, all agents cannot observe the true state information, in an MG-SPA, adversaries can access the true state and utilize it to select state perturbation actions. Based on these differences in problem formulation, Dec-POMDP and POSG methods cannot solve the proposed MG-SPA problem. 
\end{remark}

