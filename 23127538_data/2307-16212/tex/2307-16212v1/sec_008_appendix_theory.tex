\label{appendix_theory}
In this section, we give the full proof of all propositions and theorems in the theoretical analysis of an MG-SPA.

In section \ref{sec_coop_efg}, we construct an extensive-form game (EFG) \citep{bacsar1998dynamic, osborne1994course, von2007theory} whose payoff function is related to value functions of an MG-SPA. We then give certain conditions under which, a Nash equilibrium for the constructed EFG exists. In section \ref{appendix_propositions}, we prove the propositions \ref{proposition_contraction} and \ref{proposition_complete}. In section \ref{appendix_new_theorem}, we give the full proof of Theorem \ref{new_theorem}. In section \ref{appendix_history}, we prove Corollary \ref{corollary_history} that Theorem \ref{new_theorem} applies to history-dependent-policy-based RE as well.

To make the appendix self-contained, we re-show the vector notations and assumptions we have presented in section \ref{sec_Theoretical_Analysis_of_MG_SPA}. Readers can also \textbf{skip} the repeated text and directly go to section~\ref{sec_coop_efg}. 

We follow and extend the vector notations in \cite{puterman2014markov}. Let $V$ denote the set of bounded real valued functions on ${S}$ with component-wise partial order and norm $\|v^i\| := \sup_{s \in {S}} |v^i(s)|$. Let $V_M$ denote the subspace of $V$ of Borel measurable functions. For discrete state space, all real-valued functions are measurable so that $V = V_M$. But when ${S}$ is a continuum, $V_M$ is a proper subset of $V$. Let $v = (v^1, \cdots, v^N) \in \mathbb{V}$ be the set of bounded real valued functions on $S \times \cdots \times S$, i.e. the across product of $N$ state set and norm $\|v\|:= \sup_{j} \|v^j\|$. We also define the set $Q$ and $\mathbb{Q}$ in a similar style such that $q^i \in Q, q \in \mathbb{Q}$.

For discrete ${S}$, let $|{S}|$ denote the number of elements in ${S}$. Let $r^i$ denote a $|S|$-vector, with $s$th component $r^i(s)$ which is the expected reward for agent $i$ under state $s$. And $P$ the $|{S}| \times |{S}|$ matrix with $(s, s^\prime)$th entry given by $p(s^\prime | s)$. We refer to $r_d^i$ as the reward vector of agent $i$, and $P_d$ as the probability transition matrix corresponding to a joint policy $d = (\pi, \rho)$. $r^i_d + \gamma P_d v^i$ is the expected total one-period discounted reward of agent $i$, obtained using the joint policy $d = (\pi, \rho)$. Let $z$ as a list of joint policy $\{d_1, d_2, \cdots\}$ and $P^0_z = I$, we denote the expected total discounted reward of agent $i$ using $z$ as $v^{i}_z = \sum_{t = 1}^{\infty}\gamma^{t-1}P^{t-1}_z r_{d_t}^i = r_{d_1}^i + \gamma P_{d_1}r_{d_2}^i + \cdots + \gamma^{n-1}P_{d_1}\cdots P_{d_{n-1}}r^i_{t_n} + \cdots$. Now, we define the following minimax operator which is used in the rest of the paper. 


\begin{definition}
[Minimax Operator, same as definition \ref{def_minimax_operator}]
\label{def_minimax_operator_appendix}
\textbf{}

For $v^i \in V, s \in S$, we define the nonlinear operator ${L^i}$ on $v^i(s)$ by ${L^i} v^i(s) := \max_{\pi^i}  \min_{    \rho^{\tilde{i}}\color{black}} [ r_d^i + \gamma P_d v^i](s)$, where $d := (\pi^{-i}_*,\pi^i,     \rho_*^{-\tilde{i}}\color{black},     \rho^{\tilde{i}}\color{black})$. We also define the operator $L v(s) = L (v^1(s), \cdots, v^N(s)) = (L^1 v^1(s), \cdots, L^N v^N(s))$. Then $L^iv^i$ is a $|S|$-vector, with $s$th component $L^iv^i(s)$. 
\end{definition}
For discrete ${S}$ and bounded $r^i$, it follows from Lemma 5.6.1 in \cite{puterman2014markov} that ${L^i} v^i \in V$ for all $v^i \in V$. Therefore $L v \in \mathbb{V}$ for all $v \in \mathbb{V}$. And in this paper, we consider the following assumptions in Markov games with state perturbation adversaries.

\begin{assumption}
[Same as assumption \ref{assumption}]
\label{assumption_appendix}
\textbf{}

(1) Bounded rewards; $|r^i(s,a,b)| \leq M^i < M <\infty$ for all $i \in \mathcal{N}$, $a \in {A}$, $b \in B$ and $s \in {S}$.

(2) Finite state  and action spaces:  all ${S}, A^i,  B^{\tilde{i}}$ are finite.

(3) Stationary transition probability and reward functions.

(4) $f(s, \cdot)$ is a bijection for any fixed $s \in S$.

(5) All agents share one common reward function.
% \begin{enumerate}
%     % \item Stationary rewards and transition probabilities.
%     \item Bounded rewards; $|r^i(s,a,b)| \leq M^i < M <\infty$ for all $i \in \mathcal{N}$, $a \in {A}$, $b \in {B}$ and $s \in {S}$.
%     \item Discrete state spaces; all ${S}^i$ is finite or countable.
%     \item Stationary transition probability and reward functions.
% \end{enumerate}
\end{assumption}



\subsection{Extensive-form game}
\label{sec_coop_efg}

% Figure environment removed


 

An extensive-form game (EFG) \citep{bacsar1998dynamic, osborne1994course, von2007theory} basically involves a tree structure with several nodes and branches, providing an explicit description of the order of players and the information available to each player at the time of his decision.

Look at Figure \ref{fig_efg_nature_apendix}, an EFG involves from the top of the tree to the tip of one of its branches. And a centralized nature player ($P1$) has $|\tilde{S}|$ alternatives (branches) to choose from, whereas a centralized agent ($P2$) has $|A|$ alternatives, and the order of play is that the centralized nature player acts before the centralized agent does. The set $A$ is the same as the agents' joint action set in an MG-SPA, set $\tilde{S}$ is a set of perturbed states constrained by a constrained parameter $\epsilon$. At the end of lower branches, some numbers will be given. These numbers represent the playoffs to the centralized agent (or equivalently, losses incurred to the centralized nature player) if the corresponding paths are selected by the players. We give the formal definition of an EFG we will use in the proof and the main text as follows:



\begin{definition}
\label{definition_efg}
An extensive-form game based on $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$ under $s \in S$ is a finite tree structure with:
\begin{enumerate}
    \item A player $P1$ has a action set $\tilde{S} = \overbrace{\mathcal{B}(\epsilon, s) \times \cdots \times \mathcal{B}(\epsilon, s)}^{N}$, with a typical element designed as $\tilde{s}$. And $P1$ moves first.
    \item Another player $P2$ has an action set $A$, with a typical element designed as $a$. And $P2$ which moves after $P1$.
    \item A specific vertex indicating the starting point of the game.
    \item A payoff function $g_s(\tilde{s}, a) = (g^1_s(\tilde{s}, a), \cdots, g^N_s(\tilde{s}, a))$ where $g^i_s(\tilde{s}, a) = r^i(s, a,  f^{-1}_s(\tilde{s})) + \sum_{s^\prime}p(s^\prime |a,  f^{-1}_s(\tilde{s}))v^i(s^\prime)$ assigns a real number to each terminal vector of the tree. Player $P1$ gets $-g_s(\tilde{s}, a)$ while player $P2$ gets $g_s(\tilde{s}, a)$.
    \item A partition of the nodes of the tree into two player sets (to be denoted by $\bar{N}^1$ and $\bar{N}^2$ for $P1$ and $P2$, respectively).
    \item A sub-partition of each player set $\bar{N}^i$ into information set $\{\eta_j^i\}$, such that the same number of immediate branches emanates from every node belonging to the same information set, and no node follows another node in the same information set.
\end{enumerate}
\end{definition}

Note that $f_s(b):= f(s,b) = (f(s,     b^{\tilde{1}}\color{black}), \cdots, f(s,     b^{\tilde{N}}\color{black}))$ is the vector version of the perturbation function $f$ in an MG-SPA. Since in an MG-SPA, $q^i(s,a,b) = r^i(s,a,b) + \sum_{s^\prime}p(s^\prime | s, a, b) v^i(s^\prime)$ for all $i = 1, \cdots, N$, $g^i_s(\tilde{s}, a) = q^i(s, a, f^{-1}_s(\tilde{s}))$ as well. We can also use $(q^1, \cdots, q^N, -q^1, \cdots, -q^N)$ to denote an extensive-form game based on $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$. Then we define the behavioral strategies for $P1$ and $P2$, respectively in the following definition.

\begin{definition}(Behavioral strategy)
Let $I^i$ denote the class of all information sets of $Pi$, with a typical element designed as $\eta^i$. Let $U^i_{\eta^i}$ denote the set of alternatives of $Pi$ at the nodes belonging to the information set $\eta^i$. Define $U^i = \cup U^i_{\eta^i}$ where the union is over $\eta^i \in I^i$. Let $Y_{\eta^1}$ denote the set of all probability distributions on $U^1_{\eta^1}$, where the latter is the set of all alternatives of $P1$ at the nodes belonging to the information set $\eta^1$. Analogously, let $Z_{\eta^2}$ denote the set of all probability distributions on $U^2_{\eta^2}$. Further define $Y = \cup_{I^1} Y_{\eta^1}, Z = \cup_{I^2} Z_{\eta^2}$. Then, a behavioral strategy ${\lambda}$ for $P1$ is a mapping from the class of all his information sets $I^1$ into $Y$, assigning one element in $Y$ for each set in $I^1$, such that ${\lambda}(\eta^1) \in Y_{\eta^1}$ for each $\eta^1 \in I^1$. A typical behavioral strategy ${\chi}$ for $P2$ is defined, analogously, as a restricted mapping from $I^2$ into $Z$. The set of all behavioral strategies for $Pi$ is called his behavioral strategy set, and it is denoted by ${\Gamma}^i$.
\end{definition}

The information available to the centralized agent ($P2$) at the time of his play is indicated on the tree diagram in Figure \ref{fig_efg_nature_apendix} by dotted lines enclosing an area (i.e. the information set) including the relevant nodes. This means the centralized agent is in a position to know exactly how the centralized nature player acts. In this case, a strategy for the centralized agent is a mapping from the collection of his information sets into the set of his actions.

And the behavioral strategy $\lambda$ for $P1$ is a mapping from his information sets and action space into a probability simplex, i.e. $\lambda (\tilde{s} | s)$ is the probability of choosing $\tilde{s}$ given $s$. Similarly, the behavioral strategy $\chi$ for $P2$ is $\chi(a | \tilde{s})$, i.e. the probability of choosing action $a$ when $\tilde{s}$ is given. Note that every behavioral strategy is a mixed strategy. We then give the definition of Nash equilibrium in behavioral strategies for an EFG.





\begin{definition}(Nash equilibrium in behavioral strategies)
\label{def_bs}
A pair of strategies $\{ \lambda_* \in \Gamma^1, \chi_* \in \Gamma^2 \}$ is said to constitute a Nash equilibrium in behavioral strategies if the following inequalities are satisfied that for all $i = 1, \cdots, N, \lambda \in \Gamma^1, \chi \in \Gamma^2$, $s \in S$:
\begin{align}
    J^i (\lambda^i, \lambda_*^{-i}, \chi^i_*, \chi_*^{-i}) \geq J^i (\lambda_*^i, \lambda_*^{-i}, \chi_*^i, \chi_*^{-i}) \geq J^i(\lambda^i_*, \lambda_*^{-i}, \chi^i, \chi_*^{-i})
\end{align}
where $J^i(\lambda, \chi)$ is the expected payoff i.e. $\mathbb{E}_{\lambda, \chi}[g_s^i]$ when $P1$ takes $\lambda$, $P2$ takes $\chi$, $\lambda(\tilde{s}|s) = \prod_{i=1}^N \lambda^i(\tilde{s}^i | s)$, $\chi(a|\tilde{s}) = \prod_{i=1}^N \chi^i(a^i|\tilde{s}^i)$.
\end{definition}
In the following parts as well as the main text, when we mention a Nash equilibrium for an EFG, it refers to a Nash equilibrium in behavioral strategies. How to solve an EFG is out of our scope since it has been investigated in much literature \citep{bacsar1998dynamic, schipper2017kuhn, slantchev2008game}. And policies $\lambda^i$ and $\chi^i$ can be attained through the marginal probabilities calculation with chain rules \citep{devore2012modern, dgh-props}. 

\subsubsection{Existence of NE for an EFG}
\label{appendix_efg_ne_exist}

In Lemma \ref{lemma_efg_ne_exist}, we give conditions (partial items of Assumption \ref{assumption}) under which an NE of the EFG based on $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$ exists.

\begin{lemma}
\label{lemma_efg_ne_exist}
Suppose $v^1 = \cdots = v^N$, and $S, A$ are finite. An NE $(\lambda_*, \chi_*)$ of the EFG based on $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$ exists.
\end{lemma}

\begin{proof}
Since $\tilde{S}$ is a subset of $S$, $\tilde{S}$ is finite when $S$ is finite. When $v^1 = \cdots = v^N$, and $\tilde{S}, A$ are finite, an EFG based on $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$ degenerates to a zero-sum two-person extensive-form game with finite strategies and perfect recall. Thus, an NE of this EFG exists \citep{bacsar1998dynamic, schipper2017kuhn, slantchev2008game}.
\end{proof}

The following Lemma \ref{lemma_efg_ne_re} provides insights into solving an MG-SPA by solving a constructed EFG.

\begin{lemma}
\label{lemma_efg_ne_re}
Suppose $f$ is a bijection when $s$ is fixed and an NE $(\lambda_*, \chi_*)$ exists for an EFG $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$. We define a joint policy $(\pi_*^v, \rho_*^v)$ as the joint policy implied from the NE $(\lambda_*, \chi_*)$ , where $\rho_*^v(b|s)=\lambda_*(\tilde{s}=f_s(b)|s), \pi_*^v(a|\tilde{s} = f_s(b))=\chi_*(a|\tilde{s})$. Then the joint policy  $(\pi_*^v, \rho_*^v)$ satisfies $L^i v^i(s) = r^i_{(\pi_*^v, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^v, \rho_*^v)}(s^\prime | s) v^i(s^\prime)$ for all $s \in S$.
\end{lemma}

\begin{proof}
The NE of the extensive-form game $(\lambda_*, \chi_*)$ implies that for all $i = 1, \cdots, N$, $s \in S, \lambda \in \Gamma^1, \chi \in \Gamma^2$, we have
\begin{align}
    J^i (\lambda, \chi_*) \geq J^i (\lambda_*, \chi_*) \geq J^i (\lambda_*, \chi), \nonumber
\end{align}
where $J^i(\lambda,\chi) = \mathbb{E}[r^i(s, a,  f^{-1}_s(\tilde{s})) + \sum_{s^\prime}p(s^\prime |s, a,  f^{-1}_s(\tilde{s}))v^i(s^\prime) | \tilde{s} \sim \lambda(\cdot|s), a \sim \chi(\cdot|\tilde{s})]$ according to Definition \ref{def_bs}. Let $b$ denote $f_s^{-1}(\tilde{s})$, because $f$ is a bijection when $s$ is fixed, $f_s(b) = (f_{s}(    b^{\tilde{1}}\color{black}), \cdots, f_{s}(    b^{\tilde{N}}\color{black}))$ is a bijection, and the inverse function $f_{s}^{-1}(\tilde{s}) = (f_{s}^{-1}(\tilde{s}^1), \cdots, f_{s}^{-1}(\tilde{s}^N))$ exists and is a bijection as well, then we have
\begin{align}
    J^i(\lambda_*,\chi_*)=&\mathbb{E}\left[r^i(s, a,  f^{-1}_s(\tilde{s})) + \sum_{s^\prime}p(s^\prime |s, a,  f^{-1}_s(\tilde{s}))v^i(s^\prime) | \tilde{s} \sim \lambda_*(\cdot|s), a \sim \chi_*(\cdot|\tilde{s}) \right]\nonumber \\
    =& \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \lambda_*(f_s(b)|s) , a \sim \chi_*(\cdot|f_s(b))\right]  \nonumber \\
    =& \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \rho_*^v(\cdot|s) , a \sim \pi_*^v(\cdot|\tilde{s})\right] \nonumber
\end{align}
Similarly, we have
\begin{align}
    J^i(\lambda_*,\chi) = \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \rho_*^v(\cdot|s) , a \sim \pi^v(\cdot|\tilde{s})\right], \nonumber \\
    J^i(\lambda,\chi_*) = \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \rho^v(\cdot|s) , a \sim \pi_*^v(\cdot|\tilde{s})\right], \nonumber
\end{align}
where $\pi^v, \rho^v$ are corresponding policies implied from behavioral strategies $\chi, \lambda$, respectively. Recall the definition of the minimax operator of $L^i v^i(s)$, we have, for all $s \in S$, 
$$L^i v^i(s) = r^i_{(\pi_*^v, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^v, \rho_*^v)}(s^\prime | s) v^i(s^\prime)$$
\end{proof}

Based on the proof, we also denote $(\pi^v_*, \rho^v_*)$ as an NE policy for the EFG $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$ for convenience, instead of calling it the joint policy derived from an NE for the EFG $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$.

We can get a corollary from Lemma \ref{lemma_efg_ne_re} that when optimal value functions of an MG-SPA exist, we are able to get a joint policy that satisfies the Bellman equations of the MG-SPA by computing an NE for a constructed EFG $(v_*^1, \cdots, v_*^N, -v_*^1, \cdots, -v_*^N)$. Later in Theorem \ref{new_theorem}, we show that a joint policy that satisfies the Bellman equations of an MG-SPA is in a robust equilibrium under Assumption \ref{assumption}. Therefore, Lemma \ref{lemma_efg_ne_re} provides insights into solving an MG-SPA by solving a corresponding EFG.

In the following proof, we aim to prove the existence of optimal value functions for an MG-SPA under Assumption \ref{assumption}.





\newpage
\subsection{Proof of two propositions}
\label{appendix_propositions}
\begin{proposition}
[Contraction mapping, same as proposition \ref{proposition_contraction} in the main text.]
\label{proof_proposition_contraction} 
Suppose $0 \leq \gamma < 1$ and Assumption \ref{assumption} hold. Then ${L}$ is a contraction mapping on $\mathbb{V}$.
\end{proposition}

\begin{proof}
Let $u$ and $v$ be in $\mathbb{V}$. {Given Assumption \ref{assumption}, these two EFGs $(u^1, \cdots, u^{N}, -u^1, \cdots, -u^{N})$, $(v^i, \cdots, v^{N}, -v^i, \cdots, -v^{N})$ both have at least one mixed Nash equilibrium according to Lemma \ref{lemma_efg_ne_exist}.} And let $(\pi_*^u, \rho_*^u)$ and $(\pi_*^v, \rho_*^v)$ be two Nash equilibriums for these two games, respectively. According to Lemma \ref{lemma_efg_ne_re}, we have the following equations hold for all $s \in S$,
\begin{align}
    L^i v^i(s) = r^i_{(\pi_*^v, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^v, \rho_*^v)}(s^\prime | s) v^i(s^\prime) \nonumber \\
    L^i u^i(s) = r^i_{(\pi_*^u, \rho_*^u)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^u, \rho_*^u)}(s^\prime | s) u^i(s^\prime) \nonumber
\end{align}

    


Then we have 
\begin{align}
    r^i_{(\pi_*^u, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^u, \rho_*^v)}(s^\prime | s) v^i(s^\prime) \leq L^i v^i(s) \leq r^i_{(\pi_*^v, \rho_*^u)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^v, \rho_*^u)}(s^\prime | s) v^i(s^\prime), \nonumber \\
    r^i_{(\pi_*^v, \rho_*^u)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^v, \rho_*^u)}(s^\prime | s) u^i(s^\prime) \leq L^i u^i(s) \leq r^i_{(\pi_*^u, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^u, \rho_*^v)}(s^\prime | s) u^i(s^\prime), \nonumber 
\end{align}
since $(\pi_*^u, \rho_*^v)$ and $(\pi_*^v, \rho_*^u)$ are derived from the Nash equilibrium of the EFG $(v^i, \cdots, v^{N}, -v^i, \cdots, -v^{N})$, and $(\pi_*^u, \rho_*^v)$ and $(\pi_*^v, \rho_*^u)$ are also derived from the Nash equilibrium of the EFG $(u^i, \cdots, u^{N}, -u^i, \cdots, -u^{N})$. We assume that $L^iv^i(s) \leq L^iu^i(s)$, then we have
\begin{align}
    0 &\leq L^i u^i(s) - L^i v^i(s) \nonumber \\
    &\leq \left[r^i_{(\pi_*^u, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^u, \rho_*^v)}(s^\prime | s) u^i(s^\prime)\right] - \left[ r^i_{(\pi_*^u, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^u, \rho_*^v)}(s^\prime | s) v^i(s^\prime) \right]\nonumber \\
    &\leq \gamma \sum_{s^\prime \in S}  p_{(\pi_*^u, \rho_*^v)}(s^\prime | s) ( u^i(s^\prime) - v^i(s^\prime)) \nonumber \\
    &\leq \gamma || v^i - u^i ||. \nonumber
\end{align}
\color{black}
Repeating this argument in the case that $L^iu^i(s) \leq L^iv^i(s)$ implies that 
$$||L^iv^i(s) - L^iu^i(s)|| \leq \gamma ||v^i - u^i||$$
for all $s \in S$, i.e. ${L^i}$ is a contraction mapping on ${V}$. Recall that $||v|| = \sup_j ||v^j||$, then we have
\begin{align}
    ||L v - L u|| = \sup_j ||L^j v^j - L^j u^j|| \leq \gamma \sup_j  ||v^j - u^j|| = \gamma ||v - u||. \nonumber
\end{align}
${L}$ is a contraction mapping on $\mathbb{V}$.

\end{proof}


\begin{proposition}[Complete Space, same as proposition \ref{proposition_complete} in the main text.]
\label{proof_proposition_complete}
$\mathbb{V}$ is a complete normed linear space.
\end{proposition}

\begin{proof}
Recall that $\mathbb{V}$ denote the set of bounded real-valued functions on $S \times \cdots \times S$, i.e. the cross product of $N$ state set with component-wise partial order and norm $||v|| := \sup_{s \in {S}}\sup_{j} |v^i(s)|$.  Since $\mathbb{V}$ is closed under addition and scalar multiplication and is endowed with a norm,  it is a normed linear space. Since every Cauchy sequence contains a limit point in $\mathbb{V}$, $\mathbb{V}$ is a complete space.
\end{proof}

\newpage
\subsection{Proof of Theorem \ref{new_theorem}}
\label{appendix_new_theorem}

In this section, our goal is to prove Theorem \ref{new_theorem}. We first prove (1) the optimal value function of an MG-SPA satisfies the Bellman equation by applying the Squeeze theorem [Theorem 3.3.6, \cite{sohrab2003basic}] in \ref{sec_(1)}. Then we prove that a unique solution of the Bellman equation exists using fixed-point theorem \citep{smart1980fixed} in \ref{proof_theorem_optimal_v_exist}. Thereby, the existence of the optimal value function gets proved. By introducing (3), we characterize the relationship between the optimal value function and a robust equilibrium. The proof of (3) can be found in \ref{proof_theorem_robust_eq_eq_optimal_v}. However, (3) does not imply the existence of an RE. To this end, in (4), we formally establish the existence of RE when the optimal value function exists. We formulate a $2N$-player Extensive-form game (EFG)~\citep{osborne1994course, von2007theory} based on the optimal value function such that its Nash equilibrium (NE) is equivalent to an RE of the MG-SPA. The details are in \ref{proof_theorem_conditions_ne_stochastic}.
\\
    
\begin{theorem}[Same as theorem \ref{new_theorem} in the main text]
\textbf{}

Suppose $0 \leq \gamma < 1$ and Assumption \ref{assumption} holds. 

(1) (Solution of Bellman equation)
A value function $v_* \in \mathbb{V}$ is an optimal value function if for all $i \in \mathcal{N}$, the point-wise value function $v^i_{*} \in V$ satisfies the corresponding Bellman Equation \eqref{def_bellman_optimality},
i.e. $v_* = Lv_*$.

(2) (Existence and uniqueness of optimal value function)
There exists a unique $v_* \in \mathbb{V}$ satisfying $Lv_* = v_*$, i.e. for all $i \in \mathcal{N}$, $L^i v^i_* = v^i_*$. 

(3) (Robust equilibrium and optimal value function)
A joint policy $d_* = (\pi_*, \rho_*)$, where $\pi_* = (\pi^1_*, \cdots, \pi^N_*)$ and $\rho_* = (\rho^{\tilde{1}}_*, \cdots, \rho^{\tilde{N}}_*)$, is a robust equilibrium if and only if $v^{d_*}$ is the optimal value function. 

(4) (Existence of robust equilibrium)
There exists a mixed RE for an MG-SPA.
\end{theorem}
\color{black}

\subsubsection{(1) Solution of Bellman equation}
\label{sec_(1)}
\begin{proof}

First, we prove that if there exists a $v^i \in V$ such that $v^i \geq L v^i$ then $v^i \geq v_*^i$. $v^i \geq L v^i$ implies
$v^i \geq \max\min [r^i + \gamma P v^i] = r_d^i + \gamma P_d v^i$, where $d = (\pi^{v,-i}_*, \pi^{v,i}_*, \rho^{v,-\tilde{i}}_*, \rho^{v,\tilde{i}}_*)$ is a Nash equilibrium for the EFG $v = (v^1, \cdots, v^{N}, -v^1, \cdots, -v^{N})$. {We omit the superscript $v$ for convenience when there is no confusion. }We choose a list of policy i.e. $z = (d_1, d_2, \cdots )$ where $d_j = (\pi^{-i}_*, \pi^i_j,     \rho_*^{-\tilde{i}}\color{black},     \rho^{\tilde{i}}\color{black}_*)$. Then we have
\begin{align}
    v^i \geq r_{d_1} + \gamma P_{d_1}v^i \geq r_{d_1}^i + \gamma P_{d_1} (r_{d_2}^i + \gamma P_{d_2}v^i)
    = r_{d_1}^i + \gamma P_{d_1}r_{d_2}^i + \gamma P_{d_1}P_{d_2} v^i 
    \nonumber
\end{align}
By induction, it follows that, for $n \geq 1$,
\begin{align}
    v^i &\geq r_{d_1}^i + \gamma P_{d_1}r_{d_2}^i + \cdots + \gamma^{n-1}P_{d_1}\cdots P_{d_{n-1}}r_{d_{n}}^i + \gamma^n P^n_z v^i \nonumber\\
    v^i - v_{z}^i &\geq \gamma^n P^n_z v^i - \sum_{t = n}^{\infty} \gamma^t P^t_z r_{d_{t+1}}^i
\end{align}
% We denote 
% \begin{align}
%     v^{z} = \sum_{t = 1}^{\infty}\gamma^{t-1}P^{t-1}_z r_{d_t} = r_{d_1} + \gamma P_{d_1}r_{d_2} + \cdots + \gamma^{n-1}P_{d_1}\cdots P_{d_{n-1}}r_{t_n} + \cdots
% \end{align}
% Thus,
% \begin{align}
%     v - v_{z} \geq \gamma^n P^n_z - \sum_{t = n}^{\infty} \gamma^t P^t_z r_{d_{t+1}}
% \end{align}

Since $||\gamma^n P^n_z v^i|| \leq \gamma^n ||v^i||$ and $\gamma \in [0, 1)$, for $\epsilon > 0$, we can find a sufficiently large $n$ such that
\begin{align}
    \epsilon e/2 \geq \gamma^n P^n_z v^i \geq - \epsilon e/2
\end{align}
where $e$ denotes a vector of 1's. And as a result of Assumption \ref{assumption}-(1), we have
\begin{align}
    - \sum_{t = n}^{\infty} \gamma^t P^t_z r_{d_{t+1}}^i \geq  -\frac{\gamma^n Me}{1-\gamma}
\end{align}

Then we have
\begin{align}
    v^i(s) - v_{z}^i(s) \geq - \epsilon
\end{align}
for all $s \in S$ and $\epsilon > 0$. Let all $d_j$ the same, since $\epsilon$ was arbitrary, we have
\begin{align}
    v^i(s) \geq \max_{\pi^i} \min_{    \rho^{\tilde{i}}\color{black}} v_{z}^i(s) = v_*^i(s)
    % v(s) \geq v_*(s)
\end{align}

Then we prove that if there exists a $v^i \in V$ such that $v^i \leq Lv^i$ then $v^i \leq v_*^i$. For arbitrary $\epsilon > 0$ there exists a joint policy $d' = (\pi^{-i}_*, \pi^i_*,     \rho_*^{-\tilde{i}}\color{black},     \rho^{\tilde{i}}\color{black})$ and a list of policy $z = (d', d', \cdots)$ such that 
\begin{align}
    v^i &\leq r_{d'}^i + \gamma P_{d'} v^i + \epsilon  \nonumber \\
    % v -  \gamma P_d v  &\leq r_d + \epsilon \nonumber \\
     (I - \gamma P_{d'}) v^i &\leq r_{d'}^i + \epsilon \nonumber \\
     &\leq (I - \gamma P_{d'})^{-1} r_{d'}^i + (1 - \gamma)^{-1} \epsilon e  = v_z^i + (1 - \gamma)^{-1} \epsilon e \nonumber \\
    %  &\leq \max_{\pi^i}\min_{    \rho^{\tilde{i}}\color{black}} v_{z}(s) + (1-\gamma)^{-1}\epsilon e \\
     &\leq v_*^i + (1-\gamma)^{-1}\epsilon e \nonumber
\end{align}
The equality holds because the Theorem 6.1.1 in \cite{puterman2014markov}. Since $\epsilon$ was arbitrary,  we have
\begin{align}
    v^i \leq v_*^i
\end{align}

So if there exists a $v^i \in V$ such that $v^i = L^iv^i$ i.e. $v^i \leq L^iv^i$ and $v^i \geq L^iv^i$, we have $v^i = v_*^i$, i.e. if $v^i$ satisfies the Bellman equation, $v^i$ is an optimal value function.

% According to the definition of $v^{*}$ in \eqref{def_optimal_v}, we have
%     \begin{align}
%         v^{*,i}(s) = v^{(\pi^{-i}_*,\pi_*^i, \rho^{-i}_*,     \rho^{\tilde{i}}\color{black}_*), i}(s) \nonumber\\
%         = \max_{\pi^i}\min_{    \rho^{\tilde{i}}\color{black}}v^{(\pi^{-i}_*,\pi^i, \rho^{-i}_*,     \rho^{\tilde{i}}\color{black}), i}(s) \nonumber\\
%         = \max_{\pi^i}\min_{    \rho^{\tilde{i}}\color{black}} \sum_{a \in A}\sum_{b \in B}\sum_{s^\prime \in S} p(s^\prime | s, a, b) \times \nonumber \\
%         \prod_{j \neq i} \pi_*^j(a^j | \tilde{s}^j) \pi^i(a^i|\tilde{s}^i) \prod_{j \neq i} \rho_*^j(a^j | {s}^j)     \rho^{\tilde{i}}\color{black}(b^i|{s}^i)\times \nonumber\\
%         [r^i(s, a, b) + \gamma v^{(\pi^{-i}_*,\pi^i, \rho^{-i}_*,     \rho^{\tilde{i}}\color{black}), i}(s^\prime)]
%     \end{align}
\end{proof}

\subsubsection{(2) Existence of optimal value function}
\label{proof_theorem_optimal_v_exist}

\begin{proof}
Proposition \ref{proposition_contraction} and \ref{proposition_complete} establish that $\mathbb{V}$ is a complete normed linear space and $L$ is a contraction mapping, so that the hypothesis of Banach Fixed-Point Theorem  are satisfied \citep{smart1980fixed}. Therefore there exists a unique solution $v_* \in \mathbb{V}$ to $L v = v$. From (1), we know if $v_*$  satisfies the Bellman equation, it is an optimal value function. Therefore, the existence of the optimal value function is proved.
\end{proof}



\subsubsection{(3) robust equilibrium and optimal value function}
\label{proof_theorem_robust_eq_eq_optimal_v}

\begin{proof}
(i) robust equilibrium $\rightarrow$ Optimal value function. 

Suppose $d^*$ is a robust equilibrium. Then $v^{d^*} = v^*$. From (2), it follows that $v^{d^*}$ satisfies $L v= v$. Thus $v^{d^*}$ is the optimal value function.

(ii) Optimal value function $\rightarrow$ robust equilibrium. 

Suppose $v^{d^*}$ is the optimal value function, i.e., $L v^{d^*} = v^{d^*}$. The proof of (1) implies that $v^{d^*} = v^*$, so $d^*$ is in robust equilibrium. 
\end{proof}

    
\subsubsection{(4) Existence of robust equilibrium}
\label{proof_theorem_conditions_ne_stochastic}

\begin{proof}
From (2), we know that there exists a solution $v_* \in \mathbb{V}$ to the Bellman equation $Lv = v$. Now, we consider an EFG based on $(v^1_*, \cdots, v^N_*, -v^1_*, \cdots, -v^N_*)$. Under Assumption \ref{assumption}, we can get an NE policy $(\pi_*^{v_*}, \rho_*^{v_*})$ by solving the EFG as a consequence of Lemma \ref{lemma_efg_ne_exist}. According to Lemma \ref{lemma_efg_ne_re}, $(\pi_*^{v_*}, \rho_*^{v_*})$ satisfies $$L^i v^i_*(s) = r^i_{(\pi_*^{v_*}, \rho_*^{v_*})}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^{v_*}, \rho_*^{v_*})}(s^\prime | s) v^i_*(s^\prime),$$ for all $s \in S$.
According to (3), $(\pi_*^{v_*}, \rho_*^{v_*})$ is a robust equilibrium.
\end{proof}

\newpage
\subsection{Proof of Corollary \ref{corollary_history}}
\label{appendix_history}

\begin{corollary}[Same as Corollary \ref{corollary_history}]
\textbf{}

Theorem \ref{new_theorem} still holds when all agents and adversaries in an MG-SPA use history-dependent policies with a finite time horizon.
\end{corollary}

\begin{proof}

From subsection \ref{sec_history}, we can find the main difference between history-dependent-policy-based RE and Markov-policy-based RE are the definitions and notations of policies and states. To prove Theorem \ref{new_theorem}, we construct an EFG based on the current state $s_t$. Similarly, to prove Corollary \ref{corollary_history}, we construct an EFG based on the concatenated states $s_{h,t}$ and $\tilde{s}_{h-1, t-1}$ that includes the current state $s_t$ and historical state information $s_{t-1}, \cdots, s_{t-h+1}, \tilde{s}_{t-1}, \cdots, \tilde{s}_{t-h+1}$. Notice that $h$ is a finite number. Hence, the concatenated state space is still finite. We now construct another extensive-form game in which a centralized nature player ($P1$) has $|\tilde{S}|$ alternatives (branches) to choose from, whereas a centralized agent ($P2$) has $|A|$ alternatives, and the order of play is that the centralized nature player acts before the centralized agent does. The set $A$ is the same as the agents' joint action set in an MG-SPA, set $\tilde{S}$ is a set of perturbed states constrained by a constrained parameter $\epsilon$. 

\textbf{}

\begin{definition}
\label{definition_efg_history}
An extensive-form game based on $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$ under concatenated states $s_{h,t} = (s_{t}, \cdots, s_{t-h+1}) \in S_h$ and $\tilde{s}_{h,t-1} = (\tilde{s}_{t-1}, \cdots, \tilde{s}_{t-h+1}) \in S_{h-1}$ is a finite tree structure with:
\begin{enumerate}
    \item A player $P1$ has a action set $\tilde{S} = \overbrace{\mathcal{B}(\epsilon, s) \times \cdots \times \mathcal{B}(\epsilon, s)}^{N}$, with a typical element designed as $\tilde{s}$. And $P1$ moves first.
    \item Another player $P2$ has an action set $A$, with a typical element designed as $a$. And $P2$ which moves after $P1$.
    \item A specific vertex indicating the starting point of the game.
    \item A payoff function $g_s(\tilde{s}, a) = (g^1_s(\tilde{s}, a), \cdots, g^N_s(\tilde{s}, a))$ where $s = s_t = s_{h,t}[1]$ is the first element of $s_{h,t}$, $\tilde{s} = \tilde{s}_t \in \tilde{S}$, $g^i_s(\tilde{s}, a) = r^i(s, a,  f^{-1}_s(\tilde{s})) + \sum_{s^\prime}p(s^\prime |a,  f^{-1}_s(\tilde{s}))v^i(s^\prime)$ assigns a real number to each terminal vector of the tree. Player $P1$ gets $-g_s(\tilde{s}, a)$ while player $P2$ gets $g_s(\tilde{s}, a)$.
    \item A partition of the nodes of the tree into two player sets (to be denoted by $\bar{N}^1$ and $\bar{N}^2$ for $P1$ and $P2$, respectively).
    \item A sub-partition of each player set $\bar{N}^i$ into information set $\{\eta_j^i\}$, such that the same number of immediate branches emanates from every node belonging to the same information set, and no node follows another node in the same information set.
\end{enumerate}
\end{definition}


The definitions of behavioral strategy and Nash equilibrium keep the same. Then the behavioral strategy $\lambda$ for $P1$ is a mapping from his information sets and action space into a probability simplex, i.e. $\lambda (\tilde{s} | s_{h,t} = (s_t, \cdots, s_{t-h+1}))$ is the probability of choosing $\tilde{s}$ given $s_{h,t}$. Similarly, the behavioral strategy $\chi$ for $P2$ is $\chi(a | \tilde{s}_{h,t} = (\tilde{s}, \tilde{s}_{t-1}, \cdots, \tilde{s}_{t-h+1}))$, i.e. the probability of choosing action $a$ when $\tilde{s}_{h,t}$ is given.

Now let us check the correctness of Lemma \ref{lemma_efg_ne_exist} when EFG is constructed following Definition \ref{definition_efg_history}. We can find that $S_h$ is a finite state space for any finite time horizon $h$ since $S_h$ is a product topology on finite spaces. Then Lemma \ref{lemma_efg_ne_exist} still holds because the EFG degenerates to a zero-sum two-person extensive-form game with finite strategies and perfect recall.

Then let us check Lemma \ref{lemma_efg_ne_re}. We re-write it in Lemma \ref{lemma_efg_ne_re_history} in which the behavioral strategy $\chi(a|\tilde{s}_{h,t})$ and $\lambda(\tilde{s}|s_{h,t})$ are used. The proof of Lemma \ref{lemma_efg_ne_re_history} is similar to that of Lemma \ref{lemma_efg_ne_re}. 

\textbf{}

\begin{lemma}
\label{lemma_efg_ne_re_history}
Suppose $f$ is a bijection when $s$ is fixed and an NE $(\lambda_*, \chi_*)$ exists for an EFG $(v^1, \cdots, v^N, -v^1, \cdots, -v^N)$. We define a joint policy $(\pi_*^v, \rho_*^v)$ as the joint policy implied from the NE $(\lambda_*, \chi_*)$ , where $\rho_*^v(b|s_h)=\lambda_*(\tilde{s}=f_s(b)|s_h), \pi_*^v(a|\tilde{s}_h = (f_s(b), \tilde{s}_{t-1}, \cdots, \tilde{s}_{t-h+1}))=\chi_*(a|\tilde{s}_h)$. Then the joint policy  $(\pi_*^v, \rho_*^v)$ satisfies $L^i v^i(s) = r^i_{(\pi_*^v, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^v, \rho_*^v)}(s^\prime | s) v^i(s^\prime)$ for all $s \in S$.
\end{lemma}

\begin{proof}
The NE of the extensive-form game $(\lambda_*, \chi_*)$ implies that for all $i = 1, \cdots, N$, $s \in S, \lambda \in \Gamma^1, \chi \in \Gamma^2$, we have
\begin{align}
    J^i (\lambda, \chi_*) \geq J^i (\lambda_*, \chi_*) \geq J^i (\lambda_*, \chi), \nonumber
\end{align}
where $J^i(\lambda,\chi) = \mathbb{E}[r^i(s, a,  f^{-1}_s(\tilde{s})) + \sum_{s^\prime}p(s^\prime |s, a,  f^{-1}_s(\tilde{s}))v^i(s^\prime) | \tilde{s} \sim \lambda(\cdot|s_h), a \sim \chi(\cdot|\tilde{s}_h)]$ according to Definition \ref{def_bs}. Let $b$ denote $f_s^{-1}(\tilde{s})$, because $f$ is a bijection when $s$ is fixed, $f_s(b) = (f_{s}(    b^{\tilde{1}}), \cdots, f_{s}(    b^{\tilde{N}}))$ is a bijection, and the inverse function $f_{s}^{-1}(\tilde{s}) = (f_{s}^{-1}(\tilde{s}^1), \cdots, f_{s}^{-1}(\tilde{s}^N))$ exists and is a bijection as well, then we have
\begin{align}
    J^i(\lambda_*,\chi_*)=&\mathbb{E}\left[r^i(s, a,  f^{-1}_s(\tilde{s})) + \sum_{s^\prime}p(s^\prime |s, a,  f^{-1}_s(\tilde{s}))v^i(s^\prime) | \tilde{s} \sim \lambda_*(\cdot|s_h), a \sim \chi_*(\cdot|\tilde{s}_h) \right]\nonumber \\
    =& \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \lambda_*(f_s(b)|s_h) , a \sim \chi_*(\cdot|(f_s(b), \tilde{s}_{t-1}, \cdots, \tilde{s}_{t-h+1}))\right]  \nonumber \\
    =& \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \rho_*^v(\cdot|s_h) , a \sim \pi_*^v(\cdot|\tilde{s}_h)\right] \nonumber
\end{align}
Similarly, we have
\begin{align}
    J^i(\lambda_*,\chi) = \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \rho_*^v(\cdot|s_h) , a \sim \pi^v(\cdot|\tilde{s}_h)\right], \nonumber \\
    J^i(\lambda,\chi_*) = \mathbb{E}\left[r^i(s,a,b) + \sum_{s^\prime}p(s^\prime|s,a,b)v^i(s^\prime) | b \sim \rho^v(\cdot|s_h) , a \sim \pi_*^v(\cdot|\tilde{s}_h)\right], \nonumber
\end{align}
where $\pi^v, \rho^v$ are corresponding policies implied from behavioral strategies $\chi, \lambda$, respectively. Recall the definition of the minimax operator of $L^i v^i(s)$, we have, for all $s \in S$, 
$$L^i v^i(s) = r^i_{(\pi_*^v, \rho_*^v)}(s) + \gamma \sum_{s^\prime \in S} p_{(\pi_*^v, \rho_*^v)}(s^\prime | s) v^i(s^\prime)$$
\end{proof}

Proposition \ref{proposition_complete} still holds when agents and adversaries adopt history-dependent policies since we do not require Markov policies in the proof. Propositions \ref{proposition_contraction} also holds which can be proved by utilizing the properties of NE for EFGs defined in Definition \ref{definition_efg_history}. Specifically, in the proof, we use EFGs defined in Definition \ref{definition_efg_history} instead of Definition \ref{definition_efg}. The subsequent proof of Propositions \ref{proposition_contraction} keeps the same.

Then in the proof of Theorem \ref{new_theorem}, we are able to continue to utilize Propositions \ref{proposition_complete} and \ref{proposition_contraction}. Similarly, the EFGs used in the proof are replaced by Definition \ref{definition_efg_history}. The definition of the minimax operator does not constrain the type of policies. The properties of the minimax operator can be continually used as well. The main body of proof keeps the same. Theorem \ref{new_theorem} still holds when agents and adversaries adopt history-dependent policies.



\end{proof}



