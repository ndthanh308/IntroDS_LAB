\label{appendix_alg}
\subsection{Robust multi-agent Q-learning (RMAQ)}
\label{appendix_rmaq}
In this section, we prove the convergence of RMAQ under certain conditions. First, let's recall the convergence theorem and certain assumptions. 

\begin{assumption}[Same as assumption \ref{assumption_convergence}]
\label{assumption_convergence_appendix}

\textbf{}

(1) State and action pairs have been visited infinitely often.
(2) The learning rate $\alpha_t$ satisfies the following conditions: $0 \leq \alpha_t < 1$, $\sum_{t \geq 0}\alpha_t^2 \leq \infty$; if $(s,a,b) \neq (s_t, a_t, b_t)$, $\alpha_t(s, a, b) = 0$.
(3) An NE of the EFG based on $(q_t^1, \cdots, q_t^N, -q_t^1, \cdots, -q_t^N)$ exists at each iteration $t$.
\end{assumption}

\begin{theorem}[Same as theorem \ref{theorem_q_convergence}]
\label{proof_theorem_q_convergence_appendix}

\textbf{}

Under Assumption \ref{assumption_convergence_appendix}, the sequence $\{ q_t \}$ obtained from \eqref{def_qlearning_appendix} converges to $\{ q_* \}$ with probability $1$, which are the optimal action-value functions that satisfy Bellman equations \eqref{def_bellman_q} for all $i = 1, \cdots, N$.
\begin{align}
\label{def_qlearning_appendix}
    &q_{t+1}^i(s_t, a_t, b_t) = (1-\alpha_t)  q_t^i(s_t, a_t, b_t) + \\
    &\alpha_t  \left[ r_t^i + \gamma\sum_{a_{t+1} \in A}\sum_{b_{t+1} \in B}  \pi_{*,t}^{q_t}(a_{t+1}|\tilde{s}_{t+1})\rho_{*,t}^{q_t}(b_{t+1}|s_{t+1})q^i_t(s_{t+1},a_{t+1},b_{t+1}) \right], \nonumber
\end{align}
\end{theorem}

\begin{proof}
Define the operator $T q_t = T (q_t^1, \cdots, q_t^N) = (T^1 q^1_t, \cdots, T^N q^N_t)$ where the operator $T^i$ is defined as below:
\begin{align}
    T^i {q}^i_t(s,a,b) = r^i_t + \gamma \sum_{a^\prime \in A}\sum_{b^\prime \in B} \pi_*^{q_t}(a^\prime|\tilde{s}^\prime)\rho_*^{q_t}(b^\prime|s^\prime)q^i_t(s^\prime,a^\prime,b^\prime)
\end{align}
for $i \in \mathcal{N}$, where $(\pi_*^{q_t},\rho_*^{q_t})$ is the tuple of Nash equilibrium policies for the EFG based on $(q^1_t, \cdots, q^N_t, -q^1_t, \cdots, -q^N_t)$ obtained from \eqref{def_qlearning_appendix}. Because of proposition~\ref{proposition_contraction_q} and proposition~\ref{proposition_q}
the Lemma 8 in \cite{hu2003nash} or Corollary 5 in \cite{szepesvari1999unified} tell that $q_{t+1} = (1-\alpha_t) q_t + \alpha_t T q_t$ converges to $q_*$ with probability 1.
\end{proof}

\begin{proposition}[Contraction mapping]
\label{proposition_contraction_q} 

\textbf{}

$T q_t = (T^1 q^1_t, \cdots, T^N q^N_t)$ is a contraction mapping.
\end{proposition}

\begin{proof}
{We omit the subscript $t$ when there is no confusion.} Assume $T^i p^i \geq T^i q^i$, we have
\begin{align}
    0 &\leq T^i p^i - T^i q^i \nonumber \\
    =& \gamma \left|\left|\sum_{a^\prime \in A}\sum_{b^\prime \in B} \pi^p_*(a^\prime|\tilde{s}^\prime)\rho^p_*(b^\prime|s^\prime) p^i(s^\prime,a^\prime,b^\prime) - \sum_{a^\prime \in A}\sum_{b^\prime \in B} \pi^q_*(a^\prime|\tilde{s}^\prime)\rho^q_*(b^\prime|s^\prime)q^i(s^\prime,a^\prime,b^\prime)  \right|\right| \nonumber \\
    \leq& \gamma \left|\left|\sum_{a^\prime \in A}\sum_{b^\prime \in B} \pi^q_*(a^\prime|\tilde{s}^\prime)\rho^q_*(b^\prime|s^\prime) p^i(s^\prime,a^\prime,b^\prime) - \sum_{a^\prime \in A}\sum_{b^\prime \in B} \pi^p_*(a^\prime|\tilde{s}^\prime)\rho^p_*(b^\prime|s^\prime)q^i(s^\prime,a^\prime,b^\prime)  \right|\right| \nonumber \\
    \leq& \gamma \left|\left| p^i - q^i  \right|\right|.
\end{align}
Repeating the case $T^i p^i \leq T^i q^i$ implies that $T^i$ is a contraction mapping such that $||T^i p^i - T^i q^i|| \leq \gamma ||p^i - q^i||$ for all $p^i, q^i \in Q$. Recall that $||p-q|| = \sup_j ||p^j - q^j||$
\begin{align}
    ||T p - T q|| = \sup_j ||T^j p^j - T^j q^j|| \leq \gamma \sup_j ||p^j - q^j|| = \gamma ||p-q|| \nonumber
\end{align}
$T$ is a contraction mapping such that $||T p - T q|| \leq \gamma ||p - q||$ for all $p, q \in \mathbb{Q}$.
\end{proof}


\begin{proposition}[A condition of Lemma 8 in \cite{hu2003nash} also Corollary 5 in \cite{szepesvari1999unified}]
\label{proposition_q} 
\begin{align}
    q_* = \mathbb{E} [T q_*]
\end{align}
\end{proposition}

\begin{proof}
\begin{align}
    \mathbb{E}\left[ T^i q^i_*(s,a,b) \right] &= \mathbb{E}\left[ r^i(s,a,b) + \gamma \sum_{a^\prime \in A}\sum_{b^\prime \in B} \pi_*(a^\prime|\tilde{s}^\prime)\rho_*(b^\prime|s^\prime)q^i_*(s^\prime,a^\prime,b^\prime)\right] \nonumber \\
    &= r^i(s,a,b) + \gamma\sum_{s^\prime \in S} p(s^\prime|s, a, b)\sum_{a^\prime \in A}\sum_{b^\prime \in B} \pi_*(a^\prime|\tilde{s}^\prime)\rho_*(b^\prime|s^\prime)q^i_*(s^\prime,a^\prime,b^\prime) \nonumber \\
    &= q_*^i(s,a,b)
\end{align}
Therefore $q_* = \mathbb{E} [T q_*]$.
\end{proof}

\newpage
\subsection{Robust multi-agent actor-critic (RMAAC)}
In this section, we first give the details of policy gradients proof in MG-SPA and then list the Pseudo code of RMAAC.

\subsubsection{Proof of policy gradients}
\label{proof_theorem_pg_rmaac}
Recall the policy gradient in RMAAC for MG-SPA in the following:
\begin{theorem}[Policy gradient in RMAAC for MG-SPA, same as the theorem \ref{theorem_pg_rmaac}]

For each agent $i \in \mathcal{N}$ and adversary   {$\tilde{i} \in \mathcal{M}$}, the policy gradients of the objective $J^i(\theta, \omega)$ with respect to the parameter $\theta, \omega$ are:
\begin{align}
    \nabla_{\theta^i} J^i(\theta, \omega) &= \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b)\gt \log \pi^{i}(a^i | \tilde{s}^i) \right]\\
    \nabla_{\omega^i} J^i(\theta, \omega) &= \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b) [\gw \log \rho^i(b^i | s) +  reg] \right]
\end{align}
where $reg = \nabla_{\tilde{s}^i} \log \pi^i(a^i|\tilde{s}^i) \nabla_{b^i}f(s,b^i) \gw \rho(b^i|s)$.
\end{theorem}
\begin{proof}
    We first start with the derivative of the state value function on $\theta^i$:
\begin{align}
    &\nabla_{\theta^i} v^{i,\pi,\rho}(s) \nonumber \\
    =& \gt \left[ \suma \sumb \jpi \jrho \qi(s,a,b) \right] \nonumber \\
    =& \suma \sumb\left[ \gt \jpi \jrho \qi(s,a,b) + \jpi \jrho \gt \qi(s,a,b)\right] \nonumber \\
    =& \suma \sumb\left[ \gt \jpi \jrho \qi(s,a,b) + \jpi \jrho \gt \sum_{s',r} p(s',r|s,a,b)(r^i + \vi(s'))\right] \nonumber \\
    =& \suma \sumb\left[  \gt \jpi \jrho \qi(s,a,b) +  \jpi \jrho \gt \sum_{s'} p(s'|s,a,b)\vi(s')\right]
\end{align}
We use $\fit(s)$ to denote $\suma \sumb[  \gt \jpi \jrho \qi(s,a,b)]$. We use $\pb(s \rightarrow x, k)$ to denote the probability of transition from state $s$ to state $x$ with agents' joint policy $\pi$ and adversaries' joint policy $\rho$ after $k$ steps. For example, $\pb(s \rightarrow s, k=0) = 1$ and $\pb(s \rightarrow s', k=1) = \suma \sumb \jpi \jrho p(s'|s,a,b)$. In the following proof, we sometimes use the superscript $i$ instead of $\tilde{i}$ to denote adversary $\tilde{i}$ when there is no confusion. Then we have:
\begin{align}
    &\nabla_{\theta^i} v^{i,\pi,\rho}(s) \nonumber \\
    =& \fit (s) + \suma \sumb \jpi \jrho \gt \sumsp p(s'|s,a,b)\vi(s') \nonumber \\
    =& \fit(s) + \suma \sumb \sumsp  \jpi \jrho \pr \gt \vi(s') \nonumber \\
    =& \fit(s) + \sumsp \pb(s \rightarrow s', 1) \gt \vi (s') \nonumber \\
    =& \fit(s) + \sumsp \pb(s \rightarrow s', 1)\left[\fit(s') + \sumspp \pb(s' \rightarrow s'', 1) \gt \vi(s'')\right] \nonumber \\
    =& \cdots \nonumber \\
    =& \sum_{x\in S} \sum_{k=0}^{\infty} \pb(s \rightarrow x, k) \fit(x)
\end{align}
By plugging in $\nabla_{\theta^i} v^{i,\pi,\rho}(s) = \sum_{x\in S} \sum_{k=0}^{\infty} \pb(s \rightarrow x, k) \fit(x)$ into the objective function $\ji$, we can get the following results:
\begin{align}
    &\gt \ji \nonumber = \gt \vi(s_1) \nonumber \\
    =& \sums \sumk \pb(s_1 \rightarrow s, k) \fit(s) \nonumber \\
    =& \sums \eta(s) \fit(s) \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \textit{\footnotesize \textcolor{blue}{;Let $\eta(s) = \sumk \pb(s_1 \rightarrow s, k) \fit(s)$}} \nonumber \\
    =& \left(\sums \eta(s)\right) \sums \frac{\eta(s)}{\sums \eta(s)}\fit(s) \nonumber \\
    \propto&  \sums \frac{\eta(s)}{\sums \eta(s)}\fit(s) \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad \quad \quad \quad  \textit{\footnotesize \textcolor{blue}{;$\left(\sums \eta(s)\right)$ is a constant}}\nonumber \\
    =& \sums \suma \sumb \pd(s) \gt \jpi \jrho \qi(s,a,b) \quad \quad \textit{\footnotesize \textcolor{blue}{;Let $\pd(s)=\frac{\eta(s)}{\sums \eta(s)}$}}\nonumber \\
    =&  \sums \suma \sumb \pd(s) \frac{\gt \jpi}{\jpi} \jrho \qi(s,a,b) \jpi \nonumber \\
    % =& \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b)\gt \log \pi(a| \tilde{s}) \right] \nonumber \\ 
    =& \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b)\gt \log \pi^{i}(a^i | \tilde{s}^i) \right]
\end{align}
Now we calculate the derivative of the state value function on $\omega^i$:
\begin{align}
    &\gw \vi(s) \nonumber \\
    =& \gw\left[\suma \sumb \jpi \jrho \qi(s,a,b) \right] \nonumber \\
    =& \suma \sumb\left[ \gw \jrho \jpi \qi(s,a,b) +  \gw \jpi \jrho \qi(s,a,b) +  \jrho \jpi \gw \qi(s,a,b) \right]
\end{align}
We let $\cio(s) = \suma \sumb\left[ \gw \jrho \jpi \qi(s,a,b) \right]$ and 

$\fio(s) = \suma \sumb\left[   \gw \jpi \jrho \qi(s,a,b) \right]$. Similar to $\gt \vi(s)$, we have:
\begin{align}
     &\gw \vi(s) \nonumber \\
    =& \cio(s) + \fio(s)  + \suma \sumb\left[\jpi \jrho \gt \sumsp \pr \vi(s') \right] \nonumber \\
    =& \cio(s) + \fio(s) + \suma \sumb \sumsp \jpi \jrho \pr \gw \vi(s') \nonumber \\
    =& \cio(s) + \fio(s) + \sumsp \pb(s \rightarrow s', 1) \gw \vi(s') \nonumber \\
    =& \cio(s) + \fio(s) + \sumsp \pb(s \rightarrow s', 1)\left[\cio(s') + \fio(s') + \sumsp \pb(s' \rightarrow s'', 1) \gw \vi(s'') \right] \nonumber \\
    =& \cdots \nonumber\\
    =& \sum_{x \in S} \sumk \pb (s \rightarrow x, k)[\cio(s) + \fio(s)]
\end{align}
By plugging in $\gw v^{i,\pi,\rho}(s) = \sum_{x \in S} \sumk \pb (s \rightarrow x, k)[\cio(s) + \fio(s)]$ into the objective function $\ji$, we can get the following results:
\begin{align}
    &\gw \ji = \gw \vi(s_1) \nonumber \\
    =& \sums \sumk \pb(s_1 \rightarrow s, k)\left[\cio(s) + \fio(s) \right] \nonumber \\
    \propto& \sums \suma \sumb \pd(s)\left[\gw \jpi \jrho \qi(s,a,b) + \gw\jrho \jpi \qi(s,a,b) \right] \nonumber \\ 
    =& \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b)\gw \log \rho(b | s) +  \qi(s,a,b)\gw \log \pi(a | \tilde{s}) \right] \nonumber \\
    =& \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b)\gw \log \rho^i(b^i | s) +  \qi(s,a,b)\gw \log \pi^i(a^i | \tilde{s}^i) \right] \nonumber \\
    =& \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b)\gw \log \rho^i(b^i | s) +  \qi(s,a,b)\frac{\nabla_{\tilde{s}^i} \pi^i(a^i|\tilde{s}^i) \nabla_{b^i}f(s,b^i) \gw \rho(b^i|s)}{\pi^i(a^i | \tilde{s}^i)} \right] \nonumber\\
    =& \E_{(s,a,b) \sim p(\pi, \rho)} \left\{\qi(s,a,b) [\gw \log \rho^i(b^i | s) +    \nabla_{\tilde{s}^i} \log \pi^i(a^i|\tilde{s}^i) \nabla_{b^i}f(s,b^i) \gw \rho(b^i|s)] \right\}
\end{align}

\end{proof}


\subsubsection{Policy gradients for deterministic polices}
\begin{theorem}[Policy gradients for deterministic polices in RMAAC for MG-SPA]

For each agent $i \in \mathcal{N}$ and adversary $\tilde{i} \in \mathcal{M}$ using deterministic policies, the policy gradients of the objective $J^i(\theta, \omega)$ with respect to the parameter $\theta, \omega$ are:
\begin{align}
    \nabla_{\theta^i} J^i(\theta, \omega) &= \frac{1}{T}\sum_{t = 1}^T \nabla_{a^i}q^i(s_t, a_t, b_t) \nabla_{\theta^i} \pi^i(\tilde{s}_t^i)|_{a^i_t = \pi^i(\tilde{s}^i_t), b^i_t = \rho^i(s_t)} \\
    \nabla_{\omega^i} J^i(\theta, \omega) &= \frac{1}{T}\sum_{t = 1}^T \left[ \nabla_{b^i}q^i(s_t, a_t, b_t) +  reg \right]\nabla_{\omega^i} \rho^i(s_t)|_{a^i_t = \pi^i(\tilde{s}^i_t), b^i_t = \rho^i(s_t)} 
\end{align}
where $reg = \nabla_{b^i_t}f(s_t, b_t^i) \nabla_{a^i}q^i(s_t, a_t, b_t) \nabla_{f}\pi^i(f)$.
\end{theorem}

\begin{proof}
Note that we here parameterize all policies $\pi^i, \rho^{\tilde{i}}$ as deterministic policies. Then we have:
\begin{align}
    \nabla_{\theta^i} J^i(\theta, \omega) &= \mathbb{E}_{s \sim p_{(\pi, \rho)}} \left[ \nabla_{\theta^i} q^i(s, a, b) \right] \nonumber \\
    \label{theta} &= \mathbb{E}_{s \sim p_{(\pi, \rho)}} \left[ \nabla_{a^i}q^i(s,a,b)\nabla_{\theta^i}\pi^i(\tilde{s}^i) \right], \\
    \nabla_{\omega^i} J^i(\theta, \omega) &= \mathbb{E}_{s \sim p_{(\pi, \rho)}} \left[ \nabla_{\omega^i} q^i(s, a, b) \right] \nonumber \\
    &= \mathbb{E}_{s \sim p_{(\pi, \rho)}} \left[ \nabla_{a^i}q^i(s,a,b)\nabla_{\tilde{s}^i}\pi^i(\tilde{s}^i)\nabla_{b^i}f(s^i, b^i)\nabla_{\omega^i}\rho^i(s) + \nabla_{b^i}q^i(s,a,b)\nabla_{\omega^i}\rho^i(s)  \right] \nonumber \\
    \label{omega} &= \mathbb{E}_{s \sim p_{(\pi, \rho)}} \left[ \nabla_{\omega^i}\rho^i(s) \left[ \nabla_{b^i}q^i(s,a,b) + reg \right] \right],
\end{align}
where $reg = \nabla_{a^i}q^i(s,a,b)\nabla_{\tilde{s}^i}\pi^i(\tilde{s}^i)\nabla_{b^i}f(s, b^i)$.  When the actors are updated in a mini-batch fashion \citep{mnih2015human, li2014efficient}, \eqref{mini_batch1} and \eqref{mini_batch2} approximate \eqref{theta} and \eqref{omega}, respectively.
\end{proof}

\begin{algorithm}
\SetAlgoLined
\caption{RMAAC with deterministic policies}
\label{alg_robust_actor_critic}
 Randomly initialize the critic network $q^i(s, a, b | \eta^i)$, the actor network $\pi^i(\cdot | \theta^i)$, and the adversary network $\rho^i(\cdot | \omega^i)$ for agent $i$. 
 Initialize target networks $q^{i\prime}, \pi^{i\prime}, \rho^{i\prime}$\;
 \For {each episode}
 {
     Initialize a random process $\mathcal{N}$ for action exploration\;
     Receive initial state ${s}$\;
    \For {each time step}
    {
        For each adversary $i$, select action $b^i = \rho^i(s) + \mathcal{N}$ w.r.t the current policy and exploration. Compute the perturbed state $\tilde{s}^i = f(s, b^i)$. Execute actions $a^i = \pi(\tilde{s}^i) + \mathcal{N}$ and observe the reward $r = (r^1,..., r^n)$ and the new state information $s^{\prime}$ and store$({s}, {a}, {b}, {\tilde{s}}, {r}, {s}^{\prime})$ in replay buffer $\mathcal{D}$. Set ${s}^\prime \rightarrow {s}$\;
        \For {agent i=1 to n}
        {
            Sample a random minibatch of $K$ samples $({s}_k, {a}_k, {b}_k, {r_k}, {s}_k^{\prime})$ from $\mathcal{D}$\;
            
            Set $y_k^i = r^i_k + \gamma q^{i\prime}( {s}_k^{\prime}, {a}_k^{\prime}, {b}_k^{\prime})|_{a^{i\prime}_k = \pi^{i\prime}(\tilde{s}_k^i), b^{i\prime}_k = \rho^{i\prime}(s_k)}$\;
            
            Update critic by minimizing the loss $\mathcal{L} = \frac{1}{K}\sum_k \left[ y_{k}^i - q^{i}({s}_k, {a}_k, b_k) \right]^2$\;
   
            
            \For {each iteration step}
            {
                Update actor $\pi^i( \cdot | \theta^i)$ and adversary $\rho^i (\cdot | \omega^i)$ using the following gradients
                
                $\theta^{i} \leftarrow \theta^i + \alpha_a \frac{1}{K}\sum_k \nabla_{\theta^i} \pi^i(\tilde{s}^i_k) \allowbreak \nabla_{a^i}q^i({s}_k, {a}_k, b_k)$ where $a^i_k = \pi^i(\tilde{s}^i_k)$, $b^i_k = \rho^i(s_k)$\;
                
                $\omega^{i} \leftarrow \omega^i - \alpha_b \frac{1}{K}\sum_k \nabla_{\omega^i}\rho^i(s_k) \left[ \nabla_{b^i}q^i(s_k, a_k, b_k) + reg \right]$ where $reg = \nabla_{a^i_k}q^i(s_k, a_k, b_k) \nabla_{\tilde{s}^i_k} \pi^i(\tilde{s}^i_k) $, $a^i_k = \pi^i(\tilde{s}^i_k)$, $b^i_k = \rho^i(s_k)$\;
            }
        }
        Update all target networks: $\theta^{i\prime} \leftarrow \tau\theta^i + (1-\tau)\theta^{i\prime}$, $\omega^{i\prime} \leftarrow \tau\omega^i + (1-\tau)\omega^{i\prime}$.
    }
}
\end{algorithm}

\subsubsection{Pseudo code of RMAAC}
\label{sec_alg_robust_actor_critic}

We provide the Pseudo code of RMAAC with deterministic policies in Algorithm \ref{alg_robust_actor_critic}. The stochastic policy version RMAAC is similar to Algorithm \ref{alg_robust_actor_critic} but uses different policy gradients.