Assumption~\ref{assumption_convergence}-(1) is a typical ergodicity assumption used in the convergence analysis of Q-learning \citep{littman1996generalized, hu2003nash, szepesvari1999unified, qu2020finite, sutton1998reinforcement}. And for Q-learning algorithm design papers that the exploration property is not the main focus,  this assumption is also a common assumption \citep{fujimoto2019off}. For exploration strategies in RL \citep{mcfarlane2018survey}, researchers use $\epsilon$-greedy exploration \citep{gomes2009dynamic}, UCB \citep{jin2018q,azar2017ucbvi}, Thompson sampling \citep{russo2018tutorial}, Boltzmann exploration \citep{cesa2017boltzmann}, etc. For assumption \ref{assumption_convergence}-(3) in multi-agent Q-learning, researchers have found that the convergence is not necessarily so sensitive to the existence of NE for the stage games during training \citep{hu2003nash, yang2018mean}.   {In particular, under Assumption \ref{assumption}, an NE of the 2$N$-player EFG exists, which has been proved in Lemma \ref{lemma_efg_ne_exist} in Appendix \ref{sec_coop_efg}.} We also provide an example in the experiment part (the two-player game) where assumptions are indeed satisfied, and our RMAQ algorithm successfully converges to an RE of the corresponding MG-SPA.
