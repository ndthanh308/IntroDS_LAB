\subsection{Robust multi-agent Q-learning (RMAQ)}
\label{sec_appendix_rmaq}
In this section, we first introduce the designed two-player game in that the reward function and transition probability function are formally defined. The MG-SPA based on the two-player game is also further explained. Then we show more experimental results about the proposed robust multi-agent Q-learning (RMAQ) algorithm, such as the training process of the RMAQ algorithm in terms of the total discounted rewards.

\subsubsection{Two-player game}

% Figure environment removed

Look at Figure~\ref{fig_game_apendix} (same as Figure \ref{fig_game} in the main text.), this is how we run the designed two-player game. The reward function $r$ and transition probability function $p$ are defined as follows.

These two players get the same rewards all the time, i.e. they share a reward function $r$.
\begin{equation}
r^i(s, a^1, a^2)=\left\{
\begin{aligned}
1 & , & a^1 = a^2, \text{and } s = s_0 \\
1 & , & a^1 \neq a^2, \text{and } s = s_1\\
0 & , & a^1 \neq a^2, \text{and } s = s_0 \\
0 & , & a^1 = a^2, \text{and } s = s_1
\end{aligned}
\right.
\end{equation}
The state does not change until these two players get a positive reward. So the transition probability function $p$ is 
\begin{equation}
p(s_1|s, a^1, a^2)=\left\{
\begin{aligned}
1 & , & a^1 = a^2, \text{and } s = s_0 \\
0 & , & a^1 \neq a^2, \text{and } s = s_0 \\
1 & , & a^1 = a^2, \text{and } s = s_1 \\
0 & , & a^1 \neq a^2, \text{and } s = s_1
\end{aligned}
\right. \quad
p(s_0|s, a^1, a^2)=\left\{
\begin{aligned}
0 & , & a^1 = a^2, \text{and } s = s_0 \\
1 & , & a^1 \neq a^2, \text{and } s = s_0\\
0 & , & a^1 = a^2, \text{and } s = s_1 \\
1 & , & a^1 \neq a^2, \text{and } s = s_1
\end{aligned}
\right.
\end{equation}

Possible Nash Equilibrium can be $\pi^*_1 = (\pi^1_1, \pi^2_1)$ or  $\pi^*_2=(\pi^1_2, \pi^2_2)$ where
\begin{equation}
\pi^1_1(a^1| s)=\left\{
\begin{aligned}
1 & , & a^1 = 1, \text{and } s = s_0 \\
0 & , & a^1 = 0, \text{and } s = s_0 \\
1 & , & a^1 = 1, \text{and } s = s_1 \\
0 & , & a^1 = 0, \text{and } s = s_1 \\
\end{aligned}
\right. \quad
\pi^2_1(a^2| s)=\left\{
\begin{aligned}
1 & , & a^2 = 1, \text{and } s = s_0 \\
0 & , & a^2 = 0, \text{and } s = s_0 \\
0 & , & a^2 = 1, \text{and } s = s_1 \\
1 & , & a^2 = 0, \text{and } s = s_1 \\
\end{aligned}
\right.
\end{equation}

\begin{equation}
\pi^1_2(a^1| s)=\left\{
\begin{aligned}
0 & , & a^1 = 1, \text{and } s = s_0 \\
1 & , & a^1 = 0, \text{and } s = s_0 \\
0 & , & a^1 = 1, \text{and } s = s_1 \\
1 & , & a^1 = 0, \text{and } s = s_1 \\
\end{aligned}
\right. \quad
\pi^2_2(a^2| s)=\left\{
\begin{aligned}
0 & , & a^2 = 0, \text{and } s = s_0 \\
1 & , & a^2 = 1, \text{and } s = s_0 \\
1 & , & a^2 = 0, \text{and } s = s_1 \\
0 & , & a^2 = 1, \text{and } s = s_1 \\
\end{aligned}
\right.
\end{equation}
NE $\pi^*_1$ means player $1$ always selects action $1$, player $2$ selects action $1$ under state $s_0$ and action $0$ under state $s_1$. NE $\pi^*_2$ means player $1$ always selects action $0$, player $2$ selects action $0$ under state $s_0$ and action $0$ under state $s_1$. 

According to the definition of MG-SPA, we add two adversaries for each player to perturb the player's observations. And adversaries get negative rewards of players. We let adversaries share a same action space $B^1 = B^2 = \{0, 1\}$, where $0$ means do not disturb, $1$ means change the observation to the opposite one. 
% Some times do not disturb would be a better choice for adversaries. For example, when the true state is $s_0$, players are using $\pi^*_1$, if adversary $1$ does not perturb player $1$'s observation, player $1$ will still select action $1$. While adversary $2$ changes player $2$'s observation to state $s_1$, player $2$ will choose action $0$ which is not same to player $1$'s action $1$. Thus, players always fail the game and get no rewards. 
Therefore, the perturbed function $f$ in this MG-SPA is defined as:
\begin{equation}
\left\{
\begin{aligned}
f(s_0, b = 0) = s_0 \\
f(s_1, b = 0) = s_1 \\
f(s_0, b = 1) = s_1 \\
f(s_1, b = 1) = s_0
\end{aligned}
\right. 
\end{equation}
Obviously, $f$ is a bijective function when $s$ is given. And the constraint parameter $\epsilon = ||S||$, where $||S|| := \max |s - s^\prime|_{\forall s, s^\prime \in S}$, i.e. no constraints for adversaries' power.

A Robust Equilibrium (RE) of this MG-SPA would be $\tilde{d}^* = (\tilde{\pi}^1_*, \tilde{\pi}^2_*, \tilde{\rho}^1_*, \tilde{\rho}^2_*)$, where
\begin{equation}
\left\{
\begin{aligned}
\tilde{\pi}^1_*(a^1| s) = 0.5, \quad \forall s \in S\\
\tilde{\pi}^2_*(a^2| s) = 0.5, \quad \forall s \in S \\
\tilde{\rho}^1_*(b^1|s) = 0.5, \quad \forall s \in S \\
\tilde{\rho}^2_*(b^2|s) = 0.5, \quad \forall s \in S
\end{aligned}
\right. 
\end{equation}

\subsubsection{Training procedure}
In Figure \ref{fig_q_training}, we show the total discounted rewards in the function of training episodes. We set the learning rate as $0.1$ and train our RMAQ algorithm for 400 episodes. And each episode contains 25 training steps. We can see the total discounted rewards converges to $50$, i.e. the optimal value in the MG-SPA, after about $280$ episodes or $7000$ steps.   

% Figure environment removed

\iffalse
\subsubsection{Testing Comparison}
\label{sec_testing_re}
We further test well-trained RE policy when 'strong' adversaries exists. 'Strong' adversary means its probability of modifying agents' observations is larger than the probability of no perturbations in state information. We make two agents play the game using 3 different policies for $1000$ steps under different adversaries. And the accumulated rewards, total discounted rewards are calculated. We use the Robust Equilibrium (of the MG-SPA), the Nash Equilibrium (of the original game) and a baseline policy and report the result in Figure~\ref{fig_two_player_rew}. The vertical axis is the accumulated/discounted reward, and the horizon axis is the probability that the adversary will attack/perturb the state. And we let these two adversaries share a same policy. We can see as the probability increase, the accumulated and discounted rewards of RE agents are stable but those rewards of NE agents and baseline agents are keep decreasing. This experiment is to validate the necessity of RE policy which is not only robust to the worst-case or adversarial state uncertainties, but also robust to some worse but note the worst cases.
% Figure environment removed
\fi


\newpage
\subsection{Robust multi-agent actor-critic (RMAAC)}
\label{sec_appendix_rmaac}
In this section, we first briefly introduce the multi-agent environments we use in our experiments. Then we provide more experimental results and explanations, such as the testing results under a cleaned environment (accurate state information can be attained) and a randomly perturbed environment (injecting standard Gaussian noise in agents' observations). In the last subsection, we list all hyper-parameters we used in the experiments, as well as the baseline source code.

\subsubsection{Multi-agent environments}
\label{sec_appendix_mpe}

% Figure environment removed


\textbf{Cooperative communication (CC):} This is a cooperative game. There are 2 agents and 3 landmarks of different colors. Each agent wants to get to their target landmark, which is known only by other agents. The reward is collective. So agents have to learn to communicate the goal of the other agent, and navigate to their landmark.

\textbf{Cooperative navigation (CN):} This is a cooperative game. There are 3 agents and 3 landmarks. Agents are rewarded based on how far any agent is from each landmark. Agents are penalized if they collide with other agents. So, agents have to learn to cover all the landmarks while avoiding collisions.


\textbf{Physical deception (PD): } This is a mixed cooperative and competitive task. There are 2 collaborative agents, 2 landmarks, and 1 adversary. Both the collaborative agents and the adversary want to reach the target, but only collaborative agents know the correct target. The collaborative agents should learn a policy to cover all landmarks so that the adversary does not know which one is the true target.

\textbf{Keep away (KA): } This is a competitive task. There is 1  agent, 1 adversary, and 1 landmark. The agent knows the position of the target landmark and wants to reach it. The adversary is rewarded if it is close to the landmark, and if the agent is far from the landmark. The adversary should learn to push the agent away from the landmark.


\textbf{Predator prey (PP):} This is a mixed game known as predator-prey. Prey agents (green) are faster and want to avoid being hit by adversaries (red). Predators are slower and want to hit good agents. Obstacles (large black circles) block the way.


\textbf{Navigate communication (NC): } This is a cooperative game that is similar to Cooperative communication. There are 2 agents and 3 landmarks of different colors. An agent is the ‘speaker’ that does not move but observes the goal of another agent. Another agent is the listener that cannot speak, but must navigate to the correct landmark.


\textbf{Predator prey+ (PP+):} This is an extension of the Predator prey environment by adding more agents. There are 2 preys, 6 adversaries, and 4 landmarks. Prey agents are faster and want to avoid being hit by adversaries. Predators are slower and want to hit good agents. Obstacles block the way.


\subsubsection{Experiments hyper-parameters}
\label{sec_appendix_para}

In Table \ref{tab_para}, we show all hyper-parameters we use to train our policies and baselines. We also provide our source code in the supplementary material. The source code of M3DDPG \citep{li2019robust} and MADDPG \citep{lowe2017multi} accept the MIT License which allows any person obtaining them to deal in the code without restriction, including without limitation the rights to use, copy, modify, etc. More information about this license refers to \url{https://github.com/openai/maddpg} and \url{https://github.com/dadadidodi/m3ddpg}.

\begin{table}[ht]
\centering
\caption{Hyper-parameters}
% \vspace{-10pt}
\begin{tabular}{l|c|c|c}
\toprule
\textbf{Parameter}                         & \textbf{RMAAC} & \textbf{M3DDPG}  & \textbf{MADDPG}\\  
\midrule
optimizer                         & Adam & Adam & Adam                  \\ 
learning rate                     & 0.01 & 0.01 & 0.01                  \\ 
adversarial learning rate                     & 0.005 & / & /                \\ 
discount factor                         & 0.95 & 0.95 & 0.95                  \\ 
replay buffer size                & $10^6$ & $10^6$ & $10^6$                  \\ 
number of hidden layers           & 2 & 2 & 2                     \\ 
activation function & Relu  & Relu & Relu  \\
number of hidden unites per layer & 64 & 64 & 64                    \\ 
number of samples per minibatch   & 1024 & 1024 & 1024                  \\ 
target network update coefficient $\tau$    & 0.01 & 0.01 & 0.01                  \\ 
iteration steps                    & 20 & 20 & 20                     \\
constraint parameter $\epsilon$ & 0.5 & / & /\\
episodes in training & 10k & 10k & 10k\\
time steps in one episode & 25 & 25 & 25 \\
\bottomrule
\end{tabular}
\label{tab_para}
% \vspace{-10pt}
\end{table}

\subsubsection{More testing results}
In this subsection, we provide more testing results under a cleaned environment (accurate state information can be attained) and a randomly disturbed environment (injecting standard Gaussian noise into agents' observations).

As we have reported the comparison of mean episode testing rewards under a cleaned environment by using 4 different methods in the main manuscript (Figures \ref{fig_test_mean_no} and \ref{fig_test_mean_random}), we further report the variance of testing results in the appendix. In Table \ref{tab_test_var_no} and \ref{tab_test_var_random}, we also report the variances of testing rewards in different scenarios under different environment settings. Our method has the lowest variance in three of the five scenarios. Notice that RM1 denotes our RMAAC policy trained with the linear noise format $f_1$, RM2 denotes our RMAAC policy trained with the Gaussian noise format $f_2$, MA denotes MADDPG (\url{https://github.com/openai/maddpg}), M3 denotes M3DDPG (\url{https://github.com/dadadidodi/m3ddpg}).

MAPPO is a multi-agent reinforcement learning algorithm that performs well in cooperative multi-agent settings \citep{yu2021surprising}. We use MP to denote MAPPO (\url{ https://github.com/marlbenchmark/on-policy.}). In Figure \ref{fig_test_mean_mappo}, we compare its performance with our RMAAC algorithm in two cooperative scenarios of MPE. The details of scenarios such as Cooperative navigation, Navigate communication can be found in the last section. We can see that under the optimally perturbed environment,  RMAAC outperforms MAPPO in all scenarios. Additionally, the reason we included MAPPO in the Appendix but not in the main text is that the current source code provider of MAPPO only provides instructions and codes for using it in cooperative environments. However, to validate our proposed method in different settings, we carefully selected experimental environments to include different game types: cooperative, competitive, and mixed. Due to the lack of MAPPO source codes/implementation in competitive and mixed environments, if we were to include the experimental results of MAPPO in the main text, it could disrupt the integrity and uniformity of the experiment section in the main text. Therefore, we included them in the appendix as supplementary content. 

In Figure \ref{fig_test_mean_tag_diff_env} and Table \ref{tab_test_var_tag}, we compare the mean episode testing rewards and variances under different environments in the complicated scenario with a large number of agents between different algorithms. We adopt the Gaussian noise format in training RMAAC polices. We can see our method has the lowest variance under two of three environments and has the highest rewards under all environments. 



% \useunder{\uline}{\ul}{}
\begin{table}[htb]
\centering
\caption{Variance of testing rewards under cleaned environment}
\label{tab_test_var_no}
\begin{tabular}{ccccccc}
\hline
% \multicolumn{1}{c|}{Perturbation function} & \multicolumn{3}{c|}{Simple noise  $f_1$}                                                       & \multicolumn{3}{c}{Gaussian noise  $f_2$ }                                                    \\ \hline
\multicolumn{1}{c|}{Algorithms}             & \multicolumn{1}{c}{RM  with $f_1$} & \multicolumn{1}{c}{RM with $f_2$} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{MA} \\ \hline
Cooperative communication (CC)                                                                 & 0.383      & {0.376}       & {\ul \textbf{0.295}}                      & 0.328\\
Cooperative navigation (CN)                         & 0.413     & {\ul \textbf{0.361}}       & 0.416                       & 0.376 \\
Physical deception (PD)        & 0.175 & 0.165 & {\ul \textbf{0.133}} & 0.143                       \\ 
Keep away (KA)        & 0.137& {\ul \textbf{0.134}}& 0.17& 0.145                       \\ 
Predator prey (PP)        & 5.139              & {\ul \textbf{1.450}}       & 4.681                       & 4.725                       \\     \hline
\end{tabular}
% \vspace{-10pt}
\end{table}

% \useunder{\uline}{\ul}{}
\begin{table}[htb]
\centering
\caption{Variance of testing rewards under randomly perturbed environment}
\label{tab_test_var_random}
\begin{tabular}{ccccccc}
\hline
% \multicolumn{1}{c|}{Perturbation function} & \multicolumn{3}{c|}{Simple noise  $f_1$}                                                       & \multicolumn{3}{c}{Gaussian noise  $f_2$ }                                                    \\ \hline
\multicolumn{1}{c|}{Algorithms}             & \multicolumn{1}{c}{RM with $f_1$} & \multicolumn{1}{c}{RM with $f_2$} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{MA} \\ \hline
Cooperative communication (CC)    & 0.592      & {\ul \textbf{0.547}}       & 1.187                       & 0.937                                              \\
Cooperative navigation (CN)      & 0.336                       & 0.33       & 0.328                       & {\ul \textbf{0.321}}                                         \\
Physical deception (PD) &0.222& 0.292& 0.209& {\ul \textbf{0.184}}\\
Keep away (KA) &{\ul \textbf{0.155}}& {\ul \textbf{0.155}}& 0.166& 0.161\\
Predator prey (PP)                & 4.629        & {\ul \textbf{2.752}}       & 3.644                       & 2.9 

\\ \hline                   
\end{tabular}
% \vspace{-10pt}
\end{table}

\begin{table}[htb]
\centering
\caption{Variance of testing rewards under different environments in Predator prey+.}
\label{tab_test_var_tag}
\begin{tabular}{cccc}
\hline
\multicolumn{1}{c}{Algorithm} & \multicolumn{1}{|c}{RM} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{MA} \\ \hline
Optimally Perturbed Env         & 4.199                   & 4.046                   & {\ul \textbf{3.924}}                  \\
Randomly Perturbed Env          & {\ul \textbf{4.664 }}                  & 5.774                   & 6.191                   \\
Cleaned Env                     & {\ul \textbf{3.928}}                   & 5.521                   & 6.006          \\\hline         
\end{tabular}
\end{table}

% Figure environment removed

% Figure environment removed


\newpage
\subsection{Ablation Study for RMAAC}
\label{appendix_ablation}

In  this subsection, we first investigate the effect of using different values of constraint parameters $\epsilon$ in the implementation of the RMAAC algorithm, then the effect of using different values of variance $\sigma$. Finally,  we study the performance of the RMAAC algorithm under other types of attacks. 

\subsubsection{Training Results Using Linear Noise with Different Constraint Parameters}

\textbf{Training Setup:} In this subsection, we train several RMAAC policies using linear noise format as the state perturbation function, i.e. $f_1(s, b^{\tilde{i}}) = s + b^{\tilde{i}}$. The constraint parameter $\epsilon$ is respectively set as $0.01, 0.05, 0.1, 0.5, 1$ and $2$, given other hyper-parameters unchanged. Other used hyper-parameters can be found in Table \ref{tab_para}.
 
 
\textbf{Training Results:} In Figure \ref{fig_diff_c1}, \ref{fig_diff_c3}, and \ref{fig_diff_c7}, we show the training process in three scenarios: Cooperative communication (CC), Cooperative navigation (CN), Predator Prey (PP), respectively. The y-axis denotes the mean episode reward of the agents and the x-axis denotes the training episodes. 

From these figures, we can see that, in general, the smaller the used variance, the higher the mean episode rewards RMAAC can achieve. However, RMAAC has different sensitivities to the value of variance in different scenarios. When we use $\epsilon = 2$, the RMAAC policies have the lowest mean episode rewards in all three scenarios. Nevertheless, when we use the smallest constraint parameter $\epsilon = 0.01$, the trained RMAAC policies do not achieve the highest mean episode rewards in all three scenarios. In these three scenarios, it is clear to see the performance of RMAAC using $\epsilon = 0.5$ is better than or similar to the performance of RMAAC using $\epsilon = 1$, and better than the performance of RMAAC using  $\epsilon = 2$, i.e. Performance($\epsilon=0.5$) $\geq$ Performance($\epsilon=1$) $>$ Performance($\epsilon=2$). The performance of the RMAAC policies is close when the constraint parameters are less or equal to than 0.1.  


% Figure environment removed


% \newpage
\subsubsection{Testing Results Using Linear Noise with Different Constraint Parameters}

In this subsection, we test well-trained RMAAC policies in perturbed environments where adversaries adopt linear noise format and different constraint parameters. 

\textbf{Testing Setup:} The tested policy $\pi_{test}$ is trained with the linear noise format $f_1 (s, b^{\tilde{i}}) = s + b^{\tilde{i}}$, constraint parameter is $0.5$, where $b^{\tilde{i}} = \rho^{\tilde{i}}_{test}(s | \epsilon = 0.5)$. The policy $\rho^{\tilde{i}}_{test}$ is adversary $\tilde{i}$'s policy which is trained with $\pi_{test}$ in RMAAC, for all $\tilde{i} = \tilde{1}, \cdots, \tilde{N}$. We use $\rho_{test}$ to denote the joint policy of adversaries which is used in the testing. In summary, we test agents' joint policy $\pi_{test}^{scenario}(\tilde{s})$ when adversaries adopt the joint policy $\rho_{test}^{scenario}(s | \epsilon)$, and  $\epsilon = 0.01, 0.05, 0.1, 0.5, 1, 2$, $scenario =$ Cooperative communication (CC), Cooperative navigation (CN), Predator Prey (PP), respectively. The testing is conducted over 400 episodes, and each episode has 25 time steps.

\textbf{Testing Results: } In Figures \ref{fig_test_c1}, \ref{fig_test_c3} and \ref{fig_test_c7}, we compare the performance of RMAAC, M3DDPG, and MADDPG in scenarios CC, CN, and PP with different values of constraint parameters. MADDPG is a MARL baseline algorithm. M3DDPG is a robust MARL baseline algorithm. The y-axis denotes the mean episode reward of the agents.

From these figures, we can see that in all three scenarios, our RMAAC policies outperform the baseline MARL and robust MARL policies in terms of mean episode testing rewards under the attacks of linear noise format with different constraint parameters $\epsilon$. Our proposed RMAAC algorithm is robust to the state information attacks of linear noise format with different constraint parameters.

% Figure environment removed



% \newpage
\subsubsection{Training Results Using Gaussian Noise with Different Variance }
\textbf{Training Setup:} In this subsection, we train several RMAAC policies using Gaussian noise format as the state perturbation function, i.e. $f_2(s, b^{\tilde{i}}) = s + \mathcal{N}(b^{\tilde{i}}, \sigma)$. The variance $\sigma$ is respectively set as $0.001, 0.05, 0.1, 0.5, 1, 2$ and $3$, given other hyper-parameters unchanged. Other used hyper-parameters can be found in Table \ref{tab_para}.

\textbf{Training Results: }In Figure \ref{fig_diff_v1}, \ref{fig_diff_v3} and \ref{fig_diff_v7}, we show the training process of RMAAC in three scenarios: Cooperative communication (CC), Cooperative navigation (CN), Predator Prey (PP). The y-axis denotes the mean episode rewards of the agents and the x-axis denotes the training episodes. 

From the figures, we can see that, in general, the smaller the value of variance used, the higher the mean episode rewards RMAAC can achieve. However, RMAAC has different sensitivities to the value of variance in different scenarios. When we use $\sigma = 3$, the RMAAC policies have the lowest mean episode rewards in all three scenarios. Nevertheless, when we use the smallest magnitude $0.001$, the trained RMAAC policies do not always achieve the highest mean episode rewards. In these three scenarios, it is clear to see the performance of RMAAC using $\sigma = 1$ is better than or close to that of using  $\sigma = 2$, and better than that of using  $\sigma = 3$, i.e. Performance($\sigma=1$) $\geq$ Performance($\sigma=2$) $>$ Performance($\sigma=3$). The performance of the RMAAC policies is close when the constraint parameters are less than or equal to 0.5.


% Figure environment removed




% \newpage
\subsubsection{Testing Results Using Gaussian Noise with Different Variance}

In this subsection, we test well-trained RMAAC policies in perturbed environments where adversaries adopt Gaussian noise format and different variances. 

\textbf{Testing Setup:} The tested policy $\pi_{test}$ is trained with Gaussian noise format $f_2 (s, b^{\tilde{i}}) = s + \mathcal{N}(b^{\tilde{i}},\sigma = 1)$, constraint parameter is $0.5$, where $b^{\tilde{i}} = \rho^{\tilde{i}}_{test}(s | \epsilon = 0.5)$. $\rho^{\tilde{i}}$ is adversary $i$'s policy which is trained with $\pi_{test}$ in RMAAC, for all $i = 1, \cdots, N$. We use $\rho_{test}$ to denote the joint policy of adversaries. In summary, we test agents' joint policy $\pi_{test}^{scenario}(\tilde{s})$ when adversaries adopt the joint policy $\rho_{test}^{scenario}(s | \epsilon = 0.5)$ and Gaussian noise format $f_2 (s, b^{\tilde{i}}) = s + \mathcal{N}(b^{\tilde{i}},\sigma)$, where $\sigma = 0.001, 0.05, 0.1, 0.5, 1, 2, 3$, $scenario = $ Cooperative communication (CC), Cooperative navigation (CN), Predator Prey (PP). The testing is conducted over 400 episodes, and each episode has 25 time steps.

\textbf{Testing Results: } In Figures \ref{fig_test_v1}, \ref{fig_test_v3} and \ref{fig_test_v7}, we respectively compare the performance of RMAAC, M3DDPG, and MADDPG in scenarios Cooperative communication, Cooperative navigation and Predator Prey with different values of constraint parameters. MADDPG is a MARL baseline algorithm. M3DDPG is a robust MARL baseline algorithm. The y-axis denotes the mean episode reward of the agents.

From these figures, we can see that in all three scenarios with all different values of constraint parameters, our RMAAC policies outperform the  MARL and robust MARL baseline policies in terms of mean episode rewards under the attacks of Gaussian noise format with different variance. Our proposed RMAAC algorithm is robust to the state information attacks of Gaussian noise format with different values of variance.

% Figure environment removed



% \newpage
\subsubsection{Testing Results under Different State Perturbation Functions}

In this subsection, we test the well-trained RMAAC policies in perturbed environments where adversaries adopt different noise formats and policies. 


\textbf{Testing Setup:} The tested agents' joint policy $\pi_{test}$ is trained with Gaussian noise format $f_2 (s, b^{\tilde{i}}) = s + Gaussian(b^{\tilde{i}}, \sigma = 1)$, constraint parameter is $0.5$, where $b^{\tilde{i}} = \rho^{\tilde{i}}_{test}(s | \epsilon = 0.5)$. $\rho^{\tilde{i}}_{test}$ is adversary $\tilde{i}$'s policy which is trained with $\pi_{test}$ in RMAAC, for all $\tilde{i} = \tilde{1}, \cdots, \tilde{N}$. We use $\rho_{test}$ to denote the joint policy of adversaries.  In a summary, we test agents' joint policy $\pi_{test}^{scenario}(\tilde{s})$ when adversaries adopt the joint policy $\rho_{test}^{scenario}(s | \epsilon=0.5)$, in three scenarios $scenario =$ Cooperative communication (CC), Cooperative navigation (CN), Predator Prey (PP), under non-optimal Gaussian format $f_3$, Uniform noise format $f_4$, fixed Gaussian noise format $f_5$ and Laplace noise format $f_6$, respectively. These noise formats are defined in the following:
\begin{align}
    f_3(s, b^{\tilde{i}}) &= s + Gaussian(b^{\tilde{i}}, 1) \quad \text{where} \quad b^{\tilde{i}} = \rho^{\tilde{i}}_{non-optimal}(s|\epsilon),\nonumber\\
    f_4(s, b^{\tilde{i}}) &= s + Uniform(-\epsilon, +\epsilon),\nonumber\\
    f_5(s, b^{\tilde{i}}) &= s + Gaussian(0,1),\nonumber\\
    f_6(s, b^{\tilde{i}}) &= s + Laplace(b^{\tilde{i}} ,1) \quad \text{where} \quad b^{\tilde{i}} = \rho^{\tilde{i}}_{test}(s|\epsilon),
\end{align}
where $\rho^{\tilde{i}}_{non-optimal}$ is a non-optimal policy of adversary $\tilde{i}$. $\rho^{\tilde{i}}_{non-optimal}$ is randomly chosen from the training process. As we can see that $f_3$ and $f_6$ are independent of the optimal joint policy of adversaries, but $f_4$ and $f_6$ are not. The testing is conducted over 400 episodes, and each episode has 25 time steps.

\textbf{Testing Results:} In Figures \ref{fig_test_d1}, \ref{fig_test_d3}, and \ref{fig_test_d7}, we compare the performance of RMAAC, M3DDPG and MADDPG in scenarios Cooperative communication, Cooperative navigation and Predator Prey under 4 different noise formats, respectively. MADDPG is a MARL baseline algorithm. M3DDPG is a robust MARL baseline algorithm. The y-axis denotes the mean episode reward of the agents.

As we can see from these figures, most of the time, our RMAAC policy outperforms the MARL (MADDPG) and robust MARL (M3DDPG) baseline policies. In Cooperative communication and Predator Prey, under all 4 different noise formats, RMAAC policies achieve the highest mean episode rewards. In Cooperative navigation, RMAAC policies have the highest mean episode rewards when the non-optimal Gaussian noise format and Laplace noise format are used. The only exception happens in Cooperative navigation when the Uniform noise format and fixed Gaussian noise format are used. However, we can find that the performance of RMAAC policies is close to that of the baseline policies in terms of mean episode testing rewards. In general, our RMAAC algorithm is robust to different types of state information attacks.

\iffalse
\subsubsection{Testing Results using History-dependent Policy}
\label{exp_history}

We conducted ablation study of RMAAC algorithm in which RMAAC algorithm is used to train history dependent policies. Other than the current information, We also use history information in the latest three time frames, i.e. $h=4$. In Table \ref{tab_hisotry}, we show the mean and variance of mean episode rewards in $10$ runs. We can see that in all five scenarios, history-dependent policies outperform Markov policies.

\begin{table}[]
\label{tab_hisotry}
\caption{Means and Variances of Mean Episode Rewards using Different Polices}
\centering
\begin{tabular}{lllll}
\hline
Scenarios & History-dependent policy            & Markov policy             &  &  \\ \hline
Cooperative communication (CC) & -52.83 $\pm$ 1.51  & -54.75 $\pm$ 3.03  &  &  \\
Cooperative navigation (CN)    & -208.19 $\pm$ 1.68 & -210.41 $\pm$ 1.13 &  &  \\
Physical deception (PD)        & 7.72 $\pm$ 0.33    & 5.71 $\pm$ 0.19    &  &  \\
Keep away (KA)                 & -20.69 $\pm$ 0.09  & -21.18 $\pm$ 0.14  &  &  \\
Predator prey (PP)             & 7.10 $\pm$ 0.17    & 6.116 $\pm$ 0.24   &  & \\ \hline
\end{tabular}
\end{table}
\fi


% Figure environment removed



\color{black}