\documentclass[10pt]{article} % For LaTeX2e
\usepackage[accepted]{tmlr}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022


% ready for submission
% \usepackage[preprint]{neurips_2022}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[linesnumbered,boxed,ruled,commentsnumbered]{algorithm2e}
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{tikz}  
\usepackage[normalem]{ulem}
\usetikzlibrary{arrows.meta}%画箭头用的包
\usepackage{wrapfig}
\usepackage{tabularx}
\usepackage{mdframed}
\usepackage{lipsum}
\usepackage{tcolorbox}
\usepackage{xcolor}

\newmdtheoremenv{theorem}{Theorem}[section]
\newmdtheoremenv{corollary}{\textbf{Corollary}}[theorem]
\newmdtheoremenv{lemma}[theorem]{\textbf{Lemma}}
\newmdtheoremenv{assumption}[theorem]{\textbf{Assumption}}
\newmdtheoremenv{definition}[theorem]{\textbf{Definition}}
\newmdtheoremenv{proposition}[theorem]{\textbf{Proposition}}
\newmdtheoremenv{remark}[theorem]{\textbf{Remark}}

\newcommand{\gt}{\nabla_{\theta^i} }
\newcommand{\gw}{\nabla_{\omega^i} }
\newcommand{\vi}{v^{i,\pi,\rho} }
\newcommand{\qi}{q^{i,\pi,\rho} }
\newcommand{\suma}{\sum_{a\in A} }
\newcommand{\sumb}{\sum_{b\in B} }
\newcommand{\sums}{\sum_{s\in S} }
\newcommand{\sumsp}{\sum_{s'\in S} }
\newcommand{\sumspp}{\sum_{s''\in S} }
\newcommand{\jpi}{\pi(a | \tilde{s}) }
\newcommand{\jrho}{\rho(b | s) }
\newcommand{\fit}{ \phi^{\theta^i} }
\newcommand{\fio}{ \phi^{\omega^i} }
\newcommand{\cio}{ \psi^{\omega^i} }
\newcommand{\pr}{p(s'|s,a,b)}
\newcommand{\pb}{p^{\pi,\rho}}
\newcommand{\ji}{J^i(\theta, \omega)}
\newcommand{\sumk}{\sum_{k=0}^{\infty}}
\newcommand{\pd}{d^{\pi,\rho}}
\newcommand{\E}{\mathbb{E}}


% \newtheorem{theorem}{\textbf{Theorem}}[section]
% \newtheorem{corollary}{\textbf{Corollary}}[theorem]
% \newtheorem{lemma}[theorem]{\textbf{Lemma}}
% \newtheorem{assumption}[theorem]{\textbf{Assumption}}
% \newtheorem{definition}[theorem]{\textbf{Definition}}
% \newtheorem{proposition}[theorem]{\textbf{Proposition}}
% \newtheorem{remark}[theorem]{\textbf{Remark}}

\newcommand{\defeq}{\mathrel{\mathop:}=}
\newcommand{\hsh}[1]{\textcolor{red}{[HSH: #1]}}

% Comments by Songyang Han
\newcommand{\songyang}[1]{\textcolor{blue}{[Songyang: #1]}}
% Comments by Shuo Han
\definecolor{darkgreen}{rgb}{0,0.5,0}
\newcommand{\sh}[1]{\textcolor{darkgreen}{[SH: #1]}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Robust Multi-Agent Reinforcement Learning with State Uncertainty}

\author{\name Sihong He \email sihong.he@uconn.edu \\
      \addr Department of Computer Science and Engineering\\
      University of Connecticut
      \AND
      \name Songyang Han \email songyang.han@uconn.edu \\
      \addr Department of Computer Science and Engineering\\
      University of Connecticut
      \AND
      \name Sanbao Su \email sanbao.su@uconn.edu \\
      \addr Department of Computer Science and Engineering\\
      University of Connecticut
      \AND
      \name Shuo Han \email  hanshuo@uic.edu\\
      \addr Department of Electrical and Computer Engineering\\
      University of Illinois, Chicago
      \AND
      \name Shaofeng Zou \email szou3@buffalo.edu \\
      \addr Department of Electrical Engineering\\ University at Buffalo, The State University of New York
      \AND
      \name Fei Miao \email fei.miao@uconn.edu \\
      \addr Department of Computer Science and Engineering\\
      University of Connecticut
      }

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\def\month{06}  % Insert correct month for camera-ready version
\def\year{2023} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=CqTkapZ6H9}} % Insert correct link to OpenReview for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
In real-world multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g., due to inaccurate measurement or malicious attacks), which challenges the robustness of agents' policies. Though robustness is getting important in MARL deployment, little prior work has studied state uncertainties in MARL, neither in problem formulation nor algorithm design. Motivated by this robustness issue and the lack of corresponding studies, we study the problem of MARL with state uncertainty in this work. We provide the first attempt to the theoretical and empirical analysis of this challenging problem. We first model the problem as a Markov Game with state perturbation adversaries (MG-SPA) by introducing a set of state perturbation adversaries into a Markov Game. We then introduce robust equilibrium (RE) as the solution concept of an MG-SPA. We conduct a fundamental analysis regarding MG-SPA such as giving conditions under which such a robust equilibrium exists. Then we propose a robust multi-agent Q-learning (RMAQ) algorithm to find such an equilibrium, with convergence guarantees. To handle high-dimensional state-action space, we design a robust multi-agent actor-critic (RMAAC) algorithm based on an analytical expression of the policy gradient derived in the paper. Our experiments show that the proposed RMAQ algorithm converges to the optimal value function; our RMAAC algorithm outperforms several MARL and robust MARL methods in multiple multi-agent environments when state uncertainty is present. The source code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.
\end{abstract}


\section{Introduction}
\input{sec_001_intro}


\section{Preliminary}
\input{sec_003_preliminary}


\section{Methodology}
\label{sec_method}
\input{sec_004_formulation}

\section{Algorithm}
\label{sec_algorithm}
\input{sec_005_alg}

\section{Experiment}
\label{sec_exp}
\input{sec_006_exp}

\section{Discussion}
\label{sec_future}
\input{sec_006_discussion}
\color{black}

\section{Conclusion}
\label{sec_conclusion}
\input{sec_007_conclusion}

\subsubsection*{Acknowledgments}
Sihong He, Songyang Han, Sanbao Su and Fei Miao are supported by the National Science Foundation under Grants CNS-1952096, CMMI-1932250, and CNS-2047354 grants. Shaofeng Zou is supported by the National Science Foundation under Grants CCF-2106560, and CCF-2007783. 

This material is based upon work supported under the AI Research Institutes program by National Science Foundation and the Institute of Education Sciences, U.S. Department of Education through Award \# 2229873 - National AI Institute for Exceptional Education. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, the Institute of Education Sciences, or the U.S. Department of Education.  

\bibliography{sec_ref}
\bibliographystyle{tmlr}



\newpage
\begin{center}  
    \begin{LARGE}  
        {\textbf{Appendix for “Robust Multi-Agent Reinforcement Learning with State Uncertainty”}}\\  
    \end{LARGE}  
\end{center}  

There are three sections in the appendix: section \ref{appendix_theory} for theoretical proof, section \ref{appendix_alg} for algorithms, and section \ref{appendix_exp} for experiments.

\appendix

\section{Theory}
\input{sec_008_appendix_theory}

\newpage
\section{Algorithm}
\input{sec_008_appendix_algorithm}

\newpage
\section{Experiments}
\label{appendix_exp}
\input{sec_008_appendix_experiments}


\end{document}