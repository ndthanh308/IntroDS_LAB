%\hsh{same as the abstract, not modified yet}
%In some multi-agent reinforcement learning (MARL) applications, agents may not have perfect state information (e.g. due to inaccurate measurement or communication hijacks), which challenges the robustness of agents' policies. Little prior work on MARL has accounted for such uncertainties, neither in problem formulation nor algorithm design. Motivated by this robustness issue, 
We study the problem of multi-agent reinforcement learning with state uncertainties in this work. We model the problem as a Markov game with state perturbation adversaries (MG-SPA), where each agent aims to find out a policy to maximize its own total discounted reward and each associated adversary aims to minimize that. This problem is challenging with little prior work on theoretical analysis or algorithm design. We provide the first attempt at theoretical analysis and algorithm design for MARL under worst-case state uncertainties. We first introduce robust equilibrium as the solution concept for MG-SPA, and prove conditions under which such an equilibrium exists. Then we propose a robust multi-agent Q-learning algorithm (RMAQ) to find such an equilibrium, with convergence guarantees under certain conditions. We also derive the policy gradients and design a robust multi-agent actor-critic (RMAAC) algorithm to handle the more general high-dimensional state-action space MARL problems. We also conduct experiments that validate our methods. 
% Our experiments demonstrate that the proposed RMAQ algorithm converges, and our RMAAC algorithm outperforms several baseline MARL methods, and both of them improve the robustness of the trained policies for the agents. 
% \textbf{Limitations:} It is also of future interest to investigate robustness of MARL to state transition uncertainty, and other types of adversary state perturbation objectives, and experiments on more multi-agent environments and systems.  \textbf{Negative societal impact:} To the best of the authors' knowledge, there is no potential negative societal impact in this work.

%\hsh{strong conditions: no good analysis yet}
%that do not consider the state uncertainties in several multi-agent environments
%\hsh{algorithm still can improve the robustness even the conditions are not held}