% In this section, we introduce preliminary concepts of Q-learning, Actor-Critic (AC), and Markov Game (MG).
\label{sec_preliminary}
\textbf{Q-learning}
is a model-free single-agent reinforcement learning algorithm~\citep{sutton1998introduction}. The core of this method is a Bellman equation that $q^*(s,a) =r(s,a) + \gamma \sum_{s^\prime}\max_{a^\prime \in A} q^*(s^{\prime}, a^{\prime})$. The Bellman equation encourages a simple value iteration update which uses the weighted average of old Q-value and the new one. Q-learning learns the optimal action-value function $q_*(s, a)$ by value iteration: $q_{new}(s,a) = (1-\alpha)q_{old}(s,a) + \alpha \left[ r(s,a) + \gamma \sum_{s^\prime} p(s'|s,a)\max_{a^\prime \in A} q_{old}(s^{\prime}, a^{\prime}) \right]$, and the optimal action $a_*(s) = \arg\max_{a \in A} q_*(s,a)$.
Deep Q-Networks (DQN) use a neural network with parameter $\theta$ to approximate Q-value~\citep{mnih2015human}. This allows the algorithm to handle larger state spaces. DQN minimizes the loss function defined in \eqref{q_learning}, where $q^\prime$ is a target network that copies the parameter $\theta$ occasionally to make training more stable. $\mathcal{D}$ is an experience replay buffer. DQN uses experience replay, which involves storing and randomly sampling previous experiences to train the neural network, to improve the stability and efficiency of the learning process. The target network helps to prevent the algorithm from oscillating or diverging during training.
\begin{align}
    \label{q_learning}
    \mathcal{L}(\theta) = \mathbb{E}_{\tau \sim \mathcal{D}} \left[ y - q(s,a|\theta) \right]^2, \quad y = r(s,a) + \gamma \max_{a^\prime \in A} q^\prime (s^\prime, a^\prime).
\end{align}

\textbf{Actor-Critic (AC)} is a single-agent reinforcement learning algorithm with two parts: an actor decides which action should be taken and a critic evaluates how well the actor performs~\citep{sutton1998introduction}. The actor is parameterized by $\pi_\theta (\cdot | s)$ and iteratively updates the parameter $\theta$ to maximize the objective function $J(\theta) = \mathbb{E}_{\tau \sim p, a \sim \pi_{\theta}}[\sum_{t=1}^{\infty}\gamma^{t-1}r_{t}(s_t, a_t)]$, where $\tau$ denotes a trajectory and $p$ is the state transition probability distribution. The critic is parameterized by $q_{\phi}(s, a)$ and evaluates actions chosen by the actor by computing the Q-value i.e. action-value function. The critic can update itself by using \eqref{q_learning}. The actor updates its parameter by using the gradient: $\nabla_\theta J(\theta) = \mathbb{E}_{s \sim p, a \sim \pi_\theta} \left[ q^{\pi}(s,a) \nabla_\theta \log \pi_\theta(a|s) \right]$.
% \begin{align}
%     \nabla_\theta J(\theta) = \mathbb{E}_{s \sim p, a \sim \pi_\theta} \left[ q^{\pi}(s,a) \nabla_\theta \log \pi_\theta(a|s) \right]
% \end{align}


\input{sup_def_001_markov_game}