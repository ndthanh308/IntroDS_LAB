\begin{wrapfigure}{r}{0.52\textwidth}
\centering
% Figure removed
\caption{Motivation of considering state uncertainty in single-agent reinforcement learning.}
\label{fig_intuition}
\end{wrapfigure}

Reinforcement Learning (RL) recently has achieved remarkable success in many decision-making problems, such as robotics, autonomous driving, traffic control, and game playing~\citep{ espeholt2018impala, silver2017mastering,mnih2015human,he2022robust}. However, in real-world applications, the agent may face \textit{state uncertainty} in which accurate information about the state is unavailable. This uncertainty may be caused by unavoidable sensor measurement errors, noise, missing information, communication issues, and/or malicious attacks. A policy not robust to state uncertainty can result in unsafe behaviors and even catastrophic outcomes. For instance, consider the path planning problem shown in Figure~\ref{fig_intuition}, where the agent (green ball) observes the position of an obstacle (red ball) through sensors and plans a safe (no collision) and shortest path to the goal (black cross). In Figure~\ref{fig_intuition}-(a), the agent can observe the true state $s$ (red ball) and choose an optimal and collision-free curve $a^*$ (in red) tangent to the obstacle. In comparison, when the agent can only observe the perturbed state $\tilde{s}$ (yellow ball) caused by inaccurate sensing or state perturbation adversaries (Figure~\ref{fig_intuition}-(b)), it will choose a straight line $\tilde{a}$ (in blue) as the shortest and collision-free path tangent to $\tilde{s}$. However, by following $\tilde{a}$, the agent actually crashes into the obstacle. To avoid collision in the worst case, one can construct a state uncertainty set that contains the true state based on the observed state. Then the robustly optimal path under state uncertainty becomes the yellow curve $\tilde{a}^*$ tangent to the uncertainty set, as shown in Figure~\ref{fig_intuition}-(c). 

In single-agent RL, imperfect information about the state has been studied in the literature of partially observable Markov decision process (POMDP)~\citep{pomdp98}. However, as pointed out in recent literature~\citep{huang2017adversarial, Kos2017attackP, yu2021robust, zhang2020robust}, the conditional observation probabilities in POMDP cannot capture the \textit{worst-case} (or adversarial) scenario, and the learned policy without considering state uncertainties may fail to achieve the agent's goal. Dealing with state uncertainty becomes even more challenging for Multi-Agent Reinforcement Learning (MARL), where each agent aims to maximize its own total return during the interaction with other agents and the environment \citep{yang2020overview}. Even if one agent receives misleading state information, its action affects both its own return and the other agents’ returns~\citep{zhang2020robust_nips} and may result in catastrophic failure. The existing literature of decentralized partially observable Markov decision process (Dec-POMDP)~\citep{dec-pomdp2016} does not provide theoretical analysis or algorithmic tools for MARL under worst-case state uncertainties either. 

To better illustrate the effect of state uncertainty in MARL, the path planning problem in Figure~\ref{fig_intuition} is modified such that two agents are trying to reach their individual goals without collision (a penalty or negative reward applied).
When the blue agent knows the true position $s^g_0$ (the subscript denotes time, which starts from $0$) of the green agent, it will get around the green agent to quickly reach its goal without collision. However, in Figure \ref{fig_intuition_marl}-(a), when the blue agent can only observe the perturbed position $\tilde{s}^g_0$ (yellow circle) of the green agent, it would choose a straight line that it thought safe (Figure~\ref{fig_intuition_marl}-(a1)), which eventually leads to a crash (Figure~\ref{fig_intuition_marl}-(a2)). In Figure~\ref{fig_intuition_marl}-(b), the blue agent adopts a robust trajectory by considering a state uncertainty set based on its observation. As shown in Figure~\ref{fig_intuition_marl}-(b1), there is no overlap between $(s^b_0, \tilde{s}^g_0)$ or $(s^b_T, \tilde{s}^g_T)$. Since the uncertainty sets centered at $\tilde{s}^g_0$ and $\tilde{s}^g_T$ (the dotted circles) include the true state of the green agent, this robust trajectory also ensures no collision between $(s^b_0, s^g_0)$ or $(s^b_T, s^g_T)$. The blue agent considers the interactions with the green agent to ensure no collisions at any time. Therefore, it is necessary to consider state uncertainty in a multi-agent setting where the dynamics of other agents should be considered. 

\begin{wrapfigure}{l}{0.62\textwidth}
\centering
% Figure removed
\caption{Motivation of considering state uncertainty in MARL.}
\label{fig_intuition_marl}
\end{wrapfigure}


In this work, we develop a robust MARL framework that accounts for state uncertainty. Specifically, we model the problem of MARL with state uncertainty as a Markov game with state perturbation adversaries (MG-SPA), in which each agent is associated with a state perturbation adversary. One state perturbation adversary always plays against its corresponding agent by preventing the agent from knowing the true state accurately. We analyze the MARL problem with adversarial or worst-case state perturbations. Compared to single-agent RL, MARL is more challenging due to the interactions among agents and the necessity of studying equilibrium policies~\citep{nash1951non,mckelvey1996computation,slantchev2008game,daskalakis2009complexity,etessami2010complexity}. The contributions of this work are summarized as follows.


\textbf{Contributions:} 
To the best of our knowledge, this work is the first attempt to systematically characterize state uncertainties in MARL and provide both theoretical and empirical analysis. First, we formulate the MARL problem with state uncertainty as a Markov game with state perturbation adversaries (MG-SPA). We define the solution concept of the game as a robust equilibrium (RE), where all players including the agents and the adversaries use policies from which no one has an incentive to deviate. In an MG-SPA, each agent not only aims to maximize its return when considering other agents’ actions but also needs to act against all state perturbation adversaries. Therefore, a robust equilibrium policy of one agent is robust to state uncertainties. Second, we study its fundamental properties and prove the existence of a robust equilibrium under certain conditions. We develop a robust multi-agent Q-learning (RMAQ) algorithm with a convergence guarantee and a robust multi-agent actor-critic (RMAAC) algorithm for handling high-dimensional state-action space. Finally, we conduct experiments in a two-player game to validate the convergence of the proposed Q-learning method RMAQ. We test our RMAAC algorithm in several benchmark multi-agent environments. We show that our RMAQ and RMAAC algorithms can learn robust policies that outperform baselines under state perturbations in multi-agent environments.

\textbf{Organization:}
The rest of the paper is organized as follows. The related work is presented in Section \ref{sec_rekated_work}. In Section \ref{sec_preliminary}, we introduce some preliminary concepts in RL and MARL. The proposed methodology and corresponding analysis are in Section \ref{sec_method}. The proposed algorithms are in Section \ref{sec_algorithm} and experiments results are in Section \ref{sec_exp}. We discuss some future work in Section \ref{sec_discus}. In Section \ref{sec_conclusion} we conclude.

\section{Related work}
\label{sec_rekated_work}
% \subsection{Robust Reinforcement Learning} 
\paragraph{Robust Reinforcement Learning:}
Recent robust reinforcement learning studied different types of uncertainties, such as action uncertainties \citep{tessler2019action} and transition kernel uncertainties~\citep{sinha2020formulazero,yu2021robust, hu2020robust_iclr_rej, wang2021transition_uncertainty_rl, lim2019kernel_uncertainty, nisioti2021robust,he2022robust}. Some recent attempts at adversarial state perturbations for single-agent validated the importance of considering state uncertainty and improving the robustness of the learned policy in Deep RL~\citep{huang2017adversarial,advDRL_ijcai17, zhang2020robust, zhang2021robust,everett2021certifiable}. The  works of \cite{zhang2020robust, zhang2021robust} formulate the state perturbation in single-agent RL as a modified Markov decision process, then study the robustness of single-agent RL policies. The works of \cite{huang2017adversarial} and \cite{advDRL_ijcai17} show that adversarial state perturbation undermines the performance of neural network policies in single-agent reinforcement learning and proposes different single-agent attack strategies. In this work, we consider the more challenging problem of adversarial state perturbation for MARL, when the environment of an individual agent is non-stationary with other agents' changing policies during the training process. 

% \subsection{Robust Multi-Agent Reinforcement Learning} 
\paragraph{Robust Multi-Agent Reinforcement Learning:}
There is very limited literature on the solution concept or theoretical analysis when considering adversarial state perturbations in MARL. Other types of uncertainties have been investigated  in the literature, such as uncertainties about training partner's type~\citep{shen2021robust}, the other agents' policies~\citep{li2019robust,sun2021romax, van2020robust}, and reward uncertainties~\citep{zhang2020robust_nips}. However, the policy considered in these papers relies on the true state information. Hence, the robust MARL considered in this work is fundamentally different since the agents do not know the true state information. Dec-POMDP enables a team of agents to optimize policies with the partial observable states~\citep{dec-pomdp2016, chen2022robust}. The work of \cite{lin2020robustness} studies state perturbation in identical-interest MARL, and proposes an attack method to attack the state of one single agent in order to decrease the team reward. In contrast, we consider the worst-case scenario that the state of every agent can be perturbed by an adversary and focus on the theoretical analysis of robust MARL including the existence of optimal value function and robust equilibrium (RE). Our work provides formal definitions of the state uncertainty challenge in MARL, and derives both theoretical analysis and practical algorithms.




\paragraph{Game Theory and MARL:}
MARL shares theoretical foundations with the game theory research field and a literature review has been provided to understand MARL from a game theoretical perspective~\citep{Yang2020gameMARL}. A Markov game, sometimes called a stochastic game models the interaction between multiple agents~\citep{owen2013game, littman1994markov}. Algorithms to compute the Nash equilibrium (NE) in Dec-POMDP~\citep{dec-pomdp2016}, POSG (partially observable stochastic game) and analysis assuming that NE exists~\citep{chades2002heuristic, hansen2004dynamic, nair2002towards} have been developed in the literature without proving the conditions for the existence of NE. The main theoretical contributions of this work include proving conditions under which the proposed MG-SPA has robust equilibrium solutions, and convergence analysis of our proposed robust multi-agent Q-learning algorithm. This is the first attempt to analyze the fundamental properties of MARL under adversarial state uncertainties.








