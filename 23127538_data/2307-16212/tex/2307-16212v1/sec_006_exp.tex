We aim to answer the following questions through experiments: (1) Can RMAQ algorithm find an RE? (2) Are RE policies robust to state uncertainties? (3) Does RMAAC algorithm outperform other MARL and robust MARL algorithms in terms of robustness? The host machine used in our experiments is a server configured with AMD Ryzen Threadripper 2990WX 32-core processors and four Quadro RTX 6000 GPUs. All experiments are performed on Python 3.5.4, Gym 0.10.5, Numpy 1.14.5, Tensorflow 1.8.0, and CUDA 9.0. Our code is public on \url{https://github.com/sihongho/robust_marl_with_state_uncertainty}.

\subsection{Robust Multi-Agent Q-learning (RMAQ)}
We show the performance of the proposed RMAQ algorithm by applying it to a two-player game. We first introduce the designed two-player game. Then, to answer the first and second questions, we investigate the convergence of this algorithm and compare the performance of robust equilibrium policies with other agents' policies under different adversaries' policies.

\begin{wrapfigure}{l}{0.5\textwidth}
\vspace{-10pt}
% Figure removed
\vspace{-10pt}
\caption{Two-player game: each player has two states and the same action set with size $2$. Under state $s_0$, two players get the same reward $1$ when they choose the same action. At state $s_1$, two players get the same reward $1$ when they choose different actions. One state switches to another state only when two players get a reward.}
\label{fig_game}
% \vspace{-10pt}
\end{wrapfigure}

\textbf{Two-player game: }For the game in Figure \ref{fig_game}, two players have the same action space $A= \{0,1\}$ and state space $S = \{s_0,s_1\}$. The two players get the same positive rewards when they choose the same action under state $s_0$ or choose different actions under state $s_1$. The state does not change until these two players get a positive reward. Possible Nash equilibrium (NE) in this game can be $\pi^*_1 = (\pi^1_1, \pi^2_1)$ that player $1$ always chooses action $1$, player $2$ chooses action $1$ under state $s_0$ and action $0$ under state $s_1$; or  $\pi^*_2=(\pi^1_2, \pi^2_2)$ that player $1$ always chooses action $0$, player $2$ chooses action $0$ under state $s_0$ and action $1$ under state $s_1$. When using the NE policy, these two players always get the same positive rewards. The optimal discounted state value of this game is $v^i_*(s) = 1/(1-\gamma)$ for all $s \in S, i \in \{1,2\}$, $\gamma$ is the reward discounted rate. We set $\gamma = 0.99$, then  $v^i_*(s) = 100$.

\textbf{MG-SPA formulation for the two-player game: }
According to the definition of MG-SPA, we add two adversaries, one for each player to perturb the state and get a negative reward of the player. They have the same action space $B = \{0, 1\}$, where $0$ means do not disturb, $1$ means perturb the observed state to another one. Sometimes no perturbation would be a good choice for adversaries. For example, when the true state is $s_0$, players are using $\pi^*_1$, if adversary $1$ does not perturb player $1$'s observation, player $1$ will still select action $1$. While adversary $2$ changes player $2$'s observation to state $s_1$, player $2$ will choose action $0$ which is not the same as player $1$'s action $1$. Thus, players always fail the game and get no rewards. A robust equilibrium for MG-SPA would be $\tilde{d}_* = (\tilde{\pi}^1_*, \tilde{\pi}^2_*, \tilde{\rho}^{\tilde{1}}_*, \tilde{\rho}^{\tilde{2}}_*)$ that each player chooses actions with equal probability and so do adversaries. The optimal discounted state value of corresponding MG-SPA is $\tilde{v}^i_*(s) = 1/2(1-\gamma)$ for all $s \in S, i \in \{1,2\}$ when players use robust equilibrium (RE) policies. We use $\gamma = 0.99$, then $\tilde{v}^i_*(s) = 50$. For more explanations of this two-player game and corresponding MG-SPA formulation, please see Appendix \ref{sec_appendix_rmaq}.

\textbf{Implementing RMAQ on the two-player game: }
We initialize $q^1(s, a, b) = q^2(s, a, b) = 0$ for all $s, a, b$. After observing the current state, adversaries choose their actions to perturb the agents' state. Then players execute their actions based on the perturbed state information. They then observe the next state and rewards. Then every agent updates its $q$ according to \eqref{def_qlearning}. In the next state, all agents repeat the process above. The training stops after $7500$ steps. When updating the Q-values, the agent applies a NE policy from the Extensive-form game based on $(q^1, q^2, -q^1, -q^2)$.

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-15pt}
    % Figure removed
    % Figure removed
    \vspace{-10pt}
    \caption{RE policy outperforms other policies in terms of total discounted rewards and total accumulated rewards when strong state uncertainties exist.}
    \vspace{-15pt}
    \label{fig_two_player_rew}
\end{wrapfigure}


\textbf{Training results: }
After $7000$ steps of training, we find that agents' Q-values stabilize at certain values. Since the dimension of $q$ is a bit high as $q \in \mathbb{R}^{32}$, we compare the optimal state value $\tilde{v}_*$ and the total discounted rewards in Table~\ref{tab_value}. The value of the total discounted reward converges to the optimal state value of the corresponding MG-SPA. This two-player game experiment result validates the convergence of our RMAQ method and the answer to the first question is 'Yes'. 

\textbf{Testing results: }
We further test well-trained RE policy when 'strong' adversaries exist. 'Strong' adversary means its probability of modifying players' observations is larger than the probability of no perturbations in the state information. We make two players play the game using 3 different policies for $1000$ steps under different adversaries. The accumulated rewards and total discounted rewards are calculated. We use the robust equilibrium (of the MG-SPA), the Nash equilibrium (of the original game), and a baseline policy (two players use deterministic policies) and report the result in Figure~\ref{fig_two_player_rew}. The vertical axis is the accumulated/discounted reward, and the horizon axis is the probability that the adversary will attack/perturb the state. And we let these two adversaries share the same policy. We can see as the probability increase, the accumulated and discounted rewards of RE players are stable but those rewards of NE players and baseline players keep decreasing. These experimental results show that the RE policy is robust to state uncertainties. It turns out the answer to the second question is 'Yes' as well. 

\textbf{Discussion: }Even for general-sum normal-form games, computing an NE is known to be PPAD-complete, which is still considered difficult in game theory literature~\citep{conitzer2002complexity, etessami2010complexity}. Therefore, we do not anticipate that the RMAQ algorithm can scale to very large MARL problems. In the next subsection, we show RMAAC with function approximation can handle large-scale MARL problems.

\subsection{Robust Multi-Agent Actor-Critic (RMAAC)}
To answer the third question, we compare our RMAAC algorithm with two benchmark MARL algorithms: MADDPG (\url{https://github.com/openai/maddpg}) \citep{lowe2017multi} which does not consider robustness, and M3DDPG (\url{https://github.com/dadadidodi/m3ddpg}) \citep{li2019robust}, a robust MARL algorithm which considers uncertainties from opponentsâ€™ policies altering. M3DDPG utilizes adversarial learning to train robust policies. We run experiments in several benchmark multi-agent scenarios, based on the multi-agent particle environments (MPE) \citep{lowe2017multi}. The hyper-parameters used to train RMAAC and the baseline algorithms are summarized in Appendix \ref{sec_appendix_para},  Table \ref{tab_para}.
\begin{table}[]
\vspace{-10pt}
\caption{Convergence Values of Total Discounted Rewards when Training Ends}
\centering
\label{tab_value}
\begin{tabular}{c|cc|cc|cc|cc}\hline
      & $v^1(s_0)$ & $v^2(s_0)$ & $v^1(s_1)$ & $v^2(s_1)$ & $\tilde{v}^1_*(s_0)$ & $\tilde{v}^2_*(s_0)$ & $\tilde{v}^1_*(s_1)$ & $\tilde{v}^2_*(s_1)$ \\\hline
value &  49.99    &   49.99  &  49.99    &   49.99 &  50.00    &   50.00  &  50.00    &   50.00 \\\hline
\end{tabular}
\vspace{-10pt}
\end{table}

\textbf{Experiment procedure: } We first train agents' policies using RMAAC, MADDPG and M3DDPG, respectively. For our RMAAC algorithm, we set the constraint parameter $\epsilon = 0.5$. And we choose two types of perturbation functions to validate the robustness of trained policies under different MG-SPA models. The first one is the linear noise format that $f_1 (s, b^{\tilde{i}}) := s + b^{\tilde{i}}$, i.e. the perturbed state $\tilde{s}^i$ is calculated by adding a random noise $b^{\tilde{i}}$ generated by adversary $\tilde{i}$ to the true state $s$. And $f_2 (s, b^{\tilde{i}}) := s + Gaussian(b^{\tilde{i}}, \Sigma)$, where the adversary $\tilde{i}$'s action $b^{\tilde{i}}$ is the mean of the Gaussian distribution. And $\Sigma$ is the covariance, we set it as $I$, i.e. an identity matrix. We call it Gaussian noise format. These two formats $f_1, f_2$ are commonly used in adversarial training~\citep{creswell2018generative, zhang2020robust,zhang2021robust}. Then we test the well-trained policies in the optimally disturbed environment (injected noise is produced by those adversaries trained with RMAAC algorithm). The testing step is chosen as $10000$ and each episode contains $25$ steps. All hyperparameters used in experiments for RMAAC, MADDPG and M3DDPG are attached in Appendix \ref{sec_appendix_para}. Note that since the rewards are defined as negative values in the used multi-agent environments, we add the same baseline ($100$) to rewards for making them positive. Then it's easier to observe the testing results and make comparisons. Those used MPE scenarios are Cooperative communication (CC), Cooperative navigation (CN), Physical deception (PD), Predator prey (PP) and Keep away (KA). The first two scenarios are cooperative games, the others are mixed games. To investigate the algorithm performance in more complicated situations, we also run experiments in a scenario with more agents, which is called Predator prey+ (PP+). More details of these games are in Appendix \ref{sec_appendix_mpe}. 

% Figure environment removed


\textbf{Experiment results: } In Figure~\ref{fig_test_mean} and Table~\ref{tab_test_var}, we report the mean episode testing rewards and variance of $10000$ steps testing rewards, respectively. We will use mean rewards and variance for short in the following experimental report and explanations. In the table and figure, we use RM, M3, MA for abbreviations of RMAAC, M3DDPG and MADDPG, respectively. In Figure~\ref{fig_test_mean}, the left five figures are mean rewards under the linear noise format $f_1$, the right ones are under the Gaussian noise format $f_2$. Under the optimally disturbed environment, agents with RMAAC policies get the highest mean rewards in almost all scenarios no matter what noise format is used. The only exception is in Keep away under linear noise. However, our RMAAC still achieves the highest rewards when testing in Keep away under Gaussian noise. In Figure \ref{fig_complicated}, we show the comparison results in a complicated scenario with a larger number of agents and RMAAC policies are trained with the Gaussian noise format $f_2$. As we can see that the RMAAC policies  get the highest reward when testing under optimally perturbed environments, cleaned and randomly perturbed environments. Higher rewards mean agents are performing better. It turns out RMAAC policies outperform the other two baseline algorithms when there exist worst-case state uncertainties. In Table~\ref{tab_test_var}, the left three columns report the variance under the linear noise format $f_1$,  and the right ones are under the Gaussian noise format $f_2$. RM1 denotes our RMAAC
policy trained with the linear noise format $f1$, RM2 denotes our RMAAC policy trained with the Gaussian noise format $f2$. The variance is used to evaluate the stability of the trained policies, i.e. the robustness to system randomness. Because the testing experiments are done in the same environments that are initialized by different random seeds. We can see that, by using our RMAAC algorithm, the agents can get the lowest variance in most scenarios under these two different perturbation formats. Therefore, our RMAAC algorithm is also more robust to the system randomness, compared with the baselines. In summary, our answer to the third question is 'Yes'.


% Figure environment removed

\begin{wrapfigure}{l}{0.6\textwidth}
\centering
% Figure removed
\vspace{-10pt}
\caption{RMAAC outperforms baseline MARL algorithms in terms of mean episode testing rewards in complicated scenarios with a larger number of agents of MPE.}
\label{fig_complicated}
% \vspace{-10pt}
\end{wrapfigure} 

\textbf{Interesting results when testing under lighter perturbations and cleaned environments: }
We also provide the testing results under a cleaned environment (accurate state information can be attained) and a randomly disturbed environment (injecting standard Gaussian noise into agents' observations). In Figures \ref{fig_test_mean_no} and \ref{fig_test_mean_random}, we respectively show the comparison of mean episode testing rewards under a cleaned environment and a randomly disturbed environment by using 4 different methods: RM1 denotes our RMAAC policy trained with the linear noise format $f_1$, RM2 denotes our RMAAC policy trained with the Gaussian noise format $f_2$, MA denotes MADDPG, M3 denotes M3DDPG. We can see that only in the Predator prey scenario, our method outperforms others under a cleaned environment. In Figure \ref{fig_test_mean_random}, we can see that our method outperforms others in the Cooperative communication, Keep away and Predator prey scenarios, and achieves similar performance as others in the Cooperative navigation scenario under a randomly perturbed environment. 

This kind of performance also happens in robust optimization \citep{beyer2007robust, Bookcvx_Boyd} and distributionally robust optimization \citep{Ye_dro, rahimian2019distributionally, miao2021data, he2020data, he2023data} where the robust solutions outperform other non-robust solutions in the worst-case scenario. Similarly, for single-agent RL with state perturbations, robust policies perform better compared with baselines under state perturbations~\citep{zhang2020robust_nips}. However, there exists a trade-off between optimizing the average performance and the worst-case performance for robust solutions in general, and the robust solutions may get relatively poor performance compared with other non-robust solutions when there is no uncertainty or perturbation in the environment even in a single agent RL problem~\citep{zhang2020robust_nips}. Improving the robustness of the trained policy may sacrifice the performance of the decisions when perturbations or uncertainties do not happen. That's why our RMAAC policies only beat all baselines in one scenario when the state uncertainty is eliminated. However, for many real-world systems, we can not assume that agents always have accurate information about the states. Hence, improving the robustness of the policies is very important for MARL as we explained in the introduction. It is worth noting that our RMAAC policies also work well in environments with random perturbations instead of only the worst-case perturbations. As shown in Fig.~\ref{fig_test_mean_random}, the performance of our RMAAC policies outperforms the baselines in most scenarios when random noise is introduced into the state.


% Figure environment removed

\color{black}

% Due to the page limits, more experiment results and explanations are in Appendix 
More experimental results and explanations are provided in Appendix \ref{sec_appendix_rmaac}. 
\useunder{\uline}{\ul}{}
\begin{table}[]
\centering
% \vspace{-15pt}
\caption{Variance of Testing Rewards under optimal perturbed environment}
\label{tab_test_var}
\begin{tabular}{lcccccc}
\hline
\multicolumn{1}{c|}{Perturbation function} & \multicolumn{3}{c|}{Linear noise  $f_1$}                                                       & \multicolumn{3}{c}{Gaussian noise  $f_2$ }                                                    \\ \hline
\multicolumn{1}{c|}{Algorithms}             & \multicolumn{1}{c}{\textbf{RM1}} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c|}{MA} & \multicolumn{1}{c}{\textbf{RM2}} & \multicolumn{1}{c}{M3} & \multicolumn{1}{c}{MA} \\ \hline
Cooperative communication (CC)                                          & {\ul \textbf{1.007}}       & 1.311                       & 1.292                       & {\ul \textbf{0.872}}       & 1.012                       & 0.976                       \\
Cooperative navigation (CN)                                         & {\ul \textbf{0.322}}       & 0.357                       & 0.351                       & {\ul \textbf{0.322}}       & 0.349                       & 0.359                       \\
Physical deception (PD)                             
& 0.225       
& 0.218                    
&  {\ul \textbf{0.217}}                     
& 0.244      
&  {\ul \textbf{0.161}}                    
& 0.252 \\       
Keep away (KA)                                      & {\ul \textbf{0.161}}       & 0.168                       & 0.175         & {\ul \textbf{0.161}}       & 0.17                       & 0.167                \\      
Predator prey (PP)              & 3.213                           & {\ul \textbf{0.161}}                         & 3.671                       & {\ul \textbf{2.304}}       & 2.711                       & 2.811  \\ \hline      
\end{tabular}
\vspace{-10pt}
\end{table}

\begin{table}[]
\vspace{-10pt}
\caption{Means and Variances of Mean Episode Rewards using Different Polices}
\centering
\begin{tabular}{lllll}
\hline
\label{tab_hisotry}
Scenarios & History-dependent policy            & Markov policy             &  &  \\ \hline
Cooperative communication (CC) & {\ul \textbf{-52.83}} $\pm$ 1.51  & -54.75 $\pm$ 3.03  &  &  \\
Cooperative navigation (CN)    & {\ul \textbf{-208.19}} $\pm$ 1.68 & -210.41 $\pm$ 1.13 &  &  \\
Physical deception (PD)        & {\ul \textbf{7.72}} $\pm$ 0.33    & 5.71 $\pm$ 0.19    &  &  \\
Keep away (KA)                 & {\ul \textbf{-20.69}} $\pm$ 0.09  & -21.18 $\pm$ 0.14  &  &  \\
Predator prey (PP)             & {\ul \textbf{7.10}} $\pm$ 0.17    & 6.116 $\pm$ 0.24   &  & \\ \hline
\end{tabular}
\end{table}
\textbf{Ablation study: } We conducted ablation studies for RMAAC algorithm. We first study the performance of RMAAC when it is used to train history-dependent policies. Other than using the current information as the input of the policy neural network, we also use history information in the latest three time steps, i.e. $h=4$. In Table \ref{tab_hisotry}, we show the mean and variance of mean episode rewards in $10$ runs. We use the same hyper-parameters in training history-dependent policies as training Markov policies. We can see that in all five scenarios, history-dependent policies outperform Markov policies. Besides, we investigate how the robustness performance of RMAAC is affected by varying variances of Gaussian noise format $\Sigma$ and the constraint parameter $\epsilon$. We also investigate the performance of RMAAC under other types of attacks. Please check the experimental setups and results of these ablation studies in Appendix \ref{appendix_ablation}.

