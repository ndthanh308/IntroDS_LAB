\subsection{Robust Multi-Agent Q-learning (RMAQ) Algorithm}
By solving the Bellman equation, we are able to get the optimal value function of an MG-SPA as shown in Theorem \ref{new_theorem}. We therefore develop a value iteration (VI)-based method called robust multi-agent Q-learning (RMAQ) algorithm.
Recall the Bellman equation using action-value function in \eqref{def_bellman_q}, the optimal action-value $q_*$ satisfies
$q^i_*(s,a,b) := r^i(s,a,b) + \gamma \mathbb{E} \left[ \sum_{s^\prime \in S} p(s^\prime | s, a, b) q^i_*(s^\prime, a^\prime, b^\prime) | a^\prime \sim \pi_*(\cdot |\tilde{s}^\prime), b^\prime \sim \rho_*(\cdot|s^\prime)\right].$
As a consequence, the tabular-setting RMAQ update can be written as below,
\footnotesize
{\begin{align}
\label{def_qlearning}
    &q_{t+1}^i(s_t, a_t, b_t) = (1-\alpha_t)  q_t^i(s_t, a_t, b_t) + \\
    &\alpha_t  \left[ r_t^i + \gamma\sum_{a_{t+1} \in A}\sum_{b_{t+1} \in B}  \pi_{*,t}^{q_t}(a_{t+1}|\tilde{s}_{t+1})\rho_{*,t}^{q_t}(b_{t+1}|s_{t+1})q^i_t(s_{t+1},a_{t+1},b_{t+1}) \right], \nonumber
\end{align}}
\normalsize
where   {$(\pi_{*,t}^{q_t}, \rho^{q_t}_{*,t})$ is an NE policy by solving the $2N$-player extensive-form game (EFG) based on a payoff function $(q_t^1, \cdots, q_t^N, -q_t^1, \cdots, -q_t^N)$. The joint policy $(\pi_{*,t}^{q_t}, \rho^{q_t}_{*,t})$ is used in updating $q_t$. All related definitions of the EFG  $(q_t^1, \cdots, q_t^N, -q_t^1, \cdots, -q_t^N)$ are introduced in Appendix \ref{sec_coop_efg}. }How to solve an EFG is out of the scope of this work, algorithms to do this exist in the literature~\citep{vcermak2017algorithm, kroer2020faster}. Note that, in RMAQ, each agent's policy is related to not only its own value function, but also other agents' value function. This \textit{multi-dependency} structure considers the interactions between agents in a game, which is different from the Q-learning in single-agent RL that considers optimizing its own value function. Meanwhile, establishing the convergence of a multi-agent Q-learning algorithm is also a general challenge. Therefore, we try to establish the convergence of \eqref{def_qlearning} in Theorem~\ref{theorem_q_convergence}, motivated from \cite{hu2003nash}. Due to space limitation, in Appendix \ref{appendix_rmaq}, we prove that RMAQ is guaranteed to get the optimal value function $q_* = (q^1_*, \cdots, q^N_*)$ by updating $q_t = (q_t^1, \cdots, q_t^N)$ recursively using \eqref{def_qlearning} under Assumptions \ref{assumption_convergence}.\\

\begin{assumption}{}
\textbf{}

(1) State and action pairs have been visited infinitely often.
(2) The learning rate $\alpha_t$ satisfies the following conditions: $0 \leq \alpha_t < 1$, $\sum_{t \geq 0}\alpha_t^2 \leq \infty$; if $(s,a,b) \neq (s_t, a_t, b_t)$, $\alpha_t(s, a, b) = 0$.
(3) An NE of the $2N$-player EFG based on $(q_t^1, \cdots, q_t^N, -q_t^1, \cdots, -q_t^N)$ exists at each iteration $t$.
\label{assumption_convergence}

\end{assumption}

\begin{theorem}{}
\textbf{}

Under Assumption \ref{assumption_convergence}, the sequence $\{ q_t \}$ obtained from \eqref{def_qlearning} converges to $\{ q_* \}$ with probability $1$, which are the optimal action-value functions that satisfy Bellman equations \eqref{def_bellman_q} for all $i = 1, \cdots, N$.
\label{theorem_q_convergence}

\end{theorem}
\input{sup_discussion_002}














\subsection{Robust Multi-Agent Actor-Critic (RMAAC) Algorithm}
According to the above descriptions of a tabular RMAQ algorithm, each learning agent has to maintain $N$ action-value functions. The total space requirement is $N|S||A|^N|B|^N$ if $|A^1| = \cdots = |A^N|, |B^1| = \cdots = |B^N|$. This space complexity is linear in the number of joint states, polynomial in the number of agents' joint actions and adversaries' joint actions, and exponential in the number of agents.   {The computational complexity is mainly related to algorithms to solve an extensive-form game \citep{vcermak2017algorithm, kroer2020faster}. However, even for general-sum normal-form games, computing an NE is known to be PPAD-complete, which is still considered difficult in game theory literature \citep{daskalakis2009complexity, chen2009settling, etessami2010complexity}.} These properties of the RMAQ algorithm motivate us to develop an actor-critic method to handle high-dimensional space-action spaces, which can incorporate function approximation into the update \citep{konda1999actor}.


We consider each agent $i$'s policy $\pi^i$ is parameterized as $\pi_{\theta^i}$ for $i \in \mathcal{N}$, and the adversary's policy $   \rho^{\tilde{i}}\color{black}$ is parameterized as $\rho_{\omega^i}$. We denote $\theta = (\theta^1, \cdots, \theta^N)$ as the concatenation of all agents' policy parameters, $\omega$ has the similar definition. For simplicity, we omit the subscript $\theta_i, \omega_i$, since the parameters can be identified by the names of policies. 
Then the value function $v^i(s)$ under policy $(\pi,\rho)$ satisfies
\begin{align}
\label{def_v_value}
            % v^{\pi,\rho,i}(s) = \sum_{a \in {A}} \sum_{b \in {B}}\rho(b | s) \pi(a | \tilde{s}) 
            % \sum_{s^{\prime} \in {S}}p(s^{\prime}| s, a, b)[r^i(s,a,b)+\gamma v^{\pi, \rho, i}(s^{\prime})] \\
            v^{\pi,\rho,i}(s) = \mathbb{E}_{a \sim \pi, b \sim \rho} 
            \left[ \sum_{s^{\prime} \in {S}}p(s^{\prime}| s, a, b)[r^i(s,a, b)+\gamma v^{\pi, \rho, i}(s^{\prime})] \right].
\end{align}
% Similarly, we also define the action-value function under policy $(\pi,\rho)$ satisfies 
% \begin{align}
%             q^{\pi,\rho,i}(s,a,b) = r^i(s,a,b) + \sum_{a^\prime \in {A}} \sum_{b^\prime \in {B}}\rho(b^\prime | s) \pi(a^\prime | \tilde{s})
%             \sum_{s^{\prime} \in {S}}p(s^{\prime}| s, a, b)[\gamma q^{\pi, \rho, i}(s^{\prime}, a^{\prime}, b^{\prime})] 
% \end{align}
We establish the general policy gradient with respect to the parameter $\theta, \omega$ in the following theorem. Then we propose our robust multi-agent actor-critic algorithm (RMAAC) which adopts a centralized-training decentralized-execution algorithm structure in MARL literature~\citep{lowe2017multi, foerster2018counterfactual}.\\
\begin{theorem}[Policy Gradient in RMAAC for MG-SPA]
\label{theorem_pg_rmaac}
For each agent $i \in \mathcal{N}$ and adversary   {$\tilde{i} \in \mathcal{M}$}, the policy gradients of the objective $J^i(\theta, \omega)$ with respect to the parameter $\theta, \omega$ are:
\begin{align}
    \nabla_{\theta^i} J^i(\theta, \omega) &= \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b)\gt \log \pi^{i}(a^i | \tilde{s}^i) \right] \label{mini_batch1}\\
    \nabla_{\omega^i} J^i(\theta, \omega) &= \E_{(s,a,b) \sim p(\pi, \rho)}\left[\qi(s,a,b) [\gw \log \rho^{\tilde{i}}(b^{\tilde{i}} | s) +  reg] \right] \label{mini_batch2}
\end{align}
where $reg = \nabla_{\tilde{s}^i} \log \pi^i(a^i|\tilde{s}^i) \nabla_{b^{\tilde{i}}}f(s,b^{\tilde{i}}) \gw \rho^{\tilde{i}}(b^{\tilde{i}}|s)$.
\end{theorem}
\begin{proof}
% Taking gradient with respect to $\theta^i, \omega^i$ for all $i$ on both sides of \eqref{def_v_value} yields the results. 
See details in Appendix \ref{proof_theorem_pg_rmaac}.
\end{proof}
We put the pseudo-code of RMAAC in Appendix \ref{sec_alg_robust_actor_critic}.

\begin{remark}[History-dependent Policy]
RMAAC can calculate history-dependent policies by using recent observations as the policy input. For example, DQN \citep{mnih2015human} maps historyâ€“action pairs to scalar estimates of Q-value. It uses the history (4 most recent frames) of the states and the action as the inputs of the neural network.
\end{remark}



