\section{Related Work}

% Figure environment removed

\subsection{Single-Agent Prediction}
\boldparagraph{Scene Representation}
In dynamic traffic scenes, representing the scene elements and modeling interactions between them play a crucial role in performance. In single-agent prediction, previous works focus on the representation of the scene and interaction modeling from the viewpoint of the agent of interest. Earlier works~\cite{Cui2019ICRA, Casas2018CoRL, Hong2019CVPR, Biktairov2020NeurIPS, Konev2022ARXIV, Luo2018CVPR} create a rasterized image to represent both the context and the interactions. The previous locations of agents are typically encoded with sequential models such as RNNs~\cite{Mercat2020ICRA, Khandelwal2020ARXIV, Alahi2016CVPR, Gupta2018CVPR, Park2020ECCV, Salzmann2020ECCV}. Combining the two, the following works explore more specialized scene representations~\cite{Tang2019NeurIPS, Chai2019CoRL, Djuric2020WACV, Gilles2021ITSC, Rhinehart2019ICCV, Buhet2020CoRL, Park2020ECCV, Mo2020ARXIV}. In contrast to rasterized representation, Graph Neural Networks~(GNNs) enable a more explicit way of modeling interactions such as with a lane graph~\cite{Liang2020ECCV} or a vectorized representation of the scene~\cite{Gao2020CVPR}. Based on their success in capturing hierarchical representations, recent works continue adapting GNNs for interaction modeling~\cite{Zhao2020CoRL, Gu2021ICCV, Zeng2021IROS, Aydemir2022ARXIV}. Towards the same purpose, more recent works~\cite{Zhou2022CVPR, Liu2021CVPR, Song2021CoRL, Ngiam2022ICLR, Gilles2022ICLR}  use transformers with multi-head attention~\cite{Vaswani2017NeurIPS}, whose success has been proven repeatedly~\cite{Dosovitskiy2020ICLR, Caron2021ICCV, Devlin2018ARXIV, Carion2020ECCV}. We also use a scene representation based on multi-head attention.

\boldparagraph{Attention-Based Encoding}
Transformers are widely used for interaction modeling due to their ability to capture the interaction between different scene elements. 
VectorNet~\cite{Gao2020CVPR} uses the same attention weights for different types of elements, \ie agents and lanes. LaneGCN~\cite{Liang2020ECCV} categorizes interactions and uses a different type of attention for each, leading to a more specialized representation. Due to its success, the following works~\cite{Liu2021CVPR, Wang2022CVPR, Zhou2022CVPR} continue modeling different types of interactions. Recently, factorized attention~\cite{Ngiam2022ICLR, Girgis2022ICLR} has been proposed to model temporal relations between scene elements efficiently. Instead of performing attention on the entire set of agents and time steps, factorized attention separately processes each axis, \ie agent, time, and road graph elements. 
A similar factorization over time and agents is explored in Autobot~\cite{Girgis2022ICLR} with a smaller and more efficient architecture. 
We also use different types of attention to model different types of interactions but enable updated information flow between different scene elements with iterative updates.




% Figure environment removed


\subsection{Multi-Agent Prediction}
\boldparagraph{Multi-Agent Prediction} 
Due to the evaluation setting on publicly available datasets~\cite{Chang2019CVPR, Caesar2020CVPR}, most existing works focus on predicting the trajectory of a single agent. While it led to great progress in scene representations and modeling of dynamic interactions, in real-life scenarios, the agent needs to account for the future trajectories of other agents as well. Multi-agent prediction has been recently addressed by SceneTransformer~\cite{Ngiam2022ICLR} with a unified architecture to jointly predict consistent trajectories for all agents. While this method can perform inference for all agents in a single forward pass, HiVT~\cite{Zhou2022CVPR} iterates over agents in an agent-centric representation, leading to the aforementioned inefficiency issues. LTP~\cite{Wang2022CVPR} follows a different approach with a lane-oriented representation to predict the most likely lane for each agent in a single pass. However, lane classification may not be as precise as regression. Our method can regress the trajectories of each agent efficiently in a single pass.

\boldparagraph{Reference Frame} 
The existing works represent the scene either from the viewpoint of an agent or from a fixed reference point as illustrated in \figref{fig:scene_vs_agent}. In the agent-centric representation~\cite{Zhou2022CVPR, Girgis2022ICLR, Ye2021CVPR, Gu2021ICCV, Zhao2020CoRL, Liang2020ECCV}, the scene is transformed so that the agent of interest is positioned at the origin of the scene. In contrast, all elements are positioned with respect to the same reference point in a scene-centric representation~\cite{Ngiam2022ICLR}. This shared context representation is especially helpful for multi-agent prediction~\cite{Wei2019ARXIV, Sun2020CVPR} while the agent-centric works better for single-agent prediction~\cite{Chang2019CVPR, Caesar2020CVPR} due to the simplification of the problem. It allows focusing on a single agent without worrying about other agents except for their relation to the agent of interest. Multi-agent prediction can be performed with sequential agent-centric predictions, \ie one agent at a time~\cite{Girgis2022ICLR, Wang2022CVPR, Gu2021ICCV, Zhou2022CVPR, Gilles2022ICLR}. However, this straightforward extension scales linearly with the number of agents in the scene and raises efficiency concerns. We use a scene-centric representation for multi-agent predictions but adapt to each agent with dynamic weights to benefit from agent-specific features as in agent-centric representation.
