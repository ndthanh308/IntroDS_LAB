\section{Training Details}
\label{sec:train_details}
As mentioned in the paper, we use the variety loss to capture multi-modal futures by calculating the loss only for the most accurate trajectory over $K$ predicted ones. Given the ground truth trajectory $\{\bs_t\}_{t=1}^T$ and the predicted trajectory with the closest endpoint $\{\hat{\bs}_t\}_{t=1}^T$ for $T$ future steps, we train our model using the endpoint loss $\mathcal{L}_{end}$, the full trajectory loss $\mathcal{L}_{traj}$, and the trajectory classification loss $\mathcal{L}_{cls}$. $\mathcal{L}_{end}$ is the difference between the closest endpoint and the ground truth endpoint:
\begin{align}
    \mathcal{L}_{end} =&~ \mathcal{L}_{\text{Smooth-}\ell_1}(\hat{\bs}_T,~\bs_T)
\end{align}
where $\hat{\bs}_T$ is the endpoint of $\hat{\bs}$, \ie the prediction at time $T$. $\mathcal{L}_{traj}$ is the mean of the per-step difference between the predicted full trajectory, $\hat{\bs}$, and the ground truth trajectory, $\bs$:
\begin{align}
    \mathcal{L}_{traj} =&~ \frac{1}{T} \sum_{t=1}^{T} \mathcal{L}_{\text{Smooth-}\ell_1}(\hat{\bs}_t,~\bs_t)
\end{align}
Finally, $\mathcal{L}_{cls}$ is the Binary Cross Entropy Loss applied to the assigned probabilities $\bp$ of $K$ trajectories where the ground truth probability of the closest trajectory $\hat{\bs}$ is set to $1$ and the others to $0$:
\begin{align}
    \mathcal{L}_{cls} =&~ \mathcal{L}_{\text{BCE}}(\bp,~\by)
\end{align}
where $\by$ denotes the ground truth probabilities assigned. Overall, our loss is the sum of these three losses:
\begin{align}
    \mathcal{L} =&~ \mathcal{L}_{end} + \mathcal{L}_{traj} + \mathcal{L}_{cls}
\end{align}
