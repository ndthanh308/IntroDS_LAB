\documentclass[aps,reprint,superscriptaddress,pra,10pt]{revtex4-1}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[breaklinks=true,hidelinks]{hyperref}
\usepackage{dsfont}
%\usepackage{ulem}
\usepackage{enumitem}
\usepackage{cases}
\usetikzlibrary{quantikz2}



\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}

%%% shortcut functions
\DeclareMathOperator{\sgn}{sgn}
\newcommand{\eq}[1]{(\ref{eq:#1})}
\newcommand{\fig}[1]{{\bf\ref{fig:#1}}}
\newcommand{\sect}[1]{{\bf\ref{sec:#1}}}
\newcommand{\tab}[1]{{\bf\ref{tab:#1}}}
\newcommand{\T}{T}
\newcommand{\U}{U}
\newcommand{\vv}[1]{\boldsymbol{#1}}
\newcommand{\bpi}{\boldsymbol{\pi}}
\newcommand{\epsrel}{\epsilon_{\text{r}}}


%%% -------------------
%%% ---- colors -----
%%% -------------------
\newcommand{\la}[1]{{\color{blue}#1}}
\newcommand{\LA}[1]{{\color{blue}$\big[\![$\,\raisebox{.7pt}{L}\!\!\:\raisebox{-.7pt}{A}: \textit{#1}\,$]\!\big]$}}
\newcommand{\gc}[1]{{\color{brown}#1}}
\newcommand{\GC}[1]{{\color{brown}$\big[\![$\,\raisebox{.7pt}{G}\!\!\:\raisebox{-.7pt}{C}: \textit{#1}\,$]\!\big]$}}
\newcommand{\tl}[1]{{\color{violet}#1}}
\newcommand{\TL}[1]{{\color{violet}$\big[\![$\,\raisebox{.7pt}{T}\!\!\:\raisebox{-.7pt}{S}: \textit{#1}\,$]\!\big]$}}
\newcommand{\ad}[1]{{\color{teal}#1}}
\newcommand{\AD}[1]{{\color{teal}$\big[\![$\,\raisebox{.7pt}{A}\!\!\:\raisebox{-.7pt}{D}: \textit{#1}\,$]\!\big]$}}
\newcommand{\Danger}[1]{\textcolor{red}{{\fontencoding{U}\fontfamily{futs}\selectfont\char 49\relax} \textit{ #1} {\fontencoding{U}\fontfamily{futs}\selectfont\char 49\relax}}}
\newcommand{\Discuss}[1]{\textcolor{red}{{\fontencoding{U}\fontfamily{futs}\selectfont\char 49\relax} \textit{discuss} {\fontencoding{U}\fontfamily{futs}\selectfont\char 49\relax}}}



\begin{document}
\title{Randomized semi-quantum matrix processing}

\author{Allan Tosta}
%\email{mail@email.com}
\affiliation{Federal University of Rio de Janeiro, Caixa Postal 68528, Rio de Janeiro, RJ 21941-972, Brazil}
\author{Thais de Lima Silva}
%\email{mail@email.com}
\affiliation{Quantum Research Center, Technology Innovation Institute, Abu Dhabi, UAE}

\author{Giancarlo Camilo}
%\email{giancarlo.camilo@tii.ae}
\affiliation{Quantum Research Center, Technology Innovation Institute, Abu Dhabi, UAE}

\author{Leandro Aolita}
%\email{mail@email.com}
\affiliation{Quantum Research Center, Technology Innovation Institute, Abu Dhabi, UAE}
\affiliation{Federal University of Rio de Janeiro, Caixa Postal 68528, Rio de Janeiro, RJ 21941-972, Brazil}

%- ABSTRACT -----------------------------------------------------------------------------------------------------------
\begin{abstract}
Quantum computers have the potential for game-changing runtime speed-ups for important matrix-arithmetic problems. 
A prominent toolbox for that is the quantum singular-value transformation (QSVT) formalism in the setting of coherent access to the input matrix via a unitary block encoding and Chebyshev approximations to a target matrix function. 
%The most powerful algorithms are quantum signal processing (QSP) and singular-value transformation (QSVT), in the setting of Chebyshev approximations to the target matrix function and quantum query access to the input matrix via a unitary block encoding.  
%Nonetheless, physical implementations for useful end-user applications are out of reach of current quantum devices.
Nonetheless, physical implementations for useful end-user applications require large-scale fault-tolerant quantum computers. 
Here, we present a hybrid quantum-classical framework for Monte-Carlo simulation of generic matrix functions tailored to early fault-tolerant quantum hardware.
Our algorithms randomize over the Chebyshev polynomials but keep the matrix oracle quantum, and are assisted by a variant of the Hadamard test that removes the need for post-selection. 
As a result, they feature a similar statistical overhead to the fully-quantum case of standard QSVT and do not incur in any degradation in circuit depth.
On the contrary, the average circuit depth is significantly smaller.
We apply our technique to four specific use cases: partition-function estimation via quantum Markov-chain Monte Carlo and via imaginary-time evolution; end-to-end linear system solvers; and ground-state energy estimation.
For these cases, we prove significant advantages of average over maximal depths, including quadratic speed-ups on costly parameters and even the removal of an approximation-error dependence.  
These translate into equivalent reductions of noise sensitivity, because the detrimental effect of noise scales with the average (and not the maximal) query depth, as we explicitly show for depolarizing noise and coherent errors.
All in all, our framework provides a practical pathway towards early fault-tolerant quantum linear-algebra applications.       
\end{abstract}

\maketitle


%- Intro -----------------------------------------------------------------------------------------------------------


\section{Introduction}\label{sec:intro}
Faster algorithms for linear algebra are a major promise of quantum computation, holding the potential for precious %(in some cases even supra-polynomial) 
runtime speed-ups over classical methods. 
A modern, unified framework for such algorithms is given by the quantum signal processing (QSP) \cite{LowChuang2017,LowChuangQuantum2019} and, more generally, quantum singular-value transformation (QSVT) \cite{Gilyen2019} formalisms. These are powerful techniques to manipulate a matrix, coherently given by a quantum oracle, via polynomial transformations on its eigenvalues and singular values, respectively.
The class of matrix arithmetic attained is remarkably broad, encompassing primitives as diverse as %amplitude amplification, factoring, 
Hamiltonian simulation, matrix inversion, ground-state energy estimation, Gibbs-state sampling, among others \cite{ChuangGrandUnification}.
Moreover, the framework often offers the state-of-the-art in asymptotic query complexities (i.e. number of oracle calls), in some cases matching known complexity lower bounds.
Nevertheless, the experimental requirements for full implementations are prohibitive for current devices, and it is not clear if the framework will be useful in practice before large-scale fault-tolerant quantum computers appear.

This has triggered a quest for {\it early fault-tolerant algorithms} for matrix processing that allow one to trade performance for nearer-term feasibility in a controlled way, i.e. with provable runtime guarantees \cite{silva_fragmented_2022,lin_heisenberg-limited_2022,wang_quantum_2022,silva2022fourierbased,campbell_random_2019,wan_randomized_2022,wang2023qubitefficient,Campbell_2021,Dong_2022,wang2023faster}. 
Particularly promising are randomized hybrid quantum-classical schemes 
to statistically simulate a matrix function via quantum implementations of more elementary ones.
For instance, this has been %recently 
applied to %studied in detail for 
the Heaviside step function $\Theta(H)$ of a Hamiltonian $H$ \cite{lin_heisenberg-limited_2022,wang_quantum_2022,campbell_random_2019,wan_randomized_2022,wang2023qubitefficient,Campbell_2021}, which allows for eigenvalue thresholding, a practical technique for Heisenberg-limited spectral analysis \cite{lin_heisenberg-limited_2022}. 
Two input access models have been considered there: quantum oracles as a controlled unitary evolution of $H$ \cite{lin_heisenberg-limited_2022,wang_quantum_2022,Dong_2022} and classical ones given by a decomposition of $H$ as a linear combination of %multi-qubit 
Pauli operators \cite{campbell_random_2019,wan_randomized_2022,wang2023qubitefficient,Campbell_2021}. 
In the former, one Monte-Carlo %samples \GC{'one...samples' structure annoyed me a bit}\LA{How about the new wording?} 
simulates the Fourier series of $\Theta(H)$ by randomly sampling its harmonics. In the latter -- in an additional 
level of randomization -- one also probabilistically samples the Pauli terms from the linear combination. 


Curiously, however, randomized quantum algorithms for matrix processing have been little explored beyond the specific case of the Heaviside function. Ref. \cite{wang2023qubitefficient} put forward a randomized, qubit-efficient technique for Fourier-based QSP \cite{silva2022fourierbased,Dong_2022} for generic functions. 
However, the additional level of randomization %used introduces an expensive statistical overhead and can also even detrimentally affect the circuit depth per run. 
can detrimentally affect the circuit depth per run, as compared to the case with coherent  oracles. 
On the other hand, in the quantum-oracle setting, the randomized algorithms above have focused mainly on controlled unitary evolution as input access model. 
This is convenient in specific cases where $H$ can be analogically implemented. 
However, it leaves aside the powerful class of {\it block-encoding oracles}, i.e. unitary matrices with the input matrix as one of its blocks \cite{LowChuangQuantum2019}. 
Besides having a wider scope of applicability (including %, e.g., 
 non-Hermitean matrices), such oracle types are also a more natural choice for digital setups. 
Moreover, randomized quantum algorithms have so far not addressed %the quintessential basis functions for approximation theory: Chebyshev polynomials \cite{trefethen_approx}, which are often more accurate than Fourier series \cite{boyd_spectral_methods}. 
Chebyshev polynomials, % --
the quintessential basis functions for approximation theory \cite{trefethen_approx}, which often attain significantly better accuracy than Fourier series \cite{boyd_spectral_methods}. 
Chebyshev polynomials, together with block-encoding oracles, provide the most sophisticated and general arena for quantum matrix arithmetic \cite{LowChuang2017,LowChuangQuantum2019,Gilyen2019,ChuangGrandUnification}. 

Here, we fill in this gap. We derive a semi-quantum algorithm for Monte-Carlo simulations of QSVT with provably better circuit complexities than fully-quantum schemes as well as notable advantages in terms of experimental feasibility. Our results are summarized next.

\section{Summary of our contributions}
\label{sec:summary}
Our method estimates state amplitudes and expectation values involving a generic matrix function $f(A)$ leveraging three main ingredients: 
$i$) it samples each component of a Chebyshev series for $f$ with a probability proportional to its coefficient in the series;
$ii$) it assumes coherent access to $A$ via a block encoding oracle; and
$iii$) $f(A)$ is automatically extracted from its block-encoding without post-selection, 
using a Hadamard test.
The combination of $i$) and $ii$) leaves untouched the maximal query complexity $k$ per run native from the Chebyshev expansion. 
In addition, the statistical overhead we pay for end-user estimations scales only with the $l_1$-norm of the Chebyshev coefficients. 
For the use cases we consider, this turns out to be similar (at worst up to logarithmic factors) to the operator norm of $f(A)$, which would govern the statistical overhead if we used fully-quantum (i.e. standard) QSVT. That is, our scheme does not incur in any significant degradation with respect to the fully-quantum case either in runtime or circuit depth. 
On the contrary, the average query complexity can be significantly smaller than $k$. 
We prove interesting speed-ups of the former over the latter for practical use cases. 

These speed-ups translate directly into equivalent reductions in noise sensitivity: 
For simple models such as depolarization or coherent errors in the quantum oracle, we show that the estimation inaccuracy due to the noise scales with the average query depth instead of the maximal depth as in fully quantum implementations. 
Importantly, we implement each sampled Chebyshev polynomial with a simple sequence of queries to the oracle using qubitization; no QSP pulses are required throughout. 
Finally, $iii$) circumvents the need for expensive %techniques to deal with post-selection, such as 
repeat until success or quantum amplitude amplification. That is, no statistical run is wasted and no overhead in circuit depth is incurred. The only price paid is the need for the control qubit in the Hadamard test, but fully-quantum implementations would require yet another ancilla controlling everything else (due to the QSP pulses). All this renders our hybrid approach significantly more experimentally-friendly than fully-quantum implementations.

As use cases, we benchmark our framework on four end-user applications: partition-function estimation of classical Hamiltonians via quantum Markov-chain Monte Carlo (MCMC);  partition-function estimation of quantum Hamiltonians via quantum imaginary-time evolution (QITE); linear system solvers (LSSs); and ground-state energy estimation (GSEE). 
The maximal and expected query depths per run as well as the total expected runtime (taking into account sample complexity) are displayed in Table \tab{main_table}, in Sec. \ref{sec:usecases}. In all cases, we systematically obtain the following advantages (both per run and in total) of expected versus maximal query complexities. 

For MCMC, we prove a quadratic speed-up on a factor $\mathcal{O}(\log (Z_\beta\, e^{\beta}/\epsrel))$, where $Z_\beta$ is the partition function to estimate, at inverse temperature $\beta$, and $\epsrel$ is the tolerated relative error. 
For QITE, we remove a factor  $\mathcal{O}(\log (D\,e^{\beta}/Z_\beta\,\epsrel))$ from the scaling, where $D$ is the system dimension. For LSSs we consider two sub-cases: estimation of an entry of the (normalized) solution vector and of the expectation value of an observable $O$ on it. 
%Denoting by $\kappa$ the condition number of the matrix and by $\epsilon$ the tolerated additive error, 
We prove quadratic speed-ups on factors $\mathcal{O}\big(\log(\kappa/\epsilon)\big)$ and $\mathcal{O}\big(\log(\kappa^2\, \|O\|/\epsilon)\big)$ for the first and second sub-cases, respectively, where $\|O\|$ is the operator norm of $O$, $\kappa$ is the condition number of the matrix, and $\epsilon$ the tolerated additive error. 
Remarkably, this places our query depth at an intermediate position between that of the best known Chebyshev-based method \cite{Childs_2017} %, $\mathcal{O}\left(\kappa\log(\kappa/\epsilon)\right)$, 
and the optimal one in general %, $\mathcal{O}\left(\kappa\log(1/\epsilon)\right)$ 
\cite{costa2021optimal}.
In turn, compared to the results obtained in \cite{wang2023qubitefficient} via full randomization, our scaling is one power of $\kappa$ superior. 
Finally, for GSEE, we prove a speed-up on a factor that depends on the overlap $\eta$ between the probe state and the ground state: the average query depth is $\mathcal{O}\big(\frac{1}{\xi}\sqrt{\log(1/\eta)}/\log(1/\xi)\big)$, whereas the  maximal query depth is $\mathcal{O}\big(\frac{1}{\xi}\log (1/\eta)\big)$, with $\xi$ the additive error in the energy estimate.

Our techniques offer a major reduction in the experimental requirements for early fault-tolerant quantum linear-algebra applications.

% \section{Structure of the paper}\label{sec:structure}

% \la{The paper is organized as follows...}






%Quantum algorithms scale better then their best classical counterparts for several tasks. 
%Despite this promise, the analysis of total resource required by quantum computers, including multiplicative factors, state preparation and error correction, can completely destroy any scaling advantage for reasonable-size problems.  The quantum resource reduction is especially relevant for mid-term devices which are expected to have some degree of error correction, but  limited space and time capabilities. For this devices, a factor of $2$, for instance, can be difference between a successfully implementable algorithm and a non-implementable or totally noisy one.

%The need for closing the gap between quantum hardware and software has encouraged the seek for new algorithms and improvements in resource management in the existing ones guided by the limited quantum resources possibly available in the near future. With this aim, one can try to reduce the width and depth of quantum circuits at the expense of increasing the statistical overhead and classical post-processing. This is the idea behind randomized algorithms \cite{campbell_random_2019,chowdhury_computing_2021,wan_randomized_2022,wang_quantum_2022} \LA{is Ref. \cite{chowdhury_computing_2021} intentionally within this list?}. A recent example for matrix manipulation is the proposal by Wang \emph{et al.} \cite{wang2023qubitefficient}. There, a Pauli decomposition of a matrix $A$ is assumed as input to sample properties of a function $f(A)$. 

%In this work, we present a randomized version of QSVT for sampling from matrix functions. Our algorithm involves sampling from the coefficients of a Chebyshev approximation, and then implementing each sampled Chebyshev polynomial using QSVT. Unlike traditional QSVT, which requires post-selection, our algorithm obtains measurement results from a Hadamard test in a non-probabilistic manner. 
%This means that no statistics are wasted, and the algorithm can achieve improved efficiency while still maintaining a comparable level of accuracy. Furthermore, the Chebyshev coefficients typically decay quickly and the most frequently sampled polynomials have low degree. As the depth of the QSVT circuit to implement a polynomial is proportional to its degree, this results in a reduction in the average number of queries to the oracle and shallower circuits, naturally suppressing cumulative errors compared to coherent QSVT. 










% \newpage
% \quad
% \newpage














%- Preliminaries -----------------------------------------------------------------------------------------------------------

\section{Preliminaries}\label{sec:prelim}

%Our basic framework is that 
We consider the basic setup of Quantum Singular Value Transformation (QSVT) \cite{Gilyen2019,ChuangGrandUnification}. 
This is a powerful technique for synthesizing polynomial functions of a linear operator embedded in a block of a unitary matrix, via polynomial transformations on its singular values. 
Combined with approximation theory \cite{Vishnoi2013}, this leads to state-of-the-art query complexities and an elegant unifying structure for a variety of quantum algorithms of interest. 
For simplicity of the presentation, in the main text we focus explicitly on the case of Hermitian matrices. There, QSVT reduces to the simpler setup of Quantum Signal Processing (QSP) \cite{LowChuang2017,LowChuangQuantum2019}, describing eigenvalue transformations. The extension of our algorithms to QSVT for generic matrices is straightforward and is left for App. \sect{QSVT}. %\GC{mention also assumption of normalized matrix}
%Nevertheless, our semi-quantum algorithm extends straightforwardly to the general case of QSVT, as shown in in App. \sect{QSVT}.
Throughout the paper, we adopt the short-hand notation $[l]:=\{0,\ldots,l-1\}$ for any $l\in\mathbb{N}$.

The basic input taken by QSP is a block-encoding $U_{A}$ of the Hermitian operator $A$ of interest (the \textit{signal}). 
A block-encoding is a unitary acting on $\mathcal{H}_s\otimes\mathcal{H}_{a}$, where $\mathcal{H}_s$ is the system Hilbert space where $A$ acts and $\mathcal{H}_{a}$ is an ancillary Hilbert space (with dimensions $D$ and $D_a$, respectively), satisfying 
\begin{equation}\label{eq:block_encoding}
\big(\bra{0}_a\otimes \mathds{1}_s\big)\,U_A\,\big(\ket{0}_a\otimes \mathds{1}_s\big) = A
%{\phantom{\ket{0}}}_a\!\mel{0}{U_{A}}{0}_{a}=A,
\end{equation}
for some suitable state $\ket{0}_{a}\in \mathcal{H}_{a}$ (here $\mathds{1}_s$ is the identity operator in $\mathcal{H}_s$). 
Designing such an oracle for arbitrary $A$ is a non-trivial task \cite{camps2023explicit}, but efficient block-encoding schemes are known in cases where some special structure is present, e.g., when $A$ is sparse or expressible as a linear combination of unitaries \cite{LowChuangQuantum2019,Gilyen2019,sunderhauf2023blockencoding}. 
In particular, we will need the following particular form of $U_A$ that makes it amenable for dealing with Chebyshev polynomials. 

\begin{definition}[Qubitized block-encoding oracle]\label{def:qubitizedoracle}
Let $\mathcal{H}_s$ and $\mathcal{H}_{a}$ denote the system and ancillary Hilbert spaces, respectively, $A$ be a Hermitian matrix on $\mathcal{H}_s$ with spectral norm $\norm{A}\leq1$, eigenvalues $\{\lambda_\gamma\}_{\gamma\in [D]}$, and eigenstates $\{\ket{\lambda}_s\}$. 
A unitary $U_{A}$ acting on $\mathcal{H}_{sa}:=\mathcal{H}_{s}\otimes\mathcal{H}_{a}$ is called a (exact) \textit{qubitized block-encoding} of $A$ if 
it has the form
\begin{align}\label{eq:qubitizedoracle}
U_{A} = \bigoplus_{\gamma\in[D]} e^{-i\,\theta_\gamma\,Y_\gamma}
% =
% \bigoplus_\lambda 
% \begin{pmatrix}
% \lambda & -\sqrt{1-\lambda^2}\\
% \sqrt{1-\lambda^2} &\lambda
% \end{pmatrix}
\,,
%= \bigoplus_{\lambda}[\lambda \mathds{1}_{a}-i\sqrt{1-\lambda^{2}}Y_{a}],
%W_{A}=\bigoplus_{\lambda}\ketbra{\lambda}\otimes[\lambda \mathds{1}_{a}-i\sqrt{1-\lambda^{2}}Y_{a}],
\end{align}
where $\theta_\gamma:=\arccos(\lambda_\gamma)$ and $Y_{\gamma}$ is the second Pauli matrix acting on the two-dimensional subspace spanned by $\big\{\ket{0}_a\otimes\ket{\lambda_\gamma}_s, \ket{\perp_{\lambda_\gamma}}_{sa}\big\}$ with ${}_{sa}\!\bra{\perp_{\lambda_\gamma}}\big(\ket{0}_a\otimes\ket{\lambda_\gamma}_s\big)=0$.
\end{definition}



\noindent %The nomenclature stems from the fact that the unitary splits into single-qubit Pauli gates $e^{-i\,\arccos(\lambda)\,\hat{Y}_\lambda}=\lambda \mathds{1}_{\lambda}-i\sqrt{1-\lambda^{2}}Y_{\lambda}$ acting independently at each eigenvalue subspace \LA{I don't think we need such clarification}. 
A qubitized oracle of the form \eq{qubitizedoracle} can be constructed from any other block-encoding $U'_A$ of $A$ using at most one query to $U'_A$ and ${U'_A}^{-1}$, at most one additional ancillary qubit, and $\mathcal{O}(\log(D_a))$ quantum gates \cite{LowChuangQuantum2019}. 
%\LA{Please, use parentheses for the log. Also, how about introducing the notation $D_a:=\text{dim}(\mathcal{H}_a)$?} 

%The standard QSP algorithm 
Standard QSP takes as input the qubitized oracle $U_A$ %, sometimes referred to as the QSP iterate \LA{Do we wanna use the term iterate for this or rather reserve it for controlled-$U_A$?}, 
and transforms it into (a block-encoding of) a polynomial function $\Tilde{f}(A)$. With the help of approximation theory \cite{trefethen_approx}, this allows the approximate implementation of generic non-polynomial functions $f(A)$.
%\la{polynomial approximation $\tilde{f}(A)$ to our target function $f(A)$ of $A$}. 
The algorithm complexity is measured by the number of queries to $U_A$, which allows for rigorous quantitative statements agnostic to details of $A$ or to hardware-specific circuit compilations. 
%We will be interested in cases where $\tilde{f}$ is a truncated Chebyshev series approximating some analytic function $f$. 
For our purposes, only %the simplest 
a simple QSP result will be needed, namely the observation \cite{LowChuangQuantum2019} that repeated applications of $U_A$ give rise to %a block-encoding of 
Chebyshev polynomials of $A$ (see App. \sect{chebyshev_proof} for a proof).

\begin{lemma}[Block encoding of Chebyshev polynomials] \label{cheb}
Let $U_A$ be a qubitized block-encoding of $A$. Then 
\begin{equation}
\big(\bra{0}_a\otimes \mathds{1}_s\big)\,U_A^j\,\big(\ket{0}_a\otimes \mathds{1}_s\big) = T_j(A)\,,
\end{equation}
for $j\in\mathbb{N}$, where $T_{j}(\cdot)$ is the $j$-th order Chebyshev polynomial of the first kind. 
% \begin{equation}\label{eq:WHk}
% {W_A}^k = \bigotimes_\lambda
% \begin{pmatrix}
% T_k(\lambda) & -\sqrt{1-\lambda^2}U_{k-1}(\lambda)\\
% \sqrt{1-\lambda^2}U_{k-1}(\lambda) &\lambda T_{k}(\lambda)
% \end{pmatrix}
% ,
% %W^{k}_{A}=\bigoplus_{\lambda}\ketbra{\lambda}_{s}\otimes[T_{k}(\lambda)Z_{a}+\sqrt{1-\lambda^{2}}U_{k-1}(\lambda)X_{a}],
% \end{equation}
% where $T_{k}(\lambda),U_{k}(\lambda)$ are the $k$-th order Chebyshev polynomials of the first and second kinds, respectively. \GC{I don't like $U_k$ for the modified Chebyshev polynomials. Use caligraphic? $\T_k(x)$ and $\U_k(x)$.} 
\end{lemma}


%In the following, we will be 
We are interested in a truncated Chebyshev series 
\begin{equation}
\label{eq:ftilde}
\Tilde{f}(x)=\sum^{k}_{j=0}a_{j}T_{j}(x) %\sideset{}{'}\sum^{k}_{j=0}a_{j}T_{j}(x)
\end{equation}
providing a $\nu$-approximation to the target real-valued function $f:[-1,1]\rightarrow\mathbb{R}$, i.e., $\max_{x\in [-1,1]}\abs{f(x)-\Tilde{f}(x)} \le \nu$. 
The Chebyshev polynomials $T_{j}$ are a key basis functions for approximation theory, often leading to near-optimal approximation errors \cite{trefethen_approx}. In particular, unless the target function is periodic and smooth, they tend to outperform Fourier approximations \cite{boyd_spectral_methods}. 
The case of complex-valued functions can be treated similarly by splitting it into its real and imaginary parts. 
% \begin{equation}
% \label{eq:ftilde}
% \Tilde{f}(x)=\sum^{k}_{j=0}a_{j}T_{j}(x) %\sideset{}{'}\sum^{k}_{j=0}a_{j}T_{j}(x)
% \end{equation}
%is such that 
% \begin{equation}\label{eq:approx_def}
% \max_{x\in [-1,1]}\abs{f(x)-\Tilde{f}(x)} \le \nu \,.
% \end{equation}
%\Discuss{\LA{Something that I was thinking is whether we should give exemplary scalings of $k$ for the use cases we tackle, or may be even a single use case as example? For instance, some comment like "For instance, for the case of the inverse function, useful for the celebrated quantum linear-system solvers, $k=\mathcal{O}(\kappa\log{(1/\nu)})$". What do you think? Not sure.} \GC{I like the suggestion, because in the way presented above it is not clear that $k$ is actually set by the precision $\nu$. See my attempt to include this comment without making ref to specific functions -->} }
The truncation order $k$ is controlled by the desired accuracy $\nu$ in a problem-specific way (see Sec. \sect{usecases} for explicit examples). 
We denote by $\vv{a}:=\big\{a_0,\ldots,a_k\big\}$ the vector of  Chebyshev coefficients of $\Tilde{f}$  %$\vv{a}:=\big\{\frac{a_0}{2},a_1,\ldots,a_k\big\}$,  %(notice the factor $\frac{1}{2}$ in $a_0$), 
and by $\norm{\vv{a}}_1:=\sum_{j=0}^k|a_j|$ its $\ell_1$-norm. 
%In addition, it will be useful to introduce the short-hand notations $[l]:=\{1,\ldots,l\}$ and $[l]_0:=\big\{0,\ldots,l\big\}$, for any $l\in\mathbb{N}$.














%- Results -----------------------------------------------------------------------------------------------------------
\section{Results} \label{sec:results}

We are now in position to state our main results. First we set up explicitly the two problems of interest and then proceed to describe 
our randomized semi-quantum algorithm to solve each one of them, proving correctness, runtime, and performing an error-robustness analysis. We conclude by applying our general framework to a number of exemplary use cases of interest. 













% Figure environment removed












\subsection{Problem statement}\label{sec:problem_statement}

We consider %will be interested in 
the following two concrete problems (throughout the paper we will use superscripts $^{(1)}$ or $^{(2)}$ on quantities referring to Problems \ref{problem.1} or \ref{problem.2}, respectively): 
\begin{problem}[Transformed vector amplitudes]
\label{problem.1}
Given access to state preparation unitaries $U_{\phi}$ and $U_{\psi}$ such that $U_{\psi}\ket{0}=\ket{\psi}$, $U_{\phi}\ket{0}=\ket{\phi}$, a Hermitean matrix $A$, and a real-valued function $f$, obtain an estimate of %$\epsilon$-precise estimate of 
% \begin{center}
% obtain an $\epsilon$-precise estimate of\quad $\frac{1}{q}\mel{\phi}{f(A)}{\psi}$
% \end{center}
\begin{equation}\label{eq:problem1}
%\text{obtain an }\epsilon\text{-precise estimate of}\quad
z^{(1)} = %\frac{1}{q}
\mel{\phi}{f(A)}{\psi}
\end{equation}
to additive precision $\epsilon$ with failure probability at most $\delta$.

\end{problem} 
%\GC{briefly illustrate why this is interesting and refer to the Section below where they appear, especially for Problem \ref{problem.1} (suggestion after NYU question)}
\noindent This class of problems is relevant for estimating the overlap between a linearly transformed state and another state of interest. This is the case, e.g., in linear system solving, where one is interested in the $i$-th computational basis component of a quantum state of the form $A^{-1}\ket{\vv{b}}$ encoding the solution to the linear system (see Sec. \sect{linearsystem} for details). % (i.e., $\ket{\psi}=\ket{\vv{b}}$ and $\ket{\phi}=\ket{\vv{x}}$). 
The unitary $U_\phi$ preparing the computational-basis state $\ket{i}$ in that case is remarkably simple, given by a sequence of bit flips. 

\begin{problem}[Transformed observable expectation values]
\label{problem.2}
Given access to a state preparation $\varrho$, a Hermitian matrix $A$, an observable $O$, and a real-valued function $f$, obtain an estimate of %$\epsilon$-precise estimate of
% \begin{center}
% obtain an $\epsilon$-precise estimate of\quad $\frac{1}{q^{2}}\Tr\big[O\,f(A)\ketbra{\psi}{\psi} f^{\dagger}(A)\big]$
% \end{center}
\begin{equation}\label{eq:problem2}
%\text{obtain an }\epsilon\text{-precise estimate of}\quad 
z^{(2)} = %\frac{1}{q^{2}}
\Tr[O\,f(A)\,\varrho\,f(A)^{\dagger}]
\end{equation}
to additive precision $\epsilon$ with failure probability at most $\delta$. 
\end{problem} 
\noindent This is of relevance, e.g., when $A=H$ is a Hamiltonian, to estimate the partition function %and observables in a thermal state 
corresponding to $H$, as discussed below in Sec. \sect{partitionfunction}. % and \sect{thermalobservables}. 

% Before proceeding, let us introduce the quantities \GC{another option would be to remove this from here and introduce $\Tilde{z}^{(1,2)}$ inline in Lemma \ref{main_lemma}}
% \begin{subequations}\label{eq:ztilde}
%     \begin{align}
%         \Tilde{z}^{(1)} &= \frac{1}{q}\mel{\phi}{\Tilde{f}(A)}{\psi} \,, \label{eq:z1tilde}\\
%         \Tilde{z}^{(2)} &= \frac{1}{q^{2}}\Tr[O\,\Tilde{f}(A)\ketbra{\psi}{\psi} \Tilde{f}(A)^{\dagger}]\,, \label{eq:z2tilde}
%     \end{align}
% \end{subequations}
% which will play an important role in the following. In App\la{.} \sect{main_lemma_proof} we show that $\Tilde{z}^{(P)}$ ($P=1,2$) is a $\nu^{(P)}$-approximation of $z^{(P)}$ with the precise $\nu^{(P)}$ given in terms of $q$, the function approximation error $\nu$, and properties of $f$ and $O$.% given in terms of properties of the function $f$ and operator $\mathcal{O}$. \GC{I dont think its worth showing the formulas here}

We present randomized hybrid classical-quantum algorithms for these problems using Chebyshev-polynomial approximations of $f$ and coherent access to a block-encoding of $A$. Similar problems have been addressed in \cite{wang2023qubitefficient} but using Fourier approximations and randomizing also over a classical description of $A$ in the Pauli basis. 
%randomized approach based on a Fourier approximation of $f$ under the assumption of \lq\lq Pauli access\rq\rq~to the matrix $A$, i.e., classical access to the coefficients %$\alpha_\ell$
%of the decomposition of $A$ into a linear combination of Pauli strings. 
%The resulting algorithms have runtimes governed by the total Pauli weight $\norm{\alpha}:=\sum_\ell |\alpha_\ell|$. 




\subsection{Randomized semi-quantum matrix processing} %framework}
\label{sec:framework}

Our framework is based on the Chebyshev approximation $\Tilde{f}$ of the function $f$ and a modified Hadamard test involving the qubitized block-encoding oracle $U_A$. 
The idea is to statistically simulate the coherent QSP algorithm using a hybrid classical/quantum procedure based on randomly choosing $j\in[k+1]$ according to its importance for Eq. \eqref{eq:ftilde} and then running a Hadamard test involving the block encoding $U_A^j$ of $T_j(A)$.
Pseudo-codes for the algorithms are presented in Fig. \fig{main}.~{\bf a)} and \fig{main}.~{\bf b)} for Problems \ref{problem.1} and \ref{problem.2}, respectively. 
In both cases, the Hadamard test is the only quantum sub-routine. %; it is also the only part of the algorithm that changes between Problems \ref{problem.1} and \ref{problem.2}.
The total number of statistical runs will be $\frac{2}{P}\,S^{(P)}$, with $P=1$ or $2$, where $S^{(P)}$ will be given in Eqs. \eqref{eq:SP} below. 
The factor $\frac{2}{P}$ is a subtle difference between Algorithms \ref{problem.1} and \ref{problem.2} coming from the fact that the target quantity is a complex-valued amplitude in the former case, while in the latter it is a real number. This implies that two different types of Hadamard tests %(one for the real and one for the imaginary part) 
(each with $S^{(1)}$ shots) are needed to estimate the real and imaginary parts of $z^{(1)}$, while $z^{(2)}$ requires a single one.
% It is important to note that Problem \ref{problem.1} requires two different Hadamard tests, %(one for the real and one for the imaginary part) 
% each of which with $S^{(1)}$ shots, since the quantity of interest $z^{(1)}$ is a complex-valued amplitude, while Problem \ref{problem.2} uses $S^{(2)}$ shots of the same test since $z^{(2)}$ is real-valued.
%The empirical mean $\Tilde{z}^{(P)}$ is our statistical estimator of the quantity of interest $z^{(P)}$. 
%\sout{that consists of importance-sampling the order $j\in[k]$ of a Chebyshev polynomial}}   \la{$T_j(A)$ }, %(according to its corresponding coefficient in the Chebyshev series approximation of $f$), 
%running a Hadamard test \la{\sout{corresponding to this particular $T_j(A)$} involving the corresponding block encoding $U_A^j$ of $T_j(A)$}, % to estimate $\mel{\phi}{T_j(A)}{\psi}$ or $\Tr[O\,T_j(A)\,\varrho\, T_l(A)]$ for Problems \ref{problem.1} or \ref{problem.2}, respectively, 
%and averaging \la{\sout{out}} the results \la{\sout{of} over} multiple samples in a classical post-processing stage. 
% The basic intuition is to statistically simulate a Hadamard test \LA{it is not the Hadamard test what is statistically simulated. On the contrary, the Hadamard test is the only actual quantum sub-routine of the algorithm (this should be explicitly mentioned). What is statistically simulated is the entire QSP algorithm.} which outputs a Chebyshev approximation to the quantity of interest, namely $z^{(1)}$ for Problem \ref{problem.1} or $z^{(2)}$ for Problem \ref{problem.2}. This is possible provided one is able to design and efficiently sample from a quantum circuit \LA{Rephrase, you don't sample from a circuit} %-- the modified Hadamard test -- 
% whose expected measurement outcomes are $\mel{\phi}{T_j(A)}{\psi}$ or $\Tr[O\,T_j(A)\ketbra{\psi}{\psi}\, T_l(A)]$, respectively. By (classically) choosing the order \la{$j\in[k]$} of the Chebyshev \la{polynomials\sout{modes from the set $\big\{0,\ldots,k\big\}$ with a properly weighted probability distribution $p_j$ so as to generate the correct linear combination of modes}} \LA{Too vague and also hard to follow. Go to the point. For instance "importance-sampling the order of the Chebyshev polynomials according to their corresponding coefficients in the expansion, i.e. choosing $j\in[k]$ with probability $p_{j}=|a_{j}|/\norm{\vv{a}}_1$..."}, this combined classical and quantum sampling procedure yields an estimator of \eq{problem1} (similarly \eq{problem2}). \LA{Rephrase, combined classical and quantum sampling procedure is not precise. Better talk about random sampling as pre-processing and then run modified Hadamard tests, and finally average as post-processing... or something like that}

% Before delving into its inner workings, we first provide a visual description of the algorithm in Figure \fig{main}. % while the technical details and proofs are deferred to the methods Sec\la{.} \ref{methods}. 
% We note that for both Problems \ref{problem.1} and \ref{problem.2} the structure of the algorithm is morally the same, the only difference being the architecture of the quantum circuit used for the quantum sampling subroutine. For that reason, the two are described in parallel in the same Figure \fig{main}. 

% Now let us move to the technical description of our algorithm. The algorithm consists of two basic steps to be repeated a number of times to be discussed in Corollary \ref{main_corollary} below: 

%\la{\sout{A pseudocode description of each algorithm is presented in Figure \fig{main}. We note that for both Problems \ref{problem.1} and \ref{problem.2} the structure of the algorithm is the same, the only difference being the form of the quantum circuit used for the quantum subroutine. For that reason, the two are described in parallel in the same Figure and in this section. T}}
%The algorithm consists of two basic steps -- one classical and one quantum -- to be repeated a number of times that will be discussed in Lemma \ref{main_lemma2} below, followed by classical post-processing to average out the results. 
More technically, the procedure goes as follows. First, for every $\alpha\in[\frac{2}{P}\,S^{(P)}]$ run the following two steps:

\begin{enumerate}[wide, labelwidth=!, labelindent=10pt]%[leftmargin=*]
\item[$i)$]{Classical subroutine: sample a Chebyshev polynomial degree $j_\alpha\in[k+1]$ (and also $l_\alpha$ for $P=2$) from a probability distribution weighted by the coefficients $\vv{a}$ of $\Tilde{f}$, defined by
\begin{align}\label{eq:pj}
p({j})=\frac{|a_{j}|_{\phantom{1}}}{\norm{\vv{a}}_1},\quad \text{ for all } j\in[k+1]\,.
\end{align}
This has classical runtime ${\tilde{\mathcal{O}}}(k)$.}
\item[$ii)$]{Quantum subroutine: if $P=1$, run the Hadamard test in Fig. \fig{main} {\bf a)} %\AD{Maybe define $z^{(1)}_{\alpha}=\norm{\vv{a}}_{1}\,\sgn(a_{j_\alpha})\,(b^{(1)}_{r,j_{\alpha}}+ib^{(1)}_{i,j_{\alpha}})$} 
with $B_\alpha=\mathds{1}$ for $\alpha\le S^{(1)}$ or $B_\alpha=S^\dagger:=\ketbra{0}-i\ketbra{1}$ for $\alpha>S^{(1)}$ %\GC{could even write $B_\alpha=\frac{1-(-1)^\alpha}{2}\mathds{1}+\frac{1+(-1)^\alpha}{2}S^\dagger$} 
and use the resulting random bit $b^{(1)}_{\alpha}\in\{-1,1\}$ to record a sample of the variable
\begin{equation}\label{eq:z1tilde}
\Tilde{z}^{(1)}_\alpha:= \norm{\vv{a}}_{1}\,\sgn(a_{j_\alpha})\,b^{(1)}_{\alpha}\,.
\end{equation}
If $P=2$, in turn, run the test in Fig. \fig{main} {\bf b)} to get as outcomes a random bit $b^{(2)}_{\alpha}\in\{-1,1\}$ and a random number $\omega_{\alpha}\in\{o_m\}_{m\in [D]}$ where $o_m$ is the $m$-th eigenvalue of $O$, and use this to record a sample of 
\begin{equation}\label{eq:z2tilde}
\Tilde{z}^{(2)}_\alpha:= \norm{\vv{a}}_{1}^2\,\sgn(a_{j_\alpha})\sgn(a_{l_\alpha})\,b^{(2)}_{\alpha}\,\omega^{\phantom{(2)}}_{\alpha}.
\end{equation}
}
\end{enumerate}
Then, in a final classical step, obtain the desired estimate $\Tilde{z}^{(P)}$ by computing the empirical mean over all the recorded samples as follows
\begin{subequations}
\label{eq:ztildeP}
\begin{align}
    \Tilde{z}^{(1)} &= \frac{1}{S^{(1)}}\sum_{\alpha=1}^{S^{(1)}} \left(\Tilde{z}^{(1)}_{\alpha}+i\,\Tilde{z}^{(1)}_{\alpha+S^{(1)}}\right),\label{eq:ztildeP1}\\
    \Tilde{z}^{(2)} &= \frac{1}{S^{(2)}}\sum_{\alpha=1}^{S^{(2)}} \Tilde{z}^{(2)}_{\alpha}\,.\label{eq:ztildeP2}
\end{align}
\end{subequations}

The following two theorems respectively prove the correctness of the estimator and establish the complexity of the algorithm. 
A simple but crucial auxiliary result for the correctness is the observation that the Hadamard test statistics (i.e. the expectation value of $b^{(P)}_{\alpha}$)  depends only on the correct block of $U_A^j$, removing the need of post-selection.  
With this, in App. \sect{main_lemma_proof}, we prove the following. 

\begin{theorem}[Correctness of the estimator]\label{main_lemma}
The empirical means $\Tilde{z}^{(1)}$ and $\Tilde{z}^{(2)}$
are unbiased estimators of  $\mel{\phi}{\Tilde{f}(A)}{\psi}$ and $\Tr[O\,\Tilde{f}(A)\,\varrho\,\Tilde{f}(A)^{\dagger}]$, respectively.     
\end{theorem}
\noindent Importantly, since $\Tilde{f}$ is a $\nu$-approximation to $f$, the obtained $\Tilde{z}^{(P)}$ are actually biased estimators of the ultimate quantities of interest $z^{(P)}$ in Eqs. \eq{problem1} and \eq{problem2}.
Such biases are always present in quantum algorithms based on approximate matrix functions, including the fully-coherent schemes for QSP \cite{LowChuang2017,LowChuangQuantum2019} and QSVT \cite{Gilyen2019,ChuangGrandUnification}.
Nevertheless, they can be made arbitrarily small in a tunable manner by increasing the truncation order $k$ in Eq. \eqref{eq:ftilde}.

Here, it is convenient to set $k$ so that $\nu^{(P)}\leq\epsilon/2$, where $\nu^{(1)}:=\nu$ and $\nu^{(2)}:=\nu\,(2\,\norm{f(A)}\,\norm{O}+\nu)$. 
This limits the approximation error in Eqs. \eq{problem1} or \eq{problem2} to at most $\epsilon/2$. Demanding, in addition, the statistical error to be also $\epsilon/2$, leads to (see App. \sect{main_lemma2_proof}) the following end-to-end sample and oracle-query complexities for the algorithm. 

\begin{theorem}[Complexity of the estimation]\label{main_lemma2}
Let $\epsilon>0$ and $\delta>0$ be respectively the tolerated additive error and failure probability; let $\vv{a}$ %\Discuss{\la{and $\nu\leq \epsilon/2$}} 
be the vector of coefficients in Eq. \eq{ftilde} 
and $\nu^{(P)}\leq\epsilon/2$ %respectively the vector of %Chebyshev
the error in $z^{(P)}$ by approximating $f$ with $\Tilde{f}$. Then, if the number of samples is at least
\begin{subequations}\label{eq:SP}
\begin{numcases}{S^{(P)} =}
        \frac{16\norm{\vv{a}}^{2}_{1}}{\epsilon^{2}}\log\frac{4}{\delta}\,, & \text{for } P=1, \label{eq:SP1}\\
        \frac{8\,\norm{O}^2\norm{\vv{a}}^{4}_{1}}{\epsilon^{2}}\log\frac{2}{\delta}\,, & \text{for } P=2, \label{eq:SP2}
\end{numcases}
\end{subequations}
Eqs. \eqref{eq:ztildeP} %\eqref{eq:ztildeP1} and \eqref{eq:ztildeP2} 
give an $\epsilon$-precise estimate of $z^{(P)}$ with confidence $1-\delta$. 
Moreover, the total expected query complexity is $Q^{(P)}:= 2\,\mathbb{E}[j]\,S^{(P)}$, where $\mathbb{E}[j]:=\sum_{j=0}^k j\,p(j)$. 
\end{theorem}

A remarkable consequence of this theorem is that the expected number of queries per statistical run is $P\times\mathbb{E}[j]$. % instead of $P\times k$ as in fully-quantum schemes. By fully-quantum here we mean the case where the full polynomial \eq{ftilde} is implemented using QSVT and a Hadamard test similar to the one in Fig. \fig{main} is used to avoid post-selection, hence keeping both schemes on equal footing.
Instead, if we used standard QSVT (together with a similar Hadamard test to avoid post-selection) each statistical run would take $P\times k$ queries (and an extra ancillary qubit coherently controlling everything else would be required).
As shown in Fig. \fig{main}.~{\bf c)}, $\mathbb{E}[j]$ can be significantly smaller than $k$ in practice. 
In fact, in Sec. \sect{usecases} we prove significant scaling advantages of $\mathbb{E}[j]$ over $k$.
These query-complexity advantages translate directly into reductions circuit depth and, hence, also in noise sensitivity (see next sub-section).
As for sample complexity, the statistical overhead of our semi-quantum algorithms scales with $\norm{\vv{a}}_{1}$, while that of fully-quantum ones would have a similar scaling with $\norm{f(A)}$, due to the required normalization for block encoding. 
Interestingly, in all the use cases analyzed, $\norm{\vv{a}}_1$ and $\norm{f(A)}$ differ at most by a logarithmic factor%, which results only in minor extra total runtimes
. 
Finally, another appealing feature is that our approach relaxes the need to compute the QSP/QSVT angles, which is currently tackled with an extra classical pre-processing stage of  runtime $\mathcal{O}\big(\text{poly}(k)\big)$ \cite{LowChuang2017,LowChuangQuantum2019,Gilyen2019,ChuangGrandUnification}.

We emphasize that here we have assumed Hermitean $A$ for the sake of clarity, but a straightforward extension of our randomized scheme from QSP to QSVT (see App. \sect{QSVT})
gives the generalization to generic $A$. Moreover, in Lemma \ref{lemma_hadamardsecondkind} in App. \ref{sec:Hadamard_test}, we also extend the construction to Chebyshev polynomials of the second kind. This is useful for ground-state energy estimation, in Sec. \sect{groundstate}. 


















%- Error robustness -----------------------------------------------------------------------------------------------------------
\subsection{Intrinsic noise-sensitivity reduction} 
\label{sec:error}

Here we study how the  reduction in query complexity per run from $k$ to the average value $\mathbb{E}[j]$ %$P\times \mathbb{E}[j]$ 
translates into sensitivity to experimental noise.
The aim is to make a quantitative but general comparison between our randomized semi-quantum approach and fully-quantum schemes, remaining agnostic to the specific choice of operator function, circuit compilation, or physical platform.
To this end, we consider two toy models of decoherence that allow one to allocate one unit of noise per oracle query. 
%These are faulty quantum oracles consisting of either the ideal iterate  followed by a globally depolarizing channel $\Lambda$ of noise strength $p$ or an imperfect iterate with coherent errors given by an $\epsilon$-approximate block encoding of the input matrix $A$.
For simplicity, we focus on Problem \ref{problem.1}, but the extension to Problem \ref{problem.2} is straightforward. 
%The aim of this section is to put in quantitative terms the intuition that the detrimental effect of noise gets smaller as the average circuit depth decreases. 
%As such, we study toy models of noise so as to be able to make general statements without entering into details of the specifics of the operator function being implemented, circuit compilations, or physical platforms being used.

Our first error model consists of a faulty quantum oracle given by the ideal oracle followed by a globally depolarizing channel $\Lambda$ of noise strength $p$, defined by \cite{Aolita15review}
\begin{equation}
\label{eq:depol}
\Lambda[\varrho]:=(1-p)\,\varrho + p\,\frac{\mathds{1}}{D_\text{tot}}.
\end{equation}
Here, $\varrho$ is the joint state of the total Hilbert space in Fig. \fig{main} {\bf a)} (system register, oracle ancilla, and Hadamard test ancilla) and $D_\text{tot}$ its dimension.
In App. \sect{error_analysis_proof} we prove:
\begin{theorem}[Average noise sensitivity]\label{thm:error_robust}
Let $\Tilde{z}^{(1)}$ be the ideal estimator \eq{ztildeP1} and $\Tilde{z}^{(1),\Lambda}$ 
its noisy version with $\Lambda$ acting after each oracle query in Fig. \fig{main} {\bf a)}. Then 
\begin{equation}
\label{eq:noise_robustness}
\abs{\mathbb{E}\big[\Tilde{z}^{(1)}\big]-\mathbb{E}\big[\Tilde{z}^{(1),\Lambda}\big]} \le
p\,E_\text{sq}\leq\, p\,\norm{\vv{a}}_{1}\,\mathbb{E}[j]\,,
\end{equation}
with $E_\text{sq} := \abs{\sum^{k}_{j=0}j\,a_{j}\mel**{\phi}{T_j(A)}{\psi}}$.
\end{theorem}
\noindent Our second model is coherent errors that make the quantum oracle no longer the exact block encoding $U_A$ of $A$ but only a $\varepsilon$-approximate block encoding (a unitary with operator-norm distance $\varepsilon$ from $U_A$).
In App. \sect{error_analysis_proof}, we show that Eq. \eq{noise_robustness} holds also there  with  $p$ replaced by $\varepsilon$. 


It is instructive to compare Eq. \eq{noise_robustness} with the corresponding inaccuracy for the case of fully-quantum schemes. 
A possible scenario for that comparison is that where the standard QSVT is equipped with a Hadamard test similar to the ones in Fig. \fig{main} so as to also circumvent the need for post-selection.
However, that would require one ancillary qubit for the QSP pulses, controlling the oracles, plus another ancillary qubit for the Hadamard test, controlling everything else.
As a consequence, the underlying gate complexity per oracle query would be significantly higher than for our schemes (with single-qubit gates becoming two-qubit gates, two-qubit gates becoming Toffoli gates, etc) and the resulting noise strength  $p_\text{fq}$ much higher.
The left-hand side of Eq. \eq{noise_robustness} would then (see App. \sect{error_analysis_proof}) be upper-bounded by $p_\text{fq}\,E_\text{fq}$, with $E_\text{fq} = k\,|\mel{\phi}{\Tilde{f}(A)}{\psi}|$, where $p_\text{fq}\gg p$ and $k>\mathbb{E}[j]$.
%Here there are multiple possibilities, depending on how one chooses to deal with the post-selection probabilities of the algorithms. 
%The first option is a fully-quantum algorithm that employs Hadamard tests similar to the ones in Fig. \fig{main} in order to avoid the need for post-selection. 
%In this case, a fair comparison requires a much larger depolarizing noise rate $p_\text{fq}$ given that the entire QSP circuit needs to be controlled by an extra ancillary qubit in the Hadamard test (e.g., single-qubit gates become two-qubit gates, two-qubit gates become Toffoli's, etc.). 
%The bound $p\,E_\text{sq}$ in \eq{noise_robustness} then gets replaced (see App. \sect{error_analysis_proof}) by $p_\text{fq}\,E_\text{fq}$ with $E_\text{fq} = k\,|\mel{\phi}{\Tilde{f}(A)}{\psi}|$ and $p_\text{fq}\gg p$. 
%Notice that, besides the different noise rate, this is proportional to the degree $k$ of $\Tilde{f}$ instead of the average query complexity $\mathbb{E}[j]$, which in practice can be significantly smaller than $k$ as discussed in Sec. \sect{framework} and illustrated in Fig. \fig{main} {\bf c)}. 

%One can proceed in a similar way (see App. \sect{error_analysis_proof}) to obtain an error bound similar to \eq{noise_robustness} with $E_\text{sq}$ replaced by $E_\text{fq}:=?$. 

Another natural scenario for comparison is that where the fully-quantum algorithm does not leverage a Hadamard test but implements post-selection measurements on the oracle ancilla, in a repeat-until-success strategy. This comparison applies only to Problem \ref{problem.2}, since one cannot directly measure the complex amplitudes for Problem \ref{problem.1}. The advantage though is that the circuits are now directly comparable, because the gate complexities per oracle query are essentially the same (the fully-quantum scheme has extra QSP pulses, but these are single-qubit gates whose error contribution is low).
Hence, similar error rates to $p$ are expected here, so that the left-hand side of Eq. \eq{noise_robustness} would be upper bounded by $\mathcal{O}(k\, p)$. This is already worse than Eq. \eq{noise_robustness} because $k>\mathbb{E}[j]$, as already discussed.
%A second option to take into account is that of a fully-quantum algorithm based on post-selection using a \lq\lq repeat-until-success\rq\rq~strategy (this is only applicable to Problem \ref{problem.2}, though, since one cannot directly measure the complex amplitudes for Problem \ref{problem.1}). 
%The reason why this scenario gives a meaningful comparison is because the type of circuit is the same and, therefore, similar noise rates $p$ are expected. 
%The total error would then be $\mathcal{O}(k\, p)$, which is comparable to \eq{noise_robustness}. 
However, crucially, the biggest disadvantage of the fully-quantum scheme manifests itself  in the sample complexity (and consequently the total runtime),
which here gains a (potentially exponentially) large factor inversely proportional to post-selection probability. Moreover, with post-selection, one additionally needs to estimate normalizing constants with an independent set of experimental runs. In contrast, our method does not suffer from this issue, as it directly gives the estimates in Eqs. \eq{problem1} or \eq{problem2} regardless of state normalization (see Sec. \sect{usecases}).

Finally, a third possibility could be to combine the fully-quantum scheme with quantum amplitude amplification to manage the post-selection. This would quadratically improve the dependence on the post-selection probability.
However, it would then be the circuit depth who would gain a factor inversely proportional to the square root of the post-selection probability.
Unfortunately, this is far out of reach of early-fault tolerant hardware. 

%\la{iv) To end up with, in a brief couple-of-sentence paragraph, we should quickly introduce the second class of faulty iterates and directly mention that it's all the same with $\epsilon$ playing the role of $p$ (another possibility would be to do it right after Lemma 5). Then, the paragraph should conclude that the analysis presented indicates a strong advantage in terms of experimental feasibility of our semi-quantum scheme over fully-quantum ones.}


%This gives our framework an intrinsic resilience to noise and indicates a strong advantage in terms of experimental feasibility of our semi-quantum scheme over fully-quantum schemes.




\subsection{End-user applications}
\label{sec:usecases}

Here we illustrate the usefulness of our framework with four use cases of practical relevance: partition function estimation (both for classical or general Hamiltonians), linear system solving, and ground-state energy estimation. These correspond to $f(x)=x^t$, $e^{-\beta x}$, $x^{-1}$, and $\Theta(x)$, respectively. %The quantities characterizing their $\nu$-approximate Chebyshev series are shown in Table \tab{chebyshev_data}, and 
%The complexity and runtime  of the corresponding algorithms are summarized in Table \tab{main_table}.
The end-to-end complexities for each case are summarized in Table \tab{main_table}. 
%\la{These complexities refer to the end-user mindset where one is  interested not in preparing a quantum state but actually in extracting a relevant quantity from a vector component or estimating an expectation value, taking into account  all corresponding proportionality constants or observable norms.}
%\GC{do we really need this?}


\begin{table*}[ht]
    \centering
    \begin{tabular}{||c||c|c|c|c||}
     \hline
     \rule{0pt}{3ex}
     \rule[-1.5ex]{0pt}{0pt}
     {\bf Problem} & {\bf App.} & {\bf Maximal query depth} 
     & {\bf Expected query depth} & {\bf Total expected runtime} \\
     \hline
     \hline
     \rule{0pt}{4ex}
     \rule[-1.5ex]{0pt}{0pt}
     Part. funct. (MCMC) & \sect{explicit_examples_monomials} & $\sqrt{\frac{2}{\Delta}}\log\Big(\frac{12\,Z_\beta\, e^{\beta E_{\vv{y}}}}{\epsrel}\Big)$%$\mathcal{O}\Big(e^{2\beta} Z_\beta^2\,\frac{\log(2/\delta)}{\epsrel^2} \Big)$  
     & $\sqrt{\frac{2}{\pi\,\Delta}\log\Big(\frac{12\,Z_\beta\, e^{\beta E_{\vv{y}}}}{\epsrel}\Big)}$ %\Big(\beta E_{\vv{y}}+\log\la{\big(}\frac{12\,Z_\beta}{\epsrel}\la{\big)}\Big)}
     & $\mathcal{O}\left(\frac{e^{2\beta E_{\vv{y}}} Z_\beta^2}{\sqrt{\Delta}}\sqrt{\log\Big(\frac{Z_\beta e^{\beta E_{\vv{y}}}}{\epsrel}\Big)}\frac{\log(1/\delta)}{\epsrel^2}\right)$ \\
     \hline
     \rule{0pt}{4ex}
     \rule[-2ex]{0pt}{0pt}
     Part. funct. (QITE) & \sect{explicit_examples_qite} & $\mathcal{O}\Big(\sqrt{\beta}\log\big(\frac{D\, e^{\beta}}{Z_\beta\,\epsrel}\big)\Big)$ %$\mathcal{O}\Big(\frac{D^2 e^{2\beta}}{Z_{\beta}^2}\frac{\log(2/\delta)}{\epsrel^2}\Big)$ 
     & $\mathcal{O}\left(\sqrt{\beta}\right)$ & ${\mathcal{O}}\left(\frac{D^2 \sqrt{\beta}\,e^{2\beta}}{Z_{\beta}^2}\frac{\log(1/\delta)}{\epsrel^2}\right)$ \\
     %\hline
     %\rule{0pt}{4ex}
     %\rule[-2ex]{0pt}{0pt}     
     %Thermal state observables & $\mathcal{O}\left(\cdots\right)$ & $\mathcal{O}\left(\cdots\right)$ & $\mathcal{O}\left(\cdots\right)$ \\
     \hline
     \rule{0pt}{4ex}
     \rule[-2ex]{0pt}{0pt}     
     QLSS: $\bra{i}A^{-1}\ket{\vv{b}}$ & \sect{explicit_examples_inverse} & $\mathcal{O}\left(\kappa\log\big(\frac{\kappa}{\epsilon}\big)\right)$ %$\Tilde{\mathcal{O}}\left(\norm{A^{-1}}^{2}\frac{\log(2/\delta)}{\epsilon^{2}}\right)$ 
     & $\mathcal{O}\left(\kappa\,\sqrt{\log\big(\frac{\kappa}{\epsilon}\big)}\right)$ & $\mathcal{O}\left(\kappa^3\log^{5/2}\!\big(\frac{\kappa}{\epsilon}\big)\frac{\log(1/\delta)}{\epsilon^2}\right)$ \\
     \hline
     \rule{0pt}{4ex}
     \rule[-2ex]{0pt}{0pt} 
     QLSS: $\bra{\vv{b}}A^{-1}OA^{-1}\ket{\vv{b}}$ & \sect{explicit_examples_inverse} & $\mathcal{O}\left(\kappa\log\Big(\frac{\kappa^2\norm{O}}{\epsilon}\Big)\right)$ % $\Tilde{\mathcal{O}}\left(\cdots\right)$ 
     & $\mathcal{O}\left(\kappa\,\sqrt{\log\Big(\frac{\kappa^2\norm{O}}{\epsilon}\Big)}\right)$ & $\mathcal{O}\left(\kappa^5\norm{O}^2\log^{9/2}\!\Big(\frac{\kappa^2\norm{O}}{\epsilon}\Big)\frac{\log(1/\delta)}{\epsilon^2}\right)$ \\
     \hline
     \rule{0pt}{4ex}
     \rule[-2ex]{0pt}{0pt}      
         Ground-state energy & \sect{explicit_examples_step} & $\mathcal{O}\left(\frac{1}{\xi}\log\big(\frac{1}{\eta}\big)\right)$ %$\Tilde{\mathcal{O}}\left(\frac{\log(2/\delta)}{\eta^{2}}\right)$ 
         & $\quad\mathcal{O}\left(\frac{1}{\xi}\frac{\sqrt{\log(1/\eta)}}{\log((1/\xi)\log(1/\eta))}\right)$
         %$\quad\mathcal{O}\left(\frac{1}{\xi}{\sqrt{\log\big(\frac{1}{\eta}\big)}}{\Big/\!\log\big(\frac{1}{\xi}\log\big(\frac{1}{\eta}\big)\big)}\right)$  
         & $%\Tilde{\mathcal{O}}\left(\frac{\log(1/\delta)}{\eta^2\xi}\right) 
    \mathcal{O}\left(\frac{1}{\eta^{2}\xi}\sqrt{\log\big(\frac{1}{\eta}\big)}\log\big(\frac{1}{\xi}%\log\big(\frac{1}{\eta}\big)
    \big)\log\big(\frac{1}{\delta}%\log(\frac{1}{\xi})
    \big)\right)
    $ \\   
     \hline
    \end{tabular}
    \caption{%\LA{The method for partition function estimation with MCMC currently holds only for classical Hamiltonians (check however Wojcian-Temme paper). For these, the runtime for exact  partition-function evaluation is $D$. Do we ever expect our total runtime to beat that? This, together with Allan's comments make me think that we should remove this use case. I think  the actual killer use-case for  MCMC  is estimation of local observables, but explicitly exploiting the locality of the observables! See my suggestions in the text} 
    %Resulting complexities of our randomized semi-quantum algorithms applied to examples
    {\bf Complexities of our algorithms for end-user applications}. 
    The first column indicates the specific use case (see Sec. \sect{usecases}).
    The second one indicates the appendix with the corresponding derivations. 
    The third column shows the maximal query complexity per run $k$. Chebyshev-based fully-quantum matrix processing (using the same Hadamard tests as us) would require the same query depth but in {\it every} run.  
    The fourth column displays the average query complexity per run $P\,\mathbb{E}[j]$, with $P=1$ for Alg. 1 and $P=2$ for Alg. 2. 
    We notice that in the last row we use $\xi$ for the additive error in the ground state energy (coming from the $\mathcal{O}\big(\log\big(\frac{1}{\xi}\big)\big)$ steps in the binary search) to distinguish from the $\epsilon$ (which here is $\mathcal{O}(\eta)$) reserved for the estimation error in the quantities $z^{(P)}$. 
    %This is to be compared to the maximal query depth $k$ %(i.e., the degree of the polynomial $\Tilde{f}$) 
    %appearing in previous efforts based on fully randomized schemes. 
    As can be seen, in all use cases the average query depth features a significantly better scaling than $k$ on {non-minor} parameters. 
    This is an interesting speed-up specific to the randomization over the Chebyshev expansion.
    Finally, the fourth column shows the expected runtime, given by $Q^{(P)}$ in Theorem \ref{main_lemma2}, namely the average query depth times the sample complexity $S^{(P)}$. 
    Here, $S^{(P)}$ scales with $\norm{\vv{a}}_1$ exactly as it would with $\norm{f(A)}$ had we used the fully-quantum algorithm. Interestingly, $\norm{\vv{a}}_1$ and  $\norm{f(A)}$  happen to be of the same order for the use cases studied, except for small logarithmic corrections for QLSSs and ground-state energy estimation (see Table \tab{chebyshev_data} in App. \sect{explicit_examples} for details). 
    All in all, the total expected runtimes in the fourth column are either similar or slightly superior to the corresponding runtimes of Chebyshev-based fully-quantum approaches. 
    Remarkably, this is achieved in tandem with important advantages in terms of quantum hardware (see, e.g., Sec. \sect{error}). 
    }
    \label{tab:main_table}
\end{table*}





%\subsubsection{\Danger{Hamiltonian simulation}} \label{sec:hamiltoniansimulation}

% \Danger{Possible new section, but we have not agreed yet on this -- think if it makes good sense to write that, but in the meantime focus on everything else more urgent}

% \LA{Yeah, not urgent.  In fact, the main candidate  for real-time Hamiltonian simulation in early-stage quantum processors may be product-formula appraoches, which require no ancillas and require anyways no post-selection. I'm a bit undecided about how useful our scheme could be for that use case... let's chat.} 



\subsubsection{{Relative-error p}artition function estimation} \label{sec:partitionfunction}

Partition function estimation is a quintessential hard computational problem, with applications {ranging from} statistical physics {to generative} machine learning, {as in Markov
random fields \cite{Ma_Peng_Wang}, Boltzmann machines \cite{KRAUSE2020103195}, and even the celebrated transformer architecture \cite{shim2022probabilistic} from large language models.}
%and computer science in general. \GC{add refs} 
Partition functions also appear naturally in other problems of practical relevance, such as constraint satisfaction problems \cite{BULATOV2005148}.

The partition function of a Hamiltonian $H$ at inverse temperature $\beta$ is defined as
\begin{equation}\label{eq:partitionfunction}
Z_{\beta}=\Tr\left[ e^{-\beta H}\right]\,.
\end{equation}
One is typically interested in the problem of estimating $Z_\beta$ to relative error $\epsrel$, that is, finding $\Tilde{Z}_\beta$ such that
\begin{align}
   \big|\Tilde{Z}_\beta - Z_\beta\big| \le \epsrel\,Z_\beta\,.
\end{align}
This allows for the estimation of relevant thermodynamic functions, such as the Helmholtz free energy $F=\frac{1}{\beta}\log Z_{\beta}$, to additive precision. The naive classical algorithm based on direct diagonalization runs in time $\mathcal{O}(D^{3})$, where $D=\text{dim}(\mathcal{H}_s)$ is the Hilbert space dimension. Although it can be improved to $\mathcal{O}(D)$ using the kernel polynomial method \cite{RevModPhys.78.275} if $H$ is sparse, one expects no general-case efficient algorithm to be possible due to complexity theory arguments \cite{bravyi2021complexity}. In turn, if the Hamiltonian is classical (diagonal), $Z_{\beta}$ can be obtained exactly in classical runtime $\mathcal{O}(D)$.  General-purpose quantum algorithms (that work for any inverse temperature and any Hamiltonian) have been proposed 
\cite{PhysRevLett.103.220502,chowdhury_computing_2021,PhysRevA.107.012421}. The list includes another algorithm \cite{chowdhury_computing_2021} that, like ours, utilizes the Hadamard test and a block-encoding of the Hamiltonian. 
%We compare their complexity to ours later.   %(see \cite{PhysRevLett.103.220502,Chowdhury_2021,PhysRevA.107.012421} for related works on quantum algorithms for partition function estimation). 
% \begin{equation}
% \Tilde{\mathcal{O}}\left(\left(\frac{\beta e^{\beta}D}{\epsrel Z_{\beta}}\right)^{2}\log(\frac{1}{\delta})\right).
% \end{equation}
%where algorithm-specific parameters, and polylogarithmic factors in $D$, $\beta$ and $1/\epsrel$ are ignored.

In the following, we present two different quantum algorithms for partition function estimation: one for classical Ising models, based on the Markov-Chain Monte-Carlo (MCMC) method, and another for generic non-commuting Hamiltonians, based on quantum imaginary-time evolution (QITE) simulation \cite{Sunetal21,silva_fragmented_2022}.

\paragraph{Partition function estimation via MCMC:}

{Here, we take $H$ as the Hamiltonian of a classical Ising model. As such, spin configurations, denoted by $\ket{\vv{y}}$, are eigenstates of $H$ with corresponding energies $E_{\vv{y}}$. Let us define the {coherent version of the} %"purified" \la{carefull, in quantum info, the term purification has a very specific meaning that is not this one. Better use "coherent version of" instead "purified"} 
Gibbs state $\ket{\sqrt{\bpi}} := Z_\beta^{-1/2}\sum_{\vv{y}}e^{-\beta E_{\vv{y}}/2}\ket{\vv{y}}$.  
 Then, for any $\ket{\vv{y}}$, the partition function satisfies the identity
\begin{align}\label{eq:Zidentity}
    Z_\beta = \frac{e^{-\beta E_{\vv{y}}}}{\mel{\vv{y}}{\Pi_{{\bpi}}}{\vv{y}}} 
\end{align}
with $\Pi_{{\bpi}}:=\ketbra{\sqrt{\bpi}}$. Below we discuss how to use our framework to obtain an estimation of $\mel{\vv{y}}{\Pi_{{\bpi}}}{\vv{y}}$ for a randomly sampled $\ket{\vv{y}}$ and, therefore, approximate the partition function. 

Let $A$ be the discriminant matrix\ \cite{szegedy2004} of a Markov chain having the Gibbs state of $H$ at inverse temperature $\beta$ as its unique stationary state. The Szegedy quantum walk unitary\ \cite{szegedy2004} provides a qubitized block-encoding $U_A$ of $A$ that can be efficiently implemented\ \cite{Lemieux2020efficientquantum}. A useful property of $A$ is that the monomial $A^t$ approaches $\Pi_{\bpi}$  for sufficiently large integer $t$ \cite{camilo_inprep} (the precise statement is given by Lemma\ \ref{lemma_partition_function_power_method} in App.\ \sect{explicit_examples_monomials}). {This} %It 
implies that  $\mel{\vv{y}}{\Pi_{{\bpi}}}{\vv{y}}$ can be estimated using  %our 
Alg. 1  with $f(A)=A^t$ and $\ket{\psi}=\ket{\phi}=\ket{\vv{y}}$. % by approximating the monomial $W^{t}$ by a truncated Chebyshev series. 
 In this case, the state preparation unitaries $U_{\psi}=U_{\phi}$ will be simple bit flips. } 

A $\nu$-approximation $\Tilde{f}(A)$ can be constructed by truncating the Chebyshev representation of $A^t$ to order $k=\sqrt{2\,t\log(2/\nu)}$  \cite{Vishnoi2013}. The %corresponding 1
{$l_1$}-norm of the {corresponding} coefficient vector is $\norm{\vv{a}}_1=1-\nu$.  For this Chebyshev series, the ratio $q^{(1)}/k$ between the average and the maximum %Chebyshev-degree 
query complexities can be shown (see Lemma \ref{lem:av_Q_mon} in App.\ \sect{explicit_examples_monomials}) to be  $%\frac{2/\sqrt{2\pi}}{\sqrt{2\log(2/\nu)}=
{1}/{\sqrt{\pi\,\log(2/\nu)}}%\approx\frac{0.56}{\sqrt{\log(2/\nu)}}
$%, which for $\nu=10^{-3}$ gives roughly \la{0.3}%$30\%$
%, for example. It 
{. This implies} that the more precise the estimation, the larger the advantage of the randomized algorithm in terms of total expected runtime.
{For instance, for $\nu=10^{-2}$, the ratio is roughly equal to 0.25.}

To estimate the partition function up to relative error $\epsrel$, Alg. 1 needs to estimate $\mel{\vv{y}}{\Pi_{{\bpi}}}{\vv{y}}$ with additive error $\epsilon=\frac{ e^{-\beta E_{\vv{y}}}}{2\,Z_\beta}\epsrel$ (see Lemma \ref{lem:estimate_z} in App.\ \sect{explicit_examples_monomials}).
In Lemma \ref{lem:complexity_mon}, in App.\ \sect{explicit_examples_monomials}, we show that the necessary $t$ and $\nu$ required for that yield a  maximum query complexity per run of $k=%\sqrt{\frac{2}{\Delta}}\big(\beta E_{\vv{y}}+\log\la{\big(}\frac{12\,Z_\beta}{\epsrel}\la{\big)}\big)=
\sqrt{\frac{2}{\Delta}}\log(\frac{12\,Z_\beta\, e^{\beta E_{\vv{y}}}}{\epsrel})$ and an average query complexity of $Q^{(1)}=\sqrt{\frac{2}{\pi\,\Delta}\log(\frac{12\,Z_\beta\, e^{\beta E_{\vv{y}}}}{\epsrel})}$.
Moreover, from Theorem \ref{main_lemma2}, the necessary sample complexity is $S^{(1)}=64\,e^{2\beta E_{\vv{y}}}\, Z_\beta^2\,\frac{\log(2/\delta)}{\epsrel^2}$. 
This leads to the total expected runtime in the third column of Table \tab{main_table}.  
%Therefore, the sample complexity from Theorem \ref{main_lemma2} gives $S^{(1)}=64\,e^{2\beta E_{\vv{y}}}\, Z_\beta^2\,\frac{\log(2/\delta)}{\epsrel^2} $. Moreover, a maximum of $k=\sqrt{\frac{2}{\Delta}}\big(\beta E_{\vv{y}}+\log\la{\big(}\frac{12\,Z_\beta}{\epsrel}\la{\big)}\big)\la{=\mathcal{O}(\sqrt{\frac{2}{\Delta}}\log(\frac{Z_\beta\, e^{\beta E_{\vv{y}}}}{\epsrel})}$ queries to the oracle must be made per circuit.  

{Three important observations about the algorithm's complexities are in place. First, the total expected runtime has no explicit dependence on the Hilbert space dimension $D$ and maintains the square-root dependence on $\Delta$ (a Szegedy-like quadratic quantum speed-up \cite{szegedy2004}).
Second, all three complexities in the first row of the  table depend on the product $Z_\beta\, e^{\beta E_{\vv{y}}}=\mathcal{O}\big(e^{\beta (E_{\vv{y}}-E_\text{min})}\big)
$, with $E_\text{min}$ the minimum eigenvalue of $H$, where the scaling holds for large $\beta$.
This  scaling plays more in our favor the lower the energy $E_{\vv{y}}$ of the initial state $\vv{y}$ is.
Hence, by} uniformly sampling a constant number of different bit-strings $\vv{y}$ and picking the lowest energy one, one ensures to start with a convenient initial state.
Third, the quadratic advantage featured by $q^{(1)}$ over $k$ on the logarithmic term is an interesting type of speed-up entirely due to the randomization over the components of the Chebyshev series.

To end up with, the total expected runtime obtained provides a potential quantum advantage over classical estimations in regimes where $\frac{e^{2\beta (E_{\vv{y}}-E_\text{min})}}{\sqrt{\Delta}\,\epsrel^2}<D$. 

\paragraph{Partition function estimation via QITE:} 
Alternatively, the partition function associated with a Hamiltonian $H$ can be estimated by quantum simulation of imaginary time evolution (QITE). $Z_{\beta}$ can be written in terms of the expectation value of the QITE propagator $e^{-\beta H}$ over the maximally mixed state $\varrho_0:=\frac{\mathds{1}}{D}$, that is,
\begin{equation}
Z_{\beta}=D\,\Tr\big[e^{-\beta H}\varrho_0\big]\,.
\end{equation}
Therefore, we can apply our Alg. 2 with $A=H$, $O=D\mathds{1},\varrho=\varrho_0$, and $f(H)=e^{-\beta H/2}$ to estimate $Z_{\beta}$ with relative precision $\epsrel$ and confidence $1-\delta$. The sample complexity is obtained from Eq.\ \eqref{eq:SP2}  as $S^{(2)}=\frac{8\, D^2e^{2\beta}}{\epsrel^2 Z_{\beta}^2}\log\frac{2}{\delta}$, by setting the additive error equal to $Z_\beta\,\epsrel$. This method can be applied to any Hamiltonian (not just classical ones) if  a block-encoding of it is given.

We use the Chebyshev approximation of the exponential function introduced in Ref.\ \cite{Vishnoi2013}, which has a quadratically better asymptotic dependence on $\beta$ than other well-known expansions such as the Jacobi-Anger decomposition \cite{silva_fragmented_2022}.  This expansion was used before to implement the QITE propagator using QSVT coherently \cite{Gilyen2019}.
The resulting truncated Chebyshev series has order $k=\sqrt{2\,\max\left\{\frac{e^2\beta}{2},\log\big(\frac{8D\,e^{\beta}}{Z_\beta\,\epsrel}\big)\right\}\,\log\big(\frac{16 D\,e^{\beta}}{Z_\beta\,\epsrel}\big)}$  and coefficient $l_1$-norm $\norm{\vv{a}}_1\leq e^{\beta/2}$ (see Lemmas \ref{lem:approx_exp} and \ref{lem:1norm_exp} in App.\ \sect{explicit_examples_qite}). 
 Interestingly, the average query depth does not depend on the precision of the estimation but scales as $\mathcal{O}(\sqrt{\beta})$ with a modest constant factor for any $\epsrel$ (see Lemma \ref{lem:query_exp} in App.\ \sect{explicit_examples_qite}). This implies an advantage of $\mathcal{O}\left(\log\big(\frac{D\,e^{\beta}}{Z_\beta\,\epsrel}\big)\right)$ in terms of overall runtime as compared to coherent QSVT, which is again entirely due to our randomization scheme. 
 
 Overall, {this} gives our algorithm a total {expected} runtime of $\mathcal{O}\left(\frac{D^2 \sqrt{\beta}\,e^{2\beta}}{Z_{\beta}^2}\frac{\log(2/\delta)}{\epsrel^2}\right)$. The previous state-of-the-art algorithm from Ref.\ \cite{chowdhury_computing_2021} has runtime $\Tilde{\mathcal{O}}\left(\frac{D^2D_a^2e^{2\beta}\beta^2}{\epsrel^2 Z_{\beta}^2}\log\frac{1}{\delta}\right)$. 
 Compared with that, we get an impressive
  quartic speed-up in $\beta$ together with the entire removal of the dependence on $D_a^2$.
 %and no dependence on the number of ancilla qubits other than through the oracle implementation complexity.
  
 The improvement comes from not estimating each Chebyshev term individually and allowing the ancillas to be pure while only the system is initialized in the maximally mixed state.

Finally, compared to the naive classical algorithm, the expected runtime offers an advantage in scaling with $D$. Moreover, in the regime of small $\beta$ such that  $Z^2_\beta>\mathcal{O}\big(\sqrt{\beta}\,e^{2\beta}\log(1/\delta)/\epsrel^2\big)$, the expected runtime can be even better than that of the kernel method, which scales as $\mathcal{O}(D)$. 

\subsubsection{Quantum linear-system solvers} \label{sec:linearsystem}

Given a matrix $A\in\mathbb{C}^{D}\times \mathbb{C}^{D}$ and a vector $\vv{b}\in\mathbb{C}^{D}$, the task is to find a vector $\vv{x}\in\mathbb{C}^{D}$ such that
\begin{equation}
A\,\vv{x}=\vv{b}\,.
\end{equation}
The best classical algorithm for a generic $A$ is based on Gaussian elimination, with a runtime $\mathcal{O}(D^3)$ \cite{trefethen97}. For $A$ positive semi-definite and sparse, with sparsity (i.e. maximal number of non-zero elements per row or column) $s$, the conjugate gradient algorithm \cite{book_iterative_methods} can reduce this to $\mathcal{O}(Ds\kappa)$, where $\kappa:=\norm{A}\,\norm{A^{-1}}$ 
%\Discuss{\LA{we are presenting our runtimes in terms of just  $\norm{A^{-1}}$ because we assume $\norm{A}=1$, right? But for QSLSs, we're presenting the runtime in terms of $\kappa$. Is this consistent?}}
is the condition number of $A$. In turn, the randomized Kaczmarz algorithm \cite{strohmer2007randomized} can yield an $\epsilon$-precise approximation of a single component of $\vv{x}$ in $\mathcal{O}\left(s\,\kappa_{F}^{2}\log(1/\epsilon)\right)$, with $\kappa_F := \norm{A}_{F}\norm{A^{-1}}$ %the scaled condition number and 
and $\norm{A}_{F}$ the Frobenius norm of $A$. % (satisfying $\kappa_{F}^{2}\geq r\kappa$ with $r$ the rank of $A$).


In contrast, quantum linear-system solvers (QLSSs) \cite{harrow_quantum_2009,Childs_2017,Gilyen2019,ChuangGrandUnification,Lin_2020,An_2022,Suba_2019,costa2021optimal,Wossnig_2018} prepare a quantum state that encodes the normalized version of the solution vector $\vv{x}$ in its amplitudes. More precisely, given quantum oracles for $A$ and $%\ket{\psi}=
\ket{\vv{b}}:=\frac{1}{\norm{\vv{b}}_2}\sum_{i}b_i\ket{i}$ as inputs, they output the state $\ket{\vv{x}}:=\frac{1}{\|\vv{x}\|_2}\sum_{i}x_i\ket{i}$, where $\norm{\cdot}_2$ is the $l_2$-norm and we assume $\norm{A}\le1$ for simplicity of presentation (see App. \sect{QSVT} for the case of unnormalized $A$). 
Interestingly, circuit compilations of block encoding oracles for $A$ with gate complexity $\mathcal{O}\left(\log(D/\epsilon)\right)$ have been explicitly worked out assuming a QRAM access model to the classical entries of $A$ \cite{Clader_2022}. 
This can be used for extracting relevant features -- such as an amplitude $\braket{\phi}{\vv{x}}$ or an expectation value $\mel{\vv{x}}{O}{\vv{x}}$ -- from the solution state, with potential exponential speed-ups over known classical algorithms, assuming that the oracles are efficiently implementable and $\kappa=\mathcal{O}\big(\text{polylog}(D)\big)$. 

An asymptotically optimal QLSS with query complexity $\mathcal{O}\left(\kappa\log(1/\epsilon)\right)$ was proposed in \cite{costa2021optimal} based on a discrete version of the adiabatic theorem. %\GC{it is something they prove in the paper (a version of the adiabatic theorem for discrete time evolution) and is a crucial element for obtaining the optimal algorithm. It appears even in the title of the paper. I am in favor of mentioning this instead of the more vague version.} \LA{Ok, cool. But then can we maybe clarify like "based on a discrete version of the adiabatic theorem"? Because that jargon is really not something universally established or known, I think}
%\Discuss{\LA{please check if this been improved to $\Tilde{\mathcal{O}}\left(\kappa\,\frac{\log \kappa}{\epsilon^{2}}\right)$? See statement after Lemma 2 of 2306.11802} \GC{they use wavelet-based preconditioning for $A$ and get $\Tilde{\mathcal{O}}\left(\kappa_p\,\frac{\log \kappa_p}{\epsilon^{2}}\right)$ with $\kappa_p$ of the preconditioned matrix $A_p$}}
Within the Chebyshev-based QSP framework of interest here, the best known QLSS 
uses $\mathcal{O}\left(\kappa\log(\kappa/\epsilon)\right)$ oracle queries \cite{Childs_2017}.  
If the final goal is, for instance, to reconstruct a computational-basis component $\braket{i}{\vv{x}}$ of the solution vector, the resulting runtime becomes $\mathcal{O}\left((\kappa^3/\epsilon^{2})\log(\kappa/\epsilon)\right)$, since this requires $\mathcal{O}\left(\kappa^2/\epsilon^2\right)$ measurements on $\ket{\vv{x}}$. 
%Even though the focus here is on oracle-based algorithms, it is worth mentioning that the gate complexity of the oracle can be made $\mathcal{O}\left(\log(D/\epsilon)\right)$ assuming a QRAM access model to the classical entries of $A$ \cite{Clader_2022}. 
% but this is still prohibitive for the early fault tolerant era. 
Importantly, however, in order to relate the abovementioned features of $\ket{\vv{x}}$ to the corresponding ones from the (unnormalized) classical solution vector $\vv{x}$, one must also independently estimate $\|\vv{x}\|_2$. 
This can still be done with QLSSs (e.g., with quantum amplitude estimation techniques), but requires extra runs. 
Our algorithms do not suffer from this issue, providing direct estimates from the unnormalized vector $A^{-1}\ket{\vv{b}}$. 

More precisely, with $f$ being the inverse function on the cut-off interval $\mathcal{I}_\kappa:=[1,-1/\kappa]\cup[1/\kappa,1]$, our Algs. 1 and 2 readily estimate amplitudes $\bra{\phi}A^{-1}\ket{\vv{b}}$ and expectation values $\bra{\vv{b}}A^{-1}OA^{-1}\ket{\vv{b}}$, respectively. 
The technical details of the polynomial approximation $\Tilde{f}$ and complexity analysis are deferred to App. \sect{explicit_examples_inverse}. 
In particular, there we show that, to approximate $f$ to error $\nu$, one needs a polynomial of degree $k=\mathcal{O}\left(\kappa\,\log(\kappa/\nu)\right)$ and $\norm{\vv{a}}_1=\mathcal{O}\left(\kappa\sqrt{\log(\kappa^{2}/\nu)}\right)$. 
For our purposes, as discussed before theorem \ref{main_lemma2}, to ensure a target estimation error $\epsilon$ on the quantity of interest one must have $\nu=\mathcal{O}(\epsilon)$ for Alg. 1 and $\nu=\mathcal{O}((\kappa\norm{O})^{-1}\epsilon)$ for Alg. 2. 
This leads to the sample complexities $S^{(1)}=\mathcal{O}\left((\kappa^2/\epsilon^2)\log^2(\kappa^{2}/\epsilon)\log(4/\delta)\right)$ and $S^{(2)}=\mathcal{O}\left((\kappa^4\norm{O}^2/\epsilon^2)\log^4(\kappa^3\,\norm{O}/\epsilon))\log(4/\delta)\right)$, respectively. 
    
The expected query depth and total expected runtimes are shown in Table \tab{main_table}. 
In particular, the former exhibits a quadratic improvement in the error dependence with respect to the maximal query depth $k$. 
This places our algorithm in between the $\mathcal{O}(\kappa\log(\kappa/\epsilon))$ \cite{Childs_2017} scaling of the fully quantum algorithm and the asymptotically optimal $\mathcal{O}(\kappa\log(1/\epsilon))$ scaling of \cite{costa2021optimal}, therefore making it more suitable for the early fault-tolerance era. 
In fact, our expected query depth can even beat this optimal scaling for $\kappa\lesssim(1/\epsilon)^{\log(1/\epsilon)-1}$ .%\LA{Not sure this sentence is useful/needed. The result is already awesome but with this sentence it seems that we're trying to cherry-pick an artificial regime where we have the optimal scaling... }.
Note also that our total expected runtimes are only logarithmically worse in $\kappa$ than the ones in the fully-quantum case. 
%Nevertheless, and most importantly, the explicit dependence on $D$ is kept logarithmic. 
For the case of Alg. 1, an interesting sub-case is that of $\bra{\phi}=\bra{i}$, as this directly gives the $i$-th component of the solution vector $\vv{x}$. %Importantly, 
The quantum oracle $U_{\phi}$ is remarkably simple there, corresponding to the preparation of a computational-basis state. As for the runtime, we recall that $\norm{A}\leq\norm{A}_{F}$ in general and $\norm{A}_{F}=\mathcal{O}(\sqrt{D}\,\norm{A})$ for high-rank matrices. Hence, Alg. 1 has potential for significant speed-ups over the randomized Kaczmarz algorithm mentioned above. In turn, for the case of Alg. 2, we stress that the estimates obtained refer directly to the target expectation values for a generic observable $O$, with no need to estimate the normalizing factor $\|\vv{x}\|_2$ separately (although, if desired, the latter can be obtained by taking $O=\openone$). 

%\LA{To end up with, it is important to mention that, if $A$ is given in terms of a Pauli decomposition with Pauli weight $\lambda$, the fully-randomized scheme of \cite{wang2023qubitefficient} offers also direct estimates, with no need of $\|\vv{x}\|_2$ either. It has, however, a total runtime of $\Tilde{O}\big(\norm{A^{-1}}^{4}\frac{\lambda^{2}}{\epsilon^{2}}\big)$, which is $\Tilde{O}\big(\norm{A^{-1}}^{4}\,\lambda^{2}\big)$ times worse than the scaling presented here. Moreover, as already mentioned, .... \LA{mention also the disadvantages in terms of fixed depth? If yes, please make sure this is mentioned in the end of the framework session}}

%\LA{Remove most of the next two paragraphs, except for the few valuable comments that I note below, which should  be incorporated into the new two paragraphs that I just suggested. }

%\la{\sout{The HHL quantum algorithm \cite{harrow_quantum_2009}, on the other hand, computes a single component}}  \la{\sout{of the normalized solution vector with runtime $\Tilde{\mathcal{O}}\left(\kappa\,\frac{\log D}{\epsilon^{2}}\right)$.This implies the existence of regimes of super-polynomial quantum advantages over known classical algorithms}}. Importantly, \la{however, the quantum solver's solution} is encoded into \la{the output} quantum state $\ket{\psi}=\ket{\vv{x}}$. \la{This implies} that the potential \la{supra-polynomial} quantum speed\lo{-}up \la{\sout{might be} is in general} lost if one is interested in the full solution vector $\vv{x}$. 
%In \la{\sout{practice, however, we typically want} several practical applications, however, one wishes only to estimate} either a single entry $x_i$ of the (non-normalized) solution vector or \la{obtain} an estimate the expected value $\mel{\vv{x}}{O}{\vv{x}}$ of some observable $O$\la{\sout{, which}. This} brings practical interest to the possibility of quantum advantage of HHL and similarly to our semi-quantum algorithm. 

%In the specific case of estimating a single entry of the solution vector, using any quantum algorithm that prepares the solution vector as a quantum state, such as HHL \LA{let us avoid the abbreviation HHL, let us refer to them as QLSSs (quantum linear-system solvers)} and their variants, one also requires the use of an algorithm to estimate the normalization constant \LA{this is a relic from previous versions I think}, since this constant is necessary to recover the non-normalized component of interest. Therefore, there are additional costs \LA{what are these costs? This and the previous sentences are content-empty unless you mention something of how the normalization is estimated. For instance via quantum amplitude estimation.} associated to using fully-coherent quantum algorithms with optimal \LA{it hasn't been said anywhere that they are optimal, this should be clarified in a separate sentence} query complexity for this particular estimation task. 
It is also interesting to compare our results with those of the fully randomized scheme of \cite{wang2023qubitefficient}. There, for $A$ given in terms of a Pauli decomposition with total Pauli weight $\lambda$, they also offer direct estimates, with no need of $\|\vv{x}\|_2$. However, their total runtime of $\Tilde{O}\big(\norm{A^{-1}}^{4}\lambda^{2}/\epsilon^{2}\big)$ is worse than the scaling presented here by a factor $\Tilde{O}\big(\norm{A^{-1}}\,\lambda^{2}\big)$ (recall that here $\kappa=\norm{A^{-1}}$ since we are assuming $\norm{A}=1$). In turn, compared to the solver in Ref. \cite{wang2023qubitefficient}, the scaling of our query depth per run is one power of $\kappa$ superior. In their case, the scaling refers readily to circuit depth, instead of query depth, but this is approximately compensated by the extra dependence on $\lambda^{2}$ in their circuit depth. 



% Our semi-quantum randomized algorithm can be applied to linear system solving by assuming $A$ Hermitian (with spectral norm $\norm{A}$) and that the classical input vector $\vv{b}$ is encoded in a normalized quantum state $\ket{\vv{b}}$. Then the task of producing with confidence $1-\delta$ an $\epsilon$-precise estimate \LA{all this should be clear and redundant by now} of
% \begin{equation}\label{eq:linear_sys_task}
% \bra{i}A^{-1}{\ket{\mathbf{b}}}\,,
% \end{equation}
% where $\ket{i}$ is the basis vector corresponding to the $i$-th component of $\ket{\vv{b}}$ \LA{you meant $\ket{\mathbf{x}}$. Anyways, we don't need this equation, only we need to say that $\ket{\phi}=\ket{i}$}, fits into the general framework of problem \ref{problem.1}. 
%To do that, we resort to the Chebyshev approximation $\Tilde{f}$ of the inverse function introduced in \cite{childs_quantum_2017} and discussed in App\la{.} \sect{explicit_examples_inverse}. The corresponding polynomial degree $k$ and coefficient 1-norm $\norm{\vv{a}}_1$ are shown in Table \tab{chebyshev_data}. The complexity of the algorithm follows from direct application of Lemma \ref{main_lemma2} and is presented in Table \tab{main_table} (see App\la{.} \sect{explicit_examples_inverse} for details). As expected, our scaling is worse than that of the fully coherent algorithm. However, it's better than the fully randomized one and gives us the matrix element directly. Most importantly, the exponential speed-up \LA{careful with the statement "exponential speed-up", there's the dependence on $\kappa$ (which can be exponentially bad) and there's dequantization.... better to soften like "the explicit logarithmic dependence on $D$ is kept intact"} in the system dimension is kept intact. 



























\subsubsection{Ground-state energy estimation} \label{sec:groundstate}

The task of estimating the ground-state energy of a quantum Hamiltonian holds paramount importance in condensed matter physics, quantum chemistry, material science, and optimization. 
In fact, it is considered one of the most promising use cases for quantum computing in the near term \cite{clinton2022nearterm}. 
However, the problem in its most general form is known to be QMA-hard \cite{kempe2005complexity}. A typical assumption -- one we will also use here -- is that one is given a Hamiltonian $H$ with $\norm{H}\leq1$ and a promise state $\varrho$ having non-vanishing overlap $\eta$ with the ground state subspace. The {\it ground state energy estimation} (GSEE) problem \cite{lin_heisenberg-limited_2022} then consists in finding an estimate of the ground state energy $E_{0}$ to additive precision $\epsilon$. %\GC{more refs?} \AD{Maybe add some of the references used in Lin and Tong's paper?? I don't know.} 

If the overlap $\eta$ is reasonably large (which is often the case in practice, e.g., for small molecular systems using the Hartree-Fock state \cite{tubman2018postponing}), the problem is known to be efficiently solvable, but without any guarantee on $\eta$ the problem is challenging. 
A variety of quantum algorithms for GSEE have been proposed (see, e.g., \cite{PhysRevLett.83.5162,ge2018faster,Lin_2020,Poulin_2009}), but the substantial resources required are prohibitive for practical implementation before full-fledged fault tolerant devices become available. 
Recent works have tried to simplify the complexity of quantum algorithms for GSEE with a view towards early fault-tolerant quantum devices. 
Notably, a semi-randomized quantum scheme was proposed in \cite{lin_heisenberg-limited_2022} with query complexity $\mathcal{O}\big(\frac{1}{\epsilon}\log\!\big(\frac{1}{\epsilon\eta}\big)\big)$ achieving Heisenberg-limited scaling in $\epsilon$ \cite{Atia_2017}. 
Importantly, their algorithm assumes access to the Hamiltonian $H$ through a time evolution oracle $e^{-iH\tau}$ (for some fixed time $\tau$), which makes it more appropriate for implementation in analog devices. 
The similar fully-randomized approach of \cite{wan_randomized_2022} gives rise to an expected circuit (not query) complexity of $\mathcal{O}\big(\frac{1}{\epsilon^2}\log\!\big(\frac{1}{\eta}\big)\big)$.  

% \vspace{1cm}

% \Danger{option 1:}\\

Here we approach the GSEE problem within our Chebyshev-based randomized semi-quantum framework. 
We follow the same strategy used in \cite{lin_heisenberg-limited_2022,wan_randomized_2022,wang2023faster,wang2023quantum} of reducing GSEE to the so-called {\it eigenvalue thresholding problem}. The problem reduces to the estimation up to additive precision $\frac{\eta}{2}$ of the filter function $F_\varrho(y):=\Tr[\varrho\,\Theta(y\mathds{1}-H)]$ for a set of $\log\big(\frac{1}{\xi}\big)$ different values of $y$ chosen from a uniform grid of cell size $\xi$ (times the length $E_\text{max}-E_0$ of the interval of energies of $H$). This allows one to find $E_0$ up to additive error $\xi$ with $\log\big(\frac{1}{\xi}\big)$ steps of a binary-like search over $y$ \cite{lin_heisenberg-limited_2022}. 
At each step, we apply our Alg. \fig{main} with $f(x)=\Theta(y-x)$, $A=H$, and $\ket{\phi}=\ket{\psi}$ to estimate $F_{\varrho}(y)$, with $\varrho=\ketbra{\psi}{\psi}$. Here, $\ket{\psi}$ is any state with promised overlap $\eta>0$ with the ground state subspace. 
The requirement of additive precision $\frac{\eta}{2}$ for $F_\varrho(x)$ requires an approximation error $\nu\le\frac{\eta}{4}$ for $f$ and a statistical error $\epsilon\le\frac{\eta}{4}$ for the estimation. 
%\GC{double check if factors $\eta/2,\eta/4$ etc are really ok} 
%In addition, our appraoch does not need to estimate $F_\varrho(y)$ at different $y$'s for the search. In Lemma \ref{lemma_ychoice} in App. \sect{explicit_examples_step}, we show that estimating $F_\varrho$ at a special point $y_*=1/\sqrt{2}$ and increasing the number of samples suffices to obtain $F_\varrho(y)$ at any other $y$. 

Interestingly, our approach does not need to estimate $F_\varrho(y)$ at different $y$'s for the search. In Lemma \ref{lemma_ychoice} in App. \sect{explicit_examples_step}, we show that estimating $F_\varrho$ at a special point $y_*=1/\sqrt{2}$ and increasing the number of samples suffices to obtain $F_\varrho(y)$ at any other $y$. 
As a core auxiliary ingredient for that, 
we develop a new $\nu$-approximation $\Tilde{f}$ %$\Tilde{f}_{\sigma,step}(x-y)$ 
to the step function with a shifted argument, $\Theta(y-x)$, given in Lemma \ref{lemma_theta} in App. \sect{explicit_examples_step}. It has the appealing property that the $x$ and $y$ dependence are separated, namely $\Tilde{f}(y-x)=\sum_{j\in[k]}\big[a_j(y)T_j(x)\big]+\sum_{j\in[k]}\big[b_j(y)\sqrt{1-x^2}U_j(x)\big]$, where $U_j$ is the $j$-th Chebyshev polynomial of the second kind. To the best of our knowledge, this is a novel Chebyshev-polynomial expansion of the step function that may be of independent interest. 
The first contribution to $\Tilde{f}$ takes the usual form \eq{ftilde} and can be directly implemented by our Alg. \fig{main}; the second contribution containing the $U_j$'s can also be implemented in a similar way, with the caveat that the required Hadamard test needs a minor modification described in Lemma \ref{lemma_hadamardsecondkind}, App. \sect{Hadamard_test}. 
The maximal degree $k=\mathcal{O}(\frac{1}{\xi}\log\big(\frac{1}{\eta}\big))$ is the same for both contributions and the coefficient 1-norms are $\norm{\vv{a}}_1=\norm{\vv{b}}_1=\mathcal{O}\left(\log\big(\frac{1}{\xi}\log\big(\frac{1}{\eta}\big)\big)\right)$. 
Putting all together and taking into account also the $\mathcal{O}\big(\log\big(\frac{1}{\xi}\big)\big)$ steps of the binary search, one obtains a total sample complexity $S^{(1)}=\mathcal{O}\left(\frac{1}{\eta^{2}}\log^2\big(\frac{1}{\xi}\log\big(\frac{1}{\eta}\big)\big)\log(\frac{4}{\delta}\log(\frac{1}{\xi}))\right)$. 
%, which gives rise to the sample complexity.
%One is of separating the variables $x$ and $y$ just like the known Fourier approximations, which allow us compute $F(x)$ for all $x$ by sampling $\Tilde{f}_{\sigma,step}(1/\sqrt{2}-y)$ and introducing $x$-dependent weight factors (see App.\ref{sec:explicit_examples_step}). The other is a maximal expansion degree of order $\mathcal{O}(\epsilon^{-1}\log(\eta^{-1}))$, which is the same as the one \cite{wan_randomized_2022} and better than the approximation used in \cite{lin_heisenberg-limited_2022} by a factor of $\epsilon^{-1}$ inside the logarithm. 

The corresponding expected query depth and total runtime are shown in Table \tab{main_table}. 
Remarkably, the query depth exhibits significant speed ups with respect to the maximal value $k$, namely a square root improvement in the $\eta$ dependence and a logarithmic improvement in the $\frac{1}{\xi}$ dependence (see Lemma \ref{lemma_step_query} in App. \sect{explicit_examples_step} for details). 
In addition, as can be seen in the table, our expected runtime %$\mathcal{O}(\epsilon^{-1}\log(\eta^{-1}))$ 
displays the same Heisenberg-scaling of \cite{wan_randomized_2022}. This is interesting given that our algorithm is based on block-encoded oracles rather than the time-evolution oracles used in \cite{wan_randomized_2022}, which may be better suited for digital platforms as discussed previously. Finally, it is interesting to note that there have been recent improvements in the precision dependence, e.g. based on a derivative Gaussian filters \cite{wang2023quantum}. Those matrix functions are also within the scope of applicability of our approach.












%In the context of algorithms for early fault-tolerant quantum devices, several recent papers have tried to simplify the circuit complexity of quantum algorithms for GSEE. \LA{For example, in Ref.... they did this and got the scaling x, in Ref ... that did that and got the scaling y, etc.} 
%In general, oracle-based quantum algorithms for GSEE should satisfy three criteria \la{why? who dictates this? Anyway, I'm not sure we need so much detail here}: $(i)$ maximal query depth of order $\Tilde{O}(\epsilon^{-1}\text{polylog}(\epsilon^{-1}\eta^{-1}))$; $(ii)$ Heisenberg-limited precision scaling, or total query complexity of order $\Tilde{O}(\epsilon^{-1}\text{poly}(\eta^{-1}))$; and $(iii)$ a single extra non-oracular ancilla qubit. As shown in \cite{lin_heisenberg-limited_2022} no known fully-coherent quantum algorithm for GSEE satisfy all those criteria. The fully randomized approach of \cite{wan_randomized_2022} provides an interesting alternative, but requires quadratically higher depths than those allowed by the criteria above (also, their figure of merit is logical gate depths instead of query depths). 
%The semi-randomized algorithm of \cite{lin_heisenberg-limited_2022} satisfy all three criteria, however they require a controlled time-evolution oracle, whose implementations may incur in additional factors of powers of $\eta^{-1},\epsilon^{-1}$ in depth costs, depending on the implementation (see Appendix D of \cite{lin_heisenberg-limited_2022}). Our semi-randomized algorithm does not incur in the loss of Heisenberg scaling on precision. 

%\Danger{Leandro's main comment about the previous text is right bellow this warning in the code editor.}
%\LA{This paragraph is a nice review on GSEE algorithms, very well written, this would be good for the intro of a paper focused entirely on this. However, for us this is just a use case deep inside the technical part of our paper. We need to go much more straight to our point. I would condense this and the previous paragraph: i) very briefly open with GSEE as an important task for condensed matter physics, quantum chemistry, material science, and optimization. ii) Mention (but not explain) and cite the previpus work on this task with early-FT algs. iii) go straight to our point, mentioning something like "Here, we build on Ref. \cite{lin_heisenberg-limited_2022} and implement the ETP with our randomized, semi-quantum approach". iv) briefly review ETP. The discussion must be much shorter than the one you do now. We don't need Eq. \eqref{eq:ETP}. All we need is just to mention that the method is based on a binary search on $x$ via the function $F(x):=\Tr[\varrho\,\Theta(x\mathds{1}-H)]$, which jumps every time $x$ passes by en eigenvalue of $H$.  v) present our approach. vi) discuss the complexity of our implementation (here is where you highlight the preservation of the Heisenberg scaling). vii) compare with advantages/disadvantages of other works, if applicable.}

%As shown in \cite{lin_heisenberg-limited_2022}, GSEE can be equivalently formulated as an {\it eigenvalue thresholding problem} (ETP). In this decision problem, we are given a threshold value $x^*$, a precision parameter $\epsilon$, and the promise state $\varrho$ having overlap $\eta>0$ with the ground subspace, and need to decide whether $x^{*}$ satisfies the thresholding conditions
%\begin{equation}
%\label{eq:ETP}
%F(x^{*}+\epsilon)>0\quad \text{ and } \quad F(x^{*}-\epsilon)<\eta
%\end{equation}
%with $F(x):=\Tr[\varrho\,\Theta(x\mathds{1}-H)]$ and $\Theta(x)$ the Heaviside function. 
% \begin{equation}
% F(x):=\Tr[\varrho\,\Theta(x\mathds{1}-H)].
% \end{equation}
%This can be solved using a binary search strategy \cite{lin_heisenberg-limited_2022} based on computing $F(x)$ for two particular values of $x$ and updating the thresholds iteratively until both conditions are satisfied. 

%In App\la{.} \sect{explicit_examples_step} we prove that the ETP can be posed within the general framework of Problem \ref{problem.1}, and therefore solved by our Algorithm 1 \LA{This is the core of our strategy, this should be anticipated much earlier in this section, not in the fourth paragraph. Moreover, the quality/clarity of the previous 3 paragraphs is higher than the one you attained here!}. 



% \ad{Our method has an expected query complexity of order $O(\epsilon^{-1}\log(\eta^{-1}))$ and total run-time of order $\Tilde{O}(\epsilon^{-1}\eta^{-2})$. This implies that both our work and \cite{lin_heisenberg-limited_2022} \LA{no, this implies this only for ouir work, rephrase} achieve Heisenberg scaling in $\epsilon$, but with a slightly smaller query complexity in general. This advantage may be compounded by the fact that we use block-encoded oracles instead of time-evolution oracles, which may be harder to implement depending on the platform.}\AD{Not so sure about this but okay. Maybe comment about trotterization-related overheads??}\ad{In terms of circuits, our method has the expected depth $O(\epsilon^{-1}\log(\eta^{-1})\log(D))$ where $D$ is the systems dimension. This gives us a dimension-related disadvantage in depth when compared to \cite{wan_randomized_2022}, but a quadratic advantage with respect to $\epsilon$, which is generally better in settings where higher precision is required}\AD{This is a bit weak I think, better argument maybe?} \LA{I don't see a special need to compare us against \cite{wan_randomized_2022} more than to any of the other early fault toleraant methods for GEES (see intro). I think the most important thing is the Heisenberg scaling retained while providing all the nice advantages in terms of near term quantum hardware and noise robustness}

%The construction makes use of a Chebyshev expansion for the shifted step function $\Theta(x-y)$ \la{which is novel, to the best of our knowledge. Its} derivation is presented in App\la{.} \sect{explicit_examples_step} \LA{But so waht? Say something of what is special about this new expansion/how it helps us}. It also requires a minor modification to Algorithm 1, namely the introduction of a controlled gate in the ancilla preparing the QSP-iterate that allows us to estimate $\mel{0}{T_{2j+1}(H)}{1}$ instead of $\mel{0}{T_{2j+1}(H)}{0}$ \LA{didn't get it. Why can't $\mel{0}{T_{2j+1}(H)}{1}$ be estimated with Alg. 1 with $\ket{\psi}=\ket{0}$ and $\ket{\phi}=\ket{1}$?}. Since the difference in complexity for different values of $x$ is at most a constant \LA{Rephrase. "Since" assumes the reader knows already this, but this piece of information you're mentioning only now}, this does not change any of the complexity statements of Lemma \ref{main_lemma2}. Then, using a binary search requires that we compute at most $\mathcal{O}(\log\frac{1}{\epsilon})$ values of $x$ to find our estimate for $E_{0}$, inflating the sampling complexity by a $\log(\epsilon^{-1})$. This can be mitigated however, sampling $F(x)$ for a specific $x$ (namely $x=1/\sqrt{2}$), and re-utilizing the samples with appropriate weight factors to compute the function for other values of $x$. This procedure, described in detail in App\la{.} \sect{explicit_examples_step}, allows us to remove the $\log\epsilon^{-1}$ factor in sampling by requiring an estimate of $F(1/\sqrt{2})$ with $\delta\log^{-1}\epsilon^{-1}$ confidence. \LA{I was not able to follow  the technicalities of the last three sentences. Either elaborate more on them or summarize, cutting technicalities out, and directing to the appendix}

% \begin{lemma}[Ground-state energy estimation]\label{lemma_groundstate}
% state    
% \end{lemma}

%The Chebyshev approximation data is shown in Table \tab{chebyshev_data} and the results are shown in Table \tab{main_table} blablabla...

%\AD{The "latex-commented" comment (ha ha ha) of Leandro asked us to shorten the discussion but I think it is too short. See what you guys think.}

%\AD{Also, I may have found an expression for the query-complexity in this case, we need to discuss this.}



\section{Final discussion} 
We presented a randomized hybrid quantum-classical framework to efficiently estimate state amplitudes and expectation values involving a generic matrix function $f(A)$. 
More precisely, our algorithms perform a Monte-Carlo simulation of the powerful quantum signal processing (QSP) and singular-value transformation (QSVT) techniques \cite{LowChuang2017,LowChuangQuantum2019,Gilyen2019,ChuangGrandUnification}. 
Our toolbox is based on three main ingredients: 
$i$) it samples each component of a Chebyshev series for $f$ weighed by its coefficient in the series;
$ii$) it assumes coherent access to $A$ via a block encoding oracle; and
$iii$) $f(A)$ is automatically extracted from its block-encoding without post-selection, 
using a Hadamard test. 
This combination allows us to deliver provably better circuit complexities than, similar total runtimes to, and major advantages in terms of experimental feasibility over the standard  QSP and QSVT algorithms.

We benchmarked our algorithms (both analytically and numerically) on four specific end-user applications: partition-function estimation via quantum Markov-chain Monte Carlo and via imaginary-time evolution; linear system solvers; and ground-state energy estimation.
The main features (functioning, performance guarantees, and noise-sensitivity) of the framework as well as the highlights for each use case are summarized in an accessible, non-technical fashion in Sec. \sect{summary}. In turn, the full end-to-end complexity scalings are detailed in Table \tab{main_table}.

Potentially interesting directions could be to explore other matrix functions with our framework. This could include recent developments such as Gaussian and derivative-Gaussian filters for precision improvements in ground-state energy estimation \cite{wang2023quantum} or estimation of Green's functions \cite{wang2023qubitefficient}, e.g., as well as a diversity of other more-established use cases \cite{ChuangGrandUnification}. 
Another interesting possibility could be to explore the applicability of our methods in the context of hybrid quantum-classical rejection sampling \cite{wang2023faster}.
Moreover, further studies on the interplay between our framework and Fourier-based matrix processing \cite{silva2022fourierbased,Dong_2022} may be in place too. Fourier-based approaches have so far focused mainly on the eigenvalue thresholding for ground-state energy estimation \cite{lin_heisenberg-limited_2022,wang_quantum_2022,wan_randomized_2022,wang2023qubitefficient}. 
Finally, an important topic is hard-ware circuit compilations of block-encoding oracles, specific to given platforms. For example, Rydberg-atom platforms, which allow for long-range multi-qubit gates  in a native way could offer interesting possibilities in this regard \cite{Bluvstein2022}.  

Our findings open a promising arena to build and optimize early fault-tolerant quantum algorithms towards practical linear-algebra applications in a nearer term.

\begin{acknowledgements}
    AT acknowledges financial support from the Serrapilheira Institute (grant number Serra-1709-17173).
    We thank Lucas Borges, Samson Wang, Sam McArdle, Mario Berta, Daniel Stilck-Fran\c{c}a,  and Juan Miguel Arrazola for helpful discussions.
\end{acknowledgements}

%\newpage







% %- Methods -----------------------------------------------------------------------------------------------------------
% \section{Methods} \label{Sec:methods}

% \GC{do we really need that?}




























%\bibliographystyle{plain}
\bibliographystyle{unsrt}
\bibliography{refs}


\appendix


%\newpage 









\section{Proof of Lemma \ref{cheb}}
\label{sec:chebyshev_proof}

%Here we prove Lemma \ref{cheb}. 

\begin{proof}
Let $U_{A}$ be given as in Def. \ref{def:qubitizedoracle}. Then 
\begin{align}
U^{j}_{A}&=\bigoplus_{\gamma}\left(e^{-i\theta_{\gamma}Y_{\gamma}}\right)^{j}\notag\\%=e^{-ij\theta_{\gamma}Y_{\gamma}}=&\\
&=\bigoplus_{\gamma}{\begin{bmatrix}
\cos(j\arccos{\lambda_\gamma}) & -\sin(j\arccos{\lambda_\gamma})\\
\sin(j\arccos{\lambda_\gamma}) & \cos(j\arccos{\lambda_\gamma})
\end{bmatrix}}
%\otimes\ketbra{\lambda_\gamma}
.
\end{align}
Since $\cos(j\arccos{x})=T_{j}(x)$, where $T_j$ is the $j$-th Chebyshev polynomial of the first kind, it follows that 
%By the trigonometric definition of the Chebyshev polynomials of the first and second kinds, $T_j(x)$ and $U_j(x)$ respectively, it follows that $\cos(j\arccos{\lambda_\gamma})=T_{j}(\lambda_\gamma)$ and $\sin(j\arccos{\lambda_\gamma})=\sqrt{1-\lambda_\gamma^{2}}\,U_{j-1}(\lambda_\gamma)$. 
% \begin{equation}
% U^{j}_{A}=\bigoplus_{\gamma}{\begin{bmatrix}
% T_{j}(\lambda_\gamma) & -\sqrt{1-\lambda_\gamma^{2}}U_{j-1}(\lambda_\gamma)\\
% \sqrt{1-\lambda_\gamma^{2}}U_{j-1}(\lambda_\gamma) & T_{j}(\lambda_\gamma)
% \end{bmatrix}}
% \otimes\ketbra{\lambda_\gamma}
% .
% \end{equation}
%Therefore, it follows that
\begin{equation}
\big(\bra{0}_a\otimes \mathds{1}_s\big)\,U_A^j\,\big(\ket{0}_a\otimes \mathds{1}_s\big)=\bigoplus_{\gamma}T_{j}(\lambda_\gamma)\ketbra{\lambda_\gamma}_{s}=T_j(A).
\end{equation}
\end{proof}


















\section{Hadamard test for block encodings}
\label{sec:Hadamard_test}

%\GC{Do we need the following Def 7, Lemma 8 and Def 9? Seems they are never used and could be removed (or at least moved to App C).} \LA{I also think we can omit the Hadamard test definition and the first figure with the standard Hadamard test. In fact, the other figure with the modified test is not necessary in my opinion either (to prove Lemma \ref{lem:Hadamard_modified}, we could simply refer to Fig. 1). But since you have already made it we can keep it. PLEASE UPDATE ALL THE NOTATION OF THIS SECTION AND THE NEXT ONE}

%\begin{lemma}[Hadamard test]\label{lem:had_test}

%The Hadamard test is the protocol described by the circuit in Fig. \ref{fig: had_gen_test}, that takes an ancilla state $\ket{+}_{1}$, a controlled unitary $U$, and an arbitrary $\varrho_{s}$. It produces a classical random variable $\mathbf{Had}(U)$ with range $\{+1,-1\}$ such that
%\begin{equation}
%\mathbb{E}\Big[\textbf{Had}(U)\Big]=
%\begin{cases}
%\Re\left\{\Tr_{s}[U\varrho_{s}]\right\}\text{, if }B=\mathds{1},\\
%\Im\left\{\Tr_{s}[U\varrho_{s}]\right\}\text{, if }B=S^{\dagger}.
%\end{cases}
%\end{equation}
%\end{lemma}

%% Figure environment removed

%\begin{proof}
%We have that
%\begin{equation}
%\mathbb{E}\Big[\textbf{Had}(U)\Big]=\sum_{b\in\{-1,1\}}p(b)b,
%\end{equation}
%where
%\begin{equation}
%\begin{split}
%p(1)&=\Tr{\ketbra{+}_{1}(CU_{s})B(\ketbra{+}_{1}\varrho_{s})B^{\dagger}(CU_{s})^{\dagger}},\\
%p(-1)&=\Tr{\ketbra{-}_{1}(CU_{s})B(\ketbra{+}_{1}\varrho_{s})B^{\dagger}(CU_{s})^{\dagger}}.
%\end{split}
%\end{equation}
%from which we have that
%\begin{equation}
%\mathbb{E}\Big[\textbf{Had}(U)\Big]=\Tr{X_{1}(CU_{s})B(\ketbra{+}_{1}\varrho_{s})B^{\dagger}(CU_{s})^{\dagger}}
%\end{equation}
%Now, for $B=\mathds{1}$ we have that
%\begin{multline}
%\Tr{X_{1}(CU_{s})(\ketbra{+}_{1}\varrho_{s})(CU_{s})^{\dagger}}=\\
%=\frac{\Tr_{s}\{U\varrho_{s}+\varrho_{s}U^{\dagger}\}}{2}=\Re{\Tr_{s}(\varrho_{s}U)}
%\end{multline}
%while for $B=S_{1}^{\dagger}=i(\ketbra{0}_{1}-\ketbra{1}_{1})$,
%\begin{multline}
%\Tr{X_{1}(CU_{s})S_{1}^{\dagger}(\ketbra{+}_{1}\varrho_{s})S_{1}(CU_{s})^{\dagger}}=\\
%=\frac{i\Tr_{s}\{U\varrho_{s}-\varrho_{s}U^{\dagger}\}}{2}=\Im{\Tr_{s}(\varrho_{s}U)}
%\end{multline}
%\end{proof}

%\begin{lemma}[Modified Hadamard test]
%\label{lem:Hadamard_modified}
%If $U_{A}$ is an exact block-encoding of $A$, and $\ketbra{0}_{a}\varrho_{s}$ is a state in $\mathcal{H}_{a}\otimes\mathcal{H}_{s}$ then the Hadamard test yields a random variable $\mathbf{Had}(U^{j}_{A})$ satisfying
%\begin{equation}
%\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A})\Big]=\begin{cases}
%\Re\{\Tr_{s}[T_j(A)\varrho_{s}]\}\text{, if }B=\mathds{1},\\
%\Im\{\Tr_{s}[T_j(A)\varrho_{s}]\}\text{, if }B=S^{\dagger},
%\end{cases}
%\end{equation}
%\end{lemma}

%% Figure environment removed

%\begin{proof}
%Here we prove it for the case of $B=\mathds{1}$, as the other case is analogous. We have by Lemma %\ref{lem:had_test} that
%\begin{equation}
%\begin{split}
%&\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A})\Big]=\Re\{\Tr_{a,s}[U^{j}_{A}(\ketbra{0}_{a}\otimes\varrho_{s})]\}=\\
%=&\Re\{\Tr_{a,s}[U_{A}(\mathds{1}_{a}\otimes\varrho_{s}))(\ketbra{0}_{a}\otimes\mathds{1}_{s})]\}=\\
%=&\Re\{\Tr_{s}[\mel{0}{U^{j}_{A}}{0}_{a}\varrho_{s}]\}=\Re\{\Tr_{s}[T_j(A)\varrho]\}
%\end{split}
%\end{equation}
%\end{proof}

\begin{lemma}[Circuit for algorithm 1]\label{lem:had_circuit_1}
The hadamard test of fig.\ref{fig:main}.a) yields a random variable $\mathbf{Had}(U^{j}_{A},\ket{\phi},\ket{\psi})$ that satisfies
\begin{equation}
\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A},\ket{\phi},\ket{\psi})\Big]=\begin{cases}
\Re{\mel{\phi}{T_j(A)}{\psi}}\text{, if }B=\mathds{1},\\
\Im{\mel{\phi}{T_j(A)}{\psi}}\text{, if }B=S^{\dagger},
\end{cases}    
\end{equation}
\end{lemma}

\begin{proof}
Here we prove it for the case $B=\mathds{1}$, since the other follows in an analogous way. We have that,
\begin{equation}
\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A},\ket{\phi},\ket{\psi})\Big]=\Tr{X_{1}G(\ketbra{+}_{1}\varrho_{a,s})G^{\dagger}},
\end{equation}
with $G=\Bar{C}U_{\phi}CU^{j}_{A}CU_{\psi}$ and $\varrho_{a,s}=\ketbra{0}_{a}\ketbra{0}_{s}$. Computing the expression inside the trace we get
\begin{multline}
X_{1}G(\ketbra{+}_{1}\varrho_{a,s})G^{\dagger}=\frac{1}{2}\ketbra{0}_{1}(U^{j}_{A}U_{\psi}\varrho_{a,s}U^{\dagger}_{\phi})+\\
+\frac{1}{2}\ketbra{1}_{1}(U_{\phi}\varrho_{a,s}U^{\dagger}_{\psi}U^{j\dagger}_{A})+\frac{1}{2}\ketbra{1}{0}_{1}(U_{\phi}\varrho_{a,s}U^{\dagger}_{\phi})+\\
+\frac{1}{2}\ketbra{0}{1}_{1}(U^{j}_{A}U_{\psi}\varrho_{a,s}U^{\dagger}_{\psi}U^{j\dagger}_{A}).
\end{multline}
Therefore,
\begin{multline}
\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A},\ket{\phi},\ket{\psi})\Big]=\Re\{\Tr_{a,s}[U^{j}_{A}\ketbra{0}_{a}\ketbra{\psi}{\phi}_{s}]\}=\\
=\Re\{\Tr_{s}[T_j(A)\ketbra{\psi}{\phi}_{s}]\}=\Re\{\mel{\phi}{T_j(A)}{\psi}\}
\end{multline}
\end{proof}

\begin{lemma}[Circuit for algorithm 2]\label{lem:had_circuit_2}
The hadamard test of fig.\ref{fig:main}.b) yields a random variable $\mathbf{Had}(U^{l}_{A},U^{j}_{A},O)$ that satisfies
\begin{equation}
\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A},U^{l}_{A},O)\Big]=\Re\{\Tr[OT_j(A)\varrho_{s}T_{l}(A)]\}
\end{equation}
\end{lemma}

\begin{proof}
We have that
\begin{equation}
\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A},U^{l}_{A},O)\Big]=\Tr{OX_{1}M(\ketbra{+}_{1}\varrho_{a,s})M^{\dagger}},
\end{equation}
where $M=\Bar{C}U^{l}_{A}CU^{j}_{A}$ and $\varrho_{a,s}=\ketbra{0}_{a}\varrho_{s}$. Note that
\begin{multline}
X_{1}M(\ketbra{+}_{1}\varrho_{a,s})M^{\dagger}=\frac{1}{2}\ketbra{0}_{1}(U^{j}_{A}\varrho_{a,s}U^{l\dagger}_{A})+\\
+\frac{1}{2}\ketbra{1}_{1}(U^{l}_{A}\varrho_{a,s}U^{j\dagger}_{A})+\frac{1}{2}\ketbra{1}{0}_{1}(U^{l}_{A}\varrho_{a,s}U^{l\dagger}_{A})+\\
+\frac{1}{2}\ketbra{0}{1}_{1}(U^{j}_{A}\varrho_{a,s}U^{j\dagger}_{A}),
\end{multline}
which implies
\begin{multline}
\mathbb{E}\Big[\mathbf{Had}(U^{j}_{A},U^{l}_{A},O)\Big]=\frac{1}{2}\Tr_{a,s}[O(U^{j}_{A}\varrho_{a,s}U^{l\dagger}_{A})]+\\
+\frac{1}{2}\Tr_{a,s}[O(U^{l}_{A}\varrho_{a,s}U^{j\dagger}_{A})]=\frac{1}{2}\Tr_{s}[O(T_j(A)\varrho_{s}T^{\dagger}_{l}(A))]+\\
+\frac{1}{2}\Tr_{s}[O(T_j(A)\varrho_{s}T^{\dagger}_{l}(A))]=\Tr_{s}[O(T_j(A)\varrho_{s}T_l(A))],
\end{multline}
since $A$ and $O$ are hermitian.
\end{proof}

\begin{lemma}\label{lemma_hadamardsecondkind}  If we modify the hadamard test in fig.\ref{fig:main}.a) by applying a $\Bar{C}NOT_{1,a}$ after any other gate, the random variable $\textbf{Had}_{alt}(U^{j}_{A},\ket{\phi},\ket{\psi})$ satisfies for $B=\mathds{1}$
\begin{equation}
\mathbb{E}\Big[\textbf{Had}_{alt}(U^{j}_{A},\ket{\phi},\ket{\psi})\Big]=\Re\{\mel{\phi}{\sqrt{\mathds{1}-A^{2}}U_{j-1}(A)}{\psi}\}
\end{equation}
and
\begin{equation}
\mathbb{E}\Big[\textbf{Had}_{alt}(U^{j}_{A},\ket{\phi},\ket{\psi})\Big]=\Im\{\mel{\phi}{\sqrt{\mathds{1}-A^{2}}U_{j-1}(A)}{\psi}\}
\end{equation}
for $B=S^{\dagger}$.
\end{lemma}

\begin{proof}
Again, here we only show the case $B=\mathds{1}$ since the other case is analogous. From the description of the circuit in fig.\ref{fig:main}.a) and the added modification we have that
\begin{equation}
\mathbb{E}\Big[\textbf{Had}_{alt}(U^{j}_{A},\ket{\phi},\ket{\psi})\Big]=\Tr[X_{1}G_{alt}(\ketbra{+}_{1}\varrho_{a,s})G_{alt}^{\dagger}]
\end{equation}
with $G_{alt}=\Bar{C}NOT_{1,a}\Bar{C}U_{\phi}CU^{j}_{A}CU_{\psi}$ and $\varrho_{a,s}=\ketbra{0}_{a}\ketbra{0}_{s}$. We have that
\begin{multline}
X_{1}G_{alt}(\ketbra{+}_{1}\varrho_{a,s})G_{alt}^{\dagger}=\frac{\ketbra{1}{0}_{1}\ketbra{1}_{a}\ketbra{\phi}_{s}}{2}+\\
+\frac{\ketbra{0}{0}_{1}U^{j}_{A}(\ketbra{0}{1}_{a}\ketbra{\psi}{\phi}_{s})}{2}+\frac{\ketbra{1}{1}_{1}(\ketbra{1}{0}_{a}\ketbra{\phi}{\psi}_{s})U^{j\dagger}_{A}}{2}+\\
+\frac{\ketbra{0}{1}_{1}U^{j}_{A}(\ketbra{0}_{a}\ketbra{\psi}_{s})U^{j\dagger}_{A}}{2},
\end{multline}
from which we obtain
\begin{equation}
\Tr[X_{1}G_{alt}(\ketbra{+}_{1}\varrho_{a,s})G_{alt}^{\dagger}]=\Re\{\mel{\phi}{\mel{1}{U^{j}_{A}}{0}_{a}}{\psi}_{s}\}.
\end{equation}
By def.\ref{def:qubitizedoracle} and the trigonometric definition of the Chebyshev polynomial $U_{j-1}(x)$ we have that
\begin{equation}
\big(\bra{1}_a\otimes \mathds{1}_s\big)\,U_A^j\,\big(\ket{0}_a\otimes \mathds{1}_s\big)=\sqrt{\mathds{1}-A^{2}}U_{j-1}(A),
\end{equation}
proving the statement.
\end{proof}


\section{Proof of Theorem \ref{main_lemma}}%{Methods} 
\label{sec:main_lemma_proof}



%Here is the proof of Theorem \ref{main_lemma}.

\begin{proof}
There will be different proofs for each $P$. For $P=1$, note that
\begin{equation}
\mel{\phi}{\Tilde{f}(A)}{\psi}=\sum^{k}_{j=0}a_{j}\mel{\phi}{T_j(A)}{\psi},
\end{equation}
and from Lemma \ref{lem:had_circuit_1} for $B=\mathds{1}$,
\begin{multline}
\Re\{\mel{\phi}{\Tilde{f}(A)}{\psi}\}=\sum^{k}_{j=0}a_{j}\,\mathbb{E}\Big[\textbf{Had}(U^{j}_{A},\ket{\phi},\ket{\psi})\Big]\\
=\sum^{k}_{j=0}\sum_{b\in\{-1,1\}}p_{j}\,p(b|j)\,\norm{\vv{a}}_{1}\sgn(a_{j})\,b.
\end{multline}
The last line implies that, for $\alpha<S^{(1)}$, $\frac{1}{S^{(1)}}\sum_\alpha \norm{\vv{a}}_{1}\sgn(a_{j_\alpha})\,b^{(1)}_\alpha$ with $j_\alpha$ sampled from the distribution $p_j$ and $b^{(1)}_\alpha$ sampled from the conditional distribution $p(b|j)$ by running the Hadamard test in Figure \fig{main}a) is an unbiased estimator of $\Re\{\mel{\phi}{\Tilde{f}(A)}{\psi}\}$. The case for $B=S^{\dagger}$ is essentially the same, implying that for $S^{1}<\alpha<2S^{(1)}$, $\frac{1}{S^{(1)}}\sum_\alpha \norm{\vv{a}}_{1}\sgn(a_{j_\alpha})\,b^{(1)}_\alpha$ is an unbiased estimator of $\Im\{\mel{\phi}{\Tilde{f}(A)}{\psi}\}$. 

Similarly, for $P=2$ note that
\begin{equation}
\Tr[O\Tilde{f}(A)\varrho\Tilde{f}^{\dagger}(A)]=\sum^{k}_{j,l}a_{j}\,a_{l}\Tr[OT_j(A)\varrho T_j(A)],
\end{equation}
which by Lemma \ref{lem:had_circuit_2} implies that
\begin{multline}
\Tr[O\Tilde{f}(A)\varrho\Tilde{f}^{\dagger}(A)]=\sum^{k}_{j,l}a_{j}\,a_{l}\,\mathbb{E}\Big[\textbf{Had}(U^{j}_{A},U^{l}_{A},O)\Big]\\
=\sum^{k}_{j,l}\sum_{b}\sum_{\omega}p_{j}\,p_{l}\,p(b,\omega|j,l)\,\norm{\vv{a}}^{2}_{1}\sgn(a_{j}a_{l})\,b\,\omega,
\end{multline}
where $b\in\{-1,1\}$ and $\omega$ in the spectrum of $O$. The last line implies that $\frac{1}{S^{(2)}}\sum_\alpha \norm{\vv{a}}^{2}_{1}\sgn(a_{j_{\alpha}})\sgn(a_{l_{\alpha}})b^{(2)}_{\alpha}\omega_{\alpha}$ with $j_\alpha,l_\alpha$ sampled from the distribution $p_j$ and $b^{(2)}_\alpha,\omega_\alpha$ sampled from the conditional distribution $p(b,\omega|j,l)$ by running the Hadamard test in Figure \fig{main}b) is an unbiased estimator of $\Tr[O\Tilde{f}(A)\varrho\Tilde{f}^{\dagger}(A)]$. 
\end{proof}
%From Eq. \eq{pj} and the definitions of $\Tilde{z}^{(1)}_\alpha$ and $\Tilde{z}^{(2)}_\alpha$, we have that for a fixed $\alpha$
%\begin{subequations}
%\begin{equation}
%\begin{split}
%\mathbb{E}\left[\Tilde{z}^{(1)}_\alpha\right]& := \sum_{j=0}^k p_j\,\norm{\vv{a}}_{1}\sgn(a_{j_{\alpha}})(b_{r,j_{\alpha}}+ib_{i,j_{\alpha}}) = \\
%&= \sum_{j=0}^k a_{j} \left( \mathbb{E}\big[b_{r,j_{\alpha}}\big] + i\,\mathbb{E}\big[b_{i,j_{\alpha}}\big]\right)\,,
%\end{split}
%\end{equation}
%\begin{equation}
%\begin{split}
%\mathbb{E}\left[\Tilde{z}^{(2)}_j\right]&:= \sum_{j,l=0}^k p_j\,p_l\,\norm{\vv{a}}_{1}^2\,\sgn(a_{j_\alpha})\sgn(a_{l_\alpha})\,b^{(2)}_{r,j_{\alpha}}\,\omega^{\phantom{(2)}}_{\alpha} = \\
%&=\sum_{j,l=0}^k a_{j}a_{l}\mathbb{E}\Big[b^{(2)}_{r,j_{\alpha}}\omega_{\alpha}\Big]
%\end{split}
%\end{equation}
%\end{subequations}  
%From the Hadamard test in fig.\ref{fig:main}.a) we have that
%\begin{equation}
%\begin{split}
%&\mathbb{E}\big[b_{r,j_{\alpha}}\big]=\Re\left\{\Tr_{a,s}[(\mathds{1}_{a}\otimes U_{\phi})U^{j}_{A}(\mathds{1}_{a}\otimes U_{\psi})\varrho_{a,s}]\right\},\\
%&\mathbb{E}\big[b_{i,j_{\alpha}}\big]=\Im\left\{\Tr_{a,s}[(\mathds{1}_{a}\otimes U_{\phi})U^{j}_{A}(\mathds{1}_{a}\otimes U_{\psi})\varrho_{a,s}]\right\},
%\end{split}
%\end{equation}
%where $\varrho_{a,s}=\ketbra{0}_{a}\otimes\ketbra{0}_{s}$. Now, from calculating
%\begin{equation}
%\begin{split}
%&\Tr[(\mathds{1}_{a}\otimes U_{\phi})U^{j}_{A}(\mathds{1}_{a}\otimes U_{\psi})\varrho_{a,s}]=\\
%&=\Tr[U^{j}_{A}\left(\ketbra{0}_{a}\otimes\ketbra{\psi}{\phi}_{s}\right)]=\\
%&=\bra{\phi}_{s} (\bra{0}_{a}\otimes\mathds{1}_{s})U^{j}_{A}(\ket{0}_{a}\otimes\mathds{1}_{s})\ket{\psi}_{s}=\\
%&=\mel{\phi}{T_j(A)}{\psi}_{s},
%\end{split}
%\end{equation}
%where we used Lemma \ref{main_lemma}, we have that
%\begin{equation}
%\begin{split}
%&\mathbb{E}\left[\Tilde{z}^{(1)}_\alpha\right]=\sum_{j=0}^k a_{j} \left( \mathbb{E}\big[b_{r,j_{\alpha}}\big] + i\,\mathbb{E}\big[b_{i,j_{\alpha}}\big]\right)=\\
%=\sum_{j=0}^k a_{j}&\left(\Re\{\mel{\phi}{T_j(A)}{\psi}\}+i\Im\{\mel{\phi}{T_j(A)}{\psi}\}\right)
%=\\
%&=\sum_{j=0}^k a_{j}\mel{\phi}{T_j(A)}{\psi}=\mel{\phi}{\Tilde{f}(A)}{\psi}.
%\end{split}
%\end{equation}

%Similarly, from the circuit description in fig.\ref{fig:main}.b) we have that
%\begin{subequations}
%\begin{align}
%&\mathbb{E}\big[\T_{j,l}^{(2)}\big[\mathds{1}\big]\big]=\Re\left[\Tr[OU^{j}_{A}\varrho_{a_{1},a_{2},s}U^{l}_{A}]\right],\\
%&\mathbb{E}\big[\T_{j,l}^{(2)}\big[S^{\dagger}\big]\big]=\Im\left[\Tr[OU^{j}_{A}\varrho_{a_{1},a_{2},s}U^{l}_{A}]\right],
%\end{align}
%\end{subequations}
%where $\varrho_{a_{1},a_{2},s}=\ketbra{0}_{a_{1}}\otimes\ketbra{0}_{a_{2}}\otimes\ketbra{\psi}_{s}$. The structure of the circuit implies that $U^{j}_{A}$ acts over subsystems $s$ and $a_{1}$, while $U^{l}_{A}$ acts over subsystems $s$ and $a_{2}$. Therefore, we have that

%\begin{widetext}
%\begin{equation}
%\begin{split}
%&\Tr[OU^{j}_{A}\varrho_{s,a_{1},a_{2}}U^{l}_{A}]=\bra{\psi}_{s}(\bra{0}_{a_{1}}\otimes\mathds{1}_{a_{2}}\otimes\mathds{1}_{s})(\mathds{1}_{a_{1}}\otimes\bra{0}_{a_{2}}\otimes\mathds{1}_{s})U^{l}_{A}OU^{j}_{A}(\ket{0}_{a_{1}}\otimes\mathds{1}_{a_{2}}\otimes\mathds{1}_{s})(\mathds{1}_{a_{1}}\otimes\ket{0}_{a_{2}}\otimes\mathds{1}_{s})\ket{\psi}_{s}=\\
%&=\bra{\psi}_{s}(\mathds{1}_{a_{1}}\otimes\bra{0}_{a_{2}}\otimes\mathds{1}_{s})U^{l}_{A}(\mathds{1}_{a_{1}}\otimes\ket{0}_{a_{2}}\otimes\mathds{1}_{s})O(\bra{0}_{a_{1}}\otimes\mathds{1}_{a_{2}}\otimes\mathds{1}_{s})U^{j}_{A}(\ket{0}_{a_{1}}\otimes\mathds{1}_{a_{2}}\otimes\mathds{1}_{s})\ket{\psi}_{s}=\Tr[OT_j(A)\ketbra{\psi}T_{l}(A)],
%\end{split}    
%\end{equation}
%\end{widetext}

%from which we obtain
%\begin{equation}
%\begin{split}
%&\mathbb{E}\left[\Tilde{z}^{(2)}_j\right]:= \sum_{j,l=0}^k p_j\,p_l\,\Tilde{z}^{(2)}_{j,l} =\\
%&= \frac{1}{q^{2}}\sum_{j,l=0}^k a_{j}a_{l} \left( \mathbb{E}\big[\T_{j,l}^{(2)}\big[\mathds{1}\big]\big] + i\,\mathbb{E}\big[\T_{j,l}^{(2)}\big[S^\dagger\big]\big]\right)\\
%&=\frac{1}{q^{2}}\sum_{j,l=0}^k a_{j}a_{l}\Tr[OT_j(A)\ketbra{\psi}T_{l}(A)]=\\
%&=\frac{1}{q^{2}}\Tr[Of(A)\ketbra{\psi}f^{\dagger}(A)]
%\end{split}
%\end{equation}
%\end{proof}

\section{Proof of Theorem \ref{main_lemma2}}%{Methods} 
\label{sec:main_lemma2_proof}
Now, we prove Theorem \ref{main_lemma2}.

\begin{proof}
Let $\Tilde{f}$ be a $\nu$-approximation to the function $f$. Then, for any two states $\ket{\psi},\ket{\phi}$ we have that
\begin{equation}
\begin{split}
&\abs{\mel{\phi}{f(A)}{\psi}-\mel{\phi}{\Tilde{f}(A)}{\psi}}\leq%\\
%&\leq\norm{\ket{\phi}}\norm{(f(A)-\Tilde{f}(A))\ket{\psi}}=\\
%&
\norm{f(A)-\Tilde{f}(A)}_{op}\le\nu.
\end{split}
\end{equation}
Here, $\norm{\cdot}_{op}$ is the operator norm, which for Hermitean operators coincides with the spectral norm $\norm{\cdot}$.%and we have used the fact that for hermitian operators $A$, $\norm{A}_{op}=\norm{A}$ where $\norm{}$ is the spectral norm. Given that 
It follows that
\begin{equation}
\begin{split}
\abs{z^{(1)}-\mathbb{E}\big[\Tilde{z}^{(1)}_{j}\big]}&\leq\abs{z^{(1)}-\Tilde{z}^{(1)}}+\abs{\Tilde{z}^{(1)}-\mathbb{E}\big[\Tilde{z}^{(1)}\big]}\\
&=\nu+\abs{\Tilde{z}^{(1)}-\mathbb{E}\big[\Tilde{z}^{(1)}\big]},
\end{split}
\end{equation}
$\Tilde{z}^{(1)}$ will be an $\epsilon$-precise estimator of $z^{(1)}$ if and only if
\begin{equation}
\abs{\Tilde{z^{(1)}}-\mathbb{E}\big[\Tilde{z}^{(1)}\big]}\le\epsilon-\nu^{(1)},
\end{equation}
where $\nu^{(1)}=\nu$. Notice from the definition of the estimator $\Tilde{z}^{(1)}$ that the range of its real and imaginary parts each is $\{-\norm{\vv{a}}_{1},\norm{\vv{a}}_{1}\}$. Therefore, Hoeffding's inequality implies that the number of samples $S^{(1)}$ necessary to attain an $\epsilon$-precise estimate with $1-\delta$ confidence is
\begin{equation}
S^{(1)}=\frac{4\norm{\vv{a}}^{2}_{1}}{(\epsilon-\nu^{(1)})^{2}}\log(\frac{4}{\delta}),
\end{equation}
where the real and imaginary parts are sampled independently. By setting the approximation error $\nu^{(1)}$ to $\epsilon/2$ we arrive at the claimed result.

In the case of problem \ref{problem.2}, we have to bound $\abs{z^{(2)}-\Tilde{z}^{(2)}}$. First, notice that
\begin{equation}
\begin{split}
&\abs{\Tr[Of(A)\varrho f^{\dagger}(A)]-\Tr[O\Tilde{f}(A)\varrho\Tilde{f}^{\dagger}(A)]}\leq\\
&\leq\abs{\Tr[O(f(A)-\Tilde{f}(A))\varrho f^{\dagger}(A)]}+\\
&+\abs{\Tr[O(f(A)-\Tilde{f}(A))\varrho(f^{\dagger}(A)-\Tilde{f}^{\dagger}(A))]}+\\
&+\abs{\Tr[Of(A)\varrho(f^{\dagger}(A)-\Tilde{f}^{\dagger}(A))]},
\end{split}
\end{equation}
where we used the triangle inequality after adding and subtracting the terms $\Tr[Of(A)\varrho\Tilde{f}(A)]$, $\Tr[Of(A)\varrho f(A)]$ and $\Tr[O\Tilde{f}(A)\varrho f(A)]$. 

By the Holder tracial matrix inequality
\begin{equation}
\abs{\Tr[AB^{\dagger}]}\leq\abs{\Tr[A]}\norm{B},
\end{equation}
we can now show that
\begin{equation}
\begin{split}
&\abs{\Tr[Of(A)\varrho f^{\dagger}(A)]-\Tr[O\Tilde{f}(A)\varrho\Tilde{f}^{\dagger}(A)]}\leq\\
&\norm{O}\norm{f(A)-\Tilde{f}(A)}\norm{f^{\dagger}(A)}+\norm{O}\norm{f(A)-\Tilde{f}(A)}^{2}+\\
&+\norm{O}\norm{f(A)-\Tilde{f}(A)}\norm{f(A)},
\end{split}
\end{equation}
where we used that $\abs{\Tr[\varrho]}=1$. Therefore, 
\begin{equation}
\abs{z^{(2)}-\Tilde{z}^{(2)}}\le\nu^{(2)}:=\nu\norm{O}\left(2\norm{f(A)}+\nu\right).
\end{equation}
%implying that $\nu^{(2)}$ is the right-hand side of the previous equation.

Similarly to the first case, $\Tilde{z}^{(2)}_{j}$ will be an $\epsilon$-precise estimate of $z^{(2)}$ if and only if
\begin{equation}
\abs{\Tilde{z}^{(2)}-\mathbb{E}\Big[\Tilde{z}^{(2)}\Big]}\le\epsilon-\nu^{(2)}.
\end{equation}
Since the range of possible values of $\Tilde{z}^{(2)}$ lie in $\{-\norm{O}\norm{\vv{a}}^{2}_{1},\norm{O}\norm{\vv{a}}^{2}_{1}\}$, Hoeffding's inequality implies that the number of samples necessary to attain an $\epsilon$-precise estimate with $1-\delta$ confidence is
\begin{equation}
S_{2}=\frac{2\norm{O}^{2}\norm{\vv{a}}^{4}_{1}}{(\epsilon-\nu^{(2)})^{2}}\log(\frac{2}{\delta}).
\end{equation}
The claim then follows by setting the approximation error $\nu^{(2)}=\epsilon/2$.
\end{proof}

\section{Error robustness analysis} \label{sec:error_analysis_proof}

%\sout{Let us consider that we have access to a faulty controlled iterate $c\Tilde{U}_{A}:=\Lambda\,\circ\, cU_{A}$} \LA{notation inconsistent. $\Lambda$ is a super-operator while controlled-$U_{A}$ is an operator. Here it's better not to introduce any notation for the faulty iterate, we simply need to define $\Lambda$, the rest can be stated with words. If needed in the appendix, define another symbol, like $\mathcal{E}_\text{iter}$ for the faulty iterate and $U_\text{iter}$ for the ideal one, e.g.}

\begin{lemma}\label{lem:dep_chanel}
Let $U$ be any unitary operator over $\mathcal{H}_{1}\otimes\mathcal{H}_{a}\otimes\mathcal{H}_{s}$. Then, if $\Lambda$ is the full depolarization channel with strength $p$, we have that, for any $\sigma\in\mathcal{H}_{1}\otimes\mathcal{H}_{a}\otimes\mathcal{H}_{s}$
\begin{equation}
(\Lambda\circ U)[\sigma]=(U\circ\Lambda)[\sigma],
\end{equation}
and, in particular,
\begin{equation}
(\Lambda\circ U)^{j}[\sigma]=U^{j}\circ\Lambda^{j}[\sigma],
\end{equation}
where $U[\sigma]$ is the action of $U$ as a unitary channel over $\sigma$.
\end{lemma}

\begin{proof}
First, we have that
\begin{equation}
\begin{split}
\Lambda\circ U[\sigma]&=\Lambda[\left(U\sigma U^{\dagger}\right)]=\\
&=(1-p)\left(U\sigma U^{\dagger}\right)+p\frac{\mathds{1}}{D_\text{tot}}=\\
&=U\left((1-p)\sigma+p\frac{\mathds{1}}{D_\text{tot}}\right)U^{\dagger}=\\
&=U\circ\Lambda[\sigma],
\end{split}
\end{equation}
where $D_\text{tot}$ is the dimension of $\mathcal{H}_{1}\otimes\mathcal{H}_{a}\otimes\mathcal{H}_{s}$. Therefore, since $\Lambda$ and $U$ commute as channels, we can pass all $U$ to the left of the $\Lambda$ copies, proving the lemma.
\end{proof}

%\begin{corollary}
%Let $U^{\Lambda}=\Lambda\circ U$, then
%\begin{equation}
%\left(U^{\Lambda}\right)^{j}[\sigma]=(1-p)^{j}U[\sigma]+\big(1-(1-p)^{j}\big)\frac{\mathds{1}}{D_\text{tot}}.
%\end{equation}
%\end{corollary}

%\begin{proof}
%Just apply the previous lemma and use the fact that
%\begin{equation}
%\Lambda^{j}[\sigma]=(1-p)^{j}\sigma+(1-(1-p)^{j})\frac{\mathds{1}}{D_\text{tot}}.
%\end{equation}
%\end{proof}

\begin{lemma}
Let $\textbf{Had}(U^{j}_{A},\Lambda,\ket{\phi},\ket{\psi})$ be the random variable obtained by the Hadamard test described in fig.\ref{fig:main}a) when we use $\Lambda\circ CU_{A}$ in the place of $CU_{A}$. Then, for $B=\mathds{1}$
\begin{equation}
\mathbb{E}\Big[\textbf{Had}(U^{j}_{A},\Lambda,\ket{\phi},\ket{\psi})\Big]=(1-p)^{j}\Re\{\mel{\phi}{T_j(A)}{\psi}\},
\end{equation}
and for $B=S^{\dagger}$
\begin{equation}
\mathbb{E}\Big[\textbf{Had}(U^{j}_{A},\Lambda,\ket{\phi},\ket{\psi})\Big]=(1-p)^{j}\Im\{\mel{\phi}{T_j(A)}{\psi}\}.
\end{equation}
\end{lemma}

\begin{proof}
Note that, by fig.\ref{fig:main}.a)
\begin{equation}
\mathbb{E}\Big[\textbf{Had}(U^{j}_{A},\Lambda,\ket{\phi},\ket{\psi})\Big]=\Tr[X_{1}\left(N[\ketbra{+}_{1}\varrho_{a,s}]\right)],
\end{equation}
where 
\begin{equation}
N=\Tilde{C}U_{\phi}\circ{(\Lambda\circ CU_{A})}^{j}\circ CU_{\psi}\circ B,
\end{equation}
and $\varrho_{a,s}=\ketbra{0}_{a}\ketbra{0}_{s}$. Now, using lem.\ref{lem:dep_chanel} we have that
\begin{equation}
\begin{split}
N&=\Tilde{C}U_{\phi}\circ CU_{A}^{j}\circ\Lambda^{j}\circ CU_{\psi}\circ B=\\
&=\Tilde{C}U_{\phi}\circ CU_{A}^{j}\circ CU_{\psi}\circ B\circ\Lambda^{j}=G\circ\Lambda^{j},
\end{split}
\end{equation}
where $G$ is the unitary channel $G[\sigma]=G\sigma G^{\dagger}$ with $G=\Tilde{C}U_{\phi}CU^{j}_{A}CU_{\psi}B$. 
Therefore, 
\begin{equation}
\begin{split}
&N[\ketbra{+}_{1}\varrho_{a,s}]=G\circ\Lambda^{j}[\ketbra{+}_{1}\varrho_{a,s}]=\\
=&(1-p)G(\ketbra{+}_{1}\varrho_{a,s})G^{\dagger}+(1-(1-p)^{j})\frac{\mathds{1}}{D_\text{tot}}.
\end{split}
\end{equation}
Now, by the proof of lem.\ref{lem:had_circuit_1}, and the fact that $X_{1}$ is trace-less, we have that
\begin{equation}
\begin{split}
\mathbb{E}\Big[\textbf{Had}(U^{j}_{A},\Lambda,\ket{\phi},\ket{\psi})\Big]=(1-p)^{j}\mathbb{E}\Big[\textbf{Had}(U^{j}_{A}\ket{\phi},\ket{\psi})\Big],
\end{split}
\end{equation}
which is equivalent to the statement of the lemma given any choice of $B$
\end{proof}

Now, we are ready for the proof of Theorem.\ref{thm:error_robust}.

\begin{proof}
By definition we have that
\begin{equation}
\abs{\mathbb{E}\big[\Tilde{z}^{(1)}\big]-\mathbb{E}\big[\Tilde{z}^{(1),\Lambda}\big]}=\abs{\sum^{k}_{j=0}a_{j}(1-(1-p)^{j})\mel{\phi}{T_j(A)}{\psi}}.
\end{equation}
By Bernouli's inequality $1-(1-p)^{j}\leq pj$ for $p<1$, implying
\begin{equation}
\abs{\mathbb{E}\big[\Tilde{z}^{(1)}\big]-\mathbb{E}\big[\Tilde{z}^{(1),\Lambda}\big]}\leq p\abs{\sum^{k}_{j=0}j\,a_{j}\mel{\phi}{T_j(A)}{\psi}}.   
\end{equation}
Now, note that for any $\ket{\phi},\ket{\psi}$ we have that
\begin{equation}
\abs{\mel{\phi}{T_j(A)}{\psi}}\leq1,
\end{equation}
therefore, by the triangle inequality
\begin{equation}
\abs{\mathbb{E}\big[\Tilde{z}^{(1)}\big]-\mathbb{E}\big[\Tilde{z}^{(1),\Lambda}\big]}\leq p\sum^{k}_{j=0}j\,\abs{a_{j}}=p\norm{\vv{a}}_{1}\mathbb{E}[j].
\end{equation}
\end{proof}

\begin{definition}
Given an hermitian operator $A$ with $\norm{A}\leq1$, an $\varepsilon$-approximate block encoding of $A$ is a unitary operator $U_{A,\varepsilon}$ satisfying
\begin{equation}
\norm{(\bra{0}_{a}\otimes\mathds{1}_{s})U_{A,\varepsilon}(\ket{0}_{a}\otimes\mathds{1}_{s})-A}_{tr}<\varepsilon,
\end{equation}
where $\norm{\,\,}_{tr}$ is the trace norm.
\end{definition}

\begin{lemma}
Let $U_{A,\varepsilon}$ be an $\varepsilon$-approximate block-encoding of $A$, and let $U_{A}$ be an exact block encoding of $A$. If $z^{(1)}$ is as in eq.[\ref{eq:ztildeP}] and $z^{(1),\varepsilon}$ is the unbiased estimator obtained from the hadamard test in fig.\ref{fig:main}.a) when exchanging $CU^{j}_{A}$ by $CU^{j}_{A,\varepsilon}$,
then 
\begin{equation}
\abs{\mathbb{E}\big[\Tilde{z}^{(1)}\big]-\mathbb{E}\big[\Tilde{z}^{(1),\varepsilon}\big]}\leq\, p\,\norm{\vv{a}}_{1}\,\mathbb{E}[j]\,,
\end{equation}
\end{lemma}

\begin{proof}
If $U_{A,\varepsilon}$ is an $\varepsilon$-approximate block-encoding then,
\begin{equation}
\norm{(\bra{0}_{a}\otimes\mathds{1}_{s})U^{j}_{A,\varepsilon}(\ket{0}_{a}\otimes\mathds{1}_{s})-T_{j}(A)}_{tr}\le j\,\varepsilon,
\end{equation}
which implies that
\begin{equation}
\abs{(\bra{0}_{a}\bra{\phi}_{s})U^{j}_{A,\varepsilon}(\ket{0}_{a}\ket{\psi}_{s})-\mel{\phi}{T_{j}(A)}{\psi}}\le j\,\varepsilon.
\end{equation}
Therefore,
\begin{equation}
\begin{split}
&\abs{\mathbb{E}\big[\Tilde{z}^{(1)}\big]-\mathbb{E}\big[\Tilde{z}^{(1),\varepsilon}\big]}=\\
=&\abs{\sum^{k}_{j=0}a_{j}\left((\bra{0}_{a}\bra{\phi}_{s})U^{j}_{A,\varepsilon}(\ket{0}_{a}\ket{\psi}_{s})-\mel{\phi}{T_{j}(A)}{\psi}\right)}\leq\\
&\leq\varepsilon\abs{\sum^{k}_{j=0}ja_{j}}\leq\varepsilon\norm{\vv{a}}_{1}\mathbb{E}[j].
\end{split}
\end{equation}
%\AD{couldn't align the equations in better way :/}
\end{proof}

\section{Specific function approximation results} \label{sec:explicit_examples}

\begin{table*}[htpb!]
    \centering
    \begin{tabular}{||c||c|c|c|c||}
     \hline
     \rule{0pt}{3ex}
     \rule[-1.5ex]{0pt}{0pt}
     {\bf Function} & {\bf Domain} & {\bf Truncation order ($k$)} & {\bf $\norm{\vv{a}}_{1}$} & {\bf $\norm{f}_\infty$} \\
     \hline
     \hline
     \rule{0pt}{3ex}
     \rule[-1.5ex]{0pt}{0pt}
     $x^{t}$ ($t\in\mathbb{N}$) & $[-1,1]$ & $\sqrt{2t\,\log\frac{2}{\nu}}$ & $\mathcal{O}(1)$ & $\mathcal{O}(1)$\\
     \hline
     \rule{0pt}{4ex}
     \rule[-2ex]{0pt}{0pt}
     $e^{-\beta x/2}$ ($\beta\in\mathbb{R}$) & $[-1,1]$ & $\sqrt{2\,\max\left\{\frac{e^2\beta}{2},\log\Big(\frac{2\, e^{\beta/2}}{\nu}\Big)\right\}\,\log\Big(\frac{4\,e^{\beta/2}}{\nu}\Big)}$ & $\mathcal{O}(e^{\beta/2})$ & $\mathcal{O}(e^{\beta/2})$ \\
     \hline
     \rule{0pt}{4ex}
     \rule[-1.5ex]{0pt}{0pt}
     $x^{-1}$ & $[-1,-1/\kappa]\cup[1/\kappa,1]$ &  $\quad 2\kappa\,\sqrt{\log\frac{\kappa}{\nu} \, \log(\frac{4\kappa^{2}}{\nu}\log\frac{\kappa}{\nu})}+1\quad $ & $\mathcal{O}\left(\kappa\sqrt{\log\frac{\kappa}{\nu}}\right)$ & $\mathcal{O}(\kappa)$ \\
     \hline
     \rule{0pt}{3ex}
     \rule[-1.5ex]{0pt}{0pt}
     $\Theta(x)$ & $[-1,-\mu]\cup[\mu,1]$ & $\mathcal{O}\left(\frac{1}{\mu}\log\frac{1}{\nu}\right)$ & {$\mathcal{O}\left(\log(\frac{1}{\mu}\log\frac{1}{\nu})\right)$} & $\mathcal{O}(1)$ \\
     \hline
    \end{tabular}
    \caption{Chebyshev $\nu$-approximation data (approximation interval $\mathcal{I}$, truncation degree $k$ and coefficient 1-norm $\norm{\vv{a}}_1$) for our exemplary functions. %$f(x)=x^n$, $f(x)=e^{-\beta x}$, $f(x)=x^{-1}$, and $f(x)=\Theta(x)$ (the step function) in the interval $D_\text{tot}:=[-1,1]$. 
    When a singular point is present at the origin, the approximation interval excludes a ball of fixed radius around it. 
    For comparison, we also provide the $\infty$-norm $\norm{f}_\infty:=\max_{x\in I} |f(x)|$, which in the matrix function setting plays the role of spectral norm $\norm{f(A)}$ of the matrix $f(A)$. Namely, the difference between $\norm{\vv{a}}_1$ and $\norm{f}_\infty$ translates into the difference between sample complexities between our semi-quantum algorithm and the corresponding fully-quantum one.}
    \label{tab:chebyshev_data}
\end{table*}


The following general lemma will be useful to compute the coefficient 1-norm $\norm{\vv{a}}_1$ in some of the specific examples.

\begin{lemma}\label{lemma_1norm}
% Notice that, since $T_j(1)=1$ %for all $j$ 
% and $T_j(-x)=(-1)^j\, T_j(x)$ for all $j$, one has $\norm{\vv{a}}_1=|\Tilde{f}(1)|$ if all $a_j$'s have the same sign and $\norm{\vv{a}}_1=|\Tilde{f}(-1)|$ if they alternate signs   
Let $\vv{a}=\{a_0,\ldots,a_k\}$ be the vector of Chebyshev coefficients of the polynomial $\Tilde{f}$ as in \eq{ftilde}. Then $\norm{\vv{a}}_1=|\Tilde{f}(1)|$ if all $a_j$'s have the same sign and $\norm{\vv{a}}_1=|\Tilde{f}(-1)|$ if they alternate signs. 
\end{lemma}

\begin{proof}
First notice that if all the coefficients have the same sign (respectively, opposite signs) then either $a_j=|a_j|$ or $a_j=-|a_j|$ for all $j$ (respectively, either $a_j=(-1)^j |a_j|$ or $a_j=(-1)^{j+1} |a_j|$ for all $j$). 
The claim then follows immediately from the fact that $T_j(1)=1$ and $T_j(-1)=(-1)^j$ for all $j$. 
\end{proof}






\subsection{Monomial function} \label{sec:explicit_examples_monomials}

\subsubsection{Polynomial approximation and its parameters}
The following lemma gives the Chebyshev approximation to the monomial function that we use to prove the complexity of our algorithm. The lemma is a version of \cite[Lemma 3.3]{Vishnoi2013} with explicit coefficients.
\begin{lemma}\label{lem:approx_mon}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be such that $f(x)=x^{t}$ for some $t\in\mathbb{N}$. Then, the function $\Tilde{f}:[-1,1]\rightarrow\mathbb{R}$ given by the Chebyshev polynomial 
\begin{equation}
\Tilde{f}(x)=
 2^{1-t} \sideset{}{'}\sum^{k}_{\substack{j=0\\ t-j\text{ even}}}\binom{t}{(t-j)/2}T_{j}(x),
\end{equation}
is a $\nu$-approximation of $f$ on $[-1,1]$, if and only if 
\begin{equation}
k\geq\sqrt{2t\log(\frac{2}{\nu})}.    
\end{equation}
\end{lemma}

\begin{lemma}
Let $\textbf{a}_\text{mon}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_\text{mon}$. Then the one-norm is
\begin{equation}
\norm{\textbf{a}_\text{mon}}_{1}=1-\nu.%\sim\erf\left\{\sqrt{\log(\frac{2}{\nu})}\right\}<1,
\end{equation}
\end{lemma}
\begin{proof}
Follows immediately from Lemma \ref{lemma_1norm}.
\end{proof}

\begin{lemma}\label{lem:av_Q_mon}
Let $\textbf{a}_\text{mon}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_\text{mon}$. Then, the average query complexity $q^{(1)}$ of estimating $\Tilde{z}^{(1)}$ is
\begin{equation}
%Q_{average}
{q^{(1)}}\leq\sqrt{\frac{t}{2\pi}}\left(\frac{2-\nu}{1-\nu}\right)+1\,.
\end{equation}
\end{lemma}

\begin{lemma}
Let $\textbf{a}_\text{mon}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_\text{mon}$. Then, the error damping factor for estimating $\Tilde{z}^{(1)}$ is
\begin{equation}
E_{damping}\leq\frac{2-\nu}{(1-\nu)2\sqrt{\pi}\sqrt{\log(\frac{2}{\nu})}},
\end{equation}
asymptotically on $t$.
\end{lemma}

%Show average query complexity (calculated using a gaussian approximation): $Q_{average}=\sqrt{t/2\pi}(2-\nu)/(1-\nu)$, notice that when $\nu\rightarrow0$, $Q_{average}=2\sqrt{t/2\pi}$ which is constant in $\nu$, while $k\rightarrow\infty$.

%Comment that 
%$$\frac{Q}{k}=\frac{\sqrt{1/2\pi}(2-\nu)/(1-\nu)}{\sqrt{2\ln{2/\nu}}}\approx\frac{1}{\sqrt{\pi\log(2/\nu)}}$$ 
%since this was mentioned in the main text to link with the histogram in Fig \fig{main}.

\subsubsection{Problem statement and complexities of the solution}

We  consider a  reversible and ergodic Markov chain on $N=2^n$ discrete states with row stochastic transition matrix $\mathcal{P}$ satisfying $\sum_{k}\mathcal{P}_{jk}=1$ whose stationary state, i.e the eigenstate corresponding to eigenvalue $\lambda=1$ has the Gibbs amplitudes $\bpi=(e^{-\beta E_0}/Z_\beta,\cdots,e^{-\beta E_{D-1}}/Z_\beta)$, with $\{E_l\}_{l\in[D]}$ the eigenvalues of the Hamiltonian $H$. 
The eigenvalues $\lambda$ of $\mathcal{P}$ lie in the interval $(-1,1]$ with $\lambda=1$ non-degenerate.
The so-called {\it discriminant matrix} $A$ is defined by $A_{jk}=\sqrt{\mathcal{P}_{jk}\mathcal{P}_{kj}}$. For the sake of our algorithm, the relevant properties of $A$ are Hermiticity and the fact that $A$ has the same spectrum of $\mathcal{P}$ with $\sqrt{\bpi}$ the eigenvector corresponding to eigenvalue $\lambda=1$ \cite{camilo_inprep}.
In particular,  $A^t$ for $t=\mathcal{O}(\Delta^{-1})$ becomes approximately a projector over the state $\sqrt{\bpi}$ as we state
in the following lemma (here $\Delta=\min_{\{\lambda\}/1}(1-|\lambda|)$ is  the spectral gap of $A$). 

\begin{lemma}[Determining the monomial degree]\label{lemma_partition_function_power_method}
Let $\nu'>0$, $A$ a Markov-chain discriminant matrix with spectral gap $0<\Delta<1$, and $\ket{\vv{y}}$ a spin configuration with energy $E_{\vv{y}}$. For any integer $t\ge\frac{\log(1/\nu')}{\Delta}$, it holds that 
\begin{align}
\left|\!\mel{\vv{y}}{A^t}{\vv{y}} - \mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}\right| \le \nu'
\end{align}
\end{lemma}

\begin{proof}
Let $A=\sum_{\lambda}\lambda\ketbra{\lambda}{\lambda}=\Pi_{\vv{\pi}}+\sum_{\lambda\ne1}\lambda\ketbra{\lambda}{\lambda}$ be the spectral decomposition of $A$, where $\lambda$ denote its eigenvalues, $\{\ket{\lambda}\}$ the corresponding eigenstates, and $\Pi_{\vv{\pi}}=\ketbra{\sqrt{\bpi}}$ is the projector onto the "purified" Gibbs state. It follows that
\begin{align}
\left|\!\mel{\vv{y}}{A^t-\Pi_{\vv{\pi}}}{\vv{y}}\right| 
&\le \sum_{\lambda\ne1}|\lambda|^t\!\cdot\!|\!\braket{\vv{y}}{\lambda}\!|^2 \notag\\
&\le (1-\Delta)^t.
\end{align}
Hence, given some tolerated error $\nu'$, one can ensure that $\left|\!\mel{\vv{y}}{A^t}{\vv{y}} - \mel{\vv{y}}{\Pi_{\vv{\pi}}}{\vv{y}}\right| \le \nu'$ for any
\begin{align}\label{eq:tbound}
t \ge \frac{\log 1/\nu'}{\Delta}\ge \frac{\log 1/\nu'}{\log(1-\Delta)^{-1}} \,.
\end{align}
\end{proof}

\begin{lemma}\label{lem:estimate_z} Let $\tilde{z}^{(1)}$ be an estimate to $\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}$ up to additive error $\epsilon<\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}/2$.  If $\epsilon\leq \epsrel\big(2\,e^{\beta E_{\vv{y}}}Z_\beta\big)$, then $\Tilde{Z}_\beta:=\frac{e^{-\beta E_{\vv{y}}}}{\tilde{z}^{(1)}}$ is an estimate for the partition function $Z_\beta$ such that
\begin{equation}\label{eq:rel_error}
    \big|\Tilde{Z}_\beta-Z_\beta\big|<\epsrel Z_\beta.
\end{equation}
\end{lemma}
\begin{proof}
The estimate $\tilde{z}^{(1)}$ satisfies $|\tilde{z}^{(1)}-\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}|\leq \epsilon$. It follows straightforwardly that 
    \begin{equation}
\begin{split}
\frac{e^{-\beta E_{\vv{y}}}}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}&\left(1+\frac{\epsilon}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}\right)^{-1}
<\frac{e^{-\beta E_{\vv{y}}}}{\Tilde{z}^{(1)}}\\
&\qquad<\frac{e^{-\beta E_{\vv{y}}}}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}\left(1-\frac{\epsilon}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}\right)^{-1}.
\end{split}    
\end{equation}
Now, using that $1-2\xi\leq(1+\xi)^{-1}$ and $1+2\xi\geq(1-\xi)^{-1}$ for $0\leq \xi\leq1/2$, we can derive the inequality:
\begin{equation}
    \begin{split}
\frac{e^{-\beta E_{\vv{y}}}}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}&\left(1-2\frac{\epsilon}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}\right)
<\frac{e^{-\beta E_{\vv{y}}}}{\Tilde{z}^{(1)}}\\
&\qquad<\frac{e^{-\beta E_{\vv{y}}}}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}\left(1+2\frac{\epsilon}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}\right),
\end{split}    
\end{equation}
from which we obtain \eqref{eq:rel_error} by using $Z_\beta=\frac{e^{-\beta E_{\vv{y}}}}{\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}}$ and setting $\epsilon\leq \epsrel\big(2\,e^{\beta E_{\vv{y}}}Z_\beta\big)$.
\end{proof}

\begin{lemma}[Complexity]\label{lem:complexity_mon}
    Let $\epsrel>0$ and $\nu'=\nu/2=\frac{\epsrel}{12\,e^{\beta E_{\vv{y}}}Z_\beta}$. Let $\tilde{z}^{(1)}$ be the estimate obtained by Algorithm 1 with inputs $\ket{\psi}=\ket{\phi}=\ket{\vv{y}}$, error $\frac{\epsrel}{2\,e^{\beta E_{\vv{y}}}Z_\beta}$, and $\tilde{f}(A)$ a $\nu$-approximation to $A^t$ with $t=\frac{\log(1/\nu')}{\Delta}$. Then $\Tilde{Z}_\beta:=\frac{e^{-\beta E_{\vv{y}}}}{\tilde{z}^{(1)}}$ is an estimate to the partition function $Z_\beta$ up to relative error $\epsrel$. Moreover, the algorithm has maximal query depth $k=\sqrt{\frac{2}{\Delta}}\Big(\beta E_{\vv{y}}+\log\frac{12\,Z_\beta}{\epsrel}\Big)$ and sample complexity $S^{(1)}=64\,e^{2\beta E_{\vv{y}}} Z_\beta^2\,\frac{\log(2/\delta)}{\epsrel^2}$.
\end{lemma}
\begin{proof}
    The estimate $\tilde{z}^{(1)}$ has three sources of error relatively to $\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}$. First, according to Lemma \ref{lemma_partition_function_power_method}, the fact that $t$ is finite causes an error $\nu'$. Second, only an approximation of $A^t$ is implemented, causing an error $\nu$. Finally, the finite number of samples incurs a statistical error $\epsilon_{st}$. Therefore, by triangular inequality, we have $|\tilde{z}^{(1)}-\mel{\vv{y}}{\Pi_{\bpi}}{\vv{y}}|\leq(\nu'+\nu+\epsilon_{st}):=\epsilon$. According to Lemma \ref{lem:estimate_z}, $\Tilde{Z}_\beta=\frac{e^{-\beta E_{\vv{y}}}}{\tilde{z}^{(1)}}$ is an estimate of $Z_\beta$ with relative error $\epsrel$ if the total error is chosen to be $\epsilon= \epsrel/\big(2\,e^{\beta E_{\vv{y}}}Z_\beta\big)$. Since each source of error is controlled individually, for convenience, we take $\nu'=\nu/2=\frac{\epsrel}{12\,e^{\beta E_{\vv{y}}}Z_\beta}$ and $\epsilon_{st}=\frac{\epsrel}{4\,e^{\beta E_{\vv{y}}}Z_\beta}$. The sample complexity is then given by Eq.\ \eqref{eq:SP1} with $\|\vv{a}\|_1=1-\nu$ and error $\epsilon$, while the maximal query depth is given by substituting the expression for $t$ in the expression for the truncation order $k=\sqrt{2\,t\,\log(2/\nu)}$.
\end{proof}




\subsection{Exponential function} \label{sec:explicit_examples_qite}

The following Lemma gives the explicit coefficients of the approximation used here and its maximal degree. It is obtained from Ref.\ \cite[Lemma 4.2]{sachdeva_approximation_2013}.
%\begin{lemma}
%Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be such that $f(x)=e^{-\beta x}$. Then, the function $\Tilde{f}_{exp}:[-1,1]\rightarrow\mathbb{R}$ given by the Chebyshev polynomial 
%\begin{equation}
%\Tilde{f}_{exp}(x):=\sum^{d}_{i=0}\frac{(-\beta)^{i}}{i!}\sideset{}{'}\sum^{k}_{\substack{j=0\\ i-j\text{ even}}}2^{1-i}\binom{i}{(i-j)/2}T_{j}(x),
%\end{equation}
%is a $\nu$-approximation of $e^{-\beta x}$ on $[-1,1]$ if
%\begin{equation}
%d\geq\left\lceil\max\left\{\beta e^{2},\log(\frac{2}{\nu})\right\}\right\rceil\text{, and }k\geq\left\lceil\sqrt{2d\log(\frac{4}{\nu})}\right\rceil.
%\end{equation}
%\end{lemma}

\begin{lemma}[Chebyshev approximation]\label{lem:approx_exp}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be such that $f(x)=e^{-\beta x/2}$. Then, for $\nu\leq \frac{e^{\beta/2}}{2}$, the  polynomial  $\Tilde{f}_{exp}:[-1,1]\rightarrow\mathbb{R}$ defined by 
\begin{equation}\label{eq:serie_exp}
\Tilde{f}_{exp}(x):=\sideset{}{'}\sum^{k}_{j=0}\left[\sum^{t}_{\substack{l=j\\ l-j\text{ even}}}\frac{(-\beta/2)^{l}}{l!}2^{1-l}\binom{l}{(l-j)/2}\right] T_{j}(x),
\end{equation}
is a $\nu$-approximation of $f(x)$ on $[-1,1]$ if
$t\geq\left\lceil\max\left\{\frac{\beta e^{2}}{2},\log(\frac{2\,e^{\beta/2}}{\nu})\right\}\right\rceil$, and $k\geq\left\lceil\sqrt{2\,t\log(\frac{4\,e^{\beta/2}}{\nu})}\right\rceil.$
\end{lemma}

\begin{lemma}[$l_1$-norm of coefficients]\label{lem:1norm_exp}
Let $\vv{a}_{exp}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_{exp}$. Then the one-norm is
\begin{equation}
\norm{\vv{a}}_{1}= e^{\beta/2}\,G(t,\beta/2)\,(1-\nu/2)\leq e^{\beta/2}+\nu,
\end{equation}
where $G(t,\beta/2)$ is the regularized incomplete Gamma function.
\end{lemma}

\begin{proof}
 The equality comes from $G(t,\beta/2)$ being the cumulative distribution function of a Poisson distributed variable with mean $\beta/2$ at point $t$. Moreover, the coefficients of $\Tilde{f}_{exp}$ in Eq.\ \eqref{eq:serie_exp} alternate sign. Hence,  Lem.\ref{lemma_1norm} implies the inequality.
\end{proof}

\begin{lemma}[Average query depth]\label{lem:query_exp}
Let $\vv{a}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_{exp}$. Then, the average query complexity $q^{(2)}$ of estimating $\Tilde{z}^{(2)}$ satisfies
\begin{equation}
q^{(2)}\leq\frac{\left(2-\frac{\nu}{2}\right)\sqrt{\beta/2}}{\sqrt{2\pi}G(t,\beta/2)}<0.8\sqrt{\beta/2}  \,.
\end{equation}
\end{lemma}

\begin{lemma}
Let $\vv{a}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_{exp}$. Then, the error damping factor for estimating $\Tilde{z}^{(2)}$ is
\begin{equation}
E_{damping}\sim\frac{\left(2-\frac{\nu}{2}\right)\sqrt{\beta/2}}{\sqrt{2\pi}G(t,\beta/2)k}.
\end{equation}
For $\beta\rightarrow\infty$, we have that
\begin{equation}
E_{damping}\sim \frac{4-\nu}{2e\sqrt{\pi}\sqrt{\log(\frac{4}{\nu})}} \,.
\end{equation}
\end{lemma}

\subsection{Inverse function} \label{sec:explicit_examples_inverse}

The following Lemma gives the explicit coefficients of
the approximation used here and its maximal degree. It
is obtained from Ref. \cite{childs_quantum_2017}.

\begin{lemma}\label{lem:approx_inv}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be such that $f(x)=1/x$. Then, the function $\Tilde{f}_{inv}:[-1,1]\rightarrow\mathbb{R}$ given by the Chebyshev polynomial
\begin{equation}
\Tilde{f}_{inv}(x)=4\sum_{j=0}^{k}(-1)^{j}\left[\frac{{\sum}^{b}_{i=j+1}\binom{2b}{b+i}}{2^{2b}}\right]T_{2j+1}(x),
\end{equation}
is $\nu$-approximation of $1/x$ on the interval $[-1,-1/\kappa)\cup(1/\kappa,1]$ if 
\begin{equation}
    k\geq\sqrt{b\log(\frac{4b}{\nu})}\text{, with }b\geq\kappa^{2}\log(\frac{\kappa}{\nu}) \,.
\end{equation}
\end{lemma}

\begin{lemma}\label{lem:1norm_inv}
Let $\vv{a}_{inv}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_{inv}$. Then the one-norm is
\begin{equation}
\norm{\vv{a}_{inv}}_{1} \le \frac{2}{\sqrt\pi}\sqrt{b+1} =\mathcal{O}\left(\kappa\,\sqrt{\log(\frac{\kappa}{\nu})}\right).
%\sim\frac{2}{\sqrt{\pi}}\left(1-\frac{\nu}{\kappa^{2}\log(\frac{\kappa}{\nu})}\right)\kappa\sqrt{\log(\frac{\kappa}{\nu})}
\end{equation}
% \GC{I can compute it exactly without using integral, 
% \begin{align}
% \norm{\vv{a}_\text{inv}} &= \frac{1}{2^{2b-1}}\left[(b+1)\binom{2b}{b+1}-\binom{2b}{b+k+2}\right.\notag\\
% &\left.\Big(b+k+2-2(k+1)\, _2F_1(1,2-b+k,3+b+k,-1)\Big)\right]
% \end{align}
% and using some inequalities show that $\norm{\vv{a}_\text{inv}}\le\frac{2}{\sqrt\pi}\sqrt{b+1}$
% }
\end{lemma}

% \begin{proof}
% Follows closely the proof used in \cite{gribling2021improving} for a slightly different Chebyshev series. First
% \end{proof}

\begin{lemma}
Let $\vv{a}_{inv}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_{inv}$. Then, the average query complexity of estimating $\Tilde{z}^{(1)}$ is $\mathcal{O}\left(\kappa\,\sqrt{\log(\frac{\kappa}{\nu})}\right)$.
% bounded from above by
% \begin{equation}
% Q_{average}\leq \kappa\sqrt{\log\frac{\kappa}{\nu}} + \frac{1}{\kappa}\sqrt{\frac{\nu\log\left(\frac{4\kappa^2}{\nu}\log\frac{\kappa}{\nu}\right)}{\log\frac{\kappa}{\nu}}}+1
% \end{equation}
\end{lemma}

\begin{lemma}
Let $\vv{a}_{inv}$ be the vector of coefficients of the Chebyshev polynomial $\Tilde{f}_{inv}$. Then, the error damping factor for estimating $\Tilde{z}^{(1)}$ is upper bounded by
\begin{equation}
E_{damping}\leq\frac{\sqrt{\kappa^2\log\left(\frac{\kappa}{\nu}\right)}+\sqrt{\frac{\nu\log\left(\frac{4\kappa^2\log\left(\frac{\kappa}{\nu}\right)}{\nu}\right)}{\kappa^2\log\left(\frac{\kappa}{\nu}\right)}}+1}{\sqrt{\kappa^{2}\log(\frac{\kappa}{\nu})\log(\frac{4(\kappa^{2}\log(\frac{\kappa}{\nu}))}{\nu})}}\,.
\end{equation}
\end{lemma}

% Show equation 
% \begin{equation}
% \Tilde{f}_{inv}(x)=4\sum_{j=0}^{k}(-1)^{j}\left[\frac{{\sum}^{\kappa}_{i=j+1}\binom{2\kappa}{\kappa+i}}{2^{2\kappa}}\right]T_{2j+1}(x)
% \end{equation}
% with $k=O(\kappa\log\frac{\kappa}{\nu})$ and $\norm{\vv{a}}_{1}=O(\kappa\log\frac{\kappa^{2}}{\nu})$. 

% \subsubsection{Problem statement and complexities of the solution}

% \GC{organize the following in some sensible way (copy-pasted from the notes)}
% With this approximation, we can use use eq.\ref{eq: sample_problem_2} with $\kappa=\norm{A^{-1}}$ and the results of \cite{wang2023qubitefficient} to argue that \LA{All this discussion connected to Eq. (47) and the derivation of the next statement should go to the proof of the (here-missing) lemma in the App.} the sample complexity for estimating $q$ to the relevant accuracy is 
% \begin{equation}
% S_{norm-est}=O\left(\frac{\norm{A^{-1}}^{4}}{q^{4}\epsilon^{2}}\log^{2}\left(\frac{\norm{A^{-1}}}{q\epsilon}\right)\log(\frac{2}{\delta})\right),
% \end{equation}
% while the estimation of the target quantity itself has sample complexity of
% \begin{equation}
% S_{lin-inv}=O\left(\frac{\norm{A^{-1}}^{2}}{q^{2}\epsilon^{2}}\log\left(\frac{\norm{A^{-1}}^{2}}{q^{2}\epsilon}\right)\log(\frac{2}{\delta})\right)
% \end{equation}
% \LA{How can the estimation of $q^2$ be more expensive than the estimation of the target quantity? In any case, I think we should present only the runtime of the target quantity. The runtime for estimating $q^2$ will automatically be covered by the estimation of $O$ (for the case of $O=\openone$)}

% \LA{In the lemmas, present only query depth and total-runtime. The sample complexities can be presented in the prooves to the lemas}

% The maximal query complexity for each run of algorithms \ref{alg: problem 2} and \ref{alg: problem 1} are respectively,
% \begin{subequations}
% \begin{align}
% Q^{max}_{norm-est}&=\norm{A^{-1}}\log(\frac{\norm{A^{-1}}}{q\epsilon}),\\
% Q^{max}_{lin-inv}&=\norm{A^{-1}}\log(\frac{\norm{A^{-1}}^{2}}{q^{2}\epsilon}).
% \end{align}
% \end{subequations}
% \la{\sout{Numerical estimates reveal that the average query complexity and the maximal query complexity have similar asymptotic growth in this particular case.}} \LA{This comment will be redundant once we have Thais's numerical histrograms as a fourth panel in Fig. 1}



















\subsection{Step function} \label{sec:explicit_examples_step}

%\subsubsection{Polynomial approximation and its parameters}

\begin{lemma}\label{lemma_theta}
%\GC{Suggestion: rephrase the lemma in a simpler way without the fancy functions in eqs F26-F28. Something straight to the point like: "the function $\Tilde{f}_{step,\sigma}:[-1,1]\cross[-1,1]\rightarrow\mathbb{R}$  is a $\nu$-approx to $\Theta$." The fancy eqs would appear in the middle of the proof and then the big $\mathcal{O}$ statement would follow from them.}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be such that $f(x)=\Theta(x)$, where $\Theta(x)$ is the unit step function. Then, the function $\Tilde{f}_{s}:[-1,1]\cross[-1,1]\rightarrow\mathbb{R}$ given by the Chebyshev expansion
\begin{equation}
\Tilde{f}_{s}(y-x)=\frac{1}{2}+\sum^{k}_{j=0}a_{j}(y)T_{2j+1}(x)+b_{j}(y)\sqrt{1-x^{2}}U_{2j}(x)
\end{equation}
where
\begin{equation}
\begin{split}
a_{j}(y)&=-\sqrt{\frac{2\sigma}{\pi}}e^{-\sigma}\frac{I_{j}(\sigma)+I_{j+1}(\sigma)}{2j+1}\sqrt{1-y^{2}}U_{2j}(y),\\
b_{j}(y)&=\sqrt{\frac{2\sigma}{\pi}}e^{-\sigma}\frac{I_{j}(\sigma)+I_{j+1}(\sigma)}{2j+1}T_{2j+1}(y),
\end{split}
\end{equation}
with $I_{n}$ being the modified Bessel function of the first kind and $U_{2j}$ is a Chebyshev polynomial of the second-kind, is a $\nu$-approximation of $f(y-x)$ on all $(x,y)\in[-1,1]\cross[-1,1]$ such that $x\in[-1,-\mu+y]\cup[\mu+y,1]$ if 
\begin{equation}
k=\mathcal{O}\left(\tfrac{1}{\mu}\log\big(\tfrac{1}{\nu}\big)\right)\text{, and }\sigma=\mathcal{O}\left(\tfrac{1}{\mu^2}\log\big(\tfrac{1}{\nu}\big)\right).
\end{equation}
\end{lemma}

\begin{proof}
We take the Chebyshev approximation 
\begin{equation}
\Tilde{f}(x)=\frac{1}{2}+\sqrt{\frac{2\sigma}{\pi}}e^{-\sigma}\sum^{k}_{j=0}(-1)^{j}\frac{I_{j}(\sigma)+I_{j+1}(\sigma)}{2j+1}T_{2j+1}(x)
\end{equation}
described in \cite{wan_randomized_2022} and extend it to the two-variable case using the identity
\begin{equation}
\Theta(y-x)=\Theta(\sin(\arcsin{y}-\arcsin{x})),
\end{equation}     
that follows from fact that $\sin$ and $\arcsin$ are monotonous functions, and expand
\begin{equation}
T_{2j+1}(\sin(\arcsin{y}-\arcsin{x}))
\end{equation}
using trigonometric identities and the trigonometric definitions of $T_{2j+1}$ and $U_{2j}$. The conditions on $k$ and $\sigma$ needed for obtaining the $\nu$-approximation end up being the same as those for the single-variable version.
\end{proof}

%\begin{equation}
%\sigma\geq\frac{1}{4\mu^{2}}W\left(\frac{9}{2\pi\nu^{2}}\right)\text{, and }k\geq\sqrt{tW\left(\frac{18}{\pi\nu^{2}}\right)}
%\end{equation}
%where $W$ is the Lambert-W function and $t$ is a positive integer satisfying
%\begin{equation}
%\begin{cases}
%t\geq \sigma&\text{, if }\nu\geq\frac{3}{2\sqrt{2\pi\log(16)}}\\
%t\geq f\left(\sigma,\frac{2\sqrt{2\pi}}{3}\nu\sqrt{W\left(\frac{18}{\pi\nu^{2}}\right)}\right)&\text{, if }\nu<\frac{3}{2\sqrt{2\pi\log(16)}}
%\end{cases}
%,
%\end{equation}
%where
%\begin{equation}
%f(\sigma,\gamma)=\frac{\log(\frac{1}{\gamma})-\sigma}{W\left(\frac{1}{e}\left(\frac{1}{\sigma}\log(\frac{1}{\gamma})-1\right)\right)}
%\end{equation}

%A Chebyshev expansion for the Step function of a single variable is obtained from the Chebyshev expansion of the error function. In short, we have that
%\begin{equation}
%\Theta(x)=\frac{1}{2}+\frac{1}{2}\sgn(x),
%\end{equation}
%and that the error function $\erf$

\begin{lemma}
[$l_1$-norm of coefficients]\label{lem:1norm_step}
Let $\vv{a}_{s}(y_*)$ and $\vv{b}_{s}(y_*)$ be the vector of coefficients of the Chebyshev expansion $\Tilde{f}_{s}(y_*-x)$ for $y_*=\frac{1}{\sqrt{2}}$. Then
\begin{equation}
\norm{\vv{a}_{s}(y_*)}_1=\norm{\vv{b}_{s}(y_*)}_1<\frac{2.07}{2\pi}(H_{k}+2\ln{2}) = \mathcal{O}(\log(k)),
\end{equation}
where $H_{k}$ is the $k$-th harmonic number.
\end{lemma}

\begin{proof}
Note that
\begin{equation}
\abs{a_{j}(y_*)}=\abs{b_{j}(y_*)}={y_*}\sqrt{\frac{2\sigma}{\pi}}e^{-\sigma}\frac{I_{j}(\sigma)+I_{j+1}(\sigma)}{2j+1}
\end{equation} 
for all $j$, implying that $\norm{\vv{a}_{s}(y_*)}_1=\norm{\vv{b}_{s}(y_*)}_1$ and that they are $\frac{1}{\sqrt{2}}$ times smaller than the one-norm of the single-variable approximation found in \cite{wan_randomized_2022}, which gives our upper bound.
\end{proof}

\begin{lemma}\label{lemma_step_query}
Let $\vv{a}_{s}(y_*)$ and $\vv{b}_{s}(y_*)$ be the vector of coefficients of the Chebyshev expansion $\Tilde{f}_{s}(y_*-x)$ for $y_*=\frac{1}{\sqrt{2}}$. Then, the average query complexity of estimating $\Tilde{z}^{(1)}$ is $q^{(1)}=\mathcal{O}\left(\tfrac{1}{\mu}\sqrt{\log\big(\tfrac{1}{\nu}\big)}\Big/\!\log\big(\frac{1}{\mu}\log\big(\frac{1}{\nu}\big)\big)\right)$.  
\end{lemma}

\begin{proof}
By definition of $q^{(1)}$, and using the fact that $\abs{a_{j}(y_*)}=\abs{b_{j}(y_*)}$ for all $j$, we have that
\begin{align}
q^{(1)} &=\frac{1}{\norm{\vv{a}_{s}(y_*)}_1}\sum^{k}_{j=0}\sqrt{\frac{4\sigma}{\pi}}e^{-\sigma}(I_{j}(\sigma)+I_{j+1}(\sigma))\notag\\
&\le \frac{1}{\norm{\vv{a}_{s}(y_*)}_1}\sum^{\infty}_{j=0}\sqrt{\frac{4\sigma}{\pi}}e^{-\sigma}(I_{j}(\sigma)+I_{j+1}(\sigma))\notag\\
&=\frac{1}{\norm{\vv{a}_{s}(y_*)}_1}\sqrt{\frac{4\sigma}{\pi}},
\end{align}
where in the last step $\sum^{\infty}_{j=0}\big(I_{j}(\sigma)+I_{j+1}(\sigma)\big)=e^\sigma$ follows from the Jacobi-Anger expansion. 
Since $\norm{\vv{a}_{s}(y_*)}=\mathcal{O}(\log(k))$ for large $k$, it follows that 
\begin{equation}
    q^{(1)} = \mathcal{O}\left(\frac{\sqrt{\sigma}}{\log(k)}\right) = \mathcal{O}\left(\frac{\tfrac{1}{\mu}\sqrt{\log\big(\tfrac{1}{\nu}\big)}}{\log\big(\frac{1}{\mu}\log\big(\frac{1}{\nu}\big)\big)}\right).
\end{equation}
% If $\norm{\vv{a}_{s}(y_*)}=\mathcal{O}(\log(k))$, then $1/\norm{\vv{a}_{s}(y_*)}<1$ for $k$ big, therefore
% \begin{equation}
% Q^{(1)} \le \sum^{\infty}_{j=0}\sqrt{\frac{4\sigma}{\pi}}e^{-\sigma}(I_{j}(\sigma)+I_{j+1}(\sigma)),
% \end{equation}
% where we take the infinite sum as an upper bound of the numerator. From the Jacobi-Anger expansion, we have the identities,
% \begin{equation}
% \sum^{\infty}_{j=0}I_{j}(\sigma)=\frac{e^{\sigma}+I_{0}(\sigma)}{2},\quad\sum^{\infty}_{j=0}I_{j+1}(\sigma)=\frac{e^{\sigma}-I_{0}(\sigma)}{2},
% \end{equation}
% implying
% \begin{equation}
% Q^{(1)}<\sqrt{\frac{4\sigma}{\pi}}=\mathcal{O}\left(\mu^{-1}\,\sqrt{\log(\nu^{-1})}\right)
% \end{equation}
\end{proof}

\begin{lemma}
Let $\vv{a}_{s}(y_*)$ and $\vv{b}_{s}(y_*)$ be the vector of coefficients of the Chebyshev expansion $\Tilde{f}_{s}(y_*-x)$ for $y_*=\frac{1}{\sqrt{2}}$. Then, the error damping factor for estimating $\Tilde{z}^{(1)}$ is $\mathcal{O}\left(\tfrac{1}{\mu}\sqrt{\log\big(\tfrac{1}{\nu}\big)}\Big/\!\log\big(\frac{1}{\mu}\log\big(\frac{1}{\nu}\big)\big)\right)$.
\end{lemma}

\begin{proof}
Follows immediately from Lemmas \ref{lemma_theta} and \ref{lemma_step_query}.
\end{proof}

%\subsubsection{Problem statement and complexities of the solution}


%\begin{definition}[GSEE]
%Let $H$ be a Hamiltonian with $\norm{H}\leq1$ and ground state energy $E_{0}$. Then the \textbf{ground state energy estimation problem} (GSEE) is to give an $\epsilon$-precise estimate of $E_{0}$ with $1-\delta$ confidence, given a promise state $\varrho$ with overlap $\eta>0$ onto the ground-state subspace.  
%\end{definition}

%Our approach to solving this problem is by solving the equivalent problem defined bellow.

%\begin{lemma}[GSEE from ETP]
%\label{lemma_ETP}
%Let $H$ be a Hamiltonian with $\norm{H}\leq1$ ground state energy $E_{0}$. Then, let $x$ be a threshold, $\epsilon>0$ a precision parameter and $\varrho_{\eta}$ a promise state that satisfies $\Tr[\varrho_{\eta}\Pi_{0}]>\eta$, where $\Pi_{0}=\ketbra{E_{0}}$ is the projector onto the Ground state of $H$. The \textbf{eigenvalue thresholding problem} consists in deciding whether $x$ satisfies
%\begin{equation}
%F_{\eta}(x+\epsilon)>0\text{, and }F_{\eta}(x-\epsilon)<\eta
%\end{equation}
%where
%\begin{equation}
%F_{\eta}(x):=\Tr[\varrho_{\eta}\Theta(x\mathbf{1}-H)].
%\end{equation}
%with $1-\delta$ confidence. Then, the ground-state energy estimation problem is reducible to the eigenvalue thresholding problem.
%\end{lemma}

%\begin{proof}
%Note that by definition $F_{\eta}(y)<\eta$ for all $y<E_{0}$ and $F_{\eta}(y)>0$ for all $y\geqE_{0}$. Now, suppose we can solve the eigenvalue thresholding problem efficiently. If $x$ is such that $F_{\eta}(x+\epsilon)>0$, then $x+\epsilon\geqE_{0}$ which implies that $-\epsilon\leq x-E_{0}$. Similarly, if $x$ is such that $F_{\eta}(x-\epsilon)>0$ then $x-\epsilon<E_{0}$ which implies that $x-E_{0}<\epsilon$. Therefore, if $x$ satisfies both conditions with confidence $1-\delta$, we have that $x$ is an $\epsilon$-precise estimate of $E_{0}$ with confidence $1-\delta$. 
%\end{proof}

%\begin{lemma}[Solution to GSEE via ETP]\label{lemma_GSEE}
%The $GSEE$ estimate $x^{*}$ is obtained by a \gc{binary-like} search through $\mathcal{O}(\log(\epsilon^{-1}))$ values $x$, using ETP as the search sieve. 
%\end{lemma}

%\begin{proof}
%\gc{Prove it}    
%\end{proof}

%We solve this problem by computing the $\nu$-approximation
%\begin{equation}
%\Tilde{F}_{\eta}(x):=\Tr[\varrho_{\eta}\Tilde{f}_{step,\sigma}(x\mathds{1}_{s}-H)],
%\end{equation}
%which satisfies
%\begin{equation}
%F_{\eta}(x-\epsilon)-\nu<\Tilde{F}_{\eta}(x)<F_{\eta}(x+\epsilon)+\nu,
%\end{equation}
%by the virtue of $\Tilde{f}_{step,\sigma}(x)$  being a $\nu$-approximation in the interval $[-1,-\epsilon]\cup[\epsilon,1]$. Therefore, in terms $\Tilde{F}_{\eta}$, the eigenvalue thresholding conditions translates to 
%\begin{equation}
%\Tilde{F}_{\eta}(x)>\nu\text{, and }\Tilde{F}_{\eta}(x)<\eta-\nu
%\end{equation}
%since $\Tilde{F}_{\eta}(x)<\eta-\nu$ implies that $F_{\eta}(x-\epsilon)<\eta$ and that $\Tilde{F}_{\eta}(x)>\nu$ implies that $F_{\eta}(x+\epsilon)>0$. This imposes the bound $\nu<\eta/2$, since otherwise it would be impossible discern between the two conditions.

%Given the form of $\Tilde{f}_{step,\sigma}$, in order to compute this approximation for a particular $x$, we need to estimate
%\begin{equation}
%\Tr\Big{[}\varrho_{\eta}T_{2j+1}(H)\Big{]}\text{, and }\Tr[\varrho_{\eta}\sqrt{1-H^{2}}U_{2j}(H)],
%\end{equation}
%for $j$ high-enough to achieve the desired approximation accuracy. The first kind of term can be computed using algorithm \ref{alg: problem 1} without using any $U_{\phi}$ or $U_{\psi}$. To estimate the second kind of term however, note that by eq.\ref{}, we have that 
%\begin{equation}
%(\bra{1}_{a}\otimes\mathds{1}_{s})U^{j}_{H}(\ket{0}_{a}\otimes\mathds{1}_{s})=\sqrt{1-H^{2}}U_{j-1}(H),
%\end{equation}
%implying that we need to modify the circuit in fig.\ref{fig: had_block_test}.a) by introducing a controlled-$X_{a}$ operator before any of the block-encoded gates $U_{H}$.

%Therefore, if for each $x$ we define random variables
%\begin{equation}
%\begin{split}
%\Tilde{z}^{(1),t}(x)_\alpha&:= \norm{\vv{t}_{\sigma}(x)}_{1}\,\sgn(t_{\sigma,j_\alpha}(x))\,b^{(1),t}_{\alpha}\,;\\
%\Tilde{z}^{(1),u}(x)_\beta&:= \norm{\vv{u}_{\sigma}(x)}_{1}\,\sgn(u_{\sigma,j_\beta}(x))\,b^{(1),u}_{\beta}\,;
%\end{split}
%\end{equation}
%where $b^{(1),t}_{\alpha}$ are the bits obtained from the unaltered Hadamard test of fig.\ref{fig:main}.a), while $b^{(1),u}_{\beta}$ are the ones obtained from the version with an additional controlled-$X_{a}$ gate, then
%\begin{equation}
%\Tilde{F}_{\eta}(x)=\frac{1}{2}+\mathbb{E}[\Tilde{z}^{(1),t}_{\alpha}(x)]+\mathbb{E}[\Tilde{z}^{(1),u}_{\beta}(x)],
%\end{equation}
%and our statistical estimate is given by
%\begin{equation}
%\mathfrak{F}_{\eta}(x)=\frac{1}{2}+\sum^{S^{(1),t}(x)}_{\alpha=0}\frac{\Tilde{z}^{(1),t}_{\alpha}(x)}{S^{(1),t}(x)}+\sum^{S^{(1),u}(x)}_{\beta=0}\frac{\Tilde{z}^{(1),u}_{\beta}(x)}{S^{(1),u}(x)},
%\end{equation}
%where 
%\begin{equation}
%\begin{split}
%S^{(1),t}(x)&=\frac{144\norm{\vv{t}_{\sigma}(x)}^{2}_{1}}{\eta^{2}}\log\frac{8}{\delta},\\
%S^{(1),u}(x)&=\frac{144\norm{\vv{u}_{\sigma}(x)}^{2}_{1}}{\eta^{2}}\log\frac{8}{\delta},
%\end{split}
%\end{equation}
%and we choose $\nu=\eta/3$ and $\epsilon_{stat}=\eta/6$ (do not confuse the statistical error $\epsilon_{stat}$ of the function estimation with the precision parameter $\epsilon$).

%Therefore, we have that if our empirical estimate of $\mathfrak{F}_{\eta}(x)$ for $x$ satisfies $\mathfrak{F}_{\eta}(x)<5\eta/6$, then $\Tilde{F}_{\eta}(x)<2\eta/3$ which implies that $F_{\eta}(x-\epsilon)<\eta$. Similarly, if $\mathfrak{F}_{\eta}(x)>\eta/6$, then $\Tilde{F}_{\eta}(x)>\eta/3$ which implies that $F_{\eta}(x+\epsilon)>0$. The question now is how to use the solution of the eigenvalue thresholding problem to find the $x^{*}$ that is an $\epsilon$-precise estimate of $E_{0}$. This can be done using the following way; let $c(x)$ be a single bit such that if $x$ satisfies only condition $\mathfrak{F}_{\eta}(x)>\eta/6$, $c(x)=0$, while if $x$ satisfies only the condition $\mathfrak{F}_{\eta}(x)<5\eta/6$, $c(x)=1$, and $c(x)$ is chosen randomly if both are true. Then using this bit $c$ apply the following sub-routine

%\begin{algorithm}
%\caption{Eigenvalue-searching}
%\begin{algorithmic}
%\Require $x_{0}\gets-1$, $x_{1}\gets1$;
%\While{$x_{1}-x_{0}>2\epsilon$} 
%\State $x\gets(x_{0}+x_{1}/2)$;
%\State $u\gets c(x)$;
%\If{$u=0$}
%\State $x_{1}\gets x+(2/3)\epsilon$
%\Else
%\State $x_{0}\gets x-(2/3)\epsilon$
%\EndIf
%\EndWhile
%\State \textbf{Output:} $x^{*}=(x_{0}+x_{1})/2$ 
%\end{algorithmic}
%\end{algorithm}

%This sub-routine works in the following way: If $c(x)=0$, then $x$ is to the right of the interval $(E_{0}-\epsilon,E_{0}+\epsilon)$, which implies that $x^{*}$ is to the left of $x$, similarly, if $c(x)=1$, then $x$ is to the left of the interval $(E_{0}-\epsilon,E_{0}+\epsilon)$ which implies that $x^{*}$ is to the right of $x$. Therefore, the eigenvalue-searching subroutine nudges the value of $x$ by smaller and smaller fractions of $\epsilon$ until it falls inside the interval $(E_{0}-\epsilon,E_{0}+\epsilon)$, in a manner akin to binary search. In fact, if $x_{0,l}$ and $x_{1,l}$ are the values of $x_{0}$ and $x_{1}$ after $l$-iterations of the algorithm, we have that
%\begin{equation}
%x_{1,l+1}-x_{0,l+1}=\frac{1}{2}(x_{1,l}-x_{0,l})+\frac{2}{3}\epsilon,
%\end{equation}
%implying that
%\begin{equation}
%x_{1,l}-x_{0,l}=\frac{2-(4/3)\epsilon}{2^{l}}+\frac{4}{3}\epsilon.    
%\end{equation}
%Therefore, the subroutine terminates when
%\begin{equation}
%\frac{2-(4/3)\epsilon}{2^{l_{term}}}+\frac{4}{3}\epsilon\leq2\epsilon,
%\end{equation}
%which implies that
%\begin{equation}
%l_{term}\geq\log_{2}\left(\frac{1}{\epsilon}-\frac{2}{3}\right).
%\end{equation}

\begin{definition}
Given $\Tilde{F}_{\varrho}(y)=\Tr[\varrho\,\Tilde{f}_{s}(y\mathds{1}-H)]$, its T-part is the expression
\begin{equation}
\sum^{k}_{j=0}a_{j}(y)\Tr[T_{2j+1}(H)\varrho],
\end{equation}
while its U-part is the expression
\begin{equation}
\sum^{k}_{j=0}b_{j}(y)\Tr[\sqrt{1-H^{2}}U_{2j}(H)\varrho].
\end{equation}

\end{definition}

\begin{lemma}\label{lemma_ychoice}
Let $y_*=\frac{1}{\sqrt{2}}$, and $\Tilde{z}^{(1),T}(y)$ and $\Tilde{z}^{(1),U}(y)$ be the unbiased estimators of the $T$-part and the $U$-part of $\Tilde{F}_{\varrho}(y)$ obtained from algorithm 1, for any point $y$, respectively. Then, for any point $y$ we have that
\begin{equation}
\Tilde{F}_{\varrho}(y)=\frac{1}{2}+\mathbb{E}_{w^{T}(y)}\left[\Tilde{z}^{(1),T}(y_*)\right]+\mathbb{E}_{w^{T}(y)}\left[\Tilde{z}^{(1),U}(y_*)\right],
\end{equation}
where $\mathbb{E}_{w^{T}(y)}$ and $\mathbb{E}_{w^{U}(y)}$ are the weighted expected values with weight functions
\begin{equation}
\begin{split}
w^{T}(y,j)=\frac{a_{j}(y)}{a_{j}(y_*)},\\
w^{U}(y,j)=\frac{b_{j}(y)}{b_{j}(y_*)}.
\end{split}
\end{equation}
\end{lemma}

\begin{proof}
To show this, just notice that by the definition of $\mathbb{E}_{w^{T}(y)}$ and $\Tilde{z}^{(1,T)}(y_*)$ that $\mathbb{E}_{w^{T}(y)}\left[\Tilde{z}^{(1),T}(y_*)\right]$ is equal to
\begin{equation}
\begin{split}
&\sum^{k}_{j=0}\sum_{b}w^{T}(y,j)p_{j}(p(b|j)\norm{\vv{a}_{s}(y_*)}\sgn(a_{j}(y_*))b)=\\
=&\sum^{k}_{j=0}w^{T}(y,j)a_{j}(y_*)\mel{\psi}{T_{2j+1}(H)}{\psi}=\\
=&\sum^{k}_{j=0}a_{j}(y)\mel{\psi}{T_{2j+1}(H)}{\psi}=\mathbb{E}\Big[\Tilde{z}^{(1),T}(y)\Big],
\end{split}
\end{equation}
while a similar computation also shows that
\begin{equation}
\mathbb{E}_{w^{U}(y)}\left[\Tilde{z}^{(1),U}(y_*)\right]=\mathbb{E}\Big[\Tilde{z}^{(1),U}(y)\Big].
\end{equation}
which proves the lemma.
\end{proof}

\begin{corollary}
To sample $\Tilde{F}_{\varrho}(y)$ for $\mathcal{O}(\log(\xi^{-1}))$ values of $y$ it is only necessary to sample $\Tilde{F}_{\varrho}(y_*)$ with $1-\mathcal{O}(\delta/\log(\xi^{-1}))$ confidence, and for each desired value of $y$, to multiply each sample by the appropriate weight factor.
\end{corollary}

\begin{proof}
Follows directly from the previous lemma.
\end{proof}

%In principle, this implies that we need to compute $\mathfrak{F}_{\eta}(x)$ for  $l_{term}$ values of $x$. However, we can instead collect a higher amount of samples for a particular $x$, and compute the other values by giving an appropriate weight for each sample. That is, choose $x=1/\sqrt{2}=\chi$. Then, it is not hard to show that
%\begin{equation}
%\abs{\sqrt{1-\chi^{2}}U_{2j}(\chi))}=\chi\text{, and }\abs{T_{2j+1}(\chi)}=\chi,
%\end{equation}
%for all $j\in\mathbb{N}$, which implies that all components of both $\vv{t}_{\sigma}(\chi)$ and $\vv{u}_{\sigma}(\chi)$ are non-zero. Therefore, we can compute $\mathfrak{F}_{\eta}(x)$ by sampling the random variables $\Tilde{z}^{(1),t}_{\alpha}(\chi)$ and $\Tilde{z}^{(1),t}_{\beta}(\chi)$ and introducing the weight factors 

%such that


%Therefore, we have that
%\begin{equation}
%\Tilde{F}_{\eta}(x)=\frac{1}{2}+\mathbb{E}\left[w^{t}_{\alpha}(x)\Tilde{z}^{(1),t}_{\alpha}(\chi)\right]+\mathbb{E}\left[w^{u}_{\beta}(x)\Tilde{z}^{(1),t}_{\beta}(\chi)\right],
%\end{equation}
%and the unbiased estimator $\mathfrak{F}_{\eta}(x)$ is given by
%\begin{multline}
%\Tilde{F}_{\eta}(x)=\frac{1}{2}+\sum^{S^{'(1),t}}_{\alpha=1}w^{t}_{\alpha}(x)\Tilde{z}^{(1),t}_{\alpha}(\chi)+\\
%+\sum^{S^{'(1),u}}_{\beta=1}w^{u}_{\beta}(x)\Tilde{z}^{(1),t}_{\beta}(\chi),
%\end{multline}
%where
%\begin{equation}
%\begin{split}
%S^{'(1),t}&=\frac{144\norm{\vv{t}_{\sigma}(\chi)}^{2}_{1}}{\eta^{2}}\log\frac{8\log_{2}\left(\frac{1}{\epsilon}-\frac{2}{3}\right)}{\delta},\\
%S^{'(1),u}&=\frac{144\norm{\vv{u}_{\sigma}(\chi)}^{2}_{1}}{\eta^{2}}\log\frac{8\log_{2}\left(\frac{1}{\epsilon}-\frac{2}{3}\right)}{\delta},
%\end{split}
%\end{equation}
%since, we need to sample from the distribution associated to $\chi$ with confidence $1-\delta/l_{term}$ in order to use the union bound to find the actual $\epsilon$-precise estimator $x^{*}$ of $E_{0}$ with confidence $1-\delta$.

\section{Extension to QSVT} \label{sec:QSVT}

For simplicity, in the main text, we considered only Hermitian matrices. In that case, there is an operation to build a qubitized oracle whose powers directly lead to the Chebyshev polynomials of the block-encoded matrix. This section shows that our algorithm can be applied to randomize the more general quantum singular value transformation technique.

Let us consider a unitary $U_A$ such that 
$\big(\bra{0}_a\otimes \mathds{1}_s\big)\,U_A\,\big(\ket{0}_a\otimes \mathds{1}_s\big) = A$ as in Eq.\ \eqref{eq:block_encoding}, except that now the block-encoded matrix $A$ does not need to be Hermitian. In fact, $A$ could even be rectangular. In this case, $U_A$ is a bock-encoding of a square matrix built from $A$ by filling the remaining entries with zeroes. The input matrix assumes a singular value decomposition, and we are interested in singular value transformations as in the definition below (Def.\ 16 in Ref.\ \cite{Gilyen2019}):

\begin{definition}[Singular value transformations] \label{def:SVT} Let $f:\mathbb{R}\rightarrow \mathbb{C}$ be an even or odd function. Let $\sum_{j=1}^{d} \sigma_j\ketbra*{\tilde{\psi}_j}{\psi_j}$ be a singular value decomposition of  the $d\times d$ matrix $A$ with right eigenvectors $\{\ket{\psi_j}\}$, left eigenvectors $\{\ket*{\tilde{\psi}_j}\}$ and singular values $\{{\sigma_j}\}$(including zero values). The singular value transformation of $A$ by  $f$ is defined as 
\begin{equation}
    f^{(SV)}(A) := \begin{cases}
        \sum_{j=1}^{d} f(\sigma_j)\ketbra{{\psi}_j}{\psi_j},\quad \text{$f$ even,}\\
        \sum_{j=1}^{d} f(\sigma_j)\ketbra*{\tilde{\psi}_j}{\psi_j},\quad \text{$f$ odd.}
    \end{cases}
\end{equation}
\end{definition}

It was shown in Ref.\ \cite{Gilyen2019}  that a quantum singular value transformation (QSVT) could be implemented from calls to $U_A$ and $U_A^\dagger$, and single qubit rotations controlled by $\ket{0}_a$. In Theorem 17 and Corollary 18, the authors state that  the singular value transformation by a real polynomial of degree $k$ with even or odd parity is always achieved by some sequence of qubit rotations $\Phi=\{\phi_1,\cdots,\phi_k\}\in\mathbb{R}$ and a circuit making $k$ queries to $U_A$ and $U_A^\dagger$, as long as the polynomial is normalized to $1$. In particular, to implement the Chebyshev polynomial $T^{(SV)}_k(A)$, the sequence of angles is simply $\phi_1=(1-k)\frac{\pi}{2}$ and $\phi_j=\frac{\pi}{2}$ for $j\neq1$.

Under singular value transformation, Probs. \ref{problem.1} and \ref{problem.2} can be phrased in the same way, except that now the Hermitean $A$ is substituted by a general matrix $A$ and the function is meant to be a singular value transformation according to Def.\ \ref{def:SVT}. Moreover, the problems are well-posed only for  functions with  defined parity.  Therefore, among the use cases presented in the main text, the ones that are extendable to general matrices are the inverse function and matrix monomials. 

The randomized QSVT solutions to Probs. \ref{problem.1} and \ref{problem.2} follow as before by sampling polynomial degrees according to their weight in the Chebyshev expansion of the target function. A Hadamard test is then run for each polynomial degree draw, the Chebyshev polynomials being implemented using the circuit in Fig.\ 1 in \cite{Gilyen2019} with the angle sequence described above. The algorithm works for the same reason as for Hermitian matrices: QSVT implements a block-encoding of the polynomial singular value transformation, while the Hadamard test has the useful feature of always selecting the correct block of a unitary block-encoding.









\section{Non-normalized matrices}\label{app:non_norm}

Suppose that, instead of having a normalized matrix $A$ (i.e., satisfying $\norm{A}\leq1$), we have a general Hermitian matrix $A$ as input. We can define the normalized matrix $A'=\frac{A}{\alpha}$, with $\alpha\geq\norm{A}$ to be block-encoded instead of $A$. Therefore, using our framework we can simulate a function $f'(A')$ defined such that 
\begin{equation}
f(A)=f(\alpha A')=f'(A'),
\end{equation}
where $f:\mathcal{D}\rightarrow\mathbb{R}$ and $f':\frac{\mathcal{D}}{\alpha}\subseteq[-1,1]\rightarrow\mathbb{R}$ have the same image. Therefore, introducing a sub-normalization will require a Chebyshev expansion for a new function whose domain is also sub-normalized relative to the original. The contribution of the sub-normalization factor $\alpha$ to the sample and query complexities of the randomized algorithm needs to be analyzed on a case-by-case basis. In the following, we analyze the maximal query depth and the $l_1$-norm of the Chebyshev coefficients for each use-case we showed in the main text. From them and  Theorem \ref{main_lemma2}, the complexities of particular instances of Problem 1 or Problem 2 can be obtained.
%The sub-normalization factor $\alpha$ will contribute to the sample and query complexities of solving each problem depending on the form of $f$. But in general, let $f:\mathcal{D}\rightarrow\mathbb{R}$ be such that its domain $\frac{\mathcal{D}}{\alpha}\subseteq[-1,1]$. We can write $f(x)$ with $x\in D$ as a function $f'$ of a new variable $x'=x/\alpha$ such that
%\begin{equation}
%f(x)=f(\alpha x')=f'(x').
%\end{equation}
%This implies that $f'$ will have the same image as $f$, whenever $x'\in D/\alpha$. Therefore, introducing a sub-normalization will require a Chebyshev expansion for a new function, whose domain is also sub-normalized with respect to the original.

In the case of monomials, we have that $f(x)=x^t$ yields $f'(x')=\alpha^t {x'}^t$. This implies that an $\nu$-approximation to $A^{n}$ is $\nu/\alpha^{n}$-approximation to $H^{'n}$. To attain an error $\nu$ in approximating $f(x)$, the approximation in Lemma \ref{lem:approx_mon} shall be used with error $\nu/\alpha^n$ and also the $l_1$-norm of the coefficients will gain a factor $\alpha^n$. Therefore, The maximal query depth and coefficients are ${k}=\sqrt{2t\log(\frac{2\alpha^{t}}{\nu})}$ and $\norm{\vv{a}}_{1}=\alpha^{t}-\nu$, respectively.


In the case of the exponential function, notice that $e^{-\beta A}=e^{-(\beta\alpha)(A/\alpha)}$. Therefore, we just need to re-scale  $\beta$ with a factor  $\alpha$  , obtaining ${k}=\mathcal{O}\left(\sqrt{\alpha\beta}\log(\frac{e^{\alpha\beta}}{\nu})\right)$ and 
$\norm{\vv{a}}_{1}=e^{\alpha\beta}$ from Lemmas \ref{lem:approx_exp} and \ref{lem:1norm_exp}, respectively.

In the case of the inverse function, $f(x)=x^{-1}$ needs to be approximated in the domain $\mathcal{D}=[-\norm{A},-1/\norm{A^{-1}}]\cup[1/\norm{A^{-1}},\norm{A}]$. In this case, the randomized algorithm should implement an approximation to $f'(A')=\alpha^{-1}{A'}^{-1}$ in the domain $\mathcal{D}/\alpha=[-1,-1/\kappa]\cup[1/\kappa,1]$, with $\alpha=\norm{A}$ and $\kappa=\norm{A}\,\norm{A^{-1}}$ as usual. It suffices to apply Lemma \ref{lem:approx_inv} to obtain a $\alpha\nu$-approximation to ${A'}^{-1}$ with resulting maximal degree  
$k=\mathcal{O}\left(\kappa\,{\log\frac{\norm{A^{-1}}}{\nu}}\right)$ and coefficients $l_1$-norm 
$\norm{\vv{a}}_{1}=\mathcal{O}\left(\norm{A^{-1}}{\log\frac{\norm{A^{-1}}}{\nu}}\right)$.


Lastly, in the case of the Heaviside step function, we have that $f(A)=f'(A')$. However, in order to approximate $\Theta(A)$ in the domain $D=(-1,-\xi]\cup[\xi,1)$, we need to be able to approximate $\Theta(A')$ in the larger domain $D=(-1,-\xi/\alpha]\cup[\xi/\alpha,1)$. Therefore, Lemmas \ref{lemma_theta} and \ref{lem:1norm_step} give $k=\mathcal{O}\left(\frac{\alpha}{\xi}\,\log\frac{1}{\nu}\right)$ and $\norm{\vv{a}}_1=\mathcal{O}\left(\log\big(\frac{\alpha}{\xi}\,\log\frac{1}{\nu}\big)\right)$.





\end{document}



















%To that end, the first step is to take $U_{A}$ into a specific type of block encoding that is more amenable for dealing with Chebyshev polynomials. 

%The basic ingredient is a quantum oracle $U_{A}$ that is a unitary block-encoding of the Hermitian operator $A$ of interest (the \textit{signal}). 

% \begin{definition}[Block-encoding oracle]\label{def:oracle}
%  Let $\mathcal{H}_{s}$ and $\mathcal{H}_{a}$ denote the system and ancillary Hilbert spaces, respectively, and $A$ be a Hermitian matrix with spectral norm $\norm{A}\leq1$ acting on $\mathcal{H}_{s}$. A unitary $U_{A}$ acting on $\mathcal{H}_{s}\otimes\mathcal{H}_{a}$ is called an exact \textit{block-encoding} of $A$ if \Danger{Changed notation for block selection}
% \begin{equation}
% \big(\bra{0}_a\otimes \mathds{1}_s\big)\,U_A\,\big(\ket{0}_a\otimes \mathds{1}_s\big) = A
% %{\phantom{\ket{0}}}_a\!\mel{0}{U_{A}}{0}_{a}=A,
% \end{equation}
% for some suitable state $\ket{0}_{a}\in \mathcal{H}_{a}$. 
% \end{definition} 

% Building this oracle is a non-trivial task, but efficient block-encoding schemes are known when some special structure is present, e.g., for $A$ sparse or given as a linear combination of unitaries \cite{LowChuangQuantum2019,Gilyen2019}. 
% The standard QSP algorithm takes the oracle $U_A$ as input and returns another unitary $U_{\Tilde{f}(A)}$ that block-encodes some polynomial function $\Tilde{f}(A)$ in the Chebyshev basis (see \cite{silva2022fourierbased} for Fourier-based QSP techniques). 
% We will be particularly interested in cases where $\tilde{f}$ is a Chebyshev polynomial approximation of some analytic function $f$. 
% To that end, the first step is to take $U_{A}$ into a specific type of block encoding that is more amenable for dealing with Chebyshev polynomials. 

% \begin{definition}[Qubitized block-encoding oracle]\label{def:qubitizedoracle}
% \GC{Let $\mathcal{H}_{s}$ and $\mathcal{H}_{a}$ denote the system and ancillary Hilbert spaces, respectively, and $A$ be a Hermitian matrix with spectral norm $\norm{A}\leq1$ acting on $\mathcal{H}_{s}$. A unitary $U_{A}$ acting on $\mathcal{H}_{s}\otimes\mathcal{H}_{a}$ is called an exact \textit{block-encoding} of $A$ if }
% Let $U_A$ be the block-encoding oracle of a Hermitian operator $A$ in the same setting of Def. \ref{def:oracle}, and $\{\ket{\lambda}\}$ be the eigenbasis of $A$. A qubitized block-encoding oracle $W_{A}$ is a block-encoding of $A$ of the form
% \begin{align}
% W_{A} = \bigoplus_\lambda 
% \begin{pmatrix}
% \lambda & -\sqrt{1-\lambda^2}\\
% \sqrt{1-\lambda^2} &\lambda
% \end{pmatrix}
% \,.
% %= \bigoplus_{\lambda}[\lambda \mathds{1}_{a}-i\sqrt{1-\lambda^{2}}Y_{a}],
% %W_{A}=\bigoplus_{\lambda}\ketbra{\lambda}\otimes[\lambda \mathds{1}_{a}-i\sqrt{1-\lambda^{2}}Y_{a}],
% \end{align}
% %where $\mathds{1}_{a},Y_{a}$ are the identity and Pauli $Y$ matrices acting on $\mathcal{H}_{a}$.
% \Danger{Discuss notation! If we stick to $W_A$ for the qubitized, then it's $W_A$ that should be in the Figure} \GC{exp}
% \end{definition}

% \noindent As shown in \cite{LowChuangQuantum2019}, this can be constructed using at most one query to $U_A$ and $U_A^{-1}$, at most one additional ancillary qubit, and $\mathcal{O}(\log \text{dim}(\mathcal{H}_a))$ quantum gates. The nomenclature stems from the fact that the unitary splits into single-qubit Pauli gates $\lambda \mathds{1}_{a}-i\sqrt{1-\lambda^{2}}Y_{a}$ acting independently at each eigenvalue subspace. 

% The qubitized oracle $W_A$ is the main building block for implementing arbitrary functions of $A$ and is sometimes referred to as the QSP iterate. An important property of $W_A$ is that repeated applications of it give rise to a block-encoding of Chebyshev polynomials of $A$ as follows.
% \begin{lemma}
% Let $W_A$ as in Def. \ref{def:qubitizedoracle} and $j\in\mathbb{N}$. Then 
% \begin{equation}
% \big(\bra{0}_a\otimes \mathds{1}_s\big)\,{W_A}^j\,\big(\ket{0}_a\otimes \mathds{1}_s\big) = T_j(A)\,,
% \end{equation}
% where $T_{j}$ is the $j$-th order Chebyshev polynomial.
% \begin{equation}\label{eq:WHk}
% {W_A}^k = \bigotimes_\lambda
% \begin{pmatrix}
% T_k(\lambda) & -\sqrt{1-\lambda^2}U_{k-1}(\lambda)\\
% \sqrt{1-\lambda^2}U_{k-1}(\lambda) &\lambda T_{k}(\lambda)
% \end{pmatrix}
% ,
% %W^{k}_{A}=\bigoplus_{\lambda}\ketbra{\lambda}_{s}\otimes[T_{k}(\lambda)Z_{a}+\sqrt{1-\lambda^{2}}U_{k-1}(\lambda)X_{a}],
% \end{equation}
% where $T_{k}(\lambda),U_{k}(\lambda)$ are the $k$-th order Chebyshev polynomials of the first and second kinds, respectively. \GC{I don't like $U_k$ for the modified Chebyshev polynomials. Use caligraphic? $\T_k(x)$ and $\U_k(x)$.} 
% \end{lemma}

% \noindent This was originally noted in \cite{LowChuangQuantum2019}. We include a proof by induction in App\la{.} \GC{ref}. 
% In particular, \eq{WHk} implies
% \begin{equation}
% (\bra{0}_a\otimes \mathds{1}_s)\,{W_A}^k\,(\ket{0}_a\otimes \mathds{1}_s) = T_k(A)
% \end{equation}
% which will be used extensively in the following. 