\begin{thebibliography}{10}

\bibitem{abbas2023semdedup}
Amro Abbas, Kushal Tirumala, Dániel Simig, Surya Ganguli, and Ari~S. Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic
  deduplication, 2023.

\bibitem{bai2022training}
Yuntao Bai, Andy Jones, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback, 2022.

\bibitem{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 41--48, 2009.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
  O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai
  Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der
  Wal.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling, 2023.

\bibitem{gpt-neo}
Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Percy Liang, et~al.
\newblock On the opportunities and risks of foundation models, 2021.

\bibitem{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, et~al.
\newblock Evaluating large language models trained on code, 2021.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.

\bibitem{coleman2019selection}
Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter
  Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning,
  2019.

\bibitem{gagne1962acquisition}
Robert~M Gagne.
\newblock The acquisition of knowledge.
\newblock {\em Psychological review}, 69(4):355, 1962.

\bibitem{gagne1961abilities}
Robert~M Gagne and Noel~E Paradise.
\newblock Abilities and learning sets in knowledge acquisition.
\newblock {\em Psychological Monographs: General and Applied}, 75(14):1, 1961.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{eval-harness}
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles
  Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
  Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,
  and Andy Zou.
\newblock A framework for few-shot language model evaluation, September 2021.

\bibitem{koala_blogpost_2023}
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey
  Levine, and Dawn Song.
\newblock Koala: A dialogue model for academic research.
\newblock Blog post, April 2023.

\bibitem{ghorbani2019data}
Amirata Ghorbani and James Zou.
\newblock Data shapley: Equitable valuation of data for machine learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2242--2251. PMLR, 2019.

\bibitem{palm2techreport}
{Google}.
\newblock Palm2 technical report.
\newblock Technical report, 2023.

\bibitem{gupta2020}
Anupam Gupta.
\newblock Advanced algorithms: Notes for cmu 15-850 (fall 2020), 2020.

\bibitem{gururangan2020dont}
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A. Smith.
\newblock Don’t stop pretraining: Adapt language models to domains and tasks.
\newblock {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, 2020.

\bibitem{guu2023simfluence}
Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga
  Bolukbasi.
\newblock Simfluence: Modeling the influence of individual training examples by
  simulating training runs, 2023.

\bibitem{hendersonkrass2022pileoflaw}
Peter Henderson*, Mark~S. Krass*, Lucia Zheng, Neel Guha, Christopher~D.
  Manning, Dan Jurafsky, and Daniel~E. Ho.
\newblock Pile of law: Learning responsible data filtering from the law and a
  256gb open-source legal dataset, 2022.

\bibitem{hern2022scaling}
Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer
  El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume,
  Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei,
  Nicholas Joseph, Jared Kaplan, and Sam McCandlish.
\newblock Scaling laws and interpretability of learning from repeated data,
  2022.

\bibitem{ilyas2022datamodels}
Andrew Ilyas, Sung~Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander
  Madry.
\newblock Datamodels: Predicting predictions from training data, 2022.

\bibitem{kim2023taskweb}
Joongwon Kim, Akari Asai, Gabriel Ilharco, and Hannaneh Hajishirzi.
\newblock Taskweb: Selecting better source tasks for multi-task nlp, 2023.

\bibitem{kirk2021bias}
Hannah~Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi,
  Frederic Dreyer, Aleksandar Shtedritski, and Yuki Asano.
\newblock Bias out-of-the-box: An empirical analysis of intersectional
  occupational biases in popular generative language models.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 2611--2624. Curran Associates, Inc., 2021.

\bibitem{kitaev-etal-2019-multilingual}
Nikita Kitaev, Steven Cao, and Dan Klein.
\newblock Multilingual constituency parsing with self-attention and
  pre-training.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 3499--3505, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem{kitaev-klein-2018-constituency}
Nikita Kitaev and Dan Klein.
\newblock Constituency parsing with a self-attentive encoder.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 2676--2686,
  Melbourne, Australia, July 2018. Association for Computational Linguistics.

\bibitem{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions, 2017.

\bibitem{kung-etal-2021-efficient}
Po-Nien Kung, Sheng-Siang Yin, Yi-Cheng Chen, Tse-Hsuan Yang, and Yun-Nung
  Chen.
\newblock Efficient multi-task auxiliary learning: Selecting auxiliary data by
  feature similarity.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 416--428, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{langberg2010universal}
Michael Langberg and Leonard~J. Schulman.
\newblock {\em Universal approximators for integrals}, pages 598--607.

\bibitem{laurenon2023bigscience}
Hugo Laurençon, Lucile Saulnier, et~al.
\newblock The bigscience roots corpus: A 1.6tb composite multilingual dataset,
  2023.

\bibitem{lee2022deduplicating}
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
  Chris Callison-Burch, and Nicholas Carlini.
\newblock Deduplicating training data makes language models better.
\newblock {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, 2022.

\bibitem{lewis1995sequential}
David~D Lewis.
\newblock A sequential algorithm for training text classifiers: Corrigendum and
  additional data.
\newblock In {\em Acm Sigir Forum}, volume~29, pages 13--19. ACM New York, NY,
  USA, 1995.

\bibitem{li2023transformers}
Yuchen Li, Yuanzhi Li, and Andrej Risteski.
\newblock How do transformers learn topic structure: Towards a mechanistic
  understanding, 2023.

\bibitem{liang2021towards}
Paul~Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov.
\newblock Towards understanding and mitigating social biases in language
  models.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 6565--6576. PMLR, 18--24 Jul 2021.

\bibitem{madaan2022language}
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig.
\newblock Language models of code are few-shot commonsense learners, 2022.

\bibitem{michaud2023quantization}
Eric~J. Michaud, Ziming Liu, Uzay Girit, and Max Tegmark.
\newblock The quantization model of neural scaling, 2023.

\bibitem{mindermann2021prioritized}
Sören Mindermann, Muhammed Razzak, Winnie Xu, Andreas Kirsch, Mrinank Sharma,
  Adrien Morisot, Aidan~N. Gomez, Sebastian Farquhar, Jan Brauner, and Yarin
  Gal.
\newblock Prioritized training on points that are learnable, worth learning,
  and not yet learned (workshop version), 2021.

\bibitem{mirzasoleiman2019coresets}
Baharan Mirzasoleiman, Jeff Bilmes, and Jure Leskovec.
\newblock Coresets for data-efficient training of machine learning models,
  2019.

\bibitem{naturalinstructions}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock In {\em ACL}, 2022.

\bibitem{moore2010intelligent}
Robert~C. Moore and William Lewis.
\newblock Intelligent selection of language model training data.
\newblock In {\em Proceedings of the {ACL} 2010 Conference Short Papers}, pages
  220--224, Uppsala, Sweden, July 2010. Association for Computational
  Linguistics.

\bibitem{mpt}
MosaicML.
\newblock Introducing mpt-7b: A new standard for open-source, commercially
  usable llms, 2023.

\bibitem{nadeem2021stereoset}
Moin Nadeem, Anna Bethke, and Siva Reddy.
\newblock Stereoset: Measuring stereotypical bias in pretrained language
  models.
\newblock {\em Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, 2021.

\bibitem{nanda2023progress}
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{nemirovskij1983problem}
Arkadij~Semenovi{\v{c}} Nemirovskij and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem{paul2021deep}
Mansheej Paul, Surya Ganguli, and Gintare~Karolina Dziugaite.
\newblock Deep learning on a data diet: Finding important examples early in
  training, 2021.

\bibitem{phillips2016coresets}
Jeff~M. Phillips.
\newblock Coresets and sketches, 2016.

\bibitem{prystawski2023think}
Ben Prystawski and Noah~D. Goodman.
\newblock Why think step-by-step? reasoning emerges from the locality of
  experience, 2023.

\bibitem{reddy2016latent}
Siddharth Reddy, Igor Labutov, and Thorsten Joachims.
\newblock Latent skill embedding for personalized lesson sequence
  recommendation, 2016.

\bibitem{ruder2017data}
Sebastian Ruder, Parsa Ghaffari, and John~G. Breslin.
\newblock Data selection strategies for multi-domain sentiment analysis, 2017.

\bibitem{ruder2017learning}
Sebastian Ruder and Barbara Plank.
\newblock Learning to select data for transfer learning with bayesian
  optimization.
\newblock {\em Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, 2017.

\bibitem{saxena2019data}
Shreyas Saxena, Oncel Tuzel, and Dennis DeCoste.
\newblock Data parameters: A new family of parameters for learning a
  differentiable curriculum.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{sorscher2022neural}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari~S.
  Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning, 2022.

\bibitem{soviany2022curriculum}
Petru Soviany, Radu~Tudor Ionescu, Paolo Rota, and Nicu Sebe.
\newblock Curriculum learning: A survey.
\newblock {\em International Journal of Computer Vision}, 130(6):1526–1565,
  Apr 2022.

\bibitem{stevenson2022putting}
Claire Stevenson, Iris Smal, Matthijs Baas, Raoul Grasman, and Han van~der
  Maas.
\newblock Putting gpt-3's creativity to the (alternative uses) test, 2022.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{redpajama}
Together.
\newblock Redpajama-data: An open source recipe to reproduce llama training
  dataset, 2023.

\bibitem{toneva2018empirical}
Mariya Toneva, Alessandro Sordoni, Remi~Tachet des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J. Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning, 2018.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem{varshney2022let}
Neeraj Varshney, Swaroop Mishra, and Chitta Baral.
\newblock Let the model decide its curriculum for multitask learning, 2022.

\bibitem{wang2023farewell}
Xiao Wang, Weikang Zhou, Qi~Zhang, Jie Zhou, Songyang Gao, Junzhe Wang, Menghan
  Zhang, Xiang Gao, Yunwen Chen, and Tao Gui.
\newblock Farewell to aimless large-scale pretraining: Influential subset
  selection for language model, 2023.

\bibitem{wang2022selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated
  instructions, 2022.

\bibitem{wang2022supernaturalinstructions}
Yizhong Wang, Swaroop Mishra, et~al.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks, 2022.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners, 2021.

\bibitem{white1973research}
Richard~T White.
\newblock Research into learning hierarchies.
\newblock {\em Review of Educational Research}, 43(3):361--375, 1973.

\bibitem{white1974past}
Richard~T. White and Robert~M. Gagné.
\newblock Past and future research on learning hierarchies.
\newblock {\em Educational Psychologist}, 11(1):19--28, 1974.

\bibitem{wu2020curricula}
Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock When do curricula work?, 2020.

\bibitem{xie2023doremi}
Sang~Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy
  Liang, Quoc~V. Le, Tengyu Ma, and Adams~Wei Yu.
\newblock Doremi: Optimizing data mixtures speeds up language model
  pretraining, 2023.

\bibitem{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang.
\newblock Data selection for language models via importance resampling, 2023.

\bibitem{zamir2018taskonomy}
Amir~R. Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik,
  and Silvio Savarese.
\newblock Taskonomy: Disentangling task transfer learning.
\newblock {\em 2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, Jun 2018.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.

\bibitem{zhang2022unveiling}
Yi~Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and
  Tal Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task, 2022.

\end{thebibliography}
