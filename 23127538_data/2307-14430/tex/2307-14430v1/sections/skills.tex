\section{Skills framework}

\vspace{-0.5em}

First, we propose definitions of skills and ordered skill sets in order to formalize our intuition around how models learn skills, and we demonstrate that not just any existing notion of data groups can characterize an ordered skill set in the dataset. Then, we demonstrate the existence of ordered skill sets on synthetic and real data, which show how viewing data through a skills-based framework can help with training and understanding model performance. Finally, we explore unsupervised skill recovery from data, finding that embedding-based approaches do not adequately recover synthetic skills.
\vspace{-0.5em}

\subsection{Definitions}\label{sec:def}


% Figure environment removed

We first present a definition of an individual skill. 
Let the input space of all possible text data be $\X$, where $x \in \X$ is an individual text sample that a next-token-prediction LM $f \in \F: \X \rightarrow \X $ is trained on.
We quantify learning via a metric $L: \F \times \X \rightarrow \R$, which maps from a model and evaluation data to a scalar quantity. In our setup, we use the cross-entropy validation loss applied over next-token predictions as our metric $L$.

\begin{definition}[Skill]\label{def:skill}
A \emph{skill} $s$ is a unit of behavior with associated data $\X_s \subseteq \X$ such that if $f$ is trained on an dataset $\D_s \subset \X_s$, then $f$ has improved metric $L$ on samples belonging to $\X_s \backslash \D_s$ on average.
\end{definition}

This definition of a skill is flexible---it simply means that given a training dataset associated with the skill, a model $f$ has an improved metric (e.g., decreasing validation loss) when evaluated on validation data associated with this skill. Under this definition, a skill could be a granular task, such as Spanish question generation for a subset of Wikipedia articles, or can be defined over a data source, such as next-token prediction of legal data from tax court rulings.
However, our next definition, the ordered skill set, has a more specific construction and provides a framework for how models learn across dependent skills.

\begin{definition}[Ordered skill set, skills graph] \label{def:oss}
An \emph{ordered skill set} for $f$ is a collection of skills $\mathcal{S} = \{s_1, \dots, s_k\}$ over which there is a directed \textit{skills graph} $G = (\mathcal{S}, E)$ on the skill set 
that is neither complete or empty, 
where $(s_i, s_j) \in E$ if the amount of data needed to learn $s_j$ when uniformly sampling from $\D_{s_i} \cup \D_{s_j}$ is no more than the amount of data needed when sampling only from $\D_{s_j}$. 
We equate learning a skill $s_j$ to $f$ attaining a certain value of $L$ or lower on average over $\X_{s_j} \backslash \D_{s_j}$.
\end{definition}
 This definition isolates complete and empty graphs as extrema that do not capture meaningful sets of skills. We discuss the three types of skill graphs---complete, empty, intermediate---and their implications for data selection. 
In particular, we discuss how several initial attempts of defining skills over datasets via semantic groupings resulted in the extrema cases (see Appendix~\ref{supp:skill_graphs} for full results):
 
\begin{itemize}[itemsep=0.05pt,topsep=0pt,leftmargin=*]
    \item The complete graph demonstrates that all skills influence each other. A random partition is an example of a skill set that yields a complete graph. This graph suggests that the best approach for learning any skill or set of skills is random sampling on the dataset. This is not a setting where we can gain much with skill-based sampling. For example, using instruction types as skills on the Alpaca dataset results in a nearly complete estimated skills graph ($97.4\%$ dense), and we find that stratified sampling on these skills only improves validation loss per skill by $0.007$ points over random sampling on average (Figure~\ref{fig:heatmaps} left), suggesting that utilizing skills does not improve model performance in this case.
    \item The empty graph demonstrates that each skill is independent. 
    This can occur if skills are too granular; for instance, learning Spanish math problems is unlikely to help with English poem generation. This graph suggests that the best approach for learning an individual skill is to train on the skill itself. We see that empty graphs exist in real data; in Figure~\ref{fig:heatmaps} (center), using data sources as skills on the Pile of Law~\cite{hendersonkrass2022pileoflaw} results in a nearly empty skills graph ($3.9\%$ dense).
    \item Graphs that are neither empty nor complete thus suggest a nontrivial order of how skill influence each other. \textit{This is the setting in which we expect that identifying skills and exploiting their ordering will help the most}. In Figure~\ref{fig:heatmaps} right, we use task categories, which capture broader reasoning patterns, as skills on Natural Instructions and find that the estimated graph has intermediate density ($42.7\%$ dense). We show concrete examples of how skills can be learned more efficiently on Natural Instructions in Section~\ref{sec:skill_examples}.
\end{itemize}

While these intuitive groupings result in ordered skill sets on some datasets (e.g., task categories on NI), this is not always the case (e.g., instruction types on Alpaca and sources on Pile of Law).
Even though these groupings capture some notion of diversity in the dataset, our findings suggest that not just any semantic grouping induces an ordered skill set.
%
We now empirically demonstrate that our definition of ordered skill sets aligns with how models learn and can be exploited for more data-efficient training.
\vspace{-0.5em}

\subsection{Examples of skills and ordered skill sets} \label{sec:skill_examples}

% Figure environment removed

We provide examples of ordered skill sets on the LEGO synthetic dataset, an addition synthetic dataset, and subsets of the Natural Instructions dataset. On these datasets, we find that certain skills are better learned when trained along with their prerequisite skills rather than in isolation.


\textbf{LEGO skills} \;
The LEGO synthetic, first introduced in~\cite{zhang2022unveiling}, can evaluate a model's ability to follow a chain of reasoning. In this synthetic, the letters of the alphabet, $\A$, are variables each with some binary label in $\{0, 1\}$. An individual sample consists of $k$ clauses for some fixed $k$ across the dataset, each of the form $a = gx$ where $a, x \in \A$ and $g$ is either a negation (``not'') or assertion (``val''), e.g. we assign $a$ to the value of $x$, or we assign $a$ to the opposite label. At the end of the sentence, we prompt the model for what the value of one of these variables is. Two samples $x \in \X$ are given below for $k = 5$:

\begin{quote}
    Input: b = not y, r = val 1, m = val b, q = val m, y = not r. Output: b = 1.  
    
    Input: c = val x, p = val f, x = val k, f = not c, k = val 0. Output: k = 0.
\end{quote}

These samples each correspond to a chain of reasoning; for instance the first sample has the chain $r, y, b, m, q$, where knowing $q$'s label requires the most reasoning steps. 
We define the $i$th skill $s_i$ as the model's ability to know the $i$th variable of the chain. From our example above, the first sample belongs to $\X_{s_3}$ and the second sample belongs to $\X_{s_1}$. To demonstrate the existence of ordered skill sets, we continually pre-train the 125M parameter GPT-Neo model~\cite{gao2020pile, gpt-neo} over various mixtures of LEGO skills with $k = 5$. In Figure~\ref{fig:ni_lego_examples} (left), we find that 
in 35.9\% fewer training steps, training on a balanced mixture of $\X_{s_1}, \X_{s_2}$, and $\X_{s_3}$ resulted in the same validation loss of $0.01$ as training solely on $\X_{s_3}$.
This suggests that $s_1, s_2$ helped unlock performance on $s_3$ and that there exist edges from $s_1$ or $s_2$ to $s_3$ in the skill graph.
Additional observations are available in Appendix~\ref{supp:lego_exp}, where we examine other edges as well as more complex reasoning chains, and the full skills graph corresponding to the ordered skill set for LEGO with $k=5$ is in Figure~\ref{fig:lego_heatmap}.

\textbf{Addition skills} \;
We consider a variant of a synthetic 5-digit addition dataset analyzed in \cite{nanda2023progress}.
We show the existence of ordered skill sets for a simplified 3-digit addition dataset where we treat each digit prediction as a skill---the outputs, in this case, are the integers $\{0, 1, ..., 9\}$. 
Examples are of the following form:
\begin{quote}
    Input: A = 1 0 6 + 0 7 1 , A 0 = ? Output: 7 $\quad$ Input: A = 6 0 6 + 8 7 9 , A 2 = ? Output: 4 
\end{quote}
% \vspace{-0.5em}
where `A 0' refers to the ones digit of the output ($s_1$) and `A 2' refers to the hundreds digit ($s_3$).
In Figure~\ref{fig:ni_lego_examples} (center), we find that in $32\%$ fewer training steps, training on a balanced mixture of $\X_{s_1}$, and $\X_{s_2}$ resulted in the same validation loss of $0.01$ as training solely on $\X_{s_1}$.
That is, the ones digit addition skill can be improved by simultaneously learning the tens digit addition skill, even though the former should not require information from the latter---this is in line with observations from prior work that models do not always learn the ones digit addition first~\cite{nanda2023progress}. The full skills graph corresponding to the ordered skill set over 3-digit addition is in Figure~\ref{fig:addition_heatmap}.

\textbf{Natural Instructions (NI) skills} \;
We show that ordered skill sets exist in NI~\cite{wang2022supernaturalinstructions} when we treat task categories as skills. 
\begin{itemize}[itemsep=0.05pt,topsep=0pt,leftmargin=*]
    \item In Figure~\ref{fig:ni_lego_examples} (top right), we show that ordered skill sets exist over crosslingual task categories. Training on Spanish question generation (QG) along with equal parts of English QG, Spanish question answering (QA), and English QA results in $4.1\%$ lower validation loss than training only on Spanish QG. Remarkably, the former only uses $25\%$ of the latter's Spanish QG data. 
    This suggests that there are edges from Spanish QA, English QA, and English QG to Spanish QG.
    \item In Figure~\ref{fig:ni_lego_examples} (bottom right), we see that training on the task category Text Matching along with Stance Detection helps decrease the loss on Stance Detection by $11\%$. This suggests that these categories, which both involve understanding the relationship between two input texts, share an edge.
\end{itemize}

The full skills graphs corresponding to the ordered skill sets over these task categories are in Figure~\ref{fig:ni_ft_heatmap}. While equating task categories to skills may be noisy, these examples suggest that there is signal within real data that suggests that ordered skill sets can improve data efficiency. 

\vspace{-0.5em}
\subsection{Skill recovery} \label{sec:skill_recovery}

\vspace{-0.5em}
A final component of characterizing skills is unsupervised recovery of ordered skill sets. We consider embedding-based clustering approaches and a loss-based clustering approach for recovering LEGO skills. When clustering data using various trained and pre-trained embeddings, we find that they were unable to achieve above $39\%$ accuracy on LEGO. Instead, we find that taking $10$ random training runs and clustering data by their \textit{loss} per timestep per run recovers the skills with $61\%$ accuracy (Table~\ref{tab:skill_recovery}). The intuition behind this method is that the validation losses on points from the same skill have similar trajectories as models learn. We discuss this approach more in Appendix~\ref{supp:skill_clustering}. 

\vspace{-0.5em}

