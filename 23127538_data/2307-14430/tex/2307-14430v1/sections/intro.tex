\section{Introduction}

% What is the problem?
Large language models (LMs) exhibit remarkable capabilities, including producing creative content~\cite{stevenson2022putting},  writing source code~\cite{chen2021evaluating}, or chatting with users~\cite{brown2020language}.
%
A key ingredient in enabling models to perform such tasks is the data on which the models are trained~\cite{palm2techreport, gururangan2020dont, touvron2023llama}. 
% 
A natural way to unlock particular capabilities is to improve this training data. However, it is unclear how to select data from a large corpus for these capabilities given a fixed budget of training tokens, as 
%
data selection methods for current state-of-the-art LMs mostly rely on heuristics for filtering and mixing together different datasets~\cite{lee2022deduplicating, touvron2023llama}. 
We lack a formal framework for capturing how data influences the model's capabilities and how to utilize this data effectively for improving LM performance. 

% Figure environment removed

To develop such a framework, we take inspiration from how humans acquire knowledge. 
%
A classic idea in education literature is the concept of \textit{skills} that form a learning hierarchy~\cite{white1973research}. 
%
For example, one study found that students learned mathematical and scientific skills most quickly when these skills were presented in a particular order~\cite{gagne1962acquisition}.
We seek to understand the extent that similar skill-based orderings characterize LM training.
%
Such orderings, if they exist, may provide a better understanding of LMs as well as a mechanism for data-efficient training. 
%
For instance, to train an LM for Spanish question generation, we wish to know if training first on related but simpler tasks, such as Spanish grammar and English question generation, helps.

We study if the idea of skill orderings can help us build a framework that relates data to LM training and behavior.
%
This requires addressing two challenges revolving around the connection between skills and data. 
%
First, in order to show that there exist sets of skills that the LM learns most efficiently in some particular order, \textit{an operational definition of LM skill and skill ordering must be developed and validated on data}.
%
In initial experiments, we investigated if semantic groupings of data, such as metadata attributes or embedding clusters, were sufficient to represent a skill and characterize how models learn. For instance, we partitioned the Alpaca dataset~\cite{alpaca} by instruction type---a technique used to capture dataset diversity~\cite{wang2022selfinstruct}---but we found that sampling based on instruction types and random sampling resulted in similar model performance, suggesting that not just any existing notion of data groups can characterize skills.

Second, \textit{these definitions of skills must be used to construct sampling distributions to actually improve model training.} 
To develop criteria for a data selection algorithm that learns skills efficiently, we identify challenges that naive selection approaches face. The standard approach of random uniform sampling over data fails to learn skills optimally due to not accounting for skill imbalance and ordering. Skills can be distributed unevenly in the data, with more complex skills being rare---for instance, Spanish and question generation (QG) are $5\%$ and $4\%$ of the Natural Instructions dataset~\cite{wang2022supernaturalinstructions}, respectively, but Spanish QG is only $0.2\%$.
Random sampling also provides no mechanism for taking into account a particular training order and dependency structure on skills. More sophisticated techniques like curriculum learning account for sample-level ordering, but not skills or their dependencies.
%
Our goal framework must account for these issues of imbalance and ordering.

\textbf{Skill-based framework}  
We define a \emph{skill} as a unit of behavior that a model can learn using an associated slice of data (Definition~\ref{def:skill}). An \emph{ordered skill set} is a collection of skills with a directed \textit{skills graph} that is neither complete nor empty, where an edge from a prerequisite skill to a skill exists if the amount of training it takes to learn the skill
can be reduced if the prerequisite skill is also learned (Definition~\ref{def:oss}, Figure~\ref{fig:banner} left, center).
We show that ordered skill sets exist in synthetic and real datasets using this operational definition. 
Interestingly, the existence of these ordered skill sets unveils 
that one can learn a skill quickly not by training solely on that skill, but on a mixture of that skill and prerequisite skills. For instance, in Figure~\ref{fig:ni_lego_examples} we observe that Spanish QG can be learned more efficiently when the model also learns English QG and Spanish---we can achieve $4\%$ lower validation loss than training on only Spanish QG over a fixed budget of overall training steps. 


Next, given an ordered skill set to train on, we use our framework to propose methods for how to select data so that the LM learn skills faster: skill-stratified sampling and an online generalization, \name. 
%
We address the issue of unevenly distributed skills in datasets by proposing skill-stratified sampling, a simple approach that allows us to explicitly optimize for learning skills by uniformly sampling relevant skills (such as a target skill and its prerequisite skills in fine-tuning). 
Skill-stratified sampling uses the construction of the ordered skill set but is static, which does not incorporate the ordering as training proceeds and results in oversampling skills that may be already learned early on in training.
%
We address this issue by proposing an online data selection algorithm, \name, for selecting mixtures of training skills that allocates more weight towards learning skills that are not yet learned or towards prerequisite influential skills (Figure~\ref{fig:banner} right). \name is derived from an online optimization problem over the training skills for minimizing loss on a set of evaluation skills given a fixed budget of data and the skills graph. \name is inspired by online mirror descent and can be adapted for continual pre-training, fine-tuning, or out-of-domain evaluation depending on the relationship between the evaluation skill set and the training skill set.

We evaluate \name on synthetic and real datasets at two model scales, 125M and 1.3B parameters. 
For the continual pre-training setting, we show on the LEGO synthetic~\cite{zhang2022unveiling} that we obtain a $35.8$ point improvement in accuracy over randomly selecting training data and curriculum learning~\cite{bengio2009curriculum}. For the fine-tuning setting, we show that on the widely-used Natural Instructions dataset~\cite{naturalinstructions, wei2021finetuned}, our algorithm over a mixture of skills is able to achieve up to 13.6\% lower loss on that skill than solely training on that skill, given the same overall training budget. For the out-of-domain setting when our training skills do not align perfectly with evaluation skills, our algorithm is able to achieve the lowest loss on 11 out of 12 evaluation skills corresponding to task categories in the Natural Instructions test tasks dataset over random and skill-stratified sampling on the training data. We finally apply our framework to a case study on the recent RedPajama 1.2 trillion token dataset~\cite{redpajama}. We use the data mixture produced by \name to continually pre-train a 3B parameter model. We find that \name achieves higher accuracy with 1B tokens than uniform sampling over data sources with 3B tokens.