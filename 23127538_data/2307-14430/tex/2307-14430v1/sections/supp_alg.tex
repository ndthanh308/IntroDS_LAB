\section{Additional Algorithmic Details}\label{supp:alg}

\subsection{Derivation of \name Update Rule}
First, we provide the derivation of our update rule from online mirror descent using the proximal point view~\cite{gupta2020}. We restate our optimization problem from~\eqref{eq:skillit_opt}:
\begin{align}\label{eq:skillit_opt_again}
    &\underset{p_1, \dots, p_T \in \Delta^{k-1}}{\text{minimize}} \; \frac{1}{m}\sum_{j = 1}^m \Leval{j}(f_T) \\
    &\text{s.t} \;\;\; \Leval{j}(f_t) = \Leval{j}(f_{t-1})(1 - \alpha A_{:, j}^\top p_{t-1}) \; \forall j \in [m], t = 1, \dots, T \nonumber \\
    & \;\;\;\;\;\; f_t = \Phi(f_{t-1}, p_{t-1}) \; \forall t = 1, \dots T \nonumber 
 \end{align}
 
Let $\bar{L}_t(p) = \frac{1}{m} \sum_{i = j}^m \Leval{j}(f_{t+1}) = \frac{1}{m} \sum_{i = j}^m \Leval{j}(\Phi(f_{t}, p))$; that is, $p$ is the mixture we must choose at time $t$ and $\bar{L}_t$ is the average loss per skill of the model after it is trained on $p$ at round $t$.
A greedy approximation of~\eqref{eq:skillit_opt_again} is $\underset{p \in \Delta^{k-1}}{\text{minimize}} \; \bar{L}_t(p)$, given the model and mixtures at previous rounds.
A linear approximation of $\bar{L}_{t}(p)$ is
\begin{align}\label{eq:approx}
    \bar{L}_{t}(p) \approx \bar{L}_{t}(p_{t-1}) + \langle \triangledown \bar{L}_{t-1}(p_{t-1}), p - p_{t-1}\rangle
\end{align}

Then, the problem of minimizing $\bar{L}_t(p)$ can be approximated as
\begin{align}
    \text{argmin}_{p\in \Delta^{k-1}} \langle \eta \triangledown \bar{L}_{t-1}(p_{t-1}), p \rangle
\end{align}

after we drop terms from~\eqref{eq:approx} that do not depend on $p$. Note that the $\eta$ is a constant and does not impact the solution. The optimal solution to this problem is selecting the $p$ that has the most weight on the slice with the largest gradient (e.g., a folow-the-leader sort of algorithm). To improve stability and prevent overfitting, we introduce regularization via a Bregman divergence $D_h(p || p_{t-1}) = h(p) - h(p_{t-1}) - \langle \triangledown h(p_{t-1}), p - p_{t-1} \rangle$. After dropping terms that do not contain $p$, our problem is now
\begin{align}
    \text{argmin}_{p\in \Delta^{k-1}} \langle \eta \triangledown \bar{L}_{t-1}(p_{t-1}), p \rangle + h(p) - \langle \triangledown h(p_{t-1}), p \rangle 
\end{align}

Taking the gradient and setting it equal to $0$ gives us
\begin{align}
    \eta \triangledown \bar{L}_{t-1}(p_{t-1}) + \triangledown h(p)-\triangledown h(p_{t-1}) = 0
\end{align}


Similar to in standard multiplicative weights, we set $h(p) = \sum_i p_i \ln p_i$ and $\triangledown h(p) =  [\ln p_i + 1]_i$. Then,
\begin{align}
    \ln p^{i}= \ln p_{t-1}^{i} - \eta \triangledown_i L_{t-1} (p_{t-1}) \nonumber \\
    \Rightarrow p^{i}_{t+1} = p_{t}^{i} \exp( -\eta\triangledown_i \bar{L}_{t} (p_{t}))\label{eq:gradient_1}
\end{align}

where $\triangledown_i$ is the $i$th element of the gradient. 
Now we wish to compute $\triangledown_i \bar{L}_{t}(p_{t}) = \frac{1}{m} \sum_{j = 1}^m \triangledown_i [\Leval{j}(f_{t+1})]  = \newline \frac{1}{m} \sum_{j = 1}^m \triangledown_i [\Leval{j}(\Phi(f_t, p_t))]$. Recall the dynamics model for $L_{\text{eval}}$:
\begin{align}
    \Leval{j}(f_{t+1})=\Leval{j}(f_t)(1- A_{:, j}^\top p_{t}),
\end{align}

The gradient of this model with respect to each training skill $s_i$ is
\begin{align}
    &\triangledown_i \Leval{j}(f_{t+1}) = - A_{ij}\Leval{j}(f_t) \\
    \Rightarrow & \triangledown_i \bar{L}_{t}(p_t) = \frac{1}{m} \sum_{j = 1}^m - A_{ij} \Leval{j}(f_t)  \nonumber 
\end{align}

Plugging this back into~\eqref{eq:gradient_1},
\begin{align}
    p_{t+1}^i= p_t^i \exp \bigg(\eta \sum_{j = 1}^m A_{ij} \Leval{j}(f_t)\bigg),
\end{align}

where we can absorb the $\frac{1}{m}$ into $\eta$.


\subsection{Graph Learning Method}\label{supp:graph_learning}

\begin{algorithm}[tb]
   \caption{\textsc{LearnGraph} (Brute-Force)}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Ordered skill set $\mathcal{S} = \{s_1, \dots, s_k\}$. Number of training steps $H$, base model $f$.
   \FOR{$j \in [k]$}
        \STATE Train $f$ on samples from $\X_{s_j}$ for $H$ steps and denote $f_{H, j}$ to be the model after training.
        \STATE Observe change in loss, $\delta_j^j = \Leval{j}(f) - \Leval{j}(f_{H, j})$.
   \ENDFOR
   \FOR{$i, j \in [k]$}
        \STATE Train $f$ on samples from $\X_{s_i} \cup \X_{s_j}$ for $H$ steps and denote $f_{H, i, j}$ to be the model after training.
        \STATE Observe change in loss, $\delta_j^{i, j} = \Leval{j}(f) - \Leval{j}(f_{H, i, j})$. 
        \IF{$\delta_j^{ij} > \delta_j^j$}
            \STATE Draw edge $s_i \rightarrow s_j$ and set $A_{ij} > 0$. 
        \ENDIF
   \ENDFOR
   \STATE \textbf{Return} Adjacency matrix $A \in \R^{k \times k}$
\end{algorithmic}
\label{alg:bruteforce_graph}
\end{algorithm}


\begin{algorithm}[tb]
   \caption{\textsc{LearnGraph} (Approximate)}
\begin{algorithmic}[1]
   \STATE {\bfseries Input:} Ordered skill sets $\strainset$ and $\sevalset$. Number of training steps $H$, base model $f$.
   \FOR{$i \in [k]$}
        \STATE Train $f$ on samples from $\X_{\strain{i}}$ for $H$ steps and denote $f_{H, i}$ to be the model after training.
        \FOR{$j \in [m]$}
        \STATE Observe change in loss, $\delta_j^i = \Leval{j}(f) - \Leval{j}(f_{H, i})$.
        \STATE If $\delta_j^i > 0$, draw edge $\strain{i} \rightarrow \seval{j}$ and set $A_{ij} > 0$.
        \ENDFOR
   \ENDFOR
   \STATE \textbf{Return} Bipartite adjacency submatrix $A \in \R^{k \times m}$
\end{algorithmic}
\label{alg:approximate_graph}
\end{algorithm}



We provide algorithms for learning the graph over an ordered skill set. In Algorithm~\ref{alg:bruteforce_graph}, we discuss the brute-force approach for learning the adjacency matrix. This approach only works when $\sevalset \subseteq \strainset$ (e.g. pre-training and fine-tuning cases), so we denote $\mathcal{S} = \strainset$ in the algorithm box. In Algorithm~\ref{alg:approximate_graph}, we discuss the linear approach for learning the adjacency matrix. This approach works even in the out-of-domain case when $\sevalset$ and $\strainset$ are disjoint.

In both approaches, the exact value of $A_{ij}$ can vary, but we can typically set it proportional to $\delta_{j}^{i, j} - \delta_j^j$, the difference between the changes in loss, in the brute-force case or $\delta_j^i$, the change in loss itself, in the approximate case. The exact constructions and methods for learning each $A$ in our experiments are in Appendix~\ref{supp:skill_graphs}.