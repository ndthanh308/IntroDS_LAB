\section{Related work}
\vspace{-0.5em}

\paragraph{Data selection for LMs}
There have been several studies of large-scale data selection for LMs. 
Data deduplication~\cite{lee2022deduplicating, abbas2023semdedup, hern2022scaling}, in which identical or nearly identical samples are removed, is a method that enables LMs to be trained on smaller, cleaned datasets and has been increasingly used as a pre-processing step for training data~\cite{touvron2023llama, biderman2023pythia, zhang2022opt}. Other methods applied at scale involve ensuring high quality of data by explicitly filtering out samples or comparing the training dataset with a cleaned reference dataset~\cite{brown2020language, touvron2023llama, laurenon2023bigscience}. Importance reweighting approaches have also been proposed for identifying training data from a large corpus that best approximates a smaller target distribution~\cite{xie2023data}, and influence functions have been used to select a subset of training data to improve performance on downstream tasks~\cite{wang2023farewell}. These approaches can identify data pertaining to a particular target distribution or filter out low quality data according to some heuristic, while our work aims to understand how the choice of data is related to the numerous skills that LMs learn. 

Recent development of LMs has shifted focus from emphasizing the scale of the model to prioritizing the training data utilized. For example, models like Alpaca~\cite{alpaca}, Vicuna~\cite{vicuna2023}, and Koala~\cite{koala_blogpost_2023} are all based on the LLaMA model combined with instruction data generated by an existing LM. Palm 2's technical report states that the data mixture was a critical component of the final model~\cite{palm2techreport}, and Mosaic ML's recent MPT model was trained on a hand-engineered mixture of the RedPajama dataset~\cite{mpt}. However, these works lack rigorous explanation for why their training datasets were constructed in this way. 

Finally, perhaps most related to our approach is the contemporary work DoReMi~\cite{xie2023doremi}, which uses group distributionally robust optimization on a smaller LM to select data source mixtures for training a larger LM. Their approach focuses on selecting data at the data source level for optimizing worst-case performance across the training data sources, rather than at the more general skills level for a variety of target skill sets. Furthermore, we focus on understanding how skills are related to each other and induce some order in how LMs learn by explicitly modeling skill graph structure, which we find to be important for data-efficient LM training (see ablations in Appendix~\ref{supp:ablations}).



\paragraph{Data selection methods}

Many data selection methods have been proposed for supervised, task-specific settings. In this setting, the most typical objective is dataset condensation, which aims to identify a small subset of data that captures the larger dataset's properties with respect to the model. Some approaches include constructing coresets~\cite{langberg2010universal, phillips2016coresets}, identifying samples that the model forgets during training~\cite{toneva2018empirical}; identifying samples with the largest gradients~\cite{paul2021deep} or gradients that approximate the overall gradient~\cite{mirzasoleiman2019coresets}; clustering in embedding space and selecting points farthest from cluster centers~\cite{sorscher2022neural}; and selecting samples with the highest uncertainty or entropy~\cite{lewis1995sequential}. These approaches have also been shown to transfer from smaller models to larger models~\cite{coleman2019selection}. Unlike these methods, we study how to select data for learning one or many skills at the mixture level for LMs instead of the instance level.

Another area of interest is data selection for domain adaptation and multitask learning. For domain adaptation, there are a wide range of methods that select data to best match the target distribution. For example, the Moore-Lewis method matches data based on the difference in cross-entropy using a model trained on the target versus a model trained on the source data~\cite{moore2010intelligent}. Several other approaches suggest training a model to distinguish between source and target and selecting points with high uncertainty~\cite{ruder2017data}, or selecting points based on some divergence in an embedding space~\cite{ruder2017learning}. In comparison to these approaches, our work focuses on learning one or many skills and also finds that embedding-based heuristics do not fully identify skills. 

\paragraph{Data attribution} Another perspective on understanding training data is data attribution, which seeks to identify what data is responsible for particular model behaviors. Influence functions~\cite{koh2017understanding} and shapley values~\cite{ghorbani2019data} are two ways to quantify the role of individual samples. Datamodels~\cite{ilyas2022datamodels} fit a model to predict behavior given a subset of training data, providing a framework for understanding individual samples as well as dataset counterfactuals. Simfluence~\cite{guu2023simfluence} fits a Markov process to a set of training trajectories for finer-grained understanding of how data impacts training. We focus on understanding how groups of data associated with skills elicit broader model capabilities, and utilize this understanding to select data for more efficient training.


\paragraph{Curriculum learning} Curriculum learning~\cite{bengio2009curriculum} proposes to show the model data in order from easy samples to hard ones. Various criteria have been used to determine hardness, and anticurriculum as well as various pacing functions and mixing rates have been explored~\cite{soviany2022curriculum}. Curriculum learning can also be performed at the group level~\cite{varshney2022let}. More sophisticated approaches include parametrizing each sample with a dynamic importance~\cite{saxena2019data}, and also accounting for irrelevant and noisy data~\cite{mindermann2021prioritized}. Our approach similarly utilizes a curriculum, but it is defined over a skills graph and does not necessarily align with training on easiest to hardest skills.

\paragraph{How LMs learn} Many different explanations for how LMs learn from data have been proposed. One hypothesis is that there exist discrete, universal building blocks of LM knowledge called quanta, and power law scaling emerges from a learning over a particular distribution of quanta in the right order~\cite{michaud2023quantization}. Another is that chain of thought reasoning emerges due to local clusters of latent variables that influence each other, which can be validated by studying the LM's ability to do conditional inference given intermediate variables~\cite{prystawski2023think}. Others have provided theoretical analysis of how transformers learn topics by studying co-occurrences of words in the training data~\cite{li2023transformers}.
Empirically, how models learn is still a mystery---for instance, models trained on code are found to perform fairly well at commensense reasoning~\cite{madaan2022language}. Our work initiates a study on how LMs learn various skills and how to exploit this for better data selection. 

\paragraph{Task selection} In multitask auxiliary learning, the goal is to train a model to perform well on a target task(s) by selecting the most beneficial source tasks to train on. One can use feature similarity to select tasks~\cite{kung-etal-2021-efficient}, but we find in our synthetics that feature similarity does not always recover skills. In Taskonomy~\cite{zamir2018taskonomy}, a hypergraph over a set of tasks is learned and used to select tasks. The methods used to develop the taxonomy can be applied to further expand our graph learning (e.g., studying transitive and higher-order properties). However, their focus is on task selection in computer vision rather than data selection for LMs to learn skills. Lastly, the contemporary work of TaskWeb~\cite{kim2023taskweb} builds a graph among $22$ common NLP tasks in order to determine what the best source tasks are for a target task. Their definition of an edge in the task graph is less strict than ours (their comparison is on if training on additional data from $s_i$ helps with $s_j$, while we fix the overall amount of training data over both $s_i$ and $s_j$). Overall, our approach is similar in use of the skills graph, but we incorporate it into a dynamic sampling algorithm. Furthermore, we look more broadly at skills, rather than tasks, and characterize when we expect using the skills graph to improve model performance.

\paragraph{Education} The notion of skill has been studied in education. Classical
research on learning hierarchies~\cite{white1974past} identify sets of skills that make up subordinate capabilities for students. For instance,~\cite{gagne1961abilities} identified that in order for students to solve linear equations, there were many prerequisite skills, ranging from the simplest being symbol recognition to the most complex being the ability to add, subtract, multiple, and divide from both sides of the equation. More recently, decision-making over lesson sequences based on skills, e.g., what the student already knows versus what the lesson teaches, has become an area of interest in personalized learning~\cite{reddy2016latent}.
