\section{Additional Experimental Details} \label{supp:details}



\subsection{Datasets}

We present details about each dataset used, including information on the skills and the validation dataset. A summary is presented in Table~\ref{tab:datasets}.


\begin{table}[h]
\small
    \caption{We list each dataset used as well as its corresponding skill. We include the number of skills in the training dataset, as well as details on how the validation dataset is constructed.}
    \centering
    \begin{tabular}{c|c |c | c}
        \toprule 
        Dataset & Skill & $\#$ skills & Validation data  \\
        \midrule 
        Alpaca & Instruction type & $38$ &  $50$ samples per skill \\
        Pile of Law & Legal data source &  $31$ & $645$ samples per skill \\
        LEGO & Reasoning chain depth & $5$ & $100$ samples per skill \\
        Addition & Digit & $3$ &  $100$ samples per skill \\
        NI (pre-training) & Task category & $23$  & $50$ samples per task \\
        NI (Spanish QG) & Task category $\times$ language & $4$  & $100$ samples per task  \\
        NI (stance detection) & Task category & $2$  & $50$ samples per task \\
        NI (out-of-domain) & Task category & $59, 12$ & $400$ samples per task \\
        RedPajama & Data source & $7$ & LM eval harness \\
    \end{tabular}
    \label{tab:datasets}
\end{table}

\begin{itemize}[itemsep=0.05pt,topsep=0pt,leftmargin=*]
    \item Alpaca dataset~\cite{alpaca}: the Alpaca dataset consists of $52$K instruction examples that were generated from text-davinci-003. We applied the Berkeley Neural Parser~\cite{kitaev-etal-2019-multilingual, kitaev-klein-2018-constituency} to each instruction, keeping $40777$ samples it was able to parse successfully. If the sample began with a question, we annotated it with the skill ``question'', and otherwise we annotated it with the verb identified from the parser. We grouped the data into a total of $38$ skills, such as "list", "edit", "calculate", "describe" and "identify".

    \item Pile of Law~\cite{hendersonkrass2022pileoflaw}: the Pile of Law dataset consists of various sources of legal and administrative data, ranging from tax rulings to the world's constitutions. We evaluate on a subset of the Pile of Law validation dataset consisting of $13883$ samples, where we selected max($645$, source size) samples per source. We truncated each sample to be no more than $100$K characters.

    \item LEGO~\cite{zhang2022unveiling}: for the LEGO synthetic, we set $k = 5$ and sample $192000$ points across the skills. Our validation dataset consisted of $100$ samples per skill.

    \item Addition: for the $3$-digit addition synthetic, we set $k = 3$ and sample $192000$ points across the skills. We use a validation dataset of $100$ samples per skill.

    \item Natural Instructions~\cite{wang2022supernaturalinstructions, naturalinstructions}: the Natural Instructions dataset is a large collection of tasks and their definitions in natural language. For the pre-training setting, we used a set of $23$ task categories that had the largest degree (in-degree + out-degree) in the learned skills graph, for a total of $1,232,437$ samples and $425$ tasks to select from. We evaluated on $50$ samples per task. 

    For the fine-tuning setting with Spanish question generation, we select data over $4$ skills (Spanish question generation, Spanish question answering, English question generation, English question answering) for a total of $513210$ samples and $212$ tasks to select from. We evaluated on $100$ samples per task.

    For the fine-tuning setting with stance detection, we select data over $2$ skills (stance detection, text matching) for a total of $50990$ samples and $19$ tasks to select from. We evaluated on $50$ samples per task. 

    For the out-of-domain setting, we select data over all $59$ task categories for a total of $2,417,867$ samples and $753$ tasks to select from. The test split consisted of $12$ task categories and $119$ tasks, and we evaluated on min($400$, task size) samples per task.

    \item RedPajama~\cite{redpajama}: the RedPajama dataset is a $1$-trillion token dataset that aims to reproduce the LLaMA~\cite{touvron2023llama} training dataset. We select over the $7$ data sources and evaluate using the LM evaluation harness~\cite{eval-harness}.
\end{itemize}


\subsection{Graph Learning Details} \label{supp:skill_graphs}


We describe how the skills graph was learned on each dataset. 
\begin{itemize}[itemsep=0.05pt,topsep=0pt,leftmargin=*]
    \item Alpaca (Figure~\ref{fig:alpaca_heatmap}): we use Algorithm~\ref{alg:approximate_graph} and train for $K = 150$ steps per skill. Each edge $i \rightarrow j$ has a weight of $\delta_j^i$, the difference in loss on skill $j$ before and after training on $i$. Next, we compare the average validation loss of skill-stratified sampling versus random sampling when we train for $K = 1000$ steps. We find that skill-stratified sampling only does 0.007 better than random sampling, confirming that Alpaca's dense skills graph suggests that random sampling is the best we can do.  
    \item Pile of Law (Figure~\ref{fig:pol_heatmap}): we use Algorithm~\ref{alg:approximate_graph} and train for $K = 150$ steps. Each edge $i \rightarrow j$ has a weight of $\delta_j^i$, the difference in loss on skill $j$ before and after training on $i$.
    \item LEGO (Figure~\ref{fig:lego_heatmap}): we use both Algorithm~\ref{alg:bruteforce_graph} and Algorithm~\ref{alg:approximate_graph} and train for $K=6000$ steps each. Each edge $i \rightarrow j$ has a weight of $0.5$ if the amount of data associated with skill $j$ that is needed to reach $0.01$ validation loss is less when training on $(i, j)$ than on $j$ (edges are set to $0$ if $0.01$ validation loss is not reached, even if loss is decreasing). Each edge $i \rightarrow j$ is also set to $0.5$ if training on $i$ decreases loss directly on $j$. We set each diagonal entry of $A$ to be $1$.
    \item Addition (Figure~\ref{fig:addition_heatmap}): we use Algorithm~\ref{alg:bruteforce_graph} and train for $K = 6000$ steps. Each edge $i \rightarrow j$ has a weight of $0.5$ if the amount of data associated with skill $j$ that is needed to reach $0.01$ validation loss is less when training on $(i, j)$ than on $j$ (edges are set to $0$ if $0.01$ validation loss is not reached, even if loss is decreasing). We set each diagonal entry of $A$ to be $1$.
    \item Natural Instructions (Figure~\ref{fig:ni_heatmap},~\ref{fig:ni_ft_heatmap},~\ref{fig:ni_test_heatmap}): we use Algorithm~\ref{alg:approximate_graph}. For the pre-training setting, we train for $K = 600$ steps and assign each edge $i \rightarrow j$ a weight $\delta_j^i$ equal to the change in loss on $j$ in the first $100$ steps for all $i, j \in [k]$, including diagonal entries. For the fine-tuning setting, we train for $K = 600$ steps and assign each edge $i \rightarrow j$ a weight $\delta_j^i$ equal to the change in loss before and after training. For the out-of-domain setting, we train for $K = 600$ steps and assign each edge $i \rightarrow j$ a weight $\delta_j^i$ equal to the change in loss before and after training in the first $100$ steps.
    \item RedPajama (Figure~\ref{fig:rp_heatmap}): we use Algorithm~\ref{alg:approximate_graph} and train for $1$ billion tokens per data source. We assign each edge $i \rightarrow j$ a weight $\delta_j^i$ equal to the change in perplexity on the validation datalsoa before and after training.
\end{itemize}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed



\subsection{Training Details}

We describe the parameters used for \name.

\paragraph{\name pre-training}

\begin{itemize}[itemsep=0.05pt,topsep=0pt,leftmargin=*]
    \item LEGO: $\eta = 0.5, T = 6, w = 3$. We train for $6000$ steps.
    \item Addition: $\eta = 0.1, T = 5, w = 3$. We train for $6000$ steps.
    \item Natural Instructions (pre-training): $\eta = 0.2, T = 1$. We train for $5000$ steps.
\end{itemize}

For the LEGO random baseline, when we selected points at random, we used an imbalanced training dataset with proportions 1:1:1:3:5.
For the addition random baseline, we used an imbalanced dataset with randomly selected proportions: 13:14:18. 
For the curriculum learning baselines, the pacing function, $g(i)$, denotes the size of the subset of the highest scoring samples that we uniformly select from in the $i$th epoch. We define our pacing function as $g(i) = \frac{iH}{M}$, where $H$ is the number of steps and $M$ is $5$ epochs for LEGO and NI, and $3$ for addition.

\paragraph{\name fine-tuning}

\begin{itemize}[itemsep=0.05pt,topsep=0pt,leftmargin=*]
    \item LEGO: $\eta = 0.5, T = 10, w = 3$. We train for $6000$ steps.
    \item Addition: $\eta = 0.1, T = 5, w = 3$. We train for $6000$ steps.
    \item Natural Instructions (Spanish QG): $\eta = 0.8, T = 6, w = 3$. We train for $600$ steps.
    \item Natural Instructions (stance detection): $\eta = 0.2, T = 6, w = 3$. We train for $600$ steps.
\end{itemize}

\paragraph{\name out-of-domain}

\begin{itemize}[itemsep=0.05pt,topsep=0pt,leftmargin=*]
    \item Natural Instructions: $\eta = 0.2, T = 10, w = 3$. We train for $5000$ steps.
    \item RedPajama: $\eta = 100, T = 1$. We train for $3$ billion tokens.
\end{itemize}

All results are computed over $5$ random seeds.


Batch sizes of $32$ and $64$ were used for the LEGO and addition synthetic on the 125M and 1.3B parameter model, respectively.
Batch sizes of $4$ and $16$ were used for the Natural Instructions experiments on the 125M and 1.3B parameter model. 

For the out-of-domain Natural Instructions experiment and Alpaca graph learning experiments, a learning rate of 5e-6 with linear scheduler and $50$ warmup steps was used. For the Natural Instructions continual pre-training experiment on the 1.3B parameter model, a learning rate of 1e-6 was used. All other experiments used a learning rate of 5e-5. All experiments used AdamW with betas = 0.9, 0.999, eps = 1e-8, and weight decay = $0.01$. A context window of $512$ was used for all experiments except LEGO and addition, which used a window of $128$.

Experiments with the Addition dataset were run using an Nvidia RTX A6000. 
Other experiments using the GPT-Neo 125M parameter model were run on an Nvidia Tesla P100. Experiments using the GPT-Neo 1.3B parameter model were run on an Nvidia Tesla A100. 


