\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{BWAA18}

\bibitem[ABC{\etalchar{+}}20]{agarwal2020efficient}
Naman Agarwal, Brian Bullins, Xinyi Chen, Elad Hazan, Karan Singh, Cyril Zhang,
  and Yi~Zhang.
\newblock Efficient full-matrix adaptive regularization, 2020.

\bibitem[BH20]{balles2020dissecting}
Lukas Balles and Philipp Hennig.
\newblock Dissecting adam: The sign, magnitude and variance of stochastic
  gradients, 2020.

\bibitem[BMR{\etalchar{+}}20]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.

\bibitem[BWAA18]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems, 2018.

\bibitem[CLH{\etalchar{+}}23]{chen2023symbolic}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu
  Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc~V. Le.
\newblock Symbolic discovery of optimization algorithms, 2023.

\bibitem[DHS11]{JMLR:v12:duchi11a}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12(61):2121--2159, 2011.

\bibitem[GKS18]{gupta2018shampoo}
Vineet Gupta, Tomer Koren, and Yoram Singer.
\newblock Shampoo: Preconditioned stochastic tensor optimization, 2018.

\bibitem[KB17]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.

\bibitem[KCLS23]{kunstner2023noise}
Frederik Kunstner, Jacques Chen, Jonathan~Wilder Lavington, and Mark Schmidt.
\newblock Noise is not the main factor behind the gap between sgd and adam on
  transformers, but sign descent might be, 2023.

\bibitem[KKN{\etalchar{+}}23]{kaddour2023train}
Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt~J. Kusner.
\newblock No train no gain: Revisiting efficient training algorithms for
  transformer-based language models, 2023.

\bibitem[LLH{\etalchar{+}}23]{liu2023sophia}
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma.
\newblock Sophia: A scalable stochastic second-order optimizer for language
  model pre-training, 2023.

\bibitem[MAC{\etalchar{+}}23]{molybog2023theory}
Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman
  Goyal, Punit~Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, Binh
  Tang, Diana Liskovich, Puxin Xu, Yuchen Zhang, Melanie Kambadur, Stephen
  Roller, and Susan Zhang.
\newblock A theory on adam instability in large-scale machine learning, 2023.

\bibitem[MG20]{martens2020optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature, 2020.

\bibitem[SS18]{shazeer2018adafactor}
Noam Shazeer and Mitchell Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost, 2018.

\bibitem[SSW22]{song2022fast}
Yue Song, Nicu Sebe, and Wei Wang.
\newblock Fast differentiable matrix square root, 2022.

\bibitem[TH12]{Tieleman2012}
Tijmen Tieleman and Geoffrey Hinton.
\newblock {RMSPROP}: Divide the gradient by a running average of its recent
  magnitude.
\newblock COURSERA: Neural networks for machine learning, Lecture 6.5, 2012.

\bibitem[TMS{\etalchar{+}}23]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien
  Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[VSP{\etalchar{+}}17]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2017.

\bibitem[ZKV{\etalchar{+}}20]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim,
  Sashank~J Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?, 2020.

\bibitem[ZTD{\etalchar{+}}20]{zhuang2020adabelief}
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek,
  Xenophon Papademetris, and James~S. Duncan.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed
  gradients, 2020.

\end{thebibliography}
