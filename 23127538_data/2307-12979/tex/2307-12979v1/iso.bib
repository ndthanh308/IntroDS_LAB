

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Unpublished{Tieleman2012,
author={Tieleman, Tijmen and Hinton, Geoffrey},
title={{RMSPROP}: Divide the gradient by a running average of its recent magnitude},
note = {COURSERA: Neural networks for machine learning, Lecture 6.5},
year = {2012},
}
@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bernstein2018signsgd,
      title={signSGD: Compressed Optimisation for Non-Convex Problems}, 
      author={Jeremy Bernstein and Yu-Xiang Wang and Kamyar Azizzadenesheli and Anima Anandkumar},
      year={2018},
      eprint={1802.04434},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{kunstner2023noise,
      title={Noise Is Not the Main Factor Behind the Gap Between SGD and Adam on Transformers, but Sign Descent Might Be}, 
      author={Frederik Kunstner and Jacques Chen and Jonathan Wilder Lavington and Mark Schmidt},
      year={2023},
      eprint={2304.13960},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{balles2020dissecting,
      title={Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients}, 
      author={Lukas Balles and Philipp Hennig},
      year={2020},
      eprint={1705.07774},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{kaddour2023train,
      title={No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models}, 
      author={Jean Kaddour and Oscar Key and Piotr Nawrot and Pasquale Minervini and Matt J. Kusner},
      year={2023},
      eprint={2307.06440},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{gupta2018shampoo,
      title={Shampoo: Preconditioned Stochastic Tensor Optimization}, 
      author={Vineet Gupta and Tomer Koren and Yoram Singer},
      year={2018},
      eprint={1802.09568},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{molybog2023theory,
      title={A Theory on Adam Instability in Large-Scale Machine Learning}, 
      author={Igor Molybog and Peter Albert and Moya Chen and Zachary DeVito and David Esiobu and Naman Goyal and Punit Singh Koura and Sharan Narang and Andrew Poulton and Ruan Silva and Binh Tang and Diana Liskovich and Puxin Xu and Yuchen Zhang and Melanie Kambadur and Stephen Roller and Susan Zhang},
      year={2023},
      eprint={2304.09871},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zhang2020adaptive,
      title={Why are Adaptive Methods Good for Attention Models?}, 
      author={Jingzhao Zhang and Sai Praneeth Karimireddy and Andreas Veit and Seungyeon Kim and Sashank J Reddi and Sanjiv Kumar and Suvrit Sra},
      year={2020},
      eprint={1912.03194},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}
@misc{chen2023symbolic,
      title={Symbolic Discovery of Optimization Algorithms}, 
      author={Xiangning Chen and Chen Liang and Da Huang and Esteban Real and Kaiyuan Wang and Yao Liu and Hieu Pham and Xuanyi Dong and Thang Luong and Cho-Jui Hsieh and Yifeng Lu and Quoc V. Le},
      year={2023},
      eprint={2302.06675},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zhuang2020adabelief,
      title={AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients}, 
      author={Juntang Zhuang and Tommy Tang and Yifan Ding and Sekhar Tatikonda and Nicha Dvornek and Xenophon Papademetris and James S. Duncan},
      year={2020},
      eprint={2010.07468},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{shazeer2018adafactor,
      title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}, 
      author={Noam Shazeer and Mitchell Stern},
      year={2018},
      eprint={1804.04235},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{liu2023sophia,
      title={Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training}, 
      author={Hong Liu and Zhiyuan Li and David Hall and Percy Liang and Tengyu Ma},
      year={2023},
      eprint={2305.14342},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{martens2020optimizing,
      title={Optimizing Neural Networks with Kronecker-factored Approximate Curvature}, 
      author={James Martens and Roger Grosse},
      year={2020},
      eprint={1503.05671},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{JMLR:v12:duchi11a,
  author  = {John Duchi and Elad Hazan and Yoram Singer},
  title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {61},
  pages   = {2121--2159},
  url     = {http://jmlr.org/papers/v12/duchi11a.html}
}
@misc{agarwal2020efficient,
      title={Efficient Full-Matrix Adaptive Regularization}, 
      author={Naman Agarwal and Brian Bullins and Xinyi Chen and Elad Hazan and Karan Singh and Cyril Zhang and Yi Zhang},
      year={2020},
      eprint={1806.02958},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{song2022fast,
      title={Fast Differentiable Matrix Square Root}, 
      author={Yue Song and Nicu Sebe and Wei Wang},
      year={2022},
      eprint={2201.08663},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}