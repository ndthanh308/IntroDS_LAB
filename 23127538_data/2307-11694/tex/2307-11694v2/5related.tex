
\section{Related Work}
\label{appendix:related}



\subsection{Molecular Language Models}
In recent years, advances in machine learning and NLP have been applied to molecule representations. Several efforts \citep{fabian2020molecular, chithrananda2020chemberta, vaucher2021inferring, schwaller2021mapping, megamolbartv2, tysinger2023can} show excellent results training on string representations of molecules \citep{weininger1988smiles, weininger1989smiles, krenn2020self, cheng2023group}. Interest has also grown in multi-modal models \citep{edwards2022translation, zeng2022deep} and multi-encoder models \citep{edwards2021text2mol, vall2021bioassayclr, xu2022protranslator, su2022molecular, liu2022multi, seidl2023enhancing, xu2023protst, zhao2023adversarial} with applications to chemistry and biology. Existing work \citep{edwards2022translation, su2022molecular, xu2023multilingual, christofidellis2023unifying} also builds on this to ``translate'' between these modalities, such as MolT5 \citep{edwards2022translation}, which translates between molecules and language. 


\subsection{In-Context Learning}

With the success of models such as GPT-3 \citep{brown2020language} and GPT-4 \citep{openai2023gpt4}, interest has grown in the theoretical properties of in-context learning. \citep{garg2022can}, which we follow in this work, investigates the ability of transformers to learn function classes. \citep{olsson2022context} investigates whether in-context learning is related to specific ``induction heads''. \citep{von2022transformers} shows that transformers do in-context learning by gradient descent. \citep{li2023transformers} frames in-context learning as algorithm learning to investigate generalization on unseen tasks. 

\subsection{Language Models for Chemistry and Knowledge Graph Completion}

Very recently, considerable interest has grown in using language models, particularly GPT-4 \citep{openai2023gpt4}, for uncovering chemical knowledge and molecular discovery \citep{hocky2022natural, white2022large, bran2023chemcrow, boiko2023emergent, white2023assessment, castro2023large}, including work in the few-shot setting \citep{ramos2023bayesian, jablonka2023gpt}. 
CancerGPT \citep{li2023cancergpt}, a related contemporaneous preprint, was recently released which explores a similar few-shot approach to drug-drug synergy prediction. It explores training literature-aware text-based GPT models on drug synergy data. The use of GPT models pretrained on massive textual corpora from the web also makes rigorous evaluation and comparison difficult. We believe our work is complementary, since we largely explore the transformer architecture without language and  we consider in-context learning which they do not. We also consider extensions such as inverse design and context optimization. Due to the recency of \citep{li2023cancergpt}, we leave additional comparisons beyond our real GPT2 baseline to future work. 
Applying language models to knowledge graphs has been investigated in the general \citep{yao2019kg, kim2020multi, youn2022kglm} and scientific domains \citep{nadkarni2021scientific, safavi2022cascader}. They can be considered similar to our tests of BERT language models applied to a drug synergy hypergraph (§~\ref{results:bert}).


\subsection{Drug Synergy Prediction}
As discussed above, there are several approaches \citep{preuer2018deepsynergy, xu2019mr, nyamabo2021ssi, wang2022deepdds, kuru2021matchmaker, sun2020structure,rozemberczki2022chemicalx} which can predict synergy scores given cell line and drug features. There has also been interest in learning representations for these settings \citep{scherer2022distributed}. Recently, work \citep{yang2021graphsynergy, rozemberczki2022moomin, lin2022pisces} has begun to incorporate additional data sources such as drug-protein interactions. This can help improve results, but it often requires creating a subset of the original synergy dataset which can bias results towards the proposed method. \citep{yang2023bliam} extracts additional training data from the literature to improve synergy prediction results, which may relate to our results in Appendix \ref{appendix:scaling}. Research also investigates the application of few-shot \citep{ma2021few} and zero-shot \citep{huang2023zero} machine learning to drug response prediction--we extend this idea to drug synergy prediction. \citep{yang2020stratification} and \citep{kuenzi2020predicting} are related but have different focuses compared to our paper; neither compare against any other synergy baselines or do large-scale evaluation. \citep{yang2020stratification} focuses on a mechanistic understanding of (drug, tumor) activity–a different task. They use this understanding to rank subsystems and predict a limited number of drug combinations to evaluate. \citep{kuenzi2020predicting} does database and experimental testing with small numbers of cell tissues and drugs.


