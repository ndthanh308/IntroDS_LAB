
\section{Results} \label{sec:results}



\paragraph{BERT can do Drug Synergy?} 
\label{results:bert}
In this section, we experiment with finetuning BERT on drug synergy data where all drugs and cell lines are seen during training (data splits detailed in Appendix~\ref{appendix:graphsynergy_results}).
As discussed earlier, %
there has been recent work using external network datasets capturing interactions between drugs, proteins and cell lines \citep{yang2021graphsynergy} for synergy prediction. %

\begin{wraptable}{r}{0.7\textwidth}
\centering
\vspace{-.5cm}
\begin{subtable}{0.7\textwidth}
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|cc|cc}%

 Model & \thead{KB} & \thead{Name} & ROC-AUC & PR-AUC \\%& Precision & Recall & F$_1$ \\
\thickhline
 DeepSynergy & $\times$ &  & 84.3 & 70.4 \\%& 72.0 & 60.0 & 65.4 \\
 \hline
 MR-GNN & $\times$ &  & 77.9 & 62.6 \\%& 67.7 & 41.8 & 51.7 \\
 \hline
 SSI-DDI & $\times$ &  & 63.3 & 41.4 \\%& 59.0 & 6.1 & 11.0 \\
 \hline
 DeepDDS & $\times$ & & 87.2 & 77.0 \\%& 74.9 & 63.1 & 68.5 \\
 \hline
 SciBERT (random) & & & 86.9 & 76.3 \\%& 67.5 & 66.4 & 66.9 \\
 \hline
 BioLinkBERT (names) & & $\times$ & 86.4 & 75.9 \\

\end{tabular}
}
\end{subtable}
\vspace{-.1cm}
\caption{
Classification results for four selected ChemicalX \citep{rozemberczki2022chemicalx} baselines and BERT on DrugCombDB \citep{liu2020drugcombdb}. SciBERT and BioLinkBERT take random token and names as input, respectively. Values are average of five runs. Notably, SciBERT (random) outperforms four of the other five baselines. KB means external knowledge is used.}
\vspace{-.4cm}
\label{tab:transductive_results}
\end{wraptable} 
To evaluate the impact of these external datasets, we compare against a strong and recent model, Graphsynergy \citep{yang2021graphsynergy} that uses over a dozen different network datasets and achieves state-of-the-art on its subset of DrugCombDB.  %

We train four BERT-based \citep{devlin2019bert} language models \citep{beltagy2019scibert, yasunaga2022linkbert} and find that they outperform GraphSynergy in both name and random token settings. %
BioLinkBERT with random tokens, for example, achieves a ROC-AUC %
score of 84.1\% compared to GraphSynergy's 83.4\% (p < 0.05 using paired $t$-test). In comparison, BioLinkBERT with drug names as input %
achieves 83.6\%. %
We checked multiple BERT configurations, and details on other BERT models are shown in Appendix \ref{appendix:graphsynergy_results} Table \ref{tab:full_graphsynergy_transductive_results}. %


A natural question here is whether the model has learnt the required knowledge during pre-training. Surprisingly, replacing drug and cell names with random tokens (ยง~\ref{method:encoder}) resulted in no drop in performance. This suggests that the transformer architecture may be the dominant factor explaining BERT's performance on the task. However, if we use a randomly-initialized BERT model without any pre-training, we find the performance is worse (by ~3 ROC-AUC pts). %

To verify our findings, we consider the ChemicalX framework \citep{rozemberczki2022chemicalx}, which implements several baselines and provides a standardized subset of DrugCombDB \citep{liu2020drugcombdb} with drug and cell line features. This standardization allows us to compare different baseline methodologies on the same dataset.%
The ChemicalX DrugCombDB dataset has 2,956 drugs, 112 cell lines, and 191,391 synergy tuples. We compare against baselines DeepSynergy \citep{preuer2018deepsynergy}, MR-GNN \citep{xu2019mr}, SSI-DDI \citep{nyamabo2021ssi}, and DeepDDS \citep{wang2022deepdds}, which we train using default hyperparameters from the original papers for 50 epochs as in \citep{rozemberczki2022chemicalx}. 
These baselines (details in Appendix \ref{appendix:few_shot_full_results}) represent the most popular approaches to drug synergy prediction and allow us to compare against transformer architecture performance. Remarkably, SciBERT with \emph{random} tokens outperforms all baselines except DeepDDS in this setting (Table~\ref{tab:transductive_results}). We see similar results on the DrugComb dataset (this database is larger but is continuously modified by volunteers; see Appendix \ref{appendix:drugcomb}). We note that, while this performance is surprising in this domain, it follows from results from other domains. For example, language models are able to learn complex grammar and interactions just by observing how words co-occur. We conjecture this may be related to the observation that pre-training on a nonsense corpus \citep{krishna2021does} can provide good weight initializiations for downstream tasks. We further discuss related work in Section \ref{appendix:related}.



\vspace{-.2cm}
\subsection{In-Context Learning for Few-Shot Drug Synergy}
\label{sec:icl_results}

We now evaluate models on the few-shot and zero-shot setting, i.e, when a new drug or cell line is introduced with limited or no interaction data.
We use the same architecture used in \citet{garg2022can}: a GPT-2 \citep{radford2019language} model with 256-dimensional embeddings, 12 layers, 4 attention heads, and batch size of 64. We use a learning rate of 2e-5. Model weights are initialized from scratch. To enable efficient experimentation in the few-shot setting, we construct a dataset split which contains multiple unknowns (i.e. $m$ held-out drugs or cells: $H := \{h_i \mid i \in [1..m]\}$). To construct our split, we remove all synergy tuples containing $h \in H$ from the dataset $\mathfrak{D}$ so that the remaining dataset only contains tuples with known drugs/cells (this is our training set $\mathfrak{D}^{Tr}$).
Then, for each $h$, we select $n$ synergy tuples randomly to form the ``context'' bank/split $\mathfrak{D^c}$. All other ``unknown'' synergy tuples are put into $\mathfrak{D}^{Te}$. %

For comparison, we use the same baselines trained in zero-shot and few-shot settings. We also test SetFit \citep{tunstall2022efficient} (a few-shot LM approach), k-nearest neighbors, off-the-shelf pre-trained GPT-2 (using entity names as input, similar to CancerGPT \citep{li2023cancergpt}), and MAML with DeepDDS (details in Appendix \ref{appendix:few_shot_full_results}). In the few-shot setting, the context bank $\mathfrak{D}^c$ is considered part of the training set, and in the zero-shot setting it is not used. Our model, SynerGPT, however, is not trained on the context bank but uses it as context (prompt) examples for evaluation. Examples are selected using the Random, Graph, or Unknown-First strategies. We separately investigate the setting where drugs are unknown and where cell lines are unknown.

\textbf{Unknown Drugs} %
To construct the dataset split, we set $m = 50$ unknown, i.e.,``held-out'' drugs and context $n = 20$ synergy tuples. Hence, our context bank contains $50 \times 20 = 1,000$ tuples. 
Overall, we find that our SynerGPT can perform better in the few-shot setting than existing baselines on on this task, as shown in Table \ref{tab:inductive_results}. Full results are in Appendix Table \ref{tab:full_inductive_results}. %
SynerGPT is trained in the zero-shot setting, which means it can be evaluated both with context examples (few-shot) and without any examples (zero-shot).  Each strategy performs roughly the same zero-shot (although since strategies are used in training there are small differences)%
, but the performance with sampled context examples is much different. Without examples, SynerGPT performs worse than DeepDDS few-shot, but the same SynerGPT model outperforms DeepDDS when given the few-shot context. Overall, we outperform all prior models in the few-shot setting and zero-shot setting. %
 In particular, Unknown-First is able to increase performance by 3.8\% absolute ROC-AUC with context, whereas DeepDDS only increases 1.3\% from zero- to few-shot. Our approach is able to leverage the few given examples more effectively as shown by this higher increase in ROC-AUC. %
It is also notable that Unknown-First outperforms Graph since the context contains more examples with the unknown drug which the model is able to utilize to produce better predictions. %

For example, the tuple (Vismodegib, Mithramycin A, NCI-H226) with unknown Vismodegib is True. Without examples, this is predicted as 0.46. For Graph with examples, it is predicted as 0.65--closer to the ground truth. For Unknown-First, the prediction further increases to 0.79. In this example, Graph only sees 15 examples containing the unknown but Unknown-First sees a full 20. Few-shot DeepDDS predicts 0.47 for this example, which is quite similar to our method without examples.
As another example, (Chlorambucil, Cylocide, SK-OV-3) consists of two unknown drugs and has label False. Without examples, it is predicted as 0.62. Graph improves this to 0.35 and Unknown-First improves to 0.23. Interestingly, few-shot DeepDDS exhibits high uncertainty and predicts 0.50. 





\begin{table}%
\centering
\begin{subtable}{.8\textwidth}
\resizebox{\columnwidth}{!}{

\begin{tabular}{ c|l|cc|cc }

 \multicolumn{2}{c}{} & \multicolumn{2}{c}{\underline{Unknown Drug}} & \multicolumn{2}{c}{\underline{Unknown Cell Line}} \\ 
Mode & \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{ROC-AUC} & PR-AUC & ROC-AUC & \multicolumn{1}{c}{PR-AUC} \\
\thickhline
 \multirow{6}{*}{Zero-Shot} & DeepSynergy & 67.5 & 47.7 & 78.6 & 63.6 \\ 
 & DeepDDS & 72.1 & 53.2 & 74.5 & 59.8 \\
 & SciBERT (random) & 67.7 & 47.4 & 79.1 & 64.4 \\
 & MAML-DeepDDS & 68.76 & 50.05 & 71.6 & 54.6 \\
 & kNN-Features & 65.4 & 45.9 & 82.0 & 70.3 \\
  & SynerGPT* (ours) & \textbf{74.0} & \textbf{57.3} & \textbf{83.5} & \textbf{72.1} \\
 \thickhline
 \multirow{8}{*}{Few-Shot} & DeepSynergy & 71.6 & 53.9 & 82.0 & 68.7 \\
 & DeepDDS & 75.5 & 57.4 & 74.2 & 60.4 \\
 & SciBERT (random) & 73.8 & 56.9 & 80.5 & 66.4 \\
 & MAML-DeepDDS & 68.79 & 50.00 & 71.4 & 54.6 \\
 & kNN-Features & 66.9 & 47.7 & 82.1 & 70.5 \\
 & SetFit-S2 & 58.8 & 39.4 & 63.3 & 44.6 \\
 & GPT-2 & 74.2 & 56.8 & 80.3 & 66.6 \\
  & SynerGPT* (ours) & \textbf{77.7} & \textbf{61.5} & \textbf{83.8} & \textbf{72.8}  \\

\end{tabular}
}
\end{subtable}
\caption{Few-shot and zero-shot results on ChemicalX DrugCombDB with 50 unknown drugs / 20 unknown cell lines. Our in-context methods perform better than baselines trained in the few-shot setting. Results are averaged over 5 runs. Zero-shot SynerGPT is evaluated without context. BERT models use random tokens. The difference between SynerGPT with and without context has p < 0.05 for both unknown drugs and cell lines based on a paired \textit{t}-test. %
Similarly, both are statistically significant from the best baseline. *For simplicity, we report the best selection strategy (Unknown-First for unknown drug and Interpolate for unknown cell line). Full results are in Appendix \ref{appendix:few_shot_full_results}.} 

\label{tab:inductive_results}
\end{table} 
\raggedbottom

\textbf{Unknown Cell Lines} Since there are only 112 cell lines, we set $m = 20$ as unknown and use $n=10$ context examples. Interestingly, we find that models perform worse with context examples. We believe this is caused by the relatively small number of patient cell lines in the data vs. 2,956 drugs, making it harder to learn higher-level types of drug-cell line interaction. In other words, we are trying to learn a complex function class (drug synergy in an unknown cell line) without a significant number of example functions $f\in \mathcal{F}$. To alleviate this issue, we use 6 layers, batch size of 128, and only 30 epochs. Nonetheless, the issue still exists--performance decreases for baselines DeepDDS and MR-GNN and our strategies Unknown-First and Graph. %
We experiment with interpolating between training initially with Random to Unknown-First at the end (see Appendix \ref{appendix:interpolate}), which helps in the unknown cell line case. We believe this creates an exploration-exploitation effect. %


\subsection{Context Optimization}
\label{sec:context_optim}

\begin{wraptable}[9]{r}{0.5\textwidth}
\centering
\vspace{-.5cm}
\begin{subtable}{0.5\textwidth}
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c }

 \multicolumn{1}{c}{} & \multicolumn{2}{c}{\underline{Unknown Drug}} & \multicolumn{2}{c}{\underline{Unknown Cell Line}} \\ 
 \multicolumn{1}{c}{Strategy} & \multicolumn{1}{c}{ROC-AUC} & PR-AUC & ROC-AUC & \multicolumn{1}{c}{PR-AUC} \\
\thickhline
 Mean UF & 79.2 & 63.8 & 85.2 & 74.9 \\
 \hline
 Best UF & 80.8 & 66.4 & 85.6 & 75.7 \\
 \hline
 GA & \textbf{81.5} & \textbf{66.9} & \textbf{86.1} & \textbf{76.5} \\

\end{tabular}}
\end{subtable}
\caption{Test-set context optimization results ($p < 0.0001$).
Model parameters are fixed, only context is changed. UF indicates Unknown-First strategy.}
\label{tab:optimization}
\end{wraptable}

As we have shown in the previous section that the context selection strategy is very important for SynerGPT performance, the natural next question is to what extent the context can affect model performance. To test this, we conduct a different split. Like before, we select 50 unknown drugs and 20 cell lines; with their respective tuples, we create three uniform splits: context, validation, and test. We train a SynerGPT Unknown-First model using hyperparameters as in our above experiments. 

In context optimization, our goal is to select examples from the context and train splits which maximize some metric on the validation split. For our experiments, we maximize ROC-AUC for our trained model using the validation set. Overall, we consider two strategies: Unknown-First, and a genetic algorithm (GA). %
For the genetic algorithm, we use the implementation and hyperparameters from PyGAD \citep{gad2021pygad} with a population of 8 for 50 epochs. Here, we consider each example in the context split to be a potential gene. For comparison, we also select the context at random according to the Unknown-First strategy. To ensure comparability, we evaluate Unknown-First the same number of times as the genetic algorithm and select the best context. %
Our results (Table \ref{tab:optimization}) show that the genetic algorithm optimizes the context from a starting average AUC of 79.2\% up to 81.5\% for unknown drugs and from 85.2\% to 86.1\% for unknown cells. Appendix \ref{appendix:GA_change} visualizes this and shows error bars. %
We further analyze the results by different tissue types (Appendix \ref{appendix:tissue_type}). For example, we find that for unknown drugs, synergy prediction in ovarian cancer is effective, but for both unknown drugs and cell lines predictive performance on bone cell lines is low. %







\subsection{Inverse drug design from  synergy examples for discovery and explainability}
\label{sec:inverse_design}
% Figure environment removed

Explainability is one of the most challenging problems in deep learning. With transformer language models, the contrast between remarkable performance gain and lack of explainability becomes even more striking. Here we propose a novel drug design task to better understand the model's ``thought process.'' As shown in Figure \ref{fig:unseen_only_example}, essentially, we look at SynerGPT's prediction as it gains more information via synergy tuples. While this is a useful step, we do recognize that retrieval doesn't fully address explainability and hope to inspire further work. We refer to Limitations (ยง~\ref{limitations}) for more discussion. 

In this novel task, we evaluate SynerGPT's ability to retrieve the structure of an unknown drug. We use the same splits as before but replace the classification head with a vector output trained using the loss in Equation \ref{clip_loss}. Using the same splits allows us to visualize the optimized context from the genetic algorithm.
Experimentally, we achieve the best performance with the weight value from equation \ref{training_eq} set to $w_i := i/k$. Two examples of the model retrieving drugs which match the context synergy pairs are shown in Figures \ref{fig:unseen_only_example} and \ref{fig:seen_example}. These show the retrieved drug after $i$ context examples have been observed by the model. Additionally, we show overall retrieval performance as the number of context examples shown to the model increases in Appendix \ref{appendix:inverse_design}. Figure \ref{fig:ranks_all}. For the weighted strategy, mean rank for seen drugs decreases from $\sim$1,500 to $\sim$400 as context increases. 
Qualitatively, we find that we can retrieve the relevant drug or a similar structure from synergy relationships in multiple cases. This is considerably more effective for drugs observed during training, but performance is also better than random for unknown drugs. This ability to visualize the model's understanding is helpful for explaining what the model predicts from observing a given context. 
Second, it enables retrieving drugs which have a desired set of synergies, which can help inform drug candidate discovery, including patient-specific scenarios. We note that we worked off a broad definition of drug design as discovering new candidate medications. While retrieval is currently a challenging version of this, future work can expand the search space with generative models. 

