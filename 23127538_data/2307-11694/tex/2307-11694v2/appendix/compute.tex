
\section{Computing and Implementation Details}

All experiments were done on an internal cluster of GPUs. Each experiment was conducted on a single NVIDIA RTX A6000 with 48 GB VRAM. Notably, multiple experiments can fit on the GPU at one time. Our BERT model experiments done within the ChemicalX framework take roughly 2.5 hours each. For SynerGPT, the Unknown-First and Graph variants took roughly 3 hours to train. Random was compute-bound by sampling, which caused it to take 9 hours to train. The inverse design variants took roughly 6-7 hours to train. We estimate that 80 days of GPU time were used for all experiments. %

BERT-base consists of 108,233,473 parameters and BERT-large is 333,476,865. Unknown drug SynerGPT contains 22,793,473 parameters. Unknown cell version is 18,044,673 parameters. 
BERT-large models (we experimented with BioLinkBERT-large) were unstable to train in many cases. BERT and SynerGPT training used a linear decay learning rate schedule. Unknown drug SynerGPT uses 10,000 steps of warm-up. BERT used 1000 steps of warm-up. Unknown cell SynerGPT used 5\% of training steps as warm-up. The training epoch for unknown drugs is 40 and unknown cell lines is 30. Since we used MegaMolBARTv2 embeddings, we used an output head size of 512 dimensions for retrieval.
SynerGPT is trained using masked context examples selected from the training dataâ€“ it does not see the unknown drug or cell line during training, which we categorize as zero-shot. Thus, when the SynerGPT model is evaluated on the test set without context examples, it is doing zero-shot prediction for the unknown drug or cell line. When in-context learning is done, it is few-shot. On ChemicalX DrugCombDB, we use a batch size of 512 with random tokens and 256 for names due to VRAM limits. For all random token BERT experiments, we use a high threshold of $k=5,000$ to ensure no common tokens are used.

For the GraphSynergy dataset experiments, BERT-base models use a learning rate of 2e-5. We use 5e-6 for large models, which we find improves training stability. We use a batch size of 32.

Dataset split size varies depending on the seed for evaluating unknown drugs and cell lines. We detail our procedure for building these splits in Section \ref{sec:icl_results} and \ref{sec:context_optim}. This is necessary because we conduct the split based on unknown drugs instead of as percentages. For Table \ref{tab:optimization} experiments, we follow the splitting procedure used in ChemicalX \cite{rozemberczki2022chemicalx}-- this yields on average (145766, 161647) training examples and (44625, 29544) test examples for unknown (drug, cell line), respectively. For the context optimization of unknown drugs and inverse design, the training set size is 145766 and the context, validation, and test set size are each 15208 examples on average.


