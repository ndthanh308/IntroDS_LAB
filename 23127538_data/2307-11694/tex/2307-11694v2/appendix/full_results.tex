
\section{Full Results Tables}
\FloatBarrier

\subsection{GraphSynergy Full Results}
\label{appendix:graphsynergy_results}


Full results for the BERT input method and GraphSynergy tests are in Table \ref{tab:full_graphsynergy_transductive_results}. We compare on the specific subset of DrugCombDB \cite{liu2020drugcombdb} which was selected to match Graphsynergy's network data (i.e. selecting the subset of DrugCombDB with drugs/cells that can be matched with external protein-protein interaction, drug-protein association, and cell-protein association networks) and a 7:1:2 train:validation:test split. This data subset also contains useful surface names (the common natural language name of the drug; e.g. dasatinib), which allows us to compare the effect that drug names have on language model synergy prediction performance. 


We consider three BERT training variations: the original BERT \cite{devlin2019bert}, SciBERT \cite{beltagy2019scibert}, and BioLinkBERT \cite{yasunaga2022linkbert}. SciBERT was trained on a corpus of scientific documents which would be considerably more focused on drugs than a general corpus. BioLinkBERT is a biomedical BERT model additionally trained using document relation prediction (e.g. citation links). We would like to reiterate the rather remarkable finding that pre-training on scientific literature does not necessarily help the model perform drug synergy prediction any better. Overall, these results indicate that models using external data may not be behaving how we think they are. 


\paragraph{ChemicalX Results} We report full results on the subset of DrugCombDB \cite{liu2020drugcombdb} used by ChemicalX \cite{rozemberczki2022chemicalx} in Table \ref{tab:full_transductive_results}. Previous work tested on different subsets of existing datasets (due to filtering for external features).

\begin{table}[h!]%
\centering
\begin{tabular}{ c|c|c|c|c|c|c }

 Input & Model & ROC-AUC & F1 & Precision & Recall & Accuracy \\
\thickhline
  & GraphSynergy & 83.4 & 72.7 & 73.5 & 71.9 & 75.5 \\
 \cline{2-7}
\thickhline
 \multirow{5}{*}{Name} & Unpretrained BERT-base & 80.6 & 
71.0 & 71.7 & 70.3 & 74.0 \\
 \cline{2-7}
  & BioLinkBERT-base & 83.6 & 73.1 & 73.4 & 72.8 & 75.7 \\
 \cline{2-7}
  & SciBERT-base & 83.8 & 73.8 & 73.3 & 74.3 & 75.8 \\
 \cline{2-7}
  & BERT-base & 83.8 & 73.3 & 74.2 & 72.4 & 76.1 \\
  \cline{2-7}
  & BioLinkBERT-large & 84.7 & 73.9 & 74.7 & 73.1 & 76.7 \\
 \cline{2-7}
 \thickhline
 \multirow{4}{*}{\thead{Random\\Token}} & BioLinkBERT-base & 84.1 & 73.7 & 73.6 & 73.8 & 76.2 \\
  \cline{2-7}
 & SciBERT-base & 83.8 & 73.3 & 74.2 & 72.4 & 76.2 \\
 \cline{2-7}
 & BERT-base & 84.0 & 73.4 & 74.1 & 72.7 & 76.1 \\ 
 \cline{2-7}
 & BioLinkBERT-large & 84.1 & 73.8 & 73.4 & 74.2 & 76.1 \\


\end{tabular}
\caption{Performance of BERT models with names and random tokens and GraphSynergy on the custom subset of DrugCombDB \cite{liu2020drugcombdb}. Results are average of 5 runs. Name indicates that the common name of the drug is used as input, while Random Token uses the strategy described in Section \ref{method:encoder}.}
\label{tab:full_graphsynergy_transductive_results}
\end{table}


\begin{table}[h!]%
\centering
\begin{tabular}{ c|c|c|c|c}%

 Model & \thead{KB Info} & \thead{Name Info} & ROC-AUC & PR-AUC \\%& Precision & Recall & F$_1$ \\
\thickhline
 DeepSynergy & $\times$ &  & 84.3 & 70.4 \\%& 72.0 & 60.0 & 65.4 \\
 \hline
 MR-GNN & $\times$ &  & 77.9 & 62.6 \\%& 67.7 & 41.8 & 51.7 \\
 \hline
 SSI-DDI & $\times$ &  & 63.3 & 41.4 \\%& 59.0 & 6.1 & 11.0 \\
 \hline
 DeepDDS & $\times$ & & 87.2 & 77.0 \\%& 74.9 & 63.1 & 68.5 \\
 \hline
 SciBERT (random) & & & 86.9 & 76.3 \\%& 67.5 & 66.4 & 66.9 \\
 \hline
 BioLinkBERT (random) & & & 86.8 & 76.4 \\
 \hline
 BioLinkBERT (name) & & $\times$ & 86.4 & 75.9 \\

\end{tabular}
\caption{Classification results for four selected ChemicalX \cite{rozemberczki2022chemicalx} baselines and two BERT-base models on DrugCombDB \cite{liu2020drugcombdb}. First two BERT models use random token inputs and last model uses drug names as input. Values are average of five runs.}
\label{tab:full_transductive_results}
\end{table} 



\FloatBarrier

\subsection{Few-Shot Full Results}
\label{appendix:few_shot_full_results}

\subsubsection{Baseline Descriptions} 
\label{appendix:baseline_descriptions}
DeepSynergy is a popular feedforward model which uses cell line features and drug fingerprints. MR-GNN is a graph convolutional network (GCN) \cite{kipf2016semi} fed into an LSTM \cite{hochreiter1997long} which takes the drug structure into account. SSI-DDI uses a graph attention network (GAT) \cite{velivckovic2017graph} with a final co-attention layer. DeepDDS uses both a GAT and GCN, which are fed into a fully connected feed forward network.

\paragraph{Real GPT-2}
We train a GPT-2 model\footnote{\: ``gpt2'' from HuggingFace.} in the few-shot setting (as opposed to SynerGPT's zero-shot) using random context and the same hyperparameters to mimic SynerGPT's training settings as much as possible. We use names of the drugs obtained from linking to PubChem \cite{kim2023pubchem} as input in the form ``Are drugs [DRUG1] and [DRUG2] synergistic in cell line [CELL]?''. 

\paragraph{SetFit} Furthermore, we test finetuning a few-shot language-model baseline, SetFit \cite{tunstall2022efficient}, on our few-shot data. We follow the original paper in using batch size 16, $R = 20$ text pairs generated for contrastive learning, and 1 epoch. Inputs to the model follow the same format as BERT in Section \ref{method:encoder}. We test using four models. 
\begin{enumerate}
    \item \textbf{SetFit-SBERT}: \textit{paraphrase-multilingual-mpnet-base-v2} from \cite{reimers2019sentence} with names as input. This model was trained to create semantic embeddings via Siamese networks.
    \item \textbf{SetFit-C}: \textit{recobo/chemical-bert-uncased-simcse} from Recobo.ai \footnote{\url{www.recobo.ai}} with names as input. This model was trained using SimCSE on chemistry text.
    \item \textbf{SetFit-S2}: \textit{allenai/specter2} from \cite{singh2022scirepeval} with names as input. This model was trained on multiple scientific classification and regression tasks, such as MeSH descriptors classification.
    \item \textbf{SetFit-SMILES}: \textit{DeepChem/ChemBERTa-77M-MTR} from \cite{ahmad2022chemberta} with SMILES strings as input. This model was pretrained by predicting 200 molecular properties for a molecule given its SMILES string.
\end{enumerate}

\paragraph{Model-Agnostic Meta-Learning} We also consider a meta-learning formulation of our problem setting. We use MAML \cite{finn2017model} to train a DeepDDS model. Since MAML\footnote{We use the implementation from \url{https://github.com/cnguyen10/few_shot_meta_learning}} does few-shot classification using episodes sampled from different learning tasks, we reframe our problem to match this. We consider predicting synergy for each drug to be a task. Then, we sample an episode for training from a random task for each mini-batch. We aggregate rare drugs without enough samples to form an episode into the same task until there are enough samples for an episode. Additionally, since we are dealing with binary classification here, we use $N=2$-way. We sample the ``validation'' portion of each episode from our training set like in SynerGPT. We use the same context bank (and context size) for ``adaptation'' during evaluation. The same learning rate ($1e-3$), batch size (512), and number of steps/epochs as DeepDDS is used. We report few-shot (first-order) and zero-shot (no adaptation) versions. Overall, we find that the MAML training procedure produces poor results, and adaptation produces insignificant performance increases. We attribute this to the episode-based sampling strategy neglecting important information in training. 

\paragraph{Protonets}
As another meta-learning baseline, we consider Protonets \cite{snell2017prototypical}. We use the same meta-learning framework as for MAML. Because we don't have drug task meta-data, we only consider the few-shot setting. %

\paragraph{k-Nearest Neighbors}
We also consider a k-Nearest Neighbors baseline using scikit-learn \cite{scikit-learn} similar to \cite{nadkarni2021scientific}. We construct embeddings for each synergy pair by concatenating (Drug1, Drug2, Cell) embeddings. In the training set, we also include (Drug2, Drug1, Cell). We consider two embedding sources. For the first, kNN-Features, we consider the drug and cell fingerprint features from ChemicalX. For the second, kNN-S2, we use name embeddings from the Specter2 model. We report both zero-shot and few-shot versions. In the few-shot setting, the context bank is added to the training data. We set $k$ equal to the context number (20 and 10 for drugs and cell lines, respectively). We find performance on cell lines to be surprisingly effective, although still less than SynerGPT. 


\subsubsection{Interpolate Details}
\label{appendix:interpolate}
In the Unknown cell line setting, we observe that Random has an interesting effect where it performs better after examples (although still worse than Unknown-First (no-ex)), so we consider a fourth strategy: interpolating between Random Unknown-First. 
Essentially, for each data mini-batch in epoch $e$ of $E$ total epochs, we select either the Random strategy with probability $\max(0.25, 1-\frac{e}{E})$ otherwise we use the Unknown-First Strategy. This is analogous to an exploration-exploitation approach where we are pretraining with Random and transitioning to Unknown-First. We use a threshold of 25\% to ensure the benefits of Random are kept until the end of training. 
We find that this interpolation strategy is effective (with p < 0.05, see Table \ref{tab:full_inductive_results}) in dealing with the unknown cell line case.

\begin{table}
\centering

\begin{tabular}{ c|c|c|c||c|c }

 \multicolumn{2}{c}{} & \multicolumn{2}{c}{\underline{Unknown Drug}} & \multicolumn{2}{c}{\underline{Unknown Cell Line}} \\ 
Mode & \multicolumn{1}{c}{Model} & \multicolumn{1}{c}{ROC-AUC} & PR-AUC & ROC-AUC & \multicolumn{1}{c}{PR-AUC} \\
\thickhline
 \multirow{9}{*}{Zero-Shot} & DeepSynergy & 67.5 & 47.7 & 78.6 & 63.6 \\ 
 \cline{2-6}
 & MR-GNN & 65.9 & 44.7 & 76.6 & 61.9 \\
 \cline{2-6}
 & SSI-DDI & 61.8 & 38.9 & 66.6 & 46.7 \\
 \cline{2-6}
 & DeepDDS & 72.1 & 53.2 & 74.5 & 59.8 \\
 \cline{2-6}
 & SciBERT & 67.7 & 47.4 & 79.1 & 64.4 \\
 \cline{2-6}
 & BioLinkBERT & 65.8 & 45.6 & 79.0 & 64.5 \\
 \cline{2-6}
 & MAML-DeepDDS & 68.76 & 50.05 & 71.6 & 54.6 \\
 \cline{2-6}
 & kNN-Features & 65.4 & 45.9 & 82.0 & 70.3 \\
 \cline{2-6}
 & kNN-S2 & 69.2 & 49.0 & 78.8 & 66.0 \\
 \thickhline
 \multirow{15}{*}{Few-Shot} & DeepSynergy & 71.6 & 53.9 & 82.0 & 68.7 \\
 \cline{2-6}
 & MR-GNN & 68.1 & 48.4 & 76.5 & 62.1 \\
 \cline{2-6}
 & SSI-DDI & 62.8 & 40.5 & 66.2 & 45.6 \\
 \cline{2-6}
 & DeepDDS & 75.5 & 57.4 & 74.2 & 60.4 \\
 \cline{2-6}
 & SciBERT & 73.8 & 56.9 & 80.5 & 66.4 \\
 \cline{2-6}
 & BioLinkBERT & 73.0 & 55.6 & 80.6 & 67.4 \\
 \cline{2-6}
 & GPT-2 & 74.2 & 56.8 & 80.3 & 66.6 \\
 \cline{2-6}
 & SetFit-SBERT & 61.4 & 40.7 & 63.6 & 44.0 \\
 \cline{2-6}
 & SetFit-C & 58.9 & 39.6 & 63.8 & 44.8 \\
 \cline{2-6}
 & SetFit-S2 & 58.8 & 39.4 & 63.3 & 44.6 \\
 \cline{2-6}
 & SetFit-SMILES & 63.6 & 43.6 & 64.6 & 44.5 \\
 \cline{2-6}
 & MAML-DeepDDS & 68.79 & 50.00 & 71.4 & 54.6 \\
 \cline{2-6}
 & Protonets-DeepDDS & 54.5 & 31.1 & 57.2 & 34.3 \\
 \cline{2-6}
 & kNN-Features & 66.9 & 47.7 & 82.1 & 70.5 \\
 \cline{2-6}
 & kNN-S2 & 70.0 & 49.9 & 79.0 & 66.2 \\
 \thickhline
 \multirow{9}{*}{SynerGPT} & Features & 73.4 & 55.5 & 70.7 & 52.7 \\
 \cline{2-6}
  & Random (no-ex) & 72.2 & 54.5 & 77.1 & 61.3  \\
 \cline{2-6}
  & Random & 73.7 & 56.8 & 82.3 & 70.2 \\
 \cline{2-6}
  & Graph (no-ex) & 73.2 & 56.1 & 83.3 & 71.7 \\
 \cline{2-6}
  & Graph & 75.5 & 59.6 & 83.2 & 71.5 \\
 \cline{2-6}
  &  Unknown-First (no-ex) & 74.0 & 57.3  & 82.9 & 71.1 \\
 \cline{2-6}
  &  Unknown-First & \textbf{77.7} & \textbf{61.5} & 81.7 & 69.9 \\
 \cline{2-6}
  & Interpolate (no-ex) &  &  & 83.5 & 72.1 \\
 \cline{2-6}
  & Interpolate & &  & \textbf{83.8} & \textbf{72.8}  \\

\end{tabular}

\caption{Few-shot and zero-shot results on ChemicalX DrugCombDB subset with 50 unknown drugs (left) and 20 unknown cell lines (right). Results are the average of 5 runs. no-ex indicates that our trained SynerGPT models were evaluated without any context examples. Features is a SynerGPT model where drug and cell line features are used instead of a randomly-initialized embedding layer. BERT models use random token inputs. Results are the average of 5 runs. The difference between Unknown-First with and without context has p < 0.05 for unknown drugs based on a paired \textit{t}-test. On unknown cell lines using the interpolate strategy, p < 0.05. Similarly, both are statistically significant from the best baseline. %
}
\label{tab:full_inductive_results}
\end{table}


\subsubsection{In-Context Implementation Details}

In the unknown drug setting, to allow for tuples with multiple unknown drugs, we use both a \textbf{[UNKNOWN]} and \textbf{[UNKNOWN2]} token (e.g. a tuple containing two unknown drugs would be $(\textbf{[UNKNOWN]}, \textbf{[UNKNOWN2]}, c)$). 

For the inverse design experiments, in some cases, context examples do not contain the unknown entity $h$ and therefore no \textbf{[UNKNOWN]} tokens, so we use $\overrightarrow{0}$ as a replacement for the ground truth representation when calculating our loss function. We use the same model, splits, and training hyperparameters as in the context optimization setting.
