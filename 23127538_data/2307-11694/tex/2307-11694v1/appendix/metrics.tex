
\section{Evaluation Metrics}

In this section, we detail the binary classification metrics that are used in this paper. Assume we have values for true positives $TP$, true negatives $TN$, false positives $FP$, and false negatives $FN$ where predictions are separated into positives and negatives based on some threshold $t$. 

\begin{itemize}
    \item Accuracy: (TP + TN) / (TP + TN + FP + FN)
    \item Precision: TP / (TP + FP)
    \item Recall: TP / (TP + FN)
    \item TPR: TP / (TP + FN)
    \item FPR: FP / (TN + FP)
    \item TNR: TN / (TN + FP)
    \item ROC-AUC \cite{bradley1997use}: The area under the curve created by plotting TPR against FPR as $t$ is varied.
    \item PR-AUC: Similar to ROC-AUC but the curve is TPR against Precision.
    \item F$_1$: 2TP / (2TP + FP + FN)

\end{itemize}

Given a list of rankings $R$,

$$ Mean Rank = \frac{1}{n}\sum_{i=1}^n R_i $$
