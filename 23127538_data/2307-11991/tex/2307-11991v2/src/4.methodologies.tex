

\section{Psy-LLM Framework}

The Psy-LLM framework aims to be an assistive mental health tool to support the workflow of professional counsellors, particularly to support those who might be suffering from depression or anxiety.

\subsection{Target Audience and Model Usage}

The contributing factor of Psy-LLM---an AI-powered conversational model is two-fold.
(1) Firstly, Psy-LLM is trained with a corpus of mental health supportive Q\&A from mental health professionals, which enables Psy-LLM to be used as an assistive tool in an online consultation context. 
When users want to seek support from an online chat, Psy-LLM can provide suggestive answers to human counsellors to ease the staff's workload.
Such an approach eases the entry barrier for newly trained mental health staff to provide useful and supportive comments for those in need.
(2) Furthermore, in the absence of human counsellors (e.g. during off-hours or high-demand periods), Psy-LLM can also be a web frontend for users to interact with the system in an online consultation manner.
Providing timely support to help-seeking individuals is especially important among suicidal individuals~\citep{hom2015evaluating}.
Therefore, an AI-powered online consultation might be the next best venue to respond to the absence of human counsellors.

\subsection{Large-scale Pre-trained LLMs}

Our project involves leveraging two large-scale pre-training models, namely \emph{WenZhong} and \emph{PanGu} to develop the question-answering language model. The utilisation of pre-training models offers several advantages, including:
(1) Enhanced Language Representations: Pre-training on extensive unlabeled data enables the model to acquire more comprehensive language representations, which in turn can positively impact downstream tasks.
(2) Improved Initialisation Parameters: Pre-training provides a superior initialisation point for the model, facilitating better generalisation performance on the target task and expediting convergence during training.
(3) Effective Regularisation: Pre-training acts as an effective regularisation technique, mitigating the risk of overfitting when working with limited or small datasets. This is especially valuable as a randomly initialised deep model is susceptible to overfitting on such datasets.
By harnessing the advantages of pre-training models, we aim to enhance the performance and robustness of our question-answering language model for psychological counselling.






\subsection{PanGu Model}
\emph{PanGu} model is the first Chinese large-scale pre-training autoregressive language model with up to 200 billion~\citep{Zeng2021PanGuLA}. In an autoregressive model, the process of generating sentences can be likened to a Markov chain, where the prediction of a token is dependent on the preceding tokens. The \emph{PanGu} model, developed within the MindSpore framework, was trained using 2048 Ascend AI processors provided by Huawei and trained on a high-quality corpus of 1.1TB. It was officially released in April 2021 and has achieved the top rank in the Chinese Language Comprehension Benchmark (CLUE), a widely recognised benchmark for Chinese language comprehension~\citep{xu2020clue}.
% Figure environment removed

The architecture of the \emph{PanGu} model follows a similar structure to that of GPT-3, employing standard transformer layers~\cref{fig:PanGu}. Each transformer layer comprises two sub-layers: multi-head attention (MHA) and a fully connected feed-forward network (FFN). The MHA involves three primary steps: calculating the similarity between the Query and Key, applying a softmax function to obtain attention scores, and multiplying the attention scores with the Value to obtain the attention output. The attention output then passes through a linear layer and undergoes softmax to generate the output embedding. The output embedding is combined with the FFN input through a residual module. The FFN consists of two linear layers with a GeLU activation function between each consecutive layer. The MHA and FFN utilise the pre-layer normalisation scheme, facilitating faster and easier training of the Transformer model.

However, the last layer of the \emph{PanGu} model deviates from the standard transformer layer structure. Instead, it incorporates a query layer designed to predict the next token, thereby enhancing the model's positional awareness and improving generation effectiveness. The query layer is a narrow yet powerful decoder that relies solely on position information. The structure of the query layer is illustrated in~\cref{fig:QueryArc}. The primary distinction between the query layer and the transformer layer lies in the query input of self-attention. While the inputs of Query, Key, and Value in other self-attention layers of the transformer remain standard, the query layer introduces a query embedding, which functions similarly to position embedding, as the query input for self-attention in the last layer.


% Figure environment removed

The \emph{PanGu} model is available in four distinct variations, each characterised by different parameter sizes (\cref{table:pangu-setting}). These variations include \emph{PanGu} 350M, \emph{PanGu} 2.6B, \emph{PanGu} 13B, and \emph{PanGu} 200B (which is not open source). The parameter sizes differ across these models, reflecting their varying levels of complexity and capacity for language understanding and generation.


\begin{table}[b]
    \centering
    \caption{The parametric size of the various settings in the \emph{PanGu} model. \label{table:pangu-setting}}
    \begin{tabular}{@{}cccccc@{}}
    \toprule
    Model & Parameters & Layers & Hidden Size & Head & Seq Length \\ \midrule
    \emph{PanGu} 350M & 350M  & 24 & 1024 & 16 & 1024\\
    \emph{PanGu} 2.6B  & 2.6B & 32 & 2560 & 40 & 1024   \\
    \emph{PanGu} 13B & 13.1B & 40 & 5120 & 40 & 1024 \\
    \emph{PanGu} 13B & 207.0B & 64 & 16384 & 128 & 1024 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{WenZhong Model}
In addition to the \emph{PanGu} model, we have also incorporated the \emph{WenZhong} model as one of the models used. The \emph{WenZhong} model is a pre-trained model based on the GPT-2 architecture and trained on a large-scale Chinese corpus. Over the past few years, pre-trained models have become the foundation of cognitive intelligence, enabling advancements in natural language processing and computer vision algorithms.

The scale of pre-trained models has been rapidly increasing, growing by a factor of 10 each year, starting from the initial BERT model with 100 million parameters to the more recent GPT models with over 100 billion parameters. Given the nature of our task, which requires a generation model with expertise in different professional domains, we have opted for the \emph{WenZhong} model.

Due to the model size of LLMs like GPT, computing resources are the limiting factor hindering further progress in the field. Universities and research institutions often need more computing power to train and utilise large-scale pre-trained models. This limitation impedes the broader implementation of AI technologies. Hence, we have adopted the \emph{WenZhong} model, which is built upon a large pre-trained model trained on a Chinese corpus to avoid training the model from scratch.

The \emph{WenZhong} model series consists of one-way language models dominated by a Decoder structure and a series of powerful generation models. The \emph{WenZhong}-3.5B model, with 3.5 billion parameters, employs 100G data and 256 A100 GPUs for 28 hours of training, exhibiting strong generation capabilities. Thus, the \emph{WenZhong} model is highly powerful, featuring 30 decoder layers and billions of parameters. We have also utilised the \emph{WenZhong}-GPT2-110M version in this project, comprising 110 million parameters and 12 layers. It is important to note that the \emph{WenZhong} model has been pre-trained on the Wudao Corpus (300G version).




\subsection{Collecting Large Scale Dataset}
Two types of data sources were obtained for this project. The first dataset, PsyQA~\citep{psyqa}, consisting of question and answer pairs, focuses on Chinese psychological health support. The authors authorised us to use this dataset containing 22,000 questions and 56,000 well-structured, lengthy answers. The PsyQA dataset includes numerous high-quality questions and answers related to psychological support, and it had already undergone basic cleaning before we received it. We selected a test set of 5,000 samples from this PsyQA dataset for our experiments.

\subsubsection{Data Crawling}
The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, whereas other users can also help to respond to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine-tuning training due to the model's input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format.
Consequently, we only obtained around 5,000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., "\textbackslash n\textbackslash n".

\Cref{table:crawled-dataset} shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2GB. The datasets from Zhihu and Yixinli were 500MB and 200MB, respectively. Overall, we spent approximately 70 hours on data collection. Although the data collected from the Internet was abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.

\begin{table}[tb]
    \centering
    \caption{dataset crawled from different platforms. \label{table:crawled-dataset}}
    \begin{tabular}{@{}ccc@{}}
    \toprule
    Platform & Data Size & Crawling Time  \\ \midrule
    Tianya & 2GB & 40h+  \\
    Zhihu & 500Mb & 20h+  \\
    Yixinli & 200Mb & 8h+ \\
    \bottomrule
    \end{tabular}
\end{table}

To address the time-consuming nature of web crawling, we implemented a distributed crawl technology that utilised idle computers connected to the Internet or other networks, effectively harnessing additional processing power~\citep{Thelwall2001AWC}. Our approach involved obtaining sub-websites from the main website and saving them using custom crawling code. This code primarily relied on Python libraries such as ``requests'', ``BeautifulSoup'', and ``webdriver''. In addition, we employed dynamic web crawlers capable of collecting clickable elements, simulating user actions, comparing web page states, manipulating the DOM tree, and handling various user-invoked events~\citep{Li2018AutomaticallyCD}. Unlike static page structures that cannot handle dynamic local refresh and asynchronous loading~\citep{Yao2012AnAF}, dynamic crawlers could extract data from behind search interfaces.

The process of the dynamic crawler involved leveraging web developer tools within the browser to obtain XHR (XMLHttpRequest) information, which included requests containing headers, previews, and responses. We acquired relevant files by systematically searching through these layers of data and capturing network packets. After obtaining the sub-websites using static and dynamic crawling methods, we distributed them across multiple idle computers. Each computer was assigned specific sub-websites, and we collected project-related data using a combination of static and dynamic crawling techniques. Ultimately, we utilised eight computers for the crawling process, which took approximately 70 hours.


\subsubsection{Data Cleaning}
In line with the \emph{PanGu} paper~\citep{Zeng2021PanGuLA}, we adopted the original data cleaning method utilised in the \emph{PanGu} model. Additionally, we incorporated some additional cleaning steps. The following are the cleaning steps we employed:
\begin{enumerate}
    \item \emph{Removal of duplicate samples}: We eliminated any duplicate samples in the dataset to ensure data uniqueness.
    \item \emph{Removal of samples containing advertised keywords}: We excluded samples that contained specific keywords associated with advertisements or promotional content.
    \item \emph{Deletion of data with less than 150 characters}: Samples with less than 150 characters were removed from the dataset, as they were deemed insufficient for effective model training.
    \item \emph{Removal of URLs}: Any URLs present in the samples were eliminated to maintain the focus on text content.
    \item \emph{Removal of user names and post time}: User names, such as "\texttt{@jack}", and post timestamps, were removed from the samples, as they were considered irrelevant to the text content.
    \item \emph{Removal of repeated punctuation}: Instances of repeated punctuation marks, such as "!!!" or ".....", were removed from the samples to ensure cleaner and more concise text.
    \item \emph{Conversion of traditional Chinese to simplified Chinese}: All traditional Chinese characters were converted to simplified Chinese characters to standardise the text.
\end{enumerate}

Following the data cleaning process, the dataset could be directly inputted into the \emph{PanGu} model. However, for training with the \emph{WenZhong} model, the samples needed further processing. Specifically, all punctuations were removed, and the samples were tokenised to ensure a consistent length of 1000 tokens for compatibility with the \emph{WenZhong} model.


\subsubsection{Data Analysis}
Data analysis plays a crucial role in understanding the fundamental characteristics of textual data. In the context of the Chinese language, the exploratory data analysis (EDA) methods may be less diverse than those used for English. In this study, we primarily employed two common methods: word frequency analysis and sentence length analysis, to gain insights into the dataset.

To analyse the distribution of characters in each sample, we referred to the character number data presented in~\cref{table:dataset-distribution}. By visualising this information using a box chart, we examined the range of character counts across the samples.
Some samples are empty after the data cleaning, which we then prune from our dataset.
\Cref{fig:data-distribution} displays the distribution of sample lengths, indicating that the majority of samples fall within the range of 10,000 characters.

Overall, these preliminary analyses allowed us to gain initial insights into the dataset and provided a foundation for further exploration and understanding of the textual data.

\begin{table}[tb]
    \centering
    \caption{Data distribution of the length of each sample. \label{table:dataset-distribution}}
    \begin{tabular}{@{}cccccccc@{}}
    \toprule
    Count & Mean & Std & Min & 25\% & 50\% & 70\% & Max \cr \midrule
    371,434 & 5,343 & 11,335 & 0 & 653 & 1,835 & 6,039 & 454,611 \cr
    \bottomrule
    \end{tabular}
\end{table}


% Figure environment removed

To examine the word frequency in our dataset, we conducted an analysis after removing the stop words. The word cloud visualisation in~\cref{fig:frequent-word} illustrates the most frequent words in the dataset. Notably, the prominent words observed include ``Anxiety'', ``Appearance'', ``Marriage'', ``Relationship'', ``Family'', and ``Stressful''. These words are highly relevant to the topic of mental health, indicating that our dataset is robust and aligns well with the focus of our task.

The presence of these mental health-related terms further underscores the suitability of our data for addressing the objectives of our study. It suggests that our dataset encompasses significant content related to psychological aspects, allowing us to effectively explore and address relevant topics in the context of our research.

% Figure environment removed




\subsection{Model Training}
\textbf{Model Size} We plan to use \emph{PanGu} 350M to generate language considering the computational power, which contains 350 million parameters, 24 layers, 1024 hidden sizes and 16 attention heads. Besides, we also want to train the \emph{WenZhong}-110M model that contains 12 layers and has 110M parameter.

\textbf{Training Data} We employ the 2.85GB psychology corpus data crawled from psychology platforms like Yixinli and Tianya, to train the original \emph{PanGu} 350M model. After that,  we use 5,6000 question-answer pairs from PsyQA dataset to fine-tune the model.

\textbf{Training Platform} We train the \emph{PanGu} model on the OpenI platform with a free V100 graphics card GPU because OpenI is the open source platform of \emph{PanGu} model, convenient for us to deploy the required files, image and GPU. The batch size is set to 8, and the training iteration is set to 100,000 because we found that 50,000 iterations is not enough for the model's loss to converge.
We train the \emph{WenZhong} model in jupyter notebook. To fine-tune this model, we need to tokenise the data, which can transform words into tokens. Besides, we also need to isolate the max length of each sentence as 500.





\subsection{Dataset Evaluation}

Determining the cleaning rules and data filtering thresholds are important aspects of the data cleaning process. We employed a data quality evaluation method that combined both manual and model-based evaluations to evaluate the dataset obtained from website crawling.

For the model-based evaluation, we utilised the \emph{PanGu} 350M model and calculated the perplexity metric after each data cleaning stage. A lower perplexity value indicates a more effective cleaning process and higher dataset quality.
In addition to the model-based evaluation, we sought input from experts in psychology. We invited two members from our University's School of Psychology, Faculty of Science, to perform a random sample check on the dataset after it had undergone the cleaning process. While this method does not cover the entire corpus comprehensively, it provides valuable insights and plays a role in data cleaning and quality evaluation.

The evaluation process involved the following steps: First, we provided the experts with a sample of the cleaned dataset and asked them to assess its quality based on their expertise and domain knowledge. They evaluated the dataset for accuracy, relevance, and coherence, providing feedback and suggestions for further improvements.

Next, we conducted a comparative analysis between the model-based evaluation and the expert evaluation. We examined the perplexity scores obtained from the \emph{PanGu} 350M model and compared them with the feedback provided by the experts. This allowed us to identify any discrepancies or areas of improvement in the dataset.

Overall, the combination of model-based evaluation and expert assessment comprehensively evaluated the dataset quality. It allowed us to identify and address any issues or shortcomings in the data cleaning process, ensuring that the final dataset used for training and evaluation was high quality and suitable for our research purposes.

\subsection{Model Training Setting}

\emph{Models:} For our training, we utilise the \emph{PanGu} 350M model, considering the available computational resources. This model consists of 350 million parameters, 24 layers, a hidden size 1024, and 16 attention heads. Additionally, we target the \emph{WenZhong}-110M model, which contains 12 layers and 110 million parameters.

\emph{Training Data:} We collected a psychology corpus dataset totalling 2.85GB, which was crawled from psychology platforms such as Yixinli and Tianya. This dataset was used for training the original \emph{PanGu} 350M model. Subsequently, we fine-tuned the model using 56,000 question-answer pairs from the PsyQA dataset.

\emph{Training Platform:} The \emph{PanGu} model was trained on the OpenI platform, utilising a free 1 V100 graphics card GPU. OpenI is an open-source platform specifically designed for the \emph{PanGu} model, allowing us to easily deploy the necessary files, images, and GPU resources.
For training with the V100 graphics card (32 GB memory), the minimum recommended configuration is one card, while the recommended configuration is two. The graphics card requirements can be adjusted based on the memory size (for example, a 16GB memory card would require twice as many cards as the V100). Increasing the number of graphics cards can help improve the training speed if the dataset is large.
We set the batch size to 8 and performed training for 100,000 iterations, as we observed that 50,000 iterations were insufficient for the model's loss to converge.
For the \emph{WenZhong} model, we used Jupyter Notebook to run the pre-trained model and fine-tuned it on a system with 64GB memory and an RTX3060 graphics card. The version details of the hardware and software components are listed in~\cref{table:hardware-software-version}.



\begin{table}[tb]
    \centering
    \caption{Hardware and Software Versions}
    \label{table:hardware-software-version}
    \begin{tabular}{@{}cc@{}}
    \toprule
    Hardware and Software & Version \\
    \midrule
    Operating System & Windows 10 \\
    numpy & 1.18.5 \\
    pandas & 1.3.4 \\
    torch & 1.11.0 \\
    tokenizers & 0.13.1 \\
    tqdm & 4.64.1 \\
    jupyter & 1.0.0 \\
    transformers & 4.23.1 \\
    \bottomrule
    \end{tabular}
\end{table}









\subsubsection{Training Process}
According to the guide of training \emph{PanGu} model with GPU, the first step is environment configuration. We prepared Pytorch, \emph{PanGu} image, one V100 graphics card and some \emph{PanGu} model files like vocabulary. The second step is data preprocessing. We put the training corpus into a text file, and each sample is a paragraph separated by two newlines, then converted it into a binary file because it is the required input format of the training \emph{PanGu} model. The third step is model training. We uploaded the \emph{PanGu} model and the bin file to the OpenI platform and set some parameters like iteration to train it. Our training procedure of \emph{PanGu} model consists of two steps. Firstly, we train the original \emph{PanGu} 350M model with all the crawling data for 100,000 iterations. The model starts to converge at about 60,000. This model has learned psychology domain knowledge based on pre-trained data. Secondly, we fine-tune it with the PsyQA dataset to improve the model's capability to provide useful answers for users on mental health support.

We used the early stop method to choose appropriate iterations. Stopping the training of the network before the validation loss increased effectively prevented the model from overfitting. For example, when the model has more than 9,000 training iterations, the validation loss of the model starts to rise, which means the phenomenon of overfitting occurs.
A similar approach is also used for the \emph{WenZhong} model.

% Figure environment removed







\subsection{Model Evaluation}

In this section, we assess the performance and effectiveness of our proposed language model for online psychological consultation. We employ a combination of intrinsic and human evaluation metrics to evaluate the model's capabilities comprehensively. We begin by utilising perplexity, ROUGE-L, and Distinct-n metrics to measure the model's language generation quality, similarity to the reference text, and diversity. Additionally, we recognise the limitations of these metrics and emphasise the importance of human evaluation in providing subjective assessments of the model's outputs, considering factors such as coherence, relevance, and overall quality. Through this comprehensive evaluation approach, we aim to gain a comprehensive understanding of our model's strengths, weaknesses, and suitability for its intended purpose in the context of online psychological consultation.

\subsubsection{Metric-based Evaluation}

Perplexity is a widely used intrinsic evaluation metric that measures how well a language model predicts a given sample. Mathematically, perplexity is defined as the reciprocal of the average probability assigned to each token in the dataset by the language model~\citep{Chen1998EvaluationMF}. In simpler terms, a lower perplexity value indicates better language model performance. Since perplexity is based on the average log-likelihood of the dataset, it can be computed quickly and is statistically robust, as it is not easily affected by outliers.

The formula for calculating perplexity is given by
 \begin{align}
    PP(W)
    &= \mathbb{P}\left( w_1 w_2 \ldots w_N \right)^{{-1}/{N}} \\
    &=\sqrt[N]{ \prod_{i=1}^N \frac{1}{\mathbb{P}(w_i)} }
 \end{align}
where $PP$ is the perplexity, $\mathbb{P}$ is the probability of the $i^{th}$ word, and $N$ is the length of a sentence.
It is important to note that perplexity tends to decrease as the dataset size increases, indicating better performance.

However, it is crucial to understand that low perplexity does not necessarily equate to high accuracy. Perplexity is primarily used as a preliminary measure and should not be solely relied upon for evaluating model accuracy. Additionally, comparing the performance of models on different datasets with varying word distributions can be challenging~\citep{Chen1998EvaluationMF}. Therefore, while perplexity provides valuable insights into model performance, it should be complemented with other evaluation metrics and considerations when assessing model accuracy.



ROUGE-L (Longest Common Subsequence) is an evaluation metric that measures the number of overlapping units between the predicted text generated by a language model and the actual reference text~\citep{Lin2004ROUGEAP}. ROUGE-L measures how closely the generated text matches the desired output by quantifying the similarity between the predicted and reference texts.

Distinct-1 and Distinct-2 are evaluation metrics that assess the diversity of the generated text. Distinct-1 calculates the number of distinct unigrams (individual words) divided by the total number of generated words. In contrast, Distinct-2 calculates the number of distinct bigrams (pairs of adjacent words) divided by the total number of generated bigrams~\citep{Li2016ADO}. These metrics reflect the degree of diversity in the generated text by quantifying the presence of unique unigrams and bigrams.

The formulas for calculating Distinct-n are as follows:

\begin{equation}
    \text{Distinct-n} := Distinct(n)=\frac{Count(\texttt{unique}, \texttt{n-gram})}{Count(\texttt{word})}
\end{equation}

Here, $Count(\texttt{unique}, \texttt{n-gram})$ represents the number of $n$-grams that are not repeated in a reply, and $Count(\texttt{word})$ indicates the total number of $n$-gram words in the reply. A higher value of $Distinct(n)$ indicates a greater diversity in the distinct generations.

These evaluation metrics, including perplexity, ROUGE-L, Distinct-1, and Distinct-2, provide insights into the quality, similarity, and diversity of the generated text by the language model. They serve as valuable tools for assessing the performance and effectiveness of the model in generating accurate and diverse outputs.

While perplexity and Distinct-n provide insights into the language model's performance in language generation, they do not necessarily indicate high accuracy. Therefore, in order to evaluate models more convincingly, human evaluation is still necessary. Human evaluators can provide subjective assessments of the generated text, considering factors such as coherence, relevance, and overall quality, which are important aspects that cannot be fully captured by automated evaluation metrics alone.


\subsubsection{Human evaluation}


For human evaluation, we have developed an online marking system to assess the performance of our language model in the context of online psychological consultation. This evaluation system aims to streamline the process and ensure effective assessment by focusing on four key metrics: Helpfulness, Fluency, Relevance, and Logic. Each metric is scored on a scale of 1 to 5, allowing evaluators to provide a quantitative assessment of each aspect.
The four metrics are defined as follows:

\begin{enumerate}
    \item \textbf{Helpfulness:} This metric evaluates whether the generated response is helpful for patients seeking psychological support.
    \item \textbf{Fluency:} Fluency refers to the degree of coherence and naturalness exhibited in the generated response.
    \item \textbf{Relevance:} Relevance assesses the extent to which the response's content directly relates to the posed question.
    \item \textbf{Logic:} Logic examines the logical consistency and coherence of the meaning conveyed in the generated response.
\end{enumerate}


To conduct the human evaluation, we invited six students from the psychological faculty to assess a set of 200 question-answer pairs generated by our model. We employed two evaluation methods to understand the model's performance comprehensively.

In the first method, evaluators compared responses generated by the \emph{PanGu} model and the \emph{WenZhong} model in response to the same question. They assigned scores to these answers based on the predetermined metrics, allowing for a direct comparison between the two models.
The second method involved incorporating the actual answers alongside the predicted responses as a whole, allowing evaluators to assess the differences and similarities between the generated and actual responses.

By employing these human evaluation methods, we aim to gain valuable insights into the performance of our language model, particularly in terms of the disparities between predicted and actual responses. This comprehensive evaluation approach will provide a deeper understanding of the model's capabilities and guide further improvements in its performance for online psychological consultation.
