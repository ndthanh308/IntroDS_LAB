\section{Experimental Results}



In this section, we present the findings and outcomes of the evaluation and experimentation conducted to assess the performance and effectiveness of our proposed language model for online psychological consultation. This section provides a comprehensive analysis of the model's performance based on intrinsic and human evaluation metrics. We discuss the results obtained from metrics such as perplexity, ROUGE-L, and Distinct-n, which shed light on language generation quality, similarity to reference text, and diversity of the generated responses. Additionally, we present the outcomes of the human evaluation, which includes scores given by evaluators based on metrics such as Helpfulness, Fluency, Relevance, and Logic. Through these rigorous evaluations, we aim to provide an in-depth understanding of the strengths and weaknesses of our language model and its suitability for the task of online psychological consultation.


\subsection{Result of Intrinsic Evaluation}

The results of the intrinsic evaluation comparing the performance of the \emph{PanGu} model and the \emph{WenZhong} model are presented in \cref{table:intrinsic-metric}. The metrics used for evaluation include perplexity, ROUGE-L, Distinct-1, and Distinct-2.

As shown in~\cref{table:intrinsic-metric}, the \emph{PanGu} model outperforms the \emph{WenZhong} model across all metrics. The \emph{PanGu} model achieves a lower perplexity value of 34.56 compared to 38.40 for the \emph{WenZhong} model, indicating that the \emph{PanGu} model better predicts the sample probabilities in the dataset.

Furthermore, the ROUGE-L score, which measures the similarity between the generated responses and the reference text, is higher for the \emph{PanGu} model (28.18) than the \emph{WenZhong} model (23.56). This suggests that the \emph{PanGu} model generates responses more aligned with the expected answers.

In terms of diversity in generated responses, the \emph{PanGu} model also exhibits higher Distinct-1 (4.57) and Distinct-2 (12.74) scores compared to the \emph{WenZhong} model (3.55 and 9.67, respectively). This indicates that the \emph{PanGu} model produces a wider range of distinct unigrams and bigrams in its responses, leading to a more diverse language generation.

The intrinsic evaluation results consistently demonstrate that the \emph{PanGu} model performs better than the \emph{WenZhong} model across all metrics. These findings highlight the superior language generation capabilities of the \emph{PanGu} model in the context of online psychological consultation.


\begin{table}[tb]
    \centering
    \caption{The result of Intrinsic Evaluation between two models. \label{table:intrinsic-metric}}
    \begin{tabular}{@{}ccccc@{}}
    \toprule
    Model & Perplexity & Rouge-L & Distinct1 & Distinct2  \\ \midrule
    WenZhong & 38.40  & 23.56 & 3.55 & 9.67\\
    PanGu & 34.56 & 28.18 & 4.57 & 12.74  \\
    \bottomrule
    \end{tabular}
\end{table}


\subsection{Case Study: Examining Psy-LLM's responses on coping with depression}

In the following, we will qualitatively evaluate the outputs of both models by asking our Psy-LLM a common question for individuals suffering from depression.

\subsubsection{PanGu Model}
To demonstrate the capabilities of the \emph{PanGu} model, we provide an example of question answering using the model on coping with depression (\cref{fig:pangu-demo}), which we have translated into English. The question posed to the model is,
\begin{displayquote}
\emph{``Question: How to deal with depression?''},
\end{displayquote}
and the model's response is as follows:
\begin{displayquote}
\emph{``Answer: Hello, first let me give you a small hug. I hope it brings you some warmth. Typically, individuals with depression are often misunderstood and even scolded by their families, which can be really disheartening. I understand that you are feeling very sad and disappointed with your family situation. I want you to know that you are a remarkable person, and you are not at fault. Remember, change starts with you, and you are incredibly strong and sensible.''}
\end{displayquote}

This response showcases the model's fluency and ability to employ different strategies.
Analysing the response in~\cref{fig:pangu-demo}, we can observe that the model utilises various strategies. The initial part, offering a virtual hug, demonstrates a reassurance strategy~\cite{psyqa}. It provides emotional support and encouragement to individuals seeking consultation, helping them relax. The second part, addressing family dynamics, employs a restatement strategy by clearly reiterating the question to provide a more realistic understanding of the individual's mental health condition. The final part utilises a guidance strategy, advising the individual on steps to improve their situation. Overall, the model's response is coherent and consistent with normal human logic, making it highly valuable.

The training of the \emph{PanGu} model consists of two steps. Initially, the model is trained with a vast amount of crawled data, enabling it to learn extensive domain knowledge in psychology. Subsequently, the model undergoes fine-tuning using the downstream PsyQA dataset. During this stage, the model primarily focuses on learning the pattern of \emph{``Question:''} and \emph{``Answer:''} to generate responses in the same format as the downstream tasks. When we input a question in the format \emph{``Question: $\cdots$''} to the model, the model outputs a response in the format \emph{``Answer: $\cdots$''}. This demonstrates that the model has successfully learned the desired pattern and can generate appropriate responses.

% Figure environment removed

\subsubsection{WenZhong Model}


% Figure environment removed


In \emph{WenZhong} model, after fine-tuning the model using Psychology data, the model can output very suitable answer.
We also utilised the \emph{WenZhong} model to answer the same question (\cref{fig:WenZhong-demo}), and the generated response was:




\begin{displayquote}
\emph{``Our experience tells us that there are many times when you just have to do things by your own will, then there will be a lot of discomfort. For example, if you are studying, the time of day, the place of study, the attitude of study, the ability to study, etc., these must be treated carefully, especially these should be avoided as much as possible, for example, then if you do not want to do something negative, leave it alone and do as much as possible to do some preventive measures for depression, so that it will not lead to the development of this disease.''}
\end{displayquote}


This example demonstrates that the \emph{WenZhong} model can provide suitable answers in the field of psychology. However, one challenge we encountered is that some generated outputs do not directly address the question. This issue may be attributed to the limited fine-tuning data specific to psychology. In order to further improve the performance of the \emph{WenZhong} model in psychology-related tasks, a larger and more diverse dataset from the field of psychology could be incorporated during the fine-tuning process.




\subsection{Human Evaluation}

To conduct an empirical evaluation of Psy-LLM's effectiveness, we enlisted the participation of six students from the psychological faculty to assess a set of 200 question-answer pairs generated by our language model. In order to obtain a comprehensive understanding of the model's performance, we employed two evaluation methods for the participants to provide ratings on the responses.
We have created a web front-end for users to access our Psy-LLM platform, and their technical details are discussed in~\cref{sec:web}.

The first method directly compared responses generated by both the \emph{PanGu} and the \emph{WenZhong} models in response to the same question. Evaluators assigned scores to these answers based on predetermined metrics, enabling a clear and direct comparison between the two models' performance.
In the second method, we presented evaluators with a combined set of predicted and actual responses. This allowed them to evaluate and assess the differences and similarities between the generated responses and the ground truth answers.

By utilising these human evaluation methods, we aim to gain valuable insights into the performance of our language model, particularly in terms of the disparities between predicted and actual responses. This comprehensive evaluation approach will provide a deeper understanding of the model's capabilities and guide further improvements in its performance for online psychological consultation.


The human evaluation results, using two different methods, are presented in~\cref{table:human-eval-1,table:human-eval-2}. These evaluate human-perceived metrics of \emph{Helpfulness, Fluency, Relevance, and Logic}.
\Cref{table:human-eval-1} shows the results of the first human evaluation method, where evaluators provided scores for each metric. Consistent with the findings from the intrinsic evaluation, the \emph{PanGu} model outperforms the \emph{WenZhong} model in terms of \emph{Helpfulness} (3.87 vs. 3.56), \emph{Fluency} (4.36 vs. 4.14), \emph{Relevance} (4.09 vs. 3.87), and \emph{Logic} (3.83 vs. 3.63). These results indicate that human evaluators generally consider the \emph{PanGu} model's generated responses more helpful, fluent, relevant, and logical than the \emph{WenZhong} model.

However, a notable observation is made when comparing the scores obtained in~\cref{table:human-eval-1} with the scores from~\cref{table:human-eval-2}. \Cref{table:human-eval-2} presents the scores for the predicted answers of both models as well as the actual answers. Interestingly, the scores for the actual answers are significantly higher than those for the predicted answers of both models across all metrics. This discrepancy suggests that the evaluators, who had the opportunity to compare the actual answers with the predicted answers, marked the predicted answers relatively lower. This finding highlights the importance of incorporating human evaluation in assessing the performance of language models and the need for further improvement in generating more accurate and satisfactory responses.

In summary, the human evaluation results align with the intrinsic evaluation findings, indicating that the \emph{PanGu} model performs better than the \emph{WenZhong} model. However, it is important to note that the scores for the actual answers are considerably higher than those for the predicted answers, implying room for improvement in the generated responses of the language models.






\begin{table}[tb]
    \centering
    \caption{Average Human ratings of Psy-LLM responses, only with the two AI-powered versions. \label{table:human-eval-1}}
    \begin{tabular}{@{}ccc@{}}
    \toprule
    Metrics & WenZhong & PanGU  \\ \midrule
    Helpfulness & 3.56  & 3.87\\
    Fluency  & 4.14 & 4.36   \\
    Relevance & 3.87 & 4.09 \\
    Logic & 3.63 & 3.83 \\
    \bottomrule
    \end{tabular}
\end{table}

\begin{table}[tb]
    \centering
    \caption{Average Human ratings of Psy-LLM responses, alongside the ground-truths from datasets. \label{table:human-eval-2}}
    \begin{tabular}{@{}cccc@{}}
    \toprule
    Rating Metrics & WenZhong & PanGU & Ground Truth \\ \midrule
    Helpfulness & 3.45  & 3.54 & 4.52\\
    Fluency  & 3.95 & 4.12 & 4.83   \\
    Relevance & 3.77 & 3.96 & 4.72 \\
    Logic & 3.61 & 3.75 & 4.56 \\
    \bottomrule
    \end{tabular}
\end{table}




\section{Web-Interface for Accessible Online Consultation}\label{sec:web}

One of the primary objectives was to explore the provision of online AI-powered consultation and question-and-answer services in psychology. We adopted a distributed architecture, separating the model's front-end, back-end, and computing servers into modular components. Each module was developed with distinct responsibilities, allowing for easier upgrades and interchangeability of combinations. Communication between the modules was achieved through API interactions, enabling them to function independently without relying on the internal functionality of other modules.

Furthermore, we placed a strong emphasis on security during the design process. We implemented measures to encrypt and protect our modular systems at a product level. The common API interface was productised and encrypted, ensuring secure communication between the components. Additionally, we implemented the HTTPS web system architecture, enhancing security by encrypting each cloud server with TLS (SSL).
By adopting a distributed and modular approach and prioritising security, we aimed to address the challenges of hosting a large-scale online consultation service model. These design choices allowed for flexibility, scalability, and enhanced security in our system architecture, contributing to our project's overall success and reliability.


\subsection{Web Technologies}

We utilised the following services and technologies for our website development:

\begin{itemize}
    \item \emph{ReactJS}: ReactJS was our front-end framework due to its extensive library support. ReactJS offers a wide range of reusable components and follows a modular, component-based architecture, making designing and enhancing the front end easier. ReactJS is responsive and provides excellent cross-platform support.
    \item \emph{AWS Amplify}: AWS Amplify is a rapid front-end deployment service provided by Amazon. It enables us to quickly deploy the front end of our website and seamlessly communicate with other system components. Amplify provides fully managed CI/CD (Continuous Integration/Continuous Deployment) and hosting, ensuring fast, secure, and reliable encryption services.
    \item \emph{Google Domain}: We utilised Google Domain services for secure encapsulation of our EC2 host DNS.
    \item \emph{Amazon EC2}: EC2 provides virtual server instances with highly available underlying designs. It offers reliable, scalable, and flexible access in terms of cost and performance. EC2 provides powerful computing resources and pre-configured environments, making it an excellent choice for running large models. Its robust network performance and high-performance computing clusters allow for high throughput and low-latency online processing. We used simple Flask-based scripts to handle concurrent requests.
    \item \emph{Python \& Flask}: We used Python as our scripting language to run the models and build APIs. Flask, a web framework written in Python, was used for creating API endpoints and handling request-response interactions.
    \item \emph{Apache}: We used Apache, an open-source web server software, for configuring port forwarding, reverse proxies, and listening.
    \item \emph{Let's Encrypt \& Certbot}: We employed Let's Encrypt and Certbot for TLS (Transport Layer Security) encryption, ensuring secure communication between the website and users.
\end{itemize}




% Figure environment removed
The diagram in~\cref{fig:web-arc} illustrates the architecture of our independently developed web system for the cloud-based site. The website's user interface is accessible through the front end, deployed on the AWS Amplify service. Built on the ReactJS framework, the front-end communicates with the back-end database through an internal API, enabling storage of user evaluation data for model effectiveness optimisation. The database is hosted within Amplify Hosting. The standalone website interacts with the model runtime server via a public API. To ensure privacy and protect the host address, we register a public domain name through Google Domains and link it to the host server's DNS.

The pre-trained model is deployed on an Amazon EC2 instance host configured as an AWS Linux virtual server. The model uses Python code and Flask scripts, allowing for local server calls. Apache is used for HTTP reverse proxy communication, forwarding external model input data to the local server where the model is waiting and generating results.
To provide secure HTTPS encryption for the web products deployed on AWS Amplify, we employ TSL encryption for the EC2 instance DNS addresses. This is achieved using Let's Encrypt and Certbot as cost-effective alternatives to commercial SSL certificates.

The website's front end is designed with simplicity, featuring an input box for users to enter Chinese questions, as depicted in~\cref{fig:web-init}. Upon submission, the system communicates with the back-end model through the API. It awaits the completion of model processing (\cref{fig:web-load}) before returning the results to the output box, as depicted in (\cref{fig:web-done}). Users can rate the results using the built-in rating system, and there is a link to an additional evaluation site at the bottom of the page.

% Figure environment removed


% Figure environment removed

