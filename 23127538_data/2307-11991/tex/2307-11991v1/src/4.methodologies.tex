

\section{Psy-LLM Framework}

Our project involves leveraging two large-scale pre-training models, namely \emph{WenZhong} and \emph{PanGu}, to develop the question-answering language model. The utilisation of pre-training models offers several advantages, including:
(1) Enhanced Language Representations: Pre-training on extensive unlabeled data enables the model to acquire more comprehensive language representations, which in turn can positively impact downstream tasks.
(2) Improved Initialisation Parameters: Pre-training provides a superior initialisation point for the model, facilitating better generalisation performance on the target task and expediting convergence during training.
(3) Effective Regularisation: Pre-training acts as an effective regularisation technique, mitigating the risk of overfitting when working with limited or small datasets. This is especially valuable as a randomly initialised deep model is susceptible to overfitting on such datasets.
By harnessing the advantages of pre-training models, we aim to enhance the performance and robustness of our question-answering language model for psychological counselling.


\subsection{PanGu Model}
\emph{PanGu} model is the first Chinese large-scale pre-training autoregressive language model with up to 200 billion~\citep{Zeng2021PanGuLA}. In an autoregressive model, the process of generating sentences can be likened to a Markov chain, where the prediction of a token is dependent on the preceding tokens. The \emph{PanGu} model, developed within the MindSpore framework, was trained using 2048 Ascend AI processors provided by Huawei and trained on a high-quality corpus of 1.1TB. It was officially released in April 2021 and has achieved the top rank in the Chinese Language Comprehension Benchmark (CLUE), a widely recognised benchmark for Chinese language comprehension~\citep{xu2020clue}.
% Figure environment removed

The architecture of the \emph{PanGu} model follows a similar structure to that of GPT-3, employing standard transformer layers~\cref{fig:PanGu}. Each transformer layer consists of two sub-layers: multi-head attention (MHA) and fully connected feed-forward network (FFN). The MHA involves three primary steps: calculating the similarity between the Query and Key, applying a softmax function to obtain attention scores, and multiplying the attention scores with the Value to obtain the attention output. The attention output then passes through a linear layer and undergoes softmax to generate the output embedding. The output embedding is further combined with the input of the FFN through a residual module. The FFN consists of two linear layers with a GeLU activation function in between. Both the MHA and FFN utilise the pre-layer normalisation scheme, facilitating faster and easier training of the Transformer model.

However, the last layer of the \emph{PanGu} model deviates from the standard transformer layer structure. Instead, it incorporates a query layer designed to predict the next token, thereby enhancing the model's positional awareness and improving generation effectiveness. The query layer serves as a narrow yet powerful decoder that solely relies on position information. The structure of the query layer is illustrated in~\cref{fig:QueryArc}. The primary distinction between the query layer and the transformer layer lies in the query input of self-attention. While the inputs of Query, Key, and Value in other self-attention layers of the transformer remain standard, the query layer introduces a query embedding, which functions similarly to position embedding, as the query input for self-attention in the last layer.


% Figure environment removed

The \emph{PanGu} model is available in four distinct variations, each characterised by different parameter sizes (\cref{table:pangu-setting}). These variations include \emph{PanGu} 350M, \emph{PanGu} 2.6B, \emph{PanGu} 13B, and \emph{PanGu} 200B (which is not open source). The parameter sizes differ across these models, reflecting their varying levels of complexity and capacity for language understanding and generation.


\begin{table}[b]
    \centering
    \caption{The parametric size of the various settings in the \emph{PanGu} model. \label{table:pangu-setting}}
    \begin{tabular}{@{}cccccc@{}}
    \toprule
    Model & Parameters & Layers & Hidden Size & Head & Seq Length \\ \midrule
    \emph{PanGu} 350M & 350M  & 24 & 1024 & 16 & 1024\\
    \emph{PanGu} 2.6B  & 2.6B & 32 & 2560 & 40 & 1024   \\
    \emph{PanGu} 13B & 13.1B & 40 & 5120 & 40 & 1024 \\
    \emph{PanGu} 13B & 207.0B & 64 & 16384 & 128 & 1024 \\
    \bottomrule
    \end{tabular}
\end{table}

\subsection{WenZhong Model}
In addition to the \emph{PanGu} model, we have also incorporated the \emph{WenZhong} model as one of the models used. The \emph{WenZhong} model is a pre-trained model based on the GPT-2 architecture and trained on a large-scale Chinese corpus. Over the past few years, pre-trained models have become the foundation of cognitive intelligence, enabling advancements in natural language processing and computer vision algorithms.

The scale of pre-trained models has been rapidly increasing, growing by a factor of 10 each year, starting from the initial BERT model with 100 million parameters to the more recent GPT models with over 100 billion parameters. Given the nature of our task, which requires a generation model with expertise in different professional domains, we have opted for the \emph{WenZhong} model.

For models like GPT, limited computing resources pose a challenge that hinders further progress in the field. Universities and research institutions often lack the necessary computing power to train and utilise large-scale pre-trained models. This limitation impedes the broader implementation of AI technologies. Hence, we have adopted the \emph{WenZhong} model, which is built upon a large pre-trained model trained on a Chinese corpus.

The \emph{WenZhong} model series consists of one-way language models dominated by a Decoder structure and a series of powerful generation models. The \emph{WenZhong}-3.5B model, with 3.5 billion parameters, employs 100G data and 256 A100 GPUs for 28 hours of training, exhibiting strong generation capabilities. Thus, the \emph{WenZhong} model is highly powerful, featuring 30 decoder layers and billions of parameters. Additionally, we have utilised the \emph{WenZhong}-GPT2-110M version in this project, which comprises 110 million parameters and 12 layers. It is important to note that the \emph{WenZhong} model has been pre-trained on the Wudao Corpus (300G version).




\subsection{Collecting Large Scale Dataset}
Two types of data sources were obtained for this project. The first dataset, called PsyQA~\citep{psyqa}, consisting of question and answer pairs, focuses on Chinese psychological health support. The authors granted us authorisation to use this dataset, which contains 22,000 questions and 56,000 well-structured, lengthy answers. The PsyQA dataset includes numerous high-quality questions and answers related to psychological support, and it had already undergone basic cleaning before we received it. For our experiments, we selected a test set of 5,000 samples from this PsyQA dataset.

\subsubsection{Data Crawling}
The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions related to mental and emotional issues, and other users can provide responses. The Yixinli website specifically focuses on professional mental health support, but it only provided approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine-tuning training due to the model's input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5,000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video sharing forums in China. However, converting audio and video data into text format was a time-consuming task, resulting in a limited amount of this type of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. In the end, our entire dataset consisted of 400,000 samples, with each sample separated by a blank line, i.e., ``\textbackslash n\textbackslash n''.

\Cref{table:crawled-dataset} shows the time spent on data crawling from different websites. It is evident that a majority of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2GB. The datasets from Zhihu and Yixinli were 500MB and 200MB, respectively. Overall, we spent approximately 70 hours on data collection. Although the data collected from the Internet was abundant and authentic, the cleaning process was challenging due to inconsistencies in the online data.

\begin{table}[tb]
    \centering
    \caption{Dataset crawled from different platforms. \label{table:crawled-dataset}}
    \begin{tabular}{@{}ccc@{}}
    \toprule
    Platform & Data Size & Crawling Time  \\ \midrule
    Tianya & 2GB & 40h+  \\
    Zhihu & 500Mb & 20h+  \\
    Yixinli & 200Mb & 8h+ \\
    \bottomrule
    \end{tabular}
\end{table}

To address the time-consuming nature of web crawling, we implemented a distributed crawl technology that utilised idle computers connected to the Internet or other networks, effectively harnessing additional processing power~\citep{Thelwall2001AWC}. Our approach involved obtaining sub-websites from the main website and saving them using custom crawling code. This code primarily relied on Python libraries such as ``requests'', ``BeautifulSoup'', and ``webdriver''. In addition, we employed dynamic web crawlers capable of collecting clickable elements, simulating user actions, comparing web page states, manipulating the DOM tree, and handling various user-invoked events~\citep{Li2018AutomaticallyCD}. Unlike static page structures that cannot handle dynamic local refresh and asynchronous loading~\citep{Yao2012AnAF}, dynamic crawlers were able to extract data from behind search interfaces.

The process of the dynamic crawler involved leveraging web developer tools within the browser to obtain XHR (XMLHttpRequest) information, which included requests containing headers, previews, and responses. By systematically searching through these layers of data and capturing network packets, we were able to acquire relevant files. After obtaining the sub-websites using both static and dynamic crawling methods, we distributed them across multiple idle computers. Each computer was assigned specific sub-websites, and we collected project-related data using a combination of static and dynamic crawling techniques. Ultimately, we utilised eight computers for the crawling process, which took approximately 70 hours.


\subsubsection{Data Cleaning}
In line with the \emph{PanGu} paper~\citep{Zeng2021PanGuLA}, we adopted the original data cleaning method utilised in the \emph{PanGu} model. Additionally, we incorporated some additional cleaning steps. The following are the cleaning steps we employed:
\begin{enumerate}
    \item \emph{Removal of duplicate samples}: We eliminated any duplicate samples present in the dataset to ensure data uniqueness.
    \item \emph{Removal of samples containing advertised keywords}: We excluded samples that contained specific keywords associated with advertisements or promotional content.
    \item \emph{Deletion of data with less than 150 characters}: Samples with less than 150 characters were removed from the dataset, as they were deemed insufficient for effective model training.
    \item \emph{Removal of URLs}: Any URLs present in the samples were eliminated to maintain the focus on text content.
    \item \emph{Removal of user names and post time}: User names, such as ``\texttt{@jack}'', and post timestamps were removed from the samples, as they were considered irrelevant to the text content.
    \item \emph{Removal of repeated punctuation}: Instances of repeated punctuation marks, such as ``!!!'' or ``.....'', were removed from the samples to ensure cleaner and more concise text.
    \item \emph{Conversion of traditional Chinese to simplified Chinese}: All traditional Chinese characters were converted to simplified Chinese characters to standardise the text.
\end{enumerate}

Following the data cleaning process, the dataset could be directly inputted into the \emph{PanGu} model. However, for training with the \emph{WenZhong} model, the samples needed further processing. Specifically, all punctuations were removed, and the samples were tokenized to ensure a consistent length of 1000 tokens for compatibility with the \emph{WenZhong} model.


\subsubsection{Data Analysis}
Data analysis plays a crucial role in understanding the fundamental characteristics of textual data. In the context of Chinese language, the exploratory data analysis (EDA) methods may not be as diverse compared to those used for English. In this study, we primarily employed two common methods: word frequency analysis and sentence length analysis, to gain insights into the dataset.

To analyse the distribution of characters in each sample, we referred to the character number data presented in~\cref{table:dataset-distribution}. By visualizing this information using a box chart, we examined the range of character counts across the samples.
There are some samples that are empty after the data cleaning, of which we then prune from our dataset.
\Cref{fig:data-distribution} displays the distribution of sample lengths, indicating that the majority of samples fall within the range of 10,000 characters.

Overall, these preliminary analyses allowed us to gain initial insights into the dataset and provided a foundation for further exploration and understanding of the textual data.

\begin{table}[tb]
    \centering
    \caption{Data distribution of the length of each sample. \label{table:dataset-distribution}}
    \begin{tabular}{@{}cccccccc@{}}
    \toprule
    Count & Mean & Std & Min & 25\% & 50\% & 70\% & Max \cr \midrule
    371,434 & 5,343 & 11,335 & 0 & 653 & 1,835 & 6,039 & 454,611 \cr
    \bottomrule
    \end{tabular}
\end{table}


% Figure environment removed

To examine the word frequency in our dataset, we conducted an analysis after removing the stop words. The word cloud visualisation in~\cref{fig:frequent-word} illustrates the most frequent words in the dataset. Notably, the prominent words observed include ``Anxiety'', ``Appearance'', ``Marriage'', ``Relationship'', ``Family'', and ``Stressful''. These words are highly relevant to the topic of mental health, indicating that our dataset is robust and aligns well with the focus of our task.

The presence of these mental health-related terms further underscores the suitability of our data for addressing the objectives of our study. It suggests that our dataset encompasses significant content related to psychological aspects, allowing us to effectively explore and address relevant topics in the context of our research.

% Figure environment removed




\subsection{Model Training}
\textbf{Model Size} We plan to use \emph{PanGu} 350M to generate language considering the computational power, which contains 350 million parameters, 24 layers, 1024 hidden size and 16 attention heads. Besides, we also want to train the \emph{WenZhong}-110M model that contain 12 layers and has 110M parameter.

\textbf{Training Data} We employ the 2.85GB psychology corpus data which is crawled from psychology platforms like Yixinli and Tianya, to train the original \emph{PanGu} 350M model. After that,  we use 5,6000 question-answer pairs from PsyQA dataset to fine-tune the model.

\textbf{Training Platform} We train the \emph{PanGu} model on OpenI platform with a free V100 graphics card GPU, because OpenI is the open source platform of \emph{PanGu} model, convenient for us to deploy the required files, image and GPU. The batch size is set to 8 and the training iteration is set to 100,000, because we found that 50,000 iteration is not enough  for the model’s loss to converge.
 Besides, we train the \emph{WenZhong} model in jupyter notebook .To fine-tune this model, we need to tokenize the data, which can transform words into the token. Besides, we also need to isolate the max length of each sentence as 500.





\subsection{Dataset Evaluation}

Determining the cleaning rules and data filtering thresholds are important aspects of the data cleaning process. To evaluate the quality of the dataset obtained from website crawling, we employed a data quality evaluation method that combined both manual assessment and model-based evaluation.

For the model-based evaluation, we utilised the \emph{PanGu} 350M model and calculated the perplexity metric after each stage of data cleaning. A lower perplexity value indicates a more effective cleaning process and higher dataset quality.
In addition to the model-based evaluation, we sought input from experts in the field of psychology. We invited two members from our University's School of Psychology, Faculty of Science to perform a random sample check on the dataset after it had undergone the cleaning process. While this method does not cover the entire corpus comprehensively, it does provide valuable insights and plays a role in both data cleaning and data quality evaluation.

The evaluation process involved the following steps: First, we provided the experts with a sample of the cleaned dataset and asked them to assess its quality based on their expertise and domain knowledge. They evaluated the dataset for accuracy, relevance, and coherence, providing feedback and suggestions for further improvements.

Next, we conducted a comparative analysis between the model-based evaluation and the expert evaluation. We examined the perplexity scores obtained from the \emph{PanGu} 350M model and compared them with the feedback provided by the experts. This allowed us to identify any discrepancies or areas of improvement in the dataset.

Overall, the combination of model-based evaluation and expert assessment provided a comprehensive evaluation of the dataset quality. It allowed us to identify and address any issues or shortcomings in the data cleaning process, ensuring that the final dataset used for training and evaluation was of high quality and suitable for our research purposes.

\subsection{Model Training Setting}

\emph{Models:} For our training, we utilise the \emph{PanGu} 350M model, considering the available computational resources. This model consists of 350 million parameters, 24 layers, a hidden size of 1024, and 16 attention heads. Additionally, we target the \emph{WenZhong}-110M model, which contains 12 layers and 110 million parameters.

\emph{Training Data:} We collected a psychology corpus dataset, totaling 2.85GB, which was crawled from psychology platforms such as Yixinli and Tianya. This dataset was used for training the original \emph{PanGu} 350M model. Subsequently, we fine-tuned the model using 56,000 question-answer pairs from the PsyQA dataset.

\emph{Training Platform:} The \emph{PanGu} model was trained on the OpenI platform, utilising a free 1 V100 graphics card GPU. OpenI is an open-source platform specifically designed for the \emph{PanGu} model, allowing us to easily deploy the necessary files, images, and GPU resources.
For training with the V100 graphics card (32 GB memory), the minimum recommended configuration is one card, while the recommended configuration is two cards. The graphics card requirements can be adjusted based on the memory size (for example, a 16GB memory card would require twice as many cards as the V100). If the dataset is large, increasing the number of graphics cards can help improve the training speed
We set the batch size to 8 and performed training for 100,000 iterations, as we observed that 50,000 iterations were insufficient for the model's loss to converge.
For the \emph{WenZhong} model, we used Jupyter Notebook to run the pre-trained model and fine-tuned it on a system with 64GB memory and an RTX3060 graphics card. The version details of the hardware and software components are listed in~\cref{table:hardware-software-version}.



\begin{table}[tb]
    \centering
    \caption{Hardware and Software Versions}
    \label{table:hardware-software-version}
    \begin{tabular}{@{}cc@{}}
    \toprule
    Hardware and Software & Version \\
    \midrule
    Operating System & Windows 10 \\
    numpy & 1.18.5 \\
    pandas & 1.3.4 \\
    torch & 1.11.0 \\
    tokenizers & 0.13.1 \\
    tqdm & 4.64.1 \\
    jupyter & 1.0.0 \\
    transformers & 4.23.1 \\
    \bottomrule
    \end{tabular}
\end{table}









\subsubsection{Training Process}
Training Process. According to the guide of training \emph{PanGu} model with GPU, the first step is environment configuration. We prepared Pytorch, \emph{PanGu} image, one V100 graphics card and some \emph{PanGu} model files like vocabulary. The second step is data preprocessing. We put the training corpus into txt file, and each sample is a paragraph separated by two newlines (Figure 6). Then converted it into bin file, because it is the required input format of training \emph{PanGu} model. The third step is model training. We uploaded the \emph{PanGu} model and the bin file to the OpenI platform and set some parameters like iteration to train it (Figure 7). The training of \emph{PanGu} model contains two steps, which is also one of the highlights of the model. Firstly, we train the original \emph{PanGu} 350M model with all the crawling data for 100,000 iterations, because we have tried to train 50,000 iterations but the loss of the model did not converge. We can see from Figure 8, when iterations are about 60,000, the loss converges, so we downloaded the model trained 60,000 iterations. This model have learned psychology domain knowledge. Secondly, we fine-tune it with the PsyQA dataset. Figure 9 reveals that the best iteration is about 9,000, so that is the final best \emph{PanGu} model after training and fine-tuning. The fourth is to download the best trained \emph{PanGu} model based on the loss, perplexity from the output log (Figure 10) and manual testing. Finally, we used the GPU environment to inference.

We used the early stop method to choose appropriate iterations. Stopping the training of the network before the validation loss started to increase effectively prevented the model from overfitting. For example, in Figure 9, when the model has more than 9,000 training iterations, the validation loss of the model starts to rise, which means the phenomenon of overfitting occurs.

In \emph{WenZhong} model,before we train the model, we need to import \emph{WenZhong} model and tokenizer. However, when we run the code in colab, there is an error called ‘ CUDA out of memory ‘. Therefore, we have to use a new way to calculate the gradient. First of all, it can sum up the gradient in every step. Second, this way can get the average loss by dividing the gradient by the steps, which means less accumulation. After solving this problem, we can free to set the batch size, the numbers of epochs and so on.

% Figure environment removed







\subsection{Model Evaluation}

In this section, we assess the performance and effectiveness of our proposed language model for online psychological consultation. We employ a combination of intrinsic evaluation metrics and human evaluation to comprehensively evaluate the model's capabilities. We begin by utilising perplexity, ROUGE-L, and Distinct-n metrics to measure the model's language generation quality, similarity to the reference text, and diversity. Additionally, we recognise the limitations of these metrics and emphasise the importance of human evaluation in providing subjective assessments of the model's outputs, considering factors such as coherence, relevance, and overall quality. Through this comprehensive evaluation approach, we aim to gain a comprehensive understanding of our model's strengths, weaknesses, and suitability for its intended purpose in the context of online psychological consultation.

\subsubsection{Metric-based Evaluation}

Perplexity is a widely used intrinsic evaluation metric that provides a measure of how well a language model predicts a given sample. Mathematically, perplexity is defined as the reciprocal of the average probability assigned to each token in the dataset by the language model~\citep{Chen1998EvaluationMF}. In simpler terms, a lower perplexity value indicates better performance of the language model. Since perplexity is based on the average log-likelihood of the dataset, it can be computed quickly and is statistically robust, as it is not easily affected by outliers.

The formula for calculating perplexity is given by
 \begin{align}
    PP(W)
    &= \mathbb{P}\left( w_1 w_2 \ldots w_N \right)^{{-1}/{N}} \\
    &=\sqrt[N]{ \prod_{i=1}^N \frac{1}{\mathbb{P}(w_i)} }
 \end{align}
where $PP$ is the perplexity, $\mathbb{P}$ is the probability of the $i^{th}$ word, and $N$ is the length of a sentence.
It is important to note that as the dataset size increases, perplexity tends to decrease, indicating better performance.

However, it is crucial to understand that low perplexity does not necessarily equate to high accuracy. Perplexity is primarily used as a preliminary measure and should not be solely relied upon for evaluating model accuracy. Additionally, comparing the performance of models on different datasets with varying word distributions can be challenging~\citep{Chen1998EvaluationMF}. Therefore, while perplexity provides valuable insights into model performance, it should be complemented with other evaluation metrics and considerations when assessing model accuracy.



ROUGE-L (Longest Common Subsequence) is an evaluation metric that measures the number of overlapping units between the predicted text generated by a language model and the actual reference text~\citep{Lin2004ROUGEAP}. By quantifying the similarity between the predicted and reference texts, ROUGE-L provides a measure of how closely the generated text matches the desired output.

Distinct-1 and Distinct-2 are evaluation metrics that assess the diversity of the generated text. Distinct-1 calculates the number of distinct unigrams (individual words) divided by the total number of generated words, while Distinct-2 calculates the number of distinct bigrams (pairs of adjacent words) divided by the total number of generated bigrams~\citep{Li2016ADO}. These metrics reflect the degree of diversity in the generated text by quantifying the presence of unique unigrams and bigrams.

The formulas for calculating Distinct-n are as follows:

\begin{equation}
    \text{Distinct-n} := Distinct(n)=\frac{Count(\texttt{unique}, \texttt{n-gram})}{Count(\texttt{word})}
\end{equation}

Here, $Count(\texttt{unique}, \texttt{n-gram})$ represents the number of $n$-grams that are not repeated in a reply, and $Count(\texttt{word})$ indicates the total number of $n$-gram words in the reply. A higher value of $Distinct(n)$ indicates a greater diversity in the distinct generations.

Together, these evaluation metrics, including perplexity, ROUGE-L, Distinct-1, and Distinct-2, provide insights into the quality, similarity, and diversity of the generated text by the language model. They serve as valuable tools for assessing the performance and effectiveness of the model in generating accurate and diverse outputs.

While metrics like perplexity and Distinct-n provide insights into the language model's performance in terms of language generation, they do not necessarily indicate high accuracy. Therefore, in order to evaluate models more convincingly, human evaluation is still necessary. Human evaluators can provide subjective assessments of the generated text, taking into account factors such as coherence, relevance, and overall quality, which are important aspects that cannot be fully captured by automated evaluation metrics alone.


\subsubsection{Human evaluation}


For the purpose of human evaluation, we have developed an online marking system to assess the performance of our language model in the context of online psychological consultation. This evaluation system aims to streamline the process and ensure effective assessment by focusing on four key metrics: Helpfulness, Fluency, Relevance, and Logic. Each metric is scored on a scale of 1 to 5, allowing evaluators to provide a quantitative assessment of each aspect.
The four metrics are defined as follows:

\begin{enumerate}
    \item \textbf{Helpfulness:} This metric evaluates whether the generated response is helpful for patients seeking psychological support.
    \item \textbf{Fluency:} Fluency refers to the degree of coherence and naturalness exhibited in the generated response.
    \item \textbf{Relevance:} Relevance assesses the extent to which the content of the response is directly related to the posed question.
    \item \textbf{Logic:} Logic examines the logical consistency and coherence of the meaning conveyed in the generated response.
\end{enumerate}


To conduct the human evaluation, we invited six students from the psychological faculty to assess a set of 200 question-answer pairs generated by our model. We employed two evaluation methods to obtain a comprehensive understanding of the model's performance.

In the first method, evaluators compared responses generated by both the \emph{PanGu} model and the \emph{WenZhong} model in response to the same question. They assigned scores to these answers based on the predetermined metrics, allowing for a direct comparison between the two models.
The second method involved incorporating the actual answers alongside the predicted responses as a whole, allowing evaluators to assess the differences and similarities between the generated responses and the actual ones.

By employing these human evaluation methods, we aim to gain valuable insights into the performance of our language model, particularly in terms of the disparities between predicted and actual responses. This comprehensive evaluation approach will provide a deeper understanding of the model's capabilities and guide further improvements in its performance for online psychological consultation.
