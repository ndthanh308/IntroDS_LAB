\section{Appendix}\label{sec:appendix}
\subsection{Slackness Resulting from Correlated Test Statistics}

In this section we briefly highlight the problem that results from a failure to account for the correlation between consecutive test statistics. First recall the four stage framework discussed in Section~\ref{sec:background}. In particular recall that whether a shift detector makes a detection at time step $t$ typically depends on whether a corresponding test statistic $\hat{d}(S,\tilde{S}_t)$ exceeds a threshold $\hat{h}_t$, where $\tilde{S}_t$ denotes the window of deployment summaries at time step $t$. Most commonly, a sliding window of a fixed size $w$ is used such that $\tilde{S}_t = \{s(x_j,y_j)\}_{j=t-w+1}^{j=t}$. 

An approach that is particularly prevalent in the literature is to use a time-invariant threshold ($\hat{h}_t=\hat{h}$ for $t=1,2,...$) that estimates the threshold $h$ exceeded by the first test statistic $\hat{d}(S,\tilde{S}_W)$ with a prescribed false positive probability $\alpha$. Note that this is equivalent to estimating p-values at each time step and making a detection if one falls below $\alpha$. If consecutive test statistics were statistically independent, and $h$ accurately estimated, this results in a detector for which the expected run time to false detection $E[T_\infty]$ approximately equals $1/\alpha$. However the correlation between test statistics means that instead it results in detectors for which $1/\alpha$ is a lower bound on $E[T_{\infty}]$. Whilst this lower bounding can indeed be considered as a form of ``control" over the false positive rate, we have found from practical experience that it frequently results in the configuration of detectors that operate at such a low false detection rate that the true detection rate is also unknowingly unacceptably low. We provide some very simple experiments that demonstrates how slack the bound can become in practice.

We consider the simple setting where the distribution of the summary statistics is simply the standard normal distribution $N(0,1)$, for both the reference set $S$ of size $n$ and deployment stream $(s_t)_{t \geq 1}$. We consider a deployment window $\tilde{S}_t = \{s(x_j,y_j)\}_{j=t-w+1}^{j=t}$ updated at each time step $t$ to contain the $w$ most recent statistics. As the test statistic we (similarly to \citet{dos2016fast}) use the Kolmogorov-Smirnov distance $\hat{d}(S,\tilde{S}_t)=\max_u|F_n(u)-G_{w,t}(u)|$, where $F_n$ and $G_{w,t}$ are the empirical cumulative distribution functions of $S$ and $\tilde{S}_t$ respectively. This allows, for a prescribed false positive probability $\alpha$, the accurate estimation of $h$ using standard methods \cite{hodges1958significance}. We wish to specify an expected run time to false detection $E[T_\infty]$  of 1000, and therefore specify $\alpha=1/1000$. Figure \ref{fig:both} shows, over 250 independent runs, the actual average run time to false detection for a number of reference set sizes and window sizes.

We see from Figure~\ref{fig:a} that for a moderately sized deployment window of $w=100$ and reference set size of $n=3000$, the actual $E[T_{\infty}]$ is already 11 times larger than the lower bound. As the window size increases, and therefore the correlation between consecutive statistics grows, the slackness increases. At a window size of $w=500$ the actual $E[T_{\infty}]$ is 72 times larger than the lower bound. Increasing the window size further would of course result in further slackness, but is beyond the computational budget of these experiments. By contrast, increasing the reference set size does not affect the correlation between consecutive test statistics, but does affect the ability to obtain statistically significant ($\alpha=0.001$) values. Figure~\ref{fig:b} shows that the actual $E[T_{\infty}]$ therefore initially decreases with increasing reference set size but then levels off to approximately 32 times the lower bound (for a window size of $w=300$).

% Figure environment removed

The takeaway from these experiments is that although the described approach can be used to specify a lower bound on the expected run time to false detection $E[T_{\infty}]$ in the absence of change, in practice it will actually take some unknown value likely orders of magnitude above this lower bound that depends on various factors such as reference set size, window size, test statistic and pre-change distribution. This makes it impossible for the approach to be used to correspond to constraints on how often practitioners are willing to respond to false detections. Thresholds need to be set using approaches that account for the correlation between consecutive test statistics. Other than the recent work of \citet{cobb2022sequential}, there is relatively little work exploring how this can be achieved in the general (multivariate and nonparametric) case, despite the availability of the large set of reference data $S$ from the pre-change distribution.