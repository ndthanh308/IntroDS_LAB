
In order to demonstrate the potential for successful model training and analysis using the provided dataset, we conducted three types of checks to ensure the accuracy and quality of the annotations.

Firstly, polygon annotations were obtained by experienced researchers. During this phase, the annotations underwent multiple rounds of double-checking to ensure that the polygons did not have intersecting edges and that they accurately represented the objects when transformed into binary masks.

Secondly, we leveraged domain knowledge to validate the annotations. 
Precisely, we tested the binary masks against our expectations regarding the sizes and shapes of the biological structures involved. This validation process relies on a quantitative evaluation concerning objects' area and diameter, complemented by a visual scrutiny of the masks to ensure they align with the expected shapes and exhibit smooth contours.
\begin{table}[htbp]
\centering
\resizebox{\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l *{3}{S[table-format=2.2]} c *{3}{S[table-format=2.0]} c *{3}{S[table-format=3.2]} c *{3}{S[table-format=2.2]} c *{3}{S[table-format=2.2]}}
\toprule
& \multicolumn{3}{c}{\textbf{signal \%}} && \multicolumn{3}{c}{\textbf{cell count}} && \multicolumn{3}{c}{\textbf{area (\si{\micro\meter\squared})}} && \multicolumn{3}{c}{\textbf{Feret diameter (\si{\micro\meter})}} && \multicolumn{3}{c}{\textbf{equivalent diameter (\si{\micro\meter})}} \\
\cmidrule(lr){2-4} \cmidrule(lr){6-8} \cmidrule(lr){10-12} \cmidrule(lr){14-16} \cmidrule(l){18-20}
\textbf{collection} & {green} & {red} & {yellow} && {green} & {red} & {yellow} && {green} & {red} & {yellow} && {green} & {red} & {yellow} && {green} & {red} & {yellow} \\ \midrule
count & \multicolumn{1}{c}{283} & \multicolumn{1}{c}{184} & \multicolumn{1}{c}{283} && \multicolumn{1}{c}{4606}\tnote{a} & \multicolumn{1}{c}{4486}\tnote{a} & \multicolumn{1}{c}{2659}\tnote{a} && \multicolumn{1}{c}{4600} & \multicolumn{1}{c}{4483} & \multicolumn{1}{c}{2621} && \multicolumn{1}{c}{4600} & \multicolumn{1}{c}{4483} & \multicolumn{1}{c}{2621} && \multicolumn{1}{c}{4600} & \multicolumn{1}{c}{4483} & \multicolumn{1}{c}{2621} \\
mean  & 0.64 & 2.61 & 0.65 && 28.65 & 34.96 & 25.76 && 74.78 & 246.52 & 132.55 && 12.20 & 25.68 & 17.24 && 9.64 & 17.13 & 12.63 \\
std   & 0.57 & 1.74 & 0.80 && 17.49 & 17.90 & 19.88 && 23.39 & 130.24 & 66.02 && 1.92 & 8.86 & 4.86 && 1.48 & 4.50 & 3.04 \\
\midrule
min   & 0 &   0 &   0 &&    0 &    0 &    0 &&   12.41 &   24.81 &   17.62 && 5.48 &    7.08 &    6.09 && 3.97 &    5.62 &    4.74 \\
10\%   &     0.10 &   0.37 &   0 &&    9 &   14 &    4 &&   48.74 &  106.72 &   66.16 && 9.85 &   15.67 &   11.93 &&                7.88 &   11.66 &    9.18 \\
25\%   &     0.25 &   1.34 &   0.13 &&   15 &   21 &    9 &&   58.58 &  144.04 &   88.02 &&              10.88 &   18.64 &   13.90 &&                8.64 &   13.54 &   10.59 \\
50\%   &     0.45 &   2.34 &   0.40 &&   25 &   31 &   19 &&   71.67 &  218.80 &  117.75 &&              12.07 &   24.38 &   16.51 &&                9.55 &   16.69 &   12.24 \\
75\%   &     0.92 &   3.53 &   0.82 &&   39.75 &   50 &   44 &&   87.33 &  320.75 &  161.17 &&              13.35 &   30.99 &   19.76 &&               10.54 &   20.21 &   14.33 \\
90\%   &     1.38 &   4.89 &   1.49 &&   56 &   58 &   57 &&  104.66 &  422.64 &  218.27 &&              14.70 &   37.64 &   23.59 &&               11.54 &   23.20 &   16.67 \\
max   &     3.36 &   8.40 &   5.54 &&   69 &   90 &   72 &&  227.04 &  842.66 &  548.20 &&              21.21 &   74.39 &   41.66 &&               17 &   32.76 &   26.42 \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item[a] The difference compared to counts in following columns comes from the contribution of empty images. These amount to 6, 3 and 38 images for green, red and yellow collections, respectively.
\end{tablenotes}
\end{threeparttable}
}
\caption{Summary statistics of key features' distribution for each image collection. The top portion highlights global indicators, while the bottom one reports given percentiles of each distribution.} \label{tab:summary-stats}
\end{table}
\Cref{tab:summary-stats} reports summary statistics for the distribution of key features at the image and object levels, that can be leveraged for technical validation. 
% The measures concerning cell area and diameters are particularly relevant for technical validation. 
For instance, the annotated objects display an average area of nearly 75, 247, and 133 $\mu m^2$ for green, red, and yellow cells, respectively. These values align with the expected size of the biological structures represented in each image collection.
Additionally, the analysis of Feret and equivalent diameters provides an understanding of the typical form of the stained objects. In particular, the Feret diameter\cite{pabst2007feret} can be interpreted as a measure of the maximum extension of an object, whereas the equivalent diameter represents the diameter the object would have if it were a perfect circle with the same area. Thus, comparing these two metrics can offer insight into the objects' shape regularity.
For green cells, the values for the two measurements are relatively close (roughly 12 VS 10 $\mu m$), suggesting that these cells are broadly circular or oval in shape.
A similar conclusion can be drawn for the yellow stains, albeit with slightly more variability (approximately 17 and 13 $\mu m$), indicating generally regular shapes with occasional deviations.
In the case of red objects, instead, the comparison is markedly different. This time we observe a Feret diameter around 26 $\mu m$ against an equivalent diameter of 17, which suggests that these stains are typically elongated in one direction rather than concentrated around a center of mass.
All these observations are also corroborated when visually inspecting annotated cells, which confirms prior expectations about objects size and shapes based on the nature of the marked structures.

\subsection*{Learning}
Thirdly, we conducted a sample training phase for each image collection using a \textbf{cell ResUnet} architecture\cite{morelli2021cresunet}, specifically designed for this type of application. 
Specifically, we trained a network from scratch for each collection using a Dice Loss\cite{sudre2017diceloss} and the Adam optimizer\cite{kingma2014adam}. The initial learning rate was set based on the ``learning rate test"\cite{smith2019hyperparms} implemented by \textit{fastai}'s\cite{howard2020fastai} \texttt{lr\_find()} method. The training phase continued for 100 epochs with cyclical learning rates\cite{smith2017cyclical,smith2019super}, and the best model was selected based on the best validation dice coefficient. For all technical details please refer to the GitHub repository\footnote{available at: \href{https://github.com/clissa/fluocells-scientific-data}{https://github.com/clissa/fluocells-scientific-data} (link will be active after acceptance)}.
This training phase aims to verify the effectiveness of the data in facilitating the learning of beneficial cell features. Additionally, the intent is to highlight the relevant metrics for result evaluations. 
In particular, we suggest performance should be assessed differently depending on the end goal of future analyses.

For \textbf{segmentation} tasks, we provided an implementation where matching of actual and predicted neurons\footnote{by this we intend the calculation of True Positives (TP), False Positives (FP) and False Negatives (FN)} is done based on their overlap, quantified as \textit{Intersection-over-Union (IoU)}\cite{kirillov2019panoptic}. This approach not only ensures a 1-1 correspondence of true and predicted objects but also assesses how closely the predictions reconstruct the shape of ground-truth cells.
Building on top of this definition, standard metrics such as \textit{precision, recall} and \textit{$F_1$ score} can be computed as measures of global performance.

\textbf{Detection} tasks, on the other hand, would benefit from a looser matching criterion, comparing predicted and true objects' centers instead of overlaps. This approach prioritizes recognition over precise shape reconstruction.

Finally, for \textbf{counting} tasks we suggest common regression metrics such as \textit{Mean Absolute Error (MAE), Median Absolute Error (MedAE)} and \textit{Mean Percentage Error (MPE)}. 

\begin{table}[ht]
\centering
\label{tab:performance-metrics}
\begin{tabular}{@{}lcccccccccc@{}}
\toprule
& \multicolumn{3}{c}{\textbf{segmentation}} & \multicolumn{3}{c}{\textbf{detection}} & \multicolumn{3}{c}{\textbf{counting}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
\textbf{metrics} & $F_1$ Score & Precision & Recall & $F_1$ Score & Precision & Recall & MAE & MedAE & MPE (\%) \\ \midrule
Green & 0.69 & 0.79 & 0.62 & 0.77 & 0.86 & 0.69 & 1.20 & 1.00 & 9\% \\
Red & 0.28 & 0.33 & 0.25 & 0.60 & 0.69 & 0.53 & 7.13 & 7.00 & 42\% \\
Yellow & 0.65 & 0.67 & 0.63 & 0.77 & 0.78 & 0.75 & 1.54 & 1.00 & 18\% \\ \bottomrule
\end{tabular}
\caption{\textbf{Performance metrics by learning task.} The \textit{segmentation} portion refers to TP, FP, and FN computed based on objects overlapping (IoU). For \textit{detection} metrics, predicted and true cells are associated based on their centers' distance. Counting metrics simply consider the difference between predicted and true objects.}\label{tab:metrics}
\end{table}

\Cref{tab:metrics} shows the results of the sample training for each image collection. These results are not intended to be a comprehensive exploration of the model's capabilities on FNC data, but rather to showcase some characteristics of various evaluation methods.  Nonetheless, they may serve as a baseline for future studies.

Despite no optimization of the training pipeline, the initial results are mainly satisfactory (except for red segmentation), confirming the technical robustness of the data collection process.
Going into more details, we observe a marked discrepancy between segmentation and detection metrics. As expected, the $F_1$ scores based on the distance between true and predicted centers of mass are significantly higher than the corresponding segmentation indicators. 
Moreover, the discrepancy is greater for image collections where the objects have more irregular shapes (green < yellow < red).
This is a consequence of the more inclusive matching criterion used for detection tasks.
% of course expected, as the matching criterion used in detection metrics is looser, thus favoring more true/predicted matches.

In terms of counting, performance is already very satisfactory. However, these metrics may not fully represent the model's performance as good results could arise due to a balancing effect between true positives and false negatives. 
% Indeed, in such cases the contributions of errors in each direction (undercounting/overcounting) tend to balance.
Interestingly, despite low absolute errors, the percentage error is relatively high due to the impact of errors in images with few or no cells. 
To address this issue, we adopt the following formula for MPE computation: 
$ \text{MPE} = \dfrac{\left( \text{predicted} - \text{true} \right)}{\max{\left( \text{true}, 1\right)}}$.
In this way, the fraction is not over-inflated when there are no cells in the original mask.
