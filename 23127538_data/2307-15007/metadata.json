{
  "title": "Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability",
  "authors": [
    "Usha Bhalla",
    "Suraj Srinivas",
    "Himabindu Lakkaraju"
  ],
  "submission_date": "2023-07-27T17:06:02+00:00",
  "revised_dates": [
    "2024-02-15T20:10:47+00:00"
  ],
  "abstract": "With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by identifying features critical to model predictions; however, prior work has shown that these explanations may not be faithful, in that they incorrectly attribute high importance to features that are unimportant or non-discriminative for the underlying task. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we identify a key reason for the lack of faithfulness of feature attributions: the lack of robustness of the underlying black-box models, especially to the erasure of unimportant distractor features in the input. To address this issue, we propose Distractor Erasure Tuning (DiET), a method that adapts black-box models to be robust to distractor erasure, thus providing discriminative and faithful attributions. This strategy naturally combines the ease of use of post hoc explanations with the faithfulness of inherently interpretable models. We perform extensive experiments on semi-synthetic and real-world datasets and show that DiET produces models that (1) closely approximate the original black-box models they are intended to explain, and (2) yield explanations that match approximate ground truths available by construction. Our code is made public at https://github.com/AI4LIFE-GROUP/DiET.",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": "NeurIPS 2023 (Thirty-seventh Conference on Neural Information Processing Systems)",
  "arxiv_id": "2307.15007",
  "pdf_url": "https://arxiv.org/pdf/2307.15007v2",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 3370427,
  "size_after_bytes": 185020
}