
\section{Proofs}

\begin{theorem}
    $\Q$-feature attributions applied to $\Q$-robust models ($\mathcal{F}_v(\Q)$) \underline{recover the signal-distractor decomposition} for Bayes-optimal predictor $f_v^* \in \mathcal{F}_v(\Q)$.
\end{theorem}

\begin{proof}
Consider QFA with $\epsilon = 0$. Let $f^*$ be the Bayes-optimal model, which implies that $f_y^*(\X) = p_{bayes}(y \mid \X) = p(y \mid \X) = \frac{p(\X \mid y)}{\sum_i p(\X \mid y=i)} $, i.e., the model estimates the correct conditional probabilities from the data given the class-conditional generative probabilities. Note that this is only defined for inputs $\X \in \mathcal{X}$ in the support of the data distribution and not outside, i.e., $p_{bayes}(y \mid \X) = p(y \mid \X)$ only when $\X \in \mathcal{X}$.

Let us define $\X \odot (1 - \M) = \X_\text{distractor} \sim \mathcal{X}_\text{distractor}$. This is the distribution of distractor images, which (recall) are independent of the label $y$. Using this, we consider an \textbf{idealized version of QFA}, with $\Q_{ideal} = \mathcal{X}_\text{distractor}$. From Definition \ref{defn:QFA}, this results in generation of simplified inputs $\X_s = \X \odot \M + q \odot (1 - \M)$. For $q \sim \Q_{ideal} = \mathcal{X}_\text{distractor}$, we observe that $\X_s \in \mathcal{X}$, the data distribution. Recall the Bayes optimal classifier is defined across the data distribution $\mathcal{X}$, and applying QFA results in the sparsest $\M$ such that $p(y \mid \X) =  p_{bayes}(y \mid \X) = p_{bayes}(y \mid \X_s) = p(y \mid \X \odot \M + q \odot (1 - \M)) = p(y \mid \X \odot \M)$. The last equality holds because $q \odot (1-\M)$ is independent of $y$. This corresponds to the definition of the signal-distractor, and thus it implies that \textbf{QFA with $\Q_{ideal}$ recovers the mask defined by the signal-distractor decomposition}. 

For any other value of $\Q \neq \Q_{ideal}$, we first consider the $\Q$-robust Bayes optimal predictor $p_{bayes}^{\Q}(y \mid \X)$. This has the property that $p_{bayes}^{\Q}(y \mid \X) = p_{bayes}(y \mid \X)$ for $\X \in \mathcal{X}$. We now compare mask $\M_{\Q}$ derived from applying QFA on $p_{bayes}^{\Q}$ and mask $\M_{\Q_{ideal}}$ from applying QFA with $\Q_{ideal}$ on $p_{bayes}^{\Q}$. From the previous paragraph, we know that $\M_{\Q_{ideal}} = \M_{ideal}$ is the ideal sparsest mask. However from Definition \ref{defn:QVIM}, $\M_{\Q}$ is the sparsest mask. Thus it is the case that $\M_{\Q} = \M_{ideal}$, proving our overall result.
\end{proof}

We now present proof for an additional statement not described in the main text, where we connect QFA to other commonly used feature attributions via the local function approximation framework \cite{han2022explanation} as follows.

\begin{theorem}
 QFA is an instance of the local function approximation framework (LFA), with (1) random binary perturbations, and (2) an interpretable model class consisting of linear models with binary weights
\end{theorem}

\begin{proof}
Assume a black box model given by $f_b(\X; \M) = \mathbbm{1}\left(\E_q \| f(\X_s(\M,q)) - f(\X) \|_2 \leq \epsilon\right)$ ,loss function $\ell(f,g,x,\xi) = (f(x; \xi) - g(\xi))^2$, neighborhood perturbation $Z = \text{Uniform}(0,1)^d$, and an interpretable model family $\mathcal{G}$ being the class of binary linear models. 

For these choices, it is easy to see that 

\begin{align*}
    &\arg\min_{g \in \mathcal{G}} \ell(f,g,\X,\xi) \\
    =& \arg\min_{g \in \mathcal{G}} \E_{\xi} \left(f_b(\X; \xi) - g^\top\xi\right)^2 + \lambda \| g \|_0 \\
    =& \arg\min_{g \in \mathcal{G}} \E_\xi \left(\mathbbm{1}\left(\E_q \| f(\X_s(\xi,q)) - f(\X) \|_2 \leq \epsilon\right) - g^\top\xi\right)^2 + \lambda \| g \|_0\\
\end{align*}

This above objective is minimized when $g = \M^*$, i.e., the ideal $\epsilon\Q$-FA mask, because this sets the first term to be zero by definition, and the second sparsity term ensures the minimality of the mask.
\end{proof}




\section{Additional Results}

% \begin{table}[h!]
% \centering
% \caption{Pixel Perturbation Results}
% \label{pixel-perturbation-table}
% \begin{tblr}{
%   cell{1}{3} = {c=3}{c},
%   cell{1}{7} = {c=3}{c},
%   cell{1}{11} = {c=3}{c},
%   cell{2}{3} = {c},
%   cell{2}{4} = {c},
%   cell{2}{5} = {c},
%   cell{2}{7} = {c},
%   cell{2}{8} = {c},
%   cell{2}{9} = {c},
%   cell{3}{1} = {r=6}{},
%   hline{1-2} = {-}{},
%   hline{3} = {3-5,7-9,11-13}{},
%   hline{9} = {2-13}{},
% }
%  &  & Hard MNIST &  &  &  & CelebA &  &  &  & Chest X-ray &  & \\
%  & Sparsity & 0.10 & 0.05 & 0.02 &  & 0.2 & 0.1 & 0.01 &  & 0.01 & 0.005 & 0.001\\
% \begin{sideways}Method\end{sideways} & Ours, $f_v$ & \textbf{0.977} & \textbf{0.966} & \textbf{0.738} &  & \textbf{0.986} & \textbf{0.956} & 0.553 &  & 0.984 & \textbf{0.982} & \textbf{0.771}\\
%  & Ours, $f_b$ & 0.971 & 0.925 & 0.258 &  & 0.936 & 0.902 & \textbf{0.572} &  & 0.968 & 0.961 & 0.742\\
%  & SmoothGrad & 0.944 & 0.748 & 0.258 &  & 0.637 & 0.619 & 0.373 &  & 0.806 & 0.739 & 0.649\\
%  & GradCAM & 0.908 & 0.514 & 0.132 &  & 0.891 & 0.817 & 0.497 &  & 0.625 & 0.625 & 0.625\\
%  & Input Grad & 0.606 & 0.254 & 0.127 &  & 0.607 & 0.497 & 0.302 &  & 0.716 & 0.678 & 0.633\\
%  & FullGrad & 0.967 & 0.735 & 0.22 &  & 0.752 & 0.689 & 0.459 &  & \textbf{0.997} & 0.981 & 0.695
% \end{tblr}
% \end{table}


% \begin{table}[h!]
% \centering
% \caption{Pixel Perturbation Results (Manipulated Gradients)}
% \label{pixel-perturbation-manip-table}
% \begin{tblr}{
%   cell{1}{3} = {c=3}{c},
%   cell{1}{7} = {c=3}{c},
%   cell{1}{11} = {c=3}{c},
%   cell{2}{3} = {c},
%   cell{2}{4} = {c},
%   cell{2}{5} = {c},
%   cell{2}{7} = {c},
%   cell{2}{8} = {c},
%   cell{2}{9} = {c},
%   cell{3}{1} = {r=6}{},
%   hline{1-2,9} = {-}{},
%   hline{3} = {3-5,7-9,11-13}{},
% }
%  &  & Hard MNIST &  &  &  & CelebA &  &  &  & Chest X-ray &  & \\
%  & Sparsity & 0.10 & 0.05 & 0.02 &  & 0.2 & 0.1 & 0.01 &  & 0.01 & 0.005 & 0.001\\
% \begin{sideways}Method\end{sideways} & Ours, $f_v$ & \textbf{0.981} & \textbf{0.974} & \textbf{0.616} &  & \textbf{0.975} & \textbf{0.922} & \textbf{0.543} &  & \textbf{0.970} & \textbf{0.956} & \textbf{0.747}\\
%  & Ours, $f_b$ & 0.801 & 0.204 & 0.116 &  & 0.963 & 0.851 & 0.333 &  & 0.958 & 0.914 & 0.742\\
%  & SmoothGrad & 0.110 & 0.113 & 0.124 &  & 0.352 & 0.334 & 0.333 &  & 0.653 & 0.685 & 0.685\\
%  & GradCAM & 0.110 & 0.104 & 0.109 &  & 0.349 & 0.333 & 0.333 &  & 0.685 & 0.685 & 0.685\\
%  & Input Grad & 0.113 & 0.107 & 0.101 &  & 0.340 & 0.335 & 0.334 &  & 0.685 & 0.685 & 0.685\\
%  & FullGrad & 0.880 & 0.334 & 0.114 &  & 0.936 & 0.809 & 0.333 &  & 0.743 & 0.685 & 0.685
% \end{tblr}
% \end{table}


% \begin{table}[h!]
% \centering
% \caption{Upsampling Ablation Study}
% \label{upsample-table}
% \begin{tblr}{
%   cell{1}{3} = {c=4}{c},
%   cell{1}{8} = {c=4}{c},
%   cell{2}{3} = {c},
%   cell{2}{4} = {c},
%   cell{2}{5} = {c},
%   cell{2}{8} = {c},
%   cell{2}{9} = {c},
%   cell{2}{11} = {c},
%   cell{3}{1} = {r=4}{},
%   hline{1-2} = {-}{},
%   hline{3} = {3-6,8-11}{},
% }
%  &  & Hard MNIST &  &  &  &  & CelebA &  &  & \\
%  & Sparsity & 0.10 & 0.05 & 0.02 & 0.01 &  & 0.1 & 0.05 & 0.02 & 0.01\\
% \begin{sideways}Upsample\end{sideways} & 8 & \textbf{0.977} & \textbf{0.966} & \textbf{0.738} & \textbf{0.234} &  & \textbf{0.892} & \textbf{0.908} & \textbf{0.903} & \textbf{0.880}\\
%  & 16 & 0.989 & 0.908 & 0.285 & 0.125 &  & 0.876 & 0.811 & 0.627 & 0.481\\
%  & 32 & 0.978 & 0.799 & 0.189 & 0.102 &  & 0.956 & 0.846 & 0.651 & 0.553\\
%  & 56 & 0.910 & 0.501 & 0.129 & 0.088 &  & 0.875 & 0.702 & 0.548 & 0.432
% \end{tblr}
% \end{table}





\paragraph{Model Verifiability.} We further test the verifiability of our models by evaluating how the model's performance changes when performing the pixel perturbation test on groundtruth attributions. This enables us to disentangle the verifiability of the model from the correctness of the attributions, as we know that our attributions are correct. We use the same groundtruth attributions as in \ref{correctness}. 
We report the $\ell_1$ norm between predictions made on the original samples and predictions made on the masked samples. We compare our verifiable models to the baseline models they approximate, as well as models trained with input dropout, which \cite{jethani2021have} proposes as their verifiable model class. Training with input dropout is equivalent to training $f_v$ with random masks and cross-entropy loss rather than optimized masks and $f_b$ prediction matching. 
Results are shown in \ref{model-verifiability}, where we see that our model performs similarly for masked and normal samples, whereas the other models do not. 

\begin{table}[h!]
\centering
\begin{tblr}{
      hline{1-2,5} = {-}{},
    }
     & {Hard \\MNIST} & {Chest\\ X-ray}\\
    \name~ Model ($f_v)$ & \textbf{0.027} & \textbf{0.0009}\\
    Original Model ($f_b)$ & 0.107 & 0.0032\\
    $f_b$ + Input Dropout & 0.167 & 0.0536\\
\end{tblr}
\caption{Model Verifiability}
\label{model-verifiability}
\end{table}




\paragraph{Robustness to Explanation Attacks.}
We report additional results on Chest X-ray and CelebA for the pixel perturbation tests, IOU tests, and model faithfulness tests for baseline models trained with manipulated gradients, as outlined in \ref{sec:manip}. We see that \name models are still  highly faithful and produce correct explanations even when derived from models adversarially trained to have manipulated explanations.  
\begin{table}[h!]
\centering
\caption{Faithfulness of \name Model for Manipulated Models}
\label{model-faithfulness-manip}
\begin{tblr}{
  cell{1}{2} = {c},
  cell{1}{3} = {c},
  cell{1}{4} = {c},
  hline{1-2,4} = {-}{},
}
 & {Hard \\ MNIST} & {Chest \\ X-ray} & {CelebA}\\
{Accuracy on \\Original Data\\$f_v(D_d) = f_b(D_d)$} & 0.990 & 1.00 & 0.970\\
{Accuracy on \\Simplified Data~\\$f_v(D_s) = f_b(D_d)$} & 0.980 & 1.00 & 0.946
\end{tblr}
\end{table}


\begin{table}[h!]
\centering
\caption{Intersection Over Union Results for Manipulated Models}
\label{iou-table-manip}
\begin{tblr}{
  cell{1}{3} = {c},
  cell{1}{4} = {c},
  cell{2}{1} = {r=5}{},
  hline{1-2} = {-}{},
  hline{7} = {2-4}{},
}
 &  & {MNIST \\(manipulated)} & {Chest X-ray\\(manipulated)}\\
\begin{sideways}Method\end{sideways} & \name (Ours) & \textbf{0.454 $\pm0.08$} & \textbf{0.631$\pm 0.12$}\\
 & SmoothGrad & 0.158 $\pm0.07$ & 0.000 $\pm0.00$\\
 & GradCAM & 0.040$\pm0.06$ & 0.000 $\pm0.00$\\
 & Input Grad & 0.002$\pm0.01$ & 0.000 $\pm0.00$\\
 & FullGrad & 0.333 $\pm0.12$ & 0.004 $\pm0.04$
\end{tblr}
\end{table}


% Figure environment removed


\paragraph{Signal vs Distractor Masking.}
For our CelebA experiments, there may have been unintended spurious correlations that we did not foresee and that our method did not recover, leading to a vacuous decomposition. To further support the results in Figure 3, we also perform experiments on masking the signal instead of the distractor, by masking out “important” pixels as opposed to the unimportant ones. The results for this experiment are shown in \ref{tab:signal_masking}.

\begin{table}[h]
\centering
\caption{Signal Masking experiments}
\label{tab:signal_masking}
\begin{tblr}{
  cells = {c},
  hline{1-2,6} = {-}{},
}
Fraction Masked & Masked Distractor Acc & Masked Signal Acc & Random Mask Acc\\
0.2 & 0.971 & 0.772 & 0.965\\
0.4 & 0.967 & 0.569 & 0.935\\
0.6 & 0.967 & 0.392 & 0.826\\
0.8 & 0.960 & 0.339 & 0.587
\end{tblr}
\end{table}

These results indicate that for the \name models, there do not exist pixels outside the shown signal regions in Figure 2 that contain information about the label. These results show that the signal-distractor decomposition for CelebA is very likely non-vacuous. 

\paragraph{Finetuning on less data.}
We recognize that a limitation of this work is that it requires further training of the given models on their training datasets. To address this issue, we briefly explore whether \name can be finetuned on a small subset of the data distribution (without labels) as follows for MNIST. The original model is trained on 8835 samples from the train split. \name is finetuned on 1500 samples from a separate unlabeled validation split. We perform pixel perturbation on the remaining 8500 test samples. We compare this to \name models finetuned on the full train set and note a minimal change in performance in \ref{tab:small_data_ft}.

\begin{table}[h]
\centering
\caption{Small Scale Finetuning}
\label{tab:small_data_ft}
\begin{tblr}{
  cells = {c},
  hline{1-2,6} = {-}{},
}
Percent Masked & \name (Finetuned on Train Split) & \name (Finetuned on Val Split)\\
90 & 0.977 & 0.962\\
95 & 0.966 & 0.937\\
98 & 0.738 & 0.691\\
99 & 0.234 & 0.288
\end{tblr}
\end{table}
Overall, this shows that there exist ways to decrease the computational complexity of our procedure when applied to larger datasets, which we haven’t fully investigated yet.

\paragraph{Effect on standard robustness.}
 We conduct additional robustness experiments for Gaussian and Bernoulli noise with varying standard deviations and probabilities. Results are shown in \ref{tab:gaus_rob} and \ref{tab:bern_rob}. We find that \name models are generally more robust than regular models. Note that neither \name models nor the original models are explicitly trained or tuned for robustness on these distributions.

\begin{table}[h]
\centering
\caption{Robustness experiments (Gaussian noise)}
\label{tab:gaus_rob}
\begin{tblr}{
  cell{1}{2} = {c=2}{c},
  cell{1}{4} = {c=2}{c},
  hline{1-3,6} = {-}{},
}
 & Hard MNIST &  & CelebA & \\
STD & Ours & Original & Ours & Original\\
0.2 & 0.974 & 0.976 & 0.959 & 0.939\\
0.4 & 0.961 & 0.892 & 0.675 & 0.537\\
1.0 & 0.61 & 0.176 & 0.335 & 0.438
\end{tblr}
\end{table}

\begin{table}[h]
\centering
\caption{Robustness experiments (Bernoulli noise)}
\label{tab:bern_rob}
\begin{tblr}{
  cell{1}{2} = {c=2}{c},
  cell{1}{4} = {c=2}{c},
  hline{1-3,7} = {-}{},
}
 & Hard MNIST &  & CelebA & \\
p & Ours & Original & Ours & Original\\
0.9 & 0.182 & 0.155 & 0.348 & 0.356\\
0.75 & 0.718 & 0.3 & 0.393 & 0.364\\
0.5 & 0.922 & 0.748 & 0.528 & 0.453\\
0.1 & 0.962 & 0.969 & 0.867 & 0.807
\end{tblr}
\end{table}


\paragraph{Additional Baselines: SHAP and BCosNets.}\label{shapbcos}
We consider two additional explanation baselines: SHAP \cite{lundberg2017shap} and BCosNets \cite{bohle2022b}. Results for pixel perturbation tests and visualizations are shown in \ref{fig:shap}. Results for IOU tests are shown in \ref{tab:shap-iou}.

We note that in general SHAP does not often perform as well as most gradient-based methods for image data. We find that it performs similarly to random attributions.

We add B-CosNets as an inherently-interpretable model baseline. We find that B-CosNets performs comparably to our method for the IOU test on Hard MNIST. We were unable to train it to convergence on the Chest X-ray dataset. We also find that visualizations created by B-CosNets align with our expectations and appear visually interpretable. However, our method still significantly outperforms B-CosNets on the pixel perturbation test, showing that B-CosNets are not robust to perturbations of the distractor (i.e. are not verifiable). We also note that our method can be applied to a trained black-box model of any architecture and training procedure, whereas B-CosNets, like all inherently interpretable models, cannot.

% Figure environment removed

\begin{table}[h!]
\centering
\caption{\textbf{SHAP and B-Cos IOU.} Note that B-CosNets did not converge for the Chest X-ray dataset.}
\label{tab:shap-iou}
\begin{tblr}{
  hline{1-2,5} = {-}{},
}
 & Hard MNIST & Chest X-ray\\
\name & {0.461 $\pm$ 0.08 } & {0.821 $\pm$ 0.05}\\
B-CosNets & {0.465 $\pm$ 0.07} & --\\
SHAP & {0.036 $\pm$ 0.02} & {0.016 $\pm$ 0.05}
\end{tblr}
\end{table}

\paragraph{Effect of Choice of Q.}
We perform an ablation to test the effect that the choice of Q has empirically. We consider various parameterizations of the normal distribution used for Q, and find that results are relatively consistent across the different choices of Q, as shown in \ref{fig:q_vis}.

% Figure environment removed


\section{Additional Implementation and Computation Details}
Models were trained on the original train/test split given by \url{https://github.com/jayaneetha/colorized-MNIST} for Hard MNIST and \cite{kermany2018labeled} for the Chest X-ray dataset and with a random 80/20 split for CelebA. Baseline models were trained with Adam for 10 epochs with learning rate $1e-4$ and batch size 256. All hyperparameters are included in the code for this paper. The model distillation and data distillation terms are weighted with $\lambda_1 = \lambda_2 = 1$. We learn our masks with SGD (lr=$300$, batch size = $128$) and our robust models with Adam (lr=$1e-4$, batch size = $128$). We ran all experiments on a single A100 80 GB GPU with 32 GB memory. 

\section{Broader Impact} 
Our method, \name, aims to transform black-box models into distractor robust, interpretable models, which produce easily verifiable feature attributions. As such, if applied correctly, it can help users and stakeholders of machine learning models better understand a model's predictions and behavior by isolating the features necessary for each prediction, which can help highlight biases, overfitting, mistakes, and more. It can also help to identify spurious correlations that naturally exist in datasets and are leveraged by models through identification of the signal-distractor decomposition. 
% However, if \name does not identify a spurious correlation, it does not mean that none exist in the dataset and that further dataset cleaning and curation are not needed, but simply that the given model did not learn those spurious correlations
However, even if \name does not identify a spurious correlation, that does not mean that further dataset cleaning, processing, or curation is not needed, as a different model may still learn a spurious correlation that was not leveraged by the original model. 
Furthermore, feature attributions often do not constitute a \textit{complete} explanation of a model. For instance, while an attribution tells us \textit{what} was important, it does not tell us \textit{how} it was important or how the model uses that feature. In all high stakes applications, it is still imperative that stakeholders think critically about each prediction and explanation, rather than blindly trusting either. 
