\paragraph*{Post-Hoc Explainability.}
Post-hoc explainability methods aim to explain the outputs of fully trained black-box models either on an instance-level or global level. The most common post-hoc methods are feature attribution methods that rank the relative importance of features, either by explicitly producing perturbations \cite{ribeiro2016should, lundberg2017shap}, or by computing variations of input gradients \cite{smilkov2017smoothgrad, srinivas2019full, selvaraju2017grad}. Perturbation-based methods are especially popular in computer vision literature \cite{zeiler2014visualizing, fong2017interpretable, fong2019understanding, dabkowski2017real}, which use feature removal strategies adapted specifically for image data. However, these methods all assume a specific form for feature removal, and we show theoretically in Section \ref{sec:theory} that this can lead to unverifiable attributions.

\paragraph*{Inherently Interpretable Models.}
Inherently interpretable models are constructed such that we know exactly what they do, either through their weights or explicit modular reasoning. As such, the explanations provided by these models are more accurate than those given by post-hoc methods; however, the performance of interpretable models often suffers when compared to unconstrained black-box architectures. The most common inherently interpretable model classes include linear models, decision trees and rules with limited depth, GLMs, GAMs \cite{hastie2017generalized}, JAMs \cite{chen2018learning, yoon2019invase, jethani2021have}, prototype- and concept-based models \cite{chen2019looks, koh2020concept}, and weight-input aligned models \cite{bohle2022b}. While \cite{chen2019looks, koh2020concept} leverage the expressivity of deep networks, they constrain hypothesis classes significantly and still often suffer from a decrease in performance. Among these, our work most closely relates to JAMs, which amortise feature attribution generation using a learnt masking function to generate attributions in a single forward pass, and trains black-box models using input dropout. On other hand, JAMs (1) trains models from scratch, whereas \name~can interpret black-box models, (2) amortises feature attributions using a masking function resulting in less accurate attributions, (3) trains models to be robust to a large set of candidate masks via input dropout, leading to low predictive accuracy, whereas \name~trains models only to be robust to the optimal mask, leading to more flexibility and higher predictive accuracy.

\paragraph*{Evaluating Correctness of Explanations.}
As explainability methods grow in number, so does the need for rigorous evaluation of each method. Research has shown that humans naively trust explanations regardless of their ``correctness'' \cite{lakkaraju2020fool}, especially when explanations confirm biases or look visually appealing. Common approaches to evaluate explanation correctness rely on feature / pixel perturbation \cite{samek2016evaluating, srinivas2019full, agarwal2022openxai}, i.e., an explanation is correct if perturbing unimportant feature results in no change of model outputs, whereas perturbing important features results in large model output change. \citet{hooker2019benchmark} proposed remove-and-retrain (ROAR) for evaluating feature attribution methods by training surrogate models on subsets of features denoted un/important by an attribution method and found that most gradient-based methods are no better than random. While prior works focused on developing metrics to evaluate correctness of explanations, our method \name~produces models that have explanations that are accurate by design, according to pixel-perturbation methods.


