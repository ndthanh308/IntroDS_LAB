An important desideratum of machine learning models is for their predictions to be explainable. 
This allows both human domain experts as well as laypeople to better understand and trust the decisions made by models, and furthermore, is also a regulatory requirement for high-stakes settings. 
For example, both the European General Data Protection Regulation (GDPR) \cite{eugdpr} and the US AI Bill of Rights \cite{aibillofrights}, require organizations to provide explanations for decisions made in high-stakes settings. A common approach to producing such explanations from black-box models in a post hoc manner is via feature attribution, which aims to identify important input features influencing a model prediction. 
These methods typically work by locally approximating non-linear models with linear ones \cite{han2022explanation} under some input perturbations such as feature erasure. 
Intuitively, if the underlying model is more sensitive to the erasure of feature A than feature B, these methods aim to attribute a higher ``importance'' to feature A than feature B. 
A fundamental prerequisite for a feature to be considered important by a model is that it must first be useful in predicting the label, that is, it must be discriminative for the task.
If a feature does not contain information relating to the output label, then it cannot be used to predict the label, and thus feature attribution methods must not consider them important.
However, recent works \cite{shah2021input, hooker2019benchmark} have found this not to be case -- feature attribution methods often highlight non-discriminative features. 

% This motivates the question considered by this paper: can we alter standard models to produce pos-hoc attributions that reliably highlight discriminative features, thus bridging the gap between the performance and conveniency benefits of post-hoc explanations and the faithfulness of inherently interterpretable models? }
This motivates a natural question: what causes feature attributions to highlight such non-discriminative features, making them unfaithful? 

% Figure environment removed

Answering this question has been hard because of a lack of theoretical understanding of the faithfulness of feature attributions. While these notions have been used empirically \cite{shah2021input, hooker2019benchmark} to assess the quality of attributions, the theoretical characterization of optimally faithful feature attributions is missing in the literature. In this work, we tackle this problem by proposing a framework for feature attribution methods emphasizing faithfulness and particularly discriminability, formalized via the \textit{signal-distractor decomposition} for datasets. Essentially, the signal denotes the discriminative parts of the input (relative to a given task), while the distractor denotes the unimportant parts. Feature attribution methods are then evaluated on how well they are able to recover the signal, thus also providing a well-defined notion of a ``ground-truth''.
We theoretically identify an important criterion to recover this ground-truth, that being the robustness of the model to the erasure of the input distractors.  
To enable black-box models to recover such ground truth attributions, we propose \emph{Distractor Erasure Tuning} (\name), a method that adapts models to be robust to the erasure of input distractors. Given that these distractor regions are not known in advance, our method works by alternating feature attribution and model adaptation. At a high level, our work still uses feature attribution methods while adapting black-box models to have faithful attributions. Thus, this strategy naturally combines the ease of use of post hoc explanation methods with the faithfulness benefits of inherently interpretable models by providing the best of both alternatives. Our contributions are the following:

\begin{enumerate}

    \item We present a formalism for feature attribution that emphasizes discriminability and allows for a notion of well-defined ground truth, via the signal-distractor decomposition of a dataset. We show that it is necessary for models to be robust to distractor erasure for them to be able to recover this ground truth.

    \item We propose \emph{distractor erasure tuning} (\name), a novel method that adapts black-box models to make them robust to distractor erasure.
    
    \item We perform experiments on semi-synthetic and real-world computer vision datasets and show that \name~ outputs models that are faithful to its original input models, and that their feature attributions are interpretable and match ground-truth feature attributions.
\end{enumerate}