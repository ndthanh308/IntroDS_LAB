

In this section, we present our empirical evaluation in detail. We consider various quantitative and qualitative metrics to evaluate the correctness of feature attributions given by \name~ models as well as the faithfulness of \name~ models to the models they are meant to explain. We also evaluate \name~ models' ability to explain models manipulated to have arbitrary uninformative input gradients. Finally, we analyze the effect of the mask downscaling hyperparameter on attributions. Comparisons to additional baselines beyond those shown in Figure \ref{fig:qual_examples} are given in \ref{shapbcos}. 

\paragraph{Datasets.}\label{datasets}
\textbf{Hard MNIST:} The first is a harder variant of MNIST where the digit is randomly placed on a small subpatch of a colored background. Each sample also contains small distractor digits and random noise patches. For this dataset, we consider the signal to be all pixels contained within the large actual digit, and the distractor to be all pixels in the background, noise, and smaller digits. 
\textbf{Chest X-ray:} Second, we consider a semi-synthetic chest x-ray dataset for pneumonia classification \cite{kermany2018labeled}. To control exactly what information the model leverages such that we can create ground truth signal-distractor decompositions, we inject a spurious correlation into this dataset. We randomly place a small, barely perceptible noise patch on each image in the ``normal" class. We confirm that the model relies only on the spurious signal during classification by testing the model's drop in performance when flipping the correlation (adding the noise patches to the ``pneumonia'' class) and seeing that the accuracy goes from 100\% to 0\%. As such, for this dataset, the signal is simply the noise patch and the distractor is the rest of the xray. 
\textbf{CelebA:} The last dataset is a subset of CelebA \cite{liu2018large} for hair color classification with classes \{dark hair, blonde hair, gray hair\}. We correlate the dark hair class with glasses to allow for qualitative evaluation of each methods' ability to recover spurious correlations. This dataset does not have a ground truth signal distractor decomposition, as there are many unknown discriminative spurious correlations the model may rely upon. 




% Figure environment removed

\paragraph{Models.}
For all experiments, we use ImageNet pre-trained ResNet18s for our baseline models, $f_b$. All models achieve over 96\% test accuracy. We train \name~ models with $Q \sim \mathbbm{1}_{d*d}*\mathcal{N}(\mu(D_d), \sigma^2(D_d))$, meaning that each image is masked with a uniform color drawn from a normal distribution around the mean color of the dataset. For all evaluation, we use $Q \sim \mathbbm{1}_{d*d}*\mathcal{N}(\mu(D_d), 0)$ (the dirac delta of the dataset mean) to ensure that masked samples are minimally out-of-distribution for the baseline models $f_b$. 


\subsection{Evaluating the Correctness of Feature Attributions}\label{correctness}


\paragraph*{Pixel Perturbation Tests.}
We test the faithfulness of our explanations via the pixel perturbation variant proposed in \cite{srinivas2019full, hooker2019benchmark}, where we mask the $k$ least salient pixels as determined by any given attribution method and check for degradation in model performance with the mean of the dataset. This metric evaluates whether the $k$ masked pixels were necessary for the model to make its prediction. As mentioned in previous works, masked samples come from a different distribution than the original samples, meaning poor performance after pixel perturbation can either be a product of the model's performance on the masks or the feature attribution scores being incorrect. To disentangle the correctness of the attributions from the robustness of the model, we perform pixel perturbation tests on ground truth feature attributions, with results reported in the appendix. Note that our method returns binary masks, but this metric requires continuous valued attributions to create rankings. As such, for this experiment we use the attributions created by our method \textit{before rounding}.

Results are shown in Figure \ref{fig:reg_perturb}. We find that the attributions produced by \name, used in conjunction with \name~models outperform all baselines. Furthermore, \name~attributions tested on the baseline model also generally perform better than gradient-based attributions. 




\paragraph*{Intersection Over Union.} 
We further evaluate the correctness of our attributions by measuring their Intersection Over Union (IOU) with the ``ground truth'' attributions. We use the signal from the ground truth signal-distractor decomposition as described in \ref{datasets} for the ground truth attributions. For each image, if the ground truth attribution is comprised of $n$ pixels, we take the intersection over union of the top $n$ pixels returned by the explanation method and the $n$ ground truth pixels, meaning an IOU of 1 corresponds to a perfectly aligned/correct attribution. Results are shown in \ref{iou-table}, where our method performs the best for both datasets. We report mean and standard deviation over the dataset for each method. 




% Figure environment removed

\subsection{Evaluating the Faithfulness of \name~ Models}\label{model_faith}

To ensure that the \name~ model ($f_v$) returned by our method is faithful the the original model ($f_b$) it approximates, we test the accuracy of \name~ models with respect to the predictions produced by the original model. Specifically, we take the predictions of the original model $f_b$ to be the labels of the dataset. We evaluate \name~ models on both the original dataset ($f_v(D_d) \approx f_b(D_d)$) and the simplified dataset ($f_v(D_s) \approx f_b(D_d)$), with results shown in \ref{model-faithfulness}. We see that \name models are highly faithful to the baseline models they approximate.


\vspace*{-0.4cm}

\begin{table}[htbp]
    \centering
\small
    \begin{minipage}{0.49\textwidth}
    \centering

    \caption{Intersection Over Union Results}
\label{iou-table}
\begin{tblr}{
  cell{1}{2} = {c},
  cell{1}{3} = {c},
  hline{1-2,7} = {-}{},
}
 & Hard MNIST & Chest X-ray\\
\name~ (Ours) & \textbf{0.461 $ \pm0.08$} & \textbf{0.821$ \pmÂ 0.05$}\\
SmoothGrad & 0.252$\pm 0.05$ & 0.045 $\pm0.05$\\
GradCAM & 0.295 $\pm0.09$ & 0.000 $\pm0.00$\\
Input Grad & 0.117 $\pm0.05$ & 0.017$\pm 0.03$\\
FullGrad & 0.389 $\pm0.10$ & 0.528 $\pm0.12$
\end{tblr}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \caption{Faithfulness of \name~ to Original Model}
\label{model-faithfulness}
\begin{tblr}{
  cell{1}{2} = {c},
  cell{1}{3} = {c},
  hline{1-2,4} = {-}{},
}
 & {Hard\\MNIST} & {Chest\\X-ray} & CelebA\\
{Faithfulness on \\Original Data} & 0.996 & 1.00 & 0.995\\
{Faithfulness on \\Simplified Data} & 0.987 & 1.00 & 0.975
\end{tblr}
\end{minipage}

\end{table}


\subsection{Qualitative Analysis of Simplified Datasets}

We first explore how well \name~ recovers signal-distractor decompositions. 
For CelebA, we see that all methods except for input gradients correctly recover the spurious correlation for the ``dark hair/glasses'' class, however only our method provides useful insights into the other two classes. We see that our method correctly identifies hair as the signal for the ``blonde hair'' class, whereas other methods simply look at the eyes, which are not discriminative. Furthermore, we see that for the ``gray hair'' class, our method picks up on hair, as well as initially unknown spurious correlations such as wrinkles and bowties. 
For Hard MNIST, we see that our method clearly isolates the signal and ignores the distractor. FullGrad and GradCAM suffer from a locality bias and tend to highlight the center of each digit. SmoothGrad and vanilla gradients are much noisier and highlight edges and many random background pixels. 
For the Chest X-ray dataset, we see that our method and FullGrad perfectly highlight the spurious signal. GradCAM again suffers from a centrality bias, and cannot highlight pixels on the edge. SmoothGrad and gradients appear mostly random to the human eye. 


We also consider the visual quality of our attributions compared with the baselines (examples are shown in \ref{fig:qual_examples}). We find that our method, FullGrad, and GradCAM appear the most interpretable, as opposed to SmoothGrad and vanilla gradients, because they consider features at the superpixel level rather than individual pixels. We also see that GradCam and FullGrad seem relatively class invariant, and tend to focus on the center of most images, rather than the discriminative features for each class, providing for less useful insights into the models and datasets. 





\input{sections/manipulated_explanations}

\subsection{Attribution Sensitivity to Hyperparamaters}\label{downscaling}
We conduct an ablation study on the choice of how much to downscale the mask by. The less we downscale by, the more fine-grained the mask is, allowing for optimization over a larger set of masks. However, the more we downscale by, the visually cohesive or ``interpretable'' to humans the masks are. We evaluate the trade-off between these two via pixel perturbation tests over multiple downscaling factors and with qualitative evaluations of the final masks in \ref{fig:ups}. We see that a downscaling factor of 8 performs the best on pixel perturbation tests. Increased factors of downscaling impose a greater locality constraint that results in informative pixels being masked, as shown in the visualization. 


% Figure environment removed