\section{Theory of Discriminative Feature Attributions}\label{sec:theory}

\newtheorem{defn}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{corr}{Corollary}
\newtheorem*{assumption}{Assumption}
\newenvironment{hproof}{%
  \renewcommand{\proofname}{Proof Idea}\proof}{\endproof}

In this section, we provide a theoretical framework for feature attribution, including a well-defined ground truth. We start by identifying a common feature of feature attributions, their reliance on perturbations or erasure. Intuitively, feature attribution methods work by simulating removing certain features and estimating how the model behaves when those features are removed: removing unimportant features should not change model behaviour. Typically, this erasure is implemented by replacing features with scalar values, such as the mean of the dataset \cite{zeiler2014visualizing, fong2017interpretable}. However, this can result in out-of-distribution inputs that can confuse a classifier, thus making it difficult to create meaningful attributions. 

To ground this argument in an example, consider a model that classifies cows and camels. For an image of a camel, a feature attribution might note that only the hump of the camel and the sand it stands on are important for classification.  As such, we would expect that the sky was irrelevant to the classifier's prediction, and we can concretely test this by altering it and creating a counterfactual sample. For example, we could mask the sky with an arbitrary uniform color; however, this may result in the sample being out-of-distribution for the model, and its prediction may change drastically even if the sky was not important for prediction. There are two strategies to overcome this problem. The first solution involves masking the sky in a manner that preserves the naturalness of the image, but this solution involves using large-scale generative models, which themselves can contain biases and be uninterpretable. The second solution requires the classifier to be invariant to the erasure of the pixels corresponding to the sky, which is our solution in this paper. We formalize this argument below by defining an erasure-based feature attribution method called $(\epsilon, \Q)$-feature attribution.

\paragraph{Notation.}\label{notations} Throughout this paper, we shall assume the task of classification with inputs $\X \sim \mathcal{X}$ with $\X \in \R^d$ and $y \in [1,2,... C]$ with $C$-classes. We consider the class of deep neural networks $f: \R^d \rightarrow \triangle^{C}$ which map inputs $\X$ onto a $C$-class probability simplex. This paper considers binary feature attributions, which are represented as binary masks $\M \in \{0,1\}^d$, where $\M_i=1$ indicates an important feature and $\M_i=0$ indicates an unimportant feature.

\subsection{Feature Attributions with Input Erasure}\label{q}
We first define an erasure-based feature attribution such that the feature replacement method is explicit, and features are replaced with samples from a counterfactual distribution $\Q$. Particularly, we are interested in binary attributions (i.e., a feature is considered important or not) instead of real-valued ones. 

\begin{defn} \label{defn:QFA} ($\epsilon, \Q$)-feature attribution (QFA) is a binary mask $\M(f, \X, \Q)$ that relies on a model $f(\cdot)$, an instance $\X$, and a $d$-dimensional counterfactual distribution $\Q$, and is given by
\begin{align*}
    \M(f, \X, \Q) &= \arg\min_{\M'} \| \M' \|_0 ~~\text{such that}~~\E_{q \sim \mathcal{Q}} \| f(\X_s(\M',q)) - f(\X) \|_1 \leq \epsilon
\end{align*}
where $\X_{s}(\M, q) = \M \odot \X + (1 - \M) \odot q$
\end{defn}

Thus, an ($\epsilon, \Q$)-feature attribution (henceforth, \emph{QFA}) refers to the sparsest mask that can be applied to an image such that the model's output remains approximately unchanged. QFA depends on the feature replacement distribution $\Q$, where $\Q$ is independent of both $\X$ and $y$. This generalizes the commonly used heuristics of replacing unimportant features with the dataset mean, in which case $\Q$ is a Dirac delta distribution at the mean value. The choice of $\Q$ is indeed critical, as an incorrect choice can hurt our ability to recover the correct attributions due to the resulting inputs being out-of-distribution and the classifier being sensitive to such changes. Specifically, an incorrect $\Q$ can result in QFA being less sparse, as masking even a few features with the wrong $\Q$ would likely cause large deviations in the model's outputs. As a result, given a model, we must aim to find the $\Q$ that leads to the sparsest QFA masks. However, the problem of searching over $\Q$ is complex, as it requires searching over the space of all $d$-dimensional distributions, and furthermore, if the underlying model is non-robust, there may not exist any $\Q$ that leads to sparse attributions. To avoid this, we consider the inverse problem: given $\Q$, we find the class of models that have the sparsest QFAs w.r.t. that particular $\Q$. We call this the $\Q$-robust model class, which we define below: 

\begin{defn} \label{defn:QVIM}
    $\Q$-robust model class $\mathcal{F}_v(\Q)$: For some given distribution $\Q$, the class of models $\mathcal{F}_v$ for which $\Q$ has the sparsest QFA mask as opposed to any other $\Q'$, such that for all $f \in \mathcal{F}_v(\Q)$, 
    \begin{align*}
        \Q = \arg \min_{\Q'} \E_{\X} \| \M(f, \X, \Q') \|_0
    \end{align*}
\end{defn}

Intuitively, $\Q$-robust models result in the sparsest QFA masks and can be thought of as being robust to the erasure of "irrelevant" input features. Recalling our example of the cows and camels, we would like models to be robust to the replacement of the pixels corresponding to the sky but not necessarily robust to pixels corresponding to the camel or the cow itself. This distinguishes it from classical robustness definitions, which require models to be robust to small perturbations (rather than erasure) at every feature uniformly. Thus $\Q$-robustness is equivalent to enforcing robustness to the erasure of distractor features, a notion that is central to this work. For the rest of this paper, we shall refer to QFA applied to a model from a $\Q$-robust model class as a "matched" feature attribution -- the same $\Q$ is used to both define the model class and the feature attribution.

\subsection{Recovering the Signal-Distractor Decomposition}

In the study of feature attribution, the `ground truth' attributions are often unspecified. Here, we show that for datasets that are signal-distractor decomposable, formally defined below, there exists a ground truth attribution, and feature attributions for optimal verifiable models are able to recover it. Intuitively, given an object classification dataset between cows and camels, the "signal" refers to the regions in the image that are discriminative, or correlated with the label, such as the cows or camels. The distractor refers to everything else, such as the background or sky. Note that if objects in the background are spuriously correlated with the label, i.e. sand or grass, those would be part of the signal, not the distractor. We first begin by formally defining the signal-distractor decomposition. 

%\begin{defn}
%    A labelled dataset $D = \{(\X, y)_{i=1}^{N}\}$ is said to be signal-distractor decomposable if its generative process is given by: 
%    \begin{align*}
%        \mathbf{s} \sim &\mathcal{X}_\text{signal}(y)~~;~~ \mathbf{d} \sim \mathcal{X}_\text{distractor}~~;~~\M \sim \mathcal{M}\\
%        &\X = \mathbf{s} \odot \M + \mathbf{d} \odot (1 - \M)
%    \end{align*}
%    where the mask $\M \in \{0,1\}^d$ and the distractor $\mathbf{d} \in \R^d$ are independent of the label $y$, and the signal $\mathbf{s} \in \R^d$ depends on $y$, with the following conditions met: (1) $p(y \mid \X) = p(y \mid \mathbf{s} \odot \M)$, and (2) for any other mask $\M \not \subset \M'$, we have $p(y \mid \mathbf{s} \odot \M') < p(y \mid \mathbf{s} \odot \M)$ 
%\end{defn}

\begin{defn}
    A labelled dataset $D = \{(\X, y)_{i=1}^{N}\}$ has a signal-distractor decomposition defined by masks $\M(\X) \in \{0,1\}^d$ for every input $\X$, where 
    \begin{enumerate}
        \item $\X \odot \M(\X)$ is the discriminative \underline{signal}, where $p(y \mid \X) = p(y \mid \X \odot \M(\X))$
        \item $\X \odot (1 - \M(\X))$ is the non-discriminative \underline{distractor}, where $p(y \mid \X \odot (1 - \M(\X))) = p(y)$
        \item $\M(\X)$ is the sparsest mask, i.e., $\M(\X) = \arg\min_{\M'(\X)} \| \M'(\X) \|_0$, such that (1) and (2) are satisfied.
    \end{enumerate} 
\end{defn}

We propose to use the masks $\M(\X)$ implied by the signal-distractor decomposition as ground truth feature attributions. These are meaningful as they precisely highlight the discriminative components of the image and ignore the non-discriminative regions. Discriminability has previously been considered an important criterion to evaluate feature attributions \cite{shah2021input, hooker2019benchmark} however, we here take a step further and propose its usage as ground truth. 

We observe first that the masks $\M(\X)$ of the signal-distractor decomposition always exist: setting $\M(\X)$ as an all-ones vector trivially satisfies conditions (1) and (2). When multiple masks exist, condition (3) requires us to choose the sparsest such mask $\M(\X)$. Using the definitions provided, we show below an asymptotic argument stating that $Q$-robustness is a necessary condition to recover the optimal masks defined by the signal-distractor decomposition. 

\textbf{Remark}: A dataset $\mathcal{D}$ is said to have a ``non-redundant signal'' when the sparsest mask in condition (3) of the signal-distractor decomposition is equal to the sparsest mask when (1) alone is satisfied.

\begin{theorem}
    QFA applied to $\Q$-robust models recover the ground-truth masks when applied to the Bayes optimal predictor $f_v^* \in \mathcal{F}_v(\Q)$, for datasets $\mathcal{D}$ with a non-redundant signal.
\end{theorem}

\begin{hproof}

We first note that the optimal $\Q$ for QFA is equal to the ground truth distractor distribution, as this leads to the sparsest QFA. If a $\Q$-robust model aims to recover the sparsest masks, then its QFA mask must equal that obtained by setting $\Q$ equal to the distractor. From the uniqueness argument in the definition of the signal-distractor decomposition, this is possible only when the optimal mask is recovered by QFA.
\end{hproof}

\begin{corr}
    QFA fails to recover the ground-truth masks when applied to predictors $f \not\in \mathcal{F}_v(\Q)$.
\end{corr}

This follows from the fact that for any $f \not\in \mathcal{F}_v(\Q)$, there exists some other $\Q'$ that results in a sparser mask, indicating that the ground truth masks are not recovered. Thus, this shows that feature attributions applied to the incorrect model class can be less effective - in this case, they fail to recover the ground truth masks. Further, the Bayes optimality is an important condition because it ensures that the resulting model is sensitive to all discriminative features in the input -- sub-optimal models are sub-optimal precisely because they fail to capture the signal from all the discriminative components of the input, and this can interfere with such models being able to recover ground truth masks. In practice, if we expect our models to be highly performant, we can expect them to be sensitive to all discriminative parts of the input and thus approximately recover ground truth masks. Finally, in practice, we do not have access to the ground truth masks for natural datasets, as the discriminative and the non-discriminative regions are not known in advance. In order to use these notions of ground truth, it is thus vital to construct semi-synthetic datasets where the discriminative parts are known. Thus, one can use semi-synthetic datasets to validate a feature attribution approach and then apply it to gain insight into real datasets with unknown signal and distractor components. 

To summarize, we have defined a feature attribution method with the feature removal process made explicit via the counterfactual distribution $\Q$. To minimize the sparsity of the attribution masks with a given $\Q$, we use models from the $\Q$-robust model class $\mathcal{F}_v(\Q)$. Finally, we find that feature attributions derived from Bayes optimal models in the model class $\mathcal{F}_v(\Q)$ are able to recover the ground-truth masks and fail to do so otherwise.