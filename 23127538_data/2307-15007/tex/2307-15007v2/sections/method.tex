\section{\name: Distractor Erasure Tuning}
In the previous section we showed that given a $\Q$-robust model $f_v \in \mathcal{F}_v(\Q)$, we are able to apply QFA to recover the ground truth masks. In this section, we shall discuss how to practically build such robust models, given a pre-defined counterfactual distribution $\Q$ that defines the erasure method.

\textbf{Relaxing QFA.} We note that QFA as defined in definition \ref{defn:QFA} is difficult to optimize in its current form due to its use of $\ell_0$ regularization and its constrained form. To alleviate this problem, we perform two relaxations: first, we relax the $\ell_0$ objective into an $\ell_1$ objective, and second, we convert the constrained objective to an unconstrained one by using a Lagrangian. The resulting objective function is given in equation \ref{l_explain}. Assuming the model $f_v$ is known to us, we can minimize this objective function to obtain ($\epsilon, \Q$)-feature attributions for each point $\X \in \mathcal{X}$.

\begin{align}
    \mathcal{L}_\text{QFA}(\theta, \{\M(\X)\}_{\X \in \mathcal{X}}) = \E_{\X \in \mathcal{X}} \left[ \underbrace{\| \M(\X) \|_1}_\text{mask sparsity} 
            + \lambda_1 \underbrace{\| f_v(\X; \theta) - f_v (\X_\text{s}(\M, q)); \theta) \|_1}_\text{data distillation} \right] \label{l_explain}
\end{align}

\textbf{Enforcing Model Robustness via Distillation.} Assuming that the optimal masks denoting the signal-distractor decomposition are known w.r.t. every training data point (i.e., $\{\M(\X)\}_{\X \in \mathcal{X}}$), one can project any black-box model into a $\Q$-robust model via distillation. Specifically, we can use equation \ref{l_train} for this purpose, which contains (1) a data distillation term to enforce the $\epsilon$ constraint in QFA, and (2) a model distillation term to enforce that the resulting model and original model are approximately equal. Accordingly, the black-box model $f_b$ and our resulting model $f_v$ both have the same model architecture, and we initialize $f_v = f_b$. 

\begin{gather}
\mathcal{L}_\text{train}(\theta, \{\M(\X)\}_{\X \in \mathcal{X}} ) = 
            \E_{\X \in \mathcal{X}} \left[\underbrace{\| f_v(\X; \theta) - f_v (\X_\text{s}(\M(\X), q)); \theta) \|_1}_\text{data distillation}
            + \lambda_2 \underbrace{\| f_b (\X) - f_v (\X; \theta) \|_1}_\text{model distillation} \right] \label{l_train}
\end{gather}

\textbf{Alternating Minimization between $\theta$ and $\M$.} We are interested in both of the above objectives: we would like to recover the optimal masks from the dataset, as well as use those masks to enforce ($\epsilon, \Q$) constraints via distillation to yield our $\Q$-robust models. We can thus formulate the overall optimization problem as the sum of these terms, as shown in equation \ref{eqn:opt_prob}. Notice that both these objectives assume that either the optimal masks, or the robust model is known, and in practice, we know neither. A common strategy in cases that involve optimizing over multiple sets of variables is to employ alternating minimization \cite{JainK17}, which involves repeatedly fixing one of the variables and optimizing the other. We handle the constrained objective on the mask variables via projection, i.e., using hard-thresholding / rounding to yield binary masks. 

\begin{gather}
    \theta^*, \{\M^*(\X)\} = \arg \min_{\theta, \M} \left( \mathcal{L}_\text{train}(\theta, \{\M(\X)\}) + \mathcal{L}_\text{QFA}(\theta, \{\M(\X)\}) \right) \label{eqn:opt_prob}\\
    \text{such that} \quad\quad \M(\X) \in \{0,1\}^d \quad \forall \X \in \mathcal{X} \nonumber
\end{gather}

\textbf{Iterative Mask Rounding with Increasing Sparsity.} In practice, mask rounding makes gradient-based optimization unstable due to sudden jumps in the variables induced by rounding. This problem commonly arises when dealing with sparsity constraints. To alleviate this problem, use a heuristic that is common in the model pruning literature \cite{han2015learning} called iterative pruning, which involves introducing a rounding schedule, where the sparsity of the mask is gradually increased during optimization steps. Inspired by this choice, we employ a similar strategy over our mask variables.

\textbf{Practical Details.} We implement these objectives as follows. First, we initialize the robust model to be the same as the original model, $f_v = f_b$, and the mask to be all ones, $D_s = m \odot D_d, m = 1$. 
We then iteratively (1) simplify $D_s$ by optimizing $\mathcal{L}_{QFA}$ until $\M$ converges, (2) round $\M$ such that it is binary (i.e. $D_s$ is a subset of features in $D_d$ rather than a weighting of them), and (3) update $f_v$ by minimizing $\mathcal{L}_{train}$ such that $D_s$ is equally as informative as $D_d$ to $f_v$ and $f_v$ is functionally equivalent to $f_b$. As per Definition \ref{defn:QVIM}, we replace masked pixels in $D_s$ with a pre-determined counterfactual distribution $\Q$. This ensures that the given $\Q$ is the optimal counterfactual distribution for $f_v$, meaning $f_v$ comes from the $\Q$-robust model class $\mathcal{F}_v(\Q)$. The pseudocode is given in Algorithm \ref{alg:cap}.

\begin{algorithm}
\caption{Distractor Erasure Tuning}\label{alg:cap}
\begin{algorithmic}
\State \textbf{Input:} Dataset $D_d := (x, y)$, model $f_b$, hyperparameter $k$ rounding steps 
\State \textbf{Hyperparameters:} $k$ rounding steps, $u$ mask scaling factor, $s(t)$ sparsity at step $t$

\State $\{\M(\X)\}, ~~\text{s.t.}~~ \M(\X) \gets $ ones with shape $\R^{d/u}$ \Comment{Init mask $\M(\X)$ to ones}
\State $f_v \gets f_b$ \Comment{Init robust model $f_v$ to $f_b$}
%\State $s({t}) \gets r_{threshold}$ \Comment{Init rounding threshold}
% \suraj{we also increase sparsity in each round, mention that here}
\For{$k$ rounding steps}
    \While{$\mathcal{L}_\text{QFA}$ not converged}
        \State $\{\M(\X)\} \gets \{\M(\X)\} + \nabla_{\M} \mathcal{L}_\text{QFA}$
    \EndWhile

    \State $\M \gets$ round($\M$, $s(t)$) ~~~$\forall~ \M \in \{ \M(\X) \}$

    \While{$\mathcal{L}_\text{train}$ not converged}
        \State $f_v \gets f_v + \nabla_{\theta} \mathcal{L}_\text{train}$
    \EndWhile
    %\State $r_{t} ++$ \Comment{Increase rounding threshold to increase mask sparsity}
    
\EndFor
\State \textbf{return} $\{\M(\X)\}, f_v$
\end{algorithmic}
\end{algorithm}



\textbf{Mask Scale.} In order to encourage greater ``human interpretability,'' we increase the pixel size of the masks by lowering their resolution. We do this by downscaling the masks before optimization. Concretely, we initialize the masks to be of size $m_d = x_d / u$, where $x_d$ is the dimension of the image $\X$ and $u$ is the pixel size we wish to consider. We then upsample the mask to be of dimension $x_d$ before applying it to $\X$. The more we downscale the mask by (i.e. the greater $u$ is), the more interpretable and visually cohesive the distilled dataset $\X_s$ is. 
% However, more downscaling also results in more constrained optimization, resulting in a less optimal $\X_s$. We explore this tradeoff empirically in \ref{downscaling}. 