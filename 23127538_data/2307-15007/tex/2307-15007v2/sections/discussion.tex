In this paper, we seek to build machine learning models such that their feature attributions remain discriminative. In particular, we propose \name, a method that adapts black-box models into those that are robust to distractor erasure. We empirically evaluate \name~ and find that the resulting models are highly faithful to the original and produce interpretable attributions that closely match the ground truth ones. Limitations of \name~include requiring full access to the training dataset and the baseline model. Furthermore, while it produces verifiable feature attributions that tell us how important each feature is to the model's prediction, it does not tell us what the relationship between important features and the output/label is, as is true with all feature attributions. %Finally, the utility of these attributions, and indeed, all feature attributions, relies on the existence of a non-trivial signal distractor decomposition of the dataset. If such a decomposition does not meaningfully exist, for example when the entire input is the signal, then ours, and all other feature attributions, are inapplicable. 