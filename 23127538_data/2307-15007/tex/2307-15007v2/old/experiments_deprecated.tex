% \subsection{Datasets and Models}
% We evaluate our method on three different datasets. 
\paragraph{Datasets.}
\textbf{Hard MNIST:} The first is a harder variant of MNIST where the digit is randomly placed on a small subpatch of a colored background. Each sample also contains small distractor digits and random noise patches. For this dataset, we consider the signal to be all pixels contained within the large actual digit, and the distractor to be all pixels in the background, noise, and smaller digits. 
\textbf{Chest X-ray:} Second, we consider a semi-synthetic chest x-ray dataset for pneumonia classification \cite{}. To control exactly what information the model leverages such that we can create groundtruth signal-distractor decompositions, we inject a spurious correlation into this dataset. We randomly place a small, barely perceptible noise patch on each image in the ``normal" class. We confirm that the model relies only on the spurious signal during classification by testing the model's drop in performance when flipping the correlation (adding the noise patches to the ``pneumonia'' class) and seeing that the accuracy goes from 100\% to 0\%. As such, for this dataset, the signal is simply the noise patch and the distractor is the rest of the xray. 
\textbf{CelebA:} The last dataset is a subset of CelebA \cite{liu2018large} for hair color classification with classes \{dark hair, blonde hair, gray hair\}. We correlate the dark hair class with glasses to allow for qualitative evaluation of each methods' ability to recover spurious correlations. This dataset does not have a groundtruth signal distractor decomposition, as there are many unknown discriminative spurious correlations the model may rely upon. 


% \paragraph{Benchmarks.}
% We benchmark our method against existing saliency-based methods including input gradients \cite{baehrens2010explain}, SmoothGrad \cite{smilkov2017smoothgrad}, GradCAM \cite{selvaraju2017grad}, and FullGrad \cite{srinivas2019full}. Examples of all datasets and all benchmarking methods are shown in \ref{fig:qual_examples}. 

\paragraph{Models.}
For all experiments, we use ImageNet pre-trained ResNets for our baseline models, $f_b$. All models achieve over 96\% test accuracy. We train our verifiable models with $Q \sim \mathbbm{1}_{d*d}*\mathcal{N}(\mu(D_d), \sigma^2(D_d))$, meaning that each image is masked with a uniform color drawn from a normal distribution around the mean color of the dataset. For all evaluation, we use $Q \sim \mathbbm{1}_{d*d}*\mathcal{N}(\mu(D_d), 0)$, meaning all samples are masked with a constant gray color, to ensure that masked samples are minimally out-of-distribution for the baseline models $f_b$. 

% Figure environment removed


\subsection{Correctness/Verifiability of Attributions}\label{correctness}

We evaluate the verifiability of our explanations with two different metrics: (1) a pixel perturbation test and (2) IOU recovery of the groundtruth signal. 


\paragraph*{Pixel Perturbation Tests.}
We test the verifiability of our explanations via the pixel perturbation variant proposed in \cite{srinivas2019full}, where we mask the $k$ least salient pixels as determined by any given attribution method and check for degradation in model performance. This metric evaluates whether the $k$ masked pixels were necessary for the model to make its prediction. As mentioned in previous works, masked samples come from a different distribution than the original samples, meaning poor performance after pixel perturbation can either be a product of the model functioning incorrectly on the new masked distribution or the feature attribution scores being incorrect. To disentangle the correctness of the attributions from the verifiability of the model, we also perform pixel perturbation tests on groundtruth feature attributions in \ref{model_eval}. For all experiments,  we choose to mask images with the mean color of the dataset (a uniform gray color) to prevent the perturbed samples from being extremely out-of-distribution. Note that our method returns binary masks, but this metric requires continuous valued attributions to create rankings. As such, for this experiment we use the attributions created by our method \textit{before rounding}.

Results are shown in Figure \ref{fig:reg_perturb}. We find that the attributions produced by our method, used in conjunction with our verifiable model, perform the best when compared to all other baselines. Furthermore, our attributions tested on the baseline model also generally perform better than gradient-based attributions. 


% Figure environment removed


% % Figure environment removed




\paragraph*{Intersection Over Union.} 
We further evaluate the correctness of our attributions by measuring the Intersection Over Union (IOU) of the generated attributions and the ``groundtruth'' attributions. For the Hard MNIST dataset, the groundtruth attribution for each image corresponds to the pixels that lie within the signal, or the main digit. For the chest x-ray dataset, the groundtruth is just the noise patch for the correlated class. Given that we do not have a verifiable signal-distractor decomposition for CelebA, we can only construct groundtruth attributions for the other two datasets. For each image, if the groundtruth attribution is comprised of $n$ pixels, we take the intersection over union of the top $n$ pixels returned by the explanation method and the $n$ groundtruth pixels, meaning IOU is on a scale of $[0, 1]$ where 1 corresponds to a perfectly aligned/correct attribution. Results are shown in \ref{iou-table}, where our method performs the best for both datasets. We report mean and standard deviation over the dataset for each method. 

\begin{table}
\centering
\caption{Intersection Over Union Results}
\label{iou-table}
\begin{tblr}{
  cell{1}{3} = {c},
  cell{1}{4} = {c},
  cell{2}{1} = {r=5}{},
  hline{1-2} = {-}{},
  hline{7} = {2-4}{},
}
 &  & {Hard MNIST} & {Chest X-ray}\\
\begin{sideways}Method\end{sideways} & VerT (Ours) & \textbf{0.461 $ \pm0.08$} & \textbf{0.821$ \pm 0.05$}\\
 & SmoothGrad & 0.252$\pm 0.05$ & 0.045 $\pm0.05$\\
 & GradCAM & 0.295 $\pm0.09$ & 0.000 $\pm0.00$\\
 & Input Grad & 0.117 $\pm0.05$ & 0.017$\pm 0.03$\\
 & FullGrad & 0.389 $\pm0.10$ & 0.528 $\pm0.12$
\end{tblr}
\end{table}

% \begin{table}
% \centering
% \caption{Intersection Over Union Results}
% \label{iou-table}
% \begin{tblr}{
%   cell{1}{3} = {c},
%   cell{1}{4} = {c},
%   cell{1}{5} = {c},
%   cell{1}{6} = {c},
%   cell{2}{1} = {r=5}{},
%   hline{1-2} = {-}{},
%   hline{7} = {2-6}{},
% }
%  &  & {MNIST \\(standard)} & {MNIST \\(manipulated)} & {Chest X-ray\\(standard)} & {Chest X-ray\\(manipulated)}\\
% \begin{sideways}Method\end{sideways} & Ours & \textbf{0.461 $ \pm0.08$} & \textbf{0.454 $\pm0.08$} & \textbf{0.821$ \pm 0.05$} & \textbf{0.631$\pm 0.12$}\\
%  & SmoothGrad & 0.252$\pm 0.05$ & 0.158 $\pm0.07$ & 0.045 $\pm0.05$ & 0.000 $\pm0.00$\\
%  & GradCAM & 0.295 $\pm0.09$ & 0.040$\pm0.06$ & 0.000 $\pm0.00$ & 0.000 $\pm0.00$\\
%  & Input Grad & 0.117 $\pm0.05$ & 0.002$\pm0.01$ & 0.017$\pm 0.03$ & 0.000 $\pm0.00$\\
%  & FullGrad & 0.389 $\pm0.10$ & 0.333 $\pm0.12$ & 0.528 $\pm0.12$ & 0.004 $\pm0.04$
% \end{tblr}
% \end{table}



\subsection{Evaluation of Verifiable Models}\label{model_eval}
We evaluate our verifiable models on (1) their faithfulness to the original model and dataset and (2) their verifiability or performance on groundtruth attributions.

\paragraph{Faithfulness of Verifiable Models.} 
To ensure that the verifiable model ($f_v$) returned by our method is faithful the the original model ($f_b$) it approximates, we test the accuracy of the verifiable model with respect to the predictions produced by the original model. Specifically, we take the predictions of the original model $f_b$ to be the labels of the dataset. We evaluate our model on both the original dataset ($f_v(D_d) \approx f_b(D_d)$) and the simplified dataset ($f_v(D_s) \approx f_b(D_d)$), with results shown in \ref{model-faithfulness}. We see that the verifiable models are highly faithful to the baseline models they approximate.

% \begin{table}
% \caption{Faithfulness of Verifiable Model to Original Model}
% \label{model-faithfulness}
% \begin{tblr}{
%   cell{1}{2} = {c},
%   cell{1}{3} = {c},
%   hline{1-2,4} = {-}{},
% }
%  & {Hard\\MNIST} & {Chest\\X-ray} & CelebA\\
% {Accuracy on \\Original Data\\$f_v(D_d) = f_b(D_d)$} & 0.996 & 1.00 & 0.995\\
% {Accuracy on \\Simplified Data~\\$f_v(D_s) = f_b(D_d)$} & 0.987 & 1.00 & 0.975
% \end{tblr}
% \end{table}


\begin{table}[htbp]
    \centering
    \begin{minipage}{0.45\textwidth}
        \caption{Faithfulness of Verifiable Model to Original Model}
\label{model-faithfulness}
\begin{tblr}{
  cell{1}{2} = {c},
  cell{1}{3} = {c},
  hline{1-2,4} = {-}{},
}
 & {Hard\\MNIST} & {Chest\\X-ray} & CelebA\\
{Faithfulness on \\Original Data} & 0.996 & 1.00 & 0.995\\
{Faithfulness on \\Simplified Data} & 0.987 & 1.00 & 0.975
\end{tblr}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \caption{Model Verifiability}
\label{model-verifiability}
\begin{tblr}{
  hline{1-2,5} = {-}{},
}
 & {Hard \\MNIST} & {Chest\\ X-ray}\\
VerT Model ($f_v)$ & \textbf{0.027} & \textbf{0.0009}\\
Original Model ($f_b)$ & 0.107 & 0.0032\\
$f_b$ + Input Dropout & 0.167 & 0.0536\\
\end{tblr}
    \end{minipage}
\end{table}
% \begin{table}
% \centering
% \caption{Faithfulness of Verifiable Model to Original Model}
% \label{model-faithfulness}
% \begin{tblr}{
%   cell{1}{2} = {c},
%   cell{1}{3} = {c},
%   cell{1}{5} = {c},
%   cell{1}{6} = {c},
%   cell{1}{7} = {c},
%   hline{1-2,4} = {-}{},
% }
%  & {Hard\\MNIST} & {Chest\\X-ray} & CelebA & {Hard MNIST\\(manipulated)} & {Chest X-ray\\(manipulated)} & {CelebA\\(manipulated)}\\
% {Accuracy on \\Original Data\\$f_v(D_d) = f_b(D_d)$} & 0.996 & 1.00 & 0.995 & 0.990 & 1.00 & 0.970\\
% {Accuracy on \\Simplified Data~\\$f_v(D_s) = f_b(D_d)$} & 0.987 & 1.00 & 0.975 & 0.980 & 1.00 & 0.946
% \end{tblr}
% \end{table}

\paragraph{Model Verifiability.} We further test the verifiability of our model by evaluating how the model's performance changes when performing the pixel perturbation test on groundtruth attributions. This enables us to disentangle the verifiability of the model from the correctness of the attributions, as we know that our attributions are correct. We use the same groundtruth attributions as in \ref{correctness}. 
We report the $\ell_1$ norm between predictions made on the original samples and predictions made on the masked samples. We compare our verifiable models to the baseline models they approximate, as well as models trained with input dropout, which \cite{jethani2021have} proposes as their verifiable model class. Training with input dropout is equivalent to training $f_v$ with random masks and cross-entropy loss rather than optimized masks and $f_b$ prediction matching. 
Results are shown in \ref{model-verifiability}, where we see that our model performs similarly for masked and normal samples, whereas the other models do not. 

% \begin{table}
% \centering
% \caption{Model Verifiability}
% \label{model-verifiability}
% \begin{tblr}{
%   hline{1-2,5} = {-}{},
% }
%  & Hard MNIST & Chest X-ray\\
% Our Model ($f_v$) & \textbf{0.003} & \textbf{0.000}\\
% Model w/ Input Dropout & 0.016 & (did not converge)\\
% Original Model ($f_b)$ & 0.016 & 0.009
% \end{tblr}
% \end{table}


\subsection{Qualitative Analysis of Simplified Datasets}
We also qualitatively evaluate the explanations produced by our method to determine if they are human interpretable and can noticeably recover the spurious correlation. 

We first explore how well our method recovers the signal, including spurious correlations, in the dataset. 
For the CelebA dataset, we see that all methods except for input gradients correctly recover the spurious correlation for the ``dark hair/glasses'' class, however only our method provides useful insights into the other two classes. We see that our method correctly identifies hair as the signal for the ``blonde hair'' class, whereas other methods simply look at the eyes, which are not discriminative. Furthermore, we see that for the ``gray hair'' class, our method picks up on hair, as well as initially unknown and unintended spurious correlations such as wrinkles and bowties. 
For the Hard MNIST dataset, we see that our method clearly isolates the signal and ignores the distracto. FullGrad and GradCAM suffer from a locality bias, and tend to highlight the center of each digit rather than neatly outlining the entire shape. SmoothGrad and vanilla gradients are much noisier and highlight edges and many random background pixels. 
For the Chest X-ray dataset, we see that our method and FullGrad perfectly highlight the spurious signal. GradCAM again suffers from a centrality bias, and cannot highlight pixels on the edge. SmoothGrad and gradients appear mostly random to the human eye. 


We also consider the visual quality of our attributions compared with the baselines (examples are shown in \ref{fig:qual_examples}). We find that our method, FullGrad, and GradCAM appear the most interpretable, as opposed to SmoothGrad and vanilla gradients, because they consider features at the superpixel level rather than individual pixels. We also see that GradCam and FullGrad seem relatively class invariant, and tend to focus on the center of most images, rather than the discriminative features for each class, providing for less useful insights into the models and datasets. 





\input{manipulated_explanations.tex}



\subsection{Mask Downscaling Hyperparameter Ablation Study}\label{downscaling}
We conduct an ablation study on the choice of how much to downscale the mask by. The less we downscale by, the more fine-grained the mask is, allowing for optimization over a larger set of masks. The more we downscale by, the greater the locality bias is in the masks, making them more interpretable to humans. We evaluate the trade-off between these two via pixel perturbation tests over multiple downscaling factors and with qualitative evaluations of the final masks in \ref{fig:ups}. We see that a downscaling factor of 8 performs the best on pixel perturbation tests. Increased factors of downscaling impose a greater locality constraint that results in informative pixels being masked, as shown in the visualization. 


% Figure environment removed