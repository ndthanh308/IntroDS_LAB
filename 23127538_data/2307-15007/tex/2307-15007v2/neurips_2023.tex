\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{tabularray}
\usepackage{rotating}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\X}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\pb}{p_{bayes}}
\newcommand{\pth}{p_{\theta}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\M}{\mathbf{m}}
\newcommand{\Q}{\mathcal{Q}}

\newcommand{\suraj}[1]{{\color{cyan} Suraj: #1}}
\newcommand{\hima}[1]{{\color{magenta}[[hima: #1]]}}
\newcommand{\usha}[2]{{\color{red} Usha: #1}}

\newcommand{\name}{\texttt{DiET}}
% \title{Verifiable Feature Attribution: Bridging the Gaps between Post hoc Explanations and Inherently Interpretable Models}

%\title{Towards Feature Attributions that Highlight Discriminative Features}

\title{Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Usha Bhalla* \\
  % Department of Computer Science\\
  Harvard University\\
  \texttt{usha\_bhalla@g.harvard.edu} \\
  \And
  Suraj Srinivas* \\
  Harvard University \\
  \texttt{ssrinivas@seas.harvard.edu} \\
  \And
  Himabindu Lakkaraju \\
  Harvard University \\
  \texttt{hlakkaraju@hbs.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}


With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by identifying features critical to model predictions; however, prior work has shown that these explanations may not be faithful, in that they incorrectly attribute high importance to features that are unimportant or non-discriminative for the underlying task. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we identify a key reason for the lack of faithfulness of feature attributions: the lack of robustness of the underlying black-box models, especially to the erasure of unimportant distractor features in the input. To address this issue, we propose \emph{Distractor Erasure Tuning} (\name), a method that adapts black-box models to be robust to distractor erasure, thus providing discriminative and faithful attributions. This strategy naturally combines the ease of use of post hoc explanations with the faithfulness of inherently interpretable models. 
We perform extensive experiments on semi-synthetic and real-world datasets, and show that \name~produces models that (1) closely approximate the original black-box models they are intended to explain, and (2) yield explanations that match approximate ground truths available by construction. Our code is made public \href{https://github.com/AI4LIFE-GROUP/DiET}{here}.

\end{abstract}

\section{Introduction}

\input{sections/intro}

\section{Related Work}
\input{sections/related}

\input{sections/theory}

\input{sections/method}

\section{Experimental Evaluation}
\input{sections/experiments}

\section{Discussion}
\input{sections/discussion}


\begin{ack}
This work is supported in part by the NSF awards IIS-2008461, IIS-2040989, IIS-2238714, Kempner Institute Graduate Fellowship, and research awards from Google, JP Morgan, Amazon, Harvard Data Science Initiative, and the Digital, Data, and Design (D$^3$) Institute at Harvard. The views expressed here are those of the authors and do not reflect the official policy or position of the funding agencies.
\end{ack}


\newpage
\bibliographystyle{unsrtnat}
\bibliography{refs}



\newpage
\section*{Supplementary Material}

\input{supplementary.tex}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}