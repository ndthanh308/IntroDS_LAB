
\section{Proofs}

\begin{theorem}
    For datasets with signal-distractor decompositions, $\epsilon\Q$-feature attributions applied to $\epsilon\Q$ verifiable models, $f_v \in \mathcal{F}_v(\Q)$, \underline{recover the signal distribution} for the optimal predictor $f_v^*$.
\end{theorem}

\begin{proof}
    We first notice that the ideal counterfactual distribution $\Q_{opt}$ is exactly the distractor distribution, i.e., if we set $\Q_{opt} = \mathcal{X}_\text{distractor}$, we recover the inputs $\X$, if we know the ground truth mask $\M_\text{dataset}$, i.e, $\X \odot \M_\text{dataset} = \mathbf{s} \odot \M_\text{dataset}$, which implies that the "simplified" inputs equal the real inputs, i.e., $\X_s = \X \odot \M_\text{dataset} + \mathbf{q}_{opt} \odot (1 - \M_\text{dataset}) = \mathbf{s} \odot \M_\text{dataset} + \mathbf{d} \odot (1 - \M_\text{dataset}) = \X$. However we do not know the ground truth mask $\M_\text{dataset}$. 
    
    Given the "non-redundancy" constraint in the signal-distractor decomposition, it follows that the ground truth mask is the sparsest mask such that $p(y \mid \mathbf{s} \odot \M) = p(y \mid \X)$, or $f^*(\X) = f^*(\X_s)$, since $f^*$ is the optimal predictor. Given the uniqueness of the ground truth mask by definition, we recover $\M_\text{dataset} = \M_\text{QFA}$ where $\M_\text{QFA}$ is computed using $\Q_{opt}$.

    For a "non-ideal" fixed distractor $\Q$, and the corresponding verifiable model class $\mathcal{F}_v(\Q)$, we know that $\Q$ attains least sparsity compared to any other counterfactual. However, we know from the above argument that $\Q_{opt} = \mathcal{X}_\text{distractor}$ attains the least sparsity for any optimal predictor. Thus it follows that these must be equal. This implies that the mask recovered by $\M_\text{QFA} = \M_\text{dataset}$.
\end{proof}

We now present proof for an additional statement not described in the main text, where we connect QFA to other commonly used feature attributions via the local function approximation framework \cite{han2022explanation} as follows.

\begin{theorem}
 QFA is an instance of the local function approximation framework (LFA), with (1) random binary perturbations, and (2) an interpretable model class consisting of linear models with binary weights
\end{theorem}

\begin{proof}
Assume a black box model given by $f_b(\X; \M) = \mathbbm{1}\left(\E_q \| f(\X_s(\M,q)) - f(\X) \|_2 \leq \epsilon\right)$ ,loss function $\ell(f,g,x,\xi) = (f(x; \xi) - g(\xi))^2$, neighborhood perturbation $Z = \text{Uniform}(0,1)^d$, and an interpretable model family $\mathcal{G}$ being the class of binary linear models. 

For these choices, it is easy to see that 

\begin{align*}
    &\arg\min_{g \in \mathcal{G}} \ell(f,g,\X,\xi) \\
    =& \arg\min_{g \in \mathcal{G}} \E_{\xi} \left(f_b(\X; \xi) - g^\top\xi\right)^2 + \lambda \| g \|_0 \\
    =& \arg\min_{g \in \mathcal{G}} \E_\xi \left(\mathbbm{1}\left(\E_q \| f(\X_s(\xi,q)) - f(\X) \|_2 \leq \epsilon\right) - g^\top\xi\right)^2 + \lambda \| g \|_0\\
\end{align*}

This above objective is minimized when $g = \M^*$, i.e., the ideal $\epsilon\Q$-FA mask, because this sets the first term to be zero by definition, and the second sparsity term ensures the minimality of the mask.
\end{proof}




\section{Additional Results}

% \begin{table}[h!]
% \centering
% \caption{Pixel Perturbation Results}
% \label{pixel-perturbation-table}
% \begin{tblr}{
%   cell{1}{3} = {c=3}{c},
%   cell{1}{7} = {c=3}{c},
%   cell{1}{11} = {c=3}{c},
%   cell{2}{3} = {c},
%   cell{2}{4} = {c},
%   cell{2}{5} = {c},
%   cell{2}{7} = {c},
%   cell{2}{8} = {c},
%   cell{2}{9} = {c},
%   cell{3}{1} = {r=6}{},
%   hline{1-2} = {-}{},
%   hline{3} = {3-5,7-9,11-13}{},
%   hline{9} = {2-13}{},
% }
%  &  & Hard MNIST &  &  &  & CelebA &  &  &  & Chest X-ray &  & \\
%  & Sparsity & 0.10 & 0.05 & 0.02 &  & 0.2 & 0.1 & 0.01 &  & 0.01 & 0.005 & 0.001\\
% \begin{sideways}Method\end{sideways} & Ours, $f_v$ & \textbf{0.977} & \textbf{0.966} & \textbf{0.738} &  & \textbf{0.986} & \textbf{0.956} & 0.553 &  & 0.984 & \textbf{0.982} & \textbf{0.771}\\
%  & Ours, $f_b$ & 0.971 & 0.925 & 0.258 &  & 0.936 & 0.902 & \textbf{0.572} &  & 0.968 & 0.961 & 0.742\\
%  & SmoothGrad & 0.944 & 0.748 & 0.258 &  & 0.637 & 0.619 & 0.373 &  & 0.806 & 0.739 & 0.649\\
%  & GradCAM & 0.908 & 0.514 & 0.132 &  & 0.891 & 0.817 & 0.497 &  & 0.625 & 0.625 & 0.625\\
%  & Input Grad & 0.606 & 0.254 & 0.127 &  & 0.607 & 0.497 & 0.302 &  & 0.716 & 0.678 & 0.633\\
%  & FullGrad & 0.967 & 0.735 & 0.22 &  & 0.752 & 0.689 & 0.459 &  & \textbf{0.997} & 0.981 & 0.695
% \end{tblr}
% \end{table}


% \begin{table}[h!]
% \centering
% \caption{Pixel Perturbation Results (Manipulated Gradients)}
% \label{pixel-perturbation-manip-table}
% \begin{tblr}{
%   cell{1}{3} = {c=3}{c},
%   cell{1}{7} = {c=3}{c},
%   cell{1}{11} = {c=3}{c},
%   cell{2}{3} = {c},
%   cell{2}{4} = {c},
%   cell{2}{5} = {c},
%   cell{2}{7} = {c},
%   cell{2}{8} = {c},
%   cell{2}{9} = {c},
%   cell{3}{1} = {r=6}{},
%   hline{1-2,9} = {-}{},
%   hline{3} = {3-5,7-9,11-13}{},
% }
%  &  & Hard MNIST &  &  &  & CelebA &  &  &  & Chest X-ray &  & \\
%  & Sparsity & 0.10 & 0.05 & 0.02 &  & 0.2 & 0.1 & 0.01 &  & 0.01 & 0.005 & 0.001\\
% \begin{sideways}Method\end{sideways} & Ours, $f_v$ & \textbf{0.981} & \textbf{0.974} & \textbf{0.616} &  & \textbf{0.975} & \textbf{0.922} & \textbf{0.543} &  & \textbf{0.970} & \textbf{0.956} & \textbf{0.747}\\
%  & Ours, $f_b$ & 0.801 & 0.204 & 0.116 &  & 0.963 & 0.851 & 0.333 &  & 0.958 & 0.914 & 0.742\\
%  & SmoothGrad & 0.110 & 0.113 & 0.124 &  & 0.352 & 0.334 & 0.333 &  & 0.653 & 0.685 & 0.685\\
%  & GradCAM & 0.110 & 0.104 & 0.109 &  & 0.349 & 0.333 & 0.333 &  & 0.685 & 0.685 & 0.685\\
%  & Input Grad & 0.113 & 0.107 & 0.101 &  & 0.340 & 0.335 & 0.334 &  & 0.685 & 0.685 & 0.685\\
%  & FullGrad & 0.880 & 0.334 & 0.114 &  & 0.936 & 0.809 & 0.333 &  & 0.743 & 0.685 & 0.685
% \end{tblr}
% \end{table}


% \begin{table}[h!]
% \centering
% \caption{Upsampling Ablation Study}
% \label{upsample-table}
% \begin{tblr}{
%   cell{1}{3} = {c=4}{c},
%   cell{1}{8} = {c=4}{c},
%   cell{2}{3} = {c},
%   cell{2}{4} = {c},
%   cell{2}{5} = {c},
%   cell{2}{8} = {c},
%   cell{2}{9} = {c},
%   cell{2}{11} = {c},
%   cell{3}{1} = {r=4}{},
%   hline{1-2} = {-}{},
%   hline{3} = {3-6,8-11}{},
% }
%  &  & Hard MNIST &  &  &  &  & CelebA &  &  & \\
%  & Sparsity & 0.10 & 0.05 & 0.02 & 0.01 &  & 0.1 & 0.05 & 0.02 & 0.01\\
% \begin{sideways}Upsample\end{sideways} & 8 & \textbf{0.977} & \textbf{0.966} & \textbf{0.738} & \textbf{0.234} &  & \textbf{0.892} & \textbf{0.908} & \textbf{0.903} & \textbf{0.880}\\
%  & 16 & 0.989 & 0.908 & 0.285 & 0.125 &  & 0.876 & 0.811 & 0.627 & 0.481\\
%  & 32 & 0.978 & 0.799 & 0.189 & 0.102 &  & 0.956 & 0.846 & 0.651 & 0.553\\
%  & 56 & 0.910 & 0.501 & 0.129 & 0.088 &  & 0.875 & 0.702 & 0.548 & 0.432
% \end{tblr}
% \end{table}





\paragraph{Model Verifiability.} We further test the verifiability of our model by evaluating how the model's performance changes when performing the pixel perturbation test on groundtruth attributions. This enables us to disentangle the verifiability of the model from the correctness of the attributions, as we know that our attributions are correct. We use the same groundtruth attributions as in \ref{correctness}. 
We report the $\ell_1$ norm between predictions made on the original samples and predictions made on the masked samples. We compare our verifiable models to the baseline models they approximate, as well as models trained with input dropout, which \cite{jethani2021have} proposes as their verifiable model class. Training with input dropout is equivalent to training $f_v$ with random masks and cross-entropy loss rather than optimized masks and $f_b$ prediction matching. 
Results are shown in \ref{model-verifiability}, where we see that our model performs similarly for masked and normal samples, whereas the other models do not. 

\begin{table}[h!]
\centering
\begin{tblr}{
      hline{1-2,5} = {-}{},
    }
     & {Hard \\MNIST} & {Chest\\ X-ray}\\
    VerT Model ($f_v)$ & \textbf{0.027} & \textbf{0.0009}\\
    Original Model ($f_b)$ & 0.107 & 0.0032\\
    $f_b$ + Input Dropout & 0.167 & 0.0536\\
\end{tblr}
\caption{Model Verifiability}
\label{model-verifiability}
\end{table}




\paragraph{Robustness to Explanation Attacks.}
We report additional results on Chest X-ray and CelebA for the pixel perturbation tests, IOU tests, and model faithfulness tests for baseline models trained with manipulated gradients, as outlined in \ref{sec:manip}. We see that \name models are still  highly faithful and produce correct explanations even when derived from models adversarially trained to have manipulated explanations.  
\begin{table}[h!]
\centering
\caption{Faithfulness of \name Model for Manipulated Models}
\label{model-faithfulness-manip}
\begin{tblr}{
  cell{1}{2} = {c},
  cell{1}{3} = {c},
  cell{1}{4} = {c},
  hline{1-2,4} = {-}{},
}
 & {Hard \\ MNIST} & {Chest \\ X-ray} & {CelebA}\\
{Accuracy on \\Original Data\\$f_v(D_d) = f_b(D_d)$} & 0.990 & 1.00 & 0.970\\
{Accuracy on \\Simplified Data~\\$f_v(D_s) = f_b(D_d)$} & 0.980 & 1.00 & 0.946
\end{tblr}
\end{table}


\begin{table}[h!]
\centering
\caption{Intersection Over Union Results for Manipulated Models}
\label{iou-table-manip}
\begin{tblr}{
  cell{1}{3} = {c},
  cell{1}{4} = {c},
  cell{2}{1} = {r=5}{},
  hline{1-2} = {-}{},
  hline{7} = {2-4}{},
}
 &  & {MNIST \\(manipulated)} & {Chest X-ray\\(manipulated)}\\
\begin{sideways}Method\end{sideways} & VerT (Ours) & \textbf{0.454 $\pm0.08$} & \textbf{0.631$\pm 0.12$}\\
 & SmoothGrad & 0.158 $\pm0.07$ & 0.000 $\pm0.00$\\
 & GradCAM & 0.040$\pm0.06$ & 0.000 $\pm0.00$\\
 & Input Grad & 0.002$\pm0.01$ & 0.000 $\pm0.00$\\
 & FullGrad & 0.333 $\pm0.12$ & 0.004 $\pm0.04$
\end{tblr}
\end{table}


% Figure environment removed


\section{Additional Implementation and Computation Details}
Models were trained on the original train/test split given by \url{https://github.com/jayaneetha/colorized-MNIST} for Hard MNIST and \cite{kermany2018labeled} for the Chest X-ray dataset and with a random 80/20 split for CelebA. Baseline models were trained with Adam for 10 epochs with learning rate $1e-4$ and batch size 256. All hyperparameters are included in the code for this paper. We learn our masks with SGD (lr=$300$, batch size = $128$ and our verifiable models with Adam (lr=$1e-4$, batch size = $128$). We ran all experiments on a single A100 80 GB GPU with 32 GB memory. 

\section{Broader Impact} 
Our method, VerT, aims to transform black-box models into verifiably interpretable models, which produce easily verifiable feature attributions. As such, if applied correctly, it can help users and stakeholders of machine learning models better understand a model's predictions and behavior by isolating the features necessary for each prediction, which can help highlight biases, overfitting, mistakes, and more. It can also help to identify spurious correlations that naturally exist in datasets and are leveraged by models through identification of the signal-distractor decomposition. 
% However, if VerT does not identify a spurious correlation, it does not mean that none exist in the dataset and that further dataset cleaning and curation are not needed, but simply that the given model did not learn those spurious correlations
However, even if VerT does not identify a spurious correlation, that does not mean that further dataset cleaning, processing, or curation is not needed, as a different model may still learn a spurious correlation that was not leveraged by the original model. 
Furthermore, feature attributions often do not constitute a \textit{complete} explanation of a model. For instance, while an attribution tells us \textit{what} was important, it does not tell us \textit{how} it was important or how the model uses that feature. In all high stakes applications, it is still imperative that stakeholders think critically about each prediction and explanation, rather than blindly trusting either. 
