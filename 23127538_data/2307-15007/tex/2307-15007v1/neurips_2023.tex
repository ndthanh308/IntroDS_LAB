\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{tabularray}
\usepackage{rotating}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\X}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\pb}{p_{bayes}}
\newcommand{\pth}{p_{\theta}}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\M}{\mathbf{m}}
\newcommand{\Q}{\mathcal{Q}}

\newcommand{\suraj}[1]{{\color{cyan} Suraj: #1}}
\newcommand{\hima}[1]{{\color{magenta}[[hima: #1]]}}

\newcommand{\usha}[2]{{\color{red} Usha: #1}}
\newcommand{\name}{{VerT }}
% \title{Verifiable Feature Attribution: Bridging the Gaps between Post hoc Explanations and Inherently Interpretable Models}

\title{Verifiable Feature Attributions: A Bridge between Post Hoc Explainability and Inherent Interpretability}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Usha Bhalla* \\
  Harvard University\\
  \texttt{usha\_bhalla@g.harvard.edu} \\
  \And
  Suraj Srinivas* \\
  Harvard University \\
  \texttt{ssrinivas@seas.harvard.edu} \\
  \And
  Himabindu Lakkaraju \\
  Harvard University \\
  \texttt{hlakkaraju@hbs.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}

With the increased deployment of machine learning models in various real-world applications, researchers and practitioners alike have emphasized the need for explanations of model behaviour. To this end, two broad strategies have been outlined in prior literature to explain models. Post hoc explanation methods explain the behaviour of complex black-box models by highlighting features that are critical to model predictions; however, prior work has shown that these explanations may not be faithful, and even more concerning is our inability to \textit{verify} them. Specifically, it is nontrivial to evaluate if a given attribution is correct with respect to the underlying model. Inherently interpretable models, on the other hand, circumvent these issues by explicitly encoding explanations into model architecture, meaning their explanations are naturally faithful and verifiable, but they often exhibit poor predictive performance due to their limited expressive power. In this work, we aim to bridge the gap between the aforementioned strategies by proposing \emph{Verifiability Tuning} (VerT), a method that transforms black-box models into models that naturally yield faithful and verifiable feature attributions. We begin by introducing a formal theoretical framework to understand verifiability and show that attributions produced by standard models cannot be verified. We then leverage this framework to propose a method to build verifiable models and feature attributions out of fully trained black-box models. Finally, we perform extensive experiments on semi-synthetic and real-world datasets, and show that VerT produces models that (1) yield explanations that are correct and verifiable and (2) are faithful to the original black-box models they are meant to explain.

\end{abstract}

\vspace*{-0.2cm}
\section{Introduction}

\input{intro.tex}



\section{Related Work}
\input{related.tex}

\vspace*{-0.2cm}
\input{theory}

\vspace*{-0.2cm}
\input{method.tex}

\vspace*{-0.2cm}
\section{Experimental Evaluation}
\input{experiments.tex}


\vspace*{-0.3cm}
\section{Discussion}
\input{discussion.tex}


% \begin{ack}
% Use unnumbered first level headings for the acknowledgments. All acknowledgments
% go at the end of the paper before the list of references. Moreover, you are required to declare
% funding (financial activities supporting the submitted work) and competing interests (related financial activities outside the submitted work).
% More information about this disclosure can be found at: \url{https://neurips.cc/Conferences/2023/PaperInformation/FundingDisclosure}.


% Do {\bf not} include this section in the anonymized submission, only in the final paper. You can use the \texttt{ack} environment provided in the style file to autmoatically hide this section in the anonymized submission.
% \end{ack}


\newpage
\bibliographystyle{unsrtnat}
\bibliography{refs}



\newpage
\section*{Supplementary Material}

\input{supplementary.tex}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}