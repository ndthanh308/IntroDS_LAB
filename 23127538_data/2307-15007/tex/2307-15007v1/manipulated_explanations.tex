\subsection{Robustness to Adversarial Manipulation of Explanations}\label{sec:manip}

In this section, we highlight our method's robustness to adversarial explanation manipulation. To this end, we follow the manipulation proposed in \cite{heo2019fooling}, which adversarially manipulates gradient-based explanations. This is achieved by adding an extra term to the training objective that encourages input gradients to equal an arbitrary uninformative mask of pixels in the top left corner of each image. Note that model accuracy on the classification task is the same as training with only cross-entropy loss. 

We repeat experiments for all evaluation metrics on these manipulated models, with pixel-perturbation shown below \ref{fig:adv_perturb}, and IOU, model faithfulness, and model verifiability in the appendix. We see that gradient-based methods perform significantly worse on manipulated models; however, our method remains relatively invariant. We also note while the models are only manipulated to have arbitrary input gradients, SmoothGrad and GradCAM are also heavily affected such that their attributions are entirely uninterpretable as well, as shown below. 


% Figure environment removed



