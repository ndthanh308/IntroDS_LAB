In this paper, we seek to bridge the gap between existing post hoc explanation methods, which are unfaithful and nonverifiable, and inherently interpretable models, which are restrictive and often less performant than black-box models. In particular, we propose, VerT, a method that can transform any black-box model into a verifiably interpretable model, whose feature attributions can be easily evaluated for faithfulness. We empirically evaluate VerT and find that the resulting verifiable models are highly faithful and produce interpretable and verifiable attributions. 

\vspace*{-0.1cm}
\paragraph{Limitations.} We note that \name requires full access to the training dataset and the baseline model. Furthermore, while it produces verifiable feature attributions that tell us how important each feature is to the model's prediction, it does not tell us what the relationship between important features and the output/label is, as is true with all feature attributions. Finally, the utility of these attributions, and indeed, all feature attributions, relies on the existence of a non-trivial signal distractor decomposition of the dataset. If such a decomposition does not meaningfully exist, for example when the entire input is the signal, then ours, and all other feature attributions, are inapplicable. 