\section{A Framework for Verifiability}\label{sec:theory}
\newtheorem{defn}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem*{corr}{Corollary}
\newtheorem*{assumption}{Assumption}
\newenvironment{hproof}{%
  \renewcommand{\proofname}{Proof Idea}\proof}{\endproof}

In this section, we first demonstrate that the attributions produced by standard models are difficult to verify. We then introduce formal notions of verifiable models and feature attributions and characterize the conditions under which such verifiability can be achieved.

Intuitively, feature attribution methods work by simulating the removal of certain features and estimating how the model behaves when those features are removed: removal of unimportant features should not change model behaviour. Typically, this removal is implemented in practice by replacing features with scalar values, such as the mean of the dataset \cite{zeiler2014visualizing, samek2016evaluating, srinivas2019full}. However, this can result in out-of-distribution inputs that the classifier mishandles, making it challenging to verify whether the classifier is solely reliant on the signal from the relevant features. 
To ground this argument in an example, consider a model that classifies cows and camels. For an image of a camel, a feature attribution might note that only the hump of camel and the sand it stands on are important for classification.  As such, we would expect that the sky was irrelevant to the classifier's prediction, and we can concretely test this by altering it and creating a counterfactual sample. For example, we could mask the sky with an arbitrary uniform color; however, this may result in the sample being out-of-distribution for the model, and its prediction may change drastically even if the sky was actually not important for prediction. Ideally, we would be able to mask the sky in a manner that preserves the on-manifoldness of the image, but this is extremely tricky and dependent on both the dataset and the model. We formalize this argument below by defining feature attributions with respect to a given counterfactual distribution of masks that determine feature replacement, which we refer to as $(\epsilon, \Q)$-feature attribution.

\paragraph{Notation.}\label{notations} Throughout this paper, we shall assume the task of classification with inputs $\X \sim \mathcal{X}$ with $\X \in \R^d$ and $y \in [1,2,... C]$ with $C$-classes. We consider the class of deep neural networks $f: \R^d \rightarrow \triangle^{C}$ which map inputs $\X$ onto a $C$-class probability simplex. This paper considers binary feature attributions, which are represented as binary masks $\M \in \{0,1\}^d$, where $\M_i=1$ indicates an important feature and $\M_i=0$ indicates an unimportant feature. We shall also use the notation $\M' \subset \M$ to represent $\M'$ such that $\M'_i = 1$ on a subset of indices $i$ for which $\M_i = 1$. We say that $\M'$ is a \emph{subset mask} of $\M$.

\subsection{Verifiable Models and Feature Attributions}\label{q}
We first define the notion of feature attribution in the following manner, where the feature replacement method is made explicit, and features are replaced with samples from a counterfactual distribution $\Q$. Particularly, we are interested in binary attributions (i.e., a feature is either considered important or not), as opposed to real-valued attributions. 

\begin{defn} \label{defn:QFA} ($\epsilon, \Q$)-feature attribution (QFA) is a binary mask $\M(f, \X, \Q)$ that relies on a model $f(\cdot)$, an instance $\X$, and a $d$-dimensional counterfactual distribution $\Q$, and is given by
\begin{align*}
    \M(f, \X, \Q) &= \arg\min_{\M'} \| \M' \|_0 ~~\text{such that}~~\E_{q \sim \mathcal{Q}} \| f(\X_s(\M',q)) - f(\X) \|_1 \leq \epsilon
\end{align*}
where $\X_{s}(\M, q) = \M \odot \X + (1 - \M) \odot q$
\end{defn}

Thus an ($\epsilon, \Q$)-feature attribution (henceforth, \emph{QFA}) refers to the sparsest mask that can be applied to an image such that the model's output remains approximately unchanged. Observe that QFA depends on the feature replacement distribution $\Q$, where $\Q$ is independent of both $\X$ and $y$. This generalizes the commonly used heuristics of replacing unimportant features with the dataset mean, in which case $\Q$ is a dirac delta distribution at the mean value. The choice of $\Q$ is indeed critical, as an incorrect choice can hurt our ability to recover the correct attributions due to the resulting inputs being out-of-distribution, and the classifier being sensitive to such changes. Specifically, an incorrect $\Q$ can result in QFA being less sparse, as masking even a few features with the wrong $\Q$ would likely cause large deviations in the model's outputs. As a result, given a model, we must aim to the find the $\Q$ that leads to the sparsest QFA masks. However, the problem of searching over $\Q$ is complex, as it requires searching over the space of all $d$-dimensional distributions. To avoid this, we reverse the problem: given $\Q$, we find the class of models which have the sparsest QFAs w.r.t. that particular $\Q$. We call this the $\Q$-verifiable model class, which we define below: 

\begin{defn} \label{defn:QVIM}
    $\Q$-verifiable model class $\mathcal{F}_v(\Q)$: For some given distribution $\Q$, the class of models $\mathcal{F}$ for which $\Q$ gives the sparsest QFA mask as opposed to any other $\Q'$, such that for all $f \in \mathcal{F}$, 
    \begin{align*}
        \Q = \arg \min_{\Q'} \E_{\X} \| \M(f, \X, \Q') \|_0
    \end{align*}
    is called the model class with verifiable QFA.
\end{defn}

For the rest of this paper, we shall refer to QFA applied to a model from a $\Q$-verifiable model class as a "verifiable" feature attribution.

\vspace*{-0.2cm}
\subsection{Recovering the Signal-Distractor Decomposition}

In the study of feature attribution, the `ground truth' attributions are often unspecified. Here, we show that for datasets that are signal-distractor decomposable, formally defined below, there exists a ground truth attribution, and feature attributions for optimal verifiable models are able to recover it. Intuitively, given an object classification dataset between cows and camels, the "signal" refers to the regions in the image that are discriminative, or correlated with the label, such as the cows or camels. The distractor refers to everything else, such as the background or sky. Note that if objects in the background are correlated with the label, i.e. sand or grass, those would be part of the signal, not the distractor. For all datasets with this decomposition, the ground truth attributions correspond to the signal. We first begin by formally defining the signal-distractor decomposition. 

\begin{defn}
    A labelled dataset $D = \{(\X, y)_{i=1}^{N}\}$ is said to be signal-distractor decomposable if its generative process is given by: 
    \begin{align*}
        \mathbf{s} \sim &\mathcal{X}_\text{signal}(y)~~;~~ \mathbf{d} \sim \mathcal{X}_\text{distractor}~~;~~\M \sim \mathcal{M}\\
        &\X = \mathbf{s} \odot \M + \mathbf{d} \odot (1 - \M)
    \end{align*}
    where the mask $\M \in \{0,1\}^d$ and the distractor $\mathbf{d} \in \R^d$ are independent of the label $y$, and the signal $\mathbf{s} \in \R^d$ depends on $y$, with the following conditions met: (1) $p(y \mid \X) = p(y \mid \mathbf{s} \odot \M) = p(y \mid \mathbf{s})$, and (2) for any other mask $\M \not \subset \M'$, we have $p(y \mid \mathbf{s} \odot \M') < p(y \mid \mathbf{s} \odot \M)$ 
\end{defn}

We have the condition here that any mask that is not a superset of the ground truth mask leads to a strict loss in predictive power of the input. It follows from this non-redundancy constraint that given the distractor $\mathbf{d}$ and the signal $\mathbf{s}$, the mask that determines $\X$ is unique, and is the sparsest mask such that condition (1) is met. 

\begin{theorem}
    For datasets which are signal-distractor decomposable, QFA \underline{recovers the signal distribution} when applied to the optimal predictor $f_v^* \in \mathcal{F}_v(\Q)$.
\end{theorem}

\vspace*{-0.2cm}
\begin{hproof}
For datasets with the signal-distractor decomposition, the optimal $\Q$ is always equal to the ground truth distractor, and this leads to the sparsest QFA. If a $\Q$-verifiable model aims to recover the sparsest masks, then its QFA mask must equal that obtained by setting $\Q$ equal to the distractor. From uniqueness arguments, this is possible only when the signal distribution is recovered by QFA, such that it is not masked by $\Q$.
\end{hproof}

\begin{corr}
    For datasets which are signal-distractor decomposable, QFA does not recover the signal distribution when applied to a predictor $f \not\in \mathcal{F}_v(\Q)$.
\end{corr}

\vspace*{-0.2cm}
This follows from the fact that for any $f \not\in \mathcal{F}_v(\Q)$, there exists some other $\Q'$ that results in a sparser mask, indicating that the true signal distribution is not recovered. Thus, this shows that feature attributions applied to the incorrect model class can be less effective - in this case they fail to recover the ground truth signal distributions.

To summarize, we have defined a feature attribution method with the feature removal process made explicit via the counterfactual distribution $\Q$. To minimize sparsity of the attribution masks, we have to either (1) find the best distribution $\Q$, which is difficult to compute, or (2) given a fixed $\Q$, use models from the $\Q$-verifiable model class $\mathcal{F}_v(\Q)$. Finally, we find that feature attributions derived from model class $\mathcal{F}_v(\Q)$ are able to recover the signal-distractor decomposition of datasets.