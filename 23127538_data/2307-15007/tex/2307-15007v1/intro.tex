The rapid adoption of machine learning in nearly all industries and facets of life has brought with it the rise in popularity of explainable AI (XAI), which attempts to demystify ML models so that users can make informed decisions about their trustworthiness, accuracy, and usefulness. XAI methods can be broadly split into two categories: post hoc explainability and inherent interpretability. The majority of post hoc explanations aim to explain instance-level decisions of pre-trained black box models through feature attribution, or labelling which features were the most relevant to a model's decision. These methods often work by approximating model behavior through local linear function approximation \cite{han2022explanation}. However, given that these methods are simply approximations, they are often not \textit{faithful} to the true behavior of models. 
This is particularly concerning because it is difficult to directly and decisively verify the faithfulness or correctness of the attributions produced.
Inherently interpretable models, on the other hand, are explainable by nature and are crafted such that humans can clearly trace the steps, reasoning, and computations made by a model. As such, the explanations yielded by these models are highly faithful and can be readily verified; however, the models themselves are often less useful in practice due to their decreased performance and limited expressivity. 

% Figure environment removed

\vspace{-0.3cm}
In this work, we aim to bridge the gap between the two aforementioned categories of methods by proposing \emph{Verifiability Tuning} (VerT), which converts black box models into verifiably-interpretable ones. Specifically, the models produced by our method generate feature attributions that are verifiable by nature and highly faithful to the underlying behavior of the original black-box model. 
Our contributions are the following:
\begin{enumerate}
    \item We first formalize a framework for understanding feature attribution verification, and show theoretically that feature attributions applied to black-box models are unverifiable.
    \item We propose \emph{verifiability tuning} (VerT), a novel method that converts black-box models into verifiably-interpretable ones, such that the feature attributions of the resulting models are verifiable, in that model predictions remain consistent after masking unimportant features.
    \item We perform experiments on semi-synthetic and real-world computer vision datasets and show that VerT outputs models that are faithful to its original input models, and that their feature attributions are interpretable, verifiable, and correct, in the sense that they match ground-truth feature attributions.
\end{enumerate}

By proposing a method to produce \textit{verifiable} and \textit{faithful} attributions while still leveraging black-box models, our work offers a promising approach to address the limitations of both post hoc explanation methods and inherently interpretable models.