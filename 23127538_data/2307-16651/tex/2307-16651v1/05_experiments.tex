
\section{Experiments}\label{experiment}
We conduct various qualitative and quantitative evaluations to validate our models on the cardio-respiratory fitness prediction task with wearable sensing (\S\ref{5.1}). We outline the model's architecture and how it differs from other baselines (\S\ref{5.2} and \S\ref{5.3}). Next, we further quantify the impact of incorporating source domain knowledge to \systemname{} (\S\ref{5.4}).
The overall workflow is depicted in Figure~\ref{fig:workflow}. Code is available at: \url{https://github.com/yvonneywu/UDAMA}. 


\subsection{Datasets and Training Strategy}\label{5.1}
\textbf{Datasets.} The source domain comes from the silver-standard measurement study (Fenland), including 11,059 participants. It contains a combined heart rate and movement signal from chest ECG sensor Actiheart and noisy VO$_{2}$max labels collected from submaximal exercise tests~\citep{feland}. 
The target domain \(\bm{D_t}\) represents the gold-standard measurement dataset BBVS, which is a subset of 181 participants from the Fenland study with directly measured gold-standard VO$_{2}$max~\citep{bvs} during the maximal exercise tests. In the BBVS study, participants need to wear a face mask to measure respiratory gas measurements~\citep{Rietjens-2001} to experience exhaustion tests.
Further, movement and heart rate are collected using the Actiheart (CamNtech, Papworth, UK) sensor. The details of the two datasets are in Appendix~(\ref{4})

\textbf{Training Strategy.} We evaluate \systemname{} on these two datasets by first pre-training a model on the \(\bm{D_s}\) Fenland. 
Second, we develop the adversarial training framework with multi-discriminators on the BBVS (\(\bm{D_t}\)), with the help of incorporated prior domain knowledge (i.e., injecting random samples from the source domain). For each domain, feature choices are listed in Appendix~\ref{apd:first}. 
After the adversarial adaptation, we predict VO$_{2}$max on the held-out test set of BBVS using 3-fold cross-validation using \systemname{}. Within each fold, the dataset is split into 70\% training and 30\% testing consisting only of target domain samples.


\subsection{Model architecture and tuning}\label{5.2}
\subsubsection{Model architecture} 
This section discusses the details of the neural networks used in this study. For the encoder network, two modules are integrated within the network to extract temporal and metadata information. In particular, we use two Bidirectional GRU layers of 32 units for the time series data module, followed by one 1D-global averaging pooling layer. On the other hand, in the meta-data module, MLP layers of dimensionality 128 are constructed to extract associated metadata representation after a Batch Normalization layer. Following this, the time and metadata module outputs are concatenated together to generate a complete embedding matrix for the subsequent regression or classification tasks. The training architecture is shown in Figure~\ref{fig:train_overflow}.

The training pipeline consists of two phases using the pre-training and fine-tuning learning scheme. First, after comparing different parameter-sharing techniques for fine-tuning the encoder utilized in the source domain, we freeze the first GRU and MLP layers and fine-tuned the remaining network. Compared with freezing all layers except re-training the output layer, the fine-tuning scheme in our network could capture the general features from the lower layers and extract problem-specific characteristics from higher layers. Lastly, the representation embedding produced from the fine-tuned encoder is transmitted to distinct tasks with a linear activation layer appropriate for predicting the fitness level or classifying the domain labels.

\subsubsection{Hyper-paremeter tuning} 
All network blocks in the framework are trained using Adam with a learning rate tuned over \{1e-2, 1e-3\}.
% \hj{just say 1e-2, 1e-3...}. \yw{fixed}
The dropout rate is tuned over the following ranges \{0.2, 0.3\}. Moreover, we tune the batch size between 8, 16, and 32 based on the efficiency and stability of the training process.
% \hj{?} \yv{to describe how to tune the variables in the total loss function} 
To tune the \systemname{} total loss, we conduct a grid search~\{0.01, 0.02, 0.03\} for the $\alpha$ and~\{(0.9, 0.1), (0.8, 0.2), (0.7, 0.3), (0.6, 0.4), (0.5, 0.5)\} for the combination of $\lambda_1$ and $\lambda_2$. We perform early stopping to combat overfitting until the validation loss stops improving after ten epochs. The details of hyperparameter selection are listed in Table~\ref{table:hyper}.


\subsection{Baselines}\label{5.3}
To verify the effectiveness of our proposed network, we compare the \systemname{} against several baselines:
\begin{itemize}
    \item \imt{\textbf{In-domain supervised model.} A multi-model network with training on the same domain train and test set. }
    \item \textbf{Out-of-domain supervised model}. A network with the same structure as the pre-training model, using wearable data and common biomarkers in \(\bm{D_t}\) as input to predict VO$_{2}$max. 
    \item \textbf{Transfer learning.}  A pre-trained model trained on \(\bm{D_s}\) is reused and fine-tuned on the target domain. 
    \imt{\item \textbf{Autoencoder~\citep{Srivastava_2015}.}  Pre-train a model with stacked recurrent autoencoders on \(\bm{D_s}\) and fine-tune the representation from the encoder to \(\bm{D_t}\). }
    \item \textbf{Deep-Coral~\citep{coral}}. A widely employed discrepancy-based domain adaptation minimizes the divergence between the source and target in feature space. In particular, it seeks to align the second-order statistics of the source and target distributions.
    \item \textbf{WDGRL~\citep{wdgrl}}. Wasserstein Distance is used to minimize the disparity between the source and target representations in the feature space. Specifically, the distance will be optimized in an adversarial way for the feature extractor, and domain-invariant features will be learned.
    \item \textbf{Domain Adversarial Neural Networks (DANN)~\citep{Ganin-2015}.} A benchmark domain adaption method that uses adversarial training for binary domain classification. 

    % \ds{we need a note here to discuss why we don't use more recent baselines. It's ok to compare to earlier work but we just make sure that reviewers won't dismiss it as "obsolete". We can list newer works we mentioned in previous sections and discuss why they are not relevant.}\yw{fixed}
    
\end{itemize}
\mlhc{Although most recent domain adaptation methods have shown enhanced performance in dealing with distribution shift through self-training or sophisticated adversarial training scheme~\citep{Du-2020, Liu_2021}, they do not specifically tackle the regression tasks or healthcare datasets that contain both noisy and gold-standard labels. Therefore, their relevance in the cardio-fitness prediction task is limited. }

\subsection{Effect of Injected Source Domain Samples}\label{5.4}
Our proposed approach incorporates prior knowledge from the source domain to create the adversarial training environment. To assess the degree to which the incorporated source domain knowledge impacts our model, we evaluate \systemname{} on $\bm{D_t}$ with different levels of 
injected samples from ($\bm{D_s}$). As such, we put \{0.1\%, 0.2\%, 0.4\%, 1\%, 2\%, 4\% \} from $\bm{D_s}$, which equals to \{1\%, 5\%, 10\%, 30\%, 50\% and 100\%\} of training data of the target domain ($\bm{D_t}$). We set the maximum amount to 100\% because the two domains are highly different, and for any larger amount, the model would be trying to predict the silver data rather than the gold data. Specifically, the number of samples differs between source and target domain samples (source/target = 25:1), which means 0.2\% of source data equals 5\% of target data. If we continually add more source data, for example, 4\% of the source (which equals 100\% of the target data), we will ultimately train on the source data instead of the target data.
Finally, we run the experiments 15 times with different seeds to assess the impact of the injected noisy samples using the average performance on the VO$_{2}$max prediction task. Results are shown in \S\ref{6.3}.


\subsection{Metrics}
The prediction performance of all models is evaluated based on standard regression evaluation metrics such as the Mean Squared Error (MSE) and Mean Absolute Error (MAE). The coefficient of determination (R$^2$) and the Pearson correlation coefficient (Corr) are also used to evaluate the model performance on the health-related outcome prediction task. 






