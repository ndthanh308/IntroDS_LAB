%\documentclass[wcp,gray]{jmlr} % test grayscale version
 %\documentclass[wcp]{jmlr}% former name JMLR W\&CP
\documentclass[pmlr]{jmlr}% new name PMLR (Proceedings of Machine Learning)

 % The following packages will be automatically loaded:
 % amsmath, amssymb, natbib, graphicx, url, algorithm2e

 %\usepackage{rotating}% for sideways figures and tables
\usepackage{longtable}% for long tables
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{siunitx}
% \usepackage[load-configurations={abbreviations}]{siunitx}
% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage[caption=true,font=footnotesize]{subfig}
\usepackage{float}
\usepackage{xcolor,color,soul,bm}
\usepackage{makecell}
 % The booktabs package is used by this sample document
 % (it provides \toprule, \midrule and \bottomrule).
 % Remove the next line if you don't require it.
\usepackage{booktabs}
\usepackage{amssymb}
\newcommand{\systemname}{UDAMA}
\newcommand{\imt}[1]{\textcolor{black}{#1}}
 % The booktabs package is used by this sample docume√∑nt
 % (it provides \toprule, \midrule and \bottomrule).
 % 
 % book quality tables
\usepackage{booktabs}
\usepackage{cleveref}
\usepackage{hyperref}
\crefname{subfigure}{}{}
 
 % The siunitx package is used by this sample document
 % to align numbers in a column by their decimal point.
 % Remove the next line if you don't require it.
% \usepackage[load-configurations=version-1]{siunitx} % newer version
 %\usepackage{siunitx}
\newcommand{\hj}[1]{\sethlcolor{orange}\hl{[HJ: #1]}}

\newcommand{\yw}[1]{\sethlcolor{lime}\hl{[YW: #1]}}
\newcommand{\ds}[1]{\sethlcolor{pink}\hl{[DS: #1]}}
\newcommand{\cm}[1]{\sethlcolor{yellow}\hl{[CM: #1]}}
\newcommand{\mlhc}[1]{\textcolor{black}{#1}}

\makeatletter
\def\set@curr@file#1{\def\@curr@file{#1}} %temp workaround for 2019 latex release
\makeatother

 % The following command is just for this sample document:
\newcommand{\cs}[1]{\texttt{\char`\\#1}}

 % Define an unnumbered theorem just for this sample document:
\theorembodyfont{\upshape}
\theoremheaderfont{\scshape}
\theorempostheader{:}
\theoremsep{\newline}
\newtheorem*{note}{Note}

 % change the arguments, as appropriate, in the following:
% \jmlrvolume{}
\jmlrvolume{219}
\jmlryear{2023}
\jmlrworkshop{Machine Learning for Healthcare}

% H: Not sure if we need this:
% Short headings should be running head and authors last names
% \ShortHeadings{A Really Awesome MLHC Article}{Lastname, PhD and Lastname, MD}
% \firstpageno{1}

% \title[Short Title]{Title of Your MLHC Article}
% \title[UDAMA]{\textit{Turning Silver into Gold}: Domain Adaptation with Weak Labels for Wearable \mlhc{Cardio-fitness} Prediction}
\title[UDAMA]{\textit{UDAMA}: Unsupervised Domain Adaptation through Multi-discriminator Adversarial Training with Noisy Labels Improves Cardio-fitness Prediction}

\author{\Name{Yu Wu\textsuperscript{1}} \Email{yw573@cam.ac.uk}\\
  % \AND
  \Name{Dimitris Spathis\textsuperscript{1,2}} \Email{ds806@cam.ac.uk}\\
  % \AND
  \Name{Hong Jia\textsuperscript{1}} \Email{hj359@cam.ac.uk}\\
  % \AND
  \Name{Ignacio Perez-Pozuelo\textsuperscript{3}} \Email{ip325@cam.ac.uk}\\
  % \AND
  \Name{Tomas I. Gonzales\textsuperscript{3}} \Email{tomas.gonzales@mrc-epid.cam.ac.uk}\\
  % \AND
  \Name{Soren Brage\textsuperscript{3}} \Email{soren.brage@mrc-epid.cam.ac.uk}\\
  % \AND
  \Name{Nicholas Wareham\textsuperscript{3}} \Email{nick.wareham@mrc-epid.cam.ac.uk}\\
  % \AND
  \Name{Cecilia Mascolo\textsuperscript{1}} \Email{cm542@cam.ac.uk}\\
  \addr \textsuperscript{1}Department of Computer Science and Technology, University of Cambridge, UK \\
  \addr \textsuperscript{2}Nokia Bell Labs, Cambridge, UK \\
  \addr \textsuperscript{3}MRC Epidemiology Unit, School of Clinical Medicine, University of Cambridge, UK
 }


% \editor{Editor's name}

\begin{document}

\maketitle

\begin{abstract}
% \yw{Make sure co-author's affliations}


% \yw{Restore to last version focus on generalizable ML for healthcare?}
% Deep learning models are increasingly used to predict wearable-based cardio-respiratory fitness (CRF), with maximal oxygen consumption (VO$_{2}$max) being the benchmark measurement. 
Deep learning models have shown great promise in various healthcare \mlhc{monitoring} applications. 
% However, most of them are typically developed and validated on small-scale datasets\cm{is it true? xray image datasets are big. is it possible it is true for time series data only?},\yw{fixed} as directly collecting high-quality (gold-standard) data for health applications is often costly and time-consuming.
\mlhc{However, most healthcare datasets with high-quality (gold-standard) labels are small-scale, as directly collecting ground truth is often costly and time-consuming. As a result, models developed and validated on small-scale datasets often suffer from overfitting and do not generalize well to unseen scenarios.}
% As a result, these models often suffer from overfitting and do not generalize well to unseen datasets. 
At the same time, large amounts of imprecise (silver-standard) labeled data, 
% of sensor\cm{sensor comes out of the blue} \yw{fixed} data, 
annotated by approximate methods with the help of modern wearables and in the absence of ground truth validation, are starting to emerge. However, due to measurement differences, this data displays significant label distribution shifts, which motivates the use of domain adaptation. To this end, we introduce \textbf{\systemname{}}, a method with two key components: \textbf{Unsupervised Domain Adaptation} and \textbf{Multi-discriminator Adversarial Training}, where we pre-train on the silver-standard data and employ adversarial adaptation with the gold-standard data along with two domain discriminators.
% \hj{to improve the model performance.}
% We validate our framework on the maximal oxygen consumption (VO$_{2}$max) prediction task using two free-living cohort studies (N=11,059 and N=181) for pre-training and fine-tuning, respectively.
% We show that \systemname{} achieves the best performance of corr = 0.701 $\pm$ 0.032 compared to vanilla transfer learning and state-of-the-art domain adaptation models, paving the way for leveraging noisy labeled data towards accurate fitness estimation at scale.
\mlhc{In particular, we showcase the practical potential of \systemname{} by applying it to Cardio-respiratory fitness (CRF) prediction. CRF is a crucial determinant of metabolic disease and mortality, and it presents labels with various levels of noise (gold- and silver-standard), making it challenging to establish an accurate prediction model. Our results show promising performance by alleviating distribution shifts in various label shift settings. Additionally, by using data from two free-living cohort studies (Fenland and BBVS), we show that \systemname{} consistently outperforms up to 12\% 
% \ds{how much better compared to the SOTA?}\yw{fixed} 
compared to competitive transfer learning and state-of-the-art domain adaptation models, paving the way for leveraging noisy labeled data to improve fitness estimation at scale.}

% \yw{pre-pare github-repo link here...}


\end{abstract}


\input{01_new_introduction}
\input{02_related_work}
\input{03_casestudy}
\input{04_methods}
\input{05_experiments}
\input{06_results_discussion}
\input{07_conclusion}



% ACKNOWLEDGEMENTS ONLY GO IN THE CAMERA-READY, NOT THE SUBMISSION
% \yw{confirm acknowledgement with C}
\acks{This work was supported by ERC Project 833296 (EAR) and by Nokia Bell Labs.}

\bibliography{main}

\newpage
\appendix
\input{appendix}

\end{document}
