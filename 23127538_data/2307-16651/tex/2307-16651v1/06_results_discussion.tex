\section{Results and Discussion}\label{6}
% \yw{should we emphasize the semi-synthetic data since we only have one datasets,}
In this section, we present the results of applying \systemname{} to the cardio-fitness prediction task and compare it with the baselines (\S\ref{6.1}). Additionally, we discuss the performance in addressing the domain shift problem (\S\ref{6.2}) and the impact of injected source domain knowledge (\S\ref{6.3}). Furthermore, we conduct an ablation study to verify the effectiveness of our framework's structure (\S\ref{6.4}). Finally, we discuss the model robustness in \S\ref{6.5} with semi-synthetic data. 

% \begin{table}
%   \caption{\textbf{Evaluation of different methods on CRF prediction task.} Each result displays the mean value with standard deviation from three-fold cross-validation. In particular, \textbf{In-domain} means the model is trained on the same \imt{domain, namely trained on \(\bm{D_t}\) and tested on $\bm{D_t}$}, while \textbf{Out-of-domain} corresponds to models trained on Fenland (\(\bm{D_s}\)) and  adapted/fine-tuned to BBVS (\(\bm{D_t}\)). All \textbf{Out-of-domain} models are evaluated on the BBVS test set.  
% %   \hj{add a seperator between ID and OoD. Also horizontally centralize OoD}\yw{fixed}
%   }
%   \resizebox{1\textwidth}{!}{%
%   \begin{tabular}
%   % {\textwidth}{p{0.15\linewidth}p{0.19\linewidth}p{0.14\linewidth}p{0.14\linewidth}p{0.14\linewidth}p{0.14\linewidth}}
%   {llllll}
%   \toprule
%     \textbf{In-domain} &\textbf{Training method} & \textbf{R$^2$} & \textbf{Corr} &  \textbf{MSE} & \textbf{MAE} \\ 
%     \midrule
%     $D_t$ \rightarrow  $D_t$ & Supervised & 0.123 $\pm$ 0.111  & 0.622 $\pm$ 0.036 & 43.778 $\pm$ 6.012  & 5.263 $\pm$ 0.277  \\
%     \toprule
%     \textbf{Out-of-domain} &\textbf{Training method} & \textbf{R$^2$} & \textbf{Corr} &  \textbf{MSE} & \textbf{MAE} \\
%     \midrule
%      $D_s$ \rightarrow  $D_t$ & \imt{Supervised} & -0.096 $\pm$ 0.100 & 0.007 $\pm$ 0.250 & 58.048 $\pm$ 10.061  & 6.336 $\pm$ 0.621  \\ 
%      &WDGRL ~\citep{wdgrl} & -0.100   $\pm$ 0.073 & 0.004 $\pm$ 0.161 & 55.611 $\pm$ 10.61 & 6.044 $\pm$ 0.615 \\
%      & Autoencoder~\citep{Srivastava_2015} & -0.067 $\pm$ 0.069 & 0.127 $\pm$ 0.222 & 53.254 $\pm$ 4.878 & 5.973 $\pm$ 0.194 \\
%     &Deep-Coral~\citep{coral} & 0.021 $\pm$ 0.073  & 0.360  $\pm$ 0.057 & 49.044 $\pm$ 6.553 &  5.638 $\pm$ 0.374\\
%     &Transfer learning (TF)  & 0.283 $\pm$ 0.037  & 0.621 $\pm$ 0.012 & 35.399 $\pm$  5.910 & 4.744 $\pm$ 0.433
%  \\ 
%   &DANN~\citep{Ganin-2015} & 0.288 $\pm$ 0.077   & 0.617 $\pm$ 0.037 & 35.458 $\pm$ 3.920 & 4.679 $\pm$ 0.382  \\
%   \hline
%     &\textbf{UDAMA (ours)} & \textbf{0.459} $\pm$ \textbf{0.063} & \textbf{0.701} $\pm$ \textbf{0.032} &  \textbf{27.469} $\pm$ \textbf{6.456} & \textbf{4.111} $\pm$ \textbf{0.353} \\
    
%     \bottomrule
%   \end{tabular}%
% }
%   \label{table-results}

% \end{table}

\begin{table}
  \caption{\textbf{Evaluation of different methods on CRF prediction task.} Each result displays the mean value with standard deviation from three-fold cross-validation. In particular, \textbf{In-domain} means the model is trained on the same domain, namely trained on \(D_t\) and tested on \(D_t\), while \textbf{Out-of-domain} corresponds to models trained on Fenland (\(D_s\)) and  adapted/fine-tuned to BBVS (\(D_t\)). All \textbf{Out-of-domain} models are evaluated on the BBVS test set.}
  \resizebox{1\textwidth}{!}{%
  \begin{tabular}
  {llllll}
  \toprule
    \textbf{In-domain} &\textbf{Training method} & \textbf{R$^2$} & \textbf{Corr} &  \textbf{MSE} & \textbf{MAE} \\ 
    \midrule
    \(D_t \rightarrow  D_t\) & \textit{Supervised} & \(0.123 \pm 0.111\)  & \(0.622 \pm 0.036\) & \(43.778 \pm 6.012\)   & \(5.263 \pm 0.277\)  \\
    \toprule
    \textbf{Out-of-domain} &\textbf{Training method} & \textbf{R$^2$} & \textbf{Corr} &  \textbf{MSE} & \textbf{MAE} \\
    \midrule
     \(D_s \rightarrow  D_t\) & \textit{Supervised} & \(-0.096 \pm 0.100\) & \(0.007 \pm 0.250\) & \(58.048 \pm 10.061\)  & \(6.336 \pm 0.621\)  \\ 
     &WDGRL ~\citep{wdgrl} & \(-0.100   \pm 0.073\) & \(0.004 \pm 0.161\) & \(55.611 \pm 10.61\) & \(6.044 \pm 0.615\) \\
     & Autoencoder~\citep{Srivastava_2015} & \(-0.067 \pm 0.069\) & \(0.127 \pm 0.222\) & \(53.254 \pm 4.878\) & \(5.973 \pm 0.194\) \\
    &Deep-Coral~\citep{coral} & \(0.021 \pm 0.073\)  & \(0.360  \pm 0.057\) & \(49.044 \pm 6.553\) &  \(5.638 \pm 0.374\)\\
    &Transfer learning (TF)  & \(0.283 \pm 0.037\)  & \(0.621 \pm 0.012\) & \(35.399 \pm  5.910\) & \(4.744 \pm 0.433\) \\ 
  &DANN~\citep{Ganin-2015} & \(0.288 \pm 0.077\)   & \(0.617 \pm 0.037\) & \(35.458 \pm 3.920\) & \(4.679 \pm 0.382\)  \\
  \hline
    &\textbf{UDAMA (ours)} & \textbf{0.459} \(\pm\) \textbf{0.063} & \textbf{0.701} \(\pm\) \textbf{0.032} &  \textbf{27.469} \(\pm\) \textbf{6.456} & \textbf{4.111} \(\pm\) \textbf{0.353} \\
    
    \bottomrule
  \end{tabular}%
}
  \label{table-results}
\end{table}



\subsection{Fitness prediction}\label{6.1}
We took 60 participants from the BBVS dataset as test samples to predict their CRF by predicting VO$_{2}$max values. The comparison between the proposed domain adaptation framework and baseline approaches is shown in Table~\ref{table-results}.

\imt{First, in the out-of-domain comparison, adversarial-based DA or transfer learning shows better performance than the discrepancy-based method under label distribution shift.} In particular, the discrepancy-based method, Deep-coral, increases the Corr and MAE to 0.36 and 5.638, respectively. Meanwhile, WDGRL aims to minimize the feature difference by employing Wasserstein distance, yielding results comparable to the out-of-domain supervised method, which directly applies the model trained on Fenland for testing BBVS. In contrast, the adversarial-based methods here display better results. DANN learns a representation that is predictive of the regression task but uninformative to the input domain and improves the Corr and MAE to 0.617 and 4.679, respectively. According to Corr and MAE, methods such as transfer learning with fine-tuning techniques also improve performance, achieving 0.621 and 4.744, compared to the discrepancy-based method. 

\imt{In general, high Corr and R2 values and low MSE and MAE demonstrate the model's ability to leverage noisy, large-scale labeled VO2max data for gold-standard VO2max prediction under label shift. However, the limited size of the test set might result in increased uncertainty during model evaluation. The results from both TF and DANN are similar as shown in Table~\ref{table-results}}. In contrast, \systemname{} utilizing both fine-tuning and adversarial-based domain adaptation methods outperforms all the abovementioned baselines. We observe that the correlation (Corr) outperforms the basic transfer learning methods by 12.9\%, the MSE increases by 22.4\%, and R$^2$ improves by 62.2\%. Moreover, compared with the in-domain supervised training on $\bm{D_t}$, \systemname{} shows a significant increase, improving R$^2$ from 0.123 to 0.459. 
% Therefore, \systemname{} has the capability to make use of large, weakly labeled VO2max data for accurate gold-standard VO2max predictions despite label shift }
Our method also achieves good performance when generalizing the model from the in-domain to out-of-domain BBVS setting, compared with the dramatic performance drop-down, as shown in Table~\ref{table-results}. Therefore, \systemname{} can leverage the large-scale noisy datasets information and alleviate the model performance degeneration performance compared with directly validating models on small-scale sensing datasets. 
% Figure environment removed


\subsection{Domain shift}\label{6.2}
%%%our method can catch the mean and the range here, while the others fail xxx 
To better compare whether \systemname{} or baseline DA methods can solve the label distribution shift problem effectively. Figure~\ref{distribution} presents the predicted label distribution of the BBVS test set from different methods. 
% \hj{move plots into 2 plots per line}\yw{fixed}
First, it shows that the \(\bm{D_s}\) dataset (i.e., Fenland) shares a different VO$_{2}$max underlying distribution compared to \(\bm{D_t}\) BBVS. Our results demonstrate that \systemname{} can learn the small dataset distribution during the adaptation phase and achieves promising results compared with other methods. Besides, we observe that both adversarial-based methods and transfer learning capture the mean and range of target domain distribution as shown in Figure~\cref{fig:distribution-shift_good}, whereas the discrepancy-based methods fail to learn the general distribution. We attribute this performance degeneration to the fact that discrepancy-based approaches, mainly designed to minimize the divergence between feature spaces, cannot alleviate the impact of noisy labeling.

Moreover, we use the Hellinger Distance (HD), which calculates the similarity of distributions between prediction and ground truth to examine the distance between two label distributions. Specifically, our framework's prediction of fitness level lies in the same range as the ground truth, while methods like Deep-coral or WDGRL fail at learning within this range. Besides, the distribution of \systemname{} ties close compared to baseline methods, where the normalized HD for \systemname{} is 0.179, and HD for TF is 0.264, for Deep-coral is 0.305. These results indicate that our framework effectively alleviates the distribution shift problem of the VO$_{2}$max prediction task and \systemname{} can leverage noisy silver-standard data to improve the performance on the gold-standard dataset. 

% Figure environment removed

% % Figure environment removed





% % Figure environment removed


\subsection{Impact of Injected Knowledge from Source Domain}\label{6.3}
% \yw{modifying the caption here...}\yw{fixed}
Instead of using a generator, we incorporated source domain knowledge to create the adversarial training environment. Figure~\cref{fig:scale} shows the average performance of adding the different scales of injected source domain samples to the target domain.
Each box plot shows the average MSE results of 15 runs with added random samples from the source domain. 
As shown in Figure~\cref{fig:scale}, our method performs better than the baseline methods with added samples in all cases, even after adding 100\% of noisy samples of $\bm{D_t}$ from $\bm{D_s}$.
Specifically, we observe that it achieves the best results and showcases the most positive transfer by only adding 0.4\% of the source domain, which equals 10\% of \(\bm{D_t}\). In contrast, if we continue to add more noisy information to the adaptation stage, the performance will gradually decrease as the source and target domain data reach the ratio of 1:1. After that, the adaptation tends to learn the noisy source domain representation instead of the target domain, yielding a negative transfer. Therefore, only injecting a few samples from the source domain to create the adversarial training environment might help to learn more domain-invariant features and achieve optimal results.

% % Figure environment removed






\subsection{Ablation Study} \label{6.4}
Our framework comprises two joint discriminators, so we perform an ablation study to understand each discriminator's effect. Based on the observation that incorporating a 10\% amount of BBVS from the \(\bm{D_t}\) Fenland dataset achieves the best transfer, we train different discriminators under this setting. We observe that a single discriminator (coarse-grained or fine-grained) exhibits more competitive performance than the baseline TF or DANN methods, as shown in Table~\ref{table:ablation study} and Figure~\cref{fig:scale}. Specifically, the fine-grained discriminator alone significantly outperforms the baseline results: +15.8\% TF (based on MSE). This indicates that utilizing a fine-grained distribution domain discriminator, which discriminates the domain label distribution, enhances the training framework. Therefore, with the combination of \(\bm{D_f}\) and \(\bm{D_c}\), \systemname{} can learn the cross-domain information without capturing the domain information of the input and alleviate the noisy labeling problem. 

\imt{Additionally, our method is unique in that it uses wearable-based CRF prediction task, unlike other methods that solely rely on anthropometric data~\citep{Nes_2011}. Our experiments have demonstrated that combining sensor data with anthropometric information leads to improved prediction accuracy. Although the sensor data alone is unreliable and insufficient for accurate VO${2}$max prediction, when combined with anthropometric data, the accuracy of \systemname{} increases from 0.679 (using only anthropometric data) to 0.701 (using anthropometric data and wearable sensor data such as acceleration, heart rate, and heart rate variability). As a result, using anthropometric and low-cost wearable devices together enables more accurate VO$_{2}$max prediction through \systemname{}.  }




\begin{table*}
  \centering 
  \small 
  \caption{\textbf{Ablation study by removing one of the discriminators.
%   \hj{Keep precisions identical}\yw{fixed}
  }}
  \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} llll}
  \toprule
    \textbf{Discriminator} & \textbf{R$^2$} & \textbf{Corr} & \textbf{MSE} & \textbf{MAE} \\
    \midrule
    Coarse-grained & 0.339 $\pm$ 0.053 & 0.626 $\pm$ 0.032 & 33.485 $\pm$ 6.740  & 4.580 $\pm$ 0.337 \\ 
    Fine-grained  & 0.394 $\pm$ 0.030 & 0.666 $\pm$ 0.030 & 30.578 $\pm$ 5.300  & 4.498 $\pm$ 0.324 \\ 
    \textbf{UDAMA (ours)} & \textbf{0.459} $\pm$ \textbf{0.063} & \textbf{0.701} $\pm$ \textbf{0.032} &  \textbf{27.469} $\pm$ \textbf{6.456} & \textbf{4.111} $\pm$ \textbf{0.353} \\
    \bottomrule
  \end{tabular*}

  \label{table:ablation study} 
\end{table*}

%%% KL-divergency and simulated shifted dataset

\begin{table*}
  \centering 
  \small 
  \caption{\imt{\textbf{Simulated Label Distribution Shift}. Kullbackâ€“Leibler divergence (KL divergence) calculates the distribution difference between the shifted $D_s$ and fixed $D_t$.}
  }
  \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} llll}
  \toprule
      Source dataset& \textbf{KL divergence} & \textbf{UDAMA Corr} & \textbf{DANN Corr }  \\
    \midrule
    Fenland  & 0.461  & 0.701 $\pm$ 0.032  &  0.617 $\pm$ 0.037 \\ 
     Left shifted Fenland & 1.607 & 0.656 $\pm$ 0.065 &  0.589 $\pm$ 0.027 \\ 
  Right shifted Fenland & 3.188  & 0.646 $\pm$ 0.035  &  0.608 $\pm$ 0.037  \\
    \bottomrule
  \end{tabular*}

  \label{table:simulation} 
  \vspace{-0.1in}
\end{table*}


% % Figure environment removed



% % Figure environment removed

\subsection{Robustness assessment with semi-synthetic data shift}\label{6.5}
% \yw{add a section to describe the synthetic label shift method, its impact of our method to fitness prediction using domain adaptation method}
% \imt{There are not many available datasets with similar label distribution shift problems in the literature, especially in healthcare domain. Therefore, we perform additional experiments with a semi-synthetic dataset to generate various label distribution shifts\cm{this is a killer sentence..you are essentially saying there is no application for your work. how about saying, while the problem is very real cite examples, there are very few open available healthcare datasets with label distribution shift on which we could try our technique }. 
\mlhc{Although the distribution shift among gold- and silver-standard labels are prevalent in healthcare applications, there are very few open available datasets with such a challenge where we can apply our method. Therefore, to further evaluate the efficacy of \systemname{}, we generate semi-synthetic datasets with various label distribution shifts for the cardio-fitness prediction.}

Motivated by the label distribution shift simulation for classification, as seen in ~\citep{Lipton_2018}, 
we shift the labels in the $\bm{D_s}$ by a fixed offset and Gaussian noise. By conducting experiments with varying degrees of label shifts, we gain a comprehensive understanding of the effect of label shifts on CRF prediction tasks. As the shift becomes greater from the target domain, the performance decreases. In particular, we show two extreme cases by pushing the source domain shift to left and right to stress-test \systemname{}, and the results are shown in Table ~\ref{table:simulation}. Despite the increasing KL divergence, which indicates a greater deviation from the ground truth data, our method still displays robust performance compared to the baseline DA method, especially in the case of low fitness on the left side. These stress tests highlight the versatility and robustness of our model in dealing with different input distributions.

% \subsection{Interpretability}\label{6.5}
% To understand the effectiveness of \systemname{}, we further examine the saliency map to interpret predictions on the data-point level. Finally, motivated by recent works~\citep{cam-timeseries,RAM,cam}, we employ the regression activation mapping (RAM) framework to visualize which timestamps were relevant to predict the respective VO$_{2}$max value.

% For the 60 participants in the testset, we plot all the RAM output for the same participants using \systemname{} and other baselines such as DeepCoral, DANN, and TF on their raw accelerometer data. The color of each  timestep in ~\ref{fig:udama_dann} represents the extracted salience weight.
% First, we demonstrate the general interpretability for each method, as shown in Figure~\ref{fig:ram_general}. 
% \imt{To provide a comprehensive view of the model interpretability, Figure~\ref{fig:ram_general} showcases the interpretability of the model for the entire test set. After performing RAM on each participant, we aggregate the mean of each timestep and display the density plot for 600 timesteps. The results in Figure~\ref{fig:ram_general} indicate that UDAMA can better differentiate the importance of each timestep compared to other methods, as evidenced by its wider range and lower density.
% }
% In contrast, other methods rate each timestep nearly equally for all the participants and manifest in low accuracy.


% %%%heatmap ; explain the weight? the value... 1/600 = 0.00167
% Notably, for the detailed salient map for a specific sample, we observe that in Figure~\ref{fig:udama_dann}, the regions in \systemname{} that contribute to estimations are distinct compared with other baseline methods. Furthermore, \systemname{} can capture the saliency of temporal features, which seems to help the model predict the fitness score more accurately. Specifically, we examine RAM plots of 4 participants to indicate that \systemname{} digest more timesteps during training. Therefore, these results demonstrate that \systemname{} has better interpretability and robustness than TF and other baseline methods. 



