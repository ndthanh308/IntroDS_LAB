\section{Methods}
This work introduces a novel unsupervised domain adaptation framework that utilizes multi-discriminator during adversarial training. The overall model architecture and multi-discriminator training scheme are shown in Figure~\ref{fig:workflow}. 
Herein, in this section, we formally discuss the problem formulation (\S\ref{3.1}) and details of our framework, which includes the first-step pre-training and second-step multi-discriminator domain adaptation training (\S\ref{3.3}). 

\subsection{Problem Formulation and Notation}\label{3.1}
Here, we denote \(\bm{D_s}\) as the source domain containing silver-standard labels and \(\bm{D_t}\) as the target domain with gold-standard labels, as shown in Figure~\ref{fig:workflow}. 
For each domain, we assume the data as \bm{$X = (x_1, ..., x_n) \in \mathbb{R}^{N\times T \times F}$} corresponds to the accelerometer and Electrocardiogram (ECG) data from a chest ECG device \imt{
and a target regression VO$_2$max labels \bm{$y = (y_1, ..., y_n)\in \mathbb{R}^{N}$}. 
}
%\bm{$x[n]$} corresponds to a specific subject with \bm{$T$} length and \bm{$F$} features. 
Additionally, we take into account contextual information such as the height or weight as metadata \(\bm{M = (m_1, ..., m_n)\in \mathbb{R}^{N \times F} }\). \imt{For the input data $\bm{X}$ and $\bm{M}$, \bm{$N$} represents the number of samples/subjects, \bm{$T$} represents the length of input sequences, and \bm{$F$} represents the number of input features.   }
Besides, \imt{
We use coarse- and fine-grained domain labels to train our model during adaptation and utilize multiple discriminators to differentiate between them. In particular, \bm{$y_c = (y_c[1], ..., (y_c[n])$} is the categorical value representing the coarse-grained binary domain label. \bm{$y_d = (y_d[1], ..., (y_d[n])$} is the numerical value that denotes the fine-grained domain distribution label. 
}
The coarse-grained domain discriminator is \(\bm{D_c}\) and the fine-grained domain discriminator is \(\bm{D_f}\). Also, for the training process, we denote the feature encoder with \(\bm{E}\) and the regression predictor with \(\bm{G_y}\). 
The overall networks thus can be represented as \(\bm{\hat{y_c}= D_c \cdot E }\), \(\bm{\hat{y_d} = D_f \cdot E }\) and \(\bm{\hat{y} = G_y \cdot E }\). The full table is shown in Table~\ref{tab:notation}.

\imt{\subsection{Unsupervised Domain Adaptation and Multi-discriminator Adversarial Training} \label{3.3}}
Domain adaptation is a method for learning a mapping between domains with distinct distributions, including data distribution shifts such as covariate shift, conditional shift, and label distribution ~\citep{da_survey}. In this paper, we propose \systemname{}, the unsupervised domain adversarial training, to address the label distribution shift problem,
particularly when the source domain contains numerous noisy labels. 
% To achieve this goal, we develop multi-discriminators to distinguish the sample domain information, force the feature extractor to generate more domain-invariant features, and boost the predictor's learning performance. In particular, \systemname{} contains a coarse-grained discriminator and a fine-grained discriminator to discriminate domain labels.


\imt{As shown in Figure~\ref{fig:workflow}, after pre-training on source domain with large-scale silver-standard labels, we first incorporate part of prior knowledge from the \(\bm{D_s}\) to create the adversarial training environment. Then we use the mixed silver-standard and gold-standard data to train the predictors and discriminator during the adaptation phase.}

\imt{
In particular, the adversarial training process consists of an encoder ($\bm{E}$), a VO2max label predictor($\bm{G_y}$), and two domain classifiers/discriminators ($\bm{D}$) designed for label shift problems by distinguishing both the domain and domain distribution information. During training, the fine- ($\bm{D_f}$)and coarse-grained ($\bm{D_c}$) discriminators are first optimized to identify the domain of each sample (i.e., max{$\bm{D_f,D_c}$}). In adversarial, the label predictor and encoder are then optimized to predict continuous fitness values from encoding(i.e., min{$\bm{E, G_y}$}).  The above-mentioned adversarial process will finally achieve the trade-off (i.e., the best prediction result in the most difficult-to-distinguish domain). 
}




\subsubsection{Coarse-grained discriminator}
The coarse-grained discriminator (\(\bm{D_c}\)) is similar to other DAs~\citep{Mathelin-2020,Zhao-2018} and follows the DANN~\citep{Ganin-2015} style. In other words, \(\bm{D_c}\) aims to discriminate the source of each data point \imt{in the mixture of pre-training and target labeled data} as a binary classification task, where 0 represents the data comes from the \(\bm{D_s}\) and 1 from \(\bm{D_t}\). In specific, After getting the representation matrix by fine-tuned feature extractor, two fully connected layers with the corresponding activation in \(\bm{D_c}\) are used to discriminate the rough binary domains labels and predict a probability vector \(\bm{\hat{y_c}}\). Let \(\bm{\hat{Y_c} = {\hat{y_c}[n]}}\) denote the predicted probability vectors for all the data points in (\(\bm{D_t}\)). The classification loss of the coarse-grained discriminator is defined as:
\begin{equation}
L_{CSE} = \sum_{N}l_c(y_c[n], \hat{{y_c}}[n])
\end{equation}
where \(l_c\) is the cross entropy loss of a single data point, and by optimizing \(\bm{L_{CSE}}\) for \(\bm{D_c}\), we can force the extractor to learn a general feature by maximizing such divergence.

\subsubsection{Fine-grained discriminator}

However, a simple binary classification task cannot properly represent the domain label distribution. Therefore, we augment adversarial training with a fine-grained discriminator (\(\bm{D_f}\)) to discriminate distribution differences. Specifically, instead of generating the binary domain labels (0 or 1) for the source or target domain for each sample, we construct a more complex pseudo-label (\(\bm{y_d}\)) to represent its domain label distribution. Based on our observation of the health outcome labels, which conform to the Gaussian distribution, as shown in Figure~\ref{distribution}, we then assign the (\(\bm{y_d}\)) for each training sample \imt{during adaptation}.
\imt{Specifically, $\bm{y_d}$ represents the mean and variance of the regression label distribution. This label is based on whether the sample is from the pre-training or target domain.
}
Therefore, after generating the feature matrix using \(\bm{E}\), \(\bm{D_f}\) is designed to distinguish the mean and variance of the label distribution using two fully connected layers with the corresponding activation. Let \(\bm{\hat{Y_d} = {\hat{y_d}}[n]}\) denote the predicted probability vectors for all the data points in (\(\bm{D_t}\)). Then, the loss of the fine-grained discriminator is defined as:
\begin{equation}
L_{GLL} = \sum_{N}l_g(y_d[n], \hat{{y_d}}[n])
\end{equation}
where \(l_g\) is each data point's Gaussian Negative Log-Likelihood (GLL) loss. In particular, GLL optimizes the mean and variance of a distribution and thus further maximizes the nuance changes among the sample and updates the discriminator.


After that, \(D_c\) and \(D_f\) can maximize the difference between the source and target domains for the multiple-domain discriminator training scheme. Meanwhile, the encoder and the predictor try to maximize the correct prediction of \(y\). These two modules play two games and finally reach a balance during the training. As a result, the encoder and predictor can learn a representation that cannot tell the difference between the source and target domains after the training is converged.


\subsubsection{Objective functions and training}
For adversarial training, the shared encoder first leverages the pre-trained model and extract general feature. Then discriminators are trained simultaneously to differentiate the domain labels. The predictor is used for the regression healthcare outcomes prediction task, and a mean squared error loss \(\bm{L_{MSE}}\) is applied to optimize the \(\bm{G_y}\). The detailed training overflow and input for each module are shown in Figure~\ref{fig:train_overflow}. Finally, the whole framework can be optimized by the total loss 
\(\bm{L}\), which is defined as:
\begin{equation}
L = \alpha L_{MSE} - \lambda_1L_{CSE} -\lambda_2L_{GLL}
\end{equation}
% \hj{loss notation is non-identical with the description}\yw{fixed}
We optimize the overall loss \(\bm{L}\) to minimize the predictor loss while maximizing the loss of domain discriminators. In detail, $\alpha$ is used to scale down the predictor loss to the same level as predictors, $\lambda_1$ and $\lambda_2$ control the relative weight of the discriminator loss, and $\lambda_1$ + $\lambda_2$ = 1. Hyperparameter choice details are discussed in Section~\S\ref{experiment}.








