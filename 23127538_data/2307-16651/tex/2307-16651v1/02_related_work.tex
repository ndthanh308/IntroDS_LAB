\section{Related Work}
% This section briefly covers the current study for applying machine learning methods to the VO$_{2}$max prediction task (\S\ref{2.1}). Next, we discuss the related work in transfer learning (\S\ref{2.2}) and domain adaptation (\S\ref{2.3}). Finally, we discuss the limitations of prior work and how our work was motivated and built upon some of these ideas.


\subsection{Cardio-fitness estimation}\label{2.1}
% \cm{do we cite dimitris' paper? i dont see it here}\yw{fixed}
% \cm{in this section we talk about vo2max and in the intro of CRF..do you ever link the two?}\yw{fixed}
Numerous prediction models have been developed recently using different testing schemes (submaximal exercise / non-exercise tests) and a variety of machine learning methods~\citep{vo2max_ml,vo2max_ml2} to substitute direct measurements of VO$_{2}$max, \mlhc{the direct indicator of CRF}. Specifically, with the help of modern wearable technology, which can track physical activity, resting heart rate (RHR), and other biosignals, various silver-standard methods have emerged for more convenient VO$_{2}$max calculation without maximal exercise testing~\citep{wearble-vo2max,vo2max-non_2,vo2max-non_1,lack-vali-1,lack-vali-3}.
\mlhc{For example, deep learning models are utilized to predict fitness prediction converting raw wearable sensor data~\citep{Spathis-2022}. Nonetheless, these approaches overlook the impact of silver-standard labels generated from imprecise testing schemes, which can result in diminished model performance and untrustworthy fitness forecasts.}
Recently, a few works applying machine learning models on CRF predicting using gold-standard VO$_{2}$max labels have been proposed~\citep{Abut-2015}.
% \hj{discuss other applications as well?}.
Nonetheless, they mainly test on small cohorts, which might lead to poor generalization performance. 
% \cm{this sentence is unclear...what do you want to say?}\yv{fixed}
This paper proposes a novel DL method that aims to alleviate the distribution difference between imprecise silver-standard and gold-standard data for better fitness prediction (Results are discussed in \S\ref{6}).

\subsection{Domain adaptation}\label{2.3}
% \yw{add the self-training}
A domain combines the input population with the output following a certain probability distribution~\citep{Kouw-2018}. DA is one of the state-of-the-art solutions~\citep{Hoffman-2017} for learning information from an abundant labeled source domain and applying it to a new and unseen target domain with a different distribution. DA trains a feature extractor to learn the shared information across domains, so the model can generalize to new settings and thus mitigate the domain shift~\citep{Kouw-2018}. Discrepancy-based and adversarial-based DA approaches are effective among DA methods.

\subsubsection{Discrepancy-based method}
For discrepancy-based approaches, the network typically shares or reuses the initial layers between the source and target domains and aims to reduce feature space divergence~\citep{da_survey}. The maximum mean discrepancy (MMD)~\citep{mmd} is an effective distance metric that measures the distribution divergence between the mean embeddings of two distributions in the reproducing kernel Hilbert space (RKHS) to minimize the difference between two distributions. Many methods~\citep{mmd_1,mmd_2} utilize the MMD metric within the network to learn domain-invariant and discriminative representations. On the other hand, the correlation alignment (CORAL) ~\citep{coral} method is proposed to align the second-order statistics of the source and target distribution with a linear transformation. Deep-Coral~\citep{coral} is an extension of CORAL that can train a non-linear transformation to align the correlations of the representation embedding in deep neural networks from the source domain to the destination domain. Besides, some works minimize empirical Wasserstein distance between source and target feature representations and also showed good results in domain-invariant representation learning approaches~\citep{wdgrl}. Although these methods are effective and easily incorporated into deep neural networks, they are mainly designed for feature-based distribution shift alignment. Unlike most discrepancy-based methods, our method mainly addresses label distribution shift problems when the source domain has large-scale noisy labels.




\subsubsection{Adversarial-based DA}
Most adversarial-based methods are motivated by theory suggesting that a good cross-domain representation contains no discriminative information about the origin of the input and shows good performance to reduce domain discrepancy~\citep{Mei-2018}. Among these methods, Domain-Adversarial Neural Network (DANN)~\citep{Ganin-2015} first introduces adversarial training to domain adaptation. 
DANN utilizes a shared feature extractor to learn feature embedding and a discriminator to maximize the domain difference. After the convergence of the whole training, general features are learned for each input while their domain information cannot be discriminated. Followed by this, Domain Separation Networks (DSN)~\citep{Bousmalis-2016} and Adversarial Discriminative Domain Adaptation (ADDA)~\citep{Tzeng-2017} propose more sophisticated feature extraction methods using both shared and private feature extractors for the task classifier and domain discriminator. Moreover, Dual Adversarial Domain Adaptation (DADA)~\citep{Du-2020} uses two discriminators to pit against each other to aid the discriminative training. 



The aforementioned methods mainly focus on discriminating domains as a coarse binary classification task while neglecting the rich information of the domain distribution. Moreover, the majority of applications of DA mainly focus on medical image segmentation and classification tasks ~\citep{Perone-2018, Mei-2020, Venkataramani-2018}. 
Our work aims to resolve the label distribution shift while \imt{keeping the input data constant for regression targets}.
We propose a novel DA framework (\systemname{}) with multi-discriminators to learn the coarse-grained and fine-grained domain information and validate it on a CRF prediction task.

% Figure environment removed


