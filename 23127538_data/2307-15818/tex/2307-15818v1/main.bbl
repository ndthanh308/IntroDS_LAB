\begin{thebibliography}{87}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn,
  Gopalakrishnan, Hausman, Herzog, et~al.]{ahn2022can}
M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn,
  K.~Gopalakrishnan, K.~Hausman, A.~Herzog, et~al.
\newblock Do as {I} can, not as {I} say: Grounding language in robotic
  affordances.
\newblock \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc,
  A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.14198}, 2022.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,
  Taropa, Bailey, Chen, et~al.]{anil2023palm}
R.~Anil, A.~M. Dai, O.~Firat, M.~Johnson, D.~Lepikhin, A.~Passos, S.~Shakeri,
  E.~Taropa, P.~Bailey, Z.~Chen, et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Brohan et~al.(2022)Brohan, Brown, Carbajal, Chebotar, Dabis, Finn,
  Gopalakrishnan, Hausman, Herzog, Hsu, et~al.]{brohan2022rt}
A.~Brohan, N.~Brown, J.~Carbajal, Y.~Chebotar, J.~Dabis, C.~Finn,
  K.~Gopalakrishnan, K.~Hausman, A.~Herzog, J.~Hsu, et~al.
\newblock Rt-1: Robotics transformer for real-world control at scale.
\newblock \emph{arXiv preprint arXiv:2212.06817}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Cer et~al.(2018)Cer, Yang, Kong, Hua, Limtiaco, John, Constant,
  Guajardo{-}Cespedes, Yuan, Tar, Sung, Strope, and Kurzweil]{kona2018cer}
D.~Cer, Y.~Yang, S.~Kong, N.~Hua, N.~Limtiaco, R.~S. John, N.~Constant,
  M.~Guajardo{-}Cespedes, S.~Yuan, C.~Tar, Y.~Sung, B.~Strope, and R.~Kurzweil.
\newblock Universal sentence encoder.
\newblock \emph{CoRR}, abs/1803.11175, 2018.
\newblock URL \url{http://arxiv.org/abs/1803.11175}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,
  Y.~Burda, N.~Joseph, G.~Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Djolonga, Padlewski, Mustafa,
  Changpinyo, Wu, Ruiz, Goodman, Wang, Tay, Shakeri, Dehghani, Salz, Lucic,
  Tschannen, Nagrani, Hu, Joshi, Pang, Montgomery, Pietrzyk, Ritter,
  Piergiovanni, Minderer, Pavetic, Waters, Li, Alabdulmohsin, Beyer, Amelot,
  Lee, Steiner, Li, Keysers, Arnab, Xu, Rong, Kolesnikov, Seyedhosseini,
  Angelova, Zhai, Houlsby, and Soricut]{chen2023palix}
X.~Chen, J.~Djolonga, P.~Padlewski, B.~Mustafa, S.~Changpinyo, J.~Wu, C.~R.
  Ruiz, S.~Goodman, X.~Wang, Y.~Tay, S.~Shakeri, M.~Dehghani, D.~Salz,
  M.~Lucic, M.~Tschannen, A.~Nagrani, H.~Hu, M.~Joshi, B.~Pang, C.~Montgomery,
  P.~Pietrzyk, M.~Ritter, A.~Piergiovanni, M.~Minderer, F.~Pavetic, A.~Waters,
  G.~Li, I.~Alabdulmohsin, L.~Beyer, J.~Amelot, K.~Lee, A.~P. Steiner, Y.~Li,
  D.~Keysers, A.~Arnab, Y.~Xu, K.~Rong, A.~Kolesnikov, M.~Seyedhosseini,
  A.~Angelova, X.~Zhai, N.~Houlsby, and R.~Soricut.
\newblock Pali-x: On scaling up a multilingual vision and language model,
  2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Wang, Changpinyo, Piergiovanni,
  Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, Kolesnikov, Puigcerver,
  Ding, Rong, Akbari, Mishra, Xue, Thapliyal, Bradbury, Kuo, Seyedhosseini,
  Jia, Ayan, Riquelme, Steiner, Angelova, Zhai, Houlsby, and
  Soricut]{chen2023pali}
X.~Chen, X.~Wang, S.~Changpinyo, A.~Piergiovanni, P.~Padlewski, D.~Salz,
  S.~Goodman, A.~Grycner, B.~Mustafa, L.~Beyer, A.~Kolesnikov, J.~Puigcerver,
  N.~Ding, K.~Rong, H.~Akbari, G.~Mishra, L.~Xue, A.~Thapliyal, J.~Bradbury,
  W.~Kuo, M.~Seyedhosseini, C.~Jia, B.~K. Ayan, C.~Riquelme, A.~Steiner,
  A.~Angelova, X.~Zhai, N.~Houlsby, and R.~Soricut.
\newblock Pali: A jointly-scaled multilingual language-image model,
  2023{\natexlab{b}}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert,
  J.~Tworek, J.~Hilton, R.~Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Cui et~al.(2022)Cui, Wang, Muhammad, Pinto, et~al.]{cui2022play}
Z.~J. Cui, Y.~Wang, N.~Muhammad, L.~Pinto, et~al.
\newblock From play to policy: Conditional behavior generation from uncurated
  robot data.
\newblock \emph{arXiv preprint arXiv:2210.10047}, 2022.

\bibitem[Dasari and Gupta(2021)]{dasari2021transformers}
S.~Dasari and A.~Gupta.
\newblock Transformers for one-shot visual imitation.
\newblock In \emph{Conference on Robot Learning}, pages 2071--2084. PMLR, 2021.

\bibitem[Dasari et~al.(2019)Dasari, Ebert, Tian, Nair, Bucher, Schmeckpeper,
  Singh, Levine, and Finn]{dasari2019robonet}
S.~Dasari, F.~Ebert, S.~Tian, S.~Nair, B.~Bucher, K.~Schmeckpeper, S.~Singh,
  S.~Levine, and C.~Finn.
\newblock Robonet: Large-scale multi-robot learning.
\newblock In \emph{Conference on Robot Learning}, 2019.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, Jenatton, Beyer, Tschannen,
  Arnab, Wang, Riquelme, Minderer, Puigcerver, Evci, Kumar, van Steenkiste,
  Elsayed, Mahendran, Yu, Oliver, Huot, Bastings, Collier, Gritsenko, Birodkar,
  Vasconcelos, Tay, Mensink, Kolesnikov, Pavetić, Tran, Kipf, Lučić, Zhai,
  Keysers, Harmsen, and Houlsby]{dehghani2023scaling}
M.~Dehghani, J.~Djolonga, B.~Mustafa, P.~Padlewski, J.~Heek, J.~Gilmer,
  A.~Steiner, M.~Caron, R.~Geirhos, I.~Alabdulmohsin, R.~Jenatton, L.~Beyer,
  M.~Tschannen, A.~Arnab, X.~Wang, C.~Riquelme, M.~Minderer, J.~Puigcerver,
  U.~Evci, M.~Kumar, S.~van Steenkiste, G.~F. Elsayed, A.~Mahendran, F.~Yu,
  A.~Oliver, F.~Huot, J.~Bastings, M.~P. Collier, A.~Gritsenko, V.~Birodkar,
  C.~Vasconcelos, Y.~Tay, T.~Mensink, A.~Kolesnikov, F.~Pavetić, D.~Tran,
  T.~Kipf, M.~Lučić, X.~Zhai, D.~Keysers, J.~Harmsen, and N.~Houlsby.
\newblock Scaling vision transformers to 22 billion parameters, 2023.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter,
  Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm}
D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid,
  J.~Tompson, Q.~Vuong, T.~Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock \emph{arXiv preprint arXiv:2303.03378}, 2023.

\bibitem[Du et~al.(2023{\natexlab{a}})Du, Nair, Sadigh, and
  Finn]{du2023behavior}
M.~Du, S.~Nair, D.~Sadigh, and C.~Finn.
\newblock Behavior retrieval: Few-shot imitation learning by querying unlabeled
  datasets.
\newblock \emph{arXiv preprint arXiv:2304.08742}, 2023{\natexlab{a}}.

\bibitem[Du et~al.(2023{\natexlab{b}})Du, Konyushkova, Denil, Raju, Landon,
  Hill, de~Freitas, and Cabi]{du2023vision}
Y.~Du, K.~Konyushkova, M.~Denil, A.~Raju, J.~Landon, F.~Hill, N.~de~Freitas,
  and S.~Cabi.
\newblock Vision-language models as success detectors.
\newblock \emph{arXiv preprint arXiv:2303.07280}, 2023{\natexlab{b}}.

\bibitem[Finn and Levine(2017)]{finn2017deep}
C.~Finn and S.~Levine.
\newblock Deep visual foresight for planning robot motion.
\newblock In \emph{2017 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 2786--2793. IEEE, 2017.

\bibitem[Finn et~al.(2017)Finn, Yu, Zhang, Abbeel, and Levine]{finn2017one}
C.~Finn, T.~Yu, T.~Zhang, P.~Abbeel, and S.~Levine.
\newblock One-shot visual imitation learning via meta-learning.
\newblock In \emph{Conference on robot learning}, pages 357--368. PMLR, 2017.

\bibitem[Fisher(1936)]{fisher1936design}
R.~A. Fisher.
\newblock Design of experiments.
\newblock \emph{British Medical Journal}, 1\penalty0 (3923):\penalty0 554,
  1936.

\bibitem[Gadre et~al.(2022)Gadre, Wortsman, Ilharco, Schmidt, and
  Song]{gadre2022clip}
S.~Y. Gadre, M.~Wortsman, G.~Ilharco, L.~Schmidt, and S.~Song.
\newblock Clip on wheels: Zero-shot object navigation as object localization
  and exploration.
\newblock \emph{arXiv preprint arXiv:2203.10421}, 2022.

\bibitem[Gan et~al.(2022)Gan, Li, Li, Wang, Liu, Gao, et~al.]{gan2022vision}
Z.~Gan, L.~Li, C.~Li, L.~Wang, Z.~Liu, J.~Gao, et~al.
\newblock Vision-language pre-training: Basics, recent advances, and future
  trends.
\newblock \emph{Foundations and Trends{\textregistered} in Computer Graphics
  and Vision}, 14\penalty0 (3--4):\penalty0 163--352, 2022.

\bibitem[Ghiasi et~al.(2021)Ghiasi, Gu, Cui, and Lin]{ghiasi2021open}
G.~Ghiasi, X.~Gu, Y.~Cui, and T.-Y. Lin.
\newblock Open-vocabulary image segmentation.
\newblock \emph{arXiv preprint arXiv:2112.12143}, 2021.

\bibitem[Grauman et~al.(2022)Grauman, Westbury, Byrne, Chavis, Furnari,
  Girdhar, Hamburger, Jiang, Liu, Liu, Martin, Nagarajan, Radosavovic,
  Ramakrishnan, Ryan, Sharma, Wray, Xu, Xu, Zhao, Bansal, Batra, Cartillier,
  Crane, Do, Doulaty, Erapalli, Feichtenhofer, Fragomeni, Fu, Gebreselasie,
  Gonzalez, Hillis, Huang, Huang, Jia, Khoo, Kolar, Kottur, Kumar, Landini, Li,
  Li, Li, Mangalam, Modhugu, Munro, Murrell, Nishiyasu, Price, Puentes,
  Ramazanova, Sari, Somasundaram, Southerland, Sugano, Tao, Vo, Wang, Wu, Yagi,
  Zhao, Zhu, Arbelaez, Crandall, Damen, Farinella, Fuegen, Ghanem, Ithapu,
  Jawahar, Joo, Kitani, Li, Newcombe, Oliva, Park, Rehg, Sato, Shi, Shou,
  Torralba, Torresani, Yan, and Malik]{grauman2022ego4d}
K.~Grauman, A.~Westbury, E.~Byrne, Z.~Chavis, A.~Furnari, R.~Girdhar,
  J.~Hamburger, H.~Jiang, M.~Liu, X.~Liu, M.~Martin, T.~Nagarajan,
  I.~Radosavovic, S.~K. Ramakrishnan, F.~Ryan, J.~Sharma, M.~Wray, M.~Xu, E.~Z.
  Xu, C.~Zhao, S.~Bansal, D.~Batra, V.~Cartillier, S.~Crane, T.~Do, M.~Doulaty,
  A.~Erapalli, C.~Feichtenhofer, A.~Fragomeni, Q.~Fu, A.~Gebreselasie,
  C.~Gonzalez, J.~Hillis, X.~Huang, Y.~Huang, W.~Jia, W.~Khoo, J.~Kolar,
  S.~Kottur, A.~Kumar, F.~Landini, C.~Li, Y.~Li, Z.~Li, K.~Mangalam,
  R.~Modhugu, J.~Munro, T.~Murrell, T.~Nishiyasu, W.~Price, P.~R. Puentes,
  M.~Ramazanova, L.~Sari, K.~Somasundaram, A.~Southerland, Y.~Sugano, R.~Tao,
  M.~Vo, Y.~Wang, X.~Wu, T.~Yagi, Z.~Zhao, Y.~Zhu, P.~Arbelaez, D.~Crandall,
  D.~Damen, G.~M. Farinella, C.~Fuegen, B.~Ghanem, V.~K. Ithapu, C.~V. Jawahar,
  H.~Joo, K.~Kitani, H.~Li, R.~Newcombe, A.~Oliva, H.~S. Park, J.~M. Rehg,
  Y.~Sato, J.~Shi, M.~Z. Shou, A.~Torralba, L.~Torresani, M.~Yan, and J.~Malik.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video, 2022.

\bibitem[Gu et~al.(2021)Gu, Lin, Kuo, and Cui]{gu2021open}
X.~Gu, T.-Y. Lin, W.~Kuo, and Y.~Cui.
\newblock Open-vocabulary object detection via vision and language knowledge
  distillation.
\newblock \emph{arXiv preprint arXiv:2104.13921}, 2021.

\bibitem[Hansen et~al.(2020)Hansen, Jangir, Sun, Aleny{\`a}, Abbeel, Efros,
  Pinto, and Wang]{hansen2020self}
N.~Hansen, R.~Jangir, Y.~Sun, G.~Aleny{\`a}, P.~Abbeel, A.~A. Efros, L.~Pinto,
  and X.~Wang.
\newblock Self-supervised policy adaptation during deployment.
\newblock \emph{arXiv preprint arXiv:2007.04309}, 2020.

\bibitem[Hao et~al.(2022)Hao, Song, Dong, Huang, Chi, Wang, Ma, and
  Wei]{hao2022language}
Y.~Hao, H.~Song, L.~Dong, S.~Huang, Z.~Chi, W.~Wang, S.~Ma, and F.~Wei.
\newblock Language models are general-purpose interfaces.
\newblock \emph{arXiv preprint arXiv:2206.06336}, 2022.

\bibitem[Hill et~al.(2020)Hill, Mokra, Wong, and Harley]{hill2020human}
F.~Hill, S.~Mokra, N.~Wong, and T.~Harley.
\newblock Human instruction-following with deep reinforcement learning via
  transfer-learning from text.
\newblock \emph{arXiv preprint arXiv:2005.09382}, 2020.

\bibitem[Huang et~al.(2023)Huang, Dong, Wang, Hao, Singhal, Ma, Lv, Cui,
  Mohammed, Liu, et~al.]{huang2023language}
S.~Huang, L.~Dong, W.~Wang, Y.~Hao, S.~Singhal, S.~Ma, T.~Lv, L.~Cui, O.~K.
  Mohammed, Q.~Liu, et~al.
\newblock Language is not all you need: Aligning perception with language
  models.
\newblock \emph{arXiv preprint arXiv:2302.14045}, 2023.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and
  Mordatch]{huang2022language}
W.~Huang, P.~Abbeel, D.~Pathak, and I.~Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable
  knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pages
  9118--9147. PMLR, 2022.

\bibitem[James et~al.(2018)James, Bloesch, and Davison]{james2018task}
S.~James, M.~Bloesch, and A.~J. Davison.
\newblock Task-embedded control networks for few-shot imitation learning.
\newblock In \emph{Conference on robot learning}, pages 783--795. PMLR, 2018.

\bibitem[Jang et~al.(2021)Jang, Irpan, Khansari, Kappler, Ebert, Lynch, Levine,
  and Finn]{jang2022bc}
E.~Jang, A.~Irpan, M.~Khansari, D.~Kappler, F.~Ebert, C.~Lynch, S.~Levine, and
  C.~Finn.
\newblock Bc-z: Zero-shot task generalization with robotic imitation learning.
\newblock In \emph{Conference on Robot Learning}, pages 991--1002. PMLR, 2021.

\bibitem[Jiang et~al.(2022)Jiang, Gupta, Zhang, Wang, Dou, Chen, Fei-Fei,
  Anandkumar, Zhu, and Fan]{jiang2022vima}
Y.~Jiang, A.~Gupta, Z.~Zhang, G.~Wang, Y.~Dou, Y.~Chen, L.~Fei-Fei,
  A.~Anandkumar, Y.~Zhu, and L.~Fan.
\newblock Vima: General robot manipulation with multimodal prompts.
\newblock \emph{arXiv preprint arXiv:2210.03094}, 2022.

\bibitem[Kaelbling(2020)]{kaelbling2020foundation}
L.~P. Kaelbling.
\newblock The foundation of efficient robot learning.
\newblock \emph{Science}, 369\penalty0 (6506):\penalty0 915--916, 2020.

\bibitem[Karamcheti et~al.(2023)Karamcheti, Nair, Chen, Kollar, Finn, Sadigh,
  and Liang]{karamcheti2023language}
S.~Karamcheti, S.~Nair, A.~S. Chen, T.~Kollar, C.~Finn, D.~Sadigh, and
  P.~Liang.
\newblock Language-driven representation learning for robotics.
\newblock \emph{arXiv preprint arXiv:2302.12766}, 2023.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson,
  Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
A.~Kirillov, E.~Mintun, N.~Ravi, H.~Mao, C.~Rolland, L.~Gustafson, T.~Xiao,
  S.~Whitehead, A.~C. Berg, W.-Y. Lo, et~al.
\newblock Segment anything.
\newblock \emph{arXiv preprint arXiv:2304.02643}, 2023.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{kostrikov2020image}
I.~Kostrikov, D.~Yarats, and R.~Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{arXiv preprint arXiv:2004.13649}, 2020.

\bibitem[Laskin et~al.(2020{\natexlab{a}})Laskin, Lee, Stooke, Pinto, Abbeel,
  and Srinivas]{laskin2020reinforcement}
M.~Laskin, K.~Lee, A.~Stooke, L.~Pinto, P.~Abbeel, and A.~Srinivas.
\newblock Reinforcement learning with augmented data.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 19884--19895, 2020{\natexlab{a}}.

\bibitem[Laskin et~al.(2020{\natexlab{b}})Laskin, Srinivas, and
  Abbeel]{laskin2020curl}
M.~Laskin, A.~Srinivas, and P.~Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5639--5650. PMLR, 2020{\natexlab{b}}.

\bibitem[Levine et~al.(2018)Levine, Pastor, Krizhevsky, Ibarz, and
  Quillen]{levine2018learning}
S.~Levine, P.~Pastor, A.~Krizhevsky, J.~Ibarz, and D.~Quillen.
\newblock Learning hand-eye coordination for robotic grasping with deep
  learning and large-scale data collection.
\newblock \emph{The International journal of robotics research}, 37\penalty0
  (4-5):\penalty0 421--436, 2018.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski,
  Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving}
A.~Lewkowycz, A.~Andreassen, D.~Dohan, E.~Dyer, H.~Michalewski, V.~Ramasesh,
  A.~Slone, C.~Anil, I.~Schlag, T.~Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock \emph{arXiv preprint arXiv:2206.14858}, 2022.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock \emph{arXiv preprint arXiv:2301.12597}, 2023.

\bibitem[Li et~al.(2019)Li, Yatskar, Yin, Hsieh, and Chang]{li2019visualbert}
L.~H. Li, M.~Yatskar, D.~Yin, C.-J. Hsieh, and K.-W. Chang.
\newblock Visualbert: A simple and performant baseline for vision and language.
\newblock \emph{arXiv preprint arXiv:1908.03557}, 2019.

\bibitem[Liu et~al.(2022)Liu, Lee, Lee, and Abbeel]{liu2022instruction}
H.~Liu, L.~Lee, K.~Lee, and P.~Abbeel.
\newblock Instruction-following agents with jointly pre-trained vision-language
  models.
\newblock \emph{arXiv preprint arXiv:2210.13431}, 2022.

\bibitem[Lu et~al.(2019)Lu, Batra, Parikh, and Lee]{lu2019vilbert}
J.~Lu, D.~Batra, D.~Parikh, and S.~Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lynch and Sermanet(2020)]{lynch2020language}
C.~Lynch and P.~Sermanet.
\newblock Language conditioned imitation learning over unstructured data.
\newblock \emph{arXiv preprint arXiv:2005.07648}, 2020.

\bibitem[Lynch et~al.(2022)Lynch, Wahid, Tompson, Ding, Betker, Baruch,
  Armstrong, and Florence]{lynch2022interactive}
C.~Lynch, A.~Wahid, J.~Tompson, T.~Ding, J.~Betker, R.~Baruch, T.~Armstrong,
  and P.~Florence.
\newblock Interactive language: Talking to robots in real time.
\newblock \emph{arXiv preprint arXiv:2210.06407}, 2022.

\bibitem[Ma et~al.(2022)Ma, Sodhani, Jayaraman, Bastani, Kumar, and
  Zhang]{ma2022vip}
Y.~J. Ma, S.~Sodhani, D.~Jayaraman, O.~Bastani, V.~Kumar, and A.~Zhang.
\newblock Vip: Towards universal visual reward and representation via
  value-implicit pre-training.
\newblock \emph{arXiv preprint arXiv:2210.00030}, 2022.

\bibitem[Ma et~al.(2023)Ma, Liang, Som, Kumar, Zhang, Bastani, and
  Jayaraman]{ma2023liv}
Y.~J. Ma, W.~Liang, V.~Som, V.~Kumar, A.~Zhang, O.~Bastani, and D.~Jayaraman.
\newblock Liv: Language-image representations and rewards for robotic control.
\newblock \emph{arXiv preprint arXiv:2306.00958}, 2023.

\bibitem[Mahler et~al.(2017)Mahler, Liang, Niyaz, Laskey, Doan, Liu, Ojea, and
  Goldberg]{mahler2017dex}
J.~Mahler, J.~Liang, S.~Niyaz, M.~Laskey, R.~Doan, X.~Liu, J.~A. Ojea, and
  K.~Goldberg.
\newblock Dex-net 2.0: Deep learning to plan robust grasps with synthetic point
  clouds and analytic grasp metrics.
\newblock \emph{arXiv preprint arXiv:1703.09312}, 2017.

\bibitem[Majumdar et~al.(2023{\natexlab{a}})Majumdar, Yadav, Arnaud, Ma, Chen,
  Silwal, Jain, Berges, Abbeel, Malik, et~al.]{majumdar2023vc1}
A.~Majumdar, K.~Yadav, S.~Arnaud, Y.~J. Ma, C.~Chen, S.~Silwal, A.~Jain, V.-P.
  Berges, P.~Abbeel, J.~Malik, et~al.
\newblock Where are we in the search for an artificial visual cortex for
  embodied intelligence?
\newblock \emph{arXiv preprint arXiv:2303.18240}, 2023{\natexlab{a}}.

\bibitem[Majumdar et~al.(2023{\natexlab{b}})Majumdar, Yadav, Arnaud, Ma, Chen,
  Silwal, Jain, Berges, Abbeel, Malik, et~al.]{majumdar2023we}
A.~Majumdar, K.~Yadav, S.~Arnaud, Y.~J. Ma, C.~Chen, S.~Silwal, A.~Jain, V.-P.
  Berges, P.~Abbeel, J.~Malik, et~al.
\newblock Where are we in the search for an artificial visual cortex for
  embodied intelligence?
\newblock \emph{arXiv preprint arXiv:2303.18240}, 2023{\natexlab{b}}.

\bibitem[Mees et~al.(2022)Mees, Hermann, and Burgard]{mees2022matters}
O.~Mees, L.~Hermann, and W.~Burgard.
\newblock What matters in language conditioned robotic imitation learning over
  unstructured data.
\newblock \emph{IEEE Robotics and Automation Letters}, 7\penalty0 (4):\penalty0
  11205--11212, 2022.

\bibitem[Minderer et~al.(2022)Minderer, Gritsenko, Stone, Neumann, Weissenborn,
  Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, et~al.]{minderer2022simple}
M.~Minderer, A.~Gritsenko, A.~Stone, M.~Neumann, D.~Weissenborn,
  A.~Dosovitskiy, A.~Mahendran, A.~Arnab, M.~Dehghani, Z.~Shen, et~al.
\newblock Simple open-vocabulary object detection with vision transformers.
\newblock \emph{arXiv preprint arXiv:2205.06230}, 2022.

\bibitem[Mu et~al.(2023)Mu, Zhang, Hu, Wang, Ding, Jin, Wang, Dai, Qiao, and
  Luo]{mu2023embodiedgpt}
Y.~Mu, Q.~Zhang, M.~Hu, W.~Wang, M.~Ding, J.~Jin, B.~Wang, J.~Dai, Y.~Qiao, and
  P.~Luo.
\newblock Embodiedgpt: Vision-language pre-training via embodied chain of
  thought.
\newblock \emph{arXiv preprint arXiv:2305.15021}, 2023.

\bibitem[Nair et~al.(2022{\natexlab{a}})Nair, Mitchell, Chen, Savarese, Finn,
  et~al.]{nair2022learning}
S.~Nair, E.~Mitchell, K.~Chen, S.~Savarese, C.~Finn, et~al.
\newblock Learning language-conditioned robot behavior from offline data and
  crowd-sourced annotation.
\newblock In \emph{Conference on Robot Learning}, pages 1303--1315. PMLR,
  2022{\natexlab{a}}.

\bibitem[Nair et~al.(2022{\natexlab{b}})Nair, Rajeswaran, Kumar, Finn, and
  Gupta]{nair2022r3m}
S.~Nair, A.~Rajeswaran, V.~Kumar, C.~Finn, and A.~Gupta.
\newblock R3m: A universal visual representation for robot manipulation.
\newblock \emph{arXiv preprint arXiv:2203.12601}, 2022{\natexlab{b}}.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pari et~al.(2021)Pari, Shafiullah, Arunachalam, and
  Pinto]{pari2021surprising}
J.~Pari, N.~M. Shafiullah, S.~P. Arunachalam, and L.~Pinto.
\newblock The surprising effectiveness of representation learning for visual
  imitation.
\newblock \emph{arXiv preprint arXiv:2112.01511}, 2021.

\bibitem[Pinto and Gupta(2016)]{pinto2016supersizing}
L.~Pinto and A.~Gupta.
\newblock Supersizing self-supervision: Learning to grasp from 50k tries and
  700 robot hours.
\newblock In \emph{2016 IEEE international conference on robotics and
  automation (ICRA)}, pages 3406--3413. IEEE, 2016.

\bibitem[Polu et~al.(2022)Polu, Han, Zheng, Baksys, Babuschkin, and
  Sutskever]{polu2022formal}
S.~Polu, J.~M. Han, K.~Zheng, M.~Baksys, I.~Babuschkin, and I.~Sutskever.
\newblock Formal mathematics statement curriculum learning.
\newblock \emph{arXiv preprint arXiv:2202.01344}, 2022.

\bibitem[Pong et~al.(2019)Pong, Dalal, Lin, Nair, Bahl, and
  Levine]{pong2019skew}
V.~H. Pong, M.~Dalal, S.~Lin, A.~Nair, S.~Bahl, and S.~Levine.
\newblock Skew-fit: State-covering self-supervised reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1903.03698}, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, et~al.]{reed2022generalist}
S.~Reed, K.~Zolna, E.~Parisotto, S.~G. Colmenarejo, A.~Novikov, G.~Barth-Maron,
  M.~Gimenez, Y.~Sulsky, J.~Kay, J.~T. Springenberg, et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint arXiv:2205.06175}, 2022.

\bibitem[Ryoo et~al.(2021)Ryoo, Piergiovanni, Arnab, Dehghani, and
  Angelova]{ryoo2021tokenlearner}
M.~Ryoo, A.~Piergiovanni, A.~Arnab, M.~Dehghani, and A.~Angelova.
\newblock Tokenlearner: Adaptive space-time tokenization for videos.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12786--12797, 2021.

\bibitem[Shah et~al.(2023)Shah, Osi\'nski, ichter, and
  Levine]{pmlr-v205-shah23b}
D.~Shah, B.~Osi\'nski, b.~ichter, and S.~Levine.
\newblock Lm-nav: Robotic navigation with large pre-trained models of language,
  vision, and action.
\newblock In K.~Liu, D.~Kulic, and J.~Ichnowski, editors, \emph{Proceedings of
  The 6th Conference on Robot Learning}, volume 205 of \emph{Proceedings of
  Machine Learning Research}, pages 492--504. PMLR, 14--18 Dec 2023.
\newblock URL \url{https://proceedings.mlr.press/v205/shah23b.html}.

\bibitem[Shah and Kumar(2021)]{shah2021rrl}
R.~Shah and V.~Kumar.
\newblock Rrl: Resnet as representation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2107.03380}, 2021.

\bibitem[Shridhar et~al.(2021)Shridhar, Manuelli, and Fox]{shridhar2021cliport}
M.~Shridhar, L.~Manuelli, and D.~Fox.
\newblock Cliport: What and where pathways for robotic manipulation.
\newblock In \emph{Proceedings of the 5th Conference on Robot Learning (CoRL)},
  2021.

\bibitem[Shridhar et~al.(2022{\natexlab{a}})Shridhar, Manuelli, and
  Fox]{shridhar2022cliport}
M.~Shridhar, L.~Manuelli, and D.~Fox.
\newblock Cliport: What and where pathways for robotic manipulation.
\newblock In \emph{Conference on Robot Learning}, pages 894--906. PMLR,
  2022{\natexlab{a}}.

\bibitem[Shridhar et~al.(2022{\natexlab{b}})Shridhar, Manuelli, and
  Fox]{shridhar2022perceiver}
M.~Shridhar, L.~Manuelli, and D.~Fox.
\newblock Perceiver-actor: A multi-task transformer for robotic manipulation.
\newblock \emph{arXiv preprint arXiv:2209.05451}, 2022{\natexlab{b}}.

\bibitem[Singh et~al.(2023)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox,
  Thomason, and Garg]{singh2022progprompt}
I.~Singh, V.~Blukis, A.~Mousavian, A.~Goyal, D.~Xu, J.~Tremblay, D.~Fox,
  J.~Thomason, and A.~Garg.
\newblock Progprompt: Generating situated robot task plans using large language
  models.
\newblock In \emph{ICRA}, 2023.

\bibitem[Smith and Coles(1973)]{smith1973design}
M.~H. Smith and L.~S. Coles.
\newblock Design of a low cost, general purpose robot.
\newblock In \emph{IJCAI}, pages 324--336, 1973.

\bibitem[Stone et~al.(2023)Stone, Xiao, Lu, Gopalakrishnan, Lee, Vuong,
  Wohlhart, Zitkovich, Xia, Finn, et~al.]{stone2023open}
A.~Stone, T.~Xiao, Y.~Lu, K.~Gopalakrishnan, K.-H. Lee, Q.~Vuong, P.~Wohlhart,
  B.~Zitkovich, F.~Xia, C.~Finn, et~al.
\newblock Open-world object manipulation using pre-trained vision-language
  models.
\newblock \emph{arXiv preprint arXiv:2303.00905}, 2023.

\bibitem[Sumers et~al.(2023)Sumers, Marino, Ahuja, Fergus, and
  Dasgupta]{sumers2023distilling}
T.~Sumers, K.~Marino, A.~Ahuja, R.~Fergus, and I.~Dasgupta.
\newblock Distilling internet-scale vision-language models into embodied
  agents.
\newblock \emph{arXiv preprint arXiv:2301.12507}, 2023.

\bibitem[Tay et~al.(2023)Tay, Dehghani, Tran, Garcia, Wei, Wang, Chung,
  Shakeri, Bahri, Schuster, Zheng, Zhou, Houlsby, and Metzler]{tay2023ul2}
Y.~Tay, M.~Dehghani, V.~Q. Tran, X.~Garcia, J.~Wei, X.~Wang, H.~W. Chung,
  S.~Shakeri, D.~Bahri, T.~Schuster, H.~S. Zheng, D.~Zhou, N.~Houlsby, and
  D.~Metzler.
\newblock Ul2: Unifying language learning paradigms, 2023.

\bibitem[Vemprala et~al.(2023)Vemprala, Bonatti, Bucker, and
  Kapoor]{vemprala2023chatgpt}
S.~Vemprala, R.~Bonatti, A.~Bucker, and A.~Kapoor.
\newblock Chatgpt for robotics: Design principles and model abilities.
\newblock \emph{Microsoft Auton. Syst. Robot. Res}, 2:\penalty0 20, 2023.

\bibitem[Wang et~al.(2022)Wang, Yang, Hu, Li, Lin, Gan, Liu, Liu, and
  Wang]{wang2022git}
J.~Wang, Z.~Yang, X.~Hu, L.~Li, K.~Lin, Z.~Gan, Z.~Liu, C.~Liu, and L.~Wang.
\newblock Git: A generative image-to-text transformer for vision and language.
\newblock \emph{arXiv preprint arXiv:2205.14100}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, E.~Chi, Q.~Le, and D.~Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Wei et~al.(2023)Wei, Hou, Lampinen, Chen, Huang, Tay, Chen, Lu, Zhou,
  Ma, and Le]{wei2023symbol}
J.~Wei, L.~Hou, A.~Lampinen, X.~Chen, D.~Huang, Y.~Tay, X.~Chen, Y.~Lu,
  D.~Zhou, T.~Ma, and Q.~V. Le.
\newblock Symbol tuning improves in-context learning in language models, 2023.

\bibitem[Wu et~al.(2023)Wu, Antonova, Kan, Lepert, Zeng, Song, Bohg,
  Rusinkiewicz, and Funkhouser]{wu2023tidybot}
J.~Wu, R.~Antonova, A.~Kan, M.~Lepert, A.~Zeng, S.~Song, J.~Bohg,
  S.~Rusinkiewicz, and T.~Funkhouser.
\newblock Tidybot: Personalized robot assistance with large language models.
\newblock \emph{arXiv preprint arXiv:2305.05658}, 2023.

\bibitem[Xiao et~al.(2022{\natexlab{a}})Xiao, Chan, Sermanet, Wahid, Brohan,
  Hausman, Levine, and Tompson]{xiao2022robotic}
T.~Xiao, H.~Chan, P.~Sermanet, A.~Wahid, A.~Brohan, K.~Hausman, S.~Levine, and
  J.~Tompson.
\newblock Robotic skill acquisition via instruction augmentation with
  vision-language models.
\newblock \emph{arXiv preprint arXiv:2211.11736}, 2022{\natexlab{a}}.

\bibitem[Xiao et~al.(2022{\natexlab{b}})Xiao, Radosavovic, Darrell, and
  Malik]{xiao2022masked}
T.~Xiao, I.~Radosavovic, T.~Darrell, and J.~Malik.
\newblock Masked visual pre-training for motor control.
\newblock \emph{arXiv preprint arXiv:2203.06173}, 2022{\natexlab{b}}.

\bibitem[Young et~al.(2021)Young, Gandhi, Tulsiani, Gupta, Abbeel, and
  Pinto]{young2021visual}
S.~Young, D.~Gandhi, S.~Tulsiani, A.~Gupta, P.~Abbeel, and L.~Pinto.
\newblock Visual imitation made easy.
\newblock In \emph{Conference on Robot Learning}, pages 1992--2005. PMLR, 2021.

\bibitem[Yu et~al.(2016)Yu, Bauza, Fazeli, and Rodriguez]{yu2016more}
K.-T. Yu, M.~Bauza, N.~Fazeli, and A.~Rodriguez.
\newblock More than a million ways to be pushed. a high-fidelity experimental
  dataset of planar pushing.
\newblock In \emph{2016 IEEE/RSJ international conference on intelligent robots
  and systems (IROS)}, pages 30--37. IEEE, 2016.

\bibitem[Yu et~al.(2018)Yu, Finn, Xie, Dasari, Zhang, Abbeel, and
  Levine]{yu2018one}
T.~Yu, C.~Finn, A.~Xie, S.~Dasari, T.~Zhang, P.~Abbeel, and S.~Levine.
\newblock One-shot imitation from observing humans via domain-adaptive
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1802.01557}, 2018.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2022scaling}
X.~Zhai, A.~Kolesnikov, N.~Houlsby, and L.~Beyer.
\newblock Scaling vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12104--12113, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Ding, Amiri, Yang, Kaminski, Esselink, and
  Zhang]{zhang2023grounding}
X.~Zhang, Y.~Ding, S.~Amiri, H.~Yang, A.~Kaminski, C.~Esselink, and S.~Zhang.
\newblock Grounding classical task planners via vision-language models.
\newblock \emph{arXiv preprint arXiv:2304.08587}, 2023.

\end{thebibliography}
