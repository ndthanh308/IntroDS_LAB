% Template for ICASSP-2021 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{algorithm,algorithmicx,algpseudocode}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}


\title{Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding}


\name{Chunyu Qiang$^{1,2,*}$, Hao Li$^{2,*}$, Hao Ni$^{2}$, He Qu$^{2}$, Ruibo Fu$^{3}$, Tao Wang$^{3}$, Longbiao Wang$^{1,\dag}$, Jianwu Dang$^{1}$}


\address{$^1$Tianjin University, Tianjin, China \\
$^2$Kuaishou Technology Co., Ltd, Beijing, China \\
$^3$Institute of Automation, Chinese Academy of Sciences, Beijing, China 
}


% \name{Chunyu Qiang$^{*}$, Hao Li$^{*}$, Hao Ni, He Qu}

% \address{ Kwai, Beijing, P.R. China}

\begin{document}
%\ninept
%
\maketitle
%


\begin{abstract}
Recently, there has been a growing interest in text-to-speech (TTS) methods that can be trained with minimal supervision by combining two types of discrete speech representations and using two sequence-to-sequence tasks to decouple TTS. However, existing methods suffer from three problems: the high dimensionality and waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model in non-autoregressive frameworks, and the information redundancy and dimension explosion problems of existing semantic encoding methods. To address these problems, three progressive methods are proposed. First, we propose Diff-LM-Speech, an autoregressive structure consisting of a language model and diffusion models, which models the semantic embedding into the mel-spectrogram based on a diffusion model to achieve higher audio quality. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. Second, we propose Tetra-Diff-Speech, a non-autoregressive structure consisting of four diffusion model-based modules that design a duration diffusion model to achieve diverse prosodic expressions. Finally, we propose Tri-Diff-Speech, a non-autoregressive structure consisting of three diffusion model-based modules that verify the non-necessity of existing semantic encoding models and achieve the best results. Experimental results show that our proposed methods outperform baseline methods. We provide a website with audio samples. \href{https://qiangchunyu.github.io/Diff-LM-Speech/}{$^1$}


\end{abstract}




\renewcommand{\thefootnote}{\fnsymbol{footnote}} %将脚注符号设置为fnsymbol类型,即特殊符号表示
\footnotetext{$*$ Equal Contribution. $\dag$ Corresponding author.}
\footnotetext{Audio samples: https://qiangchunyu.github.io/Diff-LM-Speech/}
% \footnotetext{Preprint. Work in progress.}

\begin{keywords}
minimal supervision, speech synthesis, semantic coding, diffusion model, language model
\end{keywords}
%

% Figure environment removed

\section{Introduction}
\label{sec:intro}
As deep learning advances, speech synthesis technology has made significant progress. Traditional speech synthesis methods have achieved satisfactory results\cite{wang2017tacotron,arik2017deep, li2019neural,ren2019fastspeech, kim2020glow, elias2021parallel}. The emergence of technologies such as GPT \cite{radford2018improving, brown2020language} has sparked increased interest in large-scale TTS systems. These TTS systems can be broadly divided into two categories: 1) autoregressive frameworks \cite{borsos2022audiolm, wang2023neural,zhang2023speak,kharitonov2023speak} and 2) non-autoregressive frameworks \cite{levkovitch2022zero, shen2023naturalspeech, le2023voicebox}. Traditional speech synthesis methods typically use mel-spectrogram as intermediate representations. However, recent advancements in neural codec for speech \cite{baevski2020wav2vec, Hsu2021HuBERTSS, Defossez2022HighFN, Zeghidour2022SoundStreamAE} have led TTS methods to convert audio waveforms into discrete codes as intermediate representations. Notable examples include VALL-E \cite{wang2023neural}, the first large-scale TTS framework based on a language model with in-context learning capabilities for zero-shot speech synthesis. However, discrete acoustic coding relies on neural codecs for speech waveform reconstruction and suffers from information loss on high-frequency fine-grained acoustic details compared to traditional audio features. Additionally, the autoregressive framework suffers from the typical problems of instability and uncontrollability. Naturalspeech2 \cite{shen2023naturalspeech} is a non-autoregressive TTS framework based on a latent diffusion model \cite{ho2020denoising}. However, the duration prediction model required by non-autoregressive frameworks can cause expression averaging issues. SPEAR-TTS \cite{zhang2023speak} is another example that splits the TTS task into two tasks (text-to-semantic and semantic-to-speech) to achieve minimally-supervised training. The information content of the semantic coding is expected to be between text coding and acoustic coding, but the semantic coding extracted by existing models suffers from excessive redundancy and dimension explosion, leading to prediction difficulties and cumulative errors. To address these three issues, we propose three progressive methods:

1) Diff-LM-Speech, an autoregressive structure consisting of {\bf diff}usion models and a {\bf l}anguage {\bf m}odel, which models the semantic embedding into the mel-spectrogram based on a diffusion model to address the high dimensionality and waveform distortion issues of existing autoregressive methods based on language models. We also introduce a prompt encoder structure based on a variational autoencoder and a prosody bottleneck to improve prompt representation ability. 
2) Tetra-Diff-Speech, a non-autoregressive structure consisting of {\bf four diff}usion model-based modules that replace the semantic language model in Diff-LM-Speech with a semantic diffusion model. The duration diffusion model achieves diverse prosodic expressions and solves the expression averaging problem caused by the duration prediction model in non-autoregressive frameworks, as well as common problems of missing and repeated words in autoregressive frameworks.
3) Tri-Diff-Speech, a non-autoregressive structure consisting of {\bf three diff}usion model-based modules that compress and merge the semantic diffusion model and acoustic diffusion model in Tetra-Diff-Speech into a mel diffusion model. This structure verifies the non-necessity of semantic coding and avoids the problems of cumulative errors and information redundancy and dimension explosion in existing semantic encoding models.









\section{Method}

\subsection{Overview}
% \subsubsection{Diff-LM-Speech}
% Diff-LM-Speech extends SPEAR-TTS\cite{zhang2023speak} by enabling the diffusion model for continuous-valued acoustic feature regression tasks. Diff-LM-Speech is divided into three main stages, as shown in Fig.\ref{fig:proposed_model}. In the first stage, text input is translated into a sequence of discrete semantic tokens by the semantic language model. The second stage maps the semantic embedding into the mel-spectrogram by the acoustic diffusion model. The third stage maps the mel-spectrogram into speech by the wave diffusion model. Diff-LM-Speech cast text-to-speech (TTS) as a composition of two primary tasks, an autoregressive discrete coding classification task (text into semantic coding) and a non-autoregressive continuous signal prediction task (semantic coding into speech). 


\subsubsection{Diff-LM-Speech}
\label{sec:diff-lm}
Diff-LM-Speech extends SPEAR-TTS\cite{zhang2023speak} by enabling the diffusion model for continuous-valued acoustic feature regression tasks. The framework consists of three main stages, as shown in Fig. \ref{fig:proposed_model}. In the first stage, text input is translated into a sequence of discrete semantic tokens by the semantic language model (12-layer decoder-only transformer model). The second stage maps the semantic embedding into the mel-spectrogram by the acoustic diffusion model. The third stage maps the mel-spectrogram into the speech by the wave diffusion model. Diff-LM-Speech performs two primary tasks: an autoregressive discrete coding classification task (text into semantic coding) and a non-autoregressive continuous signal prediction task (semantic coding into speech).

\subsubsection{Tetra-Diff-Speech}
\label{sec:tetra-diff}
% As shown in Fig.2, Tetra-Diff-Speech consists of four diffusion-based modules, which differ from Diff-LM-Speech in that a non-autoregressive Semantic Diffusion model is used to achieve the prediction from text to semantic embedding. A duration diffusion model is also designed to predict the corresponding duration of phonemes to solve the problem of mismatch between the length of phoneme sequences and semantic sequences. During training, the ground-truth duration is used to expand the phoneme sequence. During inference, the corresponding predicted duration is used.

Tetra-Diff-Speech, as shown in Fig. \ref{fig:proposed_model}, consists of four diffusion-based modules that differ from Diff-LM-Speech. A non-autoregressive semantic diffusion model is used to achieve the prediction from text to semantic embedding. A duration diffusion model is also designed to predict the corresponding duration of phonemes to solve the problem of mismatch between the length of phoneme sequences and semantic sequences. During training, the ground-truth duration is used to expand the phoneme sequence, while during inference, the corresponding predicted duration is used.


\subsubsection{Tri-Diff-Speech}
\label{sec:tri-diff}
% As shown in Fig.2, Tri-Diff-Speech consists of three diffusion-based modules, where the difference with Tetra-Diff-Speech is the use of a Mel Diffusion model to make direct predictions from text to mel-spectrogram, to verify whether the two-stage process based on semantic coding is really effective relative to the traditional one-stage process.

Tri-Diff-Speech, also shown in Fig. \ref{fig:proposed_model}, consists of three diffusion-based modules that use a mel diffusion model to make direct predictions from text to mel-spectrogram. This framework aims to verify whether the two-stage process based on semantic coding in the existing semantic encoding models is really effective relative to the traditional one-stage process. 

\subsection{Prompt Feature Extractor}
\subsubsection{Semantic Encoder}

Similar to SPEAR-TTS\cite{zhang2023speak}, we use semantic coding as an intermediate representation between text and acoustic coding. To preserve high-frequency fine-grained acoustic details, we replace SoundStream's\cite{zeghidour2021soundstream} discrete acoustic coding with mel-spectrogram. The purpose of semantic coding is to provide coarse, high-level conditioning to subsequently produce mel-spectrogram.  Thus, semantic coding should provide a representation of speech in which linguistic content (phonetics-to-semantics) is salient, while paralinguistic information such as speaker identity and acoustic details are removed. We obtain a 512-dimensional embedding with 1024 discrete values by fine-tuning a HuBert\cite{hsu2021hubert} model on an automatic speech recognition (ASR) task. This approach enables the model to learn a more robust and discriminative representation of speech that captures phonetic and semantic information. Additionally, we use Whisper's\cite{radford2023robust} encoder module as a comparison model to extract semantic coding (512-dimensional).

% Similar to SPEAR-TTS\cite{zhang2023speak}, we use semantic coding as an intermediate representation between text and acoustic coding. To preserve high-frequency fine-grained acoustic details, we replace the discrete acoustic coding generated by SoundStream\cite{zeghidour2021soundstream} with mel-spectrogram. The purpose of semantic coding is to provide coarse, high-level conditioning to subsequently produce mel-spectrogram.  Thus, semantic coding should provide a representation of speech in which linguistic content (phonetics-to-semantics) is salient, while paralinguistic information such as speaker identity and acoustic details are removed. To obtain such a representation(512 dimensions of embedding, 1024 discrete values), we trained a HuBert\cite{hsu2021hubert} model and fine-tuned it on an automatic speech recognition(ASR) task. This approach enables the model to learn a more robust and discriminative representation of speech that captures phonetic and semantic information. Additionally, we used an encoder module of the Whisper\cite{radford2023robust} as a control group to extract semantic coding(512-dimensional).

\subsubsection{Prompt Encoder}

The prompt encoder is a VAE-based model \cite{qiang2023improving} that extracts paralinguistic information, such as timbre, style, and rhyme, from the prompt speech. It comprises a 6-layer 2D convolutional network and a SE-ResNet block \cite{hu2018squeeze}, which recalibrates channel-wise feature responses by modeling interdependencies among channels, resulting in significant performance improvements. The VAE structure enables the model to obtain a continuous and complete latent space distribution of styles, improving the ability to extract paralinguistic information. A 64-dimensional vector is sampled from the Gaussian distribution as the prompt embedding. To address the KL collapse problem, three tricks are used: 1) introducing KL annealing, 2) adopting a staged optimization method to optimize the reconstruction loss first and then the KL loss, and 3) A margin $\Delta$ is introduced to limit the minimum value of the KL loss as shown:$\mathcal{L}_{kl} = max(0, D_{KL}[\mathcal{N}({\hat{\mu}},{\hat{\sigma}}^2)||\mathcal{N}(0, I)]-\Delta)$. 

% Figure environment removed

\subsection{Conditional Diffusion Model}
\subsubsection{Diffusion  Formulation}
The acoustic diffusion model calculation is shown in Algorithms \ref{alg:training} and \ref{alg:sampling}. The model uses $q(data), x_0, s, t$, and $p$ to represent data distribution, acoustic coding, semantic coding, diffusion step, and prompt embedding, respectively. One notable feature of the model is that it allows for closed-form sampling of $x_t$ at any timestep $t$ using $\bar{\alpha}_t$ and $\alpha_t$. The non-autoregressive network $\epsilon_{\theta}$ predicts $\epsilon$ from $x_t, t, p$, and $s$. The training objective is to minimize the unweighted variant of the ELBO\cite{ho2020denoising}, as shown in line 7 of Algorithm \ref{alg:training}. The sampling process is shown in Algorithm \ref{alg:sampling}, where $x_T \sim \mathcal{N}(0, I)$ is first sampled, followed by sampling $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T, T-1,\cdots,1$. The output $x_0$ is the sampled data. 
\subsubsection{Diffusion  Architecture}
As shown in Fig. \ref{fig:acoustic_model}, the acoustic diffusion model uses a bidirectional dilated convolution architecture with $N$ residual layers grouped into $m$ blocks, each containing $n = \frac{N}{m}$ layers. The dilation is doubled at each layer within each block. Skip connections from all residual layers are summed up, similar to WaveNet\cite{oord2016wavenet}. The model takes in semantic and prompt embeddings as conditional information. The semantic embedding is input to the transformer encoder, upsampled by length regulator, and added as a bias term for the dilated convolution in each residual layer. The prompt embedding and diffusion step embedding are broadcast over length and added to the input of each residual layer.
%\subsubsection{Diffusion  Formulation}
% The acoustic diffusion model calculation is shown in Algorithm \ref{alg:training} and \ref{alg:sampling}. Throughout the paper, we use $q(data), x_0, s, t$ and $p$ to represent data distribution, acoustic coding, semantic coding, diffusion step and prompt embedding, respectively. A notable property of the forward process is that it admits sampling $x_t$ at an arbitrary timestep $t$ in closed form: using the notation $\bar{\alpha}_t$ and $\alpha_t$, we have $x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1-\bar\alpha_t}\varepsilon$. $\epsilon_{\theta}$ is a function approximator intended to predict $\epsilon$ from $x_t, t, p$ and $s$. In this paper, we use minimizing the unweighted variant of the ELBO\cite{ho2020denoising} as the training objective, as shown in line 7 of Algorithm\ref{alg:training}.Algorithm\ref{alg:sampling} shows the sampling process, the generative procedure involves first sampling $x_T \sim \mathcal{N}(0, I)$, and then sampling $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T, T-1,\cdots,1$. The reparameterization is shown in line 5 of Algorithm\ref{alg:sampling}. Note that both $\mu_{\theta}$ and $\sigma_{\theta}$ take four inputs: $x_t, t, p$ and $s$. The aim of $p_{\theta}(x_{t-1}|x_t)$ is to eliminate the Gaussian noise added in the diffusion process. The output $x_0$ is the sampled data. 

% The conditional diffusion model involves a diffusion process and a reverse process, with $T$ diffusion steps. The acoustic diffusion model calculation is shown in Algorithm \ref{alg:training} and \ref{alg:sampling}. Throughout the paper, we use $q(data), x_0, s, t$ and $p$ to represent data distribution, acoustic coding, semantic coding, diffusion step and prompt embedding, respectively. A notable property of the forward process is that it admits sampling $x_t$ at an arbitrary timestep $t$ in closed form: using the notation $\bar{\alpha}_t$ and $\alpha_t$, we have $x_t = \sqrt{\bar\alpha_t} x_0 + \sqrt{1-\bar\alpha_t}\varepsilon$. $\epsilon_{\theta}$ is a function approximator intended to predict $\epsilon$ from $x_t, t, p$ and $s$. In this paper, we use minimizing the unweighted variant of the ELBO\cite{ho2020denoising} as the training objective, as shown in line 7 of Algorithm\ref{alg:training}.


% Algorithm\ref{alg:sampling} shows the sampling process, the generative procedure involves first sampling $x_T \sim \mathcal{N}(0, I)$, and then sampling $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ for $t=T, T-1,\cdots,1$. The reparameterization is shown in line 5 of Algorithm\ref{alg:sampling}. Note that both $\mu_{\theta}$ and $\sigma_{\theta}$ take four inputs: $x_t, t, p$ and $s$. The aim of $p_{\theta}(x_{t-1}|x_t)$ is to eliminate the Gaussian noise added in the diffusion process. The output $x_0$ is the sampled data. 

%\subsubsection{Diffusion  Architecture}
% The acoustic diffusion model is a non-autoregressive network $\epsilon_{\theta}$ that employs a bidirectional dilated convolution architecture, similar to DiffWave\cite{kong2020diffwave}. The network comprises $N$ residual layers, which are grouped into $m$ blocks, and each containing $n = \frac{N}{m}$ layers. In each layer, we use a dilated convolution with a kernel size of $3$. The dilation is doubled at each layer within each block, i.e., $[1, 2, 4, \cdots, 2^{n-1}]$. We sum the skip connections from all residual layers as in WaveNet\cite{oord2016wavenet}. In the acoustic diffusion model, the conditional information includes semantic embedding and prompt embedding. The semantic embedding is first input to the transformer encoder. Then, it is upsampled by length regulator (transposed 2-D convolutions) to match the length of the mel-spectrogram. Subsequently, it is added as a bias term for the dilated convolution in each residual layer. On the other hand, the prompt embedding and diffusion step embedding are broadcast over length and added to the input of each residual layer.

The other diffusion-based modules have similar structures, but differ in input, diffusion-step, and conditional information. Fig. \ref{fig:proposed_model} shows that the duration diffusion model (diffusion-step=5) is conditioned on the phoneme sequence, while the semantic diffusion model (diffusion-step=200) is conditioned on the phoneme sequence upsampled by duration. The mel diffusion model (diffusion-step=500) is also conditioned on the phoneme sequence upsampled by duration and the prompt embedding. The wave diffusion model (diffusion-step=50) is conditioned on the mel-spectrogram.
% The other diffusion-based modules have a similar structure, except for the input, diffusion-step and conditional information. As shown in Fig.2, the conditional information of the duration diffusion model(diffusion-step=5) is the phoneme sequence, while the conditional information of the semantic diffusion model(diffusion-step=200) is the phoneme sequence that has been upsampled based on duration. The conditional information of the mel diffusion model(diffusion-step=500) is also the phoneme sequence that has been upsampled based on duration, along with the prompt embedding. Finally, the wave diffusion model(diffusion-step=50) is conditioned on the mel-spectrogram.


\section{Experiments}

\subsection{Experimental Step}
In the experimental step, we utilized an internal dataset in conjunction with the AISHELL-3 dataset\cite{shi2020aishell}, which consists of 215705 recordings of 304 native Mandarin Chinese speakers (73 males and 231 females). All speech waveforms were sampled at 24kHz and converted to mel-spectrograms with a frame size of 960 and a hop size of 240. Limited by the amount of data, we conducted our experiments in a few-shot mode, where we used a test set consisting of 15 minutes of data from each speaker.

\subsection{Compared Models}
We compare our proposed model with four other models, namely {\bf Tacotron-VAE}\cite{qiang2022style}, {\bf VALL-E}\cite{wang2023neural}, {\bf NaturalSpeech2}\cite{shen2023naturalspeech}, and {\bf SpearTTS}\cite{zhang2023speak}. To ensure fairness, we modify all methods to utilize the same language model and diffusion model framework, and train with the same data. Specifically, the prompt encoder, wave diffusion model, and Grapheme-to-Phoneme (G2P)\cite{qiang2022back} structure of our proposed model are identical across all compared models. The duration diffusion models of both {\bf Tri-Diff-Speech} (described in Sec \ref{sec:tri-diff}) and {\bf Tetra-Diff-Speech} (described in Sec \ref{sec:tetra-diff}) are also the same. Additionally, the semantic encoder and acoustic diffusion model of both {\bf Tetra-Diff-Speech} and {\bf Diff-LM-Speech} (described in Sec \ref{sec:diff-lm}) are identical. Hubert and Whisper encoders are both used as control groups for semantic encoding.


\subsection{Test Metrics}
We conducted all subjective tests using 11 native judgers, with each metric consisting of 20 sentences per speaker. The test metrics used in the evaluation included prosody measurement, which involved mean square error for pitch ({\bf MSEP}) and duration ({\bf MSED}) to assess prosody similarity against ground-truth speech, word error rate ({\bf WER}), which utilized an ASR model to transcribe the generated speech and calculate the word error rate, and mean opinion score ({\bf MOS}), which verified speech quality and similarity in expected speaking prosody and timbre between source speech and synthesized speech.



\algrenewcommand\algorithmicindent{0.5em}%
% Figure environment removed


% \begin{table*}[]
%  \caption{Prosody Measurement \& WER \& MOS}
%  \label{tab:prosody}
%  \centering
% \begin{tabular}{llllll}
% \hline
% Model              & MSEP                    & MSED      & WER      & Prosody Sim           & Speaker Sim           \\ \hline
% Tacotron-VAE\cite{qiang2022style}          & 97.4          & \textbf{18.7} & 7.8            & 3.82 ± 0.072          & 3.92 ± 0.087          \\ \hline
% VALL-E\cite{wang2023neural}                 & 91.6         & 19.5          & 6.1            & 3.64 ± 0.050          & 3.70 ± 0.052          \\ \hline
% NaturalSpeech2\cite{shen2023naturalspeech} & 95.9          & 25.1          & \textbf{4.5}   & 3.73 ± 0.054          & 4.04 ± 0.086          \\ \hline
% SpearTTS(Hubert)\cite{zhang2023speak}      & 110.5         & 19.0          & 8.5            & 3.60 ± 0.059          & 3.68 ± 0.030          \\ \hline
% Tetra-Diff-Speech(Hubert)                                   & 103.5         & 20.1          & 4.6         & 3.89 ± 0.013          & 3.71 ± 0.077          \\ \hline
% Tetra-Diff-Speech(Whisper)                                  & 104.4         & 21.2          & 4.6          & 3.79 ± 0.098          & 3.93 ± 0.057          \\ \hline
% Diff-LM-Speech(Hubert)                                      & 107.2         & 19.6          & 7.2          & 3.80 ± 0.090          & 3.71 ± 0.015          \\ \hline
% Diff-LM-Speech(Whisper)                                     & 107.0         & \textbf{18.7} & 7.7          & 3.80 ± 0.064          & 3.79 ± 0.044          \\ \hline
% Tri-Diff-Speech                                             & \textbf{95.2} & 19.0          & \textbf{4.5}  & \textbf{3.90 ± 0.047} & \textbf{4.06 ± 0.010} \\ \hline
% \end{tabular}
% \end{table*}






\subsection{Results}
The results in Table \ref{tab:prosody} demonstrate that one-stage models, including Tri-Diff-Speech, NaturalSpeech2, VALL-E, and Tacotron-VAE, outperform two-stage models like Spear-TTS, Tetra-Diff-Speech, and Diff-LM-Speech in terms of MSEP. Additionally, we observed that existing models extract redundant information and cause dimensionality explosion in semantic coding, making overall task modelling more challenging than one-stage modelling. Due to the limited number of open-source Mandarin datasets, we plan to further verify this conclusion in future work. In terms of MSED, Diff-LM-Speech achieves the best results, while Tri-Diff-Speech and Tetra-Diff-Speech also perform better than the non-autoregressive structure of NaturalSpeech2 due to the duration diffusion model. Furthermore, Tri-Diff-Speech and NaturalSpeech2 have significant advantages over other autoregressive structures (VALL-E, Diff-LM-Speech, etc.) in synthesizing high-quality and robust speech due to their non-autoregressive structure, as demonstrated by the WER results.
Table \ref{tab:mos} reveals that the proposed methods outperform SpearTTS, NaturalSpeech2, and VALL-E in terms of prosody similarity MOS. This is due to the introduction of more randomness in the duration diffusion model, which enables diverse prosodic expressions. Among them, Tri-Diff-Speech achieves the best results. For speaker similarity MOS, both Tri-Diff-Speech and NaturalSpeech2 with non-autoregressive structure perform better. Moreover, all models using mel-spectrogram as acoustic features achieve better speech quality MOS scores than those with discrete acoustic coding. Specifically, Tri-Diff-Speech and Tacotron-VAE achieve the best results, highlighting the importance of continuous acoustic features for speech quality.


\begin{table}[]
\captionsetup{skip=0pt} % 设置标题与表格之间的间距为10pt
 \caption{Prosody Measurement \& WER}
 \label{tab:prosody}
 \centering
 \resizebox{\linewidth}{!}{ 
\begin{tabular}{llll}
\hline
Model              & MSEP                    & MSED      & WER      \\ \hline
Tacotron-VAE\cite{qiang2022style}          & 97.4          & \textbf{18.7} & 7.8          \\ \hline
VALL-E\cite{wang2023neural}                 & 98.6         & 19.5          & 6.1         \\ \hline
NaturalSpeech2\cite{shen2023naturalspeech} & 95.9          & 25.1          & \textbf{4.5} \\ \hline
SpearTTS(Hubert)\cite{zhang2023speak}      & 110.5         & 19.0          & 8.5          \\ \hline
Tetra-Diff-Speech(Hubert)                                   & 103.5         & 20.1          & 4.6          \\ \hline
Tetra-Diff-Speech(Whisper)                                  & 104.4         & 21.2          & 4.6          \\ \hline
Diff-LM-Speech(Hubert)                                      & 107.2         & \textbf{18.7}          & 7.2          \\ \hline
Tri-Diff-Speech                                             & \textbf{95.2} & 19.0          & \textbf{4.5} \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[]
\captionsetup{skip=0pt} % 设置标题与表格之间的间距为10pt
 \caption{MOS}
 \label{tab:mos}
 \centering
\resizebox{\linewidth}{!}{ 
\begin{tabular}{llll}
\hline
Model                      & Prosody Sim           & Speaker Sim           & Speech Quality         \\ \hline
Tacotron-VAE               & 3.82 ± 0.072          & 3.92 ± 0.087          & \textbf{4.01 ± 0.023}          \\ \hline
VALL-E                     & 3.64 ± 0.050          & 3.70 ± 0.052          & 3.61 ± 0.013          \\ \hline
NaturalSpeech2             & 3.73 ± 0.054          & 4.04 ± 0.086          & 3.79 ± 0.070          \\ \hline
SpearTTS(Hubert)           & 3.60 ± 0.059          & 3.68 ± 0.030          & 3.50 ± 0.081          \\ \hline
Tetra-Diff-Speech(H)  & 3.89 ± 0.013          & 3.71 ± 0.077          & 3.83 ± 0.002          \\ \hline
Tetra-Diff-Speech(W) & 3.79 ± 0.098          & 3.93 ± 0.057          & 3.99 ± 0.017          \\ \hline
Diff-LM-Speech(H)     & 3.80 ± 0.090          & 3.71 ± 0.015          & 3.88 ± 0.042          \\ \hline
Tri-Diff-Speech            & \textbf{3.90 ± 0.047} & \textbf{4.06 ± 0.010} & \textbf{4.01 ± 0.080} \\ \hline
\end{tabular}
}
\end{table}

\section{Conclusions and future work}
In this paper, we proposed three progressive methods, namely Diff-LM-Speech, Tetra-Diff-Speech, and Tri-Diff-Speech, to address several issues in existing systems. These issues include the high dimensionality and waveform distortion of discrete speech representations, the prosodic averaging problem caused by the duration prediction model, and the information redundancy and dimension explosion problems of existing semantic encoding methods. Our proposed methods have been shown to be effective in experiments. In future work, we aim to design an intermediate representation that is agnostic to paralinguistic information to achieve better results in minimally-supervised TTS.

\vfill\pagebreak





\bibliographystyle{IEEEbib}

\footnotesize
\bibliography{strings,refs}




\end{document}


