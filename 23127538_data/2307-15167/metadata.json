{
  "title": "PEANUT: A Human-AI Collaborative Tool for Annotating Audio-Visual Data",
  "authors": [
    "Zheng Zhang",
    "Zheng Ning",
    "Chenliang Xu",
    "Yapeng Tian",
    "Toby Jia-Jun Li"
  ],
  "submission_date": "2023-07-27T19:56:02+00:00",
  "revised_dates": [],
  "abstract": "Audio-visual learning seeks to enhance the computer's multi-modal perception leveraging the correlation between the auditory and visual modalities. Despite their many useful downstream tasks, such as video retrieval, AR/VR, and accessibility, the performance and adoption of existing audio-visual models have been impeded by the availability of high-quality datasets. Annotating audio-visual datasets is laborious, expensive, and time-consuming. To address this challenge, we designed and developed an efficient audio-visual annotation tool called Peanut. Peanut's human-AI collaborative pipeline separates the multi-modal task into two single-modal tasks, and utilizes state-of-the-art object detection and sound-tagging models to reduce the annotators' effort to process each frame and the number of manually-annotated frames needed. A within-subject user study with 20 participants found that Peanut can significantly accelerate the audio-visual data annotation process while maintaining high annotation accuracy.",
  "categories": [
    "cs.HC"
  ],
  "primary_category": "cs.HC",
  "doi": "10.1145/3586183.3606776",
  "journal_ref": null,
  "arxiv_id": "2307.15167",
  "pdf_url": "https://arxiv.org/pdf/2307.15167v1",
  "comment": "18 pages, published in UIST'23",
  "num_versions": null,
  "size_before_bytes": 16887435,
  "size_after_bytes": 2149161
}