\section{The \textsc{Peanut} System}

% TL: reorganize --> 
\subsection{Task: Sounding Object Localization}

\zzhighlight{\textsc{Peanut} focuses on annotating data for the sounding object localization task. As defined in~\cite{hu2021class}, given a video, sounding object localization aims to semantically correlate each sound to the visual regions containing the sounding source and recognize the category of the sound in each frame. Therefore, in the annotation task, the annotator should first identify all the sounds in the current frame, and for each sound, provide a semantic label (e.g., church bell, dog bark) and associate it to its sounding object (represented as a bounding box) for each and every frame in a video. A frame may contain multiple sounding objects at the same time as well as silent objects that are physically capable of making sounds.} \looseness=-1

% A video may contain multiple sounding objects at the same time as well as silent objects that are physically capable of making sounds. The sounding object localization task has recently gained increasing attention from the computer vision community as it is critical for enabling machine to proceed towards human understandings of complex scenes by leveraging the temporal and cross-modal correspondences between visual and auditory cues \cite{li2021space}.


\subsection{Design Goals}
\label{sec:design_challenges}
Informed by results from prior studies on data annotation~\cite{askhtorab_aiassisted_2021,mitra_comparing_2015,ratner2017snorkel}, \zzhighlight{status quo of relevant ML models,} and our experience with data annotation and human-AI collaborative tools, \crhighlight{we summarized the following four design goals in our design of an AI-assisted audio-visual data annotation tool to address potential human and AI challenges in sounding object localization task}:
% \subsubsection{Challenges}
\paragraph{DG1: Improving annotation efficiency without compromising accuracy with imperfect AI models} \crhighlight{The ultimate goal of the annotation is to collect data to train an end-to-end ML model for the sounding object localization task. Although there are some end-to-end models for this task ~\zzhighlight{\cite{owens2018audio,senocak2018learning,tian2018audio,hu2020discriminative}}, their performance is limited due to the lack of annotated data, which our work seeks to address. While some off-the-shelf models (e.g., object detectors, audio tagging models) can contribute to the annotation process of audio-visual data, they are often limited in several ways: (1) they often process input data in only one modality; (2) they have limited accuracy (as they are not specifically trained for our domain); and (3) they only assist with a part of the annotation process. In contrast, our annotation task requires: (1) processing multi-modal audio-visual data; (2) achieving high accuracy in annotation; and (3) providing an end-to-end annotation from raw videos to annotated sounding objects in each frame of the video.}

\crhighlight{However, the limitations of AI models do not preclude their potential to function as assistant to human annotators. Prior works \cite{berg2019ilastik, choi2022ai, kehl2021artificial} have demonstrated that using AI models to automate parts of data annotation tasks could significantly improve annotation efficiency. Nonetheless, previous research ~\cite{mitra_comparing_2015} also suggested that without effective human intervention strategy, imperfect AI could result in lower quality in annotations despite the higher efficiency. Therefore, our main motivation is: how can we make annotating audio-visual data more efficient by introducing AI assistance that reduces human effort and cognitive load? Meantime, it is also crucial that improved efficiency does not come at the cost of compromising accuracy. As discussed in~\cite{sambasivan_2021_everyone}, data quality issues can cause significant compounding negative effects on downstream tasks, resulting in ``data cascades'' that harm users and communities.}

\paragraph{DG2: Supporting users' agency and mitigating their overreliance on AI models}

\zzhighlight{Prior work \cite{langer1989minding} find that users tend to give premature cognitive commitment to automation if the assisted task is routine, repetitive, and demanding. In particular, Ashktorab et al. \cite{askhtorab_aiassisted_2021} showed that AI-assisted data labeling tasks conform to these characteristics in which users are inclined to overrely on AI: despite the inaccuracies of the models, human users sometimes perceive \textit{higher} qualities and \textit{higher} capabilities of the models than what they actually are when they observe their high performance in common situations. However, when uncommon situations arise, human users may overrely on AI automation \crhighlight{which could threat} human discernment \crhighlight{and agency} in annotation~\cite{askhtorab_aiassisted_2021}. This poses a challenge in our task given that the frame-wise annotation is highly repetitive and demanding, which may make users less wary of the cross-modal mismatches and inaccurate annotations by the AI.} Therefore, our system should make it easy for users to identify potential errors in AI results and ensure that users fulfill their duties of reviewing and validating AI results in the annotation workflow. \crhighlight{Furthermore, our system should support user agency in the annotation process, providing the flexibility to control the degree of human involvement based on their perception of AI's accuracy change over the annotation process.}\looseness=-1

% \subsubsection{Goals}
% \paragraph{\zzhighlight{DG3: Improving annotation efficiency without compromising accuracy}} \zzhighlight{The primary goal of AI-assisted annotation tools is to help annotators label data more accurately and faster. However, prior work~\cite{mitra_comparing_2015} suggests that, if no effective human intervention strategy is enabled, imperfect AI could result in lower quality in annotations despite the higher efficiency. Therefore, our main motivation is: } how can we make annotating audio-visual data more efficient by introducing AI assistance that reduces human effort and cognitive load? Meantime, it is also crucial that improved efficiency does not come at the cost of compromising accuracy. As discussed in~\cite{sambasivan_2021_everyone}, data quality issues can cause significant compounding negative effects on downstream tasks, resulting in ``data cascades'' that harm users and communities.

\paragraph{DG3: Minimizing the learning barrier}
Data annotation tasks are often conducted by people without significant AI/ML expertise---many users of data annotation tools are either domain experts (e.g., physicians and radiologists who annotate medical imaging data \zzhighlight{\cite{neves2014survey}}) or laypeople who only annotate data either as a one-off task or only occasionally (e.g., Mechanic Turk workers \zzhighlight{\cite{mitra_comparing_2015}}). Therefore, it would be prohibitive if the tool has a high learning barrier or requires extensive expertise from the users. Ideally, the system should not require users to learn new skills beyond what they would already need if they labeled the data manually.


\paragraph{\zzhighlight{DG4: Supporting annotation for diverse video topics}}

\zzhighlight{Audio-visual scenarios are intrinsically diverse in terms of the object, sound, and event type involved in videos \cite{arandjelovic2017look}. Therefore, our annotation tool should be able to provide users with effective AI assistance regardless of the topics in the videos being labeled. To meet this need, models used in the pipeline should be generalizable and not limited to specific domains. While some of these models might be pre-trained to bootstrap the system's performance in cold-start situations for common topics, they should also be able to learn about new object and audio types quickly from the user's few-shot example data in real time.}

% Lastly, it is important that the AI assistance should not rely on any domain-specific knowledge, as the tool should support video annotation in any domain. Pre-trained models used in the tool should be trained on large domain-general datasets and has the capability to adapt to new domains when needed.


% Figure environment removed
% \tlcomment{The ``imperfection'' of AI models: 1. single modal; 2. limited accuracy; 3. not end-to-end vs. The requirement of good data annotation: 1. multi-modal; 2. high accuracy; 3. end-to-end === strategies to help users work with imperfect AI models}
% \tlcomment{Over-reliance -- expect users to do (1)(2)(3), otherwise the accuracy will suffer}
% \tlcomment{No new learning barrier or additional effort -- data annotators often not-trained e.g., gig worker}
% \tlcomment{Need to be domain-general -- work with data from all domains}
% ------------
% \tlcomment{DG: faster}
% \tlcomment{DG: less tedious/effort required}






% TL: need to SEPARATE design goals from approaches -- design goals should not be BIASED towards a certain approach e.g., "enable the active learning..." is not a goal, but an approach
\subsection{System Design}

% \cmt {
%     To help lay users annotate audio-visual data both efficiently and accurately, we developed \textsc{Peanut}. The design of \textsc{Peanut} was guided by the following goals:
%     \begin{itemize}
%         \item \textbf{DG1}: Realize human-AI collaborative annotation where AI can automate audio-visual labeling of most video frames under the guidance of a few human annotations
%         \item \textbf{DG2}: Enable annotators to efficiently review and modify the labeling resulting from the human-AI collaboration
%         \item \textbf{DG3}: Reduce the cognitive effort of the annotator to identify and locate the sounding objects
%         \item \textbf{DG4}: Enable active learning of the AI "annotator" so that the accuracy of automatic labeling can implicitly improve from a stream of human-annotated data
%     \end{itemize}
% }

To address the aforementioned design challenges and design goals, we designed and implemented \textsc{Peanut}, a human-AI collaborative audio-visual annotation tool that seeks to make annotation more efficient using novel interaction strategies, features, and algorithms. Figure~\ref{fig:system_screenshot} shows the main annotation workspace of \textsc{Peanut}, which consists of three components: (1) a canvas that displays the video frame and its current annotation state (C); (2) top and side toolbars that allow the annotator to perform different operations (A, B); (3) an information panel that summarizes the meta-information about the current object types and displays the operation history (D). The system architecture of \textsc{Peanut} is shown in Figure~\ref{fig:architecture}.

\cmt{
    Following these four design goals, we implemented \textsc{Peanut} as shown in Figure \ref{fig:system_screenshot}. The workspace of \textsc{Peanut} consists of three components: (1) A canvas that bears the human and AI labeling (C); (2) Top and side toolbars that allow the annotator to perform different operations (A, B); (3) An information panel that summarizes the meta-information (D).
}

\subsubsection{Human annotation} For each frame, \textsc{Peanut} lowers the user's effort and cognitive load to annotate it by: (1) facilitating the user's access to both global and local audio-visual contexts; (2) inferring bounding boxes for potential candidates of sounding objects; and (3) predicting the audio tags for sounds. 

When the user moves to a frame that corresponds to the \textit{i}-th millisecond (ms) of the video, \textsc{Peanut} will auto-play a one-second-long audio clip that contains the soundtrack from \textit{i-500} to \textit{i+500} ms, which gives the annotator a sense of the local audio context. The annotator can replay the local soundtrack by clicking the ``Audio'' button. In addition, \textsc{Peanut} allows the annotator to watch the entire video at any time during the annotation process, which helps the annotator disambiguate between the identities of local sound sources with the information of the global context. 

\textsc{Peanut} helps the annotator locate and tag the sounding object. When using \textsc{Peanut} to annotate a frame, instead of manually recognizing the audio type and drawing the bounding box for its sounding object, the user, in most cases, selects a sound type from a list of predictions made by an audio-tagging model and matches it to one of the visual objects detected by an object detector (red boxes shown in Figure~\ref{fig:system_screenshot}). This process can reduce the annotator's cognitive load for locating and labeling the sounding objects \crhighlight{(DG1)} \crhighlight{and demand no domain knowledge from annotators (DG3)}. If none of the predicted sound types or visual objects is correct, the annotator can manually enter a sound type and draw a new bounding box by clicking the \faIcon{expand} button.


% Figure environment removed

\subsubsection{Automatic annotation}
\label{sec:automatic_annotation}
Besides reducing the efforts required for the user to annotate each frame, \textsc{Peanut} allows the user to annotate fewer frames. Instead of asking the annotator to label every frame in the video, \textsc{Peanut} uses two complementary strategies to automatically infer the annotation result of the remaining frames based on the human annotation of ``key frames''. The combination of two strategies provides the annotator with the flexibility to adjust the granularity of automatic annotation.

For the first strategy, \textsc{Peanut} dynamically navigates the annotator to the next keyframe that requires human annotation, from which \textsc{Peanut} can infer the annotation results of the frames between two human-annotated ones. The annotator can go to the next recommended keyframe by clicking the \faIcon{angle-double-right} button. Under the hood, \textsc{Peanut} uses an \textit{audio-visual-sensitive binary search} algorithm (see Section~\ref{sec:recomendFrame}) to identify the next keyframe that needs human annotation. Note that the recommended key frames may not be in sequential order. For example, after an annotator annotates the 10th and 20th frame consecutively, \textsc{Peanut} may roll back to the 15th frame if there was a significant visual or auditory change between the 10th frame and the 20th frame according to the human annotation results. When the annotator finishes the annotation of two adjacent recommended keyframes, if both are determined to be continuous in both auditory and visual spaces, \textsc{Peanut} will automatically interpolate the annotation results of the in-between frames by tracking the movement of known sounding objects, as explained in Section~\ref{sec:recomendFrame}.

In the second strategy, the annotator can choose to automate the annotation of each frame by clicking on the \faIcon{angle-right} button. \textsc{Peanut} will preempt the annotation of the immediate next frame for the annotator to review and confirm. In this way, the annotator can closely inspect the automatic annotation result to ensure its correctness. The second strategy is especially useful when the auditory or visual context changes quickly in the video.

\crhighlight{ It's important to note that PEANUT provides users with the choice to select their preferred strategy at any moment during the video annotation, thus allowing dynamic control over the level of human engagement in the annotation process (DG2). }

\subsubsection{Annotation result review}
\label{sec:annotation_review}
\textsc{Peanut} uses two interfaces for the annotator to review the annotation result: \textit{frame-by-frame thumbnail} and \textit{annotated video playback preview} (Figure \ref{fig:system_review_screenshot}). The \textit{frame-by-frame thumbnail} interface displays the annotation result of each frame in a grid view, enabling the annotator to quickly detect frames with inappropriate annotations. The \textit{annotated video playback preview} interface allows the annotator to examine how the entire video looks after the annotation process is finished. The sound types are also displayed in semi-transparent white boxes above the bounding boxes. The annotator can also go to a specific frame to review and modify the annotation result by clicking the \textit{Move To} button. \crhighlight{These two features are designed to assist humans in spotting potential inaccuracies in AI annotations by employing supplementary review strategies, in order to mitigate  possible overreliance (DG2).}

% \hl{talk about how this help alleviate the overreliance problem}

\subsubsection{Active learning}
\label{sec:active_learning}

% \tlcomment{generalize to different topics -- how active learning can help}

\zzhighlight{Human-in-the-loop audio-visual data annotation may face two predominant challenges. First, the pre-trained object detector and sound tagging model could only be able to tackle certain objects or sound types and provide little support in those unfamiliar data. Plus, the detection accuracy of object and sound type may be contingent to event scenarios, while audio-visual scenarios are often highly diverse. It is possible that the model has trouble in recognizing a learned type from an unseen scenario. To address these challenges, as shown in Figure~\ref{fig:architecture}, \textsc{Peanut} adopts an active learning strategy to optimize the visual sound grounding network model, object detector and audio tagging model incrementally in real time as the users annotate more data. Because video frames and sounding objects in the same video are usually similar to each other, this active learning strategy allows the model to learn from ground truth data that likely closely resemble input data that it will process in the future, effectively adapting the model to the domain of the video. In this way, when encountering data type or event scenario unknown to AI model, human annotators can provide the model with a few ground truth annotations to enable it to classify data in the current specific scenario. This strategy also eliminates the need of granting a model with a generalized ability to handle a wide range of diverse scenarios in a single training \crhighlight{(DG4)}, which is formidable for current audio-visual models. } \looseness=-1

\begin{algorithm}
\caption{The audio-visual-sensitive binary search algorithm}\label{alg:nextFrame}
\KwName{\textbf{AudioVisualSensitiveBinarySearch}}
\KwInput{$LeftBoundFrame, CurrentHumanAnnotatedFrame,$\\$ GlobalRightBoundStack$}
\KwOutput{$NextHumanAnnotatedFrame$}


$left \gets LeftBoundFrame $\;
$right \gets CurrentHumanAnnotatedFrame $\;
$grs \gets GlobalRightBoundFrameStack$\;
$predObjects \gets PredictSoundingObjectFromPriorAnnotation(left, right) $\;

{
    \uIf{$right$ ==0 \textbf{or} ($predObjects == SelectedObjects[right]$ \textbf{and} $grs.length == 0$)} {
        \Return FarthestFrameNeedHumanAnnotation()\;
    }\uElseIf{$predObjects == SelectedObjects[right]$ \textbf{and} $grs.length > 0$)} {
        $cur \gets right$\; 
        $right \gets grs.pop()$\;
        \While{right != Null}{
            $predObjects \gets PredictSoundingObjectFromPriorAnnotation(cur, right) $\;
            \uIf{pred\_objects == SelectedObjects[right]}{
                PopulateFrameAnnotation($cur$, $right$)\;
                $cur \gets right$\; 
                $right \gets grs.pop()$\;
            } 
            \Else{
                $next \gets \floor{\frac{cur+right}{2}}$\;
                \Return $next$\;
            }
        }
        \Return FarthestFrameNeedHumanAnnotation()\;
    }
    \Else{
        $grs.push(right)$\;
        $next \gets \floor{\frac{left+right}{2}}$\;
        \Return $next$\;
    }
}
\label{alg:search}
\end{algorithm}

% \begin{algorithm}
%     \caption{Predict the sounding objects based on prior human annotation} \label{alg:predict}
%     \KwName{\textbf{PredictSoundingObjectFromPriorAnnotation}}
%     \KwInput{$RefFrame, TargetFrame$}
%     \KwOutput{$PredObjects$}
%     $targetObjects \gets DetectedObjects[TargetFrame] $\;
%     $PredObjects \gets \textit{[ ]} $\;
%     \For{$refObject$ \textbf{in} $DetectedObjects[RefFrame]$} 
%     {
%         $candidateObject \gets GetObjectwithClosestBox(refObject, targetObjects) $\;
%         \uIf{CheckObjectMatch(refObject, candidateObject) == true}{
%             $PredObjects.append(candidateObject)$\;
%         }
%         \Else {
%             $refObjectCopy \gets refObject.copy()$\;
%             $PredObjects.append(refObjectCopy)$\;
%         }
%     }
% \Return $PredObjects$\;
% \end{algorithm}


% \begin{algorithm}
%     \caption{Examine the auditory and visual consistency between two frames} \label{alg:change}
%     \KwName{\textbf{DetectAudioVisualChange}}
%     \KwInput{$left, right$}
%     \KwOutput{$isChanged$}
%     % $offset \gets right-left$\;
%     %  \For{$i \gets left+1$ \KwTo $right$}{
%         \If{DetectedObjects[left] != DetectedObjects[right]} {
%             \Return $true$\;
%         }
     
%         $left\_tags \gets AudioTags[left][0:3] $\;
%         $right\_tags \gets AudioTags[right][0:3]$\;
%         \If{haveCommonTags(left\_tags, right\_tags) == false} {
%             \Return $true$\;
%         }
%     % }
%     \Return  $false$\;
% \end{algorithm}


    \begin{algorithm}
        \caption{Automatically interpolate the annotation results for the frames between two human-annotated frames} \label{alg:populate}
        \KwName{\textbf{PopulateFrameAnnotation}}
        \KwInput{$left, right$}
        \KwOutput{$None$}
        \For{$i \gets left+1$ \KwTo $right$}{
            $predAnnotation \gets PredictSoundingObjectFromPriorAnnotation(left, i) $\;
            \If{AduioTags[i].contains(videoTag)} {
                $Annotate(i, predAnnotation)$\;
            }
        }
    \end{algorithm}
    
    \begin{algorithm}
        \caption{Return the farthest frame that needs human annotation } \label{alg:farthest}
        \KwName{\textbf{FarthestFrameNeedHumanAnnotation}}
        \KwInput{$None$}
        \KwOutput{$next$}
        
        $frameIndex \gets getFarthestHumanAnnotatedFrame()$\;
        \For{$i \gets 1$ \KwTo k}{
            \If{DetectAudioVisualChange(frameIndex, frameIndex+i) == false} {
                $continue$
            }
            \Else {
                \Return $frameIndex+i$\;
            }
        }
    \end{algorithm}

% Figure environment removed

  %\textsc{Peanut} first predicts the annotation for the present frame $(F_{\textit{j}})$ based on the nearest human-annotated frame to the left ($F_{i}$). Then \textsc{Peanut} uses the similarity between the predicted annotation and the actual human annotation on $F_{j}$ to decide the next frame for human labeling. If the similarity is high and no human-annotated frame exists to the right, \textsc{Peanut} will bring the annotator to the next frame that has a significant visual or auditory change relative to $F_j$ (or $F_{j+k}$ if no change is detected within $k$ steps); If the similarity is low, \textsc{Peanut} will move to the middle point between $F_i$ and $F_j$; If the similarity is high but a human-annotated frame ($F_k$) exists to the right, \textsc{Peanut} will use the same strategy to predict the annotation on $F_k$, and decide the next step according to the predicted annotation result's similarity with the human annotation result on $F_k$. 

\subsection{Algorithmic Methods}

\subsubsection{Recommending the next frame for human annotation}

\label{sec:recomendFrame}

We developed an \textit{audio-visual sensitive binary search} algorithm for \textsc{Peanut} to decide the next ``keyframe'' that needs human annotation (illustrated in Figure \ref{fig:algorithm_illustration}). The details of the algorithm are shown in Algorithm \ref{alg:nextFrame}. The index of the next key frame is decided by the similarity of the annotation between the left bound frame (\textit{lb}), the current human annotation frame (\textit{cur}), and a stack of right bound frames (\textit{rbs}). The left-bound frame refers to the preceding human-annotated frame that is the closest to the current frame on the timeline. On the contrary, the right-bound frames are those after the current frame, while the annotation similarity between the current frame and the right-bound frames needs to be confirmed. The calculation of the annotation similarity uses a "\textit{predict-select-compare}" strategy. Given the current frame, \textsc{Peanut} first predicts the sounding objects in the current frame by inheriting the human annotation of \textit{lb}. Then \textsc{Peanut} compares the user-selected sounding objects and the predicted sounding objects. If these two sets of objects are not the same, this indicates that there is a visual discontinuity between \textit{lb} and \textit{cur} that is not captured by \textsc{Peanut}. To locate the first frame of this ``discontinuity'' efficiently, \textsc{Peanut} adopts binary search and asks the human annotator to label the frame $\floor{\frac{lb+cur}{2}}$, meanwhile \textsc{Peanut} pushes \textit{cur} into \textit{rbs}. On the other hand, if the predicted sounding objects are the same as the user-selected one, \textsc{Peanut} will further examine whether a right-bound frame exists. If \textit{rbs} is not empty, \textsc{Peanut} will calculate the similarity between \textit{cur} and \textit{tf}, where \textit{tf} is the frame at the top of \textit{rbs}. \textsc{Peanut} will remove \textit{tf} from \textit{rbs} if the similarity holds and automatically interpolate the annotation results for the intermediate frames in between (Algorithm \ref{alg:populate}). After that, \textsc{Peanut} continues to compare \textit{cur} and the new frame at the top of \textit{rbs} until \textit{rbs} becomes empty. If the similarity does not hold, \textsc{Peanut} will ask the human annotator to annotate the frame $\floor{\frac{cur+tf}{2}}$. 

Otherwise, if the predicted sounding objects are the same as the user-selected ones and no right bound frame exists, \textsc{Peanut} will move to the first frame that follows the farthest human-annotated frame \textit{hf} and has a significant auditory or visual change compared to \textit{hf} (Algorithm \ref{alg:farthest}). If there is no significant auditory or visual change in the next $k$ frames (we used $k=10$ in our implementation of \textsc{Peanut}), \textsc{Peanut} will ask the human annotator to annotate the frame $hf+k$.



\subsubsection{Detecting visual and auditory changes}
\label{sec:algo_key_frame}

When a frame has a significant visual or auditory change compared to the last human-annotated frame denoted \textit{src}, \textsc{Peanut} needs a human annotator to annotate it. A frame denoted \textit{target} is considered to have a significant visual change relative to \textit{src} if (1) the number of detected objects varies between these two frames or (2) a bounding box in \textit{src} does not have a correspondence in \textit{target}, where the correspondence establishes if, given a bounding box \textit{i} on left, there is a bounding box in \textit{target} that overlaps \textit{i} and the overlapping area satisfies the Condition~\ref{cond:1}, where overlapping parameters $\alpha$ is 0.8 and $\beta$ is 0.05. The overlapping threshold decreases as the time difference between the target and source frames increases.

  \begin{equation}
    \label{cond:1}
    \begin{aligned}
        & OverlapArea(B_{highest}, A_i)  > \\ & [\alpha - (Index_{target}-Index_{src}) \times \beta] \times Area(A_i)
    \end{aligned}
    \end{equation}

The detection of significant auditory changes is based on an audio tagging model. \textsc{Peanut} uses a state-of-the-art pre-trained Audio Neural Networks~\cite{Kong2020PANNsLP} to predict the audio tags for each frame. An audio tag describes the possible type (e.g., train horn, race car, truck, as shown in Figure~\ref{fig:architecture}) of the sound corresponding to that frame.

\cmt{
    \subsubsection{Predicting the sounding objects for unannotated frame}
    \label{sec:predictNext}
    
    Given a human-annotated frame denoted as \textit{Src}, \textsc{Peanut} can automatically annotate the following frame \textit{Target}. For each bounding box $A_i$ selected in \textit{Src}, \textsc{Peanut} scores each candidate bounding box $B_j$ in \textit{Target} by adding up the distance of their center coordinates and the width and height difference. \textsc{Peanut} will inherit the tag and annotate the box of the highest score as a sound source in \textit{Target} if the overlapping area between $B_{highest}$ and $A_i$ satisfies:
    
    \begin{equation}
    \label{cond:1}
    \begin{aligned}
        & OverlapArea(B_{highest}, A_i)  > \\ & [\alpha - (Index_{target}-Index_{src}) \times \beta] \times Area(A_i)
    \end{aligned}
    \end{equation}
        
    \noindent where overlapping thresholds $\alpha$ is 0.8 and $\beta$ is 0.05. These thresholds decrease as the time difference between the target and the source frame grows; such thresholds accommodate the accumulated spatial offset of a sound source across a series of frames.  Otherwise, \textsc{Peanut} will copy $B_i$ to \textit{Target} and treat it as the sound source.
}

\subsubsection{Tackling complex audio-visual scenarios}

\crhighlight{Audio-visual data often inherently possess various complexities. For example, there might be situations where multiple sound-producing objects are active at the same time, making it challenging to discern the specific source of a sound. Besides, some sounds may start at different times and overlap with each other. Additionally, the video may lack a visual indicator of the sounding object, which could complicate the correlation between two modalities. To tackle those complexities, our algorithm focuses on identifying discrepancies or ambiguity in the auditory or visual modality, and asks humans to annotate keyframes. For example, in cases of simultaneous sounds from multiple objects, \textsc{Peanut} requests human assistance when the audio-tagging model's confidence score is low, indicating auditory uncertainty. Also, the detection of sound changes prompts annotation at the frames with new sound sources. When sounding objects are not visible, \textsc{Peanut} invokes human intervention at the initial sound frame, predicting the same annotation for subsequent frames until significant changes occur (sound cessation or the introduction of a new sound). The ``jumpback'' (Algorithm \ref{alg:search}) solicits human input at the midpoint when successive key frames have inconsistent annotations due to sounding object changing with no visual cue. }

\crhighlight{Our approach echos prior research \cite{gebreegziabher2023patat, zhang2023visar, bobes2021improving, jarrahi2018artificial} in human-AI collaboration for the completion of complex tasks where machine learning models primarily target the automation of repetitive and mundane tasks, resorting to human assistance when the uncertainty score surpasses a threshold.  }

\subsection{Implementation}
\subsubsection{Web app}

The front-end web application of \textsc{Peanut} is implemented in React based on \textit{react-image-annotate}\footnote{\url{https://github.com/UniversalDataTool/react-image-annotate}}, an open-source framework for the development of image annotation tools and hosted using Python's built-in HTTP server. The back-end server is developed using the Flask framework with a MongoDB database that stores user annotations and log data.

\subsubsection{Object detector}
\label{sec:object_detector}
\textsc{Peanut} uses the off-the-shelf Detectron2 \cite{wu2019detectron2} library for implementing its object detector. The object detector is built using the Faster R-CNN~\cite{ren2015faster} neural network architecture with R101-FPN feature pyramid networks~\cite{lin2017feature} and is pre-trained on the domain-general MS-COCO dataset~\cite{lin2014microsoft}. The object detector does not need to be retrained when using \textsc{Peanut} on videos from a new topic domain. From the input of a video frame in bitmap format, the object detector can identify objects of 80 different types (e.g., ``train'', ``violin'', ``dog'', etc.) and return the corresponding coordinates of the bounding box and the type of each object. \looseness=-1

\subsubsection{Active visual sound grounding}
The Visual Sound Grounding (VSG) network model is trained to identify objects that make sounds among the candidate objects in each video frame. The network is built on top of~\cite{Tian_2021_CVPR}. The network is first pre-trained on the AVE dataset~\cite{tian2018audio}. Afterward, it is iteratively and incrementally fine-tuned with newly annotated data during the active learning stage. The network takes the feature of the current audio clip and objects that are proposed by the object detector (Section~\ref{sec:object_detector}) as inputs and predicts the likelihood that each object is associated with the sound, which allows the model to identify and remove bounding boxes that correspond to likely-silent objects from the potential candidates of bounding boxes.

 \looseness=-1