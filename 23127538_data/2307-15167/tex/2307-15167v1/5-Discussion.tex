\section{Discussion and Implications}
In this section, we discuss several novel interaction strategies that we used in the design of \textsc{Peanut}, the lessons we learned in the user study,  and their implications for human-AI collaboration.

\subsection{Connecting and Unifying Different Data Modalities}
\zzhighlight{The design of \textsc{Peanut} introduces a strategy for human-AI collaboration in multi-modal tasks that demand high perceptive and cognitive overhead to process and associate input from different modalities.} \textsc{Peanut} uses two single-modal models that can provide partial automation. The human user then contributes by (1) validating the partial automation result; (2) associating the partial automation result of one model from one input modality with that of a different model from a different input modality to achieve the multi-modal end goal. Many participants found this partial automation approach effective for reducing cognitive load and fatigue when annotating a large number of similar frames. For example, P8 said ``\textit{I almost fell asleep when using the first interface (baseline). In comparison, the second one (\textsc{Peanut}) allowed me to focus on those (frames) worth my effort so that I did not need to do a tedious, repeated annotation for every picture}''. \looseness=-1

This new interaction strategy represents the adoption and development of classic theories in multi-modal interfaces~\cite{oviatt_ten_1999, oviatt_2000_perceptual} in a new domain. Instead of an interface that processes user input data in multiple (usually complementary) modalities and tries to better understand the input from one modality using the input from another modality (mutual disambiguation~\cite{oviatt_mutual_1999}), \textsc{Peanut} works in the reversed direction, utilizing humans' perceptive and cognitive capabilities of understanding inputs from one modality using inputs from another modality in order to ground auditory data to visual data. \crhighlight{The strategy discussed in this paper and design implications from the study could translate to other multi-modal data annotation tasks like natural language visual grounding~\cite{anayurt2019} visual question answering~\cite{antol2015vqa}. Besides,} we expect this interaction strategy to be useful in other application domains beyond data annotation, such as helping users with visual or hearing impairments better understand videos as discussed in Section~\ref{sec:future_work}.

% \tlcomment{talk about the connection of this method to classic multi-modal paper, and the applications}


% \tlcomment{implications on combining audio and visual modalities in data annotation -- how purely visual algorithms (e.g., object detection) and purely auditory algorithms (what we used for detecting changes in audio?) can assist in annotations in complementary modalities}

% \tlcomment{Represents a new paradigm of partial automation -- partial automation by modality}

\subsection{Minimizing the Overhead Cost of AI in Human-AI Collaboration}
When we design the user workflow of \textsc{Peanut}, an important goal is that the use of \textsc{Peanut} should not introduce additional burdens or learning barriers compared to what the user would experience if they manually annotated the same data. The user does not need to learn a new skill or make any configuration of \textsc{Peanut} before starting to use it. Anything the user does in \textsc{Peanut} is either a sub-process of what they would need to do in manual annotation (e.g., choosing the sounding object from several candidate boxes instead of identifying the sounding object in the video and drawing a box for it) or the same process but less repetitive (e.g., annotating for only the key frames instead of all the frames). This is different from many other human-AI collaboration scenarios, where the use of AI incurs significant overhead, requiring justifying the use of AI by assessing whether the benefit exceeds the cost.

To achieve this goal, we used a strategy of keeping the user's original workflow as much as possible. For example, \textsc{Peanut} can identify the next key frame that the user can label (Section~\ref{sec:automatic_annotation}). This intelligent feature does not require any additional input from the user, as it relies on only the user's annotation result for the current frame and the current state of the frame sequence. The user's interaction with the system also remains the same as in a manual labeling tool---they click on the ``Next'' button to work on a different frame after finishing annotating the current one with the only difference being that the ``next'' frame is no longer necessarily the $(n+1)^{th}$ frame after the $n^{th}$ frame. The user retains the ability to freely move along the sequence of frames, edit previously annotated frames, or annotate frames out of the ``recommended'' order as they wish. \textsc{Peanut} will automatically track the next recommended frame to be annotated without requiring user intervention, allowing users to retain control and agency in the human-AI collaborative data annotation process. The participants confirmed the effectiveness of human-AI collaboration in the interview. For example, P5 said that ``\textit{I felt it is smooth to work with the algorithm behind \textsc{Peanut}. It takes care of many simple annotations that had to be done by myself in the previous fully manual version}''.


Another common type of ``cost'' in human-AI collaboration that we address in \textsc{Peanut} is the degraded accuracy due to users' overreliance on AI, which has been identified as a key issue in AI-assisted data annotation~\cite{askhtorab_aiassisted_2021}. When the audio-visual sound grounding model identifies candidate objects and removes likely silent objects from the candidates, we still expect the user to maintain their attention and edit the results if needed.  To address this challenge, \textsc{Peanut} provides video playback and thumbnail preview features (Section~\ref{sec:annotation_review}) that allow the user to quickly validate the result and identify annotation issues if there are any. In the user study, these features indeed helped participants locate incorrect automatic annotations and consequently calibrate their trust in AI automation. For example, P14 said that ``\textit{I totally trusted the system to label the frames at the beginning. However, as I looked at the review of the annotation result, I realized the system could make mistakes and I have to go back to improve the annotation}''.


\cmt {
    \tlcomment{Goals: no additional burden/learning barrier for the user -- the user doesn't need to do anything extra or learn any new tasks compared with manual annotation}
    
    \tlcomment{Fulfill a secondary agent initiative (locating key frames with changes) through the manipulation of human annotation order using a binary search approach}
    
    \tlcomment{Perhaps discuss the overreliance issue in human-AI collaboration in labeling~\cite{askhtorab_aiassisted_2021} -- how designs, e.g., video review address the overreliance issue}
}

\subsection{The Role of Partial Automation in Pursuit of Full Automation}
The traditional workflow of data annotation for ML models represents an approach that goes from \textit{full manual} efforts directly to \textit{full automation}---human users go through a fully manual process to create a dataset, which is then used to train a fully-automated ML model. In contrast, \textsc{Peanut}'s approach highlights the role of incrementally-trained partial-automation models that can bridge the two ends in pursuit of full automation.

In \textsc{Peanut}'s model, as the user is annotating the data in a manual process, partial-automation models are incrementally trained with the user's incomplete annotation of each frame. Thank to the characteristic of video data that frames in the same video are usually similar to each other, domain-general partial-automation models can quickly adapt to the specific domain of the video in a \textit{few-shot} fashion. The active learning process guided by the key frame selections (Section~\ref{sec:algo_key_frame}) in \textsc{Peanut} incrementally improves the performance of partial-automation models as the user annotates more frames, reducing human efforts to reach the data size and data quality required for training an end-to-end fully-automated model. During the interview, P4 commented: ``\textit{One feature I can imagine is that, with my annotation on a few frames at the beginning of a video, the system can learn to label the following frames or even other videos with similar content and do the remaining annotation on behalf of me}''. \looseness=-1

We expect that such an approach can be useful for a variety of other human-in-the-loop ML applications. An example is the human/ML hybrid sensing approach such as Zensor~\cite{laput_2015_zensors} where sensors can switch between crowd intelligence and ML to adapt to environmental changes. However, unlike Zensor which toggles between either full automation or full manual for the primary sensing task and uses human annotation results as a validation method, \textsc{Peanut}'s approach allows for more flexible partial-automation states in between, taking advantage of the partially annotated data to accelerate the annotation before a fully automated model is ready.

\subsection{Mitigating Biases in AI-Assisted Data Annotation}

\zzhighlight{While the issue of biases is less prominent in task domain of \textsc{Peanut} since unlike many other domains vulnerable to subjective bias (e.g., hate speech detection \cite{Mollas2020ETHOSAM}), the result of sounding object localization does come with an objective truth, mitigating the biases of AI and human annotators in this task is still an important consideration. Presumably, the addition of AI assistance in data annotation could introduce or amplify two kinds of biases in annotation.} 

\zzhighlight{First, AI models in \textsc{Peanut} may introduce intrinsic biases to annotation results~\cite{Dar2021AFT, Leavy2020MitigatingGB, Blanzeisky2021AlgorithmicFI}. These biases originated from the dataset on which these models are pre-trained on. To address them, \textsc{Peanut} identifies key frames with previously unseen objects and proactively requests human annotation in these key frames. On those key frames, the role of AI is to \textit{assist} with human annotation by suggesting label candidates for user to use rather than attempting to fully automating their annotations using the pre-trained model. The design allows users to take control and accountability of annotations on keyframes and offsets AI biases with human judgement.}

\zzhighlight{In addition to introducing biases themselves, AI models may also amplify human errors when performing automated annotation based on previous human annotations \cite{Ahmed2021AttenuationOH, Chen2021UnderstandingAM, Bhargava2019ExposingAC}. For example, when there are multiple guitars in a key frame in the scene, the human annotator may have difficulty in identifying which guitar is currently making the sound  and select a wrong guitar as the sound source. In this case, the object detector in \textsc{Peanut}  will propagate this wrong annotation to automated annotations in subsequent frames. The playback reviews (Section~\ref{sec:annotation_review}) could be useful to mitigate this issue by providing a global context that facilitates users to identify errors in continuous scenes.}

\zzhighlight{We also expect to introduce other bias-reducing strategies in the future version of \textsc{Peanut}. For example, we may implement assistance functions to support human decision-making of the sound source at key frames when the human annotator is unsure. For example, \textsc{Peanut}  may leverage the model-inferred depth and direction information of the sound to indicate the likelihood of each visual region in the frame containing the sound source object. \looseness=-1}



\section{Limitations \& Future work}
\label{sec:future_work}


\subsection{Incorporating Speech and Natural Language Models}
The current version of \textsc{Peanut} only supports the use of auditory and visual models, but not natural language understanding (NLU) models that process the content of speech in videos. Speech is one of the most ubiquitous sound sources in videos. Human speech usually contains important semantic information relevant to the surrounding audio and visual scenes in physical environments. Moreover, unlike other sounds (\emph{e.g., traffic, rain, dog barking}), speech can be transcribed into text using automatic speech recognition~\cite{hinton2012deep}. Benefitting from advances in NLU~\cite{schmitz2012open,berant2014semantic,chen2015event,song2018graph}, we can automatically extract structured and abstracted semantic content from transcribed text. This could provide opportunities for us to incorporate NLU models with \textsc{Peanut} to facilitate more powerful multimodal data collection to support multidisciplinary research across audio, visual, and language.

\subsection{Expanding to Audio-Visual Tasks beyond Sounding Object Localization}
 We implemented and evaluated the current version of \textsc{Peanut} in the context of the sounding object localization task. With its support for user interaction with both audio and visual modalities, \textsc{Peanut} can be easily expanded to a range of multi-modal audio-visual tasks, such as audio-visual event localization~\cite{tian2018audio,wu2019DAM}, audio-visual video parsing~\cite{tian2020unified,wu2021exploring}, and audio-visual video captioning~\cite{tian2018attempt,rahman2019watch}. 

Audio-visual event localization aims to temporally localize audio-visual events\footnote{Audio-visual events are synchronized video segments in which the sound sources are visible and their sounds are audible.} and recognize the categories of events. Toward more unified multi-sensory perception, audio-visual video parsing aims to recognize event categories bind to sensory modalities and find temporal boundaries of when such an event starts and ends. To train models for addressing these two tasks, two datasets: AVE~\cite{tian2018audio} and LLP~\cite{tian2020unified} have been collected, respectively. Due to the lack of efficient annotation tools, only second-level temporal boundaries with the corresponding categories are fully manually annotated in these datasets. We believe that more precise frame-level annotations will enable more accurate audio-visual ML models and facilitate the development of future research.

In addition to facilitating existing tasks, our system has the potential to help researchers investigate new problems. For example, by collecting temporal boundary, object box, and category annotations, we can formulate a new space-time audio-visual parsing task that aims to perform spatio-temporal multi-modal analysis over videos to predict temporal boundaries of audio, visual, and audio-visual events, their associated semantic categories, and spatially localized sounding objects.   

\subsection{Release and Deployment}
We plan to release the \textsc{Peanut} tool for public use. We are also currently planning a large-scale deployment to complete the annotation of all 4,143 video clips in the AVE dataset~\cite{tian2018audio} for sounding object localization. \cmt{The deployment will use Amazon Mechanical Turk workers as human annotators, which can help us understand \textsc{Peanut}'s performance with a big group of users without any ML background and annotation experience} The annotation result on the full AVE dataset will allow us to train a new \textit{supervised} sounding object localization model with the dataset, compare its performance with the current state-of-art model, and illustrate \textsc{Peanut}'s effectiveness in improving the performance of ML models by enabling the creation of better-quality annotated datasets.
    
    % \tlcomment{use implications of this system on tools to help users with visual/hearing impairment to understand videos}
    


\section{Conclusion}
In this paper, we presented \textsc{Peanut}, a human-AI collaborative audio-visual annotation tool for improving the data annotation efficiency of the sounding object localization task. A controlled user study of \textsc{Peanut} demonstrated that a human-AI collaborative approach with several new mixed-initiative partial-automation strategies can enable human annotators to perform the data annotation task faster while maintaining high accuracy. Our findings provide design implications for AI assistance in data annotation as well as human-AI collaboration tools for working with multi-modal data. \looseness=-1