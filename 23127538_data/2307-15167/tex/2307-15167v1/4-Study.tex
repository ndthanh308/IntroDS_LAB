

\section{User Evaluation}
To evaluate \textsc{Peanut}, we conducted a user study\footnote{The study protocol has been reviewed and approved by the IRB at our institution.} with 20 users to compare the efficiency and accuracy of audio-visual data annotation using \textsc{Peanut} with those using a baseline system without intelligent features. The results of the study suggest that \textsc{Peanut} can help users annotate data for the sounding object localization task at a faster speed than in the baseline condition, while also improving the annotation accuracy at the same time. The user study also validated the usability of \textsc{Peanut} and provided insight into user reflections on their experiences using \textsc{Peanut}. 

% We conducted a user evaluation to assess the efficiency and usability of \hl{SystemName}, and user's annotation accuracy. \hl{\{\textit{assess model performance improvement or human-in-the-loop object detector improvement?}\}} 

% Specifically, our evaluation study aims (1) to see whether participants are able to annotate raw videos efficiently and correctly using \hl{SystemName}, (2) to gain insights into the advantages, disadvantages, and usability of \hl{SystemName}.

% In a within-subjects experiment, we compared the full features of \hl{SystemName} to a baseline version of the interface. The quantitative and subjective results were strongly in favor of the efficiency, usability and annotation accuracy supplied by \hl{StoryName} over the baseline version.

% \subsection{Baseline interface}

% Because there is no existing audio-visual data annotation tool, we created a baseline interface to compare with the \hl{SystemName}. The baseline interface has the same UI layout as the \hl{StoryName}, while it does not feature automatic object detection and sounding object recommendation. Therefore, users need to manually box and classify the sounding object(s) at each frame. The design of baseline interface is intended to ensure that the speed, accuracy, and usability difference between two conditions are mainly attributed to the presence of advanced features implemented in \hl{StoryName}.  

% \subsection{Dataset}



\subsection{Participants}

For this study, we recruited 20 participants through university mailing lists. 7 were undergraduate students, 8 were Master's students, 4 were doctoral students, and 1 was a high school student. Each participant was compensated with \$15 USD for their time.

Our participants had varied levels of prior data annotation experiences and ML backgrounds. 8 participants had no prior experience with data annotation and 12 had annotated data at least once. 5 participants had no ML background, 5 participants had at least a beginner level of ML knowledge (have taken introductory ML courses or had basic ML knowledge), 5 had an intermediate level of ML expertise (have taken advanced ML courses or with equivalent expertise), and 5 identified themselves as experts in ML (experienced researchers or practitioners of ML).


\subsection{Study Design}

Each user study session lasted around 70 minutes and was conducted remotely on Zoom due to the impact of the COVID-19 global pandemic. Before the beginning of each session, the participant signed the consent form and completed a demographic questionnaire. Participants accessed \textsc{Peanut} using the browser on their own computers and shared their screens with the experimenter. After receiving a 5-minute tutorial on how to use \textsc{Peanut}'s interface, each participant completed the annotation tasks under two conditions in random order (see Section~\ref{sec:condition}) and completed a post-study questionnaire on their perceived usability and usefulness of \textsc{Peanut}. The study session ended with a 10-minute semi-structured interview with the participant in which they reflected on their experience interacting with \textsc{Peanut}. All user study sessions were video recorded. 

\subsubsection{Dataset}
\label{sec:dataset}
% \begin{itemize}
%     \item AVE dataset
%     \item Frame sampling
%     \item Sampling of audio clips
%     \item Expert annotation
%     \item Data split for different condition groups
% \end{itemize}
In this study, we trained and evaluated \textsc{Peanut} using the Audio-Visual Event (AVE) dataset~\cite{tian2018audio}, which is a widely used benchmark dataset for the audio-visual localization task. The full AVE dataset contains 4,143 video clips from a wide range of topics and domains (e.g., ``Church Bell'', ``Male speech'', ``Dog Bark'') in 28 event categories. Each video clip in the AVE dataset is about 10 seconds long. We re-sampled each video at 8 FPS (a common practice in audio-visual data annotation so there are fewer frames to annotate in each video). For the user study, we used a sample of 30 video clips from the AVE dataset. \zzhighlight{The sampled dataset contains 10 different event categories such as music play, car race, male/female speech to investigate the effectiveness of \textsc{Peanut} on videos with a variety of topics.} Each of these 30 video clips was manually annotated by two experts as ground truth data for evaluating the accuracy of user annotation. Two authors, who were experts in audio-visual learning, annotated the ground truth data independently using the baseline Full Manual version of \textsc{Peanut}. We used cIoU (see Section \ref{sec:accuracy}) to measure the inter-annotator agreement. The average cIoU score between the annotation results by two experts is 0.96, suggesting a very high agreement between the two expert annotators.


\subsubsection{Conditions}
\label{sec:condition}
The study used a within-subject design, where each participant performed tasks under two conditions in random order. In the experiment condition (\textsc{Peanut}), the participant used the fully functional \textsc{Peanut} tool to label videos from a split of our sample dataset in 25 minutes. In the control condition (Full Manual), the participant used a baseline version of \textsc{Peanut} with all ``intelligent features'' (object detector, active visual sound grounding, and annotation interpolation) turned off to label the videos from the other split of our sample dataset in 25 minutes. The control condition reflected the essential practices of the current video or image data annotation tools \cite{labelme2016, xtract, dutta_2019_via}. The split of the videos between the two conditions and the order of the videos in each condition were randomized in each study session. In Table~\ref{tab:performance}, we also include the accuracy score of a ``Fully Automated'' model using the pre-trained VSG network as a baseline for annotation accuracy. 


\subsubsection{Procedure}

In the study, participants were asked to annotate audio-visual data for sounding object localization using \textsc{Peanut} and the baseline tool. We randomized the order to control for learning effects. The study procedure consisted of three parts: a 30-minute session with the first interface, a 30-minute session with the second interface, and a 10-minute session for the post-study interview and a post-study questionnaire. In each 30-minute session, the experimenter started with a 5-minute tutorial teaching participants how to use the interface in the condition. Subsequently, participants had 25 minutes to annotate as many video frames as possible using the tool provided. After completing both sessions, each participant filled out a post-study questionnaire. The study session ended with a 10-minute semi-structured interview.


% Experimenters started by introducing study goals to the participants. Afterwards, experimenters played a \hl{X}-minute video to instruct participants on how to use the first interface. The participants 

% \begin{table}
%     %\small
%     \def\arraystretch{1.1}
%     \centering
%         \begin{tabular}{ |c|c|c|c|c|c|c|  }
%             %  \hline
%             %  \multicolumn{6}{|c|}{Participants' demographics} \\
%              \hline
%              \textbf{Participant ID} &\textbf{SoC (c)}&\textbf{SoC (e)}& \textbf{\# frame (c)} & \textbf{\# frame (e)} & \textbf{cIoU (c)} & \textbf{cIoU (e)} \\
%              \hline
%              P1 & 6.21 & 2.49 & 169 & 560 & More than 6 times a week & \\ 
%              P2 & 7.94 & 9.17 & 126 & 410 & More than 6 times a week &\\ 
%              P3 & 12.94 & 16.42 & 56 & 152 & More than 6 times a week &\\ 
%              P4 & Female & Female & 25-34 & 4 & 1-3 times a week &\\ 
%              P5 & Female & Female & 35-44 & 7 & 4-6 times a week &\\ 
%              P6 & Female & Male & 25-34 & 4 & More than 6 times a week &\\ 
%              P7 & Female & Female & 25-34 & 7 & More than 6 times a week &\\ 
%              P8 & Female & Male & 25-34 & 5 & 1-3 times a week &\\ 
%              P9 & Male & Male & 25-34 & 5 & 1-3 times a week &\\ 
%              P10 & Female & Female & 35-44 & 7 & Not reported &\\
%              P11 & Female & Female & 35-44 & 6 & More than 6 times a week &\\
%              P12 & Female & Female & 35-44 & 6 & More than 6 times a week &\\ 
%              P13 & Female & Female & 35-44 & 6 & More than 6 times a week &\\ 
%              P14 & Female & Female & 35-44 & 6 & More than 6 times a week &\\ 
%              P15 & Female & Female & 35-44 & 6 & 4-6 times a week &\\ \hline
%         \end{tabular}
%     \centering
%     \caption{Statistics of user performance in control and experiment conditions. (c) represents control condition, (e) represents experiment condition, SoC represents the average seconds of completion for annotating a frame, \# frame represents the total number of frames that a participant has annotated in the study, cIoU is a quantitative measure for annotation accuracy (see Section \ref{sec:accuracy}) } 
%     \vspace{-0.4cm}
%     \label{tab:demographics}
% \end{table}

\begin{table}[t!]
    \def\arraystretch{1.2}
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
      & \textbf{Average SoC $\downarrow$} &\textbf{ \# of Frames $\uparrow$} & \textbf{cIoU $\uparrow$} \\
     \hline
        Full Automated & N/A & N/A & 0.33\\
        Full Manual & 7.73 & 169.45 & 0.72\\
        \textsc{Peanut} & 5.12 & 488.85 & 0.93\\ \hline
    \end{tabular}
    \caption{Statistics of participants' performance in control (Full Manual) and experiment (\textsc{Peanut}) conditions. \zzhighlight{SoC means second of completion per frame, \# of frames means the total number of frames that a participant annotates in the condition, cIoU means consensus intersection over union, which is a well-accepted measure for the accuracy of bounding box annotation.} The cIoU accuracy of the fully automated model is also provided as a reference.}
    \label{tab:performance}
\end{table}

\subsection{Results on Annotation Performance}
We expect \textsc{Peanut} to accelerate the annotation task for participants in two ways: (H1) the user can annotate each frame faster because of the model-suggested bounding boxes of detected visual objects and the predicted audio tags; (H2) the user needs to manually annotate fewer frames due to the visual-audio-sensitive binary search process.

As shown in Table~\ref{tab:performance}, we report three statistics. The average \zzhighlight{second of completion (SoC)} validates H1, and the \# of Frames validates the combined effect of H1 and H2. cIoU measures
the impact of using \textsc{Peanut} on the accuracy of the annotated data. \zzhighlight{When reporting those statistics, we also report the standard deviation among all users in the target condition.} We will explain each statistics and its results in this section.\looseness=-1

\subsubsection{Time to completion}

We calculated the average seconds of completion (Average SoC) on human-annotated frames. SoC measures the average time participants spent annotating each video frame (not including those automatically annotated by the model). The difference between the two conditions in SoC demonstrates the effectiveness of \textsc{Peanut} in reducing the effort and cognitive load in the annotation of individual frames, such as displaying the potential candidates of objects and removing silent objects. As shown in Table \ref{tab:performance}, the average SoC is 5.12 per frame \zzhighlight{($SD = 1.87$)} in the experiment condition and 7.73 per frame \zzhighlight{($SD = 2.23$)} in the control condition. The paired t-test showed that there is a significant difference between the average SoC under the two conditions ($p<0.01$), indicating that participants can annotate a frame faster with \textsc{Peanut} than with the baseline interface.


\subsubsection{The number of annotated frames}

We calculated the average number of frames that a participant annotated in a 25-minute session (\# of Frames). Note that the count includes the frames automatically annotated by \textsc{Peanut} in the experiment condition. In addition to capturing the effect of \textsc{Peanut} features reflected in average SoC, the difference in the average number of frames between two conditions also reflects the effectiveness of automatic annotation in \textsc{Peanut}---instead of annotating all the video frames as in the baseline condition, participants only need to annotate key frames identified by \textsc{Peanut} and verify automatic annotation in the experiment condition. As shown in Table \ref{tab:performance}, the average number of annotated frames is 488.85 \zzhighlight{($SD = 167.93$)} in the experiment condition, and 169.45 \zzhighlight{($SD = 68.26$)} in the control condition. The paired t-test showed that there is a significant difference between the average number of frames annotated under the two conditions ($p<0.001$), indicating that participants can annotate more frames with \textsc{Peanut} than with the baseline interface in a 25-minute session. \looseness=-1

% We found no significant difference in the average number of annotated frames among participants with different levels of ML expertise when using \textsc{Peanut}. Interestingly, users who had no prior annotation experience, on average, annotated more frames using \textsc{Peanut} (\zzhighlight{$AVG = 630.4, SD = 187.78$)} than those who had previously annotated data \zzhighlight{($AVG = 394.5, SD = 95.46$) ($p<0.05$)}. A possible explanation of this phenomenon is that experienced data annotations would be more careful with examining and validating model-generated results. We will leave the follow-up investigation of this phenomenon for future work.

\subsubsection{Annotation accuracy}
\label{sec:accuracy}

We used \textit{consensus intersection over union} (cIoU)~\cite{senocak2018learning} to assess the participants' annotation accuracy in each condition. cIoU is a common metric for quantitatively evaluating the accuracy of bounding box annotations. Given a video frame, cIoU assigns scores to each pixel according to the consensus of multiple expert annotations. In specific, the ground-truth bounding boxes annotated by experts are first converted into binary maps $\{\textbf{b}_j\}^{N}_{j=1}$, where N is the number of expert annotators. Then, we calculate a representative score map $\textbf{g}$ from $\{\textbf{b}_j\}$ considering the consensus of experts:  

\begin{equation}
    \textbf{g} = min(\sum_{j=1}^{N} \frac{\textbf{b}_j}{\#consensus}, 1)
\end{equation}

\noindent where $\#consensus$ is a parameter indicating the minimum number of expert annotations to reach agreement. Since we have two expert annotators, we set $\#consensus$=1 by the majority rule in our study. Given this weighted score map $\textbf{g}$ and participant's annotation \textbf{$\alpha$}, we define cIoU as:

\begin{equation}
    cIoU = \frac{\sum_{i \in A} \textbf{g}_i}{\sum_i \textbf{g}_i + \sum_{i \in A-G} 1}
\end{equation}

\noindent where \textit{i} indicates the pixel index of the map, $A = \{i | \alpha_i = 1 \}$ means the set of pixels that the participant annotates, $ G = \{i | g_i > 0\}$ means the set of pixels in the weighted ground truth annotation.\looseness=-1

We calculate the cIoU score for each annotated frame of each participant. The average cIoU score for all annotated frames (all by human) in the control condition is 0.72 \zzhighlight{($SD = 0.14$)}, and is 0.93 \zzhighlight{($SD = 0.09$)} for all annotated frames (by human or by the system) in the experiment condition. A paired sample t-test showed a significant difference between the average cIoU under the two conditions ($p<0.001$). For comparison, the cIoU of a fully-automated VSG (off-the-shelf pre-trained without any human annotation) on the same sample of videos is 0.33 \zzhighlight{($SD = 0.08$)}. The paired t-test shows that the cIoU for the fully-automated model is significantly lower ($p<0.001$) than both human-annotated conditions (Full Manual and \textsc{Peanut}). The result indicates that the use of \textsc{Peanut} can achieve a high annotation accuracy (and even higher than the full-manual control condition in our experiment).



We suspect two possible reasons for the observed improvement in accuracy in the \textsc{Peanut} condition compared to the Full Manual condition: (1) The use of object detectors may have improved consistency in selecting the bounding boxes. The annotation of some sounding objects can be inherently ambiguous---for example, when a sounding object is a person playing the violin, should the annotator draw a box for the person (that includes the violin) or only the violin? The labeling of situations like this can be inconsistent in the Full Manual condition (e.g., the violin is selected in some frames, while the person is selected in other frames). In this example situation, the objector detector would consistently go with the person because it prefers larger boxes when choosing from two overlapped ones (which is often consistent with the common best practice of sounding object localization), reducing the bounding box inconsistency in data annotation. (2) Drawing accurate bounding boxes is a challenging task on its own. The bounding boxes drawn manually by users often do not align as accurately with the edges of the object as the model-detected boxes.  Moreover, because data annotation with the baseline interface is a highly repetitive and tedious process, the precision of participants' manually created bounding boxes could fluctuate due to fatigue (e.g., not modifying a box when an object has moved ``a little bit'', but still aligns mostly with the box). We plan to further investigate the factors that contribute to the improvement of accuracy as a future work direction.\looseness=-1


\subsubsection{The impact of user expertise}

% \tlhighlight{We found no significant difference in annotation accuracy between participants who had no prior annotation experience and participants who had previously annotated data. We found no significant difference in annotation accuracy among participants with different levels of ML expertise, either.}

\zzhighlight{Comparing the annotation experience and efficiency, users without prior annotation experience annotated faster than those with prior annotation experience in the \textsc{Peanut} condition. The average numbers of frames that participants with and without data annotation experience annotated in the experiment condition were 394.5 ($SD = 95.46$) and 630.4 ($SD = 187.78$) respectively. An unpaired t-test shows that participants without data annotation experience annotated significantly more frames than those with annotation experience ($p=0.043<0.05$). Participants without annotation experience also on average spent less time on each frame ($AVG = 4.35s, SD = 0.75$) than those with annotation experience ($AVG = 6.26s, SD = 0.58$) ($p=0.031<0.05$). There was no significant difference between their average number of frames in the Full Manual condition. The efficiency improvement from using \textsc{Peanut} (comparing \# of Frames between \textsc{Peanut} and Full Manual conditions) is significant for both groups.}

\zzhighlight{In terms of ML expertise, there was no significant difference in all three metrics (Average SoC, number of Frames, cIoU) among groups of participants with different levels of ML expertise in a one-way ANOVA test. We found no significant difference in annotation accuracy between participants who had no prior annotation experience and those who had prior annotation experience, either.} 


% the average of total frames finished by participants with no ML background, beginner, intermediate and expert ML level were 732.2, 812.4, 405.4, and 683.2 respectively. The one-way ANOVA test did not find statistical significant difference among them. As to SoC, these four groups on average took 6.67s, 7.79s, 8.89s, and 5.34s respectively, and no statistically significant difference was found among them. We neither found statistically significant difference for cIoU among different ML expertise levels.}

\zzhighlight{For the observed difference in efficiency between groups with and without prior annotation experiences, a possible explanation is that users with prior data annotation experience may be more skeptical to automated annotation and thus spent more time double-checking the results. We plan to investigate this phenomenon more closely in future studies.}\looseness=-1

% \hl{There are two possible reasons accounting for the accuracy increase}: First, since we used a manually optimized object detector, the AI-recommended bounding boxes usually cover whole sounding objects which are consistent with our experts’ annotations. However, certain participants sometimes might draw smaller sounding regions due to the ambiguity of labeling boxes. Second, the precision of one participant's annotation could fluctuate in the study due to the fatigue or their implicitly varied standard for drawing an appropriate bounding box.

% We also analyzed the sources of annotation errors raised in the experiment condition by reviewing the study recordings. We found that most of errors still arose from the participants' overreliance on \textsc{Peanut}. For example, participants sometimes ignored the wrong \textsc{Peanut} annotation as they kept clicking \textit{Next} button fast. Besides, when clicking \textit{Next Label} button, some participants did not check the automatic annotation of the intermediate frames properly though \textsc{Peanut} made mistake. Nevertheless, many participants were able to realize the limitation of the automatic annotation via the frame-by-frame thumbnail and annotation playback preview at the end of each video annotation session, and manually modified the incorrect annotations by themselves.


% Figure environment removed


\subsection{Results on User Behaviors and Experiences}

\subsubsection{Usage statistics}
We analyzed the interaction logs and screen recordings of participants in the experiment (\textsc{Peanut}) condition to understand how participants interacted with \textsc{Peanut}'s AI assistance, especially on the ratio between manual vs. automated annotation and how often they edit the automated annotation results. As we see in Table~\ref{tab:performance}, each participant, on average, annotated 488.85 frames in a session. Among them, an average of 37.2 (7.6\%) frames are manually annotated by the participant, 421.7 (86.2\%) are automatically annotated without any user modification, 29.9 (6.1\%) are first automatically annotated by the model and then modified by the user. In each session, a participant on average resized/moved the model-predicted bounding boxes of visual objects in 12.6 frames (2.6\%), created new bounding boxes for visual objects in 14.2 frames (2.9\%), and edited the model-predicted audio tags in 3.1 frames (0.6\%).\looseness=-1

\zzhighlight{Among the annotations made in the \textsc{Peanut} condition, the average cIoU of the 13.8\% human annotated frames is 0.81 ($SD = 0.12$) and average cIoU of the 86.2\% automated annotated frames is 0.96 ($SD = 0.07$). This result indicates that with a small portion of user annotation on mostly ``key frames'' identified by our algorithm, the model can get very accurate at automated annotations (compared to the average cIoU of 0.33 in the Full Automated condition), indicating the effectiveness of \textsc{Peanut}'s active learning pipeline.} 


\subsubsection{Post-study questionnaire}

In a post-study questionnaire, we asked each participant to rate seven statements about the usability, usefulness, and user experience of \textsc{Peanut} on a 7-point Likert scale from ``strongly agree'' to ``strongly disagree''. The results are shown in Figure \ref{fig:questionnaire}. Specifically, \textsc{Peanut} scored on average 4.9 ($SD = 1.92$) on ``\textit{\textsc{Peanut} is easy-to-use}'', 5.7 ($SD = 2.10$) on ``\textit{I can learn to use \textsc{Peanut} easily}'', 5.6 ($SD = 2.19$) on ``\textit{I found the feature of recommending candidate boxes is useful}'', 5.15 ($SD = 2.08$) on ``\textit{I found the feature of navigating to next frame to label is useful}'', 5.15 ($SD = 2.16$) on ``\textit{I found the features of reviewing annotation result are useful}'', 5.5 ($SD = 2.16$) ``\textit{I found the feature of playing audio corresponding to a frame is useful}'', and 4.8 ($SD = 2.11$) on ``\textit{I enjoy using \textsc{Peanut}}''. The results indicate that our participants generally found \textsc{Peanut} easy to use, and the design features are useful for their annotations.

When a participant rated a statement lower than ``agree'', the experimenter would take a note and ask the participant about the specific issues they had encountered and solicit their suggestions for addressing these issues in the interview. We will report on these findings in Section \ref{sec:interview} below.

\subsubsection{User experiences, challenges, and feedback}
\label{sec:interview}

In the interview, we discussed with the participants about their post-study questionnaire responses, the difficulties they encountered when using \textsc{Peanut}, the different user experiences of annotating data in two conditions, and their suggestions for the design of \textsc{Peanut}. The leading author first coded all the interview transcripts independently and discussed the codebook with another author to reach a consensus. The unified codebook was then used by the other author to code all the interview transcripts independently again to validate the result. The overall inter-rater reliability is 0.72.

Most of the reported difficulties originated from the default positions and sizes of the canvas and the annotation box. Due to an implementation bug, the video frames would be shown in a smaller window in the upper left corner and users ``\textit{had to drag and resize them every time}'' (P13). The panel for predicted bounding boxes may also appear outside the frame sometimes, so the user has to drag it back to the current view. Both of these implementation issues can be easily fixed in new versions of \textsc{Peanut}. Besides, after carefully watching the video multiple times, participants still sometimes had trouble recognizing the sound type or locating the source due to ``\textit{the background noise, ambiguity of the sound, and existence of multiple objects of a similar type}'' (P15). Lastly, a few participants found the difference between the \textit{Next} button and \textit{Next Label} confusing, especially in videos that had rapid visual or auditory changes, for which \textsc{Peanut} tends to conservatively recommend the immediate next frame when participants clicked the \textit{Next Label} button, which could confuse the user (P13).

When asked to describe the pros and cons of the AI-assisted \textsc{Peanut} system compared to the baseline, the 20 participants mentioned that \textsc{Peanut} significantly accelerated their annotation process and reduced their workload. Participants thought ``\textit{\textsc{Peanut} saved their time from doing repetitive and tedious manual labeling, especially when frames varied little once a time}'' (P6). With the assistance of \textsc{Peanut}, they only need to ``\textit{focus on a handful of key frames}'' (P10) or ``\textit{keep clicking the Next button to oversee the automatic annotation frame-by-frame}'' (P12). In addition, some participants thought that the systems recommended more accurate bounding boxes than what they can manually create, thus they did not need to ``\textit{struggle with creating precise boxes manually}'' (P13). Compared to the baseline tool, \textsc{Peanut} ``\textit{makes the annotation process much less exhausting}'' (P13).  On the weakness of \textsc{Peanut}, participants thought that with the \textit{Next Label} button they may easily overtrust the system from the beginning, and ``\textit{merely realized the wrong or missing annotation when reviewing back at the end}'' (P4).\looseness=-1

\crhighlight{With regard to human agency, participants thought the system ``\textit{allow them to jump in and take control back at any time when they feel the AI model start going wrong}'' (P16). P9 suggested that ``\textit{the system should provide more information or clear warning to them so that they can better notice the issue and decide when to get in}''. Moreover, as to their reliance on AI models, some participants said they ``\textit{tend to trust AI more when the sounding object was obvious in the video and AI correctly labelled them for consecutive frames}'' (P10). In contrast, they ``\textit{would choose to do it on their own when sounding object changes a lot}'' (P5). }

For the design of \textsc{Peanut}. P4 suggested enabling keyboard shortcuts for frequently used operations such as drawing, deletion, and frame switch. Furthermore, some participants thought "\textit{the algorithm behind the \textit{Next Label} button should be more robust}" (P15) especially ``\textit{when the scene contains complex objects or has a large variation between the frames}'' (P12).\looseness=-1


% \begin{itemize}
%     \item Difficulties:
%         \begin{itemize}
%             \item P4: Did not support high-resolution screen
%             \item P4: Should give default label based on the prior annotation
%             \item P4: Add key shortcut to perform operation, like "NEXT" button
%             \item P5: Sometimes the frame appeared, I have to drag them to the center of UI so that I can use the selection box
%             \item P7: Difficulty in selecting the object, I need to zoom the whole window. The difference between Next and NextLabel is quite different, in some situations I should know which to click
%             \item P15: Not really
%             \item P16: When I tried to annotate, I used the background knowledge rather than depending on the audio clip. Besides, it is hard to find the location of the box, I had to move the selection box every time. 
%             \item P17: When I select the picture, I found the picture didn't appear at the center of the window
%             \item P18: When I'm going to choose the label, sometimes it didn't zoom appropriately, I have to drag and adjust the frame size. Have no idea about the difference between next button and the NextLabel button, when I keep clicking the next, for the most of time, the label of the next frame is already annotated. 
%             \item P19: The frame is always shwon on the top left corner, I have to always drag the picture to the right position.
%         \end{itemize}
%     \item How \textsc{Peanut} helps annotation:
%         \begin{itemize}
%             \item P4: You can skip several duplicate frames, so that I can label more in shorter time. But for some videos, the algorithm didn't work very well, it still gave duplicate frames to label, though the sound and frame are very similar.
%             \item P5: For the action repeated across the whole video, the recommendation and auto-labelling saved me lots of time.
%             \item P6: Absolutely. Every time I turned to a new frame, there is a box already chosen, so that I don't need to draw box. Besides, after I draw a box in the first frame, the system can automatically label the following frame, so that I only to keep clicking the Next frame until there are something different between the frames.
%             \item P15: What I only need to do is to annotate those that were not annotated by system.
%             \item P16: Definitely. It is very useful to accelerate annotation, in the first version, I have to annotate all the frames manually, while in the second version, I just needed to 
%             \item P17: I don't need to select box for each time
%             \item P18: Especially for the video for which the frames are similar with each other. I just need to keep clicking "Next" button.
%             \item P19: It helps a lot. They already marked the box so I can just choose the classification.
%         \end{itemize}
%     \item Favorite feature
%         \begin{itemize}
%             \item P4: The recommendation of bounding box is very useful， I just need to select the bounding box (instead of drawing from scratch)
%             \item P4: Another feature is that you can draw a box and the system will automatically label the next frame
%             \item P5: The recommendation is good, I felt smooth when I used the tools. Also the button to go to next button is helpful so that I can keep clicking the button fast to skim over the similar frames
%             \item P15: The next button, help me to skip lots of images, I can keep clicking the Next button
%             \item P16: In the second version, I just need to annotate in the first frame, then the model can detect the object automatically, and make the annotation much less exhausting
%             \item P17: The automatically select the sounding object, it is very useful
%             \item P18: The Next button is my favorite feature
%             \item P19: It is easy to learn and use. 
%         \end{itemize}
%     \item Dislike
%         \begin{itemize}
%             \item P15: Maybe the UI. 
%             \item P16: The UI can be improved, each time the selection box appears on the top of the bounding box, if the object appears on the top of the window, the box will be hidden.
%             \item P18: sometimes the size of the frame was not compatible to the windows
%             \item P19: I have to hear the sound for every picture, it is kind of noisy.
%         \end{itemize}
%     \item Suggestion
%         \begin{itemize}
%             \item P4: I think the algorithm judging whether the objects in next frame is duplicate with the current one need to be improved, right now it still sometimes give user the duplicate frames to label
%             \item P5: Video clip pool can be more diverse. Besides, if you can provide us the class label, sometimes I didn't you which label it is.
%             \item P15: Make the picture smaller, otherwise we have to move the picture every time
%             \item P17: The AI tends to select some objects that didn't make any sound
%             \item P18: You may need to make the manipulation of classification more explicit
%         \end{itemize}
%     \item Trust
%         \begin{itemize}
%             \item P4: At certain level, I am. But from the thumbnail I also noticed system could make error or miss labeling sometimes, I have to go back and relabel them.
%             \item P5: Most time.
%             \item P15: Yes. Because I feel those bounding box and annotation are pretty well and I would have done a same anntoation by myself.
%             \item P16: The detected results is really precise, I think it is reliable. The review function help me ajust trust as well.
%             \item P17: Yes
%             \item P18: Not quite, for some videos it worked very well, but for videos containing complex objects, it doesn't work very appropriately
%             \item P19: I can trust it partially. Based on my epxerience, it can recognize part of hte objects, but it do need human to double check. Like soem of them accurate, but some of them are not accurate. The review video and thumbnail helped me to recognize the incorrect annotation.
%         \end{itemize}
% \end{itemize}
