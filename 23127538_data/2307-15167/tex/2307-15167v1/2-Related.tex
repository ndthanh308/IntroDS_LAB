\section{Background and Related Work}

\subsection{Audio-Visual Learning}

Audio-visual learning aims to build a multi-sensory perception system that learns from perceived auditory and visual scenes. Mimicking human perception capacity, it can enable a variety of novel applications in many fields, such as multimedia~\cite{zeng2018audio,liu2019use,hamroun2021multimodal}, affective computing~\cite{noroozi2017audio,ma2019audio}, accessibility~\cite{pavel2020rescribe,wang2021toward}, and AR/VR~\cite{pavel2020rescribe,wang2021toward}. Utilizing and learning from both auditory and visual modalities has attracted significant attention in the AI community. 

We have seen great progress in the development of new audiovisual learning problems and applications, such as representation learning~\cite{aytar2016soundnet,arandjelovic2017look,owens2018audio},
audio-visual sound separation~\cite{ephrat2018looking,gao2018learning,owens2018audio,zhao2018sound}, sounding object localization~\cite{owens2018audio,senocak2018learning,tian2018audio,hu2020discriminative}, audio-visual event localization~\cite{tian2018audio,lin2019dual,wu2019DAM}, audio-visual captioning~\cite{tian2018attempt,rahman2019watch}, and multimodal embodied AI~\cite{chen2020learning,gao2021objectfolder}. 

% Among these tasks, one core problem in audio-visual learning is sounding object visual localization, which aims to identify visible sound sources in our daily life. Pioneering works utilize mutual information~\cite{hershey2000audio} and canonical correlation analysis (CCA)~\cite{kidron2005pixels} to localize sounding visual regions. Very recently, deep audio-visual networks are developed to spatially locate sound sources based on cross-modal embedding similarity~\cite{arandjelovic2018objects, owens2018audio, hu2019deep}, audio-guided visual attention~\cite{senocak2018learning, tian2018audio}, audio-visual class activation mapping~\cite{qian2020multiple}, class-aware object localization map, and sounding object visual grounding~\cite{Tian_2021_CVPR}. These approaches typically take advantage of natural synchronization between audio and visual content and are learned in self-supervised and weakly-supervised manners. Since ground-truth locations of sounding objects are usually not available during training, they tend to make inaccurate predictions. Consequently, certain ethical concerns would be raised. For example, their learned models without using precise labels might incorrectly localize women faces given men's speech.  To solve the problems, training videos with high-quality human annotated sounding object annotations are desired.


\subsubsection{Sounding Object Localization}
\label{sec:sounding_object_localization}
Among audio-visual learning tasks, one important task is the localization of sounding objects. For example, in a symphony concert scenario, the model should identify which instrument is making a particular sound and where the instrument is located in the video. Early work in this area utilized mutual information~\cite{hershey2000audio} and canonical correlation analysis (CCA)~\cite{kidron2005pixels} to localize the sounding visual regions. Recently, deep audio-visual networks have been developed to spatially locate sound sources based on cross-modal embedding similarity~\cite{arandjelovic2018objects, owens2018audio, hu2019deep}, audio-guided visual attention~\cite{senocak2018learning, tian2018audio}, audio-visual class activation mapping~\cite{qian2020multiple}, class-aware object localization map~\cite{hu2020discriminative}, and sounding object visual grounding~\cite{Tian_2021_CVPR}. 

These approaches take advantage of the natural synchronization between audio and visual contents and are trained using self-supervised or weakly-supervised methods (i.e., using no or limited annotated training data). Due to the lack of ground truth annotation for training, they tend to make inaccurate predictions. A relevant audio-visual ML task is active speaker detection which focuses on detecting the active speaker from speech, due to the narrower task domain (human speech only) and the availability of datasets such as~\cite{kim21k_interspeech}, the state-of-the-art active speaker detection models generally perform better than the domain-general sounding object localization ones~\cite{alcazar2021maas,kim21k_interspeech,hu2020discriminative,hu2020discriminative}. Our work seeks to address this problem by making it easier to annotate large audio-visual datasets.


% It is also hard for the model to train when audio and vision are out of sync or there are objects of the same category making sounds together (e.g. several violins playing at the same time). 


\subsection{Data Annotation for Machine Learning}
High-quality data is the foundation of most ML models. The lack of high-quality data has been a long-time bottleneck for many ML tasks~\cite{brew2010interaction,halevy2009unreasonable}. While the undervaluing of data work compared to the lionized work of building novel models and algorithms is common in AI development~\cite{sambasivan_2021_everyone}, ``data excellence'' played a crucial role in the quality of AI systems~\cite{sambasivan_2021_everyone}.\looseness=-1

Despite that many unsupervised, self-supervised, and weakly-supervised models have been found useful in many task domains~\cite{doersch2015unsupervised,zhou2018brief}, supervised learning (i.e., models learning from annotated example input-output pairs) still shows important advantages in model performances and robustness. However, collecting annotated ground truth data is usually a costly and time-consuming process that requires extensive human labor. 

There are two types of data annotation---(1) Explicit data annotation, where human annotators use their perceptive and cognitive abilities to categorize and label data for the purpose of creating annotated datasets~\cite{askhtorab_aiassisted_2021}. This is often a repetitive and tedious process especially because datasets need to be large in order to be effective; (2) Implicit data annotation, where users of a computing system generate useful datasets as a side product of interacting with the system (e.g., users of a recommender system generate useful data when interacting with recommended items). While this approach does not incur additional user efforts, it requires the system to be deployed at a large scale in order to collect sufficient data, which requires significant effort and is not always feasible in all task domains and at all stages of the project. Implicit annotation also faces the ``cold start'' problem~\cite{schein2002methods}---it still needs a dataset for training the initial model to provide acceptable performance at the beginning before implicitly-annotated data from user interactions come in.

\textsc{Peanut} is designed to reduce the human effort required in \textit{explicit} data annotation, making the process more efficient while maintaining data accuracy. Meanwhile, \textsc{Peanut} also uses \textit{implicit} data annotation strategies to improve the performance of its own object detection model in real-time while the user is in the process of explicit interactive data annotation (detail in Section~\ref{sec:active_learning}). 


\subsubsection{Assistance for Explicit Data Annotation}
Explicit data annotation is traditionally a fully manual process---a human annotator examines the input data and determines the output result that the ML model should produce using their human knowledge and cognitive abilities~\cite{cassidy_2017_tools} (e.g., the commonly used VIA tool~\cite{dutta_2019_via} for manual audio-visual annotation). Recently, several interactive tools have been developed to assist human annotators with the process~\cite{zhang2022onelabeler,evensen-etal-2020-ruler,ratner2017snorkel,shnarch2022label,rietz2021cody}. Notably, \textsc{Ruler}~\cite{evensen-etal-2020-ruler} is an interactive system that synthesizes labeling rules while the human annotator manually assigns labels in textual data (known as the \textit{data programming} process~\cite{ratner2017snorkel}). Like \textsc{Peanut}, \textsc{Ruler} uses a partial-automation approach where an intelligent system helps the human annotator by automating some parts, but not the full end-to-end annotation task. The two systems also share the ``explicit+implicit'' approach as \textsc{Ruler} learns to synthesize new labeling rules while the user manually labels the data. Desmond et al. introduced an AI labeling assistant that uses a semi-supervised learning algorithm to predict the most probable labels for each example in the labeling intents of user natural language inquiries~\cite{desmond_increasing_2021}. \tlhighlight{\textsc{Peanut}'s interfaces for users to verify labels predicted by a model and correct model-generated bounding boxes are similar to prior work in improving object detection models in computer vision~\cite{kaul2022label,liu2020faster}.}

\zzhighlight{Some ML-enabled annotation assistance tools use off-the-shelf models to pre-label data as suggestions for users. For example, CVAT\footnote{\url{https://github.com/opencv/cvat}} uses a deep learning model to pre-label the images. Similarly, Ilastik~\cite{berg2019ilastik} provides pre-labels to support semi-automatic image segmentation using edge detection and watershed models. Although pre-labeling is effective for accelerating the annotation, it risks annotating data with model biases, especially when the data is in a new domain previously unseen in the model's training process~\cite{Xiao2018AddressingTB}. The approach used in \textsc{Peanut} has significant differences from these pre-labeling approaches. Instead of performing pre-labeling independent of human annotation, \textsc{Peanut} grounds its annotation suggestions on human-labeled key frames in real-time to balance model performance and human effort for achieving high-quality annotations and ensuring user control of the annotation process at the same time. The key frames are determined in real-time according to video contents and intermediate annotation results (Section~\ref{sec:algo_key_frame}). This approach also allows \textsc{Peanut} to learn new topics from the user's few shot annotations.}\looseness=-1

\tlhighlight{The problem domain in \textsc{Peanut} is also more complex than the domain in existing AI-enabled annotation support tools for single-modal data (e.g., images~\cite{berg2019ilastik,10.1145/3411764.3445165}, text~\cite{rietz2021cody,labelsleuth2022,desmond_increasing_2021, gao2023collabcoder}). \textsc{Peanut} works in a task domain with data in multiple modalities (audio and visual). The task explicitly addresses the explicit and implicit relations between data in different frames.}\looseness=-1

Another type of assistance for explicit data annotation uses the strategy of \textit{batching} (e.g.,~\cite{askhtorab_aiassisted_2021,vajda_2015_semiautomatic,demirkus2014robust}). The system first puts ``similar'' data into batches using unsupervised clustering models or pre-trained models (e.g., semantic similarity for NLP tasks) and then asks the human annotator to annotate data by batch. The underlying assumption is that it would be easier and faster to annotate similar data together than to annotate them individually because they are likely to be assigned with the same or similar labels. The batching strategy has been shown to be effective in accelerating the data annotation process~\cite{askhtorab_aiassisted_2021}. A potential concern with batching is users' overreliance on AI---the human annotator might assign the same label to a batch without carefully examining each data point because ``the AI model thought that they were all similar''~\cite{askhtorab_aiassisted_2021}. \textsc{Peanut} also  uses batching---but it was not achieved using an unsupervised clustering model. Instead, \textsc{Peanut} leverages the characteristics of videos so that adjacent frames are often similar to each other. The auditory and visual models used in \textsc{Peanut} detect changes in the scene or sudden movements, which are used to batch frames so that the human annotator only annotates key frames. A guided workflow for human annotation (Section~\ref{sec:automatic_annotation}), video playback, and thumbnail preview features (Section~\ref{sec:annotation_review}) in \textsc{Peanut} alleviate the overreliance issue. \looseness=-1 



% \tlcomment{our system effectively uses batching -- based on the characteristics that frames near each other on the timeline are usually similar}


% Data annotation is an important step in training a good ML model. Normally, it requires humans to label answers based on the raw data for the models in training set, so that the model can learn from the answer and predict reliable results in the testing set. Although unsupervised and semi-supervised training paradigms that requires less annotated data but more raw data are adopted in some specific tasks recently\cite{xx}, the performance is hard to compete with models that are trained with well-annotated data. However, obtaining high-quality data usually needs tedious human labor which would sometimes be extremely expensive; besides, there is inevitable bias exists among annotators which will cause significantly side-effect to the dataset's quality.

% Basically, human involves in the data annotation process in two ways: 1) explicit data annotation, where the main purpose of the human computer interaction is to provide valuable "answers";\cite{xx} 2) Implicit data annotation, where the human-annotated data will come as a useful side-product of the interaction in a mix-initiated way.Our system integrates both features that can either provides intelligent assistance for explicit annotation and meanwhile leverages implicit data annotation to improve the system performance.

% \tlcomment{Define data annotation, why is it important}

% \tlcomment{ML models need data to train}
% \tlcomment{Data need to be provided by human}
% \tlcomment{Two ways for human to provide data -- explicit data annotation where the main purpose of the interaction is to provide data vs. implicit data annotation where human-annotated data useful for the mode comes as a side-product of an interaction}
% \tlcomment{Our system is an example of a system that provides intelligent assistance for explicit data annotation, but the system also leverages implicit data annotation (i.e., the active learning part in object detector) to improve its performance}



\subsubsection{Implicit Data Annotation}
Systems that use implicit data annotation strategy include (1) those that collect data from their interactions with users for the purpose of a \textit{different} data task, such as reCAPTCHA~\cite{von2008recaptcha} that collects user-annotated data for training computer vision models through its interactive process of distinguishing human users from bots for authentication purposes and the Foldit game~\cite{cooper2010predicting} that collects user-annotated protein structures through an online game; (2) those that collect data from their interactions with users for the purpose of improving the \textit{same} interaction, such as recommender systems that learn about user personal preferences as the user interacts with the recommended items~\cite{rashid_getting_2002} and intelligent agents that learn about tasks while helping users with task automation~\cite{li_sugilite:_2017, li_pumice:_2019}.

\textsc{Peanut}'s use of implicit data annotation falls into the latter category. As discussed in Section~\ref{sec:active_learning}, the human annotation result for each keyframe is used to fine-tune the visual-sound grounding model, which reduces human effort in annotating the rest of the frames. This strategy is also an example of \textit{active learning}~\cite{settles_activelearning_2012,cohn1996active}, where the system chooses which data the visual-sound grounding model should learn from by querying the user through \textsc{Peanut}'s selection of keyframes (Section~\ref{sec:algo_key_frame}).


\subsection{Human-AI Collaboration in Data Science}
\textsc{Peanut} belongs to a fast-growing list of AI-powered interactive tools that assist and augment human capabilities in different subtasks in the data science workflow~\cite{dakuo_autods_2021,wang_human-ai_2019,heer2019agency, ning2023empirical}. Besides data annotation, human-AI collaborative tools have also been developed for data wrangling (i.e., cleaning and formatting data to make it suitable for analysis~\cite{kandel_enterprise_2012}) (e.g.,~\cite{guo_proactive_2011}), exploratory data analysis and sensemaking (e.g.,~\cite{wongsuphasawat_voyager_2017}), selection of ML models (e.g.,~\cite{he2021automl}), generating new data features (e.g.,~\cite{galhotra_automated_2019}), testing and debugging ML models (e.g.,~\cite{wu2021polyjuice}), and fine-tuning parameters in ML models (e.g.,~\cite{liu2020admm}).\looseness=-1

The design of \textsc{Peanut} is informed by empirical studies on how data science workers work with data~\cite{muller_how_2019, muller_2021_ground-truth} and data workers' perceptions and mental models of human-AI collaborative data science tools~\cite{wang_human-ai_2019}. For example, studies~\cite{muller_how_2019,muller_2021_ground-truth} reported that the difficulty with finding reliable labels for ground truth is a common problem that data science practitioners encounter. In industry settings, external domain experts often need to be hired~\cite{muller_2021_ground-truth}. Spreadsheet is commonly used as a tool for labeling---while specialized tools such as CrowdFlower\footnote{https://appen.com/} have also been used, they are used to facilitate group collaboration on data annotation ~\cite{muller_2021_ground-truth} with no intelligent automation feature that reduces the workload of the annotation.\looseness=-1

% \hl{Discuss the implications}.

More broadly, facilitating effective collaboration between human and intelligent systems has been a long-standing topic since the origin of HCI research in the seminal paper on man-computer symbiosis~\cite{licklider1960man} where computers can ``do the routinizable work that must be done to prepare the way for insights'' meanwhile human users can leverage their domain expertise to make decisions that computers cannot. The design of \textsc{Peanut} follows this pattern where the system marks potential object candidates and identifies keyframes for users to annotate using single-modal partial-automation models while the user ''connects the final dots'' with their annotations that finish the end-to-end multi-modal process using human perceptive and cognitive capabilities that ML models do not yet possess. 

Later work such as the principles in mixed-initiative interactions~\cite{horivitz1999principles} identified strategies such as considering uncertainties in user intents, assessing the added-value of automation, providing mechanisms for refining automation results, and maintaining user working memory of interactions. More recently, guidelines in human-AI interaction~\cite{amershi_2019_guidelines} have been proposed to address challenges that came with the popularity of ``black-box-like'' data-driven AI models in interaction systems (as opposed to the ``traditional'' planning-based techniques). These guidelines, principles, and theoretical frameworks have been widely used in the design of human-AI collaborative systems in domains like healthcare~\cite{cai_human-centered_2019}, creativity support~\cite{louie_noivce_2020}, and error repairs in chatbots~\cite{li_sovite:_2020}. Two key human-AI collaboration challenges we specifically address in the design of \textsc{Peanut} are to accommodate the \textit{imperfection} of AI models and to enable the continuous learning of partial-automation models, which we discuss in Section~\ref{sec:design_challenges}.