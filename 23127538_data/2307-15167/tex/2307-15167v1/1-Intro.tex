\section{Introduction}

Most of our real-world perceptual experiences are specified by multiple cooperating human senses with multi-sensory integration~\cite{murray2011neural}. For example, we can perceive spoken language words and sentiments from lip movements, facial expressions, and speech sounds of the other speakers.  
To mimic human perception capability, researchers in Artificial Intelligence (AI) community have begun to explore audio-visual machine learning (ML) approaches~\cite{aytar2016soundnet,arandjelovic2017look,owens2018audio,hu2019deep,hu2020discriminative}. As an emerging research field, audio-visual learning has attracted a lot of attention from both the academic community and the industry because of its potential to solve many challenging problems in real-world applications such as video retrieval~\cite{zeng2018audio,liu2019use}, AR/VR~\cite{morgado2018self,richard2021audio}, and accessibility~\cite{pavel2020rescribe,wang2021toward}. 

\tlhighlight{A fundamental task in audio-visual learning is \textit{sounding object localization}, which identifies and localizes sounds to visual objects in videos. This task associates audio data with the corresponding visual data, which represents the critical step for many downstream audio-visual tasks such as audio-visual scene-aware dialogs~\cite{alamri2019audio}, video sound separation~\cite{gao2019co,Tian_2021_CVPR}, audio spatialization~\cite{Gao_2019_CVPR,morgado2018self}, audio-visual captioning~\cite{tian2018attempt,rahman2019watch}, and multimodal embodied AI~\cite{chen2020learning,gao2021objectfolder}.} 

% \tlhighlight{However, the progress in audio-visual learning tasks such as sounding object localization is impeded by the lack of high-quality datasets. Since there are various audible and inaudible visual objects in video frames and even a single sound source can make different sounds with different intensities, the sounding object localization task is hungry for large-scale and high-quality training data to capture the diverse audio-visual correspondences and mitigate data variations.}\looseness=-1

\tlhighlight{However, the performance and wide adoption of audio-visual learning have been impeded by the availability of high-quality datasets. For example, commonly used ``gold-standard'' datasets such as AVE~\cite{tian2018audio} is only weakly supervised for this task (i.e., events are annotated for video segments, but object-level ground-truth labels are not available for frames). Therefore, previous works~\cite{aytar2016soundnet,arandjelovic2017look,owens2018audio,hu2019deep,hu2020discriminative} focus on weakly-supervised or self-supervised methods. These methods not only result in lower model accuracy~\cite{senocak2018learning,tian2018audio}, but also introduce biases in the models. In these methods, existing common label noises will be propagated into learned models, leading to compromised results with ethical issues~\cite{tian2020unified,wyatte2019biasing} (e.g., \crhighlight{introducing biases and stereotypes presented in datasets into annotation results}). Since there are various audible and inaudible visual objects in video frames and even a single sound source can make different sounds with different intensities, the sounding object localization task is hungry for large-scale and high-quality training data to capture the diverse audio-visual correspondences and mitigate data variations.}\looseness=-1

% Due to the lack of high-quality annotated data, previous works~\cite{aytar2016soundnet,arandjelovic2017look,owens2018audio,hu2019deep,hu2020discriminative} focus on weakly-supervised or self-supervised methods. These methods not only result in lower model accuracy~\cite{senocak2018learning,tian2018audio}, but also introduce biases in the models. In these methods, existing common label noises will be propagated into learned models, leading to compromised results with ethical issues~\cite{tian2020unified,wyatte2019biasing} (\emph{e.g.,} localizing women's faces as sound sources of men's speech). To mitigate the problem, high-quality audio-visual annotated datasets are desired.




% A particularly useful audio-visual machine learning task that can benefit from high-quality datasets is \textit{sounding object localization}. It aims to identify and localize visible sounding objects in videos. The task is a fundamental problem in audio-visual learning and can help solve a wide range of downstream tasks,  Since there are various audible and inaudible visual objects in video frames and even a single sound source can make different sounds with different intensities, the sounding object localization task is hungry for large-scale and high-quality training data to capture the diverse audio-visual correspondences and mitigate data variations.
% \hl{summarize what it is, useful applications, and why it can benefit from more data}. 

The lack of high-quality datasets is a direct consequence of the large effort required to create such datasets. Annotating audio-visual data is laborious, expensive, and time-consuming. \tlhighlight{With the current annotation tools of for audio-visual data (e.g., VIA~\cite{dutta_2019_via}), annotators need to watch each frame of the video, listen to the corresponding sound, identify the sounding object, draw a bounding box, and indicate the type of sound.} Considering that even a short video would require the annotation of hundreds, if not thousands, of frames, this process is highly repetitive and tedious. \looseness=-1

% The bottleneck imposed by the lack of annotated data is not unique to audiovisual learning, but is also common in many machine learning (ML) tasks in different application domains~\cite{brew2010interaction,halevy2009unreasonable,askhtorab_aiassisted_2021}. The accuracy, quantity, and domain-specificity of data play a crucial role in ensuring the quality and performance of AI systems~\cite{sambasivan_2021_everyone}. 

\tlhighlight{Some prior intelligent tools (e.g.,~\cite{evensen-etal-2020-ruler, askhtorab_aiassisted_2021, demirkus2014robust, vajda_2015_semiautomatic, desmond_increasing_2021, 10.1145/3411764.3445165} have been introduced to provide AI-enabled assistance to users in the data annotation process with promising outcomes to improve annotation efficiency using strategies such as  batching~\cite{askhtorab_aiassisted_2021}, rule synthesis~\cite{evensen-etal-2020-ruler}, and active learning~\cite{labelsleuth2022}. However, these existing tools are limited to the annotation of data in \textit{one} modality (e.g., text categorization~\cite{askhtorab_aiassisted_2021, desmond_increasing_2021}, head post recognition~\cite{demirkus2014robust}, handwriting recognition~\cite{vajda_2015_semiautomatic}, image labeling~\cite{10.1145/3411764.3445165, berg2019ilastik}, and video segmentation~\cite{Qiao2022HumanintheLoopVS}) while audio-visual data annotation requires the user and its AI assistance to process data from \textit{two} modalities and explicitly connect them together.}

In this paper, we present \textsc{Peanut}\footnote{The name \textsc{Peanut} is an acronym for \textbf{P}latform for \textbf{E}fficient \textbf{A}nnotation with \textbf{N}o \textbf{U}nnecessary \textbf{T}edium.}, a new data annotation tool for improving the efficiency of audio-visual data annotation. \zzhighlight{To tackle the unique challenge in \textit{multi-modal} data annotation, \textsc{Peanut} encapsulates a novel human-AI collaborative active learning pipeline where the user validates, revises, and connects the output from multiple single-modal models through a mixed-initiative interface~\cite{horivitz1999principles}.} 

\zzhighlight{Instead of using a fixed ML model to pre-label data, \textsc{Peanut} uses an active learning architecture ~\cite{cohn1996active, settles_activelearning_2012} that} allows these partial-automation models to incrementally learn from the user's annotations in real-time to improve model performance, {learn about visual and auditory data in new video topics, and adapt to the specific domain of the current video}. Several design features are presented to ensure the user's sense of agency and control~\cite{dakuo_autods_2021, wang_designing_2021} and alleviate users' overreliance on AI~\cite{askhtorab_aiassisted_2021}, both are notable issues found in human-AI collaboration of data works from previous studies. A within-subjects study with 20 participants showed that \textsc{Peanut} can significantly accelerate the annotation of audio-visual data (annotate almost 3 times the number of frames compared to the baseline condition) while also achieving high data accuracy.




% \zzcomment{Write the audio grounding process: sound identification, localization and tagging}

In summary, this paper presents the following three main contributions:
\begin{itemize}
    \item \tlhighlight{A set of interaction mechanisms for incorporating outcomes of \textit{single-modal} ML models into a new human-AI collaborative annotation workflow of \textit{multi-modal} audio-visual data while improving model performance with user annotations in real time using an active learning approach.}
    
    \item \textsc{Peanut}, a human-AI collaborative annotation tool that implements these strategies to \tlhighlight{reduce user efforts and improve efficiency in annotating audio-visual data for sounding object localization.}
    
    \item A within-subjects user study with 20 participants \tlhighlight{with diverse annotation and ML expertises} showing that \textsc{Peanut} can improve efficiency in the annotation of audio-visual data while also achieving high data accuracy.
\end{itemize}