{
  "title": "LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference",
  "authors": [
    "Cong Wang",
    "Yu-Ping Wang",
    "Dinesh Manocha"
  ],
  "submission_date": "2023-07-23T03:38:55+00:00",
  "revised_dates": [
    "2023-08-10T00:15:15+00:00"
  ],
  "abstract": "We propose a novel method, LoLep, which regresses Locally-Learned planes from a single RGB image to represent scenes accurately, thus generating better novel views. Without the depth information, regressing appropriate plane locations is a challenging problem. To solve this issue, we pre-partition the disparity space into bins and design a disparity sampler to regress local offsets for multiple planes in each bin. However, only using such a sampler makes the network not convergent; we further propose two optimizing strategies that combine with different disparity distributions of datasets and propose an occlusion-aware reprojection loss as a simple yet effective geometric supervision technique. We also introduce a self-attention mechanism to improve occlusion inference and present a Block-Sampling Self-Attention (BS-SA) module to address the problem of applying self-attention to large feature maps. We demonstrate the effectiveness of our approach and generate state-of-the-art results on different datasets. Compared to MINE, our approach has an LPIPS reduction of 4.8%-9.0% and an RV reduction of 73.9%-83.5%. We also evaluate the performance on real-world images and demonstrate the benefits.",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12217",
  "pdf_url": null,
  "comment": "Accepted by ICCV 2023",
  "num_versions": null,
  "size_before_bytes": 9185694,
  "size_after_bytes": 210814
}