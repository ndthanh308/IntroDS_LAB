
% Template for Elsevier CRC journal article
% version 1.2 dated 09 May 2011

% This file (c) 2009-2011 Elsevier Ltd.  Modifications may be freely made,
% provided the edited file is saved under a different name

% This file contains modifications for Procedia Computer Science
% but may easily be adapted to other journals

% Changes since version 1.1
% - added "procedia" option compliant with ecrc.sty version 1.2a
%   (makes the layout approximately the same as the Word CRC template)
% - added example for generating copyright line in abstract

%-----------------------------------------------------------------------------------

%% This template uses the elsarticle.cls document class and the extension package ecrc.sty
%% For full documentation on usage of elsarticle.cls, consult the documentation "elsdoc.pdf"
%% Further resources available at http://www.elsevier.com/latex

%-----------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                          %%
%% Important note on usage                                  %%
%% -----------------------                                  %%
%% This file should normally be compiled with PDFLaTeX      %%
%% Using standard LaTeX should work but may produce clashes %%
%%                                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% The '3p' and 'times' class options of elsarticle are used for Elsevier CRC
%% Add the 'procedia' option to approximate to the Word template
\documentclass{article}


\usepackage{arxiv}


\usepackage{svg}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
\usepackage{url}
\usepackage{microtype}
\usepackage{subcaption}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
%\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{graphicx}
%\usepackage{appendix}
\usepackage{chngcntr}
\newcommand\Tstrut{\rule{0pt}{2.0ex}}       % "top" strut
\newcommand\Bstrut{\rule[-0.9ex]{0pt}{0pt}} % "bottom" strut
\newcommand{\TBstrut}{\Tstrut\Bstrut} % top&bottom struts

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}

%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}

% if you have landscape tables
\usepackage[figuresright]{rotating}

% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
%   \newtheorem{def}{Definition}[section]
%   ...

% add words to TeX's hyphenation exception list
%\hyphenation{author another created financial paper re-commend-ed Post-Script}

% declarations for front matter

\begin{document}


%% Title, authors and addresses


\title{Event-based Vision for Early Prediction of Manipulation Actions}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\author{
    Daniel~Deniz \\
    Computer Architecture and Technology \\
    CITIC, University of Granada\\
    Granada, Spain \\
    \texttt{danideniz@ugr.es} \\
\And
    Cornelia~Ferm\"uller \\
    Perception and Robotics Group \\
   University of Maryland \\ 
   Maryland, USA\\
    \texttt{fermulcm@umd.edu} \\
\And
    Eduardo~Ros \\
    Computer Architecture and Technology \\
    CITIC, University of Granada\\
    Granada, Spain \\
    \texttt{eros@ugr.es} \\
\And
    Manuel~Rodriguez-Alvarez \\
    Computer Architecture and Technology \\
    CITIC, University of Granada\\
    Granada, Spain \\
    \texttt{manolo@ugr.es} \\   
\And
    Francisco~Barranco \\
    Computer Architecture and Technology \\
    CITIC, University of Granada\\
    Granada, Spain \\
    \texttt{fbarranco@ugr.es} \\
}

\maketitle

\begin{abstract}
Neuromorphic visual sensors are artificial retinas that output sequences of asynchronous events when brightness changes occur in the scene. These sensors offer 
many advantages including very high temporal resolution, no motion blur and smart data compression ideal for real-time processing.
In this study, we introduce an event-based dataset on fine-grained manipulation actions and perform an experimental study on the use of transformers for action prediction with events. There is enormous interest in the fields of cognitive robotics and human-robot interaction on understanding and predicting human actions as early as possible. Early prediction allows anticipating complex stages for planning, enabling effective and real-time interaction. Our Transformer network uses events to predict manipulation actions as they occur, using online inference. The model succeeds at predicting actions early on, building up confidence over time and achieving state-of-the-art classification. Moreover, the attention-based transformer architecture allows us to study the role of the spatio-temporal patterns selected by the model. Our experiments show that the Transformer network captures action dynamic features outperforming video-based approaches and succeeding with scenarios where the differences between actions lie in very subtle cues. Finally, we release the new event dataset, which is the first in the literature for manipulation action recognition. Code will be available at \url{https://github.com/DaniDeniz/EventVisionTransformer}.
\end{abstract}

\keywords{event-based vision, online prediction, manipulation action prediction}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text

\section{Introduction}
\label{sec:introduction}

% Figure environment removed

Human Activity Recognition (HAR) aims at interpreting human activities by analyzing data from videos or wearables that embed inertial sensors. Advances in HAR have greatly contributed to the development of solutions for lifestyle monitoring \cite{deniz2020reconfigurable} and surveillance security \cite{roy2018suspicious, isern2020reconfigurable}. HAR is even more relevant for applications that involve robot-human interaction \cite{anagnostis2021human,yang2014cognitive,yang2015robot} -- a real-time task that requires anticipating %as much as possible 
the actions that humans are performing. 

%HAR is usually categorized into two main approaches: sensor-based and vision-based HAR \cite{dang2020sensor}. Sensor-based HAR retrieves information from environmental or wearable sensors such as smart wristbands or inertial sensors to identify activities \cite{bai2019motion2vector}. On the other hand, vision-based HAR recognize human activities processing information obtained from vision sensors such as images or videos represented by RGB or RGB-Depth data \cite{gupta2022human}. The non-intrusive nature of vision-based HAR has facilitated its adoption, as users are not required to wear cumbersome devices on different parts of their body \cite{beddiar2020vision}. 

Nowadays, state-of-the-art solutions for HAR use Deep Learning techniques. Most approaches use complex 2D and 3D Convolutional Neural Networks (CNNs) \cite{carreira2017quo, ddeniz2022efficient}, or combine 2D CNNs with recurrent neural networks (RNNs)~\cite{donahue2015long}. 
% For example, in \cite{tran2015learning} authors employ 3D CNNs for analyzing spatio-temporal features from video clips; these resource-intensive operations allow the model to simultaneously learn from the appearance and motion in the scene. In \cite{donahue2015long}, authors introduce a Long-term Recurrent Convolutional Network (LRCN) that combines a 2D convolutional architecture to extract spatial features from the images and a Long-short Term Memory (LSTM) layer to capture the dynamics of the movement in the scene. Contrary to 3D CNN approaches, it admits variable-length inputs. 
%Other solutions that offer state-of-the-art performance include the Two Stream network \cite{carreira2017quo, ddeniz2022efficient}, that identifies activities from appearance and motion cues using two 3D CNN streams.
More recently, \textit{Transformers} have become very popular due to their great performance
%originally for natural language processing
\cite{vaswani2017attention}
%and then, for
in many fields 
%such as 
including computer vision. For example, \cite{eusanio2020trans} proposes a combination of a ResNet network with a Transformer encoder module with multiple self-attention heads for hand gesture recognition, or \cite{mazzia2022action} proposes a Transformer that exploits human pose estimates for accurate and real-time activity recognition. Our work focuses on predicting manipulation actions, considering humans interacting with their hands with tools or objects to perform specific tasks.

%Importance of manipulation actions. Relevant contributions to systems based on their prediction (eg. robots, indoor action recognition) - Should be interesting to mention indoor action recognition here.
For humans, action prediction is a continuous process that plays a crucial role, especially in collaborative tasks. In fact, humans constantly update their predictions based on new information, which modifies their belief of the ongoing action. %as it happens. 
Consequently, we consider manipulation action prediction as more than only assigning a label to a video. We consider that action prediction is not limited to the action itself but also to the prediction of its dynamics and effects while interacting with the objects or the environment. Moreover, since vision processing is computationally intensive, the delay to understand the action is unacceptable if classification takes place after completing the action. Reducing the latency means an advantage for planning ahead and acting, 
%as fast as possible with the classification of the human intended action, 
enabling closed perception-action loops in real time \cite{waltemate2016impact}, one of the main issues robotics has always struggled with \cite{corke1996dynamic}. Some works 
%have focused on this topic 
refer to ongoing prediction as \textit{online inference} that  provides predictions using only the historic information  \cite{chen2022gatehub}, different from conventional action classification, which works on the complete action segments. %Obviously, such a process is of great interest e.g. for robots that interact with humans, that need to anticipate human intended actions as early as possible. 

% DVS sensors and why doing HAR with DVS sensors 
Neuromorphic visual sensors encode temporal luminance contrast, triggering asynchronous events at each pixel independently and with very-high temporal resolution. As a result, neuromorphic sensors smartly compress and reduce the  data to be processed only to changes on moving object contours and textures, and provide outputs with very low latency. 
%Reducing the input size directly impacts the computational complexity of the downstream application and, therefore, the latency due to the processing time. 
These properties make event sensors the best fit for applications of visual motion, such as optical flow \cite{barranco2014contour}, tracking \cite{barranco2018real, gallego2017event}, motion segmentation \cite{mitrokhin2018event,mitrokhin2020learning, parameshwara20210}, or gait recognition \cite{wang2019ev}. In our case, online prediction is crucial for the development of low-latency predictive solutions that allow for updating the belief of the ongoing action as new asynchronous events come in. %Event sensor with their very low-temporal latency and their intrinsic smart compression of the scene that reduce the overall computation are the best candidates for it. 

Using visual cues from the early stages of videos suffices for making a reasonably accurate prediction of the ongoing action \cite{ansuini2015intentions, fermuller2018prediction}. Studies suggest that observers catch subtle visual cues for recognition, such as the positioning of the fingers or the angles of adjacent fingers during the reaching phase \cite{ansuini2015predicting}. In this work, we argue that events or changes in the scene are sufficient to detect these cues.
%also visual cues from events or changes in the scene are enough for it and consequently, recognition might benefit from the advantages of event vision.
There are notable differences between frame-based and event-based recognition. 
%when using a fixed neuromorphic sensor. 
For example, since only motion is ``seen'', %event-based recognition does not profit from cues from the scene or the object that will be used in the action until it is touched and thus, its position changes. Therefore, 
until the hand touches the object, only hand dynamics can be observed. %In this work, to show the relevance of the event-based hand dynamics and to prove that it is enough for classification, an event-based tracker \cite{delbruck2008frame} is used to estimate the hand trajectory. In addition, we compare the recognition performance of the event-based solution with respect to a video-based approach. 
Furthermore, while video-based approaches need to reconstruct action dynamics from the sequence of frames, neuromorphic solutions naturally focus on dynamic features. These are crucial for recognition when the differences between actions lie in very subtle cues, as in manipulation actions where subjects perform different actions with the same objects or tools.  

Deep learning methods are data hungry; training requires a large amount of diverse data which is costly due to the time required for collection and curation. Event cameras are still novel sensors that became popular around 2008, and thus there is  a lack of event datasets for specialized tasks. %Some authors have proposed simulators to partially overcome this problem \cite{gehrig2020video, rebecq2018esim}. However, it is very hard to artificially generate realistic events from videos given the low temporal resolution of the input source \cite{delbruck2021feedback}. 
In this work, we release, to the best of our knowledge, the first event dataset 
%in the literature 
for manipulation action classification, called \textit{Event-based Manipulation Action Dataset}\footnote{https://github.com/DaniDeniz/DavisHandDataset-Events}. \textit{E-MAD} was recorded during the \textit{Telluride Neuromorphic Cognition Engineering Workshop}\footnote{https://sites.google.com/view/telluride-2023/} and contains 750 samples of event sequences performing 30 manipulation actions using 6 different objects. It has recordings of actions with very subtle differences in the motion, thus, analyzing the dynamics of the movement of the hand and objects is crucial to distinguish between activities. 

%We propose a novel architecture for online prediction of manipulation actions using event information: a Transformer \cite{vaswani2017attention}. Specifically, 
We propose a Transformer encoder architecture that captures temporal dependencies by processing embedding tokens extracted by a 2D CNN network relying on self-attention (inspired by \cite{vaswani2017attention}). This end-to-end causal Transformer is adapted for online prediction and event-wise computation. One of the most important results is that the Transformer is able to successfully capture motion dynamics from actions. 

%In our comparison with the state of the art, we include some other architectures categorized into two groups: 1) classification models that require the whole sequence to output a prediction; 2) online prediction models that output predictions as actions occur solely with the information observed so far. Next, we also present an ablation study to assess the generalization capabilities and evaluation performance of the proposed model. Finally, as Transformers are attention-based models, we discuss the spatio-temporal patterns extracted by the self-attention module. This analysis seeks to understand the role these patterns play in the continuous prediction specifically, regarding the difference between cyclic and discrete manipulation actions.

%Mention the lack of datasets, specially for event sensor, and introduce our dataset and advance that we are releasing it upon publication.

The main contributions of our paper are summarized next:
\begin{itemize}
    \item The novel Event-based Manipulation Action Dataset (\textit{E-MAD}) featuring subjects manipulating objects and performing different actions with them. The dataset is the event-based counterpart of the frame-based \textit{MAD} \cite{fermuller2018predictionurl}, and thus it can also be used in future comparisons between event- and frame-based approaches.
    
    \item An event-wise predictive \textit{Transformer} model for manipulation action recognition that updates its belief on the ongoing action with every new asynchronous event. The model is compared with the state of the art, including other predictive models and networks that require the full sequence of events  for classification.
    \begin{itemize}
        \item A study of the model generalization capacity. Also, a subject- and object-agnostic approach is introduced, which shows that cues only from the hand dynamics suffice for the recognition while considerably reducing the computations.
        \item A discussion on self-attention maps that illuminates which spatio-temporal patterns are key for the recognition. It is shown that there are significant differences between \textit{discrete} and \textit{cyclic} actions. 
    \end{itemize}
    %\item An ablation study that considers the capacity of generalization of the model. Also, a subject- and object-agnostic approach is introduced to prove that cues only from the hand dynamics suffice for the recognition while considerably reducing the computational complexity.
    %\item A discussion on self-attention maps to understand which spatio-temporal patterns are key for the recognition, with significant differences between \textit{discrete} and \textit{cyclic} actions. 
   %Also, note that \textit{E-MAD} was not artificially generated from frames but effectively collected performing the real manipulation actions.
\end{itemize}


\section{Related Methods}
\label{sec:related}
This section briefly reviews the principles of event-based vision and then the literature on related tasks of action recognition using event sensors.
%such as action classification in general and gesture recognition as a popular particular case. 
Finally, we describe event-based datasets for action classification. 

\textbf{Event generation}: Event cameras are bio-inspired sensors that trigger asynchronous events independently for each pixel when the brightness for that specific location changes beyond a given threshold. The output is a sequence of events where each event is represented as a tuple $e_i=\{u_i,t_i,p_i\}$ with $u_i=(x_i,y_i)$  the event coordinates, $t_i$ the time when the event occurred, and $p_i \in \{-1,1\}$ the polarity of the change (negative if the light decreases and positive if it increases). 
% An event is generated when the log intensity ($\mathcal{L}$) change for that location (see Eq. \ref{eq:brightness_change}) exceeds the contrast sensitivity threshold $C$ (Eq. \ref{eq:contrast_threshold}), where $\Delta t_i$ is the time between consecutive events for the same pixel.
% \begin{equation}
%     \Delta \mathcal{L}(x_i, y_i, t_i) = \mathcal{L}(x_i, y_i, t_i)-\mathcal{L}(x_i, y_i, t_i- \Delta t_i)
%     \label{eq:brightness_change}
% \end{equation}

% \begin{equation}
% \Delta \mathcal{L}(x_i, y_i, t_i) \geq p_k~C
% \label{eq:contrast_threshold}
% \end{equation}

%Bear in mind that events can be transformed into different representations to ease the extraction of processing features for solving different tasks. Events are used as: 1) plain individual events, which is useful for local processing solutions and usually simple; 2) packets of events, where events in a near spatio-temporal neighborhood are processed in batch \cite{mueggler2018continuous}; 3) event frames, a packet of events that are translated into a 2D grid, for example, a 2D histogram that counts the events at each spatial position \cite{gallego2020event}; or 4) Time Surfaces, this representation is a 2D map where each pixel intensity is a time value that provides information about when events in each spatial location occurred. %This representation exposes rich temporal information from events and can be processed using classical computer vision algorithms \cite{ahad2012motion}.

% Event-driven methods use plain individual events or small packets, where events in a near spatio-temporal neighborhood are processed in batch \cite{mueggler2018continuous} for simpler solutions. However, more complex processing requires of more spatial and temporal support than just a few events. Then, some works propose event frames, for example, a 2D histogram that counts the events at each spatial position \cite{gallego2020event}. The main drawback of event frames is that they neglect time information. As a result, other methods in the literature also propose building Time Surfaces: 2D maps where each pixel value is a time value that provides information about when events in that spatial location occurred. For example, authors in \cite{lagorce2016hots} present the HOTS method, that processes packets of events in a spatio-temporal window defined by a time constant to create hierarchical Time Surfaces at different spatial scales. Moreover, they use an exponential kernel to emphasize recent events over past events on the Time Surface (see also \cite{afshar2020event}). In our work, we also build Time Surfaces with fixed spatio-temporal windows, with exponential decay.

%or 4) Time Surfaces,  %This representation exposes rich temporal information from events and can be processed using classical computer vision algorithms \cite{ahad2012motion}. 

% Action recognition solutions using events (action recognition and gesture recognition)
\textbf{Event-based Action Recognition}: Recently, several solutions have been presented for action recognition using event data. Many approaches bin events from small time intervals into images to apply conventional vision approaches, such as CNNs and Recurrent Neural Networks (RNNs) \cite{moreno2022visual}.
%to learn sequences from frames \cite{moreno2022visual}.
%use spatial grid representations to structure events in order to apply conventional vision approaches. The naive approach consists on creating artificial images from events and use conventional frame-based CNNs and Recurrent Neural Networks (RNNs) to learn sequences from frames \cite{moreno2022visual}.
%\cite{pansuriya2020human, moreno2022visual}. 
One of the main drawbacks of these approaches 
%that work on artificial event images 
is the loss of temporal information, precisely, one of the main advantages of event data. Other works quantize events maintaining the temporal information. For example, in N-HAR \cite{pradhan2019n}, authors use memory surfaces built from sparse events as input to a deep CNN, specifically a variant of the Inception V3 architecture.
%pretrained with ImageNet (although the network is only trained on a small action dataset). 
% Also, in \cite{ghosh2019spatiotemporal} authors propose the use of spatiotemporal filters to convey event data into CNNs. These filters are spatiotemporal matrices learned in an unsupervised manner showing good performance, in this work for gesture recognition. 
The study \cite{annamalai2022event} proposes an unsupervised task-agnostic LSTM architecture, called Event-LSTM, for converting events into time surfaces. The proposal ensures velocity invariance through an asynchronous 2D spatial grid sampling stage.

%Finally, an interesting approach is presented in \cite{chadha2019neuromorphic} where authors embed an event emulator into a framework used for knowledge distillation. Although the proposal uses conventional images, the student model is fed with event data generated by an emulator from the images. The teacher network, on the other hand, is a model pre-trained on optical flow estimated from the sequence of frames. Thanks to the distillation framework, this teacher knowledge is transferred to the student by minimizing the teacher-student output cross-entropy. Although the work is an interesting alternative where the emulator can be replaced afterwards by a real event sensor, the teacher model needs to perform inference on optical flow. 

% Action/activity recognition, gesture recognition is also a similar field ...
%Another close application field is 
Another application is human gesture recognition, which is used in tasks such as the translation of sign language
and in human-computer interfaces.
%Several works  have developed techniques for the application of human gesture recognition, because of its interest in tasks such as translation of sign language or human-computer interfaces. 
These are very complex tasks where the motion dynamics over time are critical for recognizing the type of action. As pointed out in \cite{vasudevan2020introduction}, sign language and activity recognition usually involve visuospatial patterns at high speeds. Different approaches have been proposed in the literature. For example, \cite{vasudevan2020introduction} proposed two approaches based on biologically-inspired Spiking Neural Networks (SNNs) using backpropagation, namely SLAYER and Spatio Temporal Back Propagation (STBP). %In particular, SLAYER trains a SNN that simultaneously learns synaptic weights and axonal delays of SNN layers with backpropagation through time. 
% Also, authors in \cite{bi2020graph} introduce a spatial feature learning module with residual-graph convolutional neural networks (RG-CNN) that models temporal dependencies from asynchronous events for object classification and action recognition. 
%Another alternative 
\cite{baldwin2021time} introduced Time-Ordered Recent Event (TORE) volumes to compactly store raw spike timing information, and evaluated them on various tasks, including gesture recognition. Finally, authors in \cite{sabater2022event} proposed an Event Transformer framework for gesture recognition that efficiently processes event-data using a patch-based event representation.
%that follows a frame-level representation. 
In their conclusions, the authors point out that approaches that avoid stacking events into frames and exploit the temporal information achieve the best results for gesture classification. 

% Finally, authors in \cite{amir2017low} take advantage of the low-power consumption of event sensors, proposing an efficient system for gesture recognition on the TrueNorth neuromorphic hardware. 

%Also, given the advantages of event sensors such as the reduction of computation due to their smart compression, a system for gesture recognition using a smartphone is presented in \cite{maro2020event}. Although the model is trained on a dataset of gestures that is very limited, the system works in real-time doing dynamic background suppression. 

\textbf{Manipulation action datasets}: There are several datasets on event-based action and gesture recognition, but not all are useful for evaluating manipulation actions. \textit{DVSACT16} \cite{hu2016dvs} is a dataset that contains event sequences of up to 50 actions (most of them sports), recorded with a \textit{DAVIS240C} event sensor viewing video sequences from the well-known \textit{UCF-50 Action Recognition} dataset \cite{reddy2013recognizing} displayed on a monitor. Although valuable for event-based action recognition, the monitor refresh rate adds unrealistic noise. Furthermore, most actions are full-body while in our task, we are focused on manipulations.
%, and thus, on the hand and object dynamics.

Closer related to our task is the \textit{DVS 128 Gesture Recognition} dataset of IBM \cite{amir2017low}. It contains 11 hand gestures from 29 different subjects and has more than 1300 recordings. However, it was designed for problems such as human-computer interaction and therefore does not include object manipulations. Interestingly, it includes different illumination conditions. Finally, the \textit{SL-Animals-DVS dataset} \cite{vasudevan2020introduction} has around 1100 samples from 58 subjects performing 19 gestures in Spanish sign language. Subjects perform different actions mainly with their hands. This makes recognition challenging because gestures are usually performed very fast, and the background scenes are diverse. %Both datasets were introduced to foster research on event-based action recognition systems. 
For gesture recognition as well as for our problem, it is crucial to take into account the dynamics of the activities encoded by the events. However, different from gesture recognition, we are interested in manipulation of objects with the hands, for which there do not exist datasets yet.

\section{Our Approach}
\label{sec:our_approach}
%In this work, we introduce a solution for 
Next we describe our algorithms for the prediction of manipulation actions using as input asynchronous events from a neuromorphic sensor. Our solution builds time surfaces from events, which are fed to a neural network architecture, each as a 2D grid.
To take advantage of the very accurate timing information of asynchronous events, the network architecture uses a transformer that captures long-range dependencies between events using self-attention. In the experiments, we compare the proposed event-based method with other \textit{classification} and \textit{prediction} architectures. %Classification networks require the whole batch of information, the sequence of frames or the whole stream of events, to predict a label. Contrarily, predictive networks are designed for online inference. They output a prediction each time a new asynchronous event is processed (or a small batch of them in order to improve the computational efficiency), significantly reducing the prediction latency and thus, enabling real-time processing.  

\subsection{Feature extraction from event-based data}
\label{sec:event_processing}
%Asynchronous event cameras output a stream of events $\{ \textbf{e}_i \}_{i \in \mathbb{N}}$. Each event $\textbf{e}_i$ is a tuple that represents the intensity change for a location at a specific time as in $\textbf{e}_i=(x_i,y_i,t_i,p_i)$, where $(x_i,y_i)$ are the pixel coordinates, $t_i$ represents the timestamp at which the event was triggered, and $p_i \in \{-1,1\}$ is the polarity of the event with $-1$ and $1$ standing respectively for a decrease or an increase in the pixel intensity.  

To process asynchronous events $\textbf{e}_i$, we build \textit{Time Surfaces}, which allow us to use conventional algorithms while taking advantage of the benefits of event-based vision. The \textit{Time Surface} representation is a local descriptor common in event-based vision, which integrates the information of groups of events over time.
%\cite{benosman2012asynchronous, vasco2016fast, alzugaray2018ace, manderscheid2019speed, hu2021less, mitrokhin2020learning}. 

In our work, we use $\Gamma_{e}(\textbf{u},t)$ as described in Eq. \ref{equation:decaying_time_surface}, which is a \textit{Time Surface} built by summing events weighted with an exponentially decaying factor \cite{afshar2020event}. $\textbf{u} = (x_i,y_i)$ denotes the set of all event spatial coordinates and $t$ is the time when the surface is built. The function $\sum_{e}(u_i)$ maps event time to spatial coordinates, and $P_e(u_i)$ maps the polarity of each event. The decaying factor weighs the information provided by the events giving more importance to recent events and decaying earlier ones smoothly towards zero over time. 

\begin{equation}
\Gamma_{e}(\textbf{u},t)= \begin{cases}
P_{e}(u_i)\cdot e^{(\frac{\sum_{e}(u_i)-t}{\tau})}, & \sum_{e}(u_i)\leq t \\ 
0 & \sum_{e}(u_i) > t 
\end{cases}
\label{equation:decaying_time_surface}
\end{equation}

%Noise reduction
One of the challenges of working with event-vision sensors is dealing with noisy and spurious events \cite{gallego2020event}. Given the intrinsic sparsity of event-vision, it is crucial to reduce noise to extract meaningful information \cite{mitrokhin2018event}. The first mechanism for noise reduction consists of removing spatio-temporal isolated events,
%with no others in their spatio-temporal neighborhood, 
using a salt-and-pepper-like filter.

% Figure environment removed

\subsection{Action Recognition via Deep Learning}
\label{sec:action_recognition_dl}
We classify action recognition architectures into classification and predictive models. \textit{Classification models} require to be fed with the data
%in a specific time span that captures the dynamics 
of the complete action. Since inference is performed with the whole data, either as frames or event temporal surfaces, this results in longer latency. On the other hand, \textit{predictive models} perform online prediction, integrating the information frame by frame or as in our case, event by event.
%, as they are triggered by the sensor. 
Online prediction systems offer benefits such as faster continuous prediction while reducing memory and computation costs \cite{kondratyuk2021movinets}. 

%% Figure environment removed

% This is our approach for dealing with events
Our approach is based on a predictive continuous solution that uses a popular attention-based \textit{Transformer} model \cite{vaswani2017attention}. This sequence-modeling network is composed of a stack of identical transformer layers with attention heads. Each independent attention head produces a different attention distribution.
%, which enables more interpretable models. %In \cite{vaswani2017attention}, authors proposed an encoder-decoder architecture built using stacked transformer layers for transduction applications. 
For our use case, only the encoder component of this highly paralellizable architecture
is used, which is sufficient  
%suffices for the purpose of this work. With it, we expect
to efficiently capture long-time range dependencies in sequences of events, and leads to interpretable online prediction. 

Our Transformer network analyzes every new time surface built from new incoming events. Specifically, a \textit{2D Mobilenet} model \cite{sandler2018mobilenetv2} extracts the spatio-temporal features from the temporal surfaces that are positionally encoded. Then, a single-layer Transformer encoder block analyzes the relationship between the new time surface and the previous ones. In this step, a look ahead mask is applied to force causal attention. %(see Figure \ref{figure:causal_self_attention}).
In other words, the search is limited to previous temporal surfaces, never to future ones, ensuring causality. This way the model provides online prediction from continuous sequences of events. Finally, fully connected layers generate the manipulation action prediction.

To fully understand how the transformer model is converted into a predictive architecture, please refer to Figure \ref{figure:transformer_architecture}. The spatio-temporal features extracted by the \textit{Mobilenet 2D} architecture are queued, along with the previous spatio-temporal features from previous time surfaces. This stateful architecture preserves the features in a queue between inferences. Thus, every time a new time surface needs to be processed, its spatio-temporal features are queued and processed by the Transformer Block to learn the temporal relationship to the previous features.
By following this approach, spatio-temporal features are extracted from the time surfaces only once, which happens to be also the most computationally expensive part of the processing. 

%This approach for online inference is inspired in how the Tesla AutoPilot \cite{tesla2021ai} system is deployed using Transformer architectures for online prediction. Tesla Autopilot uses a Resnet backbone that extracts the spatial features from the vehicle RGB cameras that work at 36 Hz. Then, these features are appended into a features queue in order to be processed by a Transformer network that captures long-range dependencies between the images. Features are retained in the queue between inferences and are popped out when the car travels a certain distance. With this, the system can make decisions every time a new frame is obtained, having also context of the temporal information. 

%\subsection{Hand tracker}
%\label{sec:hand_tracking}
%As part of our study, an event-based hand tracker \cite{delbruck2008frame} is also used. This tracker is a very fast method that implements a cluster tracker, assuming spatially-connected events in rectangular areas. This simple method only requires a first source that is used to build the model of the object to be tracked. Moreover, it runs in real-time since only pixels that generate events need to be processed in the region of interest. The computational cost is limited to the search of the nearest cluster, that is generally very close due to the high-temporal resolution of the event sensor output.

%The hand tracker locates and tracks the hand that manipulates the object and performs the action, limiting the data to be processed to the area around the hand. In this case, our hypothesis is that the action should be identified using only the hand dynamics and therefore, events from this region will suffice for this task. Note though that, after the contact point (when the hand touches the object), events that belong to the object itself are partially contained in the hand region since hand and object cannot be split apart from there on. 

% The aim of using the hand tracker as a previous step to select the region of interest that contains the hand performing the action is two-folded. On the one hand, reducing the spatial resolution of the input time surface has a direct impact on the prediction performance of the model. The resolution reduction limits the floating-point operations (FLOPS) required to perform inference and thus, makes the prediction more efficient. On the other hand, restricting the input to the hand data helps the model to better generalize since other details about the subject or the scene are excluded. Furthermore, using only the output from the hand tracker favours the model to focus on the hand dynamics.

\subsection{Event training procedure}
\label{sec:event-training}
Firstly, event-data is processed following the approach for feature extraction described in Section \ref{sec:event_processing}. We project events into time surfaces.
%in order to proceed with the analysis of the activity using the predictive and classification models. %In particular, for this use case we define the constant $\tau=33ms$. 
%With $\tau=33ms$, 
A new time surface is built every 33 ms and thus, the system generates around 30 time surfaces per second.
%from the asynchronous events captured by a neuromorphic vision sensor. 

%The analysis of a single \textit{Time Surface} does not suffice for the classification of manipulation actions, specially when they involve long, cyclic or repetitive actions that extend over time such as "poking a hole" with a knife or "shaking" a bottle. In order to correctly identify the action, it is crucial to capture its dynamics. For this reason, 
For training, we feed our neural model with the time surfaces from a time span of two seconds (60 Time Surfaces). This is the average time required to capture the dynamics of the manipulation actions in the different datasets included in our evaluation. However, our predictive network architectures are designed for online prediction, %being capable of producing a prediction for only 
to update the prediction on any new incoming time surface.
%at a time, greatly reducing the prediction latency. %Briefly, our dataset includes 30 manipulation actions using 6 different objects (\textit{cup, stone, sponge, spoon, knife} and \textit{spatula}), further details are introduced in Section \ref{sec:manipulation_actions_recognition}.

%Also, note the importance of data augmentation when the available data is limited. %Data augmentation is useful to improve the quality and increase the size of the dataset \cite{shorten2019survey}. 
%This approach helps to reduce overfitting, resulting in models with better generalization. 
%Concretely, we defined a preprocessing 
Our data augmentation
%is as follows. We augment the samples by applying 
applies random rotations, cropping and horizontal flipping operations.
%by randomly rotating the position of the events in the 2D space. Next, we randomly crop a portion of the spatial domain of the events. Finally, we randomly flip the position of the events in the horizontal axis. 
%The goal of this procedure is to feed the network at training time with different data every epoch, with the focus in preventing overfitting. As a result, 
The spatial resolution of Time Surfaces is set to $144\times144$.

%The training procedure is done as follows. First, a random time slice window of two seconds of events from a sample is selected.
%; the time slice is defined in order to ensure that action dynamics is contained in it.
%Next, events are pre-processed and follow a data augmentation process; asynchronous events are transformed into Time Surfaces and training starts. As a regular basis, 
Training is done in two phases. First, we freeze the model backbone weights and train only the output layers for 60 epochs. Then the model is fine-tuned by retraining all layers for 60 more epochs. For our \textit{EMAD} dataset (see Section \ref{sec:manipulation_actions_recognition} for a detailed description), because of the limited number of samples per class (only 25), training is done in three stages. First, models are trained to distinguish between the 6 categories of object interactions for 60 epochs using a batch size of 8; second, weights are frozen and transfer learning is applied to identify the 30 manipulation actions by retraining only the final layers for 10 epochs. Finally, models are end-to-end fine-tuned for 100 additional epochs.

\section{Discussion and Results} 
In this section we describe the implementation details and evaluate our model against the state of the art on event-based datasets for action and gesture recognition. Next we describe our \textit{Event-based Manipulation Action Dataset}.
We analyze the model's self-attention mechanisms on event patterns. Finally, we compare our approach to similar frame-based approaches in accuracy and time performance.
%, we first outline the implementation details about the models for event-based manipulation action recognition and the training procedure. After that, we conduct an analysis comparing the evaluation performance of different models in the state of the art for action recognition, using the event datasets for action and gesture recognition in the literature. Next, we describe our \textit{Event-based Manipulation Action Dataset}, with actions manipulating different objects. The section also includes an ablation study to assess the generalization capabilities of the different alternative models.

%Since the transformer is based on self-attention mechanisms, we present an analysis of event patterns and their emerged relationships to distinguish categories of manipulation actions. Finally, we compare our event-based approach with respect to similar frame-based approaches in accuracy and time performance. %Finally, a particular section presents the benefits of using the hand tracker to feed the models with the events only from the hand that performs the action. %In this point, we also compare the different alternatives when hand-tracking is performed to assess the impact in terms of computational complexity and evaluation performance.

%\subsection{Event-based vision data}
%We decided to study how our solution classifies manipulation actions, in other words, activities that involve human-object manipulation interactions. % \cite{yang2014cognitive}. 
%Concretely, the identification of manipulation actions from event-based data is useful for the development of human-computer interfaces or human-robot interaction applications where the low-latency requirement is a crucial factor \cite{zhu2020technologies,wu2021hurai}.

\subsection{Event-based action recognition models}
\label{sec:models_discussion}
We have implemented and adapted four architectures, two classification and two predictive models. %All the original models were frame-based neural models that have been adapted to perform action recognition from events.

The two classification architectures are:  \textit{Mobilenet v2 3D} \cite{sandler2018mobilenetv2} and  \textit{Inception 3D} \cite{szegedy2015going}, both 3D inflated versions of their 2D convolutional variants based on 3D convolution operations capturing the spatio-temporal features of the input. The \textit{Mobilenet v2 3D} model has been designed to minimize the floating-point operations (FLOPS), different from high-end vision architectures such as \textit{Inception}. 
As mentioned, 
%before, these models are classification networks 
since these are not memory-based architectures, they require as input the complete sequence of frames or the whole stream of events at inference. 

%On the other hand, 
The predictive models are: A \textit{Mobilenet LSTM} and our \textit{Mobilenet Transformer} \cite{vaswani2017attention}. The \textit{Mobilenet LSTM} is a Convolutional Recurrent Neural Network. A 2D Mobilenet backbone analyzes the time surfaces, then, a Long-short Term Memory (LSTM) \cite{hochreiter1997long} layer models the temporal features of the activity.  Our \textit{Mobilenet Transformer} has a 2D Mobilenet backbone and a Transformer encoder block (see Section \ref{sec:action_recognition_dl} for more details). As for the implementation details, we use a single Transformer encoder layer with 4 self-attention heads that process 256-dimensional features estimated from temporal surfaces constructed from packets of events. %This architecture configuration was selected due to its simplicity and great recognition performance. 
For the predictive versions, we introduce dense layers before the temporal analysis to project the features into a smaller latent dimension. This significantly reduces the computational complexity of the temporal analysis (with LSTM or Transformer blocks).

\subsection*{Comparison of event-based models}
Table \ref{table:eval_other_dataset} shows the accuracy performance for the \textit{DVS 128 Gesture Recognition} and the \textit{SL-Animals-DVS} datasets. For the \textit{DVS 128 Gesture Dataset} we used the train and test splits defined by the authors \cite{amir2017low} and for the \textit{SL-Animals-DVS dataset} we used a K-fold cross validation ($K=4$), following the same approach than in \cite{vasudevan2020introduction}. 

The first 4 rows summarize the results of state-of-the-art models using biologically-plausible solutions based on Spiking Neural Networks (SNNs). The performance values reported here are taken from their original papers. The next three rows show the results of the models described above, which are popular frame-based alternatives adapted to learn on temporal surfaces built from events. The last row shows the results from our Transformer architecture. 

For the DVS Gesture dataset all models have close to the state-of-the-art performance. For the SL-Animals-DVS dataset,
the models adapted from frame-based popular architectures reach much higher high accuracy performance than the SNN architectures. Our transformer-based approach reaches up to 36\% higher performance compared to the SLAYER approach \cite{vasudevan2020introduction}.
%method that processes event-by-event asynchronously using a SNN architecture. Furthermore, our solution also reaches state-of-the art performance on this dataset compared to the other event-based biologically plausible solutions such as TORE \cite{baldwin2021time} or EvT \cite{sabater2022event}. 
%Poor accuracy performance is still very common to the current biologically plausible alternatives. Finally, accuracy figures may also indicate that these event-based datasets do not carry that much information in the action dynamics, or have some spatial bias and lack intra-class variability \cite{vasudevan2021sl}. 

\begin{table}[t]
    \centering
    \caption{Accuracy of models for event-based action detection}
    \label{table:eval_other_dataset}
    \setlength{\tabcolsep}{1.8pt}
    \begin{tabular}{ l  r  r  r }
        \hline
        Architecture & DVS 128 Gesture & SL-Animals-DVS  \\
        \hline
        RG-CNN \cite{bi2020graph} & 97.20 & - \\
        SLAYER \cite{vasudevan2020introduction} & - & 60.09\\
        TORE \cite{baldwin2021time} & 96.20 & 85.10\\
        EvT \cite{sabater2022event} & 96.20 & 88.12\\
        
        \hline \Tstrut
        Inception 3D & \textbf{97.72} & \textbf{96.53}~$\pm$~0.85 \\ 
        Mobilenet v2 3D & 96.21 & $92.78\pm1.21$ \\
        % Shufflenet v2 3D & 94.31\% & $91.15\%\pm1.25\%$ \\
        Mobilenet LSTM & 96.21 & $94.47\pm0.61$ \\
        \hline \hline
        \textbf{Mobilenet Transf. (Ours)} & 96.21 & $94.11\pm0.59$ \\
        \hline
    \end{tabular}
\end{table}

\subsection{Dataset for manipulation action recognition}
\label{sec:manipulation_actions_recognition}
%In the literature, there is a complete lack of datasets for manipulation actions using asynchronous event sensors. The datasets in the previous section were designed mainly for gesture recognition in human-computer interaction, %including crucial differences such as the lack of object manipulation. 
%Moreover, they also present some drawbacks already mentioned in the previous section. Consequently, 
Next we describe the \textit{Event-based Manipulation Action Dataset} (\textit{E-MAD}), the first dataset on manipulation actions using events. It is the event-based counterpart of the RGB video dataset (see Figure \ref{figure:samples_mad}) \textit{Manipulation Action Dataset} introduced in \cite{fermuller2018prediction, fermuller2018predictionurl}.
%that contains only RGB image data (see Figure \ref{figure:samples_mad}). %The recordings were taken with a regular video camera and a neuromorphic vision sensor simultaneously to capture the activities carried out by the subjects, although they were not synchronized. 

\textit{E-MAD} was recorded using a \textit{DAVIS240} neuromorphic sensor with 240x180 resolution. Subjects performed  manipulations on single objects while seated at a table.
%, where there is an object to perform the action. All samples were recorded under similar lighting conditions. 
All actions started with the subjects with their hands on the table (palms down). When the recording starts, the subject moves his/her hand to reach the object, performs the action, and then returns to the initial resting position. \textit{E-MAD} contains actions for  one more object than \textit{MAD}, namely the spatula.
%Apart from the nature of the data, the main difference with respect to the \textit{MAD}, is that \textit{E-MAD} includes an additional object, a \textit{spatula}. 
We can roughly categorize the recorded actions into two kinds:
%Finally, it is worth mentioning that some actions due to its inherent nature, are 
\textbf{cyclic} actions, which are repetitive and performed continuously, such as \textit{shaking a cup} or \textit{scraping with a spatula}, and \textbf{discrete} actions, done only once such as \textit{drinking from a cup} or \textit{scooping with a spoon} (see the discussion of results in section \ref{sec:self_attention}). 

The recordings are from five subjects (\textit{S1, S2, S3, S4, and S5}) with 750 samples of 30 manipulation actions using 6 different objects. The dataset is balanced: every manipulation action is represented with exactly 25 samples. The list of actions recorded with each object are as follows:
\begin{itemize}
    \item \textbf{Cup}: \textit{drink, pound, shake, move}, and \textit{pour}.
    \item \textbf{Stone}: \textit{pound, move, play, grind}, and \textit{carve}.
    \item \textbf{Sponge}: \textit{squeeze, flip, wash, wipe}, and \textit{scratch}.
    \item \textbf{Spoon}: \textit{scoop, stir, hit, eat}, and \textit{sprinkle}.
    \item \textbf{Knife}: \textit{cut, chop, poke a hole, peel}, and \textit{spread}.
    \item \textbf{Spatula}: \textit{flip, lift, cut, squeeze}, and \textit{scrape}.
\end{itemize}

% Figure environment removed


\begin{table}[t]
    \centering
    \caption{Ablation study: Accuracy for actions on object classification ($N=6$)}
    \label{table:eval_six_objects}
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{ l r}
        \hline
        DL architecture & Accuracy  \\
        \hline \Tstrut
        Inception 3D &  91.98 $\pm$ 7.11\\ 
        Mobilenet v2 3D &  93.33 $\pm$ 3.59 \\
        Mobilenet LSTM & 83.60 $\pm$ 10.3 \\
        \textbf{Mob. Transf. (Ours)} & 91.46 $\pm$ 7.66 \\
        \hline
    \end{tabular}%
    % }
\end{table}

% \begin{table}[t]
%     \centering
%     \caption{Ablation study: per-subject accuracy for actions on object classification ($N=6$)}
%     \label{table:eval_six_objects}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{ l r r r r r }
%         \hline
%         DL architecture & S1 & S2 & S3 & S4 & S5  \\
%         \hline \Tstrut
%         Inception 3D &  \textbf{88.00\%} & 96.00\% & 97.33\% & 97.33\% & 81.33\%\\ 
%         Mobilenet v2 3D & 87.33\% & \textbf{96.67\%} & 95.33\% & 94.00\% & \textbf{93.33\%} \\
%         % Shufflenet v2 3D & 77.33\% & 91.33\% & 96.67\% & 94.00\% & 87.33\%\\
%         Mobilenet LSTM & 68.66\% & 87.33\% & 81.33\% & 97.33\% & 83.33\% \\
%         \textbf{Mob. Transf. (Ours)} &  82.00\% & 87.33\% & \textbf{99.33\%} & \textbf{99.33\%} & 89.33\%\\
%         \hline
%     \end{tabular}%
%     }
% \end{table}
\begin{table}[t]
    \centering
    \caption{Ablation study: Accuracy for manipulation action classification ($N=30$)}
    \label{table:eval_manipulation_actions}
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{ l r }
        \hline
        DL architecture & Accuracy  \\
        \hline \Tstrut
        Inception 3D & 77.33 $\pm$ 7.14\\ 
        Mobilenet v2 3D &  80.53 $\pm$ 7.35\\
        Mobilenet LSTM & 81.19 $\pm$ 2.55\\
        \textbf{Mob. Transf. (Ours)} & 86.80 $\pm$ 4.04 \\
        \hline
    \end{tabular}%
    % }
\end{table}

% \begin{table}[t]
%     \centering
%     \caption{Ablation study: per-subject accuracy for manipulation action classification ($N=30$)}
%     \label{table:eval_manipulation_actions}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{ l r r r r r }
%         \hline
%         DL architecture & S1 & S2 & S3 & S4 & S5  \\
%         \hline \Tstrut
%         Inception 3D & 76.67\% & 66.00\% & 85.33\% & 80.66\% & 78.00\% \\ 
%         Mobilenet v2 3D & 69.33\% & 81.33\% & 86.66\% & 87.33\% & 78.00\% \\
%         % Shufflenet v2 3D & 76.67\% & 78.00\% & 78.76\% & 75.33\% & 71.33\% \\
%         Mobilenet LSTM & 78.00\% & 79.33\% & 81.33\% & 83.33\% & \textbf{84.00\%} \\
%         \textbf{Mob. Transf. (Ours)} & \textbf{88.00\%} & \textbf{82.66\%} & \textbf{87.33\%} & \textbf{92.67\%} & 83.33\% \\
%         \hline
%     \end{tabular}%
%     }
% \end{table}

\subsection{Prediction study}
In this section, we study the generalization of the classification over subjects. We first evaluate over the 
six action categories on the different objects (referred to as super-categories), then on the 30 different actions.
%and predictive models on the \textit{E-MAD}. In order to do that, we evaluate the model results for the actions on object super-categories and subjects separately, leaving one out each time. 
We trained 5 classifiers, leaving each time one subject out that was used only for testing purposes. For each classifier, the data was split into 85\% for training and 15\% for validation. The training procedure was explained in Section \ref{sec:event-training}.

% Table \ref{table:eval_six_objects} shows the accuracy of the models for the classification of the 6 super-categories. We observe that when training with \textit{S1, S2, S3} and \textit{S5}, and testing with \textit{S4}, actions on objects super-categories are recognized better than in the other cases, and performance is the poorest when testing on \textit{S1} after training on the others. Accuracy  reaches a minimum average of about 81\% for all the models and subjects, with an average variation of 7-8 \%. 

Table \ref{table:eval_six_objects} shows the accuracy of the models for the classification of the 6 super-categories. The solutions reach a minimum average accuracy of about 83.60\% (\textit{Mobilenet LSTM}). In addition, most alternatives offer a recognition performance above 90\% with an average variation of 7-8\%. Hence, the architectures are capable of accurately identifying the objects that subjects manipulate. 

Table \ref{table:eval_manipulation_actions} shows the results for the classification on all 30 actions. We see that models based on 3D convolutions (\textit{Inception 3D, Mobilenet v2 3D}) in general have lower recognition performance with an average accuracy of about 80\%. For the worst case, the average accuracy of the most complex architecture, \textit{Inception 3D} is only $77.33\%\pm7.14$. 

%Our belief is that this poor performance compared to the other alternatives is due to their complexity. Finding the optimal weights for the 3D-convolution operations to extract spatio-temporal information is a very complex task that also requires more samples to converge. Note for example how all these alternatives reached very high evaluation performance identifying only the super-categories actions on objects, a task for which evaluating the dynamics of the hand motion, or in general the temporal information, is not as crucial as in the action recognition. 
%In addition, the results also highlight that 3D-based architectures are more prone to overfitting when they are trained with few data and evaluated on subjects that have not seen before. 

Table \ref{table:eval_manipulation_actions} also shows the performance of the continuous predictive models.
%: the \textit{Mobilenet LSTM}, and the \textit{Mobilenet Transformer}. Concretely, the
The \textit{Mobilenet LSTM} model achieves an average accuracy of $81.19\%\pm2.55\%$ about a point higher than the best 3D-convolution-based model. But the highest average accuracy is obtained by the \textit{Mobilenet Transformer}, reaching up to $86.80\%\pm4.04\%$. Furthermore, note how this last model also tends to generalize better than the others, reaching the best recognition performance.  
% for 4 out of 5 subjects in the dataset.
This finding is relevant since online predictive models allow inference as soon as the first data arrives, and they improve over time, reducing the latency of prediction.

\subsection{Online prediction evaluation}
% Figure environment removed

%As described above, two models were designed for efficient online inference: a \textit{Mobilenet LSTM} and a \textit{Mobilenet Transformer}. In this section, we address a comparison of these two architectures, feeding event-by-event as they are triggered, while studying the variation of the accuracy depending on the portion of sequence analyzed up to the actual event.

%Bear in mind that classification architectures are designed to receive a batch of events that represents a time slice of two seconds as mentioned in Sections \ref{sec:event_processing} and \ref{sec:manipulation_actions_recognition}. Therefore, the inference with the classification models has a latency of at least two seconds at the starting point. Afterwards, to provide online inference the network should be fed with a moving time slice window of two seconds for the rest of the sequence. As a result, when new events occur, we need to re-analyze past events to provide a prediction, a very inefficient approach and thus, impractical for online inference. 

%Focusing back on the predictive solutions, 
Figure \ref{figure:online_prediction_eval} shows the accuracy of the \textit{Mobilenet LSTM} and the \textit{Mobilenet Transformer} for online event-based prediction over time. % In particular, the left chart shows the evolution of the accuracy at different time steps for every subject of the ablation study using the \textit{Mobilenet LSTM} model, while the right chart shows the accuracy for the \textit{Mobilenet Transformer}. 
The x-axis represents the time since the beginning of the sequence. 
%One should take into account that, contrarily to the frame-based case, before the hand touches it, the object does not appear in the scene. 
Since the event sensor only records motion, the object is not ``seen'' before the hand touches it. At the start of the recordings, the object rests on top of the table and thus does not trigger events. After 250 ms, when the hand starts moving, the predictive models can classify some manipulation actions (accuracy above chance, about 3.3\%).

%As already said, the object becomes visible in the event space when the subject touches it (touching point).
Classification becomes clearer after the hand touches the object.
%for both models and this is appreciated in the graph since accuracy for both models builds up faster after this point. For instance, 
Within the [750 ms - 1s] time interval (touching point occurs on average around 500 ms), the models show a confidence of about 40\%.  At this time the subject grasps the object and starts interacting with it. %Which results in reaching for up to 40\% accuracy at this point. Afterwards, the subject starts performing the action. 
After only 1.6 to 1.7~s, the predictive models provide an average accuracy of over 70\%. At this time
%, events seem more informative compared to the beginning of the action, and 
the temporal information captures the dynamics of the activity. After 3~s, the overall average accuracy reaches 82\%. 

The two predictive architectures have similar performance in the classification. However, the \textit{Mobilenet Transformer} architecture has better generalization, as observed by its lower inter-subject variability.

 Table \ref{table:eval_predictive_manipulation_actions} summarizes data on the computational complexity. With less than 4 million weight parameters, each model computes around 8.5 Giga floating point operations (GFlops) when processing a two-second event sequence. Note how the \textit{Mobilenet Transformer} network is able to process more than 4700 Time Surfaces per second when running on an RTX 2080 GPU. This means that our efficient event-processing solution has the potential of processing around 150 times more times surfaces during the same time interval. %($\tau=33ms$).  

\begin{table}[t]
    \centering
    \caption{Time Performance of predictive models}
    \label{table:eval_predictive_manipulation_actions}
    \begin{tabular}{ l r | r r }
        \hline
        %\multicolumn{1}{c}{\multirow{2}{*}{DL architecture}} & \multicolumn{1}{c|}{\multirow{2}{*}{\# Params}} & \multicolumn{2}{c|}{} & \multicolumn{2}{c}{Using hand tracker} \\
        \multicolumn{1}{c}{\multirow{2}{*}{DL architecture}} & \multicolumn{1}{c|}{\# Params} & \multicolumn{1}{c}{\multirow{2}{*}{GFlops}} &  \multicolumn{1}{c}{\multirow{2}{*}{TS/s}} \\
        \multicolumn{1}{c}{} & \multicolumn{1}{c|}{(Millions)} \\ 
        \hline
        \Tstrut
        
       Mobilenet LSTM & 3.11 & 8.43 & 4662\\
       \textbf{Mob. Transf. (Ours)} & 3.90 & 8.47 & 4750\\
        \hline
    \end{tabular}
\end{table}


% Figure environment removed


% Figure environment removed

% % Figure environment removed

\subsection{Contribution of causal self-attention}
\label{sec:self_attention}
%The architecture of the \textit{Mobilenet Transformer} is described in detail in Section \ref{sec:action_recognition_dl}. This model is designed for online prediction, using a 2D convolutional backbone (Mobilenet v2) processing new events in consecutive time surfaces, 
%The proposed Transformer-based architecture includes a causal encoder architecture that captures the temporal relationships between time surfaces, over time. In particular, the Transformer block is designed to accomplish causal self-attention. It is a mechanism that extracts relations between elements within a sequence, but with the limitation that each element only attends to previous elements in that sequence. We use four self-attention heads for extracting these causal temporal relationships.
The Transformer architecture has blocks for causal self-attention. We can analyze these blocks to gain insight into the relationship between attention maps and the prediction of the model.
%to interpret the model. 
Attention maps show relationships within the sequence of event temporal surfaces and denote their relevance for classifying a predicted action. 
We 
%assess the study of causal attention for action recognition by analyzing 
next analyze the attention scores assigned by the \textit{Transformer block} for a few sample actions. 
%Four test samples representative of actions using different objects and level of motion are selected in this analysis, to understand which patterns of attention contribute to predict the manipulation actions. 
%In particular, these samples include "poking a hole" with a knife, "shaking" a cup, "eating" with a spoon, and "lifting" a spatula from the table. The two first activities are categorized as \textbf{cyclic} actions, meaning that the action is characterized by a repetitive pattern. For example, in "shaking" a cup the hand moves the cup quickly from left to right continuously. Also, "poking a hole" with a knife is done by holding the knife in a vertical position against the table and rotating it continuously around its own axis. Contrarily, actions such as "eating" with a spoon or "lifting" a spatula from the table are \textbf{discrete} actions.

%Figure \ref{figure:attention_map} shows the attention scores assigned by the \textit{Transformer Block} in the four attention heads for a test sample of "poking a hole" with a knife (a \textbf{cyclic} or repetitive action) over 60 time surfaces representing (about 2 s). %From the attention scores extracted by the heads (top of the figure), an analysis on the attention focus can be addressed. 

%Analyzing the maps, one can observe that \textit{TS 01} (Time Surface 1) (33-66ms) deserves significantly more attention from the next surfaces in the sequence. \textit{TS 01} corresponds to the initial movements of the hand just after leaving the resting position on top of the table. We consider that this part of the sequence encloses crucial data for the prediction in the hand pose and its movement just before grasping the knife. Following with the analysis, after 350 ms, self-attention maps show that new time surfaces now attend to the events that represent the touching point and then, when the hand grasps the knife. Obviously, the touching point is very significant for the prediction since it is the first time in the sequence when the object involved (the knife) moves. Bear in mind that, since we are using event cameras that focus on image temporal change, this is the first time the knife becomes "visible" in the event space. The next most attended surface supports the hypothesis of the motion dynamics relevance for the prediction: the peak on the focus of attention after 1100~ms that corresponds to the subject manipulating the knife and carrying it to the vertical position, ready for poking. After that, the range 1250 - 1600 ms corresponds to the subject poking the hole with the knife, rotating it (around \textit{TS 40}). When this part of the sequence starts, attention is not focused on a specific time surface but distributed on all the time surfaces in this interval while the rotation movement is performed.




%Figure \ref{figure:prediction_over_time} shows the prediction of the \textit{Mobilenet Transformer} model and how the confidence of the predicted action builds up over time as new events are analyzed. In particular, note how after the subject grasps the knife (after 350 ms), the model already shows confidence on the action "cut" with the knife. Then, when the dynamics of the action is captured by the events at the moment the subject starts placing the knife in a vertical position after 1100 ms (Time Surface 33), confidence on the label "cut" abruptly decreases while the confidence on the label "poking a hole" with the knife peaks. Afterwards, the confidence of this last action keeps increasing by the time new events representing the rotation motion of the knife in the vertical position occurs. As mentioned before, one of the advantages of this online model is that the prediction is very accurate almost a second before the action is finished. Such a reduction in the prediction latency is crucial for providing rapid responses to changing environments in real-world applications.



% % Figure environment removed




% Figure environment removed

\begin{table}[t]
    \centering
    \caption{4-Fold Cross Validation Manipulation Action classification}
    \label{table:eval_manipulation_actions_video}
    \begin{tabular}{l r r | r}
        \hline
        DL architecture & GFlops & Time Performance & Accuracy  \\
        \hline \Tstrut
        Mob. Transf. (Video) & 7.06  & 4750 FPS & $90.02 \pm 4.40$ \\
        Mob. Transf. (Events) & 8.47 & 4750 TS/s & $96.94 \pm 2.50$ \\
        \hline
    \end{tabular}%
\end{table}

% Figure environment removed

% Figure environment removed

%Firstly,
Figure \ref{figure:attention_cup_shake} shows the attention scores assigned by one of the attention heads of the \textit{Mobilenet Transformer} network (Layer 1, Head 4) when analyzing events captured from a subject \textit{shaking a cup} (a \textbf{cyclic} or repetitive action). The left side of the figure shows the attention scores assigned by the model to the different time surfaces. For example, one observes that time surfaces for the initial position of the hand (\textit{TS 06}) and for the hand grasping the cup (\textit{TS 24}) have a higher attention score than others. 
%Just after this, the attention shifts to a different pattern. At this point, the subject starts shaking the object left and right and thus, attention is distributed in consecutive time surfaces, paying more 
Following  attention is on surfaces 
%that show  the object in the vertical position with respect to shaking the cup to the left and the right sides (key frames are shown on the top of the figure, with almost a bird's eye perspective). The interpretation teaches us that the key time surfaces for this action according to the attention model are the grasping of the cup and later on, 
corresponding to the cyclic motion of shaking the cup left and right.% We observe a larger number of events in intervals \textit{TS 35} and \textit{TS 42} correspond  to intervals when the subject moved the object at a very high speed.
These surfaces are sufficient for a correct prediction with the neural model. 
%Finally, just note that the 



%On the other hand, 
Figure \ref{figure:online_predictions}-left shows the online prediction results of the sample in Figure \ref{figure:attention_cup_shake}. Prediction starts after 
%Note how the model is not capable of predicting any action until after 1000~ms. This is because in this sample 
the cup is picked at 850~ms, as 
%and then, 
only then the cup becomes visible to the event sensor. % Up until then, only the hand is moving to reach the cup and that is not enough information for the prediction. 
%Subsequently, when the subject moves the cup (\textit{TS 34}), the \textit{Mobilenet Transformer} architecture does not distinguish properly whether the subject is just moving the cup or pounding with it. However, 
Then after 1400~ms, the model captures the fast movements of the cup, which are  specific to the action \textit{shaking a cup}, and the 
%. After then, the shaking part of the action keeps happening and one can see how the 
confidence for \textit{shaking} builds up rapidly  after 2~s. % In summary, after only 2~s of observation the model obtained a very high accuracy of the action.



% Figure \ref{figure:prediction_over_time} shows the output confidence for a sample of "poking a hole" with a knife (\textbf{cyclic} action). Note how just after the touching point the model identifies the object properly. However, the confidence of the correct action starts increasing by the time new events representing the repetitive rotation motion of the knife in the vertical position occurs.

 Figure \ref{figure:spoon_spatula_confidence} shows the attention maps and the output confidence for \textit{eating with a spoon} and \textit{lifting a spatula}, two discrete actions. In \textit{eating}  using a spoon (Figure \ref{figure:spoon_spatula_confidence}-left)  the initial position of the hand receives attention (\textit{TS 00}) until the subject grasps the spoon (\textit{TS 16}). Other attended surfaces correspond to the spoon starting going up and reaching the mouth of the subject. In the output confidence chart, after the touching point (around 500~ms) the model identifies the object \textit{spoon} and the confidence for the action \textit{eating} increases when the subject raises the spoon to his/her mouth. When the subject puts down the spoon the confidence increases to 90\%. 
 %It takes less than 1.8~s to correctly identify the action in this case. 
 For  \textit{lifting the spatula} (Figure \ref{figure:spoon_spatula_confidence}-right)  the identifying patterns are due to the hand pose before grasping the object (\textit{TS 01}), the touching point (\textit{TS 14}), and the motion of moving the spatula up. After only 1~s, the predictive \textit{Mobilenet Transformer} is capable of identifying the right action with very high confidence. %As the subject further moves up the object, the confidence continues to increase.



Figure \ref{figure:online_predictions}-right shows the action  \textit{wiping} with a sponge. Here the hand pose before reaching the sponge for 250~ms suffices for the identification of the activity with very high confidence. This is much earlier than the touching point (dashed line around 500~ms).


%This indicates that the action is defined almost exclusively by the hand pose while reaching the sponge. %For the stone, 1 s is enough for correctly identifying the action with the touching point happening around 550 ms. 

In summary, the attention mechanism allows us to analyze the patterns most relevant for distinguishing between manipulation actions. Note that causal attention forces the network to pay attention only to past events. % This benefits the performance of the architecture when deployed for online prediction. 
Some of the clear patterns of attention are:
%Then, the analysis of the attention maps ensures interpretability: several clear patterns of attention have been identified. For example, the attention to 
There is attention to the hand pose at the beginning of the action. %; for some sequences only this information is enough for the classification. 
The touching point is crucial for most 
actions.
%samples analyzed, since it is the exact time at which the object starts to be \textit{visible} by the neuromorphic sensor. Finally, the rest of the 
Other key features are determined by the action category: for repetitive (\textit{cyclic}) manipulation actions, 
patterns characterizing the cycles get more attention, distributing attention equally among consecutive time surfaces to capture the dynamics of the action during the cycles; for discrete actions, attention is on characteristic movements such as specific gestures or the hand position and pose just before starting the manipulation.




\subsection{Manipulation Action Detection from RGB videos}
We next compare the performance of our event-based Transformer with the corresponding video-based Transformer. Video data provides more information about the static content. However, events provide significantly more accurate information about the motion in the scene. 


We train a video-based Transformer on the video-based \textit{MAD} dataset \cite{fermuller2018predictionurl}. The video dataset was collected simultaneously with the event dataset. However, the viewing angles are slightly different, and it only has actions on five of the objects (leaving out the \textit{spatula}).

The computational cost is similar. Referring to  Table~\ref{table:eval_manipulation_actions_video}, we see that the video Transformer has worse performance. The event-based alternative is on average 7.6\% more accurate. The confusion matrix in Figure \ref{figure:confusion_matrix_video} shows how the video model is very accurate at identifying the object that the subject is interacting with. However, it predicts wrong labels when distinguishing between the different actions on the same object. 


Additionally, Figure \ref{figure:shaking_cup_video} illustrates attention maps from head 4 and output confidence for the video-based Transformer model processing a sample of ``shaking a cup'' (the same sample that was illustrated in Figure \ref{figure:attention_cup_shake} for the event-based transformer). Note that now, attention is mainly drawn to the first frame which only shows the object on top of the table. Later on, attention is directed to the last frames where the hand is shaking the cup (frames 41 - 45). The model is able to identify with high confidence the object (above 90\% confidence) even before the touching point. This is because, contrarily to the event-based approach, the video-based model captures the object appearance features already from the first frame. However, it takes for the video-based approach almost 2~s to extract features of the action itself to infer the final correct label (\textit{shake}), but with low confidence (and lower than the event-based transformer model). Again, the results seem to show that the event-based model better captures the scene dynamics, outperforming the video-based model.



%In terms of computational cost, 
Table \ref{table:eval_manipulation_actions_video} shows that the two approaches have similar computational complexity. On an RTX 2080 GPU, the event-based model processes up to 4750 time surfaces per second, performing faster than real-time. The event-based approach has the additional advantage of lower latency between predictions at online inference - the time to wait to build a new time surface is 33~ms, but this could easily be reduced. 
%In the best case, for every new asynchronous event that comes in, a new time surface could be built and processed. However, significant changes for recognition do no occur with every event and, massively parallel architectures such as GPUs required of larger packets of events to be processed in order to fully exploit their real-time capabilities.  




~

\section{Conclusions}
This work demonstrates manipulation action prediction using neuromorphic sensors, achieving state-of-the-art results using a Transformer-based neural model. Predictive architectures show better performance in the ablation study, additionally enabling online inference. % in opposition to conventional classification models. 
%Online inference is accomplished by processing event-by-event, or more concretely, Time Surface by Time Surface (a chunk of events during a short period of time) to provide a prediction every time new data is collected.  (to fully take advantage of high-performance GPU architectures). 
Online inference helps to leverage the asynchronous nature of event data and to reduce the latency of inferring new predictions. Moreover, online continuous prediction reduces latencies by up to 2-3~s in our experiments compared to classification models. %, since online prediction models do not need the whole sequence to produce a prediction. 
Significant latency reduction is crucial for many applications, such as robotic collaboration or gesture recognition. 

%While action recognition using conventional frames is greatly influenced by the object features from the beginning of the action, objects are not "visible" for event sensors until the touching point. This difference forces the neural model to look for additional cues, for example, in the hand pose and trajectory before grasping the object. 

Another key aspect of our attentional model is the interpretability of the prediction decision process. In our case, a by-product of the model is the taxonomy of actions that naturally emerges from the learning process: \textit{cyclic} actions that show a repetitive pattern such as \textit{shaking a cup} or \textit{poking a hole}, and \textit{discrete} actions that are done only once such as \textit{eating with a spoon}. The analysis shows that attention for \textit{cyclic} actions is drawn by the time surfaces that correspond to the starting and end points of the cycles. For \textit{discrete} actions, the focus of attention is on the hand pose before the manipulation and the trajectory of the hand + object while performing the action. A comparison showed that video-based approaches extract features that are mostly appearance-based while event-driven approaches extract dynamic features of the actions. This makes event-driven solutions better candidates for classifying actions with subtle dynamic differences, for example, different manipulation actions on the same objects. % This was not intended when our \textit{E-MAD} dataset was recorded and collected, but it is a valuable result obtained from the interpretation of the attention maps obtained from each attention head.


%After analyzing the different models for classification and prediction, the conclusion is that
We conclude that the Transformer, because of its ability to perform online prediction and because of its good performance, is well suited for identifying manipulation actions from event-driven sensors. %The discussion shows the potential of attentional approaches using events that fully exploit dynamics in the scene in real time, firstly from the hand and then from the hand+object during the manipulation. 
Compared to the video-based model, the event-driven model has  better performance with shorter latencies, %inferring predictions performing much further than 
and infers in real-time.      

% Moreover, when operating with constraints in terms of computational power, the \textit{Mobilenet Transformer} architecture coupled with the hand tracker is a solution that offers a great accuracy vs. computational complexity trade-off.


\section*{Acknowledgements}

\noindent This work was supported by the Spanish National Grant PID2019-109434RA-I00/ SRA (State Research Agency /10.13039/501100011033). We acknowledge the Telluride Neuromorphic Cognition Engineering Workshop (\url{http://www.ine-web.org}), supported by NSF grant  OISE 2020624 for the fruitful discussions on neuromorphic cognition and their participants for helping with the recording of the dataset.



%% References with BibTeX database:

\bibliographystyle{unsrt}
\bibliography{bibliography}

%% Authors are advised to use a BibTeX database file for their reference list.
%% The provided style file elsarticle-num.bst formats references in the required Procedia style

%% For references without a BibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have the following form:
%%   \bibitem{key}...
%%

% \bibitem{}

% \end{thebibliography}

\end{document}

%%
%% End of file `ecrc-template.tex'. 