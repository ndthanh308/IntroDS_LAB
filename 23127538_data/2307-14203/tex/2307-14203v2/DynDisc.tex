\documentclass[12pt,notitlepage]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{float}
\usepackage[utf8]{inputenc}
\usepackage{comment}
\usepackage{tabularx}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{tocloft}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{subcaption}

% --- Define a new list for the appendix TOC ---
\newlistof{appsec}{appsec}{Appendix Contents}

% Command to print the Appendix Contents
\newcommand{\listofappendixcontents}{%
	\listof{appsec}{Appendix}%
}

% --- Set formatting for the custom list to mimic the normal TOC ---
\setlength{\cftsecindent}{0em}                 % no indent for sections
\setlength{\cftsecnumwidth}{2.3em}              % space for section numbers
\setlength{\cftsubsecindent}{1.5em}             % indent for subsections
\setlength{\cftsubsecnumwidth}{3em}             % space for subsection numbers
\setlength{\cftsubsubsecindent}{3em}            % indent for subsubsections
\setlength{\cftsubsubsecnumwidth}{3.5em}        % space for subsubsection numbers

% --- Custom commands for appendix entries ---
% These commands both typeset the sectioning heading and add an entry to the custom list.

\newcommand{\appsection}[1]{%
	\section{#1}%
	\addcontentsline{appsec}{section}{\protect\numberline{\thesection}#1}%
}

\newcommand{\appsubsection}[1]{%
	\subsection{#1}%
	\addcontentsline{appsec}{subsection}{\protect\numberline{\thesubsection}#1}%
}

\newcommand{\appsubsubsection}[1]{%
	\subsubsection{#1}%
	\addcontentsline{appsec}{subsubsection}{\protect\numberline{\thesubsubsection}#1}%
}

\allowdisplaybreaks

\setcounter{MaxMatrixCols}{10}

\topmargin=-1.5cm \textheight=23cm \oddsidemargin=-0.0cm
\evensidemargin=-0.0cm \textwidth=16.5cm
\usepackage[bottom]{footmisc}
\newtheorem{ass}{Assumption}
\newtheorem{definit}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{conj}{Conjecture}
\newtheorem{cor}{Corollary}
\newtheorem{rem}{Remark}
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\arabic{section}.\arabic{subsection}.\arabic{subsubsection}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% References
\usepackage{natbib}
\bibliographystyle{../../references/aea}

\newcommand{\I}{\mathbb{I}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Ll}{\mathrm{L}}
\renewcommand{\L}{\mathbb{L}}
\newcommand{\Var}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\supp}{\mathrm{supp}}
\newcommand{\notimplies}{\mathrel{{\ooalign{\hidewidth$\not\phantom{=}$\hidewidth\cr$\implies$}}}}

% -- BEGIN CODE TO IMPORT ONLY \widecheck FROM MATHABX --
\makeatletter
\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{
	<-> mathx10
}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareFontSubstitution{U}{mathx}{m}{n}

% "71 is the slot in mathx that contains \widecheck
\DeclareMathAccent{\widecheck}{0}{mathx}{"71}
\makeatother
% -- END CODE TO IMPORT ONLY \widecheck FROM MATHABX --

\hypersetup{
    pdftitle={DynDisc},    % title
    pdfauthor={Francesco Ruggieri},     % author
    pdfnewwindow=true,      % links in new window
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=black,          % color of internal links
    citecolor=blue,        % color of links to bibliography
    filecolor=black,      % color of file links
    urlcolor=blue           % color of external links
}

\begin{document}

\title{Dynamic Regression Discontinuity: \\An Event-Study Approach\thanks{I am grateful to Alex Torgovitsky for his guidance and support. I also thank Scott Behmer, St\'ephane Bonhomme, Matias Cattaneo, Hazen Eckert, Nadav Kunievsky, Jonathan Roth, Noah Sobel-Lewin, and participants in the Econometrics Advising group at the University of Chicago for their feedback and comments.}}

\author{Francesco Ruggieri\thanks{University of Chicago, Kenneth C. Griffin Department of Economics. E-mail: \href{mailto:ruggieri@uchicago.edu}{ruggieri@uchicago.edu}}}

\date{\today\thanks{An earlier version of this paper was circulated under the title ``Dynamic Regression Discontinuity: A Within-Design Approach'' and dated July 26, 2023. The present version supersedes and subsumes the contents of that draft.}}
\maketitle

\medskip

\begin{abstract}
	I propose a novel argument to identify economically interpretable intertemporal treatment effects in dynamic regression discontinuity designs (RDDs). Specifically, I develop a dynamic potential outcomes model and reformulate two assumptions from the difference-in-differences literature---no anticipation and common trends---to attain point identification of cutoff-specific impulse responses. The estimand of each target parameter can be expressed as the sum of two static RDD contrasts, thereby allowing for nonparametric estimation and inference with standard local polynomial methods. I also propose a nonparametric approach to aggregate treatment effects across calendar time and treatment paths, leveraging a limited path independence restriction to reduce the dimensionality of the parameter space. I apply this approach to estimate the dynamic effects of school district expenditure authorizations on housing prices in Wisconsin.
\end{abstract}

\setcounter{page}{0}\thispagestyle{empty}
\baselineskip1.47\baselineskip%


\newpage

\section{Introduction}

Regression discontinuity designs (RDDs) are commonly used to identify causal parameters when the probability of exposure to a treatment changes discontinuously at a known deterministic threshold. In a seminal contribution, \cite{cfr2010} extended the canonical design to multi-period settings in which treatment assignment is sequential and exhibits path dependence. Dynamic RDDs have proven popular in the empirical public finance literature that exploits local referenda to estimate the effect of increased government expenditure on various outcomes (\citealt{darolia2013}, \citealt{hongzimmer2016}, \citealt{martorelletal2016}, \citealt{abottetal2020}, \citealt{baron2022}, \citealt{rohlinetal2022}, \citealt{baronetal2024}, \citealt{bilaschon2024}). 

Despite their growing relevance in empirical research, dynamic RDDs are methodologically understudied. This is especially true in relation to the challenge posed by the identification of economically interpretable long-term causal parameters. Because units are repeatedly exposed to treatment assignment and the outcome at any point in time may reflect the contribution of past and future treatment states, disentangling effects associated with sequential RD settings is inherently difficult. In recent work, \cite{hsushen2024} addresses this challenge in a potential outcomes framework that allows for flexible patterns of path dependence and heterogeneity in treatment effects. To identify the intertemporal effects of treatment exposure in a given period, they impose a selection-on-observables assumption that exploits future realizations of the running variable and, potentially, additional covariates.

In this paper, I consider a dynamic model in which potential outcomes at any point in time depend on past, contemporaneous, and future treatment states (\citealt{robins1986}). I subsequently propose an identification argument that does not incorporate external observables and formalizes a connection between dynamic RDDs and event-study designs---one that has been implicitly recognized in recent empirical work (\citealt{bilaschon2024}). Specifically, I reformulate two assumptions from the difference-in-differences literature---no anticipation and common trends---and show that they are sufficient to point identify interpretable causal parameters. The resulting, time-specific estimands can be expressed as sums of two standard RD outcome contrasts and can be aggregated, for interpretability and exposition, using a set of nonnegative weights that add to one. Finally, recognizing that the number of estimands grows exponentially with the number of time periods in the model, I reduce the dimensionality of the parameter space with a limited path independence assumption. This restriction takes advantage of the sparse and cyclical nature of treatment assignment that is typically true of empirical applications of dynamic RDDs. A more detailed comparison between the approach developed in this paper and that of \cite{hsushen2024} is provided in Section~\ref{sec_comparison_hsushen}.

For nonparametric estimation and statistical inference, I use local polynomial methods that are widely applied in empirical research. Specifically, I estimate time-specific parameters using local linear regression and aggregate them with convex sample weights (as in \citealt{callawaysantanna2021}). In the process, I incorporate standard bias correction techniques to address the distortion that arises when estimating cutoff-specific parameters with observations away from the cutoff. To quantify statistical uncertainty, I follow the method developed by \cite{cct2014} and construct nonparametric confidence intervals that account for the noise introduced by first-stage bias correction. Additionally, I derive bandwidth formulas that minimize the estimator's mean squared error, following standard optimality criteria (\citealt{ik2012}).

I implement the proposed method in the most common empirical application of dynamic RDDs: referenda through which school districts and other local jurisdictions authorize additional expenditures and bypass state-imposed fiscal policy constraints. I focus on Wisconsin, where such referenda are routinely held to finance both operational and capital spending (\citealt{baron2022}). The outcome of interest is housing prices in the years following referendum approval, a margin that has long been studied in public finance as a measure of how potential homebuyers value the tradeoff between improved public services and higher property taxes (see \citealt{rossyinger1999} for a review of the earlier literature).

The approach developed in this paper is similar in spirit to recently proposed solutions to extrapolate effects away from the threshold in static designs only leveraging features of the joint distribution of the outcome, the running variable, and one or more cutoffs (\citealt{donglewbel2015}, \citealt{bertanhaimbens2020}, \citealt{cattaneoetal2021}). I view this approach as offering several advantages. First, by construction, it is applicable to settings in which external information may not be available or fail to satisfy prerequisites for validity, such as observed characteristics co-determined by the treatment of interest. This concern may be especially salient in dynamic settings in which units are repeatedly subject to treatment assignment\footnote{Statisticians and econometricians have long cautioned against adjusting for post-treatment observed confounders. See, for example, \cite{rosenbaum1984}, \cite{pearl1995}, \cite{rosenbaum2002}, \cite{wooldridge2005}, \cite{mostlyharmlsess}, \cite{rubin2009}, \cite{wooldridge2010}, \cite{masteringmetrics}, \cite{imbensrubin2015}, \cite{gelman2021regression}.}. Second, my proposed approach is based on well-understood identification assumptions with testable suggestive implications. Third, the method is easy to implement because the resulting estimators can be expressed as sums of standard regression discontinuity contrasts.

{\sloppy
This paper contributes to three related strands of the econometric literature. First, it proposes a novel approach to point identify intertemporal treatment effects in dynamic RDDs. Second, it adds to the broad literature on the identification of economically interpretable dynamic effects under treatment effect heterogeneity (\citealt{dcdh2020}, \citealt{goodmanbacon2021}, \citealt{callawaysantanna2021}, \citealt{sunabraham2021}, \citealt{dcdh_intertemp2024}, \citealt{callawaybaconsantanna2024}, \citealt{dcdhvazquez2024}) and path dependence in treatment assignment (\citealt{heckmanhumphriesveramendi2016}). Third, it contributes to the growing literature on the causal interpretation of standard estimands in the presence of multiple treatments (\citealt{dcdh_multi2023}, \citealt{pgphullkolesar2022}) or multiple instrumental variables (\citealt{mogstadtorgowalters2021}, \citealt{magnetorgohole2025}) when treatment effects are allowed to be stochastic.
\par}

The remainder of the paper is organized as follows. In Section \ref{sec:background}, I introduce the main identification challenge and discuss previous approaches to address it. In Section \ref{sec_twoperiod}, I develop a simple two-period regression discontinuity design and illustrate the core of my identification strategy. In Section \ref{sec_multiperiod}, I generalize the model to a setting with multiple time periods. In Section \ref{sec_aggreg}, I propose an approach to aggregate time-specific estimands and reduce the dimensionality of the parameter space. In Section \ref{sec_estim}, I illustrate how to use standard local polynomial methods to estimate the target parameters and construct nonparametric confidence intervals around them. In Section \ref{sec_simulation}, I conduct a Monte Carlo simulation to confirm that my proposed approach correctly recovers the parameters of interest. In Section \ref{sec:application}, I apply my method to estimate the average effects of school district expenditure authorizations on housing prices in Wisconsin. Section \ref{sec_concl} concludes.

\section{Background}\label{sec:background}

In standard regression discontinuity designs, a treatment is assigned based on whether an observed running variable exceeds a known deterministic threshold. For example, \cite{lee2008} exploits U.S. House elections to estimate the average effect of a party’s candidate narrowly winning an election on the party’s subsequent victory in future elections for the same seat. Dynamic RDDs extend this framework by distinguishing between current and future treatment states, which depend on whether the running variable crosses the cutoff in each period.

In the context of \cite{lee2008}, the outcome of a House election in year $t$ determines political representation in both $t$ and $t+1$, allowing for the estimation of short-term treatment effects\footnote{This example assumes that House elections occur at the beginning of a calendar year and that outcomes are observed at the end of it.}. Estimating the average effect of winning an election in year $t$ on outcomes in years $t+2$ and beyond introduces an additional challenge. Because House elections occur every two years, a party that wins a seat in year $t$ may lose it in year $t+2$, and a party that loses in year $t$ may regain control. This dynamic turnover complicates the identification of interpretable long-term causal parameters. The key challenge lies in disentangling the long-term effects of the year-$t$ election outcome from the short-term effects of the year-$t+2$ election outcome. More broadly, the recurrence of elections introduces dynamic fuzziness, as future treatment states may not align with initial treatment assignments. Figure \ref{fig:dynfuzz} below provides a stylized representation of the main identification challenge in dynamic RDDs.

% Figure environment removed

\vspace{-2mm}

Since the seminal contribution by \cite{cfr2010}, dynamic regression discontinuity designs have been employed almost exclusively in the field of local public finance. Numerous empirical studies leverage school district referenda on property tax hikes to estimate the effects of increased school expenditures on outcomes such as housing prices and student test scores. Similar to the stylized example above, dynamic fuzziness arises because school districts hold repeated referenda to authorize additional spending. A ballot measure that is initially rejected may be resubmitted and later approved. As a result, a researcher might observe that a referendum was rejected at time $t$ but approved at time $t+1$. If a study uses the year-$t$ referendum outcome to estimate the long-term effects of extra spending at time $t$, labeling the district as untreated at time $t$ overlooks its transition into a treated state at time $t+1$, potentially conflating the effects of treatment received in successive periods.

Most empirical studies employing a dynamic regression discontinuity design adopt the so called ``one-step'' approach proposed by \cite{cfr2010}. This method relies on the following dynamic two-way fixed effects specification:
\begin{align}\label{eq:onestep}
	Y_{it} = \alpha_i + \beta_t + \sum_{s=0}^{\overline{s}} \left ( \theta_s D_{i,t-s} + \gamma_s Q_{i,t-s} + p_g \left ( \delta_s, R_{i,t-s} \right ) \right ) + U_{it}
\end{align}
where $D$ is a binary treatment indicator, $Q$ is an indicator for whether a referendum is held, and $p_g \left ( \cdot \right )$ represents a $g$th-degree polynomial of the approval vote share $R$. This specification is typically estimated without bandwidth restrictions, allowing researchers to flexibly control for both current and past treatment states, as well as for current and past realizations of the running variable (\citealt{darolia2013}, \citealt{hongzimmer2016}, \citealt{martorelletal2016}, \citealt{baronetal2024}).

More recently, empirical researchers have proposed modifications to the ``one-step'' approach introduced by \cite{cfr2010}. \cite{abottetal2020} and \cite{baron2022} extend the estimating equation in \eqref{eq:onestep} by including lead treatment indicators, allowing $s$ to take negative values. Similarly, \cite{bilaschon2024} incorporates leads but estimates the resulting linear regression in a ``stacked'' sample, where each component corresponds to a cohort-specific subsample. Here, a cohort is defined as a calendar year in which referenda occur\footnote{\cite{bilaschon2024} restricts each cohort to include only treated jurisdictions and untreated jurisdictions that held a referendum in the same year but did not approve any ballot measure in the subsequent ten years.}. The authors interact each regressor in specification \eqref{eq:onestep} with cohort indicators, effectively estimating the model for each cohort separately. Departing from \cite{cfr2010}, \cite{hsushen2024} develops a dynamic potential outcomes model that accounts for heterogeneity in treatment effects and path dependence in treatment assignments. To identify interpretable long-term effects, the authors impose a selection-on-observables assumption that exploits future approval vote shares and, possibly, additional covariates. More details on their framework are provided in the following section.

\section{A Two-Period Regression Discontinuity Design}\label{sec_twoperiod}

In this section, I present the core of my identification strategy within a simple two-period model. Sections \ref{sec_multiperiod} and \ref{sec_aggreg} extend this framework to accommodate an arbitrary number of time periods.

\subsection{Model Setup}

Consider a dynamic setting in which time periods are indexed by $t \in \left \{ 1, 2 \right \}$. Let $D_t \in \left \{ 0,1 \right \}$ denote a binary treatment that is assigned at the beginning of period $t$. The treatment is determined by whether the realization of an observed running variable $R_t \in \mathbb{R}$ exceeds a known deterministic constant $c \in \mathbb{R}$:
\begin{align}
	D_t \equiv \mathbb{I} \left [ R_t \geq c \right ]
\end{align}
Let $Y_t \in \mathbb{R}$ be an outcome that is observed at the end of period $t$. In the canonical static setting, potential outcomes depend on a single treatment state, and the observed outcome satisfies the switching equation $Y_t = \sum_{d \in \left \{ 0,1 \right \}} \mathbb{I} \left [ D_t = d \right ] Y_t \left ( d \right )$. In a two-period model with repeated treatment assignment, the potential outcome in period 1 may reflect the treatment state in period 2 due to anticipation and, symmetrically, the potential outcome in period 2 may be affected by the treatment state in period 1 due to path dependence. Thus, following \cite{robins1986}, it is natural to model potential outcomes as depending on the path of past, contemporaneous, and future treatment states at any point in time. Specifically, let the period-$t$ potential outcome be $Y_t \left ( d_1, d_2 \right )$, where $d_1, d_2 \in \left \{ 0,1 \right \}$. Without further assumptions, the evolution of treatment states is unrestricted. In fact, a unit may be repeatedly assigned the treatment, never receive it, or be exposed to it in only one period. As in \cite{hsushen2024}, I allow for path dependence in treatment assignment by modeling potential treatments as functions of the history of treatment states. Specifically, let $D_1$ and $D_2 \left ( d_1 \right )$ denote the first- and second-period potential treatments, respectively\footnote{A more flexible setting could allow for anticipation in treatment assignment by letting the potential treatment in period 1 depend on the treatment state in period 2, i.e., $D_1 \left ( d_2 \right )$. For simplicity, I abstract from this scenario and allow for anticipation only in potential outcomes.}. In the canonical application of dynamic RDDs to local referenda, it is useful to interpret potential treatments as characterizing jurisdiction ``types''. For example, a school district in which voters repeatedly reject a bond for school construction ($D_1 = 0$ and $D_2 \left ( 0 \right ) = D_2 \left ( 1 \right ) = 0$) is likely more fiscally conservative than one in which increased public expenditure is approved at the first round of elections ($D_1 = 1$) or at the second round ($D_1 = 0$ and $D_2 \left ( 0 \right ) = 1$). In this two-period model, the observed outcome in period $t$ is related to potential outcomes as
\begin{align}\label{potout}
	Y_t = \sum_{\left ( d_1, d_2 \right ) \in \left \{ 0,1 \right \}^2} \mathbb{I} \left [ D_1 = d_1, D_2 \left ( d_1 \right ) = d_2 \right ] Y_t \left ( d_1, d_2 \right )
\end{align}

\subsection{Target Parameters}

Having outlined features of the dynamic potential outcomes model, it is natural to define target parameters. As in static RDDs, all causal parameters of interest are local to the nonstochastic threshold above which the treatment is assigned (\citealt{hahnetal2001}). Without loss, the analysis will focus on target parameters implied by the first-period discontinuity. Letting $\tau \in \left \{ 0,1 \right \}$ denote the number of leads, consider a class of threshold-specific Average Treatment Effects (ATEs),
\begin{align}
	\mathrm{ATE}_{1,\tau} \left ( c, d_2, d_2' \right ) \equiv \E \left [ Y_{1+\tau} \left ( 1,d_2 \right ) - Y_{1+\tau} \left ( 0,d_2' \right ) | R_1 = c \right ]
\end{align}
where $d_2, d_2' \in \left \{ 0,1 \right \}$. Each of these average effects is constructed as the average contrast between a potential outcome that is treated in the first period and a potential outcome that is not. The choice of a second-period treatment state naturally gives rise to causal parameters with a different interpretation.

Since \cite{cfr2010}, the empirical public finance literature that studies the effects of increased public expenditure authorized by local referenda has focused on a class of Average Direct Treatment Effects that sets $d_2 = d_2' = 0$,
\begin{align}
	\mathrm{ADTE}_{1,\tau} \left ( c \right ) \equiv \E \left [ Y_{1+\tau} \left ( 1,0 \right ) - Y_{1+\tau} \left ( 0,0 \right ) | R_1 = c \right ]
\end{align}
Hereafter, I will refer to $\mathrm{ADTE}_{1,0} \left ( c \right )$ and $\mathrm{ADTE}_{1,1} \left ( c \right )$ as the instantaneous and cumulative Average Direct Treatment Effects, respectively. These target parameters can be interpreted as tracing out the impulse response of the outcome to the first-period discontinuity. Economically, they correspond to the thought experiment of measuring the cutoff-specific average effect of first-period treatment assignment in the counterfactual scenario in which exposure to the treatment is set to zero thereafter.

\subsection{What Do Standard RD Estimands Identify?}

In what follows, I will show that standard RD estimands do not, in general, identify ADTEs and will subsequently specify assumptions to achieve this goal. First, the (sharp) RD estimand implied by the first-period discontinuity is defined as
\begin{align}
	\mathrm{RD}_{t} \left ( c \right ) \equiv \lim_{r \downarrow c} \E \left [ Y_{t} | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_{t} | R_1 = r \right ]
\end{align}
\cite{hahnetal2001} shows that continuity in average potential outcomes at the cutoff is sufficient to point identify the Average Treatment Effect at the threshold. It is natural to generalize this assumption to a dynamic setting.
\begin{ass}[Continuity at the Cutoff]\label{ass_cont0} For any $t \in \left \{ 1,2 \right \}$ and $d_1, d_2 \in \left \{ 0,1 \right \}$,
	\begin{align*}
		\E \left [ D_{2} \left ( d_1 \right ) | R_1 = r \right ] \quad \text{ and } \quad \E \left [ Y_{t} \left ( d_1, d_2 \right )  | R_1 = r, D_{2} \left ( d_1 \right ) \right ]
	\end{align*}
	are continuous at $r=c$.
\end{ass}
Assumption \ref{ass_cont0} generalizes the restriction that agents do not sort nonrandomly around the threshold. First, the probability of potential treatment assignment in the second period evolves smoothly at the first-period cutoff. Importantly, this statement does \textit{not} imply that the probability of observing $D_2 = 1$ is unaffected by the first-period treatment state. Instead, it restricts the probability of any jurisdiction ``type'' -- as pinned down by the potential treatment $D_2 \left ( d_1 \right )$ -- not to vary abruptly at the first-period cutoff. Analogously, Assumption \ref{ass_cont0} rules out a scenario in which agents systematically choose either side of the first-period cutoff based on observed and unobserved determinants of the outcome, within each jurisdiction ``type'' and for each possible path of treatment states. Under this assumption, it is immediate to derive the following identification argument.

\begin{prop}[Standard RD Estimand]\label{prop_rd}
	Suppose that Assumption \ref{ass_cont0} holds. Then
	\begin{align*}
		\mathrm{RD}_t \left ( c \right ) & = \sum_{d_2 \in \left \{ 0,1 \right \}} \E \left [ Y_t \left ( 1,d_2 \right )  | R_1 = c, D_2 \left ( 1 \right ) = d_2 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = d_2 | R_1 = c \right ) \\
		& - \sum_{d_2 \in \left \{ 0,1 \right \}} \E \left [ Y_t \left ( 0,d_2 \right )  | R_1 = c, D_2 \left ( 0 \right ) = d_2 \right ] \times \Prob \left ( D_2 \left ( 0 \right ) = d_2 | R_1 = c \right )
	\end{align*}
\end{prop}
Without further assumptions, the sharp regression discontinuity estimand identifies the contrast between two weighted sums of potential outcomes over second-period treatment states, where weights are nonnegative and add up to one. As discussed in \cite{cfr2010} and similarly in \cite{heckmanhumphriesveramendi2016}, this target parameter may be interpreted as the ``intent-to-treat'' implied by the first-period discontinuity. Because treatment assignment exhibits path dependence and outcomes reflect past and future treatment states, the ITT is generally hard to interpret. By way of exposition, consider the extreme scenario in which $\Prob \left ( D_2 \left ( 1 \right ) = 1 | R_1 = c \right ) = 1$ and $\Prob \left ( D_2 \left ( 0 \right ) = 0 | R_1 = c \right ) = 1$. In this case, $\mathrm{RD}_{2} \left ( c \right )$ identifies $\E \left [ Y_2 \left ( 1,1 \right ) - Y_2 \left ( 0,0 \right )  | R_1 = c \right ]$. However, the treatment state in period 1 perfectly predicts the treatment state in period 2, making it infeasible to disentangle the cumulative effect of the first-period discontinuity, namely $\mathrm{ADTE}_{1,1} \left ( c \right )$, from the instantaneous effect of the second-period discontinuity. Formally,
\begin{align}
	\mathrm{RD}_2 \left ( c \right ) = \underbrace{\E \left [ Y_2 \left ( 1,1 \right ) - Y_2 \left ( 1,0 \right )  | R_1 = c \right ]}_{\text{instantaneous effect of $D_2$}} + \underbrace{\E \left [ Y_2 \left ( 1,0 \right ) - Y_2 \left ( 0,0 \right )  | R_1 = c \right ]}_{\text{cumulative effect of $D_1$}}
\end{align}

Estimating the ``intent-to-treat'' from Proposition \ref{prop_rd} may suffice should a researcher be interested in measuring the reduced-form (or ``total'') effect of a policy initiated by the first-period discontinuity, regardless of the subsequent path of treatment states. However, the composition of a long-term causal parameter is intrinsically of value in several empirical settings. For instance, does increased funding for local schools lead to sustained and permanent gains in student achievement or the effect of extra expenditure decays over time? To answer this and similar policy-relevant questions, a researcher may prefer to impose additional restrictions and estimate Average Direct Treatment Effects that trace out the impulse response of an outcome to the first-period round of treatment assignment.
 
Motivated by this negative result, in the next two sections I specify additional sufficient restrictions to identify the instantaneous and cumulative Average Direct Treatment Effects implied by the first-period discontinuity.

\subsection{Identification of the Instantaneous ADTE}

The goal of this section is to provide an identification argument for the instantaneous Average Direct Treatment Effect, $\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c \right ] - \E \left [ Y_1 \left ( 0,0 \right ) | R_1 = c \right ]$.
By an application of the Law of Iterated Expectations, the first term can be expressed as
\begin{align}\label{apte10}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c \right ] & = \overbrace{\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ]}^{\text{identified}} \times \overbrace{\Prob \left ( D_2 \left ( 1 \right ) = 0 | R_1 = c \right )}^{\text{identified}} \nonumber \\
	& + \underbrace{\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ]}_{\text{counterfactual}} \times \underbrace{\Prob \left ( D_2 \left ( 1 \right ) = 1 | R_1 = c \right )}_{\text{identified}}
\end{align}
Notice that both probabilities are identified under Assumption \ref{ass_cont0}. In fact, for $d_2 \in \left \{ 0,1 \right \}$,
\begin{align}\label{probs}
	\Prob \left ( D_2 \left ( 1 \right ) = d_2 | R_1 = c \right ) = \lim_{r \downarrow c} \Prob \left ( D_{2} = d_2 | R_1 = r \right )
\end{align}
Similarly, the first conditional expectation in equation \eqref{apte10} is identified as
\begin{align}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ] = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 0 \right ]
\end{align}
On the other hand, $\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ]$ is counterfactual. Intuitively, this term captures the average treated-untreated potential outcome among units who would receive the treatment in $t=2$ after receiving it in $t=1$. To identify this counterfactual expectation, it is sufficient to assume that, on average and within bins implied by potential treatment states, the first-period outcome is independent of the second-period treatment state.
\begin{ass}[No Anticipation] \label{ass_noant} For any $d_1, d_2 \in \left \{ 0,1 \right \}$,
	\begin{align*}
		\E \left [ Y_{1} \left ( d_1, 0 \right ) | R_1 = c, D_{2} \left ( d_1 \right ) = d_2 \right ] = \E \left [ Y_{1} \left ( d_1, 1 \right ) | R_1 = c, D_{2} \left ( d_1 \right ) = d_2 \right ]
	\end{align*}
\end{ass}
Assumption \ref{ass_noant} is a cutoff-specific version of the canonical no anticipation assumption required to point identify the Average Treatment Effect on the Treated in difference-in-differences designs (\citealt{abbringvanderberg2003}). It restricts current potential outcomes not to reflect (expectations over) future treatment states. While this statement may appear to rule out the applicability of this argument to forward-looking outcome variables, it should be noted that the lack of anticipatory behavior must hold only at the first-period cutoff. In the application of dynamic RDDs to the effect of increased school spending on student outcomes, Assumption \ref{ass_noant} entails that average test scores after a narrowly rejected bond do not reflect the possibility that a bond proposition may be approved or rejected in a subsequent referendum. Under Assumptions \ref{ass_cont0} and \ref{ass_noant}, the counterfactual conditional mean is identified as
\begin{align}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ] = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 1 \right ]
\end{align}
Thus, the left-hand side of $\textsc{ADTE}_{1,0} (c)$ can be expressed as a function of the following observables:
\begin{align}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c \right ] & = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 0 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2}=0 | R_1 = r \right ) \nonumber \\
	& + \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 1 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2}=1 | R_1 = r \right ) \nonumber \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ]
\end{align}
where the last equality follows from an application of the Law of Iterated Expectations. By a symmetric argument, the right-hand side of $\textsc{ADTE}_{1,0} (c)$ is point identified, too. Combining results yields the following proposition.

\begin{prop}[Identification of the Instantaneous ADTE]\label{prop_apte0}
	Suppose that Assumptions \ref{ass_cont0} and \ref{ass_noant} hold. Then
	\begin{align*}
		\textsc{ADTE}_{1,0} (c) = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ] \equiv \mathrm{RD}_1 \left ( c \right )
	\end{align*}
\end{prop}
This result follows from considering Proposition \ref{prop_rd} in the case in which Assumption \ref{ass_noant} holds. Intuitively, the lack of anticipation implies that the second-period potential treatment state is irrelevant for the first-period outcome. Setting $d_2 = 0$ yields the identification result in Proposition \ref{prop_apte0}. To summarize, continuity and no anticipation are sufficient for the static regression discontinuity estimand to identify an interpretable causal parameter in this model.

\subsection{Identification of the Cumulative ADTE}

Consider, instead, the more interesting goal of identifying the cumulative Average Direct Treatment Effect, $\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c \right ] - \E \left [ Y_2 \left ( 0,0 \right ) | R_1 = c \right ]$. Mirroring the argument in the previous section, an application of the Law of Iterated Expectations implies that the first term can be expressed as
\begin{align}\label{apte11}
	\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c \right ] & = \overbrace{\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ]}^{\text{identified}} \times \overbrace{\Prob \left ( D_2 \left ( 1 \right ) = 0 | R_1 = c \right )}^{\text{identified}} \nonumber \\
	& + \underbrace{\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ]}_{\text{counterfactual}} \times \underbrace{\Prob \left ( D_2 \left ( 1 \right ) = 1 | R_1 = c \right )}_{\text{identified}}
\end{align}
As shown in equation \eqref{probs}, both conditional probabilities are identified under Assumption \ref{ass_cont0}. In addition, the first conditional expectation in equation \eqref{apte11} is identified as
\begin{align}
	\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ] = \lim_{r \downarrow c} \E \left [ Y_2 | R_1 = r, D_2 = 0 \right ]
\end{align}
On the other hand, $\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ]$ is counterfactual because $Y_2 \left ( 1,0 \right )$ is unobserved among units who would sort into the treated arm in $t=2$ after being treated in $t=1$. Differently from the previous section, Assumption \ref{ass_noant} alone is not sufficient to identify the counterfactual mean because the outcome is measured in $t=2$. However, point identification of the target parameter may be achieved by also assuming that the expected growth in untreated-at-2 potential outcomes is independent of the potential second-period treatment state at the cutoff.
\begin{ass}[Common Trends] \label{ass_ct0} For $d_1 \in \left \{ 0,1 \right \}$,
	\begin{align*}
		\E \left [ \Delta_{1,2} \left ( d_1, 0 \right ) | R_1 = c, D_{2} \left ( d_1 \right ) = 0 \right ] = \E \left [ \Delta_{1,2} \left ( d_1, 0 \right ) | R_1 = c, D_{2} \left ( d_1 \right ) = 1 \right ]
	\end{align*}
	where $\Delta_{1,2} \left ( d_1, 0 \right ) \equiv Y_{2} \left ( d_1,0 \right ) - Y_1 \left ( d_1, 0 \right )$.
\end{ass}
Assumption \ref{ass_ct0} is a cutoff-specific version of the common trends assumption required to point identify the Average Treatment Effect on the Treated in difference-in-differences designs (\citealt{abadie2005}). As in that setting, groups (``treatment'' and ``control'') are implied by paths of potential treatment states. Differently from that setting, the treatment is available also in the first period, implying that there are four, not two, groups or ``types''. As a consequence, the parallel trends restriction also applies to units who are potentially treated in the first period. In the classic application of dynamic RDDs to local public finance, Assumption \ref{ass_ct0} entails that, at the cutoff implied by the first-period referendum, the expected growth in potential test scores absent a second referendum approval is independent of a jurisdiction's ``type'', i.e., it is independent of whether a jurisdiction would approve or reject a school bond referendum in the second period. Under Assumptions \ref{ass_cont0}, \ref{ass_noant}, and \ref{ass_ct0}, the counterfactual conditional mean is identified as

\begingroup
\small
\begin{align}
	\E \left [ Y_{2} \left ( 1,0 \right ) | R_1 = c, D_{2} \left ( 1 \right )=1 \right ] = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=1 \right ] + \lim_{r \downarrow c} \E \left [ Y_{2} - Y_1 | R_1 = r, D_{2}=0 \right ]
\end{align}
\endgroup
As in the canonical difference-in-differences design, the counterfactual term is identified as the sum of a ``level'' and a ``growth'' term. In this case, the former measures the average observed first-period outcome among units who later sort into the treated arm. The latter captures the average observed outcome growth among units who sort into the untreated arm in the second period. Thus, the left-hand side of $\mathrm{ADTE}_{1,1} \left ( c \right )$ can be expressed as a function of the following observables:
\begin{align}
	\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c \right ] & = \lim_{r \downarrow c} \E \left [ Y_2 | R_1 = r, D_2 = 0 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2}=0 | R_1 = r \right ) \nonumber \\
	& + \left ( \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=1 \right ] + \lim_{r \downarrow c} \E \left [ Y_{2} - Y_1 | R_1 = r, D_{2}=0 \right ] \right ) \nonumber \\
	& \times \lim_{r \downarrow c} \Prob \left ( D_{2}=1 | R_1 = r \right )
\end{align}
By a symmetric argument, the right-hand side of $\textsc{ADTE}_{1,1} (c)$ is point identified, too. Combining results yields the following proposition.

\begin{prop}[Identification of the Cumulative ADTE]\label{prop_apte1}
	Suppose that Assumptions \ref{ass_cont0}, \ref{ass_noant}, and \ref{ass_ct0} hold. Then
	\begin{align*}
		\textsc{ADTE}_{1,1} (c) & = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ] \\
		& + \lim_{r \downarrow c} \E \left [ Y_2 - Y_1 | R_1 = r, D_2 =0 \right ] - \lim_{r \uparrow c} \E \left [ Y_2 - Y_1 | R_1 = r, D_2 =0 \right ]
	\end{align*}
\end{prop}
This proposition entails that an interpretable long-term causal parameter determined by the first-period discontinuity is identified by the sum of two standard sharp Regression Discontinuity estimands, namely the RD estimand that uses $Y_1$ as an outcome and the RD estimand that uses $Y_2 - Y_1$ as an outcome in the subpopulation implied by $D_2 = 0$.

\subsection{Relation to \cite{hsushen2024}}\label{sec_comparison_hsushen}

The fundamental difference between this paper and \cite{hsushen2024} lies in the assumption used to identify the counterfactual expectation $\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ]$. \cite{hsushen2024} introduces a potential indicator $Q_2 \left ( d_1 \right )$, which denotes whether a round of treatment assignment would occur in the second period if the first-period treatment state were $d_1 \in \left \{ 0,1 \right \}$. When $Q_2 \left ( d_1 \right ) = 1$, a corresponding potential second-period running variable $R_2 \left ( d_1 \right )$ is defined. Their key identifying assumption states that, if a second-period treatment assignment occurs, the potential outcome $Y_2 \left ( 1,0 \right )$ is mean independent of $R_2 \left ( d_1 \right )$ in a neighborhood of the first-period cutoff. This condition is further generalized to incorporate covariates $X$, such that, for any $r \in \supp \left ( R_2 \right )$,
\begin{align}
	& \E \left [ Y_2 \left ( d_1 ,0 \right ) | R_1 = c, Q_2 \left ( d_1 \right ) = 1, X = x, R_2 \left ( d_1 \right ) = r \right ] \notag \\
	& = \E \left [ Y_2 \left ( d_1 ,0 \right ) | R_1 = c, Q_2 \left ( d_1 \right ) = 1, X = x \right ]
\end{align}
This assumption has the appealing feature of restricting comparisons to units for which treatment assignment occurs in the second period as well. However, it may be vulnerable to violations if unobserved determinants of the second-period outcome also influence the second-period vote share. For instance, in the canonical application examining the effect of school expenditure referenda on student test scores, unobserved parental valuation of education may drive both support for a second-round referendum and student performance—e.g., because more involved parents are more likely to vote in favor of school funding (\citealt{poterba1997}) and also assist their children academically (\citealt{guryanhurstkearney2008}). Notably, such a violation can occur even if the unobservable is time-invariant and the first-round approval margin is idiosyncratically close to the cutoff.

Another distinction between the identification strategy developed in this paper and that in \cite{hsushen2024} pertains to the role of anticipation in the potential outcomes framework. The dynamic model presented here allows potential outcomes at any given time to depend on past, contemporaneous, and future treatment states. In a setting with two periods, this implies that potential outcomes in the first period are denoted as $Y_1 \left ( d_1, d_2 \right )$, while those in the second period are given by $Y_2 \left ( d_1, d_2 \right )$. By contrast, the setup considered by \cite{hsushen2024} restricts the sequence of treatment states to end in the period in which the outcome is realized, with first-period outcomes modeled as $Y_1 \left ( d_1 \right )$ and second-period outcomes as $Y_2 \left ( d_1, d_2 \right )$. In empirical applications where the outcome of interest may respond to anticipated future treatments---such as settings involving housing prices or other forward-looking variables commonly analyzed in public finance (e.g., \citealt{cfr2010} and \citealt{bilaschon2024})---it may be useful to explicitly state the role of anticipation in the underlying model.

Finally, the approach developed in this paper is related in spirit to event-study-type specifications commonly employed by empirical researchers in dynamic RDD applications. The inclusion of leads in the estimating equation \eqref{eq:onestep} by \cite{abottetal2020} and \cite{baron2022}, together with the cohort-by-cohort identification strategy based on ``clean controls'' proposed by \cite{bilaschon2024}, suggests that a conceptual link between dynamic RDDs and difference-in-differences designs has already been implicitly recognized in the literature. This paper formalizes that connection.


\section{Generalization to Multiple Leads}\label{sec_multiperiod}

It is natural to generalize previous arguments to identify long-term causal parameters in a dynamic potential outcomes model with more than two periods. In this section, I extend the setup and identifying assumptions of the two-period model to a setting with an arbitrary number of time periods indexed by $t \in \left \{ 1, 2, \dots, \overline{t} \right \}$.

First, potential outcomes at any point in time reflect the path of past, contemporaneous, and future treatment states. Specifically, for $\left \{ d_j \right \}_{j=1}^{\overline{t}}$ with $d_j \in \left \{ 0,1 \right \}$ for all $j$, the period-$t$ potential outcome is denoted as $Y_t \left ( d_1, \dots, d_{\overline{t}} \right )$. Second, to allow for path dependence in treatment assignment, the period-$t$ potential treatment is modeled as $D_t \left ( d_1, \dots, d_{t-1} \right )$. As a generalization of equation \eqref{potout}, the relationship between potential and observed outcomes can thus be characterized as
\begin{align}\label{potout_gen}
	Y_t = \sum_{\left ( d_1, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}}} \mathbb{I} \left [ D_1 = d_1, D_2 \left ( d_1 \right ) = d_2, \dots, D_{\overline{t}} \left ( d_1, \dots, d_{\overline{t}-1} \right ) = d_{\overline{t}} \right ] Y_t \left (  d_1, \dots, d_{\overline{t}} \right )
\end{align}
To keep notation concise, let $P: \left \{ 0,1 \right \}^{\overline{t}} \to \left \{ 0,1 \right \}$ be a function mapping a path of potential treatment states to an indicator that takes the value one if a unit's treatment-taking behavior for a given $d_1 \in \left \{ 0,1 \right \}$ is described by that path, i.e.,
\begin{align}
	P \left ( d_1, \dots, d_{\overline{t}} \right ) = \mathbb{I} \left [ D_2 \left ( d_1 \right ) = d_2, D_3 \left ( d_1, d_2 \right ) = d_3, \dots, D_{\overline{t}} \left ( d_1, \dots, d_{\overline{t}-1} \right ) = d_{\overline{t}} \right ]
\end{align} 
Therefore, a compact formulation of equation \eqref{potout_gen} is
\begin{align}\label{potout_gen2}
	Y_t = \sum_{\left ( d_1, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}}} \mathbb{I} \left [ D_1 = d_1 \right ] P \left ( d_1, \dots, d_{\overline{t}} \right ) Y_t \left (  d_1, \dots, d_{\overline{t}} \right )
\end{align}
The target parameter of interest is a class of Average Direct Treatment Effects implied by the first-period discontinuity. Letting $0_{\overline{t}-1}$ denote a vector of $\overline{t}-1$ zeros, the $\tau$-period-ahead ADTE at the first-period cutoff is defined as
\begin{align}\label{class_apte_mult}
	\mathrm{ADTE}_{1,\tau} \left ( c \right ) \equiv \E \left [ Y_{1+\tau} \left ( 1, 0_{\overline{t}-1} \right ) - Y_{1+\tau} \left ( 0, 0_{\overline{t}-1} \right ) | R_1 = c \right ]
\end{align}
It is convenient to interpret this class of target parameters as tracing out the impulse response function of the outcome $Y$ in a dynamic model in which the initial ``shock'' is determined by the threshold-crossing rule $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. Because treatment assignment exhibits path dependence, the sequence of states $0_{\overline{t}-1}$ may not be observed. As a matter of fact, it is counterfactual in most empirical applications. For instance, local jurisdictions in which voters reject a referendum for increased public spending may submit the same proposition again and have it approved in a subsequent round of elections. In this scenario, a standard regression discontinuity estimand will fail to identify an ADTE or a causal parameter with a clear economic interpretation. To formalize this argument, the following assumption generalizes the continuity restriction naturally embedded in the standard RDD model.

\begin{ass}[Continuity at the Cutoff]\label{ass_cont1} For any $t \in \left \{ 1, \dots, \overline{t} \right \}$, any $\left \{ d_j \right \}_{j=1}^{\overline{t}}$ with $d_j \in \left \{ 0,1 \right \}$ for all $j$, and any $\left \{ d'_j \right \}_{j=2}^{\overline{t}}$ with $d'_j \in \left \{ 0,1 \right \}$ for all $j$,
	\begin{align*}
		\E \left [ P \left ( d_1, \dots, d_{\overline{t}} \right ) | R_1 = r \right ] \quad \text{ and } \quad \E \left [ Y_{t} \left ( d_1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, P \left ( d_1, d'_2, \dots, d'_{\overline{t}} \right ) \right ]
	\end{align*}
	are continuous at $r=c$.
\end{ass}
Assumption \ref{ass_cont1} generalizes Assumption \ref{ass_cont0} to a model with an arbitrary number of time periods. Intuitively, restricting $\E \left [ P \left ( d_1, \dots, d_{\overline{t}} \right ) | R_1 = r \right ]$ to be continuous at the first-period cutoff entails that agents must not systematically sort around the threshold to make one treatment path more likely than another. Similarly, the continuity of $\E \big [ Y_{t} \left ( d_1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, P \left ( d_1, d'_2, \dots, d'_{\overline{t}} \right ) \big ]$ implies that observed and unobserved determinants of the outcome evolve smoothly at the cutoff, within each possible ``type'' defined by a path of potential treatment states. Under Assumption \ref{ass_cont1} and without further restrictions, it is immediate to show that a naive regression discontinuity estimand identifies a causal parameter with an undesirable interpretation.

\begin{prop}[Standard RD Estimand]\label{prop_rd1}
	Suppose that Assumption \ref{ass_cont1} holds. Then the estimand $\mathrm{RD}_t \left ( c \right ) \equiv \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_t | R_1 = r \right ]$ identifies
	
	\begin{small}
	\begin{align*}
		& \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}-1}} \E \left [ Y_t \left ( 1,d_2, \dots, d_{\overline{t}} \right )  | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \times \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ] \\
		- \ & \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}-1}} \E \left [ Y_t \left ( 0, d_2, \dots, d_{\overline{t}} \right )  | R_1 = c, P \left ( 0, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \times \E \left [ P \left ( 0, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]
	\end{align*}
	\end{small}
\end{prop}
Since $P \left ( d_1, \dots, d_{\overline{t}} \right )$ is a Bernoulli random variable, each $\E \left [ P \left ( d_1, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]$ is a conditional probability and $\sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}-1}} \E \left [ P \left ( d_1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]$ is equal to one for $d_1 \in \left \{ 0,1 \right \}$. Thus, the causal parameter identified by a standard RD estimand under Assumption \ref{ass_cont1} can be viewed as the contrast between two weighted sums of potential outcome means, where only the first-period treatment state is held fixed and each subsequent treatment path is weighted according to its probability at the focal threshold.

In the remainder of this section, I first provide an identification argument for the instantaneous ADTE and then consider long-term parameters. To identify the cutoff-specific effect of the first-period discontinuity on the outcome at the end of the same period, it is sufficient to assume that the outcome of interest does not reflect agents' knowledge of or expectations over future treatment states.

\begin{ass}[No Anticipation] \label{ass_noant1} For any $\left \{ d_j \right \}_{j=1}^{\overline{t}}$ with $d_j \in \left \{ 0,1 \right \}$ for all $j$, any $\left \{ d'_j \right \}_{j=2}^{\overline{t}}$ with $d'_j \in \left \{ 0,1 \right \}$ for all $j$, and any $\left \{ d''_j \right \}_{j=2}^{\overline{t}}$ with $d''_j \in \left \{ 0,1 \right \}$ for all $j$,
	\begin{equation*}
		\E \left [ Y_{1} \left ( d_1, d_2, \dots, d_{\overline{t}} \right ) - Y_{1} \left ( d_1, d'_2, \dots, d'_{\overline{t}} \right ) \big | R_1 = c, P \left ( d_1, d''_{2}, \dots,  d''_{\overline{t}} \right ) \right ] = 0
	\end{equation*}
\end{ass}
Starting from Proposition \ref{prop_rd1}, it is easy to show that that the instantaneous ADTE is point identified under Assumption \ref{ass_noant1}. In fact, letting $t=1$, the no anticipation restriction entails that the treatment path subsequent to the first period can be equivalently replaced with a sequence of $t-1$ zeros. Then, repeated applications of the Law of Iterated Expectations and Assumption \ref{ass_cont1} yield the following proposition.

\begin{prop}[Identification of the Instantaneous ADTE]\label{prop_apte0_gen}
	Suppose that Assumptions \eqref{ass_cont1} and \eqref{ass_noant1} hold. Then
	\begin{align*}
		\textsc{ADTE}_{1,0} (c) = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ] \equiv \mathrm{RD}_1 \left ( c \right )
	\end{align*}
\end{prop}
This result generalizes Proposition \ref{prop_apte0} to a dynamic potential outcomes model with an arbitrary number of periods. In words, the instantaneous regression discontinuity estimand identifies an impulse response parameter provided that the outcome is not forward-looking at the first-period cutoff. However, by placing restrictions only on the conditional expectation of the first-period outcome, the no anticipation assumption is not sufficient to trace out the full impulse response function. To identify long-term Average Direct Treatment Effects, it is sufficient to generalize the common trends assumption introduced in the previous section.

\begin{ass}[Common Trends] \label{ass_ct1} For all $\left \{ d_j \right \}_{j=1}^{\overline{t}}$ with $d_j \in \left \{ 0,1 \right \}$,
	\begin{align*}
		& \E \left [ \Delta_{1,t} \left ( d_1, 0_{\overline{t}-1} \right ) | R_1 = c, P \left ( d_1, 0_{\overline{t}-1} \right ) \right ] = \E \left [ \Delta_{1,t} \left ( d_1, 0_{\overline{t}-1} \right ) | R_1 = c, P \left ( d_1, d_2, \dots, d_{\overline{t}} \right ) \right ]
	\end{align*}
with $\Delta_{1,t} \left ( d_1, 0_{\overline{t}-1} \right ) \equiv Y_{t} \left ( d_1, 0_{\overline{t}-1} \right ) - Y_1 \left ( d_1, 0_{\overline{t}-1} \right )$.
\end{ass}
For any initial potential treatment state $d_1$, $\Delta_{1,t} \left ( d_1, 0_{\overline{t}-1} \right )$ is a random variable that measures the level shift experienced by the never-treated potential outcome between the first and a subsequent period. Assumption \ref{ass_ct1} restricts the expectation of this random variable, conditional on $R_1 = c$, not to depend on the potential treatment path after $d_1$. In the canonical application of dynamic RDDs to the effect of local school bonds on student test scores, the path indicator $P \left ( d_1, d_2, \dots, d_{\overline{t}} \right )$ collapses the heterogeneity in jurisdiction ``types'' into a sequence of potential treatment states. Thus, Assumption \ref{ass_ct1} restricts the expected change in test scores after the first period, absent any subsequent referendum approval, not to depend on whether a jurisdiction is more or less likely to approve said referenda. This local ``parallel trends'' assumption, jointly with continuity and no anticipation, are sufficient to point identify all Average Direct Treatment Effects.

\begin{prop}[Identification of the Cumulative ADTE]\label{prop_apte1_gen}
	Suppose that Assumptions \ref{ass_cont1}, \ref{ass_noant1}, and \ref{ass_ct1} hold. Then, for $\tau \in \left \{ 1, \dots, \overline{t}-1 \right \}$,
	
	\begin{small}
	\begin{align*}
		\mathrm{ADTE}_{1,\tau} \left ( c \right ) & = \lim_{r \downarrow c} \E \left [ Y_{1} | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_{1} | R_1 = r \right ] \\
		& + \lim_{r \downarrow c} \E \left [ Y_{1+\tau} - Y_{1} \bigg | R_1 = r, \bigcap_{s=2}^{1+\tau} \left \{ D_{s} = 0 \right \} \right ] - \lim_{r \uparrow c} \E \left [ Y_{1+\tau} - Y_{1} \bigg | R_1 = r, \bigcap_{s=2}^{1+\tau} \left \{ D_{s} = 0 \right \} \right ]
	\end{align*}
	\end{small}
\end{prop}
Proposition \ref{prop_apte1_gen} states that continuity, no anticipation, and common trends are sufficient for any long-term Average Direct Treatment Effect to be point identified as the sum of two regression discontinuity estimands implied by the first-period cutoff. The first estimand measures the average outcome contrast at the cutoff immediately after the first round of treatment assignment. On the other hand, the second term compares the average outcome growth on the two sides of the cutoff among agents who are not exposed to the treatment forever after.

\section{Aggregation of Time-Specific Parameters}\label{sec_aggreg}

The identification arguments presented so far have focused on causal parameters implied by the first-period discontinuity. In practice, the focal round of treatment assignment may not occur at $t=1$. Moreover, agents may be exposed to multiple rounds of treatment assignment at different points in time.

To allow for the focal discontinuity to occur in an arbitrary period $t$, it is convenient to introduce a random variable $H_{t-1} \in \left \{ 0,1 \right \}^{t-1}$ that summarizes the history of treatment states up to and excluding period $t$. Without further restrictions, past treatment states may affect the probability distribution of potential outcomes when the treatment is assigned. Thus, for a causal parameter to be interpretable, it must contrast agents who have been exposed to an identical path of treatment states. Letting $t$ denote the focal discontinuity period, the Average Direct Treatment Effect at the period-$t$ cutoff subsequent to the treatment history $h_{t-1} \in \supp \left ( H_{t-1} \right )$ is defined as
\begin{align}
	\mathrm{ADTE}_{t,\tau} \left ( h_{t-1}, c \right ) \equiv \E \left [ Y_{t+\tau} \left ( h_{t-1}, 1, 0_{\overline{t}-t} \right ) - Y_{t+\tau} \left ( h_{t-1}, 0, 0_{\overline{t}-t} \right ) | H_{t-1} = h_{t-1}, R_t = c \right ]
\end{align}
for any $\tau \in \left \{ 0, 1, \dots, \overline{t} - t \right \}$\footnote{It is not necessary for the conditioning set to include the event $H_{t-1} = h_{t-1}$. However, an empirical researcher is unlikely to be interested in the average effect of a treatment subsequent to a treatment history $h_{t-1}$ in the subpopulation implied by a \textit{different} treatment history $h'_{t-1}$. For practical purposes, it is natural for \textit{one} treatment history to both be an argument of potential outcomes and belong to the conditioning set.}. Importantly, potential outcomes differ solely by the period-$t$ treatment state, implying that this class of impulse response parameters generalizes equation \eqref{class_apte_mult}.

Because each $\mathrm{ADTE}_{t,\tau} \left ( h_{t-1}, c \right )$ is specific to a history of treatment states, focal period of treatment assignment, and number of leads, it is natural to think of $\mathrm{ADTE}_{t,\tau} \left ( h_{t-1}, c \right )$ as a building block to construct aggregate policy-relevant parameters. This interpretation is similar in spirit to the one put forward by \cite{callawaysantanna2021}, who proposes several approaches to aggregate cohort- and time-specific causal parameters in the context of difference-in-differences designs with staggered adoption of an absorbing treatment. Analogously, \cite{bojinovetal2021} aggregates unit-time-specific causal parameters in sequentially randomized experiments. In dynamic RDDs, a natural first step is to aggregate target parameters over histories of treatment states, for given focal period of treatment assignment and number of leads. The resulting parameter is defined as
\begin{align}
	\mathrm{ADTE}_{t,\tau} \left ( c \right ) \equiv \sum_{h_{t-1} \in \supp \left ( H_{t-1} \right )} \mathrm{ADTE}_{t,\tau} \left ( h_{t-1}, c \right ) \times \Prob \left ( H_{t-1} = h_{t-1} \right )
\end{align}
$\mathrm{ADTE}_{t,\tau} \left ( c \right )$ integrates Average Direct Treatment Effects over the probability distribution of histories up to the focal round of treatment assignment. It can thus be interpreted as the average impulse response of the outcome to the period-$t$ discontinuity, $\tau$ periods after the initial shock.

In addition, an empirical researcher may be interested in aggregating time-specific parameters into a single summary measure of the $\tau$-period-ahead effect of the treatment. In this case, assuming a balanced panel, each term of the aggregate impulse response function is defined as
\begin{align}\label{eq:aggr_reltime}
	\mathrm{ADTE}_{\tau} \left ( c \right ) \equiv \frac{1}{\overline{t}-\tau} \sum_{t=1}^{\overline{t}-\tau} \mathrm{ADTE}_{t,\tau} \left ( c \right )
\end{align}

\subsection{Dimension Reduction via Limited Path Independence}\label{sec:markov}

Assumptions \ref{ass_cont1}, \ref{ass_noant1}, and \ref{ass_ct1} are easily generalized to achieve the point identification of each possible $\mathrm{ADTE}_{t,\tau} \left ( h_{t-1}, c \right )$. In observational settings, however, it is generally infeasible to estimate one target parameter for each possible treatment history, focal calendar time, and number of leads. For instance, in the canonical application of dynamic RDDs to the effects of local referenda, units are infrequently subject to treatment assignment because jurisdictions typically hold new elections only once funds approved by a previously approved proposition are close to expiration. As a consequence, only a limited number of local governments participates in treatment assignment every year and any given jurisdiction does so in a cyclical fashion. In addition, a narrowly rejected referendum is generally submitted to voters again only once or twice briefly after the initial attempt, implying that the identification challenge posed by repeated treatment assignment is practically salient only in a narrow time window. To summarize, in the local public finance application of dynamic RDDs, treatment assignment is sparse and cyclical.

Motivated by this fact, it is natural to impose additional restrictions to limit the number of target parameters at stake and reduce the dimensionality of the problem. In a similar fashion to \cite{hsushen2024}, I assume that all causal parameters of interest share a weak form of limited path independence. Specifically, for any given focal round of treatment assignment, I restrict the Average Direct Treatment Effect not to depend on the history of treatment states provided that units are not exposed to the treatment for a specific number of periods prior to the focal round of treatment assignment.

\begin{ass}[Limited Path Independence] \label{ass_mark} Let $k \in \mathbb{N}_{+}$ be a deterministic constant. For any $h_{t-k-1}, h'_{t-k-1} \in \supp \left ( H_{t-k-1} \right )$ such that $h_{t-1} = \left ( h_{t-k-1},0_k \right )$ and $h'_{t-1} = \left ( h'_{t-k-1},0_k \right )$,
	\begin{align*}
		& \E \left [ Y_{t} \left ( h_{t-1},1,0_{\overline{t}-t} \right ) - Y_{t} \left ( h_{t-1},0,0_{\overline{t}-t} \right ) | H_{t-1} = h_{t-1}, R_t = c \right ] \\
		= \ & \E \left [ Y_{t} \left ( h'_{t-1},1,0_{\overline{t}-t} \right ) - Y_{t} \left ( h'_{t-1},0,0_{\overline{t}-t} \right ) | H_{t-1} = h'_{t-1}, R_t = c \right ]
	\end{align*}
	for all $t > k$. Thus, $\mathrm{ADTE}_{t,\tau} \left ( h_{t-1}, c \right ) = \mathrm{ADTE}_{t,\tau} \left ( h'_{t-1}, c \right )$ for all $\tau \in \left \{ 0,1, \dots, \overline{t} - t \right \}$.
\end{ass}
The constant $k$ is to be determined by the researcher depending on the empirical application\footnote{In a similar spirit, \cite{callawaysantanna2021} considers a limited treatment anticipation assumption, based on which the researcher must specify the number of time periods the outcome variable may reflect agents' knowledge of (or expectations over) future exposure to the treatment.}. Clearly, the choice of a larger $k$ entails a more stringent requirement for the limited path independence assumption to be satisfied. On the other hand, a smaller $k$ restricts ADTEs not to be determined in time periods further antecedent to the focal round of treatment assignment.

The practical implication of this Markov-type assumption is that, for the purpose of estimating a target parameter, units that share a common ``recent'' past can be pooled, thereby improving statistical inference. Another consequence of Assumption \ref{ass_mark} is that the plausibility of the local common trends restriction, namely Assumption \ref{ass_ct1}, may be tested as in difference-in-differences and event-study designs. Because all units are untreated for $k$ periods before the focal calendar time period, a researcher may test whether the conditional mean of the outcome exhibits pre-trends at the focal cutoff.

For ease of exposition, consider a four-period model ($\overline{t}=4$) in which all units are untreated until $t=3$, when a round of treatment assignment occurs. For $\left ( t,\tau \right ) = \left ( 3,1 \right )$, let the target parameter be
\begin{align}
	\mathrm{ADTE}_{t,\tau} \left ( 0_2, c \right ) \equiv \E \left [ Y_{t+\tau} \left ( 0_2,1,0 \right ) - Y_{t+\tau} \left ( 0_2,0,0 \right ) | H_2 = 0_2, R_3 = c \right ]
\end{align}
This is the cutoff-specific, one-period-ahead average effect of the treatment assigned in $t=3$, conditional on a never-treated path in the first two periods. Because, by construction, $D_1 = D_2 = 0$, the history of treatment states $H_2 = 0_2$ is observed. To identify $\mathrm{ADTE}_{3,1} \left ( 0_2, c \right )$, Assumption \ref{ass_ct1} restricts $\E \left [ Y_4 \left ( 0_2,d_1,0 \right ) - Y_3 \left ( 0_2,d_1,0 \right ) | H_2 = 0_2, R_3 = c, P \left ( 0_2, d_1, 0 \right ) = 1 \right ]$ to be equal to $\E \left [ Y_4 \left ( 0_2,d_1,0 \right ) - Y_3 \left ( 0_2,d_1,0 \right ) | H_2 = 0_2, R_3 = c, P \left ( 0_2, d_1, 1 \right ) = 1 \right ]$ for both $d_1 = 0$ and $d_1 = 1$. Leveraging the intuition that the path indicator random variable $P$ describes a unit's treatment-taking behavior and thus their ``type'', it is natural to expect that a similar common trends assumption hold in time periods preceding the focal round of treatment assignment. In other words, if a unit's ``type'' is time-invariant, an empirical researcher may plausibly conjecture that
\begin{align}\label{eq:null_simple}
	& \E \left [ Y_2 \left ( 0_2,d_1,0 \right ) - Y_1 \left ( 0_2,d_1,0 \right ) | H_2 = 0_2, R_3 = c, P \left ( 0_2, d_1, 0 \right ) = 1 \right ] \nonumber \\
	= \ & \E \left [ Y_2 \left ( 0_2,d_1,0 \right ) - Y_1 \left ( 0_2,d_1,0 \right ) | H_2 = 0_2, R_3 = c, P \left ( 0_2, d_1, 1 \right ) = 1 \right ]
\end{align}
for $d_1 \in \left \{ 0,1 \right \}$. If the outcome variable is non-anticipatory and Assumption \ref{ass_cont1} holds, these conditional expectations are identified for both the upper and lower limit of the running variable in $t=3$. Specifically,
\begin{align}
	\lim_{r \downarrow c} \E \left [ Y_2 - Y_1 | H_2 = 0_2, R_3 = r, D_4 = 0 \right ] = \lim_{r \downarrow c} \E \left [ Y_2 - Y_1 | H_2 = 0_2, R_3 = r, D_4 = 1 \right ]
\end{align}
% \lim_{r \uparrow c} \E \left [ Y_2 - Y_1 | H_2 = 0_2, R_3 = r, D_4 = 0 \right ] = \lim_{r \uparrow c} \E \left [ Y_2 - Y_1 | H_2 = 0_2, R_3 = r, D_4 = 1 \right ]
and symmetrically for the lower limit.

These equalities naturally give rise to testable restrictions aimed at assessing the plausibility of Assumption \ref{ass_ct1}. Specifically, an empirical researcher may provide a more compelling argument in favor of the local common trends restriction by failing to reject the equality of average differences in outcome trends at the cutoff in all available ``pre-periods''. In the canonical difference-in-differences design, it is common practice to test the equality of pre-period outcome trends across groups implied by whether units are assigned to the ``treatment'' or ``control'' arm. In a dynamic RDD, groups are implicitly determined by the treatment path subsequent to the focal time period. Unless more restrictions are placed on the evolution of treatment states, the number of groups or ``types'' will in general exceed two, thereby increasing the number of testable equalities.

Under Assumption \ref{ass_mark}, the focal time period is preceded by $k$ periods for which $D_{t-1} = \dots = D_{t-k} = 0$. Then, based on Assumption \ref{ass_ct1}, the null hypothesis in equation \eqref{eq:null_simple} may be generalized to any sequence of future treatment states $d_{t+1}, \dots, d_{t+\tau} \in \left \{ 0,1 \right \}$ and any pair of periods $u$ and $v$ such that $t-k \leq u < v \leq t$. Specifically,
\begin{align}
	& \E \left [ \Delta_{u,v} \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) | H_{t-1} = h_{t-1}, R_t = c, P \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) = 1 \right ] \nonumber \\
	= \ & \E \left [ \Delta_{u,v} \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) | H_{t-1} = h_{t-1}, R_t = c, P \left ( h_{t-k-1}, 0_k, d_t, d_{t+1}, \dots, d_{t+\tau} \right ) = 1 \right ]
\end{align}
with $\Delta_{u,v} \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) \equiv Y_v \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) - Y_u \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right )$. Because units are untreated in the $k$ periods preceding $t$, both conditional expectations are identified using the upper or lower limit of $R_t$:
\begin{align}\label{eq:pretrends_identified}
	& \lim_{r \downarrow c} \E \left [ Y_v - Y_u \bigg | H_{t-1} = h_{t-1}, R_t = r, \bigcap_{s=1}^{\tau} \left \{ D_{t+s} = 0 \right \} \right ] \notag \\
	& = \lim_{r \downarrow c} \E \left [ Y_v - Y_u \bigg | H_{t-1} = h_{t-1}, R_t = r, \bigcup_{s=1}^{\tau} \left \{ D_{t+s} \neq 0 \right \} \right ]
\end{align}
and the equality of lower limits follows from a symmetric argument. Appendix \ref{app_testct} develops statistical theory to conduct tests of hypotheses pertaining to the plausibility of the common trends assumption.


\section{Estimation and Statistical Inference}\label{sec_estim}

In this section, I illustrate that standard local polynomial methods are applicable to the estimation of Average Direct Treatment Effects in a dynamic RDD. Hereafter, I do not explicitly condition on the history of treatment states $H_{t-1} = h_{t-1}$ to keep notation concise. Then, by an application of the Law Iterated Expectations and the properties of limits, the target estimand from Proposition \ref{prop_apte1_gen} can be equivalently expressed as
\begin{align}
	\mathrm{ADTE}_{t,\tau} \left ( c \right ) & = \lim_{r \downarrow c} \E \left [ Y_{t} | R_t = r \right ] - \lim_{r \uparrow c} \E \left [ Y_{t} | R_t = r \right ] \notag \\
	& + \frac{\lim_{r \downarrow c} \E \left [ \left ( Y_{t+\tau} - Y_t \right ) \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) | R_t = r \right ]}{\lim_{r \downarrow c} \E \left [ \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) | R_t = r \right ]} \notag \\
	& - \frac{\lim_{r \uparrow c} \E \left [ \left ( Y_{t+\tau} - Y_t \right ) \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) | R_t = r \right ]}{\lim_{r \uparrow c} \E \left [ \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) | R_t = r \right ]} \label{apte_gen_lie}
\end{align}
In this expression, the parameter of interest is a nonlinear function of cutoff-specific conditional means, all of which can be nonparametrically estimated in the full sample, rather than in subsamples implied by realizations of $\left \{ D_{t+s} \right \}_{s=1}^{\tau}$. This feature will prove convenient for the purpose of constructing robust nonparametric confidence intervals. To keep notation compact, I also define the following random variables:
\begin{align}
	R \equiv R_t \qquad Y \equiv Y_t \qquad W \equiv \left ( Y_{t+\tau} - Y_t \right ) \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) \qquad D \equiv \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right )
\end{align}
For any random variable $A$, let $\mu_{A} \left ( r \right ) \equiv \E \left [ A | R = r \right ]$ and $\sigma^2_{AA} \left ( r \right ) \equiv \Var \left [ A | R = r \right ]$. Furthermore, let $\mu_{A+} \equiv \lim_{r \downarrow c} \mu_{A} \left ( r \right )$ and $\mu_{A-} \equiv \lim_{r \uparrow c} \mu_{A} \left ( r \right )$. Then the parameter of interest can be written as
\begin{align}
	\theta \equiv \mu_{Y+} - \mu_{Y-} + \frac{\mu_{W+}}{\mu_{D+}} - \frac{\mu_{W-}}{\mu_{D-}}
\end{align}
Prior to discussing statistical inference on $\theta$, Assumption \ref{ass_moments} outlines a number of necessary conditions pertaining to moments and features of the running variable. These requirements match those listed in Assumptions 1 and 3 of \cite{cct2014}.

\begin{ass} \label{ass_moments}
	Let $\delta \in \mathbb{N}$. For $\kappa_0 \in \mathbb{R}_{++}$, the following conditions hold in a neighborhood of the cutoff $\left ( c-\kappa_0, c+\kappa_0 \right )$.
	\begin{enumerate}[(a)]
		\item The density  $f_R \left ( r \right )$ is continuous and bounded away from zero.
		\item $\E \left [ Y^4 | R=r \right ]$ is bounded.
		\item $\mu_{Y} \left ( r \right )$, $\mu_{W} \left ( r \right )$, $\mu_{D} \left ( r \right )$ are $\delta$ times continuously differentiable.
		\item $\sigma^2_{YY} \left ( r \right )$, $\sigma^2_{WW} \left ( r \right )$, $\sigma^2_{DD} \left ( r \right )$ are continuous and bounded away from zero.
	\end{enumerate}
\end{ass}
Consider a random sample $\left \{ \left [ R_{i}, Y_{i}, W_{i}, D_{i} \right ]' \right \}_{i=1}^{n}$. Given a positive bandwidth $h_n > 0$, let $\mathcal{R} \left ( h_n \right ) = \left [ c-h_n, c+h_n \right ]$ be a discontinuity window implied by realizations of the running variable around the first-period cutoff. Moreover, let $\mathcal{R}^{-} \left ( h_n \right ) = \left [ c-h_n, c \right )$ and $\mathcal{R}^{+} \left ( h_n \right ) = \left [ c, c+h_n \right ]$ indicate the left and right discontinuity windows, respectively. Throughout this section, I employ standard local polynomial estimators to approximate the value of several conditional means on either side of the cutoff. Specifically, for any random variable $A$, I consider local linear estimators defined as
\begin{align}
	\left [ \widehat{\mu}^{(0)}_{A+,1} \left ( h_n \right ), \widehat{\mu}^{(1)}_{A+,1} \left ( h_n \right ) \right ]' & \equiv \arg \min_{b_0,b_1 \in \mathbb{R}} \sum_{i=1}^{n} \mathbb{I} \left [ R_{i} \in \mathcal{R}^{+} \left ( h_n \right ) \right ] \left ( A_{i} - b_0 - b_1 R_{i} \right )^2 k_{h_n} \left ( R_{i} \right ) \label{loclinplus} \\
	\left [ \widehat{\mu}^{(0)}_{A-,1} \left ( h_n \right ), \widehat{\mu}^{(1)}_{A-,1} \left ( h_n \right ) \right ]' & \equiv \arg \min_{b_0,b_1 \in \mathbb{R}} \sum_{i=1}^{n} \mathbb{I} \left [ R_{i} \in \mathcal{R}^{-} \left ( h_n \right ) \right ] \left ( A_{i} - b_0 - b_1 R_{i} \right )^2 k_{h_n} \left ( R_{i} \right ) \label{loclinminus}
\end{align}
where $k_h \left ( u \right ) \equiv k \left ( \frac{u}{h} \right ) \slash h$, with $k \left ( \cdot \right )$ denoting a kernel function. Mirroring Assumption 2 in \cite{cct2014}, Assumption \ref{ass_kernel} below reports minimal conditions for the validity of $k \left ( \cdot \right )$.

\begin{ass}\label{ass_kernel}
	For $\kappa \in \mathbb{R}_{++}$, the kernel function $k \left ( \cdot \right ): \left [ 0,\kappa \right ] \to \mathbb{R}$ is bounded and nonnegative, zero outside its support, and positive and continuous on $\left ( 0, \kappa \right )$.
\end{ass}
In the notation of equations \eqref{loclinplus} and \eqref{loclinminus}, the subscript $p$ in $\widehat{\mu}^{(\nu)}_{+,p}$ indicates the chosen polynomial degree in the regression specification, while the superscript $\nu$ denotes the order of the derivative of interest. For example, in standard sharp designs, the estimated coefficient of interest is $\widehat{\mu}^{(0)}_{+,1} \left ( h_n \right ) - \widehat{\mu}^{(0)}_{-,1} \left ( h_n \right )$, namely the difference between two intercepts in a local linear regression specification. In this paper's dynamic design, the estimator of interest replaces the conditional means in equation \eqref{apte_gen_lie} with their sample counterparts. As a matter of fact, a natural estimator for $\theta$ is
\begin{align}\label{estimator}
	\widehat{\theta} \left ( h_n \right ) \equiv \widehat{\mu}^{(0)}_{Y+,1} \left ( h_n \right ) - \widehat{\mu}^{(0)}_{Y-,1} \left ( h_n \right ) + \frac{\widehat{\mu}^{(0)}_{W+,1} \left ( h_n \right )}{\widehat{\mu}^{(0)}_{D+,1} \left ( h_n \right )} - \frac{\widehat{\mu}^{(0)}_{W-,1} \left ( h_n \right )}{\widehat{\mu}^{(0)}_{D-,1} \left ( h_n \right )}
\end{align}
As in any regression discontinuity design, the estimator in equation \eqref{estimator} is a function of the width of the discontinuity window $h_n$. The choice of a larger bandwidth may lead to more precise estimates, but also increase their bias relative to the true intercept at the cutoff. Concerns about bias are particularly salient when adopting the prevalent data-driven approach to bandwidth selection, namely choosing one that minimizes the mean squared error of the estimator (\citealt{ik2012}). A natural solution is to first estimate the bias using a pilot bandwidth $b_n$ and then subtract the estimated bias from the point estimate of interest. \cite{cct2014} shows how to account for the statistical uncertainty stemming from the first step and construct robust nonparametric confidence intervals for canonical estimands in sharp and fuzzy regression discontinuity and kink designs.

In this section, I leverage the same approach to construct robust nonparametric confidence intervals for the target estimand in equation \eqref{apte_gen_lie}. The choice of notation and the structure of proofs largely mirror those in \cite{cct2014supp}. Similarly to standard estimators in fuzzy regression discontinuity and kink designs, the estimator in equation \eqref{estimator} is nonlinear. Thus, to obtain the bias expression in equation \eqref{bias}, I proceed as in \cite{cct2014} and linearize $\widehat{\theta} \left ( h_n \right )$ with a second-order Taylor expansion. The resulting estimator is defined as $\widetilde{\theta} \left ( h_n \right ) \equiv \widetilde{\eta}_{+} \left ( h_n \right ) - \widetilde{\eta}_{-} \left ( h_n \right )$, with
\begin{align}
	\widetilde{\eta}_{+} \left ( h_n \right ) & \equiv \frac{1}{\mu_{D+}} \left ( \widehat{\mu}^{(0)}_{W+,1} \left ( h_n \right ) - \mu_{W+} \right ) - \frac{\mu_{W+}}{\mu_{D+}^2} \left ( \widehat{\mu}^{(0)}_{D+,1} \left ( h_n \right ) - \mu_{D+} \right ) \label{eq:etaplus}\\
	\widetilde{\eta}_{-} \left ( h_n \right ) & \equiv \frac{1}{\mu_{D-}} \left ( \widehat{\mu}^{(0)}_{W-,1} \left ( h_n \right ) - \mu_{W-} \right ) - \frac{\mu_{W-}}{\mu_{D-}^2} \left ( \widehat{\mu}^{(0)}_{D-,1} \left ( h_n \right ) - \mu_{D-} \right ) \label{eq:etaminus}
\end{align}
Letting $\widehat{\textsc{B}} \left ( h_n, b_n \right )$ denote the estimated bias of $\widetilde{\theta} \left ( h_n \right )$, the bias-corrected estimator of $\theta$ is
\begin{align}
	\widehat{\theta}^{\textsc{bc}} \left ( h_n, b_n \right ) \equiv \widehat{\theta} \left ( h_n \right ) - h^2_n \widehat{\textsc{B}} \left ( h_n, b_n \right )
\end{align}
with
\begin{align}
	\widehat{\textsc{B}} \left ( h_n, b_n \right ) & \equiv \left [ \frac{\widehat{\mu}^{(2)}_{Y+,2} \left ( b_n \right )}{2} + \frac{1}{\widehat{\mu}^{(0)}_{D+,1} \left ( h_n \right )} \frac{\widehat{\mu}^{(2)}_{W+,2} \left ( b_n \right )}{2} - \frac{\widehat{\mu}^{(0)}_{W+,1} \left ( h_n \right )}{\left ( \widehat{\mu}^{(0)}_{D+,1} \left ( h_n \right ) \right )^2} \frac{\widehat{\mu}^{(2)}_{D+,2} \left ( b_n \right )}{2} \right ] \mathcal{B}_{+} \left ( h_n \right ) \notag \\
	& - \left [ \frac{\widehat{\mu}^{(2)}_{Y-,2} \left ( b_n \right )}{2} + \frac{1}{\widehat{\mu}^{(0)}_{D-,1} \left ( h_n \right )} \frac{\widehat{\mu}^{(2)}_{W-,2} \left ( b_n \right )}{2} - \frac{\widehat{\mu}^{(0)}_{W-,1} \left ( h_n \right )}{\left ( \widehat{\mu}^{(0)}_{D-,1} \left ( h_n \right ) \right )^2} \frac{\widehat{\mu}^{(2)}_{D-,2} \left ( b_n \right )}{2} \right ] \mathcal{B}_{-} \left ( h_n \right ) \label{bias}
\end{align}
where $\mathcal{B}_{+} \left ( h_n \right )$ and $\mathcal{B}_{-} \left ( h_n \right )$ are observed and asymptotically bounded quantities defined in Appendix \ref{app_def_bias}. Proposition \ref{prop_confint} below establishes the asymptotic distribution of the bias-corrected estimator $\widehat{\theta}^{\textsc{bc}} \left ( h_n, b_n \right )$.

\begin{prop}[Asymptotic Distribution]\label{prop_confint}
	Suppose that Assumptions \ref{ass_moments} and \ref{ass_kernel} hold. Let $\mu_{D+} \neq 0$ and $\mu_{D-} \neq 0$. If $n \min \left \{ h^5_n, b^5_n \right \} \max \left \{ h^2_n, b^2_n \right \} \to 0$ and $n \min \left \{ h_n, b_n \right \} \to \infty$,
		\begin{align*}
			T^{\textsc{rbc}} \left ( h_n , b_n \right ) \equiv \frac{\widehat{\theta}^{\textsc{bc}} \left ( h_n, b_n \right ) - \theta}{\sqrt{\textsc{V}^{\textsc{bc}} \left ( h_n, b_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right )
		\end{align*}
	provided that $h_n \to 0$ and $\kappa b_n < \kappa_0$. The exact expression for $\textsc{V}^{\textsc{bc}} \left ( h_n, b_n \right )$ is provided in Appendix \ref{app_prop_confint}.
\end{prop}

\subsection{Bandwidth Selection}\label{sec:bandwidth}

I adopt the solution proposed by \cite{cct2014} by computing both an optimal main bandwidth $h$ and an optimal pilot bandwidth $b$. The main bandwidth is used to estimate the parameter of interest, while the pilot bandwidth is employed to estimate its bias.

Following \cite{ik2012}, I define optimality in terms of minimizing the mean squared error (MSE) of the estimator. Two distinct MSE criteria are considered depending on the bandwidth. First, the main bandwidth minimizes the MSE of the linearized estimator $\widetilde{\theta} \left ( h_n \right )$, which consists of a linear combination of intercepts in local linear regression specifications. Second, the pilot bandwidth minimizes the MSE of an alternative linearized estimator $\widetilde{\theta}_{2,2} \left ( h_n \right )$, in which the estimated intercepts are replaced by second-order derivatives in local quadratic specifications and, analogously, population means are replaced by the corresponding second-order derivatives\footnote{In general, $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ denotes the linearized estimator involving $\nu$th-order derivatives in local polynomial specifications of order $p$. Thus, the main linearized estimator $\widetilde{\theta} \left ( h_n \right )$ corresponds to $\widetilde{\theta}_{0,1} \left ( h_n \right )$.}. The following proposition provides expression for both MSEs and derives the corresponding optimal bandwidths.

\begin{prop}\label{prop_optbandwidth}
	Suppose Assumptions \ref{ass_moments} and \ref{ass_kernel} hold with $\delta \geq 3$. Let $h_n \to 0$ and $n h_n \to \infty$. The mean squared error of the linearized estimator $\widetilde{\theta} \left ( h_n \right )$ is
	\begin{align*}
		\text{MSE} \left ( \widetilde{\theta} \left ( h_n \right ) \right ) & = h^4_{n} \left ( \dot{\textsc{B}}^2_{h} + o_p \left ( 1 \right ) \right ) + \frac{1}{n h_n} \left ( \dot{\textsc{V}}_{h} + o_p \left ( 1 \right ) \right )
	\end{align*}
	and the MSE-optimal main bandwidth is
	\begin{align*}
		h_{\textsc{mse}} = \left ( \frac{\dot{\textsc{V}}_{h}}{4 \dot{\textsc{B}}^2_{h}} \right )^{1 \slash 5} n^{-1 \slash 5}
	\end{align*}
	The mean squared error of the linearized estimator $\widetilde{\theta}_{2,2} \left ( h_n \right )$ is
	\begin{align*}
		\text{MSE} \left ( \widetilde{\theta}_{2,2} \left ( h_n \right ) \right ) & = h^{2}_{n} \left ( \dot{\textsc{B}}^2_{b} + o_p \left ( 1 \right ) \right ) + \frac{1}{n h_n^{5}} \left ( \dot{\textsc{V}}_{b} + o_p \left ( 1 \right ) \right )
	\end{align*}
	and the MSE-optimal pilot bandwidth is
	\begin{align*}
		b_{\textsc{mse}} = \left ( \frac{5 \dot{\textsc{V}}_{b}}{2 \dot{\textsc{B}}^2_{b}} \right )^{1 \slash 7} n^{-1 \slash 7}
	\end{align*}
	Exact expressions for $\dot{\textsc{B}}_{h}$, $\dot{\textsc{V}}_{h}$, $\dot{\textsc{B}}_{b}$, and $\dot{\textsc{V}}_{b}$ are provided in Appendix \ref{app_prop_bandwidth}.
\end{prop}
It is important to note that the MSE-optimal bandwidths specified in Proposition \ref{prop_optbandwidth} are asymptotic and, hence, not directly implementable. In practice, I follow the steps outlined in Section S.2.6 of \cite{cct2014supp} and employ analogous direct plug-in bandwidth selectors.

\subsection{Standard Errors}\label{sec:stderr}

The variance of the bias-corrected estimator $\textsc{V}^{\textsc{bc}} \left ( h_n, b_n \right )$ in Proposition \ref{prop_confint} is asymptotic. To compute standard errors and conduct statistical inference on the parameter of interest, it is natural to replace unknown population moments with their sample analogues. First, note that the linearized estimator in equations \eqref{eq:etaplus} and \eqref{eq:etaminus} is defined as a linear combination of estimated intercepts, where the constant coefficients are unknown population means. I replace these means with their corresponding estimates computed within each discontinuity half-window. Second, population variances and covariances are unknown. For any pair of random variables $A$ and $B$, define these second-order moments as $\sigma^2_{AB+} \equiv \lim_{r \downarrow c} \Cov \left [ A,B | R=r \right ]$ and $\sigma^2_{AB-} \equiv \lim_{r \uparrow c} \Cov \left [ A,B | R=r \right ]$. To estimate these moments, I follow the approach in \cite{abadieimbens2006} and \cite{cct2014}, employing a nearest-neighbor method that computes each sample covariance using a fixed number of observations. Specifically, letting $j^*$ denote this tuning parameter,
\begin{align}
	\widehat{\sigma}^2_{AB+} \left ( R_i \right ) & \equiv \mathbb{I} \left [ R_i \geq c \right ] \frac{j^*}{j^*+1} \left ( A_i - \sum_{j \in \mathcal{J}^{+}_{i}} \frac{A_{j}}{j^*} \right ) \left ( B_i - \sum_{j \in \mathcal{J}^{+}_{i}} \frac{B_{j}}{j^*} \right ) \\
	\widehat{\sigma}^2_{AB-} \left ( R_i \right ) & \equiv \mathbb{I} \left [ R_i < c \right ] \frac{j^*}{j^*+1} \left ( A_i - \sum_{j \in \mathcal{J}^{-}_{i}} \frac{A_{j}}{j^*} \right ) \left ( B_i - \sum_{j \in \mathcal{J}^{-}_{i}} \frac{B_{j}}{j^*} \right )
\end{align}
where $\mathcal{J}^{+}_{i}$ and $\mathcal{J}^{-}_{i}$ denote the sets of the $j^*$ closest units to unit $i$ in the right and left discontinuity half-windows, respectively. Once all of the first- and second-order moments are estimated, the resulting estimated variance $\widehat{\textsc{V}}^{\textsc{bc}} \left ( h_n, b_n \right )$ can be used to compute the standard error of $\widehat{\theta}^{\textsc{bc}} \left ( h_n, b_n \right )$ and construct confidence intervals for the target parameter.

\section{Simulation}\label{sec_simulation}

In this section, I apply the methods developed in the preceding sections to simulated data. I employ a data generating process similar to that presented by \cite{bilaschon2023}\footnote{Specifically, the authors conduct a Monte Carlo simulation in Appendix B.}, which models both school districts' decisions to hold local referenda and the subsequent evolution of an outcome of interest based on referendum results. The primary objective of this exercise is to demonstrate that the estimator developed in this paper effectively recovers the parameter of interest.

\subsection{Data Generating Process}\label{sec:dgp}

Let $i \in \left \{ 1, \dots, n \right \}$ and $t \in \left \{ 1, \dots, \overline{t} \right \}$ index schools districts and calendar years, respectively. Let $Q_{it}$ be a Bernoulli random variable equal to one if district $i$ holds a referendum in year $t$. Denote by $R_{it}$ the approval vote margin and define $D_{it} \equiv \mathbb{I} \left [ R_{it} \geq 0 \right ]$ as the indicator for referendum approval. In each period, a district's decision to hold a referendum is influenced, among other factors, by whether referenda were recently approved or rejected. Specifically, 
\begin{align}\label{sim:hold}
	Q_{it} = \mathbb{I} \left [ A^q_i + B^q_t + U^q_{it} + \sum_{s=1}^{\overline{s}} \rho^q_s D_{i,t-s} + \sum_{s=1}^{\overline{s}} \delta^q_s Q_{i,t-s} \left ( 1 - D_{i,t-s} \right ) > \overline{q} \right ]
\end{align}
where $A^q_i$ is a district-specific intercept, $B^q_t$ is a time-specific intercept, $U^q_{it}$ is an idiosyncratic shock, and $\overline{q}$ is a deterministic threshold that must be exceeded for a referendum to be proposed. If a referendum is held, the approval vote margin is determined according to a similar statistical model:
\begin{align}\label{sim:vote}
	R_{it} = A^r_i + B^r_t + U^r_{it} + \sum_{s=1}^{\overline{s}} \rho^r_s D_{i,t-s} + \sum_{s=1}^{\overline{s}} \delta^r_s Q_{i,t-s} \left ( 1 - D_{i,t-s} \right ) + \sum_{s=1}^{\overline{s}} \gamma_s \left ( 1 - Q_{i,t-s} \right )
\end{align} 
where $A^r_i$ and $B^r_t$ are district- and time-specific intercepts, respectively, and $U^r_{it}$ is an idiosyncratic shock. This specification allows the vote margin to flexibly depend on whether similar ballot measures were recently proposed and, if so, whether they were approved or rejected. The outcome of interest is generated according to
\begin{align}\label{sim:outcome}
	Y_{it} = \sum_{g=1}^{t} \theta_{igt} D_{ig} + \eta \overline{R}_i + V_{it}
\end{align}
where $V_{it}$ is an idiosyncratic shock and $R_i \equiv \left ( \sum_{g=1}^{\overline{t}} Q_{ig} \right )^{-1} \sum_{g=1}^{\overline{t}} Q_{ig} R_{ig}$ represents the average approval vote margin for district $i$ over the period of interest. Importantly, the stochastic treatment effect $\theta_{igt}$ is determined by
\begin{align}\label{sim:effect}
	\theta_{igt} = \theta_{0g} + \theta_{1g} \left ( t - g \right ) + \phi R_{ig}
\end{align}
This formulation allows the effect of referendum approval to vary across districts, calendar times, and cohorts (with a cohort defined as the calendar time at which a referendum is approved). Here, $\theta_{0g}$ is a cohort-specific random variable capturing the permanent effect of a referendum approved in calendar time $g$, and $\theta_{1g}$ is a cohort-specific random variable that measures the incremental effect per unit of time elapsed since approval, meaning that this additional effect is proportional to the relative time $t-g$. Finally, $\phi$ is a deterministic coefficient that quantifies the extent to which the treatment effect varies with the magnitude of the approval vote margin.

\subsection{Parameterization}\label{sec:params}

The distributional assumptions and parameter values largely mirror those in \cite{bilaschon2023}. In the referendum equation \eqref{sim:hold}, the unit intercept is assumed to be $A^q_i \sim \mathcal{N} \left ( 0, \sigma^2_q \right )$, the time intercept is $B^q_t \sim \mathcal{N} \left ( 0, \sigma^2_q \right )$, and the idiosyncratic shock is $U^q_{it} \sim \mathcal{N} \left ( 0, \sigma^2_{uq} \right )$. Similarly, in the vote margin equation \eqref{sim:vote}, $A^r_i \sim \mathcal{N} \left ( 0, \sigma^2_r \right )$, $B^r_t \sim \mathcal{N} \left ( 0, \sigma^2_r \right )$, and $U^r_{it} \sim \mathcal{N} \left ( 0, \sigma^2_{ur} \right )$. In both cases, draws are independently distributed. In the outcome equation \eqref{sim:outcome}, the idiosyncratic shock is given by $V_{it} \sim \mathcal{N} \left ( 0, \sigma^2_v \right )$. In the treatment effect equation \eqref{sim:effect}, the permanent effect is modeled as $\theta_{0g} \sim \mathcal{N} \left ( \overline{\theta}_{0} , \sigma^2_0 \right )$ and the additional, time-proportional effect is $\theta_{1g} \sim \mathcal{N} \left ( \overline{\theta}_{1} , \sigma^2_1 \right )$. I simulate data for $n = 30,000$ school districts over a period of $\overline{t} = 30$ years. To fix initial conditions, I set $Q_{i0} \sim \text{Binomial} \left ( 1, 0.1 \right )$ and, conditional on $Q_{i0} = 1$, $R_{i0}  \sim \mathcal{N} \left ( \mu_r,\sigma^2_r \right )$. The chosen parameter values are as follows: $\overline{q} = 1.7$; $\sigma^2_q = 0.5$; $\sigma^2_{uq} = 1$; $\sigma^2_r = 0.01$; $\sigma^2_{ur} = 0.02$; $\mu_r = 0.05$; $\sigma^2_v = 0.2$; $\eta = 0$; $\phi = 0.1$; $\overline{\theta}_{0} = 0.4$; $\overline{\theta}_{1} = -0.1$; $\sigma^2_{0} = 0.08^2$; $\sigma^2_{1} = 0.02^2$. In addition, I cap the incremental effect per unit of time at $t-g = 4$, so that $\theta_1 \left ( t - g \right ) = 4 \times \theta_1$ for any $t-g > 4$. Finally, I set $\overline{s} = 3$ and $\left ( \rho^q_1, \rho^q_2, \rho^q_3 \right ) = \left ( -0.9, -0.5, -0.1 \right )$, $\left ( \delta^q_1, \delta^q_2, \delta^q_3 \right ) = \left ( 0.5, 0.3, 0.1 \right )$, $\left ( \rho^r_1, \rho^r_2, \rho^r_3 \right ) = \left ( -0.05, -0.03, -0.01 \right )$, $\left ( \delta^r_1, \delta^r_2, \delta^r_3 \right ) = \left ( 0.05, 0.03, 0.01 \right )$, and $\left ( \gamma_1, \gamma_2, \gamma_3 \right ) = \left ( 0.05, 0.03, 0.01 \right )$.

\subsection{Target Parameters}\label{sec:targetparam}

By definition, $\theta_{igt}$ represents the unit-specific effect of a referendum approved in period $g$ on the outcome observed in period $t$. Equivalently, letting $\tau = t-g$ denote the number of periods since approval, $\theta_{igt}$ can be written as $\theta_{ig\tau}$. The average effect of referendum approval for cohort $g$ at relative time $\tau$ is then
\begin{align}
	\theta_{g\tau} \equiv \E \left [ \theta_{ig\tau} \right ] & = \theta_{0g} + \tau \theta_{1g} + \phi \E \left [ R_{ig} \right ]
\end{align}
where the expectation is taken over the distribution of $R_{ig}$ across units in cohort $g$. As in any regression discontinuity design, the running variable must be evaluated at the cutoff, implying that the target parameter of interest is $\theta_{0g} + \tau \theta_{1g}$. Using the notation introduced earlier, this corresponds to $\mathrm{ADTE}_{g,\tau} \left ( 0 \right )$, i.e., the $\tau$-period-ahead Average Direct Treatment Effect when the focal discontinuity occurs in period $g$. To summarize effects across cohorts, I aggregate the cohort-specific parameters for each relative time $\tau$ following equation \eqref{eq:aggr_reltime}:
\begin{align}\label{eq:adte_sim}
	\mathrm{ADTE}_{\tau} \left ( 0 \right ) \equiv \sum_{g=1}^{\overline{t}-\tau} \omega_{g \tau} \mathrm{ADTE}_{g,\tau} \left ( 0 \right )
\end{align}
where the weight $\omega_{g \tau} \equiv \Prob \left ( Q_{ig}=1 \right ) \slash \sum_{\ell=1}^{\overline{t}-\tau} \Prob \left ( Q_{i\ell}=1 \right )$ is the probability that a referendum occurs in period $g$, conditional on a referendum occurring at any time within the window $g \leq \overline{t} - \tau$. This weighting ensures that the aggregate effect appropriately reflects the distribution of referenda over time (\citealt{callawaysantanna2021}).

The remainder of this section presents a simulation exercise that evaluates how accurately the estimated $\mathrm{ADTE}_{\tau} \left ( 0 \right )$ recovers the true average effect at relative time $\tau$, defined as
\begin{align}
	\theta_\tau \equiv \E \left [ \theta_{g\tau} \right ] = \E \left [ \theta_{0g} \right ] + \tau \E \left [ \theta_{1g} \right ]
\end{align}
where the expectation is taken over the distribution of $\theta_{0g}$ and $\theta_{1g}$ across cohorts.

\subsection{Results}

To reduce the number of estimands, I leverage Assumption \ref{ass_mark} by defining a cohort $g$ as the set of jurisdictions that held a referendum in period $g$ and did not approve a ballot measure in any of the preceding $k=3$ periods, i.e., $Q_{ig} = 1$ and $D_{i,g-1} = D_{i,g-2} = D_{i,g-3} = 0$. Then, for each cohort and for relative times $\tau \in \left \{ 1, 2, 3, 4, 5 \right \}$, I estimate $\mathrm{ADTE}_{g,\tau} \left ( 0 \right )$ using a local linear specification within the discontinuity window implied by the MSE-optimal bandwidth, as detailed in Section \ref{sec:bandwidth}. I then aggregate cohort-specific point estimates by replacing the weights in equation \eqref{eq:adte_sim} with their sample counterparts in the simulated data. To estimate the instantaneous causal parameter $\mathrm{ADTE}_{g,0} \left ( 0 \right )$ and to conduct a placebo analysis on pre-period outcomes ($\tau \in \left \{ -3, -2, -1 \right \}$), I use standard (i.e., static) sharp RD estimators. Figure \ref{fig:eventstudy} below reports the true DGP-implied treatment effects alongside the corresponding estimates, all of which are averaged over 250 replications.

% Figure environment removed

\vspace{-2mm}

Figure \ref{fig:eventstudy} confirms that the estimated ADTEs accurately capture the dynamic effects of referendum approval on the outcome. As expected, outcomes measured in negative relative time periods remain unaffected by the approval of referenda in period $t=g$. In Section \ref{sec:markov}, I suggest that pre-period outcomes can serve as a useful check on the plausibility of the local common trends assumption. For example, for any cohort $g$, consider the first difference $\Delta_{g-3,g-1} \equiv Y_{g-1}- Y_{g-3}$. By construction, this difference reflects untreated potential outcomes. It is therefore natural to test whether, near the cutoff at $\tau = 0$, the difference $\Delta_{g-3,g-1}$ is mean independent of the treatment path following the focal referendum. For instance, one might compare the $D_{g+1} = D_{g+2} = D_{g+3} = D_{g+4} = D_{g+5} = 0$ path with any alternative path involving at least one treated period:
\begin{align}\label{eq:sim_null}
	\lim_{r \downarrow 0} \E \left [ \Delta_{g-3,g-1} \bigg | R_g = r, \bigcap_{s=1}^{5} \left \{ D_{g+s} = 0 \right \} \right ] = \lim_{r \downarrow 0} \E \left [ \Delta_{g-3,g-1} \bigg | R_g = r, \bigcup_{s=1}^{5} \left \{ D_{g+s} \neq 0 \right \} \right ]
\end{align}
and similarly for the lower limit. I conduct a test of the null hypothesis implied by the equality in \eqref{eq:sim_null} and its symmetric counterpart for $r \uparrow 0$. Following the procedure described in Appendix \ref{app_testct}, I construct a test statistic for these two equalities and compare its value in each simulated sample to the appropriate quantile of a $\chi^2 \left ( 2 \right )$-distributed random variable. Across 250 replications, the null hypothesis is not rejected at the conventional 0.05 significance level in approximately 92 percent of cases.


\section{Empirical Application: School District Expenditure Authorization and Housing Prices in Wisconsin}\label{sec:application}

In this section, I apply the proposed method to estimate the intertemporal effects of changes in school district expenditures on housing prices in Wisconsin. Due to property tax limits imposed by state governments, local jurisdictions often face constraints in their fiscal policy decisions \citep{lincolntaxlimits}. To secure additional funding, school districts may hold referenda seeking voter authorization for property tax increases designated to finance specific expenditure initiatives.

Housing prices are chosen as the outcome variable for two primary reasons. First, research in local public finance has long recognized that the extent to which property tax changes are capitalized into housing prices is an informative measure of the efficiency of local public goods provision (\citealt{bickerdike1902}, \citealt{marshall1948}, \citealt{oates1969}, \citealt{brueckner1982}, \citealt{cushing1984}, \citealt{barrowrouse2004}, \citealt{figliolucas2004}, \citealt{cfr2010}, \citealt{bilaschon2024}). Evidence of positive capitalization following an expenditure increase is typically interpreted as an indication that prospective homebuyers value the associated improvement in public service provision more than the change in the tax burden required to fund it. Second, many capital investment projects approved through school referenda unfold over multiple years, and their long-term implications may not be fully internalized by potential buyers at the time of the vote. This feature underscores the importance of identifying interpretable dynamic treatment effects over extended horizons.

\subsection{Data}

I focus my application on the state of Wisconsin, where school districts routinely hold referenda to authorize both operational and capital expenditures (\citealt{baron2022}). The Wisconsin Department of Public Instruction collects and publishes comprehensive data on all such local referenda held in the state since 1990 (\citealt{wiscreferenda}). This dataset includes, among other variables, information on the approval vote share, which I use as the running variable in the dynamic RDD.

To construct the outcome of interest, I follow an approach similar to that of \cite{bilaschon2024}. Specifically, I rely on a repeat-sales house price index developed by \cite{contatlarson2024}, which covers all Census tracts located within Core-Based Statistical Areas\footnote{The term “Core-Based Statistical Area” refers collectively to both Metropolitan Statistical Areas and Micropolitan Statistical Areas (\citealt{censusglossary}).} in the United States from 1989 to 2021. The index is normalized to 100 in 1989 for all tracts, allowing for within-tract temporal comparisons but not cross-sectional ones. To allow for level comparisons across school districts, I incorporate data on the average value of owner-occupied single-family homes at the Census tract level, as reported in the U.S. Census Bureau’s 2000 Decennial Census\footnote{The collection of this variable was discontinued beginning with the 2010 Decennial Census.}. For each tract, I compute a calibration factor as the ratio of the 2000 Census home value to the 2000 value of the house price index from \cite{contatlarson2024}, and apply this factor to the full time series of the index. The resulting measure of housing prices allows for both cross-sectional and intertemporal comparisons. Next, I compute the centroid of each Census tract and assign it to the corresponding elementary, secondary, or unified school district based on the 2010 TIGER/Line shapefiles provided by the U.S. Census Bureau\footnote{I use 2010 tract and school district boundaries because the house price index constructed by \cite{contatlarson2024} is based on 2010 Census tracts.} (\citealt{censustiger}). Finally, for each district, I calculate a population-weighted average of housing prices across its constituent Census tracts\footnote{Although I compute population-weighted averages, this choice is not consequential, as Census tracts are designed to contain approximately 4,000 inhabitants (\citealt{censusglossary}).}. This yields the outcome variable used in the dynamic RDD.

The matched sample includes 1,001 referenda\footnote{To construct a balanced panel in event time, I restrict the sample to referenda with outcomes observed in all relative years from –5 to 5, where relative year is defined with respect to the year of the referendum.}, of which 57.6 percent were approved. The average approval vote share margin is 2.05 percentage points, with an average of 9.86 percentage points among approved referenda and –8.57 percentage points among those that were rejected. Figure \ref{fig_runningvar} displays a histogram of the approval vote share margin. To assess the validity of the design, I test for discontinuities in the density of the running variable at the cutoff using the local polynomial density estimators developed by \cite{cattaneojanssonma2020}. The null hypothesis of equal densities on either side of the cutoff is not rejected ($p$-value = 0.80), suggesting that manipulation of the running variable around the threshold is unlikely to be a concern in this setting.

% Figure environment removed

\subsection{Results}

As in Section~\ref{sec_simulation}, I leverage Assumption~\ref{ass_mark} by defining a cohort as the set of jurisdictions that held a referendum in a given year and did not approve a ballot measure in any of the preceding three years. Given the limited sample size, estimating a separate parameter for each cohort is not feasible. To address this constraint, I partition the sample into groups of adjacent years, ensuring that each group contains a roughly similar number of observations. The results presented in this section are based on a specification with six groups, each comprising approximately 150 observations. The findings are broadly robust to alternative groupings\footnote{Pooling observations across adjacent cohorts implicitly imposes the restriction $\mathrm{ADTE}_{t,\tau} = \mathrm{ADTE}_{t',\tau}$ for all periods $t$ and $t'$ within a group.}. To quantify statistical uncertainty, I compute standard errors using the nearest-neighbor method described in Section~\ref{sec:stderr}, setting the tuning parameter to $j^* = 3$\footnote{This choice corresponds to the default setting in the \texttt{rdrobust} package (\citealt{rdrobust2014}, \citealt{rdrobust2015}, \citealt{rdrobust2017}).}. Cohort-specific standard errors are then aggregated using the Delta method.

\begin{table}[H]
	\begin{center}
		\caption{Estimated $\mathrm{ADTE}$s of Referendum Approval on Housing Prices}\label{table_wi_results}
		\vspace{0mm}
		\begin{tabular}{>{\raggedright\arraybackslash}p{2.5cm}*{5}{>{\centering\arraybackslash}p{1.8cm}}}
			\toprule\toprule
			& \multicolumn{5}{c}{Year Relative to Referendum} \\
			\cmidrule(lr){2-6}
			& 1 & 2 & 3 & 4 & 5 \\
			\midrule
			\addlinespace
			\multicolumn{6}{l}{\textit{Panel A: Cohort-by-Cohort Estimates}} \\
			1995-1996 & -0.033 & 0.302 & 0.584 & 0.547 & -0.025 \\
			& (0.198) & (0.267) & (0.456) & (0.382) & (0.165) \\
			\addlinespace
			1997-1998 & -0.099 & -0.069 & -0.103 & -0.177 &  \\
			& (0.174) & (0.177) & (0.191) & (0.196) &  \\
			\addlinespace
			1999-2001 & 0.298 & 0.304 & 0.325 & 0.355 & 0.286 \\
			& (0.200) & (0.214) & (0.218) & (0.234) & (0.236) \\
			\addlinespace
			2002-2005 & 0.168 & 0.189 & 0.211 & 0.266 & 0.199 \\
			& (0.136) & (0.127) & (0.133) & (0.140) & (0.144) \\
			\addlinespace
			2006-2008 & -0.009 & -0.042 & -0.032 & -0.085 & -0.072 \\
			& (0.119) & (0.127) & (0.122) & (0.120) & (0.129) \\
			\addlinespace
			2009-2015 & 0.098 & 0.107 & 0.109 & 0.156 & 0.182 \\
			& (0.106) & (0.111) & (0.125) & (0.151) & (0.139) \\
			\addlinespace
			\midrule
			\multicolumn{6}{l}{\textit{Panel B: Aggregated Estimates}} \\
			& 0.080 & 0.127 & 0.167 & 0.168 & 0.124 \\
			\addlinespace
			& (0.062) & (0.068) & (0.085) & (0.083) & (0.075) \\
			\addlinespace
			\bottomrule\bottomrule
		\end{tabular}
	\end{center}
	\vspace{-1mm}
	\begin{footnotesize}
		\begin{spacing}{1}
			\noindent
			\textsc{Notes:} This table reports estimates of average direct treatment effects (ADTEs) of the approval of school district expenditure referenda on housing prices in Wisconsin between 1995 and 2015. Panel~A presents cohort-specific estimates of $\mathrm{ADTE}_{g,\tau}(0)$, whose estimand is stated in equation~\eqref{apte_gen_lie}. Cohorts (indexed by $g$) correspond to groups of adjacent years shown in the leftmost column, and relative time (indexed by $\tau$) ranges from 1 to 5 years. Panel~B reports aggregated ADTEs, calculated as weighted averages of cohort-specific estimates according to equation~\eqref{eq:adte_sim}, with weights proportional to each cohort’s share of the sample. Standard errors are computed using the nearest-neighbor method described in Section~\ref{sec:stderr}, with tuning parameter $j^* = 3$, and aggregated using the Delta method.
		\end{spacing}
	\end{footnotesize}
\end{table}

\vspace{-2mm}

Table~\ref{table_wi_results} presents the results. Panel~A reports cohort-specific estimates, while Panel~B shows aggregated parameters. The latter indicate that, on average, the approval of school expenditure measures leads to cumulative increases in housing prices of approximately 12 percent after five years. These estimates are consistent with recent findings in the literature (\citealt{bilaschon2024}), although they also highlight substantial heterogeneity across cohorts. In particular, the aggregate effects appear to be primarily driven by the 1999--2001 and 2002-2005 year groups.

% Figure environment removed

\vspace{-2mm}

To assess the validity of the identification strategy, I conduct a placebo analysis by estimating treatment effects on housing prices in periods prior to the referenda. These estimates rely on standard (i.e., static) local linear regression discontinuity estimators. Figure~\ref{fig_eventplot} presents event-study-style plots that show average direct treatment effects for each relative year, based on two specifications: one that aggregates cohort-specific estimates, and another that pools observations across all cohorts. The two approaches yield comparable results in both magnitude and precision. Reassuringly, the estimated effects in pre-treatment periods are close to zero and not statistically significant, suggesting no systematic differences in housing prices between jurisdictions that later approved versus rejected referenda.

\begin{table}[H]
	\begin{center}
		\caption{Checks on the Plausibility of the Common Trends Assumption}\label{table_wi_testct}
		\vspace{0mm}
		\begin{tabular}{>{\raggedright\arraybackslash}p{2.5cm}*{3}{>{\centering\arraybackslash}p{2.5cm}}}
			\toprule\toprule
			& \multicolumn{3}{c}{$\Delta_{g-u,g-v} \equiv Y_{g-v} - Y_{g-u}$} \\
			\cmidrule(lr){2-4}
			& $u=3, v=2$ & $u=2, v=1$ & $u=3, v=1$ \\
			\midrule
			\addlinespace
			\multicolumn{4}{l}{\textit{Panel A: Aggregated Cohort-Specific Estimates}} \\
			$R_g \in \mathcal{R}^{+}$ & -0.004 & -0.020 & -0.036 \\
			& (0.018) & (0.011) & (0.026) \\
			$R_g \in \mathcal{R}^{-}$ & -0.013 & -0.021 & 0.055 \\
			& (0.029) & (0.015) & (0.365) \\
			$p$-value & 0.873 & 0.068 & 0.381 \\
			\addlinespace
			\midrule
			\multicolumn{4}{l}{\textit{Panel B: Pooled Estimates}} \\
			$R_g \in \mathcal{R}^{+}$ & 0.000 & 0.010 & 0.010 \\
			& (0.026) & (0.022) & (0.047) \\
			$R_g \in \mathcal{R}^{-}$ & 0.008 & 0.000 & 0.008 \\
			& (0.010) & (0.013) & (0.024) \\
			$p$-value & 0.747 & 0.909 & 0.923 \\
			\addlinespace
			\bottomrule\bottomrule
		\end{tabular}
	\end{center}
	\vspace{-1mm}
	\begin{footnotesize}
		\begin{spacing}{1}
			\noindent
			\textsc{Notes:} This table reports estimates used to assess the plausibility of the local common trends assumption, as described in Section~\ref{sec:markov}. For each pair of relative years $(u,v) \in \{(3,2), (2,1), (3,1)\}$, the table displays estimated differences in average pre-referendum outcome trends, defined as $\Delta_{g-u,g-v} \equiv Y_{g-v} - Y_{g-u}$, between the subpopulation implied by $D_{g+1} = D_{g+2} = D_{g+3} = D_{g+4} = D_{g+5} = 0$ and its complement. These differences are evaluated separately on either side of the cutoff, with $\mathcal{R}^{+}$ and $\mathcal{R}^{-}$ denoting the right and left discontinuity half-windows, respectively. Panel~A presents estimates aggregated across cohorts, while Panel~B reports estimates from a pooled specification in which all cohorts are combined. Standard errors, shown in parentheses, are computed using the nearest-neighbor method described in Section~\ref{sec:stderr}, with tuning parameter $j^* = 3$. The final row in each panel reports the $p$-value from an $F$-test of the null hypothesis that the upper and lower limits of the estimated difference are jointly equal to zero.
		\end{spacing}
	\end{footnotesize}
\end{table}

\vspace{-2mm}

As a final validation exercise, I assess the plausibility of the local common trends assumption, as discussed in Section~\ref{sec:markov}. By construction, outcomes are untreated in the three periods preceding each referendum. If the common trends assumption holds, an empirical researcher may be willing to conjecture that, at the cutoff of the running variable, trends in untreated outcomes prior to the referendum are similar across jurisdictions that are subsequently treated and those that remain untreated after the focal period. To test this suggestive implication, I examine all possible pairs of relative years $(u, v) \in \{ (3,2), (2,1), (3,1) \}$ and nonparametrically estimate the difference in conditional means of $Y_{g-v} - Y_{g-u}$ between two subpopulations: jurisdictions for which $D_{g+1} = D_{g+2} = D_{g+3} = D_{g+4} = D_{g+5} = 0$, and its complement. These differences correspond to contrasts in pre-referendum outcome trends, evaluated at the cutoff. Table~\ref{table_wi_testct} reports the estimated differences for each pair of years, both using aggregated cohort-specific estimates and a specification that pools observations across all cohorts. In all cases, the estimated differences are not statistically distinguishable from zero at the 0.05 significance level. For each pair $(u, v)$, I also conduct an $F$-test of the null hypothesis that the upper and lower limits of the conditional mean differences are jointly equal to zero. The null is not rejected in any case, suggesting that the local common trends assumption might be plausible in this setting.


\section{Conclusion}\label{sec_concl}

This paper proposed a novel argument to point identify economically interpretable intertemporal treatment effects in dynamic regression discontinuity designs. I developed a dynamic potential outcomes model and reformulated two assumptions from the difference-in-differences literature---no anticipation and common trends---to point identify impulse response-like treatment effects at the cutoff. The estimand of each target parameter can be expressed as the sum of two static RD outcome contrasts, thereby allowing for nonparametric estimation and inference with standard local polynomial methods. I also proposed a nonparametric approach to aggregate treatment effects across calendar time and treatment paths, relying on a limited path independence restriction to reduce the dimensionality of the parameter space. I applied the proposed method to study the dynamic effects of school district expenditure referenda on housing prices in Wisconsin. The results indicate that, on average, referendum approval leads to a cumulative increase in housing prices of approximately 12 percent within five years.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%% REFERENCES %%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\begin{spacing}{1} 
	%\def\bibfont{\small}
	\renewcommand\refname{References}
	\bibliography{../../references/references}
\end{spacing}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% APPENDIX %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\appendix

%\pagenumbering{arabic}
\renewcommand{\thesubsection}{\Alph{section}.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\Alph{section}.\arabic{subsection}.\arabic{subsubsection}}

\setcounter{figure}{0} \renewcommand{\thefigure}{\Alph{section}\arabic{figure}}
\setcounter{table}{0} \renewcommand{\thetable}{\Alph{section}\arabic{table}}
\setcounter{equation}{0} \renewcommand{\theequation}{\Alph{section}.\arabic{equation}}
\setcounter{theorem}{0} \renewcommand{\thetheorem}{\Alph{section}.\arabic{theorem}}

%\section*{Appendix}
\addcontentsline{toc}{section}{Appendix}

\listofappendixcontents

\appsection{Identification}

\appsubsection{Proof of Proposition \ref{prop_rd}}

The sharp RD estimand implied by the first-period discontinuity is defined as
\begin{align*}
	\mathrm{RD}_{t} \left ( c \right ) \equiv \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_t | R_1 = r \right ] 
\end{align*}
Consider the first term:
\begin{align}
	\lim_{r \downarrow c} \E \left [ Y_t | R_1 = r \right ] & = \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r, D_1 = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_1 = 1, D_2 \left ( 1 \right ) = 0 \right ] Y_t \left ( 1,0 \right )  | R_1 = r, D_1 = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_1 = 1, D_2 \left ( 1 \right ) = 1 \right ] Y_t \left ( 1,1 \right )  | R_1 = r, D_1 = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_1 = 0, D_2 \left ( 1 \right ) = 0 \right ] Y_t \left ( 0,0 \right )  | R_1 = r, D_1 = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_1 = 0, D_2 \left ( 1 \right ) = 1 \right ] Y_t \left ( 0,1 \right )  | R_1 = r, D_1 = 1 \right ]
\end{align}
The first equality exploits the assumption that $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$, which implies that the conditioning set can be expanded to include the event $D_1=1$. The second equality replaces the observed outcome $Y_t$ with its stochastic linear combination of potential outcomes, according to equation \eqref{potout}. Furthermore, because each of the four expectations is conditional on the event $D_1 = 1$, any indicator containing the event $D_1 = 0$ will be false. For the same reason, the event $D_1 = 1$ can be subsumed from any indicator that contains it. Thus,
\begin{align}
	\lim_{r \downarrow c} \E \left [ Y_t | R_1 = r \right ] & = \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_2 \left ( 1 \right ) = 0 \right ] Y_t \left ( 1,0 \right )  | R_1 = r, D_1 = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_2 \left ( 1 \right ) = 1 \right ] Y_t \left ( 1,1 \right )  | R_1 = r, D_1 = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_2 \left ( 1 \right ) = 0 \right ] Y_t \left ( 1,0 \right )  | R_1 = r \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ \mathbb{I} \left [ D_2 \left ( 1 \right ) = 1 \right ] Y_t \left ( 1,1 \right )  | R_1 = r \right ] \\
	& =  \lim_{r \downarrow c} \E \left [ Y_t \left ( 1,0 \right )  | R_1 = r, D_2 \left ( 1 \right ) = 0 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 0 | R_1 = r \right ) \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_t \left ( 1,1 \right )  | R_1 = r, D_2 \left ( 1 \right ) = 1 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 1 | R_1 = r \right ) \\
	& = \E \left [ Y_t \left ( 1,0 \right )  | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 0 | R_1 = c \right ) \notag \\
	& + \E \left [ Y_t \left ( 1,1 \right )  | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 1 | R_1 = c \right )
\end{align}
As above, the second equality exploits the assumption that $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality follows from an application of the Law of Iterated Expectations. The fourth equality leverages Assumption \ref{ass_cont0} and the algebraic properties of limits. By a symmetric argument, the right-hand side of $\mathrm{RD}_t \left ( c \right )$ identifies
\begin{align}
	\lim_{r \uparrow c} \E \left [ Y_t | R_1 = r \right ] & = \E \left [ Y_t \left ( 0,0 \right )  | R_1 = c, D_2 \left ( 0 \right ) = 0 \right ] \times \Prob \left ( D_2 \left ( 0 \right ) = 0 | R_1 = c \right ) \notag \\
	& + \E \left [ Y_t \left ( 0,1 \right )  | R_1 = c, D_2 \left ( 0 \right ) = 1 \right ] \times \Prob \left ( D_2 \left ( 0 \right ) = 1 | R_1 = c \right )
\end{align}
which completes the proof.

\appsubsection{Proof of Proposition \ref{prop_apte0}}

The instantaneous Average Direct Treatment Effect (ADTE) implied by the first-period discontinuity is defined as
\begin{align*}
	\textsc{ADTE}_{1,0} \left ( c \right ) \equiv \E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c \right ] - \E \left [ Y_1 \left ( 0,0 \right ) | R_1 = c \right ]
\end{align*}
By an application of the Law of Iterated Expectations, the first term can be expressed as
\begin{align}\label{apte0_decomp}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c \right ] & = \E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 0 | R_1 = c \right ) \notag \\
	& + \E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 1 | R_1 = c \right )
\end{align}
Both conditional probabilities are identified under Assumption \ref{ass_cont0}. In fact, for $d_2 \in \left \{ 0,1 \right \}$,
\begin{align}
	\Prob \left ( D_2 \left ( 1 \right ) = d_2 | R_1 = c \right ) & = \lim_{r \downarrow c} \Prob \left ( D_{2} \left ( 1 \right ) = d_2 | R_1 = r \right ) \\
	& = \lim_{r \downarrow c} \Prob \left ( D_{2} \left ( 1 \right ) = d_2 | R_1 = r, D_1 = 1 \right ) \\
	& = \lim_{r \downarrow c} \Prob \left ( D_{2} = d_2 | R_1 = r, D_1 = 1 \right ) \\
	& = \lim_{r \downarrow c} \Prob \left ( D_{2} = d_2 | R_1 = r \right )
\end{align}
The first equality leverages Assumption \ref{ass_cont0}, which states that $\E \left [ D_2 \left ( 1 \right ) | R_1 = r \right ]$ is a continuous function of $r$ at $r=c$. The second and fourth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality stems from the fact that, conditional on $D_1 = 1$, the second-period potential treatment $D_2 \left ( 1 \right )$ is observed. By a similar argument, the first conditional expectation in equation \eqref{apte0_decomp} is identified as
\begin{align}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ] & = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,0 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,0 \right ) | R_1 = r, D_1 = 1, D_2 \left ( 1 \right ) = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,0 \right ) | R_1 = r, D_1 = 1, D_2 = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_1 = 1, D_2 = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 0 \right ]
\end{align}
The first equality leverages Assumption \ref{ass_cont0}, which states that $\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 0 \right ]$ is a continuous function of $r$ at $r=c$. The second and fifth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality stems from the fact that, conditional on $D_1 = 1$, the second-period potential treatment $D_2 \left ( 1 \right )$ is observed. Analogously, the potential outcome $Y_1 \left ( 1,0 \right )$ is observed conditional on $D_1 = 1$ and $D_2 = 0$, thus justifying the fourth equality. Finally, under Assumptions \ref{ass_cont0} Assumption \ref{ass_noant}, the counterfactual conditional mean in equation \eqref{apte0_decomp} is identified as
\begin{align}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ] & = \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ]\\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_1 = 1, D_2 \left ( 1 \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_1 = 1, D_2 = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_1 = 1, D_2 = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 1 \right ]
\end{align}
The first equality applies the no anticipation restriction embedded in Assumption \ref{ass_noant}. The second equality leverages Assumption \ref{ass_cont0}, which states that $\E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 1 \right ]$ is a continuous function of $r$ at $r=c$. The third and sixth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The fourth equality stems from the fact that, conditional on $D_1 = 1$, the second-period potential treatment $D_2 \left ( 1 \right )$ is observed. Analogously, the potential outcome $Y_1 \left ( 1,1 \right )$ is observed conditional on $D_1 = 1$ and $D_2 = 1$, thus justifying the fifth equality. Combining these identification results, the first term in $\textsc{ADTE}_{1,0} (c)$ can be expressed as
\begin{align}
	\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c \right ] & = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 0 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2}=0 | R_1 = r \right ) \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 1 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2}=1 | R_1 = r \right ) \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ]
\end{align}
where the second equality follows from another application of the Law of Iterated Expectations. By a symmetric argument, $\lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ]$ identifies $\E \left [ Y_1 \left ( 0,0 \right ) | R_1 = c \right ]$. Thus,
\begin{align}
	\textsc{ADTE}_{1,0} \left ( c \right ) = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ]
\end{align}


\appsubsection{Proof of Proposition \ref{prop_apte1}}

The cumulative Average Direct Treatment Effect (ADTE) implied by the first-period discontinuity is defined as
\begin{align*}
	\textsc{ADTE}_{1,1} \left ( c \right ) \equiv \E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c \right ] - \E \left [ Y_2 \left ( 0,0 \right ) | R_1 = c \right ]
\end{align*}
By an application of the Law of Iterated Expectations, the first term can be expressed as
\begin{align}\label{apte1_decomp}
	\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c \right ] & = \E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 0 | R_1 = c \right ) \notag \\
	& + \E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 1 \right ] \times \Prob \left ( D_2 \left ( 1 \right ) = 1 | R_1 = c \right )
\end{align}
As shown in the previous section, both conditional probabilities are identified under Assumption \ref{ass_cont0}. Furthermore, the first conditional expectation is identified as
\begin{align}
	\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c, D_2 \left ( 1 \right ) = 0 \right ] & = \lim_{r \downarrow c} \E \left [ Y_2 \left ( 1,0 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_2 \left ( 1,0 \right ) | R_1 = r, D_1 = 1, D_2 \left ( 1 \right ) = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_2 \left ( 1,0 \right ) | R_1 = r, D_1 = 1, D_2 = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_2 | R_1 = r, D_1 = 1, D_2 = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_2 | R_1 = r, D_2 = 0 \right ]
\end{align}
The first equality leverages Assumption \ref{ass_cont0}, which states that $\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 0 \right ]$ is a continuous function of $r$ at $r=c$. The second and fifth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality stems from the fact that, conditional on $D_1 = 1$, the second-period potential treatment $D_2 \left ( 1 \right )$ is observed. Analogously, the potential outcome $Y_2 \left ( 1,0 \right )$ is observed conditional on $D_1 = 1$ and $D_2 = 0$, thus justifying the fourth equality. Finally, under Assumptions \ref{ass_cont0}, \ref{ass_noant}, and \ref{ass_ct0}, the counterfactual conditional mean in equation \eqref{apte1_decomp} is identified as
\begin{align}
	& \E \left [ Y_{2} \left ( 1,0 \right ) | R_1 = c, D_{2} \left ( 1 \right )=1 \right ] \\
	& = \E \left [ Y_1 \left ( 1,0 \right ) | R_1 = c, D_{2} \left ( 1 \right ) = 1 \right ] \notag \\
	& + \E \left [ Y_{2} \left ( 1,0 \right ) - Y_1 \left ( 1,0 \right ) | R_1 = c, D_{2} \left ( 1 \right ) = 0 \right ] \\
	& = \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = c, D_{2} \left ( 1 \right ) = 1 \right ] \notag \\
	& + \E \left [ Y_{2} \left ( 1,0 \right ) - Y_1 \left ( 1,0 \right ) | R_1 = c, D_{2} \left ( 1 \right ) = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_{2} \left ( 1 \right ) = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{2} \left ( 1,0 \right ) - Y_1 \left ( 1,0 \right ) | R_1 = r, D_{2} \left ( 1 \right ) = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_1 = 1, D_{2} \left ( 1 \right ) = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{2} \left ( 1,0 \right ) - Y_1 \left ( 1,0 \right ) | R_1 = r, D_1 = 1, D_{2} \left ( 1 \right ) = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_1 = 1, D_{2} = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{2} \left ( 1,0 \right ) - Y_1 \left ( 1,0 \right ) | R_1 = r, D_1 = 1, D_{2} = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_1 = 1, D_{2}=1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{2} - Y_1 | R_1 = r, D_1 = 1, D_{2}=0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{2} - Y_1 | R_1 = r, D_{2}=0 \right ]
\end{align}
The first equality uses Assumption \ref{ass_ct0}, which states that $Y_{2} \left ( 1,0 \right ) - Y_1 \left ( 1,0 \right )$ is mean independent of $D_{2} \left ( 1 \right )$ at the first-period cutoff. The second equality applies the no anticipation restriction embedded in Assumption \ref{ass_noant}. The third equality leverages Assumption \ref{ass_cont0}, which states that $\E \left [ Y_1 \left ( 1,1 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 1 \right ]$, $\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 0 \right ]$, and $\E \left [ Y_1 \left ( 1,0 \right ) | R_1 = r, D_2 \left ( 1 \right ) = 0 \right ]$ are continuous functions of $r$ at $r=c$. The fourth and seventh equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The fifth equality stems from the fact that, conditional on $D_1 = 1$, the second-period potential treatment $D_2 \left ( 1 \right )$ is observed. Analogously, the potential outcome $Y_1 \left ( 1,1 \right )$ is observed conditional on $D_1 = D_2 = 1$ and the potential outcomes $Y_2 \left ( 1,0 \right )$ and $Y_1 \left ( 1,0 \right )$ are observed conditional on $D_1 = 1$ and $D_2 = 0$, thus justifying the sixth equality. Combining these results, the first term in $\textsc{ADTE}_{1,1} (c)$ can be expressed as
\begin{align}
	\E \left [ Y_2 \left ( 1,0 \right ) | R_1 = c \right ] & = \lim_{r \downarrow c} \E \left [ Y_2 | R_1 = r, D_2 = 0 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2}=0 | R_1 = r \right ) \notag \\
	& + \left ( \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=1 \right ] + \lim_{r \downarrow c} \E \left [ Y_{2} - Y_1 | R_1 = r, D_{2}=0 \right ] \right ) \notag \\
	& \times \lim_{r \downarrow c} \Prob \left ( D_{2}=1 | R_1 = r \right )
\end{align}
$\E \left [ Y_2 \left ( 0,0 \right ) | R_1 = c \right ]$ is point identified with a symmetric argument. Thus,
\begin{align}
	\textsc{ADTE}_{1,1} \left ( c \right ) & = \lim_{r \downarrow c} \E \left [ Y_2 | R_1 = r, D_2 = 0 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2}=0 | R_1 = r \right ) \notag \\
		& + \left ( \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=1 \right ] + \lim_{r \downarrow c} \E \left [ Y_{2} - Y_1 | R_1 = r, D_{2}=0 \right ] \right ) \notag \\
		& \times \lim_{r \downarrow c} \Prob \left ( D_{2}=1 | R_1 = r \right ) \notag \\
		& - \lim_{r \uparrow c} \E \left [ Y_2 | R_1 = r, D_2 = 0 \right ] \times \lim_{r \uparrow c} \Prob \left ( D_{2}=0 | R_1 = r \right ) \notag \\
		& - \left ( \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r, D_{2}=1 \right ] + \lim_{r \uparrow c} \E \left [ Y_{2} - Y_1 | R_1 = r, D_{2}=0 \right ] \right ) \notag  \\
		& \times \lim_{r \uparrow c} \Prob \left ( D_{2}=1 | R_1 = r \right ) \\
		& = \lim_{r \downarrow c} \E \left [ \left ( 1 - D_2 \right ) Y_2 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ \left ( 1 - D_2 \right ) Y_2 | R_1 = r \right ] \notag \\
		& + \lim_{r \downarrow c} \E \left [ D_2 Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ D_2 Y_1 | R_1 = r \right ] \notag \\
		& - \lim_{r \downarrow c} \E \left [ \left ( 1 - D_2 \right ) \left ( Y_2 - Y_1 \right ) | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ \left ( 1 - D_2 \right ) \left ( Y_2 - Y_1 \right ) | R_1 = r \right ] \notag \\
		& + \lim_{r \downarrow c} \E \left [ Y_2 - Y_1 | R_1 = r, D_2 =0 \right ] - \lim_{r \uparrow c} \E \left [ Y_2 - Y_1 | R_1 = r, D_2 =0 \right ] \\
		& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ] \notag \\
		& + \lim_{r \downarrow c} \E \left [ Y_2 - Y_1 | R_1 = r, D_2 =0 \right ] - \lim_{r \uparrow c} \E \left [ Y_2 - Y_1 | R_1 = r, D_2 =0 \right ]
\end{align}
where the second equality follows from an application of the Law of Iterated Expectations.

\appsubsection{Proof of Proposition \ref{prop_rd1}}

The sharp RD estimand implied by the first-period discontinuity is defined as
\begin{align*}
	\mathrm{RD}_{t} \left ( c \right ) \equiv \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_t | R_1 = r \right ] 
\end{align*}
Consider the first term:
\begin{align}
	& \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r \right ] \notag \\
	& = \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r, D_1 = 1 \right ] \\
	& = \lim_{r \downarrow c} \sum_{\left ( d_2, \dots, d_{\overline{t}} \right )} \E \left [ \mathbb{I} \left [ D_1 = 1 \right ] P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) Y_t \left ( 1,d_2, \dots, d_{\overline{t}} \right )  | R_1 = r, D_1 = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \sum_{\left ( d_2, \dots, d_{\overline{t}} \right )} \E \left [ \mathbb{I} \left [ D_1 = 0 \right ] P \left ( 0, d_2, \dots, d_{\overline{t}} \right ) Y_t \left ( 0,d_2, \dots, d_{\overline{t}} \right )  | R_1 = r, D_1 = 1 \right ]
\end{align}
The first equality exploits the assumption that $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$, which implies that the conditioning set can be expanded to include the event $D_1=1$. The second equality replaces the observed outcome $Y_t$ with its stochastic linear combination of potential outcomes, according to equation \eqref{potout_gen2}. Furthermore, because each of the four expectations is conditional on the event $D_1 = 1$, any indicator containing the event $D_1 = 0$ will be false. For the same reason, the event $D_1 = 1$ can be subsumed from any indicator that contains it. Thus,
\begin{align}
	\lim_{r \downarrow c} \E \left [ Y_t | R_1 = r \right ] & = \lim_{r \downarrow c} \sum_{\left ( d_2, \dots, d_{\overline{t}} \right )} \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) Y_t \left ( 1,d_2, \dots, d_{\overline{t}} \right )  | R_1 = r, D_1 = 1 \right ] \\
	& = \lim_{r \downarrow c} \sum_{\left ( d_2, \dots, d_{\overline{t}} \right )} \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) Y_t \left ( 1,d_2, \dots, d_{\overline{t}} \right )  | R_1 = r \right ] \\
	& = \lim_{r \downarrow c} \sum_{\left ( d_2, \dots, d_{\overline{t}} \right )} \E \left [ Y_t \left ( 1,d_2, \dots, d_{\overline{t}} \right )  | R_1 = r, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& \times \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = r \right ] \\
	& = \sum_{\left ( d_2, \dots, d_{\overline{t}} \right )} \E \left [ Y_t \left ( 1,d_2, \dots, d_{\overline{t}} \right )  | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& \times \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]
\end{align}
As above, the second equality exploits the assumption that $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality follows from an application of the Law of Iterated Expectations. The fourth equality leverages Assumption \ref{ass_cont1} and the algebraic properties of limits. By a symmetric argument, the right-hand side of $\mathrm{RD}_t \left ( c \right )$ identifies
\begin{align}
	\lim_{r \uparrow c} \E \left [ Y_t | R_1 = r \right ] & = \sum_{\left ( d_2, \dots, d_{\overline{t}} \right )} \E \left [ Y_t \left ( 0,d_2, \dots, d_{\overline{t}} \right )  | R_1 = c, P \left ( 0, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& \times \E \left [ P \left ( 0, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]
\end{align}
which completes the proof.


\appsubsection{Proof of Proposition \ref{prop_apte0_gen}}

The instantaneous Average Direct Treatment Effect (ADTE) implied by the first-period discontinuity is defined as
\begin{align*}
	\textsc{ADTE}_{1,0} \left ( c \right ) \equiv \E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c \right ] - \E \left [ Y_1 \left ( 0,0_{\overline{t}-1} \right ) | R_1 = c \right ]
\end{align*}
By an application of the Law of Iterated Expectations, the first term can be expressed as
\begin{align}\label{apte0_decomp}
	\E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c \right ] & = \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}-1}} \E \left [ Y_t \left ( 1,0_{\overline{t}-1} \right )  | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& \times \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]
\end{align}
Each conditional expectation of $P$ is identified under Assumption \ref{ass_cont1}. In fact, for any sequence of treatment states $\left ( d_2, \dots, d_{\overline{t}} \right )$,
\begin{align}
	\E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ] & = \lim_{r \downarrow c} \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = r \right ] \\
	& = \lim_{r \downarrow c} \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, D_1 = 1 \right ] \\
	& = \lim_{r \downarrow c} \Prob \left ( D_{2} = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} | R_1 = r, D_1 = 1 \right ) \\
	& = \lim_{r \downarrow c} \Prob \left ( D_{2} = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} | R_1 = r \right )
\end{align}
The first equality leverages Assumption \ref{ass_cont1}, which states that $\E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = r \right ]$ is a continuous function of $r$ at $r=c$. The second and fourth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality stems from the fact that, conditional on $D_1 = 1$, the path of potential treatment states $P \left ( 1, d_2, \dots, d_{\overline{t}} \right )$ is observed. By a similar argument, one conditional expectation in equation \eqref{apte0_decomp} is identified without further restrictions. Specifically,
\begin{align}
	& \E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \notag \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, D_1 = 1, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, D_1 = 1, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_1 = 1, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ]
\end{align}
The first equality leverages Assumption \ref{ass_cont1}, which states that $\E \big [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \big ]$ is a continuous function of $r$ at $r=c$. The second and fifth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality stems from the fact that, conditional on $D_1 = 1$, the path of potential treatment states $P \left ( 1, d_2, \dots, d_{\overline{t}} \right )$ is observed. Analogously, the potential outcome $Y_1 \left ( 1,0_{\overline{t}-1} \right )$ is observed conditional on $D_1 = 1$ and $D_2 = \dots = D_{\overline{t}} = 0$, thus justifying the fourth equality. Finally, under Assumptions \ref{ass_cont1} and \ref{ass_noant1}, any counterfactual conditional mean in equation \eqref{apte0_decomp} is identified as
\begin{align}
	& \E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& = \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ]\\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, D_1 = 1, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, D_1 = 1, D_2 = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_1 = 1, D_2 = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ]
\end{align}
The first equality applies the no anticipation restriction embedded in Assumption \ref{ass_noant1}. The second equality leverages Assumption \ref{ass_cont1}, which states that $\E \big [ Y_1 \left ( 1,,0_{\overline{t}-1} \right ) | R_1 = r, P ( 1, d_2, \dots, d_{\overline{t}} ) = 1 \big ]$ is a continuous function of $r$ at $r=c$. The third and sixth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The fourth equality stems from the fact that, conditional on $D_1 = 1$, the path of potential treatment states $P \left ( 1, d_2, \dots, d_{\overline{t}} \right )$ is observed. Analogously, the potential outcome $Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right )$ is observed conditional on $D_1 = 1$ and $D_2 = d_2, \dots, D_{\overline{t}} = d_{\overline{t}}$, thus justifying the fifth equality. Combining these identification results, the first term in $\textsc{ADTE}_{1,0} (c)$ can be expressed as
\begin{align}
	\E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c \right ] & = \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}-1}} \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \notag \\
	& \times \lim_{r \downarrow c} \Prob \left ( D_{2}=0, \dots, D_{\overline{t}} = d_{\overline{t}} | R_1 = r \right ) \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ]
\end{align}
where the second equality follows from another application of the Law of Iterated Expectations and the algebraic properties of limits. By a symmetric argument, $\lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ]$ identifies $\E \left [ Y_1 \left ( 0,0_{\overline{t}-1} \right ) | R_1 = c \right ]$. Thus,
\begin{align}
	\textsc{ADTE}_{1,0} \left ( c \right ) = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ]
\end{align}


\appsubsection{Proof of Proposition \ref{prop_apte1_gen}}

For $\tau \in \left \{ 1, \dots, \overline{t} - 1 \right \}$, the cumulative Average Direct Treatment Effect (ADTE) implied by the first-period discontinuity is defined as
\begin{align*}
	\textsc{ADTE}_{1,\tau} \left ( c \right ) \equiv \E \left [ Y_{1+\tau} \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c \right ] - \E \left [ Y_{1+\tau} \left ( 0,0_{\overline{t}-1} \right ) | R_1 = c \right ]
\end{align*}
To keep notation concise, let $Y_t$ denote $Y_{1+\tau}$ for $t \in \left \{ 2, \dots, \overline{t} \right \}$. By an application of the Law of Iterated Expectations, the first term can be expressed as
\begin{align}\label{apte1_decomp}
	\E \left [ Y_t \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c \right ] & = \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \in \left \{ 0,1 \right \}^{\overline{t}-1}} \E \left [ Y_t \left ( 1,0_{\overline{t}-1} \right )  | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& \times \E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]
\end{align}
As shown in the previous section, each $\E \left [ P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) | R_1 = c \right ]$ is identified under Assumption \ref{ass_cont1}. Furthermore, the expected potential outcome conditional on a never-untreated path indicator is identified as
\begin{align}
	& \E \left [ Y_t \left ( 1,0_{\overline{t}-1} \right )  | R_1 = c, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \notag \\
	& = \lim_{r \downarrow c} \E \left [ Y_t \left ( 1,0_{\overline{t}-1} \right )  | R_1 = r, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_t \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, D_1 = 1, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_t \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, D_1 = 1, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r, D_1 = 1, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ]
\end{align}
The first equality leverages Assumption \ref{ass_cont1}, which states that $\E \big [ Y_t \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \big ]$ is a continuous function of $r$ at $r=c$. The second and fifth equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The third equality stems from the fact that, conditional on $D_1 = 1$, the path of potential treatment states $P \left ( 1, d_2, \dots, d_{\overline{t}} \right )$ is observed. Analogously, the potential outcome $Y_t \left ( 1,d_2, \dots, d_{\overline{t}} \right )$ is observed conditional on $D_1 = 1$ and $D_2 = d_2, \dots, D_{\overline{t}} = d_{\overline{t}}$, thus justifying the fourth equality. Finally, under Assumptions \ref{ass_cont1}, \ref{ass_noant1}, and \ref{ass_ct1}, any counterfactual conditional mean in equation \eqref{apte1_decomp} is identified as
\begin{align}
	& \E \left [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& = \E \left [ Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& + \E \left [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) - Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& + \E \left [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) - Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) - Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, D_1 = 1, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) - Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, D_1 = 1, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = r, D_1 = 1, D_{2} = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) - Y_1 \left ( 1,0_{\overline{t}-1} \right ) | R_1 = r, D_1 = 1, D_{2} = 0, \dots, D_{\overline{t}} = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_1 = 1, D_{2}=d_2, D_{\overline{t}} = d_{\overline{t}} \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{t} - Y_1 | R_1 = r, D_1 = 1, D_{2}=0, \dots, D_{\overline{t}} = 0 \right ] \\
	& = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{t} - Y_1 | R_1 = r, D_{2}=0, \dots, D_{\overline{t}} = 0 \right ]
\end{align}
The first equality uses Assumption \ref{ass_ct1}, which states that $Y_{t} \left ( 1,0_{\overline{t}-1} \right ) - Y_1 \left ( 1,0_{\overline{t}-1} \right )$ is mean independent of $P \left ( 1,d_2, \dots, d_{\overline{t}} \right )$ at the first-period cutoff. The second equality applies the no anticipation restriction embedded in Assumption \ref{ass_noant1}. The third equality leverages Assumption \ref{ass_cont1}, which states that $\E \big [ Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right ) | R_1 = c, P \left ( 1, d_2, \dots, d_{\overline{t}} \right ) = 1 \big ]$, $\E \big [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \big ]$, and $\E \big [ Y_{1} \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c, P \left ( 1, 0_{\overline{t}-1} \right ) = 1 \big ]$ are continuous functions of $r$ at $r=c$. The fourth and seventh equalities follow from the threshold-crossing definition of the treatment, $D_1 \equiv \mathbb{I} \left [ R_1 \geq c \right ]$. The fifth equality stems from the fact that, conditional on $D_1 = 1$, the paths of potential treatment states $P \left ( 1, d_2, \dots, d_{\overline{t}} \right )$ and $P \left ( 1, 0_{\overline{t}-1} \right )$ are observed. Analogously, the potential outcome $Y_1 \left ( 1,d_2, \dots, d_{\overline{t}} \right )$ is observed conditional on $D_1 = d_1, \dots, D_{\overline{t}} = d_{\overline{t}}$ and the potential outcomes $Y_t \left ( 1,0_{\overline{t}-1} \right )$ and $Y_1 \left ( 1,0_{\overline{t}-1} \right )$ are observed conditional on $D_1 = 0, D_2 = 0, \dots, D_{\overline{t}} = 0$, thus justifying the sixth equality. Combining these results, the first term in $\textsc{ADTE}_{1,\tau} (c)$ can be expressed as
\begin{align}
	\E \left [ Y_{t} \left ( 1,0_{\overline{t}-1} \right ) | R_1 = c \right ] & = \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \notag \\
	& \times \lim_{r \downarrow c} \Prob \left ( D_{2} = 0, \dots, D_{\overline{t}} = 0 | R_1 = r \right ) \notag \\
	& + \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \neq 0^{\overline{t}-1}} \bigg ( \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \notag  \\
	& + \lim_{r \downarrow c} \E \left [ Y_{t} - Y_1 | R_1 = r, D_{2}=0, \dots, D_{\overline{t}} = 0 \right ] \bigg ) \notag \\
	& \times \lim_{r \downarrow c} \Prob \left ( D_{2} = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} | R_1 = r \right )
\end{align}
$\E \left [ Y_t \left ( 0,0_{\overline{t}-1} \right ) | R_1 = c \right ]$ is point identified with a symmetric argument. Thus,
\begin{align}
	& \textsc{ADTE}_{1,1} \left ( c \right ) \notag \\
	& = \lim_{r \downarrow c} \E \left [ Y_t | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \times \lim_{r \downarrow c} \Prob \left ( D_{2} = 0, \dots, D_{\overline{t}} = 0 | R_1 = r \right ) \notag \\
	& + \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \neq 0_{\overline{t}-1}} \bigg ( \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r, D_{2}=d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{t} - Y_1 | R_1 = r, D_{2}=0, \dots, D_{\overline{t}} = 0 \right ] \bigg ) \notag  \\
	& \times \lim_{r \downarrow c} \Prob \left ( D_{2} = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} | R_1 = r \right ) \notag \\
	& - \lim_{r \uparrow c} \E \left [ Y_t | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \times \lim_{r \uparrow c} \Prob \left ( D_{2} = 0, \dots, D_{\overline{t}} = 0 | R_1 = r \right ) \notag \\
	& - \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \neq 0_{\overline{t}-1}} \bigg ( \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r, D_{2}=d_2, \dots, D_{\overline{t}} = d_{\overline{t}} \right ] \notag \\
	& + \lim_{r \uparrow c} \E \left [ Y_{t} - Y_1 | R_1 = r, D_{2}=0, \dots, D_{\overline{t}} = 0 \right ] \bigg ) \notag \\
	& \times \lim_{r \uparrow c} \Prob \left ( D_{2} = d_2, \dots, D_{\overline{t}} = d_{\overline{t}} | R_1 = r \right )
\end{align}
By an application of the Law of Iterated Expectations, the target parameter can be compactly expressed as
\begin{align}
	& \textsc{ADTE}_{1,1} \left ( c \right ) \notag \\
	& = \lim_{r \downarrow c} \E \left [ \prod_{s=2}^{\overline{t}} \mathbb{I} \left [ D_s = 0 \right ] Y_t \big | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ \prod_{s=2}^{\overline{t}} \mathbb{I} \left [ D_s = 0 \right ] Y_t \big | R_1 = r \right ] \notag \\
	& + \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \neq 0_{\overline{t}-1}} \lim_{r \downarrow c} \E \left [ \prod_{s=2}^{\overline{t}} \mathbb{I} \left [ D_s = d_s \right ] Y_1 | R_1 = r \right ] \notag \\
	& - \sum_{\left ( d_2, \dots, d_{\overline{t}} \right ) \neq 0_{\overline{t}-1}} \lim_{r \uparrow c} \E \left [ \prod_{s=2}^{\overline{t}} \mathbb{I} \left [ D_s = d_s \right ] Y_1 | R_1 = r \right ] \notag \\
	& - \lim_{r \downarrow c} \E \left [ \prod_{s=2}^{\overline{t}} \mathbb{I} \left [ D_s = 0 \right ] \left ( Y_t - Y_1 \right ) \big | R_1 = r \right ] \notag \\
	& + \lim_{r \uparrow c} \E \left [ \prod_{s=2}^{\overline{t}} \mathbb{I} \left [ D_s = 0 \right ] \left ( Y_t - Y_1 \right ) \big | R_1 = r \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_t - Y_1 | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \notag \\
	& - \lim_{r \uparrow c} \E \left [ Y_t - Y_1 | R_1 = r, D_2 =0, \dots, D_{\overline{t}} = 0 \right ]
\end{align}
To conclude,
\begin{align}
	\textsc{ADTE}_{1,\tau} \left ( c \right ) & = \lim_{r \downarrow c} \E \left [ Y_1 | R_1 = r \right ] - \lim_{r \uparrow c} \E \left [ Y_1 | R_1 = r \right ] \notag \\
	& + \lim_{r \downarrow c} \E \left [ Y_{1+\tau} - Y_1 | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ] \notag \\
	& - \lim_{r \uparrow c} \E \left [ Y_{1+\tau} - Y_1 | R_1 = r, D_2 = 0, \dots, D_{\overline{t}} = 0 \right ]
\end{align}
which completes the proof.

\setcounter{equation}{0}

\appsection{Statistical Inference}\label{app_statinf}

In this section, I state and prove more general versions of Propositions \ref{prop_confint} and \ref{prop_optbandwidth}. These theorems apply to derivatives of arbitrary order of the conditional means in the estimator of interest $\widehat{\theta} \left ( h_n \right )$. In a similar fashion, I also develop statistical theory to assess the plausibility of Assumption \ref{ass_ct1}. The notation, intermediate propositions, and structure of proofs largely mirror those in \cite{cct2014supp}. 

\appsubsection{Notation}

For any pair of random variables $A$ and $B$, consider the following conditional moments:
\begin{align}
	\mu_{A} \left ( r \right ) \equiv \E \left [ A | R = r \right ] \qquad \sigma^2_{AA} \left ( r \right ) \equiv \Var \left [ A | R = r \right ] \qquad \sigma^2_{AB} \left ( r \right ) \equiv \Cov \left [ A,B | R=r \right ]
\end{align}
In addition, for $s \in \mathbb{N}$, the $s$th order derivative of the conditional mean with respect to the running variable is
\begin{align}
	\mu^{(s)}_{A} \equiv \frac{\partial^s \mu_{A} \left ( r \right )}{\partial r^s}
\end{align}
Clearly, $\mu^{(0)}_{A} \left ( r \right ) = \mu_{A} \left ( r \right )$. Then define the following limits for the running variable to the deterministic cutoff $c$:
\begin{align}
	\mu^{(s)}_{A+} \equiv \lim_{r \downarrow c} \mu^{(s)}_{A} \left ( r \right ) \quad & \quad \mu^{(s)}_{A-} \equiv \lim_{r \uparrow c} \mu^{(s)}_{A} \left ( r \right ) \\
	\sigma^2_{AA+} \equiv \lim_{r \downarrow c} \sigma^2_{AA} \left ( r \right ) \quad & \quad \sigma^2_{AA-} \equiv \lim_{r \uparrow c} \sigma^{2}_{AA} \left ( r \right ) \\
	\sigma^2_{AB+} \equiv \lim_{r \downarrow c} \sigma^2_{AB} \left ( r \right ) \quad & \quad \sigma^2_{AB-} \equiv \lim_{r \uparrow c} \sigma^2_{AB} \left ( r \right )
\end{align}
Focusing on the conditional mean, define
\begin{align}
	\beta_{A+,p} \equiv \left [ \mu_{A+}, \mu^{(1)}_{A+}, \frac{\mu^{(2)}_{A+}}{2}, \dots, \frac{\mu^{(p)}_{A+}}{p!} \right ]' \qquad \beta_{A-,p} \equiv \left [ \mu_{A-}, \mu^{(1)}_{A-}, \frac{\mu^{(2)}_{A-}}{2}, \dots, \frac{\mu^{(p)}_{A-}}{p!} \right ]'
\end{align}
so that
\begin{align}
	\mu^{(s)}_{A+} = s! e_{s}' \beta_{A+,p} \qquad \mu^{(s)}_{A-} = s! e_{s}' \beta_{A-,p}
\end{align}
where $e_s$ is the conformable $(s+1)$th unit vector. For instance, if $p=2$, then $e_1 = [0,1,0]'$. Recall that the estimand of interest is
\begin{align}
	\theta \equiv \mu^{(0)}_{Y+} - \mu^{(0)}_{Y-} + \frac{\mu^{(0)}_{W+}}{\mu^{(0)}_{D+}} - \frac{\mu^{(0)}_{W-}}{\mu^{(0)}_{D-}}
\end{align}
For consistency with the original paper, I consider the general target parameter
\begin{align}
	\theta_{\nu} \equiv \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + \frac{\mu^{(\nu)}_{W+}}{\mu^{(\nu)}_{D+}} - \frac{\mu^{(\nu)}_{W-}}{\mu^{(\nu)}_{D-}}
\end{align}
where $\nu \in \mathbb{N}$. Let $h$ be a positive bandwidth sequence and let
\begin{align}
	k_h \left ( u \right ) = k \left ( \frac{u}{h} \right ) \Big \slash h
\end{align}
where $k \left ( \cdot \right )$ is the kernel function. Then, given the polynomial basis $x_p \left ( r \right ) = \left [ 1, r, r^2, \dots, r^p \right ]'$, the local polynomial estimators of the conditional mean of $A$ are
\begin{align}
	\widehat{\beta}_{A+,p} \left ( h_n \right ) & \equiv \arg \min_{b \in \mathbb{R}^p} \sum_{i=1}^{n} \mathbb{I} \left [ R_i \geq c \right ] \left ( A_i - x_p \left ( R_i \right )' b \right )^2 k_{h_n} \left ( R_i \right ) \label{polest_plus} \\
	\widehat{\beta}_{A-,p} \left ( h_n \right ) & \equiv \arg \min_{b \in \mathbb{R}^p} \sum_{i=1}^{n} \mathbb{I} \left [ R_i < c \right ] \left ( A_i - x_p \left ( R_i \right )' b \right )^2 k_{h_n} \left ( R_i \right ) \label{polest_minus}
\end{align}
It follows that one can estimate the conditional $\nu$th derivative of a random variable $A$ with
\begin{align}
	\widehat{\mu}^{(\nu)}_{A+,p} \left ( h_n \right ) = \nu! e_{\nu}' \widehat{\beta}_{A+,p} \left ( h_n \right ) \qquad \widehat{\mu}^{(\nu)}_{A-,p} \left ( h_n \right ) = \nu! e_{\nu}' \widehat{\beta}_{A-,p} \left ( h_n \right )
\end{align}
As a consequence, a local polynomial estimator for the target parameter $\theta_\nu$ is
\begin{align}
	\widehat{\theta}_{\nu,p} \left ( h_n \right ) \equiv \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) + \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} - \frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )}
\end{align}
Define the following data vectors and matrices:
\begin{align}
	\bm{Y} \equiv \left [ Y_1, \dots, Y_n \right ]' \quad \bm{W} \equiv \left [ W_1, \dots, W_n \right ]' \quad \bm{D} \equiv \left [ D_1, \dots, D_n \right ]' \quad \bm{R} \equiv \left [ R_1, \dots, R_n \right ]'
\end{align}
Also, for any $i \in \left \{ 1, \dots, n \right \}$, define the residual
\begin{align}
	\varepsilon_{A,i} \equiv A_i - \mu_A \left ( R_i \right )
\end{align}
and
\begin{align}
	\bm{\varepsilon_{A}} \equiv \left [ \varepsilon_{A,1}, \dots, \varepsilon_{A,n} \right ]' \qquad \bm{\Sigma_{AB}} \equiv \E \left [ \bm{\varepsilon_{A}} \bm{\varepsilon_{B}}' | \bm{R} \right ] = \text{diag} \left ( \sigma^2_{AB} \left ( R_1 \right ), \dots, \sigma^2_{AB} \left ( R_n \right ) \right )
\end{align}
In addition, define the following intermediate data vectors and matrices:
\begin{align}
	\bm{X_{p} \left ( h_n \right )} & \equiv \left [ x_p \left ( R_1 \slash h_n \right ), \dots, x_p \left ( R_n \slash h_n \right ) \right ]' \\
	\bm{S_{q} \left ( h_n \right )} & \equiv \left [ \left ( R_1 \slash h_n \right )^q, \dots, \left ( R_n \slash h_n \right )^q \right ]' \\
	\bm{Z_{+} \left ( h_n \right )} & \equiv \text{diag} \left ( \mathbb{I} \left [ R_1 \geq c \right ] k_{h_n} \left ( R_1 \right ), \dots, \mathbb{I} \left [ R_n \geq c \right ] k_{h_n} \left ( R_n \right ) \right ) \\
	\bm{Z_{-} \left ( h_n \right )} & \equiv \text{diag} \left ( \mathbb{I} \left [ R_1 < c \right ] k_{h_n} \left ( R_1 \right ), \dots, \mathbb{I} \left [ R_n < c \right ] k_{h_n} \left ( R_n \right ) \right )
\end{align}
Finally, given another positive bandwidth sequence $b_n$, define the scaled matrices
\begin{align}
	\Gamma_{+,p} \left ( h_n \right ) & \equiv \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{X_{p} \left ( h_n \right )} \slash n \\
	\Gamma_{-,p} \left ( h_n \right ) & \equiv \bm{X_{p} \left ( h_n \right )}' \bm{Z_{-} \left ( h_n \right )} \bm{X_{p} \left ( h_n \right )} \slash n \\
	\vartheta_{+,p,q} \left ( h_n \right ) & \equiv \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{S_{q} \left ( h_n \right )} \slash n \\
	\vartheta_{-,p,q} \left ( h_n \right ) & \equiv \bm{X_{p} \left ( h_n \right )}' \bm{Z_{-} \left ( h_n \right )} \bm{S_{q} \left ( h_n \right )} \slash n \\
	\Psi_{AB+,p,q} \left ( h_n,b_n \right ) & \equiv \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{\Sigma_{AB}} \bm{Z_{+} \left ( b_n \right )} \bm{X_{q} \left ( b_n \right )} \slash n \\
	\Psi_{AB-,p,q} \left ( h_n,b_n \right ) & \equiv \bm{X_{p} \left ( h_n \right )}' \bm{Z_{-} \left ( h_n \right )} \bm{\Sigma_{AB}} \bm{Z_{-} \left ( b_n \right )} \bm{X_{q} \left ( b_n \right )} \slash n \\
	\Psi_{AB+,p} \left ( h_n \right ) & \equiv \Psi_{AB+,p,p} \left ( h_n,h_n \right ) \\
	\Psi_{AB-,p} \left ( h_n \right ) & \equiv \Psi_{AB-,p,p} \left ( h_n,h_n \right )
\end{align}
It follows that the vector of polynomial estimators in \eqref{polest_plus} and \eqref{polest_minus} can be expressed as
\begin{align}
	\widehat{\beta}_{A+,p} \left ( h_n \right ) & = H_p \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{A} \slash n \\
	\widehat{\beta}_{A-,p} \left ( h_n \right ) & = H_p \left ( h_n \right ) \Gamma^{-1}_{-,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{-} \left ( h_n \right )} \bm{A} \slash n
\end{align}
with $H_p \left ( h_n \right ) \equiv \text{diag} \left ( 1, h_n^{-1}, h_n^{-2}, \dots, h_n^{-p} \right )$. To conclude,
\begin{align}
	\Gamma_{p} & \equiv \int_{0}^{\infty} k \left ( u \right ) x_p \left ( u \right ) x_p \left ( u \right )' du \\
	\vartheta_{p,q} & \equiv \int_{0}^{\infty} k \left ( u \right ) u^q x_p \left ( u \right ) du \\
	\Psi_{p} & \equiv \int_{0}^{\infty} k \left ( u \right )^2 x_p \left ( u \right ) x_p \left ( u \right )' du
\end{align}

\appsubsection{Bias-Corrected Confidence Intervals}

In this section, I derive the asymptotic distribution of the bias-corrected estimator for the target parameter $\theta_{\nu}$. Following a similar approach to \cite{cct2014}, I first linearize the estimator $\widehat{\theta}_{\nu,p} \left ( h_n \right )$ and compute the asymptotic bias and variance of the linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$. Then, I derive the asymptotic distribution of an estimator that subtracts the true (rather than estimated) bias from $\widehat{\theta}_{\nu,p} \left ( h_n \right )$. Before stating the main proposition, it is convenient to define a few recurring bias- and variance-related terms.

\appsubsubsection{Definitions of Bias Terms}\label{app_def_bias}

First, let us define two widely used bias terms:
\begin{align}
	\mathcal{B}_{+,\nu,p,q} \left ( h_n \right ) \equiv \nu! e_{\nu}' \Gamma^{-1}_{+,p} \left ( h_n \right ) \vartheta_{+,p,q} \left ( h_n \right ) \qquad \mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) \equiv \nu! e_{\nu}' \Gamma^{-1}_{-,p} \left ( h_n \right ) \vartheta_{-,p,q} \left ( h_n \right )
\end{align}
As outlined in the proof of part (B) of Lemma S.A.3 in \cite{cct2014supp}, Lemma S.A.1 in the same paper implies that, if $n h_n \to \infty$ and $h_n \to 0$,
\begin{align}
	\mathcal{B}_{+,\nu,p,q} \left ( h_n \right ) = \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right ) \qquad \mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) = \left ( -1 \right )^{\nu+q} \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right )
\end{align}
where $e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q}$ is observed and bounded. In addition, for any random variable $A$, define the following recurring bias terms:
\begin{align}
	\textsc{B}_{A+,\nu,p,q} \left ( h_n \right ) \equiv \frac{\mu^{(q)}_{A+}}{q!} \mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) \qquad \textsc{B}_{A-,\nu,p,q} \left ( h_n \right ) \equiv \frac{\mu^{(q)}_{A-}}{q!} \mathcal{B}_{-,\nu,p,q} \left ( h_n \right )
\end{align}

\appsubsubsection{Definitions of Variance Terms}

Let us define several variance and covariance terms. First, for any random variable $A$, the conditional variances of $\mu^{(\nu)}_{A+,p} \left ( h_n \right )$ and $\mu^{(\nu)}_{A-,p} \left ( h_n \right )$ are, respectively,
\begin{align}
	\mathcal{V}_{AA+,\nu,p} \left ( h_n \right ) \equiv \Var \left [ \mu^{(\nu)}_{A+,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \frac{\nu!^2}{n h_n^{2\nu}} e'_{\nu} \Gamma^{-1}_{+,p} \left ( h_n \right ) \Psi_{AA+,p} \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) e_{\nu} \\
	\mathcal{V}_{AA-,\nu,p} \left ( h_n \right ) \equiv \Var \left [ \mu^{(\nu)}_{A-,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \frac{\nu!^2}{n h_n^{2\nu}} e'_{\nu} \Gamma^{-1}_{-,p} \left ( h_n \right ) \Psi_{AA-,p} \left ( h_n \right ) \Gamma^{-1}_{-,p} \left ( h_n \right ) e_{\nu}
\end{align}
Moreover, by part (V) of Lemma S.A.3 in \cite{cct2014supp}, if $n h_n \to \infty$ and $h_n \to 0$,
\begin{align}
	\mathcal{V}_{AA+,\nu,p} \left ( h_n \right ) = \frac{1}{n h_n^{1+2\nu}} \frac{\sigma^2_{AA+}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \left \{ 1 + o_p \left ( 1 \right ) \right \} \\
	\mathcal{V}_{AA-,\nu,p} \left ( h_n \right ) = \frac{1}{n h_n^{1+2\nu}} \frac{\sigma^2_{AA-}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \left \{ 1 + o_p \left ( 1 \right ) \right \}
\end{align}
where $e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu}$ is observed and bounded. Analogously, for any pair of random variables $A$ and $B$, the conditional covariance between $\mu^{(\nu)}_{A+,p} \left ( h_n \right )$ and $\mu^{(\nu)}_{B+,p} \left ( h_n \right )$ is
\begin{align}
	& \mathcal{V}_{AB+,\nu,p} \left ( h_n \right ) \notag \\
	& \equiv \Cov \left [ \mu^{(\nu)}_{A+,p} \left ( h_n \right ), \mu^{(\nu)}_{B+,p} \left ( h_n \right ) \Big | \bm{R} \right ] \notag \\
	& = n^{-2} \nu!^2 e_{\nu}' H_p \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{\Sigma_{AB}} \bm{Z_{+} \left ( h_n \right )} \bm{X_{p} \left ( h_n \right )} \Gamma^{-1}_{+,p} H_p \left ( h_n \right ) e_{\nu} \notag \\
	& = n^{-2} \nu!^2 h_n^{-2\nu} e_{\nu}' \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{\Sigma_{AB}} \bm{Z_{+} \left ( h_n \right )} \bm{X_{p} \left ( h_n \right )} \Gamma^{-1}_{+,p} e_{\nu} \notag \\
	& = n^{-1} \nu!^2 h_n^{-2\nu} e_{\nu}' \Gamma^{-1}_{+,p} \left ( h_n \right ) \Psi_{AB+,p} \left ( h_n \right ) \Gamma^{-1}_{+,p} e_{\nu} \notag \\
	& = \frac{\nu!^2}{n h_n^{2\nu}} e'_{\nu} \Gamma^{-1}_{+,p} \left ( h_n \right ) \Psi_{AB+,p} \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) e_{\nu}
\end{align}
Symmetrically,
\begin{align}
	\mathcal{V}_{AB-,\nu,p} \left ( h_n \right ) & \equiv \Cov \left [ \mu^{(\nu)}_{A-,p} \left ( h_n \right ), \mu^{(\nu)}_{B-,p} \left ( h_n \right ) \Big | \bm{R} \right ] \notag \\
	& = \frac{\nu!^2}{n h_n^{2\nu}} e'_{\nu} \Gamma^{-1}_{-,p} \left ( h_n \right ) \Psi_{AB-,p} \left ( h_n \right ) \Gamma^{-1}_{-,p} \left ( h_n \right ) e_{\nu}
\end{align}
As above, Lemma S.A.1 in \cite{cct2014supp} implies that, if $n h_n \to \infty$ and $h_n \to 0$,
\begin{align}
	\mathcal{V}_{AB+,\nu,p} \left ( h_n \right ) = \frac{1}{n h_n^{1+2\nu}} \frac{\sigma^2_{AB+}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \left \{ 1 + o_p \left ( 1 \right ) \right \} \\
	\mathcal{V}_{AB-,\nu,p} \left ( h_n \right ) = \frac{1}{n h_n^{1+2\nu}} \frac{\sigma^2_{AB-}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \left \{ 1 + o_p \left ( 1 \right ) \right \}
\end{align}
where $e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu}$ is observed and bounded.

\appsubsubsection{Linearization of the Estimator}\label{app_linearization}

Recall that the target estimand is
\begin{align}
	\theta_{\nu} \equiv \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + \frac{\mu^{(\nu)}_{W+}}{\mu^{(\nu)}_{D+}} - \frac{\mu^{(\nu)}_{W-}}{\mu^{(\nu)}_{D-}}
\end{align}
and the estimator of interest is
\begin{align}
	\widehat{\theta}_{\nu,p} \left ( h_n \right ) \equiv \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) + \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} - \frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )}
\end{align}
Let us define the following ratios of polynomial estimators on either side of the cutoff:
\begin{align}
	\widehat{\eta}_{+,\nu,p} \left ( h_n \right ) \equiv \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} \qquad 	\widehat{\eta}_{-,\nu,p} \left ( h_n \right ) \equiv \frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )}
\end{align}
Their population counterparts are
\begin{align}
	\eta_{+,\nu} \equiv \frac{\mu^{(\nu)}_{W+}}{\mu^{(\nu)}_{D+}} \qquad \eta_{-,\nu} \equiv \frac{\mu^{(\nu)}_{W-}}{\mu^{(\nu)}_{D-}}
\end{align}
To determine the asymptotic behavior of $\widehat{\eta}_{+,\nu,p} \left ( h_n \right )$ and $\widehat{\eta}_{-,\nu,p} \left ( h_n \right )$, I proceed as in \cite{cct2014} and perform a second-order Taylor expansion:
\begin{align}
	\widehat{\eta}_{+,\nu,p} \left ( h_n \right ) - \eta_{+,\nu} & \approx \frac{1}{\mu^{(\nu)}_{D+}} \left ( \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right ) \notag \\
	& - \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} \right ) \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right ) \notag \\
	& + \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right )^2
\end{align}
Analogously,
\begin{align}
	\widehat{\eta}_{-,\nu,p} \left ( h_n \right ) - \eta_{-,\nu} & \approx \frac{1}{\mu^{(\nu)}_{D-}} \left ( \widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right ) - \mu^{(\nu)}_{W-} \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) - \mu^{(\nu)}_{D-} \right ) \notag \\
	& - \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right ) - \mu^{(\nu)}_{W-} \right ) \left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) - \mu^{(\nu)}_{D-} \right ) \notag \\
	& + \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) - \mu^{(\nu)}_{D-} \right )^2
\end{align}
Let us now define a linear term and a remainder term:
\begin{align}
	\widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \left ( \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right ) \\
	R_{+,n} & \equiv \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right )^2 \notag \\
	& - \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} \right ) \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right )
\end{align}
Symmetrically,
\begin{align}
	\widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \left ( \widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right ) - \mu^{(\nu)}_{W-} \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) - \mu^{(\nu)}_{D-} \right ) \\
	R_{-,n} & \equiv \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) - \mu^{(\nu)}_{D-} \right )^2 \notag \\
	& - \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right ) - \mu^{(\nu)}_{W-} \right ) \left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) - \mu^{(\nu)}_{D-} \right )
\end{align}
Clearly,
\begin{align}
	\widehat{\eta}_{+,\nu,p} \left ( h_n \right ) - \eta_{+,\nu} & = \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) + R_{+,n} \\
	\widehat{\eta}_{-,\nu,p} \left ( h_n \right ) - \eta_{-,\nu} & = \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) + R_{-,n}
\end{align}
Finally, define
\begin{align}
	\widetilde{\eta}_{\nu,p} \left ( h_n \right ) \equiv \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) - \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \qquad R_{n} \equiv R_{+,n} - R_{-,n} 
\end{align}
which implies that
\begin{align}
	\widehat{\eta}_{\nu,p} \left ( h_n \right ) = \widetilde{\eta}_{\nu,p} \left ( h_n \right ) + R_{n}
\end{align}
To conclude, the linearized estimator for $\theta_\nu$ is
\begin{align}
	\widetilde{\theta}_{\nu,p} \left ( h_n \right ) \equiv \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) + \widetilde{\eta}_{\nu,p} \left ( h_n \right )
\end{align}

\appsubsubsection{General Theorem: Statement}

\begin{theorem}\label{thm_asydist}
	Suppose Assumptions \ref{ass_moments} and \ref{ass_kernel} hold with $\delta \geq p+2$ and $n h_n \to \infty$.
	\begin{enumerate}[(R)]
		\item If $h_n \to 0$ and $n h_n^{1+2\nu} \to 0$, then
		\begin{align*}
			R_n = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right )
		\end{align*}
	\end{enumerate}
	
	\begin{enumerate}[(B)]
		\item If $h_n \to 0$, then
		\begin{align*}
			\E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right )\\
			& + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		where
		\begin{align*}
			\textsc{B}_{\nu,p,q} \left ( h_n \right ) \equiv \textsc{B}_{Y,\nu,p,q} \left ( h_n \right ) + \widetilde{\textsc{B}}_{\nu,p,q} \left ( h_n \right )
		\end{align*}
		with
		\begin{align*}
			\textsc{B}_{Y,\nu,p,q} \left ( h_n \right ) & \equiv \textsc{B}_{Y+,\nu,p,q} \left ( h_n \right ) - \textsc{B}_{Y-,\nu,p,q} \left ( h_n \right ) \\
			\widetilde{\textsc{B}}_{\nu,p,q} \left ( h_n \right ) &  \equiv \widetilde{\textsc{B}}_{+,\nu,p,q} \left ( h_n \right ) - \widetilde{\textsc{B}}_{-,\nu,p,q} \left ( h_n \right )
		\end{align*}
		and
		\begin{align*}
			\widetilde{\textsc{B}}_{+,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \textsc{B}_{W+,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \textsc{B}_{D+,\nu,p,q} \left ( h_n \right ) \\
			\widetilde{\textsc{B}}_{-,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \textsc{B}_{W-,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \textsc{B}_{D-,\nu,p,q} \left ( h_n \right )
		\end{align*}
	\end{enumerate}
	
	\begin{enumerate}[(V)]
		\item If $h_n \to 0$, then
		\begin{align*}
			\textsc{V}_{\nu,p} \left ( h_n \right ) \equiv \Var \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \textsc{V}_{Y,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{\nu,p} \left ( h_n \right ) + 2 \widetilde{\textsc{V}}_{Y,\nu,p} \left ( h_n \right )
		\end{align*}
		where
		\begin{align*}
			\textsc{V}_{Y,\nu,p} \left ( h_n \right ) & \equiv \mathcal{V}_{YY+,\nu,p} \left ( h_n \right ) + \mathcal{V}_{YY-,\nu,p} \left ( h_n \right ) \\
			\widetilde{\textsc{V}}_{\nu,p} \left ( h_n \right ) & \equiv \widetilde{\textsc{V}}_{+,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{-,\nu,p} \left ( h_n \right ) \\
			\widetilde{\textsc{V}}_{Y,\nu,p} \left ( h_n \right ) & \equiv \widetilde{\textsc{V}}_{Y+,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{Y-,\nu,p} \left ( h_n \right )
		\end{align*}
		with 
		\begin{align*}
			\widetilde{\textsc{V}}_{+,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{V}_{WW+,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \mathcal{V}_{DD+,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{V}_{WD+,\nu,p} \left ( h_n \right ) \\
			\widetilde{\textsc{V}}_{-,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{V}_{WW-,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{W-} \right )^2}{\left ( \mu^{(\nu)}_{D-} \right )^4} \mathcal{V}_{DD-,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \mathcal{V}_{WD-,\nu,p} \left ( h_n \right )
		\end{align*}
		and 
		\begin{align*}
			\widetilde{\textsc{V}}_{Y+,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \mathcal{V}_{YW+,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{V}_{YD+,\nu,p} \left ( h_n \right ) \\
			\widetilde{\textsc{V}}_{Y-,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \mathcal{V}_{YW-,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{V}_{YD-,\nu,p} \left ( h_n \right )
		\end{align*}
	\end{enumerate}
	
	\begin{enumerate}[(D)]
		\item If $n h_n^{2p+5} \to 0$ and $n h_n^{1+2\nu} \to 0$, then
		\begin{align*}
			T^{\textsc{bc}} \left ( h_n \right ) \equiv \frac{\widetilde{\theta}_{\nu,p} \left ( h_n \right ) - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right ) - h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right )}{\sqrt{\textsc{V}_{\nu,p} \left ( h_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right ) 
		\end{align*}
	\end{enumerate}
	
\end{theorem}

\appsubsubsection{General Theorem: Proof}
	
	\begin{enumerate}[(R)]
		\item Consider $R_{+,n}$. If $h_n \to 0$ and $n h_n^{2p+5} \to 0$,  part (D) of Lemma S.A.3 in \cite{cct2014supp} implies that
		\begin{align*}
			T_{W+,\nu,p} \left ( h_n \right ) \equiv \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} - h_n^{p+1-\nu} \frac{\mu^{(p+1)}_{+}}{(p+1)!} \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\sqrt{\mathcal{V}_{WW+,\nu,p} \left ( h_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right )
		\end{align*}
		with
		\begin{align*}
			\mathcal{B}_{+,\nu,p,r} \left ( h_n \right ) = \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,r} + o_p \left ( 1 \right )
		\end{align*}
		and
		\begin{align*}
			\mathcal{V}_{WW+,\nu,p} \left ( h_n \right ) = \frac{1}{n h_n^{1+2\nu}} \frac{\sigma^2_{WW+}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \left \{ 1 + o_p \left ( 1 \right ) \right \}
		\end{align*}
		Thus,
		\begin{align*}
			T_{W+,\nu,p} \left ( h_n \right ) = O_p \left ( 1 \right ) & \iff \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} = O_p \left ( h_n^{p+1-\nu} \right ) + O_p \left ( \sqrt{\frac{1}{n h_n^{1+2\nu}}} \right ) \\
			& \iff \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} = O_p \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1+2\nu}}} \right )
		\end{align*}
		Following similar steps,
		\begin{align*}
			\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} = O_p \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1+2\nu}}} \right )
		\end{align*}
		As a consequence, their product is
		\begin{align*}
			\left ( \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} \right ) \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right ) & = O_p \left ( \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1+2\nu}}} \right )^2 \right ) \\
			& = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right )
		\end{align*}
		where the second equality follows from the fact that, for any $a,b \in \mathbb{R}_{+}$, $a + b \leq \left ( \sqrt{a} + \sqrt{b} \right )^2 \leq 2 \left ( a + b \right )$ and thus $\left ( \sqrt{a} + \sqrt{b} \right )^2 = O \left ( a + b \right )$. Analogously,
		\begin{align*}
			\left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right )^2 = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right )
		\end{align*}
		Finally, because $\frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3}$ and $\frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2}$ are deterministic constants,
		\begin{align*}
			R_{+,n} & = \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right ) + \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right ) \\
			& = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right ) + O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right ) \\
			& = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right )
		\end{align*}
		which is the desired result. The proof for $R_{-,n}$ is symmetric. Combining results,
		\begin{align*}
			R_n & \equiv R_{+,n} - R_{-,n} \\
			& = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right ) - O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right ) \\
			& = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right )
		\end{align*}
	\end{enumerate}
	
	\begin{enumerate}[(B)]
		\item First, consider $\widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right )$ and $\widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right )$. Part (B) of Lemma S.A.3 in \cite{cct2014supp} implies that, as $h_n \to 0$, their conditional expectations are, respectively,
		\begin{align*}
			\E \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \mu^{(\nu)}_{Y+} + h^{p+1-\nu}_{n} \textsc{B}_{Y+,\nu,p,p+1} \left ( h_n \right ) \\
			& + h^{p+2-\nu}_{n} \textsc{B}_{Y+,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right ) \\
			\E \left [ \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{Y-,\nu,p,p+1} \left ( h_n \right ) \\
			& + h^{p+2-\nu}_{n} \textsc{B}_{Y-,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		Define
		\begin{align*}
			\textsc{B}_{Y,\nu,p,q} \left ( h_n \right ) & \equiv \textsc{B}_{Y+,\nu,p,q} \left ( h_n \right ) - \textsc{B}_{Y-,\nu,p,q} \left ( h_n \right )
		\end{align*}
		Then, by the linearity of the expectation operator,
		\begin{align*}
			\E \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{Y,\nu,p,p+1} \left ( h_n \right )\\
			& + h^{p+2-\nu}_{n} \textsc{B}_{Y,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		Second, consider $\widetilde{\eta}_{+,\nu,p} \left ( h_n \right )$. Part (B) of Lemma S.A.3 in \cite{cct2014supp} implies that, as $h_n \to 0$,
		\begin{align*}
			\E \left [ \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \mu^{(\nu)}_{W+} \Big | \bm{R} \right ] & = h^{p+1-\nu}_{n} \textsc{B}_{W+,\nu,p,p+1} \left ( h_n \right ) \\
			& + h^{p+2-\nu}_{n} \textsc{B}_{W+,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right ) \\
			\E \left [ \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \Big | \bm{R} \right ] & = h^{p+1-\nu}_{n} \textsc{B}_{D+,\nu,p,p+1} \left ( h_n \right ) \\
			& + h^{p+2-\nu}_{n} \textsc{B}_{D+,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		Then, by the linearity of the expectation operator,
		\begin{align*}
			\E \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) | \bm{R} \right ] & = \frac{1}{\mu^{(\nu)}_{D+}} \left ( h^{p+1-\nu}_{n} \textsc{B}_{W+,\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \textsc{B}_{W+,\nu,p,p+2} \left ( h_n \right ) \right ) \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \left ( h^{p+1-\nu}_{n} \textsc{B}_{D+,\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \textsc{B}_{D+,\nu,p,p+2} \left ( h_n \right ) \right ) \\
			& + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		Further define
		\begin{align*}
			\widetilde{\textsc{B}}_{+,\nu,p,r} \left ( h_n \right ) \equiv \frac{1}{\mu^{(\nu)}_{D+}} \textsc{B}_{W+,\nu,p,r} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \textsc{B}_{D+,\nu,p,r} \left ( h_n \right )
		\end{align*}
		Thus,
		\begin{align*}
			\E \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) | \bm{R} \right ] & = h^{p+1-\nu}_{n} \widetilde{\textsc{B}}_{+,\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \widetilde{\textsc{B}}_{+,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		A similar set of arguments yields
		\begin{align*}
			\E \left [ \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) | \bm{R} \right ] & = h^{p+1-\nu}_{n} \widetilde{\textsc{B}}_{-,\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \widetilde{\textsc{B}}_{-,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		where
		\begin{align*}
			\widetilde{\textsc{B}}_{-,\nu,p,r} \left ( h_n \right ) \equiv \frac{1}{\mu^{(\nu)}_{D-}} \textsc{B}_{W-,\nu,p,r} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \textsc{B}_{D-,\nu,p,r} \left ( h_n \right )
		\end{align*}
		Also define
		\begin{align*}
			\widetilde{\textsc{B}}_{\nu,p,r} \left ( h_n \right ) \equiv \widetilde{\textsc{B}}_{+,\nu,p,r} \left ( h_n \right ) - \widetilde{\textsc{B}}_{-,\nu,p,r} \left ( h_n \right )
		\end{align*}
		Leveraging the linearity of the expectation operator,
		\begin{align*}
			\E \left [ \widetilde{\eta}_{\nu,p} \left ( h_n \right ) | \bm{R} \right ] & = h^{p+1-\nu}_{n} \widetilde{\textsc{B}}_{\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \widetilde{\textsc{B}}_{\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		Finally, define
		\begin{align*}
			\textsc{B}_{\nu,p,r} \left ( h_n \right ) \equiv \textsc{B}_{Y,\nu,p,r} \left ( h_n \right ) + \widetilde{\textsc{B}}_{\nu,p,r} \left ( h_n \right )
		\end{align*}
		Since $\widetilde{\theta}_{\nu,p} \left ( h_n \right ) \equiv \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) + \widetilde{\eta}_{\nu,p} \left ( h_n \right )$, the linearity of the expectation implies that
		\begin{align*}
			\E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \\
			& + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
		\end{align*}
		which completes the proof.
	\end{enumerate}
	
	\begin{enumerate}[(V)]
		\item The conditional variance of the linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ can be written as
		\begin{align*}
			\Var \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \Var \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) \Big | \bm{R} \right ] + \Var \left [ \widetilde{\eta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& + 2 \times \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) , \widetilde{\eta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ]
		\end{align*}
		First, consider the conditional variance of $\widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right )$. Notice that
		\begin{align*}
			\textsc{V}_{Y,\nu,p} \left ( h_n \right ) & \equiv \Var \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \Var \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) \Big | \bm{R} \right ] + \Var \left [ \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) \Big | \bm{R} \right ]
		\end{align*}
		where the second equality follows from the bilinearity of the covariance and the conditional independence of observations on either side of the cutoff $c$. Furthermore, part (V) of Lemma S.A.3 in \cite{cct2014supp} implies that, as $h_n \to 0$,
		\begin{align*}
			\textsc{V}_{Y,\nu,p} \left ( h_n \right ) = \mathcal{V}_{YY+,\nu,p} \left ( h_n \right ) + \mathcal{V}_{YY-,\nu,p} \left ( h_n \right )
		\end{align*}
		Second, consider the conditional variance of $\widetilde{\eta}_{\nu,p} \left ( h_n \right )$. By its definition,
		\begin{align*}
			\Var \left [ \widetilde{\eta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \Var \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) - \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \Var \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] + \Var \left [ \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ]
		\end{align*}
		where, as above, the second equality follows from the bilinearity of the covariance and the conditional independence of observations on either side of the cutoff $c$. Furthermore, by the bilinearity of the covariance,
		\begin{align*}
			\Var \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Var \left [ \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) \Big | \bm{R} \right ] + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \Var \left [ \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{2 \mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \Cov \left [ \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ), \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \Big | \bm{R}\right ] \\
			\Var \left [ \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \Var \left [ \widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right ) \Big | \bm{R} \right ] + \frac{\left ( \mu^{(\nu)}_{W-} \right )^2}{\left ( \mu^{(\nu)}_{D-} \right )^4} \Var \left [ \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{2 \mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \Cov \left [ \widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right ), \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \Big | \bm{R}\right ]
		\end{align*}
		Leveraging part (V) of Lemma S.A.3 in \cite{cct2014supp}, define the following variance terms:
		\begin{align*}
			\widetilde{\textsc{V}}_{+,\nu,p} \left ( h_n \right ) & \equiv \Var \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{V}_{WW+,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \mathcal{V}_{DD+,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{V}_{WD+,\nu,p} \left ( h_n \right ) \\
			\widetilde{\textsc{V}}_{-,\nu,p} \left ( h_n \right ) & \equiv \Var \left [ \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{V}_{WW-,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{W-} \right )^2}{\left ( \mu^{(\nu)}_{D-} \right )^4} \mathcal{V}_{DD-,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \mathcal{V}_{WD-,\nu,p} \left ( h_n \right )
		\end{align*}
		Thus, the conditional variance of $\widetilde{\eta}_{\nu,p} \left ( h_n \right )$ is
		\begin{align*}
			\widetilde{\textsc{V}}_{\nu,p} \left ( h_n \right ) \equiv \Var \left [ \widetilde{\eta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \widetilde{\textsc{V}}_{+,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{-,\nu,p} \left ( h_n \right )
		\end{align*}
		Finally, consider the covariance of $\widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right )$ and $\widetilde{\eta}_{\nu,p} \left ( h_n \right )$:
		\begin{align*}
			\Cov \left [\widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) , \widetilde{\eta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ]	& = \Cov \left [  \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ), \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ] \\
			& + \Cov \left [ \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ), \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ] 
		\end{align*}
		where the second equality follows from the bilinearity of the covariance and the conditional independence of observations on either side of the cutoff $c$. Expanding the covariances,
		\begin{align*}
			\Cov \left [  \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ), \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ] & = \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [  \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ), \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ), \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \Big | \bm{R}\right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} \mathcal{V}_{YW+,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{V}_{YD+,\nu,p} \left ( h_n \right ) \\
			\Cov \left [  \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ), \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ] & = \frac{1}{\mu^{(\nu)}_{D-}} \Cov \left [  \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ), \widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ), \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \Big | \bm{R}\right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D-}} \mathcal{V}_{YW-,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{V}_{YD-,\nu,p} \left ( h_n \right )
		\end{align*}
		For compactness, define
		\begin{align*}
			\widetilde{\textsc{V}}_{Y+,\nu,p} \left ( h_n \right ) & \equiv \Cov \left [  \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ), \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} \mathcal{V}_{YW+,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{V}_{YD+,\nu,p} \left ( h_n \right ) \\
			\widetilde{\textsc{V}}_{Y-,\nu,p} \left ( h_n \right ) & \equiv \Cov \left [  \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ), \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D-}} \mathcal{V}_{YW-,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{V}_{YD-,\nu,p} \left ( h_n \right )
		\end{align*}
		Then the conditional covariance of of $\widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right )$ and $\widetilde{\eta}_{\nu,p} \left ( h_n \right )$ can be expressed as
		\begin{align*}
			\widetilde{\textsc{V}}_{Y,\nu,p} \left ( h_n \right ) \equiv \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,p} \left ( h_n \right ) - \widehat{\mu}^{(\nu)}_{Y-,p} \left ( h_n \right ) , \widetilde{\eta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R}\right ] = \widetilde{\textsc{V}}_{Y+,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{Y-,\nu,p} \left ( h_n \right )
		\end{align*}
		To conclude, the conditional variance of the linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ is
		\begin{align*}
			\textsc{V}_{\nu,p} \left ( h_n \right ) & \equiv \Var \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \textsc{V}_{Y,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{\nu,p} \left ( h_n \right ) + 2 \widetilde{\textsc{V}}_{Y,\nu,p} \left ( h_n \right )
		\end{align*}
		which completes the proof.
	\end{enumerate}
	
	\begin{enumerate}[(D)]
		\item Recall that, for any random variable $A$,
		\begin{align*}
			\widehat{\mu}^{(\nu)}_{A+,p} \left ( h_n \right ) = \nu! e_\nu' \widehat{\beta}_{A+,p} \left ( h_n \right ) \qquad \widehat{\mu}^{(\nu)}_{A-,p} \left ( h_n \right ) = \nu! e_\nu' \widehat{\beta}_{A-,p} \left ( h_n \right )
		\end{align*}
		with
		\begin{align*}
			\widehat{\beta}_{A+,p} \left ( h_n \right ) & = H_p \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{A} \slash n \\
			\widehat{\beta}_{A-,p} \left ( h_n \right ) & = H_p \left ( h_n \right ) \Gamma^{-1}_{-,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{-} \left ( h_n \right )} \bm{A} \slash n
		\end{align*}
		Then, define
		\begin{align*}
			\widetilde{\beta}_{+,p} \left ( h_n \right ) & \equiv \widehat{\beta}_{Y+,p} \left ( h_n \right ) + \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\beta}_{W+,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\beta}_{D+,p} \left ( h_n \right ) \\
			& = H_p \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \left ( \bm{Y} + \frac{1}{\mu^{(\nu)}_{D+}} \bm{W} - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \bm{D} \right ) \slash n \\
			\widetilde{\beta}_{-,p} \left ( h_n \right ) & \equiv \widehat{\beta}_{Y-,p} \left ( h_n \right ) + \frac{1}{\mu^{(\nu)}_{D-}} \widehat{\beta}_{W-,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \widehat{\beta}_{D-,p} \left ( h_n \right ) \\
			& = H_p \left ( h_n \right ) \Gamma^{-1}_{-,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{-} \left ( h_n \right )} \left ( \bm{Y} + \frac{1}{\mu^{(\nu)}_{D-}} \bm{W} - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \bm{D} \right ) \slash n
		\end{align*}
		Further define
		\begin{align*}
			\widetilde{\mu}^{(\nu)}_{+,p} \left ( h_n \right ) \equiv \nu! e_\nu' \widetilde{\beta}_{+,p} \left ( h_n \right ) \qquad \widetilde{\mu}^{(\nu)}_{-,p} \left ( h_n \right ) \equiv \nu! e_\nu' \widetilde{\beta}_{-,p} \left ( h_n \right )
		\end{align*}
		In words, the linearized estimator of interest $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ is numerically equivalent to $\widetilde{\mu}^{(\nu)}_{+,p} \left ( h_n \right ) - \widetilde{\mu}^{(\nu)}_{-,p} \left ( h_n \right )$, the sharp regression discontinuity estimator that uses $Y + \frac{1}{\mu^{(\nu)}_{D}} W - \frac{\mu^{(\nu)}_{W}}{\left ( \mu^{(\nu)}_{D} \right )^2} D$ as the outcome. Thus, by part (D) of Lemma A.1 in \cite{cct2014supp}, if $n h_n^{2p+5} \to 0$,
		\begin{align*}
			\frac{\widetilde{\theta}_{\nu,p} \left ( h_n \right ) - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right ) - h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right )}{\sqrt{\textsc{V}_{\nu,p} \left ( h_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right ) 
		\end{align*}
		The bias-corrected test statistic for the non-linearized estimator $\widehat{\theta}_{\nu,p} \left ( h_n \right )$ is
		\begin{align*}
			T^{\textsc{bc}} \left ( h_n \right ) & \equiv \frac{\widehat{\theta}_{\nu,p} \left ( h_n \right ) - \theta_{\nu} - h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right )}{\sqrt{\textsc{V}_{\nu,p} \left ( h_n \right )}} \\
			& = \frac{\widetilde{\theta}_{\nu,p} \left ( h_n \right ) - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right ) - h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right )}{\sqrt{\textsc{V}_{\nu,p} \left ( h_n \right )}} + \frac{R_n}{\sqrt{\textsc{V}_{\nu,p} \left ( h_n \right )}}
		\end{align*}
		Recall that part (R) of this theorem states that
		\begin{align*}
			R_n = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right )
		\end{align*}
		Leveraging part (V) of this theorem,
		\begin{align*}
			\frac{R_n}{\sqrt{\textsc{V}_{\nu,p} \left ( h_n \right )}} & = O_p \left ( \sqrt{n h_n^{1+2\nu}} h_n^{2 (p+1-\nu)} + \frac{\sqrt{n h_n^{1+2\nu}}}{n h_n^{1+2\nu}} \right ) \\
			& = O_p \left ( \sqrt{n h_n^{1+2\nu} h_n^{4 (p+1-\nu)}} + \frac{1}{\sqrt{n h_n^{1+2\nu}}} \right ) \\
			& = O_p \left ( \sqrt{n h_n^{2p + 5} h_n^{2 (p-\nu)}} + \frac{1}{\sqrt{n h_n^{1+2\nu}}} \right ) \\
			& = o_p \left ( 1 \right )
		\end{align*}
		because $p \geq \nu$ and, by assumption, $n h_n^{1+2\nu} \to \infty$ and $n h_n^{2p+5} \to 0$. Thus, 
		\begin{align*}
			T^{\textsc{bc}} \left ( h_n \right ) & \equiv \frac{\widehat{\theta}_{\nu,p} \left ( h_n \right ) - \theta_{\nu} - h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right )}{\sqrt{\textsc{V}_{\nu,p} \left ( h_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right )
		\end{align*}
		which completes the proof.
	\end{enumerate}
	

\appsubsection{Robust Bias-Corrected Confidence Intervals}

In this section, I derive the asymptotic distribution of the robust bias-corrected estimator for the target parameter $\theta_\nu$. This estimator is defined as the difference between $\widehat{\theta}_{\nu,p} \left ( h_n \right )$ and its estimated (rather than true) bias. Before stating the main proposition, it is convenient to define a few additional bias- and variance-related terms.

\appsubsubsection{Definitions of Bias Terms}

To begin with, recall that the bias estimand of interest is 
\begin{align}
	\textsc{B}_{\nu,p,q} \left ( h_n \right ) \equiv \textsc{B}_{Y,\nu,p,q} \left ( h_n \right ) + \widetilde{\textsc{B}}_{\nu,p,q} \left ( h_n \right )
\end{align}
where
\begin{align}
	\textsc{B}_{Y,\nu,p,q} \left ( h_n \right ) & \equiv \textsc{B}_{Y+,\nu,p,q} \left ( h_n \right ) - \textsc{B}_{Y-,\nu,p,q} \left ( h_n \right ) \\
	\widetilde{\textsc{B}}_{\nu,p,q} \left ( h_n \right ) & \equiv \widetilde{\textsc{B}}_{+,\nu,p,q} \left ( h_n \right ) - \widetilde{\textsc{B}}_{-,\nu,p,q} \left ( h_n \right )
\end{align}
with
\begin{align}
	\widetilde{\textsc{B}}_{+,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \textsc{B}_{W+,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \textsc{B}_{D+,\nu,p,q} \left ( h_n \right ) \\
	\widetilde{\textsc{B}}_{-,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \textsc{B}_{W-,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \textsc{B}_{D-,\nu,p,q} \left ( h_n \right )
\end{align}
Also recall that, for any random variable $A$, 
\begin{align}
	\textsc{B}_{A+,\nu,p,q} \left ( h_n \right ) \equiv \frac{\mu^{(q)}_{A+}}{q!} \mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) \qquad \textsc{B}_{A-,\nu,p,q} \left ( h_n \right ) \equiv \frac{\mu^{(q)}_{A-}}{q!} \mathcal{B}_{-,\nu,p,q} \left ( h_n \right )
\end{align}
where
\begin{align}
	\mathcal{B}_{+,\nu,p,q} \left ( h_n \right ) & \equiv \nu! e_{\nu}' \Gamma^{-1}_{+,p} \left ( h_n \right ) \vartheta_{+,p,q} \left ( h_n \right ) = \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right ) \\
	\mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) & \equiv \nu! e_{\nu}' \Gamma^{-1}_{-,p} \left ( h_n \right ) \vartheta_{-,p,q} \left ( h_n \right ) = \left ( -1 \right )^{\nu+q} \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right )
\end{align}
with the second equalities following from applications of Lemma S.A.1 in \cite{cct2014supp}. Finally, for any random variable $A$, define the following recurring bias-corrected bias terms:
\begin{align}
	\textsc{B}^{\textsc{bc}}_{A+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{A+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
	\textsc{B}^{\textsc{bc}}_{A-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{A-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
\end{align}
Natural estimators for $\textsc{B}_{A+,\nu,p,q} \left ( h_n \right )$ and $\textsc{B}_{A-,\nu,p,q} \left ( h_n \right )$ are the following:
\begin{align}
	\widehat{\textsc{B}}_{A+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\widehat{\mu}^{(p+1)}_{A+,q} \left ( b_n \right )}{\left ( p+1 \right )!} \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )\\
	\widehat{\textsc{B}}_{A-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\widehat{\mu}^{(p+1)}_{A+,q} \left ( b_n \right )}{\left ( p+1 \right )!} \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )
\end{align}
with $h_n$ and $b_n$ denoting the main and pilot bandwidths, respectively. Thus, an estimator for $\textsc{B}_{Y,\nu,p,q} \left ( h_n \right )$ is
\begin{align}
	\widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n \right ) & \equiv \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n \right ) - \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n \right )
\end{align}
In addition, define two distinct estimators for the bias of the linearized estimator $\widetilde{\eta} \left ( h_n \right )$. First, consider
\begin{align}
	\widecheck{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
with
\begin{align}
	\widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \widehat{\textsc{B}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \widehat{\textsc{B}}_{D-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
Second, define an estimator that replaces population conditional means with their sample counterparts:
\begin{align}
	\widehat{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widehat{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) - \widehat{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
with
\begin{align}
	\widehat{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widehat{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )} \widehat{\textsc{B}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{D-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
Thus, two candidate estimands for the bias of the linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ are the following:
\begin{align}
	\widecheck{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) \\
	\widehat{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) + \widehat{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right )
\end{align}

\appsubsubsection{Definitions of Bias-Corrected Estimators}

The bias-corrected estimator for the target parameter $\widehat{\theta}_{\nu,p} \left ( h_n \right )$ is
\begin{align}
	\widehat{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \equiv \widehat{\theta}_{\nu,p} \left ( h_n \right ) - h_n^{p+1-\nu} \widehat{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right )
\end{align}
Analogously, define the bias-corrected linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ as
\begin{align}
	\widetilde{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \equiv \widetilde{\theta}_{\nu,p} \left ( h_n \right ) - h_n^{p+1-\nu} \widecheck{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right )
\end{align}
Recall that
\begin{align}
	R_n \equiv \widehat{\eta}_{\nu,p} \left ( h_n \right ) - \widetilde{\eta}_{\nu,p} \left ( h_n \right ) = \widehat{\theta}_{\nu,p} \left ( h_n \right ) - \widetilde{\theta}_{\nu,p} \left ( h_n \right )
\end{align}
Similarly, the difference between bias-corrected estimators is
\begin{align}
	\widehat{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) & = \widehat{\theta}_{\nu,p} \left ( h_n \right ) - \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \notag \\
	& - h_n^{p+1-\nu} \left ( \widehat{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right ) \right ) \\
	& = R_n - h_n^{p+1-\nu} \left ( \widehat{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) \right )
\end{align}
where the last equality follows from the fact that $\widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n, b_n \right )$ enters additively in both definitions of estimated bias. Finally, define
\begin{align}
	R^{\textsc{bc}}_{n} \equiv h_n^{p+1-\nu} \left ( \widehat{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) \right )
\end{align}

\appsubsubsection{Definitions of Variance Terms}

In this section, I define general covariance terms involving estimators of conditional derivatives of random variables, one being estimated using the main bandwidth $h_n$ and the other using the pilot bandwidth $b_n$. Recall that, for any pair of random variables $A$ and $B$,
\begin{align}
	\Cov \left [ \widehat{\mu}^{(\nu)}_{A+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{B+,q} \left ( b_n \right ) \Big | \bm{R} \right ] = \Cov \left [ \nu! e_{\nu}' \widehat{\beta}_{A+,p} \left ( h_n \right ), \left ( p+1 \right )! e_{p+1}' \widehat{\beta}_{B+,q} \left ( b_n \right ) \Big | \bm{R}\right ]
\end{align}
with
\begin{align}
	\widehat{\beta}_{A+,p} \left ( h_n \right ) & = H_p \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{A} \slash n \\
	\widehat{\beta}_{B+,q} \left ( b_n \right ) & = H_q \left ( h_n \right ) \Gamma^{-1}_{+,q} \left ( b_n \right ) \bm{X_{q} \left ( b_n \right )}' \bm{Z_{+} \left ( b_n \right )} \bm{B} \slash n
\end{align}
Then
\begin{align}
	\mathcal{C}_{AB+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Cov \left [ \widehat{\mu}^{(\nu)}_{A+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{B+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \notag \\
	& = n^{-2} \nu! \left ( p+1 \right )! e_{\nu}' H_p \left ( h_n \right ) \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{\Sigma_{AB}} \notag \\
	& \times \bm{Z_{+} \left ( b_n \right )} \bm{X_{q} \left ( b_n \right )} \Gamma^{-1}_{+,q} \left ( b_n \right ) H_q \left ( b_n \right ) e_{p+1} \\
	& = n^{-2} \nu! \left ( p+1 \right )! h_n^{-\nu} b_n^{-(p+1)} e_{\nu}' \Gamma^{-1}_{+,p} \left ( h_n \right ) \bm{X_{p} \left ( h_n \right )}' \bm{Z_{+} \left ( h_n \right )} \bm{\Sigma_{AB}} \notag \\
	& \times \bm{Z_{+} \left ( b_n \right )} \bm{X_{q} \left ( b_n \right )} \Gamma^{-1}_{+,q} \left ( b_n \right ) H_q \left ( b_n \right ) e_{p+1} \\
	& = n^{-1} \nu! \left ( p+1 \right )! h_n^{-\nu} b_n^{-(p+1)} e_{\nu}' \Gamma^{-1}_{+,p} \left ( h_n \right ) \Psi_{AB+,p,q} \left ( h_n, b_n \right ) \Gamma^{-1}_{+,q} \left ( b_n \right ) e_{p+1} \\
	& = \frac{\nu! \left ( p+1 \right )!}{n h_n^{\nu} b_n^{p+1}} e'_{\nu} \Gamma^{-1}_{+,p} \left ( h_n \right ) \Psi_{AB+,p,q} \left ( h_n, b_n \right ) \Gamma^{-1}_{+,q} \left ( b_n \right ) e_{p+1} \\
	& = \frac{1}{n h_n^{\nu} b_n^{p+1}} \frac{\sigma^2_{AB+}}{f_R \left ( c \right )} \nu! \left ( p+1 \right )! e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{q} e_{p+1} \left \{ 1 + o_p \left ( 1 \right ) \right \}
\end{align}
where the last equality follows from an application of Lemma S.A.1 in \cite{cct2014supp}. Symmetrically,
\begin{align}
	\mathcal{C}_{AB-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Cov \left [ \widehat{\mu}^{(\nu)}_{A-,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{B-,q} \left ( b_n \right ) \Big | \bm{R} \right ] \notag \\
	& = \frac{\nu! \left ( p+1 \right )!}{n h_n^{\nu} b_n^{p+1}} e'_{\nu} \Gamma^{-1}_{-,p} \left ( h_n \right ) \Psi_{AB-,p,q} \left ( h_n, b_n \right ) \Gamma^{-1}_{-,q} \left ( b_n \right ) e_{p+1} \\
	& = \frac{1}{n h_n^{\nu} b_n^{p+1}} \frac{\sigma^2_{AB-}}{f_R \left ( c \right )} \nu! \left ( p+1 \right )! e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{q} e_{p+1} \left \{ 1 + o_p \left ( 1 \right ) \right \}
\end{align}

\appsubsubsection{General Theorem: Statement}

\begin{theorem}\label{thm_asydist_bias}
	Suppose Assumptions \ref{ass_moments} and \ref{ass_kernel} hold with $\delta \geq p+2$, and $n \min \left \{ h_n, b_n \right \} \to \infty$.
	
	\begin{enumerate}[(R)]
		\item If $h_n \to 0$ and $n h_n^{1+2\nu} \to \infty$, and provided that $\kappa b_n < \kappa_0$, then
		\begin{align*}
			R^{\textsc{bc}}_{n} = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{h_n^{p+1-\nu}}{\sqrt{n h_n^{1 + 2\nu}}} \right ) O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right )
		\end{align*}
	\end{enumerate}
	
	\begin{enumerate}[(B)]
		\item If $\max \left \{ h_n, b_n \right \} \to 0$, then
		\begin{align*}
			\E \left [ \widetilde{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] & = h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) \left \{ 1 + o_p \left ( 1 \right ) \right \} \notag \\
			& + h^{p+1-\nu}_{n} b_n^{q-p} \textsc{B}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \left \{ 1 + o_p \left ( 1 \right ) \right \}
		\end{align*}
		where
		\begin{align*}
			\textsc{B}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) & \equiv \textsc{B}^{\textsc{bc}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) + \widetilde{\textsc{B}}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		with
		\begin{align*}
			\textsc{B}^{\textsc{bc}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \textsc{B}^{\textsc{bc}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) - \textsc{B}^{\textsc{bc}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widetilde{\textsc{B}}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widetilde{\textsc{B}}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) - \widetilde{\textsc{B}}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		and
		\begin{align*}
			\widetilde{\textsc{B}}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \textsc{B}^{\textsc{bc}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \textsc{B}^{\textsc{bc}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widetilde{\textsc{B}}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \textsc{B}^{\textsc{bc}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \textsc{B}^{\textsc{bc}}_{D-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		
	\end{enumerate}
	
	\begin{enumerate}[(V)]
		\item If $\max \left \{ h_n, b_n \right \} \to 0$, then
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \equiv \Var \left [ \widetilde{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] = \textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) + \textsc{V}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		where
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \textsc{V}_{+,\nu,p} \left ( h_n \right ) + h_n^{2(p+1-\nu)} \textsc{V}_{+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \notag \\
			& - 2 h_n^{p+1-\nu} \textsc{C}_{+,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			\textsc{V}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \textsc{V}_{-,\nu,p} \left ( h_n \right ) + h_n^{2(p+1-\nu)} \textsc{V}_{-,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \notag \\
			& - 2 h_n^{p+1-\nu} \textsc{C}_{-,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		with
		\begin{align*}
			\textsc{V}_{+,u,v} \left ( x_n \right ) & \equiv \mathcal{V}_{YY+,u,v} \left ( x_n \right ) + \widetilde{\textsc{V}}_{+,u,v} \left ( x_n \right ) + 2 \widetilde{\textsc{V}}_{Y+,u,v} \left ( x_n \right ) \\
			\textsc{V}_{-,u,v} \left ( x_n \right ) & \equiv \mathcal{V}_{YY-,u,v} \left ( x_n \right ) + \widetilde{\textsc{V}}_{-,u,v} \left ( x_n \right ) + 2 \widetilde{\textsc{V}}_{Y-,u,v} \left ( x_n \right ) \\
			\textsc{C}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widetilde{\textsc{C}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widetilde{\textsc{C}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\textsc{C}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widetilde{\textsc{C}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widetilde{\textsc{C}}_{-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		and
		\begin{align*}
			\widetilde{\textsc{C}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \mathcal{C}_{YY+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} \mathcal{C}_{YW+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{YD+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widetilde{\textsc{C}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \mathcal{C}_{YY-,\nu,p,q} \left ( h_n, b_n \right ) \\
			& + \frac{1}{\mu^{(\nu)}_{D-}} \mathcal{C}_{YW-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{C}_{YD-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		and
		\begin{align*}
			\widetilde{\textsc{C}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \mathcal{C}_{WY+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{DY+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& + \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{WW+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{C}_{DW+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{C}_{WD+,\nu,p,q} \left ( h_n, b_n \right ) + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \mathcal{C}_{DD+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widetilde{\textsc{C}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \mathcal{C}_{WY-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{C}_{DY-,\nu,p,q} \left ( h_n, b_n \right ) \\
			& + \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{C}_{WW-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \mathcal{C}_{DW-,\nu,p,q} \left ( h_n, b_n \right ) \\
			& - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \mathcal{C}_{WD-,\nu,p,q} \left ( h_n, b_n \right ) + \frac{\left ( \mu^{(\nu)}_{W-} \right )^2}{\left ( \mu^{(\nu)}_{D-} \right )^4} \mathcal{C}_{DD-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
	\end{enumerate}
	
	\begin{enumerate}[(D)]
		\item If $n \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \max \left \{ h^2_n , b^{2 (q-p)}_n \right \} \to 0$ and $\kappa \max \left \{ h_n, b_n \right \} < \kappa_0$, then
		\begin{align*}
			T^{\textsc{rbc}} \left ( h_n, b_n \right ) \equiv \frac{\widehat{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) - \theta_{\nu}}{\sqrt{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right ) 
		\end{align*}
	\end{enumerate}
	
\end{theorem}

\appsubsubsection{General Theorem: Proof}

	\begin{enumerate}[(R)]
		\item Recall that
		\begin{align*}
			R^{\textsc{bc}}_{n} & \equiv h_n^{p+1-\nu} \left ( \widehat{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) \right ) \\
			& = h_n^{p+1-\nu} \left ( \widehat{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \right ) \\
			& - h_n^{p+1-\nu} \left ( \widehat{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \right )
		\end{align*}
		with
		\begin{align*}
			\widehat{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widehat{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )} \widehat{\textsc{B}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{D-,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \widehat{\textsc{B}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \widehat{\textsc{B}}_{D-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		Thus,
		\begin{align*}
			R^{\textsc{bc}}_{n} & = h_n^{p+1-\nu} \left ( \frac{1}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} -  \frac{1}{\mu^{(\nu)}_{D+}} \right ) \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \right )^2} -  \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \right ) \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \frac{1}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )} -  \frac{1}{\mu^{(\nu)}_{D-}} \right ) \widehat{\textsc{B}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) \\
			& + h_n^{p+1-\nu} \left ( \frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \right )^2} -  \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \right ) \widehat{\textsc{B}}_{D-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		Expanding further,
		\begin{align*}
			R^{\textsc{bc}}_{n} & = h_n^{p+1-\nu} \left ( \frac{1}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} -  \frac{1}{\mu^{(\nu)}_{D+}} \right ) e_{p+1}' \widehat{\beta}_{W+,q} \left ( b_n \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \right )^2} -  \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \right ) e_{p+1}' \widehat{\beta}_{D+,q} \left ( b_n \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \frac{1}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )} -  \frac{1}{\mu^{(\nu)}_{D-}} \right ) e_{p+1}' \widehat{\beta}_{W-,q} \left ( b_n \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right ) \\
			& + h_n^{p+1-\nu} \left ( \frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \right )^2} -  \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \right ) e_{p+1}' \widehat{\beta}_{D-,q} \left ( b_n \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )
		\end{align*}
		Recall that, by Lemma S.A.1 in \cite{cct2014supp},
		\begin{align*}
			\mathcal{B}_{+,\nu,p,q} \left ( h_n \right ) = \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right ) \qquad \mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) = \left ( -1 \right )^{\nu+q} \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right )
		\end{align*}
		In addition, by a similar set of arguments as those in the proof of part (R) of Theorem \ref{thm_asydist},
		\begin{align*}
			e_{p+1}' \widehat{\beta}_{W+,q} \left ( b_n \right ) = \frac{\widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right )}{\left ( p+1 \right )!} = O_p \left ( b_n^{p+1-(p+1)} + \sqrt{\frac{1}{n b_n^{1+2(p+1)}}} \right ) = O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right ) \\
			e_{p+1}' \widehat{\beta}_{D+,q} \left ( b_n \right ) = \frac{\widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right )}{\left ( p+1 \right )!} = O_p \left ( b_n^{p+1-(p+1)} + \sqrt{\frac{1}{n b_n^{1+2(p+1)}}} \right ) = O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right ) \\
			e_{p+1}' \widehat{\beta}_{W-,q} \left ( b_n \right ) = \frac{\widehat{\mu}^{(p+1)}_{W-,q} \left ( b_n \right )}{\left ( p+1 \right )!} = O_p \left ( b_n^{p+1-(p+1)} + \sqrt{\frac{1}{n b_n^{1+2(p+1)}}} \right ) = O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right ) \\
			e_{p+1}' \widehat{\beta}_{D-,q} \left ( b_n \right ) = \frac{\widehat{\mu}^{(p+1)}_{D-,q} \left ( b_n \right )}{\left ( p+1 \right )!} = O_p \left ( b_n^{p+1-(p+1)} + \sqrt{\frac{1}{n b_n^{1+2(p+1)}}} \right ) = O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right )
		\end{align*}
		Following again the arguments proposed in the proof of part (R) of Theorem \ref{thm_asydist} and with repeated applications of the Continuous Mapping Theorem and the Delta Method,
		\begin{align*}
			\frac{1}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} -  \frac{1}{\mu^{(\nu)}_{D+}} & = O_p \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1 + 2\nu}}} \right ) \\
			\frac{\widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \right )^2} -  \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} & = O_p \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1 + 2\nu}}} \right ) \\
			\frac{1}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )} -  \frac{1}{\mu^{(\nu)}_{D-}} & = O_p \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1 + 2\nu}}} \right ) \\
			\frac{\widehat{\mu}^{(\nu)}_{W-,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \right )^2} -  \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} & = O_p \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1 + 2\nu}}} \right )
		\end{align*}
		because the variance of any polynomial estimator for $\mu^{(\nu)}_{A+}$ and $\mu^{(\nu)}_{A-}$ is multiplicative in $\frac{1}{n h_n^{1 + 2\nu}}$, and this feature is preserved by the variance of continuously differentiable functions of these estimators. Thus, if $n h_n^{1 + 2\nu} \to \infty$,
		\begin{align*}
			R^{\textsc{bc}}_{n} & = h_n^{p+1-\nu} O_p \left ( h_n^{p+1-\nu} + \sqrt{\frac{1}{n h_n^{1 + 2\nu}}} \right ) O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right ) \\
			& = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{h_n^{p+1-\nu}}{\sqrt{n h_n^{1 + 2\nu}}} \right ) O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right )
		\end{align*}
		which completes the proof.
	\end{enumerate}
	
	\begin{enumerate}[(B)]
		\item The conditional expectation of the bias-corrected linearized estimator $\widetilde{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )$ is
		\begin{align*}
			\E \left [ \widetilde{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] & = \E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) - h_n^{p+1-\nu} \widecheck{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right ) | \bm{R} \right ] \\
			& = \textsc{B}_1 - h^{p+1-\nu} \textsc{B}_2
		\end{align*}
		First, define
		\begin{align*}
			\textsc{B}_1 & \equiv \E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) - h_n^{p+1-\nu} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \Big | \bm{R} \right ]\\
			& = \E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] - h_n^{p+1-\nu} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \\
			& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right ) \\
			& - h_n^{p+1-\nu} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \\
			& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right ) \\
			& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) + h^{p+2-\nu}_{n} o_p \left ( 1 \right ) \\
			& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) \left \{ 1 + o_p \left ( 1 \right ) \right \}
		\end{align*}
		where the third equality follows from part (B) of Theorem \ref{thm_asydist}. Second, define
		\begin{align*}
			\textsc{B}_2 & \equiv \E \left [ \widecheck{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right ) - \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \E \left [ \widecheck{\textsc{B}}_{\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] - \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \\
			& = \E \left [ \widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] - \left ( \textsc{B}_{Y,\nu,p,p+1} \left ( h_n \right ) + \widetilde{\textsc{B}}_{\nu,p,p+1} \left ( h_n \right ) \right ) \\
			& = \E \left [ \widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - \left ( \textsc{B}_{Y,\nu,p,p+1} \left ( h_n \right ) + \widetilde{\textsc{B}}_{+,\nu,p,p+1} \left ( h_n \right ) - \widetilde{\textsc{B}}_{-,\nu,p,p+1} \left ( h_n \right ) \right )
		\end{align*}
		Recall that
		\begin{align*}
			\E \left [ \widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] & = \E \Big [ e_{p+1}' \widehat{\beta}_{Y+,q} \left ( b_n \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - e_{p+1}' \widehat{\beta}_{Y-,q} \left ( b_n \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right ) \Big | \bm{R} \Big ] \\
			\textsc{B}_{Y,\nu,p,p+1} \left ( h_n \right ) & \equiv e_{p+1}' \beta_{Y+,q} \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) - e_{p+1}' \beta_{Y-,q} \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )
		\end{align*}
		which implies that
		\begin{align*}
			& \E \left [ \widehat{\textsc{B}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] - \textsc{B}_{Y,\nu,p,p+1} \left ( h_n \right ) \\
			& = e_{p+1}' \left ( \E \left [ \widehat{\beta}_{Y+,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{Y+,q} \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - e_{p+1}' \left ( \E \left [ \widehat{\beta}_{Y-,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{Y-,q} \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )
		\end{align*}
		Similarly,
		\begin{align*}
			\E \left [ \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] & = \E \left [ \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \Bigg | \bm{R} \right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} \E \left [ \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \E \left [ \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} \E \left [ e_{p+1}' \widehat{\beta}_{W+,q} \left ( b_n \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \E \left [ e_{p+1}' \widehat{\beta}_{D+,q} \left ( b_n \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			\widetilde{\textsc{B}}_{+,\nu,p,p+1} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \textsc{B}_{W+,\nu,p,p+1} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \textsc{B}_{D+,\nu,p,p+1} \left ( h_n \right ) \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} e_{p+1}' \beta_{W+,q} \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} e_{p+1}' \beta_{D+,q} \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )
		\end{align*}
		which implies that
		\begin{align*}
			& \E \left [ \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] - \widetilde{\textsc{B}}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{W+,q} \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{D+,q} \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )
		\end{align*}
		Symmetrically,
		\begin{align*}
			& \E \left [ \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] - \widetilde{\textsc{B}}_{-,\nu,p,p+1} \left ( h_n \right ) \\
			& = \frac{1}{\mu^{(\nu)}_{D-}} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{W-,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{W-,q} \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right ) \\
			& - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{D-,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{D-,q} \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )
		\end{align*}
		Combining these derivations,
		\begin{align*}
			\textsc{B}_2 & \equiv e_{p+1}' \left ( \E \left [ \widehat{\beta}_{Y+,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{Y+,q} \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - e_{p+1}' \left ( \E \left [ \widehat{\beta}_{Y-,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{Y-,q} \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right ) \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{W+,q} \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{D+,q} \right ) \mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right ) \\
			& - \frac{1}{\mu^{(\nu)}_{D-}} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{W-,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{W-,q} \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right ) \\
			& + \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} e_{p+1}' \left ( \E \left [ \widehat{\beta}_{D-,q} \left ( b_n \right ) \Big | \bm{R} \right ] - \beta_{D-,q} \right ) \mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )
		\end{align*}
		Furthermore, by several applications of Lemma S.A.3 in \cite{cct2014supp}, for any random variable $A$,
		\begin{align*}
			\E \left [ \left ( p+1 \right )! e_{p+1}' \widehat{\beta}_{A+,q} \left ( b_n \right ) \Big | \bm{R} \right ] & = e_{p+1}' \beta_{A+,q} + b_n^{q-p} \frac{\mu^{(q+1)}_{A+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right ) \\
			\E \left [ \left ( p+1 \right )! e_{p+1}' \widehat{\beta}_{A-,q} \left ( b_n \right ) \Big | \bm{R} \right ] & = e_{p+1}' \beta_{A-,q} + b_n^{q-p} \frac{\mu^{(q+1)}_{A-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right )
		\end{align*}
		Thus,
		\begin{align*}
			\textsc{B}_2 & = \left ( b_n^{q-p} \frac{\mu^{(q+1)}_{Y+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right ) \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \left ( b_n^{q-p} \frac{\mu^{(q+1)}_{Y-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right ) \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} \left ( b_n^{q-p} \frac{\mu^{(q+1)}_{W+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right ) \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \left ( b_n^{q-p} \frac{\mu^{(q+1)}_{D+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right ) \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \frac{1}{\mu^{(\nu)}_{D-}} \left ( b_n^{q-p} \frac{\mu^{(q+1)}_{W-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right ) \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& + \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \left ( b_n^{q-p} \frac{\mu^{(q+1)}_{D-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) + o_p \left ( b_n^{q-p} \right ) \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Define the following bias terms:
		\begin{align*}
			\textsc{B}^{\textsc{bc}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{Y+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			\textsc{B}^{\textsc{bc}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{Y-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			\textsc{B}^{\textsc{bc}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{W+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			\textsc{B}^{\textsc{bc}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{D+}}{\left ( q+1 \right )!} \mathcal{B}_{+,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			\textsc{B}^{\textsc{bc}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{W-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			\textsc{B}^{\textsc{bc}}_{D-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{\mu^{(q+1)}_{D-}}{\left ( q+1 \right )!} \mathcal{B}_{-,p+1,q,q+1} \left ( b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Furthermore,
		\begin{align*}
			\textsc{B}^{\textsc{bc}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \textsc{B}^{\textsc{bc}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) - \textsc{B}^{\textsc{bc}}_{Y-,\nu,p,q} \left ( h_n, b_n \right )\\
			\widetilde{\textsc{B}}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \textsc{B}^{\textsc{bc}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \textsc{B}^{\textsc{bc}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\widetilde{\textsc{B}}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \textsc{B}^{\textsc{bc}}_{W-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \textsc{B}^{\textsc{bc}}_{D-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		And
		\begin{align*}
			\widetilde{\textsc{B}}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \equiv \widetilde{\textsc{B}}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) - \widetilde{\textsc{B}}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		Finally,
		\begin{align*}
			\textsc{B}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \equiv \textsc{B}^{\textsc{bc}}_{Y,\nu,p,q} \left ( h_n, b_n \right ) + \widetilde{\textsc{B}}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		As a consequence,
		\begin{align*}
			\textsc{B}_2 & = b_n^{q-p} \textsc{B}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) + o_p \left ( b_n^{q-p} \right ) \\
			& = b_n^{q-p} \textsc{B}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) + b_n^{q-p} o_p \left ( 1 \right ) \\
			& = b_n^{q-p} \textsc{B}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \left \{ 1 + o_p \left ( 1 \right ) \right \}
		\end{align*}
		Taken together, the conditional expectation of the linearized bias-corrected estimator $\widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right )$ is
		\begin{align*}
			\E \left [ \widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] & = \textsc{B}_1 - h^{p+1-\nu} \textsc{B}_2 \\
			& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) \left \{ 1 + o_p \left ( 1 \right ) \right \} \\
			& - h^{p+1-\nu} b_n^{q-p} \textsc{B}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) \left \{ 1 + o_p \left ( 1 \right ) \right \}
		\end{align*}
		which completes the proof.
	\end{enumerate}
	
	\begin{enumerate}[(V)]
		\item The conditional variance of the linearized bias-corrected estimator $\widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right )$ is
		\begin{align*}
			& \Var \left [ \widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \Var \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) - h_n^{p+1-\nu} \left ( \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \right ) \Big | \bm{R} \right ] \\
			& + \Var \left [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) - h_n^{p+1-\nu} \left ( \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \right ) \Big | \bm{R} \right ]
		\end{align*}
		where the equality follows from the bilinearity of the covariance and the conditional independence of observations on either side of the cutoff $c$. For compactness, define
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Var \bigg [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \right ) \Big | \bm{R} \bigg ] \\
			& = \Var \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& + h_n^{2 (p+1-\nu)} \Var \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - 2 h_n^{p+1-\nu} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - 2 h_n^{p+1-\nu} \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ]
		\end{align*}
		Symmetrically,
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Var \bigg [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \right ) \Big | \bm{R} \bigg ] \\
			& = \Var \left [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& + h_n^{2 (p+1-\nu)} \Var \left [ \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - 2 h_n^{p+1-\nu} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - 2 h_n^{p+1-\nu} \Cov \left [ \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ]
		\end{align*}
		First, leveraging the bilinearity of the covariance and the definitions from part (V) of Theorem \ref{thm_asydist},
		\begin{align*}
			\textsc{V}_{+,\nu,p} \left ( h_n \right ) & \equiv \Var \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \Var \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] + \Var \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& + 2 \times \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \mathcal{V}_{YY+,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{+,\nu,p} \left ( h_n \right ) + 2 \widetilde{\textsc{V}}_{Y+,\nu,p} \left ( h_n \right )
		\end{align*}
		Symmetrically,
		\begin{align*}
			\textsc{V}_{-,\nu,p} \left ( h_n \right ) & \equiv \Var \left [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \Var \left [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] + \Var \left [ \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& + 2 \times \Cov \left [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ), \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] \\
			& = \mathcal{V}_{YY-,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{-,\nu,p} \left ( h_n \right ) + 2 \widetilde{\textsc{V}}_{Y-,\nu,p} \left ( h_n \right ) 
		\end{align*}
		Second,
		\begin{align*}
			& \Var \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \Var \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] + \Var \left [ \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& + 2 \times \Cov \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ), \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R}\right ]
		\end{align*}
		Expanding the variance terms,
		\begin{align*}
			\Var \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] & = \Var \left [ \widehat{\mu}^{(p+1)}_{Y+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& = \mathcal{V}_{YY+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2}\\
			\Var \left [ \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] & = \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Var \left [  \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \Var \left [ \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - 2 \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \Cov \left [ \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ), \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Var \left [ \widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \Var \left [ \widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& - 2 \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \Cov \left [ \widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right ), \widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& = \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{V}_{WW+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \mathcal{V}_{DD+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& - 2 \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{V}_{WD+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& = \widetilde{\textsc{V}}_{+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2}
		\end{align*}
		Expanding the covariance term,
		\begin{align*}
			& \Cov \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ), \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R}\right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [  \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ), \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ), \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [ \widehat{\mu}^{(p+1)}_{Y+,q} \left ( b_n \right ), \widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widehat{\mu}^{(p+1)}_{Y+,q} \left ( b_n \right ), \widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& = \widetilde{\textsc{V}}_{Y+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2}
		\end{align*}
		Combining these terms,
		\begin{align*}
			& \Var \left [ \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \left ( \mathcal{V}_{YY+,p+1,q} \left ( b_n \right ) + \widetilde{\textsc{V}}_{+,p+1,q} \left ( b_n \right ) + 2 \widetilde{\textsc{V}}_{Y+,p+1,q} \left ( b_n \right ) \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& = \textsc{V}_{+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2}
		\end{align*}
		Symmetrically,
		\begin{align*}
			& \Var \left [ \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \left ( \mathcal{V}_{YY-,p+1,q} \left ( b_n \right ) + \widetilde{\textsc{V}}_{-,p+1,q} \left ( b_n \right ) + 2 \widetilde{\textsc{V}}_{Y-,p+1,q} \left ( b_n \right ) \right ) \frac{\mathcal{B}^2_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& = \textsc{V}_{-,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2}
		\end{align*}
		Third, 
		\begin{align*}
			& \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{Y+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Define
		\begin{align*}
			\widetilde{\textsc{C}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \mathcal{C}_{YY+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} \mathcal{C}_{YW+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{YD+,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		and $\widetilde{\textsc{C}}_{Y-,\nu,p,q} \left ( h_n, b_n \right )$ analogously. Thus, 
		\begin{align*}
			& \Cov \left [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \widetilde{\textsc{C}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Symmetrically,
		\begin{align*}
			& \Cov \left [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \widetilde{\textsc{C}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Fourth, recalling that $\widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \equiv \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )$,
		\begin{align*}
			& \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{W+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{Y+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& + \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& = \frac{1}{\mu^{(\nu)}_{D+}} \Cov \left [ \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{Y+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{Y+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& + \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \Cov \left [ \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \Cov \left [ \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{W+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \Cov \left [ \widehat{\mu}^{(\nu)}_{W+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!} \\
			& + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \Cov \left [ \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ), \widehat{\mu}^{(p+1)}_{D+,q} \left ( b_n \right ) \Big | \bm{R} \right ] \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Define
		\begin{align*}
			\widetilde{\textsc{C}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \mathcal{C}_{WY+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{DY+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& + \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{WW+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{C}_{DW+,\nu,p,q} \left ( h_n, b_n \right ) \\
			& - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{C}_{WD+,\nu,p,q} \left ( h_n, b_n \right ) + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \mathcal{C}_{DD+,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		and $\widetilde{\textsc{C}}_{-,\nu,p,q} \left ( h_n, b_n \right )$ analogously. Thus, 
		\begin{align*}
			& \Cov \left [ \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \widetilde{\textsc{C}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Symmetrically,
		\begin{align*}
			& \Cov \left [ \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ), \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \\
			& = \widetilde{\textsc{C}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Also define
		\begin{align*}
			\textsc{C}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widetilde{\textsc{C}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widetilde{\textsc{C}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \\
			\textsc{C}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widetilde{\textsc{C}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widetilde{\textsc{C}}_{-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
		Combining these derivations,
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Var \bigg [ \widehat{\mu}^{(\nu)}_{Y+,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{+,\nu,p} \left ( h_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \widehat{\textsc{B}}_{Y+,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \right ) \Big | \bm{R} \bigg ] \\
			& = \textsc{V}_{+,\nu,p} \left ( h_n \right ) + h_n^{2 (p+1-\nu)} \textsc{V}_{+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& - 2 h_n^{p+1-\nu} \textsc{C}_{+,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		Symmetrically,
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Var \bigg [ \widehat{\mu}^{(\nu)}_{Y-,\nu,p} \left ( h_n \right ) + \widetilde{\eta}_{-,\nu,p} \left ( h_n \right ) \\
			& - h_n^{p+1-\nu} \left ( \widehat{\textsc{B}}_{Y-,\nu,p,q} \left ( h_n, b_n \right ) + \widecheck{\widetilde{\textsc{B}}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \right ) \Big | \bm{R} \bigg ] \\
			& = \textsc{V}_{-,\nu,p} \left ( h_n \right ) + h_n^{2 (p+1-\nu)} \textsc{V}_{-,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \\
			& - 2 h_n^{p+1-\nu} \textsc{C}_{-,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
		\end{align*}
		To conclude, the conditional variance of the linearized bias-corrected estimator $\widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right )$ is
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Var \left [ \widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] = \textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) + \textsc{V}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right )
		\end{align*}
	\end{enumerate}
	
	\begin{enumerate}[(D)]
		\item Part (V) of this paper implies that
		\begin{align*}
			\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right ) = O_p \left ( \frac{1}{n h_n^{1+2\nu}} + \frac{h_n^{2 (p+1-\nu)}}{n b_n^{2p+3}} \right ) 
		\end{align*}
		In addition, by part (R) of Theorem \ref{thm_asydist},
		\begin{align*}
			R_n = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{1+2\nu}} \right )
		\end{align*}
		which implies that
		\begin{align*}
			R^2_n = O_p \left ( h_n^{4 (p+1-\nu)} + \frac{1}{n^2 h_n^{2+4\nu}} \right )
		\end{align*}
		As a consequence,
		\begin{align*}
			\frac{R^2_n}{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )} & = O_p \left ( \min \left \{ n h_n^{1+2\nu}, \frac{n b_n^{2p+3}}{h_n^{2 (p+1-\nu)}} \right \} \right ) O_p \left ( h_n^{4 (p+1-\nu)} + \frac{1}{n^2 h_n^{2+4\nu}} \right )  \\
			& = O_p \left ( \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) O_p \left ( n h_n^{2 (p+1-\nu)} + \frac{1}{n h_n^{2p+4+2\nu}} \right )  \\
			& = O_p \left ( \min \left \{ \frac{1}{n h_n^{1+2\nu}}, \frac{b_n^{2p+3}}{n h_n^{2p+4+2\nu}} \right \} \right ) O_p \left ( n^2 h_n^{2 (2p+3)} + 1 \right )  \\
			& = O_p \left ( \min \left \{ \frac{1}{n h_n^{1+2\nu}}, \frac{b_n^{2p+3}}{n h_n^{2p+4+2\nu}} \right \} \right ) \\
			& + O_p \left ( \min \left \{ n h_n^{4p+5-2\nu}, n b_n^{2p+3} h_n^{2 (p+1-\nu)} \right \} \right ) \\
			& = O_p \left ( \frac{1}{n h_n^{1+2\nu}} \min \left \{ 1, \frac{b_n^{2p+3}}{h_n^{2p+3}} \right \} \right ) \\
			& + O_p \left ( n h_n^{2 (p+1-\nu)} \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) \\
			& = O_p \left ( \frac{1}{n h_n^{1+2\nu}} \min \left \{ 1, \frac{1}{\rho_n^{2p+3}} \right \} \right ) + O_p \left ( n h_n^{2 (p+1-\nu)} \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right )
		\end{align*}
		where $\rho_n \equiv \frac{h_n}{b_n}$. By assumption, $n h_n^{1+2\nu} \to \infty$, implying that
		\begin{align*}
			O_p \left ( \frac{1}{n h_n^{1+2\nu}} \min \left \{ 1, \frac{1}{\rho_n^{2p+3}} \right \} \right ) = o_p \left ( 1 \right )
		\end{align*}
		Again by assumption, $n h^2_n \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \to 0$, implying that
		\begin{align*}
			O_p \left ( n h_n^{2 (p+1-\nu)} \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) = o_p \left ( 1 \right )
		\end{align*}
		for any $p \geq \nu$. Taken together,
		\begin{align*}
			\frac{R^2_n}{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )} = o_p \left ( 1 \right )
		\end{align*}
		Furthermore, by part (R) of this theorem,
		\begin{align*}
			R^{\textsc{bc}}_{n} = O_p \left ( h_n^{2 (p+1-\nu)} + \frac{h_n^{p+1-\nu}}{\sqrt{n h_n^{1 + 2\nu}}} \right ) O_p \left ( 1 + \sqrt{\frac{1}{n b_n^{2p+3}}} \right )
		\end{align*}
		which implies that
		\begin{align*}
			\left ( R^{\textsc{bc}}_{n} \right )^2 & = O_p \left ( h_n^{4 (p+1-\nu)} + \frac{h_n^{2 (p+1-\nu)}}{n h_n^{1 + 2\nu}} \right ) O_p \left ( 1 + \frac{1}{n b_n^{2p+3}} \right ) \\
			& = O_p \left ( h_n^{4 (p+1-\nu)} + \frac{h_n^{2p+3}}{n h_n^{2 + 4\nu}} \right ) O_p \left ( 1 + \frac{1}{n b_n^{2p+3}} \right )
		\end{align*}
		As a consequence,
		\begin{align*}
			\frac{R^{\textsc{bc}}_{n}}{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )} & = O_p \left ( \min \left \{ n h_n^{1+2\nu}, \frac{n b_n^{2p+3}}{h_n^{2 (p+1-\nu)}} \right \} \right ) \\
			& \times O_p \left ( h_n^{4 (p+1-\nu)} + \frac{h_n^{2p+3}}{n h_n^{2 + 4\nu}} \right ) O_p \left ( 1 + \frac{1}{n b_n^{2p+3}} \right )
		\end{align*}
		First, with similar derivations as in previous steps,
		\begin{align*}
			& O_p \left ( \min \left \{ n h_n^{1+2\nu}, \frac{n b_n^{2p+3}}{h_n^{2 (p+1-\nu)}} \right \} \right ) O_p \left ( h_n^{4 (p+1-\nu)} + \frac{h_n^{2p+3}}{n h_n^{2 + 4\nu}} \right ) \\
			& = O_p \left ( \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) O_p \left ( n h_n^{2 (p+1-\nu)} + \frac{h_n^{2p+3}}{n h_n^{2p+4+2\nu}} \right )  \\
			& = O_p \left ( \min \left \{ \frac{1}{n h_n^{1+2\nu}}, \frac{b_n^{2p+3}}{n h_n^{2p+4+2\nu}} \right \} \right ) O_p \left ( n^2 h_n^{2 (2p+3)} + h_n^{2p+3} \right )  \\
			& = O_p \left ( h_n^{2p+3} \min \left \{ \frac{1}{n h_n^{1+2\nu}}, \frac{b_n^{2p+3}}{n h_n^{2p+4+2\nu}} \right \} \right ) \\
			& + O_p \left ( \min \left \{ n h_n^{4p+5-2\nu}, n b_n^{2p+3} h_n^{2 (p+1-\nu)} \right \} \right ) \\
			& = O_p \left ( \frac{h_n^{2p+3}}{n h_n^{1+2\nu}} \min \left \{ 1, \frac{b_n^{2p+3}}{h_n^{2p+3}} \right \} \right ) \\
			& + O_p \left ( n h_n^{2 (p+1-\nu)} \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) \\
			& = O_p \left ( \frac{h_n^{2p+3}}{n h_n^{1+2\nu}} \min \left \{ 1, \frac{1}{\rho_n^{2p+3}} \right \} \right ) + O_p \left ( n h_n^{2 (p+1-\nu)} \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) \\
			& = O_p \left ( \frac{1}{n h_n^{1+2\nu}} \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) + o_p \left ( 1 \right ) \\
			& = o_p \left ( 1 \right )
		\end{align*}
		where the last equality follows from the assumption that $n h_n^{1+2\nu} \to \infty$. Thus,
		\begin{align*}
			\frac{R^{\textsc{bc}}_{n}}{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )} & = o_p \left ( 1 \right ) \\
			& + O_p \left ( \frac{h_n^{2p+3}}{n h_n^{1+2\nu}} \min \left \{ 1, \frac{1}{\rho_n^{2p+3}} \right \} \right ) O_p \left ( \frac{1}{n b_n^{2p+3}} \right ) \\
			& + O_p \left ( n h_n^{2 (p+1-\nu)} \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \right ) O_p \left ( \frac{1}{n b_n^{2p+3}} \right ) \\
			& = o_p \left ( 1 \right ) \\
			& + O_p \left ( \frac{1}{n h_n^{1+2\nu}} \frac{1}{n} \min \left \{ \rho_n^{2p+3}, 1 \right \} \right ) + O_p \left ( h_n^{2 (p+1-\nu)} \min \left \{ \rho_n^{2p+3}, 1 \right \} \right ) \\
			& = o_p \left ( 1 \right ) 
		\end{align*}
		where the last equality follows from the assumptions that $n h_n^{1+2\nu} \to \infty$ and $h_n \to 0$. The robust bias-corrected test statistic for the estimator $\widehat{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right )$ is
		\begin{align*}
			T^{\textsc{rbc}} \left ( h_n, b_n \right ) & \equiv \frac{\widehat{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right ) - \theta_{\nu}}{\sqrt{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )}} \\
			& = \frac{\widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right ) - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right )}{\sqrt{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )}} + \frac{R_n + R^{\textsc{bc}}_n}{\sqrt{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )}} \\
			& = \frac{\widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right ) - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right )}{\sqrt{\textsc{V}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )}} + o_p \left ( 1 \right ) \\
			& \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right )
		\end{align*}
		where the last line is proved by recognizing that the bias-corrected linearized estimator $\widetilde{\theta}^{\textsc{bc}}_{\nu,p} \left ( h_n, b_n \right )$ is numerically equivalent to the sharp regression discontinuity estimator that uses the sum of $Y + \frac{1}{\mu^{(\nu)}_{D}} W - \frac{\mu^{(\nu)}_{W}}{\left ( \mu^{(\nu)}_{D} \right )^2} D$ and a bias correction term as the outcome. Thus, provided that $n \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \max \left \{ h^2_n , b^{2 (q-p)}_n \right \} \to 0$ and $\kappa \max \left \{ h_n, b_n \right \} < \kappa_0$, the result follows from part (D) of Theorem A.1 in \cite{cct2014supp}.
	\end{enumerate}

\appsubsubsection{Proof of Proposition \ref{prop_confint}}\label{app_prop_confint}

The statement follows directly from specializing Theorem \ref{thm_asydist_bias} to $\nu=0$, $p=1$, and $q=2$. As a matter of fact, the asymptotic variance of the bias-corrected estimator $\widetilde{\theta} \left ( h_n, b_n \right )$ is
\begin{align*}
	\textsc{V}^{\textsc{bc}} \left ( h_n, b_n \right ) \equiv \Var \left [ \widetilde{\theta}^{\textsc{bc}} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] = \textsc{V}^{\textsc{bc}}_{+,0,1,2} \left ( h_n, b_n \right ) + \textsc{V}^{\textsc{bc}}_{-,0,1,2} \left ( h_n, b_n \right )
\end{align*}
where $\textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right )$ and $\textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right )$ are defined in the statement of Theorem \ref{thm_asydist_bias}.

\appsubsection{MSE-Optimal Bandwidths}

In this section, I derive the asymptotic (infeasible) bandwidth $h_{\textsc{mse},\nu,p}$ that minimizes the Mean Squared Error (MSE) of the linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$. I subsequently specialize this expression to compute MSE-optimal main and pilot bandwidths for the dynamic RD estimand in the paper. Before stating the general theorem, it is convenient to define additional recurring bias- and variance-related terms.

\appsubsubsection{Definitions of Bias Terms}

Recall that, by part (B) of Lemma S.A.3 in \cite{cct2014supp},
\begin{align}
	\mathcal{B}_{+,\nu,p,q} \left ( h_n \right ) = \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right ) \qquad \mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) = \left ( -1 \right )^{\nu+q} \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} + o_p \left ( 1 \right )
\end{align}
which implies that the $h_n$ argument can be suppressed. To eliminate the dependence on $h_n$, define the following bias terms:
\begin{align}
	\dot{\mathcal{B}}_{+,\nu,p,q} \equiv \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q} \qquad \dot{\mathcal{B}}_{-,\nu,p,q} \equiv \left ( -1 \right )^{\nu+q} \nu! e_{\nu}' \Gamma^{-1}_{p} \vartheta_{p,q}
\end{align}
Clearly,
\begin{align}
	\mathcal{B}_{+,\nu,p,q} \left ( h_n \right ) = \dot{\mathcal{B}}_{+,\nu,p,q} + o_p \left ( 1 \right ) \qquad \mathcal{B}_{-,\nu,p,q} \left ( h_n \right ) = \dot{\mathcal{B}}_{-,\nu,p,q} + o_p \left ( 1 \right )
\end{align}
In turn, for any random variable $A$, define
\begin{align}
	\dot{\textsc{B}}_{A+,\nu,p,q} \equiv \frac{\mu^{(q)}_{A+}}{q!} \dot{\mathcal{B}}_{+,\nu,p,q} \qquad	\dot{\textsc{B}}_{A-,\nu,p,q} \equiv \frac{\mu^{(q)}_{A-}}{q!} \dot{\mathcal{B}}_{-,\nu,p,q}
\end{align}

\appsubsubsection{Definitions of Variance Terms}

Recall that, by part (V) of Lemma S.A.3 in \cite{cct2014supp}, for any pair of random variables $A$ and $B$,
\begin{align}
	\mathcal{V}_{AB+,\nu,p} \left ( h_n \right ) = \frac{1}{n h_n^{1+2\nu}} \frac{\sigma^2_{AB+}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \left ( 1 + o_p \left ( 1 \right ) \right ) \\
	\mathcal{V}_{AB-,\nu,p} \left ( h_n \right ) = \frac{1}{n h_n^{1+2\nu}} \frac{\sigma^2_{AB-}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \left ( 1 + o_p \left ( 1 \right ) \right )
\end{align}
To eliminate the dependence on $h_n$, define the following variance terms:
\begin{align}
	\dot{\mathcal{V}}_{AB+,\nu,p} \equiv \frac{\sigma^2_{AB+}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu} \\
	\dot{\mathcal{V}}_{AB-,\nu,p} \equiv \frac{\sigma^2_{AB-}}{f_R \left ( c \right )} \nu!^2 e'_{\nu} \Gamma^{-1}_{p} \Psi_p \Gamma^{-1}_{p} e_{\nu}
\end{align}
Clearly,
\begin{align}
	\mathcal{V}_{AB+,\nu,p} \left ( h_n \right ) & = \frac{1}{n h_n^{1+2\nu}} \dot{\mathcal{V}}_{AB+,\nu,p} \left ( 1 + o_p \left ( 1 \right ) \right ) \\
	\mathcal{V}_{AB-,\nu,p} \left ( h_n \right ) & = \frac{1}{n h_n^{1+2\nu}} \dot{\mathcal{V}}_{AB-,\nu,p} \left ( 1 + o_p \left ( 1 \right ) \right ) 
\end{align}

\appsubsubsection{General Theorem: Statement}

\begin{theorem}\label{thm_optbandwidth}
	Suppose Assumptions \ref{ass_moments} and \ref{ass_kernel} hold with $\delta \geq p+1$ and $\nu \leq p$. If $h_n \to 0$ and $n h_n \to \infty$, then the Mean Squared Error of the linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ is
	\begin{align*}
		\text{MSE} \left ( \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \right ) & = h^{2(p+1-\nu)}_{n} \left ( \dot{\textsc{B}}^2_{\nu,p,p+1} + o_p \left ( 1 \right ) \right ) + \frac{1}{n h_n^{1+2\nu}} \left ( \dot{\textsc{V}}_{\nu,p} + o_p \left ( 1 \right ) \right )
	\end{align*}
	First,
	\begin{align*}
		\dot{\textsc{B}}_{\nu,p,q} \equiv \dot{\textsc{B}}_{Y,\nu,p,q} + \dot{\widetilde{\textsc{B}}}_{\nu,p,q}
	\end{align*}
	with
	\begin{align*}
		\dot{\textsc{B}}_{Y,\nu,p,q} & \equiv \dot{\textsc{B}}_{Y+,\nu,p,q} - \dot{\textsc{B}}_{Y-,\nu,p,q} \\
		\dot{\widetilde{\textsc{B}}}_{\nu,p,q} & \equiv \dot{\widetilde{\textsc{B}}}_{+,\nu,p,q} - \dot{\widetilde{\textsc{B}}}_{-,\nu,p,q}
	\end{align*}
	and
	\begin{align*}
		\dot{\widetilde{\textsc{B}}}_{+,\nu,p,q} & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \dot{\textsc{B}}_{W+,\nu,p,q} - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \dot{\textsc{B}}_{D+,\nu,p,q} \\
		\dot{\widetilde{\textsc{B}}}_{-,\nu,p,q} & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \dot{\textsc{B}}_{W-,\nu,p,q} - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \dot{\textsc{B}}_{D-,\nu,p,q}
	\end{align*}
	Second, 
	\begin{align*}
		\dot{\textsc{V}}_{\nu,p} \equiv \dot{\textsc{V}}_{Y,\nu,p} + \dot{\widetilde{\textsc{V}}}_{\nu,p} + 2 \dot{\widetilde{\textsc{V}}}_{Y,\nu,p}
	\end{align*}
	where
	\begin{align*}
		\dot{\textsc{V}}_{Y,\nu,p} & \equiv \dot{\mathcal{V}}_{YY+,\nu,p} + \dot{\mathcal{V}}_{YY-,\nu,p} \\
		\dot{\widetilde{\textsc{V}}}_{\nu,p} & \equiv \dot{\widetilde{\textsc{V}}}_{+,\nu,p} + \dot{\widetilde{\textsc{V}}}_{-,\nu,p} \\
		\dot{\widetilde{\textsc{V}}}_{Y,\nu,p} & \equiv \dot{\widetilde{\textsc{V}}}_{Y+,\nu,p} + \dot{\widetilde{\textsc{V}}}_{Y-,\nu,p}
	\end{align*}
	with
	\begin{align*}
		\dot{\widetilde{\textsc{V}}}_{+,\nu,p} & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \dot{\mathcal{V}}_{WW+,\nu,p} + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \dot{\mathcal{V}}_{DD+,\nu,p} - \frac{2 \mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \dot{\mathcal{V}}_{WD+,\nu,p} \\
		\dot{\widetilde{\textsc{V}}}_{-,\nu,p} & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \dot{\mathcal{V}}_{WW-,\nu,p} + \frac{\left ( \mu^{(\nu)}_{W-} \right )^2}{\left ( \mu^{(\nu)}_{D-} \right )^4} \dot{\mathcal{V}}_{DD-,\nu,p} - \frac{2 \mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \dot{\mathcal{V}}_{WD-,\nu,p}
	\end{align*}
	and
	\begin{align*}
		\dot{\widetilde{\textsc{V}}}_{Y+,\nu,p} & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \dot{\mathcal{V}}_{YW+,\nu,p} - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \dot{\mathcal{V}}_{YD+,\nu,p} \\
		\dot{\widetilde{\textsc{V}}}_{Y-,\nu,p} & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \dot{\mathcal{V}}_{YW-,\nu,p} - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \dot{\mathcal{V}}_{YD-,\nu,p}
	\end{align*}
	Finally, the MSE-minimizing bandwidth is
	\begin{align*}
		h_{\textsc{mse},\nu,p} = \left ( \frac{\left ( 1+2\nu \right ) \dot{\textsc{V}}_{\nu,p}}{2(p+1-\nu) \dot{\textsc{B}}^2_{\nu,p,p+1}} \right )^{\frac{1}{2p+3}} n^{-\frac{1}{2p+3}}
	\end{align*}
\end{theorem}

\appsubsubsection{General Theorem: Proof}

The conditional Mean Squared Error of $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ is
\begin{align*}
	\text{MSE} \left ( \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \right ) & \equiv \E \left [ \left ( \widetilde{\theta}_{\nu,p} \left ( h_n \right ) - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right ) \right )^2 \Big | \bm{R} \right ] \\
	& = \left ( \E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right ) \right )^2 + \Var \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] 
\end{align*}
First, part (B) of Theorem \ref{thm_asydist} states that, as $h_n \to 0$,
\begin{align*}
	\E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \notag \\
	& + h^{p+2-\nu}_{n} \textsc{B}_{\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right ) \\
	& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \notag \\
	& + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \left ( \frac{h_n \textsc{B}_{\nu,p,p+2} \left ( h_n \right )}{\textsc{B}_{\nu,p,p+1} \left ( h_n \right )} + \frac{o_p \left (h_n \right )}{\textsc{B}_{\nu,p,p+1} \left ( h_n \right )} \right ) \\
	& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) o_p \left ( 1 \right ) \\
	& = \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} + h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \left ( 1 + o_p \left ( 1 \right ) \right )
\end{align*}
Thus,
\begin{align*}
	\left ( \E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right ) \right )^2 & = \left ( h^{p+1-\nu}_{n} \textsc{B}_{\nu,p,p+1} \left ( h_n \right ) \left ( 1 + o_p \left ( 1 \right ) \right ) \right )^2 \\
	& = h^{2(p+1-\nu)}_{n} \textsc{B}^2_{\nu,p,p+1} \left ( h_n \right ) \left ( 1 + o_p \left ( 1 \right ) \right )^2 \\
	& = h^{2(p+1-\nu)}_{n} \textsc{B}^2_{\nu,p,p+1} \left ( h_n \right ) \left ( 1 + o_p \left ( 1 \right ) \right ) \\
	& = h^{2(p+1-\nu)}_{n} \left ( \textsc{B}^2_{\nu,p,p+1} \left ( h_n \right ) + o_p \left ( 1 \right ) \right )
\end{align*}
Then, for $h_n \to 0$, the bias terms can be redefined as follows:
\begin{align*}
	\dot{\textsc{B}}_{Y,\nu,p,q} & \equiv \dot{\textsc{B}}_{Y+,\nu,p,q} - \dot{\textsc{B}}_{Y-,\nu,p,q} \\
	\dot{\widetilde{\textsc{B}}}_{+,\nu,p,q} & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \dot{\textsc{B}}_{W+,\nu,p,q} - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \dot{\textsc{B}}_{D+,\nu,p,q} \\
	\dot{\widetilde{\textsc{B}}}_{-,\nu,p,q} & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \dot{\textsc{B}}_{W-,\nu,p,q} - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \dot{\textsc{B}}_{D-,\nu,p,q}
\end{align*}
Furthermore,
\begin{align*}
	\dot{\widetilde{\textsc{B}}}_{\nu,p,q} \equiv \dot{\widetilde{\textsc{B}}}_{+,\nu,p,q} - \dot{\widetilde{\textsc{B}}}_{-,\nu,p,q}
\end{align*}
Finally,
\begin{align*}
	\dot{\textsc{B}}_{\nu,p,q} \equiv \dot{\textsc{B}}_{Y,\nu,p,q} + \dot{\widetilde{\textsc{B}}}_{\nu,p,q}
\end{align*}
To conclude,
\begin{align*}
	\left ( \E \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] - \left ( \mu^{(\nu)}_{Y+} - \mu^{(\nu)}_{Y-} \right ) \right )^2 = h^{2(p+1-\nu)}_{n} \left ( \dot{\textsc{B}}^2_{\nu,p,p+1} + o_p \left ( 1 \right ) \right )
\end{align*}
Second, part (V) of Theorem \ref{thm_asydist} states that, as $h_n \to 0$,
\begin{align*}
	\Var \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \textsc{V}_{Y,\nu,p} \left ( h_n \right ) + \widetilde{\textsc{V}}_{\nu,p} \left ( h_n \right ) + 2 \widetilde{\textsc{V}}_{Y,\nu,p} \left ( h_n \right )
\end{align*}
Then, for $h_n \to 0$, the variance terms can be redefined as follows:
\begin{align*}
	\dot{\widetilde{\textsc{V}}}_{Y+,\nu,p} & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \dot{\mathcal{V}}_{YW+,\nu,p} - \frac{\mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \dot{\mathcal{V}}_{YD+,\nu,p} \\
	\dot{\widetilde{\textsc{V}}}_{Y-,\nu,p} & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \dot{\mathcal{V}}_{YW-,\nu,p} - \frac{\mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \dot{\mathcal{V}}_{YD-,\nu,p}
\end{align*}
Furthermore,
\begin{align*}
	\dot{\widetilde{\textsc{V}}}_{+,\nu,p} & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \dot{\mathcal{V}}_{WW+,\nu,p} + \frac{\left ( \mu^{(\nu)}_{W+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \dot{\mathcal{V}}_{DD+,\nu,p} - \frac{2 \mu^{(\nu)}_{W+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \dot{\mathcal{V}}_{WD+,\nu,p} \\
	\dot{\widetilde{\textsc{V}}}_{-,\nu,p} & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \dot{\mathcal{V}}_{WW-,\nu,p} + \frac{\left ( \mu^{(\nu)}_{W-} \right )^2}{\left ( \mu^{(\nu)}_{D-} \right )^4} \dot{\mathcal{V}}_{DD-,\nu,p} - \frac{2 \mu^{(\nu)}_{W-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \dot{\mathcal{V}}_{WD-,\nu,p}
\end{align*}
And
\begin{align*}
	\dot{\textsc{V}}_{Y,\nu,p} & \equiv \dot{\mathcal{V}}_{YY+,\nu,p} + \dot{\mathcal{V}}_{YY-,\nu,p} \\
	\dot{\widetilde{\textsc{V}}}_{\nu,p} & \equiv \dot{\widetilde{\textsc{V}}}_{+,\nu,p} + \dot{\widetilde{\textsc{V}}}_{-,\nu,p} \\
	\dot{\widetilde{\textsc{V}}}_{Y,\nu,p} & \equiv \dot{\widetilde{\textsc{V}}}_{Y+,\nu,p} + \dot{\widetilde{\textsc{V}}}_{Y-,\nu,p}
\end{align*}
Finally,
\begin{align*}
	\dot{\textsc{V}}_{\nu,p} \equiv \dot{\textsc{V}}_{Y,\nu,p} + \dot{\widetilde{\textsc{V}}}_{\nu,p} + 2 \dot{\widetilde{\textsc{V}}}_{Y,\nu,p}
\end{align*}
To conclude,
\begin{align*}
	\Var \left [ \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = \frac{1}{n h_n^{1+2\nu}} \dot{\textsc{V}}_{\nu,p} \left ( 1 + o_p \left ( 1 \right ) \right ) = \frac{1}{n h_n^{1+2\nu}} \left ( \dot{\textsc{V}}_{\nu,p} + o_p \left ( 1 \right ) \right )
\end{align*}
Taken together, as $h_n \to 0$, the Mean Squared Error of the linearized estimator $\widetilde{\theta}_{\nu,p} \left ( h_n \right )$ is
\begin{align*}
	\text{MSE} \left ( \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \right ) & = h^{2(p+1-\nu)}_{n} \left ( \dot{\textsc{B}}^2_{\nu,p,p+1} + o_p \left ( 1 \right ) \right ) + \frac{1}{n h_n^{1+2\nu}} \left ( \dot{\textsc{V}}_{\nu,p} + o_p \left ( 1 \right ) \right )
\end{align*}
The MSE-optimal bandwidth for the linearized estimator $h_{\textsc{mse},\nu,p}$ can then be computed by minimizing $\text{MSE} \left ( \widetilde{\theta}_{\nu,p} \left ( h_n \right ) \right )$ with respect to $h_n$:
\begin{align*}
	h_{\textsc{mse},\nu,p} \equiv \arg \min_{h} \left \{ h^{2(p+1-\nu)} \dot{\textsc{B}}^2_{\nu,p,p+1} + \frac{1}{n h^{1+2\nu}} \dot{\textsc{V}}_{\nu,p} \right \}
\end{align*}
Taking the first-order condition and solving for the MSE-minimizing bandwidth yields
\begin{align*}
	& 2(p+1-\nu) h_{\textsc{mse},\nu,p}^{2p+1-2\nu} \dot{\textsc{B}}^2_{\nu,p,p+1} = \left ( 1+2\nu \right ) h_{\textsc{mse},\nu,p}^{-2-2\nu} n^{-1} \dot{\textsc{V}}_{\nu,p} \\
	& \iff h_{\textsc{mse},\nu,p}^{2p+3} = \frac{\left ( 1+2\nu \right ) \dot{\textsc{V}}_{\nu,p}}{2(p+1-\nu) \dot{\textsc{B}}^2_{\nu,p,p+1}} \times n^{-1}\\
	& \iff h_{\textsc{mse},\nu,p} = \left ( \frac{\left ( 1+2\nu \right ) \dot{\textsc{V}}_{\nu,p}}{2(p+1-\nu) \dot{\textsc{B}}^2_{\nu,p,p+1}} \right )^{\frac{1}{2p+3}} n^{-\frac{1}{2p+3}}
\end{align*}
which completes the proof.

\appsubsubsection{Proof of Proposition \ref{prop_optbandwidth}}\label{app_prop_bandwidth}

The first half of the statement follows from specializing Theorem \ref{thm_optbandwidth} to $\nu=0$ and $p=1$. As a matter of fact, the Mean Squared Error of the linearized estimator $\widetilde{\theta} \left ( h_n \right )$ is
\begin{align*}
	\text{MSE} \left ( \widetilde{\theta} \left ( h_n \right ) \right ) = \text{MSE} \left ( \widetilde{\theta}_{0,1} \left ( h_n \right ) \right ) & = h^4_{n} \left ( \dot{\textsc{B}}^2_{0,1,2} + o_p \left ( 1 \right ) \right ) + \frac{1}{n h_n} \left ( \dot{\textsc{V}}_{0,1} + o_p \left ( 1 \right ) \right )
\end{align*}
Thus, the MSE-optimal main bandwidth is
\begin{align*}
	h_{\textsc{mse}} = \left ( \frac{\dot{\textsc{V}}_{0,1}}{4 \dot{\textsc{B}}^2_{0,1,2}} \right )^{1 \slash 5} n^{- 1 \slash 5}
\end{align*}
To complete the proof, define $\dot{\textsc{B}}_{h} \equiv \dot{\textsc{B}}_{0,1,2}$ and $\dot{\textsc{V}}_{h} \equiv \dot{\textsc{V}}_{0,1}$. Analogously, the second half of the statement follows from specializing Theorem \ref{thm_optbandwidth} to $\nu=2$ and $p=2$. As a matter of fact, the Mean Squared Error of the linearized estimator $\widetilde{\theta}_{2,2} \left ( b_n \right )$ is
\begin{align*}
	\text{MSE} \left ( \widetilde{\theta}_{2,2} \left ( b_n \right ) \right ) & = b^{2}_{n} \left ( \dot{\textsc{B}}^2_{2,2,3} + o_p \left ( 1 \right ) \right ) + \frac{1}{n b_n^{5}} \left ( \dot{\textsc{V}}_{2,2} + o_p \left ( 1 \right ) \right )
\end{align*}
Thus, the MSE-optimal pilot bandwidth is
\begin{align*}
	b_{\textsc{mse}} = \left ( \frac{5 \dot{\textsc{V}}_{2,2}}{2 \dot{\textsc{B}}^2_{2,2,3}} \right )^{1 \slash 7} n^{-1 \slash 7}
\end{align*}
To complete the proof, define $\dot{\textsc{B}}_{b} \equiv \dot{\textsc{B}}_{2,2,3}$ and $\dot{\textsc{V}}_{b} \equiv \dot{\textsc{V}}_{2,2}$.

\appsubsection{Assessing the Plausibility of the Common Trends Assumption}\label{app_testct}

The goal of this section is to derive a statistic to test hypotheses pertaining to the plausibility of the common trends assumption. As in standard difference-in-differences designs, this test hinges on the availability of ``pre-periods'', namely periods prior to the focal round of treatment assignment. Under Assumption \ref{ass_mark}, the focal time period is preceded by $k$ periods for which $D_{t-1} = \dots = D_{t-k} = 0$. Then, based on Assumption \ref{ass_ct1}, an empirical researcher may expect that, for $t-k \leq u < v \leq t$ and any $d_{t+1}, \dots, d_{t+\tau} \in \left \{ 0,1 \right \}$,
\begin{align}
	& \E \left [ \Delta_{u,v} \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) | H_{t-1} = h_{t-1}, R_t = c, P \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) = 1 \right ] \nonumber \\
	= \ & \E \left [ \Delta_{u,v} \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) | H_{t-1} = h_{t-1}, R_t = c, P \left ( h_{t-k-1}, 0_k, d_t, d_{t+1}, \dots, d_{t+\tau} \right ) = 1 \right ]
\end{align}
with $\Delta_{u,v} \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) \equiv Y_v \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right ) - Y_u \left ( h_{t-k-1}, 0_k, d_t, 0_\tau \right )$. Because units are untreated in the $k$ periods preceding $t$, both conditional expectations are identified using the upper or lower limit of $R_t$. Specifically,
\begin{align}\label{eq:pretrends_identified}
	& \lim_{r \downarrow c} \E \left [ Y_v - Y_u \bigg | H_{t-1} = h_{t-1}, R_t = r, \bigcap_{s=1}^{\tau} \left \{ D_{t+s} = 0 \right \} \right ] \notag \\
	& = \lim_{r \downarrow c} \E \left [ Y_v - Y_u \bigg | H_{t-1} = h_{t-1}, R_t = r, \bigcup_{s=1}^{\tau} \left \{ D_{t+s} \neq 0 \right \} \right ]
\end{align}
and the equality of lower limits follows from a symmetric argument. In addition, by an application of the Law Iterated Expectations and the properties of limits, equation \eqref{eq:pretrends_identified} can be written as
\begin{align}
	& \frac{\lim_{r \downarrow c} \E \left [ \left ( Y_{v} - Y_{u} \right ) \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) | H_{t-1} = h_{t-1}, R_t = r \right ]}{\lim_{r \downarrow c} \E \left [ \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) | H_{t-1} = h_{t-1}, R_t = r \right ]} \notag \\
	& = \frac{\lim_{r \downarrow c} \E \left [ \left ( Y_{v} - Y_{u} \right ) \left ( 1 - \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) \right ) | H_{t-1} = h_{t-1}, R_t = r \right ]}{\lim_{r \downarrow c} \E \left [ 1 - \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) | H_{t-1} = h_{t-1}, R_t = r \right ]}
\end{align}
Hereafter, to keep notation compact, I omit the event $H_{t-1} = h_{t-1}$ from the conditioning set and define the following random variables:
\begin{align}
	& J \equiv \left ( Y_{v} - Y_{u} \right ) \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) \quad && D \equiv \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) \\
	& K \equiv \left ( Y_{v} - Y_{u} \right ) \left ( 1 - \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right ) \right ) \quad && G \equiv 1 - \prod_{s=1}^{\tau} \left ( 1 - D_{t+s} \right )
\end{align}
so that the two equalities of interest can be concisely expressed as
\begin{align}\label{eq:hypotheses}
	\pi_{+} \equiv \frac{\mu_{J+}}{\mu_{D+}} - \frac{\mu_{K+}}{\mu_{G+}} = 0 \qquad \qquad \pi_{-} \equiv \frac{\mu_{J-}}{\mu_{D-}} - \frac{\mu_{K-}}{\mu_{G-}} = 0
\end{align}
For consistency with previous sections, I consider more general parameters that involve derivatives of arbitrary order $\nu$:
\begin{align}\label{eq:hypotheses}
	\pi_{+,\nu} \equiv \frac{\mu^{(\nu)}_{J+}}{\mu^{(\nu)}_{D+}} - \frac{\mu^{(\nu)}_{K+}}{\mu^{(\nu)}_{G+}} \qquad \qquad \pi_{-,\nu} \equiv \frac{\mu^{(\nu)}_{J-}}{\mu^{(\nu)}_{D-}} - \frac{\mu^{(\nu)}_{K-}}{\mu^{(\nu)}_{G-}}
\end{align}
In the remainder of this section, I construct a statistic to test the null hypothesis implied by $\pi_{+,\nu} = 0$ and $\pi_{-,\nu} = 0$ jointly. The argument for constructing bias-corrected estimators for these parameters and computing their variances is analogous to the one required to derive the asymptotic distribution of $\widehat{\theta}^{\textsc{bc}}_{\nu,p,q} \left ( h_n, b_n \right )$.

\appsubsubsection{Definitions of Estimators}

Given a bandwidth sequence $h_n$, natural estimators for the target parameters are
\begin{align}\label{eq:hypotheses}
	\widehat{\pi}_{+,\nu,p} \left ( h_n \right ) \equiv \frac{\widehat{\mu}^{(\nu)}_{J+,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} - \frac{\widehat{\mu}^{(\nu)}_{K+,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{G+,p} \left ( h_n \right )} \qquad \widehat{\pi}_{-,\nu,p} \left ( h_n \right ) \equiv \frac{\widehat{\mu}^{(\nu)}_{J-,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )} - \frac{\widehat{\mu}^{(\nu)}_{K-,p} \left ( h_n \right )}{\widehat{\mu}^{(\nu)}_{G-,p} \left ( h_n \right )}
\end{align}
Proceeding as in previous sections, a second-order Taylor expansion yields the following linearized estimators:
\begin{align}
	\widetilde{\pi}_{+,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \left ( \widehat{\mu}^{(\nu)}_{J+,p} \left ( h_n \right ) - \mu^{(\nu)}_{J+} \right ) - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) - \mu^{(\nu)}_{D+} \right ) \notag \\
	& - \frac{1}{\mu^{(\nu)}_{G+}} \left ( \widehat{\mu}^{(\nu)}_{K+,p} \left ( h_n \right ) - \mu^{(\nu)}_{K+} \right ) + \frac{\mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{G+,p} \left ( h_n \right ) - \mu^{(\nu)}_{G+} \right ) \\
	\widetilde{\pi}_{-,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \left ( \widehat{\mu}^{(\nu)}_{J-,p} \left ( h_n \right ) - \mu^{(\nu)}_{J-} \right ) - \frac{\mu^{(\nu)}_{J-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) - \mu^{(\nu)}_{D-} \right ) \notag \\
	& - \frac{1}{\mu^{(\nu)}_{G-}} \left ( \widehat{\mu}^{(\nu)}_{K-,p} \left ( h_n \right ) - \mu^{(\nu)}_{K-} \right ) + \frac{\mu^{(\nu)}_{K-}}{\left ( \mu^{(\nu)}_{G-} \right )^2} \left ( \widehat{\mu}^{(\nu)}_{G-,p} \left ( h_n \right ) - \mu^{(\nu)}_{G-} \right )
\end{align}

\appsubsubsection{Bias of Linearized Estimators}

As in Theorem \ref{thm_asydist}, suppose that Assumptions \ref{ass_moments} and \ref{ass_kernel} hold with $\delta \geq p+2$, and $n h_n \to \infty$. Then, if $h_n \to 0$, the conditional expectation of the linearized estimator $\widetilde{\pi}_{+,\nu,p} \left ( h_n \right )$ is
\begin{align}
	\E \left [ \widetilde{\pi}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = h^{p+1-\nu}_{n} \textsc{B}_{+,\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \textsc{B}_{+,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
\end{align}
where
\begin{align}
	\textsc{B}_{+,\nu,p,q} \left ( h_n \right ) \equiv \textsc{B}_{1+,\nu,p,q} \left ( h_n \right ) - \textsc{B}_{2+,\nu,p,q} \left ( h_n \right )
\end{align}
with
\begin{align}
	\textsc{B}_{1+,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \textsc{B}_{J+,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \textsc{B}_{D+,\nu,p,q} \left ( h_n \right ) \\
	\textsc{B}_{2+,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{G+}} \textsc{B}_{K+,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^2} \textsc{B}_{G+,\nu,p,q} \left ( h_n \right )
\end{align}
Symmetrically, the conditional expectation of the linearized estimator $\widetilde{\pi}_{-,\nu,p} \left ( h_n \right )$ is
\begin{align}
	\E \left [ \widetilde{\pi}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] & = h^{p+1-\nu}_{n} \textsc{B}_{-,\nu,p,p+1} \left ( h_n \right ) + h^{p+2-\nu}_{n} \textsc{B}_{-,\nu,p,p+2} \left ( h_n \right ) + o_p \left ( h^{p+2-\nu}_{n} \right )
\end{align}
where
\begin{align}
	\textsc{B}_{-,\nu,p,q} \left ( h_n \right ) \equiv \textsc{B}_{1-,\nu,p,q} \left ( h_n \right ) - \textsc{B}_{2-,\nu,p,q} \left ( h_n \right )
\end{align}
with
\begin{align}
	\textsc{B}_{1-,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \textsc{B}_{J-,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{J-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \textsc{B}_{D-,\nu,p,q} \left ( h_n \right ) \\
	\textsc{B}_{2-,\nu,p,q} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{G-}} \textsc{B}_{K-,\nu,p,q} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{K-}}{\left ( \mu^{(\nu)}_{G-} \right )^2} \textsc{B}_{G-,\nu,p,q} \left ( h_n \right )
\end{align}

\appsubsubsection{Variance of Linearized Estimators}

Maintaining the assumptions of Theorem \ref{thm_asydist}, if $h_n \to 0$, the conditional variance of the linearized estimator $\widetilde{\pi}_{+,\nu,p} \left ( h_n \right )$ is
\begin{align}
	\textsc{V}_{+,\nu,p} \left ( h_n \right ) \equiv \Var \left [ \widetilde{\pi}_{+,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \textsc{V}_{11+,\nu,p} \left ( h_n \right ) + \textsc{V}_{22+,\nu,p} \left ( h_n \right ) - 2 \textsc{V}_{12+,\nu,p} \left ( h_n \right )
\end{align}
where
\begin{align}
	\textsc{V}_{11+,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{V}_{JJ+,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{J+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \mathcal{V}_{DD+,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{V}_{JD+,\nu,p} \left ( h_n \right ) \\
	\textsc{V}_{22+,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\left ( \mu^{(\nu)}_{G+} \right )^2} \mathcal{V}_{KK+,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{K+} \right )^2}{\left ( \mu^{(\nu)}_{G+} \right )^4} \mathcal{V}_{GG+,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^3} \mathcal{V}_{KG+,\nu,p} \left ( h_n \right )
\end{align}
and
\begin{align}
	\textsc{V}_{12+,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+} \mu^{(\nu)}_{G+}} \mathcal{V}_{JK+,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\mu^{(\nu)}_{D+} \left ( \mu^{(\nu)}_{G+} \right )^2} \mathcal{V}_{JG+,\nu,p} \left ( h_n \right ) \notag \\
	& - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^2 \mu^{(\nu)}_{G+}} \mathcal{V}_{DK+,\nu,p} \left ( h_n \right ) + \frac{\mu^{(\nu)}_{J+} \mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{D+} \mu^{(\nu)}_{G+} \right )^2} \mathcal{V}_{DG+,\nu,p} \left ( h_n \right ) 
\end{align}
Symmetrically, the conditional variance of the linearized estimator $\widetilde{\pi}_{-,\nu,p} \left ( h_n \right )$ is
\begin{align}
	\textsc{V}_{-,\nu,p} \left ( h_n \right ) \equiv \Var \left [ \widetilde{\pi}_{-,\nu,p} \left ( h_n \right ) \Big | \bm{R} \right ] = \textsc{V}_{11-,\nu,p} \left ( h_n \right ) + \textsc{V}_{22-,\nu,p} \left ( h_n \right ) - 2 \textsc{V}_{12-,\nu,p} \left ( h_n \right )
\end{align}
where
\begin{align}
	\textsc{V}_{11-,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\left ( \mu^{(\nu)}_{D-} \right )^2} \mathcal{V}_{JJ-,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{J-} \right )^2}{\left ( \mu^{(\nu)}_{D-} \right )^4} \mathcal{V}_{DD-,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{J-}}{\left ( \mu^{(\nu)}_{D-} \right )^3} \mathcal{V}_{JD-,\nu,p} \left ( h_n \right ) \\
	\textsc{V}_{22-,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\left ( \mu^{(\nu)}_{G-} \right )^2} \mathcal{V}_{KK-,\nu,p} \left ( h_n \right ) + \frac{\left ( \mu^{(\nu)}_{K-} \right )^2}{\left ( \mu^{(\nu)}_{G-} \right )^4} \mathcal{V}_{GG-,\nu,p} \left ( h_n \right ) - \frac{2 \mu^{(\nu)}_{K-}}{\left ( \mu^{(\nu)}_{G-} \right )^3} \mathcal{V}_{KG-,\nu,p} \left ( h_n \right )
\end{align}
and
\begin{align}
	\textsc{V}_{12-,\nu,p} \left ( h_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-} \mu^{(\nu)}_{G+}} \mathcal{V}_{JK-,\nu,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{K-}}{\mu^{(\nu)}_{D-} \left ( \mu^{(\nu)}_{G-} \right )^2} \mathcal{V}_{JG-,\nu,p} \left ( h_n \right ) \notag\\
	& - \frac{\mu^{(\nu)}_{J-}}{\left ( \mu^{(\nu)}_{D-} \right )^2 \mu^{(\nu)}_{G-}} \mathcal{V}_{DK-,\nu,p} \left ( h_n \right ) + \frac{\mu^{(\nu)}_{J-} \mu^{(\nu)}_{K-}}{\left ( \mu^{(\nu)}_{D-} \mu^{(\nu)}_{G+} \right )^2} \mathcal{V}_{DG-,\nu,p} \left ( h_n \right ) 
\end{align}

\appsubsubsection{Definitions of Bias-Corrected Estimators}

The bias-corrected estimators for $\pi_{+,\nu}$ and $\pi_{-,\nu}$ are defined as
\begin{align}
	\widehat{\pi}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widehat{\pi}_{+,\nu,p} \left ( h_n \right ) - h^{p+1-\nu} \widehat{\textsc{B}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widehat{\pi}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widehat{\pi}_{-,\nu,p} \left ( h_n \right ) - h^{p+1-\nu} \widehat{\textsc{B}}_{-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
where $\widehat{\textsc{B}}_{+,\nu,p,q} \left ( h_n, b_n \right )$ and $\widehat{\textsc{B}}_{-,\nu,p,q} \left ( h_n, b_n \right )$ are estimators for the bias of $\widetilde{\pi}_{+,\nu,p} \left ( h_n \right )$ and $\widetilde{\pi}_{-,\nu,p} \left ( h_n \right )$, respectively. Specifically,
\begin{align}
	\widehat{\textsc{B}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widehat{\textsc{B}}_{1+,\nu,p,q} \left ( h_n, b_n \right ) - \widehat{\textsc{B}}_{2+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widehat{\textsc{B}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widehat{\textsc{B}}_{1-,\nu,p,q} \left ( h_n, b_n \right ) - \widehat{\textsc{B}}_{2-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
with
\begin{align}
	\widehat{\textsc{B}}_{1+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right )} \widehat{\textsc{B}}_{J+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{J+,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widehat{\textsc{B}}_{2+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{G+,p} \left ( h_n \right )} \widehat{\textsc{B}}_{K+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{K+,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{G+,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{G+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widehat{\textsc{B}}_{1-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right )} \widehat{\textsc{B}}_{J-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{J-,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{D-,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{D-,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widehat{\textsc{B}}_{2-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\widehat{\mu}^{(\nu)}_{G-,p} \left ( h_n \right )} \widehat{\textsc{B}}_{K-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\widehat{\mu}^{(\nu)}_{K-,p} \left ( h_n \right )}{\left ( \widehat{\mu}^{(\nu)}_{G-,p} \left ( h_n \right ) \right )^2} \widehat{\textsc{B}}_{G-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
In addition, the bias-corrected linearized estimators for $\pi_{+,\nu}$ and $\pi_{-,\nu}$ are defined as
\begin{align}
	\widetilde{\pi}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widetilde{\pi}_{+,\nu,p} \left ( h_n \right ) - h^{p+1-\nu} \widecheck{\textsc{B}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widetilde{\pi}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widetilde{\pi}_{-,\nu,p} \left ( h_n \right ) - h^{p+1-\nu} \widecheck{\textsc{B}}_{-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
where
\begin{align}
	\widecheck{\textsc{B}}_{+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widecheck{\textsc{B}}_{1+,\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\textsc{B}}_{2+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widecheck{\textsc{B}}_{-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \widecheck{\textsc{B}}_{1-,\nu,p,q} \left ( h_n, b_n \right ) - \widecheck{\textsc{B}}_{2-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
with
\begin{align}
	\widecheck{\textsc{B}}_{1+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\textsc{B}}_{J+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\textsc{B}}_{D+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widecheck{\textsc{B}}_{2+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{G+}} \widehat{\textsc{B}}_{K+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^2} \widehat{\textsc{B}}_{G+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widecheck{\textsc{B}}_{1-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{D-}} \widehat{\textsc{B}}_{J-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{J-}}{\left ( \mu^{(\nu)}_{D-} \right )^2} \widehat{\textsc{B}}_{D-,\nu,p,q} \left ( h_n, b_n \right ) \\
	\widecheck{\textsc{B}}_{2-,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \frac{1}{\mu^{(\nu)}_{G-}} \widehat{\textsc{B}}_{K-,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{K-}}{\left ( \mu^{(\nu)}_{G-} \right )^2} \widehat{\textsc{B}}_{G-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}

\appsubsubsection{Variance of Bias-Corrected Linearized Estimators}

As in Theorem \ref{thm_asydist_bias}, suppose that Assumptions \ref{ass_moments} and \ref{ass_kernel} hold with $\delta \geq p+2$, and $n \min \left \{ h_n, b_n \right \} \to \infty$. Then, if $\max \left \{ h_n, b_n \right \} \to 0$, the conditional variance of the bias-corrected linearized estimator $\widetilde{\pi}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right )$ is
\begin{align}
	& \textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \equiv \Var \left [ \widetilde{\pi}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \notag \\
	& = \textsc{V}_{+,\nu,p} \left ( h_n \right ) + h_n^{2(p+1-\nu)} \textsc{V}_{+,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \notag \\
	& - 2 h_n^{p+1-\nu} \textsc{C}_{+,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{+,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
\end{align}
where
\begin{align}
	& \textsc{C}_{+,\nu,p,q} \left ( h_n, b_n \right ) \notag \\
	& \equiv \textsc{C}_{11+,\nu,p,q} \left ( h_n, b_n \right ) - \textsc{C}_{12+,\nu,p,q} \left ( h_n, b_n \right ) - \textsc{C}_{21+,\nu,p,q} \left ( h_n, b_n \right ) + \textsc{C}_{22+,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
with
\begin{align}
	\textsc{C}_{11+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Cov \left [ \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\mu}^{(\nu)}_{J+,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) , \widecheck{\textsc{B}}_{1+,\nu,p,q} \left ( h_n, b_n \right )\right ] \\
	\textsc{C}_{12+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Cov \left [ \frac{1}{\mu^{(\nu)}_{D+}} \widehat{\mu}^{(\nu)}_{J+,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^2} \widehat{\mu}^{(\nu)}_{D+,p} \left ( h_n \right ) , \widecheck{\textsc{B}}_{2+,\nu,p,q} \left ( h_n, b_n \right )\right ] \\
	\textsc{C}_{21+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Cov \left [ \frac{1}{\mu^{(\nu)}_{G+}} \widehat{\mu}^{(\nu)}_{K+,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^2} \widehat{\mu}^{(\nu)}_{G+,p} \left ( h_n \right ) , \widecheck{\textsc{B}}_{1+,\nu,p,q} \left ( h_n, b_n \right )\right ] \\
	\textsc{C}_{22+,\nu,p,q} \left ( h_n, b_n \right ) & \equiv \Cov \left [ \frac{1}{\mu^{(\nu)}_{G+}} \widehat{\mu}^{(\nu)}_{K+,p} \left ( h_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^2} \widehat{\mu}^{(\nu)}_{G+,p} \left ( h_n \right ) , \widecheck{\textsc{B}}_{2+,\nu,p,q} \left ( h_n, b_n \right )\right ]
\end{align}
or, equivalently,
\begin{align}
	\textsc{C}_{11+,\nu,p,q} \left ( h_n, b_n \right ) & = \frac{1}{\left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{JJ+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{C}_{JD+,\nu,p,q} \left ( h_n, b_n \right ) \notag \\
	& - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^3} \mathcal{C}_{DJ+,\nu,p,q} \left ( h_n, b_n \right ) + \frac{\left ( \mu^{(\nu)}_{J+} \right )^2}{\left ( \mu^{(\nu)}_{D+} \right )^4} \mathcal{C}_{DD+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\textsc{C}_{12+,\nu,p,q} \left ( h_n, b_n \right ) & = \frac{1}{\mu^{(\nu)}_{D+} \mu^{(\nu)}_{G+}} \mathcal{C}_{JK+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\mu^{(\nu)}_{D+} \left ( \mu^{(\nu)}_{G+} \right )^2} \mathcal{C}_{JG+,\nu,p,q} \left ( h_n, b_n \right ) \notag \\
	& - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{D+} \right )^2 \mu^{(\nu)}_{G+}} \mathcal{C}_{DK+,\nu,p,q} \left ( h_n, b_n \right ) + \frac{\mu^{(\nu)}_{J+} \mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{D+} \right )^2 \left ( \mu^{(\nu)}_{G+} \right )^2} \mathcal{C}_{DG+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\textsc{C}_{21+,\nu,p,q} \left ( h_n, b_n \right ) & = \frac{1}{\mu^{(\nu)}_{G+} \mu^{(\nu)}_{D+}} \mathcal{C}_{KJ+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\mu^{(\nu)}_{G+} \left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{KD+,\nu,p,q} \left ( h_n, b_n \right ) \notag \\
	& - \frac{\mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{G+} \right )^2 \mu^{(\nu)}_{D+}} \mathcal{C}_{GJ+,\nu,p,q} \left ( h_n, b_n \right ) + \frac{\mu^{(\nu)}_{K+} \mu^{(\nu)}_{J+}}{\left ( \mu^{(\nu)}_{G+} \right )^2 \left ( \mu^{(\nu)}_{D+} \right )^2} \mathcal{C}_{GD+,\nu,p,q} \left ( h_n, b_n \right ) \\
	\textsc{C}_{22+,\nu,p,q} \left ( h_n, b_n \right ) & = \frac{1}{\left ( \mu^{(\nu)}_{G+} \right )^2} \mathcal{C}_{KK+,\nu,p,q} \left ( h_n, b_n \right ) - \frac{\mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^3} \mathcal{C}_{KG+,\nu,p,q} \left ( h_n, b_n \right ) \notag \\
	& - \frac{\mu^{(\nu)}_{K+}}{\left ( \mu^{(\nu)}_{G+} \right )^3} \mathcal{C}_{GK+,\nu,p,q} \left ( h_n, b_n \right ) + \frac{\left ( \mu^{(\nu)}_{K+} \right )^2}{\left ( \mu^{(\nu)}_{G+} \right )^4} \mathcal{C}_{GG+,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
Symmetrically, the conditional variance of the bias-corrected linearized estimator $\widetilde{\pi}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right )$ is
\begin{align}
	& \textsc{V}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \equiv \Var \left [ \widetilde{\pi}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) \Big | \bm{R} \right ] \notag \\
	& = \textsc{V}_{-,\nu,p} \left ( h_n \right ) + h_n^{2(p+1-\nu)} \textsc{V}_{-,p+1,q} \left ( b_n \right ) \frac{\mathcal{B}^2_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!^2} \notag \\
	& - 2 h_n^{p+1-\nu} \textsc{C}_{-,\nu,p,q} \left ( h_n, b_n \right ) \frac{\mathcal{B}_{-,\nu,p,p+1} \left ( h_n \right )}{\left ( p+1 \right )!}
\end{align}
where
\begin{align}
	& \textsc{C}_{-,\nu,p,q} \left ( h_n, b_n \right ) \notag \\
	& \equiv \textsc{C}_{11-,\nu,p,q} \left ( h_n, b_n \right ) - \textsc{C}_{12-,\nu,p,q} \left ( h_n, b_n \right ) - \textsc{C}_{21-,\nu,p,q} \left ( h_n, b_n \right ) + \textsc{C}_{22-,\nu,p,q} \left ( h_n, b_n \right )
\end{align}
with the definitions of $\textsc{C}_{11-,\nu,p,q} \left ( h_n, b_n \right )$, $\textsc{C}_{12-,\nu,p,q} \left ( h_n, b_n \right )$, $\textsc{C}_{21-,\nu,p,q} \left ( h_n, b_n \right )$, and $\textsc{C}_{22-,\nu,p,q} \left ( h_n, b_n \right )$ mirroring the positive case.

\appsubsubsection{Asymptotic Distribution of Bias-Corrected Estimators}

As in part (D) of Theorem \ref{thm_asydist_bias}, provided that $n \min \left \{ h_n^{2p+3}, b_n^{2p+3} \right \} \max \left \{ h^2_n , b^{2 (q-p)}_n \right \} \to 0$ and $\kappa \max \left \{ h_n, b_n \right \} < \kappa_0$, then
\begin{align}
	T^{\textsc{rbc}}_{+} \left ( h_n, b_n \right ) & \equiv \frac{\widehat{\pi}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right ) - \pi_{+,\nu}}{\sqrt{\textsc{V}^{\textsc{bc}}_{+,\nu,p,q} \left ( h_n, b_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right ) \\
	T^{\textsc{rbc}}_{-} \left ( h_n, b_n \right ) & \equiv \frac{\widehat{\pi}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right ) - \pi_{-,\nu}}{\sqrt{\textsc{V}^{\textsc{bc}}_{-,\nu,p,q} \left ( h_n, b_n \right )}} \stackrel{d}{\to} \mathcal{N} \left ( 0,1 \right )
\end{align}
These statistics can be used to test whether $\pi_{+,\nu} = 0$ and $\pi_{-,\nu} = 0$, respectively. Failure to reject any of these null hypotheses may provide suggestive evidence in favor of the plausibility of the common trends assumption.


\end{document}