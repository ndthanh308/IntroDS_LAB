\section{Introduction}
3D object detection is a fundamental task in many real-world applications such as robotics and autonomous driving. Although LiDAR-based methods~\cite{yin2021center, zhou2018voxelnet, pan20213d, wang2021object} have achieved impressive results with the help of accurate 3D perception from LiDAR, multi-view camera-based methods have recently received extensive attention because of their low cost for deployment and distinctive capability of long-range and color perception. Classical multi-view camera-based 3D object detection approaches mostly follow monocular frameworks which first perform 2D/3D object detection in each individual view and then conduct cross-view post-processing to obtain the final result. While remarkable progress has been made~\cite{wang2022probabilistic,wang2021fcos3d,park2021pseudo,reiher2020sim2real,bruls2019right}, such a framework cannot fully utilize cross-view information and usually leads to low performance.

To eliminate the ineffective cross-view post-processing, several end-to-end approaches~\cite{philion2020lift,li2022bevdepth,huang2021bevdet, li2022bevformer, wang2022detr3d,liu2022petr,liu2022petrv2} have been developed. These approaches normally contain three important modules: a backbone for 2D image feature extraction, a feature lifting module for transforming multi-view 2D image features into a unified 3D space (e.g. BEV space in an ego coordinate system) to obtain lifted features, and a detection head for performing object detection by taking as input the lifted features. Among these modules, the feature lifting module serves as an important component to bridge the 2D backbone and the 3D detection head, whose quality will greatly affect the final detection performance.

% Figure environment removed

To perform feature lifting, recent methods usually predefine a set of 3D anchors in the ego coordinate system sparsely or uniformly, with randomly initialized content features. After that, they lift 2D image features into the 3D anchors to obtain the lifted features.
Some methods~\cite{li2022bevdepth,huang2021bevdet,huang2022bevdet4d,wang2022sts,li2022bevstereo} utilize a straightforward lift and splat mechanism~\cite{philion2020lift} by first lifting 2D image features into pseudo LiDAR features in an ego coordinate system using estimated depth and then assigning the pseudo LiDAR features to their closest 3D anchors to obtain the final lifted features, as shown in Fig.~\ref{fig.intro}(a). Although such methods have achieved remarkable performance, due to their excessive resource consumption when lifting 2D image features, they normally cannot utilize multi-scale feature maps, which are crucial for detecting small (faraway) objects. 

% \vspace{-1.568mm}
Instead of lift and splat, some other works utilize 2D attention to perform feature lifting. Such methods treat predefined 3D anchors as 3D queries and propose several ways to aggregate 2D image features (keys) to 3D anchors. Typical 2D attention mechanisms that have been explored include dense attention ~\cite{liu2022petr}, point attention (a degenerated deformable attention) ~\cite{detr,chen2022graph}, and deformable attention ~\cite{li2022bevformer,yang2022bevformer,chu2023oa}. 2D attention-based methods can naturally work with Transformer-like architectures to progressively refine the lifted features layer by layer. Moreover, deformable attention makes it possible to use multi-scale 2D features, thanks to its introduced sparse attention computation. 
However, the major weakness of 2D attention-based approaches is that they suffer from a crucial depth ambiguity problem, as shown in Fig.~\ref{fig.intro}(b,c). That is, due to the ignorance of depth information, when projecting 3D queries to a 2D view, many 3D queries will end up with the same 2D position with similar sampling points in the 2D view. This will result in highly entangled aggregated features and lead to wrong predictions along a ray connecting the ego car and a target object (as visualized in Fig.~\ref{fig.visualization}).
Although some concurrent works~\cite{lin2022sparse4d, dabev} have tried to alleviate this issue 
, they can not fix the problem from the root. 
The root cause lies in the lack of depth information when applying 2D attention to sample features in a 2D pixel coordinate system\footnote{The pixel coordinate system that only considers the $u,v$ axes.}.

To address the above problems, in this paper, we propose a basic operator called 3D deformable attention (DFA3D), built upon which we develop a novel feature lifting approach, as shown in Fig. \ref{fig.intro}(d). We follow \cite{philion2020lift} to leverage a depth estimation module to estimate a depth distribution for each 2D image feature. We expand the dimension of each single-view 2D image feature map by computing the outer product of them and their estimated depth distributions to obtain the expanded 3D feature maps in a 3D pixel coordinate system\footnote{The pixel coordinate system that considers all of the $u,v,d$ axes.}. Each 3D query in the BEV space is projected to the 3D pixel space with a set of predicted 3D sampling offsets to specify its 3D receptive field and pool features from the expanded 3D feature maps. 
In this way, a 3D query close to the ego car only pools image features with smaller depth values, whereas a 3D query far away from the ego car mainly pools image features with larger depth values, and thus the depth ambiguity problem is effectively alleviated.

In our implementation, as each of the expanded 3D feature maps is an outer product between a 2D image feature map and its estimated depth distributions, we do not need to maintain a 3D tensor in memory to avoid excessive memory consumption. Instead, we seek assistance from math to simplify DFA3D into a mathematically equivalent depth-weighted 2D deformable attention and implement it through CUDA, making it both memory-efficient and fast. The Pytorch interface of DFA3D is very similar to 2D deformable attention and only requires a few modifications to replace 2D deformable attention, making our feature lifting approach easily portable.

In summary, our main contributions are:
\begin{enumerate}[itemsep=0pt, topsep=0pt, parsep=0pt]

    \item We propose a basic operator called 3D deformable attention (DFA3D) for feature lifting. Leveraging the property of outer product between 2D features and their estimated depth, we develop a memory-efficient and fast implementation. 
    
    \item Based on DFA3D, we develop a novel feature lifting approach, which not only alleviates the depth ambiguity problem from the root, but also benefits from multi-layer feature refinement of a Transformer-like architecture.
    Thanks to the simple interface of DFA3D, 
    our DFA3D-based feature lifting can be implemented in any method that utilizes 2D deformable attention (also its degeneration)-based feature lifting with only a few modifications in code.
    
    \item The consistent performance improvement (+1.41 mAP on average) in comparative experiments on the nuScenes~\cite{caesar2020nuscenes} dataset demonstrate the superiority and generalization ability of DFA3D-based feature lifting.
\end{enumerate}
  