\section{Related Work}

\noindent
\textbf{Multi-view 3D Detectors with Lift-Splat-based Feature Lifting: }
To perform 3D tasks in the multi-view context in an end-to-end pipeline, Lift-Splat~\cite{philion2020lift} proposes to first lift multi-view image features by performing outer product between 2D image feature maps and their estimated depth, and then transform them to a unified 3D space through camera parameters to obtain the lifted pseudo LiDAR features. Following the LiDAR-based methods, the pseudo LiDAR features are then splatted to predefined 3D anchors through a voxel-pooling operation to obtain the lifted features. The lifted features are then utilized to facilitate the following downstream tasks. For example, BEVDet~\cite{huang2021bevdet} proposes to adopt such a pipeline for 3D detection and verifies its feasibility. Although being effective, such a method suffers from the problem of huge memory consumption, which prevents them from using mutli-scale feature maps. Moreover, due to the fixed assignment rule between the pseudo LiDAR features and the 3D anchors, each 3D anchor can only interact with image features once without any further adjustment later. This problem makes Lift-Splat-based methods reliant on the quality of depth. The recently proposed BEVDepth~\cite{li2022bevdepth} makes a full analysis and validates that the learned depth quality has a great impact on performance. As the implicitly learned depth in previous works does not satisfy the accuracy requirement, BEVDepth proposes to resort to monocular depth estimation for help. By explicitly supervising monocular depth estimation with ground truth depth obtained by projecting LiDAR points onto multi-view images, BEVDepth achieves a remarkable performance. Inspired by BEVDepth, instead of focusing on the above-discussed problems of refinement or huge memory consumption, many works~\cite{li2022bevstereo,wang2022sts} try to improve their performance through improving depth quality by resorting to multi-view stereo or temporal information. However, due to small overlap regions between multi-view images and also the movements of surrounding objects, they still rely on the monocular depth estimation, which is an ill-posed problem and can not be accurately addressed. 



\noindent
\textbf{Multi-view 3D Detectors with 2D Attention-based Feature Lifting: }
Motivated by the progress in 2D detection~\cite{ren2023detrex, ren2023strong, liu2023detection, liu2023grounding, zhang2022dino, li2022dn, liu2022dab}, many works propose to introduce attention into camera-based 3D detection. They treat multi-view image features as keys and values and the predefined 3D anchors as 3D queries in the unified 3D space. The 3D queries are projected onto multi-view 2D image feature maps according to the extrinsic and intrinsic camera parameters and lift features from the image feature maps through cross attention like the decoder part of 2D detection methods~\cite{detr,deformable_detr}. 
PETR~\cite{liu2022petr} and PETRv2~\cite{liu2022petrv2} are based on the classical (dense) attention mechanism, where the interactions between each 3D query and all image features (keys) make it inefficient. 
DETR3D-like methods~\cite{wang2022detr3d, chen2022graph} proposes to utilize the point attention which lets a query interact with only one key obtained by bilinear interpolation according to its projected location (also called point deformable attention~\cite{li2022bevformer}). Although being efficient, the point attention has a small receptive field in multi-view 2D image feature maps and normally results in a relatively weak performance. 
By contrast, BEVFormer~\cite{li2022bevformer, yang2022bevformer} proposes to apply 2D deformable attention to let each 3D query interact with a local region around the location where the 3D query is projected to and obtains a remarkable result. OA-BEV~\cite{chu2023oa} tries to introduce 2D detection into the pipeline to guide the network to further focus on the target objects. 
Benefiting from the Transformer-like architecture, the lifted features obtained by 2D attention can be progressively refined layer-by-layer.
However, the methods with 2D attention-based feature lifting suffer from the problem of depth ambiguity: when projecting 3D queries onto a camera view, those with the same projected coordinates $(u, v)$ but different depth values $d$ end up with the same reference point and similar sampling points in the view. Consequently, these 3D queries will aggregate features that are strongly entangled and thus result in duplicate predictions along a ray connecting the ego car and a target object in BEV. 
Some concurrent works~\cite{dabev,lin2022sparse4d} have noticed this problem, but none of them tackle the problem from the root.

Instead of conducting feature lifting using existing operators, such as voxel-pooling and 2D deformable attention, we develop a basic operator DFA3D to perform feature lifting. As summarized in Table~\ref{tab:compare_rel}, our proposed DFA3D-based feature lifting can not only benefit from multi-layer refinement but also address the depth ambiguity problem. Moreover, DFA3D-based feature lifting can take multi-scale feature maps into consideration efficiently.

\input{tab/related_work_compare}
