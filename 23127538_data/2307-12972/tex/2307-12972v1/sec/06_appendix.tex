\onecolumn
\section*{Appendix}

\appendix



\section{Closer visualization of the expanded 3D image features.}
In Sec.~\ref{subsec.feature_expanding}, for a specific view (the $n^{th}$ view for example), we obtain the expanded 3D image feature map by conducting outer product at last dimension between the estimated depth distribution $\boldsymbol{D}_{n} \in \mathbb{R}^{H\times W\times D}$ and 2D image feature maps $\boldsymbol{X}_n \in \mathbb{R}^{H\times W\times C}$. To further explain the resulting expanded 3D image feature $\boldsymbol{F}_n \in \mathbb{R}^{H\times W\times D\times C}$, we show a closer visualization in Fig.~\ref{fig.further_explanation}. For the features with the same $(u_i, v_j)$ coordinate (zoomed in), they share the same 2D image feature  $\boldsymbol{X}_{n,u_i,v_j}$ and depth distribution $\boldsymbol{D}_{n,u_i,v_j}$. However, since they have different depth values ${d_k}$, they select different depth confidence scores ${\boldsymbol{D}_{n,u_i,v_j,d_k}}$, resulting in different features $\{\boldsymbol{D}_{n,u_i,v_j,d_k}\cdot\boldsymbol{X}_{n,u_i,v_j}\}$.
% Figure environment removed

\section{Can 3D deformable attention be trivially implemented by feature weighting followed with a common 2D deformable attention?}
As each sampling point in 3D deformable attention has its own 3D location, it will lead to its own depth score for 4 adjacent image features when conducting weighted bilinear interpolation. 
Inevitably, there will be features ($\boldsymbol{X}_{n,u_1,v_0}$ and $\boldsymbol{X}_{n,u_1,v_1}$ in Fig.~\ref{fig.on_the_fly}) referred by more than one sampling point when sampling points are located in neighboring grids.  In such cases, feature weighting will results in conflicts. Thus, we can not simply prepare 2D features by depth-weighting and then conduct the common 2D deformable attention. The depth-weighted feature computation should be conducted on the fly. 

% Figure environment removed

\newpage
\section{Applicability}
As the comparison shown in Fig.~\ref{fig.code_compare}, when integrating our {\methodname} in any 2D deformable attention-based feature lifting only requires a few modifications in code. The main modification lies in the addition of DepthNet and replacing 2D deformable attention with our 3D deformable attention.

% Figure environment removed

\section{Visualization}
We visualize the predictions of BEVFormer and BEVFormer-{\methodname} in Fig.~\ref{fig.visualization}. The shaded triangles correspond to camera rays, in which BEVFormer makes more duplicate predictions behind or in front of ground truth objects compared with BEVFormer-{\methodname}.
It demonstrates the negative effects of depth ambiguity in 2D deformable attention. 
After integrating with \methodname, the wrong predictions caused by the depth ambiguity problem are reduced.


% Figure environment removed