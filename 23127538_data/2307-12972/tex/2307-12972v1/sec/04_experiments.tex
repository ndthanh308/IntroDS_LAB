\section{Experiments}
We conduct extensive experiments to evaluate the effectiveness of DFA3D-based feature lifting compared with other feature lifting methods. We integrate DFA3D into several open-source methods to verify its generalization ability and portability.  

\subsection{Experiment Setup}

\paragraph{Dataset and Metrics}
We follow previous works~\cite{li2022bevformer, li2022bevdepth, wang2022detr3d} to conduct experiments on the nuScenes dataset~\cite{caesar2020nuscenes}. The nuScenes dataset contains 1,000 sequences, which are captured by a variety of sensors (e.g. cameras, RADAR, LiDAR). Each sequence is about 20 seconds, 
and is annotated in 2 frames/second. Since we focus on the camera-based 3D detection task, we use only the image data taken from 6 cameras. 
There are 40k samples with 1.4M annotated 3D bounding boxes in total. We use the classical splits of training, validation, and testing set with 28k, 6k, and 6k samples, respectively.
We follow previous works~\cite{li2022bevformer, li2022bevdepth, wang2022detr3d} to take the official evaluation metrics, including mean average precision (mAP), ATE, ASE, AOE, AVE, and AAE for our evaluations. 
These six metrics evaluate results from center distance, translation, scale, orientation, velocity, and attribute, respectively. 
In addition, a comprehensive metric called nuScenes detection score (NDS) is also provided.


\vspace{-0.5cm}
\paragraph{Implementation Details}
For fair comparisons, when implementing DFA3D on the open-source methods, we keep all configurations the same without bells and whistles.
%, including backbone, batch size, and learning schedule. 
For all experiments, without specification, we take BEVFormer-base~\cite{li2022bevformer} as our baseline, which uses a ResNet101-DCN~\cite{he2016deep,dai2017deformable} backbone initialized from FCOS3D~\cite{wang2021fcos3d} checkpoint and a Transformer with 6 encoder layers and 6 decoder layers. 
All experiments are trained with 24 epochs using AdamW optimizer~\cite{adamw} with a base learning rate of $2\times 10^{-4}$ on 8 NVIDIA Tesla A100 GPUs.

\subsection{Comparisons of Feature Lifting Methods}
To compare different feature lifting methods fairly, we set feature lifting as the only variable and keep the others the same. More specifically, we block the temporal information and utilize the same 2D backbone and 3D detection head. The results are shown in Table~\ref{tab:operator}.

We first compare four different feature lifting methods in the first four rows in Table~\ref{tab:operator}. All models use one layer for a fair comparison. The results show that {\methodname} outperforms all previous works, which demonstrates the effectiveness. 
Although the larger receptive field makes 2D deformable attention (DFA2D)-based method obtains much better result than the point attention-based method, its performance is not satisfactory enough compared with the Lift-Splat-based one. One key difference between DFA2D and Lift-Splat is their depth utilization. The extra depth information enables the Lift-Splat-based method to achieve a better performance, although the depth information is predicted and not accurate enough. 
Different from the DFA2D-based feature lifting, DFA3D enables the usage of depth information in the deformable attention mechanism and helps our DFA3D-based feature lifting achieve the best performance.

As discussed in Sec.~\ref{subsec.analysis}, the adjustable assignment rule enables attention-based methods to conduct multi-layer refinement. With the help of one more layer refinement, DFA2D-based feature lifting surpasses Lift-Splat. However, benefitting from the better depth utilization, our DFA3D-based feature lifting still maintains the superiority.


\input{tab/operator.tex}

\input{tab/generalization_method_size.tex}

\subsection{Generalizations for Different Models}
To verify the generalization and portability of DFA3D-based feature lifting, we evaluate it on various open-sourced methods that rely on 2D deformable attention or point attention-based feature lifting. 
Thanks to the mathematical simplification of DFA3D, our DFA3D-based feature lifting can be easily integrated into these methods with only a few code modifications (please refer to the appendix for a more intuitive comparison). 


We compare baselines to those that integrate DFA3D-based feature lifting in Table~\ref{tab:generalization_method_size}. 
The results show that DFA3D brings consistent improvements in different methods, indicating its generalization ability across different models. Furthermore, DFA3D introduces significant gains of $1.7$, $3.1$, and $1.4$ mAP on BEVFormer-t\footnote{We use the BEVFormer-t, BEVFormer-s, and BEVFormer-b for the BEVFormer-tiny, BEVFormer-small, and BEVFormer-base variants in \url{https://github.com/fundamentalvision/BEVFormer}.}, BEVFormer-s, and BEVFormer-b, respectively, showing the effectiveness of DFA3D across different model sizes. Note that, for a fair comparison, we use one sampling point and fixed sampling offsets when conducting experiments on DETR3D~\cite{wang2022detr3d}.


For a more comprehensive comparison, we also conduct experiments on two concurrent works DA-BEV~\cite{dabev} and Sparse4D~\cite{lin2022sparse4d}, who design their own modules to address the depth ambiguity problem implicitly or through post-refinement. 
Differently, we solve the problem from the root at the feature lifting process, which is a more principled solution. The results show that, even with their efforts as a foundation, DFA3D achieves $+0.7$ mAP, $+0.8$ mAP and $+1.0$ mAP improvement over DA-BEV-S, DA-BEV~\cite{dabev} and Sparse4D~\cite{lin2022sparse4d} respectively, which verifies the necessity of our new feature lifting approach.






\subsection{Ablations}
\paragraph{Mixture of 3D and 2D Deformable Attention-based Feature Lifting.}
We verify the effectiveness of our DFA3D-based feature lifting by mixing DFA2D-based and DFA3D-based feature lifting in the encoder of BEVFormer. 
As shown in Table~\ref{tab:num_layer}, in the first $N_{2D}$ layers we use the DFA2D-based feature lifting, and in the following $N_{3D}$ layers we use the DFA3D-based one.
The results show a consistent improvement with the usage of DFA3D-based feature lifting increases. The progressive improvements indicate that the more DFA3D introduced, the less depth ambiguity problem is, which finally results in a better performance.


\input{tab/num_layer.tex}






\input{tab/dep_sup_and_quality}

\paragraph{Effects of DepthNet Module and Depth Quality.}
As shown in Table~\ref{tab:depth_sup_and_quality}, we first present the influence of the DepthNet module.
Compared with the 1.4\% mAP gain (Row 4 vs. Row 1) brought by DFA3D, simply equipping BEVFormer with DepthNet and supervising it brings in a 0.4\% mAP gain (Row 2 vs. Row 1), which is relatively marginal.
The comparison indicates that the main improvement does not come from the additional parameters and supervision of DepthNet, but from a better feature lifting.

We then evaluate the influence of depth qualities. We experiment with three different depth, 1) learned from unsupervised learning, 2) learned from supervised learning, and 3) ground truths generated from LiDAR. The corresponding results of these different depth qualities are available in Rows 3, 4, and 5 in Table~\ref{tab:depth_sup_and_quality}. The results show that the performance improves with better depth quality. Remarkably, the model variant with ground truth depth achieves 56.7\% mAP, outperforming baselines by 15.1\% mAP. It indicates a big improvement space which deserves a further in-depth study of obtaining better depth estimation. 



\paragraph{Effects of Feature Querying Methods.}
Deformable attention enables querying features dynamically. It generates sampling points according to the queried features from the previous layer. Inspired by previous work~\cite{wang2022detr3d}, we freeze the position of sampling points, under which the attention operation actually degenerates to the trilinear interpolation. We compare the degenerated one with the default one in Table~\ref{tab:onepointordeform}. The results show the effectiveness of the deformable operation. The dynamic sampling can help focus on more important positions and enable better feature querying.
\vspace{-0.15cm}

\input{tab/onepoint_or_deformable}

\input{tab/efficiency_source_consum}
\paragraph{Efficiency and resource consumption.}
We evaluate the efficiency and resource consumption of DFA3D based on BEVFormer, as shown in Table~\ref{tab:efficiency}. DFA3D-based feature lifting requires a little more resources and is slightly slower. We clarify that the side effects, especially the extra parameters mainly come from DepthNet, which is adjustable and can be optimized with the development of depth estimation.

