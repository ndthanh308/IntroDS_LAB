\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{adi2018turning}
Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.
\newblock Turning your weakness into a strength: Watermarking deep neural
  networks by backdooring.
\newblock In {\em 27th USENIX Security Symposium (USENIX Security 18)}, pages
  1615--1631, 2018.

\bibitem{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12449--12460, 2020.

\bibitem{barni2019new}
Mauro Barni, Kassem Kallas, and Benedetta Tondi.
\newblock A new backdoor attack in cnns by training set corruption without
  label poisoning.
\newblock In {\em 2019 IEEE International Conference on Image Processing
  (ICIP)}, pages 101--105. IEEE, 2019.

\bibitem{chen2018detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock {\em arXiv preprint arXiv:1811.03728}, 2018.

\bibitem{chen2017targeted}
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock {\em arXiv preprint arXiv:1712.05526}, 2017.

\bibitem{chou2020sentinet}
Edward Chou, Florian Tramer, and Giancarlo Pellegrino.
\newblock Sentinet: Detecting localized universal attacks against deep learning
  systems.
\newblock In {\em 2020 IEEE Security and Privacy Workshops (SPW)}, pages
  48--54. IEEE, 2020.

\bibitem{clements2018backdoor}
Joseph Clements and Yingjie Lao.
\newblock Backdoor attacks on neural network operations.
\newblock In {\em 2018 IEEE Global Conference on Signal and Information
  Processing (GlobalSIP)}, pages 1154--1158. IEEE, 2018.

\bibitem{du2019robust}
Min Du, Ruoxi Jia, and Dawn Song.
\newblock Robust anomaly detection and backdoor attack detection via
  differential privacy.
\newblock {\em arXiv preprint arXiv:1911.07116}, 2019.

\bibitem{dumford2020backdooring}
Jacob Dumford and Walter Scheirer.
\newblock Backdooring convolutional neural networks via targeted weight
  perturbations.
\newblock In {\em 2020 IEEE International Joint Conference on Biometrics
  (IJCB)}, pages 1--9. IEEE, 2020.

\bibitem{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock {\em arXiv preprint arXiv:1708.06733}, 2017.

\bibitem{hayase2021spectre}
Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh.
\newblock Spectre: Defending against backdoor attacks using robust statistics.
\newblock In {\em International Conference on Machine Learning}, pages
  4129--4139. PMLR, 2021.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hong2020effectiveness}
Sanghyun Hong, Varun Chandrasekaran, Yi{\u{g}}itcan Kaya, Tudor Dumitra{\c{s}},
  and Nicolas Papernot.
\newblock On the effectiveness of mitigating data poisoning attacks with
  gradient shaping.
\newblock {\em arXiv preprint arXiv:2002.11497}, 2020.

\bibitem{huang2022backdoor}
Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren.
\newblock Backdoor defense via decoupling the training process.
\newblock {\em arXiv preprint arXiv:2202.03423}, 2022.

\bibitem{kaviani2021defense}
Sara Kaviani and Insoo Sohn.
\newblock Defense against neural trojan attacks: A survey.
\newblock {\em Neurocomputing}, 423:651--667, 2021.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{le2015tiny}
Ya Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 7(7):3, 2015.

\bibitem{li2020invisible}
Shaofeng Li, Minhui Xue, Benjamin Zi~Hao Zhao, Haojin Zhu, and Xinpeng Zhang.
\newblock Invisible backdoor attacks on deep neural networks via steganography
  and regularization.
\newblock {\em IEEE Transactions on Dependable and Secure Computing},
  18(5):2088--2105, 2020.

\bibitem{li2018hu}
Wenshuo Li, Jincheng Yu, Xuefei Ning, Pengjun Wang, Qi Wei, Yu Wang, and
  Huazhong Yang.
\newblock Hu-fu: Hardware and software collaborative attack framework against
  neural networks.
\newblock In {\em 2018 IEEE Computer Society Annual Symposium on VLSI
  (ISVLSI)}, pages 482--487. IEEE, 2018.

\bibitem{li2021deeppayload}
Yuanchun Li, Jiayi Hua, Haoyu Wang, Chunyang Chen, and Yunxin Liu.
\newblock Deeppayload: Black-box backdoor attack on deep learning models
  through neural payload injection.
\newblock In {\em 2021 IEEE/ACM 43rd International Conference on Software
  Engineering (ICSE)}, pages 263--274. IEEE, 2021.

\bibitem{li2022backdoor}
Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia.
\newblock Backdoor learning: A survey.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem{li2021invisible}
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 16463--16472, 2021.

\bibitem{li2021anti}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock {\em Advances in Neural Information Processing Systems},
  34:14900--14912, 2021.

\bibitem{li2021neural}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma.
\newblock Neural attention distillation: Erasing backdoor triggers from deep
  neural networks.
\newblock {\em arXiv preprint arXiv:2101.05930}, 2021.

\bibitem{li2020open}
Yiming Li, Ziqi Zhang, Jiawang Bai, Baoyuan Wu, Yong Jiang, and Shu-Tao Xia.
\newblock Open-sourced dataset protection via backdoor watermarking.
\newblock {\em arXiv preprint arXiv:2010.05821}, 2020.

\bibitem{liu2017trojaning}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock 2017.

\bibitem{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem{moosavi2017universal}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
  Frossard.
\newblock Universal adversarial perturbations.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1765--1773, 2017.

\bibitem{nguyen2021wanet}
Anh Nguyen and Anh Tran.
\newblock Wanet--imperceptible warping-based backdoor attack.
\newblock {\em arXiv preprint arXiv:2102.10369}, 2021.

\bibitem{nguyen2020input}
Tuan~Anh Nguyen and Anh Tran.
\newblock Input-aware dynamic backdoor attack.
\newblock {\em Advances in Neural Information Processing Systems},
  33:3454--3464, 2020.

\bibitem{qiu2021deepsweep}
Han Qiu, Yi Zeng, Shangwei Guo, Tianwei Zhang, Meikang Qiu, and Bhavani
  Thuraisingham.
\newblock Deepsweep: An evaluation framework for mitigating dnn backdoor
  attacks using data augmentation.
\newblock In {\em Proceedings of the 2021 ACM Asia Conference on Computer and
  Communications Security}, pages 363--377, 2021.

\bibitem{rakin2020tbt}
Adnan~Siraj Rakin, Zhezhi He, and Deliang Fan.
\newblock Tbt: Targeted neural network attack with bit trojan.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13198--13207, 2020.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{saha2020hidden}
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash.
\newblock Hidden trigger backdoor attacks.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 11957--11965, 2020.

\bibitem{shan2021using}
Shawn Shan.
\newblock Using honeypots to catch adversarial attacks on neural networks.
\newblock In {\em Proceedings of the 8th ACM Workshop on Moving Target
  Defense}, pages 25--25, 2021.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{tang2021demon}
Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang.
\newblock Demon in the variant: Statistical analysis of $\{$DNNs$\}$ for robust
  backdoor contamination detection.
\newblock In {\em 30th USENIX Security Symposium (USENIX Security 21)}, pages
  1541--1558, 2021.

\bibitem{tran2018spectral}
Brandon Tran, Jerry Li, and Aleksander Madry.
\newblock Spectral signatures in backdoor attacks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{turner2018clean}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Clean-label backdoor attacks.
\newblock 2018.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2019symmetric}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 322--330, 2019.

\bibitem{weng2020trade}
Cheng-Hsin Weng, Yan-Ting Lee, and Shan-Hung~Brandon Wu.
\newblock On the trade-off between adversarial and backdoor robustness.
\newblock {\em Advances in Neural Information Processing Systems},
  33:11973--11983, 2020.

\bibitem{wu2021adversarial}
Dongxian Wu and Yisen Wang.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16913--16925, 2021.

\bibitem{xia2022data}
Pengfei Xia, Ziqiang Li, Wei Zhang, and Bin Li.
\newblock Data-efficient backdoor attacks.
\newblock {\em arXiv preprint arXiv:2204.12281}, 2022.

\bibitem{xiong2016achieving}
Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank Seide, Mike Seltzer, Andreas
  Stolcke, Dong Yu, and Geoffrey Zweig.
\newblock Achieving human parity in conversational speech recognition.
\newblock {\em arXiv preprint arXiv:1610.05256}, 2016.

\bibitem{zeng2022narcissus}
Yi Zeng, Minzhou Pan, Hoang~Anh Just, Lingjuan Lyu, Meikang Qiu, and Ruoxi Jia.
\newblock Narcissus: A practical clean-label backdoor attack with limited
  information.
\newblock {\em arXiv preprint arXiv:2204.05255}, 2022.

\bibitem{zhao2020bridging}
Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan~Natesan Ramamurthy, and Xue Lin.
\newblock Bridging mode connectivity in loss landscapes and adversarial
  robustness.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{zhao2021deep}
Shihao Zhao, Xingjun Ma, Yisen Wang, James Bailey, Bo Li, and Yu-Gang Jiang.
\newblock What do deep nets learn? class-wise patterns revealed in the input
  space.
\newblock {\em arXiv preprint arXiv:2101.06898}, 2021.

\bibitem{zhao2020clean}
Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang
  Jiang.
\newblock Clean-label backdoor attacks on video recognition models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 14443--14452, 2020.

\end{thebibliography}
