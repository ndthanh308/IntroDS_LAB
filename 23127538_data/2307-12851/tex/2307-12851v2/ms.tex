\documentclass[11pt,letter]{article}
\usepackage[papersize={8.5in,11in},margin=1in]{geometry}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{pdfpages}
\newcommand{\dtlinkcolor}{{0.8 0.8 1}} % link color, RGB
\usepackage[hyperindex=true,pdfpagemode=UseOutlines,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,pdfstartview=FitH,pdfborder={0 0 1},linkbordercolor=\dtlinkcolor,citebordercolor=\dtlinkcolor,urlbordercolor=\dtlinkcolor,pagebordercolor=\dtlinkcolor]{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{palatino}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{enumitem}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{hanch_sty}
\input{edits.tex}
\usepackage{bbold,amsmath,amsthm,amssymb}
\usepackage{multirow}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\sign}{\mathrm{sign}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}

\newtheorem{innercustomasump}{Assumption}
\newenvironment{customasump}[1]
  {\renewcommand\theinnercustomasump{#1}\innercustomasump}
  {\endinnercustomasump}

\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
 \newtheorem{innercustomlem}{Lemma}
\newenvironment{customlem}[1]
  {\renewcommand\theinnercustomlem{#1}\innercustomlem}
  {\endinnercustomlem}
  
   \newtheorem{innercustomprop}{Proposition}
\newenvironment{customprop}[1]
  {\renewcommand\theinnercustomprop{#1}\innercustomprop}
  {\endinnercustomprop}
  
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem*{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{theorem_sec}{Theorem}[section]
\newtheorem{lemma_sec}[theorem_sec]{Lemma}
\newtheorem{example}{Example}
\newtheorem*{claim}{Claim}
\newtheorem{claim_sec}{Claim}[section]

\title{Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization}



\author{%
  Hancheng Min\footnotemark[1], Enrique Mallada\footnotemark[2], and Ren\'e Vidal\footnotemark[1] \\
  \\
  \footnotemark[1] Center for Innovation in Data Engineering and Science, University of Pennsylvania \\
  \footnotemark[2] Electrical and Computer Engineering, Johns Hopkins University 
  \\
}

\begin{document}

\maketitle
\begin{abstract}
%    This paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks under small initialization. 
    This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.
\end{abstract}

\section{Introduction}
\vspace{-1mm}
Neural networks have shown excellent empirical performance in many application domains such as vision~\citep{krizhevsky2012imagenet}, speech~\citep{hinton2012deep} and video games~\citep{silver2016mastering}. Despite being highly overparametrized, networks trained by gradient descent with random initialization and without explicit regularization enjoy good generalization performance. One possible explanation for this phenomenon is the implicit bias or regularization induced by first-order algorithms under certain initialization assumptions. For example, first-order methods applied to (deep) matrix factorization models may produce solutions that have low nuclear norm~\citep{gunasekar2017implicit} and/or low rank~\citep{arora2019implicit}, and similar phenomena have been observed for deep tensor factorization~\citep{razin2022implicit}. Moreover, prior work such as \citep{saxe2014exact,stoger2021small} has found that deep linear networks sequentially learn the dominant singular values of the input-output correlation matrix. 

It is widely known that these sparsity-inducing biases can often be achieved by small initialization. This has motivated a series of works that theoretically analyze the training dynamics of first-order methods for neural networks with small initialization. For linear networks, the implicit bias of small initialization has been studied in the context of linear regression~\citep{saxe2014exact,Gidel2019,mtvm21icml,varre2023on} and matrix factorization~\citep{gunasekar2017implicit,arora2019implicit,li2018small,li2021towards,stoger2021small,yaras2023law,soltanolkotabi2023implicit}. Recently, the effect of small initialization has been studied for two-layer ReLU networks~\citep{maennel2018gradient,lyu2021gradient,phuong2021inductive,boursier2022gradient}. For example, \citet{maennel2018gradient} observes that during the early stage of training, neurons in the first layer converge to one out of finitely many directions determined by the dataset. Based on this observation, \citet{phuong2021inductive} shows that in the case of well-separated data, where any pair of input data with the same label are positively correlated and any pair with different labels are negatively correlated, there are only two directions the neurons tend to converge to: the positive data center and the negative one. Moreover, \citet{phuong2021inductive} shows that if such directional convergence holds, then the loss converges, and the resulting first-layer weight matrix is low-rank. However, directional convergence is assumed in their analysis; there is no explicit characterization of how long it takes to achieve directional convergence and how the time to convergence depends on the initialization scale. 
% \enrique{The above paragraph points out to some limitations of [19], but what's not explicitly said is that the "analysis" of [17] is (at best) valid for vanishing initialization. In the next section, you say that we provide a complete analysis, but I don't think that case that the prior analysis is incomplete is fully fleshed out.}
% \hancheng{left to in-person discussion}
\paragraph{Paper contributions:} In this paper, we provide a complete analysis of the dynamics of gradient flow for training a two-layer ReLU network on well-separated data with small initialization. Specifically, we show that if the initialization is sufficiently small, during the early phase of training the neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. Moreover, through a careful analysis of the neuron's directional dynamics we show that the time it takes for all neurons to achieve good alignment with the input data is upper bounded by $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$, where $n$ is the number of data points and $\mu$ measures how well the data are separated. We also show that after the early alignment phase the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate and that the weight matrix on the first layer is approximately low-rank. 
%In this paper, we provide a complete analysis for the problem of training two-layer ReLU networks on well-separated data under small initialization. Specifically, we show under a sufficiently small initialization scale, we show that during the early phase of  training, the neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\log\frac{1}{\epsilon})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $\epsilon$ is the initialization scale. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. 


% \subsection{Other related work}
% \begin{itemize}
%     \item NTK
%     \item convergence and implicit bias of linear networks
% \end{itemize}

\paragraph{Notations:} We denote the Euclidean norm of a vector $x$ by $\|x\|$, the inner product between the vectors $x$ and $y$ by $\lan x,y\ran=x^\top y$, and the cosine of the angle between them as $\cos(x,y)=\langle \frac{x}{\|x\|},\frac{y}{\|y\|}\rangle$. For an $n\by m$ matrix $A$, we let $A^\top$ denote its transpose. We also let $\|A\|_2$ and $\|A\|_F$ denote the spectral norm and Frobenius norm of $A$, respectively. For a scalar-valued or matrix-valued function of time, $F(t)$, we let $\dot{F}=\dot{F}(t)=\frac{d}{dt}F(t)$ denote its time derivative. Furthermore, we define $\one_A$ to be the indicator for a statement $A$: $\one_A=1$ if $A$ is true and $\one_A=0$ otherwise. We also let $I$ denote the identity matrix, and $\mathcal{N}(\mu,\sigma^2)$ denote the normal distribution with mean $\mu$ and variance $\sigma^2$
% and $\text{Unif}(S)$ denote the uniform distribution over a set $S$
.

\section{Preliminaries}
% This paper studies the problem of training a two-layer (single-hidden-layer) ReLU network over some training dataset. In particular, we are interested in training our network by gradient flow with small initialization. 
In this section, we first discuss problem setting. We then present some key ingredients for analyzing the training dynamics of ReLU networks under small initialization, and discuss some of the weaknesses/issues from prior work.

\subsection{Problem setting}\label{ssec_setting}
We are interested in a binary classification problem with dataset $[x_1,\cdots,x_n]\in\mathbb{R}^{D\times n}$ (input data) and $[y_1,\cdots,y_n]^\top \in \{-1,+1\}^n$ (labels). For the classifier, $f:\mathbb{R}^D\ra \mathbb{R}$, we consider a two-layer ReLU network:
\be
        f(x;W,v)=v^\top \sigma (W^\top x)=\sum\nolimits_{j=1}^hv_j\sigma(w_j^\top x)\,,
\ee
parametrized by network weights $W:=[w_1,\cdots,w_h]\in\mathbb{R}^{D\times h},v:=[v_1,\cdots,v_h]^\top \in\mathbb{R}^{h\times 1}$,
    where $\sigma(\cdot)=\max\{\cdot,0\}$ is the ReLU activation function. We aim to find the network weights that minimize the training loss $
    \mathcal{L}(W,v)=\sum\nolimits_{i=1}^n \ell(y_i,f(x_i;W,v))$, where $\ell: \mathbb{R}\times \mathbb{R}\ra \mathbb{R}_{\geq 0}$ is either the exponential loss $\ell(y,\hat{y})=\exp(-y\hat{y})$ or the logistic loss $\ell(y,\hat{y})=\log(1+\exp(-y\hat{y}))$. The network is trained via the gradient flow (GF) dynamics
    % \begin{align}
    %     \frac{d}{dt}w_j=-\frac{1}{n}\sum_{i=1}^n\one_{\lan x_i,w_j\ran > 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.\\
    %     \frac{d}{dt}v_j=-\frac{1}{n}\sum_{i=1}^n\one_{\lan x_i,w_j\ran > 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.
    % \end{align}
    \be\label{eq_gf}
        \dot{W}\in \partial_W\mathcal{L}(W,v),\ \dot{v}\in \partial_v\mathcal{L}(W,v),
    \ee
    where $\partial_W\mathcal{L},\partial_v\mathcal{L}$ are Clark sub-differentials of $\mathcal{L}$. Therefore, \eqref{eq_gf} is a differential inclusion~\citep{bolte2010characterizations}. For simplicity of presentation, instead of directly working on this differential inclusion, our theoretical results will be stated for the Caratheodory solution~\citep{reid1971ode} of \eqref{eq_gf} when the ReLU subgradient is fixed as $\sigma'(x)=\one_{x>0}$\footnote{In Appendix \ref{app_diff}, we discuss how our results can be extended to the solution to differential inclusion.}. In Appendix \ref{app_cara}, we show that under the data assumption of our interest (to be introduced later), the Caratheodory solution (s) $\{W(t),v(t)\}$ exists globally for all $t\in [0,\infty)$, which we call the solution (s) of \eqref{eq_gf} throughout this paper.

    To initialize the weights, we consider the following initialization scheme. First, we start from a weight matrix $W_0\in\mathbb{R}^{D\times h}$
    % with bounded norm $\|W_0\|\leq W_{\max}$}\footnote{\color{red}the norm bound $W_{\max}$ can be arbitrarily large, we will omit the dependency on $W_{\max}$ when presenting our main results (will be shown in Appendix).}
    , and then and then initialize the weights as
    \be
        W(0)=\epsilon W_0,\quad\ v_j(0)\in\{\|w_j(0)\|,-\|w_j(0)\|\}, \forall j\in[h]\,.\label{eq_init}
    \ee
    That is, the weight matrix $W_0$ determines the initial shape of the first-layer weights $W(0)$ and we use $\epsilon$ to control the initialization scale and we are interested in the regime where $\epsilon$ is sufficiently small. For the second layer weights $v(0)$, 
    each $v_j(0)$ has magnitude $\|w_j(0)\|$ and we only need to decide its sign. Our results in later sections are stated for a deterministic choice of $\epsilon, W_0$, and $v(0)$, then we comment on the case where $W_0$ is chosen randomly via some distribution.
    
    The resulting weights in \eqref{eq_init} are always "balanced", i.e., $v_j^2(0)-\|w_j(0)\|^2=0,\forall j\in[h]$, because $v_j(0)$ can only take two values: either $\|w_j(0)\|$ or $-\|w_j(0)\|$. More importantly, under GF~\eqref{eq_gf}, this balancedness is preserved~\citep{Du&Lee}: $v_j^2(t)-\|w_j(t)\|^2=0,\forall t\geq 0, \forall j\in[h]$. In addition, it is shown in~\citet{boursier2022gradient} that $\sign(v_j(t))=\sign(v_j(0)), \forall t\geq 0,\forall j\in[h]$, and the dynamical behaviors of neurons will be divided into two types, depending on $\sign(v_j(0))$.
    
    
    % To initialize the weights, we consider the following initialization scheme. First, we randomly sample a matrix  $W_0\in\mathbb{R}^{D\times h}$ such that $[W_0]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,1/D\rp$, and then initialize the weights as
    % \be
    %     W(0)=\epsilon W_0,\quad\ v_j(0)=\text{Unif}(\{-1,+1\})\|w_j(0)\|, \forall j\in[h]\,.\label{eq_init}
    % \ee
    % Through this initialization, we can control the initialization scale via $\epsilon$ and we are interested in the regime where $\epsilon$ is sufficiently small. Moreover, the resulting weights are always "balanced", i.e., $v_j^2(0)-\|w_j(0)\|^2=0,\forall j\in[h]$, because $v_j(0)$ can only take two values: either $\|w_j(0)\|$ or $-\|w_j(0)\|$. More importantly, under GF~\eqref{eq_gf}, this balancedness is preserved~\cite{Du&Lee}: $v_j^2(t)-\|w_j(t)\|^2=0,\forall t\geq 0, \forall j\in[h]$. In addition, it is shown in~\cite{boursier2022gradient} that $\sign(v_j(t))=\sign(v_j(0)), \forall t\geq 0,\forall j\in[h]$, and the dynamical behaviors of neurons will be divided into two types, depending on $\sign(v_j(0))$.
    
    \begin{remark}
        For our theoretical results, the balancedness condition is assumed for technical purposes: it simplifies the dynamics of GF and thus the analysis. It is a common assumption for many existing works on both linear~\citep{arora2018optimization} and nonlinear~\citep{phuong2021inductive,boursier2022gradient} neural networks. For the experiments in Section \ref{sec_num}, we use a standard Gaussian initialization (not balanced) with a small variance to validate our theoretical findings.
    \end{remark}
    \begin{remark}
        Without loss of generality, we consider the case where all columns of $W_0$ are nonzero, i.e., $\|w_j(0)\|>0,\forall j\in[h]$. We make this assumption because whenever $w_j(0)=0$, we also have $v_j(0)=0$ from the balancedness, which together would imply $\dot{v}_j\equiv 0,\dot{w}_j\equiv 0$ under gradient flow. As a result, $w_j$ and $v_j$ would remain zero and thus they could be ignored in the convergence analysis.
    \end{remark}
    \begin{remark}
    Our main results will depend on both $\max_j\|w_j(0)\|$ and $\min_j\|w_j(0)\|$, as shown in our proofs in Appendices \ref{app_pf_thm_align} and \ref{app_pf_thm_conv}. Therefore, whenever we speak of small initialization, we will say that $\epsilon$ is small without worrying about the scale of $W_0$, which is already considered in our results.
    \end{remark}

    \subsection{Neural alignment with small initialization: an overview}\label{ssec_conv_pre}

    Prior work argues that the gradient flow dynamics~\eqref{eq_gf} under small initialization~\eqref{eq_init}, i.e., when $\epsilon$ is sufficiently small, can be roughly described as "align then fit"~\citep{maennel2018gradient,boursier2022gradient}
    % (We also refer readers to Appendix \ref{app_pf_lem_small_norm} for a detailed explanation with dynamical equations for $\|w_j\|$ and $\frac{w_j}{\|w_j\|}$)
    : During the early phase of training, every neuron $w_j,j\in[h]$ keeps a small norm $\|w_j\|^2\ll 1$ while changing their directions $\frac{w_j}{\|w_j\|}$ significantly in order to locally maximize a "signed coverage"~\citep{maennel2018gradient} of itself w.r.t. the training data.
    % This so-called alignment phase lasts $\Theta(\log \frac{1}{\epsilon})$.
    After the alignment phase, part of (potentially all) the neurons grow their norms in order to fit the training data, and the loss decreases significantly. The analysis for the fitting phase generally depends on the resulting neuron directions at the end of the alignment phase~\citep{phuong2021inductive,boursier2022gradient}. %Therefore, developing a rigorous analysis of the alignment phase is key to fully understand GF with small initialization. 
    However, prior analysis of the alignment phase either is based on a vanishing initialization argument that can not be directly translated into the case finite but small initialization~\citep{maennel2018gradient} or assumes some stringent assumption on the data~\citep{boursier2022gradient}. 
    In this section, we provide a brief overview of the existing analysis for neural alignment and then point out several weaknesses in prior work.
    
    \begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-3mm}
    \centering
    % Figure removed
      \caption{Illustration of $\frac{d}{dt}\frac{w_j}{\|w_j\|}$ during the early alignment phase. $x_1$ has $+1$ label, and $x_2,x_3$ have $-1$ labels, $x_1,x_2$ lie inside the halfspace $\lan x,w_j\ran>0$ (gray shaded), thus $x_a(w_j)=x_1-x_2$. Since $\sign(v_j(0))>0$, GF pushes $w_j$ towards $x_a(w_j)$.}
      \label{fig_dir_deriv}
        \vspace{-0.8cm}
    \end{wrapfigure}
    \paragraph{Prior analysis of the alignment phase:}
    Since during the alignment phase all neurons have small norm, prior work mainly focuses on the directional dynamics, i.e.,  $\frac{d}{dt}\frac{w_j}{\|w_j\|}$, of the neurons. The analysis relies on the following approximation of the dynamics of every neuron $w_j,j\in[h]$:
    \be
        \frac{d}{dt} \frac{w_j}{\|w_j\|}\simeq \sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j)\,,\label{eq_dir_flow}
    \ee
    where $\mathcal{P}_{w}=I-\frac{ww^\top }{\|w\|^2}$ is the projection onto the subspace orthogonal to $w$ and 
    \be
    x_a(w):= \sum\nolimits_{i: \lan x_i,w\ran> 0}y_ix_i
    \ee
    denotes the signed combination of the data points activated by $w$. 
% {\color{red}As long as we pick $\epsilon$ small enough such that the approximation in \eqref{eq_dir_flow} is valid\footnote{This also requires $\mathcal{P}_{w_j(t)} x_a(w_j(t))$ to be non-vanishing at any $t\in[0,T]$, and showing this depends on some specific assumption on the training data.} for all $0\leq t\leq T$, it is easier to analyze the neural alignment with \eqref{eq_dir_flow}}:
First of all, \eqref{eq_dir_flow} implies that the dynamics $\frac{w_j}{\|w_j\|}$ are approximately decoupled, and thus one can study each $\frac{w_j}{\|w_j\|}$ separately. Moreover, as illustrated in Figure \ref{fig_dir_deriv}, if $\sign(v_j(0))>0$, the flow \eqref{eq_dir_flow} pushes $w_j$ towards $x_a(w_j)$, since $w_j$ is attracted by its currently activated positive data and repelled by its currently activated negative data. Intuitively, during the alignment phase, a neuron $w_j$ with $\sign(v_j(0))>0$ would try to find a direction where it can activate as much positive data and as less negative data as possible. If $\sign(v_j(0))<0$, the opposite holds.

Indeed,~\citet{maennel2018gradient} claims that the neuron $w_j$ would be aligned with some "extreme vectors", defined as vector $w\in\mathbb{S}^{D-1}$ that locally maximizes $\sum_{i\in[n]} y_i\sigma(\lan x_i,w\ran)$ (similarly, $w_j$ with $\sign(v_j(0))<0$ would be aligned with the local minimizer), and there are only finitely many such vectors.
% thus the neurons are expected to converge to one of these extreme vectors in direction. 
The analysis is done under the limit $\epsilon\ra 0$, where the approximation in~\eqref{eq_dir_flow} is exact. 

\paragraph{Weakness in prior analyses:} Although~\citet{maennel2018gradient} provides great insights into the dynamical behavior of the neurons in the alignment phase, the validity of the aforementioned approximation for finite but small $\epsilon$ remains in question. First, one needs to make sure that the error $\lV \frac{d}{dt} \frac{w_j}{\|w_j\|}- \sign(v_j(0)) \mathcal{P}_{w_j}x_a(w_j)\rV$ is sufficiently small when $\epsilon$ is finite in order to justify \eqref{eq_dir_flow} as a good approximation. Second, the error bound needs to hold for the entire alignment phase.~\citet{maennel2018gradient} assumes $\epsilon\ra 0$; hence there is no formal error bound.
In addition, prior analyses on small initialization~\citep{stoger2021small,boursier2022gradient} suggest the alignment phase only holds for $\Theta(\log\frac{1}{\epsilon})$ time. Thus, the claim in~\citet{maennel2018gradient} would only hold if good alignment is achieved before the alignment phase ends. However, \citet{maennel2018gradient} provides no upper bound on the time it takes to achieve good alignment. Therefore, without a finite $\epsilon$ analysis, ~\citet{maennel2018gradient} fails to fully explain the training dynamics under small initialization. Understanding the alignment phase with finite $\epsilon$ requires additional quantitative analysis. To the best of our knowledge, this has only been studied under a stringent assumption that all data points are orthogonal to each other~\citep{boursier2022gradient}, or that there are effectively two data points~\cite{wang2023understanding}.
    
\paragraph{Goal of this paper:}
In this paper, we want to address some of the aforementioned issues by developing a formal analysis for the early alignment phase with a finite but small initialization scale $\epsilon$. We first discuss our main theorem that shows that a directional convergence can be achieved within bounded time under data assumptions that are less restrictive and have more practical relevance. Then, we discuss the error bound for justifying \eqref{eq_dir_flow} in the proof sketch of the main theorem.

%     Our analysis of the neural alignment is rooted on the following Lemma:
%     \begin{lemma}\label{lem_small_norm_informal}
%     Given some initialization from \eqref{eq_init}, if $\epsilon=\mathcal{O}\lp \frac{1}{\sqrt{h}}\rp$, then exists $T=\Theta\lp\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon}\rp$ such that any solution to the gradient flow dynamics \eqref{eq_gf} satisfies that $\forall t\leq T$,
%     % $\max_j\|w_j(t)\|^2=\mathcal{O}(\frac{\epsilon}{\sqrt{h}})$, $\max_i|f(x_i;W(t),v(t))|=\mathcal{O}(\epsilon \sqrt{h})$ and 
%     \be
%         \max_j\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}-\sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV=\mathcal{O}\lp\epsilon \sqrt{h}\rp\,.
%     \ee
% \end{lemma}
% \enrique{The goal of this section seems to be not clearly defined. From the start up until before the first bold paragraph, it mixes: a. a general  overview of the convergence process at an intuitive level, b. some of our results (Lemma 1), c. some criticism to [17]. 

% Then it has more weaknesses and ends with our goal.

% I would reorder things as follows:

% 1. first, state clearly the qualitative stages of convergence. 

% 2. Stating the missing gaps of in the literature.

% 3. Then stating our goal. and perhaps a overall strategy (maybe including lemma 1 as first step).

% Alternatively, I question whether Lemma 1 should be here. Isn't this one of our contributions?, if so, shouldn't instead be in the next section?
% }
% \hancheng{left to in-person discussion}
    
% \enrique{In the above paragraph, it is difficult to understand the difference between the alignment phase and the directional convergence. This terminology is confusing. I think that a better explanation of the convergence phases at that beginning of this section will help a lot in understanding the difference between the two.}
% \hancheng{left to in-person discussion}

% Indeed, the flow in \eqref{eq_dir_flow} is implicitly trying to locally maximize a signed coverage of $w_j$ on the training data:
% \be
%     \mathcal{H}(w_j)=\sign(v_j(0))\sum
% \ee

\section{Convergence of Two-layer ReLU Networks with Small Initialization}
% As we discussed in Section \ref{ssec_conv_pre}, the convergence analysis for two-layer networks under small initialization heavily depends on some underlying assumption on training data $\{x_i,y_i\}_{i=1}^n$, and ours makes no exception. 

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
  \vspace{-5mm}
% Figure removed
  \vspace{-2mm}
  \caption{Neuron alignment under Assumption \ref{assump_data}. For neurons in $\mathcal{V}_+$, \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {1}}} if it lies inside $\mathcal{S}_-$, then it gets repelled by $x_-$ and escapes $\mathcal{S}_-$; Once outside $\mathcal{S}_-$, it may \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {2}}} get repelled by some negative data and eventually enters $\mathcal{S}_\text{dead}$, or may \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {3}}} gain some activation on positive data and eventually enter $\mathcal{S}_+$, then get constantly attracted by $x_+$.}
  \label{fig_dir_flow}
\end{wrapfigure}
% In this section, we present our main results, which require the following assumption on the training data (we will compare our assumption with those in prior work after the main theorem):
Our main results require the following data assumption:
\begin{assumption}\label{assump_data}
Any pair of data with the same (different) label is positively (negatively) correlated, i.e.,
        $\min_{i,j}\frac{\lan x_iy_i,x_jy_j \ran}{\|x_i\|\|x_j\|}\!:=\!\mu\!>0.$
\end{assumption}
% For now, we want to point out that, under Assumption 1, the training data is linearly separable, i.e., $\exists \beta, s.t. (\beta^\top x_i)y_i>0, \forall i$, thus even a linear classifier would achieve zero training loss. Our goal, however, is to understand how gradient flow on a two-layer ReLU network would lead to a different classifier that better captures the intrinsic property of the training data. 
Given a training dataset, we define $\mathcal{S}_+:=\{z\in \mathbb{R}^D:\one_{\lan x_i,z\ran> 0}=\one_{y_i>0},\forall i\}$ to be the cone in $\mathbb{R}^n$ such that whenever neuron $w\in\mathcal{S}_+$, $w$ is activated exclusively by every $x_i$ with a positive label (see Figure \ref{fig_dir_flow}). Similarly, for $x_i$ with negative labels, we define $\mathcal{S}_-:=\{z\in \mathbb{R}^D:\one_{\lan x_i,z\ran> 0}=\one_{y_i<0},\forall i\}$. Finally, we define $\mathcal{S}_{\text{dead}}:=\{z\in\mathbb{R}^D:\lan z,x_i\ran\leq 0,\forall i\}$ to be the cone such that whenever $w\in \mathcal{S}_{\text{dead}}$, no data activates $w$. Given Assumption \ref{assump_data}, it can be shown (see Appendix \ref{app_pf_thm_align}) that $\mathcal{S}_+$ ($\mathcal{S}_-$) is a non-empty, convex cone that contains all positive data $x_i,i\in\mathcal{I}_+$ (negative data $x_i,i\in\mathcal{I}_-$). $\mathcal{S}_\text{dead}$ is a convex cone as well, but not necessarily non-empty. We illustrate these cones in Figure \ref{fig_dir_flow} given some training data (red solid arrow denotes positive data and blue denotes negative ones). 

% \enrique{It takes some time to get used to $\mathcal{S}_+, \mathcal{S}_- and \mathcal{S}_{dead}$. I'm wondering if it would be better to first define $\mathcal{S}_+$, together with its properties, pointing to Figure 2, then do the same with $\mathcal{S}_-$ and $\mathcal{S}_{dead}$, respectively. Also, when defining the set of indices, I would say the meaning with words like $\mathcal{I}_+$ is the set of indices of positive data, etc. }
% \enrique{$J_+$ appears in figure 1 in the previous section. Also, I think it would be easier to remember the set $\mathcal{V}_+$ if it was called $\mathcal{V}_+$.}

Moreover, given some initialization from \eqref{eq_init}, we define $\mathcal{I}_+:=\{i\in[n]:y_i>0\}$ to be the set of indices of positive data, and $\mathcal{I}_-:=\{i\in[n]:y_i<0\}$ for negative data. We also define $\mathcal{V}_+:=\{j\in[h]:\sign(v_j(t))>0\}$ to be the set of indices of neurons with positive second-layer entry and $\mathcal{V}_-:=\{j\in[h]:\sign(v_j(t))<0\}$ for neurons with negative second-layer entry. Note that, as discussed in Section \ref{ssec_setting}, $\sign(v_j(t))$ does not change under balanced initialization, thus $\mathcal{V}_+, \mathcal{V}_-$ are time invariant.
Further, as we discussed in Section \ref{ssec_conv_pre} about the early alignment phase, we expect that every neuron in $\mathcal{V}_+$ will drift toward the region where positive data concentrate and thus eventually reach $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$, as visualized in Figure \ref{fig_dir_flow} ($x_+,x_-$ shown in the figure are defined in Assumption \ref{assump_non_degen}).   Similarly, all neurons in $\mathcal{V}_-$ would chase after negative data and thus  reach $\mathcal{S}_-$ or $\mathcal{S}_\text{dead}$. Our theorem precisely characterizes this behavior.  

\subsection{Main results}\label{ssec_main}
Our main results are stated for solutions to the GF dynamics \eqref{eq_gf}. However, in rare cases, solutions to \eqref{eq_gf} could be non-unique and there are potentially ``irregular solutions" (please refer to Appendix \ref{app_sec_non_unique_cara} for details) that allow some neurons to regain activation even after becoming completely deactivated in $\mathcal{S}_{\mathrm{dead}}$. We deem such irregular solutions of little practical relevance since when implementing gradient descent algorithm in practice, neurons in $\mathcal{S}_{\mathrm{dead}}$ 
would receive zero update and thus stay in $\mathcal{S}_{\mathrm{dead}}$. Therefore, our main theorem concerns some \emph{regular} solutions to \eqref{eq_gf} (the existence of such solutions is shown in Appendix \ref{app_sec_exist_cara}), as defined below.

% In this work, we study the properties of solutions of the GF dynamics \eqref{eq_gf} that satisfy a certain notion of regularity, as defined below.
\begin{definition}\label{def_reg}
    A solution $\{W(t),v(t)\}$ to \eqref{eq_gf} is \emph{regular} if it satisfy that $w_j(t_0)\in\mathcal{S}_{\mathrm{dead}}$ for some $j\in[h]$ and some $t_0\geq 0$ implies $w_j(t)\in\mathcal{S}_{\mathrm{dead}},\forall t\geq t_0$.
\end{definition}
% In rare cases, solutions to \eqref{eq_gf} could be non-unique and there are potentially ``irregular solutions" (please refer to Appendix \ref{app_sec_non_unique_cara} for details) that allows some neuron to regain activation even after becoming completely deactivated in $\mathcal{S}_{\mathrm{dead}}$. We deem such irregular solutions of little practical relevance since when implementing gradient descent algorithm in practice, neurons in $\mathcal{S}_{\mathrm{dead}}$ would receive zero update and thus stay in $\mathcal{S}_{\mathrm{dead}}$. Therefore, our main theorem concerns the regular solutions to \eqref{eq_gf} (the existence of such solutions is shown in Appendix \ref{app_sec_exist_cara}).

Before we present our main theorem, we also need the following assumption on the initialization, for technical reasons, essentially asking the neuron $w_j(0), j\in\mathcal{V}_+$ (or $w_j(0), j\in\mathcal{V}_-$, resp.) to not be completely aligned with $x_+$ (or $x_-$, resp.).
\begin{assumption}\label{assump_non_degen}
    The initialization from \eqref{eq_init} satisfies that $\max_{j\in \mathcal{V}_+}\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_-}{\|x_-\|}\rangle<1$, and
    
    $\max_{j\in \mathcal{V}_-}\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_+}{\|x_+\|}\rangle<1$, where $x_+=\sum_{i\in\mathcal{I}_+}x_i$ and $x_-=\sum_{i\in\mathcal{I}_-}x_i$. 
\end{assumption}

% {\color{red} In addition, our main theorem is restricted to solutions of \eqref{eq_gf} that satisfy the following
% \begin{assumption}\label{assump_cara}
%     The solution of \eqref{eq_gf} satisfies that if $w_j(t_0)\in\mathcal{S}_{\mathrm{dead}}$ for some $j\in[h]$ and some $t_0\geq 0$, then $w_j(t)\in\mathcal{S}_{\mathrm{dead}},\forall t\geq t_0$.
% \end{assumption}
% While Assumption \ref{assump_cara} seems obvious, we show that, in Appendix \ref{app_sec_non_unique_cara}, there exist solutions of \eqref{eq_gf} that violate Assumption \ref{assump_cara}, for which keeping tracking of neuron positions become impossible: Some neuron in $\mathcal{S}_{\mathrm{dead}}$ could leave $\mathcal{S}_{\mathrm{dead}}$ at any time.}
% We will explain how Assumption \ref{assump_non_degen} plays a role in the analysis later.

% Since we randomly sample the entries of $W_0$ with i.i.d. Gaussians, the initialization from \eqref{eq_init} will satisfy Assumption \eqref{assump_non_degen} with probability one.

% \enrique{It sounds to me that to get the rates, you will have to require the inner products to be $\leq 1-\gamma$ for some $\gamma>0$. My concern is for the case where $\gamma\ll \epsilon$. Then, I don't see how you escape the saddle/unstable equilibrium.}
% \hancheng{The gap will show up in the upper bound for $\epsilon$}
% \enrique{But my point is that Assumption 2 is not correct. Because Assumption 2 allows for a gap of, e.g., $e^{-1/\epsilon}$. Do you still get $t_1=\mathcal{O}(\log(1/\epsilon))$ in that case?}
% \hancheng{The flow is the following, "give me the initialization with a gap $\gamma$" that could be close to zero, then my results hold for any $\epsilon=\mathcal{O}(\exp\lp-\frac{1}{\gamma}\rp)$}
% \enrique{Is $\gamma$ the gap in assumption 2? HM: yes. Ok I see.}
% \hancheng{I will have a remark to discuss how sufficiently small $\epsilon$ should be and what affects it. EM: Ok, sounds good.}
We are now ready to present our main result (given Assumption \ref{assump_data} and Assumption \ref{assump_non_degen}):
\begin{theorem}\label{thm_conv_main}
    Given some initialization from \eqref{eq_init}, if $\epsilon=\mathcal{O}( \frac{1}{\sqrt{h}}\exp( -\frac{n}{\sqrt{\mu}}\log n))$, then for any regular solution to the gradient flow dynamics \eqref{eq_gf}, we have
    \begin{enumerate}[leftmargin=0.5cm]
        \item (Directional convergence in early alignment phase) $\exists t_1=\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$, such that 
        \begin{itemize}[leftmargin=0.2cm]
            \item $\forall j\in\mathcal{V}_+$, either $w_j(t_1)\in \mathcal{S}_+$ or $w_j(t_1)\in \mathcal{S}_{\text{dead}}$. Moreover, if $\max_{i\in\mathcal{I}_+}\lan w_j(0),x_i\ran>0$, then $w_j(t_1)\in \mathcal{S}_+$.
            \item $\forall j\in\mathcal{V}_-$, either $w_j(t_1)\in \mathcal{S}_-$ or $w_j(t_1)\in \mathcal{S}_{\text{dead}}$. Moreover, if $\max_{i\in\mathcal{I}_-}\lan w_j(0),x_i\ran>0$, then $w_j(t_1)\in \mathcal{S}_-$.
        \end{itemize}
        \item (Final convergence and low-rank bias) $\forall t\geq t_1$ and $\forall j\in[h]$, neuron $w_j(t)$ stays within $\mathcal{S}_+$ ($\mathcal{S}_-$, or $\mathcal{S}_{\text{dead}}$) if $w_j(t_1)\in\mathcal{S}_+$ ($\mathcal{S}_-$, or $\mathcal{S}_{\text{dead}}$ resp.). Moreover, if both $\mathcal{S}_+$ and $\mathcal{S}_-$ contains at least one neuron at time $t_1$, then
        \begin{itemize}[leftmargin=0.2cm]
            % \item $\exists\alpha>0$ and $\exists t_2$ with $t_1\leq t_2=\Theta\lp\log\frac{1}{\epsilon}\rp$ such that $\mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1},\ \forall t\geq t_2$.
            \item $\exists\alpha>0$ and $\exists t_2$ with $t_1\leq t_2=\Theta (\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$, such that $\mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1},\ \forall t\geq t_2$.
            \item As $t\ra \infty$, $\|W(t)\|\ra \infty$ and $
                \|W(t)\|_F^2\leq 2\|W(t)\|^2_2+\mathcal{O}(\epsilon)$. Thus, the stable rank of $W(t)$ satisfies $\lim\sup_{t\ra \infty}\|W(t)\|_F^2/\|W(t)\|^2_2\leq 2$.
        \end{itemize}
    \end{enumerate}
\end{theorem}
We provide a proof sketch that highlights the technical novelty of our results in Section \ref{ssec_pf_sketch}. Our $\mathcal{O}(\cdot)$ notations hide additional constants that depend on the data and initialization, for which we refer readers to the complete proof of Theorem \ref{thm_conv_main} in Appendix \ref{app_pf_thm_align} and \ref{app_pf_thm_conv}. We make the following remarks:

\paragraph{Early neuron alignment:} The first part of the Theorem \ref{thm_conv_main} describes the configuration of \emph{all} neurons at the end of the alignment phase. Every neuron in $\mathcal{V}_+$ reaches either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$ by $t_1$, and stays there for the remainder of training. Obviously, we care about those neurons reaching $\mathcal{S}_+$ as any neuron in $\mathcal{S}_\text{dead}$ does not contribute to the final convergence at all. Luckily, Theorem \ref{thm_conv_main} suggests that any neuron in $\mathcal{V}_+$ that starts with some activation on the positive data, i.e., it is initialized in the union of halfspaces $\cup_{i\in\mathcal{I}_+}\{w: \lan w,x_i\ran>0\}$, will eventually reach $\mathcal{S}_+$. 
% (which happens with at least probability $1/2$, under random initialization as in \eqref{eq_init})
A similar discussion holds for neurons in $\mathcal{V}_-$. We argue that randomly initializing $W_0$ ensures that with high probability, there will be at least a pair of neurons reaching $\mathcal{S}_+$ and $\mathcal{S}_-$ by time $t_1$ (please see the next remark).  Lastly, we note that it is possible that $\mathcal{S}_\text{dead}=\emptyset$, in which case every neuron reaches either $\mathcal{S}_+$ or $\mathcal{S}_-$. 
% \enrique{Can you state the conditions for $\mathcal{S}_{dead}$ being empty?}

\paragraph{Merits of random initialization:} Our theorem is stated for a deterministic initialization \eqref{eq_init} given an initial shape $W_0$. In practice, one would use random initialization to find a $W_0$, for example, $[W_0]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,1/D\rp$. First, our Theorem \ref{thm_conv_main} applies to this Gaussian initialization: Assumption \ref{assump_non_degen} is satisfied with probability one because the events $\lb\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_-}{\|x_-\|}\rangle=1\rb$ and $\lb\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_+}{\|x_+\|}\rangle=1\rb$ have probability zero. Moreover, any neuron in $\mathcal{V}_+$ has at least probability $1/2$ of being initialized within the union of halfspaces $\cup_{i\in\mathcal{I}_+}\{w: \lan w,x_i\ran>0\}$, which ensures that this neuron reaches $\mathcal{S}_+$. Thus when there are $m$ neurons in $\mathcal{V}_+$, the probability that $\mathcal{S}_+$ has at least one neuron at time $t_1$ is lower bounded by $1-2^{-m}$ (same argument holds for $\mathcal{S}_-$), Therefore, with only very mild overparametrization on the network width $h$, one can make sure that with high probability there is at least one neuron in both $\mathcal{S}_+$ and $\mathcal{S}_-$, leading to final convergence.

\paragraph{Importance of a quantitative bound on $t_1$:}
The analysis for neural alignment relies on the approximation in \eqref{eq_dir_flow}, which, through our analysis (see Lemma \ref{lem_small_norm_informal}), is shown to only hold before $T=\Theta(\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$, thus if one proves, through the approximation in \eqref{eq_dir_flow}, that good alignment is achieved within $t_1$ time, then the initialization scale $\epsilon$ must be chosen to be $\mathcal{O}( \frac{1}{\sqrt{h}}\exp( -n t_1))$ so that $t_1\leq T$, i.e. the proved alignment should finish before the approximation \eqref{eq_dir_flow} fails. Therefore, without an explicit bound on $t_1$, one does not know a prior how small $\epsilon$ should be. Our quantitative analysis shows that under \eqref{eq_dir_flow}, directional convergence
% , i.e., all neurons reaching either $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$, 
is achieved within $t_1=\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ time. This bound, in return, determines the bound for initialization scale $\epsilon$. Moreover, our bound quantitatively reveals the non-trivial dependency on the "data separation" $\mu$ for such directional convergence to occur. Indeed, through a numerical illustration in Appendix \ref{app_ssec_data_sep}, we show that the dependence on the data separability $\mu>0$ is crucial in determining the scale of the initialization: As $\mu$ approaches zero, the time needed for the desired alignment increases, necessitate the use of a smaller $\epsilon$.  

% To the best of our knowledge, this is the first non-asymptotic bound on the time it takes for all neurons to achieve a desired configuration.~\cite{maennel2018gradient} only shows such $t_1>0$ exists using an $\epsilon\ra 0$ argument, without analyzing how large $t_1$ can be.~\cite{boursier2022gradient} studies a different data assumption (we compare it with ours in later remarks) under which the alignment is studied only for neurons that has a specific activation pattern at initialization. Lastly, we note that $\mu \ra 0$ leads to $t_1\ra \infty$, this is because when $\mu=0$, there are more limiting directions to which neurons can converge, hence not all of them are "attracted" by $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$.

\paragraph{Refined alignment within $\mathcal{S}_+,\mathcal{S}_-$:} Once a neuron in $\mathcal{V}_+$ reaches $\mathcal{S}_+$, it never leaves $\mathcal{S}_+$. Moreover, it always gets attracted by $x_+$. Therefore, every neuron gets well aligned with $x_+$, i.e., $\cos(w_j,x_+)\simeq 1,\forall w_j\in\mathcal{S}_+$. A similar argument shows neurons in $\mathcal{S}_-$ get attracted by $x_-$. We opt not to formally state it in Theorem \ref{thm_conv_main} as the result would be similar to that in~\citep{boursier2022gradient}, and alignment with $x_+,x_-$ is not necessary to guarantee convergence. Instead, we show this refined alignment through our numerical experiment in Section \ref{sec_num}.
% \enrique{I'm confused as to whether this stage is at all needed. Once you get $\mathcal{S}_+$ can't we just apply your other work? This stage does not seem necessary to guarantee convergence. }
% \hancheng{this section is stated for explaining the numerical experiments because the experiments show the alignment with $x_+,x_-$.}

\paragraph{Final convergence and low-rank bias:} We present the final convergence results mostly for the completeness of the analysis. GF after $t_1$ can be viewed as fitting positive data $x_i,i\in\mathcal{I}_+$, with a subnetwork consisting of neurons in $\mathcal{S}_+$, and fitting negative data with neurons in $\mathcal{S}_-$. By the fact that all neurons in $\mathcal{S}_+$ activate all $x_i,i\in\mathcal{I}_+$, the resulting subnetwork is linear, and so is the subnetwork for fitting $x_i,i\in\mathcal{I}_-$. The convergence analysis reduces to establishing $\mathcal{O}(1/t)$ convergence for two linear networks~\citep{arora2018convergence,mtvm21icml,yun2020unifying}. The non-trivial and novel part is to show that right after the alignment phase ends, one can expect a substantial decrease of the loss (starting from time $t_2=\Theta (\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$).
An alternative way of proving convergence is by observing that at $t_1$, all data has been correctly classified (w.r.t. sign of $f$), which is sufficient for showing $\mathcal{O}(\frac{1}{t\log t})$ convergence~\citep{lyu2019gradient, ji2020dir} of the loss, but this asymptotic rate does not suggest a time after which the loss start to decrease significantly.
As for the stable rank, our result follows the analysis in~\citet{le2022training}, but in a simpler form since ours is for linear networks. Although convergence is established partially by existing results, we note that these analyses are all possible because we have quantitatively bound $t_1$ in the alignment phase.  


\subsection{Comparison with prior work}
Our results provide a complete (from alignment to convergence), non-asymptotic (finite $\epsilon$), quantitative (bounds on $t_1,t_2$) analysis for the GF in \eqref{eq_gf} under small initialization. Similar neural alignment has been studied in prior work for \emph{orthogonally separable} data (same as ours) and for \emph{orthogonal} data, and we shall discuss them separately.

\paragraph{Alignment under orthogonally separable data:} ~\citet{phuong2021inductive} assumes that there exists a time $t_1$ such that at $t_1$, the neurons are in either $\mathcal{S}_+,\mathcal{S}_-$ or $\mathcal{S}_\text{dead}$ and their main contribution is the analysis of the implicit bias for the later stage of the training. they justify their assumption by the analysis in~\citet{maennel2018gradient}, which does not necessarily apply to the case of finite $\epsilon$, as we discussed in Section \ref{ssec_conv_pre}. Later ~\citet{wang2022the} shows $t_1$ exists, provided that the initialization scale $\epsilon$ is sufficiently small, but still with no explicit analysis showing how $t_1$ depends on the data separability $\mu$ and the size of the training data $n$. Moreover, there is no quantification on how small $\epsilon$ should be. In our work, all the results are non-asymptotic and quantitative: we show that good alignment is achieved within $t_1=\mathcal{O}( \frac{\log n}{\sqrt{\mu}})$ time and provide an explicit upper bound on $\epsilon$. Moreover, our results highlight the dependence on the separability $\mu>0$, (Further illustrated in Appendix \ref{app_ssec_data_sep}) which is not studied in~\citet{phuong2021inductive,wang2022the}.

\paragraph{Alignment under orthogonal data:} In~\citet{boursier2022gradient}, the neuron alignment is carefully analyzed for the case all data points are orthogonal to each other, i.e., $\lan x_i,x_j\ran=0, \forall i\neq j\in[n]$. We point out that neuron behavior is different under orthogonal data (illustrated in Appendix \ref{app_ssec_orth}): only the positive (negative) neurons initially activate all the positive (negative) data will end up in $\mathcal{S}_+$ ($\mathcal{S}_-$). In our case, all positive (negative) neurons will arrive at $\mathcal{S}_+$ ($\mathcal{S}_-$), unless they become a dead neuron. Moreover, due to such distinction, the analysis is different: ~\citet{boursier2022gradient} restrict their results to positive (negative) neurons $w_j$ that initially activate all the positive (negative) data, and there is no need for analyzing neuron activation. However, since our analysis is on all positive neurons, regardless of their initial activation pattern, it utilizes novel techniques to track the evolution of the activation pattern (see Section \ref{ssec_pf_sketch}). 

\paragraph{Other related work:} Convergence of two-layer (leaky-)ReLU networks are also studied under non-small initialization settings, mainly for gradient descent~\cite{wang2022early} and for training only the first-layer weights~\cite{frei2022implicit,kou2023implicit}. There is no direct comparison to them as they study the convergence in other regimes. Nonetheless, the analyses of neural alignment remain essential in these works but are done through different tools (one no longer has an approximation in~\eqref{eq_dir_flow}). We note that such analyses also require certain restrictive data assumptions. For example,~\cite{wang2022early} assumes orthogonal separability, together with some geometric constraint on the data;~\cite{frei2022implicit,kou2023implicit} assumes high-dimensional near-orthogonal data.  

% \textbf{Comparison with~\cite{phuong2021inductive}}: Prior work~\cite{phuong2021inductive} considers a similar data assumption to ours. However,~\cite{phuong2021inductive} assumes that there exists a time $t_1$ such that at $t_1$, the neurons are in either $\mathcal{S}_+,\mathcal{S}_-$ or $\mathcal{S}_\text{dead}$ and their main contribution is the analysis of the implicit bias for the later stage of the training.~\cite{phuong2021inductive} justifies their assumption by the analysis in~\cite{maennel2018gradient}, which does not necessarily apply to the case of finite $\epsilon$, as we discussed in Section \ref{ssec_conv_pre}.
% % However,~\cite{phuong2021inductive} assumes \hl{directional convergence} in~\cite{maennel2018gradient} for their analysis, which does not necessarily apply to the case of finite $\epsilon$, as we discussed in Section \ref{ssec_conv_pre}. 
% Our work precisely establishes such directional convergence for finite but small $\epsilon$, showing indeed the neurons achieve some good alignment with $x_+,x_-$ within $\mathcal{O}( \frac{\log n}{\sqrt{\mu}})$ time before they start to grow in norm. Moreover,~\cite{phuong2021inductive} has no characterization on the convergence rate of the loss after the alignment phase, while we provide a $\mathcal{O}(1/t)$ bound on the loss. In addition,~\cite{phuong2021inductive} considers the case where input data $x_i,i\in[n]$, spans the entire $\mathbb{R}^{D}$, which leads to $\mathcal{S}_\text{dead}=\emptyset$. This implicitly imposes the constraint that the number of data points $n$ must be larger than the input dimension $D$. Our analysis allows for the case $\mathcal{S}_\text{dead}\neq\emptyset$ as we provide a sufficient condition for preventing a neuron from reaching $\mathcal{S}_\text{dead}$.
% % \enrique{I think that \cite{phuong2021inductive} chooses a slightly weaker condition. It seems to me there's a reason why you need to have a slightly stricter condition, and that the argument. It is necessary to have a stricter condition to ensure a finite $\frac{1}{\varepsilon}$ time. }
% % \hancheng{left to in-person discussion}

% \textbf{Comparison with~\cite{boursier2022gradient}}: In~\cite{boursier2022gradient}, the neuron alignment is carefully analyzed for the case all data points are orthogonal to each other, i.e., $\lan x_i,x_j\ran=0, \forall i\neq j\in[n]$. Such an assumption restricts the number of data points $n$ to be smaller than the input dimension $D$ and is often unrealistic. Our assumption does not restrict the size of the dataset and thus has more practical relevance (see our numerical experiments in Section \ref{sec_num}). 


\subsection{Proof sketch for the alignment phase}\label{ssec_pf_sketch}
In this section, we sketch the proof for our Theorem \ref{thm_conv_main}. First of all, it can be shown that $\mathcal{S}_+,\mathcal{S}_{\text{dead}}$ are trapping regions for all $w_j(t),j\in\mathcal{V}_+$, that is, whenever $w_j(t)$ gets inside $\mathcal{S}_+$ (or $\mathcal{S}_{\text{dead}}$), it never leaves $\mathcal{S}_+$ (or $\mathcal{S}_{\text{dead}}$). Similarly, $\mathcal{S}_-,\mathcal{S}_{\text{dead}}$ are trapping regions for all $w_j(t),j\in\mathcal{V}_-$. The alignment phase analysis concerns how long it takes for all neurons to reach one of the trapping regions, followed by the final convergence analysis on fitting data with $+1$ label by neurons in $\mathcal{S}_+$ and fitting data with $-1$ label by those in $\mathcal{S}_-$. We have discussed the final convergence analysis in the remark "Final convergence and low-rank bias", thus we focus on the proof sketch for the early alignment phase here, which is considered as our main technical contribution.

\paragraph{Approximating $\frac{d}{dt}\frac{w_j}{\|w_j\|}$:} Our analysis for the neural alignment is rooted in the following Lemma:
\begin{lemma}\label{lem_small_norm_informal}
Given some initialization from \eqref{eq_init}, if $\epsilon=\mathcal{O}( \frac{1}{\sqrt{h}})$, then there exists $T=\Theta(\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$ such that any solution to the gradient flow dynamics \eqref{eq_gf} satisfies that $\forall t\leq T$,
% $\max_j\|w_j(t)\|^2=\mathcal{O}(\frac{\epsilon}{\sqrt{h}})$, $\max_i|f(x_i;W(t),v(t))|=\mathcal{O}(\epsilon \sqrt{h})$ and 
\be
    \max_j\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}-\sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV=\mathcal{O}\lp\epsilon n\sqrt{h}\rp\,.\label{eq_err}
\ee

\end{lemma}
This Lemma shows that the error between $\frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}$ and $\sign(v_j(0))\mathcal{P}_{w_j(t)} x_a(w_j(t))$ can be arbitrarily small with some appropriate choice of $\epsilon$ (to be determined later). This allows one to analyze the true directional dynamics $\frac{w_j(t)}{\|w_j(t)\|}$ using some property of $\mathcal{P}_{w_j(t)} x_a(w_j(t))$,
which leads to a $t_1=\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for the neuron direction to converge to the sets $\mathcal{S}_+$, $\mathcal{S}_-$, or $\mathcal{S}_{\text{dead}}$. Moreover, it also suggests $\epsilon$ can be made sufficiently small so that the error bound holds until the directional convergence is achieved, i.e. $T\geq t_1$. We will first illustrate the analysis for directional convergence, then close the proof sketch with the choice of a sufficiently small $\epsilon$.

% This lemma suggests how small $\epsilon$ should be in order to approximate $\frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}$ as in \eqref{eq_dir_flow} so that one can analyze the true directional dynamics $\frac{w_j(t)}{\|w_j(t)\|}$ using some property of $\mathcal{P}_{w_j(t)} x_a(w_j(t))$, which leads to a $t_1=\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for the neuron direction to converge to the sets $\mathcal{S}_+$, $\mathcal{S}_-$, $\mathcal{S}_{\text{dead}}$. Moreover, it also suggests $\epsilon$ can be made sufficiently small so that the error bound holds until the directional convergence is achieved, i.e. $T\geq t_1$. We will first illustrate the analysis for directional convergence, then close the proof sketch with the choice of sufficiently small $\epsilon$.

\paragraph{Activation pattern evolution:}~
Given a sufficiently small $\epsilon$, one can show that under Assumption \ref{assump_data}, for every neuron $w_j$ that is not in $\mathcal{S}_\text{dead}$ we have:
\be
\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}>0,\forall i\in [n], \text{if } j\in \mathcal{V}_+\,,\label{eq_mono_pos}
\ee
    \be
    \left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}<0,\forall i\in [n], \text{if } j\in \mathcal{V}_-\,.\label{eq_mono_neg}
\ee
This is because if a neuron satisfies $\lan x_i,w_j\ran=0$ for some $i$, and is not in $\mathcal{S}_\text{dead}$, GF moves $w_j$ towards $x_a(w_j)=\sum_{i:\lan x_i, w_j\ran>0}x_iy_i$. Interestingly, Assumption \ref{assump_data} implies $\lan x_iy_i,x_a(w_j)\ran>0,\forall i\in[n]$, which makes $\frac{d}{dt}\frac{w_j}{\|w_j\|}\simeq \sign(v_j(0))\mathcal{P}_{w_j}x_a(w_j)$ point inward (or outward) the halfspace $\lan x_iy_i,w_j\ran>0$, if $\sign(v_j(0))>0$ (or $\sign(v_j(0))<0$, respectively). See Figure \ref{fig_mono_activation} for illustration.
% \enrique{Figure three does not seem to have a number.}


As a consequence, a neuron can only change its activation pattern in a particular manner: a neuron in $\mathcal{V}_+$, whenever it is activated by some $x_i$ with $y_i=+1$, never loses the activation on $x_i$ thereafter, because \eqref{eq_mono_pos} implies that GF pushes $\frac{w_j}{\|w_j\|}$ towards $x_i$ at the boundary $\lan w_j,x_i\ran=0$. Moreover, \eqref{eq_mono_pos} also shows that a neuron in $\mathcal{V}_+$ will never regain activation on a $x_i$ with $y_i=-1$ once it loses the activation because GF pushes $\frac{w_j}{\|w_j\|}$ against $x_i$ at the boundary $\lan w_i,x_i\ran=0$. Similarly, a neuron in $\mathcal{V}_-$ never loses activation on negative data and never gains activation on positive data.

% Figure environment removed

\paragraph{Bound on activation transitions and duration:}
Equations \eqref{eq_mono_pos} and \eqref{eq_mono_neg} are key in the analysis of alignment because they limit how many times a neuron can change its activation pattern: a neuron in $\mathcal{V}_+$ can only gain activation on positive data and lose activation on negative data, thus at maximum, a neuron $w_j,\ j\in\mathcal{V}_+$, can start with full activation on all negative data and no activation on any positive one (which implies $w_j(0)\in\mathcal{S}_-$) then lose activation on every negative data and gain activation on every positive data as GF training proceeds (which implies $w_j(t_1)\in\mathcal{S}_+$), taking at most $n$ changes on its activation pattern. See Figure \ref{fig_activation_traj} for an illustration. 
Then, since it is possible to show that a neuron $w_j$ with $j\in \mathcal{V}_+$ that has $\cos(w_j,x_-)<1$ (guaranteed by Assumption \ref{assump_non_degen}) and is not in $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$, must change its activation pattern after $\mathcal{O}( \frac{1}{n_a\sqrt{\mu}})$ time (that does not depend on $\epsilon$), where $n_a$ is the number of data that currently activates $w_j$, one can upper bound the time for $w_j$ to reach $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$ by some $t_1=\mathcal{O}( \frac{\log n}{\sqrt{\mu}})$ constant independent of $\epsilon$. 
% Thus by making $\epsilon$ sufficiently small, one can make $t_1$ larger than this time.
% Moreover, we can show that this neuron $w_j$ must change its activation pattern after $\mathcal{O}\lp \frac{1}{n_a}\rp$ time (that does not depend on $\epsilon$), where $n_a$ is the number of data that currently activates $w_j$, unless either it reaches $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$, from which one can upper bound the time for $w_j$ to reach $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$ by some $\mathcal{O}\lp \log n\rp$ constant, and $\epsilon$ only needs to be sufficiently small for $t_1$ to be larger this constant time. 
Moreover, $w_j$ must reach $\mathcal{S}_+$ if it initially has activation on at least one positive data, i.e., $\max_{i\in\mathcal{I}_+}\lan w_j(0),x_i\ran>0$ since it cannot lose this activation. A similar argument holds for $w_j, j\in\mathcal{V}_-$ that they reaches either $\mathcal{S}_-$ or $\mathcal{S}_{\text{dead}}$ before $t_1$. 

\paragraph{Choice of $\epsilon$:}
All the aforementioned analyses rely on the assumption that the approximation in equation \eqref{eq_dir_flow} holds with some specific error bound. We show in Appendix \ref{app_pf_thm_align} that the desired bound is
$\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}- \sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV\leq \mathcal{O}(\sqrt{\mu})$, which, by Lemma \ref{lem_small_norm_informal}, can be achieved by a sufficiently small initialization scale $\epsilon_1=\mathcal{O}(\frac{\sqrt{\mu}}{\sqrt{h}n})$. Moreover, the directional convergence (which takes $\mathcal{O}( \frac{\log n}{\sqrt{\mu}})$ time) should be achieved before the alignment phase ends, which happens at $T=\Theta(\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$. This is ensured by choosing another sufficiently small initialization scale $\epsilon_2=\mathcal{O}( \frac{1}{\sqrt{h}}\exp( -\frac{n}{\sqrt{\mu}}\log n))$. Overall, the initialization scale should satisfy $\epsilon\leq \min\{\epsilon_1,\epsilon_2\}$. We opt to present $\epsilon_2$ in our main theorem because $\epsilon_2$ beats $\epsilon_1$ when $n$ is large.

% {\color{red} \textbf{Technical contribution}: To be completed...

% \textbf{Sufficiently small $\epsilon$}: We further discuss here the upper bound on $\epsilon$ in Theorem \ref{thm_conv_main}. More precisely, we require $\epsilon\leq \min\lb\frac{C_2\min\{\mu,\zeta_1,\zeta_2\}\sqrt{\mu}}{\sqrt{h}n}, \frac{1}{\sqrt{h}}\exp\lp -\frac{C_1n}{\min\{\zeta_1,\zeta_2\} \sqrt{\mu}}\log n\rp\rb$, where $C_1, C_2$ depends on data and neuron norms $\|x_i\|$,$\|w_j\|$ and we refer readers to Appendix \ref{app_pf_thm_align} for the full definition. First, we would point out there are essentially two upper bounds for $\epsilon$: the first $\mathcal{O}(\frac{1}{\sqrt{h}n})$ is required for a good approximation of $\frac{d}{dt}\frac{w_j}{\|w_j\|}$ as in \eqref{eq_dir_flow}, which enables a careful analysis for the alignment phase; the second $\mathcal{O}(\frac{1}{\sqrt{h}\exp(-n\log n)})$ is required for the alignment phase to last long enough so that all neurons reaches either $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$.

% Here we would like to highlight in the upper bound the dependence on $\mu$, characterizing how data are separated, and on two quantities: $\zeta_1=1-\max_{w\in \mathcal{S}_+^c\cap \mathcal{S}_-^c\cap\mathcal{S}_\text{dead}^c}\cos(w,x_a(w))$, and $\zeta_2=\min\{1-\max_{j\in\mathcal{V}_+}\cos(w_j(0),x_-),1-\max_{j\in\mathcal{V}_-}\cos(w_j(0),x_+)\}$. $\zeta_1$ depends solely on the data and Assumption \ref{assump_data} ensures $\zeta_1>0$; $\zeta_2$ depends on the initialization and Assumption \ref{assump_non_degen} ensures $\zeta_2>0$.

% }
% \enrique{This is a good place, to make a more refined comparison, perhaps highlighting some of the differences with the literature.}

% \enrique{How is Assumption 2 being used? There should be a reference to it at some point.}


\section{Numerical Experiments}\label{sec_num}

% {\color{red}[...we did great experiments to validate our theorem...]}
% \subsection{Binary classification on two MNIST digits}
% \subsection{Illustrative example}
% We first illustrate our theorem using a toy example: we train a two-layer ReLU network with $h=50$ neurons under a toy dataset in $\mathbb{R}^2$ (See Figure. \ref{fig_illust_ex}) that satisfies our Assumption \ref{assump_data}, and initialize all entries of the weights as $[W]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha\rp, v_j\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha\rp,\forall i\in[n],j\in[h]$ with $\alpha=10^{-6}$. Then we run gradient descent on both $W$ and $v$ with step size $\eta=2\times 10^{-3}$. Our theorem well predicts the dynamics of neurons at the early stage of the training: aside from neurons that ended up in $\mathcal{S}_\text{dead}$, neurons in $\mathcal{V}_+$ reach $\mathcal{S}_+$ and achieve good alignment with $x_+$, and neurons in $\mathcal{V}_-$ are well aligned with $x_-$ in $\mathcal{S}_-$. Note that after alignment, the loss experiences two sharp decreases before it gets close to zero, which is studied and explained in~\cite{boursier2022gradient}.


% \subsection{Binary classification on two MNIST digits}
We use a toy example in Appendix \ref{app_ssec_illu_exmpl} to clearly visualize the neuron alignment during training (due to space constraints). In the main body of this paper, we validate our theorem using a binary classification task for two MNIST digits. Such training data do not satisfy Assumption \ref{assump_data} since every data vector is a grayscale image with non-negative entries, making the inner product between any pair of data non-negative, regardless of their labels. However, we can preprocess the training data by centering: $x_i\leftarrow x_i-\bar{x}$, where $\bar{x}=\sum_{i\in[n]}x_i/n$. The preprocessed data, then,  approximately satisfies our assumption (see the left-most plot in Figure \ref{fig_mnist}): a pair of data points is very likely to have a positive correlation if they have the same label and to have a negative correlation if they have different labels. Thus we expect our theorem to make reasonable predictions on the training dynamics with preprocessed data. 
% Figure environment removed
For the remaining section, we use $x_i,i\in[n]$, to denote the preprocessed (centered) data and use $\bar{x}$ to denote the mean of the original data. 
% Here, we pick digits $1$ and $0$ for this section and present additional experiments with different choices of digits in Appendix \ref{app_addi_experiments}.

We build a two-layer ReLU network with $h=50$ neurons and initialize all entries of the weights as $[W]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha^2\rp, v_j\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha^2\rp,\forall i\in[n],j\in[h]$ with $\alpha=10^{-6}$. Then we run gradient descent on both $W$ and $v$ with step size $\eta=2\times 10^{-3}$. Notice that here the weights are not initialized to be balanced as in \eqref{eq_init}. The numerical results are shown in Figure \ref{fig_mnist}. 

\paragraph{Alignment phase:} Without balancedness, one no longer has $\sign(v_j(t))=\sign(v_j(0))$. With a little abuse of notation, we denote $\mathcal{V}_+(t)=\{j\in[h]:\sign(v_j(t))>0\}$ and $\mathcal{V}_+(t)=\{j\in[h]:\sign(v_j(t))>0\}$, and we expect that at the end of the alignment phase, neurons in $\mathcal{V}_+$ are aligned with $x_+=\sum_{i\in\mathcal{I}_+}x_i$, and neurons in $\mathcal{V}_-$ with $x_-=\sum_{i\in\mathcal{I}_-}x_i$. The second plot in Figure \ref{fig_mnist} shows such an alignment between neurons and $x_+,x_-$. In the top part, the red solid line shows $\cos(\bar{w}_+,x_+)$ during training, where $\bar{w}_+=\sum_{j\in\mathcal{V}_+}w_j/|\mathcal{V}_+|$, and the shaded region defines the range between $\min_{j\in\mathcal{V}_+}\cos(w_i,x_+)$ and $\max_{j\in\mathcal{V}_+}\cos(w_i,x_+)$. Similarly,  in the bottom part, the green solid line shows $\cos(\bar{w}_-,x_-)$ during training, where $\bar{w}_-=\sum_{j\in\mathcal{V}_-}w_j/|\mathcal{V}_-|$, and the shaded region delineates the range between $\min_{j\in\mathcal{V}_-}\cos(w_i,x_-)$ and $\max_{j\in\mathcal{V}_-}\cos(w_i,x_-)$. Initially, every neuron is approximately orthogonal to $x_+,x_-$ due to random initialization. Then all neurons in $\mathcal{V}_+$ ($\mathcal{V}_-$) start to move towards $x_+$ ($x_-$) and achieve good alignment after ${\sim}2000$ iterations. When the loss starts to decrease, the alignment drops. We conjecture that because Assumption \ref{assump_data} is not exactly satisfied, neurons in $\mathcal{V}_+$ have to fit some negative data, for which $x_+$ is not the best direction.

\paragraph{Final convergence:} After ${\sim 3000}$ iterations, the norm $\|W\|_2^2$ starts to grow and the loss decreases, as shown in the third plot in Figure \ref{fig_mnist}. Moreover, the stable rank $\|W\|_F^2/\|W\|_2^2$ decreases below $2$. For this experiment, we almost have $\cos(x_+,x_-)\simeq -1$, thus the neurons in $\mathcal{V}_+$ (aligned with $x_+$) and those in $\mathcal{V}_-$ (aligned with $x_-$) are almost co-linear. Therefore, the stable rank $\|W\|_F^2/\|W\|_2^2$ is almost $1$, as seen from the plot. Finally, at iteration $15000$, we visualize the mean neuron $\bar{w}_+=\sum_{j\in\mathcal{V}_+}w_j/|\mathcal{V}_+|$, $\bar{w}_-=\sum_{j\in\mathcal{V}_-}w_j/|\mathcal{V}_-|$ as grayscale images, and compare them with $\bar{x}_+=x_+/|\mathcal{I}_+|,x_-=x_-/|\mathcal{I}_-|$, showing good alignment.
% We also show the images when the original data center $\bar{x}$ is added back.

\paragraph{Comparison with other training schemes:} For two-layer ReLU networks, there is another line of work~\citep{brutzkus2018sgd,wang2019} that studies GD/SGD only on the first-layer weights $W$ and keeping the second-layer $v$ fixed throughout training. In Appendix \ref{app_addi_experiments_comp}, we compare our training schemes to those in~\citet{brutzkus2018sgd,wang2019}, and show that while both schemes achieve small training loss, the aforementioned two-phase training (alignment then final convergence) does no happen if only the first-layer in trained.
% 
\section{Conclusion}
\vspace{-3mm}
This paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks under small initialization. We consider a training dataset with well-separated input vectors. A careful analysis of the neurons' directional dynamics allows us to provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data. 
% After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Lastly, the 
Numerical experiment on classifying two digits from the MNIST dataset correlates with our theoretical findings.
% Future directions include extending our results for gradient descent and considering multi-class classification problems.

% \textbf{Limitation of our work}: First, we would like to point out that although we show, via numerical experiments, that our theorem works even when Assumption \ref{assump_data} approximately holds, the assumption that data are well-separated is still limiting since it can not explain why training with uncentered data still achieve good generalization performance. Besides, the network architecture considered in this paper is simple fully connected two-layer ReLU network, and it is unclear whether a similar analysis for neuron alignment would work for deep networks. Addressing these limitations is also an interesting future research direction.
\section*{Acknowledgement}
The authors thank the support of the NSF-Simons Research Collaborations on the Mathematical and Scientific Foundations of Deep Learning (NSF grant 2031985), the NSF HDR TRIPODS Institute for the Foundations of Graph and Deep Learning (NSF grant 1934979), the ONR MURI Program (ONR grant 503405-78051), and the NSF CAREER Program (NSF grant 1752362). The authors thank Ziqing Xu and Salma Tarmoun for the insightful discussions. 

\bibliographystyle{unsrtnat}
\bibliography{ref.bib}

\newpage
\appendix
\input{appendix.tex}
\end{document}