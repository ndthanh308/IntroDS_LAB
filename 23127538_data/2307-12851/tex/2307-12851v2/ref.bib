@book{horn1994topics,
  title={Topics in Matrix Analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={1994},
  publisher={Cambridge University Press}
}

@ARTICLE{wang1986,
  author={ {Sheng-De Wang} and  {Te-Son Kuo} and  {Chen-Fa Hsu}},
  journal={IEEE Transactions on Automatic Control}, 
  title={Trace bounds on the solution of the algebraic matrix Riccati and Lyapunov equation}, 
  year={1986},
  volume={31},
  number={7},
  pages={654-656},}
  
 @article{gronwall1919,
 ISSN = {0003486X},
 author = {T. H. Gr\"onwall},
 journal = {Annals of Mathematics},
 number = {4},
 pages = {292--296},
 publisher = {Annals of Mathematics},
 title = {Note on the Derivatives with Respect to a Parameter of the Solutions of a System of Differential Equations},
 volume = {20},
 year = {1919}
}

@article{davidson2001local,
  title={Local operator theory, random matrices and Banach spaces},
  author={Davidson, Kenneth R and Szarek, Stanislaw J},
  journal={Handbook of the geometry of Banach spaces},
  volume={1},
  number={317-366},
  pages={131},
  year={2001}
}

@inproceedings{zhang2017,
title	= {Understanding deep learning requires rethinking generalization},
author	= {Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
year	= {2017},
booktitle = {Proceedings of the International Conference on Learning Representations (ICLR), 2017}
}

@inproceedings{du2019a,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  booktitle={International Conference on Learning Representations(ICLR), 2019},
  year={2019}
}

@inproceedings{du2019width,
  title={Width Provably Matters in Optimization for Deep Linear Neural Networks},
  author={Du, Simon and Hu, Wei},
  booktitle={International Conference on Machine Learning},
  pages={1655--1664},
  year={2019}
}

@inproceedings{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6158--6169},
  year={2019}
}

@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={35th International Conference on Machine Learning},
  year={2018}
}

@inproceedings{saxe2014exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural network},
  author={Saxe, Andrew M and Mcclelland, James L and Ganguli, Surya},
  booktitle={International Conference on Learning Representations},
  year={2014},
}

@article{buchanan2020deep,
  title={Deep Networks and the Multiple Manifold Problem},
  author={Buchanan, Sam and Gilboa, Dar and Wright, John},
  journal={arXiv preprint arXiv:2008.11245},
  year={2020}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8141--8150},
  year={2019}
}

@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2937--2947},
  year={2019}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}

@inproceedings{du2019b,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10836--10846},
  year={2019}
}

@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}

@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}

@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}

@inproceedings{arora2018convergence,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{bartlett2018gradient,
  title={Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks},
  author={Bartlett, Peter and Helmbold, Dave and Long, Philip},
  booktitle={International conference on machine learning},
  pages={521--530},
  year={2018},
  organization={PMLR}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}


@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{rawat2017deep,
  title={Deep convolutional neural networks for image classification: A comprehensive review},
  author={Rawat, Waseem and Wang, Zenghui},
  journal={Neural computation},
  volume={29},
  number={9},
  pages={2352--2449},
  year={2017},
  publisher={MIT Press}
}

@inproceedings{graves2013speech,
  title={Speech recognition with deep recurrent neural networks},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6645--6649},
  year={2013},
  organization={IEEE}
}
@article{hinton2012deep,
  title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  journal={IEEE Signal processing magazine},
  volume={29},
  number={6},
  pages={82--97},
  year={2012},
  publisher={IEEE}
}


@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{vinyals2017starcraft,
  title={Starcraft ii: A new challenge for reinforcement learning},
  author={Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:1708.04782},
  year={2017}
}

@book{hirsch1974differential,
  title={Differential equations, dynamical systems, and linear algebra},
  author={Hirsch, Morris W and Devaney, Robert L and Smale, Stephen},
  volume={60},
  year={1974},
  publisher={Academic press}
}

@book{Horn:2012:MA:2422911,
 author = {Horn, Roger A. and Johnson, Charles R.},
 title = {Matrix Analysis},
 year = {2012},
 edition = {2nd},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 
@inproceedings{Du&Lee,
Author = {Du, Simon S and Hu, Wei and Lee, Jason D},
Booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
Title = {Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced},
Year = {2018}}

@inproceedings{ji2019gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@inproceedings{lyu2019gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={Lyu, Kaifeng and Li, Jian},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={6152--6160},
  year={2017}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{Gidel2019,
 author = {Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {3202--3211},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Regularization of Discrete Gradient Dynamics in Linear Neural Networks},
 volume = {32},
 year = {2019}
}

@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}

@article{moroshko2020implicit,
  title={Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author={Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing Implicit Bias in Terms of Optimization Geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={In International Conference on Machine Learning},
  year={2018}
}

@inproceedings{bai2019beyond,
  title={Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author={Bai, Yu and Lee, Jason D},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@InProceedings{pmlr-v139-tarmoun21a,
  title = 	 {Understanding the Dynamics of Gradient Flow in Overparameterized Linear models},
  author =       {Tarmoun, Salma and Fran\c{c}a, Guilherme and Haeffele, Benjamin D and Vidal, Ren\'e},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10153--10161},
  year = 	 {2021},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/tarmoun21a/tarmoun21a.pdf},
  abstract = 	 {We provide a detailed analysis of the dynamics ofthe gradient flow in overparameterized two-layerlinear models. A particularly interesting featureof this model is that its nonlinear dynamics can beexactly solved as a consequence of a large num-ber of conservation laws that constrain the systemto follow particular trajectories. More precisely,the gradient flow preserves the difference of theGramian matrices of the input and output weights,and its convergence to equilibrium depends onboth the magnitude of that difference (which isfixed at initialization) and the spectrum of the data.In addition, and generalizing prior work, we proveour results without assuming small, balanced orspectral initialization for the weights. Moreover,we establish interesting mathematical connectionsbetween matrix factorization problems and differ-ential equations of the Riccati type.}
}

@article{mtvm22prpt,
  title={Convergence and Implicit Bias of Gradient Flow on Overparametrized Linear Networks},
  author={Min, Hancheng and Tarmoun, Salma and Vidal, Ren\'e and Mallada, Enrique},
  journal={arXiv preprint arXiv:2105.06351},
  year={2022}
}


@InProceedings{mtvm21icml,
  title = 	 {On the Explicit Role of Initialization on the Convergence and Implicit Bias of Overparametrized Linear Networks},
  author =       {Min, Hancheng and Tarmoun, Salma and Vidal, Ren\'e and Mallada, Enrique},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7760--7768},
  year = 	 {2021},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/min21c/min21c.pdf},
  abstract = 	 {Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is to study how initialization and overparametrization affect convergence and implicit bias of training algorithms. In this paper, we present a novel analysis of single-hidden-layer linear networks trained under gradient flow, which connects initialization, optimization, and overparametrization. Firstly, we show that the squared loss converges exponentially to its optimum at a rate that depends on the level of imbalance of the initialization. Secondly, we show that proper initialization constrains the dynamics of the network parameters to lie within an invariant set. In turn, minimizing the loss over this set leads to the min-norm solution. Finally, we show that large hidden layer width, together with (properly scaled) random initialization, ensures proximity to such an invariant set during training, allowing us to derive a novel non-asymptotic upper-bound on the distance between the trained network and the min-norm solution.}
}


@article{schacke2004kronecker,
  title={On the kronecker product},
  author={Schacke, Kathrin},
  journal={Master's thesis, University of Waterloo},
  year={2004}
}


@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{WANG201965,
title = {The converse of Weyl's eigenvalue inequality},
journal = {Advances in Applied Mathematics},
volume = {109},
pages = {65-73},
year = {2019},
issn = {0196-8858},
doi = {https://doi.org/10.1016/j.aam.2019.05.003},
author = {Yi Wang and Sainan Zheng},
keywords = {Hermitian matrix, Eigenvalue, Weyl's inequality, Inertia index},
abstract = {We establish the converse of Weyl's eigenvalue inequality for additive Hermitian perturbations of a Hermitian matrix.}
}

@inproceedings{gunasekar2018convo,
 author = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Bias of Gradient Descent on Linear Convolutional Networks},
 volume = {31},
 year = {2018}
}

@inproceedings{yun2020unifying,
  title={A unifying view on implicit bias in training linear neural networks},
  author={Yun, Chulhee and Krishnan, Shankar and Mobahi, Hossein},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{zhou2017characterization,
  title={Characterization of gradient dominance and regularity conditions for neural networks},
  author={Zhou, Yi and Liang, Yingbin},
  journal={arXiv preprint arXiv:1710.06910},
  year={2017}
}

@inproceedings{Wu2019ResLin,
 author = {Wu, Lei and Wang, Qingcan and Ma, Chao},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Global Convergence of Gradient Descent  for Deep Linear Residual Networks},
 volume = {32},
 year = {2019}
}

@inproceedings{
Hu2020Provable,
title={Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks},
author={Wei Hu and Lechao Xiao and Jeffrey Pennington},
booktitle={International Conference on Learning Representations},
year={2020}
}

@book {rudin_bb,
    AUTHOR = {Rudin, Walter},
     TITLE = {Principles of mathematical analysis},
 PUBLISHER = {McGraw-Hill Book Company, Inc., New York-Toronto-London},
      YEAR = {1953},
     PAGES = {ix+227},
   MRCLASS = {27.2X},
  MRNUMBER = {0055409 (14,1070c)},
MRREVIEWER = {U. S. Haslam-Jones},
  BOEKCODE = {26-01},
}


@InProceedings{lee2016gd,
  title = 	 {Gradient Descent Only Converges to Minimizers},
  author = 	 {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1246--1257},
  year = 	 {2016},
  editor = 	 {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v49/lee16.pdf},
  abstract = 	 {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.}
}


@InProceedings{jin2017escape,
  title = 	 {How to Escape Saddle Points Efficiently},
  author =       {Chi Jin and Rong Ge and Praneeth Netrapalli and Sham M. Kakade and Michael I. Jordan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1724--1732},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/jin17a/jin17a.pdf},
  abstract = 	 {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost “dimension-free”). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.}
}

@inproceedings{du2017escape,
author = {Du, Simon S. and Jin, Chi and Lee, Jason D. and Jordan, Michael I. and P\'{o}czos, Barnab\'{a}s and Singh, Aarti},
title = {Gradient Descent Can Take Exponential Time to Escape Saddle Points},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points—it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {1067–1077},
numpages = {11},
location = {Long Beach, California, USA},
series = {NIPS'17}
}

@misc{sun2015nonconvex,
  doi = {10.48550/ARXIV.1510.06096},
  author = {Sun, Ju and Qu, Qing and Wright, John},
  keywords = {Optimization and Control (math.OC), Information Theory (cs.IT), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {When Are Nonconvex Problems Not Scary?},
  publisher = {arXiv},
  year = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@InProceedings{ge2017spurious,
  title = 	 {No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis},
  author =       {Rong Ge and Chi Jin and Yi Zheng},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1233--1242},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/ge17a/ge17a.pdf},
  abstract = 	 {In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.}
}

@inproceedings{kawaguchi2016deep,
 author = {Kawaguchi, Kenji},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Learning without Poor Local Minima},
 volume = {29},
 year = {2016}
}


@InProceedings{li2018small,
  title = 	 {Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations},
  author =       {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {2--47},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/li18a/li18a.pdf},
  abstract = 	 {We show that the gradient descent algorithm provides an implicit regularization effect in the learning of over-parameterized matrix factorization models and one-hidden-layer neural networks with quadratic activations. Concretely, we show that given $\tilde{O}(dr^{2})$ random linear measurements of a rank $r$ positive semidefinite matrix $X^{\star}$, we can recover $X^{\star}$ by parameterizing it by $UU^\top$ with $U\in \mathbb R^{d\times d}$ and minimizing the squared loss, even if $r \ll d$. We prove that starting from a small initialization, gradient descent recovers $X^{\star}$ in $\tilde{O}(\sqrt{r})$ iterations approximately. The results solve the conjecture of Gunasekar et al.’17 under the restricted isometry property.  The technique can be applied to analyzing neural networks with one-hidden-layer quadratic activations with some technical modifications.}
}


@article{stoger2021small,
  title={Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction},
  author={St{\"o}ger, Dominik and Soltanolkotabi, Mahdi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{
li2021sparse,
title={Implicit Sparse Regularization: The Impact of Depth and Early Stopping},
author={Jiangyuan Li and Thanh V Nguyen and Chinmay Hegde and Raymond K. W. Wong},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021}
}

@inproceedings{
li2021towards,
title={Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning},
author={Zhiyuan Li and Yuping Luo and Kaifeng Lyu},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{
elkabetz2021continuous,
title={Continuous vs. Discrete Optimization of Deep Neural Networks},
author={Omer Elkabetz and Nadav Cohen},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
}

@inproceedings{
le2022training,
title={Training invariances and the low-rank phenomenon: beyond linear networks},
author={Thien Le and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2022},
}

@article{grzegorz2021noether,
  author = {Głuch, Grzegorz and Urbanke, Rüdiger},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Noether: The More Things Change, the More Stay the Same},
  
  publisher = {arXiv},
  
  year = {2021},
  journal={arXiv preprint arXiv:2104.05508},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Teschl2014MathematicalMI,
  title={Mathematical Methods in Quantum Mechanics: With Applications to Schr{\"o}dinger Operators, Second Edition},
  author={Gerald Teschl},
  year={2014}
}

@inproceedings{haeffele2017global,
  title={Global optimality in neural network training},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7331--7339},
  year={2017}
}

@article{haeffele2015global,
  title={Global optimality in tensor factorization, deep learning, and beyond},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  journal={arXiv preprint arXiv:1506.07540},
  year={2015}
}

@inproceedings{hao2018visual,
 author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Visualizing the Loss Landscape of Neural Nets},
 volume = {31},
 year = {2018}
}

@book{polyak1987optimization,
 author = {Polyak, Boris T.},
 title = {Introduction to optimization
},
 year = {1987},
 publisher = {New York: Optimization Software, Publications Division}
} 

@misc{philipp2017exploding,
  doi = {10.48550/ARXIV.1712.05577},
  
  author = {Philipp, George and Song, Dawn and Carbonell, Jaime G.},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The exploding gradient problem demystified - definition, prevalence, impact, origin, tradeoffs, and solutions},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{razin2022implicit,
  title={Implicit regularization in hierarchical tensor factorization and deep convolutional neural networks},
  author={Razin, Noam and Maman, Asaf and Cohen, Nadav},
  booktitle={International Conference on Machine Learning},
  pages={18422--18462},
  year={2022},
  organization={PMLR}
}
@article{soltanolkotabi2023implicit,
  title={Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing},
  author={Soltanolkotabi, Mahdi and St{\"o}ger, Dominik and Xie, Changzhi},
  journal={arXiv preprint arXiv:2303.14244},
  year={2023}
}

@article{lyu2021gradient,
  title={Gradient descent on two-layer nets: Margin maximization and simplicity bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12978--12991},
  year={2021}
}

@article{maennel2018gradient,
  title={Gradient descent quantizes relu network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}

@inproceedings{phuong2021inductive,
  title={The inductive bias of relu networks on orthogonally separable data},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={International Conference on Learning Representations},
  year={2021}
}


@inproceedings{boursier2022gradient,
 author = {Boursier, Etienne and Pullaud-Vivien, Loucas and Flammarion, Nicolas},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {20105--20118},
 title = {Gradient flow dynamics of shallow ReLU networks for square loss and orthogonal inputs},
 volume = {35},
 year = {2022}
}

@misc{
min2023on,
title={On the Convergence of Gradient Flow on Multi-layer Linear Models},
author={Hancheng Min and Ren\'e Vidal and Enrique Mallada},
year={2023},
}

@article{bolte2010characterizations,
  title={Characterizations of {\L}ojasiewicz inequalities: subgradient flows, talweg, convexity},
  author={Bolte, J{\'e}r{\^o}me and Daniilidis, Aris and Ley, Olivier and Mazet, Laurent},
  journal={Transactions of the American Mathematical Society},
  volume={362},
  number={6},
  pages={3319--3363},
  year={2010}
}
@article{persson1975,
title = {A generalization of Carathéodory's existence theorem for ordinary differential equations},
journal = {Journal of Mathematical Analysis and Applications},
volume = {49},
number = {2},
pages = {496-503},
year = {1975},
issn = {0022-247X},
doi = {https://doi.org/10.1016/0022-247X(75)90192-4},
author = {Jan Persson}
}

@book{schaft2000,
title = "An Introduction to Hybrid Dynamical Systems",
author = "{van der Schaft}, A.J. and J.M. Schumacher",
year = "2000",
language = "English",
series = "Lecture Notes in Control and Information Sciences",
publisher = "Springer Verlag",
number = "251",
address = "Germany",
}

@article{ercan1997,
title = {Extension and Separation of Vector Valued Functions},
journal = {Turkish Journal of Mathematics},
volume = {21},
number = {4},
year = {1997},
author = {Zafer Ercan}
}

@inproceedings{
wang2022the,
title={The Convex Geometry of Backpropagation: Neural Network Gradient Flows Converge to Extreme Points of the Dual Convex Program},
author={Yifei Wang and Mert Pilanci},
booktitle={International Conference on Learning Representations},
year={2022},
}

@inproceedings{ji2020dir,
author = {Ji, Ziwei and Telgarsky, Matus},
title = {Directional Convergence and Alignment in Deep Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1441},
numpages = {11},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{filippov1971existence,
	title = {The existence of solutions of generalized differential equations},
	volume = {10},
	issn = {1573-8876},
	doi = {10.1007/BF01464722},
	abstract = {The existence is proved of a solution of the equation xɛ F(t, x), where F(t, x) is a nonempty closed set depending continuously on t and x, with no assumptions concerning the convexity of this set.},
	language = {en},
	number = {3},
	urldate = {2023-09-27},
	journal = {Mathematical notes of the Academy of Sciences of the USSR},
	author = {Filippov, A. F.},
	month = sep,
	year = {1971},
	keywords = {Differential Equation, Generalize Differential Equation},
	pages = {608--611}
}


@book{reid1971ode,
  title={Ordinary Differential Equations},
  author={Reid, W. T.},
  year={1971},
  publisher={Wiley, New York}
}

@inproceedings{
brutzkus2018sgd,
title={{SGD} Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data},
author={Alon Brutzkus and Amir Globerson and Eran Malach and Shai Shalev-Shwartz},
booktitle={International Conference on Learning Representations},
year={2018},
}

@ARTICLE{wang2019,
  author={Wang, Gang and Giannakis, Georgios B. and Chen, Jie},
  journal={IEEE Transactions on Signal Processing}, 
  title={Learning ReLU Networks on Linearly Separable Data: Algorithm, Optimality, and Generalization}, 
  year={2019},
  volume={67},
  number={9},
  pages={2357-2370},
  doi={10.1109/TSP.2019.2904921}}

@inproceedings{
varre2023on,
title={On the spectral bias of two-layer linear networks},
author={Aditya Vardhan Varre and Maria-Luiza Vladarean and Loucas Pillaud-Vivien and Nicolas Flammarion},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@misc{yaras2023law,
      title={The Law of Parsimony in Gradient Descent for Learning Deep Linear Networks}, 
      author={Can Yaras and Peng Wang and Wei Hu and Zhihui Zhu and Laura Balzano and Qing Qu},
      year={2023},
      eprint={2306.01154},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{frei2022implicit,
  title={Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data},
  author={Frei, Spencer and Vardi, Gal and Bartlett, Peter and Srebro, Nathan and Hu, Wei},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{wang2023understanding,
  title={Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks},
  author={Wang, Mingze and Ma, Chao},
  journal={arXiv preprint arXiv:2305.12467},
  year={2023}
}

@article{boursier2024early,
  title={Early alignment in two-layer networks training is a two-edged sword},
  author={Boursier, Etienne and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2401.10791},
  year={2024}
}

@inproceedings{
kou2023implicit,
title={Implicit Bias of Gradient Descent for Two-layer Re{LU} and Leaky Re{LU} Networks on Nearly-orthogonal Data},
author={Yiwen Kou and Zixiang Chen and Quanquan Gu},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023}
}

@inproceedings{
wang2022early,
title={Early Stage Convergence and Global Convergence of Training Mildly Parameterized Neural Networks},
author={Mingze Wang and Chao Ma},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022}
}