
\section{Additional Experiments}\label{app_addi_experiments}
\subsection{Illustrative example}\label{app_ssec_illu_exmpl}

We illustrate our theorem using a toy example: we train a two-layer ReLU network with $h=50$ neurons under a toy dataset in $\mathbb{R}^2$ (See Figure. \ref{fig_illust_ex}) that satisfies our Assumption \ref{assump_data}, and initialize all entries of the weights as $[W]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha^2\rp, v_j\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha^2\rp,\forall i\in[n],j\in[h]$ with $\alpha=10^{-6}$. Then we run gradient descent on both $W$ and $v$ with step size $\eta=2\times 10^{-3}$. Our theorem well predicts the dynamics of neurons at the early stage of the training: aside from neurons that ended up in $\mathcal{S}_\text{dead}$, neurons in $\mathcal{V}_+$ reach $\mathcal{S}_+$ and achieve good alignment with $x_+$, and neurons in $\mathcal{V}_-$ are well aligned with $x_-$ in $\mathcal{S}_-$. Note that after alignment, the loss experiences two sharp decreases before it gets close to zero, which is studied and explained in~\cite{boursier2022gradient}.
% Figure environment removed
\subsection{Effect of data separability $\mu$}\label{app_ssec_data_sep}
This section investigates the effect of data separability $\mu$ on the time required to achieve the desired alignment as in Theorem \ref{thm_conv_main}, through a simple example. we consider a similar setting as in \ref{app_ssec_illu_exmpl}, and explore the cases when data separability $\mu \ll 1$. We expect that as separability $\mu$ decreases, the time for neurons to achieve the desired alignment as in Theorem \ref{thm_conv_main} increases, necessitating a smaller initialization scale. For simplicity, we consider a dataset with only two positive data $(x_1,y_1=+1),(x_2,y_2=+1)$. 

In Figure \ref{fig_app_small_pos}, we first set $\mu=\lan x_1,x_2\ran=\sin(0.1)$, and the neuron alignment is consistent with Theorem \ref{thm_conv_main}: positive neurons (that are not dead) eventually enters $\mathcal{S}_+$, activating both data points, and then final convergence follows. 
% Figure environment removed

However, in Figure \ref{fig_app_tiny_pos}, as we decrease the separability $\mu$ to $\sin(0.001)$ (other settings remain unchanged), the neural alignment becomes slower: 1) at iteration 7000, there are still neurons (that are not dead) outside $\mathcal{S}_+$, namely those aligned with either $x_1$ or $x_2$, while in our previous setting ($\mu=\sin(0.1)$), all neurons (that are not dead) have reached $\mathcal{S}_+$; 2) In this particular instance of the experiment, we also see one neuron remains outside $\mathcal{S}_+$ at the late stage of the training (at iteration 21000). This clearly shows that as data separability $\mu$ decreases, the time needed for all neurons (that are not dead) to reach $\mathcal{S}_+$ increases, and if the initialization scale is not small enough for the alignment phase to hold for a long time, there will be neurons remains outside $\mathcal{S}_+$.
% Figure environment removed
\subsection{Neuron dynamics under orthogonal data}\label{app_ssec_orth}
We have seen in the last section how a small $\mu$ affects the neuron dynamics. The orthogonal data assumption studied in~\citet{boursier2022gradient} is precisely the extreme case of $\mu\ra 0$, where the neuron behavior changes substantially. We follow exactly the same setting in Appendix \ref{app_ssec_data_sep} and consider the case of $\mu=0$.

In Figure \ref{fig_app_orth}, we see that \textbf{$\mathcal{S}_+$ is no longer the region that contains all (non-dead) positive neurons at the end of the alignment phase}. Depending on where each neuron is initialized, it could end up being in $\mathcal{S}_+$, aligned with $x_1$, or aligned with $x_2$. Moreover, for final convergence, only the neurons ended up in $\mathcal{S}_+$ grow their norms and fit the data, whose number is clearly less than that in the case of $\mu>0$.
% Figure environment removed

This difference in neurons' dynamical behavior makes the analysis in~\citet{boursier2022gradient} different than ours: First,~\citet{boursier2022gradient} only studies the dynamics of the positive (negative) neurons that initially activate all positive (negative) data, which will end up in $\mathcal{S}_+$ ($\mathcal{S}_-$) and fit the data, and the analysis does not evolve the changes in their activation pattern. In our case, any positive (negative) neurons could potentially end up in $\mathcal{S}_+$ ($\mathcal{S}_-$), and in particular, it will if it initially activates at least one positive (negative) data, thus it becomes necessary to track the evolution of the activation pattern of all these neurons (novelty in our analysis). Moreover, consider the case that neurons are being randomly initialized,~\citet{boursier2022gradient} requires the set of positive (negative) neurons that initially activate all positive (negative) data being non-empty, which needs the number of neurons $h$ to scale exponentially in number training data $n$ (extremely overparametrized). In our case, we only require $h=\Theta(1)$ (See \textbf{Merits of overparametrization} after Theorem \ref{thm_conv_main}), a mild overparamerization.

In summary, while~\citet{boursier2022gradient} also provides quantitative analysis on neural alignment under small initialization, it is done under the assumption that all data are orthogonal to each other, leading to a different neuron dynamical behavior than ours. Due to such differences, their analysis cannot be directly applied to the case of orthogonally separable data (ours), for which we develop novel analyses on the evolution of neuron activation patterns (See proof sketch in Section \ref{ssec_pf_sketch}).

\subsection{Additional experiments on MNIST dataset}
We use exactly the same experimental setting as in the main paper and only use a different pair of digits. The results are as follows:
% Figure environment removed
% Figure environment removed

{\color{black}\subsection{Discussion on the two-phase convergence}\label{app_addi_experiments_comp}
With the same two-digit MNIST dataset in Section \ref{sec_num}, we further discuss the two-phase convergence under small initialization. We use a two-layer ReLU network with $h=50$ neurons and initialize all entries of the weights as $[W]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha^2\rp, v_j\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha^2\rp,\forall i\in[n],j\in[h]$ with $\alpha=10^{-6}$. Then we run stochastic gradient descent (SGD) with batch size 2000 on both $W$ and $v$ with step size $\eta=2\times 10^{-3}$. For comparison, we also consider the training schemes studied in~\citet{brutzkus2018sgd,wang2019}, where only the first-layer weight $W$ is trained starting from a small initialization $[W]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha^2\rp$, and $v_j$ are chosen to be either $+1$ or $-1$ with equal probability, then fixed throughout training.

We consider the changes in neuron norms and directions separately. In particular, these quantities are defined as
\be
    \sum_i\left.\frac{d}{dt}\|w_j\|^2\right\vert_{\dot{w}_j=-\nabla_{w_j}\mathcal{L}}=\sum_i2\lan -\nabla_{w_j}\mathcal{L}\,, w_j\ran\tag{changes in neuron norms}
\ee
\be
    \sum_i\lV\left.\frac{d}{dt}\frac{w_j}{\|w_j\|}\right\vert_{\dot{w}_j=-\nabla_{w_j}\mathcal{L}}\rV=\sum_i\lV\mathcal{P}_{w_j}\lp\frac{-\nabla_{w_j}\mathcal{L}}{\|w_j\|} \rp\rV\,,\tag{changes in neuron directions}
\ee
and they measure, at the end of every epoch, how much the neuron norms and directions will change if one uses a one-step full gradient descent with a small step size. 
% Figure environment removed

\textbf{Training both layers}: In Figure \ref{fig_minist_two_phase}, we show the changes in neuron norms and directions over the training trajectory when we run stochastic gradient descent (SGD) on both first- and second-layer weights. The two-phase (alignment phase then final convergence) is clearly shown by comparing the relative scale of changes in neuron norms and directions in different phases of the training.

% Figure environment removed

\textbf{Training only the first layer}: In Figure \ref{fig_minist_one_phase}, we show the changes in neuron norms and directions over the training trajectory when we run stochastic gradient descent (SGD) on ONLY the first-layer weights~\citep{brutzkus2018sgd,wang2019}. The plot indicates that two-phase training does not happen in this case.
} 
% \subsection{Error bars}
% We provide here error bars for the numerical experiments in Section \ref{sec_num}. We run each experiment 20 times and report the mean loss curve below, the shaded region presents the values within 3 times the standard deviation.
% % Figure environment removed

\newpage
\section{Proof of Lemma \ref{lem_small_norm_informal}: Neuron Dynamics under Small Initialization}\label{app_pf_lem_small_norm}
The following property of $\ell$ (exponential loss $\ell(y,\hat{y})=\exp(-y\hat{y})$ or logistic loss $\ell(y,\hat{y})=2\log(1+\exp(-y\hat{y}))$) will be used throughout the Appendix for proofs of several results:
    \begin{lemma}\label{assump_loss}
        For $\ell$, we have
        % \ell$ is differentiable w.r.t. its second argument and given a label set $\mathcal{Y}\subseteq \mathbb{R}$, $\exists C,\xi$ s.t.
        \be
            |-\nabla_{\hat{y}}\ell (y,\hat{y})-y|\leq 2|\hat{y}|,\forall y\in\{+1,-1\},\quad \forall |\hat{y}|\leq 1\,.
        \ee
    \end{lemma}
    \begin{proof}
        \textbf{Exponential loss: when $\ell(y,\hat{y})=\exp(-y\hat{y})$:}
        \begin{align*}
            |-\nabla_{\hat{y}}\ell (y,\hat{y})-y|&=\;|y\exp(-y\hat{y})-y|\\
            &\leq\; |y||\exp(-y\hat{y})-1|\\
            &\leq\; |\exp(-y\hat{y})-1|\leq 2|\hat{y}|\,,
        \end{align*}
        where the last inequality is due to the fact that $2x\geq \max\{1-\exp(-x),\exp(x)-1\},\forall x\in[0,1]$.
        \textbf{Logistic loss: when $\ell(y,\hat{y})=2\log(1+\exp(-y\hat{y}))$:}
        \begin{align*}
            |-\nabla_{\hat{y}}\ell (y,\hat{y})-y|&=\;\lvt 2y\frac{\exp(-y\hat{y})}{1+\exp(-y\hat{y})}-y\rvt\\
            &=\; \lvt \frac{y\exp(-y\hat{y})-y}{1+\exp(-y\hat{y})}\rvt\\
            &\leq\; |y||\exp(-y\hat{y})-1|\\
            &\leq\; |\exp(-y\hat{y})-1|\leq 2|\hat{y}|\,,
        \end{align*}
    \end{proof}
\begin{remark}
    More generally, our subsequent results regarding neuron dynamics under small initialization hold for any loss function that satisfies the condition stated in Lemma \ref{assump_loss}, which includes the $l_2$ loss $\ell(y,\hat{y})=\frac{1}{2}(y-\hat{y})^2$ studied in~\citet{boursier2022gradient}. 
\end{remark}
\subsection{Formal statement}
Our results for neuron direction dynamics during the early phase of the training will be stated for networks with any $\alpha$-leaky ReLU activation $\sigma(x)=\max\{x,\alpha x\}$ with $\alpha\in [0,1]$. In particular, it is the ReLU activation when $\alpha=0$, which is the activation function we considered in the main paper, and it is the linear activation when $\alpha=1$.

Denote: $X_{\max}=\max_i\|x_i\|, W_{\max}=\max_j\|[W_0]_{:,j}\|$. The formal statement of Lemma \ref{lem_small_norm_informal} is as follow:
\begin{customlem}{1}
    Let the activation function be an $\alpha$-leaky ReLU activation $\sigma(x)=\max\{x,\alpha x\}$. Given some initialization from \eqref{eq_init}, for any $\epsilon\leq \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}$, then any solution to the gradient flow dynamics \eqref{eq_gf} satisfies that $\forall t\leq T=\frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}
    $,
    % $\max_j\|w_j(t)\|^2=\mathcal{O}(\frac{\epsilon}{\sqrt{h}})$, $\max_i|f(x_i;W(t),v(t))|=\mathcal{O}(\epsilon \sqrt{h})$ and 
    \ben
        \max_j\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}-\sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV\leq 4\epsilon n\sqrt{h}X_{\max}^2W_{\max}^2\,,
    \een
    where
    \ben
        x_a(w_j)=\sum_{i=1}^nx_iy_i\sigma'(\lan x_i,w_j\ran)=\sum_{i:\lan x_i,w_j\ran>0}x_iy_i+\alpha\sum_{i:\lan x_i,w_j\ran\leq 0}x_iy_i\,.
    \een
\end{customlem}

With Lemma \ref{lem_small_norm_informal}, and set $\alpha=0$, we obtain the results stated in the main paper. Lemma \ref{lem_small_norm_informal} is a direct result of the following two lemmas.
\begin{lemma}\label{lem_small_norm}
     Let the activation function be an $\alpha$-leaky ReLU activation $\sigma(x)=\max\{x,\alpha x\}$. Given some initialization in \eqref{eq_init}, then for any $\epsilon\leq \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}$, any solution to the gradient flow dynamics \eqref{eq_gf} satisfies
    \be
        \max_j\|w_j(t)\|^2\leq \frac{2\epsilon W_{\max}^2}{\sqrt{h}},\quad \max_i|f(x_i;W(t),v(t))|\leq 2\epsilon \sqrt{h}X_{\max}W_{\max}^2\,,
    \ee
    $\forall t\leq \frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}
    $.
\end{lemma}
\begin{lemma}\label{lem_dir_flow_approx}
    Let the activation function be an $\alpha$-leaky ReLU activation $\sigma(x)=\max\{x,\alpha x\}$. Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Whenever $\max_i|f(x_i;W,v)|\leq 1$, we have, $\forall i\in[n]$,
    \be
        \lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0)) \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp  x_a(w_j)\rV\leq 2n X_{\max} \max_i|f(x_i;W,v)|\,,
    \ee
    where
    \ben
        x_a(w_j)=\sum_{i=1}^nx_iy_i\sigma'(\lan x_i,w_j\ran)=\sum_{i:\lan x_i,w_j\ran>0}x_iy_i+\alpha\sum_{i:\lan x_i,w_j\ran\leq 0}x_iy_i\,.
    \een
\end{lemma}
\begin{remark}
    By stating our approximation results for neuron directional dynamics with any $\alpha$-leaky ReLU activation function, we highlight that even for some networks with other activation functions than ReLU, there is a similar notion of neuron alignment at the early stage of the training, and the analytical tools used in this paper can be applied to them. However, we note that our main results (Theorem \ref{thm_conv_main}) will not directly apply as the neuron directional dynamics have changed as we consider an activation function different than ReLU (see the general definition of $x_a(w_j)$), and additional efforts are required to establish the directional convergence for general leaky-ReLU functions.
\end{remark}
\subsection{Proof of Lemma \ref{lem_small_norm}: Bounds on Neuron Norms}
\begin{proof}[Proof of Lemma \ref{lem_small_norm}]
    Under gradient flow, we have
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.
    \ee
    Balanced initialization enforces $v_j=\sign(v_j(0))\|w_j\|$, hence
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\sigma'(\lan x_i,w_j\ran)\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
    \ee
    Let $T:=\inf\{t:\ \max_{i}|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\}$,  then $\forall t\leq T, j\in[h]$, we have 
    \begin{align}
        \frac{d}{dt}\|w_j\|^2
        &=\;\lan w_j, \frac{d}{dt}w_j \ran & \nonumber\\
        &=\; -2\sum_{i=1}^n\sigma'(\lan x_i,w_j\ran)\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|&\nonumber\\
        &\leq\; 2\sum_{i=1}^n\lvt\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\rvt \lvt\lan x_i, w_j\ran\rvt \|w_j\|&\nonumber\\
        &\leq\; 2\sum_{i=1}^n(|y_i|+2|f(x_i;W,v)|) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{by Lemma \ref{assump_loss}})\nonumber\\
        &\leq\; 2\sum_{i=1}^n(1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{Since } t\leq T)\nonumber\\
        &\leq\; 2\sum_{i=1}^n(1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2) \|x_i\|\|w_j\|^2& \nonumber\\
        &\leq \; 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\|w_j\|^2\,.&
    \end{align}
    Let $\tau_j:=\inf\{t: \|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}\}$, and let $j^*:=\arg\min_j \tau_j$, then $\tau_{j^*}=\min_{j}\tau_j\leq T$ due to the fact that 
    \ben
        |f(x_{i};W,v)|= \lvt\sum_{j\in[h]}\sigma'(\lan x_i,w_j\ran)v_j\lan w_j,x_i\ran\rvt\leq \sum_{j\in[h]} \|w_j\|^2\|x_i\|\leq hX_{\max}\max_{j\in[h]}\|w_j\|^2\,,
    \een
    which implies "$|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\Rightarrow \exists j, s.t.\|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}$". 
    
    Then for $t\leq \tau_{j^*}$, we have
    \be
        \frac{d}{dt}\|w_{j^*}\|^2\leq 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\|w_{j^*}\|^2\,.
    \ee
    By Gr\"onwall's inequality, we have $\forall t\leq \tau_{j^*}$
    \begin{align*}
        \|w_{j^*}(t)\|^2&\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\|w_{j^*}(0)\|^2\,,\\
         &=\;\exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\epsilon^2\|[W_0]_{:,j^*}\|^2\\
         &\leq \;\exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\epsilon^2W_{\max}^2\,.
    \end{align*}
    Suppose $\tau_{j^*}<\frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$, then by the continuity of $\|w_{j^*}(t)\|^2$, we have 
    \begin{align*}
        \frac{2\epsilon W_{\max}^2}{\sqrt{h}}\leq \|w_{j^*}(\tau_{j^*})\|^2&\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\tau_{j^*}\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 
        \frac{1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2}{2}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 
        \log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2=\frac{\epsilon W_{\max}^2}{\sqrt{h}}\,,
    \end{align*}
    which leads to a contradiction $2\epsilon\leq \epsilon$. Therefore, one must have $T\geq \tau_{j^*}\geq \frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$. This finishes the proof.
\end{proof}
\subsection{Proof of Lemma \ref{lem_dir_flow_approx}: Directional Dynamics of Neurons}
\begin{proof}[Proof of Lemma \ref{lem_dir_flow_approx}]
    As we showed in the proof for Lemma \ref{lem_small_norm}, under balanced initialization, 
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
    \ee
    Then for any $i\in[n]$,
    \begin{align*}
        \frac{d}{dt} \frac{w_j}{\|w_j\|}&=\; -\sign(v_j(0))\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lp x_i-\frac{\lan x_i,w_j\ran}{\|w_j\|^2}w_j \rp\\
        &=\; -\sign(v_j(0))\sum_{i: \lan x_i,w_j\ran>0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lp x_i-\frac{\lan x_i,w_j\ran}{\|w_j\|^2}w_j \rp\\
        &=\; -\sign(v_j(0))\lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp\lp\sum_{i=1}^n\sigma'(\lan x_i,w_j\ran) \nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\rp\,.
    \end{align*}
    Therefore, whenever $\max_i|f(x_i;W,v)|\leq 1$,
    \begin{align}
        &\;\lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0))\lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp x_a(w_j)\rV\nonumber\\
        =&\;\lV \sign(v_j(0))\lp\sum_{i=1}^n\sigma'(\lan x_i,w_j\ran) \lp\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))+y_i\rp x_i\rp\rV\nonumber\\
        \leq &\; \sum_{i=1}^n |\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))+y_i |\cdot \|x_i\|\nonumber\\
        \leq &\; \sum_{i=1}^n2|f(x_i;W,v)|\cdot \|x_i\|\leq 2nM_x \max_i|f(x_i;W,v)|\,.
    \end{align}
\end{proof}

\newpage
\section{Proof for Theorem \ref{thm_conv_main}: Early Alignment Phase}\label{app_pf_thm_align}
We break the proof of Theorem \ref{thm_conv_main} into two parts: In Appendix \ref{app_pf_thm_align} we prove the first part regarding directional convergence. Then in Appendix \ref{app_pf_thm_conv} we prove the remaining statement on final convergence and low-rank bias.
\subsection{Auxiliary lemmas}
The first several Lemmas concern mostly some conic geometry given the data assumption:

Consider the following conic hull
 \be
 K=\mathcal{CH}(\{x_iy_i,i\in[n]\})=\lb\sum_{i=1}^na_ix_iy_i: a_i\geq 0,i\in[n]\rb\,.
 \ee
It is clear that $x_iy_i\in K,\forall i$, and $x_a(w)\in K,\forall w$. 
The following lemma shows any pair of vectors in $K$ is $\mu$-coherent.
 \begin{lemma}\label{lem_app_K_coherence}
    $\cos(z_1,z_2)\geq \mu,\forall 0\neq z_1,z_2\in K$.
\end{lemma}
\begin{proof}
    Since $z_1,z_2\in K$, we let $z_1=\sum_{i=1}^nx_iy_ia_{1i}$, and$z_2=\sum_{j=1}^nx_jy_ja_{2j}$, where $a_{1i},a_{2j}\geq 0$ but not all of them.
    \begin{align*}
        \cos(z_1,z_2)= \frac{1}{\|z_1\|\|z_2\|}\lan z_1,z_2\ran
        &=\; \frac{1}{\|z_1\|\|z_2\|}\sum_{i,j\in[n]}a_{1i}a_{2j}\lan x_iy_i,x_jy_j\ran\\
        &=\; \frac{\sum_{i,j\in[n]}\|x_i\|\|x_j\|a_{1i}a_{2j}\mu}{\|z_1\|\|z_2\|}\geq \mu\,,
    \end{align*}
    where the last inequality is due to $$\|z_1\|\|z_2\|\leq \lp\sum_{i=1}^n\|x_i\|a_{1i}\rp\lp\sum_{j=1}^n\|x_j\|a_{2j}\rp=\sum_{i,j\in[n]}\|x_i\|\|x_j\|a_{1i}a_{2j}\,.$$
\end{proof}
The following lemma is some basic results regarding $\mathcal{S}_+$ and $\mathcal{S}_-$:
\begin{lemma}
    $\mathcal{S}_+$ and $\mathcal{S}_-$ are convex cones (excluding the origin).
\end{lemma}
\begin{proof}
    Since $\one_{\lan x_i,z\ran}=\one_{\lan x_i,az\ran},\forall i\in[n],a>0$, $\mathcal{S}_+,\mathcal{S}_-$ are cones. Moreover, $\lan x_i,z_1\ran > 0$ and $\lan x_i,z_2\ran > 0$ implies $\lan x_i,a_1z_1+a_2z_2\ran> 0,\forall a_1,a_2>0$, thus $\mathcal{S}_+,\mathcal{S}_-$ are convex cones.
\end{proof}
Now we consider the complete metric space $\mathbb{S}^{D-1}$ (w.r.t. $\arccos(\lan\cdot,\cdot\ran)$) and we are interested in its subsets $K\cap \mathbb{S}^{D-1}$, $\mathcal{S}_+\cap \mathbb{S}^{D-1}$, and $\mathcal{S}_-\cap \mathbb{S}^{D-1}$. First, we have (we use $\mathrm{Int}(S)$ to denote the interior of $S$)
\begin{lemma}\label{lem_app_int}
    $K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1})$, and $-K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_-\cap \mathbb{S}^{D-1})$
\end{lemma}
\begin{proof}
    Consider any $x_c=\sum_{j=1}^na_jx_jy_j\in K\cap \mathbb{S}^{D-1}$, For any $x_i,y_i, i\in[n]$, we have 
    \begin{align*}
        \lan x_c,x_i\ran&=\; \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\\
        &\geq\; \mu y_i\|x_i\| \sum_{i=j}^na_j\|x_j\|\begin{cases}
            \geq \mu X_{\min}>0, & y_i>0\\
            \leq -\mu X_{\min}<0, & y_i<0\\
        \end{cases}\,.
    \end{align*}
    Depending on the sign of $y_i$, we have either
    \ben
        \lan x_c,x_i\ran= \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\geq \mu \frac{\|x_i\|}{y_i} \sum_{i=j}^na_j\|x_j\|\geq \mu X_{\min}>0\,,\ (y_i=+1)
    \een
    or 
    \ben
        \lan x_c,x_i\ran= \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\leq \mu \frac{\|x_i\|}{y_i} \sum_{i=j}^na_j\|x_j\|\leq -\mu X_{\min}<0\,,\ (y_i=-1)
    \een
    where we use the fact that $1=\|x_c\|=\|\sum_{j=1}^na_jx_jy_j\|\leq \sum_{j=1}^na_j\|x_j\|$. This already tells us $x_c\in \mathcal{S}_+\cap \mathbb{S}^{D-1}$.
    
    Since $f_i(z)=\lan z, x_i\ran$ is a continuous function of $z\in\mathbb{S}^{D-1}$. There exists an open ball $\mathcal{B}\lp x_c,\delta_i\rp$ centered at $x_c$ with some radius $\delta_i>0$, such that $\forall z\in \mathcal{B}\lp x_c,\delta_i\rp$, one have $\lvt f_i(z)-f_i\lp x_c\rp\rvt\leq \frac{\mu X_{\min}}{2}$, which implies 
    \ben
        \lan z, x_i\ran\begin{cases}
            \geq \mu X_{\min}/2>0, & y_i>0\\
            \leq -\mu X_{\min}/2<0, & y_i<0\\
        \end{cases}\,.
    \een
    Hence $\cap_{i=1}^n\mathcal{B}\lp \frac{x_c}{\|x_c\|},\delta_i\rp\in\mathcal{S}_+\cap \mathbb{S}^{D-1}$. Therefore, $x_c\in \mathrm{Int}(\mathcal{S}_+\cap \mathbb
    {S}^{D-1})$. This suffices to show $K\cap \mathbb
    {S}^{D-1}\subset \mathrm{Int}(\mathcal{S}_+\cap \mathbb
    {S}^{D-1})$. The other statement $-K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_-\cap \mathbb{S}^{D-1})$ is proved similarly.
\end{proof}
The following two lemmas are some direct results of Lemma \ref{lem_app_int}.
\begin{lemma}\label{lem_app_zeta1}
    $\exists \zeta_1>0$ such that 
    \be
        \mathcal{S}_{x_+}^{\zeta_1}\subset \mathcal{S_+},\qquad\mathcal{S}_{x_-}^{\zeta_1}\subset \mathcal{S_-}\,,
    \ee
    where $\mathcal{S}_x^\zeta:=\{z\in\mathbb{R}^D:\ \cos(z,x)\geq \sqrt{1-\zeta}\}$.
\end{lemma}
\begin{proof}
    By Lemma \ref{lem_app_int}, $\frac{x_+}{\|x_+\|}\in K\subset \mathrm{Int}(S_+)$. Since $\mathbb{S}^{D-1}$ is a complete metric space (w.r.t $\arccos \lan \cdot,\cdot\ran$), there exists a open ball centered at $\frac{x_+}{\|x_+\|}$  of some radius $\arccos(\sqrt{1-\zeta_1})$ that is a subset of $\mathcal{S}_+$, from which one can show $\mathcal{S}_{x_+}^{\zeta_1}\subset \mathcal{S}_+$. The other statement $\mathcal{S}_{x_-}^{\zeta_1}\subset \mathcal{S}_-$ simply comes from the fact that $x_+=-x_-$ and $\mathrm{Int}(\mathcal{S}_+)=-\mathrm{Int}(\mathcal{S}_-)$.
\end{proof}

\begin{lemma}\label{lem_app_gamma}
    $\exists \xi>0$, such that
    \be\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_+\cap\mathbb{S}^{D-1})^c\cap (\mathcal{S}_-\cap\mathbb{S}^{D-1})^c}|\cos(x_1,x_2)|\leq \sqrt{1-\xi}\,.\ee
    ($S^c$ here is defined to be $\mathbb{S}^{D-1}-S$, the set complement w.r.t. complete space $\mathbb{S}^{D-1}$)
\end{lemma}
\begin{proof}
    Notice that
    \ben
        \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\lan x_1,x_2\ran=\inf_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\arccos \lan x_1,x_2\ran\,.
    \een
    Since $\mathbb{S}^{D-1}$ is a complete metric space (w.r.t $\arccos \lan \cdot,\cdot\ran$) and $K\cap \mathbb{S}^{D-1}$ and $x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c$ are two of its compact subsets. Suppose 
    \ben
        \inf_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\arccos \lan x_1,x_2\ran=0\,,
    \een
    then $\exists x_1\in K\cap \mathbb{S}^{D-1}, x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c$ such that $\arccos \lan x_1,x_2\ran=0$, i.e., $x_1=x_2$, which contradicts the fact that $ K\cap \mathbb{S}^{D-1}\subseteq \mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1})$ (Lemma \ref{lem_app_int}). Therefore, we have the infimum strictly larger than zero, then
    \be
        \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (S_+\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran\leq \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\lan x_1,x_2\ran<1\,.
    \ee
    Similarly, one can show that
    \be
        \sup_{x_1\in -K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_-\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran<1\,.
    \ee
    Finally, find $\xi<1$ such that
    \ben
        \max\lb\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_+\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran,\sup_{x_1\in -K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_-\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran\rb=\sqrt{1-\xi}\,,
    \een
    then for any $x_1\in K\cap\mathbb{S}^{D-1}$ and $x_2\in (\mathcal{S}_+\cap\mathbb{S}^{D-1})^c\cap (\mathcal{S}_-\cap\mathbb{S}^{D-1})^c$, we have
    \ben
        -\sqrt{1-\xi}\leq \lan x_1,x_2\ran \leq \sqrt{1-\xi}\,,
    \een
    which is the desired result.
\end{proof}


The remaining two lemmas are technical but extensively used in the main proof.
\begin{lemma}\label{lem_app_psi_rj}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $x_r\in \mathbb{S}^{n-1}$ be some reference direction, we define
    \be
        \psi_{rj}=\lan x_r,\frac{w_j}{\|w_j\|}\ran,\ \psi_{ra}=\lan x_r, \frac{x_a(w_j)}{\|x_a(w_j)\|}\ran,\ \psi_{aj}=\lan \frac{w_j}{\|w_j\|}, \frac{x_a(w_j)}{\|x_a(w_j)\|}\ran\,,
    \ee
    where $x_a(w_j)=\sum_{i: \lan x_i,w_j\ran> 0}y_ix_i$.
    
    Whenever $\max_i|f(x_i;W,v)|\leq 1$, we have
    \be
        \lvt\frac{d}{dt} \psi_{rj} -\sign(v_j(0))\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \ee
\end{lemma}
\begin{proof}
    A simple application of Lemma \ref{lem_dir_flow_approx}, together with Cauchy-Schwartz:
    \begin{align*}
        &\;\lvt\frac{d}{dt} \psi_{rj} -\sign(v_j(0))\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\\
        &=\; \lvt x_r^\top \lp\frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0)) \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran> 0}y_ix_i\rp\rp\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \end{align*}
\end{proof}
\begin{lemma}\label{lem_app_x_a_lb}
    \be
        \|x_a(w)\|\geq \sqrt{\mu}n_a(w) X_{\min}\,,
    \ee
    where $n_a(w)=|\{i\in[n]: \lan x_i,w\ran>0\}|$.
\end{lemma}
\begin{proof}
    Let $\mathcal{I}_a(w)$ denote $\{i\in[n]: \lan x_i,w\ran>0\}$, then
    \begin{align*}
        \|x_a(w)\|= \lV\sum_{i:\lan x_i,w\ran>0} x_iy_i\rV
        &=\; \sqrt{\sum_{i\in \mathcal{I}_a(w)}\|x_i\|^2y_i^2+\sum_{i,j\in \mathcal{I}_a(w), i<j}\|x_i\|\|x_j\|\lan \frac{x_iy_i}{\|x_i\|},\frac{x_jy_j}{\|x_j\|}\ran }\\
        &\geq\; \sqrt{\sum_{i\in \mathcal{I}_a(w)}\|x_i\|^2y_i^2+\sum_{i,j\in \mathcal{I}_a(w), i<j}\|x_i\|\|x_j\||y_i||y_j|\mu }\\
        &\geq\; \sqrt{n_a(w)X_{\min}^2+\mu n_a(w)\lp n_a(w)-1\rp X_{\min}^2}\\
        &\geq\; \sqrt{n_a(w)(1+\mu (n_a(w)-1))} X_{\min}\\
        &\geq\; \sqrt{\mu}n_a(w)X_{\min}\,.
    \end{align*}
\end{proof}
% \begin{lemma}
%     Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Whenever $\max_i|f(x_i;W,v)|\leq \frac{\mu^{\frac{3}{2}}X_{\min}}{4nX_{\max}}$, we have
%     \begin{align}
%     &\;\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}>0,\forall i\in [n], \text{if } j\in \mathcal{V}_+\,,\\
%     &\;\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}<0,\forall i\in [n], \text{if } j\in \mathcal{V}_-\,.
% \end{align}
% \end{lemma}
\subsection{Proof for early alignment phase}
\begin{proof}[Proof of Theorem \ref{thm_conv_main}: First Part]Given some initialization in \eqref{eq_init}, by Assumption \ref{assump_non_degen}, $\exists \zeta_2>0$, such that
\be
    \max_{j\in\mathcal{V}_+}\cos(w_j(0),x_-)< \sqrt{1-\zeta_2},\quad \max_{j\in\mathcal{V}_-}\cos(w_j(0),x_+)< \sqrt{1-\zeta_2}\,.\label{eq_app_assump_non_degen}
\ee
We define $\zeta:=\max\{\zeta_1,\zeta_2\}$, where $\zeta_1$ is from Lemma \ref{lem_app_zeta1}. In addition, by Lemma \ref{lem_app_gamma}, $\exists \xi>0$, such that
\be\label{eq_app_ub_gamma}
\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in \mathcal{S}_-^c\cap\mathcal{S}_+^c\cap \mathbb{S}^{D-1}}|\cos(x_1,x_2)|\leq \sqrt{1-\xi}\,.
\ee

We pick a initialization scale $\epsilon$ that satisfies:
\be
    \epsilon\leq \min\lb\frac{\min\{\mu,\zeta,\xi\}\sqrt{\mu}X_{\min}}{4\sqrt{h}nX_{\max}^2W_{\max}^2}, \frac{1}{\sqrt{h}}\exp\lp -\frac{64nX_{\max}}{\min\{\zeta,\xi\} \sqrt{\mu}X_{\min}}\log n\rp\rb\leq \frac{1}{4\sqrt{h}X_{\max}W_{\max}^2}\,.\label{eq_app_ub_epsilon}
\ee
By Lemma \ref{lem_small_norm}, $\forall t\leq T=\frac{1}{4nX_{\max}}\log \frac{1}{\sqrt{h}\epsilon}$, we have 
\be
\max_i|f(x_i;W,v)|\leq \frac{\min\{\mu,\zeta,\xi\}\sqrt{\mu}X_{\min}}{4nX_{\max}}\,,\label{eq_app_ub_f}
\ee
which is the key to analyzing the alignment phase. For the sake of simplicity, we only discuss the analysis of neurons 
in $\mathcal{V}_+$ here, the proof for neurons in $\mathcal{V}_-$ is almost identical.

\textbf{Activation pattern evolution:} Pick any $w_j$ in $\mathcal{V}_+$ and pick $x_r=x_iy_i$ for some $i\in[n]$, and consider the case when $\lan w_j, x_i\ran=0$. From Lemma \ref{lem_app_psi_rj},we have
\ben
    \lvt\frac{d}{dt} \psi_{rj} -\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
$\lan w_j,x_i\ran=0$ implies $\psi_{rj}=\lan \frac{x_iy_i}{\|x_i\|},\frac{w_j}{\|w_j\|}\ran=0$, thus we have
\ben
    \lvt\frac{d}{dt} \psi_{rj}\vert_{\lan w_j,x_i\ran=0} -\psi_{ra}\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
Then whenever $w_j\notin \mathcal{S}_{\text{dead}}$, we have
\begin{align*}
    \frac{d}{dt} \psi_{rj}\vert_{\lan w_j,x_i\ran=0}&\geq\; \psi_{ra}\|x_a(w_j)\|-2nX_{\max}\max_i|f(x_i;W,v)| &\\
    &\geq\; \mu \|x_a(w_j)\| -2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_K_coherence}})\\
    &\geq \; \mu^{3/2}X_{\min}-2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_x_a_lb}})\\
    &\geq\; \mu^{3/2}X_{\min}/2>0\,. &\quad (\text{by \eqref{eq_app_ub_f}})
\end{align*}
This is precisely \eqref{eq_mono_pos} in Section \ref{ssec_pf_sketch}.

\textbf{Bound on activation transitions and duration:}  Next we show that if at time $t_0<T$, $w_j(t_0)\notin\mathcal{S}_+\cup \mathcal{S}_\text{dead}$, and the activation pattern of $w_j$ is $\one_{\lan x_i,w_j(t_0)\ran>0}$, then $\one_{\lan x_i,w_j(t_0+\Delta t))\ran>0}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$, where $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a(w_j(t_0))}$ and $n_a(w_j(t_0))$ is defined in Lemma \ref{lem_app_x_a_lb} as long as $t_0+\Delta t<T$ as well. That is, during the alignment phase $[0, T]$, $w_j$ must change its activation pattern within $\Delta t$ time. There are two cases:
\begin{itemize}[leftmargin=0.4cm]
    \item The first case is when $w_j(t_0)\in \mathcal{S}_+^c\cap \mathcal{S}_-^c\cap \mathcal{S}_\text{dead}^c$. In this case, suppose that $\one_{\lan x_i,w_j(t_0+\tau))\ran>0}= \one_{\lan x_i,w_j(t_0)\ran>0}, \forall 0\leq \tau\leq \Delta t$, i.e. $w_j$ fixes its activation during $[t_0,t_0+\Delta t]$, then we have $x_a(w_j(t_0+\tau))=x_a(w_j(t_0)),\forall 0\leq \tau\leq \Delta t$. Let us pick $x_r=x_a(w_j(t_0))$, then Lemma \ref{lem_app_psi_rj} leads to
    \ben
    \lvt\frac{d}{dt} \cos(w_j,x_a(w_j)) -\lp 1-\cos^2(w_j,x_a(w_j))\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \een
    Since $x_a(w_j)$ is fixed, we have $\forall t\in[t_0,t_0+\Delta t]$,
    \begin{align*}
        \lvt\frac{d}{dt} \cos(w_j,x_a(w_j(t_0))) -\lp 1-\cos^2(w_j,x_a(w_j(t_0)))\rp\|x_a(w_j(t_0))\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
    \end{align*}
    \begin{align*}
    \frac{d}{dt} \cos(w_j,x_a(w_j(t_0)))&\geq\; \lp 1-\cos^2(w_j,x_a(w_j(t_0)))\rp\|x_a(w_j(t_0))\|\\
    &\;\qquad\qquad -2nX_{\max}\max_i|f(x_i;W,v)| &\\
    &\geq\; \xi \|x_a(w_j(t_0))\| -2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by \eqref{eq_app_ub_gamma}})\\
    &\geq \; \xi \sqrt{\mu}n_a(w_j(t_0))X_{\min}-2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_x_a_lb}})\\
    &\geq\; \xi\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2 \,. &\quad (\text{by \eqref{eq_app_ub_f}})\\
    & \geq\; \min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,, &
\end{align*}
which implies that, by the Fundamental Theorem of Calculus,
\begin{align*}
    &\;\cos(w_j(t_0+\Delta t),x_a(w_j(t_0)))\\
    &=\;\cos(w_j(t_0),x_a(w_j(t_0)))+\int_0^{\Delta t}\frac{d}{dt} \cos(w_j(t_0+\tau),x_a(w_j(t_0)))d\tau\\
    &\geq\; \cos(w_j(t_0),x_a(w_j(t_0)))+\Delta t \cdot\min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\\
    &=\; \cos(w_j(t_0),x_a(w_j(t_0)))+2\geq 1\,,
\end{align*}
which leads to $\cos(w_j(t_0+\Delta t),x_a(w_j(t_0)))=1$. This would imply $w_j(t_0+\Delta t)\in\mathcal{S}_+$ because $x_a(w_j(t_0))\in\mathcal{S}_+$, which contradicts our original assumption that $w_j$ fixes the activation pattern. Therefore, $\exists 0<\tau_0\leq \Delta t$ such that $\one_{\lan x_i,w_j(t_0+\tau_0))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$, due to the restriction on how $w_j$ can change its activation pattern, it cannot return to its previous activation pattern, then one must have $\one_{\lan x_i,w_j(t_0+\Delta t))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$.
\item  The other case is when $w_j(t_0)\in\mathcal{S}_-$. For this case, we need first show that $w_j(t_0+\tau)\notin \mathcal{S}_{x_-}^{\zeta},\forall 0\leq \tau\leq \Delta t$, or more generally, $\mathcal{S}_{x_-}^{\zeta}$ does not contain any $w_j$ in $\mathcal{V}_+$ during $[0,T]$. To see this, let us pick $x_r=x_-$, then Lemma \ref{lem_app_psi_rj} suggests that
\ben
    \lvt\frac{d}{dt} \psi_{rj} -\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
Consider the case when $\cos(w_j,x_-)=\sqrt{1-\zeta}$, i.e. $w_j$ is at the boundary of $\mathcal{S}_{x_-}^{\zeta}$. We know that in this case, $w_j\in \mathcal{S}_{x_-}^{\zeta}\subseteq \mathcal{S}_-$ thus $x_a(w_j)=-x_-$, and
\ben
    \lvt\left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}} +\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
\een
which is
\begin{align*}
    &\;\lvt\left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}} +\zeta\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)| &\\
    \Ra&\; \left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}}\\
    &\leq\; -\zeta\|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)| &\\
     &\leq \;-\zeta\sqrt{\mu}X_{\min}+2nX_{\max}\max_i|f(x_i;W,v)|&(\text{by Lemma \ref{lem_app_x_a_lb}})\\
     &\leq \;-\zeta\sqrt{\mu}X_{\min}/2<0\,. & (\text{by \eqref{eq_app_ub_f}}) 
\end{align*}
Therefore, during $[0,T]$, neuron $w_j$ in $\mathcal{V}_+$ cannot enter $\mathcal{S}_{x_-}^{\zeta}$ if at initialization, $w_j(0)\notin \mathcal{S}_{x_-}^{\zeta}$, which is guaranteed by \eqref{eq_app_assump_non_degen}.

With the argument above, we know that $w_j(t_0+\tau)\notin\mathcal{S}_{x_-}^{\zeta}, \forall 0\leq \tau\leq \Delta t$. Again we suppose that $w_j(t)\in \mathcal{S}_--\mathcal{S}_{x_-}^{\zeta},\forall t\in[t_0,t_0+\Delta t]$, i.e.,$w_j$ fixes its activation during $[t_0,t_0+\Delta t]$. Let us pick $x_r=x_-$, then Lemma \ref{lem_app_psi_rj} suggests that
\ben
    \lvt\frac{d}{dt} \cos(w_j,x_-) +\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
    \een
    which leads to $\forall t\in[t_0,t_0+\Delta t]$,
    \begin{align*}
        \frac{d}{dt} \cos(w_j,x_-)&\leq\; -\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)|&\\
        &\leq\; -\zeta \|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)|& (w_j\notin \mathcal{S}_{x_-}^{\zeta})\\
        &\leq\; -\zeta \sqrt{\mu}n_a(w_j(t_0))X_{\min}+2nX_{\max}\max_i|f(x_i;W,v)|& (\text{by Lemma \ref{lem_app_x_a_lb}})\\
        &\leq\; -\zeta \sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,. & (\text{by \eqref{eq_app_ub_f}})\\
        & \leq\; -\min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,, &
    \end{align*}
    Similarly, by FTC, we have
    \ben
        \cos(w_j(t_0+\Delta t),x_-)\leq -1\,.
    \een
    This would imply $w_j(t_0+\Delta t)\in\mathcal{S}_+$ because $-x_-=x_a(w_j(t_0))\in\mathcal{S}_+$, which contradicts our original assumption that $w_j$ fixes its activation pattern. Therefore, one must have $\one_{\lan x_i,w_j(t_0+\Delta t))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$.
\end{itemize}
In summary, we have shown that, during $[0,T]$, a neuron in $\mathcal{V}_+$ can not keep a fixed activation pattern for a time longer than $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a}$, where $n_a$ is the number of data points that activate $w_j$ under the fixed activation pattern.

\textbf{Bound on total travel time until directional convergence} As we have discussed in Section \ref{ssec_pf_sketch} and also formally proved here, during alignment phase $[0,T]$, a neuron in $\mathcal{V}_+$ must change its activation pattern within $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a}$ time unless it is in either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$. And the new activation it is transitioning into must contain no new activation on negative data points and must keep all existing activation on positive data points, together it shows that a neuron must reach either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$ within a fixed amount of time, which is the remaining thing we need to formally show here.

For simplicity of the argument, we first assume $T=\infty$, i.e., the alignment phase lasts indefinitely, and we show that a neuron in $\mathcal{V}_+$ must reach $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$ before $t_1=\frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}$. Lastly, such directional convergence can be achieved if $t_1\leq T$, which is guaranteed by our choice of $\epsilon$ in \eqref{eq_app_ub_epsilon}. 

\begin{itemize}[leftmargin=0.3cm]
    \item For a neuron in $\mathcal{V}_+$ that reaches $\mathcal{S}_\text{dead}$, the analysis is easy: It must start with no activation on positive data and then lose activation on negative data one by one until losing all of its activation. Therefore, it must reach $\mathcal{S}_\text{dead}$ before
    \ben
        \sum_{k=1}^{n_a(w_j(0))}\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}k}\leq \frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp\sum_{k=1}^n\frac{1}{k}\rp\leq \frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}=t_1\,.
    \een
    \item For a neuron in $\mathcal{V}_+$ that reaches $\mathcal{S}_+$, there is no difference conceptually, but it can switch its activation pattern in many ways before reaching $\mathcal{S}_+$, so it is not straightforward to see its travel time until $\mathcal{S}_+$ is upper bounded by $t_1$.

    To formally show the upper bound on the travel time, we need some definition of a path that keeps a record of the activation patterns of a neuron $w_j(t)$ before it reaches $\mathcal{S}_+$.

    Let $n_+=|\mathcal{I}_+|$, $n_-=|\mathcal{I}_-|$ be the number of positive, negative data respectively, then we call $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ a \emph{path} of length-$L$, if
    \begin{enumerate}[leftmargin=0.5cm]
        \item $\forall 0\leq l\leq L$, we have $k^{(l)}=(k_+^{(l)},k_-^{(l)})\in \mathbb{N}\times\mathbb{N}$ with $0\leq k_+^{(l)}\leq n_+$, $0\leq k_-^{(l)}\leq n_-$;
        \item For $k^{(l_1)},k^{(l_2)}$ with $l_1< l_2$, we have either $k_+^{(l_1)}>k_+^{(l_2)}$ or $k_-^{(l_1)}<k_-^{(l_2)}$;
        \item $k^{(L)}=(n_+,0)$;
        \item $k^{(l)}\neq (0,0),\forall 0\leq l\leq L$.
    \end{enumerate}
    % Figure environment removed

    Given all our analysis on how a neuron $w_j(t)$ can switch its activation pattern in previous parts, we know that for any $w_j(t)$ that reaches $\mathcal{S}_+$, there is an associated $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ that keeps an ordered record of encountered values of
    \ben
        \lp|\{i\in\mathcal{I}_+:\lan x_i,w_j(t)\ran>0\}|,\ |\{i\in\mathcal{I}_-:\lan x_i,w_j(t)\ran>0\}|\rp\,,
    \een
    before $w_j$ reaches $\mathcal{S}_+$. That is, a neuron $w_j$ starts with some activation pattern that activates $k_+(0)$ positive data and $k_-(0)$ negative data, then switch its activation pattern (by either losing negative data or gaining positive data) to one that activates $k_+(1)$ positive data and $k_-(1)$ negative data. By keep doing so, it reaches $\mathcal{S}_+$ that activates $k_+(L)=n_+$ positive data and $k_-(L)=0$ negative data. Please see Figure \ref{fig_path} for an illustration of a path.

    Given a path $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ of neuron $w_j$, we define the \emph{travel time} of this path as
    \ben
        T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})=\sum_{l=0}^{L-1}\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}(k_+^{(l)}+k_-^{(l)})}\,,
    \een
    which is exactly the traveling time from $k^{(0)}$ to $k^{(L)}$ if one spends $\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}(k_+^{(l)}+k_-^{(l)})}$ on the edge between $k^{(l)}$ and  $k^{(l+1)}$.
    
    Our analysis shows that if $w_j$ reaches $\mathcal{S}_+$, then
    \ben
        \inf\{t:w_j(t)\in\mathcal{S}_+\}\leq  T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\,.
    \een

    Now we define the maximal path $\mathcal{P}_{\max}$ as a path that has the maximum length $n=n_++n_-$, which is uniquely determined by the following trajectory of $k^{(l)}$
    \ben
        (0,n_-),(0,n_--1),(0,n_--2),\cdots,(0,1),(1,1),(1,0),\cdots,(n_+-1,0),(n_+,0)\,.
    \een
    Please see Figure \ref{fig_path_max_path} for an illustration.

    The traveling time for $\mathcal{P}_{\max}$ is
    \begin{align*}
         T(\mathcal{P}_{\max})&=\;\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp\sum_{k=1}^{n_-}\frac{1}{k}+\frac{1}{2}+\sum_{k=1}^{n_+-1}\frac{1}{k}\rp\\
         &\leq\; \frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp 2\sum_{k=1}^{n}\frac{1}{k}+\frac{1}{2}\rp\\
         &\leq\; \frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}=t_1\,.
    \end{align*}
    The proof is complete by the fact that any path satisfies
    \ben
        T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\leq T(\mathcal{P}_{\max})\,.
    \een
    This is because there is a one-to-one correspondence between the edges $(k^{(l)}, k^{(l+1)})$ in $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ and a subset of edges in $\mathcal{P}_{\max}$, and the travel time from of edge $(k^{(l)}, k^{(l+1)})$ is shorter than the corresponding edge in $\mathcal{P}_{\max}$. Formally stating such correspondence is tedious and a visual illustration in Figure \ref{fig_comp_path_h} and \ref{fig_comp_path_v} is more effective (Putting all correspondence makes a clustered plot thus we split them into two figures):
    % Figure environment removed
    
    Therefore, if $w_j$ reaches $\mathcal{S}_+$, then it reaches $\mathcal{S}_+$ within $t_1$:
    \ben
        \inf\{t:w_j(t)\in\mathcal{S}_+\}\leq  T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\leq T(\mathcal{P}_{\max})\leq t_1\,.
    \een
    
\end{itemize}

So far we have shown when the alignment phase lasts long enough, i.e., $T$ large enough, the directional convergence is achieved by $t_1$. We simply pick $\epsilon$ such that
\ben
    T=\frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}\geq t_1=\frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\,,
\een
and \eqref{eq_app_ub_epsilon} suffices.
\end{proof}

\newpage
\section{Proof for Theorem \ref{thm_conv_main}: Final Convergence}\label{app_pf_thm_conv}
Since we have proved the first part of Theorem \ref{thm_conv_main} in Section \ref{app_pf_thm_align}, we will use it as a fact, then prove the remaining part of Theorem \ref{thm_conv_main}. 
\subsection{Auxiliary lemmas}
First, we show that $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$ are trapping regions.

\begin{lemma}\label{lem_app_trapping}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf}, we have the following:
    \begin{itemize}[leftmargin=0.3cm]
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_\text{dead}$, then $w_j(t_1+\tau)\in\mathcal{S}_\text{dead},\ \forall \tau\geq 0$;
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_+$ for some $j\in\mathcal{V}_+$, then $w_j(t_1+\tau)\in\mathcal{S}_+,\ \forall \tau\geq 0$;
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_-$ for some $j\in\mathcal{V}_-$, then $w_j(t_1+\tau)\in\mathcal{S}_-,\ \forall \tau\geq 0$;
    \end{itemize}
\end{lemma}
\begin{proof}
    The first statement is simple, if $w_j\in\mathcal{S}_\text{dead}$, then one have $\dot{w}_j=0$, thus $w_j$ remains in $\mathcal{S}_\text{dead}$.

    For the second statement, we have, since $j\in\mathcal{V}_+$,
    \ben
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|\,.
    \een
    When $\ell$ is the exponential loss, by the Fundamental Theorem of Calculus, one writes, $\forall \tau\geq 0$,
    \begin{align*}
        w_j(t_1+\tau)&=\;w_j(t_1)+\int_0^\tau \frac{d}{dt}w_jd\tau\\
        &=\;w_j(t_1)+\int_0^\tau -\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|d\tau\\
        &=\;w_j(t_1)+\int_0^\tau \sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}y_i\exp(-y_if(x_i;W,v))x_i\|w_j\|d\tau\\
        &=\;w_j(t_1)+\underbrace{\sum_{i\in\mathcal{I}_+}\lp\int_0^\tau \exp(-y_if(x_i;W,v))\|w_j\|d\tau \rp x_i}_{:=\tilde{x}_+}\,.
    \end{align*}
    Here $w_j(t_1)\in\mathcal{S}_+$ by our assumption, $\tilde{x}_+\in K\subseteq \mathcal{S}_+$ because $\tilde{x}_+$ is a conical combination of $x_i,i\in\mathcal{I}_+$. Since $\mathcal{S}_+$ is a convex cone, we have $w_j(t_1+\tau)\in\mathcal{S}_+$ as well.

    When $\ell$ is the logistic loss, we have, similarly,
    \begin{align*}
        w_j(t_1+\tau)&=\;w_j(t_1)+\int_0^\tau \sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}y_i\frac{2\exp(-y_if(x_i;W,v))}{1+\exp(-y_if(x_i;W,v))}x_i\|w_j\|d\tau\\
        &=\;w_j(t_1)+\underbrace{\sum_{i\in\mathcal{I}_+}\lp\int_0^\tau \frac{2\exp(-y_if(x_i;W,v))}{1+\exp(-y_if(x_i;W,v))}\|w_j\|d\tau \rp x_i}_{:=\tilde{x}_+}\in \mathcal{S}_+\,.
    \end{align*}
    
    The proof of the third statement is almost identical (we only show the case of exponential loss here): when $j\in\mathcal{V}_-$, we have
    \ben
        \frac{d}{dt}w_j=\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|\,,
    \een
    and 
    \ben
        w_j(t_1+\tau)= w_j(t_1)+\underbrace{\sum_{i\in\mathcal{I}_-}\lp\int_0^\tau \exp(-y_if(x_i;W,v))\|w_j\|d\tau \rp x_i}_{:=\tilde{x}_-}.
    \een
    Again, here $w_j(t_1)\in\mathcal{S}_-$ by our assumption, $\tilde{x}_-\in -K\subseteq \mathcal{S}_-$ because $\tilde{x}_-$ is a conical combination of $x_i,i\in\mathcal{I}_-$. Since $\mathcal{S}_-$ is a convex cone, we have $w_j(t_1+\tau)\in\mathcal{S}_+$ as well.
    \end{proof}

    Then the following Lemma provides a lower bound on neuron norms upon $t_1$.
    \begin{lemma}\label{lem_app_norm_lb_t1}
        Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $t_1$ be the time when directional convergence is achieved, as defined in Theorem \ref{thm_conv_main}, and we define $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. If both $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$ are non-empty, we have 
        \ben
            \sum_{j\in \tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2, 
        \een
        \ben
            \sum_{j\in \tilde{\mathcal{V}}_-}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(0)\|^2, 
        \een
    \end{lemma}
    \begin{proof}
        We have shown that
        \ben
        \frac{d}{dt}\|w_j\|^2
        = -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|\,.
        \een
        Then before $t_1$, we have $\forall j\in[h]$
        \begin{align*}
            \frac{d}{dt}\|w_j\|^2 &=\; -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|\\
            &\geq\;-2\sum_{i=1}^n(|y_i|+2\max_i|f(x_i;W,v)|)\|x_i\|\|w_j\|^2\\
            &\geq\; -4\sum_{i=1}^n\|x_i\|\|w_j\|^2\geq -4nX_{\max}\|w_j\|^2\,,
        \end{align*}
        where the second last inequality is because $\max_i|f(x_i;W,v)|\leq \frac{1}{2}$ before $t_1$. Summing over $j\in\tilde{\mathcal{V}}_+$, we have
        \ben
            \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\geq -4nX_{\max}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\,.
        \een
        Therefore, we have the following bound:
        \ben
            \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2\,.
        \een
    \end{proof}
    Moreover, after $t_1$, the neuron norms are non-decreasing, as suggested by 
    \begin{lemma}\label{lem_app_mono_norm}
        Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $t_1$ be the time when directional convergence is achieved, as defined in Theorem \ref{thm_conv_main}, and we define $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. If both $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$ are non-empty, we have $\forall \tau\geq 0$ and $t_2\geq t_1$,
        \be
            \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2+\tau)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2)\|,\qquad \sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t_2+\tau)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t_2)\| 
        \ee
    \end{lemma} 
    \begin{proof}
        It suffices to show that after $t_1$, the following derivatives:
        \ben
            \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2, \quad \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\,,
        \een
        are non-negative. 

        For $j\in \tilde{\mathcal{V}}_+$, $w_j$ stays in $\mathcal{S}_+$ by Lemma \ref{lem_app_trapping}, and we have
        \begin{align*}
            \frac{d}{dt}\|w_j\|^2
            &=\; -2\sum_{i\in\mathcal{I}_+}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \|w_j\|\,.\\
            &=\; \begin{cases}
                2\sum_{i\in\mathcal{I}_+}y_i\exp(-y_if(x_i;W,v))\lan x_i, w_j\ran \|w_j\| & (\ell \text{ is exponential})\\
                2\sum_{i\in\mathcal{I}_+}y_i\frac{2\exp(-y_if(x_i;W,v))}{1+\exp(-y_if(x_i;W,v))}\lan x_i, w_j\ran \|w_j\| & (\ell \text{ is logistic})
            \end{cases}\\
            &\geq\; 0\,.
        \end{align*}
        Summing over $j\in \tilde{\mathcal{V}}_+$, we have $\frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2\geq 0$. Similarly one has $\frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\geq 0$.
    \end{proof}
    Finally, the following lemma is used for deriving the final convergence.
    \begin{lemma}\label{lem_app_gpl}
        Consider the following loss function
        \ben
            \mathcal{L}_{\text{lin}}(W,v)=\sum_{i=1}^n\ell\lp y_i,v^\top W^\top x_i)\rp\,,
        \een    
        if $\{x_i,y_i\},i\in[n]$ are linearly separable, i.e., $\exists \gamma>0$ and $z\in\mathbb{S}^{D-1}$ such that $y_i\lan z,x_i\ran\geq \gamma,\forall i\in[n]$, then under the gradient flow on $\mathcal{L}_{\text{lin}}(W,v)$, whenever $y_iv^\top W^\top x_i\geq 0$, $\forall i$, we have
        \be
            \dot{\mathcal{L}}_{\text{lin}}\leq -\frac{1}{4}\|v\|^2\mathcal{L}^2\gamma^2\,.
        \ee
        
    \end{lemma}
    \begin{proof}
    For $\ell$ being exponential loss, we have:
        \begin{align*}
            \dot{\mathcal{L}}=-\|\nabla_W\mathcal{L}\|^2_F-\|\nabla_v\mathcal{L}\|^2_F
            &\leq\; -\|\nabla_W\mathcal{L}\|^2_F\\
            &=\; -\lV \sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_iv^\top \rV_F^2\\
            &=\; -\|v\|^2\lV \sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_i\rV^2\\
            &\leq\; -\|v\|^2 \lvt\lan z,\sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_i\ran\rvt^2\\
            &\leq\; -\|v\|^2 \lvt\sum_{i=1}^n\ell(y_i,v^\top W^\top x_i)\gamma\rvt^2\\
            &\leq\; -\|v\|^2\mathcal{L}^2\gamma^2\leq -\frac{1}{4}\|v\|^2\mathcal{L}^2\gamma^2\,.
        \end{align*}
    For $\ell$ being logistic loss, we have:
        \begin{align*}
            \dot{\mathcal{L}}=-\|\nabla_W\mathcal{L}\|^2_F-\|\nabla_v\mathcal{L}\|^2_F
            &\leq\; -\|\nabla_W\mathcal{L}\|^2_F\\
            &=\; -\lV \sum_{i=1}^ny_i\frac{2\exp(-y_iv^\top W^\top x_i)}{1+\exp(-y_iv^\top W^\top x_i)}x_iv^\top \rV_F^2\\
            &=\; -\|v\|^2\lV \sum_{i=1}^ny_i\frac{2\exp(-y_iv^\top W^\top x_i)}{1+\exp(-y_iv^\top W^\top x_i)}x_i\rV^2\\
            &\leq\; -\|v\|^2 \lvt\lan z,\sum_{i=1}^ny_i\frac{2\exp(-y_iv^\top W^\top x_i)}{1+\exp(-y_iv^\top W^\top x_i)}x_i\ran\rvt^2\\
            &\leq\; -\|v\|^2 \lvt\sum_{i=1}^n\frac{2\exp(-y_iv^\top W^\top x_i)}{1+\exp(-y_iv^\top W^\top x_i)}\gamma\rvt^2\\
            &=\; -\|v\|^2\gamma^2\lvt\sum_{i=1}^n\frac{2\exp(-y_iv^\top W^\top x_i)}{1+\exp(-y_iv^\top W^\top x_i)}\rvt^2\\
            &\leq\; -\|v\|^2\gamma^2\lvt\sum_{i=1}^n\log(1+\exp(-y_iv^\top W^\top x_i))\rvt^2\\
            &=\; -\frac{1}{4}\|v\|^2\mathcal{L}^2\gamma^2\,,
        \end{align*}
        where the last inequality uses the fact that $2\frac{z}{1+z}\geq \log(1+z)$ when $z\in[0,1]$.
    \end{proof}


\subsection{Proof of final convergence}

\begin{proof}[Proof of Theorem \ref{thm_conv_main}: Second Part]By Lemma \ref{lem_app_trapping}, we know that after $t_1$, neurons in $\mathcal{S}_+$ ($\mathcal{S}_-$) stays in $\mathcal{S}_+$ ($\mathcal{S}_-$). Thus the loss can be decomposed as
\be\label{eq_app_L_decouple}
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_-}v_j\lan w_j,x_i\ran\rp}_{\mathcal{L}_-}\,,
\ee
where $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. Therefore, the training after $t_1$ is decoupled into 1) using neurons in $\tilde{\mathcal{V}}_+$ to fit positive data in $\mathcal{I}_+$ and 2) using neurons in $\tilde{\mathcal{V}}_-$ to fit positive data in $\mathcal{I}_-$. 

We define $f_+(x_i;W,v)=\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran$ and let $t_2^+=\inf\{t: \max_{i\in\mathcal{I}_+}|f_+(x_i;W,v)|>\frac{1}{4}\}$. Similarly, we also define $f_-(x_i;W,v)=\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran$ and let $t_2^-=\inf\{t: \max_{i\in\mathcal{I}_-}|f_-(x_i;W,v)|>\frac{1}{4}\}$. Then $t_1\leq \min\{t_2^+,t_2^-\}$, by Lemma \ref{lem_small_norm}. 

\textbf{$\mathcal{O}\lp 1/t\rp$ convergence after $t_2$}: We first show that when both $t_2^+,t_2^-$ are finite, then it implies $\mathcal{O}(1/t)$ convergence on the loss. Then we show that they are indeed finite and $t_2:=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$.

At $t_2=\max\{t_2^+,t_2^-\}$, by definition, $\exists i_+\in\mathcal{I}_+$ such that 
\be
    \frac{1}{4}\leq f_+(x_{i_+};W,v)\leq \sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_{i_+}\ran\leq \sum_{j\in\tilde{\mathcal{V}}_+} \|w_j\|^2\|x_{i_+}\|\,,
\ee
which implies, by Lemma \ref{lem_app_mono_norm}, $\forall t\geq t_2$
\be\label{eq_app_w_p_sum_lb}
    \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2)\|^2\geq \frac{1}{4\|x_{i_+}\|}\geq \frac{1}{4X_{\max}}\,.
\ee
Similarly, we have $\forall t\geq t_2$,
\be\label{eq_app_w_m_sum_lb}
\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\geq\frac{1}{4X_{\max}}\,.
\ee
Under the gradient flow dynamics \eqref{eq_gf}, we apply Lemma \ref{lem_app_gpl} to the decomposed loss \eqref{eq_app_L_decouple}
\begin{align*}
    4\dot{\mathcal{L}}&\leq\;-\lp\sum_{j\in\tilde{\mathcal{V}}_+}v_j^2 \rp\cdot \mathcal{L}_+^2\cdot (\mu X_{\min})^2-\lp\sum_{j\in\tilde{\mathcal{V}}_+}v_j^2 \rp\cdot \mathcal{L}_-^2\cdot (\mu X_{\min})^2\,.
\end{align*}
Here, we can pick the same $\gamma=\mu X_{\min}$ for both $\mathcal{L}_+$ and $\mathcal{L}_-$ because $\{x_i,y_i\},i\in\mathcal{I}_+$ is linearly separable with $z=\frac{y_1x_1}{\|x_1\|}$: $\lan z,x_iy_i\ran\geq \mu\|x_i\|\geq \mu X_{\min}$ by Assumption \ref{assump_data}. And similarly, $\{x_i,y_i\},i\in\mathcal{I}_-$ is linearly separable with $\lan z,x_iy_i\ran\geq \mu\|x_i\|\geq \mu X_{\min}$. Replace $v_i^2$ by $\|w_j\|^2$ from balancedness, together with \eqref{eq_app_w_p_sum_lb}\eqref{eq_app_w_m_sum_lb}, we have
\begin{align*}
    4\dot{\mathcal{L}}&\leq\;-\lp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2 \rp\cdot \mathcal{L}_+^2\cdot (\mu X_{\min})^2-\lp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2 \rp\cdot \mathcal{L}_-^2\cdot (\mu X_{\min})^2\\
    &\leq\; -\frac{(\mu X_{\min})^2}{4X_{\max}} (\mathcal{L}_+^2+\mathcal{L}_-^2)\leq -\frac{(\mu X_{\min})^2}{8X_{\max}} (\mathcal{L}_++\mathcal{L}_-)^2=-\frac{(\mu X_{\min})^2}{8X_{\max}}\mathcal{L}^2\,,
\end{align*}
which is
\ben
    \frac{1}{\mathcal{L}^2} \dot{\mathcal{L}} \leq -\frac{(\mu X_{\min})^2}{32X_{\max}}\,.
\een
Integrating both side from $t_2$ to any $t\geq t_2$, we have
\ben
\left.\frac{1}{\mathcal{L}}\rvt_{t_2}^\top \leq-\frac{(\mu X_{\min})^2}{32X_{\max}}(t-t_2)\,,
\een
which leads to
\ben
    \mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1}\,, \text{ where } \alpha =\frac{(\mu X_{\min})^2}{32X_{\max}}\,.
\een
\textbf{Showing $t_2=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$}: The remaining thing is to show $t_2$ is $\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. 

Since after $t_1$, the gradient dynamics are fully decoupled into two gradient flow dynamics (on $\mathcal{L}_+$ and on $\mathcal{L}_-$), it suffices to show $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ and $t_2^-=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ separately, then combine them to show $t_2=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. The proof is almost identical for $\mathcal{L}_+$ and $\mathcal{L}_-$, thus we only prove $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ here.

Suppose 
\be\label{eq_app_t2_assump}
    t_2\geq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}+\frac{4}{\sqrt{\mu}n_+X_{\min}}\lp \log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}+4nX_{\max}t_1\rp\,,
\ee
where $n_+=|\mathcal{I}_+|$. It takes two steps to show a contradiction: First, we show that for some $t_a\geq 0$, a refined alignment $\cos(w_j(t_1+t_a),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ is achieved, and such refined alignment is maintained until 
 at least $t_2^+$: $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+t_a\leq t\leq t_2^+$. Then, keeping this refined alignment leads to a contradiction.
 \begin{itemize}[leftmargin=0.3cm]
     \item For $j\in\tilde{\mathcal{V}}_+$, we have
     \ben
        \frac{d}{dt} \frac{w_j}{\|w_j\|}= \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp\underbrace{\lp\sum_{i\in\mathcal{I}_+} -\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))x_i\rp}_{:=\tilde{x}_a}\,.
     \een
     Then 
     \begin{align*}
         \frac{d}{dt} \cos(x_+,w_j)&=\;\lp\cos(x_+,\tilde{x}_a)-\cos(x_+,w_j)\cos(\tilde{x}_a,w_j)\rp \|\tilde{x}_a\|\\
         &\geq\; \lp\cos(x_+,\tilde{x}_a)-\cos(x_+,w_j)\rp \|\tilde{x}_a\|\,.
     \end{align*}
     We can show that $\cos(x_+,\tilde{x}_a)\geq \frac{1}{3}$ and $\|\tilde{x}_a\|\geq \sqrt{\mu}n_+X_{\min}/2$ when $t_1\leq t\leq t_2^+$ (we defer the proof to the end as it breaks the flow), thus within $[t_1,t_2^+]$, we have
     \be\label{eq_app_ref_align}
        \frac{d}{dt} \cos(x_+,w_j)\geq \lp\frac{1}{3}-\cos(x_+,w_j)\rp\sqrt{\mu}n_+X_{\min}/2\,.
     \ee
     We use \eqref{eq_app_ref_align} in two ways: First, since 
     \ben
        \left.\frac{d}{dt} \cos(x_+,w_j)\right\rvert_{\cos(x_+,w_j)=\frac{1}{4}}\geq \frac{\sqrt{\mu}n_+X_{\min}}{24}>0\,,
     \een
     $\cos(x_+,w_j)\geq \frac{1}{4}$ is a trapping region for $w_j$ during $[t_1,t_2^+]$. Define $t_a:=\inf\{t\geq t_1: \min_{j\in\tilde{\mathcal{V}}_+} \cos(x_+,w_j(t))\geq \frac{1}{4}\}$, then clearly, if $t_a\leq t_2^+$, then $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+t_a\leq t\leq t_2^+$.

     Now we use \eqref{eq_app_ref_align} again to show that $t_a\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$: Suppose that $t_a\geq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$, then $\exists j^*$ such that $\cos(x_+,w_{j^*}(t))< \frac{1}{4},\forall t\in[t_1,t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}]$, and we have
    \be
        \frac{d}{dt} \cos(x_+,w_{j^*})\geq \lp\frac{1}{3}-\cos(x_+,w_j)\rp\sqrt{\mu}n_+X_{\min}/2\geq \frac{\sqrt{\mu}n_+X_{\min}}{24}\,.
     \ee
     This shows
     \ben
        \cos(x_+,w_{j^*}(t_1+1))\geq \cos(x_+,w_{j^*}(t_1))+\frac{1}{4}\geq \frac{1}{4}\,,
     \een
     which contradicts that $\cos(x_+,w_{j^*}(t))< \frac{1}{4}$. Hence we know $t_a\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$.

     In summary, we have $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}\leq t\leq t_2^+$.
     \vspace{0.3cm}
     \item Now we check the dynamics of $\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2$ during $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}\leq t\leq t_2^+$. For simplicity, we denote $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}:=t_1'$.

     For $j\in\tilde{\mathcal{V}}_+$, we have, for $t_1'\leq t\leq t_2^+$,
     \begin{align*}
            \frac{d}{dt}\|w_j\|^2 &=\; 2\sum_{i\in\mathcal{I}_+}-\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \|w_j\|&\\
            &\geq\; \sum_{i\in\mathcal{I}_+}\lan x_i, w_j\ran \|w_j\|& (\text{by }\eqref{eq_app_conv_grad_bd})\\
            &=\; \lan x_+, w_j\ran \|w_j\|&\\
            &=\; \|x_+\|\|w_j\|^2\cos(x_+,w_j)&\\
            &\geq\; \frac{1}{4}\|x_+\|\|w_j\|^2&(\text{Since } t\geq t_1') \\
            &\geq\; \frac{\sqrt{\mu}n_+X_{\min}}{4}\|w_j\|^2\,, & (\text{by Lemma \ref{lem_app_x_a_lb}})
    \end{align*}
    which leads to (summing over $j\in\tilde{\mathcal{V}}_+$)
    \ben
        \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\geq \frac{\sqrt{\mu}n_+X_{\min}}{4}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\,.
    \een
    By Gronwall's inequality, we have
    \begin{align*}
        &\;\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2^+)\|^2&\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1')\|^2&\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2& (\text{By Lemma \ref{lem_app_mono_norm}})\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \exp\lp -4nX_{\max}t_1\rp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2& (\text{By Lemma \ref{lem_app_norm_lb_t1}})\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \exp\lp -4nX_{\max}t_1\rp \epsilon^2W_{\min}^2\geq \frac{2}{\sqrt{\mu}X_{\min}}\,.& (\text{by }\eqref{eq_app_t2_assump})
    \end{align*}
    However, at $t_2^+$, we have
    \begin{align*}
        \frac{1}{4}\geq \frac{1}{n_+}\sum_{i\in\mathcal{I}_+}f_+(x_i;W,v)&=\;\frac{1}{n_+}\sum_{i\in\mathcal{I}_+}\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran&\\
        &=\; \frac{1}{n_+}\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_+\ran*\\
        &=\;\frac{1}{n_+}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\cos(w_j,x_+)\|x_+\|&\\
        &\geq\;\frac{1}{4n_+}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\|x_+\|& (\text{Since } t\geq t_1')\\
        &\geq\;\frac{1}{4}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\sqrt{\mu}X_{\min}\,, & (\text{by Lemma \ref{lem_app_x_a_lb}})
    \end{align*}
    which suggests $\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\leq \frac{1}{\sqrt{\mu}X_{\min}}$. A contradiction.
 \end{itemize}
 Therefore, we must have
 \be
    t_2^+\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}+\frac{4}{\sqrt{\mu}n_+X_{\min}}\lp \log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}+4nX_{\max}t_1\rp\,.
\ee
Since the dominant term here is $\frac{4}{\sqrt{\mu}n_+X_{\min}}\log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}$, we have $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. A similar analysis shows $t_2^-=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. Therefore $t_2=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$

\textbf{Complete the missing pieces}
 We have two claims remaining to be proved. The first is $\cos(x_+,\tilde{x}_a)\geq\frac{1}{2}$ when $t_1\leq t\leq t_2^+$. Since $x_+=\sum_{i\in\mathcal{I}_+}x_i$ and $\tilde{x}_a=\sum_{i\in\mathcal{I}_+}-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))x_i$. We simply use the fact that before $t_2^+$, we have, by Lemma \ref{assump_loss},
 \be\label{eq_app_conv_grad_bd}
    \frac{1}{2}\leq -\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))=\leq \frac{3}{2}\,,
 \ee
 to show the following
 \begin{align*}
     \cos(x_+,\tilde{x}_a)&=\;\frac{\lan x_+,\tilde{x}_a\ran}{\|x_+\|\|\tilde{x}_a\|}\\
     &=\; \frac{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}}\\
     &\geq \; \frac{\frac{1}{2}\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}}\\
     &\geq \; \frac{\frac{1}{2}\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(\frac{3}{2})^2\lan x_i,x_j\ran}}\geq \frac{1}{3}\,,
 \end{align*}
 since all $\lan x_i,x_j\ran,i,j\in\mathcal{I}_+$ are non-negative.

 The second claim is $\|\tilde{x}_a\|\geq \sqrt{\mu}n_+ X_{\min}/2$ is due to that
 \ben
    \|\tilde{x}_a\|=\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}\geq \frac{1}{2}\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}=\frac{\|x_+\|}{2}\geq \frac{\sqrt{\mu}n_+X_{\min}}{2}\,,
 \een
 where the last inequality is from Lemma \ref{lem_app_x_a_lb}.
\end{proof}
\subsection{Proof of low-rank bias}
So far we have proved the directional convergence at the early alignment phase and final $\mathcal{O}(1/t)$ convergence of the loss in the later stage. The only thing that remains to be shown is the low-rank bias. The proof is quite straightforward but we need some additional notations.

As we proved above, after $t_1$, neurons in $\mathcal{S}_+$ ($\mathcal{S}_-$) stays in $\mathcal{S}_+$ ($\mathcal{S}_-$). Thus the loss can be decomposed as
\ben
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_-}v_j\lan w_j,x_i\ran\rp}_{\mathcal{L}_-}\,,
\een
where $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. Therefore, the training after $t_1$ is decoupled into 1) using neurons in $\tilde{\mathcal{V}}_+$ to fit positive data in $\mathcal{I}_+$ and 2) using neurons in $\tilde{\mathcal{V}}_-$ to fit positive data in $\mathcal{I}_-$. We use
\ben
    W_+=[W]_{:,\tilde{\mathcal{V}}_+},\quad W_-=[W]_{:,\tilde{\mathcal{V}}_-}
\een
to denote submatrices of $W$ by picking only columns in  $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$, respectively. Similarly, we define
\ben
    v_+=[v]_{\tilde{\mathcal{V}}_+}, \quad v_-=[v]_{\tilde{\mathcal{V}}_-}
\een
for the second layer weight $v$. Lastly, we also define
\ben
    W_\text{dead}=[W]_{:,\tilde{\mathcal{V}}_\text{dead}}, v_\text{dead}=[v]_{\tilde{\mathcal{V}}_\text{dead}}\,,
\een
where $\tilde{\mathcal{V}}_\text{dead}:=\{j: w_j(t_1)\in\mathcal{S}_\text{dead}\}$. Given these notations, after $t_1$ the loss is decomposed as
\ben
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,x_i^\top W_+v_+\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,x_i^\top W_-v_-\rp}_{\mathcal{L}_-}\,,
\een
and the GF on $\mathcal{L}$ is equivalent to GF on $\mathcal{L}_+$ and $\mathcal{L}_-$ separately. It suffices to study one of them. For GF on $\mathcal{L}_+$, we have the following important invariance~\cite{arora2018optimization} $\forall t\geq t_1$:
\ben
    W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)=W_+^\top(t_1)  W_+(t_1)-v_+(t_1)v_+^\top(t_1)\,,
\een
from which one has
\begin{align*}
    \|W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)\|_2&=\; \|W_+^\top(t_1)  W_+(t_1)-v_+(t_1)v_+^\top(t_1)\|_2\\
    &\leq \; \|W_+^\top(t_1)  W_+(t_1)\|_2-\|v_+(t_1)v_+^\top(t_1)\|_2\\
    &\leq\; \tr(W_+^\top(t_1)  W_+(t_1))+\|v_+(t_1)\|^2\\
    &=\;2\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\leq \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,,
\end{align*}
where the last inequality is by Lemma \ref{lem_small_norm}. Then one can immediately get
\ben
    \|v_+(t)v_+^\top (t)\|_2-\|W_+^\top(t) W_+(t)\|_2\leq\|W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)\|_2\leq  \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,,
\een
which is precisely
\be\label{eq_app_lrank_1}
    \|W_+(t)\|_F^2\leq \|W_+(t)\|_2^2+\frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,.
\ee
Similarly, we have
\be\label{eq_app_lrank_2}
    \|W_-(t)\|_F^2\leq \|W_-(t)\|_2^2+\frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_-|\,.
\ee
Lastly, one has
\be\label{eq_app_lrank_3}
\|W_{\text{dead}}\|^2_F=\sum_{j\in\tilde{\mathcal{V}}_{\text{dead}}}\|w_j(t_1)\|^2\leq \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_\text{dead}|
\ee
Adding \eqref{eq_app_lrank_1}\eqref{eq_app_lrank_2}\eqref{eq_app_lrank_3} together, we have
\begin{align*}
    \|W(t)\|^2_F&=\;\|W_+(t)\|_F^2+\|W_-(t)\|_F^2+\|W_{\text{dead}}\|^2_F\\
    &\leq\; \|W_+(t)\|_2^2+\|W_-(t)\|_2^2+\frac{4\sqrt{h}\epsilon W_{\max}^2}{\sqrt{h}}\leq 2\|W(t)\|_2^2+4\sqrt{h}\epsilon W_{\max}^2\,.
\end{align*}
Finally, since we have shown $\mathcal{L}\ra 0$ as $t\ra \infty$, then $\forall i\in[n]$, we have $\ell(y_i,f(x_i;W,v))\ra 0$. This implies
\ben
    f(x_i;W,v)=-\frac{1}{y_i}\log \ell(y_i,f(x_i;W,v))\ra \infty\,.
\een
Because we have shown that 
\ben
    f(x_i;W,v)\leq \sum_{j\in[h]}\|w_j\|^2\|x_i\|\leq \|W\|_F^2X_{\max}\,,
\een
$f(x_i;W,v)\ra \infty$ enforces $\|W\|_F^2\ra \infty$ as $t\ra \infty$, thus $\|W\|_2^2\ra \infty$ as well. This gets us
\ben
    \lim\sup_{t\ra \infty}\frac{\|W\|_F^2}{\|W\|_2^2}=2\,.
\een
\newpage
\section{Existence of Caratheodory Solution under Fixed Subgradient $\sigma'(x)=\one_{x>0}$}\label{app_cara}
In this Appendix, we first introduce the notion of solution we are interested in for the GF \eqref{eq_gf}: Caratheodory solutions that satisfy \eqref{eq_gf} for almost all time $t$. Next, in Appendix \ref{app_sec_exist_cara}, we show that if we fix the ReLU subgradient as $\sigma'(x)=\one_{x>0}$, then global Caratheodory solutions exists for \eqref{eq_gf} under Assumption \ref{assump_data}. Finally, we use simple examples to illustrate two points: 1)  Caratheodory solutions cease to exist when ReLU subgradient at zero is chosen to be a fixed non-zero value, highlighting the importance of choosing the right subgradient for analysis; 2) Caratheodory solutions are potentially non-unique, the neurons' dynamical behavior could become somewhat irregular if certain solutions are not excluded, justifying the introduction of regular solutions (Definition \ref{def_reg}).
\subsection{Caratheodory Solutions}
Given an differential equation \be\dot{\theta}=F(\theta),\theta(0)=\theta_0\,,\label{app_eq_ode_disc}\ee with $F$ potentially be discontinuous, $\theta(t)$ is said to be a Caratheodory solution of \eqref{app_eq_ode_disc} if it satisfies the following integral equation
\be
    \theta(t)=\theta_0+\int_0^tF(\theta(\tau))d\tau\,,\label{app_eq_cara_int}
\ee
for all $t\in[0,a)$, where $a\in \mathbb{R}_{\geq 0}\cup \infty$. In this section, we are interested in global Caratheodory solutions: $\theta(t)$ that satisfies \eqref{app_eq_cara_int} for all time $t\geq 0$.
\subsection{Proof of existence of Regular Caratheodory solutions under Assumption \ref{assump_data}}\label{app_sec_exist_cara}
In this section, we show the existence of global regular (Definition \ref{def_reg}) Caratheodory solutions to $\dot{\theta}=F(\theta),\theta(0)=\theta_0$, where $\theta:=\{W,v\}$ and $F:=\nabla_{W,v}\mathcal{L}$ defined from a fixed choice of ReLU subgradient $\sigma'(x)=\one_{x>0}$, under Assumption \ref{assump_data}. For the sake of a clear presentation, we first discuss the case of $\mathcal{S}_{\mathrm{dead}}=\emptyset$, where all solutions are regular. then discuss the modifications one needs to make when $\mathcal{S}_{\mathrm{dead}}\neq \emptyset$.

\textbf{Existence of Caratheodory solutions when $\mathcal{S}_{\mathrm{dead}}=\emptyset$}: First of all, notice that $\nabla_{W,v}\mathcal{L}$ is continuous almost everywhere except for a zero measure set $\mathcal{A}=\{W,v: \exists i\in[n],j\in[h]\ s.t. \lan x_i,w_j\ran=0\}$, since discontinuity only happens when one has to evaluate $\sigma'(\lan x_i,w_j\ran)$ at $\lan x_i,w_j\ran=0$ for some $i,j$. Being a finite union of hyperplanes, $\mathcal{A}$ has zero measure.

For points outside $\mathcal{A}$, the existence of a local solution is guaranteed by the generalized Caratheodory existence theorem in~\citet{persson1975} (We refer readers to Appendix \ref{app_sec_cara_pf} for the construction of such a local solution). The local solution can be extended to a global solution, as long as it does not encounter any point in $\mathcal{A}$ (the set where the flow is discontinuous). Whenever a point in $\mathcal{A}$ is reached, one requires extra certificates to extend the solution beyond that point.
% Figure environment removed
    Simply speaking, the existence of a local solution around a point in $\mathcal{A}$ requires that the flow around this point does not push trajectories towards $\mathcal{A}$ from both sides of the zero measure set, causing an infinite number of crossings of $\mathcal{A}$, called Zeno behavior~\citep{schaft2000,maennel2018gradient}. See Figure \ref{fig_cara_disc_non_exist} and \ref{fig_cara_disc_exist} for an illustration. In Appendix \ref{app_sec_cara_pf}, we formally show that if there is no Zeno behavior, then a solution can be extended until reaching discontinuity in $\mathcal{A}$, and gets extended by leaving $\mathcal{A}$ immediately.\footnote{ Strictly speaking, Appendix \ref{app_sec_cara_pf} is part of the proof but discussing the technical part right now disrupts the presentation.}

One sufficient condition for avoiding Zeno behavior is to show: For each hyperplane $\mathcal{A}_{ij}:=\{\lan x_i,w_j\ran=0\}$, all points in a neighborhood around this hyperplane $\mathcal{A}_{ij}$ must satisfy that the inner products between the normal vector of $\mathcal{A}_{ij}$ and the flow $F$ have the same sign. Formally speaking, we need that there exists $\delta>0$, such that for all pair of $\theta_k,\theta_l\in \{\theta=(W,v): 0<|\lan x_i,w_j\ran|<\delta\}$, we have $\lan\mathcal{N}_{\mathcal{A}_{ij}}, F(\theta_k)\ran\lan\mathcal{N}_{\mathcal{A}_{ij}}, F(\theta_l)\ran>0$, here $\mathcal{N}_{\mathcal{A}_{ij}}$ should be a fixed choice of the normal vector of hyperplane $\mathcal{A}_{ij}$.

This inner product $\lan\mathcal{N}_{\mathcal{A}_{ij}}, F(\theta_k)\ran$ between the normal vector and the flow is exactly computed as $\lan x_i,\nabla_{w_j}\mathcal{L}\ran$. Under Assumption \ref{assump_data}, we have a much stronger result than what is required in the last paragraph: we can show that on the entire parameter space, we have (shown in Appendix \ref{app_sec_cara_pf})
\be 
y_i\sign(v_j)\lan x_i,\nabla_{w_j}\mathcal{L}\ran>0\,,\label{app_eq_act_mono}
\ee
% Fortunately, under Assumption \ref{assump_data}, we can show that $\forall i\in[h]$ and $w_j\notin \mathcal{S}_{\mathrm{dead}}$, we have (shown in Appendix \ref{app_sec_cara_pf})
% \be 
% y_i\sign(v_j)\frac{d}{dt}\lan x_i,w_j\ran>0\,,\label{app_eq_act_mono}
% \ee
% suggesting that each subset (hyperplane defined by $\lan x_i,w_j\ran=0$ for some $i,j$) of $\mathcal{A}$ can only be crossed once.
As such, since $v_j(t)$ does not change sign,  Assumption \ref{assump_data} prevents Zeno behavior and ensures the existence of local solution around points in $\mathcal{A}$. 

In summary, from any initialization, the Caratheodory solution can be extended~\citep{persson1975} until the trajectory encounters points of discontinuity in $\mathcal{A}$, then the existence of a local solution is guaranteed by ensuring that the flow forces the solution to leave $\mathcal{A}$ immediately. Moreover, \eqref{app_eq_act_mono} ensures that $\mathcal{A}$ can only be crossed a finite number of times (every hyperplane can only be crossed once), after which no discontinuity is encountered and the solution can be extended to $t=\infty$. Therefore a global Caratheodory solution always exists.

\textbf{Existence of Caratheodory solutions when $\mathcal{S}_{\mathrm{dead}}\neq\emptyset$}:
Notice that when $\mathcal{S}_{\mathrm{dead}}\neq\emptyset$. $\mathcal{A}$ contains boundary of $\mathcal{S}_{\mathrm{dead}}$. If the solution gets extended to $\mathcal{A}$ where one neuron lands on the boundary of $\mathcal{S}_{\mathrm{dead}}$, then this neuron stays at the boundary of $\mathcal{S}_{\mathrm{dead}}$, i.e. the solution stays at $\mathcal{A}$. Therefore, the previous argument about existence does not apply.

However, one only needs very a minor modification: If at time $t_0$, the solution enters $\mathcal{A}$ by having one neuron (say $w_j(t)$) land on the boundary of $\mathcal{S}_{\mathrm{dead}}$, set $w_j(t)\equiv w_j(t_0)$ and $v_j(t)\equiv v_j(t_0)$ for $t\geq t_0$, then exclude $\{w_j,v_j\}$ from the parameter space and continue constructing and extending local solutions for other parameters via the previous argument. This shows the existence of the Caratheodory solution under non-empty $\mathcal{S}_{\mathrm{dead}}$, and by our construction, the solution is regular.

% Figure environment removed
\subsection{Non-existence of Caratheodory solution under other fixed Subgradient}\label{app_sec_non_exist_cara}
Consider the following simple example: The training data consists of a single data point $x=[1,0]^\top$, $y=-1$, and the network consists of a single neuron $\{w,v\}$ initialized at $\{w(0)=[0,1]^\top,v(0)=1\}$. See Figure \ref{fig_cara_non_exist} for an illustration.

When the ReLU subgradient is chosen to be $\sigma'(x)=\one_{x>0}$, the Caratheodory solution $\{w(t)\equiv [0,1], v(t)\equiv 1\}$ exists, i.e. the neuron stays at the boundary of $\mathcal{S}_{\mathrm{dead}}:=\{w: \lan x,w\ran\leq 0\}$.

If the ReLU subgradient is chosen to be $\sigma'(0)=a>0$, then the Caratheodory solution ceases to exist: the neuron cannot stay at the boundary $\lan x,w\ran=0$ of $\mathcal{S}_{\mathrm{dead}}$, because the non-zero $\sigma'(0)$ pushes it towards the interior of $\mathcal{S}_{\mathrm{dead}}$. However, the neuron cannot enter the interior of $\mathcal{S}_{\mathrm{dead}}$ because the flow is all zero within the interior of $\mathcal{S}_{\mathrm{dead}}$. 

To see this formally, suppose $\{w(t)=w(0),v(t)=v(0)\}$ for $t\in[0,t_0]$ (neuron stay at $\{w(0),v(0)\}$), then by definition of Caratheodory solution, we have
\ben
    \int_0^{t_0}\nabla_{w,v}\mathcal{L}(w(0),v(0)) dt=t_0\nabla_{w,v}\mathcal{L}(w(0),v(0))=0\,,
\een
suggesting $\nabla_{w,v}\mathcal{L}(w(0),v(0))=0$, which is not true when $\sigma'(0)>0$, thus a contradiction. Now suppose $w(t_0)\in \mathrm{Int}(\mathcal{S}_{\mathrm{dead}})$ for some $t_0$, then it must be that $w(t)\in \mathrm{Int}(\mathcal{S}_{\mathrm{dead}}), \forall 0<t\leq t_0$, otherwise it leads to the same contradiction as in previous paragraph. By definition of Caratheodory solution, we have
\ben
    \int_0^{t_0}\nabla_{w,v}\mathcal{L}(w(t),v(t)) dt=w(t_0)-w(0)\,.
\een
The left-hand side is zero because $w(t)\in \mathrm{Int}(\mathcal{S}_{\mathrm{dead}})\Rightarrow \nabla_{w,v}\mathcal{L}(w(t),v(t))=0, \forall 0<t\leq t_0$. The right-hand side is non-zero because $w(t_0)\in \mathrm{Int}(\mathcal{S}_{\mathrm{dead}})$, thus a contradiction. Similarly, $w(t)$ cannot enter $\mathcal{S}_{\mathrm{dead}}^c$. Therefore, the Caratheodory solution $\{w(t),v(t)\}$ does not exist for any $t>0$.

\subsection{Non-uniqueness of Caratheodory solutions}\label{app_sec_non_unique_cara}
Consider the following simple example: The training data consists of a single data point $x=[1,0]^\top$, $y=1$, and the network consists of a single neuron $(w,v)$ initialized at $w(0)=[0,1]^\top,v(0)=1$. See Figure \ref{fig_cara_non_unique} for an illustration.

We consider the case when the ReLU subgradient is chosen to be $\sigma'(x)=\one_{x>0}$. There exists one Caratheodory solution $w(t)\equiv [0,1]^\top$, $v(t)\equiv 1$, i.e. the neuron stays at the boundary of $\mathcal{S}_{\mathrm{dead}}:=\{w: \lan x,w\ran\leq 0\}$. However, consider $\tilde{w}(t),\tilde{v}(t)$ being the solution to the following ode (the one that neuron follows once enters the positive orthant):
\be
    \dot{\tilde{w}}=y\exp(-y\tilde{v}\lan x,\tilde{w}\ran)\tilde{v}x,\  \dot{\tilde{v}}=y\exp(-y\tilde{v}\lan x,\tilde{w}\ran)\lan x,\tilde{w}\ran, \tilde{w}(0)=w(0),\tilde{v}(0)=v(0)\,.
\ee
Then for any $t_0\geq 0$, 
\ben 
w(t)=\one_{t< t_0}w(0)+\one_{t\geq t_0}\tilde{w}(t-t_0)\,,
v(t)=\one_{t< t_0}v(0)+\one_{t\geq t_0}\tilde{v}(t-t_0)
\een
is a Caratheodory solution. This example shows that the Caratheodory solution could be non-unique. 

This is somewhat troublesome for our analysis, one would like that all neurons in $\mathcal{S}_{\mathrm{dead}}$ stay within $\mathcal{S}_{\mathrm{dead}}$, but Caratheodory solutions do not have this property, and in fact, as long as the neuron is on the boundary of $\mathcal{S}_{\mathrm{dead}}$, and the flow outside $\mathcal{S}_{\mathrm{dead}}$ is pointing away from the boundary, the neuron can leave $\mathcal{S}_{\mathrm{dead}}$ at any time and it does not violate the definition of a Caratheodory solution. Therefore, for our main theorem, we added an additional regularity condition (Definition \ref{def_reg}) on the solution, forcing neurons to stay within $\mathcal{S}_{\mathrm{dead}}$.
% \enrique{Assumption 3 is artificial. I'm wondering if it is better to assume that the initial condition is not in the boundary of $\mathcal{S}_\text{dead}$. In that way, we just make a statement except for a zero-measure set. We can still have this section explaining why we have such an assumption.}
\begin{remark}
    This issue of having irregular solutions is not specific to our choice of the notion of solutions. Even if one considers more generally the Filippov solution~\citet{filippov1971existence} of the differential inclusion in \eqref{eq_gf}, the same issue of non-uniqueness persists and needs attention when analyzing neuron dynamics.
\end{remark}
\begin{remark}
    Although irregular solutions are not desired for analyzing neuron behaviors, as we see in this example, they are rare cases under very specific initialization of the neurons and thus can be avoided by randomly initializing the weights.
\end{remark}

\subsection{Constructing Global Caratheodory Solution}\label{app_sec_cara_pf}
In this section, we formally show that if there is no Zeno behavior, then a solution can be extended until reaching discontinuity in $\mathcal{A}$, and gets extended by leaving $\mathcal{A}$ immediately, leading to a construction of global Caratheodory solution. The only ingredient that is needed is the existence theorem in~\citet[Theorem 2.3]{persson1975}, showing that if $F(\theta)$ is continuous and $\forall \theta$
\be
    \|F(\theta)\|_F\leq M(1+\|\theta\|_F)\,,
\ee
for some $M>0$, then global solution of $\dot{\theta}=F(\theta)$ exists. Obviously, this result cannot be applied directly for two reasons: a) it requires continuity of the flow; b) it requires linear growth of $\|F(\theta)\|_F$ w.r.t. $\|\theta\|_F$. The key idea is constructing a local solution by restricting the flow to a neighborhood of initial conditions where a) and b) are satisfied, and then extending this solution to a global one. 

As we discussed in Appendix \ref{app_sec_exist_cara}, we can assume $\mathcal{S}_{\mathrm{dead}}=\emptyset$ without loss of generality. Moreover, it suffices to show that starting from an initialization $\theta(0)=\{W(0),v(0)\}$ outside $\mathcal{A}$\footnote{initial condition within $\mathcal{A}$ is taken care of by 2).}, we can construct either: 1) a global solution without encountering any point in $\mathcal{A}$; or 2) a local solution that lands on $\mathcal{A}$ at some $t_0$ then leave $\mathcal{A}$ immediately. Because if 2) happens, we take the end of this local solution as a new initial condition and repeat this argument. Importantly, 2) cannot happen infinitely many times because we have shown in Appendix \ref{app_sec_exist_cara} that $\mathcal{A}$ can only be crossed finitely many times, thus 1) must happen, resulting in a global solution.

\textbf{Construct local solution from initial condition}: Now given an initial condition $\theta(0)=\{W(0),v(0)\}$, define the following two sets (Notation-wise, we drop the dependency on $\{W(0),v(0)\}$ for simplicity):
\begin{align*}
    &\Theta_0:=\{\theta=(W,v): \mathcal{L}(W,v)\leq \mathcal{L}(W(0),v(0)), \sign(v_j)=\sign(v_j(0)),\forall j\in[h] \}\,,\\
    &\Theta_1:=\{\theta=(W,v): \forall i\in[n], j\in[h], \lan x_i,w_j\ran \lan x_i,w_j(0)\ran>0\}\,,
\end{align*}
$\Theta_1$ is the positive invariant set of $\{W(0),v(0)\}$: all solutions from $\{W(0),v(0)\}$ never leaves $\Theta_1$, so it suffices to study the flow within $\Theta_1$ for the existence of solutions. Moreover, $\Theta_0$ is the intersection of a closed set $\{v:\sign(v_j)=\sign(v_j(0))\}$ and the pre-image of a continuous function $\mathcal{L}$ on the range $[0,\mathcal{L}(W(0),v(0))]$ thus closed. $\Theta_2$ is the largest connected set that contains $\{W(0),v(0)\}$ without point of discontinuity.

Consider the following set
\be
    \tilde{\Theta}_{1}:=\Theta_0\cap \mathrm{cl}(\Theta_1)\,.
\ee
Then $\tilde{\Theta}_{1}$ is closed. Consider a new flow $F^{\mathrm{cl}}_{1}$ on $\tilde{\Theta}_{1}$ such that  $F^{\mathrm{cl}}_{1}=F=\nabla_{W,v}\mathcal{L}$ for all $\theta\in \mathrm{Int}(\tilde{\Theta}_{1})$, and $F^{\mathrm{cl}}_{1}(\theta)=\lim_{k\ra \infty} F(\theta_k)$ for all $\theta\in \tilde{\Theta}\setminus \mathrm{Int}(\tilde{\Theta}_{1})$, where $\theta_k\in\mathrm{Int}(\tilde{\Theta}_{1}),k=1,2,\cdots$ is a convergent sequence to $\theta$.

$F^{\mathrm{cl}}_{1}|_{\tilde{\Theta}_{1}}$ is continuous by construction, and we can show that (at the end of this section) 
\be
    \|F^{\mathrm{cl}}_{1}(\theta)\|_F\leq C\|\theta\|_F\,,\forall \theta\in\tilde{\Theta}_{1}\label{app_eq_f_cl_1_bd}\,.
\ee
By a generalized version of the Tietze extension theorem~\citep{ercan1997}, there exists continuous $\tilde{F}_1$ on the entire parameter space, such that \be F^{\mathrm{cl}}_{1}(\theta)=\tilde{F}_1(\theta),\forall\theta\in \tilde{\Theta}_{1}\,,\ee
and
\be
    \|\tilde{F}_1(\theta)\|_F\leq C\|\theta\|_F, \forall \theta\,.
\ee 
Because now $F=F^{\mathrm{cl}}_{1}=\tilde{F}_1$ on $\mathrm{Int}(\tilde{\Theta}_{1})$, any solution $\tilde{\theta}_1(t)$ of $\dot{\tilde{\theta}}_1=\tilde{F}_1(\tilde{\theta}_1), \tilde{\theta}_1(0)=\theta(0)$ (existence guaranteed by~\citet{persson1975}) gives a local solution of $\dot{\theta}=F(\theta),\theta(0)=\theta(0)$, for $t\leq t_0$, where $t_0:=\inf\{t: \tilde{\theta}_1(t)\notin \tilde{\Theta}_{1}\}$.

If $t_0=\infty$, one has a global solution and the construction is finished. If $t_0<\infty$, it must be that $\tilde{\theta}_1(t_0)\in \mathcal{A}$ (since $\tilde{\theta}_1$ must leave $\tilde{\Theta}$ via the boundary of $\Theta_2$). Now we need to construct a solution that leaves $\mathcal{A}$ immediately.

\textbf{Construct local solution that leaves $\mathcal{A}$}: As we discussed, $\mathcal{A}$ is a union of hyperplanes. For simplicity, let us assume $\tilde{\theta}(t_0)$ is not at the intersection of two hyperplanes (the treatment is similar but tedious, we will make remarks in the end). 

Now $\tilde{\theta}(t_0)$ lands on a single hyperplane, let it be $\{\theta: \lan x_{i^*},w_{j^*}\ran=0\}$, we define
\begin{align*}
    \Theta_2:=\{\theta=(W,v):&\;\forall i\neq i^*, j\neq j^*, \lan x_i,w_j\ran \lan x_i,w_j(0)\ran>0\,,\\
    &\; \qquad\qquad \text{and }\lan x_{i^*},w_{j^*}\ran \lan x_{i^*},w_{j^*}(0)\ran<0\}\,,    
\end{align*}
and we let
\be
    \tilde{\Theta}_2:=\Theta_0\cap \mathrm{cl}(\Theta_2)\,,
\ee
It is clear that, from the definition of $\Theta_2$, any solution we construct that leaves $\mathcal{A}$ immediately after $t_0$ must enter $\mathrm{Int}(\tilde{\Theta}_2)$. To construct the solution, we just need to repeat the first part, but now for 
$\tilde{\Theta}_2$: We construct $F^{\mathrm{cl}}_{2}$ that is continuous on $\tilde{\Theta}_2$ and agrees with $F$ on the interior, then extends $F^{\mathrm{cl}}_{2}$ to $\tilde{F}$ on the entire parameter space. Consider the solution $\tilde{\theta}_2(t)$ of
\be
    \dot{\tilde{\theta}}_2=F(\tilde{\theta}_2),\tilde{\theta}_2(0)=\tilde{\theta}_1(t_0)\,,
\ee
gives a local solution of
\be
    \dot{\theta}=F(\theta),\theta(0)=\tilde{\theta}_1(t_0)\,.
\ee
Because we have shown that Zeno behavior does not happen, $\tilde{\theta}_2(t)$ leaves $\mathcal{A}$ immediately and enters $\mathrm{Int}(\tilde{\Theta}_2)$. We just pick any $\tau_0>0$ such that $\tilde{\theta}_2(\tau_0)\in \mathrm{Int}(\tilde{\Theta}_2)$ then
\be
    \theta(t)=\one_{t\leq t_0}\tilde{\theta}_1(t)+\one_{t_0<t\leq t_0+\tau_0}\tilde{\theta}_2(t-t_0)\,,
\ee
is a Caratheodory solution to $\dot{\theta}=F(\theta),\theta(0)=\theta(0)$ for $t\leq t_0+\tau_0$. This is exactly what we intended to show.
\begin{remark}
    When $\tilde{\theta}(t_0)$ lands at the intersection of two (or more) hyperplanes, the only difference is that now there could be more regions to escape to. But under Assumption \ref{assump_data}, \eqref{app_eq_act_mono} suggests that the solution must cross all hyperplanes after $t_0$, leaving one unique region similar to $\Theta_2$. Then one constructs the local solution following previous procedures.
\end{remark}

\textbf{Complete the missing pieces}
To complete the proof, there are two statements (\eqref{app_eq_act_mono} and \eqref{app_eq_f_cl_1_bd}) left to be shown. 

To show \eqref{app_eq_act_mono}, we start from the derivative
\begin{align*}
    \nabla_{w_j}\mathcal{L}&=\;-\sum_{k=1}^n\one_{\lan x_k,w_j\ran>0}\nabla_{\hat{y}}\ell(y_k,f(x_k;W,v))x_k\sign(v_j(0))\|w_j\|\,,\\
    &=\;\sum_{k=1}^n\one_{\lan x_k,w_j\ran>0}y_k\exp(-y_kf(x_k;W,v))x_k\sign(v_j(0))\|w_j\|\,,
\end{align*}
and we have
\begin{align*}
    y_i\sign(v_j)\lan x_i,\nabla_{w_j}\mathcal{L}\ran&=\;\sum_{k=1}^n\one_{\lan x_k,w_j\ran>0}\exp(-y_kf(x_k;W,v))\lan y_ix_i, y_kx_k\ran \|w_j\|\\
    &\geq \;\sum_{k=1}^n\one_{\lan x_k,w_j\ran>0}\exp(-y_kf(x_k;W,v))\mu\|x_k\|\|x_i|\|w_j\|>0\,,
\end{align*}
since there is at least one summand ($\mathcal{S}_{\mathrm{dead}}=\emptyset$), the summation is always positive.

To show \eqref{app_eq_f_cl_1_bd}\footnote{We show it for exponential loss, the case of logistic loss is similar}, we first consider $\theta\in \mathrm{Int}(\tilde{\Theta}_{1})$, and we have
\begin{align*}
    \sum_{j=1}^h\|\nabla_{w_j}\mathcal{L}\|^2&=\;\sum_{j=1}^h\lV\sum_{k=1}^n\one_{\lan x_k,w_j\ran>0}y_k\exp(-y_kf(x_k;W,v))x_kv_j\rV^2\\
    &\leq\; \sum_{j=1}^h\lp\sum_{k=1}^n\exp(-y_kf(x_k;W,v))\|x_k\||v_j|\rp^2\\
    &\leq\; \sum_{j=1}^h |v_j|^2\cdot \lp X_{\mathrm{max}} \sum_{k=1}^n\exp(-y_kf(x_k;W,v))\rp^2\\
    &=\; \sum_{j=1}^h |v_j|^2\cdot \lp X_{\mathrm{max}}\mathcal{L}(W,v)\rp^2\\
    &\leq\; \sum_{j=1}^h |v_j|^2\cdot \lp X_{\mathrm{max}} \mathcal{L}(W(0),v(0))\rp^2=X^2_{\mathrm{max}}\mathcal{L}^2(W(0),v(0))\|v\|^2\,,
\end{align*}
similarly, we also have
\begin{align*}
    \sum_{j=1}^h\|\nabla_{v_j}\mathcal{L}\|^2&=\;\sum_{j=1}^h\lV\sum_{k=1}^n\one_{\lan x_k,w_j\ran>0}y_k\exp(-y_kf(x_k;W,v))\lan x_k, w_j\ran\rV^2\\
    &\leq\; \sum_{j=1}^h\lp\sum_{k=1}^n\exp(-y_kf(x_k;W,v))\|x_k\|\|w_j\|\rp^2\\
    &\leq\; \sum_{j=1}^h \|w_j\|^2\cdot \lp X_{\mathrm{max}} \sum_{k=1}^n\exp(-y_kf(x_k;W,v))\rp^2\\
    &=\; \sum_{j=1}^h \|w_j\|^2\cdot \lp X_{\mathrm{max}}\mathcal{L}(W,v)\rp^2\\
    &\leq\; \sum_{j=1}^h \|w_j\|\cdot \lp X_{\mathrm{max}} \mathcal{L}(W(0),v(0))\rp^2=X^2_{\mathrm{max}}\mathcal{L}^2(W(0),v(0))\|W\|_F^2\,.
\end{align*}
Therefore, we have $\forall \theta\in \mathrm{Int}(\tilde{\Theta}_{1})$
\begin{align*}
    \|F^{\mathrm{cl}}_{1}(\theta)\|_F^2&=\;\|F(\theta)\|_F^2=\sum_{j=1}^h(\|\nabla_{w_j}\mathcal{L}\|^2+\|\nabla_{v_j}\mathcal{L}\|^2)\\
    &\leq\; X_{\max}^2\mathcal{L}^2(W(0),v(0))(\|W\|_F^2+\|v\|^2)=X_{\max}^2\mathcal{L}^2(W(0),v(0))\|\theta\|^2_F\,,
\end{align*}
which gives \eqref{app_eq_f_cl_1_bd} with $C=X_{\max}\mathcal{L}(W(0),v(0))$. 

Then for $\theta\in \tilde{\Theta}\setminus \mathrm{Int}(\tilde{\Theta}_{1})$, $\|F^{\mathrm{cl}}_{1}(\theta)\|=\lim_{k\ra \infty} \|F(\theta_k)\|\leq C\lim_{k\ra \infty}\|\theta_k\|=C\|\theta\|$, given some Cauchy sequence $\theta_k\in\mathrm{Int}(\tilde{\Theta}_{1}),k=1,2,\cdots$ convergent to $\theta$. This finishes proving \eqref{app_eq_f_cl_1_bd}.

\newpage
\section{Extend main results to solutions to differential inclusion}\label{app_diff}

For~\cite{filippov1971existence} solutions (regular according to Definition \ref{def_reg}) to the differential inclusion~\eqref{eq_gf},  our Theorem \ref{thm_conv_main} remains the same. The only difference is that the notion of $x_a(w)$ in \eqref{eq_dir_flow} is no longer a singleton, but rather an element from a set:
\be
    x_a(w)\in\lb\sum\nolimits_{i}\sigma'(\lan x_i,w\ran)y_ix_i\rb\,,
\ee
where $\sigma'(\lan x_i,w\ran)$ is a subgradient of ReLU activation $\sigma(z)$ at $z=\lan x_i,w\ran$. Therefore, the proof of Theorem \ref{thm_conv_main} shall be modified (which can be done) to consider all possible choices of $x_a(w)$.

In the case of $\left.\sigma'(z)\rvt_{z=0}=0$, $x_a(w)$ become a singleton $\sum\nolimits_{i: \lan x_i,w\ran> 0}y_ix_i$, which simplifies our discussions. This is the main reason we opt to fix this subgradient $\sigma'(z)$ in the main paper.
