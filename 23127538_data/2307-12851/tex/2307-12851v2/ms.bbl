\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, et~al.]{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N Sainath,
  et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal processing magazine}, 29\penalty0 (6):\penalty0
  82--97, 2012.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and
  Nathan Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6152--6160, 2017.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Razin et~al.(2022)Razin, Maman, and Cohen]{razin2022implicit}
Noam Razin, Asaf Maman, and Nadav Cohen.
\newblock Implicit regularization in hierarchical tensor factorization and deep
  convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  18422--18462. PMLR, 2022.

\bibitem[Saxe et~al.(2014)Saxe, Mcclelland, and Ganguli]{saxe2014exact}
Andrew~M Saxe, James~L Mcclelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural network.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[St{\"o}ger and Soltanolkotabi(2021)]{stoger2021small}
Dominik St{\"o}ger and Mahdi Soltanolkotabi.
\newblock Small random initialization is akin to spectral learning:
  Optimization and generalization guarantees for overparameterized low-rank
  matrix reconstruction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{Gidel2019}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pages 3202--3211. Curran Associates, Inc., 2019.

\bibitem[Min et~al.(2021)Min, Tarmoun, Vidal, and Mallada]{mtvm21icml}
Hancheng Min, Salma Tarmoun, Ren\'e Vidal, and Enrique Mallada.
\newblock On the explicit role of initialization on the convergence and
  implicit bias of overparametrized linear networks.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of \emph{Proceedings of Machine Learning Research},
  pages 7760--7768. PMLR, 18--24 Jul 2021.

\bibitem[Varre et~al.(2023)Varre, Vladarean, Pillaud-Vivien, and
  Flammarion]{varre2023on}
Aditya~Vardhan Varre, Maria-Luiza Vladarean, Loucas Pillaud-Vivien, and Nicolas
  Flammarion.
\newblock On the spectral bias of two-layer linear networks.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018small}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In Sébastien Bubeck, Vianney Perchet, and Philippe Rigollet,
  editors, \emph{Proceedings of the 31st Conference On Learning Theory},
  volume~75 of \emph{Proceedings of Machine Learning Research}, pages 2--47.
  PMLR, 06--09 Jul 2018.

\bibitem[Li et~al.(2021)Li, Luo, and Lyu]{li2021towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yaras et~al.(2023)Yaras, Wang, Hu, Zhu, Balzano, and Qu]{yaras2023law}
Can Yaras, Peng Wang, Wei Hu, Zhihui Zhu, Laura Balzano, and Qing Qu.
\newblock The law of parsimony in gradient descent for learning deep linear
  networks, 2023.

\bibitem[Soltanolkotabi et~al.(2023)Soltanolkotabi, St{\"o}ger, and
  Xie]{soltanolkotabi2023implicit}
Mahdi Soltanolkotabi, Dominik St{\"o}ger, and Changzhi Xie.
\newblock Implicit balancing and regularization: Generalization and convergence
  guarantees for overparameterized asymmetric matrix sensing.
\newblock \emph{arXiv preprint arXiv:2303.14244}, 2023.

\bibitem[Maennel et~al.(2018)Maennel, Bousquet, and Gelly]{maennel2018gradient}
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly.
\newblock Gradient descent quantizes relu network features.
\newblock \emph{arXiv preprint arXiv:1803.08367}, 2018.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
\newblock Gradient descent on two-layer nets: Margin maximization and
  simplicity bias.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12978--12991, 2021.

\bibitem[Phuong and Lampert(2021)]{phuong2021inductive}
Mary Phuong and Christoph~H Lampert.
\newblock The inductive bias of relu networks on orthogonally separable data.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Boursier et~al.(2022)Boursier, Pullaud-Vivien, and
  Flammarion]{boursier2022gradient}
Etienne Boursier, Loucas Pullaud-Vivien, and Nicolas Flammarion.
\newblock Gradient flow dynamics of shallow relu networks for square loss and
  orthogonal inputs.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pages 20105--20118, 2022.

\bibitem[Bolte et~al.(2010)Bolte, Daniilidis, Ley, and
  Mazet]{bolte2010characterizations}
J{\'e}r{\^o}me Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet.
\newblock Characterizations of {\l}ojasiewicz inequalities: subgradient flows,
  talweg, convexity.
\newblock \emph{Transactions of the American Mathematical Society},
  362\penalty0 (6):\penalty0 3319--3363, 2010.

\bibitem[Reid(1971)]{reid1971ode}
W.~T. Reid.
\newblock \emph{Ordinary Differential Equations}.
\newblock Wiley, New York, 1971.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{Du&Lee}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Arora et~al.(2018{\natexlab{a}})Arora, Cohen, and
  Hazan]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{35th International Conference on Machine Learning},
  2018{\natexlab{a}}.

\bibitem[Wang and Ma(2023)]{wang2023understanding}
Mingze Wang and Chao Ma.
\newblock Understanding multi-phase optimization dynamics and rich nonlinear
  behaviors of relu networks.
\newblock \emph{arXiv preprint arXiv:2305.12467}, 2023.

\bibitem[Arora et~al.(2018{\natexlab{b}})Arora, Cohen, Golowich, and
  Hu]{arora2018convergence}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.

\bibitem[Yun et~al.(2020)Yun, Krishnan, and Mobahi]{yun2020unifying}
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ji and Telgarsky(2020)]{ji2020dir}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Le and Jegelka(2022)]{le2022training}
Thien Le and Stefanie Jegelka.
\newblock Training invariances and the low-rank phenomenon: beyond linear
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wang and Pilanci(2022)]{wang2022the}
Yifei Wang and Mert Pilanci.
\newblock The convex geometry of backpropagation: Neural network gradient flows
  converge to extreme points of the dual convex program.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wang and Ma(2022)]{wang2022early}
Mingze Wang and Chao Ma.
\newblock Early stage convergence and global convergence of training mildly
  parameterized neural networks.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Frei et~al.(2022)Frei, Vardi, Bartlett, Srebro, and
  Hu]{frei2022implicit}
Spencer Frei, Gal Vardi, Peter Bartlett, Nathan Srebro, and Wei Hu.
\newblock Implicit bias in leaky relu networks trained on high-dimensional
  data.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem[Kou et~al.(2023)Kou, Chen, and Gu]{kou2023implicit}
Yiwen Kou, Zixiang Chen, and Quanquan Gu.
\newblock Implicit bias of gradient descent for two-layer re{LU} and leaky
  re{LU} networks on nearly-orthogonal data.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem[Brutzkus et~al.(2018)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2018sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wang et~al.(2019)Wang, Giannakis, and Chen]{wang2019}
Gang Wang, Georgios~B. Giannakis, and Jie Chen.
\newblock Learning relu networks on linearly separable data: Algorithm,
  optimality, and generalization.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (9):\penalty0 2357--2370, 2019.
\newblock \doi{10.1109/TSP.2019.2904921}.

\bibitem[Persson(1975)]{persson1975}
Jan Persson.
\newblock A generalization of carathéodory's existence theorem for ordinary
  differential equations.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 49\penalty0
  (2):\penalty0 496--503, 1975.
\newblock ISSN 0022-247X.
\newblock \doi{https://doi.org/10.1016/0022-247X(75)90192-4}.

\bibitem[{van der Schaft} and Schumacher(2000)]{schaft2000}
A.J. {van der Schaft} and J.M. Schumacher.
\newblock \emph{An Introduction to Hybrid Dynamical Systems}.
\newblock Number 251 in Lecture Notes in Control and Information Sciences.
  Springer Verlag, Germany, 2000.

\bibitem[Filippov(1971)]{filippov1971existence}
A.~F. Filippov.
\newblock The existence of solutions of generalized differential equations.
\newblock \emph{Mathematical notes of the Academy of Sciences of the USSR},
  10\penalty0 (3):\penalty0 608--611, September 1971.
\newblock ISSN 1573-8876.
\newblock \doi{10.1007/BF01464722}.

\bibitem[Ercan(1997)]{ercan1997}
Zafer Ercan.
\newblock Extension and separation of vector valued functions.
\newblock \emph{Turkish Journal of Mathematics}, 21\penalty0 (4), 1997.

\end{thebibliography}
