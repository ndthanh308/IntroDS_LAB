% \section*{Correction and Clarification}
% As we are preparing the supplement material, we realize a typo in the main paper: 
% \begin{itemize}[leftmargin=0.3cm]
%     \item line 191: the correct statement should be "$\exists\alpha>0$ and $\exists t_2$ with $t_1\leq t_2=\mathcal{O} (\frac{1}{n}\log\frac{1}{\epsilon})$, such that $\mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1},\ \forall t\geq t_2$." Only the upper bound on $t_2$ is wrong, the other part of the Theorem is unaffected.
% \end{itemize}
% In addition, we clarify that although in our theorem we considered loss $\mathcal{L}=\sum_{i=1}^n\ell(x_i;W,v)$, our numerical experiments use the averaged loss $\bar{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^n\ell(x_i;W,v)$, which does not affect the dynamic behavior of GD with appropriate choice of step size.
% \newpage
\section{Additional Experiments}\label{app_addi_experiments}
\subsection{Additional experiments on MNIST dataset}
We use exactly the same experimental setting as in the main paper and only use a different pair of digits. The results are as follows:
% Figure environment removed
% Figure environment removed
\subsection{Error bars}
We provide here error bars for the numerical experiments in Section \ref{sec_num}. We run each experiment 20 times and report the mean loss curve below, the shaded region presents the values within 3 times the standard deviation.
% Figure environment removed

\newpage
\section{Proof of Lemma \ref{lem_small_norm_informal}}\label{app_pf_lem_small_norm}
The following property of the exponential loss $\ell$ will be used throughout the Appendix for proofs of several results:
    \begin{lemma}\label{assump_loss}
        For exponential loss $\ell$, we have
        % \ell$ is differentiable w.r.t. its second argument and given a label set $\mathcal{Y}\subseteq \mathbb{R}$, $\exists C,\xi$ s.t.
        \be
            |-\nabla_{\hat{y}}\ell (y,\hat{y})-y|\leq 2|\hat{y}|,\forall y\in\{+1,-1\},\quad \forall |\hat{y}|\leq 1\,.
        \ee
    \end{lemma}
    \begin{proof}
        \begin{align*}
            |-\nabla_{\hat{y}}\ell (y,\hat{y})-y|&=\;|y\exp(-y\hat{y})-y|\\
            &\leq\; |y||\exp(-y\hat{y})-1|\\
            &\leq\; |\exp(-y\hat{y})-1|\leq 2|\hat{y}|\,,
        \end{align*}
        where the last inequality is due to the fact that $2x\geq \max\{1-\exp(-x),\exp(x)-1\},\forall x\in[0,1]$.
    \end{proof}
\subsection{Formal statement}
% Denote: $M_x=\max_i\|x_i\|, M_w=\max_j\|[W_0]_{:,j}\|$.
% \begin{lemma}\label{lem_small_norm}
%     Let $\mathcal{Y}=\{y:|y|\leq 1\}$ and $C,\xi$ be the constant from Assumption \ref{assump_loss}. then given any $\delta\leq \xi$, $\epsilon\leq \frac{\delta}{(1+\delta C)hM_xM_w^2}$, any solution to the gradient flow dynamics \eqref{eq_gf} starting from initialization \eqref{eq_init} satisfies
%     \be
%         \max_j\|w_j(t)\|^2\leq \frac{\delta}{hM_x},\quad \max_i|f(x_i;W(t),v(t))|\leq \delta\,,\quad \forall t\leq \frac{1}{2M_x}\log\frac{1}{\epsilon}
%     \ee
% \end{lemma}
% \begin{lemma}
%     Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Given any $\delta\leq \xi$, whenever $\max_i|f(x_i;W(t),v(t))|\leq \delta$, we have
%     \be
%         \lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0))\lp I-\frac{w_jw_j^\top  }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran\geq 0}y_ix_i\rp\rV\leq \tilde{C}\delta\,,
%     \ee
%     for some $\tilde{C}>0$.
% \end{lemma}
Denote: $X_{\max}=\max_i\|x_i\|, W_{\max}=\max_j\|[W_0]_{:,j}\|$. The formal statement of Lemma \ref{lem_small_norm_informal} is as follow:
\begin{customlem}{1}
    Given some initialization from \eqref{eq_init}, for any $\epsilon\leq \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}$, then any solution to the gradient flow dynamics \eqref{eq_gf} satisfies that $\forall t\leq T=\frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}
    $,
    % $\max_j\|w_j(t)\|^2=\mathcal{O}(\frac{\epsilon}{\sqrt{h}})$, $\max_i|f(x_i;W(t),v(t))|=\mathcal{O}(\epsilon \sqrt{h})$ and 
    \ben
        \max_j\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}-\sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV\leq 4\epsilon n\sqrt{h}X_{\max}^2W_{\max}^2\,.
    \een
\end{customlem}

Lemma \ref{lem_small_norm_informal} is a direct result of the following two lemmas.
\begin{lemma}\label{lem_small_norm}
     Given some initialization in \eqref{eq_init}, then for any $\epsilon\leq \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}$, any solution to the gradient flow dynamics \eqref{eq_gf} satisfies
    \be
        \max_j\|w_j(t)\|^2\leq \frac{2\epsilon W_{\max}^2}{\sqrt{h}},\quad \max_i|f(x_i;W(t),v(t))|\leq 2\epsilon \sqrt{h}X_{\max}W_{\max}^2\,,
    \ee
    $\forall t\leq \frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}
    $.
\end{lemma}
\begin{lemma}\label{lem_dir_flow_approx}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Whenever $\max_i|f(x_i;W,v)|\leq 1$, we have, $\forall i\in[n]$,
    \be
        \lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0)) \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran> 0}y_ix_i\rp\rV\leq 2n X_{\max} \max_i|f(x_i;W,v)|\,.
    \ee
\end{lemma}
\subsection{Proof of Lemma \ref{lem_small_norm} and Lemma \ref{lem_dir_flow_approx}}
\begin{proof}[Proof of Lemma \ref{lem_small_norm}]
    Under gradient flow, we have
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.
    \ee
    Balanced initialization enforces $v_j=\sign(v_j(0))\|w_j\|$, hence
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
    \ee
    Let $T:=\inf\{t:\ \max_{i}|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\}$,  then $\forall t\leq T, j\in[h]$, we have 
    \begin{align}
        \frac{d}{dt}\|w_j\|^2
        &=\;\lan w_j, \frac{d}{dt}w_j \ran & \nonumber\\
        &=\; -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|&\nonumber\\
        &\leq\; 2\sum_{i=1}^n\lvt\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\rvt \lvt\lan x_i, w_j\ran\rvt \|w_j\|&\nonumber\\
        &\leq\; 2\sum_{i=1}^n(|y_i|+2|f(x_i;W,v)|) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{by Lemma \ref{assump_loss}})\nonumber\\
        &\leq\; 2\sum_{i=1}^n(1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{Since } t\leq T)\nonumber\\
        &\leq\; 2\sum_{i=1}^n(1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2) \|x_i\|\|w_j\|^2& \nonumber\\
        &\leq \; 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\|w_j\|^2\,.&
    \end{align}
    Let $\tau_j:=\inf\{t: \|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}\}$, and let $j^*:=\arg\min_j \tau_j$, then $\tau_{j^*}=\min_{j}\tau_j\leq T$ due to the fact that 
    \ben
        |f(x_{i};W,v)|= \lvt\sum_{j\in[h]}\one_{\lan w_j,x_i\ran>0}v_j\lan w_j,x_i\ran\rvt\leq \sum_{j\in[h]} \|w_j\|^2\|x_i\|\leq hX_{\max}\max_{j\in[h]}\|w_j\|^2\,,
    \een
    which implies "$|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\Rightarrow \exists j, s.t.\|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}$". 
    
    Then for $t\leq \tau_{j^*}$, we have
    \be
        \frac{d}{dt}\|w_{j^*}\|^2\leq 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\|w_{j^*}\|^2\,.
    \ee
    By Gr\"onwall's inequality, we have $\forall t\leq \tau_{j^*}$
    \begin{align*}
        \|w_{j^*}(t)\|^2&\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\|w_{j^*}(0)\|^2\,,\\
         &=\;\exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\epsilon^2\|[W_0]_{:,j^*}\|^2\\
         &\leq \;\exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\epsilon^2W_{\max}^2\,.
    \end{align*}
    Suppose $\tau_{j^*}<\frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$, then by the continuity of $\|w_{j^*}(t)\|^2$, we have 
    \begin{align*}
        \frac{2\epsilon W_{\max}^2}{\sqrt{h}}\leq \|w_{j^*}(\tau_{j^*})\|^2&\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\tau_{j^*}\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 
        \frac{1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2}{2}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 
        \log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2=\frac{\epsilon W_{\max}^2}{\sqrt{h}}\,,
    \end{align*}
    which leads to a contradiction $2\epsilon\leq \epsilon$. Therefore, one must have $T\geq \tau_{j^*}\geq \frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$. This finishes the proof.
\end{proof}
% \begin{proof}[Proof of Lemma \ref{lem_small_norm}]
%     Under gradient flow, we have
%     \be
%         \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.
%     \ee
%     Balanced initialization enforces $v_j=\sign(v_j(0))\|w_j\|$, hence
%     \be
%         \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
%     \ee
%     Let $T:=\inf\{t:\ \max_{i}|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\}$,  then $\forall t\leq T, j\in[h]$, we have 
%     \begin{align}
%         \frac{d}{dt}\|w_j\|^2
%         &=\;\lan w_j, \frac{d}{dt}w_j \ran & \nonumber\\
%         &=\; -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|&\nonumber\\
%         &\leq\; 2\sum_{i=1}^n\lvt\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\rvt \lvt\lan x_i, w_j\ran\rvt \|w_j\|&\nonumber\\
%         &\leq\; 2\sum_{i=1}^n(|y_i|+2|f(x_i;W,v)|) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{by Lemma \ref{assump_loss}})\nonumber\\
%         &\leq\; 2\sum_{i=1}^n(1+2hX_{\max} \|w_j\|^2) \|x_i\|\|w_j\|^2&\nonumber\\
%         &\leq \; 2n(X_{\max}+2hX_{\max}^2 \|w_j\|^2)\|w_j\|^2\,,&
%     \end{align}
% %     where the second last inequality uses the fact that
% %     \be
% %     f(x_{i};W,v)= \sum_{j\in[h]]}\one_{\lan w_j,x_i\ran>0}v_j\lan w_j,x_i\ran\leq \sum_{j\in[h]} \|w_j\|^2\|x_i\|\leq hX_{\max}\|w_j\|\,,
% % \ee
%     % For simplicity, we let $M_x:=\max_{i}\|x_i\|$.
    
%     Let $\tau_j:=\inf\{t: \|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}\}$, and let $j^*:=\arg\min_j \tau_j$, then $\tau_{j^*}=\min_{j}\tau_j\leq T$ due to the fact that "$|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\Rightarrow \exists j, s.t.\|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}$". 
    
%     Then for $t\leq \tau_{j^*}$, we have
%     \be
%         \frac{d}{dt}\|w_{j^*}\|^2\leq 2(X_{\max}+ChX_{\max}^2 \|w_{j^*}\|^2)\|w_{j^*}\|^2\,,
%     \ee
%     The ode $\frac{d}{dt}z=2(a+b z)z, z(t)=z_0\geq 0, a,b>0$ has solution on finite interval
%     \be
%         z(t)=\frac{a \exp(2 a t)z_0}{a+b z_0-b \exp(2 a t)z_0}\,, \forall t<\frac{1}{2a}\log\lp \frac{a}{bz_0}+1\rp\,,\label{eq_app_lem_norm_ode}
%     \ee
%     then we have
%     \begin{align}
%         \|w_{j^*}(t)\|^2&\leq\; \frac{X_{\max} \exp(2 n X_{\max} t)\|w_{j^*}(0)\|^2}{X_{\max}+2hX_{\max}^2 (1-\exp(2 nX_{\max} t)\|w_{j^*}(0)\|^2}\nonumber\\
%         &=\; \frac{\exp(2 nX_{\max} t)\epsilon^2\|[W_0]_{:,j^*}\|^2}{1+2hX_{\max} (1-\exp(2n X_{\max} t))\epsilon^2\|[W_0]_{:,j^*}\|^2}\,.\label{eq_thm1_pf_1}
%     \end{align}
%     When $t=\frac{1}{2nX_{\max}}\log\frac{1}{\sqrt{h} \epsilon}$, the right-hand side of \eqref{eq_thm1_pf_1} reduces to, then upper bounded as
%     \begin{align}
%         &\;\frac{\epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}}{1+2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2-2\sqrt{h}X_{\max}\epsilon\|[W_0]_{:,j^*}\|^2}\nonumber\\
%         &\;\leq \frac{\epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}}{1+2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2-2\sqrt{h}X_{\max}\|[W_0]_{:,j^*}\|^2\lp \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}\rp}\nonumber\\
%         &\;\leq \frac{\epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}}{\frac{1}{2}+2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2}\leq \epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}\leq \epsilon W_{\max}^2/\sqrt{h}\,.\nonumber
%         % &\;\frac{\exp(2 M_x t)\epsilon^2\|[W_0]_{:,j^*}\|^2}{1+ChM_x (1-\exp(2 M_x t))\epsilon^2\|[W_0]_{:,j^*}\|^2}\nonumber\\
%         % =&\;\frac{\epsilon\|[W_0]_{:,j^*}\|^2}{1+ChM_x \epsilon(1-\epsilon)\|[W_0]_{:,j^*}\|^2}
%     \end{align}
%     As long as $t=\frac{1}{2nX_{\max}}\log\frac{1}{\sqrt{h} \epsilon}$ is within the finite time interval in \eqref{eq_app_lem_norm_ode}. Indeed, we have 
%     \begin{align*}
%         t&\;= \frac{1}{2nX_{\max}}\log\frac{1}{\sqrt{h} \epsilon}\\
%         &\;\leq \frac{1}{2nX_{\max}}\log\lp\frac{1}{4hX_{\max}\epsilon^2W_{\max}^2}\rp\\
%         &\;\leq \frac{1}{2nX_{\max}}\log\lp \frac{1}{2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2}+1\rp\,,
%     \end{align*} 
%     where the last term is precisely the time epoch before which the bound in \eqref{eq_thm1_pf_1} holds.
    
%     Finally, suppose $\tau_{j^*}<\frac{1}{2nX_{\max}}\log\lp\frac{1}{\epsilon}\rp$, then by the continuity of $\|w_{j^*}(t)\|^2$, we have 
%     \begin{align}
%         \frac{2\epsilon W_{\max}^2}{\sqrt{h}}\leq \|w_{j^*}(\tau_{j^*})\|^2&\leq\; \epsilon W_{\max}^2/\sqrt{h}\,,
%     \end{align}
%     which leads to a contradiction $2\epsilon\leq \epsilon$. Therefore, one must have $T\geq \tau_{j^*}\geq \frac{1}{2nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$. This finishes the proof.
% \end{proof}
\begin{proof}[Proof of Lemma \ref{lem_dir_flow_approx}]
    As we showed in the proof for Lemma \ref{lem_small_norm}, under balanced initialization, 
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
    \ee
    Then for any $i\in[n]$,
    \begin{align*}
        \frac{d}{dt} \frac{w_j}{\|w_j\|}&=\; -\sign(v_j(0))\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lp x_i-\frac{\lan x_i,w_j\ran}{\|w_j\|^2}w_j \rp\\
        &=\; -\sign(v_j(0))\sum_{i: \lan x_i,w_j\ran>0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lp x_i-\frac{\lan x_i,w_j\ran}{\|w_j\|^2}w_j \rp\\
        &=\; -\sign(v_j(0))\lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp\lp\sum_{i: \lan x_i,w_j\ran>0} \nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\rp\,.
    \end{align*}
    Therefore, whenever $\max_i|f(x_i;W,v)|\leq 1$,
    \begin{align}
        &\;\lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0))\lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran> 0}y_ix_i\rp\rV\nonumber\\
        =&\;\lV \sign(v_j(0))\lp\sum_{i: \lan x_i,w_j\ran>0} \lp\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))+y_i\rp x_i\rp\rV\nonumber\\
        \leq &\; \sum_{i=1}^n |\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))+y_i |\cdot \|x_i\|\nonumber\\
        \leq &\; \sum_{i=1}^n2|f(x_i;W,v)|\cdot \|x_i\|\leq 2nM_x \max_i|f(x_i;W,v)|\,.
    \end{align}
\end{proof}

\newpage
\section{Proof for Theorem \ref{thm_conv_main}: Early Alignment Phase}\label{app_pf_thm_align}
We break the proof of Theorem \ref{thm_conv_main} into two parts: In Appendix \ref{app_pf_thm_align} we prove the first part regarding directional convergence. Then in Appendix \ref{app_pf_thm_conv} we prove the remaining statement on final convergence and low-rank bias.
\subsection{Auxiliary lemmas}
The first several Lemmas concern mostly some conic geometry given the data assumption:

Consider the following conic hull
 \be
 K=\mathcal{CH}(\{x_iy_i,i\in[n]\})=\lb\sum_{i=1}^na_ix_iy_i: a_i\geq 0,i\in[n]\rb\,.
 \ee
It is clear that $x_iy_i\in K,\forall i$, and $x_a(w)\in K,\forall w$. 
The following lemma shows any pair of vectors in $K$ is $\mu$-coherent.
 \begin{lemma}\label{lem_app_K_coherence}
    $\cos(z_1,z_2)\geq \mu,\forall 0\neq z_1,z_2\in K$.
\end{lemma}
\begin{proof}
    Since $z_1,z_2\in K$, we let $z_1=\sum_{i=1}^nx_iy_ia_{1i}$, and$z_2=\sum_{j=1}^nx_jy_ja_{2j}$, where $a_{1i},a_{2j}\geq 0$ but not all of them.
    \begin{align*}
        \cos(z_1,z_2)= \frac{1}{\|z_1\|\|z_2\|}\lan z_1,z_2\ran
        &=\; \frac{1}{\|z_1\|\|z_2\|}\sum_{i,j\in[n]}a_{1i}a_{2j}\lan x_iy_i,x_jy_j\ran\\
        &=\; \frac{\sum_{i,j\in[n]}\|x_i\|\|x_j\|a_{1i}a_{2j}\mu}{\|z_1\|\|z_2\|}\geq \mu\,,
    \end{align*}
    where the last inequality is due to $$\|z_1\|\|z_2\|\leq \lp\sum_{i=1}^n\|x_i\|a_{1i}\rp\lp\sum_{j=1}^n\|x_j\|a_{2j}\rp=\sum_{i,j\in[n]}\|x_i\|\|x_j\|a_{1i}a_{2j}\,.$$
\end{proof}
The following lemma is some basic results regarding $\mathcal{S}_+$ and $\mathcal{S}_-$:
\begin{lemma}
    $\mathcal{S}_+$ and $\mathcal{S}_-$ are convex cones (excluding the origin).
\end{lemma}
\begin{proof}
    Since $\one_{\lan x_i,z\ran}=\one_{\lan x_i,az\ran},\forall i\in[n],a>0$, $\mathcal{S}_+,\mathcal{S}_-$ are cones. Moreover, $\lan x_i,z_1\ran > 0$ and $\lan x_i,z_2\ran > 0$ implies $\lan x_i,a_1z_1+a_2z_2\ran> 0,\forall a_1,a_2>0$, thus $\mathcal{S}_+,\mathcal{S}_-$ are convex cones.
\end{proof}
Now we consider the complete metric space $\mathbb{S}^{D-1}$ (w.r.t. $\arccos(\lan\cdot,\cdot\ran)$) and we are interested in its subsets $K\cap \mathbb{S}^{D-1}$, $\mathcal{S}_+\cap \mathbb{S}^{D-1}$, and $\mathcal{S}_-\cap \mathbb{S}^{D-1}$. First, we have (we use $\mathrm{Int}(S)$ to denote the interior of $S$)
\begin{lemma}\label{lem_app_int}
    $K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1})$, and $-K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_-\cap \mathbb{S}^{D-1})$
\end{lemma}
\begin{proof}
    Consider any $x_c=\sum_{j=1}^na_jx_jy_j\in K\cap \mathbb{S}^{D-1}$, For any $x_i,y_i, i\in[n]$, we have 
    \begin{align*}
        \lan x_c,x_i\ran&=\; \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\\
        &\geq\; \mu y_i\|x_i\| \sum_{i=j}^na_j\|x_j\|\begin{cases}
            \geq \mu X_{\min}>0, & y_i>0\\
            \leq -\mu X_{\min}<0, & y_i<0\\
        \end{cases}\,.
    \end{align*}
    Depending on the sign of $y_i$, we have either
    \ben
        \lan x_c,x_i\ran= \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\geq \mu \frac{\|x_i\|}{y_i} \sum_{i=j}^na_j\|x_j\|\geq \mu X_{\min}>0\,,\ (y_i=+1)
    \een
    or 
    \ben
        \lan x_c,x_i\ran= \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\leq \mu \frac{\|x_i\|}{y_i} \sum_{i=j}^na_j\|x_j\|\leq -\mu X_{\min}<0\,,\ (y_i=-1)
    \een
    where we use the fact that $1=\|x_c\|=\|\sum_{j=1}^na_jx_jy_j\|\leq \sum_{j=1}^na_j\|x_j\|$. This already tells us $x_c\in \mathcal{S}_+\cap \mathbb{S}^{D-1}$.
    
    Since $f_i(z)=\lan z, x_i\ran$ is a continuous function of $z\in\mathbb{S}^{D-1}$. There exists an open ball $\mathcal{B}\lp x_c,\delta_i\rp$ centered at $x_c$ with some radius $\delta_i>0$, such that $\forall z\in \mathcal{B}\lp x_c,\delta_i\rp$, one have $\lvt f_i(z)-f_i\lp x_c\rp\rvt\leq \frac{\mu X_{\min}}{2}$, which implies 
    \ben
        \lan z, x_i\ran\begin{cases}
            \geq \mu X_{\min}/2>0, & y_i>0\\
            \leq -\mu X_{\min}/2<0, & y_i<0\\
        \end{cases}\,.
    \een
    Hence $\cap_{i=1}^n\mathcal{B}\lp \frac{x_c}{\|x_c\|},\delta_i\rp\in\mathcal{S}_+\cap \mathbb{S}^{D-1}$. Therefore, $x_c\in \mathrm{Int}(\mathcal{S}_+\cap \mathbb
    {S}^{D-1})$. This suffices to show $K\cap \mathbb
    {S}^{D-1}\subset \mathrm{Int}(\mathcal{S}_+\cap \mathbb
    {S}^{D-1})$. The other statement $-K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_-\cap \mathbb{S}^{D-1})$ is proved similarly.
\end{proof}
The following two lemmas are some direct results of Lemma \ref{lem_app_int}.
\begin{lemma}\label{lem_app_zeta1}
    $\exists \zeta_1>0$ such that 
    \be
        \mathcal{S}_{x_+}^{\zeta_1}\subset \mathcal{S_+},\qquad\mathcal{S}_{x_-}^{\zeta_1}\subset \mathcal{S_-}\,,
    \ee
    where $\mathcal{S}_x^\zeta:=\{z\in\mathbb{R}^D:\ \cos(z,x)\geq \sqrt{1-\zeta}\}$.
\end{lemma}
\begin{proof}
    By Lemma \ref{lem_app_int}, $\frac{x_+}{\|x_+\|}\in K\subset \mathrm{Int}(S_+)$. Since $\mathbb{S}^{D-1}$ is a complete metric space (w.r.t $\arccos \lan \cdot,\cdot\ran$), there exists a open ball centered at $\frac{x_+}{\|x_+\|}$  of some radius $\arccos(\sqrt{1-\zeta_1})$ that is a subset of $\mathcal{S}_+$, from which one can show $\mathcal{S}_{x_+}^{\zeta_1}\subset \mathcal{S}_+$. The other statement $\mathcal{S}_{x_-}^{\zeta_1}\subset \mathcal{S}_-$ simply comes from the fact that $x_+=-x_-$ and $\mathrm{Int}(\mathcal{S}_+)=-\mathrm{Int}(\mathcal{S}_-)$.
\end{proof}

\begin{lemma}\label{lem_app_gamma}
    $\exists \xi>0$, such that
    \be\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_+\cap\mathbb{S}^{D-1})^c\cap (\mathcal{S}_-\cap\mathbb{S}^{D-1})^c}|\cos(x_1,x_2)|\leq \sqrt{1-\xi}\,.\ee
    ($S^c$ here is defined to be $\mathbb{S}^{D-1}-S$, the set complement w.r.t. complete space $\mathbb{S}^{D-1}$)
\end{lemma}
\begin{proof}
    Notice that
    \ben
        \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\lan x_1,x_2\ran=\inf_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\arccos \lan x_1,x_2\ran\,.
    \een
    Since $\mathbb{S}^{D-1}$ is a complete metric space (w.r.t $\arccos \lan \cdot,\cdot\ran$) and $K\cap \mathbb{S}^{D-1}$ and $x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c$ are two of its compact subsets. Suppose 
    \ben
        \inf_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\arccos \lan x_1,x_2\ran=0\,,
    \een
    then $\exists x_1\in K\cap \mathbb{S}^{D-1}, x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c$ such that $\arccos \lan x_1,x_2\ran=0$, i.e., $x_1=x_2$, which contradicts the fact that $ K\cap \mathbb{S}^{D-1}\subseteq \mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1})$ (Lemma \ref{lem_app_int}). Therefore, we have the infimum strictly larger than zero, then
    \be
        \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (S_+\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran\leq \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\lan x_1,x_2\ran<1\,.
    \ee
    Similarly, one can show that
    \be
        \sup_{x_1\in -K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_-\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran<1\,.
    \ee
    Finally, find $\xi<1$ such that
    \ben
        \max\lb\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_+\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran,\sup_{x_1\in -K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_-\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran\rb=\sqrt{1-\xi}\,,
    \een
    then for any $x_1\in K\cap\mathbb{S}^{D-1}$ and $x_2\in (\mathcal{S}_+\cap\mathbb{S}^{D-1})^c\cap (\mathcal{S}_-\cap\mathbb{S}^{D-1})^c$, we have
    \ben
        -\sqrt{1-\xi}\leq \lan x_1,x_2\ran \leq \sqrt{1-\xi}\,,
    \een
    which is the desired result.
\end{proof}


The remaining two lemmas are technical but extensively used in the main proof.
\begin{lemma}\label{lem_app_psi_rj}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $x_r\in \mathbb{S}^{n-1}$ be some reference direction, we define
    \be
        \psi_{rj}=\lan x_r,\frac{w_j}{\|w_j\|}\ran,\ \psi_{ra}=\lan x_r, \frac{x_a(w_j)}{\|x_a(w_j)\|}\ran,\ \psi_{aj}=\lan \frac{w_j}{\|w_j\|}, \frac{x_a(w_j)}{\|x_a(w_j)\|}\ran\,,
    \ee
    where $x_a(w_j)=\sum_{i: \lan x_i,w_j\ran> 0}y_ix_i$.
    
    Whenever $\max_i|f(x_i;W,v)|\leq 1$, we have
    \be
        \lvt\frac{d}{dt} \psi_{rj} -\sign(v_j(0))\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \ee
\end{lemma}
\begin{proof}
    A simple application of Lemma \ref{lem_dir_flow_approx}, together with Cauchy-Schwartz:
    \begin{align*}
        &\;\lvt\frac{d}{dt} \psi_{rj} -\sign(v_j(0))\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\\
        &=\; \lvt x_r^\top \lp\frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0)) \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran> 0}y_ix_i\rp\rp\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \end{align*}
\end{proof}
\begin{lemma}\label{lem_app_x_a_lb}
    \be
        \|x_a(w)\|\geq \sqrt{\mu}n_a(w) X_{\min}\,,
    \ee
    where $n_a(w)=|\{i\in[n]: \lan x_i,w\ran>0\}|$.
\end{lemma}
\begin{proof}
    Let $\mathcal{I}_a(w)$ denote $\{i\in[n]: \lan x_i,w\ran>0\}$, then
    \begin{align*}
        \|x_a(w)\|= \lV\sum_{i:\lan x_i,w\ran>0} x_iy_i\rV
        &=\; \sqrt{\sum_{i\in \mathcal{I}_a(w)}\|x_i\|^2y_i^2+\sum_{i,j\in \mathcal{I}_a(w), i<j}\|x_i\|\|x_j\|\lan \frac{x_iy_i}{\|x_i\|},\frac{x_jy_j}{\|x_j\|}\ran }\\
        &\geq\; \sqrt{\sum_{i\in \mathcal{I}_a(w)}\|x_i\|^2y_i^2+\sum_{i,j\in \mathcal{I}_a(w), i<j}\|x_i\|\|x_j\||y_i||y_j|\mu }\\
        &\geq\; \sqrt{n_a(w)X_{\min}^2+\mu n_a(w)\lp n_a(w)-1\rp X_{\min}^2}\\
        &\geq\; \sqrt{n_a(w)(1+\mu (n_a(w)-1))} X_{\min}\\
        &\geq\; \sqrt{\mu}n_a(w)X_{\min}\,.
    \end{align*}
\end{proof}
% \begin{lemma}
%     Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Whenever $\max_i|f(x_i;W,v)|\leq \frac{\mu^{\frac{3}{2}}X_{\min}}{4nX_{\max}}$, we have
%     \begin{align}
%     &\;\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}>0,\forall i\in [n], \text{if } j\in \mathcal{V}_+\,,\\
%     &\;\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}<0,\forall i\in [n], \text{if } j\in \mathcal{V}_-\,.
% \end{align}
% \end{lemma}
\subsection{Proof for early alignment phase}
\begin{proof}[Proof of Theorem \ref{thm_conv_main}: First Part]Given some initialization in \eqref{eq_init}, by Assumption \ref{assump_non_degen}, $\exists \zeta_2>0$, such that
\be
    \max_{j\in\mathcal{V}_+}\cos(w_j(0),x_-)< \sqrt{1-\zeta_2},\quad \max_{j\in\mathcal{V}_-}\cos(w_j(0),x_+)< \sqrt{1-\zeta_2}\,.\label{eq_app_assump_non_degen}
\ee
We define $\zeta:=\max\{\zeta_1,\zeta_2\}$, where $\zeta_1$ is from Lemma \ref{lem_app_zeta1}. In addition, by Lemma \ref{lem_app_gamma}, $\exists \xi>0$, such that
\be\label{eq_app_ub_gamma}
\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in \mathcal{S}_-^c\cap\mathcal{S}_+^c\cap \mathbb{S}^{D-1}}|\cos(x_1,x_2)|\leq \sqrt{1-\xi}\,.
\ee

We pick a initialization scale $\epsilon$ that satisfies:
\be
    \epsilon\leq \min\lb\frac{\min\{\mu,\zeta,\xi\}\sqrt{\mu}X_{\min}}{4\sqrt{h}nX_{\max}^2W_{\max}^2}, \frac{1}{\sqrt{h}}\exp\lp -\frac{64nX_{\max}}{\min\{\zeta,\xi\} \sqrt{\mu}X_{\min}}\log n\rp\rb\leq \frac{1}{4\sqrt{h}X_{\max}W_{\max}^2}\,.\label{eq_app_ub_epsilon}
\ee
By Lemma \ref{lem_small_norm}, $\forall t\leq T=\frac{1}{4nX_{\max}}\log \frac{1}{\sqrt{h}\epsilon}$, we have 
\be
\max_i|f(x_i;W,v)|\leq \frac{\min\{\mu,\zeta,\xi\}\sqrt{\mu}X_{\min}}{4nX_{\max}}\,,\label{eq_app_ub_f}
\ee
which is the key to analyzing the alignment phase. For the sake of simplicity, we only discuss the analysis of neurons 
in $\mathcal{V}_+$ here, the proof for neurons in $\mathcal{V}_-$ is almost identical.

\textbf{Activation pattern evolution:} Pick any $w_j$ in $\mathcal{V}_+$ and pick $x_r=x_iy_i$ for some $i\in[n]$, and consider the case when $\lan w_j, x_i\ran=0$. From Lemma \ref{lem_app_psi_rj},we have
\ben
    \lvt\frac{d}{dt} \psi_{rj} -\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
$\lan w_j,x_i\ran=0$ implies $\psi_{rj}=\lan \frac{x_iy_i}{\|x_i\|},\frac{w_j}{\|w_j\|}\ran=0$, thus we have
\ben
    \lvt\frac{d}{dt} \psi_{rj}\vert_{\lan w_j,x_i\ran=0} -\psi_{ra}\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
Then whenever $w_j\notin \mathcal{S}_{\text{dead}}$, we have
\begin{align*}
    \frac{d}{dt} \psi_{rj}\vert_{\lan w_j,x_i\ran=0}&\geq\; \psi_{ra}\|x_a(w_j)\|-2nX_{\max}\max_i|f(x_i;W,v)| &\\
    &\geq\; \mu \|x_a(w_j)\| -2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_K_coherence}})\\
    &\geq \; \mu^{3/2}X_{\min}-2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_x_a_lb}})\\
    &\geq\; \mu^{3/2}X_{\min}/2>0\,. &\quad (\text{by \eqref{eq_app_ub_f}})
\end{align*}
This is precisely \eqref{eq_mono_pos} in Section \ref{ssec_pf_sketch}.

\textbf{Bound on activation transitions and duration:}  Next we show that if at time $t_0<T$, $w_j(t_0)\notin\mathcal{S}_+\cup \mathcal{S}_\text{dead}$, and the activation pattern of $w_j$ is $\one_{\lan x_i,w_j(t_0)\ran>0}$, then $\one_{\lan x_i,w_j(t_0+\Delta t))\ran>0}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$, where $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a(w_j(t_0))}$ and $n_a(w_j(t_0))$ is defined in Lemma \ref{lem_app_x_a_lb} as long as $t_0+\Delta t<T$ as well. That is, during the alignment phase $[0, T]$, $w_j$ must change its activation pattern within $\Delta t$ time. There are two cases:
\begin{itemize}[leftmargin=0.4cm]
    \item The first case is when $w_j(t_0)\in \mathcal{S}_+^c\cap \mathcal{S}_-^c\cap \mathcal{S}_\text{dead}^c$. In this case, suppose that $\one_{\lan x_i,w_j(t_0+\tau))\ran>0}= \one_{\lan x_i,w_j(t_0)\ran>0}, \forall 0\leq \tau\leq \Delta t$, i.e. $w_j$ fixes its activation during $[t_0,t_0+\Delta t]$, then we have $x_a(w_j(t_0+\tau))=x_a(w_j(t_0)),\forall 0\leq \tau\leq \Delta t$. Let us pick $x_r=x_a(w_j(t_0))$, then Lemma \ref{lem_app_psi_rj} leads to
    \ben
    \lvt\frac{d}{dt} \cos(w_j,x_a(w_j)) -\lp 1-\cos^2(w_j,x_a(w_j))\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \een
    Since $x_a(w_j)$ is fixed, we have $\forall t\in[t_0,t_0+\Delta t]$,
    \begin{align*}
        \lvt\frac{d}{dt} \cos(w_j,x_a(w_j(t_0))) -\lp 1-\cos^2(w_j,x_a(w_j(t_0)))\rp\|x_a(w_j(t_0))\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
    \end{align*}
    \begin{align*}
    \frac{d}{dt} \cos(w_j,x_a(w_j(t_0)))&\geq\; \lp 1-\cos^2(w_j,x_a(w_j(t_0)))\rp\|x_a(w_j(t_0))\|\\
    &\;\qquad\qquad -2nX_{\max}\max_i|f(x_i;W,v)| &\\
    &\geq\; \xi \|x_a(w_j(t_0))\| -2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by \eqref{eq_app_ub_gamma}})\\
    &\geq \; \xi \sqrt{\mu}n_a(w_j(t_0))X_{\min}-2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_x_a_lb}})\\
    &\geq\; \xi\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2 \,. &\quad (\text{by \eqref{eq_app_ub_f}})\\
    & \geq\; \min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,, &
\end{align*}
which implies that, by the Fundamental Theorem of Calculus,
\begin{align*}
    &\;\cos(w_j(t_0+\Delta t),x_a(w_j(t_0)))\\
    &=\;\cos(w_j(t_0),x_a(w_j(t_0)))+\int_0^{\Delta t}\frac{d}{dt} \cos(w_j(t_0+\tau),x_a(w_j(t_0)))d\tau\\
    &\geq\; \cos(w_j(t_0),x_a(w_j(t_0)))+\Delta t \cdot\min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\\
    &=\; \cos(w_j(t_0),x_a(w_j(t_0)))+2\geq 1\,,
\end{align*}
which leads to $\cos(w_j(t_0+\Delta t),x_a(w_j(t_0)))=1$. This would imply $w_j(t_0+\Delta t)\in\mathcal{S}_+$ because $x_a(w_j(t_0))\in\mathcal{S}_+$, which contradicts our original assumption that $w_j$ fixes the activation pattern. Therefore, $\exists 0<\tau_0\leq \Delta t$ such that $\one_{\lan x_i,w_j(t_0+\tau_0))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$, due to the restriction on how $w_j$ can change its activation pattern, it cannot return to its previous activation pattern, then one must have $\one_{\lan x_i,w_j(t_0+\Delta t))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$.
\item  The other case is when $w_j(t_0)\in\mathcal{S}_-$. For this case, we need first show that $w_j(t_0+\tau)\notin \mathcal{S}_{x_-}^{\zeta},\forall 0\leq \tau\leq \Delta t$, or more generally, $\mathcal{S}_{x_-}^{\zeta}$ does not contain any $w_j$ in $\mathcal{V}_+$ during $[0,T]$. To see this, let us pick $x_r=x_-$, then Lemma \ref{lem_app_psi_rj} suggests that
\ben
    \lvt\frac{d}{dt} \psi_{rj} -\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
Consider the case when $\cos(w_j,x_-)=\sqrt{1-\zeta}$, i.e. $w_j$ is at the boundary of $\mathcal{S}_{x_-}^{\zeta}$. We know that in this case, $w_j\in \mathcal{S}_{x_-}^{\zeta}\subseteq \mathcal{S}_-$ thus $x_a(w_j)=-x_-$, and
\ben
    \lvt\left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}} +\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
\een
which is
\begin{align*}
    &\;\lvt\left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}} +\zeta\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)| &\\
    \Ra&\; \left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}}\\
    &\leq\; -\zeta\|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)| &\\
     &\leq \;-\zeta\sqrt{\mu}X_{\min}+2nX_{\max}\max_i|f(x_i;W,v)|&(\text{by Lemma \ref{lem_app_x_a_lb}})\\
     &\leq \;-\zeta\sqrt{\mu}X_{\min}/2<0\,. & (\text{by \eqref{eq_app_ub_f}}) 
\end{align*}
Therefore, during $[0,T]$, neuron $w_j$ in $\mathcal{V}_+$ cannot enter $\mathcal{S}_{x_-}^{\zeta}$ if at initialization, $w_j(0)\notin \mathcal{S}_{x_-}^{\zeta}$, which is guaranteed by \eqref{eq_app_assump_non_degen}.

With the argument above, we know that $w_j(t_0+\tau)\notin\mathcal{S}_{x_-}^{\zeta}, \forall 0\leq \tau\leq \Delta t$. Again we suppose that $w_j(t)\in \mathcal{S}_--\mathcal{S}_{x_-}^{\zeta},\forall t\in[t_0,t_0+\Delta t]$, i.e.,$w_j$ fixes its activation during $[t_0,t_0+\Delta t]$. Let us pick $x_r=x_-$, then Lemma \ref{lem_app_psi_rj} suggests that
\ben
    \lvt\frac{d}{dt} \cos(w_j,x_-) +\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
    \een
    which leads to $\forall t\in[t_0,t_0+\Delta t]$,
    \begin{align*}
        \frac{d}{dt} \cos(w_j,x_-)&\leq\; -\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)|&\\
        &\leq\; -\zeta \|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)|& (w_j\notin \mathcal{S}_{x_-}^{\zeta})\\
        &\leq\; -\zeta \sqrt{\mu}n_a(w_j(t_0))X_{\min}+2nX_{\max}\max_i|f(x_i;W,v)|& (\text{by Lemma \ref{lem_app_x_a_lb}})\\
        &\leq\; -\zeta \sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,. & (\text{by \eqref{eq_app_ub_f}})\\
        & \leq\; -\min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,, &
    \end{align*}
    Similarly, by FTC, we have
    \ben
        \cos(w_j(t_0+\Delta t),x_-)\leq -1\,.
    \een
    This would imply $w_j(t_0+\Delta t)\in\mathcal{S}_+$ because $-x_-=x_a(w_j(t_0))\in\mathcal{S}_+$, which contradicts our original assumption that $w_j$ fixes its activation pattern. Therefore, one must have $\one_{\lan x_i,w_j(t_0+\Delta t))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$.
\end{itemize}
In summary, we have shown that, during $[0,T]$, a neuron in $\mathcal{V}_+$ can not keep a fixed activation pattern for a time longer than $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a}$, where $n_a$ is the number of data points that activate $w_j$ under the fixed activation pattern.

\textbf{Bound on total travel time until directional convergence} As we have discussed in Section \ref{ssec_pf_sketch} and also formally proved here, during alignment phase $[0,T]$, a neuron in $\mathcal{V}_+$ must change its activation pattern within $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a}$ time unless it is in either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$. And the new activation it is transitioning into must contain no new activation on negative data points and must keep all existing activation on positive data points, together it shows that a neuron must reach either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$ within a fixed amount of time, which is the remaining thing we need to formally show here.

For simplicity of the argument, we first assume $T=\infty$, i.e., the alignment phase lasts indefinitely, and we show that a neuron in $\mathcal{V}_+$ must reach $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$ before $t_1=\frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}$. Lastly, such directional convergence can be achieved if $t_1\leq T$, which is guaranteed by our choice of $\epsilon$ in \eqref{eq_app_ub_epsilon}. 

\begin{itemize}[leftmargin=0.3cm]
    \item For a neuron in $\mathcal{V}_+$ that reaches $\mathcal{S}_\text{dead}$, the analysis is easy: It must start with no activation on positive data and then lose activation on negative data one by one until losing all of its activation. Therefore, it must reach $\mathcal{S}_\text{dead}$ before
    \ben
        \sum_{k=1}^{n_a(w_j(0))}\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}k}\leq \frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp\sum_{k=1}^n\frac{1}{k}\rp\leq \frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}=t_1\,.
    \een
    \item For a neuron in $\mathcal{V}_+$ that reaches $\mathcal{S}_+$, there is no difference conceptually, but it can switch its activation pattern in many ways before reaching $\mathcal{S}_+$, so it is not straightforward to see its travel time until $\mathcal{S}_+$ is upper bounded by $t_1$.

    To formally show the upper bound on the travel time, we need some definition of a path that keeps a record of the activation patterns of a neuron $w_j(t)$ before it reaches $\mathcal{S}_+$.

    Let $n_+=|\mathcal{I}_+|$, $n_-=|\mathcal{I}_-|$ be the number of positive, negative data respectively, then we call $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ a \emph{path} of length-$L$, if
    \begin{enumerate}[leftmargin=0.5cm]
        \item $\forall 0\leq l\leq L$, we have $k^{(l)}=(k_+^{(l)},k_-^{(l)})\in \mathbb{N}\times\mathbb{N}$ with $0\leq k_+^{(l)}\leq n_+$, $0\leq k_-^{(l)}\leq n_-$;
        \item For $k^{(l_1)},k^{(l_2)}$ with $l_1< l_2$, we have either $k_+^{(l_1)}>k_+^{(l_2)}$ or $k_-^{(l_1)}<k_-^{(l_2)}$;
        \item $k^{(L)}=(n_+,0)$;
        \item $k^{(l)}\neq (0,0),\forall 0\leq l\leq L$.
    \end{enumerate}
    % Figure environment removed

    Given all our analysis on how a neuron $w_j(t)$ can switch its activation pattern in previous parts, we know that for any $w_j(t)$ that reaches $\mathcal{S}_+$, there is an associated $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ that keeps an ordered record of encountered values of
    \ben
        \lp|\{i\in\mathcal{I}_+:\lan x_i,w_j(t)\ran>0\}|,\ |\{i\in\mathcal{I}_-:\lan x_i,w_j(t)\ran>0\}|\rp\,,
    \een
    before $w_j$ reaches $\mathcal{S}_+$. That is, a neuron $w_j$ starts with some activation pattern that activates $k_+(0)$ positive data and $k_-(0)$ negative data, then switch its activation pattern (by either losing negative data or gaining positive data) to one that activates $k_+(1)$ positive data and $k_-(1)$ negative data. By keep doing so, it reaches $\mathcal{S}_+$ that activates $k_+(L)=n_+$ positive data and $k_-(L)=0$ negative data. Please see Figure \ref{fig_path} for an illustration of a path.

    Given a path $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ of neuron $w_j$, we define the \emph{travel time} of this path as
    \ben
        T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})=\sum_{l=0}^{L-1}\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}(k_+^{(l)}+k_-^{(l)})}\,,
    \een
    which is exactly the traveling time from $k^{(0)}$ to $k^{(L)}$ if one spends $\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}(k_+^{(l)}+k_-^{(l)})}$ on the edge between $k^{(l)}$ and  $k^{(l+1)}$.
    
    Our analysis shows that if $w_j$ reaches $\mathcal{S}_+$, then
    \ben
        \inf\{t:w_j(t)\in\mathcal{S}_+\}\leq  T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\,.
    \een

    Now we define the maximal path $\mathcal{P}_{\max}$ as a path that has the maximum length $n=n_++n_-$, which is uniquely determined by the following trajectory of $k^{(l)}$
    \ben
        (0,n_-),(0,n_--1),(0,n_--2),\cdots,(0,1),(1,1),(1,0),\cdots,(n_+-1,0),(n_+,0)\,.
    \een
    Please see Figure \ref{fig_path_max_path} for an illustration.

    The traveling time for $\mathcal{P}_{\max}$ is
    \begin{align*}
         T(\mathcal{P}_{\max})&=\;\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp\sum_{k=1}^{n_-}\frac{1}{k}+\frac{1}{2}+\sum_{k=1}^{n_+-1}\frac{1}{k}\rp\\
         &\leq\; \frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp 2\sum_{k=1}^{n}\frac{1}{k}+\frac{1}{2}\rp\\
         &\leq\; \frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}=t_1\,.
    \end{align*}
    The proof is complete by the fact that any path satisfies
    \ben
        T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\leq T(\mathcal{P}_{\max})\,.
    \een
    This is because there is a one-to-one correspondence between the edges $(k^{(l)}, k^{(l+1)})$ in $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ and a subset of edges in $\mathcal{P}_{\max}$, and the travel time from of edge $(k^{(l)}, k^{(l+1)})$ is shorter than the corresponding edge in $\mathcal{P}_{\max}$. Formally stating such correspondence is tedious and a visual illustration in Figure \ref{fig_comp_path_h} and \ref{fig_comp_path_v} is more effective (Putting all correspondence makes a clustered plot thus we split them into two figures):
    % Figure environment removed
    
    Therefore, if $w_j$ reaches $\mathcal{S}_+$, then it reaches $\mathcal{S}_+$ within $t_1$:
    \ben
        \inf\{t:w_j(t)\in\mathcal{S}_+\}\leq  T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\leq T(\mathcal{P}_{\max})\leq t_1\,.
    \een
    
\end{itemize}

So far we have shown when the alignment phase lasts long enough, i.e., $T$ large enough, the directional convergence is achieved by $t_1$. We simply pick $\epsilon$ such that
\ben
    T=\frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}\geq t_1=\frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\,,
\een
and \eqref{eq_app_ub_epsilon} suffices.
\end{proof}

\newpage
\section{Proof for Theorem \ref{thm_conv_main}: Final Convergence}\label{app_pf_thm_conv}
Since we have proved the first part of Theorem \ref{thm_conv_main} in Section \ref{app_pf_thm_align}, we will use it as a fact, then prove the remaining part of Theorem \ref{thm_conv_main}. 
\subsection{Auxiliary lemmas}
First, we show that $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$ are trapping regions.

\begin{lemma}\label{lem_app_trapping}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf}, we have the following:
    \begin{itemize}[leftmargin=0.3cm]
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_\text{dead}$, then $w_j(t_1+\tau)\in\mathcal{S}_\text{dead},\ \forall \tau\geq 0$;
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_+$ for some $j\in\mathcal{V}_+$, then $w_j(t_1+\tau)\in\mathcal{S}_+,\ \forall \tau\geq 0$;
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_-$ for some $j\in\mathcal{V}_-$, then $w_j(t_1+\tau)\in\mathcal{S}_-,\ \forall \tau\geq 0$;
    \end{itemize}
\end{lemma}
\begin{proof}
    The first statement is simple, if $w_j\in\mathcal{S}_\text{dead}$, then one have $\dot{w}_j=0$, thus $w_j$ remains in $\mathcal{S}_\text{dead}$.

    For the second statement, we have, since $j\in\mathcal{V}_+$,
    \ben
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|\,.
    \een
    By the Fundamental Theorem of Calculus, one writes, $\forall \tau\geq 0$,
    \begin{align*}
        w_j(t_1+\tau)&=\;w_j(t_1)+\int_0^\tau \frac{d}{dt}w_jd\tau\\
        &=\;w_j(t_1)+\int_0^\tau -\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|d\tau\\
        &=\;w_j(t_1)+\int_0^\tau \sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}y_i\exp(-y_if(x_i;W,v))x_i\|w_j\|d\tau\\
        &=\;w_j(t_1)+\underbrace{\sum_{i\in\mathcal{I}_+}\lp\int_0^\tau \exp(-y_if(x_i;W,v))\|w_j\|d\tau \rp x_i}_{:=\tilde{x}_+}\,.
    \end{align*}
    Here $w_j(t_1)\in\mathcal{S}_+$ by our assumption, $\tilde{x}_+\in K\subseteq \mathcal{S}_+$ because $\tilde{x}_+$ is a conical combination of $x_i,i\in\mathcal{I}_+$. Since $\mathcal{S}_+$ is a convex cone, we have $w_j(t_1+\tau)\in\mathcal{S}_+$ as well.

    The proof of the third statement is almost identical: when $j\in\mathcal{V}_-$, we have
    \ben
        \frac{d}{dt}w_j=\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|\,,
    \een
    and 
    \ben
        w_j(t_1+\tau)= w_j(t_1)+\underbrace{\sum_{i\in\mathcal{I}_-}\lp\int_0^\tau \exp(-y_if(x_i;W,v))\|w_j\|d\tau \rp x_i}_{:=\tilde{x}_-}.
    \een
    Again, here $w_j(t_1)\in\mathcal{S}_-$ by our assumption, $\tilde{x}_-\in -K\subseteq \mathcal{S}_-$ because $\tilde{x}_-$ is a conical combination of $x_i,i\in\mathcal{I}_-$. Since $\mathcal{S}_-$ is a convex cone, we have $w_j(t_1+\tau)\in\mathcal{S}_+$ as well.
    \end{proof}

    Then the following Lemma provides a lower bound on neuron norms upon $t_1$.
    \begin{lemma}\label{lem_app_norm_lb_t1}
        Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $t_1$ be the time when directional convergence is achieved, as defined in Theorem \ref{thm_conv_main}, and we define $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. If both $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$ are non-empty, we have 
        \ben
            \sum_{j\in \tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2, 
        \een
        \ben
            \sum_{j\in \tilde{\mathcal{V}}_-}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(0)\|^2, 
        \een
    \end{lemma}
    \begin{proof}
        We have shown that
        \ben
        \frac{d}{dt}\|w_j\|^2
        = -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|\,.
        \een
        Then before $t_1$, we have $\forall j\in[h]$
        \begin{align*}
            \frac{d}{dt}\|w_j\|^2 &=\; -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|\\
            &\geq\;-2\sum_{i=1}^n(|y_i|+2\max_i|f(x_i;W,v)|)\|x_i\|\|w_j\|^2\\
            &\geq\; -4\sum_{i=1}^n\|x_i\|\|w_j\|^2\geq -4nX_{\max}\|w_j\|^2\,,
        \end{align*}
        where the second last inequality is because $\max_i|f(x_i;W,v)|\leq \frac{1}{2}$ before $t_1$. Summing over $j\in\tilde{\mathcal{V}}_+$, we have
        \ben
            \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\geq -4nX_{\max}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\,.
        \een
        Therefore, we have the following bound:
        \ben
            \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2\,.
        \een
    \end{proof}
    Moreover, after $t_1$, the neuron norms are non-decreasing, as suggested by 
    \begin{lemma}\label{lem_app_mono_norm}
        Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $t_1$ be the time when directional convergence is achieved, as defined in Theorem \ref{thm_conv_main}, and we define $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. If both $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$ are non-empty, we have $\forall \tau\geq 0$ and $t_2\geq t_1$,
        \be
            \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2+\tau)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2)\|,\qquad \sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t_2+\tau)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t_2)\| 
        \ee
    \end{lemma} 
    \begin{proof}
        It suffices to show that after $t_1$, the following derivatives:
        \ben
            \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2, \quad \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\,,
        \een
        are non-negative. 

        For $j\in \tilde{\mathcal{V}}_+$, $w_j$ stays in $\mathcal{S}_+$ by Lemma \ref{lem_app_trapping}, and we have
        \begin{align*}
            \frac{d}{dt}\|w_j\|^2
            &=\; -2\sum_{i\in\mathcal{I}_+}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \|w_j\|\,.\\
            &=\; 2\sum_{i\in\mathcal{I}_+}y_i\ell(y_i,f(x_i; W,v))\lan x_i, w_j\ran \|w_j\|\geq 0\,.
        \end{align*}
        Summing over $j\in \tilde{\mathcal{V}}_+$, we have $\frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2\geq 0$. Similarly one has $\frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\geq 0$.
    \end{proof}
    Finally, the following lemma is used for deriving the final convergence.
    \begin{lemma}\label{lem_app_gpl}
        Consider the following loss function
        \ben
            \mathcal{L}_{\text{lin}}(W,v)=\sum_{i=1}^n\ell\lp y_i,v^\top W^\top x_i)\rp\,,
        \een    
        if $\{x_i,y_i\},i\in[n]$ are linearly separable, i.e., $\exists \gamma>0$ and $z\in\mathbb{S}^{D-1}$ such that $y_i\lan z,x_i\ran\geq \gamma,\forall i\in[n]$, then under the gradient flow on $\mathcal{L}_{\text{lin}}(W,v)$, we have
        \be
            \dot{\mathcal{L}}_{\text{lin}}\leq -\|v\|^2\mathcal{L}^2\gamma^2\,.
        \ee
        
    \end{lemma}
    \begin{proof}
        \begin{align*}
            \dot{\mathcal{L}}=-\|\nabla_W\mathcal{L}\|^2_F-\|\nabla_v\mathcal{L}\|^2_F
            &\leq\; -\|\nabla_W\mathcal{L}\|^2_F\\
            &=\; -\lV \sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_iv^\top \rV_F^2\\
            &=\; -\|v\|^2\lV \sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_i\rV^2\\
            &\leq\; -\|v\|^2 \lvt\lan z,\sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_i\ran\rvt^2\\
            &\leq\; -\|v\|^2 \lvt\sum_{i=1}^n\ell(y_i,v^\top W^\top x_i)\gamma\rvt^2\leq -\|v\|^2\mathcal{L}^2\gamma^2\,.
        \end{align*}
    \end{proof}


\subsection{Proof of final convergence}

\begin{proof}[Proof of Theorem \ref{thm_conv_main}: Second Part]By Lemma \ref{lem_app_trapping}, we know that after $t_1$, neurons in $\mathcal{S}_+$ ($\mathcal{S}_-$) stays in $\mathcal{S}_+$ ($\mathcal{S}_-$). Thus the loss can be decomposed as
\be\label{eq_app_L_decouple}
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_-}v_j\lan w_j,x_i\ran\rp}_{\mathcal{L}_-}\,,
\ee
where $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. Therefore, the training after $t_1$ is decoupled into 1) using neurons in $\tilde{\mathcal{V}}_+$ to fit positive data in $\mathcal{I}_+$ and 2) using neurons in $\tilde{\mathcal{V}}_-$ to fit positive data in $\mathcal{I}_-$. 

We define $f_+(x_i;W,v)=\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran$ and let $t_2^+=\inf\{t: \max_{i\in\mathcal{I}_+}|f_+(x_i;W,v)|>\frac{1}{4}\}$. Similarly, we also define $f_-(x_i;W,v)=\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran$ and let $t_2^-=\inf\{t: \max_{i\in\mathcal{I}_-}|f_-(x_i;W,v)|>\frac{1}{4}\}$. Then $t_1\leq \min\{t_2^+,t_2^-\}$, by Lemma \ref{lem_small_norm}. 

\textbf{$\mathcal{O}\lp 1/t\rp$ convergence after $t_2$}: We first show that when both $t_2^+,t_2^-$ are finite, then it implies $\mathcal{O}(1/t)$ convergence on the loss. Then we show that they are indeed finite and $t_2:=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$.

At $t_2=\max\{t_2^+,t_2^-\}$, by definition, $\exists i_+\in\mathcal{I}_+$ such that 
\be
    \frac{1}{4}\leq f_+(x_{i_+};W,v)\leq \sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_{i_+}\ran\leq \sum_{j\in\tilde{\mathcal{V}}_+} \|w_j\|^2\|x_{i_+}\|\,,
\ee
which implies, by Lemma \ref{lem_app_mono_norm}, $\forall t\geq t_2$
\be\label{eq_app_w_p_sum_lb}
    \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2)\|^2\geq \frac{1}{4\|x_{i_+}\|}\geq \frac{1}{4X_{\max}}\,.
\ee
Similarly, we have $\forall t\geq t_2$,
\be\label{eq_app_w_m_sum_lb}
\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\geq\frac{1}{4X_{\max}}\,.
\ee
Under the gradient flow dynamics \eqref{eq_gf}, we apply Lemma \ref{lem_app_gpl} to the decomposed loss \eqref{eq_app_L_decouple}
\begin{align*}
    \dot{\mathcal{L}}&\leq\;-\lp\sum_{j\in\tilde{\mathcal{V}}_+}v_j^2 \rp\cdot \mathcal{L}_+^2\cdot (\mu X_{\min})^2-\lp\sum_{j\in\tilde{\mathcal{V}}_+}v_j^2 \rp\cdot \mathcal{L}_-^2\cdot (\mu X_{\min})^2\,.
\end{align*}
Here, we can pick the same $\gamma=\mu X_{\min}$ for both $\mathcal{L}_+$ and $\mathcal{L}_-$ because $\{x_i,y_i\},i\in\mathcal{I}_+$ is linearly separable with $z=\frac{y_1x_1}{\|x_1\|}$: $\lan z,x_iy_i\ran\geq \mu\|x_i\|\geq \mu X_{\min}$ by Assumption \ref{assump_data}. And similarly, $\{x_i,y_i\},i\in\mathcal{I}_-$ is linearly separable with $\lan z,x_iy_i\ran\geq \mu\|x_i\|\geq \mu X_{\min}$. Replace $v_i^2$ by $\|w_j\|^2$ from balancedness, together with \eqref{eq_app_w_p_sum_lb}\eqref{eq_app_w_m_sum_lb}, we have
\begin{align*}
    \dot{\mathcal{L}}&\leq\;-\lp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2 \rp\cdot \mathcal{L}_+^2\cdot (\mu X_{\min})^2-\lp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2 \rp\cdot \mathcal{L}_-^2\cdot (\mu X_{\min})^2\\
    &\leq\; -\frac{(\mu X_{\min})^2}{4X_{\max}} (\mathcal{L}_+^2+\mathcal{L}_-^2)\leq -\frac{(\mu X_{\min})^2}{8X_{\max}} (\mathcal{L}_++\mathcal{L}_-)^2=-\frac{(\mu X_{\min})^2}{8X_{\max}}\mathcal{L}^2\,,
\end{align*}
which is
\ben
    \frac{1}{\mathcal{L}^2} \dot{\mathcal{L}} \leq -\frac{(\mu X_{\min})^2}{8X_{\max}}\,.
\een
Integrating both side from $t_2$ to any $t\geq t_2$, we have
\ben
\left.\frac{1}{\mathcal{L}}\rvt_{t_2}^\top \leq-\frac{(\mu X_{\min})^2}{8X_{\max}}(t-t_2)\,,
\een
which leads to
\ben
    \mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1}\,, \text{ where } \alpha =\frac{(\mu X_{\min})^2}{8X_{\max}}\,.
\een
\textbf{Showing $t_2=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$}: The remaining thing is to show $t_2$ is $\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. 

Since after $t_1$, the gradient dynamics are fully decoupled into two gradient flow dynamics (on $\mathcal{L}_+$ and on $\mathcal{L}_-$), it suffices to show $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ and $t_2^-=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ separately, then combine them to show $t_2=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. The proof is almost identical for $\mathcal{L}_+$ and $\mathcal{L}_-$, thus we only prove $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ here.

Suppose 
\be\label{eq_app_t2_assump}
    t_2\geq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}+\frac{4}{\sqrt{\mu}n_+X_{\min}}\lp \log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}+4nX_{\max}t_1\rp\,,
\ee
where $n_+=|\mathcal{I}_+|$. It takes two steps to show a contradiction: First, we show that for some $t_a\geq 0$, a refined alignment $\cos(w_j(t_1+t_a),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ is achieved, and such refined alignment is maintained until 
 at least $t_2^+$: $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+t_a\leq t\leq t_2^+$. Then, keeping this refined alignment leads to a contradiction.
 \begin{itemize}[leftmargin=0.3cm]
     \item For $j\in\tilde{\mathcal{V}}_+$, we have
     \ben
        \frac{d}{dt} \frac{w_j}{\|w_j\|}= \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp\underbrace{\lp\sum_{i\in\mathcal{I}_+} -\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))x_i\rp}_{:=\tilde{x}_a}\,.
     \een
     Then 
     \begin{align*}
         \frac{d}{dt} \cos(x_+,w_j)&=\;\lp\cos(x_+,\tilde{x}_a)-\cos(x_+,w_j)\cos(\tilde{x}_a,w_j)\rp \|\tilde{x}_a\|\\
         &\geq\; \lp\cos(x_+,\tilde{x}_a)-\cos(x_+,w_j)\rp \|\tilde{x}_a\|\,.
     \end{align*}
     We can show that $\cos(x_+,\tilde{x}_a)\geq \frac{1}{3}$ and $\|\tilde{x}_a\|\geq \sqrt{\mu}n_+X_{\min}/2$ when $t_1\leq t\leq t_2^+$ (we defer the proof to the end as it breaks the flow), thus within $[t_1,t_2^+]$, we have
     \be\label{eq_app_ref_align}
        \frac{d}{dt} \cos(x_+,w_j)\geq \lp\frac{1}{3}-\cos(x_+,w_j)\rp\sqrt{\mu}n_+X_{\min}/2\,.
     \ee
     We use \eqref{eq_app_ref_align} in two ways: First, since 
     \ben
        \left.\frac{d}{dt} \cos(x_+,w_j)\right\rvert_{\cos(x_+,w_j)=\frac{1}{4}}\geq \frac{\sqrt{\mu}n_+X_{\min}}{24}>0\,,
     \een
     $\cos(x_+,w_j)\geq \frac{1}{4}$ is a trapping region for $w_j$ during $[t_1,t_2^+]$. Define $t_a:=\inf\{t\geq t_1: \min_{j\in\tilde{\mathcal{V}}_+} \cos(x_+,w_j(t))\geq \frac{1}{4}\}$, then clearly, if $t_a\leq t_2^+$, then $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+t_a\leq t\leq t_2^+$.

     Now we use \eqref{eq_app_ref_align} again to show that $t_a\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$: Suppose that $t_a\geq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$, then $\exists j^*$ such that $\cos(x_+,w_{j^*}(t))< \frac{1}{4},\forall t\in[t_1,t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}]$, and we have
    \be
        \frac{d}{dt} \cos(x_+,w_{j^*})\geq \lp\frac{1}{3}-\cos(x_+,w_j)\rp\sqrt{\mu}n_+X_{\min}/2\geq \frac{\sqrt{\mu}n_+X_{\min}}{24}\,.
     \ee
     This shows
     \ben
        \cos(x_+,w_{j^*}(t_1+1))\geq \cos(x_+,w_{j^*}(t_1))+\frac{1}{4}\geq \frac{1}{4}\,,
     \een
     which contradicts that $\cos(x_+,w_{j^*}(t))< \frac{1}{4}$. Hence we know $t_a\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$.

     In summary, we have $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}\leq t\leq t_2^+$.
     \vspace{0.3cm}
     \item Now we check the dynamics of $\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2$ during $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}\leq t\leq t_2^+$. For simplicity, we denote $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}:=t_1'$.

     For $j\in\tilde{\mathcal{V}}_+$, we have, for $t_1'\leq t\leq t_2^+$,
     \begin{align*}
            \frac{d}{dt}\|w_j\|^2 &=\; 2\sum_{i\in\mathcal{I}_+}-\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \|w_j\|&\\
            &\geq\; \sum_{i\in\mathcal{I}_+}\lan x_i, w_j\ran \|w_j\|& (\text{by }\eqref{eq_app_conv_grad_bd})\\
            &=\; \lan x_+, w_j\ran \|w_j\|&\\
            &=\; \|x_+\|\|w_j\|^2\cos(x_+,w_j)&\\
            &\geq\; \frac{1}{4}\|x_+\|\|w_j\|^2&(\text{Since } t\geq t_1') \\
            &\geq\; \frac{\sqrt{\mu}n_+X_{\min}}{4}\|w_j\|^2\,, & (\text{by Lemma \ref{lem_app_x_a_lb}})
    \end{align*}
    which leads to (summing over $j\in\tilde{\mathcal{V}}_+$)
    \ben
        \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\geq \frac{\sqrt{\mu}n_+X_{\min}}{4}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\,.
    \een
    By Gronwall's inequality, we have
    \begin{align*}
        &\;\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2^+)\|^2&\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1')\|^2&\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2& (\text{By Lemma \ref{lem_app_mono_norm}})\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \exp\lp -4nX_{\max}t_1\rp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2& (\text{By Lemma \ref{lem_app_norm_lb_t1}})\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \exp\lp -4nX_{\max}t_1\rp \epsilon^2W_{\min}^2\geq \frac{2}{\sqrt{\mu}X_{\min}}\,.& (\text{by }\eqref{eq_app_t2_assump})
    \end{align*}
    However, at $t_2^+$, we have
    \begin{align*}
        \frac{1}{4}\geq \frac{1}{n_+}\sum_{i\in\mathcal{I}_+}f_+(x_i;W,v)&=\;\frac{1}{n_+}\sum_{i\in\mathcal{I}_+}\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran&\\
        &=\; \frac{1}{n_+}\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_+\ran*\\
        &=\;\frac{1}{n_+}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\cos(w_j,x_+)\|x_+\|&\\
        &\geq\;\frac{1}{4n_+}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\|x_+\|& (\text{Since } t\geq t_1')\\
        &\geq\;\frac{1}{4}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\sqrt{\mu}X_{\min}\,, & (\text{by Lemma \ref{lem_app_x_a_lb}})
    \end{align*}
    which suggests $\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\leq \frac{1}{\sqrt{\mu}X_{\min}}$. A contradiction.
 \end{itemize}
 Therefore, we must have
 \be
    t_2^+\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}+\frac{4}{\sqrt{\mu}n_+X_{\min}}\lp \log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}+4nX_{\max}t_1\rp\,.
\ee
Since the dominant term here is $\frac{4}{\sqrt{\mu}n_+X_{\min}}\log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}$, we have $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. A similar analysis shows $t_2^-=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. Therefore $t_2=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$

\textbf{Complete the missing pieces}
 We have two claims remaining to be proved. The first is $\cos(x_+,\tilde{x}_a)\geq\frac{1}{2}$ when $t_1\leq t\leq t_2^+$. Since $x_+=\sum_{i\in\mathcal{I}_+}x_i$ and $\tilde{x}_a=\sum_{i\in\mathcal{I}_+}-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))x_i$. We simply use the fact that before $t_2^+$, we have, by Lemma \ref{assump_loss},
 \be\label{eq_app_conv_grad_bd}
    \frac{1}{2}\leq -\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))=\leq \frac{3}{2}\,,
 \ee
 to show the following
 \begin{align*}
     \cos(x_+,\tilde{x}_a)&=\;\frac{\lan x_+,\tilde{x}_a\ran}{\|x_+\|\|\tilde{x}_a\|}\\
     &=\; \frac{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}}\\
     &\geq \; \frac{\frac{1}{2}\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}}\\
     &\geq \; \frac{\frac{1}{2}\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(\frac{3}{2})^2\lan x_i,x_j\ran}}\geq \frac{1}{3}\,,
 \end{align*}
 since all $\lan x_i,x_j\ran,i,j\in\mathcal{I}_+$ are non-negative.

 The second claim is $\|\tilde{x}_a\|\geq \sqrt{\mu}n_+ X_{\min}/2$ is due to that
 \ben
    \|\tilde{x}_a\|=\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}\geq \frac{1}{2}\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}=\frac{\|x_+\|}{2}\geq \frac{\sqrt{\mu}n_+X_{\min}}{2}\,,
 \een
 where the last inequality is from Lemma \ref{lem_app_x_a_lb}.
\end{proof}
\subsection{Proof of low-rank bias}
So far we have proved the directional convergence at the early alignment phase and final $\mathcal{O}(1/t)$ convergence of the loss in the later stage. The only thing that remains to be shown is the low-rank bias. The proof is quite straightforward but we need some additional notations.

As we proved above, after $t_1$, neurons in $\mathcal{S}_+$ ($\mathcal{S}_-$) stays in $\mathcal{S}_+$ ($\mathcal{S}_-$). Thus the loss can be decomposed as
\ben
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_-}v_j\lan w_j,x_i\ran\rp}_{\mathcal{L}_-}\,,
\een
where $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. Therefore, the training after $t_1$ is decoupled into 1) using neurons in $\tilde{\mathcal{V}}_+$ to fit positive data in $\mathcal{I}_+$ and 2) using neurons in $\tilde{\mathcal{V}}_-$ to fit positive data in $\mathcal{I}_-$. We use
\ben
    W_+=[W]_{:,\tilde{\mathcal{V}}_+},\quad W_-=[W]_{:,\tilde{\mathcal{V}}_-}
\een
to denote submatrices of $W$ by picking only columns in  $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$, respectively. Similarly, we define
\ben
    v_+=[v]_{\tilde{\mathcal{V}}_+}, \quad v_-=[v]_{\tilde{\mathcal{V}}_-}
\een
for the second layer weight $v$. Lastly, we also define
\ben
    W_\text{dead}=[W]_{:,\tilde{\mathcal{V}}_\text{dead}}, v_\text{dead}=[v]_{\tilde{\mathcal{V}}_\text{dead}}\,,
\een
where $\tilde{\mathcal{V}}_\text{dead}:=\{j: w_j(t_1)\in\mathcal{S}_\text{dead}\}$. Given these notations, after $t_1$ the loss is decomposed as
\ben
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,x_i^\top W_+v_+\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,x_i^\top W_-v_-\rp}_{\mathcal{L}_-}\,,
\een
and the GF on $\mathcal{L}$ is equivalent to GF on $\mathcal{L}_+$ and $\mathcal{L}_-$ separately. It suffices to study one of them. For GF on $\mathcal{L}_+$, we have the following important invariance~\cite{arora2018optimization} $\forall t\geq t_1$:
\ben
    W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)=W_+^\top(t_1)  W_+(t_1)-v_+(t_1)v_+^\top(t_1)\,,
\een
from which one has
\begin{align*}
    \|W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)\|_2&=\; \|W_+^\top(t_1)  W_+(t_1)-v_+(t_1)v_+^\top(t_1)\|_2\\
    &\leq \; \|W_+^\top(t_1)  W_+(t_1)\|_2-\|v_+(t_1)v_+^\top(t_1)\|_2\\
    &\leq\; \tr(W_+^\top(t_1)  W_+(t_1))+\|v_+(t_1)\|^2\\
    &=\;2\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\leq \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,,
\end{align*}
where the last inequality is by Lemma \ref{lem_small_norm}. Then one can immediately get
\ben
    \|v_+(t)v_+^\top (t)\|_2-\|W_+^\top(t) W_+(t)\|_2\leq\|W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)\|_2\leq  \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,,
\een
which is precisely
\be\label{eq_app_lrank_1}
    \|W_+(t)\|_F^2\leq \|W_+(t)\|_2^2+\frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,.
\ee
Similarly, we have
\be\label{eq_app_lrank_2}
    \|W_-(t)\|_F^2\leq \|W_-(t)\|_2^2+\frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_-|\,.
\ee
Lastly, one has
\be\label{eq_app_lrank_3}
\|W_{\text{dead}}\|^2_F=\sum_{j\in\tilde{\mathcal{V}}_{\text{dead}}}\|w_j(t_1)\|^2\leq \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_\text{dead}|
\ee
Adding \eqref{eq_app_lrank_1}\eqref{eq_app_lrank_2}\eqref{eq_app_lrank_3} together, we have
\begin{align*}
    \|W(t)\|^2_F&=\;\|W_+(t)\|_F^2+\|W_-(t)\|_F^2+\|W_{\text{dead}}\|^2_F\\
    &\leq\; \|W_+(t)\|_2^2+\|W_-(t)\|_2^2+\frac{4\sqrt{h}\epsilon W_{\max}^2}{\sqrt{h}}\leq 2\|W(t)\|_2^2+4\sqrt{h}\epsilon W_{\max}^2\,.
\end{align*}
Finally, since we have shown $\mathcal{L}\ra 0$ as $t\ra \infty$, then $\forall i\in[n]$, we have $\ell(y_i,f(x_i;W,v))\ra 0$. This implies
\ben
    f(x_i;W,v)=-\frac{1}{y_i}\log \ell(y_i,f(x_i;W,v))\ra \infty\,.
\een
Because we have shown that 
\ben
    f(x_i;W,v)\leq \sum_{j\in[h]}\|w_j\|^2\|x_i\|\leq \|W\|_F^2X_{\max}\,,
\een
$f(x_i;W,v)\ra \infty$ enforces $\|W\|_F^2\ra \infty$ as $t\ra \infty$, thus $\|W\|_2^2\ra \infty$ as well. This gets us
\ben
    \lim\sup_{t\ra \infty}\frac{\|W\|_F^2}{\|W\|_2^2}=2\,.
\een
% \section{Proof for Auxiliary Lemmas}
