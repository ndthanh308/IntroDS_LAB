\documentclass[11pt,letter]{article}
\usepackage[papersize={8.5in,11in},margin=1in]{geometry}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{pdfpages}
\newcommand{\dtlinkcolor}{{0.8 0.8 1}} % link color, RGB
\usepackage{hyperref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{palatino}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{enumitem}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{hanch_sty}
\input{edits.tex}
\usepackage{bbold,amsmath,amsthm,amssymb}
\usepackage{multirow}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\rank}{\mathrm{rank}}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\sign}{\mathrm{sign}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newtheorem{theorem}{Theorem}

\newtheorem{innercustomasump}{Assumption}
\newenvironment{customasump}[1]
  {\renewcommand\theinnercustomasump{#1}\innercustomasump}
  {\endinnercustomasump}

\newtheorem{innercustomthm}{Theorem}
\newenvironment{customthm}[1]
  {\renewcommand\theinnercustomthm{#1}\innercustomthm}
  {\endinnercustomthm}
 \newtheorem{innercustomlem}{Lemma}
\newenvironment{customlem}[1]
  {\renewcommand\theinnercustomlem{#1}\innercustomlem}
  {\endinnercustomlem}
  
   \newtheorem{innercustomprop}{Proposition}
\newenvironment{customprop}[1]
  {\renewcommand\theinnercustomprop{#1}\innercustomprop}
  {\endinnercustomprop}
  
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem*{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{theorem_sec}{Theorem}[section]
\newtheorem{lemma_sec}[theorem_sec]{Lemma}
\newtheorem{example}{Example}
\newtheorem*{claim}{Claim}
\newtheorem{claim_sec}{Claim}[section]

\title{Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization}



\author{%
  Hancheng Min\footnotemark[1], Ren\'e Vidal\footnotemark[2], and Enrique Mallada\footnotemark[1] \\
  \\
  \footnotemark[1] Electrical and Computer Engineering, Johns Hopkins University \\
  \footnotemark[2] Center for Innovation in Data Engineering and Science, University of Pennsylvania \\
}

\begin{document}

\maketitle
\begin{abstract}
%    This paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks under small initialization. 
    This paper studies the problem of training a two-layer ReLU network for binary classification using gradient flow with small initialization. We consider a training dataset with well-separated input vectors: Any pair of input data with the same label are positively correlated, and any pair with different labels are negatively correlated. Our analysis shows that, during the early phase of training, neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $n$ is the number of data points and $\mu$ measures how well the data are separated. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Numerical experiments on the MNIST dataset illustrate our theoretical findings.
\end{abstract}

\section{Introduction}
\vspace{-1mm}
Neural networks have shown excellent empirical performance in many application domains such as vision~\cite{krizhevsky2012imagenet,rawat2017deep}, speech~\cite{hinton2012deep,graves2013speech} and video games~\cite{silver2016mastering,vinyals2017starcraft}. Despite being highly overparametrized, networks trained by gradient descent with random initialization and without explicit regularization enjoy good generalization performance. One possible explanation for this phenomenon is the implicit bias or regularization induced by first-order algorithms under certain initialization assumptions. For instance, first-order methods applied to (deep) matrix factorization models may produce solutions that have low nuclear norm~\cite{gunasekar2017implicit} and/or low rank~\cite{arora2019implicit}, and similar phenomena have been observed for deep tensor factorization~\cite{razin2022implicit}. Moreover, prior work such as \cite{saxe2014exact,Gidel2019,stoger2021small,li2021towards} has found that deep linear networks sequentially learn the dominant singular values of the input-output correlation matrix. 

It is widely known that these sparsity-inducing biases can often be achieved by small initialization. This has motivated a series of works that theoretically analyze the training dynamics of first-order methods for neural networks with small initialization. For linear networks, the implicit bias of small initialization has been studied in the context of linear regression~\cite{saxe2014exact,Gidel2019,mtvm21icml} and matrix factorization~\cite{gunasekar2017implicit,arora2019implicit,li2018small,li2021towards,stoger2021small,soltanolkotabi2023implicit}. Recently, the effect of small initialization has been studied for two-layer ReLU networks~\cite{maennel2018gradient,lyu2021gradient,phuong2021inductive,boursier2022gradient}. For example, \cite{maennel2018gradient} observes that during the early stage of training, neurons in the first layer converge to one out of finitely many directions determined by the dataset. Based on this observation, \cite{phuong2021inductive} shows that in the case of well-separated data, where any pair of input data with the same label are positively correlated and any pair with different labels are negatively correlated, there are only two directions the neurons tend to converge to: the positive data center and the negative one. Moreover, \cite{phuong2021inductive} shows that if such directional convergence holds, then the loss converges, and the resulting first-layer weight matrix is low-rank. However, directional convergence is assumed in their analysis; there is no explicit characterization of how long it takes to achieve directional convergence and how the time to convergence depends on the initialization scale. 

% \enrique{The above paragraph points out to some limitations of [19], but what's not explicitly said is that the "analysis" of [17] is (at best) valid for vanishing initialization. In the next section, you say that we provide a complete analysis, but I don't think that case that the prior analysis is incomplete is fully fleshed out.}
% \hancheng{left to in-person discussion}
\subsection{Paper contributions}

In this paper, we provide a complete analysis of the dynamics of gradient flow for the problem of training a two-layer ReLU network on well-separated data under the assumption of small initialization. Specifically, we show that if the initialization is sufficiently small, during the early phase of training the neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. Moreover, through a careful analysis of the neuron's directional dynamics we show that the time it takes for all neurons to achieve good alignment with the input data is upper bounded by $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$, where $n$ is the number of data points and $\mu$ measures how well the data are separated. We also show that after the early alignment phase the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate and that the weight matrix on the first layer is approximately low-rank. 
%In this paper, we provide a complete analysis for the problem of training two-layer ReLU networks on well-separated data under small initialization. Specifically, we show under a sufficiently small initialization scale, we show that during the early phase of  training, the neurons in the first layer try to align with either the positive data or the negative data, depending on its corresponding weight on the second layer. A careful analysis of the neurons' directional dynamics allows us to provide an $\mathcal{O}(\log\frac{1}{\epsilon})$ upper bound on the time it takes for all neurons to achieve good alignment with the input data, where $\epsilon$ is the initialization scale. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. 


% \subsection{Other related work}
% \begin{itemize}
%     \item NTK
%     \item convergence and implicit bias of linear networks
% \end{itemize}

\textbf{Notation}: We denote the Euclidean norm of a vector $x$ by $\|x\|$, the inner product between vectors $x$ and $y$ by $\lan x,y\ran=x^\top y$, and the cosine of the angle between them as $\cos(x,y)=\langle \frac{x}{\|x\|},\frac{y}{\|y\|}\rangle$. For an $n\by m$ matrix $A$, we let $A^\top$ denote its transpose. We also let $\|A\|_2$ and $\|A\|_F$ denote the spectral norm and Frobenius norm of $A$, respectively. For a scalar-valued or matrix-valued function of time, $F(t)$, we let $\dot{F}=\dot{F}(t)=\frac{d}{dt}F(t)$ denote its time derivative. Additionally, we define $\one_A$ to be the indicator for a statement $A$: $\one_A=1$ if $A$ is true and $\one_A=0$ otherwise. We also let $I$ denote the identity matrix, and $\mathcal{N}(\mu,\sigma^2)$ denote the normal distribution with mean $\mu$ and variance $\sigma^2$
% and $\text{Unif}(S)$ denote the uniform distribution over a set $S$
.

\section{Preliminaries}
% This paper studies the problem of training a two-layer (single-hidden-layer) ReLU network over some training dataset. In particular, we are interested in training our network by gradient flow with small initialization. 
In this section, we first discuss the problem setting. We then present some key ingredients for analyzing the training dynamics of ReLU networks under small initialization, and discuss some of the weaknesses/issues from prior work.

\subsection{Problem setting}\label{ssec_setting}
We are interested in a binary classification problem with dataset $[x_1,\cdots,x_n]\in\mathbb{R}^{D\times n}$ (input data) and $[y_1,\cdots,y_n]^\top \in \{-1,+1\}^n$ (labels). For the classifier, $f:\mathbb{R}^D\ra \mathbb{R}$, we consider a two-layer ReLU network:
\be
        f(x;W,v)=v^\top \sigma (W^\top x)=\sum\nolimits_{j=1}^hv_j\sigma(w_j^\top x)\,,
\ee
parametrized by network weights $W:=[w_1,\cdots,w_h]\in\mathbb{R}^{D\times h},v:=[v_1,\cdots,v_h]^\top \in\mathbb{R}^{h\times 1}$,
    where $\sigma(\cdot)=\max\{\cdot,0\}$ is the ReLU activation function. We aim to find the network weights that minimize the training loss $
    \mathcal{L}(W,v)=\sum\nolimits_{i=1}^n \ell(y_i,f(x_i;W,v))$, where $\ell: \mathbb{R}\times \mathbb{R}\ra \mathbb{R}_{\geq 0}$ is the exponential loss $\ell(y,\hat{y})=\exp(-y\hat{y})$. The network is trained via the gradient flow (GF) dynamics
    % \begin{align}
    %     \frac{d}{dt}w_j=-\frac{1}{n}\sum_{i=1}^n\one_{\lan x_i,w_j\ran > 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.\\
    %     \frac{d}{dt}v_j=-\frac{1}{n}\sum_{i=1}^n\one_{\lan x_i,w_j\ran > 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.
    % \end{align}
    \be\label{eq_gf}
        \dot{W}\in \partial_W\mathcal{L}(W,v),\ \dot{v}\in \partial_v\mathcal{L}(W,v),
    \ee
    where $\partial_W\mathcal{L},\partial_v\mathcal{L}$ are Clark sub-differentials of $\mathcal{L}$. Therefore, \eqref{eq_gf} is a differential inclusion~\cite{bolte2010characterizations}. The work of~\cite{boursier2022gradient} shows that there exist global solutions to \eqref{eq_gf} if one uses $\sigma'(x)=\one_{x>0}$ as the ReLU subgradient. Therefore, we follow this choice of subgradient for our analysis.

    To initialize the weights, we consider the following initialization scheme. First, we start from a weight matrix $W_0\in\mathbb{R}^{D\times h}$ 
    % with bounded norm $\|W_0\|\leq W_{\max}$}\footnote{\color{red}the norm bound $W_{\max}$ can be arbitrarily large, we will omit the dependency on $W_{\max}$ when presenting our main results (will be shown in Appendix).}
    , and then and then initialize the weights as
    \be
        W(0)=\epsilon W_0,\quad\ v_j(0)\in\{\|w_j(0)\|,-\|w_j(0)\|\}, \forall j\in[h]\,.\label{eq_init}
    \ee
    That is, the weight matrix $W_0$ determines the initial shape of the first-layer weights $W(0)$ and we use $\epsilon$ to control the initialization scale and we are interested in the regime where $\epsilon$ is sufficiently small. For the second layer weights $v(0)$, 
    each $v_j(0)$ has magnitude $\|w_j(0)\|$ and we only need to decide its sign. Our results in later sections are stated for a deterministic choice of $\epsilon, W_0$, and $v(0)$, then we comment on the case where $W_0$ is chosen randomly via some distribution.
    
    The resulting weights in \eqref{eq_init} are always "balanced", i.e., $v_j^2(0)-\|w_j(0)\|^2=0,\forall j\in[h]$, because $v_j(0)$ can only take two values: either $\|w_j(0)\|$ or $-\|w_j(0)\|$. More importantly, under GF~\eqref{eq_gf}, this balancedness is preserved~\cite{Du&Lee}: $v_j^2(t)-\|w_j(t)\|^2=0,\forall t\geq 0, \forall j\in[h]$. In addition, it is shown in~\cite{boursier2022gradient} that $\sign(v_j(t))=\sign(v_j(0)), \forall t\geq 0,\forall j\in[h]$, and the dynamical behaviors of neurons will be divided into two types, depending on $\sign(v_j(0))$.
    
    
    % To initialize the weights, we consider the following initialization scheme. First, we randomly sample a matrix  $W_0\in\mathbb{R}^{D\times h}$ such that $[W_0]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,1/D\rp$, and then initialize the weights as
    % \be
    %     W(0)=\epsilon W_0,\quad\ v_j(0)=\text{Unif}(\{-1,+1\})\|w_j(0)\|, \forall j\in[h]\,.\label{eq_init}
    % \ee
    % Through this initialization, we can control the initialization scale via $\epsilon$ and we are interested in the regime where $\epsilon$ is sufficiently small. Moreover, the resulting weights are always "balanced", i.e., $v_j^2(0)-\|w_j(0)\|^2=0,\forall j\in[h]$, because $v_j(0)$ can only take two values: either $\|w_j(0)\|$ or $-\|w_j(0)\|$. More importantly, under GF~\eqref{eq_gf}, this balancedness is preserved~\cite{Du&Lee}: $v_j^2(t)-\|w_j(t)\|^2=0,\forall t\geq 0, \forall j\in[h]$. In addition, it is shown in~\cite{boursier2022gradient} that $\sign(v_j(t))=\sign(v_j(0)), \forall t\geq 0,\forall j\in[h]$, and the dynamical behaviors of neurons will be divided into two types, depending on $\sign(v_j(0))$.
    
    \begin{remark}
        For our theoretical results, the balancedness condition is assumed for technical purposes: it simplifies the dynamics of GF and thus the analysis. It is a common assumption for many existing works on both linear~\cite{arora2018optimization} and nonlinear~\cite{phuong2021inductive,boursier2022gradient} neural networks. For the experiments in Section \ref{sec_num}, we use a standard Gaussian initialization with small variance, which is not balanced.
    \end{remark}
    \begin{remark}
        Without loss of generality, we consider the case where all columns of $W_0$ are nonzero, i.e., $\|w_j(0)\|>0,\forall j\in[h]$. We make this assumption because whenever $w_j(0)=0$, we also have $v_j(0)=0$ from the balancedness, which together would imply $\dot{v}_j\equiv 0,\dot{w}_j\equiv 0$ under gradient flow. As a result, $w_j$ and $v_j$ would remain zero and thus they could be ignored in the convergence analysis.
    \end{remark}
    \begin{remark}
    {\color{black}Our main results will depend on both $\max_j\|w_j(0)\|$ and $\min_j\|w_j(0)\|$, as shown in our proofs in Appendices \ref{app_pf_thm_align} and \ref{app_pf_thm_conv}. Therefore, whenever we speak of small initialization, we will say that $\epsilon$ is small without worrying about the scale of $W_0$, which is already considered in our results.}
    \end{remark}

    \subsection{Neural alignment with small initialization: an overview}\label{ssec_conv_pre}

    Prior work argues that the gradient flow dynamics~\eqref{eq_gf} under small initialization~\eqref{eq_init}, i.e., when $\epsilon$ is sufficiently small, can be roughly described as "align then fit"~\cite{maennel2018gradient,boursier2022gradient} (We also refer readers to Appendix \ref{app_pf_lem_small_norm} for a detailed explanation with dynamical equations for $\|w_j\|$ and $\frac{w_j}{\|w_j\|}$): During the early phase of training, every neuron $w_j,j\in[h]$ keeps a small norm $\|w_j\|^2\ll 1$ while changing their directions $\frac{w_j}{\|w_j\|}$ significantly in order to locally maximize a "signed coverage"~\cite{maennel2018gradient} of itself w.r.t. the training data.
    % This so-called alignment phase lasts $\Theta(\log \frac{1}{\epsilon})$.
    After the alignment phase, part of the neurons (potentially all neurons) start to grow their norms in order to fit the training data, and the loss decreases significantly. The analysis for the fitting phase generally depends on the resulting neuron directions at the end of the alignment phase~\cite{phuong2021inductive,boursier2022gradient}. %Therefore, developing a rigorous analysis of the alignment phase is key to fully understand GF with small initialization. 
    However, prior analysis of the alignment phase either is based on a vanishing initialization argument that can not be directly translated into the case finite but small initialization~\cite{maennel2018gradient} or assumes some stringent assumption on the data~\cite{boursier2022gradient}. 
    In this section, we provide a brief overview of the existing analysis for neural alignment and then point out several weaknesses in prior work.
    
    \begin{wrapfigure}{r}{0.4\textwidth}
    % \vspace{-3mm}
    \centering
    % Figure removed
      \caption{Illustration of $\frac{d}{dt}\frac{w_j}{\|w_j\|}$ during the early alignment phase. $x_1$ has $+1$ label, and $x_2,x_3$ have $-1$ labels, $x_1,x_2$ lie inside the halfspace $\lan x,w_j\ran>0$ (gray shaded), thus $x_a(w_j)=x_1-x_2$. Since $\sign(v_j(0))>0$, GF pushes $w_j$ towards $x_a(w_j)$.}
      \label{fig_dir_deriv}
        \vspace{-0.4cm}
    \end{wrapfigure}
    \textbf{Prior analysis of the alignment phase:}
    Since during the alignment phase all neurons have small norm, prior work mainly focuses on the directional dynamics, i.e.,  $\frac{d}{dt}\frac{w_j}{\|w_j\|}$, of the neurons. The analysis relies on the following approximation of the dynamics of every neuron $w_j,j\in[h]$:
    \be
        \frac{d}{dt} \frac{w_j}{\|w_j\|}\simeq \sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j)\,,\label{eq_dir_flow}
    \ee
    where $\mathcal{P}_{w}=I-\frac{ww^\top }{\|w\|^2}$ is the projection onto the subspace orthogonal to $w$ and 
    \be
    x_a(w):= \sum\nolimits_{i: \lan x_i,w\ran> 0}y_ix_i
    \ee
    denotes the signed combination of the data points activated by $w$. 
% {\color{red}As long as we pick $\epsilon$ small enough such that the approximation in \eqref{eq_dir_flow} is valid\footnote{This also requires $\mathcal{P}_{w_j(t)} x_a(w_j(t))$ to be non-vanishing at any $t\in[0,T]$, and showing this depends on some specific assumption on the training data.} for all $0\leq t\leq T$, it is easier to analyze the neural alignment with \eqref{eq_dir_flow}}:
First of all, \eqref{eq_dir_flow} implies that the dynamics $\frac{w_j}{\|w_j\|}$ are approximately decoupled, and thus one can study each $\frac{w_j}{\|w_j\|}$ separately. Moreover, as illustrated in Figure \ref{fig_dir_deriv}, if $\sign(v_j(0))>0$, the flow \eqref{eq_dir_flow} pushes $w_j$ towards $x_a(w_j)$, since $w_j$ is attracted by its currently activated positive data and repelled by its currently activated negative data. Intuitively, during the alignment phase, a neuron $w_j$ with $\sign(v_j(0))>0$ would try to find a direction where it can activate as much positive data and as less negative data as possible. If $\sign(v_j(0))<0$, the opposite holds.

Indeed,~\cite{maennel2018gradient} claims that the neuron $w_j$ would be aligned with some "extreme vectors," defined as vector $w\in\mathbb{S}^{D-1}$ that locally maximizes $\sum_{i\in[n]} y_i\sigma(\lan x_i,w\ran)$ (similarly, $w_j$ with $\sign(v_j(0))<0$ would be aligned with the local minimizer), and there are only finitely many such vectors; thus the neurons are expected to converge to one of these extreme vectors in direction. The analysis is done by taking the limit $\epsilon\ra 0$ on the initialization scale, under which the approximation in~\eqref{eq_dir_flow} is exact. 

\textbf{Weakness in prior analyses}: Although~\cite{maennel2018gradient} provides great insights into the dynamical behavior of the neurons in the alignment phase, the validity of the aforementioned approximation for finite but small $\epsilon$ remains in question. First, one needs to make sure that the error $\lV \frac{d}{dt} \frac{w_j}{\|w_j\|}- \sign(v_j(0)) \mathcal{P}_{w_j}x_a(w_j)\rV$ is sufficiently small when $\epsilon$ is finite in order to justify \eqref{eq_dir_flow} as a good approximation. Second, the error bound needs to hold for the entire alignment phase.~\cite{maennel2018gradient} assumes $\epsilon\ra 0$; hence there is no formal error bound.
In addition, prior analyses on small initialization~\cite{stoger2021small,boursier2022gradient} suggest the alignment phase only holds for $\Theta(\log\frac{1}{\epsilon})$ time. Thus, the claim in~\cite{maennel2018gradient} would only hold if good alignment is achieved before the alignment phase ends. However, \cite{maennel2018gradient} provides no upper bound on the time it takes to achieve good alignment. Therefore, without a finite $\epsilon$ analysis, ~\cite{maennel2018gradient} fails to fully explain the training dynamics under small initialization. Understanding the alignment phase with finite $\epsilon$ requires additional analytical tools from dynamical systems theory. To the best of our knowledge, this has only been studied under a stringent assumption that all data points are orthogonal to each other~\cite{boursier2022gradient}.
    
\textbf{Goal of this paper}:
In this paper, we want to address some of the aforementioned issues by developing a formal analysis for the early alignment phase with a finite but small initialization scale $\epsilon$. We first discuss our main theorem that shows that a directional convergence can be achieved within bounded time under data assumptions that are less restrictive and have more practical relevance. Then, we discuss the error bound for justifying \eqref{eq_dir_flow} in the proof sketch for the main theorem.

%     Our analysis of the neural alignment is rooted on the following Lemma:
%     \begin{lemma}\label{lem_small_norm_informal}
%     Given some initialization from \eqref{eq_init}, if $\epsilon=\mathcal{O}\lp \frac{1}{\sqrt{h}}\rp$, then exists $T=\Theta\lp\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon}\rp$ such that any solution to the gradient flow dynamics \eqref{eq_gf} satisfies that $\forall t\leq T$,
%     % $\max_j\|w_j(t)\|^2=\mathcal{O}(\frac{\epsilon}{\sqrt{h}})$, $\max_i|f(x_i;W(t),v(t))|=\mathcal{O}(\epsilon \sqrt{h})$ and 
%     \be
%         \max_j\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}-\sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV=\mathcal{O}\lp\epsilon \sqrt{h}\rp\,.
%     \ee
% \end{lemma}
% \enrique{The goal of this section seems to be not clearly defined. From the start up until before the first bold paragraph, it mixes: a. a general  overview of the convergence process at an intuitive level, b. some of our results (Lemma 1), c. some criticism to [17]. 

% Then it has more weaknesses and ends with our goal.

% I would reorder things as follows:

% 1. first, state clearly the qualitative stages of convergence. 

% 2. Stating the missing gaps of in the literature.

% 3. Then stating our goal. and perhaps a overall strategy (maybe including lemma 1 as first step).

% Alternatively, I question whether Lemma 1 should be here. Isn't this one of our contributions?, if so, shouldn't instead be in the next section?
% }
% \hancheng{left to in-person discussion}
    
% \enrique{In the above paragraph, it is difficult to understand the difference between the alignment phase and the directional convergence. This terminology is confusing. I think that a better explanation of the convergence phases at that beginning of this section will help a lot in understanding the difference between the two.}
% \hancheng{left to in-person discussion}

% Indeed, the flow in \eqref{eq_dir_flow} is implicitly trying to locally maximize a signed coverage of $w_j$ on the training data:
% \be
%     \mathcal{H}(w_j)=\sign(v_j(0))\sum
% \ee

\section{Convergence of Two-layer ReLU Networks with Small Initialization}
% As we discussed in Section \ref{ssec_conv_pre}, the convergence analysis for two-layer networks under small initialization heavily depends on some underlying assumption on training data $\{x_i,y_i\}_{i=1}^n$, and ours makes no exception. 

\begin{wrapfigure}{r}{0.32\textwidth}
\centering
  % \vspace{-3mm}
% Figure removed
  \caption{Neuron alignment under data that satisfies Assumption \ref{assump_data}. For neurons in $\mathcal{V}_+$, \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {1}}} if it lies inside $\mathcal{S}_-$, then it gets repelled by $x_-$ and eventually escapes $\mathcal{S}_-$; Once it is outside $\mathcal{S}_-$, it may \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {2}}} get continuously repelled by some negative data and eventually enters $\mathcal{S}_\text{dead}$. or \raisebox{.5pt}{\textcircled{\raisebox{-.9pt} {3}}} gain some activation on positive data and eventually enter $\mathcal{S}_+$, after which it gets constantly attracted by $x_+$.}
  \label{fig_dir_flow}
  \vspace{-2mm}
\end{wrapfigure}
In this section, we present our main results, which require the following assumption on the training data (we will compare our assumption with those in prior work after the main theorem):
\begin{assumption}\label{assump_data}
Any pair of input data with the same label are positively correlated, and any pair of inputs with different labels are negatively correlated, i.e.,
    \be
        \min_{i,j}\frac{\lan x_iy_i,x_jy_j \ran}{\|x_i\|\|x_j\|}:=\mu>0.
    \ee
\end{assumption}
% For now, we want to point out that, under Assumption 1, the training data is linearly separable, i.e., $\exists \beta, s.t. (\beta^\top x_i)y_i>0, \forall i$, thus even a linear classifier would achieve zero training loss. Our goal, however, is to understand how gradient flow on a two-layer ReLU network would lead to a different classifier that better captures the intrinsic property of the training data. 
Given a training dataset, we define $\mathcal{S}_+:=\{z\in \mathbb{R}^D:\one_{\lan x_i,z\ran> 0}=\one_{y_i>0},\forall i\}$ to be the cone in $\mathbb{R}^n$ such that whenever neuron $w\in\mathcal{S}_+$, $w$ is activated exclusively by every $x_i$ with a positive label (see Figure \ref{fig_dir_flow}). Similarly, for $x_i$ with negative labels, we define $\mathcal{S}_-:=\{z\in \mathbb{R}^D:\one_{\lan x_i,z\ran> 0}=\one_{y_i<0},\forall i\}$. Finally, we define $\mathcal{S}_{\text{dead}}:=\{z\in\mathbb{R}^D:\lan z,x_i\ran\leq 0,\forall i\}$ to be the cone such that whenever $w\in \mathcal{S}_{\text{dead}}$, no data activates $w$. Given Assumption \ref{assump_data}, it can be shown (see Appendix \ref{app_pf_thm_align}) that $\mathcal{S}_+$ ($\mathcal{S}_-$) is a non-empty, convex cone that contains all positive data $x_i,i\in\mathcal{I}_+$ (negative data $x_i,i\in\mathcal{I}_-$). $\mathcal{S}_\text{dead}$ is a convex cone as well, but not necessarily non-empty. We illustrate these cones in Figure \ref{fig_dir_flow} given some training data (red solid arrow denotes positive data and blue denotes negative ones). 

% \enrique{It takes some time to get used to $\mathcal{S}_+, \mathcal{S}_- and \mathcal{S}_{dead}$. I'm wondering if it would be better to first define $\mathcal{S}_+$, together with its properties, pointing to Figure 2, then do the same with $\mathcal{S}_-$ and $\mathcal{S}_{dead}$, respectively. Also, when defining the set of indices, I would say the meaning with words like $\mathcal{I}_+$ is the set of indices of positive data, etc. }
% \enrique{$J_+$ appears in figure 1 in the previous section. Also, I think it would be easier to remember the set $\mathcal{V}_+$ if it was called $\mathcal{V}_+$.}

Moreover, given some initialization from \eqref{eq_init}, we define $\mathcal{I}_+:=\{i\in[n]:y_i>0\}$ to be the set of indices of positive data, and $\mathcal{I}_-:=\{i\in[n]:y_i<0\}$ for negative data. We also define $\mathcal{V}_+:=\{j\in[h]:\sign(v_j(t))>0\}$ to be the set of indices of neurons with positive second-layer entry and $\mathcal{V}_-:=\{j\in[h]:\sign(v_j(t))<0\}$ for neurons with negative second-layer entry. Note that, as discussed in Section \ref{ssec_setting}, $\sign(v_j(t))$ does not change under balanced initialization, thus $\mathcal{V}_+, \mathcal{V}_-$ are time invariant.
Further, as we discussed in Section \ref{ssec_conv_pre} about the early alignment phase, we expect that every neuron in $\mathcal{V}_+$ will drift toward the region where positive data concentrate and thus eventually reach $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$, as visualized in Figure \ref{fig_dir_flow} ($x_+,x_-$ shown in the figure are defined in Assumption \ref{assump_non_degen}).   Similarly, all neurons in $\mathcal{V}_-$ would chase after negative data and thus  reach $\mathcal{S}_-$ or $\mathcal{S}_\text{dead}$. Our theorem precisely characterizes this behavior.  

\subsection{Main results}\label{ssec_main}
Before we present our main theorem, we need the following assumption on the initialization, mostly for technical reasons.
\begin{assumption}\label{assump_non_degen}
    The initialization from \eqref{eq_init} satisfies that $\max_{j\in \mathcal{V}_+}\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_-}{\|x_-\|}\rangle<1$, and

    \noindent
    $\max_{j\in \mathcal{V}_-}\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_+}{\|x_+\|}\rangle<1$, where $x_+=\sum_{i\in\mathcal{I}_+}x_i$ and $x_-=\sum_{i\in\mathcal{I}_-}x_i$. 
\end{assumption}
Assumption \eqref{assump_non_degen} essentially asks the neuron $w_j(0), j\in\mathcal{V}_+$ (or $w_j(0), j\in\mathcal{V}_-$, resp.) to not be completely aligned with $x_+$ (or $x_-$, resp.). 
% We will explain how Assumption \ref{assump_non_degen} plays a role in the analysis later.

% Since we randomly sample the entries of $W_0$ with i.i.d. Gaussians, the initialization from \eqref{eq_init} will satisfy Assumption \eqref{assump_non_degen} with probability one.

% \enrique{It sounds to me that to get the rates, you will have to require the inner products to be $\leq 1-\gamma$ for some $\gamma>0$. My concern is for the case where $\gamma\ll \epsilon$. Then, I don't see how you escape the saddle/unstable equilibrium.}
% \hancheng{The gap will show up in the upper bound for $\epsilon$}
% \enrique{But my point is that Assumption 2 is not correct. Because Assumption 2 allows for a gap of, e.g., $e^{-1/\epsilon}$. Do you still get $t_1=\mathcal{O}(\log(1/\epsilon))$ in that case?}
% \hancheng{The flow is the following, "give me the initialization with a gap $\gamma$" that could be close to zero, then my results hold for any $\epsilon=\mathcal{O}(\exp\lp-\frac{1}{\gamma}\rp)$}
% \enrique{Is $\gamma$ the gap in assumption 2? HM: yes. Ok I see.}
% \hancheng{I will have a remark to discuss how sufficiently small $\epsilon$ should be and what affects it. EM: Ok, sounds good.}
We are now ready to present our main result (given Assumption \ref{assump_data} and Assumption \ref{assump_non_degen}):
\begin{theorem}\label{thm_conv_main}
    Given some initialization from \eqref{eq_init}, if $\epsilon=\mathcal{O}( \frac{1}{\sqrt{h}}\exp( -\frac{n}{\sqrt{\mu}}\log n))$, then any solution to the gradient flow dynamics \eqref{eq_gf} satisfies 
    \begin{enumerate}[leftmargin=0.5cm]
        \item (Directional convergence in early alignment phase) $\exists t_1=\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$, such that 
        \begin{itemize}[leftmargin=0.2cm]
            \item $\forall j\in\mathcal{V}_+$, either $w_j(t_1)\in \mathcal{S}_+$ or $w_j(t_1)\in \mathcal{S}_{\text{dead}}$. Moreover, if $\max_{i\in\mathcal{I}_+}\lan w_j(0),x_i\ran>0$, then $w_j(t_1)\in \mathcal{S}_+$.
            \item $\forall j\in\mathcal{V}_-$, either $w_j(t_1)\in \mathcal{S}_-$ or $w_j(t_1)\in \mathcal{S}_{\text{dead}}$. Moreover, if $\max_{i\in\mathcal{I}_-}\lan w_j(0),x_i\ran>0$, then $w_j(t_1)\in \mathcal{S}_-$.
        \end{itemize}
        \item (Final convergence and low-rank bias) $\forall t\geq t_1$ and $\forall j\in[h]$, neuron $w_j(t)$ stays within $\mathcal{S}_+$ ($\mathcal{S}_-$, or $\mathcal{S}_{\text{dead}}$) if $w_j(t_1)\in\mathcal{S}_+$ ($\mathcal{S}_-$, or $\mathcal{S}_{\text{dead}}$ resp.). Moreover, if both $\mathcal{S}_+$ and $\mathcal{S}_-$ contains at least one neuron at time $t_1$, then
        \begin{itemize}[leftmargin=0.2cm]
            % \item $\exists\alpha>0$ and $\exists t_2$ with $t_1\leq t_2=\Theta\lp\log\frac{1}{\epsilon}\rp$ such that $\mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1},\ \forall t\geq t_2$.
            \item $\exists\alpha>0$ and $\exists t_2$ with $t_1\leq t_2=\Theta (\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$, such that $\mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1},\ \forall t\geq t_2$.
            \item As $t\ra \infty$, $\|W(t)\|\ra \infty$ and $
                \|W(t)\|_F^2\leq 2\|W(t)\|^2_2+\mathcal{O}(\epsilon)$. Thus, the stable rank of $W(t)$ satisfies $\lim\sup_{t\ra \infty}\|W(t)\|_F^2/\|W(t)\|^2_2\leq 2$.
        \end{itemize}
    \end{enumerate}
\end{theorem}
We provide a proof sketch that highlights the technical novelty of our results in Section \ref{ssec_pf_sketch}. Our $\mathcal{O}(\cdot)$ notations hide additional constants that depend on the data and initialization, for which we refer readers to the complete proof of Theorem \ref{thm_conv_main} in Appendix \ref{app_pf_thm_align} and \ref{app_pf_thm_conv}. We make the following remarks:

\textbf{Early neuron alignment}: The first part of the Theorem \ref{thm_conv_main} describes the configuration of \emph{all} neurons at the end of the alignment phase. Every neuron in $\mathcal{V}_+$ reaches either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$ by $t_1$, and stays there for the remainder of training. Obviously, we care about those neurons reaching $\mathcal{S}_+$ as any neuron in $\mathcal{S}_\text{dead}$ does not contribute to the final convergence at all. Luckily, Theorem \ref{thm_conv_main} suggests that any neuron in $\mathcal{V}_+$ that starts with some activation on the positive data, i.e., it is initialized in the union of halfspaces $\cup_{i\in\mathcal{I}_+}\{w: \lan w,x_i\ran>0\}$, will eventually reach $\mathcal{S}_+$. 
% (which happens with at least probability $1/2$, under random initialization as in \eqref{eq_init})
A similar discussion holds for neurons in $\mathcal{V}_-$. We argue that randomly initializing $W_0$ ensures that with high probability, there will be at least a pair of neurons reaching $\mathcal{S}_+$ and $\mathcal{S}_-$ by time $t_1$ (please see the next remark).  Lastly, we note that it is possible that $\mathcal{S}_\text{dead}=\emptyset$, in which case every neuron reaches either $\mathcal{S}_+$ or $\mathcal{S}_-$. 
% \enrique{Can you state the conditions for $\mathcal{S}_{dead}$ being empty?}

\textbf{Merits of random initialization}: Our theorem is stated for a deterministic initialization \eqref{eq_init} given an initial shape $W_0$. In practice, one would use random initialization to find a $W_0$, for example, $[W_0]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,1/D\rp$. First, our Theorem \ref{thm_conv_main} applies to this Gaussian initialization: Assumption \ref{assump_non_degen} is satisfied with probability one because the events $\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_-}{\|x_-\|}\rangle=1$ and $\langle \frac{w_j(0)}{\|w_j(0)\|},\frac{x_+}{\|x_+\|}\rangle=1$ have probability zero. Moreover, any neuron in $\mathcal{V}_+$ has at least probability $1/2$ of being initialized within the union of halfspaces $\cup_{i\in\mathcal{I}_+}\{w: \lan w,x_i\ran>0\}$, which ensures that this neuron reaches $\mathcal{S}_+$. Thus when there are $m$ neurons in $\mathcal{V}_+$, the probability that $\mathcal{S}_+$ has at least one neuron at time $t_1$ is lower bounded by $1-2^{-m}$ (same argument holds for $\mathcal{S}_-$), Therefore, with only very mild overparametrization on the network width $h$, one can make sure that with high probability there is at least one neuron in both $\mathcal{S}_+$ and $\mathcal{S}_-$, leading to final convergence.

\textbf{Duration of alignment phase}: Our theorem shows that for sufficiently small $\epsilon$, directional convergence, i.e., all neurons reaching either $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$, is achieved within $\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ time (notably, independent of $\epsilon$). Our bound quantitatively reveals the non-trivial dependency on the "data separation" $\mu$ for such directional convergence to occur. To the best of our knowledge, this is the first non-asymptotic bound on the time it takes for all neurons to achieve a desired configuration.~\cite{maennel2018gradient} only shows such $t_1>0$ exists using an $\epsilon\ra 0$ argument, without analyzing how large $t_1$ can be.~\cite{boursier2022gradient} studies a different data assumption (we compare it with ours in later remarks) under which the alignment is studied only for neurons that has a specific activation pattern at initialization. Lastly, we note that $\mu \ra 0$ leads to $t_1\ra \infty$, this is because when $\mu=0$, there are more limiting directions to which neurons can converge, hence not all of them are "attracted" by $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$.


\textbf{Refined alignment within $\mathcal{S}_+,\mathcal{S}_-$}: Once a neuron in $\mathcal{V}_+$ reaches $\mathcal{S}_+$, it never leaves $\mathcal{S}_+$. Moreover, it always gets attracted by $x_+$. Therefore, every neuron gets well aligned with $x_+$, i.e., $\cos(w_j,x_+)\simeq 1,\forall w_j\in\mathcal{S}_+$. A similar argument shows neurons in $\mathcal{S}_-$ get attracted by $x_-$. We opt not to formally state it in Theorem \ref{thm_conv_main} as the result would be similar to that in~\cite{boursier2022gradient}, and alignment with $x_+,x_-$ is not necessary to guarantee convergence. Instead, we show this refined alignment through our numerical experiment in Section \ref{sec_num}.
% \enrique{I'm confused as to whether this stage is at all needed. Once you get $\mathcal{S}_+$ can't we just apply your other work? This stage does not seem necessary to guarantee convergence. }
% \hancheng{this section is stated for explaining the numerical experiments because the experiments show the alignment with $x_+,x_-$.}

\textbf{Final convergence and low-rank bias}: The convergence analysis after $t_1$ is simple: All neurons in $\mathcal{S}_\text{dead}$ have small norm and do not move thus they can be ignored from the analysis. More interestingly, GF after $t_1$ can be viewed as fitting positive data $x_i,i\in\mathcal{I}_+$, with a subnetwork consisting of all neurons in $\mathcal{S}_+$, and fitting negative data with neurons in $\mathcal{S}_-$. By the fact that all neurons in $\mathcal{S}_+$ activate all $x_i,i\in\mathcal{I}_+$, the resulting subnetwork is linear, and so is the subnetwork for fitting $x_i,i\in\mathcal{I}_-$. The convergence analysis reduces to establishing $\mathcal{O}(1/t)$ convergence for two linear networks~\cite{arora2018convergence,mtvm21icml,yun2020unifying}. As for the stable rank, our result follows the analysis in~\cite{le2022training}, but in a simpler form since ours is for linear networks. 
%Note that~\cite{phuong2021inductive} shows the resulting approximately low-rank solution is related to the two respective max-margin classifiers of positive and negative data.

% \hancheng{$\epsilon\leq \min\{\frac{1}{\sqrt{h}}\exp\lp -\frac{CnX_{\max}}{\zeta \sqrt{\mu} X_{\min}}\log n\rp, \frac{\min\{\mu,\zeta\}\sqrt{\mu}}{8\sqrt{h}nX_{\max}^2W_{\max}^2}\}$}
\textbf{Comparison with~\cite{phuong2021inductive}}: Prior work~\cite{phuong2021inductive} considers a similar data assumption to ours. However,~\cite{phuong2021inductive} assumes that there exists a time $t_1$ such that at $t_1$, the neurons are in either $\mathcal{S}_+,\mathcal{S}_-$ or $\mathcal{S}_\text{dead}$ and their main contribution is the analysis of the implicit bias for the later stage of the training.~\cite{phuong2021inductive} justifies their assumption by the analysis in~\cite{maennel2018gradient}, which does not necessarily apply to the case of finite $\epsilon$, as we discussed in Section \ref{ssec_conv_pre}.
% However,~\cite{phuong2021inductive} assumes \hl{directional convergence} in~\cite{maennel2018gradient} for their analysis, which does not necessarily apply to the case of finite $\epsilon$, as we discussed in Section \ref{ssec_conv_pre}. 
Our work precisely establishes such directional convergence for finite but small $\epsilon$, showing indeed the neurons achieve some good alignment with $x_+,x_-$ within $\mathcal{O}( \frac{\log n}{\sqrt{\mu}})$ time before they start to grow in norm. Moreover,~\cite{phuong2021inductive} has no characterization on the convergence rate of the loss after the alignment phase, while we provide a $\mathcal{O}(1/t)$ bound on the loss. In addition,~\cite{phuong2021inductive} considers the case where input data $x_i,i\in[n]$, spans the entire $\mathbb{R}^{D}$, which leads to $\mathcal{S}_\text{dead}=\emptyset$. This implicitly imposes the constraint that the number of data points $n$ must be larger than the input dimension $D$. Our analysis allows for the case $\mathcal{S}_\text{dead}\neq\emptyset$ as we provide a sufficient condition for preventing a neuron from reaching $\mathcal{S}_\text{dead}$.
% \enrique{I think that \cite{phuong2021inductive} chooses a slightly weaker condition. It seems to me there's a reason why you need to have a slightly stricter condition, and that the argument. It is necessary to have a stricter condition to ensure a finite $\frac{1}{\varepsilon}$ time. }
% \hancheng{left to in-person discussion}

\textbf{Comparison with~\cite{boursier2022gradient}}: In~\cite{boursier2022gradient}, the neuron alignment is carefully analyzed for the case all data points are orthogonal to each other, i.e., $\lan x_i,x_j\ran=0, \forall i\neq j\in[n]$. Such an assumption restricts the number of data points $n$ to be smaller than the input dimension $D$ and is often unrealistic. Our assumption does not restrict the size of the dataset and thus has more practical relevance (see our numerical experiments in Section \ref{sec_num}). 


\subsection{Proof sketch for the alignment phase}\label{ssec_pf_sketch}
In this section, we sketch the proof for our Theorem \ref{thm_conv_main}. First of all, it can be shown that $\mathcal{S}_+,\mathcal{S}_{\text{dead}}$ are trapping regions for all $w_j(t),j\in\mathcal{V}_+$, that is, whenever $w_j(t)$ gets inside $\mathcal{S}_+$ (or $\mathcal{S}_{\text{dead}}$), it never leaves $\mathcal{S}_+$ (or $\mathcal{S}_{\text{dead}}$). Similarly, $\mathcal{S}_-,\mathcal{S}_{\text{dead}}$ are trapping regions for all $w_j(t),j\in\mathcal{V}_-$. The alignment phase analysis concerns how long it takes for all neurons to reach one of the trapping regions, followed by the final convergence analysis on fitting data with $+1$ label by neurons in $\mathcal{S}_+$ and fitting data with $-1$ label by those in $\mathcal{S}_-$. We have discussed the final convergence analysis in the remark "Final convergence and low-rank bias", thus we focus on the proof sketch for the early alignment phase here, which is considered as our main technical contribution.

\textbf{Approximating $\frac{d}{dt}\frac{w_j}{\|w_j\|}$}: Our analysis for the neural alignment is rooted in the following Lemma:
\begin{lemma}\label{lem_small_norm_informal}
Given some initialization from \eqref{eq_init}, if $\epsilon=\mathcal{O}( \frac{1}{\sqrt{h}})$, then there exists $T=\Theta(\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$ such that any solution to the gradient flow dynamics \eqref{eq_gf} satisfies that $\forall t\leq T$,
% $\max_j\|w_j(t)\|^2=\mathcal{O}(\frac{\epsilon}{\sqrt{h}})$, $\max_i|f(x_i;W(t),v(t))|=\mathcal{O}(\epsilon \sqrt{h})$ and 
\be
    \max_j\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}-\sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV=\mathcal{O}\lp\epsilon n\sqrt{h}\rp\,.\label{eq_err}
\ee
\end{lemma}
This Lemma shows that the error between $\frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}$ and $\sign(v_j(0))\mathcal{P}_{w_j(t)} x_a(w_j(t))$ can be arbitrarily small with some appropriate choice of $\epsilon$ (to be determined later). This allows one to analyze the true directional dynamics $\frac{w_j(t)}{\|w_j(t)\|}$ using some property of $\mathcal{P}_{w_j(t)} x_a(w_j(t))$,
which leads to a $t_1=\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for the neuron direction to converge to the sets $\mathcal{S}_+$, $\mathcal{S}_-$, or $\mathcal{S}_{\text{dead}}$. Moreover, it also suggests $\epsilon$ can be made sufficiently small so that the error bound holds until the directional convergence is achieved, i.e. $T\geq t_1$. We will first illustrate the analysis for directional convergence, then close the proof sketch with the choice of a sufficiently small $\epsilon$.

% This lemma suggests how small $\epsilon$ should be in order to approximate $\frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}$ as in \eqref{eq_dir_flow} so that one can analyze the true directional dynamics $\frac{w_j(t)}{\|w_j(t)\|}$ using some property of $\mathcal{P}_{w_j(t)} x_a(w_j(t))$, which leads to a $t_1=\mathcal{O}(\frac{\log n}{\sqrt{\mu}})$ upper bound on the time it takes for the neuron direction to converge to the sets $\mathcal{S}_+$, $\mathcal{S}_-$, $\mathcal{S}_{\text{dead}}$. Moreover, it also suggests $\epsilon$ can be made sufficiently small so that the error bound holds until the directional convergence is achieved, i.e. $T\geq t_1$. We will first illustrate the analysis for directional convergence, then close the proof sketch with the choice of sufficiently small $\epsilon$.

\textbf{Activation pattern evolution:}~
Given a sufficiently small $\epsilon$, one can show that under Assumption \ref{assump_data}, for every neuron $w_j$ that is not in $\mathcal{S}_\text{dead}$ we have:
\be
\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}>0,\forall i\in [n], \text{if } j\in \mathcal{V}_+\,,\label{eq_mono_pos}
\ee
    \be
    \left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}<0,\forall i\in [n], \text{if } j\in \mathcal{V}_-\,.\label{eq_mono_neg}
\ee
This is because whenever a neuron satisfies $\lan x_i,w_j\ran=0$ for some $i$, and has activation on some other data, GF moves $w_j$ towards $x_a(w_j)=\sum_{i:\lan x_i, w_j\ran>0}x_iy_i$. Interestingly, Assumption \ref{assump_data} implies $\lan x_iy_i,x_a(w_j)\ran>0,\forall i\in[n]$, which makes $\frac{d}{dt}\frac{w_j}{\|w_j\|}\simeq \sign(v_j(0))\mathcal{P}_{w_j}x_a(w_j)$ point inward (or outward) the halfspace $\lan x_iy_i,w_j\ran>0$, if $\sign(v_j(0))>0$ (or $\sign(v_j(0))<0$, respectively). See Figure \ref{fig_mono_activation} for illustration.
% \enrique{Figure three does not seem to have a number.}

% Figure environment removed

As a consequence, a neuron can only change its activation pattern in a particular manner: a neuron in $\mathcal{V}_+$, whenever it is activated by some $x_i$ with $y_i=+1$, never loses the activation on $x_i$ thereafter, because \eqref{eq_mono_pos} implies that GF pushes $\frac{w_j}{\|w_j\|}$ towards $x_i$ at the boundary $\lan w_j,x_i\ran=0$. Moreover, \eqref{eq_mono_pos} also shows that a neuron in $\mathcal{V}_+$ will never regain activation on a $x_i$ with $y_i=-1$ once it loses the activation because GF pushes $\frac{w_j}{\|w_j\|}$ against $x_i$ at the boundary $\lan w_i,x_i\ran=0$. Similarly, a neuron in $\mathcal{V}_-$ never loses activation on negative data and never gains activation on positive data.

\textbf{Bound on activation transitions and duration:}
Equations \eqref{eq_mono_pos} and \eqref{eq_mono_neg} are key in the analysis of alignment because they limit how many times a neuron can change its activation pattern: a neuron in $\mathcal{V}_+$ can only gain activation on positive data and lose activation on negative data, thus at maximum, a neuron $w_j,\ j\in\mathcal{V}_+$, can start with full activation on all negative data and no activation on any positive one (which implies $w_j(0)\in\mathcal{S}_-$) then lose activation on every negative data and gain activation on every positive data as GF training proceeds (which implies $w_j(t_1)\in\mathcal{S}_+$), taking at most $n$ changes on its activation pattern. See Figure \ref{fig_activation_traj} for an illustration. 
Then, since it is possible to show that a neuron $w_j$ with $j\in \mathcal{V}_+$ that has $\cos(w_j,x_-)<1$ (guaranteed by Assumption \ref{assump_non_degen}) and is not in $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$, must change its activation pattern after $\mathcal{O}( \frac{1}{n_a\sqrt{\mu}})$ time (that does not depend on $\epsilon$), where $n_a$ is the number of data that currently activates $w_j$, one can upper bound the time for $w_j$ to reach $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$ by some $t_1=\mathcal{O}( \frac{\log n}{\sqrt{\mu}})$ constant independent of $\epsilon$. 
% Thus by making $\epsilon$ sufficiently small, one can make $t_1$ larger than this time.
% Moreover, we can show that this neuron $w_j$ must change its activation pattern after $\mathcal{O}\lp \frac{1}{n_a}\rp$ time (that does not depend on $\epsilon$), where $n_a$ is the number of data that currently activates $w_j$, unless either it reaches $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$, from which one can upper bound the time for $w_j$ to reach $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$ by some $\mathcal{O}\lp \log n\rp$ constant, and $\epsilon$ only needs to be sufficiently small for $t_1$ to be larger this constant time. 
Moreover, $w_j$ must reach $\mathcal{S}_+$ if it initially has activation on at least one positive data, i.e., $\max_{i\in\mathcal{I}_+}\lan w_j(0),x_i\ran>0$ since it cannot lose this activation. A similar argument holds for $w_j, j\in\mathcal{V}_-$ that they reaches either $\mathcal{S}_-$ or $\mathcal{S}_{\text{dead}}$ before $t_1$. 

\textbf{Choice of $\epsilon$:}
All the aforementioned analyses rely on the assumption that the approximation in equation \eqref{eq_dir_flow} holds with some specific error bound. We show in Appendix \ref{app_pf_thm_align} that the desired bound is
$\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}- \sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV\leq \mathcal{O}(\sqrt{\mu})$, which, by Lemma \ref{lem_small_norm_informal}, can be achieved by a sufficiently small initialization scale $\epsilon_1=\mathcal{O}(\frac{\sqrt{\mu}}{\sqrt{h}n})$. Moreover, the directional convergence (which takes $\mathcal{O}( \frac{\log n}{\sqrt{\mu}})$ time) should be achieved before the alignment phase ends, which happens at $T=\Theta(\frac{1}{n}\log\frac{1}{\sqrt{h}\epsilon})$. This is ensured by choosing another sufficiently small initialization scale $\epsilon_2=\mathcal{O}( \frac{1}{\sqrt{h}}\exp( -\frac{n}{\sqrt{\mu}}\log n))$. Overall, the initialization scale should satisfy $\epsilon\leq \min\{\epsilon_1,\epsilon_2\}$. We opt to present $\epsilon_2$ in our main theorem because $\epsilon_2$ beats $\epsilon_1$ when $n$ is large.

% {\color{red} \textbf{Technical contribution}: To be completed...

% \textbf{Sufficiently small $\epsilon$}: We further discuss here the upper bound on $\epsilon$ in Theorem \ref{thm_conv_main}. More precisely, we require $\epsilon\leq \min\lb\frac{C_2\min\{\mu,\zeta_1,\zeta_2\}\sqrt{\mu}}{\sqrt{h}n}, \frac{1}{\sqrt{h}}\exp\lp -\frac{C_1n}{\min\{\zeta_1,\zeta_2\} \sqrt{\mu}}\log n\rp\rb$, where $C_1, C_2$ depends on data and neuron norms $\|x_i\|$,$\|w_j\|$ and we refer readers to Appendix \ref{app_pf_thm_align} for the full definition. First, we would point out there are essentially two upper bounds for $\epsilon$: the first $\mathcal{O}(\frac{1}{\sqrt{h}n})$ is required for a good approximation of $\frac{d}{dt}\frac{w_j}{\|w_j\|}$ as in \eqref{eq_dir_flow}, which enables a careful analysis for the alignment phase; the second $\mathcal{O}(\frac{1}{\sqrt{h}\exp(-n\log n)})$ is required for the alignment phase to last long enough so that all neurons reaches either $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$.

% Here we would like to highlight in the upper bound the dependence on $\mu$, characterizing how data are separated, and on two quantities: $\zeta_1=1-\max_{w\in \mathcal{S}_+^c\cap \mathcal{S}_-^c\cap\mathcal{S}_\text{dead}^c}\cos(w,x_a(w))$, and $\zeta_2=\min\{1-\max_{j\in\mathcal{V}_+}\cos(w_j(0),x_-),1-\max_{j\in\mathcal{V}_-}\cos(w_j(0),x_+)\}$. $\zeta_1$ depends solely on the data and Assumption \ref{assump_data} ensures $\zeta_1>0$; $\zeta_2$ depends on the initialization and Assumption \ref{assump_non_degen} ensures $\zeta_2>0$.

% }
% \enrique{This is a good place, to make a more refined comparison, perhaps highlighting some of the differences with the literature.}

% \enrique{How is Assumption 2 being used? There should be a reference to it at some point.}


\section{Numerical Experiments}\label{sec_num}

% {\color{red}[...we did great experiments to validate our theorem...]}
% \subsection{Binary classification on two MNIST digits}
\subsection{Illustrative example}
We first illustrate our theorem using a toy example: we train a two-layer ReLU network with $h=50$ neurons under a toy dataset in $\mathbb{R}^2$ (See Figure. \ref{fig_illust_ex}) that satisfies our Assumption \ref{assump_data}, and initialize all entries of the weights as $[W]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha\rp, v_j\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha\rp,\forall i\in[n],j\in[h]$ with $\alpha=10^{-6}$. Then we run gradient descent on both $W$ and $v$ with step size $\eta=2\times 10^{-3}$. Our theorem well predicts the dynamics of neurons at the early stage of the training: aside from neurons that ended up in $\mathcal{S}_\text{dead}$, neurons in $\mathcal{V}_+$ reach $\mathcal{S}_+$ and achieve good alignment with $x_+$, and neurons in $\mathcal{V}_-$ are well aligned with $x_-$ in $\mathcal{S}_-$. Note that after alignment, the loss experiences two sharp decreases before it gets close to zero, which is studied and explained in~\cite{boursier2022gradient}.


\subsection{Binary classification on two MNIST digits}
Next, we consider a binary classification task for two MNIST digits. Such training data do not satisfy Assumption \ref{assump_data} since every data vector is a grayscale image with non-negative entries, making the inner product between any pair of data non-negative, regardless of their labels. However, we can preprocess the training data by centering: $x_i\leftarrow x_i-\bar{x}$, where $\bar{x}=\sum_{i\in[n]}x_i/n$. The preprocessed data, then,  approximately satisfies our assumption (see the left-most plot in Figure \ref{fig_mnist}): a pair of data points is very likely to have a positive correlation if they have the same label and to have a negative correlation if they have different labels. Thus we expect our theorem to make reasonable predictions on the training dynamics with preprocessed data. 
% Figure environment removed
For the remaining part of this section, we use $x_i,i\in[n]$, to denote the preprocessed (centered) data and use $\bar{x}$ to denote the mean of the original data. Here, we pick digits $1$ and $0$ for the numerical experiment, and we present additional experiments with different choices of digits in Appendix \ref{app_addi_experiments}.

We build a two-layer ReLU network with $h=50$ neurons and initialize all entries of the weights as $[W]_{ij}\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha\rp, v_j\overset{i.i.d.}{\sim} \mathcal{N}\lp 0,\alpha\rp,\forall i\in[n],j\in[h]$ with $\alpha=10^{-5}$. Then we run gradient descent on both $W$ and $v$ with step size $\eta=2\times 10^{-3}$. Notice that here the weights are not initialized to be balanced as in \eqref{eq_init}. The numerical results are shown in Figure \ref{fig_mnist}. 

\textbf{Alignment phase}: Without balancedness, one no longer has $\sign(v_j(t))=\sign(v_j(0))$. With a little abuse of notation, we denote $\mathcal{V}_+(t)=\{j\in[h]:\sign(v_j(t))>0\}$ and $\mathcal{V}_+(t)=\{j\in[h]:\sign(v_j(t))>0\}$, and we expect that at the end of the alignment phase, neurons in $\mathcal{V}_+$ are aligned with $x_+=\sum_{i\in\mathcal{I}_+}x_i$, and neurons in $\mathcal{V}_-$ with $x_-=\sum_{i\in\mathcal{I}_-}x_i$. The second plot in Figure \ref{fig_mnist} shows such an alignment between neurons and $x_+,x_-$. In the top part, the red solid line shows $\cos(\bar{w}_+,x_+)$ during training, where $\bar{w}_+=\sum_{j\in\mathcal{V}_+}w_j/|\mathcal{V}_+|$, and the shaded region defines the range between $\min_{j\in\mathcal{V}_+}\cos(w_i,x_+)$ and $\max_{j\in\mathcal{V}_+}\cos(w_i,x_+)$. Similarly,  in the bottom part, the green solid line shows $\cos(\bar{w}_-,x_-)$ during training, where $\bar{w}_-=\sum_{j\in\mathcal{V}_-}w_j/|\mathcal{V}_-|$, and the shaded region delineates the range between $\min_{j\in\mathcal{V}_-}\cos(w_i,x_-)$ and $\max_{j\in\mathcal{V}_-}\cos(w_i,x_-)$. Initially, every neuron is approximately orthogonal to $x_+,x_-$ due to random initialization. Then all neurons in $\mathcal{V}_+$ ($\mathcal{V}_-$) start to move towards $x_+$ ($x_-$) and achieve good alignment after ${\sim}2000$ iterations. When the loss starts to decrease (after ${\sim 3000}$ iterations), the alignment drops a little. We conjecture that this is because the dataset does not exactly satisfy our Assumption \ref{assump_data}, and the neurons in $\mathcal{V}_+$ have to fit some negative data, for which $x_+$ is not the best direction.

\textbf{Final convergence}: After ${\sim 3000}$ iterations, the norm $\|W\|_2^2$ starts to grow and the loss decreases, as shown in the third plot in Figure \ref{fig_mnist}. Moreover, the stable rank $\|W\|_F^2/\|W\|_2^2$ decreases below $2$. For this experiment, we almost have $\cos(x_+,x_-)\simeq -1$, thus the neurons in $\mathcal{V}_+$ (aligned with $x_+$) and those in $\mathcal{V}_-$ (aligned with $x_-$) are almost co-linear. Therefore, the stable rank $\|W\|_F^2/\|W\|_2^2$ is almost $1$, as seen from the plot. Finally, at iteration $15000$, we visualize the mean neuron $\bar{w}_+=\sum_{j\in\mathcal{V}_+}w_j/|\mathcal{V}_+|$, $\bar{w}_-=\sum_{j\in\mathcal{V}_-}w_j/|\mathcal{V}_-|$ as grayscale images, and compare them with $\bar{x}_+=x_+/|\mathcal{I}_+|,x_-=x_-/|\mathcal{I}_-|$, showing good alignment. We also show the images when the original data center $\bar{x}$ is added back.


% 
\section{Conclusion}
\vspace{-3mm}
This paper studies the problem of training a binary classifier via gradient flow on two-layer ReLU networks under small initialization. We consider a training dataset with well-separated input vectors. A careful analysis of the neurons' directional dynamics allows us to provide an upper bound on the time it takes for all neurons to achieve good alignment with the input data. After the early alignment phase, the loss converges to zero at a $\mathcal{O}(\frac{1}{t})$ rate, and the weight matrix on the first layer is approximately low-rank. Lastly, the numerical experiment on classifying two digits from the MNIST dataset correlates with our theoretical findings. Future directions include extending our results for gradient descent and considering multi-class classification problems. 

% First, we would like to point out that although we show, via numerical experiments, that our theorem works even when Assumption \ref{assump_data} approximately holds, the assumption that data are well-separated is still limiting since it can not explain why training with uncentered data still achieve good generalization performance. Besides, the network architecture considered in this paper is simple fully connected two-layer ReLU network, and it is unclear whether a similar analysis for neuron alignment would work for deep networks. Addressing these limitations is also an interesting future research direction.


\bibliographystyle{plain}
\bibliography{./ref.bib}

\newpage
\appendix
\section{Additional Experiments}\label{app_addi_experiments}
\subsection{Additional experiments on MNIST dataset}
We use exactly the same experimental setting as in the main paper and only use a different pair of digits. The results are as follows:
% Figure environment removed
% Figure environment removed
\subsection{Error bars}
We provide here error bars for the numerical experiments in Section \ref{sec_num}. We run each experiment 20 times and report the mean loss curve below, the shaded region presents the values within 3 times the standard deviation.
% Figure environment removed

\newpage
\section{Proof of Lemma \ref{lem_small_norm_informal}}\label{app_pf_lem_small_norm}
The following property of the exponential loss $\ell$ will be used throughout the Appendix for proofs of several results:
    \begin{lemma}\label{assump_loss}
        For exponential loss $\ell$, we have
        % \ell$ is differentiable w.r.t. its second argument and given a label set $\mathcal{Y}\subseteq \mathbb{R}$, $\exists C,\xi$ s.t.
        \be
            |-\nabla_{\hat{y}}\ell (y,\hat{y})-y|\leq 2|\hat{y}|,\forall y\in\{+1,-1\},\quad \forall |\hat{y}|\leq 1\,.
        \ee
    \end{lemma}
    \begin{proof}
        \begin{align*}
            |-\nabla_{\hat{y}}\ell (y,\hat{y})-y|&=\;|y\exp(-y\hat{y})-y|\\
            &\leq\; |y||\exp(-y\hat{y})-1|\\
            &\leq\; |\exp(-y\hat{y})-1|\leq 2|\hat{y}|\,,
        \end{align*}
        where the last inequality is due to the fact that $2x\geq \max\{1-\exp(-x),\exp(x)-1\},\forall x\in[0,1]$.
    \end{proof}
\subsection{Formal statement}
% Denote: $M_x=\max_i\|x_i\|, M_w=\max_j\|[W_0]_{:,j}\|$.
% \begin{lemma}\label{lem_small_norm}
%     Let $\mathcal{Y}=\{y:|y|\leq 1\}$ and $C,\xi$ be the constant from Assumption \ref{assump_loss}. then given any $\delta\leq \xi$, $\epsilon\leq \frac{\delta}{(1+\delta C)hM_xM_w^2}$, any solution to the gradient flow dynamics \eqref{eq_gf} starting from initialization \eqref{eq_init} satisfies
%     \be
%         \max_j\|w_j(t)\|^2\leq \frac{\delta}{hM_x},\quad \max_i|f(x_i;W(t),v(t))|\leq \delta\,,\quad \forall t\leq \frac{1}{2M_x}\log\frac{1}{\epsilon}
%     \ee
% \end{lemma}
% \begin{lemma}
%     Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Given any $\delta\leq \xi$, whenever $\max_i|f(x_i;W(t),v(t))|\leq \delta$, we have
%     \be
%         \lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0))\lp I-\frac{w_jw_j^\top  }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran\geq 0}y_ix_i\rp\rV\leq \tilde{C}\delta\,,
%     \ee
%     for some $\tilde{C}>0$.
% \end{lemma}
Denote: $X_{\max}=\max_i\|x_i\|, W_{\max}=\max_j\|[W_0]_{:,j}\|$. The formal statement of Lemma \ref{lem_small_norm_informal} is as follow:
\begin{customlem}{1}
    Given some initialization from \eqref{eq_init}, for any $\epsilon\leq \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}$, then any solution to the gradient flow dynamics \eqref{eq_gf} satisfies that $\forall t\leq T=\frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}
    $,
    % $\max_j\|w_j(t)\|^2=\mathcal{O}(\frac{\epsilon}{\sqrt{h}})$, $\max_i|f(x_i;W(t),v(t))|=\mathcal{O}(\epsilon \sqrt{h})$ and 
    \ben
        \max_j\lV \frac{d}{dt} \frac{w_j(t)}{\|w_j(t)\|}-\sign(v_j(0)) \mathcal{P}_{w_j(t)} x_a(w_j(t))\rV\leq 4\epsilon n\sqrt{h}X_{\max}^2W_{\max}^2\,.
    \een
\end{customlem}

Lemma \ref{lem_small_norm_informal} is a direct result of the following two lemmas.
\begin{lemma}\label{lem_small_norm}
     Given some initialization in \eqref{eq_init}, then for any $\epsilon\leq \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}$, any solution to the gradient flow dynamics \eqref{eq_gf} satisfies
    \be
        \max_j\|w_j(t)\|^2\leq \frac{2\epsilon W_{\max}^2}{\sqrt{h}},\quad \max_i|f(x_i;W(t),v(t))|\leq 2\epsilon \sqrt{h}X_{\max}W_{\max}^2\,,
    \ee
    $\forall t\leq \frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}
    $.
\end{lemma}
\begin{lemma}\label{lem_dir_flow_approx}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Whenever $\max_i|f(x_i;W,v)|\leq 1$, we have, $\forall i\in[n]$,
    \be
        \lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0)) \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran> 0}y_ix_i\rp\rV\leq 2n X_{\max} \max_i|f(x_i;W,v)|\,.
    \ee
\end{lemma}
\subsection{Proof of Lemma \ref{lem_small_norm} and Lemma \ref{lem_dir_flow_approx}}
\begin{proof}[Proof of Lemma \ref{lem_small_norm}]
    Under gradient flow, we have
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.
    \ee
    Balanced initialization enforces $v_j=\sign(v_j(0))\|w_j\|$, hence
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
    \ee
    Let $T:=\inf\{t:\ \max_{i}|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\}$,  then $\forall t\leq T, j\in[h]$, we have 
    \begin{align}
        \frac{d}{dt}\|w_j\|^2
        &=\;\lan w_j, \frac{d}{dt}w_j \ran & \nonumber\\
        &=\; -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|&\nonumber\\
        &\leq\; 2\sum_{i=1}^n\lvt\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\rvt \lvt\lan x_i, w_j\ran\rvt \|w_j\|&\nonumber\\
        &\leq\; 2\sum_{i=1}^n(|y_i|+2|f(x_i;W,v)|) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{by Lemma \ref{assump_loss}})\nonumber\\
        &\leq\; 2\sum_{i=1}^n(1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{Since } t\leq T)\nonumber\\
        &\leq\; 2\sum_{i=1}^n(1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2) \|x_i\|\|w_j\|^2& \nonumber\\
        &\leq \; 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\|w_j\|^2\,.&
    \end{align}
    Let $\tau_j:=\inf\{t: \|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}\}$, and let $j^*:=\arg\min_j \tau_j$, then $\tau_{j^*}=\min_{j}\tau_j\leq T$ due to the fact that 
    \ben
        |f(x_{i};W,v)|= \lvt\sum_{j\in[h]}\one_{\lan w_j,x_i\ran>0}v_j\lan w_j,x_i\ran\rvt\leq \sum_{j\in[h]} \|w_j\|^2\|x_i\|\leq hX_{\max}\max_{j\in[h]}\|w_j\|^2\,,
    \een
    which implies "$|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\Rightarrow \exists j, s.t.\|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}$". 
    
    Then for $t\leq \tau_{j^*}$, we have
    \be
        \frac{d}{dt}\|w_{j^*}\|^2\leq 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\|w_{j^*}\|^2\,.
    \ee
    By Gr\"onwall's inequality, we have $\forall t\leq \tau_{j^*}$
    \begin{align*}
        \|w_{j^*}(t)\|^2&\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\|w_{j^*}(0)\|^2\,,\\
         &=\;\exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\epsilon^2\|[W_0]_{:,j^*}\|^2\\
         &\leq \;\exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )t\rp\epsilon^2W_{\max}^2\,.
    \end{align*}
    Suppose $\tau_{j^*}<\frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$, then by the continuity of $\|w_{j^*}(t)\|^2$, we have 
    \begin{align*}
        \frac{2\epsilon W_{\max}^2}{\sqrt{h}}\leq \|w_{j^*}(\tau_{j^*})\|^2&\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\tau_{j^*}\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 2n(X_{\max}+4\epsilon\sqrt{h}X_{\max}^2W_{\max}^2 )\frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 
        \frac{1+4\epsilon \sqrt{h}X_{\max}W_{\max}^2}{2}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2\\
        &\leq\; \exp\lp 
        \log\lp\frac{1}{\sqrt{h}\epsilon}\rp\rp\epsilon^2W_{\max}^2=\frac{\epsilon W_{\max}^2}{\sqrt{h}}\,,
    \end{align*}
    which leads to a contradiction $2\epsilon\leq \epsilon$. Therefore, one must have $T\geq \tau_{j^*}\geq \frac{1}{4nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$. This finishes the proof.
\end{proof}
% \begin{proof}[Proof of Lemma \ref{lem_small_norm}]
%     Under gradient flow, we have
%     \be
%         \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_iv_j\,.
%     \ee
%     Balanced initialization enforces $v_j=\sign(v_j(0))\|w_j\|$, hence
%     \be
%         \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
%     \ee
%     Let $T:=\inf\{t:\ \max_{i}|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\}$,  then $\forall t\leq T, j\in[h]$, we have 
%     \begin{align}
%         \frac{d}{dt}\|w_j\|^2
%         &=\;\lan w_j, \frac{d}{dt}w_j \ran & \nonumber\\
%         &=\; -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|&\nonumber\\
%         &\leq\; 2\sum_{i=1}^n\lvt\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\rvt \lvt\lan x_i, w_j\ran\rvt \|w_j\|&\nonumber\\
%         &\leq\; 2\sum_{i=1}^n(|y_i|+2|f(x_i;W,v)|) \lvt\lan x_i, w_j\ran\rvt \|w_j\|& (\text{by Lemma \ref{assump_loss}})\nonumber\\
%         &\leq\; 2\sum_{i=1}^n(1+2hX_{\max} \|w_j\|^2) \|x_i\|\|w_j\|^2&\nonumber\\
%         &\leq \; 2n(X_{\max}+2hX_{\max}^2 \|w_j\|^2)\|w_j\|^2\,,&
%     \end{align}
% %     where the second last inequality uses the fact that
% %     \be
% %     f(x_{i};W,v)= \sum_{j\in[h]]}\one_{\lan w_j,x_i\ran>0}v_j\lan w_j,x_i\ran\leq \sum_{j\in[h]} \|w_j\|^2\|x_i\|\leq hX_{\max}\|w_j\|\,,
% % \ee
%     % For simplicity, we let $M_x:=\max_{i}\|x_i\|$.
    
%     Let $\tau_j:=\inf\{t: \|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}\}$, and let $j^*:=\arg\min_j \tau_j$, then $\tau_{j^*}=\min_{j}\tau_j\leq T$ due to the fact that "$|f(x_i;W(t),v(t))|>2\epsilon \sqrt{h}X_{\max}W_{\max}^2\Rightarrow \exists j, s.t.\|w_j(t)\|^2>\frac{2\epsilon W_{\max}^2}{\sqrt{h}}$". 
    
%     Then for $t\leq \tau_{j^*}$, we have
%     \be
%         \frac{d}{dt}\|w_{j^*}\|^2\leq 2(X_{\max}+ChX_{\max}^2 \|w_{j^*}\|^2)\|w_{j^*}\|^2\,,
%     \ee
%     The ode $\frac{d}{dt}z=2(a+b z)z, z(t)=z_0\geq 0, a,b>0$ has solution on finite interval
%     \be
%         z(t)=\frac{a \exp(2 a t)z_0}{a+b z_0-b \exp(2 a t)z_0}\,, \forall t<\frac{1}{2a}\log\lp \frac{a}{bz_0}+1\rp\,,\label{eq_app_lem_norm_ode}
%     \ee
%     then we have
%     \begin{align}
%         \|w_{j^*}(t)\|^2&\leq\; \frac{X_{\max} \exp(2 n X_{\max} t)\|w_{j^*}(0)\|^2}{X_{\max}+2hX_{\max}^2 (1-\exp(2 nX_{\max} t)\|w_{j^*}(0)\|^2}\nonumber\\
%         &=\; \frac{\exp(2 nX_{\max} t)\epsilon^2\|[W_0]_{:,j^*}\|^2}{1+2hX_{\max} (1-\exp(2n X_{\max} t))\epsilon^2\|[W_0]_{:,j^*}\|^2}\,.\label{eq_thm1_pf_1}
%     \end{align}
%     When $t=\frac{1}{2nX_{\max}}\log\frac{1}{\sqrt{h} \epsilon}$, the right-hand side of \eqref{eq_thm1_pf_1} reduces to, then upper bounded as
%     \begin{align}
%         &\;\frac{\epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}}{1+2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2-2\sqrt{h}X_{\max}\epsilon\|[W_0]_{:,j^*}\|^2}\nonumber\\
%         &\;\leq \frac{\epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}}{1+2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2-2\sqrt{h}X_{\max}\|[W_0]_{:,j^*}\|^2\lp \frac{1}{4\sqrt{h} X_{\max}W_{\max}^2}\rp}\nonumber\\
%         &\;\leq \frac{\epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}}{\frac{1}{2}+2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2}\leq \epsilon\|[W_0]_{:,j^*}\|^2/\sqrt{h}\leq \epsilon W_{\max}^2/\sqrt{h}\,.\nonumber
%         % &\;\frac{\exp(2 M_x t)\epsilon^2\|[W_0]_{:,j^*}\|^2}{1+ChM_x (1-\exp(2 M_x t))\epsilon^2\|[W_0]_{:,j^*}\|^2}\nonumber\\
%         % =&\;\frac{\epsilon\|[W_0]_{:,j^*}\|^2}{1+ChM_x \epsilon(1-\epsilon)\|[W_0]_{:,j^*}\|^2}
%     \end{align}
%     As long as $t=\frac{1}{2nX_{\max}}\log\frac{1}{\sqrt{h} \epsilon}$ is within the finite time interval in \eqref{eq_app_lem_norm_ode}. Indeed, we have 
%     \begin{align*}
%         t&\;= \frac{1}{2nX_{\max}}\log\frac{1}{\sqrt{h} \epsilon}\\
%         &\;\leq \frac{1}{2nX_{\max}}\log\lp\frac{1}{4hX_{\max}\epsilon^2W_{\max}^2}\rp\\
%         &\;\leq \frac{1}{2nX_{\max}}\log\lp \frac{1}{2hX_{\max}\epsilon^2\|[W_0]_{:,j^*}\|^2}+1\rp\,,
%     \end{align*} 
%     where the last term is precisely the time epoch before which the bound in \eqref{eq_thm1_pf_1} holds.
    
%     Finally, suppose $\tau_{j^*}<\frac{1}{2nX_{\max}}\log\lp\frac{1}{\epsilon}\rp$, then by the continuity of $\|w_{j^*}(t)\|^2$, we have 
%     \begin{align}
%         \frac{2\epsilon W_{\max}^2}{\sqrt{h}}\leq \|w_{j^*}(\tau_{j^*})\|^2&\leq\; \epsilon W_{\max}^2/\sqrt{h}\,,
%     \end{align}
%     which leads to a contradiction $2\epsilon\leq \epsilon$. Therefore, one must have $T\geq \tau_{j^*}\geq \frac{1}{2nX_{\max}}\log\lp\frac{1}{\sqrt{h}\epsilon}\rp$. This finishes the proof.
% \end{proof}
\begin{proof}[Proof of Lemma \ref{lem_dir_flow_approx}]
    As we showed in the proof for Lemma \ref{lem_small_norm}, under balanced initialization, 
    \be
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\sign(v_j(0))\|w_j\|\,.
    \ee
    Then for any $i\in[n]$,
    \begin{align*}
        \frac{d}{dt} \frac{w_j}{\|w_j\|}&=\; -\sign(v_j(0))\sum_{i=1}^n\one_{\lan x_i,w_j\ran> 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lp x_i-\frac{\lan x_i,w_j\ran}{\|w_j\|^2}w_j \rp\\
        &=\; -\sign(v_j(0))\sum_{i: \lan x_i,w_j\ran>0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lp x_i-\frac{\lan x_i,w_j\ran}{\|w_j\|^2}w_j \rp\\
        &=\; -\sign(v_j(0))\lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp\lp\sum_{i: \lan x_i,w_j\ran>0} \nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\rp\,.
    \end{align*}
    Therefore, whenever $\max_i|f(x_i;W,v)|\leq 1$,
    \begin{align}
        &\;\lV \frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0))\lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran> 0}y_ix_i\rp\rV\nonumber\\
        =&\;\lV \sign(v_j(0))\lp\sum_{i: \lan x_i,w_j\ran>0} \lp\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))+y_i\rp x_i\rp\rV\nonumber\\
        \leq &\; \sum_{i=1}^n |\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))+y_i |\cdot \|x_i\|\nonumber\\
        \leq &\; \sum_{i=1}^n2|f(x_i;W,v)|\cdot \|x_i\|\leq 2nM_x \max_i|f(x_i;W,v)|\,.
    \end{align}
\end{proof}

\newpage
\section{Proof for Theorem \ref{thm_conv_main}: Early Alignment Phase}\label{app_pf_thm_align}
We break the proof of Theorem \ref{thm_conv_main} into two parts: In Appendix \ref{app_pf_thm_align} we prove the first part regarding directional convergence. Then in Appendix \ref{app_pf_thm_conv} we prove the remaining statement on final convergence and low-rank bias.
\subsection{Auxiliary lemmas}
The first several Lemmas concern mostly some conic geometry given the data assumption:

Consider the following conic hull
 \be
 K=\mathcal{CH}(\{x_iy_i,i\in[n]\})=\lb\sum_{i=1}^na_ix_iy_i: a_i\geq 0,i\in[n]\rb\,.
 \ee
It is clear that $x_iy_i\in K,\forall i$, and $x_a(w)\in K,\forall w$. 
The following lemma shows any pair of vectors in $K$ is $\mu$-coherent.
 \begin{lemma}\label{lem_app_K_coherence}
    $\cos(z_1,z_2)\geq \mu,\forall 0\neq z_1,z_2\in K$.
\end{lemma}
\begin{proof}
    Since $z_1,z_2\in K$, we let $z_1=\sum_{i=1}^nx_iy_ia_{1i}$, and$z_2=\sum_{j=1}^nx_jy_ja_{2j}$, where $a_{1i},a_{2j}\geq 0$ but not all of them.
    \begin{align*}
        \cos(z_1,z_2)= \frac{1}{\|z_1\|\|z_2\|}\lan z_1,z_2\ran
        &=\; \frac{1}{\|z_1\|\|z_2\|}\sum_{i,j\in[n]}a_{1i}a_{2j}\lan x_iy_i,x_jy_j\ran\\
        &=\; \frac{\sum_{i,j\in[n]}\|x_i\|\|x_j\|a_{1i}a_{2j}\mu}{\|z_1\|\|z_2\|}\geq \mu\,,
    \end{align*}
    where the last inequality is due to $$\|z_1\|\|z_2\|\leq \lp\sum_{i=1}^n\|x_i\|a_{1i}\rp\lp\sum_{j=1}^n\|x_j\|a_{2j}\rp=\sum_{i,j\in[n]}\|x_i\|\|x_j\|a_{1i}a_{2j}\,.$$
\end{proof}
The following lemma is some basic results regarding $\mathcal{S}_+$ and $\mathcal{S}_-$:
\begin{lemma}
    $\mathcal{S}_+$ and $\mathcal{S}_-$ are convex cones (excluding the origin).
\end{lemma}
\begin{proof}
    Since $\one_{\lan x_i,z\ran}=\one_{\lan x_i,az\ran},\forall i\in[n],a>0$, $\mathcal{S}_+,\mathcal{S}_-$ are cones. Moreover, $\lan x_i,z_1\ran > 0$ and $\lan x_i,z_2\ran > 0$ implies $\lan x_i,a_1z_1+a_2z_2\ran> 0,\forall a_1,a_2>0$, thus $\mathcal{S}_+,\mathcal{S}_-$ are convex cones.
\end{proof}
Now we consider the complete metric space $\mathbb{S}^{D-1}$ (w.r.t. $\arccos(\lan\cdot,\cdot\ran)$) and we are interested in its subsets $K\cap \mathbb{S}^{D-1}$, $\mathcal{S}_+\cap \mathbb{S}^{D-1}$, and $\mathcal{S}_-\cap \mathbb{S}^{D-1}$. First, we have (we use $\mathrm{Int}(S)$ to denote the interior of $S$)
\begin{lemma}\label{lem_app_int}
    $K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1})$, and $-K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_-\cap \mathbb{S}^{D-1})$
\end{lemma}
\begin{proof}
    Consider any $x_c=\sum_{j=1}^na_jx_jy_j\in K\cap \mathbb{S}^{D-1}$, For any $x_i,y_i, i\in[n]$, we have 
    \begin{align*}
        \lan x_c,x_i\ran&=\; \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\\
        &\geq\; \mu y_i\|x_i\| \sum_{i=j}^na_j\|x_j\|\begin{cases}
            \geq \mu X_{\min}>0, & y_i>0\\
            \leq -\mu X_{\min}<0, & y_i<0\\
        \end{cases}\,.
    \end{align*}
    Depending on the sign of $y_i$, we have either
    \ben
        \lan x_c,x_i\ran= \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\geq \mu \frac{\|x_i\|}{y_i} \sum_{i=j}^na_j\|x_j\|\geq \mu X_{\min}>0\,,\ (y_i=+1)
    \een
    or 
    \ben
        \lan x_c,x_i\ran= \sum_{i=j}^na_j\|x_j\|\lan \frac{x_jy_j}{\|x_j\|},\frac{x_iy_i}{\|x_i\|}\ran \frac{\|x_i\|}{y_i}\leq \mu \frac{\|x_i\|}{y_i} \sum_{i=j}^na_j\|x_j\|\leq -\mu X_{\min}<0\,,\ (y_i=-1)
    \een
    where we use the fact that $1=\|x_c\|=\|\sum_{j=1}^na_jx_jy_j\|\leq \sum_{j=1}^na_j\|x_j\|$. This already tells us $x_c\in \mathcal{S}_+\cap \mathbb{S}^{D-1}$.
    
    Since $f_i(z)=\lan z, x_i\ran$ is a continuous function of $z\in\mathbb{S}^{D-1}$. There exists an open ball $\mathcal{B}\lp x_c,\delta_i\rp$ centered at $x_c$ with some radius $\delta_i>0$, such that $\forall z\in \mathcal{B}\lp x_c,\delta_i\rp$, one have $\lvt f_i(z)-f_i\lp x_c\rp\rvt\leq \frac{\mu X_{\min}}{2}$, which implies 
    \ben
        \lan z, x_i\ran\begin{cases}
            \geq \mu X_{\min}/2>0, & y_i>0\\
            \leq -\mu X_{\min}/2<0, & y_i<0\\
        \end{cases}\,.
    \een
    Hence $\cap_{i=1}^n\mathcal{B}\lp \frac{x_c}{\|x_c\|},\delta_i\rp\in\mathcal{S}_+\cap \mathbb{S}^{D-1}$. Therefore, $x_c\in \mathrm{Int}(\mathcal{S}_+\cap \mathbb
    {S}^{D-1})$. This suffices to show $K\cap \mathbb
    {S}^{D-1}\subset \mathrm{Int}(\mathcal{S}_+\cap \mathbb
    {S}^{D-1})$. The other statement $-K\cap \mathbb{S}^{D-1} \subset \mathrm{Int}(\mathcal{S}_-\cap \mathbb{S}^{D-1})$ is proved similarly.
\end{proof}
The following two lemmas are some direct results of Lemma \ref{lem_app_int}.
\begin{lemma}\label{lem_app_zeta1}
    $\exists \zeta_1>0$ such that 
    \be
        \mathcal{S}_{x_+}^{\zeta_1}\subset \mathcal{S_+},\qquad\mathcal{S}_{x_-}^{\zeta_1}\subset \mathcal{S_-}\,,
    \ee
    where $\mathcal{S}_x^\zeta:=\{z\in\mathbb{R}^D:\ \cos(z,x)\geq \sqrt{1-\zeta}\}$.
\end{lemma}
\begin{proof}
    By Lemma \ref{lem_app_int}, $\frac{x_+}{\|x_+\|}\in K\subset \mathrm{Int}(S_+)$. Since $\mathbb{S}^{D-1}$ is a complete metric space (w.r.t $\arccos \lan \cdot,\cdot\ran$), there exists a open ball centered at $\frac{x_+}{\|x_+\|}$  of some radius $\arccos(\sqrt{1-\zeta_1})$ that is a subset of $\mathcal{S}_+$, from which one can show $\mathcal{S}_{x_+}^{\zeta_1}\subset \mathcal{S}_+$. The other statement $\mathcal{S}_{x_-}^{\zeta_1}\subset \mathcal{S}_-$ simply comes from the fact that $x_+=-x_-$ and $\mathrm{Int}(\mathcal{S}_+)=-\mathrm{Int}(\mathcal{S}_-)$.
\end{proof}

\begin{lemma}\label{lem_app_gamma}
    $\exists \xi>0$, such that
    \be\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_+\cap\mathbb{S}^{D-1})^c\cap (\mathcal{S}_-\cap\mathbb{S}^{D-1})^c}|\cos(x_1,x_2)|\leq \sqrt{1-\xi}\,.\ee
    ($S^c$ here is defined to be $\mathbb{S}^{D-1}-S$, the set complement w.r.t. complete space $\mathbb{S}^{D-1}$)
\end{lemma}
\begin{proof}
    Notice that
    \ben
        \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\lan x_1,x_2\ran=\inf_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\arccos \lan x_1,x_2\ran\,.
    \een
    Since $\mathbb{S}^{D-1}$ is a complete metric space (w.r.t $\arccos \lan \cdot,\cdot\ran$) and $K\cap \mathbb{S}^{D-1}$ and $x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c$ are two of its compact subsets. Suppose 
    \ben
        \inf_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\arccos \lan x_1,x_2\ran=0\,,
    \een
    then $\exists x_1\in K\cap \mathbb{S}^{D-1}, x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c$ such that $\arccos \lan x_1,x_2\ran=0$, i.e., $x_1=x_2$, which contradicts the fact that $ K\cap \mathbb{S}^{D-1}\subseteq \mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1})$ (Lemma \ref{lem_app_int}). Therefore, we have the infimum strictly larger than zero, then
    \be
        \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (S_+\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran\leq \sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathrm{Int}(\mathcal{S}_+\cap \mathbb{S}^{D-1}))^c}\lan x_1,x_2\ran<1\,.
    \ee
    Similarly, one can show that
    \be
        \sup_{x_1\in -K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_-\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran<1\,.
    \ee
    Finally, find $\xi<1$ such that
    \ben
        \max\lb\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_+\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran,\sup_{x_1\in -K\cap \mathbb{S}^{D-1},x_2\in (\mathcal{S}_-\cap \mathbb{S}^{D-1})^c}\lan x_1,x_2\ran\rb=\sqrt{1-\xi}\,,
    \een
    then for any $x_1\in K\cap\mathbb{S}^{D-1}$ and $x_2\in (\mathcal{S}_+\cap\mathbb{S}^{D-1})^c\cap (\mathcal{S}_-\cap\mathbb{S}^{D-1})^c$, we have
    \ben
        -\sqrt{1-\xi}\leq \lan x_1,x_2\ran \leq \sqrt{1-\xi}\,,
    \een
    which is the desired result.
\end{proof}


The remaining two lemmas are technical but extensively used in the main proof.
\begin{lemma}\label{lem_app_psi_rj}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $x_r\in \mathbb{S}^{n-1}$ be some reference direction, we define
    \be
        \psi_{rj}=\lan x_r,\frac{w_j}{\|w_j\|}\ran,\ \psi_{ra}=\lan x_r, \frac{x_a(w_j)}{\|x_a(w_j)\|}\ran,\ \psi_{aj}=\lan \frac{w_j}{\|w_j\|}, \frac{x_a(w_j)}{\|x_a(w_j)\|}\ran\,,
    \ee
    where $x_a(w_j)=\sum_{i: \lan x_i,w_j\ran> 0}y_ix_i$.
    
    Whenever $\max_i|f(x_i;W,v)|\leq 1$, we have
    \be
        \lvt\frac{d}{dt} \psi_{rj} -\sign(v_j(0))\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \ee
\end{lemma}
\begin{proof}
    A simple application of Lemma \ref{lem_dir_flow_approx}, together with Cauchy-Schwartz:
    \begin{align*}
        &\;\lvt\frac{d}{dt} \psi_{rj} -\sign(v_j(0))\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\\
        &=\; \lvt x_r^\top \lp\frac{d}{dt} \frac{w_j}{\|w_j\|}-\sign(v_j(0)) \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp \lp \sum_{i: \lan x_i,w_j\ran> 0}y_ix_i\rp\rp\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \end{align*}
\end{proof}
\begin{lemma}\label{lem_app_x_a_lb}
    \be
        \|x_a(w)\|\geq \sqrt{\mu}n_a(w) X_{\min}\,,
    \ee
    where $n_a(w)=|\{i\in[n]: \lan x_i,w\ran>0\}|$.
\end{lemma}
\begin{proof}
    Let $\mathcal{I}_a(w)$ denote $\{i\in[n]: \lan x_i,w\ran>0\}$, then
    \begin{align*}
        \|x_a(w)\|= \lV\sum_{i:\lan x_i,w\ran>0} x_iy_i\rV
        &=\; \sqrt{\sum_{i\in \mathcal{I}_a(w)}\|x_i\|^2y_i^2+\sum_{i,j\in \mathcal{I}_a(w), i<j}\|x_i\|\|x_j\|\lan \frac{x_iy_i}{\|x_i\|},\frac{x_jy_j}{\|x_j\|}\ran }\\
        &\geq\; \sqrt{\sum_{i\in \mathcal{I}_a(w)}\|x_i\|^2y_i^2+\sum_{i,j\in \mathcal{I}_a(w), i<j}\|x_i\|\|x_j\||y_i||y_j|\mu }\\
        &\geq\; \sqrt{n_a(w)X_{\min}^2+\mu n_a(w)\lp n_a(w)-1\rp X_{\min}^2}\\
        &\geq\; \sqrt{n_a(w)(1+\mu (n_a(w)-1))} X_{\min}\\
        &\geq\; \sqrt{\mu}n_a(w)X_{\min}\,.
    \end{align*}
\end{proof}
% \begin{lemma}
%     Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Whenever $\max_i|f(x_i;W,v)|\leq \frac{\mu^{\frac{3}{2}}X_{\min}}{4nX_{\max}}$, we have
%     \begin{align}
%     &\;\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}>0,\forall i\in [n], \text{if } j\in \mathcal{V}_+\,,\\
%     &\;\left.\frac{d}{dt}\lan \frac{w_j}{\|w_j\|},\frac{x_iy_i}{\|x_i\|}\ran \right\rvert_{\lan w_i,x_i\ran=0}<0,\forall i\in [n], \text{if } j\in \mathcal{V}_-\,.
% \end{align}
% \end{lemma}
\subsection{Proof for early alignment phase}
\begin{proof}[Proof of Theorem \ref{thm_conv_main}: First Part]Given some initialization in \eqref{eq_init}, by Assumption \ref{assump_non_degen}, $\exists \zeta_2>0$, such that
\be
    \max_{j\in\mathcal{V}_+}\cos(w_j(0),x_-)< \sqrt{1-\zeta_2},\quad \max_{j\in\mathcal{V}_-}\cos(w_j(0),x_+)< \sqrt{1-\zeta_2}\,.\label{eq_app_assump_non_degen}
\ee
We define $\zeta:=\max\{\zeta_1,\zeta_2\}$, where $\zeta_1$ is from Lemma \ref{lem_app_zeta1}. In addition, by Lemma \ref{lem_app_gamma}, $\exists \xi>0$, such that
\be\label{eq_app_ub_gamma}
\sup_{x_1\in K\cap \mathbb{S}^{D-1},x_2\in \mathcal{S}_-^c\cap\mathcal{S}_+^c\cap \mathbb{S}^{D-1}}|\cos(x_1,x_2)|\leq \sqrt{1-\xi}\,.
\ee

We pick a initialization scale $\epsilon$ that satisfies:
\be
    \epsilon\leq \min\lb\frac{\min\{\mu,\zeta,\xi\}\sqrt{\mu}X_{\min}}{4\sqrt{h}nX_{\max}^2W_{\max}^2}, \frac{1}{\sqrt{h}}\exp\lp -\frac{64nX_{\max}}{\min\{\zeta,\xi\} \sqrt{\mu}X_{\min}}\log n\rp\rb\leq \frac{1}{4\sqrt{h}X_{\max}W_{\max}^2}\,.\label{eq_app_ub_epsilon}
\ee
By Lemma \ref{lem_small_norm}, $\forall t\leq T=\frac{1}{4nX_{\max}}\log \frac{1}{\sqrt{h}\epsilon}$, we have 
\be
\max_i|f(x_i;W,v)|\leq \frac{\min\{\mu,\zeta,\xi\}\sqrt{\mu}X_{\min}}{4nX_{\max}}\,,\label{eq_app_ub_f}
\ee
which is the key to analyzing the alignment phase. For the sake of simplicity, we only discuss the analysis of neurons 
in $\mathcal{V}_+$ here, the proof for neurons in $\mathcal{V}_-$ is almost identical.

\textbf{Activation pattern evolution:} Pick any $w_j$ in $\mathcal{V}_+$ and pick $x_r=x_iy_i$ for some $i\in[n]$, and consider the case when $\lan w_j, x_i\ran=0$. From Lemma \ref{lem_app_psi_rj},we have
\ben
    \lvt\frac{d}{dt} \psi_{rj} -\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
$\lan w_j,x_i\ran=0$ implies $\psi_{rj}=\lan \frac{x_iy_i}{\|x_i\|},\frac{w_j}{\|w_j\|}\ran=0$, thus we have
\ben
    \lvt\frac{d}{dt} \psi_{rj}\vert_{\lan w_j,x_i\ran=0} -\psi_{ra}\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
Then whenever $w_j\notin \mathcal{S}_{\text{dead}}$, we have
\begin{align*}
    \frac{d}{dt} \psi_{rj}\vert_{\lan w_j,x_i\ran=0}&\geq\; \psi_{ra}\|x_a(w_j)\|-2nX_{\max}\max_i|f(x_i;W,v)| &\\
    &\geq\; \mu \|x_a(w_j)\| -2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_K_coherence}})\\
    &\geq \; \mu^{3/2}X_{\min}-2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_x_a_lb}})\\
    &\geq\; \mu^{3/2}X_{\min}/2>0\,. &\quad (\text{by \eqref{eq_app_ub_f}})
\end{align*}
This is precisely \eqref{eq_mono_pos} in Section \ref{ssec_pf_sketch}.

\textbf{Bound on activation transitions and duration:}  Next we show that if at time $t_0<T$, $w_j(t_0)\notin\mathcal{S}_+\cup \mathcal{S}_\text{dead}$, and the activation pattern of $w_j$ is $\one_{\lan x_i,w_j(t_0)\ran>0}$, then $\one_{\lan x_i,w_j(t_0+\Delta t))\ran>0}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$, where $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a(w_j(t_0))}$ and $n_a(w_j(t_0))$ is defined in Lemma \ref{lem_app_x_a_lb} as long as $t_0+\Delta t<T$ as well. That is, during the alignment phase $[0, T]$, $w_j$ must change its activation pattern within $\Delta t$ time. There are two cases:
\begin{itemize}[leftmargin=0.4cm]
    \item The first case is when $w_j(t_0)\in \mathcal{S}_+^c\cap \mathcal{S}_-^c\cap \mathcal{S}_\text{dead}^c$. In this case, suppose that $\one_{\lan x_i,w_j(t_0+\tau))\ran>0}= \one_{\lan x_i,w_j(t_0)\ran>0}, \forall 0\leq \tau\leq \Delta t$, i.e. $w_j$ fixes its activation during $[t_0,t_0+\Delta t]$, then we have $x_a(w_j(t_0+\tau))=x_a(w_j(t_0)),\forall 0\leq \tau\leq \Delta t$. Let us pick $x_r=x_a(w_j(t_0))$, then Lemma \ref{lem_app_psi_rj} leads to
    \ben
    \lvt\frac{d}{dt} \cos(w_j,x_a(w_j)) -\lp 1-\cos^2(w_j,x_a(w_j))\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
    \een
    Since $x_a(w_j)$ is fixed, we have $\forall t\in[t_0,t_0+\Delta t]$,
    \begin{align*}
        \lvt\frac{d}{dt} \cos(w_j,x_a(w_j(t_0))) -\lp 1-\cos^2(w_j,x_a(w_j(t_0)))\rp\|x_a(w_j(t_0))\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
    \end{align*}
    \begin{align*}
    \frac{d}{dt} \cos(w_j,x_a(w_j(t_0)))&\geq\; \lp 1-\cos^2(w_j,x_a(w_j(t_0)))\rp\|x_a(w_j(t_0))\|\\
    &\;\qquad\qquad -2nX_{\max}\max_i|f(x_i;W,v)| &\\
    &\geq\; \xi \|x_a(w_j(t_0))\| -2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by \eqref{eq_app_ub_gamma}})\\
    &\geq \; \xi \sqrt{\mu}n_a(w_j(t_0))X_{\min}-2nX_{\max}\max_i|f(x_i;W,v)|&\quad (\text{by Lemma \ref{lem_app_x_a_lb}})\\
    &\geq\; \xi\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2 \,. &\quad (\text{by \eqref{eq_app_ub_f}})\\
    & \geq\; \min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,, &
\end{align*}
which implies that, by the Fundamental Theorem of Calculus,
\begin{align*}
    &\;\cos(w_j(t_0+\Delta t),x_a(w_j(t_0)))\\
    &=\;\cos(w_j(t_0),x_a(w_j(t_0)))+\int_0^{\Delta t}\frac{d}{dt} \cos(w_j(t_0+\tau),x_a(w_j(t_0)))d\tau\\
    &\geq\; \cos(w_j(t_0),x_a(w_j(t_0)))+\Delta t \cdot\min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\\
    &=\; \cos(w_j(t_0),x_a(w_j(t_0)))+2\geq 1\,,
\end{align*}
which leads to $\cos(w_j(t_0+\Delta t),x_a(w_j(t_0)))=1$. This would imply $w_j(t_0+\Delta t)\in\mathcal{S}_+$ because $x_a(w_j(t_0))\in\mathcal{S}_+$, which contradicts our original assumption that $w_j$ fixes the activation pattern. Therefore, $\exists 0<\tau_0\leq \Delta t$ such that $\one_{\lan x_i,w_j(t_0+\tau_0))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$, due to the restriction on how $w_j$ can change its activation pattern, it cannot return to its previous activation pattern, then one must have $\one_{\lan x_i,w_j(t_0+\Delta t))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$.
\item  The other case is when $w_j(t_0)\in\mathcal{S}_-$. For this case, we need first show that $w_j(t_0+\tau)\notin \mathcal{S}_{x_-}^{\zeta},\forall 0\leq \tau\leq \Delta t$, or more generally, $\mathcal{S}_{x_-}^{\zeta}$ does not contain any $w_j$ in $\mathcal{V}_+$ during $[0,T]$. To see this, let us pick $x_r=x_-$, then Lemma \ref{lem_app_psi_rj} suggests that
\ben
    \lvt\frac{d}{dt} \psi_{rj} -\lp \psi_{ra}-\psi_{rj}\psi_{aj}\rp\|x_a(w_j)\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,.
\een
Consider the case when $\cos(w_j,x_-)=\sqrt{1-\zeta}$, i.e. $w_j$ is at the boundary of $\mathcal{S}_{x_-}^{\zeta}$. We know that in this case, $w_j\in \mathcal{S}_{x_-}^{\zeta}\subseteq \mathcal{S}_-$ thus $x_a(w_j)=-x_-$, and
\ben
    \lvt\left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}} +\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
\een
which is
\begin{align*}
    &\;\lvt\left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}} +\zeta\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)| &\\
    \Ra&\; \left.\frac{d}{dt} \cos(w_j,x_-)\right\vert_{\cos(w_j,x_-)=\sqrt{1-\zeta}}\\
    &\leq\; -\zeta\|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)| &\\
     &\leq \;-\zeta\sqrt{\mu}X_{\min}+2nX_{\max}\max_i|f(x_i;W,v)|&(\text{by Lemma \ref{lem_app_x_a_lb}})\\
     &\leq \;-\zeta\sqrt{\mu}X_{\min}/2<0\,. & (\text{by \eqref{eq_app_ub_f}}) 
\end{align*}
Therefore, during $[0,T]$, neuron $w_j$ in $\mathcal{V}_+$ cannot enter $\mathcal{S}_{x_-}^{\zeta}$ if at initialization, $w_j(0)\notin \mathcal{S}_{x_-}^{\zeta}$, which is guaranteed by \eqref{eq_app_assump_non_degen}.

With the argument above, we know that $w_j(t_0+\tau)\notin\mathcal{S}_{x_-}^{\zeta}, \forall 0\leq \tau\leq \Delta t$. Again we suppose that $w_j(t)\in \mathcal{S}_--\mathcal{S}_{x_-}^{\zeta},\forall t\in[t_0,t_0+\Delta t]$, i.e.,$w_j$ fixes its activation during $[t_0,t_0+\Delta t]$. Let us pick $x_r=x_-$, then Lemma \ref{lem_app_psi_rj} suggests that
\ben
    \lvt\frac{d}{dt} \cos(w_j,x_-) +\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|\rvt\leq 2nX_{\max}\max_i|f(x_i;W,v)|\,,
    \een
    which leads to $\forall t\in[t_0,t_0+\Delta t]$,
    \begin{align*}
        \frac{d}{dt} \cos(w_j,x_-)&\leq\; -\lp 1-\cos^2(w_j,x_-)\rp\|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)|&\\
        &\leq\; -\zeta \|x_-\|+2nX_{\max}\max_i|f(x_i;W,v)|& (w_j\notin \mathcal{S}_{x_-}^{\zeta})\\
        &\leq\; -\zeta \sqrt{\mu}n_a(w_j(t_0))X_{\min}+2nX_{\max}\max_i|f(x_i;W,v)|& (\text{by Lemma \ref{lem_app_x_a_lb}})\\
        &\leq\; -\zeta \sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,. & (\text{by \eqref{eq_app_ub_f}})\\
        & \leq\; -\min\{\xi,\zeta\}\sqrt{\mu}n_a(w_j(t_0))X_{\min}/2\,, &
    \end{align*}
    Similarly, by FTC, we have
    \ben
        \cos(w_j(t_0+\Delta t),x_-)\leq -1\,.
    \een
    This would imply $w_j(t_0+\Delta t)\in\mathcal{S}_+$ because $-x_-=x_a(w_j(t_0))\in\mathcal{S}_+$, which contradicts our original assumption that $w_j$ fixes its activation pattern. Therefore, one must have $\one_{\lan x_i,w_j(t_0+\Delta t))\ran}\neq \one_{\lan x_i,w_j(t_0)\ran>0}$.
\end{itemize}
In summary, we have shown that, during $[0,T]$, a neuron in $\mathcal{V}_+$ can not keep a fixed activation pattern for a time longer than $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a}$, where $n_a$ is the number of data points that activate $w_j$ under the fixed activation pattern.

\textbf{Bound on total travel time until directional convergence} As we have discussed in Section \ref{ssec_pf_sketch} and also formally proved here, during alignment phase $[0,T]$, a neuron in $\mathcal{V}_+$ must change its activation pattern within $\Delta t=\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}n_a}$ time unless it is in either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$. And the new activation it is transitioning into must contain no new activation on negative data points and must keep all existing activation on positive data points, together it shows that a neuron must reach either $\mathcal{S}_+$ or $\mathcal{S}_\text{dead}$ within a fixed amount of time, which is the remaining thing we need to formally show here.

For simplicity of the argument, we first assume $T=\infty$, i.e., the alignment phase lasts indefinitely, and we show that a neuron in $\mathcal{V}_+$ must reach $\mathcal{S}_+$ or $\mathcal{S}_{\text{dead}}$ before $t_1=\frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}$. Lastly, such directional convergence can be achieved if $t_1\leq T$, which is guaranteed by our choice of $\epsilon$ in \eqref{eq_app_ub_epsilon}. 

\begin{itemize}[leftmargin=0.3cm]
    \item For a neuron in $\mathcal{V}_+$ that reaches $\mathcal{S}_\text{dead}$, the analysis is easy: It must start with no activation on positive data and then lose activation on negative data one by one until losing all of its activation. Therefore, it must reach $\mathcal{S}_\text{dead}$ before
    \ben
        \sum_{k=1}^{n_a(w_j(0))}\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}k}\leq \frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp\sum_{k=1}^n\frac{1}{k}\rp\leq \frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}=t_1\,.
    \een
    \item For a neuron in $\mathcal{V}_+$ that reaches $\mathcal{S}_+$, there is no difference conceptually, but it can switch its activation pattern in many ways before reaching $\mathcal{S}_+$, so it is not straightforward to see its travel time until $\mathcal{S}_+$ is upper bounded by $t_1$.

    To formally show the upper bound on the travel time, we need some definition of a path that keeps a record of the activation patterns of a neuron $w_j(t)$ before it reaches $\mathcal{S}_+$.

    Let $n_+=|\mathcal{I}_+|$, $n_-=|\mathcal{I}_-|$ be the number of positive, negative data respectively, then we call $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ a \emph{path} of length-$L$, if
    \begin{enumerate}[leftmargin=0.5cm]
        \item $\forall 0\leq l\leq L$, we have $k^{(l)}=(k_+^{(l)},k_-^{(l)})\in \mathbb{N}\times\mathbb{N}$ with $0\leq k_+^{(l)}\leq n_+$, $0\leq k_-^{(l)}\leq n_-$;
        \item For $k^{(l_1)},k^{(l_2)}$ with $l_1< l_2$, we have either $k_+^{(l_1)}>k_+^{(l_2)}$ or $k_-^{(l_1)}<k_-^{(l_2)}$;
        \item $k^{(L)}=(n_+,0)$;
        \item $k^{(l)}\neq (0,0),\forall 0\leq l\leq L$.
    \end{enumerate}
    % Figure environment removed

    Given all our analysis on how a neuron $w_j(t)$ can switch its activation pattern in previous parts, we know that for any $w_j(t)$ that reaches $\mathcal{S}_+$, there is an associated $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ that keeps an ordered record of encountered values of
    \ben
        \lp|\{i\in\mathcal{I}_+:\lan x_i,w_j(t)\ran>0\}|,\ |\{i\in\mathcal{I}_-:\lan x_i,w_j(t)\ran>0\}|\rp\,,
    \een
    before $w_j$ reaches $\mathcal{S}_+$. That is, a neuron $w_j$ starts with some activation pattern that activates $k_+(0)$ positive data and $k_-(0)$ negative data, then switch its activation pattern (by either losing negative data or gaining positive data) to one that activates $k_+(1)$ positive data and $k_-(1)$ negative data. By keep doing so, it reaches $\mathcal{S}_+$ that activates $k_+(L)=n_+$ positive data and $k_-(L)=0$ negative data. Please see Figure \ref{fig_path} for an illustration of a path.

    Given a path $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ of neuron $w_j$, we define the \emph{travel time} of this path as
    \ben
        T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})=\sum_{l=0}^{L-1}\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}(k_+^{(l)}+k_-^{(l)})}\,,
    \een
    which is exactly the traveling time from $k^{(0)}$ to $k^{(L)}$ if one spends $\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}(k_+^{(l)}+k_-^{(l)})}$ on the edge between $k^{(l)}$ and  $k^{(l+1)}$.
    
    Our analysis shows that if $w_j$ reaches $\mathcal{S}_+$, then
    \ben
        \inf\{t:w_j(t)\in\mathcal{S}_+\}\leq  T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\,.
    \een

    Now we define the maximal path $\mathcal{P}_{\max}$ as a path that has the maximum length $n=n_++n_-$, which is uniquely determined by the following trajectory of $k^{(l)}$
    \ben
        (0,n_-),(0,n_--1),(0,n_--2),\cdots,(0,1),(1,1),(1,0),\cdots,(n_+-1,0),(n_+,0)\,.
    \een
    Please see Figure \ref{fig_path_max_path} for an illustration.

    The traveling time for $\mathcal{P}_{\max}$ is
    \begin{align*}
         T(\mathcal{P}_{\max})&=\;\frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp\sum_{k=1}^{n_-}\frac{1}{k}+\frac{1}{2}+\sum_{k=1}^{n_+-1}\frac{1}{k}\rp\\
         &\leq\; \frac{4}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\lp 2\sum_{k=1}^{n}\frac{1}{k}+\frac{1}{2}\rp\\
         &\leq\; \frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}=t_1\,.
    \end{align*}
    The proof is complete by the fact that any path satisfies
    \ben
        T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\leq T(\mathcal{P}_{\max})\,.
    \een
    This is because there is a one-to-one correspondence between the edges $(k^{(l)}, k^{(l+1)})$ in $\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})}$ and a subset of edges in $\mathcal{P}_{\max}$, and the travel time from of edge $(k^{(l)}, k^{(l+1)})$ is shorter than the corresponding edge in $\mathcal{P}_{\max}$. Formally stating such correspondence is tedious and a visual illustration in Figure \ref{fig_comp_path_h} and \ref{fig_comp_path_v} is more effective (Putting all correspondence makes a clustered plot thus we split them into two figures):
    % Figure environment removed
    
    Therefore, if $w_j$ reaches $\mathcal{S}_+$, then it reaches $\mathcal{S}_+$ within $t_1$:
    \ben
        \inf\{t:w_j(t)\in\mathcal{S}_+\}\leq  T(\mathcal{P}_{(k^{(0)},k^{(1)},\cdots,k^{(L)})})\leq T(\mathcal{P}_{\max})\leq t_1\,.
    \een
    
\end{itemize}

So far we have shown when the alignment phase lasts long enough, i.e., $T$ large enough, the directional convergence is achieved by $t_1$. We simply pick $\epsilon$ such that
\ben
    T=\frac{1}{4nX_{\max}}\log\frac{1}{\sqrt{h}\epsilon}\geq t_1=\frac{16\log n}{\min\{\zeta,\xi\}\sqrt{\mu}X_{\min}}\,,
\een
and \eqref{eq_app_ub_epsilon} suffices.
\end{proof}

\newpage
\section{Proof for Theorem \ref{thm_conv_main}: Final Convergence}\label{app_pf_thm_conv}
Since we have proved the first part of Theorem \ref{thm_conv_main} in Section \ref{app_pf_thm_align}, we will use it as a fact, then prove the remaining part of Theorem \ref{thm_conv_main}. 
\subsection{Auxiliary lemmas}
First, we show that $\mathcal{S}_+,\mathcal{S}_-,\mathcal{S}_\text{dead}$ are trapping regions.

\begin{lemma}\label{lem_app_trapping}
    Consider any solution to the gradient flow dynamic \eqref{eq_gf}, we have the following:
    \begin{itemize}[leftmargin=0.3cm]
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_\text{dead}$, then $w_j(t_1+\tau)\in\mathcal{S}_\text{dead},\ \forall \tau\geq 0$;
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_+$ for some $j\in\mathcal{V}_+$, then $w_j(t_1+\tau)\in\mathcal{S}_+,\ \forall \tau\geq 0$;
        \item If at some time $t_1\geq 0$, we have $w_j(t_1)\in\mathcal{S}_-$ for some $j\in\mathcal{V}_-$, then $w_j(t_1+\tau)\in\mathcal{S}_-,\ \forall \tau\geq 0$;
    \end{itemize}
\end{lemma}
\begin{proof}
    The first statement is simple, if $w_j\in\mathcal{S}_\text{dead}$, then one have $\dot{w}_j=0$, thus $w_j$ remains in $\mathcal{S}_\text{dead}$.

    For the second statement, we have, since $j\in\mathcal{V}_+$,
    \ben
        \frac{d}{dt}w_j=-\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|\,.
    \een
    By the Fundamental Theorem of Calculus, one writes, $\forall \tau\geq 0$,
    \begin{align*}
        w_j(t_1+\tau)&=\;w_j(t_1)+\int_0^\tau \frac{d}{dt}w_jd\tau\\
        &=\;w_j(t_1)+\int_0^\tau -\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|d\tau\\
        &=\;w_j(t_1)+\int_0^\tau \sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}y_i\exp(-y_if(x_i;W,v))x_i\|w_j\|d\tau\\
        &=\;w_j(t_1)+\underbrace{\sum_{i\in\mathcal{I}_+}\lp\int_0^\tau \exp(-y_if(x_i;W,v))\|w_j\|d\tau \rp x_i}_{:=\tilde{x}_+}\,.
    \end{align*}
    Here $w_j(t_1)\in\mathcal{S}_+$ by our assumption, $\tilde{x}_+\in K\subseteq \mathcal{S}_+$ because $\tilde{x}_+$ is a conical combination of $x_i,i\in\mathcal{I}_+$. Since $\mathcal{S}_+$ is a convex cone, we have $w_j(t_1+\tau)\in\mathcal{S}_+$ as well.

    The proof of the third statement is almost identical: when $j\in\mathcal{V}_-$, we have
    \ben
        \frac{d}{dt}w_j=\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))x_i\|w_j\|\,,
    \een
    and 
    \ben
        w_j(t_1+\tau)= w_j(t_1)+\underbrace{\sum_{i\in\mathcal{I}_-}\lp\int_0^\tau \exp(-y_if(x_i;W,v))\|w_j\|d\tau \rp x_i}_{:=\tilde{x}_-}.
    \een
    Again, here $w_j(t_1)\in\mathcal{S}_-$ by our assumption, $\tilde{x}_-\in -K\subseteq \mathcal{S}_-$ because $\tilde{x}_-$ is a conical combination of $x_i,i\in\mathcal{I}_-$. Since $\mathcal{S}_-$ is a convex cone, we have $w_j(t_1+\tau)\in\mathcal{S}_+$ as well.
    \end{proof}

    Then the following Lemma provides a lower bound on neuron norms upon $t_1$.
    \begin{lemma}\label{lem_app_norm_lb_t1}
        Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $t_1$ be the time when directional convergence is achieved, as defined in Theorem \ref{thm_conv_main}, and we define $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. If both $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$ are non-empty, we have 
        \ben
            \sum_{j\in \tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2, 
        \een
        \ben
            \sum_{j\in \tilde{\mathcal{V}}_-}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(0)\|^2, 
        \een
    \end{lemma}
    \begin{proof}
        We have shown that
        \ben
        \frac{d}{dt}\|w_j\|^2
        = -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|\,.
        \een
        Then before $t_1$, we have $\forall j\in[h]$
        \begin{align*}
            \frac{d}{dt}\|w_j\|^2 &=\; -2\sum_{i=1}^n\one_{\lan x_i,w_j\ran\geq 0}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \sign(v_j(0))\|w_j\|\\
            &\geq\;-2\sum_{i=1}^n(|y_i|+2\max_i|f(x_i;W,v)|)\|x_i\|\|w_j\|^2\\
            &\geq\; -4\sum_{i=1}^n\|x_i\|\|w_j\|^2\geq -4nX_{\max}\|w_j\|^2\,,
        \end{align*}
        where the second last inequality is because $\max_i|f(x_i;W,v)|\leq \frac{1}{2}$ before $t_1$. Summing over $j\in\tilde{\mathcal{V}}_+$, we have
        \ben
            \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\geq -4nX_{\max}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\,.
        \een
        Therefore, we have the following bound:
        \ben
            \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\geq \exp(-4nX_{\max}t_1)\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2\,.
        \een
    \end{proof}
    Moreover, after $t_1$, the neuron norms are non-decreasing, as suggested by 
    \begin{lemma}\label{lem_app_mono_norm}
        Consider any solution to the gradient flow dynamic \eqref{eq_gf} starting from initialization \eqref{eq_init}. Let $t_1$ be the time when directional convergence is achieved, as defined in Theorem \ref{thm_conv_main}, and we define $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. If both $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$ are non-empty, we have $\forall \tau\geq 0$ and $t_2\geq t_1$,
        \be
            \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2+\tau)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2)\|,\qquad \sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t_2+\tau)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t_2)\| 
        \ee
    \end{lemma} 
    \begin{proof}
        It suffices to show that after $t_1$, the following derivatives:
        \ben
            \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2, \quad \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\,,
        \een
        are non-negative. 

        For $j\in \tilde{\mathcal{V}}_+$, $w_j$ stays in $\mathcal{S}_+$ by Lemma \ref{lem_app_trapping}, and we have
        \begin{align*}
            \frac{d}{dt}\|w_j\|^2
            &=\; -2\sum_{i\in\mathcal{I}_+}\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \|w_j\|\,.\\
            &=\; 2\sum_{i\in\mathcal{I}_+}y_i\ell(y_i,f(x_i; W,v))\lan x_i, w_j\ran \|w_j\|\geq 0\,.
        \end{align*}
        Summing over $j\in \tilde{\mathcal{V}}_+$, we have $\frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2\geq 0$. Similarly one has $\frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\geq 0$.
    \end{proof}
    Finally, the following lemma is used for deriving the final convergence.
    \begin{lemma}\label{lem_app_gpl}
        Consider the following loss function
        \ben
            \mathcal{L}_{\text{lin}}(W,v)=\sum_{i=1}^n\ell\lp y_i,v^\top W^\top x_i)\rp\,,
        \een    
        if $\{x_i,y_i\},i\in[n]$ are linearly separable, i.e., $\exists \gamma>0$ and $z\in\mathbb{S}^{D-1}$ such that $y_i\lan z,x_i\ran\geq \gamma,\forall i\in[n]$, then under the gradient flow on $\mathcal{L}_{\text{lin}}(W,v)$, we have
        \be
            \dot{\mathcal{L}}_{\text{lin}}\leq -\|v\|^2\mathcal{L}^2\gamma^2\,.
        \ee
        
    \end{lemma}
    \begin{proof}
        \begin{align*}
            \dot{\mathcal{L}}=-\|\nabla_W\mathcal{L}\|^2_F-\|\nabla_v\mathcal{L}\|^2_F
            &\leq\; -\|\nabla_W\mathcal{L}\|^2_F\\
            &=\; -\lV \sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_iv^\top \rV_F^2\\
            &=\; -\|v\|^2\lV \sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_i\rV^2\\
            &\leq\; -\|v\|^2 \lvt\lan z,\sum_{i=1}^ny_i\ell(y_i,v^\top W^\top x_i)x_i\ran\rvt^2\\
            &\leq\; -\|v\|^2 \lvt\sum_{i=1}^n\ell(y_i,v^\top W^\top x_i)\gamma\rvt^2\leq -\|v\|^2\mathcal{L}^2\gamma^2\,.
        \end{align*}
    \end{proof}


\subsection{Proof of final convergence}

\begin{proof}[Proof of Theorem \ref{thm_conv_main}: Second Part]By Lemma \ref{lem_app_trapping}, we know that after $t_1$, neurons in $\mathcal{S}_+$ ($\mathcal{S}_-$) stays in $\mathcal{S}_+$ ($\mathcal{S}_-$). Thus the loss can be decomposed as
\be\label{eq_app_L_decouple}
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_-}v_j\lan w_j,x_i\ran\rp}_{\mathcal{L}_-}\,,
\ee
where $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. Therefore, the training after $t_1$ is decoupled into 1) using neurons in $\tilde{\mathcal{V}}_+$ to fit positive data in $\mathcal{I}_+$ and 2) using neurons in $\tilde{\mathcal{V}}_-$ to fit positive data in $\mathcal{I}_-$. 

We define $f_+(x_i;W,v)=\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran$ and let $t_2^+=\inf\{t: \max_{i\in\mathcal{I}_+}|f_+(x_i;W,v)|>\frac{1}{4}\}$. Similarly, we also define $f_-(x_i;W,v)=\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran$ and let $t_2^-=\inf\{t: \max_{i\in\mathcal{I}_-}|f_-(x_i;W,v)|>\frac{1}{4}\}$. Then $t_1\leq \min\{t_2^+,t_2^-\}$, by Lemma \ref{lem_small_norm}. 

\textbf{$\mathcal{O}\lp 1/t\rp$ convergence after $t_2$}: We first show that when both $t_2^+,t_2^-$ are finite, then it implies $\mathcal{O}(1/t)$ convergence on the loss. Then we show that they are indeed finite and $t_2:=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$.

At $t_2=\max\{t_2^+,t_2^-\}$, by definition, $\exists i_+\in\mathcal{I}_+$ such that 
\be
    \frac{1}{4}\leq f_+(x_{i_+};W,v)\leq \sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_{i_+}\ran\leq \sum_{j\in\tilde{\mathcal{V}}_+} \|w_j\|^2\|x_{i_+}\|\,,
\ee
which implies, by Lemma \ref{lem_app_mono_norm}, $\forall t\geq t_2$
\be\label{eq_app_w_p_sum_lb}
    \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2\geq \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2)\|^2\geq \frac{1}{4\|x_{i_+}\|}\geq \frac{1}{4X_{\max}}\,.
\ee
Similarly, we have $\forall t\geq t_2$,
\be\label{eq_app_w_m_sum_lb}
\sum_{j\in\tilde{\mathcal{V}}_-}\|w_j(t)\|^2\geq\frac{1}{4X_{\max}}\,.
\ee
Under the gradient flow dynamics \eqref{eq_gf}, we apply Lemma \ref{lem_app_gpl} to the decomposed loss \eqref{eq_app_L_decouple}
\begin{align*}
    \dot{\mathcal{L}}&\leq\;-\lp\sum_{j\in\tilde{\mathcal{V}}_+}v_j^2 \rp\cdot \mathcal{L}_+^2\cdot (\mu X_{\min})^2-\lp\sum_{j\in\tilde{\mathcal{V}}_+}v_j^2 \rp\cdot \mathcal{L}_-^2\cdot (\mu X_{\min})^2\,.
\end{align*}
Here, we can pick the same $\gamma=\mu X_{\min}$ for both $\mathcal{L}_+$ and $\mathcal{L}_-$ because $\{x_i,y_i\},i\in\mathcal{I}_+$ is linearly separable with $z=\frac{y_1x_1}{\|x_1\|}$: $\lan z,x_iy_i\ran\geq \mu\|x_i\|\geq \mu X_{\min}$ by Assumption \ref{assump_data}. And similarly, $\{x_i,y_i\},i\in\mathcal{I}_-$ is linearly separable with $\lan z,x_iy_i\ran\geq \mu\|x_i\|\geq \mu X_{\min}$. Replace $v_i^2$ by $\|w_j\|^2$ from balancedness, together with \eqref{eq_app_w_p_sum_lb}\eqref{eq_app_w_m_sum_lb}, we have
\begin{align*}
    \dot{\mathcal{L}}&\leq\;-\lp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2 \rp\cdot \mathcal{L}_+^2\cdot (\mu X_{\min})^2-\lp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2 \rp\cdot \mathcal{L}_-^2\cdot (\mu X_{\min})^2\\
    &\leq\; -\frac{(\mu X_{\min})^2}{4X_{\max}} (\mathcal{L}_+^2+\mathcal{L}_-^2)\leq -\frac{(\mu X_{\min})^2}{8X_{\max}} (\mathcal{L}_++\mathcal{L}_-)^2=-\frac{(\mu X_{\min})^2}{8X_{\max}}\mathcal{L}^2\,,
\end{align*}
which is
\ben
    \frac{1}{\mathcal{L}^2} \dot{\mathcal{L}} \leq -\frac{(\mu X_{\min})^2}{8X_{\max}}\,.
\een
Integrating both side from $t_2$ to any $t\geq t_2$, we have
\ben
\left.\frac{1}{\mathcal{L}}\rvt_{t_2}^\top \leq-\frac{(\mu X_{\min})^2}{8X_{\max}}(t-t_2)\,,
\een
which leads to
\ben
    \mathcal{L}(t)\leq \frac{\mathcal{L}(t_2)}{\mathcal{L}(t_2)\alpha (t-t_2)+1}\,, \text{ where } \alpha =\frac{(\mu X_{\min})^2}{8X_{\max}}\,.
\een
\textbf{Showing $t_2=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$}: The remaining thing is to show $t_2$ is $\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. 

Since after $t_1$, the gradient dynamics are fully decoupled into two gradient flow dynamics (on $\mathcal{L}_+$ and on $\mathcal{L}_-$), it suffices to show $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ and $t_2^-=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ separately, then combine them to show $t_2=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. The proof is almost identical for $\mathcal{L}_+$ and $\mathcal{L}_-$, thus we only prove $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$ here.

Suppose 
\be\label{eq_app_t2_assump}
    t_2\geq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}+\frac{4}{\sqrt{\mu}n_+X_{\min}}\lp \log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}+4nX_{\max}t_1\rp\,,
\ee
where $n_+=|\mathcal{I}_+|$. It takes two steps to show a contradiction: First, we show that for some $t_a\geq 0$, a refined alignment $\cos(w_j(t_1+t_a),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ is achieved, and such refined alignment is maintained until 
 at least $t_2^+$: $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+t_a\leq t\leq t_2^+$. Then, keeping this refined alignment leads to a contradiction.
 \begin{itemize}[leftmargin=0.3cm]
     \item For $j\in\tilde{\mathcal{V}}_+$, we have
     \ben
        \frac{d}{dt} \frac{w_j}{\|w_j\|}= \lp I-\frac{w_jw_j^\top }{\|w_j\|^2}\rp\underbrace{\lp\sum_{i\in\mathcal{I}_+} -\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))x_i\rp}_{:=\tilde{x}_a}\,.
     \een
     Then 
     \begin{align*}
         \frac{d}{dt} \cos(x_+,w_j)&=\;\lp\cos(x_+,\tilde{x}_a)-\cos(x_+,w_j)\cos(\tilde{x}_a,w_j)\rp \|\tilde{x}_a\|\\
         &\geq\; \lp\cos(x_+,\tilde{x}_a)-\cos(x_+,w_j)\rp \|\tilde{x}_a\|\,.
     \end{align*}
     We can show that $\cos(x_+,\tilde{x}_a)\geq \frac{1}{3}$ and $\|\tilde{x}_a\|\geq \sqrt{\mu}n_+X_{\min}/2$ when $t_1\leq t\leq t_2^+$ (we defer the proof to the end as it breaks the flow), thus within $[t_1,t_2^+]$, we have
     \be\label{eq_app_ref_align}
        \frac{d}{dt} \cos(x_+,w_j)\geq \lp\frac{1}{3}-\cos(x_+,w_j)\rp\sqrt{\mu}n_+X_{\min}/2\,.
     \ee
     We use \eqref{eq_app_ref_align} in two ways: First, since 
     \ben
        \left.\frac{d}{dt} \cos(x_+,w_j)\right\rvert_{\cos(x_+,w_j)=\frac{1}{4}}\geq \frac{\sqrt{\mu}n_+X_{\min}}{24}>0\,,
     \een
     $\cos(x_+,w_j)\geq \frac{1}{4}$ is a trapping region for $w_j$ during $[t_1,t_2^+]$. Define $t_a:=\inf\{t\geq t_1: \min_{j\in\tilde{\mathcal{V}}_+} \cos(x_+,w_j(t))\geq \frac{1}{4}\}$, then clearly, if $t_a\leq t_2^+$, then $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+t_a\leq t\leq t_2^+$.

     Now we use \eqref{eq_app_ref_align} again to show that $t_a\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$: Suppose that $t_a\geq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$, then $\exists j^*$ such that $\cos(x_+,w_{j^*}(t))< \frac{1}{4},\forall t\in[t_1,t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}]$, and we have
    \be
        \frac{d}{dt} \cos(x_+,w_{j^*})\geq \lp\frac{1}{3}-\cos(x_+,w_j)\rp\sqrt{\mu}n_+X_{\min}/2\geq \frac{\sqrt{\mu}n_+X_{\min}}{24}\,.
     \ee
     This shows
     \ben
        \cos(x_+,w_{j^*}(t_1+1))\geq \cos(x_+,w_{j^*}(t_1))+\frac{1}{4}\geq \frac{1}{4}\,,
     \een
     which contradicts that $\cos(x_+,w_{j^*}(t))< \frac{1}{4}$. Hence we know $t_a\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}$.

     In summary, we have $\cos(w_j(t),x_+)\geq \frac{1}{4},\forall j\in\tilde{\mathcal{V}}_+$ for all $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}\leq t\leq t_2^+$.
     \vspace{0.3cm}
     \item Now we check the dynamics of $\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t)\|^2$ during $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}\leq t\leq t_2^+$. For simplicity, we denote $t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}:=t_1'$.

     For $j\in\tilde{\mathcal{V}}_+$, we have, for $t_1'\leq t\leq t_2^+$,
     \begin{align*}
            \frac{d}{dt}\|w_j\|^2 &=\; 2\sum_{i\in\mathcal{I}_+}-\nabla_{\hat{y}}\ell(y_i,f(x_i;W,v))\lan x_i, w_j\ran \|w_j\|&\\
            &\geq\; \sum_{i\in\mathcal{I}_+}\lan x_i, w_j\ran \|w_j\|& (\text{by }\eqref{eq_app_conv_grad_bd})\\
            &=\; \lan x_+, w_j\ran \|w_j\|&\\
            &=\; \|x_+\|\|w_j\|^2\cos(x_+,w_j)&\\
            &\geq\; \frac{1}{4}\|x_+\|\|w_j\|^2&(\text{Since } t\geq t_1') \\
            &\geq\; \frac{\sqrt{\mu}n_+X_{\min}}{4}\|w_j\|^2\,, & (\text{by Lemma \ref{lem_app_x_a_lb}})
    \end{align*}
    which leads to (summing over $j\in\tilde{\mathcal{V}}_+$)
    \ben
        \frac{d}{dt}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\geq \frac{\sqrt{\mu}n_+X_{\min}}{4}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\,.
    \een
    By Gronwall's inequality, we have
    \begin{align*}
        &\;\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_2^+)\|^2&\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1')\|^2&\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2& (\text{By Lemma \ref{lem_app_mono_norm}})\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \exp\lp -4nX_{\max}t_1\rp\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(0)\|^2& (\text{By Lemma \ref{lem_app_norm_lb_t1}})\\
        \geq &\; \exp\lp \frac{\sqrt{\mu}n_+X_{\min}}{4}(t_2^+-t_1')\rp \exp\lp -4nX_{\max}t_1\rp \epsilon^2W_{\min}^2\geq \frac{2}{\sqrt{\mu}X_{\min}}\,.& (\text{by }\eqref{eq_app_t2_assump})
    \end{align*}
    However, at $t_2^+$, we have
    \begin{align*}
        \frac{1}{4}\geq \frac{1}{n_+}\sum_{i\in\mathcal{I}_+}f_+(x_i;W,v)&=\;\frac{1}{n_+}\sum_{i\in\mathcal{I}_+}\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran&\\
        &=\; \frac{1}{n_+}\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_+\ran*\\
        &=\;\frac{1}{n_+}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\cos(w_j,x_+)\|x_+\|&\\
        &\geq\;\frac{1}{4n_+}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\|x_+\|& (\text{Since } t\geq t_1')\\
        &\geq\;\frac{1}{4}\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\sqrt{\mu}X_{\min}\,, & (\text{by Lemma \ref{lem_app_x_a_lb}})
    \end{align*}
    which suggests $\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j\|^2\leq \frac{1}{\sqrt{\mu}X_{\min}}$. A contradiction.
 \end{itemize}
 Therefore, we must have
 \be
    t_2^+\leq t_1+\frac{6}{\sqrt{\mu} n_+X_{\min}}+\frac{4}{\sqrt{\mu}n_+X_{\min}}\lp \log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}+4nX_{\max}t_1\rp\,.
\ee
Since the dominant term here is $\frac{4}{\sqrt{\mu}n_+X_{\min}}\log\frac{2}{\epsilon^2\sqrt{\mu}X_{\min}W^2_{\min}}$, we have $t_2^+=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. A similar analysis shows $t_2^-=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$. Therefore $t_2=\max\{t_2^+,t_2^-\}=\mathcal{O}(\frac{1}{n}\log\frac{1}{\epsilon})$

\textbf{Complete the missing pieces}
 We have two claims remaining to be proved. The first is $\cos(x_+,\tilde{x}_a)\geq\frac{1}{2}$ when $t_1\leq t\leq t_2^+$. Since $x_+=\sum_{i\in\mathcal{I}_+}x_i$ and $\tilde{x}_a=\sum_{i\in\mathcal{I}_+}-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))x_i$. We simply use the fact that before $t_2^+$, we have, by Lemma \ref{assump_loss},
 \be\label{eq_app_conv_grad_bd}
    \frac{1}{2}\leq -\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v))=\leq \frac{3}{2}\,,
 \ee
 to show the following
 \begin{align*}
     \cos(x_+,\tilde{x}_a)&=\;\frac{\lan x_+,\tilde{x}_a\ran}{\|x_+\|\|\tilde{x}_a\|}\\
     &=\; \frac{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}}\\
     &\geq \; \frac{\frac{1}{2}\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}}\\
     &\geq \; \frac{\frac{1}{2}\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}{\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}\sqrt{\sum_{i,j\in\mathcal{I}_+}(\frac{3}{2})^2\lan x_i,x_j\ran}}\geq \frac{1}{3}\,,
 \end{align*}
 since all $\lan x_i,x_j\ran,i,j\in\mathcal{I}_+$ are non-negative.

 The second claim is $\|\tilde{x}_a\|\geq \sqrt{\mu}n_+ X_{\min}/2$ is due to that
 \ben
    \|\tilde{x}_a\|=\sqrt{\sum_{i,j\in\mathcal{I}_+}(-\nabla_{\hat{y}}\ell(y_i,f_+(x_i;W,v)))^2\lan x_i,x_j\ran}\geq \frac{1}{2}\sqrt{\sum_{i,j\in\mathcal{I}_+}\lan x_i,x_j\ran}=\frac{\|x_+\|}{2}\geq \frac{\sqrt{\mu}n_+X_{\min}}{2}\,,
 \een
 where the last inequality is from Lemma \ref{lem_app_x_a_lb}.
\end{proof}
\subsection{Proof of low-rank bias}
So far we have proved the directional convergence at the early alignment phase and final $\mathcal{O}(1/t)$ convergence of the loss in the later stage. The only thing that remains to be shown is the low-rank bias. The proof is quite straightforward but we need some additional notations.

As we proved above, after $t_1$, neurons in $\mathcal{S}_+$ ($\mathcal{S}_-$) stays in $\mathcal{S}_+$ ($\mathcal{S}_-$). Thus the loss can be decomposed as
\ben
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_+}v_j\lan w_j,x_i\ran\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,\sum_{j\in\tilde{\mathcal{V}}_-}v_j\lan w_j,x_i\ran\rp}_{\mathcal{L}_-}\,,
\een
where $\tilde{\mathcal{V}}_+:\{j: w_j(t_1)\in\mathcal{S}_+\}$ and $\tilde{\mathcal{V}}_-:\{j: w_j(t_1)\in\mathcal{S}_-\}$. Therefore, the training after $t_1$ is decoupled into 1) using neurons in $\tilde{\mathcal{V}}_+$ to fit positive data in $\mathcal{I}_+$ and 2) using neurons in $\tilde{\mathcal{V}}_-$ to fit positive data in $\mathcal{I}_-$. We use
\ben
    W_+=[W]_{:,\tilde{\mathcal{V}}_+},\quad W_-=[W]_{:,\tilde{\mathcal{V}}_-}
\een
to denote submatrices of $W$ by picking only columns in  $\tilde{\mathcal{V}}_+$ and $\tilde{\mathcal{V}}_-$, respectively. Similarly, we define
\ben
    v_+=[v]_{\tilde{\mathcal{V}}_+}, \quad v_-=[v]_{\tilde{\mathcal{V}}_-}
\een
for the second layer weight $v$. Lastly, we also define
\ben
    W_\text{dead}=[W]_{:,\tilde{\mathcal{V}}_\text{dead}}, v_\text{dead}=[v]_{\tilde{\mathcal{V}}_\text{dead}}\,,
\een
where $\tilde{\mathcal{V}}_\text{dead}:=\{j: w_j(t_1)\in\mathcal{S}_\text{dead}\}$. Given these notations, after $t_1$ the loss is decomposed as
\ben
    \mathcal{L}=\underbrace{\sum_{i\in\mathcal{I}_+}\ell\lp y_i,x_i^\top W_+v_+\rp }_{\mathcal{L}_+}+\underbrace{\sum_{i\in\mathcal{I}_-}\ell\lp y_i,x_i^\top W_-v_-\rp}_{\mathcal{L}_-}\,,
\een
and the GF on $\mathcal{L}$ is equivalent to GF on $\mathcal{L}_+$ and $\mathcal{L}_-$ separately. It suffices to study one of them. For GF on $\mathcal{L}_+$, we have the following important invariance~\cite{arora2018optimization} $\forall t\geq t_1$:
\ben
    W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)=W_+^\top(t_1)  W_+(t_1)-v_+(t_1)v_+^\top(t_1)\,,
\een
from which one has
\begin{align*}
    \|W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)\|_2&=\; \|W_+^\top(t_1)  W_+(t_1)-v_+(t_1)v_+^\top(t_1)\|_2\\
    &\leq \; \|W_+^\top(t_1)  W_+(t_1)\|_2-\|v_+(t_1)v_+^\top(t_1)\|_2\\
    &\leq\; \tr(W_+^\top(t_1)  W_+(t_1))+\|v_+(t_1)\|^2\\
    &=\;2\sum_{j\in\tilde{\mathcal{V}}_+}\|w_j(t_1)\|^2\leq \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,,
\end{align*}
where the last inequality is by Lemma \ref{lem_small_norm}. Then one can immediately get
\ben
    \|v_+(t)v_+^\top (t)\|_2-\|W_+^\top(t) W_+(t)\|_2\leq\|W_+^\top(t)  W_+(t)-v_+(t)v_+^\top(t)\|_2\leq  \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,,
\een
which is precisely
\be\label{eq_app_lrank_1}
    \|W_+(t)\|_F^2\leq \|W_+(t)\|_2^2+\frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_+|\,.
\ee
Similarly, we have
\be\label{eq_app_lrank_2}
    \|W_-(t)\|_F^2\leq \|W_-(t)\|_2^2+\frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_-|\,.
\ee
Lastly, one has
\be\label{eq_app_lrank_3}
\|W_{\text{dead}}\|^2_F=\sum_{j\in\tilde{\mathcal{V}}_{\text{dead}}}\|w_j(t_1)\|^2\leq \frac{4\epsilon W_{\max}^2}{\sqrt{h}}|\tilde{\mathcal{V}}_\text{dead}|
\ee
Adding \eqref{eq_app_lrank_1}\eqref{eq_app_lrank_2}\eqref{eq_app_lrank_3} together, we have
\begin{align*}
    \|W(t)\|^2_F&=\;\|W_+(t)\|_F^2+\|W_-(t)\|_F^2+\|W_{\text{dead}}\|^2_F\\
    &\leq\; \|W_+(t)\|_2^2+\|W_-(t)\|_2^2+\frac{4\sqrt{h}\epsilon W_{\max}^2}{\sqrt{h}}\leq 2\|W(t)\|_2^2+4\sqrt{h}\epsilon W_{\max}^2\,.
\end{align*}
Finally, since we have shown $\mathcal{L}\ra 0$ as $t\ra \infty$, then $\forall i\in[n]$, we have $\ell(y_i,f(x_i;W,v))\ra 0$. This implies
\ben
    f(x_i;W,v)=-\frac{1}{y_i}\log \ell(y_i,f(x_i;W,v))\ra \infty\,.
\een
Because we have shown that 
\ben
    f(x_i;W,v)\leq \sum_{j\in[h]}\|w_j\|^2\|x_i\|\leq \|W\|_F^2X_{\max}\,,
\een
$f(x_i;W,v)\ra \infty$ enforces $\|W\|_F^2\ra \infty$ as $t\ra \infty$, thus $\|W\|_2^2\ra \infty$ as well. This gets us
\ben
    \lim\sup_{t\ra \infty}\frac{\|W\|_F^2}{\|W\|_2^2}=2\,.
\een

\end{document}