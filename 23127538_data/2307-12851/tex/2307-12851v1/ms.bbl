\begin{thebibliography}{10}

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{rawat2017deep}
Waseem Rawat and Zenghui Wang.
\newblock Deep convolutional neural networks for image classification: A
  comprehensive review.
\newblock {\em Neural computation}, 29(9):2352--2449, 2017.

\bibitem{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N Sainath,
  et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock {\em IEEE Signal processing magazine}, 29(6):82--97, 2012.

\bibitem{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em 2013 IEEE international conference on acoustics, speech and
  signal processing}, pages 6645--6649. IEEE, 2013.

\bibitem{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem{vinyals2017starcraft}
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander~Sasha
  Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K{\"u}ttler, John
  Agapiou, Julian Schrittwieser, et~al.
\newblock Starcraft ii: A new challenge for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1708.04782}, 2017.

\bibitem{gunasekar2017implicit}
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and
  Nathan Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6152--6160, 2017.

\bibitem{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{razin2022implicit}
Noam Razin, Asaf Maman, and Nadav Cohen.
\newblock Implicit regularization in hierarchical tensor factorization and deep
  convolutional neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  18422--18462. PMLR, 2022.

\bibitem{saxe2014exact}
Andrew~M Saxe, James~L Mcclelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural network.
\newblock In {\em International Conference on Learning Representations}, 2014.

\bibitem{Gidel2019}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32, pages 3202--3211. Curran Associates, Inc., 2019.

\bibitem{stoger2021small}
Dominik St{\"o}ger and Mahdi Soltanolkotabi.
\newblock Small random initialization is akin to spectral learning:
  Optimization and generalization guarantees for overparameterized low-rank
  matrix reconstruction.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{li2021towards}
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{mtvm21icml}
Hancheng Min, Salma Tarmoun, Ren\'e Vidal, and Enrique Mallada.
\newblock On the explicit role of initialization on the convergence and
  implicit bias of overparametrized linear networks.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning}, volume 139 of {\em Proceedings of Machine Learning Research},
  pages 7760--7768. PMLR, 18--24 Jul 2021.

\bibitem{li2018small}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In SÃ©bastien Bubeck, Vianney Perchet, and Philippe Rigollet,
  editors, {\em Proceedings of the 31st Conference On Learning Theory},
  volume~75 of {\em Proceedings of Machine Learning Research}, pages 2--47.
  PMLR, 06--09 Jul 2018.

\bibitem{soltanolkotabi2023implicit}
Mahdi Soltanolkotabi, Dominik St{\"o}ger, and Changzhi Xie.
\newblock Implicit balancing and regularization: Generalization and convergence
  guarantees for overparameterized asymmetric matrix sensing.
\newblock {\em arXiv preprint arXiv:2303.14244}, 2023.

\bibitem{maennel2018gradient}
Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly.
\newblock Gradient descent quantizes relu network features.
\newblock {\em arXiv preprint arXiv:1803.08367}, 2018.

\bibitem{lyu2021gradient}
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
\newblock Gradient descent on two-layer nets: Margin maximization and
  simplicity bias.
\newblock {\em Advances in Neural Information Processing Systems},
  34:12978--12991, 2021.

\bibitem{phuong2021inductive}
Mary Phuong and Christoph~H Lampert.
\newblock The inductive bias of relu networks on orthogonally separable data.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{boursier2022gradient}
Etienne Boursier, Loucas Pullaud-Vivien, and Nicolas Flammarion.
\newblock Gradient flow dynamics of shallow relu networks for square loss and
  orthogonal inputs.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~35, pages 20105--20118, 2022.

\bibitem{bolte2010characterizations}
J{\'e}r{\^o}me Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet.
\newblock Characterizations of {\l}ojasiewicz inequalities: subgradient flows,
  talweg, convexity.
\newblock {\em Transactions of the American Mathematical Society},
  362(6):3319--3363, 2010.

\bibitem{Du&Lee}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In {\em 35th International Conference on Machine Learning}, 2018.

\bibitem{arora2018convergence}
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{yun2020unifying}
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{le2022training}
Thien Le and Stefanie Jegelka.
\newblock Training invariances and the low-rank phenomenon: beyond linear
  networks.
\newblock In {\em International Conference on Learning Representations}, 2022.

\end{thebibliography}
