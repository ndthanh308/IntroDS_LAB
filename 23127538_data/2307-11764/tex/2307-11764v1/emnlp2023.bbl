\begin{thebibliography}{24}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and
  Carbin}]{chen2020lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
  Wang, and Michael Carbin. 2020.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{Advances in neural information processing systems},
  33:15834--15846.

\bibitem[{Devlin et~al.(2018)Devlin, Chang, Lee, and
  Toutanova}]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}.

\bibitem[{Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu}]{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu. 2019.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1909.10351}.

\bibitem[{Kundu et~al.(2023)Kundu, Lu, Zhang, Liu, and
  Beerel}]{kundu2023learning}
Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Liu, and Peter Beerel. 2023.
\newblock Learning to linearize deep neural networks for secure and efficient
  private inference.
\newblock \emph{International Conference on Learning Representation}.

\bibitem[{Kundu et~al.(2021)Kundu, Sun, Fu, Pedram, and
  Beerel}]{kundu2021analyzing}
Souvik Kundu, Qirui Sun, Yao Fu, Massoud Pedram, and Peter Beerel. 2021.
\newblock Analyzing the confidentiality of undistillable teachers in knowledge
  distillation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:9181--9192.

\bibitem[{Kurtic et~al.(2022)Kurtic, Campos, Nguyen, Frantar, Kurtz, Fineran,
  Goin, and Alistarh}]{kurtic2022optimal}
Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin
  Fineran, Michael Goin, and Dan Alistarh. 2022.
\newblock The optimal bert surgeon: Scalable and accurate second-order pruning
  for large language models.
\newblock \emph{arXiv preprint arXiv:2203.07259}.

\bibitem[{Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut}]{lan2020albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut. 2020.
\newblock \href {http://arxiv.org/abs/1909.11942} {Albert: A lite bert for
  self-supervised learning of language representations}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock \href {http://arxiv.org/abs/1907.11692} {Roberta: A robustly
  optimized bert pretraining approach}.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
\newblock \href {http://arxiv.org/abs/1606.05250} {Squad: 100,000+ questions
  for machine comprehension of text}.

\bibitem[{Ren et~al.(2019)Ren, Zhang, Ye, Li, Xu, Qian, Lin, and
  Wang}]{ren2019admm}
Ao~Ren, Tianyun Zhang, Shaokai Ye, Jiayu Li, Wenyao Xu, Xuehai Qian, Xue Lin,
  and Yanzhi Wang. 2019.
\newblock Admm-nn: An algorithm-hardware co-design framework of dnns using
  alternating direction methods of multipliers.
\newblock In \emph{Proceedings of the Twenty-Fourth International Conference on
  Architectural Support for Programming Languages and Operating Systems}, pages
  925--938.

\bibitem[{Sajjad et~al.(2020)Sajjad, Dalvi, Durrani, and
  Nakov}]{sajjad2020poor}
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2020.
\newblock Poor manâ€™s bert: Smaller and faster transformer models.
\newblock \emph{arXiv preprint arXiv:2004.03844}, 2(2).

\bibitem[{Sanh et~al.(2020{\natexlab{a}})Sanh, Debut, Chaumond, and
  Wolf}]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
  2020{\natexlab{a}}.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}.

\bibitem[{Sanh et~al.(2020{\natexlab{b}})Sanh, Wolf, and
  Rush}]{sanh2020movement}
Victor Sanh, Thomas Wolf, and Alexander Rush. 2020{\natexlab{b}}.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:20378--20389.

\bibitem[{Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts}]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts. 2013.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642.

\bibitem[{Sridhar et~al.(2022)Sridhar, Sarah, and
  Sundaresan}]{sridhar2022trimbert}
Sharath~Nittur Sridhar, Anthony Sarah, and Sairam Sundaresan. 2022.
\newblock \href {http://arxiv.org/abs/2202.12411} {Trimbert: Tailoring bert for
  trade-offs}.

\bibitem[{Sun et~al.(2019)Sun, Cheng, Gan, and Liu}]{sun2019patient}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu. 2019.
\newblock Patient knowledge distillation for bert model compression.
\newblock \emph{arXiv preprint arXiv:1908.09355}.

\bibitem[{Sun et~al.(2020)Sun, Yu, Song, Liu, Yang, and
  Zhou}]{sun2020mobilebert}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
  2020.
\newblock Mobilebert: a compact task-agnostic bert for resource-limited
  devices.
\newblock \emph{arXiv preprint arXiv:2004.02984}.

\bibitem[{Turc et~al.(2019)Turc, Chang, Lee, and Toutanova}]{turc2019well}
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock Well-read students learn better: On the importance of pre-training
  compact models.
\newblock \emph{arXiv preprint arXiv:1908.08962}.

\bibitem[{Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman. 2018.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}.

\bibitem[{Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman}]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman. 2019.
\newblock \href {http://arxiv.org/abs/1804.07461} {Glue: A multi-task benchmark
  and analysis platform for natural language understanding}.

\bibitem[{Williams et~al.(2017)Williams, Nangia, and
  Bowman}]{williams2017broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman. 2017.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock \emph{arXiv preprint arXiv:1704.05426}.

\bibitem[{Xu et~al.(2020)Xu, Zhou, Ge, Wei, and Zhou}]{xu2020bert}
Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020.
\newblock Bert-of-theseus: Compressing bert by progressive module replacing.
\newblock \emph{arXiv preprint arXiv:2002.02925}.

\bibitem[{Xu et~al.(2021)Xu, Tan, Luo, Song, Li, Qin, and Liu}]{xu2021bert}
Jin Xu, Xu~Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, and Tie-Yan Liu.
  2021.
\newblock Nas-bert: task-agnostic and adaptive-size bert compression with
  neural architecture search.
\newblock In \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 1933--1943.

\bibitem[{Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat}]{Zafrir_2019}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019.
\newblock \href {https://doi.org/10.1109/emc2-nips53020.2019.00016} {Q8bert:
  Quantized 8bit {BERT}}.
\newblock In \emph{2019 Fifth Workshop on Energy Efficient Machine Learning and
  Cognitive Computing - {NeurIPS} Edition ({EMC}2-{NIPS})}. {IEEE}.

\end{thebibliography}
