@article{kurtic2022optimal,
  title={The optimal BERT surgeon: Scalable and accurate second-order pruning for large language models},
  author={Kurtic, Eldar and Campos, Daniel and Nguyen, Tuan and Frantar, Elias and Kurtz, Mark and Fineran, Benjamin and Goin, Michael and Alistarh, Dan},
  journal={arXiv preprint arXiv:2203.07259},
  year={2022}
}

@article{chen2020lottery,
  title={The lottery ticket hypothesis for pre-trained bert networks},
  author={Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={15834--15846},
  year={2020}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2020}
}

@article{sajjad2020poor,
  title={Poor manâ€™s bert: Smaller and faster transformer models},
  author={Sajjad, Hassan and Dalvi, Fahim and Durrani, Nadir and Nakov, Preslav},
  journal={arXiv preprint arXiv:2004.03844},
  volume={2},
  number={2},
  year={2020}
}

@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}

@article{xu2020bert,
  title={Bert-of-theseus: Compressing bert by progressive module replacing},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.02925},
  year={2020}
}

@article{sun2019patient,
  title={Patient knowledge distillation for bert model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}

@inproceedings{xu2021bert,
  title={NAS-BERT: task-agnostic and adaptive-size BERT compression with neural architecture search},
  author={Xu, Jin and Tan, Xu and Luo, Renqian and Song, Kaitao and Li, Jian and Qin, Tao and Liu, Tie-Yan},
  booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
  pages={1933--1943},
  year={2021}
}

@article{kundu2021analyzing,
  title={Analyzing the confidentiality of undistillable teachers in knowledge distillation},
  author={Kundu, Souvik and Sun, Qirui and Fu, Yao and Pedram, Massoud and Beerel, Peter},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={9181--9192},
  year={2021}
}

@article{kundu2023learning,
  title={Learning to linearize deep neural networks for secure and efficient private inference},
  author={Kundu, Souvik and Lu, Shunlin and Zhang, Yuke and Liu, Jacqueline and Beerel, Peter},
  journal={International Conference on Learning Representation},
  year={2023}
}

@article{zhang2023salvit,
  title={SAL-ViT: Towards Latency Efficient Private Inference on ViT using Selective Attention Search with a Learnable Softmax Approximation},
  author={Zhang, Yuke and Chen, Dake  and Kundu, Souvik and Li, Chenghao  and Beerel, Peter},
  journal={International Conference on Computer Vision},
  year={2023}
}

@inproceedings{ren2019admm,
  title={Admm-nn: An algorithm-hardware co-design framework of dnns using alternating direction methods of multipliers},
  author={Ren, Ao and Zhang, Tianyun and Ye, Shaokai and Li, Jiayu and Xu, Wenyao and Qian, Xuehai and Lin, Xue and Wang, Yanzhi},
  booktitle={Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={925--938},
  year={2019}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@article{devlin2018bert,
    title={Bert: Pre-training of deep bidirectional transformers for language understanding},
    author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    journal={arXiv preprint arXiv:1810.04805},
    year={2018}
}

@misc{liu2019roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{peters2018deep,
      title={Deep contextualized word representations}, 
      author={Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
      year={2018},
      eprint={1802.05365},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lan2020albert,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rajpurkar2016squad,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sridhar2022trimbert,
      title={TrimBERT: Tailoring BERT for Trade-offs}, 
      author={Sharath Nittur Sridhar and Anthony Sarah and Sairam Sundaresan},
      year={2022},
      eprint={2202.12411},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{sun2020mobilebert,
  title={Mobilebert: a compact task-agnostic bert for resource-limited devices},
  author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
  journal={arXiv preprint arXiv:2004.02984},
  year={2020}
}

@inproceedings{Zafrir_2019,
	doi = {10.1109/emc2-nips53020.2019.00016}, 
	year = 2019,
	month = {dec}, 
	publisher = {{IEEE}},
  	author = {Ofir Zafrir and Guy Boudoukh and Peter Izsak and Moshe Wasserblat},
	title = {Q8BERT: Quantized 8Bit {BERT}},
	booktitle = {2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - {NeurIPS} Edition)}
}

@article{turc2019well,
  title={Well-read students learn better: On the importance of pre-training compact models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962},
  year={2019}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{floridi2020gpt,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and Machines},
  volume={30},
  pages={681--694},
  year={2020},
  publisher={Springer}
}

@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{roziere2023code,
  title={Code Llama: Open Foundation Models for Code},
  author={Rozi{\`e}re, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
}


@article{Rajpurkar_2016,
    title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
    url={http://dx.doi.org/10.18653/v1/D16-1264},
    DOI={10.18653/v1/d16-1264},
    journal={Proceedings of the 2016 Conference on Empirical Methods in Natural
          Language Processing},
    publisher={Association for Computational Linguistics},
    author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
    year={2016}
}