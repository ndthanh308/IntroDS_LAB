{
  "title": "Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT",
  "authors": [
    "Souvik Kundu",
    "Sharath Nittur Sridhar",
    "Maciej Szankin",
    "Sairam Sundaresan"
  ],
  "submission_date": "2023-07-14T17:24:15+00:00",
  "revised_dates": [
    "2023-09-01T00:35:18+00:00"
  ],
  "abstract": "Large pre-trained language models have recently gained significant traction due to their improved performance on various down-stream tasks like text classification and question answering, requiring only few epochs of fine-tuning. However, their large model sizes often prohibit their applications on resource-constrained edge devices. Existing solutions of yielding parameter-efficient BERT models largely rely on compute-exhaustive training and fine-tuning. Moreover, they often rely on additional compute heavy models to mitigate the performance gap. In this paper, we present Sensi-BERT, a sensitivity driven efficient fine-tuning of BERT models that can take an off-the-shelf pre-trained BERT model and yield highly parameter-efficient models for downstream tasks. In particular, we perform sensitivity analysis to rank each individual parameter tensor, that then is used to trim them accordingly during fine-tuning for a given parameter or FLOPs budget. Our experiments show the efficacy of Sensi-BERT across different downstream tasks including MNLI, QQP, QNLI, SST-2 and SQuAD, showing better performance at similar or smaller parameter budget compared to various alternatives.",
  "categories": [
    "cs.CL"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.11764",
  "pdf_url": null,
  "comment": "6 pages, 5 figures, 2 tables",
  "num_versions": null,
  "size_before_bytes": 4185980,
  "size_after_bytes": 208194
}