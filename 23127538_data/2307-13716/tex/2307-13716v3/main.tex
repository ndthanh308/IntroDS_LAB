\documentclass[10pt,a4paper]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}
% Useful packages
% \usepackage{amsmath}
\usepackage[utf8]{inputenc}
% figure config
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{csquotes}

% reference config
\usepackage[
backend=biber,
style=numeric,
sorting=none
]{biblatex}
\addbibresource{reference.bib}

% front config
\usepackage{indentfirst}
\setcounter{secnumdepth}{3}
% algorithm table config
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}  
\renewcommand{\algorithmicrequire}{\textbf{Input:}} 
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% table config
\usepackage{float}%提供float浮动环境
\usepackage{booktabs}%提供命令\toprule、\midrule、\bottomrule
\usepackage{multirow}%提供跨列命令\multicolumn{}{}{}

% math config
\usepackage{amsmath, amsfonts, amsthm, stackrel}

\usepackage{authblk}
\usepackage[T1]{fontenc}


\title{FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning}
% \author {you}
\author[a]{Leiming Chen \thanks{Corresponding author: chenleiming2020@163.com}}
% \author[a]{Leiming Chen}
%chenleiming2020@163.com
\author[a]{Cihao Dong}
\author[a]{Sibo Qiao}
%dch142857@163.com
\author[a]{Ziling Huang}
\author[a]{Yuming Nie}
\author[b]{Zhaoxiang Hou}
% wangkai_922@hotmail.com
\author[c]{Cheewei Tan \thanks{Corresponding author: cheewei.tan@ntu.edu.sg}}
\affil[a]{China University of Petroleum (East China), China}
\affil[b]{Digital Research Institute, ENN Group, China}
\affil[c]{Nanyang Technological University, Singapore}
\renewcommand*{\Affilfont}{\small\it} % 修改机构名称的字体与大小
\renewcommand\Authands{ and } % 去掉 and 前的逗号
\date{} % 去掉日期

\begin{document}
\maketitle

\begin{abstract}
Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues.
To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and aggregates the optimal global model. We also define five model fusion scenarios and compare our method with two baseline algorithms in those scenarios. The experimental results show that our algorithm has higher reliability than other algorithms while maintaining accuracy.
\end{abstract}

\section{Introduction} 


With the development of deep learning technologies, numerous industries are applying them to their respective fields, resulting in the emergence of intelligent transportation, smart logistics, and smart healthcare models. These technologies reduce the industry’s production and management costs, increase operational efficiency, and accelerate the industry’s digitization process. However, most current deep learning-based model training uses a supervised learning method.  The number and diversity of samples is the key to training a high-quality model. Therefore, how to obtain massive and diverse data samples has become the first step to training deep learning models. This method leads to the source of these samples being no longer limited to a single industry but to multiple industries collaborating to construct large-scale datasets.
Moreover, with the emergence of cross-industry collaborative data analysis models, data analysis sources are no longer limited to a single organization but involve multiple parties. Although the participation of multiple parties in the data analysis process can solve the problem of cross-domain data analysis, this strategy also faces a new challenge. The traditional data analysis method requires uploading each participant’s raw data to a central node and training models on the central node. However, this workflow involving raw data transfer poses security and privacy risks. Meanwhile, raw data transmission is not permitted for data-sensitive industries, such as hospital patient and financial industry data. How to achieve federated data analysis of multiple parties while protecting data security is becoming hot research.

Federated learning is an emerging paradigm for distributed computing. The structure of federated learning has a central node and multiple participants, where multiple participants train models with their private data, and the central node achieves knowledge extraction and information fusion of each participant by fusing the models uploaded by each client. Since the process does not directly use the participants' data, federated learning can achieve joint data analysis of multiple participants while protecting user privacy in advance. 
To solve mobile device word input privacy protection issues, Google proposed federated learning and the FedAvg method \cite{FedAvg}, which ensure user privacy by transmitting models instead of raw data. Increasingly, companies are applying federated learning in various domains to tackle cross-industry data analysis challenges. However, in practical federated learning environments, the heterogeneity of data among participants gives rise to the non-iid problem, which makes model convergence challenging during the fusion stage of the FedAvg method. Additionally, variations in computational resources among participants lead to slow convergence of the global model. 
To address these challenges, some research has optimized the global model convergence problem by adding control parameters in the client-side model fusion process, such as FedProx \cite{FedProx}, Scaffold \cite{Scaffold}, and FedNova \cite{FedNova}. The Moon algorithm \cite{MOON} optimizes the convergence of the global model by introducing a comparative loss between the client-side model and the global model.
There is also some research \cite{PGFFL} \cite{Favor} to improve the accuracy of the global model by selecting the clients involved in the global model.
These algorithms somewhat solve the global model's convergence problem in the non-iid data among participants. 

Current traditional federal learning methods, such as FedAvg and FedProx, use a ratio based on the number of samples of each participant to the total number of samples to calculate the model weights of each participant. However, in a practical scenario, the method has two problems. On the one hand: the method does not consider the different participants' model quality, which gives higher weights to the client with a larger number of samples. 
However, due to the variability of computational resources and sample quality across clients, it does not mean that a client can train a higher-quality model when a client has more samples. Therefore, using these algorithms assumes that the sample quality and computational resources are consistent across clients.
On the other hand, if the participants have malicious attack behavior and intentionally upload malicious models to participate in fusion, which will seriously influence the global model convergence process. Therefore it becomes increasingly important to design adaptive defenses for federated learning systems \cite{AttackRL}. How to eliminate malicious models in federated learning model fusion has become a vital issue. At the same time, when the client uploads low-quality models to participate in fusion, how to adaptively adjust each model's fusion weights based on the models' quality is also an urgent problem to be solved.
Some research has used reinforcement learning methods to address these issues of weighting. Such as, Favor \cite{Favor} method uses the DDPG  to calculate the weights of the participant models. 
Some research has also used reinforcement learning to solve the problems of device selection \cite{zhangPeiYingRL} \cite{digitalTwinRL}, resource optimization \cite{RoF} \cite{DDQNTrust}, and communication optimization in IoT federated learning scenarios.

Reinforcement learning (RL) methods use a try-and-learn strategy. The core of the reinforcement learning approach is to train an intelligent agent. The agent uses different actions to interact with the external environment. The environment provides reward and punishment values based on the selected actions of the intelligence. The intelligent body achieves optimal action selection by maximizing the reward value.
However, when we use reinforcement learning, we must address some issues. On the one hand, we must collect training samples by continuously interacting with the environment. When the cost of interacting with the external environment is too high, or the interaction cost is unacceptable (e.g., driverless scenario), how to collect samples quickly will directly affect the training time of reinforcement learning. On the other hand, when the action space of the agent is high-latitude and continuous, this will lead to an increasing sampling time. 
Therefore, due to these two problems, traditional single-agent reinforcement learning training methods can take much time.
When we apply reinforcement learning to federated learning, we must solve those problems. As the number of participants grows, the time cost of training agents will increase. How to optimize the action space for reinforcement learning and speed up the training process of the agent is an urgent problem to be solved.

To address these issues, we propose a staged reinforcement learning algorithm (FedDRL) to solve the problem of model weight calculation in federated learning. The algorithm consists of two phases: the selection of trusted clients and the adaptive assignment of weights. We use a staged approach to accelerate the agent's training. We use a distributed reinforcement learning algorithm to speed up the collection of training samples. The contributions of this paper is as follows.
\begin{itemize}
\item We propose a federated learning model fusion method based on reinforcement learning, which solves the problem of malicious clients' participation in model fusion and clients' model weight assignment.
\item We propose a trusted client selection strategy that eliminates malicious models from participating in global model fusion while enabling the selection of trusted clients.
\item We propose a adaptive weight assignment method that adaptively adjusts the weights corresponding to the clients according to the quality of their uploaded models and therefore computes the optimal global model.
\item We present five types of model fusion scenarios to validate the performance of each algorithm. We also compare the performance of our algorithm with the baseline algorithm on three public datasets.
\end{itemize}

\section{Related Work}
\subsection{Federated learning}
Research of federated learning focuses on solving three problems: how to improve generalization of the global model on the server side, how to personalize the model on the client side.Therefore, Federated learning can be divided into server-side and client-side optimization algorithms. Google first proposed the FedAvg method \cite{FedAvg} to solve the server-side global model fusion problem . However, the client in the actual scenario has device and sample heterogeneity problems, which can lead to global model convergence problems in the FedAvg method. To optimize the global model convergence problem, Karimireddy et al. proposed the Scaffold algorithm \cite{Scaffold}, which accelerates the convergence of the global model by introducing a control variable to correct the client-side drift problem. Li et al. proposed the FedProx algorithm \cite{FedProx}, which implements the correction of deviated clients by adding the regularization function to the client model. Wang et al. proposed the FedNova algorithm \cite{FedNova}, which solves the global model convergence problem by normalizing the parameters on both the client and server sides. Li et al. also proposed the MOON \cite{MOON} method based on model comparison learning, which solves the global model convergence problem by calculating the similarity between the global model and the client model and continuously decreasing the distance representation of both to achieve the training of the client-side model.

\subsection{ The challenges of Non-IID Data Distribution}
The Non-IID problem is the key that leads to the convergence of the federated learning model. Zhao et al. study the performance of different federated learning methods on the Non-IID dataset and experimentally demonstrate that the Non-IID problem poses a significant challenge to the accuracy of the federated learning algorithm \cite{nonIIDProblem}. Therefore some research focuses on solving the Non-IID problem of federated learning. For example, Zhang et al. proposed the FedPD method \cite{FedPD}, which offers strategies that can handle non-convex objective functions to optimize models and communication. In addition, Gong et al. proposed the AutoCFL method \cite{AutoCFL}, which adaptively adjusts the number of training rounds for the client while introducing weighted voting of the client clustering strategy to eliminate the effects of Non-IID and imbalanced data.
Huang et al. proposed the FedAMP method \cite{FedAMP}, which solves the client-side model personalization problem caused by Non-IID data by implementing a personalized update of the client-side model. 
Li et al. proposed the Fedbn method \cite{Fedbn}, which adds a batch normalization layer to the local model to solve the problem of feature shift in data heterogeneity.
Briggs et al. proposed a hierarchical clustering approach (FL+HC)\cite{FLHC}, which optimizes the model's performance on Non-IID datasets by training clients in groups and then having each group trained independently on a dedicated model. Furthermore, Gao et al. proposed the Feddc method \cite{Feddc}, which addresses the data heterogeneity problem by bridging the gap between client and global model parameters through a control variable. Finally, mu et al. proposed the Fedproc method \cite{Fedproc}, which guides the client model training by introducing a loss of comparison between the client model and the global model, thus enabling the global model to perform well on Non-IID data to achieve good performance.

\subsection{ Federated reinforcement learning}
Since reinforcement learning has the advantage of adaptive learning, some research applies reinforcement learning to federated learning.  
Some research focuses on improving the performance of global models based on reinforcement learning methods. Such as Wang et al. proposed the Favor method \cite{Favor}, which adaptively selects clients participating in the model fusion. Sun et al. also proposed the PG-FFL framework \cite{PGFFL}, which can solve the problem of calculating client weights during model fusion.
Some work uses reinforcement learning to solve device optimization problems in a federated IoT environment. such as Zhang et al. uses the DDPG method \cite{zhangPeiYingRL} to achieve the selection of optimal devices. 
Zhang proposed the FedMarl algorithm \cite{FedMarl}, which uses Multi-agent reinforcement learning to achieve node selection in federated learning.
Yang et al. also proposed a digital twin architecture (DTEI) \cite{digitalTwinRL}, which uses reinforcement learning methods to solve the device selection problem. 
Some research focus on solving the resource optimization and scheduling problem in IoT environment. Zhang et al. proposed the RoF  approach\cite{RoF}, which achieves optimal resource scheduling based on multi-intelligent reinforcement learning.
Rjoub et al. designed a trusted device selection method \cite{trustClient} based on a reinforcement learning approach to achieve resource scheduling.
Rjoub et al. proposed the DDQN-Trust method \cite{DDQNTrust}, which uses the Q-learning approach to evaluate the credit scores of devices to achieve optimal scheduling.
To optimize the communication problem of federated learning, Yang et al. proposed a reinforcement learning-based model evaluation method \cite{evaluationRL} to participate in model training and model fusion by selecting the optimal devices.
Most current research uses reinforcement learning to solve federated learning problems in IoT environments. These include optimal device selection, resource optimization, and communication optimization.
Some work uses reinforcement learning methods to solve the client-side weight assignment problem in federated learning. However, most of these methods are based on single-agent training methods and need to address the training time overhead of reinforcement learning.
% Zhang et al. also proposed the R2Fed framework \cite{R2Fed}, which uses the DDPG reinforcement learning approach to calculate client weights adaptively.

\section{Method}
\subsection{Problem Definition}
In this section, we first analyze the problems of the current federated learning approach and then propose a solution to the problem. In the federation learning process, we aim to get a higher-accuracy global model by aggregating all clients' local models through the server side.

In the federated learning environment, 
we define n clients are involved in model fusion, and the client is denoted as $C_{i}$ 
where $C_{i} \in\left\{C_{1}, C_{2}, C_{3} \ldots C_{n}\right\}$. 
Each client has a network model $M_{i}$, 
where $M_{i} \in\left\{M_{1}, M_{2}, M_{3} \ldots M_{n}\right\}$.
Each client has its private data $D_{i}$,
where $D_{i} \in\left\{D_{1}, D_{2}, D_{3} \ldots D_{n}\right\}$.
The number of samples in each dataset is $S_{i}$, where $S_{i} \in\left\{S_{1}, S_{2}, S_{3} \ldots S_{n}\right\}$. The total number of samples is $\sum_{i=1}^{N} S_{i}$.
We define the $\theta_{i}$ as a model parameter of $M_{i}$.
where $\theta_{i} \in\left\{\theta_{1}, \theta_{2}, \theta_{3} \ldots \theta_{n}\right\}$.

Also, we define the server-side model aggregation process for each round as equation \ref{eqn-1}.
\begin{equation}
\label{eqn-1} 
\theta_{\text {global }}=\sum_{i=1}^{N} w_{i} \theta_{i},
\text { where } w_{i}=\frac{S_{i}}{\sum_{i=1}^{N} S_{i}}, \quad  w_{i} \geq 0, \sum_{i=1}^{N} w_{i}=1
\end{equation}

Where $ w_{i}$ is the weight of each client model parameter.
The traditional method of calculating this weight in federal learning is determined by the ratio of the number of samples of the client to the total number of samples. This means that the larger the number of samples corresponding to the client, the larger the weight assigned. However, in the actual environment, due to the heterogeneity of the client data and the computing resources, it is impossible to determine the weights by the ratio of the number of samples for each client. Therefore, we need to solve two problems. 

\textbf{Problem 1}: The sample size of a client accounts for 20\% of the total samples, but the accuracy of this client model is only 53\%, which will reduce the accuracy of the global model if the traditional federated model fusion algorithm is used.

\textbf{Problem 2}: If the client involved in the model fusion has malicious attacks and intentionally modifies the model output to make its model accuracy 10\%, the global model accuracy will be seriously affected if the malicious models are selected for fusion using the traditional federated model fusion algorithm.

To solve these two problems, we need an adaptive weight calculation method that can set the weight of the malicious model to 0, which means that malicious models are prohibited from participating in the fusion process. At the same time, we realize the adaptive assignment of weights to each client and try to increase the weights of high-quality client models so that the global model gets the optimal accuracy rate.

If we use the single-agent reinforcement learning approach to solve the above problems, we face another issue again. On the one hand, the action space of the agent can be huge as the number of fused clients increases, which also leads to a long time required to train the agent model. On the other hand, a single agent method can only interact with a single environment, and that sampling process takes a long time. We solve these two problems in two steps by borrowing the idea of hierarchical reinforcement learning, which can solve the problem of the long training time of reinforcement learning. These two stages are selecting trusted clients and the optimal weight assignment process. 

\textbf{Stage 1}: In this phase, We need select K-trusted models from N models to participate in the global model fusion. However, we do not know which client uploaded a malicious model. To solve the problem, We use reinforcement learning to achieve dynamic selection and autonomous screening of client models as equation \ref{eqn-2}. 
\begin{equation}\label{eqn-2} 
\left\{M_{a}, M_{\mathrm{b}}, \ldots M_{\mathrm{k}}\right\} \leftarrow \text { SelectTrustedModel }\left(\left\{M_{1}, M_{2}, \ldots M_{\mathrm{n}}\right\}\right)
\end{equation}

\textbf{Stage 2}: Based on step 1, we assign optimal weights to the trusted models to improve the accuracy of the global model and represent the process as equation \ref{eqn-3}.
\begin{equation}\label{eqn-3} 
\left\{W_{1}, W_{2}, \ldots W_{\mathrm{n}}\right\} \leftarrow \operatorname{AdaptCalculateWeight}\left(\left\{M_{a}, M_{b}, \ldots M_{\mathrm{k}}\right\}\right)
\end{equation}
$AdaptCalculateWeight(.)$ is a weight-adaptive computation method.
$W_{i}$ is the optimal computational weight for each client of the output.

Why do we use a phased approach to reinforce learning? Let's use an example to illustrate the reason for using it. We need a robot to learn the cooking process using a reinforcement learning approach. The cooking steps are divided into three stages: washing, chopping, and cooking. The robot needs to learn the best actions to cook a good dish. Traditional reinforcement learning is to find the best action that satisfies all three stages at the same time. However, if the robot does not wash the vegetables well, it cannot cook a good dish in the subsequent stages either. Therefore, it is a complete waste of time to try the next step of action if the optimal action of the previous stage is not learned. If the robot learns in multiple stages, it first learns the optimal action in the cleaning process and then proceeds to the next step of learning the optimal action. This will save time and make it easier to get the best solution.

Therefore, if the malicious models are not screened and the weight assignment method is used for malicious models, it will increase the cost of trial and error time in the reinforcement learning process. Therefore, we screen the trusted clients for weight assignment by a staged approach. This can exclude the trial and error cost of malicious models involved in weight assignment, thus optimizing the action space. Also, we use distributed reinforcement learning methods to increase the sampling speed and thus reduce the time overhead of training reinforcement learning. We will describe the implementation of each step in detail in the next section.


\subsection{Trusted Client Selection Method} 

We propose the FedDRL approach to solve the two problems in the previous section. In the first stage, we select the trusted clients to achieve the fusion of the models by excluding the malicious clients. In the second stage, we compute the optimal model weights to improve the accuracy of the global model. The flow of the FedDRL method is shown in Figure \ref{fig:FedDRL}.

We select trusted clients using the A2C algorithm.
A2C is a synchronization algorithm achieved by improving the A3C algorithm \cite{A3C}. The selection process of a trusted client for each communication round of federated learning can be treated as a Markov decision process.
The reinforcement learning modeling in this phase is described as follows.

\textbf{State}: For the m-th communication round, we collect n models uploaded by each client and select k clients to participate in the fusion of global models. We define the $S^{t}$ as a select k client to fusion model in state t,  which is shown as Equation \ref{eqn-4}, and the state includes the selected client models, the accuracy of each model, and the accuracy of the global model aggregated out by the selected client.

\begin{equation}\label{eqn-4} 
S^{t}=\left\{\theta_{1}^{t}, \theta_{2}^{t}, \theta_{3}^{t} \ldots \theta_{k}^{t}, acc_{1}^{t}, acc_{2}^{t}, acc_{3}^{t} \ldots acc_{k}^{t}, acc_{global}^{t}   \right\}
\end{equation}


\textbf{Action}: Our main objective is to eliminate the malicious models and select the trusted models to participate in the fusion. To achieve this purpose, we randomly choose several models to fusion the global model and evaluate the accuracy of global models. If a malicious model is selected to participate in the fusion, the accuracy of the global model will be very low. If only trusted clients are selected to participate in the fusion, the accuracy of the global model will also increase.
We define the action of selecting a model as $a^{t} \in \left\{0, 1\right\}$, where one means that the model is selected and 0 means that the model is not selected. Thus, n models correspond to the action space that can be expressed as equation \ref{eqn-5}.
\begin{equation}\label{eqn-5} 
A^{t}=\left\{a_{1}^{t}, a_{2}^{t}, a_{3}^{t} \ldots a_{n}^{t}\right\}, \quad a^{t} \in \left\{0, 1\right\}
\end{equation}

\textbf{Reward}: We define the accuracy of selecting all models aggregated as $Acc_{all}$; we take this accuracy as the baseline and also represent the global model accuracy $Acc_{m}$ obtained by the m-th randomly selected client models, and calculate the reward value $Reward_{1}$ by the difference of $Acc_{m}-Acc_{all}$. Meanwhile, to avoid the intelligent body only, It can be realized that the agent selects all trusted clients as much as possible. The higher the number of models the agent sets, the higher the value of this reward. Therefore, the total reward is defined as equation \ref{eqn-6}.
\begin{equation}\label{eqn-6} 
Reward = \begin{cases} \alpha*(Acc_{m}-Acc_{all} ) + \beta *(Number)  ,  Acc_{m} >Acc_{all} \\
 \alpha*(Acc_{m}-Acc_{all} ), Acc_{m} <Acc_{all} \end{cases}
\end{equation}
Where $\alpha$ and $\beta$ are fixed values that can be custom adjusted to balance the importance of Reward1 and Reward2, preventing one of the rewards from being too large and affecting agent training.

The agent's goal is to obtain the maximum long-term reward value based on the discount factor $\gamma$, and the process is expressed as equation \ref{eqn-7}.
\begin{equation}\label{eqn-7} 
R(\tau)=\sum_{\mathrm{t}=1}^{\mathrm{T}} \gamma^{t}\mathrm{r}_{\mathrm{t}}
\end{equation}
% Where is an interaction sequence denoted as the discount factor and is the cumulative reward for the entire sequence.

Since we use an A2C distributed reinforcement learning-based approach, as shown in Figure \ref{fig:FedDRL}, the A2C architecture consists of a central node and K workers. Each worker contains an Actor and a Critic network, where the Actor-network generates action A, and the Critic network evaluates the action and gives the corresponding reward. Meanwhile, each worker independently interacts with the related environment to achieve sampling and training of the Actor and Critic networks. In addition, the Actor and Critic networks of the central node are used to synchronize the network information of each worker and to achieve the fusion and sharing of network parameters of multiple workers.

% figure The method of FedDRL
% Figure environment removed

Therefore our main objective is to train Actor and Critic networks. We define the Actor-network parameters as $\pi(\theta)$ and the Critic network parameters as $V(w)$. Our working process for each worker and the central node is as follows. The process is shown in algorithm \ref{Algorithm 1}.

\textbf{Step 1}: First, each worker initializes the local network by pulling the global network model parameters. Then, both networks train their respective Actor and Critic networks by interacting with the environment independently. Finally, the trained networks are uploaded to the central node.

\textbf{Step 2}: After the central node collects the network parameters uploaded by all workers, it updates the global model by weighted averaging. Finally, it sends it to each worker to update the network parameters.

Steps 1 and 2 are repeated according to the total number of times to obtain the final global model.



The training process of the step 1 neutralization network is as follows. The gradient of the primary communication algorithm of the policy network is calculated as equation \ref{eqn-8}.


\begin{equation}\label{eqn-8} 
\nabla_{\theta} J(\theta)=\nabla_{\theta} \log \pi\left(a_{t} \mid s_{t} ; \theta\right) A\left(s_{t}, a_{t} ; \mathrm{w}\right)
\end{equation}

Where $A\left(s_{t}, a_{t} ; \theta_{v}\right)$ is the advantage function. The k-step sampling strategy is used in the A2C algorithm to calculate the advantage function, so the definition is expressed as equation \ref{eqn-9}.

\begin{equation}\label{eqn-9} 
A\left(s_{t}, a_{t} ; \theta_{v}\right)=\sum_{i=0}^{k-1} \gamma^{i} r_{t+i}+\gamma^{k} V\left(s_{t+k} ; \mathrm{w}\right)-V\left(s_{t} ; \mathrm{w}\right)
\end{equation}
The Loss function of the Actor-network is calculated as in equation \ref{eqn-10}, and The Critic network is calculated as in equation \ref{eqn-11}.
\begin{equation}\label{eqn-10} 
\nabla_{\theta}J(\theta)=\nabla_{\theta}\log \pi\left(a_{t} \mid s_{t} ; \theta\right)\left(\sum_{i=0}^{k-1} \gamma^{i} r_{t+i}+\gamma^{k} V\left(s_{t+k} ; \mathrm{w}\right)-V\left(s_{t} ; \mathrm{w}\right)\right)
\end{equation}

\begin{equation}\label{eqn-11} 
\nabla_{w}J(w)=\nabla_{w}\left(\sum_{i=0}^{k-1} \gamma^{i} r_{t+i}+\gamma^{k} V\left(s_{t+k} ; \mathrm{w}\right)-V\left(s_{t} ; w\right)\right)^{2}
\end{equation}
We update the Actor and Critic network parameters using the derivative formula as equation \ref{eqn-12}.
\begin{equation}\label{eqn-12} 
w \leftarrow w+\nabla_{w} J(w), \quad \theta \leftarrow \theta+\nabla_{\theta} J(\theta)
\end{equation}
Finally, the network parameters of the central node are calculated by the weighted average method. The process is equation \ref{eqn-13}.
\begin{equation}\label{eqn-13} 
w_{global}=\frac{1}{n}   {\textstyle \sum_{1}^{n}} w_{i},\theta_{global} =\frac{1}{n}   {\textstyle \sum_{1}^{n}} \theta_{i}, i\in [1,n]
\end{equation}

% Algorithm 1
\begin{algorithm}[H]
    \caption{ FedDRL Stage 1 Algorithm Process}
    \label{Algorithm 1}
    \begin{algorithmic}[1]
        \Require { 
        Client Models $ \left\{m_{1}^{t}, m_{2}^{t}, m_{3}^{t}, \ldots m_{\mathrm{n}}^{t}\right\}$, Round T, Worker Number K, 
        Sampling Step Length S
        }
        \Ensure{ Chosen Credible Client Model List M =  $ \left\{m_{2}^{t}, m_{3}^{t}, \ldots m_{\mathrm{k}}^{t}\right\}$}

        \State /* Each Worker Training Step */
        \State worker $ (\theta, w) \leftarrow  GetGlobalParamter  \left(\theta_{\text {global }}, w_{\text {global }}\right)$
        \State  the Client Upload Current Epoch Model, Turn to State $\mathrm{s}_{0}$, $t_{\text {start }}= t = 1$
        \For{$e$ from 1 to $S$} 
            \State According to Current State $\mathrm{s}_{0}$ Randomly Choose Action $\mathrm{s}_{t}$
            \State ${s_{t}, a_{t}, r, s_{t+1}} \leftarrow \operatorname{Step}\left(\mathrm{a}_{t}\right)$ 
            // Execute Action $\mathrm{a}_{t}$ to Acquire Reward $\mathrm{r}$ and Next State $\mathrm{s}_{t+1}$
            \State $\mathrm{t_{start}}$ = $\mathrm{t_{start}}$ + 1
            \State if $\mathrm{s_{t}}$ != terminal: R $\leftarrow$ V$(\mathrm{s_{t}}; w )$ else: R = 0
        \EndFor
        \For{ $\mathrm{i} \in\left\{t-1, \ldots, \mathrm{t}_{\text {start }}\right\} $}
                \State R $\leftarrow {\mathrm{r_{i}} + \gamma}$R // Compute Target TD
                \State $\nabla_{\theta} J(\theta)=\nabla_{\theta} \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)\left(R-v\left(s_{i} ; w\right)\right)$ // Compute Strategy Gradient
            \State $\nabla_{w} J(w)=\nabla_{w}\left(R-v\left(s_{i} ; w\right)\right)^{2}$ // Compute Critic Network Gradient
            \State Update Actor Network Parameters: $\theta\leftarrow{\theta+\nabla_{\theta}J(\theta)}$
            \State Update Critic Network Parameters:          $w \leftarrow w+\nabla_{w} J(w)$
            
        \EndFor
        \State /* Center Node Process */
        \For{$round$ from 1 to $T$} 
            \For{$\mathrm{worker}_{i}$ from 1 to $K$} 
                \State Receive Each Worker Parameters$(\theta, w)$
                \State $\operatorname{Global}\left(\theta_{\text {global }}, w_{\text {global }}\right) \leftarrow \mathbf{A g g}\left(\left\{\left(\theta_{1}, w_{1}\right),\left(\theta_{2}, w_{2}\right), \ldots\right\}\right)$ // Aggregate Parameters
                \State $\mathrm{worker}_{i}\leftarrow{SendGlobal\left(\theta_{\text {global }}, w_{\text{global }}\right)}$ // Send New Parameters to Worker
            \EndFor
        \EndFor
    \State /* Results Process */
    % \State Get Trusted Client Model List By the Output of Trained Network
    \State Output Trusted Client Model List M = $\left\{m_1^t, m_2^t \ldots m_k^t\right\}$
    \end{algorithmic}
\end{algorithm}

\subsection{Adaptive weights calculation method}

In this phase, we implement a distributed reinforcement learning approach based on TD3 \cite{TD3}.
In this phase, our main objective is to achieve the optimal weight assignment for each model. Since the weight values of the client models correspond to a continuous action space, we use the TD3 algorithm to accomplish this process. For each communication round in federation learning, we assume that K plausible client models are screened in the previous stage, where the parameters of the client models are set. The number of samples corresponding to the clients is defined as. These two terms are fixed in each federal communication round.
We need to assign the weight values corresponding to each model to the k models based on the quality of the models, which is defined. We validate the accuracy Acc of each model based on the retained data.
We need to train the reinforcement learning intelligences in each communication round of federation learning and achieve the global model fusion by assigning the optimal model weights by this intelligences. The process is described as follows.

(1) We define the t-th time, the action adopted by the agent to assign the weights of each model as equation \ref{eqn-14}.
\begin{equation}\label{eqn-14} 
\mathrm{W^{t}}=\left\{w_{1}^{t}, w_{2}^{t}, w_{3}^{t} \ldots w_{k}^{t}\right\}
\end{equation}

(2) We aggregate the global models based on the model weights assigned by the agent, and the process is expressed as equation \ref{eqn-15}.
\begin{equation}\label{eqn-15} 
\theta_{global}^{k}=\sum_{\mathrm{i}=1}^{\mathrm{t}} w_{i}^{t} \theta_{i}
\end{equation}

(3) We use the global model aggregated in step 2 to calculate the accuracy of this model and the reward value based on the accuracy rate. And the reward value is returned to the intelligence.
The intelligence is trained by steps 1-3 until the reward value of the intelligence is stable. Finally, the optimal weights of each model are output by the intelligent body. We model the process as follows.

\textbf{State}: We define the state of the teeth reinforcement learning as including the number of data samples S for each client, the accuracy Acc of each client's model, and the global model Acc based on this aggregation. The process is as in \ref{eqn-16}.
\begin{equation}\label{eqn-16} 
\mathrm{S^{t}}=\left\{s_{1}^{t}, s_{2}^{t}, s_{3}^{t} \ldots s_{k}^{t}, acc_{1}^{t}, acc_{2}^{t}, acc_{3}^{t} \ldots acc_{k}^{t}, acc_{global}^{t}\right\}
\end{equation}
\textbf{Action}: In each stage, we need to reassign each model's weights based on the model's quality.
The action space is shown as \ref{eqn-17}. $a_{i}^{t}$ denotes the weight value assigned to the i-th client in the state t, while the sum of the corresponding weight values of all clients is 1.
\begin{equation}\label{eqn-17} 
\mathrm{A^{t}}=\left\{a_{1}^{t},a_{2}^{t},...a_{k}^{t}\right\},  
 {\textstyle \sum_{1}^{k}} a_{i}^{t}=1,a_{i}^{t}\in (0,1)
\end{equation}
\textbf{Reward}: We define the model accuracy aggregated using the weighted average method as $Acc_{all}$. Also, we define the global model accuracy obtained by the overall model weights assigned to the client for the mth time as $Acc_{m}$. We calculate the reward value by subtracting the difference of $Acc_{m}$ from $Acc_{all}$. If the calculated result is greater than zero, this indicates that the weights assigned by the agent improve the accuracy of the global model, and we give a positive reward. Conversely, we give a penalty reward. $\varphi$, $ \phi$ denotes the reward and penalty factors, respectively. So the reward is defined as equation \ref{eqn-18}.
% \begin{equation}\label{eqn-18} 
% reward=\left(Acc_{m}-Acc_{all}\right)*\alpha
\begin{equation}\label{eqn-18} 
Reward = \begin{cases} \varphi*(Acc_{m}-Acc_{all} ) 
,  Acc_{m} >Acc_{all} \\
  \phi*(Acc_{m}-Acc_{all} ), Acc_{m} <Acc_{all} \end{cases}
\end{equation}

We use multiple parallel environments to accelerate the sampling to speed up the training agent in this phase. The stage is shown in figure \ref{fig:FedDRL}. This stage includes a central Learner and multiple Worker nodes. Each Worker corresponds to a parallel environment. The workflow of each Worker is as follows: first, each Worker performs global model fusion based on the assigned weights; then verifies the accuracy of the global model by interacting with the parallel environment; and finally receives the reward values from the parallel environment feedback. Finally, each Worker stores the corresponding ones in the sampling buffer pool. Multiple workers interact with each environment independently, thus achieving parallel sampling to improve the sampling efficiency. After each worker collects a certain batch of samples, Learner trains the agent by taking a certain amount of sample data from the experience pool.

We use the TD3 algorithm for this phase of the method design. The TD3 algorithm consists of six network models, including an Actor-network $P(w)$, two Critic networks $Q_{1}\left(\theta_{1}\right), Q_{2}\left(\theta_{2}\right)$, and a target Actor-network $P^{\prime}(w)$, two target Critic networks $Q^{\prime}_{1}\left(\theta_{1}\right), Q^{\prime}_{2}\left(\theta_{2}\right)$. The network results of the TD3 algorithm are shown in figure \ref{fig:FedDRL}.
The learner randomly draws N batches of sample data from the buffer pool every certain round for training the model. The training processes are as follows. The process is shown in algorithm \ref{Algorithm 2}

(1) First, select the action  $a_{t+1}$ based on the target Actor-network  $\mathrm{P}^{\prime}\left(\mathrm{~s}_{t+1} \right)$. The state $\mathrm{s}_{t+1}$  and action $a_{t+1}$ are input to the target Critic network$Q_{1}^{\prime}\left(\theta_{1}^{\prime}\right)$ and $Q_{2}^{\prime}\left(\theta_{2}^{\prime}\right)$, respectively. The two target Critic networks will calculate the predicted reward $q_{1}$ and $q_{2}$.

(2) The TD target value is calculated using equation \ref{eqn-19}, where $\operatorname{Min}\left(q_{1}, q_{2}\right)$ takes the minimum value of both.
\begin{equation}\label{eqn-19} 
y_{t} \leftarrow r+\gamma \operatorname{Min}(q 1, q 2)
\end{equation}
    
(3) Select the action based on the Actor-network, input the state and action into the Critic network separately, and let these two networks output the corresponding prediction reward sum.


(4) Calculate the TD error. The calculation formula is as equation \ref{eqn-20}.
\begin{equation}\label{eqn-20} 
\delta_{1, t}=q_{1, t}-y_{t}, \quad \delta_{2, t}=q_{2, t}-y_{t}
\end{equation}
   
(5) Update the Critic network as equation \ref{eqn-21}.
\begin{equation}\label{eqn-21} 
\begin{array}{l}\theta_{1} \leftarrow \theta_{1}-\alpha \cdot \delta_{1, t} \cdot \nabla_{w} Q_{1}\left(s_{t}, a_{t} ; \theta_{1}\right) \\\theta_{2} \leftarrow \theta_{2}-\alpha \cdot \delta_{2, t} \cdot \nabla_{w} Q_{2}\left(s_{t}, a_{t} ; \theta_{2}\right)\end{array}
\end{equation}
   
(6) Update the strategy network every d rounds through the Actor-network output action as equation \ref{eqn-22}.
\begin{equation}\label{eqn-22} 
\mathrm{w} \leftarrow \mathrm{w}+\beta \cdot \nabla_{w} P\left(s_{t} ; \mathrm{w}\right) \cdot \nabla_{w} Q_{1}\left(s_{t}, a_{t} ; \theta_{1}\right)
\end{equation}

(7) Update the target Actor and Critic network parameters every d rounds as equation \ref{eqn-23}.
\begin{equation}\label{eqn-23} 
\begin{array}{l}w^{\prime} \leftarrow \tau w+(1-\tau) w^{\prime} \\\theta_{1}^{\prime} \leftarrow \tau \theta_{1}+(1-\tau) \theta_{1}^{\prime} 
\\\theta_{2}^{\prime} \leftarrow \tau \theta_{2}+(1-\tau) \theta_{2}^{\prime}
\end{array}
\end{equation}

% Algorithm 2
\begin{algorithm}[H]
    \caption{ FedDRL Stage 2 Algorithm Process}
    \label{Algorithm 2}
    \begin{algorithmic}[1]
        \Require { 
        Client Models $ \left\{\theta_{1}^{t}, \theta_{2}^{t}, \theta_{3}^{t}, \ldots \theta_{\mathrm{n}}^{t}\right\}$, Round R, Worker Number N, 
        Buffer Memory Pool M
        
        Initialize Learner Parameters: Actor Parameter P$\left(w\right)$, Critic Network $Q_{1}\left(\theta_{1}\right), Q_{2}\left(\theta_{2}\right)$
        
        Target Actor Parameter $P^{\prime}\left(w^{\prime}\right)$, Target Critic Network $Q_{1}^{\prime}\left(\theta_{1}^{\prime}\right), Q_{2}^{\prime}\left(\theta_{2}^{\prime}\right)$

        $w^{\prime}\leftarrow w, \theta_{1}^{\prime}\leftarrow\theta_{1}, \theta_{2}^{\prime}\leftarrow\theta_{2}$
        }
        \Ensure{ Optimized Client Model Weight $W = \left\{w_{1}^{t}, w_{2}^{t}, \ldots w_{\mathrm{k}}^{t}\right\}$}

        \State /* Each Worker Sampling Step */
        \For{$worker$ from 1 to $N$} 
            \State $a_{t}\leftarrow P\left(s_{t}, w\right)$  // Randomly Choose an Act from $P\left(s_{t}, w\right)$
            \State $\left\{w_{1}^t, w_{2}^t, w_{3}^t \ldots w_{k}^t\right\}\leftarrow Step \left(a_{t}\right)$
            \State $\theta_{global}^t\leftarrow Agg \left(\sum_{i=1}^k w_{i}^t\theta_{i}\right)$
            \State $R_{t}\leftarrow CaculateReward \left(ACC_{t}-ACC_{avg}\right)$
            \State $M\leftarrow Store \left(<S_{t}, A_{t}, R_{t}, S_{t+1}>\right)$
        \EndFor
        \State /* Center Learner Training Step */
        \For{$r$ from 1 to R}
            \State Randomly Sampling N Batches of Data from M
            \State $a_{t+1}^{\prime}\leftarrow P^{\prime}\left(s_{t+1}\right)$
            \State $y\leftarrow r+\gamma Min\left(Q_{1}^{\prime}\left(s_{t+1}, a_{t}^{\prime}\right), Q_{2}^{\prime}\left(s_{t+1}, a_{t}^{\prime}\right)\right)$
            \State Update Critic Network $\theta_{1}\leftarrow argmin_{\theta_{1}}\frac{1}{N}\sum \left(y-Q_{\theta_{1}}\left(s, a\right)\right)^2$
            \State Update Critic Network $\theta_{2}\leftarrow argmin_{\theta_{2}}\frac{1}{N}\sum \left(y-Q_{\theta_{2}}\left(s, a\right)\right)^2$
            \State Every d Rounds:
                \State Update Actor Network: $\nabla_{w}J\left(w\right)=N^{-1}\sum\nabla_{w}Q_{\theta_{1}}\left(s, a\right)\mid_{a=P\left({s}\right)}\nabla_{w}P\left({s}\right)$
                \State Update Target Critic Network: $\theta_1^{\prime}\leftarrow\tau\theta_1+\left(1-\tau\right)\theta_1^{\prime}, \theta_2^{\prime}\leftarrow\tau\theta_2+\left(1-\tau\right)\theta_2^{\prime}$
                \State Update Target Actor Network: $w^{\prime}\leftarrow\tau w+\left(1-\tau\right)w^{\prime}$
        \EndFor
    \State After R Rounds, Save Trained Model
    \State Output Optimized Model Weight W = $\left\{w_1^t, w_2^t \ldots w_k^t\right\}$
    \end{algorithmic}
\end{algorithm}


\section{SYSTEM DESIGN}
We propose a trusted federated model fusion framework(FedDRL) to solve the client weight assignment problem. The main workflow is divided into five steps. Figure \ref{fig:System} shows the system architecture of FedDRL.

% figure The system architecture of FedBKD
% Figure environment removed

\textbf{Step 1:} First, each client downloads the global model, initializes the model parameters using the global model, and then trains the model using local private data. 

\textbf{Step 2:}  After each client's training, each client uploads its respective model parameters to the server side.

\textbf{Step 3:}  After receiving the client's model parameters, the server side performs the trusted client selection and adaptive weight assignment in two stages according to the FedDRL algorithm. 

In stage 1, the server uses the SelectTrustClient(.) algorithm to train the agent, then updates each network parameter of the agent, and finally outputs the optimal action to achieve the selection of trusted clients.

In stage 2, the server uses the AdaptCalculateWeight(.) algorithm to train the agent, then updates each network parameter of the agent, and finally achieves the optimal weight assignment for each client through the agent.

\textbf{Step 4:}  The server side fuses the client models based on the calculated optimal weights and finally fuses the global models.

\textbf{Step 5:}  The server side distributes the global model to each client.

We set several communication rounds and repeat steps 1-5 until the global model converges. The process is shown in Algorithm \ref{Algorithm 3}.



% Algorithm 3
\begin{algorithm}[H]
    \caption{ The FedDRL framework}
    \label{Algorithm 3}
    \begin{algorithmic}[1]
        \Require { 
        Private Dataset $ \left\{D_{1}, D_{2}, \ldots D_{\mathrm{n}}\right\}$, communication round E
        }
        \Ensure{ The Global model $ \left\{M_{global}\right\}$}

        \State /* Client Process */
            \For{$C_{i}$ from 1 to $N$}
               \State $ M_{i} \leftarrow \operatorname{GetGlobalModel}(round=i)$
                  // Get global model and init cleint model
                \State $ M_{i} \leftarrow \operatorname{TrainLocalModel}(D_{i})$
                  // Train model $ M_{i}$ based Dataset $\left\{D_{i} \right\}$
                \State $\text { Server } \leftarrow \operatorname{Send}(M_{i})$
            \EndFor
            
         \State /* Server Process */
            \For{$e$ from 1 to $E$} 
                    \State \text { Store }$(\left\{M_{1}, M_{2}, \ldots M_{n}\right\}) \leftarrow   \operatorname{Receive}\left(M_{i}\right)$ // Receive Client Model

                    /* FedDRL Algorithm Process */
                    \State Train the Stage 1 Agent
                        \State Update the $\operatorname{SelectTrustClient}\left(.\right) 
                        $  Algorithm parameters   // According Algorithm 1
                        \State $\left\{M_{a}, M_{b}, \ldots M_{\mathrm{k}}\right\} 
                        \leftarrow \operatorname{SelectTrustClient}\left(\left\{M_{1}, M_{2}, \ldots M_{n}\right\}\right)$
                    \State Train the Stage 2 Agent
                        \State Update the $\operatorname{AdaptCalculateWeight}\left(.\right)$ 
                         Algorithm parameters  // According Algorithm 2
                        \State $\left\{W_{1}, W_{2}, \ldots W_{\mathrm{n}}\right\} \leftarrow \operatorname{AdaptCalculateWeight}\left(\left\{M_{a}, M_{b}, \ldots M_{k}\right\}\right)$
                    \State $ M_{global} \leftarrow \operatorname{FusionGlobalModel}\left(\left\{W_{1}, W_{2}, \ldots W_{n}\right\}\right)$
                    \State $ C_{i} \leftarrow \operatorname{SendGlobalModel} \left(M_{global}\right)$
            \EndFor
    \end{algorithmic}
\end{algorithm}


\section{EXPERIMENT}
\subsection{Experiment setup}
\subsubsection{Experiment datasets}
We validate the FedDRL framework on three image classification datasets.

% \textbf{Fashion-MNIST dataset}: Fashion-MNIST consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image associated with a label from 10 classes.

% \textbf{CIFAR-10 dataset}: CIFAR-10 dataset consists of 60000 images in 10 classes, with 6000 images per class. 

% \textbf{CIFAR-100 dataset}: CIFAR100 has 100 classes containing 600 images for each class.
\textbf{Fashion-MNIST dataset}: Fashion-MNIST consists of 70,000 samples in 10 classes.  

\textbf{CIFAR-10 dataset}: CIFAR-10 dataset consists of 60000 images in 10 classes. 

\textbf{CIFAR-100 dataset}: CIFAR-100 has 100 classes containing 600 images for each class.

\textbf{Data set partitioning}: To simulate the distribution of non-IID datasets on the clients, we use the method in the paper \cite{Dirichlet} to partition different clients' data on the open-source dataset using the Dirichlet function. The Dirichlet function could simulate the data partitioning of each client by setting the alpha parameter. When the alpha parameter tends to zero, the data distribution of clients is assigned to only a certain class of the public dataset; when the alpha parameter tends to positive infinity, the closer the distribution of client data,  which is the amount and class of data is consistent across clients. To simulate the distribution of non-iid data, we set alpha to 1 to split the three datasets into ten clients. The data distribution is shown in Figure \ref{fig:Non-iidDistribution}.

% data-figure-1
% Figure environment removed




\subsubsection{Comparison of methods}
We compared the FedDRL algorithm with two traditional federated learning algorithms.

\textbf{FedAvg}\cite{FedAvg}: The FedAvg method is the baseline of federated learning, which allocates the weight of the client model by calculating the ratio of the number of samples from each client to the total number of samples.

\textbf{FedProx}\cite{FedProx}: this algorithm improves the FedAvg method by adding a regularization term to the client model to optimize the performance of the federated learning.


\subsubsection{Experimental metrics}
We use the Accuracy metric to evaluate the global model for multi-classification experiments and each client. We define n clients to participate in model fusion and conduct m times of Federated learning communication. In the t-th round, The global model accuracy is $A_{\text {gobal }}^{(t)}$, The Client c model accuracy is $A_{\text {c }}^{(t)}$.The set of global model accuracy for each communication round is expressed as \ref{eqn-24}.
\begin{equation}\label{eqn-24} 
A_{\text {gobal }}=\left\{A_{\text {gobal }}^{(1)} \ldots A_{\text {gobal }}^{(t)} \ldots A_{\text {gobal }}^{(m)}\right\}
\end{equation}

To demonstrate the behavior of each client. We also calculated the accuracy of the local model for each client in each round, which is expressed as \ref{eqn-25}.
\begin{equation}\label{eqn-25} 
A_{\text {c}}=\left\{A_{\text {c}}^{(1)} \ldots A_{\text {c}}^{(t)} \ldots A_{\text {c}}^{(m)}\right\} , c \in \left [ 1,2...n \right ] 
\end{equation}

\subsubsection{Experimental configuration}
\textbf{Hardware configuration}: We completed the experiments using a high-performance workstation with an Intel i9-12900k processor, 64GB RAM, and an NVIDIA RTX3090 graphics card.

\textbf{Software configuration}: We used two frameworks to complete the Federated Learning and Reinforcement Learning experiments. We use the FedBolt, our self-developed experimental framework, to conduct the Federated Learning experiments. We use this framework to simulate a Federated Learning experiment environment with a different number of clients and different data distributions. For the training process of the reinforcement learning model, we use the stablebaseline3 framework \cite{stablebaseline3}, based on which we design two algorithms for trusted client selection and model weight assignment.

\textbf{Federeted Learning Network setup}: We designed different network models for different datasets. For the CIFAR-10 and CIFAR-100 datasets, we create a 6-layer CNN network to train the model. We designed a 4-layer MLP network for the FASHION-MNIST dataset.

\textbf{DRL Network setup}: Since we use a staged reinforcement learning strategy, we use a distributed A2C algorithm in the first stage. Each worker and the central node include a  6-layer MLP  for the Actor and a  6-layer MLP for the Critic Network. In the second stage, we used the TD3 algorithm and a 6-layer MLP network for each module of TD3.

\subsection{Experimental Results}

We validate the FedDRL algorithm in three experiments. These include client attack experiments, low-quality model fusion experiments, and hybrid experiments. The client attack experiments focus on validating our proposed trusted client selection algorithm of stage 1. The low-quality model experiment mainly validates our proposed adaptive weight calculation algorithm of stage 2. The hybrid experiment contains both client attack and low-quality model components and mainly verifies the overall performance of the FedDRL method.

\subsubsection{Malicious Client Attack Experiment}

We define three attack types to simulate the client-side attack behavior in federated learning. To accomplish the experiments, we simulated different numbers of clients and different types of attacks in three datasets. The experimental configuration of the client attack is shown in table \ref{table:AttackExperimentalSettingsTable}.

\textbf{Type 1}: The client directly uploads the initialized model or makes the model accuracy less than 10\% by modifying the model's hyperparameters.

\textbf{Type 2}:  We use falsified data to perform the attack. We use a certain percentage of forged data to participate in model training (e.g., mix the cifar10 dataset with $80\%$ of cifar100 data and generate these cifar100 data labels as cifar10 corresponding label types). We conduct the attack by faking sample data to train the client's local model, thus reducing the client model's accuracy.

\textbf{Type 3}: We select some clients to simulate the attack and divide the training process of these clients into standard and attack rounds. In the standard round, each client does not perform the attack behavior. Instead, each client deliberately uploads the prepared malicious model in the attack round. We also set that these clients alternately initiate the attack behavior.



% experiment-table-1
\begin{table}[H]
\centering
\caption{Client Attack Experimental Settings for Different Numbers of Clients}
\begin{tabular}{ccccccc} % 指定表格列数和对齐方式
\toprule % 顶部横线
\multirow{1}{*}{Number} & 
\multicolumn{1}{c}{Attack Type} &
\multicolumn{1}{c}{Malicious ID} &
\multicolumn{1}{c}{Number of samples}&
\multicolumn{1}{c}{Accuracy of models ($\leq$)} \\
\midrule % 中间横线

\multirow{3}{*}{Client=5} 
&Type 1 & Client1 & 7750 & $A \leq 10\%$ \\
&Type 2 & Client1 & 7750 & $10\% \leq A\leq 20\%$\\
&Type 3 & Client1 & 7750 & Attack round $A \leq 10\%$\\
\hline

\multirow{3}{*}{Client=10} 
&Type 1 & Client1,Client6 & 4222, 4938 & $A\leq 10\%$ 
\\
&Type 2 & Client1,Client6 & 4222, 4938 & $10\% \leq A\leq 20\%$ 
\\
&Type 3 & Client1,Client6 & 4222, 4938 &Attack round $A \leq 10\%$ \\
\hline

\multirow{3}{*}{Client=15} 
&Type 1 & Client1,Client6,Client11 & 3670, 3314, 4454 & $A\leq 10\%$ 
\\
&Type 2 & Client1,Client6,Client11 & 3670, 3314, 4453  & $10\% \leq A\leq 20\%$ 
\\
&Type 3 & Client1,Client6,Client11 & 3670, 3314, 4453 &Attack round $A \leq 15\%$
\\


\bottomrule % 底部横线
\end{tabular}
\label{table:AttackExperimentalSettingsTable}
\end{table}

According to the experimental setup, we compared the FedDRL algorithm with the two baseline algorithms. In the attack experiments, we set the total number of communication rounds to 100 rounds, and each node performs local model training with 1 epoch. To show the attack behavior of each client and the accuracy of different algorithms in each attack scenario in more detail, we counted the accuracy of each client's local model and the accuracy of the server-side global model in each communication round. The specific experimental results are shown in table \ref{table:GlobalAccuracyInClientAttack}.


\begin{table}[H]
\centering
\caption{Global model accuracy for different number of client attack types}
\resizebox{\textwidth}{26mm}{
\begin{tabular}{|l|c|lll|lll|lll|} 
\toprule % 顶部横线
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{DataSet}}}                            
& \multirow{2}{*}{\textbf{Method}} 
& \multicolumn{3}{c|}{Clients=5}
& \multicolumn{3}{c|}{Clients=10}                                                 
& \multicolumn{3}{c}{Clients=15}                                                 
\\ \cline{3-11} 

\multicolumn{1}{c|}{}&                                  
& \multicolumn{1}{l}{Type1} & \multicolumn{1}{l}{Type2} & Type3                  
& \multicolumn{1}{l}{Type1} & \multicolumn{1}{l}{Type2} & Type3                  
& \multicolumn{1}{l}{Type1} & \multicolumn{1}{l}{Type2} &  \multicolumn{1}{l}{Type3}                  
\\ 
\hline
\multicolumn{1}{c|}{\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}fashion\\ -mnist\end{tabular}}}
& FedAvg                           
& \multicolumn{1}{c}{0.862}     & \multicolumn{1}{c}{0.875}     & \multicolumn{1}{c|}{0.863}
& \multicolumn{1}{c}{0.792}     & \multicolumn{1}{c}{0.881}     & \multicolumn{1}{c|}{0.821} 
& \multicolumn{1}{c}{0.776}     & \multicolumn{1}{c}{0.878}     & \multicolumn{1}{c}{0.812} 
\\ 
% \cline{2-11} 

\multicolumn{1}{c|}{}     

& FedProx                           
& \multicolumn{1}{c}{0.873}     & \multicolumn{1}{c}{\pmb{0.884}}    & \multicolumn{1}{c|}{0.864} 
& \multicolumn{1}{c}{0.791}     & \multicolumn{1}{c}{0.882}     & \multicolumn{1}{c|}{0.824} 
& \multicolumn{1}{c}{0.764}     & \multicolumn{1}{c}{0.879}     & \multicolumn{1}{c}{0.811} 
\\ 
% \cline{2-11} 


\multicolumn{1}{c|}{}     
& Ours                          
& \multicolumn{1}{c}{\pmb{0.885}}     & \multicolumn{1}{c}{0.878}    & \multicolumn{1}{c|}{\pmb{0.883}} 
& \multicolumn{1}{c}{\pmb{0.877}}     & \multicolumn{1}{c}{\pmb{0.886}}     & \multicolumn{1}{c|}{\pmb{0.887}} 
& \multicolumn{1}{c}{\pmb{0.881}}     & \multicolumn{1}{c}{\pmb{0.886}}     & \multicolumn{1}{c}{\pmb{0.882}} 
                      
\\ 
\hline

\multicolumn{1}{c|}{\multirow{3}{*}{Cifar10}}                                               & FedAvg                           
& \multicolumn{1}{c}{0.596}     & \multicolumn{1}{c}{0.691}    & \multicolumn{1}{c|}{0.751} 
& \multicolumn{1}{c}{0.335}     & \multicolumn{1}{c}{0.681}     & \multicolumn{1}{c|}{0.314} 
& \multicolumn{1}{c}{0.139}     & \multicolumn{1}{c}{0.664}     & \multicolumn{1}{c}{0.197} 
\\ 
% \cline{2-11} 

\multicolumn{1}{c|}{}     

& FedProx                           
& \multicolumn{1}{c}{0.586}     & \multicolumn{1}{c}{\pmb{0.732}}    & \multicolumn{1}{c|}{\pmb{0.775}} 
& \multicolumn{1}{c}{0.331}     & \multicolumn{1}{c}{\pmb{0.716}}     & \multicolumn{1}{c|}{0.363} 
& \multicolumn{1}{c}{0.122}     & \multicolumn{1}{c}{\pmb{0.719}}     & \multicolumn{1}{c}{0.202} 
\\ 
% \cline{2-11} 

\multicolumn{1}{c|}{}     

& Ours                          
& \multicolumn{1}{c}{\pmb{0.731}}     & \multicolumn{1}{c}{0.701}    & \multicolumn{1}{c|}{0.747} 
& \multicolumn{1}{c}{\pmb{0.694}}     & \multicolumn{1}{c}{0.711}     & \multicolumn{1}{c|}{\pmb{0.727}} 
& \multicolumn{1}{c}{\pmb{0.679}}     & \multicolumn{1}{c}{0.702}     & \multicolumn{1}{c}{\pmb{0.689}} 
\\ \hline

\multicolumn{1}{c|}{\multirow{3}{*}{Cifar100}}                                              & FedAvg                           
& \multicolumn{1}{c}{0.376}     & \multicolumn{1}{c}{0.412}    & \multicolumn{1}{c|}{0.298} 
& \multicolumn{1}{c}{0.273}     & \multicolumn{1}{c}{0.416}     & \multicolumn{1}{c|}{0.287} 
& \multicolumn{1}{c}{0.162}     & \multicolumn{1}{c}{0.398}     & \multicolumn{1}{c}{0.176} 
\\ 
% \cline{2-11} 

\multicolumn{1}{c|}{}     

& FedProx                           
& \multicolumn{1}{c}{0.398}     & \multicolumn{1}{c}{\pmb{0.442}}    & \multicolumn{1}{c|}{0.321} 
& \multicolumn{1}{c}{0.223}     & \multicolumn{1}{c}{\pmb{0.436}}     & \multicolumn{1}{c|}{0.208} 
& \multicolumn{1}{c}{0.172}     & \multicolumn{1}{c}{\pmb{0.426}}     & \multicolumn{1}{c}{0.183} 
\\ 
% \cline{2-11} 

\multicolumn{1}{c|}{}     

& Ours                          
& \multicolumn{1}{c}{\pmb{0.412}}     & \multicolumn{1}{c}{0.438}    & \multicolumn{1}{c|}{\pmb{0.431}} 
& \multicolumn{1}{c}{\pmb{0.426}}     & \multicolumn{1}{c}{0.432}     & \multicolumn{1}{c|}{\pmb{0.421}} 
& \multicolumn{1}{c}{\pmb{0.423}}     & \multicolumn{1}{c}{0.412}     & \multicolumn{1}{c}{\pmb{0.422}}                   
\\
\bottomrule % 底部横线
\end{tabular}}
\label{table:GlobalAccuracyInClientAttack}
\end{table}



To show the effect of the FedDRL algorithm on global model fusion at each communication round. We conducted experiments using the CIFAR10 dataset on 5, 10, and 15 clients. And We compared FedDRL with FedAvg and FedProx algorithms for the global model accuracy. 


We analyze the experimental results for different numbers of client models and different client data. In attack type 1, our algorithm outperforms the FedAvg and FedProx algorithms on all three data sets. In malicious data attack type 2, our algorithm outperforms the FedAvg algorithm and slightly underperforms the FedProx algorithm alone. In the attack type 3 scenario, our algorithm outperforms the comparison algorithm in most cases, especially when multiple malicious clients are involved in model fusion.
To show the relationship between the global and client model's accuracy in each attack scenario. We conducted more detailed experiments on the Cifar10 dataset. The experimental results are shown in Figure \ref{fig:GlobalAccuracy1}, Figure \ref{fig:GlobalAccuracy2}, and Figure \ref{fig:GlobalAccuracy3}.

In attack type 1, our trained agent can dynamically select trusted clients for model fusion and eliminate malicious models from participating, so our algorithm has higher reliability.

In attack type 2, our algorithm is lower than FedProx because the FedProx algorithm uses control parameters to force the models of all client nodes to converge to the global model, which will improve the accuracy of the global model by improving the accuracy of the malicious model to some extent. Our trained reinforcement learning agent will filter out low-accuracy models to participate in the fusion after several communication rounds.

In attack type 3, the trained agent can dynamically adjust the trusted clients, so our algorithm works reliably even in the case of rotating attacks by malicious clients.

% experiment-figture-3
% Figure environment removed


% Figure environment removed


% Figure environment removed



\subsubsection{Low-quality model fusion experiments}

To validate the performance of our proposed algorithm FedDRL, we performed validation on three datasets, Fashion-MNIST, CIFAR-10, and CIFAR-100. Since all three are open source, these data are high-quality data. Therefore each client used these sample data to train their respective models, and these models did not exhibit significant accuracy differences. We set up the following experiments to verify the impact of low-quality models involved in global model fusion in a real-world scenario.

(1) To verify the impact of different numbers of customer nodes on the algorithm, we conducted experiments on three datasets with 5, 10, and 15 clients, respectively. We also set the parameter alpha=1 of Dirichlet to complete the dataset partitioning corresponding to each client.

(2) We introduced a method for low-quality models to participate in the fusion. At the beginning of each round of federated learning, these clients initialize the local model using the global model. After several rounds of communication, when the accuracy of the local model reaches a certain accuracy, we simulate the participation of low-quality models in global model fusion by controlling the accuracy of these client models (e.g., model accuracy is kept between 50\% and 60\%). These clients upload low-quality models to the server side. Our experimental configuration of low-quality simulation for a different number of clients in table \ref{table:Low-qualitySetting}.

% experiment-table-2
\begin{table}[H]
\centering
\caption{Experimental Settings for Low-quality Model experiments}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc} % 指定表格列数和对齐方式
\toprule % 顶部横线
\multirow{1}{*}{Number} & 
\multicolumn{1}{c}{Dataset} &
\multicolumn{1}{c}{Low-quality Model ID} &
\multicolumn{1}{c}{Number of samples} &
\multicolumn{1}{c}{Accuracy of models ($\leq$)} \\
\midrule % 中间横线

\multirow{3}{*}{Client=5} 
&Fashion-MINST & Client1 & 9061 & 53\%\\
&CIFAR-10 & Client1 & 7750 & 52\%\\
&CIFAR-100 & Client1 & 9278  & 22\% \\
\hline

\multirow{3}{*}{Client=10} 
&Fashion-MINST & Client1,Client5 & 5071, 7245 & 51\%, 52\%  \\
&CIFAR-10 & Client1,Client5 & 4222, 6039 & 50\%, 54\%  \\
&CIFAR-100 & Client1,Client5 &4191, 5491 & 22\% \\
\hline

\multirow{3}{*}{Client=15} 
&Fashion-MINST & Client1,Client5,Client10 & 4405, 3752, 1809 & 52\%, 51\%, 53\%  \\
&CIFAR-10 & Client1,Client5,Client10 & 3670, 3128, 1509 & 49\%, 52\%, 55\% \\
&CIFAR-100 & Client1,Client5,Client10 &3073, 3494, 2910 & 22\% 19\% 23\% \\

\bottomrule % 底部横线
\end{tabular}}
\label{table:Low-qualitySetting}
\end{table}

We compared the FedDRL algorithm with both FedAvg and FedProx algorithms. We set the number of communication rounds to 100. Each client performs one epoch of local model training. The experimental results are shown in table \ref{table:GlobalAccuracy}.

% experiment-table-2
\begin{table}[H]
\centering
\caption{Global model accuracy of Low-quality model experiments}
\resizebox{\textwidth}{13mm}{
\begin{tabular}{cccccccccccc} % 指定表格列数和对齐方式
\toprule % 顶部横线
\multirow{2}{*}{Method} &
 & Fashion-MINST & & & CIFAR-10 & & & CIFAR-100  & 
\\
\cline{2-10}
 & Clients=5 & Clients=10 & Clients=15 & Clients=5 & Clients=10 & Clients=15 & Clients=5 & Clients=10 & Clients=15 &\\ 
\midrule % 中间横线

FedAvg & 0.857 & 0.858 & 0.841 & 0.705 & 0.664 & 0.602 & 0.386 & 0.373 & 0.365 \\
\hline

FedProx & 0.865 & 0.861 & 0.829 & 0.714 & 0.652 & 0.607 & 0.402 & 0.391 & 0.386\\
\hline

Ours & \pmb{0.885} & \pmb{0.887} & \pmb{0.884} & \pmb{0.725} & \pmb{0.706} & \pmb{0.698} & \pmb{0.422} & \pmb{0.418} & \pmb{0.407}\\


\bottomrule % 底部横线
\end{tabular}}
\label{table:GlobalAccuracy}
\end{table}

To demonstrate the FedAvg, FedProx, and FedDRL algorithms for global model fusion at each communication round. We have taken the CIFAR10 dataset as an example and conducted experimental comparisons with 5, 10, and 15 clients, respectively. The experimental results of each algorithm are shown in Figure \ref{table:HybridConfig}.

The comparison experiments of CIFAR-10 show that the performance of FedAvg and FedProx algorithms keeps decreasing as the number of low-quality models involved in fusion increases. Also, these two algorithms fluctuate up and down with the accuracy of the low-quality models due to the number of samples used by these two algorithms to calculate the global model. Therefore, when the weight value of the model is fixed, the change in the accuracy of the model affects the change in the accuracy of the global model. Meanwhile, FedDRL has a faster convergence speed than both FedAvg and FedProx algorithms because the FedDRL algorithm can adjust the corresponding weights adaptively according to the quality of each client model, which mitigates the impact of low-quality models on the accuracy of the global model and thus accelerates the convergence speed of the global model to a certain extent.

% Figure environment removed

\subsubsection{Hybrid experiment}

In this section, we fuse two attack types (type 1 and type 3) and simulate a low-quality model to participate in the fusion. We verify The performance of the FedDRL algorithm in this hybrid Scenario. We also compare with FedAvg and FedProx methods. 

We used the cifar10 dataset and conducted experiments with 10 and 15 client cases. Client 1 only uploads the initialization model for each training, Client 6 simulates low-quality models to participate in fusion, client 10 or Client 11 initiates attack behavior in odd-numbered communication rounds, and even-numbered rounds participate in training normally. The remaining nodes normally participate in each round of Federated learning tasks. The detailed configuration of the experiments is shown in table \ref{table:HybridConfig}.
After 100 communication rounds, the global model accuracy of each algorithm is shown in table \ref{table:HybridGlobalAccuracy}.
% experiment-table-1
\begin{table}[H]
\centering
\caption{Hybrid Experimental Settings for Different Numbers of Clients}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccc} % 指定表格列数和对齐方式
\toprule % 顶部横线
\multirow{1}{*}{Number} & 
\multicolumn{1}{c}{Client ID} &
\multicolumn{1}{c}{Type} &
\multicolumn{1}{c}{Number of samples} &
\multicolumn{1}{c}{Model Accuracy} \\
\midrule % 中间横线

\multirow{3}{*}{Client=10} 
& Client1 &Attack Type 1   & 4222 &  $A \leq 10\%$  
\\
& Client6  &Low-quality Model  & 4938  &  $45\%\leq A \leq50\%$  \\
& Client10 &Attack Type 3  & 3560 & Attack round   $A \leq15\%$
\\
\multirow{3}{*}{Client=15} 
& Client1 &Attack Type 1   & 3670 &  $A \leq 10\%$  
\\
& Client6  &Low-quality Model  & 3314  &  $45\%\leq A \leq50\%$  \\
& Client11 &Attack Type 3  & 4453 & Attack round   $A \leq15\%$
\\
% &Type 4 & Client1,Client6,Client11 & 3670, 3314, 4453 \\
\bottomrule % 底部横线
\end{tabular}}
\label{table:HybridConfig}
\end{table}



\begin{table}[H]
\centering
\caption{Global model Accuracy of Hybrid Experimental in different clients’ number}
\resizebox{\textwidth}{14mm}{
\begin{tabular}{ccccccc} % 指定表格列数和对齐方式
\toprule % 顶部横线
\multirow{2}{*}{Method}
&\multicolumn{2}{c}{Fashion-MINST}&\multicolumn{2}{c}{CIFAR-10}&\multicolumn{2}{c}{CIFAR-100}\\
\cline{2-7}
& Clients=10 & Clients=15 & Clients=10 & Clients=15  & Clients=10 & Clients=15\\ 
\midrule % 中间横线

FedAvg &  0.835 & 0.823 & 0.368 & 0.348 & 0.223 & 0.238 \\
\hline

FedProx  &  0.821 & 0.846 & 0.308 & 0.341 & 0.241 & 0.266 \\
\hline

Ours & \pmb{0.876} & \pmb{0.883}  & \pmb{0.701} & \pmb{0.698} & \pmb{0.426} & \pmb{0.418}\\

\bottomrule % 底部横线
\end{tabular}}
\label{table:HybridGlobalAccuracy}
\end{table}

% Figure environment removed

The hybrid scenario experiments show that the FedAvg and FedProx algorithms can no longer perform global model fusion properly due to the attack behavior of the client. Meanwhile, malicious models' involvement in the global model fusion makes the accuracy of the global model based on the traditional algorithm very low. In the FedDRL algorithm, we adopt the two-stage method. In the first stage, we eliminate malicious models from participating in a fusion. We use an adaptive weight fusion strategy in the second stage, reducing low-quality models' influence. Thus our algorithms work correctly even in this hybrid scenario. The global model and the model accuracy of each client for each communication round of these three algorithms are shown in Figure \ref{fig:HybridAccuracy}.


\subsubsection{Discussion}
We analyze the experimental results for different model fusion scenarios as well as for different numbers of client models. In the client-side attack experiments, under attack type 1, our algorithm works better than the FedAvg method on all three datasets. Meanwhile, as the number of malicious nodes increases, the global model accuracy of both FedAvg and FedProx algorithms shows a serious decrease. In contrast, our algorithm still maintains a smooth convergence trend.
Under attack type 2, our algorithm is better than the FedAvg algorithm and slightly lower than the FedProx algorithm. Under attack type 3, our algorithm outperforms the edAvg and FedProx algorithms on all three datasets, which no longer work under rotating attacks with multiple malicious clients. In contrast, our algorithm can work reliably.
In the low-quality model experiments, our algorithm outperforms the FedAvg and FedProx algorithms on all three datasets.
In the hybrid experiments, the FedAvg and FedProx algorithms no longer work properly, and our algorithm can fuse the global models properly.

Since the FedAvg and FedProx methods use the number of samples to calculate the model weights, they cannot adaptively adjust the responses of malicious and low-quality models. They thus cannot be applied to these attack scenarios. Our proposed method is based on staged reinforcement learning for malicious node removal and optimal weight assignment for trusted clients and thus can work reliably in various environments.


\section{CONCLUSION}
To solve the problem of assigning client weights due to data heterogeneity in a federated learning environment. We propose FedDRL, an adaptive weight calculation method based on staged reinforcement learning. The method comprises two stages: selecting trusted clients and adaptive weight assignment. In the first stage, our approach can eliminate malicious client models from participating in the global model fusion. It also can dynamically select trusted client models to participate in the next stage of model weight assignment. In the second stage, Our method can adaptively calculate the corresponding weights based on the model quality of each client. Also, we propose a distributed reinforcement learning method to accelerate the agent's training. Finally, we proposed five types of model fusion scenarios to validate our approach, and experiments show that our proposed algorithm can work reliably in all kinds of model fusion scenarios while maintaining the accuracy of the global model.

Currently, our algorithm requires separate training of the agent according to the client fusion scenarios. In future work, we will extend the proposed algorithm to accommodate model fusion tasks for different task scenarios. We will also continue improving the algorithm's performance to achieve more client tasks.

% \section{ACKNOWLEDGMENT}
% The work is supported by the Academic Research Fund of the  Education Ministry of Singapore (Tier 1 No.022307), the Hong Kong ITF Project
%  (No. ITS/188/20), and the National Natural Science Foundation of China (No.62072469). The work is also supported by the China Scholarship Council (CSC) for the research of Chen Leiming at Nanyang Technological University.

\begin{thebibliography}{100}

\bibitem{FedAvg} Brendan McMahan et al. “Communication-efficient learning of deep networks from decentralized
data”. In: Artificial intelligence and statistics. PMLR. 2017, pp. 1273–1282.
\bibitem{FedProx} Tian Li et al. “Federated optimization in heterogeneous networks”. In: Proceedings of Machine
learning and systems 2 (2020), pp. 429–450.
\bibitem{Scaffold} Sai Praneeth Karimireddy et al. “Scaffold: Stochastic controlled averaging for federated learning”.
In: International Conference on Machine Learning. PMLR. 2020, pp. 5132–5143.
\bibitem{FedNova} Jianyu Wang et al. “Tackling the objective inconsistency problem in heterogeneous federated
optimization”. In: Advances in neural information processing systems 33 (2020), pp. 7611–7623.
\bibitem{MOON} Qinbin Li, Bingsheng He, and Dawn Song. “Model-contrastive federated learning”. In: Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021, pp. 10713–
10722.
\bibitem{PGFFL} Yaqi Sun et al. “A Fair Federated Learning Framework With Reinforcement Learning”. In: 2022
International Joint Conference on Neural Networks (IJCNN). IEEE. 2022, pp. 1–8.
\bibitem{Favor} Hao Wang et al. “Optimizing federated learning on non-iid data with reinforcement learning”. In:
IEEE INFOCOM 2020-IEEE Conference on Computer Communications. IEEE. 2020, pp. 1698–
1707.
\bibitem{AttackRL} Henger Li, Xiaolin Sun, and Zizhan Zheng. “Learning to Attack Federated Learning: A Model-
based Reinforcement Learning Attack Framework”. In: Advances in Neural Information Process-
ing Systems. 2022.
\bibitem{zhangPeiYingRL} Peiying Zhang et al. “Deep reinforcement learning assisted federated learning algorithm for data
management of IIoT”. In: IEEE Transactions on Industrial Informatics 17.12 (2021), pp. 8475–
8484.
\bibitem{digitalTwinRL} Wei Yang et al. “Optimizing federated learning with deep reinforcement learning for digital
twin empowered industrial IoT”. In: IEEE Transactions on Industrial Informatics 19.2 (2022),
pp. 1884–1893.
\bibitem{RoF} Weiting Zhang et al. “Optimizing federated learning in distributed industrial IoT: A multi-agent
approach”. In: IEEE Journal on Selected Areas in Communications 39.12 (2021), pp. 3688–3703.
\bibitem{DDQNTrust} Gaith Rjoub et al. “Trust-driven reinforcement selection strategy for federated learning on IoT
devices”. In: Computing (2022), pp. 1–23.

\bibitem{nonIIDProblem} Yue Zhao et al. “Federated learning with non-iid data”. In: arXiv preprint arXiv:1806.00582
(2018).
\bibitem{FedPD} Xinwei Zhang et al. “FedPD: A federated learning framework with adaptivity to non-iid data”.
In: IEEE Transactions on Signal Processing 69 (2021), pp. 6055–6070.
\bibitem{AutoCFL} Biyao Gong et al. “Adaptive client clustering for efficient federated learning over non-iid and
imbalanced data”. In: IEEE Transactions on Big Data (2022).
\bibitem{FedAMP} Yutao Huang et al. “Personalized cross-silo federated learning on non-iid data”. In: Proceedings
of the AAAI Conference on Artificial Intelligence. Vol. 35. 9. 2021, pp. 7865–7873.
\bibitem{Fedbn} Xiaoxiao Li et al. “Fedbn: Federated learning on non-iid features via local batch normalization”.
In: arXiv preprint arXiv:2102.07623 (2021).
\bibitem{FLHC} Christopher Briggs, Zhong Fan, and Peter Andras. “Federated learning with hierarchical cluster-
ing of local updates to improve training on non-IID data”. In: 2020 International Joint Conference
on Neural Networks (IJCNN). IEEE. 2020, pp. 1–9.
\bibitem{Feddc} Liang Gao et al. “Feddc: Federated learning with non-iid data via local drift decoupling and
correction”. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 2022, pp. 10112–10121.
\bibitem{Fedproc} Xutong Mu et al. “Fedproc: Prototypical contrastive federated learning on non-iid data”. In:
Future Generation Computer Systems 143 (2023), pp. 93–104.
\bibitem{FedMarl} Sai Qian Zhang, Jieyu Lin, and Qi Zhang. “A multi-agent reinforcement learning approach for ef-
ficient client selection in federated learning”. In: Proceedings of the AAAI Conference on Artificial
Intelligence. Vol. 36. 8. 2022, pp. 9091–9099.
\bibitem{trustClient} Gaith Rjoub et al. “Trust-augmented deep reinforcement learning for federated learning client
selection”. In: Information Systems Frontiers (2022), pp. 1–18.
\bibitem{evaluationRL} Nuocheng Yang et al. “Model-Based Reinforcement Learning for Quantized Federated Learning
Performance Optimization”. In: GLOBECOM 2022-2022 IEEE Global Communications Confer-
ence. IEEE. 2022, pp. 5063–5068.
\bibitem{A3C} Volodymyr Mnih et al. “Asynchronous methods for deep reinforcement learning”. In: International
conference on machine learning. PMLR. 2016, pp. 1928–1937.
\bibitem{TD3} Scott Fujimoto, Herke Hoof, and David Meger. “Addressing function approximation error in
actor-critic methods”. In: International conference on machine learning. PMLR. 2018, pp. 1587–
1596.
\bibitem{Dirichlet} Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. “Measuring the effects of non-identical
data distribution for federated visual classification”. In: arXiv preprint arXiv:1909.06335 (2019).
\bibitem{stablebaseline3} Antonin Raffin et al. “Stable-baselines3: Reliable reinforcement learning implementations”. In:
The Journal of Machine Learning Research 22.1 (2021), pp. 12348–12355.


\end{thebibliography}
\end{document}

