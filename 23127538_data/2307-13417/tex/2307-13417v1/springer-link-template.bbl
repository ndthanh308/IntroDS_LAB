% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{Mars2023}{inreference}{}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labeltitlesource}{title}
      \field{abstract}{Mars steht für: Mars (Mythologie), Kriegsgott im antiken Italien Mars (Planet), vierter Planet des Sonnensystems Mars (Schokoriegel), Süßware Mars (Schifffahrt), Teil des Segelschiffmastes Mars (Raumsonde), Serie sowjetischer Raumsonden Mars, Automarke (1913), siehe Slatiňanská továrna automobilů R. A. Smekal MArs, Marinearsenal, Dienststelle der Bundeswehr, siehe Marinearsenal (Deutschland) MaRS Discovery District, Forschungspark in Toronto für Medizin und angelehnte ForschungsgebietePersonen: Mars (Familie), US-amerikanische Familie Mars (Familienname) – siehe dort zu Namensträgern Mars, Künstlername des deutschen Sängers und Schauspielers Marcel SaibertKultur: Mars (Band), US-amerikanische No-Wave-Band (1975–1978) Mars (Fritz Zorn), autobiografisches Werk von Fritz Zorn (1977) Mars (Computerspiel), Computerspiel von Aardvark aus dem Jahr 1982 Mars (Manga), japanisches Manga von Fuyumi Sōryō (ab 1995) Mars (Fernsehserie), Fernsehserie des National Geographic Channel (2016)Unternehmen: Mars-Film, deutsche Filmproduktionsgesellschaften Mars Incorporated, US-amerikanischer Nahrungsmittelkonzern Mars Austria, Tochter von Mars Incorporated Mars Deutschland, Tochter von Mars Incorporated Mars Motor Company, britischer Automobilhersteller Mars-Werke, deutscher MotorradherstellerSchiffe: HMS Mars (1794), 74-Kanonen-Linienschiff der britischen Royal Navy HMS Mars (1896), Vor-Dreadnought-Linienschiff der Majestic-Klasse der britischen Royal Mars (Schiff, 1705), 54-Kanonen-Linienschiff der französischen Marine (1706–1719) Mars (Schiff, 1740), 64-Kanonen-Linienschiff der französischen Marine (1741–1755) Mars (Schiff, 1769), 64-Kanonen-Linienschiff der französischen Marine (1770–1773) Mars (Schiff, 1563), schwedisches Kriegsschiff Mars (Schiff, 1922) schweizerisches Passagierschiff auf dem Bodensee Mars (Schiff, 1927), Fahrgastschiff in Berlin, siehe Tempelhof (Schiff, 1927) USS Mars (AFS-1), Versorgungsschiff der United States Pacific Fleet Martin JRM Mars, US-amerikanischer Flugboottyp, siehe Martin JRM Liste von Schiffen mit dem Namen Mars Liste der Namen deutscher Kriegsschiffe (A–M)\#MaMars ist der Name folgender geografischer Objekte: Mars, Verwaltungseinheit oder Provinz in Armenien, siehe Armenien\#Verwaltungseinheiten Mars (Ukraine), ukrainisch Марс, Dorf in der Nordukraine Mars (Pennsylvania), Ortschaft in den Vereinigten StaatenFrankreich: Mars (Fluss), Fluss im Einzugsbereich der Dordogne, Frankreich Mars (Ardèche), Gemeinde im Département Ardèche Mars (Gard), Gemeinde im Département Gard (ehemalige Gemeinde) Mars (Loire), Gemeinde im Département Loire Mars-la-Tour, Gemeinde im Nordosten Frankreichs Mars-sous-Bourcq, Gemeinde im Département Ardennes Mars-sur-Allier, Gemeinde im Département Nièvre Les Mars, Gemeinde im Département CreuseAntarktis: Mars-Gletscher, Gletscher auf der Alexander-I.-Insel Mars Col, Gebirgspass auf der James-Ross-Insel Mars Hills, Hügelgruppe im Viktorialand Mars Island, Insel vor der Knox-Küste des Wilkeslands Mars-Oase, auf der Alexander-I.-Insel MARS steht für: MARS (Software), Emulator zur Simulierung der MIPS32-Architektur (MIPS Assembler and Runtime Simulator) MARS (Verschlüsselung), Verschlüsselungsalgorithmus Mauritius Amateur Radio Society, Amateurfunkverband auf Mauritius Memory Array Redcode Simulator, virtuelle Maschine für das Programmierspiel Core War Mid-Atlantic Regional Spaceport, US-amerikanischer Weltraumbahnhof Military Auxiliary Radio System der US-Streitkräfte Mittleres Artillerieraketensystem der Bundeswehr, siehe Multiple Launch Rocket System Modern Architectural Research Group, britische Architektengruppe (1933–1957) Molecular Adsorbent Recirculation System, medizinisches Gerät zur Reinigung des Bluts außerhalb des Körpers Mongolian Amateur Radio Society, ein mongolischer AmateurfunkverbandSiehe auch: Liste aller Wikipedia-Artikel, deren Titel mit Mars beginnt Marrs (Begriffsklärung) Marz (Begriffsklärung)}
      \field{annotation}{Page Version ID: 232706227}
      \field{booktitle}{Wikipedia}
      \field{day}{11}
      \field{hour}{23}
      \field{langid}{ngerman}
      \field{minute}{57}
      \field{month}{4}
      \field{second}{18}
      \field{timezone}{Z}
      \field{title}{Mars}
      \field{urlday}{27}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/QU5827WH/Mars.html
      \endverb
      \verb{urlraw}
      \verb https://de.wikipedia.org/w/index.php?title=Mars&oldid=232706227
      \endverb
      \verb{url}
      \verb https://de.wikipedia.org/w/index.php?title=Mars&oldid=232706227
      \endverb
    \endentry
    \entry{wittgenstein1953philosophical}{book}{}
      \name{author}{1}{}{%
        {{hash=3234c99a3eebf68ef7d42684cdb8ed8f}{%
           family={Wittgenstein},
           familyi={W\bibinitperiod},
           given={Ludwig},
           giveni={L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Oxford}%
      }
      \list{publisher}{1}{%
        {Basil Blackwell}%
      }
      \strng{namehash}{3234c99a3eebf68ef7d42684cdb8ed8f}
      \strng{fullhash}{3234c99a3eebf68ef7d42684cdb8ed8f}
      \strng{bibnamehash}{3234c99a3eebf68ef7d42684cdb8ed8f}
      \strng{authorbibnamehash}{3234c99a3eebf68ef7d42684cdb8ed8f}
      \strng{authornamehash}{3234c99a3eebf68ef7d42684cdb8ed8f}
      \strng{authorfullhash}{3234c99a3eebf68ef7d42684cdb8ed8f}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{0631119000}
      \field{title}{Philosophical Investigations}
      \field{year}{1953}
      \keyw{acts conversational dialogue speech theory}
    \endentry
    \entry{vaswani2017attention}{misc}{}
      \name{author}{8}{}{%
        {{hash=7f28e84700536646dd6620a0db07ad09}{%
           family={Vaswani},
           familyi={V\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=62efade83d70f0323fe248755e6c90c5}{%
           family={Shazeer},
           familyi={S\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
        {{hash=06649ebab1ea5cac0250746a19764975}{%
           family={Parmar},
           familyi={P\bibinitperiod},
           given={Niki},
           giveni={N\bibinitperiod}}}%
        {{hash=831027ee0ebf22375e2a86afc1881909}{%
           family={Uszkoreit},
           familyi={U\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=2fd2982e30ebcec93ec1cf76e0d797fd}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Llion},
           giveni={L\bibinitperiod}}}%
        {{hash=27b07e4eacbf4ef7a1438e3badb7dd8d}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Aidan\bibnamedelima N.},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=95595a0fefb86187cbc36e551017d332}{%
           family={Polosukhin},
           familyi={P\bibinitperiod},
           given={Illia},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{fullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \strng{bibnamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authorbibnamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authornamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authorfullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{title}{Attention Is All You Need}
      \field{year}{2017}
      \verb{eprint}
      \verb 1706.03762
      \endverb
    \endentry
    \entry{ChatGPT}{online}{}
      \field{sortinit}{5}
      \field{sortinithash}{20e9b4b0b173788c5dace24730f47d8c}
      \field{labeltitlesource}{title}
      \field{title}{ChatGPT}
      \field{urlday}{12}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://openai.com/blog/chatgpt
      \endverb
      \verb{url}
      \verb https://openai.com/blog/chatgpt
      \endverb
    \endentry
    \entry{mikolov2013efficient}{misc}{}
      \name{author}{4}{}{%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=ee3f7d7b96add98106db907e189d6c13}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=3d11e6f2a0d0a1183b2cf62996525afc}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{fullhash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{bibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authorbibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authornamehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{authorfullhash}{f24c60896b6daa69474b40efb61f4e88}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{title}{Efficient Estimation of Word Representations in Vector Space}
      \field{year}{2013}
      \verb{eprint}
      \verb 1301.3781
      \endverb
    \endentry
    \entry{DensitybasedAlgorithmDiscovering1996}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=2c062e64ed26aacc08a62155e7944f04}{%
           family={Ester},
           familyi={E\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=9559fe65ed2c0877cf14a66fe1f8e9b3}{%
           family={Kriegel},
           familyi={K\bibinitperiod},
           given={Hans-Peter},
           giveni={H\bibinithyphendelim P\bibinitperiod}}}%
        {{hash=802157026f850823b2027c2100cb359a}{%
           family={Sander},
           familyi={S\bibinitperiod},
           given={Jörg},
           giveni={J\bibinitperiod}}}%
        {{hash=2dda16c0a5d50fc830d0d4a3787937fa}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Xiaowei},
           giveni={X\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Portland, Oregon}%
      }
      \list{publisher}{1}{%
        {AAAI Press}%
      }
      \strng{namehash}{93f88297c66aca5bc89c3c5131ea9566}
      \strng{fullhash}{3270dfaa31e8210b3bd04b1bcf4a29a3}
      \strng{bibnamehash}{3270dfaa31e8210b3bd04b1bcf4a29a3}
      \strng{authorbibnamehash}{3270dfaa31e8210b3bd04b1bcf4a29a3}
      \strng{authornamehash}{93f88297c66aca5bc89c3c5131ea9566}
      \strng{authorfullhash}{3270dfaa31e8210b3bd04b1bcf4a29a3}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic models and real models of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.}
      \field{booktitle}{Proceedings of the {{Second International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}}
      \field{day}{2}
      \field{month}{8}
      \field{series}{{{KDD}}'96}
      \field{title}{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}
      \field{year}{1996}
      \field{dateera}{ce}
      \field{pages}{226\bibrangedash 231}
      \range{pages}{6}
      \keyw{arbitrary shape of clusters,clustering algorithms,efficiency on large spatial databases,handling nlj4-275oise}
    \endentry
    \entry{mitraDualEmbeddingSpace2016}{online}{}
      \name{author}{4}{}{%
        {{hash=29578de74e3c76b88989de43125385ca}{%
           family={Mitra},
           familyi={M\bibinitperiod},
           given={Bhaskar},
           giveni={B\bibinitperiod}}}%
        {{hash=32d37e1a4205d86bb4fd9e9e07b9607d}{%
           family={Nalisnick},
           familyi={N\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod}}}%
        {{hash=7a3b8c75373fcf308a7e8cda62ddd9ce}{%
           family={Craswell},
           familyi={C\bibinitperiod},
           given={Nick},
           giveni={N\bibinitperiod}}}%
        {{hash=599c5251d489b92dc7856277d56cb1a9}{%
           family={Caruana},
           familyi={C\bibinitperiod},
           given={Rich},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{ee3a11f489d1029fa4b7db7e1fea43c0}
      \strng{fullhash}{55895676a53820f51f1d43e676b4ab69}
      \strng{bibnamehash}{55895676a53820f51f1d43e676b4ab69}
      \strng{authorbibnamehash}{55895676a53820f51f1d43e676b4ab69}
      \strng{authornamehash}{ee3a11f489d1029fa4b7db7e1fea43c0}
      \strng{authorfullhash}{55895676a53820f51f1d43e676b4ab69}
      \field{sortinit}{8}
      \field{sortinithash}{a231b008ebf0ecbe0b4d96dcc159445f}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A fundamental goal of search engines is to identify, given a query, documents that have relevant text. This is intrinsically difficult because the query and the document may use different vocabulary, or the document may contain query words without being relevant. We investigate neural word embeddings as a source of evidence in document ranking. We train a word2vec embedding model on a large unlabelled query corpus, but in contrast to how the model is commonly used, we retain both the input and the output projections, allowing us to leverage both the embedding spaces to derive richer distributional relationships. During ranking we map the query words into the input space and the document words into the output space, and compute a query-document relevance score by aggregating the cosine similarities across all the query-document word pairs. We postulate that the proposed Dual Embedding Space Model (DESM) captures evidence on whether a document is about a query term in addition to what is modelled by traditional term-frequency based approaches. Our experiments show that the DESM can re-rank top documents returned by a commercial Web search engine, like Bing, better than a term-matching based signal like TF-IDF. However, when ranking a larger set of candidate documents, we find the embeddings-based approach is prone to false positives, retrieving documents that are only loosely related to the query. We demonstrate that this problem can be solved effectively by ranking based on a linear mixture of the DESM and the word counting features.}
      \field{day}{2}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{2}
      \field{pubstate}{preprint}
      \field{title}{A {{Dual Embedding Space Model}} for {{Document Ranking}}}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1602.01137
      \endverb
      \verb{eprint}
      \verb 1602.01137
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/X5I2IZ4R/Mitra et al. - 2016 - A Dual Embedding Space Model for Document Ranking.pdf;/home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/2BE3ATRL/1602.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1602.01137
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1602.01137
      \endverb
      \keyw{Computer Science - Information Retrieval}
    \endentry
    \entry{WordNetElectronicLexical1998}{book}{}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{WordNet is an on-line lexical reference system whose design isinspired by current psycholinguistic theories of human lexical memory;version 1.6 is the most up-t}
      \field{day}{22}
      \field{langid}{english}
      \field{month}{5}
      \field{shorttitle}{{{WordNet}}}
      \field{title}{{{WordNet}}: {{An Electronic Lexical Database}}}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{1998}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.7551/mitpress/7287.001.0001
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/SFA8TYGG/WordNetAn-Electronic-Lexical-Database.html
      \endverb
      \verb{urlraw}
      \verb https://direct.mit.edu/books/book/1928/WordNetAn-Electronic-Lexical-Database
      \endverb
      \verb{url}
      \verb https://direct.mit.edu/books/book/1928/WordNetAn-Electronic-Lexical-Database
      \endverb
    \endentry
    \entry{orkpholWordSenseDisambiguation2019}{article}{}
      \name{author}{2}{}{%
        {{hash=ea700ace01fa64e74c5e828b4b7351db}{%
           family={Orkphol},
           familyi={O\bibinitperiod},
           given={Korawit},
           giveni={K\bibinitperiod}}}%
        {{hash=04a3b352c2bacf8357d1ba9ea1d109a9}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Wu},
           giveni={W\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Multidisciplinary Digital Publishing Institute}%
      }
      \strng{namehash}{aab3b068f202e9c66c2a2be90adf4827}
      \strng{fullhash}{aab3b068f202e9c66c2a2be90adf4827}
      \strng{bibnamehash}{aab3b068f202e9c66c2a2be90adf4827}
      \strng{authorbibnamehash}{aab3b068f202e9c66c2a2be90adf4827}
      \strng{authornamehash}{aab3b068f202e9c66c2a2be90adf4827}
      \strng{authorfullhash}{aab3b068f202e9c66c2a2be90adf4827}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Words have different meanings (i.e., senses) depending on the context. Disambiguating the correct sense is important and a challenging task for natural language processing. An intuitive way is to select the highest similarity between the context and sense definitions provided by a large lexical database of English, WordNet. In this database, nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms interlinked through conceptual semantics and lexicon relations. Traditional unsupervised approaches compute similarity by counting overlapping words between the context and sense definitions which must match exactly. Similarity should compute based on how words are related rather than overlapping by representing the context and sense definitions on a vector space model and analyzing distributional semantic relationships among them using latent semantic analysis (LSA). When a corpus of text becomes more massive, LSA consumes much more memory and is not flexible to train a huge corpus of text. A word-embedding approach has an advantage in this issue. Word2vec is a popular word-embedding approach that represents words on a fix-sized vector space model through either the skip-gram or continuous bag-of-words (CBOW) model. Word2vec is also effectively capturing semantic and syntactic word similarities from a huge corpus of text better than LSA. Our method used Word2vec to construct a context sentence vector, and sense definition vectors then give each word sense a score using cosine similarity to compute the similarity between those sentence vectors. The sense definition also expanded with sense relations retrieved from WordNet. If the score is not higher than a specific threshold, the score will be combined with the probability of that sense distribution learned from a large sense-tagged corpus, SEMCOR. The possible answer senses can be obtained from high scores. Our method shows that the result (50.9\% or 48.7\% without the probability of sense distribution) is higher than the baselines (i.e., original, simplified, adapted and LSA Lesk) and outperforms many unsupervised systems participating in the SENSEVAL-3 English lexical sample task.}
      \field{issn}{1999-5903}
      \field{issue}{5}
      \field{journaltitle}{Future Internet}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{5}
      \field{title}{Word {{Sense Disambiguation Using Cosine Similarity Collaborates}} with {{Word2vec}} and {{WordNet}}}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{volume}{11}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{114}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/fi11050114
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/5B2RRPRF/Orkphol und Yang - 2019 - Word Sense Disambiguation Using Cosine Similarity .pdf
      \endverb
      \verb{urlraw}
      \verb https://www.mdpi.com/1999-5903/11/5/114
      \endverb
      \verb{url}
      \verb https://www.mdpi.com/1999-5903/11/5/114
      \endverb
      \keyw{natural language processing,word embedding,word sense disambiguation,Word2vec,WordNet}
    \endentry
    \entry{nurifanDevelopingCorporaUsing2018}{article}{}
      \name{author}{3}{}{%
        {{hash=89e42d9d009c0b86e8f049889fe87d2f}{%
           family={Nurifan},
           familyi={N\bibinitperiod},
           given={Farza},
           giveni={F\bibinitperiod}}}%
        {{hash=2fc2d9be8db1e30ff5152ce1e38fcf46}{%
           family={Sarno},
           familyi={S\bibinitperiod},
           given={Riyanarto},
           giveni={R\bibinitperiod}}}%
        {{hash=17e354e355ce10457b8c5f74253b397c}{%
           family={Wahyuni},
           familyi={W\bibinitperiod},
           given={Cahyaningtyas},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{db9521cad317ee0cb0d31fae587c5b33}
      \strng{fullhash}{db9521cad317ee0cb0d31fae587c5b33}
      \strng{bibnamehash}{db9521cad317ee0cb0d31fae587c5b33}
      \strng{authorbibnamehash}{db9521cad317ee0cb0d31fae587c5b33}
      \strng{authornamehash}{db9521cad317ee0cb0d31fae587c5b33}
      \strng{authorfullhash}{db9521cad317ee0cb0d31fae587c5b33}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Word Sense Disambiguation (WSD) is one of the most difficult problems in the artificial intelligence field or well known as AI-hard or AI-complete. A lot of problems can be solved using word sense disambiguation approach such as sentiment analysis, machine translation, search engine relevance, coherence, anaphora resolution, and inference. This research is done to solve WSD problem with two small corpora. The use of Word2vec and Wikipedia are proposed to develop the corpora. After developing the corpora, the similarity of the sentence with the corpora is measured using cosine similarity to determine the meaning of the ambiguous word. Lastly, to improve accuracy, Lesk algorithms and Wu Palmer similarity are used to deal with problems when there is no word from a sentence in the corpus. The results of the research show an 85.51\% accuracy rate and the semantic similarity improve the accuracy rate by 8.02\% in determining the meaning of ambiguous words. © 2018 Institute of Advanced Engineering and Science. All rights reserved.}
      \field{day}{1}
      \field{journaltitle}{Indonesian Journal of Electrical Engineering and Computer Science}
      \field{month}{12}
      \field{title}{Developing Corpora Using Word2vec and Wikipedia for Word Sense Disambiguation}
      \field{volume}{12}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{pages}{1239\bibrangedash 1246}
      \range{pages}{8}
      \verb{doi}
      \verb 10.11591/ijeecs.v12.i3.pp1239-1246
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/6I7FSCVM/Nurifan et al. - 2018 - Developing corpora using word2vec and wikipedia fo.pdf
      \endverb
    \endentry
    \entry{kutuzovLemmatizeNotLemmatize2019}{online}{}
      \name{author}{2}{}{%
        {{hash=ce3299cf2e9aab02a7bf6305fdc3dc03}{%
           family={Kutuzov},
           familyi={K\bibinitperiod},
           given={Andrey},
           giveni={A\bibinitperiod}}}%
        {{hash=66eba27bf795964e923d798d1d1de705}{%
           family={Kuzmenko},
           familyi={K\bibinitperiod},
           given={Elizaveta},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{78f57550b5b9db66ad7705ecbd1546ed}
      \strng{fullhash}{78f57550b5b9db66ad7705ecbd1546ed}
      \strng{bibnamehash}{78f57550b5b9db66ad7705ecbd1546ed}
      \strng{authorbibnamehash}{78f57550b5b9db66ad7705ecbd1546ed}
      \strng{authornamehash}{78f57550b5b9db66ad7705ecbd1546ed}
      \strng{authorfullhash}{78f57550b5b9db66ad7705ecbd1546ed}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We critically evaluate the widespread assumption that deep learning NLP models do not require lemmatized input. To test this, we trained versions of contextualised word embedding ELMo models on raw tokenized corpora and on the corpora with word tokens replaced by their lemmas. Then, these models were evaluated on the word sense disambiguation task. This was done for the English and Russian languages. The experiments showed that while lemmatization is indeed not necessary for English, the situation is different for Russian. It seems that for rich-morphology languages, using lemmatized training and testing models yields small but consistent improvements: at least for word sense disambiguation. This means that the decisions about text pre-processing before training ELMo should consider the linguistic nature of the language in question.}
      \field{day}{6}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{9}
      \field{pubstate}{preprint}
      \field{shorttitle}{To Lemmatize or Not to Lemmatize}
      \field{title}{To Lemmatize or Not to Lemmatize: How Word Normalisation Affects {{ELMo}} Performance in Word Sense Disambiguation}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1909.03135
      \endverb
      \verb{eprint}
      \verb 1909.03135
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/L2ISIVFP/Kutuzov und Kuzmenko - 2019 - To lemmatize or not to lemmatize how word normali.pdf;/home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/BMTAG98T/1909.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1909.03135
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1909.03135
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{petersDeepContextualizedWord2018}{online}{}
      \name{author}{7}{}{%
        {{hash=711e37ce316a72d79bd008a205513ef0}{%
           family={Peters},
           familyi={P\bibinitperiod},
           given={Matthew\bibnamedelima E.},
           giveni={M\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=93f95e4f65833691b9a15cbe2791e2df}{%
           family={Neumann},
           familyi={N\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
        {{hash=48ea6f65be60970c81190df53e86239e}{%
           family={Iyyer},
           familyi={I\bibinitperiod},
           given={Mohit},
           giveni={M\bibinitperiod}}}%
        {{hash=6adacaf607ef85a7b55da44661636929}{%
           family={Gardner},
           familyi={G\bibinitperiod},
           given={Matt},
           giveni={M\bibinitperiod}}}%
        {{hash=9edd3110f9350150b9a22ef5b7d45d25}{%
           family={Clark},
           familyi={C\bibinitperiod},
           given={Christopher},
           giveni={C\bibinitperiod}}}%
        {{hash=8dde73b4194f5bc4230c4808f3fc1534}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Kenton},
           giveni={K\bibinitperiod}}}%
        {{hash=1dbd3a5b42828fb2cebd7786488ba425}{%
           family={Zettlemoyer},
           familyi={Z\bibinitperiod},
           given={Luke},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{67a67e50175f2ab2af2075a1779f2f06}
      \strng{fullhash}{e4ae16c29e91a94c3ee954353fd212ab}
      \strng{bibnamehash}{67a67e50175f2ab2af2075a1779f2f06}
      \strng{authorbibnamehash}{67a67e50175f2ab2af2075a1779f2f06}
      \strng{authornamehash}{67a67e50175f2ab2af2075a1779f2f06}
      \strng{authorfullhash}{e4ae16c29e91a94c3ee954353fd212ab}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.}
      \field{day}{22}
      \field{eprintclass}{cs}
      \field{eprinttype}{arxiv}
      \field{month}{3}
      \field{pubstate}{preprint}
      \field{title}{Deep Contextualized Word Representations}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1802.05365
      \endverb
      \verb{eprint}
      \verb 1802.05365
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/QDXQXLPW/Peters et al. - 2018 - Deep contextualized word representations.pdf;/home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/7Z8TDQ7L/1802.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1802.05365
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1802.05365
      \endverb
      \keyw{Computer Science - Computation and Language}
    \endentry
    \entry{alkhatlanWordSenseDisambiguation2018}{article}{}
      \name{author}{3}{}{%
        {{hash=64c0e619bb4d5d091dc89d2af462fd6d}{%
           family={Alkhatlan},
           familyi={A\bibinitperiod},
           given={Ali},
           giveni={A\bibinitperiod}}}%
        {{hash=ce02eab0712746bf0b0877dfbbc3d3bd}{%
           family={Kalita},
           familyi={K\bibinitperiod},
           given={Jugal},
           giveni={J\bibinitperiod}}}%
        {{hash=9894416672463ad2dcb37d47eb0917d6}{%
           family={Alhaddad},
           familyi={A\bibinitperiod},
           given={Ahmed},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{e8871adb062f2a2c6d312268542fe095}
      \strng{fullhash}{e8871adb062f2a2c6d312268542fe095}
      \strng{bibnamehash}{e8871adb062f2a2c6d312268542fe095}
      \strng{authorbibnamehash}{e8871adb062f2a2c6d312268542fe095}
      \strng{authornamehash}{e8871adb062f2a2c6d312268542fe095}
      \strng{authorfullhash}{e8871adb062f2a2c6d312268542fe095}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Word Sense Disambiguation (WSD) is a task which aims to identify the meaning of a word given its context. This problem has been investigated and analyzed in depth in English. However, work in Arabic has been limited despite the fact that there are half a billion native Arabic speakers. In this work, we present multiple approaches for the problem of WSD in Arabic utilizing recent developments and successes in learning word embeddings with approaches such as GloVe, and Word2vec. The primary shortcoming of word embeddings is the single vector representation of a word’s meaning, although many words are polysemous. Our main contribution in this work is to computationally obtain an embedding for each sense, using an Arabic WordNet (AWN) to overcome the problem of WSD. We also compute word semantic similarity giving thought to multiple Arabic stemming algorithms. Finally, we make available a large pre-processed corpus that is ready to be used for further experiments and a WSD test models based on AWN,1 seeking to fill gaps in Arabic NLP (ANLP) compared to English.}
      \field{day}{1}
      \field{issn}{1877-0509}
      \field{journaltitle}{Procedia Computer Science}
      \field{langid}{english}
      \field{month}{1}
      \field{series}{Arabic {{Computational Linguistics}}}
      \field{title}{Word {{Sense Disambiguation}} for {{Arabic Exploiting Arabic WordNet}} and {{Word Embedding}}}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{volume}{142}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{50\bibrangedash 60}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1016/j.procs.2018.10.460
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/T4KT7Z87/Alkhatlan et al. - 2018 - Word Sense Disambiguation for Arabic Exploiting Ar.pdf;/home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/MZMFIK4Y/S1877050918321616.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S1877050918321616
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S1877050918321616
      \endverb
      \keyw{Arabic WordNet,Neural Network,Word Embedding,Word Sense Disambiguation}
    \endentry
    \entry{huangImprovingWordRepresentations2012}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=db1973092153d2cd94afd3a421a867d4}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Eric\bibnamedelima H.},
           giveni={E\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=d5670b2600fea169724521e252d9d09d}{%
           family={Socher},
           familyi={S\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
        {{hash=2214edb8305f7ccd7cdc310b3a8ae1b4}{%
           family={Manning},
           familyi={M\bibinitperiod},
           given={Christopher\bibnamedelima D.},
           giveni={C\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=49e889356ff39df159461bc2895c7e16}{%
           family={Ng},
           familyi={N\bibinitperiod},
           given={Andrew\bibnamedelima Y.},
           giveni={A\bibinitperiod\bibinitdelim Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {USA}%
      }
      \list{publisher}{1}{%
        {Association for Computational Linguistics}%
      }
      \strng{namehash}{18b861fec8a9242ca1af3fe79e2115d9}
      \strng{fullhash}{fd6660e4fab4dd5cc4fd527589a4bbf1}
      \strng{bibnamehash}{fd6660e4fab4dd5cc4fd527589a4bbf1}
      \strng{authorbibnamehash}{fd6660e4fab4dd5cc4fd527589a4bbf1}
      \strng{authornamehash}{18b861fec8a9242ca1af3fe79e2115d9}
      \strng{authorfullhash}{fd6660e4fab4dd5cc4fd527589a4bbf1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.}
      \field{booktitle}{Proceedings of the 50th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Long Papers}} - {{Volume}} 1}
      \field{day}{8}
      \field{month}{7}
      \field{series}{{{ACL}} '12}
      \field{title}{Improving Word Representations via Global Context and Multiple Word Prototypes}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{pages}{873\bibrangedash 882}
      \range{pages}{10}
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/IASPB7I7/Huang et al. - 2012 - Improving word representations via global context .pdf
      \endverb
    \endentry
    \entry{schutze-1998-automatic}{article}{}
      \name{author}{1}{}{%
        {{hash=9a80de0e5ec6df57363aaf321a5d05be}{%
           family={Schütze},
           familyi={S\bibinitperiod},
           given={Hinrich},
           giveni={H\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge, MA}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{9a80de0e5ec6df57363aaf321a5d05be}
      \strng{fullhash}{9a80de0e5ec6df57363aaf321a5d05be}
      \strng{bibnamehash}{9a80de0e5ec6df57363aaf321a5d05be}
      \strng{authorbibnamehash}{9a80de0e5ec6df57363aaf321a5d05be}
      \strng{authornamehash}{9a80de0e5ec6df57363aaf321a5d05be}
      \strng{authorfullhash}{9a80de0e5ec6df57363aaf321a5d05be}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Computational Linguistics}
      \field{number}{1}
      \field{title}{Automatic Word Sense Discrimination}
      \field{volume}{24}
      \field{year}{1998}
      \field{pages}{97\bibrangedash 123}
      \range{pages}{27}
      \verb{urlraw}
      \verb https://aclanthology.org/J98-1004
      \endverb
      \verb{url}
      \verb https://aclanthology.org/J98-1004
      \endverb
    \endentry
    \entry{WikimediaDownloads}{online}{}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labeltitlesource}{title}
      \field{title}{Wikimedia {{Downloads}}}
      \field{urlday}{19}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/YBK37649/dumps.wikimedia.org.html
      \endverb
      \verb{urlraw}
      \verb https://dumps.wikimedia.org/
      \endverb
      \verb{url}
      \verb https://dumps.wikimedia.org/
      \endverb
    \endentry
    \entry{spaCy}{unpublished}{}
      \name{author}{2}{}{%
        {{hash=31cf0cd70714219d39b46b0f8aae95f9}{%
           family={Honnibal},
           familyi={H\bibinitperiod},
           given={Matthew},
           giveni={M\bibinitperiod}}}%
        {{hash=47c9fa7dcc4eb3484a88f4bb42655895}{%
           family={Montani},
           familyi={M\bibinitperiod},
           given={Ines},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{8660080fbd0e3e56dd4bd2ac6c3a639d}
      \strng{fullhash}{8660080fbd0e3e56dd4bd2ac6c3a639d}
      \strng{bibnamehash}{8660080fbd0e3e56dd4bd2ac6c3a639d}
      \strng{authorbibnamehash}{8660080fbd0e3e56dd4bd2ac6c3a639d}
      \strng{authornamehash}{8660080fbd0e3e56dd4bd2ac6c3a639d}
      \strng{authorfullhash}{8660080fbd0e3e56dd4bd2ac6c3a639d}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{note}{To appear}
      \field{title}{{spaCy 2}: Natural language understanding with {B}loom embeddings, convolutional neural networks and incremental parsing}
      \field{year}{2017}
    \endentry
    \entry{alomranChoosingNLPLibrary2017}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=88e0690120a827f91c692bf9d43b456a}{%
           family={Al\bibnamedelima Omran},
           familyi={A\bibinitperiod\bibinitdelim O\bibinitperiod},
           given={Fouad\bibnamedelimb Nasser\bibnamedelima A},
           giveni={F\bibinitperiod\bibinitdelim N\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=fc21c10b2d386c8460fa30c762643c06}{%
           family={Treude},
           familyi={T\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{3d482b77c217436666c3823decd6bab6}
      \strng{fullhash}{3d482b77c217436666c3823decd6bab6}
      \strng{bibnamehash}{3d482b77c217436666c3823decd6bab6}
      \strng{authorbibnamehash}{3d482b77c217436666c3823decd6bab6}
      \strng{authornamehash}{3d482b77c217436666c3823decd6bab6}
      \strng{authorfullhash}{3d482b77c217436666c3823decd6bab6}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{To uncover interesting and actionable information from natural language documents authored by software developers, many researchers rely on "out-of-the-box" NLP libraries. However, software artifacts written in natural language are different from other textual documents due to the technical language used. In this paper, we first analyze the state of the art through a systematic literature review in which we find that only a small minority of papers justify their choice of an NLP library. We then report on a series of experiments in which we applied four state-of-the-art NLP libraries to publicly available software artifacts from three different sources. Our results show low agreement between different libraries (only between 60\% and 71\% of tokens were assigned the same part-of-speech tag by all four libraries) as well as differences in accuracy depending on source: For example, spaCy achieved the best accuracy on Stack Overflow models with nearly 90\% of tokens tagged correctly, while it was clearly outperformed by Google's SyntaxNet when parsing GitHub ReadMe files. Our work implies that researchers should make an informed decision about the particular NLP library they choose and that customizations to libraries might be necessary to achieve good results when analyzing software artifacts written in natural language.}
      \field{booktitle}{2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})}
      \field{eventtitle}{2017 {{IEEE}}/{{ACM}} 14th {{International Conference}} on {{Mining Software Repositories}} ({{MSR}})}
      \field{month}{5}
      \field{shorttitle}{Choosing an {{NLP Library}} for {{Analyzing Software Documentation}}}
      \field{title}{Choosing an {{NLP Library}} for {{Analyzing Software Documentation}}: {{A Systematic Literature Review}} and a {{Series}} of {{Experiments}}}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{187\bibrangedash 197}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/MSR.2017.42
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/JZS5KF6Q/Al Omran und Treude - 2017 - Choosing an NLP Library for Analyzing Software Doc.pdf;/home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/7MHRF4RY/7962368.html
      \endverb
      \keyw{Bibliographies,Libraries,Natural language processing,NLP libraries,Part-of-Speech tagging,Software,Software documentation,Software engineering,Tools}
    \endentry
    \entry{SpacyCoreNews}{online}{}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labeltitlesource}{title}
      \field{abstract}{We’re on a journey to advance and democratize artificial intelligence through open source and open science.}
      \field{title}{Spacy/De\_core\_news\_md · {{Hugging Face}}}
      \field{urlday}{24}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/M2H824V4/de_core_news_md.html
      \endverb
      \verb{urlraw}
      \verb https://huggingface.co/spacy/de_core_news_md
      \endverb
      \verb{url}
      \verb https://huggingface.co/spacy/de_core_news_md
      \endverb
    \endentry
    \entry{gensim}{article}{}
      \name{author}{2}{}{%
        {{hash=d34e74524ffd551d0047dc195b0f95cf}{%
           family={Rehurek},
           familyi={R\bibinitperiod},
           given={Radim},
           giveni={R\bibinitperiod}}}%
        {{hash=6279e9bacedcff3b6605bcd9faaaf41c}{%
           family={Sojka},
           familyi={S\bibinitperiod},
           given={Petr},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{686af22555b8f26efbffee5369e001eb}
      \strng{fullhash}{686af22555b8f26efbffee5369e001eb}
      \strng{bibnamehash}{686af22555b8f26efbffee5369e001eb}
      \strng{authorbibnamehash}{686af22555b8f26efbffee5369e001eb}
      \strng{authornamehash}{686af22555b8f26efbffee5369e001eb}
      \strng{authorfullhash}{686af22555b8f26efbffee5369e001eb}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic}
      \field{number}{2}
      \field{title}{Gensim--python framework for vector space modelling}
      \field{volume}{3}
      \field{year}{2011}
    \endentry
    \entry{schubertDBSCANRevisitedRevisited2017}{article}{}
      \name{author}{5}{}{%
        {{hash=d5aa8a82c7032184011fd502a43e205a}{%
           family={Schubert},
           familyi={S\bibinitperiod},
           given={Erich},
           giveni={E\bibinitperiod}}}%
        {{hash=802157026f850823b2027c2100cb359a}{%
           family={Sander},
           familyi={S\bibinitperiod},
           given={Jörg},
           giveni={J\bibinitperiod}}}%
        {{hash=2c062e64ed26aacc08a62155e7944f04}{%
           family={Ester},
           familyi={E\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=68ec73c25c1ba143d4500012f60e9182}{%
           family={Kriegel},
           familyi={K\bibinitperiod},
           given={Hans\bibnamedelima Peter},
           giveni={H\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=2dda16c0a5d50fc830d0d4a3787937fa}{%
           family={Xu},
           familyi={X\bibinitperiod},
           given={Xiaowei},
           giveni={X\bibinitperiod}}}%
      }
      \strng{namehash}{54a5ae67831c565b0d8652bb8fa618e3}
      \strng{fullhash}{4ca8d17723f44bf8dce996accb92cc3d}
      \strng{bibnamehash}{4ca8d17723f44bf8dce996accb92cc3d}
      \strng{authorbibnamehash}{4ca8d17723f44bf8dce996accb92cc3d}
      \strng{authornamehash}{54a5ae67831c565b0d8652bb8fa618e3}
      \strng{authorfullhash}{4ca8d17723f44bf8dce996accb92cc3d}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{At SIGMOD 2015, an article was presented with the title “DBSCAN Revisited: Mis-Claim, Un-Fixability, and Approximation” that won the conference’s best paper award. In this technical correspondence, we want to point out some inaccuracies in the way DBSCAN was represented, and why the criticism should have been directed at the assumption about the performance of spatial index structures such as R-trees and not at an algorithm that can use such indexes. We will also discuss the relationship of DBSCAN performance and the indexability of the dataset, and discuss some heuristics for choosing appropriate DBSCAN parameters. Some indicators of bad parameters will be proposed to help guide future users of this algorithm in choosing parameters such as to obtain both meaningful results and good performance. In new experiments, we show that the new SIGMOD 2015 methods do not appear to offer practical benefits if the DBSCAN parameters are well chosen and thus they are primarily of theoretical interest. In conclusion, the original DBSCAN algorithm with effective indexes and reasonably chosen parameter values performs competitively compared to the method proposed by Gan and Tao.}
      \field{day}{31}
      \field{issn}{0362-5915}
      \field{journaltitle}{ACM Trans. Database Syst.}
      \field{month}{7}
      \field{number}{3}
      \field{shorttitle}{{{DBSCAN Revisited}}, {{Revisited}}}
      \field{title}{{{DBSCAN Revisited}}, {{Revisited}}: {{Why}} and {{How You Should}} ({{Still}}) {{Use DBSCAN}}}
      \field{urlday}{1}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{42}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{19:1\bibrangedash 19:21}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1145/3068335
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/4FVVJUMK/Schubert et al. - 2017 - DBSCAN Revisited, Revisited Why and How You Shoul.pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1145/3068335
      \endverb
      \verb{url}
      \verb https://doi.org/10.1145/3068335
      \endverb
      \keyw{DBSCAN,density-based clustering,range-search complexity}
    \endentry
    \entry{rousseeuwSilhouettesGraphicalAid1987}{article}{}
      \name{author}{1}{}{%
        {{hash=72d8129c9f4e6afddacb834fb2d8226f}{%
           family={Rousseeuw},
           familyi={R\bibinitperiod},
           given={Peter\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{72d8129c9f4e6afddacb834fb2d8226f}
      \strng{fullhash}{72d8129c9f4e6afddacb834fb2d8226f}
      \strng{bibnamehash}{72d8129c9f4e6afddacb834fb2d8226f}
      \strng{authorbibnamehash}{72d8129c9f4e6afddacb834fb2d8226f}
      \strng{authornamehash}{72d8129c9f4e6afddacb834fb2d8226f}
      \strng{authorfullhash}{72d8129c9f4e6afddacb834fb2d8226f}
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the models configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an ‘appropriate’ number of clusters.}
      \field{day}{1}
      \field{issn}{0377-0427}
      \field{journaltitle}{Journal of Computational and Applied Mathematics}
      \field{langid}{english}
      \field{month}{11}
      \field{shorttitle}{Silhouettes}
      \field{title}{Silhouettes: {{A}} Graphical Aid to the Interpretation and Validation of Cluster Analysis}
      \field{urlday}{26}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{volume}{20}
      \field{year}{1987}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{53\bibrangedash 65}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/0377-0427(87)90125-7
      \endverb
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/VRDBKPYU/Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation.pdf;/home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/ZU2WRBQJ/0377042787901257.html
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/0377042787901257
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/0377042787901257
      \endverb
      \keyw{classification,cluster analysis,clustering validity,Graphical display}
    \endentry
    \entry{SklearnMetricsSilhouettescore}{online}{}
      \list{organization}{1}{%
        {scikit-learn}%
      }
      \field{sortinit}{3}
      \field{sortinithash}{ad6fe7482ffbd7b9f99c9e8b5dccd3d7}
      \field{labeltitlesource}{title}
      \field{abstract}{Examples using sklearn.metrics.silhouette\_score: A demo of K-Means clustering on the handwritten digits models A demo of K-Means clustering on the handwritten digits models Demo of DBSCAN clustering al...}
      \field{langid}{english}
      \field{title}{Sklearn.Metrics.Silhouette-Score}
      \field{urlday}{31}
      \field{urlmonth}{3}
      \field{urlyear}{2023}
      \field{urldateera}{ce}
      \verb{file}
      \verb /home/reisingerjohannes/snap/zotero-snap/common/Zotero/storage/XH9NIT3Z/sklearn.metrics.silhouette_score.html
      \endverb
      \verb{urlraw}
      \verb https://scikit-learn/stable/modules/generated/sklearn.metrics.silhouette_score.html
      \endverb
      \verb{url}
      \verb https://scikit-learn/stable/modules/generated/sklearn.metrics.silhouette_score.html
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

