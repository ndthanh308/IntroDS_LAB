\section{Conclusion}\label{sec:conclusion}
We have shown that different meanings of words can be identified and resolved by clustering context vectors of these words.
Each resulting cluster represents a specific meaning of the word based on specific occurrences in the text corpus.
The main contribution of this paper is an automated approach for DBSCAN parameter selection, based on established,
reviewed, and proven heuristics.
We still see some potential to improve the clustering.
E.g.\ we would like to use an IDF-weighting (inverse document frequency) when computing custer vectors to decrease
the impact of unspecific words.
Furthermore, the work of \citeauthor{huangImprovingWordRepresentations2012}~\cite{huangImprovingWordRepresentations2012}
indicates, that a bigger window size could produce clusters with domain-level, and thus less specific meanings.
However, practical applications require plausible labels for the clusters that can be presented to users.
We think that besides statistical criteria, such as similarity in embedding spaces,
more general linguistic criteria are worth investigating.
Nouns or noun phrases probably make the good labels,
especially noun phrases that contain the original target word seem reasonable choices.
For German, compound words are excellent choices, especially if they contain the original target word,
e.g. \textit{Computermaus} for the target word \textit{Maus}.
The first cluster for the target word \textit{Maus} contains \textit{Burg} as cluster label.
It should be \textit{Burg Maus}, but unfortunately, this entity was not recognized,
as spaCy did not generate a separate word embedding.
We would like to switch to a more elaborate tokenizer, that better identifies named entities and noun phrases, such
as \textit{artificial intelligence}.
If these embeddings also exist for individual tokens and concepts, usually represented as noun phrases,
they could be chosen as cluster labels.

%% Future Work
%\todo{Reuse ideas from thesis.}
%\begin{itemize}
%    \item Generate context vectors with inverse document frequency (idf) to decrease impact of unspecific words
%    \item Using non-static corpus / adding items to the cluster $\to$ Knowledge graph?
%    \item How to proceed with new information? $\to$ Data Structures
%    \item Overlapping clustering methods with outliers, such as \dots
%    \item Automatically finding domain labels (in unsupervised manner? $\to$ LLMs?)
%    \item Decide if a related context is a hierarchical, synonymous or homonymous relationship.
%\end{itemize}