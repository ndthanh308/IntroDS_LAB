\section{Data and Preprocessing}\label{sec:data-and-preprocessing}
This section contains data selection, preprocessing, word embedding training, and context vector generation.

\subsection{Data Selection}\label{subsec:data-selection}
We decided to use the wikimedia dump of all german wikipedia pages, containing
2\,724\,305 documents~\cite{WikimediaDownloads}.
Wikipedia summarizes their known ambiguities of words through a generic url,
allowing us to manually compare our results with this reference.
The utilized fields of the wikipedia dump are \textit{text} and \textit{auxiliary\_text},
disregarding the \textit{title} field, as it carries little information word embeddings training.
The \textit{text} field contains the body of the Wikipedia document, while the \textit{auxiliary\_text} field includes
thumbnail captions, tables, and other things which are not part of the \textit{text} field.
No disambiguation information is used for training as we focus on an unsupervised method.

\subsection{Preprocessing \& Tokenization}\label{subsec:tokenization}
We decided to use spaCy~\cite{spaCy} to tokenize and lemmatize our German text content since the SpaCy tokenization
pipeline achieved the most promising accuracy for tokenization and part-of-speech tagging,
compared to other state-of-the-art NLP libraries like Google's SyntaxNet, Stanford's CoreNLP,
and NLTK~\cite{alomranChoosingNLPLibrary2017}.
We also used the built-in named entity recognition to generate compound tokens, and the medium-sized spaCy core news
model, which was trained on german news~\cite{SpacyCoreNews}.

\subsection{Word Embedding Training}\label{subsec:word-embedding-training}
We used the Word2Vec~\cite{mikolov2013efficient} implementation of the Gensim~\cite{gensim} library to train word
embeddings with the CBOW model.
The word embeddings were created by utilizing the multiprocess functionality.
We used a \textit{window size} of 5, 12 \textit{worker threads}, a \textit{min count} of 10 and trained for
5 \textit{epochs}.
Training took approximately 40 minutes on a workstation with a 12 core Intel(R) Xeon(R) W-2235 CPU, 128 GB RAM,
with no GPUs used.

\subsection{Context Vector Generation}\label{subsec:context-vector-generation}
In a CBOW network a window of words/tokens (e.g 5 tokens to the left and 5 tokens to the right) defines
the context of a target word for each occurrence of this target word in the text corpus.
For each occurrence the embeddings of the context words (IN layer of the network) are summed up to get a context vector.
Table~\ref{tab:word-embeddings} shows the number of context vectors for some test words.
The context vectors for each word were generated in about 145 seconds to 155 seconds on a 12-core Intel i7-12700 CPU.
We manually counted the number of domains and Wikipedia Pages of Homonyms, to then manually evaluate the clustering
results.

\begin{table}[h]
    \centering
    \caption{Selected words with their number of manually counted domains, wikipedia pages of homonyms,
        and context vector information.}
    \label{tab:word-embeddings}
    \begin{tabular}{lrrrr}
        \toprule
        \thead{Word} & \thead{Domains} & \thead{Wikipedia Pages \\ of Homonyms} & \thead{Context Vectors} &
        \thead{Context Vector \\ File Size {[}kB{]}}  \\ \midrule
        Mars & 9 & 86 & 35899 & 14.359 \\
        \makecell[l]{Maus \\ (en. mouse)} & 14 & 51  & 28547  & 11.419    \\
        \makecell[l]{Kleeblatt \\ (en. cloverleaf)} & 6 & 14 & 4192 & 1.676 \\
        Pepsi & 2 & 4 & 2315 & 926 \\
        \makecell[l]{Gabelbein \\ (en. wishbone)} & 2 & 2 & 132 & 53 \\
        \makecell[l]{Datenstruktur \\ (en. data structure)} & 1 & 1 & 2592 & 1.036 \\
        \bottomrule
    \end{tabular}
\end{table}

Based on the assumption that the meaning of a word is defined by its context, we cluster the context
vectors of a target word to identify its meanings.
For each cluster, we can then compute a representative (centroid) embedding vector.