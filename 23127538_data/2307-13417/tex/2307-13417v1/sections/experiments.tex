\section{Clustering}\label{sec:clustering-and-automatic-parameter-selection}
In this section we describe our method to select clustering parameters which is derived with the goal
to create a similar number of clusters as there are meanings in Wikipedia.
We also describe our methods for deriving cluster labels.

\subsection{Clustering the Context Vectors}\label{subsec:clustering-the-context-vectors}
We chose DBSCAN~\cite{DensitybasedAlgorithmDiscovering1996}, a density-based clustering method, to retrieve both clearly
defined and easily interpretable clusters.
Each occurrence of a potentially ambiguous target word should be assigned to a single cluster based on its actual context.
The~\emph{noise cluster}, also generated by DBSCAN, contains those context vectors which are ambiguous in presence of
the target word and hence relate to multiple clusters.
These can not be assigned to a single cluster as their vectors lie in between multiple clusters.

DBSCAN uses two parameters to obtain its results.
Epsilon $\epsilon$ is the radius that defines the equidistant neighborhood of every data point,
while the minimum samples parameter $m$ defines the number of points required within the neighborhood of a given
data point, for it to be labeled \emph{core point}.
Data points with at least 1 neighbouring point, but fewer data points in their neighbourhood than $m$ are labeled
\emph{border point}.
Clusters are then generated from core points and border points.
Outlier points, which do not contain other points within their radius $\epsilon$, are assigned to the noise cluster through
the label ``-1''.

We clustered the context vectors for all combinations of the minimum sample sizes $m = \{ 5,10,20,30, \dots, 100 \}$
and $\epsilon = \{ 0.01, 0.02, \dots, 0.99 \}$.
The lowest minimum sample size $m$ is 5 to detect clusters that were generated from few-context words.
We do not set minPts to 2dim-1 as proposed in~\cite{schubertDBSCANRevisitedRevisited2017},
as we use cosine similarity rather than the Euclidean distance metric.
Following this heuristic would lead to clusterings where the numbers of clusters fails to reach the expected amount.

% Figure environment removed

Figure~\ref{fig:cluster_count} displays a section of the heatmap, visualizing the number of clusters for every
parameter combination for the word `mars'.
The heatmaps of all chosen words matched this heatmap pattern, where the highest number of clusters are in a range
of about $0.05 \leq \epsilon \leq 0.30$ and $5 \leq m \leq 20$, and the highest number of clusters occurring for $m = 5$.

\subsection{Silhouette Score Calculation}\label{subsec:silhouette-score-calculation}
To evaluate cluster quality, we calculated the silhouette score~\cite{rousseeuwSilhouettesGraphicalAid1987}
for every cluster, every word, and every parameter combination, using the respective sklearn
function~\cite{SklearnMetricsSilhouettescore}.
The silhouette score $s(c)$ of a cluster $c$ equals the difference between its mean intra-cluster
distance $a(c)$ and the mean inter-cluster distance $b(c)$ to its nearest cluster, w.r.t.\ the larger distance of
the two.
We use the cosine similarity metric instead of the Euclidean distance metric, analog to the clustering.

\begin{equation}
{s}(c)
    = \frac{b(c) - a(c)}{\max(a(c), b(c))}
    \label{eq:silhouette_score}
\end{equation}

Finally, the overall silhouette score $\overline{s}(C)$, of a clustering $C$ with $n(C)$ clusters,
is calculated as the mean of all silhouette scores ${s}(c)$, $c \in C$.

\subsection{Automatic DBSCAN Parameters Selection}\label{subsec:automatic-dbscan-parameters-selection}
\citeauthor{schubertDBSCANRevisitedRevisited2017} (\cite{schubertDBSCANRevisitedRevisited2017}) revisited common
heuristics, as well as `red flags for degenerate clustering results' used to determine DBSCAN parameters.
Following these heuristics and `red flags', we propose a formula to evaluate the results for a set of DBSCAN parameters.
The parameter score uses four clustering-specific variables: The number of clusters $n(c)$, the average
silhouette score $\overline{s}(c)$, the DBSCAN parameter $\epsilon$, and the noise ratio $noise(c)$.
Every parameter within formula~\ref{eq:dbscan-parameter-score} is normalized to ease interpretation of the
parameter score.

\begin{equation}
    p(c) = 2 \frac{\frac{n(c)}{max(n(c_i))} \cdot 0.5(\overline{s}(c) + 1)}
    {1 + (\frac{\epsilon(c)}{max(\epsilon(c_i))} \cdot \frac{noise(c)}{0.3})}
    \label{eq:dbscan-parameter-score}
\end{equation}

A high average silhouette score represents disjunctive clusters, and is simply shifted to the value range of $[0;1]$
for this equation.
Next, \citeauthor{schubertDBSCANRevisitedRevisited2017} advocate towards a lower epsilon value rather,
to preserve high cluster quality.
Dividing the number of data points in the noise cluster by the total number of data points equals the noise ratio.
As the desirable amount of noise is in the rage of 1\% to 30\%,
the parameter score is set to 0 when the noise score exceeds this range.
This cut is clearly visible in figure~\ref{fig:parameter-score}, where the noise ratio limits the parameter score
from the lower $\epsilon$ range, while the silhouette score limits the parameter score from upper $\epsilon$ range,
which equals 0 if only a single cluster except the noise cluster exists.
Both noise ratio and silhouette score are roughly proportional to the $\epsilon$ parameter,
but the number of clusters increases for lower to middle epsilon values and low minimum sample sizes.

\citeauthor{schubertDBSCANRevisitedRevisited2017} recommend to evaluate the relative size of clusters.
Yet, since we do not expect contexts to occur equally often in presence of the target word,
we do not apply this heuristic.

We chose to normalize all variables as well as the parameter score itself to a range of $[0;1]$ for eased interpretability.
This step is not mandatory when comparing parameters of the same set of vectors where only the clustering parameters
differ, but results in an interpretable score.
The number of clusters is divided by the highest cluster count, the epsilon value is divided by the highest epsilon
value that contains more than one cluster, except the noise cluster, and the noise ratio is divided by the noise
threshold of 0.3.

\subsection{Labeling Clusters}\label{subsec:identifying-contexts}
So far we have described how we generate context vectors and how we cluster them.
Each cluster represents one meaning of the target word.
However, for the intended applications, these clusters have to be presented to the users.
Therefore, we need topic words to label and describe them.
We use two methods to generate cluster labels, and both are based on using the average embedding vector,
i.e.\ context vector, which represent the cluster and thus the context.

In our first approach we search for words with the highest cosine-similarity in the IN-embedding space.
These words should represent typical context words for the cluster meaning.
The second approach focuses on the OUT-embedding space, delivering words the network predicts with high
probability as center word for the cluster representative.
