\section{Related Work}\label{sec:related-work}

\subsection{Word2Vec and Dual Embedding Space}\label{subsec:word2vec}
\citeauthor{mitraDualEmbeddingSpace2016}~\cite{mitraDualEmbeddingSpace2016} focus on the role of the two different
embedding spaces in Word2Vec~\cite{mikolov2013efficient} models.
Word2Vec calculates two different weight matrices with dimension $V \times D$, where $V$ is the vocabulary size and $D$
is the feature (embedding) size.
For the CBOW model, the embeddings in the output space (OUT) are optimized towards a large inner-product with the
mean of their context word vectors in the input space (IN).
After all the goal is to predict the center word based on its context words.
The authors show that the embeddings from these two spaces are dissimilar.
For the IN-to-OUT case, the most similar words to a given target word are words typically occurring in its context,
resulting in thematic similarity.
For example, the resulting top 2 words for the target word ``eminem'' are ``rap'' and ``featuring''.
But for the IN-to-IN and OUT-to-OUT case, the most similar words to a given target word are similar in type and function.
Here, the resulting top 2 words for the target word ``eminem'' are ``rihanna'' and ``ludacris'',
and ``rihanna'' and ``dre'', respectively.
These words are similar, since they occur in similar contexts.
These insights on the dual embedding space help us to reflect our final results.

\subsection{Word Sense Disambiguation}\label{subsec:wsd}
Most work on word sense disambiguation (WSD) is focused on supervised methods.
There is either a training corpus for different meanings of ambiguous words, or an external ressource such as
WordNet~\cite{WordNetElectronicLexical1998} to provide examples of different meanings of ambiguous words.
For the latter, actual contexts of ambiguous words are compared to the contexts of these examples to disambiguate
the word~\cite{orkpholWordSenseDisambiguation2019, nurifanDevelopingCorporaUsing2018}.
\citeauthor{kutuzovLemmatizeNotLemmatize2019}~\cite{kutuzovLemmatizeNotLemmatize2019} work with
\textit{ELMo}~\cite{petersDeepContextualizedWord2018} networks.
As with other embedding architectures, lemmatisation (token normalization) is unnecessary and usually not applied.
However, their experiments show that for morphology-rich languages like Russian, lemmatized training data yields
improvements for WSD.
\citeauthor{alkhatlanWordSenseDisambiguation2018}~\cite{alkhatlanWordSenseDisambiguation2018} utilise
Word2Vec and GloVe to disambiguate polysemous arabic words by using an Arabic version of WordNet,
as well as stemming to normalize the tokens.
Since we work with a German corpus and since German is also a morphology-rich language, we lemmatize the words
before training.

In contrast to most work on WSD our method is unsupervised and does not require an external database or labelled data.
We cluster the context vectors of potentially ambiguous words similar to~\cite{huangImprovingWordRepresentations2012}
and~\cite{schutze-1998-automatic}.
\citeauthor{huangImprovingWordRepresentations2012}~\cite{huangImprovingWordRepresentations2012} use a special network
architecture with both global and local context to produce word embeddings.
\citeauthor{schutze-1998-automatic}~\cite{schutze-1998-automatic} uses sparse vector representations for words.
Both authors make no attempt to automatically identify ambiguous words, but apply context-clustering with a fixed
number of clusters to all words or to known, ambiguous words, respectively.
However, we are investigating whether the results of clustering can be used to decide whether a word is ambiguous
and how ambiguous it is.
