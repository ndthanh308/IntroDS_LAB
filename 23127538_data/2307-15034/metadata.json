{
  "title": "Guaranteed Approximation Bounds for Mixed-Precision Neural Operators",
  "authors": [
    "Renbo Tu",
    "Colin White",
    "Jean Kossaifi",
    "Boris Bonev",
    "Nikola Kovachki",
    "Gennady Pekhimenko",
    "Kamyar Azizzadenesheli",
    "Anima Anandkumar"
  ],
  "submission_date": "2023-07-27T17:42:06+00:00",
  "revised_dates": [
    "2023-08-31T16:37:28+00:00",
    "2024-05-05T04:01:31+00:00"
  ],
  "abstract": "Neural operators, such as Fourier Neural Operators (FNO), form a principled approach for learning solution operators for PDEs and other mappings between function spaces. However, many real-world problems require high-resolution training data, and the training time and limited GPU memory pose big barriers. One solution is to train neural operators in mixed precision to reduce the memory requirement and increase training speed. However, existing mixed-precision training techniques are designed for standard neural networks, and we find that their direct application to FNO leads to numerical overflow and poor memory efficiency. Further, at first glance, it may appear that mixed precision in FNO will lead to drastic accuracy degradation since reducing the precision of the Fourier transform yields poor results in classical numerical solvers. We show that this is not the case; in fact, we prove that reducing the precision in FNO still guarantees a good approximation bound, when done in a targeted manner. Specifically, we build on the intuition that neural operator learning inherently induces an approximation error, arising from discretizing the infinite-dimensional ground-truth input function, implying that training in full precision is not needed. We formalize this intuition by rigorously characterizing the approximation and precision errors of FNO and bounding these errors for general input functions. We prove that the precision error is asymptotically comparable to the approximation error. Based on this, we design a simple method to optimize the memory-intensive half-precision tensor contractions by greedily finding the optimal contraction order. Through extensive experiments on different state-of-the-art neural operators, datasets, and GPUs, we demonstrate that our approach reduces GPU memory usage by up to 50% and improves throughput by 58% with little or no reduction in accuracy.",
  "categories": [
    "cs.LG",
    "math.NA"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.15034",
  "pdf_url": "https://arxiv.org/pdf/2307.15034v3",
  "comment": "ICLR 2024",
  "num_versions": null,
  "size_before_bytes": 6943942,
  "size_after_bytes": 1484375
}