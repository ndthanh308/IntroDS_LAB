\section{Introduction} \label{sec:intro}


Real-world problems in science and engineering often involve solving systems of partial differential equations (PDEs). 
These problems typically require large-scale, high-resolution data. 
For example, meteorologists solve large systems of PDEs every day in order to forecast the weather with reasonable prediction uncertainties \citep{pathak2022fourcastnet,nguyen2023climax,lam2022graphcast}.
Traditional PDE solvers often require hundreds of compute hours to solve real-world problems, such as climate modeling or 3D fluid dynamics simulations \citep{jasak2009openfoam}.
These problems generally require extreme computational resources and high-memory GPUs.

On the other hand, \emph{neural operators} \citep{li2020neural,kovachki2023neural,kossaifi2023multigrid,li2021fourier,li2020multipole}
are a powerful \emph{data-driven} technique for solving PDEs.
Neural operators learn maps between function spaces, and they can be used to approximate the solution operator of a given PDE.
By training on pairs of input and solution functions, the trained neural operator models are orders of magnitude faster than traditional PDE solvers.
In particular, the Fourier neural operator (FNO) \citep{li2021fourier} has been immensely successful in solving PDE-based problems deemed intractable by conventional solvers \citep{liu2022learning,li2021physics,gopakumar2023fourier}.

Despite their success, neural operators, including FNO, are still computation- and memory-intensive when faced with extremely high-resolution and large-scale problems.
For example, when forecasting 2D Navier-Stokes equations in $1024\times 1024$ resolution, a single training datapoint is 45MB.
For standard deep learning models, there is a wealth of knowledge on mixed precision training, in order to reduce training time and memory usage.
However, these techniques are designed for real-valued datatypes and therefore do not directly translate to FNO, whose most expensive operations are complex-valued.
%
Furthermore, learning systems of PDEs often inherently involves learning subtle patterns across a wide frequency spectrum, which can be challenging to learn at half precision, since its range of representation is significantly less than in full precision.
%
On the other hand, the Fourier transform within FNO is already approximated by the discrete Fourier transform (because the training dataset is an approximation of the ground-truth continuous signal); since we already incur approximation error from the discretization, there is no need to run the discrete Fourier transform in high precision.


% Figure environment removed


In this work, we devise a new mixed-precision training routine for FNO which significantly improves runtime and memory usage.
% implementing the FNO block in half precision
We start by profiling the runtime of FNO in full and mixed precision, showing the potential speedups in mixed precision.
However, directly running FNO in mixed precision leads to overflow and underflow errors caused by numerical instability, typically manifesting in the first few epochs.
To address this issue, we study mixed-precision stabilizing techniques, such as using a pre-activation function before the Fourier transform.
We also study the loss caused by aliasing, and we show it can be mitigated by truncating the frequency modes.
Based on our study, we devise a new FNO training routine, which includes a \texttt{tanh} pre-activation and frequency mode truncation.
See \cref{fig:overview} for our full method.
In order to achieve the top accuracy, we also propose a precision schedule training routine, which converts the FNO block from half, to mixed, to full precision throughout training.
We conduct a thorough evaluation of our mixed precision training routines using the Navier-Stokes and Darcy flow equations, resulting in up to a 34\% improvement in runtime and memory with little or no reduction in accuracy.
We release our codebase and all materials needed to reproduce our results in the main neural operator codebase,
\url{https://github.com/neuraloperator/neuraloperator}.\\


\noindent\textbf{Our contributions.}
We summarize our main contributions below:
\begin{itemize}[topsep=2pt, itemsep=2pt, parsep=0pt, leftmargin=5mm]
\item We profile memory and runtime for FNO with full and mixed-precision training, and we conduct a study on the numerical stability of mixed-precision training of FNO, finding that \texttt{tanh} pre-activation is particularly suited to mitigate numerical instability.
\item We show that our final mixed-precision training routine substantially decreases training time and memory usage (up to 34\%), with little or no reduction in accuracy, on the Navier-Stokes  and Darcy flow equations.
Our method, when combined with the recent tensorized FNO \citep{kossaifi2023multigrid}, achieves far better performance while also being significantly faster than the original FNO.
\end{itemize}
