
\section{Background and Related Work} \label{sec:related_work}

\paragraph{Neural Operators.}
Many real-world scientific and engineering problems rely on solving partial differential equations (PDEs).
As such, there is a recent focus on using machine learning-based methods to solve PDEs~\citep{gupta2021multiwavelet,bhatnagar2019prediction,lu2019deeponet,adler2017solving}.
However, most of these methods use standard neural networks and are therefore limited to a mapping between fixed input and output grid. In other words, they operate on a fixed, regular discretization and cannot learn the continuous mapping between function spaces.
\emph{Neural operators} are a new technique that addresses this limitation by directly learning maps between function spaces \citep{li2021physics,li2020neural,li2020multipole,kovachki2023neural}.
The input and output functions to neural operators can be in any resolution or mesh, and the output function can be evaluated at any point in the domain; therefore, neural operators are \emph{discretization convergent}:
once the neural operator is trained, it can be evaluated, without any retraining, at any resolution, and it converges to a unique limit under mesh refinement \citep{kovachki2023neural}.


\textbf{FNO and extensions.} The Fourier neural operator, inspired by spectral methods, is a highly successful neural operator \citep{li2021fourier,gopakumar2023fourier,renn2023forecasting,wen2022accelerating}.
Let $\A:\{a:D_{\A}\rightarrow \R^{d_{\A}}\}$ and $\U:\{u:D_{\U}\rightarrow \R^{d_{\U}}\}$ denote the input and output function spaces, respectively.
In this work, we consider the case where $D_{\A}=D_{\U}\subset \R^d$ for $d\in\N$.
Given a dataset of pairs of initial conditions and solution functions $\{a_j,u_j\}_{j=1}^N$, which are consistent with an operator $\G(a_j)=u_j$ for all $1\leq j\leq N$, the goal is to learn a \emph{neural operator} $\G_\theta$ that approximates $\G$.
The primary operation in FNO is the Fourier convolution operator,
$(\K v_t) (x)=\F^{-1} (R\cdot T_K (\F v_t))(x),$ $\forall x\in D$,
where $\F$ and $\F^{-1}$ denote the Fourier transform and its inverse, $R$ denotes a learnable transformation, $T_K$ denotes a truncation operation, and $v_t$ denotes the function at the current layer of the neural operator.
We use the discrete Fast Fourier Transform (FFT) and its inverse (iFFT) to implement this operator on discrete data.

Building on FNO,~\citet{kossaifi2023multigrid} introduced the Tensorized Fourier Neural Operator (\textbf{TFNO}). Building on tensor methods, which have proven very successful in deep learning~\cite{9420085,PANAGAKIS20241009}, TFNO computes a tensor factorization~\citep{kolda2009tensor} of the weight tensors in the spectral domain, acting as a regularizer that boosts performance while decreasing parameter count.
Additionally, two FNO-based neural operators have recently been proposed to improve performance in non-regular geometries.
The Spherical Fourier Neural Operator (\textbf{SFNO})~\citep{bonev2023sfno} is an extension of FNO to the spherical domain, which is achieved by the use of the spherical convolution theorem \citep{driscoll1994fourier}.
%
The Geometry-Informed Neural Operator (\textbf{GINO}) \citep{li2023gino} is a highly efficient FNO-based architecture for solving PDEs with varying, irregular geometries. 
The architecture consists of an FNO, along with graph neural operators to transform the irregular grid inputs into and from regular latent grids on which FNO can be efficiently applied~\citep{li2020neural}.

% longer versions of SFNO and GINO:
%While most science and engineering problems are formulated in the Cartesian domain, some applications, such as geophysical flows or quantum mechanics, favor a formulation on the sphere, which respects the distinct symmetries inherent to the sphere. \emph{Spherical Fourier Neural Operators} are an extension of Fourier Neural Operators to the spherical domain \citep{bonev2023sfno}, which is achieved by the use of a spherical convolution theorem \citep{driscoll1994fourier}. This formulation significantly improves the performance and parameter efficiency for problems on the sphere with the inherent benefits of FNOs \citep{bonev2023sfno}.
%The geometry-informed neural operator (GINO) \citep{li2023gino} is a highly efficient FNO-based architecture for solving PDEs with varying geometries. The architecture consists of a FNO together with a graph neural operator \citep{li2020neural} to transform the irregular grid inputs into a regular latent grid.

\paragraph{Mixed-Precision Training.}
Mixed-precision training of neural networks consists of reducing runtime and memory usage by representing input tensors and weights (and performing operations) at lower-than-standard precision.
For example, PyTorch has a built-in mixed-precision mode called automatic mixed precision (AMP), which places all operations at \texttt{float16} rather than \texttt{float32}, with the exception of reduction operations, weight updates, normalization operations, and specialized operations \citep{paszke2019pytorch}.
\texttt{bfloat16} and \texttt{TF32} are common mixed-precision methods, yet they do not support discrete Fourier transforms, which are essential to FNOs \citep{kalamkar2019study,dean2012large}.
There is a variety of work targeted at quantization for inference, yet these works do not reduce memory during training \citep{goel2020survey}.

While well-studied for standard neural nets, mixed-precision training has not been studied for FNO \citep{zhao2022lns,de2018high,micikevicius2017mixed,jia2018highly}.
The most similar work to ours is FourCastNet~\citep{pathak2022fourcastnet}, a large-scale climate model that uses mixed precision with Adaptive Fourier Neural Operators \citep{guibas2022adaptive}. However, mixed precision is not applied to the FFT or complex-valued multiplication operations, a key challenge the current work addresses.
Very recently, another work studies the method of quantization for FNO at inference time \citep{dool2023efficient}. However, unlike our work, they only study methods that improve memory usage at inference time, not training time.
%Furthermore, while their metric of interest is the number of multiplicative and addition operations, we report memory usage, wall-clock time, and throughput.
Another paper has appeared recently, which proposes to apply mixed precision to PINNs and DeepONet \citep{hayford2024speeding}.


