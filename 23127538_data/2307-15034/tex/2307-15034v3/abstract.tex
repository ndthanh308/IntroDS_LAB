\begin{abstract}
Neural operators, such as Fourier Neural Operators (FNO), form a principled approach for learning solution operators for partial differential equations (PDE) and other mappings between function spaces. However, many real-world problems require high-resolution training data, and the training time and limited GPU memory pose big barriers. 
One solution is to train neural operators in mixed precision to reduce the memory requirement and increase training speed. However, existing mixed-precision training techniques are designed for standard neural networks, and we find that their direct application to FNO leads to numerical overflow and poor memory efficiency. Further, at first glance, it may appear that mixed precision in FNO will lead to drastic accuracy degradation since reducing the precision of the Fourier transform yields poor results in classical numerical solvers. We show that this is not the case; in fact, we prove that reducing the precision in FNO still guarantees a good approximation bound, when done in a targeted manner. 
Specifically, we build on the intuition that neural operator learning inherently induces an approximation error, arising from discretizing the infinite-dimensional ground-truth input function, implying that training in full precision is not needed.
We formalize this intuition by rigorously characterizing the approximation and precision errors of FNO and bounding these errors for general input functions. We prove that the precision error is asymptotically comparable to the approximation error. Based on this, we design a simple method to optimize the memory-intensive half-precision tensor contractions by greedily finding the optimal contraction order. Through extensive experiments on different state-of-the-art neural operators, datasets, and GPUs, we demonstrate that our approach reduces GPU memory usage by up to 50\% and improves throughput by 58\% with little or no reduction in accuracy.


% inline version of above:
% Neural operators, such as Fourier Neural Operators (FNO), form a principled approach for learning solution operators for partial differential equations (PDE) and other mappings between function spaces. However, many real-world problems require high-resolution training data, and the training time and limited GPU memory pose big barriers. One solution is to train neural operators in mixed precision to reduce the memory requirement and increase training speed. However, existing mixed-precision training techniques are designed for standard neural networks, and we find that their direct application to FNO leads to numerical overflow and poor memory efficiency. Further, at first glance, it may appear that mixed precision in FNO will lead to drastic accuracy degradation since reducing the precision of the Fourier transform yields poor results in classical numerical solvers. We show that this is not the case; in fact, we prove that reducing the precision in FNO still guarantees a good approximation bound, when done in a targeted manner. Specifically, we build on the intuition that neural operator learning inherently induces an approximation error, arising from discretizing the infinite-dimensional ground-truth input function, implying that training in full precision is not needed. We formalize this intuition by rigorously characterizing the approximation and precision errors of FNO and bounding these errors for general input functions. We prove that the precision error is asymptotically comparable to the approximation error. Based on this, we design a simple method to optimize the memory-intensive half-precision tensor contractions by greedily finding the optimal contraction order. Through extensive experiments on different state-of-the-art neural operators, datasets, and GPUs, we demonstrate that our approach reduces GPU memory usage by up to 50% and improves throughput by 58\% with little or no reduction in accuracy.


%This work solves these critical issues and introduces the first mixed-precision neural operator training method.
% The Fourier Neural Operator (FNO) and its extensions are powerful techniques for learning surrogate maps for partial differential equation (PDE) solution operators. Training time and GPU memory usage are significant bottlenecks for many real-world applications, which often require high-resolution data points. While there are mixed-precision training techniques for standard neural networks, those are designed for real-valued operations and perform poorly on the (complex-valued) Fourier domain operations of FNO-based neural operators. 
% % todo: "perform poorly" or "cannot be applied" ....
% % On the other hand, operator learning inherently contains approximation error from discretizing the infinite-dimensional ground-truth input function, so there is no need to train in full precision. 
% Intuitively, operator learning inherently contains approximation error from discretizing the infinite-dimensional ground-truth input function, meaning training in full precision is suboptimal.
% In this work, we formalize this intuition by rigorously characterizing the approximation and precision errors of FNO-based operators, giving tight bounds for general input functions, and showing that the precision error is comparable to the approximation error. Given this motivation, we introduce the first mixed-precision neural operator training routine, which addresses key issues that arise from na\"{i}vely applying existing neural net-based mixed precision tools, notably numerical overflow and lack of complex-valued tensor contraction optimization. Through extensive experiments on different state-of-the-art neural operators, datasets, and hardware, we demonstrate that our approach reduces GPU memory usage by up to 50\% with little or no reduction in accuracy.
\end{abstract}

% % renbo:
% The Fourier Neural Operator (FNO) and its extensions are powerful techniques for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and GPU memory usage are significant bottlenecks. While there exists in practice many mixed-precision training techniques, the precision error for standard neural networks can be arbitrarily high in theory. In contrast, for FNOs, since the discrete Fourier transform requires an approximation  (due to discretization), the precision error could cancel out the approximation error, leading to lossless mixed-precision training. In this work, we prove this intuition both theoretically and empirically, first by giving tight bounds on the approximation and precision errors, and then by showing little-to-none reduction in accuracy from mixed-precision training on three state-of-the-art neural operators across four datasets. 
% Our custom training routine addresses key issues that arise from naively applying the existing mixed-precision tools, most notably numerical overflow and lack of complex-valued tensor contraction support.
% To accommodate high-resolution inputs, we optimize GPU memory usage and demonstrate that our approach substantially reduces usage in different settings (28\% to 50\%).

% % older one:
% The Fourier neural operator (FNO) is a powerful technique for learning surrogate maps for partial differential equation (PDE) solution operators. For many real-world applications, which often require high-resolution data points, training time and memory usage are significant bottlenecks. While there are mixed-precision training techniques for standard neural networks, those work for real-valued datatypes on finite dimensions and therefore cannot be directly applied to FNO, which crucially operates in the (complex-valued) Fourier domain and in function spaces. On the other hand, since the Fourier transform is already an approximation (due to discretization error), we do not need to perform the operation at full precision. In this work, we (i) profile memory and runtime for FNO with full and mixed-precision training, (ii) conduct a study on the numerical stability of mixed-precision training of FNO, and (iii) devise a training routine which substantially decreases training time and memory usage (up to 34%), with little or no reduction in accuracy, on the Navier-Stokes  and Darcy flow equations. Combined with the recently proposed tensorized FNO (Kossaifi et al., 2023), the resulting model has far better performance while also being significantly faster than the original FNO.


