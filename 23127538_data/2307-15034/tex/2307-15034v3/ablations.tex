\section{Ablation Experiments} \label{sec:ablations}
In this section, we introduce additional experiments of our mixed precision procedure on different parametrizations of FNOs, regularization via reducing frequency modes, and training with other numerical systems. 

\paragraph{Decomposition of FNO Weights.}
On the Navier Stokes and Darcy Flow datasets, we adopt a Canonical-Polyadic (\textbf{CP}) factorization of the FNO weights using \cite{kossaifi2019tensorly} to ensure better final error. For a generic problem, FNO weights are usually not factorzied and are saved in their original, \textbf{dense} form. As shown in Figure \ref{fig:factorization_ablation}, we experiment with both scenarios and show that our mixed-precision approach improves runtime without sacrificing accuracy.
% Figure environment removed


\vspace{-5pt}
%\subsection{Frequency Mode Truncation}
\begin{comment}
\paragraph{Frequency Mode Truncation.}
Now we study frequency mode truncation in mixed precision.
When solving systems of PDEs, typically the most important information in the input function is in the low frequency modes. The higher frequencies are typically noisier and must be truncated to prevent overfitting \citep{li2021fourier}.
We show that in half precision, with a lower range of representable numbers, it helps to truncate more frequency modes than in full precision.
%
%Now we validate this phenomenon empirically on the Navier Stokes dataset with FNO.
We run the full-precision FNO and the mixed-precision FNO from the previous section (\algo) across four frequency modes: 128, 64, 32, and 16, and we otherwise keep the same experimental setup as in Section \ref{subsec:stability}; see Figure \ref{fig:mode_ablation}.
Note that the default number of modes for Navier Stokes is 64 \citep{kossaifi2023multigrid}.
% todo: verify and talk about in the context of theory.
Using 16 modes does not yield good results for either model, and we find that the decrease in accuracy when going from 64 to 32 modes is significantly less for mixed-precision FNO, compared to full-precision FNO.
In Appendix \ref{app:experiments}, we also run an experiment on synthetic data to demonstrate that the error caused by half precision is higher for higher frequencies, relative to the amplitude.
\end{comment}

\paragraph{Number of Frequency Modes.}
Recall that in the FNO architecture, after the FFT, we crucially truncate to a fixed number of frequency modes to improve generalization, typically $\nicefrac{1}{3}$ to $\nicefrac{2}{3}$.
Now we run an ablation study on the number of frequency modes used in the FNO architecture.
We run frequency modes $\{16, 32, 64, 128\}$ on the $128\times 128$ Darcy flow dataset in full and half precision; see \cref{fig:mode_ablation}.
We find that using two few frequency modes hurts accuracy substantially, while using too many frequency modes increases runtime substantially. 
There is not a significant difference between half precision and full precision, for all frequencies.

 \begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-15pt}
\begin{center}
% Figure removed
\caption{
\textbf{Comparison of the full-precision and mixed-precision (\algo) FNO} 
 with different frequency modes, on the Darcy flow dataset.
 For 16 frequency modes, the half precision error (compared to full precision) is higher than for 32, 64, or 128 modes.
}
\label{fig:mode_ablation}
\end{center}
\vspace{-5pt}
\end{wrapfigure}
\renbo{maybe this can go to the end of the theory section, as a simple proof of intuition}
Next, we run an experiment on synthetic data to demonstrate that the error caused by half precision is higher for higher frequencies, relative to the amplitude.
We create a signal based on sine and cosine waves with frequencies from 1 to 10, with randomly drawn, exponentially decaying amplitudes.
Then we plot the Fourier spectrum in full and half precision, as well as the absolute error of the half-precision spectrum as a percentage of the true amplitudes. 
See \cref{fig:synthetic};
we find that the percentage error exponentially increases.
Since in real-world data, the energy is concentrated in the lowest frequency modes, and the higher frequency modes are truncated, this gives further justification for our half precision method.

\paragraph{Other Mixed Precision Options.}
Among other numerical systems, we experiment with Brain Float16 (BF16) and TensorFloat32 (TF32). However,  PyTorch does not support BF16 for discrete fourier transforms, which are essential to FNOs. Even when applied to the rest of the network, BF16 suffers from error degradation on Navier Stokes, possibly due to having less precision bits compared to FP16. On the other hand, TF32 is not as efficient as our approach, even on optimized hardware such as the A100 GPU. See Appendix Table for more details on TF32 timings compared to our mixed-precision approach. 




