\section{Empirical Study of our Mixed-Precision Neural Operator} \label{sec:experiments}\vspace{-5pt}

% Motivated by the theoretical analysis from the previous section, now we present the first mixed-precision training routine for neural operators. 
% We start by presenting our mixed-precision FNO block pipeline, showing significant memory usage reduction and increase in training throughput on different hardware.
% %
% We then show failure modes of na\"{i}vely applying mixed precision and present our empirically-justified solution of pre-activation.
% Finally, we present extensive experiments for our end-to-end training routine across different architectures and datasets.
We empirically study our proposed mixed-precision pipeline for FNO training and demonstrate significant memory usage reduction and large increases in training throughput on various GPUs.
%
We then study failure modes of a na\"{i}ve application of mixed precision and present our empirically justified pre-activation-based stabilizer solution.
Finally, we perform extensive experiments and ablations on our end-to-end training method across different architectures and datasets.

\subsection{Datasets and Experimental Setup} \label{subsec:setup}\vspace{-5pt}




We summarize each of the four datasets we use; see \cref{app:datasets} for detailed descriptions. 

\textbf{Navier-Stokes.} We consider the Navier-Stokes equations for a viscous, incompressible fluid in vorticity form on the unit torus. We use the same dataset as~\citet{kossaifi2023multigrid}, with a Reynolds number of $500$, composed of 10\,000 training samples and 2000 test samples with resolution $128\times 128$.
\textbf{Darcy Flow.}
We consider the steady-state 2D Darcy Flow equation, which models fluid flow through a porous medium. We use the same dataset as~\citet{li2021fourier}, with 5000 training samples and 1000 test samples at resolution $128\times 128$.
\textbf{Spherical Shallow Water Equations.} We use the dataset from~\citet{bonev2023sfno}, which generates random initial conditions on the sphere at resolution $256\times 512$. At each epoch, 120 training samples and 20 validation samples are generated on the fly.
\textbf{Shape-Net Car and Ahmed-body.}
Our final two datasets are 3D real-world car dataset generated by prior work \citep{umetani2018learning,li2023gino}, which consists of mesh points that represent a unique 3D car, and the goal is to predict the full 3D pressure field.
We use 611 water-tight shapes from car surfaces from Shape-Net \citep{chang2015shapenet}, with 500 samples for training and the rest for the test set. For Ahmed-body, we have 500 for training and 51 for test. 
The spatial resolution for both is $64 \times 64 \times 64$.

\begin{comment} % original description that only has Shape-net car
\textbf{Shape-Net Car.}
Our final dataset is a 3D real-world car dataset generated by prior work \citep{umetani2018learning,li2023gino} using the Reynolds Averaged Navier Stokes (RANS) equations \citep{wilcox1998turbulence}.
Each data point consists of mesh points that represent a unique 3D car, and the goal is to predict the full 3D pressure field, given an inlet velocity of 20m/s.
We use 611 water-tight shapes from car surfaces from Shape-Net \citep{chang2015shapenet}, with 500 samples for training and the rest for the test set. 
The spatial resolution is $64 \times 64 \times 64$.
\end{comment}


\textbf{Experimental Setup.} 
We run the Navier Stokes and Darcy flow experiments on the TFNO architecture \citep{kossaifi2023multigrid}. For the Spherical SWE, we use SFNO \citep{bonev2023sfno} to handle the spherical geometry, and for Shape-Net Car and Ahmed-body, we use GINO \citep{li2023gino} to handle the large-scale, irregular geometry. All models have the FNO as backbone. 
We use the official implementation and default hyperparameters for all models.
%, unless otherwise noted. We release our codebase and all materials needed to reproduce our results at \url{https://anonymous.4open.science/r/MP-FNO-B801}. 
%
%
\subsection{Mixed-Precision Module for Complex-Valued Tensor Contraction} \label{subsec:memory}\vspace{-1pt}

\begin{wrapfigure}{r}{0.6\textwidth}
%\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-15pt}
\begin{center}
%% Figure removed
%% Figure removed
% Figure removed
\caption{
\textbf{Overview of our Mixed-FNO pipeline.}
}
\label{fig:overview}
\end{center}
\vspace{-10pt}
\end{wrapfigure}


Now we introduce our theoretically and empirically motivated mixed-precision pipeline for the FNO block.
We start by profiling FNO training workloads, identifying the complex-valued tensor contraction within the FNO block as the computational bottleneck, accounting for 4 out of the 5 most time-consuming GPU kernels in the entire training pipeline (see \cref{app:profiling} for the full profiling results).
Furthermore, existing mixed-precision tools such as Pytorch's Automatic Mixed Precision (AMP) \citep{paszke2019pytorch} leave the operator in full precision since it only autocasts real-valued modules in FNO. 
%there are no available CUDA library implementations for complex-valued half-precision tensor contraction. 




% todo: polish this part
We introduce a simple and lightweight workaround: we break down each tensor contraction into sub-expressions consisting of at most two terms and perform each of these contractions by temporarily converting tensors to reals. To optimize the memory usage, we use a simple greedy algorithm to select the next \texttt{einsum} step that minimizes the intermediate tensor size (see \cref{fig:overview}). The complex-valued operations, including the forward FFT, tensor contraction, and inverse FFT, are done in half-precision. This completes the half-precision FNO block as AMP manages non-FNO, real-valued operations. 
%Note that the greedy algorithm is only performed once at the beginning of training because the \texttt{einsum} path can be stored for future use.

Our simple half-precision FNO block module achieves up to 38.6\% reduction in memory and up to 50\% reduction in memory when combined with Pytorch's native AMP; see \cref{fig:memory}.
Note that the reduction from AMP + Half-Prec FNO is greater than the sum of its parts due to lack of the additional casting to and from half-precision.
%Our memory-efficient implementation synergizes with PyTorch's native AMP, since we always observe a more-than-linear reduction in memory usage by combining half-precision FNO with AMP. 
The memory usage reduction can then be used to train on a larger batch size, thereby improving the overall training throughput and efficiency. On three different Nvidia GPUs, RTX 3090 Ti, V100, and RTX A6000, we demonstrate a consistent improvement in training throughput, in terms of numbers of training samples processed per second, from 1.23X to 1.58X over the baseline in \cref{fig:efficiency}.

% Figure environment removed

% Figure environment removed

\subsection{Numerical Stability via Pre-Activation} \label{subsec:stability}\vspace{-5pt}
Mixed-precision training is prone to underflow and overflow because its dynamic range is significantly smaller than that of full precision \citep{rakka2022mixed}. 
% todo: any better references? I just picked one
Notably, for all four of the datasets in our study, na\"{i}vely running FNO in mixed precision results in training failure due to \texttt{NaN} outputs. 
Furthermore, we empirically show that many common solutions, including loss scaling, gradient clipping, normalization, and delaying updates, all fail to address the numerical instability of mixed-precision FNO (see \cref{app:stability}).
To mitigate this overflow issue, we find that pre-activation before each forward FFT is a very effective method for overcoming numerical instability. We also find that the \texttt{tanh} pre-activation is the highest-performing operation that we considered according to \cref{tab:preactivation}. 
%\texttt{Hard-clip} to $[-1, 1]$ and \texttt{2$\sigma$-clip} to $[mean-2\sigma, mean+2\sigma]$ are two other viable options for range truncation, but they are less efficient than \texttt{tanh}. 
Unlike other functions, \texttt{tanh} minimizes changes to small inputs, as it is approximately the identity function near 0 and is smoothly differentiable. Furthermore, \texttt{tanh} preserves the discretization-convergent property of FNO. 
The strong performance of \texttt{tanh} is also predicted by our theoretical results in \cref{sec:theory}, since it decreases the $L_\infty$ norm and the Lipschitz constant of the input function.
%orce all values within the range [âˆ’1, 1] while being robust to outliers: the shape of tanh maintains the trends in the values near the mean
%As an indispensable module for training FNOs in mixed precision, 
In \cref{app:stability}, we further show that the \texttt{tanh} pre-activation minimally alters the frequency-domain signal in both amplitude and phase. Finally, we demonstrate through an ablation that \texttt{tanh} has negligible impact on the model's final error (See Appendix \cref{tab:full-prec-tanh}).
%The clipping techniques that we have experimented with can tackle numerical overflow but do not possess the key resolution-invariant property \renbo{is this right}. \renbo{I also have saved checkpoints before and after NaNs appear in naive half precision training, which could become an interesting study.}



\subsection{Error Comparison with Full-Precision}\label{subsec:accuracy}\vspace{-5pt}
Having resolved the critical issue of numerical stability, we demonstrate that our mixed-precision approach achieves errors within 1\% of the full-precision baseline across the four datasets. Additionally, we propose a precision-scheduling technique that transitions from mixed to full precision during training, which performs better than full precision in zero-shot super-resolution inference. 

% Figure environment removed

\textbf{Mixed- vs. Full-Precision Training Curves.}
\cref{fig:full_results} illustrates that our mixed-precision approach achieves test errors on par with full precision throughout the training process, remaining within 1\% of the full-precision baseline. We ensure apples-to-apples comparison by keeping all hyperparameters constant across the two precision settings. These hyperparameter configurations are originally optimized for full precision training, so we show that they are directly transferable to mixed precision. 

%We run experiments on both Navier Stokes and Darcy flow data, with and without Canonical-Polyadic factorization \citep{kolda2009tensor} of the weight matrices (which was recently shown to substantially improve performance and memory usage of FNO \citep{kossaifi2023multigrid}).

%\paragraph{Better Generalization with Precision Scheduling.} 
%We also observe improved final error from adopting mixed precision for a fraction of training. We propose a precision schedule, in which the first 25\% of training is in \algo{}, the middle 50\% of training puts the FFT in full precision, and the final 25\% of training is in full precision. The results are reported in Table \renbo{placeholder}.

\textbf{Precision Scheduling and Zero-Shot Inference.}
An important property of neural operators is their \emph{discretization convergence}, meaning that they can be trained on one resolution and tested on a higher resolution (zero-shot super-resolution) \citep{kovachki2023neural}. To achieve the best result, we propose a precision schedule, in which the first 25\% of training is in mixed-precision, the middle 50\% applying only AMP, and the final 25\% in full precision.
This follows a simple intuition: in the early stages of training, it is okay for gradient updates to be coarser, since the gradient updates are larger overall. However, in the later stages of training, the average gradient updates are much smaller, so full precision is more important. 
%In order to show that our mixed-precision training pipeline maintains discretization convergence, we show zero-shot super-resolution results. 
We use the Navier-Stokes dataset, trained on $128\times 128$ resolution (the same setting and model as \cref{fig:full_results}), and tested on $256\times 256$, $512\times 512$, and $1024\times 1024$ resolutions; see \cref{tab:super-res}.
We find that %similarly to $128\times 128$ test data, 
half-precision has a small decrease in accuracy compared to full precision, and using a precision schedule achieves significantly better generalization.

\begin{table}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{lrr rr rr rr}
\toprule
{} & \multicolumn{2}{c}{128x128} & \multicolumn{2}{c}{256x256} & \multicolumn{2}{c}{512x512} & \multicolumn{2}{c}{1024x1024} \\
\cmidrule{2-9}
{} & $H^1$ & $L^2$ & $H^1$ & $L^2$ & $H^1$ & $L^2$ & $H^1$ & $L^2$ \\
\midrule
Full FNO & 0.00557 & 0.00213 & 0.00597 & 0.00213 & 0.00610 & 0.00213 & 0.00616 & 0.00213 \\
Mixed FNO (Ours) & 0.00624 & 0.00236 & 0.00672 & 0.00228 & 0.00688 & 0.00226 & 0.00693 & 0.00226 \\
Precision schedule (Ours) & \textbf{0.00503} & \textbf{0.00170} & \textbf{0.00542} & \textbf{0.00170} & \textbf{0.00555} & \textbf{0.00170} & \textbf{0.00558} & \textbf{0.00170} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Zero-shot super resolution}. 
We test zero-shot super-resolution by training each model on $128\times 128$ resolution for 19 hours.
Mixed precision has a small decrease in accuracy compared to full precision, and using a precision schedule achieves significantly better accuracy. % with the same training time.
\label{tab:super-res}
}
\vspace{-3mm}
\end{table}


%\paragraph{Training on High-Resolution PDEs with Mixed Precision.}
%Under the same hardware constraints, mixed precision enables training on higher-resolution data and achieves better results, while training in full precision leads to out-of-memory errors. See Table \renbo{placeholder} for experiments on the 2D Shallow Water Equations and the 3D Shape-Net Car datasets. 
%\renbo{the idea is that we train and evaluate on high resolution, showing the accuracy is better than training on lower resolution.}



\subsection{Comparison against U-Nets}\vspace{-5pt}
Despite being orignally designed for computer vision tasks, U-Nets have recently been used as PDE surrogates \citep{unet}.
%A U-Net learns both global and local spatial information through its down-sampling and up-sampling layers, passing information with skip connections. 
Here, we compare FNOs against the U-Net baseline on the Darcy Flow and Navier-Stokes datasets. As shown in \cref{tab:unet}, FNO outperforms UNet: our mixed-precision approach yields higher memory reduction compared to AMP applied to U-Nets. 
\begin{table}
\centering
\resizebox{0.8\textwidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
    \multirow{ 2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{Navier-Stokes}} & \multicolumn{2}{c}{\textbf{Darcy Flow}} \\
    \cmidrule{2-3} \cmidrule{4-5}
    & \textbf{Error} & \textbf{Memory Reduction} & \textbf{Error} & \textbf{Memory Reduction} \\
    \midrule
    Full FNO& \textbf{0.003} & \multirow{2}{*}{50.4\%} & 0.01 & \multirow{2}{*}{25.8\%} \\
    Mixed FNO (Ours) & 0.004 &  & \textbf{0.007} &  \\
    \midrule
    Full U-Net& 0.111 & \multirow{2}{*}{20.9\%} & 0.024 & \multirow{2}{*}{24.9\%} \\
    U-Net + AMP & 0.111 &  & 0.022 &  \\
    \bottomrule
    \end{tabular}
    }
    \caption{\textbf{Comparison of $L^2$ error and memory reduction with U-Nets}. 
    FNO consistently shows better final error, and 
    our mixed-precision approach yields significantly more memory reduction on Navier-Stokes than AMP applied to U-Nets. 
    \label{tab:unet}
    }
\vspace{-2mm}
\end{table}


\subsection{Ablation studies}\label{subsec:ablations}\vspace{-5pt}
Here, we perform ablations of our mixed-precision procedure on different parameterizations of FNOs, regularization via reducing frequency modes, and training with other numerical systems.

\textbf{Decomposition of FNO Weights.}
On the Navier-Stokes and Darcy Flow datasets, we adopt a Canonical-Polyadic (\textbf{CP}) factorization of the FNO weights using \cite{kossaifi2019tensorly} to ensure better final error. For a generic problem, FNO weights are usually not factorzied and are saved in their original, \textbf{dense} form. As shown in \cref{fig:factorization_ablation}, we experiment with both scenarios and show that our mixed-precision approach improves runtime without sacrificing accuracy.
% Figure environment removed



%\vspace{-5pt}
%\subsection{Frequency Mode Truncation}
\begin{comment}
\paragraph{Frequency Mode Truncation.}
Now we study frequency mode truncation in mixed precision.
When solving systems of PDEs, typically the most important information in the input function is in the low frequency modes. The higher frequencies are typically noisier and must be truncated to prevent overfitting \citep{li2021fourier}.
We show that in half precision, with a lower range of representable numbers, it helps to truncate more frequency modes than in full precision.
%
%Now we validate this phenomenon empirically on the Navier Stokes dataset with FNO.
We run the full-precision FNO and the mixed-precision FNO from the previous section (\algo) across four frequency modes: 128, 64, 32, and 16, and we otherwise keep the same experimental setup as in \cref{subsec:stability}; see \cref{fig:mode_ablation}.
Note that the default number of modes for Navier Stokes is 64 \citep{kossaifi2023multigrid}.
% todo: verify and talk about in the context of theory.
Using 16 modes does not yield good results for either model, and we find that the decrease in accuracy when going from 64 to 32 modes is significantly less for mixed-precision FNO, compared to full-precision FNO.
In \cref{app:experiments}, we also run an experiment on synthetic data to demonstrate that the error caused by half precision is higher for higher frequencies, relative to the amplitude.
\end{comment}



\textbf{Number of Frequency Modes.}
Recall that in the FNO architecture, after the FFT, we truncate to a fixed fraction of frequency modes to improve generalization, typically $\nicefrac{1}{3}$ to $\nicefrac{2}{3}$.
We run an ablation study on the number of frequency modes used in the FNO architecture.
%
We run frequency modes $\{16, 32, 64, 128\}$ on the Darcy flow dataset in full and half precision; see~\cref{fig:mode_ablation}.
We find that using two few frequency modes hurts accuracy substantially while using too many frequency modes increases runtime substantially. 
There is not a significant difference between half-precision and full precision for all frequencies. In addition, we show in \cref{appsubsec:frequency} that for synthetic data, the precision error from half precision is higher for higher frequencies relative to the amplitude.


\textbf{Other Mixed Precision Options.}
We also experimented with BrainFloat16 (BF16) and TensorFloat32 (TF32). However,  PyTorch does not support BF16 for discrete Fourier transforms, which are essential to FNOs. Even when applied to the rest of the network, BF16 suffers from error degradation on Navier Stokes, possibly due to having fewer precision bits than FP16. On the other hand, TF32 is not as efficient as our approach, even on optimized hardware such as the A100 GPU. Moreover, we simulated FP8 training via clipping.
See \cref{appsubsec:othernumeric} for additional details.