
\vspace{-3mm}
\section{Conclusions and Future Work} \label{sec:conclusion}
\vspace{-2mm}
In this work, we introduced the first mixed-precision training method for neural operators. We derived approximation bounds for mixed-precision neural operator 
and rigorously characterized the approximation and precision errors of FNO-based operators, proving that the precision error is comparable to the approximation error. 
Using this insight, we show, in practice, a significant reduction of memory usage and improvement in throughput, without sacrificing accuracy, solving critical issues from standard neural net-based training methods.
% We first rigorously characterize the approximation and precision errors of FNO-based operators, proving that the precision error is comparable to the approximation error. 
% With this theoretical motivation, we devise a new mixed-precision neural operator training method that solves critical issues from standard neural net-based training methods.
Through extensive experiments on different state-of-the-art neural operators, datasets, and hardware, we demonstrate that our approach reduces GPU memory usage by up to 50\% with little or no reduction in accuracy.

Overall, half-precision FNO makes it possible to train on significantly larger data points with the same batch size. 
Going forward, we plan to apply this to real-world applications that require super-resolution to enable larger-scale training.
%Furthermore, giving theoretical and empirical results for mixed-precision physics-informed neural operators is an exciting future direction.




% However, existing mixed-precision training techniques were developed for standard, real-valued neural networks. 
% As a result, a direct application of these techniques to complex-valued neural operators, which operate in the spectral domain, leads to poor memory efficiency and numerical overflow.
% This work solves these critical issues and introduces the first mixed-precision neural operator training method.
% Specifically, we build on the intuition that neural operator learning inherently induces approximation error from discretizing the infinite-dimensional ground-truth input function, implying that training in full precision is suboptimal.
% We formalize this intuition by rigorously characterizing the approximation and precision errors of FNO-based operators, bounding these errors for general input functions. We mathematically prove that the precision error is comparable to the approximation error. 
% Through extensive experiments on different state-of-the-art neural operators, datasets, and hardware, we demonstrate that our approach reduces GPU memory usage by up to 50\% with little or no reduction in accuracy.


% On the other hand, operator learning inherently contains approximation error from discretizing the infinite-dimensional ground-truth input function, so there is no need to train in full precision. In this work, we make this intuition explicit by fully characterizing the approximation and precision errors of FNO-based operators, giving tight bounds for general input functions, and showing that the precision error is comparable to the approximation error. Given this motivation, we introduce the first mixed-precision neural operator training routine, which addresses key issues that arise from na\"{i}vely applying existing neural net-based mixed precision tools, notably numerical overflow and lack of complex-valued tensor contraction optimization. Through extensive experiments on different state-of-the-art neural operators, datasets, and hardware, we demonstrate that our approach reduces GPU memory usage by up to 50\% with little or no reduction in accuracy.
