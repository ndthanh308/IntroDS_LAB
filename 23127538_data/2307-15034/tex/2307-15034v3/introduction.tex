\section{Introduction} \label{sec:intro}

Real-world problems in science and engineering often involve solving systems of partial differential equations (PDEs) \citep{strang2007computational}. 
These problems typically require a fine grid for numerical solvers to guarantee convergence, e.g.,  climate modeling and 3D fluid dynamics simulations, and the full-scale solutions to these real-world systems are out of reach even for the world's largest supercomputers \citep{schneider2017climate}.
%hersbach2020era5,jasak2009openfoam,
%weather~\citep{pathak2022fourcastnet,nguyen2023climax,lam2022graphcast}.Es every day to forecast
\begin{comment}
%%% Backup of the first plot: the bar plot

\begin{wrapfigure}{r}{0.37\textwidth}
\vspace{-15pt}
\begin{center}
% Figure removed
\caption{
\textbf{GPU Memory Usage Reduction for neural operators using our mixed-precision procedure.} 
}
\label{fig:memory_summary}
\end{center}
\vspace{-20pt}
\end{wrapfigure}

\end{comment}


To overcome the computational challenges of numerical solvers, fast surrogates using machine learning have been developed. Among them, \emph{neural operators}
are a powerful \emph{data-driven} technique for solving PDEs \citep{li2020neural,li2021fourier,kovachki2023neural,lu2021learning}.
Neural operators learn maps between function spaces, and they can be used to approximate the solution operator of a given PDE.
%
The input and output functions in neural operators can be at any resolution or on any mesh, and the output function can be evaluated at any point in the domain; therefore, neural operators are \emph{discretization convergent}:
once the neural operator is trained, it can be evaluated, without any retraining, at any resolution, and it converges to a unique limit under mesh refinement \citep{kovachki2023neural}.
By learning from discretized data from input and solution functions, the trained models can perform inference orders of magnitude faster than traditional PDE solvers \citep{kovachki2023neural}.
In particular, the Fourier Neural Operator (FNO) and its extensions have been successful in solving PDE-based problems with significant speedups \citep{li2021fourier,li2023gino,bonev2023sfno,kossaifi2023multigrid,liu2022learning,gopakumar2023fourier}.

Despite their successes,  training of neural operators is still compute- and memory-intensive when faced with extremely high-resolution and large-scale problems.
For conventional deep learning models, there is a wealth of knowledge on automatic mixed precision (AMP) training in order to reduce memory usage and increase computational throughput~\citep{rakka2022mixed}.
We find that their direct application to neural operators results in poor memory efficiency and numerical overflow; when applied to FNO,  most memory-intensive operations are in the spectral domain, which is complex-valued and not handled well by the standard AMP, as seen in \cref{fig:ball_figure}.
%However, these techniques are designed and optimized for real-valued operations and do not exploit the complex-valued processing present in the Fourier transform in FNO. In our experiments, directly applying AMP leads to poor memory efficiency and numerical overflow when applied to FNO, as seen in \cref{fig:ball_figure}.

%However, these techniques are designed and optimized for standard neural nets and lead to poor memory efficiency and numerical overflow when applied to FNO, whose bottleneck is the discrete Fourier transform and tensor contraction in the spectral domain. In our experiments, directly applying AMP leads to poor performance, as seen in \cref{fig:ball_figure}.

Conventional wisdom may suggest that reducing precision in  FNO without drastic accuracy degradation is not feasible. The presence of the Fourier transform in FNO appears to be a limitation in reducing precision since it suffers from numerical instabilities on high-frequency modes under reduced precision. Hence, numerical solvers, such as pseudo-spectral solvers, do indeed require very high precision to ensure numerical stability~\citep{trefethen2000spectral}, and time-stepping leads to an accumulation of round-off errors under low precision that quickly explode. 
%However, we show that this is not the case with FNO and other operator learning approaches and provide a theoretical justification as well. 
However, we show both theoretically and empirically that this is not the case for FNO and other operator learning approaches. 

%FNO perceives a theoretically founded trade-off between approximation and precision by distributing the precision error across many layers of deep learning models.
%cannot be applied to FNO, whose most expensive operations are complex-valued.
%



% Figure environment removed

\begin{comment}
% Figure environment removed
\end{comment}

\begin{comment}
% Figure environment removed
\end{comment}


{\bf Our approach: }In this work, we devise the first mixed-precision training method for neural operators and also derive approximation bounds that guarantee the expressivity of mixed-precision operators. We use the intuition that the Fourier transform within neural operators is already approximated by the discrete Fourier transform, since the training dataset is a discrete approximation of the ground-truth continuous signal. Intuitively, since we already incur approximation error from discretization, there is no need to run the discrete Fourier transform in full precision.
Building on this,  we show that, in a single FNO layer, the round-off error due to lower precision is small. Therefore, the overall round-off error remains small since a full FNO architecture is only made up of a few layers---much smaller than the number of steps in a classical pseudo-spectral solver where errors accumulate and explode.

We make the above intuitions concrete, and we develop the following theoretical approximation bounds: we characterize the precision error and resolution error of the FNO block, proving asymptotic bounds of $n^{-\nicefrac{2}{d}}$ and $\epsilon$, respectively, for mesh size $n$, dimension $d$, and dynamic range $\epsilon$. Therefore, we justify using mixed precision since its error is comparable to the discretization error already present in FNO.



Motivated by these theoretical results, we introduce a mixed-precision training method by optimizing the memory-intensive half-precision tensor contractions. We devise a simple and lightweight greedy strategy to find the optimal contraction order, which considers vectorization for each intermediate tensor.
Unfortunately, na\"{i}vely training neural operators in mixed precision leads to prohibitive numerical instability due to overflows in the FNO block.
To address this issue, we study numerical stabilization techniques for neural operators, finding that \texttt{tanh} pre-activation before each FFT consistently avoids numerical instability.
Further, we incorporate additional vectorization steps for complex-valued inputs, not present in standard packages designed for real-valued networks. 
%In order to mitigate numerical instability, we conduct a study on numerical stabilization techniques for neural operators, showing that using \texttt{tanh} pre-activations results in the best performance.
%See \cref{fig:overview} for a full overview of our method.


We carry out extensive experiments to show a significant reduction in memory usage and improved throughput without sacrificing accuracy; see \cref{fig:ball_figure}. We consider three variants of FNO: tensorized (TFNO) \citep{kossaifi2023multigrid}, spherical (SFNO) \citep{bonev2023sfno}, and geometry-informed (GINO) \citep{li2023gino}.
Across different datasets and GPUs, our method results in up to 58\% improvement in training throughput and 50\% reduction in GPU memory usage with little or no reduction in accuracy.

Furthermore, we show that our method is discretization convergent via zero-shot super-resolution experiments. 
% colin: this sentence is not the most important, we could drop if needed:
Finally, we propose a precision schedule training routine, which transitions from mixed to full precision during training; we show this method achieves better than the baseline full-precision accuracy.
Our mixed-precision training routine is lightweight and easily added to new neural operators. 
We release our codebase and all materials needed to reproduce our results at \url{https://github.com/neuraloperator/neuraloperator}. %\url{https://anonymous.4open.science/r/MP-FNO-B801}.



\noindent\textbf{We summarize our main contributions below:}
\begin{itemize}[topsep=1pt, itemsep=2pt, parsep=0pt, leftmargin=5mm]
\item We introduce the first mixed-precision training routine for neural operators, which optimizes the memory-intensive tensor contraction operations in the spectral domain, and we propose using \texttt{tanh} pre-activations to minimize numerical instability.

%
\item  We theoretically ground our work by characterizing the precision and discretization errors of the FNO block, showing that these errors are comparable, proving that, done right, mixed-precision training of neural operators leads to little or no performance degradation.
% \item  We provide theoretical justification that motivates mixed-precision training for neural operators by proving tight bounds on the precision and resolution errors of the FNO block, showing that the errors are comparable.
%
\item  We empirically verify the superiority of our mixed-precision training approach on three state-of-the-art neural operators, TFNO, GINO, and SFNO, across four different datasets and GPUs.
Our method uses \textbf{half the memory} and \textbf{increases training throughput up to 58\%} across different GPUs with little or no reduction in accuracy ($<0.1\%$).

% Our method substantially decreases memory usage (up to 50\%) and increases training throughput (up to 58 \%) across different GPUs with little or no reduction in accuracy.
\item We provide an efficient implementation of our approach in PyTorch, which we open-source, along with all data needed to reproduce our results.
\end{itemize}


% todo: mention that this has never been studied, since numerical sovlers operate at high precision?