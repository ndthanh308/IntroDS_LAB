

\section{Additional Details from \cref{sec:experiments}} \label{app:experiments}

In this section, we give additional details on the datasets and experimental setup, as well as additional experiments.

\subsection{Ahmed-Body CFD Experiment} \label{app:ahmed}
Similar to Shape-Net Car, we train the Geometry-Informed Neural Operator on the Ahmed-body dataset, using the same hyperparameters as the original implementation \citep{li2023gino}.
The memory, error, and throughput results for Ahmed-body are appended to \cref{fig:ball_figure}. We add the L2 error curves here following the format of \cref{fig:full_results}.

% Figure environment removed



\subsection{Dataset Details} \label{app:datasets}
\paragraph{Navier-Stokes.}
We consider the 2D Navier-Stokes equation for a viscous, incompressible fluid in vorticity form on the unit torus  $\T^2\cong [0,2\pi)^2$, 
\begin{align}
    \partial_t\omega + \nabla^{\bot}\phi\cdot\omega = \frac{1}{\text{Re}}\Delta\omega + f,\quad&\text{for }x\in \T^2,~t\in (0,T]\nonumber\\
    -\Delta\phi=\omega,\quad \int_{\T^2}\phi=0,\quad&\text{for }x\in\T^2,~t\in (0,T]\label{eq:navier}
\end{align}
% todo: dot above L?
where $f\in L^2(\T^2,\R)$ is a forcing function and $\text{Re}>0$ is the Reynolds number, for the initial condition $\omega(0,\cdot)=0$.
The goal is to learn the non-linear operator $\G^\dagger:f \mapsto \omega(T,\cdot)$ for $T=5$ and $\text{Re}=500$.
%that is, the weak solution to \cref{eq:navier} at 5 timesteps into the future.
We use the dataset from prior work \citep{kossaifi2023multigrid}, which sets $\text{Re}=500$ and generates forcing functions from the Gaussian measure, $\mathcal{N}(0,27(-\Delta+9I)^{-4})$, and computes the solution function via a pseudo-spectral solver \citep{chandler2013invariant}.
There are 10\,000 training and 2000 test samples with resolution $128\times 128$.


\paragraph{Darcy Flow.}
We consider the steady-state 2D Darcy Flow equation, which models fluid flow through a porous medium. 
% could mention applications in pressure of subsurface flow, the deformation of linearly elastic materials, and the electric potential in conductive materials
Given $D=(0,1)^2$, the diffusion coefficient $a\in L^\infty ((0,1)^2;\R_+)$, 
and the forcing function $f\in L^2((0,1)^2;\R)$, the pressure $u$ satisfies
\begin{align}
-\nabla\cdot (a(x)\nabla u(x))=f(x),\quad&\text{for }x\in D\\
u(x)=0,\quad&\text{for }x\in\partial D
\end{align}

We seek to learn the non-linear operator 
$\G^\dagger: a\mapsto u$, the mapping from the diffusion coefficient $a$ to the solution function $u$.
We use the dataset from prior work \citep{li2021fourier}, which fixes $f\equiv 1$ and generates 5000 training and 1000 test samples with resolution $128\times 128$.

\paragraph{Spherical Shallow Water Equations.}
The spherical Shallow Water Equations on the rotating sphere are a model for a variety of geophysical flow phenomena such as atmospheric flows, tsunamis and storm surges. We consider the evolution of two state variables $\varphi$, $u$ (geopotential height and tangential velocity of the fluid column), governed by the equations
\begin{align}
\partial_t\varphi + \nabla \cdot (\varphi u) = 0, \quad&\text{for }x\in \mathbb{S}^2, t \in (0,T]\\
\partial_t(\varphi u) + \nabla \cdot F = S, \quad&\text{for }x\in \mathbb{S}^2, t \in (0,T]
\end{align}

for initial conditions $\varphi(\cdot, 0)=\varphi_0$, $u(\cdot, 0) = u_0$,
where $\mathbb{S}^2$ is the unit sphere, $F$ the momentum flux tensor with $F^{ij} = \varphi u^i u^j + \frac{1}{2} \varphi^2$, and $S$ a source term $S = - 2 \Omega x \times (\varphi u)$, which models the Coriolis force due to the rotation of the sphere with angular velocity $\Omega$.
%
We use the dataset from prior work \citep{bonev2023sfno}, which generates random initial conditions on the sphere at resolution $256\times 512$ and solves the PDE using the spectral solver in the \texttt{torch-harmonics} package \citep{bonev2023sfno}. 
%To train the models,  training data is generated on the fly, at a resolution of $256\times 512$. In each epoch, 120 training samples and 20 validation samples are used.

\paragraph{Shape-Net Car}
We evaluate on 3D real-world car dataset generated by prior work \citep{umetani2018learning,li2023gino} using the Reynolds Averaged Navier Stokes (RANS) equations \citep{wilcox1998turbulence}.
Each data point consists of mesh points representing a unique 3D car, and the goal is to predict the full 3D pressure field, given an inlet velocity of 20m/s.


\paragraph{Ahmed-Body CFD} 
Including another 3D real-world dataset for evaluation, we the dataset from \citep{li2023gino}, which is based on Ahmed-body shapes from \citep{ahmed1984some}, for which steady-state aerodynamic simulations were run using the GPU-accelerated Open-FOAM solver, with varying inlet velocity from 10m/s to 70m/s. The input and output data formats are similar to those of Shape-Net Car, but Ahmed-body has many more mesh points (100k) on the surface. 

\subsection{Model Details} \label{app:model_details}
We use the official implementation of each model, with the default hyperparameters, unless otherwise specified.
\footnote{\url{https://github.com/neuraloperator/neuraloperator}}
Furthermore, we release all of our code at \url{https://github.com/neuraloperator/neuraloperator}.
%\url{https://anonymous.4open.science/r/MP-FNO-B801}.

For the case of Shape-Net Car and Ahmed Body on GINO, we note that the default batch size is 1, in fact, the \emph{only allowed} batch size is 1, since each car geometry is unique.
As shown in \cref{fig:ball_figure}, throughput is identical (in fact, our memory saving is enough to have batch size 2, but this is not possible).
Although each individual Shape-Net Car fits in memory, other irregular-geometry datasets are too large to fit in memory, so sub-sampling is used \citep{li2023gino}. Therefore, mixed-precision GINO allows sampling significantly more mesh points than the full-precision default. 

\subsection{FNO Profiling} \label{app:profiling}

Using Nvidia's RTX 3090 Ti, we start by profiling full-precision FNO training on the Navier-Stokes dataset with the PyTorch profiler. \cref{fig:fno_profile} provides an overview of runtime breakdown by PyTorch modules and by individual GPU kernels. We observe that complex-valued operations are accountable for the majority of the runtime. 

% Figure environment removed

\subsection{Numerical Stability: Post-Forward Methods} \label{app:stability-global}
Most commonly, numerical underflows and overflows that occur during mixed-precision training are handled after the forward pass using loss scaling, gradient clipping, and other methods \citep{micikevicius2017mixed}. For experimentation, we implemented the following: \emph{(1)} automatic loss scaling via AMP; \emph{(2)} gradient clipping to a default value (5.0); \emph{(3)} delayed updates via gradient accumulation every 3 batches. These methods are compared to our \texttt{tanh} approach and a no-stabilizer baseline on the Darcy Flow experiment setting. In \cref{fig:global_stabilizer}, we show that all three baselines diverge during the first epoch of training, while \texttt{tanh} remains stable numerically. It is noteworthy that while loss scaling almost does not diverge, its scale decreases drastically with each update and becomes infinitesimal. The common problem for global stabilizers is that they do not directly address the numerical overflow of forward FFT operations within each FNO block. In the next subsection, we discuss local methods that acts on the FNO block itself. 

% Figure environment removed

\subsection{Numerical Stability: Pre-FFT Methods} \label{app:stability}

Here, we give additional details and experiments for our study on numerical stability of mixed-precision FNO.
%We find that there are various problems with existing numerical stability techniques. In general, methods that act on the global model, e.g., loss scaling, gradient clipping, and delayed updates, fail to address the problem because the instability happens within the local forward FFT module, which involves values at many different scales.

% todo: add results 

A simple idea for addressing overflow in FP16 is to scale down all values by adding a fixed pointwise division operation before the FFT.
However, this is an sub-optimal solution because all the values are scaled down equally, removing the data outliers but simultaneously squashing normal data. 
This forces all numbers into a very small range, which half precision cannot distinguish, preventing the model from converging to an acceptable performance. We find that accuracy deteriorates as the division factor grows: $10^1, 10^2, 10^3$. At $10^4$, the network completely fails to learn due to vanishing gradients. 
We also find that adding a normalization layer before the Fourier transform (essentially equivalent to scaling) does not fix the numerical instability in the long term. Additionally, normalization layers present extra GPU memory overhead because their statistics have to be tracked for gradient computation. 

\cref{tab:preactivation} compares several pre-FFT stabilization methods that \textbf{can} address numerical instability with acceptable computational costs. \texttt{hard-clip} forces maximum and minimum values in the input to a default range. \texttt{2$\sigma$-clip} similarly performs clipping but based on the data's mean and standard deviation. Out of these methods, \texttt{tanh} is founded to be the highest-performing. 
%
After adopting \texttt{tanh} as our pre-activation function, we assess its impact on the signal using a trained model, as shown in \cref{fig:preact_study}. We find that the loss of signal information is minimal in amplitude and phase. 

%\begin{wraptable}{r}{10cm}
\begin{table}
\label{tab:preactivation}
\begin{center}
\begin{small}
%\begin{tabular}{p{1.2cm}p{0.6cm}p{1.4cm}p{1.2cm}p{1.3cm}}
\begin{tabular}{ccccc}
\toprule
\textbf{Fourier} & \textbf{AMP} & \textbf{Pre-} & \textbf{Runtime} & \textbf{Train} \\
\textbf{Precision} & \textbf{(T/F)} & \textbf{Activation} & \textbf{per epoch} & \textbf{loss} \\
%Fourier\newline Precision & AMP \newline (T/F) & Pre- \newline Activation & Runtime \newline per epoch & Train \newline Error \\
\midrule
Full & F & N/A  & 44.4 & 0.0457 \\
Full & T & N/A  & 42.3 & 0.0457 \\ 
Half & F & N/A  & N/A & N/A \\ 
Half & T & \texttt{hard-clip} & 37.1 & 0.0483\\ 
Half & T & \texttt{2$\sigma$-clip}  & 40.0 & 0.0474\\ 
Half & T & \texttt{tanh}  & 36.5 & 0.0481 \\ 
\bottomrule
\end{tabular}
\end{small}
\vspace{-10pt}
\end{center}
\caption{\textbf{Comparison of different pre-activation functions used for numerical stability.}
\texttt{tanh} achieves the fastest runtime without significantly compromising on loss.
Since our focus now is on numerical stability, we compare only the average train loss over the 5 final epochs (we compare the test losses in \cref{subsec:accuracy}).
}
\end{table}
%\end{wraptable}



% Figure environment removed

\subsection{FNO Block Precision Ablation Study}\label{appsubsec:fnoablation}

As shown in \cref{fig:overview}, our FNO block has three complex-valued operations that are performed in half-precision: the forward FFT, the tensor contraction, and the inverse FFT. We investigate the effect of altering the precision setting of each on FNO training. This creates a total of eight settings since each operations have two precision configurations: full or half. In \cref{tab:fno_ablation}, we show error, runtime, and memory results from training for 30 epochs on the Darcy Flow dataset for each setting. The empirical results demonstrates the acorss-the-board advantage of our approach, which keeps all three operations in half-precision in the forward pass. 

\begin{table}[t]
\centering
\begin{tabular}{cccccc}
\hline
\textbf{Forward FFT} & \textbf{Contraction} & \textbf{Inverse FFT} & \textbf{Runtime per epoch} & \textbf{Memory} & \textbf{Train Error} \\
(F/H) & (F/H) & (F/H) & (sec) & (MB) & (L2 loss) \\
\hline
F & F & F & 17.06 & 8870 & 9.00 \\
F & F & H & 16.55 & 8908 & 8.71 \\
F & H & F & 17.11 & 8614 & 8.76 \\
F & H & H & 16.96 & 8658 & 8.15 \\
H & F & F & 17.64 & 7824 & 9.13 \\
H & F & H & 16.81 & 8004 & \textbf{7.51} \\
H & H & F & 16.57 & \textbf{7558} & 8.75 \\
H & H & H & \textbf{15.63} & \textbf{7550} & \textbf{7.49} \\
\hline
\end{tabular}
\caption{Performance of each setting on Darcy Flow trained for 30 epochs. The fully half-precision setting (our method) is advantageous across all the metrics. Note that the numerical stabilizer is only used when forward FFT is in half-precision.}
\label{tab:fno_ablation}
\end{table}




\subsection{Tanh Ablation Study}
A natural question from using pre-activation is to ask how a hyperbolic tangent would affect the full precision FNO model with no other changes.
In \cref{tab:full-prec-tanh}, we answer this question by running the full precision FNO model on the Navier-Stokes dataset.
We find that there is no noticeable change in the test losses, and the runtime-per-epoch increases by 0.8 seconds, or 1.6\%.
This concludes that \texttt{tanh} is a reasonable and practical choice to improve numerical stability for half precision methods.

\begin{table}[b]
\centering
{\small
\caption{\textbf{Ablation study on full-precision FNO with and without tanh} on the Navier-Stokes dataset.
There is no noticeable change in accuracy, showing that \texttt{tanh} is a practical choice to improve numerical stability in low precision methods. 
}
\label{tab:full-prec-tanh}
\begin{tabular}{lrrr}
\toprule
{} & $H^1$ & $L^2$ & time-per-epoch (sec) \\
\midrule
Full precision & 0.0121 & 0.00470 & 51.72 \\
Full precision + tanh & 0.0122 & 0.00465 & 52.56 \\
\bottomrule
\end{tabular}
}
\end{table}

%\subsection{Experiment Details} \label{appsubsec:experiments}


\subsection{Comparing the $H^1$ Loss} \label{appsubsec:additional}

We present additional experiments similar to the results in \cref{sec:experiments}, but using the $H^1$ metric instead of $L^2$.
In \cref{fig:mode_ablation_l2}, we plot an ablation study on the number of frequency modes for Darcy flow, similar to \cref{fig:mode_ablation} with $L^2$ loss. We see that the overall trends are similar, but test $L^2$ loss is noisier than test L1 loss; this makes sense, because the training loss is $H^1$.
%
In \cref{fig:full_results_l2}, we plot the performance-vs-time and Pareto frontier plots for Navier Stokes and Darcy flow, similar to \cref{fig:full_results} with $L^2$ loss. 
As before, we see largely the same trends as with $H^1$ loss, but the results are noisier.
%
In \cref{tab:full-8layer}, we plot the final performance from the models run in \cref{fig:full_results}.

 % Figure environment removed


% Figure environment removed


\begin{table}
\centering
{\small
\caption{Results for FNO with full precision, mixed precision, and precision schedule. All results are the average over 3 seeds trained to 19 hours each (which is the time it takes full precision FNO to reach 500 epochs).}
\label{tab:full-8layer}
\begin{tabular}{lrrr}
\toprule
{} & $H^1$ & $L^2$ & time-per-epoch (sec) \\
\midrule
Full FNO & $.00536\pm 2.1e-5$ & $.00214\pm 3.5e-5$ & 121.4 \\
Mixed FNO (Ours) & $.00645\pm 6.6e-5$ & $.00212\pm 1.4e-5$ & 80.2 \\
Precision schedule (Ours) & $.00515\pm 8.3e-5$  & $.00812\pm 4.1e-5$ & 80.2,~83.8,~121.4 \\
\bottomrule
\end{tabular}
}
\end{table}


% \begin{wrapfigure}{r}{0.5\textwidth}
 % Figure environment removed
%\end{wrapfigure}

\subsection{Frequency Mode Ablation Study}\label{appsubsec:frequency}
We run an experiment on synthetic data to demonstrate that the error caused by half precision is higher for higher frequencies, relative to the amplitude.
We create a signal based on sine and cosine waves with frequencies from 1 to 10, with randomly drawn, exponentially decaying amplitudes.
Then we plot the Fourier spectrum in full and half precision, as well as the absolute error of the half-precision spectrum as a percentage of the true amplitudes. 
See \cref{fig:synthetic};
we find that the percentage error exponentially increases.
Since in real-world data, the energy is concentrated in the lowest frequency modes, and the higher frequency modes are truncated, this gives further justification for our half precision method.

% Figure environment removed


\begin{table}
\centering
\begin{tabular}{ccc}
\toprule
& Navier-Stokes& Darcy Flow\\
\midrule
FNO + TF32 & 57.44& 14.12\\
Mixed FNO (Ours)& \textbf{53.72} & \textbf{13.52}\\
\bottomrule
\end{tabular}
\caption{\textbf{Training time per epoch on a Nvidia A100 GPU.}}
\label{tab:tf32}
\end{table}

\subsection{Other Numeric Systems}\label{appsubsec:othernumeric}
We compare training curves of full FNO, our mixed FNO pipeline, and FNO trained with AMP using Brain Float 16 (BF16) on the Navier-Stokes dataset in \cref{fig:bf16}. In addition to suffering from reduced precision, BF16 is not supported for most FFT-related operations in PyTorch, which makes its application on FNOs extremely difficult. 

Furthermore, we compare the efficiency of our mixed precision approach with TensorFloat 32 (TF32) on an Nvidia A100 GPU, on which TF32 is highly optimized. We report the time per epoch when training on Navier-Stokes and Darcy Flow datasets in \cref{tab:tf32}. Our method is still more efficient in comparison. 

In addition, we simulate 8-bit floating point (FP8) training using our method on the Darcy Flow dataset via clipping out-of-range values to the maximum and minimum representable under the E5M2 format, which has a higher dynamic range than the E4M3 format \citep{micikevicius2022fp8}. However, the GINO network diverges during training due to less available precision bits as shown in \cref{fig:bf16}.

Furthermore, the worse result of FP8, is actually predicted \cref{thm:precision_error}: while the precision constant for FP16 corresponds to $\epsilon=10^{-4}$, it corresponds to $\epsilon>10^{-2}$ for FP8 (even when using E4M3), and we can no longer claim that the precision error is lower than the resolution error.

% Figure environment removed

\subsection{Ablation Study on Contract and View-as-real Usage}
This ablation study shows the benefit of (1) decomposition of einsum into sub-equations and appropriate view-as-real usage, (2) our einsum path caching, (3) our greedy tensor contraction procedure, and (4) both weights and inputs in half-precision.

The results are as follows:
\begin{enumerate}
    \item The usage of view-as-real is different for different tensor contract implementations: 
    \begin{itemize}
        \item Option A: View-as-real on all tensors, and compute a single einsum in one equation (naive); 
        \item Option B: View-as-real on two tensors at a time, and compute step-by-step einsums with two-tensor sub-equations (optimized); 
        \item Option C: View-as-real only on high-dimensional einsums, and contracting low-dimension sub-equations in complex format (ours, optimal).
    \end{itemize} 
    Results on the Navier-Stokes can be found in \cref{tab:contract-implementation}. 

    \item Re-computing contract paths (naive) vs. Caching contract paths (ours):
    
    Since tensor shapes are static, we avoid repeated path computation in the default contract implementation. Path computation could take up to 75\% of the cost of the contract operation in the forward pass, which we try to avoid. 
    
    Results on the Navier-Stokes and Darcy Flow datasets can be found in \cref{tab:path-caching}. Our method reduces \textit{time to calculate paths} to almost zero during each iteration of einsum in FNOs.  

    \item Opt-einsum's FLOP-optimal path (naive) vs. memory-greedy path (ours):

    By default, opt-einsum minimizes FLOPs for the contraction, but our method optimizes memory usage to allow higher-resolution PDEs. Results on Ahmed-body and Shape-Net Car could be found in \cref{tab:greedy-path}. On memory-consuming 3D datasets, our method could save up to 12\%.

    \item Approximate only weights in half-precision (naive) vs. inputs and weights both half-precision (ours):

    If only weights were in half-precision, the memory reduction would have been significantly limited for the contract operation since the inputs are in full-precision.
    Results on Darcy Flow and Navier-Stokes can be found in \cref{tab:half-weights-inputs}.
    Note that in the case of Navier-Stokes, pytorch uses a much less memory-efficient kernel if inputs were in full-precision. Hence, it is critical to cast both weights and inputs to half-precision. 






    
\begin{table}[t] 
\centering
\caption{Comparison of tensor contraction implementations on the Navier-Stokes dataset}
\label{tab:contract-implementation}
\begin{tabular}{lcccc}
\hline
Metric                      & Option A   & Option B & Option C, Ours & Reduction from Option A (\%) \\ \hline
Time per Epoch (s)          & 1730       & 101.7    & \textbf{92.59}          & 94.65                        \\ 
Memory Usage (MB)           & 10310      & 5048     & \textbf{4832}           & 53.12                        \\ 
Contract Forward Time (s)   & 0.178      & 0.0084   & \textbf{0.0076}         & 95.73                        \\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{Re-computing contract paths vs. Caching contract paths} \label{tab:path-caching}
\begin{tabular}{lccc}
\hline
Dataset         & Time to calculate paths (s) & Time to compute einsums (s) & Path/einsum (\%) \\ \hline
Navier-Stokes   & 0.000573                    & 0.000751                   & 76.3             \\ 
Darcy Flow      & 0.000441                    & 0.000716                   & 61.6             \\ \hline
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{FLOP-optimal path vs. memory-greedy path}\label{tab:greedy-path}
\begin{tabular}{lccc}
\hline
Dataset     & Greedy Memory (MB) & Default Memory (MB) & Reduction (\%) \\ \hline
Shape-Net Car     & 7906               & 8662                & 8.72           \\ 
Ahmed-body  & 11990              & 13610               & 11.91          \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Approximate precision in weights vs. inputs and weights} \label{tab:half-weights-inputs}
\begin{tabular}{lccc}
\hline
Dataset         & Half-Prec Memory (MB) & Inputs-Full Memory (MB) & Reduction (\%) \\ \hline
Darcy Flow      & 7550                  & 8166                    & 7.54           \\ 
Navier-Stokes   & 4832                  & 9380                    & 48.46          \\ \hline
\end{tabular}
\end{table}



\end{enumerate}




