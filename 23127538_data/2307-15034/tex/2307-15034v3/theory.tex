


\section{Guaranteed approximation bounds}\label{sec:theory}

In this section, to motivate our mixed-precision neural operator training routine in~\cref{sec:experiments}, we theoretically show that the inherent \emph{discretization error} in the FNO block, from computing the discrete Fourier transform instead of the Fourier transform, is comparable to the \emph{precision error}, from computing the FNO block in half precision rather than in full precision.
We present our results in terms of a forward Fourier transform, and we give the full details and discussion in \cref{app:theory}.

We start by formally defining the discretization error.
Let $D$ denote the closed unit hypercube $[0,1]^d$ for dimension $d\in\N$.
Let $Q_1,\dots,Q_n$ denote the unique (up to $d$) partitioning of $D$, such that each $Q_j$ is a hypercube with sidelength $\nicefrac{1}{m}$.
For each $1\leq j\leq n$, let $\xi_j\in Q_j$ denote the vertex that is closest to the origin; formally, we have $Q_j=\prod_{k=1}^d [s_{j,k},t_{j,k}]$, and we define $\xi_j=(s_{j,1},\dots,s_{j,d})$.

Let $v:D\rightarrow \R$ denote an intermediate function within the FNO.
Recall from the previous section that the primary operation in FNO is the Fourier convolution operator, $(\K v_t) (x)$.
We say the \emph{discretization error} of $\F(v)$ is the absolute difference between the Fourier transform of $v$, and the discrete Fourier transform of $v$ via discretization of $v$ on $\Q_d=(\{Q_1,\dots,Q_n\},\{\xi_1,\dots,\xi_n\})$.
Formally, given Fourier basis function $\varphi_\omega(x)=e^{2\pi i  \langle \omega, x \rangle}$,
\begin{equation}
\texttt{Disc}(v,\Q_d,\omega)=\Big|\int_D v(x)\varphi_\omega(x)dx - \sum_{j=1}^n v(\xi_j)\varphi_\omega(\xi_j)|Q_j|\Big|.
\end{equation}
%
In other words, if we knew the true (infinite-dimensional) input function, we could run FNO using the (continuous) Fourier transform.
However, for real-world applications, we only have access to a discretization of the input, for example, on a $128\times 128$ grid, so we incur a discretization error.
We bound the discretization error as follows.
For the full proof details, see \cref{app:theory}.

\begin{theorem} \label{thm:discretization_error}
For any $D=[0,1]^d$, $M> 0$, and $L \geq 1$, let $\K \subset C(D)$ be the set of L-Lipschitz functions, bounded by $||v||_\infty\leq M$. Then there exists constants $c_1, c_2 > 0$ such that for all $n, d, \omega$, we have
\begin{equation*}
c_1\sqrt{d}\cdot Mn^{-\nicefrac{2}{d}}\leq 
\sup_{v\in\K}\left(\texttt{Disc}(v,\Q_d,1)\right) \text{ and }
\sup_{v\in\K}\left(\texttt{Disc}(v,\Q_d,\omega)\right)\leq c_2\sqrt{d}(|\omega|+L)M n^{-\nicefrac{1}{d}}.
\end{equation*}
\end{theorem}

Note that the upper bound is true for all $\omega$, while the lower bound is shown for $\omega=1$. It is an interesting question for future work to show the lower bound for $\omega>1$.

Intuitively, we bound the Riemann sum approximation of the true integral by showing that in the case where the function $v$ is bounded and Lipschitz, then each of the $n$ intervals contributes $n^{-(1+\nicefrac{1}{d})}$ error, up to parameters of the function.
Note that the discretization error upper bound also scales linearly for the frequency $\omega$, although we empirically show in \cref{sec:experiments} that the energy is concentrated in the lower frequency modes. 
The lower bound is satisfied when $v(x)=x_1 \cdots x_d$.
%
Furthermore, note that given a function $v$, if $n$ is not large enough, the discretization error can become arbitrarily large due to aliasing. For example, the function $v(x)=M\sin\left(2\pi (n+\omega) x\right)$ has discretization error $\Omega(M)$.

Next, we say that the \emph{precision error} of $\F(v)$ is the absolute difference between $\F(v)$ and $\overline{\F(v)}$, computing the discrete Fourier transform in half precision. Specifically, we define an $(a_0, \epsilon,T)$-\emph{precision system} as a mapping $q:\R\rightarrow S$ for the set $S=\{0\}\cup\{a_0(1+\epsilon)^i\}_{i=0}^T\cup\{-a_i(1+\epsilon)^i\}_{i=0}^T$, such that for all $x\in\R$, $q(x)=\texttt{argmin}_{y\in S}|x-y|$.
This represents a simplified version of the true mapping used by Python from $\R$ to \texttt{float32} or \texttt{float16} (we give further justification of this definition in \cref{app:theory}).
Then, we define 
\begin{equation}
\texttt{Prec}(v,\Q_d,q,\omega)=\Big|\sum_{j=1}^n v(\xi_j)\varphi_\omega(\xi_j)|Q_j|
-\sum_j q(v(\xi_j))q(\varphi_\omega(\xi_j))|Q_j|\Big|.
\end{equation}

Now, we bound the precision error as follows.

\begin{theorem} \label{thm:precision_error}
For any $D=[0,1]^d$, $M> 0$, and $L \geq 1$, let $\K \subset C(D)$ be the set of L-Lipschitz functions, bounded by $||v||_\infty\leq M$. Furthermore let $q$ be an $(a_0, \epsilon,T)$-precision system.
There exists $c>0$ such that for all $n,d,\omega$,
\begin{equation*}
\sup_{v\in\K}\left(\texttt{Prec}(v,\Q_d,q,\omega)\right)\leq c\cdot \epsilon M.
\end{equation*}
\end{theorem}


% todo: add more discussion
Once again, we show that each interval contributes $\nicefrac{\epsilon}{n}$ error up to the function's parameters.
Taken together, \cref{thm:discretization_error} and \cref{thm:precision_error} show that asymptotically, the discretization error can be as large as $M n^{-\nicefrac{2}{d}}$ (up to constants), while the precision error is always bounded by $\epsilon M$.
For example, for \texttt{float16} precision ($\epsilon=10^{-4}$), our theory suggests that the precision error is comparable to the discretization error for three-dimensional meshes up to size 1\,000\,000.
Given this motivation, we present our mixed-precision training pipeline in the next section.

