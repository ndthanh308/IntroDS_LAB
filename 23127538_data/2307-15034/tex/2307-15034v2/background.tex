
\section{Background and Related Work} \label{sec:related_work}

\paragraph{Fourier Neural Operator.}
Many real-world scientific and engineering problems are based on solving partial differential equations (PDEs).
Recently, many works have focused on machine learning-based methods to solve PDEs \citep{gupta2021multiwavelet,bhatnagar2019prediction,lu2019deeponet,adler2017solving}.
However, the majority of these methods are based on standard neural networks and are therefore limited to a fixed input and/or output grid, although it is desirable in many applications to have a map between function spaces.

\emph{Neural operators} are a new technique that addresses this limitation by directly learning maps between function spaces \citep{li2021physics,li2020neural,li2020multipole,kovachki2023neural}.
The input functions to neural operators can be in any resolution or mesh, and the output function can be evaluated at any point in the domain; therefore, neural operators are \emph{discretization invariant}.
%
The Fourier neural operator (FNO) \citep{li2021fourier}, inspired by the spectral method, is a highly successful neural operator \citep{wen2022u,gopakumar2023fourier,li2021learning,renn2023forecasting,wen2022accelerating}.

Now we give a formal description of FNO.
Let $\A:\{a:D_{\A}\rightarrow \R^{d_{\A}}\}$ and $\U:\{u:D_{\U}\rightarrow \R^{d_{\U}}\}$ denote the input and output function spaces, respectively.
In this work, we consider the case where $D_{\A}=D_{\U}\subset \R^d$ for $d\in\N$.
Given a dataset of pairs of initial conditions and solution functions $\{a_j,u_j\}_{j=1}^N$, which are consistent with an operator $\G(a_j)=u_j$ for all $1\leq j\leq N$, the goal is to learn a \emph{neural operator} $\G_\theta$ that approximates $\G$.
The primary operation in FNO is the Fourier convolution operator,
$(\K v_t) (x)=\F^{-1} (R\cdot T_K (\F v_t))(x),$ $\forall x\in D$,
where $\F$ and $\F^{-1}$ denote the Fourier transform and its inverse, $R$ denotes a learnable transformation, $T_K$ denotes a truncation operation, and $v_t$ denotes the function at the current layer of the neural operator.
In order to implement this operator on discrete data, we use the discrete Fast Fourier Transform (FFT) and its inverse (iFFT).
Recent work \citep{kossaifi2023multigrid} made a number of improvements to the FNO architecture, including 
%layer normalization, channel mixing, skip connections, and 
Canonical-Polyadic factorization \citep{kolda2009tensor} of the weight tensors in Fourier space, which significantly improves performance while decreasing memory usage.

\paragraph{Mixed-Precision Training.}
Mixed-precision training of neural networks consists of reducing runtime and memory usage by representing input tensors and weights (and performing operations) at lower-than-standard precision.
For example, PyTorch \citep{paszke2019pytorch} has a built-in mixed precision mode called automatic mixed precision (AMP), which places all operations at \texttt{float16} rather than \texttt{float32}, with the exception of reduction operations, weight updates, normalization operations, and specialized operations such as ones that are complex-valued.

Mixed-precision training has been well studied for standard neural nets \citep{zhao2022lns,de2018high,micikevicius2017mixed,jia2018highly}, but has not been studied for FNO.
The most similar work to ours is FourCastNet \citep{pathak2022fourcastnet}, a large-scale climate model that uses mixed precision with Adaptive Fourier Neural Operators \citep{guibas2022adaptive}. However, mixed precision is not applied to the FFT or complex-valued multiplication operations, which is a key challenge that the current work addresses.
%
Very recently, another work \citep{dool2023efficient} studies method of quantization for FNO at inference time. 
On the other hand, our work studies quantized \emph{training} and inference.
Furthermore, while their metric is the number of multiplicative and addition operations, we report memory usage, wall-clock time, and throughput.
