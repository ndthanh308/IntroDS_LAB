\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{acronym}
\usepackage{hyperref}
\usepackage{xcolor}

\usepackage{fullpage}
%%%%%%%%%%%%%%%%
\usepackage{mathtools}% http://ctan.org/pkg/mathtools
\usepackage{xparse}% http://ctan.org/pkg/xparse
\makeatletter
\newcommand{\raisemath}[1]{\mathpalette{\raisem@th{#1}}}% \raisemath{<len>}{...}
\newcommand{\raisem@th}[3]{\raisebox{#1}{$#2#3$}}
\makeatother
\NewDocumentCommand{\newbbar}{O{0pt} O{0pt}}{% \newhbar[<horz len>][<vert len>]
  \ensuremath{\mathrlap{\raisemath{#2}{\hspace*{#1}{\mathchar'26\mkern-9mu}}}b}%
}

\newcommand{\msun}{\ensuremath{M_\odot}}

\acrodef{GW}[GW]{gravitational-wave}
\acrodef{BBH}[BBH]{Binary black hole}  % First occurs at beginning of sentence :/
\acrodef{}[]{}

%%%%%%%%%%%%%%%%%%%%%%
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 right=20mm,
 top=10mm,
 bottom=15mm
 }
% By default, an article has some vary large margins to fit the smaller page format.  This allows us to use more standard margins.
%\setlength{\parskip}{1em}

\title{bla}
%\author{jamlucky.qau }
\date{\today}%October 2022}

\begin{document}

\maketitle

\begin{abstract} 
  \ac{BBH} systems detected via \ac{GW} emission are a recently opened astrophysical frontier with many unknowns and uncertainties.  Accurate reconstruction of the binary mass distribution with as few assumptions as possible is desirable in order to allow inference on formation channels and environments. 
  KDE-based methods allow us to dispense with assumptions such as underlying power laws or a universal mass-ratio distribution.  We deploy a self-consistent iterative method to estimate the full BBH mass distribution, finding features in primary mass consistent with previous investigations and [DESCRIBE WHAT WE SEE IN M2 AND Q].  [SOMETHING ABOUT ASTRO IMPLICATIONS??]
\end{abstract}


\section{Introduction}

In our first paper \cite{Sadiq:2021fin} we introduced a fast and flexible adaptive width kernel density estimation method as a non parametric estimation method for population reconstructions of binary black hole distribution from observed gravitational wave data. 
The observed gravitational wave data that we used are obtained from parameter estimation results and for each events the samples have uncertainties that can effect the overall population distribution. \\

In this work we are proposing a new method to reduce this  uncertainty estimate in our populations distribution by using iterative re-weighting scheme of samples for each observed events using the awKDE density estimates as a probability for re-weighting of samples. The idea is similar to standard expectation maximization algorithm 


\section{Method}


We construct adaptive KDE using randomly selecting parameter estimation (PE) samples obtained from PE for each observed gravitational wave event. These PE samples involve uncertainty, roughly this mean that for each event we get samples that roughly a Gaussian distribution but with no true median and hence  and the distribution of all observed event will be affected by this fact of PE uncertainty. In this work we worked out a method of how to do a  proper treatment of PE uncertainty and hence reach to a true probability density function of BBH distribution  as close as possible. 

\section{Iterative  Re weighting Method}
The main work in this analysis is to apply a similar idea as used in standard expectation maximization algorithm. The Expectation-Maximization (EM) algorithm \cite{10.2307/2984875} is a popular iterative method used in statistics to estimate parameters for statistical models when there are missing or incomplete data. In our case we are using parameter estimation results which do not represents true data values and hence have uncertainty which affect the accuracy of our estimate of awKDE results. 

\section{Analytical Formulation for PE Error Correction}
Our goal is to make KDE a self consistent scheme. In order to do it, we will follow the idea what is done in Bayesian Hierarchical analysis. Note that we cannot do exactly what Bayesian hiearchical analysis do with hyperparameters priors etc. See a nice review by Thrane and Talbot \href{https://arxiv.org/abs/1809.02293}{Bayesian analysis}.

We want to extend awKDE to work like hierarchical method in population analysis. 
This first step is to get new median of PE samples by some error corrections using a probability density or population function representing all events (a KDE or rate distribution from all events median KDE). To correct these medians  we can start of with a simple idea with some idea let all events PE sample follows a Gaussian distribution with a mean and fixed standard distribution and we can reweighted PE samples by using a trick as below which requires some algebraic calculation.

Lets assume that PE samples of each event follows a Gaussian distribution
\begin{equation*}
    p(x, \Tilde{x}_i, \Tilde{\sigma}_i) = \exp\left( -  \frac{(x-\tilde{x}_i)^2}{2 \Tilde{\sigma_i}^2}\right). 
\end{equation*}
Lets a population of all events follow some function $f_{pop}$
which have a form 
\begin{equation*}
   f_{pop}(x, x_i) = K\,*\, \exp\left(g_{pop} (x, x_i) \right), 
\end{equation*}
where  K is constant which we can define as 

\begin{equation*}
K = \frac{1}{f_{pop}(x=x_i)} = \exp \left( - g(x=x_i) \right)
 \end{equation*}
and that 
\begin{equation*}
g_{pop}(x)= g(x=x_i) + \frac{\partial g}{\partial x}|_{x_i} (x - x_i) + \frac{1}{2}\frac{\partial^2 g}{\partial x^2}|_{x_i} (x - x_i)^2+ ...
\end{equation*}

Simplify with constant we can write normalized $f_{pop}$ as 
\begin{equation}
   f_{pop}(x) =  \exp \left( \frac{\partial g}{\partial x}|_{x_i} (x - x_i) + \frac{1}{2}\frac{\partial^2 g}{\partial x^2}|_{x_i} (x - x_i)^2+ ... \right) , 
\end{equation}
lets use  $X = x - x_i, \, g_1 = \frac{\partial g}{\partial x}|_{x = x_i}, , \, g_2 = \frac{\partial^2 g}{\partial x^2}|_{x = x_i}$
% 
\begin{equation}
   f_{pop}(x) =  \exp \left(  g_1 X +  g_2 \frac{X^2}{2}+ ... \right) , 
\end{equation}
and 
\begin{equation*}
    p(x, \Tilde{x}_i, \Tilde{\sigma}_i) = \exp\left( -  \frac{X^2}{2 \Tilde{\sigma_i}^2}\right). 
\end{equation*}

Now we can have likelihood of \textbf{an event} using Bayes theorem
\begin{equation}
    P_{rw}( \mathbf{x_i} | x, f_{pop}) = K_c * f_{pop} *p(x)
\end{equation}
 where $K_c$ is a constant term and we can also write  
\begin{equation}
\label{eq:6}
    P_{rw}(x_i| x, f_{pop}) = K_c \, * \exp \left(  g_1 X  +  g_2 \frac{X^2}{2} - \frac{ X^2}{2\sigma^2} + ... \right)
\end{equation}


Now the goal is to write above equation \eqref{eq:6} in Gaussian distribution form to find its mean and standard deviation in terms of given $g_{pop}$ and this will give us re-weighted PE samples that can reach close to true distribution. 

Let's ignore this constant and use upto $X^2$ terms in the exponential we get RHS of above equation as
 
\begin{equation*}
    RHS = \exp\left(Xg_1 + \frac{X^2 g_2}{2} -  \frac{X^2}{2\sigma^2} \right)
\end{equation*}
where $X = x - x_i, \, g_1 = \frac{\partial g}{\partial x}|_{x = x_i}$, $g_2 = \frac{\partial^2 g}{\partial x^2}|_{x = x_i}$,

\begin{equation*}
    RHS = \exp\left( - \left( \frac{ 1 -g_2 \sigma^2}{2\sigma^2} X^2 - g_1 X \right) \right)
\end{equation*}
%
Using  $a = \frac{1 - g_2 \sigma^2}{2\sigma^2}$ and $b = g_1$
and using 
\begin{equation*}
    aX^2 - bX =  a \left( \left(X - \frac{b}{2a}\right)^2 - 
    \left(\frac{b}{2a}\right)^2 \right)
\end{equation*}
We can write 


\begin{equation}
  P_{rw}(x_i)  = K_c \, * \exp\left( \frac{b^2}{4a}  \right) \exp \left( -a\left(X - \frac{b}{2a} \right)^2 \right)
\end{equation}
or
\begin{equation}
P_{rw}(x_i)    = K_c \, * \exp\left( \frac{b^2}{4a}  \right) \exp \left( - \frac{(X - b/2a)^2}{1/a} \right)
\end{equation}

Plug back   $a = \frac{1 - g_2 \sigma^2}{2\sigma^2}$ and $b = g_1$
\begin{equation}
    P_{rw}(x_i) = K_c\, *\exp \left( \frac{(g_1 \sigma)^2}{2(1- g_2 \sigma^2)} \right)  \exp\left(- \frac{(x- (x_i + \frac{g_1 \sigma^2}{1-g_2\sigma^2})^2}{2\sigma^2/(1 - g_2 \sigma^2)} \right)
\end{equation}

Simplifying and put into equation \eqref{eq:6}
we get
\begin{equation*}
    P_{rw} =K_c \,* \exp \left(  \frac{(g_1 \sigma)^2}{2(1- g_2 \sigma^2)}  \right) \exp \left(  -  \frac{(x- (x_i + \frac{g_1 \sigma^2}{1-g_2 \sigma^2})^2}{2\sigma^2/(1 - g_2 \sigma^2)} \right), 
\end{equation*}
so re-weighted median of a PE sample for an event can be 
\begin{equation}
\label{eq:shifted_mean}
\color{blue}
\Tilde{x}_{new} = x_i  + \frac{g_1 \sigma^2}{1-g_2\sigma^2}
\end{equation}
and standard deviation is
\begin{equation}
\color{blue}
\Tilde{\sigma}^2 _{new} = \frac{\sigma^2}{(1 - g_2 \sigma^2) } 
\end{equation}
and constant is 
\begin{equation*}
const =  K_c \, * \exp\left( \frac{(g_1 \sigma)^2}{2(1- g_2 \sigma^2)} \right)
\end{equation*}


So now we can use this idea for an iterative scheme and for each iteration we can get a new $f_{pop}$ and see it it converges that can take us to true distribution as close as possible.

\subsection{Algorithm Steps}
 We implement this idea and find out that we have issues with shifting the data and hence new $f_{pop}$ do not converge to previous ones with reweighted values from itertive scheme.

 We follow these steps in our code
 
\begin{itemize}
    \item First we do have all PE samples file of detected GW events. Using python we will compute means $\Tilde{x}_i$ and standard deviations $\sigma_i$ of all those events PE-samples. 
    \item If we want, using means and standard deviations we can compute $p(x_i)$ for each event. we will see these cancel outs below 
    \item From the mean values (from all events) we get $f_{pop}$. One possible $f_{pop}$ choice can be  awKDE  as we did in paper \href{https://arxiv.org/abs/2112.12659}{awkde}. Gaussian Kernel KDE. We can also use rate estimate as $f_{pop}$ by using VT corrections as function of m1 (as in  paper \href{https://arxiv.org/abs/2112.12659}{awkde}) and  will compute rate using $rate \sim KDE/VT(m1)$ 
    \item For each event we can get $P_{rw}(x_i) = f(g_1, g_2,\Tilde{x}_i,\sigma_i)$
    \item Now we will compute $g_1$ and $g_2$ for each event  using     




\begin{equation*}
   P_{rw} = \exp \left(  g_1 (x- x_i) + \frac{1}{2} g_2 (x-x_i)^2 \right) * p = f_{pop} * p
\end{equation*}
where $f_{pop} =$ rate or kde. we can cancel $p$  and taking $\log$ of equation, we get 

\begin{equation*}
    g_1 (x- x_i) + 1/2 g_2 (x-x_i)^2 = \ln\{f_{pop}\}  
\end{equation*}
Taking derivative with respect to x and g is already independent of x because it is value at $x=x_i$ (we normalized it)
\begin{equation*}
    g_1  = \frac{d ln\{f_{pop(x)}\}}{dx} \bigg|_{x_i}  
\end{equation*}
another derivative gives
\begin{equation*}
    g_2  = \frac{d^2 ln\{f_{pop(x)}\}}{dx^2}  \bigg|_{x_i}
\end{equation*}

\item From the first and second derivatives of $f_{pop}$, we can compute $g_1$ and  $g_2$ which are function of $x_i$ and  we can get their value at specific x (specific GW event) using  and this can be used to get new-mean and new-sigma for each event of GW.
\item Using new mean values of each event we can get new $f_{pop}$ and repeat this procedure. Note that in each iteration new-mean and new sigma will be obtained from updated g1 and g2 but the  we always use original mean and sigma values from PE samples. 
\item If $f_{pop}$ converges from one iteration to next several one we get a success.
\end{itemize}
\subsection{Results and Issues}
When we use $f_{pop}$ to be rate (or KDE) their numerical derivative make $g_1$ and $g_2$ to be not normalize or smoothed and that can cause huge shifts in shifted mean values both in negative and positive. One of reason can be second derivatives issues at peaks and troughs. To overcome such issues we tried to apply running averages for (first only) $g_2$ and (than also of) $g_1$   at each event values following this procedure
\begin{itemize}
    \item We compute $g_2$ ($g_1$) with second (first) derivative of $\ln f_{pop}$
    \item For each event we have a $g_2$ value and we also have sigma. Using a sigma window we compute average of g2 using 
    \begin{equation*}
        g_{2}^{smooth} = \frac{\sum_{i=x_i-\sigma_i}^{x_i+\sigma_i }g_2}{N}
    \end{equation*}
    
    where N is total points between $x_i-\sigma_i$ and $x_i+\sigma_i$
   \item Use this $g_{2}^{smooth}$ values in the shifted formula \eqref{eq:shifted_mean}
\end{itemize}


This did not solve issue as we see a large shifts in the mean value which can be due to $\frac{1}{\sigma^2}$ to be larger than $g_2$. 

\subsection{Modification: Product of $f_{pop}$ with a Gaussian Distribution }
We Try new idea to avoid huge shifts in reweighting sample 
\begin{itemize}
    \item  For each event  we have its mean and (fixed) standard deviation. We multiply $f_pop$ with some (random) gaussian sample created using mean and sigma of each event to get reweighted samples
    
    \begin{equation*}
        P_{rw}(x_i) = f_{pop} \,* \, p(x_i) 
    \end{equation*}
    and compute the mean of this new reweighted sample $P_{rw}$ as shifted mean and follow this procedure iteratively.

    \item To compute median of this $P_{rw}$ (product), we  compute the index of our grid (onto which we compute $f_{pop}$) at the 50\% area under curve and value of grid at that index gives us the shifted mean
\end{itemize}

\subsection{Modification: Re-weighting based on median of  previous 100 iterative $f_{pop}$ results}
\subsubsection{Using a mockdata samples}
   Although we need to fix iterative procedure but we also need to show the robustness of this method for which we can try a mock data test. We have mock data of 60 values in each of 10 thousands datasets which is data for uniform distribution with a gaussian peak. We need both mean (that is those 60 values in each dataset)  and sigma. For sigma we will follow what we tried in peak analysis that is we tried error estimate from 90\% credible intervals of real distribution of GW events and use it to compute sigma in each of 60 values of each dataset. We will save this mock dataset with mean and sigma in hdf file and apply our method for each 10 thousand datasets and see if this iterative procedure can find KDE to converge to exact gaussian peak as we obtained in first data. This is to show this algorithm is robust.

\newpage


\section{Optimization over bandwidth and location Variable}

A nice \href{https://www.sciencedirect.com/science/article/pii/S0167947309001261?via%3Dihub}{article} that hints on improving adaptive KDE is . Here we worked on algebraic expressions to get maximized bandwidth and location variables. 
%More details will be added later
The likelihood for Gaussian Kernels with inverse bandwidths $\newbbar$ and locations $m_i$
is given by
\begin{equation*}
    \mathcal{L} = \frac{1}{(\sqrt{2\pi})^N}\prod_{k=1}^{N} \left[ \sum_{i=1}^{N} \newbbar_i \, \exp{\left(\frac{-(x_k-m_i)^2 \,  \newbbar_{i}^{2}}{2 } \right)} \right]
\end{equation*}
log Likehood with some amendments can be written as 
\begin{equation}
\label{likelihoodbandwidthandlocation}
   \log\, \mathcal{L} =  \frac{-N}{2} \log(2\pi) +  \sum_{k=1}^{N} \log \left[  \sum_{i=1}^{N} \left( \newbbar_i \, \exp{\left(\frac{-(x_k-m_i)^2 \,  \newbbar_{i}^{2}}{2 } \right)}   \right) \right]
\end{equation}
Now for maximizing this likelihood with respect to inverse bandwidth and locations we need 
\begin{equation*}
   \frac{d}{d\newbbar_i}\, \log\, \mathcal{L} =  0, \,\,\,\,
   \frac{d}{dm_i}\, \log\, \mathcal{L} =  0,  
\end{equation*}




So lets first try simple case for just one location variable function as in the paper and then generalize it to 2 variable case
\\
In paper we have 
\begin{equation*}
   \hat{f}_m (x)  = n^{-1} \sum_{i=1}^{n} K_h(x - m_i)
\end{equation*}
where $K_h(x - m_i) = \exp{(-(x-m_i)^2/2)}$

The Loglikelihood function is  (product to sum with log)
\begin{equation*}
    \log \mathcal{L} = \sum_{k=1}^{n} \log \hat{f}_m (X_k)
\end{equation*}
Now they take derivative with respect to $m_l$ where $l=1,2,3 ...$
\begin{equation*}
    \frac{d}{dm_l} \log \mathcal{L} = \sum_{k=1}^{n} \frac{1} {\hat{f}_m (X_k)} \frac{d (\hat{f}_m (X_k))}{dm_l}
\end{equation*}
Now 

\begin{equation*}
    \frac{d}{dm_l} (\hat{f}_m (X_k)) =   \frac{d}{dm_l}  \sum_{i=1}^{n} \exp{\left(\frac{-1}{2} (X_k - m_i)^2 \right)}
\end{equation*}

Now in the sum only $l=i$ term will be non zero and rest will be zero


\begin{equation*}
    \frac{d}{dm_l} (\hat{f}_m (X_k)) =      \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)} \frac{d  (\frac{-1}{2} (X_k - m_l)^2) }{dm_l}
\end{equation*}

which gives

\begin{equation*}
    \frac{d}{dm_l} (\hat{f}_m (X_k)) = ( X_k - m_l)     \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)} 
\end{equation*} 


we finally get
\begin{equation*}
    \frac{d}{dm_l} \log \mathcal{L} = \sum_{k=1}^{n} \frac{1} {\hat{f}_m (X_k)} ( X_k - m_l)     \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)}
\end{equation*}

which is what is given in paper section 2
\begin{equation*}
    \sum_{k=1}^{n} \frac{1} {\hat{f}_m (X_k)} ( X_k - m_l)     \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)} = 0 , \, \, \, \, l=1,2,...,n
\end{equation*}

\begin{equation*}
    \sum_{k=1}^{n} \frac{1} {\hat{f}_m (X_k)}  X_k *  \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)}    - \,  \textcolor{blue}{m_l} \, \sum_{k=1}^{n} \frac{1} {\hat{f}_m (X_k)} \,  * \,    \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)} = 0 , \, \, \, \, l=1,2,...,n
\end{equation*}


\textcolor{red}{They get Iterative formula solving for $m_l$ but somehow in the paper they do not take into account $m_l$ in the  exponential function and in main $\hat{f}_m(X_k)$. 
They have iterative scheme}

\begin{equation*}
\color{blue}
    m_{l}^{new} =  \frac{1} { \sum_{k=1}^{n} \frac{1} {\hat{f}_m (X_k)} \,      \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)} } *\, \sum_{k=1}^{n} \frac{1} {\hat{f}_m (X_k)} ( X_k *  \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \right)})
\end{equation*}


If we follow same for the generic case $\hat{f}_{m \, , \, b}$ and maximize for both variables we can get similar formulae


\begin{equation*}
   \hat{f}_{m,\newbbar} (x)  = n^{-1} \sum_{i=1}^{n} K_h[(x - m_i )*\newbbar_i]
\end{equation*}
where $K_h[(x - m_i)*\newbbar_i] = \newbbar_i \, \exp{\left( \frac{-(x-m_i)^2\newbbar_i^2}{2} \right)}$

The log likelihood function is  (product to sum with log)
\begin{equation*}
    \log \mathcal{L} = \sum_{k=1}^{n} \log \hat{f}_{m\, ,\, \newbbar} (X_k)
\end{equation*}
Now they take derivative with respect to $m_l$ or $\newbbar_l$ where $l=1,2,3 ...$
\begin{equation*}
    \frac{d}{dm_l} \log \mathcal{L} = \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)} \frac{d (\hat{f}_{m\, ,\, \newbbar} (X_k))}{dm_l}
\end{equation*}

\begin{equation*}
    \frac{d}{d\newbbar_l} \log \mathcal{L} = \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, , \, \newbbar} (X_k)} \frac{d (\hat{f}_{m\, ,\, \newbbar} (X_k))}{d\newbbar_l}
\end{equation*}

Now 
\begin{equation*}
    \frac{d}{dm_l} (\hat{f}_{m \, , \, \newbbar} (X_k)) =   \frac{d}{dm_l}  \sum_{i=1}^{n} \newbbar_i \exp{\left(\frac{-1}{2} (X_k - m_i)^2 \, \newbbar_i^2 \right)}
\end{equation*}

which, after simplifying and having only term in sum ith index $l$ survived, gives

\begin{equation*}
    \frac{d}{dm_l} (\hat{f}_{m \, , \, \newbbar} (X_k)) =   \newbbar_l \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, * \, \left( \newbbar_l^2 \, (X_k -m_l)  \right)
\end{equation*}

So now we get
\begin{equation*}
     \frac{d}{dm_l} \log \mathcal{L} = \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \, \, \newbbar_l \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, * \, \left( \newbbar_l^2 \, (X_k -m_l)  \right)  = 0 \,\, , \,\,\,\, l = 1,2,3...,n
\end{equation*}
Solving above for $m_l$  as for simple case assuming $m_l$ inside exponential and $\hat{f}$ is old we get iterative scheme


\begin{equation*}
    m_{l}^{new} = \frac{ \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \, \, \newbbar_l \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, * \, \left( \newbbar_l^2 \, X_k  \right) } { \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \, \, \newbbar_l \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, * \, \left( \newbbar_l^2   \right)}
\end{equation*}

This looks same as in simple case as we can cancel $\newbbar_l^2$ term
So final expression looks like

\begin{equation*}
\color{blue}
    m_{l}^{new} = \frac{ \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \, \, \newbbar_l \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, * \,  X_k  } { \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \, \, \newbbar_l \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, }  \,\, , \,\,\,\, l = 1,2,3...,n
\end{equation*}
Repeating above with maximization with respect to $\newbbar_l$
\begin{equation*}
    \frac{d}{d\newbbar_l} (\hat{f}_{m \, , \, \newbbar} (X_k)) =   \frac{d}{d\newbbar_l}  \sum_{i=1}^{n} \newbbar_i \exp{\left(\frac{-1}{2} (X_k - m_i)^2 \, \newbbar_i^2 \right)}
\end{equation*}
Again in sum only term $l=i$ survives and we need now product of terms in derivative
\begin{equation*}
    \frac{d}{d\newbbar_l} (\hat{f}_{m \, , \, \newbbar} (X_k)) =  [1]*  \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)} + \newbbar_l \, [\exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)} \, *  2\,\newbbar_l \frac{-1}{2} (X_k - m_l)^2 ]
\end{equation*}
which gives 
\begin{equation*}
    \frac{d}{d\newbbar_l} (\hat{f}_{m \, , \, \newbbar} (X_k)) =    \newbbar_l \,\exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)} \left(  \frac{1}{\newbbar_l} - \newbbar_l \, *  (X_k - m_l)^2  \right) 
\end{equation*}

We have now 

\begin{equation*}
     \frac{d}{d\newbbar_l} \log \mathcal{L} = \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \, \, \newbbar_l \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, * \, \left(\frac{1}{\newbbar_l} - \newbbar_l \, *  (X_k - m_l)^2  \right)  = 0 \,\, , \,\,\,\, l = 1,2,3...,n
\end{equation*}
 or 
 \begin{equation*}
      \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \,  \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, * \, \left( 1 - \newbbar_l^2 \, *  (X_k - m_l)^2  \right)  = 0 \,\, , \,\,\,\, l = 1,2,3...,n
\end{equation*}



If we simplify this we have expressions for $\newbbar_l$ as
\begin{equation*}
\color{blue}
    \newbbar_l^{new}=  \sqrt{ \frac{  \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \,  \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}} {  \sum_{k=1}^{n} \frac{1} {\hat{f}_{m\, ,\, \newbbar} (X_k)}  \, \,  \exp{\left(\frac{-1}{2} (X_k - m_l)^2 \, \newbbar_l^2 \right)}  \, \, *  (X_k - m_l)^2 } }    \,\, , \,\,\,\, l = 1,2,3...,n
\end{equation*}


\bibliography{reference}
\end{document}
