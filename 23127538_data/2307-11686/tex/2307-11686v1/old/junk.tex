
% For the purposes of this paper, it will be helpful to view a Kronecker product $A \otimes B$ as a linear operator which acts on matrices by modifying the rows according to $A$ and the columns according to $B$.  Specifically, for any matrices $A \in \mathbb{R}^{n_1 \times m_1}$ and $B\in \mathbb{R}^{n_2 \times m_2}$ the Kronecker product $(A\otimes B)$ can be understood as a linear operator on the space of $m_1 \times m_2$  matrices defined by
% \[
% ((A \otimes B)Z)_{i,j} = \sum_k \sum_\ell A_{ik} B_{j\ell} Z_{k\ell}.
% \]
% This point of view also extends to third-order tensors such as $X \in \mathbb{R}^{R \times P \times G}$.  For any triple of matrices $A \in \mathbb{R}^{R' \times R}$, $B \in \mathbb{R}^{P' \times P}$, and $C \in \mathbb{R}^{G' \times G}$, the Kronecker product $A\otimes B \otimes C$ may be viewed as the linear operator on the space of $R\times P \times G$ tensors such that
% \[
% ((A \otimes B\otimes C)X)_{r',p',g'} = \sum_{r,p,g} A_{r',r}B_{p',p}C_{g',g} X_{r,p,g}.
% \]
% To articulate statements about Kronecker products it is helpful to introduce a notation for the basis of vector spaces.  We take $\mathbb{I}$ to denote the indicator function and $\basisvec{N}{i}$ to denote the matrix in $\mathbb{R}^{N\times 1}$ with $\basisvecfull{N}{i}{j}= \mathbb{I}_{i = j}$.  Thus, for example, we may compute that $(\basisvec{P}{p}^\top \otimes \basisvec{G}{g}^\top) \theta = \theta_{p,g}$---though it is important to note that this equality is only well-defined because we may identify the tensor space $\mathbb{R}^{1 \times 1}$ with $\mathbb{R}$.  The statement $\mathrm{cov}(\theta)=K\otimes \Psi$ can thus be understood to signify that 
% \begin{align*}
% \mathrm{cov}(\theta_{p,g},\theta_{p',g'})
% &=(\basisvec{P}{p}^\top \otimes \basisvec{G}{g}^\top) \mathrm{cov}(\theta) (\basisvec{P}{p'} \otimes \basisvec{G}{g'})\\
% &= (\basisvec{P}{p}^\top \otimes \basisvec{G}{g}^\top) (K \otimes \Psi)(\basisvec{P}{p'} \otimes \basisvec{G}{g'}) = K_{p,p'} \Psi_{g,g'}
% \end{align*}

For example, the rows of $\theta \in \mathbb{R}^{P \times G}$ are independent if and only if the covariance of $\theta$ can be expressed as a sum of the form
\[
\sum_p (\basisvec{P}{p} \basisvec{P}{p}^\top)\otimes A_p
\]
for some collection of matrices $A_1,\ldots,A_P$ with $A_p\in \mathbb{R}^{G \times G}$ for each $p$.  This can be verified directly, as such a covariance implies that
\[
\mathrm{cov}(\theta_{p,g},\theta_{p',g}) = \sum_p \mathbb{I}_{p=p'} (A_p)_{g,g'}
\]
will be zero unless $p=p'$.