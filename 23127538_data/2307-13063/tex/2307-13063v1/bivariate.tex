\section{Bivariate Correlation Analysis}\label{sec:bivariate}
The simplest \AU relations try to directly relate two different features of neutron stars, i.e., they are \emph{bivariate} relations. We believe that by evaluating the correlation between different features, we can automate finding such bivariate relations to a high degree. The main issue, however, is identifying which correlation measure is best suited to the task of finding \AU relations (for neutron stars). 

In this section, we describe four different correlation measures that we consider viable for the identification of \AU relations, and describe how we utilize these measures for this purpose.

\subsection{Correlation Measures for Bivariate Relations}
The standard correlation measure that is typically considered when on talks about correlation is the \emph{Pearson correlation coefficient}. In the following, we briefly introduce both this measure, and 3 other correlation measures that are generally considered to perform better when evaluating non-linear correlation in particular. %We evaluate the ability of four different correlation measures to identify universally related features in neutron star model data. We will first briefly introduce each measure, and discuss them below.

\medskip
\noindent
\textbf{Pearson Correlation.}
The Pearson correlation coefficient $\rho$ of two random variables $X$ and $Y$ is given by
\begin{equation}
\rho(X,Y) = \frac{\cov(X,Y)}{\sigma_X \sigma_Y}
\label{eq:pearson}
\end{equation}
where $\cov(X,Y)$ is the covariance of the two random variables, and $\sigma_X$ and $\sigma_Y$ their standard deviations.

\medskip
\noindent
\textbf{Distance Correlation.}
The Distance correlation\cite{DistanceCor} $\dCor$ of two random variables $X$ and $Y$ is defined similarly to the Pearson correlation by 
\begin{equation}
\dCor(X,Y) = \frac{\dCov^2(X,Y)}{\sqrt{\dVar(X)\dVar(Y)}}
\label{eq:distance}
\end{equation}
where the standard notions of covariance and standard deviation are replaced by sample distance covariance $\dCov$ and distance standard deviation $\dVar$. 
While for covariance and standard deviation are computed based the distance of each sample from means $\bar X$ and $\bar Y$ of the random variables, $\dCov$ and $\dVar$ denote similar quantities that are instead based on the \emph{pairwise} distance of all samples.

\medskip
\noindent
\textbf{Mutual Information.}
The mutual information\cite{MutualInf} $I(X;Y)$ of two random variables $X$ and $Y$ is given by
\begin{equation}
I(X;Y) = \sum_{x,y} P_{XY}(x,y) \log \frac{P_{XY}(x,y)}{P_X(x) P_Y(y)}
\label{eq:mutual}
\end{equation}
where $P_{XY}$ is the joint probability distribution of $X$ and $Y$, and $P_X$ and $P_Y$ are the marginal distributions given by
\begin{equation}
P_X(x) = \sum_y P_{X,Y}(x,y)\,.
\end{equation}
Mutual information measures how much we can learn about one random variable $Y$ by having knowledge of another random variable $X$ (or vice versa), and is zero exactly when the two distribution are independent (i.e. knowledge about $X$ does not tell us anything about $Y$). As a quantity, it measures how many bits can be saved if we try to encode $Y$ while assuming knowledge of $X$ (in contrast to encoding $Y$ on its own without any further knowledge).

Technically the above definition for mutual information is for discrete variables, and our use case is centered around continuous random variables. However, in practice, the data vectors we use are discrete, and computational methods have to be used to obtain the sample distribution from the actual sample vectors. In this paper, we rely on the \textsc{mutual\_info\_regression} method implemented in the \textsc{sklearn} Python package.

\medskip
\noindent
\textbf{Maximal Information.}
Maximal information~\cite{MaxInf} is a direct extension of mutual information to continuous variables that puts a given pair of random variables X and Y into histograms of varying bin-sizes, computes the mutual information for each such histogram, and finally chooses the binning that maximizes the mutual information. That is, the maximal information coefficient $\MIC$ of two random variables $X,Y$ is given by
\begin{equation}
\MIC(X;Y) = \max_{B} \frac{I(X;Y)}{\log_2(\min(B_X, B_Y))}
\label{eq:mic}
\end{equation}
where $B$ is the total number of used bins (typically with some upper bound, cf.~\cite{MaxInf}), and $B_X$ and $B_Y$ are the number of bins used for $X$ and $Y$ respectively. To compute the maximal information between two vectors, we will utilize the \textsc{minepy} package for Python~\cite{10.1093/bioinformatics/bts707}.

\medskip
\noindent
\textbf{Comparison of Correlation Measures.}
The main issue with the more prominent Pearson correlation measure is that it only identifies linear correlation of features. While we can adjust to this to some degree by computing some function values of our features (i.e. computing some polynomials or exponential function on the features values), this can become fairly cumbersome in practice. In recent years, especially with the advent of Big Data and the necessity of finding non-linear correlations in various applications, the other above mentioned correlation measures have been developed~\cite{DistanceCor,MaxInf}. 
The main idea behind them is that instead of looking for a global, linear correlation, they instead approximate global correlation by finding local (linear correlation), i.e. correlation of data points that are in close proximity, and generalize it over the whole data set. This applies to both Distance correlation, which to some degree generalizes the Pearson correlation in such a manner, and Maximal Information, which directly generalizes the measure of Mutual Information.

A similar comparison has already been performed in the past by Clark~\cite{clark2013comparison}. They find that, in particular for non-linear relations, distance correlation and mutual/maximal information outperform Pearson correlation in identifying correlated variables. Our purpose for this work is to verify that the same observations can also be made for the use case of finding \AU relations in neutron star model data, and evaluate which correlation measure indeed performs best for this use case. 

\subsection{General Methodology}
Our general approach to evaluating the different correlation measures introduced above, and also for later automatically finding bivariate \AU relations, is the following:
\begin{enumerate}
\item Obtain neutron star model data with features $F_1, \ldots, F_n$ from theoretical/numerical computations.
\item Compute the pairwise correlation of all feature pairs using on of the above correlation measures. This provides us with the correlation matrix $\mathbf{M}$, where the entry $\mathbf{M}_{i,j}$ is the correlation between features $F_i$ and $F_j$.
\item Specify a correlation threshold $\tau$ above which we will consider feature pairs correlated, i.e. find all entries in $\mathbf{M}$ with
\begin{equation}
\mathbf{M}_{i,j} \geq \tau\,.
\end{equation}
This threshold will depend on the correlation measure used, and finding the best value for it is something we want to achieve here, but might need to be further explored in future work.
\item For each selected feature pair, choose a suitable model. Here, model denotes the expected functional relation between the two selected features. This can be, e.g., a linear, polynomial, exponential model, etc. Model selection is a notoriously difficult task in data analysis, and we will here simply choose to evaluate a number of preset templates for the functional relations, and choose the one with the best fit after the following step.
\item Fit the model to the given data to determine the coefficients of the best fit for the \AU relation.
\end{enumerate}


