\section{Multivariate Correlation Analysis}\label{sec:multivariate}
Until now, we have only considered the functional relation between two features, and tried to find such pairs of features that allow for  \AU  relations across different equations of state. A straightforward extension then, of course, is to look for multivariate relations, i.e. such relations where one predicted/target features is described in terms of a function that depends on more than one explanatory feature.

The field of high-dimensional data analysis is a widely studied field that in particular gained a lot of notoriety in recent times due to the advent of the big data paradigm. While many different approaches, theories and methods have been developed to deal with high dimensionality in data, we will here consider one very prominent method: principal component analysis (PCA). PCA is a dimensionality reduction and feature extraction technique that has been used to great success in various data analysis use cases~\cite{barnett1987origins}. Recently, Soldateschi et al.~\cite{2021A&A...654A.162S} utilized PCA to construct multivariate  \AU  relations for magnetized neutron stars. Here, we will investigate how we can apply PCA in general to identify potential  \AU  relations, and evaluate how well this approach performs on our own data.

\subsection{Finding Multivariate Correlation using PCA}
The general idea behind PCA is to identify the principal directions in which a given set of data varies the most and use this knowledge to reduce the dimensionality of our data: the principal components are independent vectors that give the direction of maximal variance within our data and allow us to construct an alternative, potentially lower dimensional vector space in which we can still express the majority of the information contained within our data.
Through this process, we can get rid of collinearities in our features, or even identify such features that do not cause any notable variance at all.

While the general use case of PCA does not directly match our goal of constructing  \AU  relations, we can make use of the properties of the principal components to potentially find multivariate  \AU  relations: after computing all principal components of our data, we identify those that show a proportionally large contribution by our target feature $F$ (i.e. what is typically called the \emph{loading} if $F$ within the principal component), if any such component exists. Usually, if there are no strong correlations within our data that lead to a large variance for $F$, all principal components will have a comparatively small contribution by $F$. However, in the case of a principal component that has a large contribution by $F$, we might be able to leverage it to construct a  \AU  relation: by projecting the considered features onto the identified principal component and solving for $F$, we potentially obtain a first-order multivariate \AU relations. 

\subsection{General Methodology}\label{sec:multivariate_methodlogy}
We now describe the general methodology we follow for finding multivariate \AU relations using PCA.
\begin{enumerate}
    \item Select a number of explanatory variables $F_1, \ldots, F_n$ and a target feature $F$.
    \item Perform PCA on the feature set 
    \begin{equation}
    \mathbf{F} = \{F_1, \ldots, F_n, F\}\,.
    \end{equation}
    %\item Find principal component $\mathbf{A}$ that explains variance in target feature $F$. This can be done by investigating the loadings of each feature within the principal components and finding one that has a comparatively large loading for $F$.
    \item For each principal component, solve the equation 
    \begin{equation}
    \begin{split}
       &\mathbf{A} \cdot \mathbf{F} = 0 \\
       \Leftrightarrow\qquad &F = \hat a_1 F_1 + \ldots + \hat a_n F_n
    \end{split}
    \label{eq:proj}
    \end{equation}
    where we denote the right hand side as the new \emph{combined feature} $\hat F$
    \begin{equation}
    \hat F = \hat a_1 F_1 + \ldots + \hat a_n F_n \,.
    \label{eq:combined}
    \end{equation}
    \item Evaluate whether there exists a strong correlation between $F$ and a combined feature $\hat F$ using bivariate correlation analysis.
    \item If strong correlation is found, choose a suitable model and fit it for the relation between $F$ and $\hat F$.
\end{enumerate}
In contrast to the bivariate case, this approach cannot be fully automated yet. A lot of guesswork is involved in identifying the principal components from which we can derive suitable combined features. The most straightforward approach for this task is to simply construct the combined feature for all principal components and then perform a bivariate correlation analysis of the target feature $F$ with each found combined feature.

Also, this method will not always yield universal relations: sometimes, there will be no principal component that will suitably explain the variance in the target feature $F$. This might happen in cases where a) $F$ simply does not present much variance across the whole data set, or b) there exist many co-linearities within the selected set of features $\mathbf{F}$. We discuss some cases where the method described above does not yield a \AU relation in Appendices~\ref{app:multivariate_special} and~\ref{app:multivariate_counter}.
