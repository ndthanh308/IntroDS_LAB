% Created 2023-04-03 Mon 13:13 Intended LaTeX compiler: xelatex
\documentclass[DIV=13]{scrartcl}
\usepackage[sb]{libertine} % or \usepackage[sb]{libertinus}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage[varqu,varl]{zi4}% inconsolata for mono, not LibertineMono
\usepackage[amsthm,upint]{libertinust1math} % slanted integrals, by default
\usepackage[scr=boondoxo,bb=boondox]{mathalpha} %Omit bb=boondox for default
\usepackage{bm}
\usepackage{mathtools}
\usepackage{todonotes}
\usepackage{booktabs}

\usepackage{placeins}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{bm}
\usepackage{biblatex}
\AtEveryBibitem{%
  \clearfield{urlyear}%
  \clearfield{urldate}%
}%
\AtEveryCitekey{%
  \clearfield{urlyear}%
  \clearfield{urldate}%
}%
\usepackage{xcolor}
\usepackage{float}
\RequirePackage{siunitx}
\RequirePackage[algoruled,norelsize, linesnumbered, lined, commentsnumbered]{algorithm2e}
\newcommand{\blue}[1]{{\color{blue} \noindent #1}}
\usepackage{mycommands}
\usepackage[format=plain,labelfont=bf]{caption}
\addbibresource{paper_biber.bib} % Output of 'biber paper --output_format bibtex'p


\usepackage[pdftex,colorlinks=true,linkcolor={blue!50!black},citecolor={red!50!black},urlcolor=blue]{hyperref}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\fvec}[1]{\bm{#1}}

\definecolor{myblue}{RGB}{0 83 139}
\definecolor{myred}{RGB}{114 16 69}
\definecolor{mygreen}{RGB}{0 94 0}
\newcommand{\bestf}[1]{{\color{myblue}\bfseries \noindent #1}}
\newcommand{\bests}[1]{{\color{mygreen}\bfseries \noindent #1}}
\newcommand{\bestt}[1]{{\color{myred}\bfseries \noindent #1}}
\newcommand\restrict[1]{\raisebox{-.5ex}{$|$}_{#1}}

\begin{document}
\date{}
\title{DNN-MG: A Hybrid Neural Network/Finite Element Method with Applications to 3D Simulations of the Navier-Stokes Equations}
\author{Nils Margenberg\thanks{Corresponding author}
  \thanks{Helmut Schmidt University,
    Faculty of Mechanical and Civil Engineering,
    Holstenhofweg 85,
    22043 Hamburg,
    Germany,
    \href{mailto:margenbn@hsu-hh.de}{\texttt{margenbn@hsu-hh.de}}}
  \and Robert Jendersie\thanks{
    University of Magdeburg,
    Institute for Analysis and Numerics,
    Universit\"atsplatz 2,
    39104 Magdeburg,
    Germany,
    \href{mailto:robert.jendersie@ovgu.de}{\texttt{robert.jendersie@ovgu.de}}}
  \and Christian Lessig\thanks{
    University of Magdeburg,
    Institute for Simulation and Graphics,
    Universit\"atsplatz 2,
    39104 Magdeburg,
    Germany,
    \href{mailto:christian.lessig@ovgu.de}{\texttt{christian.lessig@ovgu.de}}}
  \and Thomas Richter\thanks{
    University of Magdeburg,
    Institute for Analysis and Numerics,
    Universit\"atsplatz 2,
    39104 Magdeburg,
    Germany,
    \href{mailto:thomas.richter@ovgu.de}{\texttt{thomas.richter@ovgu.de}}}
}

\maketitle
\vspace{-7ex}
\begin{abstract}
We extend and analyze the deep neural network multigrid solver (DNN-MG) for the Navier-Stokes equations in three dimensions. The idea of the method is to augment of finite element simulations on coarse grids with fine scale information obtained using deep neural networks.

This network operates locally on small patches of grid elements. The
local approach proves to be highly efficient, since the network can be kept (relatively) small and since it can be applied in parallel on all grid patches. However, the main advantage of the local approach is the inherent good generalizability of the method. Since the network is only ever trained on small sub-areas, it never ``sees'' the global problem and thus does not learn a false bias.

We describe the method with a focus on the interplay between finite element method and deep neural networks. Further, we demonstrate with numerical examples the excellent efficiency of the hybrid approach, which allows us to achieve very high accuracies on coarse grids and thus reduce the computation time by orders of magnitude. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
Accurate flow simulations remain a challenging task. The success of deep neural
networks in machine translation, computer vision, and many other fields on the
other hand, has lead to a growing (and renewed) interest to apply neural
networks to problems in computational science and engineering. The combination
of classical finite element approximation techniques with deep neural networks
adds new aspects to the pure numerics-oriented approaches. At higher Reynolds
numbers, accurate simulations, especially in 3D become increasingly difficult, and
the classical methods reach their limits. Although the finite element method is
highly efficient and established for the discretization of the Navier-Stokes
equations, fundamental problems, such as the resolution of fine structures or a
correct information transport between scales, are still not
sufficiently solved. Even if linear solvers with optimal complexity
(such as multigrid methods) are used, nonlinear equations such as the
Navier-Stokes equations show more and more local features under mesh
refinement and these turn out to be a hurdle for the approximation.

We usually consider finite elements as a hierarchical method, where natural hierarchies of finite element meshes and spaces can be constructed (adaptively) to yield approximations of increasing accuracy. This hierarchical setup is then used within multigrid methods for efficient solution of the algebraic system. Here, we connect such a hierarchic finite element approach with neural networks in a hybrid setting: while coarse meshes are handled in the traditional finite element multigrid way, updates in finer meshes are learned by the neural network. They are hence used whenever a full resolution of the effects does not seem possible or efficient.

We label this approach the Deep Neural Network Multigrid Solver (DNN-MG) as it is based on hierarchies of meshes and functions spaces and combines the tools of multigrid solvers with deep neural networks (cf.\ Figure~\ref{fig:g-abstract}). However, the neural networks are used to predict updates to the nonlinear problem and hence, the approach can be used for an upsampling of any kind of finite element (also of finite volume or finite difference) solutions towards a representation on a finer mesh.

% Figure environment removed

We demonstrate the efficiency, generalizability, and scalability of our proposed
approach using 3D simulations. We first train the neural network on data from
the classical flow with one circular obstacle. To analyze the generalization
capability of DNN-MG, we test the network on channel flows with one or two
obstacles at different Reynolds numbers. The obstacles have an elliptical
cross-section with varying eccentricities. The obtained solutions as well as the
lift and drag functionals demonstrate that DNN-MG obtains considerably better
accuracy than a coarse multigrid solution on mesh-level $L$ while taking less
than $3\%$ of the computation time required by a full solution on $L+2$ levels.
Therefore, DNN-MG offers a speedup by a factor of \(35\). DNN-MG's efficiency is
evident, as it requires only double the time of a coarse mesh solution on level
$L$, yet offers substantial improvements in the overall solution quality.

The paper is organized as follows. In the Section~\ref{sec:related-work}, we review related work
on using neural networks for the simulation of partial differential equations.
Sections~\ref{sec:num-meth} provides a recap of the Navier-Stokes equations' solution using the
geometric multigrid method, while Section~\ref{sec:nn} introduces the neural networks used
later in our numerical experiments. In Section~\ref{sec:dnnmg}, we present the deep neural
network multigrid solver, discussing its design in a general form, which makes
it applicable to other problems.
Finally, we present our numerical results in Section~\ref{sec:num-example}.

\section{Related works}
\label{sec:related-work}
We discuss different approaches to the simulation of physical systems using deep
neural networks.

% Maybe remove self citation margenbergOptimalDirichletBoundary2023 above?
The most direct approach to utilize (deep) neural networks for the solution of partial differential equations is to represent the solution of the PDE as such a network. In~\cite{lagarisArtificialNeuralNetworks1998} already, a least squares formulation has been employed to minimize the residual in the training process. The idea has recently been used again in the Deep Ritz Method~\cite{eDeepRitzMethod2018} and a variety of similar approaches have emerged in the last years and coined Physics Informed Neural Network (PINNs) in the literature~\cite{luDeepXDEDeepLearning2021}. For a comprehensive review of PINN and related approaches, we refer to the comprehensive review~\cite{cuomoScientificMachineLearning2022}. Such approaches use an existing partial differential equation and can be considered data free since no explicit training data is required. They are, however, static in that the solution to one specific problem instance is learned. PINNs show superiority for instance in very high dimensional problems~\cite{luPrioriGeneralizationAnalysis2021} but they do not reach the efficiency of established methods when standard problems, e.\,g.\ from elasticity or fluid dynamics, are considered (cf.~\cite{grossmannCanPhysicsInformedNeural2023} for case studies on the Poisson, Allen-Cahn and SchrÃ¶dinger equation). The idea of PINNs has also been extended to learning solution operators~\cite{luLearningNonlinearOperators2021,chenUniversalApproximationNonlinear1995,kutyniok2022atheoretical}. The main advantage of this approach is that once the solution operator is trained, it can be applied to other settings. This enables the application to inverse problems~\cite{molinaroNeuralInverseOperators2023,kaltenbachSemisupervisedInvertibleNeural2023,caoResidualbasedErrorCorrection2023} or optimal control~\cite{margenbergOptimalDirichletBoundary2023}. Inspired by classical time stepping schemes, another approach to a neural network-based simulation of (evolutionary) partial differential equations is to consider these as time series and predict the next state of the system based on the previous ones. With this approach, network architecture for sequential data can be used, in particular recurrent neural networks and their extensions such as Long-Short-Term-Memory units (LSTM)~\cite{hochreiterLongShorttermMemory1997} and Gated Recurrent Units (GRUs)~\cite{choLearningPhraseRepresentations2014} that we also used for the DNN-MG Navier-Stokes solver~\cite{margenbergStructurePreservationDeep2021,margenbergNeuralNetworkMultigrid2022}.

At the other end of the spectrum are approaches that are purely data-based. For example, in~\cite{eichingerStationaryFlowPredictions2021} flow fields are learned in convolutional neural networks based on the flow geometry and using techniques from image processing. Recent work~\cite{pathakFourCastNetGlobalDatadriven2022,biPanguWeather3DHighResolution2022,lamGraphCastLearningSkillful2022} uses purely data-driven neural network models trained on historical weather measurements and these perform on par with one of the most sophisticated weather forecasting models based on partial differential equations. The authors of~\cite{genevaTransformersModelingPhysical2022} use transformer models for the prediction of dynamical systems. In the context of dynamical systems, the combination of Data Assimilation, Uncertainty Quantification, and Machine Learning techniques has gained significant interest, we refer to~\cite{chengMachineLearningData2023} for an in-depth review. For a recent overview on different neural network based approaches to approximate partial differential equations and related inverse problems we refer to~\cite{Tanyu2023}. 

One of the central open questions in the current literature is
to what extent and how physical constraints or existing knowledge
about the dynamics should be used for the training of a neural
network, see also~\cite{ghattasLearningPhysicsbasedModels2021}. The Deep Neural Network Multigrid Solver (DNN-MG) is a hybrid approach that combined the model based finite element method with deep neural networks that also integrate model knowledge. DNN-MG is not a PINN since we do not represent the solution by only aim for fine scale updates. Also, the general setup of DNN-MG aims for generalization, and it is closer to learning a numerical solver than to learning a specific solution.

The close incorporation of the deep neural network into the finite element method, e.\,g.\ that we learn fine-scale solution coefficients, gives rise to analytical tools that can be used in first steps of error estimation~\cite{minakowskiPrioriPosterioriError2023}.


In our first work on DNN-MG~\cite{margenbergNeuralNetworkMultigrid2022} we introduced a method which
combines a geometric multigrid method with ANNs but only considered 2D
simulations. We also adapted the method In 2D to ensure divergence-freedom by
construction through a network that predicts the streamfunction~\cite{margenbergStructurePreservationDeep2021}. In later studies we showed that
the corrections added by the neural network improve the guesses in the Newton
method, which leads to a reduction in wall-time compared to the coarse grid
solutions~\cite{margenbergDeepNeuralNetworks2021}. Similar to the ideas of
DNN-MG, the authors of~\cite{delaraAcceleratingHighOrder2022,manriquedelaraAcceleratingHighOrder2023} use numerical
solutions of higher order to train a neural network which generates corrective
forcing terms.
Similarly, in~\cite{fabraFiniteElementApproximation2022}, low-order simulations
of wave equations
are enhanced using neural networks to provide corrective forcing. This approach
includes improvements in the temporal resolution of the simulations.
Learning corrective forcing terms shares similarities with the approach of
DNN-MG, as both methods introduce corrections to the solution through the
right-hand side, which can be seen as corrective forcing terms. However, DNN-MG
goes further by directly correcting the solution, potentially providing
advantages in computing goal quantities like drag and lift.
All these
methods have a resemblance to reduced order models (ROMs). Along these lines,
in~\cite{baigesFiniteElementReducedorder2020}, a ROM based on adaptive finite
elements and correction terms combined with Artificial Neural Networks (ANNs) is
presented.
In~\cite{kharaNeuFENetNeuralFinite2021} the authors developed a
mesh based approach to ANN based solvers, leveraging existing FEM theory. The
authors of~\cite{brevisNeuralControlDiscrete2022} propose a neural framework
for optimizing weak formulations in FEM.\@ An ANN acting as control variable is
included in the weak form and the minimization of a cost function yields
desirable approximations where known data or stabilization mechanisms can be
easily incorporated. Ainsworth and Dong propose a framework for adaptively
generating basis functions, each represented by a neural network with a single
hidden layer, which may be used in a standard Galerkin method~\cite{ainsworthGalerkinNeuralNetworks2021}.
In~\cite{mituschHybridFEMNNModels2021} the authors augment PDEs by ANNs to represent
unknown terms. The hybrid model is then discretized using FEM.\@ 

The MgNet framework introduced in~\cite{heMgNetUnifiedFramework2019} draws
connections from convolutional neural networks (CNNs) to Multigrid methods and thereby improve the design and
understanding of CNNs. The authors of~\cite{luzLearningAlgebraicMultigrid2020}
propose a framework for unsupervised learning of Algebraic Multigrid (AMG)
prolongation operators for linear systems which are defined directly on graphs.
In the Multigrid framework, selecting an appropriate smoother is crucial to
ensure convergence and efficiency, but the optimal choice of a smoother is
problem dependent and is in general a major challenge for many applications.
In~\cite{huangLearningOptimalMultigrid2023}, the authors optimize the smoothers
in Multigrid methods, which they form by CNNs.

\section{Numerical Methods}
\label{sec:num-meth}
We solve model problems of incompressible viscous flow around a cylinder in 2D
and 3D domains. This leads to the solution of the Navier-Stokes equations. Our
approach involves a geometric multigrid method with a cell-based Vanka-smoother
as a preconditioner to a Newton-Krylov method.

We consider a domain \(\Omega\in \R^d\) with \(d\in \{2,\,3\}\) and
Lipschitz-continuous boundary \(\Gamma\) and a bounded time interval
\([0,\,T]\). For solving the instationary Navier-Stokes equations we seek the
velocity \(\vec v\colon [0,\,T]\times \Omega \to \R^d\) and pressure
\(p\colon [0,\,T]\times \Omega \to \R\), such that
\begin{equation}
  \begin{alignedat}{2}\label{eq:nsstrong}
    \partial_t \vec v + (\vec v\cdot \nabla)\vec v - \nu\Delta \vec v
    +\nabla p &= f \quad &&\text{on } [0,\,T] \times \Omega\\
    \nabla \cdot \vec v &= 0 \quad &&\text{on } [0,\,T] \times \Omega,
  \end{alignedat}
\end{equation}
where \(\nu>0\) is the kinematic viscosity and \(f\) an external force. The
initial and boundary conditions are given by
\begin{equation}\label{eq:boundary}
  \begin{alignedat}{2}
    \vec v(0,\,\cdot) &= \vec v_0(\cdot)\quad &&\text{on }\Omega\\
    \vec v &= \vec v^D \quad &&\text{on } [0,\,T] \times \Gamma^D\\
    \nu(\vec n\cdot\nabla)\vec v - p\vec n &=0 \quad &&\text{in } [0,\,T] \times \Gamma^N,
  \end{alignedat}
\end{equation}
where \(\vec n\) denotes the outward facing unit normal vector on the boundary
\(\partial\Omega\) of the domain. The boundary
\(\Gamma = \Gamma^D \cup \Gamma^N\) is the union of \(\Gamma^D\) with Dirichlet
boundary conditions and \(\Gamma^N\) with Neumann type conditions.

\subsection{Notation and variational formulation of the Navier-Stokes equations}
\label{sec:variational}
By \(L^2(\Omega)\) we denote the space of square integrable functions on the
domain \(\Omega\subset\mathbb{R}^d\) with scalar product \((\cdot,\cdot)\) and
by \(H^1(\Omega)\) those \(L^2(\Omega)\) functions with weak first derivative in
\(L^2(\Omega)\). The function spaces for the velocity and pressure are then
\begin{equation}\label{eq:VQ}
  \begin{alignedat}{1}
     \vec V &\coloneqq \vec v^D + H^1_0(\Omega;\Gamma^D)^d,\quad H_0^1 (\Omega;\Gamma^D)^d\coloneqq \brc{\vec v\in H^1(\Omega)^d\colon \vec v=0 \text{  on } \Gamma^D}\\
    L &\coloneqq \brc{p\in L^2(\Omega),\text{ and, if }\Gamma^N=\emptyset,\; \int_{\Omega}p \drv x = 0},
  \end{alignedat}
\end{equation}
where \(\vec v^D\in H^1(\Omega)^d\) is an extension of the Dirichlet data on
\(\Gamma^D\) into the domain. We normalize the pressure to yield uniqueness, if
Dirichlet data is given on the whole boundary. With these spaces, the
variational formulation of~\eqref{eq:nsstrong} is given by
\begin{equation}
\begin{alignedat}{2}\label{eq:ns}
  (\partial_t \vec v,\,\fvec \phi) + (\vec v\cdot \nabla \vec v,\, \fvec \phi) +
  \nu(\nabla \vec v,\, \nabla \fvec\phi) -
  (p,\,\nabla\cdot \fvec\phi)
  &= (\vec f,\,\fvec \phi)\quad&&\forall \fvec \phi \in H^1_0(\Omega;\Gamma^D)^d\\
  (\nabla \cdot \vec v, \, \xi) &= 0\quad &&\forall \xi \in L\\
  \vec v(0,\,\cdot ) &= \vec v_0(\cdot)\quad &&\text{on }\Omega.
\end{alignedat}
\end{equation}

Let \(\Omega_h\) be a quadrilateral or hexahedral finite element mesh of the
domain \(\Omega\) satisfying the usual requirements on the structural and form
regularity such that the standard interpolation results hold,
compare~\cite[Section 4.2]{richterFluidstructureInteractionsModels2017}.
By \(h_T\) we denote the diameter of an element
\(T\in\Omega_h\) and by \(h\) the maximum diameter of all \(T\in\Omega_h\).

\subsection{Semidiscretization in space}
\label{sec:space-disc}
For the finite element discretization of~\eqref{eq:ns} we choose equal-order continuous finite elements of degree two for velocity and pressure on hexahedral meshes. By \(\mathbb{Q}_h^{(r)}\) we denote the space of continuous functions which are polynomials of maximum directional degree \(r\) on each element \(T\in\Omega_h\).
%
Then we define the
discrete trial- and test-spaces for the discretization of~\eqref{eq:ns} as
\(\vec v_h,\,\fvec\psi_h \in \vec V_h = [\mathbb{Q}_h^{(2)}]^d\) and
\(p_h,\,\xi_h \in L_h = \mathbb{Q}_h^{(2)}\). Since the resulting equal order
finite element pair \(\vec V_h\times L_h\) does not fulfill the inf-sup
condition, we add stabilization terms of local projection type~\cite{beckerFiniteElementPressure2001}.
We further add convection stabilization, also based on
local projections~\cite{beckerTwolevelStabilizationScheme2004}. The
resulting semidiscrete variational problem then reads
%
\begin{equation}
  \begin{alignedat}{2}\label{eq:disc:ns}
    (\partial_t \vec v_h,\,\fvec\psi_h) + (\vec v_h\cdot \nabla \vec v_h,\, \fvec\psi_h) +
    \nu (\nabla \vec v_h,\, \nabla \fvec\psi_h) -
    (p_h,\,\nabla\cdot \fvec\psi_h)\qquad&\\
    +\sum_{T\in\Omega_h}(\vec v_h\cdot \nabla \vec v_h - \pi_h \vec v_h\cdot
    \nabla \pi_h \vec v_h,\,\delta_T(\vec v_h\cdot \nabla \fvec \psi_h-\pi_h \vec v_h\cdot
    \nabla \pi_h \fvec \psi_h))
    &= (\vec f,\,\fvec\psi_h)\quad&&\forall \fvec\psi_h\in  \vec V_h
    \\[5pt]
    (\nabla \cdot \vec v_h, \, \xi_h)
    +\sum_{T\in\Omega_h}\alpha_T (\nabla (p_h-\pi_h p_h),\nabla (\xi_h-\pi_h \xi_h))
    &= 0\quad &&\forall \xi_h \in L_h
  \end{alignedat}
\end{equation}
%
Here, we denote by $\pi_h:Q^{(2)}_h\to Q^{(1)}_h$ the interpolation into the space of linear finite elements and by $\alpha_T,\beta_T$ two local stabilization parameters specified in~\eqref{stabparams}.

\subsection{Time discretization}
\label{sec:time-disc}
For temporal discretization, the time interval \([0,\,T]\) is split into discrete time steps of uniform size
\[
0=t_0<t_1<\cdots <t_N=T,\: k = t_n-t_{n-1}.
\]
The generalization to a non-equidistant time discretization is straightforward
and only omitted for ease of presentation. We define
\(\vec v_n\coloneqq \vec v_h(t_n)\) and \(p_n\coloneqq p_h(t_n)\) for the fully
discrete approximation of velocity and pressure at time \(t_n\) and apply the
second order Crank-Nicolson method to~\eqref{eq:disc:ns}, resulting in the
fully discrete problem
\begin{align}
\label{eq:4cranknicholson3}
(\nabla \cdot \vec v_{n}, \, \xi_h)+
\sum_{T\in\Omega_h}\alpha_T (\nabla (p_n-\pi_h p_n),\nabla (\xi_h-\pi_h \xi_h))
&=0 & \forall \xi_h&\in L_h, \nonumber \\[4pt]
%%%
\frac{1}{k}(\vec v_n,\,\fvec\phi_h)\,
+{}\frac{1}{2} (\vec v_n\cdot \nabla \vec v_n,\,\fvec\phi_h)
+\frac{\nu}{2}(\nabla \vec v_n,\,\nabla \fvec\phi_h)
-(p_n,\,\nabla \cdot \fvec\phi_h)\quad& \nonumber \\
\qquad \quad \quad+\sum_{T\in\Omega_h}(\vec v_n\cdot \nabla \vec v_n - \pi_h \vec v_n\cdot
    \nabla \pi_h \vec v_n,\,\delta_T(\vec v_n\cdot \nabla \fvec\phi_h-\pi_h \vec v_n\cdot
    \nabla \pi_h \fvec\phi_h)) \nonumber
    &=\frac{1}{k}(\vec v_{n-1},\, \fvec\phi_h)\\
  +\frac{1}{2}(\vec f_n,\fvec\phi_h)
  +\frac{1}{2}(\vec f_{n-1},\,\fvec\phi_h)
  -\frac{1}{2}(\vec v_{n-1}\cdot\nabla \vec v_{n-1},\,\fvec\phi_h)
  &- \frac{\nu}{2}(\nabla \vec v_{n-1},\,\nabla \fvec\phi_h)
  & \forall \fvec\phi_h&\in \vec V_h.
\end{align}
The right hand side only depends on the velocity \(\vec v_{n-1}\) at the last
time step \(n-1\) and we will denote it as \(\vec b_{n-1}\) in the following.

The stabilization parameters $\alpha_T$ and $\delta_T$ depend on the mesh Peclet number and with two parameters $\alpha_0>0$ and $\delta_0\ge 0$ we define them as
%
\begin{equation}\label{stabparams}
  \alpha_T = \alpha_0 \left(\frac{\nu}{h_T^2}+\frac{\|\vec v_h\|_\infty}{h_T} + \frac{1}{k} \right)^{-1},\quad
  \delta_T = \delta_0 \left(\frac{\nu}{h_T^2}+\frac{\|\vec v_h\|_\infty}{h_T} + \frac{1}{k} \right)^{-1},
\end{equation}
%
see~\cite{braackStabilizedFiniteElement2007} for details.
Introducing the unknown $\vec x_n=(p_n,\,\vec v_n)$, we write a short form of the equations~\eqref{eq:4cranknicholson3} as
\begin{equation}\label{nonlinearshort}
  \begin{aligned}
    {\cal A}_h(\vec x_n) &= F_h,
  \end{aligned}
\end{equation}
where $[{\cal A}_h(\vec x_n)]_i$ and $[F_h]_i$ are the left and right hand sides
of~\eqref{eq:4cranknicholson3} for all test functions
$(\xi_h^i,\,\fvec \phi_h^i)$.

In the presented form the Crank-Nicolson time discretization is sufficiently robust for smooth
initial data \(v_0\); we refer to~\cite{heywoodFiniteElementApproximation1990} for small
modifications with improved robustness and stability.

\subsection{Solution of the algebraic systems}
\label{sec:algebraic}
Discretization in space and time~\eqref{eq:4cranknicholson3} leads to a nonlinear system of equations with a saddle-point character. The nonlinearity problem is solved by Newton iteration, where we usually omit the dependency of the stabilization parameters~\eqref{stabparams} on the velocity when setting up the Jacobian. The Jacobian matrix within the Newton iteration is kept for several iterations and also for consecutive time steps and only re-assembled when the nonlinear convergence rates deteriorate above a certain threshold, usually set to $0.1$ or $0.05$. As outer solver for the linear problems GMRES iteration~\cite{kelleyIterativeMethodsLinear1995b} is employed which is preconditioned with a geometric multigrid solver~\cite{beckerMultigridTechniquesFinite2000}.

To cope with the saddle-point structure of the incompressible Navier-Stokes equations and to allow for efficient shared memory parallelization, a Vanka smoother is used within the geometric multigrid. Therefore, small matrices describing the local problems on each mesh element are inverted in a parallel loop with Jacobi coupling. Using quadratic finite elements on hexahedral meshes these local matrices are of size $108\times 108$. The complete layout of the algebraic solver and its parallelization is described in~\cite{failerNewtonMultigridFramework2020}.



\section{Neural Networks}
\label{sec:nn}
Various neural network architectures are in principle suitable for our approach. Having previously worked with recurrent neural networks designed for sequence prediction, we settled for feedforward neural networks for this work. The simple structure makes them faster to train and to evaluate. Furthermore, we found that with proper regularization, the feedforward neural network described below performs more consistent than the alternatives we tried.
% maybe remove headline since this is the only subsection?
%\subsection{Feedforward Neural Networks}
%\label{sec:fnn}

A general feedforward neural network is a composition of multiple parameterized nonlinear functions defined by
\begin{equation}
F(x) \coloneq F(x;\:\mathcal{W}) = (f_L\circ\cdots\circ f_1)(x;\:\mathcal{W})\,,
\end{equation}
where \(\mathcal{W}\) denotes the set of optimizable (learnable) parameters. The \(i\)-th layer of the prototypical ANN we consider here consists of a weight matrix
\(W_i \in \R^{n_i\times n_{i+1}}\) %a bias \(b_i \in \R^{n_{i+1}}\)
and a nonlinear function \(\sigma\colon \R\to \R\), called the activation function, which is
applied component wise, $\sigma\big(Wx\big)_i = \sigma\big((Wx)_i\big)$.
%\({\sigma}_{i}\colon \mathcal{X}^{i+1}\to \mathcal{X}^{i+1}\).
Furthermore, we employ two techniques common in deep learning to accelerate training and to improve generalization. Batch normalization $N_i \colon \R^{n_{i+1}} \to \R^{n_{i+1}}$, applied before each activation, re-scales each component by
\[
N_i(x) = \frac{x - \mu_i}{s_i} \odot \gamma_i + \beta_i,
\]
where $x \odot y$ is the Hadamard product of two matrices (or tensors) and
$ \mu_i \in \R^{n_{i+1}}$ and $s_i \in \R^{n_{i+1}} $ are respectively, mean and standard deviation of samples estimated during training. The additional parameters $\gamma_i \in \R^{n_{i+1}}$ and $\beta_i \in \R^{n_{i+1}}$ are a learnable, component-wise affine transformation.
Skip connections are inserted where input and output dimensions match, by adding
the inputs of a layer to its outputs, leading to layers $f_i$ of the structure
% \todo{Bias haben wir nicht?} Nils: Haben wir nicht, gerade nochmal gecheckt
\begin{align*}
f_1(x) &= \sigma_1 (N_1( W_1 x)) \\ %\in \mathcal{X}^1 = \mathcal{X}\\
f_i(x) &= \sigma_i (N_i(W_i x)) + x,\quad i = 2,\dots,\,L-1\, \\
f_L(x) &= W_L x.
\end{align*}

\section{The Deep Neural Network Multigrid Method}
\label{sec:dnnmg}
In this section, we review the Deep Neural Network Multigrid (DNN-MG) solver,
that we introduced in~\cite{margenbergNeuralNetworkMultigrid2022}. DNN-MG
leverages a deep neural network to predict the correction of a coarse mesh
solution that has been prolongated onto one or multiple finer mesh levels. The
objective is to obtain solutions that are more accurate than using the coarse
mesh alone, while increasing computational efficiency compared to performing
full multigrid computations on the fine mesh levels. We develop the DNN-MG
solver in a general formulation, while maintaining the connection to the
Navier-Stokes equations, for which we developed DNN-MG.\@ We begin by providing an
overview of the modifications made in DNN-MG compared to the MG method during a
time step of the Navier-Stokes simulation. We continue by describing the
structure, inputs and outputs of the Neural-Network in DNN-MG.\ In particular,
the network is designed to make localized predictions over specific parts of the
simulation domain.

\paragraph{Notation}
In the context of the Geometric Multigrid we always have a hierarchy of meshes
$\Omega_{h}^{0},\dots,\,\Omega_h^{L+J}$, obtained by successive refinement of the
initial mesh $\Omega_h^0$. We denote the finite element space $\vec V_h$ and
$L_h$ introduced in Section~\ref{sec:space-disc} on level
$l\in\brc{0,\dots,\,L+J}$ by $V_h^l$ and $L_h^l$. With their nested structure,
these spaces reflect the grid hierarchy. Then, let $\Omega_{h}^l$ be the quadrilateral or
hexahedral finite element mesh on level $l\in\brc{0,\dots,\,L+J}$ with
$N_{\text{cells}}^{l}= \operatorname{card}(\Omega_{h}^l)$ cells and the index
set $Z_l$ of all global degrees of freedom
with cardinality $N_{\text{dof}}^{l}\coloneq\operatorname{card}(Z_l)$.
This notation and the description in Section~\ref{sec:dnnnet} is inspired by the description of the Vanka smoother in~\cite{anselmannEfficiencyLocalVanka2023}.

\LinesNumbered%
\SetFuncSty{textbf} \SetCommentSty{textsf} \SetKwInOut{Input}{Input} %
\SetKwProg{Function}{function}{}{end} %
\SetKwFunction{MG}{Multigrid}%
\SetKwFunction{rhs}{Rhs}%
\begin{algorithm}[H]
  \Function{DNN-MG}{ %
    \For{all time steps $n$}{%
      \While{not converged}{%
        $\delta z_i =$\label{alg:nkbegin}
        \MG{$L,\,A_{L}^n,\,b_{L}^n,\,\delta z_i$}\tcp*{Solution of the linear
          system (cf.~Section~\ref{sec:algebraic})}%
        $z_{i+1} = z_i + \varepsilon \, \delta z_i$ }%
      \label{alg:nkend}
      \blue{$\tilde{\vec x}_n^{L+J} = \mathcal{P}(\vec x_n^{L}) $}\tcp*{Prolongation on
        level $L+J$}%
      \label{alg:prolong}
      \blue{$\vec r_n^{L+J}=F_h^{L+J}-{\mathcal A}_{h}^{L+J}({\mathcal P}(\vec x_n^{L}))$}\tcp*{Residual
        on level $L+J$ (cf.~\eqref{nonlinearshort})}
      \label{alg:residual}%
      \blue{$\vec d_n^{L+J} =
        \mathcal{L}\circ\mathcal{N}(\mathcal{Q}(\tilde{\vec x}_n^{L+J}),\,\mathcal{Q}
        (\vec r_n^{L+J}),\,\Omega_{h}^{L+J})$}\tcp*{Prediction of velocity
        correction}
      \label{alg:predict}%
      \blue{$b_{n+1}^{L+J} =$
        \rhs{$\tilde{\vec x}_n^{L+J} + \vec d_n^{L+J},f_n,f_{n+1}$}}\tcp*{Set up rhs of~\eqref{eq:4cranknicholson3} for next time step}
      \label{alg:rhs}%
      \blue{$b_{n+1}^{L} = \mathcal{R}(b_{n+1}^{L+J})$}\tcp*{Restriction of rhs
        to level $L$}
      \label{alg:restrictrhs}%
    }%
  }%
  \caption{DNN-MG for the solution of the Navier-Stokes equations. Lines 6-9
    (blue) provide the modifications of the DNN-MG method compared to a classical
    Newton-Krylov simulation with geometric multigrid preconditioning.}\label{alg:dnnmg}
\end{algorithm}

\subsection{Time stepping using DNN-MG}
We present a detailed description of one time step in the simulation of the
Navier-Stokes equations using the DNN-MG solver. The computations involved are
summarized in Algorithm~\ref{alg:dnnmg}.

At the beginning of time step $n$, we solve for the unknown velocity $\vec x_n^{L}$
and pressure $p_n^{L}$ on the coarse level $L$ using the classical Newton-Krylov
simulation (Algorithm~\ref{alg:dnnmg}, lines~\ref{alg:nkbegin}--\ref{alg:nkend})
as described in Section~\ref{sec:algebraic}. Subsequently, we prolongate
$\vec x_n^{L}$ to $\tilde{\vec x}_{n}^{L+J}$ on the next finer level $L+J$
(Algorithm~\ref{alg:dnnmg}, line~\ref{alg:prolong}) where a richer function
space $L_h^{L+J}\times \vec V_h^{L+J}$ is available.

Using the prolongated solution, we calculate the residual on level $L+J$ for the
input to the neural network (Algorithm~\ref{alg:dnnmg},
line~\ref{alg:residual}). The residual is calculated according to~\eqref{nonlinearshort}.
We then use the neural network of the DNN-MG solver to
predict the velocity update $\vec d_n^{L+J}$, which represents the difference
$\vec d^{L+J}_{n} = \bar{\vec x}_n^{L+J}- \tilde{\vec x}_n^{L+J}$ between the prolongated
$\tilde{\vec x}_n^{L+J}$ and the unknown ground truth solution $\bar{\vec x}_n^{L+J}$ on
level $L+J$ (Algorithm~\ref{alg:dnnmg}, line~\ref{alg:predict}). The prediction
is based on $\tilde{\vec x}_{n}^{L+J} \in \vec V_h^{L+J}\times L_h^{L+J}$ and $\vec r_n^{L+J}$ and further
utilizes information about the local mesh structure
$\omega\in\R^{n_{\text{Geo}}}$. This information is extracted from
$\Omega_h^{L+J}$.

To conclude the $n^{\textrm{th}}$ time step, we compute the right-hand side
$b_{n+1}^{L+J}$ of Eq.~\ref{eq:4cranknicholson3} on level $L+J$ using the
corrected velocity $\tilde{\vec v}_n^{L+J} + \vec d_n^{L+J}$ (Algorithm~\ref{alg:dnnmg},
line~\ref{alg:rhs}), and then restrict it to level $L$
(Algorithm~\ref{alg:dnnmg}, line~\ref{alg:restrictrhs}). This right-hand side,
incorporating the neural network-based correction, becomes part of the multigrid
computations in the next time step (Algorithm~\ref{alg:dnnmg},
line~\ref{alg:nkbegin}), thereby influencing the overall time evolution of the
flow. This approach of constructing the right-hand side $b_{n+1}^{L+J}$ on level
$L+J$ and subsequently restricting it is a crucial aspect of the DNN-MG solver.

We note that in the case of the Navier-Stokes equations the pressure is handled
implicitly on level $L$ in the multigrid solve and is not directly receive a
correction by the neural network. However, the pressure is included in the
network's input through the prolongated solution and the residual, as it is an
integral part of the solution and is indirectly influenced through the
corrections to the velocity.

\subsection{The Neural Network of DNN-MG}\label{sec:dnnnet}
% Figure environment removed
The neural network component forms the core of the DNN-MG
solver. It plays a crucial role in enhancing computational efficiency,
facilitating fast training procedures, and ensuring robust generalization across
diverse flow regimes beyond the training set. We achieve this
through the careful design of a compact neural network architecture with a
moderate number of parameters and a localized, patch-based structure.
Figure~\ref{fig:network_patch} provides an overview of the network component,
illustrating the local approach, which we introduce next.

\paragraph{A patch-based neural network}
To ensure computational efficiency, the DNN-MG approach adopts a neural network
that operates on localized patches of the mesh. The network's input and output
are constrained to a patch, which reduces the computational cost compared to a
prediction over the entire domain. This local approach leads to more compact
neural networks, with a smaller number of parameters.

We define a patch $P_{L,\,J}^M\subset\Omega_{h}^{L+J}$ as a collection of cells
$K\in \Omega_{h}^{L+J}$ on level $L+J$. The neural network operates on a single
patch $P_{L,\,J}^M$ and predicts the velocity update
$\vec d_{n,\,P}^{L+J}$ specifically for the degrees of freedom within that
patch. Since a single neural network is used in parallel on each patch, all patches must have the same structure and consist of the same number of fine mesh elements. 
Formally, let
$L(P_{L,\,J}^M)\times \vec V({P_{L,\,J}^M})\subset L_h^{L+J}\times \vec
V_h^{L+J}$ be the function space associated with the degrees of freedom in patch
$P_{L,\,J}^M$. The neural network learns a mapping
\begin{equation}
  \label{eq:NN}
  \begin{alignedat}{1}
\mathcal{N}: \paran{L(P_{L,\,J}^M)\times \vec V(P_{L,\,J}^M)}^2\times \R^{n_{\text{Geo}}} &\to L(P_{L,\,J}^M)\times
    \vec V(P_{L,\,J}^M)\,,\\
    (\tilde{\vec x}_n^{L+J}\restrict{P_{L,\,J}^M},\,\vec r_n^{L+J}\restrict{P_{L,\,J}^M},\,\omega_{P_{L,\,J}^M}) &\mapsto \vec d_{n,\,P}^{L+J}\,,
  \end{alignedat}
\end{equation}
that predicts a local velocity update $\vec d_{n,\,P}^{L+J}\in \vec
V(P_{L,\,J}^M)$ based on local data which is also restricted to the patch $P_{L,\,J}^M$.
This prediction is repeated
independently for each patch in the domain, covering the entire domain
$\Omega_h^{L+J}$. The predictions of shared degrees of freedom in adjacent
patches are averaged to ensure consistency. We now formulate the process we just sketched in
more mathematical terms.

We define a
patch $P_{L,\,J}^M$, as a collection of cells on level $L+J$ that are formed by
$J$ successive refinements of a coarse mesh cell $M\in \Omega_h^{L}$. In other words,
$P_{L,\,J}^M$ comprises the $(2^d)^J$ cells obtained by successive refinement of
the coarse mesh cell $M\in \Omega_h^L$ for $J$ levels and can be defined rigorously by
\begin{equation}
  \label{eq:patch}
  P_{L,\,J}^M \coloneq  \{ K \in \Omega_{h}^{L+J} \mid \mathring{K} \cap M \neq \emptyset,\:M\in \Omega_h^L\}\,.
\end{equation}
Additionally, we define
$\Omega_h^{\text{Patch}}\coloneq \brc{P_{L,\,J}^M\;\text{for}\;M\in \Omega_h^{L} }$.
Note that $\operatorname{card}(\Omega_h^{\text{Patch}})= N_{\text{cells}}^L$. We
denote the set of global degrees of freedom associated with the patch
$P_{L,\,J}^M$ by $Z(P_{L,\,J}^M)\subset Z_{L+J}$. The cardinality of $Z(P_{L,\,J}^M)$ is
denoted by $N_{\text{dof}}^{P}\coloneq \operatorname{card}(Z(P_{L,\,J}^M))$. Further, the
index set $\hat Z(P_{L,\,J}^M) \coloneq\{0,\dots,\,N_{\text{dof}}^{P}-1\}$ contains all local
degrees of freedom on $P_{L,\,J}^M$. For a given patch $P_{L,\,J}^M$ and a local
degree of freedom with index $\hat\iota \in \hat Z(P_{L,\,J}^M)$ the mapping
\begin{equation}
  \operatorname{dof}:\Omega_h^{L+J}\times \hat Z(P_{L,\,J}^M)  \rightarrow Z_l\,,\quad(P_{L,\,J}^M,\,\hat\iota) \mapsto i \,,
\end{equation}
provides the uniquely defined global index $i \in Z_{L+J}$.

We can now define the prediction of the defect by the neural network in precise
terms. It consists of multiple steps:
\begin{enumerate}
\item \emph{Local restriction by}
  $\displaystyle \mathcal Q \colon \R^{N_{\text{dof}}^{L+J}}\to
  \R^{N_{\text{cells}}^L\times N_{\text{dof}}^{P}}$: Using a local restriction
  operator, the globally defined data of size $N_{\text{dof}}^{L+J}$ is
  transformed into local, patchwise defined input of size $N_{\text{dof}}^{P}$
  suitable for the DNN.\@ This is done for each patch, resulting in a matrix of
  size $\R^{N_{\text{cells}}^L\times N_{\text{dof}}^{P}}$.
\item \emph{Patch-wise prediction by the neural network}
  $\displaystyle \mathcal N \colon \R^{(2N_{\text{dof}}^{P} +
    n_{\text{Geo}})}\to \R^{N_{\text{dof}}^{P}}$: For each patch, the neural
  network $\mathcal{N}$ maps the local input data of size
  $(2N_{\text{dof}}^{P} + n_{\text{Geo}})$ to a local output of size
  $N_{\text{dof}}^{P}$, predicting the velocity update for the degrees of
  freedom within the patch $P_{L,,J}^M$.
\item \emph{Global extension by}
  $\displaystyle \mathcal L \colon \R^{N_{\text{cells}}^L \times
    N_{\text{dof}}^{P}} \to \R^{N_{\text{dof}}^{L+J}}$: The locally defined
  output of the DNN is processed by the extension operator $\mathcal{L}$, which
  maps the local data of size $N_{\text{cells}}^L \times N_{\text{dof}}^{P}$ to
  a single globally defined defect vector of size $N_{\text{dof}}^{L+J}$.
\end{enumerate}
In order to define $\mathcal Q$, we need the $P_{L,\,J}^M$-local restriction
$\vec Q\colon\R^{N_{\text{dof}}^{L+J}}\times\Omega_h^{\text{Patch}} \to \R^{N_{\text{dof}}^{P}}$ defined by
\begin{equation}
  \label{Eq:DefR}
  (\vec Q (\vec x,\, P_{L,\,J}^M))[\hat\iota] = \vec x[\operatorname{dof}(P_{L,\,J}^M,\hat\iota)]\,, \quad \text{for} \; \hat\iota \in \hat Z(P_{L,\,J}^M)\,.
\end{equation}
Then, we define $\mathcal Q$ such that it maps a global vector
$\vec x \in \R^{N_{\text{dof}}^{L+J}}$ to a matrix containing local
restrictions to a patch for all $P_0,\dots,\,P_{N_{\text{cells}}^{L}-1}\in\Omega_h^{\text{Patch}}$, stacked on top of each other:
\begin{equation}
  \label{eq:defQ}
  \mathcal Q\colon \R^{N_{\text{dof}}^{L+J}} \to \R^{N_{\text{cells}}^{L}\times N_{\text{dof}}^{P}},\quad \vec x \mapsto \paran{\vec Q_{P}(x,P_0),\dots,\,\vec Q_{P}(x,\,P_{N_{\text{cells}}^{L}-1})}^{\top}
\end{equation}
After applying $\mathcal Q$ to the residual $\vec r_n^{L+J}$ and
$\tilde{\vec x}_n^{L+J}$, we obtain the patch-wise, local data required as input
to the DNN.\@ According to~\eqref{eq:defQ}, the local data is stacked along the
first dimension, forming a single batch. The network is then evaluated
independently for each patch, with each row of the input representing a
different patch. The output of the DNN then consists of a batch of the patchwise
defect $\vec d_{n,\,P}^{L+J}$. To obtain the global defect vector
$\vec d_{n}^{L+J}$, we use the extension operator $\mathcal{L}$ to transfer this
batch of local data back to the global domain. To this end, we first introduce a
$P_{L,\,J}^M$-local extension operator
$\vec L\colon\R^{N_{\text{dof}}^{P}}\times\Omega_h^{\text{Patch}} \to
\R^{N_{\text{dof}}^{L+J}}$ defined by
\begin{equation*}
  (\vec L( \vec y,\,P_{L,\,J}^M))[\iota]
  =
  \begin{cases}
    \vec y[{\hat \iota}]\,, & \text{if}\; \exists \hat \iota \in \hat Z(P_{L,\,J}^M)\colon \; \iota = \operatorname{dof}(K,\hat \iota),\: K\in P_{L,\,J}^M\,,  \\[1ex]
    0\,, & \text{if}\; \iota \not\in Z_l(P_{L,\,J}^M)
  \end{cases} \,.
\end{equation*}
We further need a scaling vector $\mu\in \R^{N_{\text{dof}}^{L+J}}$ which contains the
reciprocal of the valence of a degree of freedom, i.\,e.\ the number of patches
a degree of freedom is contained in. Then we define $\mathcal L$ such that it
maps $N_{\text{cells}}^{L}$ local vectors to one global vector as
\begin{equation}
  \label{eq:defL}
  \mathcal L \colon \R^{N_{\text{dof}}^{P}\times N_{\text{cells}}^L} \to \R^{N_{\text{dof}}^{L+J}},\quad \vec y\mapsto \mu\odot\sum_{P_{L,\,J}^M\in\Omega_h^{\text{Patch}}} \vec L(\vec y[M],\,P_{L,\,J}^M)\,.
\end{equation}
For the sake of brevity, we used $M$ not only as an element of $\Omega_h^L$
in~\eqref{eq:defL} but also to index the rows of the collection of local
vectors.

This concludes the description of all operations necessary to integrate the
neural network with its local approach into the multigrid solver. In particular
\begin{equation}
  \label{eq:defect-predict}
\mathcal{L}\circ\mathcal{N}(\mathcal{Q}(\tilde{\vec x}_n^{L+J}),\,\mathcal{Q}
        (\vec r_n^{L+J}),\,\Omega_{h}^{L+J})\colon L_h^{L+J} \times \vec
        V_h^{L+J} \to L_h^{L+J} \times \vec V_h^{L+J}\,,\quad
\tilde{\vec x}_n^{L+J}\mapsto \vec d_n^{L+J}\,,
\end{equation}
is a well-defined operation (cf.~Algorithm~\ref{alg:dnnmg} line~\ref{alg:predict}).
The mapping~\eqref{eq:NN} also fits into this framework and interpreting the
definition~\eqref{Eq:DefR} in a finite element space context, the restrictions
$\tilde{\vec x}_n^{L+J}\restrict{P_{L,\,J}^M}$ and
$\vec r_n^{L+J}\restrict{P_{L,\,J}^M}$ are obtained. The network operates on
each patch independently, which allow us to take advantage of parallelization
through the localized nature of the problem. This shows that the local approach
is a key feature in our method, which enables the network to handle a wide range
of flow scenarios while maintaining computational efficiency, as we will show
later in Section~\ref{sec:num-example}. The rest of this section consists of a
detailed description of the inputs to the neural network and the training
methodology.

\paragraph{Neural network inputs}
The effectiveness of neural network prediction hinges upon the selection
of appropriate network inputs that provide enough information
regarding the coarse flow characteristics at level $L$ and the local patch
geometry, akin to the significance of mesh cell geometry in classical finite
element simulations. A well-informed choice of these inputs holds the potential to
keep the number of parameters in the neural network low, thereby reducing
evaluation time during runtime and minimizing the data and time prerequisites
for training.

In our implementation, we use the following inputs to the neural network:
\begin{itemize}
\item nonlinear residuals
  $\vec r_n^{L+J}\restrict{P_{L,\,J}^M}=\paran{F_h^{L+J}-{\mathcal A}_h^{L+J}(\vec{\tilde x}_n^{L+J})}\restrict{P_{L,\,J}^M}\in \R^{N_{\text{dof}}^{P}}$ of
  the prolongated coarse mesh solution for~\eqref{eq:4cranknicholson3};
\item the velocities and pressure
  $\vec{\tilde x}_n^{L+J}\restrict{P_{L,\,J}^M} \in \R^{N_{\text{dof}}^{P}}$ on mesh level $L+J$;
\item the geometry of the cells, which for the 3D case consist of
  \begin{itemize}
  \item the edge lengths $h^c\in \R^{12}$;
  \item the lengths of the diagonals connecting the vertices which do not share
    the same face $a^{c} \in \R^{4}$;
  \item for each vertex, the average of the 3 angles between the faces
    $\alpha^c\in\R^8$.
  \end{itemize}
\end{itemize}
The nonlinear residual plays a crucial role as it quantifies the local error or
defect at each degree of freedom on level $L+J$ and therefore carries essential
information about the underlying partial differential equation. Furthermore, the
velocity and pressure fields contain details about the current solution and
provide knowledge about the flow within the current patch. It is important to
note that the geometric information of the cells also plays a significant role in
our approach, as we impose no specific restrictions on the cells, except for the
standard shape regularity requirements commonly employed in finite element
methods.

\paragraph{Training of DNN-MG}
The training of DNN-MG is based on simulations of the Navier-Stokes equations
for which a multigrid representation of the velocity $\vec v_n^l$ with two levels $L$
and $L+J$ is available. The velocity $\vec v_n^{L+J}$ thereby serves as ground truth
$\bar{\vec v}_n^{L+J}$. The goal of the training is to optimize the network
parameters such that the norm
$\big\Vert (\tilde{\vec v}_n^{L+J} + \vec d_n^{L+J}) - \bar{\vec v}_n^{L +1 } \big\Vert$
of the difference between the predicted velocity
$\tilde{\vec v}_n^{L+J} + \vec d_n^{L+J}$ (i.\,e.\ the velocity after the correction)
and the ground truth $\bar{\vec v}_n^{L}$ is minimized throughout the simulation.
Typically, the training data for the network comprises only a few snapshots of
simulation data from simple scenarios, as we will see in the next section.

\section{Numerical examples}
\label{sec:num-example}

In the following paragraphs we will document different numerical
simulations. The test cases are based on a well established 3D
Navier-Stokes benchmark
problem~\cite{schferBenchmarkComputationsLaminar1996,braackSolutions3DNavierStokes2006}. While
the neural network is trained on the original benchmark configuration,
we study its performance and generalization on a set of modified
problems. As perturbations we consider increasing or
decreasing the Reynolds number and also a substantial change to the
problem geometry by introducing a second obstacle. It is important to stress that the network is
trained only once and no retraining is applied for the further test cases. 


\subsection{Implementation aspects}
\label{sec:implementation}
The DNN-MG method is implemented mainly using two libraries: The FEM library
Gascoigne3D~\cite{beckerFiniteElementToolkit} and the machine learning library PyTorch~\cite{paszkePyTorchImperativeStyle2019}.
The Newton-Krylov geometric multigrid
implemented in Gascoigne3D method has been shown to be efficient in~\cite{failerNewtonMultigridFramework2020}.

We implement the deep neural networks with \texttt{libtorch}, the \texttt{C++} interface of
\texttt{PyTorch}. Using \texttt{C++} for the
implementation allows us to couple the numerical methods and neural networks in
a performant manner without unnecessary overhead. PyTorch also supports
parallelization with MPI, which is used here to distribute training on multiple
GPUs.




\subsection{The 3D Benchmark Flow Around a Cylinder}
\label{sec:3d-benchmark}

% Figure environment removed



We examine a variant of the three-dimensional flow benchmark presented in~\cite{schferBenchmarkComputationsLaminar1996} in a setting similar to the
\emph{3D-2Z} one, with different Reynolds numbers \(Re \in \{200,240\}\). We
show the geometry in Figure~\ref{fig:3d-1-geo}. While the original benchmark
description considers an obstacle with circular cross-section, we allow
elliptical ones for testing the generalizability of the approach. We define the
Reynolds number as
\begin{equation}\label{reynolds}
  \text{Re} = \frac{\bar{\vec v}\cdot L}{\nu},
\end{equation}
where $\bar{\vec v}$ is the average inflow velocity and $L$ the length of the major
axis of the obstacle. When the cross-section of the cylinder is circular, $L$
represents its diameter. If the cross-section is elliptical,
$L$ corresponds to the height of the obstacle, as the major axis is always
parallel to the $y$-axis.

The time step size is chosen as \(k=\num{0.008}\) on the
interval \(I=(0,8]\). The initial velocity is \(\vec{v}(0) = \vec{0}\), and the
inflow boundary condition is prescribed for \(t>0\), with a smooth startup process.
The flow is driven by a Dirichlet
profile \(\vec v=\vec v^D\) at the left boundary \(\Gamma_{\text{in}}\) given by
\begin{equation}
  \label{eq:inflow}
  \begin{aligned}
    \vec v_{in}^{3d}(t,\,y,\,z)&=\omega(t)\frac{y(H-y)(H^2-z^2)}{(H/2)^2H^2}\frac{9}{8}\bar{\vec v}_{3d} \ \text{  on  } \ \Gamma_{in}\coloneqq
                              0\times [0,\,H],
    \\[4pt]
    \; \omega(t) &= \begin{cases} \frac{1}{2}- \frac{1}{2}\cos( 5\pi t), & t\le \frac{1}{5} \\ 1 & t>\frac{1}{5}.
                    \end{cases}
  \end{aligned}
\end{equation}
\(H=0.41\) is the height and width of the channel and \(\omega(t)\) acts,
as a regularization of the startup phase. On the wall boundary \(\Gamma_{wall}\)
and on the obstacle we use no-slip boundary conditions. On the outflow boundary
\(\Gamma_{\text{out}}\) we use a do-nothing outflow condition~\cite{heywoodArtificialBoundariesFlux1992}.
The average flow rate is \(\bar{\vec
  v}_{3d}=1\) and the viscosity is \(\nu = \num{5.e-4}\). The training data is generated considering the circular obstacle, i.\,e.\ $L=0.1$ which results in the Reynolds number $\text{Re}=200$. For testing, the elliptical cross-section with $L=0.12$ is considered, leading to the slightly increased Reynolds number $\text{Re}=240$. See Section~\ref{sec:dnnnet} for details.

As in the classical benchmark description we analyze the drag and lift coefficients of the obstacle
\begin{equation}\label{functionals}
  J_\text{drag} = \int_\Gamma
  \big( \nu\nabla\vec v\vec n-p\vec n\big)\vec e_x\,\text{d}s,\quad
  J_\text{lift} = \int_\Gamma
  \big( \nu\nabla\vec v\vec n-p\vec n\big)\vec e_y\,\text{d}s.
\end{equation}
To compute the drag and lift coefficients, we use the Babuszka-Miller trick;
cf.~\cite{braackSolutions3DNavierStokes2006,babukaPostprocessingApproachFinite1984}
and rewrite the surface integrals over the volume for obtaining super
convergence of fourth order.


\begin{table}[t]
  \caption{Spatial meshes with increasing number of
    multigrid levels and degrees of freedom (DoF). For the DNN-MG setups we
    indicate the size of the patches (counted as number of fine mesh elements)
    where the network is applied.}
    \label{tab:org62ab3c1}
  \centering
  \begin{tabular}{lcrr}
    \toprule
    & Level & \# DoF& Patch size\\[0pt]
    \midrule
    Coarse & MG(\(3\)) & \(\num{390720}\) & \num{0}\\[0pt]
    Reference for $\text{DNN-MG}(3+1)$ & MG(\(4\)) & \(\num{3003520}\) &\num{8}\\[0pt]
    Reference for $\text{DNN-MG}(3+2)$ & MG(\(5\)) & \(\num{23546112}\)& \num{64}\\[0pt]
    \bottomrule
  \end{tabular}
\end{table}




\subsection{Neural Network parameters}
\label{sec:nn-parameter}
DNN-MG uses a neural network that operates on patches. In our experiments, a
patch is once or twice refined cell on level \(L\). In
Table~\ref{tab:orgef2be82} we list the dimensions and number of trainable
parameters of the resulting neural networks. The hidden size is arbitrary, but a
size of 512 or 750 has been shown to give good results, while keeping the
network size relatively small. We generally use 8 hidden layers of the same
size. Unless mentioned otherwise, the results presented here were obtained with
a hidden size of 750. Networks with a width of 512 are considered only for
comparison.

\begin{table}[t]
  \caption{The input size \(N_{\text{in}}\), hidden size
    \(N_{\text{hidden}}\), output size \(N_{\text{out}}\) and the number of
    trainable parameters of the feedforward network, for different numbers of
    predicted levels.}
    \label{tab:orgef2be82}
  \centering
  \begin{tabular}{rrrrr}
    \toprule
    Predicted levels & \(N_{\text{in}}\) & \(N_{\text{hidden}}\) & \(N_{\text{out}}\) & Trainable parameters\\[0pt]
    \midrule
    1 & 240 & 512 & 81 & \num{2007552}\\[0pt]
    2 & 1024 & 512 & 375 & \num{2559488}\\[0pt]
    1 & 240 & 750 & 81 & \num{4190250}\\[0pt]
    2 & 1024 & 750 & 375 & \num{4998750}\\[0pt]
    \bottomrule
  \end{tabular}
\end{table}


\subsection{Neural Network training}
\label{sec:nn-train}

Training data are generated on grids with three successive levels of refinement, see Table~\ref{tab:org62ab3c1}. Here, we denote by MG(\(3\)) the coarse grid. The grid levels MG(\(4\)) and MG(\(5\)) are the training data for a network prediction over one, respectively over two grid levels.

A single simulation is sufficient since,
by the patch-wise application of the network, a single simulation provides
\(N_c \times N_T\) training items, where \(N_c\) is the number of patches and
\(N_T\) is the length of the time series. From the experience gained in the 2D
case~\cite{margenbergNeuralNetworkMultigrid2022}, we consider a small subinterval of
\(I=[0,\,8]\). Here we use \(t \in [4,\,7]\) resulting in \(N_T=375\). The
interval from \([2,\,4]\) is used as validation set. The full dataset consists
of 1\,TB of data.


The networks were trained using 2 GPU nodes, each with 2 Nvidia A100 GPUs and 2
Intel Xeon Platinum 8360Y CPUs. The number of MPI processes were chosen equal to
the number of GPUs such that one MPI process uses one GPU and CPU.\@ Using this
setup, the training of the feedforward network takes one day, regardless of the
network size. Although the data loading process is implemented efficiently and
the average load of the GPUs ranges from $80\%$ for small networks to $90\%$ for
large networks, the limiting factor in terms of performance is the memory
bandwidth.

As loss function \(\mathcal{L}\) we employed a simple \(l^2\)-loss
% \todo{Quadrat fehlt?} - Check below
\[
  \mathcal{L} = \sum_{n=1}^{N_T} \frac{1}{N_c} \sum_{c=1}^{N_c}
  l^2(\tilde{v}_{n,c}^{L+J} + d_{n,c}^{L+J} ,\,\bar {v}_{n,c}^{L+J})^2 =
  \sum_{i=1}^{N_T} \frac{1}{N_c} \sum_{c=1}^{N_c} \norm{ \big(
    \tilde{v}_{n,c}^{L+J} + d_{n,c}^{L+J} \big) - \bar{v}_{n,c}^{L+J}
  }_2^2,\quad J\in \{1,\,2\} .
\]
The parameter $J$ is the number of levels we skip with the prediction by the DNN.\@
We use a Tikhonov-regularization with a scaling factor \(\alpha=10^{-5}\). To
optimize our model, we utilize the AdamW~\cite{loshchilovDecoupledWeightDecay2019} optimizer with a
maximum of 1000 epochs. Finally, we select the model with the lowest validation
loss. The convergence of the training for the different network
configurations is reported in Figure~\ref{fig:so-losses}.

% Figure environment removed

\subsection{Flow prediction}
\label{sec:flow-pred}
For testing we use the flow around an elliptic obstacle with an increased height
of \(0.12\) compared to the training data (see Figure~\ref{fig:3d-1-geo}). The resulting finite element meshes have the same structure as in the training case, but the elements are distorted. We
observe that DNN-MG is indeed able to predict high frequency fluctuations that
are not visible in the coarse mesh solution. In particular in the vicinity of
the obstacle the quality of the solution is strongly enhanced with distinct
features in the wake being apparent in the DNN-MG simulation.

In addition to a viscosity of \(\nu=\num{5.e-4}\) and \(Re=240\), we
consider fluids with \(\nu=\num{4e-4}\) corresponding to a Reynolds number of
\(Re=300\) and \(\nu=\num{6.67e-4}\) which corresponds to a Reynolds number of
180. Both of these Reynolds numbers are calculated for the elliptic obstacle
with a height of \(0.12\), which is our typical test case. The network
is trained only for \(Re=200\) using the obstacle with circular cross-section.

\paragraph*{Channel flow at $\text{Re}=\boldsymbol{180}$}
In Figure~\ref{fig:dl-3d-1-re180} and Table~\ref{tab:dl-3d-1-re180}, the drag
and lift forces at Reynolds number 180, computed using DNN-MG and the classical MG
method on different levels are presented. The results show the predictive
capacity of the DNN-MG approach in reproducing the dynamic behavior of the flow.
In terms of the drag coefficient, DNN-MG yields a slight improvement in
terms of the magnitude. The lift coefficients on the other hand improve greatly compared to the
coarse solution, as the overall flow dynamics are well reconstructed,
albeit with temporal deviations.  We observe this in
Figure~\ref{fig:fields-1obs-re240} too, where different flow patters emerge in
the wake of the flow. Furthermore, with respect to the minimum and maximum
lift values, our approach is in good agreement with the reference solution.
Experience with finite element simulations usually show that the lift is the functional that is harder to approximate by far~\cite{braackSolutions3DNavierStokes2006}.

\paragraph*{Channel flow at $\text{Re}=\boldsymbol{240}$}
Similarly, Figure~\ref{fig:dl-3d-1-re240} and Table~\ref{tab:dl-3d-1-re240}
depict the drag and lift forces at Reynolds number 240, obtained through DNN-MG
and the multigrid solution on the coarse and reference level.
Figure~\ref{fig:fields-1obs-re240} show a comparison of the velocity fields
obtained using DNN-MG with the coarse mesh and reference solution. The local
corrections improve the solution at a global level and DNN-MG is clearly able to
capture and improve the flow characteristics. In particular, it has the capacity
to significantly enhance the flow resolution, yielding improved solutions and
reproducing intricate features that were previously unobservable at lower levels.
This underscores the method's efficacy in capturing fine-scale details. We see
similar behavior as in $\text{Re}=240$, indicating good generalization of DNN-MG
to different Reynolds numbers, considering the network was trained on
$\text{Re}=200$.

\paragraph*{Channel flow at $\text{Re}=\boldsymbol{300}$}
For Reynolds number 300, Figure~\ref{fig:dl-3d-1-re300} and
Table~\ref{tab:dl-3d-1-re300} present the drag and lift forces predicted by
DNN-MG.\@ Remarkably, the good results in terms of drag and lift as well as
improvement of the velocity field we found for the drag and lift coefficients at
$\text{Re}=180$ and $\text{Re}=240$ extend to significantly higher Reynolds
numbers. DNN-MG shows good robustness with respect to a generalization to higher
and lower Reynolds numbers than it was trained on. At $\text{Re}=300$, the
improvement in terms of the minima and maxima of the drag and lift coefficients
are even better than the results for lower Reynolds numbers and are often the
closest results to the reference solution. Analyzing 
Tables~\ref{tab:dl-3d-1-re180}, \ref{tab:dl-3d-1-re240}
and~\ref{tab:dl-3d-1-re300}, we see that the results of DNN-MG mostly outperform
even the MG(4) solution in terms of the drag and lift coefficients.

In Figure~\ref{fig:vp-errors-1obs} we present the presenting the velocity and
pressure error. The results show good improvement in terms of the accuracy of
DNN-MG over the coarse mesh solutions. Further, DNN-MG even outperforms the
geometric multigrid level at the intermediate level 4, between coarse and
reference level.

% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed
% Figure environment removed
\subsection{Performance measurements}
\label{sec:perf-measures}
% Figure environment removed
For the DNN-MG method to be advantageous, it must achieve two goals: Firstly, it
must enhance the accuracy compared to the solution on a coarse mesh. Secondly,
it must reduce the computation time compared to a solution on a fine mesh.
However, the Newton-Krylov geometric multigrid method already is of optimal
complexity~\cite{kimmritzParallelMultigrid2011,failerNewtonMultigridFramework2020}. In~\cite{margenbergNeuralNetworkMultigrid2022} this already raised
the question what advantage DNN-MG offers, and we addressed it there for the 2D
case. Here we adapt the theoretical findings to the 3D case.

\paragraph{Complexity analysis}
The Newton-Krylov
geometric multigrid method has linear complexity \(\mathcal{O}(N)\) in the
number of DoFs \(N\). For each global mesh refinement the number of DoFs
increase by a factor of 8, i.\,e.\ \(N^{L+1}\approx 8N^L\). The constant hidden in
\(\mathcal{O}(N)\), however, can be very significant, since on average 5 Newton
steps are required in each time step and within each Newton step one has to
compute on average 12 GMRES steps with one sweep of the geometric multigrid
solver as preconditioning. In addition, the geometric multigrid method usually
involves six pre-smoothing and six post-smoothing steps. Thus, on each mesh
level \(L,\,L-1,\,\dots,\,0\), a total of approximately 700 Vanka smoothing
steps must be performed. One Vanka step thereby requires the inversion of about
\(\mathcal{O}(N^L/8)\) small matrices of size \(108\times 108\), one on each
element of the mesh, resulting in approximately \(108^3\approx \num{1259712}\)
operations. Since the complexity of all mesh levels sums up to about
\(N^L + N^{L-1}+\dots N_0 \approx N^L(1+8^{-1}+\cdots + 8^{-L})\approx
\frac{8}{7}N^L\) we can estimate the effort for the complete solution process on
level \(L\) as
\(5\cdot 12\cdot (6+6)\cdot 108^3\cdot \frac{8}{7}\approx 10^9 N^L\). We thereby
only counted the effort for smoothing and neglected all further matrix, vector
and scalar products. If we were to solve the problem on mesh level \(L+1\), the
required effort would increase by a factor of 8 to approximately
\(\approx 8\cdot 10^9 N^L\) operations.

On the other hand, the DNN-MG method only requires the prolongation of the
solution to the next finer mesh and the evaluation of the neural network on each
patch. If we again consider patches of minimal size (one patch is one element of
mesh layer \(L+1\)) about \(\mathcal{O}(N^{L+1}/8)=\mathcal{O}(N^L)\) patches
must be processed. The effort for the evaluation of the network can be estimated
by the number of trainable parameters with \(N_c\) inputs. The DNN-MG approach
requires only one evaluation of the network in each time step. The number of
trainable parameters is for all network models of the order of single digit
millions. Hence, the effort for correcting the level \(L\) Newton-Krylov
solution by the neural network on level \(L+1\) can be estimated by
\(10^{7} N^L\), which is almost a factor of thousand lower than number of FLOPS
needed to solve on \(L+1\) (\(\approx 8\cdot10^9 N^L\)).

For the prediction of two levels, the estimate for the computational cost of
DNN-MG remains largely unchanged. While the size of the patches and the inputs
and outputs of the networks increase, the number of patches and network
evaluations don't change. Surprisingly, we don't need to significantly increase
the network size and therefore give an upper bound of \(2\cdot 10^{7} N^L\)
trainable parameters. This is even more favorable than the prediction of a
single mesh level, as the numerical solution on level \(L+2\) incurs a much
higher cost of approximately \(64\cdot10^9 N^L\).

\paragraph{Computational performance of DNN-MG}
In Figure~\ref{fig:perf}\,(a) the runtimes of DNN-MG and a purely numerical MG
solution are plotted. While for the MG solution with each added mesh level, the
wall time increases by a factor of 8, the wall time of DNN-MG increases at most
by a factor of \(1.5\). Note that the wall time of $\text{DNN-MG}(3+2)$ and
$\text{MG}(5)$ thereby differ by a factor of \(35\). Although this has to be
weighed against the loss in accuracy, DNN-MG is clearly capable of increasing
the computational efficiency. Moreover, $\text{DNN-MG}(3+2)$ consistently
outperforms MG(4) in terms of the accuracy in previous tests. Remarkably, it
still has an advantage in terms of the runtime which once more underlines the
computational efficiency of DNN-MG.

As can be seen in Figure~\ref{fig:perf}\,(b), the time spent in the solver in
DNN-MG is constant, which shrinks the share of the solver in the wall time while
other parts of the program gain weight. The main increase in computation time is
due to the increased cost of the assembly of the right hand side and the
evaluation of the functionals, since these calculations are done on the higher
levels. Note that these operations are also of order \(O(N)\), but with a much
smaller constant than other operations in the program, e.\,g.\ the operations
mentioned above in our theoretical considerations. This makes the DNN-MG method
highly efficient for 1 or 2 level predictions. More levels are not feasible at
this point due to limitations in the underlying FEM library. Further
investigations in the scalability of the method when predicting more mesh-levels
is part of future work. Predicting weights of higher order polynomials instead
of the weights of basis functions on refined meshes could further reduce the
computational burden and increase the computational efficiency.

The evaluation of the DNN takes less than $3\%$ of the wall time, which is less
than our previous results~\cite{margenbergNeuralNetworkMultigrid2022} in 2D.
This is due to substantial improvements of our implementation: collecting the
input for the neural network and processing the output is parallelized and the
evaluation of the network is done on a GPU.\@ Therefore, DNN-MG not only reduces
the cost of accurate simulations, but it is also well-suited for heterogeneous
computing environments.

\subsection{Generalizability}
\label{sec:generalize}
In order to ensure the practicality of the DNN-MG method, it must generalize
well to similar flows beyond those seen during training. Previous results on the
3D benchmark demonstrate the network's ability to do so under small
geometric perturbations and varying Reynolds number. Here, we evaluate the
network's performance under more substantial changes considering a channel with two
obstacles shown in Figure~\ref{fig:2obs-geo}. As for the single obstacle case,
we test the performance under varying Reynolds numbers with a viscosity of
\(\nu=\num{5.e-4}\) and \(Re=240\), \(\nu=\num{4e-4}\) (\(Re=300\)) and
\(\nu=\num{6.67e-4}\) (\(Re=300\)). We adopt some settings from the single
obstacle case. For instance, the boundary conditions, in particular the inflow
at the left boundary \(\Gamma_{\text{in}}\) given by~\eqref{eq:inflow}, the
initial velocity \(\vec{v}(0) = \vec{0}\) and we chose the time step size
\(k=\num{0.008}\) on the interval \(I=(0,8]\).

  % Figure environment removed
  
\begin{table}[t]
  \caption{Spatial meshes with increasing number of
    multigrid levels and unknowns.}
    \label{tab:org6b4401b}
  \centering
  \begin{tabular}{llrr}
    \toprule
                              & Level & \# DoF             & Patch size \\[0pt]
    \midrule
                       Coarse & MG(\(3\)) & \(\num{533148}\)   & 0          \\[0pt]
    Reference for $\text{DNN-MG}(3+1)$ & MG(\(4\)) & \(\num{4097340}\)  & 8         \\[0pt]
    Reference for $\text{DNN-MG}(3+2)$ & MG(\(5\)) & \(\num{32115324}\) & 64        \\[0pt]
    \bottomrule
  \end{tabular}
\end{table}

We reuse the network trained for the single-obstacle channel at
Reynolds number \(Re=200\). The neural network
component of DNN-MG operates on level \(5\) as fine resolution and level \(3\)
as coarse resolution in this section. In order to not overburden this paper we
leave out the detailed results of a single level prediction. The complexity of the finite element meshes is given in Table~\ref{tab:org6b4401b}.

\paragraph*{Channel with two obstacles at $\text{Re}=\boldsymbol{180}$}
In Figure~\ref{fig:dl-3d-2-re180} and Table~\ref{tab:dl-3d-2-re180}, the drag
and lift coefficients obtained using DNN-MG at Reynolds number 180 are presented. The
results demonstrate the method's ability to accurately predict the dynamics of
the flow. In terms of the drag, the improvement by DNN-MG greatly improves the
magnitude of the drag and closely follows the trajectory of the reference
solution up to the small-scale dynamics. The lift is very similar in that we are
able to reconstruct the overall dynamics of the flow, although we do not get the
same temporal behavior. We also observe this in
Figure~\ref{fig:fields-2obs-re240}, through the different flow patterns. In
terms of $\min$ and $\max$ of the lift we are very close to the reference
solution.

\paragraph*{Channel with two obstacles at $\text{Re}=\boldsymbol{240}$}
Likewise, Figure~\ref{fig:dl-3d-2-re240} and Table~\ref{tab:dl-3d-2-re240} show
the drag and lift forces at Reynolds number 240, further substantiating the
accuracy of DNN-MG and its ability to improve the solution.
Figure~\ref{fig:fields-2obs-re240} further demonstrates the effectiveness of DNN-MG in
accurately capturing the velocity magnitudes for scenarios on which the DNN was
not trained. The figure shows the capability of the method to effectively
increase the resolution of the flow, to improve the solution and to add features that
could not be observed on lower levels, which are present on higher levels. This is
a great example of the efficacy of the localized predictions of DNN-MG, which
enable this generalization. Analogous to the case with one obstacle, we observe
good generalization of DNN-MG to different Reynolds numbers. The great results
we found for the drag and lift coefficients and the velocity field at
$\text{Re}=180$, translate similarly to $\text{Re}=240$ case.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed
% Figure environment removed

\paragraph*{Channel with two obstacles at $\text{Re}=\boldsymbol{300}$}
For Reynolds number 300, Figure~\ref{fig:dl-3d-2-re300} and
Table~\ref{tab:dl-3d-2-re300} present the drag and lift forces predicted by
DNN-MG.\@ The results demonstrate the method's ability to capture the complex flow
characteristics at higher Reynolds numbers. We see that DNN-MG generalizes well
to different geometries and Reynolds numbers. Considering the scenarios we
tested so far, we determine that DNN-MG indeed offers great potential in terms
of computational efficiency.

Figure~\ref{fig:vp-errors-2obs} demonstrates the good performance of DNN-MG
in terms of velocity and pressure error. The figure
shows a great improvement of the accuracy over an $\text{MG}(3)$ solution and
remarkably even outperforms the $\text{MG}(4)$ solution. This further
substantiates the efficiency in terms of runtime we demonstrated in Section~\ref{sec:perf-measures}.
Overall, DNN-MG is robust with respect to changes of the geometry and material
parameters and is able to improve solutions to PDEs in terms of characteristic
quantities of interest and error measures.


\section{Conclusion}
\label{sec:conc}
We have presented the deep neural network multigrid solver (DNN-MG) which uses a
deep neural network to improve the efficiency of a geometric multigrid solver,
e.\,g.\ for the simulation of the Navier-Stokes equations. Previously demonstrated
for 2D simulations, we extended DNN-MG to 3D and reformulated it
in a rigorous manner.
Despite the increased complexity of 3D flows, the algorithm remained applicable and
delivered even larger speed-ups compared to 2D, while retaining the efficiency,
generalizability, and scalability.

We established the efficacy of DNN-MG for large-scale simulations in regimes
where direct solvers are not feasible anymore. The overhead is small and in 3D
we can accelerate high fidelity simulations by a factor of 35. This comes at a
trade-off in terms of quality compared to the high fidelity solution. However, we
were able to consistently outperform classical solutions on the intermediate
levels, both in terms of solution quality and wall time.
This efficiency-performance trade-off establishes DNN-MG as a highly
promising approach for accelerating numerical solution methods for PDEs.

We ascertained a great generalization capability of DNN-MG due to the local
approach. We only trained the network with a single flow scenario at
$\text{Re}=200$ and got great results for lower and higher Reynolds numbers. Our
results showed that DNN-MG substantially improves the solution accuracy, as
measured by the $l^{2}$-errors, and reduces the errors of the drag and lift
functionals across all tested scenarios. The experiments with a two obstacle
flow at different Reynolds numbers were particularly successful.

Especially noteworthy is that DNN-MG is able to predict flow profiles of completely different dynamics. The grid finenesses MG(\(3\)) and MG(\(5\)) considered by us produce flows with very different character. Thus, especially in the test problem with two obstacles, there are hardly any oscillations on the coarse grid. These are correctly reproduced by DNN-MG. The local approach is thus able to identify global structures of the Navier-Stokes solution and to correct the solution in both local and temporal dynamics.

Although DNN-MG generalizes well, there are limits to this approach. In future
work we want to develop an online learning approach to retrain the network
adaptively based on the uncertainty of the predictions. Further, we want to
incorporate physical information, by including the residual of the PDE into the
loss function. We further want to investigate the application of DNN-MG to other
PDEs.

\FloatBarrier%

\subsection*{Acknowledgement}
NM acknowledges support by the Helmholtz-Gesellschaft grant number HIDSS-0002
DASHH. Computational resources (HSUper) were provided by the project
hpc.bw, funded by dtec.bw â Digitalization and Technology Research Center of the
Bundeswehr. dtec.bw is funded by the European Union â NextGenerationEU.
The work of TR was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - 314838170, GRK 2297 MathCoRe.



%\bibliographystyle{abbrv}
%\bibliography{paper_biber}

\printbibliography
\appendix
\section{Error Plots of the 3D Setting}
In Section~\ref{sec:num-example}, we have presented the numerical results
obtained from the DNN-MG method including plots of the velocity and pressure
error for different test cases (cf.~\ref{fig:vp-errors-1obs}
and~\ref{fig:vp-errors-2obs}), showing the improvement in accuracy. To further
substantiate the advantages of DNN-MG we present spatial representations of the
error distribution over the computational domain. The case with 1 obstacle for a
Reynolds number of $\mathrm{Re}=240$, both
for pressure and velocity, is shown in Figure~\ref{fig:fields-1obs-re240-p-error}
and~\ref{fig:fields-1obs-re240-error}. Analogously, the case with 2 obstacles at
the same Reynolds number is shown in Figure~\ref{fig:fields-2obs-re240-p-error}
and~\ref{fig:fields-2obs-re240-error}.

We have seen great improvement in terms of the drag and lift coefficients in
Section~\ref{sec:num-example}. In accordance with this, we see that the
error around the obstacles is greatly reduced by DNN-MG.\@ However, we observe a
significant error reduction over the entire domain. The main error contributions
are due to the different flow patterns evolving in the wake of the channel,
which we can also observe in Figures~\ref{fig:fields-1obs-re240}
and~\ref{fig:fields-2obs-re240}. Towards the end of the channels these
differences seem to dissipate. Overall the spatial distribution of the error
shows that DNN-MG is capable of globally improving the errors via the local
approach. Through the localized approach the method could be extended to use
localized error estimators to train the network on the critical regions in the
combination with an online learning approach.
% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed
\end{document}
