@article{ahmedAssessmentSolversSaddle2018,
  title = {An Assessment of Some Solvers for Saddle Point Problems Emerging from the Incompressible {{Navier}}–{{Stokes}} Equations},
  author = {Ahmed, Naveed and Bartsch, Clemens and John, Volker and Wilbrandt, Ulrich},
  date = {2018-04-01},
  journaltitle = {Computer Methods in Applied Mechanics and Engineering},
  volume = {331},
  pages = {492--513},
  issn = {0045-7825},
  doi = {10.1016/j.cma.2017.12.004},
  abstract = {Efficient incompressible flow simulations, using inf–sup stable pairs of finite element spaces, require the application of efficient solvers for the arising linear saddle point problems. This paper presents an assessment of different solvers: the sparse direct solver UMFPACK, the flexible GMRES (FGMRES) method with different coupled multigrid preconditioners, and FGMRES with Least Squares Commutator (LSC) preconditioners. The assessment is performed for steady-state and time-dependent flows around cylinders in 2d and 3d. Several pairs of inf–sup stable finite element spaces with second order velocity and first order pressure are used. It turns out that for the steady-state problems often FGMRES with an appropriate multigrid preconditioner was the most efficient method on finer grids. For the time-dependent problems, FGMRES with LSC preconditioners that use an inexact iterative solution of the velocity subproblem worked best for smaller time steps.},
  file = {/home/nils/Zotero/storage/YEF43PCU/Ahmed et al. - 2018 - An assessment of some solvers for saddle point pro.pdf;/home/nils/Zotero/storage/JQARRKP3/S0045782517307557.html}
}

@INPROCEEDINGS{kingmaAdamMethodStochastic2015,
  AUTHOR = {Kingma, Diederik P. and Ba, Jimmy},
  EDITOR = {Bengio, Yoshua and LeCun, Yann},
  URL = {http://arxiv.org/abs/1412.6980},
  BOOKTITLE = {3rd {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2015, {{San Diego}}, {{CA}}, {{USA}}, {{May}} 7-9, 2015, {{Conference Track Proceedings}}},
  DATE = {2015},
  SHORTTITLE = {Adam},
  TITLE = {Adam: {{A Method}} for {{Stochastic Optimization}}},
}

@article{arndtDealIILibrary2023,
  title = {The Deal.{{II Library}}, {{Version}} 9.5},
  author = {Arndt, Daniel and Bangerth, Wolfgang and Bergbauer, Maximilian and Feder, Marco and Fehling, Marc and Heinz, Johannes and Heister, Timo and Heltai, Luca and Kronbichler, Martin and Maier, Matthias and Munch, Peter and Pelteret, Jean-Paul and Turcksin, Bruno and Wells, David and Zampini, Stefano},
  date = {2023-09-01},
  journaltitle = {Journal of Numerical Mathematics},
  volume = {31},
  number = {3},
  pages = {231--246},
  publisher = {{De Gruyter}},
  issn = {1569-3953},
  doi = {10.1515/jnma-2023-0089},
  urldate = {2023-10-20},
  abstract = {This paper provides an overview of the new features of the finite element library deal.II , version 9.5.},
  langid = {english}
}

@online{kapustsinErrorAnalysisHybrid2023,
  title = {Error Analysis for Hybrid Finite Element/Neural Network Discretizations},
  author = {Kapustsin, Uladzislau and Kaya, Utku and Richter, Thomas},
  date = {2023-10-17},
  eprint = {2310.11271},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2310.11271},
  urldate = {2023-10-19},
  abstract = {We describe and analyze a hybrid finite element/neural network method for predicting solutions of partial differential equations. The methodology is designed for obtaining fine scale fluctuations from neural networks in a local manner. The network is capable of locally correcting a coarse finite element solution towards a fine solution taking the source term and the coarse approximation as input. Key observation is the dependency between quality of predictions and the size of training set which consists of different source terms and corresponding fine \& coarse solutions. We provide the a priori error analysis of the method together with the stability analysis of the neural network. The numerical experiments confirm the capability of the network predicting fine finite element solutions. We also illustrate the generalization of the method to problems where test and training domains differ from each other.},
  pubstate = {preprint},
  file = {/home/nils/Zotero/storage/9825ECIH/Kapustsin et al. - 2023 - Error analysis for hybrid finite elementneural ne.pdf;/home/nils/Zotero/storage/5RWE8JDH/2310.html}
}

@article{kapustsinHybridFiniteElement,
  title = {A Hybrid Finite Element/Neural Network Solver and Its Application to the {{Poisson}} Problem},
  author = {Kapustsin, Uladzislau and Kaya, Utku and Richter, Thomas},
  journaltitle = {PAMM},
  volume = {n/a},
  number = {n/a},
  pages = {e202300135},
  issn = {1617-7061},
  doi = {10.1002/pamm.202300135},
  urldate = {2023-10-19},
  abstract = {We analyze a hybrid method that enriches coarse grid finite element solutions with fine scale fluctuations obtained from a neural network. The idea stems from the Deep Neural Network Multigrid Solver (DNN-MG) which embeds a neural network into a multigrid hierarchy by solving coarse grid levels directly and predicting the corrections on fine grid levels locally (e.g., on small patches that consist of several cells) by a neural network. Such local designs are quite appealing, as they allow a very good generalizability. In this work, we formalize the method and describe main components of the a-priori error analysis. Moreover, we numerically investigate how the size of training set affects the solution quality.},
  langid = {english},
  file = {/home/nils/Zotero/storage/R32ZQPW2/Kapustsin et al. - A hybrid finite elementneural network solver and .pdf;/home/nils/Zotero/storage/CJRWSQ8Z/pamm.html}
}

@article{huangLearningOptimalMultigrid2023,
  title = {Learning {{Optimal Multigrid Smoothers}} via {{Neural Networks}}},
  author = {Huang, Ru and Li, Ruipeng and Xi, Yuanzhe},
  year = {2023},
  month = jun,
  journal = {SIAM J. Sci. Comput.},
  volume = {45},
  number = {3},
  pages = {S199-S225},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/21M1430030},
  urldate = {2023-07-27},
  abstract = {Multigrid methods are one of the most efficient techniques for solving large sparse linear systems arising from partial differential equations (PDEs) and graph Laplacians from machine learning applications. One of the key components of multigrid is smoothing, which aims at reducing high-frequency errors on each grid level. However, finding optimal smoothing algorithms is problem-dependent and can impose challenges for many problems. In this paper, we propose an efficient adaptive framework for learning optimized smoothers from operator stencils in the form of convolutional neural networks (CNNs). The CNNs are trained on small-scale problems from a given type of PDEs based on a supervised loss function derived from multigrid convergence theories and can be applied to large-scale problems of the same class of PDEs. Numerical results on anisotropic rotated Laplacian problems and variable coefficient diffusion problems demonstrate improved convergence rates and solution time compared with classical hand-crafted relaxation methods. Keywords machine learning multigrid methods multigrid smoothers convolutional neural networks MSC codes 65M55 65F08 65F10 15A60},
  file = {/home/nils/Zotero/storage/MXBC9GZ4/Huang et al. - 2023 - Learning Optimal Multigrid Smoothers via Neural Ne.pdf}
}

@article{anselmannEfficiencyLocalVanka2023,
  title = {Efficiency of Local {{Vanka}} Smoother Geometric Multigrid Preconditioning for Space-Time Finite Element Methods to the {{Navier}}\textendash{{Stokes}} Equations},
  author = {Anselmann, Mathias and Bause, Markus},
  year = {2023},
  journal = {PAMM},
  volume = {23},
  number = {1},
  pages = {e202200088},
  issn = {1617-7061},
  doi = {10.1002/pamm.202200088},
  urldate = {2023-07-27},
  abstract = {Numerical simulation of incompressible viscous flow continues to remain a challenging task, in particular if three space dimensions are involved. Space-time finite element methods feature the natural construction of higher order discretization schemes. They offer the potential to achieve accurate results on computationally feasible grids. Using a temporal test basis supported on the subintervals and linearizing the resulting algebraic problems by Newton's method yield linear systems of equations with block matrices built of (k + 1) \texttimes{} (k + 1) saddle point systems, where k denotes the polynomial order of the variational time discretization. We demonstrate numerically the efficiency of preconditioning GMRES iterations for solving these linear systems by a V-cycle geometric multigrid approach based on a local Vanka smoother. The studies are done for the two- and three-dimensional benchmark problem of flow around a cylinder. The robustness of the solver with respect to the piecewise polynomial orderkin time is analyzed and confirmed numerically.},
  copyright = {\textcopyright{} 2023 The Authors. Proceedings in Applied Mathematics \& Mechanics published by Wiley-VCH GmbH.},
  langid = {english},
  file = {/home/nils/Zotero/storage/UWFXNM82/Anselmann and Bause - 2023 - Efficiency of local Vanka smoother geometric multi.pdf;/home/nils/Zotero/storage/VPKHZ8YU/pamm.html}
}

@Article{Tanyu2023,
  title = {Deep Learning Methods for Partial Differential Equations and Related Parameter Identification Problems},
  author = {Nganyu Tanyu, Derick and Ning, Jianfeng and Freudenberg, Tom and Heilenkoetter, Nick and Rademacher, Andreas and Iben, Uwe and Maass, Peter},
  year = {2023},
  journal = {Inverse Problems},
  issn = {0266-5611},
  doi = {10.1088/1361-6420/ace9d4},
  urldate = {2023-07-27},
  abstract = {Recent years have witnessed a growth in mathematics for deep learning\textemdash which seeks a deeper understanding of the concepts of deep learning with mathematics and explores how to make it more robust\textemdash and deep learning for mathematics, where deep learning algorithms are used to solve problems in mathematics. The latter has popularised the field of scientific machine learning where deep learning is applied to problems in scientific computing. Specifically, more and more neural network architectures have been developed to solve specific classes of partial differential equations (PDEs). Such methods exploit properties that are inherent to PDEs and thus solve the PDEs better than standard feed-forward neural networks, recurrent neural networks, or convolutional neural networks. This has had a great impact in the area of mathematical modeling where parametric PDEs are widely used to model most natural and physical processes arising in science and engineering. In this work, we review such methods as well as their extensions for parametric studies and for solving the related inverse problems. We equally proceed to show their relevance in some industrial applications.},
  langid = {english}
}


@article{baigesFiniteElementReducedorder2020,
  title = {A Finite Element Reduced-Order Model Based on Adaptive Mesh Refinement and Artificial Neural Networks},
  author = {Baiges, Joan and Codina, Ramon and Casta{\~n}ar, Inocencio and Castillo, Ernesto},
  year = {2020},
  journal = {International Journal for Numerical Methods in Engineering},
  volume = {121},
  number = {4},
  pages = {588--601},
  issn = {1097-0207},
  doi = {10.1002/nme.6235},
  urldate = {2023-07-26},
  abstract = {In this work, a reduced-order model based on adaptive finite element meshes and a correction term obtained by using an artificial neural network (FAN-ROM) is presented. The idea is to run a high-fidelity simulation by using an adaptively refined finite element mesh and compare the results obtained with those of a coarse mesh finite element model. From this comparison, a correction forcing term can be computed for each training configuration. A model for the correction term is built by using an artificial neural network, and the final reduced-order model is obtained by putting together the coarse mesh finite element model, plus the artificial neural network model for the correction forcing term. The methodology is applied to nonlinear solid mechanics problems, transient quasi-incompressible flows, and a fluid-structure interaction problem. The results of the numerical examples show that the FAN-ROM is capable of improving the simulation results obtained in coarse finite element meshes at a reduced computational cost.},
  copyright = {\textcopyright{} 2019 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/home/nils/Zotero/storage/E9T85X84/Baiges et al. - 2020 - A finite element reduced-order model based on adap.pdf;/home/nils/Zotero/storage/LEFISPPB/nme.html}
}

@article{fabraFiniteElementApproximation2022,
  title = {Finite Element Approximation of Wave Problems with Correcting Terms Based on Training Artificial Neural Networks with Fine Solutions},
  author = {Fabra, Arnau and Baiges, Joan and Codina, Ramon},
  year = {2022},
  month = sep,
  journal = {Computer Methods in Applied Mechanics and Engineering},
  volume = {399},
  pages = {115280},
  issn = {0045-7825},
  doi = {10.1016/j.cma.2022.115280},
  urldate = {2023-07-26},
  abstract = {In this paper we present a general idea to correct coarse models by introducing a correcting term designed from fine solutions that can be then applied to situations in which a fine solution is not available. We apply this idea to the wave equation in both the time domain and the frequency domain. This correcting term is computed and trained by making use of learning algorithms, such as the least squares model or a model constructed from an artificial neural network. The performance of the method is tested through different acoustic numerical examples, where the fine solutions are characterized for having a finer discretization either in time or in space.},
  langid = {english},
  file = {/home/nils/Zotero/storage/H7MPZ5EE/S0045782522003991.html}
}

@article{chengMachineLearningData2023,
  title = {Machine {{Learning With Data Assimilation}} and {{Uncertainty Quantification}} for {{Dynamical Systems}}: {{A Review}}},
  shorttitle = {Machine {{Learning With Data Assimilation}} and {{Uncertainty Quantification}} for {{Dynamical Systems}}},
  author = {Cheng, Sibo and {Quilodr{\'a}n-Casas}, C{\'e}sar and Ouala, Said and Farchi, Alban and Liu, Che and Tandeo, Pierre and Fablet, Ronan and Lucor, Didier and Iooss, Bertrand and Brajard, Julien and Xiao, Dunhui and Janjic, Tijana and Ding, Weiping and Guo, Yike and Carrassi, Alberto and Bocquet, Marc and Arcucci, Rossella},
  year = {2023},
  month = jun,
  journal = {IEEE/CAA Journal of Automatica Sinica},
  volume = {10},
  number = {6},
  pages = {1361--1387},
  issn = {2329-9274},
  doi = {10.1109/JAS.2023.123537},
  abstract = {Data assimilation (DA) and uncertainty quantification (UQ) are extensively used in analysing and reducing error propagation in high-dimensional spatial-temporal dynamics. Typical applications span from computational fluid dynamics (CFD) to geoscience and climate systems. Recently, much effort has been given in combining DA, UQ and machine learning (ML) techniques. These research efforts seek to address some critical challenges in high-dimensional dynamical systems, including but not limited to dynamical system identification, reduced order surro-gate modelling, error covariance specification and model error correction. A large number of developed techniques and methodologies exhibit a broad applicability across numerous domains, resulting in the necessity for a comprehensive guide. This paper provides the first overview of state-of-the-art researches in this interdisciplinary field, covering a wide range of applications. This review is aimed at ML scientists who attempt to apply DA and UQ techniques to improve the accuracy and the interpretability of their models, but also at DA and UQ experts who intend to integrate cutting-edge ML approaches to their systems. Therefore, this article has a special focus on how ML methods can overcome the existing limits of DA and UQ, and vice versa. Some exciting perspectives of this rapidly developing research field are also discussed.},
  file = {/home/nils/Zotero/storage/FBXBMICB/Cheng et al. - 2023 - Machine Learning With Data Assimilation and Uncert.pdf;/home/nils/Zotero/storage/5CNG4267/authors.html}
}

@article{cuomoScientificMachineLearning2022,
  title = {Scientific {{Machine Learning Through Physics}}\textendash{{Informed Neural Networks}}: {{Where}} We Are and {{What}}'s {{Next}}},
  shorttitle = {Scientific {{Machine Learning Through Physics}}\textendash{{Informed Neural Networks}}},
  author = {Cuomo, Salvatore and Di Cola, Vincenzo Schiano and Giampaolo, Fabio and Rozza, Gianluigi and Raissi, Maziar and Piccialli, Francesco},
  year = {2022},
  month = jul,
  journal = {J Sci Comput},
  volume = {92},
  number = {3},
  pages = {88},
  issn = {1573-7691},
  doi = {10.1007/s10915-022-01939-z},
  urldate = {2023-06-29},
  abstract = {Physics-Informed Neural Networks (PINN) are neural networks (NNs) that encode model equations, like Partial Differential Equations (PDE), as a component of the neural network itself. PINNs are nowadays used to solve PDEs, fractional equations, integral-differential equations, and stochastic PDEs. This novel methodology has arisen as a multi-task learning framework in which a NN must fit observed data while reducing a PDE residual. This article provides a comprehensive review of the literature on PINNs: while the primary goal of the study was to characterize these networks and their related advantages and disadvantages. The review also attempts to incorporate publications on a broader range of collocation-based physics informed neural networks, which stars form the vanilla PINN, as well as many other variants, such as physics-constrained neural networks (PCNN), variational hp-VPINN, and conservative PINN (CPINN). The study indicates that most research has focused on customizing the PINN through different activation functions, gradient optimization techniques, neural network structures, and loss function structures. Despite the wide range of applications for which PINNs have been used, by demonstrating their ability to be more feasible in some contexts than classical numerical techniques like Finite Element Method (FEM), advancements are still possible, most notably theoretical issues that remain unresolved.},
  langid = {english},
  file = {/home/nils/Zotero/storage/BVEKR6X8/Cuomo et al. - 2022 - Scientific Machine Learning Through Physics–Inform.pdf}
}

@misc{margenbergOptimalDirichletBoundary2023,
  title = {Optimal {{Dirichlet Boundary Control}} by {{Fourier Neural Operators Applied}} to {{Nonlinear Optics}}},
  author = {Margenberg, Nils and K{\"a}rtner, Franz X. and Bause, Markus},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07292},
  eprint = {2307.07292},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2307.07292},
  urldate = {2023-07-20},
  abstract = {We present an approach for solving optimal Dirichlet boundary control problems of nonlinear optics by using deep learning. For computing high resolution approximations of the solution to the nonlinear wave model, we propose higher order space-time finite element methods in combination with collocation techniques. Thereby, \$C\^\{l\}\$-regularity in time of the global discrete is ensured. The resulting simulation data is used to train solution operators that effectively leverage the higher regularity of the training data. The solution operator is represented by Fourier Neural Operators and Gated Recurrent Units and can be used as the forward solver in the optimal Dirichlet boundary control problem. The proposed algorithm is implemented and tested on modern high-performance computing platforms, with a focus on efficiency and scalability. The effectiveness of the approach is demonstrated on the problem of generating Terahertz radiation in periodically poled Lithium Niobate, where the neural network is used as the solver in the optimal control setting to optimize the parametrization of the optical input pulse and maximize the yield of \$0.3\textbackslash,\$THz-frequency radiation. We exploit the periodic layering of the crystal to design the neural networks. The networks are trained to learn the propagation through one period of the layers. The recursive application of the network onto itself yields an approximation to the full problem. Our results indicate that the proposed method can achieve a significant speedup in computation time compared to classical methods. A comparison of our results to experimental data shows the potential to revolutionize the way we approach optimization problems in nonlinear optics.},
  archiveprefix = {arxiv},
  file = {/home/nils/Zotero/storage/MXTRCW2I/Margenberg et al. - 2023 - Optimal Dirichlet Boundary Control by Fourier Neur.pdf;/home/nils/Zotero/storage/HSMCL836/2307.html}
}

@article{caoResidualbasedErrorCorrection2023,
  title = {Residual-Based Error Correction for Neural Operator Accelerated Infinite-Dimensional {{Bayesian}} Inverse Problems},
  author = {Cao, Lianghao and {O'Leary-Roseberry}, Thomas and Jha, Prashant K. and Oden, J. Tinsley and Ghattas, Omar},
  year = {2023},
  month = aug,
  journal = {Journal of Computational Physics},
  volume = {486},
  pages = {112104},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2023.112104},
  urldate = {2023-07-24},
  abstract = {We explore using neural operators, or neural network representations of nonlinear maps between function spaces, to accelerate infinite-dimensional Bayesian inverse problems (BIPs) with models governed by nonlinear parametric partial differential equations (PDEs). Neural operators have gained significant attention in recent years for their ability to approximate the parameter-to-solution maps defined by PDEs using as training data solutions of PDEs at a limited number of parameter samples. The computational cost of BIPs can be drastically reduced if the large number of PDE solves required for posterior characterization are replaced with evaluations of trained neural operators. However, reducing error in the resulting BIP solutions via reducing the approximation error of the neural operators in training can be challenging and unreliable. We provide an a priori error bound result that implies certain BIPs can be ill-conditioned to the approximation error of neural operators, thus leading to inaccessible accuracy requirements in training. To reliably deploy neural operators in BIPs, we consider a strategy for enhancing the performance of neural operators: correcting the prediction of a trained neural operator by solving a linear variational problem based on the PDE residual. We show that a trained neural operator with error correction can achieve a quadratic reduction of its approximation error, all while retaining substantial computational speedups of posterior sampling when models are governed by highly nonlinear PDEs. The strategy is applied to two numerical examples of BIPs based on a nonlinear reaction\textendash diffusion problem and deformation of hyperelastic materials. We demonstrate that posterior representations of the two BIPs produced using trained neural operators are greatly and consistently enhanced by error correction.},
  langid = {english},
  file = {/home/nils/Zotero/storage/L2IVG4QN/Cao et al. - 2023 - Residual-based error correction for neural operato.pdf;/home/nils/Zotero/storage/X3FFFP8Q/S0021999123001997.html}
}

@article{kaltenbachSemisupervisedInvertibleNeural2023,
  title = {Semi-Supervised Invertible Neural Operators for {{Bayesian}} Inverse Problems},
  author = {Kaltenbach, Sebastian and Perdikaris, Paris and Koutsourelakis, Phaedon-Stelios},
  year = {2023},
  month = sep,
  journal = {Comput Mech},
  volume = {72},
  number = {3},
  pages = {451--470},
  issn = {1432-0924},
  doi = {10.1007/s00466-023-02298-8},
  urldate = {2023-07-24},
  abstract = {Neural Operators offer a powerful, data-driven tool for solving parametric PDEs as they can represent maps between infinite-dimensional function spaces. In this work, we employ physics-informed Neural Operators in the context of high-dimensional, Bayesian inverse problems. Traditional solution strategies necessitate an enormous, and frequently infeasible, number of forward model solves, as well as the computation of parametric derivatives. In order to enable efficient solutions, we extend Deep Operator Networks (DeepONets) by employing a RealNVP architecture which yields an invertible and differentiable map between the parametric input and the branch-net output. This allows us to construct accurate approximations of the full posterior, irrespective of the number of observations and the magnitude of the observation noise, without any need for additional forward solves nor for cumbersome, iterative sampling procedures. We demonstrate the efficacy and accuracy of the proposed methodology in the context of inverse problems for three benchmarks: an anti-derivative equation, reaction-diffusion dynamics and flow through porous media.},
  langid = {english},
  file = {/home/nils/Zotero/storage/NK2BAIWP/Kaltenbach et al. - 2023 - Semi-supervised invertible neural operators for Ba.pdf}
}
@inproceedings{molinaroNeuralInverseOperators2023,
  title = {Neural Inverse Operators for Solving {{PDE}} Inverse Problems},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Molinaro, Roberto and Yang, Yunan and Engquist, Björn and Mishra, Siddhartha},
  date = {2023-07-23},
  series = {{{ICML}}'23},
  volume = {202},
  pages = {25105--25139},
  publisher = {{JMLR.org}},
  location = {{Honolulu, Hawaii, USA}},
  abstract = {A large class of inverse problems for PDEs are only well-defined as mappings from operators to functions. Existing operator learning frameworks map functions to functions and need to be modified to learn inverse maps from data. We propose a novel architecture termed Neural Inverse Operators (NIOs) to solve these PDE inverse problems. Motivated by the underlying mathematical structure, NIO is based on a suitable composition of DeepONets and FNOs to approximate mappings from operators to functions. A variety of experiments are presented to demonstrate that NIOs significantly outperform baselines and solve PDE inverse problems robustly, accurately and are several orders of magnitude faster than existing direct and PDE-constrained optimization methods.}
}

@article{manriquedelaraAcceleratingHighOrder2023,
  title = {Accelerating High Order Discontinuous {{Galerkin}} Solvers Using Neural Networks: {{3D}} Compressible {{Navier-Stokes}} Equations},
  shorttitle = {Accelerating High Order Discontinuous {{Galerkin}} Solvers Using Neural Networks},
  author = {{Manrique de Lara}, Fernando and Ferrer, Esteban},
  year = {2023},
  month = sep,
  journal = {Journal of Computational Physics},
  volume = {489},
  pages = {112253},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2023.112253},
  abstract = {We propose to accelerate a high order discontinuous Galerkin solver using neural networks. We include a corrective forcing to a low polynomial order simulation to enhance its accuracy. The forcing is obtained by training a deep fully connected neural network, using a high polynomial order simulation but only for a short time frame. With this corrective forcing, we can run the low polynomial order simulation faster (with large time steps and low cost per time step) while improving its accuracy. We explored this idea for a 1D Burgers' equation in [1], and we have extended this work to the 3D Navier-Stokes equations, with and without a Large Eddy Simulation closure model. We test the methodology with the turbulent Taylor Green Vortex case and for various Reynolds numbers (30, 200 and 1600). In addition, the Taylor Green Vortex evolves with time and covers laminar, transitional, and turbulent regimes, as time progresses. The proposed methodology proves to be applicable to a variety of flows and regimes. The results show that the corrective forcing is effective in all Reynolds numbers and time frames (excluding the initial flow development). We can train the corrective forcing with a polynomial order of 8, to increase the accuracy of simulations from a polynomial order 3 to 6, when correcting outside the training time frame. The low order corrected solution is 4 to 5 times faster than a simulation with comparable accuracy (polynomial order 6). Additionally, we explore changes in the hyperparameters and use transfer learning to speed up the training. We observe that it is not useful to train a corrective forcing using a different flow condition. However, an already trained corrective forcing can be used to initialise a new training (at the correct flow conditions) to obtain an effective forcing with only a few training iterations.},
  langid = {english},
  file = {/home/nils/Zotero/storage/3SR8DIBI/Manrique de Lara and Ferrer - 2023 - Accelerating high order discontinuous Galerkin sol.pdf;/home/nils/Zotero/storage/JFFJXUE9/S0021999123003480.html}
}


@misc{biPanguWeather3DHighResolution2022,
  title = {Pangu-{{Weather}}: {{A 3D High-Resolution Model}} for {{Fast}} and {{Accurate Global Weather Forecast}}},
  shorttitle = {Pangu-{{Weather}}},
  author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
  year = {2022},
  month = nov,
  number = {arXiv:2211.02556},
  eprint = {2211.02556},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.02556},
  urldate = {2023-07-01},
  abstract = {In this paper, we present Pangu-Weather, a deep learning based system for fast and accurate global weather forecast. For this purpose, we establish a data-driven environment by downloading \$43\$ years of hourly global weather data from the 5th generation of ECMWF reanalysis (ERA5) data and train a few deep neural networks with about \$256\$ million parameters in total. The spatial resolution of forecast is \$0.25\^\textbackslash circ\textbackslash times0.25\^\textbackslash circ\$, comparable to the ECMWF Integrated Forecast Systems (IFS). More importantly, for the first time, an AI-based method outperforms state-of-the-art numerical weather prediction (NWP) methods in terms of accuracy (latitude-weighted RMSE and ACC) of all factors (e.g., geopotential, specific humidity, wind speed, temperature, etc.) and in all time ranges (from one hour to one week). There are two key strategies to improve the prediction accuracy: (i) designing a 3D Earth Specific Transformer (3DEST) architecture that formulates the height (pressure level) information into cubic data, and (ii) applying a hierarchical temporal aggregation algorithm to alleviate cumulative forecast errors. In deterministic forecast, Pangu-Weather shows great advantages for short to medium-range forecast (i.e., forecast time ranges from one hour to one week). Pangu-Weather supports a wide range of downstream forecast scenarios, including extreme weather forecast (e.g., tropical cyclone tracking) and large-member ensemble forecast in real-time. Pangu-Weather not only ends the debate on whether AI-based methods can surpass conventional NWP methods, but also reveals novel directions for improving deep learning weather forecast systems.},
  archiveprefix = {arxiv}
}

@misc{lamGraphCastLearningSkillful2022,
  title = {{{GraphCast}}: {{Learning}} Skillful Medium-Range Global Weather Forecasting},
  shorttitle = {{{GraphCast}}},
  author = {Lam, Remi and {Sanchez-Gonzalez}, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Pritzel, Alexander and Ravuri, Suman and Ewalds, Timo and Alet, Ferran and {Eaton-Rosen}, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Stott, Jacklynn and Vinyals, Oriol and Mohamed, Shakir and Battaglia, Peter},
  year = {2022},
  month = dec,
  number = {arXiv:2212.12794},
  eprint = {2212.12794},
  primaryclass = {physics},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.12794},
  urldate = {2023-07-01},
  abstract = {We introduce a machine-learning (ML)-based weather simulator--called "GraphCast"--which outperforms the most accurate deterministic operational medium-range weather forecasting system in the world, as well as all previous ML baselines. GraphCast is an autoregressive model, based on graph neural networks and a novel high-resolution multi-scale mesh representation, which we trained on historical weather data from the European Centre for Medium-Range Weather Forecasts (ECMWF)'s ERA5 reanalysis archive. It can make 10-day forecasts, at 6-hour time intervals, of five surface variables and six atmospheric variables, each at 37 vertical pressure levels, on a 0.25-degree latitude-longitude grid, which corresponds to roughly 25 x 25 kilometer resolution at the equator. Our results show GraphCast is more accurate than ECMWF's deterministic operational forecasting system, HRES, on 90.0\% of the 2760 variable and lead time combinations we evaluated. GraphCast also outperforms the most accurate previous ML-based weather forecasting model on 99.2\% of the 252 targets it reported. GraphCast can generate a 10-day forecast (35 gigabytes of data) in under 60 seconds on Cloud TPU v4 hardware. Unlike traditional forecasting methods, ML-based forecasting scales well with data: by training on bigger, higher quality, and more recent data, the skill of the forecasts can improve. Together these results represent a key step forward in complementing and improving weather modeling with ML, open new opportunities for fast, accurate forecasting, and help realize the promise of ML-based simulation in the physical sciences.},
  archiveprefix = {arxiv},
  file = {/home/nils/Zotero/storage/42WP9RDK/Lam et al. - 2022 - GraphCast Learning skillful medium-range global w.pdf;/home/nils/Zotero/storage/3YAIL2K5/2212.html}
}

@ARTICLE{ainsworthGalerkinNeuralNetworks2021,
  ABSTRACT = {We present a new approach to using neural networks to approximate variational equations, based on the adaptive construction of a sequence of finite-dimensional subspaces whose basis functions are realizations of a sequence of neural networks. The finite-dimensional subspaces can be used to define a standard Galerkin approximation of the variational equation. This approach enjoys advantages including the following: the sequential nature of the algorithm offers a systematic approach to enhancing the accuracy of a given approximation; the sequential enhancements provide a useful indicator for the error that can be used as a criterion for terminating the sequential updates; the basic approach is to some extent oblivious to the nature of the partial differential equation under consideration; and some basic theoretical results are presented regarding the convergence (or otherwise) of the method which are used to formulate basic guidelines for applying the method.},
  AUTHOR = {Ainsworth, Mark and Dong, Justin},
  PUBLISHER = {Society for Industrial and Applied Mathematics},
  YEAR = {2021},
  DOI = {10.1137/20M1366587},
  FILE = {/home/nils/Zotero/storage/SSAXKUCS/Ainsworth und Dong - 2021 - Galerkin Neural Networks A Framework for Approxim.pdf;/home/nils/Zotero/storage/Y2C6WT4P/20M1366587.html},
  JOURNAL = {SIAM Journal on Scientific Computing},
  LANGID = {english},
  SHORTTITLE = {Galerkin {{Neural Networks}}},
  TITLE = {Galerkin {{Neural Networks}}: {{A Framework}} for {{Approximating Variational Equations}} with {{Error Control}}},
}

@ARTICLE{babukaPostprocessingApproachFinite1984,
  ABSTRACT = {In the context of a model problem we describe post-processing techniques for the calculation of generalized stress intensity factors as well as displacements, stresses, etc., near corner points. We discuss two broad classes of methods, one involving an ‘influence’ function, and the other related to the well-known energy release principle of fracture mechanics. An error analysis is sketched and two numerical examples are given to illustrate the effectiveness of the techniques.},
  AUTHOR = {Babuška, I. and Miller, A.},
  year = {1984},
  DOI = {10.1002/nme.1620200611},
  FILE = {/home/nils/Zotero/storage/XQPL37H5/Babuška and Miller - 1984 - The post-processing approach in the finite element.pdf;/home/nils/Zotero/storage/7WRMAC45/nme.html},
  ISSN = {1097-0207},
  JOURNAL = {International Journal for Numerical Methods in Engineering},
  LANGID = {english},
  NUMBER = {6},
  PAGES = {1111--1129},
  SHORTTITLE = {The Post-Processing Approach in the Finite Element Method—{{Part}} 2},
  TITLE = {The Post-Processing Approach in the Finite Element Method—{{Part}} 2: {{The}} Calculation of Stress Intensity Factors},
  VOLUME = {20},
}

@ARTICLE{beckerFiniteElementPressure2001,
  AUTHOR = {Becker, R. and Braack, M.},
  year = {2001},
  DOI = {10.1007/s10092-001-8180-4},
  IDS = {BeckerBraack2001},
  JOURNAL = {Calcolo},
  LIBRARY = {private},
  NUMBER = {4},
  PAGES = {173--199},
  TITLE = {A Finite Element Pressure Gradient Stabilization for the {{Stokes}} Equations Based on Local Projections},
  VOLUME = {38},
}

@INPROCEEDINGS{beckerTwolevelStabilizationScheme2004,
  AUTHOR = {Becker, R. and Braack, M.},
  EDITOR = {al. M. Feistauer, et.},
  PUBLISHER = {Springer},
  BOOKTITLE = {Numerical {{Mathematics}} and {{Advanced Applications}}, {{ENUMATH}} 2003},
  year = {2004},
  PAGES = {123--130},
  TITLE = {A Two-Level Stabilization Scheme for the {{Navier-Stokes}} Equations},
}

@ARTICLE{beckerMultigridTechniquesFinite2000,
  AUTHOR = {Becker, R. and Braack, M.},
  year = {2000},
  DOI = {10.1002/1099-1506(200009)7:6<363::AID-NLA202>3.0.CO;2-V},
  JOURNAL = {Numerical Linear Algebra with Applications},
  PAGES = {363--379},
  TITLE = {Multigrid Techniques for Finite Elements on Locally Refined Meshes},
  VOLUME = {7},
}


@article{beckerFiniteElementToolkit,
  AUTHOR = {Becker, R. and Braack, M. and Meidner, D. and Richter, T. and Vexler, B.},
  TITLE = {The Finite Element Toolkit {{{\textsc{Gascoigne}}}}},
  journal = {Zenodo},
  year = {2021},
  doi = {10.5281/zenodo.5574969},
    note = {\url{https://gascoigne.math.uni-magdeburg.de/}},
}

@ARTICLE{braackSolutions3DNavierStokes2006,
  AUTHOR = {Braack, M. and Richter, T.},
  YEAR = {2006},
  DOI = {10.1016/j.compfluid.2005.02.001},
  JOURNAL = {Computers and Fluids},
  NUMBER = {4},
  PAGES = {372--392},
  TITLE = {Solutions of {{3D Navier-Stokes}} Benchmark Problems with Adaptive Finite Elements},
  VOLUME = {35},
}

@ARTICLE{braackStabilizedFiniteElement2007,
  AUTHOR = {Braack, M. and Burman, E. and John, V. and Lube, G.},
  year = {2007},
  DOI = {10.1016/j.cma.2006.07.011},
  JOURNAL = {Comput. Methods Appl. Mech. Engrg.},
  PAGES = {853--866},
  TITLE = {Stabilized Finite Element Methods for the Generalized {{Oseen}} Problem},
  VOLUME = {196},
}

@ARTICLE{brevisNeuralControlDiscrete2022,
  ABSTRACT = {There is tremendous potential in using neural networks to optimize numerical methods. In this paper, we introduce and analyze a framework for the neural optimization of discrete weak formulations, suitable for finite element methods. The main idea of the framework is to include a neural-network function acting as a control variable in the weak form. Finding the neural control that (quasi-) minimizes a suitable cost (or loss) functional, then yields a numerical approximation with desirable attributes. In particular, the framework allows in a natural way the incorporation of known data of the exact solution, or the incorporation of stabilization mechanisms (e.g., to remove spurious oscillations). The main result of our analysis pertains to the well-posedness and convergence of the associated constrained-optimization problem. In particular, we prove under certain conditions, that the discrete weak forms are stable, and that quasi-minimizing neural controls exist, which converge quasi-optimally. We specialize the analysis results to Galerkin, least squares and minimal-residual formulations, where the neural-network dependence appears in the form of suitable weights. Elementary numerical experiments support our findings and demonstrate the potential of the framework.},
  AUTHOR = {Brevis, Ignacio and Muga, Ignacio and van der Zee, Kristoffer G.},
  YEAR = {2022},
  DOI = {10.1016/j.cma.2022.115716},
  FILE = {/home/nils/Zotero/storage/9IHRWH7Z/Brevis et al. - 2022 - Neural control of discrete weak formulations Gale.pdf;/home/nils/Zotero/storage/TTZRBBFH/S0045782522006715.html},
  ISSN = {0045-7825},
  JOURNAL = {Computer Methods in Applied Mechanics and Engineering},
  LANGID = {english},
  PAGES = {115716},
  SERIES = {A {{Special Issue}} in {{Honor}} of the {{Lifetime Achievements}} of {{J}}. {{Tinsley Oden}}},
  SHORTJOURNAL = {Computer Methods in Applied Mechanics and Engineering},
  SHORTTITLE = {Neural Control of Discrete Weak Formulations},
  TITLE = {Neural Control of Discrete Weak Formulations: {{Galerkin}}, Least Squares \& Minimal-Residual Methods with Quasi-Optimal Weights},
  VOLUME = {402},
}

@ARTICLE{chenUniversalApproximationNonlinear1995,
  ABSTRACT = {The purpose of this paper is to investigate neural network capability systematically. The main results are: 1) every Tauber-Wiener function is qualified as an activation function in the hidden layer of a three-layered neural network; 2) for a continuous function in S'(R/sup 1/) to be a Tauber-Wiener function, the necessary and sufficient condition is that it is not a polynomial; 3) the capability of approximating nonlinear functionals defined on some compact set of a Banach space and nonlinear operators has been shown; and 4) the possibility by neural computation to approximate the output as a whole (not at a fixed point) of a dynamical system, thus identifying the system.{$<>$}},
  AUTHOR = {Chen, Tianping and Chen, Hong},
  YEAR = {1995},
  DOI = {10.1109/72.392253},
  EVENTTITLE = {{{IEEE Transactions}} on {{Neural Networks}}},
  FILE = {/home/nils/Zotero/storage/VMICSQ87/392253.html},
  ISSN = {1941-0093},
  JOURNAL = {IEEE Transactions on Neural Networks},
  NUMBER = {4},
  PAGES = {911--917},
  TITLE = {Universal Approximation to Nonlinear Operators by Neural Networks with Arbitrary Activation Functions and Its Application to Dynamical Systems},
  VOLUME = {6},
}

@INPROCEEDINGS{choLearningPhraseRepresentations2014,
  AUTHOR = {Cho, K. and van Merrienboer, B. and Gulcehre, C. and Bougares, F. and Schwenk, H. and Bengio, Y.},
  BOOKTITLE = {Conference on Empirical Methods in Natural Language Processing ({{EMNLP}} 2014)},
  year = {2014},
  DOI = {10.3115/v1/D14-1179},
  TITLE = {Learning Phrase Representations Using {{RNN}} Encoder-Decoder for Statistical Machine Translation},
}

@ARTICLE{delaraAcceleratingHighOrder2022,
  ABSTRACT = {High order discontinuous Galerkin methods allow accurate solutions through the use of high order polynomials inside each mesh element. Increasing the polynomial order leads to high accuracy, but increases the cost. On the one hand, high order polynomials require more restrictive time steps when using explicit temporal schemes, and on the other hand the quadrature rules lead to more costly evaluations per iteration. We propose to accelerate high order discontinuous Galerkin methods using Neural Networks. To this aim, we train a Neural Network using a high order discretisation, to extract a corrective forcing that can be applied to a low order solution with the aim of recovering high order accuracy. With this corrective forcing term, we can run a low order solution (low cost) and correct the solution to obtain high order accuracy. We provide error bounds to quantify the various errors included in the methodology (e.g. related to the discretisation or the Neural Network) . The methodology and bounds are examined for a variety of meshes, polynomial orders and viscosity values for the 1D viscous Burgers’ equation. The result show good accuracy and accelerations specially when considering high polynomial orders.},
  AUTHOR = {Manrique de Lara, Fernando and Ferrer, Esteban},
  YEAR = {2022},
  DOI = {10.1016/j.compfluid.2021.105274},
  FILE = {/home/nils/Zotero/storage/AV8WURMY/de Lara and Ferrer - 2022 - Accelerating high order discontinuous Galerkin sol.pdf;/home/nils/Zotero/storage/N24EAY4A/S0045793021003698.html},
  ISSN = {0045-7930},
  JOURNAL = {Computers \& Fluids},
  LANGID = {english},
  PAGES = {105274},
  SHORTJOURNAL = {Computers \& Fluids},
  SHORTTITLE = {Accelerating High Order Discontinuous {{Galerkin}} Solvers Using Neural Networks},
  TITLE = {Accelerating High Order Discontinuous {{Galerkin}} Solvers Using Neural Networks: {{1D Burgers}}’ Equation},
  VOLUME = {235},
}

@article{kimmritzParallelMultigrid2011,
author = {M. Kimmritz and T. Richter},
year = {2011},
title = {Parallel multigrid method for finite element simulations of complex flow problems on locally refined meshes},
journal = {Numerical Linear Algebra with Applications},
volume = {18},
pages = {615-636},
}

@ARTICLE{eDeepRitzMethod2018,
  ABSTRACT = {We propose a deep learning-based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz Method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
  AUTHOR = {E, Weinan and Yu, Bing},
  year = {2018},
  DOI = {10.1007/s40304-018-0127-z},
  ISBN = {2194-671X},
  JOURNAL = {Communications in Mathematics and Statistics},
  NUMBER = {1},
  PAGES = {1--12},
  TITLE = {The Deep Ritz Method: {{A}} Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
  VOLUME = {6},
}




@Article{eichinger2022,
  author = 	 {M. Eichinger and A. Heinlein and A. Klawonn},
  title = 	 {Surrogate Convolutional Neural Network Models for Steady Computational Fluid Dynamics Simulations},
  journal = 	 {Electronic Transactions on Numerical Analysis},
  year = 	 {2022},
  volume = 	 {56},
  pages = 	 {235-255},
}

@INPROCEEDINGS{eichingerStationaryFlowPredictions2021,
  ABSTRACT = {Computational Fluid Dynamics (CFD) simulations are a numerical tool to model and analyze the behavior of fluid flow. However, accurate simulations are generally very costly because they require high grid resolutions. In this paper, an alternative approach for computing flow predictions using Convolutional Neural Networks (CNNs) is described; in particular, a classical CNN as well as the U-Net architecture are used. First, the networks are trained in an expensive offline phase using flow fields computed by CFD simulations. Afterwards, the evaluation of the trained neural networks is very cheap. Here, the focus is on the dependence of the stationary flow in a channel on variations of the shape and the location of an obstacle. CNNs perform very well on validation data, where the averaged error for the best networks is below 3\%. In addition to that, they also generalize very well to new data, with an averaged error below 10\%.},
  AUTHOR = {Eichinger, Matthias and Heinlein, Alexander and Klawonn, Axel},
  EDITOR = {Vermolen, Fred J. and Vuik, Cornelis},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  BOOKTITLE = {Numerical {{Mathematics}} and {{Advanced Applications ENUMATH}} 2019},
  year = {2021},
  DOI = {10.1007/978-3-030-55874-1_53},
  FILE = {/home/nils/Zotero/storage/JI45XGZM/Eichinger et al. - 2021 - Stationary Flow Predictions Using Convolutional Ne.pdf},
  ISBN = {978-3-030-55874-1},
  LANGID = {english},
  PAGES = {541--549},
  SERIES = {Lecture {{Notes}} in {{Computational Science}} and {{Engineering}}},
  TITLE = {Stationary {{Flow Predictions Using Convolutional Neural Networks}}},
}

@ARTICLE{failerNewtonMultigridFramework2020,
  AUTHOR = {Failer, L. and Richter, T.},
  year = {2020},
  JOURNAL = {Optimization and Engineering},
  VOLUME = {22},
  NUMBER = {4},
  DOI={10.1007/s11081-020-09498-8},
  TITLE = {A {{Newton}} Multigrid Framework for Optimal Control of Fluid-Structure Interactions},
}

@ARTICLE{failerParallelNewtonMultigrid2021,
  TITLE = {A Parallel Newton Multigrid Framework for Monolithic Fluid-Structure Interactions},
  AUTHOR = {Failer, L. and Richter, T.},
  year = {2021},
  JOURNAL = {Journal of Scientific Computing},
  VOLUME = {82},
  NUMBER = {28},
  DOI = {10.1007/s10915-019-01113-y}
}


@article{genevaTransformersModelingPhysical2022,
  title = {Transformers for Modeling Physical Systems},
  author = {Geneva, Nicholas and Zabaras, Nicholas},
  year = {2022},
  month = feb,
  journal = {Neural Networks},
  volume = {146},
  pages = {272--289},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.11.022},
  abstract = {Transformers are widely used in natural language processing due to their ability to model longer-term dependencies in text. Although these models achieve state-of-the-art performance for many language related tasks, their applicability outside of the natural language processing field has been minimal. In this work, we propose the use of transformer models for the prediction of dynamical systems representative of physical phenomena. The use of Koopman based embeddings provides a unique and powerful method for projecting any dynamical system into a vector representation which can then be predicted by a transformer. The proposed model is able to accurately predict various dynamical systems and outperform classical methods that are commonly used in the scientific machine learning literature.11Code available at: https://github.com/zabaras/transformer-physx.},
  langid = {english},
  file = {/home/nils/Zotero/storage/6C6288H7/Geneva and Zabaras - 2022 - Transformers for modeling physical systems.pdf;/home/nils/Zotero/storage/YI2J43SS/S0893608021004500.html}
}

@ARTICLE{ghattasLearningPhysicsbasedModels2021,
  ABSTRACT = {This article addresses the inference of physics models from data, from the perspectives of inverse problems and model reduction. These fields develop formulations that integrate data into physics-based models while exploiting the fact that many mathematical models of natural and engineered systems exhibit an intrinsically low-dimensional solution manifold. In inverse problems, we seek to infer uncertain components of the inputs from observations of the outputs, while in model reduction we seek low-dimensional models that explicitly capture the salient features of the input–output map through approximation in a low-dimensional subspace. In both cases, the result is a predictive model that reflects data-driven learning yet deeply embeds the underlying physics, and thus can be used for design, control and decision-making, often with quantified uncertainties. We highlight recent developments in scalable and efficient algorithms for inverse problems and model reduction governed by large-scale models in the form of partial differential equations. Several illustrative applications to large-scale complex problems across different domains of science and engineering are provided.},
  AUTHOR = {Ghattas, Omar and Willcox, Karen},
  PUBLISHER = {Cambridge University Press},
  YEAR = {2021},
  DOI = {10.1017/S0962492921000064},
  FILE = {/home/nils/Zotero/storage/JZIAAJIQ/Ghattas and Willcox - 2021 - Learning physics-based models from data perspecti.pdf},
  ISSN = {0962-4929, 1474-0508},
  JOURNAL = {Acta Numerica},
  LANGID = {english},
  PAGES = {445--554},
  SHORTTITLE = {Learning Physics-Based Models from Data},
  TITLE = {Learning Physics-Based Models from Data: Perspectives from Inverse Problems and Model Reduction},
  VOLUME = {30},
}

@misc{grossmannCanPhysicsInformedNeural2023,
  ABSTRACT = {Partial differential equations play a fundamental role in the mathematical modelling of many processes and systems in physical, biological and other sciences. To simulate such processes and systems, the solutions of PDEs often need to be approximated numerically. The finite element method, for instance, is a usual standard methodology to do so. The recent success of deep neural networks at various approximation tasks has motivated their use in the numerical solution of PDEs. These so-called physics-informed neural networks and their variants have shown to be able to successfully approximate a large range of partial differential equations. So far, physics-informed neural networks and the finite element method have mainly been studied in isolation of each other. In this work, we compare the methodologies in a systematic computational study. Indeed, we employ both methods to numerically solve various linear and nonlinear partial differential equations: Poisson in 1D, 2D, and 3D, Allen-Cahn in 1D, semilinear Schr\textbackslash "odinger in 1D and 2D. We then compare computational costs and approximation accuracies. In terms of solution time and accuracy, physics-informed neural networks have not been able to outperform the finite element method in our study. In some experiments, they were faster at evaluating the solved PDE.},
  AUTHOR = {Grossmann, Tamara G. and Komorowska, Urszula Julia and Latz, Jonas and Schönlieb, Carola-Bibiane},
  YEAR = {2023},
  doi = {10.48550/arXiv.2302.04107},
  EPRINT = {2302.04107},
  EPRINTCLASS = {cs, math},
  EPRINTTYPE = {arxiv},
  FILE = {/home/nils/Zotero/storage/ILN7WVDA/Grossmann et al. - 2023 - Can Physics-Informed Neural Networks beat the Fini.pdf;/home/nils/Zotero/storage/3VVXQS9G/2302.html},
  PUBSTATE = {preprint},
  TITLE = {Can {{Physics-Informed Neural Networks}} Beat the {{Finite Element Method}}?},
}

@ARTICLE{heMgNetUnifiedFramework2019,
  ABSTRACT = {We develop a unified model, known as MgNet, that simultaneously recovers some convolutional neural networks (CNN) for image classification and multigrid (MG) methods for solving discretized partial differential equations (PDEs). This model is based on close connections that we have observed and uncovered between the CNN and MG methodologies. For example, pooling operation and feature extraction in CNN correspond directly to restriction operation and iterative smoothers in MG, respectively. As the solution space is often the dual of the data space in PDEs, the analogous concept of feature space and data space (which are dual to each other) is introduced in CNN. With such connections and new concept in the unified model, the function of various convolution operations and pooling used in CNN can be better understood. As a result, modified CNN models (with fewer weights and hyperparameters) are developed that exhibit competitive and sometimes better performance in comparison with existing CNN models when applied to both CIFAR-10 and CIFAR-100 data sets.},
  AUTHOR = {He, Juncai and Xu, Jinchao},
  PUBLISHER = {Springer},
  YEAR = {2019},
  DOI = {10.1007/s11425-019-9547-2},
  FILE = {/home/nils/Zotero/storage/2E2UWNPB/He and Xu - 2019 - MgNet A unified framework of multigrid and convol.pdf;/home/nils/Zotero/storage/XQK9YPSI/He and Xu - 2019 - MgNet A unified framework of multigrid and convol.pdf},
  IDS = {he2019mgnet},
  ISSN = {1869-1862},
  JOURNAL = {Science China Mathematics},
  LANGID = {english},
  NUMBER = {7},
  PAGES = {1331--1354},
  SHORTJOURNAL = {Sci. China Math.},
  SHORTTITLE = {{{MgNet}}},
  TITLE = {{{MgNet}}: {{A}} Unified Framework of Multigrid and Convolutional Neural Network},
  VOLUME = {62},
}

@ARTICLE{heywoodFiniteElementApproximation1990,
  AUTHOR = {Heywood, J. and Rannacher, R.},
  year = {1990},
  DOI = {10.1137/0727022},
  IDS = {HeywoodRannacher1990},
  JOURNAL = {SIAM J. Numer. Anal.},
  NUMBER = {3},
  PAGES = {353--384},
  TITLE = {Finite {{Element Approximation}} of the {{Nonstationary Navier-Stokes Problem}}. {{IV}}. {{Error Analysis}} for {{Second-Order Time Discretization}}},
  VOLUME = {27},
}

@ARTICLE{heywoodArtificialBoundariesFlux1992,
  AUTHOR = {Heywood, J. G. and Rannacher, R. and Turek, S.},
  year = {1992},
  DOI = {10.1002/(SICI)1097-0363(19960315)22:5<325::AID-FLD307>3.0.CO;2-Y},
  JOURNAL = {Int. J. Numer. Math. Fluids.},
  PAGES = {325--352},
  TITLE = {Artificial Boundaries and Flux and Pressure Conditions for the Incompressible {{Navier-Stokes}} Equations},
  VOLUME = {22},
}

@ARTICLE{hochreiterLongShorttermMemory1997,
  ABSTRACT = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter...},
  AUTHOR = {Hochreiter, S. and Schmidhuber, J.},
  PUBLISHER = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
  YEAR = {1997},
  DOI = {10.1162/neco.1997.9.8.1735},
  ISSN = {0899-7667},
  JOURNAL = {Neural Computation},
  NUMBER = {8},
  PAGES = {1735--1780},
  TITLE = {Long Short-Term Memory},
  VOLUME = {9},
}

@BOOK{kelleyIterativeMethodsLinear1995b,
  AUTHOR = {Kelley, C. T.},
  PUBLISHER = {SIAM, Philadelphia},
  year = {1995},
  LIBRARY = {Fakultaet Mathematik (Rannacher), Uni-HD},
  TITLE = {Iterative Methods for Linear and Nonlinear Equations},
}

@UNPUBLISHED{kharaNeuFENetNeuralFinite2021,
  ABSTRACT = {We consider a mesh-based approach for training a neural network to produce field predictions of solutions to parametric partial differential equations (PDEs). This approach contrasts current approaches for "neural PDE solvers" that employ collocation-based methods to make point-wise predictions of solutions to PDEs. This approach has the advantage of naturally enforcing different boundary conditions as well as ease of invoking well-developed PDE theory -- including analysis of numerical stability and convergence -- to obtain capacity bounds for our proposed neural networks in discretized domains. We explore our mesh-based strategy, called NeuFENet, using a weighted Galerkin loss function based on the Finite Element Method (FEM) on a parametric elliptic PDE. The weighted Galerkin loss (FEM loss) is similar to an energy functional that produces improved solutions, satisfies a priori mesh convergence, and can model Dirichlet and Neumann boundary conditions. We prove theoretically, and illustrate with experiments, convergence results analogous to mesh convergence analysis deployed in finite element solutions to PDEs. These results suggest that a mesh-based neural network approach serves as a promising approach for solving parametric PDEs with theoretical bounds.},
  AUTHOR = {Khara, Biswajit and Balu, Aditya and Joshi, Ameya and Sarkar, Soumik and Hegde, Chinmay and Krishnamurthy, Adarsh and Ganapathysubramanian, Baskar},
  URL = {http://arxiv.org/abs/2110.01601},
  YEAR = {2021},
  EPRINT = {2110.01601},
  EPRINTCLASS = {cs, math},
  EPRINTTYPE = {arxiv},
  FILE = {/home/nils/Zotero/storage/2GS89VIY/Khara et al. - 2021 - NeuFENet Neural Finite Element Solutions with The.pdf;/home/nils/Zotero/storage/GU3754UB/2110.html},
  SHORTTITLE = {{{NeuFENet}}},
  TITLE = {{{NeuFENet}}: {{Neural Finite Element Solutions}} with {{Theoretical Bounds}} for {{Parametric PDEs}}},
}

@ARTICLE{kutyniok2022atheoretical,
  AUTHOR = {Kutyniok, Gitta and Petersen, Philipp and Raslan, Mones and Schneider, Reinhold},
  year = {2022},
  DOI = {10.1007/s00365-021-09551-4},
  FILE = {/home/nils/Zotero/storage/BGQHBLQN/Kutyniok et al. - 2022 - A theoretical analysis of deep neural networks and.pdf},
  JOURNAL = {Constructive Approximation},
  NUMBER = {1},
  PAGES = {73--125},
  TITLE = {A Theoretical Analysis of Deep Neural Networks and Parametric {{PDEs}}},
  VOLUME = {55},
}

@ARTICLE{lagarisArtificialNeuralNetworks1998,
  ABSTRACT = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE), to systems of coupled ODE and also to partial differential equations (PDE). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galerkin finite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.},
  AUTHOR = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
  PUBLISHER = {IEEE Press},
  YEAR = {1998},
  DOI = {10.1109/72.712178},
  EVENTTITLE = {{{IEEE Transactions}} on {{Neural Networks}}},
  FILE = {/home/nils/Zotero/storage/7WKXFM3W/Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf;/home/nils/Zotero/storage/KB78FTRS/Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf;/home/nils/Zotero/storage/WMPQYJYH/Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf},
  IDS = {lagaris1998artificial,lagarisArtificialNeuralNetworks1998a},
  ISSN = {1941-0093},
  JOURNAL = {IEEE Transactions on Neural Networks},
  NUMBER = {5},
  PAGES = {987--1000},
  PAGETOTAL = {14},
  TITLE = {Artificial Neural Networks for Solving Ordinary and Partial Differential Equations},
  VOLUME = {9},
}

@UNPUBLISHED{loshchilovDecoupledWeightDecay2019,
  ABSTRACT = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  AUTHOR = {Loshchilov, Ilya and Hutter, Frank},
  URL = {http://arxiv.org/abs/1711.05101},
  YEAR = {2019},
  EPRINT = {1711.05101},
  EPRINTCLASS = {cs, math},
  EPRINTTYPE = {arxiv},
  PUBSTATE = {preprint},
  FILE = {/home/nils/Zotero/storage/A2NYSGLK/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/home/nils/Zotero/storage/JJ4ZZEJC/1711.html},
  TITLE = {Decoupled {{Weight Decay Regularization}}},
}
@inproceedings{loshchilovDecoupledWeightDecay2018,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=Bkg6RiCqY7},
  urldate = {2023-10-29},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it ``weight decay'' in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at \textbackslash url\{https://github.com/loshchil/AdamW-and-SGDW\}},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/nils/Zotero/storage/GU2QC9VH/Loshchilov and Hutter - 2018 - Decoupled Weight Decay Regularization.pdf}
}


@ARTICLE{luDeepXDEDeepLearning2021,
  ABSTRACT = {Deep learning has achieved remarkable success in diverse applications; however, its use in solving partial differential equations (PDEs) has emerged only recently. Here, we present an overview of physics-informed neural networks (PINNs), which embed a PDE into the loss of the neural network using automatic differentiation. The PINN algorithm is simple, and it can be applied to different types of PDEs, including integro-differential equations, fractional PDEs, and stochastic PDEs. Moreover, from an implementation point of view, PINNs solve inverse problems as easily as forward problems. We propose a new residual-based adaptive refinement (RAR) method to improve the training efficiency of PINNs. For pedagogical reasons, we compare the PINN algorithm to a standard finite element method. We also present a Python library for PINNs, DeepXDE, which is designed to serve both as an educational tool to be used in the classroom as well as a research tool for solving problems in computational science and engineering. Specifically, DeepXDE can solve forward problems given initial and boundary conditions, as well as inverse problems given some extra measurements. DeepXDE supports complex-geometry domains based on the technique of constructive solid geometry and enables the user code to be compact, resembling closely the mathematical formulation. We introduce the usage of DeepXDE and its customizability, and we also demonstrate the capability of PINNs and the user-friendliness of DeepXDE for five different examples. More broadly, DeepXDE contributes to the more rapid development of the emerging scientific machine learning field. Keywords education software DeepXDE differential equations deep learning physics-informed neural networks scientific machine learning MSC codes 65-01 65-04 65L99 65M99 65N99},
  AUTHOR = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  PUBLISHER = {Society for Industrial and Applied Mathematics},
  YEAR = {2021},
  DOI = {10.1137/19M1274067},
  FILE = {/home/nils/Zotero/storage/GVQGLW7I/Lu et al. - 2021 - DeepXDE A Deep Learning Library for Solving Diffe.pdf},
  ISSN = {0036-1445},
  JOURNAL = {SIAM Review},
  NUMBER = {1},
  PAGES = {208--228},
  SHORTJOURNAL = {SIAM Rev.},
  SHORTTITLE = {{{DeepXDE}}},
  TITLE = {{{DeepXDE}}: {{A Deep Learning Library}} for {{Solving Differential Equations}}},
  VOLUME = {63},
}

@ARTICLE{luLearningNonlinearOperators2021,
  ABSTRACT = {It is widely known that neural networks (NNs) are universal approximators of continuous functions. However, a less known but powerful result is that a NN with a single hidden layer can accurately approximate any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the structure and potential of deep neural networks (DNNs) in learning continuous operators or complex systems from streams of scattered data. Here, we thus extend this theorem to DNNs. We design a new network with small generalization error, the deep operator network (DeepONet), which consists of a DNN for encoding the discrete input function space (branch net) and another DNN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, such as integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study different formulations of the input function space and its effect on the generalization error for 16 different diverse applications.},
  AUTHOR = {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang, Zhongqiang and Karniadakis, George Em},
  PUBLISHER = {Nature Publishing Group},
  YEAR = {2021},
  DOI = {10.1038/s42256-021-00302-5},
  FILE = {/home/nils/Zotero/storage/LHH75MNM/Lu et al. - 2021 - Learning nonlinear operators via DeepONet based on.pdf},
  ISSN = {2522-5839},
  ISSUE = {3},
  JOURNAL = {Nature Machine Intelligence},
  LANGID = {english},
  NUMBER = {3},
  PAGES = {218--229},
  SHORTJOURNAL = {Nat Mach Intell},
  TITLE = {Learning Nonlinear Operators via {{DeepONet}} Based on the Universal Approximation Theorem of Operators},
  VOLUME = {3},
}

@INPROCEEDINGS{luPrioriGeneralizationAnalysis2021,
  ABSTRACT = {This paper concerns the a priori generalization analysis of the Deep Ritz Method (DRM) [W. E and B. Yu, 2017], a popular neural-network-based method for solving high dimensional partial differential equations. We derive the generalization error bounds of two-layer neural networks in the framework of the DRM for solving two prototype elliptic PDEs: Poisson equation and static Schrödinger equation on the \$d\$-dimensional unit hypercube. Specifically, we prove that the convergence rates of generalization errors are independent of the dimension \$d\$, under the a priori assumption that the exact solutions of the PDEs lie in a suitable low-complexity space called spectral Barron space. Moreover, we give sufficient conditions on the forcing term and the potential function which guarantee that the solutions are spectral Barron functions. We achieve this by developing a new solution theory for the PDEs on the spectral Barron space, which can be viewed as an analog of the classical Sobolev regularity theory for PDEs.},
  AUTHOR = {Lu, Yulong and Lu, Jianfeng and Wang, Min},
  PUBLISHER = {PMLR},
  URL = {https://proceedings.mlr.press/v134/lu21a.html},
  BOOKTITLE = {Proceedings of {{Thirty Fourth Conference}} on {{Learning Theory}}},
  YEAR = {2021},
  EVENTTITLE = {Conference on {{Learning Theory}}},
  FILE = {/home/nils/Zotero/storage/CYUMABK3/Lu et al. - 2021 - A Priori Generalization Analysis of the Deep Ritz .pdf},
  ISSN = {2640-3498},
  LANGID = {english},
  PAGES = {3196--3241},
  TITLE = {A {{Priori Generalization Analysis}} of the {{Deep Ritz Method}} for {{Solving High Dimensional Elliptic Partial Differential Equations}}},
}

@inproceedings{luzLearningAlgebraicMultigrid2020,
  title = {Learning {{Algebraic Multigrid Using Graph Neural Networks}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Luz, Ilay and Galun, Meirav and Maron, Haggai and Basri, Ronen and Yavneh, Irad},
  year = {2020},
  month = nov,
  pages = {6489--6499},
  publisher = {{PMLR}},
  url =  {https://proceedings.mlr.press/v119/luz20a.html},
  issn = {2640-3498},
  urldate = {2023-07-27},
  abstract = {Efficient numerical solvers for sparse linear systems are crucial in science and engineering. One of the fastest methods for solving large-scale sparse linear systems is algebraic multigrid (AMG). The main challenge in the construction of AMG algorithms is the selection of the prolongation operator\textemdash a problem-dependent sparse matrix which governs the multiscale hierarchy of the solver and is critical to its efficiency. Over many years, numerous methods have been developed for this task, and yet there is no known single right answer except in very special cases. Here we propose a framework for learning AMG prolongation operators for linear systems with sparse symmetric positive (semi-) definite matrices. We train a single graph neural network to learn a mapping from an entire class of such matrices to prolongation operators, using an efficient unsupervised loss function. Experiments on a broad class of problems demonstrate improved convergence rates compared to classical AMG, demonstrating the potential utility of neural networks for developing sparse system solvers.},
  langid = {english},
  file = {/home/nils/Zotero/storage/MB56QQMV/Luz et al. - 2020 - Learning Algebraic Multigrid Using Graph Neural Ne.pdf}
}

@ARTICLE{margenbergStructurePreservationDeep2021,
  ABSTRACT = {The simulation of partial differential equations is a central subject of numerical analysis and an indispensable tool in science, engineering, and related fields. Existing approaches, such as finite elements, provide (highly) efficient tools but deep neural network-based techniques emerged in the last few years as an alternative with very promising results. We investigate the combination of both approaches for the approximation of the Navier-Stokes equations and to what extent structural properties such as divergence freedom can and should be respected. Our work is based on DNN-MG, a deep neural network multigrid technique, that we introduced recently and which uses a neural network to represent fine grid fluctuations not resolved by a geometric multigrid finite element solver. Although DNN-MG provides solutions with very good accuracy and is computationally highly efficient, we noticed that the neural network-based corrections substantially violate the divergence freedom of the velocity vector field. In this contribution, we discuss these findings and analyze three approaches to address the problem: a penalty term to encourage divergence freedom of the network output; a penalty term for the corrected velocity field; and a network that learns the stream function and which hence yields divergence-free corrections by construction. Our experimental results show that the third approach based on the stream function outperforms the other two and not only improves the divergence freedom but also the overall fidelity of the simulation.},
  AUTHOR = {Margenberg, Nils and Lessig, Christian and Richter, Thomas},
  year = {2021},
  DOI = {10.1553/etna_vol56s86},
  FILE = {/home/nils/Zotero/storage/EPBZELLN/Margenberg et al. - 2021 - Structure preservation for the Deep Neural Network.pdf},
  ISSN = {1068-9613, 1068-9613},
  JOURNAL = {ETNA - Electronic Transactions on Numerical Analysis},
  LANGID = {english},
  PAGES = {86--101},
  SHORTJOURNAL = {etna},
  TITLE = {Structure Preservation for the {{Deep Neural Network Multigrid Solver}}},
  VOLUME = {56},
}

@ARTICLE{margenbergNeuralNetworkMultigrid2022,
  ABSTRACT = {We present the deep neural network multigrid solver (DNN-MG) that we develop for the instationary Navier-Stokes equations. DNN-MG improves computational efficiency using a judicious combination of a geometric multigrid solver and a recurrent neural network with memory. DNN-MG uses the multi-grid method to classically solve on coarse levels while the neural network corrects interpolated solutions on fine ones, thus avoiding the increasingly expensive computations that would have to be performed there. This results in a reduction in computation time through DNN-MG's highly compact neural network. The compactness results from its design for local patches and the available coarse multigrid solutions that provides a “guide” for the corrections. A compact neural network with a small number of parameters also reduces training time and data. Furthermore, the network's locality facilitates generalizability and allows one to use DNN-MG trained on one mesh domain also on different ones. We demonstrate the efficacy of DNN-MG for variations of the 2D laminar flow around an obstacle. For these, our method significantly improves the solutions as well as lift and drag functionals while requiring only about half the computation time of a full multigrid solution. We also show that DNN-MG trained for the configuration with one obstacle can be generalized to other time dependent problems that can be solved efficiently using a geometric multigrid method.},
  AUTHOR = {Margenberg, Nils and Hartmann, Dirk and Lessig, Christian and Richter, Thomas},
  YEAR = {2022},
  DOI = {10.1016/j.jcp.2022.110983},
  FILE = {/home/nils/Zotero/storage/TBF6WSRY/Margenberg et al. - 2022 - A neural network multigrid solver for the Navier-S.pdf;/home/nils/Zotero/storage/S7S4KSTZ/S0021999122000456.html},
  ISSN = {0021-9991},
  JOURNAL = {Journal of Computational Physics},
  LANGID = {english},
  PAGES = {110983},
  SHORTJOURNAL = {Journal of Computational Physics},
  TITLE = {A Neural Network Multigrid Solver for the {{Navier-Stokes}} Equations},
  VOLUME = {460},
}

@ONLINE{margenbergDeepNeuralNetworks2021,
  ABSTRACT = {We investigate scaling and efficiency of the deep neural network multigrid method (DNN-MG). DNN-MG is a novel neural network-based technique for the simulation of the Navier-Stokes equations that combines an adaptive geometric multigrid solver, i.e. a highly efficient classical solution scheme, with a recurrent neural network with memory. The neural network replaces in DNN-MG one or multiple finest multigrid layers and provides a correction for the classical solve in the next time step. This leads to little degradation in the solution quality while substantially reducing the overall computational costs. At the same time, the use of the multigrid solver at the coarse scales allows for a compact network that is easy to train, generalizes well, and allows for the incorporation of physical constraints. Previous work on DNN-MG focused on the overall scheme and how to enforce divergence freedom in the solution. In this work, we investigate how the network size affects training and solution quality and the overall runtime of the computations. Our results demonstrate that larger networks are able to capture the flow behavior better while requiring only little additional training time. At runtime, the use of the neural network correction can even reduce the computation time compared to a classical multigrid simulation through a faster convergence of the nonlinear solve that is required at every time step.},
  AUTHOR = {Margenberg, Nils and Jendersie, Robert and Richter, Thomas and Lessig, Christian},
  YEAR = {2021},
  DOI = {10.48550/arXiv.2106.07687},
  EPRINT = {2106.07687},
  EPRINTCLASS = {cs, math},
  EPRINTTYPE = {arxiv},
  FILE = {/home/nils/Zotero/storage/3MDJJIG3/Margenberg et al. - 2021 - Deep neural networks for geometric multigrid metho.pdf;/home/nils/Zotero/storage/QG3ZQCX2/2106.html},
  PUBSTATE = {preprint},
  TITLE = {Deep Neural Networks for Geometric Multigrid Methods},
}

@ARTICLE{minakowskiPrioriPosterioriError2023,
  ABSTRACT = {We analyze neural network solutions to partial differential equations obtained with Physics Informed Neural Networks. In particular, we apply tools of classical finite element error analysis to obtain conclusions about the error of the Deep Ritz method applied to the Laplace and the Stokes equations. Further, we develop an a posteriori error estimator for neural network approximations of partial differential equations. The proposed approach is based on the dual weighted residual estimator. It is destined to serve as a stopping criterion that guarantees the accuracy of the solution independently of the design of the neural network training. The result is equipped with computational examples for Laplace and Stokes problems.},
  AUTHOR = {Minakowski, P. and Richter, T.},
  YEAR = {2023},
  DOI = {10.1016/j.cam.2022.114845},
  FILE = {/home/nils/Zotero/storage/RUCDXWV6/Minakowski and Richter - 2023 - A priori and a posteriori error estimates for the .pdf;/home/nils/Zotero/storage/G35KTIRJ/S0377042722004435.html},
  ISSN = {0377-0427},
  JOURNAL = {Journal of Computational and Applied Mathematics},
  LANGID = {english},
  PAGES = {114845},
  SHORTJOURNAL = {Journal of Computational and Applied Mathematics},
  TITLE = {A Priori and a Posteriori Error Estimates for the {{Deep Ritz}} Method Applied to the {{Laplace}} and {{Stokes}} Problem},
  VOLUME = {421},
}

@ARTICLE{mituschHybridFEMNNModels2021,
  ABSTRACT = {We present a methodology combining neural networks with physical principle constraints in the form of partial differential equations (PDEs). The approach allows to train neural networks while respecting the PDEs as a strong constraint in the optimisation as apposed to making them part of the loss function. The resulting models are discretised in space by the finite element method (FEM). The method applies to both stationary and transient as well as linear/nonlinear PDEs. We describe implementation of the approach as an extension of the existing FEM framework FEniCS and its algorithmic differentiation tool dolfin-adjoint. Through series of examples we demonstrate capabilities of the approach to recover coefficients and missing PDE operators from observations. Further, the proposed method is compared with alternative methodologies, namely, physics informed neural networks and standard PDE-constrained optimisation. Finally, we demonstrate the method on a complex cardiac cell model problem using deep neural networks.},
  AUTHOR = {Mitusch, Sebastian K. and Funke, Simon W. and Kuchta, Miroslav},
  YEAR = {2021},
  DOI = {10.1016/j.jcp.2021.110651},
  FILE = {/home/nils/Zotero/storage/FF8GX7SM/Mitusch et al. - 2021 - Hybrid FEM-NN models Combining artificial neural .pdf},
  ISSN = {0021-9991},
  JOURNAL = {Journal of Computational Physics},
  LANGID = {english},
  PAGES = {110651},
  SHORTJOURNAL = {Journal of Computational Physics},
  SHORTTITLE = {Hybrid {{FEM-NN}} Models},
  TITLE = {Hybrid {{FEM-NN}} Models: {{Combining}} Artificial Neural Networks with the Finite Element Method},
  VOLUME = {446},
}

@INCOLLECTION{paszkePyTorchImperativeStyle2019,
  AUTHOR = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  EDITOR = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d'Alché- Buc, F. and Fox, E. and Garnett, R.},
  PUBLISHER = {Curran Associates, Inc.},
  URL = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
  BOOKTITLE = {Advances in Neural Information Processing Systems 32},
  year = {2019},
  PAGES = {8024--8035},
  TITLE = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
}

@ONLINE{pathakFourCastNetGlobalDatadriven2022,
  ABSTRACT = {FourCastNet, short for Fourier ForeCasting Neural Network, is a global data-driven weather forecasting model that provides accurate short to medium-range global predictions at 0.25◦ resolution. FourCastNet accurately forecasts high-resolution, fast-timescale variables such as the surface wind speed, precipitation, and atmospheric water vapor. It has important implications for planning wind energy resources, predicting extreme weather events such as tropical cyclones, extra-tropical cyclones, and atmospheric rivers. FourCastNet matches the forecasting accuracy of the ECMWF Integrated Forecasting System (IFS), a state-of-the-art Numerical Weather Prediction (NWP) model, at short lead times for large-scale variables, while outperforming IFS for small-scale variables, including precipitation. FourCastNet generates a week-long forecast in less than 2 seconds, orders of magnitude faster than IFS. The speed of FourCastNet enables the creation of rapid and inexpensive large-ensemble forecasts with thousands of ensemble-members for improving probabilistic forecasting. We discuss how data-driven deep learning models such as FourCastNet are a valuable addition to the meteorology toolkit to aid and augment NWP models.},
  AUTHOR = {Pathak, Jaideep and Subramanian, Shashank and Harrington, Peter and Raja, Sanjeev and Chattopadhyay, Ashesh and Mardani, Morteza and Kurth, Thorsten and Hall, David and Li, Zongyi and Azizzadenesheli, Kamyar and Hassanzadeh, Pedram and Kashinath, Karthik and Anandkumar, Animashree},
  URL = {http://arxiv.org/abs/2202.11214},
  YEAR = {2022},
  EPRINT = {2202.11214},
  EPRINTCLASS = {physics},
  EPRINTTYPE = {arxiv},
  FILE = {/home/nils/Zotero/storage/JUZ97VZT/Pathak et al. - 2022 - FourCastNet A Global Data-driven High-resolution .pdf},
  LANGID = {english},
  PUBSTATE = {preprint},
  SHORTTITLE = {{{FourCastNet}}},
  TITLE = {{{FourCastNet}}: {{A Global Data-driven High-resolution Weather Model}} Using {{Adaptive Fourier Neural Operators}}},
}

@BOOK{richterFluidstructureInteractionsModels2017,
  AUTHOR = {Richter, Thomas},
  LOCATION = {Cham},
  PUBLISHER = {Springer International Publishing},
  year = {2017},
  DOI = {10.1007/978-3-319-63970-3},
  FILE = {/home/nils/Zotero/storage/HN63QFT4/Richter - 2017 - Fluid-structure Interactions Models, Analysis and.pdf},
  IDS = {Richter2017},
  ISBN = {978-3-319-63969-7 978-3-319-63970-3},
  LANGID = {english},
  SERIES = {Lecture {{Notes}} in {{Computational Science}} and {{Engineering}}},
  SHORTTITLE = {Fluid-Structure {{Interactions}}},
  TITLE = {Fluid-Structure {{Interactions}}: {{Models}}, {{Analysis}} and {{Finite Elements}}},
  VOLUME = {118},
}

@INCOLLECTION{schferBenchmarkComputationsLaminar1996,
  AUTHOR = {Schäfer, M. and Turek, S.},
  EDITOR = {Hirschel, E.H.},
  PUBLISHER = {Vieweg, Wiesbaden},
  BOOKTITLE = {Flow Simulation with High-Performance Computers {{II}}. {{DFG priority research program results 1993-1995}}},
  year = {1996},
  LIBRARY = {private},
  NUMBER = {52},
  PAGES = {547--566},
  SERIES = {Notes Numer. {{Fluid}} Mech.},
  TITLE = {Benchmark Computations of Laminar Flow around a Cylinder. ({{With}} Support by {{F}}. {{Durst}}, {{E}}. {{Krause}} and {{R}}. {{Rannacher}})},
}

@Book{zhang2023dive,
  author    = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
  title     = {Dive into Deep Learning},
  note      = {\url{https://D2L.ai}},
  publisher = {Cambridge University Press},
  year      = {2023},
}

@Article{Huang2023,
  author       = {Lei Huang and Jie Qin and Yi Zhou and Fan Zhu and Li Liu and Ling Shao},
  date         = {2023-08},
  journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title        = {Normalization Techniques in Training {DNNs}: Methodology, Analysis and Application},
  doi          = {10.1109/tpami.2023.3250241},
  number       = {8},
  pages        = {10173--10196},
  volume       = {45},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Comment{jabref-meta: databaseType:biblatex;}
