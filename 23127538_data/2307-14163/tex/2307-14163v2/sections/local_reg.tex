% !TeX root = ../MVFD_arxiv.tex


\section{Local regularity estimation approach}\label{sec_*}
The idea is to relate the functional parameters $H_1,H_2$ and $\mathbf L$ to quantities which are estimable from the data. In other words, we build estimating equations for each of the parameters we want to estimate. The parameter estimate is then obtained from the sample version of the estimating equation.  

Before proceeding with the local regularity estimation, let us discuss on the identification aspect. Definition \ref{def} is too general and does not allow to identify all the unknown parameters without further restrictions. To be more clear, let $H_1,H_2,\widetilde H_1$ and $\widetilde H_2$  be  continuously differentiable functions  taking values in $(0,1).$ Assume that  $X\in \mathcal {H}^{H_1,H_2}(\mathbf{L},\mathcal{T})$ and  $X\in \mathcal {H}^{\widetilde H_1,\widetilde H_2}(\widetilde{\mathbf L},\mathcal{T})$, for some $\mathbf{L}$ and $\widetilde{\mathbf L}$. Then necessarily  
$$
\min\{H_1(\Tt),H_2(\Tt)\} \! = \! \min\{\widetilde H_1(\Tt),\widetilde H_2(\Tt)\}\hspace{0.1cm}\text{ and } \hspace{0.1cm}\max\{H_1(\Tt),H_2(\Tt)\} \! = \! \max\{\widetilde H_1(\Tt),\widetilde H_2(\Tt)\},
$$ 
 for any $\Tt\in \cT$, and, modulo a permutation of the components,   $\mathbf{L}\equiv \widetilde{\mathbf L}$. In general, the permutation depends on the domain point $\Tt$. We deduce from these facts that, for instance, only  
$$\underline H (\Tt)=\min\{H_1(\Tt),H_2(\Tt)\}\quad \text{  and  } \quad \overline{H}(\Tt)=\max\{H_1(\Tt),H_2(\Tt)\},
$$ 
are expected to be identifiable in the general framework we consider. Concerning the components of  $\mathbf{L}$,  the identifiable quantities are provided below.   

\subsection{ Estimating equations for $\underline{H}(\Tt)$ and $\overline{H}(\Tt)$}
Let  $X\in \mathcal H^{H_1,H_2}$. 
Since $\Delta^{b}$ is negligible compared to $\Delta^{a}$ if $0< a <b$ and $\Delta$ is small, in view of Definition \ref{def} we first define  the estimation equation for $\underline{H}(\Tt)$, for some fixed $\Tt\in \mathcal T$. 

For $i=1,2$ and $\Delta$ sufficiently small, we have
$$\theta_{\Tt}^{(i)}(\Delta)= K_1^{(i)}(\Tt)\Delta^{2\underline H (\Tt)}+O(\Delta^{\widetilde{\beta}})=K_1^{(i)}(\Tt)\Delta^{2\underline H (\Tt)}+K_2^{(i)}(\Tt)\Delta^{2\overline H (\Tt)}+O(\Delta^{\overline H (\Tt)+\beta}),$$
where 
$$K_1^{(i)}(\Tt) =\left\lbrace\begin{array}{lll}
	\!\!  L_1^{(i)} (\Tt)\quad &\!\!\! \text{if }  H_1(\Tt)< H_2(\Tt)\\
	\!\! L_2^{(i)}(\Tt)\quad &\!\!\!\text{if }   H_2(\Tt)< H_1(\Tt)\\
	\!\! L_1^{(i)}(\Tt)+L_2^{(i)}(\Tt) \quad &\!\!\!\text{if }   H_1(\Tt)=H_2(\Tt) 
\end{array}\right. \!\!, \quad K_2^{(i)}(\Tt) =\left\lbrace\begin{array}{lll}
	\!\!  L_1^{(i)} (\Tt)\quad &\!\!\! \text{if }  H_1(\Tt)> H_2(\Tt)\\
	\!\! L_2^{(i)}(\Tt)\quad &\!\!\!\text{if }   H_2(\Tt)>H_1(\Tt)\\
	\!\! 0\quad &\!\!\!\text{if }   H_1(\Tt)=H_2(\Tt) 
\end{array}\right. \!\!,
$$
and 
$$
\widetilde \beta =\left\lbrace\begin{array}{lll}
	\!\! 2\overline{H}(\Tt)& \text{if}&  \underline H(\Tt)< \overline{H}(\Tt) \\
	\!\! 2\underline H(\Tt)+\beta & \text{if}&  \underline H(\Tt)=\overline{H}(\Tt) 
\end{array}\right. ,\;\; \text{  } \;\; 
$$
Related to the previous discussion on the identifiability, 
similarly to the role of $\underline H (\Tt)$ and $\overline H (\Tt)$ for $H_1(\Tt)$ and $H_2(\Tt)$, the functions $K_1^{(i)}(\Tt)$ and $K_2^{(i)}(\Tt)$, $i=1,2$
are the identifiable functionals of $\mathbf L$. More precisely, given  the order choice in the case $H_1(\Tt)\neq H_2(\Tt)$, the functions $K_1^{(i)}(\Tt)$ and $K_2^{(i)}(\Tt)$ represent  the identifiable components of $\mathbf L$. When $H_1(\Tt)=H_2(\Tt) $, only the $ L_1^{(i)}(\Tt)+L_2^{(i)}(\Tt) $ are identifiable. See also the discussion following Proposition \ref{conc_Lest}. 


Next, we define $$\gamma_{\Tt}(\Delta)=\theta^{(1)}_{\Tt}(\Delta)+\theta^{(2)}_{\Tt}(\Delta).$$
The reason for considering this quantity, instead of considering separately $\theta^{(1)}_{\Tt}(\Delta)$ and $\theta^{(2)}_{\Tt}(\Delta)$, 
is that the Hölder constant associated to $\underline{H} (\Tt)$ can vanish, and this would prevent from estimating the lower regularity exponent. On contrary,  $\gamma_{\Tt}(\Delta)$ can be written as 
\begin{multline}\label{eq:K1K2}
\gamma_{\Tt}(\Delta)=\left(K_1^{(1)}(\Tt)+K_1^{(2)}(\Tt)\right)\Delta^{2\underline H (\Tt)} +
\left(K_2^{(1)}(\Tt)+K_2^{(2)}(\Tt)\right)
\Delta^{2\overline H (\Tt)}+ O(\Delta^{ 2\overline H (\Tt) +\beta})\\=: K_1(\Tt)\Delta^{2\underline H (\Tt)}
+ K_2(\Tt)\Delta^{2\overline H (\Tt)}+O(\Delta^{2\overline H (\Tt) +\beta}),
\end{multline}
and  condition \eqref{id_L} guarantees  $K_1(\Tt), K_2(\Tt)>0$, and thus allows  to consistently estimate $\underline H (\Tt)$. We also consider 
\begin{equation}\label{eq:alpha}
	\alpha_{\Tt}(\Delta)=\left|\frac{\gamma_{\Tt}(2\Delta)}{(2\Delta)^{2\underline{H}(\Tt)}}-\frac{\gamma_{\Tt}(\Delta)}{\Delta^{2\underline{H}(\Tt)}}\right|.
\end{equation}

\quad 

\begin{proposition}\label{proprox}
Let $X$ belong to the class $ \mathcal {H}^{H_1,H_2}(\mathbf{L},\mathcal{T})$, introduced by Definition \ref{def}. Then, for any $\Tt\in\cT$, 
	\begin{equation}\label{proxy_H_low}
		\underline{H}(\Tt) = \frac{\log(\gamma_{\Tt}(2\Delta))-\log(\gamma_{\Tt}(\Delta))}{2\log(2)} + O(\Delta^{\widetilde \beta -2\underline H(\Tt)}),
	\end{equation}
	and
	\begin{equation*}
		\overline{H}(\Tt)-\underline{H}(\Tt) = \frac{\log(\alpha_{\Tt}(2\Delta))-\log(\alpha_{\Tt}(\Delta))}{2\log(2)} + O(\Delta^{ \beta }).
	\end{equation*}
\end{proposition}

\smallskip


To estimate $\underline H (\Tt)$ we thus use the dominating term on the right-hand side of the  representation \eqref{proxy_H_low} as a proxy, for which we compute an estimate. 
To build a sample counterpart of the proxy quantity, we can estimate 
$\theta_{\Tt}^{(i)}(\Delta)$ by 
\begin{equation} \label{eq_theta_hat}
\widehat{\theta}_{\Tt}^{(i)}(\Delta)= \frac{1}{N}\sum_{j=1}^{N}\left\{\widetilde{X}^{(j)}(\Tt-(\Delta/2) e_i)-\widetilde{X}^{(j)}(\Tt+(\Delta/2) e_i)\right\}^2, \quad i=1,2,
\end{equation}
where $\widetilde X^{(j)}$ is the observable approximation of $\Xn$, In the ideal, infeasible scenario where    the sheets $X^{(j)}$ are completely observed, 
%as in our infeasible first scenario, 
$\widetilde X^{(j)} = X^{(j)}$. When $X^{(j)}$ are observed  only at  some discrete points $\Tnm$,  the  $\widetilde X^{(j)}(\Tt)$'s can be obtained by simple interpolation   or using nearest neighbors. 
Finally, with noisy, discretely observed sheets,   $\widetilde X^{(j)}$  can be a pilot nonparametric estimator  of $X^{(j)}$, such as bivariate kernel smoothing, splines \emph{etc}. 


An estimator of $\gamma_{\Tt}(\Delta)$ is then given by $\widehat{\gamma}_{\Tt}(\Delta)=\widehat{\theta}_{\Tt}^{(1)}(\Delta)+\widehat{\theta}_{\Tt}^{(2)}(\Delta).$ By plugging this estimator of $\gamma_{\Tt}(\Delta)$ into \eqref{proxy_H_low}, we obtain an estimator of $\underline{H}(\Tt)$~:
\begin{equation}\label{est_under}
	\widehat{\underline{H}}(\Tt)=\left \lbrace 
	\begin{array}{cl}
		\frac{\log(\widehat{\gamma}_{\Tt}(2\Delta))-\log(\widehat{\gamma}_{\Tt}(\Delta))}{2\log(2)}\quad &\text{if}\quad \widehat{\gamma}_{\Tt}(2\Delta),\widehat{\gamma}_{\Tt}(\Delta)>0\\
		1&\text{otherwise}
	\end{array}
	\right..
\end{equation}
Moreover, replacing $\gamma_{\Tt}$ by  $\widehat{\gamma}_{\Tt}$  and $\underline{H}(\Tt)$ by  $\widehat{\underline{H}}(\Tt)$ in \eqref{eq:alpha}, we get an estimator of $\alpha_{\Tt}$~: 
\begin{equation}\label{eq:alpha_hat}
	\widehat{\alpha}_{\Tt}(\Delta)=\left \lbrace 
	\begin{array}{cl}
		\left|\frac{\widehat{\gamma}_{\Tt}(2\Delta)}{(2\Delta)^{2\widehat{\underline{H}}(\Tt)}}-\frac{\widehat{\gamma}_{\Tt}(\Delta)}{\Delta^{2\widehat{\underline{H}}(\Tt)}}\right|\quad &\text{if}\quad \frac{\widehat{\gamma}_{\Tt}(2\Delta)}{(2\Delta)^{2\widehat{\underline{H}}(\Tt)}}\ne\frac{\widehat{\gamma}_{\Tt}(\Delta)}{\Delta^{2\widehat{\underline{H}}(\Tt)}}\\
		1&\text{otherwise}.
	\end{array}
	\right..
\end{equation}
Finally,  using the second part of Proposition \ref{proprox}, we estimator an estimator of $\overline{H}(\Tt)-\underline{H}(\Tt)$ under the form 
$$
\widehat{(\overline{H}-\underline{H})}(\Tt)=\frac{\log(\widehat{\alpha}_{\Tt}(2\Delta))-\log(\widehat{\alpha}_{\Tt}(\Delta))}{2\log(2)}.
$$

%We deduce from above an issue of interest is to know whether

It will be shown below that, for the pointwise estimation of $\overline{H}$ and $\mathbf L$,  we have to distinguish between the isotropic and anisotropic cases. Here, the \emph{isotropic} and \emph{anisotropic} cases are defined locally, and correspond to  $\underline{H}(\Tt)=\overline{H}(\Tt)$ and $\underline{H}(\Tt)< \overline{H}(\Tt)$, respectively. We herein propose an estimator of $\overline{H}(\Tt)$ which  adapts to isotropy. Let us consider  the  event 
\begin{equation}\label{def_A_N}
	 A_N(\tau)=A_N(\tau; \Tt)=  \left\{ \widehat{(\overline{H}-\underline{H})}(\Tt)\geq \tau\right\},
\end{equation}
for some appropriate, small threshold $\tau>0$. 
%In the case where $\widehat{(\overline{H}-\underline{H})}(\Tt)<\tau$, we can postulate that $\underline{H}(\Tt)=\overline{H}(\Tt).$
We then define  the following estimator for $\overline{H}(\Tt)$~:
\begin{equation}\label{est_over}
	\widehat{\overline{H}}(\Tt)=\widehat{\underline{H}}(\Tt)+\widehat{(\overline{H}-\underline{H})}(\Tt)\mathbf{1}_{A_N(\tau)}.
\end{equation}
 Here, for a set $S$, $\mathbf 1_S$ denotes the indicator of $S$.   In Section \ref{info_tau}, we provide an exponential bound  for the probability that the anisotropy detection rule defined by $\mathbf{1}_{A_N(\tau)}$ fails. In particular, that indicates how small $\tau $ is allowed to be such that $\mathbf{1}_{A_N(\tau)}$ detects anisotropy with high probability. 



\subsection{Estimating equations for $\mathbf{L}(\Tt)$}

Assume for the moment that $\underline{H}(\Tt)< \overline{H}(\Tt)$. A sample-based diagnosis   procedure for detecting this situation  can be built using the results in Section \ref{info_tau} below. Without 
loss of generality, we consider  $\underline{H}(\Tt)=H_1(\Tt)$.
Let us recall that, for $i=1,2$,
$$
\theta_{\Tt}^{(i)}(\Delta)=L_1^{(i)}(\Tt)\Delta^{2H_1(\Tt)} +L_2^{(i)}(\Tt)\Delta^{2H_2(\Tt)}+O(\Delta^{2{H}_2+\beta}).
$$



\begin{proposition}\label{prop_Lest} Let $ X\in \mathcal {H}^{H_1,H_2}$.
	%(\mathbf{L},\mathcal{T})$.
	Denote $D(\Tt)= {H}_2(\Tt)-{H}_1(\Tt) >0$.
	For $i=1,2$,
	\begin{equation*}
		L_1^{(i)}(\Tt)= \frac{\theta_{\Tt}^{(i)}(\Delta)}{\Delta^{2 H_1 (\Tt)}}+O(\Delta^{2{D}(\Tt)}),
	\end{equation*}
and
	\begin{equation*}
		L_2^{(i)}(\Tt)= \frac{1}{(2^{2D(\Tt)}-1)\Delta^{2D(\Tt)}}\left|\frac{\theta_{\Tt}^{(i)}(2\Delta)}{(2\Delta)^{ 2H_1(\Tt)}}-\frac{\theta_{\Tt}^{(i)}(\Delta)}{\Delta^{ 2H_1(\Tt)}}\right|+O(\Delta^\beta).
	\end{equation*}
\end{proposition}
We denote the estimators of the local Hölder constants by
\begin{equation}\label{est_Lcomp}
\widehat{L_1^{(i)}}(\Tt), \quad \widehat{L_2^{(i)}}(\Tt), \qquad i=1,2.
\end{equation}
The estimators of $L_1^{(i)}(\Tt)$, $i=1,2$, are obtained by plugging into its dominating term derived in Proposition \ref{prop_Lest} the estimators  in \eqref{eq_theta_hat}, \eqref{est_under}. For the estimators of $L_2^{(i)}(\Tt)$, we first consider  
$$
\widehat D(\Tt) =  \widehat{(\overline{H}-\underline{H})}(\Tt) \;\;\; \text{ if } \; \widehat{(\overline{H}-\underline{H})}(\Tt)\neq 0,\quad \text{ and } \; \widehat D(\Tt) = 0 \;\text{ otherwise}.
$$
If $\widehat D(\Tt) \neq  0$,  the estimators of $L_2^{(i)}(\Tt)$, $i=1,2$, are obtained by plugging into its dominating term the estimated quantities, otherwise they are set equal to zero. 

