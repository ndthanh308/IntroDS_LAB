% !TeX root = ../MVFD_arxiv.tex



\section{Non-asymptotic results }\label{sec4}
We now derive concentration inequalities for the pointwise estimators of the  parameters $(H_1,H_2)$ and $\mathbf{L}= (L_1^{(1)},L_2^{(1)},L_1^{(2)},L_2^{(2)})$. For this purpose, we need to measure the error between each realizations $X^{(j)}$ of $X$ and its observable approximation of $\Xtp{j}$, as  defined in Section \ref{sec2}. We consider the following $\mathbb{L}^p$-risk~: 
$$
R_p (\mathfrak m) =\sup_{\Tt\in\cT }\EE[|\xi^{(j)}(\Tt)|^p] ,\qquad \xi^{(j)}(\Tt)=\Xtp{j}(\Tt)-\Xp{j}(\Tt).
$$
In general, the $\mathbb{L}^p$-risk depends on the expected number $\mathfrak m$ of observed points $\Tnm$. In the ideal scenario where $\Xp{j}$ is observed everywhere without error, $R_p \equiv 0$.
We also consider the following assumptions. Below, $B(\Tt; r)$ denote the ball of radius $r$ centered at $\Tt$. 

 


\begin{assumptionH}
	\item\label{ass_D}  Let $X$ belong to the class $ \mathcal {H}^{H_1,H_2}$,
	%(\mathbf{L},\mathcal{T})$, 
	introduced by Definition \ref{def}, and let $X^{(j)}$, $1\leq j \leq N$,  be independent realizations of $X$.
	
%		\item\label{ass_M} A constant $C_{\mathfrak m}$ exists such that 
%		$$
%		C_{\mathfrak m}^{-1} \leq \frac{M_j}{\mathfrak m}\leq C_{\mathfrak m},\quad \forall 1\leq j\leq N
%		$$		
	\item\label{ass_H1}  Three positive constants $\mathfrak{a}$, $\mathfrak{A}$ and $r$ exist such that, for any $\Tt\in\mathcal T$,  %(\textcolor{red}{introduire $\rho$})
	$$
	\EE\left| 	X^{(j)}\left(\Tt\right)-
	 X^{(j)}\left(\Ss
	 \right)\right|^{2p}\leq  \frac{p!}{2}\mathfrak{a} \mathfrak{A}^{p-2} \|\Tt-\Ss\|^{2p\underline H (\Tt)}
	\qquad \forall \Ss\in B(\Tt; r) ,\; \forall p\geq 1.
	$$
	
	\item\label{ass_H2} Two positive constants $\mathfrak{c}$ and $\mathfrak{D}$, and a function $\rho(\mathfrak m)\leq 1$, exist such that 
	$$
	R_{2p} (\mathfrak m) \leq  \frac{p!}{2}\mathfrak{c} \mathfrak{D}^{p-2}\rho(\mathfrak m)^{2p}, \qquad \forall p\geq 1,\; \forall \mathfrak m>1.
	$$
	\item\label{ass_H3} Two positive constants  $\mathfrak L $ and $\nu$ exist such that 
	$$
	R_2(\mathfrak m) \leq \mathfrak L \mathfrak m ^{-\nu},\qquad \forall \mathfrak m >1.
	$$
\end{assumptionH}

\smallskip

The condition in (H\ref{ass_H1}) imposes  sub-Gaussian  local increments for $X$.  It is satisfied by the processes in the wide class of
multi-fractional Brownian  sheet (MfBs) with a  domain-deformation, as considered  in Section \ref{BfMs}. In the case of noisy, discretely observed realizations $X^{(j)}$, the observable approximation can be obtained from existing bivariate nonparametric smoothing approaches. Under mild conditions, the standard nonparametric smoothers satisfy Assumption \ref{ass_H2}, with $\rho(\mathfrak m) =1$, and Assumption \ref{ass_H3}. See \cite{fan2016multivariate} for the case of local polynomials, and \cite{BELLONI2015} for general series estimators. In the second scenario, where the $X^{(j)}$ are observed  without noise  at discrete points $\Tnm$ in the domain $\mathcal T$, we can simply define $ \widetilde{X}^{(j)}(\Tt)$ as the value of $X^{(j)}$ at the nearest observed point $\Tnm$  to $\Tt$. To provide a simple justification that this simple choice is valid, let us consider that 
a constant $C>0$ exists such that 
$$
C^{-1} \leq  M_j/\mathfrak m\leq C,\quad \forall 1\leq j\leq N.
$$	
Then, with probability exponentially close to 1, there exists at least one point $\Tnm$ in the ball  $B(\Tt; \widetilde r)$, provided $\widetilde r = \mathfrak m^{-\delta}$, for some $\delta \in(1/2,1)$. 
Assumption \ref{ass_H2} is then implied by \ref{ass_H1} with    $\rho(\mathfrak m)=\mathfrak m^{-\delta\underline \beta}$, and $\underline \beta $ from \eqref{low_thres}.   In particular, this also guarantees \ref{ass_H3} with   $\nu = 2\delta\underline \beta$. 

 




\subsection{Concentration bounds for the regularity estimates}
% With  non-noisy sample paths observed everywhere.}

We first derive  the exponential bound for the concentration of the local regularity exponents.  On the one hand, the concentration will depend on the non-stochastic approximation error between
the true parameter and their respective dominating terms. From Proposition  \ref{proprox} these approximation errors are
\begin{equation*}
	R(\underline H )(\Tt) = 	\underline{H}(\Tt) - \frac{\log(\gamma_{\Tt}(2\Delta))-\log(\gamma_{\Tt}(\Delta))}{2\log(2)} ,
\end{equation*}
and
\begin{equation*}
	R(\overline H - \underline H )(\Tt) = \{\overline H -\underline H \}(\Tt) - \frac{\log(\alpha_{\Tt}(2\Delta))-\log(\alpha_{\Tt}(\Delta))}{2\log(2)},
\end{equation*}
respectively. We have
\begin{equation}\label{rates_eRR}
	R(\underline H )(\Tt)  = O(\Delta^{\widetilde \beta -2\underline H(\Tt)}) \quad \text{and} \quad 
	R(\overline H - \underline H )(\Tt) = 	 O(\Delta^{ \beta }).
\end{equation}
On the other hand, the concentration of the  local regularity exponents estimators will also depend on the error between the realizations of $X$ and their observable approximations $\widetilde X^{(j)}$. Finally, since we use Bernstein's inequality, the concentration will also depend on the bound of the moments  in Assumption \ref{ass_H1}. To account for these, let 
$$
\varrho(\Delta,\mathfrak m) = \max\{ \Delta^{2\underline H (\Tt)},\rho^{2}(\mathfrak m)\}^{-1}.
$$ 
Note that $\varrho(\Delta,\mathfrak m) = \Delta^{-2\underline H (\Tt)}$ in the ideal case where $\widetilde X^{(j)} = X^{(j)}$ and thus $\rho (\mathfrak m)=0$. 

\begin{proposition}\label{propCH}
Assumptions (H\ref{ass_D}) to  (H\ref{ass_H3}) hold true. Let $\widehat{\underline{H}}(\Tt) $  and $ \widehat{\overline{H}}(\Tt)$ be the estimators defined in \eqref{est_under} and \eqref{est_over}, respectively. If $\Delta$ is sufficiently small and $\mathfrak m$ sufficiently large,  constants $C_1,\dots,C_5$ exist such that,  
\begin{equation}\label{eq:cdt_eps}
\forall \varepsilon,\tau  \in (0,1) \quad \text{satisfying} \quad \max \{ |\log(\Delta)| |R(\underline H )(\Tt)|, \;|R(\overline H - \underline H )(\Tt)  |\}\leq \varepsilon \leq 2\tau,
\end{equation}
then
\begin{equation}\label{eq:conc-Hhat-around-H_main}
	\mathbb{P}\left[
	|\underline{\widehat{H}}(\Tt)-\underline{H}(\Tt)|\geq 
	\varepsilon 
	\right]
	\leq  p_1,
\end{equation}
and 
\begin{equation}\label{eq:concentration-overlineH_main}
	\PP\left[\left|\widehat{\overline{H}}(\Tt)-\overline{H}(\Tt)\right|\geq \varepsilon\right] 
	\leq C_3\{p_1+p_2+p_3\},
\end{equation}
with
\begin{align}
	p_1&= C_1\exp \left(-C_2N \times \varepsilon^2 \times \Delta ^{4\underline{H}(\Tt)}\varrho(\Delta,\mathfrak m)\right),\\
	p_2 &=	
	\exp\left[ - C_4N\times \varepsilon^2\times \frac{\Delta^{4\overline{H}(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}\Delta^{4D(\Tt)}
	\right]\mathbf1_{\{\underline H(\Tt)<\overline H (\Tt)\}},
	\\ p_3&=  \exp\left[
	- C_5N\times \tau^2 \times \frac{\Delta^{4\overline{H}(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}\Delta^{4D(\Tt)}
	\right],
\end{align}
where 
$$
D(\Tt)= \overline{H}(\Tt)-\underline{H}(\Tt) \quad \text{ and }  \quad 
\varrho(\Delta,\mathfrak m) = \max\{ \Delta^{2\underline H (\Tt)},\rho(\mathfrak m)^{2}\}^{-1}.
$$ 
\end{proposition}

\medskip 

The term $p_2$ is specific to the anisotropic case, it disappears when $\underline H(\Tt)=\overline H (\Tt)$. We next derive  the bounds for the concentration of the local Hölder constants' estimators. 
In the case where $\underline{H}(\Tt) \neq \overline{H}(\Tt)$, without loss of generality, we set
$$
\underline{H}(\Tt)=H_1(\Tt) < H_2(\Tt)= \overline{H}(\Tt),
$$
such that $L_1^{(1)}(\Tt)$ and $L_1^{(2)}(\Tt)$ are the Hölder constants corresponding to $\underline{H}(\Tt)$.
et 
\begin{equation*}
	R(L_1^{(i)})(\Tt) = 	L_1^{(i)}(\Tt) -\frac{\theta_{\Tt}^{(i)}(\Delta)}{\Delta^{2 H_1 (\Tt)}}=O(\Delta^{2{D}(\Tt)}),
\end{equation*}
and
\begin{equation*}
	R(L_2^{(i)})(\Tt) = 	L_2^{(i)}(\Tt) - \frac{1}{(2^{2D(\Tt)}-1)\Delta^{2D(\Tt)}}\left|\frac{\theta_{\Tt}^{(i)}(2\Delta)}{(2\Delta)^{ 2H_1(\Tt)}}-\frac{\theta_{\Tt}^{(i)}(\Delta)}{\Delta^{ 2H_1(\Tt)}}\right| = O(\Delta^\beta), \quad i=1,2.
\end{equation*}

\medskip

\begin{proposition}\label{conc_Lest}
Assume that the conditions of Proposition \ref{propCH} hold true. Then, for the estimators in \eqref{est_Lcomp}, positive constants $\mathfrak C_1,...,\mathfrak C_4$ exists such that,  for $i=1,2$, and \color{black}	
\begin{equation}\label{eq:cdt_epsL}
	\forall \varepsilon   \in (0,1)  \text{ satisfying }  \max \left\{|R(L_1^{(i)})(\Tt)|,\; |\log(\Delta)| |R(\underline H )(\Tt)|, \; |R(\overline H \!- \!\underline H )(\Tt)|  \right\}\leq \varepsilon ,
\end{equation}
 we have 

\begin{equation}\label{eq:conc_L1_main}
\PP\left(\left|\widehat{L_1^{(i)}}(\Tt)-L_1^{(i)}(\Tt)\right| \geq \varepsilon \right) \leq 
\mathfrak C_1 \exp\left(
	- \mathfrak C_2 N \times \varepsilon^2\times \frac{\Delta^{4\underline H(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}
	\right).
\end{equation} 
 Moreover,  if in addition $ |R(L_2^{(i)})(\Tt)|\leq \varepsilon$, $i=1,2$ then
\begin{multline}\label{eq:conc_L2_main}
	\PP\left(\left|\widehat{L_2^{(i)}}(\Tt)-L_2^{(i)}(\Tt)\right|
	\geq \varepsilon \right) \\ \leq\mathfrak  C_3\exp\left(
	-\mathfrak C_4N\times \varepsilon\Delta^{4D(\Tt)}\min\{\varepsilon,\Delta^{4D(\Tt)}\}
	\times \frac{\Delta^{4\overline H (\Tt)}\varrho(\Delta,\mathfrak m)}{\log^4(\Delta)}\times 
(2^{2D(\Tt)}-1)^2	\right).
\end{multline} 
\end{proposition}


\medskip

The second exponential bound in Proposition \ref{conc_Lest} becomes trivial when 
$D(\Tt)=0$, and this reveals that the case $H_1(\Tt)=H_2(\Tt)$  requires special attention. 
In  this case,  the estimator proposed for  $L_1^{(i)}(\Tt)$  becomes an estimator of $L_1^{(i)}(\Tt)+L_2^{(i)}(\Tt)$, $i=1,2$. 
 The indicator of the set defined in \eqref{def_A_N} provides a tool for detecting whether $H_1(\Tt)=H_2(\Tt)$ or not, given a data set. In the following, we investigate  the risk associated to this diagnosis tool.


\subsection{A risk bound for the anisotropy detection}\label{info_tau}
Assume without loss of generality that $H_1(\Tt)\leq H_2(\Tt)$. 
Equation \eqref{eq:K1K2} then becomes 
\begin{multline}
\gamma_{\Tt}(\Delta)=\theta_{\Tt}^{(1)}(\Delta)+\theta_{\Tt}^{(2)}(\Delta)\\
=\left(L_1^{(1)}(\Tt)+L_1^{(2)}(\Tt)\right)\Delta^{2{H}_1(\Tt)} +\left(L_2^{(1)}(\Tt)+L_2^{(2)}(\Tt)\right)\Delta^{2{H}_2(\Tt)}+O(\Delta^{2{H}_2(\Tt)+\beta})\\ 
=K_1(\Tt)\Delta^{2H_1(\Tt)}+K_2(\Tt)\Delta^{2H_2(\Tt)} +O(\Delta^{2H_2(\Tt) +\beta}).
\end{multline}
We can now write
$$
\frac{\log(\alpha_{\Tt}(2\Delta))-\log(\alpha_{\Tt}(\Delta))}{2\log2}= D(\Tt)+O\left(\Delta^\beta\right).
$$
Therefore, if $D(\Tt)=H_2(\Tt)- H_1(\Tt)=0$, we get 
$$
\frac{\log(\alpha_{\Tt}(2\Delta))-\log(\alpha_{\Tt}(\Delta))}{2\log2}= O\left(\Delta^\beta\right).
%= o(\Delta^{a}),
$$ 
%for any $0<a<\beta$. 
We deduce that,  for the event $A_N(\tau)$
% $$
% A_N(\tau)=\left\{ \widehat{(\overline{H}-\underline{H})}(\Tt)\geq \tau\right\}, 
% $$
introduced in \eqref{def_A_N},  we have to  choose $\tau$ such that $\Delta = o(\tau^{1/\beta})$. The following result proposes an exponential bound for the risk associated to the rule defined by the indicator $\mathbf{1}_{A_N(\tau)}$ in the definition \eqref{est_over}. 




\medskip


\begin{proposition}\label{prop5_simple} Assume that the conditions of Proposition \ref{propCH} hold true. Let 
	$$  \max \{ |\log(\Delta)| |R(\underline H )(\Tt)|, \;|R(\overline H - \underline H )(\Tt)  |\}\leq  2\tau  \leq \left\{ \overline H(\Tt)-\underline H(\Tt)\right\} + \mathbf{1}_{\{\underline H(\Tt)=\overline H(\Tt)\}} .
	$$
If $\Delta$ is sufficiently small and $\mathfrak m$ sifficiently large, for $	A_N(\tau)$ defined in \eqref{def_A_N}, we have 
$$
%\PP(A_N(\tau))\mathbf{1}_{\underline H(\Tt)=\overline H(\Tt)}+\PP\left(\Bar{A}_N(\tau)\right)\mathbf{1}_{\underline H(\Tt)\ne\overline H(\Tt)}
\PP\left( \mathbf{1}_{A_N(\tau)}\neq \mathbf{1}_{\{\underline H(\Tt)<\overline H(\Tt)\}}\right)
\leq  C_3\exp\left[
	- C_5N\times \tau^2 \times \frac{\Delta^{4\overline{H}(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}  \Delta^{4D(\Tt)}
	\right],
$$
where $ C_3$ and $ C_5$ are the positive constants defined as in Proposition \ref{propCH}.
\end{proposition}

For a choice of $\Delta$,  Proposition \ref{prop5_simple} allows to determine the rate of decrease for $\tau$ such that  the indicator of $A_N(\tau)$  detects with high accuracy whether $H_1(\Tt)=H_2(\Tt)$ or not. The fastest rate  depends on the approximation errors \eqref{rates_eRR}, which are characteristics of the process $X$. 
