% !TeX root = ../MVFD_arxiv.tex


\section{Local regularity estimation approach}\label{sec_*}
The idea is to relate the functional parameters $H_1,H_2$ and $\mathbf L$ to quantities easily estimable from the data. In other words, we build estimating equations for each of the parameters we want to estimate. The parameter estimate is then obtained from the sample version of the estimating equation.  
However, Definition \ref{def} is too general and does not allow to identify all the unknown parameters without further restrictions, which are discussed  below. 


\subsection{Identification}\label{identification}
Let $H_1,H_2,\widetilde H_1$ and $\widetilde H_2$  be  continuously differentiable functions  taking values in $(0,1).$ Assume that  $X\in \mathcal {H}^{H_1,H_2}(\mathbf{L},\mathcal{T})$ and  $X\in \mathcal {H}^{\widetilde H_1,\widetilde H_2}(\widetilde{\mathbf L},\mathcal{T})$, for some $\mathbf{L}$ and $\widetilde{\mathbf L}$. Then necessarily  
$$
\min\{H_1(\Tt),H_2(\Tt)\}=\min\{\widetilde H_1(\Tt),\widetilde H_2(\Tt)\}\quad \text{ and } \quad \max\{H_1(\Tt),H_2(\Tt)\}=\max\{\widetilde H_1(\Tt),\widetilde H_2(\Tt)\},
$$ 
$\forall \Tt\in\cT$, and, modulo a permutation of the components,   $\mathbf{L}\equiv \widetilde{\mathbf L}$. In general, the permutation depends on the domain point $\Tt$. We deduce from these facts that only  
$$\underline H (\Tt)=\min\{H_1(\Tt),H_2(\Tt)\}\quad \text{  and  } \quad \overline{H}(\Tt)=\max\{H_1(\Tt),H_2(\Tt)\},
$$ 
are identifiable in the general framework we consider. 

\subsection{ Estimating equations for $\underline{H}(\Tt)$ and $\overline{H}(\Tt)$}
Let  $X\in \mathcal H^{H_1,H_2}$. 
Since $\Delta^{b}$ is negligible compared to $\Delta^{a}$ if $0< a <b$ and $\Delta$ is small, in view of Definition \ref{def} we first define  the estimation equation for $\underline{H}(\Tt)$, for some fixed $\Tt\in \mathcal T$. 

For $i=1,2$ and $\Delta$ sufficiently small, we have
$$\theta_{\Tt}^{(i)}(\Delta)= K_1^{(i)}(\Tt)\Delta^{2\underline H (\Tt)}+O(\Delta^{\widetilde{\beta}})=K_1^{(i)}(\Tt)\Delta^{2\underline H (\Tt)}+K_2^{(i)}(\Tt)\Delta^{2\overline H (\Tt)}+O(\Delta^{\overline H (\Tt)+\beta}),$$
where 
$$K_1^{(i)}(\Tt) =\left\lbrace\begin{array}{lll}
	\!\!  L_1^{(i)} (\Tt)\quad &\!\!\! \text{if }  H_1(\Tt)< H_2(\Tt)\\
	\!\! L_2^{(i)}(\Tt)\quad &\!\!\!\text{if }   H_2(\Tt)< H_1(\Tt)\\
	\!\! L_1^{(i)}(\Tt)+L_2^{(i)}(\Tt) \quad &\!\!\!\text{if }   H_1(\Tt)=H_2(\Tt) 
\end{array}\right. \!\!, \quad K_2^{(i)}(\Tt) =\left\lbrace\begin{array}{lll}
	\!\!  L_1^{(i)} (\Tt)\quad &\!\!\! \text{if }  H_1(\Tt)> H_2(\Tt)\\
	\!\! L_2^{(i)}(\Tt)\quad &\!\!\!\text{if }   H_2(\Tt)>H_1(\Tt)\\
	\!\! 0\quad &\!\!\!\text{if }   H_1(\Tt)=H_2(\Tt) 
\end{array}\right. \!\!,
$$
and 
$$
\widetilde \beta =\left\lbrace\begin{array}{lll}
	\!\! 2\overline{H}(\Tt)& \text{if}&  \underline H(\Tt)< \overline{H}(\Tt) \\
	\!\! 2\underline H(\Tt)+\beta & \text{if}&  \underline H(\Tt)=\overline{H}(\Tt) 
\end{array}\right. ,\;\; \text{  } \;\; 
$$
Next, we define $$\gamma_{\Tt}(\Delta)=\theta^{(1)}_{\Tt}(\Delta)+\theta^{(2)}_{\Tt}(\Delta).$$
The reason for which we are interested in this quantity, instead of considering separately $\theta^{(1)}_{\Tt}(\Delta)$ and $\theta^{(2)}_{\Tt}(\Delta)$, 
is that the HÃ¶lder constant associated to $\underline{H} (\Tt)$ can vanish, and this would prevent from estimating the lower regularity exponent. On contrary,  $\gamma_{\Tt}(\Delta)$ can be written as 
\begin{multline}\label{eq:K1K2}
\gamma_{\Tt}(\Delta)=\left(K_1^{(1)}(\Tt)+K_1^{(2)}(\Tt)\right)\Delta^{2\underline H (\Tt)} +
\left(K_2^{(1)}(\Tt)+K_2^{(2)}(\Tt)\right)
\Delta^{2\overline H (\Tt)}+ O(\Delta^{ 2\overline H (\Tt) +\beta})\\=: K_1(\Tt)\Delta^{2\underline H (\Tt)}
+ K_2(\Tt)\Delta^{2\overline H (\Tt)}+O(\Delta^{2\overline H (\Tt) +\beta}),
\end{multline}
and  condition \eqref{id_L} guarantees  $K_1(\Tt), K_2(\Tt)>0$, and thus allows  to consistently estimate $\underline H (\Tt)$. We also consider 
\begin{equation}\label{eq:alpha}
	\alpha_{\Tt}(\Delta)=\left|\frac{\gamma_{\Tt}(2\Delta)}{(2\Delta)^{2\underline{H}(\Tt)}}-\frac{\gamma_{\Tt}(\Delta)}{\Delta^{2\underline{H}(\Tt)}}\right|.
\end{equation}



\begin{proposition}\label{proprox}
Let $X$ belong to the class $ \mathcal {H}^{H_1,H_2}(\mathbf{L},\mathcal{T})$, introduced by Definition \ref{def}. Then, for any $\Tt\in\cT$, 
	\begin{equation}\label{proxy_H_low}
		\underline{H}(\Tt) = \frac{\log(\gamma_{\Tt}(2\Delta))-\log(\gamma_{\Tt}(\Delta))}{2\log(2)} + O(\Delta^{\widetilde \beta -2\underline H(\Tt)}),
	\end{equation}
	and
	\begin{equation*}
		\overline{H}(\Tt)-\underline{H}(\Tt) = \frac{\log(\alpha_{\Tt}(2\Delta))-\log(\alpha_{\Tt}(\Delta))}{2\log(2)} + O(\Delta^{ \beta }).
	\end{equation*}
\end{proposition}

\quad


To estimate $\underline H (\Tt)$ we thus use the dominating term on the right-hand side of the  representation \eqref{proxy_H_low} as a proxy, for which we compute an estimate. 
To build a sample counterpart of the proxy quantity, we can estimate 
$\theta_{\Tt}^{(i)}(\Delta)$ by 
\begin{equation} \label{eq_theta_hat}
\widehat{\theta}_{\Tt}^{(i)}(\Delta)= \frac{1}{N}\sum_{j=1}^{N}\left\{\widetilde{X}^{(j)}(\Tt-(\Delta/2) e_i)-\widetilde{X}^{(j)}(\Tt+(\Delta/2) e_i)\right\}^2, \quad i=1,2,
\end{equation}
where $\widetilde X^{(j)}$ is the observable approximation of $\Xn$, as defined in Section \ref{sec:data}.
Therefore, an estimator of $\gamma_{\Tt}(\Delta)$ is given by $\widehat{\gamma}_{\Tt}(\Delta)=\widehat{\theta}_{\Tt}^{(1)}(\Delta)+\widehat{\theta}_{\Tt}^{(2)}(\Delta).$ By plugging this estimator of $\gamma_{\Tt}(\Delta)$ into \eqref{proxy_H_low}, we obtain an estimator of $\underline{H}(\Tt)$~:
\begin{equation}\label{est_under}
	\widehat{\underline{H}}(\Tt)=\left \lbrace 
	\begin{array}{cl}
		\frac{\log(\widehat{\gamma}_{\Tt}(2\Delta))-\log(\widehat{\gamma}_{\Tt}(\Delta))}{2\log(2)}\quad &\text{if}\quad \widehat{\gamma}_{\Tt}(2\Delta),\widehat{\gamma}_{\Tt}(\Delta)>0\\
		1&\text{otherwise}
	\end{array}
	\right..
\end{equation}
Moreover, replacing $\gamma_{\Tt}$ by its estimator $\widehat{\gamma}_{\Tt}$, and $\underline{H}(\Tt)$ by its estimator $\widehat{\underline{H}}(\Tt)$, we get an estimator of $\alpha_{\Tt}$~: 
\begin{equation}\label{eq:alpha_hat}
	\widehat{\alpha}_{\Tt}(\Delta)=\left \lbrace 
	\begin{array}{cl}
		\left|\frac{\widehat{\gamma}_{\Tt}(2\Delta)}{(2\Delta)^{2\widehat{\underline{H}}(\Tt)}}-\frac{\widehat{\gamma}_{\Tt}(\Delta)}{\Delta^{2\widehat{\underline{H}}(\Tt)}}\right|\quad &\text{if}\quad \frac{\widehat{\gamma}_{\Tt}(2\Delta)}{(2\Delta)^{2\widehat{\underline{H}}(\Tt)}}\ne\frac{\widehat{\gamma}_{\Tt}(\Delta)}{\Delta^{2\widehat{\underline{H}}(\Tt)}}\\
		1&\text{otherwise}.
	\end{array}
	\right..
\end{equation}
Finally,  using the second part of Proposition \ref{proprox}, we obtain an estimator of $\overline{H}(\Tt)-\underline{H}(\Tt)$ under the form 
$$
\widehat{(\overline{H}-\underline{H})}(\Tt)=\frac{\log(\widehat{\alpha}_{\Tt}(2\Delta))-\log(\widehat{\alpha}_{\Tt}(\Delta))}{2\log(2)}.
$$

%We deduce from above an issue of interest is to know whether

It will be shown below that, for the pointwise estimation of $\overline{H}$ and $\mathbf L$,  we have to distinguish between the isotropic and anisotropic cases. Here, the \emph{isotropic} and \emph{anisotropic} cases are defined locally, and correspond to  $\underline{H}(\Tt)=\overline{H}(\Tt)$ and $\underline{H}(\Tt)< \overline{H}(\Tt)$, respectively. We herein propose an estimator of $\overline{H}(\Tt)$ which  adapts to isotropy. Let us consider  the  event 
\begin{equation}\label{def_A_N}
	 A_N(\tau)=A_N(\tau; \Tt)=  \left\{ \widehat{(\overline{H}-\underline{H})}(\Tt)\geq \tau\right\},
\end{equation}
for some appropriate, small threshold $\tau>0$. 
%In the case where $\widehat{(\overline{H}-\underline{H})}(\Tt)<\tau$, we can postulate that $\underline{H}(\Tt)=\overline{H}(\Tt).$
We then define  the following estimator for $\overline{H}(\Tt)$~:
\begin{equation}\label{est_over}
	\widehat{\overline{H}}(\Tt)=\widehat{\underline{H}}(\Tt)+\widehat{(\overline{H}-\underline{H})}(\Tt)\mathbf{1}_{A_N(\tau)}.
\end{equation}
 Here, for a set $S$, $\mathbf 1_S$ denotes the indicator of $S$.   In Section \ref{info_tau}, we provide an exponential bound  for the probability that the anisotropy detection rule defined by $\mathbf{1}_{A_N(\tau)}$ is correct. In particular, this provides insight on how small $\tau $ is allowed to be such that $\mathbf{1}_{A_N(\tau)}$ detects anisotropy with high probability. 








\subsection{Estimating equations for $\mathbf{L}(\Tt)$}

Assume for the moment that $\underline{H}(\Tt)< \overline{H}(\Tt)$. A sample-based diagnosis   procedure for detecting this situation  can be built using the results in Section \ref{info_tau} below. Without 
loss of generality, we consider  $\underline{H}(\Tt)=H_1(\Tt)$.
Let us recall that, for $i=1,2$,
$$
\theta_{\Tt}^{(i)}(\Delta)=L_1^{(i)}(\Tt)\Delta^{2H_1(\Tt)} +L_2^{(i)}(\Tt)\Delta^{2H_2(\Tt)}+O(\Delta^{2{H}_2+\beta}).
$$

\medskip

\begin{proposition}\label{prop_Lest} Let $ X\in \mathcal {H}^{H_1,H_2}$.
	%(\mathbf{L},\mathcal{T})$.
	Denote $D(\Tt)= {H}_2(\Tt)-{H}_1(\Tt) >0$.
	For $i=1,2$,
	\begin{equation*}
		L_1^{(i)}(\Tt)= \frac{\theta_{\Tt}^{(i)}(\Delta)}{\Delta^{2 H_1 (\Tt)}}+O(\Delta^{2{D}(\Tt)}),
	\end{equation*}
and
	\begin{equation*}
		L_2^{(i)}(\Tt)= \frac{1}{(2^{2D(\Tt)}-1)\Delta^{2D(\Tt)}}\left|\frac{\theta_{\Tt}^{(i)}(2\Delta)}{(2\Delta)^{ 2H_1(\Tt)}}-\frac{\theta_{\Tt}^{(i)}(\Delta)}{\Delta^{ 2H_1(\Tt)}}\right|+O(\Delta^\beta).
	\end{equation*}
\end{proposition}

