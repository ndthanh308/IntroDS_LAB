% !TeX root = ../MVFD_arxiv.tex



\section{Non-asymptotic results }\label{sec4}
We now derive concentration inequalities for the estimators of the  parameters $(H_1,H_2)$ and $\mathbf{L}= (L_1^{(1)},L_2^{(1)},L_1^{(2)},L_2^{(2)})$. For this purpose, we need to measure the error between the realizations of $X$ and the observable approximation of $\Xp{j}$, as  defined in Section \ref{sec2}. We consider the following $\mathbb{L}^p$-risk~: 
$$
R_p (\mathfrak m) =\sup_{\Tt\in\cT }\EE[|\xi^{(j)}(\Tt)|^p] ,\qquad \xi^{(j)}(\Tt)=\Xtp{j}(\Tt)-\Xp{j}(\Tt).
$$
In general, the $\mathbb{L}^p$-risk depends on the expected number $\mathfrak m$ of observed points $\Tnm$. In the ideal scenario where $\Xp{j}$ is observed everywhere without error, $R_p \equiv 0$.
We also consider the following assumptions. Below, $B(\Tt; r)$ denote the ball of radius $r$ centered at $\Tt$. 


\begin{assumptionH}
	\item\label{ass_D}  Let $X$ belong to the class $ \mathcal {H}^{H_1,H_2}$,
	%(\mathbf{L},\mathcal{T})$, 
	introduced by Definition \ref{def}, and let $X^{(j)}$, $1\leq j \leq N$,  be independent realizations of $X$.
	
%		\item\label{ass_M} A constant $C_{\mathfrak m}$ exists such that 
%		$$
%		C_{\mathfrak m}^{-1} \leq \frac{M_j}{\mathfrak m}\leq C_{\mathfrak m},\quad \forall 1\leq j\leq N
%		$$		
	\item\label{ass_H1}  Three positive constants $\mathfrak{a}$, $\mathfrak{A}$ and $r$ exist such that, for any $\Tt\in\mathcal T$,  %(\textcolor{red}{introduire $\rho$})
	$$
	\EE\left| 	X^{(j)}\left(\Tt\right)-
	 X^{(j)}\left(\Ss
	 \right)\right|^{2p}\leq  \frac{p!}{2}\mathfrak{a} \mathfrak{A}^{p-2} \|\Tt-\Ss\|^{2p\underline H (\Tt)}
	\qquad \forall \Ss\in B(\Tt; r) ,\; \forall p\geq 1.
	$$
	
	\item\label{ass_H2} Two positive constants $\mathfrak{c}$ and $\mathfrak{D}$, and a function $\rho(\mathfrak m)\leq 1$, exist such that 
	$$
	R_{2p} (\mathfrak m) \leq  \frac{p!}{2}\mathfrak{c} \mathfrak{D}^{p-2}\rho(\mathfrak m)^{2p}, \qquad \forall p\geq 1,\; \forall \mathfrak m>1.
	$$
	\item\label{ass_H3} Two positive constants  $\mathfrak L $ and $\nu$ exist such that 
	$$
	R_2(\mathfrak m) \leq \mathfrak L \mathfrak m ^{-\nu},\qquad \forall \mathfrak m >1.
	$$
\end{assumptionH}

\quad

The condition in (H\ref{ass_H1}) imposes  sub-Gaussian  local increments for $X$.  It is satisfied by the processes in the wide class of
multi-fractional Brownian  sheet (MfBs) with a  domain-deformation, as considered  in Section \ref{BfMs}. In the case of noisy, discretely observed realizations $X^{(j)}$, the observable approximation can be obtained from existing bivariate nonparametric smoothing approaches. The standard nonparametric smoothers satisfy the mild conditions in Assumptions  (H\ref{ass_H2}) and (H\ref{ass_H3}). See \cite{fan2016multivariate} for the case of local polynomials, and \cite{BELLONI2015} for general series estimators. In the second scenario, where the $X^{(j)}$ are observed  without noise  at discrete points $\Tnm$ in the domain $\mathcal T$, we can simply define $ \widetilde{X}^{(j)}(\Tt)$ as the value of $X^{(j)}$ at the nearest observed point $\Tnm$  to $\Tt$. To provide a simple justification that this simple choice is valid, let us consider that 
a constant $C>0$ exists such that 
$$
C^{-1} \leq  M_j/\mathfrak m\leq C,\quad \forall 1\leq j\leq N.
$$	
Then, with probability exponentially close to 1,there exists at least one point $\Tnm$ in the ball  $B(\Tt; \widetilde r)$, provided $\widetilde r = \mathfrak m^{-\delta}$, for some $\delta \in(1/2,1)$. 
Assumption (H\ref{ass_H2}) is then implied by (H\ref{ass_H1}) with $\rho(\mathfrak m)$ a negative power of $\mathfrak m$. In particular, this also guarantees  (H\ref{ass_H3}).

 




\subsection{Concentration bounds for the regularity estimates}
% With  non-noisy sample paths observed everywhere.}

We first derive  the exponential bound for the concentration of the local regularity exponents. 

\begin{proposition}\label{propCH}
Assumptions (H\ref{ass_D}) to  (H\ref{ass_H3}) hold true. Let $\widehat{\underline{H}}(\Tt) $  and $ \widehat{\overline{H}}(\Tt)$ be the estimators defined in \eqref{est_under} and \eqref{est_over}, respectively. 
Five positive constants $C_1,\dots,C_5$ exist such that,  $\forall \varepsilon \in (0,1)$,
\begin{equation}\label{eq:conc-Hhat-around-H_main}
	\mathbb{P}\left[
	|\underline{\widehat{H}}(\Tt)-\underline{H}(\Tt)|\geq 
	\varepsilon 
	\right]
	\leq  p_1,
\end{equation}
and 
\begin{equation}\label{eq:concentration-overlineH_main}
	\PP\left[\left|\widehat{\overline{H}}(\Tt)-\overline{H}(\Tt)\right|\geq \varepsilon\right] 
	\leq C_3\{p_1+p_2+p_3\},
\end{equation}
with
\begin{align}
	p_1&= C_1\exp \left(-C_2N \times \varepsilon^2 \times \Delta ^{4\underline{H}(\Tt)}\varrho(\Delta,\mathfrak m)\right),\\
	p_2 &=	
	\exp\left[ - C_4N\times \varepsilon^2\times \frac{\Delta^{4\overline{H}(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}\Delta^{4D(\Tt)}
	\right]\mathbf1_{\{\underline H(\Tt)<\overline H (\Tt)\}},
	\\ p_3&=  \exp\left[
	- C_5N\times \tau^2 \times \frac{\Delta^{4\overline{H}(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}\Delta^{4D(\Tt)}
	\right],
\end{align}
where 
$$
D(\Tt)= \overline{H}(\Tt)-\underline{H}(\Tt) \quad \text{ and }  \quad 
\varrho(\Delta,\mathfrak m) = \max\{ \Delta^{2\underline H (\Tt)},\rho(\mathfrak m)^{2}\}^{-1}.
$$ 
\end{proposition}

\quad

The term $p_2$ is specific to the anisotropic case, it disappears when $\underline H(\Tt)=\overline H (\Tt)$. We next derive  the bounds for the concentration of the local Hölder constants' estimators. 
In the case where $\underline{H}(\Tt) \neq \overline{H}(\Tt)$, without loss of generality, let 
$$
\underline{H}(\Tt)=H_1(\Tt) < H_2(\Tt)= \overline{H}(\Tt),
$$
such that $L_1^{(1)}(\Tt)$ and $L_1^{(2)}(\Tt)$ are the Hölder constants corresponding to $\underline{H}(\Tt)$.
Let 
$$
\widehat{L_1^{(i)}}(\Tt), \quad \widehat{L_2^{(i)}}(\Tt), \qquad i=1,2,
$$
be the estimators obtained from Proposition \ref{prop_Lest} after replacing the theoretical quantities by the estimators defined above. 

\medskip

\begin{proposition}\label{conc_Lest}
Assume that the conditions of Proposition \ref{propCH} hold true. Then, four positive constants $\mathfrak C_1,...,\mathfrak C_4$ exists such that, for any $\varepsilon\in (0,1)$, and $i=1,2$, we have  

\begin{equation}\label{eq:conc_L1_main}
\PP\left(\left|\widehat{L_1^{(i)}}(\Tt)-L_1^{(i)}(\Tt)\right| \geq \varepsilon \right) \leq 
\mathfrak C_1 \exp\left(
	- \mathfrak C_2 N \times \varepsilon^2\times \frac{\Delta^{4\underline H(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}
	\right),
\end{equation} 
and 
\begin{multline}\label{eq:conc_L2_main}
	\PP\left(\left|\widehat{L_2^{(i)}}(\Tt)-L_2^{(i)}(\Tt)\right|
	\geq \varepsilon \right) \\ \leq\mathfrak  C_3\exp\left(
	-\mathfrak C_4N\times \varepsilon\Delta^{4D(\Tt)}\min\{\varepsilon,\Delta^{4D(\Tt)}\}
	\times \frac{\Delta^{4\overline H (\Tt)}\varrho(\Delta,\mathfrak m)}{\log^4(\Delta)}\times 
(2^{2D(\Tt)}-1)^2	\right).
\end{multline} 
\end{proposition}


\quad 

The second exponential bound in Proposition \ref{conc_Lest} becomes trivial when 
$D(\Tt)=0$, and this reveals that the case $H_1(\Tt)=H_2(\Tt)$  requires special attention. 
In  this case,  the estimator proposed for  $L_1^{(i)}(\Tt)$  becomes an estimator of $L_1^{(i)}(\Tt)+L_2^{(i)}(\Tt)$, $i=1,2$. 
 The indicator of the set defined in \eqref{def_A_N} provides a tool for detecting whether $H_1(\Tt)=H_2(\Tt)$ or not, given a data set. In the following, we investigate  the risk associated to this diagnosis tool.


\subsection{A risk bound for the anisotropy detection}\label{info_tau}
Assume without loss of generality that $H_1(\Tt)\leq H_2(\Tt)$. 
Equation \eqref{eq:K1K2} then becomes 
\begin{multline}
\gamma_{\Tt}(\Delta)=\theta_{\Tt}^{(1)}(\Delta)+\theta_{\Tt}^{(2)}(\Delta)\\
=\left(L_1^{(1)}(\Tt)+L_1^{(2)}(\Tt)\right)\Delta^{2{H}_1(\Tt)} +\left(L_2^{(1)}(\Tt)+L_2^{(2)}(\Tt)\right)\Delta^{2{H}_2(\Tt)}+O(\Delta^{2{H}_2(\Tt)+\beta})\\ 
=K_1(\Tt)\Delta^{2H_1(\Tt)}+K_2(\Tt)\Delta^{2H_2(\Tt)} +O(\Delta^{2H_2(\Tt) +\beta}).
\end{multline}
We can now write
$$
\frac{\log(\alpha_{\Tt}(2\Delta))-\log(\alpha_{\Tt}(\Delta))}{2\log2}= D(\Tt)+O\left(\Delta^\beta\right).
$$
Therefore, if $D(\Tt)=H_2(\Tt)- H_1(\Tt)=0$, we get 
$$
\frac{\log(\alpha_{\Tt}(2\Delta))-\log(\alpha_{\Tt}(\Delta))}{2\log2}= O\left(\Delta^\beta\right).
%= o(\Delta^{a}),
$$ 
%for any $0<a<\beta$. 
We deduce that,  for the event $A_N(\tau)$
% $$
% A_N(\tau)=\left\{ \widehat{(\overline{H}-\underline{H})}(\Tt)\geq \tau\right\}, 
% $$
introduced in \eqref{def_A_N}, we have to  choose $\Delta$ such that $\tau = o(\Delta^\beta)$. The following result proposes an exponential bound for the risk associated to the rule defined by the indicator $\mathbf{1}_{A_N(\tau)}$ in the definition \eqref{est_over}. 




\medskip


\begin{proposition}\label{prop5_simple} Assume that the conditions of Proposition \ref{propCH} hold true. Let 
	$$\tau\leq \left\{ \overline H(\Tt)-\underline H(\Tt)\right\}/2 + \mathbf{1}_{\{\underline H(\Tt)=\overline H(\Tt)\}} \quad \text{ and } \quad 
	\Delta = \exp(-\log^\varkappa(1/\tau)),
	$$
 for some $\varkappa \in (0,1)$.   Then, for $	A_N(\tau)$ defined in \eqref{def_A_N}, we have 
$$
%\PP(A_N(\tau))\mathbf{1}_{\underline H(\Tt)=\overline H(\Tt)}+\PP\left(\Bar{A}_N(\tau)\right)\mathbf{1}_{\underline H(\Tt)\ne\overline H(\Tt)}
\PP\left( \mathbf{1}_{A_N(\tau)}\neq \mathbf{1}_{\{\underline H(\Tt)<\overline H(\Tt)\}}\right)
\leq  C_3\exp\left[
	- C_5N\times \tau^2 \times \frac{\Delta^{4\overline{H}(\Tt)}\varrho(\Delta,\mathfrak m)}{\log^2(\Delta)}  \Delta^{4D(\Tt)}
	\right],
$$
where $ C_3$ and $ C_5$ are the positive constants defined as in Proposition \ref{propCH}.
\end{proposition}

The bound in Proposition \ref{prop5_simple} provides a lower rate for $\tau$ above which the indicator of $A_N(\tau)$ detects with high accuracy whether $H_1(\Tt)=H_2(\Tt)$ or not. 
