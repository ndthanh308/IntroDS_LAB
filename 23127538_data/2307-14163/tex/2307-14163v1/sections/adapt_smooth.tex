% !TeX root = ../MVFD_arxiv.tex


\subsection{Adaptive optimal bivariate smoothing }\label{sec6}

Let us consider the problem of nonparametric pointwise estimation of a 2-dimensional  aniso\-tropic regression function from a class of functions which are $\gamma_i-$Hölder continuous in the direction $e_i$, with $\gamma_i\in (0,1]$,  $i=1,2$. It is well-known that, under some conditions on the noise and given an iid sample of size $M_0$, 
the minimax rate of convergence for the estimation of a regression function $f$  over the class is
$$
M_0^{-\frac{\boldsymbol \gamma}{2\boldsymbol \gamma+1}},
$$
where the effective smoothness $\boldsymbol {\gamma}$ is defined by the formula 
$$
\frac{1}{\boldsymbol \gamma}=\frac{1}{\gamma_1}+\frac{1}{\gamma_2}.
$$
See \cite{lep_hoff}, \cite{opt_var}, \cite{GK}, \cite{dun_aniso}. 


In the context of multivariate functional data, a natural issue is the reconstruction of the realizations of $X$ using the data.
To match the standard nonparametric regression setup, we hereafter consider the case where the set $\mathcal{H}^{H_1,H_2}$ in Definition \ref{def} is built with the restriction $\boldsymbol L = (L_1,0,0,L_2)$. Fortunately,  the local regularity of a process $X\in \mathcal H^{H_1,H_2}$  is intrinsically linked to the regularity of the sample paths of the process.
Let $\Tt $ be some fixed point in the domain $\cT$, and assume that 
\begin{equation}\label{Yor_us}
	\max_{i=1,2}\sup_{0<\Delta \leq \Delta_0}  \frac{\EE\left[\left\{X\left(\Tt-\Delta e_i/2\right)-X\left(\Tt+\Delta e_i/2\right)\right\}^{2p}\right]}{\EE\left[\left\{X\left(\Tt-\Delta e_i/2\right)-X\left(\Tt+ \Delta e_i/2\right)\right\}^2\right]^p}<\infty, \qquad \forall  p\in \mathbb N.
\end{equation}
By \citet[Theorem 2.1, page 26]{Yor}, 
almost  any realization of $X$  is locally $\alpha-$Hölder continuous in the direction $e_i$, for any order $0\leq \alpha < H_i(\Tt)$. See also Lemma SM.\ref{reg_RY_SM} in the Supplementary Material. 




%This fact leads to the so-called \textit{adaptive procedures}, where the estimating procedure need to adapt to the regularity of the target function $f$ (see \textcolor{red}{*****}.....). 


Let us notice that, with the simplified structure of $\boldsymbol L$ in the definition of $ \mathcal{H}^{H_1,H_2}$,  the  identification problem mentioned in Section \ref{identification} no longer occurs, and we have
\begin{equation}\label{main_eq}
	\theta_{\Tt}^{(i)}(\Delta)= L_i(\Tt)\Delta^{2H_i(\Tt)}+O(\Delta^{2\overline{H}(\Tt)+\beta}),\quad i=1,2.
\end{equation}
Following the methodology introduced in Section \ref{sec_*},  we  consider  the  estimating equations for the local regularity exponents~:
\begin{equation*}
	H_i(\Tt) = \frac{\log(\theta^{(i)}_{\Tt}(2\Delta))-\log(\theta^{(i)}_{\Tt}(\Delta))}{2\log(2)} + O(\Delta^{ \beta }),\quad i=1,2.
\end{equation*}
Applying these equations with a learning set of realizations of $X$, we get the estimators  
$\widehat H_i(\Tt)$. 

Consider a new  realization
$X^{new}$ of $X$, from which we observe   $(Y^{new}_m,\Tt^{new}_m), 1\leq m\leq M_0$ with 
\begin{equation}
	Y^{new}_m=X^{new}(\Tt^{new}_m)+\varepsilon^{new}_m,\quad \quad 1\leq m\leq M_0.
\end{equation}
Here, $M_0$ is a realization of the variable $M$, while the $\Tt^{new}_m$ are independent realizations of the bi-dimensional vector $\boldsymbol T$, with $M$ and $\boldsymbol T$  introduced in Section \ref{sec:data}. We propose to use the Nadaraya-Watson estimator to estimate $X^{new}(\Tt)$, and we consider the simpler version with two bandwidths. Formally, let $K:\mathbb R ^2\to \Rplus$ be a density with the support in $[-1,1]\times [-1,1]$,  and $\mathbf{B}=\operatorname{diag}(1/h_1,1/h_2)$ a positive, $2\times 2$ bandwidth matrix. Considering the 2-dimensional vectors $\Tt$ and $\Tt^{new}_m$ as column matrices, the Nadaraya-Watson estimator is then given by 
$$
\widehat X^{new}(\Tt;\mathbf{B} )=\sum_{m=1}^{M_0}Y^{new}_m\frac{K\left(\mathbf{B}(\Tt^{new}_m-\Tt)\right)}{\sum_{m=1}^{M_0}K\left(\mathbf{B}(\Tt^{new}_m-\Tt)\right)}.
%:=\sum_{m=1}^{M_0}Y^{new}_mW_m(\Tt).
$$
To achieve the optimal rate of convergence, the bandwidths have to be selected  according to the regularity of the sheet. 


For deriving the properties of $\widehat X^{new}(\Tt)$, we impose the following mild assumptions.



\begin{assumptionLP}
	\item\label{LP1} Two positive constants $\kappa$ and $r$ exist such that  
	$$\kappa^{-1}\mathbf{1}_{B(0,r)}(\Tt)\leq K(\Tt)\leq\kappa ,\quad \forall \Tt\in \cT. $$
	$h_1,h_2\in\mathcal H$ with $\mathcal H$ a bandwidth range satisfying $\sqrt{\mathfrak m} \inf  \mathcal H \rightarrow \infty$ and $\sup  \mathcal H \rightarrow 0$. 
%	where $B(0,r)$ denote the ball of center 0 and radius $r$.
	
	\item\label{LP2} A constant $c$ exists such that $f_{\mathbf{T}}(\Tt)\geq c>0$,  $\forall \Tt\in \cT$, where $f_{\mathbf{T}}$ is the density function of the random vector $\boldsymbol{T}$ that generated the independent copies  $\Tt^{new}_m$, $1\leq m\leq M_0 $.
	
	\item\label{LP3e} 
	The error terms $\varepsilon^{new}_m$ are iid,  zero mean random variables with  constant variance $\sigma^2$. The variables  $M_0$, $X^{new}$, $\Tt^{new}_m$, and $\varepsilon^{new}_m$,  $1\leq m\leq M_0 $, are mutually independent. 	  A constant $c>0$ exists such that $c^{-1}\leq M_0/\mathfrak m \leq c$. 	
	
	
	\item \label{LP4} The estimators $\widehat H_i(\Tt)$ and $\widehat L_i(\Tt)$ are independent of the variables $M_0$, $X^{new}$, $\Tt^{new}_m$, and $\varepsilon^{new}_m$. Moreover,  
	$$
	\PP\left(\max\left\{|\widehat H_i(\Tt)-H_i(\Tt)|, |\widehat L_i(\Tt)-L_i(\Tt)|\right\}>\log^{-a}(\mathfrak m)\right)\leq \mathfrak k_1 \exp \left(-\mathfrak m\right), \qquad i=1,2,
	$$
	where $\mathfrak k_1$ is some positive constant and $a>1$.
\end{assumptionLP}

In view of our result from Section \ref{sec4},  condition LP\ref{LP4} holds true under mild conditions. 
Let us consider the pointwise, conditional mean square risk of $\widehat X^{new}$,  given the integer $M_0$, that is
$$
\mathcal R \left( \Tt;\textbf B, M_0\right)=\EE\left[\left\{\widehat X ^{new}(\Tt;\textbf B)-X^{new}(\Tt)\right\}^2\Big{|} M_0\right].
$$
We first derive a bound of this risk when $H_1$, $H_2$ and $L_1,L_2$ are given. 




\medskip

\begin{proposition}\label{prop_risk1}
	Assume that  (LP\ref{LP1}), (LP\ref{LP2}) and (LP\ref{LP3e})  hold true. Then 
	$$
	\mathcal{R}(\Tt; \textbf B,M_0)\leq \frac{\kappa^2}{c\pi}\frac{\sigma^2}{M_0 h_1h_2}+ 2L_1(\Tt)h_1^{2
		H_1(\Tt)}+ 2L_2(\Tt)h_2^{2H_2(\Tt)}+\text{ negligible terms.}
	$$
\end{proposition}



\medskip


Minimizing the dominating terms in the upper bound of the risk yields  optimal  bandwidths. This choice of the bandwidths, and the resulting risk rate,  will depend on the regularity of the process and the Hölder constants. These facts are gathered in the following result. Let
$$
\mathcal H (\Tt) = 2H_1(\Tt)H_2(\Tt)+H_1(\Tt)+H_2(\Tt).
$$


\medskip

\begin{corollary}\label{cor_ad}
	The minimum of the dominant terms in the risk bound in Proposition \ref{prop_risk1} is attained at $(h_1^*, h_2^*)$, with 
	$$
	h^*_1=
	\left(\frac{1}{M_0}\right)^
	{\frac{H_2(\Tt)}{\mathcal H (\Tt)}}
		%{2H_1(\Tt)H_2(\Tt)+H_1(\Tt)+H_2(\Tt)}}
		 \left(\frac{\Lambda_1(\Tt)^{2H_2(\Tt)+1}}{\Lambda_2(\Tt)}\right)^{\frac{1}{2\mathcal H (\Tt)}}
		 	%{4H_1(\Tt)H_2(\Tt)+2H_1(\Tt)+2H_2(\Tt)}},
	\quad 
	\text{ and } \quad 
	h^*_2=\left(\frac{1}{M_0}\right)^{\frac{H_1(\Tt)}{\mathcal H (\Tt)}}
		%{2H_1(\Tt)H_2(\Tt)+H_1(\Tt)+H_2(\Tt)}}
	\left(\frac{\Lambda_2(\Tt)^{2H_1(\Tt)+1}}{\Lambda_1(\Tt)}\right)^{\frac{1}{2\mathcal H (\Tt)}},
		%{4H_1(\Tt)H_2(\Tt)+2H_1(\Tt)+2H_2(\Tt)}},
	$$
	where $\Lambda_i(\Tt)=\kappa^2\sigma^2/\{4c\pi H_i(\Tt)L_i(\Tt)\},$ $i=1,2.$
	Then, up to negligible terms,
	\begin{equation}\label{risk_1}
		\mathcal{R}(\Tt;\textbf B^*, M_0)\leq M_0^{-\frac{2\omega(\Tt)}{2\omega(\Tt)+1}}\Gamma_1 (\Tt),
	\end{equation}
	where $\mathbf{B}^*=\operatorname{diag}(1/h^*_1,1/h^*_2)$,
	%$1/\omega(\Tt)=1/H_1(\Tt)+1/H_2(\Tt),$ and 
	$$
	\frac{1}{\omega(\Tt)} = \frac{1}{H_1(\Tt)}+ \frac{1}{H_2(\Tt)}\quad \text{and} \quad \Gamma_1(\Tt)=  \frac{\kappa^2}{\pi} \frac{\sigma^2 }{c}  \Lambda_1(\Tt)^{\frac{H_2(\Tt)}{\mathcal H (\Tt)} }\Lambda_2(\Tt)^{\frac{H_1(\Tt)}{\mathcal H (\Tt)}}
		%{4H_1(\Tt)H_2(\Tt)+2H_1(\Tt)+2H_2(\Tt)}}
	\left\{1+2H_1(\Tt)+2H_2(\Tt)\right\}.$$
\end{corollary}

\medskip

The rate of $\mathcal{R}(\Tt;\textbf B^*, M_0)$ derived in Corollary \ref{cor_ad} matches the minimax rate for bivariate regression, provided that condition \eqref{Yor_us} is also satisfied. 

Finally, we derive the bound of the pointwise, conditional mean square risk when the regularity parameters are estimated, following our methodology. Let $\widehat h_1^*$ and $\widehat h_2^*$ be the bandwidths obtained by replacing $H_i(\Tt)$ and $L_i(\Tt)$ by their estimates $\widehat H_i(\Tt)$ and $\widehat L_i (\Tt)$ in the expressions of $h_1^*$ and $h_2^*$, respectively. Let $\widehat {\textbf B}^*$ be the corresponding bandwidth matrix.



\medskip


\begin{proposition}\label{risk_2}
	Assume the conditions of 
	Proposition \ref{prop_risk1} and  (LP\ref{LP4}) hold true. Then  
	$$
	\mathcal R(\Tt;\widehat {\textbf B}^*,M_0)\leq \Gamma_2(\Tt) M_0^{-\frac{2\omega(\Tt)}{2\omega(\Tt)+1}+2\log^{-a}(\mathfrak m)}\times \{1+o(\log^{-a}(\mathfrak m))\},
	$$
	where 
	$$
	\Gamma_2(\Tt)= \frac{\kappa^2\sigma^2}{c\pi\Lambda_1^{\alpha_1(\Tt)}(\Tt)\Lambda_2^{\alpha_2(\Tt)}(\Tt)}+L_1(\Tt)\left(\frac{\Lambda_1(\Tt)^{2H_1(\Tt)+1}}{\Lambda_2(\Tt)}\right)^{\alpha_1(\Tt)}+L_2(\Tt)\left(\frac{\Lambda_2(\Tt)^{2H_2(\Tt)+1}}{\Lambda_1(\Tt)}\right)^{\alpha_2(\Tt)},
	$$
	and 
	$$
	\alpha_i(\Tt)= \frac{\omega(\Tt)}{H_i(\Tt)(2\omega(\Tt)+1)},\qquad i=1,2.
	$$
\end{proposition}

\medskip

Proposition \ref{risk_2} shows that, modulo some constant terms,  the price for the estimation of the local regularity is the factor
$M_0^{2 \log^{-a} (\mathfrak m)}$, for some $a>1$. Since 
$\mathfrak m^{\log^{-1} (\mathfrak m)}=e$ for any $\mathfrak m >0$, 
the factor is essentially equal to 1 under very mild condition. 

%With functional data, the adaptation to the regularity of the surfaces is granted  almost for free, \emph{i.e.,} without deterioration of the rate by a logarithmic factor as it happens for the adaptation in standard nonparametric statistics.  


%The reason is the replication feature of this type of data, that means several realizations of a same process are observed instead of only one 


