\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
    \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\input{math_commands.tex}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{bbm}
\usepackage{footmisc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{blindtext}
\usepackage{color}
\usepackage{mathtools}

\definecolor{C0}{RGB}{31,119,180}
\definecolor{C1}{RGB}{255,127,14}
\definecolor{C2}{RGB}{44,160,44}
\definecolor{C3}{RGB}{214,39,40}
\definecolor{C4}{RGB}{148,103,189}
\definecolor{C5}{RGB}{140,86,75}
\definecolor{C6}{RGB}{227,119,194}
\definecolor{C7}{RGB}{127,127,127}
\definecolor{C8}{RGB}{188,189,34}
\definecolor{C9}{RGB}{23,190,207}

\renewcommand{\topfraction}{1.00}
\renewcommand{\bottomfraction}{1.00}
\renewcommand{\textfraction}{0.00}
\setcounter{topnumber}{100}
\setcounter{bottomnumber}{100}
\setcounter{totalnumber}{100}

\newtheorem{dfn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}

\title{Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Takashi Matsubara\\
  Osaka University\\
  Osaka, Japan 560--8531\\
  \texttt{matsubara@sys.es.osaka-u.ac.jp} \\
  \And
  Takaharu Yaguchi \\
  Kobe University \\
  Kobe, Japan 657--8501\\
  \texttt{yaguchi@pearl.kobe-u.ac.jp} \\
}

\begin{document}


\maketitle


\begin{abstract}
  Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs).
  Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution.
  However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain.
  Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked.
  In this paper, we propose a new technique called \emph{good lattice training} (GLT) for PINNs, inspired by number theoretic methods for numerical analysis.
  GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces.
  Our experiments demonstrate that GLT requires 2--20 times fewer collocation points (resulting in lower computational cost) than uniformly random sampling or Latin hypercube sampling, while achieving competitive performance.
\end{abstract}


\section{Introduction}
Many real-world phenomena can be modeled as partial differential equations (PDEs), and solving PDEs has been a central topic in the field of computational science.
The applications include, but are not limited to, weather forecasting, vehicle design~\cite{Hirsch2006}, economic analysis~\cite{Achdou2014}, and computer vision~\cite{Logan2015}.
A PDE is defined as $\mathcal{N}[u]=0$, where $\mathcal{N}$ is a (possibly nonlinear) differential operator, $u:\Omega\rightarrow\R$ is an unknown function, and $\Omega\subset\R^s$ is the domain.
For most PDEs that appear in physical simulations, the well-posedness (i.e., the uniqueness of the solution $u$ and the continuous dependence on the initial and the boundary conditions) has been well studied, and this is typically guaranteed under certain conditions.
Various computational methods, such as finite difference, finite volume, and spectral methods, have been explored for solving PDEs~\cite{Furihata2010,Morton2005,Thomas1995}.
However, the development of computer architecture has become slower, leading to a growing need for computationally efficient alternatives.
A promising approach is physics-informed neural networks (PINNs)~\cite{Raissi2019}, which train a neural network so that its output $\tilde{u}$ satisfies the PDE $\mathcal{N}[\tilde{u}]=0$.
Specifically, given a PDE, PINNs train a neural network to minimize the squared error $\|\mathcal{N}[\tilde{u}](\bm{x}_i)\|^2$ at a finite set of collocation points $\bm{x}_i$ so that the output $\tilde{u}$ satisfies the equation $\mathcal{N}[\tilde{u}](\bm{x_i})=0$.
This objective, known as the physics-informed loss (or PINNs loss), constitutes the core idea of PINNs~\cite{Wang2022b,Wang2023}.

However, the solutions $u$ to PDEs are inherently infinite-dimensional, and the distance involving the output $\tilde{u}$ or the solution $u$ must be defined by an integral over the domain $\Omega$.
In this regard, the physics-informed loss is a finite approximation to the squared 2-norm $|\mathcal{N}[\tilde{u}]|^2_2=\int_\Omega \|\mathcal{N}[\tilde{u}](\bm{x}_i)\|^2\mathrm{d}\bm{x}$ on the function space $L^2(\Omega)$ for $\mathcal{N}[u]\in L^2(\Omega)$, and hence the discretization errors should affect the training efficiency.
A smaller number $N$ of collocation points leads to a less accurate approximation and inferior performance, while a larger number $N$ increases the computational cost~\cite{Bihlo2022,Sharma2022}.
Despite the importance of selecting appropriate collocation points, insufficient emphasis has been placed on this aspect.
\citet{Raissi2019}, \citet{Zeng2023}, and many other studies employed Latin hypercube sampling (LHS) to determine the collocation points.
Alternative approaches include uniformly random sampling~\cite{Jin2021,Krishnapriyan2022} and uniformly spaced sampling~\cite{Wang2021c,Wang2022a}.
These methods are exemplified in Fig.~\ref{fig:collocation}.

In the field of numerical analysis, the relationship between integral approximation and collocation points has been extensively investigated.
Inspired by number theoretic methods for numerical analysis, we propose \emph{good lattice training (GLT)} for PINNs and their variants.
The GLT provides an optimal set of collocation points, as shown in Fig.~\ref{fig:collocation}, when the activation functions of the neural networks are smooth enough.
Our experiments demonstrate that the proposed GLT requires far fewer collocation points than comparison methods while achieving similar errors, significantly reducing computational cost.
The contribution and significance of the proposed GLT are threefold.

% Figure environment removed

\textbf{Computationally Efficient: }
The proposed GLT offers an optimal set of collocation points to compute a loss function that can be regarded as a finite approximation to an integral over domain, such as the physics-informed loss, if the activation functions of the neural networks are smooth enough.
It requires significantly fewer collocation points to achieve accuracy comparable to other methods, or can achieve lower errors with the same computational budget.

\textbf{Applicable to PINNs Variants:}
As the proposed GLT involves changing only the collocation points, it can be applied to various versions of PINNs without modifying the learning algorithm or objective function.
In this study, we investigate a specific variant, the competitive PINN (CPINN)~\cite{Zeng2023}, and demonstrate that CPINNs using the proposed GLT achieve superior convergence speed with significantly fewer collocation points than CPINNs using LHS.

\textbf{Theoretically Solid:}
Number theory provides a theoretical basis for the efficacy of the proposed GLT.
This guarantees the general applicability of GLT and enables users to assess its suitability for a given problem setting prior to implementation.

\section{Related Work}
\paragraph{Neural Networks for Data-Driven Physics}
Neural networks have emerged as a powerful tool for processing information from data and have achieved significant success in various fields such as image recognition and natural language processing in recent years~\cite{He2015a,Vaswani2017}.
They are also employed for black-box system identification, that is, to learn the differential equations that govern the dynamics of a target system and predict its future behavior~\cite{Chen2018e,Chen1990,Wang1998}.
By integrating knowledge from analytical mechanics, neural networks can learn the dynamics that adheres to physical laws, such as the conservation of energy, and even uncover these laws from data~\cite{Finzi2020,Greydanus2019,Matsubara2023ICLR}.

\paragraph{Physics-Informed Neural Networks}
Neural networks have also gained attention as computational tools for solving differential equations, particularly PDEs~\cite{Dissanayake1994,Lagaris1998}.
Recently, \citet{Raissi2019} introduced an elegant refinement to this approach and named it PINNs.
The key concept behind PINNs is the physics-informed loss, also known as PINNs loss~\cite{Wang2022b,Wang2023}.
This loss function evaluates the extent to which the output $\tilde{u}$ of the neural network satisfies a given PDE $\mathcal{N}[u]=0$ and its associated initial and boundary conditions $\mathcal B[u]=0$.
The physics-informed loss can also be integrated into other models like DeepONet~\cite{Lu2021a,Wang2023}. These advancements have been made possible by recent developments in automatic differentiation algorithms, which allow us to automatically compute the partial derivatives of neural networks' outputs~\cite{Paszke2017}.

PINNs have already been applied to various PDEs~\cite{Bihlo2022,Jin2021,Mao2020}, and considerable efforts have been dedicated to improving learning algorithms and objective functions~\cite{Hao2023,Heldmann2023,Lu2022,Pokkunuru2023,Sharma2022,Zeng2023}.
In these studies, the loss functions based on PDE rather than data, including the original physics-informed loss, are defined by evaluating errors at a finite set of collocation points.
As shown in \cite{Bihlo2022,Sharma2022}, there is a trade-off between the number of collocation points (and hence computational cost) and the accuracy of the solution.
Thus, selecting collocation points that efficiently cover the entire domain is essential for achieving better results, although this aspect has often been overlooked.
An exceptional instance is found in \citet{Mao2020}, in which more collocation points were sampled around regions where the solution is discontinuous, efficiently capturing the discontinuity.
However, such an approach is not always feasible.

\section{Method}
\paragraph{Preliminary} For simplicity, we consider the cases where the target PDE is defined on an $s$-dimensional unit cube $[0, 1]^s$. Otherwise, the equations should be transformed to such a domain by an appropriate coordinate transformation.
Methods for reducing computations on complex regions to those on rectangular regions have been studied in the research field of grid generation for domain decomposition methods. See \cite{Knupp2020-ix, Thompson1985-tx} for example.

PINNs employ the PDE that describes the target physical phenomena as loss functions. Specifically, first, the appropriate collocation points $\{ \bm{x}_j \mid j=1,\ldots,N\}$ are fixed, and then the sum of the residuals of the PDE at these points
\begin{equation}\label{eq:dloss}
  \frac{1}{N}\sum_{j=1}^{N} \| \mathcal{N}[\tilde{u}](\bm{x}_j) \|^2
\end{equation}
is minimized.
$\tilde{u}$ is an approximate solution of the equation represented by a neural network.
This type of loss function is referred to as a physics-informed loss.
Although the sum of such residuals over collocation points is practically minimized, for $\tilde{u}$ to be the exact solution, this physics-informed loss should be 0 for all $\bm{x}$, not just at collocation points.
Therefore, the loss function defined by the following integral must be minimized:
\begin{equation}\label{eq:closs}
  \int_{[0, 1]^s} \| \mathcal{N}[\tilde{u}](\bm{x}) \|^2 \mathrm{d}\bm{x}.
\end{equation}
In other words, the practical minimization of \eqref{eq:dloss} essentially minimizes the approximation of \eqref{eq:closs} with the expectation that {\eqref{eq:closs}} will be small enough and hence $\tilde{u}$ becomes an accurate approximation to the exact solution.

One of the typical methods often used in practice is to set $\bm{x}_j$'s to be uniformly distributed random numbers.
PINNs typically target PDEs in multi-dimensional domains, and the loss function is often integrated over a certain multi-dimensional domain.
In such multi-dimensional integration, the effect of the curse of dimensionality cannot be ignored. The method of sampling collocation points from the uniform distributions can be interpreted as the Monte Carlo approximation of the value of the integral \eqref{eq:closs}.
As is well known, the Monte Carlo method is not affected by the curse of dimensionality and can approximate the integral within an error of $O(1/\sqrt{N})$~\cite{Sloan1994-cl}.
Therefore, such a choice of collocation points is reasonable for computing the physics-informed loss in multi-dimensional domains.
However, if feasible, a method less affected by the curse of dimensionality and more efficient than the Monte Carlo method is desired.
In this paper, we propose such a method, which is inspired by number theoretic numerical analysis.

\paragraph{Good Lattice Training: Number Theoretic Method for Computing Physics-Informed Loss} In this section, we propose the \emph{good lattice training (GLT)}, in which a number theoretic method is used to accelerate the training of PINNs.
The proposed method is inspired by the number theoretic numerical analysis~\cite{Niederreiter1992-qb, Sloan1994-cl, Zaremba2014-gj}, and hence in the following, we use some tools from this theory. First, we define a lattice for computing the loss function.
\begin{dfn}[\cite{Niederreiter1992-qb, Sloan1994-cl, Zaremba2014-gj}]
  A lattice $L$ in $\mathbb{R}^s$ is defined as a finite set of points in $\mathbb{R}^s$ that is closed under addition and subtraction.
\end{dfn}
Given a lattice $L$, the set of collocation points is defined as $L^*=\{\bm{x}_1,\ldots,\bm{x}_N\}\coloneq L \cap [0, 1]^s$.
In this paper, we consider an appropriate lattice for computing the physics-informed loss.
Considering that the loss function to be minimized is \eqref{eq:closs}, it is natural to determine the lattice $L$ (and hence the set of collocation points $\bm{x}_j$'s) so that the difference $| \eqref{eq:closs} - \eqref{eq:dloss}|$ of the two loss functions is minimized.

Suppose that $\varepsilon(\bm{x}) \coloneq \| \mathcal{N}[\tilde{u}](\bm{x}) \|^2$ is smooth enough and admits the Fourier series expansion:
\begin{equation*}
  \varepsilon(\bm{x}) \coloneq \| \mathcal{N}[\tilde{u}](\bm{x})\|^2 = \sum_{\bm{h}} \hat{\varepsilon}(\bm{h}) \exp(2 \pi \mathrm{i} \bm{h} \cdot \bm{x}),
\end{equation*}
where $\mathrm{i}$ denotes the imaginary unit and $\bm{h}=(h_1,h_2,\ldots,h_s) \in \mathbb{Z}^s$.
Substitution of this Fourier series into $| \eqref{eq:closs} - \eqref{eq:dloss}|$ yields
\begin{equation}
  \begin{aligned}
     & \left|\int_{[0, 1]^s} \| \mathcal{N}[\tilde{u}](\bm{x}) \|^2 \mathrm{d}\bm{x}- \frac{1}{N} \sum_{j=1}^{N}                                                                         %\frac{1}{N} \sum_{\bm{x}_j \in L^*}
    \sum_{\bm{h} \in \mathbb{Z}^s}
    \hat{\varepsilon}(\bm{h}) \exp( 2 \pi \mathrm{i} \bm{h} \cdot \bm{x}_j) \right| \hspace*{3cm}                                                                                        \\
     & \hspace*{5cm} = \left|\frac{1}{N} \sum_{j=1}^{N}    \sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0} \hat{\varepsilon}(\bm{h}) \exp(2 \pi \mathrm{i}  \bm{h} \cdot \bm{x}_j)\right|,
    % &\leq \frac{1}{N} \sum_{j=1}^{N} \sum_{h \in \mathbb{Z}^s, h \neq 0} \| \hat{\varepsilon}(h) \|
  \end{aligned}
  \label{eq:interror0}
\end{equation}
where the equality follows from the fact that the Fourier mode of $\bm{h}=0$ is equal to the integral $\int_{[0, 1]^s} \| \mathcal{N}[\tilde{u}](\bm{x}_j) \|^2 \mathrm{d}\bm{x}$.
Before optimizing \eqref{eq:interror0}, the dual lattice of lattice $L$ and an insightful lemma are introduced as follows.
\begin{dfn}[\cite{Niederreiter1992-qb, Sloan1994-cl, Zaremba2014-gj}]
  A dual lattice $L^\top$ of a lattice $L$ is defined as $L^\top \coloneq \{ \bm{h} \in \mathbb{R}^s \mid \bm{h} \cdot \bm{x} \in \mathbb{Z}, \ {}^\forall \bm{x} \in L \}$.
\end{dfn}
\begin{lem}[\cite{Niederreiter1992-qb, Sloan1994-cl, Zaremba2014-gj}]\label{lem:lemma}
  For $\bm{h} \in \mathbb{Z}^s$, it holds that
  \begin{equation*}
    %\frac{1}{N} \sum_{j=1}^{N}  \exp( 2 \pi \mathrm{i} \bm{h} \cdot \bm{z})
    \frac{1}{N} \sum_{j=1}^{N} %\sum_{\bm{x}_j \in L^*}
    \exp( 2 \pi \mathrm{i} \bm{h} \cdot \bm{x}_j)
    =\begin{cases}
      1 & (\bm{h} \in L^\top)   \\ %(h \cdot x_j \mathrm{mod} N = 0) \\
      0 & (\mathrm{otherwise}.)
    \end{cases}
  \end{equation*}
\end{lem}
Based on this lemma, we restrict the lattice point $L$ to the form $\{\bm{x} \mid \bm{x}=\bm{s}+\frac{j}{N} \bm{z}\mbox{ for } \bm{s}\in\mathbb{Z}^s, j\in\mathbb{Z}\}$ with a fixed integer vector $\bm{z}$; the set $L^*$ of collocation points is $L\cap [0,1]^s=\{\mbox{decimal part of\ }\frac{j}{N} \bm{z}\mid j=0,\ldots,N-1\}$.
Then, instead of searching $\bm{x}_j$'s, a vector $\bm{z}$ is searched.
This form has the following advantages.
By restricting to this form, $\bm{x}_j$'s can be obtained automatically from a given $\bm{z}$, and hence the optimal collocation points $\bm{x}_j$'s do not need to be stored as a table of numbers, making a significant advantage in implementation.
Another advantage is theoretical; the optimization problem of the collocation points can be reformulated in a number theoretic way. In fact, for $L$ as shown above, it is confirmed that $L^\top = \{ \bm{h} \mid \bm{h} \cdot \bm{z} \equiv 0 \pmod N \}$. If $\bm{h} \cdot \bm{z} \equiv 0 \pmod N$ then there exists an $m \in \mathbb{Z}$ such that $\bm{h} \cdot \bm{z} = m N$ and hence $\frac{j}{N}\bm{h} \cdot \bm{z} = m j \in \mathbb{Z}$. Conversely, if $\bm{h} \cdot \bm{z} \not\equiv 0 \pmod N$, clearly $\frac{1}{N}\bm{h} \cdot \bm{z} \notin \mathbb{Z}$.

Therefore, from the above lemma, \eqref{eq:interror0} is bounded from above by
\begin{equation}\label{eq:interror}
  \sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} | \hat{\varepsilon}(\bm{h}) |,
\end{equation}
and hence the collocation points $\bm{x}_j$'s should be determined so that \eqref{eq:interror} becomes small.
This problem is a number theoretic problem in the sense that it is a minimization problem of finding an integer vector $\bm{h}$ subject to the condition $\bm{h} \cdot \bm{z}  \equiv 0 \pmod N$.
This problem has been considered in the field of number theoretic numerical analysis. In particular, optimal solutions have been investigated for integrands in the Korobov spaces, which are spaces of functions that satisfy a certain smoothness condition.
\begin{dfn}[\cite{Niederreiter1992-qb, Sloan1994-cl, Zaremba2014-gj}]\label{dfn:Korobov}
  The function space that is defined as $E_\alpha = \{ f:[0, 1]^s \to \R \mid {}^\exists c, |\hat{f}(\bm{h})| \leq c/(\bar{h}_1 \bar{h}_2 \cdots \bar{h}_s)^\alpha \}$ is called the Korobov space, where $\hat{f}(\bm{h})$ is the Fourier coefficients of $f$ and $\bar{k} = \max(1, |k|)$ for $k \in \mathbb{R}$.
\end{dfn}
It is known that if $\alpha$ is an integer, for a function $f$ to be in $E_\alpha$, it is sufficient that $f$ has continuous partial derivatives  $\partial^{q_1+ q_2 + \cdots + q_s} f/\partial_1^{q_1} \cdot \partial_2^{q_2} \cdots \partial_s^{q_s}, 0 \leq q_k \leq \alpha\ (k=1,\ldots,s)$. For example, if a function $f(x, y):\mathbb{R}^2 \to \mathbb{R}$ has continuous $f_x, f_y, f_{xy}$, then $f \in E_1$.


The main result of this paper is the following.
\begin{thm}\label{thm:goodlattice}
  Suppose that the activation function of $\tilde{u}$ and hence $\tilde{u}$ itself are sufficiently smooth so that there exists an $\alpha > 0$ such that $\|\mathcal{N}[\tilde{u}]\|^2 \in E_\alpha$. Then, for given integers $N \geq 2$ and $s \geq 2$, there exists an integer vector $\bm{z} \in \mathbb{Z}^s$ such that $L^*=  \{ \mbox{decimal part of\ } \frac{j}{N} \bm{z}  \mid j=0,\ldots,N-1\}$ is a ``good lattice'' in the sense that
  \begin{equation}\label{eq:goodlattice}
    %\sum_{h \in \mathbb{Z}^s, h \neq 0, h \cdot z \pmod N \equiv 0} \frac{1}{(\bar{h}_1 \bar{h}_2 \cdots \bar{h}_s)^\alpha}
    \int_{[0, 1]^s} \| \mathcal{N}[\tilde{u}](\bm{x}) \|^2 \mathrm{d}\bm{x}
    \leq
    \frac{1}{N}\sum_{\bm{x}_j \in L^*} \| \mathcal{N}[\tilde{u}](\bm{x}_j) \|^2
    +
    O\left(\frac{(\log N)^{\alpha s}}{N^\alpha}\right). %+O(\frac{(\log N)^{\alpha s - 1}}{N^\alpha}).
  \end{equation}
\end{thm}

For a proof of this theorem, see Appendix A. In this paper, we call the learning method that minimizes
\begin{equation*}
  \frac{1}{N}\sum_{\bm{x}_j \in L^*} \| \mathcal{N}[\tilde{u}](\bm{x}_j) \|^2
\end{equation*}
for a lattice $L$ that satisfies \eqref{eq:goodlattice} \textit{the good lattice training (GLT)}.

The proposed GLT, of which the order of the convergence is $O(\frac{(\log N)^{\alpha s}}{N^\alpha})$, can converge much faster to the integrated loss \eqref{eq:closs} than the uniformly random sampling (i.e., the Monte Carlo method, of which the order of the convergence is $O(1/\sqrt{N})$) if the activation function of $\tilde{u}$ and hence the neural network $\tilde{u}$ itself are sufficiently smooth so that $\mathcal{N}[\tilde{u}] \in E_\alpha$ for a large $\alpha$.
Furthermore, the error increases only in proportion to $(\log N)^{\alpha s}$ as the dimension increases.
Therefore, the proposed GLT remains effective for high-dimensional problems, almost unaffected by the curse of dimensionality.
This makes PINNs available for higher dimensional problems.

Although the above theorem shows the existence of a good lattice, we can obtain a good lattice in the following way in practice.
When $s=2$, we can use the Fibonacci sequence $1, 1, 2, 3, 5, 8, 13, \ldots$, which is defined by $F_1 = F_2 = 1, \ F_k = F_{k-1} + F_{k-2} \ (k \geq 3)$.
It is known that by setting $N = F_k$ and $\bm{z} = (1, F_{k-1})$~\cite{Niederreiter1992-qb, Sloan1994-cl}, we obtain a good lattice.
For example, when $N=13$, $\bm{z} = (1, 8)$ gives a good lattice $L=\{(0,0), (\frac{1}{13},\frac{8}{13}),(\frac{2}{13},\frac{3}{13}),(\frac{3}{13},\frac{11}{13}),\ldots\}$.

When $s=2$ and $N$ is not a Fibonacci number, or when $s>2$, there is no known method to generate a good lattice using a sequence of numbers. However, it is possible to numerically find the optimal $\bm{z}$ for a given $N$ with a computational cost of $O(N^2)$. The procedure is as follows: First, if $\| \mathcal{N}[\tilde{u}] \|^2$ belongs to Koborov space, then \eqref{eq:interror} can be estimated as
\begin{equation}\label{eq:palpha}
  \sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} \frac{c}{(\bar{h}_1 \bar{h}_2 \cdots \bar{h}_s)^\alpha}.
\end{equation}
In particular, essentially this upper bound is achieved when $\| \mathcal{N}[\tilde{u}] \|^2$ becomes the following function:
\begin{equation*}
  \| \mathcal{N}[\tilde{u}] \|^2= \prod_{k=1}^s F_\alpha(x_k), \quad
  F_\alpha(x_k) = 1 + \sum_{h \in \mathbb{Z}, h \neq 0} \frac{\exp(2 \pi \mathrm{i} h x_k)}{|h|^\alpha},
  %  \sum_{h \in \mathbb{Z}^s, h \neq 0, h \cdot x_j \mathrm{mod} N = 0} \frac{c}{(\bar{h}_1 \bar{h}_2 \cdots \bar{h}_s)^\alpha}.
\end{equation*}
Actually, when $\alpha$ is even, this function is known to be explicitly written by using the Bernoulli polynomials $B_\alpha(x)$~\cite{Sloan1994-cl}:
\begin{equation*}
  F_\alpha(x_k) = 1 - (-1)^\frac{\alpha}{2}\frac{(2 \pi)^\alpha B_\alpha}{\alpha!}.
\end{equation*}
Since this is a polynomial function, the integral can be found exactly; hence it is possible to find a loss function for this function for each lattice $L$. Therefore, to find an optimal $\bm{z}$, the loss function with respect to the above function should be minimized.
The computational cost for computing the loss function for each $L$ is $O(N)$.

Even considering that each component of $\bm{z}$ is in $\{0, \ldots, N-1\}$, searching for the optimal $\bm{z} \in \mathbb{Z}^s$ would require the computational cost $O(N^s)$; however, if $N$ is a prime number or the product of two prime numbers, the existence of a $\bm{z}$ of the form $\bm{z} = (1, l, l^2 \pmod N, \ldots, l^{s-1} \pmod N)$ that gives a good lattice is known \cite{Korobov1959-qd,Korobov1960-lj,Sloan1994-cl}. Since $l \in \{ 0, 1, \ldots, N-1\}$, there are only $N$ candidates for $l$, and hence the computational cost for finding the optimal $\bm{z}$ is only $O(N^2)$ for each $N$.
Furthermore, as this optimization process can be fully parallelized, the computational time is quite short practically.
The values of $\bm{z}$ have been explored in the field of number theoretic numerical analysis, and typical values are available as numerical tables in, e.g., \cite{Fang1994,Keng1981}.

\paragraph{Limitations}
Not limited to the proposed GLT, but many methods to determine collocation points are not directly applicable to non-rectangular or non-flat domain $\Omega$~\cite{Shankar2018}.
To achieve the best performance, it is recommended to deform the domain $\Omega$ to fit a flat rectangle. For such a technique, see, e.g., \cite{Knupp2020-ix, Thompson1985-tx}. In addition, because the proposed GLT is based on the Fourier series expansion, it is basically assumed that the loss function is periodic. This problem can be addressed by variable transformations~\cite{Sloan1994-cl}. For example, if an $s$-dimensional integral $\int_{[0, 1]^s} f(\bm{x}) \mathrm{d} \bm{x}$ is to be computed, the integrand $f$ can be transformed to a periodic one by using a smooth increasing function $\phi: [0, 1] \to [0, 1]$ with $\phi^\prime(0) = \phi^\prime(1) = 0$:
\begin{equation*}
  \int_{[0,1]^s}\!\!\! g(x_1,x_2,\ldots,x_s) \mathrm{d} \bm{x},\
  g(x_1,x_2,\ldots,x_s)\! =\! f(\phi(x_1), \phi(x_2),\ldots,\phi(x_s)) \phi^\prime(x_1)\phi^\prime(x_2) \cdots \phi^\prime(x_s).
\end{equation*}
However, as demonstrated in the following section, the proposed GLT works effectively in practice even for non-periodic loss functions without the above transformation.

For the GLT to be effective, the neural network should be smooth, and as is well-known, this condition is satisfied if the activation function is smooth enough. However, if the true solution $u$ is not a smooth function, the smoothness of the neural network $\tilde{u}$ is expected to decrease as $\tilde{u}$ approaches $u$. Actually, in such cases, although the differentiability of the activation function guarantees that $u \in E_\alpha$, the Fourier coefficients may increase. Therefore, the convergence in the final stage of training may be slowed down if the true solution is not smooth.

\paragraph{Practical Implementation}
As the GLT is based on the Fourier series, the lattice can be shifted periodically.
Therefore, the physics-informed loss based on the proposed GLT is as follows.
\begin{equation}
  \frac{1}{N}\sum_{j=0}^{N-1} \left\| \mathcal{N}[\tilde{u}]\left(\left\{\frac{z_1}{N}j+r_1\right\},\left\{\frac{z_2}{N}j+r_2\right\},\ldots,\left\{\frac{z_s}{N}j+r_s\right\} \right)\right\|^2, \ r_1,\ldots,r_s \sim U([0, 1]), \label{eq:goodlatticeimplementation}
\end{equation}
% where $U([0, 1]^s)$ is the uniform distribution over the interval $[0, 1]^s$. The random numbers $\bm{r}_j$'s are resampled at each training iteration.
where $\{\cdot\}$ gives the decimal part of the input, and $U([0, 1])$ is the uniform distribution over the interval $[0, 1]$.
One can take the integer vector $\bm{z}=(z_1,z_2,\ldots,z_s)$ from numerical tables or find them using the above procedure.
In the above proof, the integrand is not limited to the original physics-informed loss $\|\mathcal N[\tilde u](\bm{x})\|^2$; the proposed GLT can be applied to a variant of PINNs whose loss function can be regarded as a finite approximation to an integral over the domain, e.g.,~\cite{Zeng2023}.
Our preliminary experiments confirmed that, if using the stochastic gradient descent (SGD) algorithms, resampling the random numbers $r_\bullet$'s at each training iteration prevents the neural network from overfitting and improves training efficiency.

\section{Experiments and Results}
\subsection{Physics-Informed Neural Networks}
\paragraph{Experimental Settings}
We modified the code from the official repository\footnote{\url{https://github.com/maziarraissi/PINNs} (MIT license)\label{footnote:PINNs}} of \citet{Raissi2019}, the original paper of PINNs.
We obtained the datasets of the nonlinear Schr\"{o}dinger (NLS) equation, Korteweg--De Vries (KdV) equation, and Allen-Cahn (AC) equation from the repository.
The SchrÃ¶dinger equation governs wave functions in quantum mechanics, while the KdV equation models shallow water waves, and the AC equation characterizes phase separation in co-polymer melts.
These datasets provide numerical solutions to initial value problems with periodic boundary conditions.
Although they contain numerical errors, we treated them as the true solutions $u$.
These equations are nonlinear versions of hyperbolic or parabolic PDEs.
Additionally, as a nonlinear version of an elliptic PDE, we created a dataset for $s$-dimensional Poisson's equation, which produces analytically solvable solutions of $2^s$ modes with the Dirichlet boundary condition.
%  following~\citet{Dissanayake1994}
We examined the cases where $s\in\{2,4\}$.
See Appendix~\ref{appendix:datasets} for further information.

Unless otherwise stated, we followed the repository's experimental settings for the NLS equation.
The physics-informed loss was defined as $\mathcal L(\tilde{u})=\frac{1}{N}\sum_{i=1}^N\|\mathcal{N}[\tilde{u}](\bm{x}_i)\|^2$ given $N$ collocation points $\bm{x}_1,\bm{x}_2,\dots,\bm{x}_N$.
This can be regarded as a finite approximation to the squared 2-norm $|\mathcal{N}[\tilde{u}]|_2^2=\int_{\Omega}\|\mathcal{N}[\tilde{u}](\bm{x})\|^2\mathrm{d}\bm{x}$.
We used additional losses to learn the initial and boundary conditions for the NLS equation, while we used the technique in \cite{Lagaris1998} to ensure the conditions for other equations.
The state of the NLS equation is complex; we simply treated it as a 2D real vector for training and used its absolute value for evaluation and visualization.
Following \citet{Raissi2019}, we evaluated the performance using the relative error, which is the normalized squared error $\mathcal L(\tilde{u}, u;\bm{x}_i)=(\sum_{i=1}^{N_e}\|\tilde{u}(\bm{x}_i)-u(\bm{x}_i)\|^2)^{1/2}/(\sum_{i=1}^{N_e}\|u(\bm{x}_i)\|^2)^{^{1/2}}$ at predefined $N_e$ collocation points $\bm{x}_i$.
This is also a finite approximation to $|\tilde{u}-u|_2/|u|_2$.
For Poisson's equation with $s=2$, we followed the original learning strategy using the L-BFGS-B method preceded by the Adam optimizer~\cite{Kingma2014b} to ensure precise convergence.
For other datasets, we trained PINNs using the Adam optimizer with cosine decay of a single cycle to zero~\cite{Loshchilov2017} for 200,000 iterations and sampling a different set of collocation points at each iteration; we found that this strategy improves the final performance.
See Appendix~\ref{appendix:datasets} for details.

We determined the collocation points using four different methods: uniformly random sampling, uniformly spaced sampling, LHS, and the proposed GLT.
For the GLT, we took the number $N$ of collocation points and the corresponding integer vector $\bm{z}$ from numerical tables in \cite{Fang1994,Keng1981}.
We also applied random shifts in \eqref{eq:goodlatticeimplementation}.
To maintain consistency, we used the same value of $N$ for both uniformly random sampling and LHS.
For uniformly spaced sampling, we selected the square number $N$ (or the biquadratic number when $s=4$) that was closest to a number used in the GLT.
Subsequently, we created a unit cube of $N^{1/s}$ points on each side.
Additionally, we introduced random shifts to the collocation points, with $r_\bullet \sim U([0, 1/N^{1/s}])$, ensuring that the collocation points were uniformly distributed.
Then, we transformed the unit cube $[0,1]^s$ to the original domain $\Omega$.

We conducted five trials for each number $N$ and each method.
All experiments were conducted using Python v3.7.16 and tensorflow v1.15.5~\cite{tensorflow} on servers with Intel Xeon Platinum 8368.

\begin{wrapfigure}{r}{5.2cm}
  \vspace*{-6mm}
  \footnotesize
  % Figure removed
  \caption{The number $N$ of collocation points and the standard deviation of the physics-informed loss.}
  \label{fig:PINNsTest}
  \vspace*{-3mm}
\end{wrapfigure}


\paragraph{Results: Accuracy of Physics-Informed Loss}
First, we evaluate the accuracy of the physics-informed loss \eqref{eq:dloss} in approximating the integrated loss \eqref{eq:closs}.
Regardless of the method, as the number $N$ of collocation points increases, the physics-informed loss \eqref{eq:dloss} approaches the integrated loss \eqref{eq:closs}.
With fewer collocation points, the physics-informed loss varies significantly depending on the selection of collocation points, resulting in a larger standard deviation.
We trained the PINNs on the NLS equation using LHS with $N=610$ collocation points.
Then, we evaluated the physics-informed loss
of the obtained PINNs for different methods and different numbers of collocation points.
For each combination of method and number, we performed 1,000 trials and summarized their standard deviation of the physics-informed loss in Fig.~\ref{fig:PINNsTest}.

Except for the GLT, the other methods exhibit a similar trend, showing a linear reduction in standard deviation on the log-log plot.
This trend aligns with the theoretical result that the convergence rate of Monte Carlo methods is $O(1/\sqrt{N})$.
On the other hand, the GLT demonstrates an accelerated reduction as the number $N$ increases.
This result implies that by using the GLT, the physics-informed loss \eqref{eq:dloss} approximates the integrated loss \eqref{eq:closs} more accurately with the same number $N$ of collocation points, leading to faster training and improved accuracy.
% In particular, the GLT can be effective for PDEs with complicated solutions or in multi-dimensional domains where they require many collocation points.


% Figure environment removed

\begin{table}[t]
  \centering
  \footnotesize
  \caption{Trade-Off between Number $N$ of Collocation Points and Relative Error $\mathcal L$.}
  \label{tab:datasets}
  \begin{tabular}{llrrrrr}
    \toprule
                                         &                                             & \multicolumn{1}{c}{NLS} & \multicolumn{1}{c}{KdV} & \multicolumn{1}{c}{AC} & \multicolumn{2}{c}{Poisson}                 \\
    \cmidrule(lr){6-7}
                                         &                                             &                         &                         &                        & $s=2$                       & $s=4$         \\
    \midrule
                                         & \textcolor{C4}{$\bullet$}\,uniformly random & 6,765                   & 10,946                  & 4,181                  & $>$17,711                   & 1,019         \\
    \# of points $N^\dagger$             & \textcolor{C8}{$\bullet$}\,uniformly spaced & 1,600                   & 6,724                   & 11,025                 & 6,724                       & 1,296         \\
                                         & \textcolor{C1}{$\bullet$}\,LHS              & 4,181                   & 17,711                  & 10,000                 & $>$17,711                   & 562           \\
                                         & \textcolor{C0}{$\bullet$}\,GLT (proposed)   & \textbf{610}            & \textbf{987}            & \textbf{987}           & \textbf{1,597}              & \textbf{307}  \\
    \midrule
                                         & \textcolor{C4}{$\bullet$}\,uniformly random & 7.60                    & 3.32                    & 1.82                   & 4.141                       & 28.7          \\
    relative error $\mathcal L^\ddagger$ & \textcolor{C8}{$\bullet$}\,uniformly spaced & 4.88                    & 3.25                    & 1.79                   & 1.859                       & 29.2          \\
                                         & \textcolor{C1}{$\bullet$}\,LHS              & 6.45                    & 2.94                    & 1.43                   & 6.174                       & 24.3          \\
                                         & \textcolor{C0}{$\bullet$}\,GLT (proposed)   & \textbf{1.55}           & \textbf{1.90}           & \textbf{0.74}          & \textbf{0.027}              & \textbf{16.4} \\
    \bottomrule                                                                                                                                                                                                   \\[-3mm]
    \multicolumn{6}{l}{$\dagger$ \# of points $N$ at competitive relative error $\mathcal L$ (\textcolor{C3}{under horizontal red line} in Fig.~\ref{fig:PINNsResults}).}                                         \\
    \multicolumn{6}{l}{$\ddagger$ relative error $\mathcal L$ at competitive \# of points $N$ (\textcolor{C2}{on vertical green line} in Fig.~\ref{fig:PINNsResults}).}                                           \\
    \multicolumn{6}{l}{\ \ Shown in the scale of $10^{-5}$ for Poisson's equation and $10^{-3}$ for others.}
  \end{tabular}
\end{table}

% Figure environment removed

\paragraph{Results: Performance of PINNs}
Figure~\ref{fig:PINNsResults} shows the average relative error $\mathcal L$ with solid lines and the maximum and minimum errors with shaded areas.
These results demonstrate that as $N$ increases, the relative error $\mathcal L$ decreases and eventually reaches saturation.
This is because of numerical errors in the datasets, discretization errors in relative error $\mathcal L$, and the network capacity.

We report the minimum numbers $N$ of collocation points with which the relative error $\mathcal L$ was saturated in Table~\ref{tab:datasets}.
Specifically, we consider a relative error $\mathcal L$ below 130 \% of the minimum observed one as saturated; the thresholds are denoted by horizontal red lines in Fig.~\ref{fig:PINNsResults}.
The proposed GLT demonstrated equivalent performance with significantly fewer collocation points.
This suggests that the proposed GLT can achieve comparable performance with significantly less computational cost.
%
Next, we equalized the number $N$ of collocation points (and therefore, the computational cost).
In the bottom rows of Table~\ref{tab:datasets}, we report the relative error $\mathcal L$ for collocation points with which the relative error $\mathcal L$ of one of the comparison methods saturated, as denoted by vertical green lines in Fig.~\ref{fig:PINNsResults}.
A smaller loss $\mathcal L$ indicates that the method performed better than others with the same computational cost.
The proposed GLT yielded nearly half or less of the relative error $\mathcal L$ for the NLS, KdV, and AC equations, and the difference is significant for Poisson's equation with $s=2$.
We show the true solutions and the residuals of example results with such $N$ in Fig.~\ref{fig:PINNsExample}.
We also investigated longer training with smaller mini-batch sizes in Appendix~\ref{appendix:results}.


\subsection{Competitive Physics-Informed Neural Networks}
\paragraph{Experimental Settings}
Competitive PINNs (CPINNs) are an improved version of PINNs that additionally uses a neural network $D:\Omega\rightarrow\R$ called a discriminator~\cite{Zeng2023}.
Its objective function is $\frac{1}{N}\sum_{i=1}^N D(\bm{x}_i)\mathcal{N}[\tilde{u}](\bm{x}_i)$; the discriminator $D$ is trained to maximize it, whereas the neural network $\tilde{u}$ is trained to minimize it.
This setting is regarded as a zero-sum game, and its Nash equilibrium offers the solution to a given PDE.
Moreover, this setting enables to use the (adaptive) competitive gradient descent (CGD) algorithm~\cite{Schaefer2019,Schaefer2020}, accelerating the convergence.
The objective function is also regarded as a finite approximation to the integral $\int_{\Omega}D(\bm{x})\mathcal{N}[\tilde{u}](\bm{x}) \mathrm{d}\bm{x}$; therefore, the proposed GLT can be applied to CPINNs.

We modified the code accompanying the manuscript and investigated the NLS and Burgers' equations\footnote{See Supplementary Material at \url{https://openreview.net/forum?id=z9SIj-IM7tn} (MIT License)\label{footenote:CPINNs}}.
The NLS equation is identical to the one above.
See Appendix~\ref{appendix:datasets} for details about Burgers' equation.
The number $N$ of collocation points was 20,000 by default and varied.
All experiments were conducted using Python v3.9.16 and Pytorch v1.13.1~\cite{Paszke2017} on servers with Intel Xeon Platinum 8368 and NVIDIA A100.

% Figure environment removed

\paragraph{Results}
We summarized the results in Fig.~\ref{fig:CPINNsResults}.
For the NLS equation, when the collocation points were determined by LHS, the relative error $\mathcal L$ of CPINNs decreased slowly even with $N=20,000$, and there was almost no improvement for $N\le 2,584$.
On the other hand, when the proposed GLT was used, the error decreased rapidly even with $N=2,584$.
In the case of the Burgers' equation, CPINNs using GLT made some progress in learning with $N=2,584$ collocation points, while CPINNs using LHS required $N=6,765$ collocation points.
These results indicate that the proposed GLT exhibits competitive or superior convergence speed with 2.5 to 8 times fewer collocation points.
The original paper demonstrated that CPINNs have a faster convergence rate than vanilla PINNs, but the GLT can further accelerate it.

\section{Conclusion}
This paper highlighted that the physics-informed loss, commonly used in PINNs and their variants, is a finite approximation to the integrated loss.
From this perspective, it proposed good lattice training (GLT) to determine collocation points.
This method enables a more accurate approximation of the integrated loss with a smaller number of collocation points.
Experimental results using PINNs and CPINNs demonstrated that the GLT can achieve competitive or superior performance with much fewer collocation points.
These results imply a significant reduction in computational cost and contribute to the large-scale computation of PINNs.

We focused on the physics-informed loss in the domain.
If the domain is $s\ge3$-dimensional, the initial and boundary conditions are of $s\ge2$ dimensions, and their loss functions are also subject to the proposed GLT.
%
As shown in Figs.~\ref{fig:PINNsResults} and \ref{fig:CPINNsResults}, the current problem setting reaches performance saturation with around $N=1,000$ collocation points.
However, Figure~\ref{fig:PINNsTest} demonstrates that the GLT significantly enhances the approximation accuracy even when using further more collocation points.
This implies that the GLT is particularly effective in addressing larger-scale problem settings, which will be explored further in future research.


  % {\small
  %   \bibliographystyle{apalike}
  %   \bibliography{library,library2}
  % }

\begin{thebibliography}{}

\bibitem[Abadi et~al., 2016]{tensorflow}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jozefowicz, R., Jia, Y., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man{\'e}, D., Schuster, M., Monga, R., Moore, S., Murray, D.,
  Olah, C., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi{\'e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X. (2016).
\newblock {{TensorFlow}}: {{Large-scale}} machine learning on heterogeneous
  systems.
\newblock {\em USENIX Symposium on Operating Systems Design and Implementation
  (OSDI)}.

\bibitem[Achdou et~al., 2014]{Achdou2014}
Achdou, Y., Buera, F.~J., Lasry, J.-M., Lions, P.-L., and Moll, B. (2014).
\newblock Partial differential equation models in macroeconomics.
\newblock {\em Philosophical Transactions of the Royal Society A: Mathematical,
  Physical and Engineering Sciences}, 372(2028):20130397.

\bibitem[Bihlo and Popovych, 2022]{Bihlo2022}
Bihlo, A. and Popovych, R.~O. (2022).
\newblock Physics-informed neural networks for the shallow-water equations on
  the sphere.
\newblock {\em Journal of Computational Physics}, 456:111024.

\bibitem[Chen et~al., 2018]{Chen2018e}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. (2018).
\newblock Neural {{Ordinary Differential Equations}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}
  ({{NeurIPS}})}, pages 1--19.

\bibitem[Chen et~al., 1990]{Chen1990}
Chen, S., Billings, S.~A., and Grant, P.~M. (1990).
\newblock Non-linear system identification using neural networks.
\newblock {\em International Journal of Control}, 51(6):1191--1214.

\bibitem[Dissanayake and {Phan-Thien}, 1994]{Dissanayake1994}
Dissanayake, M. W. M.~G. and {Phan-Thien}, N. (1994).
\newblock Neural-network-based approximations for solving partial differential
  equations.
\newblock {\em Communications in Numerical Methods in Engineering},
  10(3):195--201.

\bibitem[Fang and Wang, 1994]{Fang1994}
Fang, K.-t. and Wang, Y. (1994).
\newblock {\em Number-Theoretic {{Methods}} in {{Statistics}}}.
\newblock Series {{Title Monographs}} on {{Statistics}} and {{Applied
  Probability}}. {Springer New York, NY}.

\bibitem[Finzi et~al., 2020]{Finzi2020}
Finzi, M., Stanton, S., Izmailov, P., and Wilson, A.~G. (2020).
\newblock Generalizing {{Convolutional Neural Networks}} for {{Equivariance}}
  to {{Lie Groups}} on {{Arbitrary Continuous Data}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}
  ({{ICML}})}, pages 3146--3157.

\bibitem[Furihata and Matsuo, 2010]{Furihata2010}
Furihata, D. and Matsuo, T. (2010).
\newblock {\em Discrete {{Variational Derivative Method}}: {{A
  Structure-Preserving Numerical Method}} for {{Partial Differential
  Equations}}}.
\newblock {Chapman and Hall/CRC}.

\bibitem[Greydanus et~al., 2019]{Greydanus2019}
Greydanus, S., Dzamba, M., and Yosinski, J. (2019).
\newblock Hamiltonian {{Neural Networks}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}
  ({{NeurIPS}})}, pages 1--16.

\bibitem[Hao et~al., 2023]{Hao2023}
Hao, Z., Ying, C., Su, H., Zhu, J., Song, J., and Cheng, Z. (2023).
\newblock Bi-level {{Physics-Informed Neural Networks}} for {{PDE Constrained
  Optimization}} using {{Broyden}}'s {{Hypergradients}}.
\newblock In {\em International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}.

\bibitem[He et~al., 2016]{He2015a}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep {{Residual Learning}} for {{Image Recognition}}.
\newblock In {\em {{IEEE Conference}} on {{Computer Vision}} and {{Pattern
  Recognition}} ({{CVPR}})}, pages 1--9.

\bibitem[Heldmann et~al., 2023]{Heldmann2023}
Heldmann, F., Berkhahn, S., Ehrhardt, M., and Klamroth, K. (2023).
\newblock {{PINN}} training using biobjective optimization: {{The}} trade-off
  between data loss and residual loss.
\newblock {\em Journal of Computational Physics}, page 112211.

\bibitem[Hirsch, 2006]{Hirsch2006}
Hirsch, C. (2006).
\newblock {\em Numerical {{Computation}} of {{Internal}} and {{External
  Flows}}}.
\newblock {Butterworth-Heinemann Limited}.

\bibitem[Jin et~al., 2021]{Jin2021}
Jin, X., Cai, S., Li, H., and Karniadakis, G.~E. (2021).
\newblock {{NSFnets}} ({{Navier-Stokes}} flow nets): {{Physics-informed}}
  neural networks for the incompressible {{Navier-Stokes}} equations.
\newblock {\em Journal of Computational Physics}, 426:109951.

\bibitem[Keng and Yuan, 1981]{Keng1981}
Keng, H.~L. and Yuan, W. (1981).
\newblock {\em Applications of {{Number Theory}} to {{Numerical Analysis}}}.
\newblock {Springer}, {Berlin, Heidelberg}.

\bibitem[Kingma and Ba, 2015]{Kingma2014b}
Kingma, D.~P. and Ba, J. (2015).
\newblock Adam: {{A Method}} for {{Stochastic Optimization}}.
\newblock In {\em International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}, pages 1--15.

\bibitem[Knupp and Steinberg, 2020]{Knupp2020-ix}
Knupp, P. and Steinberg, S. (2020).
\newblock {\em Fundamentals of Grid Generation}.
\newblock CRC Press.

\bibitem[Korobov, 1959]{Korobov1959-qd}
Korobov, N.~M. (1959).
\newblock The approximate computation of multiple integrals.
\newblock In {\em Dokl. Akad. Nauk {SSSR}}, volume 124, pages 1207--1210.
  cir.nii.ac.jp.

\bibitem[Korobov, 1960]{Korobov1960-lj}
Korobov, N.~M. (1960).
\newblock Properties and calculation of optimal coefficients.
\newblock In {\em Dokl. Akad. Nauk {SSSR}}, volume 132, pages 1009--1012.
  mathnet.ru.

\bibitem[Krishnapriyan et~al., 2022]{Krishnapriyan2022}
Krishnapriyan, A., Gholami, A., Zhe, S., Kirby, R., and Mahoney, M.~W. (2022).
\newblock Characterizing possible failure modes in physics-informed neural
  networks.
\newblock In {\em Advances in {{Neural Information Processing Systems}}
  ({{NeurIPS}})}, pages 1--13.

\bibitem[Lagaris et~al., 1998]{Lagaris1998}
Lagaris, I.~E., Likas, A., and Fotiadis, D.~I. (1998).
\newblock Artificial neural networks for solving ordinary and partial
  differential equations.
\newblock {\em IEEE Transactions on Neural Networks}, 9(5):987--1000.

\bibitem[Logan, 2015]{Logan2015}
Logan, J.~D. (2015).
\newblock {\em Applied {{Partial Differential Equations}}}.
\newblock Undergraduate {{Texts}} in {{Mathematics}}. {Springer Cham}, {Cham}.

\bibitem[Loshchilov and Hutter, 2017]{Loshchilov2017}
Loshchilov, I. and Hutter, F. (2017).
\newblock {{SGDR}}: {{Stochastic}} gradient descent with warm restarts.
\newblock In {\em International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}, pages 1--16.

\bibitem[Lu et~al., 2019]{Lu2021a}
Lu, L., Jin, P., and Karniadakis, G.~E. (2019).
\newblock {{DeepONet}}: {{Learning}} nonlinear operators for identifying
  differential equations based on the universal approximation theorem of
  operators.
\newblock {\em arXiv}, 3(3):218--229.

\bibitem[Lu et~al., 2022]{Lu2022}
Lu, Y., Blanchet, J., and Ying, L. (2022).
\newblock Sobolev {{Acceleration}} and {{Statistical Optimality}} for
  {{Learning Elliptic Equations}} via {{Gradient Descent}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}
  ({{NeurIPS}})}.

\bibitem[Mao et~al., 2020]{Mao2020}
Mao, Z., Jagtap, A.~D., and Karniadakis, G.~E. (2020).
\newblock Physics-informed neural networks for high-speed flows.
\newblock {\em Computer Methods in Applied Mechanics and Engineering},
  360:112789.

\bibitem[Matsubara and Yaguchi, 2023]{Matsubara2023ICLR}
Matsubara, T. and Yaguchi, T. (2023).
\newblock {{FINDE}}: {{Neural Differential Equations}} for {{Finding}} and
  {{Preserving Invariant Quantities}}.
\newblock In {\em International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}.

\bibitem[Morton and Mayers, 2005]{Morton2005}
Morton, K.~W. and Mayers, D.~F. (2005).
\newblock {\em Numerical {{Solution}} of {{Partial Differential Equations}}:
  {{An Introduction}}}.
\newblock {Cambridge University Press}, {Cambridge}, second edition.

\bibitem[Niederreiter, 1992]{Niederreiter1992-qb}
Niederreiter, H. (1992).
\newblock {\em Random Number Generation and {Quasi-Monte} Carlo Methods}.
\newblock CBMS-NSF Regional Conference Series in Applied Mathematics. Society
  for Industrial and Applied Mathematics.

\bibitem[Paszke et~al., 2017]{Paszke2017}
Paszke, A., Chanan, G., Lin, Z., Gross, S., Yang, E., Antiga, L., and Devito,
  Z. (2017).
\newblock Automatic differentiation in {{PyTorch}}.
\newblock In {\em {{NeurIPS2017 Workshop}} on {{Autodiff}}}, pages 1--4.

\bibitem[Pokkunuru et~al., 2023]{Pokkunuru2023}
Pokkunuru, A., Rooshenas, P., Strauss, T., Abhishek, A., and Khan, T. (2023).
\newblock Improved {{Training}} of {{Physics-Informed Neural Networks Using
  Energy-Based Priors}}: A {{Study}} on {{Electrical Impedance Tomography}}.
\newblock In {\em International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}.

\bibitem[Raissi et~al., 2019]{Raissi2019}
Raissi, M., Perdikaris, P., and Karniadakis, G.~E. (2019).
\newblock Physics-informed neural networks: {{A}} deep learning framework for
  solving forward and inverse problems involving nonlinear partial differential
  equations.
\newblock {\em Journal of Computational Physics}, 378:686--707.

\bibitem[Schaefer and Anandkumar, 2019]{Schaefer2019}
Schaefer, F. and Anandkumar, A. (2019).
\newblock Competitive {{Gradient Descent}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}
  ({{NeurIPS}})}, pages 1--11.

\bibitem[Schaefer et~al., 2020]{Schaefer2020}
Schaefer, F., Zheng, H., and Anandkumar, A. (2020).
\newblock Implicit competitive regularization in {{GANs}}.
\newblock In {\em International {{Conference}} on {{Machine Learning}}
  ({{ICML}})}, pages 8533--8544. {PMLR}.

\bibitem[Shankar et~al., 2018]{Shankar2018}
Shankar, V., Kirby, R.~M., and Fogelson, A.~L. (2018).
\newblock Robust {{Node Generation}} for {{Mesh-free Discretizations}} on
  {{Irregular Domains}} and {{Surfaces}}.
\newblock {\em SIAM Journal on Scientific Computing}, 40(4):A2584--A2608.

\bibitem[Sharma and Shankar, 2022]{Sharma2022}
Sharma, R. and Shankar, V. (2022).
\newblock Accelerated {{Training}} of {{Physics-Informed Neural Networks}}
  ({{PINNs}}) using {{Meshless Discretizations}}.
\newblock In {\em Advances in {{Neural Information Processing Systems}}}.

\bibitem[Sloan and Joe, 1994]{Sloan1994-cl}
Sloan, I.~H. and Joe, S. (1994).
\newblock {\em Lattice Methods for Multiple Integration}.
\newblock Clarendon Press.

\bibitem[Thomas, 1995]{Thomas1995}
Thomas, J.~W. (1995).
\newblock {\em Numerical {{Partial Differential Equations}}: {{Finite
  Difference Methods}}}, volume~22 of {\em Texts in {{Applied Mathematics}}}.
\newblock {Springer}, {New York, NY}.

\bibitem[Thompson et~al., 1985]{Thompson1985-tx}
Thompson, J.~F., Warsi, Z. U.~A., and Mastin, C.~W. (1985).
\newblock {\em Numerical grid generation: foundations and applications}.
\newblock Elsevier North-Holland, Inc., USA.

\bibitem[Vaswani et~al., 2017]{Vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In {\em Advances in {{Neural Information Processing Systems}}
  ({{NeurIPS}})}.

\bibitem[Wang et~al., 2022a]{Wang2022b}
Wang, C., Li, S., He, D., and Wang, L. (2022a).
\newblock Is \${{L}}\^2\$ {{Physics Informed Loss Always Suitable}} for
  {{Training Physics Informed Neural Network}}?
\newblock In {\em Advances in {{Neural Information Processing Systems}}
  ({{NeurIPS}})}.

\bibitem[Wang and Perdikaris, 2023]{Wang2023}
Wang, S. and Perdikaris, P. (2023).
\newblock Long-time integration of parametric evolution equations with
  physics-informed {{DeepONets}}.
\newblock {\em Journal of Computational Physics}, 475:111855.

\bibitem[Wang et~al., 2021]{Wang2021c}
Wang, S., Teng, Y., and Perdikaris, P. (2021).
\newblock Understanding and {{Mitigating Gradient Flow Pathologies}} in
  {{Physics-Informed Neural Networks}}.
\newblock {\em SIAM Journal on Scientific Computing}, 43(5):A3055--A3081.

\bibitem[Wang et~al., 2022b]{Wang2022a}
Wang, S., Yu, X., and Perdikaris, P. (2022b).
\newblock When and why {{PINNs}} fail to train: {{A}} neural tangent kernel
  perspective.
\newblock {\em Journal of Computational Physics}, 449:110768.

\bibitem[Wang and Lin, 1998]{Wang1998}
Wang, Y.~J. and Lin, C.~T. (1998).
\newblock Runge-{{Kutta}} neural network for identification of dynamical
  systems in high accuracy.
\newblock {\em IEEE Transactions on Neural Networks}, 9(2):294--307.

\bibitem[Zaremba, 2014]{Zaremba2014-gj}
Zaremba, S.~K. (2014).
\newblock {\em Applications of Number Theory to Numerical Analysis}.
\newblock Academic Press.

\bibitem[Zeng et~al., 2023]{Zeng2023}
Zeng, Q., Kothari, Y., Bryngelson, S.~H., and Schaefer, F.~T. (2023).
\newblock Competitive {{Physics Informed Networks}}.
\newblock In {\em International {{Conference}} on {{Learning Representations}}
  ({{ICLR}})}, pages 1--12.

\end{thebibliography}

\clearpage
\newpage
\appendix
\renewcommand\thetable{A\arabic{table}}
\setcounter{table}{0}
\renewcommand\thefigure{A\arabic{figure}}
\setcounter{figure}{0}

{\Huge Appendix}

\section{Theoretical Background}\label{appendix:theory}
In this section, as a theoretical background, we will provide a proof of Theorem 1, explain why the Fibonacci sequence gives a good lattice, and show how to transform a non-periodic function into a periodic one.

\subsection{Proof of Theorem 1}

While Theorem~\ref{thm:goodlattice} in the main body focuses on the original physics-informed loss, in this section, we start with a general form.
We aim at the objective function expressed as the form
\begin{equation}\label{eq:dlossgen}
  \frac{1}{N}\sum_{j=1}^{N} \mathcal{P}[\tilde u](\bm{x}_j),
\end{equation}
where $\mathcal{P}$ is an operator that maps a function of $\bm{x}$ to another function of $\bm{x}$.
The original physics-informed loss $\frac{1}{N}\sum_{j=1}^{N} \|\mathcal N[\tilde{u}](\bm{x}_j)\|^2$ is an example of this form, where its operator is $\mathcal{P}:\tilde u\mapsto \|\mathcal N[\tilde{u}]\|^2$.
This form also includes the loss function $\frac{1}{N}\sum_{j=1}^{N}\|\tilde u(\bm{x}_j)-g(\bm{x}_j)\|^2$ for learning the Dirichlet boundary condition $u(\bm{x})=g(\bm{x})$ at $\bm{x}\in\partial\Omega$, the objective function $\frac{1}{N}\sum_{j=1}^{N}D(\bm{x}_j)\mathcal N[\tilde{u}](\bm{x}_j)$ for CPINNs~\cite{Zeng2023}, and those for other variants~\cite{Sharma2022}.
The objective function \eqref{eq:dlossgen} can be regarded as a finite approximation to
\begin{equation}\label{eq:clossgen}
  \int_{[0,1]^s} \mathcal{P}[\tilde u](\bm{x})\mathrm{d}\bm{x}.
\end{equation}

The main result is the following theorem.

\begin{thm}\label{thm:goodlattice2}
  Suppose that the activation function of $\tilde{u}$ and hence $\tilde{u}$ itself are sufficiently smooth so that there exists an $\alpha > 0$ such that $\mathcal{P}[\tilde{u}] \in E_\alpha$. Then, for given integers $N \geq 2$ and $s \geq 2$, there exists an integer vector $\bm{z} \in \mathbb{Z}^s$ such that $L=\{ \mbox{decimal part of\ } \frac{j}{N} \bm{z}  \mid j=0,\ldots,N-1\}$ and
  \begin{equation*}%\label{eq:goodlattice}
    %\sum_{h \in \mathbb{Z}^s, h \neq 0, h \cdot z \pmod N \equiv 0} \frac{1}{(\bar{h}_1 \bar{h}_2 \cdots \bar{h}_s)^\alpha}
    \left|
    \int_{[0, 1]^s} \mathcal{P}[\tilde u](\bm{x}) \mathrm{d}\bm{x}
    -
    \frac{1}{N}\sum_{\bm{x}_j \in L^*} \mathcal{P}[\tilde u](\bm{x}_j)
    \right|
    =
    O\left(\frac{(\log N)^{\alpha s}}{N^\alpha}\right). %+O(\frac{(\log N)^{\alpha s - 1}}{N^\alpha}).
  \end{equation*}
\end{thm}
Intuitively, if $\mathcal{P}[\tilde{u}]$ satisfies certain conditions, we can find a set $L^*$ of collocation points with which the objective function \eqref{eq:dlossgen} approximates the integral \eqref{eq:clossgen} only within an error of $O(\frac{(\log N)^{\alpha s}}{N^\alpha})$.
This rate is much better than that of the Monte Carlo method, which is of $O(1/\sqrt{N})$~\cite{Sloan1994-cl}.

To prove Theorem~\ref{thm:goodlattice2}, we introduce a theorem in the field of number theoretic numerical analysis, in which the function
\begin{equation*}
  P_\alpha(\bm{z}, N) = \sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} \frac{1}{(\bar{h}_1 \bar{h}_2 \cdots \bar{h}_s)^\alpha}
\end{equation*}
is bounded:
\begin{thm}[\cite{Sloan1994-cl}]\label{thm:support}
  For integers $N \geq 2$ and $s \geq 2$, there exists a $\bm{z} \in \mathbb{Z}^s$ such that
  \begin{equation*}
    P_\alpha(\bm{z}, N) \leq \frac{(2 \log N)^{\alpha s}}{N^\alpha} + O\left(\frac{(\log N)^{\alpha s -1}}{N^\alpha}\right).
  \end{equation*}
\end{thm}
Recall that $\bar{k} = \max(1, |k|)$ for $k \in \mathbb{R}$.
Suppose $\mathcal{P}[\tilde{u}]$ is sufficiently smooth and can be expanded into Fourier series using Fourier coefficients $\hat{\mathcal{P}}[\tilde{u}]$.
Then, $|\eqref{eq:clossgen}-\eqref{eq:dlossgen}|$ yields
\begin{equation*}
  \left|\int_{[0, 1]^s} \mathcal{P}[\tilde u](\bm{x}) \mathrm{d}\bm{x}
  -
  \frac{1}{N}\sum_{j=1}^{N} \mathcal{P}[\tilde u](\bm{x}_j)\right|
  =
  \left|\frac{1}{N} \sum_{j=1}^{N}\sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0} \hat{\mathcal{P}}[\tilde u](\bm{h}) \exp(2 \pi \mathrm{i}  \bm{h} \cdot \bm{x}_j)\right|.
\end{equation*}
We restrict collocation points $\bm{x}_j$ to the form $ L= \{ \mbox{decimal part of\ } \frac{j}{N} \bm{z} \mid j=0,\ldots,N-1\}$ with a fixed integer vector $\bm{z}$.
Following Lemma~\ref{lem:lemma}, we get
\begin{equation*}
  \begin{aligned}
    \left|
    \int_{[0, 1]^s} \mathcal{P}[\tilde u](\bm{x}) \mathrm{d}\bm{x}
    -
    \frac{1}{N}\sum_{\bm{x}_j\in L^*} \mathcal{P}[\tilde u](\bm{x}_j)
    \right|
    %  & =
    % \left|\sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} \hat{\mathcal{P}}[\tilde{u}](\bm{h}) \right| \\
     & \le
    \sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} \left|\hat{\mathcal{P}}[\tilde{u}](\bm{h}) \right|.
  \end{aligned}
\end{equation*}
Because of Definition \ref{dfn:Korobov}, if $\mathcal{P}[\tilde{u}] \in E_\alpha$ for some $\alpha$, there exists $c > 0$ such that
\begin{equation*}
  \left|\int_{[0, 1]^s} \mathcal{P}[\tilde{u}] \mathrm{d}\bm{x} - \frac{1}{N}\sum_{\bm{x}_j\in L^*} \mathcal{P}[\tilde{u}](\bm{x}_j) \right|
  \le \sum_{\bm{h} \in \mathbb{Z}^s, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} \frac{c}{(\bar{h}_1 \bar{h}_2 \cdots \bar{h}_s)^\alpha}=c P_\alpha(\bm{z}, N).
\end{equation*}
Because of Theorem~\ref{thm:support}, for integers $N \geq 2$ and $s \geq 2$, there exists a $\bm{z} \in \mathbb{Z}^s$ such that
\begin{equation}\label{eq:bounded}
  \left|\int_{[0, 1]^s} \mathcal{P}[\tilde{u}] \mathrm{d}\bm{x} - \frac{1}{N}\sum_{\bm{x}_j\in L^*} \mathcal{P}[\tilde{u}](\bm{x}_j) \right|
  \le \frac{c(2 \log N)^{\alpha s}}{N^\alpha} + O\left(\frac{(\log N)^{\alpha s -1}}{N^\alpha}\right).
\end{equation}
Then, we can obtain Theorem~\ref{thm:goodlattice2}.

The assumption that $\mathcal{P}[\tilde{u}]$ can be expanded using Fourier coefficients and is in the Korobov space $E_\alpha$ suggests that $\mathcal{P}[\tilde{u}]$ is periodic.
In general, $\mathcal{P}[\tilde{u}]$ is not periodic; see Appendix~\ref{appendix:periodization} for such cases.

Since $\mathcal{P}[\tilde{u}](\bm{x})=\|\mathcal{N}[\tilde{u}](\bm{x})\|^2$ for the original physics-informed loss is non-negative, from Theorem~\ref{thm:goodlattice2}, we can say Theorem~\ref{thm:goodlattice}:
% \begin{equation*}
%   \left|\int_{[0, 1]^s} \mathcal{P}[\tilde{u}] \mathrm{d}\bm{x} - \frac{1}{N}\sum_{j=1}^{N} \mathcal{P}[\tilde{u}](\bm{x}_j) \right|\le
% \end{equation*}
% \begin{equation*}
%   \int_{[0, 1]^s} \mathcal{P}[\tilde{u}] \mathrm{d}\bm{x}
%   \leq
%   \frac{1}{N}\sum_{j=1}^{N} \mathcal{P}[\tilde{u}](\bm{x}_j)
%   + c P_\alpha(\bm{z}, N).
% \end{equation*}
% Meanwhile, from the above theorem, there exists a $\bm{z} \in \mathbb{Z}^s$ that gives
% \begin{equation*}
%   P_\alpha(\bm{z}, N)
%   \leq \frac{(2 \log N)^{\alpha s}}{N^\alpha}  + O\left(\frac{(\log N)^{\alpha s -1}}{N^\alpha}\right).
% \end{equation*}
% Therefore we get for this $\bm{z}$
\begin{equation*}
  \int_{[0, 1]^s} \| \mathcal{N}[\tilde{u}](\bm{x}) \|^2 \mathrm{d}\bm{x}
  \leq
  \frac{1}{N}\sum_{\bm{x_j}\in L^*} \| \mathcal{N}[\tilde{u}](\bm{x}_j) \|^2
  + O\left(\frac{(\log N)^{\alpha s}}{N^\alpha}\right).
  %  + O(\frac{d(\alpha, s)(\log N)^{\alpha s}}{N^\alpha}), \quad d(\alpha, s) = 2^{\alpha s}.
\end{equation*}
This implies that the minimization of the physics-informed loss \eqref{eq:dloss} minimizes the integral \eqref{eq:closs} only with a small gap.

%\subsection{Why the Fibonacci sequence gives a good lattice.}
\subsection{Lattice Defined by Fibonacci Sequence and Diophantine Approximation}
It is known that for $s=2$, $\bm{z} \in \mathbb{Z}^2$ in Theorem~\ref{thm:goodlattice2} can be constructed by using the Fibonacci sequence~\cite{Niederreiter1992-qb, Sloan1994-cl}. Specifically,
\begin{align*}
  F_1 = F_2 = 1, \quad F_{k} = F_{k-1} + F_{k-2} \ (k \geq 3),
\end{align*}
$\bm{z} = (1, F_{k-1})^\top$ with $N= F_k$ satisfies the inequality in \eqref{eq:bounded}.
It is known that $\bm{h} \cdot \bm{z} \equiv 0 \pmod N$ gives a small $\bar{h}_1 \bar{h}_2$, making $P_\alpha(\bm{z}, N)$ large.
Hence, it is preferable to choose $\bm{z}$ so that $\bm{z}$  maximizes a minimized $\bar{h}_1 \bar{h}_2$:
\begin{align*}
  \max_{\bm{z} \in \mathbb{Z}^2} \quad \min_{\bm{h} \in \mathbb{Z}^2, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} \bar{h}_1 \bar{h}_2.
\end{align*}
Because $\{ F_k \}$ is the Fibonacci sequence, it holds that for $\bm{h} = (F_{k-2}, 1)^\top$
\begin{align*}
  \bm{h} \cdot \bm{z}=
  % \bm{z} \cdot \begin{pmatrix} F_{k-2} \\ 1 \end{pmatrix}
  \begin{pmatrix} F_{k-2} \\ 1 \end{pmatrix}
  \cdot
  \begin{pmatrix} 1 \\ F_{k-1} \end{pmatrix}
  = F_{k-2} + F_{k-1} = F_{k} = N \equiv 0 \pmod N.
\end{align*}
This $\bm{h}$ gives $\bar{h}_1 \bar{h}_2 = F_{k-2}$. % but
It is known that this achieves a small enough $\bar{h}_1 \bar{h}_2$.
Since $F_k$ is written as
\begin{align*}
  F_k = \frac{1}{\sqrt{5}} \left(
  \left(\frac{1+\sqrt{5}}{2}\right)^k - \left(\frac{1-\sqrt{5}}{2}\right)^k
  \right),
\end{align*}
$F_k/F_{k-2} \to ((1+\sqrt{5})/2)^2$ as $k \to \infty$. Hence, if $N=F_k$, then $\bar{h}_1 \bar{h}_2 = F_{k-2} \to N/((1+\sqrt{5})/2)^2$ as $k \to \infty$. Hence, roughly, $1/ (\bar{h}_1 \bar{h}_2)^\alpha$ in $P_\alpha(\bm{z}, N)$ becomes $O(1/N^\alpha)$.
See~\cite{Niederreiter1992-qb, Sloan1994-cl} for more strict proof.

The following explains why the Fibonacci sequence is better from a more number-theoretic point of view. For an integer $N \geq 2$, let $\bm{z} = (1, z_2) \in \mathbb{Z}^2$ with $\gcd(z_2, N)=1$. Let the continued fraction expansion of the rational number $z_2/N$ be
\begin{align*}
  \frac{z_2}{N}=a_0 + 1/(a_1 + 1/(a_2 + \cdots) ),
\end{align*}
where $a_0 = \lfloor \frac{z_2}{N} \rfloor$ and $a_i \in \mathbb{N}$ for $1\le i\le k$.
Let $r_i$ be the rational number obtained by truncating this expansion up to $a_i$. It can be confirmed that $r_i$ can be written as $r_i = p_i/q_i$ using $q_i, p_i \in \mathbb{Z}$, obtained as follows.
\begin{align*}
  %& p_{-2} = 0, p_{-1}=1, p_i = a_i p_{i-1}+p_{i-2}, \\
  %& q_{-2} = 1, q_{-1}=0, q_i = a_i q_{i-1}+q_{i-2}.
   & p_{0} = a_0, p_{1}=a_0 a_1 + 1, p_i = a_i p_{i-1}+p_{i-2}, \\
   & q_{0} = 1, q_{1}=a_1, q_i = a_i q_{i-1}+q_{i-2}.
\end{align*}
It is also confirmed that this gives the reduced form: $\gcd(p_i, q_i )=1$~\cite{Niederreiter1992-qb}.

Such a continued fraction expansion is used in the Diophantine approximation. The Diophantine approximation is one of the problems in number theory, which includes the problem of approximating irrational numbers by rational numbers. The approximations are classified into ``good'' approximation and ``best'' approximation according to the degree of the approximation, and it is known that the above continued fraction expansion gives a ``good'' approximation.
This is why the lattice $L$ is called a ``good'' lattice.
The accuracy of the approximation has also been studied, and it is known that in the above case, the following holds~\cite{Niederreiter1992-qb}:
\begin{align*}
  \frac{1}{q_i (q_i + q_{i+1})} \leq \left| \frac{z_2}{N}  - \frac{p_i}{q_i}\right| \leq \frac{1}{q_i q_{i+1}},
\end{align*}
from which it follows \cite{Niederreiter1992-qb} that
\begin{align*}
  \frac{N}{\max_{1 \leq i \leq k} a_i + 2} \leq \min_{\bm{h} \in \mathbb{Z}^2, \bm{h} \neq 0, \bm{h} \cdot \bm{z} \equiv 0 \pmod N} \bar{h}_1 \bar{h}_2 \leq \frac{N}{\max_{1 \leq i \leq k} a_i}.
\end{align*}
Hence, to maximize $\min_{\bm{h}} \bar{h}_1 \bar{h}_2$, it is preferable to use the pair of $z_2$ and $N$ such that $\max_{1 \leq i \leq k} a_i$ is as small as possible. To this end, $a_i$'s should be determined by $a_0 = 0$ and $a_i = 1$ for all $i \geq 1$. Substituting these $a_i$'s into the formulas for $q_i$ and $p_i$ results in
\begin{align*}
   & p_0 = 0, p_1 = 1, p_2 = 1, p_3 = 2, \ldots \\
   & q_0 = 1, q_1 = 1, q_2 = 2, q_3 = 3, \ldots
\end{align*}
which means $p_i = F_{i-1}, q_i = F_{i}$ where $F_i$'s are the Fibonacci sequence. Hence, the pair of $z_2 = F_{k-1}$ and $N = F_k$ gives a ``good'' lattice for computing objective functions.

\subsection{Periodization of Integrand}\label{appendix:periodization}
The method in this paper relies on the Fourier series expansion and hence assumes that the integrand can be periodically extended. In order to extend a given function periodically, it is convenient if the function always vanishes on the boundary. To this end, variable transformations are useful~\cite{Niederreiter1992-qb, Sloan1994-cl}.

For example, suppose that a function $f: [0, 1] \to \mathbb{R}$ does not satisfy $f(0) = f(1)$ and the integral on an interval $[0, 1]$
\begin{align*}
  \int_0^1 f(x) \mathrm{d} x
\end{align*}
must be evaluated as an objective function. Let $y: [0, 1] \to [0, 1]$ be a monotonically increasing smooth map. Then the change of variables
\begin{align*}
  x = y(z) \quad z \in [0, 1]
\end{align*}
transforms the integral into
\begin{align*}
  \int_0^1 f(x) \mathrm{d} x = \int_0^1  f(y(z)) \frac{\mathrm{d} y}{\mathrm{d} z}(z) \mathrm{d}z.
\end{align*}
Hence, if the derivative $\mathrm{d}y/\mathrm{d}z$ vanishes at $z = 0$ and $z=1$, the integrand becomes periodic. Functions that satisfy these conditions can be easily constructed using polynomials. For example, it is sufficient that this derivative is proportional to $z(1-z)$. Since $z (1-z) \geq 0$ on the interval $[0, 1]$, integration of $z(1-z)$ defines a monotonically increasing smooth function, and hence this can be used to define the function $y(z)$. Smoother transformations can be designed in a similar way by using higher-order polynomials with the constraints such as $\mathrm{d} y/\mathrm{d}z = \mathrm{d}^2 y/\mathrm{d}z^2 = 0$ at $z=0$ and $z=1$~\cite{Sloan1994-cl}.


\section{Datasets and Experimental Settings}\label{appendix:datasets}
In the main body, we denote the set of coordinates by $\bm{x}$.
In this section, we denote each coordinate separately: the space by $x$ or $y$ and the time by $t$.
We obtained the datasets of the nonlinear Schr\"{o}dinger (NLS) equation, Korteweg--De Vries (KdV) equation, and Allen-Cahn (AC) equation from the official repository\footref{footnote:PINNs} of \citet{Raissi2019}.
These are 2D PDEs, involving 1D space $x$ and 1D time $t$.
The solutions were obtained numerically using spectral Fourier discretization and a fourth-order explicit Runge--Kutta method.
In addition, we created datasets of Poisson's equation.

\paragraph{Nonlinear Schr\"odinger Equation}
A 1D NLS equation is expressed as
\begin{equation}
  \mathrm{i} u_t+\gamma u_{xx}+ |u|^{p-1}u=0,
\end{equation}
where the coordinate is $\bm{x}=(x,t)$, the state $u$ is complex, and $\mathrm{i}$ denotes the imaginary unit.
This equation is a governing equation of the wave function in quantum mechanics.
The linear version, which does not have the third term, is hyperbolic.
We simply treated a complex state $u$ as a 2D real vector $(w,v)$ for $u=w+\mathrm{i}v$ for training
We used $\gamma=0.5$ and $p=3$, which resulted in the following equations:
\begin{equation}
  \begin{aligned}
    w_{t}  + 0.5v_{xx} + (w^2+v^2)v =0, \\
    v_{t}  - 0.5w_{xx} - (w^2+v^2)w =0.
  \end{aligned}
\end{equation}
We used the domain $\Omega=[-5,5]\times[0,\pi/2]\ni(x,t)$ and the periodic boundary condition.
The initial condition was $u(x,0)=u_0(x)=\mathrm{sech}(x)$.
For evaluation, the numerical solution $u$ at $N_e=256\times201$ uniformly spaced points was provided, at which we calculated the relative error $\mathcal L$ of the absolute value $h=\sqrt{u^2+v^2}$.
To learn the initial condition, $N_0=50$ collocation points $x_1,x_w,\dots,x_{N_0}$ were randomly sampled, with which the mean squared error of the state was calculated; namely $\frac{1}{N_0}\sum_{j=1}^{N_0} |\tilde{u}(x_j,0)-u_0(x_j)|^2_2$.
Also for the boundary condition, $N_b=50$ collocation points $t_1,t_2,\dots,t_{N_b}$ were randomly sampled, with which the mean squared errors of the state and the derivative were calculated; namely $\frac{1}{N_b}\sum_{j=1}^{N_b} |\tilde{u}(-5,t_j)-\tilde{u}(5,t_j)|^2_2$ and $\frac{1}{N_b}\sum_{j=1}^{N_b} |\tilde{u}_x(-5,t_j)-\tilde{u}_x(5,t_j)|^2_2$.
Then, the neural network was trained to minimize the sum of these three errors and the physics-informed loss.

\paragraph{Korteweg--De Vries (KdV) Equation}
A 1D KdV equation is a hyperbolic PDE expressed as
\begin{equation}
  \textstyle u_t+\lambda_1 uu_x+\lambda_2u_{xxx}=0,
\end{equation}
for the coordinate $\bm{x}=(x,t)$.
The KdV equation is a model of shallow water waves.
We used $\lambda_1=1$ and $\lambda_2=0.0025$, the domain $\Omega=[-1,1]\times[0,1]\ni(x,t)$, and the periodic boundary condition.
The initial condition was $u(x,0)=u_0(x)=\cos(\pi x)$.
For evaluation, the numerical solution $u$ at $N_e=512\times201$ uniformly spaced points were provided.
To guarantee the periodic boundary condition, we normalized the space $x$ as $\hat x=\frac{x+1}{2}\in[0,1]$ and mapped it into the unit circle as $(x_1,x_2)=(\cos(2 \pi \hat x),\sin(2 \pi \hat x))$; hence, the input to the neural network is $(x_1,x_2,t)$.
Also, we mixed the initial condition $u_0(x)$ and the output $\tilde{u}(x,t)$ of the neural network as $\exp(-t)u_0(x)+(1-\exp(-t))\tilde{u}(x,t)$, thereby guaranteeing the initial condition.
Hence, the physics-informed loss is only the objective function.
This technique was introduced by \citet{Lagaris1998}; it improved the final performance and, most importantly, evaluated the effect of the number $N$ of collocation points independently from the initial and boundary conditions.

\paragraph{Allen--Cahn Equation}
A 1D AC equation is a parabolic PDE expressed as
\begin{equation}
  \textstyle u_t-pu^3-ru-qu_{xx}=0,
\end{equation}
for the coordinate $\bm{x}=(x,t)$.
The AC equation describes the phase separation of co-polymer melts.
We used $p=0.0001$, $r=-5$, $q=5$, the domain $\Omega=[-1,1]\times[0,1]\ni(x,t)$, and the periodic boundary condition.
The initial condition was $u(x,0)=u_0(x)=x^2\cos(\pi x)$.
For evaluation, the numerical solution $u$ at $N_e=512\times201$ uniformly spaced points was provided.
We guaranteed the initial and boundary conditions in the same manner as for the KdV equation.

\paragraph{Poisson's Equation}
2D Poisson's equation is expressed as
\begin{equation}
  \Delta u+f=0,
\end{equation}
where $\Delta$ is Laplacian and $f$ is a function of coordinates, state, and its first-order derivatives.
It is elliptic if the term $f$ is linear.
The case of $s=2$ dimensions with the coordinate $\bm{x}=(x,y)$ is expressed as
\begin{equation}
  u_{xx}+u_{yy}+f(x,y,u_x,u_y)=0.
\end{equation}
For the coordinate $\bm{x}=(x^{(1)},\dots,x^{(s)})$ in general, we used the domain $\Omega=[0,1]^s$, the Dirichlet boundary condition $u(\bm{x})=0$ at $\partial\Omega$, and $f(\bm{x})=\prod_{i=1}^s\sin(k\pi x^{(i)})$.
Then, we obtained the analytic solution as $u(\bm{x})=\prod_{i=1}^s\sin(k\pi x^{(i)})/(-sk^2\pi^2)$; there are $k^s$ peaks or valleys.
To ensure the boundary condition, we multiplied the output $\tilde u$ of the neural network by $\prod_{i=1}^sx^{(i)}(x^{(i)}-1)$.
For evaluation, we obtained the analytic solution $u$ at uniformly spaced points of $N_e=999^2$ for $s=2$ and $N_e=49^4$ for $s=4$.

\begin{table}[t]
  \centering
  \footnotesize
  \caption{Best Results when Varying Numbers of Collocation Points and Iterations}
  \label{tab:appendix:datasets}
  \begin{tabular}{llrrr}
    \toprule
    % &                & Schr\"{o}dinger & Korteweg--De Vries & Allen-Cahn & Poisson's \\
                                         &                                             & \multicolumn{1}{c}{NLS} & \multicolumn{1}{c}{KdV} & \multicolumn{1}{c}{AC} \\
    % & & hyperbolic      & hyperbolic         & parabolic  & elliptic  \\
    \midrule
                                         & \textcolor{C4}{$\bullet$}\,uniformly random & 4.07                    & 2.69                    & 0.900                  \\
    relative error $\mathcal L^\ddagger$ & \textcolor{C8}{$\bullet$}\,uniformly spaced & 2.23                    & 3.06                    & 0.478                  \\
                                         & \textcolor{C1}{$\bullet$}\,LHS              & 3.32                    & 2.24                    & 0.628                  \\
                                         & \textcolor{C0}{$\bullet$}\,GLT (proposed)   & \textbf{1.26}           & \textbf{1.48}           & \textbf{0.434}         \\
    \bottomrule
  \end{tabular}\\
  $\ddagger$  Shown in the scale of $10^{-3}$.
\end{table}

% Figure environment removed

\paragraph{Burgers' Equation}
1D Burgers' equation is expressed as
\begin{equation*}
  u_t+uu_x-\nu u_{xx}=0
\end{equation*}
for the coordinate $\bm{x}=(x,t)$.
Burgers' equation is a convection-diffusion equation describing a nonlinear wave.
We used $\nu=(0.01/\pi)$, the domain $\Omega=[-1,1]\times[0,1]\ni(x,t)$, the Dirichlet boundary condition $u(-1,t)=t(1,t)=0$, and the initial condition $u(x,0)=u_0(x)=-\sin(\pi x)$.
For evaluation, the numerical solution $u$ at $N_e=256\times 100$ uniformly spaced points was provided.
From these points, we used $N_0=256$ and $N_b=100$ collocation points for learning the initial condition and boundary condition, respectively.
The loss functions for these conditions were identical to those for the NLS equation.

\section{Additional Experiments and Results}\label{appendix:results}
When neural networks are trained using the SGD methods, such as Adam optimizer, and a different set of collocation points is sampled at every iteration, its number $N$ can be regarded as the mini-batch size.
Neural networks might be undertrained with a small number $N$ and potentially work much better after a longer training period.
% This assumption is evidenced by the well-known fact that even though the batch size is small, the SGD methods converge to the local minimum after a sufficiently long time.
To confirm this assumption, we equalized the product of the number $N$ of collocation points and the number of training iterations.
Specifically, we set the number of training iterations to $200000\times 1000/N$, indicating that the training period becomes longer for $N<1000$ and shorter for $N>1000$ compared to Table~\ref{tab:datasets} and Fig.~\ref{fig:PINNsResults}.

We summarized the results in Table~\ref{tab:appendix:datasets} and depicted them in Fig.~\ref{fig:appendix:PINNsResults}.
The proposed GLT outperformed comparison methods across all datasets and for a wide range of collocation points.
In each iteration, PINNs are trained locally using a given set of collocation points, which may lead to forgetting training outcomes outside those regions, resulting in poor overall performance.
Therefore, also in this case, it is crucial to select collocation points that cover the entire domain efficiently, and once again, the proposed GLT outperforms alternative methods.




\end{document}
