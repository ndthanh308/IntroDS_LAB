\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[preprint]{neurips_2023}
\usepackage{graphicx}
\usepackage{natbib}
\setcitestyle{numbers,square}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}

\usepackage{amsmath,amsfonts,amssymb,amsthm,mathrsfs,bm}
% Notice: complete ams package for equation
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}
\crefname{figure}{Fig.}{Figs.}

% \usepackage{subcaption} % for 'subtable' env
\usepackage{multirow} % for cmd 'multirow', 'multicolumn'

\title{Curvature-based Transformer for Molecular Property Prediction}

    
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  %Yili Chen$^1$, Zhengyu Li$^2$, Zheng Wan$3$,Hui Yu$^4$, Xian Wei$^4$\\
  %$^1$Fujian Normal University\\
  %$^2$East China Normal University\\
  %$^3$Jiangxi Normal University\\
  %$^4$FJIRSM, Chinese Academy of Sciences\\
  %\texttt{xwei@sei.ecnu.edu.cn} \\
  Yili Chen\\
  Fujian Normal University\\
  \And
  Zhengyu Li\\
  East China Normal University\\
  \And
  Zheng Wan\\
  Jiangxi Normal University\\
  \And
  Hui Yu\\
  FJIRSM, Chinese Academy of Sciences\\
  \And
  Xian Wei\\
  FJIRSM, Chinese Academy of Sciences\\
  \texttt{xwei@sei.ecnu.edu.cn} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And I'm
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}



\maketitle

\begin{abstract}
The prediction of molecular properties is one of the most important and challenging tasks in the field of artificial intelligence-based drug design. Among the current mainstream methods, the most commonly used feature representation for training DNN models is based on SMILES and molecular graphs, although these methods are concise and effective, they also limit the ability to capture spatial information. In this work, we propose Curvature-based Transformer to improve the ability of Graph Transformer neural network models to extract structural information on molecular graph data by introducing Discretization of Ricci Curvature. To embed the curvature in the model, we add the curvature information of the graph as positional Encoding to the node features during the attention-score calculation. This method can introduce curvature information from graph data without changing the original network architecture, and it has the potential to be extended to other models. We performed experiments on chemical molecular datasets including PCQM4M-LST, MoleculeNet and compared with models such as Uni-Mol, Graphormer, and the results show that this method can achieve the state-of-the-art results. It is proved that the discretized Ricci curvature also reflects the structural and functional relationship while describing the local geometry of the graph molecular data. 

\end{abstract}


\section{Introduction}

Drug development is a lengthy, costly, and intricate process, involving drug discovery, clinical trials, and production approval. 
In recent years, deep learning-based molecular property prediction methods using data represented in SMILES~\cite{1984JOURNAL} strings have gained attention, for their potential to assist in drug discovery. 
Natural language processing (NLP) techniques have been applied to directly handle molecular SMILES, treating molecule generation as a Seq2Seq problem~\cite{2019N}. 
However, these methods disregard the natural topology of molecules and are insufficient for analyzing molecular data with temporal models alone. 

Molecular data can be effectively represented as a graph, where atoms are nodes and chemical bonds are edges. 
This graph representation preserves the topological relationship between atoms. 
Graph convolutional networks update node features by aggregating information from adjacent nodes and edges, improving the competitiveness of molecular modeling tasks~\cite{2017Neural,Kearnes0Molecular}. 
Some researchers have extended Transformers to graphs, combining attention mechanisms from NLP with Graph Neural Network (GNN) models, yielding promising results~\cite{2020A,2021Do,dwivedi2022benchmarking}. 
However, existing methods mostly use simple graph structure information, for example, Laplacian matrix, degree information, shortest path information, etc., for positional encoding, overlooking  structural similarity, chemical properties, and complex geometric characteristics of molecules. 
Real-world graphical data exhibit heterogeneous topologies with diverse local structures, including tree and circular structures~\cite{2015Ricci,2018Learning}. 
topologies~\cite{2015Ricci,2018Learning}. 

Recently, researchers have incorporated 3D information of molecular structures into Transformers, recognizing that molecular properties and drug effects are heavily influenced by their 3D structures~\cite{2022ChemRxiv,2021Molformer}. However, capturing spatial information requires introducing additional units in the generator to account for Euclidean symmetries such as rotation, translation, and reflection. These generators are effective for small molecular systems, but they face increased complexity when applied to macro-molecules~\cite{ijms232113568}. 

Excitingly, mathematical invariants derived from differential geometry and algebraic topology are being viewed as descriptors for molecules. 
Advanced learning models based on these invariants have shown remarkable success in drug design, for their high level of abstraction and portability~\cite{Quantitative,2018Representability,2020Machine}. Ricci curvature, as a basic concept in differential geometry, captures the intrinsic properties of a manifold surface. 

Ricci curvature can also be applied in protein-ligand binding affinity prediction, evidenced by its state-of-the-art performance\cite{OllivierPersistentRicci}. 
Interestingly, this approach has also been extended to other organic and inorganic nanoscale particles, as highlighted in a recent study\cite{2022Unifying}. 
Additionally, significant progress has been made in the field of discrete curvature studies on graph data, particularly with respect to Ollivier Ricci curvature and Forman curvature, which effectively capture the intrinsic shape of discrete curvature information\cite{2020Ricci}. 

In this paper, we proposed \textit{Curvature-based Graph Transformer}, namely Curvature-GT, which utilizes Forman curvature and Ricci (Coarse) curvature on Graphomer to describe the local shape of graphs, while the latter one involves optimal transmission problem. The incorporation of curvature information in the Positional Encoding of Graph Transformer has two advantages: (1) a larger receptive field as Forman curvature operates on edges involving two nodes, and (2) better preservation of molecular chemical properties, addressing the limitations of node distance representation in functional groups. 
Experimental results demonstrate that Curvature-GT outperforms previous GNN-based models in molecular regression and prediction tasks. 

\section{Related Works}

\subsection{Transformer on Graph.} 
We will present the current progress in incorporating Transformers into graph structural data in three classical ways. 

First, by making Transformer as the infrastructure to inject GNNs modules, GraphTrans~\cite{Wu2021GraphTrans}, GraphiT~\cite{mialon2021graphit}, Graph-BERT~\cite{2020Graph} have adopted this way in their work. They first use the GNN layer to extract the feature vector of the graph, and then reanalyze the interaction relationship between these vectors by Transformer. Mesh Graphormer~\cite{lin2021mesh} alternately stacked GNN blocks and Transformer blocks to form a network layer and enhance the information interaction between the network layers through graph convolution. Graph-BERT~\cite{2020Graph} adopts the way to parallel the GNN block and Transformer block into a network layer. 

Secondly, 
Yao \textit{et al.}~[5], 
Min \textit{et al.}~[6], 
and MAT~\cite{li2022mat} use the information of the graph to enhance the attention matrix. Specifically, they use the graph mask mechanism to make different attention heads attend to different feature subspaces, so as to improve the model's feature extraction ability on the graph. This idea is also applied in the work of GraphiT~\cite{mialon2021graphit} and PLAN~\cite{2020Interpretable}. The former uses relative position coding of kernel functions on the graph to improve attention scores, and the latter  proposes a structure-aware self-attention to model the structural relationships. In addition, there are some graph Transformer models based on 3D Atomistic Graphs, such as Equiformer~\cite{liao2023equiformer} and Molformer~\cite{2021Molformer}, which can effectively capture the 3D representation of graphs. 

The last approach involves encoding the graph structure into the Positional Encoding (PE) vector before inputting it into the Transformer model. 
Hussain \textit{et al.}~\cite{2021Edge}, Cai and Lam~\cite{2020CL} take the adjacency matrix and the distance from a node to the root as the source of information for positional encoding, respectively. Kreuzer \textit{et al.}~\cite{kreuzer2021rethinking} uses a full Laplacian spectrum to learn the location of each node in a given graph, proposing a learnable PE. Graph-BERT~\cite{2020Graph} introduces three PE types to embed node location information, which shows strong performance on node classification and graph clustering tasks. It is worth mentioning that the design of Graphormer~\cite{2021Do} involves all of the above methods and demonstrates state-of-the-art results. The author applied a transformer to the message passing calculation of GNN and introduced three structural codings, namely Centrality Encoding, Spatial Encoding, and Edge Encoding. Spatial Encoding is then designed as a distance function and serves as a graph bias term via a learnable bias. 
And it captures the positional information with its centrality of the node degree. The author believes that the degree of a node reflects its importance in a graph. However, in a molecule, the number of bonds (degree) of atoms often does not accurately indicate their significance. The condition is shown in the right Molecular structure diagram of the upper half of \cref{fig:01}. 

\subsection{Discrete Ricci Curvature on Graph}

% Figure environment removed

The discrete curvature is taken as a measure of the graph structure on the manifold, and this measure does not change its topology~\cite{2013Stochastic}. It describes the inter-correlation case of the neighborhoods between a pair of nodes.
%
Most of the previous work on graph curvature in combining graph neural networks was done to optimize the data structure. Specifically, different curvature calculation methods are used to smooth the curvature of the graph data. In addition, the curvature on the graph is modified by adding the links or modifying the weights of the nodes on the graph, so that the curvature of the overall data tends to smooth, so as to enhance the network performance or alleviate the over-squashing~\cite{2020ICLR,2021Understanding}.
Current mainstream approaches are Ollivier Ricci curvature~\cite{OLLIVIER2009810,Lin2010Ricci}and Forman curvature \cite{Forman2003}.

Among them, Ollivier Ricci Curvature has been proved to be very successful in the communication network, but its calculation process involves the optimal transmission calculation problem between nodes. It has high complexity and is not suitable for graph prediction and graph regression problems with huge data volumes. However, the calculation of Forman Ricci curvature is relatively simple and applicable to both directed and undirected weighted graphs. It is well-suited for studying interaction relationship networks, protein structure networks, and molecular networks ~\cite{2016Forman}. This paper focuses on the prediction of molecular properties on chemical molecular formula data sets, so we propose to adopt the Forman Curvature and Ollivier-Ricci Coarse Curvature to design the new Positional Encoding.

\section{Curvature-based Transformer}
In this section, we will provide a comprehensive explanation of our approach, which involves incorporating curvature into the Transformer structure for predicting molecular properties. Firstly, we will provide a concise overview of the network pipeline. Subsequently, we will delve into the details of obtaining the curvature of the molecular data and describe how we  implement it in our model.

% Figure environment removed

\subsection{Graphformer for Molecular Property Prediction}

Graphformer has achieved success in predicting molecular properties, but its Centrality Encoding, derived from social networks~\cite{2021Do}, may not be as applicable in the context of chemistry. 
In chemistry, the importance of an atom is often determined by its bonding flexibility and its ability to connect with other groups. 
Therefore, introducing curvature-based encoding into the Graph Transformer model becomes necessary to generate more scientifically accurate results in the field of biochemical molecules. 

To incorporate curvature-based Transformer into molecular property prediction, it is crucial to preserve the graph structure of molecules. Building upon Graphformer, we have modified the vanilla Transformer\cite{2017Attention} by incorporating curvature-based structural encoding. 

\subsubsection{Encoder architectures} 
Curvature-based Transformer consists of multi-layer encoders, the structure of each layer is similar to the vanilla Transformer\cite{2017Attention}, with two modules: multi-head self-attention and position-wise feed-forward network. For better convergence\cite{2020On}, we modified the residual connections and layer normalization layers to be placed before the two modules mentioned above, illustrated in \cref{fig:pipeline}. 

Given a specific molecular data (SMILES string) with $n$ atoms, we can convert it into an undirected graph $G= (V, E)$, where $V = \{v_1, v_2, \cdots, v_n\}$ denote the atoms of the molecular and $E$ denotes the bond between two atoms. Then the node features of $V$ can be described as $X \in \mathbb{R}^{n \times 9}\ (X = \{x_1, x_2, \cdots, x_n\})$ and the edge features are $H= \{h_1, h_2, h_3,\ldots\}$, $h_i\in \mathbb{R}$. Preserving the molecular  graph structure in the Transformer is a problem that we urgently need to solve. Graphformer implements structural encoding before self-attention to assign Graph structure as additional signal into the network. Following the similar way, we propose our encoding methods as follows.

\paragraph{Curvature Encoding.} 

Degree centrality introduced in Graphformer~\cite{2021Do} fails to capture the influence of long-range molecular forces. To address this limitation, we propose using Ricci curvature as a measure of node importance. Ricci curvature, commonly used for smooth surfaces, can be extended to discrete structures like graphs. It precisely quantifies the sparsity or denseness of connections within a local structure. 
Edges with positive curvature indicate well-connected clusters, while edges with negative curvature represent connections between clusters. Therefore, curvature provides a more comprehensive representation of connectivity than degree centrality, with a broader receptive field. To incorporate curvature information into the graph structure, we introduce \textit{Curvature Encoding}. Please refer to Section \ref{sec:RicciCurvatureComputation} for further details. 

Assuming that the input $x_i\in \mathbb{R}^d$ is a $d$-dimensional embedding representation of the node features $X\in \mathbb{R}^{n \times d}$, we can simply assign the curvature information to the input $x_i$ as:
\begin{equation}
    h_i = x_i + z_{cur(v_i)},
\end{equation}

where $z_{cur(v_i)} \in \mathbb{R}^n$ is a learnable embedding vector specified by node curvature. 

\paragraph{Spatial Encoding.} To achieve a global receptive field in the Transformer and obtain the structural information of the graph, we implemented a \emph{Spatial Encoding} to encode the position of the input graph $G$ \cite{2021Do}. 
Specifically, we utilize the function $\phi\left(v_{i}, v_{j}\right): V \times V \rightarrow \mathbb{R}$ to measure the spatial relation between $v_i$ and $v_j$. $\phi(v_i,v_j)$ is shortest path computed by \emph{Dijkstra} algorithm, or -1 if disconnected. 
% If $v_i$ and $v_j$ are connected, the value of the function is the shortest path distance computed through \emph{Dijkstra} algorithm between these two nodes; otherwise, we set it to $-1$. 
In addition, we add a learnable bias $b$ to suppress extreme values. Therefore, the \emph{Spatial Encoding} can be represented as $b_{\phi\left(v_{i}, v_{j}\right)}$. 

\paragraph{Edge Encoding.} In molecular property prediction, the features of the edge structure indicate important properties. For example, the bonding between atoms determines the magnitude of their forces and the geometric structure of the molecule. Therefore, we drew inspiration from previous work \cite{lin2023multi,wang2020direct,2021Do} and set our \emph{Edge Encoding} as follows:
\begin{equation}
    c_{i j}=\frac{1}{N} \sum_{n=1}^{N} x_{e_{n}}\left(w_{n}^{E}\right)^{T},
\end{equation}
where $x_{e_{n}}$ is the feature of the $n$-th edge $e_n$ in the shortest path between the nodes $v_i$ and $v_j$, and $w_{n}^{E}$ is the corresponding weight embedding.

\paragraph{Self-Attention Computation.}
With the aggregated structural encoding information, the self-attention mechanism effectively captures the intricate correlations within the molecular graph. The comprehensive computation of our self-attention mechanism is represented as: 
\begin{equation}
    A_{i j}=\frac{\left(h_{i} W_{Q}\right)\left(h_{j} W_{K}\right)^{T}}{\sqrt{d}}+b_{\phi\left(v_{i}, v_{j}\right)}+c_{i j}.
\end{equation}
In the subsequent sections, we will unveil the remarkable enhancements achieved through the introduction of curvature with certain modifications.

\subsection{Ricci Curvature Computation}\label{sec:RicciCurvatureComputation}
After successfully incorporating \emph{Curvature Encoding} into our network, we hereby discuss how to calculate the curvature of a molecular graph.
\paragraph{Ollivier-Ricci Curvature.} As one of the most attractive graph curvatures, the effectiveness of Ollivier-Ricci curvature has been widely discussed in the literature. \cite{Villani2014Optimal} comes up with a mathematical interpretation of the notion of optimal transport and Ricci curvature on a graph. Ollivier’s Ricci curvature can quantify the strength of interaction or overlap between neighbors of a pair of nodes. To calculate it, we should first define the Wasserstein distance. 

Given two probability measures $\mu$ and $v$ on the metric space $M$, the coupling $\gamma(\mu,v)$ is a probability measure on $M$, such that the respective marginal distributions correspond to $\mu$ and $v$. Let $\Gamma(\mu,v)= \left \{ \gamma  \mid  \gamma(\mu,v)\ \texttt{is a coupling} \right \}$. 
Then the 1-Wasserstein distance in the continuous situation can be defined as: 
\begin{equation}
    W(\mu,v)=\inf \left\{\int_{M \times M} d(x,y) d \gamma(x,y) \mid \gamma \in \Gamma(\mu, v)\right\}. 
\end{equation}
For a graph $G = (V,E)$, where $v \in V$ is a vertex, $ C(v)$ = $\{ u\mid(v,u) \in E \}$. 
% Let $N(v) = C(v) \cup \{v\}$ be the neighbourhood of $v$
Thus, $d_v = |C(v)|$ is the degree of $v$, we define a probability measure on graph $G$, parameterized by $\alpha$ $\in$ [0,1], as follows: 
\begin{equation}
    m_v^\alpha(u) = 
    \begin{cases}
        \alpha, & u=v \\
        (1-\alpha)/d_v, & u\in C(v) \\
        0, & otherwise
    \end{cases}. 
\end{equation}

Then given two nodes $x,y \in V$ and $\alpha\in [0,1]$, we can define $m_x^{\alpha}$ and $x_y^{\alpha}$, and compute $W(m_x^{\alpha}, x_y^{\alpha})$. The discrete version of the calculation is presented as follows: 
\begin{equation}
    W(m_x^{\alpha}, m_y^{\alpha})=\inf_A \left\{ \sum_{x_i,y_j \ \in \ V} d(x_i, y_j) A(x_i, y_j) \mid A:\ \texttt{transportation plan} \right\}. 
\end{equation}

Let distance metric $d(x,y)$ be the length of the shortest path between $x$ and $y$. Then, by comparing the distance between Wasserstein and two nodes, We obtain the $\alpha$-Ricci curvature of the two node\cite{2019Ollivier} for $\kappa_\alpha(x,y)$: 
\begin{equation}
    \kappa_\alpha(x,y) = 1 - \frac{W(m_x^{\alpha}, m_y^{\alpha})}{d(x, y)}. 
\end{equation}

\paragraph{Ollivier-Ricci Coarse Curvature.} 
Since the computational process of an optimal transfer plan involves the linear planning problem, the cost of calculating Ollivier-Ricci curvature is enormous when facing large-scale datasets. 

We instead refer to a sub-optimal transportation plan proposed by Ni \textit{et al.}~\cite{2018Network}, using the average transportation distance $A(m_x, m_y)$ to calculate the curvature. Specifically, the transmission distance between node $x$ and node $y$ is determined by transporting an equal mass from each neighbor node $x_i$ of $x$ to the corresponding neighbor node $y_i$ of $y$, or vice versa. The transmission distance is calculated as the minimum value among these distances. The calculation is given as follows: 
\begin{equation}
    \kappa_\alpha(x,y) = 1 - \frac{A(m_x^\alpha, m_y^\alpha)}{d(x, y)}. 
\end{equation}

\paragraph{Forman-Ricci Curvature.} While Ollivier Ricci curvature stands out in the forefront, it is considered to be more suitable for studying information transfer in communication networks and might have possible limitations in interaction networks, such as inter-protein interaction networks and molecular networks. Therefore, we introduce the Forman curvature as an alternative metric. The calculation of Forman-Ricci curvature is based on the edges and is specifically applicable to undirected and weighted networks. Unlike Ollivier Ricci curvature, its computation is defined as follows:
\begin{equation}
    \mathcal{F}(e)=w_e\left(
        \frac{w_{v_i}}{w_e}+ \frac{w_{v_j}}{w_e} - \sum_{e_{v_i}\sim e,\ e_{v_j}\sim e}{\left[
            \frac{w_{v_i}}{\sqrt{w_e w_{e_{v_i}}}} + 
            \frac{w_{v_j}}{\sqrt{w_e w_{e_{v_j}}}}
        \right]}
    \right)
\end{equation}
where $e$ is the edge to be calculated, connected to $v_i$ and $v_j$, 
$w_e$ is the weight of edge $e$, 
$w_{v_i}$ and $w_{v_j}$ are the weights of two nodes, 
$e_{v_i}\sim e$, $e_{v_j}\sim e$ are sets of edges connected to $e$ via nodes $v_i$ and $v_j$. 
Furthermore, the Forman curvature on a node can be obtained by averaging all curvatures of the edges connected to node $v$ as follows: 
\begin{equation} 
    \mathcal{F}(v)=\frac{1}{\deg{(v)}}\sum _{e_v\sim v}\mathcal{F}(e_v)
\end{equation}
When Forman curvature is applied to undirected and unweighted networks, the weights of nodes and edges default to 1, reducing the curvature calculation to a simpler and more intuitive form: 
\begin{equation} 
    \mathcal{F}(e)=4-\sum _{v\sim e}{\deg{(v)}}
,\qquad
    \mathcal{F}(v)=4-\frac{1}{\deg{(v)}}\sum _{v_i\sim v}\deg{(v)}+\deg{(v_i)}
\end{equation}
where $v\sim e$ is the set of nodes connected to $e$, $v_i\sim v$ is the set of neighbor nodes of $v$. Since the molecular datasets in this paper are all undirected and unweighted graphs after converting into the graph structure, it's proper to conduct the simplification. 

\paragraph{Negative Curvature Transformation.} Due to the inevitable presence of negative curvature values in the computation, the excessive incorporation of negative curvature as an additional signal in the network can potentially impede the performance of \emph{Curvature Encoding} and feature embedding. To address this issue, we devise a negative curvature transformation function that maps the curvature values into a non-negative range,\cref{Eq:14}.
Given the various methods available for negative curvature transformation, we have conducted an ablation experiment, in \cref{tab:02}, to showcase the results. 

In the following chapters, we will reveal the significant potential of the Curvature-based Transformer in predicting molecular properties. 

\section{Experiments}

Our model framework mainly relies on Graphormer, as such, while exploring the performance of Curvature GT in the field of molecular properties prediction, we also tested its performance on large datasets. 
The codes are available in Supplementary Material. 

\subsection{Ablation study}
To explore the effect of degree information and curvature information as Positional Encoding, we performed ablation experiments on the  Centrality Encoding of Graphormer to study the learning ability of the model under different input situations. The data set is Freesolv, and experiments were performed with a 12-layer graphormer encoder with 300 epochs per experiment.

The results of ablation studies are summarized in \cref{tab:01}. Specifically, the inclusion of Centrality Encoding in the Graphormer model demonstrates a significant enhancement in capturing structural information from graph data, leading to improved prediction accuracy. Furthermore, by incorporating curvature information into the Centrality Encoding of Graphormer, the competitiveness of the model is further enhanced.

Let $\textit{Cur}(\cdot)$ be curvature obtained above. To explore how the curvature maps best to an integer domain, we tried three mapping methods: Max-Min Mapping \cref{Eq:12}, Sigmoid Mapping \cref{Eq:13}, and Linear Mapping \cref{Eq:14}. The specific formula is as follows: 
\begin{align}
    \mathcal{F}(v_i) =& \frac{\textit{Cur}(v_i) - \min (\textit{Cur}(V))}{\max (\textit{Cur}(V))-\min (\textit{Cur}(V))},
    \label{Eq:12} \\
    \mathcal{F}(v_i) =& \frac{1}{1+\exp{(-\textit{Cur}(v_i))}},
    \label{Eq:13} \\
    \mathcal{F}(v_i) =& \textit{Cur}(v_i) - \min (\textit{Cur}(V)),
    \label{Eq:14}
\end{align}

where $\textit{Cur}(\cdot)$ denotes the curvature of the specific atom, and $V = \{v_1, v_2, v_3, \dots, v_n\}$ are the atoms of the molecular. The effects of the different mapping methods are shown in \cref{tab:02}. We speculate that the curvature of the is prone to lose topological information during the nonlinear transformation, making the model unable to accurately capture positional information. The linear transformation can ensure that the geometric measure on the graph does not deform.


\begin{table}[htbp]
  \begin{minipage}{0.5\textwidth}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{ll}
      \toprule
      Method & FreeSolve($\downarrow$)  \\
      \midrule
     node feature\cite{2021Do}        & 1.460   \\
     + degree embedding\cite{2021Do}  & 1.318  \\
     + degree \& curvature embedding   & 1.229   \\
     + curvature embedding            & \textbf{1.227}   \\
      \bottomrule
    \end{tabular}
    }
    \caption{Ablation studies on Centrality Encoding}
    \label{tab:01}
  \end{minipage}%
  \hfill
  \begin{minipage}{0.5\textwidth}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{ll}
      \toprule
      Method  & FreeSolve($\downarrow$) \\
      \midrule
      Max-Min Mapping    & 1.231 \\
      Sigmoid Mapping    & 1.429                 \\
      Linear Mapping     & \textbf{1.227}     \\
      \bottomrule
    \end{tabular}
    }
    \caption{Ablation studies on Curvature Mapping}
    \label{tab:02}
  \end{minipage}
\end{table}

\subsection{Molecular property prediction}

\begin{table}[ht]
    \caption{The performance comparison. 
    The optimal results are shown in \textbf{bold}, and the sub-optimal results are shown in \underline{\textit{underline}}.}
    \label{tab:03}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{lccccc}
      \toprule
      % \cmidrule(r){1-2}
      Method     &
    \begin{tabular}{c}BBBP     \\(AUC)$\uparrow$   \end{tabular} & 
    \begin{tabular}{c}BACE     \\(AUC)$\uparrow$   \end{tabular} & 
    \begin{tabular}{c}ClinTox  \\(AUC)$\uparrow$   \end{tabular} & 
    \begin{tabular}{c}ESOL     \\(RMSE)$\downarrow$\end{tabular} & 
    \begin{tabular}{c}FreeSolve\\(RMSE)$\downarrow$\end{tabular} \\
      \midrule
      Uni-Mol\cite{2022ChemRxiv}       
      &0.729 &0.857 &0.919   & 0.788 &1.620 \\
      ChemRL-GEM\cite{2021ChemRL}
      & 0.724 &0.856 &0.901 & 0.798 &1.877  \\
      ChemBERTa-2\cite{2022ChemBERTa}
      & 0.728 &0.799 &0.563 & 0.889 & - \\
      D-MPNN\cite{HAN2022100201}
      & 0.710 &0.809 &0.906 & 1.050  & 2.082 \\
      SPMM\cite{2022Bidirectional}
      & 0.733 &0.830 &0.910 & 0.810 &1.859 \\
      GROVER$_{\textit{base}}$\cite{2020GROVER}
      & 0.700 &0.826 &0.812 & 0.888 &2.176 \\
      GROVER$_{\textit{large}}$\cite{2020GROVER}
      & 0.695 &0.810 &0.762 & 0.831 &2.272 \\
      Graphormer\cite{2021Do}    
      & 0.837 &0.823 &0.926 & \underline{0.502} &1.318 \\
      \textbf{OURS}$_{\textit{Forman}}$          
      & \textbf{0.874} & \underline{0.864} & \textbf{0.941}  & \textbf{0.493} & \underline{1.214} \\
      \textbf{OURS}$_{\textit{Coarse\_Ollivier}}$ 
      & \underline{0.853} & \textbf{0.889} & \underline{0.937}  & 0.519  & \textbf{1.144} \\
      \bottomrule
    \end{tabular}}
\end{table}
\paragraph{Datasets.} 
In this paper, we performed the experiments on the MoleculeNet~\cite{2018MoleculeNet} (a benchmark for machine learning methods specifically designed to test molecular properties). 
The dataset is randomly split during the preprocessing stage, and the same split subset is guaranteed in the experiments with different network models. 
One-tenth of the data is used as the test set, and the rest is used for training and validation. 
The RMSE evaluation indicators were used in the molecular regression task and the ROC-AUC in the molecular prediction task. 
Test performance is based on the model that gives the best results in the validation setting. 

The tasks and experimental datasets include ESOL, FreeSolv.~\cite{2018MoleculeNet}, Blood-brain barrier permeability (BBBP)~\cite{2012JOURNAL}, BACE~\cite{Denny2016Computational}, ClinTox~\cite{Kaitlyn2016A}, PCQM4M-LSC~\cite{2021OGB}. Please refer to the Supplementary Material for a detailed introduction of our datasets.

\paragraph{Baselines.} 
We benchmarked the proposed Curvature GT against Graphormer and some popular baselines from MoleculeNet \cite{2018MoleculeNet}. Among them, Uni-Mol\cite{2022ChemRxiv}, GROVER\cite{2020GROVER}, ChemRL-GEM\cite{2021ChemRL},ChemBERTa-2\cite{2022ChemBERTa} are pretraining methods. D-MPNN \cite{2019Analyzing} and SPMM\cite{2022Bidirectional} are supervised GNNs methods. In particular,  ChemRL-GEM also considers the geometric information of the data in the network.

\paragraph{Results.} 
The experimental results of Curvature GT and competitive baselines are presented in \cref{tab:03}. 
Most results of baseline are from Uni-Mo paper\cite{2022ChemRxiv}, except for the recent works ChemBERTa-2, ChemRL-GEM, and Graphormer. The results of ChemBERTa-2 and ChemRL-GEM were obtained from their papers. 
To explore the effect of the Graphormer, we ran it using the same data split setting as other baselines. 
The results of the experiments showed that our model outperforms other algorithms on synthetic and real graphs, especially on dense graphs. 
This is largely due to our Positional Embedding considering the local structural correlation between nodes, reducing the embedding distortion. 
Experiments show that our curvature model consistently and significantly outperforms state-of-the-art methods on multiple tasks and shows superior robustness and generalization ability. 

\subsection{OGB Large-Scale Challenge}

\paragraph{Baselines.} On the PCQM4M-LSC dataset, we compare Froman-Curvature GT with Graphormer. Our experiment adopted the same parameter setting as in the Graphormer paper\cite{2021Do}. 
Specifically, the number of encoders is \texttt{L=12}, the number of self-attention heads is \texttt{h=32}, and the number of self-attention dimensions is \texttt{d=768}. The same parameter settings were also used for Small-scale models (\texttt{L=6, d=512}). 
The optimizer is Adam, with learning rate \texttt{lr=2e-4} (\texttt{lr=3e-4} for Small-scale models). 

\paragraph{Results.}
\cref{tab:04} displays the performance of Forman-Curvature GT under different specifications and Graphormer under the same test set and the parameter size. It can be seen that introducing curvature information requires fewer learning parameters. Moreover, our method converges nearly a third of epochs faster than Graphormer. 

\subsection{Analysis}
To explore the optimization of curvature information (Forman Curvature) in molecular tasks of different scales. We split the BACE and BBBP datasets using the average number of atoms of the dataset as the dividing line. We divided each dataset into two subsets with similar numbers and re-ran Graphormer with Curvature GT on each subset. The experimental results are shown in \cref{tab:05}. By analyzing the results, we found that both graphormer and our model showed a decreased competitiveness with the increasing graph size. Reassuringly, similar phenomena do not appear for the gain ratio of curvature to the model. This suggests that curvature can still provide more information in the face of larger graph data. 

We also extracted a batch of data from the BBBP test set to analyze the prediction results of Graphormer and Curvature GT (Forman Curvature). We observed that the prediction value of graphormer on some data was larger, such as c1c (c (c (c 1 Cl) Cl) Cl) c2cc (c (c (c 2 Cl) Cl) Cl) Cl (absolute error: 1.482), while our model performed better (absolute error: 0.165). Therefore, we conducted a structural analysis of the molecule, see \cref{fig:01}. 
We observed that degree information alone is insufficient to differentiate between carbon atoms in the structure. By incorporating curvature information enabled us to identify key sites more effectively. Additionally, as shown in \cref{fig:03}, the inclusion of curvature enhanced both the stability and speed of model convergence.

\begin{table}[htbp]
  \begin{minipage}{0.4\textwidth}
  % \raggedleft
    \caption{Results on PCQM4M-LSC.}
  \label{tab:04}
  \centering
  \scalebox{0.7}{
  \begin{tabular}{lll}
    \toprule
    % \cmidrule(r){1-3}
    Method     & param.     & MAE($\downarrow$)  \\
    \midrule
Graphormer$_{\textit{Small}}$\cite{2021Do} & 13M  & 0.1264  \\
Graphormer\cite{2021Do}  & 48.3M  & 0.1201  \\
Curvature GT$_{\textit{Forman-Small}}$ (\textbf{Ours}) & 12.8M  & 0.1238  \\
Curvature GT$_{\textit{Forman}}$ (\textbf{Ours}) & 47.8M  & \textbf{0.1184}  \\
    \bottomrule
  \end{tabular}}
  \end{minipage}%
  \hfill
  \begin{minipage}{0.6\textwidth}
  % \raggedright
    \centering
\caption{Comparison between Graphormer and Curvature-GT (Forman) on BACE and BBBP}
\scalebox{0.8}{
\begin{tabular}{@{}lllc@{}}
\toprule
Dataset                                    & Molecule Size                                                & Method                                                                                 & AUC ($\uparrow$)                \\ \midrule
\multicolumn{1}{l|}{\multirow{4}{*}{BACE}} & \multicolumn{1}{l|}{\multirow{2}{*}{atom\_num\textless{}34}} & \multicolumn{1}{l|}{Graphormer\cite{2021Do}}                          & 0.861                           \\
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{}                                        & \multicolumn{1}{l|}{Curvature GT$_{\textit{Forman}}$ (\textbf{Ours})} & \textbf{0.926} \\ \cmidrule(l){2-4} 
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{\multirow{2}{*}{atom\_num$\geq$34}}      & \multicolumn{1}{l|}{Graphormer\cite{2021Do}}                          & 0.771                           \\
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{}                                        & \multicolumn{1}{l|}{Curvature GT$_{\textit{Forman}}$ (\textbf{Ours})} & 0.837                           \\ \midrule
\multicolumn{1}{l|}{\multirow{4}{*}{BBBP}} & \multicolumn{1}{l|}{\multirow{2}{*}{atom\_num\textless{}23}} & \multicolumn{1}{l|}{Graphormer\cite{2021Do}}                          & 0.754                           \\
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{}                                        & \multicolumn{1}{l|}{Curvature GT$_{\textit{Forman}}$ (\textbf{Ours})} & \textbf{0.801} \\ \cmidrule(l){2-4} 
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{\multirow{2}{*}{atom\_num$\geq$23}}      & \multicolumn{1}{l|}{Graphormer\cite{2021Do}}                          & 0.677                           \\
\multicolumn{1}{l|}{}                      & \multicolumn{1}{l|}{}                                        & \multicolumn{1}{l|}{Curvature GT$_{\textit{Forman}}$ (\textbf{Ours})} & 0.721                           \\ \bottomrule
\end{tabular}}
\label{tab:05}
  \end{minipage}
\end{table}



% Figure environment removed

\section{Discussion}
Although the inclusion of \textit{Curvature Encoding} has yielded satisfactory results, the graph structure of molecular transformations lacks the ability to distinguish specific atoms from chemical bonds. As a result, the curvature information can only partially differentiate atoms connecting different functional groups, which remains insufficient. 
In the future, a potential approach could involve treating molecules as directed and weighted graphs, where different initial weights are assigned to atoms and chemical bonds during the curvature calculation process. This would align the curvature calculation more closely with chemical principles, allowing the node curvature information to encompass a greater range of molecular structural features.

\section{Conclusion}
In this paper, we introduce the \textit{Curvature-based Graph Transformer}, a network that incorporates curvature information from a discrete graph. 
This approach captures edge curvature by examining the structural correlation between nodes and their neighbors, followed by averaging the curvatures of connected edges to obtain node curvature. 
By encoding this curvature information as positional encoding in the Graph Transformer, the model gains the ability to capture additional structural details and enhance its generalization capabilities without increasing the model's parameters. 
Experimental results demonstrate the effectiveness of our approach compared to previous methods. Furthermore, the improvement is particularly pronounced in biochemical molecular datasets with smaller data volume, as the node curvature encapsulates abstract local structural information. 


% \medskip
% Uncomment the 2 lines to display bibtex citations
% \bibliographystyle{plain}
% \bibliography{refs}

\bibliographystyle{plain}
\bibliography{main}
% \begin{thebibliography}{10}

% \bibitem{2022ChemBERTa}
% W.~Ahmad, E.~Simon, S.~Chithrananda, G.~Grand, and B.~Ramsundar.
% \newblock Chemberta-2: Towards chemical foundation models.
% \newblock 2022.

% \bibitem{2013Stochastic}
% S.~Bonnabel.
% \newblock Stochastic gradient descent on riemannian manifolds.
% \newblock {\em IEEE Transactions on Automatic Control}, 58(9):2217--2229, 2013.

% \bibitem{2022Unifying}
% M.~Cha, E.S.T. Emre, X.~Xiao, J.Y. Kim, P.~Bogdan, J.S. VanEpps, A.~Violi, and
%   N.A. Kotov.
% \newblock Unifying structural descriptors for biological and bioinspired
%   nanoscale complexes.
% \newblock 2022.

% \bibitem{2022Bidirectional}
% J.~Chang and Ye~J. C.
% \newblock Bidirectional generation of structure and properties through a single
%   molecular foundation model.
% \newblock 2022.

% \bibitem{2020CL}
% C.~Deng and W.~Lam.
% \newblock Graph transformer for graph-to-sequence learning.
% \newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
%   34(5):7464--7471, 2020.

% \bibitem{Denny2016Computational}
% Denny, Rajiah, Aldrin, Pande, Vijay, Subramanian, Govindan, Ramsundar, and
%   Bharath.
% \newblock Computational modeling of beta-secretase 1 (bace-1) inhibitors using
%   ligand based approaches.
% \newblock {\em Journal of chemical information and modeling},
%   56(10):1936--1949, 2016.

% \bibitem{2020A}
% V.~P. Dwivedi and X.~Bresson.
% \newblock A generalization of transformer networks to graphs.
% \newblock 2020.

% \bibitem{dwivedi2022benchmarking}
% Vijay~Prakash Dwivedi, Chaitanya~K. Joshi, Anh~Tuan Luu, Thomas Laurent, Yoshua
%   Bengio, and Xavier Bresson.
% \newblock Benchmarking graph neural networks, 2022.

% \bibitem{2021ChemRL}
% X.~Fang, L.~Liu, J.~Lei, D.~He, S.~Zhang, J.~Zhou, F.~Wang, H.~Wu, and H.~Wang.
% \newblock Chemrl-gem: Geometry enhanced molecular representation learning for
%   property prediction.
% \newblock 2021.

% \bibitem{Forman2003}
% Robin Forman.
% \newblock {Bochner’s method for cell complexes and combinatorial Ricci
%   curvature}.
% \newblock {\em Discrete and Computational Geometry}, 29(3):323–374, 2003.

% \bibitem{2017Neural}
% J.~Gilmer, Samuel~S Schoenholz, Patrick~F Riley, O.~Vinyals, and George~E Dahl.
% \newblock Neural message passing for quantum chemistry.
% \newblock 2017.

% \bibitem{2018Learning}
% A.~Gu, F.~Sala, B.~Gunel, and C~Ré.
% \newblock Learning mixed-curvature representations in product spaces.
% \newblock In {\em International Conference on Learning Representations}, 2018.

% \bibitem{HAN2022100201}
% Xu~Han, Ming Jia, Yachao Chang, Yaopeng Li, and Shaohua Wu.
% \newblock Directed message passing neural network (d-mpnn) with graph edge
%   attention (gea) for property prediction of biofuel-relevant species.
% \newblock {\em Energy and AI}, 10:100201, 2022.

% \bibitem{2021OGB}
% W.~Hu, M.~Fey, H.~Ren, M.~Nakata, Y.~Dong, and J.~Leskovec.
% \newblock Ogb-lsc: A large-scale challenge for machine learning on graphs.
% \newblock 2021.

% \bibitem{2021Edge}
% M.~S. Hussain, M.~J. Zaki, and D.~Subramanian.
% \newblock Edge-augmented graph transformers: Global self-attention is enough
%   for graphs.
% \newblock 2021.

% \bibitem{Kearnes0Molecular}
% Kearnes, Steven, McCloskey, Kevin, Berndl, Marc, Pande, Vijay, Riley, and
%   Patrick.
% \newblock Molecular graph convolutions: moving beyond fingerprints.

% \bibitem{2020Interpretable}
% Lms Khoo, H.~L. Chieu, Z.~Qian, and J.~Jiang.
% \newblock Interpretable rumor detection in microblogs by attending to user
%   interactions.
% \newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
%   2020.

% \bibitem{kreuzer2021rethinking}
% Devin Kreuzer, Dominique Beaini, William~L. Hamilton, Vincent Létourneau, and
%   Prudencio Tossou.
% \newblock Rethinking graph transformers with spectral attention, 2021.

% \bibitem{2020Machine}
% H.~Li, K.~H. Sze, G.~Lu, and P.~J. Ballester.
% \newblock Machine-learning scoring functions for structure-based drug lead
%   optimization.
% \newblock {\em Wiley interdisciplinary reviews: Computational Molecular
%   Science}, 10(10):e1465, 2020.

% \bibitem{li2022mat}
% Wenbo Li, Zhe Lin, Kun Zhou, Lu~Qi, Yi~Wang, and Jiaya Jia.
% \newblock Mat: Mask-aware transformer for large hole image inpainting, 2022.

% \bibitem{liao2023equiformer}
% Yi-Lun Liao and Tess Smidt.
% \newblock Equiformer: Equivariant graph attention transformer for 3d atomistic
%   graphs, 2023.

% \bibitem{lin2021mesh}
% Kevin Lin, Lijuan Wang, and Zicheng Liu.
% \newblock Mesh graphormer, 2021.

% \bibitem{lin2023multi}
% Xi~Victoria Lin, Caiming Xiong, and Richard Socher.
% \newblock Multi-hop knowledge graph reasoning with reward shaping, April~18
%   2023.
% \newblock US Patent 11,631,009.

% \bibitem{Lin2010Ricci}
% Yong Lin, Linyuan Lu, and Shing-Tung Yau.
% \newblock {Ricci curvature of graphs}.
% \newblock {\em Tohoku Mathematical Journal}, 63(4):605 -- 627, 2011.

% \bibitem{2019N}
% S.~Liu, Mehmet~Furkan Demirel, and Y.~Liang.
% \newblock N-gram graph: Simple unsupervised representation for graphs, with
%   applications to molecules.
% \newblock In {\em Advances in Neural Information Processing Systems}, page
%   8464–8476. 2019.

% \bibitem{Kaitlyn2016A}
% Kaitlyn M., Gayvert, Neel S., Madhukar, Olivier, and Elemento.
% \newblock A data-driven approach to predicting successes and failures of
%   clinical trials.
% \newblock {\em Cell Chemical Biology}, 2016.

% \bibitem{2012JOURNAL}
% A.~L. Martins, I. F.and Ana~Teixeira, L.~Pinheiro, and Falcao~A. O.
% \newblock A bayesian approach to in silico blood-brain barrier penetration
%   modeling.
% \newblock {\em Journal of chemical information and modeling}, 2012.

% \bibitem{mialon2021graphit}
% Grégoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal.
% \newblock Graphit: Encoding graph structure in transformers, 2021.

% \bibitem{2018Network}
% C.~C. Ni, Y.~Y. Lin, J.~Gao, and X.~D. Gu.
% \newblock Network alignment by discrete ollivier-ricci flow.
% \newblock 2018.

% \bibitem{2015Ricci}
% C.~C. Ni, Y.~Y. Lin, G.~Jie, X.~D. Gu, and E.~Saucan.
% \newblock Ricci curvature of the internet topology.
% \newblock {\em IEEE}, 2015.

% \bibitem{OLLIVIER2009810}
% Yann Ollivier.
% \newblock Ricci curvature of markov chains on metric spaces.
% \newblock {\em Journal of Functional Analysis}, 256(3):810--864, 2009.

% \bibitem{2020Ricci}
% Ginte Petrulionyte.
% \newblock Ricci curvature in network embedding and clustering, 2020.

% \bibitem{2020GROVER}
% Y.~Rong, Y.~Bian, T.~Xu, W.~Xie, and J.~Huang.
% \newblock Grover: Self-supervised message passing transformer on large-scale
%   molecular data.
% \newblock 2020.

% \bibitem{2019Ollivier}
% J.~Sia, E.~Jonckheere, and P.~Bogdan.
% \newblock Ollivier-ricci curvature-based method to community detection in
%   complex networks.
% \newblock {\em Scientific Reports}, 9(1), 2019.

% \bibitem{2016Forman}
% R.~P. Sreejith, K.~Mohanraj, J.~Jost, E.~Saucan, and A.~Samal.
% \newblock Forman curvature for complex networks.
% \newblock {\em Journal of Statistical Mechanics Theory \& Experiment},
%   2016(6):063206, 2016.

% \bibitem{2021Understanding}
% J.~Topping, F.~D. Giovanni, B.~P. Chamberlain, X.~Dong, and M.~M. Bronstein.
% \newblock Understanding over-squashing and bottlenecks on graphs via curvature.
% \newblock {\em arXiv e-prints}, 2021.

% \bibitem{2017Attention}
% A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
%   L.~Kaiser, and I.~Polosukhin.
% \newblock Attention is all you need.
% \newblock {\em arXiv}, 2017.

% \bibitem{Villani2014Optimal}
% Villani and Cédric.
% \newblock {\em Optimal transport : old and new}.
% \newblock Optimal transport : old and new, 2014.

% \bibitem{wang2020direct}
% Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec.
% \newblock Direct multi-hop attention based graph neural network.
% \newblock {\em arXiv preprint arXiv:2009.14332}, 2020.

% \bibitem{OllivierPersistentRicci}
% JunJie Wee and Kelin Xia.
% \newblock Ollivier persistent ricci curvature-based machine learning for the
%   protein–ligand binding affinity prediction.
% \newblock {\em Journal of Chemical Information and Modeling}, 61(4):1617--1626,
%   2021.
% \newblock PMID: 33724038.

% \bibitem{1984JOURNAL}
% D.~Weininger, A.~Weininger, and J.~L. Weininger.
% \newblock Smiles.2.algorithm for generation of unique smiles notation.
% \newblock {\em JOURNAL OF CHEMICAL INFORMATION AND COMPUTER SCIENCES}, 1989.

% \bibitem{2021Molformer}
% F.~Wu, D.~Radev, and S.~Z. Li.
% \newblock Molformer: Motif-based transformer on 3d heterogeneous molecular
%   graphs.
% \newblock 2021.

% \bibitem{Quantitative}
% Kedi Wu and Guo-Wei Wei.
% \newblock Quantitative toxicity prediction using topology based multitask deep
%   neural networks.
% \newblock {\em Journal of Chemical Information and Modeling}, 58(2):520--531,
%   2018.
% \newblock PMID: 29314829.

% \bibitem{2018MoleculeNet}
% Z.~Wu, B.~Ramsundar, E.~N. Feinberg, J.~Gomes, C.~Geniesse, A.~S. Pappu,
%   K.~Leswing, and V.~S. Pande.
% \newblock Moleculenet: a benchmark for molecular machine learning.
% \newblock {\em The Royal Society of Chemistry}, 2018.

% \bibitem{Wu2021GraphTrans}
% Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph~E Gonzalez,
%   and Ion Stoica.
% \newblock Representing long-range context for graph neural networks with global
%   attention.
% \newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
%   2021.

% \bibitem{2020On}
% R.~Xiong, Y.~Yang, D.~He, K.~Zheng, and T.~Y. Liu.
% \newblock On layer normalization in the transformer architecture.
% \newblock 2020.

% \bibitem{2020ICLR}
% Ze~Y., S.~L. Kin, M.~Tengfei, G.~Jie, and C.~Chao.
% \newblock Curvature graph network.
% \newblock 2020.

% \bibitem{2019Analyzing}
% K.~Yang, K.~Swanson, W.~Jin, C.~W. Coley, and R.~Barzilay.
% \newblock Analyzing learned molecular representations for property prediction.
% \newblock {\em Journal of Chemical Information and Modeling}, 59(8), 2019.

% \bibitem{2021Do}
% C.~Ying, T.~Cai, S.~Luo, S.~Zheng, and T.~Y. Liu.
% \newblock Do transformers really perform bad for graph representation?
% \newblock 2021.

% \bibitem{2020Graph}
% J.~Zhang, H.~Zhang, C.~Xia, and L.~Sun.
% \newblock Graph-bert: Only attention is needed for learning graph
%   representations.
% \newblock 2020.

% \bibitem{ijms232113568}
% Yue Zhang, Mengqi Luo, Peng Wu, Song Wu, Tzong-Yi Lee, and Chen Bai.
% \newblock Application of computational biology and artificial intelligence in
%   drug design.
% \newblock {\em International Journal of Molecular Sciences}, 23(21), 2022.

% \bibitem{2022ChemRxiv}
% Z.~Zhou, G.and~Gao, Q.~Ding, H.~Zheng, H.~Xu, Z.~Wei, L.~Zhang, and G.~Ke.
% \newblock Uni-mol: A universal 3d molecular representation learning framework.
% \newblock 2022.

% \bibitem{2018Representability}
% C.~Zixuan, M.~Lin, G.~W. Wei, and P.~Jian.
% \newblock Representability of algebraic topology for biomolecules in machine
%   learning based scoring and virtual screening.
% \newblock {\em Plos Computational Biology}, 14(1):e1005929, 2018.

% \end{thebibliography}

\clearpage
\appendix
\section{ Dataset details.}\label{Appendix:A}
We hereby present a more detailed description of the datasets used in this work in \cref{tab:06}, including their size and task.

\begin{table}[ht]
    \caption{Statistics of the datasets.}
    \label{tab:06}
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
     Dataset  &
     Scale     & 
     \# Graphs    & 
     \# Nodes      &  
     \# Edges      &
     Task Type     \\
      \midrule
       ESOL & Small & 1128 & 15,002 & 30,907 & Regression \\
       FreeSolv & Small & 642 & 5,585 & 10,785 & Regression \\
       BBBP & Small & 2050 & 48,995 & 105,780 & Binary classification \\
       BACE & Small & 1513 & 51,593 & 111,508 & Binary classification \\
       ClinTox & Small & 1,484 & 38,732 & 82362 & Classification \\
       PCQM4M-LSC & Large & 3,803,453 &  53,814,542 &  55,399,880 & Regression \\
       OGBG-MolHIV & Small & 41,127 &   1,048,738 &  1,130,993 & Binary classification \\
       ZINC (sub-set) & Small & 12,000 &    277,920 &  597,960 &  Regression \\
     \bottomrule
     \end{tabular}}
\end{table}

\begin{itemize}
  \item \textbf{ESOL, FreeSolv.} It is dataset for regression task. ESOL contains the log solubility in mols per litre  of 1,128 molecules. FreeSolv is used to predict the  water solubility in terms of the hydration free energy of molecules and contains 642 molecules.
  \item \textbf{Blood-brain barrier permeability (BBBP)}. A Binary classification task to predict whether a molecule has the ability to penetrate the blood-brain barrier. In this way, scientists can determine whether drugs can affect the human central nervous system. This dataset contains 2,050 molecules. 
  \item \textbf{BACE}. A classification task, predicting BACE-1 inhibitors provides quantitative IC50 and qualitative (binary) combination results.
  \item \textbf{ClinTox.} Including two classification tasks for 1,484 pharmaceutical compounds with known chemical structures. Labels are clinical trial FDA approval status and toxicity status.
  \item \textbf{PCQM4M-LSC}. A Large-Scale regression task, which contains more than 3.8M graphs.PCQM4M-LSC is a regression data set of 2D molecular graphs to predict DFT (density functional theory) -calculated HOMO-LUMO energy gap, which is one of the most practically-relevant quantum chemical properties of molecule science.
  \item \textbf{OGBG-MolHIV}. A Binary classification task.The task is to predict as accurately as possible whether the target molecule is able to inhibit HIV replication.
  \item \textbf{ZINC}. One of the most popular real-world molecular dataset to predict graph property regression for contrained solubility. Different from the scaffold spliting in other datasets, uniform sampling is adopted in ZINC for data splitting.

\end{itemize}

\section{Experiment Details.}\label{Appendix:B}
\subsection{Details of Training Strategies.}\label{Appendix:B.1}
In this section we include details for hyperparameters and
training settings used in Section 4.2.
\begin{table}[ht]
    \caption{Model Configurations and Hyper-parameters of Curvature GT on Benchmark }
    \label{tab:07}
    \centering
    \resizebox{1\textwidth}{!}{
    \begin{tabular}{lccc}
      \toprule
      % \cmidrule(r){1-2}
           &
    \begin{tabular}{c}Graphormer     \end{tabular} &   
    \begin{tabular}{c}Curvature GT$_{Forman}$     \end{tabular} & 
    \begin{tabular}{c}Curvature GT$_{Coarse\_Ollivier}$     \end{tabular} \\
      \midrule
     \#Layers &  12 & 12 & 12     \\
     Hidden Dimension $\emph{d}$  & 768   & 768 & 768 \\
     FFN Inner-layer Dimension  & 768   & 768 & 768\\
     \#Attention Heads          & 32    & 32 & 32  \\
     Attention Dropout          & 0.1   & 0.1 & 0.1 \\
     FFN Dropout                & 0.1   & 0.1 & 0.1 \\
     Embedding Dropout          & 0.0   & 0.0 & 0.0 \\
     Batch Size                 & 32    & 32 & 32   \\
     Warm-up Steps              & 60K   & 60K & 60K \\
     Learning Rate Decay        &Linear &Linear &Linear\\
     Adam $\epsilon$              &1e-8    &1e-8  &1e-8\\
     Adam ($\beta_1$,$\beta_2$) &(0.9,0.999)     &(0.9,0.999)  &(0.9,0.999)\\
     \bottomrule
    \end{tabular}}
\end{table}
We report the detailed hyper-parameter settings used for training Graphormer in \cref{tab:07}.  The embedding dropout ratio is set to 0.1 by default in many previous Transformer works.And due to the molecular graph is relative small, we set embedding dropout ratio to 0.0\cite{2021Do}. The batch size is set to 32. We trained with 8 NVIDIA RTX3090 GPUS for about 2 days on the PCQM4M-LSC dataset. The other datasets were trained on a RTX2070, and the training end condition was: the optimal loss no longer drops in more than 50 epoch.


\subsection{Forman-Ricci Curvature vs Coarse Ollivier-Ricci Curvature.}\label{Appendix:B.2}
We performed statistics on the curvature information of the nodes in the BBBP dataset to observe the distribution of Forman-Ricci Curvature and Coarse Ollivier-Ricci Curvature\cref{fig:04}.Because the calculation of Coarse Ollivier-Ricci Curvature involves sub-optimal transportation plan, the calculation process is more complicated, so the accuracy is higher than Forman-Ricci Curvature, and the range of mapping is wider.And the two curvatures are similar in the overall statistical distribution. Overall we believe that the utility of Forman-Ricci Curvature would be higher.

% Figure environment removed


\section{Additional Experimental Results.}\label{Appendix:C}
\subsection{OGBG-MolHIV.}\label{Appendix:C.1}
The purpose of this complementary experiment was to probe the performance of Curvature GT on the Graphormer pre-trained model.

\begin{table}[ht]
    \caption{Model Configurations and Hyper-parameters on OGBG-MolHIV}
    \label{tab:08}
    \centering
    \resizebox{0.5\textwidth}{!}{
    \begin{tabular}{lc}
      \toprule
           &
    \begin{tabular}{c}Curvature GT$_{Forman}$     \end{tabular} \\
      \midrule
      Max Epochs & 4          \\
      Peak Learning Rate & 2e-4 \\
      Batch Size  & 64       \\
      Warm-up Ratio & 0.06     \\
      Dropout     &   0.1    \\
      Attention Dropout        & 0.1   \\
      $\mathit{m}$ & 3 \\
      $\alpha$  & 0.01 \\
      $\epsilon$ & 0    \\
     \bottomrule
    \end{tabular}}
\end{table}

\paragraph{ Pre-training.}As with Graphormer, we tested the effect of the pretrained model on the OGBG-MolHIV dataset. We use the Graphormer reported in \cref{tab:04} as the pre-trained model for OGBG-MolHIV, where the pre-training hyper-parameters are summarized in \cref{tab:07}.

\paragraph{Fine-tuning.}The hyper-parameters for fine-tuning Graphormer on OGBG-MolHIV are presented in \cref{tab:08}We use FLAG with minor modifications for graph data augmentation. And the hyper-parameters of FLAG are as follows: the step size $\alpha=0.01$, the number of steps $\mathit{m} =3$ and the maximum perturbation $\epsilon = 0$.


\begin{table}[ht]
    \caption{Results on MolHIV. $*$ indicates that additional
features for molecule are used.}
    \label{tab:09}
    \centering
  \begin{tabular}{lll}
    \toprule
    % \cmidrule(r){1-3}
    Method     & param.     & AUC($\uparrow$)  \\
    \midrule
    GROVER$*$\cite{2020GROVER} &48.8M & 79.33 \\
    GROVER$_{\textit{large}}*$\cite{2020GROVER} &107.7M & \textbf{80.32} \\ \hline
    Graphormer-FLAG\cite{2021Do} & 48.3M  &  79.71 \\
    Curvature GT-FLAG$_{\textit{Forman}}$  & 47.8M  & 79.84 \\
    \bottomrule
  \end{tabular}
\end{table}
\paragraph{Results.} As with Graphormer, we observed from the table that the performance of Curvature GT also could close to GROVER even without any additional molecular features. Please remember, from the leaderboard\cite{2021OGB}, such additional molecular features are very effective on MolHIV dataset. According to \cite{2021Do}, we know that different hyper-parameters of FLAG choices can greatly affect the outcome of Molhiv. However, the purpose of our experiment was to explore the performance of Curvature GT on the pre-trained model, so we did not fully explore the optimal hyper-parameters choice.

\subsection{ZINC.}\label{Appendix:C.2}
In this section, we tested the performance of the Curvature GT for Graphormer$_{SLIM}$ size on the ZINC dataset.The detailed hyper-parameters in \cref{tab:10}
\begin{table}[ht]
    \caption{Model Configurations and Hyper-parameters on ZINC(sub-set).}
    \label{tab:10}
    \centering
    \resizebox{0.8\textwidth}{!}{
    \begin{tabular}{lc}
      \toprule
      % \cmidrule(r){1-2}
           &
    \begin{tabular}{c}Curvature GT$_{Forman SLIM}$     \end{tabular} \\
      \midrule
      \#Layers &   12   \\
      Hidden Dimension & 80 \\
      FFN Inner-Layer Hidden Dimension & 80\\
      \#Attention Heads   & 8 \\
      Hidden Dimension of Each Head & 10 \\
      Max Epochs & 10K          \\
      Peak Learning Rate & 2e-4 \\
      Batch Size  & 64      \\
      Warm-up Steps & 40K     \\
      FFN Dropout  & 0.1  \\
      Attention Dropout        & 0.1   \\
      Embedding Dropout     &   0.0    \\
      Learning Rate Decay  & Linear   \\
      Adam $\epsilon$ & 1e-8 \\
      Adam $(\beta_1,\beta_2)$ & (0.9, 0.999) \\
      Gradient Clip Norm & 5.0  \\ 
      Weight Decay & 0.01     \\
     \bottomrule
    \end{tabular}}
\end{table}

\begin{table}[ht]
    \caption{Results on ZINC(sub-set).}
    
    \centering
  \begin{tabular}{lc}
    \toprule
    % \cmidrule(r){1-3}
    Method       & test MAE($\downarrow$)  \\
    \midrule
    Graphormer$_{\textit{SLIM}}$\cite{2021Do} & 0.122  \\
    Curvature GT$_{\textit{Forman SLIM}}$  & \textbf{0.120}  \\
    \bottomrule
  \end{tabular}
  \label{tab:11}
\end{table}
\paragraph
{Results.}\cref{tab:11} summarize performance of Graphormer$_{SLIM}$ and  Curvature GT$_{\textit{Forman SLIM}}$ on ZINC(sub-set) datasets. We can see that the Curvature GT$_{\textit{Forman SLIM}}$ still performs well with the small number of parameters.

% \bibliographystyle{plain}
% \bibliography{refs}

% [\textit{Cur}(v_i) - \min (\textit{Cur}(V))]
% \div
% [(\max (\textit{Cur}(V)-\min (\textit{Cur}(V))]
% \cdot 511

% 要引用的文献

\end{document}
