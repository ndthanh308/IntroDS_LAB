
@misc{noauthor_6_nodate,
	title = {6. {Camera} {Hardware} — {Picamera} 1.13 {Documentation}},
	url = {https://picamera.readthedocs.io/en/latest/fov.html#sensor-modes},
	urldate = {2022-08-18},
}

@article{de_croon_flapping_2020,
	title = {Flapping wing drones show off their skills},
	volume = {5},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abd0233},
	doi = {10.1126/scirobotics.abd0233},
	number = {44},
	urldate = {2022-08-17},
	journal = {Science Robotics},
	author = {de Croon, Guido},
	month = jul,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabd0233},
}

@article{matus-vargas_ground_2021,
	title = {Ground effect on rotorcraft unmanned aerial vehicles: a review},
	volume = {14},
	issn = {1861-2784},
	shorttitle = {Ground effect on rotorcraft unmanned aerial vehicles},
	url = {https://doi.org/10.1007/s11370-020-00344-5},
	doi = {10.1007/s11370-020-00344-5},
	abstract = {This article aims at collecting and discussing the results reached by the research community regarding the study of the ground effect on small rotorcraft unmanned aerial vehicles, especially from the modeling and control point of view. Rotorcraft performance is affected by the presence of the ground or any other boundary that alters the flow into the rotors. Specifically, the ground effect can induce perturbations in the flight stability, when operating near the ground. For a rotorcraft, an accident is likely to happen when the vehicle leaves or enters the ground effect region, which may cause crashes and property damages. Today, the use of unmanned aerial vehicles has grown widespread, which raises safety concerns when they are flying at very low altitudes and near the ground. Consequently, studying the influence of the ground over rotorcrafts is of paramount importance for general safety. Also, these investigations can be used to design systems of guidance, navigation, and control. In this review, we break down the most relevant works to date. We discuss aspects related to modeling, control, and application of the ground effect for small-scale multirotors, as well as other aerodynamic proximity effects, such as the ceiling and wall effects. We conclude by mentioning potential avenues of research when studying the ground effect from the point of view of the robotics and artificial intelligence fields.},
	language = {en},
	number = {1},
	urldate = {2022-08-17},
	journal = {Intelligent Service Robotics},
	author = {Matus-Vargas, Antonio and Rodriguez-Gomez, Gustavo and Martinez-Carranza, Jose},
	month = mar,
	year = {2021},
	keywords = {Ground effect, Micro-air vehicle, Rotary-wing aircraft, Rotorcraft, Unmanned aerial vehicle},
	pages = {99--118},
}

@misc{noauthor_ethz-aslkalibr_nodate,
	title = {ethz-asl/kalibr: {The} {Kalibr} visual-inertial calibration toolbox},
	url = {https://github.com/ethz-asl/kalibr},
	urldate = {2022-08-15},
}

@misc{noauthor_allan_2022,
	title = {Allan {Variance} {ROS}},
	copyright = {BSD-3-Clause},
	url = {https://github.com/ori-drs/allan_variance_ros},
	abstract = {ROS compatible tool to generate Allan Deviation plots},
	urldate = {2022-08-15},
	publisher = {Oxford Dynamic Robot Systems Group},
	month = aug,
	year = {2022},
	note = {original-date: 2021-11-12T15:11:18Z},
}

@article{sihite_computational_2020,
	title = {Computational {Structure} {Design} of a {Bio}-{Inspired} {Armwing} {Mechanism}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.3010217},
	abstract = {Bat membranous wings possess unique functions that make them a good example to take inspiration from and transform current aerial drones. In contrast with other flying vertebrates, bats have an extremely articulated musculoskeletal system which is key to their energetic efficiency with impressively adaptive and multimodal locomotion. Biomimicry of this flight apparatus is a significant engineering ordeal and we seek to achieve mechanical intelligence through sophisticated interactions of morphology. Such morphological computation or mechanical intelligence draws our attention to the obvious fact that there is a common interconnection between the boundaries of morphology and closed-loop feedback. In this work, we demonstrate that several biologically meaningful degrees of freedom can be interconnected to one another by mechanical intelligence and, as a result, the responsibility of feedback-driven components (e.g., actuated joints) is subsumed under computational morphology. The results reported in this work significantly contribute to the design of bio-inspired Micro Aerial Vehicles (MAVs) with articulated body and attributes such as efficiency, safety, and collision-tolerance.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Sihite, Eric and Kelly, Peter and Ramezani, Alireza},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Animals, Biomimetics, Fasteners, Morphology, Robots, Three-dimensional displays, Transforms, mechanism design, soft robot materials and design},
	pages = {5929--5936},
}

@article{boutet_unsteady_2018,
	title = {Unsteady {Lifting} {Line} {Theory} {Using} the {Wagner} {Function} for the {Aerodynamic} and {Aeroelastic} {Modeling} of {3D} {Wings}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2226-4310},
	url = {https://www.mdpi.com/2226-4310/5/3/92},
	doi = {10.3390/aerospace5030092},
	abstract = {A method is presented to model the incompressible, attached, unsteady lift and pitching moment acting on a thin three-dimensional wing in the time domain. The model is based on the combination of Wagner theory and lifting line theory through the unsteady Kutta–Joukowski theorem. The results are a set of closed-form linear ordinary differential equations that can be solved analytically or using a Runge–Kutta–Fehlberg algorithm. The method is validated against numerical predictions from an unsteady vortex lattice method for rectangular and tapered wings undergoing step or oscillatory changes in plunge or pitch. Further validation is demonstrated on an aeroelastic test case of a rigid rectangular finite wing with pitch and plunge degrees of freedom.},
	language = {en},
	number = {3},
	urldate = {2022-08-15},
	journal = {Aerospace},
	author = {Boutet, Johan and Dimitriadis, Grigorios},
	month = sep,
	year = {2018},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Wagner theory, aeroelasticity, finite wings, lifting line theory, unsteady aerodynamics},
	pages = {92},
}

@article{kaess_isam2_2012,
	title = {{iSAM2}: {Incremental} smoothing and mapping using the {Bayes} tree},
	volume = {31},
	issn = {0278-3649},
	shorttitle = {{iSAM2}},
	url = {https://doi.org/10.1177/0278364911430419},
	doi = {10.1177/0278364911430419},
	abstract = {We present a novel data structure, the Bayes tree, that provides an algorithmic foundation enabling a better understanding of existing graphical model inference algorithms and their connection to sparse matrix factorization methods. Similar to a clique tree, a Bayes tree encodes a factored probability density, but unlike the clique tree it is directed and maps more naturally to the square root information matrix of the simultaneous localization and mapping (SLAM) problem. In this paper, we highlight three insights provided by our new data structure. First, the Bayes tree provides a better understanding of the matrix factorization in terms of probability densities. Second, we show how the fairly abstract updates to a matrix factorization translate to a simple editing of the Bayes tree and its conditional densities. Third, we apply the Bayes tree to obtain a completely novel algorithm for sparse nonlinear incremental optimization, named iSAM2, which achieves improvements in efficiency through incremental variable re-ordering and fluid relinearization, eliminating the need for periodic batch steps. We analyze various properties of iSAM2 in detail, and show on a range of real and simulated datasets that our algorithm compares favorably with other recent mapping algorithms in both quality and efficiency.},
	language = {en},
	number = {2},
	urldate = {2022-08-15},
	journal = {The International Journal of Robotics Research},
	author = {Kaess, Michael and Johannsson, Hordur and Roberts, Richard and Ila, Viorela and Leonard, John J and Dellaert, Frank},
	month = feb,
	year = {2012},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {SLAM, clique tree, graphical models, junction tree, nonlinear optimization, probabilistic inference, smoothing and mapping, sparse linear algebra},
	pages = {216--235},
}

@article{bloesch_iterated_2017,
	title = {Iterated extended {Kalman} filter based visual-inertial odometry using direct photometric feedback},
	volume = {36},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364917728574},
	doi = {10.1177/0278364917728574},
	abstract = {This paper presents a visual-inertial odometry framework that tightly fuses inertial measurements with visual data from one or more cameras, by means of an iterated extended Kalman filter. By employing image patches as landmark descriptors, a photometric error is derived, which is directly integrated as an innovation term in the filter update step. Consequently, the data association is an inherent part of the estimation process and no additional feature extraction or matching processes are required. Furthermore, it enables the tracking of noncorner-shaped features, such as lines, and thereby increases the set of possible landmarks. The filter state is formulated in a fully robocentric fashion, which reduces errors related to nonlinearities. This also includes partitioning of a landmark’s location estimate into a bearing vector and distance and thereby allows an undelayed initialization of landmarks. Overall, this results in a compact approach, which exhibits a high level of robustness with respect to low scene texture and motion blur. Furthermore, there is no time-consuming initialization procedure and pose estimates are available starting at the second image frame. We test the filter on different real datasets and compare it with other state-of-the-art visual-inertial frameworks. Experimental results show that robust localization with high accuracy can be achieved with this filter-based framework.},
	language = {en},
	number = {10},
	urldate = {2022-08-15},
	journal = {The International Journal of Robotics Research},
	author = {Bloesch, Michael and Burri, Michael and Omari, Sammy and Hutter, Marco and Siegwart, Roland},
	month = sep,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Visual-inertial odometry, iterated extended Kalman filter, multiple cameras, photometric error, tight information fusion},
	pages = {1053--1072},
}

@article{weinstein_visual_2018,
	title = {Visual {Inertial} {Odometry} {Swarm}: {An} {Autonomous} {Swarm} of {Vision}-{Based} {Quadrotors}},
	volume = {3},
	issn = {2377-3766},
	shorttitle = {Visual {Inertial} {Odometry} {Swarm}},
	doi = {10.1109/LRA.2018.2800119},
	abstract = {In this letter, we present the system infrastructure for a swarm of quadrotors, which perform all estimation on board using monocular visual inertial odometry. This is a novel system since it does not require an external motion capture system or GPS and is able to execute formation tasks without inter-robot collisions. The swarm can be deployed in nearly any indoor or outdoor scenario and is scalable to higher numbers of robots. We discuss the system architecture, estimation, planning, and control for the multirobot system. The robustness and scalability of the approach is validated in both indoor and outdoor environments with up to 12 quadrotors.},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Weinstein, Aaron and Cho, Adam and Loianno, Giuseppe and Kumar, Vijay},
	month = jul,
	year = {2018},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Aerial systems, Cameras, Estimation, Global Positioning System, Robot kinematics, Robot vision systems, applications, swarms, visual-based navigation},
	pages = {1801--1807},
}

@article{rhodes_autonomous_2022,
	title = {Autonomous {Source} {Term} {Estimation} in {Unknown} {Environments}: {From} a {Dual} {Control} {Concept} to {UAV} {Deployment}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {Autonomous {Source} {Term} {Estimation} in {Unknown} {Environments}},
	doi = {10.1109/LRA.2022.3143890},
	abstract = {In the gas source search and localisation problem, the use of autonomous robots is of increasing interest due to their deployment speed and lack of human interaction with hazardous materials. This letter presents an aerial robotic platform for performing source term estimation of an unknown chemical release in a challenging a-priori unknown and GPS-denied environment. The proposed system forms the search strategy using the state-of-the-art control concept, dual control for exploitation and exploration, and realises such a function in the aforementioned challenging scenario using an RRT* based path planner. A novel downsampling process on the RRT* is also proposed that addresses the computational infeasibility of calculating the utility of a large number of sample states, whilst still maintaining sample state diversity. The proposed algorithm is tested in a high fidelity simulation environment under a number of configurations, and compared against competing algorithms. The system architecture is also brought forward into a bespoke UAV platform and experimentally tested in real-world conditions. The proposed system is shown to be capable of performing source term estimation robustly and efficiently, which provides a step forward in showing the real world application of previously academic functions.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Rhodes, Callum and Liu, Cunjia and Chen, Wen-Hua},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Chemicals, Drones, Estimation, Robotics in hazardous fields, Robots, Search problems, Sensors, Trajectory, planning under uncertainty, sensor-based control},
	pages = {2274--2281},
}

@article{di_luca_bioinspired_2017,
	title = {Bioinspired morphing wings for extended flight envelope and roll control of small drones},
	volume = {7},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsfs.2016.0092},
	doi = {10.1098/rsfs.2016.0092},
	abstract = {Small-winged drones can face highly varied aerodynamic requirements, such as high manoeuvrability for flight among obstacles and high wind resistance for constant ground speed against strong headwinds that cannot all be optimally addressed by a single aerodynamic profile. Several bird species solve this problem by changing the shape of their wings to adapt to the different aerodynamic requirements. Here, we describe a novel morphing wing design composed of artificial feathers that can rapidly modify its geometry to fulfil different aerodynamic requirements. We show that a fully deployed configuration enhances manoeuvrability while a folded configuration offers low drag at high speeds and is beneficial in strong headwinds. We also show that asymmetric folding of the wings can be used for roll control of the drone. The aerodynamic performance of the morphing wing is characterized in simulations, in wind tunnel measurements and validated in outdoor flights with a small drone.},
	number = {1},
	urldate = {2022-08-14},
	journal = {Interface Focus},
	author = {Di Luca, M. and Mintchev, S. and Heitz, G. and Noca, F. and Floreano, D.},
	month = feb,
	year = {2017},
	note = {Publisher: Royal Society},
	keywords = {bioinspired aerodynamics, bioinspired drone, feathered wing, micro air vehicles, morphing wing},
	pages = {20160092},
}

@article{tedrake_learning_nodate,
	title = {Learning to {Fly} like a {Bird}},
	abstract = {Birds routinely execute aerial maneuvers that are far beyond the capabilities of our best aircraft control systems. The complexity and variability of the aerodynamics during these maneuvers are formidable, with dominant ﬂow structures (e.g., vortices) that are diﬃcult to predict robustly from ﬁrst-principles (Navier-Stokes) models. Here we argue that machine learning will play an important role in the control design process for agile ﬂight by building data-driven approximate models of the aerodynamics and by synthesizing high-performance nonlinear feedback policies based on these approximate models and trial-and-error experience. This article highlights some of the more remarkable characteristics of nature’s ﬂyers, and describes the challenges involved in replicating this performance in our machines. We conclude by describing our two-meter wingspan autonomous robotic bird and some initial results using machine learning to design control systems for bird-scale, supermaneuverable ﬂight.},
	language = {en},
	author = {Tedrake, Russ and Jackowski, Zack and Cory, Rick and Roberts, John William and Hoburg, Warren},
	pages = {8},
}

@misc{tedrake_categories_nodate,
	title = {Categories and {Subject} {Descriptors}},
	abstract = {Birds routinely execute aerial maneuvers that are far beyond the capabilities of our best aircraft control systems. The complexity and variability of the aerodynamics during these maneuvers are formidable, with dominant flow structures (e.g., vortices) that are difficult to predict robustly from first-principles (Navier-Stokes) models. Here we argue that machine learning will play an important role in the control design process for agile flight by building data-driven approximate models of the aerodynamics and by synthesizing high-performance nonlinear feedback policies based on these approximate models and trial-and-error experience. This article highlights some of the more remarkable characteristics of nature’s flyers, and describes the challenges involved in replicating this performance in our machines. We conclude by describing our two-meter wingspan autonomous robotic bird and some initial results using machine learning to design control systems for bird-scale, supermaneuverable flight.},
	author = {Tedrake, Russ and Jackowski, Zack and Cory, Rick and Roberts, John William and Hoburg, Warren},
}

@article{chang_soft_2020,
	title = {Soft biohybrid morphing wings with feathers underactuated by wrist and finger motion},
	volume = {5},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.aay1246},
	doi = {10.1126/scirobotics.aay1246},
	number = {38},
	urldate = {2022-08-14},
	journal = {Science Robotics},
	author = {Chang, Eric and Matloff, Laura Y. and Stowers, Amanda K. and Lentink, David},
	month = jan,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaay1246},
}

@article{send_artificial_nodate,
	title = {{ARTIFICIAL} {HINGED}-{WING} {BIRD} {WITH} {ACTIVE} {TORSION} {AND} {PARTIALLY} {LINEAR} {KINEMATICS}},
	language = {en},
	author = {Send, Wolfgang and Fischer, Markus and Jebens, Kristof and Mugrauer, Rainer and Nagarathinam, Agalya and Scharstein, Felix},
	pages = {10},
}

@article{gerdes_robo_2014,
	title = {Robo {Raven}: {A} {Flapping}-{Wing} {Air} {Vehicle} with {Highly} {Compliant} and {Independently} {Controlled} {Wings}},
	volume = {1},
	issn = {2169-5172},
	shorttitle = {Robo {Raven}},
	url = {https://www.liebertpub.com/doi/full/10.1089/soro.2014.0019},
	doi = {10.1089/soro.2014.0019},
	abstract = {Many current bird-inspired flapping-wing air vehicles (FWAVs) achieve their flight characteristics through deformations associated with compliant wings during the flapping cycle. Most FWAVs use a single actuator to flap both wings. This couples and synchronizes motions of the wings, which only provides variable rate flapping at constant amplitude to control wing deformations. Independent wing control has the potential to provide a greater flight envelope through the ability to program wing motions to achieve a desired wing shape and associated aerodynamic forces. This approach requires the use of at least two actuators with position and velocity control that can be programmed to drive the wings independently. Integration of two actuators in a flying platform significantly increases the weight and hence makes it challenging to achieve flight. Based on our previous designs with synchronized wing flapping, we developed a new FWAV platform using programmable digital servo motors and a compatible highly compliant wing design that enables shape control of the wings during the flapping cycle. The wings and flapping characteristics can generate the highest possible lift near the maximum power operating point for the servos. The servos were integrated into a wing drive subsystem consisting of 3D printed and laser-etched/cut structural components to reduce part count and weight. A servo-driven tail was also used to augment the steering control and lift of the FWAV. The platform reported in this article, known as Robo Raven, was the first demonstration of a bird-inspired platform doing outdoor aerobatics using independently actuated and controlled wings. This platform successfully performed dives, flips, and buttonhook turns, demonstrating the capability of bioinspired aerobatic maneuvers afforded by the new design.},
	number = {4},
	urldate = {2022-08-14},
	journal = {Soft Robotics},
	author = {Gerdes, John and Holness, Alex and Perez-Rosado, Ariel and Roberts, Luke and Greisinger, Adrian and Barnett, Eli and Kempny, Johannes and Lingam, Deepak and Yeh, Chen-Haur and Bruck, Hugh A. and Gupta, Satyandra K.},
	month = dec,
	year = {2014},
	note = {Publisher: Mary Ann Liebert, Inc., publishers},
	pages = {275--288},
}

@article{peterson_wing-assisted_2011,
	title = {A wing-assisted running robot and implications for avian flight evolution},
	volume = {6},
	issn = {1748-3190},
	url = {https://doi.org/10.1088/1748-3182/6/4/046008},
	doi = {10.1088/1748-3182/6/4/046008},
	abstract = {DASH+Wings is a small hexapedal winged robot that uses flapping wings to increase its locomotion capabilities. To examine the effects of flapping wings, multiple experimental controls for the same locomotor platform are provided by wing removal, by the use of inertially similar lateral spars, and by passive rather than actively flapping wings. We used accelerometers and high-speed cameras to measure the performance of this hybrid robot in both horizontal running and while ascending inclines. To examine consequences of wing flapping for aerial performance, we measured lift and drag forces on the robot at constant airspeeds and body orientations in a wind tunnel; we also determined equilibrium glide performance in free flight. The addition of flapping wings increased the maximum horizontal running speed from 0.68 to 1.29 m s−1, and also increased the maximum incline angle of ascent from 5.6° to 16.9°. Free flight measurements show a decrease of 10.3° in equilibrium glide slope between the flapping and gliding robot. In air, flapping improved the mean lift:drag ratio of the robot compared to gliding at all measured body orientations and airspeeds. Low-amplitude wing flapping thus provides advantages in both cursorial and aerial locomotion. We note that current support for the diverse theories of avian flight origins derive from limited fossil evidence, the adult behavior of extant flying birds, and developmental stages of already volant taxa. By contrast, addition of wings to a cursorial robot allows direct evaluation of the consequences of wing flapping for locomotor performance in both running and flying.},
	language = {en},
	number = {4},
	urldate = {2022-08-14},
	journal = {Bioinspiration \&amp\${\textbackslash}mathsemicolon\$ Biomimetics},
	author = {Peterson, K. and Birkmeyer, P. and Dudley, R. and Fearing, R. S.},
	month = oct,
	year = {2011},
	note = {Publisher: IOP Publishing},
	pages = {046008},
}

@inproceedings{peterson_experimental_2011,
	title = {Experimental dynamics of wing assisted running for a bipedal ornithopter},
	doi = {10.1109/IROS.2011.6095041},
	abstract = {BOLT is a lightweight bipedal ornithopter capable of high-speed dynamic running and effecting transitions between aerial and terrestrial locomotion modes. The gait dynamics of both quasi-static and dynamic locomotion are examined through the use of an on-board accelerometer, part of a one gram electronics package also containing a processor and radio. We discuss the accelerations in the context of the traditional spring-loaded inverted pendulum model seen in nearly all legged locomotion in organisms. Flapping wings are shown to provide damping along with propulsive force. The aerodynamic forces of the flapping wings also impart passive stability to the robot, enabling it to run bipedally with only a single actuator. BOLT transitions from ground running to aerial hovering in as little as one meter of runway. Overall, the advantages provided by wings in terrestrial locomotion, coupled with aerial capabilities, allow BOLT to navigate complex three dimensional environments, switching between locomotion modes when necessary.},
	booktitle = {2011 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Peterson, Kevin and Fearing, Ronald S.},
	month = sep,
	year = {2011},
	note = {ISSN: 2153-0866},
	keywords = {Acceleration, Aerodynamics, Carbon, Fasteners, Force, Legged locomotion},
	pages = {5080--5086},
}

@article{phan_kubeetle-s_2019,
	title = {{KUBeetle}-{S}: {An} insect-like, tailless, hover-capable robot that can fly with a low-torque control mechanism},
	volume = {11},
	issn = {1756-8293},
	shorttitle = {{KUBeetle}-{S}},
	url = {https://doi.org/10.1177/1756829319861371},
	doi = {10.1177/1756829319861371},
	abstract = {For an insect-like tailless flying robot, flapping wings should be able to produce control force as well as flight force to keep the robot staying airborne. This capability requires an active control mechanism, which should be integrated with lightweight microcontrol actuators that can produce sufficient control torques to stabilize the robot due to its inherent instability. In this work, we propose a control mechanism integrated in a hover-capable, two-winged, flapping-wing, 16.4 g flying robot (KUBeetle-S) that can simultaneously change the wing stroke-plane and wing twist. Tilting the stroke plane causes changes in the direction of average thrust and the wing twist distribution to produce control torques for pitch and roll. For yaw (heading change), root spars of left and right wings are adjusted asymmetrically to change the wing twist during flapping motion, resulting in yaw torque generation. Changes in wing kinematics were validated by measuring wing kinematics using three synchronized high-speed cameras. We then performed a series of experiments using a six-axis force/torque load cell to evaluate the effectiveness of the control mechanism via torque generation. We prototyped the robot by integrating the control mechanism with sub-micro servos as control actuators and flight control board. Free flight tests were finally conducted to verify the possibility of attitude control.},
	language = {en},
	urldate = {2022-08-14},
	journal = {International Journal of Micro Air Vehicles},
	author = {Phan, Hoang Vu and Aurecianus, Steven and Kang, Taesam and Park, Hoon Cheol},
	month = jan,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Flapping wing micro aerial vehicle, biomimetics, control mechanism, insect flight, stroke plane modulation},
	pages = {1756829319861371},
}

@misc{noauthor_insectothopter_nodate,
	title = {Insectothopter - {CIA}},
	url = {https://www.cia.gov/legacy/museum/artifact/insectothopter/},
	urldate = {2022-08-14},
}

@article{ristroph_stable_2014,
	title = {Stable hovering of a jellyfish-like flying machine},
	volume = {11},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rsif.2013.0992},
	doi = {10.1098/rsif.2013.0992},
	abstract = {Ornithopters, or flapping-wing aircraft, offer an alternative to helicopters in achieving manoeuvrability at small scales, although stabilizing such aerial vehicles remains a key challenge. Here, we present a hovering machine that achieves self-righting flight using flapping wings alone, without relying on additional aerodynamic surfaces and without feedback control. We design, construct and test-fly a prototype that opens and closes four wings, resembling the motions of swimming jellyfish more so than any insect or bird. Measurements of lift show the benefits of wing flexing and the importance of selecting a wing size appropriate to the motor. Furthermore, we use high-speed video and motion tracking to show that the body orientation is stable during ascending, forward and hovering flight modes. Our experimental measurements are used to inform an aerodynamic model of stability that reveals the importance of centre-of-mass location and the coupling of body translation and rotation. These results show the promise of flapping-flight strategies beyond those that directly mimic the wing motions of flying animals.},
	number = {92},
	urldate = {2022-08-14},
	journal = {Journal of The Royal Society Interface},
	author = {Ristroph, Leif and Childress, Stephen},
	month = mar,
	year = {2014},
	note = {Publisher: Royal Society},
	keywords = {biomimetics, flight control, flight stability, micro air vehicle, unsteady aerodynamics},
	pages = {20130992},
}

@article{de_croon_design_2009,
	title = {Design, {Aerodynamics}, and {Vision}-{Based} {Control} of the {DelFly}},
	volume = {1},
	issn = {1756-8293},
	url = {https://doi.org/10.1260/175682909789498288},
	doi = {10.1260/175682909789498288},
	abstract = {Light-weight, autonomous ornithopters form a promise to observe places that are too small or too dangerous for humans to enter. In this article, we discuss the DelFly project, in which we follow a top-down approach to ever smaller and more autonomous ornithopters. Top-down signifies that the project always focuses on complete flying systems equipped with camera. We give arguments for the approach by explaining which findings on the DelFly I and DelFly II recently led to the development of the DelFly Micro: a 3.07-gram ornithopter carrying a camera and transmitter onboard. These findings concern the design, aerodynamics, and vision-based control of the DelFly. In addition, we identify main obstacles on the road to fly-sized ornithopters.},
	language = {en},
	number = {2},
	urldate = {2022-08-14},
	journal = {International Journal of Micro Air Vehicles},
	author = {de Croon, G.C.H.E. and de Clercq, K.M.E. and Ruijsink, R. and Remes, B. and de Wagter, C.},
	month = jun,
	year = {2009},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {71--97},
}

@article{richter_untethered_2011,
	title = {Untethered {Hovering} {Flapping} {Flight} of a {3D}-{Printed} {Mechanical} {Insect}},
	volume = {17},
	issn = {1064-5462},
	doi = {10.1162/artl_a_00020},
	abstract = {This project focuses on developing a flapping-wing hovering insect using 3D-printed wings and mechanical parts. The use of 3D printing technology has greatly expanded the possibilities for wing design, allowing wing shapes to replicate those of real insects or virtually any other shape. It has also reduced the time of a wing design cycle to a matter of minutes. An ornithopter with a mass of 3.89 g has been constructed using the 3D printing technique and has demonstrated an 85-s passively stable untethered hovering flight. This flight exhibits the functional utility of printed materials for flapping-wing experimentation and ornithopter construction and for understanding the mechanical principles underlying insect flight and control.},
	number = {2},
	journal = {Artificial Life},
	author = {Richter, Charles and Lipson, Hod},
	month = apr,
	year = {2011},
	note = {Conference Name: Artificial Life},
	keywords = {3D printing, insect flight, micro air vehicle, ornithopter, untethered hovering},
	pages = {73--86},
}

@inproceedings{rosen_development_2016,
	title = {Development of a 3.2g untethered flapping-wing platform for flight energetics and control experiments},
	doi = {10.1109/ICRA.2016.7487492},
	abstract = {This paper presents a biologically inspired, 3.2g untethered vehicle capable of both active (flapping) and passive (gliding) flight. We discuss the overall vehicle design, as well as its validation with thrust data from benchtop testing, simulation, and flight test results. The vehicle has one pair of flapping wings for thrust generation, making it a good analogue for insects of the same scale. Flight energetics and control can be thoroughly explored through the array of simulation and testing that have been implemented. Integrated electronics provide wireless communication, sensing, and basic open-loop flight control, making flight test iteration fast and providing additional dynamics data. All of the testing setups and the physical vehicle working together have created a robust development environment for future iterations on the vehicle. The successful flight of the vehicle, including the data collection from onboard sensors and an external motion capture arena, show that this platform is ideal to study flight energetics and control schemes at an insect scale.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Rosen, Michelle H. and le Pivain, Geoffroy and Sahai, Ranjana and Jafferis, Noah T. and Wood, Robert J.},
	month = may,
	year = {2016},
	keywords = {Batteries, Insects, Robot sensing systems, Testing, Vehicles},
	pages = {3227--3233},
}

@article{foehn_time-optimal_2021,
	title = {Time-optimal planning for quadrotor waypoint flight},
	volume = {6},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abh1221},
	doi = {10.1126/scirobotics.abh1221},
	number = {56},
	urldate = {2022-08-06},
	journal = {Science Robotics},
	author = {Foehn, Philipp and Romero, Angel and Scaramuzza, Davide},
	month = jul,
	year = {2021},
	pages = {eabh1221},
}

@article{foehn_alphapilot_2022,
	title = {{AlphaPilot}: autonomous drone racing},
	volume = {46},
	issn = {1573-7527},
	shorttitle = {{AlphaPilot}},
	url = {https://doi.org/10.1007/s10514-021-10011-y},
	doi = {10.1007/s10514-021-10011-y},
	abstract = {This paper presents a novel system for autonomous, vision-based drone racing combining learned data abstraction, nonlinear filtering, and time-optimal trajectory planning. The system has successfully been deployed at the first autonomous drone racing world championship: the 2019 AlphaPilot Challenge. Contrary to traditional drone racing systems, which only detect the next gate, our approach makes use of any visible gate and takes advantage of multiple, simultaneous gate detections to compensate for drift in the state estimate and build a global map of the gates. The global map and drift-compensated state estimate allow the drone to navigate through the race course even when the gates are not immediately visible and further enable to plan a near time-optimal path through the race course in real time based on approximate drone dynamics. The proposed system has been demonstrated to successfully guide the drone through tight race courses reaching speeds up to \$\$\{8\}{\textbackslash},\{{\textbackslash}hbox \{m\}/{\textbackslash}hbox \{s\}\}\$\$and ranked second at the 2019 AlphaPilot Challenge.},
	language = {en},
	number = {1},
	urldate = {2022-08-06},
	journal = {Autonomous Robots},
	author = {Foehn, Philipp and Brescianini, Dario and Kaufmann, Elia and Cieslewski, Titus and Gehrig, Mathias and Muglikar, Manasi and Scaramuzza, Davide},
	month = jan,
	year = {2022},
	keywords = {Drone racing . Agile flight . Aerial vehicles},
	pages = {307--320},
}

@article{best_resilient_2022,
	title = {Resilient {Multi}-{Sensor} {Exploration} of {Multifarious} {Environments} with a {Team} of {Aerial} {Robots}},
	copyright = {info:eu-repo/semantics/openAccess},
	url = {https://opus.lib.uts.edu.au/handle/10453/157741},
	language = {en},
	urldate = {2022-08-06},
	author = {Best, G. and Garg, R. and Keller, J. and Hollinger, G. A. and Scherer, S.},
	year = {2022},
}

@article{petrlik_robust_2020,
	title = {A {Robust} {UAV} {System} for {Operations} in a {Constrained} {Environment}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.2970980},
	abstract = {In this letter we present an autonomous system intended for aerial monitoring, inspection and assistance in Search and Rescue (SAR) operations within a constrained workspace. The proposed system is designed for deployment in demanding real-world environments with extremely narrow passages only slightly wider than the aerial platform, and with limited visibility due to the absence of illumination and the presence of dust. The focus is on precise localization in an unknown environment, high robustness, safety and fast deployment without any need to install an external infrastructure such as an external computer and localization system. These are the main requirements of the targeted SAR scenarios. The performance of the proposed system was successfully evaluated in the Tunnel Circuit of the DARPA Subterranean Challenge, where the UAV cooperated with ground robots to precisely localize artifacts in a coal mine tunnel system. The challenge was unique due to the intention of the organizers to emulate the unpredictable conditions of a real SAR operation, in which there is no prior knowledge of the obstacles that will be encountered.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Petrlík, Matěj and Báča, Tomáš and Heřt, Daniel and Vrba, Matouš and Krajník, Tomáš and Saska, Martin},
	month = apr,
	year = {2020},
	keywords = {Aerial systems: perception and autonomy, Buildings, Cameras, Fuel processing industries, Inspection, Robots, Robustness, Three-dimensional displays, robotics in hazardous fields, search and rescue robots},
	pages = {2169--2176},
}

@article{chen_aerial_2022,
	title = {Aerial {Grasping} and the {Velocity} {Sufficiency} {Region}},
	volume = {7},
	issn = {2377-3766},
	doi = {10.1109/LRA.2022.3192652},
	abstract = {A largely untapped potential for aerial robots is to capture airborne targets in flight. We present an approach in which a simple dynamic model of a quadrotor/target interaction leads to the design of a gripper and associated velocity sufficiency region with a high probability of capture. A model of the interaction dynamics maps the gripper force sufficiency region to an envelope of relative velocities for which capture should be possible without exceeding the capabilities of the quadrotor controller. The approach motivates a gripper design that emphasizes compliance and is passively triggered for a fast response. The resulting gripper is lightweight (23 g) and closes within 12 ms. With this gripper, we demonstrate in-flight experiments that a 550 g drone can capture an 85 g target at various relative velocities between 1 m/s and 2.7 m/s.},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Chen, Tony G. and Hoffmann, Kenneth A. W. and Low, Jun En and Nagami, Keiko and Lentink, David and Cutkosky, Mark R.},
	month = oct,
	year = {2022},
	keywords = {Atmospheric modeling, Drones, Force, Grasping, Grippers, Sensors, Torque, aerial systems: application, mechanism design},
	pages = {10009--10016},
}

@inproceedings{kaufmann_beauty_2019,
	title = {Beauty and the {Beast}: {Optimal} {Methods} {Meet} {Learning} for {Drone} {Racing}},
	shorttitle = {Beauty and the {Beast}},
	doi = {10.1109/ICRA.2019.8793631},
	abstract = {Autonomous micro aerial vehicles still struggle with fast and agile maneuvers, dynamic environments, imperfect sensing, and state estimation drift. Autonomous drone racing brings these challenges to the fore. Human pilots can fly a previously unseen track after a handful of practice runs. In contrast, state-of-the-art autonomous navigation algorithms require either a precise metric map of the environment or a large amount of training data collected in the track of interest. To bridge this gap, we propose an approach that can fly a new track in a previously unseen environment without a precise map or expensive data collection. Our approach represents the global track layout with coarse gate locations, which can be easily estimated from a single demonstration flight. At test time, a convolutional network predicts the poses of the closest gates along with their uncertainty. These predictions are incorporated by an extended Kalman filter to maintain optimal maximum-a-posteriori estimates of gate locations. This allows the framework to cope with misleading high-variance estimates that could stem from poor observability or lack of visible gates. Given the estimated gate poses, we use model predictive control to quickly and accurately navigate through the track. We conduct extensive experiments in the physical world, demonstrating agile and robust flight through complex and diverse previously-unseen race tracks. The presented approach was used to win the IROS 2018 Autonomous Drone Race Competition, outracing the second-placing team by a factor of two.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Kaufmann, Elia and Gehrig, Mathias and Foehn, Philipp and Ranftl, René and Dosovitskiy, Alexey and Koltun, Vladlen and Scaramuzza, Davide},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	keywords = {Current measurement, Drones, Layout, Logic gates, Navigation, Training data, Uncertainty},
	pages = {690--696},
}

@article{foehn_agilicious_2022,
	title = {Agilicious: {Open}-source and open-hardware agile quadrotor for vision-based flight},
	volume = {7},
	shorttitle = {Agilicious},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abl6259},
	doi = {10.1126/scirobotics.abl6259},
	number = {67},
	urldate = {2022-08-06},
	journal = {Science Robotics},
	author = {Foehn, Philipp and Kaufmann, Elia and Romero, Angel and Penicka, Robert and Sun, Sihao and Bauersfeld, Leonard and Laengle, Thomas and Cioffi, Giovanni and Song, Yunlong and Loquercio, Antonio and Scaramuzza, Davide},
	month = jun,
	year = {2022},
	pages = {eabl6259},
}

@article{tang_aggressive_2018,
	title = {Aggressive {Flight} {With} {Suspended} {Payloads} {Using} {Vision}-{Based} {Control}},
	volume = {3},
	issn = {2377-3766},
	doi = {10.1109/LRA.2018.2793305},
	abstract = {Payload manipulation with aerial robots has been an active research area for many years. Recent approaches have sought to plan, control, and execute maneuvers with large, yet deliberate, load swings for more agile, energy-optimal maneuvering. Unfortunately, the system's nonlinear dynamics make executing such trajectories a significant challenge and experimental demonstrations thus far have relied completely on a motion capture system and non-negligible simplifications like restriction of the system to a two-dimensional workspace or closing of the control loop on the quadrotor, instead of the payload. In this work, we observe the payload using a downward-facing camera and estimate its state relative to the quadrotor using an extended Kalman filter. We demonstrate closed-loop payload control in the full three-dimensional workspace, with the planning, estimation, and control pipeline implemented on an onboard processor. We show control of load swings up to 53o from the vertical axis. To the best of our knowledge, this represents the first realization of closed-loop control of agile slung-load maneuvers and the largest achieved payload angle.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Tang, Sarah and Wüest, Valentin and Kumar, Vijay},
	month = apr,
	year = {2018},
	keywords = {Aerial systems: mechanics and control, Attitude control, Cameras, Estimation, Payloads, Robot sensing systems, Trajectory, motion control},
	pages = {1152--1159},
}

@misc{sihite_unsteady_2022,
	title = {Unsteady aerodynamic modeling of {Aerobat} using lifting line theory and {Wagner}'s function},
	url = {http://arxiv.org/abs/2207.12353},
	doi = {10.48550/arXiv.2207.12353},
	abstract = {Flying animals possess highly complex physical characteristics and are capable of performing agile maneuvers using their wings. The flapping wings generate complex wake structures that influence the aerodynamic forces, which can be difficult to model. While it is possible to model these forces using fluid-structure interaction, it is very computationally expensive and difficult to formulate. In this paper, we follow a simpler approach by deriving the aerodynamic forces using a relatively small number of states and presenting them in a simple state-space form. The formulation utilizes Prandtl's lifting line theory and Wagner's function to determine the unsteady aerodynamic forces acting on the wing in a simulation, which then are compared to experimental data of the bat-inspired robot called the Aerobat. The simulated trailing-edge vortex shedding can be evaluated from this model, which then can be analyzed for a wake-based gait design approach to improve the aerodynamic performance of the robot.},
	urldate = {2022-08-06},
	publisher = {arXiv},
	author = {Sihite, Eric and Ghanem, Paul and Salagame, Adarsh and Ramezani, Alireza},
	month = jul,
	year = {2022},
	note = {arXiv:2207.12353 [cs, eess]},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@article{ghanem_efficient_2021,
	title = {Efficient {Modeling} of {Morphing} {Wing} {Flight} {Using} {Neural} {Networks} and {Cubature} {Rules}},
	url = {http://arxiv.org/abs/2110.01057},
	abstract = {Fluidic locomotion of flapping Micro Aerial Vehicles (MAVs) can be very complex, particularly when the rules from insect flight dynamics (fast flapping dynamics and light wings) are not applicable. In these situations, widely used averaging techniques can fail quickly. The primary motivation is to find efficient models for complex forms of aerial locomotion where wings constitute a large part of body mass (i.e., dominant inertial effects) and deform in multiple directions (i.e., morphing wing). In these systems, high degrees of freedom yields complex inertial, Coriolis, and gravity terms. We use Algorithmic Differentiation (AD) and Bayesian filters computed with cubature rules conjointly to quickly estimate complex fluid-structure interactions. In general, Bayesian filters involve finding complex numerical integration (e.g., find posterior integrals). Using cubature rules to compute Gaussian-weighted integrals and AD, we show that the complex multi-degrees-of-freedom dynamics of morphing MAVs can be computed very efficiently and accurately. Therefore, our work facilitates closed-loop feedback control of these morphing MAVs.},
	author = {Ghanem, Paul and Bicer, Yunus and Erdogmus, Deniz and Ramezani, Alireza},
	year = {2021},
	note = {arXiv: 2110.01057},
}

@inproceedings{sihite_enforcing_2020,
	title = {Enforcing nonholonomic constraints in {Aerobat}, a roosting flapping wing model},
	doi = {10.1109/CDC42340.2020.9304158},
	abstract = {Flapping wing flight is a challenging dynamical problem and is also a very fascinating subject to study in the field of biomimetic robotics. A Bat, in particular, has a very articulated armwing mechanism with high degrees-of-freedom and flexibility which allows the animal to perform highly dynamic and complex maneuvers, such as upside-down perching. This paper presents the derivation of a multi-body dynamical system of a bio-inspired bat robot called Aerobat which captures multiple biologically meaningful degrees-of-freedom for flapping flight that is present in biological bats. Then, the work attempts to manifest closed-loop aerial body reorientation and preparation for landing through the manipulation of inertial dynamics and aerodynamics by enforcing nonholonomic constraints onto the system. The proposed design paradigm assumes for rapidly exponentially stable controllers that enforce holonomic constraints in the joint space of the model. A model and optimization-based nonlinear controller is applied to resolve the joint trajectories such that the desired angular momentum about the roll axis is achieved.},
	booktitle = {2020 59th {IEEE} {Conference} on {Decision} and {Control} ({CDC})},
	author = {Sihite, Eric and Ramezani, Alireza},
	month = dec,
	year = {2020},
	note = {ISSN: 2576-2370},
	keywords = {Aerodynamics, Biological system modeling, Birds, Joints, Kinetic theory, Manipulator dynamics, Trajectory},
	pages = {5321--5327},
}

@misc{sihite_mechanism_2020,
	title = {Mechanism {Design} of a {Bio}-inspired {Armwing} {Mechanism} for {Mimicking} {Bat} {Flapping} {Gait}},
	url = {http://arxiv.org/abs/2010.04702},
	doi = {10.48550/arXiv.2010.04702},
	abstract = {The objective of this work is to design and develop a bio-inspired soft and articulated armwing structure which will be an integral component of a morphing aerial co-bot, Aerobat. In our design, we draw inspiration from bats. Bat membranous wings possess unique functions that make them a good example to take inspiration from and transform current aerial drones. In contrast with other flying vertebrates, bats have an extremely articulated musculoskeletal system, key to their body impact-survivability and deliver an impressively adaptive and multimodal locomotion behavior. Bats exclusively use this capability with structural flexibility to generate the controlled force distribution on each wing membrane. The wing flexibility, complex wing kinematics, and fast muscle actuation allow these creatures to change the body configuration within a few tens of milliseconds. These characteristics are crucial to the unrivaled agility of bats and copying them can potentially transform the state-of-the-art aerial drone design.},
	urldate = {2022-08-06},
	publisher = {arXiv},
	author = {Sihite, E. and Kelly, P. and Ramezani, A.},
	month = oct,
	year = {2020},
	note = {arXiv:2010.04702 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{hoff_optimizing_2018,
	title = {Optimizing the structure and movement of a robotic bat with biological kinematic synergies},
	volume = {37},
	issn = {0278-3649},
	url = {https://doi.org/10.1177/0278364918804654},
	doi = {10.1177/0278364918804654},
	abstract = {In this article, we present methods to optimize the design and flight characteristics of a biologically inspired bat-like robot. In previous, work we have designed the topological structure for the wing kinematics of this robot; here we present methods to optimize the geometry of this structure, and to compute actuator trajectories such that its wingbeat pattern closely matches biological counterparts. Our approach is motivated by recent studies on biological bat flight that have shown that the salient aspects of wing motion can be accurately represented in a low-dimensional space. Although bats have over 40 degrees of freedom (DoFs), our robot possesses several biologically meaningful morphing specializations. We use principal component analysis (PCA) to characterize the two most dominant modes of biological bat flight kinematics, and we optimize our robot’s parametric kinematics to mimic these. The method yields a robot that is reduced from five degrees of actuation (DoAs) to just three, and that actively folds its wings within a wingbeat period. As a result of mimicking synergies, the robot produces an average net lift improvesment of 89\% over the same robot when its wings cannot fold.},
	language = {en},
	number = {10},
	urldate = {2022-08-06},
	journal = {The International Journal of Robotics Research},
	author = {Hoff, Jonathan and Ramezani, Alireza and Chung, Soon-Jo and Hutchinson, Seth},
	month = sep,
	year = {2018},
	keywords = {Aerial robotics, biologically inspired robots, kinematics},
	pages = {1233--1252},
}

@inproceedings{hoff_reducing_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Reducing {Versatile} {Bat} {Wing} {Conformations} to a 1-{DoF} {Machine}},
	isbn = {9783319635378},
	doi = {10.1007/978-3-319-63537-8_16},
	abstract = {Recent works have shown success in mimicking the flapping flight of bats on the robotic platform Bat Bot (B2). This robot has only five actuators but retains the ability to flap and fold-unfold its wings in flight. However, this bat-like robot has been unable to perform folding-unfolding of its wings within the period of a wingbeat cycle, about 100 ms. The DC motors operating the spindle mechanisms cannot attain this folding speed. Biological bats rely on this periodic folding of their wings during the upstroke of the wingbeat cycle. It reduces the moment of inertia of the wings and limits the negative lift generated during the upstroke. Thus, we consider it important to achieve wing folding during the upstroke. A mechanism was designed to couple the flapping cycle to the folding cycle of the robot. We then use biological data to further optimize the mechanism such that the kinematic synergies of the robot best match those of a biological bat. This ensures that folding is performed at the correct point in the wingbeat cycle.},
	language = {en},
	booktitle = {Biomimetic and {Biohybrid} {Systems}},
	publisher = {Springer International Publishing},
	author = {Hoff, Jonathan and Ramezani, Alireza and Chung, Soon-Jo and Hutchinson, Seth},
	editor = {Mangan, Michael and Cutkosky, Mark and Mura, Anna and Verschure, Paul F.M.J. and Prescott, Tony and Lepora, Nathan},
	year = {2017},
	keywords = {Aerial robotics, Bats, Biologically-inspired robots, Kinematics},
	pages = {181--192},
}

@article{ramezani_biomimetic_2017,
	title = {A biomimetic robotic platform to study flight specializations of bats},
	volume = {2},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.aal2505},
	doi = {10.1126/scirobotics.aal2505},
	number = {3},
	urldate = {2022-08-06},
	journal = {Science Robotics},
	author = {Ramezani, Alireza and Chung, Soon-Jo and Hutchinson, Seth},
	month = feb,
	year = {2017},
	pages = {eaal2505},
}

@misc{sihite_bang-bang_2022,
	title = {Bang-{Bang} {Control} {Of} {A} {Tail}-less {Morphing} {Wing} {Flight}},
	url = {http://arxiv.org/abs/2205.06395},
	abstract = {Bats' dynamic morphing wings are known to be extremely high-dimensional, and they employ the combination of inertial dynamics and aerodynamics manipulations to showcase extremely agile maneuvers. Bats heavily rely on their highly flexible wings and are capable of dynamically morphing their wings to adjust aerodynamic and inertial forces applied to their wing and perform sharp banking turns. There are technical hardware and control challenges in copying the morphing wing flight capabilities of flying animals. This work is majorly focused on the modeling and control aspects of stable, tail-less, morphing wing flight. A classical control approach using bang-bang control is proposed to stabilize a bio-inspired morphing wing robot called Aerobat. Robot-environment interactions based on horseshoe vortex shedding and Wagner functions is derived to realistically evaluate the feasibility of the bang-bang control, which is then implemented on the robot in experiments to demonstrate first-time closed-loop stable flights of Aerobat.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Sihite, Eric and Hu, Xintao and Li, Bozhen and Salagame, Adarsh and Ghanem, Paul and Ramezani, Alireza},
	month = may,
	year = {2022},
	note = {arXiv:2205.06395 [cs, eess]},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@misc{noauthor_pupil-labsapriltags_nodate,
	title = {pupil-labs/apriltags: {Python} bindings for the apriltags3 library},
	url = {https://github.com/pupil-labs/apriltags},
	urldate = {2022-08-03},
}

@misc{noauthor_pupil-labsapriltags_nodate-1,
	title = {pupil-labs/apriltags: {Python} bindings for the apriltags3 library},
	url = {https://github.com/pupil-labs/apriltags},
	urldate = {2022-08-03},
}

@misc{noauthor_pupil-labsapriltags_nodate-2,
	title = {pupil-labs/apriltags: {Python} bindings for the apriltags3 library},
	url = {https://github.com/pupil-labs/apriltags},
	urldate = {2022-08-03},
}

@misc{noauthor_pupil-labsapriltags_nodate-3,
	title = {pupil-labs/apriltags: {Python} bindings for the apriltags3 library},
	url = {https://github.com/pupil-labs/apriltags},
	urldate = {2022-08-03},
}

@misc{noauthor_apriltags-source_2019,
	title = {apriltags-source},
	copyright = {BSD-2-Clause},
	url = {https://github.com/pupil-labs/apriltags-source},
	abstract = {AprilTag is a visual fiducial system popular for robotics research.},
	urldate = {2022-08-03},
	publisher = {Pupil Labs},
	month = sep,
	year = {2019},
	note = {original-date: 2019-07-23T13:42:18Z},
}

@inproceedings{engel_lsd-slam_2014,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{LSD}-{SLAM}: {Large}-{Scale} {Direct} {Monocular} {SLAM}},
	isbn = {978-3-319-10605-2},
	shorttitle = {{LSD}-{SLAM}},
	doi = {10.1007/978-3-319-10605-2_54},
	abstract = {We propose a direct (feature-less) monocular SLAM algorithm which, in contrast to current state-of-the-art regarding direct methods, allows to build large-scale, consistent maps of the environment. Along with highly accurate pose estimation based on direct image alignment, the 3D environment is reconstructed in real-time as pose-graph of keyframes with associated semi-dense depth maps. These are obtained by filtering over a large number of pixelwise small-baseline stereo comparisons. The explicitly scale-drift aware formulation allows the approach to operate on challenging sequences including large variations in scene scale. Major enablers are two key novelties: (1) a novel direct tracking method which operates on \${\textbackslash}mathfrak\{sim\}(3)\$, thereby explicitly detecting scale-drift, and (2) an elegant probabilistic solution to include the effect of noisy depth values into tracking. The resulting direct monocular SLAM system runs in real-time on a CPU.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2014},
	publisher = {Springer International Publishing},
	author = {Engel, Jakob and Schöps, Thomas and Cremers, Daniel},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	year = {2014},
	keywords = {Augmented Reality, Convergence Radius, Image Alignment, Inverse Depth, Visual Odometry},
	pages = {834--849},
}

@inproceedings{latif_slam_2019,
	title = {{SLAM} algorithms implementation in a {UAV}, based on a heterogeneous system: {A} survey},
	shorttitle = {{SLAM} algorithms implementation in a {UAV}, based on a heterogeneous system},
	doi = {10.1109/ICoCS.2019.8930783},
	abstract = {Simultaneous localization and mapping (SLAM) is a solid problem in robotics, this problem can be solved using an algorithm dedicated to the resolution of this problem. In this context we find large variety algorithms like FastSLAM2.0, GraphSLAM, EKFSLAM, VoSLAM, ORBSLAM and other. The drone by its ease and speed to implement allows realizing various types of aerial mapping increasingly used in many sectors of professional activities such as building, public works or agriculture. In this study we propose an implementation of the VoSLAM algorithm in a UAV, based on a heterogeneous system using the OpenCL parallel programming. In the agricultural cities, for agricole process parameters controlling, we can use the heterogeneous DE1-Soc architecture of VoSLAM algorithm based on the OpenCL parallel programming. This architecture permits the control in real-time of the humidity based on a treatment of image to transmit thereafter its data..},
	booktitle = {2019 4th {World} {Conference} on {Complex} {Systems} ({WCCS})},
	author = {Latif, Rachid and Saddik, Amine},
	month = apr,
	year = {2019},
	keywords = {Agricultural cities, Complexity theory, Computer architecture, DE1-Soc, Drone, Field programmable gate arrays, Heterogeneous systems, OpenCL architecure, Real-time, Simultaneous localization and mapping, Smoothing methods, Trajectory, VOSLAM, humidity control},
	pages = {1--6},
}

@article{hu_bang-bang_2022,
	title = {Bang-bang control of a tail-less morphing wing flight},
	url = {https://repository.library.northeastern.edu/files/neu:4f17gn39n},
	abstract = {Bats' dynamic morphing wings are known to be extremely high-dimensional, and theyemploy the combination of inertial dynamics and aerodynamics manipulations to showcase extremely agile maneuvers. Bats heavily rely on their highly flexible wings and are capable of dynamically morphing their wings to adjust aerodynamic and inertial forces applied to their wing and perform sharp banking turns. There are technical hardware and control challenges in copying the morphing wing flight capabilities of flying animals. This work is majorly focused on the modeling and control aspects of stable, tail-less, morphing wing flight. A classical control approach using bang- bang control is proposed to stabilize a bio-inspired morphing wing robot called Aerobat. Robot- environment interactions based on horseshoe vortex shedding and Wagner functions are derived to realistically evaluate the feasibility of the bang-bang control, which is then implemented on the robot in experiments to demonstrate first-time closed-loop stable flights of Aerobat.--Author's abstract},
	urldate = {2022-08-01},
	author = {Hu, Xintao},
	year = {2022},
}

@inproceedings{rehder_extending_2016,
	title = {Extending kalibr: {Calibrating} the extrinsics of multiple {IMUs} and of individual axes},
	shorttitle = {Extending kalibr},
	doi = {10.1109/ICRA.2016.7487628},
	abstract = {An increasing number of robotic systems feature multiple inertial measurement units (IMUs). Due to competing objectives-either desired vicinity to the center of gravity when used in controls, or an unobstructed field of view when integrated in a sensor setup with an exteroceptive sensor for ego-motion estimation-individual IMUs are often mounted at considerable distance. As a result, they sense different accelerations when the platform is subjected to rotational motions. In this work, we derive a method for spatially calibrating multiple IMUs in a single estimator based on the open-source camera/IMU calibration toolbox kalibr. We further extend the toolbox to determine IMU intrinsics, enabling accurate calibration of low-cost IMUs. The results suggest that the extended estimator is capable of precisely determining these intrinsics and even of localizing individual accelerometer axes inside a commercial grade IMU to millimeter precision.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Rehder, Joern and Nikolic, Janosch and Schneider, Thomas and Hinzmann, Timo and Siegwart, Roland},
	month = may,
	year = {2016},
	keywords = {Acceleration, Accelerometers, Calibration, Cameras, Estimation, Gyroscopes, Robot sensing systems},
	pages = {4304--4311},
}

@inproceedings{miller_robust_2022,
	title = {Robust {Semantic} {Mapping} and {Localization} on a {Free}-{Flying} {Robot} in {Microgravity}},
	doi = {10.1109/ICRA46639.2022.9811862},
	abstract = {We propose a system that uses semantic object detections to localize a microgravity free-flyer. Many applications require absolute localization in a known reference frame, such as the execution of waypoint trajectories defined by human operators. Classical geometric methods build a map of point features, which may not be able to be associated after lighting or environmental changes. By contrast, semantics remain invariant to changes up to the robustness of the detection algorithm and motion of the semantic objects. In this work, we describe our approaches for both offline semantic map generation as well as online localization against a semantic map, intended to run in real-time on the robot. We additionally demonstrate how our semantic localizer outperforms image-feature matching in some cases, and show the robustness of the algorithm to environmental changes. Crucially, we show in our experiments that when semantics are used to supplement point features, localization is always improved. To our knowledge, these experiments demonstrate the first use of learned semantics for localization on a free-flying robot in microgravity.},
	booktitle = {2022 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Miller, Ian D. and Soussan, Ryan and Coltin, Brian and Smith, Trey and Kumar, Vijay},
	month = may,
	year = {2022},
	keywords = {Heating systems, Lighting, Location awareness, Object detection, Real-time systems, Robustness, Semantics},
	pages = {4121--4127},
}

@misc{rosinol_kimera_2020,
	title = {Kimera: an {Open}-{Source} {Library} for {Real}-{Time} {Metric}-{Semantic} {Localization} and {Mapping}},
	shorttitle = {Kimera},
	url = {http://arxiv.org/abs/1910.02490},
	doi = {10.48550/arXiv.1910.02490},
	abstract = {We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.},
	urldate = {2022-07-17},
	publisher = {arXiv},
	author = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
	month = mar,
	year = {2020},
	note = {arXiv:1910.02490 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@misc{rosinol_kimera_2020-1,
	title = {Kimera: an {Open}-{Source} {Library} for {Real}-{Time} {Metric}-{Semantic} {Localization} and {Mapping}},
	shorttitle = {Kimera},
	url = {http://arxiv.org/abs/1910.02490},
	doi = {10.48550/arXiv.1910.02490},
	abstract = {We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS- Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.},
	urldate = {2022-07-17},
	publisher = {arXiv},
	author = {Rosinol, Antoni and Abate, Marcus and Chang, Yun and Carlone, Luca},
	month = mar,
	year = {2020},
	note = {arXiv:1910.02490 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{qin_vins-mono_2018,
	title = {{VINS}-{Mono}: {A} {Robust} and {Versatile} {Monocular} {Visual}-{Inertial} {State} {Estimator}},
	volume = {34},
	issn = {15523098},
	doi = {10.1109/TRO.2018.2853729},
	abstract = {One camera and one low-cost inertial measurement unit (IMU) form a monocular visual-inertial system (VINS), which is the minimum sensor suite (in size, weight, and power) for the metric six degrees-of-freedom (DOF) state estimation. In this paper, we present VINS-Mono: a robust and versatile monocular visual-inertial state estimator. Our approach starts with a robust procedure for estimator initialization. A tightly coupled, nonlinear optimization-based method is used to obtain highly accurate visual-inertial odometry by fusing preintegrated IMU measurements and feature observations. A loop detection module, in combination with our tightly coupled formulation, enables relocalization with minimum computation. We additionally perform 4-DOF pose graph optimization to enforce the global consistency. Furthermore, the proposed system can reuse a map by saving and loading it in an efficient way. The current and previous maps can be merged together by the global pose graph optimization. We validate the performance of our system on public datasets and real-world experiments and compare against other state-of-the-art algorithms. We also perform an onboard closed-loop autonomous flight on the microaerial-vehicle platform and port the algorithm to an iOS-based demonstration. We highlight that the proposed work is a reliable, complete, and versatile system that is applicable for different applications that require high accuracy in localization. We open source our implementations for both PCs (https://github.com/HKUST-Aerial-Robotics/VINS-Mono) and iOS mobile devices ( https://github.com/HKUST-Aerial-Robotics/VINS-Mobile).},
	number = {4},
	journal = {IEEE Transactions on Robotics},
	author = {Qin, Tong and Li, Peiliang and Shen, Shaojie},
	month = aug,
	year = {2018},
	note = {arXiv: 1708.03852
Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Monocular visual-inertial systems (VINSs), sensor fusion, simultaneous localization and mapping, state estimation},
	pages = {1004--1020},
}

@article{campos_orb-slam3_2021,
	title = {{ORB}-{SLAM3}: {An} {Accurate} {Open}-{Source} {Library} for {Visual}, {Visual}-{Inertial} and {Multi}-{Map} {SLAM}},
	volume = {37},
	issn = {1552-3098, 1941-0468},
	shorttitle = {{ORB}-{SLAM3}},
	url = {http://arxiv.org/abs/2007.11898},
	doi = {10.1109/TRO.2021.3075644},
	abstract = {This paper presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a feature-based tightly-integrated visual-inertial SLAM system that fully relies on Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization phase. The result is a system that operates robustly in real-time, in small and large, indoor and outdoor environments, and is 2 to 5 times more accurate than previous approaches. The second main novelty is a multiple map system that relies on a new place recognition method with improved recall. Thanks to it, ORB-SLAM3 is able to survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting mapped areas. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information. This allows to include in bundle adjustment co-visible keyframes, that provide high parallax observations boosting accuracy, even if they are widely separated in time or if they come from a previous mapping session. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature, and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting representative of AR/VR scenarios. For the benefit of the community we make public the source code.},
	number = {6},
	urldate = {2022-07-17},
	journal = {IEEE Transactions on Robotics},
	author = {Campos, Carlos and Elvira, Richard and Rodríguez, Juan J. Gómez and Montiel, José M. M. and Tardós, Juan D.},
	month = dec,
	year = {2021},
	note = {arXiv:2007.11898 [cs]},
	keywords = {Computer Science - Robotics},
	pages = {1874--1890},
}

@misc{neuman_tiny_2022,
	title = {Tiny {Robot} {Learning}: {Challenges} and {Directions} for {Machine} {Learning} in {Resource}-{Constrained} {Robots}},
	shorttitle = {Tiny {Robot} {Learning}},
	url = {http://arxiv.org/abs/2205.05748},
	doi = {10.48550/arXiv.2205.05748},
	abstract = {Machine learning (ML) has become a pervasive tool across computing systems. An emerging application that stress-tests the challenges of ML system design is tiny robot learning, the deployment of ML on resource-constrained low-cost autonomous robots. Tiny robot learning lies at the intersection of embedded systems, robotics, and ML, compounding the challenges of these domains. Tiny robot learning is subject to challenges from size, weight, area, and power (SWAP) constraints; sensor, actuator, and compute hardware limitations; end-to-end system tradeoffs; and a large diversity of possible deployment scenarios. Tiny robot learning requires ML models to be designed with these challenges in mind, providing a crucible that reveals the necessity of holistic ML system design and automated end-to-end design tools for agile development. This paper gives a brief survey of the tiny robot learning space, elaborates on key challenges, and proposes promising opportunities for future work in ML system design.},
	urldate = {2022-07-17},
	publisher = {arXiv},
	author = {Neuman, Sabrina M. and Plancher, Brian and Duisterhof, Bardienus P. and Krishnan, Srivatsan and Banbury, Colby and Mazumder, Mark and Prakash, Shvetank and Jabbour, Jason and Faust, Aleksandra and de Croon, Guido C. H. E. and Reddi, Vijay Janapa},
	month = may,
	year = {2022},
	note = {arXiv:2205.05748 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@article{zufferey_design_2021,
	title = {Design of the {High}-{Payload} {Flapping} {Wing} {Robot} {E}-{Flap}},
	volume = {6},
	issn = {2377-3766},
	doi = {10.1109/LRA.2021.3061373},
	abstract = {Autonomous lightweight flapping-wing robots show potential to become a safe and affordable solution for rapidly deploying robots around humans and in complex environments. The absence of propellers makes such vehicles more resistant to physical contact, permitting flight in cluttered environments, and collaborating with humans. Importantly, the provision of thousands of species of birds that have already mastered the challenging task of flapping flight is a rich source of solutions. However, small wing flapping technology is still in its beginnings, with limited levels of autonomy and physical interaction capability with the environment. One significant limitation to this is the low payload available. Here we show the Eagle-inspired Flapping-wing robot E-Flap, a 510 g novel design capable of a 100\% of payload, exceeding the requirement of the computing and sensing package needed to fly with a high degree of autonomy. The concept is extensively characterized, both in a tracked indoor space and in outdoor conditions. We demonstrate flight path angle of up to 50° and velocities from as low as 2 m/s to over 6 m/s. Overall, the robotic platform has been proven to be reliable, having performed over 100 flights. Through mechanical and electronics advances, the E-Flap is a robust vehicle prototype and paves the way towards flapping-wing robots becoming a practical fully autonomous flying solution. Video attachment: https://youtu.be/GpAa176TMf0.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Zufferey, Raphael and Tormo-Barbero, Jesús and Guzmán, M. Mar and Maldonado, Fco. Javier and Sanchez-Laulhe, Ernesto and Grau, Pedro and Pérez, Martín and Acosta, José Ángel and Ollero, Anibal},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Birds, Carbon, Fasteners, Flapping-wing flight, Payloads, Propellers, Robots, Task analysis, payload-capable, prototype design, unmanned autonomous vehicles},
	pages = {3097--3104},
}

@article{fuller_four_2019,
	title = {Four {Wings}: {An} {Insect}-{Sized} {Aerial} {Robot} {With} {Steering} {Ability} and {Payload} {Capacity} for {Autonomy}},
	volume = {4},
	issn = {2377-3766},
	shorttitle = {Four {Wings}},
	doi = {10.1109/LRA.2019.2891086},
	abstract = {This letter introduces a new aerial insect-sized robot weighing 143 mg-slightly more than a honeybee-actuated by four perpendicular wings splayed outward. This arrangement gives the robot more capabilities than previous two-winged designs. These include the ability to actuate around a vertical axis (steering), and enough payload capacity ({\textgreater}260 mg) to carry components such as sensor packages or power systems. To validate the design, the author demonstrated steering actuation in flight, as well as hovering position control using motion capture feedback. Analysis and preliminary experiment additionally suggests that the robot may be passively stable in attitude. The letter concludes by proposing a minimal set of components the robot would need to carry to achieve either sensor-autonomous flight, or power autonomous flight powered by supercapacitors. In both cases, earlier two-winged designs do not have enough lift. This robot therefore represents a mechanical platform that is well-suited to future sensor- and power-autonomous insect robots.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Fuller, Sawyer B.},
	month = apr,
	year = {2019},
	keywords = {Attitude control, Insects, Micro/nano robots, Payloads, Robot sensing systems, Torque, Wires, aerial systems: mechanics and control, automation at micro-nano scales},
	pages = {570--577},
}

@inproceedings{duhamel_altitude_2012,
	title = {Altitude feedback control of a flapping-wing microrobot using an on-board biologically inspired optical flow sensor},
	doi = {10.1109/ICRA.2012.6225313},
	abstract = {We present experimental results on the controlled vertical flight of a flapping-wing flying microrobot, in which for the first time an on-board sensing system is used for measuring the microrobot's altitude for feedback control. Both the control strategy and the sensing system are biologically inspired. The control strategy relies on amplitude modulation mediated by optical flow. The research presented here is a key step toward achieving the goal of complete autonomy for flying microrobots, since this demonstrates that strategies for controlling flapping-wing microrobots in vertical flight can rely on optical flow sensors.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	author = {Duhamel, Pierre-Emile J. and Pérez-Arancibia, Néstor O. and Barrows, Geoffrey L. and Wood, Robert J.},
	month = may,
	year = {2012},
	note = {ISSN: 1050-4729},
	keywords = {Force, Optical device fabrication, Optical feedback, Optical imaging, Optical sensors, Robot sensing systems},
	pages = {4228--4235},
}

@inproceedings{garcia_bermudez_optical_2009,
	title = {Optical flow on a flapping wing robot},
	doi = {10.1109/IROS.2009.5354337},
	abstract = {Optical flow sensing techniques are promising for obstacle avoidance, distance regulation, and moving target tracking, particularly for small mobile robots with limited power and payload constraints. Most optical flow sensing experimental work has been done on mobile platforms which are relatively steady in rotation, unlike the pitching motion expected on flapping wing flyers. In order to assess the feasibility of using optical flow to control an indoor flapping flyer, an 7 gram commercially available ornithopter airframe was equipped with on-board camera and CPU module with mass of 2.5 grams and 2.6 gram battery. An experiment was conducted capturing optical flow information during flapping and gliding flight on the same platform. As expected, flapping introduced substantial systematic bias to the direction estimates to the point of flipping the true direction periodically. Nonetheless, since the optical flow results oscillated at the same frequency as the flapping wings, it is envisioned that one could disambiguate the jittering optic flow measurements by correlating these with real-time feedback from the motor current.},
	booktitle = {2009 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Garcia Bermudez, Fernando and Fearing, Ronald},
	month = oct,
	year = {2009},
	note = {ISSN: 2153-0866},
	keywords = {Cameras, Image motion analysis, Mobile robots, Optical control, Optical feedback, Optical sensors, Payloads, Robot sensing systems, Target tracking, Weight control},
	pages = {5027--5032},
}

@article{roshanbin_colibri_2017,
	title = {{COLIBRI}: {A} hovering flapping twin-wing robot},
	volume = {9},
	issn = {1756-8293},
	shorttitle = {{COLIBRI}},
	url = {https://doi.org/10.1177/1756829317695563},
	doi = {10.1177/1756829317695563},
	abstract = {This paper describes the results of a six-year project aiming at designing and constructing a flapping twin-wing robot of the size of hummingbird (Colibri in French) capable of hovering. Our prototype has a total mass of 22 g, a wing span of 21 cm and a flapping frequency of 22 Hz; it is actively stabilized in pitch and roll by changing the wing camber with a mechanism known as wing twist modulation. The proposed design of wing twist modulation effectively alters the mean lift vector with respect to the center of gravity by reorganization of the airflow. This mechanism is modulated by an onboard control board which calculates the corrective feedback control signals through a closed-loop PD controller in order to stabilize the robot. Currently, there is no control on the yaw axis which is passively stable, and the vertical position is controlled manually by tuning the flapping frequency. The paper describes the recent evolution of the various sub-systems: the wings, the flapping mechanism, the generation of control torques, the avionics and the PD control. The robot has demonstrated successful hovering flights with an on-board battery for the flight autonomy of 15–20 s.},
	language = {en},
	number = {4},
	urldate = {2022-07-17},
	journal = {International Journal of Micro Air Vehicles},
	author = {Roshanbin, A and Altartouri, H and Karásek, M and Preumont, A},
	month = dec,
	year = {2017},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Hovering flapping wing robot, active stabilization, hummingbird, wing twist modulation},
	pages = {270--282},
}

@inproceedings{eguiluz_towards_2019,
	title = {Towards flapping wing robot visual perception: {Opportunities} and challenges},
	shorttitle = {Towards flapping wing robot visual perception},
	doi = {10.1109/REDUAS47371.2019.8999674},
	abstract = {The development of perception systems for bio-inspired flapping wing robots, or ornithopters, is very challenging due to their fast flying maneuvers and the high amount of vibrations and motion blur originated by the wing flapping. Visual sensors have been widely used in aerial robot perception due to their size, weight, and energy consumption capabilities. This paper analyzes the issues and challenges for vision sensors onboard ornithopter robots. Two visual sensors are evaluated: a monocular camera and an event-based camera. First, the pros and cons of integrating different sensors on flapping wing robots are studied. Second, the paper experimentally evaluates the impact of wing flapping frequency on both sensors using experiments with the ornithopter developed in the EU-funded GRIFFIN ERC project.},
	booktitle = {2019 {Workshop} on {Research}, {Education} and {Development} of {Unmanned} {Aerial} {Systems} ({RED} {UAS})},
	author = {Eguíluz, A. Gómez and Rodríguez-Gómez, J.P. and Paneque, J.L. and Grau, P. and de Dios, J.R. Martínez and Ollero, A.},
	month = nov,
	year = {2019},
	keywords = {Robotic perception, bio-inspired robots, event-based vision, flapping wings, ornithopter},
	pages = {335--343},
}

@article{rayner_aerodynamics_1991,
	title = {On the aerodynamics of animal flight in ground effect},
	volume = {334},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.1991.0101},
	doi = {10.1098/rstb.1991.0101},
	abstract = {Flight in ground effect above a flat, smooth surface may give an animal considerable performance advantages, including a reduction in cost of transport of up to 15\%, and a reduction in mechanical flight power of as much as 35\%, compared with values for flight out of ground effect. Previous theories modelling the phenomenon have either been incomplete or marred by typographical errors. A complete lifting line theory of flight in ground effect with a fixed wing is developed, and instructions are given so that it may be applied to animals such as skimmers, pelicans and myotid bats which fly and forage close above water. Several predictions are made about likely flight behaviour in ground effect, and about the appropriate flight morphology for taking advantage of the potential performance improvements. The most important conclusion, differing from previous analyses, is that slow flight performance in ground effect is very poor, owing to the horizontal air velocity induced around the wing in the presence of the ground.},
	number = {1269},
	urldate = {2022-07-16},
	journal = {Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences},
	author = {Rayner, Jeremy M. V. and Bone, Quentin},
	month = oct,
	year = {1991},
	note = {Publisher: Royal Society},
	pages = {119--128},
}

@inproceedings{zhang_design_2017,
	title = {Design optimization and system integration of robotic hummingbird},
	doi = {10.1109/ICRA.2017.7989639},
	abstract = {Flying animals with flapping wings may best exemplify the astonishing ability of natural selection on design optimization by excelling both stability and maneuverability at insect/hummingbird scale. Flapping Wing Micro Air Vehicle (FWMAV) holds great promise in bridging the performance gap between engineering system and their natural counterparts. Designing and constructing such a system is a challenging problem under stringent size, weight and power (SWaP) constraints. In this work, we presented a systematic approach for design optimization and integration for a hummingbird inspired FWMAV. Our formulation covers aspects of actuation, dynamics, flight stability and control, which was validated by experimental data for both rigid and flexible wings, ranging from low to high wing loading. The optimization yields prototypes with onboard sensors, electronics, and computation units. The prototype flaps at 30Hz to 40Hz, with 7.5 to 12 grams of system weight and 12 to 20 grams of maximum lift. Liftoff was demonstrated with added payloads. Flapping wing platforms with different requirements and scales can now be designed and optimized with minor modifications of proposed formulation.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Zhang, Jian and Fei, Fan and Tu, Zhan and Deng, Xinyan},
	month = may,
	year = {2017},
	keywords = {DC motors, Design optimization, Gears, Prototypes, Springs, System integration, Vehicle dynamics},
	pages = {5422--5428},
}

@inproceedings{fei_flappy_2019,
	title = {Flappy {Hummingbird}: {An} {Open} {Source} {Dynamic} {Simulation} of {Flapping} {Wing} {Robots} and {Animals}},
	shorttitle = {Flappy {Hummingbird}},
	doi = {10.1109/ICRA.2019.8794089},
	abstract = {Insects and hummingbirds exhibit extraordinary flight performance and can simultaneously master seemingly conflicting goals: stable hovering and aggressive maneuvering, which are unmatched by conventional small scale man-made vehicles. Flapping Wing Micro Air Vehicles (FWMAVs) hold great promise for closing this performance gap. However, design and control of such systems remain challenging. Here, we present an open source high fidelity dynamic simulation for FWMAVs. The simulator serves as a testbed for the design, optimization and flight control of FWMAVs. To validate the simulation, we recreated the at-scale hummingbird robot developed in our lab in the simulation. System identification was performed to obtain the model parameters. Force generation and dynamic response of open-loop and closed loop systems between simulated and experimental flights were compared. The unsteady aerodynamics and the highly nonlinear flight dynamics present challenging control problems for conventional and learning control algorithms such as Reinforcement Learning. The interface of the simulation is fully compatible with OpenAI Gym environment. As a benchmark study, we present a linear controller for hovering stabilization and a Deep Reinforcement Learning control policy for goal-directed maneuvering. Finally, we demonstrate direct simulation-to-real transfer of both control policies onto the physical robot, further demonstrating the fidelity of the simulation.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Fei, Fan and Tu, Zhan and Yang, Yilun and Zhang, Jian and Deng, Xinyan},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	keywords = {Aerodynamics, Animals, Force, Robots, Torque, Vehicle dynamics},
	pages = {9223--9229},
}

@article{wensing_proprioceptive_2017,
	title = {Proprioceptive {Actuator} {Design} in the {MIT} {Cheetah}: {Impact} {Mitigation} and {High}-{Bandwidth} {Physical} {Interaction} for {Dynamic} {Legged} {Robots}},
	volume = {33},
	issn = {1941-0468},
	shorttitle = {Proprioceptive {Actuator} {Design} in the {MIT} {Cheetah}},
	doi = {10.1109/TRO.2016.2640183},
	abstract = {Designing an actuator system for highly dynamic legged robots has been one of the grand challenges in robotics research. Conventional actuators for manufacturing applications have difficulty satisfying design requirements for high-speed locomotion, such as the need for high torque density and the ability to manage dynamic physical interactions. To address this challenge, this paper suggests a proprioceptive actuation paradigm that enables highly dynamic performance in legged machines. Proprioceptive actuation uses collocated force control at the joints to effectively control contact interactions at the feet under dynamic conditions. Modal analysis of a reduced leg model and dimensional analysis of DC motors address the main principles for implementation of this paradigm. In the realm of legged machines, this paradigm provides a unique combination of high torque density, high-bandwidth force control, and the ability to mitigate impacts through backdrivability. We introduce a new metric named the “impact mitigation factor” (IMF) to quantify backdrivability at impact, which enables design comparison across a wide class of robots. The MIT Cheetah leg is presented, and is shown to have an IMF that is comparable to other quadrupeds with series springs to handle impact. The design enables the Cheetah to control contact forces during dynamic bounding, with contact times down to 85 ms and peak forces over 450 N. The unique capabilities of the MIT Cheetah, achieving impact-robust force-controlled operation in high-speed three-dimensional running and jumping, suggest wider implementation of this holistic actuation approach.},
	number = {3},
	journal = {IEEE Transactions on Robotics},
	author = {Wensing, Patrick M. and Wang, Albert and Seok, Sangok and Otten, David and Lang, Jeffrey and Kim, Sangbae},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Actuators, Dynamics, Force control, Legged locomotion, Measurement, Torque, design engineering, dynamics, legged locomotion},
	pages = {509--522},
}

@misc{placed_survey_2022,
	title = {A {Survey} on {Active} {Simultaneous} {Localization} and {Mapping}: {State} of the {Art} and {New} {Frontiers}},
	shorttitle = {A {Survey} on {Active} {Simultaneous} {Localization} and {Mapping}},
	url = {http://arxiv.org/abs/2207.00254},
	doi = {10.48550/arXiv.2207.00254},
	abstract = {Active Simultaneous Localization and Mapping (SLAM) is the problem of planning and controlling the motion of a robot to build the most accurate and complete model of the surrounding environment. Since the first foundational work in active perception appeared, more than three decades ago, this field has received increasing attention across different scientific communities. This has brought about many different approaches and formulations, and makes a review of the current trends necessary and extremely valuable for both new and experienced researchers. In this work, we survey the state-of-the-art in active SLAM and take an in-depth look at the open challenges that still require attention to meet the needs of modern applications. \% in order to achieve real-world deployment. After providing a historical perspective, we present a unified problem formulation and review the classical solution scheme, which decouples the problem into three stages that identify, select, and execute potential navigation actions. We then analyze alternative approaches, including belief-space planning and modern techniques based on deep reinforcement learning, and review related work on multi-robot coordination. The manuscript concludes with a discussion of new research directions, addressing reproducible research, active spatial perception, and practical applications, among other topics.},
	urldate = {2022-07-10},
	publisher = {arXiv},
	author = {Placed, Julio A. and Strader, Jared and Carrillo, Henry and Atanasov, Nikolay and Indelman, Vadim and Carlone, Luca and Castellanos, José A.},
	month = jul,
	year = {2022},
	note = {arXiv:2207.00254 [cs]},
	keywords = {Computer Science - Robotics},
}

@article{rodriguez-gomez_free_2022,
	title = {Free as a {Bird}: {Event}-{Based} {Dynamic} {Sense}-and-{Avoid} for {Ornithopter} {Robot} {Flight}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {Free as a {Bird}},
	doi = {10.1109/LRA.2022.3153904},
	abstract = {Autonomous flight of flapping-wing robots is a major challenge for robot perception. Most of the previous sense-and-avoid works have studied the problem of obstacle avoidance for flapping-wing robots considering only static obstacles. This letter presents a fully onboard dynamic sense-and-avoid scheme for large-scale ornithopters using event cameras. These sensors trigger pixel information due to changes of illumination in the scene such as those produced by dynamic objects. The method performs event-by-event processing in low-cost hardware such as those onboard small aerial vehicles. The proposed scheme detects obstacles and evaluates possible collisions with the robot body. The onboard controller actuates over the horizontal and vertical tail deflections to execute the avoidance maneuver. The scheme is validated in both indoor and outdoor scenarios using obstacles of different shapes and sizes. To the best of the authors’ knowledge, this is the first event-based method for dynamic obstacle avoidance in a flapping-wing robot.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Rodríguez-Gómez, Juan Pablo and Tapia, Raul and Garcia, Maria del Mar Guzmán and Dios, Jose Ramiro Martínez-de and Ollero, Anibal},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Collision avoidance, Dynamics, Robot vision systems, Robots, Sensors, Vehicle dynamics, aerial systems: perception and autonomy, event camera, flapping-wing robot, ornithopter},
	pages = {5413--5420},
}

@inproceedings{eguiluz_why_2021,
	title = {Why fly blind? {Event}-based visual guidance for ornithopter robot flight},
	shorttitle = {Why fly blind?},
	doi = {10.1109/IROS51168.2021.9636315},
	abstract = {The development of perception and control methods that allow bird-scale flapping-wing robots (a.k.a. ornithopters) to perform autonomously is an under-researched area. This paper presents a fully onboard event-based method for ornithopter robot visual guidance. The method uses event cameras to exploit their fast response and robustness against motion blur in order to feed the ornithopter control loop at high rates (100 Hz). The proposed scheme visually guides the robot using line features extracted in the event image plane and controls the flight by actuating over the horizontal and vertical tail deflections. It has been validated on board a real ornithopter robot with real-time computation in low-cost hardware. The experimental evaluation includes sets of experiments with different maneuvers indoors and outdoors.},
	booktitle = {2021 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Eguíluz, A. Gómez and Rodríguez-Gómez, J.P. and Tapia, R. and Maldonado, F.J. and Acosta, J.Á. and Martínez-de Dios, J.R. and Ollero, A.},
	month = sep,
	year = {2021},
	note = {ISSN: 2153-0866},
	keywords = {Cameras, Feature extraction, Hardware, Real-time systems, Robot vision systems, Robustness, Visualization, computer vision, event camera, flapping-wing, line features, ornithopter, tracking, visual servoing},
	pages = {1958--1965},
}

@article{rodriguez-gomez_griffin_2021,
	title = {The {GRIFFIN} {Perception} {Dataset}: {Bridging} the {Gap} {Between} {Flapping}-{Wing} {Flight} and {Robotic} {Perception}},
	volume = {6},
	issn = {2377-3766},
	shorttitle = {The {GRIFFIN} {Perception} {Dataset}},
	doi = {10.1109/LRA.2021.3056348},
	abstract = {The development of automatic perception systems and techniques for bio-inspired flapping-wing robots is severely hampered by the high technical complexity of these platforms and the installation of onboard sensors and electronics. Besides, flapping-wing robot perception suffers from high vibration levels and abrupt movements during flight, which cause motion blur and strong changes in lighting conditions. This letter presents a perception dataset for bird-scale flapping-wing robots as a tool to help alleviate the aforementioned problems. The presented data include measurements from onboard sensors widely used in aerial robotics and suitable to deal with the perception challenges of flapping-wing robots, such as an event camera, a conventional camera, and two Inertial Measurement Units (IMUs), as well as ground truth measurements from a laser tracker or a motion capture system. A total of 21 datasets of different types of flights were collected in three different scenarios (one indoor and two outdoor). To the best of the authors' knowledge this is the first dataset for flapping-wing robot perception.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Rodríguez-Gómez, Juan Pablo and Tapia, Raul and Paneque, Julio L. and Grau, Pedro and Gómez Eguíluz, Augusto and Martínez-de Dios, Jose Ramiro and Ollero, Anibal},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Data sets for robotic vision, Drones, Payloads, Robot sensing systems, Robot vision systems, Robots, Sensors, aerial systems: Perception and autonomy, event-based cameras, flapping-wing robots, vision-based navigation},
	pages = {1066--1073},
}

@article{breuer_flight_2019,
	title = {Flight of the {RoboBee}},
	volume = {570},
	copyright = {2021 Nature},
	url = {https://www.nature.com/articles/d41586-019-01964-3},
	doi = {10.1038/d41586-019-01964-3},
	abstract = {The sustained flight of an untethered insect-sized robot.},
	language = {en},
	number = {7762},
	urldate = {2022-07-10},
	journal = {Nature},
	author = {Breuer, Kenny},
	month = jun,
	year = {2019},
	note = {Bandiera\_abtest: a
Cg\_type: News And Views
Number: 7762
Publisher: Nature Publishing Group
Subject\_term: Engineering, Applied physics, Technology},
	keywords = {Applied physics, Engineering, Technology},
	pages = {448--449},
}

@article{chukewad_robofly_2021,
	title = {{RoboFly}: {An} {Insect}-{Sized} {Robot} {With} {Simplified} {Fabrication} {That} {Is} {Capable} of {Flight}, {Ground}, and {Water} {Surface} {Locomotion}},
	volume = {37},
	issn = {1941-0468},
	shorttitle = {{RoboFly}},
	doi = {10.1109/TRO.2021.3075374},
	abstract = {Insect-sized (\${\textbackslash}sim\$100 mg) aerial robots have advantages over larger robots because of their small size, low weight, and low materials cost. Previous iterations have demonstrated controlled flight but were difficult to fabricate because they consisted of many separate parts assembled together and were also unable to perform locomotion modes besides flight. This article presents a new design of a 74-mg flapping-wing robot that dramatically reduces the number of parts and simplifies fabrication. The robot also has a lower center of mass, which allows the robot to additionally land without the need for long legs, even in case of unstable flight. We also show that the new design allows for wing-driven ground and air–water interfacial locomotion, improving the versatility of the robot. During surface ambulation, forward thrust is generated by increasing the speed of the upstroke relative to the downstroke of the flapping wings. Adjusting relative wing stroke amplitudes also allows for steering. The ability to land and subsequently move along the ground first presented here allows the robot to negotiate extremely confined spaces and underneath obstacles. We present results demonstrating these capabilities, as well as hovering flight and controlled landing.},
	number = {6},
	journal = {IEEE Transactions on Robotics},
	author = {Chukewad, Yogesh M. and James, Johannes and Singh, Avinash and Fuller, Sawyer},
	month = dec,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Aerial systems, Bio-inspired engineering, Insects, Legged locomotion, Microfabrication, air–water interfacial locomotion, ground locomotion, insect-scale flapping-wing robot, mechanics and control, microfabrication},
	pages = {2025--2040},
}

@article{pornsin-sirirak_microbat_nodate,
	title = {Microbat: {A} {Palm}-{Sized} {Electrically} {Powered} {Ornithopter}},
	abstract = {This paper reports the successful development of “Microbat,” the first electrically powered palm-sized ornithopter. This first prototype was flown for 9 seconds in October 1998. It was powered by two 1-farad super capacitors. Due to the rapid discharge of the capacitor power source, the flight duration was limited. To achieve a longer flight, a rechargeable battery as a power source is preferred. The second prototype houses a small 3-gram rechargeable Ni-Cad battery. The best flight performance for this prototype lasted 22 seconds. The latest and current prototype is radio-controlled and is capable of turning left or right, pitching up or down. It weighs approximately 12.5 grams. So far, the best flight duration achieved is 42 seconds. The paper also discusses the study of flapping-wing flight in the wind tunnel using wings developed by MEMS technology. This enables a better understanding the key elements in developing efficient wings to achieve aerodynamic advantage in flapping-wing flight.},
	language = {en},
	author = {Pornsin-Sirirak, T Nick and Tai, Yu-Chong and Ho, Chih-Ming and Keennon, Matt},
	pages = {13},
}

@article{harvey_birds_2022,
	title = {Birds can transition between stable and unstable states via wing morphing},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-022-04477-8},
	doi = {10.1038/s41586-022-04477-8},
	abstract = {Abstract
            
              Birds morph their wing shape to accomplish extraordinary manoeuvres
              1–4
              , which are governed by avian-specific equations of motion. Solving these equations requires information about a bird’s aerodynamic and inertial characteristics
              5
              . Avian flight research to date has focused on resolving aerodynamic features, whereas inertial properties including centre of gravity and moment of inertia are seldom addressed. Here we use an analytical method to determine the inertial characteristics of 22 species across the full range of elbow and wrist flexion and extension. We find that wing morphing allows birds to substantially change their roll and yaw inertia but has a minimal effect on the position of the centre of gravity. With the addition of inertial characteristics, we derived a novel metric of pitch agility and estimated the static pitch stability, revealing that the agility and static margin ranges are reduced as body mass increases. These results provide quantitative evidence that evolution selects for both stable and unstable flight, in contrast to the prevailing narrative that birds are evolving away from stability
              6
              . This comprehensive analysis of avian inertial characteristics provides the key features required to establish a theoretical model of avian manoeuvrability.},
	language = {en},
	urldate = {2022-03-19},
	journal = {Nature},
	author = {Harvey, C. and Baliga, V. B. and Wong, J. C. M. and Altshuler, D. L. and Inman, D. J.},
	month = mar,
	year = {2022},
}

@misc{wisth_vilens_2022,
	title = {{VILENS}: {Visual}, {Inertial}, {Lidar}, and {Leg} {Odometry} for {All}-{Terrain} {Legged} {Robots}},
	shorttitle = {{VILENS}},
	url = {http://arxiv.org/abs/2107.07243},
	doi = {10.48550/arXiv.2107.07243},
	abstract = {We present VILENS (Visual Inertial Lidar Legged Navigation System), an odometry system for legged robots based on factor graphs. The key novelty is the tight fusion of four different sensor modalities to achieve reliable operation when the individual sensors would otherwise produce degenerate estimation. To minimize leg odometry drift, we extend the robot's state with a linear velocity bias term which is estimated online. This bias is observable because of the tight fusion of this preintegrated velocity factor with vision, lidar, and IMU factors. Extensive experimental validation on different ANYmal quadruped robots is presented, for a total duration of 2 h and 1.8 km traveled. The experiments involved dynamic locomotion over loose rocks, slopes, and mud which caused challenges like slippage and terrain deformation. Perceptual challenges included dark and dusty underground caverns, and open and feature-deprived areas. We show an average improvement of 62\% translational and 51\% rotational errors compared to a state-of-the-art loosely coupled approach. To demonstrate its robustness, VILENS was also integrated with a perceptive controller and a local path planner.},
	urldate = {2022-07-02},
	publisher = {arXiv},
	author = {Wisth, David and Camurri, Marco and Fallon, Maurice},
	month = feb,
	year = {2022},
	note = {arXiv:2107.07243 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@article{paz_cfd_2020,
	title = {{CFD} analysis of the aerodynamic effects on the stability of the flight of a quadcopter {UAV} in the proximity of walls and ground},
	volume = {206},
	issn = {01676105},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167610520302889},
	doi = {10.1016/j.jweia.2020.104378},
	abstract = {Quadcopters are attracting a growing interest in many applications such as cargo delivery or surfaces inspection. These applications are often subjected to ﬂights in the proximity of walls or ground that generate external forces and torques on the vehicle due to aerodynamic effects, because of what strong safety guarantees are required. In this research a methodology based on dynamic meshes was developed and applied to computational simulations to reproduce the ﬂight of the drone over an obstacle. Thus, the effect of the ground proximity on the drone performance was assessed, and also its combination with the ﬂow around the body at different translational velocities. The results shown a decrease of the drag force, and an increase of the lift and forward pitch moment due to the presence of the ground. These effects are magniﬁed by the translational velocity, which also deviates the ﬂow generated by the propellers and delays the interaction with the obstacle. Both in the approaching and leaving to the obstacle, increases of up to 60\% in the pitch moment are observed. This sudden variations must be properly counteracted to guarantee the stability and safety of the drone operation.},
	language = {en},
	urldate = {2022-07-01},
	journal = {Journal of Wind Engineering and Industrial Aerodynamics},
	author = {Paz, C. and Suárez, E. and Gil, C. and Baker, C.},
	month = nov,
	year = {2020},
	pages = {104378},
}

@misc{kaufmann_benchmark_2022,
	title = {A {Benchmark} {Comparison} of {Learned} {Control} {Policies} for {Agile} {Quadrotor} {Flight}},
	url = {http://arxiv.org/abs/2202.10796},
	doi = {10.48550/arXiv.2202.10796},
	abstract = {Quadrotors are highly nonlinear dynamical systems that require carefully tuned controllers to be pushed to their physical limits. Recently, learning-based control policies have been proposed for quadrotors, as they would potentially allow learning direct mappings from high-dimensional raw sensory observations to actions. Due to sample inefficiency, training such learned controllers on the real platform is impractical or even impossible. Training in simulation is attractive but requires to transfer policies between domains, which demands trained policies to be robust to such domain gap. In this work, we make two contributions: (i) we perform the first benchmark comparison of existing learned control policies for agile quadrotor flight and show that training a control policy that commands body-rates and thrust results in more robust sim-to-real transfer compared to a policy that directly specifies individual rotor thrusts, (ii) we demonstrate for the first time that such a control policy trained via deep reinforcement learning can control a quadrotor in real-world experiments at speeds over 45km/h.},
	urldate = {2022-06-30},
	publisher = {arXiv},
	author = {Kaufmann, Elia and Bauersfeld, Leonard and Scaramuzza, Davide},
	month = feb,
	year = {2022},
	note = {arXiv:2202.10796 [cs]},
	keywords = {Computer Science - Robotics},
}

@misc{kahn_badgr_2020,
	title = {{BADGR}: {An} {Autonomous} {Self}-{Supervised} {Learning}-{Based} {Navigation} {System}},
	shorttitle = {{BADGR}},
	url = {http://arxiv.org/abs/2002.05700},
	abstract = {Mobile robot navigation is typically regarded as a geometric problem, in which the robot's objective is to perceive the geometry of the environment in order to plan collision-free paths towards a desired goal. However, a purely geometric view of the world can can be insufficient for many navigation problems. For example, a robot navigating based on geometry may avoid a field of tall grass because it believes it is untraversable, and will therefore fail to reach its desired goal. In this work, we investigate how to move beyond these purely geometric-based approaches using a method that learns about physical navigational affordances from experience. Our approach, which we call BADGR, is an end-to-end learning-based mobile robot navigation system that can be trained with self-supervised off-policy data gathered in real-world environments, without any simulation or human supervision. BADGR can navigate in real-world urban and off-road environments with geometrically distracting obstacles. It can also incorporate terrain preferences, generalize to novel environments, and continue to improve autonomously by gathering more data. Videos, code, and other supplemental material are available on our website https://sites.google.com/view/badgr},
	urldate = {2022-06-26},
	publisher = {arXiv},
	author = {Kahn, Gregory and Abbeel, Pieter and Levine, Sergey},
	month = apr,
	year = {2020},
	note = {Number: arXiv:2002.05700
arXiv:2002.05700 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{kolvenbach_towards_2019,
	title = {Towards {Jumping} {Locomotion} for {Quadruped} {Robots} on the {Moon}},
	doi = {10.1109/IROS40897.2019.8967552},
	abstract = {Jumping locomotion has the potential to enable legged robots to overcome obstacles and travel efficiently on low-gravity celestial bodies. We present how the 22 kg quadruped robot SpaceBok exploits lunar gravity conditions to perform energy-efficient jumps. The robot achieves repetitive, vertical jumps of more than 0.9m meter and powerful single leaps of up to 1.3m. We present the implementation of a reaction wheel, which allows for control of the robots pitch orientation during the flight phase. We also demonstrate the implementation of a parallel elasticity in the legs providing the capability of temporarily storing and reusing energy during jumping. The jumping and attitude controller are subsequently presented. Finally, we analyze the energetics of the system and show that jumping with the integrated elasticity significantly reduces energy consumption compared to non-elastic jumps.},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Kolvenbach, Hendrik and Hampp, Elias and Barton, Patrick and Zenkl, Radek and Hutter, Marco},
	month = nov,
	year = {2019},
	note = {ISSN: 2153-0866},
	pages = {5459--5466},
}

@phdthesis{kolvenbach_quadrupedal_2021,
	title = {Quadrupedal {Robots} for {Planetary} {Exploration}},
	abstract = {So far, mobile, robotic exploration of celestial bodies has been performed solely with wheeled systems, making access to highly unstructured, compressible, and sloped areas very challenging. Additionally, the systems have to account for unknowns, such as unclear soil properties or the environment in uncharted areas such as caves. Scientists, however, have identified those challenging places as potential targets for future missions. Meanwhile, versatile, terrestrial walking robots have advanced rapidly over the last decade and have reached a maturity level where the commercial application becomes viable. Thus, it is only a matter of time until walking robots will be used to explore so far unreachable areas in our solar system. Nevertheless, it is still unclear which underlying technologies need to be adapted and further developed for space and how such a robot would look like. In this thesis, we investigate legged systems in the context of space exploration. We were particularly interested in increasing and validating the system performance in partially unknown, sandy, sloped, and lower-than-earth gravity settings. We analyze the state-of-the-art of legged robots and the scalability of underlying technologies in view of future missions and carried out initial simulation studies and field trials. The studies revealed that dynamic walking gaits, especially those with extended flight phases, become increasingly efficient in low-gravity. Although the tests also showed that large contact forces lead to deep sinkage and ultimately increased the risk of falling. We introduce the quadruped robot SpaceBok and respective hardware modifications to perform efficient, repetitive, low-gravity jumping. To this end, we installed a reaction wheel that allows for stabilization during the extensive flight phase and parallel elasticities in the legs to increase energy efficiency. To tackle the sand-walking challenge, we developed novel passive-adaptive planar feet, which reduced sinkage and increase traction on highly compressible, dry granular media. Together with a terrain-adapting walking controller, we demonstrate the traversal of steep, representative Martian analog slopes. We evaluate the robot's performance in terms of safety and efficiency and propose effective strategies for overcoming slopes. Lastly, we present an approach to perform a tactile inspection of the terrain in front of the robot using one of the limbs. This inspection-before-traversal increases the certainty about the terrain type and thus the safety of the system. We spun off this approach to the terrestrial application of concrete inspection in sewers. Here, we performed semi-autonomous missions in a subterranean environment where a tactile inspection motion was carried out every few meters to assess the state of deterioration. The technology developments presented in this thesis have been thoroughly tested and validated on two advanced quadrupedal robots, namely ANYmal and SpaceBok.},
	author = {Kolvenbach, Hendrik},
	month = jun,
	year = {2021},
	doi = {10.3929/ethz-b-000489008},
}

@inproceedings{britto_model_2015,
	title = {Model identification of an unmanned underwater vehicle via an adaptive technique and artificial fiducial markers},
	doi = {10.23919/OCEANS.2015.7404391},
	abstract = {To control any kind of unmanned underwater vehicle (UUV) it is important to know the dynamic model of the system. Creating the model itself is only the first step, the second is identifying the model's parameters. Due to the nonlinear and coupled characteristics of UUVs this is a complex and operationally demanding task. Several methods to identify the model parameters have been published in recent years. One of the biggest challenges is providing a data set of the vehicle's true motion, which has not been solved completely. This paper presents and evaluates a new method to identify the motion model parameters of a UUV. The method uses the UUV's pose estimation, the on-board cameras, fiducial markers and an adaptive method to combine the resulting data. The new method is used to parameterize the model of the FlatFish AUV. The paper closes with a comparison of the new method for model identification and the traditional least-squares method using onboard sensors.},
	booktitle = {{OCEANS} 2015 - {MTS}/{IEEE} {Washington}},
	author = {Britto, João and Cesar, Diego and Saback, Rafael and Arnold, Sascha and Gaudig, Christopher and Albiez, Jan},
	month = oct,
	year = {2015},
	keywords = {Adaptation models, Cameras, Computational modeling, Robots, Sensors, Vehicles},
	pages = {1--6},
}

@inproceedings{nissler_robot--camera_2017,
	title = {Robot-to-{Camera} {Calibration}: {A} {Generic} {Approach} {Using} {6D} {Detections}},
	shorttitle = {Robot-to-{Camera} {Calibration}},
	doi = {10.1109/IRC.2017.66},
	abstract = {For vision controlled manipulation with a robotic arm, knowledge of the camera to end effector transformation is important. This is typically done by classical hand-eye calibration, where a transformation is estimated by observing a known calibration object under different robot poses. The same holds for mobile bases as well, where the mounted cameras' poses with respect to the robot need to be known. However, there are cases where this robot/hand-eye calibration is not possible, for example because of limited movability or reduced precision of the robot. We want to present a new calibration method, which allows for a precise calibration also in these situations. This is achieved by rotating the robot around one axis and sampling a high number of visual features, in our case AprilTag markers. By fitting a Bingham distribution to the sampled markers around several configurations of the robot and determining the rotation center and axis, a mathematical model connecting the kinematics of the robot to the camera frame of reference can be found. We show that this method can be applied to cases where a classical hand-eye calibration fails, e.g. a robot base with limited movability. Our method allows an estimation of the camera to robot transformation even under those challenging conditions, e.g. where no part of the robot is visible from the camera.},
	booktitle = {2017 {First} {IEEE} {International} {Conference} on {Robotic} {Computing} ({IRC})},
	author = {Nissler, Christian and Marton, Zoltan-Csaba},
	month = apr,
	year = {2017},
	keywords = {AprilTags, Bingham distribution, Calibration, Cameras, Estimation, Manipulators, Robot vision systems, calibration, hand-eye calibration},
	pages = {299--302},
}

@article{abbas_analysis_2019,
	title = {Analysis and {Improvements} in {AprilTag} {Based} {State} {Estimation}},
	volume = {19},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/19/24/5480},
	doi = {10.3390/s19245480},
	abstract = {In this paper, we analyzed the accuracy and precision of AprilTag as a visual fiducial marker in detail. We have analyzed error propagation along two horizontal axes along with the effect of angular rotation about the vertical axis. We have identified that the angular rotation of the camera (yaw angle) about its vertical axis is the primary source of error that decreases the precision to the point where the marker system is not potentially viable for sub-decimeter precise tasks. Other factors are the distance and viewing angle of the camera from the AprilTag. Based on these observations, three improvement steps have been proposed. One is the trigonometric correction of the yaw angle to point the camera towards the center of the tag. Second, the use of a custom-built yaw-axis gimbal, which tracks the center of the tag in real-time. Third, we have presented for the first time a pose-indexed probabilistic sensor error model of the AprilTag using a Gaussian Processes based regression of experimental data, validated by particle filter tracking. Our proposed approach, which can be deployed with all three improvement steps, increases the system’s overall accuracy and precision by manifolds with a slight trade-off with execution time over commonly available AprilTag library. These proposed improvements make AprilTag suitable to be used as precision localization systems for outdoor and indoor applications.},
	language = {en},
	number = {24},
	urldate = {2022-06-23},
	journal = {Sensors},
	author = {Abbas, Syed Muhammad and Aslam, Salman and Berns, Karsten and Muhammad, Abubakr},
	month = jan,
	year = {2019},
	note = {Number: 24
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {localization, robot sensing and perception, sensor modelling},
	pages = {5480},
}

@article{sun_autonomous_2021,
	title = {Autonomous {Quadrotor} {Flight} {Despite} {Rotor} {Failure} {With} {Onboard} {Vision} {Sensors}: {Frames} vs. {Events}},
	volume = {6},
	issn = {2377-3766},
	shorttitle = {Autonomous {Quadrotor} {Flight} {Despite} {Rotor} {Failure} {With} {Onboard} {Vision} {Sensors}},
	doi = {10.1109/LRA.2020.3048875},
	abstract = {Fault-tolerant control is crucial for safety-critical systems, such as quadrotors. State-of-art flight controllers can stabilize and control a quadrotor even when subjected to the complete loss of a rotor. However, these methods rely on external sensors, such as GPS or motion capture systems, for state estimation. To the best of our knowledge, this has not yet been achieved with only onboard sensors. In this letter, we propose the first algorithm that combines fault-tolerant control and onboard vision-based state estimation to achieve position control of a quadrotor subjected to complete failure of one rotor. Experimental validations show that our approach is able to accurately control the position of a quadrotor during a motor failure scenario, without the aid of any external sensors. The primary challenge to vision-based state estimation stems from the inevitable high-speed yaw rotation (over 20 rd/s) of the damaged quadrotor, causing motion blur to cameras, which is detrimental to visual inertial odometry (VIO). We compare two types of visual inputs to the vision-based state estimation algorithm: standard frames and events. Experimental results show the advantage of using an event camera especially in low light environments due to its inherent high dynamic range and high temporal resolution. We believe that our approach will render autonomous quadrotors safer in both GPS denied or degraded environments. We release both our controller and VIO algorithm open source. The source code of both our controller and VIO algorithm is available at: https://github.com/uzh-rpg/fault\_tolerant\_control. A video of the experiments is available at: https://youtu.be/Ww8u0KH7Ugs.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Sun, Sihao and Cioffi, Giovanni and de Visser, Coen and Scaramuzza, Davide},
	month = apr,
	year = {2021},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Aerial systems: perception and autonomy, Aerodynamics, Cameras, Heuristic algorithms, Rotors, Sensors, Standards, State estimation, event camera, robot safety, sensor-based control},
	pages = {580--587},
}

@article{loquercio_learning_2021,
	title = {Learning high-speed flight in the wild},
	volume = {6},
	issn = {2470-9476},
	url = {https://www.science.org/doi/10.1126/scirobotics.abg5810},
	doi = {10.1126/scirobotics.abg5810},
	abstract = {Deep Learning enables agile flight in challenging environments with onboard sensing and computation.
          , 
            Quadrotors are agile. Unlike most other machines, they can traverse extremely complex environments at high speeds. To date, only expert human pilots have been able to fully exploit their capabilities. Autonomous operation with onboard sensing and computation has been limited to low speeds. State-of-the-art methods generally separate the navigation problem into subtasks: sensing, mapping, and planning. Although this approach has proven successful at low speeds, the separation it builds upon can be problematic for high-speed navigation in cluttered environments. The subtasks are executed sequentially, leading to increased processing latency and a compounding of errors through the pipeline. Here, we propose an end-to-end approach that can autonomously fly quadrotors through complex natural and human-made environments at high speeds with purely onboard sensing and computation. The key principle is to directly map noisy sensory observations to collision-free trajectories in a receding-horizon fashion. This direct mapping drastically reduces processing latency and increases robustness to noisy and incomplete perception. The sensorimotor mapping is performed by a convolutional network that is trained exclusively in simulation via privileged learning: imitating an expert with access to privileged information. By simulating realistic sensor noise, our approach achieves zero-shot transfer from simulation to challenging real-world environments that were never experienced during training: dense forests, snow-covered terrain, derailed trains, and collapsed buildings. Our work demonstrates that end-to-end policies trained in simulation enable high-speed autonomous flight through challenging environments, outperforming traditional obstacle avoidance pipelines.},
	language = {en},
	number = {59},
	urldate = {2022-06-22},
	journal = {Science Robotics},
	author = {Loquercio, Antonio and Kaufmann, Elia and Ranftl, René and Müller, Matthias and Koltun, Vladlen and Scaramuzza, Davide},
	month = oct,
	year = {2021},
	pages = {eabg5810},
}

@article{loquercio_autotune_2022,
	title = {{AutoTune}: {Controller} {Tuning} for {High}-{Speed} {Flight}},
	volume = {7},
	issn = {2377-3766},
	shorttitle = {{AutoTune}},
	doi = {10.1109/LRA.2022.3146897},
	abstract = {Due to noisy actuation and external disturbances, tuning controllers for high-speed flight is very challenging. In this letter, we ask the following questions: How sensitive are controllers to tuning when tracking high-speed maneuvers? What algorithms can we use to automatically tune them? To answer the first question, we study the relationship between parameters and performance and find out that the faster the maneuver, the more sensitive a controller becomes to its parameters. To answer the second question, we review existing methods for controller tuning and discover that prior works often perform poorly on the task of high-speed flight. Therefore, we propose AutoTune, a sampling-based tuning algorithm specifically tailored to high-speed flight. In contrast to previous work, our algorithm does not assume any prior knowledge of the drone or its optimization function and can deal with the multi-modal characteristics of the parameters’ optimization space. We thoroughly evaluate AutoTune both in simulation and in the physical world. In our experiments, we outperform existing tuning algorithms by up to 90\% in trajectory completion. The resulting controllers are tested in the AirSim Game of Drones competition, where we outperform the winner by up to 25\% in lap-time. Finally, we validate AutoTune in real-world flights in one of the world’s largest motion-capture systems. In these experiments, we outperform human experts on the task of parameter tuning for trajectory tracking, achieving flight speeds over \$50 {\textbackslash},{\textbackslash}mathrmk{\textbackslash}mathrmm{\textbackslash}mathrmh{\textasciicircum}-1\$.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Loquercio, Antonio and Saviolo, Alessandro and Scaramuzza, Davide},
	month = apr,
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Heuristic algorithms, Noise measurement, Optimization, Robot learning, Robots, Task analysis, Trajectory, Tuning, unmanned aerial vehicles},
	pages = {4432--4439},
}

@inproceedings{kim_vision_2020,
	title = {Vision {Aided} {Dynamic} {Exploration} of {Unstructured} {Terrain} with a {Small}-{Scale} {Quadruped} {Robot}},
	doi = {10.1109/ICRA40945.2020.9196777},
	abstract = {Legged robots have been highlighted as promising mobile platforms for disaster response and rescue scenarios because of their rough terrain locomotion capability. In cluttered environments, small robots are desirable as they can maneuver through small gaps, narrow paths, or tunnels. However small robots have their own set of difficulties such as limited space for sensors, limited obstacle clearance, and scaled-down walking speed. In this paper, we extensively address these difficulties via effective sensor integration and exploitation of dynamic locomotion and jumping. We integrate two Intel RealSense sensors into the MIT Mini-Cheetah, a 0.3 m tall, 9 kg quadruped robot. Simple and effective filtering and evaluation algorithms are used for foothold adjustment and obstacle avoidance. We showcase the exploration of highly irregular terrain using dynamic trotting and jumping with the small-scale, fully sensorized Mini-Cheetah quadruped robot.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Kim, D. and Carballo, D. and Di Carlo, J. and Katz, B. and Bledt, G. and Lim, B. and Kim, S.},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Cameras, Legged locomotion, Robot kinematics, Robot vision systems},
	pages = {2464--2470},
}

@misc{margolis_learning_2021,
	title = {Learning to {Jump} from {Pixels}},
	url = {http://arxiv.org/abs/2110.15344},
	doi = {10.48550/arXiv.2110.15344},
	abstract = {Today's robotic quadruped systems can robustly walk over a diverse range of rough but continuous terrains, where the terrain elevation varies gradually. Locomotion on discontinuous terrains, such as those with gaps or obstacles, presents a complementary set of challenges. In discontinuous settings, it becomes necessary to plan ahead using visual inputs and to execute agile behaviors beyond robust walking, such as jumps. Such dynamic motion results in significant motion of onboard sensors, which introduces a new set of challenges for real-time visual processing. The requirement for agility and terrain awareness in this setting reinforces the need for robust control. We present Depth-based Impulse Control (DIC), a method for synthesizing highly agile visually-guided locomotion behaviors. DIC affords the flexibility of model-free learning but regularizes behavior through explicit model-based optimization of ground reaction forces. We evaluate the proposed method both in simulation and in the real world.},
	urldate = {2022-06-16},
	publisher = {arXiv},
	author = {Margolis, Gabriel B. and Chen, Tao and Paigwar, Kartik and Fu, Xiang and Kim, Donghyun and Kim, Sangbae and Agrawal, Pulkit},
	month = oct,
	year = {2021},
	note = {Number: arXiv:2110.15344
arXiv:2110.15344 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{kocer_aerial_2019,
	title = {Aerial {Robot} {Control} in {Close} {Proximity} to {Ceiling}: {A} {Force} {Estimation}-based {Nonlinear} {MPC}},
	shorttitle = {Aerial {Robot} {Control} in {Close} {Proximity} to {Ceiling}},
	doi = {10.1109/IROS40897.2019.8967611},
	abstract = {Being motivated by ceiling inspection applications via unmanned aerial vehicles (UAVs) which require close proximity flight to surfaces, a systematic control approach enabling safe and accurate close proximity flight is proposed in this work. There are two main challenges for close proximity flights: (i) the trust characteristics varies drastically for the different distance from the ceiling which results in a complex nonlinear dynamics; (ii) the system needs to consider physical and environmental constraints to safely fly in close proximity. To address these challenges, a novel framework consisting of a constrained optimization-based force estimation and an optimization-based nonlinear controller is proposed. Experimental results illustrate that the performance of the proposed control approach can stabilize UAV down to 1 cm distance to the ceiling. Furthermore, we report that the UAV consumes up to 12.5\% less power when it is operated 1 cm distance to ceiling, which is promising potential for more battery-efficient inspection flights.},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Kocer, Basaran Bahadir and Tiryaki, Mehmet Efe and Pratama, Mahardhika and Tjahjowidodo, Tegoeh and Seet, Gerald Gim Lee},
	month = nov,
	year = {2019},
	note = {ISSN: 2153-0866},
	pages = {2813--2819},
}

@article{loquercio_64-mw_2019,
	title = {A 64-{mW} {DNN}-{Based} {Visual} {Navigation} {Engine} for {Autonomous} {Nano}-{Drones}},
	volume = {6},
	issn = {2327-4662},
	doi = {10.1109/JIOT.2019.2917066},
	abstract = {Fully miniaturized robots (e.g., drones), with artificial intelligence (AI)-based visual navigation capabilities, are extremely challenging drivers of Internet-of-Things edge intelligence capabilities. Visual navigation based on AI approaches, such as deep neural networks (DNNs) are becoming pervasive for standard-size drones, but are considered out of reach for nano-drones with a size of a few cm2. In this paper, we present the first (to the best of our knowledge) demonstration of a navigation engine for autonomous nano-drones capable of closed-loop end-to-end DNN-based visual navigation. To achieve this goal we developed a complete methodology for parallel execution of complex DNNs directly on board resource-constrained milliwatt-scale nodes. Our system is based on GAP8, a novel parallel ultralow-power computing platform, and a 27-g commercial, open-source Crazyflie 2.0 nano-quadrotor. As part of our general methodology, we discuss the software mapping techniques that enable the DroNet state-of-the-art deep convolutional neural network to be fully executed aboard within a strict 6 frame-per-second real-time constraint with no compromise in terms of flight results, while all processing is done with only 64 mW on average. Our navigation engine is flexible and can be used to span a wide performance range: at its peak performance corner, it achieves 18 frames/s while still consuming on average just 3.5\% of the power envelope of the deployed nano-aircraft. To share our key findings with the embedded and robotics communities and foster further developments in autonomous nano-unmanned aerial vehicles (UAVs), we publicly release all our code, datasets, and trained networks.},
	number = {5},
	journal = {IEEE Internet of Things Journal},
	author = {Loquercio, Antonio and Conti, Francesco and Flamand, Eric and Scaramuzza, Davide and Benini, Luca},
	month = oct,
	year = {2019},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Autonomous UAV, CNNs, Drones, Engines, Internet of Things, Navigation, Robot sensing systems, Visualization, end-to-end learning, nano-UAV, ultralow-power},
	pages = {8357--8371},
}

@techreport{ghanem_efficient_2021-1,
	title = {Efficient {Modeling} of {Morphing} {Wing} {Flight} {Using} {Neural} {Networks} and {Cubature} {Rules}},
	url = {http://arxiv.org/abs/2110.01057},
	abstract = {Fluidic locomotion of flapping Micro Aerial Vehicles (MAVs) can be very complex, particularly when the rules from insect flight dynamics (fast flapping dynamics and light wings) are not applicable. In these situations, widely used averaging techniques can fail quickly. The primary motivation is to find efficient models for complex forms of aerial locomotion where wings constitute a large part of body mass (i.e., dominant inertial effects) and deform in multiple directions (i.e., morphing wing). In these systems, high degrees of freedom yields complex inertial, Coriolis, and gravity terms. We use Algorithmic Differentiation (AD) and Bayesian filters computed with cubature rules conjointly to quickly estimate complex fluid-structure interactions. In general, Bayesian filters involve finding complex numerical integration (e.g., find posterior integrals). Using cubature rules to compute Gaussian-weighted integrals and AD, we show that the complex multi-degrees-of-freedom dynamics of morphing MAVs can be computed very efficiently and accurately. Therefore, our work facilitates closed-loop feedback control of these morphing MAVs.},
	number = {arXiv:2110.01057},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Ghanem, Paul and Bicer, Yunus and Erdogmus, Deniz and Ramezani, Alireza},
	month = oct,
	year = {2021},
	doi = {10.48550/arXiv.2110.01057},
	note = {arXiv:2110.01057 [cs, eess]
type: article},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@techreport{agrawal_vision-aided_2022,
	title = {Vision-aided {Dynamic} {Quadrupedal} {Locomotion} on {Discrete} {Terrain} using {Motion} {Libraries}},
	url = {http://arxiv.org/abs/2110.00891},
	abstract = {In this paper, we present a framework rooted in control and planning that enables quadrupedal robots to traverse challenging terrains with discrete footholds using visual feedback. Navigating discrete terrain is challenging for quadrupeds because the motion of the robot can be aperiodic, highly dynamic, and blind for the hind legs of the robot. Additionally, the robot needs to reason over both the feasible footholds as well as robot velocity by speeding up and slowing down at different parts of the terrain. We build an offline library of periodic gaits which span two trotting steps on the robot, and switch between different motion primitives to achieve aperiodic motions of different step lengths on an A1 robot. The motion library is used to provide targets to a geometric model predictive controller which controls stance. To incorporate visual feedback, we use terrain mapping tools to build a local height map of the terrain around the robot using RGB and depth cameras, and extract feasible foothold locations around both the front and hind legs of the robot. Our experiments show a Unitree A1 robot navigating multiple unknown, challenging and discrete terrains in the real world.},
	number = {arXiv:2110.00891},
	urldate = {2022-06-02},
	institution = {arXiv},
	author = {Agrawal, Ayush and Chen, Shuxiao and Rai, Akshara and Sreenath, Koushil},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2110.00891},
	note = {arXiv:2110.00891 [cs, eess, math]
type: article},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
}

@article{tranzatto_cerberus_2022,
	title = {{CERBERUS} in the {DARPA} {Subterranean} {Challenge}},
	volume = {7},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abp9742},
	doi = {10.1126/scirobotics.abp9742},
	number = {66},
	urldate = {2022-05-31},
	journal = {Science Robotics},
	author = {Tranzatto, Marco and Miki, Takahiro and Dharmadhikari, Mihir and Bernreiter, Lukas and Kulkarni, Mihir and Mascarich, Frank and Andersson, Olov and Khattak, Shehryar and Hutter, Marco and Siegwart, Roland and Alexis, Kostas},
	month = may,
	year = {2022},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabp9742},
}

@article{oelsch_ro-loam_2022,
	title = {{RO}-{LOAM}: {3D} {Reference} {Object}-based {Trajectory} and {Map} {Optimization} in {LiDAR} {Odometry} and {Mapping}},
	issn = {2377-3766},
	shorttitle = {{RO}-{LOAM}},
	doi = {10.1109/LRA.2022.3177846},
	abstract = {We propose an extension to the LiDAR Odometry and Mapping framework (LOAM) that enables reference object-based trajectory and map optimization. Our approach assumes that the location and geometry of a large reference object are known, e.g., as a CAD model from Building Information Modeling (BIM) or a previously captured dense point cloud model. We do not expect the reference object to be present in every LiDAR scan. Our approach uses the poses of the LOAM algorithm as an initial guess to refine them with scan-to-model alignment. To evaluate if the alignment was accurate, an EKF-based motion prior filtering step is employed. Subsequently, the past trajectory is optimized by adding the model-aligned pose as a pose graph constraint and the map of the LOAM algorithm is corrected to improve future localization and mapping. We evaluate our approach with data captured in a visual airplane inspection scenario inside an aircraft hangar. A 3D LiDAR sensor is mounted via a gimbal on an Unmanned Aerial Vehicle (UAV) and is continuously actuated. We compare the localization accuracy of the LOAM and R-LOAM algorithms when enabling or disabling our proposed reference object-based trajectory and map optimization extension. For three recorded datasets, enabling the proposed extension yields a reduction in Absolute Pose Error compared to conventional LOAM and R-LOAM, while being able to run online. This reduces drift and improves map quality.},
	journal = {IEEE Robotics and Automation Letters},
	author = {Oelsch, Martin and Karimi, Mojtaba and Steinbach, Eckehard},
	year = {2022},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Laser radar, Localization, Location awareness, Mapping, Optimization, Range Sensing, SLAM, Simultaneous localization and mapping, Solid modeling, Three-dimensional displays, Trajectory},
	pages = {1--1},
}

@inproceedings{ohradzansky_reactive_2020,
	title = {Reactive {Control} and {Metric}-{Topological} {Planning} for {Exploration}},
	doi = {10.1109/ICRA40945.2020.9197381},
	abstract = {Autonomous navigation in unknown environments with the intent of exploring all traversable areas is a significant challenge for robotic platforms. In this paper, a simple yet reliable method for exploring unknown environments is presented based on bio-inspired reactive control and metric-topological planning. The reactive control algorithm is modeled after the spatial decomposition of wide and small-field patterns of optic flow in the insect visuomotor system. Centering behaviour and small obstacle detection and avoidance are achieved through wide-field integration and Fourier residual analysis of instantaneous measured nearness respectively. A topological graph is estimated using image processing techniques on a continuous occupancy grid. Node paths are rapidly generated to navigate to the nearest unexplored edge in the graph. It is shown through rigorous field-testing that the proposed control and planning method is robust, reliable, and computationally efficient.},
	booktitle = {2020 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Ohradzansky, Michael T. and Mills, Andrew B. and Rush, Eugene R. and Riley, Danny G. and Frew, Eric W. and Sean Humbert, J.},
	month = may,
	year = {2020},
	note = {ISSN: 2577-087X},
	keywords = {Harmonic analysis, Mathematical model, Navigation, Optical feedback, Optical imaging, Optical sensors, Planning, centering, control, exploration, mapping},
	pages = {4073--4079},
}

@techreport{li_unconventional_2022,
	title = {Unconventional {Visual} {Sensors} for {Autonomous} {Vehicles}},
	url = {http://arxiv.org/abs/2205.09383},
	abstract = {Autonomous vehicles rely on perception systems to understand their surroundings for further navigation missions. Cameras are essential for perception systems due to the advantages of object detection and recognition provided by modern computer vision algorithms, comparing to other sensors, such as LiDARs and radars. However, limited by its inherent imaging principle, a standard RGB camera may perform poorly in a variety of adverse scenarios, including but not limited to: low illumination, high contrast, bad weather such as fog/rain/snow, etc. Meanwhile, estimating the 3D information from the 2D image detection is generally more difficult when compared to LiDARs or radars. Several new sensing technologies have emerged in recent years to address the limitations of conventional RGB cameras. In this paper, we review the principles of four novel image sensors: infrared cameras, range-gated cameras, polarization cameras, and event cameras. Their comparative advantages, existing or potential applications, and corresponding data processing algorithms are all presented in a systematic manner. We expect that this study will assist practitioners in the autonomous driving society with new perspectives and insights.},
	number = {arXiv:2205.09383},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Li, You and Moreau, Julien and Ibanez-Guzman, Javier},
	month = may,
	year = {2022},
	note = {arXiv:2205.09383 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
}

@phdthesis{tu_onboard_2022,
	type = {thesis},
	title = {Onboard {Sensing}, {Flight} {Control}, and {Navigation} of {A} {Dual}-motor {Hummingbird}-scale {Flapping} {Wing} {Robot}},
	url = {https://hammer.purdue.edu/articles/thesis/Onboard_Sensing_Flight_Control_and_Navigation_of_A_Dual-motor_Hummingbird-scale_Flapping_Wing_Robot/9964106/1},
	abstract = {Insects and hummingbirds not only can perform long-term stationary hovering but also are capable of acrobatic maneuvers. At their body scale, such extraordinary flight performance remains unmatched by state-of-the-art conventional man-made aerial vehicles with fixed or rotary wings. Insects' and hummingbirds' near maximal performance come from their highly intricate and powerful wing-thorax actuation systems, sophisticated sensory system, and precise neuromotor control. Flapping Wing Micro Air Vehicles (FWMAVs) with bio-inspired flapping flight mechanisms hold great promise in matching the performance gap of their natural counterparts. Developing such autonomous flapping-wing vehicles to achieve animal-like flight, however, is challenging. The difficulties are mainly from the high power density requirements under the stringent constraints of scale, weight, and power, severe system oscillations induced by high-frequency wing motion, high nonlinearity of the system, and lack of miniature navigation sensors, which impede actuation system design, onboard sensing, flight control, and autonomous navigation. To address these open issues, in this thesis, we first introduce systematic modeling of a dual-motor hummingbird-scale flapping wing robot. Based upon it, we then present studies of the onboard sensor fusion, flight control, and navigation method. By taking the key inspiration from its natural counterparts, the proposed hummingbird robot has a pair of independently controlled wings. Each wing is directly actuated by a dc motor. Motors undergo reciprocating motion. Such a design is a severely underactuated system, namely, it relies on only two actuators (one per wing) to control full six degrees of freedom body motion. As a bio-inspired design, it also requires the vehicle close to its natural counterparts’ size and weight meanwhile provide sufficient lift and control effort for autonomy. Due to stringent payload limitation from severe underactuation and power efficiency challenges caused by motor reciprocating motion, the design and integration of such a system is a challenging task. In this thesis, we present the detailed modeling, optimization, and system integration of onboard power, actuation, sensing, and flight control to address these unique challenges. As a result, we successfully prototyped such dual-motor powered hummingbird robot, either with power tethers or fully untethered. The tethered platform is used for designing onboard sensing, control, and navigation algorithms. Untethered design tackles system optimization and integration challenges. Both tethered/untethered versions demonstrate sustained stable flight. For onboard attitude sensing, a real-time sensor fusion algorithm is proposed with model-based adaptive compensation for both sensor reading drift and wing motion induced severe system vibration. With accurate and robust sensing results, a nonlinear robust control law is designed to stabilize the system during flight. Stable hovering and waypoint tracking flight were experimentally conducted to demonstrate the control performance. In order to achieve natural flyers' acrobatic maneuverability, we propose a hybrid control scheme by combining a model-based robust controller with a model-free reinforcement learning maneuver policy to perform aggressive maneuvers. The model-based control is responsible for stabilizing the robot in nominal flight scenarios. The reinforcement learning policy pushes the flight envelope to pilot fierce maneuvers. To demonstrate the effectiveness of the proposed control method, we experimentally show animal-like tight flip maneuver on the proposed hummingbird robot, which is actuated by only two DC motors. These successful results show the promise of such a hybrid control design on severely underactuated systems to achieve high-performance flight.In order to navigate confined space while matching the design constraints of such a small robot, we propose to use its wings in dual functions - combining sensing and actuation in one element, which is inspired by animals' multifunctional flapping wings. Based on the interpretation of the motor current feedback which directly indicates wing load changes, the onboard somatosensory-like feedback has been achieved on our hummingbird robot. For navigation purposes, such a method can sense the presence of environmental changes, including grounds, walls, stairs, and obstacles, without the need for any other sensory cues. As long as the robot can fly, it can sense surroundings. To demonstrate this capability, three challenging tasks have been conducted onto the proposed hummingbird robot: terrain following, wall detection and bypass, and navigating a confined corridor. Finally, we integrate the proposed methods into the untethered platform, which enables stable untethered flight of such a design in both indoor and outdoor tests. To the best of our knowledge, this result presents the first bio-inspired FWMAV powered by only two actuators and capable of performing sustained stable flight in both indoor and outdoor environment. It is also the first untethered flight of an at-scale tailless hummingbird robot with independently controlled wings, a key inspiration from its natural counterparts.},
	language = {en},
	urldate = {2022-05-23},
	school = {Purdue University Graduate School},
	author = {Tu, Zhan},
	month = jan,
	year = {2022},
	doi = {10.25394/PGS.9964106.v1},
}

@article{ma_controlled_2013,
	title = {Controlled {Flight} of a {Biologically} {Inspired}, {Insect}-{Scale} {Robot}},
	volume = {340},
	url = {https://www.science.org/doi/full/10.1126/science.1231806},
	doi = {10.1126/science.1231806},
	number = {6132},
	urldate = {2022-05-23},
	journal = {Science},
	author = {Ma, Kevin Y. and Chirarattananon, Pakpong and Fuller, Sawyer B. and Wood, Robert J.},
	month = may,
	year = {2013},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {603--607},
}

@inproceedings{tu_acting_2019,
	title = {Acting {Is} {Seeing}: {Navigating} {Tight} {Space} {Using} {Flapping} {Wings}},
	shorttitle = {Acting {Is} {Seeing}},
	doi = {10.1109/ICRA.2019.8794084},
	abstract = {Wings of flying animals can not only generate lift and control torques but also can sense their surroundings. Such dual functions of sensing and actuation coupled in one element are particularly useful for small sized bio-inspired robotic flyers, whose weight, size, and power are under stringent constraint. In this work, we present the first flapping-wing robot using its flapping wings for environmental perception and navigation in tight space, without the need for any visual feedback. As the test platform, we introduce the Purdu Hummingbird, a flapping-wing robot with 17cm wingspan and 12 grams weight, with a pair of 30-40Hz flapping wings driven by only two actuators. By interpreting the wing loading feedback and its variations, the vehicle can detect the presence of environmental changes such as grounds, walls, stairs, obstacles and wind gust. The instantaneous wing loading can be obtained through the measurements and interpretation of the current feedback by the motors that actuate the wings. The effectiveness of the proposed approach is experimentally demonstrated on several challenging flight tasks without vision: terrain following, wall following and going through a narrow corridor. To ensure flight stability, a robust controller was designed for handling unforeseen disturbances during the flight. Sensing and navigating one's environment through actuator loading is a promising method for mobile robots, and it can serve as an alternative or complementary method to visual perception.},
	booktitle = {2019 {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Tu, Zhan and Fei, Fan and Zhang, Jian and Deng, Xinyan},
	month = may,
	year = {2019},
	note = {ISSN: 2577-087X},
	keywords = {Aerodynamics, DC motors, Loading, Navigation, Robot sensing systems},
	pages = {95--101},
}

@incollection{keennon_development_2012,
	series = {Aerospace {Sciences} {Meetings}},
	title = {Development of the {Nano} {Hummingbird}: {A} {Tailless} {Flapping} {Wing} {Micro} {Air} {Vehicle}},
	shorttitle = {Development of the {Nano} {Hummingbird}},
	url = {https://arc.aiaa.org/doi/10.2514/6.2012-588},
	urldate = {2022-05-23},
	booktitle = {50th {AIAA} {Aerospace} {Sciences} {Meeting} including the {New} {Horizons} {Forum} and {Aerospace} {Exposition}},
	publisher = {American Institute of Aeronautics and Astronautics},
	author = {Keennon, Matthew and Klingebiel, Karl and Won, Henry},
	month = jan,
	year = {2012},
	doi = {10.2514/6.2012-588},
	keywords = {Aerodynamic Characteristics, Aerodynamic Performance, Defense Advanced Research Projects Agencies, Flapping Frequency, Flight Vehicle, Ground Station, Micro Air Vehicle, Microelectromechanical Systems, Roll Control, Saturn},
}

@article{mcguire_efficient_2017,
	title = {Efficient {Optical} {Flow} and {Stereo} {Vision} for {Velocity} {Estimation} and {Obstacle} {Avoidance} on an {Autonomous} {Pocket} {Drone}},
	volume = {2},
	issn = {2377-3766},
	doi = {10.1109/LRA.2017.2658940},
	abstract = {Micro Aerial Vehicles (FOV) are very suitable for flying in indoor environments, but autonomous navigation is challenging due to their strict hardware limitations. This paper presents a highly efficient computer vision algorithm called Edge-FS for the determination of velocity and depth. It runs at 20 Hz on a 4 g stereo camera with an embedded STM32F4 microprocessor (168 MHz, 192 kB) and uses edge distributions to calculate optical flow and stereo disparity. The stereo-based distance estimates are used to scale the optical flow in order to retrieve the drone's velocity. The velocity and depth measurements are used for fully autonomous flight of a 40 g pocket drone only relying on on-board sensors. This method allows the MAV to control its velocity and avoid obstacles.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {McGuire, Kimberly and de Croon, Guido and De Wagter, Christophe and Tuyls, Karl and Kappen, Hilbert},
	month = apr,
	year = {2017},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Aerial systems: Perception and autonomy, Cameras, Drones, Image edge detection, Navigation, Optical imaging, Optical sensors, autonomous vehicle navigation, micro/nano robots, visual-based navigation},
	pages = {1070--1076},
}

@inproceedings{de_wagter_autonomous_2014,
	title = {Autonomous flight of a 20-gram {Flapping} {Wing} {MAV} with a 4-gram onboard stereo vision system},
	doi = {10.1109/ICRA.2014.6907589},
	abstract = {Autonomous flight of Flapping Wing Micro Air Vehicles (FWMAVs) is a major challenge in the field of robotics, due to their light weight and the flapping-induced body motions. In this article, we present the first FWMAV with onboard vision processing for autonomous flight in generic environments. In particular, we introduce the DelFly `Explorer', a 20-gram FWMAV equipped with a 0.98-gram autopilot and a 4.0-gram onboard stereo vision system. We explain the design choices that permit carrying the extended payload, while retaining the DelFly's hover capabilities. In addition, we introduce a novel stereo vision algorithm, LongSeq, designed specifically to cope with the flapping motion and the desire to attain a computational effort tuned to the frame rate. The onboard stereo vision system is illustrated in the context of an obstacle avoidance task in an environment with sparse obstacles.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {De Wagter, C. and Tijmons, S. and Remes, B. D. W. and de Croon, G. C. H. E.},
	month = may,
	year = {2014},
	note = {ISSN: 1050-4729},
	keywords = {Cameras, Collision avoidance, Optimization, Robots, Sensors, Stereo vision, Streaming media},
	pages = {4982--4987},
}

@article{howard_experiments_2006,
	title = {Experiments with a {Large} {Heterogeneous} {Mobile} {Robot} {Team}: {Exploration}, {Mapping}, {Deployment} and {Detection}},
	volume = {25},
	issn = {0278-3649},
	shorttitle = {Experiments with a {Large} {Heterogeneous} {Mobile} {Robot} {Team}},
	url = {https://doi.org/10.1177/0278364906065378},
	doi = {10.1177/0278364906065378},
	abstract = {We describe the design and experimental validation of a large heterogeneous mobile robot team built for the DARPA Software for Distributed Robotics (SDR) program. The core challenge for the SDR program was to develop a multi-robot system capable of carrying out a specific mission: to deploy a large number of robots into an unexplored building, map the building interior, detect and track  intruders, and transmit all of the above information to a remote operator. To satisfy these requirements, we developed a heterogeneous robot team consisting of approximately 80 robots. We sketch the key technical elements of this team, focusing on the novel aspects, and present selected results from supervised experiments conducted in a 600 m 2 indoor environment.},
	language = {en},
	number = {5-6},
	urldate = {2022-05-10},
	journal = {The International Journal of Robotics Research},
	author = {Howard, Andrew and Parker, Lynne E. and Sukhatme, Gaurav S.},
	month = may,
	year = {2006},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {deployment, exploration, multi-robot systems, simultaneous localization and mapping},
	pages = {431--447},
}

@article{dang_graph-based_2020,
	title = {Graph-based subterranean exploration path planning using aerial and legged robots},
	volume = {37},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21993},
	doi = {10.1002/rob.21993},
	abstract = {Autonomous exploration of subterranean environments remains a major challenge for robotic systems. In response, this paper contributes a novel graph-based subterranean exploration path planning method that is attuned to key topological properties of subterranean settings, such as large-scale tunnel-like networks and complex multibranched topologies. Designed both for aerial and legged robots, the proposed method is structured around a bifurcated local- and global-planner architecture. The local planner utilizes a rapidly exploring random graph to reliably and efficiently identify paths that optimize an exploration gain within a local subspace, while simultaneously avoiding obstacles, respecting applicable traversability constraints and honoring dynamic limitations of the robots. Reflecting the fact that multibranched and tunnel-like networks of underground environments can often lead to dead-ends and accounting for the robot endurance, the global planning layer works in conjunction with the local planner to incrementally build a sparse global graph and is engaged when the system must be repositioned to a previously identified frontier of the exploration space, or commanded to return-to-home. The designed planner is detailed with respect to its computational complexity and compared against state-of-the-art approaches. Emphasizing field experimentation, the method is evaluated within multiple real-life deployments using aerial robots and the ANYmal legged system inside both long-wall and room-and-pillar underground mines in the United States and in Switzerland, as well as inside an underground bunker. The presented results further include missions conducted within the Defense Advanced Research Projects Agency (DARPA) Subterranean Challenge, a relevant competition on underground exploration.},
	language = {en},
	number = {8},
	urldate = {2022-05-06},
	journal = {Journal of Field Robotics},
	author = {Dang, Tung and Tranzatto, Marco and Khattak, Shehryar and Mascarich, Frank and Alexis, Kostas and Hutter, Marco},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21993},
	keywords = {aerial robots, legged robots, path planning, subterranean robotics},
	pages = {1363--1388},
}

@article{miller_mine_2020,
	title = {Mine {Tunnel} {Exploration} {Using} {Multiple} {Quadrupedal} {Robots}},
	volume = {5},
	issn = {2377-3766},
	doi = {10.1109/LRA.2020.2972872},
	abstract = {Robotic exploration of underground environments is a particularly challenging problem due to communication, endurance, and traversability constraints which necessitate high degrees of autonomy and agility. These challenges are further exacerbated by the need to minimize human intervention for practical applications. While legged robots have the ability to traverse extremely challenging terrain, they also engender new challenges for planning, estimation, and control. In this work, we describe a fully autonomous system for multi-robot mine exploration and mapping using legged quadrupeds, as well as a distributed database mesh networking system for reporting data. In addition, we show results from DARPA Subterranean Challenge (SubT) Tunnel Circuit demonstrating localization of artifacts after traversals of hundreds of meters. These experiments describe fully autonomous exploration of an unknown Global Navigation Satellite System (GNSS)-denied environment undertaken by legged robots.},
	number = {2},
	journal = {IEEE Robotics and Automation Letters},
	author = {Miller, Ian D. and Cladera, Fernando and Cowley, Anthony and Shivakumar, Shreyas S. and Lee, Elijah S. and Jarin-Lipschitz, Laura and Bhat, Akhilesh and Rodrigues, Neil and Zhou, Alex and Cohen, Avraham and Kulkarni, Adarsh and Laney, James and Taylor, Camillo Jose and Kumar, Vijay},
	month = apr,
	year = {2020},
	note = {Conference Name: IEEE Robotics and Automation Letters},
	keywords = {Cameras, Legged locomotion, Mining robotics, Navigation, Planning, Robot vision systems, field robots, legged robots},
	pages = {2840--2847},
}

@article{hudson_heterogeneous_2021,
	title = {Heterogeneous {Ground} and {Air} {Platforms}, {Homogeneous} {Sensing}: {Team} {CSIRO} {Data61}'s {Approach} to the {DARPA} {Subterranean} {Challenge}},
	shorttitle = {Heterogeneous {Ground} and {Air} {Platforms}, {Homogeneous} {Sensing}},
	url = {http://arxiv.org/abs/2104.09053},
	abstract = {Heterogeneous teams of robots, leveraging a balance between autonomy and human interaction, bring powerful capabilities to the problem of exploring dangerous, unstructured subterranean environments. Here we describe the solution developed by Team CSIRO Data61, consisting of CSIRO, Emesent and Georgia Tech, during the DARPA Subterranean Challenge. These presented systems were fielded in the Tunnel Circuit in August 2019, the Urban Circuit in February 2020, and in our own Cave event, conducted in September 2020. A unique capability of the fielded team is the homogeneous sensing of the platforms utilised, which is leveraged to obtain a decentralised multi-agent SLAM solution on each platform (both ground agents and UAVs) using peer-to-peer communications. This enabled a shift in focus from constructing a pervasive communications network to relying on multi-agent autonomy, motivated by experiences in early circuit events. These experiences also showed the surprising capability of rugged tracked platforms for challenging terrain, which in turn led to the heterogeneous team structure based on a BIA5 OzBot Titan ground robot and an Emesent Hovermap UAV, supplemented by smaller tracked or legged ground robots. The ground agents use a common CatPack perception module, which allowed reuse of the perception and autonomy stack across all ground agents with minimal adaptation.},
	urldate = {2022-05-06},
	journal = {arXiv:2104.09053 [cs]},
	author = {Hudson, Nicolas and Talbot, Fletcher and Cox, Mark and Williams, Jason and Hines, Thomas and Pitt, Alex and Wood, Brett and Frousheger, Dennis and Surdo, Katrina Lo and Molnar, Thomas and Steindl, Ryan and Wildie, Matt and Sa, Inkyu and Kottege, Navinda and Stepanas, Kazys and Hernandez, Emili and Catt, Gavin and Docherty, William and Tidd, Brendan and Tam, Benjamin and Murrell, Simon and Bessell, Mitchell and Hanson, Lauren and Tychsen-Smith, Lachlan and Suzuki, Hajime and Overs, Leslie and Kendoul, Farid and Wagner, Glenn and Palmer, Duncan and Milani, Peter and O'Brien, Matthew and Jiang, Shu and Chen, Shengkang and Arkin, Ronald C.},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.09053},
	keywords = {Computer Science - Robotics},
}

@inproceedings{bouman_autonomous_2020,
	title = {Autonomous {Spot}: {Long}-{Range} {Autonomous} {Exploration} of {Extreme} {Environments} with {Legged} {Locomotion}},
	shorttitle = {Autonomous {Spot}},
	doi = {10.1109/IROS45743.2020.9341361},
	abstract = {This paper serves as one of the first efforts to enable large-scale and long-duration autonomy using the Boston Dynamics Spot robot. Motivated by exploring extreme environments, particularly those involved in the DARPA Subterranean Challenge, this paper pushes the boundaries of the state-of-practice in enabling legged robotic systems to accomplish real-world complex missions in relevant scenarios. In particular, we discuss the behaviors and capabilities which emerge from the integration of the autonomy architecture NeBula (Networked Belief-aware Perceptual Autonomy) with next-generation mobility systems. We will discuss the hardware and software challenges, and solutions in mobility, perception, autonomy, and very briefly, wireless networking, as well as lessons learned and future directions. We demonstrate the performance of the proposed solutions on physical systems in real-world scenarios.3 The proposed solution contributed to winning 1st-place in the 2020 DARPA Subterranean Challenge, Urban Circuit.4},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Bouman, Amanda and Ginting, Muhammad Fadhil and Alatur, Nikhilesh and Palieri, Matteo and Fan, David D. and Touma, Thomas and Pailevanian, Torkom and Kim, Sung-Kyun and Otsu, Kyohei and Burdick, Joel and Agha-Mohammadi, Ali-akbar},
	month = oct,
	year = {2020},
	note = {ISSN: 2153-0866},
	keywords = {Computer architecture, Hardware, Legged locomotion, Planning, Robots, Software, Wireless communication},
	pages = {2518--2525},
}

@article{miki_learning_nodate,
	title = {Learning robust perceptive locomotion for quadrupedal robots in the wild},
	volume = {7},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abk2822},
	doi = {10.1126/scirobotics.abk2822},
	number = {62},
	urldate = {2022-05-06},
	journal = {Science Robotics},
	author = {Miki, Takahiro and Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabk2822},
}

@article{kim_quadruped_nodate,
	title = {Quadruped robots venture into the wild with open eyes},
	volume = {7},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.abn6798},
	doi = {10.1126/scirobotics.abn6798},
	number = {62},
	urldate = {2022-05-06},
	journal = {Science Robotics},
	author = {Kim, Donghyun},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabn6798},
}

@inproceedings{hutter_anymal_2016,
	title = {{ANYmal} - a highly mobile and dynamic quadrupedal robot},
	doi = {10.1109/IROS.2016.7758092},
	abstract = {This paper introduces ANYmal, a quadrupedal robot that features outstanding mobility and dynamic motion capability. Thanks to novel, compliant joint modules with integrated electronics, the 30 kg, 0.5 m tall robotic dog is torque controllable and very robust against impulsive loads during running or jumping. The presented machine was designed with a focus on outdoor suitability, simple maintenance, and user-friendly handling to enable future operation in real world scenarios. Performance tests with the joint actuators indicated a torque control bandwidth of more than 70 Hz, high disturbance rejection capability, as well as impact robustness when moving with maximal velocity. It is demonstrated in a series of experiments that ANYmal can execute walking gaits, dynamically trot at moderate speed, and is able to perform special maneuvers to stand up or crawl very steep stairs. Detailed measurements unveil that even full-speed running requires less than 280 W, resulting in an autonomy of more than 2 h.},
	booktitle = {2016 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Hutter, Marco and Gehring, Christian and Jud, Dominic and Lauber, Andreas and Bellicoso, C. Dario and Tsounis, Vassilios and Hwangbo, Jemin and Bodie, Karen and Fankhauser, Peter and Bloesch, Michael and Diethelm, Remo and Bachmann, Samuel and Melzer, Amir and Hoepflinger, Mark},
	month = oct,
	year = {2016},
	note = {ISSN: 2153-0866},
	keywords = {Actuators, Dynamics, Legged locomotion, Robot sensing systems, Torque},
	pages = {38--44},
}

@article{mcguire_minimal_2019,
	title = {Minimal navigation solution for a swarm of tiny flying robots to explore an unknown environment},
	volume = {4},
	url = {https://www.science.org/doi/10.1126/scirobotics.aaw9710},
	doi = {10.1126/scirobotics.aaw9710},
	number = {35},
	urldate = {2022-05-05},
	journal = {Science Robotics},
	author = {McGuire, K. N. and De Wagter, C. and Tuyls, K. and Kappen, H. J. and de Croon, G. C. H. E.},
	month = oct,
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaaw9710},
}

@article{servieres_visual_2021,
	title = {Visual and {Visual}-{Inertial} {SLAM}: {State} of the {Art}, {Classification}, and {Experimental} {Benchmarking}},
	volume = {2021},
	issn = {1687-725X},
	shorttitle = {Visual and {Visual}-{Inertial} {SLAM}},
	url = {https://www.hindawi.com/journals/js/2021/2054828/},
	doi = {10.1155/2021/2054828},
	abstract = {Simultaneous Localization and Mapping is now widely adopted by many applications, and researchers have produced very dense literature on this topic. With the advent of smart devices, embedding cameras, inertial measurement units, visual SLAM (vSLAM), and visual-inertial SLAM (viSLAM) are enabling novel general public applications. In this context, this paper conducts a review of popular SLAM approaches with a focus on vSLAM/viSLAM, both at fundamental and experimental levels. It starts with a structured overview of existing vSLAM and viSLAM designs and continues with a new classification of a dozen main state-of-the-art methods. A chronological survey of viSLAM’s development highlights the historical milestones and presents more recent methods into a classification. Finally, the performance of vSLAM is experimentally assessed for the use case of pedestrian pose estimation with a handheld device in urban environments. The performance of five open-source methods Vins-Mono, ROVIO, ORB-SLAM2, DSO, and LSD-SLAM is compared using the EuRoC MAV dataset and a new visual-inertial dataset corresponding to urban pedestrian navigation. A detailed analysis of the computation results identifies the strengths and weaknesses for each method. Globally, ORB-SLAM2 appears to be the most promising algorithm to address the challenges of urban pedestrian navigation, tested with two datasets.},
	language = {en},
	urldate = {2022-05-05},
	journal = {Journal of Sensors},
	author = {Servières, Myriam and Renaudin, Valérie and Dupuis, Alexis and Antigny, Nicolas},
	month = feb,
	year = {2021},
	note = {Publisher: Hindawi},
	pages = {e2054828},
}

@inproceedings{quattrini_li_experimental_2017,
	address = {Cham},
	series = {Springer {Proceedings} in {Advanced} {Robotics}},
	title = {Experimental {Comparison} of {Open} {Source} {Vision}-{Based} {State} {Estimation} {Algorithms}},
	isbn = {978-3-319-50115-4},
	doi = {10.1007/978-3-319-50115-4_67},
	abstract = {The problem of state estimation using primarily visual data has received a lot of attention in the last decade. Several open source packages have appeared addressing the problem, each supported by impressive demonstrations. Applying any of these packages on a new dataset however, has been proven extremely challenging. Suboptimal performance, loss of localization, and challenges in customization have not produced a clear winner. Several other research groups have presented superb performance without releasing the code, sometimes materializing as commercial products. In this paper, ten of the most promising open source packages are evaluated, by cross validating them on the datasets provided for each package and by testing them on eight different datasets collected over the years in our laboratory. Indoor and outdoor, terrestrial and flying vehicles, in addition to underwater robots, cameras, and buoys were used to collect data. An analysis on the motions required for the different approaches and an evaluation of their performance is presented.},
	language = {en},
	booktitle = {2016 {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Quattrini Li, Alberto and Coskun, A. and Doherty, S. M. and Ghasemlou, S. and Jagtap, A. S. and Modasshir, M. and Rahman, S. and Singh, A. and Xanthidis, M. and O’Kane, J. M. and Rekleitis, I.},
	editor = {Kulić, Dana and Nakamura, Yoshihiko and Khatib, Oussama and Venture, Gentiane},
	year = {2017},
	keywords = {Localization, SLAM, Vision based state estimation},
	pages = {775--786},
}

@inproceedings{joshi_experimental_2019,
	title = {Experimental {Comparison} of {Open} {Source} {Visual}-{Inertial}-{Based} {State} {Estimation} {Algorithms} in the {Underwater} {Domain}},
	doi = {10.1109/IROS40897.2019.8968049},
	abstract = {A plethora of state estimation techniques have appeared in the last decade using visual data, and more recently with added inertial data. Datasets typically used for evaluation include indoor and urban environments, where supporting videos have shown impressive performance. However, such techniques have not been fully evaluated in challenging conditions, such as the marine domain. In this paper, we compare ten recent open-source packages to provide insights on their performance and guidelines on addressing current challenges. Specifically, we selected direct and indirect methods that fuse camera and Inertial Measurement Unit (IMU) data together. Experiments are conducted by testing all packages on datasets collected over the years with underwater robots in our laboratory. All the datasets are made available online.},
	booktitle = {2019 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Joshi, Bharat and Rahman, Sharmin and Kalaitzakis, Michail and Cain, Brennan and Johnson, James and Xanthidis, Marios and Karapetyan, Nare and Hernandez, Alan and Li, Alberto Quattrini and Vitzilaios, Nikolaos and Rekleitis, Ioannis},
	month = nov,
	year = {2019},
	note = {ISSN: 2153-0866},
	pages = {7227--7233},
}

@incollection{kulic_experimental_2017,
	address = {Cham},
	title = {Experimental {Comparison} of {Open} {Source} {Vision}-{Based} {State} {Estimation} {Algorithms}},
	volume = {1},
	isbn = {978-3-319-50114-7 978-3-319-50115-4},
	url = {http://link.springer.com/10.1007/978-3-319-50115-4_67},
	abstract = {The problem of state estimation using primarily visual data has received a lot of attention in the last decade. Several open source packages have appeared addressing the problem, each supported by impressive demonstrations. Applying any of these packages on a new dataset however, has been proven extremely challenging. Suboptimal performance, loss of localization, and challenges in customization have not produced a clear winner. Several other research groups have presented superb performance without releasing the code, sometimes materializing as commercial products. In this paper, ten of the most promising open source packages are evaluated, by cross validating them on the datasets provided for each package and by testing them on eight diﬀerent datasets collected over the years in our laboratory. Indoor and outdoor, terrestrial and ﬂying vehicles, in addition to underwater robots, cameras, and buoys were used to collect data. An analysis on the motions required for the diﬀerent approaches and an evaluation of their performance is presented.},
	language = {en},
	urldate = {2022-05-05},
	booktitle = {2016 {International} {Symposium} on {Experimental} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Quattrini Li, Alberto and Coskun, A. and Doherty, S. M. and Ghasemlou, S. and Jagtap, A. S. and Modasshir, M. and Rahman, S. and Singh, A. and Xanthidis, M. and O’Kane, J. M. and Rekleitis, I.},
	editor = {Kulić, Dana and Nakamura, Yoshihiko and Khatib, Oussama and Venture, Gentiane},
	year = {2017},
	doi = {10.1007/978-3-319-50115-4_67},
	note = {Series Title: Springer Proceedings in Advanced Robotics},
	pages = {775--786},
}

@inproceedings{van_beeck_real-time_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Real-{Time} {Embedded} {Computer} {Vision} on {UAVs}:},
	isbn = {978-3-030-66823-5},
	shorttitle = {Real-{Time} {Embedded} {Computer} {Vision} on {UAVs}},
	doi = {10.1007/978-3-030-66823-5_40},
	abstract = {In this paper we present an overview of the contributed work presented at the UAVision2020 (International workshop on Computer Vision for UAVs) ECCV workshop. Note that during ECCV2020 this workshop was merged with the VisDrone2020 workshop. This paper only summarizes the results of the regular paper track and the ERTI challenge. The workshop focused on real-time image processing on-board of Unmanned Aerial Vehicles (UAVs). For such applications the computational complexity of state-of-the-art computer vision algorithms often conflicts with the need for real-time operation and the extreme resource limitations of the hardware. Apart from a summary of the accepted workshop papers and an overview of the challenge, this work also aims to identify common challenges and concerns which were addressed by multiple authors during the workshop, and their proposed solutions.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2020 {Workshops}},
	publisher = {Springer International Publishing},
	author = {Van Beeck, Kristof and Ophoff, Tanguy and Vandersteegen, Maarten and Tuytelaars, Tinne and Scaramuzza, Davide and Goedemé, Toon},
	editor = {Bartoli, Adrien and Fusiello, Andrea},
	year = {2020},
	keywords = {Computer vision, Deep learning, Embedded hardware, GPUs, Hardware optimizations, Real-time, UAVs},
	pages = {665--674},
}

@inproceedings{sturm_benchmark_2012,
	title = {A benchmark for the evaluation of {RGB}-{D} {SLAM} systems},
	doi = {10.1109/IROS.2012.6385773},
	abstract = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an office environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, specifications of the data formats, sample code, and evaluation tools.},
	booktitle = {2012 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	author = {Sturm, Jürgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
	month = oct,
	year = {2012},
	note = {ISSN: 2153-0866},
	keywords = {Calibration, Cameras, Simultaneous localization and mapping, Trajectory, Visualization},
	pages = {573--580},
}

@article{engel_scale-aware_2014,
	series = {Special {Issue} on {Visual} {Control} of {Mobile} {Robots}},
	title = {Scale-aware navigation of a low-cost quadrocopter with a monocular camera},
	volume = {62},
	issn = {0921-8890},
	url = {https://www.sciencedirect.com/science/article/pii/S0921889014000566},
	doi = {10.1016/j.robot.2014.03.012},
	abstract = {We present a complete solution for the visual navigation of a small-scale, low-cost quadrocopter in unknown environments. Our approach relies solely on a monocular camera as the main sensor, and therefore does not need external tracking aids such as GPS or visual markers. Costly computations are carried out on an external laptop that communicates over wireless LAN with the quadrocopter. Our approach consists of three components: a monocular SLAM system, an extended Kalman filter for data fusion, and a PID controller. In this paper, we (1) propose a simple, yet effective method to compensate for large delays in the control loop using an accurate model of the quadrocopter’s flight dynamics, and (2) present a novel, closed-form method to estimate the scale of a monocular SLAM system from additional metric sensors. We extensively evaluated our system in terms of pose estimation accuracy, flight accuracy, and flight agility using an external motion capture system. Furthermore, we compared the convergence and accuracy of our scale estimation method for an ultrasound altimeter and an air pressure sensor with filtering-based approaches. The complete system is available as open-source in ROS. This software can be used directly with a low-cost, off-the-shelf Parrot AR.Drone quadrocopter, and hence serves as an ideal basis for follow-up research projects.},
	language = {en},
	number = {11},
	urldate = {2022-05-04},
	journal = {Robotics and Autonomous Systems},
	author = {Engel, Jakob and Sturm, Jürgen and Cremers, Daniel},
	month = nov,
	year = {2014},
	keywords = {AR.Drone, Monocular SLAM, Quadrocopter, Scale estimation, Visual SLAM, Visual navigation},
	pages = {1646--1656},
}

@article{engel_scale-aware_2014-1,
	title = {Scale-aware navigation of a low-cost quadrocopter with a monocular camera},
	volume = {62},
	issn = {09218890},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0921889014000566},
	doi = {10.1016/j.robot.2014.03.012},
	abstract = {We present a complete solution for the visual navigation of a small-scale, low-cost quadrocopter in unknown environments. Our approach relies solely on a monocular camera as the main sensor, and therefore does not need external tracking aids such as GPS or visual markers. Costly computations are carried out on an external laptop that communicates over wireless LAN with the quadrocopter. Our approach consists of three components: a monocular SLAM system, an extended Kalman ﬁlter for data fusion, and a PID controller. In this paper, we (1) propose a simple, yet effective method to compensate for large delays in the control loop using an accurate model of the quadrocopter’s ﬂight dynamics, and (2) present a novel, closed-form method to estimate the scale of a monocular SLAM system from additional metric sensors. We extensively evaluated our system in terms of pose estimation accuracy, ﬂight accuracy, and ﬂight agility using an external motion capture system. Furthermore, we compared the convergence and accuracy of our scale estimation method for an ultrasound altimeter and an air pressure sensor with ﬁltering-based approaches. The complete system is available as open-source in ROS. This software can be used directly with a low-cost, off-the-shelf Parrot AR.Drone quadrocopter, and hence serves as an ideal basis for follow-up research projects.},
	language = {en},
	number = {11},
	urldate = {2022-05-04},
	journal = {Robotics and Autonomous Systems},
	author = {Engel, Jakob and Sturm, Jürgen and Cremers, Daniel},
	month = nov,
	year = {2014},
	pages = {1646--1656},
}

@inproceedings{mingachev_comparison_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Comparison of {ROS}-{Based} {Monocular} {Visual} {SLAM} {Methods}: {DSO}, {LDSO}, {ORB}-{SLAM2} and {DynaSLAM}},
	isbn = {978-3-030-60337-3},
	shorttitle = {Comparison of {ROS}-{Based} {Monocular} {Visual} {SLAM} {Methods}},
	doi = {10.1007/978-3-030-60337-3_22},
	abstract = {Stable and robust path planning of a ground mobile robot requires a combination of accuracy and low latency in its state estimation. Yet, state estimation algorithms should provide these under computational and power constraints of a robot embedded hardware. The presented study offers a comparative analysis of four cutting edge publicly available within robot operating system (ROS) monocular simultaneous localization and mapping methods: DSO, LDSO, ORB-SLAM2, and DynaSLAM. The analysis considers pose estimation accuracy (alignment, absolute trajectory, and relative pose root mean square error) and trajectory precision of the four methods at TUM-Mono and EuRoC datasets.},
	language = {en},
	booktitle = {Interactive {Collaborative} {Robotics}},
	publisher = {Springer International Publishing},
	author = {Mingachev, Eldar and Lavrenov, Roman and Tsoy, Tatyana and Matsuno, Fumitoshi and Svinin, Mikhail and Suthakorn, Jackrit and Magid, Evgeni},
	editor = {Ronzhin, Andrey and Rigoll, Gerhard and Meshcheryakov, Roman},
	year = {2020},
	keywords = {Benchmark testing, Monocular SLAM, Path planning, Robot sensing systems, Simultaneous localization and mapping, State estimation, Visual SLAM, Visual odometry},
	pages = {222--233},
}

@article{lopez_multi-sensorial_2017,
	title = {A {Multi}-{Sensorial} {Simultaneous} {Localization} and {Mapping} ({SLAM}) {System} for {Low}-{Cost} {Micro} {Aerial} {Vehicles} in {GPS}-{Denied} {Environments}},
	volume = {17},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/17/4/802},
	doi = {10.3390/s17040802},
	abstract = {One of the main challenges of aerial robots navigation in indoor or GPS-denied environments is position estimation using only the available onboard sensors. This paper presents a Simultaneous Localization and Mapping (SLAM) system that remotely calculates the pose and environment map of different low-cost commercial aerial platforms, whose onboard computing capacity is usually limited. The proposed system adapts to the sensory configuration of the aerial robot, by integrating different state-of-the art SLAM methods based on vision, laser and/or inertial measurements using an Extended Kalman Filter (EKF). To do this, a minimum onboard sensory configuration is supposed, consisting of a monocular camera, an Inertial Measurement Unit (IMU) and an altimeter. It allows to improve the results of well-known monocular visual SLAM methods (LSD-SLAM and ORB-SLAM are tested and compared in this work) by solving scale ambiguity and providing additional information to the EKF. When payload and computational capabilities permit, a 2D laser sensor can be easily incorporated to the SLAM system, obtaining a local 2.5D map and a footprint estimation of the robot position that improves the 6D pose estimation through the EKF. We present some experimental results with two different commercial platforms, and validate the system by applying it to their position control.},
	language = {en},
	number = {4},
	urldate = {2022-05-04},
	journal = {Sensors},
	author = {López, Elena and García, Sergio and Barea, Rafael and Bergasa, Luis M. and Molinos, Eduardo J. and Arroyo, Roberto and Romera, Eduardo and Pardo, Samuel},
	month = apr,
	year = {2017},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {SLAM, aerial robots, sensor fusion},
	pages = {802},
}

@inproceedings{famili_rolatin_2020,
	title = {{ROLATIN}: {Robust} {Localization} and {Tracking} for {Indoor} {Navigation} of {Drones}},
	shorttitle = {{ROLATIN}},
	doi = {10.1109/WCNC45663.2020.9120619},
	abstract = {In many drone applications, drones need the ability to fly fully or partially autonomously to carry out their mission. To enable such fully/partially autonomous flights, the ground control station that is supporting the drone’s operation needs to constantly localize and track the drone, and send this information to the drone’s navigation controller to enable autonomous/semiautonomous navigation. In outdoor environments, localization and tracking can be readily carried out using GPS and the drone’s Inertial Measurement Units (IMUs). However, in indoor areas or GPS-denied environments, such an approach is not feasible. In this paper, we propose a localization and tracking scheme for drones called ROLATIN (Robust Localization and Tracking for Indoor Navigation of drones) that was specifically devised for GPS-denied environments. Instead of GPS signals, ROLATIN relies on speakergenerated ultrasonic acoustic signals to estimate the target drone’s location and track its movement. Compared to vision and RF signal-based methods, our scheme offers a number of advantages in terms of performance and cost.},
	booktitle = {2020 {IEEE} {Wireless} {Communications} and {Networking} {Conference} ({WCNC})},
	author = {Famili, Alireza and Park, Jung-Min Jerry},
	month = may,
	year = {2020},
	note = {ISSN: 1558-2612},
	keywords = {Acoustics, Doppler shift, Estimation, FHSS, Indoor navigation, Kalman filter, Location awareness, Measurement units, Radio frequency, Target tracking, drones, indoor localization, ultrasound transceiver},
	pages = {1--6},
}

@article{roldan_mini-uav_2015,
	title = {Mini-{UAV} {Based} {Sensory} {System} for {Measuring} {Environmental} {Variables} in {Greenhouses}},
	volume = {15},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/15/2/3334},
	doi = {10.3390/s150203334},
	abstract = {This paper describes the design, construction and validation of a mobile sensory platform for greenhouse monitoring. The complete system consists of a sensory system on board a small quadrotor (i.e., a four rotor mini-UAV). The goals of this system include  taking measures of temperature, humidity, luminosity and CO2 concentration and plotting maps of these variables. These features could potentially allow for climate control, crop monitoring or failure detection (e.g., a break in a plastic cover). The sensors have been selected by considering the climate and plant growth models and the requirements for their integration onboard the quadrotor. The sensors layout and placement have been determined through a study of quadrotor aerodynamics and the influence of the airflows from its rotors. All components of the system have been developed, integrated and tested through a set of field experiments in a real greenhouse. The primary contributions of this paper are the validation of the quadrotor as a platform for measuring environmental variables and the determination of the optimal location of sensors on a quadrotor.},
	language = {en},
	number = {2},
	urldate = {2022-05-04},
	journal = {Sensors},
	author = {Roldán, Juan Jesús and Joossen, Guillaume and Sanz, David and Del Cerro, Jaime and Barrientos, Antonio},
	month = feb,
	year = {2015},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {UAVs, agriculture, environmental monitoring, greenhouse, robotics, sensory system},
	pages = {3334--3350},
}

@article{krul_visual_2021,
	title = {Visual {SLAM} for {Indoor} {Livestock} and {Farming} {Using} a {Small} {Drone} with a {Monocular} {Camera}: {A} {Feasibility} {Study}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-446X},
	shorttitle = {Visual {SLAM} for {Indoor} {Livestock} and {Farming} {Using} a {Small} {Drone} with a {Monocular} {Camera}},
	url = {https://www.mdpi.com/2504-446X/5/2/41},
	doi = {10.3390/drones5020041},
	abstract = {Real-time data collection and decision making with drones will play an important role in precision livestock and farming. Drones are already being used in precision agriculture. Nevertheless, this is not the case for indoor livestock and farming environments due to several challenges and constraints. These indoor environments are limited in physical space and there is the localization problem, due to GPS unavailability. Therefore, this work aims to give a step toward the usage of drones for indoor farming and livestock management. To investigate on the drone positioning in these workspaces, two visual simultaneous localization and mapping (VSLAM)—LSD-SLAM and ORB-SLAM—algorithms were compared using a monocular camera onboard a small drone. Several experiments were carried out in a greenhouse and a dairy farm barn with the absolute trajectory and the relative pose error being analyzed. It was found that the approach that suits best these workspaces is ORB-SLAM. This algorithm was tested by performing waypoint navigation and generating maps from the clustered areas. It was shown that aerial VSLAM could be achieved within these workspaces and that plant and cattle monitoring could benefit from using affordable and off-the-shelf drone technology.},
	language = {en},
	number = {2},
	urldate = {2022-05-04},
	journal = {Drones},
	author = {Krul, Sander and Pantos, Christos and Frangulea, Mihai and Valente, João},
	month = jun,
	year = {2021},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {drones, farming, livestock, unmanned aerial vehicles (UAV), visual SLAM},
	pages = {41},
}

@inproceedings{brand_pidrone_2018,
	title = {{PiDrone}: {An} {Autonomous} {Educational} {Drone} {Using} {Raspberry} {Pi} and {Python}},
	shorttitle = {{PiDrone}},
	doi = {10.1109/IROS.2018.8593943},
	abstract = {A compelling robotics course begins with a compelling robot. We introduce a new low-cost aerial educational platform, the PiDrone, along with an associated college-level introductory robotics course. In a series of projects, students incrementally build, program, and test their own drones to create an autonomous aircraft capable of using a downward facing RGB camera and infrared distance sensor to visually localize and maintain position. The PiDrone runs Python and the Robotics Operating System (ROS) framework on an onboard Raspberry Pi, providing an accessible and inexpensive platform for introducing students to robotics. Students can use any web and SSH capable computer as a base station and programming platform. The projects and supplementary homeworks introduce PID control, state estimation, and high-level planning, giving students the opportunity to exercise their new skills in an exciting long-term project.},
	booktitle = {2018 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Brand, Isaiah and Roy, Josh and Ray, Aaron and Oberlin, John and Oberlix, Stefanie},
	month = oct,
	year = {2018},
	note = {ISSN: 2153-0866},
	keywords = {Drones, Educational robots, Hardware, Python, Robot sensing systems, Service robots},
	pages = {1--7},
}

@article{sareh_rotorigami_2018,
	title = {Rotorigami: {A} rotary origami protective system for robotic rotorcraft},
	volume = {3},
	shorttitle = {Rotorigami},
	url = {https://www.science.org/doi/full/10.1126/scirobotics.aah5228},
	doi = {10.1126/scirobotics.aah5228},
	number = {22},
	urldate = {2022-04-29},
	journal = {Science Robotics},
	author = {Sareh, Pooya and Chermprayong, Pisak and Emmanuelli, Marc and Nadeem, Haris and Kovac, Mirko},
	month = sep,
	year = {2018},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eaah5228},
}

@article{lin_autonomous_2018,
	title = {Autonomous aerial navigation using monocular visual-inertial fusion},
	volume = {35},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21732},
	doi = {10.1002/rob.21732},
	abstract = {Autonomous micro aerial vehicles (MAVs) have cost and mobility benefits, making them ideal robotic platforms for applications including aerial photography, surveillance, and search and rescue. As the platform scales down, MAVs become more capable of operating in confined environments, but it also introduces significant size and payload constraints. A monocular visual-inertial navigation system (VINS), consisting only of an inertial measurement unit (IMU) and a camera, becomes the most suitable sensor suite in this case, thanks to its light weight and small footprint. In fact, it is the minimum sensor suite allowing autonomous flight with sufficient environmental awareness. In this paper, we show that it is possible to achieve reliable online autonomous navigation using monocular VINS. Our system is built on a customized quadrotor testbed equipped with a fisheye camera, a low-cost IMU, and heterogeneous onboard computing resources. The backbone of our system is a highly accurate optimization-based monocular visual-inertial state estimator with online initialization and self-extrinsic calibration. An onboard GPU-based monocular dense mapping module that conditions on the estimated pose provides wide-angle situational awareness. Finally, an online trajectory planner that operates directly on the incrementally built three-dimensional map guarantees safe navigation through cluttered environments. Extensive experimental results are provided to validate individual system modules as well as the overall performance in both indoor and outdoor environments.},
	language = {en},
	number = {1},
	urldate = {2022-04-30},
	journal = {Journal of Field Robotics},
	author = {Lin, Yi and Gao, Fei and Qin, Tong and Gao, Wenliang and Liu, Tianbo and Wu, William and Yang, Zhenfei and Shen, Shaojie},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21732},
	keywords = {aerial robotics, mapping, planning, position estimation},
	pages = {23--51},
}

@article{zhang_visual-inertial_2017,
	title = {Visual-{Inertial} {Odometry} on {Chip}: {An} {Algorithm}-and-{Hardware} {Co}-design {Approach}},
	copyright = {Creative Commons Attribution-Noncommercial-Share Alike},
	shorttitle = {Visual-{Inertial} {Odometry} on {Chip}},
	url = {https://dspace.mit.edu/handle/1721.1/109522},
	abstract = {Autonomous navigation of miniaturized robots (e.g., nano/pico aerial vehicles) is currently a grand challenge for robotics research, due to the need of processing a large amount of sensor data (e.g., camera frames) with limited on-board computational resources. In this paper we focus on the design of a visual-inertial odometry (VIO) system in which the robot estimates its ego-motion (and a landmark-based map) from on- board camera and IMU data. We argue that scaling down VIO to miniaturized platforms (without sacrificing performance) requires a paradigm shift in the design of perception algorithms, and we advocate a co-design approach in which algorithmic and hardware design choices are tightly coupled. Our contribution is four-fold. First, we discuss the VIO co-design problem, in which one tries to attain a desired resource-performance trade-off, by making suitable design choices (in terms of hardware, algorithms, implementation, and parameters). Second, we characterize the design space, by discussing how a relevant set of design choices affects the resource-performance trade-off in VIO. Third, we provide a systematic experiment-driven way to explore the design space, towards a design that meets the desired trade-off. Fourth, we demonstrate the result of the co-design process by providing a VIO implementation on specialized hardware and showing that such implementation has the same accuracy and speed of a desktop implementation, while requiring a fraction of the power.},
	language = {en\_US},
	urldate = {2022-04-30},
	journal = {Sze},
	author = {Zhang, Zhengdong and Suleiman, Amr AbdulZahir and Carlone, Luca and Sze, Vivienne and Karaman, Sertac},
	month = jul,
	year = {2017},
	note = {Accepted: 2017-06-01T21:09:22Z},
}

@inproceedings{delmerico_benchmark_2018,
	title = {A {Benchmark} {Comparison} of {Monocular} {Visual}-{Inertial} {Odometry} {Algorithms} for {Flying} {Robots}},
	doi = {10.1109/ICRA.2018.8460664},
	abstract = {Flying robots require a combination of accuracy and low latency in their state estimation in order to achieve stable and robust flight. However, due to the power and payload constraints of aerial platforms, state estimation algorithms must provide these qualities under the computational constraints of embedded hardware. Cameras and inertial measurement units (IMUs) satisfy these power and payload constraints, so visual-inertial odometry (VIO) algorithms are popular choices for state estimation in these scenarios, in addition to their ability to operate without external localization from motion capture or global positioning systems. It is not clear from existing results in the literature, however, which VIO algorithms perform well under the accuracy, latency, and computational constraints of a flying robot with onboard state estimation. This paper evaluates an array of publicly-available VIO pipelines (MSCKF, OKVIS, ROVIO, VINS-Mono, SVO+MSF, and SVO+GTSAM) on different hardware configurations, including several single-board computer systems that are typically found on flying robots. The evaluation considers the pose estimation accuracy, per-frame processing time, and CPU and memory load while processing the EuRoC datasets, which contain six degree of freedom (6DoF) trajectories typical of flying robots. We present our complete results as a benchmark for the research community.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Delmerico, Jeffrey and Scaramuzza, Davide},
	month = may,
	year = {2018},
	note = {ISSN: 2577-087X},
	keywords = {Hardware, Optimization, Pipelines, Robot sensing systems, State estimation, Visualization},
	pages = {2502--2509},
}

@article{forster_-manifold_2017,
	title = {On-{Manifold} {Preintegration} for {Real}-{Time} {Visual}–{Inertial} {Odometry}},
	volume = {33},
	issn = {1941-0468},
	doi = {10.1109/TRO.2016.2597321},
	abstract = {Current approaches for visual-inertial odometry (VIO) are able to attain highly accurate state estimation via nonlinear optimization. However, real-time optimization quickly becomes infeasible as the trajectory grows over time; this problem is further emphasized by the fact that inertial measurements come at high rate, hence, leading to the fast growth of the number of variables in the optimization. In this paper, we address this issue by preintegrating inertial measurements between selected keyframes into single relative motion constraints. Our first contribution is a preintegration theory that properly addresses the manifold structure of the rotation group. We formally discuss the generative measurement model as well as the nature of the rotation noise and derive the expression for the maximum a posteriori state estimator. Our theoretical development enables the computation of all necessary Jacobians for the optimization and a posteriori bias correction in analytic form. The second contribution is to show that the preintegrated inertial measurement unit model can be seamlessly integrated into a visual-inertial pipeline under the unifying framework of factor graphs. This enables the application of incremental-smoothing algorithms and the use of a structureless model for visual measurements, which avoids optimizing over the 3-D points, further accelerating the computation. We perform an extensive evaluation of our monocular VIO pipeline on real and simulated datasets. The results confirm that our modeling effort leads to an accurate state estimation in real time, outperforming state-of-the-art approaches.},
	number = {1},
	journal = {IEEE Transactions on Robotics},
	author = {Forster, Christian and Carlone, Luca and Dellaert, Frank and Scaramuzza, Davide},
	month = feb,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Robotics},
	keywords = {Computational modeling, Computer vision, Estimation, Jacobian matrices, Manifolds, Optimization, Real-time systems, Smoothing methods, sensor fusion, visual–inertial odometry (VIO)},
	pages = {1--21},
}
