\chapter{Perception Challenges and Preliminary Works}
\label{chap:perception}

As described in the introduction (Chap. \ref{chap:introduction}), Aerobat needs both high level and low level control in order to execute autonomous flight. This chapter describes the progress made towards developing perception and state estimation onboard Aerobat, from selecting the electronics required to hardware and software integration and sensor calibration. 

\section{Onboard Electronics}
\label{sec:electronics}

When selecting the onboard electronics for Aerobat, a soft payload limit of 15g was imposed on the selection for autonomy electronics. This was done to allow for additional stabilizers used for testing in the initial stages of development while the controls are still under research.

\subsection{Processor}
\label{subsec:processor}

In order to achieve autonomy, the onboard processor must be powerful enough to interface with multiple sensors and execute control algorithms at a high enough rate. With a 15g payload limit, this did not offer a lot of options, forcing a compromise between processing power and weight.

Multirotors have a larger payload capacity and can carry large processors. \cite{tang_aggressive_2018, foehn_agilicious_2022, kaufmann_beauty_2019, chen_aerial_2022} use Odroid XU4 and Intel UP board onboard, both of which weigh around 40g and come with 4-core ~2GHz 64-bit processors with 2/4GB of RAM. Some such as \cite{petrlik_robust_2020, best_resilient_2022, foehn_alphapilot_2022, foehn_time-optimal_2021} use even larger processors such as Intel NUC (9 cores, 1.1GHz, 16GB RAM) or one of the NVIDIA Jetson series: Nano (4-core CPU, 128-core GPU, 4GB RAM), TX2 (2-core CPU, 256-core GPU, 4GB RAM), Xavier (8-core CPU, 512-core GPU, 32 GB RAM), which all weigh in the order of a few 100g. All these options are far beyond the payload capacity of Aerobat.

At the other extreme, small microcontroller boards such as Arduino Nano, Arduino Pico, and Raspberry Pi Pico are becoming more capable. Arduino Pico (weighing just 1g) was used in the initial stages of Aerobat flight tests to control the actuator and stabilizer motors (Fig. \ref{fig:beta_old}). However, all of these have under 1MB of memory and are not practical for high-level computation. 

Arduino Portenta is a small 2-core microcontroller board that can simultaneously run an Arduino loop on one core and computer vision algorithms on the other, with support for TensorFlow Lite. Arduino Nicla Vision is another lightweight microcontroller board option that has a camera, IMU, and distance sensor embedded in the board itself and supports TinyML, OpenMV, and MicroPython. Both Arduino Portenta and Nicla come with Bluetooth and WiFi embedded.

These are potentially attractive options for specialized applications. However, Portenta has just 8MB of memory (expandable up to 64MB) and Nicla is even lower at just 2MB, which is not sufficient for the level of autonomous computation targeted for Aerobat. 

After some consideration, the Raspberry Pi Zero 2 W (Fig. \ref{fig:pizero}) was chosen as the ideal compromise. Weighing 11g, the Raspberry Pi Zero 2 W runs on a 4-core 1GHz 64-bit ARM processor with a Linux-based operating system. It has 512MB of RAM, Wi-Fi, and Bluetooth capability, and has an interface for a Raspberry Pi camera. This is still relatively powerful for its size, and at the time of writing to the best of my knowledge, is the most powerful processor weighing less than 15g available on the market.

% Figure environment removed

\subsection{Sensors}

The choice of onboard sensors depends on the approach used for autonomy and the environment in which it is designed to operate. For example, an IR camera might be suitable for dark environments such as night flight, and simple laser rangefinders might suffice for maintaining a steady heading within a confined space such as a tunnel. Other options considered included Sonar rangefinders, optical flow sensors, and stereo cameras. 

At this early development stage, however, priority is given to versatility that would allow for testing under a range of environments and applications. With this in mind, a single monocular RGB camera and IMU were chosen as the onboard sensors, using a visual-inertial odometry approach for localization.  

\subsubsection{Camera}

% Figure environment removed

% Figure environment removed

In the original configuration of Aerobat (Fig. \ref{fig:beta_old}), to get a sense of what images from an onboard camera would look like, a small FPV wireless camera was used (Fig. \ref{fig:fpvcamcomponents}). It consisted of the camera itself with an attached antenna and dedicated battery mounted on Aerobat, and a radio receiver connected to a laptop offboard through USB. This streamed 640x480 resolution image at 30Hz with no noticeable lag with line of sight communication. The camera weighs 4.53g. Fig. \ref{fig:fpvcampic} shows an image from this camera. 

Without a microprocessor onboard, this camera allowed us to see the world from Aerobat's perspective while it was flying. However, this could not be a long-term solution as the only interface to this camera was through the wireless receiver. Although a few options for camera modules were compared, with the Raspberry Pi Zero 2 W selected as the onboard processor, the natural choice was to use the Raspberry Pi Camera (Fig. \ref{fig:rpicamera}) as the interfaces were already in place. 

The Raspberry Pi Camera Model 2 weighs 3g and has an 8-megapixel sensor that offers video streaming up to 1080p at 30 fps and 720p at 60 fps. Fig. \ref{fig:camera_pics_comparison} compares images from the Raspberry Pi camera and the previously used FPV camera. While the FPV camera has a wider field of view, definition in features is lost in the center of the scene when compared with the Raspberry Pi Camera. The Raspberry Pi Camera also offers a few additional perks. It allows for setting the internal sensor update rate, independent of the rate at which images are read from the camera. This allows for low motion blur in images even when reading from the camera at low frame rates. It also has an internal GPU that gives it the ability to adjust exposure, shutter speed, brightness, contrast, saturation, and rotation, taking the load off the processor. Section \ref{sec:camera_integration} describes development of camera drivers.

\subsubsection{Inertial Measurement Unit (IMU)}

In this early stage of development, flight tests typically last just a few seconds at a time, removing IMU drift as a factor. However, there were two requirements for the IMU to meet: 

\begin{enumerate}
    \item \textbf{Data rate:} For good visual inertial odometry, it is ideal to have visual data at at around 20 Hz and inertial data at around 200 Hz. Therefore, the IMU must be able to provide data at 200 Hz or more.
    \item \textbf{Weight:} With 14g of payload taken up by the processor (11g) and camera (3g), there is only 1g of the imposed payload limit left for the IMU. Therefore, the IMU must weigh 1g or under. 
\end{enumerate}

Professional grade IMUs such as the VN-100 would be superfluous and expensive options at this stage when hobby-grade components are able to meet the requirements while being lighter in weight and lower in cost. Popular hobby-grade IMUs such as Adafruit's MPU6050 and ICM-20948 can comfortably meet these requirements, weighing just 1g and giving high data rates up to 400 kHz through I2C, limited only by the read/write speed of the interfaced processor. At the time of development, however, these were unavailable due to the ongoing chip shortage. Some IMUs such as Adafruit's BNO055 and WIT-motion's WT901 have similar weights and perform sensor fusion onboard to provide useful data such as absolute orientation, gravity-corrected linear acceleration, and gravity vectors. BNO055 only has a maximum rate of 100 Hz but WT901 has a maximum update rate of 200 Hz, meeting the desired data rate. Due to its availability and suitability to the requirements, this was chosen as the IMU for Aerobat (Fig. \ref{fig:imu}).

The sensor is interfaced by I2C communication and internally updates registers containing the following information:

\begin{itemize}
    \item Linear Acceleration (x, y, z)
    \item Angular Velocity (x, y, z)
    \item Magnetic Field Strength (x, y, z)
    \item Kalman Filtered Absolute Orientation in Euler angles (Roll, Pitch, Yaw)
    \item Kalman Filtered Absolute Orientation in Quaternion (x, y, z, w)
    \item Temperature
\end{itemize}

Each value is stored in 2 Bytes of memory, bringing a total of 34 Bytes of information available to be read in each sampling of the IMU. Section \ref{sec:imu_integration} describes how this data is read onboard by the IMU driver.

% Figure environment removed

\begin{table}[]
    \centering
    \begin{tabular}{l c c}
        Aerobat Structure + Wings  &  22g\\
        Motor + ESC + Gearbox  & 8g\\
        Electronics Mount & 3g\\
        Battery & 18g\\
        Processor & 11g\\
        Camera & 3g\\
        IMU & 1g\\
        \textbf{Total} & \textbf{66g}
    \end{tabular}
    \caption{Breakdown of each of the components on Aerobat by weight}
    \label{tab:weight_breakdown}
\end{table}

\section{Sensor Integration}
\label{sec:sensor_integration}

Results from \cite{delmerico_benchmark_2018} indicated that most standard perception approaches would struggle to run on the Raspberry Pi Zero's 512 MB of RAM, and so initial efforts were focused on testing the capabilities of the processor, the camera, and the IMU.

\subsection{Camera}
\label{sec:camera_integration}

% Figure environment removed

Camera drivers were implemented onboard using Python3 and various algorithms were tested for performance with greyscale images at resolutions of 640x480 and 320x240 and 30 frames per second, including SIFT feature detection, Sparse and Dense Optical Flow, and Apriltag Detection. All of these comfortably ran onboard, utilizing less than 20\% of available memory. The field of view in these cases, however, appeared to be smaller than the field of view of the camera. Upon investigation, it was determined that the field of view was intentionally being clipped based on the resolution of the image and the framerate. This behaviour is described by the chart in Fig. \ref{fig:sensor_areas} from the Picamera documentation. To get around the clipped field of view issue, the driver was modified to read images at the full resolution (1640x922) and then resize it on the processor to the desired resolution. This gives the full field of view shown in Fig. \ref{fig:rpicampic}.

\subsection{IMU}
\label{sec:imu_integration}

IMU drivers were also implemented and interfaced using I2C communication and read rates of up to 1 kHz were achieved. Confident that the processor was capable of handling more, a bare-bones version of ROS Noetic was installed from source, containing only libraries required for the ROS perception stack.

% Figure environment removed

Continuing further testing of the processor, IMU and camera ROS drivers were implemented and tested at various publishing rates. Initial results showed that while memory usage was acceptable at around 25\%, processing times were highly variable at higher rates. Figure \ref{fig:200hzIMU} shows the large variation in the time period of the IMU data being published over a period of 2 min of recording at a desired rate of 200 Hz. The variation in timestamps looks asymmetrical because of the way ROS handles timing. When a message takes longer than the desired rate to publish, the next message is published almost immediately with no delay. This leads to a number of messages with close to zero time difference from the previous message. This variation is further increased as the additional node to publish camera data is run alongside it \ref{fig:200hzIMUCamera}.

Investigation showed one of the causes to be long read times for I2C communication with the IMU. This was initially implemented as individual register reads for each of the data provided by the IMU, but optimization by reading a large contiguous block of data instead allowed reducing the number of I2C reads from 16 to 2. This considerably sped up the operation, improving performance.

% Figure environment removed

Further improvements to the timing were made by using timed callback functions using ros::Timers rather than rate-based sleep functions in loops to read data from the sensors and reducing the frequency from 200 Hz to 150 Hz. Figure \ref{fig:newIMU150} shows the improved performance of the IMU at 150 Hz. 

There are still non-trivial variations in the time periods in the data. This is likely due to the number of parallel processes in operation. Running two nodes (one for camera and one for IMU) through ROS spawns over 10 threads, which, on a 4-core processor such as the Raspberry Pi Zero, would cause interruptions that increase the time between successive data reads. On a faster processor, this may not pose a challenge, with the processor able to keep up with the desired rate despite interruptions. However, this is likely a hardware limitation of the Raspberry Pi Zero.

\section{Sensor Calibration}

Camera and IMU were first individually calibrated. The standard checkerboard and Matlab's camera calibration toolbox was used to calibrate the camera and 6 hours of stationary bias-corrected data was used with \cite{noauthor_allan_2022} to calibrate the IMU.

With this calibration data, camera and IMU were calibrated together using Kalibr's \cite{noauthor_ethz-aslkalibr_nodate} camera-imu calibration script. Using a 4x6 1cm tag size aprilgrid as the calibration target, RISE Arena's manipulator was used to move the robot around, exciting all axes of the IMU. However, despite low camera re-projection errors and estimated accelerometer and gyroscope errors in the prior, optimization fails to find a solution for this setup. This is as yet an open issue, with potential sources of error being the same timing issues still affecting the data, IMU axes not being excited enough or the camera not getting a wide enough field of view for the data.

\section{Concluding remarks}

This chapter described the challenges in selecting and integrating electronics on a tight payload budget, and the challenges associated with running the perception stack onboard with limited hardware. With a fully integrated perception system, future work will be focused on utilizing RISE Arena (Section \ref{sec:rise}) to test VIO algorithms onboard and evaluating their feasibility and challenges in implementing autonomous flight for Aerobat. Work will also be required to integrate the kinematics and dynamics of Aerobat into the perception algorithm for more robust state estimation. Using these, Aerobat will be flown autonomously within RISE Arena using the perception system to stay within the boundaries of the confined space while performing aerial maneuvers.    

% --- EOF ---