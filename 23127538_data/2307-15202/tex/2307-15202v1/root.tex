%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{graphicx}
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{algorithm}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{algpseudocode}
\usepackage{verbatim}
\usepackage{tabularx}
\makeatletter
\let\NAT@parse\undefined
\makeatother
\usepackage[
%hidelinks,
%breaklinks,
colorlinks,
linkcolor=black,
citecolor=black,
filecolor=red,
urlcolor=blue]{hyperref}
\usepackage{balance}
\usepackage{booktabs} % for toprule/bottomrule
\usepackage{tabularx} % some extra functions for tables such as stretching on width
%\usepackage[none]{hyphenat}

\newcommand{\gb}[1]{{\color{RawSienna}\textit{(GB) #1}}}
\newcommand{\micah}[1]{\textcolor{purple}{\textit{(MC) #1}}}
\newcommand{\basti}[1]{\textcolor{red}{\textit{(basti) #1}}}

\usepackage[numbers,sort&compress]{natbib}
\newcommand{\etal}{\textit{et al.}~}
\title{\LARGE \bf
Multi-Robot Multi-Room Exploration with \\ Geometric Cue Extraction and Spherical Decomposition
}


\author{Seungchan Kim$^{1}$, Micah Corah$^{1}$, John Keller$^{1}$, Graeme Best$^{2}$, Sebastian Scherer$^{1}$ 
\thanks{$^{1}$ S. Kim, M. Corah, J. Keller, and S. Scherer are with Robotics Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA.  \tt{\{seungch2, micahc, jkeller2, basti\}\linebreak[0]{}@andrew.cmu.edu}}
\thanks{$^{2}$ G. Best is with School of Mechanical and Mechatroninc Engineering,  University of Technology Sydney, Ultimo NSW 2007, Austrailia. \tt{graeme.best@uts.edu.au}}
\thanks{This work is supported by Defense Science and Technology Agency Singapore.}
}



\begin{document}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
This work proposes an autonomous multi-robot exploration pipeline that coordinates the behaviors of robots in an indoor environment composed of multiple rooms. Contrary to simple frontier-based exploration approaches,  we aim to enable robots to methodically explore and observe an unknown set of rooms in a structured building, keeping track of which rooms are already explored and sharing this information among robots to coordinate their behaviors in a distributed manner. To this end, we propose (1) a geometric cue extraction method that processes 3D map point cloud data and detects the locations of potential cues such as doors and rooms, (2) a spherical decomposition for open spaces used for target assignment. Using these two components, our pipeline effectively assigns tasks among robots, and enables a methodical exploration of rooms. We evaluate the performance of our pipeline using a team of up to 3 aerial robots, and show that our method outperforms the baseline by $36.6\%$ in simulation and $26.4\%$ in real-world experiments. \textcolor{blue}{(Video link: \href{https://youtu.be/wRLEY4FHZko}{https://youtu.be/wRLEY4FHZko})}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Multi-robot exploration \cite{Burgard2005, braga2017} in unfamiliar, unknown environments has attracted attention in the robotics research community, due to its potential to accomplish duties faster than a single robot, and its wide applicability in tasks including search \& rescue operations \cite{search-and-rescue}, hazardous source detection in turbulent environments \cite{hazardous}, and planetary missions \cite{schuster2019towards}. Recently, in light of the DARPA Subterranean Challenge, there is a growing attention on exploration of large underground and indoor environments by teams of robots, with realistic communication constraints, sensor coverage, and compute conditions \cite{kulkarni2022subt, Scherer:2022, agha2021nebula}.
In this work, we aim to develop a more structured multi-robot autonomous exploration pipeline for operation in indoor environments, taking advantage of geometric properties of structures in a building. 


Specifically, we aim to build algorithms for multiple robots exploring inside a building composed of multiple rooms, whose locations and sizes are not known in advance. Rather than following a frontier-based approach~\citep{yamauchi1998frontier}, we explicitly model the geometry of rooms to enable a methodical exploration of structural environments, empowering robots to explore the rooms one by one, observing each room in-turn before moving on to another. Furthermore, we also want coordinated behaviors such that robots select rooms to avoid redundant observations by multiple robots, and we are interested in performing detailed observation of these confined spaces (rooms) with a primary sensor, such as an RGB camera, at close range~\cite{best2022rss}.

Why do we want this type of multi-room exploration? First, rooms are usually the parts of the building where meaningful target objects are placed in, compared to corridors and hallways.
Thus, they warrant focused attention during exploration. Second, rooms are non-overlapping and structural units that constitute the building; the entire indoor space can be segmented using a room-oriented search. One thing to consider is that, when we deploy multiple robots for room-oriented search, it is inefficient to assign multiple robots to the same small room; thus, we need a coordinated target assignment scheme for multi-robot, multi-room exploration.

% Figure environment removed

To this end, we propose our method, Multi-Robot Multi-Room (MRMR). Unlike learning-based geometry prediction for exploration, our method does not require an offline dataset for representative environments to learn from. Instead, we use a simple yet effective and generalizable technique to extract geometric cues that indicate the potential locations of doors and rooms, only from 3D point cloud LiDAR data. Our pipeline processes map data observed by LiDAR sensors onboard, converting 3D point cloud voxels into a 2D binary image, and then into a 2D distance transform map. Using the 2D distance transform map, robots perform real-time geometric analysis to discover structural cues that signal the doors and rooms. Robots update global plans and execute local planners accordingly, actively searching for unreached doors and unexplored rooms in the space.


We also propose the idea of representing the confined spaces (e.g. rooms) with spheres. We show that spherical decomposition is a compact representation of the environment for robots exploring rooms and sharing necessary information with other robots via communication. 

We evaluate our autonomous exploration piepline using multiple unmanned aerial vehicles (UAV), both in simulation and real-world experiments. Built upon the multi-robot exploration and planning component of the complete autonomy stack \cite{best2022rss} of Team Explorer, which showed most successful exploration by aerial robots in the final round of DARPA SubT Challenge competition, our methods record significant performance gain in fast discovery and exploration of multiple rooms, and coordination of behaviors for multiple robots. 


In summary, our contributions are:
\begin{itemize}
    \item An autonomous multi-robot exploration pipeline that coordinates behaviors of robots in a building composed of multiple rooms.
    \item Incorporation of two new modules for multi-robot multi-room exploration: (1) a geometric cue extraction method that detects the locations of doors and rooms from 3D LiDAR point cloud data, and (2) spherical decomposition of spaces for room representation, target assignments, and communication.
    \item Empirical validation of our multi-robot exploration pipeline via simulated and real-world experiments.
\end{itemize}

\section{Related Work}

\subsection{Multi-robot Exploration}
Multi-robot exploration problems have been studied with various approaches. Many prior works  \cite{Burgard2005, yamauchi1998frontier} view this problem as an assignment of frontiers (the boundaries between known and unknown space), where robots explore environments by continuously moving toward nearby, unexplored frontiers. Other approaches include sampling-based \cite{sample-based-motion-planning}, information-theoretic \cite{charrow2015information, submodular-micah2019}, graph search-based \cite{dang2020graph}, recursive tree-based search \cite{plume2019}, and sub-map merging \cite{MUI-TARE}. Recent works include hybrid approaches, such as combining frontier-based approach and graph-search \cite{best2022rss}. 

Multi-robot exploration research can also be categorized by whether decision-making is centralized or decentralized. In centralized schemes \cite{luna2011efficient, gul2022centralized}, a central entity plans out tasks for a team of robots with an access to the global information of the environment, which could hypothetically produce globally optimal solutions.
However, a single failure of a robot or communication link could lead to the failure of the entire system. Instead, we follow the decentralized \cite{li2020graph, zhai2021decentralized} schemes, which are more robust to the single point of failure. Each robot makes decisions and optimizes trajectories based on its own understanding of the environment, with realistic communication constraints among robots.


\subsection{Space Partitioning and Decomposition}
In robotic exploration research, space partitioning approaches decompose space into subparts or partitions and seek to cover the whole space by consecutively exploring the partitions. \citet{voronoi-based-space-partitioning} proposes Voronoi-based partitioning to coordinate multi-robot exploration, dividing the entire space into a set of polygons. \citet{unsup-clustering-space} propose using unsupervised clustering for multi-robot coordination. \citet{Voronoi-deep-rl} uses the combination of deep reinforcement learning and Voronoi-based partitions to improve coordination strategies for multi-robot exploration. 


% Figure environment removed

\subsection{Room Detection}
Division of a floorplan into rooms, or identifying the potential location and size of rooms is essential to room-based search and exploration. Most popular approach to detect rooms is Voronoi-based approach by \citet{thrun1998}, which utilizes distance transform and Voronoi graph to find critical points in the map to detect passages to the room. \citet{wurm2008coordinated} uses this idea to divide the space map into segments that can correspond to individual rooms, and generate Voronoi graph to assign targets for each robot in the multi-robot teams, in a centralized manner. 

Other works include graph-based partitioning which uses topological map to cluster nodes and build higher-level hierarchical map \cite{brunskill2007topological}, or feature-based room segmentation approaches \cite{sjoo2012semantic} which learns features and geometric shapes that match with the characteristic of rooms. More recently, \citet{3d-dynamic-scene-graphs} proposed 3D dynamics scene graph, with multiple layers in the hierarchy representing different levels of semantic structures, where rooms are composed using lower-level layer graph nodes like walls, floors, and ceiling. 

In this work, we focus on LiDAR processing approach that enables high-speed, low-compute processing onboard and robust exploration in light-degraded environments, as opposed to learning-based approaches that often require a large amount of computations and pretraining of offline datasets. We revisit the idea of distance transform map \cite{thrun1998}, but improve this idea by using a different cue detection method and space decompositions that are compatible with state-of-the-art fully autonomous multi-robot exploration algorithms. %in unknown, unfamiliar, degraded, communication-limited environments. 

\section{Problem Definition}

% Robots move through environment
Consider a team of $n$ robots ($i=1,2,...,n$) that are exploring an indoor environment. Denote the trajectory traveled by each robot as $\xi_i$. The robots build a map of the environment with LiDAR, which is represented with a 3D occupancy voxel grid in a shared coordinate system (each voxel represents a cube with side length $0.2$m).
% Sensor model
The robots observe surfaces of the environment with a primary sensor, such as a camera or RGBD sensor with limited field of view. The intersection between the occupied voxels of the LiDAR map and the primary sensor field of view is denoted by the voxel grid $O_i$ (for robot $i$). The observation model for the primary sensor can be defined based on the specifications of the robot and the application; we model the primary sensor as a forward-facing fish-eye camera with a $5$m field of view, modelling occlusions with ray casting, as in~\cite{best2022rss}.

Let us assume that there are total $K$ rooms to be explored in the building, and denote $V^\mathrm{rm}_j$ ($j=1,2,...,K$) as voxels within each room.
Then, the total number of voxels in these rooms that are observed by the primary sensor of robot $i$ is
\begin{align}
\Bigl|\underset{j=1,...,K} {\bigcup}{(O_i \cap V^\mathrm{rm}_j)}\Bigr|
\label{room-voxel-objective}
\end{align}
($|\cdot|$ is the number of voxels).
In this work, we will reward robots for exploring rooms in a building rather than increasing total coverage by traversing corridors and hallways. 


In the multi-robot setting, our objective is to maximize the union of such voxels, explored by all robots collectively.
Each robot finds its own trajectory $\xi_i$ seeking to maximize the union of observed voxels in rooms by all robots,
\begin{align}
\xi_1^*, \xi_2^*, ..., \xi_n^* = \underset{\xi_1, \xi_2, ..., \xi_n}{\arg \max} \Bigl|\underset{i=1..n}{\bigcup} \bigl(\underset{j=1..K} {\bigcup}(O_i \cap V^\mathrm{rm}_j)\bigr)\Bigr|
\label{eq:multi-robot-objective}
\end{align}
given a fixed time. The robots will solve \eqref{eq:multi-robot-objective} approximately and in a decentralized manner, so that each robot will plan its own path $\xi_i^*$ while communicating with other robots.


\section{Distributed Exploration Method}
%\gb{Fig.~\ref{fig:diagram} needs to be referenced somewhere, probably at the start of this section}
In this section, we present our autonomous distributed exploration pipeline, which coordinates the behaviors of multiple robots exploring in a building composed of multiple rooms. The pipeline overview is displayed in Fig.~\ref{fig:diagram}. We first explain preliminary background on the autonomous robot exploration baseline \cite{best2022rss}, which we build upon and improve (Sec.~\ref{sec:baseline}). Then, we describe the two main building blocks of our method, geometric cue extraction (Sec.~\ref{sec:geometric_cues}, Alg.~\ref{alg:extractCues}) and spherical decomposition of space (Sec.~\ref{sec:spherical_decomposition}, Alg.~\ref{alg:Spheres}). We also explain the multi-robot communication and target assignment (Sec.~\ref{sec:communication}, Alg.~\ref{alg:target}), and finally explain how they all come together to form our Multi-Robot Multi-Room (MRMR) method (Sec.~\ref{sec:exploration_system}, Alg.~\ref{alg:MRMR}). 

\subsection{Preliminary: Autonomous Exploration Baseline}
\label{sec:baseline}
The baseline method we use is the open source autonomous aerial robot exploration pipeline \cite{best2022rss} developed by Team Explorer, to compete in DARPA Subterranean Challenge. This baseline enables robots to navigate in a wide range of challenging underground or indoor environments such as a mine, subway, tunnel, or cave, with limited communication, sensor coverage, and light conditions. 

The baseline method leverages LiDAR sensors to discover the geometry of surrounding environments, using OpenVDB \cite{open-vdb} as a data structure for representing map occupancy grids, and SLAM solutions generated by Super Odometry \cite{super-odometry}. 

The path planning component of the baseline method, which we focus on in this work, can be loosely categorized as a frontier-based exploration approach with graph search and selection heuristics.
%\micah{"using vision and range sensors" The sensors don't make the viewpoints.}
Using vision and range sensors, the robot generates a set of \textit{viewpoints} at the frontiers. The viewpoints are scored with heuristics, and the viewpoints with high scores are selected. Then, paths are planned to reach the viewpoints. RRT-Connect \cite{RRT-connect} is used to find a feasible global path to viewpoints, and A* graph search and motion primitives are used for local path planning.

For multi-robot exploration, onboard communication hardware is used, and the plans for robots are coordinated implicitly by sharing knowledge of the world. This includes knowing the take-off locations of each robot, and communicated shared map in a global reference frame. 



\subsection{Extracting Geometric Cues of Doors and Rooms}
\label{sec:geometric_cues}

The first component of our method is detection of doors and rooms via geometric cue extraction, as shown in Alg.~\ref{alg:extractCues}. The intuition for this algorithm is that \emph{saddle points} on the distance transform are approximately equivalent to the locations of doors. We also extract local maxima to approximate locations of centers of  open spaces such as rooms.

% Figure environment removed



The robot incrementally obtains 3D point cloud observations from onboard sensors which it uses to maintain a voxel grid map. The 3D point cloud map voxels $O$ are flattened into 2D binary map $B$, by taking all occupied voxels within the range of height $z \in [z_{\text{low}},z_{\text{high}}]$. In practice, we set $z_{\text{low}}=0$ and $z_{\text{high}}=1.8$. Any obstacles like walls are represented as 1, and any free spaces like narrow passageways or corridors are represented as 0, in this binary map. %This allows reasoning over 2D space, rather than directly handling 3D data. 

\begin{algorithm}[t]
\caption{ExtractCues}\label{alg:extractCues}
\begin{algorithmic}[1]
\Require 3D Point Cloud Map Voxel Grid Data $O$
\State Create 2D binary map $B$ by taking $z \in [z_{\text{low}},z_{\text{high}}]$
\State Get distance map $M$ = \textit{distanceTransform(filter$(B)$)}
\State Compute Hessian matrix $H \gets \textit{Hessian}(M)$
\
\State Obtain a set of saddle points (doors) $P^\mathrm{sadd}$ from $H$
\State Obtain a set of local maxima (center) $P^\mathrm{max}$ from $H$
\State For each $c\in P^\mathrm{max}$, obtain distance $\delta$ between $c$ to the closest wall using distance map $M$. This set is $\Delta$
 \Ensure $P^\mathrm{sadd}$ as potential doors, $(P^\mathrm{max},\Delta)$ as potential center locations of rooms and minimum distances to wall  
\end{algorithmic}
\end{algorithm}


Then the robot converts the binary map $B$ into distance transform map $M$, which computes the minimal distance to closest occupied pixel for all pixels. Then the robot obtains a second-order partial derivative matrix of the distance map, or hessian $H$, from which it can compute saddle points $P^\mathrm{sadd}$ and local maxima $P^\mathrm{max}$, using the determinant \footnote{For second-order derivative test, we empirically found that detecting points that meet the condition $\det(x,y)<-0.1$ (instead of $\det(x,y)<0$) works well in practice to find saddle points. Similarly, we set $f_{\mathrm{xx}}(x,y)<-0.1$ as a threshold for detecting local maxima.} : 
%\micah{I adjusted typesetting to use upright letters with mathrm and such.}
\begin{align}
\det(x,y) = f_{\mathrm{xx}}(x,y) f_{\mathrm{yy}}(x,y) - (f_{\mathrm{xy}}(x,y))^2, 
\label{determinant}
\end{align}
and with $P^\mathrm{sadd}$ and $P^\mathrm{max}$ defined as
\begin{align}
\begin{cases}
(x,y) \in P^\mathrm{sadd} & \text{if}\ \det(x,y) < -0.1 \\
(x,y) \in P^\mathrm{max} & \text{if}\ \det(x,y) >0\ \& \ f_{\mathrm{xx}}(x,y) < -0.1
\end{cases}
\label{saddle-localmax}
\end{align}



A local maximum $c \in P^\mathrm{max}$ is a point, whose distance $\delta \in \Delta$ to the wall is the highest compared to other neighboring pixels. In practice, these points correspond to the center points of spaces surrounded by walls, such as rooms.

The more interesting part is the use of saddle points (visualized in Fig.~\ref{saddle-points}).
A saddle point refers to a critical point that is not a local extremum; any points that are a relative minimum along one axis and relative maximum along  another perpendicular axis are classified as saddle points.
In our setting, doors are detected as saddle points: a door point is at its relative maximum in terms of distance to a wall along the wall-axis (the voxels occupied by walls have zero distance to the walls); the door point is at its relative minimum along the crossing axis, which is composed of free voxels that lie perpendicular to the walls, that have higher distance to wall than door. %Lastly, there are some saddle points that are not doors; for example, a narrow passageways between walls that are not doors can be detected as false positives, but they are negligible in practice. 
%\micah{Do we classify all saddle points as doors? How do we deal with saddle points that are not doors?}


\subsection{Spherical Decomposition of Space}
\label{sec:spherical_decomposition}

We make a design choice to represent the space by decomposing it into a set of spheres. A sphere is defined by the (x,y,z) coordinates of the center and a radius. The output of Alg.~\ref{alg:extractCues} includes pairs of local maximum and its distance to the wall; by taking each pair of local maximum and its distance from the wall, we can generate spheres of center at $c \in P^\mathrm{max}$ and radius of $\delta \in \Delta$. 

% Figure environment removed

We argue that this is a compact representation of the world, in the context of room exploration. A typical square-shaped room, as shown in Fig.~\ref{spheres}(a), can be represented by using a single sphere, whose center is located in the middle of the room and the sphere’s boundary is adjoining to the walls of the room (as the radius is distance from the center to the closest wall). Other types of rooms, for example, a concave room like Fig.~\ref{spheres}(b) can be composed into multiple convex parts and represented by a set of spheres.

Why do we use spheres as representation for spaces like rooms? First, it is efficient for robot’s room exploration. In order for a robot to cover the space and observe information around $360^{\circ}$ surroundings, it suffices to command the robot to reach to the center of the sphere; at the center of the sphere, the robot can observe a large area surrounded by the wall, as the center is the local maximum on the distance transform and radius is its distance to occupied space. Second, it is an efficient data structure (just center and radius) to be shared among robots under limited communication. This will be explained in the Sec.~\ref{sec:communication}.





\begin{algorithm}[t]
\caption{UpdateSpheres}\label{alg:Spheres}
\begin{algorithmic}[1]
\Require A pre-existing set of spheres $S$ (previous spherical decomposition of space), a set of pair of local maxima and distance $(P^\mathrm{max}, \Delta)$ from the output of Alg.~\ref{alg:extractCues} %New spheres $Q$
\For {each $(c, \delta)$ pair} \Comment{$c\in P^\mathrm{max}, \delta \in \Delta$}
\State generate a sphere $s$ of center $c$ and radius $\delta$
\For{each pre-existing sphere $s' \in S$}
\State $r1$ and $r2 \gets $ radius of $s$ and $s'$
\State $d \gets$ distance between centers of $s$ and $s'$
\State \textbf{if} $d<0.5(r_1+r_2)$: \textit{\Comment{if spheres are too close}}
\State \hspace{2mm} choose / save the larger one between $s$ and $s'$
\State \textbf{elif} $d<(r_1+r_2)$: \textit{\Comment{if medium-distanced}}
\State \hspace{2mm} merge the spheres by weighted-average them
\EndFor
\State \textbf{if} $s$ isn't merged to any pre-existing $s' \in S$:
\State \hspace{2mm} simply add $s$ to $S$
\EndFor
%\For{each new incoming sphere $q \in Q$}
%\For{each pre-existing sphere $s \in S$}
%\State $(s_x,s_y),(q_x,q_y)$: $(x,y)$ positions of center of $s,q$
%\State $(s_x,s_y) \gets$ $(x,y)$ coordinates of center of $s$
%\State $(q_x,q_y) \gets$ $(x,y)$ coordinates of center of $q$
%\State $r_s, r_q$: radius of $s$ and $q$
%\State $d \gets$ distance between $(s_x,s_y)$ and $(q_x,q_y)$
%\State \textbf{if} $d<0.5(r_s+r_q)$:
%\State \hspace{4mm}$s\leftarrow q \text{ \textbf{if} } r_s < r_q$
%\State \hspace{3mm} \textbf{if} $r_1 < r_2$:
%\State \hspace{6mm} replace $s$ with $q$
%\State \textbf{else if} $0.5(r_s+r_q)<d<(r_s+r_q)$:
%\State \hspace{3mm} 
%$x' = (r_ss_x+r_qq_x)/(r_s+r_q)$
%\State \hspace{3mm}
%$y' = (r_ss_y+r_qq_y)/(r_s+r_q)$
%\State \hspace{3mm}
%$r'=((r_s+r_q+d)/2) \cdot (d/(r_s+r_q))$
%$x=\frac{(r_1p_x+r_2q_x)}{(r_1+r_2)}, y=\frac{(r_1p_y+r_2q_y)}{(r_1+r_2)}$
%\State\hspace{3mm} $r=\frac{r_1+r_2+d}{2} \cdot\frac{d}{r_1+r_2}$
%\State \hspace{3mm} $s$ $\gets$ sphere with center $(x',y',1)$, radius $r'$
%\EndFor
%\State \textbf{if} $q$ isn't merged to any $s$ of the $S$
%\State \hspace{3mm} add $q$ to $S$
%\EndFor
\Ensure $S$
\end{algorithmic}
\end{algorithm}



The spherical decomposition of the space is not static, as the robot updates the map of the environment by incrementally obtaining more information of the surrounding (e.g. discovers a new wall). An exemplary scenario is Fig.~\ref{spheres} (c),(d),(e): When the robot moves straight along the corridor, the geometry beyond the wall is unknown, so it assumes that there is a large open space, as in Fig.~\ref{spheres}(c). When observations are made through the door into the room, a partial map of the room is formed. As in Fig.~\ref{spheres}(d), the robot updates the sphere so that it adjoins the walls. In Fig.~\ref{spheres}(e), the robot has finished exploring the first room, and starts collecting data on the right side of the first room, adjusting the size and location of a new sphere accordingly. 

While rooms are represented by spheres, they are not equivalent with rooms; in fact, any free space between walls or surrounded by walls can be represented by spheres. For instance, corridors are also filled with spheres. In practice, their existence doesn't affect the algorithm, as the target sphere is chosen after a door is reached. (See Sec.~\ref{sec:exploration_system})

Algorithm~\ref{alg:Spheres} is a process of maintaining and updating a set of spheres, when new inputs (pairs of local maxima and distance to the wall) are continuously fed into the system. If the new sphere is closely located to any of the pre-existing spheres, it is merged into one of them; otherwise, it is simply added to the spherical decomposition set.  


\subsection{Multi-Robot Communication and Target Assignment}
\label{sec:communication}

As discussed in Sec.~\ref{sec:geometric_cues}, the robot extracts geometric cues and detects saddle points as potential doors, and it also represents rooms with spherical decomposition of spaces as discussed in Sec.~\ref{sec:spherical_decomposition}. For distributed target assignment among multiple robots, we utilize the doors and spheres as information to be shared. Each robot maintains a set of doors $D_\mathrm{r}$ and a set of spheres $S_\mathrm{r}$ it has reached based on its local map; these two sets are also shared with other robots via communication. Sets of doors (lists of points in 3D coordinates) and sets of spheres (lists of points and radii) serve as a compact representation of the explored space. %\footnote{Communication for this approach is quadratic in the number of robots. Our approach seeks to use compact representations and messages to enable coordination across small teams of robots.} 
 Each robot then maintains a set of doors $D_\mathrm{o}$ and spheres $S_\mathrm{o}$ reached by other robots alongside the local sets ($D_\mathrm{r}$, $S_\mathrm{r}$).

% Figure environment removed

\begin{algorithm}[t]
\caption{TargetAssignmentAndCommunication}\label{alg:target}
\begin{algorithmic}[1]
\Require Set of spheres and doors robot reached $S_\mathrm{r}$, $D_\mathrm{r}$, set of spheres and doors other robots reached $S_\mathrm{o}$, $D_\mathrm{o}$, threshold parameters $\epsilon_d=1.0$ and $\epsilon_s=1.5$
%\begin{itemize}
%\item a set of spheres it already reached, $S_\mathrm{r}$
%\item a set of doors it already reached, $D_\mathrm{r}$
%\item a set of spheres other robots have reached $S_\mathrm{o}$
%\item a set of doors other robots have reached $D_\mathrm{o}$
%\end{itemize}
\State \textbf{Targeting a new door:}
\State \hspace{2mm} given a set of candidate doors $D$, exclude the doors that were already reached, thus $D \gets D\setminus D_\mathrm{r}$ 
\State \hspace{2mm} \textbf{for} each $d \in D$:
\State \hspace{4mm} \textbf{if} $\exists d'\in D_\mathrm{o}
$ such that $dist(d,d') < \epsilon_d$:
\State \hspace{6mm} exclude this $d$, thus $D \gets D\setminus \{d\}$
\State \hspace{2mm} choose the closest $d \in D$ as a new target door
\State \textbf{Targeting a new sphere:}
\State \hspace{2mm} given a set of candidate spheres $S$, exclude the spheres that were already reached, thus $S \gets S\setminus S_\mathrm{r}$ 
\State \hspace{2mm} \textbf{for} each $s \in S$:
\State \hspace{4mm} \textbf{if} $\exists s'\in oS$ such that $dist(s.center,s'.center) < \epsilon_s$:
\State \hspace{6mm} exclude this $s$, thus $S \gets S\setminus \{s\}$
\State \hspace{2mm} choose the closest $s \in S$ as a new target sphere
\State \textbf{Inter-Robot Communication:}
\State \hspace{2mm} robot publishes spheres $S_\mathrm{r}$ and doors $D_\mathrm{r}$ it reached
\State \hspace{2mm} robot subscribes to other robots' published $S_\mathrm{r}, D_\mathrm{r}$ and save them to $S_\mathrm{o}$ and $D_\mathrm{o}$
\end{algorithmic}
\end{algorithm}

When targeting a new door, the robot not only excludes doors reached by itself but also doors reached by other robots.
Any candidate door $d$ that is distanced less than $\epsilon_d$ from any of the doors reached by other robots $D_\mathrm{o}$ is excluded.
Likewise, robots exclude both spheres they have reached along with spheres reached by other robots when targeting a new sphere. The details of this distributed target assignment scheme are explained in Alg.~\ref{alg:target}. This algorithm runs on each robot, without any centralized planner.

\subsection{Multi-Robot Multi-Room (MRMR) Exploration}
\label{sec:exploration_system}


\begin{algorithm}[t]
\caption{Multi-robot Multi-room (MRMR) exploration. \\ Algorithm is run on each robot. %Lines in \textcolor{teal}{green}/\textcolor{brown}{brown}: the robot \textcolor{teal}{publishes}/\textcolor{brown}{subscribes} to other robots for communication
}\label{alg:MRMR}
\begin{algorithmic}[1]
\Require robot $R$, initialize observed\_map $O$, spheres $S$ with empty sets $\{\}$, initialize $S_\mathrm{r}, D_\mathrm{r}, S_\mathrm{o}, D_\mathrm{o}$ (as defined in Alg~\ref{alg:target}) with empty sets $\{\}$
%, space partitions $P_i$, viewpoints $V_i$ with $\phi$
\For{each timestep}
%\For{each robot $r_i$}
%\State \textit{$\triangleright$ Mapping}
%\State updates $M_i$ from lidar and odometry sensors
%\State $f_i \gets \text{FrontierExtract}(O_i)$
%\State \textbf{\textit{$\triangleright$ 3D Space Partition}}
\State \textcolor{blue}{$\vartriangleright$ \textit{geometric analysis of observed map:}}
\State $P^\mathrm{sadd}, (P^\mathrm{max}, \Delta) \gets \textbf{ExtractCues}(O)$ \Comment{Alg~\ref{alg:extractCues}}
%\State $Q_t \leftarrow $ \text{spheres} $q$ of center $c\in C$ and radius  $\delta \in \Delta$
%\State $S_t \gets $\textbf{$\text{Add-Merge-Spheres}(S_{t-1}, Q_t)$}
\State $S \gets $\textbf{$\text{UpdateSpheres}(S,(P^\mathrm{max}, \Delta))$} \Comment{Alg~\ref{alg:Spheres}}
\State \textcolor{blue}{$\vartriangleright$ \textit{door finding:}}
\State \textbf{if} R is not targeting any door: 
\State\hspace{2mm} target closest door $d \in P^\mathrm{sadd}$ \Comment{Alg~\ref{alg:target}}
%\State \hspace{3mm} \textcolor{teal}{share with other robots: $R$ is targeting $d$}
\State\hspace{2mm} generate path to $d$ and execute %\textcolor{blue}{\Comment{RRT-Connect}}
%\State \hspace{3mm}
%$\xi \leftarrow \xi + $ path to $d$ \textcolor{blue}{\Comment{RRT-connect plan}}
\State \textbf{if} R reached the target door: 
\State \hspace{3mm}$D_\mathrm{r} \leftarrow D_\mathrm{r} \cup \{d\}$ \textit{\Comment{mark this door as reached}} 
%\State \hspace{2.5mm} \textcolor{teal}{share with other robots: $R$ has reached $d$}
\State \hspace{3mm}\textcolor{blue}{$\vartriangleright$ \textit{room exploration:}}
\State\hspace{2mm} target closest sphere $s\in S$ \Comment{Alg~\ref{alg:target}}
%\State \hspace{2.5mm} \textcolor{teal}{share with other robots: $R$ is targeting $s$}
%\textcolor{blue}{\Comment{RRT-Connect}}
%\State \hspace{3mm}
%$\xi \leftarrow \xi + $ path to $s.center$ \textcolor{blue}{\Comment{RRT-connect plan}}
\State \hspace{2mm} generate and execute path to $s.center$
\State \hspace{3.5mm}\textbf{if} the room is composed of multiple spheres:
\State \hspace{4mm} \textbf{while} spheres remaining in the room:
\State \hspace{6mm} target next adjacent sphere $s$ \Comment{Alg~\ref{alg:target}}
\State \hspace{6mm} generate path to $s.center$ and execute
\State \hspace{2mm}\textbf{for} each sphere $s$ that was reached:
\State \hspace{4mm} $S_\mathrm{r} \leftarrow S_\mathrm{r} \cup \{s\}$ \textit{\Comment{mark spheres as reached}}
\State \textcolor{blue}{$\vartriangleright$ \textit{multi-robot communication:}}
\State update $S_\mathrm{o}, D_\mathrm{o}$ via communication \Comment{Alg~\ref{alg:target}}
%\State \textcolor{teal}{share with other robots: completed-spheres $cS$}
%\State \textcolor{brown}{$oS \leftarrow$ completed/targeted spheres from other robots} 
%\State \textcolor{brown}{$rD \leftarrow$ reached/targeted doors from other robots}
\State \textcolor{blue}{$\vartriangleright$ \textit{no-target phase:}}
\State \textbf{if} there is no door or room to target:
\State \hspace{2mm} run baseline \cite{best2022rss} until finding next door
%\State \hspace{3mm}
%$\xi \leftarrow \xi + $ path to random frontiers
\EndFor
\Ensure Path trajectory traveled by $R$
\end{algorithmic}
\end{algorithm}

%In this subsection
Here, we explain how the previous algorithms come together and fit into one method, Multi-Robot Multi-Room (MRMR) exploration. The detailed procedure is in Alg.~\ref{alg:MRMR}.
At every timestep, the robot extracts geometric cues, detecting saddle points, local maxima, and distances to the walls, and updates the spherical decomposition of the space. Then the robot targets a door, reaches the door, and explores a room by targeting a sphere or a set of the spheres that constitute the room. Whenever the robot reaches a door and a center of the sphere, these are marked as \textit{reached}, so that it can avoid re-targeting the doors or rooms that were already explored. This information is also shared with other robots, which are concurrently running the algorithm; each robot shares what it has already reached and what other robots have already reached which enables effective distributed target assignment. Lastly, when there is no door or room to explore, the robot runs a frontier-based RRT-exploration baseline~\cite{best2022rss} until it finds the next geometric cues from observations.


 %At every timestep, the robot processes and updates 3D voxel grid data of the observed map $O_t$. From this map, the robot extracts potential locations of doors and rooms (line~3), and adjusts the locations and sizes of spheres (line~5). Then the robot seeks for a closest door $d$ that is not yet reached by itself or not targeted/reached by other robots. (line~7). Once the robot is targeting towards the door $d$, it is being shared with other robots so that they can avoid choosing the same door (line~8). 

%When the robot reaches the door, it sets a closest sphere $s$ that was not completed by itself or taken by other robots (line~14) as target sphere, and explore the room that is represented by this sphere (line~18) or a set of spheres (line~19-20). It suffices for the robot to reach to the center of the spheres, in order to get full information of the surrounding (line~16-17). When the room exploration is complete, this sphere is marked as completed (line~22), and shared with other robots (line~23), so that the other robots can avoid exploring same rooms redundantly. At each timestep, the robot receives information from other robots: the list of completed or targeted spheres by other robots, as well as the list of reached and targeted doors by other robots are shared. The information shared among robots has succinct data structures: each door is a just point of $(x,y,z)$-coordinates and each sphere is represented with a center point of $(x,y,z)$-coordinates and radius $r$, which enables an efficient communication scheme. Finally, when there is no further door or room to target, the robot falls back to the baseline frontier-based RRT-connect exploration~\cite{best2022rss} to extend its frontiers and explore the space until it receives new geometric cues from the ExtractCue method. 

%The core of this algorithm is continuous and real-time sharing of geometric cues among robots. Each robot shares which door it is targeting at the moment, which doors have been reached so far, which room it is exploring now, and which rooms are already explored. When the robot selects a target door or room, it considers the information received from other robots. It avoids selecting a door that is targeted or was reached by other robots; it also avoids exploring the same sphere repetitively. This mechanism is run on each root, and each robot plans a path trajectory to explore different rooms collectively, without any centralized planner.

% Figure environment removed


\section{Experiments \& Results}
In this section, we explain the experimental setup and display results to evaluate our algorithm, MRMR. We first elaborate on the experimental setup and discuss results for simulation environments. Then we discuss the experimental setup and results for real-robot experiments. For comparison, we chose the previous frontier-based exploration \cite{best2022rss} as a baseline, both in simulation and real-robot experiments. 

\subsection{Simulation Experiments}
\begin{table}[]
    \centering
        \caption{The details of the simulation test environments. }
    %\setlength{\extrarowheight}{10pt}
    \newlength{\mapspace}
    \setlength{\mapspace}{2pt}
    \begin{tabularx}{\linewidth}{cXccc}%{|p{1cm}|p{3cm}|p{1cm}|p{1.7cm}|}
       \toprule
       Name & Images  & Rooms & Volume ($m^3$) & Voxels\\
       \midrule
       Env1 & \raisebox{-0.5\height}{%
       % Figure removed}  & 6 & 462.6 & 9587 \\
       \addlinespace[\mapspace]
       Env2 & \raisebox{-0.5\height}{%
       % Figure removed} & 8 & 776.7 & 21313\\
       \addlinespace[\mapspace]
       Env3 & \raisebox{-0.5\height}{%
      % Figure removed}  & 5 & 402.9 & 11869\\
       \addlinespace[\mapspace]
       Env4 & \raisebox{-0.5\height}{%
      % Figure removed}  & 9 & 729.1 & 20105\\
       \addlinespace[\mapspace]
       Env5 & \raisebox{-0.5\height}{%
      % Figure removed}  & 12 & 1179.1 & 33371\\
       \bottomrule
    \end{tabularx}
    \label{tab:sim-environments}
\end{table}

\subsubsection{Experimental Setup}
% Figure environment removed
% We divide this section into simulation experiments and real-world experiments. 
We used the SubT UAV code released by \cite{best2022rss}, developed to simulate real subterranean environments. We designed five new indoor simulation environments with different configurations and numbers of rooms, and 
Table~\ref{tab:sim-environments} illustrates the floorplan, number of rooms, volume of the total room spaces (in $m^3$), and number of grid voxels in rooms of each environment. %Each environment is composed of multiple rooms, with different lengths of corridors, different sizes of halls, and different complexities of floorplan. 
We tested with different numbers of robots ($n=1,2,3$) and measured the number of union of observed voxels in the rooms by the robots collectively with their primary sensor cameras over time (max time = 2 minutes).
Each graph curve is an average of three independent runs. We also measured percentages of voxels in rooms observed (out of total voxels in rooms) and number of rooms (out of total number of rooms) by each method, and displayed the results in a separate table.

% Figure environment removed

\subsubsection{Results}

\begin{table}[]
    \centering
        \caption{The summary of simulation experiments}
    \begin{tabular}
{p{1.4cm}p{0.75cm}p{0.75cm}p{0.75cm}p{0.75cm}p{0.75cm}p{0.75cm}}
        \toprule
        \textbf{Method} & \multicolumn{2}{c}{\textbf{1 Robot}} & \multicolumn{2}{c}{\textbf{2 Robots}}  & \multicolumn{2}{c}{\textbf{3 Robots}} \\
        & Voxels & Rooms & Voxels & Rooms & Voxels & Rooms \\
       \midrule
       Baseline & 26.72\% & 17.36\% & 40.46\% & 39.28\% & 51.29\% & 53.95\% \\
       MRMR & \textbf{36.73\%} & \textbf{39.63\%} & \textbf{59.28\%} & \textbf{48.95\%} & \textbf{64.51\%} & \textbf{60.68\%} \\
       \addlinespace[4pt]
       \emph{Improvement} & 37.46\% & 128.28\% & 46.51\% & 24.62\% & 25.77\% & 12.47\% \\
       \bottomrule
    \end{tabular}
    \label{tab:sim-summary}
\end{table}


The results of the simulation experiments are displayed in Fig.~\ref{sim-graphs}. The plots in the first row of the figure are single-robot case, and the plots in the second and third row are multi-robot cases (2-robots and 3-robots). These plots demonstrate that our Method (MRMR) observes more voxels in the rooms than frontier-based baseline, across different environments and different number of robots. In average, MRMR observes $37.46\%, 46.51\%, 25.77\%$ more room grid voxels than baseline in single-robot, two-robot, and three-robot cases. In the Table~\ref{tab:sim-summary}, we reported how MRMR method observes more voxels (out of total room voxels) and explore more rooms (out of total number of rooms) than baseline; MRMR outperforms the baseline both in terms of voxels observed and number of rooms explored. 

More qualitative explanations of difference between the baseline and MRMR are following: as shown in Fig.~\ref{sim-visualize}(a), using the baseline, robots first choose to travel corridors fast by moving straight to increase the coverage area without turning to the doors or rooms. In contrary, as in Fig.~\ref{sim-visualize}(b), the robots quickly turn directions to reach the doors and enter rooms, rather than simply increasing coverage. Using the baseline (Fig.~\ref{sim-visualize}(c),(e)), multiple robots often enter the same rooms redundantly and sometimes miss exploring rooms; in contrary our method (Fig.~\ref{sim-visualize}(d),(f)) enables robots to uniquely visit each room without redundancy. 

\subsection{Real-Robot Experiments}

\subsubsection{Experimental Setup}

For real-world experiments, we use custom quadrotors as shown in  Fig.~\ref{drone}. Each drone is 68cm wide, 81cm long, weighs 5.2kg, and is equipped with a Velodyne VLP-16 Lidar, Realsense RGBD sensors, Intel NUC computer with Intel Core i7-8550U CPU.
We run experiments in an abandoned hospital in Pittsburgh, PA.
The main results compare our method to the baseline with two to three robots; drones fly sequentially as the mechanism for inter-robot collision avoidance for real robots is limited.
\textit{Additionally}, we report results of a trial with three robots flying \textit{simultaneously} with our method only.
We report the number of voxels in rooms, as well as number of rooms collectively observed by the robots for both experiments.

\subsubsection{Results}

% Figure environment removed

\begin{table}[]
    \centering
        \caption{The summary of real-robot experiments over three trials}
    \begin{tabularx}{\linewidth}{lcccccccc}
    \toprule
       & \multicolumn{4}{c}{\textbf{2 Robots}} & 
       \multicolumn{4}{c}{\textbf{3 Robots}} \\
       \cmidrule(lr){2-5}\cmidrule(lr){6-9}
       \# & \multicolumn{2}{c} {Baseline} & \multicolumn{2}{c} {MRMR} & 
       \multicolumn{2}{c} {Baseline} & 
       \multicolumn{2}{c} {MRMR}
 \\
   & Vxl. & Rm. & Vxl. & Rm. & Vxl. & Rm. & Vxl. & Rm. \\ 
 \midrule
       1. & 6204 & 3 & \textbf{10285} & \textbf{5} & 11584 & 5 & \textbf{15732} & \textbf{8} \\
      2. & \textbf{10721} & 5 & 9428 & 5 & 14336 & 7 & \textbf{17320} & \textbf{9} \\
      3. & 8437 & 4 & \textbf{11724} & \textbf{6} & 14561 & 7 & \textbf{15976} & \textbf{8} \\
       \bottomrule
    \end{tabularx}
    \label{tab:real-summary}
\end{table}

The results of the real-robot experiments are displayed in the Table~\ref{tab:real-summary}.
In the comparative tests, both in two-robot and three-robot cases, the robots generally observe more grid voxels and explore more rooms using MRMR.
The exception is trial 2 of the two-robot case where the baseline outperforms MRMR but by an insignificant amount.
On average, MRMR observes $30.66\%, 22.09\%$ more grid voxels in rooms than the baseline, in two-robot and three-robot cases respectively. Additionally, Fig.~\ref{real-compare} visualizes the results or running both methods with two robots.
Finally, in the simultaneous execution experiment (Fig.~\ref{real-visualize}), the robots successfully explored 8 rooms using MRMR, observing $16793$ voxels in the rooms, with three drones.








% \section{RESULTS}




% Text heads organize the topics on a relational, hierarchical basis. For example, the paper title is the primary text head because all subsequent material relates and elaborates on this one topic. If there are two or more sub-topics, the next level head (uppercase Roman numerals) should be used and, conversely, if there are not at least two sub-topics, then no subheads should be introduced. Styles named ÒHeading 1Ó, ÒHeading 2Ó, ÒHeading 3Ó, and ÒHeading 4Ó are prescribed.
%% Figure environment removed
%% Figure environment removed

% Figure environment removed

\section{Conclusion \& Future Work}
We proposed a multi-robot multi-room autonomous exploration pipeline (MRMR) that methodically explores and performs detailed observations of rooms in a building and coordinates behaviors of robot teams. To this end, we presented a geometric cue extraction method that detects locations of doors and rooms from point clouds, and spherical decomposition of open spaces for target assignment. We validated performance of our method in simulated and real experiments.
Some limitations are that the approach is only applicable to single story buildings due to flattening of 3D voxels into a 2D distance transform map, and that the collision avoidance among aerial robots is implicit, rather than explicit. For future work, we plan to extend our approach to multi-floor buildings and to consider exploration with heterogeneous multi-robot systems and to improve such systems with advanced vision and learning modules. 


% The balance command serves this purpose
%\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
{
    \bibliographystyle{IEEEtranN}
    \scriptsize
    \balance
    \bibliography{IEEEabrv, IEEEexample}
}

\end{document}
