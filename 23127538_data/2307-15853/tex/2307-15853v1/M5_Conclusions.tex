\section{Conclusions}\label{sect:conclusion}
In this work, we propose to use k-th percentile performance (\KPP) instead of widely used average performance as a metric to evaluate the realistic worst-case performance of a DNN model. By analyzing the properties of DNN models and noise injection-based training, we show that the conventional Gaussian noise injection training is far from optimal in improving \KPP. Thus, we propose TRICE which injects right-censored noise during training. Extensive experiments show that TRICE clearly outperforms SOTA baselines in improving the k-th percentile performance of DNN models.