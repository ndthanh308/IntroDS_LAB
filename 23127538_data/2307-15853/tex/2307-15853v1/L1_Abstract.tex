\begin{abstract}
%Compute-in-Memory (CiM) using non-volatile memory (NVM) devices is a promising candidate for deep neural network (DNN) acceleration thanks to their high memory density and superior energy efficiency.
% One possible obstacle to its mass deployment is that NVMs suffer from device variations, resulting in weights mapped to the devices deviating from the desired value and causing DNN performance degradation.
%However, a significant challenge arises from device variations in NVMs, which causes weights mapped to the devices to deviate from the desired values, leading to performance degradation in DNNs.
% Various approaches have been proposed to tackle this issue, but most of them only focus on improving the average performance of DNNs under the impact of device variations.
%Although various approaches have been proposed to address this issue, most of them focus on improving the average performance of DNNs under the impact of device variations. 
% Although suitable for general-purpose scenarios, these approaches fall short in safety-critical applications.
% This would work fine in general-purpose situations but is not suitable for safety-critical scenarios.
%Alternatively, in this work, we propose to use k-th percentile performance to capture the worst-case performance of DNN models. 
% By formally analyzing the properties of k-th percentile performance of DNNs and the training process of noise injection DNN training process, we show that injecting a novel right-censored Gaussian noise instead of the conventional Gaussian noise significantly improves the k-th percentile performance of DNNs. 
% The proposed method improves the k-th percentile performance by up to 26.01\% compared with state-of-the-art methods used to improve DNN robustness under the impact of device variations.

Compute-in-Memory (CiM), built upon non-volatile memory (NVM) devices, is promising for accelerating deep neural networks (DNNs) owing to its in-situ data processing capability and superior energy efficiency. Unfortunately, the well-trained model parameters, after being mapped to NVM devices, can often exhibit large deviations from their intended values due to device variations, resulting in notable performance degradation in these CiM-based DNN accelerators. There exists a long list of solutions to address this issue. However, they mainly focus on improving the mean performance of CiM DNN accelerators. How to guarantee the worst-case performance under the impact of device variations, which is crucial for many safety-critical applications such as self-driving cars, has been far less explored. In this work, we propose to use the k-th percentile performance (\KPP) to capture the realistic worst-case performance of DNN models executing on CiM accelerators.
Through a formal analysis of the properties of \KPP~and 
%the training process of 
the noise injection-based DNN training, we demonstrate that injecting a novel right-censored Gaussian noise, as opposed to the conventional Gaussian noise, significantly improves the \KPP~of DNNs.
We further propose an automated method to determine the optimal hyperparameters for injecting this right-censored Gaussian noise during the training process. Our method achieves up to a 26\% improvement in \KPP~compared to the state-of-the-art methods employed to enhance DNN robustness under the impact of device variations.

\end{abstract}

% \begin{IEEEkeywords}
% component, formatting, style, styling, insert
% \end{IEEEkeywords}