\section{Related Works}\label{sect:related}

\subsection{Crossbar-based Computing Engine}\label{sec:2.1}

% Figure environment removed

The computation engine driving NVCiM DNN accelerators is the crossbar array structure, which can perform matrix-vector multiplication in a single clock cycle. Crossbar arrays store matrix values (\emph{e.g.}, weights in DNNs) at the intersection of vertical and horizontal lines using NVM devices (\emph{e.g.}, RRAMs and FeFETs), while vector values (\emph{e.g.}, inputs for DNNs) are fed through horizontal data lines (word lines) in the form of voltage. The output is then transmitted through vertical lines (bit lines) in the form of current. While the crossbar array performs calculations in the analog domain according to Kirchhoff's laws, peripheral digital circuits are needed for other key DNN operations such as shift \& add, pooling, and non-linear activation. Additional buffers are also needed to store intermediate data. 
% Digital-to-analog (DAC) and analog-to-digital converters (ADC) are needed between different components. Specifically, DACs are used to convert digital input data into voltage levels, and ADCs or sense amplifiers are used to convert analog output currents into digital values.
Digital-to-analog and analog-to-digital conversions are also needed between components in different domains.

% % Figure environment removed


Crossbar arrays based on NVM devices are subject to a number of sources of variations and noise, including spatial and temporal variations. Spatial variations arise from defects that occur during fabrication and can be both local and global in nature. In addition, NVM devices are susceptible to temporal variations that result from stochastic fluctuations in the device material. These variations in conductance can occur when the device is programmed at different times. Unlike spatial variations, temporal variations are usually independent of the device but could be subject to the programmed value~\cite{feinberg2018making}.
For the purpose of this study, we have considered the non-idealities to be uncorrelated among the NVM devices. However, our framework can be adapted to account for other sources of variations with appropriate modifications.

\subsection{Evaluating DNN Robustness in the Presence of Device Variations}

Most existing research uses Monte Carlo (MC) simulations to assess the robustness of NVCiM DNN accelerators in the presence of device variations. This process typically involves extracting a device variation model and a circuit model from physical measurements. The DNN to be evaluated is then mapped onto the circuit model, and the desired value for each NVM device is calculated. In each MC run, one instance of a non-ideal device is randomly sampled from the device variation model, and the actual conductance value of each NVM device is determined. DNN performance (\emph{e.g.}, classification accuracy) in this non-ideal accelerator, is then recorded. This process is repeated numerous times until the collected DNN performance distribution converges. Existing practices~\cite{he2019noise, liu2019fault} generally include around 300 MC runs. This number of MC runs is empirically sufficient according to the central limit theorem~\cite{yan2021uncertainty}.

Only a few researchers are focusing on the worst-case scenarios of NVCiM DNN accelerators in the presence of device variations. A line of research~\cite{wu2020adversarial, tsai2021formalizing, yan2022computing} focuses on determining the worst-case performance by identifying weight perturbation patterns that can cause the most significant decrease in DNN inference performance, while still adhering to the physical bounds of device value deviations. 
One representative work
~\cite{yan2022computing} shows that DNN classification accuracy can drop to random guesses level when adding a less than 3\% perturbation to weights. However, the likelihood of such a worst-case scenario occurring is lower than $<10^{-100}$, which can be safely ignored in common natural environments~\cite{yan2022computing}. Thus, such kinds of worst-case analyses are impractical in terms of accessing the robustness of an NVCiM DNN accelerator.

Thus, in this work, we advocate using k-th percentile performance, a metric that is both practical and precise, for capturing the worst-case performances of a DNN model.


\subsection{Addressing Device Variations}\label{sec:2.3}
Various approaches have been proposed to deal with the issue of device variations in NVCiM DNN accelerators. Here we briefly review the two most common types: enhancing DNN robustness and reducing device variations. 

A common method used to enhance DNN robustness in the presence of device variations is variation-aware training~\cite{jiang2020device,peng2019dnn+,he2019noise,yang2022tolerating}. Also known as noise injection training, the method injects variation to DNN weights in the training process, which can provide a DNN model that is statistically robust in the presence of device variations. In each iteration, in addition to traditional gradient descent, an instance of variation is sampled from a variation distribution and added to the weights in the forward pass. In the backpropagation pass, the same noisy weight and noisy feature maps are used to calculate the gradient of weights in a deterministic and noise-free manner. Once the gradients are collected, this variation is cleared and the variation-free weight is updated according to the previously collected gradients. The details of noise injection training are shown in Alg.~\ref{alg:ni}. Another fashion of training more robust DNN weights is CorrectNet~\cite{eldebiky2022correctnet}. This approach uses a modified Lipschitz constant regularization during DNN training so that the regularized weights are less prone to the impact of device variations. Other approaches include designing more robust DNN architectures~\cite{jiang2020device, yan2021uncertainty, gao2021bayesian} and pruning~\cite{chen2021pruning}.

% \setlength{\textfloatsep}{0.5\baselineskip plus  0.2\baselineskip minus 0.4\baselineskip}
\begin{algorithm}[b]
\caption{NoiseTrain~($\mathcal{M}$, $\mathbf{w}$, $\mathcal{D}ist$, $ep$, $\mathbf{D}$, $\alpha$)}
\begin{algorithmic}[1]\label{alg:ni}
\STATE \cmtColor{// INPUT: DNN topology $\mathcal{M}$, DNN weight $\mathbf{w}$, noise distribution $\mathcal{D}ist$, \# of training epochs $ep$, dataset $\mathbf{D}$, learning rate $\alpha$;}

% \STATE Initialize three DNN models $\mathcal{M}(\mathbf{w})$ of topology $\mathcal{M}$;
\FOR{($i=0$; $i < ep$; $i++$)}
    \FOR{$x$, $GT$ in $\mathbf{D}$}
        \STATE Sample $\Delta\mathbf{w}_i$ from $\mathcal{D}ist$;
        \STATE $loss = $ CrossEntropyLoss($\mathcal{M}(\mathbf{w}+\Delta\mathbf{w}_i, x)$, $GT$);
        \STATE $\mathbf{w} = \mathbf{w} - \alpha \frac{\partial loss}{\partial \mathbf{w}+\Delta\mathbf{w}_i}$
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

To reduce device variations induced device value deviation, write-verify\cite{shim2020two, yao2020fully} is commonly used during the programming process. 
\todo{An NVM device is first programmed to an initial state using a pre-defined pulse pattern. Then the value of the device is read out to verify if its conductance falls within a certain margin from the desired value (\emph{i.e.}, if its value is precise). If not, an additional update pulse is applied, aiming to bring the device conductance closer to the desired one. This process is repeated until the difference between the value programmed into the device and the desired value is acceptable. }
This approach is highly effective in reducing the device value deviations, but the process typically requires a few iterations, which is time-consuming. 
% Most recently, researchers have demonstrated that it is possible to only selectively write-verify a small number of critical devices to maintain the average accuracy of a DNN \cite{yan2022swim}. 
There are also various circuit design efforts~\cite{shin2021fault, jeong2022variation} that try to 
mitigate the device variations. 

% \setlength{\textfloatsep}{1.55\baselineskip plus  0.2\baselineskip minus 0.4\baselineskip}

