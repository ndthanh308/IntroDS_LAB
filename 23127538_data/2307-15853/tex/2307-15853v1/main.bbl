\begin{thebibliography}{10}

\bibitem{ramesh2021zero}
A.~Ramesh, M.~Pavlov, G.~Goh, S.~Gray, C.~Voss, A.~Radford, M.~Chen, and
  I.~Sutskever, ``Zero-shot text-to-image generation,'' in {\em International
  Conference on Machine Learning}, pp.~8821--8831, PMLR, 2021.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, {\em et~al.}, ``Language
  models are few-shot learners,'' {\em Advances in neural information
  processing systems}, vol.~33, pp.~1877--1901, 2020.

\bibitem{jiang2020device}
W.~Jiang, Q.~Lou, Z.~Yan, L.~Yang, J.~Hu, X.~S. Hu, and Y.~Shi,
  ``Device-circuit-architecture co-exploration for computing-in-memory neural
  accelerators,'' {\em IEEE Transactions on Computers}, vol.~70, no.~4,
  pp.~595--605, 2020.

\bibitem{chen2016eyeriss}
Y.-H. Chen, J.~Emer, and V.~Sze, ``Eyeriss: A spatial architecture for
  energy-efficient dataflow for convolutional neural networks,'' {\em ACM
  SIGARCH computer architecture news}, vol.~44, no.~3, pp.~367--379, 2016.

\bibitem{yan2020single}
Z.~Yan, Y.~Shi, W.~Liao, M.~Hashimoto, X.~Zhou, and C.~Zhuo, ``When single
  event upset meets deep neural networks: Observations, explorations, and
  remedies,'' in {\em 2020 25th Asia and South Pacific Design Automation
  Conference (ASP-DAC)}, pp.~163--168, IEEE, 2020.

\bibitem{shafiee2016isaac}
A.~Shafiee, A.~Nag, N.~Muralimanohar, R.~Balasubramonian, J.~P. Strachan,
  M.~Hu, R.~S. Williams, and V.~Srikumar, ``Isaac: A convolutional neural
  network accelerator with in-situ analog arithmetic in crossbars,'' {\em ACM
  SIGARCH Computer Architecture News}, vol.~44, no.~3, pp.~14--26, 2016.

\bibitem{shim2020two}
W.~Shim, J.-s. Seo, and S.~Yu, ``Two-step write--verify scheme and impact of
  the read noise in multilevel rram-based inference engine,'' {\em
  Semiconductor Science and Technology}, vol.~35, no.~11, p.~115026, 2020.

\bibitem{yan2021uncertainty}
Z.~Yan, D.-C. Juan, X.~S. Hu, and Y.~Shi, ``Uncertainty modeling of emerging
  device based computing-in-memory neural accelerators with application to
  neural architecture search,'' in {\em 2021 26th Asia and South Pacific Design
  Automation Conference (ASP-DAC)}, pp.~859--864, IEEE, 2021.

\bibitem{yan2022radars}
Z.~Yan, W.~Jiang, X.~S. Hu, and Y.~Shi, ``Radars: Memory efficient
  reinforcement learning aided differentiable neural architecture search,'' in
  {\em 2022 27th Asia and South Pacific Design Automation Conference
  (ASP-DAC)}, pp.~128--133, IEEE, 2022.

\bibitem{gao2021bayesian}
D.~Gao, Q.~Huang, G.~L. Zhang, X.~Yin, B.~Li, U.~Schlichtmann, and C.~Zhuo,
  ``Bayesian inference based robust computing on memristor crossbar,'' in {\em
  2021 58th ACM/IEEE Design Automation Conference (DAC)}, pp.~121--126, IEEE,
  2021.

\bibitem{he2019noise}
Z.~He, J.~Lin, R.~Ewetz, J.-S. Yuan, and D.~Fan, ``Noise injection adaption:
  End-to-end reram crossbar non-ideal effect adaption for neural network
  mapping,'' in {\em Proceedings of the 56th Annual Design Automation
  Conference 2019}, pp.~1--6, 2019.

\bibitem{yang2022tolerating}
X.~Yang, C.~Wu, M.~Li, and Y.~Chen, ``Tolerating noise effects in
  processing-in-memory systems for neural networks: A hardware--software
  codesign perspective,'' {\em Advanced Intelligent Systems}, vol.~4, no.~8,
  p.~2200029, 2022.

\bibitem{yan2022computing}
Z.~Yan, X.~S. Hu, and Y.~Shi, ``Computing-in-memory neural network accelerators
  for safety-critical systems: Can small device variations be disastrous?,'' in
  {\em Proceedings of the 41st IEEE/ACM International Conference on
  Computer-Aided Design}, pp.~1--9, 2022.

\bibitem{feinberg2018making}
B.~Feinberg, S.~Wang, and E.~Ipek, ``Making memristive neural network
  accelerators reliable,'' in {\em 2018 IEEE International Symposium on High
  Performance Computer Architecture (HPCA)}, pp.~52--65, IEEE, 2018.

\bibitem{liu2019fault}
T.~Liu, W.~Wen, L.~Jiang, Y.~Wang, C.~Yang, and G.~Quan, ``A fault-tolerant
  neural network architecture,'' in {\em 2019 56th ACM/IEEE Design Automation
  Conference (DAC)}, pp.~1--6, IEEE, 2019.

\bibitem{wu2020adversarial}
D.~Wu, S.-T. Xia, and Y.~Wang, ``Adversarial weight perturbation helps robust
  generalization,'' {\em Advances in Neural Information Processing Systems},
  vol.~33, pp.~2958--2969, 2020.

\bibitem{tsai2021formalizing}
Y.-L. Tsai, C.-Y. Hsu, C.-M. Yu, and P.-Y. Chen, ``Formalizing generalization
  and adversarial robustness of neural networks to weight perturbations,'' {\em
  Advances in Neural Information Processing Systems}, vol.~34, 2021.

\bibitem{peng2019dnn+}
X.~Peng, S.~Huang, Y.~Luo, X.~Sun, and S.~Yu, ``Dnn+ neurosim: An end-to-end
  benchmarking framework for compute-in-memory accelerators with versatile
  device technologies,'' in {\em 2019 IEEE international electron devices
  meeting (IEDM)}, pp.~32--5, IEEE, 2019.

\bibitem{eldebiky2022correctnet}
A.~Eldebiky, G.~L. Zhang, G.~Boecherer, B.~Li, and U.~Schlichtmann,
  ``Correctnet: Robustness enhancement of analog in-memory computing for neural
  networks by error suppression and compensation,'' {\em Design, Automation and
  Test in Europe Conference (DATE) 2023}, 2023.

\bibitem{chen2021pruning}
C.-Y. Chen and K.~Chakrabarty, ``Pruning of deep neural networks for
  fault-tolerant memristor-based accelerators,'' in {\em 2021 58th ACM/IEEE
  Design Automation Conference (DAC)}, pp.~889--894, IEEE, 2021.

\bibitem{yao2020fully}
P.~Yao, H.~Wu, B.~Gao, J.~Tang, Q.~Zhang, W.~Zhang, J.~J. Yang, and H.~Qian,
  ``Fully hardware-implemented memristor convolutional neural network,'' {\em
  Nature}, vol.~577, no.~7792, pp.~641--646, 2020.

\bibitem{shin2021fault}
H.~Shin, M.~Kang, and L.-S. Kim, ``Fault-free: A fault-resilient deep neural
  network accelerator based on realistic reram devices,'' in {\em 2021 58th
  ACM/IEEE Design Automation Conference (DAC)}, pp.~1039--1044, IEEE, 2021.

\bibitem{jeong2022variation}
S.~Jeong, J.~Kim, M.~Jeong, and Y.~Lee, ``Variation-tolerant and low r-ratio
  compute-in-memory reram macro with capacitive ternary mac operation,'' {\em
  IEEE Transactions on Circuits and Systems I: Regular Papers}, 2022.

\bibitem{yan2022swim}
Z.~Yan, X.~S. Hu, and Y.~Shi, ``Swim: Selective write-verify for
  computing-in-memory neural accelerators,'' in {\em 2022 59th ACM/IEEE Design
  Automation Conference (DAC)}, IEEE, 2022.

\bibitem{dangel2020backpack}
F.~Dangel, F.~Kunstner, and P.~Hennig, ``Backpack: Packing more into
  backprop,'' in {\em International Conference on Learning Representations},
  2020.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning
  applied to document recognition,'' {\em Proceedings of the IEEE}, vol.~86,
  no.~11, pp.~2278--2324, 1998.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in {\em Proceedings of the IEEE conference on computer vision
  and pattern recognition}, pp.~770--778, 2016.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, {\em et~al.}, ``Learning multiple layers of features
  from tiny images,'' 2009.

\bibitem{chakraborty2020beol}
W.~Chakraborty, B.~Grisafe, H.~Ye, I.~Lightcap, K.~Ni, and S.~Datta, ``Beol
  compatible dual-gate ultra thin-body w-doped indium-oxide transistor with
  ion= 370$\mu$a/$\mu$m, ss= 73mv/dec and ion/ioff ratio> 4$\times$ 109,'' in
  {\em 2020 IEEE Symposium on VLSI Technology}, pp.~1--2, IEEE, 2020.

\end{thebibliography}
