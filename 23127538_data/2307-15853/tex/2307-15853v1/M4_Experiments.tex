\section{Experimental Evaluation}\label{sect:exp}

In this section, we comprehensively evaluate our proposed TRICE method in terms of \KPP~improvement for CiM DNN accelerators suffering from device variations.
%how TRICE outperforms the state-of-the-art (SOTA) training solutions in terms of \KPP~ improvement for CiM DNN accelerators suffering from device variations.
%show by experiments that TRICE is superior to other existing training methods in improving the \KPP~of DNNs. 
We first discuss how to link the device value variations to additive noise on weights based on the noise model. We then compare the effectiveness of TRICE against SOTA baselines using different datasets, models, and different types of NVM devices that can be used to build NVCiM DNN accelerators. Ablation studies that show the advantages of RC-Gaussian noise over different noise candidates are also conducted.


%We then show the effectiveness of TRICE by comparing it with existing SOTA baselines on various models across various datasets. We also demonstrate the effectiveness of TRICE by comparing it with existing SOTA baselines on NVCiM platforms using different types of NVM devices. Ablation studies to show the effectiveness of RC-Gaussian noise over different noise candidates are also performed.


%We first discuss our noise model by showing how we link the device value variations to additive noise on weights. We then show the effectiveness of TRICE by comparing it with existing SOTA baselines on various models across various datasets. We also demonstrate the effectiveness of TRICE by comparing it with existing SOTA baselines on NVCiM platforms using different types of NVM devices. Ablation studies to show the effectiveness of RC-Gaussian noise over different noise candidates are also performed. 

%We further perform ablation studies to show the effectiveness of injecting RC-Gaussian noise over different noise candidates.



% % Figure environment removed

% Figure environment removed

%\subsection{Modeling of Weight Perturbation Incurred by Device Variations}
\subsection{Modeling of Device Variation-induced Weight Perturbation}
\label{sect:model}
Without loss of generality, we mainly focus on device variations originating from the programming process, in which the conductance value programmed to NVM devices can deviate from the desired value. Next, we show how to model the impact of device variations on DNN weights. 

%Here we show how we model the impact of device variations on DNN weights. In this paper, we are majorly concerned about the impact of device variations in the programming process, \emph{i.e.}, the conductance value programmed to NVM devices is different from the desired value.
Assume a $H$ bits DNN weight, the desired weight value after quantization ($\mathcal{W}_{des}$) can be represented as: 

%For a DNN weight represented by a total of $H$ bits, let its desired weight value after quantization $\mathcal{W}_{des}$ can be represented as
\vspace{-0.2cm}
\begin{equation}
    \vspace{-0.1cm}
    \mathcal{W}_{des} = \frac{\max{|\mathcal{W}|}}{2^H - 1}\sum_{j=0}^{H-1}{h_j \times 2^j}
\end{equation}
where $h_j \in \{0,1\}$ is the value of the $j^{th}$ bit of the desired weight value, $\mathcal{W}$ is the floating point weight value and $\max{|\mathcal{W}|}$ is the maximum absolute value of the weight. 
%Moreover,
For an NVM device capable of representing $B$ bits of data, since each weight value can be represented by $H/B$ devices\footnote{Without loss of generality, we assume that $H$ is a multiple of $B$.}, the corresponding mapping process can be expressed as: 
%each NVM device is capable of representing $B$ bits of data. Thus, each weight value of the DNNs needs $H/B$ devices to represent\footnote{Without loss of generality, we assume that $H$ is a multiple of $B$.}. 
%This mapping process can be represented as
\vspace{-0.3cm}
\begin{equation}
    \vspace{-0.1cm}
    g_i = \sum_{j=0}^{B -1} h_{i\times B + j} \times 2^j
\end{equation}
where $g_i$ is the desired conductance of the $i^{th}$ device representing a weight. Note that negative weights are mapped in a similar manner.
Considering the impact of device variations, the actually programmed conductance value $gp_i$ is as follows:
% The device mapping process is subjected to the impact of device variations so the actually programmed conductance is noisy, which can be represented as
\vspace{-0.2cm}
\begin{equation}
    \vspace{-0.1cm}
    gp_i = g_i + \Delta{g}
\end{equation}
where $\Delta{g}$ is the deviation from the desired conductance value $g_i$. %caused by device variations.
%where $gp_i$ is the actually programmed conductance and $\Delta{g}$ is the device value deviation noise caused by device variations.

Thus when weight is programmed, the actual value $\mathcal{W}_{p}$ mapped on the devices would be: 
\vspace{-0.2cm}
\begin{align}
\vspace{-0.2cm}
\begin{split}
    \mathcal{W}_{p}     & =\frac{\max{|\mathcal{W}|}}{2^H - 1} \sum_{i=0}^{H/B -1}2^{i\times B}{gp_i } \\
                        % & = \sum_{i=0}^{M/B -1}2^{i\times B}{g_i + \Delta{g}} \\
                        & = \mathcal{W}_{des} + \frac{\max{|\mathcal{W}|}}{2^H - 1} \sum_{i=0}^{H/B-1}{\Delta{g} \times 2^{i\times B}}\\
\end{split}
\vspace{0.0 cm}
\end{align}

To simulate the above process, we follow the settings consistent with existing works. Specifically, we set $B=2$ based on existing works~\cite{jiang2020device, yan2022swim}, while $H$ is specified by each model. For the device variation model, we adopt $\Delta{g} \sim \mathcal{N}(0, \sigma_d)$ (if not specified), which indicates that $\Delta{g}$ follows Gaussian distribution with a mean of zero and a standard deviation of $\sigma_d$. We constrain $\sigma_d \leq 0.4$ as this is a reasonable range that can be realized by device-level optimizations such as write-verify based on the measurement results. Our model and parameter settings are in line with that of RRAM devices reported in~\cite{shim2020two}. 

% \todo{Note that various different works use different definitions for the magnitude of the impact of device variations. Some of them use the definition of weight value deviation and have $\Delta w \sim \mathcal{N}(0, \sigma_w)$. When $B=2$, $\sigma_w = 3 \sigma_d$, so their device variation magnitude numbers are greater than ours.}


%and is in line with the reported numbers 


%In this paper, we set $B=2$ as in~\cite{jiang2020device, yan2022swim}. The $H$ values are specified by each model. If not specified, in this paper, we use the device value deviation model of $\Delta{g} \sim \mathcal{N}(0, \sigma_d)$, which means $\Delta{g}$ follows Gaussian distribution with a mean of zero and a standard deviation of $\sigma_d$. According to the measurement results, we have $\sigma_d \leq 0.4$. Note that device-level optimizations like write-verify can be utilized, so $\sigma_d$ values of $0 < \sigma_d \leq 0.4$ would all be valid scenarios for device value deviations depending on how much effort is paid to accurately program the device. These numbers are in line with the properties of RRAM devices reported in~\cite{shim2020two}, which confirms the validity of our model and parameters. \todo{Note that various different works use different definitions for the magnitude of the impact of device variations. Some of them use the definition of weight value deviation and have $\Delta w \sim \mathcal{N}(0, \sigma_w)$. When $B=2$, $\sigma_w = 3 \sigma_d$, so their device variation magnitude numbers are greater than ours.}

\subsection{Experimental Setup}
\textbf{Platforms and Metrics}: All experiments are conducted on PyTorch using an on-the-shelf GPU. To precisely capture the performance (accuracy) of the DNN model under device variations, our report data points are averaged from 5 identical runs. For the evaluation metric, if not specified, we report the $1^{st}$ percentile accuracy, which is \KPP~using accuracy as the performance metric and with $k=1$. To obtain the \KPP~of a DNN model under sufficiently high precision, we choose to run 10,000 Monte Carlo simulations ($N_{sample}$ = 10,000). Since our experiments show that 10,000 runs can output $1^{st}$ percentile accuracy whose 95\% confidence interval is $\pm 0.009$ based on the central limit theorem. 
%sufficiently precise.

%All of the data points shown in this paper are averaged from at least five identical experimental runs. If not specifically stated, in this section, we use $1^{st}$ percentile accuracy, which is \KPP~using accuracy as the performance metric and with $k=1$. To evaluate the \KPP~of a DNN model, we use a Monte Carlo simulation of 10,000 runs ($N_{sample}$ = 10,000). Our experiments show through the central limit theorem that 10,000 runs of evaluation can get a $1^{st}$ percentile accuracy whose 95\% confidence interval is $\pm 0.009$, which is sufficiently precise.
\textbf{Baselines for Comparison}: We compare TRICE with three baselines that are built upon training: (1) training w/o noise injection, (2) CorrectNet~\cite{eldebiky2022correctnet}, and (3) injecting Gaussian noise in training~\cite{jiang2020device, yang2022tolerating}. For a fair comparison, we do not compare TRICE with other orthogonal methods like NAS-based DNN topologies design~\cite{yan2021uncertainty, jiang2020device} or Bayesian Neural Networks~\cite{gao2021bayesian}, given TRICE can be used together with them. 
%because TRICE is orthogonal to these methods and can thus be used together with them.


\textbf{Hyperparameters Setting}: For all experiments, TRICE uses the same hyperparameter setups: $start = 0$, $end = 2\times\sigma_d$, $th = 2$, $ep=100$, $warm=5$ and $N_{train}=300$, where $\sigma_d$ is the standard deviation for device variation. We limit the range of $\sigma_d$ as suggested by Sect.~\ref{sect:model} and report the effectiveness of TRICES across different $\sigma_d$ values within that range. For other training hyperparameters such as learning rate, batch size, and learning rate schedulers, we follow the best practice in training a noise-free model. 
%We show the effectiveness of TRICE across different $\sigma_d$ values in the range specified in \ref{sect:model}. 
%Other training hyperparameters such as learning rate, batch size, and learning rate schedulers are the same as the best practice for training a noise-free model.



\subsection{The Effectiveness of TRICE on MNIST Dataset}

% In this work, we compare TRICE with three baselines: (1) training w/o noise injection, (2) CorrectNet~\cite{eldebiky2022correctnet}, and (3) injecting Gaussian noise in training~\cite{jiang2020device}. We do not show the comparison between TRICE and methods that propose new DNN topologies using NAS~\cite{yan2021uncertainty, jiang2020device} or Bayesian Neural Networks~\cite{gao2021bayesian} because TRICE is orthogonal to these methods and can thus be used together with them.

\begin{table}[ht]
    % \vspace{-0.1cm}
    \centering
    \caption{Effectiveness of TRICE method on model LeNet for MNIST across different $\sigma_d$ values. The performance is shown in $1^{st}$ percentile accuracy. The baselines are vanilla DNN training w/o noise injection, CorrectNet~\cite{eldebiky2022correctnet}, and injecting Gaussian noise in training~\cite{jiang2020device,yang2022tolerating
    }. Injecting RC-Gaussian noise with hand-picked hyperparameters (RC-Manual) is also shown as an ablation study.}
    \vspace{-0.1cm}
    \begin{tabular}{cccccc}
        \toprule
        Dev. var.  &\multicolumn{5}{c}{Training Method}\\
        ($\sigma_d$)    & w/o noise & CorrectNet    & Gauss.  & RC-Manual & TRICE \\
        \midrule
        % 0.00    & 99.01     & 97.99         & 98.86     & 98.88     & 98.94 \\
        % 0.05    & 93.31     & 97.56         & 97.45     & 96.89     & \textbf{98.16} \\
        % 0.10    & 70.72     & 90.66         & \textbf{95.59}     & 95.47     & 95.26 \\
        % 0.15    & 38.15     & 67.70         & 87.60     & \textbf{90.43}     & 89.61 \\
        % 0.20    & 19.81     & 39.54         & 66.04     & 75.47     & \textbf{76.95} \\
        % 0.25    & 11.95     & 22.26         & 40.27     & 50.14     & \textbf{57.68} \\
        % 0.30    & 08.58     & 14.26         & 23.09     & 28.56     & \textbf{34.60} \\
        % 0.35    & 06.89     & 10.83         & 14.38     & 16.83     & \textbf{24.43} \\
        % 0.40    & 06.05     & 09.23         & 10.38     & 11.61     & \textbf{14.62} \\
        % 0.45  & 05.65     & 08.35         & 08.39     & ---       & 09.15 \\
        0.00    & 99.01     & 97.99         & 98.86     & 98.88     & 98.94 \\
        0.05    & 93.31     & 97.56         & 97.45     & 96.89     & \textbf{98.08} \\
        0.10    & 70.72     & 90.66         & 95.59     & 95.47     & \textbf{95.99} \\
        0.15    & 38.15     & 67.70         & 87.60     & 90.43     & \textbf{90.58} \\
        0.20    & 19.81     & 39.54         & 66.04     & 75.47     & \textbf{77.82} \\
        0.25    & 11.95     & 22.26         & 40.27     & 50.14     & \textbf{54.12} \\
        0.30    & 08.58     & 14.26         & 23.09     & 28.56     & \textbf{38.51} \\
        0.35    & 06.89     & 10.83         & 14.38     & 16.83     & \textbf{25.29} \\
        0.40    & 06.05     & 09.23         & 10.38     & 11.61     & \textbf{17.94} \\
        \bottomrule
    \end{tabular}
    \label{tab:GvPT}
    \vspace{-0.4cm}
\end{table}

We first compare TRICE with the aforementioned baselines using the model LeNet to recognize the 10-class handwritten digits dataset MNIST~\cite{lecun1998gradient}. LeNet is a plain convolutional neural network consisting of two convolution layers and three fully connected layers. All weights and layer outputs (\emph{i.e.}, activations) are quantized to four bits ($H=4$). We also compare TRICE with injecting right-censored Gaussian noise with handpicked hyperparameters (RC-Manual) as an ablation study. Table~\ref{tab:GvPT} shows the $1^{st}$ percentile accuracy of models trained with different training methods under different levels of device variations ($\sigma_d$) following the noise model discussed in Section~\ref{sect:model}. As shown in Table~\ref{tab:GvPT}, compared with training w/o noise, CorrectNet improves the $1^{st}$ percentile accuracy by up to 19.94\%, but this is not comparable to the improvement of up to 49.44\% by injecting Gaussian noise and up to 58.01\% by our proposed TRICE. We can also observe that, compared with injecting Gaussian noise, TRICE can improve the $1^{st}$ percentile accuracy by up to 15.42\%. It is clear that TRICE outperforms all baselines in generating models with higher $1^{st}$ percentile accuracy in all simulated $\sigma_d$ values. Moreover, TRICE demonstrates more significant improvement when facing large device variations while still delivering comparable accuracy when $\sigma_d$ is too small to distinguish the difference between different training methods. Because CorrectNet cannot generate a model with higher robustness compared with injecting Gaussian noise, we do not show the results for it in the latter experiments. The ablation study also shows that TRICE outperforms injection right-censored Gaussian with handpicked hyperparameters (RC-Manual) and the improvement in $1^{st}$ percentile accuracy is up to 9.95\%, so we do not show the result of RC-Manual in the remainder of this paper.

%We first compare TRICE with the aforementioned baselines on model LeNet for MNIST~\cite{lecun1998gradient}. This model is a plain convolutional neural network with two convolution layers and three fully connected layers. All weights and layer outputs (\emph{i.e.}, activations) are quantized to four bits ($H=4$). We also compare TRICE with injecting right-censored Gaussian noise with handpicked hyperparameters (RC-Manual) as an ablation study. Table~\ref{tab:GvPT} shows the $1^{st}$ percentile accuracy of models trained with different training methods under different magnitudes of device value variations ($\sigma_d$) following the noise model discussed in Sect.~\ref{sect:model}. As shown in Table~\ref{tab:GvPT}, compared with training w/o noise, CorrectNet improves the $1^{st}$ percentile accuracy by up to 19.94\%, but this cannot compare to the improvement of up to 49.44\% by injecting Gaussian noise and up to 58.01\% by TRICE. We can also observe that, compared with injecting Gaussian noise, TIRCE can improve the $1^{st}$ percentile accuracy by up to 15.42\%. It is clear that TRICE is superior to all baselines in generating models with higher $1^{st}$ percentile accuracy in all $\sigma_d$ values. TRICE shows more improvement when $\sigma_d$ is large and has comparable accuracy when $\sigma_d$ is too small to distinguish the difference between different training methods. Because CorrectNet cannot generate a model with higher robustness compared with injecting Gaussian noise, we do not show the results for it in the latter experiments. The ablation study also shows that TRICE outperforms injection right-censored Gaussian with handpicked hyperparameters (RC-Manual) and the improvement in $1^{st}$ percentile accuracy is up to 9.95\%, so we do not show the result of RC-Manual in the remainder of this paper.

% \begin{table}[ht]
%     \centering
%     \caption{Hyperparameters used in different experiments.}
%     \vspace{-0.1cm}
%     \begin{tabular}{ccccccc}
%         \toprule
%         \multirow{2}{*}{Models} &\multicolumn{5}{c}{Hyperparameters}\\
%                     & $th$  & $start$   & $end$             & $Train_{ep}$  & $Eval_{ep}$   & Time (h)\\
%         \midrule
%         MNIST       & 2.0   & 0         & $2\times\sigma$   & 100           & 300           & 1.3\\
%         VGG-8       & 0.7   & 0         & $2\times\sigma$   & 100           & 300           & 8.3\\
%         ResNet-18   & 2.0   & 0         & $2\times\sigma$   & 200           & 300           & 10.0\\
%         \bottomrule
%     \end{tabular}
%     \label{tab:hype}
%     \vspace{-0.4cm}
% \end{table}

% Figure environment removed

\subsection{The Effectiveness of TRICE in Large Models}

After showing the effectiveness of TRICE in a small model LeNet for MNIST, here we further demonstrate the effectiveness of TRICE by comparing it with the baselines in larger DNN models for larger datasets. We choose two representative models VGG-8~\cite{simonyan2014very} and ResNet-18~\cite{he2016deep}. Both models use a 6-bit quantization ($H=6$) for weights and activations. They both perform image classification tasks for dataset CIFAR-10~\cite{krizhevsky2009learning}. 
As shown in Fig.~\ref{fig:cifar} and Fig.~\ref{fig:res18}, 
% As shown in Fig.~\ref{fig:cifar}, Fig.~\ref{fig:res18}, and Fig.~\ref{fig:resC100},
TRICE clearly outperforms all baselines in most device value deviation values and performs similarly as baselines in some rare cases where device value deviation is too small to make an impact or too large to perform a valid classification. Compared with injecting Gaussian noise, 
TRICE improves the $1^{st}$ percentile accuracy by up to 25.09\%, and 26.01\% in VGG-8 for CIFAR-10 and ResNet-18 for CIFAR-10, respectively.
% TRICE improves the $1^{st}$ percentile accuracy by up to 25.09\%, 26.01\%, and 14.25\% in VGG-8 for CIFAR-10, ResNet-18 for CIFAR-10, and ResNet-18 for CIFAR-100 respectively.

% % Figure environment removed

% \vspace{-0.4cm}
% % Figure environment removed


\subsection{The Effectiveness of TRICE in Different Devices}

To demonstrate the scalability of TRICE, we also show the effectiveness of TRICE on NVCiM platforms using different types of NVM devices. As discussed in Section~\ref{sect:model}, previous experiments use a four level (2-bit, $B=2$) device as in~\cite{jiang2020device, yan2022swim}. More specifically, it is a four-level RRAM device whose device value deviation model is $\Delta{g} \sim \mathcal{N}(0, \sigma_d)$, which means $\Delta{g}$ follows Gaussian distribution with a mean of zero and a standard deviation of $\sigma_d$, independent of the programmed device conductance.

We further analyze the effectiveness of TRICE on two real-world FeFET devices whose device value deviation magnitude varies as its programmed conductance changes. Their device models are derived from measurement results in~\cite{chakraborty2020beol}. Specifically, a generalized device value variation model for a four-level device is:
\vspace{-0.1cm}
\begin{equation}
    \vspace{-0.1cm}
    \begin{aligned}
        \begin{array}{ll}
            gp_i &= g_i + \Delta{g} \\
            \Delta{g} &\sim \mathcal{N}(0, \sigma_h)
        \end{array}, \ \ \ \  
        \sigma_h = & \begin{cases}
            \sigma_{d0},\ \ \  if\  g_i = 0\\
            \sigma_{d1},\ \ \  if\  g_i = 1\\
            \sigma_{d2},\ \ \  if\  g_i = 2\\
            \sigma_{d3},\ \ \  if\  g_i = 3\\
        \end{cases}
    \end{aligned}
\end{equation}
which means $\Delta{g}$ follows Gaussian distribution with a mean of zero and a standard deviation of $\sigma_h$ but the $\sigma_h$ value differs as its programmed conductance changes. We abstract the behaviors of the two FeFET devices to be:
\vspace{-0.1cm}
\begin{align}
\vspace{-0.1cm}
    \text{FeFET}_1 \rightarrow \{\sigma_{d0} = \sigma_{d3} = \sigma_d, \sigma_{d1} = \sigma_{d2} = 4\sigma_d\}\label{eq:fe1} \\
    \text{FeFET}_2 \rightarrow \{\sigma_{d0} = \sigma_{d3} = \sigma_d, \sigma_{d1} = \sigma_{d2} = 2\sigma_d\}\label{eq:fe2}
\end{align}

% Figure environment removed
% FeFET$_1 \rightarrow$ \{$\sigma_{d0} = \sigma_{d3} = \sigma_d$, $\sigma_{d1} = \sigma_{d2} = 4\sigma_d$\} and FeFET$_2 \rightarrow$ \{$\sigma_{d0} = \sigma_{d3} = \sigma_d$, $\sigma_{d1} = \sigma_{d2} = 2\sigma_d$\}. 
This means the devices suffer from more device variations when they are programmed to value 1 and 2 and suffer from less device variations when they are programmed to value 0 and 3. As a comparsion, we show the conductance ($gp$) distribution of the previously used RRAM device and FeFET$_2$ in Fig.~\ref{fig:devices}a and  Fig.~\ref{fig:devices}b, respectively. 


We report the effectiveness of TRICE in NVCiM platforms using FeFET$_1$ and FeFET$_2$ in Fig.~\ref{fig:H4} and Fig.~\ref{fig:H2}, respectively. As expected, again, it is obvious that TRICE outperforms all baselines in most $\sigma_d$ values and performs similarly as baselines where device value deviation is too small to make an impact. Compared with injecting Gaussian noise, TRICE improves the $1^{st}$ percentile accuracy by up to 15.61\%, and 12.34\% in FeFET$_1$ and FeFET$_2$, respectively.

% % Figure environment removed

% % Figure environment removed



% \subsection{Ablation study to compare TRICE vs RC Manual}

% Here, we further show the effectiveness of the adaptive training method for TRICE by comparing it with the handpicked hyperparameters of TRICE. As shown in Fig.~\ref{fig:lenet}, AdapTRICE outperforms TRICE with handpicked hyperparameters in all device value deviation values and the improvement in $1^{st}$ percentile accuracy is up to 7.60\%. Because AdapTRICE clearly outperforms TRICE with handpicked hyperparameters, we do not show the result of TRICE with handpicked hyperparameters in the remainder of this paper.

% % Figure environment removed

\subsection{Ablation Study for Different Noise Candidates}
% \begin{table}[ht]
%     \centering
%     \caption{Effectiveness of PT-Gaussian on MNIST, 1\% quantile.}
%     \begin{tabular}{ccccc}
%         \toprule
%         \multirow{2}{*}{Dev. var.} &\multicolumn{4}{c}{Training Method}\\
%                 & RC-Gaussian & LC-Gaussian   & RT-Gaussian & LT-Gaussian\\
%         \midrule
%         0.0     &  xxxxx      &  xxxxx        &  xxxxx      &  xxxxx \\
%         0.2     &  50.14      &  41.38        &  36.66      &  33.99 \\
%         0.4     &  09.15\\

%         \bottomrule
%     \end{tabular}
%     \label{tab:RCVAll}
% \end{table}

% Figure environment removed

We also show the effectiveness of injecting RC-Gaussian noise in the training process by comparing it against injecting other three noise candidates: LC-Gaussian, RT-Gaussian, and LT-Gaussian noise. Here the result of training with Gaussian noise is also included as a baseline. Without loss of generality, we perform this study on the LeNet for MNIST dataset using uniform RRAM devices with $\sigma_d = 0.25$. As shown in Fig.~\ref{fig:RCVAll}, training with RC-Gaussian noise shows a clear advantage over training with other types of noise by at least 8.76\%. Note that training with left and right truncated Gaussian performs even worse than injecting Gaussian noise because they exhibit lower accuracy w/o the presence of device variations.