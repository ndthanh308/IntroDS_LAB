\section{Introductions}

Deep neural networks (DNNs) have demonstrated remarkable advancements, surpassing human performance in a wide range of perception tasks.
% ~\cite{liang2022variational, liang2021can, wang2021exploration}. 
%Furthermore, 
The recent emergence of deep learning-based generation models, such as DALL-E~\cite{ramesh2021zero} and the GPT family~\cite{brown2020language}, has further reshaped our workflows. To date, the trend of incorporating on-device intelligence across edge platforms such as mobile phones, watches, and cars, has become an evident~\cite{jiang2020device,chen2016eyeriss,yan2020single}, transforming every walk of life. 
However, the limited computational resources and strict power constraints of these edge platforms present challenges. These circumstances necessitate more energy-efficient DNN hardware beyond the general-purpose CPUs and GPUs. 

%Consequently, relying solely on CPUs and GPUs is not always suitable for powering DNNs on edge platforms.

Compute-in-Memory (CiM) DNN accelerators~\cite{shafiee2016isaac}, on the other hand, are competitive alternatives to replace CPUs and GPUs in accelerating DNN inference on edge. In contrast to the traditional von Neumann architecture platforms, which involve frequent data movements between memory and computation components, CiM DNN accelerators reduce energy consumption by enabling in-situ computation directly at the storage location of weight data. Moreover, emerging non-volatile memory (NVM) devices, such as ferroelectric field-effect transistors (FeFETs) and resistive random-access memories (RRAMs),
% magnetoresistive random-access memories (MRAMs), and phase-change memories (PCMs), 
allows NVCiM accelerators to achieve higher memory density and improved energy efficiency compared to conventional MOSFET-based designs~\cite{chen2016eyeriss}. 
However, the reliability of NVM devices can be a concern due to device-to-device (D2D) variations incurred by fabrication defects and cycle-to-cycle (C2C) variations due to thermal, radiation, and other physical impacts. These variations can have a notable negative impact on NVCiM DNN accelerators' inference accuracy, as they may introduce significant differences between the weight values read out from NVM devices during inference and their intended values.
%leading to significant accuracy drop.
%leading to 
%Consequently, these variations can lead to a significant degradation in DNN performance.

Various strategies have been proposed to mitigate the impact of device variations. These strategies can be broadly categorized into two categories: reducing device value deviations and enhancing the robustness of DNNs in the presence of device variations. 
Device value deviations can be reduced through methods such as write-verify~\cite{shim2020two}, which iteratively applies programming pulses to reduce device value deviation from the desired value after each write.
On the other hand, there exist various approaches that enhance DNN robustness in the presence of device variations. One direction is to identify novel DNN topologies that are more robust in the presence of device variations. This can be achieved through techniques such as neural architecture search~\cite{yan2021uncertainty, yan2022radars} or by leveraging Bayesian Neural Networks~\cite{gao2021bayesian} which use variational training to improve DNN robustness. Another line of methods focuses on training more robust DNN weights using noise injection training~\cite{jiang2020device, he2019noise, yang2022tolerating}. 
%Along this direction, noise is introduced in DNN training. 
In this approach, randomly sampled noise is injected into DNN weights during the forward and backpropagation phases of DNN training.
%during the forward and backpropagation phases of DNN training, a random sample of noise is incorporated by adding it to DNN weights. 
After the gradient is calculated through backpropagation, the noise is then removed and the weight value without noise is updated by gradient descent. By simulating a noisy inference environment, the noise injection training methods significantly enhance the robustness of DNN models across various DNN topologies.


However, all aforementioned methods 
%mentioned above 
merely focus on improving the average accuracy performance of CiM DNN accelerators in the presence of device variations, which may be acceptable for non-safety critical applications. In safety-critical applications like airplanes, autonomous driving, and medical devices, even a prediction failure that happens with an extremely low probability (namely worst-case scenario), 
%happened by an extremely low-probability occurence i
is not affordable because it may result in loss of life, as has been demonstrated in the recent work~\cite{yan2022computing}. 
\todo{The worst-case performance of a DNN model in the presence of device variations can be determined by carefully calibrating the perturbation injected on each weight value to reach the lowest possible DNN performance. Recent work~\cite{yan2022computing} has demonstrated that even a weight value perturbation of less than 3\% can degrade a DNN model's performance to the level of random guessing. However, the likelihood of such a worst-case scenario occurring is extremely low ($<10^{-100}$), which can be safely ignored in common natural environments~\cite{yan2022computing}. Consequently, a more suitable metric to depict the realistic worst-case performance of DNNs in the presence of device variations is needed.}

\todo{To capture realistic worst-case scenarios precisely in the presence of device variations, in this work, we propose to use the k-th percentile performance (\KPP) metric, instead of the average or absolute worst-case performance. With a predetermined $K$ value, the \KPP~metric aims to identify a performance score that the model's performance is consistently greater than this score in all but k\% of cases.\footnote{This research was partially supported by NSF under grants 
% CNS-1822099, CCF-1919167, and CCF-2028879
CNS-1919167, CCF-2006748, and CCF-2011236, also by ACCESS â€“ AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR.}
For example, if a model has a \KPP~of 0.912 when $K=1$, this suggests that the likelihood of a model's performance being greater than 0.912 is 99\% (except the 1\% of the cases). 
When a realistically small $K$ value is given, such as $K=1$, \KPP~can capture a realistic worst-case performance of a DNN model because it (1) guarantees a lower bound of the model's performance and (2) filters out extreme corner cases.
Given the same $K$ value, a higher \KPP~for a DNN model is desirable as it signifies that the model can consistently deliver high performance within a certain probability threshold.}

\todo{Since improving \KPP~guarantees higher realistic worst-case performance of a DNN model,} 
\edd{we revisited the state-of-the-art (SOTA) Gaussian noise injection training method to analyze its effectiveness in improving \KPP. Gaussian noise injection training is widely used simply because it injects noises that statistically mirror the noises in the inference environment. Although it is empirically valid to state that a precise simulation of the inference environment during training would yield optimal results, there is no theoretical proof for it.}
\todo{Thus, \edd{to prove the effectiveness of Gaussian noise injection training}, we thoroughly analyze the relationship between \KPP~of a DNN model and the properties of DNN weights to show what kind of models would provide higher \KPP. Surprisingly, our analysis shows that Gaussian noise injection training is far from optimal in generating robust DNN models in the presence of device variations.}

\todo{Specifically, our key observation is that achieving a higher \KPP~in the presence of device variation needs to satisfy the following three requirements simultaneously: (1) higher DNN accuracy under no device variation; (2) smaller $2^{nd}$ derivatives \emph{w.r.t.} DNN weights, and (3) larger $1^{st}$ derivatives \emph{w.r.t.} DNN weights.
\edd{However}, our analysis (see Section~\ref{sect:train_ana}) shows that the conventional Gaussian noise-injected training approaches can only fulfill the first two requirements, but not the third, making them ineffective for \KPP~improvement. Specifically, the third requirement necessitates distributions with non-zero expected values, a condition that the Gaussian distribution fails to satisfy.}

%Based on this, we further study the noise injection training process to understand how the conventional approach that injects Gaussian noise in the training process helps to improve the \KPP~under device variations.

% To improve the \KPP~of a DNN model under device variation, we first thoroughly analyze the relationship between \KPP~of a DNN model and the properties of DNN weights. We show that higher \KPP~requires (1) higher DNN performance when there is no device variation, (2) smaller $2^{nd}$ derivatives \emph{w.r.t.} DNN weights, and (3) larger $1^{st}$ derivatives \emph{w.r.t.} DNN weights. We then study the noise injection training process to understand how the conventional approach that injects Gaussian noise in the training process helps to improve the \KPP~under device variations. Our analysis demonstrates that injecting Gaussian noise effectively addresses the first two requirements but has no impact on fulfilling the third requirement.

% Thus, we propose to inject Right-Censored Gaussian (RC-Gaussian) Noise in the DNN training process to improve the \KPP~under device variations.

To this end, we develop TRICE, a method that injects adaptively optimized right-censored Gaussian (RC-Gaussian) noise in the training process. The abbreviation of this method is derived from the name \underline{T}raining with \underline{RI}ght-\underline{C}ensored Gaussian Nois\underline{E} (TRICE), to address all aforementioned three requirements simultaneously.    
%a novel Right-Censored Gaussian (RC-Gaussian) noise-injected training method to address all aforementioned three requirements. 
TRICE differs from existing approaches in several aspects: (1) rather than using the general Gaussian noise, TRICE uses RC-Gaussian noise which exhibits a unique feature--for all sampled values greater than a designated threshold, the sample value is fixed (\emph{i.e.}, censored) to the threshold. \edd{This results in a negative expected value for the injected noise, thus meeting the third requirement.}
%The difference between RC-Gaussian noise and Gaussian noise lies in that for all sampled values greater than a designated threshold, the sample value is fixed (\emph{i.e.}, censored) to the threshold.  
(2) TRICE requires additional hyperparameters tuning, \emph{e.g.}, via a dedicated adaptive training method to identify the optimal noise hyperparameters within a single run of DNN training, which is different from the conventional Gaussian noise-based approaches using the same noise hyperparameters in training and inference. The main contributions of this work are multi-fold: 
\begin{itemize}
    \item We analytically derive the relationship between \KPP~and the gradients of weights and demonstrate how noise injection training can improve \KPP.
    \item We propose to inject right-censored Gaussian noise during DNN training to improve the \KPP~in the presence of device variations. An adaptive training method that can automatically identify optimal noise hyperparameters in the training process is developed accordingly. 
    %forming \underline{T}raining with \underline{RI}ght-\underline{C}ensored Gaussian Nois\underline{E} (TRICE) .
    \item Extensive experimental results show that TRICE improves the $1^{st}$ percentile performance (in terms of top-1 accuracy) in the presence of device variations by up to 15.42\%, 25.09\%, and 26.01\% in LeNet for MNIST, VGG-8 for CIFAR-10 and ResNet-18 for CIFAR-10, respectively compared with SOTA baselines.
    \item We also demonstrate the scalability of our proposed TRICE. That is, in addition to evaluations on uniform RRAM devices, TRICE also improves the $1^{st}$ percentile accuracy by up to 15.61\%, and 12.34\% in two different types of FeFET devices respectively.
    \item To the best of our knowledge, this is the first work that advocates improving \KPP~in NVCiM DNN accelerators with device variations specifically for safety-critical applications. 
\end{itemize}






% As a result, we propose the injection of Right-Censored Gaussian (RC-Gaussian) noise as an alternative approach because it addresses all three requirements.
% RC-Gaussian noise is similar to Gaussian noise, except that for all sampled values greater than a designated threshold, the sample value is fixed (\emph{i.e.}, censored) to the threshold. 
% The proposed training method improves k-th percentile accuracy by up to 10\% compared to injecting Gaussian noise during training on the MNIST dataset.

% % Figure environment removed

% Different from injecting the conventional Gaussian noise whose training process uses the same noise hyper-parameters as inference, injecting RC-Gaussian noise requires additional hyper-parameter tuning. By empirically analyzing the relationship between hyperparameters and final \KPP~through grid search, we propose an adaptive training method that can automatically find the optimal noise hyperparameters within a single run of DNN training. We name the final method that injects adaptively optimized right-censored Gaussian noise in the training process to be \underline{T}raining with \underline{RI}ght-\underline{C}ensored Gaussian Nois\underline{E} (TRICE). 
% % TRICE improves k-th percentile accuracy by up to 15.42\% compared to injecting Gaussian noise during training
% % TRICE further improves k-th percentile accuracy by up to 5\% compared with training with handpicked hyperparameters on MNIST dataset. 
% Extensive results show that TRICE outperforms state-of-the-art (SOTA) baselines on various models for various datasets. 
% % An illustration is shown in Fig.~\ref{fig:Intro}.
% We also demonstrate through experiments that TRICE works for NVCiM DNN accelerators using different NVM devices with device value variation distributions other than Gaussian.


% The main contributions of this work are multi-fold: 
% \begin{itemize}
%     \item This is the first work that advocates improving \KPP~in NVCiM DNN accelerators with device variations specifically for safety-critical applications. 
%     \item We analytically show the relationship between \KPP~and the gradients of weights and demonstrate how noise injection training can improve \KPP~.
%     \item We propose to inject right-censored Gaussian noise during DNN training to improve the \KPP~under device variations. We also propose an adaptive training method that can automatically identify optimal noise hyperparameters in the training process. This forms \underline{T}raining with \underline{RI}ght-\underline{C}ensored Gaussian Nois\underline{E} (TRICE) .
%     \item Experimental results show that TRICE improves the $1^{st}$ percentile performance (in terms of top-1 accuracy) under device variations by up to 15.42\%, 25.09\%, and 26.01\% in LeNet for MNIST, VGG-8 for CIFAR-10 and ResNet-18 for CIFAR-10, respectively compared with SOTA baselines.
%     \item Experimental results show that, other than uniform RRAM devices, TRICE improves the $1^{st}$ percentile accuracy by up to 15.61\%, and 12.34\% in two different types of FeFET devices respectively.
% \end{itemize}
