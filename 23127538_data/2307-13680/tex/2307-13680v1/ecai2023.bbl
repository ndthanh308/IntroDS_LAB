\begin{thebibliography}{10}

\bibitem{allen2014linear}
Zeyuan Allen-Zhu and Lorenzo Orecchia, `Linear coupling: An ultimate
  unification of gradient and mirror descent', {\em arXiv preprint
  arXiv:1407.1537}, (2014).

\bibitem{bassily2020stability}
Raef Bassily, Vitaly Feldman, Crist{\'o}bal Guzm{\'a}n, and Kunal Talwar,
  `Stability of stochastic gradient descent on nonsmooth convex losses', in
  {\em Advances in Neural Information Processing Systems}, pp. 4381--4391,
  (2020).

\bibitem{bottou2010large}
L{\'e}on Bottou, `Large-scale machine learning with stochastic gradient
  descent', in {\em Proceedings of COMPSTAT'2010},  177--186, (2010).

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal, `Optimization methods for
  large-scale machine learning', {\em Siam Review}, {\bf 60}(2),  223--311,
  (2018).

\bibitem{bottou201113}
Olivier Bousquet and L{\'e}on Bottou, `The tradeoffs of large scale learning',
  in {\em Advances in Neural Information Processing Systems}, pp. 161--168,
  (2007).

\bibitem{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff, `Stability and generalization', {\em
  Journal of Machine Learning Research}, {\bf 2},  499--526, (2002).

\bibitem{camuto2021asymmetric}
Alexander Camuto, Xiaoyu Wang, Lingjiong Zhu, Chris Holmes, Mert
  G{\"u}rb{\"u}zbalaban, and Umut {\c{S}}im{\c{s}}ekli, `Asymmetric heavy tails
  and implicit bias in gaussian noise injections', in {\em International
  Conference on Machine Learning}, (2021).

\bibitem{cutkosky2020momentum}
Ashok Cutkosky and Harsh Mehta, `Momentum improves normalized sgd', in {\em
  International Conference on Machine Learning}, pp. 2260--2268, (2020).

\bibitem{cutkosky2021high}
Ashok Cutkosky and Harsh Mehta, `High-probability bounds for non-convex
  stochastic optimization with heavy tails', in {\em Advances in Neural
  Information Processing Systems}, (2021).

\bibitem{davis2020high}
Damek Davis and Dmitriy Drusvyatskiy, `High probability guarantees for
  stochastic convex optimization', in {\em Conference on Learning Theory}, pp.
  1411--1427, (2020).

\bibitem{davis2021low}
Damek Davis, Dmitriy Drusvyatskiy, Lin Xiao, and Junyu Zhang, `From low
  probability to high confidence in stochastic convex optimization.', {\em
  Journal of Machine Learning Research}, {\bf 22},  49--1, (2021).

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, `Bert:
  Pre-training of deep bidirectional transformers for language understanding',
  {\em arXiv preprint arXiv:1810.04805}, (2018).

\bibitem{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer, `Adaptive subgradient methods for
  online learning and stochastic optimization.', {\em Journal of machine
  learning research}, {\bf 12}(7), (2011).

\bibitem{feldman2019high}
Vitaly Feldman and Jan Vondrak, `High probability generalization bounds for
  uniformly stable algorithms with nearly optimal rate', in {\em Conference on
  Learning Theory}, pp. 1270--1279, (2019).

\bibitem{foster2018uniform}
Dylan~J. {Foster}, Ayush {Sekhari}, and Karthik {Sridharan}, `Uniform
  convergence of gradients for non-convex learning and optimization', in {\em
  Advances in Neural Information Processing Systems}, pp. 8745--8756, (2018).

\bibitem{ghadimi2013stochastic}
Saeed {Ghadimi} and Guanghui {Lan}, `Stochastic first- and zeroth-order methods
  for nonconvex stochastic programming', {\em Siam Journal on Optimization},
  {\bf 23}(4),  2341--2368, (2013).

\bibitem{ghadimi2016accelerated}
Saeed Ghadimi and Guanghui Lan, `Accelerated gradient methods for nonconvex
  nonlinear and stochastic programming', {\em Mathematical Programming}, {\bf
  156}(1),  59--99, (2016).

\bibitem{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville, {\em Deep learning}, MIT
  press, 2016.

\bibitem{gorbunov2020stochastic}
Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov, `Stochastic
  optimization with heavy-tailed noise via accelerated gradient clipping', in
  {\em Advances in Neural Information Processing Systems}, pp. 15042--15053,
  (2020).

\bibitem{gorbunov2021near}
Eduard Gorbunov, Marina Danilova, Innokentiy Shibaev, Pavel Dvurechensky, and
  Alexander Gasnikov, `Near-optimal high probability complexity bounds for
  non-smooth stochastic optimization with heavy-tailed noise', {\em arXiv
  preprint arXiv:2106.05958}, (2021).

\bibitem{gurbuzbalaban2021heavy}
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu, `The heavy-tail
  phenomenon in sgd', in {\em International Conference on Machine Learning},
  pp. 3964--3975, (2021).

\bibitem{hardt2016train}
Moritz {Hardt}, Benjamin {Recht}, and Yoram {Singer}, `Train faster, generalize
  better: stability of stochastic gradient descent', in {\em International
  Conference on Machine Learning}, pp. 1225--1234, (2016).

\bibitem{harvey2019tight}
Nicholas~JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa, `Tight
  analyses for non-smooth stochastic gradient descent', in {\em Conference on
  Learning Theory}, pp. 1579--1613, (2019).

\bibitem{hazan2016introduction}
Elad Hazan et~al., `Introduction to online convex optimization', {\em
  Foundations and Trends{\textregistered} in Optimization}, {\bf 2}(3-4),
  157--325, (2016).

\bibitem{hazan2014beyond}
Elad Hazan and Satyen Kale, `Beyond the regret minimization barrier: optimal
  algorithms for stochastic strongly-convex optimization', {\em Journal of
  Machine Learning Research}, {\bf 15}(1),  2489--2512, (2014).

\bibitem{jain2017non}
Prateek Jain and Purushottam Kar, `Non-convex optimization for machine
  learning', {\em Foundations and Trends{\textregistered} in Machine Learning},
  {\bf 10}(3-4),  142--336, (2017).

\bibitem{jain2019making}
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli, `Making the last
  iterate of sgd information theoretically optimal', in {\em Conference on
  Learning Theory}, pp. 1752--1755, (2019).

\bibitem{kakade2009generalization}
Sham~M Kakade and Ambuj Tewari, `On the generalization ability of online
  strongly convex programming algorithms', in {\em Advances in Neural
  Information Processing Systems}, pp. 801--808, (2009).

\bibitem{kavis2021high}
Ali Kavis, Kfir~Yehuda Levy, and Volkan Cevher, `High probability bounds for a
  class of nonconvex algorithms with adagrad stepsize', in {\em International
  Conference on Learning Representations}, (2022).

\bibitem{lan2020first}
Guanghui Lan, {\em First-order and Stochastic Optimization Methods for Machine
  Learning}, Springer Nature, 2020.

\bibitem{lei2021generalization}
Yunwen Lei, Ting Hu, and Ke~Tang, `Generalization performance of multi-pass
  stochastic gradient descent with convex loss functions.', {\em Journal of
  Machine Learning Research}, {\bf 22},  25--1, (2021).

\bibitem{lei2018stochastic}
Yunwen Lei and Ke~Tang, `Stochastic composite mirror descent: Optimal bounds
  with high probabilities', in {\em Advances in Neural Information Processing
  Systems}, pp. 1519--1529, (2018).

\bibitem{lei2021learning}
Yunwen Lei and Ke~Tang, `Learning rates for stochastic gradient descent with
  nonconvex objectives', {\em IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, (2021).

\bibitem{li2022high}
Shaojie Li and Yong Liu, `High probability guarantees for nonconvex stochastic
  gradient descent with heavy tails', in {\em International Conference on
  Machine Learning}, pp. 12931--12963, (2022).

\bibitem{li2019convergence}
Xiaoyu Li and Francesco Orabona, `On the convergence of stochastic gradient
  descent with adaptive stepsizes', in {\em International Conference on
  Artificial Intelligence and Statistics}, pp. 983--992, (2019).

\bibitem{li2020high}
Xiaoyu Li and Francesco Orabona, `A high probability analysis of adaptive sgd
  with momentum', in {\em Workshop on Beyond First Order Methods in ML Systems
  at ICML}, (2020).

\bibitem{london2017pac}
Ben {London}, `A pac-bayesian analysis of randomized learning with application
  to stochastic gradient descent', in {\em Advances in Neural Information
  Processing Systems}, pp. 2931--2940, (2017).

\bibitem{madden2020high}
Liam Madden, Emiliano Dall'Anese, and Stephen Becker, `High-probability
  convergence bounds for non-convex stochastic gradient descent', {\em arXiv
  preprint arXiv:2006.05610v4}, (2021).

\bibitem{mai2021stability}
Vien~V Mai and Mikael Johansson, `Stability and convergence of stochastic
  gradient clipping: Beyond lipschitz continuity and smoothness', in {\em
  International Conference on Machine Learning}, pp. 7325--7335, (2021).

\bibitem{mcmahan2010adaptive}
H~Brendan McMahan and Matthew Streeter, `Adaptive bound optimization for online
  convex optimization', {\em arXiv preprint arXiv:1002.4908}, (2010).

\bibitem{menon2019can}
Aditya~Krishna Menon, Ankit~Singh Rawat, Sashank~J Reddi, and Sanjiv Kumar,
  `Can gradient clipping mitigate label noise?', in {\em International
  Conference on Learning Representations}, (2019).

\bibitem{nesterov2014introductory}
Iu.~E. {Nesterov}, {\em Introductory Lectures on Convex Optimization: A Basic
  Course}, 2014.

\bibitem{nesterov1983method}
Yurii~E Nesterov, `A method for solving the convex programming problem with
  convergence rate $o (1/k^{2} )$', in {\em Dokl. akad. nauk Sssr}, volume 269,
  pp. 543--547, (1983).

\bibitem{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro,
  `Exploring generalization in deep learning', in {\em Advances in Neural
  Information Processing Systems}, pp. 5947--5956, (2017).

\bibitem{panigrahi2019non}
Abhishek Panigrahi, Raghav Somani, Navin Goyal, and Praneeth Netrapalli,
  `Non-gaussianity of stochastic gradient noise', {\em arXiv preprint
  arXiv:1910.09626}, (2019).

\bibitem{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio, `On the difficulty of
  training recurrent neural networks', in {\em International conference on
  machine learning}, pp. 1310--1318, (2013).

\bibitem{polyak1964some}
Boris~T Polyak, `Some methods of speeding up the convergence of iteration
  methods', {\em Ussr computational mathematics and mathematical physics}, {\bf
  4}(5),  1--17, (1964).

\bibitem{rakhlin2012making}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan, `Making gradient descent
  optimal for strongly convex stochastic optimization', in {\em International
  Conference on Machine Learning}, pp. 1571--1578, (2012).

\bibitem{ramezani2021generalization}
Ali Ramezani-Kebrya, Ashish Khisti, and Ben Liang, `On the generalization of
  stochastic gradient descent with momentum', {\em arXiv preprint
  arXiv:2102.13653}, (2021).

\bibitem{reddi2016stochastic}
Sashank~J. {Reddi}, Ahmed {Hefny}, Suvrit {Sra}, Barnab\'{a}s {P\'{o}cz\'{o}s},
  and Alex {Smola}, `Stochastic variance reduction for nonconvex optimization',
  in {\em International Conference on Machine Learning}, pp. 314--323, (2016).

\bibitem{robbins1951stochastic}
Herbert Robbins and Sutton Monro, `A stochastic approximation method', {\em The
  annals of mathematical statistics},  400--407, (1951).

\bibitem{csimcsekli2019heavy}
Umut {\c{S}}im{\c{s}}ekli, Mert G{\"u}rb{\"u}zbalaban, Thanh~Huy Nguyen,
  Ga{\"e}l Richard, and Levent Sagun, `On the heavy-tailed theory of stochastic
  gradient descent for deep neural networks', {\em arXiv preprint
  arXiv:1912.00018}, (2019).

\bibitem{simsekli2019tail}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban, `A tail-index analysis of
  stochastic gradient noise in deep neural networks', in {\em International
  Conference on Machine Learning}, pp. 5827--5837, (2019).

\bibitem{tarres2014online}
Pierre {Tarres} and Yuan {Yao}, `Online learning as stochastic approximation of
  regularization paths: Optimality and almost-sure convergence', {\em IEEE
  Transactions on Information Theory}, {\bf 60}(9),  5716--5735, (2014).

\bibitem{tran2021better}
Hoang Tran and Ashok Cutkosky, `Better sgd using second-order momentum', {\em
  arXiv preprint arXiv:2103.03265}, (2021).

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin, `Attention is all you
  need', in {\em Advances in neural information processing systems}, pp.
  5998--6008, (2017).

\bibitem{ward2019adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou, `Adagrad stepsizes: Sharp convergence
  over nonconvex landscapes', in {\em International Conference on Machine
  Learning}, pp. 6677--6686, (2019).

\bibitem{you2017scaling}
Yang You, Igor Gitman, and Boris Ginsburg, `Scaling sgd batch size to $32k$ for
  imagenet training', {\em arXiv preprint arXiv:1708.03888}, (2017).

\bibitem{zhang2020improved}
Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang, `Improved analysis of
  clipping algorithms for non-convex optimization', {\em arXiv preprint
  arXiv:2010.02519}, (2020).

\bibitem{zhang2019gradient}
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie, `Why gradient
  clipping accelerates training: A theoretical justification for adaptivity',
  in {\em International Conference on Learning Representations}, (2019).

\bibitem{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
  Reddi, Sanjiv Kumar, and Suvrit Sra, `Why are adaptive methods good for
  attention models?', in {\em Advances in Neural Information Processing
  Systems}, (2020).

\bibitem{zhang2019adam}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim,
  Sashank~J Reddi, Sanjiv Kumar, and Suvrit Sra, `Why adam beats sgd for
  attention models', (2019).

\bibitem{zhang2005data}
Tong {Zhang}, `Data dependent concentration bounds for sequential prediction
  algorithms', in {\em Conference on Learning Theory}, pp. 173--187, (2005).

\bibitem{zhou2018convergence}
Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu,
  `On the convergence of adaptive gradient methods for nonconvex optimization',
  {\em arXiv preprint arXiv:1808.05671}, (2018).

\end{thebibliography}
