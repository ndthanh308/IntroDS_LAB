@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author =   "T. M. Mitchell",
  title =    "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year =     "1980",
  address =  "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor =   "R. S. Michalski and J. G. Carbonell and T.
          M. Mitchell",
  title =    "Machine Learning: An Artificial Intelligence
          Approach, Vol. I",
  publisher =    "Tioga",
  year =     "1983",
  address =  "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author =   "A. L. Samuel",
  title =    "Some Studies in Machine Learning Using the Game of
          Checkers",
  journal =  "IBM Journal of Research and Development",
  year =     "1959",
  volume =   "3",
  number =   "3",
  pages =    "211--229"
}
@article{bastianello2021stochastic,
  title={A Stochastic Operator Framework for Inexact Static and Online Optimization},
  author={Bastianello, Nicola and Madden, Liam and Carli, Ruggero and Dall'Anese, Emiliano},
  journal={arXiv preprint arXiv:2105.09884},
  year={2021}
}

@article{vladimirova2020sub,
  title={Sub-Weibull distributions: Generalizing sub-Gaussian and sub-Exponential properties to heavier tailed distributions},
  author={Vladimirova, Mariia and Girard, St{\'e}phane and Nguyen, Hien and Arbel, Julyan},
  journal={Stat},
  volume={9},
  number={1},
  pages={e318},
  year={2020}
}
@article{madden2020high,
  title={High-probability Convergence Bounds for Non-convex Stochastic Gradient Descent
},
  author={Madden, Liam and Dall'Anese, Emiliano and Becker, Stephen},
  journal={arXiv preprint arXiv:2006.05610v4},
  year={2021}
}
@article{wong2020lasso,
  title={Lasso guarantees for $\beta $-mixing heavy-tailed time series},
  author={Wong, Kam Chung and Li, Zifan and Tewari, Ambuj},
  journal={The Annals of Statistics},
  volume={48},
  number={2},
  pages={1124--1142},
  year={2020}
}
@article{li2018note,
  title={A note on concentration inequality for vector-valued martingales with weak exponential-type tails},
  author={Li, Chris Junchi},
  journal={arXiv preprint arXiv:1809.02495V3},
  year={2021}
}
@article{fan2019large,
  title={Large deviation inequalities for martingales in Banach spaces},
  author={Fan, Xiequan and Giraudo, Davide},
  journal={arXiv preprint arXiv:1909.05584},
  year={2019}
}
@inproceedings{li2020high,
  title={A High Probability Analysis of Adaptive SGD with Momentum},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={Workshop on Beyond First Order Methods in ML Systems at ICML},
  year={2020}
}
@article{lei2021learning,
  title={Learning Rates for Stochastic Gradient Descent with Nonconvex Objectives},
  author={Lei, Yunwen and Tang, Ke},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021}
}
@article{bottou2018optimization,
  title="Optimization Methods for Large-Scale Machine Learning",
  author="Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge",
  journal="Siam Review",
  volume="60",
  number="2",
  pages="223--311",
  year="2018"
}
@article{ghadimi2013stochastic,
  title="Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming",
  author="Saeed {Ghadimi} and Guanghui {Lan}",
  journal="Siam Journal on Optimization",
  volume="23",
  number="4",
  pages="2341--2368",
  year="2013"
}

@article{nemirovski2009robust,
  title="Robust Stochastic Approximation Approach to Stochastic Programming",
  author="A. {Nemirovski} and A. {Juditsky} and G. {Lan} and A. {Shapiro}",
  journal="Siam Journal on Optimization",
  volume="19",
  number="4",
  pages="1574--1609",
  year="2008"
}
@inproceedings{harvey2019tight,
  title={Tight analyses for non-smooth stochastic gradient descent},
  author={Harvey, Nicholas JA and Liaw, Christopher and Plan, Yaniv and Randhawa, Sikander},
  booktitle={Conference on Learning Theory},
  pages={1579--1613},
  year={2019}
}
@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964}
}
@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999}
}
@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}
@article{xu2020towards,
  title="Towards Optimal Problem Dependent Generalization Error Bounds in Statistical Learning Theory.",
  author="Yunbei {Xu} and Assaf {Zeevi}",
  journal="arXiv preprint arXiv:2011.06186",
  year="2020"
}
@inproceedings{xu2020niptowards,
  title={Towards Problem-dependent Optimal Learning Rates.},
  author={Xu, Yunbei and Zeevi, Assaf},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
@article{li2021improved,
  title={Improved Learning Rates for Stochastic Optimization: Two Theoretical Viewpoints},
  author={Li, Shaojie and Liu, Yong},
  journal={arXiv preprint arXiv:2107.08686},
  year={2021}
}
@inproceedings{zhang2020adaptive,
  title={Why are Adaptive Methods Good for Attention Models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}
@inproceedings{li2019convergence,
  title={On the convergence of stochastic gradient descent with adaptive stepsizes},
  author={Li, Xiaoyu and Orabona, Francesco},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={983--992},
  year={2019}
}
@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}
@article{lei2021generalization,
  title={Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions.},
  author={Lei, Yunwen and Hu, Ting and Tang, Ke},
  journal={Journal of Machine Learning Research},
  volume={22},
  pages={25--1},
  year={2021}
}
@inproceedings{zhang2005data,
  title="Data dependent concentration bounds for sequential prediction algorithms",
  author="Tong {Zhang}",
  booktitle="Conference on Learning Theory",
  pages="173--187",
  year="2005"
}
@inproceedings{cutkosky2020momentum,
  title={Momentum improves normalized sgd},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={International Conference on Machine Learning},
  pages={2260--2268},
  year={2020}
}
@inproceedings{foster2018uniform,
  title="Uniform Convergence of Gradients for Non-Convex Learning and Optimization",
  author="Dylan J. {Foster} and Ayush {Sekhari} and Karthik {Sridharan}",
  booktitle="Advances in Neural Information Processing Systems",
  pages="8745--8756",
  year="2018"
}
@inproceedings{lei2021sharper,
  title="Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions",
  author="Yunwen {Lei} and Yiming {Ying}",
  booktitle="International Conference on Learning Representations",
  year="2021"
}
@inproceedings{lei2021generalization,
  title="Generalization Guarantee of SGD for Pairwise
Learning
",
  author="Yunwen {Lei} and Mingrui {Liu} and Yiming {Ying}",
  journal="Advances in Neural Information Processing Systems",
  year="2021"
}
@inproceedings{reddi2016stochastic,
  title="Stochastic variance reduction for nonconvex optimization",
  author="Sashank J. {Reddi} and Ahmed {Hefny} and Suvrit {Sra} and Barnab\'{a}s {P\'{o}cz\'{o}s} and Alex {Smola}",
  booktitle="International Conference on Machine Learning",
  pages="314--323",
  year="2016"
}
@article{zhou2018generalization,
  title="Generalization Error Bounds with Probabilistic Guarantee for SGD in Nonconvex Optimization",
  author="Yi {Zhou} and Yingbin {Liang} and Huishuai {Zhang}",
  journal="arXiv preprint arXiv:1802.06903",
  year="2018"
}
@inproceedings{karimi2016linear,
  title="Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-{\L}ojasiewicz Condition",
  author="Hamed {Karimi} and Julie {Nutini} and Mark {Schmidt}",
  booktitle="European Conference on Machine Learning and Knowledge Discovery in Databases",
  pages="795--811",
  year="2016"
}
@inproceedings{charles2018stability,
  title="Stability and Generalization of Learning Algorithms that Converge to Global Optima",
  author="Zachary B. {Charles} and Dimitris S. {Papailiopoulos}",
  booktitle="International Conference on Machine Learning",
  pages="744--753",
  year="2018"
}

@inproceedings{li2017convergence,
  title="Convergence Analysis of Two-layer Neural Networks with ReLU Activation",
  author="Yuanzhi {Li} and Yang {Yuan}",
  booktitle="Advances in Neural Information Processing Systems",
  pages="597--607",
  year="2017"
}
@inproceedings{hardt2016identity,
  title="Identity Matters in Deep Learning",
  author="Moritz {Hardt} and Tengyu {Ma}",
  booktitle="International Conference on Learning Representations",
  year="2016"
}
@inproceedings{liu2016quadratic,
  title={Quadratic optimization with orthogonality constraints: Explicit lojasiewicz exponent and linear convergence of line-search methods},
  author={Liu, Huikang and Wu, Weijie and So, Anthony Man-Cho},
  booktitle={International Conference on Machine Learning},
  pages={1158--1167},
  year={2016}
}
@article{hardt2016gradient,
  title="Gradient Descent Learns Linear Dynamical Systems",
  author="Moritz {Hardt} and Tengyu {Ma} and Benjamin {Recht}",
  journal="Journal of Machine Learning Research",
  volume="19",
  number="29",
  pages="1--44",
  year="2018"
}
@inproceedings{liu2016quadratic,
  title={Quadratic optimization with orthogonality constraints: Explicit lojasiewicz exponent and linear convergence of line-search methods},
  author={Liu, Huikang and Wu, Weijie and So, Anthony Man-Cho},
  booktitle={International Conference on Machine Learning},
  pages={1158--1167},
  year={2016}
}
@article{sun2018geometric,
  title="A Geometric Analysis of Phase Retrieval",
  author="Ju {Sun} and Qing {Qu} and John {Wright}",
  journal="Foundations of Computational Mathematics",
  volume="18",
  number="5",
  pages="1131--1198",
  year="2018"
}
@article{li2019rapid,
  title={Rapid, robust, and reliable blind deconvolution via nonconvex optimization},
  author={Li, Xiaodong and Ling, Shuyang and Strohmer, Thomas and Wei, Ke},
  journal={Applied and computational harmonic analysis},
  volume={47},
  number={3},
  pages={893--934},
  year={2019}
}
@article{balakrishnan2017statistical,
  title="Statistical guarantees for the EM algorithm: From population to sample-based analysis",
  author="Sivaraman {Balakrishnan} and Martin J. {Wainwright} and Bin {Yu}",
  journal="Annals of Statistics",
  volume="45",
  number="1",
  pages="77--120",
  year="2017"
}
@book{nesterov2014introductory,
  title="Introductory Lectures on Convex Optimization: A Basic Course",
  author="Iu. E. {Nesterov}",
  year="2014"
}
@inproceedings{cutkosky2021high,
  title={High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@inproceedings{feldman2019high,
  title={High probability generalization bounds for uniformly stable algorithms with nearly optimal rate},
  author={Feldman, Vitaly and Vondrak, Jan},
  booktitle={Conference on Learning Theory},
  pages={1270--1279},
  year={2019}
}
@inproceedings{hardt2016train,
  title="Train faster, generalize better: stability of stochastic gradient descent",
  author="Moritz {Hardt} and Benjamin {Recht} and Yoram {Singer}",
  booktitle="International Conference on Machine Learning",
  pages="1225--1234",
  year="2016"
}
@article{mei2018landscape,
  title={The landscape of empirical risk for nonconvex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea and others},
  journal={Annals of Statistics},
  volume={46},
  number={6A},
  pages={2747--2774},
  year={2018}
}
@inproceedings{reddi2016stochastic,
  title="Stochastic variance reduction for nonconvex optimization",
  author="Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex",
  booktitle="International Conference on Machine Learning",
  pages="314--323",
  year="2016"
}
@inproceedings{kuzborskij2018data,
  title={Data-dependent stability of stochastic gradient descent},
  author={Kuzborskij, Ilja and Lampert, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={2815--2824},
  year={2018}
}
@article{li2021last,
  title={On the Last Iterate Convergence of Momentum Methods},
  author={Li, Xiaoyu and Liu, Mingrui and Orabona, Francesco},
  journal={arXiv preprint arXiv:2102.07002},
  year={2021}
}
@article{ghadimi2013stochastic,
  title="Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming",
  author="Saeed {Ghadimi} and Guanghui {Lan}",
  journal="Siam Journal on Optimization",
  volume="23",
  number="4",
  pages="2341--2368",
  year="2013"
}
@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}
@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in sgd},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  pages={3964--3975},
  year={2021}
}
@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  pages={5827--5837},
  year={2019}
}
@article{panigrahi2019non,
  title={Non-Gaussianity of stochastic gradient noise},
  author={Panigrahi, Abhishek and Somani, Raghav and Goyal, Navin and Netrapalli, Praneeth},
  journal={arXiv preprint arXiv:1910.09626},
  year={2019}
}
@article{csimcsekli2019heavy,
  title={On the Heavy-Tailed Theory of Stochastic Gradient Descent for Deep Neural Networks},
  author={{\c{S}}im{\c{s}}ekli, Umut and G{\"u}rb{\"u}zbalaban, Mert and Nguyen, Thanh Huy and Richard, Ga{\"e}l and Sagun, Levent},
  journal={arXiv preprint arXiv:1912.00018},
  year={2019}
}
@article{zhang2020improved,
  title={Improved Analysis of Clipping Algorithms for Non-convex Optimization},
  author={Zhang, Bohang and Jin, Jikai and Fang, Cong and Wang, Liwei},
  journal={arXiv preprint arXiv:2010.02519},
  year={2020}
}
@inproceedings{zhang2019gradient,
  title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
  author={Zhang, Jingzhao and He, Tianxing and Sra, Suvrit and Jadbabaie, Ali},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{gorbunov2020stochastic,
  title={Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  booktitle={Advances in Neural Information Processing Systems},
  pages={15042--15053},
  year={2020}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}
@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}
@inproceedings{vladimirova2019understanding,
  title={Understanding priors in bayesian neural networks at the unit level},
  author={Vladimirova, Mariia and Verbeek, Jakob and Mesejo, Pablo and Arbel, Julyan},
  booktitle={International Conference on Machine Learning},
  pages={6458--6467},
  year={2019}
}
@article{bottou2018optimization,
  title="Optimization Methods for Large-Scale Machine Learning",
  author="Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge",
  journal="Siam Review",
  volume="60",
  number="2",
  pages="223--311",
  year="2018"
}
@inproceedings{zhang2004solving,
  title="Solving large scale linear prediction problems using stochastic gradient descent algorithms",
  author="Tong {Zhang}",
  booktitle="International Conference on Machine Learning",
  pages="116",
  year="2004"
}
@inproceedings{bottou201113,
  title="The Tradeoffs of Large Scale Learning",
  author="Bousquet, Olivier and Bottou, L{\'e}on",
  booktitle="Advances in Neural Information Processing Systems",
  pages="161--168",
  year="2007"
}
@book{lan2020first,
  title={First-order and Stochastic Optimization Methods for Machine Learning},
  author={Lan, Guanghui},
  year={2020},
  publisher={Springer Nature}
}
@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={International Conference on Machine Learning},
  pages={6677--6686},
  year={2019}
}
@inproceedings{neyshabur2017exploring,
  title={Exploring Generalization in Deep Learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}
@inproceedings{allen2016variance,
  title="Variance reduction for faster non-convex optimization",
  author="Zeyuan {Allen-Zhu} and Elad {Hazan}",
  booktitle="International Conference on Machine Learning",
  pages="699--707",
  year="2016"
}
@article{ghadimi2016mini,
  title={Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  author={Ghadimi, Saeed and Lan, Guanghui and Zhang, Hongchao},
  journal={Mathematical Programming},
  volume={155},
  number={1-2},
  pages={267--305},
  year={2016}
}
@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}
@inproceedings{kakade2009generalization,
  title={On the generalization ability of online strongly convex programming algorithms},
  author={Kakade, Sham M and Tewari, Ambuj},
  booktitle={Advances in Neural Information Processing Systems},
  pages={801--808},
  year={2009}
}
@article{hazan2014beyond,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014}
}
@inproceedings{rakhlin2012making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  booktitle={International Conference on Machine Learning},
  pages={1571--1578},
  year={2012}
}
@article{lei2021generalization,
  title={Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions.},
  author={Lei, Yunwen and Hu, Ting and Tang, Ke},
  journal={Journal of Machine Learning Research},
  volume={22},
  pages={25--1},
  year={2021}
}
@inproceedings{davis2020high,
  title={High probability guarantees for stochastic convex optimization},
  author={Davis, Damek and Drusvyatskiy, Dmitriy},
  booktitle={Conference on Learning Theory},
  pages={1411--1427},
  year={2020}
}
@article{davis2021low,
  title={From Low Probability to High Confidence in Stochastic Convex Optimization.},
  author={Davis, Damek and Drusvyatskiy, Dmitriy and Xiao, Lin and Zhang, Junyu},
  journal={Journal of Machine Learning Research},
  volume={22},
  pages={49--1},
  year={2021}
}
@article{gorbunov2021near,
  title={Near-Optimal High Probability Complexity Bounds for Non-Smooth Stochastic Optimization with Heavy-Tailed Noise},
  author={Gorbunov, Eduard and Danilova, Marina and Shibaev, Innokentiy and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2106.05958},
  year={2021}
}
@inproceedings{jain2019making,
  title={Making the last iterate of sgd information theoretically optimal},
  author={Jain, Prateek and Nagaraj, Dheeraj and Netrapalli, Praneeth},
  booktitle={Conference on Learning Theory},
  pages={1752--1755},
  year={2019}
}
@inproceedings{lei2018stochastic,
  title={Stochastic composite mirror descent: Optimal bounds with high probabilities},
  author={Lei, Yunwen and Tang, Ke},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1519--1529},
  year={2018}
}
@inproceedings{camuto2021asymmetric,
  title={Asymmetric Heavy Tails and Implicit Bias in Gaussian Noise Injections},
  author={Camuto, Alexander and Wang, Xiaoyu and Zhu, Lingjiong and Holmes, Chris and G{\"u}rb{\"u}zbalaban, Mert and {\c{S}}im{\c{s}}ekli, Umut},
  booktitle={International Conference on Machine Learning},
  year={2021}
}
@article{kuchibhotla2018moving,
  title={Moving beyond sub-Gaussianity in high-dimensional statistics: Applications in covariance estimation and linear regression},
  author={Kuchibhotla, Arun Kumar and Chakrabortty, Abhishek},
  journal={arXiv preprint arXiv:1804.02605},
  year={2018}
}
@inproceedings{attia2021algorithmic,
  title={Algorithmic instabilities of accelerated gradient descent},
  author={Attia, Amit and Koren, Tomer},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002}
}
@inproceedings{savarese2021domain,
  title={Domain-independent dominance of adaptive methods},
  author={Savarese, Pedro and McAllester, David and Babu, Sudarshan and Maire, Michael},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  pages={16286--16295},
  year={2021}
}
@article{bubeck2015convex,
  title={Convex Optimization: Algorithms and Complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015}
}
@article{hazan2016introduction,
  title={Introduction to Online Convex Optimization},
  author={Hazan, Elad and others},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={2},
  number={3-4},
  pages={157--325},
  year={2016}
}
@article{jain2017non,
  title={Non-convex Optimization for Machine Learning},
  author={Jain, Prateek and Kar, Purushottam},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={10},
  number={3-4},
  pages={142--336},
  year={2017}
}
@inproceedings{nguyen2019first,
  title={First Exit Time Analysis of Stochastic Gradient Descent Under Heavy-Tailed Gradient Noise},
  author={Nguyen, Thanh Huy and Simsekli, Umut and Gurbuzbalaban, Mert and RICHARD, Ga{\"e}l},
  booktitle={Advances in Neural Information Processing Systems},
  pages={273--283},
  year={2019}
}
@inproceedings{hodgkinson2021multiplicative,
  title={Multiplicative noise and heavy tails in stochastic optimization},
  author={Hodgkinson, Liam and Mahoney, Michael},
  booktitle={International Conference on Machine Learning},
  pages={4262--4274},
  year={2021}
}
@article{bakhshizadeh2020sharp,
  title={Sharp concentration results for heavy-tailed distributions},
  author={Bakhshizadeh, Milad and Maleki, Arian and de la Pena, Victor H},
  journal={arXiv preprint arXiv:2003.13819},
  year={2020}
}
@article{fan2019large,
  title={Large deviation inequalities for martingales in Banach spaces},
  author={Fan, Xiequan and Giraudo, Davide},
  journal={arXiv preprint arXiv:1909.05584},
  year={2019}
}
@inproceedings{lei2021pairwise,
  title={Generalization Guarantee of SGD for Pairwise Learning},
  author={Lei, Yunwen and Liu, Mingrui and Ying, Yiming},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@article{srebro2010optimistic,
  title={Optimistic rates for learning with a smooth loss},
  author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1009.3896},
  year={2010}
}
@inproceedings{lei2021sharper,
  title="Sharper Generalization Bounds for Learning with Gradient-dominated Objective Functions",
  author="Yunwen {Lei} and Yiming {Ying}",
  booktitle="International Conference on Learning Representations",
  year="2021"
}
@inproceedings{bassily2020stability,
  title="Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses",
  author="Bassily, Raef and Feldman, Vitaly and Guzm{\'a}n, Crist{\'o}bal and Talwar, Kunal",
  booktitle="Advances in Neural Information Processing Systems",
  pages="4381--4391",
  year="2020"
}
@inproceedings{wang2021convergence,
  title={Convergence Rates of Stochastic Gradient Descent under Infinite Noise Variance},
  author={Wang, Hongjian and G{\"u}rb{\"u}zbalaban, Mert and Zhu, Lingjiong and {\c{S}}im{\c{s}}ekli, Umut and Erdogdu, Murat A},
  booktitle="Advances in Neural Information Processing Systems",
  year={2021}
}
@inproceedings{gurbuzbalaban2021fractional,
  title={Fractional moment-preserving initialization schemes for training deep neural networks},
  author={Gurbuzbalaban, Mert and Hu, Yuanhan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2233--2241},
  year={2021}
}
@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}
@article{davis2021graphical,
  title="Graphical Convergence of Subgradients in Nonconvex Optimization and Learning",
  author="Damek {Davis} and Dmitriy {Drusvyatskiy}",
  journal="Mathematics of Operations Research",
  year="2021"
}
@inproceedings{lei2020fine,
  title="Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent",
  author="Yunwen {Lei} and Yiming {Ying}",
  booktitle="International Conference on Machine Learning",
  pages="5809--5819",
  year="2020"
}
@inproceedings{zhang2017empirical,
  title="Empirical Risk Minimization for Stochastic Convex Optimization: $\mathcal{O}(1/n)$- and $\mathcal{O}(1/n^2)$-type of Risk Bounds",
  author="Lijun {Zhang} and Tianbao {Yang} and Rong {Jin}",
  booktitle="Conference on Learning Theory",
  pages="1954--1979",
  year="2017"
}

@inproceedings{zhang2019stochastic,
  title="Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the $\mathcal{O}(1/T)$ Convergence Rate",
  author="Lijun {Zhang} and Zhi-Hua {Zhou}",
  booktitle="Conference on Learning Theory",
  pages="3160--3179",
  year="2019"
}
@inproceedings{london2017pac,
  title="A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent",
  author="Ben {London}",
  booktitle="Advances in Neural Information Processing Systems",
  pages="2931--2940",
  year="2017"
}
@inproceedings{mou2018generalization,
  title={Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  booktitle={Conference on Learning Theory},
  pages={605--638},
  year={2018}
}
@article{neu2021information,
  title={Information-Theoretic Generalization Bounds for Stochastic Gradient Descent},
  author={Neu, Gergely},
  journal={arXiv preprint arXiv:2102.00931},
  year={2021}
}
@article{barsbey2021heavy,
  title={Heavy Tails in SGD and Compressibility of Overparametrized Neural Networks},
  author={Barsbey, Melih and Sefidgaran, Milad and Erdogdu, Murat A and Richard, Ga{\"e}l and {\c{S}}im{\c{s}}ekli, Umut},
  journal={arXiv preprint arXiv:2106.03795},
  year={2021}
}
@inproceedings{farghly2021time,
  title={Time-independent Generalization Bounds for SGLD in Non-convex Settings},
  author={Farghly, Tyler and Rebeschini, Patrick},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@inproceedings{liu2018fast,
  title="Fast Rates of ERM and Stochastic Approximation: Adaptive to Error Bound Conditions",
  author="Mingrui {Liu} and Xiaoxuan {Zhang} and Lijun {Zhang} and Rong {Jin} and Tianbao {Yang}",
  booktitle="Advances in Neural Information Processing Systems",
  pages="4678--4689",
  year="2018"
}
@article{zhang2019adam,
  title={Why ADAM beats SGD for attention models},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  year={2019}
}
@article{tarres2014online,
  title="Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence",
  author="Pierre {Tarres} and Yuan {Yao}",
  journal="IEEE Transactions on Information Theory",
  volume="60",
  number="9",
  pages="5716--5735",
  year="2014"
}
@inproceedings{kavis2021high,
  title={High Probability Bounds for a Class of Nonconvex Algorithms with AdaGrad Stepsize},
  author={Kavis, Ali and Levy, Kfir Yehuda and Cevher, Volkan},
  booktitle={International Conference on Learning Representations},
  year={2022}
}
@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951}
}
@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@article{mcmahan2010adaptive,
  title={Adaptive bound optimization for online convex optimization},
  author={McMahan, H Brendan and Streeter, Matthew},
  journal={arXiv preprint arXiv:1002.4908},
  year={2010}
}
@article{wilson1611lyapunov,
  title={A Lyapunov analysis of momentum methods in optimization. arXiv 2016},
  author={Wilson, AC and Recht, B and Jordan, MI},
  journal={arXiv preprint arXiv:1611.02635}
}
@article{ramezani2021generalization,
  title={On the Generalization of Stochastic Gradient Descent with Momentum},
  author={Ramezani-Kebrya, Ali and Khisti, Ashish and Liang, Ben},
  journal={arXiv preprint arXiv:2102.13653},
  year={2021}
}
@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate $O (1/k^{2} )$},
  author={Nesterov, Yurii E},
  booktitle={Dokl. akad. nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}
@inproceedings{cutkosky2021high,
  title={High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{mai2021stability,
  title={Stability and convergence of stochastic gradient clipping: Beyond lipschitz continuity and smoothness},
  author={Mai, Vien V and Johansson, Mikael},
  booktitle={International Conference on Machine Learning},
  pages={7325--7335},
  year={2021}
}
@article{mikolov2012statistical,
  title={Statistical language models based on neural networks},
  author={Mikolov, Tom{\'a}{\v{s}} and others},
  journal={Presentation at Google, Mountain View, 2nd April},
  volume={80},
  pages={26},
  year={2012}
}
@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}
@article{you2017scaling,
  title={Scaling sgd batch size to $32k$ for imagenet training},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@inproceedings{menon2019can,
  title={Can gradient clipping mitigate label noise?},
  author={Menon, Aditya Krishna and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@article{tran2021better,
  title={Better SGD using Second-order Momentum},
  author={Tran, Hoang and Cutkosky, Ashok},
  journal={arXiv preprint arXiv:2103.03265},
  year={2021}
}
@article{ghadimi2016accelerated,
  title={Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={Mathematical Programming},
  volume={156},
  number={1},
  pages={59--99},
  year={2016}
}
@article{allen2014linear,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014}
}
@inproceedings{li2022high,
  title={High Probability Guarantees for Nonconvex Stochastic Gradient Descent with Heavy Tails},
  author={Li, Shaojie and Liu, Yong},
  booktitle={International Conference on Machine Learning},
  pages={12931--12963},
  year={2022}
}
@article{liu2016algorithm,
  title={Algorithm-dependent generalization bounds for multi-task learning},
  author={Liu, Tongliang and Tao, Dacheng and Song, Mingli and Maybank, Stephen J},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={39},
  number={2},
  pages={227--241},
  year={2016}
}
@article{liu2020fast,
  title="Fast Cross-Validation for Kernel-Based Algorithms",
  author="Yong {Liu} and Shizhong {Liao} and Shali {Jiang} and Lizhong {Ding} and Hailun {Lin} and Weiping {Wang}",
  journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
  volume="42",
  number="5",
  pages="1083--1096",
  year="2020"
}
@article{li2019orthogonal,
  title={Orthogonal deep neural networks},
  author={Li, Shuai and Jia, Kui and Wen, Yuxin and Liu, Tongliang and Tao, Dacheng},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={4},
  pages={1352--1368},
  year={2019}
}
@article{krishnapuram2005sparse,
  title={Sparse multinomial logistic regression: Fast algorithms and generalization bounds},
  author={Krishnapuram, Balaji and Carin, Lawrence and Figueiredo, M{\'a}rio AT and Hartemink, Alexander J},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={27},
  number={6},
  pages={957--968},
  year={2005}
}
@article{musavi1994generalization,
  title={On the generalization ability of neural network classifiers},
  author={Musavi, Mohamad T and Chan, Khue Hiang and Hummels, Donald M and Kalantri, K},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={16},
  number={6},
  pages={659--663},
  year={1994}
}