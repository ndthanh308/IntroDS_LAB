\section{Related Work}

\noindent
\textbf{Fuzz Driver Generation.} \tab 
Several works~\cite{fuzzgen, fudge, apicraft, winnie, intelligen, rubick, utopia, chen2023hopper} have focused on developing automatic approaches to generate fuzz drivers.
Most of these works follow a common methodology, which involves generating fuzz drivers based on the API usage existed in consumer programs, \textit{i.e.,} programs containing code that uses these APIs.
For instance, by abstracting the API usage as specific models such as trees~\cite{apicraft}, graphs~\cite{fuzzgen}, and automatons~\cite{rubick}, several works propose program analysis-based methods to learn the usage models from consumer programs and conduct model-based driver synthesis.
In addition, a recent work~\cite{utopia} emphasizes that unit tests are high quality consumer programs and proposes techniques to convert existing unit tests to fuzz drivers.
Though these approaches can produce effective fuzz drivers, their heavy requirements on the quality of the consumer programs, \textit{i.e.,} the consumers must contain complete API usage and are statically/dynamically analyzable, limit their generality.
Furthermore, synthesized code often lacks human readability and maintainability, limiting their practical application.
\revision{
Some parallel works~\cite{oss-fuzz-gen,prompt-fuzz} have also explored the LLM-based fuzz driver generation.
However, their main goal is to build tools demonstrating the potential of LLM-based generation.
Our study complements them by focusing on delivering the first comprehensive understanding of the fundamental issues in this direction.
}
\compactline

% utopia, rubick, apicraft, ...

\noindent
\textbf{LLM for Generative Tasks.} \tab 
Recent works have explored the potential of LLM models for various generative tasks, such as code completion~\cite{wei2023magicoder}, test case generation~\cite{guifill, zhanglingming-llm-are-zero-shot-fuzzers, yang2023white, deng2023large, schafer2023adaptive, siddiq2023exploring, xia2023universal} and code repairing~\cite{sp-repair, abhik-repair, xia2023keep}.
These works utilize the natural language processing capabilities of LLM models and employ specific prompt designs to achieve their respective tasks.
To further improve the models' performance, some works incorporate iterative/conversational strategies or use fine-tuning/in-context learning techniques.
In test case generation, previous research works have primarily targeted on testing deep learning libraries~\cite{zhanglingming-llm-are-zero-shot-fuzzers, deng2023large} and unit test generation~\cite{schafer2023adaptive, siddiq2023exploring}. 
Considering the intrinsic differences between fuzz drivers and other tests and the difference on studied programming languages, these works cannot answer the fundamental effectiveness issues of LLM-based fuzz driver generation, indicating the unique values of our study.
% On one hand, unit test has distinct robustness requirements of the API usage compared with fuzz driver.
% on testing specific APIs, they are different in the underlying design logic.
% The former focuses on validating the program's outputs (mainly based on \texttt{assert} statements) under certain given inputs while fuzz driver is a different scenario where the code has to work properly for any randomly mutated input.
% This further incurs the distinct robustness requirements on the API usages.
% On the other hand, existing works mainly target on memory-safe languages while our study focuses on the C language.
% these works evaluate on text generation or code generation in memory-safe languages while our study targets on C languages.
% On the other hand, they 
% cannot directly answer the fundamental issues about the effectiveness on applying LLM models for fuzz driver generation.
% there is a significant lack of research on the effectiveness of LLM models for fuzz test generation in C projects. 
% Given the importance of generating fuzz driver for C project, this represents a critical area for further investigation.


% unit test generation
% evaluate copilot
% code repair
% ...
