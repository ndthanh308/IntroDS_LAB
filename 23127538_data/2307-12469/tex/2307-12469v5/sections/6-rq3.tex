\section{Characteristics of Key Design (\textbf{RQ3})}

% Figure environment removed

\subsection{Repeatedly Query}
% most are solved in the repeat process
% effectiveness of repeat degrades with the effectiveness of the configuration
% repeat 6 times
Repeated querying is a critical aspect of prompt strategies, greatly enhancing the success rate in generating fuzz drivers regardless of employed models, temperatures, and prompt designs.
Specifically, for the optimal configuration <gpt-4-0613, 0.5, ALL-ITER-K>, approximately 47.44\% of the issues were resolved by reinitiating the query process (37 out of 78 total resolved issues were solved upon repetition).
For the top-20 configurations, this contribution remains significantly high at an average of 67.50\%.
% all configurations: 56.63\%

Figure~\ref{fig:eff-repeatedly-query-a} displays the count of questions resolved through repeated querying across all evaluated configurations, ranked by their overall effectiveness as detailed in Table~\ref{tab:overall_eva_rslt}.
This demonstrates a direct correlation between the benefit of repeated queries and the efficacy of the configurationâ€”\textbf{the more effective a configuration, the greater the gains from repeating the queries}.

Additionally, Figure~\ref{fig:eff-repeatedly-query-b} presents the average percentage of questions resolved in each subsequent round of querying for the top-20 configurations.
Here, the percentage for round X is determined by $\frac{Rslt(X) - Rslt(X - 1)}{Rslt(1)}$, with $Rslt(X)$ indicating the number of questions resolved by round X.
The X-axis starting from round two, highlighting that the first round corresponds to the initial query.
This data shows that \textbf{the gain of repeated queries drops significantly after the initial few rounds}.
From our evaluation, we recommend limiting repeated queries to no more than six, where the sixth round still manages to resolve an additional 20\% of questions compared to the results of the first round.

% Repeatedly query is one essential design in prompt strategies which significantly contributes to the overall effectiveness of generating fuzz drivers.
% Specifically, for the best configuration <gpt-4-0613, 0.5, ALL-ITER-K>, around 47.44\% questions are solved by repeating the queries (37 solved by repeat out of 78 solved in total).
% For the top-20 configurations, the average figure keep as high as xx\%.

% Figure~\ref{fig:eff-repeatedly-query-a} lists the number of questions solved by repeatedly query for all evaluated configurations.
% The configurations are sorted by the number of their solved questions shown in Table~\ref{tab:eval_full}.
% The result clearly shows that the effectiveness of repeatedly query is proportional to the effectiveness of the configuration. 
% In other words, \textbf{more effective a configuration is, more benefits you will earn by employing repeatedly query design}.

% Figure~\ref{fig:eff-repeatedly-query-b} details the average percentage of solved question for each repeat round in top-20 configurations.
% Given one configuration, the percentage of round X is calculated by $\frac{Rslt(X) - Rslt(X - 1)}{Rslt(1)}$, where $Rslt(X)$ represents the number of solved questions in round X.
% Note that the round number in figure X-axis starting from two, \textit{i.e.}, the second round, which is the first round of repeatedly query.
% The plot illustrates that, \textbf{the benefits of repeatedly query significantly drops after the first several rounds of the repeat}.
% According to our evaluation, we suggest the time of repeatedly query is set less than six, the last round which has additionally solved 20\% number of questions comparing to the first round's result.

% top 1, 5, 10, 20 configurations
% Pct. solved by repeat: 0.430233, 0.532558, 0.493023, 0.448837
% solved by repeat: 37, 45.8, 42.4, 38.6
%
% suggested repeated time: 5 - 8 (0.2 & 0.1)
%
%
% TOP 20
%
%model: gpt-4-0613, temp: 0.5, strategy: ALL-ITER-K, final_tp: 78, rounds: 40
%model: gpt-4-0613, temp: 0.0, strategy: ALL-ITER-K, final_tp: 77, rounds: 40
%model: gpt-4-0613, temp: 1.0, strategy: ALL-ITER-K, final_tp: 76, rounds: 40
%model: gpt-3.5-turbo-0613, temp: 0.5, strategy: ALL-ITER-K, final_tp: 68, rounds: 40
%model: gpt-3.5-turbo-0613, temp: 0.0, strategy: ALL-ITER-K, final_tp: 65, rounds: 40
%model: gpt-3.5-turbo-0613, temp: 1.0, strategy: ALL-ITER-K, final_tp: 65, rounds: 40
%model: gpt-4-0613, temp: 0.5, strategy: UGCTX-K, final_tp: 63, rounds: 40
%model: gpt-4-0613, temp: 1.0, strategy: UGCTX-K, final_tp: 62, rounds: 40
%model: gpt-4-0613, temp: 1.0, strategy: BA-ITER-K, final_tp: 62, rounds: 40
%model: gpt-4-0613, temp: 0.5, strategy: BA-ITER-K, final_tp: 57, rounds: 40
%model: gpt-4-0613, temp: 0.0, strategy: BA-ITER-K, final_tp: 56, rounds: 40
%model: gpt-4-0613, temp: 0.0, strategy: UGCTX-K, final_tp: 55, rounds: 40
%model: wizardcoder-15b-v1.0, temp: 1.0, strategy: ALL-ITER-K, final_tp: 53, rounds: 40
%model: wizardcoder-15b-v1.0, temp: 0.5, strategy: UGCTX-K, final_tp: 50, rounds: 40
%model: wizardcoder-15b-v1.0, temp: 0.5, strategy: ALL-ITER-K, final_tp: 48, rounds: 40
%model: wizardcoder-15b-v1.0, temp: 1.0, strategy: UGCTX-K, final_tp: 48, rounds: 40
%model: gpt-3.5-turbo-0613, temp: 0.5, strategy: UGCTX-K, final_tp: 47, rounds: 40
%model: gpt-3.5-turbo-0613, temp: 0.5, strategy: BA-ITER-K, final_tp: 47, rounds: 40
%model: gpt-3.5-turbo-0613, temp: 1.0, strategy: UGCTX-K, final_tp: 43, rounds: 40
%model: gpt-3.5-turbo-0613, temp: 1.0, strategy: BA-ITER-K, final_tp: 43, rounds: 40


% \begin{table}[t]
% \centering
% \caption{Questions Solved by Repeat in Top-X <Model, Temperature, Strategy>.}
% \label{tab:solved-by-repeat-in-top-x}
% \resizebox{0.8\linewidth}{!}{
% \begin{tabular}{lllll}
% % \hline
% \toprule
% Top-X 
 % & X=1
 % & X=5
 % & X=10
 % & X=20 \\
% \midrule
% % \rowcolor{black!10}
% Solved by Repeat & 43\% (37) & 53\% (46) & 49\% (42) & 45\% (39) \\
% % \hline
% \bottomrule
% \end{tabular}
% }
% \label{tab:cmp-different-k}
% \end{table}

% The observation \ding{184} indicates the usefulness of repetitive queries.
% properly exploiting the randomness in LLM replies can significantly improve the overall performance.
% For \ding{184}, though the ratio of total corrected questions for 40 rounds to one round can more than nine, the benefits of the repetitive queries rapidly decrease after certain rounds of repeated queries, roughly following the Pareto Principle~\cite{pareto-principle}.
% Specifically, as shown in Table~\ref{tab:cmp-different-k}, roughly 80\% of the overall performance of repetitive queries are contributed in the initial 20\% rounds of queries. 
% all settings have reached nearly or more than 80\% performance at the sixth round comparing with the total outcomes of 40 rounds.
% In general, a simple stop condition can be proposed, \textit{e.g.}, no performance increase in last X rounds, to balance the benefits and query costs.
% the number of repeat times, \textit{a.k.a.} the value of \textbf{K}, can be adaptively determined according to the past statistics of the new correct answers.

% \noindent
% \textbf{Question-Level Performance} \tab 
% % Besides discussing the overall performance, we analyzed these strategies on question-level statistics.
% Figure~\ref{fig:upset-plot-for-simple-strategies} shows the UpSet plot.
% % analyzing the solved questions sets for the above strategies.
% Among all 86 questions, 35 of them (40.70\%) have not been solved by any basic strategy, which shows a large space for further improvement.
% An interesting observation is that most strategies have uniquely solved questions: \texttt{gpt3.5-BACTX-K}, \texttt{gpt4-NAIVE-K}, and \texttt{gpt4-BACTX-K} have 2, 5, and 11 uniquely solved questions respectively. 
% On the one hand, it is an evidence that the current most effective strategy \texttt{gpt4-BACTX-K} still cannot outperform the rests in all respects. 
% On the other hand, it reminds us the probabilistic nature of the language models.
% For instance, though \texttt{gpt4-NAIVE-K} have same prompts/model as \texttt{gpt4-BACTX-K} and contain less descriptive information about the target API, it still can solve 5 questions that \texttt{gpt4-BACTX-K} failed to solve in 40 rounds.
% One possible explanation is that, for these questions, the less descriptive prompts generated by \texttt{gpt4-NAIVE-K} can happenly guide the \texttt{gpt4} model produce effective answers.
% A supportive finding is that all 5 questions uniquely solved by \texttt{gpt4-NAIVE-K} are of one project and their API usage follow similar design patterns.
% Besides, the average query success rate of \texttt{gpt4-BACTX-K} is higher than the rest questions, which indicates that \texttt{gpt4-BACTX-K} not only solves more questions but also solves them more reliably.
% % Specifically, the 11 questions uniquely solved by \texttt{gpt4-BACTX-K} cover 6 projects and have an average success rate 28.41\% on query, while the \texttt{gpt3.5-BACTX-K} and \texttt{gpt4-NAIVE-K}'s 2/5 questions cover 1/1 projects, with a success rate of 2.50\%/10.50\%.


% Figure environment removed

\subsection{Query With Extended Information}
% QSTN: EX, NOEX, IN, OUT
% 0.673333 0.359167 0.9275 0.143333
% QUERY: EX, NOEX, IN, OUT
% 0.300833 0.0875 0.193333 0.0491667

\noindent
\textbf{Querying With API Documentation.}
\tab
By comparing DOCTX-K and BACTX-K, we found that \textbf{there is no significant changes between their results in the metrics of resolved questions}.
On one hand, a significant percentage (43\%) of APIs in the evaluated questions do not have API documentation (49 out of 86 have).
When there is no documentation for an API, the DOCTX-K queries are identical to BACTX-K's.
On the other hand, adding API documentation in the queries may not provide enough details directly stating the API usage.
This is because these API documentations usually contain a high-level description of the usage, typically a summary of main functionality with one-sentence explanations for arguments. 
However, the blocker-solving usage information discussed in Section~\ref{sec:failure-analysis}, such as low level argument initialization specifics, control flow dependencies, or the usages of its dependent APIs, is usually not included.
% \zhc{add some data on these 49 Qs?}
% For all 49 questions that have API documentation, we compared the performance of \texttt{DOCTX-K} with that of \texttt{BACTX-K} under \texttt{gpt3.5} and \texttt{gpt4} models to understand its effectiveness.
% The first row of Figure~\ref{fig:upset-plot-for-extended} shows upset plot and Figure~\ref{fig:extended-info-succ-rate-plots-per-score}'s plots the comparison in metrics of question and query success rate.
% Line color represent strategy type while line style stands for the model type.
% The black lines represent the performance of \texttt{gpt4} while the red for \texttt{gpt3.5}'s.
% The lines with cross sign markers are of \texttt{BACTX-K} and triangle markers are of \texttt{DOCTX-K}.
% Overall, \texttt{DOCTX-K} slightly outperforms \texttt{BACTX-K} in \texttt{gpt3.5} model while performs nearly identical in \texttt{gpt4}.
% under \texttt{gpt4} model, both strategies solve the same number of questions and their query success rates are nearly identical.
% In \texttt{gpt3.5}, it uniquely solves three more questions than \texttt{BACTX-K}.
% only had a slightly better performance than \texttt{BACTX-K} by uniquely solving four questions, whereas \texttt{BACTX-K} could only solve one unique question.
% in the documentation.
% usage information required to solve the blockers analyzed in

% % \zhc{add a case analysis here to support the conclusions in detail.}
\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
API documentation has minor performance benefits due to the limited usage description it contained.
\end{tcolorbox}
% while adding API documentation in prompt causes no negative effects on the effectiveness, it can only improve the performance in limited cases.}

%% Figure environment removed



% 7712/32367
% example sources analysis
\noindent
\textbf{Querying With Example Code Snippets.} 
\tab
When comparing the results of BACTX-K and UGCTX-K presented in Table~\ref{tab:overall_eva_rslt}, we can clearly observe that incorporating example code snippets substantially enhances performance in most configurations.
In particular, the addition of example snippets results in an average resolution of 104\% more questions across the 22 evaluated configurations, which includes five models and five different temperature settings.

Nonetheless, further analysis reveals that \textbf{the inclusion of usage examples incurs a much higher token cost, with an average increase of tenfold}.
The ratio of token costs for these two approaches varies from 4.20 to 39.71 across all configurations, with an average ratio of 14.65.
Notably, the UGCTX-K approach demands an average of 32,367 tokens to generate a single correct solution.

Figure~\ref{fig:succ-rate-of-different-ex-sources} depicts our investigation into the impact of different sources of example snippets on the quality of solutions.
This figure assesses the success rates of queries/questions associated with various example sources, which are categorized in two distinct manners based on their file paths: first, as \ding{182} \textit{External} vs. \textit{Internal}, with \textit{Internal} comprising the target project and its variations, and \textit{External} consisting of all other sources; second, as \ding{183} \textit{Test \& Example} vs. \textit{Others}, where the first group includes files with paths that contain "test" or "example" in any capitalization.
The underlying data for these plots stems from questions that were solved by UGCTX-K but not by BACTX-K across all tested configurations.
According to this analysis, it is clear that \textbf{both \textit{Internal} and \textit{Test \& Example} sources are associated with significantly higher quality example snippets in comparison to their counterparts}.

% \zhc{analysis on the distraction?}

% The plots states that, for all quiz questions, the number of correct fuzz drivers generated from the prompts built based on examples from internal source is two times more than the external's.
% For the questions that are not 
% \zhc{missing interpretation on data}
% \zhc{do we need to mention how we did this source identification? in appendix?}
% high effectiveness
% cost high
% the effectiveness depends on the quality of example sources, we identified two high quality sources
% This indicates that example snippets are not free to add since unhelpful code can distract or even confuse the model, leading to unsuccessful generation.
% In buckets have higher score, this disturbance is not observable since the query success rate of \texttt{BACTX-K} is typically low.
% \texttt{UGCTX-K} is evaluated on all quiz questions.
% The second row of Figure~\ref{fig:upset-plot-for-extended} shows the upset plot while Figure~\ref{fig:extended-info-succ-rate-plots-per-score}'s compares in metrics of question and query success rate.
% Similarly, line colors/styles distinguishes prompt templates/models.
% Overall, \texttt{UGCTX-K} clearly outperforms \texttt{BACTX-K}:
% in terms of overall effectiveness.
% The union results of \texttt{gpt3.5-UGCTX-K} and \texttt{gpt4-UGCTX-K} correctly generates effective fuzz drivers for 
% Considering the union results of \texttt{UGCTX-K},
% \texttt{gpt3.5-UGCTX-K} and \texttt{gpt4-UGCTX-K} 
% it solved 70 questions which covers almost all questions solved by \texttt{BACTX-K}. 
% For each model, Figure~\ref{fig:extended-qstn-succ-rate-per-score} shows that \texttt{UGCTX-K} (red lines) is always higher than or equal to corresponding \texttt{BACTX-K} (black lines).
% , indicating a clear advantage for \texttt{UGCTX-K}.
% Interestingly, Figure~\ref{fig:extended-query-succ-rate-per-score} shows that \texttt{UGCTX-K} has lower query success rates than \texttt{BACTX-K} in low score questions.
% \yaowen{need to check} Additionally, the intersection point (the third score bucket $[7,9]$) is the point where the question success rates of \texttt{BACTX-K} drop significantly. 
% This effect is particularly pronounced for questions that are likely to be solved without the help of snippets (buckets with lower scores).
% significantly affected by the questions it cannot solve.



\noindent
\textbf{Case Studies.}
\tab
% Two cases are discussed to show how usage snippets help in solving the common blockers.
% counter-intuitive case: 
\# 9 \texttt{wc\_Str\_conv\_with\_detect}
\tab
This case is challenging due to the unintuitiveness of its API usage.
The API declaration is "\texttt{Str wc\_Str\_conv\_with\_detect(Str is,wc\_ces * f\_ces,wc\_ces hint,wc\_ces t\_ces)}".
It is used for converting the input stream \texttt{is} from one CES (character encoding scheme, \texttt{f\_ces}) to another (\texttt{t\_ces}).
Most basic strategy drivers made mistakes on the creations of either \texttt{is} (the confusing type \texttt{Str}) or CESs, where \texttt{is} has to be created using particular APIs like \texttt{Strnew\_charp\_n} and CESs should be specific macros or carefully initialized \texttt{struct}.
Example helps here by directly providing the usage to models.

% control flow conditions: parse_xxx
\# 37 \texttt{igraph\_read\_graph\_graphdb}
\tab
The hardest part in this case is the implicit control flow dependency it required.
Besides correctly initializing the arguments, it has to call an API to mute the builtin error handlers.
By default, the API will abort immediately when any abnormal input is detected, which causes frequent false crashes blocking the fuzzing progress.
% when an abnormal input is detected, the API will abort frequently which blocks the fuzzing progress and raise large amount of false crashes.
To mute it, the driver needs to custmoize the error handler, \textit{e.g.}, call \texttt{igraph\_set\_error\_handler\-(igraph\-\_error\_handler\_ignore)}.
% a customized error handler needs to be 
% the driver has to set a silent error handler, such as calling 
This requirement is hard to be inferred beforehand due to its semantic nature and few inference clues.
However, some unit tests in project such as \texttt{foreign\_empty.c} contain this usage, which directly instructs the generation.


% Figure environment removed
\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
    \revision{
Example code snippets can greatly enhance model performance by providing direct insights on API usage.
    }
"test/example files", "code files from the target/variant projects" are high quality sources.
% In summary, example code snippets can significantly improve the overall performance.
% They directly provides the usages which models usually failed to infer.
% However, adding usage snippets can cause performance decline if low quality candidates are used.
\end{tcolorbox}

\subsection{Iterative Query}
% performance is better than others
% average cost is even higher 
% 159% baiter improves basic
% 23% alliter improves usage
The iterative query strategy is another key design that can lead to significant improvements in performance.
Referring to Table~\ref{tab:overall_eva_rslt}, we find that, on average, incorporating an iterative query strategy into BACTX-K -- that is, adopting the BA-ITER-K approach -- helps solve 159\% more questions.
Similarly, ALL-ITER-K resolves 23\% more questions than UGCTX-K. 
However, this strategy does come at a cost.
\textbf{The inclusion of iterative design tends to lead to higher token usage when generating correct solutions}.
On average, the iterative strategy increases token costs by 57\% for BACTX-K per successful driver generation and by 17\% for UGCTX-K.
% Specifically, the token cost ratio between BA-ITER-K and BACTX-K varies from 1.29 to 2.45 across all configurations, while the ratio of ALL-ITER-K to UGCTX-K ranges from 0.67 to 1.85.

The effectiveness of the iterative strategy can be attributed to two key factors.
Firstly, \textbf{it leverages a wider array of information}, including error data generated from validating previously generated drivers.
Secondly, \textbf{it tackles the problem incrementally}, employing a step-by-step, divide-and-conquer approach that simplifies the complexity of the generation task.
This methodology is exemplified in the case studies that follow, illustrating how the iterative strategy typically operates through practical examples.

% 
% 
%% Figure environment removed
%
%% Figure environment removed

%% Figure environment removed

% \noindent
% \textbf{Effectiveness of Iterative Query}
% \tab
% which questions are compared
% introduce the plots
% Due to the search nature and high cost of the evaluation, we use partial questions for evaluation.
% In our experiments, both the maximum iteration round and the \texttt{K} are set as 20.
% Figure~\ref{fig:all-iter-succ-rate-plots-per-score} and ~\ref{fig:upset-plot-for-iter} provide comparison results.
% compare \texttt{ITER-K} and \texttt{UGCTX-K} and the two iterative strategies. 
% conclusions draw from plots
% Generally, \texttt{ITER-K} strategies have clear performance benefits.
% They solve almost all questions solved by \texttt{UGCTX-K} and uniquely solves nine questions, leaving only six questions unsolved.
% Besides, its question success rate is always higher than its counterpart's (red lines are higher than black lines).
% It is worth to mention that mostly \texttt{ITER-K} has lower query success rate than \texttt{UGCTX-K}, indicating its higher average search cost in generation.
% overall performance advantage -> can solve hard question
% low success rate -> high cost 
% Indeed, averagely the number of queries required by iterative strategies is 3-5 times to non-iterative strategies (see detailed data in~\cite{fuzz-drvier-study-website}).

\noindent
\textbf{Case Studies.}
\tab
% Two cases are discussed to demonstrate how this strategy solves the questions others failed to.
\#5 \texttt{md\_html}
\tab
This API requires the preparation of a customized callback function pointer as the argument, where all previous strategies failed to figure out.
% Previous strategies failed to provide a valid callback function pointer as the argument in this case.
% The API usage required in this case is not complex but the previous strategies failed to provide a valid callback function pointer as the argument.
The callback function is used to handle the output data of API.
% To correctly execute the API, the driver has to prepare a function which handles the outputted data of the API and passes it to the API.
All the drivers generated by \texttt{UGCTX} either pass a \texttt{NULL} pointer or a non-existing function name.
Iterative query guides the fix by providing the link error highlighting that this referred function is undefined.

\#73 \texttt{pj\_stun\_msg\_decode}
\tab
This is another typical case why iterative strategy works.
% suits the multi-round iterative solution.
% one of the iterative query found the correct answer in five iteration rounds.
% The initialization of its first argument, a pointer to a memory pool used for runtime memory management, has multi-level API dependencies.
The initialization of its first argument has multi-level API dependencies.
The dependency chain is: 
\ding{182} the API ->
\ding{183} \texttt{pj\_pool\-\_create} -> 
\ding{184} \texttt{pj\_caching\_pool\-\_init}, where -> means depends.
All non-iterative strategies failed to prepare a driver with all correct usage detail of these indirect dependencies while iterative strategies solve this by providing error related feedback to LLMs and solving multiple errors one by one.
% identifying the error related API and providing correct usage for correction. and it solves multiple errors one by one.
% requires the calls of the two APIs in order:
% \ding{182}  for initializing caching pool object \texttt{pj\_ca\-ching\_pool};
% \ding{183} \texttt{pj\_pool\-\_create} for initializing memory pool object \texttt{pj\_pool\_t}.
% The iterative query solves this by first correcting the wrongly used API for initializing caching pool
% first corrects the wrongly used API for initializing caching pool object (\ding{182}), then figures out the mismatched type error for calling \ding{183}.
In one of the solved iterative query, it first corrects the incorrect used API of \ding{184}, then figures out the mismatched type error when calling \ding{183}.
Lastly, for the driver's runtime crash, LLMs use two rounds to fix according to the assertion code located from crash stacks.
% related implementation code of failed assertion lines located by crash stacks, the iterative query uses two iterations to test out the correct size check conditions for the mutated input data.

% md html, shows the iterative advantage (for fixing easy errors)
% a hard question, shows the necessity of the iterative for solving this question

\noindent
% % \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
    \revision{
Iterative query helps in utilizing more diverse information and solving the problem in a step-by-step manner.
However, it has higher token cost and increased complexity.
    }
\end{tcolorbox}