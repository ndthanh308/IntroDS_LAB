\section{Introduction}

% \zhc{What is fuzz driver generation}

Fuzz testing, aka fuzzing, has become the standard approach for discovering zero-day vulnerabilities.
Fuzz drivers are necessary components for fuzzing library APIs since fuzzing requires a directly executable target program.
% Since fuzzing requires a directly executable target program, fuzz drivers have become a necessary component of library API fuzzing.
Essentially, a fuzz driver is a piece of code responsible for accepting mutated input from fuzzers and executing the APIs accordingly.
An effective driver must contain a correct and robust API usage since incorrect or unsound usage can result in extensive false positive or negative fuzzing results, incurring extra manual validation efforts or testing resources waste.
Due to the high standard required, fuzz drivers are typically written by human experts, which is a labor-intensive and time-consuming process.
For instance, OSS-Fuzz~\cite{oss-fuzz}, the largest fuzzing framework for open-source projects, maintains thousands of fuzz drivers written by hundreds of contributors over the past seven years.

% Fuzz testing \lyt{(aka fuzzing)} has emerged as the de-facto standard for discovering zero-day vulnerabilities.
% Since applying fuzzing requires a directly executable target program, fuzz drivers are necessary components of API fuzzing.
% Essentially, it is a piece of code responsible for accepting mutated input from fuzzers and executing the APIs accordingly.
% An effective fuzzing driver must ensure both correctness and robustness in the API usage since any incorrect or unsound API usage may lead to extensive false positives or negatives during fuzzing, wasting the testing resources or incurring extra human validation efforts.
% Due to the high standard, fuzz drivers used to be composed by human experts, which is laber-intensive and time-consuming.
% For example, OSS-Fuzz~\cite{oss-fuzz}, the largest continuous fuzzing framework for open source projects, maintains thousands of fuzz drivers written by hundreds of contributors in the past seven years.

% \zhc{What are SOTA methods and their limitations}
% program analysis assisted, rule-based generation/transformation.

% Most of these works follow a common methodology, which involves generating fuzz drivers based on the API usage existed in consumer programs, \textit{i.e.,} programs containing code that uses these APIs.
% For instance, by abstracting the API usage as specific models such as trees~\cite{apicraft}, graphs~\cite{fuzzgen}, and automatons~\cite{rubick}, several works propose program analysis-based methods to learn the usage models from consumer programs and conduct model-based driver synthesis.
% In addition, a recent work~\cite{utopia} emphasizes that unit tests are high quality consumer programs and proposes techniques to convert existing unit tests to fuzz drivers.
% Though these approaches can produce effective fuzz drivers, their heavy requirements on the quality of the consumer programs, \textit{i.e.,} the consumers must contain complete API usage and are statically/dynamically analyzable, limit their generality.
% Furthermore, code synthesized by many works lacks human readability and maintainability, which can limit their practical application.



% Recently, various works~\cite{fuzzgen, fudge, apicraft, winnie, intelligen, rubick, utopia} have focused on developing automatic approaches to generate fuzz drivers.
% Most works follow a methodology which generates fuzz drivers by learning or converting API usages from existing example programs (aka consumer programs).
% Though these approaches can produce effective drivers, they rely on the consumer programs heavily, \textit{i.e.}, the consumers must contain complete API usage and are statically/dynamically analyzable, which limits their practicality.

% \zhc{LLM-based generation for fuzz drivers is important, promising but under-studied}

% \zhc{motivation of our study}
Generative LLMs (Large Language Models) have gained significant attention for their ability in code generation tasks~\cite{chatgpt,openai2023gpt4, roziere2023codellama, luo2023wizardcoder}.
They are language models trained on vast quantities of text and code, providing a conversational workflow where natural language based queries are posed and answered.
LLM-based fuzz driver generation is an attractive direction.
On one hand, LLMs inherently support fuzz driver generation as API usage inference is a basic scenario in LLM-based code generation.
On the other hand, LLMs are lightweight and general code generation platforms.
Existing works~\cite{fuzzgen, fudge, apicraft, winnie, intelligen, rubick, utopia}, which generate drivers by learning API usage from examples, requires program analysis on examples, while LLM-based generation can mostly work on texts.
% Compared to existing works which generate drivers by learning API usage from examples via program analysis based techniques, LLM-based generation can mostly work on texts.
This offers enhanced generality which facilitates not only the application on massive quantity of real-world projects but also the utilization of learning inputs in different forms.
Various sources of API usage knowledge such as documentation, error information, and code snippets can be seamlessly integrated in text form, benefiting the generation.
Moreover, LLMs can generate human-friendly code.
While some research efforts have been devoted to LLM-based code generation tasks~\cite{arxiv-adaptive-unit-test-gen,zhanglingming-llm-are-zero-shot-fuzzers,jiang2023selfplanning,pearce2022asleep,jain2022jigsaw,liu2023your,oss-fuzz-gen,prompt-fuzz}, none of them can provide a fundamental understanding on this direction.
% have addressed the fundamental issues concerning the application of LLMs to fuzz driver generation such as the effects, challenges, and techniques.

% First, the generality of the generation approaches is promisingly high.
% This is because the consumer programs are no longer prerequisites but auxiliaries in generation.
% Besides, benefited by the multilingual support of LLMs, utilizing various sources of information such as documentation for further improvement is more convenient.
% Moreover, LLM-generated code is human-friendly.
% While some research efforts have been devoted to LLM-based code generation tasks~\cite{arxiv-adaptive-unit-test-gen, zhanglingming-llm-are-zero-shot-fuzzers, jiang2023selfplanning,pearce2022asleep,jain2022jigsaw}, none of them have addressed the fundamental issues concerning the application of LLMs to fuzz driver generation such as the effects, challenges, and techniques.
% such as whether and to what extent LLMs can generate effective fuzz drivers, and the challenges and strategies for improvement, and the pros and cons of the generated fuzz drivers compared to those practically used.
% \zhc{perhaps the first half of this paragraph can be shrinked, add more reasoning why existing efforts cannot answer?}

% \zhc{How we do to fill the gap}

% % Figure environment removed

%% To address this issue, we propose and evaluate various query strategies that can generate fuzz drivers for a specific API.
% The issue is explored by proposing and evaluating various query strategies for generation.
%% that generate fuzz drivers for a specific API.
%For a systematic study, we categorized the strategies and studied them from basic to enhanced.
%As shown in Figure~\ref{fig:coordinates-for-study}, the categorization is based on the used information type and conversation type of a strategy.
%Basic strategies only query with fundamental API information, such as header file name and function declaration, and communicate with the model in a one-and-done manner.
%Enhanced strategies can add extended information such as the API documentation and example code snippets into the query prompt, and can interactively communicate with the model to iteratively improve the generated drivers.
%When proposing specific strategies, both the prompt design and the number of times a query is repeated are considered.
%% After investigating the strategies' effectiveness and characteristics, we compared the generated drivers with OSS-Fuzz drivers to obtain practical implications.
%Besides, we also compared the generated drivers with OSS-Fuzz drivers to obtain practical implications.
%% positions, advantages, and disadvantages of LLM-generated drivers.

% We started from examining basic query strategies (\textbf{RQ1}), then explored the complex ones (\textbf{RQ2}).
% Lastly, we compared the generated drivers with OSS-Fuzz drivers for further understanding (\textbf{RQ3}).

% Since many factors influencing the evaluation of a strategy, we first divided the strategies as basic and enhanced strategies, and studied them from basic to enhanced.
% As shown in Figure~\ref{fig:coordinates-for-study}, the categorization is based on the type of information used in the query and the type of the conversation.
% Basic strategies only contain fundamental information of the target in the query and communicates with the models 

% established a complexity standard categorizing the strategies and examined them from basic to enhanced.
% the type and the organization of information used in the query, the number of iteration and repetition of the query, we established a complexity standard and examined the strategies from basic to extended.
% including what information will be used and the way to organize them in the prompt, the number of times a query is repeated, and the conversation is interactive or not, 
% Figure~\ref{fig:coordinates-for-study} 

% This approach is significant because simplicity offers greater generality.
% Understanding the limits of simpler strategies can inform the design and utilization of more complex ones.
% To investigate the effectiveness of LLM in generating effective fuzz drivers, we first adopt the basic strategy that employs fundamental information about the target API, such as function declaration and header file name, to construct prompts for instructing the LLM.
% We prioritize this strategy because (1) \textit{Necessity}: fundamental information is necessary to explain the target API clearly and ensure the basic quality of the query. (2) \textit{Generality}: fundamental information is widely accessible, ensuring the generality of the task.

% After analyzing the results of the basic strategy, we can identify the challenges that LLM faces in generating effective fuzz drivers.
% This understanding helps us optimize the technique of using LLM to generate fuzz drivers.
% Based on these challenges, we propose advanced strategies that improve on two dimensions, which involve more types of information and interactive times of conversation.
% These advanced strategies can address the ineffectiveness of LLM in handling some complex tasks.

% To explain the strategies more clearly, we defined both basic strategy and advanced strategy as shown in Figure~\ref{fig:coordinates-for-study}.
% Basic strategy is the one that utilizes \textbf{fundamental} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}and \textbf{Non-Interactive} conversations.
% Otherwise, it is advanced strategy.
% As defined in Figure~\ref{fig:coordinates-for-study}, a strategy is simple if it only uses \textbf{Basic} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.} and \textbf{Non-Interactive} conversations.
% Otherwise, it is complex.
% the complexity of query strategies is measured in two dimensions: \ding{182} the type of information used for prompt composition\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}, \ding{183} and the type of the conversation.
% The \textbf{fundamental} type covers widely accessible information including function declaration and the header file name an API belongs to, while \textbf{Extended} type additionally includes API usage information which cannot be assumed generally accessible such as documentation, implementation code, and usage example code.
% The difference between \textbf{Non-Interactive} and \textbf{Interactive} conversations is whether the intermediate information produced during the conversation is utilized for improving the generation.
% \textbf{Interactive} coversations may be multi-round while the \textbf{Non-Interactive} ones' are one-round.

% \zhc{detailed objective of our study}
To address this gap, we conducted an empirical study for understanding the effectiveness of zero-shot fuzz driver generation using LLMs.
Note that our primary goal is to understand the basics towards generating "\textit{more}" effective fuzz drivers, rather than generating "\textit{more effective}" fuzz drivers.
This is because creating effective drivers for more targets is a more fundamental issue than improving existing ones.
Overall, four research questions are studied:
\begin{itemize}
    \item \textbf{RQ1}\tab 
    To what extent can current LLMs generate effective fuzz drivers for software testing? 
    \item \textbf{RQ2}\tab 
    What are the primary challenges associated with generating effective fuzz drivers using LLMs?
    \item \textbf{RQ3}\tab 
    What are the effectiveness and characteristics for different prompting strategies?
    \item \textbf{RQ4}\tab 
    How do LLM-generated drivers perform comparing to those practically used in the industry?
    % \textit{i.e.}, OSS-Fuzz drivers?
    % both in code quality and fuzzing effectiveness?
\end{itemize}

To answer these RQs, we assembled a dataset of 86 fuzz driver generation questions collected from 30 widely-used C projects from OSS-Fuzz projects.
Each question represents a library API for which a corresponding fuzz driver is needed to conduct effective fuzz testing.
We devised six prompt strategies, taking into account three key factors: the content of the prompts, the nature of interactions between the strategies and models, and the repetition of the entire query process.
Our evaluation encompassed five state-of-the-art LLMs with five different temperature settings.
The assessed LLMs included closed-source LLMs such as gpt-4-0613~\cite{gpt-4-models}, gpt-3.5-turbo-0613~\cite{gpt-3.5-models}, and text-bison-001~\cite{palm2-text-models}, as well as open-source LLMs optimized for code generation, namely, codellama-34b-instruct~\cite{codellama-34b-instruct} and wizardcoder-15b-v1.0~\cite{wizardcoder-15b-v1.0}.
For a rigorous assessment, we developed an evaluation framework automatically validating the generated drivers based on the results of compilation and short-term fuzzing, and manually crafted checkers on API usage semantic correctness.
In total, 736,430 fuzz drivers, at the cost of 0.85 billion tokens (\$8,000+ charged tokens, 0.17/0.21 billion for gpt-4-0613/gpt-3.5-turbo-0613), were evaluated.
Besides, comparison with manually written drivers in industry on code and fuzzing metrics, \textit{e.g.}, 24-hour fuzzing experiments (3.75 CPU-year), are conducted to seek practical insights.
\compactline

% \zhc{methodology of our study}
% \zhc{What we practically do for answering the RQs}
% To assess the effectiveness of the generated fuzz drivers in scale, we developed a semi-automated evaluation framework containing a quiz and a set of effectiveness criteria.
% The APIs used in the questions were collected by analyzing 51 fuzz drivers from 30 popular C projects.
% We aimed at C projects since it is a typical memory-unsafe language where fuzz testing has shown great effectiveness in finding vulnerabilities and bugs. 

% prepared a set of APIs along with an evaluation framework.
% Each API is set as a generation question asking the LLMs to generate fuzz driver for conducing effective fuzz testing to that API.
% Overall, there are 86 APIs collected from 30 popular OSS-Fuzz C projects.
% And the framework facilitates the automation of 
% criteria checks whether a driver is effective or not.
% Precisely validating fuzz driver's effectiveness in large scale is challenging since it requires validating the driver's API usage semantics, involving the identification of both false positives (bugs caused by drivers) and negatives (ineffective but non-crash usage).
% To address this, our criteria have two parts: general checks for successfully launching short-term fuzzing, and API-specific driver tests written manually for usage semantics validation.
% % We also built a framework that can validate the generated drivers and provide taxonomized results, reducing the manual efforts required for evaluation.
% In total, 189,628 drivers generated by five strategies (0.22 billion tokens) are evaluated.

% from 51 fuzz drivers covering 30 popular C projects 
% The involved APIs in quiz are collected from 51 fuzz drivers covering 30 popular C projects fuzzed by OSS-Fuzz.
% And the criteria is a semi-automatic approach containing API specific checks for precise effectiveness validation.
% Upon these, an evaluation framework is built to maximize the automation in driver evaluation process.

% designed a quiz consisting of fuzz driver generation questions and an evaluation standard.
% Each question in the quiz required writing a fuzz driver for an API qualified for fuzz testing.
% We identified the qualified APIs from popular open-source projects that are suitable for fuzzing by extracting the key APIs from the fuzz drivers of projects fuzzed by OSS-Fuzz repositories.
% The key APIs were either explicitly specified according to the author's comments and driver file names or picked according to the complexity score calculated using OSS-Fuzz Introspector~\cite{fuzz-introspector}.\zhc{check here later}
% The quiz comprised \zhc{X} questions from \zhc{Y C/C++} projects in total.
% An effective fuzz driver was defined as one that, under empty input seed, it can be built and fuzzed for a period of time (1 minute in our study) without discovering false positive bugs.
% As it is challenging to automatically determine whether a fuzz driver is effective or not, we wrote specific tests for some questions, filtered the frequently found true bugs based on their crash stack signatures, and used them to correct evaluation results.
% Besides, we built an evaluation framework based on the quiz and evaluation standard to reduce manual efforts required for evaluation.
% With these tools, we conducted our study on the research questions outlined above.

% \zhc{findings of our study}
% The optimal configuration, which uses gpt-4-0614 under temperature 0.5 with our most sophisticated query strategy, can generate at least one effective fuzz drivers for 91\% questions (78/86).
% And half of the questions can be resolved by top 20 configurations, covering the two OpenAI gpt models and one open-source model wizardcoder-15b-v1.0.
The overall implications for the effectiveness of using LLM to generate fuzz drivers are two-fold.
On one hand, LLMs have demonstrated outstanding performance in evaluated configurations\footnote{A configuration stands for a combination of <Model, Prompt Strategy, Temperature>.}, suggesting a strong potential for this approach.
For instance, the optimal configuration can address 91\% questions (78/86) and all top 20 configurations can address at least half of the questions.
On the other hand, resolving a question means successful generation of at least one effective fuzz driver for the assessed API, which does not necessarily imply full practicality.
For high automation and usability, three challenges have been identified:
\ding{182} Improving the success rate to reduce generation costs.
Although most questions can be resolved by LLMs, the cost can be exceptionally high.
Typically, 71\% of questions are resolved by repeating the entire query process at least five times and 45\% require repeating the process ten times.
By enhancing their accuracy, significant financial costs for automation can be saved.
\ding{183} Ensuring semantic correctness in API usage.
Occasionally, validating the effectiveness of a generated driver requires the understanding of API usage semantics.
Failed to do so can result in ineffective fuzzing with false positive or negative results.
In our evaluation, this requirement was observed in 34\% of the assessed APIs (29/86), impeding practical application.
\ding{184} Addressing complex API dependencies.
6\% of questions (5/86) cannot be resolved by any evaluated configurations since their drivers' generation requires nontrivial preparation of the API execution contexts, which cannot be appropriately hinted by any collected usage information.
For example, some drivers require a standby network server or client to be created for interacting with the target API.
These are typical cases representing complex real-world testing requirements which deserves exploration of advanced solutions.

Prompt strategy, temperature, and model are key factors considerably affect the overall performance.
Our evaluation suggests that the dominant strategy is the one incorporating three key designs: repeatedly query, query with extended information, and iterative query.
Comparing with naive strategy, its question resolve rate soars from 10\% to 91\%.
Evaluation with lower temperature settings, especially below the threshold of 1.0, have higher performance.
This is intuitive since lower temperatures lead to more consistent and predictable outputs, which fits the goal of generating an effective fuzz driver.
Besides, the optimal temperature setting in our evaluation is 0.5.
As for models, gpt-4-0613, wizardcoder-15b-v1.0 are the best closed-source, open-source models, respectively.

Fundamentally, LLMs struggle to generate fuzz drivers which require complex API usage specifics.
We identified three beneficial designs that have distinct characteristics:
\ding{182} repeatedly queries.
When the configurations are stronger, the benefits of repetition become higher.
Besides, the benefits significantly drop after the first few rounds.
A suggested repetition value is 6.
\ding{183} query with extended information.
Adding API documentation is less helpful while adding example snippets can help significantly.
Specifically, test/example files of the target project or its variants are high-quality example sources;
\ding{184} iterative queries. 
It adds a cyclic driver fix progress after the initial query, which improves LLM's performance through its step-by-step problem-solving approach and a more thorough utilization of existing usage.
Besides, all the above designs will significantly increase the token cost. 
In comparison to OSS-Fuzz drivers, LLM-generated drivers demonstrated comparable fuzzing outcomes.
However, since LLMs tend to generate fuzz drivers with minimal API usages, significant room is still left for improving generated drivers, such as expanding API usage and incorporating semantic oracles.

\revision{
To further translate our research insights into practical values, part of our prompting strategies, including checking, categorizing, and fixing driver with runtime errors, have been implemented into OSS-Fuzz-Gen~\cite{oss-fuzz-gen}, the largest LLM-based fuzz driver generation framework operated by Google OSS-Fuzz team, facilitating the continuous fuzzing of real-world projects.
}
\compactline

% For the optimal configuration, which uses gpt-4-0613 under temperature 0.5 with our most comprehensive query strategy, it can successfully generate at least one effective fuzz driver for 91\% (78/86) questions.

% Overall, the results demonstrated encouraging practicality of the LLM-based generation.
% The union results of these strategies have generated correct fuzz drivers for 55 (64\%) questions entirely automatically, and an additional 23 (91\%) questions can be solved by integrating manually configured semantic validators.
% Regarding to query strategies specifics, basic approaches have shown decent results (53\%, 46/86), particularly given its low technical complexity and limited information requirements about the target API.
% The repeatedly query design incorporated in basic approaches contributes significantly but its benefits gradually become minor when the time of repeat increases (roughly follows 80/20 rule).
% However, basic strategies struggle to generate fuzz drivers which require complex API usage detail.
% Two key designs help significantly: querying with examples (84\%, 72/86) and iteratively querying (91\%, 78/86).
% The effects of the former design highly relies on the quality of the code snippets and test/example files of the target project or its variants are identified high-quality sources.
% The iterative strategy, which can incorporate all key designs, shows the dominant performance.
% It outperforms all others by its step-by-step problem-solving approach and a more thorough utilization of existing usage.
% However, it suffers from a higher search costs (2-4 times more queries in evaluation).
% Compared to OSS-Fuzz drivers, LLM-generated drivers demonstrated comparable fuzzing outcomes.
% However, significant rooms are still left for improving generated drivers, such as enhancing semantic correctness filtering to increase practicality, expanding API usage, and incorporating semantic oracles.

% Overall, the combined results of all strategies show that they can pass 91\% (78/86) of the questions.
% While this result indicates the strong potential of LLM-based driver generation, it does not necessarily represent its practicality.
% This is because the contribution of the validation criteria in filtering out ineffective drivers cannot be ignored.
% Without precise validation, the results may contain many ineffective fuzz drivers, which seriously hinders the practical application.
% Therefore, further improvements are needed to address these challenges and maximize the practicality.
% of LLM-based fuzz driver generation.
% Overall, the union results of all strategies can pass xx\% (xx/86) questions.
% However, this result only proves the strong potential of LLM-based driver generation but cannot represents its practicality.
% This is because the contribution of the validation criteria filtering out the ineffective drivers cannot be ignored.
% Without a precise validation, the results can contain many ineffective fuzz drivers, hindering its practical application.
% The interpretation of the result is two-fold.
% On one hand, \textbf{the high correct rate shows high potential of LLM-based fuzz driver generation}.
% On the other hand, the high correct rate is based on the results filtered from xxx queries.
% Since the validation criteria contain manually collected API specific checks, how to precisely filtering the ineffective drivers is a major obstacle for the practical application of LLM-based approaches.
% \textbf{Without a carefully developed validation criteria for the given targets, LLM-based approaches can have spam issues}.
% \zhc{The evaluation results}
% LLMs struggle to generate fuzz drivers that require complex API usage specifics. Three key designs can help: repeatedly querying, querying with examples, and iteratively querying. A combination of these designs yields a dominant query strategy (ITER-K).
% For specific strategies, the basic ones have shown decent results (\zhc{xx\%}), particularly when considering the fact that they require few information about the target and have low technical complexity.
% However, its performance will decline significantly when the complexity of the API specific usage increases.
% For the questions they cannot figure out, adding example code snippets and query iteratively can help significantly (additionally solves \zhc{xx\%}).
% The example code snippets helps by directly providing the usage to model, which highly depends on the quality of the snippets.
% We identified that "test/example file" and "internal code files of the target/variant projects" are two high quality sources (contributes \%);
% The iterative queries can solve complex questions due to its comprehensive utilization of the usage information and the step-by-step problem solving approach.
% However, it suffers from high search costs and strategy complexity.
% most failures are mistakes made on API specific usages.
% The involved usages are broad, covering various dimensions from the target API usage to its dependent APIs', from the grammatical errors to semantic errors, raising challenges for improvement.
% For enhanced strategies:
% \ding{182} adding API documentation in prompt can slightly improve overall performance (xx\%) due to the limited usage description it contains;
% \ding{183} example code snippets can greatly enhance the overall performance (xx\%).
% Besides, "test/example file", "internal code files of the target/variant projects" are high quality sources for code snippets (contributes \%);
% \ding{184} query iteratively can solve questions that non-iterative strategies cannot solve.
% The superiority comes from its more comprehensive utilization of the usage information and the step-by-step problem solving approach.

% Comparing with OSS-Fuzz drivers, LLM-generated drivers provide competent fuzzing outcomes.
% However, as previously discussed, practical validation methods must be adopted to prevent ineffective testing and false alarms.
% Besides, manually written fuzz drivers offer unique advantages that can guide the improvement of LLM-generated drivers, such as the ability to include semantic oracles that detect logical bugs.

% By comparing with OSS-Fuzz drivers, we found that the LLM-generated drivers can provide competent fuzzing outcomes.
% However, as discussed above, practical validation methods should be adopted for preventing ineffective testing or the massive false alarms.
% Besides, manually written fuzz drivers showed some unique advantages directing the improvement of LLM-generated, such as the inclusion of semantic oracles which can detect logical bugs.

% \ding{182} LLMs tend to produce fuzz drivers with minimal API usages while adding example snippets improves the contained usages;
% \ding{183} OSS-Fuzz drivers contain oracles which can check semantic bugs while LLM-generated do not have any;
% \ding{184} LLM-generated drivers can reach comparable fuzzing outcomes as OSS-Fuzz drivers.
% However, as discussed above, how to practically pick effective drivers is the major challenge for its large scale application.

% the interpretation of the result is two-fold
% 1. it shows strong potential 
% 2. the validation criteria is still a major obstacle for its practical application, which requires appropriate approaches/methods
% specifically, 
%  basic strategies performance, characteristics, etc
%  enhanced strategies and their characteristics
%  the metrics comparison conclusion

% \zhc{Contribution}
In summary, our contributions are:
\begin{itemize}
    \item we conducted the first in-depth study on the effectiveness of LLM-based fuzz driver generation, which showcases the potentials and challenges of this direction;
    \item we designed and implemented six driver generation strategies.
    They are evaluated in large scale, with a systematic analysis on the effectiveness, the pros and the cons;
%    \item we built the first fuzz driver evaluation framework which can evaluate the generated fuzz drivers in scale;
    \item we compared generated drivers with industrial used ones, and summarized the implications on future improvements.
    \item \revision{we ported our strategies to improve the largest industrial fuzz driver generation framework, facilitating the continuous fuzzing of hundreds open-source projects.}
\end{itemize}
