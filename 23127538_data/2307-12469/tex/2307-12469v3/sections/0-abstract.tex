% regarding the fundamental issues, such as the effectiveness and challenges, associated with this approach.

% 848280090, 736430, 
% gpt-4, 166869179
% 0.17 billion gpt-4-0613

% We found that three prompting techniques can be notably beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process;

Fuzz drivers are essential for library API fuzzing. However, automatically generating fuzz drivers is a complex task, as it demands the creation of high-quality, correct, and robust API usage code.
An LLM-based (Large Language Model) approach for generating fuzz drivers is a promising area of research.
Unlike traditional program analysis-based generators, this text-based approach is more generalized and capable of harnessing a variety of API usage information, resulting in code that is friendly for human readers.
However, there is still a lack of understanding regarding the fundamental issues on this direction, such as its effectiveness and potential challenges.

To bridge this gap, we conducted the first in-depth study targeting the important issues of using LLMs to generate effective fuzz drivers.
Our study features a curated dataset with 86 fuzz driver generation questions from 30 widely-used C projects.
Six prompting strategies are designed and tested across five state-of-the-art LLMs with five different temperature settings.
In total, our study evaluated 736,430 generated fuzz drivers, with 0.85 billion token costs (\$8,000+ charged tokens).
Additionally, we compared the LLM-generated drivers against those utilized in industry, conducting extensive fuzzing experiments (3.75 CPU-year).
Our study uncovered that:
\ding{182} While LLM-based fuzz driver generation is a promising direction, it still encounters several obstacles towards practical applications;
\ding{183} LLMs face difficulties in generating effective fuzz drivers for APIs with intricate specifics.
Three featured design choices of prompt strategies can be beneficial: issuing repeat queries, querying with examples, and employing an iterative querying process;
\ding{184} While LLM-generated drivers can yield fuzzing outcomes that are on par with those used in the industry, there are substantial opportunities for enhancement, such as extending contained API usage, or integrating semantic oracles to facilitate logical bug detection.
\revision{
Our insights have been implemented to improve the OSS-Fuzz-Gen project, facilitating practical fuzz driver generation in industry.
}
\compactline

% Fuzz drivers are necessary for library API fuzzing.
% Automatic fuzz driver generation is challenging since it requires generating high quality API usage code which is correct and robust.
% LLM-based (Large Language Model) fuzz driver generator is a promising direction.
% Compared to traditional program analysis based generators, it is a text-based approach which is more general.
% It can easily leverage various dimensions of API usage information during the generation and produce human-friendly code.
% Nonetheless, there still lacks the basic understanding on this direction.

% To fill this gap, we conducted the first in-depth study targeting the fundamental issues of using LLMs on effective fuzz driver generation.
% For comprehensive understanding, we assembled a dataset covering 86 fuzz driver generation questions collected from 30 popular C projects.
% Six prompting strategies have been further designed and evaluated towards five state-of-the-art LLMs and five different temperatures.
% In total, \zhc{xxx,xxx} generated fuzz drivers using \zhc{xxx} billion tokens are evaluated.
% Besides, generated drivers were compared with industrial used ones to obtain practical insights (\zhc{xxx} CPU year fuzzing experiments).
% Our study revealed that:
% \ding{182} LLM-based fuzz driver generation has promising potential but faces several challenges towards practical application;
% \ding{183} LLMs struggle to generate effective fuzz drivers for APIs requiring complex API usage specifics. Three prompt designs can be helpful: repeatedly query, query with examples, and iteratively query;
% \ding{184} Drivers generated by LLMs can produce competent fuzzing outcomes compared to industrial applied ones. However, significant rooms still left for improvement, such as extending API usage or incorporating more semantic oracles for logical bug detection.

% -- the best configuration in evaluation can generate at least one effective fuzz driver for 91\% assessed questions.
% \ding{182}
% LLM-based generation has shown promising practicality.
% 64\% questions can be solved entirely automatically and the number raises to 91\% if manual semantic validators are incorporated.
% Besides, the generated drivers showed comparative performance compared to industrial used ones;
% Moreover, the generated drivers exhibited competitive performance to those commonly employed in the industry;
% \ding{183}
% LLMs struggle to generate fuzz drivers that require complex API usage specifics.
% Three key designs can help: repeatedly querying, querying with examples, and iteratively querying.
% Combining them yields a dominant strategy;
% \ding{184}
% Significant rooms for improvement are still left, such as automatic semantic correctness validation, API usage expansion, and semantic oracle generation.

% \ding{182} while the overall performance was promising (passing 91\% of questions), there were still practical challenges in filtering out the ineffective fuzz drivers for large scale application;
% \ding{183} basic strategies achieved a decent correctness rate (53\%), but struggled with complex API-specific usage questions.
% In such cases, example code snippets and iterative queries proved helpful;
% \ding{184} while LLM-generated drivers showed competent fuzzing outcomes compared to manually written ones, there was still significant room for improvement, such as incorporating semantic oracles for logical bugs detection.

% Fuzzing are de-facto standard for finding zero-day vulnerabilities.
% Fuzz drivers are necessary for fuzzing the API targets.
% For fuzzing API targets which are not directly executable, fuzz drivers have to be introduced.
% They act as bridges which first accept the mutated input from fuzzer and then executes the target APIs accordingly.
% Fuzz drivers are necessary for fuzzing API targets.
% Automatic fuzz driver generation is hard due to the high requirements on driver's correctness and robustness.
% Comparing with existing fuzz driver generation approaches, LLM-based (Large Language Model) generation is a promising direction since it has low requirements on consumer programs, native support for utilizing multiple dimensions of API usage information, and human-friendly output code.
% However, there still lacks an understanding on the challenges and effects for LLM-based fuzz driver generation.

% In this work, we studied the effects, challenges, and techniques of LLM-based fuzz driver generation.
% We first built a quiz containing 86 fuzz driver questions and their effectiveness validation criteria from 30 popular C projects.
% Then a framework was developed to maximize the automation of evaluation.
% Five query strategies are designed and their generated xxx fuzz drivers are studied.
% Besides, the generated fuzz drivers are compared with manually written drivers to obtain practical implications.
% Our evaluation reveals that:
% \ding{182} though the overall performance is quite promising (pass xx\% questions), its practical application still faces challenges such as how to precisely filter out ineffective fuzz drivers;
% \ding{183} while basic strategies can already achieve a decent correct rate (xx\%), it cannot solve the questions require a complex API specific usage.
% Adding example snippets and query iteratively can help in this scenario;
% \ding{184} LLM-generated drivers can show competent fuzzing outcomes comparing with manually-written ones.
% However, it still has large improvement space such as adding semantic oracles.

% the precise effectiveness validation standard is the major concern for its practical application;
% \ding{183} carefully designed basic strategies which only use fundamental information and non-iterative conversation can reach good results (xx\%), while the enhanced ones 
% based on fundamental information and non-iterative conversations can reach xx\% results while adding example snippets and iterative queries can solve more;
% \ding{184} LLM-generated drivers can reach comparable fuzzing outcomes as manually written ones 

% different query strategies show their unique pros and cons.

% the basic challenges of LLM-based generation are the API-specific semantic constraints and indirectly dependent usages;
% \ding{183} combining all strategies, current LLMs can provide effective fuzz drivers for xx\% questions, showing certain practicality;
% \ding{184} LLM-generated drivers are complement with manually-written drivers. besides, it can still be improved in oracles, usage extending, etc.
