% \section{Discussion}
% \label{sec:discussion}
\section{Discussion}
\label{sec:ttv}

\revision{
\noindent
\textbf{Relationships With OSS-Fuzz-Gen}
\tab
The Google OSS-Fuzz team has undertaken a parallel work called OSS-Fuzz-Gen~\cite{oss-fuzz-gen} for leveraging LLMs to generate fuzz drivers.
Based on their public information, \textit{i.e.}, one security blog~\cite{oss-fuzz-gen-blog} and the source code repository~\cite{oss-fuzz-gen}, our work is complementary to theirs.
Overall, current stage of OSS-Fuzz-Gen puts high efforts on filling the engineering gap between LLM interfaces and OSS-Fuzz projects.
Their experiments are conducted on top commercial LLMs, aiming to showcase that LLM-generated fuzz drivers can help in finding zero-day vulnerabilities and reaching new testing coverage.
However, there is no experiments or documentation discussing fundamental generation questions such as the design choices behind their prompt strategy, what are the pros and cons for different strategies, how the effectiveness varies for different models and model parameters, and what are the inherent challenges and potential future directions.
Our study, on the other hand, complements theirs by exploring these fundamental issues.
We carefully designed prompt strategies, evaluated them on various models (open and commerical LLMs) and temperatures, and distilled findings from the results. 
}

\revision{
\noindent
\textbf{Contributing to OSS-Fuzz-Gen}
\tab
%To turn our insights into practical industry values,
We carefully examined the prompt strategies of OSS-Fuzz-Gen from their implementation and validated where our insights can help.
Interestingly, their current strategy support part of our insights.
For instance, they adopted 10 time repeat results~\cite{oss-exp-repeat} and used a lower temperature (0.4) in experiments~\cite{oss-default-temperature}.
Besides, we found that OSS-Fuzz-Gen only identifies and fixes build errors while ignoring the runtime errors caused by driver.
Their generation ends when a compilable fuzz driver is synthesized and then they manually checks the validity of these drivers.
To improve this, we implemented our strategies for drivers with fuzzing runtime errors in their platform, including the identification (automatic part of validation process)~\cite{oss-pr-191, oss-pr-187, oss-pr-199, oss-pr-185}, categorization, and the corresponding iterative fix procedure~\cite{oss-pr-204, oss-pr-198}.
%The implementation has lasts over one month.
% and includes the implementation of fuzzing runtime error identification~\cite{oss-pr-191, oss-pr-187, oss-pr-199, oss-pr-185} and the expansion of their iterative logic to rectify drivers with fuzzing errors~\cite{oss-pr-204, oss-pr-198}.
These enhancements added new functionalities refining the generation results, where the cases showing its effectiveness are quickly identified~\cite{oss-pr-effective-case, oss-pr-fp-filter-case} during their benchmark tests (29 APIs, 18 projects).
Currently, the improvement is merged into the main branch and is actively used to fuzz all 282 supported projects, marking a significant milestone to us.
We are keeping refine our commitments, such as integrating more fine-grained error information during fix. 
%\compactline
}

\revision{
\noindent
\textbf{Potential Improvements}
\tab
% domain knowledge understanding
%  - agent, rag
%  - combination with traditional methods
% model fundamental ability
From our perspective, to improve the performance of LLM-based fuzz driver generation, efforts from three dimensions can be further explored.
First, the domain knowledge contained inside the target scope can be modeled and utilized for better generation.
For instance, to test network protocol APIs, the communication state machine of that protocol can be learned first and then used to guide the driver generation.
Besides, more sophisticated prompt-based solutions can be explored, such as hybrid approaches combining traditional program analysis and prompt strategies, or agent-based approaches.
Lastly, fine-tuning based methods is also a promising direction since this can enhance both the generation's effectiveness and efficiency from a model level.
%\compactline
}

% However, they do not provide a systematic analysis on the fundamental issues of LLM-based fuzz driver generation, such as the effectiveness on different model settings or parameters.
% Different from them, we aim to provide a comprehensive understanding on the fundamental issues of LLM-based fuzz driver generation.  

%\noindent
%\textbf{Evaluating More LLMs}
%\tab
%Previous sections have analyzed the results based on two state-of-the-art OpenAI LLMs.
%They are selected due to their superior performance comparing to other LLMs.
%Besides, we also evaluated Google Palm2 \texttt{text-bison-001}~\cite{anil2023palm}.
%% , CodeGen 1 (2b/8b/16b)~\cite{xxx}, and Codex (code-cushman-002/code-davinci-002)~\cite{xxx}.
%% The latter two series are significantly worse than gpt-3.5-turbo-0314 and 
%On our quiz questions, Palm2 has shown a close performance with \texttt{gpt-3.5-turbo\-0301} but still performed observably worse than it.
%And the main conclusions draw from Palm2 data is consistent with the above presented.
%We are continuously evaluating more code related LLMs such as CodeGen 2~\cite{nijkamp2023codegen2}, Llama 2~\cite{touvron2023llama}, etc.
%All these additional results are updated at~\cite{fuzz-drvier-study-website}.

% \noindent
% \textbf{Practical Application}
% \tab
% While the evaluations show high question success rates for LLM-based generation approaches, this does not necessarily mean they are ready to be applied at scale.
% The main challenge lies in identifying effective fuzz drivers from the large number of generated ones.
% This is difficult since precise identification needs correct classification of false positives (bugs caused by the driver) and negatives (ineffective usage), which require semantic understanding of API usage.
% Future directions for addressing this challenge include developing advanced automatic approaches and balancing automation with human interventions.
% Root cause analysis methods and project-specific domain knowledge can be useful in building automatic approaches.
% Human-assisted approaches require a well balance between automation requirements and manual efforts.
% Recent examples of this approach include Github Copilot~\cite{copilot}.
% Adopting or improving on this approach to support security experts could be a meaningful and interesting direction.

%
% the validation problem, spam issue 
%
% domain knowledge, human intervention, 
%  iterative collection or refinement
%

% \noindent
% \textbf{Improvements on Query Strategies}
% \tab
% % \textbf{Unsolved Questions}
% % Written with high level planning
% % Missing Contextual Knowledge
% % \textbf{Further Improvements: More Languages, Better Refinements, Fine-tuned Model, ...}
% % The query strategies evaluated can be further improved in two directions:
% % towards generating fuzz drivers
% Besides the directions discussed in Section~\ref{sec:rq3}, the contextual issue reflected by these high score questions which cannot be solved by any strategy is also an interesting direction.
% These questions failed since their driver generation requires the understanding of specific contexts.
% For instance, generating the driver for \texttt{tmux}~\cite{tmux-ossfuzz-driver-link} requires the understanding of various concepts, such as session, window, pane, etc, and their relationships.
% Similarly, for network-related questions~\cite{libmodbus-ossfuzz-driver-link, civetweb-ossfuzz-driver-link}, a standby network server or client is required to be created before calling the target API.
% The effective drivers can only be generated by respecting these specific contextual requirements.
% % Ignoring these contextual requirements can never 
% % Thus, 
% One future direction can be exploring advanced methods for these contextual challenges.
% % Besides, the direction discussed in 

% \zhc{xx} questions are still cannot be answered by any strategy in evaluation.
% The common features of these questions indicates that it is  to correctly generate the fuzz driver without a global understanding of the context requirement.
% For example, for \texttt{tmux} driver~\cite{tmux-ossfuzz-driver-link}, it requires the understanding of general concepts in \texttt{tmux} programs like the relationships between session, window, pane, etc.
% For the questions related with fuzzing network functionalities~\cite{libmodbus-ossfuzz-driver-link, civetweb-ossfuzz-driver-link}, they require the creation of a standby network server or client before calling the target API.
% Without a global understanding and planning, these contextual requirements will be ignored by the models, resulting in failed generation of effective drivers.
% More advanced strategies can be explored on solving these questions.

% Extending fuzz drivers?
% Oracles

\noindent
\textbf{Threat to Validity}
\tab
% Also discuss clone detection here?
One internal threat comes from the effectiveness validation of the generated drivers.
% There are more than 35,000 fuzz drivers have been validated based on our effectiveness criteria.
To address this, we carefully examined the APIs and manually wrote tests for them to check whether the semantic constraints of a specific API have been satisfied or not.
Another threat to validity comes from the fact that some OSS-Fuzz drivers, \textit{e.g.}, code written before Sep 2021, may already be contained in the model training data, which raises a question that whether the driver is directly memorized by the model from the training data.
Though it is infeasible to thoroughly prove its generation ability, which requires the retrain of LLMs, we found several evidences that supports the answers provided by these models are not memorized:
Many generated drivers contain APIs that do not appear in the OSS-Fuzz drivers, especially for those drivers hinted by example usage snippets or iteratively fixed by usage and error information.
Besides, the generated drivers share a distinct coding style as OSS-Fuzz drivers.
For example, the generated code are commented with explanation on why the API is used and what it is used for, etc.
The main external threat to validity comes from our evaluation datasets.
Our study focused on C projects while the insights may not be necessarily generalizable to other languages.
\compactline

% First, many drivers contain comments for the usage of each line of code, which does not exist in the manually written driver. 
% Second, not only the coding patterns between two kinds of drivers are different, but also the generated drivers can contain APIs that do not appear in the OSS-Fuzz drivers.

% \noindent
% \textbf{Pros and Cons with SOTA Methods}
% \tab
