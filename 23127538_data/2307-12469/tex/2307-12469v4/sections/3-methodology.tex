
% \begin{table}[t]
% \caption{Two Types of Basic Strategies.
% \footnotesize{"\texttt{\$\{XXX\}}" represents a variable inside prompt, which will be instantiated according to the API information.
% Specifically, \texttt{\$\{API\_NAME\}} represents the target API name, \texttt{\$\{HEADER\_FILE\}} stands for the header file the API belongs to, and \texttt{\$\{API\_DECALRATION\}} represents the declaration statement of the function found in the header.
% \textbf{K} means that the same prompt will be queried \textbf{K} times for one question (API).
% }
% }
% \label{tab:simple-strategies-detail}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lll}
% \toprule
% Strategy Type & Prompt Template & Repetition Times\\
% \midrule

% \rowcolor{black!10}
% \begin{tabular}[t]{l}
% \multirow{3}{*}{NAIVE-K} \\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \Verb|// the following function fuzzes ${API_NAME}| \\
% \Verb|extern int LLVMFuzzerTestOneInput|\texttt{(\ } \\
% \Verb| const uint8_t *Data, size_t Size|\texttt{) \{} \\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \multirow{3}{*}{K (K $\geq$ 1)} \\
% \end{tabular}
% \\
% \midrule

% \begin{tabular}[t]{l}
% \multirow{3}{*}{BACTX-K} \\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \texttt{\#}\Verb|include "${HEADER_FILE}"| \\
% \Verb|${API_DECLARATION}| \\
% \Verb|${NAIVE-K_TEMPLATE}| \\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \multirow{3}{*}{K (K $\geq$ 1)} \\
% \end{tabular}
% \\

% \bottomrule

% \end{tabular}
% }
% \end{table}


% \begin{table}[t]
% \caption{Designed Templates for DOCTX-K and UGCTX-K.
% }
% \label{tab:advanced-strategies-detail}
% \resizebox{\linewidth}{!}{
% \begin{tabular}{lll}
% \toprule
% Strategy Type & Prompt Template & Repetition Times\\
% \midrule

% \rowcolor{black!10}
% \begin{tabular}[t]{l}
% \multirow{4}{*}{DOCTX-K} \\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \texttt{\#}\Verb|include "${HEADER_FILE}"| \\
% \Verb|${API_DOCUMENTATION}| \\
% \Verb|${API_DECLARATION}| \\
% \Verb|${NAIVE-K_TEMPLATE}|\\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \multirow{4}{*}{K (K $\geq$ 1)} \\
% \end{tabular}
% \\
% \midrule

% \begin{tabular}[t]{l}
% \multirow{6}{*}{UGCTX-K} \\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \texttt{\#}\Verb|include "${HEADER_FILE}"| \\
% \Verb|// @ examples of API usage \ | \\
% \Verb|        ${EXAMPLE_FILE_NAME}| \\
% \Verb|${EXAMPLE_CODE}| \\
% \Verb|${EXAMPLE_API_DECLARATIONS}| \\
% \Verb|${NAIVE-K_TEMPLATE}|\\
% \end{tabular}
% &
% \begin{tabular}[t]{l}
% \multirow{6}{*}{K (K $\geq$ 1)} \\
% \end{tabular}
% \\

% \bottomrule

% \end{tabular}
% }
% \end{table}


% \begin{algorithm}[t]
%     \caption{Iterative Query Strategy Overview}
%     \label{alg:iterative-query}
%     \small
%     % \algsetup{linenosize=\tiny}
%     \begin{algorithmic}[1] % The number tells where the line numbering should start
%         \Require \algofont{f}, target API for generating fuzz driver
%         \Ensure \algofont{A}, generated driver, or \algofont{null} if failed to generate effective driver
%         % \Ensure \algofont{$G_{target}$} (Event Sequence Graph)
%         \Procedure{iterative-query}{\algofont{f}}
%             \State \algofont{Q} $\leftarrow$ Produce-Initial-Prompt(\algofont{f}) \Comment{Divergence Point in Search}
%             \While{True}
%                 \State \algofont{A} $\leftarrow$ Do-Query(\algofont{Q})
%                 \State \algofont{R} $\leftarrow$ Do-Validation(\algofont{Q})
%                 \If {Is-Effective-Fuzz-Driver(\algofont{R})}
%                     \Return \algofont{A}
%                 \EndIf
%                 \If {Cannot-Continue-Fix(\algofont{A}, \algofont{R})}
%                     \Return \algofont{null}
%                 \EndIf
%                 \State \algofont{Q} $\leftarrow$ Produce-Fix-Prompt(\algofont{A}, \algofont{R}) \Comment{Divergence Point in Search}
%                 \If {Exceeds Max Rounds of Iterations} \Comment{Depth of Search}
%                     \Return \algofont{null}
%                 \EndIf
%             \EndWhile
%             \EndProcedure
%     \end{algorithmic}
% \end{algorithm}

% \begin{table}[t]
% \caption{General Template For Fix Prompts.
% \footnotesize{
% * represents the content is optional depending on the error and strategy.
% }
% }
% \label{tab:fix-prompt-general-template}
% \resizebox{0.9\linewidth}{!}{
% \begin{tabular}{l}
% \toprule
% % Fix Prompt Template\\
% % \midrule

% \rowcolor{black!10}
% \begin{tabular}[t]{l}
% \Verb|```| \\
% \Verb|${FUZZ_DRIVER_CODE}| \\
% \Verb|```| \\
% \Verb|${ERR_SUMMARY_IN_ONE_SENTENCE}|\\
% \Verb|${ERR_LINE_CODE}|* \\
% \Verb|${ERR_DETAIL}|* \\
% \Verb|${SUPPLEMENTAL_INFO}|* \\
% \Verb|Based on the above information, fix the code.| \\
% \end{tabular}
% \\
% \bottomrule
% \end{tabular}
% }
% \end{table}

\section{Methodology}
\label{sec:methodology}

% This section is for: How we conduct our study to answer these RQs?
% Problem scope declaration: effective fuzz driver generation, ...

% \subsection{Overview of Study}

% \noindent
% \textbf{Overview of Study} \tab 
% Figure~\ref{fig:overview-for-study} shows the study overview.
% To understand the effectiveness of different generation strategies, we built an evaluation framework containing a set of generation questions and the effectiveness criteria.
% Each question requires the LLMs to generate a fuzz driver according to a given API and the effectiveness of the generated driver will be evaluated based on the criteria.
% % An API is qualified if its project is significant, \textit{i.e.}, tested by OSS-Fuzz, and it is the core API inside the existing fuzz drivers.
% % The effectiveness criteria of an API are the conditions or checks to distinguish ineffective drivers and effective ones. 
% % Then we built an evaluation framework to maximize the automation of the evaluation.
% % The framework will add general contents of the prompts, launch LLM queries, validate the effectiveness of replies, and classify the failed validation.
% Upon these, we studied the effectiveness of five query strategies from basic to enhanced (the first two \textbf{RQs}).
% % the first two RQs are studied by comparing and analyzing the evaluation results of different query strategies.
% In \textbf{RQ1}, we designed and explored the effectiveness of basic strategies which use fundamental API information and have simple interactions with LLMs;
% In \textbf{RQ2}, enhanced strategies which leverage extended API usage information and interactive queries are studied.
% Necessary in-depth analysis were conducted to explain the benefits and limitations of these strategies.
% Lastly, in \textbf{RQ3}, we compared LLM-generated fuzz drivers with OSS-Fuzz drivers to understand their positions, characteristics, and limitations.
% % The comparison involves the static metrics of the code such as the number of unique APIs and dynamic metrics including the testing coverage and unique crashes.

% % Figure environment removed

% \begin{table}[!t]
% \centering
% \caption{Categorization of Prompt Strategies.}
% \vspace{-5pt}
% \label{tab:categorization_prompt_strategies}
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{lll}
% \toprule
%  & Basic API Info & Extended API Info \tabularnewline
% \midrule 
% \rowcolor{black!10}
% Non-Iterative \& Repeat & NAIVE-K, BACTX-K & DOCTX-K, UGCTX-K \tabularnewline
% Iterative \& Repeat & BA-ITER-K & ALL-ITER-K \tabularnewline
% \bottomrule
% \end{tabular}
% }
% \vspace{-5pt}
% \end{table}

\subsection{Design of Prompt Strategies}

% As shown in the left-top table of the Figure~\ref{fig:prompt-strategy-design}, there are six kinds of prompt strategies.

% \noindent
% \textbf{Overview} \tab 
% introduce the figure 2
\revision{
Figure~\ref{fig:prompt-strategy-design} illustrates our designed prompt strategies.
From left to right, the figure first provides a tabular overview for all proposed strategies, then details two types of prompt templates involved, and lastly maps the templates to concrete query examples.
Note that the listed examples are simplified for demonstration purposes, while unmodified real-world examples for each strategy can be found at \cite{fuzz-drvier-study-website}.
}
\compactline

\revision{
\noindent
\textbf{Key Designs} \tab 
% the key factors behind the design and why they matters
As shown in the top-left side of Figure~\ref{fig:prompt-strategy-design}, there are three key designs for prompt strategies, including query with different types of API information, query repeatedly, and query iteratively.
Design I aims for understanding the generation effectiveness given different API information as query contexts.
The information is divided as two types: the basic API information and the extended.
The former includes fundamental information such as header file name and API declaration, which are precisely specified and generally accessible in library API fuzzing scenario, while the latter requires additional resources like API-specific documentation or usage example code snippets, whose quality and availability vary for different targets.
To account for the inherent randomness in LLM output generation, design II, repeatedly query, is introduced.
Given repetition time as value K, the entire query process of a strategy will be repeated K times ($K \ge 1$), generating K independent drivers.
The maximum value of K is set as 40 in our study.
This is an empirically value we believe is comprehensive enough to understand the effectiveness of repetition.
Design III is used to understand the effectiveness of different query workflows.
The driver generation in non-iterative strategies follows a one-and-done manner where the final driver is synthesized via a single query without further refinement.
Iterative strategies have a generate-and-fix workflow.
If the driver generated in first query fails to pass the automatic validation, subsequent fix prompts are composed based on the error feedback and queried.
The fix continues until the driver passes validation or a pre-defined maximum number of iterations is reached.
The iteration is limit as five in our evaluation.
}

\revision{
\noindent
\textbf{Acronym} \tab 
% the acronym 
Strategies are named by concatenating the abbreviations of the three key designs.
For all strategies, there is a suffix "K" indicating that the repetition times of their query process.
If a strategy name contains "ITER", it is an iterative prompt strategy.
Otherwise, it is non-iterative.
Besides, different combinations of API information used in generation prompt have different abbreviations.
As shown in the bottom-left side of the Figure~\ref{fig:prompt-strategy-design}, there are four different combinations: the \textbf{NAIVE} query context (abbr as NAIVE, \ding{172}), the \textbf{BA}sic query \textbf{C}on\textbf{T}e\textbf{X}t (abbr as BACTX, \ding{172} + \ding{173}), extending API \textbf{DOC}umentation to basic \textbf{C}on\textbf{T}e\textbf{X}t (abbr as DOCTX, \ding{172} + \ding{173} + \ding{175}), and extending example \textbf{U}sa\textbf{G}e code snippets to the basic \textbf{C}on\textbf{T}e\textbf{X}t (abbr as UGCTX, \ding{172} + \ding{173} + \ding{174}).
%A strategy acronym can be inferred by concatenating the abbreviation strings of these three designs.
Lastly, the prefix ALL in ALL-ITER-K indicates that its prompts can be the prompt designed in any other strategies.
}

% Table~\ref{tab:categorization_prompt_strategies} categorized six designed prompt strategies based on the type of information used and the query workflow.
% Strategies positioned at the top-left of the table require minimal information, exhibit lowest complexity, while those at the bottom-right display the opposite characteristics.
% Strategies that rely exclusively on basic API information interact with LLMs by providing only fundamental API details, such as the header file name and function declaration.
% Conversely, strategies that utilize extended API information include additional resources such as API documentation or example code snippets in their queries.
% The table also distinguishes between non-iterative and iterative strategies.
% The former requires LLMs generate the final fuzz driver from a single query, while the latter involves a cyclic driver repair process where the generated driver can be fixed using fix prompts.
% If the first round generated drivers failed to pass the automatic validation, subsequent prompts for corrections are composed based on the validation feedback.
% This validation-and-fix cycle continues until the driver is corrected or a pre-defined maximum number of iterations is reached.
% To account for the inherent randomness in LLM output generation, the evaluation includes the effectiveness of repetition for all strategies.
% This is denoted by the suffix 'K' in their names, implying that the entire query process is repeated K times (K $\geq$ 1).
% In this study, the maximum \texttt{K} is set as 40.

% The difference between \textbf{Non-Interactive} and \textbf{Interactive} conversations is whether the intermediate information produced during the conversation is utilized for improving the generation.
% \textbf{Interactive} coversations may be multi-round while the \textbf{Non-Interactive} ones' are one-round.

% We started from examining basic query strategies (\textbf{RQ1}), then explored the complex ones (\textbf{RQ2}).
% Lastly, we compared the generated drivers with OSS-Fuzz drivers for further understanding (\textbf{RQ3}).

% Since many factors influencing the evaluation of a strategy, we first divided the strategies as basic and enhanced strategies, and studied them from basic to enhanced.
% As shown in Figure~\ref{fig:coordinates-for-study}, the categorization is based on the type of information used in the query and the type of the conversation.
% Basic strategies only contain fundamental information of the target in the query and communicates with the models 

% established a complexity standard categorizing the strategies and examined them from basic to enhanced.
% the type and the organization of information used in the query, the number of iteration and repetition of the query, we established a complexity standard and examined the strategies from basic to extended.
% including what information will be used and the way to organize them in the prompt, the number of times a query is repeated, and the conversation is interactive or not, 
% Figure~\ref{fig:coordinates-for-study} 

% This approach is significant because simplicity offers greater generality.
% Understanding the limits of simpler strategies can inform the design and utilization of more complex ones.
% To investigate the effectiveness of LLM in generating effective fuzz drivers, we first adopt the basic strategy that employs fundamental information about the target API, such as function declaration and header file name, to construct prompts for instructing the LLM.
% We prioritize this strategy because (1) \textit{Necessity}: fundamental information is necessary to explain the target API clearly and ensure the basic quality of the query. (2) \textit{Generality}: fundamental information is widely accessible, ensuring the generality of the task.

% After analyzing the results of the basic strategy, we can identify the challenges that LLM faces in generating effective fuzz drivers.
% This understanding helps us optimize the technique of using LLM to generate fuzz drivers.
% Based on these challenges, we propose advanced strategies that improve on two dimensions, which involve more types of information and interactive times of conversation.
% These advanced strategies can address the ineffectiveness of LLM in handling some complex tasks.

% To explain the strategies more clearly, we defined both basic strategy and advanced strategy as shown in Figure~\ref{fig:coordinates-for-study}.
% Basic strategy is the one that utilizes \textbf{fundamental} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}and \textbf{Non-Interactive} conversations.
% Otherwise, it is advanced strategy.
% As defined in Figure~\ref{fig:coordinates-for-study}, a strategy is simple if it only uses \textbf{Basic} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.} and \textbf{Non-Interactive} conversations.
% Otherwise, it is complex.
% the complexity of query strategies is measured in two dimensions: \ding{182} the type of information used for prompt composition\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}, \ding{183} and the type of the conversation.
% The \textbf{fundamental} type covers widely accessible information including function declaration and the header file name an API belongs to, while \textbf{Extended} type additionally includes API usage information which cannot be assumed generally accessible such as documentation, implementation code, and usage example code.
% The difference between \textbf{Non-Interactive} and \textbf{Interactive} conversations is whether the intermediate information produced during the conversation is utilized for improving the generation.
% \textbf{Interactive} coversations may be multi-round while the \textbf{Non-Interactive} ones' are one-round.

\noindent
\textbf{NAIVE-K \& BACTX-K} \tab 
\revision{
Both two strategies only use basic API information in query and non-iterative workflow.
Their only difference is the richness of the prompt context information.
}
Specifically, NAIVE-K directly asks LLMs to implement the fuzz driver solely based on a specified function name, while BACTX-K provides a basic description of the API.
In prompts of BACTX-K, it first indicates the task scope using \texttt{\#include} statement, then provides the function declaration, and finally requests implementation.
The declaration is extracted from the Abstract Syntax Tree (AST) of the header file, including both the signature and argument variable names.
\compactline
% For instance, the declaration of \texttt{parse\_msg} in the \texttt{kamailio} project is "\texttt{extern int parse\_msg(char *const buf,const unsigned int len,struct sip\_msg *const msg);}".
% The suffix \texttt{K} in the strategy name indicates the query will be repeated \texttt{K} times until solving a given question.
% \ding{182} the LLM model the strategy used;
% This is motivated by the observation that replies of LLMs have randomness, and they can even provide quite different answers for two same queries.
% Therefore, it is necessary to include it for more comprehensive understanding of strategies' effectiveness.
% We also considered the effect of repetitiveness in the design of these strategies: the \texttt{K} in their names represents the number of times the query is repeated per API.
% \noindent
% \textbf{Experiment Setting} \tab 
% Our experiment covers all 86 questions in the quiz, two latest LLMs in Table~\ref{tab:evaluated_models}.
% For brevity, \texttt{gpt3.5} stands for \texttt{gpt3.5-turbo} in the following.
% Since the prompt for one question will be repeatedly queried, we use one round of query to refer to a whole pass of query for all questions.
% In total, four strategies are evaluated: \texttt{gpt3.5-NAIVE-K}, \texttt{gpt3.5-BACTX-K}, \texttt{gpt4-NAIVE-K}, and \texttt{gpt4-BACTX-K}, where \texttt{K} = 40.
% These strategies are evaluated on all 86 quiz questions.
% The \textbf{K} has been set as 40 for sufficient evaluation of the repetitiveness, \textit{i.e.}, each generated prompt will be repeatedly queried for 40 times.
% Besides, both two LLM models listed in Table~\ref{tab:evaluated_models}, \textit{i.e.}, \texttt{gpt-3.5-turbo} and \texttt{gpt-4}, are evaluated.

\noindent
\textbf{DOCTX-K \& UGCTX-K} \tab 
These two strategies are extended from BACTX-K by adding extended usage information in query.
Their effectiveness represents the effects of two types of extended information: API documentation and example code snippets. 
% For DOCTX-K, it additionally provides the documentation before function declaration.
% the only difference with BACTX-K is the inclusion of the target API documentation before the function declaration.
Note that, for DOCTX-K, not all APIs have associated documentation (49/86 questions in our study).
The documentation of 20 questions was automatically extracted from the header files, while the remaining 29 were manually collected from sources like project websites, repositories, and developer manuals. 
% For UGCTX-K, the example code snippet is added after the \texttt{\#include} statement, followed by the declarations of all APIs used in the example, and the rest are the same as those of BACTX-K. 
\revision{
For UGCTX-K, example code snippets of an API are collected as follows:
\ding{182} retrieving the files containing usage code via \texttt{SourceGraph} \texttt{cli}~\cite{sourcegraph-cli-tool}.
This is a keyword search among all public code repositories including Github, Gitlab, etc.
The crawling command is \texttt{src search -json "file:.*\.c lang:c count:all {API}"} where \texttt{API} should be replaced by the target API name.
% \zhc{add ref to the crawled command}
\ding{183} identifying and excluding fuzz drivers by removing the files containing function \texttt{LLVMFuzzerTestOneInput}.
\ding{184} extracting all functions directly calling the target API as example code snippets via \texttt{ANTLR} based source code analysis.
% each function inside the files that directly calls the target API is extracted as one example using \texttt{ANTLR};
\ding{185} deduplicating the snippets by if the Jaccard Similarity~\cite{jaccard-similarity} of any two snippets $\geq$95\%.
% all the collected code snippets by comparing their similarity using Jaccard Similarity~\cite{jaccard-similarity} and removing any snippets that had a similarity of over 95\%.
UGCTX-K will randomly use one snippet in the prompt.
For snippet that was too long to be included into prompt, it is truncated line by line until satisfying the token length limitation.
}

\revision{
\noindent
\textbf{BA-ITER-K \& ALL-ITER-K} \tab 
Iterative strategies have two types of prompt templates for initial generation query and subsequent fix query.
The initial generation prompt can be either of BACTX-K's, DOCTX-K's, and UGCTX-K's.
As for fix queries, we have designed seven fix templates to address seven prevalent types of errors in the generated drivers.
They follow one general fix template shown in Figure~\ref{fig:prompt-strategy-design} but are filled with error type specific details.
Due to the page limit, we discuss the key concepts of these fix prompts, leaving the detailed designs and examples in~\cite{fuzz-drvier-study-website}.
These errors are of compilation errors (1/7), linkage errors (1/7), and fuzzing runtime errors (5/7).
The error information (abbr of \texttt{\color{templateblue}[One sentence error summary]}, \texttt{\color{templateblue}[Error line code]}, and \texttt{\color{templateblue}[Error details]}) for the first two error types can be programmatically retrieved from the compiler.
According to different abnormal behaviors observed in fuzzing, fuzzing runtime errors have five subtypes, including memory leakage, out-of-memory, timeout, crash, and non-effective fuzzing (no coverage increase in one-minute short-term fuzzing).
The error information of runtime errors are retrieved by extracting the crash stacks and sanitizer summary from \texttt{libfuzzer} logs.
Lastly, for the errors that can locate its error line, we further infer its root cause API and fill API information like declaration, documentation, or usage snippets into \texttt{\color{templateblue}[Other supplemental information]} in fix prompt.
For simplicity, root cause API is identified by naively finding the last executed API based on the error line located.
}

\revision{
% Within the fix templates, \texttt{\color{templateblue}[ERR\_XXX]} placeholders represent error type-specific content, while \texttt{\color{templateblue}[[API\_XXX]]}, as seen in \texttt{VI}, are placeholders for extended information corresponding to the root cause API.
% Grammatical errors allow for a direct association of the root cause API using the compiler's error message.
% For semantic errors, the root cause API is determined by tracing back from the error line to the first API encountered.
Note that iterative strategies exclusively utilize automated checkers, ensuring that the manually crafted semantic checkers described in Section~\ref{sec:eval-framework} are only used for thorough evaluation of the effectiveness of strategies.
The principal distinction between the two iterative strategies, BA-ITER-K and ALL-ITER-K, lies in the scope of information utilized within the queries.
BA-ITER-K confines its use to only basic API details and error information, whereas ALL-ITER-K encompasses all available information.
As shown in Figure~\ref{fig:prompt-strategy-design}, this leads to multiple options for the included extended information.
The \textit{ALL-ITER-K} strategy selects options randomly.
% During our evaluation, the maximum iteration round is set as five.
}

% We developed two strategies BA-ITER-K and \texttt{EX-ITER-K} (collectively referred to as \texttt{ITER-K}).
% the iterative strategy is a search problem
% general workflow of the search
% based on the two different setting points, we propose two types of strategies: BA-ITER-K, ALL-ITER-K
% Leveraging the interactive feature of the conversation with LLMs,
% we can design strategies which iteratively improves the generated driver until it becomes effective.
% Algorithm~\ref{alg:iterative-query} shows the general workflow of the designed iterative query strategies.
% The main idea is to iteratively improve the generated driver until reaching stop conditions.
% It first generates an initial query (line 2), then comes into the loop for iterative improvement.
% where the prompt can be one of the BACTX-K, DOCTX-K, and UGCTX-K.
% If the generated driver is ineffective and the stop conditions have not been triggered, the algorithm will generate a query for asking LLM to fix the generated driver based on the driver code, error information, and other usage information we have (line 10).
% Intrinsically, it is a search problem, where the choices it made during the query generation (line 2, 10), and the search depth (line 11) together make up the search space.
% can be used for generating diverse startup drivers.
% we first detail its design then point out the divergence point.
% The intuition here is that the declaration helps in fixing grammar errors and semantic errors may be fixed by providing usage snippets.
% Since the driver even may not be grammatical valid, the root cause API is located by identifying the first API found starting from the error line of the code.
% Determining which usage information should be included in prompt is another divergence point.
% The stop conditions are either the maximum rounds of iteration has been reached or the LLM's reply cannot be fixed (contain no code or corrupted code).
% The former only uses  information about the API and error to generate queries while the latter can use any accessible information.
% \texttt{K} represents the times of iterative queries will be conducted for one question.

\vspace{-2pt}
\subsection{Evaluation Framework}
\label{sec:eval-framework}

\noindent
\textbf{Evaluation Question Collection} \tab 
% What is a question for fuzz driver generation
% This is based on the intuition that any fuzz driver can be divided into one or more simpler fuzz drivers targeting different APIs.
% In other words, complicate, multi-purpose fuzz drivers are essentially the combination code which fuzzes multiple API targets simultaneously.
% Therefore, as the first practical evaluation on LLM-based fuzz driver generation, we focused on the more fundamental scenario.
% why selecting an appropriate question is not naive
One question used in evaluation is simply designed as generating fuzz drivers for one given API.
However, not all APIs are suitable to be set as questions.
Naively collecting all APIs from projects will lead to the creation of meaningless or confusing questions which influences the evaluation result.
Specifically, some APIs, such as \texttt{void libxxx\_init(void)}, are meaningless fuzz targets since the code executed behind the APIs can not be affected by any input data.
Some APIs can only be supplemental APIs rather than the main fuzzing target due to the nature of their functionalities.
For example, given two APIs \texttt{object *parse\_from\_str(char *input)} and \texttt{void free\_object(object *obj)}, feeding the mutated data into \texttt{input} is apparently a better choice than feeding a meaningless pointer to the \texttt{obj} argument.
However, calling the latter API when fuzzing the former is meaningful since 
\ding{182} it may uncover the hidden bug when the former does not correctly initialize the \texttt{object *} by freeing the members of \texttt{object};
\ding{183} it releases the resources allocated in this iteration of fuzzing, which prevents the falsely reported memory leak issue.

% how did we select qualified APIs
To guarantee the collected APIs are qualified and representative, we collected the core APIs of existing fuzz drivers from OSS-Fuzz projects.
% To build a high quality quiz, we need to collect a set of qualified APIs which are both representative and suitable to be set as fuzzing targets.
% Our intuition is to collect core APIs of the existing fuzz drivers from popular projects fuzzed by OSS-Fuzz.
A driver's core APIs are identified using the following criteria:
\ding{182} they are the target APIs explicitly pointed out by the author in its driver file name or the code comments, \textit{e.g.}, \texttt{dns\_name\_fromwire} is the core API of driver \textit{dns\_name\_fromwire.c};
\ding{183} otherwise, we pick the basic APIs as the core rather than the supplemental ones.
For example, we picked the former between parse and use/free APIs.
For the fuzz drivers which are composite drivers fuzzing multiple APIs simultaneously, we identified multiple core APIs from them.
Specifically, we randomly selected 30 projects from OSS-Fuzz (commit \texttt{135b000926}) C projects, manually extracted 86 core APIs from 51 fuzz drivers.
Full list of questions are post at~\cite{fuzz-drvier-study-website}.
% More details are listed in \appe~\ref{sec:quiz-questions-detail}.

% Figure environment removed


\revision{
\noindent
\textbf{Effectiveness Validation Criteria} \tab 
% What is an effective fuzz driver
% general validation method is hard
% can greatly influence the evaluation result
% our approach
% -> semi-automatic criteria: automatic + manually built checkers
% An effective fuzz driver represents the drivers which have correct API usage and produce no false positives.
% Precisely validating the effectiveness of fuzz drivers is crucial for evaluating fuzz driver generation methods.
% However, it is hard to propose a general validation technique since the effectiveness
% general validation techniques do not work well due to the diverse semantics on the API usages.
% An effective fuzz driver should substantially test the target API while producing no false positives.
Assessing the effectiveness of a generated fuzz driver is complex since identifying both false positives (bugs caused by the driver code) and negatives (can never find bugs given its incorrect usage) rely on the understanding of API usage semantics.
Figure~\ref{fig:validation-checker} is a streamlined four-step semi-automatic validation process:
\ding{182} Use a compiler to check for grammatical errors in the driver code.
\ding{183} Observe the driver in a one-minute fuzzing session starting with no initial seed.
It is ineffective if it either fails to show coverage progress or reports any bugs.
The assumption behind is that, given a poor fuzzing setup, neither the zero coverage progress nor the quick identification of bugs for a well-tested API are normal behaviours.
Considering that this criteria can still lead to incorrect validation, two additional steps are introduced for result refinements.
\ding{184} If the driver reports bugs in \ding{183}, we filter the true bugs contained inside.
This is done by first collecting target true bugs via ten cpu-day fuzzing using existing OSS-Fuzz drivers, then manually building filters based on the root causes of these bugs.
\ding{185} For drivers reporting no bugs after \ding{184}, we check whether they are substantially testing the target API or not.
To this end, we write API-specific semantic tests to detect common ineffective patterns observed in LLM-generated fuzz drivers.
The tests include verifying the target API is called for all 86 questions, checking the correct usage of fuzzing data to key arguments for 8 questions (such as fuzzing file contents instead of file names), ensuring critical dependent APIs are invoked in 16 questions, and confirming necessary execution contexts are prepared for 5 questions (for instance, having a standby server process available for testing client APIs).
We implement these tests by injecting hooking code into the driver.
For more details on these tests, please refer to our website~\cite{fuzz-drvier-study-website}.
}
\compactline
% \zhc{check here}
% \zhc{explain 29, 33\%?}

%The first two are automatic and general checks about whether a short-term fuzzing can be conducted successfully given the generated driver.
%If the compilation failed or the short-term fuzzing (one minute fuzzing with empty initial seed) reported any bug or had no coverage progress, it is an ineffective driver.
%% It checks the compilation result the  the driver reports any bugs, \textit{i.e.}, crashes or timeout, or does not have any coverage progress in a short time period with a default fuzzing setup (empty seed, no dictionary, etc).
%% checks require manual configuration for each project while the rest two need to be configured per API, \textit{a.k.a.} per question in our quiz.
%% The first check examines the grammatical correctness of a driver using compiler, while the second one checks the existence of abnormal fuzzing behaviours via short-term fuzzing.
%The intuition here is that, under a default setup, neither the zero coverage progress nor the quick identification of bugs are normal behaviours.
%% In our study, we set the time period as one minute.
%In general, these automatic checks are rough measurements which can cause false validations.
%To refine the validation, two additional checkers are introduced.
%By fuzzing the OSS-Fuzz provided drivers, we collected the signatures of real bugs that can be found quickly.
%We filtered these bugs in the third checker.
%Lastly, by manually examining API usages, we summarized the semantic constraints of APIs and wrote tests checking these generated drivers.
%For example, assuming an API requires passing mutated input via file, the semantic test is hooking the API and checking whether the passed filename points to a existing file carrying the mutated input.
%In total, semantic checkers are built for 34\% (29 out of 86) questions.
%% summarized semantic constraints of a correct driver per API.
%% These constraints are written as tests to the fuzz drivers.
%% Our framework also collected and taxonomized all unpassed checks (Section~\ref{subsec:evaluation-framework}), helping us to iteratively improve the inexact checks.
%% See ~\cite{fuzz-drvier-study-website} for more details.
%% mostly the validation does not require introducing semantic checkers, their detailed contribution statistics can be found at ~\cite{fuzz-drvier-study-website}.
%% showed a detailed statistics data for the contribution of these checkers.
%\zhc{add explanation \& example cases}

% \subsection{Evaluation Framework}
% \label{subsec:evaluation-framework}

% % what the evaluation framework is and how to use
% \noindent
% \textbf{Evaluation Framework} \tab 
% We built a framework for automating the driver evaluation in scale.
% In total, the framework is written in 9,342/1,542/3,857 lines of Python/HTML/JSON, YAML, and Bash scripts.
% It takes a prompt produced by a query strategy as input, and outputs the taxonomized validation result.
% To focus on evaluating the generated driver code, the compilation environment is prepared in advance and each driver is validated in a fresh, isolated container.
% The validated results are firstly taxonomized according to the string patterns of checkers' results~\texttt{clang}~\cite{clang-lex-diagnostics, clang-parse-diagnostics, clang-sema-diagnostics} and \texttt{libfuzzer}~\cite{libfuzzer, sanitizers}.
% Then the result is refined by manual review and correction.

% Based on the quiz and criteria, 
% To boost the evaluation of a large amount of fuzz drivers, we developed a framework to maximize the automation.
% As shown in Figure~\ref{fig:overview-for-study}, the framework takes a prompt produced by a query strategy as input, and outputs the taxonomized validation result.
% Figure~\ref{fig:overview-for-study} shows the framework workflow.
% It also provides a website to ease the manual analysis.
% Considering the non-deterministic nature of LLM, we summarized patterns for extracting code inside answer when it replies both text and code.
% \textbf{Effectiveness Validation} \tab 
% Due to the non-deterministic nature of the LLM, some replies still can contain both code and text.
% We summarized the patterns and extracted the code in replies.
% Besides, to focus on evaluating the effectiveness of the generated code, our framework automatically configures the required header files and build options for a driver.
% This is done by manually configuring the rules of header inclusion, and build options for the selected 30 projects, which guarantees all compilation and link errors are caused by the incorrect code rather than the unsuitable configurations.
% Besides, each validation of a fuzz driver is in a fresh, isolated container, excluding the environment disturbances to the results.
% \noindent
% \textbf{Failure Taxonomy} \tab 
% automatic taxonomy
% semi-automatically taxonomy
% The taxonomy of root causes for ineffective results also follows a semi-automatic approach.
% We first did a rough categorization of the compilation, link, and fuzz errors based on the string patterns of the errors outputted by \texttt{clang}~\cite{clang-lex-diagnostics, clang-parse-diagnostics, clang-sema-diagnostics} and \texttt{libfuzzer}~\cite{libfuzzer, sanitizers}.
% Then for each API, we manually identified the root causes per category and mapped each category into the final categories. 
% The manual mappings were written in code and will be iteratively improved once we found a new unclassified failure. 

% \begin{table}[!t]
% \centering
% \caption{Main Evaluated LLMs.}
% \label{tab:evaluated_models}
% \resizebox{0.8\linewidth}{!}{
% \begin{tabular}{llll}
% \toprule
%  Model & Abbr & Max Tokens & Training Data \tabularnewline
% \midrule 
% \rowcolor{black!10}
% gpt-3.5-turbo-0301 & \texttt{gpt3.5} & 4,096 & Up to Sep 2021 \tabularnewline
% gpt-4-0314 & \texttt{gpt4} & 8,192 & Up to Sep 2021 \tabularnewline
% \bottomrule
% \end{tabular}
% }
% % \vspace{-10pt}
% \end{table}

\noindent
\textbf{Evaluation Configuration} \tab 
In this paper, a configuration represents a specific combination of the three factors: the LLM used, the prompt strategy employed, and the selected temperature setting, abbr as <model, prompt strategy, temperature>.
As shown in Table~\ref{tab:overall_eva_rslt}, we evaluated six prompt strategies on five LLMs with five different temperatures.
A configuration's top\_p is set as its model's default value.
And the system role~\cite{system-role-usage} is set as "\sethlcolor{templategrey}\hl{\texttt{You are a security auditor who writes fuzz drivers for library APIs.}}".
% The LLMs include gpt-4-0613~\cite{xxx}, gpt3.5-turbo-0613~\cite{xxx}, wizardcoder-15b-v1.0~\cite{xxx}, text-bison-001~\cite{xxx}, and codellama-34b-instruct~\cite{xxx}.
% The parameters of the evaluated models are the default values in their websites~\cite{chatgpt-default-config}, \textit{e.g.}, temperature is set as 0.9 and top\_p is 1.
% All the LLM-generated fuzz drivers evaluated in this study are retrieved via ChatGPT web interfaces~\cite{chatgpt-website} (release version 23 Mar 2023) based on \texttt{chatgpt-wrapper}\cite{chatgpt-wrapper} v0.7.1 (\texttt{dabe72101b}).
% All the LLM-generated fuzz drivers evaluated in this study are retrieved via \texttt{chatgpt-wrapper}~\cite{chatgpt-wrapper} v0.7.1 (\texttt{dabe72101b}).
% \noindent
% \textbf{Prompt Standardization} \tab 
% controlling the output
% extract code part only
% preparing headers
% What are already assumed to be correctly provided, and how we postprocessing the answers

% \noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
% \begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
% \small\texttt{You are a security auditor who writes fuzz drivers for library APIs.}
% \end{tcolorbox}
% \noindent
% \textbf{LLM Query} \tab 
% token limitation, how many left for query and how many for answer
% perhaps discuss this in iteration section is better
% one query one conversation
% restful deisgn?
% LLMs limit the maximum token numbers for the sum of tokens in query and answer.
% We set 6,000 tokens for the prompt of \texttt{gpt4}, and 3,600 tokens for \texttt{gpt3.5}.

