\section{Fundamental Challenge (\textbf{RQ2})}

% % Figure environment removed

% \noindent
% \textbf{Boundaries of Effectiveness} \tab 

% \subsection{Boundaries of Effectiveness}
\subsection{Links Between Question and Performance}

% Figure environment removed

% The difference among LLM replies for the same query is the power source of the effectiveness for repetitive queries.
% Specifically, a second time answer from the LLM may add or remove certain condition checks, change the macro used in the option argument of an API, and even switch the entire API usage comparing with previous answer.
% Our main observation is that LLMs' performance degrades as they are required to consider more API specific usages when writing a driver.
% Before delving into a detailed discussion, we will first outline the minimum API usages required for writing a fuzz driver.
% Then we examine the challenges that LLMs may encounter when attempting to correctly synthesize these usages.
% Finally, evidence are presented to support our findings.
% We first hypothesized that LLMs' performance is related to complexity of API usage. 
% Then we examined the
% To test the hypothesis, we propose the evaluation steps as follows: first, we identify the minimum API usage required for fuzz driver creation; second, we examine the potential difficulties LLMs may encounter when accurately synthesizing these usages; and finally, we design a score system to quantify the degree of the minimum API specific usage, and associate it with the performance of fuzz driver generation.

To investigate the core difficulties in generating fuzz drivers with LLMs, we scrutinized the outcomes of the BACTX-K strategy.
\revision{
This strategy is a proper starting point for understanding the fundamental challenges since it merely uses generally accessible information and has simple query workflow.
}
% This strategy has 
% The analysis results of this strategy are suitable to represent the fundamental challenge since it only utilizes basic information and conducts basic query workflow.
% Comparing with other strategies which may include target specific examples, \texttt{BACTX-K} only utilizes basic information and conducts basic query workflow.
% Besides, nearly all strategies are built upon \texttt{BACTX-K} except \texttt{NAIVE-K}.
% Therefore, understanding it can help better understand the characteristics of other advanced strategies.
In Figure~\ref{fig:rq2-query-succ-rates-per-score}, \textbf{there is a clear inverse proportion relationship between the query success rate and the complexity of a question, irrespective of the used models and temperatures}.
The complexity of a question is measured by first constructing the minimal fuzz driver of each question and then quantifying the API specific usage contained in the minimized code.
A minimal effective driver for a question is created based on the OSS-Fuzz driver by removing the unnecessary part of the code and replacing the argument initialization into a simpler solution according to the cases enumerated in Section~\ref{sec:preliminaries}.
Then the complexity is quantified as the sum of the count of the following elements inside code:
\ding{182} unique project APIs;
\ding{183} unique common API usage patterns;
\ding{184} unique identifiers including non-zero literals and project global variables excluding the common API usage code;
\ding{185} branches and loops excluding the common API usage code.
Note that all branches of one condition will be counted as one.
Overall, \ding{182}, \ding{184} measure API specific vocabularies while \ding{185} for API specific control flow dependencies'.
We put detailed calculation examples at \cite{fuzz-drvier-study-website}.
\compactline

% as follows:
% \ding{182} using the OSS-Fuzz driver as initial one;
% \ding{183} removing the unnecessary code.
% A piece of code is unnecessary if its removal does not affect the driver's effectiveness;
% \ding{184} simplifying the remaining code by replacing the argument initialization to a simpler form.
% The level of the simplicity is based on the cases classified in Section~\ref{sec:preliminaries}.
% We classified the initialization to the four cases mentioned in Section~\ref{sec:preliminaries},
% and the ascending order on simplicity for these cases are: \ding{172}, \ding{173}, \ding{174}, and \ding{175}.
% Figure~\ref{fig:qstn-succ-rate-per-score}/~\ref{fig:query-succ-rate-per-score} presents the success rate of questions/queries in given score buckets.
% Note that each question contains 40 repeated queries in our experiments.
% There is a stronger relationship in query success rate.
% Note that the query success rate shows a stronger relationship.
% This is reasonable since:
% \ding{182} query success rate is a more direct and fine-grained metric reflecting question difficulty;
% \ding{183} the query success rate is calculated based on 40 repeated queries per question, providing more data and leading to less fluctuation in the overall success rate calculation.
% Specifically, when score $\geq$ 7, all query success rates $<$ 10\%, and the rate drops to zero when score $\in [13, \infty]$.


% Our observation is that \textbf{LLMs' performance degrades when they need to consider more API specific usages during driver generation.}

% Although LLMs may employ different mechanisms for generating code compared to the traditional workflow of driver composition, they still must meet certain minimum requirements.
Considering the generation process, it is intuitive that \textbf{LLMs' performance degrades when the complexity of target API specific usage increases}.
To generate effective drivers, LLMs should at least generate code satisfying minimal requirements.
In other words, they must accurately predict the API argument usage and control flow dependencies.
However, this is challenging since LLMs cannot validate their predictions against documentation or implementations as humans do.
It is reasonable to assume that LLMs have learned the language basics and common programming practices due to their training on vast amounts of code.
But the API specific usage, such as the semantic constraints on the argument, cannot be assumed.
On one hand, there may only have limited data about this in training.
On the other hand, details can be lost during preprocessing or the learning stage while the accurate generation is required.
% , such as its definitions, implementations, documentation, and usage examples, comprises only a negligible proportion of the training data.
% Furthermore, details may have been lost during preprocessing or the learning stage.
Therefore, the more API usage a LLM needs to predict, the greater the likelihood of errors, particularly for less common usages that do not follow the mainstream design patterns or have special semantic constraints.
Such situations are common in C projects, whose APIs often contain low-level project-specific details.

% Figure~\ref{fig:rq2-query-succ-rates-per-score} shows the relationship between query success rate and score.
% The plot only considers the data of the best temperature of \texttt{BACTX-K} since this is an ideal case for understanding 
% \textbf{The plot reveals a clear inverse proportion relationship between success rate and score, irrespective of the used models.}

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
% \textbf{Key Understanding:}
The performance of LLM-based generation declines significantly when the complexity of API specific usage increases.
% Our score can be used to quantify this association.
\end{tcolorbox}

% Figure environment removed


\subsection{Failure Analysis}
\label{sec:failure-analysis}

%	analyze failures on hard to solve/unsolved questions
% 
%	-> have common blockers/obstacles 
% 
%	-> analyze/classify the root cause of the blockers
%
%	1. nonintuitive API usage
% 
%	2. indirectly dependent API usage
%
%        case studies for repetitive queries, do we need that?

% \noindent
% \textbf{Overview} \tab 
% what failure analysis can bring
% Although we have established understanding on why, it is still unclear of the detailed reasons making the generation fail.
To understand how the generation fails on API specifics, we conducted failure analysis on \texttt{BACTX-K}.
The direct failure reason of the driver is collected to reveal the current main blockers for generation.
Specifically, the runtime errors, involving 11,095 drivers, are semi-automatically analyzed while the compilation and link errors are categorized based on the compiler outputs.
In total, 52,824 ineffective drivers were analyzed.
% the direct reason of the error instead of all potential root causes.
% It is technically challenging to exhaustively count root causes as one error can be fixed in multiple ways, resulting in numerous enumeration list.
% there can exist multiple effective drivers for a given question, resulting in numerous enumeration lists.
% Instead, focusing on the direct root cause helps reveal the current main blockers for generation.
% follows the semi-automatic approach mentioned in Section~\ref{subsec:evaluation-framework}. 
% \zhc{fix the ungrouped 276!}

\noindent
\textbf{General Taxonomies} \tab 
% results of failure analysis
Figure~\ref{fig:rq2-failure-taxonomy} details the root cause taxonomy.
There are nine root causes fallen into two categories:
the grammatical errors reported by compilers in build stage, and the semantic errors which are abnormal runtime behaviors identified from the short-term fuzzing results.
\ding{182} \textit{G1 - Corrupted Code}, the drivers do not contain a complete function of code due to either the token limitation or mismatched brackets;
\ding{183} \textit{G2 - Language Basics Violation}, the code violates the language basics like variable redefinition, parentheses mismatch, incomplete expressions, etc;
\ding{184} \textit{G3 - Non-Existing Identifier}, the code refers to non-existing things such as header files, macros, global variables, members of a \texttt{struct}, etc;
\ding{185} \textit{G4 - Type Error}. One main subcategory here is the code passes mismatched number of arguments to a function.
The rest are either unsupported type conversions or operations such as calling non-callable object, assigning \texttt{void} to a variable, allocating an incomplete \texttt{struct}, etc;
\ding{186} \textit{S1 - Incorrect Input Arrangement}, the input size check either is missed when required or contains an incorrect condition;
\ding{187} \textit{S2 - Misinitialized Function Args}, the value or inner status of initialized argument does not fit the requirements of callee function.
Typical cases are closing a file handle before passing it, using wrong enumeration value as option parameter, missing required APIs for proper initialization, etc;
\ding{188} \textit{S3 - Inexact Ctrl-Flow Deps}, the control-flow dependencies of a function does not properly implemented.
Typical cases are missing condition checks such as ensuring a pointer is not \texttt{NULL}, missing APIs for setting up execution context, missing APIs for ignoring project internal abort, using incorrect conditions, etc.
\ding{189} \textit{S4 - Improper Resource Cleaning}, the cleaning API such as \texttt{xxxfree} is either missing when required or is used without proper condition checks; 
\ding{190} \textit{S5 - Failure on Common Practices}, the code failed on standard libraries function usage like messing up memory boundary in \texttt{memcpy}, passing read-only buffer to \texttt{mkstemp}, etc.
Examples of these categories are shown in ~\cite{fuzz-drvier-study-website}.

%\noindent
%\textbf{Failure Distributions} \tab 
%Figure~\ref{fig:rq1-failure-taxonomy}b, ~\ref{fig:rq1-failure-taxonomy}c show the root cause distribution.
%The plots only distinguish prompt templates since the distributions among models for a given template do not have significant difference.
%Therefore, they are omitted here for brevity.
%% , a more detailed version can be found at \zhc{add ref to more detailed version}.
%Overall, the proportion of the grammatical and semantic errors are quite different in \texttt{NAIVE-K} and \texttt{BACTX-K}.
%More than 90\% of failures in \texttt{NAIVE-K} is of grammatical error while only 54\% in \texttt{BACTX-K}.
%This indicates that most drivers generated by \texttt{NAIVE-K} failed at early stage.
%In \texttt{BACTX-K}, \texttt{G3}, \texttt{G4}, \texttt{S2}, and \texttt{S3} are top four types of root causes and together they take up 93\% of all.
%From the prospective of fix, all these four types indicate that there is a need for providing more API specific knowledge to the models.
%% , including both grammatical and semantic usages.

% For instance, for <gpt-4-0613, 0.5, BACTX-K>, the percentage of its grammatical error is less than 50\%.
% , is automatically retrievable via code analysis approaches. 
% it corresponds to information required for the fix, \textit{e.g.}, function signatures and type declaration, is relatively easy to be automatically retrieved.
Note that the percentage shown in the taxonomy represents the union results of all BACTX-K configurations.
For each configuration, its distribution breakdown may be different.
Generally, both grammatical criteria and the API semantics are common mistakes made by these LLMs.
For grammatical mistakes, many of them root in misuse of indirect usages of the target API, such as passing mismatched number of arguments to a dependent API or referencing a non-existing \texttt{struct} member.
Mostly, it is easy to identify and correct these failures since the symptoms directly map to the root causes, \textit{e.g.}, messing up the function or type declarations.
As for semantic errors, these failures are challenging for locating root causes and correction usages due to the challenges on handling program semantics.
However, reporting the existence of an error with a relevant usage description may still be helpful.
% Few cases (4\% for \texttt{G4}, \texttt{S5}) fail on coding basics or practices while a negligible part (1\% for \texttt{G1}) are of incomplete code.
% For semantic errors, 21\% cases are of \texttt{S1-4}, indicating that \textbf{nearly half of the cases failed to satisfy the semantic constraints on API usage}.
% Similarly, the errors appear on most APIs.

Overall, the failures cover API usages in various dimensions: from grammatical level detail to semantic level direction, and from target API control flow conditions to dependent APIs' declarations.
Improving this is challengeable since:
\ding{182} \textbf{the involved usage is too broad to be fully put into one prompt}, which may either exceed the token limitation or distract the model;
\ding{183} \textbf{the useful usage for generating one driver cannot be fully predetermined}.
On one hand, models are inherently blackbox and probabilistics, whose mistakes cannot be fully predicted.
On the other hand, there are usually multiple implementation choices for a given API.

% For example, models are inherently blackbox and probabilistic, whose mistakes cannot be fully predicted.
% Besides, for a given API, models usually have multiple feasible choices during implementation.

% 27\% of the \texttt{BACTX-K} cases failed to figure out the correct way to initialize the arguments of API declared in the project.

% Besides, 16\% of failures are caused by missing contextual or control flow dependencies (\texttt{S2} and \texttt{S3}).
% This type of failure is 

% \textbf{xx\% failures are indirectly related with the target API.}
% While more than 90\% of the failures can be reasoned as the lack of detailed usage information, \zhc{xx} of them are indirectly dependent by the target API.

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
% \textbf{Key Challenges:}
\revision{
Most failures are of mistakes in API usage specifics.
The broadness of the involved usage is the major challenge.
}
% They are too broad to be generally handled.
% from target API to its dependents, and from grammatical details to semantic constraints.
% The broadness of the involved usages raises challenge.
% for further improvement.
% including not only usages of the API itself but also its dependencies.
% One major type of error is grammatical, which can be easily identified and corrected, while another is of semantic error that are harder to locate and fix automatically.
\end{tcolorbox}