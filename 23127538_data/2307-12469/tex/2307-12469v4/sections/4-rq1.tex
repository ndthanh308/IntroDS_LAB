\section{Overall Effectiveness (\textbf{RQ1})}

% % Figure environment removed

 % \multirow{2}{*}{Index} & \multirow{2}{*}{Question} & \multirow{2}{*}{Score} &   \multicolumn{6}{c}{GPT4} &  \multicolumn{6}{c}{GPT3.5} \\

\begin{table}[!t]
\centering
\caption{Overall Evaluation Result.
\footnotesize{K represents 40, "-" means failed to retrieve full query results or not applicable for the given model.}
}
\vspace{-10pt}
\label{tab:overall_eva_rslt}
\resizebox{1.0\linewidth}{!}{
\setlength\arrayrulewidth{0.1pt}
\begin{tabular}{cl!{\color{white}\vrule}c!{\color{white}\vrule}c!{\color{white}\vrule}c!{\color{white}\vrule}c!{\color{white}\vrule}c}
& & \multicolumn{5}{c}{Temperature} \tabularnewline
\multicolumn{2}{c}{Strategy, Model} & \cellcolor{black!20}\texttt{0.0} & \cellcolor{black!20}\texttt{0.5} & \cellcolor{black!20}\texttt{1.0} & \cellcolor{black!20}\texttt{1.5} & \cellcolor{black!20}\texttt{2.0} \tabularnewline
\arrayrulecolor{white}\hline\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-4-0613}& \cellcolor{green!10}{\large 9}/{\footnotesize 86}& \cellcolor{green!10}{\large 9}/{\footnotesize 86}& \cellcolor{green!10}{\large 9}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-3.5-turbo-0613}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize wizardcoder-15b-v1.0}& \cellcolor{green!10}{\large 3}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize text-bison-001}& \cellcolor{green!10}{\large 2}/{\footnotesize 86}& \cellcolor{green!10}{\large 2}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& {\large -}{\tiny -}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\multirow{-5}{*}{\cellcolor{black!0} \rotatebox[origin=c]{90}{{NAIVE-1}}}& \cellcolor{black!20} {\normalsize codellama-34b-instruct}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\arrayrulecolor{white}\hline\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-4-0613}& \cellcolor{green!20}{\large 12}/{\footnotesize 86}& \cellcolor{green!40}{\large 30}/{\footnotesize 86}& \cellcolor{green!40}{\large 30}/{\footnotesize 86}& \cellcolor{green!10}{\large 5}/{\footnotesize 86}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-3.5-turbo-0613}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!10}{\large 6}/{\footnotesize 86}& \cellcolor{green!10}{\large 8}/{\footnotesize 86}& \cellcolor{green!10}{\large 8}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize wizardcoder-15b-v1.0}& \cellcolor{green!10}{\large 3}/{\footnotesize 86}& \cellcolor{green!10}{\large 8}/{\footnotesize 86}& \cellcolor{green!20}{\large 11}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize text-bison-001}& \cellcolor{green!10}{\large 2}/{\footnotesize 86}& \cellcolor{green!10}{\large 5}/{\footnotesize 86}& \cellcolor{green!10}{\large 5}/{\footnotesize 86}& {\large -}{\tiny -}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\multirow{-5}{*}{\cellcolor{black!0} \rotatebox[origin=c]{90}{{NAIVE-K}}}& \cellcolor{black!20} {\normalsize codellama-34b-instruct}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!10}{\large 3}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\arrayrulecolor{white}\hline\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-4-0613}& \cellcolor{green!40}{\large 29}/{\footnotesize 86}& \cellcolor{green!50}{\large 41}/{\footnotesize 86}& \cellcolor{green!50}{\large 41}/{\footnotesize 86}& \cellcolor{green!30}{\large 21}/{\footnotesize 86}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-3.5-turbo-0613}& \cellcolor{green!20}{\large 12}/{\footnotesize 86}& \cellcolor{green!40}{\large 29}/{\footnotesize 86}& \cellcolor{green!40}{\large 30}/{\footnotesize 86}& \cellcolor{green!30}{\large 24}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize wizardcoder-15b-v1.0}& \cellcolor{green!10}{\large 7}/{\footnotesize 86}& \cellcolor{green!30}{\large 23}/{\footnotesize 86}& \cellcolor{green!30}{\large 25}/{\footnotesize 86}& \cellcolor{green!20}{\large 17}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize text-bison-001}& \cellcolor{green!10}{\large 7}/{\footnotesize 86}& \cellcolor{green!20}{\large 13}/{\footnotesize 86}& \cellcolor{green!20}{\large 15}/{\footnotesize 86}& {\large -}{\tiny -}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\multirow{-5}{*}{\cellcolor{black!0} \rotatebox[origin=c]{90}{{BACTX-K}}}& \cellcolor{black!20} {\normalsize codellama-34b-instruct}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!20}{\large 11}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\arrayrulecolor{white}\hline\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-4-0613}& \cellcolor{green!40}{\large 29}/{\footnotesize 86}& \cellcolor{green!50}{\large 40}/{\footnotesize 86}& \cellcolor{green!50}{\large 41}/{\footnotesize 86}& \cellcolor{green!30}{\large 22}/{\footnotesize 86}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-3.5-turbo-0613}& \cellcolor{green!20}{\large 11}/{\footnotesize 86}& \cellcolor{green!30}{\large 22}/{\footnotesize 86}& \cellcolor{green!40}{\large 29}/{\footnotesize 86}& \cellcolor{green!30}{\large 24}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize wizardcoder-15b-v1.0}& \cellcolor{green!10}{\large 7}/{\footnotesize 86}& \cellcolor{green!30}{\large 24}/{\footnotesize 86}& \cellcolor{green!30}{\large 25}/{\footnotesize 86}& \cellcolor{green!20}{\large 12}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize text-bison-001}& \cellcolor{green!10}{\large 9}/{\footnotesize 86}& \cellcolor{green!20}{\large 14}/{\footnotesize 86}& \cellcolor{green!20}{\large 14}/{\footnotesize 86}& {\large -}{\tiny -}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\multirow{-5}{*}{\cellcolor{black!0} \rotatebox[origin=c]{90}{{DOCTX-K}}}& \cellcolor{black!20} {\normalsize codellama-34b-instruct}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!10}{\large 9}/{\footnotesize 86}& \cellcolor{green!20}{\large 13}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\arrayrulecolor{white}\hline\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-4-0613}& \cellcolor{green!70}{\large 55}/{\footnotesize 86}& \cellcolor{green!80}{\large 63}/{\footnotesize 86}& \cellcolor{green!80}{\large 62}/{\footnotesize 86}& \cellcolor{green!30}{\large 26}/{\footnotesize 86}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-3.5-turbo-0613}& \cellcolor{green!40}{\large 30}/{\footnotesize 86}& \cellcolor{green!60}{\large 47}/{\footnotesize 86}& \cellcolor{green!50}{\large 43}/{\footnotesize 86}& \cellcolor{green!40}{\large 31}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize wizardcoder-15b-v1.0}& \cellcolor{green!50}{\large 39}/{\footnotesize 86}& \cellcolor{green!60}{\large 50}/{\footnotesize 86}& \cellcolor{green!60}{\large 48}/{\footnotesize 86}& \cellcolor{green!20}{\large 13}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize text-bison-001}& \cellcolor{green!30}{\large 21}/{\footnotesize 86}& \cellcolor{green!40}{\large 27}/{\footnotesize 86}& \cellcolor{green!50}{\large 38}/{\footnotesize 86}& {\large -}{\tiny -}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\multirow{-5}{*}{\cellcolor{black!0} \rotatebox[origin=c]{90}{{UGCTX-K}}}& \cellcolor{black!20} {\normalsize codellama-34b-instruct}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!10}{\large 8}/{\footnotesize 86}& \cellcolor{green!30}{\large 21}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\arrayrulecolor{white}\hline\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-4-0613}& \cellcolor{green!70}{\large 56}/{\footnotesize 86}& \cellcolor{green!70}{\large 57}/{\footnotesize 86}& \cellcolor{green!80}{\large 62}/{\footnotesize 86}& \cellcolor{green!30}{\large 23}/{\footnotesize 86}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-3.5-turbo-0613}& \cellcolor{green!40}{\large 32}/{\footnotesize 86}& \cellcolor{green!60}{\large 47}/{\footnotesize 86}& \cellcolor{green!50}{\large 43}/{\footnotesize 86}& \cellcolor{green!40}{\large 28}/{\footnotesize 86}& \cellcolor{green!10}{\large 2}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize wizardcoder-15b-v1.0}& \cellcolor{green!10}{\large 8}/{\footnotesize 86}& \cellcolor{green!30}{\large 24}/{\footnotesize 86}& \cellcolor{green!50}{\large 37}/{\footnotesize 86}& \cellcolor{green!20}{\large 13}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize text-bison-001}& \cellcolor{green!10}{\large 9}/{\footnotesize 86}& \cellcolor{green!20}{\large 15}/{\footnotesize 86}& \cellcolor{green!30}{\large 20}/{\footnotesize 86}& {\large -}{\tiny -}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\multirow{-5}{*}{\cellcolor{black!0} \rotatebox[origin=c]{90}{{BA-ITER-K}}}& \cellcolor{black!20} {\normalsize codellama-34b-instruct}& \cellcolor{green!10}{\large 6}/{\footnotesize 86}& \cellcolor{green!40}{\large 28}/{\footnotesize 86}& \cellcolor{green!30}{\large 22}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\arrayrulecolor{white}\hline\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-4-0613}& \cellcolor{green!90}{\large 77}/{\footnotesize 86}& \cellcolor{green!90}{\large 78}/{\footnotesize 86}& \cellcolor{green!90}{\large 76}/{\footnotesize 86}& \cellcolor{green!30}{\large 25}/{\footnotesize 86}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize gpt-3.5-turbo-0613}& \cellcolor{green!80}{\large 65}/{\footnotesize 86}& \cellcolor{green!80}{\large 68}/{\footnotesize 86}& \cellcolor{green!80}{\large 65}/{\footnotesize 86}& \cellcolor{green!50}{\large 37}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize wizardcoder-15b-v1.0}& \cellcolor{green!50}{\large 41}/{\footnotesize 86}& \cellcolor{green!60}{\large 48}/{\footnotesize 86}& \cellcolor{green!70}{\large 53}/{\footnotesize 86}& \cellcolor{green!20}{\large 11}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\cellcolor{black!0}& \cellcolor{black!20} {\normalsize text-bison-001}& \cellcolor{green!30}{\large 21}/{\footnotesize 86}& \cellcolor{green!50}{\large 37}/{\footnotesize 86}& \cellcolor{green!50}{\large 42}/{\footnotesize 86}& {\large -}{\tiny -}& {\large -}{\tiny -} \tabularnewline\arrayrulecolor{white}\hline
\multirow{-5}{*}{\cellcolor{black!0} \rotatebox[origin=c]{90}{{ALL-ITER-K}}}& \cellcolor{black!20} {\normalsize codellama-34b-instruct}& \cellcolor{green!20}{\large 13}/{\footnotesize 86}& \cellcolor{green!20}{\large 18}/{\footnotesize 86}& \cellcolor{green!30}{\large 26}/{\footnotesize 86}& \cellcolor{green!10}{\large 1}/{\footnotesize 86}& \cellcolor{green!0}{\large 0}/{\footnotesize 86} \tabularnewline\arrayrulecolor{white}\hline
\arrayrulecolor{white}\hline\hline

\end{tabular}
}
\vspace{-10pt}
\end{table}

Table~\ref{tab:overall_eva_rslt} presents the results of all evaluated configurations.
The principal data displayed in the table are the question solve rates, formatted as X/Y, where X denotes the number of questions a language model successfully solves, and Y represents the total number of questions presented.
A configuration is considered to have solved a question if at least one effective fuzz driver has been generated.
For gpt-4-0613, the results of temperature 2.0 were incomplete due to the services' slow response time in extreme temperature settings\footnote{OpenAI and Azure gpt-4-0613 API frequently raise timeout for requests with high temperature setting. Temperature 2.0 results will be updated at~\cite{fuzz-drvier-study-website} once completed.}.
For the text-bison-001, Google's query API limits the requests with a temperature setting above 1.0.
Nevertheless, given the poor or even zero performance of all other models with a temperatures setting 2.0, the data absence does not substantially affect our evaluation.
\revision{
This table only lists the number of solved questions while \cite{fuzz-drvier-study-website} posts full evaluation details of each model such as success rate per question.
}
\compactline

% \noindent
% \textbf{Performance Overview} \tab 
% promising best performance, many settings have high question solve rate
% high variance range
% observation -> changing any factor can significantly affect the result
% observation -> the key for getting a decent result is avoid the bad choice of all three bad factor
\textbf{Overall, the results offer promising evidence of the practicality of utilizing language model-based fuzz driver generation.}
The optimal configurations, namely <gpt-4-0613, ALL-ITER-K, 0.5>, achieved impressive success rates, effectively generating fuzz drivers that solved about 91\% (78/86) questions.
Moreover, three out of five LLMs assessed -- including an open-source option -- and half of the strategies explored can resolve over half of the questions.

\textbf{The substantial variation in success rates across different configurations underscores the significant influence of the three factors.}
By analyzing the data, we observe that results can greatly fluctuate when varying a single factor -- such as changing the temperature in a row, or switching models or prompting strategies in a column.
For example, <gpt-3.5-turbo-0613, NAIVE-1, 0.0> failed to solve any questions, whereas <gpt-3.5-turbo-0613, ALL-ITER-K, 0.0> managed to correctly address 76\% (65/86) of them. 
This indicates that achieving a high solve rate relies heavily on avoiding suboptimal combinations of factors.
Given that the table is sorted to reflect performance trends, the better outcomes tend to cluster in 'green areas', highlighting configurations where all contributing factors are well-adjusted.

\subsection{Analysis of Effectiveness Factors}

\noindent
\textbf{Prompt Strategies} \tab 
% varies much (NAIVE-1 -> ALL-ITER-K)
% iterative & usage example 
The observed impacts of different prompting strategies exceeded our initial expectations during their design phase.
A comparison between NAIVE-1 and ALL-ITER-K showcases a dramatic improvement in optimal question solve rates, soaring from 10\% to 90\%, emphasizing the critical role of prompt design on tool effectiveness. 
To better understand the performance trends, Table~\ref{tab:overall_eva_rslt} presents the prompting strategies ranked by their overall effectiveness.
The trends in the results are intuitive: \textbf{in general, strategies that more comprehensively leverage available information tend to yield superior results}.
For example, the strategy UGCTX-K markedly outperforms BACTX-K.
This can be attributed to UGCTX-K's inclusion of example code snippets that illustrate certain usage of the target API.
A notable performance discrepancy is also seen when comparing BA-ITER-K with BACTX-K.
Despite starting with the same initial information, BA-ITER-K significantly surpasses BACTX-K.
The reason for this performance difference lies in BA-ITER-K's iterative method -- collecting debugging information to guide the model to fix the previous fuzz driver if it is ineffective.
Among all the strategies, ALL-ITER-K stands out as the most effective across different combinations of temperature settings and models.
This makes sense considering that ALL-ITER-K not only incorporates all extended API information but also adopts a recursive problem-solving methodology.
Conclusively, its design leads to the superior performance in our evaluation.
The detailed analysis of these strategies are discussed in Section~\ref{sec:rq3}.

\noindent
\textbf{Temperatures} \tab 
% 0.0 - 1.0
% the nature of the task or evaluation does not require high randomness of the reply, as long as it can provide one effective answer 
Table~\ref{tab:overall_eva_rslt} clearly demonstrates that \textbf{configurations with a temperature setting of 0.5 tend to achieve the highest success rates}.
In contrast, models under a temperature setting above 1.0 experience a noticeable drop in performance.
Interestingly, it appears that \textbf{in general, lower temperatures, especially below the threshold of 1.0, show substantial performance advantage compared to models operating at higher temperatures}.
A surprising outcome is that models with 0.0 temperature perform remarkably well.
For instance, both <gpt-4-0613, ALL-ITER-K, 0.0> and <gpt-3.5-turbo-0613, BA-ITER-K, 0.0> stand out as second-best configuration when compared across the various temperature settings.
These results are reasonable considering the nature of fuzz driver generation task.
With a lower temperature setting, models tend to generate more consistent and predictable outputs, which benefits the synthesis of high-quality code.
High temperatures, while fostering creativity and randomness, may not provide any notable advantages in this context.
Specifically, these features are either substituted by the randomness contained in prompt strategies or deemed irrelevant by the assessment criteria.
For example, a prompting strategy like ALL-ITER-K inherently contains a built-in search process that brings the randomness from model input.
And the evaluation strictly assesses the quantity of effective drivers without considering the API usage diversity.
This criteria fits our evaluation goal, but discounts the creative diversity that could be introduced by higher temperatures.
% Instead, it strictly assesses the quantity of effective ones.

\noindent
\textbf{Open-Source LLMs vs Closed-Source LLMs} \tab 
% controversial discussion, concentrated area that how open-source LLMs can achieve comparing with openai models
% open-source LLMs can achieve adequate performance
As commonly understood in the industry, closed-source LLMs tend to outperform their open-source counterparts.
Among these, gpt-4-0613 is considered the front-runner in terms of generation capabilities.
Following closely behind is gpt-3.5-turbo-0613, which offers a cost-effective alternative due to its significantly lower token pricing.
However, it's worth noting that in the open-source domain, wizardcoder-15b-v1.0 has made remarkable strides, even surpassing Google's closed-source model, text-bison-001.
While wizardcoder-15b-v1.0 is nearly on par with gpt-3.5-turbo-0613, certain performance gaps can still be observed, but it stands as a commendable achievement for an open-source model.

% \noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
% \begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
% Each factor affects the performance significantly.
% \end{tcolorbox}


\subsection{How Far Are We to Total Practicality?}
% breakdown of the overall effectiveness based on questions
The above evaluation indicates that with the optimal configuration <gpt-4-0613, ALL-ITER-K, 0.5>, the LLM can solve 91\% predefined questions.
In other words, it can produce at least one effective fuzz driver for 78 out of 86 APIs examined.
However, this does not necessarily mean that LLMs are ready to be used in production.
Upon further examination of our APIs, we identified three primary challenges and detailed them as follows.

% Figure environment removed

% \begin{table}[!t]
% \centering
% \caption{
% Percentage of High Cost Questions in Top-10 Evaluation Configuration.
% \footnotesize{
% Question Cost = 1 / (success rate in evaluation),
% Percentage = (\# of solved questions with high cost) / (\# of all solved questions).
% \zhc{change to pie chart?}
% }
% }
% \label{tab:topx-high-cost-Q}
% % \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{llllll}
% \toprule
% Question Cost & $\geq$ 2 & $\geq$ 5 & $\geq$ 10 & $\geq$ 20 & $\geq$ 40 \tabularnewline
% \midrule 
% \rowcolor{black!10}
% Avg Percentage & 90\% & 64\% & 45\% & 31\% & 18\% \tabularnewline
% \bottomrule
% \end{tabular}
% % }
% \end{table}

\noindent
\textbf{C1: High Token Cost in Fuzz Driver Generation} \tab 
% xx tokens for one correct driver
Though many configurations have shown a high rate of successful problem resolution, our analysis indicates the results come with substantial costs.
The data in Figure~\ref{fig:topx-high-cost-pie} details the percentage of high cost questions for all evaluated configurations.
Remarkably, it reveals that on average, resolving 45\% questions entail costs exceeding 10.
This suggests that for 50\% of the resolved questions, a prompt-based strategy may yield just one effective fuzz driver after repeating the entire query process 10 times or more.
When considering only the questions with costs surpassing 20 or even 40, the percentages remain notable at 31\% and 18\%, respectively.
% Another supportive fact for the high cost is that some questions\zhc{edge_connect, dns_message_checksig} have one of ten thousands of success rate in the whole evaluation.
These findings underscore a strong incentive for further research into cost reduction techniques.
Reducing costs is not only a practical concern with direct financial consequences but also essential for improving the efficiency of LLM-based fuzz driver generation.
% \zhc{make more concise}

\noindent
\textbf{C2: Ensuring Semantic Correctness of API Usage} \tab 
% xx% questions cannot be fully automatically solved
In our evaluation, we found that there is a discrepancy for approximately 34\% (29/86) of the APIs -- assuming LLMs can successfully create at least one effective fuzz driver for each in the evaluation setting, this success cannot be translated into the full automation of fuzz driver generation for them.
The issue at hand lies in the potential misuse of APIs within the generated drivers, which requires validation to ensure semantic correctness.
For example, LLMs may incorrectly initializing the argument of an API, such as passing a mutated filename to the API instead of passing a created file first and then mutating its content for fuzzing, or missing some condition checks before calling API.
In our evaluation process, we manually implemented semantic checkers to identify such API misuses for accurate assessment (details on our semantic checkers are provided in Section~\ref{sec:eval-framework}). 
However, fully automating the validation of semantic correctness remains a significant hurdle.
Consequently, even though it is feasible to generate effective fuzz drivers with the help of these LLMs, distinguishing them from the ineffective ones can be problematic due to the absence of automated methods for validating semantic correctness.
This challenge underscores the need for developing robust techniques to automatically ensure the semantic accuracy of generated fuzz drivers before they can be reliably deployed in production.

% For around 34\% APIs (29 out of 86) estimated in evaluation, there exists a gap between successfully generating one effective fuzz driver for them in evaluation with capable of fully automatically generating fuzz drivers for them in production.
% The root of the gap here is that the fuzz drivers of these APIs may contain API usage misuse which require the validation of semantic correctness.
% For example, xxx.
% In evaluation, we can manually prepare semantic checkers to help determine  these API misuses for correct evaluation (see detail of our semantic checkers in Section~\ref{sec:xxx}).
% While in production phase, it is challenging to automatically validate them.
% This causes a fact that, even it is possible to generate effective fuzz drivers using these LLMs, we cannot correctly pick it out from ineffective ones due to the lack of automatic semantic correctness validation methods.

% \noindent
\textbf{C3: Satisfying Complex API Usage Dependencies} \tab 
% contextual deps
% complex data/control deps
Overall, there are five questions cannot be resolved by any assessed configurations.
These questions are challengeable since their driver generation requires the deep understanding of specific contexts.
For instance, generating the driver for \texttt{tmux}~\cite{tmux-ossfuzz-driver-link} requires the construction of various concepts, such as session, window, pane, etc, and their relationships.
Similarly, for network-related questions~\cite{libmodbus-ossfuzz-driver-link, civetweb-ossfuzz-driver-link}, a standby network server or client is required to be created before calling the target API.
The effective drivers can only be generated by respecting these specific contextual requirements.

% \noindent
% \textbf{Quiz-Level Performance} \tab 
% Figure~\ref{fig:results-on-correct-questions} plots the overall performance for the proposed strategies.
% Specifically, 
% Figure~\ref{fig:corr-qstn-per-round} illustrates the amount of correct questions per round while the Figure~\ref{fig:stacked-corr-qstn-per-round} details the stacked amount.
% In both plots, the line style is used to distinguish the model, \textit{i.e.}, solid/dotted for \texttt{gpt4}/\texttt{gpt3.5}, and the line colors represent the type of prompt templates, \textit{i.e.}, black/red for \texttt{BACTX-K}/\texttt{NAIVE-K}.
% % \zhc{add full detail table in the appendix and mention that here}
% Apparently, \texttt{gpt4-BACTX-K} shows its performance superiority than the rest three strategies while \texttt{gpt3.5-NAIVE-K} have the lowest performance.
% This is intuitive since \texttt{gpt4-BACTX-K} is configured with a model expected to be more powerful and generates more descriptive prompt for the question.
% In general, LLM model, prompt template, and the degree of repetitive query are three independent key factors:
% \ding{182} solid lines are almost always higher than the paired dotted lines, which means that, keeping other settings the same, \texttt{gpt4} almost always perform better than \texttt{gpt3.5};
% \ding{183} black lines are significantly higher than paired red lines, indicating that, using the same LLM, prompt template of \texttt{BACTX-K} always have a significant overall performance than \texttt{NAIVE-K}'s;
% \ding{184} the stacked results in Figure~\ref{fig:stacked-corr-qstn-per-round} shows clear performance advantage than one round performance, indicating the effectiveness of repeatedly query.
% \ding{184} while the lines in Figure~\ref{fig:corr-qstn-per-round} show a relative stability on one round performance, the stacked results in Figure~\ref{fig:stacked-corr-qstn-per-round} reveal that combining the answers of all rounds can remarkably improve the overall performance, regardless of the used model and prompt template.

% \begin{table}[t]
% \centering
% \caption{Outcome Comparison Between Different K Values.}
% \label{tab:cmp-different-k}
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{lllll}
% % \hline
% \toprule
%  & \texttt{gpt3.5-NAIVE-K} 
%  & \texttt{gpt3.5-BACTX-K} 
%  & \texttt{gpt4-NAIVE-K} 
%  & \texttt{gpt4-BACTX-K} \\
% \midrule
% \rowcolor{black!10}
% R (K=40/K=1) & 3.25 (13/4) & 2.73 (30/11) & 9.33 (28/3) & 1.76 (44/25) \\
% P (K=6/K=40) & 76.92\% (10/13) & 86.67\% (26/30) & 64.29\% (18/28) & 88.64\% (39/44) \\
% % \hline
% \bottomrule
% \end{tabular}
% }
% \label{tab:cmp-different-k}
% \end{table}

%% The observation \ding{184} indicates the usefulness of repetitive queries.
%% properly exploiting the randomness in LLM replies can significantly improve the overall performance.
%For \ding{184}, though the ratio of total corrected questions for 40 rounds to one round can more than nine, the benefits of the repetitive queries rapidly decrease after certain rounds of repeated queries, roughly following the Pareto Principle~\cite{pareto-principle}.
%Specifically, as shown in Table~\ref{tab:cmp-different-k}, roughly 80\% of the overall performance of repetitive queries are contributed in the initial 20\% rounds of queries. 
%% all settings have reached nearly or more than 80\% performance at the sixth round comparing with the total outcomes of 40 rounds.
%In general, a simple stop condition can be proposed, \textit{e.g.}, no performance increase in last X rounds, to balance the benefits and query costs.
%% the number of repeat times, \textit{a.k.a.} the value of \textbf{K}, can be adaptively determined according to the past statistics of the new correct answers.

% % Figure environment removed

% \begin{table}[t]
% \centering
% \caption{Outcome Comparison Between Different K Values.}
% \label{tab:cmp-different-k}
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{llllllll}
% % \hline
% \toprule
% Strategy & S2 & S3 & S4 & S5 & S6 & S7 & S8\\
% \midrule
% \texttt{gpt3.5-NAIVE-K} & -         &-          &-      &-      &-      &-      &0.00\% \\
% \texttt{gpt4-NAIVE-K}   & -         &-          &10.50\%&-      &0.00\% &0.00\% &0.00\% \\
% \texttt{gpt3.5-BACTX-K} & -         &2.50\%     &-      &0.00\% &-      &0.00\% &0.00\% \\
% \texttt{gpt4-BACTX-K}   & 28.41\%   &-          &-      &0.00\% &0.00\% &0.00\% &0.00\% \\
% % \hline
% \bottomrule
% \end{tabular}
% }
% \label{tab:cmp-different-k}
% \end{table}

% \noindent
% \textbf{Question-Level Performance} \tab 
% % Besides discussing the overall performance, we analyzed these strategies on question-level statistics.
% Figure~\ref{fig:upset-plot-for-simple-strategies} shows the UpSet plot.
% % analyzing the solved questions sets for the above strategies.
% Among all 86 questions, 35 of them (40.70\%) have not been solved by any basic strategy, which shows a large space for further improvement.
% An interesting observation is that most strategies have uniquely solved questions: \texttt{gpt3.5-BACTX-K}, \texttt{gpt4-NAIVE-K}, and \texttt{gpt4-BACTX-K} have 2, 5, and 11 uniquely solved questions respectively. 
% On the one hand, it is an evidence that the current most effective strategy \texttt{gpt4-BACTX-K} still cannot outperform the rests in all respects. 
% On the other hand, it reminds us the probabilistic nature of the language models.
% For instance, though \texttt{gpt4-NAIVE-K} have same prompts/model as \texttt{gpt4-BACTX-K} and contain less descriptive information about the target API, it still can solve 5 questions that \texttt{gpt4-BACTX-K} failed to solve in 40 rounds.
% One possible explanation is that, for these questions, the less descriptive prompts generated by \texttt{gpt4-NAIVE-K} can happenly guide the \texttt{gpt4} model produce effective answers.
% A supportive finding is that all 5 questions uniquely solved by \texttt{gpt4-NAIVE-K} are of one project and their API usage follow similar design patterns.
% Besides, the average query success rate of \texttt{gpt4-BACTX-K} is higher than the rest questions, which indicates that \texttt{gpt4-BACTX-K} not only solves more questions but also solves them more reliably.
% % Specifically, the 11 questions uniquely solved by \texttt{gpt4-BACTX-K} cover 6 projects and have an average success rate 28.41\% on query, while the \texttt{gpt3.5-BACTX-K} and \texttt{gpt4-NAIVE-K}'s 2/5 questions cover 1/1 projects, with a success rate of 2.50\%/10.50\%.
% % \zhc{add full detail in appendix and add reference here}

% it reminds us the statistical nature of LLM-based methods.
% We cannot debug or reason the LLM-based methods in traditional way for one or two single questions.
% For instance, though \texttt{gpt4-NAIVE-K} have same prompts/model as \texttt{gpt4-BACTX-K} and contain less information about the target API, it still can solve 5 questions that \texttt{gpt4-BACTX-K} failed to solve.

%We also analyzed question-level performance by comparing the uniquely solved question sets.
%A main conclusion is that though most strategies have their uniquely solved questions, \texttt{gpt4-BACTX-K} outperforms others in most projects, which behaves more reliable.
%% outperforms others in most projects, which behaves more reliable.
%Due to the page limit, we put the detailed data at website~\cite{fuzz-drvier-study-website}.
%
\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
While LLM-based generation has shown promising potential, it still faces certain challenges towards high practicality.
\end{tcolorbox}

