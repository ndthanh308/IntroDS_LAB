%  \begin{table*}[t]
%  \centering
%  \caption{Comparison of Metrics for LLM-Generated and OSS-Fuzz Drivers.
%  \footnotesize{
%  X-axis represents the question id.
%  For clarity, the value in x-axis is omitted.
%  All subfigures should be interpreted separately since they are evaluated on different question id groups.
%  Y-axis represents the number of used APIs, coverage, and the number of crashes for the 1st/2nd/3rd row respectively.
%  The y-axis of the 3rd row is in log scale.
%  }
%  }
%  \label{fig:rq3-cmps}
%  \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{llllllllllllllllllllllllllllllllllllllllllllllllllllllllll}
% % \hline
% \toprule
% 11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11&11
% \\
% % \begin{tabular}[c]{@{}c@{}}Time\\(sec)\end{tabular} \\ 
% % \hline
% \midrule
% % \grayrow
% \bottomrule
% \end{tabular}
% }
%  \end{table*}

% % Figure environment removed

\section{OSS-Fuzz Driver Comparison (\textbf{RQ4})}
\label{sec:rq3}

\noindent
\textbf{Comparison Overview}
\tab
% three groups:
% BACTX
% UGCTX
% ITER
% In previous sections, we analyzed the effectiveness of different query strategies.
% Most fuzz drivers in OSS-Fuzz are manually written, applied practically on industrial projects, and continuously maintained and improved for years.
% strategies covered
% The comparison includes four types of query strategies: \texttt{BACTX-K}, \texttt{DOCTX-K}, \texttt{UGCTX-K}, and \texttt{ITER-K}, where the \texttt{K} remains the same as in the previous section.
We compared LLM-generated drivers with OSS-Fuzz's to obtain more practical insights.
Note that OSS-Fuzz drivers are practically used in industry for continuous fuzzing and most of them are manually written and improved for years.
Particularly, LLM-generated drivers under comparison are from gpt-4-0613 and wizardcoder-15b-v1.0 using iterative strategies with temperature 0.5.
These two configurations are the best representative for closed-source and open-source LLMs.
In total, we evaluated 53 questions which are both resolved by all configurations.
Multiple drivers of one question are merged as one to ease the comparison.
This is done by adding a wrapper snippet which links the seed scheduling with the selection of the executed logic from merged drivers.
Specifically, a switch structure is added to determine which driver it will execute based on a part of the input data.
During each fuzzing iteration, only the logic of one merged driver is executed.
Besides, some compound OSS-Fuzz drivers are designed to fuzz multiple APIs.
For clear comparison, we merged all drivers of questions involved in one compound driver as one.
In total, we prepared 38 drivers for each assessed LLM or OSS-Fuzz.
The comparisons cover both code and fuzzing metrics such as the number of used APIs, oracles, coverage, and crashes.
% which is a switch structure selecting the execution of each original driver based on the input (inspired coalescing method in FuzzGen~\cite{fuzzgen}).
% The wrapper is mainly a switch statement calling the logic of each original driver based on the first byte of the input data.
% By using this approach, fuzzer can automatically control the exploration of the logics from multiple drivers during its seed scheduling.
% The compared drivers and questions are not in one-to-one relationship, \textit{e.g.}, some OSS-Fuzz drivers are compound drivers fuzzing multiple APIs and multiple drivers can exist for fuzzing one API.
% Note that each strategy can only provide effective fuzz drivers for a part of the questions in the quiz.
% questions covered
% To investigate the main features of each strategy, the comparison focuses on each strategy's good performance questions.
% The good performance question for a strategy is the one that at least one effective fuzz driver has been generated on all evaluated LLMs in previous evaluation.
% In total, 72 questions are evaluated.
% Details of the question groups are listed in \appe~\ref{rq3-question-group-detail}.
% \zhc{or website?}
% prepare the compared driver
% metrics covered
% The comparisons covers both static and dynamic metrics including the number of used APIs, the number of oracles, the fuzzing coverage, and the number of  crashes.
% \ding{182} the number of unique APIs;
% \ding{183} the number of oracles;
% \ding{184} the fuzzing coverage;
% \ding{185} and the number of unique crashes.

\noindent
\textbf{Fuzzing Setup}
\tab
% how dynamic experiments are conducted
% For dynamic metrics \ding{184} and \ding{185}, fuzzing experiments are conducted.
Considering the randomness of fuzzing, we followed the suggestions from ~\cite{fuzz-eval}: 
the fuzzing experiments are conducted with five times of repeat for collecting average coverage information and the fuzzing of each driver lasts for 24 hours.
We used \texttt{libfuzzer}~\cite{libfuzzer} and \texttt{AFL++}~\cite{fioraldi2020afl++} as fuzzers with empty initial seed and dictionary.
"\texttt{-close\_fd\_mask=3 -rss\_limit\_mb=2048 -timeout=30}" is used for \texttt{libfuzzer} while \texttt{AFL++}'s is the default setup of \texttt{aflpp\_driver}.
For fair comparison, the coverage of fuzz driver itself is excluded in post-fuzzing data collection stage (the merged driver can have thousands of lines of code) but kept in fuzzing stage for obtaining coverage feedback.
% the fuzz driver coverage feedback during the fuzzing.
In total, the experiments took 3.75 CPU year.
% Due to page limit, Figure~\ref{fig:rq4-cmps} shows overall comparison results while full detailed data can be found at~\cite{fuzz-drvier-study-website}.

% static: 
%   api num (base & extended)
%   oracles comparison

% dynamic:
% cov
% crash

% \subsection{Comparison on Code Metrics}

\noindent
\textbf{Code Metric: API Usage}
\tab
% number of unique APIs (divided as Basic APIs and Extended APIs)
% Figure~\ref{fig:cmp-apinum} shows the results.
% Note that all \texttt{gpt4-BACTX-K} drivers (ttl 35) are additionally added in plot for supporting the findings.
% most \texttt{gpt4-ITER-K} drivers (86\%, 49/57, red bars) have used more or equal project APIs compared with OSS-Fuzz's (black bars).
% in most cases, \texttt{gpt4-ITER-K} (red bars) are higher than or equal to the OSS-Fuzz (black bars).
% In most questions, \texttt{BACTX-K} and \texttt{DOCTX-K} used less APIs than OSS-Fuzz while \texttt{UGCTX-K} and \texttt{ITER-K} contain at least the same amount of APIs.
% This is intuitive since the latter two can provide examples guiding models to use more diverse API usages while the former two cannot.
% conservative
% Mostly, it code will soon end the execution after the call of the target API with some necessary cleaning steps.
The API usage is measured by the number of unique project APIs used in the fuzz driver.
Overall, 14\% (17/35) \texttt{gpt-4-0613} drivers have used less project APIs than OSS-Fuzz's while 39\% for \texttt{wizardcoder-15b-v1.0}.
By manually investigating these drivers, we found that \textbf{LLMs conservatively use APIs in driver generation if no explicit guidance in prompts}.
For instance, some drivers only contain necessary usages such as argument initialization.
And the API usage is hardly extended such as adding APIs to use an object after parsing it.
This is a reasonable strategy since aggressively extending APIs increases the risk of generating invalid drivers.
Adding example snippets in the prompt can alleviate this situation.
% for example, src/igraph/tests/unit/foreign_empty.c provides many APIs
% models will use the API usage explicitly shown in the 
As for OSS-Fuzz drivers, the API usage diversity is case by case since they are from different contributors.
Some drivers, \textit{e.g.},~\cite{croaring-ossfuzz-driver-link} are minimally composed and some are extensively exploring more features of the target, \textit{e.g.},~\cite{lua-ossfuzz-driver-link}.
One interesting finding is that some OSS-Fuzz drivers are modified from the test files rather than written from scratch, which is a quite similar process as querying LLM with examples.
For example, \texttt{kamailio} driver~\cite{kamailio-ossfuzz-driver-link} is modified from test file~\cite{kamailio-test-example-link}.
Prompting with this example, LLM can generate similar driver code.

% an example for example code can provide high diversity of API usage
% intersting finding

% 
% case study mention the ugctx, kamailio thing (similar approach as human experts, based on example code)
% usage & iter can have greater number of apis, but its structure may not be as good as human written drivers 

\noindent
\textbf{Code Metric: Oracle}
\tab
% oracles comparison & taxonomy
% manual written has many oracles
% generated has very few (is the generated oracle correct?)
% give some cases on what are usually patterns of manually written oracles
We did statistics on the oracles of the drivers.
The result is quite clear:
in all 78 questions resolved by LLMs, OSS-Fuzz drivers of 15 questions contain at least one oracle which can detect semantic bugs, while there are \textbf{no LLM-generated drivers have oracles}.
The used semantic oracles can be categorized as following:
\ding{182} check whether the return value or output content of an API is expected, \textit{e.g.}, ~\cite{bind9-api-output-oracle};
\ding{183} check whether the project internal status has expected value, \textit{e.g.}, ~\cite{igraph-internal-status-oracle};
\ding{184} compare whether the outputs of multiple APIs conform to specific relationships, \textit{e.g.}, ~\cite{bind9-check-two-apis-oracle}.

% brief intro on oracles

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
LLMs tend to generate fuzz drivers with minimal API usages, significant space are left for further improvement such as extending the use of API outputs or adding semantic oracles.
% for better semantic bug detection.
\end{tcolorbox}

% \subsection{Comparison on Fuzzing Metrics}

\noindent
\textbf{Fuzzing Metric: Coverage and Crash}
\tab
% average value
% the relation between coverage and API usage
% the conclusion is it is comparable with human written drivers
% however, this also with the help of manually filtering
Figure~\ref{fig:cmp-cov},~\ref{fig:cmp-crash} plot the coverage and crash comparison results.
% Considering that the comparison experiments involve results of 228 different drivers, 
Instead of presenting every detail of the experiments for hundreds of drivers, the plots lists the comparison in certain metrics while the full experiment details can be found at~\cite{fuzz-drvier-study-website}.
Overall, \textbf{in most questions, the LLM-generated drivers demonstrate similar or better performance in metrics of both coverage and the number of uniquely found crashes}.
% Particularly, the drivers of \texttt{UGCTX-K} and \texttt{ITER-K} reach similar coverage in most questions.
% One interesting finding is that the performance of the merged fuzz driver can be decreased due to the poor usage driver it incorporates.
% Specifically, in Figure~\ref{fig:cmp-ugctx}, there is a segment of the relatively flat line, where the \texttt{UGCTX-K} performs significantly worse than OSS-Fuzz.
% However, according to its corresponding segment in Figure~\ref{fig:cmp-ugctx}, the drivers have used as many unique APIs as OSS-Fuzz's.
% The reason is that there are dozens of drivers insides the merged drivers for these questions and the large portion of poor usages distracted the fuzzer.
% This indicates that handling various LLM-generated drivers is not a naive thing. 
% Proper filtering for retrieving high quality drivers may be necessary for driver effectiveness improvement.
% the potential spam issue that LLM-based driver generation can face.
% We discuss this in Section~\ref{xxx} in detail.
% \noindent
% \textbf{Fuzzing Metric: Crash}
% \tab
% Figure~\ref{fig:cmp-crash} shows the crash results.
% In plots, the LLM-generated fuzz drivers show qualified bug finding abilities.
% In most questions that can be found bugs in our experiments, LLM-generated fuzz drivers show better crash finding outcomes.
Note that there are no false positive since the generated fuzz drivers are already filtered by the semantic checkers provided from our evaluation framework.
If only the fully automatic validation process are adopted, \textit{i.e.}, removing the last two checkers in Figure~\ref{fig:validation-checker}, the fuzzing outcome will be messed with huge number of false positives, incurring significant manual analysis efforts.
% our evaluation framework, which has manually written semantic checkers.
% one invalid or unsound usage incorporated in the merged driver can quickly raise large number of false positives, which require the additional processing efforts.

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
LLM-generated drivers can produce comparable fuzzing outcomes as OSS-Fuzz drivers.
In large scale application, how to practically pick effective fuzz drivers is the major challenge.
\end{tcolorbox}
