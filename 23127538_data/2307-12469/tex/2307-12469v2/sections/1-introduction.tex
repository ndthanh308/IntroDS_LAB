\section{Introduction}

% \zhc{What is fuzz driver generation}

Fuzz testing, aka fuzzing, has become the standard approach for discovering zero-day vulnerabilities.
Fuzz drivers are necessary components for fuzzing library APIs since fuzzing requires a directly executable target program.
% Since fuzzing requires a directly executable target program, fuzz drivers have become a necessary component of library API fuzzing.
Essentially, a fuzz driver is a piece of code responsible for accepting mutated input from fuzzers and executing the APIs accordingly.
An effective driver must contain a correct and robust API usage since incorrect or unsound usage can result in extensive false positive or negative fuzzing results, incurring extra manual validation efforts or testing resources waste.
Due to the high standard required, fuzz drivers are typically written by human experts, which is a labor-intensive and time-consuming process.
For instance, OSS-Fuzz~\cite{oss-fuzz}, the largest fuzzing framework for open-source projects, maintains thousands of fuzz drivers written by hundreds of contributors over the past seven years.

% Fuzz testing \lyt{(aka fuzzing)} has emerged as the de-facto standard for discovering zero-day vulnerabilities.
% Since applying fuzzing requires a directly executable target program, fuzz drivers are necessary components of API fuzzing.
% Essentially, it is a piece of code responsible for accepting mutated input from fuzzers and executing the APIs accordingly.
% An effective fuzzing driver must ensure both correctness and robustness in the API usage since any incorrect or unsound API usage may lead to extensive false positives or negatives during fuzzing, wasting the testing resources or incurring extra human validation efforts.
% Due to the high standard, fuzz drivers used to be composed by human experts, which is laber-intensive and time-consuming.
% For example, OSS-Fuzz~\cite{oss-fuzz}, the largest continuous fuzzing framework for open source projects, maintains thousands of fuzz drivers written by hundreds of contributors in the past seven years.

% \zhc{What are SOTA methods and their limitations}
% program analysis assisted, rule-based generation/transformation.

% Most of these works follow a common methodology, which involves generating fuzz drivers based on the API usage existed in consumer programs, \textit{i.e.,} programs containing code that uses these APIs.
% For instance, by abstracting the API usage as specific models such as trees~\cite{apicraft}, graphs~\cite{fuzzgen}, and automatons~\cite{rubick}, several works propose program analysis-based methods to learn the usage models from consumer programs and conduct model-based driver synthesis.
% In addition, a recent work~\cite{utopia} emphasizes that unit tests are high quality consumer programs and proposes techniques to convert existing unit tests to fuzz drivers.
% Though these approaches can produce effective fuzz drivers, their heavy requirements on the quality of the consumer programs, \textit{i.e.,} the consumers must contain complete API usage and are statically/dynamically analyzable, limit their generality.
% Furthermore, code synthesized by many works lacks human readability and maintainability, which can limit their practical application.



% Recently, various works~\cite{fuzzgen, fudge, apicraft, winnie, intelligen, rubick, utopia} have focused on developing automatic approaches to generate fuzz drivers.
% Most works follow a methodology which generates fuzz drivers by learning or converting API usages from existing example programs (aka consumer programs).
% Though these approaches can produce effective drivers, they rely on the consumer programs heavily, \textit{i.e.}, the consumers must contain complete API usage and are statically/dynamically analyzable, which limits their practicality.

% \zhc{LLM-based generation for fuzz drivers is important, promising but under-studied}

Generative LLMs (Large Language Models) have gained significant attention for their ability in code generation tasks~\cite{chatgpt,openai2023gpt4}.
They are language models trained on vast quantities of text and code, providing a conversational workflow where natural language based queries are posed and answered.
LLM-based fuzz driver generation is an attractive direction.
On one hand, LLMs inherently support fuzz driver generation as API usage inference is a basic scenario in LLM-based code generation.
On the other hand, LLMs are lightweight and general code generation platforms.
Existing works~\cite{fuzzgen, fudge, apicraft, winnie, intelligen, rubick, utopia}, which generate drivers by learning API usage from examples, requires program analysis on examples, while LLM-based generation can mostly work on texts.
% Compared to existing works which generate drivers by learning API usage from examples via program analysis based techniques, LLM-based generation can mostly work on texts.
This offers enhanced generality which facilitates not only the application on massive quantity of real-world projects but also the utilization of learning inputs in different forms.
Various sources of API usage knowledge such as documentation, error information, and code snippets can be seamlessly integrated in text form, benefiting the generation.
Moreover, LLMs can generate human-friendly code.
While some research efforts have been devoted to LLM-based code generation tasks~\cite{arxiv-adaptive-unit-test-gen,zhanglingming-llm-are-zero-shot-fuzzers,jiang2023selfplanning,pearce2022asleep,jain2022jigsaw}, none of them can provide a fundamental understanding on this direction.
% have addressed the fundamental issues concerning the application of LLMs to fuzz driver generation such as the effects, challenges, and techniques.

% First, the generality of the generation approaches is promisingly high.
% This is because the consumer programs are no longer prerequisites but auxiliaries in generation.
% Besides, benefited by the multilingual support of LLMs, utilizing various sources of information such as documentation for further improvement is more convenient.
% Moreover, LLM-generated code is human-friendly.
% While some research efforts have been devoted to LLM-based code generation tasks~\cite{arxiv-adaptive-unit-test-gen, zhanglingming-llm-are-zero-shot-fuzzers, jiang2023selfplanning,pearce2022asleep,jain2022jigsaw}, none of them have addressed the fundamental issues concerning the application of LLMs to fuzz driver generation such as the effects, challenges, and techniques.
% such as whether and to what extent LLMs can generate effective fuzz drivers, and the challenges and strategies for improvement, and the pros and cons of the generated fuzz drivers compared to those practically used.
% \zhc{perhaps the first half of this paragraph can be shrinked, add more reasoning why existing efforts cannot answer?}

% \zhc{How we do to fill the gap}

% Figure environment removed

To address this gap, we conducted an empirical study.
Our primary goal is to understand the basics towards generating "\textit{more}" effective fuzz drivers, rather than generating "\textit{more effective}" fuzz drivers.
We believe that creating effective drivers for more targets is a more fundamental issue than improving the existing ones.
% To address this issue, we propose and evaluate various query strategies that can generate fuzz drivers for a specific API.
The issue is explored by proposing and evaluating various query strategies for generation.
% that generate fuzz drivers for a specific API.
For a systematic study, we categorized the strategies and studied them from basic to enhanced.
As shown in Figure~\ref{fig:coordinates-for-study}, the categorization is based on the used information type and conversation type of a strategy.
Basic strategies only query with fundamental API information, such as header file name and function declaration, and communicate with the model in a one-and-done manner.
Enhanced strategies can add extended information such as the API documentation and example code snippets into the query prompt, and can interactively communicate with the model to iteratively improve the generated drivers.
When proposing specific strategies, both the prompt design and the number of times a query is repeated are considered.
% After investigating the strategies' effectiveness and characteristics, we compared the generated drivers with OSS-Fuzz drivers to obtain practical implications.
Besides, we also compared the generated drivers with OSS-Fuzz drivers to obtain practical implications.
% positions, advantages, and disadvantages of LLM-generated drivers.

% We started from examining basic query strategies (\textbf{RQ1}), then explored the complex ones (\textbf{RQ2}).
% Lastly, we compared the generated drivers with OSS-Fuzz drivers for further understanding (\textbf{RQ3}).

% Since many factors influencing the evaluation of a strategy, we first divided the strategies as basic and enhanced strategies, and studied them from basic to enhanced.
% As shown in Figure~\ref{fig:coordinates-for-study}, the categorization is based on the type of information used in the query and the type of the conversation.
% Basic strategies only contain fundamental information of the target in the query and communicates with the models 

% established a complexity standard categorizing the strategies and examined them from basic to enhanced.
% the type and the organization of information used in the query, the number of iteration and repetition of the query, we established a complexity standard and examined the strategies from basic to extended.
% including what information will be used and the way to organize them in the prompt, the number of times a query is repeated, and the conversation is interactive or not, 
% Figure~\ref{fig:coordinates-for-study} 

% This approach is significant because simplicity offers greater generality.
% Understanding the limits of simpler strategies can inform the design and utilization of more complex ones.
% To investigate the effectiveness of LLM in generating effective fuzz drivers, we first adopt the basic strategy that employs fundamental information about the target API, such as function declaration and header file name, to construct prompts for instructing the LLM.
% We prioritize this strategy because (1) \textit{Necessity}: fundamental information is necessary to explain the target API clearly and ensure the basic quality of the query. (2) \textit{Generality}: fundamental information is widely accessible, ensuring the generality of the task.

% After analyzing the results of the basic strategy, we can identify the challenges that LLM faces in generating effective fuzz drivers.
% This understanding helps us optimize the technique of using LLM to generate fuzz drivers.
% Based on these challenges, we propose advanced strategies that improve on two dimensions, which involve more types of information and interactive times of conversation.
% These advanced strategies can address the ineffectiveness of LLM in handling some complex tasks.

% To explain the strategies more clearly, we defined both basic strategy and advanced strategy as shown in Figure~\ref{fig:coordinates-for-study}.
% Basic strategy is the one that utilizes \textbf{fundamental} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}and \textbf{Non-Interactive} conversations.
% Otherwise, it is advanced strategy.
% As defined in Figure~\ref{fig:coordinates-for-study}, a strategy is simple if it only uses \textbf{Basic} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.} and \textbf{Non-Interactive} conversations.
% Otherwise, it is complex.
% the complexity of query strategies is measured in two dimensions: \ding{182} the type of information used for prompt composition\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}, \ding{183} and the type of the conversation.
% The \textbf{fundamental} type covers widely accessible information including function declaration and the header file name an API belongs to, while \textbf{Extended} type additionally includes API usage information which cannot be assumed generally accessible such as documentation, implementation code, and usage example code.
% The difference between \textbf{Non-Interactive} and \textbf{Interactive} conversations is whether the intermediate information produced during the conversation is utilized for improving the generation.
% \textbf{Interactive} coversations may be multi-round while the \textbf{Non-Interactive} ones' are one-round.

% \zhc{Our RQs}
Our study targets on answering three research questions:
\begin{itemize}
    \item \textbf{RQ1}\tab Can LLM generate effective fuzz drivers using the basic query strategies, and to what extent? What are the challenges and implications?
    % If there exists improvement space, what are the fundamental challenges?
    \item \textbf{RQ2}\tab Can LLM-based generation be further improved by enhanced query strategies? What are their characteristics?
    \item \textbf{RQ3}\tab How LLM-generated drivers perform comparing with OSS-Fuzz drivers in code and fuzzing metrics? 
    % What are the implications?
    % What are the pros and cons comparing LLM-generated drivers with the practically used ones? What are the implications?
\end{itemize}

% \zhc{What we practically do for answering the RQs}
To assess the effectiveness of the generated fuzz drivers in scale, we developed a semi-automated evaluation framework containing a quiz and a set of effectiveness criteria.
The quiz covers 86 questions collected from 51 fuzz drivers of 30 popular OSS-Fuzz C projects.
Each question is a qualified API required driver generation.
And the criteria checks whether a driver is effective or not.
% The APIs used in the questions were collected by analyzing 51 fuzz drivers from 30 popular C projects.
% We aimed at C projects since it is a typical memory-unsafe language where fuzz testing has shown great effectiveness in finding vulnerabilities and bugs. 
Precisely validating fuzz driver's effectiveness in large scale is challenging since it requires validating the driver's API usage semantics, involving the identification of both false positives (bugs caused by drivers) and negatives (ineffective but non-crash usage).
To address this, our criteria have two parts: general checks for successfully launching short-term fuzzing, and API-specific driver tests written manually for usage semantics validation.
% We also built a framework that can validate the generated drivers and provide taxonomized results, reducing the manual efforts required for evaluation.
In total, 189,628 drivers generated by five strategies (0.22 billion tokens) are evaluated.

% from 51 fuzz drivers covering 30 popular C projects 
% The involved APIs in quiz are collected from 51 fuzz drivers covering 30 popular C projects fuzzed by OSS-Fuzz.
% And the criteria is a semi-automatic approach containing API specific checks for precise effectiveness validation.
% Upon these, an evaluation framework is built to maximize the automation in driver evaluation process.

% designed a quiz consisting of fuzz driver generation questions and an evaluation standard.
% Each question in the quiz required writing a fuzz driver for an API qualified for fuzz testing.
% We identified the qualified APIs from popular open-source projects that are suitable for fuzzing by extracting the key APIs from the fuzz drivers of projects fuzzed by OSS-Fuzz repositories.
% The key APIs were either explicitly specified according to the author's comments and driver file names or picked according to the complexity score calculated using OSS-Fuzz Introspector~\cite{fuzz-introspector}.\zhc{check here later}
% The quiz comprised \zhc{X} questions from \zhc{Y C/C++} projects in total.
% An effective fuzz driver was defined as one that, under empty input seed, it can be built and fuzzed for a period of time (1 minute in our study) without discovering false positive bugs.
% As it is challenging to automatically determine whether a fuzz driver is effective or not, we wrote specific tests for some questions, filtered the frequently found true bugs based on their crash stack signatures, and used them to correct evaluation results.
% Besides, we built an evaluation framework based on the quiz and evaluation standard to reduce manual efforts required for evaluation.
% With these tools, we conducted our study on the research questions outlined above.

Overall, the results demonstrated encouraging practicality of the LLM-based generation.
The union results of these strategies have generated correct fuzz drivers for 55 (64\%) questions entirely automatically, and an additional 23 (91\%) questions can be solved by integrating manually configured semantic validators.
% Overall, the combined results of all strategies show that they can pass 91\% (78/86) of the questions.
% While this result indicates the strong potential of LLM-based driver generation, it does not necessarily represent its practicality.
% This is because the contribution of the validation criteria in filtering out ineffective drivers cannot be ignored.
% Without precise validation, the results may contain many ineffective fuzz drivers, which seriously hinders the practical application.
% Therefore, further improvements are needed to address these challenges and maximize the practicality.
% of LLM-based fuzz driver generation.
% Overall, the union results of all strategies can pass xx\% (xx/86) questions.
% However, this result only proves the strong potential of LLM-based driver generation but cannot represents its practicality.
% This is because the contribution of the validation criteria filtering out the ineffective drivers cannot be ignored.
% Without a precise validation, the results can contain many ineffective fuzz drivers, hindering its practical application.
% The interpretation of the result is two-fold.
% On one hand, \textbf{the high correct rate shows high potential of LLM-based fuzz driver generation}.
% On the other hand, the high correct rate is based on the results filtered from xxx queries.
% Since the validation criteria contain manually collected API specific checks, how to precisely filtering the ineffective drivers is a major obstacle for the practical application of LLM-based approaches.
% \textbf{Without a carefully developed validation criteria for the given targets, LLM-based approaches can have spam issues}.
% \zhc{The evaluation results}
Regarding to query strategies specifics, basic approaches have shown decent results (53\%, 46/86), particularly given its low technical complexity and limited information requirements about the target API.
The repeatedly query design incorporated in basic approaches contributes significantly but its benefits gradually become minor when the time of repeat increases (roughly follows 80/20 rule).
However, basic strategies struggle to generate fuzz drivers which require complex API usage detail.
Two key designs help significantly: querying with examples (84\%, 72/86) and iteratively querying (91\%, 78/86).
% LLMs struggle to generate fuzz drivers that require complex API usage specifics. Three key designs can help: repeatedly querying, querying with examples, and iteratively querying. A combination of these designs yields a dominant query strategy (ITER-K).
The effects of the former design highly relies on the quality of the code snippets and test/example files of the target project or its variants are identified high-quality sources.
The iterative strategy, which can incorporate all key designs, shows the dominant performance.
It outperforms all others by its step-by-step problem-solving approach and a more thorough utilization of existing usage.
However, it suffers from a higher search costs (2-4 times more queries in evaluation).
% For specific strategies, the basic ones have shown decent results (\zhc{xx\%}), particularly when considering the fact that they require few information about the target and have low technical complexity.
% However, its performance will decline significantly when the complexity of the API specific usage increases.
% For the questions they cannot figure out, adding example code snippets and query iteratively can help significantly (additionally solves \zhc{xx\%}).
% The example code snippets helps by directly providing the usage to model, which highly depends on the quality of the snippets.
% We identified that "test/example file" and "internal code files of the target/variant projects" are two high quality sources (contributes \%);
% The iterative queries can solve complex questions due to its comprehensive utilization of the usage information and the step-by-step problem solving approach.
% However, it suffers from high search costs and strategy complexity.
% most failures are mistakes made on API specific usages.
% The involved usages are broad, covering various dimensions from the target API usage to its dependent APIs', from the grammatical errors to semantic errors, raising challenges for improvement.
% For enhanced strategies:
% \ding{182} adding API documentation in prompt can slightly improve overall performance (xx\%) due to the limited usage description it contains;
% \ding{183} example code snippets can greatly enhance the overall performance (xx\%).
% Besides, "test/example file", "internal code files of the target/variant projects" are high quality sources for code snippets (contributes \%);
% \ding{184} query iteratively can solve questions that non-iterative strategies cannot solve.
% The superiority comes from its more comprehensive utilization of the usage information and the step-by-step problem solving approach.
Compared to OSS-Fuzz drivers, LLM-generated drivers demonstrated comparable fuzzing outcomes.
However, significant rooms are still left for improving generated drivers, such as enhancing semantic correctness filtering to increase practicality, expanding API usage, and incorporating semantic oracles.

% Comparing with OSS-Fuzz drivers, LLM-generated drivers provide competent fuzzing outcomes.
% However, as previously discussed, practical validation methods must be adopted to prevent ineffective testing and false alarms.
% Besides, manually written fuzz drivers offer unique advantages that can guide the improvement of LLM-generated drivers, such as the ability to include semantic oracles that detect logical bugs.

% By comparing with OSS-Fuzz drivers, we found that the LLM-generated drivers can provide competent fuzzing outcomes.
% However, as discussed above, practical validation methods should be adopted for preventing ineffective testing or the massive false alarms.
% Besides, manually written fuzz drivers showed some unique advantages directing the improvement of LLM-generated, such as the inclusion of semantic oracles which can detect logical bugs.

% \ding{182} LLMs tend to produce fuzz drivers with minimal API usages while adding example snippets improves the contained usages;
% \ding{183} OSS-Fuzz drivers contain oracles which can check semantic bugs while LLM-generated do not have any;
% \ding{184} LLM-generated drivers can reach comparable fuzzing outcomes as OSS-Fuzz drivers.
% However, as discussed above, how to practically pick effective drivers is the major challenge for its large scale application.

% the interpretation of the result is two-fold
% 1. it shows strong potential 
% 2. the validation criteria is still a major obstacle for its practical application, which requires appropriate approaches/methods
% specifically, 
%  basic strategies performance, characteristics, etc
%  enhanced strategies and their characteristics
%  the metrics comparison conclusion

% \zhc{Contribution}
In summary, our contributions are:
\begin{itemize}
    \item we conducted the first in-depth study on the effectiveness of LLM-based fuzz driver generation, which demonstrates the practicality and challenges in this direction;
    \item we designed and implemented five driver generation strategies.
    They are evaluated in large scale, with a systematic analysis on the effectiveness, the pros and the cons;
    \item we built the first fuzz driver evaluation framework which can evaluate the generated fuzz drivers in scale;
    \item we compared generated drivers with industrial used ones, and summarized the implications on future improvements.
\end{itemize}
We are releasing all the code and data utilized in our study~\cite{fuzz-drvier-study-website}.
% The code and data used in our study is gradually releasing~\cite{fuzz-drvier-study-website}.
% to facilitate the community.
% To facilitate the community, we will open source our quiz, evaluation framework, and the data involved in the study at~\cite{fuzz-drvier-study-website}.
