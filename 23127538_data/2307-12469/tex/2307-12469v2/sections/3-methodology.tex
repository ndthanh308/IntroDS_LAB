\section{Methodology}
\label{sec:methodology}

% This section is for: How we conduct our study to answer these RQs?
% Problem scope declaration: effective fuzz driver generation, ...

% \subsection{Overview of Study}

\noindent
\textbf{Overview of Study} \tab 
Figure~\ref{fig:overview-for-study} shows the study overview.
To understand the effectiveness of different generation strategies, we built an evaluation framework containing a set of generation questions and the effectiveness criteria.
Each question requires the LLMs to generate a fuzz driver according to a given API and the effectiveness of the generated driver will be evaluated based on the criteria.
% An API is qualified if its project is significant, \textit{i.e.}, tested by OSS-Fuzz, and it is the core API inside the existing fuzz drivers.
% The effectiveness criteria of an API are the conditions or checks to distinguish ineffective drivers and effective ones. 
% Then we built an evaluation framework to maximize the automation of the evaluation.
% The framework will add general contents of the prompts, launch LLM queries, validate the effectiveness of replies, and classify the failed validation.
Upon these, we studied the effectiveness of five query strategies from basic to enhanced (the first two \textbf{RQs}).
% the first two RQs are studied by comparing and analyzing the evaluation results of different query strategies.
In \textbf{RQ1}, we designed and explored the effectiveness of basic strategies which use fundamental API information and have simple interactions with LLMs;
In \textbf{RQ2}, enhanced strategies which leverage extended API usage information and interactive queries are studied.
Necessary in-depth analysis were conducted to explain the benefits and limitations of these strategies.
Lastly, in \textbf{RQ3}, we compared LLM-generated fuzz drivers with OSS-Fuzz drivers to understand their positions, characteristics, and limitations.
% The comparison involves the static metrics of the code such as the number of unique APIs and dynamic metrics including the testing coverage and unique crashes.

% Figure environment removed


% \subsection{Quiz Construction}
% \label{sec:quiz-construction}

\noindent
\textbf{Quiz Construction} \tab 
% What is a question for fuzz driver generation
The question in quiz is designed as generating fuzz drivers for one given API.
% This is based on the intuition that any fuzz driver can be divided into one or more simpler fuzz drivers targeting different APIs.
% In other words, complicate, multi-purpose fuzz drivers are essentially the combination code which fuzzes multiple API targets simultaneously.
% Therefore, as the first practical evaluation on LLM-based fuzz driver generation, we focused on the more fundamental scenario.
% why selecting an appropriate question is not naive
However, not all APIs are suitable to be set as questions.
Naively setting all APIs as questions will lead to the creation of meaningless or confusing questions which influences the evaluation result.
Specifically, some APIs, such as \texttt{void libxxx\_init(void)}, are meaningless fuzz targets since the code executed behind the APIs can not be affected by any input data.
Some APIs can only be supplemental APIs rather than the main fuzzing target due to the nature of their functionalities.
For example, given two APIs \texttt{object *parse\_from\_str(char *input)} and \texttt{void free\_object(object *obj)}, feeding the mutated data into \texttt{input} is apparently a better choice than feeding a meaningless pointer to the \texttt{obj} argument.
However, calling the latter API when fuzzing the former is meaningful since 
\ding{182} it may uncover the hidden bug when the former does not correctly initialize the \texttt{object *} by freeing the members of \texttt{object};
\ding{183} it releases the resources allocated in this iteration of fuzzing, which prevents the falsely reported memory leak issue.

% how did we select qualified APIs
To guarantee the questions of our quiz are qualified and representative, we collected the core APIs of existing fuzz drivers from OSS-Fuzz projects.
% To build a high quality quiz, we need to collect a set of qualified APIs which are both representative and suitable to be set as fuzzing targets.
% Our intuition is to collect core APIs of the existing fuzz drivers from popular projects fuzzed by OSS-Fuzz.
A driver's core APIs are identified using the following criteria:
\ding{182} they are the target APIs explicitly pointed out by the author in its driver file name or the code comments, \textit{e.g.}, \texttt{dns\_name\_fromwire} is the core API of driver \textit{dns\_name\_fromwire.c};
\ding{183} otherwise, we pick the basic APIs as the core rather than the supplemental ones.
For example, we picked the former between parse and use/free APIs.
For the fuzz drivers which are composite drivers fuzzing multiple APIs simultaneously, we identified multiple core APIs from them.
Specifically, we randomly selected 30 projects from OSS-Fuzz (commit \texttt{135b000926}) C projects, manually extracted 86 core APIs from 51 fuzz drivers.
See full quiz at~\cite{fuzz-drvier-study-website}.
% More details are listed in \appe~\ref{sec:quiz-questions-detail}.

\noindent
\textbf{Effectiveness Criteria Establishment} \tab 
% What is an effective fuzz driver
% general validation method is hard
% can greatly influence the evaluation result
% our approach
% -> semi-automatic criteria: automatic + manually built checkers
% An effective fuzz driver represents the drivers which have correct API usage and produce no false positives.
% Precisely validating the effectiveness of fuzz drivers is crucial for evaluating fuzz driver generation methods.
% However, it is hard to propose a general validation technique since the effectiveness
% general validation techniques do not work well due to the diverse semantics on the API usages.
An effective fuzz driver should use the API effectively while producing no false positives.
Determining a driver's effectiveness is challenging since it involves the semantic classification of the false positives (bugs caused by the driver code) and negatives (ineffective usage).
To validate precisely, we adopted a semi-automatic method.
As shown in Figure~\ref{fig:validation-checker}, it has four checkers.
The first two are automatic and general checks about whether a short-term fuzzing can be conducted successfully given the generated driver.
If the compilation failed or the short-term fuzzing (one minute fuzzing with empty initial seed) reported any bug or had no coverage progress, it is an ineffective driver.
% It checks the compilation result the  the driver reports any bugs, \textit{i.e.}, crashes or timeout, or does not have any coverage progress in a short time period with a default fuzzing setup (empty seed, no dictionary, etc).
% checks require manual configuration for each project while the rest two need to be configured per API, \textit{a.k.a.} per question in our quiz.
% The first check examines the grammatical correctness of a driver using compiler, while the second one checks the existence of abnormal fuzzing behaviours via short-term fuzzing.
The intuition here is that, under a default setup, neither the zero coverage progress nor the quick identification of bugs are normal behaviours.
% In our study, we set the time period as one minute.
Obviously, the general check is a rough measurement which can cause false validations.
The rest two checkers are introduced to refine the validation.
By fuzzing the OSS-Fuzz provided drivers, we collected the signatures of real bugs that can be found quickly.
We filtered these bugs in the third checker.
Lastly, by manually examining API usages, we summarized the semantic constraints of APIs and wrote tests checking these generated drivers.
% summarized semantic constraints of a correct driver per API.
% These constraints are written as tests to the fuzz drivers.
For example, assuming an API requires passing mutated input via file, the semantic test is hooking the API and checking whether the passed filename points to a existing file carrying the mutated input.
% Our framework also collected and taxonomized all unpassed checks (Section~\ref{subsec:evaluation-framework}), helping us to iteratively improve the inexact checks.
In our study, mostly the validation does not require introducing semantic checkers, their detailed contribution statistics can be found at ~\cite{fuzz-drvier-study-website}.
% showed a detailed statistics data for the contribution of these checkers.

% \subsection{Evaluation Framework}
% \label{subsec:evaluation-framework}

% Figure environment removed

% what the evaluation framework is and how to use
\noindent
\textbf{Evaluation Framework} \tab 
Based on the quiz and criteria, we built a framework for automating the driver evaluation in scale.
% To boost the evaluation of a large amount of fuzz drivers, we developed a framework to maximize the automation.
% As shown in Figure~\ref{fig:overview-for-study}, the framework takes a prompt produced by a query strategy as input, and outputs the taxonomized validation result.
Figure~\ref{fig:overview-for-study} shows the framework workflow.
% It also provides a website to ease the manual analysis.
In total, the framework is written in 9,342/1,542/3,857 lines of Python/HTML/JSON, YAML, and Bash scripts.
% Instead of illustrating every detail, we discussed several key configurations here.
% \noindent
It takes a prompt produced by a query strategy as input, and outputs the taxonomized validation result.
To properly evaluate the generated drivers of LLMs, format control declarations are added to prompts and the code will be extracted if an answer is a hybrid of code and text.
Besides, to focus on driver code evaluation, the compilation environment is prepared in advance and each driver is validated in a fresh, isolated container.
% Considering the non-deterministic nature of LLM, we summarized patterns for extracting code inside answer when it replies both text and code.
% \textbf{Effectiveness Validation} \tab 
% Due to the non-deterministic nature of the LLM, some replies still can contain both code and text.
% We summarized the patterns and extracted the code in replies.
% Besides, to focus on evaluating the effectiveness of the generated code, our framework automatically configures the required header files and build options for a driver.
% This is done by manually configuring the rules of header inclusion, and build options for the selected 30 projects, which guarantees all compilation and link errors are caused by the incorrect code rather than the unsuitable configurations.
% Besides, each validation of a fuzz driver is in a fresh, isolated container, excluding the environment disturbances to the results.
% \noindent
% \textbf{Failure Taxonomy} \tab 
% automatic taxonomy
% semi-automatically taxonomy
The validated results are taxonomized according to the string patterns of checkers' results~\texttt{clang}~\cite{clang-lex-diagnostics, clang-parse-diagnostics, clang-sema-diagnostics} and \texttt{libfuzzer}~\cite{libfuzzer, sanitizers}.
The taxonomy logic is developed iteratively by manual review and correction.
% The taxonomy of root causes for ineffective results also follows a semi-automatic approach.
% We first did a rough categorization of the compilation, link, and fuzz errors based on the string patterns of the errors outputted by \texttt{clang}~\cite{clang-lex-diagnostics, clang-parse-diagnostics, clang-sema-diagnostics} and \texttt{libfuzzer}~\cite{libfuzzer, sanitizers}.
% Then for each API, we manually identified the root causes per category and mapped each category into the final categories. 
% The manual mappings were written in code and will be iteratively improved once we found a new unclassified failure. 

\begin{table}[!t]
\centering
\caption{Main Evaluated LLMs.}
\label{tab:evaluated_models}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{llll}
\toprule
 Model & Abbr & Max Tokens & Training Data \tabularnewline
\midrule 
\rowcolor{black!10}
gpt-3.5-turbo-0301 & \texttt{gpt3.5} & 4,096 & Up to Sep 2021 \tabularnewline
gpt-4-0314 & \texttt{gpt4} & 8,192 & Up to Sep 2021 \tabularnewline
\bottomrule
\end{tabular}
}
\vspace{-10pt}
\end{table}

\noindent
\textbf{Model Setup} \tab 
Table~\ref{tab:evaluated_models} shows the mainly evaluated LLMs and parameters.
See the detail of other evaluated LLMs in Section~\ref{sec:discussion}.
Both the temperature and top\_p are the default value 1.
Prompts exceeding the token limits will be adjusted in strategies.
% The parameters of the evaluated models are the default values in their websites~\cite{chatgpt-default-config}, \textit{e.g.}, temperature is set as 0.9 and top\_p is 1.
% All the LLM-generated fuzz drivers evaluated in this study are retrieved via ChatGPT web interfaces~\cite{chatgpt-website} (release version 23 Mar 2023) based on \texttt{chatgpt-wrapper}\cite{chatgpt-wrapper} v0.7.1 (\texttt{dabe72101b}).
% All the LLM-generated fuzz drivers evaluated in this study are retrieved via \texttt{chatgpt-wrapper}~\cite{chatgpt-wrapper} v0.7.1 (\texttt{dabe72101b}).
% \noindent
% \textbf{Prompt Standardization} \tab 
% controlling the output
% extract code part only
% preparing headers
% What are already assumed to be correctly provided, and how we postprocessing the answers
For the models providing role parameters, the system role is set as \texttt{"You are a security auditor who writes fuzz drivers for library APIs"}.
Unless specifically mentioned in the strategy, the following sentence is inserted at the beginning of the prompt to help standardize the output format.

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
\small  \texttt{// The following is a fuzz driver written in C language, complete the implementation. Output the continued code in reply only.{\textbackslash}n{\textbackslash}n}
\end{tcolorbox}
% \noindent
% \textbf{LLM Query} \tab 
% token limitation, how many left for query and how many for answer
% perhaps discuss this in iteration section is better
% one query one conversation
% restful deisgn?
% LLMs limit the maximum token numbers for the sum of tokens in query and answer.
% We set 6,000 tokens for the prompt of \texttt{gpt4}, and 3,600 tokens for \texttt{gpt3.5}.

