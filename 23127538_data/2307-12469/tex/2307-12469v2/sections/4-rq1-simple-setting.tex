\begin{table}[t]
\caption{Two Types of Basic Strategies.
\footnotesize{"\texttt{\$\{XXX\}}" represents a variable inside prompt, which will be instantiated according to the API information.
Specifically, \texttt{\$\{API\_NAME\}} represents the target API name, \texttt{\$\{HEADER\_FILE\}} stands for the header file the API belongs to, and \texttt{\$\{API\_DECALRATION\}} represents the declaration statement of the function found in the header.
\textbf{K} means that the same prompt will be queried \textbf{K} times for one question (API).
}
}
\label{tab:simple-strategies-detail}
\resizebox{\linewidth}{!}{
\begin{tabular}{lll}
\toprule
Strategy Type & Prompt Template & Repetition Times\\
\midrule

\rowcolor{black!10}
\begin{tabular}[t]{l}
\multirow{3}{*}{\texttt{NAIVE-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\Verb|// the following function fuzzes ${API_NAME}| \\
\Verb|extern int LLVMFuzzerTestOneInput|\texttt{(\ } \\
\Verb| const uint8_t *Data, size_t Size|\texttt{) \{} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{3}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\
\midrule

\begin{tabular}[t]{l}
\multirow{3}{*}{\texttt{BACTX-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\texttt{\#}\Verb|include "${HEADER_FILE}"| \\
\Verb|${API_DECLARATION}| \\
\Verb|${NAIVE-K_TEMPLATE}| \\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{3}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\

\bottomrule

\end{tabular}
}
\end{table}

\section{Basic Query Strategies (\textbf{RQ1})}

\subsection{Overall Performance}

\noindent
\textbf{Design of Strategies} \tab 
% Simple strategies can use basic types of information and non-inter\-active conversations.
% Within this scope, we have designed two types of strategies, \textbf{NAIVE-K} and \textbf{BACTX-K}, whose details are illustrated in Table~\ref{tab:simple-strategies-detail}.
Table~\ref{tab:simple-strategies-detail} details the designed two strategies: \texttt{NAIVE-K} and \texttt{BACTX-K}.
The key difference between these two exists in their prompt templates.
Specifically, \texttt{NAIVE-K} directly asks LLMs to implement the fuzz driver solely based on a specified function name, while \texttt{BACTX-K} provides a basic description of the API.
In prompts of \texttt{BACTX-K}, it first indicates the task scope using \texttt{\#include} statement, then provides the function declaration, and finally requests implementation.
The declaration is extracted from the Abstract Syntax Tree (AST) of the header file, including both the signature and argument variable names.
% For instance, the declaration of \texttt{parse\_msg} in the \texttt{kamailio} project is "\texttt{extern int parse\_msg(char *const buf,const unsigned int len,struct sip\_msg *const msg);}".
The suffix \texttt{K} in the strategy name indicates the query will be repeated \texttt{K} times until solving a given question.
% \ding{182} the LLM model the strategy used;
This is motivated by the observation that replies of LLMs have randomness, and they can even provide quite different answers for two same queries.
Therefore, it is necessary to include it for more comprehensive understanding of strategies' effectiveness.
In our study, the maximum \texttt{K} is set as 40.
% We also considered the effect of repetitiveness in the design of these strategies: the \texttt{K} in their names represents the number of times the query is repeated per API.
% \noindent
% \textbf{Experiment Setting} \tab 
% Our experiment covers all 86 questions in the quiz, two latest LLMs in Table~\ref{tab:evaluated_models}.
% For brevity, \texttt{gpt3.5} stands for \texttt{gpt3.5-turbo} in the following.
% Since the prompt for one question will be repeatedly queried, we use one round of query to refer to a whole pass of query for all questions.
% In total, four strategies are evaluated: \texttt{gpt3.5-NAIVE-K}, \texttt{gpt3.5-BACTX-K}, \texttt{gpt4-NAIVE-K}, and \texttt{gpt4-BACTX-K}, where \texttt{K} = 40.
% These strategies are evaluated on all 86 quiz questions.
% The \textbf{K} has been set as 40 for sufficient evaluation of the repetitiveness, \textit{i.e.}, each generated prompt will be repeatedly queried for 40 times.
% Besides, both two LLM models listed in Table~\ref{tab:evaluated_models}, \textit{i.e.}, \texttt{gpt-3.5-turbo} and \texttt{gpt-4}, are evaluated.

% \subsection{Effectiveness Analysis}

% Figure environment removed

\noindent
\textbf{Quiz-Level Performance} \tab 
% Figure~\ref{fig:results-on-correct-questions} plots the overall performance for the proposed strategies.
% Specifically, 
Figure~\ref{fig:corr-qstn-per-round} illustrates the amount of correct questions per round while the Figure~\ref{fig:stacked-corr-qstn-per-round} details the stacked amount.
In both plots, the line style is used to distinguish the model, \textit{i.e.}, solid/dotted for \texttt{gpt4}/\texttt{gpt3.5}, and the line colors represent the type of prompt templates, \textit{i.e.}, black/red for \texttt{BACTX-K}/\texttt{NAIVE-K}.
% \zhc{add full detail table in the appendix and mention that here}
Apparently, \texttt{gpt4-BACTX-K} shows its performance superiority than the rest three strategies while \texttt{gpt3.5-NAIVE-K} have the lowest performance.
This is intuitive since \texttt{gpt4-BACTX-K} is configured with a model expected to be more powerful and generates more descriptive prompt for the question.
In general, LLM model, prompt template, and the degree of repetitive query are three independent key factors:
\ding{182} solid lines are almost always higher than the paired dotted lines, which means that, keeping other settings the same, \texttt{gpt4} almost always perform better than \texttt{gpt3.5};
\ding{183} black lines are significantly higher than paired red lines, indicating that, using the same LLM, prompt template of \texttt{BACTX-K} always have a significant overall performance than \texttt{NAIVE-K}'s;
\ding{184} the stacked results in Figure~\ref{fig:stacked-corr-qstn-per-round} shows clear performance advantage than one round performance, indicating the effectiveness of repeatedly query.
% \ding{184} while the lines in Figure~\ref{fig:corr-qstn-per-round} show a relative stability on one round performance, the stacked results in Figure~\ref{fig:stacked-corr-qstn-per-round} reveal that combining the answers of all rounds can remarkably improve the overall performance, regardless of the used model and prompt template.

\begin{table}[t]
\centering
\caption{Outcome Comparison Between Different K Values.}
\label{tab:cmp-different-k}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lllll}
% \hline
\toprule
 & \texttt{gpt3.5-NAIVE-K} 
 & \texttt{gpt3.5-BACTX-K} 
 & \texttt{gpt4-NAIVE-K} 
 & \texttt{gpt4-BACTX-K} \\
\midrule
\rowcolor{black!10}
R (K=40/K=1) & 3.25 (13/4) & 2.73 (30/11) & 9.33 (28/3) & 1.76 (44/25) \\
P (K=6/K=40) & 76.92\% (10/13) & 86.67\% (26/30) & 64.29\% (18/28) & 88.64\% (39/44) \\
% \hline
\bottomrule
\end{tabular}
}
\label{tab:cmp-different-k}
\end{table}

% The observation \ding{184} indicates the usefulness of repetitive queries.
% properly exploiting the randomness in LLM replies can significantly improve the overall performance.
For \ding{184}, though the ratio of total corrected questions for 40 rounds to one round can more than nine, the benefits of the repetitive queries rapidly decrease after certain rounds of repeated queries, roughly following the Pareto Principle~\cite{pareto-principle}.
Specifically, as shown in Table~\ref{tab:cmp-different-k}, roughly 80\% of the overall performance of repetitive queries are contributed in the initial 20\% rounds of queries. 
% all settings have reached nearly or more than 80\% performance at the sixth round comparing with the total outcomes of 40 rounds.
In general, a simple stop condition can be proposed, \textit{e.g.}, no performance increase in last X rounds, to balance the benefits and query costs.
% the number of repeat times, \textit{a.k.a.} the value of \textbf{K}, can be adaptively determined according to the past statistics of the new correct answers.

% % Figure environment removed

% \begin{table}[t]
% \centering
% \caption{Outcome Comparison Between Different K Values.}
% \label{tab:cmp-different-k}
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{llllllll}
% % \hline
% \toprule
% Strategy & S2 & S3 & S4 & S5 & S6 & S7 & S8\\
% \midrule
% \texttt{gpt3.5-NAIVE-K} & -         &-          &-      &-      &-      &-      &0.00\% \\
% \texttt{gpt4-NAIVE-K}   & -         &-          &10.50\%&-      &0.00\% &0.00\% &0.00\% \\
% \texttt{gpt3.5-BACTX-K} & -         &2.50\%     &-      &0.00\% &-      &0.00\% &0.00\% \\
% \texttt{gpt4-BACTX-K}   & 28.41\%   &-          &-      &0.00\% &0.00\% &0.00\% &0.00\% \\
% % \hline
% \bottomrule
% \end{tabular}
% }
% \label{tab:cmp-different-k}
% \end{table}

% \noindent
% \textbf{Question-Level Performance} \tab 
% % Besides discussing the overall performance, we analyzed these strategies on question-level statistics.
% Figure~\ref{fig:upset-plot-for-simple-strategies} shows the UpSet plot.
% % analyzing the solved questions sets for the above strategies.
% Among all 86 questions, 35 of them (40.70\%) have not been solved by any basic strategy, which shows a large space for further improvement.
% An interesting observation is that most strategies have uniquely solved questions: \texttt{gpt3.5-BACTX-K}, \texttt{gpt4-NAIVE-K}, and \texttt{gpt4-BACTX-K} have 2, 5, and 11 uniquely solved questions respectively. 
% On the one hand, it is an evidence that the current most effective strategy \texttt{gpt4-BACTX-K} still cannot outperform the rests in all respects. 
% On the other hand, it reminds us the probabilistic nature of the language models.
% For instance, though \texttt{gpt4-NAIVE-K} have same prompts/model as \texttt{gpt4-BACTX-K} and contain less descriptive information about the target API, it still can solve 5 questions that \texttt{gpt4-BACTX-K} failed to solve in 40 rounds.
% One possible explanation is that, for these questions, the less descriptive prompts generated by \texttt{gpt4-NAIVE-K} can happenly guide the \texttt{gpt4} model produce effective answers.
% A supportive finding is that all 5 questions uniquely solved by \texttt{gpt4-NAIVE-K} are of one project and their API usage follow similar design patterns.
% Besides, the average query success rate of \texttt{gpt4-BACTX-K} is higher than the rest questions, which indicates that \texttt{gpt4-BACTX-K} not only solves more questions but also solves them more reliably.
% % Specifically, the 11 questions uniquely solved by \texttt{gpt4-BACTX-K} cover 6 projects and have an average success rate 28.41\% on query, while the \texttt{gpt3.5-BACTX-K} and \texttt{gpt4-NAIVE-K}'s 2/5 questions cover 1/1 projects, with a success rate of 2.50\%/10.50\%.
% % \zhc{add full detail in appendix and add reference here}

% it reminds us the statistical nature of LLM-based methods.
% We cannot debug or reason the LLM-based methods in traditional way for one or two single questions.
% For instance, though \texttt{gpt4-NAIVE-K} have same prompts/model as \texttt{gpt4-BACTX-K} and contain less information about the target API, it still can solve 5 questions that \texttt{gpt4-BACTX-K} failed to solve.

We also analyzed question-level performance by comparing the uniquely solved question sets.
A main conclusion is that though most strategies have their uniquely solved questions, \texttt{gpt4-BACTX-K} outperforms others in most projects, which behaves more reliable.
% outperforms others in most projects, which behaves more reliable.
Due to the page limit, we put the detailed data at website~\cite{fuzz-drvier-study-website}.

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
Both LLM model, prompt template, and repetitive query significantly contribute to the overall performance of a strategy.
Specifically, the benefits of repetitive query roughly follow 80/20 rule.
With 40 times of repeat, the best basic strategy \texttt{gpt4-BACTX-K} can correctly answer 40.70\% questions.
\end{tcolorbox}

% Figure environment removed

% \noindent
% \textbf{Boundaries of Effectiveness} \tab 

% Figure environment removed

% \subsection{Boundaries of Effectiveness}
\subsection{Links Between Question and Performance}

% The difference among LLM replies for the same query is the power source of the effectiveness for repetitive queries.
% Specifically, a second time answer from the LLM may add or remove certain condition checks, change the macro used in the option argument of an API, and even switch the entire API usage comparing with previous answer.
% Our main observation is that LLMs' performance degrades as they are required to consider more API specific usages when writing a driver.
% Before delving into a detailed discussion, we will first outline the minimum API usages required for writing a fuzz driver.
% Then we examine the challenges that LLMs may encounter when attempting to correctly synthesize these usages.
% Finally, evidence are presented to support our findings.
Though the key factors for overall performance have been discussed, the relation between the performance and question is still unclear.
% key factor affecting the performance is still unknown.
The main observation is that LLMs' performance degrades when they need to consider more API specific usages during driver generation.
To solid the observation, we first explain the challenges LLMs may encounter in generation, then a score system quantifying the degree of API specific usage is introduced to prove the association.
% We first hypothesized that LLMs' performance is related to complexity of API usage. 
% Then we examined the
% To test the hypothesis, we propose the evaluation steps as follows: first, we identify the minimum API usage required for fuzz driver creation; second, we examine the potential difficulties LLMs may encounter when accurately synthesizing these usages; and finally, we design a score system to quantify the degree of the minimum API specific usage, and associate it with the performance of fuzz driver generation.

% Although LLMs may employ different mechanisms for generating code compared to the traditional workflow of driver composition, they still must meet certain minimum requirements.
For generating effective drivers, LLMs should at least generate code satisfying minimal requirements.
In other words, they must accurately predict the API argument usage and control flow dependencies.
However, this is challenging since LLMs cannot validate their predictions against documentation or implementations as humans do.
It is reasonable to assume that LLMs have learned the language basics and common programming practices due to their training on vast amounts of code.
But the API specific usage, such as the semantic constraints on the argument, cannot be assumed.
On one hand, there may only have limited data about this in training.
On the other hand, details can be lost during preprocessing or the learning stage while the accurate generation is required.
% , such as its definitions, implementations, documentation, and usage examples, comprises only a negligible proportion of the training data.
% Furthermore, details may have been lost during preprocessing or the learning stage.
Therefore, the more API usage a LLM needs to predict, the greater the likelihood of errors, particularly for less common usages that do not follow the mainstream design patterns or have special semantic constraints.
Such situations are particularly common in C languages, whose APIs often contain low-level project-specific details.

To prove our reasoning, we designed a score system to quantify the degree of the minimal API specific usage for each question.
We first created minimized fuzz drivers for each question, then calculated their scores accordingly.
The process of creating minimal drivers is as follows:
\ding{182} using the OSS-Fuzz driver as initial one;
\ding{183} removing the unnecessary code.
A piece of code is unnecessary if its removal does not affect the driver's effectiveness;
\ding{184} simplifying the remaining code by replacing the argument initialization to a simpler form.
The level of the simplicity is based on the cases classified in Section~\ref{sec:preliminaries}.
% We classified the initialization to the four cases mentioned in Section~\ref{sec:preliminaries},
% and the ascending order on simplicity for these cases are: \ding{172}, \ding{173}, \ding{174}, and \ding{175}.
The score is calculated as the sum of the count of the followings:
\ding{182} unique project APIs;
\ding{183} unique common API usage patterns;
\ding{184} unique identifiers which are not naive values after removing the common API usage code;
\ding{185} branches and loops after removing the common API usage code.
\ding{182}, \ding{184} are of the API specific vocabulary score while \ding{185} stands for API specific control flow dependencies'.
Note that all branches of one condition will be counted as one.
The calculation examples are detailed in~\cite{fuzz-drvier-study-website}.

Figure~\ref{fig:succ-rate-plots-per-score} shows the relationship between success rate and score.
Figure~\ref{fig:qstn-succ-rate-per-score}/~\ref{fig:query-succ-rate-per-score} presents the success rate of questions/queries in given score buckets.
% Note that each question contains 40 repeated queries in our experiments.
\textbf{Both plots reveal a clear inverse proportion relationship between success rate and score, irrespective of the used models and prompts, supporting our reasoning.}
% There is a stronger relationship in query success rate.
Note that the query success rate shows a stronger relationship.
This is reasonable since:
\ding{182} query success rate is a more direct and fine-grained metric reflecting question difficulty;
\ding{183} the query success rate is calculated based on 40 repeated queries per question, providing more data and leading to less fluctuation in the overall success rate calculation.
Specifically, when score $\geq$ 7, all query success rates $<$ 10\%, and the rate drops to zero when score $\in [13, \infty]$.

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
% \textbf{Key Understanding:}
The performance of LLM-based generation declines significantly when the complexity of API specific usage increases.
% Our score can be used to quantify this association.
\end{tcolorbox}

\subsection{Failure Analysis}
\label{sec:failure-analysis}

%	analyze failures on hard to solve/unsolved questions
% 
%	-> have common blockers/obstacles 
% 
%	-> analyze/classify the root cause of the blockers
%
%	1. nonintuitive API usage
% 
%	2. indirectly dependent API usage
%
%        case studies for repetitive queries, do we need that?

\noindent
\textbf{Overview} \tab 
% what failure analysis can bring
% Although we have established understanding on why, it is still unclear of the detailed reasons making the generation fail.
We conducted failure analysis to answer why the generation fails, focusing on questions that basic strategies failed to solve.
Specifically, the direct reason of the union of unsolved questions (58 questions) for \texttt{gpt3.5/4-BACTX-K} are collected to reveal the current main blockers for generation.
% the direct reason of the error instead of all potential root causes.
% It is technically challenging to exhaustively count root causes as one error can be fixed in multiple ways, resulting in numerous enumeration list.
% there can exist multiple effective drivers for a given question, resulting in numerous enumeration lists.
% Instead, focusing on the direct root cause helps reveal the current main blockers for generation.
In total, 9,107 ineffective drivers were analyzed.
The root cause identification process is based on the taxonomy provided by our framework.
% follows the semi-automatic approach mentioned in Section~\ref{subsec:evaluation-framework}. 
Specifically, the runtime errors, involving 2,734 drivers, are manually analyzed while the compilation and link errors are categorized based on the error information provided by compiler.
% \zhc{fix the ungrouped 276!}

\noindent
\textbf{General Taxonomies} \tab 
% results of failure analysis
Figure~\ref{fig:rq1-failure-taxonomy}a details the root cause taxonomy.
There are nine root causes fallen into two categories:
the grammatical errors reported by compilers in build stage, and the semantic errors which are abnormal runtime behaviors identified from the short-term fuzzing results.
\ding{182} \textit{G1 - Corrupted Code}, the drivers do not contain a complete function of code due to either the token limitation or mismatched brackets;
\ding{183} \textit{G2 - Language Basics Violation}, the code violates the language basics like variable redefinition, parentheses mismatch, incomplete expressions, etc;
\ding{184} \textit{G3 - Non-Existing Identifier}, the code refers to non-existing things such as header files, macros, global variables, members of a \texttt{struct}, etc;
\ding{185} \textit{G4 - Type Error}. One main subcategory here is the code passes mismatched number of arguments to a function.
The rest are either unsupported type conversions or operations such as calling non-callable object, assigning \texttt{void} to a variable, allocating an incomplete \texttt{struct}, etc;
\ding{186} \textit{S1 - Incorrect Input Arrangement}, the input size check either is missed when required or contains an incorrect condition;
\ding{187} \textit{S2 - Misinitialized Function Args}, the value or inner status of initialized argument does not fit the requirements of callee function.
Typical cases are closing a file handle before passing it, using wrong enumeration value as option parameter, missing required APIs for proper initialization, etc;
\ding{188} \textit{S3 - Inexact Ctrl-Flow Deps}, the control-flow dependencies of a function does not properly implemented.
Typical cases are missing condition checks such as ensuring a pointer is not \texttt{NULL}, missing APIs for setting up execution context, missing APIs for ignoring project internal abort, using incorrect conditions, etc.
\ding{189} \textit{S4 - Improper Resource Cleaning}, the cleaning API such as \texttt{xxxfree} is either missing when required or is used without proper condition checks; 
\ding{190} \textit{S5 - Failure on Common Practices}, the code failed on standard libraries function usage like messing up memory boundary in \texttt{memcpy}, passing read-only buffer to \texttt{mkstemp}, etc.
A detailed mapping with examples is shown in ~\cite{fuzz-drvier-study-website}.

\begin{table}[t]
\caption{Designed Templates for DOCTX-K and UGCTX-K.
}
\label{tab:advanced-strategies-detail}
\resizebox{\linewidth}{!}{
\begin{tabular}{lll}
\toprule
Strategy Type & Prompt Template & Repetition Times\\
\midrule

\rowcolor{black!10}
\begin{tabular}[t]{l}
\multirow{4}{*}{\texttt{DOCTX-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\texttt{\#}\Verb|include "${HEADER_FILE}"| \\
\Verb|${API_DOCUMENTATION}| \\
\Verb|${API_DECLARATION}| \\
\Verb|${NAIVE-K_TEMPLATE}|\\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{4}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\
\midrule

\begin{tabular}[t]{l}
\multirow{6}{*}{\texttt{UGCTX-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\texttt{\#}\Verb|include "${HEADER_FILE}"| \\
\Verb|// @ examples of API usage \ | \\
\Verb|        ${EXAMPLE_FILE_NAME}| \\
\Verb|${EXAMPLE_CODE}| \\
\Verb|${EXAMPLE_API_DECLARATIONS}| \\
\Verb|${NAIVE-K_TEMPLATE}|\\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{6}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\

\bottomrule

\end{tabular}
}
\end{table}


\noindent
\textbf{Failure Distributions} \tab 
Figure~\ref{fig:rq1-failure-taxonomy}b, ~\ref{fig:rq1-failure-taxonomy}c show the root cause distribution.
The plots only distinguish prompt templates since the distributions among models for a given template do not have significant difference.
Therefore, they are omitted here for brevity.
% , a more detailed version can be found at \zhc{add ref to more detailed version}.
Overall, the proportion of the grammatical and semantic errors are quite different in \texttt{NAIVE-K} and \texttt{BACTX-K}.
More than 90\% of failures in \texttt{NAIVE-K} is of grammatical error while only 54\% in \texttt{BACTX-K}.
This indicates that most drivers generated by \texttt{NAIVE-K} failed at early stage.
In \texttt{BACTX-K}, \texttt{G3}, \texttt{G4}, \texttt{S2}, and \texttt{S3} are top four types of root causes and together they take up 93\% of all.
From the prospective of fix, all these four types indicate that there is a need for providing more API specific knowledge to the models.
% , including both grammatical and semantic usages.

Specifically, 50\% of failures in \texttt{BACTX-K} are of \texttt{G3} and \texttt{G4}, showing that \textbf{half of the generated drivers confuse grammatical information of the APIs or types involved}.
Note that many mistakes root in misuse of indirect usages of the target API, such as passing mismatched number of arguments to a dependent API or referencing a non-existing \texttt{struct} member.
Mostly, it is easy to identify and correct these failures since the symptoms directly map to the root causes, \textit{e.g.}, messing up the function or type declarations.
% , is automatically retrievable via code analysis approaches. 
% it corresponds to information required for the fix, \textit{e.g.}, function signatures and type declaration, is relatively easy to be automatically retrieved.
45\% cases are of \texttt{S1-4}, indicating that \textbf{nearly half of the cases failed to satisfy the semantic constraints on API usage}.
Similarly, the errors appear on most APIs.
These failures are challenging for locating root causes and correction usages due to the challenges on handling program semantics.
However, indicating the existence of an error with a roughly relevant usage description may still be helpful.
Few cases (4\% for \texttt{G4}, \texttt{S5}) fail on coding basics or practices while a negligible part (1\% for \texttt{G1}) are of incomplete code.

Overall, the failures cover API usages in various dimensions: from grammatical level detail to semantic level direction, and from target API control flow conditions to dependent APIs' declarations.
Improving this is challengeable since:
\ding{182} \textbf{the involved usage is too broad to be fully put into one prompt}, which may either exceed the token limitation or distract the model;
\ding{183} \textbf{the useful usage for generating one driver cannot be fully predetermined}.
On one hand, models are inherently blackbox and probabilistics, whose mistakes cannot be fully predicted.
On the other hand, there are usually multiple implementation choices for a given API.
% For example, models are inherently blackbox and probabilistic, whose mistakes cannot be fully predicted.
% Besides, for a given API, models usually have multiple feasible choices during implementation.

% 27\% of the \texttt{BACTX-K} cases failed to figure out the correct way to initialize the arguments of API declared in the project.

% Besides, 16\% of failures are caused by missing contextual or control flow dependencies (\texttt{S2} and \texttt{S3}).
% This type of failure is 

% \textbf{xx\% failures are indirectly related with the target API.}
% While more than 90\% of the failures can be reasoned as the lack of detailed usage information, \zhc{xx} of them are indirectly dependent by the target API.

\noindent
% \begin{tcolorbox}[size=title, opacityfill=0.1, nobeforeafter, breakable]
\begin{tcolorbox}[size=title, opacityfill=0.1, breakable]
% \textbf{Key Challenges:}
Most failures are mistakes made on API specific usage, involving various dimensions including the target API and its dependent APIs, from grammatical details to semantic constraints.
The broadness of the involved usages raises great challenges for further improvement.
% including not only usages of the API itself but also its dependencies.
% One major type of error is grammatical, which can be easily identified and corrected, while another is of semantic error that are harder to locate and fix automatically.
\end{tcolorbox}
