\begin{table}[t]
\caption{Two Types of Basic Strategies.
\footnotesize{"\texttt{\$\{XXX\}}" represents a variable inside prompt, which will be instantiated according to the API information.
Specifically, \texttt{\$\{API\_NAME\}} represents the target API name, \texttt{\$\{HEADER\_FILE\}} stands for the header file the API belongs to, and \texttt{\$\{API\_DECALRATION\}} represents the declaration statement of the function found in the header.
\textbf{K} means that the same prompt will be queried \textbf{K} times for one question (API).
}
}
\label{tab:simple-strategies-detail}
\resizebox{\linewidth}{!}{
\begin{tabular}{lll}
\toprule
Strategy Type & Prompt Template & Repetition Times\\
\midrule

\rowcolor{black!10}
\begin{tabular}[t]{l}
\multirow{3}{*}{\texttt{NAIVE-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\Verb|// the following function fuzzes ${API_NAME}| \\
\Verb|extern int LLVMFuzzerTestOneInput|\texttt{(\ } \\
\Verb| const uint8_t *Data, size_t Size|\texttt{) \{} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{3}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\
\midrule

\begin{tabular}[t]{l}
\multirow{3}{*}{\texttt{BACTX-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\texttt{\#}\Verb|include "${HEADER_FILE}"| \\
\Verb|${API_DECLARATION}| \\
\Verb|${NAIVE-K_TEMPLATE}| \\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{3}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\

\bottomrule

\end{tabular}
}
\end{table}

\section{Basic Query Strategies (\textbf{RQ1})}

\subsection{Overall Performance}

\noindent
\textbf{Design of Strategies} \tab 
% Simple strategies can use basic types of information and non-inter\-active conversations.
% Within this scope, we have designed two types of strategies, \textbf{NAIVE-K} and \textbf{BACTX-K}, whose details are illustrated in Table~\ref{tab:simple-strategies-detail}.
As shown in Table~\ref{tab:simple-strategies-detail}, we have designed two types of strategies, \texttt{NAIVE-K} and \texttt{BACTX-K}.
The key difference between these two exists in the design of their prompt templates.
Specifically, \texttt{NAIVE-K} directly asks LLMs to implement the fuzz driver solely based on a specified function name, while \texttt{BACTX-K} provides a basic description of the API.
In the prompt generated by \texttt{BACTX-K}, it first indicates the scope of the task using the \texttt{\#include} statement, then provides the function declaration details, and finally requests implementation.
The declaration statement is extracted from the Abstract Syntax Tree (AST) of the header file, which usually includes both the signature and argument variable names.
% For instance, the declaration of \texttt{parse\_msg} in the \texttt{kamailio} project is "\texttt{extern int parse\_msg(char *const buf,const unsigned int len,struct sip\_msg *const msg);}".
Both strategies have two parameters: 
\ding{182} the LLM model the strategy used;
\ding{183} and the number of times the query is repeated for solving a given question, which is reflected as the letter \texttt{K} in strategies' name.
The design of parameter \ding{183} is motivated by the observation that replies of current LLMs have randomness, and they can even provide quite different answers for two same queries.
Therefore, it is necessary to include it for more comprehensive understanding of strategies' effectiveness.
% We also considered the effect of repetitiveness in the design of these strategies: the \texttt{K} in their names represents the number of times the query is repeated per API.

\noindent
\textbf{Experiment Setting} \tab 
Our experiment covers all 86 questions in the quiz, two latest LLMs in Table~\ref{tab:evaluated_models}.
% For brevity, \texttt{gpt3.5} stands for \texttt{gpt3.5-turbo} in the following.
Since the prompt for one question will be repeatedly queried, we use one round of query to refer to a whole pass of query for all questions.
In total, we evaluated four strategies: \texttt{gpt3.5-NAIVE-K}, \texttt{gpt3.5-BACTX-K}, \texttt{gpt4-NAIVE-K}, and \texttt{gpt4-BA\-CTX-K}, where \texttt{K} = 40.
% These strategies are evaluated on all 86 quiz questions.
% The \textbf{K} has been set as 40 for sufficient evaluation of the repetitiveness, \textit{i.e.}, each generated prompt will be repeatedly queried for 40 times.
% Besides, both two LLM models listed in Table~\ref{tab:evaluated_models}, \textit{i.e.}, \texttt{gpt-3.5-turbo} and \texttt{gpt-4}, are evaluated.

% \subsection{Effectiveness Analysis}

% Figure environment removed

\noindent
\textbf{Quiz-Level Performance} \tab 
% Figure~\ref{fig:results-on-correct-questions} plots the overall performance for the proposed strategies.
% Specifically, 
Figure~\ref{fig:corr-qstn-per-round} illustrates the amount of correct questions per round while the Figure~\ref{fig:stacked-corr-qstn-per-round} details the stacked amount.
In both plots, the line style is used to distinguish the model, \textit{i.e.}, solid/dotted for \texttt{gpt4}/\texttt{gpt3.5}, and the line colors represent the type of prompt templates, \textit{i.e.}, black/red for \texttt{BACTX-K}/\texttt{NAIVE-K}.
% \zhc{add full detail table in the appendix and mention that here}
Apparently, \texttt{gpt4-BACTX-K} shows its performance superiority than the rest three strategies while \texttt{gpt3.5-NAIVE-K} have the lowest performance.
This is intuitive since \texttt{gpt4-BACTX-K} is configured with a model expected to be more powerful and generates more descriptive prompt for the question.
Considering the overall performance, we observed that LLM model, prompt template, and the degree of repetitive query are three independent key factors.
\ding{182} solid lines are almost always higher than the paired dotted lines, which means that, keeping other settings the same, \texttt{gpt4} almost always perform better than \texttt{gpt3.5};
\ding{183} in both two plots, black lines are significantly higher than paired red lines, indicating that, using the same LLM model, prompt template of \textbf{BACTX-K} always have a significant overall performance than \textbf{NAIVE-K}'s;
\ding{184} while the lines in Figure~\ref{fig:corr-qstn-per-round} show a relative stability on one round performance, the stacked results in Figure~\ref{fig:stacked-corr-qstn-per-round} reveal that combining the answers of all rounds can remarkably improve the overall performance, regardless of the used model and prompt template.

\begin{table}[t]
\centering
\caption{Outcome Comparison Between Different K Values.}
\label{tab:cmp-different-k}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lllll}
% \hline
\toprule
 & \texttt{gpt3.5-NAIVE-K} 
 & \texttt{gpt3.5-BACTX-K} 
 & \texttt{gpt4-NAIVE-K} 
 & \texttt{gpt4-BACTX-K} \\
\midrule
\rowcolor{black!10}
R (K=40/K=1) & 3.25 (13/4) & 2.73 (30/11) & 9.33 (28/3) & 1.76 (44/25) \\
P (K=6/K=40) & 76.92\% (10/13) & 86.67\% (26/30) & 64.29\% (18/28) & 88.64\% (39/44) \\
% \hline
\bottomrule
\end{tabular}
}
\label{tab:cmp-different-k}
\end{table}

The observation \ding{184} indicates the usefulness of repetitive queries.
% properly exploiting the randomness in LLM replies can significantly improve the overall performance.
Specifically, as shown in Table~\ref{tab:cmp-different-k}, the ratio of total corrected questions for 40 rounds to one round can be as high as nine.
However, the benefits of the repetitive queries will rapidly decrease after certain rounds of repeated queries, which roughly follow the Pareto Principle~\cite{pareto-principle}.
Specifically, as shown in Table~\ref{tab:cmp-different-k}, all settings have reached nearly or more than 80\% performance at the sixth round comparing with the total outcomes of 40 rounds.
In other words, roughly 80\% of the overall performance of repetitive queries are contributed in the initial 20\% rounds of queries. 
In general cases, a simple stop condition can be proposed, \textit{e.g.}, no performance increase in last X rounds, to balance the benefits and query costs.
% the number of repeat times, \textit{a.k.a.} the value of \textbf{K}, can be adaptively determined according to the past statistics of the new correct answers.

% Figure environment removed

% \begin{table}[t]
% \centering
% \caption{Outcome Comparison Between Different K Values.}
% \label{tab:cmp-different-k}
% \resizebox{1.0\linewidth}{!}{
% \begin{tabular}{llllllll}
% % \hline
% \toprule
% Strategy & S2 & S3 & S4 & S5 & S6 & S7 & S8\\
% \midrule
% \texttt{gpt3.5-NAIVE-K} & -         &-          &-      &-      &-      &-      &0.00\% \\
% \texttt{gpt4-NAIVE-K}   & -         &-          &10.50\%&-      &0.00\% &0.00\% &0.00\% \\
% \texttt{gpt3.5-BACTX-K} & -         &2.50\%     &-      &0.00\% &-      &0.00\% &0.00\% \\
% \texttt{gpt4-BACTX-K}   & 28.41\%   &-          &-      &0.00\% &0.00\% &0.00\% &0.00\% \\
% % \hline
% \bottomrule
% \end{tabular}
% }
% \label{tab:cmp-different-k}
% \end{table}

\noindent
\textbf{Question-Level Performance} \tab 
% Besides discussing the overall performance, we analyzed these strategies on question-level statistics.
Figure~\ref{fig:upset-plot-for-simple-strategies} shows the UpSet plot analyzing the solved questions sets for the above strategies.
Among all 86 questions, 35 of them (40.70\%) have not been solved by any basic strategy, which shows a large space for further improvement.
An interesting observation is that most strategies have uniquely solved questions:
\texttt{gpt3.5-BACTX-K}, \texttt{gpt4-NAIVE-K}, and \texttt{gpt4-BACTX-K} have 2, 5, and 11 uniquely solved questions respectively. 
On the one hand, the finding itself is an evidence that the current most effective strategy \texttt{gpt4-BACTX-K} still cannot outperform the rests in all respects. 
On the other hand, it reminds us the probabilistic nature of the language models.
For instance, though \texttt{gpt4-NAIVE-K} have same prompts/model as \texttt{gpt4-BACTX-K} and contain less descriptive information about the target API, it still can solve 5 questions that \texttt{gpt4-BACTX-K} failed to solve in 40 rounds.
One possible explanation is that, for these questions, the less descriptive prompts generated by \texttt{gpt4-NAIVE-K} can happenly guide the \texttt{gpt4} model produce effective answers.
A supportive finding is that all 5 questions uniquely solved by \texttt{gpt4-NAIVE-K} are of one project and their API usage follow similar design patterns.
Besides, the average query success rate of \texttt{gpt4-BACTX-K} is higher than the rest questions, which indicates that \texttt{gpt4-BACTX-K} not only solves more questions but also solves them more reliably.
% Specifically, the 11 questions uniquely solved by \texttt{gpt4-BACTX-K} cover 6 projects and have an average success rate 28.41\% on query, while the \texttt{gpt3.5-BACTX-K} and \texttt{gpt4-NAIVE-K}'s 2/5 questions cover 1/1 projects, with a success rate of 2.50\%/10.50\%.
% \zhc{add full detail in appendix and add reference here}

% it reminds us the statistical nature of LLM-based methods.
% We cannot debug or reason the LLM-based methods in traditional way for one or two single questions.
% For instance, though \texttt{gpt4-NAIVE-K} have same prompts/model as \texttt{gpt4-BACTX-K} and contain less information about the target API, it still can solve 5 questions that \texttt{gpt4-BACTX-K} failed to solve.

\begin{tcolorbox}[size=title, opacityfill=0.1]
Both LLM model, prompt template, and repetitive query significantly contributes to the overall performance of a strategy.
Specifically, the benefits of repetitive query roughly follows 80/20 rule.
With 40 times of repeat, the best basic strategy \texttt{gpt4-BACTX-K} can correctly answer 40.70\% questions.
\end{tcolorbox}

% Figure environment removed

% \noindent
% \textbf{Boundaries of Effectiveness} \tab 

% \subsection{Boundaries of Effectiveness}
\subsection{Links Between Questions and Performance}

% The difference among LLM replies for the same query is the power source of the effectiveness for repetitive queries.
% Specifically, a second time answer from the LLM may add or remove certain condition checks, change the macro used in the option argument of an API, and even switch the entire API usage comparing with previous answer.
% Our main observation is that LLMs' performance degrades as they are required to consider more API specific usages when writing a driver.
% Before delving into a detailed discussion, we will first outline the minimum API usages required for writing a fuzz driver.
% Then we examine the challenges that LLMs may encounter when attempting to correctly synthesize these usages.
% Finally, evidence are presented to support our findings.
Though the key factors for overall performance of basic strategies have been discussed, the relations between the performance and the questions are still unclear.
% key factor affecting the performance is still unknown.
Our main observation is that LLMs' performance degrades when they are required to consider more API specific usages during driver generation.
To solid the observation, we first explain the challenges LLMs may encounter for accurately synthesizing these usages, then a score system quantifying the degree of the API usage is introduced to prove the association.
% We first hypothesized that LLMs' performance is related to complexity of API usage. 
% Then we examined the
% To test the hypothesis, we propose the evaluation steps as follows: first, we identify the minimum API usage required for fuzz driver creation; second, we examine the potential difficulties LLMs may encounter when accurately synthesizing these usages; and finally, we design a score system to quantify the degree of the minimum API specific usage, and associate it with the performance of fuzz driver generation.

% Although LLMs may employ different mechanisms for generating code compared to the traditional workflow of driver composition, they still must meet certain minimum requirements.
For generating effective fuzz drivers, LLMs should generate code where at least minimum requirements are met.
In other words, they must accurately predict the usage of each API argument as well as the control flow dependencies.
However, this prediction is challenging for LLMs as they cannot validate their predictions against documentation or implementations as humans do during the generation.
It is reasonable to assume that LLMs have learned the language basics and common programming practices due to their training on vast amounts of code.
But the API specific usage, such as the semantic constraints on the argument, cannot be assumed.
On one hand, there may only have limited data about this in the training.
On the other hand, some details can be lost during preprocessing or the learning stage while the accurate generation is required.
% , such as its definitions, implementations, documentation, and usage examples, comprises only a negligible proportion of the training data.
% Furthermore, details may have been lost during preprocessing or the learning stage.
Therefore, the more API usage an LLM needs to predict, the greater the likelihood of errors, particularly for less common usages that do not follow the mainstream design patterns or have special semantic constraints.
Such situations are particularly common in C languages, whose APIs often contain low-level project-specific details.

To prove our reasoning, we designed a score to quantify the degree of the minimum API specific usage for each question.
We first created minimized fuzz drivers for each question, then calculated their scores accordingly.
The process of creating minimal drivers is as follows:
\ding{182} the OSS-Fuzz driver is used as the initial one;
\ding{183} the unnecessary code is removed.
A piece of code is unnecessary if its removal does not affect the driver's effectiveness;
\ding{184} the remaining code will be simplified.
This mainly involves replacing the argument initialization to a simpler form of code.
The level of the simplicity is based on the cases classified in Section~\ref{sec:preliminaries}.
% We classified the initialization to the four cases mentioned in Section~\ref{sec:preliminaries},
% and the ascending order on simplicity for these cases are: \ding{172}, \ding{173}, \ding{174}, and \ding{175}.
The score is calculated as the sum of the following:
\ding{182} the number of unique project APIs used in driver;
\ding{183} the amount of unique common API usage patterns used in driver;
\ding{184} the number of unique identifiers which are not naive values after removing the common API usage code;
\ding{185} the count of branches and loops after removing the common API usage code.
All branches of one condition will be counted as one.
A detailed calculation process with examples is shown in ~\cite{fuzz-drvier-study-website}.

Figure~\ref{fig:succ-rate-plots-per-score} illustrates the relationship between success rates and scores.
Figure~\ref{fig:qstn-succ-rate-per-score} (\emph{resp.}~\ref{fig:query-succ-rate-per-score}) presents the success rate of questions (\emph{resp.} queries) in given score buckets.
Note that each question contains 40 repeated queries in our experiments.
\textbf{Both plots reveal a clear inverse proportion relationship between success rates and scores, irrespective of the used models and prompt templates, thus providing evidence to support our reasoning.}
% There is a stronger relationship in query success rate.
Note that the query success rate shows a stronger relationship.
This is reasonable since:
\ding{182} query success or failure is a more direct and fine-grained metric that reflects question difficulty.
\ding{183} the query success rate is calculated based on 40 repeated queries per question, providing more data and leading to less fluctuation in the overall success rate calculation.
Specifically, when the score is greater than or equal to 7, the query success rate for all strategies is lower than 10\%, and the rate drops to zero when the score falls within the range of $[13, \infty]$.

\begin{tcolorbox}[size=title, opacityfill=0.1]
% \textbf{Key Understanding:}
The performance of LLM-based generation declines significantly when the complexity of API specific usage increases.
% Our score can be used to quantify this association.
\end{tcolorbox}

\subsection{Failure Analysis}
\label{sec:failure-analysis}

% Figure environment removed

%	analyze failures on hard to solve/unsolved questions
% 
%	-> have common blockers/obstacles 
% 
%	-> analyze/classify the root cause of the blockers
%
%	1. nonintuitive API usage
% 
%	2. indirectly dependent API usage
%
%        case studies for repetitive queries, do we need that?

\noindent
\textbf{Overview} \tab 
% what failure analysis can bring
Although we have built general understanding on why the performance drops, it is still unclear of the detailed reasons making the generation fail.
To gain insights, we conducted failure analysis, focusing on hard questions that existing strategies failed to solve.
Specifically, we analyzed the union of unsolved questions (58 questions in total) for \texttt{gpt3.5-BACTX-K} and \texttt{gpt4-BACTX-K}, and only examined the direct reason for the caught error instead of enumerating all potential root causes.
It is often impractical to exhaustively enumerate all potential root causes, as there can exist multiple effective drivers for a given question, resulting in numerous enumeration lists.
Besides, focusing on the direct root cause helps reveal the current main blockers for generation.
In total, we analyzed 9,107 ineffective fuzz drivers.
The workflow of root cause identification follows the semi-automatic approach mentioned in Section~\ref{subsec:evaluation-framework}. 
Specifically, the runtime errors (involves 2,734 drivers) need to be manually debugged and analyzed while the compilation and link errors can be quickly categorized based on the error information provided by compilers.
% \zhc{fix the ungrouped 276!}

\noindent
\textbf{General Taxonomies} \tab 
% results of failure analysis
Figure~\ref{fig:rq1-failure-taxonomy}a details the root cause taxonomy.
In total, there are nine root causes fallen into two general categories:
the grammatical errors reported by compilers during the build stage, and the semantic errors which are abnormal runtime behaviors identified based on the short-term fuzzing results.

Here we briefly explain the nine root causes:
\ding{182} \textit{G1 - Corrupted Code}, the drivers do not contain a complete function of code due to either the token limitation or mismatched brackets;
\ding{183} \textit{G2 - Language Basics Violation}, the code violates the basic programming language rules such as variable redefinition, parentheses mismatch, incomplete expressions, etc;
\ding{184} \textit{G3 - Non-Existing Identifier}, the code refers to non-existing things such as header files, macros, global variables, members of a struct, etc;
\ding{185} \textit{G4 - Type Error}. One main subcategory here is the code passes mismatched number of arguments to a function. And the rest are either unsupported type conversions or unsupported operations on specific types such as calling non-callable object, assigning \texttt{void} to a variable, allocating an incomplete \texttt{struct}, etc;
\ding{186} \textit{S1 - Incorrect Input Arrangement}, the input size check either is missed when required or contains an incorrect condition;
\ding{187} \textit{S2 - Misinitialized Function Args}, the incorrect initialization of an argument which causes its value or inner status does not fit the requirements of the called function. Typical cases are closing a file handle before passing it, using wrong enumeration value as option parameter, missing required APIs for the argument initialization, etc;
\ding{188} \textit{S3 - Inexact Ctrl-Flow Deps}, the control-flow dependencies of a function does not properly implemented. Typical cases are missing condition checks such as ensuring a pointer is not \texttt{NULL}, missing APIs for setting up execution context, missing APIs for ignoring project internal abort, using incorrect conditions, etc.
\ding{189} \textit{S4 - Improper Resource Cleaning}, the cleaning API such as \texttt{xxxfree} is either missing when required or is used without proper condition checks; 
\ding{190} \textit{S5 - Failure on Common Practices}, the code failed on using functions from standard libraries such as messing up memory operations when using \texttt{memcpy}, passing read-only buffer to \texttt{mkstemp}, etc.
A detailed mapping with examples is shown in ~\cite{fuzz-drvier-study-website}.

\noindent
\textbf{Failure Distributions} \tab 
Figure~\ref{fig:rq1-failure-taxonomy}b, ~\ref{fig:rq1-failure-taxonomy}c show the distribution of root causes.
The plots do not distinguish models but considers prompt templates.
This is because, for a given prompt template, the distributions among models do not have significant difference.
Therefore, they are omitted here for brevity.
% , a more detailed version can be found at \zhc{add ref to more detailed version}.
Overall, the proportion of the grammatical and semantic errors are quite different in \texttt{NAIVE-K} and \texttt{BACTX-K}.
More than 90\% of failures in \texttt{NAIVE-K} is of grammatical errors while only 54\% in \texttt{BACTX-K}.
This indicates that most drivers generated by \texttt{NAIVE-K} failed at early stage.
In \texttt{BACTX-K}, \texttt{G3}, \texttt{G4}, \texttt{S2}, and \texttt{S3} are top four types of root causes and together they take up 93\% of all.
From the prospective of fix, all these four types indicate that there is a need for providing more project/API specific knowledge to the models.
% , including both grammatical and semantic usages.

Specifically, 50\% of failures in \texttt{BACTX-K} are of \texttt{G3} and \texttt{G4}, which represents that \textbf{half of the generated drivers confuse grammatical information of the project APIs or types involved}.
Note that many mistakes root in misuse of indirect usages of the target API, such as passing mismatched number of arguments to a dependent API or referencing a non-existing \texttt{struct} member.
Mostly, these failures are easy to be identified and corrected since its symptoms directly map to the root causes and the correct usage, \textit{e.g.}, function or type declarations, is automatically retrievable via code analysis approaches. 
% it corresponds to information required for the fix, \textit{e.g.}, function signatures and type declaration, is relatively easy to be automatically retrieved.
45\% cases are of \texttt{S1}, \texttt{S2}, \texttt{S3}, \texttt{S4}, indicating that \textbf{nearly half of the cases failed to satisfy the semantic constraints on API usage}.
Similarly, the errors spread among all involved APIs.
However, this type of failures are significantly harder for locating root causes and correcting usages due to the challenges on handling program semantics.
However, indicating the existence of an error with a roughly relevant usage description may still be helpful.
Few cases (4\% for \texttt{G4} and \texttt{S5}) fail on common practices or coding basics while a negligible part (1\% for \texttt{G1}) of failures are caused by generating incomplete code.

Overall, the failures cover API usages in various dimensions, \textit{e.g.}, from the grammatical level detail to the semantic level direction, and from the control flow conditions of the target API to the declarations of the dependent APIs.
Improving this is challengeable since:
\ding{182} \textbf{the involved usage is too broad to be fully put into one prompt}, which may either exceeds the token limitation or distracts the models;
\ding{183} \textbf{the useful usages for one driver generation cannot be fully predetermined}.
Specifically, models are blackbox and probabilistic, which are hard to reason the mistakes it may encounter.
Besides, for one given API, models usually have various feasible choices during implementation.

% 27\% of the \texttt{BACTX-K} cases failed to figure out the correct way to initialize the arguments of API declared in the project.

% Besides, 16\% of failures are caused by missing contextual or control flow dependencies (\texttt{S2} and \texttt{S3}).
% This type of failure is 

% \textbf{xx\% failures are indirectly related with the target API.}
% While more than 90\% of the failures can be reasoned as the lack of detailed usage information, \zhc{xx} of them are indirectly dependent by the target API.

\begin{tcolorbox}[size=title, opacityfill=0.1]
% \textbf{Key Challenges:}
Most failures are mistakes made on the API specific usage, involving various dimensions including the target API usage and its dependents', the grammatical details and the semantic constraints.
The broadness of the involved usages raises great challenges for further improvement.
% including not only usages of the API itself but also its dependencies.
% One major type of error is grammatical, which can be easily identified and corrected, while another is of semantic error that are harder to locate and fix automatically.
\end{tcolorbox}
