\section{Methodology}

% Figure environment removed

% This section is for: How we conduct our study to answer these RQs?
% Problem scope declaration: effective fuzz driver generation, ...

\subsection{Overview of Study}

\noindent
\textbf{Overall Design} \tab 
Figure~\ref{fig:overview-for-study} shows the overall workflow of this study.
To understand the effectiveness of different LLM-based fuzz driver generation strategies, we first constructed a quiz, which contains a set of evaluation questions and the effectiveness criteria.
Each question requires the LLMs to generate a fuzz driver according to a given API and the effectiveness of the generated driver will be evaluated based on the criteria.
% An API is qualified if its project is significant, \textit{i.e.}, tested by OSS-Fuzz, and it is the core API inside the existing fuzz drivers.
% The effectiveness criteria of an API are the conditions or checks to distinguish ineffective drivers and effective ones. 
Then we built an evaluation framework to maximize the automation of the evaluation.
% The framework will add general contents of the prompts, launch LLM queries, validate the effectiveness of replies, and classify the failed validation.
Upon these, we designed and evaluated the effectiveness of different query strategies from basic to enhanced (the first two \textbf{RQs}).
% the first two RQs are studied by comparing and analyzing the evaluation results of different query strategies.
In \textbf{RQ1}, we designed and explored the effectiveness of basic strategies which use fundamental API information and have simple interactions with LLMs;
In \textbf{RQ2}, enhanced strategies which leverage extended API usage information and interactive queries are studied.
Necessary in-depth analysis were conducted to explain the benefits and limitations of these strategies.
Lastly, in \textbf{RQ3}, we compared LLM-generated fuzz drivers with OSS-Fuzz drivers to understand their positions, advantages, and disadvantages.
% The comparison involves the static metrics of the code such as the number of unique APIs and dynamic metrics including the testing coverage and unique crashes.

\noindent
\textbf{Evaluated Language Models} \tab 
As shown in Table~\ref{tab:evaluated_models}, two state-of-the-art language models of OpenAI have been studied~\cite{openai-models}.
% All the LLM-generated fuzz drivers evaluated in this study are retrieved via ChatGPT web interfaces~\cite{chatgpt-website} (release version 23 Mar 2023) based on \texttt{chatgpt-wrapper}\cite{chatgpt-wrapper} v0.7.1 (\texttt{dabe72101b}).
All the LLM-generated fuzz drivers evaluated in this study are retrieved via \texttt{chatgpt-wrapper}~\cite{chatgpt-wrapper} v0.7.1 (\texttt{dabe72101b}).
The parameters of the models are kept same as the default values in the ChatGPT official website~\cite{chatgpt-default-config}.
% , \textit{e.g.}, temperature is set as 0.9 and top\_p is 1.

% Figure environment removed

\begin{table}[!t]
\centering
\caption{LLMs Evaluated in This Work.}
\label{tab:evaluated_models}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{lllll}
\toprule
 Model & Abbr & ChatGPT Version & Max Tokens & Training Data \tabularnewline
\midrule 
\rowcolor{black!10}
gpt3.5-turbo & \texttt{gpt3.5} & 23 Mar 2023 & 4,096 tokens & Up to Sep 2021 \tabularnewline
gpt4 & \texttt{gpt4} & 23 Mar 2023 & 8,192 tokens & Up to Sep 2021 \tabularnewline
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Quiz Construction}
\label{sec:quiz-construction}

\noindent
\textbf{Qualified APIs Collection} \tab 
% What is a question for fuzz driver generation
The question in quiz is designed as generating fuzz drivers for one given API.
This is based on the intuition that any fuzz driver can be divided into one or more simpler fuzz drivers targeting different APIs.
In other words, complicate, multi-purpose fuzz drivers are essentially the combination code which fuzzes multiple API targets simultaneously.
Therefore, as the first practical evaluation on LLM-based fuzz driver generation, we focused on the more fundamental scenario.

% why selecting an appropriate question is not naive
Not all APIs are suitable to be set as questions.
Naively setting all APIs as questions will lead to the creation of meaningless or confusing questions which influences the evaluation result.
Specifically, some APIs, such as \texttt{void libxxx\_init(void)}, are meaningless fuzz targets since the code executed behind the APIs can not be affected by any input data.
Some APIs can only be supplemental APIs rather than the main fuzzing target due to the nature of their functionalities.
For example, given two APIs \texttt{object *parse\_from\_str(char *input)} and \texttt{void free\_object(object *obj)}, feeding the mutated data into \texttt{input} is apparently a better choice than feeding a meaningless pointer to the \texttt{obj} argument.
However, calling the latter API when fuzzing the former is meaningful since 
\ding{182} it may uncover the hidden bug when the former does not correctly initialize the \texttt{object *} by freeing the members of \texttt{object};
\ding{183} it releases the resources allocated in this iteration of fuzzing, which prevents the falsely reported memory leak issue.

% how did we select qualified APIs
To build a high quality quiz, we need to collect a set of qualified APIs which are both representative and suitable to be set as fuzzing targets.
Our intuition is to collect core APIs of the existing fuzz drivers from popular projects fuzzed by OSS-Fuzz.
A driver's core APIs are located based on the following criteria:
\ding{182} they are the target APIs explicitly pointed out by the author in its driver file name or the code comments, \textit{e.g.}, \texttt{dns\_name\_fromwire} API is the core API of driver \textit{dns\_name\_fromwire.c};
\ding{183} otherwise, we pick the basic APIs as the core rather than the supplemental ones.
For example, we pick the former between parse and use/free APIs.
Some fuzz drivers are composite drivers which fuzzes multiple APIs simultaneously, we identified multiple core APIs from them.
Specifically, we randomly selected 30 projects from OSS-Fuzz (commit hash \texttt{135b000926}) C projects, manually extracted 86 core APIs from 51 fuzz drivers.
More details are listed in \appe~\ref{sec:quiz-questions-detail}.

\noindent
\textbf{Effectiveness Criteria Establishment} \tab 
% What is an effective fuzz driver
% general validation method is hard
% can greatly influence the evaluation result
% our approach
% -> semi-automatic criteria: automatic + manually built checkers
% An effective fuzz driver represents the drivers which have correct API usage and produce no false positives.
% Precisely validating the effectiveness of fuzz drivers is crucial for evaluating fuzz driver generation methods.
% However, it is hard to propose a general validation technique since the effectiveness
% general validation techniques do not work well due to the diverse semantics on the API usages.
An effective fuzz driver should use the API effectively while producing no false positives.
Determining the effectiveness of a fuzz driver is important but challengeable since it requires the correct classification of its false positives (bugs caused by the driver code) and negatives (ineffective usage).
To validate precisely, we adopted a semi-automatic validation method.
As shown in Figure~\ref{fig:validation-checker}, it has four checkers.
The first two checks require manual configuration for each project while the rest two need to be configured per API, \textit{a.k.a.} per question in our quiz.
The first check examines the grammatical correctness of a driver using compiler, while the second one checks the existence of abnormal fuzzing behaviours via short-term fuzzing.
It checks whether the driver reports any bugs, \textit{i.e.}, crashes or timeout, or does not have any coverage progress in a short time period with a default fuzzing setup (empty seed, no dictionary, etc).
The intuition behind this is that, under a default setup, neither the zero coverage progress nor the quick identification of bugs are common cases.
In our study, we set the time period as one minute.
Obviously, the second check can make both false positive and negative decisions.
The rest two checks are introduced to refine the check results.
By fuzzing the OSS-Fuzz provided drivers, we collected the signatures of real bugs that can be found quickly.
We filtered these bugs in the third check.
Lastly, based on the examination of the API usage, we summarized semantic constraints that a correct driver should obey on a given API.
These constraints are written as tests to the fuzz drivers.
For example, assuming an API require the mutated input be stored in a file, one semantic test will be hook the API and check whether the filename argument represents a valid file and contains the mutated input.
Our validation framework can taxonomize the failures (Section~\ref{subsec:evaluation-framework}), which can help us iteratively improve the inexact checks when validating more fuzz drivers.

\subsection{Evaluation Framework}
\label{subsec:evaluation-framework}

% what the evaluation framework is and how to use
To boost the evaluation of a large amount of fuzz drivers, we developed a framework to maximize the automation.
As shown in Figure~\ref{fig:overview-for-study}, the framework takes a prompt generated by a query strategy as input, and outputs the classified validation result.
It also includes a website to ease the manual analysis.
In total, the framework is written in 9,342/1,542 lines of Python/HTML, and 3,857 lines of JSON, YAML, and Bash scripts.
Instead of illustrating every detail, we discussed significant design choices as follows.

\noindent
\textbf{Prompt Standardization} \tab 
% controlling the output
% extract code part only
% preparing headers
% What are already assumed to be correctly provided, and how we postprocessing the answers
Unless specifically mentioned in the strategy, a sentence is inserted at the beginning of the prompt to help standardize the output format of LLMs.
The sentence is listed as follows (Python literal), which instructs LLMs to reply in code.
\begin{tcolorbox}[size=title, opacityfill=0.1]
\small  \texttt{// The following is a fuzz driver written in C language, complete the implementation. Output the continued code in reply only.{\textbackslash}n{\textbackslash}n}
\end{tcolorbox}

\noindent
\textbf{LLM Query} \tab 
% token limitation, how many left for query and how many for answer
% perhaps discuss this in iteration section is better
% one query one conversation
% restful deisgn?
LLMs limit the maximum token numbers for the sum of tokens in query and answer.
We set 6,000 tokens for the prompt of \texttt{gpt4}, and 3,600 tokens for \texttt{gpt3.5}.
Prompts exceeding the limit need to be adjusted in strategy.

\noindent
\textbf{Effectiveness Validation} \tab 
Due to the non-deterministic nature of the LLM, some replies still can contain both code and text.
We summarized the patterns and extracted the code in replies.
Besides, to focus on evaluating the effectiveness of the generated code, our framework automatically configures the required header files and build options for a driver.
This is done by manually configuring the rules of header inclusion, and build options for the selected 30 projects, which guarantees all compilation and link errors are caused by the incorrect code rather than the unsuitable configurations.
Besides, each validation of a fuzz driver is in a fresh, isolated container, excluding the environment disturbances to the results.

\noindent
\textbf{Failure Taxonomy} \tab 
% automatic taxonomy
% semi-automatically taxonomy
The taxonomy of root causes for ineffective results also follows a semi-automatic approach.
We first did a rough categorization of the compilation, link, and fuzz errors based on the string patterns of the errors outputted by \texttt{clang}~\cite{clang-lex-diagnostics, clang-parse-diagnostics, clang-sema-diagnostics} and \texttt{libfuzzer}~\cite{libfuzzer, sanitizers}.
Then for each API, we manually identified the root causes per category and mapped each category into the final categories. 
The manual mappings were written in code and will be iteratively improved once we found a new unclassified failure. 
