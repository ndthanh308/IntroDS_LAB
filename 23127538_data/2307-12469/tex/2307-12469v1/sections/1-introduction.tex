\section{Introduction}

% \zhc{What is fuzz driver generation}

Fuzz testing (aka fuzzing) has become the standard approach for discovering zero-day vulnerabilities.
Since this technique requires a directly executable target program, fuzz drivers have become a necessary component of API fuzzing.
Essentially, a fuzz driver is a piece of code responsible for accepting mutated input from fuzzers and executing the APIs accordingly.
An effective fuzz driver must ensure correctness and robustness in API usage, as any incorrect or unsound API usage may result in extensive false positives or negatives during fuzzing.
This can waste testing resources or require additional human validation efforts.
Due to the high standard required, fuzz drivers have traditionally been composed by human experts, making the process labor-intensive and time-consuming.
For instance, OSS-Fuzz~\cite{oss-fuzz}, the largest continuous fuzzing framework for open-source projects, maintains thousands of fuzz drivers written by hundreds of contributors over the past seven years.

% Fuzz testing \lyt{(aka fuzzing)} has emerged as the de-facto standard for discovering zero-day vulnerabilities.
% Since applying fuzzing requires a directly executable target program, fuzz drivers are necessary components of API fuzzing.
% Essentially, it is a piece of code responsible for accepting mutated input from fuzzers and executing the APIs accordingly.
% An effective fuzzing driver must ensure both correctness and robustness in the API usage since any incorrect or unsound API usage may lead to extensive false positives or negatives during fuzzing, wasting the testing resources or incurring extra human validation efforts.
% Due to the high standard, fuzz drivers used to be composed by human experts, which is laber-intensive and time-consuming.
% For example, OSS-Fuzz~\cite{oss-fuzz}, the largest continuous fuzzing framework for open source projects, maintains thousands of fuzz drivers written by hundreds of contributors in the past seven years.

% \zhc{What are SOTA methods and their limitations}
% program analysis assisted, rule-based generation/transformation.

% Most of these works follow a common methodology, which involves generating fuzz drivers based on the API usage existed in consumer programs, \textit{i.e.,} programs containing code that uses these APIs.
% For instance, by abstracting the API usage as specific models such as trees~\cite{apicraft}, graphs~\cite{fuzzgen}, and automatons~\cite{rubick}, several works propose program analysis-based methods to learn the usage models from consumer programs and conduct model-based driver synthesis.
% In addition, a recent work~\cite{utopia} emphasizes that unit tests are high quality consumer programs and proposes techniques to convert existing unit tests to fuzz drivers.
% Though these approaches can produce effective fuzz drivers, their heavy requirements on the quality of the consumer programs, \textit{i.e.,} the consumers must contain complete API usage and are statically/dynamically analyzable, limit their generality.
% Furthermore, code synthesized by many works lacks human readability and maintainability, which can limit their practical application.

Recently, various works~\cite{fuzzgen, fudge, apicraft, winnie, intelligen, rubick, utopia} have focused on developing automatic approaches to generate fuzz drivers.
Most works follow a methodology which generates fuzz drivers by learning or converting API usages from existing example programs (aka consumer programs).
Though these approaches can produce effective drivers, they rely on the consumer programs heavily, \textit{e.g.}, the consumers must contain complete API usage and are statically/dynamically analyzable, which limits their practicality.

% \zhc{LLM-based generation for fuzz drivers is important, promising but under-studied}

Generative LLMs (Large Language Models) have gained significant attention for their ability in code generation tasks~\cite{chatgpt,openai2023gpt4}.
LLMs are highly capable language models trained on vast quantities of text and code, and they can be utilized in a conversational workflow, where natural language based queries are posed and answered.
LLM-based fuzz driver generation is an attractive direction.
First, the generality of the generation approaches is promisingly high.
This is because the consumer programs are no longer prerequisites but auxiliaries in generation.
Besides, benefited by the multilingual support of LLMs, utilizing various sources of information such as documentation for further improvement is more convenient.
Moreover, LLM-generated code is human-friendly.
While some research efforts have been devoted to LLM-based code generation tasks~\cite{arxiv-adaptive-unit-test-gen, zhanglingming-llm-are-zero-shot-fuzzers, jiang2023selfplanning,pearce2022asleep,jain2022jigsaw}, none of them have addressed the fundamental issues concerning the application of LLMs to fuzz driver generation such as the effects, challenges, and techniques.
% such as whether and to what extent LLMs can generate effective fuzz drivers, and the challenges and strategies for improvement, and the pros and cons of the generated fuzz drivers compared to those practically used.
% \zhc{perhaps the first half of this paragraph can be shrinked, add more reasoning why existing efforts cannot answer?}

% \zhc{How we do to fill the gap}

% Figure environment removed

To address this gap, we conducted an empirical study.
Our primary goal is to understand the basics towards generating "\textit{more}" effective fuzz drivers, rather than generating "\textit{more effective}" fuzz drivers.
We believe that creating effective drivers for more targets is a more fundamental issue than improving the existing ones.
% To address this issue, we propose and evaluate various query strategies that can generate fuzz drivers for a specific API.
The issue is explored by proposing and evaluating various query strategies that generate fuzz drivers for a specific API.
For a systematic study, we first categorized the strategies as two types: basic and enhanced.
Then we studied them from basic to enhanced.
As shown in Figure~\ref{fig:coordinates-for-study}, the categorization is based on the used information type and conversation type of a strategy.
Basic strategies only query with fundamental API information, such as header file name and function declaration, and communicate with the model in a one-and-done manner.
Enhanced strategies can add extended information such as the API documentation and example code snippets into the query prompt, and can interactively communicate with the model to iteratively improve the generated drivers.
When proposing specific strategies, we mainly consider their prompt design and the number of times a query is repeated.
After investigating the strategies' effectiveness and characteristics, we compared the generated drivers with OSS-Fuzz drivers to obtain practical implications.
% positions, advantages, and disadvantages of LLM-generated drivers.

% We started from examining basic query strategies (\textbf{RQ1}), then explored the complex ones (\textbf{RQ2}).
% Lastly, we compared the generated drivers with OSS-Fuzz drivers for further understanding (\textbf{RQ3}).

% Since many factors influencing the evaluation of a strategy, we first divided the strategies as basic and enhanced strategies, and studied them from basic to enhanced.
% As shown in Figure~\ref{fig:coordinates-for-study}, the categorization is based on the type of information used in the query and the type of the conversation.
% Basic strategies only contain fundamental information of the target in the query and communicates with the models 

% established a complexity standard categorizing the strategies and examined them from basic to enhanced.
% the type and the organization of information used in the query, the number of iteration and repetition of the query, we established a complexity standard and examined the strategies from basic to extended.
% including what information will be used and the way to organize them in the prompt, the number of times a query is repeated, and the conversation is interactive or not, 
% Figure~\ref{fig:coordinates-for-study} 

% This approach is significant because simplicity offers greater generality.
% Understanding the limits of simpler strategies can inform the design and utilization of more complex ones.
% To investigate the effectiveness of LLM in generating effective fuzz drivers, we first adopt the basic strategy that employs fundamental information about the target API, such as function declaration and header file name, to construct prompts for instructing the LLM.
% We prioritize this strategy because (1) \textit{Necessity}: fundamental information is necessary to explain the target API clearly and ensure the basic quality of the query. (2) \textit{Generality}: fundamental information is widely accessible, ensuring the generality of the task.

% After analyzing the results of the basic strategy, we can identify the challenges that LLM faces in generating effective fuzz drivers.
% This understanding helps us optimize the technique of using LLM to generate fuzz drivers.
% Based on these challenges, we propose advanced strategies that improve on two dimensions, which involve more types of information and interactive times of conversation.
% These advanced strategies can address the ineffectiveness of LLM in handling some complex tasks.

% To explain the strategies more clearly, we defined both basic strategy and advanced strategy as shown in Figure~\ref{fig:coordinates-for-study}.
% Basic strategy is the one that utilizes \textbf{fundamental} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}and \textbf{Non-Interactive} conversations.
% Otherwise, it is advanced strategy.
% As defined in Figure~\ref{fig:coordinates-for-study}, a strategy is simple if it only uses \textbf{Basic} information to compose prompts\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.} and \textbf{Non-Interactive} conversations.
% Otherwise, it is complex.
% the complexity of query strategies is measured in two dimensions: \ding{182} the type of information used for prompt composition\footnote{Following the convention, we use prompt to represent the content of the query\zhc{add reference and may introduce prompt engineering here}.}, \ding{183} and the type of the conversation.
% The \textbf{fundamental} type covers widely accessible information including function declaration and the header file name an API belongs to, while \textbf{Extended} type additionally includes API usage information which cannot be assumed generally accessible such as documentation, implementation code, and usage example code.
% The difference between \textbf{Non-Interactive} and \textbf{Interactive} conversations is whether the intermediate information produced during the conversation is utilized for improving the generation.
% \textbf{Interactive} coversations may be multi-round while the \textbf{Non-Interactive} ones' are one-round.

% \zhc{Our RQs}
Our study targets on answering three research questions:
\begin{itemize}
    \item \textbf{RQ1}\tab Can LLM generate effective fuzz drivers using the basic query strategies, and to what extent? What are the challenges and implications?
    % If there exists improvement space, what are the fundamental challenges?
    \item \textbf{RQ2}\tab Can LLM-based generation be further improved by enhanced query strategies? What are their characteristics?
    \item \textbf{RQ3}\tab How LLM-generated drivers perform comparing with OSS-Fuzz drivers in code and fuzzing metrics? 
    % What are the implications?
    % What are the pros and cons comparing LLM-generated drivers with the practically used ones? What are the implications?
\end{itemize}

% \zhc{What we practically do for answering the RQs}
To assess the effectiveness of the generated fuzz drivers, we developed a quiz consisting of 86 questions with precise validation criteria.
Each question asks the models to generate drivers for a given API and the criteria checks whether the generated driver is a qualified answer or not.
The APIs used in the questions were collected by analyzing 51 fuzz drivers from 30 popular C projects.
We aimed at C projects since it is a typical memory-unsafe language where fuzz testing has shown great effectiveness in finding vulnerabilities and bugs. 
Precisely validating the effectiveness of fuzz drivers on a large scale is difficult, as it requires identifying false positives (bugs caused by drivers) and false negatives (ineffective but non-crash usage).
To address this, our criteria consist of two parts: general checks by the compiler and dynamic testing, and API-specific tests manually written for semantic validation.
We also built a framework that can validate the generated drivers and provide taxonomized results, reducing the manual efforts required for evaluation.
In total, 36,506 fuzz drivers generated by five strategies are evaluated.

% from 51 fuzz drivers covering 30 popular C projects 
% The involved APIs in quiz are collected from 51 fuzz drivers covering 30 popular C projects fuzzed by OSS-Fuzz.
% And the criteria is a semi-automatic approach containing API specific checks for precise effectiveness validation.
% Upon these, an evaluation framework is built to maximize the automation in driver evaluation process.

% designed a quiz consisting of fuzz driver generation questions and an evaluation standard.
% Each question in the quiz required writing a fuzz driver for an API qualified for fuzz testing.
% We identified the qualified APIs from popular open-source projects that are suitable for fuzzing by extracting the key APIs from the fuzz drivers of projects fuzzed by OSS-Fuzz repositories.
% The key APIs were either explicitly specified according to the author's comments and driver file names or picked according to the complexity score calculated using OSS-Fuzz Introspector~\cite{fuzz-introspector}.\zhc{check here later}
% The quiz comprised \zhc{X} questions from \zhc{Y C/C++} projects in total.
% An effective fuzz driver was defined as one that, under empty input seed, it can be built and fuzzed for a period of time (1 minute in our study) without discovering false positive bugs.
% As it is challenging to automatically determine whether a fuzz driver is effective or not, we wrote specific tests for some questions, filtered the frequently found true bugs based on their crash stack signatures, and used them to correct evaluation results.
% Besides, we built an evaluation framework based on the quiz and evaluation standard to reduce manual efforts required for evaluation.
% With these tools, we conducted our study on the research questions outlined above.

Overall, the combined results of all strategies show that they can pass 91\% (78/86) of the questions.
While this result indicates the strong potential of LLM-based driver generation, it does not necessarily represent its practicality.
This is because the contribution of the validation criteria in filtering out ineffective drivers cannot be ignored.
Without precise validation, the results may contain many ineffective fuzz drivers, which seriously hinders the practical application.
Therefore, further improvements are needed to address these challenges and maximize the practicality.
% of LLM-based fuzz driver generation.

% Overall, the union results of all strategies can pass xx\% (xx/86) questions.
% However, this result only proves the strong potential of LLM-based driver generation but cannot represents its practicality.
% This is because the contribution of the validation criteria filtering out the ineffective drivers cannot be ignored.
% Without a precise validation, the results can contain many ineffective fuzz drivers, hindering its practical application.

% The interpretation of the result is two-fold.
% On one hand, \textbf{the high correct rate shows high potential of LLM-based fuzz driver generation}.
% On the other hand, the high correct rate is based on the results filtered from xxx queries.
% Since the validation criteria contain manually collected API specific checks, how to precisely filtering the ineffective drivers is a major obstacle for the practical application of LLM-based approaches.
% \textbf{Without a carefully developed validation criteria for the given targets, LLM-based approaches can have spam issues}.
% \zhc{The evaluation results}

Regarding specific strategies, the basic approach has shown decent results (53\%, 46/86), particularly given its low technical complexity and limited information requirements about the target API.
However, its performance significantly declines as the complexity of the API-specific usage increases.
For questions it cannot answer, adding example code snippets can significantly help (83\%, 71/86).
However, the quality of the snippets is critical.
Through our research, we have identified "test/example files" and "internal code files of the target/variant projects" as two high-quality sources that contribute most.
In addition to example code snippets, iterative queries can also help address complex questions (solved 4 more questions), leveraging comprehensive usage information and a step-by-step problem-solving approach.
However, the approach suffers from high search costs and increased strategy complexity.

% For specific strategies, the basic ones have shown decent results (\zhc{xx\%}), particularly when considering the fact that they require few information about the target and have low technical complexity.
% However, its performance will decline significantly when the complexity of the API specific usage increases.
% For the questions they cannot figure out, adding example code snippets and query iteratively can help significantly (additionally solves \zhc{xx\%}).
% The example code snippets helps by directly providing the usage to model, which highly depends on the quality of the snippets.
% We identified that "test/example file" and "internal code files of the target/variant projects" are two high quality sources (contributes \%);
% The iterative queries can solve complex questions due to its comprehensive utilization of the usage information and the step-by-step problem solving approach.
% However, it suffers from high search costs and strategy complexity.

% most failures are mistakes made on API specific usages.
% The involved usages are broad, covering various dimensions from the target API usage to its dependent APIs', from the grammatical errors to semantic errors, raising challenges for improvement.
% For enhanced strategies:
% \ding{182} adding API documentation in prompt can slightly improve overall performance (xx\%) due to the limited usage description it contains;
% \ding{183} example code snippets can greatly enhance the overall performance (xx\%).
% Besides, "test/example file", "internal code files of the target/variant projects" are high quality sources for code snippets (contributes \%);
% \ding{184} query iteratively can solve questions that non-iterative strategies cannot solve.
% The superiority comes from its more comprehensive utilization of the usage information and the step-by-step problem solving approach.

Comparing with OSS-Fuzz drivers, LLM-generated drivers provide competent fuzzing outcomes.
However, as previously discussed, practical validation methods must be adopted to prevent ineffective testing and false alarms.
Besides, manually written fuzz drivers offer unique advantages that can guide the improvement of LLM-generated drivers, such as the ability to include semantic oracles that detect logical bugs.

% By comparing with OSS-Fuzz drivers, we found that the LLM-generated drivers can provide competent fuzzing outcomes.
% However, as discussed above, practical validation methods should be adopted for preventing ineffective testing or the massive false alarms.
% Besides, manually written fuzz drivers showed some unique advantages directing the improvement of LLM-generated, such as the inclusion of semantic oracles which can detect logical bugs.

% \ding{182} LLMs tend to produce fuzz drivers with minimal API usages while adding example snippets improves the contained usages;
% \ding{183} OSS-Fuzz drivers contain oracles which can check semantic bugs while LLM-generated do not have any;
% \ding{184} LLM-generated drivers can reach comparable fuzzing outcomes as OSS-Fuzz drivers.
% However, as discussed above, how to practically pick effective drivers is the major challenge for its large scale application.

% the interpretation of the result is two-fold
% 1. it shows strong potential 
% 2. the validation criteria is still a major obstacle for its practical application, which requires appropriate approaches/methods
% specifically, 
%  basic strategies performance, characteristics, etc
%  enhanced strategies and their characteristics
%  the metrics comparison conclusion

% \zhc{Contribution}
In summary, our contributions are:
\begin{itemize}
    \item we conducted the first in-depth study on the effectiveness of LLM-based fuzz driver generation, which demonstrates the potential on this direction and highlights the practical challenges;
    \item we designed and implemented five query strategies for fuzz driver generation.
    Furthermore, we analyzed their performance, characteristics and limitations;
    \item we built the first fuzz driver generation quiz along with a semi-automatic evaluation framework, which facilitates the following researches.
\end{itemize}

To facilitate the community, we will open source our quiz, evaluation framework, and the data involved in the study at~\cite{fuzz-drvier-study-website}.
