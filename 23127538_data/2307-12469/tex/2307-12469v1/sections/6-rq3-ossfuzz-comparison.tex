% Figure environment removed

\section{OSS-Fuzz Driver Comparison (\textbf{RQ3})}

\noindent
\textbf{Comparison Overview}
\tab
% three groups:
% BACTX
% UGCTX
% ITER
% In previous sections, we analyzed the effectiveness of different query strategies.
% Most fuzz drivers in OSS-Fuzz are manually written, applied practically on industrial projects, and continuously maintained and improved for years.
We designed experiments to compare LLM-generated drivers with OSS-Fuzz drivers to understand their respective positions, advantages, and disadvantages.
% strategies covered
The comparison includes four types of query strategies: \texttt{BACTX-K}, \texttt{DOCTX-K}, \texttt{UGCTX-K}, and \texttt{ITER-K}, where the \texttt{K} remains the same as in the previous section.
Note that each strategy can only provide effective fuzz drivers for a part of the questions in the quiz.
% questions covered
To investigate the main features of each strategy, the comparison focuses on each strategy's good performance questions.
The good performance question for a strategy is the one that at least one effective fuzz driver has been generated on all evaluated LLMs in previous evaluation.
In total, 72 questions are evaluated, with a detailed breakdown of 28/22/35/9 questions for \texttt{BACTX-K}, \texttt{DOCTX-K}, \texttt{UGCTX-K}, and \texttt{ITER-K}.
% Details of the question groups are listed in \appe~\ref{rq3-question-group-detail}.
% \zhc{or website?}

% prepare the compared driver
Since there can have multiple drivers for one question from both evaluated strategies and OSS-Fuzz, we merged them as one driver for ease of comparison.
To achieve this, we used a simple wrapper code inspired by FuzzGen's driver coalescing method~\cite{fuzzgen}.
The wrapper is mainly a switch statement calling the logic of each original driver based on the first byte of the input data.
During each fuzzing iteration, only the logic of one original driver is executed.
By using this approach, fuzzer can automatically control the exploration of the logics from multiple drivers using its seed scheduling functionality.
% metrics covered
The comparisons cover both code and fuzzing metrics including the number of used APIs, oracles, coverage and crashes.
% The comparisons covers both static and dynamic metrics including the number of used APIs, the number of oracles, the fuzzing coverage, and the number of  crashes.
% \ding{182} the number of unique APIs;
% \ding{183} the number of oracles;
% \ding{184} the fuzzing coverage;
% \ding{185} and the number of unique crashes.

\noindent
\textbf{Fuzzing Setup}
\tab
% how dynamic experiments are conducted
% For dynamic metrics \ding{184} and \ding{185}, fuzzing experiments are conducted.
Considering the randomness of fuzzing, we followed the suggestions from ~\cite{fuzz-eval}: 
the fuzzing experiments are conducted with five times of repeat for collecting average coverage information and the fuzzing of each driver lasts for 24 hours.
We used \texttt{libfuzzer}~\cite{libfuzzer} for experiments with empty initial seed and dictionary.
Besides, the command options of \texttt{libfuzzer} are:
\texttt{-close\_fd\_mask=3 -rss\_limit\_mb=2048 -timeout=30}.
For fair coverage comparison, we excluded the coverage of fuzz driver in collection stage (the merged driver can have thousands of lines of code) while still kept the fuzz driver coverage feedback during the fuzzing.
In total, the fuzzing experiments take 1.97 CPU year.

% static: 
%   api num (base & extended)
%   oracles comparison

% dynamic:
% cov
% crash

\subsection{Comparison on Code Metrics}

\noindent
\textbf{API Usage}
\tab
% number of unique APIs (divided as Basic APIs and Extended APIs)
The API usages are measured by the number of unique project APIs used in the fuzz driver.
The first row of Figure~\ref{fig:rq3-cmps} shows the comparison result.
In most questions, \texttt{BACTX-K} and \texttt{DOCTX-K} used less APIs than OSS-Fuzz while \texttt{UGCTX-K} and \texttt{ITER-K} contain at least the same amount of APIs.
This is intuitive since the latter two can provide examples guiding models to use more diverse API usages while the former two cannot.

% conservative
By manually investigating the drivers, we found that \textbf{models conservatively use APIs in driver generation}.
For example, for \texttt{BACTX-K} and \texttt{DOCTX-K}, mostly models tend to provide a fuzz driver with only necessary usages such as argument initialization.
They hardly extend the API usage such as adding APIs using the parsed object after parsing it.
% Mostly, it code will soon end the execution after the call of the target API with some necessary cleaning steps.
This is a reasonable strategy since aggressively extending APIs increases the risk of generating invalid drivers.
The examples provided in \texttt{UGCTX-K} and \texttt{ITER-K} alleviate this situation.
% for example, src/igraph/tests/unit/foreign_empty.c provides many APIs
% models will use the API usage explicitly shown in the 
As for drivers in OSS-Fuzz, the API usage diversity is case by case since they are from different contributors.
Some drivers~\cite{croaring-ossfuzz-driver-link} are minimally composed and some are extensively exploring more features of the target~\cite{lua-ossfuzz-driver-link}.
One interesting finding is that part of the OSS-Fuzz drivers are modified from the test files rather than written from scratch, which is a quite similar process as the query strategy \texttt{UGCTX-K}.
For example, OSS-Fuzz \texttt{kamailio} driver~\cite{kamailio-ossfuzz-driver-link} is modified from the test file~\cite{kamailio-test-example-link}.
Using this test code, \texttt{UGCTX-K} generates similar code as OSS-Fuzz driver.

% an example for example code can provide high diversity of API usage
% intersting finding

% 
% case study mention the ugctx, kamailio thing (similar approach as human experts, based on example code)
% usage & iter can have greater number of apis, but its structure may not be as good as human written drivers 

\noindent
\textbf{Oracle}
\tab
% oracles comparison & taxonomy
% manual written has many oracles
% generated has very few (is the generated oracle correct?)
% give some cases on what are usually patterns of manually written oracles
We did statistics on the oracles of the drivers.
The result is quite clear:
in all 72 questions, OSS-Fuzz drivers of 15 questions have contained at least one oracle which can detect semantic bugs, while there are no LLM-generated drivers have oracles.
The semantic oracles used in the OSS-Fuzz drivers can be divided into three categories:
\ding{182} check whether the return value or output content of an API is expected, \textit{e.g.}, ~\cite{bind9-api-output-oracle};
\ding{183} check whether the project internal status has expected value, \textit{e.g.}, ~\cite{igraph-internal-status-oracle};
\ding{184} compare whether the outputs of multiple APIs conform to specific relationships, \textit{e.g.}, ~\cite{bind9-check-two-apis-oracle}.

% brief intro on oracles

\begin{tcolorbox}[size=title, opacityfill=0.1]
LLMs tend to generate fuzz drivers with minimal API usages.
While example code can help LLM generate drivers containing more usages than OSS-Fuzz drivers, it still can be improved in adding oracles for better semantic bug detection.
\end{tcolorbox}

\subsection{Comparison on Fuzzing Metrics}

\noindent
\textbf{Coverage}
\tab
% average value
% the relation between coverage and API usage
% the conclusion is it is comparable with human written drivers
% however, this also with the help of manually filtering
The second row of Figure~\ref{fig:rq3-cmps} plots the coverage comparison results.
Generally, the LLM-generated drivers can demonstrate similar or better coverage performance on a considerable part of questions.
Particularly, the drivers of \texttt{UGCTX-K} and \texttt{ITER-K} reach similar coverage in most questions.
One interesting finding is that the performance of the merged fuzz driver can be decreased due to the poor usage driver it incorporates.
Specifically, in Figure~\ref{fig:cmp-ugctx}, there is a segment of the relatively flat line, where the \texttt{UGCTX-K} performs significantly worse than OSS-Fuzz.
However, according to its corresponding segment in Figure~\ref{fig:cmp-ugctx}, the drivers have used as many unique APIs as OSS-Fuzz's.
The reason is that there are dozens of drivers insides the merged drivers for these questions and the large portion of poor usages distracted the fuzzer.
This indicates that handling various LLM-generated drivers is not a naive thing. 
Proper filtering for retrieving high quality drivers may be necessary for driver effectiveness improvement.
% the potential spam issue that LLM-based driver generation can face.
% We discuss this in Section~\ref{xxx} in detail.

\noindent
\textbf{Crash}
\tab
The third row of Figure~\ref{fig:rq3-cmps} shows the crash results.
In plots, the LLM-generated fuzz drivers show qualified bug finding abilities.
In most questions that can be found bugs in our experiments, LLM-generated fuzz drivers show comparable crash finding outcomes.
Note that there are no false positive since the generated fuzz drivers are already filtered by our evaluation framework, which has manually written semantic checkers.
If only the automatic validation process are adopted, one invalid or unsound usage incorporated in the merged driver can quickly raise large number of false positives, which require the additional processing efforts.

\begin{tcolorbox}[size=title, opacityfill=0.1]
LLM-generated drivers can reach comparable fuzzing outcomes as OSS-Fuzz drivers.
For large scale application, how to practically pick effective fuzz drivers is the major challenge.
\end{tcolorbox}
