\begin{table}[t]
\caption{Designed Template for DOCTX-K and UGCTX-K.
}
\label{tab:advanced-strategies-detail}
\resizebox{\linewidth}{!}{
\begin{tabular}{lll}
\toprule
Strategy Type & Prompt Template & Repetition Times\\
\midrule

\rowcolor{black!10}
\begin{tabular}[t]{l}
\multirow{4}{*}{\texttt{DOCTX-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\texttt{\#}\Verb|include "${HEADER_FILE}"| \\
\Verb|${API_DOCUMENTATION}| \\
\Verb|${API_DECLARATION}| \\
\Verb|${NAIVE-K_TEMAPLTE}|\\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{4}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\
\midrule

\begin{tabular}[t]{l}
\multirow{6}{*}{\texttt{UGCTX-K}} \\
\end{tabular}
&
\begin{tabular}[t]{l}
\texttt{\#}\Verb|include "${HEADER_FILE}"| \\
\Verb|// @ examples of API usage \ | \\
\Verb|        ${EXAMPLE_FILE_NAME}| \\
\Verb|${EXAMPLE_CODE}| \\
\Verb|${EXAMPLE_API_DECLARATIONS}| \\
\Verb|${NAIVE-K_TEMAPLTE}|\\
\end{tabular}
&
\begin{tabular}[t]{l}
\multirow{6}{*}{K (K $\geq$ 1)} \\
\end{tabular}
\\

\bottomrule

\end{tabular}
}
\end{table}


\section{Enhanced Query Strategies (\textbf{RQ2})}

\subsection{Query With Extended Information}

\noindent
\textbf{Design of Strategies}
\tab
We conducted a study to examine the impact of including two types of \textbf{Extended} information in the prompt: API documentation and code snippets that use the API. 
Table~\ref{tab:advanced-strategies-detail} provides details on the two corresponding strategies that we developed by extending the prompt template of \texttt{BACTX-K}.
In the case of \texttt{DOCTX-K}, the only difference compared to  \texttt{BACTX-K} is the inclusion of the target API documentation before the function declaration.
It should be noted that not all the questions have associated API documentation, with only 49 out of 86 questions having such documentation. 
The documentation for 20 questions was automatically extracted from the header files, while the remaining 29 were manually collected from various sources such as project websites, repositories, and developer references. 
In the case of \texttt{UGCTX-K}, the example code snippet is added after the \texttt{\#include} statement, followed by the declarations of all APIs used in the example, and the rest are the same as those of \texttt{BACTX-K}. 
To obtain the example code snippets of each API, we followed a specific procedure: 
\ding{182} we retrieved the files containing the usage code using the \texttt{SourceGraph} \texttt{cli} tool~\cite{sourcegraph-cli-tool};
% \zhc{add ref to the crawled command}
\ding{183} we identified and excluded the files that are fuzz drivers;
\ding{184} each function inside the files that directly calls the target API is extracted as one example (using \texttt{ANTLR});
\ding{185} lastly, we deduplicated all the collected code snippets by comparing their similarity using Jaccard Similarity~\cite{jaccard-similarity} and removing any snippets that had a similarity of over 95\%.
For code snippets that were too long to be included in the prompt, we truncated them line by line until the prompt satisfied the maximum token length discussed in Section~\ref{subsec:evaluation-framework}. 
Each prompt generated by \texttt{UGCTX-K} includes one randomly selected snippet from its collection.

% Figure environment removed

% Figure environment removed

\noindent
\textbf{Effectiveness of DOCTX-K}
\tab
For all 49 questions that have API documentation, we compared the performance of \texttt{DOCTX-K} with that of \texttt{BACTX-K} under \texttt{gpt3.5} and \texttt{gpt4} models to understand its effectiveness.
The first row of Figure~\ref{fig:upset-plot-for-extended} shows upset plot and Figure~\ref{fig:extended-info-succ-rate-plots-per-score}'s details the comparison results in metrics of question and query success rate.
The line colors represent different strategies while the line styles stand for the used models.
% The black lines represent the performance of \texttt{gpt4} while the red for \texttt{gpt3.5}'s.
% The lines with cross sign markers are of \texttt{BACTX-K} and triangle markers are of \texttt{DOCTX-K}.

In general, \texttt{DOCTX-K} has a slight performance advantage over \texttt{BACTX-K}.  
Specifically, under \texttt{gpt4} model, both strategies solve the same number of questions and their query success rates are nearly identical.
Under \texttt{gpt3.5} model, \texttt{DOCTX-K} only had a slightly better performance than \texttt{BACTX-K} by uniquely solving four questions, whereas \texttt{BACTX-K} could only solve one unique question.
This result is reasonable because API documentation usually provides only a limited description of the API usage, typically a brief summary of the main function's functionality and a one-sentence explanation of its arguments. 
However, the usage information required to solve the blockers analyzed in Section~\ref{sec:failure-analysis}, such as control flow dependencies or indirectly dependent usage information on other APIs, is usually not included in the documentation.

% \zhc{add a case analysis here to support the conclusions in detail.}
\begin{tcolorbox}[size=title, opacityfill=0.1]
Adding API documentation in prompt can improve the overall performance.
However, the improvement is minor due to the limited usage description it contained.
\end{tcolorbox}
% while adding API documentation in prompt causes no negative effects on the effectiveness, it can only improve the performance in limited cases.}

%% Figure environment removed

% Figure environment removed

\noindent
\textbf{Effectiveness of UGCTX-K}
\tab
The performance of \texttt{UGCTX-K} is evaluated on all quiz questions with \textbf{K} value 40.
The second row of Figure~\ref{fig:upset-plot-for-extended} shows the upset plot while Figure~\ref{fig:extended-info-succ-rate-plots-per-score}'s details the comparison results in metrics of question and query success rate.
Similarly, line colors/styles are used for distinguishing different prompt templates/models.

In general, \texttt{UGCTX-K} exhibits a clear performance advantage over \texttt{BACTX-K} in terms of overall effectiveness.
% The union results of \texttt{gpt3.5-UGCTX-K} and \texttt{gpt4-UGCTX-K} correctly generates effective fuzz drivers for 
Considering the union results of \texttt{UGCTX-K}, \texttt{gpt3.5-UGCTX-K} and \texttt{gpt4-UGCTX-K} together solved 71 out of 86 questions, and covers all the questions solved by \texttt{BACTX-K}. 
For each model, Figure~\ref{fig:extended-qstn-succ-rate-per-score} shows that \texttt{UGCTX-K} lines (red lines) are always higher than or equal to corresponding \texttt{BACTX-K} lines (black lines), indicating a clear advantage for \texttt{UGCTX-K}.
Interestingly, Figure~\ref{fig:extended-query-succ-rate-per-score} shows that \texttt{UGCTX-K} has lower query success rates than \texttt{BACTX-K} for questions in the first two score buckets.
% \yaowen{need to check} Additionally, the intersection point (the third score bucket $[7,9]$) is the point where the question success rates of \texttt{BACTX-K} drop significantly. 
This indicates that adding less helpful usage snippets to the prompts can have negative effects on model performance, as the model may become confused or distracted, leading to lower success rates.
% This effect is particularly pronounced for questions that are likely to be solved without the help of snippets (buckets with lower scores).
In higher bucket scores, this effect is not observable since the rate of \texttt{BACTX-K} is significantly affected by the questions it cannot solve.

% example sources analysis
We further investigated the contributions of different types of example sources.
Figure~\ref{fig:statistics-on-different-example-sources} compares sources based on two kinds of divisions:
\ding{182} \textit{External} and \textit{Internal} sources.
The internal source includes the target project and its variants, while the external source covers the rest.
\ding{183} \textit{Test \& Example} source and \textit{Others} source.
% The plots states that, for all quiz questions, the number of correct fuzz drivers generated from the prompts built based on examples from internal source is two times more than the external's.
% For the questions that are not 
% \zhc{missing interpretation on data}
% \zhc{do we need to mention how we did this source identification? in appendix?}
One key observation from the plots is that both \textit{Internal} and \textit{Test \& Example} are higher quality sources than their counterparts, especially for these questions which are not likely to be solved without usage snippets (have not been solved by \texttt{BACTX-K}).

\noindent
\textbf{Case Studies}
\tab
Two cases are discussed to show how usage snippets help in solving the common blockers.

% counter-intuitive case: 
\# 9 \texttt{wc\_Str\_conv\_with\_detect}
\tab
This simple case challenges basic strategies due to the unintuitiveness of its API usage.
The API declaration is "\texttt{Str wc\_Str\_conv\_with\_detect(Str is,wc\_ces * f\_ces,wc\_ces hint,wc\_ces t\_ces)}".
It is used for converting the input stream \texttt{is} from one CES (character encoding scheme) to another, \textit{i.e.}, from \texttt{f\_ces} to \texttt{t\_ces}.
Most basic strategy drivers made mistakes on the creations of either \texttt{is} (the confusing type \texttt{Str}) or CESs, where the former has to be created using specific APIs such as \texttt{Strnew\_charp\_n} and the later should be specific macros or carefully initialized empty struct.
Example code snippets help here by directly providing the usage to models.

% control flow conditions: parse_xxx
\# 37 \texttt{igraph\_read\_graph\_graphdb}
\tab
The hardest part of this case for generating effective fuzz driver is the implicit control flow dependency it required.
Besides correctly initializing the function arguments, it has to call an API to change the default error handling behavior.
By default, when an abnormal input is detected, the API will abort frequently which blocks the fuzzing progress and raise large amount of false crashes.
To solve it, the driver has to set a silent error handler, such as calling \texttt{igraph\_set\_error\_handler(igraph\-\_error\_handler\_ignore)}.
This requirement is hard to be inferred beforehand due to its semantic nature and few inference clues.
However, some unit tests in the project such as \texttt{foreign\_empty.c} contain this usage, which directly instructs the generation.

\begin{tcolorbox}[size=title, opacityfill=0.1]
Example code snippets can greatly enhance model performance by providing direct insights into code usage.
Besides, "test/example files", "code files from the target/variant projects" are high quality sources for collecting code snippets.
% In summary, example code snippets can significantly improve the overall performance.
% They directly provides the usages which models usually failed to infer.
% However, adding usage snippets can cause performance decline if low quality candidates are used.
\end{tcolorbox}

\subsection{Iterative Query}
% output requires iterative refine & additional information

\begin{algorithm}[t]
    \caption{Iterative Query Strategy Overview}
    \label{alg:iterative-query}
    \small
    % \algsetup{linenosize=\tiny}
    \begin{algorithmic}[1] % The number tells where the line numbering should start
        \Require \algofont{f}, target API for generating fuzz driver
        \Ensure \algofont{A}, generated driver, or \algofont{null} if failed to generate effective driver
        % \Ensure \algofont{$G_{target}$} (Event Sequence Graph)
        \Procedure{iterative-query}{\algofont{f}}
            \State \algofont{Q} $\leftarrow$ Produce-Initial-Prompt(\algofont{f}) \Comment{Divergence Point in Search}
            \While{True}
                \State \algofont{A} $\leftarrow$ Do-Query(\algofont{Q})
                \State \algofont{R} $\leftarrow$ Do-Validation(\algofont{Q})
                \If {Is-Effective-Fuzz-Driver(\algofont{R})}
                    \Return \algofont{A}
                \EndIf
                \If {Cannot-Continue-Fix(\algofont{A}, \algofont{R})}
                    \Return \algofont{null}
                \EndIf
                \State \algofont{Q} $\leftarrow$ Produce-Fix-Prompt(\algofont{A}, \algofont{R}) \Comment{Divergence Point in Search}
                \If {Exceeds Max Rounds of Iterations} \Comment{Depth of Search}
                    \Return \algofont{null}
                \EndIf
            \EndWhile
            \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{table}[t]
\caption{General Template For Fix Prompts.
\footnotesize{
The content marked with * represents it is optional depends on the error type and the strategy.
}
}
\label{tab:fix-prompt-general-template}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{l}
\toprule
Fix Prompt Template\\
\midrule

\rowcolor{black!10}
\begin{tabular}[t]{l}
\Verb|```| \\
\Verb|${FUZZ_DRIVER_CODE}| \\
\Verb|```| \\
\Verb|${ERR_SUMMARY_IN_ONE_SENTENCE}|\\
\Verb|${ERR_LINE_CODE}|* \\
\Verb|${ERR_DETAIL}|* \\
\Verb|${SUPPLEMENTAL_INFO}|* \\
\Verb|Based on the above information, fix the code.| \\
\end{tabular}
\\
\bottomrule
\end{tabular}
}
\end{table}

\noindent
\textbf{Design of Strategies}
\tab
% the iterative strategy is a search problem
% general workflow of the search
% based on the two different setting points, we propose two types of strategies: BA-ITER-K, ALL-ITER-K
% Leveraging the interactive feature of the conversation with LLMs,
% we can design strategies which iteratively improves the generated driver until it becomes effective.
Algorithm~\ref{alg:iterative-query} shows the general workflow of the designed iterative query strategies.
The main idea of the algorithm is to iteratively improve the generated driver until reaching stop conditions.
% It first generates an initial query (line 2), then comes into the loop for iterative improvement.
% where the prompt can be one of the \texttt{BACTX-K}, \texttt{DOCTX-K}, and \texttt{UGCTX-K}.
% If the generated driver is ineffective and the stop conditions have not been triggered, the algorithm will generate a query for asking LLM to fix the generated driver based on the driver code, error information, and other usage information we have (line 10).
Intrinsically, it is a search problem, where the choices it made during the query generation (line 2 and line 10), and the search depth (line 11) together make up the search space.
Specifically, for initial query generation, \texttt{BACTX-K}, \texttt{DOCTX-K}, or \texttt{UGCTX-K} can be used for generating diverse startup drivers.
For fix query generation, we first detail its design then point out the divergence point.
Seven fix templates are designed towards fixing different types of errors observed in the driver.
These templates consider parse, linkage, and runtime fuzzing errors categorized by their symptoms.
Note that only the automatically identifiable symptoms are considered while the manually added tests and filters discussed in Section~\ref{sec:quiz-construction} will only be used for quiz evaluation.
The general design of the seven templates is shown in Table~\ref{tab:fix-prompt-general-template} while their full details are recorded in \appe\ref{sec:fix-templates-detail} Table~\ref{tab:fix-templates-detail}.
In general template, contents of "\texttt{\$\{ERR\_XXX\}}" are error type specific.
As for "\texttt{\$\{SUPPLEMENTAL\_INFO\}}", it will be set as the declaration or one of the usage snippets of the root cause API based on the error type.
The intuition here is that the declaration helps in fixing grammar errors and semantic errors may be fixed by providing usage snippets.
Since the driver even may not be grammatical valid, the root cause API is located by identifying the first API found starting from the error line of the code.
Determining which snippet should be included in prompt is another divergence point.
The stop conditions are either the maximum rounds of iteration has been reached or the LLM's reply cannot be fixed (contain no code or corrupted code).

Based on the general algorithm, we developed two strategies \texttt{BA-ITER-K} and \texttt{EX-ITER-K} (collectively referred to as \texttt{ITER-K}).
The former only uses the \textit{Fundamental} information about the API and error to generate queries while the latter can use any accessible information.
Conclusively, in divergence points, the former will only have one choice while the latter can meet multiple.
\textit{EX-ITER-K} will make the choice randomly.
Besides, the \texttt{K} represents the times of iterative queries will be conducted for one question.

% Figure environment removed

% Figure environment removed

%% Figure environment removed

\noindent
\textbf{Effectiveness of Iterative Query}
\tab
% which questions are compared
% introduce the plots
Due to the search nature and high cost of the evaluation, we use partial questions for evaluation.
The \texttt{ITER-K} strategies are evaluated on 23 questions which are not solved by \texttt{UGCTX-K} on \texttt{gpt3.5} or \texttt{gpt-4} model.
In our experiments, both the maximum iteration round and the \texttt{K} are set as 20.
Figure~\ref{fig:all-iter-succ-rate-plots-per-score} and Figure~\ref{fig:upset-plot-for-iter} illustrates the comparison results of \texttt{ITER-K} with \texttt{UGCTX-K} and the two iterative strategies. 

% conclusions draw from plots
Generally, \texttt{ITER-K} strategies show the significant performance advantage in metrics of overall performance.
\texttt{ITER-K} strategies solve almost all questions solved by \texttt{UGCTX-K} and uniquely solves four questions, leaving only 5 questions unsolved.
Besides, its question success rate is always higher than its counterpart's (red lines are higher than black lines).
It is worth to mention that \texttt{ITER-K} shows a lower query success rate than \texttt{UGCTX-K} in most score buckets, which means that \texttt{ITER-K} has a higher average search cost in solving questions.
% overall performance advantage -> can solve hard question
% low success rate -> high cost 

\noindent
\textbf{Case Studies}
\tab
Two cases are discussed to demonstrate how this strategy solves the questions others failed to.

\#5 \texttt{md\_html}
\tab
The API usage required in this case is not complex but the previous strategies failed to provide a valid callback function pointer as the argument.
To correctly execute the API, the driver has to prepare a function which handles the outputted data of the API and passes it to the API.
All the drivers generated by \texttt{UGCTX} either pass a \texttt{NULL} pointer or uses a non-existing function name.
Iterative query guides the fix by providing the link error which highlights that this referred function is undefined.

\#73 \texttt{pj\_stun\_msg\_decode}
\tab
In this case, one of the iterative query found the correct answer in five iteration rounds.
The initialization of the first argument (a pointer to a memory pool required for memory allocation during processing) is challengeable since it requires the calls of the two APIs in order:
\ding{182} \texttt{pj\_caching\_pool\_init} for initializing caching pool object \texttt{pj\_ca\-ching\_pool};
\ding{183} \texttt{pj\_pool\-\_create} for initializing memory pool object \texttt{pj\_pool\_t}.
The iterative query first corrects the wrongly used API for initializing caching pool object (\ding{182}), then figures out the mismatched type error for calling \ding{183}.
Despite these efforts, the driver still raises runtime crashes.
By providing the related implementation code of failed assertion lines located by crash stacks, the iterative query uses two iterations to test out the correct size check conditions for the mutated input data.

% md html, shows the iterative advantage (for fixing easy errors)
% a hard question, shows the necessity of the iterative for solving this question

\begin{tcolorbox}[size=title, opacityfill=0.1]
Iterative query strategies can generate effective fuzz drivers for more targets compared with non-iterative ones.
This advantage stems from their abilities which can utilize diverse usage information during generation, and solve the problem in a step-by-step manner.
However, this comes at the cost of high search cost and increased complexity.
\end{tcolorbox}