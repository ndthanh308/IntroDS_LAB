\begin{thebibliography}{10}

\bibitem{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou,
  `{What learning algorithm is in-context learning? Investigations with linear
  models}', in {\em The Eleventh International Conference on Learning
  Representations (ICLR)}, (2023).

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, `Neural machine translation
  by jointly learning to align and translate', in {\em 3rd International
  Conference on Learning Representations (ICLR)}, (2015).

\bibitem{NEURIPS2020_1457c0d6}
Tom Brown, Benjamin Mann, et~al., `Language models are few-shot learners', in
  {\em Advances in Neural Information Processing Systems}, volume~33, pp.
  1877--1901, (2020).

\bibitem{SMOTE}
Nitesh~V. Chawla, Kevin~W. Bowyer, Lawrence~O. Hall, and W.~Philip Kegelmeyer,
  `{SMOTE}: Synthetic minority over-sampling technique', {\em Journal of
  Artificial Intelligence Research}, {\bf 16}(1),  321–357, (2002).

\bibitem{pmlr-v97-chen19g}
Pengfei Chen, Ben~Ben Liao, Guangyong Chen, and Shengyu Zhang, `Understanding
  and utilizing deep neural networks trained with noisy labels', in {\em
  Proceedings of the 36th International Conference on Machine Learning}, pp.
  1062--1070, (2019).

\bibitem{chen-etal-2022-meta}
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He~He, `Meta-learning
  via language model in-context tuning', in {\em Proceedings of the 60th Annual
  Meeting of the Association for Computational Linguistics (Volume 1: Long
  Papers)}, pp. 719--730, (2022).

\bibitem{10.1007/11736790_9}
Ido Dagan, Oren Glickman, and Bernardo Magnini, `{The PASCAL Recognising
  Textual Entailment Challenge}', in {\em Proceedings of the First
  International Conference on Machine Learning Challenges: Evaluating
  Predictive Uncertainty Visual Object Classification, and Recognizing Textual
  Entailment}, pp. 177--190, (2005).

\bibitem{Marneffe2019TheCI}
Marie-Catherine de~Marneffe, Mandy Simons, and Judith Tonhauser, `{The
  CommitmentBank: Investigating projection in naturally occurring discourse}',
  {\em Proceedings of Sinn und Bedeutung}, {\bf 23}(2),  107--124, (2019).

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, `{BERT}:
  Pre-training of deep bidirectional transformers for language understanding',
  in {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of
  the Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pp. 4171--4186, (2019).

\bibitem{https://doi.org/10.48550/arxiv.2002.06305}
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi,
  and Noah Smith, `Fine-tuning pretrained language models: Weight
  initializations, data orders, and early stopping', {\em arXiv preprint:
  arXiv:2002.06305}, (2020).

\bibitem{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant, `What can
  transformers learn in-context? a case study of simple function classes', in
  {\em Advances in Neural Information Processing Systems}, volume~35, pp.
  30583--30598, (2022).

\bibitem{10.5555/3327546.3327707}
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel, `Using trusted
  data to train deep networks on labels corrupted by severe noise', in {\em
  Advances in Neural Information Processing Systems}, volume~31, pp.
  10456--10465, (2018).

\bibitem{hovy-etal-2001-toward}
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak
  Ravichandran, `Toward semantics-based answer pinpointing', in {\em
  Proceedings of the First International Conference on Human Language
  Technology Research}, pp. 1--7, (2001).

\bibitem{IVANOVS2021228}
Maksims Ivanovs, Roberts Kadikis, and Kaspars Ozols, `Perturbation-based
  methods for explaining deep neural networks: A survey', {\em Pattern
  Recognition Letters}, {\bf 150},  228--234, (2021).

\bibitem{pmlr-v80-jiang18c}
Lu~Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li~Fei-Fei,
  `{M}entor{N}et: Learning data-driven curriculum for very deep neural networks
  on corrupted labels', in {\em Proceedings of the 35th International
  Conference on Machine Learning}, pp. 2304--2313, (2018).

\bibitem{7837934}
Ishan Jindal, Matthew Nokleby, and Xuewen Chen, `Learning deep networks from
  noisy labels with dropout regularization', in {\em 2016 IEEE 16th Int.\
  Conference on Data Mining (ICDM)}, pp. 967--972, (2016).

\bibitem{https://doi.org/10.48550/arxiv.1909.11299}
Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang, `Mixout: Effective
  regularization to finetune large-scale pretrained language models', in {\em
  Proceedings of International Conference on Learning Representations (ICLR)},
  (2020).

\bibitem{pmlr-v97-lee19f}
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo~Li, and Jinwoo Shin, `Robust
  inference via generative classifiers for handling noisy labels', in {\em
  Proceedings of the 36th International Conference on Machine Learning}, pp.
  3763--3772, (2019).

\bibitem{li-roth-2002-learning}
Xin Li and Dan Roth, `Learning question classifiers', in {\em {COLING} 2002:
  The 19th Int.\ Conference on Computational Linguistics}, (2002).

\bibitem{8237473}
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia
  Li, `Learning from noisy labels with distillation', in {\em 2017 IEEE Int.\
  Conf.\ on Computer Vision (ICCV)}, pp. 1928--1936, (2017).

\bibitem{8237586}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár, `Focal
  loss for dense object detection', in {\em 2017 IEEE International Conference
  on Computer Vision (ICCV)}, pp. 2999--3007, (2017).

\bibitem{Ling2008CostSensitiveLA}
Charles~X. Ling and Victor~S. Sheng, `Cost-sensitive learning', in {\em
  Encyclopedia of Machine Learning}, eds., Claude Sammut and Geoffrey~I. Webb,
  231--235, Springer, (2008).

\bibitem{https://doi.org/10.48550/arxiv.2205.05638}
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
  Bansal, and Colin Raffel, `Few-shot parameter-efficient fine-tuning is better
  and cheaper than in-context learning', in {\em Advances in Neural Information
  Processing Systems}, volume~35, pp. 1950--1965, (2022).

\bibitem{liu-etal-2022-makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu
  Chen, `What makes good in-context examples for {GPT}-3?', in {\em Proceedings
  of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge
  Extraction and Integration for Deep Learning Architectures}, pp. 100--114,
  (2022).

\bibitem{Menon2020CanGC}
Aditya~Krishna Menon, Ankit~Singh Rawat, Sashank~J. Reddi, and Sanjiv Kumar,
  `Can gradient clipping mitigate label noise?', in {\em Eighth International
  Conference on Learning Representations (ICLR)}, (2020).

\bibitem{min-etal-2022-metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi, `{M}eta{ICL}:
  Learning to learn in context', in {\em Proc.\ of the 2022 Conf.\ of the North
  American Chapter of the Association for Computational Linguistics: Human
  Language Technologies}, pp. 2791--2809, (2022).

\bibitem{sewon-etal-2022-rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer, `Rethinking the role of demonstrations:
  What makes in-context learning work?', in {\em Proceedings of the 2022
  Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  pp. 11048--11064, (2022).

\bibitem{mishra-etal-2022-reframing}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh
  Hajishirzi, `Reframing instructional prompts to {GPT}k{'}s language', in {\em
  Findings of the Association for Computational Linguistics: ACL 2022}, pp.
  589--612, (2022).

\bibitem{pang-lee-2005-seeing}
Bo~Pang and Lillian Lee, `Seeing stars: Exploiting class relationships for
  sentiment categorization with respect to rating scales', in {\em Proceedings
  of the 43rd Annual Meeting of the Association for Computational Linguistics},
  pp. 115--124, (2005).

\bibitem{Pereyra2017RegularizingNN}
Gabriel Pereyra, G.~Tucker, Jan Chorowski, Lukasz Kaiser, and Geoffrey~E.
  Hinton, `Regularizing neural networks by penalizing confident output
  distributions', in {\em 5th International Conference on Learning
  Representations, (ICLR)}, (2017).

\bibitem{Phang2018SentenceEO}
Jason Phang, Thibault F{\'e}vry, and Samuel~R. Bowman, `Sentence encoders on
  stilts: Supplementary training on intermediate labeled-data tasks', {\em
  arXiv preprint arXiv:1811.01088}, (2018).

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al., `Language models are unsupervised multitask learners',
  {\em OpenAI blog}, {\bf 1}(8), ~9, (2019).

\bibitem{JMLR:v21:20-074}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu, `Exploring the limits of
  transfer learning with a unified text-to-text transformer', {\em J.\ of
  Machine Learning Research}, {\bf 21}(140),  1--67, (2020).

\bibitem{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts, `Recursive deep models for semantic
  compositionality over a sentiment treebank', in {\em Proceedings of the 2013
  Conference on Empirical Methods in Natural Language Processing}, pp.
  1631--1642, (2013).

\bibitem{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B}: A 6 billion parameter autoregressive language model.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{7929355}
Ruxin Wang, Tongliang Liu, and Dacheng Tao, `Multiclass learning with partially
  corrupted labels', {\em IEEE Transactions on Neural Networks and Learning
  Systems}, {\bf 29}(6),  2568--2580, (2018).

\bibitem{7727770}
Shoujin Wang, Wei Liu, Jia Wu, Longbing Cao, Qinxue Meng, and Paul~J. Kennedy,
  `Training deep neural networks on imbalanced data sets', in {\em 2016
  International Joint Conference on Neural Networks (IJCNN)}, pp. 4368--4374,
  (2016).

\bibitem{wiegreffe-pinter-2019-attention}
Sarah Wiegreffe and Yuval Pinter, `Attention is not not explanation', in {\em
  Proceedings of the 2019 Conference on Empirical Methods in Natural Language
  Processing and the 9th International Joint Conference on Natural Language
  Processing (EMNLP-IJCNLP)}, pp. 11--20, (2019).

\bibitem{7298885}
Tong Xiao, Tian Xia, Yi~Yang, Chang Huang, and Xiaogang Wang, `Learning from
  massive noisy labeled data for image classification', in {\em 2015 IEEE
  Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.
  2691--2699, (2015).

\bibitem{https://doi.org/10.48550/arxiv.2111.02080}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma, `An
  explanation of in-context learning as implicit bayesian inference', in {\em
  The Tenth Int.\ Conf.\ on Learning Representations, (ICLR)}, (2022).

\bibitem{https://doi.org/10.48550/arxiv.2205.12685}
Kang~Min Yoo, Junyeob Kim, Hyuhng~Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo
  Lee, Sang-goo Lee, and Taeuk Kim, `Ground-truth labels matter: A deeper look
  into input-label demonstrations', in {\em Proceedings of the 2022 Conference
  on Empirical Methods in Natural Language Processing}, pp. 2422--2437, (2022).

\bibitem{Zhang03}
J.~Zhang and I.~Mani, `{KNN} approach to unbalanced data distributions: A case
  study involving information extraction', in {\em {Proceedings of the
  ICML'2003 Workshop on Learning from Imbalanced Datasets}}, (2003).

\bibitem{10.5555/2969239.2969312}
Xiang Zhang, Junbo Zhao, and Yann LeCun, `Character-level convolutional
  networks for text classification', in {\em Advances in Neural Information
  Processing Systems}, volume~28, pp. 649--657, (2015).

\bibitem{zhang-etal-2022-active}
Yiming Zhang, Shi Feng, and Chenhao Tan, `Active example selection for
  in-context learning', in {\em Proc.\ of the 2022 Conference on Empirical
  Methods in Natural Language Processing}, pp. 9134--9148, (2022).

\bibitem{pmlr-v139-zhao21c}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh, `Calibrate
  before use: Improving few-shot performance of language models', in {\em
  Proceedings of the 38th International Conference on Machine Learning}, pp.
  12697--12706, (2021).

\end{thebibliography}
