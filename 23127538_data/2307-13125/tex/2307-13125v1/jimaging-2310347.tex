%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[jimaging,review,accept,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal and change "submit" to "accept". The document class line would be, e.g., \documentclass[preprints,article,accept,moreauthors,pdftex]{mdpi}. This is especially recommended for submission to arXiv, where line numbers should be removed before posting. For preprints.org, the editorial staff will make this change immediately prior to posting.

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% acoustics, actuators, addictions, admsci, adolescents, aerospace, agriculture, agriengineering, agronomy, ai, algorithms, allergies, alloys, analytica, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biotech, birds, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinpract, clockssleep, cmd, coasts, coatings, colloids, colorants, commodities, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryptography, crystals, csmf, ctn, curroncol, currophthalmol, cyber, dairy, data, dentistry, dermato, dermatopathology, designs, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecologies, econometrics, economies, education, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, environsciproc, epidemiologia, epigenomes, est, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, foundations, fractalfract, fuels, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gels, genealogy, genes, geographies, geohazards, geomatics, geosciences, geotechnics, geriatrics, hazardousmatters, healthcare, hearts, hemato, heritage, highthroughput, histories, horticulturae, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, ijerph, ijfs, ijgi, ijms, ijns, ijtm, ijtpp, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmmp, jmp, jmse, jne, jnt, jof, joitmc, jor, journalmedia, jox, jpm, jrfm, jsan, jtaer, jzbg, kidney, kidneydial, knowledge, land, languages, laws, life, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrology, micro, microarrays, microbiolres, micromachines, microorganisms, microplastics, minerals, mining, modelling, molbank, molecules, mps, msf, mti, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nri, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, pollutants, polymers, polysaccharides, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, publications, quantumrep, quaternary, qubs, radiation, reactions, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, ruminants, safety, sci, scipharm, seeds, sensors, separations, sexes, signals, sinusitis, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, sports, standards, stats, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, taxonomy, technologies, telecom, test, textiles, thalassrep, thermo, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, viruses, vision, waste, water, wem, wevj, wind, women, world, youth, zoonoticdis 

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, book, bookreview, briefreport, casereport, comment, commentary, communication, conferenceproceedings, correction, conferencereport, entry, expressionofconcern, extendedabstract, datadescriptor, editorial, essay, erratum, hypothesis, interestingimage, obituary, opinion, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, systematicreview, supfile, technicalnote, viewpoint, guidelines, registeredreport, tutorial
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. If eps figures are used, remove the option pdftex and use LaTeX and dvi2pdf.

%=================================================================
% MDPI internal commands
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{9}
\issuenum{4}
\articlenumber{81}
\pubyear{2023}
\copyrightyear{2023}
\externaleditor{{Academic Editors: Cecilia Di Ruberto, Alessandro Stefano, Albert Comelli, Lorenzo Putzu and Andrea Loddo}}
\datereceived{13 March 2023} 
\daterevised{31 March 2023} % Only for the journal Acoustics
\dateaccepted{7 April 2023} 
\datepublished{13 April 2023} 
%\datecorrected{} % Corrected papers include a "Corrected: XXX" date in the original paper.
%\dateretracted{} % Corrected papers include a "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/10.3390/\linebreak jimaging9040081} % If needed use \linebreak
%\doinum{}
%------------------------------------------------------------------
% The following line should be uncommented if the LaTeX file is uploaded to arXiv.org
%\pdfoutput=1

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, lineno, float, amsmath, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, cleveref, amsthm, hyphenat, natbib, hyperref, footmisc, url, geometry, newfloat, caption

\makeatletter
\let\c@lofdepth\relax
\let\c@lotdepth\relax
\makeatother
\usepackage{subfigure}
\makeatletter
\renewcommand{\@thesubfigure}{(\textbf{\alph{subfigure}})~}
\makeatother 


%=================================================================
%% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================

% Full title of the paper (Capitalized)
\Title{Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review}

% MDPI internal command: Title for citation in the left column
\TitleCitation{Deep Learning Approaches for Data Augmentation in Medical Imaging: A Review}

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0000-0002-2638-6795} % Add \orcidA{} behind the author's name
\newcommand{\orcidauthorB}{0000-0001-8785-6917} % Add \orcidB{} behind the author's name
\newcommand{\orcidauthorC}{0009-0000-6114-7733}

% Authors, for the paper (add full first names)
\Author{Aghiles Kebaili \orcidC{}, Jérôme Lapuyade-Lahorgue \orcidA{} and Su Ruan *\orcidB{}}%Author: Names and affiliations correctly checked.

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Aghiles Kebaili, Jerome Lapuyade-Lahorgue and Ruan Su}

% MDPI internal command: Authors, for citation in the left column
\AuthorCitation{Kebaili %MDPI: Please carefully check the accuracy of names and affiliations. While checking the affiliations, kindly note that multiple affiliations/addressed cannot be listed in one item. Please keep only the necessary information from inferior to superior: Group/Department/Institute/Faculty..etc, City, postal code (or equivalent) and Country.
, A.; Lapuyade-Lahorgue, J.; Ruan, S.}
% If this is a Chicago style journal: Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address[1]{Université Rouen Normandie, INSA Rouen Normandie, Université Le Havre Normandie, Normandie Univ, \mbox{LITIS UR 4108},  F-76000 Rouen, France
%Author: Unfortunately, due to university regulations, we are obliged to use this nomenclature.
}

% Contact information of the corresponding author
\corres{\hangafter=1 \hangindent=1.0em \hspace{-1em} {Correspondence: su.ruan@univ-rouen.fr}}

% Current address and/or shared authorship
% The commands \thirdnote{} till \eighthnote{} are available for further notes

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper

% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Deep learning has become a popular tool for medical image analysis, but the limited availability of training data remains a major challenge, particularly in the medical field where data acquisition can be costly and subject to privacy regulations. Data augmentation techniques offer a solution by artificially increasing the number of training samples, but these techniques often produce limited and unconvincing results. To address this issue, a growing number of studies have proposed the use of deep generative models to generate more realistic and diverse data that conform to the true distribution of the data. In this review, we focus on three types of deep generative models for medical image augmentation: variational autoencoders, generative adversarial networks, and diffusion models. We provide an overview of the current state of the art in each of these models and discuss their potential for use in different downstream tasks in medical imaging, including classification, segmentation, and cross-modal translation. We also evaluate the strengths and limitations of each model and suggest directions for future research in this field. Our goal is to provide a comprehensive review about the use of deep generative models for medical image augmentation and to highlight the potential of these models for improving the performance of deep learning algorithms in medical image analysis.}

% Keywords
\keyword{data augmentation; deep learning; medical imaging; generative models; variational autoencoders; diffusion models} 

% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%

%\noindent This is an obligatory section in “Advances in Respiratory Medicine”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \setcounter{section}{-1} %% Remove this when starting to work on the template.
\section{Introduction}

In recent years, advances in deep learning have been remarkable in many fields, including medical imaging. Deep learning is used to solve a wide variety of tasks such as classification \cite{amyar2022weakly,brochet2022quantitative}, segmentation \cite{zhou2022tri}, and detection \cite{chen2018unsupervised} using different types of medical imaging modalities, for instance, magnetic resonance imaging (MRI) \cite{lundervold2019overview}, computed tomography (CT) \cite{song2021deep}, and positron emission tomography (PET) \cite{islam2020gan}. Most of these modalities are defined as very high-dimensional data, and the number of training samples is often limited in the medical domain (e.g., the rarity of certain diseases). As deep learning algorithms rely on large amounts of data, running such applications in a low-sample-size regime can be very challenging. Data augmentation can increase the size of the training set by artificially synthesizing new samples. It is a very popular technique in computer vision~\cite{krizhevsky2017imagenet} and has become inseparable from deep learning applications when rich training sets are not available. Data generation is also used in the case of missing modalities for multimodal image segmentation \cite{zhou2022missing}. As a result, the model can be trained to generalize images with better quality and avoid overfitting. In addition, some deep learning frameworks, including PyTorch \cite{paszke2019pytorch}, allow for on-the-fly data augmentation during training, rather than physically expanding the training dataset. Basic data augmentation operations include random rotations, cropping, flipping, or noise injection. However, these simple operations are not sufficient when dealing with complex data such as medical images.



% % Figure environment removed

Several studies have been conducted to propose data augmentation schemes more suitable for the medical domain. The ultimate goal would be to reproduce a data distribution as close as possible to the real data, such that it is impossible, or at least difficult, to distinguish the newly sampled data from the real data. Recent performance improvements in deep generative models have made them particularly attractive for data augmentation. For example, generative adversarial networks (GANs) \cite{goodfellow2014generative} have demonstrated their ability to generate realistic images. As a result, this architecture has been widely used in the medical field \cite{sandfort2019data,mahapatra2019image} and has been included in several data augmentation reviews \cite{yi2019generative,ali2022role,chen2022generative}. Nevertheless, GANs also have their drawbacks, such as learning instability, difficulty in converging, and suffering from mode collapse \cite{mescheder2018training}, which is a state where the generator produces only a few samples. Variational autoencoders (VAEs) \cite{kingma2013auto} are another type of deep generative model that has received less attention in data augmentation. VAEs outperform GANs in terms of output diversity and are free of mode collapse. However, the major problem is their tendency to often produce blurry and hazy output images. This undesirable effect is due to the regularization term in the loss function. Recently, a new type of deep generative model called diffusion models (DMs) \cite{sohl2015deep,ho2020denoising} has emerged and promises remarkable results with a great ability to generate realistic and diverse outputs. However, DMs are still in their infancy and are not yet well established in the medical field, but are expected to be a promising alternative to previous generative models. One of the drawbacks of DMs is their high computational cost and huge sampling time.

Different approaches have been proposed to solve this generative learning trilemma of quality sampling, fast sampling, and diversity \cite{xiao2021tackling}. In this paper, we review the state of the art of deep learning architectures for data augmentation, focusing on three types of deep generative models for medical image augmentation: VAEs, GANs, and DMs. To provide an accurate review, we harvested a large number of publications via the PubMed and Google Scholar search engines. We selected only publications dating from at least 2017 using various keywords related to data augmentation in medical imaging. Following this, a second manual filtering was performed to eliminate all cases of false positives (publications not related to the medical field and/or data augmentation). In conclusion, 72 publications have been kept, mainly from journals such as IEEE Transactions In Medical Imaging or Medical Image Analysis and conferences such as Medical Image Computing and Computer Assisted Intervention and IEEE International Symposium on Biomedical Imaging. Some publications will be described in more detail in Section \ref{sec3}; these have been selected according to two criteria: date of publication and number of citations. Nevertheless, all the articles are available in descriptive tables as well as other information such as datasets used to perform training. These different papers were organized according to the deep generative model employed and the main downstream tasks targeted by the generated data (i.e., classification, segmentation, and cross-modal translation). Knowing the dominance of GANs for data augmentation in the medical imaging domain, the objective of this article is to highlight other generative models. To the best of our knowledge, this is the first review article that compares different deep generative models for data augmentation in medical imaging and does not focus exclusively on GANs \cite{yi2019generative,ali2022role,chen2022generative}, nor traditional data augmentation methods~\cite{chlap2021review,shorten2019survey}. The quantitative ratio of GAN-based articles to the rest of the deep generative models is very unbalanced; nevertheless, we try to bring some equilibrium to this ratio in the hope that an unbiased comparative study following this paper may be possible in the future. To further illustrate our findings, we present a graphical representation of the selected publications in Figure \ref{fig:stats}. This figure provides a comprehensive overview of the number of publications per year, per modality, and per downstream task. By analyzing these graphics, we can observe the trends and the preferences of the scientific community in terms of the use of deep generative models for data augmentation in medical imaging.

% Figure environment removed

This article is organized as follows: Section \ref{sec2} presents a brief theoretical view of the above deep generative models. Section \ref{sec3} reviews deep generative models for medical imaging data augmentation, grouped by the targeted application. Section \ref{sec4} discusses the advantages and disadvantages of each architecture and proposes a direction for future research. Finally, Section \ref{sec5} concludes the paper.

\section{Background}\label{sec2}
The main goal of deep generative models is to learn the underlying distribution of the data and to generate new samples that are similar to the real data. Our deep generative model can be represented as a function $g : z \longrightarrow x$ that maps a low-dimensional latent vector $z \in \mathbb{R}^d$ to a high-dimensional data point $x \in \mathbb{R}^D$ such as $d \leq D$. The latent variable $z$ is a realization of a random vector that is sampled from a prior distribution $p(z)$. The data point $x$ is another realization sampled from the data distribution $p(x)$. The goal of the deep generative model is to learn the mapping function $g$ such that the generated data $g(z)$ are similar to the real data $x$ associated with $z$. Each deep generative model proposes its own approach to learn the mapping function $g$. In this section we present a brief overview of the most popular deep generative models. Figure \ref{fig:deep generative models} provides a visual representation of their respective architectures.

% Figure environment removed

\subsection{Generative Adversarial Networks}
GAN \cite{goodfellow2014generative} is a class of deep generative models composed of two separate networks: a generator and a discriminator. The generator can be seen as a mapping function $G$ from a random latent vector $z$ to a data point $x$, where $z$ is sampled from a fixed prior distribution $p(z)$ commonly modelled as a Gaussian distribution. The discriminator $D$ is a binary classifier that takes a data point $x$ as input and outputs a probability $D(x)$ such that $x$ is a real data point. During the training process, the generator $G$ is trained to replicate data points $x_g$ so that the discriminator cannot distinguish between real data points $x_r$ and the generated data points $x_g$. On the other hand, the discriminator $D$ is trained to differentiate the fake from the real data points. Those two networks are trained simultaneously in an adversarial manner, hence the name generative adversarial network. The loss functions of $G$ and $D$ can be expressed as follow : 

\begin{equation}
\begin{aligned}
    \mathcal{L}_G &= \min_{\theta} \mathbb{E}_{z \sim p(z)}[\log D_\phi(G_\theta(z))] \\
    \mathcal{L}_D &= \max_{\phi} \mathbb{E}_{x \sim p(x)}[\log D_\phi(x)] + \mathbb{E}_{z \sim p(z)}[\log (1 - D_\phi(G_\theta(z)))]
\end{aligned}
\end{equation}
where $\theta$ and $\phi$ are the corresponding learnable parameters for the generator and discriminator neural networks, respectively.

This adversarial learning has proven to be effective in capturing the underlying distribution of the real data distribution $p(x)$. This has been inspired by game theory and can be seen as a minimax game between the generator and the discriminator. It is ultimately desirable to reach a Nash equilibrium where both the generator and discriminator are equally effective at their tasks. The loss function can be summarized as follows:

\begin{equation}
\mathcal{L}_{GAN} = \min_{\theta} \max_{\phi} \mathbb{E}_{x \sim p(x)}[\log D_\phi(x)]  + \mathbb{E}_{z \sim p(z)}[\log (1 - D_\phi(G_\theta(z)))]
\end{equation}

Once trained, new data points can be synthesized by sampling a random latent vector $z$ from the prior distribution $p(z)$ and feeding it to the generator.

\subsection{Variational Autoencoders}
Variational inference is a Bayesian inference technique that allows us to estimate the posterior distribution $p(z|x)$ with a simpler distribution $q(z|x)$.
The aim of variational inference is to minimize a Kullback--Leibler divergence between the posterior distribution $p_\theta(z|x)$ and the variational distribution $q_\phi(z|x)$, where $\theta$ and $\phi$ are the posterior and variational distribution parameters, respectively. The Kullback--Leibler is the most commonly used. The loss function based on Kullback--Leibler is defined as follows:

\begin{equation} 
\min_{\theta,\phi} D_{KL}(q_\phi(z|x) || p_\theta(z|x)) = \min_{\theta,\phi} \mathbb{E}_{z \sim q_\phi}[\log \frac{q_\phi(z|x)}{p_\theta(z|x)}]
\end{equation} 

With further simplifications, and applying Jensen's inequality, we can rewrite the above equation as:
\vspace{-6pt}
\begin{equation}
    \log p_\theta(x) = -\mathbb{E}_{z\sim q_\phi}[\log q_\phi(z|x)] + \mathbb{E}_{z \sim q_\phi}[\log p_\theta(z, x)]
    + D_{KL}(q_\phi(z|x) || p_\theta(z|x))
\end{equation}

\begin{equation} 
\begin{aligned}
    \log p_\theta(x) &\geq  -\mathbb{E}_{z\sim q_\phi}[\log q_\phi(z|x)] + \mathbb{E}_{z\sim q_\phi}[\log p_\theta(z, x)] \\
    &\geq \mathbb{E}_{z\sim q_\phi}[\log p_\theta(x|z)] - \mathbb{E}_{z\sim q_\phi}[\log \frac{q_\phi(z|x)}{p(z)}] = ELBO
\end{aligned}
\end{equation}
where $\log p_\theta(x)$ is the marginal log likelihood of the data $x$, $p(z)$ is the prior distribution of the latent variable $z$, generally modeled as a Gaussian distribution, and ELBO is the evidence lower bound. The variational distribution $q_\phi(z|x)$ can be learned by minimizing $D_{KL}(q_\phi(z|x) || p_\theta(z|x))$, which is equivalent to maximizing the ELBO given a fixed $\theta$. This ELBO term can be further decomposed into two terms: the reconstruction term and the regularization term. The reconstruction term measures the difference between the input data and its reconstruction, and it is typically calculated using binary cross-entropy loss. The regularization term ensures that the latent variables follow a desired distribution, such as a normal distribution, and it is calculated using the Kullback--Leibler divergence between the latent distribution and the desired distribution. Together, these two terms form the ELBO loss function, which is used to train the VAE model. The VAE is composed of an encoder $q_\phi(z|x)$ and a decoder $p_\theta(x|z)$. The encoder $q_\phi(z|x)$ is a neural network that maps the data $x$ to the latent variable $z$. The decoder $p_\theta(x|z)$ is a neural network that maps the latent variable $z$ to the data $x$. The VAE is trained by minimizing the reconstruction and regularization terms \eqref{lrec}.
\vspace{-6pt}
\begin{equation}
    \mathcal{L}_{rec} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)], \;\;
    \mathcal{L}_{reg} = \mathbb{E}_{q_\phi(z|x)}[\log \frac{q_\phi(z|x)}{p(z)}]
\label{lrec}
\end{equation}

Once trained, new data points can be synthesized by sampling a random latent vector $z$ from the prior distribution $q_\phi$ and feeding it to the decoder. In other words, the decoder represents the generative model.

\subsection{Diffusion Probabilistic Models}
Diffusion models \cite{sohl2015deep,ho2020denoising} are a class of generative models that are based on the diffusion process. The diffusion process is a stochastic process that can be seen as a parameterized Markov chain. Each transition in the chain gradually adds a Gaussian noise to an initial data point $x_0$ of distribution $q(x)$. The diffusion process can be expressed as follow:
\begin{equation}
\begin{aligned}
    q(x_t|x_{t-1}) &= \mathcal{N}(\sqrt{\alpha_t}x_{t-1}, \beta_t \text{I})\\
    q(x_{1:T}|x_0) &= \prod_{t=1}^T q(x_t|x_{t-1})
\end{aligned}
\end{equation}
where $\beta_t \in [0, 1], t = 1, \ldots, T$ is the predefined noise variance at step $t$, $\alpha_t = 1 - \beta_t$, and $T$, the total number of steps. The diffusion model is trained to reverse the diffusion process starting with a noise input $x_T \sim \text{N}(\text{0}, \text{I})$ and reconstructing the initial data point $x_0$. This denoising process can be seen as a generative model. The reverse diffusion process can be expressed as follows:
\vspace{-6pt}
\begin{equation}
p_\theta(x_{0:T}) = p(x_T)\prod_{t=1}^{T} p_\theta(x_{t-1}|x_t), \;\; q(x_{t - 1}|x_{t}) = \mathcal{N}(\mu(x_t, t), \Sigma(x_t, t))
\end{equation}
where $\mu(x_t, t)$ and $\Sigma(x_t, t)$ are the mean and the variance of the denoising model at step $t$. Similarly to the VAE, diffusion models learn to recreate the true sample at each step by maximizing the evidence lower bound (ELBO), matching the true denoising distribution $q(x_{t-1}|x_t)$ and the learned denoising distribution $p_\theta(x_{t-1}|x_t)$. By the end of the training, the diffusion model will be able to map a noise input $x_T$ to the initial data point $x_0$ throught reverse diffusion; hence, new data points can be synthesized by sampling a random noise vector $x_T$ from the prior distribution $\mathcal{N}(0, \text{I})$ and feeding it to the model.

\subsection{Exploring the Trade-Offs in Deep Generative Models: The Generative Learning Trilemma}
\subsubsection{Generative Adversarial Networks}
The design and training of VAEs, GANs, and DMs is often subject to trade-offs between fast sampling, high-quality samples, and mode coverage, known as the generative learning trilemma \cite{xiao2021tackling}. Among these models, GANs have received particular attention due to their ability to generate realistic images and are the first deep generative models to be extensively used for medical image augmentation. They are known for their ability to generate high-quality samples that are difficult to distinguish from real data. However, they may suffer from mode collapse, a phenomenon where the model only generates samples from a limited number of modes or patterns in the data distribution, potentially leading to poor coverage of the data distribution and a lack of diversity in the generated samples. To address mode collapse, several variations of  GAN have been proposed. One popular approach is the Wasserstein GAN (WGAN) \cite{arjovsky2017wasserstein}, which replaces the Jensen--Shannon divergence used in the original GAN with the Wasserstein distance, a metric that measures the distance between two probability distributions. This has the benefit of improving the quality of the generated samples. Another widely used extension is the conditional GAN (CGAN) \cite{mirza2014conditional}, which adds a conditioning variable $y$ to the latent vector $z$ in the generator, allowing for more control over the generated samples and partially mitigating mode collapse. The CGAN can be seen as a generative model that can generate data points $x$ conditioned on $y$ and models the joint distribution $p(x, y)$. A GAN with a conditional generator has been introduced by Isola et al. \cite{pix2pix} to learn to translate images from one domain to another by replacing the traditional noise-to-image generator with a U-Net \cite{ronneberger2015u}. The adversarial learning process allows the U-Net to generate more realistic images based on a better understanding of the underlying data distribution.

\textls[-15]{Other variations of the GAN include deep convolutional GAN (DCGAN) \cite{dcgan}, progressive growing GAN (PGGAN) \cite{karras2017progressive}, CycleGAN \cite{cyclegan}, auxiliary classifier GAN (ACGAN)~ \cite{odena2016conditional}, VAE-GAN \cite{vaegan}, and many others, which have been proposed to address various issues such as training stability, scalability, and quality of the generated samples. While these variants have achieved good results in a variety of tasks, they also come with their own set of trade-offs. Despite these limitations, GANs are generally fast at generating new images, making them a good choice for data augmentation when well-trained. As an example, Figure \ref{fig:gan} showcases the capacity of a CycleGAN to generate realistic synthetic medical images.}



\subsubsection{Variational Autoencoders}
VAEs are a another type of deep generative model that has gained popularity for their ease of training and good coverage of the data distribution. Unlike GANs, VAEs are trained to maximize the likelihood of the data rather than adversarially, making them a good choice for tasks that require fast sampling and good coverage of the data distribution. Using variational inference methods, VAEs are able to better approximate the real data distribution given a random noise vector, thus making them less vulnerable to mode collapse. Moreover, VAEs enable the extraction of relevant features and can learn a smooth latent representation of the data, which allows for the interpolation of points in the space providing more control over the generated samples \cite{higgins2016early}. 

% Figure environment removed

VAEs have not been as commonly used for data augmentation compared to GANs due to the blurry and hazy nature of the generated samples. However, several proposals, such as inverse autoregressive flow \cite{iafvae}, InfoVAE \cite{zhao2017infovae}, or VQ-VAE2 \cite{vqvae2}, have been made to improve the quality of VAE-generated samples as well as the variational aspect of the model. Despite this, most of these extensions have not yet been applied to medical image augmentation. A more effective approach to addressing the limitations of VAEs in this context is to utilize a hybrid model called a VAE-GAN, which combines the strengths of both VAEs and GANs to generate high-quality, diverse, and realistic synthetic samples. While VAE-GANs cannot fully fix the low-quality generation of VAEs, they do partially address this issue by incorporating the adversarial training objective of GANs, which allows for the improvement of visual quality and sharpness of the generated samples while still preserving the ability of VAEs to learn a compact latent representation of the data. In addition to VAE-GANs, another common architecture for medical image augmentation is the use of conditional VAEs (CVAEs), which allows for the control of the output samples by conditioning the generation process on additional information, such as class labels or attributes. This can be particularly useful in medical imaging, as it allows for the generation of synthetic samples that are representative of specific subgroups or conditions within the data. By using conditional VAEs, it is possible to generate synthetic samples that are more targeted and relevant to specific tasks or analyses. In summary, VAEs, VAE-GANs, and conditional VAEs are all viable approaches for medical image augmentation, each offering different benefits and trade-offs in terms of diversity, quality, and fidelity of the generated~samples.

\subsubsection{Diffusion Models}
There has been a recent surge in the use of DMs for image synthesis in the academic literature due to their superior performance in generating high-quality and realistic synthesized images compared to other deep generative models such as VAEs and GANs \cite{dhariwal2021diffusion}. This success can be attributed to the way in which DMs model the data distribution by approximating it using a series of simple distributions combined through the diffusion process, allowing them to capture complex, high-dimensional distributions and generate samples that are highly representative of the underlying data. This is especially useful for synthesizing images as natural images often have a wide range of textures, colors, and other visual features that can be difficult to model using simpler parametric models. This can also be applied to medical imaging where data tends to be complex. However, DMs can also have some limitations, such as being computationally intensive to solve, especially for large or complex systems, and requiring a significant amount of data to be accurately calibrated. In addition, DMs have a long sampling time compared to other deep generative models such as VAEs and GANs due to the high number of steps in the reverse diffusion process (ranging from several hundreds to thousands). This issue is compounded when the model is being used in real-time applications or when it is necessary to generate large numbers of samples. As a result, researchers have proposed several solutions and variants of diffusion models that aim to improve the sampling speed while maintaining high-quality and diverse samples. {\color{black} These include 
strategies such as progressive distillation \cite{salimans2022progressive}. This method involves distilling a trained deterministic diffusion sampler, using many steps, into a new diffusion model that takes half as many sampling steps. Another way to improve the sampling time is the use of improved variants such as Fast Diffusion Probabilistic Model (FastDPM) \cite{kong2021fast}, which uses a modified optimization algorithm to reduce the sampling time and introduces a concept of continuous diffusion process, or with non-Markovian diffusion models such as Denoising Diffusion Implicit Model (DDIM) \cite{song2020denoising}}. Similarly to VAE-GAN, ref. \cite{xiao2021tackling} proposes the denoising diffusion GAN, which is a hybrid architecture between DMs and multimodal conditional GANs \cite{mirza2014conditional}, which have been shown to produce high-quality and diverse samples at a much faster sampling speed compared to the original diffusion models (factor of $\times$2000). Overall, while diffusion models have demonstrated great potential in the field of image synthesis, their long sampling time remains a challenge that researchers are actively working to address.

% --- ICI
\section{Deep Generative Models for Medical Image Augmentation}\label{sec3}

Medical image processing and analysis using deep learning has developed rapidly in the past years, and it has been able to achieve state-of-the-art results in many tasks. However, the lack of data is still a major issue in this field. To address this, medical image augmentation became a crucial task, and many studies have been conducted in this direction. In this section, we will review the different deep generative models that have been proposed to generate synthetic medical images. This review is organized into three different categories corresponding to each one of the  deep generative models. The publications are further classified according to the downstream task targeted by the generated images. We address here the most common tasks in medical imaging: classification, segmentation, and cross-model image translation, which will be summarized in the form of tables.

\subsection{Generative Adversarial Networks}
As part of their study, Han et al. \cite{han2018gan} proposed the use of two variants of GANs for generating (2D) MRI sequences: a WGAN \cite{arjovsky2017wasserstein} and a DCGAN \cite{dcgan}, in which combinations of convolutions and batch normalizations replace the fully-connected layers. The results of this study were presented in the form of a visual Turing test where an expert physician was asked to classify real and synthetic images. For all MRI sequences except FLAIR images, WGAN was significantly more successful at deceiving the physician than DCGAN (62\% compared to 54\%). The same author further proposes using PGGAN \cite{karras2017progressive} combined with traditional data augmentation techniques such as geometric transformations. PGGAN is a GAN with a multi-stage training strategy that progressively increases the resolution of the generated images. The results indicate that combining PGGAN with traditionally augmented data can slightly improve the performance of the classifier when compared to using  PGGAN alone.


\textls[-20]{Conditional synthesis is a technique that allows the generation of images conditioned on a specific variable $y$. This is particularly useful in medical imaging, where tasks such as segmentation or cross-modal translation are widespread. A variable $y$ serves as the ground truth for the generated images and can be expressed in various ways, including class labels, segmentation maps, or translation maps. In this context, Frid-Adar et al. \cite{frid2018gan} propose to use an ACGAN \cite{odena2016conditional} for synthesizing liver lesions in CT images. The ACGAN is a GAN with a discriminator conditioned on a class label. Three label classes were considered: cysts, metastases, and hemangiomas. Based solely on conventional data augmentation, the classification results produced a sensitivity of 78.6\% and a specificity of 88.4\%. By adding the synthetic data augmentation, the results increased to a sensitivity of 85.7\% and a specificity of 92.4\%. \mbox{Guibas et al. \cite{guibas2017synthetic}} propose a two-stage pipeline for generating synthetic images of fundus photographs with associated blood vessel segmentation masks. In the first stage, synthetic segmentation masks are generated using DCGAN, and in the second stage, these synthetic masks are translated into photorealistic fundus images using CGAN. Comparing the Kullback--Leibler divergence between the real and synthetic images revealed no significant differences between the two distributions. In addition, the authors evaluated the generated images on a segmentation task using only synthetic images, showing an F1 score of 0.887 versus 0.898 when using real images. This negligible difference indicates the quality of the generated images. By the same token, Platscher et al. \cite{platscher2020image} propose using a two-step image translation approach to generate MRI images with ischemic stroke lesion masks. The first step consists of generating synthetic stroke lesion masks using a WGAN. The newly generated fake lesions are implanted on healthy brain anatomical segmentation masks. Finally, those segmentation masks are fed into a pretrained image-translation model that maps the mask into a real ischemic stroke MRI. The authors studied three different image translation models, CycleGAN \cite{cyclegan}, Pix2Pix~\cite{pix2pix}, and SPADE \cite{park2019semantic}, and reported that Pix2Pix was the most successful in terms of visual quality. A U-Net \cite{ronneberger2015u} was trained using both clinical and generated images and showed an improvement in the Dice score compared to the model trained only on clinical images (63.7\% to 72.8\%).}


Regarding cross-modal translation, Yurt et al. \cite{yurt2021mustgan} propose a multi-stream approach for generating missing or corrupted MRI contrasts from other high-quality ones using a GAN-based architecture. The generator is composed of multiple one-to-one streams and a joint many-to-one stream, which are designed to learn latent representations sensitive to unique and common features of the source, respectively. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many-to-one stream are combined with a fusion block and fed to a joint network that infers the final image. In their experiments, the authors compare their approach to other state-of-the-art translation GANs and show that the proposed method is more effective in terms of quantitative and radiological assessments. The synthesized images presented in this study demonstrate the effectiveness of deep learning approaches applied to data augmentation in medical imaging. Specifically, the study investigated two tasks: (a) T1-weighted image synthesis from T2- and PD-weighted images and (b) PD-weighted image synthesis from T1- and T2-weighted images. The results obtained from the proposed method outperformed other variants of GANs such as pGAN \cite{dar2019image} and MM-GAN \cite{sun2020mm}, highlighting its effectiveness for image synthesis in medical imaging.

In summary, the use of GANs for data augmentation has been demonstrated to be a successful approach. The studies discussed in this section have employed some of the most innovative and known GAN architectures in the medical field, including WGAN, DCGAN, and Pix2Pix, and have primarily focused on three tasks: classification, segmentation, and cross-modal translation. Custom-made GAN variants have also been proposed in the current state of the art (see Table \ref{tab:gan}), some of which could be explored further. Notably, conditional synthesis has proven to be particularly useful for tasks such as segmentation and cross-modal translation, as seen with the ACGAN and Pix2Pix, resulting in an improved classification performance. Additionally, two-stage pipeline approaches have been proposed for generating synthetic images conditioned on segmentation masks. To further illustrate the use of GANs for medical image augmentation, we present a summary of the relevant studies in Table \ref{tab:gan}. This table includes information about the dataset, imaging modality, and evaluation metrics used in each study, as well as the specific type of GAN architecture employed. A further discussion will be presented in Section \ref{sec4}.

\begin{table}[H]
\tablesize{\footnotesize}
\caption{Overview of GAN-based architectures for medical image augmentation, including hybrid status of architectures (if applicable), indicating used combinations of VAEs, GANs, and DMs.\label{tab:gan}}

\setlength{\cellWidtha}{\fulllength/7-2\tabcolsep-0.2in}
\setlength{\cellWidthb}{\fulllength/7-2\tabcolsep+0.2in}
\setlength{\cellWidthc}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthd}{\fulllength/7-2\tabcolsep+0.1in}
\setlength{\cellWidthe}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthf}{\fulllength/7-2\tabcolsep-0.6in}
\setlength{\cellWidthg}{\fulllength/7-2\tabcolsep+0.5in}
	   \begin{adjustwidth}{-\extralength}{0cm}
		\begin{tabularx}{\fulllength}{>{\raggedright\arraybackslash}m{\cellWidtha}>{\raggedright\arraybackslash}m{\cellWidthb}>{\raggedright\arraybackslash}m{\cellWidthc}>{\raggedright\arraybackslash}m{\cellWidthd}>{\raggedright\arraybackslash}m{\cellWidthe}>{\raggedright\arraybackslash}m{\cellWidthf}>{\raggedright\arraybackslash}m{\cellWidthg}}
% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
		\toprule % --------------------------------------------------
            \textbf{Reference}	& 
            \textbf{Architecture} &
            \textbf{Hybrid Status} & 
            \textbf{Dataset} &
            \textbf{Modality} &
            \textbf{3D} &
            \textbf{Eval. Metrics}\\
		\midrule % ---------------------------------------------------
            \textbf{Classification} &&&&&&\\
            \midrule
            \cite{frid2018gan} & DCGAN, ACGAN & & Private & CT & & Sens., Spec.\\
            \cite{han2018gan} & DCGAN, WGAN & & BraTS2016 & MR & & Acc.\\
            \cite{han2019combining} & PGGAN, MUNIT & & BraTS2016 & MR & \checkmark & Acc., Sens., Spec.,\\
            \cite{kwon2019generation} & AE-GAN & Hybrid (V~+~G)& BraTS2018, ADNI & MR & \checkmark & MMD, MS-SSIM\\
            \cite{zhuang2019fmri} & ICW-GAN & & OpenfMRI, HCP & MR & \checkmark & Acc., Prec., F1\\  
            &&& NeuroSpin, IBC &&&Recall\\ 
            \cite{waheed2020covidgan} & ACGAN & & IEEE CCX & X-ray & &Acc., Sens., Spec.\\
            &&&&&&Prec., Recall, F1 \\ 
            \cite{han2020infinite} & PGGAN & & BraTS2016 & MR & & Acc., Sens., Spec.\\
            \cite{sun2020adversarial}  & ANT-GAN & & BraTS2018 & MR & & Acc. \\
            \cite{wang2020class}  & MG-CGAN & & LIDC-IDRI & CT & & Acc., F1 \\
            \cite{geng2020deep} & FC-GAN & Hybrid (V~+~G) & ADHD, ABIDE & MR & &Acc., Sens., Spec., AUC\\
            \cite{pang2021semi} & TGAN & & Private & Ultrasound & & Acc., Sens., Spec.\\
            \cite{barile2021data} & AAE & & Private & MR & & Prec., Recall, F1\\
            \cite{shen2021mass} & DCGAN, InfillingGAN & & DDSM & CT & & LPIPS, Recall\\
            \cite{ambita2021covit} & SAGAN & & COVID-CT, SARS-COV2 & CT & & Acc.\\
            \cite{hirte2021realistic} & StyleGAN & & Private & MR & & -\\
            \cite{kaur2021mr}  & DCGAN & & PPMI & MR & & Acc., Spec., Sens.\\
            \cite{guan2022medical} & TMP-GAN & & CBIS-DDMS, Private & CT & & Prec., Recall, F1, AUC\\
            \cite{ahmad2022brain} & VAE-GAN & Hybrid (V~+~G) & Private & MR & & Acc., Sens., Spec.\\
            \cite{pombo2022equitable} & CounterSynth & & UK Biobank, OASIS & MR & \checkmark & Acc., MSE, SSIM, MAE\\
            \midrule
            \textbf{Segmentation} &&&&&&\\
            \midrule
            \cite{guibas2017synthetic} & CGAN & & DRIVE & Fundus photography & & KLD, F1\\
            \cite{neff2017generative} & DCGAN & & SCR & X-ray & & Dice, Hausdorff\\
            \cite{mok2018learning} & CB-GAN & & BraTS2015 & MR & & Dice, Prec., Sens.\\
            \cite{shin2018medical} & Pix2Pix & & BraTS2015, ADNI & MR &\checkmark& Dice\\
            \cite{sandfort2019data} & CycleGAN & & NIHPCT & CT & & Dice\\
            \cite{jiang2019cross} & CM-GAN & & Private & MR & & KLD, Dice \\
            &&&&&&hausdorff \\ 
            \cite{jiang2020covid} & CGAN & & COVID-CT &CT & & FID, PSNR, SSIM, RMSE\\
            \cite{qasim2020red} & Red-GAN & & BraTS2015, ISIC & MR & & Dice\\
            \cite{platscher2020image} & Pix2Pix, SPADE, CycleGAN & & Private & MR & & Dice\\
            \cite{shi2020novel} & StyleGAN & & LIDC-IDRI & CT & & Dice, Pres., Sens.\\
            \cite{shen2022image} & DCGAN, GatedConv & & Private & X-ray & & MAE, PSNR, SSIM, FID, AUC\\


%\bottomrule
%		\end{tabularx}
%\end{adjustwidth}
%\end{table}

%\begin{table}[H]
%\tablesize{\footnotesize}
%\caption{Overview of GAN-based architectures for medical image augmentation, including hybrid status of architectures (if applicable), indicating used combinations of VAEs, GANs, and DMs.\label{tab:gan}}
%	   \begin{adjustwidth}{-\extralength}{0cm}
% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
%		\toprule % --------------------------------------------------
%            \textbf{Reference}	& 
%            \textbf{Architecture} &
%            \textbf{Hybrid Status} & 
%            \textbf{Dataset} &
%            \textbf{Modality} &
%            \textbf{3D} &
%            \textbf{Eval. Metrics}\\
%		\midrule % ---------------------------------------------------
%            \textbf{Classification} &&&&&&\\
%            \midrule
%            \cite{frid2018gan} & DCGAN, ACGAN & & Private & CT & & Sens., Spec.\\
%            \cite{han2018gan} & DCGAN, WGAN & & BraTS2016 & MR & & Acc.\\
%            \cite{han2019combining} & PGGAN, MUNIT & & BraTS2016 & MR & \checkmark & Acc., Sens., Spec.,\\
%            \cite{kwon2019generation} & AE-GAN & Hybrid (V~+~G)& BraTS2018, ADNI & MR & \checkmark & MMD, MS-SSIM\\
%            \cite{zhuang2019fmri} & ICW-GAN & & OpenfMRI, HCP & MR & \checkmark & Acc., Prec., F1\\  
%            &&& NeuroSpin, IBC &&&Recall\\ 
%            \cite{waheed2020covidgan} & ACGAN & & IEEE CCX & X-ray & &Acc., Sens., Spec.\\
%            &&&&&&Prec., Recall, F1 \\ 
%            \cite{han2020infinite} & PGGAN & & BraTS2016 & MR & & Acc., Sens., Spec.\\
%            \cite{sun2020adversarial}  & ANT-GAN & & BraTS2018 & MR & & Acc. \\
%            \cite{wang2020class}  & MG-CGAN & & LIDC-IDRI & CT & & Acc., F1 \\
%            \cite{geng2020deep} & FC-GAN & Hybrid (V~+~G) & ADHD, ABIDE & MR & &Acc., Sens., Spec., AUC\\
%            \cite{pang2021semi} & TGAN & & Private & Ultrasound & & Acc., Sens., Spec.\\
%            \cite{barile2021data} & AAE & & Private & MR & & Prec., Recall, F1\\
%            \cite{shen2021mass} & DCGAN, InfillingGAN & & DDSM & CT & & LPIPS, Recall\\
%            \cite{ambita2021covit} & SAGAN & & COVID-CT, SARS-COV2 & CT & & Acc.\\
%            \cite{hirte2021realistic} & StyleGAN & & Private & MR & & -\\
%            \cite{kaur2021mr}  & DCGAN & & PPMI & MR & & Acc., Spec., Sens.\\
%            \cite{guan2022medical} & TMP-GAN & & CBIS-DDMS, Private & CT & & Prec., Recall, F1, AUC\\
%            \cite{ahmad2022brain} & VAE-GAN & Hybrid (V~+~G) & Private & MR & & Acc., Sens., Spec.\\
%            \cite{pombo2022equitable} & CounterSynth & & UK Biobank, OASIS & MR & \checkmark & Acc., MSE, SSIM, MAE\\
%            \midrule
%            \textbf{Segmentation} &&&&&&\\
%            \midrule
%            \cite{guibas2017synthetic} & CGAN & & DRIVE & Fundus photography & & KLD, F1\\
%            \cite{neff2017generative} & DCGAN & & SCR & X-ray & & Dice, Hausdorff\\
%            \cite{mok2018learning} & CB-GAN & & BraTS2015 & MR & & Dice, Prec., Sens.\\
%            \cite{shin2018medical} & Pix2Pix & & BraTS2015, ADNI & MR &\checkmark& Dice\\
%            \cite{sandfort2019data} & CycleGAN & & NIHPCT & CT & & Dice\\
%            \cite{jiang2019cross} & CM-GAN & & Private & MR & & KLD, Dice \\
%            &&&&&&hausdorff \\ 
%            \cite{jiang2020covid} & CGAN & & COVID-CT &CT & & FID, PSNR, SSIM, RMSE\\
%            \cite{qasim2020red} & Red-GAN & & BraTS2015, ISIC & MR & & Dice\\
%            \cite{platscher2020image} & Pix2Pix, SPADE, CycleGAN & & Private & MR & & Dice\\
%            \cite{shi2020novel} & StyleGAN & & LIDC-IDRI & CT & & Dice, Pres., Sens.\\
%            \cite{shen2022image} & DCGAN, GatedConv & & Private & X-ray & & MAE, PSNR, SSIM, FID, AUC\\
%
%
%\bottomrule
%\end{tabularx}
%\end{adjustwidth}
%\end{table}



%\begin{table}[H]\ContinuedFloat
%\tablesize{\footnotesize}
%\caption{\textit{Cont.}\label{tab:gan}}
%\setlength{\cellWidtha}{\fulllength/7-2\tabcolsep-0.2in}
%\setlength{\cellWidthb}{\fulllength/7-2\tabcolsep+0.2in}
%\setlength{\cellWidthc}{\fulllength/7-2\tabcolsep-0in}
%\setlength{\cellWidthd}{\fulllength/7-2\tabcolsep-0in}
%\setlength{\cellWidthe}{\fulllength/7-2\tabcolsep-0in}
%\setlength{\cellWidthf}{\fulllength/7-2\tabcolsep-0.5in}
%\setlength{\cellWidthg}{\fulllength/7-2\tabcolsep+0.5in}
%	   \begin{adjustwidth}{-\extralength}{0cm}
%		\begin{tabularx}{\fulllength}{>{\raggedright\arraybackslash}m{\cellWidtha}>{\raggedright\arraybackslash}m{\cellWidthb}>{\raggedright\arraybackslash}m{\cellWidthc}>{\raggedright\arraybackslash}m{\cellWidthd}>{\raggedright\arraybackslash}m{\cellWidthe}>{\raggedright\arraybackslash}m{\cellWidthf}>{\raggedright\arraybackslash}m{\cellWidthg}}
%% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        
%%		\begin{tabularx}{\fulllength}{RAHDMIE}
%		\toprule % --------------------------------------------------
%            \textbf{Reference}	& 
%            \textbf{Architecture} &
%            \textbf{Hybrid Status} & 
%            \textbf{Dataset} &
%            \textbf{Modality} &
%            \textbf{3D} &
%            \textbf{Eval. Metrics}\\
%%\begin{adjustwidth}{-\extralength}{0cm}%\centering

            \midrule
            
            \textbf{Cross-modal translation} &&&&&&\\
            \midrule
            \cite{chartsias2017adversarial} & CycleGAN & & Private & MR $\leftrightarrow$ CT & \checkmark & Dice\\
            \cite{wolterink2017deep} & CycleGAN & & Private & MR $\rightarrow$ CT & & MAE, PSNR\\
            \cite{nie2018medical} & Pix2Pix & & ADNI, Private & MR $\rightarrow$ CT & \checkmark & MAE, PSNR, Dice\\
            \cite{armanious2020medgan}  & MedGAN & & Private & PET $\rightarrow$ CT & & SSIM, PSNR, MSE\\
            &&&&&& VIF, UQI, LPIPS \\ 
            \cite{dar2019image}  & pGAN, CGAN & & BraTS2015, MIDAS, IXI & T1 $\longleftrightarrow$ T2 & & SSIM, PSNR\\
            \cite{jiang2019cross}  & CM-GAN & & Private & MR & & KLD, Dice \\
            &&&&&&hausdorff \\ 
            \cite{yurt2021mustgan}  & mustGAN & & IXI, ISLES & T1 $\leftrightarrow$ T2 $\leftrightarrow$ PD & & SSIM, PSNR\\
            \cite{yang2021synthesizing} & CAE-ACGAN & Hybrid (V~+~G) & Private & CT $\rightarrow$ MR & \checkmark & PSNR, SSIM, MAE\\
            \cite{sikka2021mri} & GLA-GAN & & ADNI & MR $\rightarrow$ PET & & SSIM, PSNR, MAE\\
            &&&&&& Acc., F1 \\ 
            \midrule
            \textbf{Other} & & & \\
            \midrule
            \cite{amirrajab2022pathology}  & VAE-CGAN & Hybrid (V~+~G) & ACDC & MR & \checkmark & -\\
		\bottomrule
		\end{tabularx}
	\end{adjustwidth}
	\noindent{\footnotesize{Note: V = variational autoencoders, G = generative adversarial networks.}}
\end{table}



\subsection{Variational Autoencoders}
Zhuang et al. \cite{zhuang2019fmri} present an empirical evaluation of 3D functional MRI data augmentation using deep generative models such as VAEs and GANs. The results indicate that CVAE and conditional WGAN can produce diverse, high-quality brain images. A 3D convolutional neural network (CNN) was used to further evaluate the generated samples on the original and augmented data in a classification task, demonstrating an accuracy improvement of $3.17\%$ when using CVAE augmented data and $3.72\%$ when using CWGAN augmented data. As part of Pesteie et al. \cite{pesteie2019adaptive}, a revised variant of the CVAE is proposed, called the ICVAE, which separates the embedding space of the input data and the conditioning variables. This allows the generated image characteristics to be independent of the conditioning variables, resulting in a more diverse output. In contrast, the standard CVAE encodes the data and conditioning variables in a shared embedding space. The authors evaluate the ICVAE on classification and segmentation tasks using transverse ultrasound images of the spine and FLAIR MRI images of the brain, respectively. The results demonstrate an improvement of $8.0\pm1.0\%$ in classification accuracy and $4.5\pm0.5\%$ in the Dice score compared to the model trained on real images only. The ICVAE model is able to generate more realistic MRI images by encoding appearance features independently of the structures in its latent space. The authors demonstrate the generation of synthetic MRI and ultrasound images using the ICVAE architecture, which are conditioned on a tumor segmentation mask and a label indicating the center-line of the spine, respectively. The CVAE architecture is also shown for comparison. Chadebec et al. \cite{chadebec2022data} introduce a novel Geometry-aware VAE for high dimensional data augmentation in low sample size settings. This model combines Riemannian metric learning with normalizing flows to improve the expressiveness of the posterior distribution and learn meaningful latent representations of the data. Additionally, the authors propose a new non-prior sampling scheme based on Hamiltonian Monte Carlo, since the standard procedure utilizing the prior distribution is highly dependent upon the data, especially for small datasets. As a result, the generated samples are remarkably more realistic than those generated by a conventional VAE, and the model is more resilient to the lack of data. An evaluation of the synthetic data on a classification task shows an improvement in accuracy from 66.3\% to 74.3\% using 50 real + 5000 synthetic MRIs, compared to using only the original data. The original paper by Chadebec et al. \cite{chadebec2022data} includes a challenge in which readers are invited to identify the real brain MRIs from fake ones.

Other studies suggest the use of VAEs to improve the segmentation task performance. Huo et al. \cite{huo2022brain} introduce a progressive VAE-based architecture (PAVAE) for generating synthetic brain lesions with associated segmentation masks. The authors propose a two-step pipeline where the first step consists in generating synthetic segmentation masks based on a conditional adversarial VAE. The CVAE is assisted by a ``condition embedding block'' that encodes high-level semantic information of the lesion into the feature space. The second step involves generating photorealistic lesion images conditioned on the lesion mask using ``mask embedding blocks'', which encodes the lesion mask into the feature space during generation, similar to SPADE. The authors compare their approach to other state-of-the-art methods and show that PAVAE can produce more realistic synthetic lesions with associated segmentation masks. A segmentation network is trained using both real and synthetic lesions and shows an improvement in the Dice score compared to the model trained only on real images (66.69\% to 74.18\%).

In a recent paper, Yang et al. \cite{yang2021synthesizing} propose a new model for cross-domain translation called conditional variational autoencoding GAN (CAE-ACGAN).  CAE-ACGAN combines the advantages of both VAEs and GANs in a single end-to-end architecture. The integration of VAE and GAN, along with the implementation of an auxiliary discriminative classifier network, allows for a partial resolution of the challenges posed by image blurriness and mode collapse. Moreover, the VAE incorporates skip connections between the encoder and decoder, which enhances the quality of the images generated. In addition to translating 3D CT images into their  corresponding MR, the CAE-ACGAN generates more realistic images as a result of its discriminator, which serves as a quality-assurance mechanism. Based on PSNR and SSIM scores, the CAE-ACGAN model showed a mild improvement over other state-of-the-art architectures, such as Pix2Pix and WGAN-GP \cite{gulrajani2017improved}.

Table \ref{tab:vae} compiles a summary of the relevant studies using VAEs in medical data augmentation. In contrast to GANs, the number of studies employing VAEs for data augmentation in medical imaging is relatively low. However, almost half of these studies have utilized hybrid architectures, combining VAEs with adversarial learning. Interestingly, we observe that unlike GANs, there are not many VAE variants in medical imaging. Most commonly used VAE architectures are either conditional, such as vanilla CVAE and ICVAE, or hybrid architectures, such as IntroVAE, PAVAE, and ALVAE. Further discussion on the effectiveness of VAEs for medical image augmentation and the specific architectures utilized in previous studies will be presented in Section \ref{sec4}.

\begin{table}[H]
\tablesize{\small}
\caption{\textls[-15]{Overview of VAE-based architectures for medical image augmentation, including hybrid status of architectures (if applicable), indicating the combination of VAEs and GANs used in each study}.\label{tab:vae}} 
\setlength{\cellWidtha}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthb}{\fulllength/7-2\tabcolsep+0.1in}
\setlength{\cellWidthc}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthd}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthe}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthf}{\fulllength/7-2\tabcolsep-0.5in}
\setlength{\cellWidthg}{\fulllength/7-2\tabcolsep+0.4in}
	   \begin{adjustwidth}{-\extralength}{0cm}
		\begin{tabularx}{\fulllength}{>{\raggedright\arraybackslash}m{\cellWidtha}>{\raggedright\arraybackslash}m{\cellWidthb}>{\raggedright\arraybackslash}m{\cellWidthc}>{\raggedright\arraybackslash}m{\cellWidthd}>{\raggedright\arraybackslash}m{\cellWidthe}>{\raggedright\arraybackslash}m{\cellWidthf}>{\raggedright\arraybackslash}m{\cellWidthg}}
% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
		\toprule % --------------------------------------------------
%	\begin{adjustwidth}{-\extralength}{0cm}
%		\newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.7\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.7\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
%		\toprule % --------------------------------------------------
            \textbf{Reference}	& 
            \textbf{Architecture} &
            \textbf{Hybrid Status} & 
            \textbf{Dataset} &
            \textbf{Modality} &
            \textbf{3D} &
            \textbf{Eval. Metrics}\\
		\midrule % ---------------------------------------------------
            \textbf{Classification} & & & \\
            \midrule
            \cite{pesteie2019adaptive} & ICVAE & & Private & MR & & Acc., Sens., Spec.\\
            &&&& Ultrasound &&Dice, Hausdroff, $\ldots$\\
            \cite{zhuang2019fmri}  & CVAE & & OpenfMRI, HCP & MR & \checkmark & Acc., Prec., F1\\  
            &&& NeuroSpin, IBC &&&Recall\\ 
            \cite{chadebec2022data}	& GA-VAE & & ADNI, AIBL & MR & \checkmark & Acc., Spec., Sens.\\
            \cite{terzopoulos2019multi}	& MAVENs & Hybrid (V~+~G) & APCXR & X-ray & & FID, F1\\
            \cite{hirte2021realistic} & IntroVAE & Hybrid (V~+~G)& Private & MR & & -\\
            \cite{qiang2021modeling}	& DR-VAE & & HCP & MR & & -\\
            \cite{ahmad2022brain}  & VAE-GAN & Hybrid (V~+~G) & Private & MR & & Acc., Sens., Spec.\\
            \cite{madan2022synthetic} & VAE & & Private & MR & & Acc.\\
            \cite{chadebec2021data} & RH-VAE & & OASIS &MR & \checkmark & Acc.\\

            \midrule
            \textbf{Segmentation} & & & \\
            \midrule
            \cite{liang2021data} 	& VAE-GAN & Hybrid (V~+~G) & Private & Ultrasound & & MMD, 1-NN, MS-SSIM\\
            \cite{gan2022esophageal}& AL-VAE & Hybrid (V~+~G) & Private & OCT \textsuperscript{1} & & MMD, MS, WD\\
            \cite{huo2022brain} 	& PA-VAE & Hybrid (V~+~G) & Private & MR & \checkmark & PSNR, SSIM, Dice \\
            &&&&&& NMSE, Jacc., $\ldots$\\
            \midrule
            
            \textbf{Cross-modal translation} & & & \\
            \midrule
            \cite{yang2021synthesizing}  & CAE-ACGAN & Hybrid (V~+~G) & Private & CT $\rightarrow$ MR & \checkmark & PSNR, SSIM, MAE\\
            \cite{hu2022domain}	& 3D-UDA & & Private & FLAIR $\leftrightarrow$ T1 $\leftrightarrow$ T2 & \checkmark & SSIM, PSNR, Dice\\

            \midrule
            \textbf{Other} & & & \\
            \midrule
            \cite{biffi2018learning} & CVAE & & ACDC, Private & MR   & \checkmark & - \\
            \cite{biffi2018learning} 	& CVAE & & Private & MR & \checkmark & Dice, Hausdorff\\
            \cite{volokitin2020modelling}& Slice-to-3D-VAE & & HCP & MR & \checkmark & MMD, MS-SSIM\\
            \cite{huang2022biomarkers}	& GS-VDAE & & MLSP & MR & & Acc.\\
            \cite{amirrajab2022pathology} & VAE-CGAN & Hybrid (V~+~G) & ACDC & MR & $\checkmark$ & -\\
            \cite{beetz2022combined}	& MM-VAE  & & UK Biobank & MR & $\checkmark$ & MMD\\
            \cite{sundgaard2022multi}	& DM-VAE & & Private & Otoscopy & & -\\
		\bottomrule
		\end{tabularx}
	\end{adjustwidth}
	\noindent{\footnotesize{\textsuperscript{1} OCT        stands for ``esophageal optical coherence             tomography''. V = variational autoencoders, G = generative adversarial networks.}}
\end{table}
 
% \newp
\subsection{Diffusion Models}
In their study, Pinaya et al. \cite{pinaya2022brain} introduce a new approach for generating high-resolution 3D MR images using a latent diffusion model (LDM) \cite{rombach2022high}. LDMs are a type of generative model that combine autoencoders and diffusion models to synthesize new data. The autoencoder component of the LDM compresses the input data into a lower-dimensional latent representation, while the diffusion model component generates new data samples based on this latent representation. The LDM in this work was trained on data from the UK Biobank dataset and conditioned on clinical variables such as age and sex. The authors compare the performance of their LDM to VAE-GAN \cite{vaegan} and LSGAN \cite{mao2016least}, using the Fréchet inception distance \cite{fid} as the evaluation metric. The results show that the LDM outperforms the other models, with an FID of 0.0076 compared to 0.1567 for VAE-GAN and 0.0231 for LSGAN (where a lower FID score indicates a better performance). Even when conditioned on specific variables, the synthetic MRIs generated by this model demonstrate  its ability to produce diverse and realistic brain MRI samples based on the ventricular volume and brain volume. As a valuable contribution to the scientific community, the authors also created a dataset of 100,000 synthetic MRIs that was made openly available for further research.

Fernandez et al. \cite{fernandez2022can} introduce a generative model, named  brainSPADE, for synthesizing labeled brain MRI images that can be used for training segmentation models. The model combines a diffusion model with a VAE-GAN, with the GAN component particularly utilizing SPADE normalization to incorporate the segmentation mask. The model consists of two components: a segmentation map generator and an image generator. The segmentation map generator is a VAE that takes as input a segmentation map, then encodes and builds a latent space from it. To focus on semantic information and disregard insignificant details, the latent code is then diffused and denoised using LDMs. This creates an efficient latent space that emphasizes meaningful information while filtering out noise and other unimportant details. A VAE decoder then generates an artificial segmentation map from this latent space. The image generator is a SPADE model that builds a style latent space from an arbitrary style and combines it with the artificial segmentation map to decode the final output image. The performance of the brainSPADE model is evaluated on a segmentation task using nnU-Net \cite{isensee2018nnu}, and the results show that the model performs comparably when trained on synthetic data compared to when it is trained on real data, and that using a combination of both significantly improves the model's performance.

Lyu and Wang \cite{lyu2022conversion} conducted a study that investigated the use of diffusion models for image translation in medical imaging, specifically the conversion of MRI to CT scans. In their study, the authors utilized two diffusion-based approaches: the conditional DDPM and conditional score-based model which utilizes stochastic differential equations \cite{song2020score}. These methods involved conditioning the reverse process on T2-weighted MRI images. To evaluate the performance of these diffusion models in comparison to other methods (conditional WGAN and U-Net), the authors conducted experiments on the Gold Atlas male pelvis dataset \cite{pelvicdataset} using three novel sampling methods and compared the results to those obtained using GAN- and CNN-based approaches. The results indicated that the diffusion models outperformed both the GAN- and CNN-based methods in terms of structural similarity index (SSIM) and peak signal-to-noise ratio (PNSR).

We present a summary of the relevant studies utilizing diffusion models for medical image augmentation in Table \ref{tab:dm}. This table includes details about the dataset, imaging modality, and evaluation metrics used in each study, as well as the specific diffusion model employed. Upon examining this table, we notice that all the studies included are relatively recent, with the earliest study dating back to 2022. This suggests that diffusion models have gained increasing attention in the field of medical image augmentation and synthesis in recent years. Additionally, we see that in 2022, diffusion models  received more attention for these tasks compared to GANs and VAEs, highlighting their growing popularity and potential for use in various scenarios.

\begin{table}[H]
\caption{Overview of the diffusion-model-based architectures for medical image augmentation that have been published to date (to our knowledge, no such studies were released before 2022). The table includes the reference, architecture name, and hybrid status (if applicable), indicating the combination of VAEs, GANs, and DMs used in each study. The table provides a useful summary of the current state of the art in this area and can help guide researchers in selecting appropriate approaches for their specific needs.\label{tab:dm}} 
\setlength{\cellWidtha}{\fulllength/7-2\tabcolsep-0.2in}
\setlength{\cellWidthb}{\fulllength/7-2\tabcolsep+0.2in}
\setlength{\cellWidthc}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthd}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthe}{\fulllength/7-2\tabcolsep-0in}
\setlength{\cellWidthf}{\fulllength/7-2\tabcolsep-0.5in}
\setlength{\cellWidthg}{\fulllength/7-2\tabcolsep+0.5in}
	   \begin{adjustwidth}{-\extralength}{0cm}
		\begin{tabularx}{\fulllength}{>{\raggedright\arraybackslash}m{\cellWidtha}>{\raggedright\arraybackslash}m{\cellWidthb}>{\raggedright\arraybackslash}m{\cellWidthc}>{\raggedright\arraybackslash}m{\cellWidthd}>{\raggedright\arraybackslash}m{\cellWidthe}>{\raggedright\arraybackslash}m{\cellWidthf}>{\raggedright\arraybackslash}m{\cellWidthg}}
% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
		\toprule % --------------------------------------------------
%	\begin{adjustwidth}{-\extralength}{0cm}
%	    \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.6\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.7\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
%		\toprule % --------------------------------------------------
            \textbf{Reference}	& 
            \textbf{Architecture} &
            \textbf{Hybrid Status} & 
            \textbf{Dataset} &
            \textbf{Modality} &
            \textbf{3D} &
            \textbf{Eval. Metrics}\\
		\midrule % ---------------------------------------------------
            \textbf{Classification} & & & & & & \\
            \midrule
		\cite{pinaya2022brain} &     
            CLDM & & UK Biobank & MR & \checkmark & FID, MS-SSIM\\
            \cite{dorjsembe2022three}  & DDPM & & ICTS & MR & \checkmark & MS-SSIM\\
            \cite{packhauser2022generation} 	& LDM &  & CXR8 & X-ray & & AUC\\
            \cite{moghadam2022morphology}	& MF-DPM & & TCGA & Dermoscopy & & Recall\\
            \cite{chambon2022roentgen}	& RoentGen & Hybrid (D~+~V) & MIMIC-CXR & X-ray & & Accuracy\\
            \cite{wolleb2022swiss}	& IITM-Diffusion & & BraTS2020 & MR & & - \\
            \cite{sagers2022improving}	& DALL-E2 & & Fitzpatrick & Dermoscopy & & Accuracy\\
            \cite{peng2022generating}	& CDDPM & & ADNI & MR & \checkmark & MMD, MS-SSIM, FID\\
            \cite{ali2023spot} & DALL-E2 & & Private & X-ray & & - \\
            \cite{saeed2023bi} & DDPM & & OPMR & MR & \checkmark & Acc., Dice\\
            \cite{weber2023cascaded} & LDM & & MaCheX & X-ray & & MSE, PSNR, SSIM\\
 	          \midrule
            \textbf{Segmentation} & & & & & \\
            \midrule
            \cite{khader2022medical}	& DDPM & & ADNI, MRNet, & MR, CT & & Dice\\
            &&& LIDC-IDRI &&&\\ 
            \cite{fernandez2022can}		& brainSPADE & Hybrid (V~+~G~+~D) & SABRE, BraTS2015 & MR & & Dice, Accuracy \\
            &&& OASIS, ABIDE &&& Precision, Recall \\
            \cite{wolleb2022swiss}		& IITM-Diffusion & & BraTS2020 & MR & & - \\

            \midrule
            \textbf{Cross-modal translation} & & & & & \\
            \midrule
            \cite{ozbey2022unsupervised} & SynDiff & Hybrid (D~+~G) & IXI, BraTS2015 & CT $\rightarrow$ MR & & PSNR, SSIM \\
            &&& MRI-CT-PTGA &&& \\
            \cite{meng2022novel}	& UMM-CSGM & & BraTS2019 & FLAIR $\leftrightarrow$ T1 $\leftrightarrow$ T1c $\leftrightarrow$ T2 & & PSNR, SSIM, MAE\\
            \cite{lyu2022conversion}& CDDPM & & MRI-CT-PTGA & CT $\leftrightarrow$ MR &  & PSNR, SSIM \\
            \midrule
            \textbf{Other} & & & & & \\
            \midrule
            \cite{kim2022diffusion} & DDM & & ACDC & MR   & \checkmark & PSNR, NMSE, DICE\\
		\bottomrule
		\end{tabularx}
	\end{adjustwidth}
	\noindent{\footnotesize{Note: V = variational autoencoders, G = generative adversarial networks, D = diffusion models.}}
\end{table}

\section{Key Findings and Implications}\label{sec4}
In this review, we focused on generative deep models applied to medical data augmentation, specifically VAEs, GANs, and diffusion models. These approaches each have their own strengths and limitations, as described by the generative learning trilemma \cite{xiao2021tackling}, which states that it is generally difficult to achieve high-quality sampling, fast sampling, and mode coverage simultaneously. As illustrated in Figure \ref{fig:stats}a, the number of publications on data augmentation using VAEs increases by approximately 81\% from 2017 to 2022, while the number using GANs has remained relatively stagnant. This trend may be due to the fact that most possible fields of research using GANs have already been explored, making it difficult to go beyond current methods using these architectures. However, we have also seen an increase in the use of more complex architectures combining multiple generative models \cite{ahmad2022brain,yang2021synthesizing}, which have shown promising results in terms of both quality and mode coverage. On the other hand, the number of studies using diffusion models has drastically increased starting from 2022, and these models have shown particular potential for synthesizing high-quality images with good mode coverage \cite{kazerouni2022diffusion}. 

{\color{black} Basic data augmentation operators such as Gaussian noise addition, cropping, and padding are commonly used to augment data and generate new images for training~ \cite{krizhevsky2017imagenet}. However, the complex structures of medical images, which encompass anatomical variation and irregular tumor shapes, may render these basic operations unsuitable, resulting in the production of irrelevant images that disrupt the logical image structure \cite{chlap2021review}, and additionally, can lead to image deformations and the generation of aberrant data that can adversely impact the model performance. One basic data augmentation operator that is not well suited for medical images is flipping images, which can sometimes cause anatomical inconsistencies \cite{abdollahi2020data}. To overcome this issue, deformable augmentation techniques have been introduced, such as random displacement fields and spline interpolation, to augment the data in a more realistic way. These techniques have proved to be useful \cite{chlap2021review}; however, they are strongly dependent on the data and limited in some cases. Recent advances in deep learning have led to the development of generative models that can be trained to generate realistic images and simulate the underlying data distribution. These synthesized images are more truthful than those generated using traditional data augmentation techniques. They guarantee a better coherence of the general structure of medical images and greater variability, providing a more effective way to generate realistic and diverse data.}


{\color{black} The use of GANs in medical imaging, as seen in Table \ref{tab:gan}, has been widespread and applied to a variety of modalities and datasets, demonstrating their versatility and potential for various applications within the field. When it comes to classification, DCGAN and WGAN have been the most-commonly used architectures and are considered safe bets in this domain. For example, Zhuang et al. \cite{zhuang2019fmri} demonstrated a 3\% accuracy improvement in generating fMRIs using an improved WGAN. These architectures, with their capacity for high-quality generation and good mode coverage, offer significant potential for the generation of synthetic images for medical imaging classification. In the case of segmentation and translation, the architectures that have shown the most promise include Pix2Pix, CycleGAN, and SPADE, all of which have proven their potential for conditional generation and cross-modal translation. Platscher et al. \cite{platscher2020image} conducted a comparative study of these three architectures, demonstrating their capacity to generate high-quality images suitable for medical image segmentation and translation tasks (improvement of 9.1\% in Dice score). These architectures can significantly reduce the need for manual annotation of medical images and thus significantly reduce the time and cost required for data annotation.}

{\color{black} On the other hand, VAEs have been utilized in fewer studies for medical image augmentation, as shown in Table \ref{tab:vae}. They have been employed in other tasks such as reconstruction, as demonstrated by Biffi et al. \cite{biffi2018learning} and Volokitin et al. \cite{volokitin2020modelling}, who used CVAE for 3D volume reconstruction, and interpretability of features, as exemplified by Hyang et al. \cite{huang2022biomarkers}, who identified biomarkers using VAEs. Furthermore, VAEs are often used in hybrid architectures with adversarial learning techniques. The most promising architectures include PAVAE \cite{huo2022brain} and IntroVAE \cite{huang2018introvae}, alongside conditional VAEs, for various purposes including classification, segmentation, and translation tasks. However, while VAEs have shown potential in these areas, there is still room for improvement. One study that particularly shows promising results is that of Chadebec and Allassonnière~\cite{chadebec2021data}, who propose to model the latent space of a VAE as a Riemannian manifold, allowing high-quality image generation comparable to GANs. Chadebec and Allassonnière \cite{chadebec2021data} demonstrated an improvement of 8\% in accuracy using synthetic images generated with their proposed VAE model. Nevertheless, this architecture requires a high computational cost and time, which is a significant drawback in practical applications.}

Table \ref{tab:dm} presents a summary of the relevant studies utilizing diffusion models for medical image augmentation. These studies, all of which are relatively recent, with the earliest dating back to 2022, suggest that diffusion models have gained increasing attention in medical image augmentation and synthesis in recent years. Furthermore, in 2022, diffusion models have been the most-commonly used generative models for medical image augmentation compared to GANs and VAEs, highlighting their growing popularity and potential for use in various scenarios. {\color{black} Of the diffusion models studied, DDPM and LDM are the most prevalent, alongside conditional variants such as CDDPM \cite{lyu2022conversion} and CLDM \cite{pinaya2022brain}. Notably, the difference between LDM and DDPM is the ability of LDM to model long-range dependencies within the data by constructing a low-dimensional latent representation and diffusing it, while DDPMs apply the diffusion process directly to the input images. This can be especially useful for medical image augmentation tasks that require capturing complex patterns and structures. For instance, Saeed et al. \cite{saeed2023bi} demonstrated the capacity of LDM conditioned on text for a task of lesion identification, achieving an accuracy improvement of 5.8\%. These findings suggest that diffusion models have a promising potential for future medical image augmentation and synthesis research. To further exemplify the potential of diffusion models in generating realistic medical images, we present in Figure \ref{fig:dm} a set of synthesized MRI images using a DDPM. These generated images exhibit high visual fidelity and are almost indistinguishable from the real images. One of the reasons for this high quality is  the DDPM's ability to model the diffusion process of the image density function. By doing so, the DDPM can generate images with increased sharpness and fine details, as seen in the synthesized MRI images.}

% Figure environment removed

These studies have covered a range of modalities, including MRI, CT, and ultrasound, as well as dermoscopy and otoscopy. Classification is the most common downstream task targeted in these studies, but there have also been multiple state-of-the-art solutions proposed for more complex tasks such as generating multimodal missing images (e.g., from CT to MRI) and multi-contrast MRI images. In order to provide ground truth segmentation masks for tasks such as segmentation, most studies have explored the field of conditional synthesis. This allows for greater control over the synthesized images and can help to stabilize training \cite{mirza2014conditional}, as the model is given explicit guidance on the desired output. For our discussion on medical image augmentation, we have also compiled two summary tables to provide a comprehensive overview of the datasets and metrics used in the reviewed studies. Table \ref{tab:datasets} presents a summary of the datasets used in the reviewed studies. This table includes information about the title of the dataset, a reference, and a link to the public repository if available, as well as the studied modality and anatomy. From examining this table, we see that MRI is the most-commonly used modality, followed by CT. In terms of anatomy, brain studies dominate, with lung studies coming in second. It is worth noting that the BraTS dataset is widely used across multiple studies, highlighting its importance in the field. Additionally, we notice the presence of private datasets in this table, which is not surprising given that many medical studies are associated with specific medical centers and may not be publicly available. When we consider the state of the art of medical imaging studies (see Figure \ref{fig:stats}b), we notice that the PET and ultrasound modalities are less represented compared to the others. One reason for the scarcity of PET studies is the limited availability of nuclear doctors compared to radiologists. Nuclear doctors specialize in nuclear medicine, and PET is one such imaging modality that uses radioactive tracers to produce 3D images of the body. Due to the limited number of nuclear doctors, there are fewer medical exams that use PET, leading to less publicly available data for research purposes \cite{amyar2020radiogan}. On the other hand, ultrasound is an operator-dependent modality and requires a certain level of field knowledge. Additionally, ultrasound is not as effective as other modalities such as CT and MRI in detecting certain pathologies, which may also contribute to its lower representation in the state of the art. Despite these limitations, both PET and ultrasound remain important imaging modalities in clinical practice, and future research should aim to explore their full potential in the field of medical imaging.

\begin{table}[H]
\tablesize{\small}
\caption{Summary of the datasets utilized in various publications of deep generative models, organized by modality and body part. For each dataset, the corresponding availability is indicated as public, private, or under certain conditions (UC). Additionally, if a public link for the dataset is available, it is provided.\label{tab:datasets}} 
\setlength{\cellWidtha}{\fulllength/6-2\tabcolsep-0.1in}
\setlength{\cellWidthb}{\fulllength/6-2\tabcolsep-0.4in}
\setlength{\cellWidthc}{\fulllength/6-2\tabcolsep-0.3in}
\setlength{\cellWidthd}{\fulllength/6-2\tabcolsep+1.1in}
\setlength{\cellWidthe}{\fulllength/6-2\tabcolsep-0.2in}
\setlength{\cellWidthf}{\fulllength/6-2\tabcolsep-0.1in}
%\setlength{\cellWidthg}{\fulllength/7-2\tabcolsep-0in}
	   \begin{adjustwidth}{-\extralength}{0cm}
		\begin{tabularx}{\fulllength}{>{\raggedright\arraybackslash}m{\cellWidtha}>{\raggedright\arraybackslash}m{\cellWidthb}>{\raggedright\arraybackslash}m{\cellWidthc}>{\raggedright\arraybackslash}m{\cellWidthd}>{\raggedright\arraybackslash}m{\cellWidthe}>{\raggedright\arraybackslash}m{\cellWidthf}%>{\raggedright\arraybackslash}m{\cellWidthg}
		}
% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
		\toprule % --------------------------------------------------
%	\begin{adjustwidth}{-\extralength}{0cm}
%	\newcolumntype{A}{>{\hsize=.6\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{B}{>{\hsize=1.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{C}{>{\hsize=.4\hsize\arraybackslash\footnotesize}X}
%		\begin{tabularx}{\fulllength}{AACBCC}
%		\toprule
            \textbf{Abbreviation} & \textbf{Reference}	& \textbf{Availability} & \textbf{Dataset} & \textbf{Modality} & \textbf{Anatomy}\\
		\midrule
            %\href{https://adni.loni.usc.edu}
            {ADNI} & & UC & Alzheimers disease \mbox{neuroimaging Initiative} & MR, PET & Brain\\
            %\href{https://www.smir.ch/BRATS/Start2015}
            {BraTS2015} & & Public & Brain tumor segmentation challenge & MR & Brain\\
            %\href{https://www.smir.ch/BRATS/Start2016}
            {BraTS2016} & & Public & Brain tumor segmentation challenge & MR & Brain\\
            %\href{https://www.med.upenn.edu/sbia/brats2017.html}
            {BraTS2017} & & Public & Brain tumor segmentation challenge & MR & Brain\\
            %\href{https://www.med.upenn.edu/cbica/brats2019/data.html}
            {BraTS2019} & & Public & Brain tumor segmentation challenge & MR & Brain\\
            %\href{https://www.med.upenn.edu/cbica/brats2020/data.html}
            {BraTS2020} & & Public & Brain tumor segmentation challenge & MR & Brain\\
            %\href{https://github.com/ieee8023/covid-chestxray-dataset}
            {IEEE CCX} & & Public & IEEE Covid Chest X-ray dataset & X-ray & Lung\\
            %\href{https://www.ukbiobank.ac.uk}
            {UK Biobank} & & UC & UK Biobank & MR & Brain, Heart\\
            %\href{https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT}
            {NIHPCT} & & Public &National Institutes of Health Pancreas-CT dataset & CT & Kidney\\
            %\href{http://medicaldecathlon.com}
            {DataDecathlon} & & Public & Medical Segmentation \mbox{Decathlon dataset} & CT & Liver, Spleen\\
            %\href{https://midas.umich.edu/research-datasets/}
            {MIDAS} & \cite{midasdataset} & Public & Michigan institute for data science & MR & Brain\\
            %\href{http://brain-development.org/ixi-dataset/}
            {IXI} & & Public & Information e\textbf{X}traction from \mbox{Images Dataset} & MR & Brain\\
            %\href{https://drive.grand-challenge.org}
            {DRIVE} &\cite{drivedataset} &  Public & Digital Retinal Images for \mbox{Vessel Extraction} & Fundus photography & Retinal fundus\\
            %\href{https://acdc.creatis.insa-lyon.fr/description/databases.html}
            {ACDC}& \cite{acdcdataset} &  Public & Automated Cardiac \mbox{Diagnosis Challenge} & MR & Heart\\
            %\href{}
            {MRI-CT PTGA} & \cite{pelvicdataset} &  Public & MRI-CT Part of the Gold Atlas project & CT, MR & Pelvis\\
            %\href{}
            {ICTS} & \cite{kwon2019generation} & Public & National Taiwan University Hospital’s Intracranial Tumor \mbox{Segmentation dataset} & MR & Brain\\
            %\href{https://nihcc.app.box.com/v/ChestXray-NIHCC}
            {CXR8} & \cite{wang2017chestx} & Public & ChestX-ray8 & X-ray & Lung\\
            %\href{https://medicalsegmentation.com/covid19/}
            {C19CT} & & Public & COVID-19 CT segmentation dataset & CT & Lung\\
            %\href{https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga/using-tcga}
            {TCGA} & & Private & The Cancer Genome Atlas Program
             & Microscopy & -\\
             %\href{https://github.com/UK-Digital-Heart-Project}
             {UKDHP} & \cite{bai2015bi} & UC & UK Digital Heart Project & MR & Heart\\
             %\href{https://www.isi.uu.nl/Research/Databases/SCR/}
             {SCR} & \cite{van2006segmentation} & Public & SCR database : Segmentation in \mbox{Chest Radiographs} & X-ray & Lung\\
             %\href{https://www.humanconnectome.org/study/hcp-young-adult/data-releases}
             {HCP} & \cite{van2013wu} & Public & Human connectom project dataset & MR & Brain\\
            %\href{https://adni.loni.usc.edu/aibl-australian-imaging-biomarkers-and-lifestyle-study-of-ageing-18-month-data-now-released/}
            {AIBL} &
            & UC & Australian Imaging Biomarkers and Lifestyle Study of Ageing & MR, PET & Brain\\
            %\href{http://openfmri.org}
            {OpenfMRI} & & Public & OpenfMRI & MR & Brain\\
            %\href{https://project.inria.fr/IBC/}
            {IBC} & & Public & Individual Brain Charting & MR & Brain\\
            %\href{https://joliot.cea.fr/drf/joliot/Pages/Entites_de_recherche/NeuroSpin.aspx}
{NeuroSpin} & & Private & Institut des sciences du vivant \mbox{Frédéric Joliot} & MR & Brain\\

          {OASIS} & & Public & The Open Access Series of \mbox{Imaging Studies} & MR & Brain\\
            %\href{https://data.mendeley.com/datasets/9xkhgts2s6/3}
            {APCXR} & \cite{kermany2018identifying} & Public & The anterior-posterior Chest \mbox{X-Ray dataset} & X-ray & Lung\\
            %\href{https://github.com/mattgroh/fitzpatrick17k}
            {Fitzpatrick} & \cite{groh2021evaluating} & Public & Fitzpatrick17k dataset & Dermoscopy & Skin\\
            
                        {ISIC} & & Public & The International Skin Imaging Collaboration dataset & Dermoscopy & Skin\\

\bottomrule
		\end{tabularx}
	\end{adjustwidth}
\end{table}
 
\begin{table}[H]\ContinuedFloat
\tablesize{\small}
\caption{\textit{Cont.} \label{tab:datasets}} 
\setlength{\cellWidtha}{\fulllength/6-2\tabcolsep-0.1in}
\setlength{\cellWidthb}{\fulllength/6-2\tabcolsep-0.4in}
\setlength{\cellWidthc}{\fulllength/6-2\tabcolsep-0.3in}
\setlength{\cellWidthd}{\fulllength/6-2\tabcolsep+1.1in}
\setlength{\cellWidthe}{\fulllength/6-2\tabcolsep-0.2in}
\setlength{\cellWidthf}{\fulllength/6-2\tabcolsep-0.1in}
%\setlength{\cellWidthg}{\fulllength/7-2\tabcolsep-0in}
	   \begin{adjustwidth}{-\extralength}{0cm}
		\begin{tabularx}{\fulllength}{>{\raggedright\arraybackslash}m{\cellWidtha}>{\raggedright\arraybackslash}m{\cellWidthb}>{\raggedright\arraybackslash}m{\cellWidthc}>{\raggedright\arraybackslash}m{\cellWidthd}>{\raggedright\arraybackslash}m{\cellWidthe}>{\raggedright\arraybackslash}m{\cellWidthf}%>{\raggedright\arraybackslash}m{\cellWidthg}
		}
% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
		\toprule % --------------------------------------------------
%	\begin{adjustwidth}{-\extralength}{0cm}
%	\newcolumntype{A}{>{\hsize=.6\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{B}{>{\hsize=1.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{C}{>{\hsize=.4\hsize\arraybackslash\footnotesize}X}
%		\begin{tabularx}{\fulllength}{AACBCC}
%		\toprule
            \textbf{Abbreviation} & \textbf{Reference}	& \textbf{Availability} & \textbf{Dataset} & \textbf{Modality} & \textbf{Anatomy}\\
		\midrule           
            %\href{http://www.oasis-brains.org}
  
            %\href{https://challenge2020.isic-archive.com/}

            %\href{https://www.kaggle.com/datasets/skooch/ddsm-mammography}
            {DDSM}& & Public & The Digital Database for Screening Mammography & CT & Breast\\
            %\href{https://www.kaggle.com/datasets/awsaf49/cbis-ddsm-breast-cancer-image-dataset}
            {CBIS-DDMS}& & Public & Curated Breast Imaging Subset \mbox{of DDSM} & CT & Breast\\
            %\href{https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI}
            {LIDC-IDRI} & & Public & The Lung Image Database Consortium (LIDC) and Image Database Resource Initiative (IDRI) & CT & Lung\\
            %\href{https://github.com/UCSD-AI4H/COVID-CT}
            {COVID-CT} & \cite{yang2020covid} & Public & - & CT & Lung\\
            %\href{https://www.kaggle.com/datasets/plameneduardo/sarscov2-ctscan-dataset}
            {SARS-COV2} & \cite{soares2020sars} & Public &  & CT & Lung\\
            %\href{https://physionet.org/content/mimic-cxr/2.0.0/}
            {MIMIC-CXR} & \cite{johnson2019mimic} & Public & Massachusetts Institute of Technology & CT & Lung\\

            %\href{https://www.ppmi-info.org/access-data-specimens/download-data}
            {PPMI} & & Public & Parkinson's Progression \mbox{Markers Initiative} & MR & Brain\\

            %\href{http://fcon_1000.projects.nitrc.org/indi/adhd200/}
            {ADHD}& & Public & Attention Deficit \mbox{Hyperactivity Disorder} & MR & Brain\\
            %\href{https://stanfordmlgroup.github.io/competitions/mrnet/}
            {MRNet}& & Public & MRNet dataset & MR & Knee\\
            %\href{https://www.kaggle.com/c/mlsp-2014-mri}
            {MLSP}& & Public & MLSP 2014 Schizophrenia Classification Challenge & MR & Brain\\
            %\href{}
            {SABRE} & \cite{jones2020cohort} & Public & The Southall and Brent
            Revisited cohort & MR & Brain, Heart\\
            %\href{https://fcon_1000.projects.nitrc.org/indi/abide/}
            {ABIDE} &  & Public & The Autism Brain Imaging \mbox{Data Exchange} & MR & Brain\\
            %\href{}
            {OPMR} & \cite{saha2022artificial} & Public & Open-source prostate MR data & MR & Pelvis\\
           % \href{https://github.com/saiboxx/machex}
            {MaCheX} & \cite{weber2023cascaded} & Public & Massive Chest X-ray Dataset & X-ray & Lung\\
		\bottomrule
		\end{tabularx}
	\end{adjustwidth}
\end{table}
 


Second, Table \ref{tab:metrics} provides a summary of the metrics used to evaluate the performance of the various models discussed in the review. It is clear from this table that a variety of metrics are employed, ranging from traditional evaluation measures to more recent ones. Currently, many studies rely on shallow metrics such as the mean absolute error, peak signal-to-noise ratio \cite{kynkaanniemi2019improved}, or structural similarity \cite{ssim}, which do not accurately reflect the visual quality of the image. For instance, while optimizing pixel-wise loss can produce a clearer image, it may result in lower numerical scores compared to using adversarial loss \cite{ledig2017photo}. To address this challenge, researchers have proposed different methods for evaluation. The most well-known approach is to validate the quality of the generated samples through downstream tasks such as segmentation or classification. {\color{black} An overview of the augmentation process using a downstream task is depicted in Figure \ref{fig:aug_flow}}. Another approach is to use deep-learning-based metrics such as the learned perceptual image patch similarity (LPIPS) \cite{zhang2018unreasonable}, Fréchet inception distance (FID) \cite
{fid}, or inception score (IS) \cite{salimans2016improved}, which are designed to better reflect human judgments of image quality. These deep-learning-based metrics take into account not only pixel-wise similarities, but also high-level features and semantic information in the images, making them more effective in evaluating the visual quality of the generated images. LPIPS, for instance, measures the perceptual similarity between two images by using a pretrained deep neural network. FID and IS are other popular deep-learning-based metrics for image generation, and they have been widely used in various image generation tasks to assess the quality and diversity of the generated samples. However, these metrics may not always align perfectly with human perception, and further studies are needed to assess their effectiveness for different types of medical images.

\begin{table}[H]
\tablesize{\footnotesize}
\caption{Summary of quantitative measures used in the reviewed publications.\label{tab:metrics}} 
\setlength{\cellWidtha}{\fulllength/4-2\tabcolsep-1in}
\setlength{\cellWidthb}{\fulllength/4-2\tabcolsep-1in}
\setlength{\cellWidthc}{\fulllength/4-2\tabcolsep-0.5in}
\setlength{\cellWidthd}{\fulllength/4-2\tabcolsep+2.5in}
%\setlength{\cellWidthe}{\fulllength/6-2\tabcolsep-0.1in}
%\setlength{\cellWidthf}{\fulllength/6-2\tabcolsep-0.1in}
%\setlength{\cellWidthg}{\fulllength/7-2\tabcolsep-0in}
	   \begin{adjustwidth}{-\extralength}{0cm}
		\begin{tabularx}{\fulllength}{>{\raggedright\arraybackslash}m{\cellWidtha}>{\raggedright\arraybackslash}m{\cellWidthb}>{\raggedright\arraybackslash}m{\cellWidthc}>{\raggedright\arraybackslash}m{\cellWidthd}%>{\raggedright\arraybackslash}m{\cellWidthe}>{\raggedright\arraybackslash}m{\cellWidthf}%>{\raggedright\arraybackslash}m{\cellWidthg}
		}
% \newcolumntype{R}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{A}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{H}{>{\hsize=0.75\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.5\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{I}{>{\hsize=0.2\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{E}{>{\hsize=0.8\hsize\arraybackslash\footnotesize}X}
        
%		\begin{tabularx}{\fulllength}{RAHDMIE}
		\toprule % --------------------------------------------------

%	\begin{adjustwidth}{-\extralength}{0cm}
%        \newcolumntype{A}{>{\hsize=0.3\hsize\arraybackslash\footnotesize}X}
%		\newcolumntype{R}{>{\hsize=0.45\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{M}{>{\hsize=0.6\hsize\arraybackslash\footnotesize}X}
%        \newcolumntype{D}{>{\hsize=1.5\hsize\arraybackslash\footnotesize}X}
%		\begin{tabularx}{\fulllength}{ARMD}
%		\toprule % --------------------------------------------------
            \textbf{{\footnotesize Abbrv.}}	& 
            \textbf{{\footnotesize Reference}} &
            \textbf{{\footnotesize Metric Name}} & 
            \textbf{{\footnotesize Description}} \\
		\midrule % ---------------------------------------------------
        Dice & \cite{sudre2017generalised} & Sørensen–Dice coefficient & A measure of the similarity between two sets of data, calculated as twice the size of the intersection of the two sets divided by the sum of the sizes of the two sets\\
        Hausdorff & \cite{rockafellar2009variational} & Hausdorff distance & A measure of the similarity between two sets of points in a metric space\\
        FID & \cite{fid} & Fréchet inception distance & A measure of the distance between the distributions of features extracted from real and generated images, based on the activation patterns of a pretrained inception model\\
        IS & \cite{salimans2016improved} & Inception score & A measure of the quality and diversity of generated images, based on the activation patterns of a pretrained Inception model\\
        MMD & \cite{gretton2012kernel} & Maximum mean discrepancy & A measure of the difference between two probability distributions, defined as the maximum value of the difference between the two means\\
        1-NN & \cite{cover1967nearest} & 1-nearest neighbor score & A method for classification or regression that involves finding the data point in a dataset that is most similar to a given query point\\
        (MS-)SSIM & \cite{ssim}& (Multi-scale) structural similarity & A measure of the similarity between two images based on their structural information, taking into account luminance, contrast, and structure.\\
        MS & \cite{bounliphone2015test} & Mode score & A measure of the quality of samples generated with two probabilistic generative models based on the difference in maximum mean discrepancies between a reference distribution and simulated distribution\\
        WD & \cite{vaserstein1969markov} & Wasserstein distance & A measure of the distance between two probability distributions, defined as the minimum amount of work required to transform one distribution into the other\\
        PSNR & \cite{kynkaanniemi2019improved} & Peak signal-to-noise ratio & A measure of the quality of an image or video, based on the ratio between the maximum possible power of a signal and the power of the noise that distorts the signal\\
        (N)MSE & - & (Normalized) mean squared error & A measure of the average squared difference between the predicted and \mbox{actual values}\\
        Jacc. & \cite{sudre2017generalised} & Jaccard index & A measure of the overlap between two sets of data, calculated as the ratio of the area of intersection to the area of union\\
        MAE & - & Mean absolute error & A measure of the average magnitude of the errors between the predicted and \mbox{actual values}\\
        AUC & \cite{fawcett2006introduction} & Area under the curve & A measure of the performance of a binary classifier, calculated as the area under the receiver operating characteristic curve\\
        LPIPS & \cite{zhang2018unreasonable} & Learned perceptual image patch similarity & An evaluation metric that measures the distance between two images in a perceptual space based on the activation of a deep CNN\\
        KLD & \cite{nguyen2010estimating} & Kullback--Leibler divergence & A measure of the difference between two probability distributions, often used to compare the similarity of the distributions, with a smaller KL divergence indicating a greater similarity\\
        VIF & \cite{vif} & Visual information fidelity &  A measure that quantifies the Shannon information that is shared between the reference and the distorted image\\
        UQI & \cite{uqi} & Universal quality index & A measure of the quality of restored images. It is based on the principle that the quality of an image can be quantified using the correlation between the original and restored images\\
		\bottomrule
		\end{tabularx}
	\end{adjustwidth}
\end{table}
 \vspace{-12pt}
% Figure environment removed



Despite the advancements made by generative models in medical data augmentation, several challenges still remain. A common issue in GANs, known as mode collapse, occurs when the generator only produces a limited range of outputs, rather than the full range of possibilities. While techniques such as minibatch discrimination and the incorporation of auxiliary tasks \cite{salimans2016improved} have been suggested as potential solutions, further research is needed to effectively address this issue. In addition, there is a balance to be struck between the sample quality and the generation speed, which affects all generative models. GANs are known for their ability to generate high-quality samples quickly, allowing them to be widely used in medical imaging and data augmentation \cite{yi2019generative,tavse2022systematic,ali2022role}. Another approach for stabilizing the training of GANs is to use WGAN \cite{arjovsky2017wasserstein}. WGAN improves upon the original GAN by using the Wasserstein distance instead of the Jensen--Shannon divergence as the cost function for training the discriminator network. While these approaches have demonstrated success in improving GAN images and partially addressing mode collapse and training instability, there is still room for improvement. Diffusion models have overshadowed GANs during the latest years, particularly due to the success of text-to-image generation architectures such as DALL-E \cite{ramesh2021zero}, Imagen \cite{saharia2022photorealistic}, and stable diffusion \cite{rombach2022high}. These diffusion models naturally produce more realistic images than GANs. However, in our view, GANs have only been set aside and not entirely disregarded. With the recent release of GigaGAN~ \cite{kang2023scaling} and StyleGAN-T \cite{sauer2023stylegan}, GANs have made a resurgence by producing comparable or even better results than diffusion models. This renewed interest in GANs demonstrates the continued relevance of this approach to image generation and indicates that GANs may still have much to offer in advancing the field. Future research could explore hybrid models that combine the strengths of both GANs and diffusion models to create even more realistic and high-quality images.

VAEs have not gained as much attention in the medical imaging field, due to their tendency to produce blurry and hazy generated images. However, some studies have used conditional VAEs or hybrid architectures to address this issue and improve the quality of the samples produced. Researchers are therefore exploring the use of hybrid models that combine the strengths of multiple generative models, as well as improved VAE variations that offer enhanced image quality. Hybrid architectures, such as VAE-GANs~\cite{vaegan}, have demonstrated the potential to partially address the issues of both VAEs and GANs, allowing a better-quality generation and good mode coverage. {\color{black} Interestingly, recent research has even combined all three generative models into a single pipeline \cite{fernandez2022can}. This study has shown comparable results on a segmentation task when using a fully synthetic dataset compared to using the real dataset. These promising results suggest that hybrid architectures could open up new possibilities.} However, these models can be complex and challenging to train, and more research is needed to fully realize their potential. In fact, many VAEs used in medical imaging are hybrid architectures, as they offer a good balance between the strengths and weaknesses of both VAEs and GANs \cite{fernandez2022can,terzopoulos2019multi}. {\color{black} It is important to note that VAEs have an advantage over GANs in operating better with smaller datasets due to the presence of an encoder \cite{delgado2021deep}, which can extract relevant features from the input images and significantly reduce the search space required for generating new images through the process of reconstruction. This feature also makes VAEs a form of dimensionality reduction, and the representation obtained by the encoder can provide a better starting point for the decoder to approximate the real data distribution more accurately. In contrast, GANs have a wider search space, which may lead to challenges in learning features effectively. For instance, we show in Figure \ref{fig:vae} a comparison between synthesized MRIs using vanilla VAE \cite{kingma2013auto} and the Hamiltonian VAE \cite{caterini2018hamiltonian}. In addition to the advantage of operating better with smaller datasets, VAEs also offer a disentangled, interpretable, and editable latent space. This means that the encoded representation of an input image can be separated into independent and interpretable features, allowing for better understanding and manipulation of the underlying data.} Another option is the use of improved variants of VAEs, which have been proposed to generate high-quality images. There has been limited exploration of improved VAE variants such as VQ-VAE2 \cite{vqvae2}, IAF-VAE \cite{iafvae}, or Hamiltonian VAE \cite{caterini2018hamiltonian} in the medical imaging field, but these variants have shown promise in generating high-quality images in other domains. It may be worth exploring their potential for medical image augmentation, as they offer the possibility of improving the quality of the generated images without sacrificing other important characteristics such as fast sampling and good mode coverage.

% Figure environment removed  

Diffusion models have more recently been applied to medical imaging \cite{kazerouni2022diffusion}, and some studies have demonstrated high-quality results \cite{dorjsembe2022three}. These models are capable of synthesizing highly realistic images and have a good mode coverage while keeping the training stable, but suffer from a long sampling time due to the high number of steps in the diffusion process. This limitation may be less significant in medical imaging applications, which are not typically used in real time, but researchers are likely to continue working on optimizing diffusion models for faster sampling. It may also be possible to trade off some sample quality for faster sampling in diffusion models \cite{xiao2021tackling}, as realism is a key requirement for data augmentation in medical imaging. For example, Song et al. \cite{song2020denoising} proposes a variant of diffusion models called Training-free Denoising Diffusion Implicit Model (DDIM) aimed to speed up the sampling process by replacing the Markovian process with a non-Markovian one in the DDPM. This resulted in a faster sampling procedure that did not significantly compromise the quality of the samples. Fast Diffusion Probabilistic Model (FastDPM) \cite{kong2021fast} introduces the concept of a continuous diffusion process with smaller time steps in order to reduce the sampling time. These efforts to improve the efficiency of diffusion models demonstrate the ongoing interest in finding ways to balance the sample quality and generation speed in medical imaging applications.

\textls[-25]{There are several other factors to consider when discussing the use of generative models for medical data augmentation. One important factor is the incorporation of domain-specific techniques and knowledge into the design of these models \cite{he2022effect}. By incorporating knowledge of anatomy and physiology, for example, researchers can improve the realism and utility of the generated data. Another important factor is the ethical considerations of using synthetic data for medical applications, including the potential for biased or unrealistic generated data and the need for proper validation and testing. To further improve the performance and efficiency of medical data augmentation, researchers are also exploring the use of generative models in combination with other techniques such as transfer learning \cite{talo2019application,shorten2019survey} or active learning \cite{ren2021survey,rahimi2021addressing}. The role of interpretability and explainability in these models is also important to consider, particularly in the context of clinical decision making and regulatory requirements. In addition to data augmentation, generative models have the potential to be used for other medical applications such as generating synthetic patient records or synthesizing medical images from non-image data~ \cite{chambon2022roentgen}.}


% -------------------------------------------------
% https://arxiv.org/pdf/2211.07804.pdf
% https://arxiv.org/pdf/2106.00132.pdf
% The role of generative adversarial networks in brain MRI: a scoping review
% A Systematic Literature Review on Applications of GAN-Synthesized Images for Brain MRI
% https://arxiv.org/pdf/1809.07294.pdf
% https://arxiv.org/pdf/2112.07804.pdf
% https://reader.elsevier.com/reader/sd/pii/S0010482522001743?token=4A9E03AA544DCC848F112137D403D6FB6134547F247CE57D6F099AB5D51E977F66271F054BB2FB19F757BE743F3BD699&originRegion=eu-west-1&originCreation=20221216082337
% https://sci-hub.hkvisa.net/10.1111/1754-9485.13261
% file:///home/aghiles/T%C3%A9l%C3%A9chargements/Recent_Advances_in_Variational_Autoencoders_With_R.pdf

\section{Conclusions}\label{sec5}
{\textls[-25]{In this review, we examine the use of deep generative models for medical image augmentation. The limited availability of training data remains a major challenge in medical image analysis with deep learning approaches, which can be addressed by data augmentation techniques. However, traditional techniques still produce limited and unconvincing results. We focus on three types of deep generative models for medical image augmentation, VAEs, GANs, and DMs, and provide an overview of the current state of the art in each of these models. While deep generative models offer several advantages over traditional data augmentation techniques, including the ability to generate realistic new images that capture the underlying distribution of the training dataset, they also have some limitations. VAEs offer the ability to learn a meaningful and disentangled representation of the data, which can be useful for interpretability and latent space addition. %please confirm edit of edition to addition
 Despite these advantages, VAEs may produce fuzzy images that lack important details, which can be especially problematic in medical imaging. To address this limitation, improved VAE variants have been developed, such as vector quantized VAE, which uses powerful priors to generate synthetic samples with higher coherence and fidelity. Another approach involves combining VAEs with adversarial learning to improve the level of detail in the generated images. Alternatively, GANs have been found to generate high-quality images with fine details, and can be memory-efficient due to their upsampling-only architecture. \textls[-15]{However, GANs can be difficult to train and may suffer from mode collapse. Techniques such as WGAN and minibatch discrimination can help stabilize GAN training, and increasing the size of the training set can also be effective. Diffusion models have also been shown to generate high-quality images with increased sharpness and fine details, better than previous generative models, but they require significant computational resources to train and may be less interpretable. Researchers are currently exploring ways to reduce the sampling time of diffusion models, such as with progressive distillation, FastDPM, and DDIM variants. Overall, while each approach has its own strengths and weaknesses, continued research and development will be crucial for improving the effectiveness of deep generative models in various applications, including medical imaging. This evaluation of the strengths and limitations of each model can suggest directions for future research in this field including the exploration of hybrid architectures and improved variants, the incorporation of domain-specific knowledge, and the combination with other techniques such as transfer learning or active learning. The aim of this review is to emphasize the potential of deep generative models in enhancing the performance of deep learning algorithms for medical image analysis. By identifying the challenges of the current methods, we seek to increase awareness of the need for further contributions in this field.}}}

\vspace{6pt}

\authorcontributions{Conceptualization, A.K., S.R. and J.L.-L.; methodology, A.K., S.R. and J.L.-L.; software, A.K.; validation, S.R. and J.L.-L.;  writing—original draft preparation, A.K.; writing—review and editing, S.R. and J.L.-L.; supervision, S.R. and J.L.-L. All authors have read and agreed to the published version of the manuscript.
}

\funding{This research received no external funding.%Please add: ``This research received no external funding'' or ``This research was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names at \url{https://search.crossref.org/funding}, any errors may affect your future funding.
}

\conflictsofinterest{The authors declare no conflict of interest. The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the~results.%Declare conflicts of interest or state ``The authors declare no conflict of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript; or in the decision to publish the results must be declared in this section. If there is no role, please state ``The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the~results''.
} 
% ---------------------------------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
%=====================================
\begin{adjustwidth}{-\extralength}{0cm}
\reftitle{References}
\begin{thebibliography}{999}

\bibitem[Amyar \em{et~al.}(2022)Amyar, Modzelewski, Vera, Morard, and
Ruan]{amyar2022weakly}
Amyar, A.; Modzelewski, R.; Vera, P.; Morard, V.; Ruan, S.
\newblock Weakly Supervised Tumor Detection in PET Using Class Response for
Treatment Outcome Prediction.
\newblock {\em J. Imaging} {\bf 2022}, {\em 8},~130. [\href{http://doi.org/10.3390/jimaging8050130}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/35621894}{PubMed}]

\bibitem[Brochet \em{et~al.}(2022)Brochet, Lapuyade-Lahorgue, Huat, Thureau,
Pasquier, Gardin, Modzelewski, Gibon, Thariat, Gr{\'e}goire,
et~al.]{brochet2022quantitative}
Brochet, T.; Lapuyade-Lahorgue, J.; Huat, A.; Thureau, S.; Pasquier, D.;
Gardin, I.; Modzelewski, R.; Gibon, D.; Thariat, J.; Gr{\'e}goire, V.;
et~al.
\newblock A Quantitative Comparison between Shannon and
Tsallis--Havrda--Charvat Entropies Applied to Cancer Outcome Prediction.
\newblock {\em Entropy} {\bf 2022}, {\em 24},~436. [\href{http://dx.doi.org/10.3390/e24040436}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/35455101}{PubMed}]

\bibitem[Zhou \em{et~al.}(2022)Zhou, Ruan, Vera, and Canu]{zhou2022tri}
Zhou, T.; Ruan, S.; Vera, P.; Canu, S.
\newblock A Tri-Attention fusion guided multi-modal segmentation network.
\newblock {\em Pattern Recognit.} {\bf 2022}, {\em 124},~108417. [\href{http://dx.doi.org/10.1016/j.patcog.2021.108417}{CrossRef}]

\bibitem[Chen and Konukoglu(2018)]{chen2018unsupervised}
Chen, X.; Konukoglu, E.
\newblock Unsupervised detection of lesions in brain MRI using constrained
adversarial auto-encoders.
\newblock {\em arXiv  } {\bf 2018}, arXiv:1806.04972.

\bibitem[Lundervold and Lundervold(2019)]{lundervold2019overview}
Lundervold, A.S.; Lundervold, A.
\newblock An overview of deep learning in medical imaging focusing on MRI.
\newblock {\em Zeitschrift f{\"u}r Medizinische Physik} {\bf 2019}, {\em
29},~102--127. [\href{http://dx.doi.org/10.1016/j.zemedi.2018.11.002}{CrossRef}]

\bibitem[Song \em{et~al.}(2021)Song, Zheng, Li, Zhang, Zhang, Huang, Chen,
Wang, Zhao, Chong, et~al.]{song2021deep}
Song, Y.; Zheng, S.; Li, L.; Zhang, X.; Zhang, X.; Huang, Z.; Chen, J.; Wang,
R.; Zhao, H.; Chong, Y.;  et~al.
\newblock Deep learning enables accurate diagnosis of novel coronavirus
(COVID-19) with CT images.
\newblock {\em IEEE/ACM Trans. Comput. Biol. Bioinform.} {\bf 2021}, {\em 18},~2775--2780. [\href{http://dx.doi.org/10.1109/TCBB.2021.3065361}{CrossRef}]

\bibitem[Islam and Zhang(2020)]{islam2020gan}
Islam, J.; Zhang, Y.
\newblock GAN-based synthetic brain PET image generation.
\newblock {\em Brain Inform.} {\bf 2020}, {\em 7},~1--12. [\href{http://dx.doi.org/10.1186/s40708-020-00104-2}{CrossRef}]

\bibitem[Krizhevsky \em{et~al.}(2017)Krizhevsky, Sutskever, and
Hinton]{krizhevsky2017imagenet}
Krizhevsky, A.; Sutskever, I.; Hinton, G.E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Commun. ACM} {\bf 2017}, {\em 60},~84--90. [\href{http://dx.doi.org/10.1145/3065386}{CrossRef}]

\bibitem[Zhou \em{et~al.}(2022)Zhou, Vera, Canu, and Ruan]{zhou2022missing}
Zhou, T.; Vera, P.; Canu, S.; Ruan, S.
\newblock Missing Data Imputation via Conditional Generator and Correlation
Learning for Multimodal Brain Tumor Segmentation.
\newblock {\em Pattern Recognit. Lett.} {\bf 2022}, {\em 158},~125--132. [\href{http://dx.doi.org/10.1016/j.patrec.2022.04.019}{CrossRef}]

\bibitem[Paszke \em{et~al.}(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan, G.; Killeen,
T.; Lin, Z.; Gimelshein, N.; Antiga, L.;  et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2019},
{\em  32}.

\bibitem[Goodfellow \em{et~al.}(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I.J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.;
Ozair, S.; Courville, A.; Bengio, Y.
\newblock Generative adversarial networks.
\newblock {\em arXiv  } {\bf 2014}, arXiv:1406.2661.

\bibitem[Sandfort \em{et~al.}(2019)Sandfort, Yan, Pickhardt, and
Summers]{sandfort2019data}
Sandfort, V.; Yan, K.; Pickhardt, P.J.; Summers, R.M.
\newblock Data augmentation using generative adversarial networks (CycleGAN) to
improve generalizability in CT segmentation tasks.
\newblock {\em Sci. Rep.} {\bf 2019}, {\em 9},~16884. [\href{http://dx.doi.org/10.1038/s41598-019-52737-x}{CrossRef}]

\bibitem[Mahapatra \em{et~al.}(2019)Mahapatra, Bozorgtabar, and
Garnavi]{mahapatra2019image}
Mahapatra, D.; Bozorgtabar, B.; Garnavi, R.
\newblock Image super-resolution using progressive generative adversarial
networks for medical image analysis.
\newblock {\em Comput. Med. Imaging Graph.} {\bf 2019}, {\em
71},~30--39. [\href{http://dx.doi.org/10.1016/j.compmedimag.2018.10.005}{CrossRef}]

\bibitem[Yi \em{et~al.}(2019)Yi, Walia, and Babyn]{yi2019generative}
Yi, X.; Walia, E.; Babyn, P.
\newblock Generative adversarial network in medical imaging: A review.
\newblock {\em Med. Image Anal.} {\bf 2019}, {\em 58},~101552. [\href{http://dx.doi.org/10.1016/j.media.2019.101552}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/31521965}{PubMed}]

\bibitem[Ali \em{et~al.}(2022)Ali, Biswas, Mohsen, Shah, Alamgir, Mousa, and
Shah]{ali2022role}
Ali, H.; Biswas, M.R.; Mohsen, F.; Shah, U.; Alamgir, A.; Mousa, O.; Shah, Z.
\newblock The role of generative adversarial networks in brain MRI: A scoping
review.
\newblock {\em Insights  Imaging} {\bf 2022}, {\em 13},~98. [\href{http://dx.doi.org/10.1186/s13244-022-01237-0}{CrossRef}]

\bibitem[Chen \em{et~al.}(2022)Chen, Yang, Wei, Heidari, Zheng, Li, Chen, Hu,
Zhou, and Guan]{chen2022generative}
Chen, Y.; Yang, X.H.; Wei, Z.; Heidari, A.A.; Zheng, N.; Li, Z.; Chen, H.; Hu,
H.; Zhou, Q.; Guan, Q.
\newblock Generative adversarial networks in medical image augmentation: A
review.
\newblock {\em Comput. Biol. Med.} {\bf 2022}, 105382. [\href{http://dx.doi.org/10.1016/j.compbiomed.2022.105382}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/35276550}{PubMed}]

\bibitem[Mescheder \em{et~al.}(10-15 July 2018)Mescheder, Geiger, and
Nowozin]{mescheder2018training}
Mescheder, L.; Geiger, A.; Nowozin, S.
\newblock Which training methods for GANs do actually converge?
\newblock In Proceedings of the International Conference on Machine Learning,
PMLR, Stockholm, Sweden,  10--15 July 2018; \mbox{pp. 3481--3490}.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Kingma, D.P.; Welling, M.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv } {\bf 2013},  arXiv:1312.6114.

\bibitem[Sohl-Dickstein \em{et~al.}(6-11 July 2015)Sohl-Dickstein, Weiss,
Maheswaranathan, and Ganguli]{sohl2015deep}
Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; Ganguli, S.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In Proceedings of the International Conference on Machine Learning,
PMLR, Lille, France,  6--11 July 2015; \mbox{pp. 2256--2265}.

\bibitem[Ho \em{et~al.}(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J.; Jain, A.; Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2020},
{\em 33},~6840--6851.

\bibitem[Xiao \em{et~al.}(2021)Xiao, Kreis, and Vahdat]{xiao2021tackling}
Xiao, Z.; Kreis, K.; Vahdat, A.
\newblock Tackling the generative learning trilemma with denoising diffusion
gans.
\newblock {\em arXiv } {\bf 2021},  arXiv:2112.07804.

\bibitem[Chlap \em{et~al.}(2021)Chlap, Min, Vandenberg, Dowling, Holloway, and
Haworth]{chlap2021review}
Chlap, P.; Min, H.; Vandenberg, N.; Dowling, J.; Holloway, L.; Haworth, A.
\newblock A review of medical image data augmentation techniques for deep
learning applications.
\newblock {\em J. Med. Imaging Radiat. Oncol.} {\bf 2021},
{\em 65},~545--563. [\href{http://dx.doi.org/10.1111/1754-9485.13261}{CrossRef}]

\bibitem[Shorten and Khoshgoftaar(2019)]{shorten2019survey}
Shorten, C.; Khoshgoftaar, T.M.
\newblock A survey on image data augmentation for deep learning.
\newblock {\em J. Big Data} {\bf 2019}, {\em 6},~1--48. [\href{http://dx.doi.org/10.1186/s40537-019-0197-0}{CrossRef}]

\bibitem[Arjovsky \em{et~al.}(6-11 August 2017)Arjovsky, Chintala, and
Bottou]{arjovsky2017wasserstein}
Arjovsky, M.; Chintala, S.; Bottou, L.
\newblock Wasserstein generative adversarial networks.
\newblock In Proceedings of the International Conference on Machine Learning,
PMLR, Sydney, Australia,  6--11 August 2017; pp. 214--223.

\bibitem[Mirza and Osindero(2014)]{mirza2014conditional}
Mirza, M.; Osindero, S.
\newblock Conditional generative adversarial nets.
\newblock {\em arXiv  } {\bf 2014}, arXiv:1411.1784.

\bibitem[Isola \em{et~al.}(21-26 July 2017)Isola, Zhu, Zhou, and
Efros]{pix2pix}
Isola, P.; Zhu, J.Y.; Zhou, T.; Efros, A.A.
\newblock Image-to-image translation with conditional adversarial networks.
\newblock In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, Honolulu, HI, USA,  21--26 July
2017; pp. 1125--1134.

\bibitem[Ronneberger \em{et~al.}(5-9 October 2015)Ronneberger, Fischer, and
Brox]{ronneberger2015u}
Ronneberger, O.; Fischer, P.; Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, Munich, Germany,  5--9 October 2015;
pp. 234--241.

\bibitem[Radford \em{et~al.}(2015)Radford, Metz, and Chintala]{dcgan}
Radford, A.; Metz, L.; Chintala, S.
\newblock Unsupervised representation learning with deep convolutional
generative adversarial networks.
\newblock {\em arXiv  } {\bf 2015}, arXiv:1511.06434.

\bibitem[Karras \em{et~al.}(2017)Karras, Aila, Laine, and
Lehtinen]{karras2017progressive}
Karras, T.; Aila, T.; Laine, S.; Lehtinen, J.
\newblock Progressive growing of gans for improved quality, stability, and
variation.
\newblock {\em arXiv  } {\bf 2017}, arXiv:1710.10196.

\bibitem[Zhu \em{et~al.}(22--29 October 2017)Zhu, Park, Isola, and
Efros]{cyclegan}
Zhu, J.Y.; Park, T.; Isola, P.; Efros, A.A.
\newblock Unpaired image-to-image translation using cycle-consistent
adversarial networks.
\newblock In Proceedings of the IEEE International
Conference on Computer Vision, Venice, Italy,  22--29 October 2017; pp.
2223--2232.

\bibitem[Odena \em{et~al.}(2016)Odena, Olah, and Shlens]{odena2016conditional}
Odena, A.; Olah, C.; Shlens, J.
\newblock Conditional image synthesis with auxiliary classifier gans. \emph{arXiv}
{\bf 2016}, arXiv:1610.09585.

\bibitem[Larsen \em{et~al.}(2015)Larsen, S{\o}nderby, Larochelle, and
Winther]{vaegan}
Larsen, A.B.L.; S{\o}nderby, S.K.; Larochelle, H.; Winther, O.
\newblock Autoencoding beyond pixels using a learned similarity metric. 2015.
\newblock {\em arXiv  } {\bf 2015}, arXiv:1512.09300.

\bibitem[Higgins \em{et~al.}(2016)Higgins, Matthey, Glorot, Pal, Uria,
Blundell, Mohamed, and Lerchner]{higgins2016early}
Higgins, I.; Matthey, L.; Glorot, X.; Pal, A.; Uria, B.; Blundell, C.; Mohamed,
S.; Lerchner, A.
\newblock Early visual concept learning with unsupervised deep learning.
\newblock {\em arXiv  } {\bf 2016}, arXiv:1606.05579.

\bibitem[Kingma \em{et~al.}(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever,
and Welling]{iafvae}
Kingma, D.P.; Salimans, T.; Jozefowicz, R.; Chen, X.; Sutskever, I.; Welling,
M.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2016},
{\em 29}.

\bibitem[Zhao \em{et~al.}(2017)Zhao, Song, and Ermon]{zhao2017infovae}
Zhao, S.; Song, J.; Ermon, S.
\newblock Infovae: Information maximizing variational autoencoders.
\newblock {\em arXiv  } {\bf 2017}, arXiv:1706.02262.

\bibitem[Razavi \em{et~al.}(2019)Razavi, Van~den Oord, and Vinyals]{vqvae2}
Razavi, A.; Van~den Oord, A.; Vinyals, O.
\newblock Generating diverse high-fidelity images with vq-vae-2.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2019},
{\em 32}.

\bibitem[Dhariwal and Nichol(2021)]{dhariwal2021diffusion}
Dhariwal, P.; Nichol, A.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2021},
{\em 34},~8780--8794.

\bibitem[Salimans and Ho(2022)]{salimans2022progressive}
Salimans, T.; Ho, J.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2202.00512.

\bibitem[Kong and Ping(2021)]{kong2021fast}
Kong, Z.; Ping, W.
\newblock On fast sampling of diffusion probabilistic models.
\newblock {\em arXiv  } {\bf 2021}, arXiv:2106.00132.

\bibitem[Song \em{et~al.}(2020)Song, Meng, and Ermon]{song2020denoising}
Song, J.; Meng, C.; Ermon, S.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv  } {\bf 2020}, arXiv:2010.02502.

\bibitem[Han \em{et~al.}(16--19 April 2018)Han, Hayashi, Rundo, Araki, Shimoda,
Muramatsu, Furukawa, Mauri, and Nakayama]{han2018gan}
Han, C.; Hayashi, H.; Rundo, L.; Araki, R.; Shimoda, W.; Muramatsu, S.;
Furukawa, Y.; Mauri, G.; Nakayama, H.
\newblock GAN-based synthetic brain MR image generation.
\newblock In Proceedings of the IEEE 15th International Symposium on Biomedical
Imaging, New York, NY, USA,  16--19 April 2018; pp. 734--738.

\bibitem[Frid-Adar \em{et~al.}(2018)Frid-Adar, Diamant, Klang, Amitai,
Goldberger, and Greenspan]{frid2018gan}
Frid-Adar, M.; Diamant, I.; Klang, E.; Amitai, M.; Goldberger, J.; Greenspan,
H.
\newblock GAN-based synthetic medical image augmentation for increased CNN
performance in liver lesion classification.
\newblock {\em Neurocomputing} {\bf 2018}, {\em 321},~321--331. [\href{http://dx.doi.org/10.1016/j.neucom.2018.09.013}{CrossRef}]

\bibitem[Guibas \em{et~al.}(2017)Guibas, Virdi, and Li]{guibas2017synthetic}
Guibas, J.T.; Virdi, T.S.; Li, P.S.
\newblock Synthetic medical images from dual generative adversarial networks.
\newblock {\em arXiv  } {\bf 2017}, arXiv:1709.01872.

\bibitem[Platscher \em{et~al.}(2020)Platscher, Zopes, and
Federau]{platscher2020image}
Platscher, M.; Zopes, J.; Federau, C.
\newblock Image Translation for Medical Image Generation--Ischemic Stroke
Lesions.
\newblock {\em arXiv  } {\bf 2020}, arXiv:2010.02745.

\bibitem[Park \em{et~al.}(16--20 June 2019)Park, Liu, Wang, and
Zhu]{park2019semantic}
Park, T.; Liu, M.Y.; Wang, T.C.; Zhu, J.Y.
\newblock Semantic image synthesis with spatially-adaptive normalization.
\newblock In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, Long Beach, CA, USA,  16--20
June 2019; pp. 2337--2346.

\bibitem[Yurt \em{et~al.}(2021)Yurt, Dar, Erdem, Erdem, Oguz, and
{\c{C}}ukur]{yurt2021mustgan}
Yurt, M.; Dar, S.U.; Erdem, A.; Erdem, E.; Oguz, K.K.; {\c{C}}ukur, T.
\newblock mustGAN: Multi-stream generative adversarial networks for MR image
synthesis.
\newblock {\em Med. Image Anal.} {\bf 2021}, {\em 70},~101944. [\href{http://dx.doi.org/10.1016/j.media.2020.101944}{CrossRef}]

\bibitem[Dar \em{et~al.}(2019)Dar, Yurt, Karacan, Erdem, Erdem, and
Cukur]{dar2019image}
Dar, S.U.; Yurt, M.; Karacan, L.; Erdem, A.; Erdem, E.; Cukur, T.
\newblock Image synthesis in multi-contrast MRI with conditional generative
adversarial networks.
\newblock {\em IEEE Trans. Med. Imaging} {\bf 2019}, {\em
38},~2375--2388. [\href{http://dx.doi.org/10.1109/TMI.2019.2901750}{CrossRef}]

\bibitem[Sun \em{et~al.}(9--1 August 2020)Sun, Yuan, and Sun]{sun2020mm}
Sun, Y.; Yuan, P.; Sun, Y.
\newblock MM-GAN: 3D MRI data augmentation for medical image segmentation via
generative adversarial networks.
\newblock In Proceedings of the 2020 IEEE International conference on knowledge
graph (ICKG), Nanjing, China,  9--1 August 2020; pp. 227--234.

\bibitem[Han \em{et~al.}(2019)Han, Rundo, Araki, Nagano, Furukawa, Mauri,
Nakayama, and Hayashi]{han2019combining}
Han, C.; Rundo, L.; Araki, R.; Nagano, Y.; Furukawa, Y.; Mauri, G.; Nakayama,
H.; Hayashi, H.
\newblock Combining noise-to-image and image-to-image GANs: Brain MR image
augmentation for tumor detection.
\newblock {\em IEEE Access} {\bf 2019}, {\em 7},~156966--156977. [\href{http://dx.doi.org/10.1109/ACCESS.2019.2947606}{CrossRef}]

\bibitem[Kwon \em{et~al.}(13--17 October 2019)Kwon, Han, and
Kim]{kwon2019generation}
Kwon, G.; Han, C.; Kim, D.s.
\newblock Generation of 3D brain MRI using auto-encoding generative adversarial
networks.
\newblock In Proceedings of the International Conference on Medical Image
Computing and Computer-Assisted Intervention, Shenzhen, China,
13--17 October 2019; pp. 118--126.

\bibitem[Zhuang \em{et~al.}(8--11 April 2019)Zhuang, Schwing, and
Koyejo]{zhuang2019fmri}
Zhuang, P.; Schwing, A.G.; Koyejo, O.
\newblock Fmri data augmentation via synthesis.
\newblock In Proceedings of the 2019 IEEE 16th International Symposium on
Biomedical Imaging (ISBI 2019), Venice, Italy,  8--11 April 2019; pp.
1783--1787.

\bibitem[Waheed \em{et~al.}(2020)Waheed, Goyal, Gupta, Khanna, Al-Turjman, and
Pinheiro]{waheed2020covidgan}
Waheed, A.; Goyal, M.; Gupta, D.; Khanna, A.; Al-Turjman, F.; Pinheiro, P.R.
\newblock Covidgan: Data augmentation using auxiliary classifier gan for
improved covid-19 detection.
\newblock {\em IEEE Access} {\bf 2020}, {\em 8},~91916--91923. [\href{http://dx.doi.org/10.1109/ACCESS.2020.2994762}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/34192100}{PubMed}]

\bibitem[Han \em{et~al.}(2020)Han, Rundo, Araki, Furukawa, Mauri, Nakayama, and
Hayashi]{han2020infinite}
Han, C.; Rundo, L.; Araki, R.; Furukawa, Y.; Mauri, G.; Nakayama, H.; Hayashi,
H.
\newblock Infinite brain MR images: PGGAN-based data augmentation for tumor
detection.
\newblock In \emph{Neural Approaches to Dynamics of Signal Exchanges}; Springer: Singapore, 2019; pp. 291--303.

\bibitem[Sun \em{et~al.}(2020)Sun, Wang, Huang, Ding, Greenspan, and
Paisley]{sun2020adversarial}
Sun, L.; Wang, J.; Huang, Y.; Ding, X.; Greenspan, H.; Paisley, J.
\newblock An adversarial learning approach to medical image synthesis for
lesion detection.
\newblock {\em IEEE J. Biomed. Health Inform.} {\bf 2020},
{\em 24},~2303--2314. [\href{http://dx.doi.org/10.1109/JBHI.2020.2964016}{CrossRef}]

\bibitem[Wang \em{et~al.}(4--8 October 2020)Wang, Zhang, Chen, Wang, and
Zhang]{wang2020class}
Wang, Q.; Zhang, X.; Chen, W.; Wang, K.; Zhang, X.
\newblock Class-aware multi-window adversarial lung nodule synthesis
conditioned on semantic features.
\newblock In Proceedings of the International Conference on Medical Image
Computing and Computer-Assisted Intervention, Lima, Peru,  4--8
October 2020; pp. 589--598.

\bibitem[Geng \em{et~al.}(2020)Geng, Yao, Jiang, and Zhu]{geng2020deep}
Geng, X.; Yao, Q.; Jiang, K.; Zhu, Y.
\newblock Deep neural generative adversarial model based on VAE+ GAN for
disorder diagnosis.
\newblock In Proceedings of the 2020 International Conference on Internet of
Things and Intelligent Applications (ITIA), Zhenjiang, China, 27--29 November 2020; pp. 1--7.

\bibitem[Pang \em{et~al.}(2021)Pang, Wong, Ng, and Chan]{pang2021semi}
Pang, T.; Wong, J.H.D.; Ng, W.L.; Chan, C.S.
\newblock Semi-supervised GAN-based radiomics model for data augmentation in
breast ultrasound mass classification.
\newblock {\em Comput. Methods Programs Biomed.} {\bf 2021}, {\em
203},~106018. [\href{http://dx.doi.org/10.1016/j.cmpb.2021.106018}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/33714900}{PubMed}]

\bibitem[Barile \em{et~al.}(2021)Barile, Marzullo, Stamile, Durand-Dubief, and
Sappey-Marinier]{barile2021data}
\textls[-20]{Barile, B.; Marzullo, A.; Stamile, C.; Durand-Dubief, F.; Sappey-Marinier, D.
\newblock Data augmentation using generative adversarial neural networks on
brain structural connectivity in multiple sclerosis.
\newblock {\em Comput. Methods Programs Biomed.} {\bf 2021}, {\em
206},~106113.} [\href{http://dx.doi.org/10.1016/j.cmpb.2021.106113}{CrossRef}]

\bibitem[Shen \em{et~al.}(2021)Shen, Hao, Gou, and Wang]{shen2021mass}
Shen, T.; Hao, K.; Gou, C.; Wang, F.Y.
\newblock Mass image synthesis in mammogram with contextual information based
on gans.
\newblock {\em Comput. Methods Programs Biomed.} {\bf 2021}, {\em
202},~106019. [\href{http://dx.doi.org/10.1016/j.cmpb.2021.106019}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/33640650}{PubMed}]

\bibitem[Ambita \em{et~al.}(14--17 September 2021)Ambita, Boquio, and
Naval]{ambita2021covit}
Ambita, A.A.E.; Boquio, E.N.V.; Naval, P.C.
\newblock Covit-gan: Vision transformer forcovid-19 detection in ct scan
imageswith self-attention gan forDataAugmentation.
\newblock In Proceedings of the International Conference on Artificial Neural
Networks, Bratislava, Slovakia,  14--17 September 2021; pp.
587--598.

\bibitem[Hirte \em{et~al.}(2021)Hirte, Platscher, Joyce, Heit, Tranvinh, and
Federau]{hirte2021realistic}
Hirte, A.U.; Platscher, M.; Joyce, T.; Heit, J.J.; Tranvinh, E.; Federau, C.
\newblock Realistic generation of diffusion-weighted magnetic resonance brain
images with deep generative models.
\newblock {\em Magn. Reson. Imaging} {\bf 2021}, {\em 81},~60--66. [\href{http://dx.doi.org/10.1016/j.mri.2021.06.001}{CrossRef}]

\bibitem[Kaur \em{et~al.}(15--17 October 2021)Kaur, Aggarwal, and
Rani]{kaur2021mr}
Kaur, S.; Aggarwal, H.; Rani, R.
\newblock MR image synthesis using generative adversarial networks for
Parkinson’s disease classification.
\newblock In Proceedings of the International Conference on
Artificial Intelligence and Applications, Jiangsu, China,  15--17
October 2021; \mbox{pp. 317--327}.

\bibitem[Guan \em{et~al.}(2022)Guan, Chen, Wei, Heidari, Hu, Yang, Zheng, Zhou,
Chen, and Chen]{guan2022medical}
Guan, Q.; Chen, Y.; Wei, Z.; Heidari, A.A.; Hu, H.; Yang, X.H.; Zheng, J.;
Zhou, Q.; Chen, H.; Chen, F.
\newblock Medical image augmentation for lesion detection using a
texture-constrained multichannel progressive GAN.
\newblock {\em Comput. Biol. Med.} {\bf 2022}, {\em
145},~105444. [\href{http://dx.doi.org/10.1016/j.compbiomed.2022.105444}{CrossRef}]

\bibitem[Ahmad \em{et~al.}(2022)Ahmad, Sun, You, Palade, and
Mao]{ahmad2022brain}
Ahmad, B.; Sun, J.; You, Q.; Palade, V.; Mao, Z.
\newblock Brain Tumor Classification Using a Combination of Variational
Autoencoders and Generative Adversarial Networks.
\newblock {\em Biomedicines} {\bf 2022}, {\em 10},~223. [\href{http://dx.doi.org/10.3390/biomedicines10020223}{CrossRef}]

\bibitem[Pombo \em{et~al.}(2022)Pombo, Gray, Cardoso, Ourselin, Rees,
Ashburner, and Nachev]{pombo2022equitable}
Pombo, G.; Gray, R.; Cardoso, M.J.; Ourselin, S.; Rees, G.; Ashburner, J.;
Nachev, P.
\newblock Equitable modelling of brain imaging by counterfactual augmentation
with morphologically constrained 3d deep generative models.
\newblock {\em Med. Image Anal.} {\bf 2022}, 102723. [\href{http://dx.doi.org/10.1016/j.media.2022.102723}{CrossRef}]

\bibitem[Neff \em{et~al.}(10--12 May 2017)Neff, Payer, Stern, and
Urschler]{neff2017generative}
Neff, T.; Payer, C.; Stern, D.; Urschler, M.
\newblock Generative adversarial network based synthesis for supervised medical
image segmentation.
\newblock In Proceedings of the OAGM and ARW Joint Workshop, Vienna,
Austria,  10--12 May 2017; p.~4.

\bibitem[Mok and Chung(16 September 2018)]{mok2018learning}
Mok, T.C.; Chung, A.
\newblock Learning data augmentation for brain tumor segmentation with
coarse-to-fine generative adversarial networks.
\newblock In Proceedings of the International MICCAI Brainlesion Workshop;
Springer: Granada, Spain,  16 September 2018; \mbox{pp. 70--80}.

\bibitem[Shin \em{et~al.}(16 September 2018)Shin, Tenenholtz, Rogers, Schwarz,
Senjem, Gunter, Andriole, and Michalski]{shin2018medical}
Shin, H.C.; Tenenholtz, N.A.; Rogers, J.K.; Schwarz, C.G.; Senjem, M.L.;
Gunter, J.L.; Andriole, K.P.; Michalski, M.
\newblock Medical image synthesis for data augmentation and anonymization using
generative adversarial networks.
\newblock In Proceedings of the International Workshop on Simulation and
Synthesis in Medical Imaging, Granada, Spain,  16 September 2018;
pp. 1--11.

\bibitem[Jiang \em{et~al.}(2019)Jiang, Hu, Tyagi, Zhang, Rimner, Deasy, and
Veeraraghavan]{jiang2019cross}
Jiang, J.; Hu, Y.C.; Tyagi, N.; Zhang, P.; Rimner, A.; Deasy, J.O.;
Veeraraghavan, H.
\newblock Cross-modality (CT-MRI) prior augmented deep learning for robust lung
tumor segmentation from small MR datasets.
\newblock {\em Med. Phys.} {\bf 2019}, {\em 46},~4392--4404. [\href{http://dx.doi.org/10.1002/mp.13695}{CrossRef}]

\bibitem[Jiang \em{et~al.}(2020)Jiang, Chen, Loew, and Ko]{jiang2020covid}
Jiang, Y.; Chen, H.; Loew, M.; Ko, H.
\newblock COVID-19 CT image synthesis with a conditional generative adversarial
network.
\newblock {\em IEEE J. Biomed. Health Inform.} {\bf 2020},
{\em 25},~441--452. [\href{http://dx.doi.org/10.1109/JBHI.2020.3042523}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/33275588}{PubMed}]

\bibitem[Qasim \em{et~al.}(6--9 July 2020)Qasim, Ezhov, Shit, Schoppe,
Paetzold, Sekuboyina, Kofler, Lipkova, Li, and Menze]{qasim2020red}
Qasim, A.B.; Ezhov, I.; Shit, S.; Schoppe, O.; Paetzold, J.C.; Sekuboyina, A.;
Kofler, F.; Lipkova, J.; Li, H.; Menze, B.
\newblock Red-GAN: Attacking class imbalance via conditioned generation. Yet
another medical imaging perspective.
\newblock In Proceedings of the Medical Imaging with Deep Learning,
Montreal, QC, Canada,  6--9 July 2020; pp. 655--668.

\bibitem[Shi \em{et~al.}(22--24 August 2020)Shi, Lu, and Zhou]{shi2020novel}
Shi, H.; Lu, J.; Zhou, Q.
\newblock A novel data augmentation method using style-based GAN for robust
pulmonary nodule segmentation.
\newblock In Proceedings of the 2020 Chinese Control and Decision Conference
(CCDC); IEEE: Hefei, China,  22--24 August 2020; \mbox{pp. 2486--2491}.

\bibitem[Shen \em{et~al.}(2022)Shen, Ouyang, Xiao, Cheng, Shen, and
Wang]{shen2022image}
Shen, Z.; Ouyang, X.; Xiao, B.; Cheng, J.Z.; Shen, D.; Wang, Q.
\newblock Image synthesis with disentangled attributes for chest X-ray nodule
augmentation and detection.
\newblock {\em Med. Image Anal.} {\bf 2022},  102708. [\href{http://dx.doi.org/10.1016/j.media.2022.102708}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/36516554}{PubMed}]

\bibitem[Chartsias \em{et~al.}(10 September 2017)Chartsias, Joyce, Dharmakumar,
and Tsaftaris]{chartsias2017adversarial}
Chartsias, A.; Joyce, T.; Dharmakumar, R.; Tsaftaris, S.A.
\newblock Adversarial image synthesis for unpaired multi-modal cardiac data.
\newblock In Proceedings of the International Workshop on Simulation and
Synthesis in Medical Imaging, Québec City, QC, Canada,  \mbox{10
September 2017}; pp. 3--13.

\bibitem[Wolterink \em{et~al.}(10 September 2017)Wolterink, Dinkla, Savenije,
Seevinck, van~den Berg, and I{\v{s}}gum]{wolterink2017deep}
Wolterink, J.M.; Dinkla, A.M.; Savenije, M.H.; Seevinck, P.R.; van~den Berg,
C.A.; I{\v{s}}gum, I.
\newblock Deep MR to CT synthesis using unpaired data.
\newblock In Proceedings of the International Workshop on Simulation and
Synthesis in Medical Imaging, Québec City, QC, Canada,  10
September 2017; pp. 14--23.

\bibitem[Nie \em{et~al.}(2018)Nie, Trullo, Lian, Wang, Petitjean, Ruan, Wang,
and Shen]{nie2018medical}
Nie, D.; Trullo, R.; Lian, J.; Wang, L.; Petitjean, C.; Ruan, S.; Wang, Q.;
Shen, D.
\newblock Medical image synthesis with deep convolutional adversarial networks.
\newblock {\em IEEE Trans. Biomed. Eng.} {\bf 2018}, {\em
65},~2720--2730. [\href{http://dx.doi.org/10.1109/TBME.2018.2814538}{CrossRef}]

\bibitem[Armanious \em{et~al.}(2020)Armanious, Jiang, Fischer, K{\"u}stner,
Hepp, Nikolaou, Gatidis, and Yang]{armanious2020medgan}
Armanious, K.; Jiang, C.; Fischer, M.; K{\"u}stner, T.; Hepp, T.; Nikolaou, K.;
Gatidis, S.; Yang, B.
\newblock MedGAN: Medical image translation using GANs.
\newblock {\em Comput. Med. Imaging Graph.} {\bf 2020}, {\em
79},~101684. [\href{http://dx.doi.org/10.1016/j.compmedimag.2019.101684}{CrossRef}]

\bibitem[Yang \em{et~al.}(2021)Yang, Lu, Wang, Lu, Yao, Jiang, and
Qian]{yang2021synthesizing}
Yang, H.; Lu, X.; Wang, S.H.; Lu, Z.; Yao, J.; Jiang, Y.; Qian, P.
\newblock Synthesizing multi-contrast MR images via novel 3D conditional
Variational auto-encoding GAN.
\newblock {\em Mob. Netw. Appl.} {\bf 2021}, {\em
26},~415--424. [\href{http://dx.doi.org/10.1007/s11036-020-01678-1}{CrossRef}]

\bibitem[Sikka \em{et~al.}(2021)Sikka, Virk, Bathula, et~al.]{sikka2021mri}
Sikka, A.; Virk, J.S.; Bathula, D.R.;  et~al.
\newblock MRI to PET Cross-Modality Translation using Globally and Locally
Aware GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease.
\newblock {\em arXiv  } {\bf 2021}, arXiv:2108.02160.

\bibitem[Amirrajab \em{et~al.}(18 September 2022)Amirrajab, Lorenz, Weese,
Pluim, and Breeuwer]{amirrajab2022pathology}
Amirrajab, S.; Lorenz, C.; Weese, J.; Pluim, J.; Breeuwer, M.
\newblock Pathology Synthesis of 3D Consistent Cardiac MR Images Using 2D VAEs
and GANs.
\newblock In Proceedings of the International Workshop on Simulation and
Synthesis in Medical Imaging, Singapore,   \mbox{18 September
2022}; pp. 34--42.

\bibitem[Pesteie \em{et~al.}(2019)Pesteie, Abolmaesumi, and
Rohling]{pesteie2019adaptive}
Pesteie, M.; Abolmaesumi, P.; Rohling, R.N.
\newblock Adaptive augmentation of medical data using independently conditional
variational auto-encoders.
\newblock {\em IEEE Trans. Med. Imaging} {\bf 2019}, {\em
38},~2807--2820. [\href{http://dx.doi.org/10.1109/TMI.2019.2914656}{CrossRef}]

\bibitem[Chadebec \em{et~al.}(2022)Chadebec, Thibeau-Sutre, Burgos, and
Allassonni{\`e}re]{chadebec2022data}
Chadebec, C.; Thibeau-Sutre, E.; Burgos, N.; Allassonni{\`e}re, S.
\newblock Data augmentation in high dimensional low sample size setting using a
geometry-based variational autoencoder.
\newblock {\em IEEE Trans. Pattern Anal. Mach. Intell.}
{\bf 2022}, \emph{45}, 2879--2896. [\href{http://dx.doi.org/10.1109/TPAMI.2022.3185773}{CrossRef}]

\bibitem[Huo \em{et~al.}(18 September 2022)Huo, Vakharia, Wu, Sharan, Ko,
Ourselin, and Sparks]{huo2022brain}
Huo, J.; Vakharia, V.; Wu, C.; Sharan, A.; Ko, A.; Ourselin, S.; Sparks, R.
\newblock Brain Lesion Synthesis via Progressive Adversarial Variational
Auto-Encoder.
\newblock In Proceedings of the International Workshop on Simulation and
Synthesis in Medical Imaging, Singapore, 18 September
2022; pp. 101--111.

\bibitem[Gulrajani \em{et~al.}(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
Courville]{gulrajani2017improved}
Gulrajani, I.; Ahmed, F.; Arjovsky, M.; Dumoulin, V.; Courville, A.C.
\newblock Improved training of wasserstein gans.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2017},
{\em 30}.

\bibitem[Terzopoulos \em{et~al.}(16--19 December 2019)Terzopoulos
et~al.]{terzopoulos2019multi}
Terzopoulos, D.;  et~al.
\newblock Multi-adversarial variational autoencoder networks.
\newblock In Proceedings of the 2019 18th IEEE International Conference On
Machine Learning And Applications (ICMLA), Boca Raton, FL, USA,  16--19
December 2019; pp. 777--782.

\bibitem[Qiang \em{et~al.}(2021)Qiang, Dong, Liang, Ge, Zhang, Sun, Zhang,
Zhang, Gao, and Liu]{qiang2021modeling}
Qiang, N.; Dong, Q.; Liang, H.; Ge, B.; Zhang, S.; Sun, Y.; Zhang, C.; Zhang,
W.; Gao, J.; Liu, T.
\newblock Modeling and augmenting of fMRI data using deep recurrent variational
auto-encoder.
\newblock {\em J. Neural Eng.} {\bf 2021}, {\em 18},~0460b6. [\href{http://dx.doi.org/10.1088/1741-2552/ac1179}{CrossRef}]

\bibitem[Madan \em{et~al.}(2022)Madan, Veetil, EA, KP,
et~al.]{madan2022synthetic}
Madan, Y.; Veetil, I.K.; EA, G.; KP, S.;  et~al.
\newblock Synthetic Data Augmentation of MRI using Generative Variational
Autoencoder for Parkinson’s Disease Detection. In {\em Evolution in
Computational Intelligence}; Springer: Berlin, Germany,  2022; pp. 171--178.

\bibitem[Chadebec and Allassonni{\`e}re(2021)]{chadebec2021data}
Chadebec, C.; Allassonni{\`e}re, S.
\newblock Data Augmentation with Variational Autoencoders and Manifold
Sampling. In {\em Deep Generative Models, and Data Augmentation, Labelling,
and Imperfections}; Springer:  {Berlin/Heidelberg, Germany,} 
2021; pp. 184--192.

\bibitem[Liang and Chen(12--15 September 2021)]{liang2021data}
Liang, J.; Chen, J.
\newblock Data augmentation of thyroid ultrasound images using generative
adversarial network.
\newblock In Proceedings of the 2021 IEEE International Ultrasonics Symposium
(IUS), Xi'an, China,  12--15 September 2021; pp. 1--4.

\bibitem[Gan and Wang(2022)]{gan2022esophageal}
Gan, M.; Wang, C.
\newblock Esophageal optical coherence tomography image synthesis using an
adversarially learned variational autoencoder.
\newblock {\em Biomed. Opt. Express} {\bf 2022}, {\em 13},~1188--1201. [\href{http://dx.doi.org/10.1364/BOE.449796}{CrossRef}]

\bibitem[Hu \em{et~al.}(18--22 September 2022)Hu, Li, and Zhang]{hu2022domain}
Hu, Q.; Li, H.; Zhang, J.
\newblock Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised
Approach.
\newblock In Proceedings of the International Conference on Medical Image
Computing and Computer-Assisted Intervention, Singapore,
18--22 September 2022; pp. 495--504.

\bibitem[Biffi \em{et~al.}(16--20 September 2018)Biffi, Oktay, Tarroni, Bai,
Marvao, Doumou, Rajchl, Bedair, Prasad, Cook, et~al.]{biffi2018learning}
Biffi, C.; Oktay, O.; Tarroni, G.; Bai, W.; Marvao, A.D.; Doumou, G.; Rajchl,
M.; Bedair, R.; Prasad, S.; Cook, S.;  et~al.
\newblock DLearning interpretable anatomical features through deep generative
models: Application to cardiac remodeling.
\newblock In Proceedings of the International Conference on Medical Image
Computing and Computer-Assisted Intervention, Grenada, Spain,
16--20 September 2018; pp. 464--471.

\bibitem[Volokitin \em{et~al.}(4--8 October 2020)Volokitin, Erdil, Karani,
Tezcan, Chen, Gool, and Konukoglu]{volokitin2020modelling}
Volokitin, A.; Erdil, E.; Karani, N.; Tezcan, K.C.; Chen, X.; Gool, L.V.;
Konukoglu, E.
\newblock Modelling the distribution of 3D brain MRI using a 2D slice VAE.
\newblock In Proceedings of the International Conference on Medical Image
Computing and Computer-Assisted Intervention, Lima, Peru,  4--8
October 2020; pp. 657--666.

\bibitem[Huang \em{et~al.}(2022)Huang, Qiao, Jing, Zhu, and
Ren]{huang2022biomarkers}
Huang, Q.; Qiao, C.; Jing, K.; Zhu, X.; Ren, K.
\newblock Biomarkers identification for Schizophrenia via VAE and GSDAE-based
data augmentation.
\newblock {\em Comput. Biol. Med.} {\bf 2022}, 105603. [\href{http://dx.doi.org/10.1016/j.compbiomed.2022.105603}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/35588680}{PubMed}]

\bibitem[Beetz \em{et~al.}(28--31 March 2020)Beetz, Banerjee, Sang, and
Grau]{beetz2022combined}
Beetz, M.; Banerjee, A.; Sang, Y.; Grau, V.
\newblock Combined Generation of Electrocardiogram and Cardiac Anatomy Models
Using Multi-Modal Variational Autoencoders.
\newblock In Proceedings of the 2022 IEEE 19th International Symposium on
Biomedical Imaging (ISBI), Kolkata, India,  28--31 March 2020; pp.
1--4.

\bibitem[Sundgaard \em{et~al.}(2022)Sundgaard, Hannemose, Laugesen, Bray,
Harte, Kamide, Tanaka, Paulsen, and Christensen]{sundgaard2022multi}
Sundgaard, J.V.; Hannemose, M.R.; Laugesen, S.; Bray, P.; Harte, J.; Kamide,
Y.; Tanaka, C.; Paulsen, R.R.; Christensen, A.N.
\newblock Multi-modal data generation with a deep metric variational
autoencoder.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2202.03434.

\bibitem[Pinaya \em{et~al.}(22 September 2022)Pinaya, Tudosiu, Dafflon,
Da~Costa, Fernandez, Nachev, Ourselin, and Cardoso]{pinaya2022brain}
Pinaya, W.H.; Tudosiu, P.D.; Dafflon, J.; Da~Costa, P.F.; Fernandez, V.;
Nachev, P.; Ourselin, S.; Cardoso, M.J.
\newblock Brain imaging generation with latent diffusion models.
\newblock In Proceedings of the MICCAI Workshop on Deep Generative Models, Singapore, 22 September 2022; pp. 117--126.

\bibitem[Rombach \em{et~al.}(18--24 June 2022)Rombach, Blattmann, Lorenz,
Esser, and Ommer]{rombach2022high}
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, New Orleans, LA, USA,  18--24
June 2022; pp. 10684--10695.

\bibitem[Mao \em{et~al.}(2016)Mao, Li, Xie, Lau, Wang, and
Smolley]{mao2016least}
Mao, X.; Li, Q.; Xie, H.; Lau, R.; Wang, Z.; Smolley, S.
\newblock Least squares generative adversarial networks. \emph{arXiv} \textbf{2016}, arXiv:
1611.04076.

\bibitem[Heusel \em{et~al.}(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
Hochreiter]{fid}
Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash
equilibrium.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2017},
{\em 30}.

\bibitem[Fernandez \em{et~al.}(18--22 September 2022)Fernandez, Pinaya, Borges,
Tudosiu, Graham, Vercauteren, and Cardoso]{fernandez2022can}
Fernandez, V.; Pinaya, W.H.L.; Borges, P.; Tudosiu, P.D.; Graham, M.S.;
Vercauteren, T.; Cardoso, M.J.
\newblock Can segmentation models be trained with fully synthetically generated
data?
\newblock In Proceedings of the International Workshop on Simulation and
Synthesis in Medical Imaging, Singapore,  18--22
September 2022; pp. 79--90.

\bibitem[Isensee \em{et~al.}(2018)Isensee, Petersen, Klein, Zimmerer, Jaeger,
Kohl, Wasserthal, Koehler, Norajitra, Wirkert, et~al.]{isensee2018nnu}
Isensee, F.; Petersen, J.; Klein, A.; Zimmerer, D.; Jaeger, P.F.; Kohl, S.;
Wasserthal, J.; Koehler, G.; Norajitra, T.; Wirkert, S.;  et~al.
\newblock nnu-net: Self-adapting framework for u-net-based medical image
segmentation.
\newblock {\em arXiv  } {\bf 2018}, arXiv:1809.10486.

\bibitem[Lyu and Wang(2022)]{lyu2022conversion}
Lyu, Q.; Wang, G.
\newblock Conversion Between CT and MRI Images Using Diffusion and
Score-Matching Models.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2209.12104.

\bibitem[Song \em{et~al.}(2020)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
Poole]{song2020score}
Song, Y.; Sohl-Dickstein, J.; Kingma, D.P.; Kumar, A.; Ermon, S.; Poole, B.
\newblock Score-based generative modeling through stochastic differential
equations.
\newblock {\em arXiv  } {\bf 2020}, arXiv:2011.13456.

\bibitem[Nyholm \em{et~al.}(2018)Nyholm, Svensson, Andersson, Jonsson, Sohlin,
Gustafsson, Kjell{\'e}n, S{\"o}derstr{\"o}m, Albertsson, Blomqvist,
et~al.]{pelvicdataset}
Nyholm, T.; Svensson, S.; Andersson, S.; Jonsson, J.; Sohlin, M.; Gustafsson,
C.; Kjell{\'e}n, E.; S{\"o}derstr{\"o}m, K.; Albertsson, P.; Blomqvist, L.;
et~al.
\newblock MR and CT data with multiobserver delineations of organs in the
pelvic area—Part of the Gold Atlas project.
\newblock {\em Med. Phys.} {\bf 2018}, {\em 45},~1295--1300. [\href{http://dx.doi.org/10.1002/mp.12748}{CrossRef}]

\bibitem[Dorjsembe \em{et~al.}(18--22 September 2022)Dorjsembe, Odonchimed, and
Xiao]{dorjsembe2022three}
Dorjsembe, Z.; Odonchimed, S.; Xiao, F.
\newblock Three-Dimensional Medical Image Synthesis with Denoising Diffusion
Probabilistic Models.
\newblock In Proceedings of the Medical Imaging with Deep Learning,
Zurich, Switzerland,  18--22 September 2022.

\bibitem[Packh{\"a}user \em{et~al.}(2022)Packh{\"a}user, Folle, Thamm, and
Maier]{packhauser2022generation}
Packh{\"a}user, K.; Folle, L.; Thamm, F.; Maier, A.
\newblock Generation of anonymous chest radiographs using latent diffusion
models for training thoracic abnormality classification systems.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2211.01323.

\bibitem[Moghadam \em{et~al.}(2022)Moghadam, Van~Dalen, Martin, Lennerz, Yip,
Farahani, and Bashashati]{moghadam2022morphology}
Moghadam, P.A.; Van~Dalen, S.; Martin, K.C.; Lennerz, J.; Yip, S.; Farahani,
H.; Bashashati, A.
\newblock A Morphology Focused Diffusion Probabilistic Model for Synthesis of
Histopathology Images.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2209.13167.

\bibitem[Chambon \em{et~al.}(2022)Chambon, Bluethgen, Delbrouck, Van~der
Sluijs, Po{\l}acin, Chaves, Abraham, Purohit, Langlotz, and
Chaudhari]{chambon2022roentgen}
Chambon, P.; Bluethgen, C.; Delbrouck, J.B.; Van~der Sluijs, R.; Po{\l}acin,
M.; Chaves, J.M.Z.; Abraham, T.M.; Purohit, S.; Langlotz, C.P.; Chaudhari, A.
\newblock RoentGen: Vision-Language Foundation Model for Chest X-ray
Generation.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2211.12737.

\bibitem[Wolleb \em{et~al.}(2022)Wolleb, Sandk{\"u}hler, Bieder, and
Cattin]{wolleb2022swiss}
Wolleb, J.; Sandk{\"u}hler, R.; Bieder, F.; Cattin, P.C.
\newblock The Swiss Army Knife for Image-to-Image Translation: Multi-Task
Diffusion Models.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2204.02641.

\bibitem[Sagers \em{et~al.}(2022)Sagers, Diao, Groh, Rajpurkar, Adamson, and
Manrai]{sagers2022improving}
Sagers, L.W.; Diao, J.A.; Groh, M.; Rajpurkar, P.; Adamson, A.S.; Manrai, A.K.
\newblock Improving dermatology classifiers across populations using images
generated by large diffusion models.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2211.13352.

\bibitem[Peng \em{et~al.}(2022)Peng, Adeli, Zhao, and Pohl]{peng2022generating}
Peng, W.; Adeli, E.; Zhao, Q.; Pohl, K.M.
\newblock Generating Realistic 3D Brain MRIs Using a Conditional Diffusion
Probabilistic Model.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2212.08034.

\bibitem[Ali \em{et~al.}(8--9 December 2022)Ali, Murad, and Shah]{ali2023spot}
Ali, H.; Murad, S.; Shah, Z.
\newblock Spot the fake lungs: Generating synthetic medical images using neural
diffusion models.
\newblock In Proceedings of the Artificial Intelligence and Cognitive Science:
30th Irish Conference, AICS 2022, Munster, Ireland,  8--9 December
2022; pp. 32--39.

\bibitem[Saeed \em{et~al.}(2023)Saeed, Syer, Yan, Yang, Emberton, Punwani,
Clarkson, Barratt, and Hu]{saeed2023bi}
Saeed, S.U.; Syer, T.; Yan, W.; Yang, Q.; Emberton, M.; Punwani, S.; Clarkson,
M.J.; Barratt, D.C.; Hu, Y.
\newblock Bi-parametric prostate MR image synthesis using pathology and
sequence-conditioned stable diffusion.
\newblock {\em arXiv  } {\bf 2023}, arXiv:2303.02094.

\bibitem[Weber \em{et~al.}(2023)Weber, Ingrisch, Bischl, and
R{\"u}gamer]{weber2023cascaded}
Weber, T.; Ingrisch, M.; Bischl, B.; R{\"u}gamer, D.
\newblock Cascaded Latent Diffusion Models for High-Resolution Chest X-ray
Synthesis.
\newblock {\em arXiv  } {\bf 2023}, arXiv:2303.11224.

\bibitem[Khader \em{et~al.}(2022)Khader, Mueller-Franzes, Arasteh, Han,
Haarburger, Schulze-Hagen, Schad, Engelhardt, Baessler, Foersch,
et~al.]{khader2022medical}
Khader, F.; Mueller-Franzes, G.; Arasteh, S.T.; Han, T.; Haarburger, C.;
Schulze-Hagen, M.; Schad, P.; Engelhardt, S.; Baessler, B.; Foersch, S.;
et~al.
\newblock Medical Diffusion--Denoising Diffusion Probabilistic Models for 3D
Medical Image Generation.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2211.03364.

\bibitem[{\"O}zbey \em{et~al.}(2022){\"O}zbey, Dar, Bedel, Dalmaz, {\"O}zturk,
G{\"u}ng{\"o}r, and {\c{C}}ukur]{ozbey2022unsupervised}
{\"O}zbey, M.; Dar, S.U.; Bedel, H.A.; Dalmaz, O.; {\"O}zturk, {\c{S}}.;
G{\"u}ng{\"o}r, A.; {\c{C}}ukur, T.
\newblock Unsupervised medical image translation with adversarial diffusion
models.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2207.08208.

\bibitem[Meng \em{et~al.}(2022)Meng, Gu, Pan, Wang, Xue, Lu, He, Zhan, and
Shen]{meng2022novel}
Meng, X.; Gu, Y.; Pan, Y.; Wang, N.; Xue, P.; Lu, M.; He, X.; Zhan, Y.; Shen,
D.
\newblock A Novel Unified Conditional Score-based Generative Framework for
Multi-modal Medical Image Completion.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2207.03430.

\bibitem[Kim and Ye(18--22 September 2022)]{kim2022diffusion}
Kim, B.; Ye, J.C.
\newblock Diffusion deformable model for 4D temporal medical image generation.
\newblock In Proceedings of the International Conference on Medical Image
Computing and Computer-Assisted Intervention, Singapore,
18--22 September 2022; pp. 539--548.

\bibitem[Kazerouni \em{et~al.}(2022)Kazerouni, Aghdam, Heidari, Azad, Fayyaz,
Hacihaliloglu, and Merhof]{kazerouni2022diffusion}
Kazerouni, A.; Aghdam, E.K.; Heidari, M.; Azad, R.; Fayyaz, M.; Hacihaliloglu,
I.; Merhof, D.
\newblock Diffusion models for medical image analysis: A comprehensive survey.
\newblock {\em arXiv  } {\bf 2022}, arXiv:2211.07804.

\bibitem[Abdollahi \em{et~al.}(2020)Abdollahi, Tomita, and
Hassanpour]{abdollahi2020data}
Abdollahi, B.; Tomita, N.; Hassanpour, S.
\newblock \emph{Data Augmentation in Training Deep Learning Models for Medical Image
Analysis}; Springer:  {Berlin/Heidelberg, Germany,}
{ 2020}.
\newblock pp. 167--180.

\bibitem[Huang \em{et~al.}(2018)Huang, He, Sun, Tan, et~al.]{huang2018introvae}
Huang, H.; He, R.; Sun, Z.; Tan, T.;  et~al.
\newblock Introvae: Introspective variational autoencoders for photographic
image synthesis.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2018},
{\em 31}.

\bibitem[Amyar \em{et~al.}(13--15 September 2020)Amyar, Ruan, Vera, Decazes,
and Modzelewski]{amyar2020radiogan}
Amyar, A.; Ruan, S.; Vera, P.; Decazes, P.; Modzelewski, R.
\newblock RADIOGAN: Deep convolutional conditional generative adversarial
network to generate PET images.
\newblock In Proceedings of the 2020 7th International Conference on
Bioinformatics Research and Applications, Berlin, Germany,  13--15
September 2020; pp. 28--33.

\bibitem[Bullitt \em{et~al.}(2005)Bullitt, Zeng, Gerig, Aylward, Joshi, Smith,
Lin, and Ewend]{midasdataset}
Bullitt, E.; Zeng, D.; Gerig, G.; Aylward, S.; Joshi, S.; Smith, J.K.; Lin, W.;
Ewend, M.G.
\newblock Vessel tortuosity and brain tumor malignancy: A blinded study1.
\newblock {\em Acad. Radiol.} {\bf 2005}, {\em 12},~1232--1240. [\href{http://dx.doi.org/10.1016/j.acra.2005.05.027}{CrossRef}]

\bibitem[Staal \em{et~al.}(2004)Staal, Abr{\`a}moff, Niemeijer, Viergever, and
Van~Ginneken]{drivedataset}
Staal, J.; Abr{\`a}moff, M.D.; Niemeijer, M.; Viergever, M.A.; Van~Ginneken, B.
\newblock Ridge-based vessel segmentation in color images of the retina.
\newblock {\em IEEE Trans. Med. Imaging} {\bf 2004}, {\em
23},~501--509. [\href{http://dx.doi.org/10.1109/TMI.2004.825627}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/15084075}{PubMed}]

\bibitem[Bernard \em{et~al.}(2018)Bernard, Lalande, Zotti, Cervenansky, Yang,
Heng, Cetin, Lekadir, Camara, Ballester, et~al.]{acdcdataset}
Bernard, O.; Lalande, A.; Zotti, C.; Cervenansky, F.; Yang, X.; Heng, P.A.;
Cetin, I.; Lekadir, K.; Camara, O.; Ballester, M.A.G.;  et~al.
\newblock Deep learning techniques for automatic MRI cardiac multi-structures
segmentation and diagnosis: Is the problem solved?
\newblock {\em IEEE Trans. Med. Imaging} {\bf 2018}, {\em
37},~2514--2525. [\href{http://dx.doi.org/10.1109/TMI.2018.2837502}{CrossRef}]

\bibitem[Wang \em{et~al.}(21--26 July 2017)Wang, Peng, Lu, Lu, Bagheri, and
Summers]{wang2017chestx}
Wang, X.; Peng, Y.; Lu, L.; Lu, Z.; Bagheri, M.; Summers, R.M.
\newblock Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax diseases.
\newblock In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, Honolulu, HI, USA,  21--26 July
2017; pp. 2097--2106.

\bibitem[Bai \em{et~al.}(2015)Bai, Shi, de~Marvao, Dawes, O’Regan, Cook, and
Rueckert]{bai2015bi}
Bai, W.; Shi, W.; de~Marvao, A.; Dawes, T.J.; O’Regan, D.P.; Cook, S.A.;
Rueckert, D.
\newblock A bi-ventricular cardiac atlas built from 1000+ high resolution MR
images of healthy subjects and an analysis of shape and motion.
\newblock {\em Med. Image Anal.} {\bf 2015}, {\em 26},~133--145. [\href{http://dx.doi.org/10.1016/j.media.2015.08.009}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/26387054}{PubMed}]

\bibitem[Van~Ginneken \em{et~al.}(2006)Van~Ginneken, Stegmann, and
Loog]{van2006segmentation}
Van~Ginneken, B.; Stegmann, M.B.; Loog, M.
\newblock Segmentation of anatomical structures in chest radiographs using
supervised methods: A comparative study on a public database.
\newblock {\em Med. Image Anal.} {\bf 2006}, {\em 10},~19--40. [\href{http://dx.doi.org/10.1016/j.media.2005.02.002}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/15919232}{PubMed}]

\bibitem[Van~Essen \em{et~al.}(2013)Van~Essen, Smith, Barch, Behrens, Yacoub,
Ugurbil, Consortium, et~al.]{van2013wu}
Van~Essen, D.C.; Smith, S.M.; Barch, D.M.; Behrens, T.E.; Yacoub, E.; Ugurbil,
K.; Consortium, W.M.H.;  et~al.
\newblock The WU-Minn human connectome project: An overview.
\newblock {\em Neuroimage} {\bf 2013}, {\em 80},~62--79. [\href{http://dx.doi.org/10.1016/j.neuroimage.2013.05.041}{CrossRef}]

\bibitem[Kermany \em{et~al.}(2018)Kermany, Goldbaum, Cai, Valentim, Liang,
Baxter, McKeown, Yang, Wu, Yan, et~al.]{kermany2018identifying}
Kermany, D.S.; Goldbaum, M.; Cai, W.; Valentim, C.C.; Liang, H.; Baxter, S.L.;
McKeown, A.; Yang, G.; Wu, X.; Yan, F.;  et~al.
\newblock Identifying medical diagnoses and treatable diseases by image-based
deep learning.
\newblock {\em Cell} {\bf 2018}, {\em 172},~1122--1131. [\href{http://dx.doi.org/10.1016/j.cell.2018.02.010}{CrossRef}]

\bibitem[Groh \em{et~al.}(20--25 June 2021)Groh, Harris, Soenksen, Lau, Han,
Kim, Koochek, and Badri]{groh2021evaluating}
Groh, M.; Harris, C.; Soenksen, L.; Lau, F.; Han, R.; Kim, A.; Koochek, A.;
Badri, O.
\newblock Evaluating deep neural networks trained on clinical images in
dermatology with the fitzpatrick 17k dataset.
\newblock In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, Nashville, TN, USA,  20--25
June 2021; pp. 1820--1828.

\bibitem[Yang \em{et~al.}(2020)Yang, He, Zhao, Zhang, Zhang, and
Xie]{yang2020covid}
Yang, X.; He, X.; Zhao, J.; Zhang, Y.; Zhang, S.; Xie, P.
\newblock COVID-CT-dataset: A CT scan dataset about COVID-19.
\newblock {\em arXiv  } {\bf 2020}, arXiv:2003.13865.

\bibitem[Soares \em{et~al.}(2020)Soares, Angelov, Biaso, Froes, and
Abe]{soares2020sars}
Soares, E.; Angelov, P.; Biaso, S.; Froes, M.H.; Abe, D.K.
\newblock SARS-CoV-2 CT-scan dataset: A large dataset of real patients CT scans
for SARS-CoV-2 identification.
\newblock {\em MedRxiv} {\bf 2020}. [\href{http://dx.doi.org/10.1101/2020.04.24.20078584}{CrossRef}]

\bibitem[Johnson \em{et~al.}(2019)Johnson, Pollard, Greenbaum, Lungren, Deng,
Peng, Lu, Mark, Berkowitz, and Horng]{johnson2019mimic}
Johnson, A.E.; Pollard, T.J.; Greenbaum, N.R.; Lungren, M.P.; Deng, C.y.; Peng,
Y.; Lu, Z.; Mark, R.G.; Berkowitz, S.J.; Horng, S.
\newblock MIMIC-CXR-JPG, a large publicly available database of labeled chest
radiographs.
\newblock {\em arXiv  } {\bf 2019}, arXiv:1901.07042.

\bibitem[Jones \em{et~al.}(2020)Jones, Tillin, Park, Williams, Rapala,
Al~Saikhan, Eastwood, Richards, Hughes, and Chaturvedi]{jones2020cohort}
Jones, S.; Tillin, T.; Park, C.; Williams, S.; Rapala, A.; Al~Saikhan, L.;
Eastwood, S.V.; Richards, M.; Hughes, A.D.; Chaturvedi, N.
\newblock Cohort Profile Update: Southall and Brent Revisited (SABRE) study: A
UK population-based comparison of cardiovascular disease and diabetes in
people of European, South Asian and African Caribbean heritage.
\newblock {\em Int. J. Epidemiol.} {\bf 2020}, {\em
49},~1441--1442e. [\href{http://dx.doi.org/10.1093/ije/dyaa135}{CrossRef}]

\bibitem[Saha \em{et~al.}(27 Nov--1 Dec 2022)Saha, Twilt, Bosma, van Ginneken,
Yakar, Elschot, Veltman, F{\"u}tterer, de~Rooij, and
Huisman]{saha2022artificial}
Saha, A.; Twilt, J.; Bosma, J.; van Ginneken, B.; Yakar, D.; Elschot, M.;
Veltman, J.; F{\"u}tterer, J.; de~Rooij, M.; Huisman, H.
\newblock Artificial Intelligence and Radiologists at Prostate Cancer Detection
in MRI: The PI CAI Challenge.
\newblock In Proceedings of the RSNA,  Chicago, IL, USA, 27 November--1 December 2022.

\bibitem[Kynk{\"a}{\"a}nniemi \em{et~al.}(2019)Kynk{\"a}{\"a}nniemi, Karras,
Laine, Lehtinen, and Aila]{kynkaanniemi2019improved}
Kynk{\"a}{\"a}nniemi, T.; Karras, T.; Laine, S.; Lehtinen, J.; Aila, T.
\newblock Improved precision and recall metric for assessing generative models.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2019},
{\em 32}.

\bibitem[Wang \em{et~al.}(2004)Wang, Bovik, Sheikh, and Simoncelli]{ssim}
Wang, Z.; Bovik, A.C.; Sheikh, H.R.; Simoncelli, E.P.
\newblock Image quality assessment: From error visibility to structural
similarity.
\newblock {\em IEEE Trans. Image Process.} {\bf 2004}, {\em
13},~600--612. [\href{http://dx.doi.org/10.1109/TIP.2003.819861}{CrossRef}] [\href{http://www.ncbi.nlm.nih.gov/pubmed/15376593}{PubMed}]

\bibitem[Ledig \em{et~al.}(21--26 July 2017)Ledig, Theis, Husz{\'a}r,
Caballero, Cunningham, Acosta, Aitken, Tejani, Totz, Wang,
et~al.]{ledig2017photo}
Ledig, C.; Theis, L.; Husz{\'a}r, F.; Caballero, J.; Cunningham, A.; Acosta,
A.; Aitken, A.; Tejani, A.; Totz, J.; Wang, Z.;  et~al.
\newblock Photo-realistic single image super-resolution using a generative
adversarial network.
\newblock In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA, 21--26 July 2017;
pp. 4681--4690.

\bibitem[Zhang \em{et~al.}(18--22 June 2018)Zhang, Isola, Efros, Shechtman, and
Wang]{zhang2018unreasonable}
Zhang, R.; Isola, P.; Efros, A.A.; Shechtman, E.; Wang, O.
\newblock The unreasonable effectiveness of deep features as a perceptual
metric.
\newblock In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, Honolulu, HI, USA, 18--22 June 2018; \mbox{pp. 586--595}.

\bibitem[Salimans \em{et~al.}(2016)Salimans, Goodfellow, Zaremba, Cheung,
Radford, and Chen]{salimans2016improved}
Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Radford, A.; Chen, X.
\newblock Improved techniques for training gans.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2016},
{\em 29}.

\bibitem[Sudre \em{et~al.}(2017)Sudre, Li, Vercauteren, Ourselin, and
Jorge~Cardoso]{sudre2017generalised}
Sudre, C.H.; Li, W.; Vercauteren, T.; Ourselin, S.; Jorge~Cardoso, M.
\newblock Generalised dice overlap as a deep learning loss function for highly
unbalanced segmentations.
\newblock In \emph{Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Proceedings of the Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada,  14 September 2017}; Springer:  {Berlin/Heidelberg, Germany,} 2017; pp. 240--248.

\bibitem[Rockafellar and Wets(2009)]{rockafellar2009variational}
Rockafellar, R.T.; Wets, R.J.B.
\newblock {\em Variational Analysis}; Springer Science \& Business
Media:  {Berlin/Heidelberg, Germany,}
2009; \mbox{Volume 317}.

\bibitem[Gretton \em{et~al.}(2012)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and
Smola]{gretton2012kernel}
Gretton, A.; Borgwardt, K.M.; Rasch, M.J.; Sch{\"o}lkopf, B.; Smola, A.
\newblock A kernel two-sample test.
\newblock {\em  J. Mach. Learn. Res.} {\bf 2012}, {\em
13},~723--773.

\bibitem[Cover and Hart(1967)]{cover1967nearest}
Cover, T.; Hart, P.
\newblock Nearest neighbor pattern classification.
\newblock {\em IEEE Trans. Inf. Theory} {\bf 1967}, {\em
13},~21--27. [\href{http://dx.doi.org/10.1109/TIT.1967.1053964}{CrossRef}]

\bibitem[Bounliphone \em{et~al.}(2015)Bounliphone, Belilovsky, Blaschko,
Antonoglou, and Gretton]{bounliphone2015test}
Bounliphone, W.; Belilovsky, E.; Blaschko, M.B.; Antonoglou, I.; Gretton, A.
\newblock A test of relative similarity for model selection in generative
models.
\newblock {\em arXiv  } {\bf 2015}, arXiv:1511.04581.

\bibitem[Vaserstein(1969)]{vaserstein1969markov}
Vaserstein, L.N.
\newblock Markov processes over denumerable products of spaces, describing
large systems of automata.
\newblock {\em Probl. Peredachi Informatsii} {\bf 1969}, {\em 5},~64--72.

\bibitem[Fawcett(2006)]{fawcett2006introduction}
Fawcett, T.
\newblock An introduction to ROC analysis.
\newblock {\em Pattern Recognit. Lett.} {\bf 2006}, {\em 27},~861--874. [\href{http://dx.doi.org/10.1016/j.patrec.2005.10.010}{CrossRef}]

\bibitem[Nguyen \em{et~al.}(2010)Nguyen, Wainwright, and
Jordan]{nguyen2010estimating}
Nguyen, X.; Wainwright, M.J.; Jordan, M.I.
\newblock Estimating divergence functionals and the likelihood ratio by convex
risk minimization.
\newblock {\em IEEE Trans. Inf. Theory} {\bf 2010}, {\em
56},~5847--5861. [\href{http://dx.doi.org/10.1109/TIT.2010.2068870}{CrossRef}]

\bibitem[Sheikh and Bovik(2005)]{vif}
Sheikh, H.R.; Bovik, A.C.
\newblock A visual information fidelity approach to video quality assessment.
\newblock  \emph{First Int. Workshop Video Process. Qual. Metrics Consum. Electron.} \textbf{2005}, \emph{7}, 2117--2128.

\bibitem[Wang and Bovik(2002)]{uqi}
Wang, Z.; Bovik, A.C.
\newblock A universal image quality index.
\newblock {\em IEEE Signal Process. Lett.} {\bf 2002}, {\em 9},~81--84. [\href{http://dx.doi.org/10.1109/97.995823}{CrossRef}]

\bibitem[Tavse \em{et~al.}(2022)Tavse, Varadarajan, Bachute, Gite, and
Kotecha]{tavse2022systematic}
Tavse, S.; Varadarajan, V.; Bachute, M.; Gite, S.; Kotecha, K.
\newblock A Systematic Literature Review on Applications of GAN-Synthesized
Images for Brain MRI.
\newblock {\em Future Internet} {\bf 2022}, {\em 14},~351. [\href{http://dx.doi.org/10.3390/fi14120351}{CrossRef}]

\bibitem[Ramesh \em{et~al.}(18--24 July 2021)Ramesh, Pavlov, Goh, Gray, Voss,
Radford, Chen, and Sutskever]{ramesh2021zero}
Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Radford, A.; Chen, M.;
Sutskever, I.
\newblock Zero-shot text-to-image generation.
\newblock In Proceedings of the International Conference on Machine Learning,
PMLR, Online,  18--24 July 2021; pp. 8821--8831.

\bibitem[Saharia \em{et~al.}(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
Ghasemipour, Gontijo~Lopes, Karagol~Ayan, Salimans,
et~al.]{saharia2022photorealistic}
Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E.L.;
Ghasemipour, K.; Gontijo~Lopes, R.; Karagol~Ayan, B.; Salimans, T.;  et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
understanding.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2022},
{\em 35},~36479--36494.

\bibitem[Kang \em{et~al.}(2023)Kang, Zhu, Zhang, Park, Shechtman, Paris, and
Park]{kang2023scaling}
Kang, M.; Zhu, J.Y.; Zhang, R.; Park, J.; Shechtman, E.; Paris, S.; Park, T.
\newblock Scaling up GANs for Text-to-Image Synthesis.
\newblock {\em arXiv  } {\bf 2023}, arXiv:2303.05511.

\bibitem[Sauer \em{et~al.}(2023)Sauer, Karras, Laine, Geiger, and
Aila]{sauer2023stylegan}
Sauer, A.; Karras, T.; Laine, S.; Geiger, A.; Aila, T.
\newblock Stylegan-t: Unlocking the power of gans for fast large-scale
text-to-image synthesis.
\newblock {\em arXiv  } {\bf 2023}, arXiv:2301.09515.

\bibitem[Delgado and Oyedele(2021)]{delgado2021deep}
Delgado, J.M.D.; Oyedele, L.
\newblock Deep learning with small datasets: Using autoencoders to address
limited datasets in construction management.
\newblock {\em Appl. Soft Comput.} {\bf 2021}, {\em 112},~107836. [\href{http://dx.doi.org/10.1016/j.asoc.2021.107836}{CrossRef}]

\bibitem[Caterini \em{et~al.}(2018)Caterini, Doucet, and
Sejdinovic]{caterini2018hamiltonian}
Caterini, A.L.; Doucet, A.; Sejdinovic, D.
\newblock Hamiltonian variational auto-encoder.
\newblock {\em Adv. Neural Inf. Process. Syst.} {\bf 2018},
{\em 31}.

\bibitem[He \em{et~al.}(21--24 October 2022)He, Wang, Yang, Clarysse, Robini,
and Zhu]{he2022effect}
He, Y.; Wang, L.; Yang, F.; Clarysse, P.; Robini, M.; Zhu, Y.
\newblock Effect of different configurations of diffusion gradient directions
on accuracy of diffusion tensor estimation in cardiac DTI.
\newblock In Proceedings of the 16th IEEE International Conference on Signal
Processing (ICSP), Beijing, China,  21--24 October 2022; Volume~1, pp.
437--441.

\bibitem[Talo \em{et~al.}(2019)Talo, Baloglu, Y{\i}ld{\i}r{\i}m, and
Acharya]{talo2019application}
Talo, M.; Baloglu, U.B.; Y{\i}ld{\i}r{\i}m, {\"O}.; Acharya, U.R.
\newblock Application of deep transfer learning for automated brain abnormality
classification using MR images.
\newblock {\em Cogn. Syst. Res.} {\bf 2019}, {\em 54},~176--188. [\href{http://dx.doi.org/10.1016/j.cogsys.2018.12.007}{CrossRef}]

\bibitem[Ren \em{et~al.}(2021)Ren, Xiao, Chang, Huang, Li, Gupta, Chen, and
Wang]{ren2021survey}
Ren, P.; Xiao, Y.; Chang, X.; Huang, P.Y.; Li, Z.; Gupta, B.B.; Chen, X.; Wang,
X.
\newblock A survey of deep active learning.
\newblock {\em ACM Comput. Surv. (CSUR)} {\bf 2021}, {\em 54},~1--40. [\href{http://dx.doi.org/10.1145/3472291}{CrossRef}]

\bibitem[Rahimi \em{et~al.}(24--25 May 2021)Rahimi, Oktay, Alvarez-Valle, and
Bharadwaj]{rahimi2021addressing}
Rahimi, S.; Oktay, O.; Alvarez-Valle, J.; Bharadwaj, S.
\newblock Addressing the exorbitant cost of labeling medical images with active
learning.
\newblock In Proceedings of the International Conference on Machine Learning in
Medical Imaging and Analysis, Barcelona, Spain,  24--25 May 2021;
p.~1.

\end{thebibliography}


%=====================================
% References, variant B: internal bibliography
%=====================================
%

% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{% Figure removed}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{% Figure removed}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \cite{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \cite{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%
%
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%
\PublishersNote{}
\end{adjustwidth}
\end{document}


