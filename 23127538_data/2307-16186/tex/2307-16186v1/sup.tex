\documentclass{ecai}


\usepackage{float}

\usepackage{graphicx}
\usepackage{latexsym}
\usepackage{subcaption}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}




%\ecaisubmission   % inserts page numbers. Use only for submission of paper.
                  % Do NOT use for camera-ready version of paper.


\begin{document}

\newtheorem{proposition}{Proposition}
\begin{frontmatter}
\title{Supplementary material for ESP: Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning}

\author[A]{\fnms{Xin}~\snm{Yu} }
\author[A]{\fnms{Rongye}~\snm{Shi} \thanks{Rongye Shi. Email: shirongye@buaa.edu.cn}}
\author[A]{\fnms{Pu}~\snm{Feng}} % use of \orcid{} is optional
\author[A]{\fnms{Yongkai}~\snm{Tian}} % use of \orcid{} is optional
\author[A]{\fnms{Jie}~\snm{Luo}} % use of \orcid{} is optional
\author[A]{\fnms{Wenjun}~\snm{Wu} } % use of \orcid{} is optional

\address[A]{Beihang University, Beijing, China}
\end{frontmatter}
%\appendix



\section{Detailed proof for proposition 1}

The Symmetry Markov game proposed in the main content is a subclass of the Markov game endowed with symmetry characterization. The Symmetry Markov game $(N, S,\left\{A_{i}\right\}_{i=1}^{n},\left\{R_{i}\right\}_{i=1}^{n}, T,\Psi , G )$ is a Markov Game that satisfies the conditions of reward invariance and transition invariance. The conditions of reward invariance and the transition invariance are given by: 
\begin{equation} 
        R(s, a)=R(L_{g}[s], K_{g}^{s}[a]), \label{Rg} 
\end{equation} 
\begin{equation} 
     T(s, a, s^{\prime})=T(L_{g}[s],K_{g}^{s}[a],L_{g}[s^{\prime}]). \label{Tg}
\end{equation}
After recapping the Symmetry Markov game, we provide the proof of proposition 1 in the following. We will use mathematical induction to prove this proposition.




\begin{proposition}[Optimal value equivalence] \label{prop1}
Consider a Symmetry Markov game $(N,S,\left\{A_{i}\right\}_{i=1}^{n},\left\{R_{i}\right\}_{i=1}^{n}, T,\Psi,G)$ with an invariant transformation $h=(L_{g},K_{g})$ where $g \in G$. For any $(s, a) \in \Psi$, it holds that $Q^{\star}(s, a)=Q^{\star}(L_{g}[s],K_{g}[a])$.
\end{proposition}


\begin{proof}
We define the $m$-step optimal discounted action value function recursively for all $(s, a) \in \Psi$ and for all non-negative integers $m$ as follows:
\begin{equation}  
\begin{aligned} 
    &Q_{m}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S}[T(s, a, s^{\prime}) \max _{a^{\prime} \in A} Q_{m-1}(s^{\prime}, a^{\prime})]. \label{eq1}
\end{aligned}
\end{equation}
where we set $ Q_{-1}\left(s^{\prime}, a^{\prime}\right)=0 $. We then define the $V_m(s)$ as: 
\begin{equation*} 
    V_{m}\left(s^{\prime}\right)=\max _{a^{\prime} \in A} Q_{m}\left(s^{\prime}, a^{\prime}\right).
\end{equation*}
We can rewrite the equation $Q_m(s,a)$ as follows:
\begin{equation*} 
    Q_{m}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S}[T(s, a, s^{\prime}) V_{m-1}(s^{\prime})].
\end{equation*}
For the base case of $m=0$, we have
\begin{equation*} 
\begin{aligned} 
Q_{0}(s, a)&=R(s,a)\\ &=R(L_{g}[s],K_{g}[a])\\ &=Q_{0}(L_{g}[s], K_{g}[a]).
\end{aligned}
\end{equation*}
As we have the base case, let's assume that $Q_{j}(s, a)=Q_{j}(L_{g}[s], K_{g}[a])$ holds for all $j<m$ and all state-action pairs $(s,a) \in\Psi$. It follows that:
\begin{equation*} 
    \max _{a \in A} Q_{j}(s, a)=\max _{a \in A} Q_{j}(L_{g}[s], K_{g}[a]),
\end{equation*}
which implies:
\begin{equation}\label{mid}  
    V_{j}(s)=V_{j}(L_{g}[s]).
\end{equation}
Now we prove by induction on $m$ that the theorem is true. For the case m, we have: 
\begin{equation}\label{casem}   
\begin{aligned}
&Q_{m}(s, a)=R(s, a)+\gamma \sum_{s^{\prime} \in S} T(s, a, s^{\prime}) V_{m-1}(s^{\prime}) \\   &=R(L_{g}[s], K_{g}[a])+\gamma \sum_{s^{\prime} \in S} T(L_{g}[s], K_{g}[a], L_{g}[s^{\prime}])V_{m-1}(s^{\prime}).
\end{aligned}
\end{equation}
Let's define:
\begin{equation*}  
    F=\gamma \sum_{s^{\prime} \in S} T(L_{g}[s], K_{g}[a], L_{g}[s^{\prime}]))V_{m-1}(s^{\prime}).
\end{equation*}
Then, by substituting Equation \eqref{mid} into $F$, we obtain:
\begin{equation*}  
    F=\gamma \sum_{s^{\prime} \in S} T(L_{g}[s], K_{g}[a], L_{g}[s^{\prime}]))V_{m-1}(L_{g}[s^{\prime}]).
\end{equation*}
Substituting $F$ back to equation \eqref{casem}, we get:
\begin{equation} \label{conc} 
    Q_{m}(s, a)=Q_{m}(L_{g}[s], K_{g}[a]).
\end{equation}
Equation \eqref{conc} shows $Q_m(s, a) = Q_m(L_g[s], K_g[a])$ for all non-negative integers $m$ via induction. The optimal action-value function $Q^*(s, a)$, the limit of $Q_m(s, a)$ as $m$ approaches infinity, also follows this property due to bounded rewards. Then we have:

\begin{equation*}  
Q^{\star}(s, a)=Q^{\star}(L_{g}[s], K_{g}[a]),
\end{equation*}
\begin{equation*}  
 V^{\star}(s)=V^{\star}(L_{g}[s]).
\end{equation*}
Therefore, by mathematical induction, the proposition 1 is true.
\end{proof}

The boundedness of the reward function is important for ensuring that the limit exists. The action-value function $Q_m(s, a)$ is computed recursively and involves summing up rewards. If the rewards weren't bounded, then these sums could potentially become infinite, and the limit as $m$ goes to infinity wouldn't exist.
\section{Examining Data Augmentation Implementation}\label{supp:chap2}
While data augmentation has been previously utilized in RL, we argue that there are practical issues with its application. To illustrate this, we will analyze the implementation of data augmentation in several parts of equation \eqref{mappo_pi}.


In the default formulation of MAPPO under the Dec-POMDP framework, the agents employ the trick of policy sharing. As such, the MAPPO learns policy $\pi_{\theta}$ by optimizing the following objective:
\begin{equation}
J_\pi(\theta)=\sum_{i=1}^n E_{ \pi_{\theta_{\text {old }}}}\left[\frac{\pi_\theta\left(a^i \mid s\right)}{\pi_{\theta_{\text {old }}}\left(a^i \mid s\right)} A^\pi\left(s, a^i\right)\right],\label{mappo_pi}
\end{equation}
where $\pi_{\theta_{\text {old }}}$ is the behavior policy used to collect trajectories, $\pi_\theta$ is the policy we want to optimize, and $A^\pi\left(s, a^i\right)$ denotes the advantage function. 



The implementation of each part in equation \eqref{mappo_pi} during the training of agents is as follows: 1)The numerator of the first term in equation \eqref{mappo_pi} is calculated using the latest policy $\pi$ and the samples obtained from the replay buffer. If the sampled data is an augmented sample, i.e., $(L_{g}[s], K_{g}[a])$, then this term is $\pi_\theta(K_{g}[a] \mid L_{g}[s])$. 2)The denominator is calculated using the state-action pair $(s,a)$ and the old policy $\pi_{\theta_{old}}$, because we can only interact with the environment through real observations and not augmented ones. This term is stored in the replay buffer during the interaction of the agent with the environment, so it is $\pi_{\theta_{old}}(K_{g}[a] \mid L_{g}[s])$. 3)The second term $A$ in equation \eqref{mappo_pi} is calculated using the rewards generated during the interaction. Therefore, this term is still based on the original samples, i.e., $A_\psi^\pi\left(s, a^i\right)$.

Thus, If transformation $(L_{g},K_{g})$ is applied to the algorithm, the MAPPO objective changes, and equation \eqref{mappo_pi} is replaced by \eqref{map}. It is worth noting that depending on the specific data augmentation method used, there may be minor differences in the details, but most papers that use data augmentation follow this approach\cite{rlaug}. Even if transforming both the numerator and denominator distributions depending on the exact implementation would result in a larger variance, which is also unreasonable.



\begin{equation}
J_\pi(\theta)=\sum_{i=1}^n E_{\pi_{\theta_{\text {old }}}}\left[\frac{\pi_\theta\left(K_{g}[a^i] \mid L_{g}[s]\right)}{\pi_{\theta_{\text {old }}}\left(a^i \mid s\right)} A_\psi^\pi\left(s, a^i\right)\right].\label{map}
\end{equation}

A certain transformation $(L_{g},K_{g})$ for data augmentation can result in an arbitrarily large ratio $\pi_\theta(K_{g}[a] \mid L_{g}[s]) / \pi_{\theta_{o l d}}(a \mid s)$. In multi-agent settings, when multiple agents are considered, more sources of variance are introduced, making the training severely unstable as shown in equation \eqref{map}. 


\section{Symmetric Transformation}


The most general form of symmetry in multi-agent systems is the global symmetry depicted in Figure \ref{example}, where rotating the global state leads to a permutation of the optimal joint policy. When a problem is characterized as a Symmetric Markov Game, it inherently exhibits this symmetry. It is important to note that the optimal policy can be expressed in terms of the optimal Q-function since the optimal action at a given state is the one that maximizes the Q-function at that state. As a result, the assertion made in Figure \ref{example} is substantiated: a global state rotation yields a permutation of the optimal joint policy. This outcome is a direct implication of the optimal Q-function equivalence, as stated in Proposition 1. We describe the specific details of symmetry transformation by taking the cooperative navigation shown in Figure \ref{example} as an example. We consider rotation transformation in the 2D space that is the set of 90 degree rotations $\left\{0^{\circ}, 90^{\circ}, 180^{\circ}, 270^{\circ}\right\}$.




% Figure environment removed


\section{ESP for Dec-POMDPs with symmetry}

Our framework ESP is based on data augmentation and is not limited by fully observable or partial observable. The method can be applied to a wide range of problems, including those modeled as Markov games and Dec-POMDPs. 

\textbf{Dec-POMDP}.A fully cooperative multi-agent task can be formalized as a decentralized partially-observable Markov decision process (Dec-POMDP) consisting of a tuple $(N,S,\left\{A_{i}\right\}_{i=1}^{n},\left\{O_{i}\right\}_{i=1}^{n},R, T,\left\{U_{i}\right\}_{i=1}^{n},\gamma)$ \cite{oliehoek2016concise}. $N,S,\left\{A_{i}\right\}_{i=1}^{n},\left\{O_{i}\right\}_{i=1}^{n}$ denote the set of states, agents, actions and observations, respectively, where a subscript $i$ represents the set pertaining to agent $i \in \mathcal{N}=\{1, \ldots, n\}$. We also write $A=\times_i A^i$ and $O=\times_i O^i$, the sets of joint actions and observations, respectively. $s_t \in S$ is the state at time $t$. $a_t \in A$ is the joint action of all agents taken at time $t$, which changes the state according to the transition distribution $s_{t+1} \sim T\left(s_{t+1} \mid s_t, a_t\right)$. The subsequent joint observation of the agents is $o_{t+1} \in O$, according to $o_{t+1} \sim U\left(o_{t+1} \mid s_{t+1}, a_t\right)$, where $U=\times_i U^i$. The reward $r_{t+1} \in R$ is according to the reward function. $\gamma \in[0,1]$ is the discount factor. Notating $\tau_t^i=\left(a_0^i, o_1^i, \ldots, a_{t-1}^i, o_t^i\right)$ for the action-observation history of agent $i$, agent $i$ acts according to a policy $a_t^i \sim \pi^i\left(a_t^i \mid \tau_t^i\right)$. The agents seek to maximize the expected discounted sum of rewards.

\textbf{Dec-POMDPs with symmetry}.
We propose Dec-POMDPs with symmetry which is a subclass of the Dec-POMDPs endowed with symmetry characterization. The Dec-POMDPs with symmetry  $(N,S,\left\{A_{i}\right\}_{i=1}^{n},\left\{O_{i}\right\}_{i=1}^{n}, R, T,\left\{U_{i}\right\}_{i=1}^{n},\gamma, G)$ is a Dec-POMDPs that satisfies the conditions of policy equivalence and value invariance. The observation transformation and action transformation in the Dec-POMDPs with symmetry are defined as $L_{g}$ and $K_{g}$, respectively. We use $\phi$ to represent symmetric transformations of the trajectory. $\phi$ acts on the action-observation history as
\begin{equation*}
    \phi\left(\tau_t^i\right)=\left(K_{g}\left(a_0^i\right),L_{g}\left(o_1^i\right), \ldots, K_{g}\left(a_{t-1}^i\right), L_{g}\left(o_t^i\right)\right).
\end{equation*}
Further, for all $g \in G$, and for all $\tau$, the conditions of policy equivalence and value invariance are given by: 
\begin{equation} 
        V(\tau)=V(\phi[\tau]), 
\end{equation} 
\begin{equation}
\pi\left(a \mid \tau\right)=\pi\left({K}_g(a) \mid \phi[\tau]\right).
\end{equation}

Our method focuses on systems with global symmetry. As depicted in Figure \ref{dec}, when the global state is rotated, the corresponding local observation also rotates. when facing partially observable problems, by utilizing the rotation transformation represented by $\phi$, we are able to rotate the observation-action pairs. A summary of the training procedure of our ESP framework for Dec-POMDPs is described in Algorithm \ref{alg}. We start by initializing the group transformation $h=(L_{g},K_{g})$ alongside several training parameters, including agent numbers, maximum steps, batch size. Then the transformation $h=(L_{g},K_{g})$ is selected according to the group $g$. After that, the real trajectories generated by agents are augmented by symmetric transformation. Both real and augmented samples are stored in an experience replay buffer. Finally, data is sampled from the experience replay buffer to update the agent network. In the execution phase, the agents can perform the policies in a distributed manner. The symmetry consistency loss for the policy and value function in Dec-POMDPs is defined as: 
\begin{equation}
S_\pi=K L\left[\pi_\theta(K_{g}[a] \mid \phi[\tau]) \mid \pi_\theta(a \mid \tau)\right],\label{symloss1}
\end{equation}
\begin{equation}
S_V=E_{\pi}\left[\left(V_\psi(\tau)-V_\psi\left(\phi[\tau]\right)\right)^2\right].\label{symloss2}
\end{equation}





\textbf{Symmetric transformation for observation.} In this task, agents need to simultaneously cover three target locations in a 2D space whilst avoiding collisions with each other. Agents observe the relative positions of other agents and landmarks and decide to move. The state of agent $i$ is represented through \emph{self velocity}, \emph{self position}, \emph{landmark relevant positions}, and \emph{other agent relevant positions}. When performing a rotation operation on a state, we can multiply each element by the rotation matrix. The element of state such as \emph{self velocity} can be represented by a vector $(V_{x}, V_{y})$. We apply rotation to generate the new velocity as shown in equation \eqref{actionp}.
\begin{equation}
\begin{aligned} 
    L_{g}[V_{x},V_{y}]&=[V_{x},V_{y}] \cdot \left[\begin{array}{cc}\cos \theta & -\sin \theta \\ \sin \theta & \cos \theta\end{array}\right]
\end{aligned} \label{actionp}
\end{equation}

\textbf{Symmetric transformation for action.} In the discrete action spaces, the action of the agent is denoted as a one-hot vector as \emph{no action}, \emph{left}, \emph{right}, \emph{down}, \emph{up}. A transformation $K_{g}$ acting on the action is represented as a matrix multiplication of the action vector with a permutation matrix. Take the action vector $[0,0,1,0,0]$ of turning right as an example. The way to apply rotation to generate a new action is shown in \eqref{actsy} where $P$ is the permutation matrix.

\begin{equation}
\begin{aligned} 
    K_{g}[a]&=[0,0,1,0,0]\cdot P \\&=[0,0,0,1,0] \label{actsy}
\end{aligned} 
\end{equation}


% Figure environment removed


\begin{algorithm}[t]
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
    \caption{Exploiting Symmetry Prior for Multi-Agent Reinforcement Learning with respect to Dec-POMDPs}
    \label{alg}
    \begin{algorithmic}[1]
        \REQUIRE Transformation $L_{g}$, $K_{g}$ and $\phi$. $g \in G$
        \STATE Initialize agent number $N$, max steps in an episode $T_e$, replay buffer $\mathcal{D}$ and the episode batch size $M$
        \STATE  Initialize $\theta$ and $\Psi$, the parameters for policy and critic
        \FOR{each episode}
            \FOR{t=1 to $T_e$}
            \STATE for each agent i, select action $a_i$ w.r.t the policy
            \STATE Execute actions $a=\left(a_1, \ldots, a_N\right)$ and Collect trajectories $\tau$
            \STATE $\hat{\tau} \longleftarrow \phi(\tau)$
            \STATE Store $\tau$ and  $\hat{\tau}$ to the replay buffer $\mathcal{B}$
            \ENDFOR
        \IF{$episodes>M$}
            \STATE Sample a minibatch  $\mathcal{B}=\left\{\tau^j\right\}_{j=0}^M \sim \mathcal{D}$
            \STATE Calculate the symmetry consistency loss using Equation \eqref{symloss1} or \eqref{symloss2}
            \STATE Update the parameters $\theta$ of the policy network
            \STATE Update the parameters $\Psi$ of the critic network
        \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}

\section{Information About Task}

In this section, we present the experimental details of the formation change task for multiple robots, which was inspired by~\cite{ijrr}. We define the observation space, action space, and reward function as follows.

\textbf{Observation space}. The observation of each robot is the concatenation of its velocity direction and the relative coordinates of the others in the environment. As shown in figure \ref{obs_space}, the observation of the robot can be denoted as :
\begin{equation*}
    (\theta_i,dx_1,dy_1,dx_2,dy_2,dx_3,dy_3),
\end{equation*}
where $\theta_i$ is the velocity direction of agent $i$. And the$(dx,dy)$indicates the relative coordinates of the others in the environment. The subscript $i$ represents the nearest obstacle, goal, and agents when equals {1,2,3} respectively.



\textbf{Action space}. We choose Epuck as the robot model in the Webots simulator. Agents can assign the two wheel-speed to control the robot. And the wheel speed are range from $-1$ to $1$.

\textbf{Reward function}. For each agent $i \in I$ we define a term for reaching the goal position and for avoiding collisions each by Equation \eqref{reward} with desired target position $g_i \in R^2$, avoidance radius $C \geq 0$. The coordination of the agent $i$ and the obstacle $j$ at step $t$ is denoted as $p_i^t$ and $c_j$.
\begin{equation}
   r_i^{t}=g_{i}^t+c_{i}^t\label{reward}.
\end{equation}
The $g_i^t$ and $c_i^t$ were defined as:
\begin{equation}
    g_{i}^t=\omega_g\left(\left\|\mathrm{p}_i^{t-1}-\mathrm{g}_i\right\|-\left\|\mathrm{p}_i^t-\mathrm{g}_i\right\|\right).
\end{equation}
The $c_i^t$ were defined as:
\begin{equation}
c_i^t= \begin{cases}c_{\text {collision }} & \text { if }\left\|\mathrm{p}_i^t-\mathrm{c}_j\right\|<C \\ 0 & \text { otherwise }\end{cases}.
\end{equation}
The parameters of the reward function are denoted as below: $r_{arrival}=0.05$, $\omega_g=100$, $c_{collision}=0.35$, $C=0.5$.


% Figure environment removed
For the cooperative navigation and predator-prey tasks, the description of the state and action spaces, as well as the reward functions, can be found in the pettingzoo library website: https://pettingzoo.farama.org/environments/mpe/.






\section{Physical Deployment}
For real-world scenarios, we believe that absolute coordinates are part of the agents' inputs, and ESP can be beneficial for performance improvement. We performed physical experiments on a set of e-puck2 robots, a widely used mini mobile robot developed by EPFL in various research and educational scenarios \cite{epuck}. To increase their computing power, we added an expansion board with Raspberry Pi, as shown in Figure \ref{pipuck} and \ref{real_epuck}. The pi-puck extension, which consists of a Raspberry Pi Zero W and adapter PCB, was directly connected to the onboard MCU and sensors for interaction \cite{pipuck}. We installed ROS2 on the Raspberry Pi Zero W, and the deep neural network was used to control the physical robot \cite{ros}. The experiments took place in a 2m $\times$ 2m indoor arena with walls on the boundary, and a centralized motion capture system was added to capture the position of each robot. We used a motion capture system that provides absolute coordinates, allowing the e-puck to benefit from the ESP. This allowed us to calculate the state and observation information required by the policy \cite{vicon}.

To make the experiments easy to implement, we removed the obstacles introduced in section 6.1. We have integrated our ESP approach into the MAPPO algorithm, which has resulted in a reduction in the number of risky states encountered by the agents during task completion. Risky states refer to those states in which the distance between agents is less than 5 centimeters, and the rate of risky states is calculated as the proportion of such states to all states. In comparison to the MAPPO algorithm, the ESP-MAPPO algorithm has demonstrated a lower rate of risky states, with only 2.2\% of states being classified as risky, while the rate for MAPPO is higher at 5.8\%. For further information and visual demonstrations of the findings, please refer to the supplementary materials, which include detailed videos.

% Figure environment removed

% Figure environment removed


% Figure environment removed



\begin{table*}[h]
\caption{Episode rewards of the ESP and baselines on the CN task; ’500k’ and ’3000k’ represent the number of training steps of the algorithms. 'N' represents the number of agents. The error bars are the standard error of the mean.}
\label{table1}
\centering
\begin{tabular}{cccccccc}\toprule
\multicolumn{1}{l}{Agent} & Step  & MADDPG       & MADDPG-ESP   & QMIX          & QMIX-ESP     & MAPPO        & MAPPO-ESP    \\ \midrule
\multirow{2}{*}{N=2} & 500K  & -101.42±3.24 & -94.12±2.15 & -106.65±3.68. & -98.75±1.89  & -99.12±3.25  & -92.68±2.65  \\
                     & 3000K & -95.72±1.25  & -90.92±2.54  & -95.95±2.42   & -89.32±2.31  & -94.67±1.87  & -87.54±2.56  \\
\multirow{2}{*}{N=3} & 500k & -145.23±3.71 & -132.22±1.27 & -154.24±3.16 & -140.12±2.25 & -132.65±2.89 & -122.25±1.62 \\
& 3000k & -122.63±6.04 & -112.21±2.75 & -124.67±3.05 & -113.84±1.43 & -126.97±2.12 & -110.22±2.12 \\

\multirow{2}{*}{N=4} & 500K  & -187.57±4.12 & -179.93±5.24 & -192.57±4.23  & -186.32±3.14 & -189.42±4.52 & -183.87±3.24 \\
                     & 3000K & -183.43±4.68 & -178.62±5.12 & -191.57±4.24  & -185.92±3.65 & -187.73±2.45 & -182.81±2.98 \\ \bottomrule \label{tableall}
\end{tabular}
\end{table*}



\section{Hyperparameters And Experimental Details}

For clarity of presentation, the MADDPG is used as an example to introduce the symmetry consistency loss for a value-based algorithm. The MADDPG adapts the centralized training with decentralized execution (CTDE) framework. The centralized action-value function $Q_i^{\boldsymbol{\mu}}$ is updated by:
\begin{equation}
    S_{MADDPG}(\phi)=E_{s,a}\left[(Q\left(s, a_1, \ldots, a_N\right)-y)^2\right]
\end{equation}
where$\quad y=r+\gamma Q(s^{\prime}, a_1^{\prime}, \ldots, a_N^{\prime})$ and $a_j^{\prime}$ is obtained by the current policy; $Q_\phi(s, a)$ represent the Q-value under the original state input, and $Q_\phi(L_{g}[s], K_{g}^{s}[a])$ to represent the $\mathrm{Q}$-value under symmetry state input where $a$ indicates the joint action of all agents. Then, the square of $L_2$ distance between the Q-network outputs under the original state, and the symmetric transformed state can be directly calculated by:
\begin{equation}
    S_{MADDPG}\left(\phi\right)=E_{s, a}\left[\left(Q_\phi(s, a )-Q_\phi(L_{g}[s], K_{g}^{s}[a])\right)^2\right].\label{symloss3}
\end{equation}
This item can help constrain the agent's value function to satisfy the defined symmetry constraints. 

In all our scenarios, the hyperparameters used the default parameters from paper \cite{mappo}. Tables \ref{tablevalue} describe the hyperparameters for MAPPO. For ESP, each interaction sample along with its augmented ones will be added all together to the buffer at each time step. We used the same buffer size for both ESP and non-ESP across all experiments.

\begin{table}
\centering
\caption{hyperparameters used in MAPPO.}
\begin{tabular}{cc}
\hline
Hyperparameters             & value                      \\ \hline
recurrent data chunk length & 10                         \\
gradient clip norm          & 10.0                       \\
gae lamda                   & 0.95                       \\
gamma                       & 0.99                       \\
value loss                  & huber loss                 \\
huber delta                 & 10.0                       \\
batch size                  & buffer length × num agents \\
mini batch size             & batch size / mini-batch    \\
optimizer                   & Adam                       \\
optimizer epsilon           & 1e-5                       \\
weight decay                & 0                          \\
network initialization      & Orthogonal                 \\
use reward normalization    & True                       \\
use feature normalization   & True                       \\ \hline \label{tablevalue}
\end{tabular}
\end{table}


Table \ref{tableppo} describe the hyperparameters for the QMix, and MADDPG. We utilize parameter sharing when there are homogenous agents because this has been the accepted practice in the majority of past MARL works \cite{wangrode,christianos2021scaling,qmix}. To train the QMIX algorithm in the formation change task, the continuous action space were discretized into a finite set of atomic actions and reduce the original task into a new task with a discrete action space. For a continuous action space of -1 to 1, the action space were discretized every 0.2. The computational experiments were performed on a Linux server with an i9-12900KF CPU. The Python version used was 3.9.12, and the PyTorch version used was 1.13.0.




\begin{table}
\centering
\caption{Hyperparameters used in QMix and MADDPG. \label{tableppo}}
\begin{tabular}{cc}
\hline
Hyperparameters           & Value                \\ \hline
Gradient clip norm        & 10.0                 \\
Random episodes           & 5                    \\
Epsilon                   & 1->0.05 \\
Epsilon anneal time       & 50000 timesteps      \\
Train interval            & 1 episode            \\
Gamma                     & 0.99                 \\
Buffer size               & 5000 episodes        \\
Batch size                & 32 episodes          \\
Optimizer eps             & 1e-5                 \\
Optimizer                 & Adam                 \\
Weight decay              & 0                    \\
Network initialization    & Orthogonal           \\
Use reward normalization  & True                 \\
Use feature normalization & True                 \\ \hline
\end{tabular}
\end{table}


\section{Additional Experimental Details}
We extended the ablation experiments in the main text to QMIX and MAPPO algorithms, as shown in Figure \ref{sup1}. To examine the impact of the number of agents on our ESP, we conducted experiments on the cumulative discounted reward under different numbers of agents, as shown in Table \ref{tableall}. It can be observed that our method is effective for various numbers of agents. 

\bibliography{ecai}
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
