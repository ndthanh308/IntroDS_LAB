@article{adahessian_2021,
  title      = {{ADAHESSIAN}: {An} {Adaptive} {Second} {Order} {Optimizer} for {Machine} {Learning}},
  volume     = {35},
  copyright  = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  issn       = {2374-3468},
  shorttitle = {{ADAHESSIAN}},
  url        = {https://ojs.aaai.org/index.php/AAAI/article/view/17275},
  doi        = {10.1609/aaai.v35i12.17275},
  language   = {en},
  number     = {12},
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author     = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael},
  month      = may,
  year       = {2021},
  note       = {Number: 12},
  keywords   = {Learning \& Optimization for SNLP},
  pages      = {10665--10673},
  file       = {Full Text PDF:/Users/swarnita/Zotero/storage/V4A6Q32J/Yao et al. - 2021 - ADAHESSIAN An Adaptive Second Order Optimizer for.pdf:application/pdf}
}

misc{adagrad_2021,
  title     = {Scalable {Second} {Order} {Optimization} for {Deep} {Learning}},
  url       = {http://arxiv.org/abs/2002.09018},
  doi       = {10.48550/arXiv.2002.09018},
  urldate   = {2022-11-30},
  publisher = {arXiv},
  author    = {Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
  month     = mar,
  year      = {2021},
  note      = {arXiv:2002.09018 [cs, math, stat]},
  keywords  = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
  annote    = {Comment: 24 pages, Code available here: https://bit.ly/3uXXtKy},
  file      = {arXiv Fulltext PDF:/Users/swarnita/Zotero/storage/Q36GFTRX/Anil et al. - 2021 - Scalable Second Order Optimization for Deep Learni.pdf:application/pdf;arXiv.org Snapshot:/Users/swarnita/Zotero/storage/ADH4KEMQ/2002.html:text/html}
}


@book{doi:10.1137/1.9780898719857,
  author       = {Conn, Andrew R. and Gould, Nicholas I. M. and Toint, Philippe L.},
  title	       = {Trust Region Methods},
  publisher    = {Society for Industrial and Applied Mathematics},
  year	       = 2000,
  doi	       = {10.1137/1.9780898719857},
  URL	       = {https://epubs.siam.org/doi/abs/10.1137/1.9780898719857},
  eprint       = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898719857}
}


@inproceedings{kaisa_2021,
author = {Pauloski, J. Gregory and Huang, Qi and Huang, Lei and Venkataraman, Shivaram and Chard, Kyle and Foster, Ian and Zhang, Zhao},
title = {KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural Networks},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476152},
doi = {10.1145/3458817.3476152},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {13},
numpages = {14},
keywords = {K-FAC, data-parallel algorithms, distributed computing, second-order optimization, machine learning},
location = {St. Louis, Missouri},
series = {SC '21}
}

  
inproceedings{kaisa_2021,
  title     = {{KAISA}: an adaptive second-order optimizer framework for deep neural networks},
  author    = {Pauloski, J Gregory and Huang, Qi and Huang, Lei and Venkataraman, Shivaram and Chard, Kyle and Foster, Ian and Zhang, Zhao},
  booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages     = {1--14},
  year      = {2021}
}

@techreport{Quasi_Gauss_Newton_2020,
      title={Deep Neural Network Learning with Second-Order Optimizers -- a Practical Study with a Stochastic Quasi-{Gauss}-Newton Method}, 
      author={Christopher Thiele and Mauricio Araya-Polo and Detlef Hohl},
      year={2020},
      institution = {ArXiV},
      number = {arXiv:2004.03040 [cs.LG]},
      url = {https://doi.org/10.48550/arXiv.2004.03040},
      doi = {10.48550/arXiv.2004.03040},
      eprint={2004.03040},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@techreport{second_order_optimization,
      title={Scalable Second Order Optimization for Deep Learning}, 
      author={Rohan Anil and Vineet Gupta and Tomer Koren and Kevin Regan and Yoram Singer},
      institution = {ArXiV},
      number = {arXiv:2002.09018 [cs.LG]},
      doi = {10.48550/arXiv.2002.09018},
      url = {https://doi.org/10.48550/arXiv.2002.09018},
      year={2021},
      eprint={2002.09018},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

article{second_order_optimization,
  title   = {Second order optimization made practical},
  author  = {Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
  journal = {arXiv preprint arXiv:2002.09018},
  year    = {2020}
}

@inbook{pytorch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12}
}


@techreport{resnet,
  doi       = {10.48550/ARXIV.1512.03385},
  institution = {ArXiV},
  number = {arXiv:1512.03385 [cs.CV]},
  url       = {https://arxiv.org/abs/1512.03385},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Deep Residual Learning for Image Recognition},
  year      = {2015},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 @techreport{cifar10,
  institution = {University of Toronto},
  address = {Toronto, ON, CA},
  title = {Learning Multiple Layers of Features from Tiny Images},
  author  = {Krizhevsky, Alex},
  year    = {2009}
}

@article{WATSON1989369,
title = {Globally convergent homotopy methods: A tutorial},
journal = {Applied Mathematics and Computation},
volume = {31},
pages = {369-396},
year = {1989},
note = {Special Issue Numerical Ordinary Diferrential Equations (Proceedings of the 1986 ODE Conference)},
issn = {0096-3003},
doi = {https://doi.org/10.1016/0096-3003(89)90129-X},
url = {https://www.sciencedirect.com/science/article/pii/009630038990129X},
author = {Layne T. Watson},
abstract = {The basic theory for probability one globally convergent homotopy algorithms was developed in 1976, and since then the theory, algorithms, and applications have considerably expanded. These are algorithms for solving nonlinear systems of (algebraic) equations, which are convergent for almost all choices of starting point. Thus they are globally convergent with probability one. They are applicable to Brouwer fixed point problems, certain classes of zero-finding problems, unconstrained optimization, linearly constrained optimization, nonlinear complementarity, and the discrezations of nonlinear two-point boundary value problems based on shooting, finite differences, collocation, and finite elements. A mathematical software package, HOMPACK, exists that implements several different strategies and handles both dense and sparse problems. Homotopy algorithms are closely related to ODE algorithms, and make heavy use of ODE techniques. Homotopy algorithms for some classes of nonlinear systems, such as polynomial systems, exhibit a large amount of coarse grain parallelism. These and other topics are discussed in a tutorial fashion.}
}                  



@article{fletcher_reeves,
  author   = {Fletcher, R. and Reeves, C. M.},
  title    = {{Function minimization by conjugate gradients}},
  journal  = {The Computer Journal},
  volume   = {7},
  number   = {2},
  pages    = {149-154},
  year     = {1964},
  month    = {01},
  abstract = {{A quadratically convergent gradient method for locating an unconstrained local minimum of a function of several variables is described. Particular advantages are its simplicity and its modest demands on storage, space for only three vectors being required. An ALGOL procedure is presented, and the paper includes a discussion of results obtained by its used on various test functions.}},
  issn     = {0010-4620},
  doi      = {10.1093/comjnl/7.2.149},
  url      = {https://doi.org/10.1093/comjnl/7.2.149},
  eprint   = {https://academic.oup.com/comjnl/article-pdf/7/2/149/959725/070149.pdf}
}
@article{lbfgs,
  author   = {Liu, Dong C.
              and Nocedal, Jorge},
  title    = {On the limited memory BFGS method for large scale optimization},
  journal  = {Mathematical Programming},
  year     = {1989},
  month    = {8},
  day      = {01},
  volume   = {45},
  number   = {1},
  pages    = {503-528},
  abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
  issn     = {1436-4646},
  doi      = {10.1007/BF01589116},
  url      = {https://doi.org/10.1007/BF01589116}
}

@article{doi:10.1137/0916069,
  author       = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
  title	       = {A Limited Memory Algorithm for Bound Constrained Optimization},
  journal      = {SIAM Journal on Scientific Computing},
  volume       = 16,
  number       = 5,
  pages	       = {1190-1208},
  year	       = 1995,
  doi	       = {10.1137/0916069},
  URL	       = { https://doi.org/10.1137/0916069 },
  eprint       = { https://doi.org/10.1137/0916069 }
,
  abstract     = { An algorithm for solving large nonlinear optimization problems with
                  simple bounds is described. It is based on the gradient projection
                  method and uses a limited memory BFGS matrix to approximate the
                  Hessian of the objective function. It is shown how to take advantage
                  of the form of the limited memory approximation to implement the
                  algorithm efficiently. The results of numerical tests on a set of
                  large problems are reported. }
}
                

@article{deng2012mnist, 
  title={The {MNIST} database of handwritten digit images for machine learning research}, 
  author={Deng, Li}, 
  journal={IEEE Signal Processing Magazine}, 
  volume={29}, 
  number={6}, 
  pages={141--142}, 
  year={2012}, 
  doi={10.1109/MSP.2012.2211477},
  publisher={IEEE} 
}



@article{scipy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
             Haberland, Matt and Reddy, Tyler and Cournapeau, David and
             Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
             Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
             Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
             Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
             Kern, Robert and Larson, Eric and Carey, C J and
             Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
             {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
             Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
             Harris, Charles R. and Archibald, Anne M. and
             Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
             {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
             Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2}
}


@article{anova_larson2008analysis,
  title     = {Analysis of variance},
  author    = {Larson, Martin G},
  journal   = {Circulation},
  volume    = {117},
  number    = {1},
  pages     = {115--121},
  year      = {2008},
  url       = {https://doi.org/10.1161/CIRCULATIONAHA.107.654335},
  doi       = {CIRCULATIONAHA.107.654335},
  publisher = {Am Heart Assoc}
}

@book{anova_scheffe1999analysis,
  title     = {The Analysis of Variance},
  author    = {Scheffe, Henry},
  volume    = {72},
  year      = {1999},
  publisher = {John Wiley \& Sons}
}

@book{Goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  url      =  {http://www.deeplearningbook.org},
  year      = {2016}
}

@misc{stackexchange-soo-a,
  title        = {Why second order {SGD} convergence methods are unpopular for deep learning?},
  author       = {{DeltaIV}},
  howpublished = {Cross Validated},
  note         = {(version: 2019-02-26)},
  url          = {https://stats.stackexchange.com/q/394108}
}

@misc{stackexchange-soo-b,
  title        = {Why is {Newton's} method not widely used in machine learning?},
  author       = {{jwimberley}},
  howpublished = {Cross Validated},
  note         = {(version: 2016-12-29)},
  url          = {https://stats.stackexchange.com/q/253636}
}

@misc{stackexchange-soo-positive,
  title        = {Why is {Newton's} method not widely used in machine learning?},
  author       = {{Nick Alger}},
  howpublished = {Cross Validated},
  note         = {(version: 2017-04-13)},
  url          = {https://stats.stackexchange.com/q/253830}
}

@article{lecun_gradient_based_learning,
  author  = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal = {Proceedings of the IEEE},
  title   = {Gradient-based learning applied to document recognition},
  year    = {1998},
  volume  = {86},
  number  = {11},
  pages   = {2278--2324},
  doi     = {10.1109/5.726791},
  url     = {https://doi.org/10.1109/5.726791}
}

@inbook{lecun2012efficient,
	abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
	address = {Berlin, Heidelberg},
	author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"u}ller, Klaus -Robert},
	booktitle = {Neural Networks: Tricks of the Trade},
	doi = {10.1007/3-540-49430-8_2},
	isbn = {978-3-540-49430-0},
	pages = {9--50},
	publisher = {Springer Berlin Heidelberg},
	title = {Efficient BackProp},
	url = {https://doi.org/10.1007/3-540-49430-8_2},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1007/3-540-49430-8_2}}


@inproceedings{le2011optimization,
author = {Le, Quoc V. and Ngiam, Jiquan and Coates, Adam and Lahiri, Abhik and Prochnow, Bobby and Ng, Andrew Y.},
title = {On Optimization Methods for Deep Learning},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {265â€“-272},
url = {http://www.icml-2011.org/papers/210_icmlpaper.pdf},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@article{byrd_stochastic_lbfgs,
  author   = {Byrd, R. H. and Hansen, S. L. and Nocedal, Jorge and Singer, Y.},
  title    = {A Stochastic Quasi-Newton Method for Large-Scale Optimization},
  journal  = {SIAM Journal on Optimization},
  volume   = {26},
  number   = {2},
  pages    = {1008-1031},
  year     = {2016},
  doi      = {10.1137/140954362},
  url      = {
              https://doi.org/10.1137/140954362
              },
  eprint   = {
              https://doi.org/10.1137/140954362
              },
  abstract = { The question of how to incorporate curvature information into
              stochastic approximation methods is challenging. The direct application of
              classical quasi-Newton updating techniques for deterministic optimization
              leads to noisy curvature estimates that have harmful effects on the
              robustness of the iteration. In this paper, we propose a stochastic
              quasi-Newton method that is efficient, robust, and scalable. It employs the
              classical BFGS update formula in its limited memory form, and is based on
              the observation that it is beneficial to collect curvature information
              pointwise, and at spaced intervals. One way to do this is through
              (subsampled) Hessian-vector products. This technique differs from the
              classical approach that would compute differences of gradients at every
              iteration, and where controlling the quality of the curvature estimates can
              be difficult. We present numerical results on problems arising in machine
              learning that suggest that the proposed method shows much promise.
              }
}

@misc{minibatch_strength,
  doi       = {10.48550/ARXIV.2102.05375},
  url       = {https://arxiv.org/abs/2102.05375},
  author    = {Ziyin, Liu and Liu, Kangqiao and Mori, Takashi and Ueda, Masahito},
  keywords  = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information
               sciences, FOS: Computer and information sciences},
  title     = {Strength of Minibatch Noise in SGD},
  publisher = {arXiv},
  year      = {2021},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

 @misc{silk_2022_anon,
  title     = {Anonymous Implementation},
  journal   = {GitHub},
  publisher = {Anonymous Publisher},
  author    = {Anonymous, Author},
  year      = {2022},
  month     = {6}
}

 @misc{silk_2022,
  title     = {PyTorch SOO: Second Order Optimizers for Machine Learning},
  url       = {https://github.com/pnnl/pytorch_soo},
  journal   = {GitHub},
  publisher = {Pacific Northwest National Lab},
  author    = {Silk, Eric},
  year      = {2022},
  month     = {6}
}

@techreport{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      institution = {ArXiV},
      number = {arXiv:1412.6980 [cs.LG]},
      doi = {10.48550/arXiv.1412.6980},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@techreport{smith2021origin,
      title={On the Origin of Implicit Regularization in Stochastic Gradient Descent}, 
      author={Samuel L. Smith and Benoit Dherin and David G. T. Barrett and Soham De},
      year={2021},
      institution = {ArXiV},
      number = {arXiv:2101.12176 [cs.LG]},
      eprint={2101.12176},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{barrett2022implicit,
      title={Implicit Gradient Regularization}, 
      author={David G. T. Barrett and Benoit Dherin},
      year={2022},
      eprint={2009.11162},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}