@article{adahessian_2021,
  title      = {{ADAHESSIAN}: {An} {Adaptive} {Second} {Order} {Optimizer} for {Machine} {Learning}},
  volume     = {35},
  copyright  = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  issn       = {2374-3468},
  shorttitle = {{ADAHESSIAN}},
  url        = {https://ojs.aaai.org/index.php/AAAI/article/view/17275},
  doi        = {10.1609/aaai.v35i12.17275},
  abstract   = {Incorporating second-order curvature information into machine learning optimization algorithms can be subtle, and doing so na√Øvely can lead to high per-iteration costs associated with forming the Hessian and performing the associated linear system solve. To address this, we introduce ADAHESSIAN, a new stochastic optimization algorithm. ADAHESSIAN directly incorporates approximate curvature information from the loss function, and it includes several novel performance-improving features, including: (i) a fast Hutchinson based method to approximate the curvature matrix with low computational overhead; (ii) a spatial averaging to reduce the variance of the second derivative; and (iii) a root-mean-square exponential moving average to smooth out variations of the second-derivative across different iterations. We perform extensive tests on NLP, CV, and recommendation system tasks, and ADAHESSIAN achieves state-of-the-art results. In particular, we find that ADAHESSIAN: (i) outperforms AdamW for transformers by0.13/0.33 BLEU score on IWSLT14/WMT14, 2.7/1.0 PPLon PTB/Wikitext-103; (ii) outperforms AdamW for Squeeze-Bert by 0.41 points on GLUE; (iii) achieves 1.45\%/5.55\%higher accuracy on ResNet32/ResNet18 on Cifar10/ImageNetas compared to Adam; and (iv) achieves 0.032\% better score than Adagrad for DLRM on the Criteo Ad Kaggle dataset. The cost per iteration of ADAHESSIANis comparable to first-order methods, and ADAHESSIAN exhibits improved robustness towards variations in hyperparameter values. The code for ADAHESSIAN is open-sourced and publicly-available [1].},
  language   = {en},
  number     = {12},
  urldate    = {2022-11-30},
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author     = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael},
  month      = may,
  year       = {2021},
  note       = {Number: 12},
  keywords   = {Learning \& Optimization for SNLP},
  pages      = {10665--10673},
  file       = {Full Text PDF:/Users/swarnita/Zotero/storage/V4A6Q32J/Yao et al. - 2021 - ADAHESSIAN An Adaptive Second Order Optimizer for.pdf:application/pdf}
}


misc{adagrad_2021,
  title     = {Scalable {Second} {Order} {Optimization} for {Deep} {Learning}},
  url       = {http://arxiv.org/abs/2002.09018},
  doi       = {10.48550/arXiv.2002.09018},
  abstract  = {Optimization in machine learning, both theoretical and applied, is presently dominated by first-order gradient methods such as stochastic gradient descent. Second-order optimization methods, that involve second derivatives and/or second order statistics of the data, are far less prevalent despite strong theoretical properties, due to their prohibitive computation, memory and communication costs. In an attempt to bridge this gap between theoretical and practical optimization, we present a scalable implementation of a second-order preconditioned method (concretely, a variant of full-matrix Adagrad), that along with several critical algorithmic and numerical improvements, provides significant convergence and wall-clock time improvements compared to conventional first-order methods on state-of-the-art deep models. Our novel design effectively utilizes the prevalent heterogeneous hardware architecture for training deep models, consisting of a multicore CPU coupled with multiple accelerator units. We demonstrate superior performance compared to state-of-the-art on very large learning tasks such as machine translation with Transformers, language modeling with BERT, click-through rate prediction on Criteo, and image classification on ImageNet with ResNet-50.},
  urldate   = {2022-11-30},
  publisher = {arXiv},
  author    = {Anil, Rohan and Gupta, Vineet and Koren, Tomer and Regan, Kevin and Singer, Yoram},
  month     = mar,
  year      = {2021},
  note      = {arXiv:2002.09018 [cs, math, stat]},
  keywords  = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
  annote    = {Comment: 24 pages, Code available here: https://bit.ly/3uXXtKy},
  file      = {arXiv Fulltext PDF:/Users/swarnita/Zotero/storage/Q36GFTRX/Anil et al. - 2021 - Scalable Second Order Optimization for Deep Learni.pdf:application/pdf;arXiv.org Snapshot:/Users/swarnita/Zotero/storage/ADH4KEMQ/2002.html:text/html}
}