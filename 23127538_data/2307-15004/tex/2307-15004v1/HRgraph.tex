\documentclass[11pt]{article}
%
%%%%%%%%%%%%
%\usepackage{refcheck}
%%%%%%%%%%%%

\usepackage{latexsym,epsfig,amssymb,amsmath,amsthm,color,url,bbm,pdfsync}
\usepackage{graphicx,float,bm}
\usepackage{subcaption}
\usepackage{amsfonts}
\usepackage{graphics}
\usepackage{mathrsfs,hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\vect}[1]{\boldsymbol{\mathbf{#1}}}
\pagestyle{myheadings}

\usepackage{fancyhdr, graphicx,tikz}
\usetikzlibrary{arrows,positioning} 

\linespread{1.2}

\textwidth 6.50in
\topmargin -0.50in
\oddsidemargin 0in
\evensidemargin 0in
\textheight 9.00in
\numberwithin{equation}{section}
\allowdisplaybreaks
%\usepackage{refcheck}
\usepackage[round,comma]{natbib}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{figur}[lemma]{Figure}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{condition}{Condition}[section]
\newcommand{\indep}{\perp \!\!\! \perp}

\title{Graphical lasso for extremes}
\author{
Phyllis Wan\footnote{Erasmus University Rotterdam; Econometric Institute, Burg.\ Oudlaan 50, 3062 PA Rotterdam, the Netherlands;
email: wan@ese.eur.nl}
\and
Chen Zhou\footnote{Erasmus University Rotterdam; Econometric Institute, Burg.\ Oudlaan 50, 3062 PA Rotterdam, the Netherlands;
email: zhou@ese.eur.nl}}
\date{}

\begin{document}
\maketitle

\begin{abstract}
In this paper we estimate the sparse dependence structure in the tail region of a multivariate random vector, potentially of high dimension. The tail dependence is modeled via a graphical model for extremes embedded in the H\"usler-Reiss distribution \citep{eng2018a}.  We propose the \textit{extreme graphical lasso} procedure to estimate the sparsity in the tail dependence, similar to the Gaussian graphical lasso method in high dimensional statistics.  We prove its consistency in identifying the graph structure and estimating model parameters. The efficiency and accuracy of the proposed method are illustrated in simulated and real examples.
\end{abstract}
{\footnotesize \noindent\it Keywords and phrases: graphical lasso; graphical models; multivariate extreme value statistics; high dimensional statistics; H\"usler-Reiss distribution; } \\
{\footnotesize {\it AMS 2010 Classification:} 62G32; 62H12; 62F12.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Consider a random Gaussian vector with mean zero and covariance matrix $\Sigma$. In a Gaussian graphical model, the precision matrix $\Theta:=\Sigma^{-1}$ encodes the conditional dependence structure among the variables -- variables $i$ and $j$ are conditionally independent given the rest of the variables if and only if $\Theta_{ij}=0$ \citep{Lauritzen}.  

Given an estimate of the covariance $\hat\Sigma$, the {\it graphical lasso} method estimates a sparse $\Theta$ using $L_1$-regularization by 
$$
	{\arg\min}_{\Theta}  \left\{- \log|\Theta| + tr \left( \hat\Sigma\Theta\right) + \gamma_n\sum_{i\ne j}|\Theta_{ij}| \right\};
$$ 
see, e.g.~\cite{yuan2007model}, \cite{banerjee2008model} and \cite{fri2008}. The advantage of the graphical lasso method is two folds.  First, it reveals the conditional dependence among the underlying random variables by producing a sparse estimate of $\Theta$.  Second, it provides a reliable estimation of $\Theta$ and $\Sigma$ in the high dimensional case where classical covariance estimation fails. The theoretical properties of the graphical lasso procedure were investigated in \cite{rothman2008sparse} and \cite{ravikumar2011high}.%, next to an early study by \cite{mei2006} using a lasso based neightborhood selection method.

In this paper, we aim to estimate the sparse dependence structure in the {\it tail region} among high dimensional random variables. With the characterization of tail dependence, one can further conduct statistical risk assessment of extreme (co-)occurrences, such as systemic banking failures (e.g.~\cite{zhou2010banks}) or compound environmental events (e.g.~\cite{col1991}). Our approach is built on the framework of \cite{eng2018a}, which introduces graphical models for extremes by defining the conditional dependence in the tail distribution.

A parametric distribution family that can accommodates sparse graphical models for extremes is the H\"usler-Reiss (HR) distribution \citep{hue1989}. The class of HR distributions describes the non-trivial limiting tail distributions of Gaussian triangular arrays. Similar to Gaussian distribution, its parametrized by bilateral relations. More specifically, a $d$-dimensional HR graphical model can be parametrized by a precision matrix $\Theta\in \mathbb{R}^{d\times d}$, such that the variables $i$ and $j$ are conditionally independent in the extremes given the rest of the variables if and only if $\Theta_{ij}=0$ \citep{eng2018a,hentschel2022statistical}.  

Unlike the Gaussian case, the precision matrix $\Theta$ in the HR model is not of full rank. As a consequence, existing statistical inference procedures for estimating $\Theta$ in a HR model require conditioning on a chosen dimension being above a high threshold. In turn, one can only estimate $\Theta^{(k)} \in \mathbb{R}^{(d-1)\times(d-1)}$, the submatrix of $\Theta$ where the $k$-th row and $k$-th column are removed \citep{eng2018a}. Estimating a HR graphical model is therefore challenging when a sparse $\Theta$ is desired: a sparse estimate of $\Theta^{(k)}$ does not guarantee sparsity on the omitted $k$-th row and column. \cite{hentschel2022statistical} proposed an estimation procedure for $\Theta$ using matrix completion when the sparsity structure of $\Theta$ was known. To date, the only sparse estimation for $\Theta$ without knowing the sparsity structure ex-ante was proposed by \cite{engelke2021learning}. They achieved this goal by aggregating sparse estimates of $\Theta^{(k)}$ for all $k=1,\ldots,d$ using a majority vote to decide whether or not each entry of $\Theta$ should be zero.  In other words, their estimation procedure requires estimating $d$ graphical models which can be computationally intensive for large $d$. 

In this paper, we propose a direct estimate of $\Theta$ with a built-in option for sparse estimation via $L_1$-regularization.  We term it the {\it extreme graphical lasso}.  The core idea is as follows.  We show that by adding a positive constant $c$ to each entry of $\Theta$, the matrix
$$
	\Theta^* := \Theta + c\mathbf1\mathbf1^T
$$
is the inverse of a covariance matrix $\Sigma^*$ which can be estimated consistently from observations. To impose sparsity on the entries of $\Theta$, we only need to shrink the off-diagonal entries of $\Theta^*$ to $c$, which can be achieved in the optimization
$$
	{\arg\min}_{\Theta^*}  \left\{- \log|\Theta^*| + tr \left( \hat\Sigma^*\Theta^*\right) + \gamma_n\sum_{i\ne j}|\Theta^*_{ij}-c|\right\}.
$$
The extreme graphical lasso method requires solving only one opimization problem and therefore is efficient in handling high dimensional situations. In addition, it results in both graph structure identification and parameters estimation simultaneously. The efficiency and accuracy are the main advantages of this novel method.

We provide the finite sample theory and asymptotic theory for the extreme graphical lasso method. In particular, we show a consistent identification of the graph and accurate estimation of the non-sparse parameters in $\Theta$. Empirically, we argue that in the high dimensional case, the extreme graphical lasso method can be further simplified by dropping the constant $c$, coinciding with the classical graphical lasso algorithm. We apply the extreme graphical lasso to a real data example to illustrate its usefulness in uncovering the underlying dependence structure of extreme events.

The remainder of the paper is structured as follows.  The background for HR graphical models is introduced in Section~\ref{sec:hrgraph}.  We present our extreme graphical lasso method in Section~\ref{sec:lasso}.  The non-asymptotic and asymptotic theories are shown in Section~\ref{sec:results}.  Finally, the performance of the method is illustrated in Section~\ref{sec:example}.

\subsection{Notation}
We will use the following notations.  Let $\mathbf0$ and $\mathbf1$ denote vectors whose elements are all 0's and all 1's respectively.  For simplicity, with a slight abuse of notation, we may let them denote vectors of different length in different contexts.  For the norms for matrices: $\Vert\cdot\Vert_\infty$ is the element-wise $L_\infty$-norm, both for vectors and matrices; $|||\cdot|||_\infty$ is the $l_\infty$-operator norm for matrices, i.e.~the row-maxima of $L_1$-norms applied to each row. We note the following properties of these norms:
\begin{itemize}
	\item Both $\Vert\cdot\Vert_\infty$ and $|||\cdot|||_\infty$ are norms.
	\item For matrix $A$ and vector $v$, $\Vert Av\Vert_\infty\leq ||| A|||_\infty \Vert v\Vert_\infty$.
	\item For matrices $A$ and $B$ with compatible dimensions, $||| AB|||_\infty\leq ||| A|||_\infty||| B|||_\infty$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{H\"usler-Reiss graphical models} \label{sec:hrgraph}

In this section, we describe the class of HR graphical models and the corresponding statistical inference procedure in existing literature.

\subsection{Graphical models for extremes}
Consider a random vector $\mathbf{X}=(X_1,\ldots,X_d)$. Denote $\tilde X_k = \frac{1}{1-F_k(X_k)}$, where $F_k$ is the marginal distribution function of $X_k$. Then $\mathbf{\tilde X}=(\tilde X_1,\ldots,\tilde X_d)$ is a random vector with standard Pareto marginals and summarizes the dependence structure of $\mathbf{X}$. Following multivariate extreme value theory, we assume that $\mathbf{\tilde X}$ belongs to the domain of attraction of a multivariate extreme value distribution, i.e. the limit of its component-wise maxima converges to a non-degenerate distribution. Specifically, given i.i.d.~copies of  $\mathbf{\tilde X}$, $\mathbf{\tilde X}^i = (\tilde X_1^i,\ldots,\tilde X_d^i),i\in\mathbb{N}$, there exists a random vector $\mathbf{Z} = (Z_1,\ldots,Z_d)$ such that
\begin{equation} \label{eq:maxima}
	P(\mathbf{Z} \le \mathbf{z}) := \lim_{n\to\infty} P\left( \max_{i=1,\ldots,n} \tilde X_1^i \le nz_1,\ldots, \max_{i=1,\ldots,n} \tilde  X_d^i \le nz_d\right) =  G(\mathbf{z}),
\end{equation}
where each marginal distribution of $G$, $G_k$, is Fr\'echet distributed $G_k(z_k) = \exp(-1/z_k)$.  By writing
$$
	G(\mathbf{z}) = \exp\left(- \Lambda(\mathbf{z})\right),
$$
where $\Lambda(\mathbf{z})$ is shorthand for $\Lambda([0,\infty)^d\backslash[\mathbf0,\mathbf{z}])$, $\Lambda$ is a Radon measure on the cone $\mathcal{E} = [0,\infty)^d\backslash\{\mathbf0\}$. The measure $\Lambda$ is known as the exponent measure and characterizes the dependence strucure of $\mathbf{X}$ in the tail region.

The domain of attraction condition \eqref{eq:maxima} can be equivalently expressed in terms of threshold exceeding. Consider the exceedances of $\mathbf{\tilde X}$ where its $L_\infty$-norm $\|\mathbf{\tilde X}\|_\infty$ is higher than a certain threshold. Then there exists a random vector $\mathbf{Y}$ such that
\begin{equation} \label{eq:pot}
	P(\mathbf{Y} \le \mathbf{z}) := 	\lim_{u\to \infty} P\left(\left. \frac{\mathbf{\tilde X}}{u}\le \mathbf{z}\right| \|\mathbf{\tilde  X}\|_\infty > u\right) = \frac{\Lambda(\mathbf{z}\wedge\mathbf1) - \Lambda(\mathbf{z})}{\Lambda(\mathbf{1})}.
\end{equation}
Here the random vector $\mathbf{Y}$ is defined with support on the $L$-shaped set $\mathcal{L}=\{\mathbf{x}\in\mathcal{E}:\|\mathbf{x}\|_\infty>1\}$.  
Its distribution is known as a multivariate Pareto distribution. 

%The ultimate goal is to infer the distribution of $\mathbf{Z}$ or $\mathbf{Y}$, which is essentially to infer the exponent measure $\Lambda$, using i.i.d observations drawn from the distribution of $\mathbf{X}$.  For that purpose, we further impose structural assumptions on $\Lambda$ via a graphical model for extremes.
%In practice, the choice of which approach to use usually depends on the type of data available.

\cite{eng2018a} proposed the framework of graphical models for extremes, by considering the conditional independence of the threshold exceedance limit $\mathbf{Y}$ in \eqref{eq:pot}.  Since $\mathbf{Y}$ is defined on the $L$-shaped set $\mathcal{L}=\{\mathbf{x}\in\mathcal{E}:\|\mathbf{x}\|_\infty>1\}$ which is not a product space, the notion of conditional independence is instead defined on the subspace $\mathcal{L}^k = \{\mathbf{x}\in\mathcal{L}: x_k>1\}$ for each $k$.  

Let $\mathcal{G}=(V,E)$ be a graph defined by a set of nodes $V=\{1,\ldots,d\}$ and a set of undirected edges between pairs of distinct nodes $E \subset V\times V$.  Define the random vector $\mathbf{Y}^k\stackrel{d}{=}\mathbf{Y}|Y_k>1$.  A graphical model for extremes based on graph $\mathcal{G}$ has a multivariate Pareto distribution $\mathbf{Y}$ that satisfies
\begin{equation} \label{eq:extreme_ci}
	\forall k\in\{1,\ldots,d\}: Y_i^k \indep Y_j^k | \mathbf{Y}^k_{\backslash\{i,j\}} \Leftrightarrow \{i,j\} \notin E,
\end{equation}
where $\mathbf{Y}^k_{\backslash\{i,j\}}$ indicates all other dimensions in $\mathbf{Y}^k$ excluding $\{i,j\}$. In short, we denote the conditional independence in extremes as
$$
	Y_i \indep_e Y_j | \mathbf{Y}_{\backslash\{i,j\}} \Leftrightarrow \{i,j\}\notin E.
$$

%\cite{eng2018a} showed that the definition \eqref{eq:extreme_ci} 
%can be weakened to the following
%$$
%	\exists k\in\{1,\ldots,d\} \backslash\{i,j\}: Y_i^k \indep Y_j^k | \mathbf{Y}^k_{\backslash\{i,j\}}.
%$$
%If the exponent measure $\Lambda$ is absolutely continuous with respect to the Lebesgue measure on $\mathcal{E}$, then its Radon-Nikodym derivative $\lambda$ exists.  Then $\mathbf{Y}$ admits a positive and continuous density
%$$
%	f_{\mathbf{Y}}(\mathbf{y}) = \frac{\partial^d}{\partial y_1 \cdots\partial y_d} P(\mathbf{Y} \le \mathbf{y}) = \frac{\lambda(\mathbf{y})}{\Lambda(\mathbf1)}
%$$

%is equivalent to the decomposition
%\begin{equation}\label{eq:ext_ci}
%	\lambda(\mathbf{y}) = \frac{\lambda_{\backslash{i}}(\mathbf{y}_{\backslash{i}})\lambda_{\backslash{j}}(\mathbf{y}_{\backslash{j}})}{\lambda_{{\backslash\{i,j\}}}(\mathbf{y}_{\backslash\{i,j\}})}.
%\end{equation}



\subsection{H\"usler-Reiss graphical models}

A $d$-dimensional HR model is parametrized by a variogram matrix $\Gamma \in \mathbb{R}^{d\times d}$, such that $\Gamma_{ij} = E(W_i-W_j)^2$ for some centered multivariate Gaussian random vector $\mathbf{W}=(W_1,\ldots,W_d)$.  It is the class of distributions describing the non-trivial tail limiting distribution of Gaussian triangular arrays \citep{hue1989}. Specifically, for any $k=1,\ldots,d$, the exponent measure $\Lambda(\cdot)$ of the HR model admits the density
$$
	\lambda(\mathbf{y}) = y_k^{-2} \prod_{i\ne k} y_{i}^{-1} \phi_{d}(\tilde{\mathbf{y}}_{k};\tilde\Sigma^{(k)}),
$$
where $\phi_{d}(\cdot;\tilde\Sigma^{(k)})$ is the density of a centered $d$-dimensional Gaussian distribution with covariance matrix $\tilde\Sigma^{(k)}$, $\tilde{\mathbf{y}}_k=\{\log(y_i/y_k) + \Gamma_{ik}/2\}_{i=1,\ldots,d}$, and
\begin{equation} \label{eq:sigma_k}
	\tilde\Sigma^{(k)} = \frac{1}{2} \{\Gamma_{ik} + \Gamma_{jk} - \Gamma_{ij}\}_{i,j} \in \mathbb{R}^{d\times d}.
\end{equation}
Note that $\tilde\Sigma^{(k)}$ is degenerate on the $k$-th row and the $k$-th column.  

%The parameters in the HR model determines the potential conditional independence in extremes as follows. 
Let $\Sigma^{(k)}$ be the $(d-1)\times(d-1)$ matrix constructed by removing the $k$-th row and the $k$-th column from $\tilde\Sigma^{(k)}$.  For convenience we index the rows and columns of $\Sigma^{(k)}$ using the original row and column numbers from $\tilde\Sigma^{(k)}$.
Then the following relation holds \citep{eng2018a}:
$$
	Y_i \indep_e Y_j|\mathbf{Y}_{\backslash\{i,j\}} \quad \Leftrightarrow \quad \left(\Sigma^{(k)}\right)^{-1}_{ij} = 0, \quad \forall k \ne i,j.
$$
Consequently, a zero element in $\Theta^{(k)}:= \left(\Sigma^{(k)}\right)^{-1}$ corresponds to a non-edge in $\mathcal{G}$.  In other words, $\Theta^{(k)}$ serves as the precision matrix for the graph $\mathcal{G}$ excluding the node $k$.

To summarize the graph structure for all dimensions, there exists a precision matrix $\Theta \in \mathbb{R}^{d\times d}$ such that removing the $k$-th column and the $k$-th row from $\Theta$ results in $\Theta^{(k)}$ \citep{hentschel2022statistical}. Based on the precision matrix $\Theta$, we have that
$$
	Y_i \indep_e Y_j|\mathbf{Y}_{\backslash\{i,j\}} \quad  \Leftrightarrow \quad \Theta^{(k)}_{ij} = 0,\, i,j \ne k \quad\Leftrightarrow \quad \Theta_{ij} = 0.
$$
We can also reconstruct $\Theta$ given a single $\Theta^{(k)}$ via
\begin{eqnarray}
	\Theta_{ij} = \Theta^{(k)}_{ij} &,& \text{if } i,j \ne k; \nonumber\\
	\Theta_{ik} = -\sum_{l\ne k}\Theta^{(k)}_{il} &,& \text{if } i \ne k \text{ and } j=k; \nonumber\\
	\Theta_{kk} = \sum_{m,l\ne k}\Theta^{(k)}_{ml} &,& \text{if } i = k \text{ and } j=k. \nonumber
\end{eqnarray}
Note that $\Theta{\bf1}=\mathbf0$, which implies that $\Theta$ is not of full rank.


\subsection{Statistical inference for the HR model}

The standard statistical inference for the HR model relies on the following result.  Let $\mathbf{Y}$ be the multivariate Pareto distribution from a HR model with variogram $\Gamma$.  \cite{engelke2015estimation} showed that
\begin{equation} \label{eq:k:gaussian}
	\left(\log\mathbf{Y}_{-k} - \log(Y_k)\cdot \mathbf1\right) |_{Y_k>1} \sim N(\Gamma_{\cdot k}/2, \Sigma^{(k)}).
\end{equation}
Given i.i.d.~observations $\mathbf{X}^i = (X_1^i,\ldots,X_d^i), 1\leq i\leq n$ drawn from $\mathbf{X}$, an empirical counterpart of $\left(\log\mathbf{Y}_{-k} - \log(Y_k)\cdot \mathbf1\right) |_{Y_k>1}$ can be constructed as follows. 
Define the transformed observations
$$
	\hat X^{i}_k= \frac{1}{1-\hat F_k(X^i_k)},
$$
where $\hat F_k(x)=\frac{1}{n+1}\sum_{i=1}^n \mathbb{I}\{X^{i}_k\leq x\}$ is the empirical distribution function based on $X^{i}_k$'s and $\mathbb{I}$ is the indicator function. Then $\mathbf{\hat X}^{i}=(\hat X^{i}_1,\ldots,\hat X^{i}_d)$ resembles a sample of $\mathbf{\tilde X}=(\tilde X_1,\ldots,\tilde X_d)$ with $\tilde X_k = \frac{1}{1-F_k(X_k)}$, albeit not i.i.d.

Consider an intermediate sequence $k_n$ such that $k_n\to \infty$ and $k_n/n\to 0$ as $n\to\infty$. Then as $n\to\infty$, $\|\mathbf{\hat X}^{i}\|_\infty>\frac{n}{k_n}$ mimicks the condition $\|\mathbf{\tilde X}\|_\infty>u$ with $u\to\infty$.
Therefore, 
$$\frac{k_n}{n}\mathbf{\hat X}^{i}\left|\|\mathbf{\hat X}^{i}\|_\infty>\frac{n}{k_n}\right.$$ 
approximately follows the same distribution as $\mathbf{Y}$.

Let $I=\{i:\|\mathbf{\hat X}^{i}\|_\infty>\frac{n}{k_n} \}=:\{i_1,\ldots,i_m\}$, where $m=|I|$, indicating the index set corresponding to $\|\mathbf{\hat X}^{i}\|_\infty>\frac{n}{k_n}$. Denote $\hat{\mathbf{Y}}_j =\frac{k_n}{n}\mathbf{\hat X}^{i_j}$ for all $j=1,\ldots,m$. Let
$$
	\hat{\mathbf{W}}^{(k)}_i = \log\hat{\mathbf{Y}}_{i,-k} - \log(\hat{Y}_{ik})\cdot \mathbf1,
$$
and $I_k$ be the index set that $I_k = \{i:\hat{Y}_{ik}>1\}$ (note that $|I_k|=k_n$) for each dimension $k=1,\ldots,d$.
Then $\Sigma^{(k)}$ can be estimated by
\begin{equation} \label{eq:hat:Sigma:k}
	\hat{\Sigma}^{(k)} := \frac{1}{k_n}\sum_{i \in I_k} \left(\hat{\mathbf{W}}^{(k)}_i  - \frac{1}{k_n}\sum_{i \in I_k} \hat{\mathbf{W}}^{(k)}_i \right)\left(\hat{\mathbf{W}}^{(k)}_i  - \frac{1}{k_n}\sum_{i \in I_k} \hat{\mathbf{W}}^{(k)}_i \right)^T,
\end{equation}
which is the sample covariance matrix using $\hat{\mathbf{W}}^{(k)}_i$ conditional on $\hat{Y}_{ik}>1$.  

Theoretically an estimate of $\Theta$ can be constructed via $\hat\Theta^{(k)}=\left(\hat{\Sigma}^{(k)}\right)^{-1}$.  To achieve sparsity in  $\Theta^{(k)}$, any sparse inverse covariance matrix estimation technique can be applied here.  However, reconstruction of $\Theta$ from a sparse $\hat\Theta^{(k)}$ does not guarantee sparsity on the omitted $k$-th row and column.  \cite{engelke2021learning} proposed to estimate a sparse $\hat\Theta^{(k)}$ for each $k$ and then to use a majority vote to decide whether or not each entry of $\Theta$ should be zero.  This approach is shown to be effective in recovering the sparse structure of $\Theta$, when the number of dimension is at low or moderate level. For high dimensional case, tuning $d$ graphical lasso models can be cumbersome.

In the following, we propose a one-step estimation of $\Theta$.  The advantage is two folds.  First, our computation requirement is significantly lower, especially in the case where $d$ is large.  Second, we simuteneously estimate the graph structure and the non-zero elements in $\Theta$. Theoretically, we provide concentration bounds for the estimate $\hat\Theta$, which would otherwise be difficult to recover through the approach of majority vote.



%Hentschel et al. recognize $\Theta$ as the Penrose-Moore inverse...



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The extreme graphical lasso} \label{sec:lasso}

\subsection{One-step estimation of $\Theta$}

Recall that for a HR distribution parametrized by a variogram $\Gamma$, there exists a centered Gaussian random vector $\mathbf{W}$ such that 
$$
	E(W_i - W_j)^2 = \Gamma_{ij}.
$$
Here the choice of $\mathbf{W}$ is not unique. However, by considering $\mathbf{W}' = \mathbf{W} - \bar{W}\cdot \mathbf{1}$ with $\bar{W}=\frac{1}{d}\sum_{i=1}^dW_i$,  for any such $\mathbf{W}$, then $\mathbf{W}'$ is a centered Gaussian random vector with unique covariance matrix
\begin{equation} \label{eq:definition of Sigma}
	\Sigma := -\frac{1}{2}\left(I - \frac{\mathbf{1}^T\mathbf{1}}{d}\right) \Gamma \left(I - \frac{\mathbf{1}^T\mathbf{1}}{d}\right).
\end{equation}
Here $\Sigma$ is not of full rank since $\Sigma \mathbf{1} = \mathbf0$.  \cite{hentschel2022statistical} showed that $\Sigma$ and $\Theta$ satisfy
$$
	\lim_{M\to\infty} \left( \Sigma + M\mathbf{1}\mathbf{1}^T\right)^{-1} = \Theta.
$$

In the following proposition, we generalize this result to any fixed $M>0$. The proof is postponed to Appendix \ref{appendix:proof of Section 3}.
\begin{proposition} \label{prop:Sigma:Theta}
For any $M>0$, 
$$
	\left( \Sigma + M\mathbf{1}\mathbf{1}^T\right)^{-1} =\Theta + \frac{1}{d^2M}\mathbf{1}\mathbf{1}^T.
$$
\end{proposition}

As a result of Proposition~\ref{prop:Sigma:Theta}, given any consistent estimator of $\Sigma$ and $M>0$, we can directly retrieve a consistent estimator of $\Theta$.

To estimate $\Sigma$, we use the following proposition which shows the link between $\Sigma^{(k)}$ and $\Sigma$.  This relationship was noted in \cite{hentschel2022statistical}. For the completeness of this paper, we provide a formal proof in Appendix \ref{appendix:proof of Section 3}.
\begin{proposition} \label{prop:Sigma:k:Sigma}
Recall the definition of $\tilde{\Sigma}^{(k)}$ in \eqref{eq:sigma_k} where $\tilde\Sigma^{(k)} \in \mathbb{R}^{d\times d}$ is the augmented matrix of $\Sigma^{(k)} \in \mathbb{R}^{(d-1)\times(d-1)}$. Also recall the definition of $\Sigma$ in \eqref{eq:definition of Sigma}. We have
$$
	\frac{1}{d}\sum_{k=1}^d {\tilde\Sigma^{(k)}} = \Sigma + M_\Sigma\mathbf1\mathbf1^T,
$$
where
$$
	M_\Sigma = \frac{1}{d^3}\sum_{k=1}^d {\mathbf1^T\tilde\Sigma^{(k)}\mathbf1} .
$$
\end{proposition}

Based on the estimators for $\Sigma^{(k)}$'s in \eqref{eq:hat:Sigma:k} and Proposition \ref{prop:Sigma:k:Sigma}, we estimate $\Sigma$ by
\begin{equation} \label{eq:sigma:hat}
	S := \frac{1}{d}\sum_{k=1}^d {\hat{\Sigma}^{(k)}} - \left(\frac{1}{d^3}\sum_{k=1}^d {\mathbf1^T\hat{\Sigma}^{(k)}\mathbf1}\right)\mathbf1\mathbf1^T.
\end{equation}
According to Proposition \ref{prop:Sigma:Theta}, for any fixed $M>0$, $\Theta$ can be estimated by
\begin{equation} \label{eq:theta:hat}
	\hat\Theta := \left(S + M \mathbf{1}\mathbf{1}^T \right)^{-1} - \frac{1}{d^2M} \mathbf{1}\mathbf{1}^T.
\end{equation}

\subsection{Interpretation as aggregated MLE}

Given any fixed $M>0$, denote
$$
	\Sigma^* = \Sigma + M\mathbf1\mathbf1^T,
$$
$$
	\Theta^* = \Theta + \frac{1}{d^2M}\mathbf1\mathbf1^T,
$$
and
$$
	S^* = S + M\mathbf1\mathbf1^T.
$$
Then the estimator in \eqref{eq:theta:hat} is equivalent to estimating $\Theta^*$  by $\left(S^*\right)^{-1}$, which can also be viewed as the optimizer of the following problem:
\begin{equation} \label{eq:opt1}
	{\arg\min}_{\Theta^*} \left\{- \log|\Theta^*| +tr\left(S^* \Theta^*\right)\right\}.
\end{equation}
We remark that \eqref{eq:opt1} is equivalent to an ``aggregated MLE'' when considering all partial optimizations in estimating $\Theta^{(k)}$ as follows.

The pseudo-MLE derived from \eqref{eq:k:gaussian} (`pseudo' because the mean of the Gaussian distribution is pre-estimated) is
$$
{\arg\min}_{\Theta^{(k)}} \left\{- \log|\Theta^{(k)}| +tr\left(\hat{\Sigma}^{(k)}\Theta^{(k)}\right) \right\}.
$$
Combining all $k$, one can solve for $\Theta$ that optimizes
\begin{equation} \label{eq:opt2}
	{\arg\min}_{\Theta} \sum_{k=1}^d \left\{- \log|\Theta^{(k)}| +tr\left(\hat{\Sigma}^{(k)}\Theta^{(k)}\right) \right\}.
\end{equation}
The following proposition shows that the solution to this aggregated MLE is exactly the same as the estimator $\hat\Theta$ defined in \eqref{eq:theta:hat}.
\begin{proposition} \label{prop:pseudo:llhd}
For any fixed $M>0$, 
$$
	\frac{1}{d}\sum_{k=1}^d \left\{- \log|\Theta^{(k)}| +tr\left(\hat{\Sigma}^{(k)} \Theta^{(k)}\right) \right\}  = 
	- \log|\Theta^*| +tr\left(S^* \Theta^*\right) + \log(M) - \frac{ tr(S^*)}{d^2M}.
$$
Consequently, the optimization problems \eqref{eq:opt1} and \eqref{eq:opt2} result in the same solution for $\Theta$.
%which is obtained at
%$$
%	\hat\Theta^* := \left(S^*\right)^{-1}.
%$$
\end{proposition}

\subsection{Sparse estimation of $\Theta$}
We aim to estimate a sparse $\Theta$ where some off-diagonal elements are zero.
Note that each zero entry of $\Theta$ corresponds to an entry of $\Theta^*$ with value $\frac{1}{d^2M}$.  We therefore propose the following extreme graphical lasso algorithm by shrinking off-diagonal entries of the matrix $\Theta^*$ towards $\frac{1}{dM^2}$:
\begin{equation} \label{eq:glasso}
	\hat\Theta^* := {\arg\min}_{\Theta^*} \left\{- \log|\Theta^*| +tr\left(S^* \Theta^*\right)  + \gamma_n \sum_{i\ne j} \left|\Theta^*_{ij} - \frac{1}{d^2M}\right|\right\},
\end{equation}
where $\gamma_n$ is a suitably chosen penalty parameter.  The sparse estimator for $\Theta$ is then:
\begin{equation} \label{eq:theta:hat:lasso}
	\hat\Theta_{lasso} := \hat\Theta^* -  \frac{1}{d^2M} \mathbf1\mathbf1^T.
\end{equation}
We term this estimation procedure as the \textit{extreme graphical lasso} method.
%In other words, our graphical lasso algorithm in \eqref{eq:glasso} aggregates all $d$ partial graphs into one graph.  Unlike Engelke et al.~(2022), our one-step approach achieves both the learning of the graphical structure and the estimation of the non-zero parameters in $\Theta$ simutaneously.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Theoretical results} \label{sec:results}

In this section, we establish the finite sample and asymptotic theories for the extreme graphical lasso method in \eqref{eq:glasso}. The goal is to learn the graphical structure for extremes and estimate the non-zero parameters in $\Theta$ simultaneously. We start with theoretical results for the estimation of $\Sigma$ by $S$, related to existing theory on the estimation of $\Gamma$ in \cite{engelke2021learning}.  

%%%%%%%%%%%%%%%%%%%%%

\subsection{Conditions for estimating $\Sigma$}

Recall $S$ as an estimator for $\Sigma$ in \eqref{eq:sigma:hat}. We first present the assumptions needed for the finite sample theory of $S$. The assumptions are in line with those needed in Theorem 3 in \cite{engelke2021learning}. 

The following condition is needed regarding the tail behavior of $\mathbf{\tilde X}=(\tilde X_1, \ldots, \tilde X_d)$.

\begin{condition}[Assumption 3 in \cite{engelke2021learning}] \label{assumption:second order} Assume that all marginal distributions of the original random vector $\mathbf X$, $F_1,\ldots,F_d$ are continuous. In addition, there exists $\xi'>0$ and $K'<\infty$ independent of $d$ such that for all $J\subset\{1,\ldots,d\}$ with $|J|\in \{2,3\}$ and $q\in(0,1]$,
$$\sup_{x\in[0,1/q]^2\times [0,1]}\left|\frac{1}{q}\mathbb{P}\left(\bigcap_{i\in J}\left\{\tilde X_i>\frac{1}{qx_i}\right\}\right)-\frac{\mathbb{P}\left\{\bigcap_{i\in J}\tilde Y_i>\frac{1}{x_i}\right\}}{\mathbb{P}(Y_1>1)}\right|\leq K' q^{\xi'}.$$
\end{condition}
Condition \ref{assumption:second order} is a standard second order condition quantifying the speed of converegence of the tail distribution of $\mathbf{\tilde X}$ towards the limiting distribution $\mathbf{Y}$ on bounded sets. It has been imposed in other asymptotic theories in multivariate extreme value statistics, see e.g. \cite{einmahl2012m} and \cite{engelke2022structure}.

Next, we assume that the variogram in the HR distribution $\Gamma$ has bounded entries.
\begin{condition}[Bounded entries] \label{assumption:non-degenerate} Assume that the variogram $\Gamma$ satisfies that $0<\underline{\lambda}<\inf_{i\neq j}\sqrt{\Gamma_{ij}}\leq \sup_{i\neq j}\sqrt{\Gamma_{ij}}<\overline{\lambda}$, with $\underline{\lambda}$ and $\overline{\lambda}$ independent of $d$. 
\end{condition}
Condition \ref{assumption:non-degenerate} implies the boundedness in the density of the exponent measure, see Assumption 2 in \cite{engelke2021learning}: this condition is required for establishing concentration bounds for estimators of $\Gamma$. In addition, this condition implies that for any pair $(i,j)$ with $i\neq j$, $X_i$ and $X_j$ are asymptotically dependent.

Then we have the following proposition. Its proof is postponed to Appendix \ref{appendix:proof of S estimation}.
\begin{proposition}\label{prop:accuracy of S estimation}
	Assume that Conditions \ref{assumption:second order} and \ref{assumption:non-degenerate} hold. Then for any $\xi<\xi'$, there exists positive constants $C_1$, $C_2$ and $C_3$, depending on $\xi$, $\xi'$, $\underline{\lambda}$ and $\overline{\lambda}$, independent of $d$, such that for any $\varepsilon\geq \varepsilon_n:=C_2 d^3\exp\{-\frac{C_3k_n}{(\log n)^8}\}$,
	\begin{equation}\label{eq:inequality for sigma}
		\mathbb{P}\left(\|S -\Sigma\|_\infty>\delta_n\right)\leq \varepsilon,
	\end{equation}
	where $$\delta_n:=C_1\left\{\left(\frac{k_n}{n}\right)^{\xi}\left(\log\left(\frac{k_n}{n}\right)\right)^2+\frac{1+\sqrt{\frac{1}{C_3}\log (C_2 d^3/\varepsilon)}}{\sqrt{k_n}}\right\}$$ and $\|S -\Sigma\|_\infty$ refers to the element-wise maximum error in the estimation.

	In addition, assuming that $(\log n)^4\sqrt{\frac{\log d}{k_n}}\to 0$ as $n\to\infty$, we have, as $n\to\infty$,
	$$\|S -\Sigma\|_\infty=O_{\text{P}}\left(\left(\frac{k_n}{n}\right)^{\xi}\left(\log\left(\frac{k_n}{n}\right)\right)^2+\sqrt{\frac{\log d}{k_n}}\right).$$
\end{proposition}
We remark that this theorem does not require a fixed $d$ and we allow for $d=d_n\to\infty$ as $n\to\infty$ in the second half of the Proposition. Nevertheless, the condition $(\log n)^4\sqrt{\frac{\log d}{k_n}}\to 0$ as $n\to\infty$ provides an upper bound for the diverging speed of $d_n$ towards infinity. It depends not only on $n$ but also on the intermediate sequence $k_n$.

%%%%%%%%%%%%%%%%%%%%%

\subsection{Conditions for graph identification}

Recall that $(V,E)$ denotes the set of nodes and edges in the graph. Denote $D=\max_{1\leq i \leq d}\sum_{j=1}^d1_{(i,j)\in E}$ as the maximum degree of all nodes. If the dimension $d=d_n\to\infty$ as the sample size $n\to\infty$, we can potentially have $D\to\infty$ and $|E|\to\infty$. Nevertheless, we always have $D=O(d)$ and $|E|=O(d^2)$.

Here we list the conditions required for learning the graph structure. The first condition concerns the structure of the graph reflected in the matrix $\Sigma$.

\begin{condition}[Mutual incoherence] \label{con:mi}  
	Given $M>0$, define $\Omega=\Sigma^*\otimes \Sigma^*$ where $\otimes$ is the Kroneker product. We assume that there exists $0<\alpha<1$ such that
	% \begin{equation}\label{eq:mi}
	$$	|||\Omega_{E^cE}(\Omega_{EE})^{-1}|||_{\infty}<1-\alpha,$$
	% \end{equation}
	where $\Omega_{EE}\in\mathbb{R}^{|E|\times|E|}$ is the submatrix $\left(\Omega_{(i,j),(k,l)}\right)_{(i,j)\in E, (k,l) \in E}$ and $\Omega_{E^cE}$ is defined similarly.
\end{condition}

Note that the \textit{mutual incoherence} condition (sometimes referred to as the irrepresentatbility condition) is comparable with Assumption 1 in \cite{ravikumar2011high}. Such a condition is often needed for theory regarding lasso-type penalization algorithms. Given a graph structure and a matrix $\Sigma$, the validity of our mutual incoherence condition depends on the choice of $M$.  We illustrate this in Section~\ref{appendix:mi} with two examples.


%%%%%%%
The next condition concerns the tuning parameter $\gamma_n$. In order to identify the graphical structure precisely, the tuning parameter should be neither too high nor too low. A low $\gamma_n$ will result in non-edges not being penalized to zero while a high $\gamma_n$ will penalize true edges to zero. Therefore, we need both an upper and a lower bound for $\gamma_n$. The following condition is formulated for a fixed constant $\epsilon>0$ to be specified in Condition \ref{con:n}.
\begin{condition} \label{con:gamma}  
	Assume that the tuning parameter $\gamma_n$ satisfies $\underline{C}_\gamma(\delta_n) \le \gamma_n \le \overline{C}_\gamma$, where
	\begin{eqnarray}
		% \overline{C}_\gamma&:=&\frac{(1-\epsilon)\alpha}{3D|||\Sigma^*|||_\infty \cdot |||\Omega_{EE} ^{-1}|||_\infty} \cdot \min\left\{ \frac{(1-\epsilon)\alpha}{3D|||\Sigma^*|||_\infty^2 \cdot |||\Omega_{EE} ^{-1}|||_\infty}, \frac{1}{2} \right\} \\
		\overline{C}_\gamma&:=&\frac{(1-\epsilon)\alpha(1-\alpha)}{D|||\Sigma^*|||_\infty |||(\Omega_{EE}) ^{-1}|||_\infty\left[(1-\epsilon)\alpha+|||\Sigma^*|||_\infty^2|||(\Omega_{EE})^{-1}|||_\infty\right]},\label{eq:upperbound rho}\\
		\underline{C}_\gamma(\delta_n)&:=&\frac{1-\alpha}{\epsilon\alpha} \cdot \delta_n, \label{eq:lowerbound rho}
	\end{eqnarray}
	where $\delta_n$ quantifies the estimation error for $S-\Sigma$ as in Proposition \ref{prop:accuracy of S estimation}.
\end{condition}
The upper bound $\overline{C}_\gamma$ is a constant related to the parameters of the graphical model only. The lower bound is a linear function of $\delta_n$. In the asymptotic setup, it tends to 0 as $n\to\infty$.

The last condition ensures that the above bounds can be achieved, that is $\underline{C}_\gamma \le \overline{C}_\gamma$.  Note that this condition will be satisfied when $n$ is sufficiently large.

%%%%%
\begin{condition}\label{con:n}  
	There exists an $\epsilon>0$ such that
	$$
		% \delta_n \le \frac{(1-\epsilon)\alpha^2}{3\epsilon(2-\alpha)D|||\Sigma^*|||_\infty \cdot |||\Omega_{EE} ^{-1}|||_\infty} \cdot \min\left\{ \frac{(1-\epsilon)\alpha}{3D|||\Sigma^*|||_\infty^2 \cdot |||\Omega_{EE} ^{-1}|||_\infty}, \frac{1}{2} \right\},
		\delta_n\le \frac{(1-\epsilon)\epsilon\alpha^2}{D|||\Sigma^*|||_\infty |||\Omega_{EE} ^{-1}|||_\infty\left[(1-\epsilon)\alpha+|||\Sigma^*|||_\infty^2|||\Omega_{EE} ^{-1}|||_\infty\right]},
	$$
	where $\delta_n$ quantifies the estimation error in $S-\Sigma$ as in Proposition \ref{prop:accuracy of S estimation}.
\end{condition}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Main theorem}

We first present the concentration bounds for $\hat\Theta_{lasso}$ for fixed $n$. The proof is shown in Appendix \ref{appendix: proof for Section 4}.
\begin{theorem} \label{thm:main}
	Assume that Conditions \ref{assumption:second order}--\ref{con:mi} holds. For some $\varepsilon\geq \varepsilon_n$ with $\varepsilon_n$ defined in Proposition \ref{prop:accuracy of S estimation}, further assume that Conditions \ref{con:gamma}--\ref{con:n} hold with $\delta_n$ defined in Proposition \ref{prop:accuracy of S estimation}. Then on an event $A_\varepsilon$ with $\mathbb{P}(A_\varepsilon)>1-\varepsilon$, the extreme graphical lasso algorithm specified in \eqref{eq:glasso} and \eqref{eq:theta:hat:lasso} has a unique solution $\hat\Theta^*$ and $\hat\Theta_{lasso}$. In addition, for this solution, denote the estimated edges as $\hat E:=\{(i,j):\hat\Theta_{lasso,ij} \neq 0\}$. We have that on $A_\varepsilon$,
	$$
		\hat E\subset E
	$$
	and 
	\begin{equation}\label{eq:definition r}
		\Vert\hat\Theta_{lasso}-\Theta\Vert_\infty\leq \frac{|||\left(\Omega_{EE} \right)^{-1}|||_\infty}{1-\alpha} \cdot \gamma_n =: r_n.
	\end{equation}
 In particular, if $\min\{\|\Theta_{ij}\|; (i,j) \in E, i\neq j\}>r_n$,  then we have that $\hat E=E$ on $A_\varepsilon$.
\end{theorem}

Next, we present the asymptotic theory when $n\to\infty$. Notice that by assuming $k_n/n\to 0$ and $(\log n)^4\sqrt{\frac{\log d}{k_n}}\to 0$ as $n \to \infty$, we have $\delta_n\to 0$. Then for any $\epsilon>0$, Condition~\ref{con:n} is satisfied for sufficiently large $n$. To achieve the lowest estimation error, we choose the lowest possible tuning parameter $\gamma_n=\underline{C}_\gamma(\delta_n)$ as in \eqref{eq:lowerbound rho}.  This implies that both $\gamma_n=O(\delta_n)$ and $r_n=O(\delta_n)$ as $n\to\infty$.  The following asymptotic result follows immediately from Theorem \ref{thm:main}. 

\begin{theorem} \label{thm:asymptotic}
    Assume that Conditions \ref{assumption:second order}--\ref{con:mi} holds. Choose the tuning parameter $\gamma_n=\underline{C}_\gamma(\delta_n)$ as in \eqref{eq:lowerbound rho}. Assume that $\min\{\|\Theta_{ij}\|; (i,j) \in E, i\neq j\}>r>0$ with some constant $r$ independent of $d$. Then as $n\to\infty$, with probability tending to 1, the solution for $\hat\Theta^*$ and $\hat\Theta_{lasso}$ is unique.  In addition, 
	$$\Pr(\hat E=E)\to 1 \text{\ \ and\ \ } \Vert\hat\Theta^*-\Theta^*\Vert_\infty=O_P\left\{\left(\frac{k}{n}\right)^{\xi}\left(\log \frac{n}{k}\right)^2+\sqrt{\frac{\log d}{k}}\right\}.$$
\end{theorem}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simulations and a real data example} \label{sec:example}

In this section, we demonstrate the performance of the  extreme graphical lasso method in both the low dimensional ($d=4$) and high dimensional ($d=20$ and $d=200$) cases. In addition, we show a data example of river discharge data, also used in \cite{eng2018a}.

\subsection{Simulations in low dimensional cases ($d=4$)} \label{appendix:mi}
For $d=4$, we investigate two theoretical examples. In particular, we focus on the Mutual Incoherence condition 
$$
	|||\Omega_{E^cE}(\Omega_{EE})^{-1}|||_{\infty}<1,
$$
and demonstrate how the validity of this condition depends on the choice of $M$.  

Figure~\ref{fig:mi} shows the graph structures for the two examples, the star graph and the diamond graph. The Mutual Incoherence condition for these two graphs in the classical graphical lasso setting was studied in \cite{ravikumar2011high}.

\tikzset{
    %Define standard arrow tip
    >=stealth',
    %Define style for boxes
    punkt/.style={
           rectangle,
           rounded corners,
           draw=black, very thick,
           text width=6.5em,
           minimum height=2em,
           text centered},
    % Define arrow style
    pil/.style={
           ->,
           thick,
           shorten <=2pt,
           shorten >=2pt,}
}
\newsavebox{\mytikzpic}
\begin{lrbox}{\mytikzpic} 
     \begin{tikzpicture}
    \begin{scope}[xshift=-2cm,yshift=1cm,scale=.8]
       \node[draw,circle] (s1) at (0,-2) {$X_1$};
      \node[draw,circle] (s2) at (-2,0) {$X_2$};
      \node[draw,circle] (s3) at (2,0) {$X_3$};
      \node[draw,circle] (s4) at (0,-4.5) {$X_4$};
      \draw[-] (s1) to node [above right] {} (s3);
      \draw[-] (s1) to node [above left] {} (s2);
      \draw[-] (s4) to node [below] {} (s1);
    \end{scope}
    
     \begin{scope}[xshift=6cm,yshift=1cm,scale=.8]
       \node[draw,circle] (s1) at (0,0) {$X_1$};
      \node[draw,circle] (s2) at (-2,-2) {$X_2$};
      \node[draw,circle] (s3) at (2,-2) {$X_3$};
      \node[draw,circle] (s4) at (0,-4) {$X_4$};
      \draw[-] (s1) to node [above right] {} (s3);
      \draw[-] (s1) to node [above left] {} (s2);
      \draw[-] (s4) to node [below left] {} (s2);
      \draw[-] (s4) to node [below left] {} (s3);
      \draw[-] (s4) to node [below] {} (s1);
    \end{scope}
    
    
     \node at (-2,-3.5) {(a)};
     \node at (6,-3.5) {(b)};
  \end{tikzpicture}    

\end{lrbox}
%\bigskip
  % Figure environment removed 

\subsubsection{Star graph} \label{sub:star}
Notice that the precision matrix $\Theta$ in the HR model satisfies the constraint $\Theta\mathbf1=\mathbf0$, which leaves limited options for $\Theta$ given a certain sparsity structure. We consider the following parameterization
$$
	\Theta = 
	\begin{pmatrix}
	3 & -1 & -1 & -1 \\
	-1 & 1 & 0 & 0 \\
	-1 & 0 & 1 & 0 \\
	-1 & 0 & 0 & 1 \\
	\end{pmatrix}
	,
$$
which reflects the Star graph in Figure~\ref{fig:mi}(a).

We plot the values of $|||\Omega_{E^cE}(\Omega_{EE})^{-1}|||_{\infty}$ against the values of $M$ on the left panel of Figure~\ref{fig:M}. The figure shows that the Mutual Incoherence condition is satisfied when $M \in (0,0.2768]$.

% Figure environment removed

We simulate 100 samples with various sample sizes $n$: ranging from $10^3$ to $10^6$. For each sample we simulate data from the following multivariate Pareto distribution
$$\mathbf{X}=Y\exp\{\mathbf W - \sigma^2(\mathbf W)/2\},$$
where $\mathbf W$ follows a zero-mean Gaussian distribution with a covariance matrix $\Sigma$ calculated based on Proposition \ref{prop:Sigma:Theta}, and $Y$ follows a standard Pareto distribution and is independent of $\mathbf W$. Here $\sigma^2(\mathbf W)$ is the diagonal vector of $\Sigma$. Note that the choice of $M$ here is irrelevant to the calculation of $\Sigma$ . The multivariate Pareto distribution used in the simulation is in the domain of attraction of a HR model with precision matrix $\Theta$.

We apply the extreme graphical lasso method to estimate the graphical structure of $\Theta$. More specifically, for the estimator $\hat\Sigma$, we use $k_n=0.05 n$. For the extreme graphical lasso, we choose $M=0.25$ which ensures the Mutual Incoherence condition and a penalty parameter $\gamma_n=0.2$. Here the optimization problem \eqref{eq:glasso} is convex and can be solved with a block coordinate descent algorithm similar to \cite{mazumder2012graphical}, see Appendix \ref{appendix:algorithm}.

After obtaining $\hat\Theta$ we further consider a thresholding by $0.01$: if an estimated off-diagonal element has an absolute value less or equal to $10^{-2}$, it will be set to zero. The last step is purely for computational reason. For each simulated sample, we consider it as a ``success'' if the estimated graph coincides with the true graph. The left panel of Figure~\ref{fig:hit rate} shows the ``success rates'' in 100 simulated samples versus the sample sizes.

We observe that the success rate of the extreme graphical lasso method approaches 100\% as $n$ increases, indicating that the graph can be consistently identified.  Note that here $k_n$ is the effective sample size and corresponds to 5\% of $n$, e.g.~$k_n=500$ corresponding to $n=10000$.

%We observe that the extreme graphical lasso performs well for sample sizes starting from $n=10000$. Note that the corresponding $k_n$ is $5\%$ of the total sample, i.e. $k_n=500$ corresponding to $n=10000$. Here we should interpret $k_n$ as the ``effective sample size''. When the sample size reaches $n=100000$ (i.e. $k=5000$), the hit rate reaches almost 100\%.


% Figure environment removed

\subsubsection{Diamond graph}

We now consider the diamond graph in Figure~\ref{fig:mi}(b) corresponding to the following precision matrix 
$$
	\Theta =  
	\begin{pmatrix}
	2 & -1 & -1 & 0 \\
	-1 & 3 & -1 & -1 \\
	-1 & -1 & 3 & -1\\
	0 & -1 & -1 & 2 \\
	\end{pmatrix}
	.
$$
Again we plot the values of $|||\Omega_{E^cE}(\Omega_{EE})^{-1}|||_{\infty}$ against the values of $M$ on the right panel of Figure~\ref{fig:M}. The figure shows that the Mutual Incoherence condition is satisfied when $M \in [0.0224,0.1588]$.

We conduct simulations for the diamond graph with the same setup as in Section~\ref{sub:star}. In the extreme graphical lasso estimation, we choose $M=0.15$ and $\gamma_n=0.1$.  The results for the success rate are shown on the right panel of Figure~\ref{fig:hit rate}.  Again the graph structure can be consistently identified for sample sizes starting from $n=5000$ corresponding to an effect sample size $k_n=250$. Compared to identifying the Star graph, identifying the Diamond graph is relatively easier.

%We simulate 100 samples with various sample sizes $n$ using the same setup as for the star graph, except using the diamond precision matrix. Then we apply the extreme graphical lasso procedure to estimate the graphical structure with $M=0.2$ and $\gamma_n=0.1$. The other tuning parameters are kept the same as for the Star graph.
%Figure~\ref{fig:hit rate}, the right panel shows the ``hit rate'' against the sample size. We observe that the extreme graphical lasso performs well for sample sizes starting from $n=5000$ corresponding to an effect sample size $k_n=250$. When the sample size reaches $n=100000$ (i.e. $k=5000$), the hit rate again reaches almost 100\%.
\subsection{Simulations in high dimensional cases ($d=20$ and $d=200$)}
We demonstrate the performance of the extreme graphical lasso method in higher dimensional situations. 

If the dimension $d$ is high, the extreme graphical lasso method can be simplified to a standard graphical lasso method as follows. Recall the extreme graphical lasso procedure in \eqref{eq:glasso} and \eqref{eq:theta:hat:lasso}. In the optimization step, we shrink off-diagonal entries of the matrix $\Theta^*$ towards $\frac{1}{d^2M}$. When $d$ is large, the term $\frac{1}{d^2M}$ is close to zero. Therefore, we can replace the optimization step \eqref{eq:glasso} by
$$
	\hat\Theta_{mlasso} := {\arg\min}_{\Theta^*} \left\{- \log|\Theta^*| +tr\left(S^* \Theta^*\right)  + \gamma_n \sum_{i\ne j} \left|\Theta^*_{ij}\right|\right\},
$$
potentially with a diffrerent penalization parameter $\gamma_n$. This practical proposal performs the classical graphical lasso procedure only once. In the estimation, we use the R package {\it glasso} in \cite{fri2008} for the optimization. We term it as the \textit{modified extreme graphical lasso} method.

We first demonstrate that the modified extreme graphical lasso procedure results in similar graphs as the extreme graphical lasso method for $d=20$. For that purpose, we simulate observations following a multivariate Pareto distribution with $d=20$. The multivariate Pareto distribution is in the domain of attraction of a HR model with a specific precision matrix goverend by a graph. The true graph is presimulated by a preferential attach model as in \cite{albert2002statistical}, see Figure~\ref{fig:d20 graphs}(a). Given the graph, the simulation of the observations is achieved by using the R package {\it graphicalExtremes} in \cite{eng2018a}. We simulate 100 samples with sample size 5000.

Across the 100 samples, we apply the extreme graphical lasso method to estimate the graph structure with $k_n=250$, $M=1$ and $\gamma_n=2.4$. In the extreme graphical lasso method, we shrink the off-diagonal elements of $\hat\Theta^*$ towards $c=1/(d^2M)$. For each pair of nodes, we count the number of samples for which an edge is identified. Figure~\ref{fig:d20 graphs}(b) shows the aggregation of 100 estimated graphs, where the thickness of each edge reflects the proportion of times an edge is identified.

Next, we apply the modified extreme graphical lasso method based on the same estimated $\Sigma$. Here we use $\gamma_n=1.2$. The aggregation of 100 estimated graphs is shown in Figure~\ref{fig:d20 graphs}(c).

The two graphs in the panels (b) and (c) of Figure~\ref{fig:d20 graphs} are virtually the same, with the modified procedure identifying slightly more wrong edges. Both graphs are comparable with the true graph in panel (a), indicating the applicability of both procedures. 

% Figure environment removed

Now we demonstrate the efficiency of the modified extreme graphical lasso in a very high dimensional case $d=200$. We perform a  simulation for $d=200$ with $n=100000$. The simulation setup is similar to the $d=20$ case, while in the estimation we use the modified extreme graphical lasso procedure with $k_n=5000$ and $\gamma_n=1.9$. Here the penalization parameter is chosen at the highest level for which the estimated graph is connected.  The true graph and the aggretaged graph from 10 simulations are shown in Figure~\ref{fig:d200 graphs}. 

We particularly focus on the efficiency of the algorithm.\footnote{This simulation is run on a Dell XPS 9320 laptop, with 16 cores (i7-1260P) and 32GB memory. Operating system: Ubuntu 22.04.2. R version: 4.3.1.} The average time for running each of the 10 iteration is 3.23 min. Per iteration, the majority time is devoted to simulating the data (0.378 min) and estimating the $\Sigma$ matrix (2.848 min), while the time to perform the modified extreme graphical lasso method is only 0.0037 min, about 0.22 second. Considering that this is for $d=200$, the modified extreme graphical lasso method has great potential for handling even higher dimensional cases.


% Figure environment removed
\subsection{Real data example: river discharge}
We apply the modified extreme graphical lasso method to the river discharge data in the upper Danube basin.  This dataset was first analyzed in \cite{asadi2015extremes} and subsequently studied in \cite{eng2018a}. The data contain river discharge at $d=31$ stations with a sample size $428$ after declustering.  The physical locations of the stations and the altitude of the area are shown in Figure \ref{fig:danube}.
\footnote{We are grateful to Sebastian Engelke for providing the figure.} We refer interested readers to \cite{asadi2015extremes} for more information about the dataset and the declustering procedure. We use $k_n=64$ in the estimation, and vary the penalization parameter $\gamma_n$ to obtain different estimated graphs. 

In Figure~\ref{fig:danube graphs}, the top left panel shows the physical connection of the stations following the Danube basin. The other three panels in the figure show different estimated graphs using different values of $\gamma_n$. With increasing $\gamma_n$ the estimated graph contains fewer edges and eventually turns to disconnected graphs. Nevertheless, such a graph will be useful for practitioners to understand the most related extreme river discharge across the stations.

For instance, we observe that the extreme river discharge at Station 1 is more related to the stream from Station 13, intead of the other stream from Station 2. The stream from Station 13 is the downstrem of the river Salzach, originally starting in the Alps, while the stream from Station 2 is the Danube river flowing over a plain area.  Similarly, the extreme river discharge at Station 2 is more related to the main stream from Station 3 (the Danube river) instead of the branch from Station 14 (the Isar river).

To conclude, by applying the (modified) extreme graphical lasso method to the Danube river discharge data, we obtain insights regarding the interrelations of extreme river discharges at a large number of stations spanning in a large spatial area.

% Figure environment removed

% Figure environment removed
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{chicago}
\bibliography{ref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Proof of propositions in Sections~\ref{sec:lasso}} \label{appendix:proof of Section 3}

%%%%%%%%%%%%%%%%%%%%%%%%%

We first prove Proposition~\ref{prop:Sigma:k:Sigma} and then use the result to prove Proposition~\ref{prop:Sigma:Theta}.

\begin{proof}[Proof of Proposition~\ref{prop:Sigma:k:Sigma}]
Note that the $(i,j)$th element of $\frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)}$ is equal to
\begin{eqnarray*}
\left(\frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)}\right)_{i,j} &=& \frac{1}{2}\frac{1}{d} \sum_{k=1}^d (\Gamma_{ik} + \Gamma_{jk} -\Gamma_{ij} ) \\
&=&-\frac{1}{2} \left(\Gamma_{ij} - \frac{1}{d} \sum_{k=1}^d \Gamma_{ik} - \frac{1}{d} \sum_{k=1}^d \Gamma_{jk} \right).
\end{eqnarray*}
Hence
\begin{eqnarray*}
\frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)} &=& -\frac{1}{2} \left(\Gamma - \frac{1}{d}\Gamma \mathbf{1}\mathbf{1}^T - \frac{1}{d} \mathbf{1}\mathbf{1}^T \Gamma \right)\\
&=& -\frac{1}{2} \left(\Gamma - \frac{1}{d}\Gamma \mathbf{1}\mathbf{1}^T - \frac{1}{d} \mathbf{1}\mathbf{1}^T \Gamma + \frac{1}{d^2} \mathbf{1}\mathbf{1}^T \Gamma\mathbf{1}\mathbf{1}^T\right) +  \frac{1}{2d^2} \mathbf{1}\mathbf{1}^T \Gamma\mathbf{1}\mathbf{1}^T\\
 &=& -\frac{1}{2}  \left(I - \frac{\mathbf{1}^T\mathbf{1}}{d}\right) \Gamma \left(I - \frac{\mathbf{1}^T\mathbf{1}}{d}\right) + \left(\frac{\mathbf{1}^T\Gamma\mathbf{1}}{d^2}\right) \cdot \mathbf{1}\mathbf{1}^T \\
 &=&  \Sigma + \left(\frac{\mathbf{1}^T\Gamma\mathbf{1}}{d^2}\right) \cdot \mathbf{1}\mathbf{1}^T.
\end{eqnarray*}
Summing up the elements of the matrices on both sides, we have
\begin{eqnarray*}
	\mathbf1^T \left( \frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)} \right) \mathbf1 &=& \mathbf1^T \Sigma \mathbf1 + \mathbf1^T \left( \left(\frac{\mathbf{1}^T\Gamma\mathbf{1}}{d^2}\right) \cdot \mathbf{1}\mathbf{1}^T \right) \mathbf1 \\
	&=& 0 + \mathbf{1}^T\Gamma\mathbf{1}.
\end{eqnarray*}
Plugging in the value for $\mathbf{1}^T\Gamma\mathbf{1}$ back into the previous equation, we obtain that
$$
	\frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)} =  \Sigma + \frac{1}{d^2}\mathbf1^T \left( \frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)} \right) \mathbf1 \cdot \mathbf{1}\mathbf{1}^T=  \Sigma + \frac{1}{d^3}\left( \sum_{k=1}^d \mathbf1^T \tilde\Sigma^{(k)} \mathbf1\right)  \cdot \mathbf{1}\mathbf{1}^T.
$$
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}[Proof of Proposition~\ref{prop:Sigma:Theta}]

Using the property $\Sigma\mathbf1 = \mathbf0$ and $\Theta\mathbf1 = \mathbf0$ and the fact that $\Sigma$ and $\Theta$ are both symmetric, we get
\begin{eqnarray*}
&& \left(\Sigma + M\mathbf1\mathbf1^T\right) \cdot \left(\Theta + \frac{1}{d^2M}\mathbf1\mathbf1^T\right) \\
&=& \Sigma\cdot\Theta + M\mathbf1\mathbf1^T \cdot \Theta + \Sigma \cdot \frac{1}{d^2M}\mathbf1\mathbf1^T + M\mathbf1\mathbf1^T \cdot  \frac{1}{d^2M}\mathbf1\mathbf1^T \\
&=& \Sigma\cdot\Theta + M\mathbf1 \cdot \left(\Theta\mathbf1\right)^T + \frac{1}{d^2M} \cdot \Sigma \mathbf1 \cdot \mathbf1^T + (M\cdot \frac{1}{d^2M})\cdot \mathbf1\mathbf1^T \cdot \mathbf1\mathbf1^T \\
&=& \Sigma \cdot \Theta + \frac{1}{d} \cdot \mathbf1\mathbf1^T 
\end{eqnarray*}
From Proposition~\ref{prop:Sigma:k:Sigma}, we can write the first term as
\begin{eqnarray*}
\Sigma \cdot \Theta &=& \left( \frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)} - M_\Sigma  \mathbf{1}\mathbf{1}^T\right)\cdot \Theta \\
&=& \frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)} \cdot \Theta - M_\Sigma  \mathbf{1}\mathbf{1}^T \cdot \Theta \\
&=& \frac{1}{d} \sum_{k=1}^d \tilde\Sigma^{(k)} \cdot \Theta \\
&=&  I_{d\times d} - \frac1d \cdot \mathbf{1}\mathbf{1}^T.
\end{eqnarray*}
Therefore
$$
	\left(\Sigma + M\mathbf1\mathbf1^T\right) \cdot \left(\Theta + \frac{1}{d^2M}\mathbf1\mathbf1^T\right) = I_{d\times d}.
$$
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%

In order to prove Proposition~\ref{prop:pseudo:llhd}, we make use of the following lemma.

\begin{lemma} \label{lemma:1}
For any $k$,
$$
	|\Theta^*| = \frac{1}{M}|\Theta^{(k)}| .
$$
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:1}]
We will first show that for $k\ne k'$,
$$
	|\Theta^{(k)}| = |\Theta^{(k')}|.
$$

Note that
$$
	\Theta^{(k)} = A_k^T \Theta A_k,
$$
where $A_k \in \mathbb{R}^{d \times (d-1)}$, $A_k[,k] = {\bf0}$ and $A_k[,-k] = I_{(d-1)\times(d-1)}$.  In other words, $A_k$ takes the $(d-1)\times(d-1)$ identity matrix and insert an extra $k$-th row with zero entries.
On the other hand, we have
$$
	\Theta = B_k \Theta^{(k)} B_k^T,
$$
where $B_k \in \mathbb{R}^{d \times (d-1)}$, $B_k[,k] = -{\mathbf{1}}$ and $B_k[,-k] = I_{(d-1)\times(d-1)}$.  In other words, $B_k$ takes the $(d-1)\times(d-1)$ identity matrix and insert an extra $k$-th row with $-1$ entries.
Therefore we have
$$
	\Theta^{(k')} = A_{k'}^TB_k \Theta^{(k)} B_k^TA_{k'}
$$
and
$$
	|\Theta^{(k')}| = |A_{k'}^TB_k| \cdot |\Theta^{(k)} | \cdot |B_k^TA_{k'}|.
$$

Now we claim that $|A_{k'}^TB_k|=1$ for any $k,k'$.  We have
\begin{eqnarray*}
	A_{k'}^TB_k &=& \sum_{h=1}^d A_{k'}[,h]^T B_k[h,] \\
	&=& \sum_{h\ne k'} A_{k'}[,h]^T B_k[h,] + A_{k'}[,k']^T B_k[k',] \\
	&=& A_{k'}[,-k']^T B_k[-k',] + A_{k'}[,k']^T B_k[k',] \\
	&=& I_{(d-1)\times(d-1)}B_k[-k',] + {\bf0}^T B_k[k',] \\
	&=& B_k[-k',].
\end{eqnarray*} 
For example, assume that $k=1$ and $k'=d$, then
$$
	B_1[-d,] = \left(
	\begin{array}{ccccc}
	-1 & -1 & \cdots& -1 & -1 \\
	1 & 0 & \cdots& 0 & 0 \\
	0 & 1 & \cdots& 0 & 0 \\
	\vdots & \vdots & \vdots & \vdots & \vdots \\
	0 & 0 & \cdots& 1 & 0 \\
	\end{array}
	\right)
$$
and it is easy to see that $|B_1[-d,]|=1$.  

It can be shown by the calculation of determinant that $|B_k[-k',]|=1$ for any $k \ne k'$.  Therefore
$$
	|A_{k'}^TB_k|=1, \quad k\ne k',
$$
and
$$
	|\Theta^{(k)}| = |\Theta^{(k')}|, \quad k\ne k'.
$$

Now to show that $|\Theta^*| = \frac{1}{M} |\Theta^{(k)}|$, it suffices to prove it for one value of $k$.  We will show it for $k=d$.  

Note that we have
$$
	\frac{1}{M} |\Theta^{(d)}| =
	\left|\left(
	\begin{array}{cc}
	\Theta^{(d)} & {\bf0} \\
	{\bf0}^T & \frac{1}{M}
	\end{array}
	\right)\right|.
$$
We establish the following transformation
\begin{eqnarray*}
	&&
	\left(
		\begin{array}{cc}
			I& {\bf0} \\
			-\mathbf{1}^T & 1
		\end{array}
	\right)
	\left(
		\begin{array}{cc}
			I& \frac{1}{d}\mathbf{1} \\
			{\bf0}^T & 1
		\end{array}
	\right)
	\left(
		\begin{array}{cc}
			\Theta^{(d)} & {\bf0} \\
			{\bf0}^T & \frac{1}{M}
		\end{array}
	\right)
	\left(
		\begin{array}{cc}
			I& {\bf0} \\
			\frac{1}{d}\mathbf{1}^T & 1
		\end{array}
	\right)
	\left(
		\begin{array}{cc}
			I& -\mathbf{1} \\
			{\bf0}^T & 1
		\end{array}
	\right) \\
	&=&
	\left(
		\begin{array}{cc}
			I& {\bf0} \\
			-\mathbf{1}^T & 1
		\end{array}
	\right)
	\left(
		\begin{array}{cc}
			\Theta^{(d)} + \frac{1}{d^2M}\mathbf{1}\mathbf{1}^T & \frac{1}{dM}{\bf1} \\
			\frac{1}{dM}{\bf1}^T & \frac{1}{M}
		\end{array}
	\right)
	\left(
		\begin{array}{cc}
			I& -\mathbf{1} \\
			{\bf0}^T & 1
		\end{array}
	\right) \\
	&=&
	\left(
		\begin{array}{cc}
			\Theta^{(d)} + \frac{1}{d^2M}\mathbf{1}\mathbf{1}^T & -\Theta^{(d)}\mathbf{1} + \frac{1}{d^2M}{\bf1} \\
			-\mathbf{1}^T\Theta^{(d)} + \frac{1}{d^2M}{\bf1}^T & \mathbf{1}^T\Theta^{(d)}\mathbf{1} + \frac{1}{d^2M}
		\end{array}
	\right) \\
	&=& \Theta + \frac{1}{d^2M}\mathbf{1}\mathbf{1}^T \\
	&=& \Theta^*.
\end{eqnarray*}
Since
$$
	\left|
	\left(
		\begin{array}{cc}
			I& {\bf0} \\
			-\mathbf{1}^T & 1
		\end{array}
	\right)
	\right|
	=
	\left|
	\left(
		\begin{array}{cc}
			I& {\bf0} \\
			\frac{1}{d}\mathbf{1}^T & 1
		\end{array}
	\right)
	\right|
	=1,
$$
we have
$$
	|\Theta^*| = 
	\left|\left(
	\begin{array}{cc}
	\Theta^{(d)} & {\bf0} \\
	{\bf0}^T & \frac{1}{M}
	\end{array}
	\right)\right|
	= \frac{1}{M} |\Theta^{(d)}|.
$$

\end{proof}


\begin{proof}[Proof of Proposition~\ref{prop:pseudo:llhd}]

The aggregated negative log-likelihood function can be written as
\begin{eqnarray*}
	&& \frac{1}{d}\sum_{k=1}^d \left\{- \log|\Theta^{(k)}| +tr\left(S_k \Theta^{(k)}\right) \right\} \\
	&=& \frac{1}{d}\sum_{k=1}^d \left\{- \log|\Theta^{(k)}| +tr\left(\tilde{S}_k \Theta\right) \right\} \\
	&=& \frac{1}{d}\sum_{k=1}^d \left\{- \log|\Theta^{(k)}| \right\}  + tr\left(\left( \frac{1}{d}\sum_{k=1}^d\tilde{S}_k\right) \Theta\right) \\
	&=& - \log|\Theta^*| + \log(M) + tr\left(\left( \frac{1}{d}\sum_{k=1}^d\tilde{S}_k - \left(\frac{1}{d^3}\sum_{k=1}^d {\mathbf1^T\tilde{S}_k\mathbf1}\right)\mathbf1\mathbf1^T + M \cdot \mathbf1\mathbf1^T\right) \Theta \right)\\
	&=& - \log|\Theta^*| + \log(M) + tr\left(S^* \left(\Theta^* - \frac{1}{d^2M} \mathbf1\mathbf1^T\right)\right).
\end{eqnarray*}
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%

\section{Proof of Proposition~\ref{prop:accuracy of S estimation}} \label{appendix:proof of S estimation}

%%%%%%%%%%%%%%%%%%%%%%%%
\begin{proof}[Proof of Proposition \ref{prop:accuracy of S estimation}] We intend to apply Theorem 3 in \cite{engelke2021learning}. For that purpose, we first verify all assumptions needed for that theorem, namely Assumptions 1 and 2 therein.

We handle Assumption 1 first. Based on Lemma S3 in \cite{engelke2021learning}, Condition \ref{assumption:non-degenerate} implies that for any $\xi''>0$, there exists $K_{\xi''}>0$ depending on $\underline{\lambda}$ and $\overline{\lambda}$, but independent of $d$, such that Assumption 4 therein holds. 
Denote $K=K'+2K_{\xi''}$ and $\xi=\xi' \xi''/(1+\xi'+\xi'')$. Together with Condition \ref{assumption:second order}, we get that Assumption 1 in \cite{engelke2021learning} holds. In particular, one can choose $\xi''$ sufficiently large such that $\xi$ can be any constant satisfying $\xi<\xi'$. 
	
Next, Assumption 2 in \cite{engelke2021learning} holds automatically for all non-degenerate HR distribution satisfying our Condition \ref{assumption:non-degenerate}. Therefore, we can then apply Theorem 3 therein to obtain that there exists positive constants $C_1$, $C_2$ and $C_3$, independent of $d$, such that for any $k_n\geq n^{\xi}$ and $\lambda\leq \sqrt{k_n}/(\log n)^4$,
	\begin{equation}\label{eq:inequality for sigma_k}
	\mathbb{P}\left(\max_{1\leq k\leq d}\|\hat\Sigma^{(k)} -\Sigma^{(k)}\|_\infty> C_1\left\{\left(\frac{k_n}{n}\right)^{\xi}\left(\log\left(\frac{k_n}{n}\right)\right)^2+\frac{1+\lambda}{\sqrt{k_n}}\right\}\right)\leq C_2 d^3e^{-C_3\lambda^2}.
	\end{equation}
	Notice that the constant $C_1$ here equals to $\frac{3}{2}\overline{C}$ in Theorem 3 in \cite{engelke2021learning} because we are estimating the matrix $\Sigma$ instead of the variogram $\Gamma$.
	
	For any $\varepsilon\geq C_2 d^3\exp\{-\frac{C_3k_n}{(\log n)^8}\}$, one can choose $\lambda=\sqrt{\frac{1}{C_3}\log (C_2 d^3/\varepsilon)}\leq \frac{\sqrt{k_n}}{(\log n)^4}$ in \eqref{eq:inequality for sigma_k} to obtain the element-wise bound for the estimation error $\hat\Sigma^{(k)} -\Sigma^{(k)}$ uniformly for all $1\leq k\leq d$.
	
	Since 
	$$S- \Sigma = \frac{1}{d}\sum_{k=1}^d(\hat\Sigma^{(k)}-\Sigma^{(k)})-\frac{1}{d}\sum_{k=1}^d\left(\frac{1}{d^2}\mathbf{1}^T(\hat\Sigma^{(k)}-\Sigma^{(k)})\mathbf{1}\right)\mathbf{1}\mathbf{1}^T,$$
	which implies that $\|S-\Sigma\|_\infty\leq 2\max_{1\leq k\leq d}\|\hat\Sigma^{(k)} -\Sigma^{(k)}\|_\infty$,
	we immediately get the inequality \eqref{eq:inequality for sigma} with replacing $C_1$ by $2C_1$. W.l.o.g., we continue using $C_1$.
	
	For the asymptotic statement, note that if $(\log n)^4\sqrt{\frac{\log d}{k_n}}\to 0$ as $n\to\infty$, then the lower bound for $\varepsilon$, $\varepsilon_n\to 0$ as $n\to\infty$. The asymptotic statement follows immediately.
	\end{proof}
	
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proof of Theorem~\ref{thm:main}} \label{appendix: proof for Section 4}

%%%%%%%%%%%%%%%%%%%%%%%%

\begin{proof}[Proof of Theorem~\ref{thm:main}]
We shall work with the event 
$$A_\varepsilon=\{\|S-\Sigma\|_\infty< \delta_n\},$$
which satisfies $\mathbb{P}(A_\varepsilon)>1-\varepsilon$ following Proposition \ref{prop:accuracy of S estimation}. Denote $c := \frac{1}{d^2M}.$ Then
$$
	\Theta^* = \Theta + c \mathbf1\mathbf1^T.
$$
Here we omit $M$ in the notation for simplicity. 

Recall that $\hat\Theta^*$ is the solution to the following graphical lasso problem
$$
	\hat{\Theta}^* := {\arg\min}_{\Theta^*} \left\{-\log|\Theta^*| + tr(S^*\Theta^*) + \gamma_n \sum_{i\ne j} |\Theta^*_{ij} - c|\right\}
$$
and $\hat{\Theta}_{lasso} := \hat{\Theta}^* - c\mathbf1\mathbf1^T$.   The estimated edge set is $\hat E:=\{(i,j):\hat\Theta_{lasso,ij} \neq 0\}.$

We aim to prove that on $A_\varepsilon$, $\hat E \subset E,$ and
$$
	\|\hat{\Theta}_{lasso} - \Theta\|_\infty \le r_n,
$$
which is equivalent to proving
$$
	\|\hat{\Theta}^* - \Theta^*\|_\infty \le r_n.
$$

%Let $m = \min\{\|\Theta_{ij}\|; (i,j) \in E\}$ be the smallest magnitude of the non-zero entry in the precision matrix $\Theta$.  For $n$ such that $r_n < m$, we have
%$$
%	\hat{E} = E,
%$$
%that is, the estimated graph is equal to the true graph.  If we can show that $r_n \to 0 $ with high probability, then it implies that the estimated graph is consistent.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We first show that the solution $\hat\Theta^*$ exsits and is unique. The proof follows the same lines as that for Lemma 3 in \cite{ravikumar2011high}. Note that the estimator $S^*$ is positive definite with all diagonal elements being positive. The rest of the proof follows exactly the same arguments therein.

Next, the solution $\hat\Theta^*$ must satisfy the following KKT condition.
\begin{equation} \label{eq:KKT for Theta}
	-\left(\hat\Theta^*\right)^{-1} + S^* + \gamma_n\hat{Z} = 0,
\end{equation}
where
$$
	\hat Z_{ij} = \begin{cases}
		0 & \text{  if } i=j,\\
		\text{sign}(\hat\Theta^*_{ij}- c) & \text{  if } i\neq j \text{ and } \hat\Theta^*_{ij} \ne c,\\
		\in[-1,1] &\text{  if } i\neq j \text{ and } \hat\Theta^*_{ij}= c.
	\end{cases}
$$
We shall construct a ``witness'' precision matrix $\tilde{\Theta}^*$ as follows. Let $\tilde\Theta^*$ be the solution to the following optimization problem,
\begin{equation} \label{eq: problem tilde Theta}
	\tilde{\Theta}^* := {\arg\min}_{\{\Theta^*:\Theta^*_{ij} = c, (i,j) \in E^c\}} -\log|\Theta^*| + tr(S^*\Theta^*) + \gamma_n \sum_{i\ne j} |\Theta^*_{ij} - c|.
\end{equation}
This is the same optimization but constrained to a smaller domain.  Let $\tilde{E}$ denote the graph recovered from $\tilde\Theta^*$. Clearly, $\tilde{\Theta}^*$ satisfies: $\tilde{\Theta}^*_{ij} = c$ for $(i,j) \in E^c$, i.e. $\tilde{E} \subset E$.

We shall show that under the conditions in Theorem \ref{thm:main},
\begin{itemize}
\item
	$\tilde{\Theta}^*$ satisfies the above KKT condition;
\item
	$\|\tilde{\Theta}^* - \Theta^*\|_\infty \le r_n$.
\end{itemize}
Then by uniqueness, $\tilde{\Theta}^* = \hat\Theta^*$ and satisfies the goal that we are aiming to prove.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


With a similar argument regarding the existence and uniqueness of the original optimization problem, the solution to the problem \eqref{eq: problem tilde Theta} also exists and is unique. In addition, it satisfies a similar KKT condition as follows,
$$
	-\left(\tilde\Theta^*\right)^{-1}_{ij} + S^*_{ij} + \gamma_n\tilde{Z}_{ij} = 0, \quad (i,j) \in E,
$$
where
$$
	\tilde Z_{ij} = \begin{cases}
		\text{sign}(\tilde\Theta^*_{ij}- c) & \text{if } \tilde\Theta^*_{ij} \ne c,\\
		\in[-1,1] &\text{if } \tilde\Theta^*_{ij}= c.
	\end{cases}
$$
Note that this coincides with the KKT condition \eqref{eq:KKT for Theta}, but only on entries indexed by $E$.  As a matter of fact, $\tilde{Z}$ is only defined on $E$.  In order to argue $\tilde{\Theta}^*$ as a candidate for $\hat\Theta^*$ and satisfies the full KKT condition, we will now extend the definition of $\tilde{Z}$ to $E^c$ as well.

Define
$$
	\tilde{Z}_{ij} := \frac{1}{\gamma_n} \left(\left(\tilde\Theta^*\right)^{-1}_{ij} - S^*_{ij}\right), \quad (i,j) \notin E.
$$
Then the pair $(\tilde\Theta^*,\tilde{Z})$ satisfies the original KKT equation \eqref{eq:KKT for Theta}.  What remains to be proved is that $\tilde{Z}$ also satisfies
$$
	|\tilde{Z}_{ij}| \le 1, \quad (i,j) \notin E.
$$

To summarize, in order to complete the proof of Theorem \ref{thm:main}, we will show that on the set $A_\varepsilon$,\\
{\bf Goal 1}: $|\tilde{Z}_{ij}| \le 1, \quad (i,j) \notin E.$\\
% \begin{itemize}
% \item
% 	This implies that $\hat\Theta^* = \tilde\Theta^*$.
% \end{itemize}
{\bf Goal 2}: $\|\tilde{\Theta}^* - \Theta^*\|_\infty \le r_n$.

In the rest of the proof we denote
$$
	\Delta := \tilde\Theta^* - \Theta^*.
$$
Note that for $(i,j) \notin E$, $\tilde\Theta_{ij} = c$ by definition and $\Theta_{ij} = c$.  Therefore, $\Delta_{E^c} = \mathbf0$ and {\bf Goal 2} above can be translated to
$$
	\|\Delta_E\| _\infty \le r_n.
$$	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
To handle the KKT condition for $\tilde\Theta^*$, we start with handling $\left(\tilde\Theta^*\right)^{-1}$ as follows:
\begin{eqnarray*}
	\left(\tilde\Theta^*\right)^{-1} &=& \left(\Theta^* + \Delta\right)^{-1} \\
	&=&  \left(\Theta^*(I  + \Sigma^*\Delta)\right)^{-1} \\
	&=& (I  + \Sigma^*\Delta)^{-1} \Sigma^* =: J\Sigma^*.
\end{eqnarray*}
Now in the case where $|||\Sigma^*\Delta|||_\infty < 1$, we can expand $J$ as
$$
	J = \sum_{k=0}^\infty (-1)^k(\Sigma^*\Delta)^k = I - \Sigma^*\Delta + (\Sigma^*\Delta)^2J.
$$
Inspired from this relation, we can use $\Sigma^* - \Sigma^*\Delta\Sigma^*$ to approximate $\left(\tilde\Theta^*\right)^{-1}$ and define
$$
	R :=\left(\tilde\Theta^*\right)^{-1} - (\Sigma^* - \Sigma^*\Delta\Sigma^*),
$$
as the approximation error. Note that $R$ is defined regardless of whether $|||\Sigma^*\Delta|||_\infty < 1$.

Recall that $\Sigma^* = \Sigma + M\mathbf1\mathbf1^T$ and $S^* = S + M\mathbf1\mathbf1^T$. Define
$$
	 R':=S^*-\Sigma^*=S-\Sigma.
$$
On the set $A_\varepsilon$, we have that $\|R'\|_\infty \le \delta_n$.  

Rewrite the KKT condition as
$$
	\Sigma^*\Delta\Sigma^* - R + R' + \gamma_n\tilde{Z} = 0.
$$
We vectorize it using the notation $\bar{\cdot}$ as the vectorization of a matrix.  Then the vectorized KKT condition is 
$$
	\overline{\Sigma^*\Delta\Sigma^*} - \overline R + \overline{R'} + \gamma_n\overline{\tilde{Z}} = 0.
$$
Note that
$$
	\overline{\Sigma^*\Delta\Sigma^*} = (\Sigma^*\otimes\Sigma^*)\overline\Delta =: \Omega\overline\Delta,
$$
where $\Omega:= \Sigma^*\otimes\Sigma^*$ denotes the Kronecker product of $\Sigma^*$ with itself.  Then we have
$$
	\Omega\overline\Delta - \overline R + \overline{R'} + \gamma_n\overline{\tilde{Z}} = 0.
$$

By examining the rows of $\Omega$ indexed by $E$ and $E^c$ separately and noting that $\Delta_{E^c}=0$, we get
\begin{eqnarray}
	\Omega_{EE}\overline\Delta_E - \overline R_E + \overline{R'}_E + \gamma_n\overline{\tilde{Z}}_E = 0, \label{eq: first equation with E rows}\\
	\Omega_{E^cE}\overline\Delta_E - \overline R_{E^c} + \overline{R'}_{E^c} + \gamma_n\overline{\tilde{Z}}_{E^c} = 0. \label{eq: second equation with Ec rows}
\end{eqnarray}

%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection*{Proof of Goal 2}

To prove Goal 2, we shall show that for any pre-specified $\|\tilde{Z}_E\|_\infty \le 1$, there exists a solution to $\Delta$ which satisfies: 
\begin{itemize}
\item
	$-\left(\Theta^* + \Delta\right)^{-1}_E + S^*_E+ \gamma_n\tilde{Z}_E = 0$;
\item
	$\Delta_{E^c} = \mathbf0$;
\item
	$\|\Delta_E\|_\infty \le r_n$.
\end{itemize}

With the statement above proven, given the $\tilde{Z}_E$ produced from the KKT condition for $\tilde\Theta^*$, a solution $\Delta$ exists and coincides with $\tilde Z_E$. Then this is the unique solution.  Hence $\|\tilde\Theta^* - \Theta^*\|_\infty \le r_n$ which concludes {\bf Goal 2}.

Now we construct such a solution $\Delta$.  Recall that $\Delta_{E^c} = \mathbf0$, we only need to construct a suitable $\Delta_E$ by utilizing the Brouwer fixed point theorem.  

The solution $\Delta_E$ satisfies \eqref{eq: first equation with E rows}, which can be rewritten as
$$
	\overline\Delta_E  = (\Omega_{EE}) ^{-1} \left(\overline R_E - \overline{R'}_E - \gamma_n\overline{\tilde{Z}}_E\right).
$$
We regard
$$
	R =\left(\tilde\Theta^*\right)^{-1} - (\Sigma^* - \Sigma^*\Delta\Sigma^*)
$$
as a function of $\Delta$ or eventually a function of $\overline\Delta_E$.  To stress this point we define it as $R = R(\Delta_E)$. Also recall that $R' = S^* - \Sigma^*$ does not depend on $\overline\Delta_E$.  Then we can write the above equation as 
$$
	\overline\Delta_E  = (\Omega_{EE})^{-1} \left(\overline R_E(\overline\Delta_E) - \overline{R'}_E - \gamma_n\overline{\tilde{Z}}_E\right): = F(\overline\Delta_{E}).
$$

Consider the closed ball $B(r_n) ;= \{x \in \mathbb{R}^{|E|}: \|x\|_\infty \le r_n\}$.   If $F$ is a continuous mapping from $B(r_n)$ onto itself, then there exists a fixed point $\overline\Delta_E$ on $B(r_n)$ such that $\overline\Delta_E = F(\overline\Delta_{E})$ following the Brouwer fixed point theorem.  This is exactly the desired solution.  Since $F$ is clearly continuous, we only need to show that $F$ projects $B(r_n)$ onto itself, that is, for any $\overline\Delta_E$ satisfying $\|\overline\Delta_E\|_\infty \le r_n$, we have $\|F(\overline\Delta_E)\|_\infty \le r_n$.

Assume that $\|\overline\Delta_E\|_\infty \le r_n$. We write
$$
	\|F(\overline\Delta_E)\|_\infty \le  |||(\Omega_{EE})^{-1}|||_\infty \left(  \|R\|_\infty + \|R'\|_\infty + \gamma_n \|\overline{\tilde{Z}}_E\|_\infty \right)\le  |||(\Omega_{EE}) ^{-1}|||_\infty \left(  \|R\|_\infty + \|R'\|_\infty + \gamma_n \right),
$$
due to the fact that $\|\overline{\tilde{Z}}_E\|_\infty \le 1$

We first handle $\|R\|_\infty$. Recall that 
$$
	R :=\left(\tilde\Theta^*\right)^{-1} - (\Sigma^* - \Sigma^*\Delta\Sigma^*)= \sum_{k=2}^\infty (-1)^k(\Sigma^*\Delta)^k \Sigma^*  = (\Sigma^*\Delta)^2J\Sigma^*,
$$
where 
$$
	J = (I + \Sigma^* \Delta)^{-1} = \sum_{k=0}^\infty (-1)^k(\Sigma^*\Delta)^k,
$$
provided that
$
	||| \Sigma^* \Delta|||_\infty < 1 $. To ensure this condition, note that 
	$$
|||\Sigma^*\Delta|||_\infty = |||\Sigma^* (\tilde\Theta - \Theta)|||_\infty \le D|||\Sigma^*|||_\infty\cdot r_n,
$$
where $D$ is the maximum degree in the graph. Therefore $||| \Sigma^* \Delta|||_\infty < 1$ holds by  requiring that
\begin{equation} \label{eq: required condition 1}
	D|||\Sigma^*|||_\infty\cdot r_n \le C_4< 1.
\end{equation}
The upper bound $C_4$ implies that $
	|||J^T|||_\infty \leq \frac{1}{1-C_4}.$

With the condition \eqref{eq: required condition 1}, we can further derive an upper bound for $\|R\|_\infty$. Consider one specific element in $R$. With denoting  $\mathbf{e}_i$ as a vector with all zero elements except a one element at the $i-$th dimension, we have that
$$R_{ij}=\mathbf{e}_i^T (\Sigma^*\Delta)^2J\Sigma^* \mathbf{e}_j\leq \|\mathbf{e}_i^T(\Sigma^*\Delta)^2\|_{\infty}\| J\Sigma^* \mathbf{e}_j\|_{1}\leq \|(\Sigma^*\Delta)^2\|_\infty|||\Sigma^* J^T|||_\infty .$$
By considering all possible $(i,j)$ we get that,
\begin{eqnarray*}
 \|R\|_\infty &\le& \|(\Sigma^*\Delta)^2\|_\infty|||\Sigma^* J^T|||_\infty\\
 &\le& |||\Sigma^*\Delta|||_\infty \|\Sigma^*\Delta\|_\infty|||J^T|||_\infty  |||\Sigma^*|||_\infty\\
&\le& D|||\Sigma^*|||_\infty\cdot r_n \cdot |||\Sigma^*|||_\infty r_n \cdot \frac{1}{1-C_4}\cdot |||\Sigma^*|||_\infty\\
&<& C_5 \cdot r_n^2,
\end{eqnarray*}
where $C_5=\frac{D}{1-C_4}|||\Sigma^*|||_\infty^3 $.

Next, since $
	R' = S^* - \Sigma^* = S - \Sigma,$ we have that on $A_\varepsilon$, $
	\|R'\|_\infty \le \delta_n.
$ Combining the upper bounds for $\|R\|_\infty$ and $\|R'\|_\infty$, we get that on $A_\varepsilon$,
$$
	\|F(\Delta_E)\|_\infty  \le  |||(\Omega_{EE})^{-1}|||_\infty ( C_5 \cdot r^2_n + \delta_n + \gamma_n) \le r_n,
$$
by requiring that
\begin{equation} \label{eq: required condition 2}
	C_5 \cdot r_n^2 + \delta_n + \gamma_n \le \frac{1}{|||(\Omega_{EE})^{-1}|||_\infty} \cdot r_n
\end{equation}

If the two required conditions \eqref{eq: required condition 1} and \eqref{eq: required condition 2} hold, we achieve \textbf{Goal 2} by utilizing the Brouwer fixed point theorem.

\subsection*{Proof of Goal 1}

To prove \textbf{Goal 1}, we shall show that with the constructed soluition above, we have
$$
	\|\overline{\tilde{Z}}_{E^c}\|_\infty \le 1.
$$

We rewrite the equation \eqref{eq: second equation with Ec rows} as
$$
	\overline{\tilde{Z}}_{E^c} = -\frac{1}{\gamma_n}\Omega_{EE^c}\overline\Delta_E + \frac{1}{\gamma_n} \overline R_{E^c} -\frac{1}{\gamma_n} \overline{R'}_{E^c},
$$
and the substitute  $\overline\Delta_E$ above using \eqref{eq: first equation with E rows} to get that 
$$
	\overline{\tilde{Z}}_{E^c} = -\frac{1}{\gamma_n}\Omega_{EE^c}(\Omega_{EE})^{-1} \left(- \overline R_E + \overline{R'}_E + \gamma_n\overline{\tilde{Z}}_E\right) + \frac{1}{\gamma_n} \overline R_{E^c} -\frac{1}{\gamma_n} \overline{R'}_{E^c}.
$$
The upper bound for $\|\overline{\tilde{Z}}_{E^c}\|_\infty $ is then  
$$
	\|\overline{\tilde{Z}}_{E^c}\|_\infty \le \frac{1}{\gamma_n}|||\Omega_{EE^c}(\Omega_{EE})^{-1}|||_\infty \left( \|R\|_\infty + \|R'\|_\infty +\gamma_n\|\overline{\tilde{Z}}_E\|_\infty \right) + \frac{1}{\gamma_n}\|R\|_\infty + \frac{1}{\gamma_n}\|R'\|_\infty.
$$
Using the same upper bounds derived in the proof of \textbf{Goal 2}, we have
\begin{eqnarray*}
	\|\overline{\tilde{Z}}_{E^c}\|_\infty &\le& \frac{1}{\gamma_n}|||\Omega_{EE^c}(\Omega_{EE})^{-1}|||_\infty \left( \delta_n + C_5 \cdot r_n^2 +\gamma_n\right) +  \frac{1}{\gamma_n}(\delta_n + C_5 \cdot r_n^2)\\
	&=&|||\Omega_{EE^c}(\Omega_{EE})^{-1}|||_\infty+\frac{1}{\gamma_n}\left(|||\Omega_{EE^c}(\Omega_{EE})^{-1}|||_\infty+1\right)\left( \delta_n + C_5 \cdot r_n^2\right).
\end{eqnarray*}
Since Condition \eqref{con:mi} ensures that $|||\Omega_{EE^c}(\Omega_{EE})^{-1}|||_\infty<1-\alpha$, 
To satisfy $\|\overline{\tilde{Z}}_{E^c}\|_\infty \le 1$, we only need to further require
\begin{equation} \label{eq: required condition 3}
	\delta_n + C_5 \cdot r_n^2 \le \frac{\alpha}{1-\alpha} \gamma_n.	
\end{equation}

To conclude, the theorem is proven provided that the three conditions \eqref{eq: required condition 1}--\eqref{eq: required condition 3} hold. The last step is to verify these three relations under the conditions in Theorem \ref{thm:main}.

Recall that $r_n$ is defined in \eqref{eq:definition r}
$$r_n:=\frac{|||\left(\Omega_{EE} \right)^{-1}|||_\infty}{1-\alpha} \cdot \gamma_n.$$
Clearly, this definition together with  \eqref{eq: required condition 3} implies  \eqref{eq: required condition 2}. Hence we only need to verify the conditions \eqref{eq: required condition 1} and  \eqref{eq: required condition 3}.

We write the two conditions in terms of  $\delta_n$ and $\gamma_n$:
\begin{eqnarray*}
	D|||\Sigma^*|||_\infty \frac{|||(\Omega_{EE})^{-1}|||_\infty}{1-\alpha} \cdot \gamma_n  &\le&C_4 \\
	\delta_n + \frac{D}{1-C_4}|||\Sigma^*|||_\infty^3 \cdot \left(\frac{|||(\Omega_{EE})^{-1}|||_\infty}{1-\alpha} \right)^2\cdot \gamma_n^2 & \le& \frac{\alpha}{1-\alpha} \cdot \gamma_n.
\end{eqnarray*}
where $C_5$ is substituted by $ \frac{D}{1-C_4}|||\Sigma^*|||_\infty^3$:

Note that the lower bound for $\gamma_n$ in \eqref{eq:lowerbound rho} ensures that $\delta_n\leq \epsilon \frac{\alpha}{1-\alpha} \gamma_n$ for some $0<\epsilon<1$, we thus need to require that
$$
\frac{D}{1-C_4}|||\Sigma^*|||_\infty^3 \cdot \left(\frac{|||(\Omega_{EE})^{-1}|||_\infty}{1-\alpha} \right)^2\cdot \gamma_n\leq (1-\epsilon) \frac{\alpha}{1-\alpha},
$$
which guarantees the second condition. Together with the first condition, we have obtained an upper bound for $\gamma_n$ as
$$\gamma_n\leq \min\left\{\frac{C_4(1-\alpha)}{D|||\Sigma^*|||_\infty |||(\Omega_{EE})^{-1}|||_\infty},\frac{(1-C_4)(1-\epsilon)\alpha(1-\alpha)}{D|||\Sigma^*|||_\infty^3|||(\Omega_{EE})^{-1}|||_\infty^2}\right\}.$$
We choose $C_4$ such that the two terms in the minimum are equal. That is
$$C_4=\frac{(1-\epsilon)\alpha}{(1-\epsilon)\alpha+|||\Sigma^*|||_\infty^2|||(\Omega_{EE})^{-1}|||_\infty}<1,$$
which leads to
$$\gamma_n\leq \frac{(1-\epsilon)\alpha(1-\alpha)}{D|||\Sigma^*|||_\infty |||(\Omega_{EE})^{-1}|||_\infty\left[(1-\epsilon)\alpha+|||\Sigma^*|||_\infty^2|||(\Omega_{EE})^{-1}|||_\infty\right]}.$$
This is exactly the required upper bound for $\gamma_n$ in \eqref{eq:upperbound rho}.

% In other words,
% $$
% 	C = \min\left\{ \frac{(1-\epsilon)\alpha}{3D|||\Sigma^*|||_\infty^2 \cdot |||\Omega_{EE} ^{-1}|||_\infty}, \frac{1}{2} \right\}
% $$
% This leads to a simplified version of Condition 2:
% $$
% 	 \delta_n  \le \epsilon \cdot \frac{\alpha}{2-\alpha} \cdot \gamma_n.
% $$
% This corresponds to the lower bound for $\gamma_n$ in Condition~\ref{con:gamma}:
% $$
% 	\gamma_n \ge \frac{2-\alpha}{\epsilon\alpha} \cdot \delta_n.
% $$
% Meanwhile, Condition 1 translates to the upper bound for $\gamma_n$ in Condition~\ref{con:gamma}:
% $$
% 	\gamma_n \le \frac{(2-\alpha)C}{2D|||\Sigma^*|||_\infty|||\Omega_{EE} ^{-1}|||_\infty} = \frac{(1-\epsilon)\alpha}{3D|||\Sigma^*|||_\infty \cdot |||\Omega_{EE} ^{-1}|||_\infty} \cdot \min\left\{ \frac{(1-\epsilon)\alpha}{3D|||\Sigma^*|||_\infty^2 \cdot |||\Omega_{EE} ^{-1}|||_\infty}, \frac{1}{2} \right\}.
% $$


\end{proof}


\section{Blockwise coordinate descent algorithm} \label{appendix:algorithm}
For the sake of clarify, we abuse the notations in this section by using $\Theta$ for $\Theta^*$ and $S$ for $S^*$. We describe the algorithm to solvethe minimization problem:
$$
	\min_{\Theta\ge0} - \log|\Theta| + tr(S\Theta) + \gamma\sum_{i\ne j}|\Theta_{ij}-c|.
$$
Here $\Theta$ is the precision matrix to be estimated and $S$ is an estimated covariance matrix guaranteed to be positive definite.  
% \subsection{The goal}
%  Instead of the usual graphical lasso where we try to shrink the elements of $\Theta$ toward 0, we instead shrink them toward $c$ using the corresponding $L_1$-penalty.

Similar to classical graphical lasso, the objective function is convex. Searching for the optimum is equivalent to solving the KKT condition
$$
	-\Theta^{-1} + S + \gamma \mathbf{Z} = \mathbf0,
$$
where $\mathbf{Z}$ is a matrix of component-wise signs of $\Theta - c\bf1\bf1^T$:
\begin{eqnarray*}
z_{ii} = 0 &\text{if}& i=j \\
z_{ij} = \text{sign}(\theta_{ij} - c) &\text{if}& i\ne j, \quad \theta_{ij} \ne c \\
z_{ij} \in [-1,1] &\text{if}& i\ne j, \quad \theta_{ij} = c.
\end{eqnarray*}

In the following we will demonstrate a blockwise coordinate descent approach to solve this problem.  A primative version is used for the conventional graphical lasso problem for $c=0$ in \cite{fri2008} and implemented in the R package {\it glasso}.  However, to apply that algorithm to our generalized problem, an additional matrix inversion of a $(d-1)\times(d-1)$ matrix is required at each iteration step.  By contrast, the algorithm in this appendix can be seen as the dual problem of that in \cite{fri2008}. Similar to \cite{mazumder2012graphical}, no matrix inversion is needed in our algorithm.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The idea}

Let us consider solving for $\Sigma = \Theta^{-1}$.  Then the KKT condition becomes
\begin{equation} \label{eq:kkt}
	-\Sigma +  S + \gamma \mathbf{Z} = \mathbf0.
\end{equation}
Since $z_{ii}=0$ for each $i$, we first get that
$$
	\hat{\sigma}_{ii} = s_{ii}, \quad \forall i = 1,\ldots,d.
$$
Let us write
$$
	\Sigma = \left(
	\begin{array}{cc}
	\Sigma_{11} & \boldsymbol\sigma_{1d} \\
	\boldsymbol\sigma_{1d}^T & \sigma_{dd}
	\end{array}
	\right)
	, \quad
	\Theta = \left(
	\begin{array}{cc}
	\Theta_{11} & \boldsymbol\theta_{1d} \\
	\boldsymbol\theta_{1d}^T & \theta_{dd}
	\end{array}
	\right)
	, \quad
	\mathbf{Z} = \left(
	\begin{array}{cc}
	\mathbf{Z}_{11} & \mathbf{z}_{1d} \\
	\mathbf{z}_{1d}^T & z_{dd}
	\end{array}
	\right).
$$
In the following, we aim to update $\Sigma$ by keeping $\Theta_{11}$ fixed.  We iterate through the columns (rows) of $\Theta$ until a convergence is reached.

From $\Sigma \cdot \Theta = I$, we have the following two presentations of $\Sigma$:
\begin{eqnarray}
\left(
	\begin{array}{cc}
	\Sigma_{11} & \boldsymbol\sigma_{1d} \\
	\boldsymbol\sigma_{1d}^T & \sigma_{dd}
	\end{array}
	\right)
	&=&
\left(
	\begin{array}{cc}
	(\Theta_{11} -  \boldsymbol\theta_{1d}\boldsymbol\theta_{1d}^T)^{-1} & -\theta_{dd}^{-1}\Sigma_{11}\boldsymbol\theta_{1d} \\
	\cdot & \theta_{dd}^{-1} - \theta_{dd}^{-2}\boldsymbol\theta_{1d}^T\Sigma_{11}\boldsymbol\theta_{1d}
	\end{array}
	\right) \label{eq:inverse1} \\
	&=&
\left(
	\begin{array}{cc}
	\Theta_{11}^{-1} +\frac{\Theta_{11} ^{-1}\boldsymbol\theta_{1d}\boldsymbol\theta_{1d}^T\Theta_{11}^{-1} }{\theta_{dd} - \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}} 
		& -\frac{\Theta_{11} ^{-1}\boldsymbol\theta_{1d}}{\theta_{dd} - \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}} \\
	\cdot 
		& \frac{1}{\theta_{dd} - \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}},
	\end{array}
	\right) \label{eq:inverse2}
\end{eqnarray}
where $\cdot$ denotes the mirroring of elements in the upper triangle. The proofs can be found in Section~\ref{sec:algo:proofs}.  The same formula can be applied for a representation of $\Theta$ using $\Sigma$.

Consider the last column of \eqref{eq:kkt}, we get
$$
	-\boldsymbol\sigma_{1d}  + \mathbf{s}_{1d} + \gamma \mathbf{z}_{1d} = \mathbf{0}.
$$
Plugging in \eqref{eq:inverse2}, we have
$$
	\sigma_{dd}\Theta_{11} ^{-1}\boldsymbol\theta_{1d}   + \mathbf{s}_{1d} + \gamma \mathbf{z}_{1d}  = \mathbf{0},
$$
where $\sigma_{dd}$ is known.
Now set $\boldsymbol\beta = (\boldsymbol\theta_{1d}-c\mathbf{1})\sigma_{dd}$.  Then the above equation becomes
\begin{equation} \label{eq:beta:solve}
	\Theta_{11}^{-1}(\boldsymbol\beta  + c\sigma_{dd} \cdot \mathbf{1}) + \mathbf{s}_{1d} + \gamma \mathbf{z}_{1d}  = \Theta_{11} ^{-1} \boldsymbol\beta  +  c\sigma_{dd} \cdot \Theta_{11} ^{-1}\mathbf{1} + \mathbf{s}_{1d} + \gamma \mathbf{z}_{1d} = \mathbf{0},
\end{equation}
where we aim to solve for $\boldsymbol\beta$.  Note that 
$$
	\mathbf{z}_{1d} = \text{sign}(\boldsymbol\theta_{1d}-c\mathbf{1})=  \text{sign}((\boldsymbol\theta_{1d}-c\mathbf{1})\sigma_{dd}) = \text{sign}(\boldsymbol\beta).
$$
Then solving for \eqref{eq:beta:solve} is equivalent to the standard quadratic lasso problem:
\begin{equation} \label{eq:beta:lasso}
	\min_{\boldsymbol\beta} f(\boldsymbol\beta; \Theta_{11}, \sigma_{dd},\mathbf{s}_{1d}, \gamma) = \frac{1}{2}\boldsymbol\beta^T \Theta_{11}^{-1}\boldsymbol\beta + \boldsymbol\beta^T(c\sigma_{dd} \cdot \Theta_{11}^{-1}\mathbf{1}+\mathbf{s}_{1d})  + \gamma \|\boldsymbol\beta\|_1,
\end{equation}
which can be solve efficiently using elementwise coordinate descent if we know $\Theta_{11}^{-1}$.

At each iteration, we aim to update $\Theta$ and then $\Sigma$.  Given $\Theta$ and $\Sigma$ from the previous iteration, we proceed as follows:
\begin{itemize}
	\item
		Calculate $\Theta_{11}^{-1}$ from
		$$
			\Theta_{11}^{-1} = \Sigma_{11} - \sigma_{dd}^{-1}\boldsymbol\sigma_{1d}\boldsymbol\sigma_{1d}^T.
		$$
		This is the opposite representation of \eqref{eq:inverse2}.
	\item
		Update $\boldsymbol\theta_{1d}$:  Solve for $\hat{\boldsymbol\beta}$ from \eqref{eq:beta:lasso} and update
		$$
			\boldsymbol\theta_{1d} \leftarrow \sigma_{dd}^{-1}\hat{\boldsymbol\beta} + c\mathbf{1}.
		$$
	\item
		Update $\theta_{dd}$:
		$$
			\theta_{dd} = \sigma_{dd}^{-1} + \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}.
		$$
		This comes from the representation in \eqref{eq:inverse2}.
	\item
		Update the entire $\Sigma$ matrix from representation \eqref{eq:inverse2} using the fixed $\Theta_{11}^{-1}$ and the updated $\boldsymbol\theta_{1d}$ and $\theta_{dd}$.
\end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The algorithm is summarized in Algorithm~\ref{alg:cap}.

\begin{algorithm}
\caption{Graphical lasso algorithm for extremes}\label{alg:cap}
\begin{algorithmic}
\State Input $c$, $S$ and $\gamma$.  
\begin{enumerate}
\item
	Initialize $\Sigma = S$ and $\Theta = S^{-1}$.
\item
	In each iteration, update $\Sigma$ while keeping a $(d-1)\times(d-1)$ submatrix of $\Theta$ fixed.  Iterate through the columns repeatedly on the following steps until convergence.
	\begin{enumerate}
	\item
		Rearrange the rows/columns such that the target column is last (implicitly).
	\item
		Calculate
		$$
			\Theta_{11}^{-1} = \Sigma_{11} - \sigma_{dd}^{-1}\boldsymbol\sigma_{1d}\boldsymbol\sigma_{1d}^T.
		$$
	\item
		Solve for \eqref{eq:beta:lasso}
		$$
			\hat{\boldsymbol\beta} = \arg\min_{\boldsymbol\beta} f(\boldsymbol\beta; \Theta_{11}, \sigma_{dd}, \mathbf{s}_{1d}, \gamma) 
		$$
	\item
		Update $\boldsymbol\theta_{1d}$: 
		$$
			\boldsymbol\theta_{1d} \leftarrow \sigma_{dd}^{-1}\hat{\boldsymbol\beta} + c\mathbf{1}.
		$$
	\item
		Update $\theta_{dd}$:
		$$
			\theta_{dd} = \sigma_{dd}^{-1} + \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}.
		$$
	\item
		Update the entire $\Sigma$ matrix from representation \eqref{eq:inverse2} using the fixed $\Theta_{11}^{-1}$ and the updated $\boldsymbol\theta_{1d}$ and $\theta_{dd}$.
	\end{enumerate}
\end{enumerate}
\end{algorithmic}
\end{algorithm}

%
%\begin{enumerate}
%\item
%	Given $c$, $S$ and $\gamma$.  Initialize $\Sigma = S$ and $\Theta = S^{-1}$.
%\item
%	In each iteration, update $\Sigma$ while keeping a $(d-1)\times(d-1)$ submatrix of $\Theta$ fixed.  Iterate through the columns repeatedly on the following steps until convergence.
%	\begin{enumerate}
%	\item
%		Rearrange the rows/columns such that the target column is last (implicitly).
%	\item
%		Calculate
%		$$
%			\Theta_{11}^{-1} = \Sigma_{11} - \sigma_{dd}^{-1}\boldsymbol\sigma_{1d}\boldsymbol\sigma_{1d}^T.
%		$$
%	\item
%		Solve for \eqref{eq:beta:lasso}
%		$$
%			\hat{\boldsymbol\beta} = \arg\min_{\boldsymbol\beta} f(\boldsymbol\beta; \Theta_{11}, \sigma_{dd}, \mathbf{s}_{1d}, \gamma) 
%		$$
%	\item
%		Update $\boldsymbol\theta_{1d}$: 
%		$$
%			\boldsymbol\theta_{1d} \leftarrow \sigma_{dd}^{-1}\hat{\boldsymbol\beta} + c\mathbf{1}.
%		$$
%	\item
%		Update $\theta_{dd}$:
%		$$
%			\theta_{dd} = \sigma_{dd}^{-1} + \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}.
%		$$
%	\item
%		Update the entire $\Sigma$ matrix from representation \eqref{eq:inverse2} using the fixed $\Theta_{11}^{-1}$ and the updated $\boldsymbol\theta_{1d}$ and $\theta_{dd}$.
%	\end{enumerate}
%\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Proofs} \label{sec:algo:proofs}
We will now prove the equations \eqref{eq:inverse1} and \eqref{eq:inverse2}.

From $\Sigma \cdot \Theta = I$, we get the following equations.
\begin{eqnarray}
	\Sigma_{11} \Theta_{11} + \boldsymbol\sigma_{1d}\boldsymbol\theta_{1d}^T &=& I \label{eq:inveq1}\\
	\Sigma_{11} \boldsymbol\theta_{1d} + \theta_{dd} \cdot \boldsymbol\sigma_{1d} &=& \mathbf{0} \label{eq:inveq2}\\
	\boldsymbol\sigma_{1d}^T \Theta_{11} + \sigma_{dd} \cdot \boldsymbol\theta_{1d}^T &=& \mathbf{0}^T \label{eq:inveq3}\\
	\boldsymbol\sigma_{1d}^T \boldsymbol\theta_{1d} + \sigma_{dd}\cdot \theta_{dd} &=& 1.\label{eq:inveq4}
\end{eqnarray}
From \eqref{eq:inveq2}, we have
$$
	 \boldsymbol\sigma_{1d} = - \theta_{dd}^{-1}\Sigma_{11} \boldsymbol\theta_{1d}.
$$
From \eqref{eq:inveq4}, we have
$$
	\sigma_{dd}  =  \theta_{dd}^{-1} (1 - \boldsymbol\sigma_{1d}^T \boldsymbol\theta_{1d}) = \theta_{dd}^{-1} + \theta_{dd}^{-2}\boldsymbol\theta_{1d}^T\Sigma_{11}\boldsymbol\theta_{1d}.
$$
From \eqref{eq:inveq1}, we have
$$
	\Sigma_{11}^{-1} = \Theta_{11} + \Sigma_{11}^{-1} \boldsymbol\sigma_{1d}\boldsymbol\theta_{1d}^T
	= \Theta_{11} - \Sigma_{11}^{-1} \theta_{dd}^{-1}\Sigma_{11} \boldsymbol\theta_{1d}\boldsymbol\theta_{1d}^Td = \Theta_{11} - \theta_{dd}^{-1}\boldsymbol\theta_{1d}\boldsymbol\theta_{1d}^T 
$$
This proves \eqref{eq:inverse1}.  

Now consider \eqref{eq:inverse2}.  From \eqref{eq:inveq1} and \eqref{eq:inveq3}, we have
\begin{eqnarray}
	\Sigma_{11} &=& \Theta_{11}^{-1} - \boldsymbol\sigma_{1d}\boldsymbol\theta_{1d}^T\Theta_{11}^{-1} \label{eq:eq1} \\
	 \boldsymbol\sigma_{1d} &=& -\sigma_{dd}\Theta_{11} ^{-1}\boldsymbol\theta_{1d} \label{eq:eq2} 
\end{eqnarray}
Plug \eqref{eq:eq2} into \eqref{eq:inveq4}, we get
$$
	 \sigma_{dd}\theta_{dd}  -\sigma_{dd}\boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d} = 1 
$$
and hence
$$
	\sigma_{dd} = \frac{1}{\theta_{dd} - \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}}.
$$
Plugging in \eqref{eq:eq2}, we have
$$
	\boldsymbol\sigma_{1d} = -\frac{\Theta_{11} ^{-1}\boldsymbol\theta_{1d}}{\theta_{dd} - \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}}.
$$
Plugging in \eqref{eq:eq1}, we have
$$
	\Sigma_{11} = \Theta_{11}^{-1} +\frac{\Theta_{11} ^{-1}\boldsymbol\theta_{1d}\boldsymbol\theta_{1d}^T\Theta_{11}^{-1} }{\theta_{dd} - \boldsymbol\theta_{1d}^T\Theta_{11} ^{-1} \boldsymbol\theta_{1d}}.
$$
This proves \eqref{eq:inverse2}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
