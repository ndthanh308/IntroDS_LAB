% \def\year{2021}\relax
%File: formatting-instructions-latex-2021.tex
%release 2021.2
\documentclass{article} % DO NOT CHANGE THIS
% \usepackage{aaai21}  % DO NOT CHANGE THIS
\usepackage[preprint, nonatbib]{nips_2018}
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[table,xcdraw]{xcolor}
% \usepackage{times}  % DO NOT CHANGE THIS
% \usepackage{helvet} % DO NOT CHANGE THIS
% \usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage{url}  % DO NOT CHANGE THIS

% \usepackage{csquotes}
\usepackage{amssymb}
% \usepackage{rotating}
\usepackage{amsmath}
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm}  % DO NOT CHANGE THIS
% % \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{array}
% \usepackage{flushend}
% \frenchspacing  % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
% % \usepackage[utf8]{inputenc}
% % \usepackage[T1]{fontenc}
% %\usepackage{lineno}
% \usepackage{setspace} 
% %\linenumbers
% \usepackage{makecell}
% \usepackage[flushleft]{threeparttable}
% \usepackage{booktabs,caption}
% \usepackage[toc,page]{appendix}
\usepackage{subcaption}
% \usepackage{rotating}
% \usepackage{multirow}

% % \usepackage{ulem}

% \usepackage[usenames,dvipsnames]{xcolor}
% \usepackage{tikz}
% \newcommand*\circled[1]{\tikz[baseline=(char.base)]{
%             \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\newcommand{\etal}{\textit{et al}. }
\newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}., }
% \def\year{2021}\relax
% %File: formatting-instructions-latex-2021.tex
% %release 2021.2
% \documentclass[letterpaper]{article} % DO NOT CHANGE THIS
% \usepackage{aaai21}  % DO NOT CHANGE THIS
% \usepackage{times}  % DO NOT CHANGE THIS
% \usepackage{helvet} % DO NOT CHANGE THIS
% \usepackage{courier}  % DO NOT CHANGE THIS
% \usepackage[hyphens]{url}  % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm}  % DO NOT CHANGE THIS
% \usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOnT ADD ANY OPTIONS TO IT
% \usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
% \usepackage{hyperref} 
% \usepackage{float}
% \usepackage{array}
% \newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
% \frenchspacing  % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS


% % svg package
% \usepackage{svg}
% \usepackage{tabularx,colortbl}
% \newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
% \newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}

%\nocopyright
%PDF Info Is REQUIRED.
% For /Author, add all authors within the parentheses, separated by commas. No accents or commands.
% For /Title, add Title in Mixed Case. No accents or commands. Retain the parentheses.
% \pdfinfo{
% /Title (Understanding Public Opinion on Chinese Technology Companies Using Reddit Data)
% /Author (Enting Zhou, Yurong Liu)
% /TemplateVersion (2021.2)
% } %Leave this
% /Title ()
% Put your actual complete title (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case
% Leave the space between \Title and the beginning parenthesis alone
% /Author ()
% Put your actual complete list of authors (no codes, scripts, shortcuts, or LaTeX commands) within the parentheses in mixed case.
% Each author should be only by a comma. If the name contains accents, remove them. If there are any LaTeX commands,
% remove them.

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference
\usepackage{amsbsy}
\usepackage{graphicx}
% \setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai21.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

\title{LLM-Rec: Personalized Recommendation via Prompting Large Language Models}


\author{
Hanjia~Lyu$^{1}$ \quad Song~Jiang$^{2}$ \quad Hanqing~Zeng$^{3}$ \quad Qifan~Wang$^{3}$ \quad Si~Zhang$^{3}$\\
\textbf{Ren~Chen$^{3}$} \quad \textbf{Chris~Leung}$^{3}$ \quad \textbf{Jiajie~Tang}$^{3}$ \quad \textbf{Yinglong~Xia}$^{3}$ \quad \textbf{Jiebo~Luo}$^{1}$\\
$^{1}$University of Rochester \quad $^{2}$UCLA \quad $^{3}$Meta AI\\
\texttt{hlyu5@ur.rochester.edu} \quad \texttt{yxia@meta.com} \quad \texttt{jluo@cs.rochester.edu}
}


\begin{document}

\maketitle



\begin{abstract}
We investigate various prompting strategies for enhancing personalized recommendation performance with large language models (LLMs) through \textit{input augmentation}. Our proposed approach, termed \textbf{LLM-Rec}, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) recommendation-driven + engagement-guided prompting. Our empirical experiments show that incorporating the augmented input text generated by LLM leads to \textit{improved} recommendation performance. Recommendation-driven and engagement-guided prompting strategies are found to elicit LLM's understanding of global and local item characteristics. This finding highlights the importance of leveraging diverse prompts and input augmentation techniques to enhance the recommendation capabilities with LLMs.
\end{abstract}

% Figure environment removed

\section{Introduction}
The use of large language models in recommender systems has garnered significant attention in recent research. Numerous studies have explored the direct use of LLMs as recommender models. The underlying principle of these approaches involves constructing prompts that encompass the recommendation task, user profiles, item attributes, and user-item interactions. These task-specific prompts are then presented as input to the LLM, which is instructed to predict the likelihood of interaction between a given user and item~\cite{dai2023uncovering, gao2023chat,geng2022recommendation,li2023exploring,liu2023chatgpt,zhang2023recommendation}.

While these works demonstrate the potential of LLMs as powerful recommender models, the focus primarily revolves around utilizing the LLM directly for recommendation purposes. However, in this study, we approach the problem from a different perspective. Rather than using LLMs as recommender models, this study delves into the exploration of prompting strategies to \textit{augment input text} with LLMs for personalized content recommendation. By leveraging LLMs, which have been fine-tuned on extensive language datasets, we seek to unlock their potential in generating high-quality and context-aware input text for enhanced recommendations.

Specifically, we propose \textbf{LLM-Rec} prompting, which encompasses various prompting strategies tailored for personalized content recommendation. These strategies include basic prompting, recommendation-driven prompting, engagement-guided prompting, and the combination of recommendation-driven and engagement-guided prompting. By leveraging these strategies, we aim to enhance the generation of input text by LLMs and improve the accuracy and relevance of content recommendations.

Through comprehensive empirical experiments, we evaluate the effectiveness of the \textbf{LLM-Rec} framework and compare it against baseline approaches. Our study provides insights into the impact of different prompting strategies on recommendation performance and sheds light on the potential of leveraging LLMs for personalized recommendation.



\section{LLM-Rec Prompting}
\subsection{Basic prompting}
We consider three basic prompting variants and refer to them as $\pmb{\tt p_{para}}$, $\pmb{\tt p_{tag}}$, and $\pmb{\tt p_{infer}}$, respectively in the following experiments. 
\begin{itemize}
    \item $\pmb{\tt p_{para}}$: This prompt instructs LLM to paraphrase the original content description, emphasizing the objective of maintaining the same information without introducing any additional details.
    \item $\pmb{\tt p_{tag}}$: This prompt instructs LLM to summarize the content description by using tags, aiming to generate a more concise overview that captures key information.
    \item $\pmb{\tt p_{infer}}$: This prompt instructs LLM to deduce the characteristics of the original content description and provide a categorical response that operates at a broader, less detailed level of granularity.
\end{itemize}
The exact prompts and corresponding responses by LLM are shown in Figure~\ref{fig:example_basic_rec} (upper).

\subsection{Recommendation-driven prompting}
This prompting strategy is to add a recommendation-driven instruction into the basic prompting. We refer to the three recommendation-driven prompting as $\pmb{\tt p_{para}^{rec}}$, $\pmb{\tt p_{tag}^{rec}}$, and $\pmb{\tt p_{infer}^{rec}}$, respectively in the following experiments, aligning with their counterparts in the basic prompts. The exact prompts and corresponding responses by LLM are shown in Figure~\ref{fig:example_basic_rec} (lower).

The use of recommendation-driven prompting exhibits several compelling characteristics, making it an appealing approach for generating high-quality content descriptions:

\begin{enumerate}
    \item \textbf{Enhanced Context:} By explicitly mentioning that the generated content description is intended for content recommendation, models gain a clearer understanding of the task at hand. This additional context helps models align their responses more closely with the purpose of generating content descriptions for recommendation purposes.
    \item \textbf{Guided Generation:} The specific instruction acts as a guiding cue for models, directing their attention towards generating content descriptions that are better suited for recommendation scenarios. The mention of ``content recommendation'' likely prompts LLM to focus on key features, relevant details, and aspects of the content that are more helpful in guiding users towards their preferred choices.
    \item \textbf{Improved Relevance:} The instruction aids LLM in generating content descriptions that are tailored to the requirements of content recommendation. This alignment with the recommendation task leads to more relevant and informative descriptions, as LLM is primed to emphasize aspects that are important for users seeking recommendations.
\end{enumerate}

% Figure environment removed


\subsection{Engagement-guided prompting}
This prompting strategy is to leverage user behavior (\ie user-item engagement) to design prompts with the intention to guide LLM to better capture the characteristics inside the content description that align with user preferences. We aim to generate more meaningful description with this type of prompts for recommendation tasks. We refer to this variant as $\pmb{\tt p^{eng}}$. 

To create the engagement-guided prompt, we combine the content description of the target item, denoted as $d_{target}$, with the content descriptions of $T$ \textbf{important} neighbor items, represented as $d_{1}, d_{2}, \cdots, d_{T}$. The importance is measured based on user engagement. We will discuss more details in the \textit{Experiment} section. This fusion of information forms the basis of the prompt, which is designed to leverage user engagement and preferences in generating more contextually relevant content descriptions:

{\it ``Summarize the commonalities among the following descriptions: `{$d_{target}$}'; `{$d_{1}$}; {$d_{2}$}; ... {$d_{T}$}'.''}

An engagement-guided prompt can assist the Language Model (LLM) in generating more useful content descriptions for content recommendation due to several reasons:
\begin{enumerate}
    \item \textbf{Contextual Relevance:} By incorporating information from both the target item and its important neighbor items, the prompt provides LLM with a broader context and a more comprehensive understanding of the content. This contextual information helps LLM generate descriptions that are more relevant to the specific item and its related items, thereby increasing their usefulness in content recommendation scenarios.
    \item  \textbf{User Preference Alignment:} Including the content descriptions of important neighbor items, which are determined based on user engagement, enables LLM to align with user preferences. By considering items that are frequently engaged by users, the generated content descriptions are more likely to capture the content characteristics and features that are appealing to the target users. This alignment enhances the usefulness of the generated descriptions in effectively recommending items that align with user preferences.
    \item \textbf{Enhanced Recommendation Quality:} The engagement-guided prompt leverages user engagement data to identify important neighbor items. By including information from these items in the prompt, LLM can potentially uncover meaningful connections, similarities, or relevant aspects between the target item and its neighbors. This can result in more accurate, informative, and high-quality content descriptions, thereby improving the overall performance of the content recommendation system.
\end{enumerate}

\subsection{Recommendation-driven + engagement-guided prompting}
This type of prompt intends to incorporate both the recommendation-driven and engagement-guided instructions (Figure~\ref{fig:llm-rec}), which we denote as $\pmb{p^{rec+eng}}$. The prompt is designed as following:

{\it ``The description of an item is as follows: `{$d_{target}$}'. What should I say if I want to recommend it to others? This content is considered to hold some similar attractive characteristics as the following descriptions: `{$d_{1}$}; {$d_{2}$}; ... {$d_{T}$}'.''}



\section{Experiment}

\subsection{Experiment Setup}
We investigate the four prompting strategies for large language models on two widely-used recommendation benchmarks. 




\begin{table}[t]
    \centering
     \caption{Statistics of the evaluation datasets.}
    \label{tab:dataset_stat}
    \begin{tabular}{llll}
    \toprule
Dataset      & \# Interaction & \# Item & \# User \\
\midrule
MovieLens-1M & 1,000,209      & 3,706   & 6,040   \\
Recipe       & 132,246        & 4,125   & 2,589  \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Benchmarks}
Two datasets are used to evaluate the effect of prompting strategies on input augmentation. Their statistics are shown in Table~\ref{tab:dataset_stat}.
\begin{enumerate}
    \item \textbf{MovieLens-1M}~\cite{harper2015movielens} is a highly recognized benchmark dataset commonly used for evaluating item recommendation systems. It contains a vast collection of 1,000,209 ratings provided by 6,040 MovieLens users, covering 3,900 movies. Each user has at least 20 ratings. Following  He~\etal~\cite{he2017neural}, we convert the rating data into implicit feedback. More specifically, each entry is marked as 0 or 1 indicating whether the user has rated the corresponding item. The original movie data only contain movie titles and genres. We employ GPT-3 ({\tt text-davinci-003}) to generate the content description of each movie using the following prompt:

    {\it ``Summarize the movie} \{{\tt title}\} {\it with one sentence. The answer cannot include the movie title.''}

    The response from GPT-3 is used as the content description. {\tt Temperature} is set at 0 to generate more focused and deterministic responses. 

    \item \textbf{Recipe}~\cite{majumder2019generating} is another benchmark dataset we use to assess the recommendation performance. This dataset consists of recipe details and reviews sourced from \url{Food.com}. The metadata includes ratings, reviews, recipe names, descriptions, ingredients, directions, and so on. For instance, an example recipe description is \textit{``all of the flavors of mac n' cheese in the form of a hot bowl of soup!''}. In our evaluation, we employ the recipe descriptions as item descriptions for the four prompting strategies. Similar to the \textbf{MovieLens-1M} dataset, we apply filtering criteria, excluding users with fewer than 20 ratings and items with fewer than 30 ratings.
\end{enumerate}



\subsubsection{Item module}
\begin{itemize}
    \item \textbf{Response generation:} In our evaluation, we focus on assessing the performance of \textbf{GPT-3}~\cite{brown2020language}, particularly the variant known as {\tt text-davinci-003}. This model is an advancement over the InstructGPT models~\cite{ouyang2022training}, incorporating several improvements. We specifically select this variant due to its ability to consistently generate high-quality writing, effectively handle complex instructions, and demonstrate enhanced proficiency in generating longer form content~\cite{raf2023davinci}.
    \item \textbf{Text encoder:} We use Sentence-BERT~\cite{reimers2019sentence} to derive the textual embeddings from the original content description and augmented text. The embedding model is {\tt all-MiniLM-L6-v2}. 
    \item \textbf{Importance measurement for engagement-guided prompting: } In our study, we show an example of use Personalized PageRank (PPR) score as the importance measurement. In particular, we first construct the user-item bipartite graph $G=(V, E)$. In this notation, $G$ represents the bipartite graph, $E$ denotes the set of nodes, and $E$ represents the set of edges. There are two types of nodes including users $V_{user} \subset V$ and items $V_{item} \subset V \:(V_{user} \cup V_{item} = V, V_{user} \cap V_{item} = \varnothing)$. An edge $e \in E$ between a user node $v_{user} \in V_{user}$ and an item node $v_{item} \in V_{item}$ is created if this user interacts with this item. 
    
    Next, we proceed by calculating the Personalized PageRank (PPR) score for each item node, which quantifies their relative importance from an individual node's perspective. For every item node, we construct a set of significant neighboring items. By identifying the top $T$ item nodes with the highest PPR scores that share the same genre as the target item node, we pinpoint essential neighbor items guided by user engagement. The rationale behind this approach lies in the observation that when users frequently engage with two items, there tends to be a greater similarity in terms of user preferences. By incorporating this information, we aim to capture user preferences more effectively, leading to enhanced performance in content recommendation. For both datasets, we set $T=3$.
\end{itemize}

\subsubsection{User module}
We employ an embedding table to convert user ID into latent representations. For both \textbf{MovieLens-1M} and \textbf{Recipe}, the output dimension is set at 128.


\subsubsection{Recommendation module}
In our study, we explore four recommendation modules. 

\begin{itemize}
    \item \textbf{ItemPop}: This method makes recommendation based on item popularity.
    \item \textbf{MLP}: This recommendation module is a combination of Multi-Layer Perceptron (MLP) and dot product. For simplicity, we refer to it as MLP. The augmented text embeddings and the original content description embeddings are combined by concatenation and then passed through a two-layer MLP. The first MLP layer's output dimension, as well as the input/output dimensions of the second MLP layer, are all set to 128. A ReLU activation function and a dropout layer are applied to the first MLP layer. Next, the dot product of the latent embeddings of the user and the item is calculated, and the resulting value is then passed through a Sigmoid function. This Sigmoid function transforms the dot product into a final relevance score between the user and the item.
    \item \textbf{AutoInt}~\cite{song2019autoint}: A multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space.
    \item \textbf{DCN-V2}~\cite{wang2021dcn}: \textbf{DCN}~\cite{wang2017deep} uses feature crossing explicitly at each layer. For our experiments, we employ the improved version of DCN, namely \textbf{DCN-V2}~\cite{wang2021dcn}.
\end{itemize}



\subsubsection{Model training.}
To train the model, we employ the Binary Cross Entropy Loss. Each user-item interaction is considered as a positive sample. Each user-item interaction within the dataset is treated as a positive sample. In addition to positive samples, we randomly select negative samples by pairing users and items that do not have any recorded interactions. To prevent overfitting and optimize training efficiency, we employ an early stop mechanism. It is worth noting that we have also explored the possibility of using the Bayesian Personalized Ranking (BPR) Loss~\cite{rendle2012bpr} within the framework. However, after experimentation, we find that the BPR Loss does not yield superior performance compared to the Binary Cross Entropy Loss. As a result, we choose to use the Binary Cross Entropy Loss as our primary loss function.


\subsubsection{Evaluation protocols.}
To assess the recommendation performance, we adopt the evaluation methodology employed by Wei~\etal~\cite{wei2019mmgcn}. Initially, we randomly divide the dataset into training, validation, and testing sets using an 8:1:1 ratio. Negative training samples are created using random negative sampling, as mentioned earlier. 

For the validation and testing sets, we pair each observed user-item interaction with 1,000 items that the user has not previously interacted with. It is important to note that there is \textit{no} overlap between the negative samples in the training set and the unobserved user-item pairs in the validation and testing sets. This ensures the independence of the evaluation data.

To evaluate the performance of top-K recommendations, we employ widely-used metrics such as Precision@K, Recall@K, and NDCG@K. In our case, we set $K=10$, indicating that we consider the top 10 recommendations. We report the average scores across five different splits of the testing sets, providing a comprehensive evaluation of the recommendation performance. 

\subsubsection{Hyper-parameter settings.}
We initialize the model parameters randomly, following a Gaussian distribution. To optimize the framework, we employ the AdamW algorithm~\cite{loshchilov2017decoupled} with a weight decay value of 0.0005. The hyper-parameter grids for the learning rate and dropout rate are discussed in the Appendix. Settings that achieve the highest Recall@K on the validation set are chosen for the evaluation on the testing set. 


\subsubsection{Implementation details.}
Our methods are implemented and experiments are conducted using PyTorch. The computation of PPR scores is facilitated by the use of the {\tt torch-ppr} library. The experiments are conducted on a NVIDIA A100 GPU with 80 GB of memory. Each experiment is run on one GPU at a time.

\section{Results}
\begin{table}[t]
    \centering
    \small
    \caption{\textbf{LLM-Rec empowers simple MLP models to achieve superior recommendation performance, surpassing other more complex feature-based recommendation methods.} The input feature for the MLP, AutoInt, and DCN-V2 models is the embeddings of the original content description. LLM-Rec in this table represents the MLP baseline whose input feature is the concatenation of the embeddings of the original content description and all responses generated by large language models via our proposed prompting strategies. Note that it is still just an MLP model.}
    \label{tab:sota_comp}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
    & \multicolumn{3}{c}{\textbf{Movielens-1M}}   & \multicolumn{3}{c}{\textbf{Recipe}}  \\ %\cline{2-13} 
      & Precision@10  & Recall@10      & NDCG@10       & Precision@10      & Recall@10    &NDCG@10         \\ \midrule
  ItemPop   & 0.0426      & 0.0428          & 0.0530   & 0.0116  &  0.0274 &  0.0201   \\
  MLP & 0.2914 & 0.2440 & 0.3626 & 0.0325 & 0.0684 & 0.0580\\
AutoInt                                                 & 0.2149   &  0.1706      & 0.2698  & 0.0351   & 0.0772 & 0.0658  \\
DCN-V2   & 0.2961     & 0.2433    & 0.3689         & 0.0360  & 0.0786  & 0.0653  \\

             \textbf{LLM-Rec}     & \textbf{0.3150}  &\textbf{0.2766} & \textbf{0.3951} & \textbf{0.0394}  & \textbf{0.0842} &  \textbf{0.0706} \\ 
             \bottomrule
\end{tabular}}
\end{table}

Table~\ref{tab:sota_comp} summarizes the recommendation performance of the baselines. Remarkably, LLM-Rec boosts simple MLP models to achieve superior recommendation performance, surpassing other more complex feature-based recommendation methods. 

To understand the effect of each prompting strategy, we design another experiment. Figure~\ref{fig:evaluate} shows the evaluation architecture. We keep the recommendation and user modules consistent across all experiments and only change the augmented text generated by our proposed prompting strategies. For each generated response, we first encode it and then concatenate the embeddings with the embeddings of the original content description. The responses generated from the basic prompting (\ie $\pmb{\tt p_{para}}$, $\pmb{\tt p_{tag}}$, $\pmb{\tt p_{infer}}$), recommendation-driven prompting (\ie $\pmb{\tt p_{para}^{rec}}$, $\pmb{\tt p_{tag}^{rec}}$, $\pmb{\tt p_{infer}^{rec}}$), engagement-guided prompting (\ie $\pmb{\tt p^{eng}}$), and recommendation-driven + engagemen guided prompting (\ie $\pmb{\tt p^{rec+eng}}$) are compared. $\pmb{CD}$ represents the original content description. No augmented text is introduced. The item input exclusively comprises the embeddings of the original content description.



% Figure environment removed


%         \item  $\pmb{p^{all}}$: It is the concatenation of the embeddings of $\pmb{CD}$ and the embeddings of the responses from $\pmb{\tt p_{para}}$, $\pmb{\tt p_{tag}}$, $\pmb{\tt p_{infer}}$, $\pmb{\tt p_{para}^{rec}}$, $\pmb{\tt p_{tag}^{rec}}$, $\pmb{\tt p_{infer}^{rec}}$, $\pmb{\tt p^{eng}}$, and $\pmb{\tt p^{rec+eng}}$.
%     \end{itemize}


% Figure environment removed

Table~\ref{fig:ablation_against_cd} shows the recommendation performance of each prompting strategy. Two key takeaways can be observed from the figure. Firstly, the combination of augmented text and the original content description leads to an improvement in recommendation performance. This finding suggests that all three types of prompting, namely basic, recommendation-driven, and engagement-guided, provide additional and valuable information for the recommendation module to effectively model content recommendation.


Secondly, the extent of this improvement may vary depending on the characteristics of the datasets used. To further investigate the reasons the recommendation performances vary across different prompting strategies, we conduct a case study comparing $\pmb{P_{para}}$ with $\pmb{P_{para}^{rec}}$. For both datasets, we choose three example item descriptions and the corresponding generated responses (Figure~\ref{fig:case_para}). The three examples selected are the top three items that the recommendation is correct based on the response of $\pmb{P_{para}^{rec}}$ while incorrect based on the response of $\pmb{P_{para}}$. We find that the most distinctive words in the response of $\pmb{P_{para}^{rec}}$ are the words that are related with user preferences. These words include the words that can express users' preferences about items such as {\tt exciting}, {\tt thought-provoking}, {\tt delicious}, and so on. We also discover words that are related to the well-defined concept in terms of user preferences such as genres (\eg {\tt classic}, {\tt action}, {\tt adventure}). We hypothesize that words generated with the recommendation-driven prompting strategy improve recommendation performance.

To validate this hypothesis, we design two variants of the response, namely $\pmb{P_{para}^{mask}}$ and $\pmb{P_{para}^{keyword}}$. To construct $\pmb{P_{para}^{mask}}$, we mask the words that appear in the response of $\pmb{P_{para}^{rec}}$ but are absent in the response of $\pmb{P_{para}}$. To construct $\pmb{P_{para}^{keyword}}$, we append the words that (1) appear in the response of $\pmb{P_{para}^{rec}}$ and (2) are well-defined user-preference-related words such as genres to the end of the response of $\pmb{P_{para}}$. These two variants of the responses are then fed into MLP models to form baselines. The recommendation performances are shown in Figure~\ref{fig:ablation_keyword}. Upon a comparative analysis of the performance between $\pmb{P_{para}^{rec}}$ and $\pmb{P_{para}^{mask}}$, a discernible decline in recommendation performance is observed when words unique to the response of $\pmb{P_{para}^{rec}}$ are selectively masked. This outcome underscores the pivotal role played by the supplementary insights introduced through the augmented text. Furthermore, our investigation reveals that the incorporation of vital keywords, as opposed to the inclusion of all response words, can yield even superior recommendation performance. This phenomenon may be attributed to potential discrepancies or extraneous elements within the response of $\pmb{P_{para}^{rec}}$.


% Figure environment removed


% Figure environment removed


When comparing $\pmb{p_{infer}}$ and $\pmb{p_{infer}^{rec}}$, which instruct LLM to deduce the characteristics of the original content description, a \textit{decrease in performance} is observed. This suggests that the reduced recommendation performance may stem from the discrepancy between the inferred context and the original context. In other words, the desired response from $\pmb{p_{infer}}$ requires inference \textit{beyond} the information provided in the original context, making it less effective. Conversely, recommendation-driven prompting proves beneficial for $\pmb{p_{para}}$ and $\pmb{p_{tag}}$, as these prompts do not rely on LLM inferring information beyond the original input.

When comparing $\pmb{p^{eng}}$ with $\pmb{CD}$, we observe improvements in recommendation performance. In a manner consistent with our preceding experiments, we curate exemplar responses from $\pmb{p^{eng}}$ for closer scrutiny (Figure~\ref{fig:case_eng}). Our analysis unveils a distinctive pattern where the response of $\pmb{p^{eng}}$ encompasses descriptive vocabulary intricately tied to user preferences. Remarkably, upon contrasting these lexicons with the terms embedded in the response of $\pmb{p_{para}^{rec}}$, a discernible refinement is evident. Notably, the composite response stemming from $\pmb{p^{rec+eng}}$, achieved through the synergy of recommendation-driven and engagement-guided prompting strategies, not only integrates user-preference-related words of a more universal nature but also embraces well-defined concepts, such as genres.


% Figure environment removed

To conduct a more in-depth exploration into the caliber of the combined augmented text, we engage in a process of concatenating the embeddings derived from diverse prompting strategies' responses. This concatenation is performed in various permutations, interwoven with the embeddings of the original content description. We then conduct the same experiment again, searching for hyper-parameters in the similar manner as discussed previously. The results are shown in Figure~\ref{fig:concat}. $\pmb{P^{all}}$ concatenates the embeddings of all responses and the original content description. Overall, concatenating more information helps improve recommendation performance. This finding emphasizes the added value of incorporating augmented text over using the original content description alone.


% Figure environment removed

Table~\ref{tab:concat} shows the recommendation performances of other concatenation variants:
\begin{itemize}
    \item \textbf{Randomizing embeddings}: We randomize the embeddings of $\pmb{P^{all}}$.
    \item \textbf{Duplicating $CD$ embeddings}: We concatenate multiple $CD$ embeddings to match the dimension of the embeddings of $\pmb{P^{all}}$.
    \item \textbf{Text concatenation}: Instead of concatenating the embeddings of all response, we concatenate the responses first, and then convert it to embeddings.
\end{itemize}

\begin{table}[t]
    \centering
    \small
    \caption{\textbf{Concatenating embeddings of the responses augmented by LLM-Rec outperforms concatenating randomized embeddings and concatenating duplicate $CD$ embeddings. It also achieves a superior performance than concatenating the raw text.} }
    \label{tab:concat}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccccc}
    \toprule
    & \multicolumn{3}{c}{\textbf{Movielens-1M}}   & \multicolumn{3}{c}{\textbf{Recipe}}  \\ %\cline{2-13} 
      & Precision@10  & Recall@10      & NDCG@10       & Precision@10      & Recall@10    &NDCG@10         \\ \midrule
      $CD$ & 0.2914 & 0.2440 & 0.3626 & 0.0325 & 0.0684 & 0.0580\\
  Randomizing embeddings   & 1e-4& 8.85e-5&3e-4   & 0.0  &  0.0 &  0.0   \\
  Duplicating $CD$ embeddings & 0.2858&0.2417&0.3567 & 0.0327&0.0694&0.0590\\
  Text concatenation & 0.3038 & 0.2615 & 0.3796 & 0.0332 & 0.0714 & 0.0591 \\

             $\pmb{P^{all}}$     & \textbf{0.3126}  &\textbf{0.2731} & \textbf{0.3932} & \textbf{0.0394}  & \textbf{0.0842} &  \textbf{0.0706} \\ 
             \bottomrule
\end{tabular}}
\end{table}



\section{Discussion}
In this study, we have investigated the effectiveness of \textbf{LLM-Rec} prompting as a straightforward yet impactful mechanism for improving personalized recommendation through large language models. Our findings reveal several key insights.

Firstly, we demonstrate that by combining augmented text with the original content description, we observe a significant enhancement in recommendation performance. It also empowers simple models such as MLPs to achieve superior recommendation performance than other more complex feature-based methods. This highlights the value of incorporating additional context to facilitate more accurate and relevant recommendations, coupled with an easier training process.

Furthermore, our experimental results on recommendation-driven and engagement-guided prompting strategies illustrate their ability to encourage the large language model to generate high-quality input text specifically tailored for recommendation purposes. These prompting strategies effectively leverage recommendation goals and user engagement signals to guide the model towards producing more desirable recommendations. More specifically, the recommendation-driven prompting strategy engenders a spectrum of broader user-preference-associated terms, including well-established concepts. This phenomenon signifies its adeptness at tapping into the global comprehension of the recommendation objective concerning the specific item to be suggested. On the other hand, the engagement-guided prompting strategy, integrating more immediate user co-engagement signals, encapsulates the capacity of LLMs to grasp nuanced, finely detailed, and localized insights about the item to be recommended.


Lastly, by combining all augmented text, we achieve the best overall recommendation performance. This suggests the complementary nature of these strategies and their collective impact in further improving recommendation quality.

Overall, our study showcases the effectiveness of \textbf{LLM-Rec} prompting in facilitating large language models to generate enhanced and relevant input text for personalized recommendation. These findings contribute to the advancement of recommendation systems, emphasizing the significance of thoughtful prompt design to enhance recommendation performance.

Throughout our experimental analysis, we also uncover a potential limitation when employing LLM for augmenting input text in recommendation systems. We notice a distinction between prompts that solely instruct LLM to modify the content description and those that prompt LLM to infer additional information. In the latter case, where inference beyond the original context is required, the recommendation-driven prompting strategy may not yield the expected benefits. Our hypothesis suggests that the quality, specifically in terms of recommendation relevance, of the inferred context might have an unknown impact on the overall recommendation performance.

This observation emphasizes the need for careful consideration and evaluation of the prompts employed, particularly when instructing LLM to infer information beyond the provided context. While recommendation-driven prompting strategies prove effective for prompts that do not necessitate inference, their effectiveness may be hindered when the prompts require LLM to extrapolate information. Further research is necessary to explore techniques for managing and improving the quality and impact of inferred context on recommendation outcomes.

In addition to its superior performance in personalized content recommendation, the incorporation of engagement signals in prompt designs may have broader associated benefits. The engagement-guided prompting strategy instructs the LLM to generate commonalities among different items, resembling the concept of neighborhood aggregation in Graph Neural Network (GNN) learning. In GNN, each target node is partially learned by aggregating information from its neighboring nodes. In this context, we highlight the potential of using engagement-guided prompts as a means to replace the learning process of GNN, thereby simplifying the overall model architecture. 

Furthermore, leveraging the fine-tuned LLM opens up possibilities for zero-shot generation without incurring any additional learning cost. Since the LLM has already undergone training to capture linguistic patterns and semantic understanding, it can be harnessed to generate responses or recommendations in unseen scenarios without requiring further training. This zero-shot generation capability enables flexibility and scalability in recommendation systems, allowing for efficient adaptation to new domains or contexts.

The combination of engagement-guided prompting and the zero-shot generation potential of the fine-tuned LLM presents promising opportunities for streamlining model architectures, reducing computational complexity, and expanding the applicability of recommendation systems. Further exploration and investigation in this direction could unlock novel techniques for efficient and effective personalized recommendation.

\section{Related Work}

In addition to leveraging LLMs directly as recommenders, there have been efforts to use LLMs for augmenting the input side of personalized recommendation. For instance, Chen~\etal~\cite{chen2023palr} incorporated user history behaviors, such as clicks, purchases, and ratings, into LLMs to generate user profiles. These profiles were then combined with the history interaction sequence and candidate items to construct the final recommendation prompt. LLMs were subsequently employed to predict the likelihood of user-item interaction based on this prompt. Xi~\etal~\cite{xi2023towards} introduced a method that leverages the reasoning knowledge of LLMs regarding user preferences and the factual knowledge of LLMs about items. However, our study takes a different approach, focusing specifically on input augmentation for items with LLMs' reasoning ability. By employing prompting strategies, we aim to generate augmented input text that better captures the characteristics and nuances of items, leading to improved personalized recommendations. 




\section{Conclusions}
In this study, we introduce \textbf{LLM-Rec} prompting strategies, which leverage large language models (LLMs) for input augmentation, aiming to enhance personalized recommendation. Through rigorous experimentation across four variants of \textbf{LLM-Rec}, we observe that the combination of augmented input text and original content descriptions yields notable improvements in recommendation performance.

These findings emphasize the potential of using LLMs and strategic prompting techniques to enhance the accuracy and relevance of personalized recommendation with an easier training process. By incorporating additional context through augmented text, we enable the recommendation algorithms to capture more nuanced information and generate recommendations that better align with user preferences.

The experimental results of \textbf{LLM-Rec} highlights the importance of innovative approaches in leveraging LLMs for content recommendation and showcases the value of input augmentation in improving recommendation performance. As personalized recommendation continues to play a pivotal role in various domains, our study provides insights into effective strategies for leveraging LLMs to deliver enhanced recommendation experiences.



\bibliography{report} 
\bibliographystyle{plain}

\appendix


\section{Supplemental Material}
\subsection{Prompts}

\begin{itemize}
    \item $\pmb{p_{1}}$: \quad The description of an item is as follows: `\{item description\}', paraphrase it.
    \item $\pmb{p_{1}^{rec}}$: \quad The description of an item is as follows: `\{item description\}', what else should I say if I want to recommend it to others? 
    \item $\pmb{p_{2}}$: \quad The description of an item is as follows: `\{item description\}', summarize it with tags.
    \item $\pmb{p_{2}^{rec}}$: \quad The description of an item is as follows: `\{item description\}', what tags should I use if I want to recommend it to others?
    \item $\pmb{p_{3}}$: \quad The description of an item is as follows: `\{item description\}', what kind of emotions can it evoke?
    \item $\pmb{p_{3}^{rec}}$: \quad The description of an item is as follows: `\{item description\}', recommend it to others with a focus on the emotions it can evoke.
    \item $\pmb{p^{eng}}$: \quad Summarize the commonalities among the following descriptions: `\{item description\}'; `\{descriptions of other important neighbors\}'.
    \item $\pmb{p^{rec+eng}}$: \quad The description of an item is as follows: `\{item description\}'. What else should I say if I want to recommend it to others? This content is considered to hold some similar attractive characteristics as the following descriptions: `\{descriptions of other important neighbors\}'.
\end{itemize}



\subsection{Hyper-parameters}
For the MLP model, the hyper-parameter grids for the learning rate and dropout rate are $\{0.0001, 0.0005, 0.001\}$ and $\{0.1, 0.3, 0.5\}$, respectively. For AutoInt, the hyper-parameter grids for the learning rate, dropout rate, hidden layer size, number of attention layers, and attention heads are $\{0.001, 0.005, 0.01\}$, $\{0.1, 0.3, 0.5\}$, $\{16, 32, 64, 128\}$, $\{1, 2\}$, and $\{1, 2\}$, respectively. For DCN-V2, the learning rate, dropout rate, hidden layer size, and number of cross layers are searched in $\{0.001, 0.005, 0.01\}$, $\{0.1, 0.3, 0.5\}$, $\{16, 32, 64, 128\}$, $\{1, 2\}$, and $\{1, 2\}$, respectively. The performance is evaluated every five epochs, and the early stop mechanism is configured to have a patience of 5. Additionally, we set the batch size to 4096 for all baselines except for AutoInt which is 1024 due to the memory limitation.


\end{document}


