\section{Additional Details of Experiment Setup}\label{sec:detailed_exp_setting}





\subsection{Datasets}\label{sec:appendix_dataset}


\input{tab_fig/dataset_stats_table}

\textbf{MovieLens-1M}~\citep{harper2015movielens} is a highly recognized benchmark dataset commonly used for evaluating item recommendation systems.\footnote{License: \url{https://files.grouplens.org/datasets/movielens/ml-1m-README.txt}} It contains a vast collection of 1,000,209 ratings provided by 6,040 MovieLens users, covering 3,900 movies. Each user has at least 20 ratings. Following  \citet{he2017neural}, we convert the rating data into implicit feedback. More specifically, each entry is marked as 0 or 1 indicating whether the user has rated the corresponding item. The original movie data only contain movie titles and genres. We employ {\sc GPT-3} ({\tt text-davinci-003}) to generate the content description of each movie using the following prompt: {\it ``Summarize the movie} \{{\tt title}\} {\it with one sentence. The answer cannot include the movie title.''} The response from {\sc GPT-3} is used as the item description. {\tt Temperature} is set at 0 to generate more focused and deterministic responses. Note that inclusion of the movie title is entirely \textit{optional}. We opt not to include the title intentionally, as our design for \model emphasizes its role as a general prompting framework. This versatility is important, as it is intended to function across a wide array of item types, including those that may not possess pre-defined titles, such as short videos. 

\textbf{Recipe}~\citep{majumder2019generating} is another benchmark dataset we use to assess the recommendation performance. This dataset consists of recipe details and reviews sourced from \url{Food.com}.\footnote{License: \url{https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions}} The metadata includes ratings, reviews, recipe names, descriptions, ingredients, directions, and so on. For instance, an example recipe description is \textit{``All the delicious flavors of mac n' cheese transformed into a warm, comforting bowl of soup!''}. In our evaluation, we employ the recipe descriptions as item descriptions for the four prompting strategies. Similar to the MovieLens-1M dataset, we apply filtering criteria, excluding users with fewer than 20 ratings and items with fewer than 30 ratings. Note that all original descriptions presented as examples in this paper have been paraphrased to protect user privacy.

The selection of these benchmarks is mainly motivated by two factors. First, we select movies and recipes as they represent two distinct types of content. Movies, being more categorically organized and widely researched, contrast sharply with recipes, which are diverse, user-generated content from social media platforms, often lacking a strict categorical structure and presenting more novelty. Second, the nature of their descriptions differs significantly: movie descriptions typically comprise narrative summaries, whereas recipe descriptions are instructional. Evaluating our model on these varied datasets allows for a comprehensive analysis of how different prompting strategies affect recommendation outcomes, providing valuable insights into their effectiveness across diverse content types.

\subsection{Baselines}\label{sec:baselines}
To assess \model's efficacy, we compare it against two distinct categories of baselines. The first category includes baselines that takes solely the original item descriptions as input. This includes models from MLP to more complex content-based approaches. Specifically, we choose three more advanced, content-based recommendation models. AutoInt is a multi-head self-attentive neural network with residual connections designed to explicitly model feature interactions within a low-dimensional space~\citep{song2019autoint}. DCN-V2 represents an enhanced version of DCN and incorporates feature crossing at each layer~\citep{wang2021dcn,wang2017deep}. Lastly, EDCN~\citep{chen2021enhancing} introduces a bridge module and a regulation module to collaboratively capture layer-wise interactive signals and learn discriminative feature distributions for each hidden layer in parallel networks, such as DCN. The purpose of this comparison is to evaluate the added value of augmented text in improving recommendation outcomes. 

The second category includes different text augmentation methodologies. Here, \model is evaluated against two recent advancements in the field of using LLMs to augment item information. The first method is Knowledge Augmented Recommendation (KAR) as proposed by \citet{xi2023towards}. KAR introduces a fusion of domain knowledge and prompt engineering to generate factual knowledge pertaining to the items (for implementation details, see Appendix~\ref{appendix:kar_details}) In contrast to KAR's approach, \model places a particular emphasis on the innate common-sense reasoning capabilities of large language models and notably does not mandate domain expertise. Since the augmented information may not necessarily be correct, we further implement a variant with ground truth knowledge. It aligns with strategies akin to those introduced by \citet{di2012linked}, who harnessed external databases to enhance item information. The second method, TagGPT, proposed by \citet{li2023taggpt}, extracts tags using LLMs, similar to one of our prompting strategies for item descriptions.


Although collaborative Filtering (CF) is another widely used technique in recommendation systems, given our primary focus on addressing the issue of incomplete item descriptions, we do not conduct experiments under CF settings~\citep{li2023exploring}. Instead, we concentrate on comparing our method with other input augmentation approaches.


\subsection{Item and User Modules}\label{sec:item_and_user_module}
We use Sentence-BERT~\citep{reimers2019sentence} to derive the textual embeddings from the original item description and augmented text. The embedding model is {\tt all-MiniLM-L6-v2}. 
We directly apply it to convert the natural language to embeddings without fine-tuning it, freezing only the text-encoder and not the user encoder to avoid the additional computational cost associated with training and fine-tuning the sentence transformer model.
For users, we employ an embedding table to convert user ID into latent representations. For both MovieLens-1M and Recipe datasets, the output dimension is set at 128. 
We have considered using embeddings derived from the LLMs. However, as our goal is to propose a general framework that can leverage both open-source and proprietary models, we do not pursue further exploration of this aspect in the current study.





\subsection{Model Training}\label{sec:model_training}
To facilitate the model training process, we employ the binary cross-entropy loss, expressed as:
\begin{align}
\begin{split}
L= & -\sum_{(u, i) \in Y} [y_{u,i}\cdot \log \hat{y}_{u, i} + \\ & (1 - y_{u, i}) \cdot \log (1-\hat{y}_{u, i})]
\end{split}
\end{align}
where $(u, i)$ represents the user-item pair, and $Y$ denotes the set that contains all positive and negative samples. The variable $y_{u,i}$ serves as a label, with a value of 1 indicating that user $u$ has engaged with item $i$, and 0 representing the absence of interaction. The prediction score $\hat{y}_{u,i}$, ranging from 0 to 1, reflects the likelihood of user $u$ interacting with item $i$. In our dataset, each instance of user-item interaction is considered a positive sample. Alongside these positive samples, we incorporate negative samples by randomly pairing users with items that lack any prior recorded interactions. To mitigate the risk of overfitting and enhance training efficiency, we implement an early stopping mechanism. The window size and evaluation frequency are both configured to be 5. It is noteworthy that we have also explored the viability of employing the Bayesian Personalized Ranking (BPR) Loss~\citep{rendle2012bpr} within our framework. However, subsequent experimentation reveals that the BPR Loss does not offer superior performance when compared to the binary cross-entropy loss. Consequently, we opt to use the binary cross-entropy loss as our loss function.




\subsection{Hyper-parameter Settings}\label{sec:hyperparameters}
\textbf{Large Language Models.} We perform experiments with two large language models. For {\sc GPT-3} ({\tt text-davinci-003}), {\tt temperature}, {\tt max\_token}, {\tt top\_p}, {\tt frequency penalty}, and {\tt presence penalty} are set as 0, 512, 1, 0.0, and 0.6, respectively. For {\sc Llama-2} (7B {\sc Llama-2-Chat}), we set {\tt do\_sample} to be {\tt True}, {\tt top\_k} 10, and the {\tt num\_return\_sequences} 1. {\sc Llama-2}'s generation is conducted on 8 NVIDIA GeForce RTX 2080 Ti GPUs, each equipped with 11 GB of memory.



\noindent\textbf{Recommendation Modules} We initialize the model parameters randomly, following a Gaussian distribution. To optimize the framework, we employ the AdamW algorithm~\cite{loshchilov2017decoupled} with a weight decay value of 0.0005. For the MLP model, the hyper-parameter grids for the learning rate and dropout rate are $\{0.0001, 0.0005, 0.001\}$ and $\{0.1, 0.3, 0.5\}$, respectively. For AutoInt~\citep{song2019autoint}, the hyper-parameter grids for the learning rate, dropout rate, hidden layer size, number of attention layers, and attention heads are $\{0.001, 0.005, 0.01\}$, $\{0.1, 0.3, 0.5\}$, $\{16, 32, 64, 128\}$, $\{1, 2\}$, and $\{1, 2\}$, respectively. For DCN-V2~\citep{wang2021dcn}, the learning rate, dropout rate, hidden layer size, and number of cross layers are searched in $\{0.001, 0.005, 0.01\}$, $\{0.1, 0.3, 0.5\}$, $\{16, 32, 64, 128\}$, $\{1, 2\}$, and $\{1, 2\}$, respectively. Since the network structure of EDCN~\citep{chen2021enhancing} is similar with DCN-V2~\citep{wang2021dcn}, we apply the hyper-parameter settings of DCN-V2 to EDCN. The performance is evaluated every five epochs, and the early stop mechanism is configured to have a patience of 5. We set the batch size to 4096 for all baselines except for AutoInt which is 1024 due to the memory limitation. Settings that achieve the highest Recall@K on the validation set are chosen for the evaluation on the testing set. 

\subsection{Importance Measurement for Engagement-guided Prompting}\label{sec:importance}
In our study, we show an example of using Personalized PageRank (PPR)~\citep{brin1998pagerank} score as the metric to find the important neighbor items. PPR is a widely employed technique for finding significant neighbors in recommendation systems~\citep{10.1145/3219819.3219890}. In particular, we first construct the user-item bipartite graph $G=(V, E)$. In this notation, $G$ represents the bipartite graph, $E$ denotes the set of nodes, and $E$ represents the set of edges. There are two types of nodes including users $V_{user} \subset V$ and items $V_{item} \subset V \:(V_{user} \cup V_{item} = V, V_{user} \cap V_{item} = \varnothing)$. An edge $e \in E$ between a user node $v \in V_{user}$ and an item node $i \in V_{item}$ is created if this user interacts with this item. 

Next, we proceed by calculating the Personalized PageRank (PPR) score for each item node. The PPR value $\pi(s, t)$, where $s$ is the source node and $t$ is the target node, signifies the probability that a random walk initiated from node $s$ concludes at node $t$. This value offers a quantified measure of their relative importance from the standpoint of an individual node~\citep{lofgren2015efficient}. For every item node, we construct a set of significant neighboring items. By identifying the top $T$ item nodes with the highest PPR scores, we pinpoint important neighbor items guided by user engagement. The rationale behind this approach lies in the observation that when users frequently engage with two items, there tends to be a greater similarity between these two items through the lens of user preferences. By incorporating this information, we aim to capture user preferences more effectively, leading to enhanced performance in content recommendation. For both datasets, we set $T=3$. For the Movielens-1M dataset, we find the important neighbor items that share the same genre as the target item.

\subsection{Implementation Details}\label{appendix:implementation}

Our methods are implemented and experiments are conducted using PyTorch. The computation of PPR scores is facilitated by the use of the {\tt torch-ppr} library. Each experiment is run on one NVIDIA A100 GPU with 80 GB of memory at a time. Further, we adapt the codes of the DeepCTR\footnote{\url{https://github.com/shenweichen/DeepCTR}} and DeepCTR-Torch\footnote{\url{https://github.com/shenweichen/DeepCTR-Torch}} repositories to implement AutoInt~\citep{song2019autoint}, DCN-V2~\citep{wang2021dcn}, and EDCN~\citep{chen2021enhancing}. Table~\ref{tab:tab_all_prompt} summarizes all prompts of \model.

\input{tab_fig/tab_all_prompts}

\subsection{KAR Augmentation Details}\label{appendix:kar_details}

In KAR, \citet{xi2023towards} applied a specific prompt to elicit factual knowledge about movies of the Movielens-1M dataset~\citep{harper2015movielens}. The prompt instructed the model to: \textit{``Introduce movie {\tt \{item description\}} and describe its attributes precisely (including but not limited to {\tt scenario-specific factors})''}. In their study, the {\tt item description} was the movie titles. Human experts were enlisted to refine the answers generated by LLMs in response to the question: \textit{``List the importance factors or features that determine whether a user will be interested in a movie.''} These refined factors were then considered as the {\tt \{scenario-specific factors\}}, including \textit{genre, actors, directors, theme, mood, production quality, and critical acclaim}. Because the responses generated using these prompts were not publicly released, we re-implement the same methodology, employing LLMs to generate the factual knowledge of items. In the case of the Recipe dataset~\citep{majumder2019generating}, we use recipe description as the {\tt item description}. The same approach is then adopted to identify {\tt scenario-specific factors}. Initially, the prompt is adapted to: \textit{``List the importance factors or features that determine whether a user will be interested in a recipe.''} Subsequently, the answers generated by {\sc ChatGPT} are validated (see Table~\ref{tab:appendix_KAR_factors}). The resulting set of {\tt scenario-specific factors} for Recipe comprises a diverse range of attributes, including \textit{dietary preferences, ingredients, cuisine type, cooking time, nutritional value, allergies, taste preferences, skill level, occasion, cost, health and wellness goals, food allure, reviews and ratings, cooking equipment, personal experience, season and weather, cultural or ethical considerations, creativity and variety, simplicity, popularity and trends}. These prompts are then employed to enrich the factual knowledge of both movies and recipes using {\sc GPT-3} ({\tt text-davinci-003}). For illustrative examples of the responses, please refer to Table~\ref{tab:appendix_KAR_examples}. KAR is also composed of a preference reasoning prompt for user information augmentation. Since we only focus on the item side, only the item factual prompt is implemented to examine how different focuses on LLMs' ability between \model and KAR affect recommendation performance.




\subsection{Keywords Construction}\label{appendix:keyword_construction}
The keyword generation process differs between the Movielens-1M and Recipe datasets. For Movielens-1M, the keywords are derived from genre labels, which are intrinsic components of the dataset's metadata. In the case of Recipe, the process involves multiple steps. Initially, we compile a list of unique words found in the responses generated through the recommendation-driven prompting strategy. Subsequently, we filter out stopwords and proceed to construct unigrams and bigrams using the {\tt NLTK} package. Following this, a manual review is conducted to identify phrases that appear at least five times in the corpus. These phrases are then scrutinized to determine whether they contain words relevant for categorizing recipes. The final list of keywords for Recipe contain ``easy'', ``homemade'', ``baking'', ``health'', ``healthy'', ``dessert'', and ``dinner''. These keywords collectively serve as indicative descriptors for recipes within the dataset.


\input{tab_fig/appendix_KAR_factors}
\input{tab_fig/appendix_KAR_example}

\clearpage