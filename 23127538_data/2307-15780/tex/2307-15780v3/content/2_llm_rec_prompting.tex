\section{LLM-Rec}

When composing a summary for recommendation purposes, it is customary to infuse it with specific emphases grounded in the author's \textit{comprehension} of the movie. This might involve accentuating the movie's distinctive attributes that set it apart from other movies. For instance, one may opt to incorporate genre information as a crucial element for classifying the movie. However, the decision to leverage the concept of genre for enhancing the summary is predicated on the author's understanding that the genre is a meaningful construct, effectively aligning the summary with the preferences and expectations of the intended audience. This paper aims to explore the potential of large language models when prompted to generate informative item descriptions and subsequently how to leverage this augmented text for enhancing personalized recommendations. Figure~\ref{fig:llm-rec} shows the diagram of \model. 
Specifically, our study focuses on investigating \textit{four} distinct LLM prompting strategies for description enrichment, namely basic prompting, recommendation-driven prompting, engagement-guided prompting, and the combination of recommendation-driven and engagement-guided prompting. The enriched text is then fed into the final recommendation module.


\noindent\textbf{Basic Prompting.} The concept of basic prompting closely resembles the task of crafting a general movie summary. Within this scope, we consider three basic prompting variants and refer to them as $p_{para}$, $p_{tag}$, and $p_{infer}$, respectively in the following experiments. $p_{para}$ instructs LLMs to paraphrase the original item description, emphasizing the objective of maintaining the same information without introducing any additional details. Given the original content description, the prompt we use is {\it ``The description of an item is as follows} `\{{\tt description}\}', {\it paraphrase it.''} $p_{tag}$ aims to guide LLMs to summarize the content description by using tags, striving to generate a more concise overview that captures key information. The corresponding prompt is  \textit{``The description of an item is as follows} `\{{\tt description}\}', {\it summarize it with tags.''} $p_{infer}$ instructs LLMs to deduce the characteristics of the original content description and provide a categorical response that operates at a broader, less detailed level of granularity. We use the following prompt in the experiments: \textit{``The description of an item is as follows} `\{{\tt description}\}', {\it what kind of emotions can it evoke?''} 

\noindent\textbf{Recommendation-driven Prompting.} This prompting strategy is to add a recommendation-driven instruction, into the basic prompting, resembling the task of creating a paragraph intended for making recommendations. We refer to the three recommendation-driven prompting as $p_{para}^{rec}$, $p_{tag}^{rec}$, and $p_{infer}^{rec}$, respectively in the following experiments, aligning with their counterparts in the basic prompting strategy. $p_{para}^{rec}$ represents the prompt: \textit{``The description of an item is as follows} `\{{\tt description}\}', {\it \purpletext{what else should I say if I want to recommend it to others?}''} The prompt for $p_{tag}^{rec}$ is \textit{``The description of an item is as follows} `\{{\tt description}\}', {\it what tags should I use \purpletext{if I want to recommend it to others?}''} The prompt for $p_{infer}^{rec}$ is \textit{``The description of an item is as follows} `\{{\tt description}\}', {\it \purpletext{recommend it to others} with a focus on the emotions it can evoke.''} 

\noindent\textbf{Engagement-guided Prompting.} As previously elucidated, the deficiency in item descriptions can also emanate from a limited comprehension of the user cohort for whom the recommendations are being generated. Typically, item descriptions are initially formulated for broad, general purposes, devoid of specific targeting toward particular user groups. As a result, they often fall short in capturing the intricate nuances of items required for a more fine-grained alignment with individual user preferences. The goal of the engagement-guided prompting strategy is to leverage user behavior, specifically the interactions between users and items (\ie user-item engagement) to devise prompts with the intention to steer LLMs towards a more precise comprehension of the attributes within the items, thus generating more insightful and contextually relevant descriptions that align more closely with the preferences of intended users. We refer to this variant as $p^{eng}$. To create the engagement-guided prompt, we combine the description of the target item, denoted as $d_{target}$, with the descriptions of $T$ \textbf{important} neighbor items, represented as \greentext{$d_{1}, d_{2}, \cdots, d_{T}$}. The importance is measured based on \textbf{user engagement}. More details can be found in Appendix~\ref{sec:importance}. The exact prompt of this prompting strategy is \textit{``\greentext{Summarize the commonalities among the following descriptions:}} ‘\{{\tt description}\}’; \greentext{‘\{{\tt descriptions of other important neighbors}\}’.}{\it''}


\noindent\textbf{Recommendation-driven + Engagement-guided Prompting.} It intends to incorporate both the recommendation-driven and engagement-guided instructions, which we denote as $p^{rec+eng}$: \textit{``The description of an item is as follows:} ‘\{{\tt description}\}’. {\it \purpletext{What else should I say if I want to recommend it to others?} This content is considered to \greentext{hold some similar attractive characteristics as the following descriptions:}} \greentext{‘\{{\tt descriptions of other important neighbors}\}’.}{\it''}   %% ok

\noindent\textbf{How does \mbox{\sc \textbf{LLM-Rec}}\xspace affect personalized recommendation?} 
In our experiments, we discover that first and foremost, \model stands out as a versatile yet simple framework, largely unrestricted by the type of items. Our experimental results on two datasets including the items that are categorically structured and extensively studied to items that are relatively novel and unclassified such as user-generated content, consistently demonstrate the substantial improvement in personalized recommendations. Simple models, such as MLP, can achieve performance on par with, or even better than, more advanced and complex models with the augmented text. This finding underscores the potential of simplified training to address challenges due to more complex models. More importantly, compared to other knowledge-based text augmentation methods, \model achieves superior recommendation performances and requires considerably less domain expertise compared to prior studies, making it much more accessible for implementation.
 
Second, \model contributes to increased recommendation transparency and explainability. The ability to directly investigate the augmented text not only enhances our understanding of the recommendation models but also offers insights into the characteristics of the items. It is invaluable for both users and system designers seeking to comprehend the rationale behind recommendations.


