\section{Experiments}\label{sec:exp}

\input{tab_fig/main_table}
\subsection{Experiment Setup}\label{sec:exp_setup}

\noindent\textbf{Datasets and Baslines.} Two widely adopted recommendation benchmarks are used, Movielens-1M~\citep{harper2015movielens} for movie recommendation, and Recipe~\citep{majumder2019generating} for recipe recommendation.  
To assess \model's efficacy, we compare it against two distinct categories of baselines. The first category includes content-based baselines that takes solely the original item descriptions as input. The second category includes different text augmentation methods.  Details including dataset statistics, preprocessing specifics, baselines, model training, hyper-parameter settings and implementation are discussed in Appendix~\ref{sec:detailed_exp_setting}. 

\noindent\textbf{Language Models.} Two large language models are selected for experiments. The first is {\sc GPT-3}~\citep{brown2020language}, particularly its variant {\tt text-davinci-003}. This variant is an advancement over the InstructGPT models~\citep{ouyang2022training}. We select this variant due to its ability to consistently generate high-quality writing, effectively handle complex instructions, and demonstrate enhanced proficiency in generating longer form content~\citep{raf2023davinci}. The second is {\sc Llama-2}~\citep{touvron2023llama2}, which is an open-sourced model that has shown superior performance across various external benchmarks in reasoning, coding, proficiency, and knowledge tests. Specifically, we use the {\sc Llama-2-Chat} variant of 7B parameters.


\noindent\textbf{Evaluation Protocols.} %To assess the recommendation performance, 
We follow the same evaluation methodology of \citet{wei2019mmgcn}. We randomly divide the dataset into training, validation, and test sets using an 8:1:1 ratio. Negative training samples are created by pairing users and items without any recorded interactions (note that these are pseudo-negative samples). For the validation and test sets, we pair each observed user-item interaction with $n$ items that the user has not previously interacted with. Here we follow the methodology outlined in the previous study~\cite{wei2019mmgcn} and set $n$ to $1,000$. It is important to note that there is \textit{no} overlap between the negative samples in the training set and the unobserved user-item pairs in the validation and test sets. This ensures the independence of the evaluation data. We use metrics such as Precision@K, Recall@K and NDCG@K to evaluate the performance of top-K recommendations, where $K$ is set to $10$. We report the average scores across five different splits of the testing sets. The recommendation module of \model is the combination of an MLP model and a dot product.






















