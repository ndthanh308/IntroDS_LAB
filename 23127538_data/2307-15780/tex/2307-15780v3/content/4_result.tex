
\subsection{Main Results}\label{sec:main_results}

\textbf{Integrating the text augmented by \mbox{\sc \textbf{LLM-Rec}}\xspace boosts recommendation performance.} Table~\ref{tab:sota_comp} shows the average recommendation performance between \model and baseline approaches across five different splits. In addition to the aforementioned baselines, we include another baseline relies only on item popularity and does not involve any learning process, referred to as Item Popularity. Note that \model uses the same recommendation module as the MLP baseline. It can be seen that \model exhibits significant relative gains over the MLP method. For instance, \model achieves improvements in NDCG@10 from 6.24\% to 8.54\% in Movielens-1M and from 8.97\% to 21.72\% in Recipe. These improvements highlight the value of the augmented text in the recommendation process. More importantly, when comparing \model with the other three content-based methods, we observe that \model empowers simple MLP models to attain comparable or even superior performance, surpassing other more complex feature-based recommendation methods.

\input{tab_fig/case_para}

\noindent\textbf{What extra information does \mbox{\sc \textbf{LLM-Rec}}\xspace incorporate that contributes to its performance improvement?}
To better understand why \model is effective, particularly regarding the additional information contributed by its prompting approach, we conduct both qualitative and quantitative studies. We find that the augmented content contains more detailed and expressive descriptions, emphasizing item characteristics which helps in understanding items more comprehensively than with their original descriptions and contributing to the performance improvement. Figure~\ref{fig:case_para} shows example responses generated by {\sc GPT-3} with $p_{para}$ and $p_{para}^{rec}$. The first example suggests that the response via $p_{para}^{rec}$ categorizes the movie as a \purpletext{psychological thriller} and recommends it as a \purpletext{must-watch} for fans of this genre. It also positions the movie as both \purpletext{exciting} and \purpletext{thought-provoking}, appealing to those looking for more than just \purpletext{entertainment}. These distinctive words describe user preferences and item characteristics including genre description, descriptive elements, and viewer recommendation. While Figure~\ref{fig:case_para} might suggest that the LLM-generated text for the Recipe dataset adds only modifiers, these phrases, like ``easy to make,'' actually reflect key characteristics valued in the Recipe dataset, such as simplicity. Some authors may also add \#easytomake to their recipe descriptions~\cite{majumder2019generating}. Consistent patterns are also observed when comparing the responses of $p_{tag}$ with $p_{tag}^{rec}$ (Tables~\ref{tab:example_gpt_tag} and \ref{tab:example_llama2_tag}), and $p_{infer}$ with $p_{infer}^{rec}$ (Tables~\ref{tab:example_gpt_infer} and \ref{tab:example_llama2_infer}). A more thorough analysis shows that \model can be applied to diverse item domains and it is not restricted to datasets with rich textual information. Please see Appendix~\ref{appendix:applicability}.






We hypothesize that these generated words contribute to improving recommendation performance. To further validate this hypothesis, we design two variants of the response generated by {\sc GPT-3}, namely $p_{para}^{mask}$ and $p_{para}^{keyword}$. To construct $p_{para}^{mask}$, we mask the words that appear in the response of $p_{para}^{rec}$ but are absent in the response of $p_{para}$. To construct $p_{para}^{keyword}$, we append the words that (1) appear in the response of $p_{para}^{rec}$ and (2) are pre-defined user-preference-related words such as genres to the end of the response of $p_{para}$ (more details in Appendix~\ref{appendix:keyword_construction}). These two variants are then fed into MLP models to form baselines. Comparing the performance of $p_{para}^{rec}$ and $p_{para}^{mask}$ in Table~\ref{tab:mask_keyword}, we observe a discernible decline in recommendation performance when words unique to the response of $p_{para}^{rec}$ are selectively masked. This outcome highlights the pivotal role played by the supplementary insights introduced through the augmented text. Further, our investigation reveals that the incorporation of vital keywords, as opposed to the inclusion of all response words, can yield even superior recommendation performance. This may be attributed to potential discrepancies or extraneous elements within the response of $p_{para}^{rec}$.

\input{tab_fig/mask_keyword}


\noindent\textbf{\mbox{\sc \textbf{LLM-Rec}}\xspace augmentation outperforms other text augmentation methods for recommendation.} We compare \model with two recent advancements in the field of using LLMs to augment item information, specifically Knowledge Augmented Recommendation (KAR) as proposed by \citet{xi2023towards}, and TagGPT, as proposed by \citet{li2023taggpt}. KAR introduces a fusion of domain knowledge and prompt engineering to generate factual knowledge pertaining to the items (for detailed implementation information, see Appendix~\ref{appendix:implementation}).  Since the augmented information may not necessarily be correct, we further implement a variant with ground truth knowledge. It aligns with strategies akin to those introduced by \citet{di2012linked}, who harnessed external databases to enhance item information. In a manner consistent with this approach, we incorporate genre information into the item descriptions. Note that genre constitutes one of the metadata components in Movielens-1M. Such categorical characteristics are absent in Recipe. Therefore, we only apply this variant to the Movielens-1M dataset. 


As shown in Table~\ref{tab:sota_comp}, the incorporation of knowledge-based text augmentation offers significant improvements in recommendation performance for well-classified items, such as movies. However, it becomes evident that this approach faces limitations when applied to items, like user-generated content, that are inherently more novel and dynamic in nature. The example response in Figure~\ref{fig:kar_recipe_example_short} shows that one key reason that knowledge augmentation approaches do not yield optimal improvement regarding recommendation performance may lie in the potential \textbf{mismatch} between the generated knowledge and the target item. For instance, while the generated ingredient information may be correct for most meatloaf recipes, it could be entirely wrong for a specific recipe without additional context. In contrast to these knowledge augmentation methods, \model's recommendation-driven prompts provide augmented information that describes the target item at a broader, less granular level, especially when compared to KAR. More importantly, \model does not require domain knowledge throughout the entire process.

In terms of the second text augmentation baseline, TagGPT~\cite{li2023taggpt}, which extracts tags using LLMs, several key observations can be made. First, we note an improvement in recommendation performance using tag generation compared to the baseline methods. Second, the prompts specifically designed within our \model framework demonstrate superior effectiveness compared to those used in TagGPT.



\input{tab_fig/kar_recipe_example_short}


\subsection{Ablation Study}\label{sec:ablation_study}


\textbf{How does each prompting strategy perform?} We conduct an ablation study to examine the impact on recommendation performance when models use either original item descriptions alone or a combination of these descriptions with augmented text derived from one of four distinct prompting strategies. The results, presented in Table~\ref{tab:ablation_different_prompt_short}, reveal a noteworthy and consistent enhancement in recommendation performance across various prompting strategies within two benchmark datasets. 


We also note variations in the performance of these strategies across different domains, aligning with our expectations. In Movielens-1M, the strategy combining recommendation-driven and engagement-guided approaches yields the best results. Conversely, in Recipe, the recommendation-driven strategy alone proves most effective. This variability suggests that combining multiple objectives in a single prompting strategy does not always lead to superior performance. When LLMs are tasked with generating descriptions serving multiple purposes, the balance of information becomes crucial. If neighboring item descriptions vary widely, it can challenge the model's ability to generate useful content, potentially leading to less optimal improvements. To address this, \model integrates all enriched text and leverages the subsequent recommendation module to effectively model the extra information. Future work can investigate different prompt designs, aiming to effectively achieve multiple objectives simultaneously.


\input{tab_fig/ablation_different_prompt_short}







\noindent\textbf{How does concatenating the augmented responses affect recommendation?} In Table~\ref{tab:sota_comp}, we show that the MLP model, which combines all augmented text with the original description, outperforms more advanced models that rely solely on the original description as input. Now we take a deeper look at the quality of the combined augmented text. We employ the same recommendation module (\ie an MLP with a dot product) and evaluate the recommendation performance of various concatenation combinations. As shown in Figure~\ref{fig:concat}, the model denoted as {\tt Basic} uses the embeddings of text augmented through $p_{para}$. {\tt Concat-Basic} represents the model that concatenates the embeddings of the input text augmented by all basic prompts. Additionally, {\tt Concat-Rec} is the model that employs the concatenation of the embeddings of input text augmented by all recommendation-driven prompts. Lastly, {\tt Concat-All} stands for the model that combines the embeddings of input text augmented by all four prompting strategies. Our findings reveal that concatenating more information \textit{consistently} enhances recommendation performance. This emphasizes the added value of incorporating augmented text as opposed to relying solely on the original content description. Complete results of Figure~\ref{fig:concat} can be found in Figure~\ref{fig:ablation_different_concat}.


\input{tab_fig/concat}

\noindent\textbf{How to effectively integrate the augmented responses to maximize improvement?} Table~\ref{tab:concat_short} shows the recommendation performances of other concatenation variants: (1) \textbf{Duplication}: We duplicate the embeddings of the original item description to match the dimension size of the embeddings of {\tt Concat-All}; (2) \textbf{Text concatenation}: Instead of concatenating the embeddings of all response (\ie {\tt Concat-All}), we concatenate the responses first, and then convert it to embeddings. Through a comparative analysis of the model's performance, contrasting the first variant with {\tt Concat-All}, it becomes evident that the observed improvement in performance is not attributable to changes in embedding size. Further, by comparing the performance of the second variant against {\tt Concat-All}, we discover that in scenarios where the text encoder remains unmodified, the most effective strategy to integrate all enriched information is by first converting the text into embeddings and then concatenating these embeddings. This approach surpasses the method of concatenating text prior to its conversion into embeddings. Future research can explore the potential of modifying the text encoder to further enhance model efficiency and effectiveness.


\input{tab_fig/concat_method_short}

\noindent\textbf{Does modifying the word choice in the designed prompts significantly affect the augmented output?} To investigate this, we construct one variant prompt for each of \model's prompts, ensuring they convey the same meaning but with different word choices.  As shown in Table~\ref{tab:appendix_variant_quantitative}, despite variations in wording of the prompts, the responses remain largely similar. The observed lower cosine similarity is primarily attributed to differences in the format of the responses which can be mitigated through various strategies, such as additional fine-tuning of the model or incorporating specific instructions within the prompts.
  


\input{tab_fig/appendix_variant_quantitative}