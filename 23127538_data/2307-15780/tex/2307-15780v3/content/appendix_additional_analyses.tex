\clearpage
\section{Additional Analysis}\label{appendix: additional_analysis}

\subsection{Additional Discussions on Augmented Text}\label{appendix:augmented_text_understanding}
Overall, we observe that \model effectively elicits LLMs to enrich item descriptions with additional information. This augmentation results in descriptions that are not only more detailed but also more expressive, enhancing the original item descriptions.  Tables~\ref{tab:example_gpt_tag} and \ref{tab:example_gpt_infer} show example responses generated by {\sc GPT-3} comparing the recommendation-driven and basic promptings in terms of {\tt tag} and {\tt infer}. Tables~\ref{tab:example_llama2_para}, \ref{tab:example_llama2_tag}, and \ref{tab:example_llama2_infer} show example responses generated by {\sc Llama-2} comparing the recommendation-driven and basic promptings. Responses from the recommendation prompting strategies provides several additional pieces of information and context compared to the responses from the basic prompting strategies. Take the responses in Table~\ref{tab:example_gpt_tag} as an example, the recommendation prompting strategy introduces new themes like ``Supernatural'', ``Paranormal'', and ``Psychological Thriller'', which are not present in the responses from the basic promptings. These themes suggest a broader and more specific context for the story, indicating not just communication with the dead, but also elements of horror and suspense. The term ``Troubled Child'' adds a new dimension to the ``Young Boy'' mentioned in the first sentence, suggesting that the child's character may face internal conflicts or challenges.



\noindent\textbf{What extra information does engagement-guided strategy prompt LLMs to augment?} Consistent with our previous experiments, we curate exemplary responses obtained from $p^{eng}$ for closer examination (Figure~\ref{fig:case_eng}). Our analysis reveals a distinct pattern compared to what we have observed with recommendation-driven prompting. There are primarily two scenarios to consider. First, if the descriptions of the important neighbor items and the target items exhibit high similarity, the impact of $p^{eng}$ resembles that of $p_{para}$, as exemplified in the second Recipe example in Figure~\ref{fig:case_eng}. Second, $p^{eng}$ guides LLMs to generate additional information, which may be derived from the descriptions of the important neighbor items. Consequently, how the engagement-guided strategy influences LLMs' text generation—whether it aligns with one of the behaviors we have described, both of them, or even other unexplored patterns—largely depends on the composition of the important neighbor items. This composition, in turn, is contingent on the neighbor sampling method which is out of the scope of our study. We leave a more in-depth exploration of this topic to future research endeavors.

Interestingly, the recommendation-driven + engagement-guided prompting strategy is able to generate text that shares similar characteristics with both sub-strategies. How they quantitatively form the final generation remains an open question. Examples can be found in Table~\ref{tab:example_gpt_rec_eng}.




Table~\ref{tab:example_llama2_eng} shows example responses of {\sc Llama-2} to the engagement-guided prompting strategy. Table~\ref{tab:example_gpt_rec_eng} shows example responses of {\sc GPT-3} to the recommendation-driven and engagement-guided prompting strategy. Overall, the 7B {\sc Llama-2-Chat} performs poorly compared to {\sc GPT-3}. In some cases, there is no generated content as we have also observed in Appendix~\ref{sec:example_responses_appendix}.






\subsection{Additional Discussions on Applicable Item Domains and Available Textual Information}\label{appendix:applicability}


The applicability of \model beyond movies and recipes, particularly in domains with sparse textual information, remains a question. To address this, we conduct an analysis to determine \model's efficacy in enriching text across various domains and text lengths. We use item descriptions from ten distinct domains in the Amazon review dataset~\cite{ni2019justifying}, which includes product metadata like descriptions, category, price, brand, and image features. The selected domains are all beauty, appliances, automotive, digital music, grocery and gourmet food, pet supplies, sports and outdoors, video games, magazine subscriptions, and industrial and scientific. For each domain, we sample 50 items and prompt {\sc GPT-3} using $p^{rec}_{para}$ (\ie recommendation-driven prompting).

Our previous discussions highlight that the most valuable information for improving recommendation performance typically aligns with expressive words pertinent to item characteristics. While no single metric directly quantifies this added information, we use the increase in the number of adjectives as a proxy. Additionally, the total word count serves as a straightforward metric to approximate the volume of augmented information. By comparing the augmented texts with the original item descriptions, we calculate the percentage increase in the number of adjectives. Note that the adjective increase is computed as a ratio of the difference in adjective count to the original word count.

As Figure~\ref{fig:applicable_domain} demonstrates, \model effectively enriches item descriptions across multiple domains, including those lacking in rich textual content. For instance, the average word counts in movie and digital music descriptions are only 20.34 and 30.18 words, respectively. \model enhances expressiveness, with a notable increase in the use of adjectives. 


\subsection{Additional Experiments on Applying LLM-Rec to Other Baselines}\label{appendix_sec: llm-rec_to_other_baselines}
We extend the application of \model to other text-based recommendation systems and replicate the experiments. The results, as presented in Table~\ref{tab:other_baselines_with_llm_rec}, indicate that \model can be easily adapted to various text-based recommendation systems and generally enhances recommendation performance compared to using the original text.



\input{tab_fig/appendix_tab_other_baselines_with_llm_rec}




\subsection{Additional Discussions on Integration Process}\label{appendix_sec: integration}
 In our setup, the text encoder is frozen (not fine-tuned), with a fixed output dimension for all vectors. The fundamental difference between the {\tt Concat-All} and Text Concatenation methods lies in their processing sequence. The {\tt Concat-All} method initially transforms individual text segments into embeddings and subsequently concatenates these embeddings. In contrast, the Text Concatenation method first concatenates the text segments and then converts this combined text into a single embedding.

The observed superiority of the {\tt Concat-All} method can be attributed to how these processes handle information density. When lengthy text segments are concatenated before encoding, there is a higher likelihood of \textbf{information loss}, particularly given the constraints of a frozen text encoder. This encoder, \textbf{not} being fine-tuned for the specific nuances of our data, may struggle to effectively capture and retain crucial information from longer text inputs. Therefore, processing shorter text segments individually before concatenation (as in {\tt Concat-All}) may help in preserving important features and nuances in the embeddings.





\subsection{Additional Discussions on Prompt Design}\label{appendix:prompt_design}

To investigate whether modifying the word choice in the designed prompts significantly affects the augmented output, we construct one variant prompt for each of \model's prompts, ensuring they convey the same meaning but with different word choices. Take $p_{para}$ as an example, $p_{para}$ is {\it ``The description of an item is as follows `\{{\tt description}\}', paraphrase it.''}. One variant is {\it ``Summarize the given item description, `\{{\tt description}\}', using different words.''} Next, we randomly sample 50 items from Movielens-1M, and prompt {\sc GPT-3} with these variants. The cosine similarity between the responses generated from the variant prompt and \model's prompt is computed and shown in Table~\ref{tab:appendix_variant_quantitative}. Tables~\ref{tab:variant_basic_para}-\ref{tab:variant_rec_eng} shows the example responses.


\subsection{Additional Discussions on Dynamic Prompts}\label{appendix:dynamic_prompt}
The concept of dynamic prompts in recommendation systems is an intriguing area that holds the potential for enhancing personalization. By incorporating descriptions of a user's most recently interacted items into prompts, the system can generate item descriptions on-the-fly that are more closely aligned with the user's current interests and preferences. This approach could lead to more precise and tailored recommendations, as the generated descriptions would reflect the user's evolving tastes.

One of the primary considerations is the computational cost associated with generating dynamic prompts. Each user interaction would require real-time processing to update the prompt, which could be resource-intensive, especially for large-scale systems with many users and items.

To mitigate computational costs, several strategies can be employed.
Developing efficient algorithms for prompt generation and item description generation can help mitigate computational costs. 
Implementing caching mechanisms for frequently accessed data can reduce the processing time required for updating prompts.
Instead of completely regenerating prompts after each interaction, the system could employ incremental updates to modify prompts based on recent changes in user behavior.


While the implementation of dynamic prompts presents several challenges, it also offers a promising avenue for enhancing personalization in recommendation systems. With careful consideration, this approach has the potential to cater more effectively to individual user needs.



\input{content/appendix_additional_related_work}

\section{Example Responses}\label{sec:example_responses_appendix}

Tables~\ref{tab:example_response_ml1m} and \ref{tab:example_response_recipe} show example responses by {\sc GPT-3} and the 7B {\sc Llama-2-Chat} on Movielens-1M~\citep{harper2015movielens} and Recipe~\citep{majumder2019generating}. Augmented components are highlighted (recommendation-driven: \purpletext{blue}; engagement-guided: \greentext{green}; rec+eng: \orangetext{orange}). In summary, both {\sc GPT-3} and {\sc Llama-2} exhibit the capability to enrich item descriptions with supplementary information. Nevertheless, the {\sc Llama-2-Chat} model with its 7B parameters demonstrates comparatively poorer performance, which could be attributed to its limited parameter scale. This limitation offers insight into the diminished recommendation quality when using {\sc Llama-2} responses in contrast to {\sc GPT-3}. Future research endeavors should focus on optimizing the \model framework, particularly concerning the selection of different large language models as backbones, to enhance recommendation outcomes.


\input{tab_fig/appendix_example_tag_gpt}

\input{tab_fig/appendix_example_infer_gpt}

\input{tab_fig/appendix_example_para_llama2}

\input{tab_fig/appendix_example_tag_llama2}

\input{tab_fig/appendix_example_infer_llama2}


\input{tab_fig/case_eng}

\input{tab_fig/appendix_example_eng_llama2}

\input{tab_fig/appendix_example_receng_gpt}

\input{tab_fig/appendix_variant_prompt_basic_para}

\input{tab_fig/appendix_variant_prompt_basic_tag}

\input{tab_fig/appendix_variant_prompt_basic_infer}

\input{tab_fig/appendix_variant_prompt_rec_para}

\input{tab_fig/appendix_variant_prompt_rec_tag}

\input{tab_fig/appendix_variant_prompt_rec_infer}


\input{tab_fig/appendix_variant_prompt_eng}

\input{tab_fig/appendix_variant_prompt_rec_eng}


\input{tab_fig/concat_method}
