\vspace{-.2cm}
\section{Related Work}
\vspace{-.2cm}
\model closely aligns with two research directions: (1) augmentation in text-based recommendation, and (2) LLM for recommendation. A comprehensive discussion is provided in Appendix~\ref{sec:detailed_related_work}. 

\noindent\textbf{Augmentation in Text-based Recommendation.} Text-based recommendation systems leverage natural language processing and machine learning techniques to provide personalized recommendations to users based on textual information~\citep{lops2019trends, qiang2020short}. However, the performance of such systems can be compromised when dealing with incomplete or insufficient textual information. To address this limitation, several studies have suggested strategies for enhancing textual information. For instance, \citet{li2010contextual} proposed to extract contextual cues from online reviews, leveraging these narratives to uncover users' preferences and underlying factors influencing their choices~\citep{sachdeva2020useful}. Other approaches infer linguistic attributes from diverse sources, including emotion, sentiment, and topic, to refine the modeling of both items and users~\citep{sun2015mining, sailunaz2019emotion, ramage2010characterizing,chen2010short}. Furthermore, some works explore the integration of external knowledge bases to enrich the contextual understanding of items~\citep{di2012linked, musto2018semantics}. In a more recent development, \citet{bai2022improving} introduced an approach that employs pre-trained language models to generate additional product attributes, such as product names, to augment item contextual information. Diverging from these prior approaches, our contribution is the \model framework, which employs large language models to enhance input text, providing a versatile solution for recommendations. A more detailed discussion on the distinctions between \model and these related work can be found in Section~\ref{sec:discussions_and_conclusions}.



\input{tab_fig/eval_framework}
\noindent\textbf{LLM for Recommendation.}
Due to LLMs' remarkable text generation ability, many studies have leveraged LLMs as a data augmentation tool~\citep{dai2023auggpt, li2022elevater}. \citet{liu2023llava} used an LLM to produce multimodal language-image instruction-following datasets. Through a process of instruction tuning using this generated data, their proposed framework demonstrated an impressive aptitude in advancing vision and language comprehension. There have also been efforts to use LLMs to augment the input side of personalized recommendation. For instance, \citet{chen2023palr} incorporated user history behaviors, such as clicks, purchases, and ratings, into LLMs to generate user profiles. These profiles were then combined with the history interaction sequence and candidate items to construct the final recommendation prompt. LLMs were subsequently employed to predict the likelihood of user-item interaction based on this prompt. \citet{xi2023towards} introduced a method that leverages the reasoning knowledge of LLMs regarding user preferences and the factual knowledge of LLMs about items. However, our study focuses specifically on using LLMs' knowledge and reasoning ability to generate augmented input text that better captures the characteristics and nuances of items, leading to improved recommendation performance. 


