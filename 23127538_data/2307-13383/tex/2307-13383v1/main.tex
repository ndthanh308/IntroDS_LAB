%\documentclass[sigconf,authordraft]{acmart}
%\documentclass[sigconf,prologue,dvipsnames]{acmart}
%\documentclass[10pt,conference,prologue,dvipsnames]{IEEEtran}
\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{EMNLP2023}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
%\setcopyright{acmcopyright}
%\copyrightyear{2018}
%\acmYear{2018}
%\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

% \usepackage[utf8]{inputenc}
% \usepackage[T1]{fontenc}

\usepackage{times}
\usepackage{latexsym}
\usepackage{microtype}
\usepackage{inconsolata}
%%
%% end of the preamble, start of the body of the document source.
\usepackage{float}
%\usepackage[dvipsnames]{xcolor}
\PassOptionsToPackage{usenames,dvipsnames}{xcolor}
%\usepackage[usenames,dvipsnames,rgb]{xcolor}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tcolorbox}
\usepackage{adjustbox}
\usepackage[frozencache,cachedir=.]{minted}
%\usepackage[finalizecache,cachedir=.]{minted}
%\usepackage{minted}
%\usepackage{xcolor}
\usepackage{array}
\usepackage{siunitx}
%\setminted{fontsize=\scriptsize}
%\setminted{fontsize=\tiny}
\setminted{fontsize=\scriptsize}
%\setminted{fontsize=\small}
\definecolor{bg}{HTML}{282828}
\usepackage{pmboxdraw}
\usepackage{multirow}
%\usepackage{balance}
% \usepackage[keeplastbox]{flushend}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{graphicx}
%\usepackage[justification=centering]{caption}
% \usepackage{listings}
% \lstset{extendedchars=true}



\definecolor{ForestGreen}{RGB}{34,139,34}
\definecolor{RoyalBlue}{RGB}{85,118,209}

\definecolor{Gray}{gray}{0.9}
\newcommand{\txtmint}[1]{\mintinline[fontsize=\scriptsize, bgcolor=Gray]{text}{#1}}

\title{Predicting Code Coverage without Execution}


\author{Michele Tufano, Shubham Chandel, Anisha Agarwal, Neel Sundaresan, Colin Clement \\
  Microsoft \\
  Redmond, WA, USA \\
  \texttt{\{mitufano, schandel, anisagarwal, neels, coclement\}@microsoft.com}}



\begin{document}

\newcommand{\dataset}{{\sc CoverageEval}\xspace}

\newcommand{\ie}{\textit{i.e.,}~}
\newcommand{\eg}{\textit{e.g.,}~}
\newcommand{\etc}{\textit{etc.}~}
\newcommand{\etal}{\textit{et al.}~}

%Comments
\newcommand{\nb}[2]{
    \fbox{\bfseries\sffamily\scriptsize#1}
    {\sf\small$\blacktriangleright$\textit{#2}$\blacktriangleleft$}
}

\newcommand\MICHELE[1]{\textcolor{blue}{\nb{MICHELE}{#1}}}
\newcommand\COLIN[1]{\textcolor{green}{\nb{COLIN}{#1}}}
\newcommand\SHUBHAM[1]{\textcolor{red}{\nb{SHUBHAM}{#1}}}





% DOUBLE BLIND

% \author{\IEEEauthorblockN{Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Neel Sundaresan}
% \IEEEauthorblockA{Microsoft\\
% Redmond, WA, USA\\
% Email: \{mitufano, dadrain, alsvyatk, neels\}@microsoft.com}\vspace{-5ex}}



% \settopmatter{authorsperrow=4}

% \author{Michele Tufano}
% \affiliation{%
%   \institution{Microsoft}
%   \city{Redmond}
%   \state{WA}
%   \country{USA}
% }

% \email{mitufano@microsoft.com}

% \author{Shubham Chandel}
% \affiliation{%
%   \institution{Microsoft}
%   \city{Redmond}
%   \state{WA}
%   \country{USA}
% }
% \email{schandel@microsoft.com}

% \author{Anisha Agarwal}
% \affiliation{%
%   \institution{Microsoft}
%   \city{Redmond}
%   \state{WA}
%   \country{USA}
% }
% \email{anisagarwal@microsoft.com}

% \author{Neel Sundaresan}
% \affiliation{%
%   \institution{Microsoft}
%   \city{Redmond}
%   \state{WA}
%   \country{USA}
% }
% \email{neels@microsoft.com}

% \author{Colin Clement}
% \affiliation{%
%   \institution{Microsoft}
%   \city{Redmond}
%   \state{WA}
%   \country{USA}
% }
% \email{coclement@microsoft.com}

% \renewcommand{\shortauthors}{Tufano, et al.}


%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%   <concept>
%       <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
%       <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
%       <concept_significance>500</concept_significance>
%       </concept>
%   <concept>
%       <concept_id>10010147.10010178.10010179.10010180</concept_id>
%       <concept_desc>Computing methodologies~Machine translation</concept_desc>
%       <concept_significance>300</concept_significance>
%       </concept>
%  </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Software and its engineering~Software testing and debugging}
% \ccsdesc[300]{Computing methodologies~Machine translation}

% \keywords{software testing, unit test, neural networks}

\maketitle

\begin{abstract}
Code coverage is a widely used metric for quantifying the extent to which program elements, such as statements or branches, are executed during testing. %It provides valuable insights into the amount of code exercised by a test suite and serves as a standard measure of test suite quality. %Higher code coverage percentages indicate a lower risk of undiscovered software bugs. 
%While code coverage is not a panacea for eliminating bugs, it remains a reliable proxy metrics for assessing code quality. 
Calculating code coverage is resource-intensive, requiring code building and execution with additional overhead for the instrumentation. Furthermore, computing coverage of any snippet of code requires the whole program context. Using Machine Learning to amortize this expensive process could lower the cost of code coverage by requiring only the source code context, and the task of code coverage prediction can be a novel benchmark for judging the ability of models to understand code.
We propose a novel benchmark task called Code Coverage Prediction for Large Language Models (LLMs). We formalize this task to evaluate the capability of LLMs in understanding code execution by determining which lines of a method are executed by a given test case and inputs. We curate and release a dataset we call \dataset by executing tests and code from the HumanEval dataset and collecting code coverage information. We report the performance of four state-of-the-art LLMs used for code-related tasks, including OpenAI's GPT-4 and GPT-3.5-Turbo, Google's BARD, and Anthropic's Claude, on the Code Coverage Prediction task. Finally, we argue that code coverage as a metric and pre-training data source are valuable for overall LLM performance on software engineering tasks.

%Our ultimate goal is to leverage LLMs for predicting code coverage, offering a viable alternative to execution-based coverage in various scenarios. This approach proves advantageous when program build and execution costs are prohibitive, code coverage needs to be invoked multiple times, only code snippets are available (e.g., server-side scenarios), or errors in the project prevent complete builds. Additionally, this task serves as a novel metric for code understanding and a valuable (pre-)training objective. Training models to excel at this task could enhance their overall performance on code-related tasks.
\end{abstract}









% Code coverage measures which program elements -- such as statements or branches -- are executed by a (set of) test case(s).
% Code coverage provides a useful metric to quantify the amount of code that is executed during testing, and is a standard method to estimate the quality of a test suite. Intuitively, the higher the percentage of statements and branches covered by a test suite, the lower the risk of undetected software bugs lingering in the program. While code coverage is not a silver bullet against bugs, it represents one of the best available proxy metrics for code quality, often required for safety standards (such as FAA and automotive software).

% Code coverage is computed by instrumenting the code, and running the test suite while monitoring the code execution. This process is expensive, since it requires building and executing code, with additional overhead for the instrumentation. Additionally, it is not currently possible to measure code coverage for a snippet of code without the availability of the entire program which contains the given snippet.

% In this paper we present an approach which aims at predicting code coverage without building or executing the target program. Our approach relies on a Transformer-based model, which takes two inputs: (i) a method or code snippet for which code coverage is needed; (ii) a test case testing the given method (or an invocation such as a \texttt{main} function). The model is then trained to generate as output the coverage-annotated method, where statements and branches that would be executed by the test case are marked appropriately.

% This approach could represent a viable alternative to execution-based coverage in several circumstances, such as: (i) program build and execution is too expensive; (ii) code coverage needs to be invoked multiple times; (iii) only a snippet of code is available, but not the entire program (e.g. server-side scenario); (iv) there are errors somewhere in the project preventing complete builds. This approach can also serve as a novel code understanding metric and training objective. Large language models should be able to perform well on this task if they truly `understand' code, and training models to perform this task should improve their general performance on code-related tasks.




%% Figure environment removed
% ---------- END Code Example ----------------


For example, coverage is one of the metrics considered by the Federal Aviation Administration (FAA) for safety certification of avionic equipment, as documented in DO-178B \cite{johnson1998178b} and DO-178C \cite{rierson2017developing}. Test coverage is also a requirement in the automotive safety standard ISO 26262 Road Vehicles - Functional Safety \cite{palin2011iso}.
 
Given a focal method $m$, which is executed \textit{directly} by the test case $t$, code coverage measures the number of statements that have been executed (\ie covered) by the test $t$. Figure \ref{fig:coverage} shows an example of a focal method $m$ (method under test) tested by $t$. The coverage obtained by $t$ on $m$ is represented in the coverage-annotated method $\mathrm{cov}(m,t)$, where executed statements are marked with \colorbox{green}{>} while missed (\ie uncovered statements) with \colorbox{red}{!} and unreachable code (\ie dead code) with \colorbox{gray}{-}. From this representation, several quantitative coverage metrics can be computed, such as functional, statement, branch, and path coverage. 
%We will discuss these coverage metrics in details in the Sec \ref{sec:background}.

Code coverage is computed by instrumenting the code and running the test suite while monitoring the code execution. This process is expensive, since it requires building and executing code, especially for large software projects or when code coverage is computed multiple times. Additionally, it is not possible to measure code coverage for a snippet of code without the availability of the entire program which contains the given snippet. This situation happens when only partial code is available, for example within a commit log/diff, or when only partial code is transmitted to a server, for security and/or networking reasons.

While Large Language Models (LLMs) have gained prominence in code-related tasks and demonstrated impressive results in areas such as code generation and test generation, it remains unclear to what extent these models truly understand code execution~\cite{liu2023code}. The task of accurately determining which lines of a method are executed based on a given test case and its inputs requires a deep understanding of the underlying code execution dynamics. This motivates the need for a dedicated task, referred to as Code Coverage Prediction, which specifically evaluates the capability of LLMs in comprehending code execution. Further, a model capable of this task is independently useful as it can amortize the expensive code coverage computation process, or function in cases where normal code coverage is not possible to compute.

In this paper we formalize the Code Coverage Prediction task, with the primary objective of evaluating the capability of LLMs in understanding code execution by accurately determining which lines of a method are executed based on a given test case. To facilitate evaluation, we have curated a comprehensive dataset named \dataset, consisting of coverage-annotated methods. This dataset is created by executing tests and code from the HumanEval dataset, allowing us to collect valuable code coverage information. We have organized and made this curated dataset available on GitHub,
%and HuggingFace Datasets
 enabling researchers to explore and advance code coverage prediction techniques and LLM code understanding.

We evaluate the performance of four state-of-the-art LLMs widely employed for code-related tasks: OpenAI's GPT-4 and GPT-3.5, Google's BARD, and Anthropic's Claude. Our ultimate goal is to gain insights into the capabilities of LLMs in predicting code coverage, offering a promising alternative to execution-based coverage measurement in various scenarios. This approach proves advantageous when the costs associated with program building and execution are prohibitive, when code coverage needs to be invoked multiple times, when only code snippets are available (e.g., in server-side scenarios), or when errors in the project prevent complete builds. Additionally, this task introduces a novel metric for assessing code understanding and serves as a valuable (pre-)training objective. By training models to excel in this task, we believe we can enhance their overall performance on code-related tasks.

This paper makes the following contributions:
\begin{itemize}
\item \textit{Code Coverage Prediction Task}: We propose a novel task to assess the capability of LLMs in understanding code execution by accurately predicting executed lines of a method based on a given test case and inputs.

\item \textit{Evaluation of State-of-the-Art LLMs}: We evaluate four prominent LLMs (GPT-4, GPT-3.5, BARD, and Claude) on the Code Coverage Prediction task, providing insights into their performance and understanding of code execution.

\item \textit{Curated Dataset}: We curate a comprehensive dataset (\dataset) of coverage-annotated methods and test cases, derived from the HumanEval dataset. This dataset is openly available on GitHub\footnote{\url{https://github.com/microsoft/coverage-eval}}~\cite{CoverageDataset_github}
%\footnote{\url{https://anonymous.4open.science/r/coverage-eval-467C/}} ~\cite{CoverageDataset_github},
%and HuggingFace\footnote{\url{To-be-released}}~\cite{CoverageDataset_huggingface}, 
enabling further research and advancement in code coverage prediction techniques.
\end{itemize}

\section{Background}
\label{sec:background}

Code coverage is a measure of the degree to which a test suite exercises a software system \cite{ivankovic2019code}. Code coverage is commonly computed by means of instrumentation. This technique inserts instrumentation code in various locations within the code or binaries of the program under test, in order to monitor its execution. This inserted code provides counters to record which function or statement of the program have been executed by the test suite. Inserting these additional statements within the original code leads to execution overhead, which can be significant especially for large software programs \cite{tikir2002efficient}.

The most common coverage metric is computed at statement level, where statement refers to a syntactic unit of code (\eg assignment, invocation, assertion), often matching a single line of code. The coverage indicates whether a statement has been executed or not, and aggregated metrics can be computed at function/program level to measure the amount of statements covered by a test suite. In the example in Figure \ref{fig:coverage}, the test case $t$ executes four statements in $m$, which constitutes $\sim44$\% statement coverage for the method $m$.


Given statement coverage information, other coverage criteria and metrics can be obtained by means of static analysis. Statement coverage information regarding control structure (\eg \texttt{if-else} and \texttt{case} statements) can be used to compute branch coverage, which measure how many logical branches in the program have been executed. In the example in Figure \ref{fig:coverage} only one branch is executed (\ie \texttt{else if (x > 0)} ), while the other two branches are missed by the test case $t$.

In the remainder of this paper we will focus on statement coverage, from which other coverage criteria can be obtained.


\section{Code Coverage Prediction Task}
\label{sec:problem}
Given a method under test (focal method) $m$, composed of $n$ statements $S_m = s_1, s_2, \dots, s_n$, and a test case $t$ which exercises the method $m$, the coverage-annotated focal method $\mathrm{cov}(m,t)$ is composed of a sequence of $n$ statements $S_{m}^{t} = s_1^*, s_2^*, \dots, s_n^*$, where each statement  $s_i^*$ represents the coverage-annotated statement of $s_i$ in $m$. Specifically, $s_i^*$ is marked with one of the three possible coverage symbols $c \in \{>, !, - \}$, where the symbol $>$ identifies statements that have been executed by $t$, the symbol $!$ identifies statements that have been missed by $t$, and the symbol $-$ identifies statements that are unreachable. This defines a sequence of $n$ coverage symbols $C_m^t = c_1, c_2, \dots, c_n$, where $c_i \in \{>, !, - \}$. %This sequence can be combined with the original sequence of statements $S_m = s_1, s_2, \dots, s_n$, to obtain the coverage-annotated sequence of statements $S_m^t = s_1^*, s_2^*, \dots, s_n^*$ comprising the coverage $\mathrm{cov}(m,t)$.

We define the Code Coverage Prediction Task as the problem of predicting the coverage-annotated sequence of statements $S_m^t$ given the focal method $m$ and a test case $t$. Formally, this problem can be defined in terms of inputs and expected output:

\textbf{Input}
        \begin{itemize}
        \item Focal Method: $m$
        \item Test Case: $t$
    \end{itemize}
    
\textbf{Output}
    \begin{itemize}
        \item $S_m^t = s_1^*, s_2^*, \dots, s_n^*$ \\
        or
        \item $C_m^t = c_1, c_2, \dots, c_n$
    \end{itemize}

Specifically, the output can be either the coverage-annotated sequence of statements $S_m^t$, or the sequence of coverage symbols $C_m^t$, which can then combined with the original sequence of statements $S_m = s_1, s_2, \dots, s_n$, to obtain the coverage-annotated sequence of statements $S_m^t = s_1^*, s_2^*, \dots, s_n^*$ comprising the coverage $\mathrm{cov}(m,t)$. This final step is performed by aligning the two sequences and obtaining $s_i^* = c_i + s_i$, where the $+$ operation refers to string concatenation. 

Let us take as example the focal method $m$ and test case $t$ in Figure \ref{fig:coverage}. The model is expected to predict either the coverage-annotated sequence of statements $S_m^t$ or the sequence of coverage symbols: \texttt{> > ! > > ! ! ! -}. 
%This sequence of symbols is then aligned with the original sequence of statements, and the coverage-annotated focal method $c$ is obtained by concatenating each coverage symbol $c_i$ to the corresponding statement $s_i$.\COLIN{Is this still the case or do we just directly predict the annotated statements?} \MICHELE{Good point. Clarified this.}


\subsection{Coverage Prediction for Pre-Training}

We propose that the code coverage prediction task introduced in our paper can serve as a valuable pre-training task for LLMs focused on code generation. While current pre-training tasks, such as Masked Language Modeling (MLM) help models understand code syntax and semantics by analyzing vast amounts of raw text representing code, our proposed task enables the model to learn about code execution, which is not technically discoverable by source code text alone.

To accomplish this pre-training, we suggest augmenting the training data with extensive coverage logs obtained from Continuous Integration/Continuous Deployment (CI/CD) pipelines. These logs contain valuable information about code coverage from regression tests executed during pull requests or commits.

By exposing the models to these coverage logs during pre-training, they can learn to associate test cases and inputs with the specific lines of code that are executed. This pre-training approach enhances the models' understanding of how different parts of the code are exercised by various test scenarios. Consequently, the models can acquire a deeper comprehension of the relationships between inputs, tests, and code execution, leading to improved code generation capabilities.

Integrating coverage prediction as a pre-training task could enable models to learn from real-world test scenarios, capturing the nuances of code execution in practical settings. This real-world exposure should enhances the models' ability to generate code that aligns with actual testing practices.

Furthermore, incorporating coverage prediction as a pre-training task opens up possibilities for transfer learning. Models pre-trained on coverage prediction can be fine-tuned on downstream tasks, such as bug detection or test case generation, where understanding code execution is crucial. The models' pre-existing knowledge of code coverage can provide a solid foundation for these related tasks, potentially improving their overall performance.

\section{\dataset Dataset}
In addition to proposing the code coverage prediction task, this paper also introduces \dataset, a dataset specifically designed for evaluating LLMs on this task. This section outlines the process of curating this dataset, which begins with the HumanEval dataset \cite{chen2021codex}. By executing test cases from the HumanEval dataset, we gather code coverage information. To create \dataset, we parse the code coverage logs generated during the execution of the test cases. This parsing step enables us to extract the relevant coverage annotations. We then carefully structure and export the dataset in a format that facilitates its use and evaluation by researchers and practitioners alike.

By curating this dataset, we aim to provide a standardized benchmark for evaluating LLMs on the code coverage prediction task. The availability of \dataset enables researchers to explore and advance code understanding, fostering innovation and enabling the development of more effective models.

\subsection{HumanEval}
The HumanEval dataset consists of 164 hand-written problems and their code solutions, where each problem is a programming task involving language comprehension, reasoning, algorithms and/or simple mathematics \cite{chen2021codex}. Each code solution in the dataset includes a function signature, a docstring containing the problem description, a function body, and several unit tests. We extend the HumanEval dataset to include coverage, calculated using the function body and the respective unit tests.



\subsection{Coverage Analysis}
In this section, we describe the steps taken to analyze the code coverage on the HumanEval dataset and create our \dataset dataset.

Each code solution in the HumanEval dataset is accompanied by a single test case, which includes multiple asserts designed to test the correctness of the code solution based on the given problem's functional requirements. These asserts cover various inputs, scenarios, and code statements/branches. To enhance the dataset and increase the complexity of each data point, we split the single test case into multiple test cases, each containing a single assert. This splitting process allows us to generate additional method-test pairs, as well as making each data point more challenging. The original test case may cover most of the lines and branches in the method, but each individual assert covers only a subset of them.

By performing this split, we create a more diverse set of method-test pairs within the dataset. Each individual test case invokes the focal method once and covers a subset of the statements and branches within the method. This enables us to evaluate the LLMs' ability to predict code coverage at a more granular level, going beyond the overall coverage of the method. It also adds complexity to the task, as predicting coverage for each assert requires a deeper understanding of the code and its potential execution paths.

Subsequently, we execute the extracted test cases individually with \texttt{pytest}. During the execution, we also enable the coverage computation using \texttt{coverage.py}. To do so, we run the following command: \texttt{coverage run -m pytest <test\_name>} where \texttt{<test\_name>} is each individual test in the dataset.

Next, for each test case $t$, we analyze the corresponding coverage report obtained by the test execution in order to extract the annotated coverage $\mathrm{cov}(m,t)$. The coverage report marks each source code line in the file with coverage information, specifying whether the statement has been executed or not.

We automatically parse this report and extract the corresponding annotated coverage $\mathrm{cov}(m,t)$. At the end of this process, we obtained a dataset where each data point is formed by a triplet $d = \{ m, t, \mathrm{cov}(m,t) \}$.

\subsection{Data Format}
The \dataset dataset maintains the structure of the HumanEval dataset, with the addition of coverage information for each test. Each record corresponds to a unique problem and contains the following fields:

\begin{itemize}
  \item Problem ID: A unique ID for the problem
  \item Problem: The name of the method written to solve the problem
  \item Method: The method contents, including a function signature, a docstring with the details of the problem, and the function body.
  \item Tests: A list of unit tests for the problem. Each item in the list includes the unique ID of the test and the code of the test. We have also added coverage information for each test in the following two forms:
    \begin{enumerate}
      \item Coverage: The code of the method, with each line annotated with \colorbox{green}{>}, \colorbox{red}{!} or \colorbox{gray}{-} for code that is executed, missed or unreachable by the given test.
    \item Coverage Sequence: A list of equal length to the number of lines in the method, where each value in the list is \colorbox{green}{>}, \colorbox{red}{!} or \colorbox{gray}{-}, depending on the status of the respective line of code in the method.
    \end{enumerate}
\end{itemize}

Figure \ref{fig:coval_data_example} (Appendix) shows a sample record from the \dataset dataset. \dataset is available to the public via GitHub~\cite{CoverageDataset_github}.
%and HuggingFace~\cite{CoverageDataset_huggingface}.

Table \ref{tab:dataset} reports the statistics for the \dataset dataset in terms of number of problems, code solutions, tests, and coverage symbols. The discrepancy between number of problems and solutions is explained by the fact that some problems have multiple solutions. It is also worth noting that while our dataset currently does not contain any unreachable code (-), we have proactively considered the potential presence of unreachable code while designing the task.

\begin{table}[]
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{@{}cccccc@{}}
\toprule
\multirow{2}{*}{Problems} & \multirow{2}{*}{Solutions} & \multirow{2}{*}{Tests} & \multicolumn{3}{c}{Coverage Symbols}                     \\ \cmidrule(l){4-6} 
                          &                            &                        & Executed (\textgreater{}) & Missed (!) & Unreachable (-) \\ \midrule
                158       &                 164     &           1160            &                      20037 & 1734      & 0               \\
%\multicolumn{1}{l}{}      & \multicolumn{1}{l}{}       &                        &                           &            &                 \\ 
\bottomrule
\label{tab:dataset}
\end{tabular}
}
\vspace{-0.5cm}
\caption{\dataset statistics.}
\label{tab:dataset}
\vspace{-0.5cm}
\end{table}



\section{Evaluating LLMs}

In this section, we present our evaluation of state-of-the-art Language Models (LLMs) for the proposed task of Code Coverage Prediction. We selected four highly regarded LLMs that are not only popular for code generation but also widely used for other Natural Language (NL) tasks. The LLMs we employed for this evaluation are OpenAI's GPT-4 and GPT-3.5, Google's BARD, and Anthropic's Claude.

GPT-3.5 \cite{brown2020language} and GPT-4 \cite{openai2023gpt4} are large language models developed by OpenAI which are Transformer-style models \cite{DBLP:journals/corr/VaswaniSPUJGKP17} pre-trained to predict the next token in a document. Both models were then fine-tuned using Reinforcement Learning from Human Feedback (RLHF) \cite{christiano2017deep}. GPT-4 improves over the predecessor by accepting as input both images and text (multimodal model) and producing text as output. BARD is a conversational AI developed by Google based on LaMDA\cite{thoppilan2022lamda} a Transformer-based language models trained on dialogue \cite{adiwardana2020towards}. Anthropic Claude is a 52-billion-parameter LLM developed by Anthropic. Claude was pretrained on a large text corpus and finetuned with "RL from AI Feedback" (RLAIF), where AI feedback are steered by a small set of principles drawn from a "constitution" defined by humans \cite{bai2022constitutional}.

\subsection{Experimental Design}
%\SHUBHAM{Updated the datapoints. Looks good to me.} \MICHELE{Great!}
When evaluating the LLMs on the code coverage prediction task, we designed the experiments to assess their performance on non-trivial coverage sequences while progressively providing more information and examples.

First, we filtered out data points $d = \{ m, t, \mathrm{cov}(m,t) \}$ where the coverage sequence is \textit{trivial} consisting exclusively of the symbol \colorbox{green}{>}. These cases represent methods with no branches or where the test case covers every statement in the focal method. Although these data points are included in the \dataset dataset, we excluded them from this specific evaluation. The subset of data points containing only trivial symbols is reported in our online appendix. It's important to note that no data points in the dataset has a coverage sequence consisting solely of \colorbox{red}{!} or \colorbox{gray}{-} symbols. After this filtering step, we were left with 478 data points on which we evaluated the LLMs.

The prompt used to evaluate the LLMs was designed to include the following sections:
\begin{itemize}
\item System NL prompt: a prompt providing a natural language description of the task, aimed at conveying the task to the LLM.
\item Examples: zero, one, or multiple examples of the task.
\item Focal Method $m$ and Test Case $t$.
\end{itemize}

In terms of the System NL prompt, our evaluation involved experimenting with various prompts and descriptions. We achieved the most favorable outcomes by utilizing a system prompt that emulates a terminal environment (e.g., python terminal). Within this prompt, we instructed the LLM to generate the code coverage output based on a given test case and method. For OpenAI models, we included this prompt in the specific system prompt section, while for BARD and Claude, we incorporated it as the initial part of the prompt.

To comprehensively assess the LLMs' performance, we conducted evaluations using different numbers of examples for the code coverage prediction task. Specifically, we employed zero-shot, one-shot, and multi-shot prompting approaches. This allowed us to examine the impact of example availability on the models' performance and their ability to generalize the task across various methods.

When selecting examples for evaluating coverage on a particular method $m_i$, we took care to prevent data leakage and encourage the LLMs to generalize their predictions to other methods. To achieve this, we randomly sampled a data point $\{m_j, t, \mathrm{cov}(m,t)\}$ where $m_j \neq m_i$ when providing examples.

Finally, the prompt provides a focal method $m$ and a corresponding test case $t$ for which we expected the model to predict the code coverage. Figure \ref{fig:prompt} shows an example of the prompt we designed.


Inference is performed on all the LLMs with temperature and topp set to 0, and generating one sample.



% ---------- Code Example ----------------
% Figure environment removed
% ---------- END Code Example ----------------


\subsection{Evaluation Metrics}
In this section we describe the evaluation metrics.

Given the method $m$, the test case $t$, and the sequence of coverage symbols $C_m^t = c_1, c_2, \dots, c_n$, where $c_i \in \{>, !, - \}$, the model generates a predicted sequence of coverage symbols $\hat{C}_m^t = \hat{c}_1, \hat{c}_2, \dots, \hat{c}_n$. We consider the following metrics to evaluate the performances of our proposed approach.




\begin{table*}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lccccccccc@{}}
\toprule
\multirow{2}{*}{Model} & \multicolumn{3}{c}{zero-shot}                                 & \multicolumn{3}{c}{one-shot}                                 & \multicolumn{3}{c}{multi-shot}                                 \\ \cmidrule(l){2-10} 
                       & \multicolumn{1}{c}{Match} & \multicolumn{1}{c}{Stmt} & Branch & \multicolumn{1}{c}{Match} & \multicolumn{1}{c}{Stmt} & Branch & \multicolumn{1}{c}{Match} & \multicolumn{1}{c}{Stmt} & Branch \\ \midrule
OpenAI GPT-4   (gpt-4)        & \textbf{25.75} & \textbf{84.47} & \textbf{20.16} &\textbf{ 22.85} & \textbf{90.71} & \textbf{22.65} & \textbf{30.04} & \textbf{90.5} & \textbf{22.5} \\
OpenAI GPT-3.5 (gpt-3.5-turbo)         &0 & 39.87 & 8.33 & 8.17 & 76.53 & 17.17 & 11.03 & 82.29 & 17.9 \\
Google BARD (text-bison-001)            &0 & 81.27 & 17.21 & 1.87 & 86.93 & 19.63 & 21.56 & 85.66 & 20.52 \\
Anthropic Claude (claude-1.3)       &3.9 & 84.47 & 20.07 & 4.83 & 83.21 & 19.16 & 6.88 & 55.7 & 12.23 \\
 \bottomrule
\end{tabular}
}
\caption{LLMs performances on the Code Coverage Prediction Task. The table reports the percentages of predicted coverage sequences that match the ground truth (Match), the percentage of correct coverage symbols for statements (Stmt), and specifically for branches (Branch). Evaluation performed for zero-shot, one-shot, and multi-shot.}
\label{tab:results}
\end{table*}









\subsubsection{Perfect Sequence Match}
The perfect sequence match metric counts the number of times that the predicted sequence $\hat{C}_m^t$ exactly matches (symbol-by-symbol) the target coverage sequence $C_m^t$. This represents the case where the model predicts the coverage with perfect accuracy for all the statements and branches.

\subsubsection{Statement Correctness}
The statement correctness metric measures the percentage of statements for which the execution prediction is correct. This is equivalent to the percentage of symbols in the predicted sequence that match the target sequence.


\subsubsection{Branch Correctness}
The branch correctness metric measures the percentage of branch-specific statements for which the execution prediction is correct.
The branch correctness only considers the symbols associated with branch statements. It measures the percentage of symbols in the predicted sequence (associated with branches) that match the symbols in the target sequence. 



\section{Results}

Table \ref{tab:results} presents the performance of different LLMs on the Code Coverage Prediction task. The table showcases the percentage of predicted coverage sequences that match the ground trught (Match), the percentage of correct coverage symbols for all the statements (Stmt), and the percentage of correct coverage symbols when only considering branch statements (Branch). Evaluation performances are computed using zero-shot, one-shot, and multi-shot prompting.

OpenAI GPT-4 demonstrates the highest performance on this task, achieving 24.75\% exact match with zero-shot prompting and improving to 30\% with multi-shot prompting, where up to 6 examples are provided in the prompt. Notably, the other LLMs achieve low exact matches with zero-shot prompting (between 0 and 4\%), suggesting that these foundational models may not have been exposed to coverage logs during their training or that. The second best-performing model is Google BARD, with an exact sequence match reaching 21.5\% with multi-shot prompting.

Regarding the percentage of correct coverage statements (see Stmt), most models demonstrate improvement as more examples are included in the prompt. OpenAI GPT-4 obtain the overall best scores between 84\% and 90\% of statement correctness.

When considering only statements involved in branches (\eg \texttt{if-else}, \texttt{while}), it becomes evident that there is a significant drop in correct predictions. In fact, the best performing model, OpenAI GPT-4, accurately predicts a modest 22\% of these symbols when one- and multi-shot is used for prompting. It is important to note that this subset of statements, which are intricately connected to branches, presents a greater challenge for evaluation because the LLM must reason about the boolean conditions that determine which branch is covered. Consequently, accurately predicting coverage symbols within this context requires the model to possess a profound understanding of the conditional logic that guides program execution.

Despite the surprisingly strong results of OpenAI GPT-4 on the Code Coverage Prediction task, it should be noted that the model still fails to generate the correct coverage for more than 70\% of the method-test pairs in the \dataset dataset. This emphasizes that LLMs have a long way to go in developing a deep understanding of code execution.

We believe that in order to enhance code generation results, these LLMs should gain a comprehensive understanding of code execution under different inputs and test cases. Therefore, we assert that our dataset and proposed task can contribute to the advancement of LLMs towards this goal.














\section{Discussion\& Applications}

LLMs trained to excel on the Code Coverage Prediction task could offer a promising alternative to traditional execution-based code coverage measurement in various scenarios. In this section, we discuss several use case scenarios where this approach can be valuable and beneficial.

\subsection{Expensive Build \& Execution}

For large software projects with millions of lines of code and numerous dependencies, the build and execution process can be time-consuming and expensive. In such cases, developers may want to analyze the code coverage obtained by newly written tests without waiting for the lengthy build phase. By leveraging LLMs trained on the Code Coverage Prediction task, developers can predict the coverage obtained by the new tests on existing methods without the need to build the entire project or execute the tests. This enables developers to quickly assess whether additional tests are required to cover missed lines or branches in the methods, saving valuable time and resources.

\subsection{Limited Code Availability}

Traditional code coverage computation requires the complete source code of the codebase to be available for instrumentation and execution. However, there are scenarios where only a partial view of the code is accessible, making code coverage computation impossible using traditional methods. 

In cases where limited code availability poses a challenge, the Code Coverage Prediction approach can be employed. For example, when utilizing an AI code generation service from an IDE, developers may transmit only a partial view of the code to the server where the AI model resides. In this scenario, the server can use the proposed approach to predict the code coverage of the AI-generated test cases on the given method. This enables estimation of the code coverage without the need for the entire codebase, addressing privacy concerns and network limitations. The predicted code coverage can then be used to make informed decisions, such as generating additional tests if coverage is insufficient or transmitting the generated tests to the user if coverage is satisfactory.

\subsection{Live Coverage}

Live Unit Testing, integrated into various IDEs, allows developers to receive real-time feedback on the impact of code changes on existing tests and identifies whether newly added or modified code is covered by existing tests. In this scenario, the Code Coverage Prediction approach can be applied by replacing the actual execution of test cases with an AI inference call to predict the coverage on the modified or newly added methods. This provides developers with immediate feedback on code coverage without the need for executing the entire test suite. By utilizing LLM-based models for code coverage prediction, developers can streamline the testing process and receive timely insights into the coverage of their code changes.

\section{Conclusion}
In this paper, we introduced the novel task of Code Coverage Prediction, which aims to assess the capabilities of Large Language Models (LLMs) in understanding code execution by accurately predicting the lines of code that are executed based on given test cases. We curated a comprehensive dataset named \dataset, consisting of coverage-annotated methods derived from the HumanEval dataset. This dataset enables researchers to explore and advance code coverage prediction techniques and LLM code understanding.

We evaluated the performance of four state-of-the-art LLMs, namely OpenAI's GPT-4 and GPT-3.5, Google's BARD, and Anthropic's Claude, on the Code Coverage Prediction task. The results demonstrated that GPT-4 achieved the highest performance, with 10.46\% exact match with zero-shot prompting and 24.48\% with multi-shot prompting. However, none of the models, including GPT-4, achieved high accuracy in predicting code coverage, indicating that LLMs still have a long way to go in developing a deep understanding of code execution.

The Code Coverage Prediction task serves as a valuable metric for assessing code understanding and can potentially contribute to the enhancement of LLMs' overall performance on code-related tasks. By training models to excel in this task, we can improve their ability to comprehend code execution dynamics, which is crucial for tasks such as code generation and test generation.










%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
%\bibliographystyle{ACM-Reference-Format}
%\bibliography{main}


%\bibliographystyle{IEEEtran}
% \bibliographystyle{ACM-Reference-Format}
\bibliography{main}
\bibliographystyle{acl_natbib}


\newpage


\appendix

\section{\dataset Example}

% ---------- Data Example ----------------
% Figure environment removed
% ---------- END Code Example ----------------


% \section{Data Collection}
% \MICHELE{This section will probably be removed entirely. However, I'm thinking whether we could use some of these paragraphs to describe future work for the dataset expansion. Like data augmentation, mutation, etc.}
% In order to train our models to predict code coverage, we require a large amount of data including methods, tests, and corresponding coverage information. To collect this data, we first extract test cases for open source GitHub repositories, then we execute test cases individually while measuring their coverage. We also employ data augmentation techniques to expand our dataset.


% \subsection{Test Extraction}
% We begin by cloning GitHub repositories locally and analyze them in order to discover and extract test cases. Our approach can be applied to any programming languages, as long as sufficient data is extracted. In this paper we provide details for extracting tests in Java, Csharp, and python, but similar steps can be performed on other programming languages. We automatically analyze the source code of the repositories to identify test cases. The identification relies on language-specific rules. For example, in Java test cases are marked with the \texttt{@Test} annotation, in Csharp tests can be marked using annotations such as \texttt{[Test]}, \texttt{[TestMethod]}, \texttt{[Fact]}, \texttt{[Theory]} based on the testing framework used by the developers. In python tests are usually defined using the prefix \texttt{test\_} in their signature. We use these rules to identify the tests, extract their source code and fully qualified signatures.


% \subsection{Test Execution \& Coverage}
% In this stage, execute the extracted test cases individually using the corresponding testing framework (\eg JUnit, MSTest, pytest). During the execution, we also enable the coverage computation using existing tools such as Cobertura for Java, Coverage.py for python, and the included coverage tool for MSTest.

% Next, for each test case $t$, we analyze the corresponding coverage report obtained by the test execution in order to extract the corresponding focal method $m$ as well as the annotated coverage $\mathrm{cov}(m,t)$. The coverage report is often organized as an xml file where each source code line in the repository is annotated with coverage information, specifying whether the line has been executed or not.

% Given the test case $t$, we analyze the coverage report and we define as focal method $m$ the method which has the following properties: (i) has a direct invocation in the test $t$; (ii) the first statement (signature) is marked as executed in the coverage report.

% Once the focal method $m$ is determined for the test case $t$, we extract the corresponding annotated coverage $\mathrm{cov}(m,t)$ from the coverage report. At the end of this process, we obtained a dataset where each data point is formed by a triplet $d = \{ m, t, \mathrm{cov}(m,t) \}$.


% \subsection{Data Augmentation}
% We employ two data augmentation techniques to increase the number of data points in our dataset.

% \subsubsection{Mutation}
% We perform random mutation (\ie small modifications in a piece code according to mutation operators) at statement- and branch-level of tests and methods. Given the pair of focal method and test $(m, t)$ we can randomly mutate the method to obtain the new pair $(m', t)$ or similarly mutate the test case and obtain a new pair $(m, t')$. Note that multiple types of mutations can be applied to obtain more pairs. Each new pair is then executed and coverage is obtained. These mutations may or may not affect the coverage obtained by the test, however they are still meaningful data points that can be used to train the model to discern whether specific differences in source code (mutations) influence the coverage.


% \subsubsection{Automated Test Generation}
% We can generate new tests by relying on automated test generation techniques, thus increasing our dataset.

% To do so, we employ classic test generation techniques that are guided by coverage, such as EvoSuite \cite{fraser2011evosuite}, as well as novel AI-based techniques such as AthenaTest \cite{DBLP:journals/corr/abs-2009-05617}.

% Both classes of techniques provide meaningful data for our model. The classic test generation technique, while creating simpler, and machine-alike tests, can be used to train our model in predicting coverage for those types of tests, which are often used by large organization. AI-based techniques are able to generate more realistic and developer-alike tests, which can augment the existing dataset extracted from open source repository.


% \section{Use Case Scenarios}

% \label{sec:scenarios}
% This approach could represent a viable alternative to execution-based coverage in several circumstances, such as: (i) program build and execution is too expensive; (ii) code coverage needs to be invoked multiple times iteratively; (iii) only a snippet of code is available, but not the entire program (e.g. server-side scenario).

% In the following subsections we'll provide some examples of these use case scenarios.

% \subsection{Expensive Build \& Execution}
% Large software projects containing millions of lines of code and hundreds 
%  of dependencies may require a long and expensive build. It is fairly normal for these project to have a build that requires hours to complete.

% Let's consider the scenario where a developer is working on creating new tests for an existing set of methods, belonging to a large codebase. The developer would like to analyze the code coverage obtained by these newly written tests, without waiting for the expensive and time consuming build phase. The developer relies on the proposed approach to predict the coverage obtained by the new tests on the existing methods, without needing to build the project and execute the tests. This allows the developers to quickly understand whether they need more tests to cover missed lines/branches in the methods.


% \subsection{Fine-Grained Code Coverage Analysis}
% To compute code coverage, the software project needs to be instrumented and tests executed on the instrumented codebase. However, when a group of tests are executed in parallel on the instrumented codebase, the obtained coverage represents the aggregated coverage for all the executed tests. Specifically, the coverage accounts for all the lines/branches covered by the tests, without fine-grained details on which test covers a specific line/branch.

% In order to obtain fine-grained code coverage for each individual test, the tests need to be executed individually one-at-a-time (in sequence, not in parallel) and code coverage shall be saved and reset after each test execution. This process is very time consuming, since test executions' cannot be parallelized and additional overhead is introduced. 

% Let's consider the scenario where a developer wants to obtain fine-grained code coverage for each individual test case. This fine-grained code coverage would allow the developer to only run a subset of tests based on what has been changed in future commits (\ie this problem is referred to as Test Selection problem in literature). To do so, instead of running each individual test one-at-a-time and saving/resetting the coverage, the developers decides to use our proposed approach to predict the code coverage of each test case. This allows the developer to quickly obtain fine-grained code coverage.


% \subsection{Limited Code Availability}
% Classic code coverage computation requires the entire codebase's source code to be available, instrumented, and executable. There are several scenarios where only a partial view of the code is available and code coverage is required. This is currently not possible with the existing code coverage computation but is feasbile with our approach.

% Let's consider the scenario where a developer is using an AI code generation service from the developer's IDE (\eg Visual Studio Code). The developer asks the AI service to generate test cases for an existing method. The code of the method, including some surrounding context, is transmitted to the server where the AI model resides. Note that only a partial view of the code is transmitted to the server, this is due to privacy reasons as well as network considerations. The AI model (on the server-side) received the input method and generates a set of candidate test cases. These test cases cannot be executed on the server-side since they require the entire codebase (available only at the user-side). In order to estimate the code coverage obtained by the AI-generated tests on the given method, the server invokes our proposed approach. The infrastructure on the server-side analyzes the code coverage results and decides whether to generate additional tests (\ie tests are missing lines/branches) or transmit the generated tests to the user (\ie the code coverage is satisfactory).

% Our approach allows code coverage computation even with limited code availability.


%  \subsection{Live Coverage}
% Live Unit Testing is a technique, integrated in several IDEs such as VS, which involves the execution of tests in background while the developer is making changes to an application. This is intended to provide feedback to developers on how the changes impacted existing tests and whether the new added code is covered by one or more existing tests.

% Our approach can be used in this scenario by replacing the actual execution of test cases with an AI inference call that predicts the new coverage on modified or newly added methods. 


\newpage

\section{Deployed Systems}
We deploy our approach in two systems covering some of the use cases described in the paper.

\subsection{System A - Live Coverage}
Figure \ref{fig:system_A} shows the deployment of System A, which provides live coverage prediction for developers directly into their IDE. System A supports the scenario where a developer is writing tests for a given method (\eg \texttt{Fibonacci(n)}) in their codebase. System A provides live coverage information (bottom of Figure \ref{fig:system_A}) where lines covered by the tests are marked with \colorbox{green}{>} and highlighted in green and the line missed are marked with \colorbox{red}{!} and highlighted in red.

The benefits provided by System A are the following: (i) no need to build the entire codebase; (ii) no need to execute the tests; (iii) live and lightweight coverage prediction.


% Figure environment removed


\subsection{System B - Test Generation with Coverage}
Figure \ref{fig:system_B} shows the deployment of System B, which provides Test Suites with a coverage guarantee. System B supports the scenario where a developer is requesting test cases for a given method and would like to obtain a certain degree of coverage on the method under test. Once the method is transmitted to the Test Generation Service, the Test Generation Model (\ie an AI-based test generation tool or any other tool) outputs a first batch of test case candidates. The Coverage Prediction Model analyzes these tests and the method under test, and predicts the coverage that these tests achieve on the method. If the coverage is satisfactory (w.r.t. a given criteria and threshold) the tests are transmitted to the IDE and shown to the developer. If the tests do not meet the criteria in terms of coverage, the Test Generation Service requests additional tests from the Test Generation Model (optionally, providing the specific lines/branches which still need to be covered). 

The benefits provided by System B are the following: (i) automated test generation with coverage guarantees; (ii) lightweight generation without need of build and test execution on the user side.


% Figure environment removed










% % ---------- Code Example ----------------
% % Figure environment removed
% % ---------- END Code Example ----------------




% \balance
%\begin{appendix}
%\section{Examples}
%\include{examples}
%\end{appendix}



\end{document}
\endinput