% ****** Start of file apssamp.tex ******
%
%   This file is part of the APS files in the REVTeX 4.2 distribution.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 The American Physical Society.
%
%   See the REVTeX 4 README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% See the REVTeX 4 README file
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex apssamp.tex
%  2)  bibtex apssamp
%  3)  latex apssamp.tex
%  4)  latex apssamp.tex
%
\documentclass[%
 %reprint,
superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps, prl,
% draft,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
]{revtex4}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{enumitem}
\usepackage{placeins}
%\usepackage{algpseudocode}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{hyperref}% add hypertext capabilities
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

%\usepackage[showframe,%Uncomment any one of the following lines to test 
%%scale=0.7, marginratio={1:1, 2:3}, ignoreall,% default settings
%%text={7in,10in},centering,
%%margin=1.5in,
%%total={6.5in,8.75in}, top=1.2in, left=0.9in, includefoot,
%%height=10in,a5paper,hmargin={3cm,0.8in},
%]{geometry}

\begin{document}


\title{Supplemental Material}


\author{Pablo Calvo-Barl\'es}
\affiliation{Instituto de Nanociencia y Materiales de Arag\'on (INMA), CSIC-Universidad de Zaragoza, 50009 Zaragoza, Spain\looseness=-1}
\affiliation{Departamento de F\'isica de la Materia Condensada, Universidad de Zaragoza, 50009 Zaragoza, Spain\looseness=-1}

\author{Sergio G. Rodrigo}
\affiliation{Instituto de Nanociencia y Materiales de Arag\'on (INMA), CSIC-Universidad de Zaragoza, 50009 Zaragoza, Spain\looseness=-1}
\affiliation{Departamento de F\'isica Aplicada, Universidad de Zaragoza, 50009 Zaragoza, Spain\looseness=-1}

\author{Eduardo S\'anchez-Burillo}
\affiliation{PredictLand S.L., 50001 Zaragoza, Spain\looseness=-1}

\author{Luis Mart\'in-Moreno}
\email[]{lmm@unizar.es}
\affiliation{Instituto de Nanociencia y Materiales de Arag\'on (INMA), CSIC-Universidad de Zaragoza, 50009 Zaragoza, Spain\looseness=-1}
\affiliation{Departamento de F\'isica de la Materia Condensada, Universidad de Zaragoza, 50009 Zaragoza, Spain\looseness=-1}




\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

%\keywords{Suggested keywords}%Use showkeys class option if keyword
                              %display desired
\maketitle

%\tableofcontents


%%%%% N-th ROOT %%%%%

\section{n-th root problem}

In this section, we present the technical details of the SSNN training algorithm and data generation for the n-th complex root problem $w = z^n$. We extend our exploration to various root problems, including the complex third root (shown in the main text), the real second root, and the complex fourth and fifth roots. Additionally, we provide visual representations to enhance the understanding of the obtained activity values.

% Figure environment removed

\subsubsection{SSNN training}

For all the NN calculations in this paper, we utilized Tensorflow \cite{tensorflow2015-whitepaper} and Keras \cite{chollet2015keras} libraries, with Adam \cite{kingma2017adam} as the gradient-descent optimizer for the loss function. Each n-th root problem was trained with 5000 training samples and 1000 validation samples. The $z$ values were randomly generated from a uniform distribution within the region $V = \{ z \in \mathbb{C} \; | \; |z| \leq 10 \}$ ($\mathbb{R}$ for the second root). Data was normalized as $z_i^{\mathrm{norm}} = z_i / 10$ and $w_i^{\mathrm{norm}} = z_i / 10^n$.

The standard NN architecture consisted of an input layer (2 neurons for the third, fourth, and fifth roots, and 1 neuron for the second), followed by two dense hidden layers with 10 neurons each using a sigmoid activation function. The latent layer had no activation function, only weights and biases. A learning rate of 0.01 and a mini-batch size of 128 samples were used. The SSNN was trained for 8000 epochs at each $M$-stage to ensure loss convergence. The maximum number of branches was $M_{\mathrm{max}}=7$ for each n-th root problem. Supplementary results for the second, fourth, and fifth roots can be observed in Fig. \ref{fig:FIG_1}.

\subsubsection{Branch activities}

We now show the visual representation of predictions and activities from active and inactive branches. Fig. \ref{fig:FIG_2} corresponds to the second root problem.  We represent, as function of the training input samples $\{ \vec{y}_i \}$, the predictions of each branch: $\{ \vec{x}^{(0)}_{\mathrm{NN}} (\vec{y}_i) \}, ..., \{ \vec{x}^{(\alpha)}_{\mathrm{NN}} (\vec{y}_i) \} ,..., \{ \vec{x}^{(M)}_{\mathrm{NN}} (\vec{y}_i) \}$. Three different $M$-stages are shown: $M=K$ (Fig. \ref{fig:FIG_2}a), $M=K+1$ (Fig. \ref{fig:FIG_2}b) and $M=K+2$ (Fig. \ref{fig:FIG_2}c). The inset displays the activities of the branches with the same color as the curves.  Active branches accurately fit the function $x = \pm \sqrt{y}$ throughout the full range of the parameter space $V$, used in the training,  while inactive branches do not provide a correct approximation, consistent with the $A_{\alpha}$ values.  This is in accordance with what is expected from the $A_{\alpha}$ values.

Figs. \ref{fig:FIG_3}, \ref{fig:FIG_4} and \ref{fig:FIG_5} correspond to the third, fourth and fifth root, respectively. In these figures, we only represent the parameter space (complex plane $z$).  Two types of scatter plots are shown for each training stage $M=K$, $M=K+1$ and $M=K+2$. The first type (upper row in the figures) displays $\{ \vec{x}^{(\alpha)}_{\mathrm{NN}} (\vec{y}_i) \}$ for $\alpha = 1,...,M$.  As expected, the active branches cover the irreducible regions maximally. However, small regions in $V$, particularly near the boundaries between irreducible regions, are not fully covered by the active branches. Notice that, close to boundaries, two branches provide predictions with similar losses.  This makes the inactive branches specialize in covering these small regions, explaining why $A_{\alpha}$ is not exactly zero for the inactive branches. The second type (lower row) plots the output samples $\{ \vec{x}_i \}$ from the training data set (uniformly distributed). The color of each point represents the branch with the minimum MSE prediction for that sample, indicating the area of the parameter space covered by each branch, directly related to its activity. The corresponding activities are shown in the insets.


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


\newpage

\section{Optical transmission in multilayer stack}

In this section, we outline the calculation details of the optical transmission problem discussed in the main text. First, we show how to calculate the transmission spectrum from a given set of dielectric materials. Subsequently, we present the SSNN training specifics, along with the convergence of the predicted symmetry matrices to the ground truth matrices.
\subsubsection{Transmission spectrum calculation}

We study a photonic system described in the third section of the main text.  The system consists of three stacked layers (labeled as 1, 2, and 3) with the same width $d$ and dielectric constants $\varepsilon_1$, $\varepsilon_2$, and $\varepsilon_3$ (see Fig. \ref{fig:FIG_6}). In our calculations, we set $d = 250$ nm, and $\varepsilon_i$ varies within the range $[2,5]$. Media $0$ and $0'$ are treated as vacuum ($\varepsilon_0 = \varepsilon_{0'} = 1$).

An incident electromagnetic wave incident, with wavelength $\lambda$, impinging normally into the system.  This incident wave is transmitted and reflected at each interface. We express the magnitude of the electric field in each medium as follows:
%
\begin{equation}
   E_0 = e^{ikz} + re^{-ikz},
\end{equation}
%
\begin{equation}
   E_j = A_j e^{i k_j \left( z-(j-1)d \right) } + B_j e^{-i k_j \left( z-(j-1)d \right) },
\end{equation}
%
\begin{equation}
   E_{0'} = t e^{ik(z-3d)}.
\end{equation}
%
In the expressions above, $A_j$ and $B_j$ are the complex amplitudes of upgoing and downgoing waves, respectively,  at each medium $j$, while $t$ and $r$ are are the complex transmission and reflection coefficients of the whole system. Besides, $k = 2 \pi / \lambda$ is the wavevector at the vacuum and $k_j = \sqrt{\varepsilon_j}k$ is the wavevector at the medium $j$.  Additionally, $k = 2\pi/\lambda$ denotes the wavevector in vacuum, and $k_j = \sqrt{\varepsilon_j}k$ is the wavevector in medium $j$. Using these electric field magnitudes, we can calculate the magnetic field modules $H_j$ by applying the following relations derived from Maxwell's equations:
%
\begin{equation}
   -\vec{u}_z \times \vec{H_j} = \pm y_j \vec{E_j},
\end{equation}
%
In the expressions above, $\vec{u}_z$ represents the unitary vector in the $z$ direction, and $y_j = \sqrt{\varepsilon_j}$ is the modal admittance. The positive sign is used for the transmitted modes, and the negative sign is used for the reflected modes. Once we have obtained the electric and magnetic fields, we apply the boundary conditions at each interface:
%
\begin{equation}
   E_{j}(jd) = E_{j+1}(jd),
\end{equation}
%
\begin{equation}
   H_{j}(jd) = H_{j+1}(jd),
\end{equation}
%
for $j = 0,1,2,3$. We then obtain a linear system of 8 equations with 8 unknowns, $r, A_1, B_1, A_2, B_2, A_3, B_3$ and $t$:
%
\begin{equation}
\begin{cases}
   r - A_1 - B_1 = -1 \\
   -r - y_1 A_1 + y_1 B_1 = -1 \\
   A_1 e^{i k_1 d} + B_1 e^{-i k_1 d} - A_2 - B_2 = 0 \\
   y_1 e^{i k_1 d} A_1 - y_1 e^{-i k_1 d} B_1 - y_2 A_2 + y_2 B_2 = 0 \\
   e^{i k_2 d} A_2 + e^{-i k_2 d} B_2 - A_3 - B_3 = 0 \\
   y_2 e^{i k_2 d} A_2 - y_2 e^{-i k_2 d} B_2 - y_3 A_3 + y_3 B_3 = 0 \\
   -t + e^{i k_3 d} A_3 + e^{-i k_3 d} B_3 = 0 \\
   -t + y_3 e^{i k_3 d} A_3 - y_3 e^{-i k_3 d} B_3 = 0.
\end{cases}
\end{equation}

To calculate the transmission coefficient $t$ for a given wavelength $\lambda$, we solve this linear system.  After this, the transmittance is obtained as denoted as $T(\lambda) = |t|^2$. For the SSNN training data set, we calculate transmittance spectra by considering a discretized set of $m = 100$ equidistant wavelengths between $\lambda_{\mathrm{min}} = 500$ nm and $\lambda_{\mathrm{max}} = 900$ nm. An illustrative example is presented in Fig. \ref{fig:FIG_7}. 

% Figure environment removed

% Figure environment removed

\subsubsection{SSNN training}

We have trained the SSNN with 9000 training samples and 1000 validation samples. For the SSNN, we have normalized the input samples $\vec{\varepsilon}_i$ and output samples $\vec{T}_i$ as follows: $\vec{\varepsilon}_i^{\mathrm{norm}} = \left( \vec{\varepsilon}_i - (2,2,2)^T \right)/3$ and $\vec{T}_i^{\mathrm{norm}} = \left( \vec{T}_i - (T_{\mathrm{min}}, ..., T_{\mathrm{min}})^T \right) /(T_{\mathrm{max}} - T_{\mathrm{min}})$, being $T_{\mathrm{max}}$ and $T_{\mathrm{min}}$ the highest and lowest transmission values in the training data set, respectively.  
%
The standard section of the SSNN architecture consists of an input layer with $m = 100$ neurons and two non-linear hidden layers, each with 200 neurons and sigmoid activation function. The latent layer is linear (i.e., we have not used an activation function). During training, we used a learning rate of 0.01 and a mini-batch size of 100 samples. The SSNN was trained for 2000 epochs for each $M$-stage, and the maximum number of branches was set to $M_{\mathrm{max}}=7$.

In Fig. \ref{fig:FIG_8}, we present the evolution of matrix distances between each learned matrix $\hat{D}_{\alpha}^{\mathrm{NN}}$ and the inversion transformation in the order of the dielectric constants $\hat{D}_2$ (a symmetry in parameter space originating from the time reversal ) during the first training stage ($M = 7$). Interestingly, only one curve decays to $10^{-2}$. 

% Figure environment removed

\newpage

\section{ENERGY SPECTRUM IN MOLECULES}

In this final section, we provide the calculation details of the molecule eigenspectrum problem as discussed in the main text. Firstly, we outline the method to calculate the eigenenergies of each molecular system. Subsequently, we present the SSNN training details, including the network hyperparameters and the branch activities at different $M$-stages. Finally, we demonstrate the convergence of the predicted symmetry matrices to the ground truth ones for each molecule.

\subsubsection{Eigenvalues calculation}
We consider the three types of 4-atom molecules introduced in the main text: rectangular, squared, and tetrahedral.  We describe these systems with a tight-binding model with one atomic level per atom. Hopping is assumed to occur only between nearest neighbors. The Hamiltonians of each system are:
%
\begin{equation}
\hat{H}_R = \begin{pmatrix}
e_1 & J_x & 0 & J_y\\
J_x & e_2 & J_y & 0\\
0 & J_y & e_3 & J_x\\
J_y & 0 & J_x & e_4
\end{pmatrix},
\end{equation}
%
\begin{equation}
\hat{H}_S = \begin{pmatrix}
e_1 & J & 0 & J\\
J & e_2 & J & 0\\
0 & J & e_3 & J\\
J & 0 & J & e_4
\end{pmatrix},
\end{equation}
%
\begin{equation}
\hat{H}_T = \begin{pmatrix}
e_1 & J & J & J\\
J & e_2 & J & J\\
J & J & e_3 & J\\
J & J & J & e_4
\end{pmatrix},
\end{equation}
%
where $\{ e_i \}$ are the atomic-energies and $J , J_x , J_y$ are the hopping terms. The eigenvalues ${ E_i }$ are obtained by diagonalizing these matrices. 

For the training data set, we generate random uniform atomic energies within specified ranges, while fixing the values of the hopping terms. In the rectangular molecule, we take $e_i \in [1,30]$, $J_x = 10.0$, and $J_y = 7.62$. In the square molecule, $e_i$ is within the range $[10,20]$, and $J$ is set to $10.0$. In the tetrahedral molecule, $e_i$ is within $[10,20]$ and $J$ is set to $1.0$.


\subsubsection{SSNN training}

% Figure environment removed

For each molecule, we have trained the SSNN with 70000 training samples and 30000 validation samples. The standard section of the SSNN is composed by an input layer with $m = 4$ neurons and 2 non-linear hidden layers with 100 neurons each, using sigmoid activation functions. The latent layer is linear.  The learning rate was set to 0.01, and a mini-batch size of 200 samples was used.  We trained the SSNN for 3000 epochs for each $M$-stage. The maximum number of branches was set to $M_{\mathrm{max}}=8$ for the rectangle, $M_{\mathrm{max}}=12$ for the square, and $M_{\mathrm{max}}=30$ for the tetrahedron.

Fig. \ref{fig:FIG_9} displays the branch activities for each molecular system at three different stages: $M = K+2$, $M = K+1$ and $M = K$. In all cases, the values $A_{\alpha}$ indicate the presence of $K$ active branches and $M-K$ inactive branches.

Figs. \ref{fig:FIG_10}, \ref{fig:FIG_11} and \ref{fig:FIG_12} depict the evolution of the distance  between each learned matrix $\hat{D}_{\alpha}^{\mathrm{NN}}$ and each ground truth transformation $\hat{D}_{\beta}$ during the first training stage $M = M_{\mathrm{max}}$. As expected, these plots demonstrate that each ground truth symmetry matrix is discovered by a different active branch.

% Figure environment removed

% Figure environment removed

% Figure environment removed


% The \nocite command causes all entries in a bibliography to be printed out
% whether or not they are actually referenced in the text. This is appropriate
% for the sample file to show the different styles of references, but authors
% most likely will not want to use it.
\nocite{*}

\newpage

%\bibliography{apssamp.bib}% Produces the bibliography via BibTeX.
\providecommand{\noopsort}[1]{}\providecommand{\singleletter}[1]{#1}%
\begin{thebibliography}{3}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname bibnamefont\endcsname\relax
  \def\bibnamefont#1{#1}\fi
\expandafter\ifx\csname bibfnamefont\endcsname\relax
  \def\bibfnamefont#1{#1}\fi
\expandafter\ifx\csname citenamefont\endcsname\relax
  \def\citenamefont#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\providecommand{\bibinfo}[2]{#2}
\providecommand{\eprint}[2][]{\url{#2}}

\bibitem[{\citenamefont{Abadi et~al.}(2015)\citenamefont{Abadi, Agarwal,
  Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin
  et~al.}}]{tensorflow2015-whitepaper}
\bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Abadi}},
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Agarwal}},
  \bibinfo{author}{\bibfnamefont{P.}~\bibnamefont{Barham}},
  \bibinfo{author}{\bibfnamefont{E.}~\bibnamefont{Brevdo}},
  \bibinfo{author}{\bibfnamefont{Z.}~\bibnamefont{Chen}},
  \bibinfo{author}{\bibfnamefont{C.}~\bibnamefont{Citro}},
  \bibinfo{author}{\bibfnamefont{G.~S.} \bibnamefont{Corrado}},
  \bibinfo{author}{\bibfnamefont{A.}~\bibnamefont{Davis}},
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Dean}},
  \bibinfo{author}{\bibfnamefont{M.}~\bibnamefont{Devin}},
  \bibnamefont{et~al.}, \emph{\bibinfo{title}{{TensorFlow}: Large-scale machine
  learning on heterogeneous systems}} (\bibinfo{year}{2015}),
  \bibinfo{note}{software available from tensorflow.org},
  \urlprefix\url{https://www.tensorflow.org/}.

\bibitem[{\citenamefont{Chollet et~al.}(2015)}]{chollet2015keras}
\bibinfo{author}{\bibfnamefont{F.}~\bibnamefont{Chollet}} \bibnamefont{et~al.},
  \emph{\bibinfo{title}{Keras}}, \bibinfo{howpublished}{\url{https://keras.io}}
  (\bibinfo{year}{2015}).

\bibitem[{\citenamefont{Kingma and Ba}(2017)}]{kingma2017adam}
\bibinfo{author}{\bibfnamefont{D.~P.} \bibnamefont{Kingma}} \bibnamefont{and}
  \bibinfo{author}{\bibfnamefont{J.}~\bibnamefont{Ba}},
  \emph{\bibinfo{title}{Adam: A method for stochastic optimization}}
  (\bibinfo{year}{2017}), \eprint{1412.6980}.

\end{thebibliography}



\end{document}
%
% ****** End of file apssamp.tex ******
