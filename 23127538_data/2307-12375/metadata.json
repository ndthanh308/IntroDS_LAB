{
  "title": "In-Context Learning Learns Label Relationships but Is Not Conventional Learning",
  "authors": [
    "Jannik Kossen",
    "Yarin Gal",
    "Tom Rainforth"
  ],
  "submission_date": "2023-07-23T16:54:41+00:00",
  "revised_dates": [
    "2023-08-08T00:41:40+00:00",
    "2023-10-04T00:19:41+00:00",
    "2024-03-14T01:02:14+00:00"
  ],
  "abstract": "The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input--label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.",
  "categories": [
    "cs.CL",
    "cs.AI",
    "cs.LG"
  ],
  "primary_category": "cs.CL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12375",
  "pdf_url": null,
  "comment": "Accepted for publication at ICLR 2024",
  "num_versions": null,
  "size_before_bytes": 46155597,
  "size_after_bytes": 896031
}