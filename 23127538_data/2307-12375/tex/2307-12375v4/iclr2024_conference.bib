@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017},
  url={https://arxiv.org/abs/1706.03762},
}

@misc{falcon2023,
  title = {Falcon LLM},
  author = {TII, Technology Innovation Institute},
  url= {https://falconllm.tii.ae/},
  year = {2023}, 
  note = "[Online; accessed 10-July-2023]"
}




@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv:2204.02311},
  url={https://arxiv.org/abs/2204.02311},
  year={2022}
}

@inproceedings{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  booktitle={NeurIPS},
  url={https://arxiv.org/abs/2203.15556},
  year={2022}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={NeurIPS},
  year={2020},
  url={https://arxiv.org/abs/2005.14165},
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/abs/2302.13971},
}


@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv:2205.01068},
  year={2022},
  url={https://arxiv.org/abs/2205.01068}
}

@misc{huszar2023implicit,
  title = {Implicit Bayesian Inference in Large Language Models},
  author = {Husz√°r, Ferenc},
  url= {https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/},
  year = {2023}, 
  note = "[Online; accessed 10-July-2023]"
}

@inproceedings{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={ICLR},
  year={2022},
  url={https://arxiv.org/abs/2111.02080},
}

@article{hahn2023theory,
  title={A theory of emergent in-context learning as implicit structure induction},
  author={Hahn, Michael and Goyal, Navin},
  journal={arXiv:2303.07971},
  year={2023},
  url={https://arxiv.org/abs/2303.07971}
}

@article{jiang2023latent,
  title={A latent space theory for emergent abilities in large language models},
  author={Jiang, Hui},
  journal={arXiv:2304.09960},
  year={2023},
  url={https://arxiv.org/abs/2304.09960},
}

@article{zhang2023and,
  title={What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization},
  author={Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv:2305.19420},
  url={https://arxiv.org/abs/2305.19420},
  year={2023}
}


@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={ICML},
  year={2023},
  url={https://arxiv.org/abs/2212.07677}
}


% ICL papers

% CoT papers
@inproceedings{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={Advances in neural information processing systems},
  url={https://arxiv. org/abs/2205.11916},
  year={2022}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  booktitle={NeurIPS},
  year={2022},
  url={https://arxiv.org/abs/2201.11903}
}

@article{turpin2023language,
  title={Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel R},
  journal={arXiv:2305.04388},
  year={2023},
  url={https://arxiv.org/abs/2305.04388}
}

% example selection
@inproceedings{zhang2022active,
  title={Active example selection for in-context learning},
  author={Zhang, Yiming and Feng, Shi and Tan, Chenhao},
  booktitle={EMNLP},
  year={2022},
  url={https://arxiv.org/abs/2211.04486}
}

@inproceedings{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3$?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  booktitle={ACL},
  url={https://arxiv.org/abs/2101.06804},
  year={2022}
}


@inproceedings{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  booktitle={ACL},
  url={https://arxiv.org/abs/2104.08786},
  year={2022},
}

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={ICML},
  year={2021},
  url={https://arxiv.org/abs/2102.09690},
}

@inproceedings{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={ACL},
  year={2022},
  url={https://arxiv.org/abs/2202.12837},
}

@article{li2023finding,
  title={Finding supporting examples for in-context learning},
  author={Li, Xiaonan and Qiu, Xipeng},
  journal={arXiv:2302.13539},
  url={https://arxiv.org/abs/2302.13539},
  year={2023}
}

@inproceedings{chang2022careful,
  title={Careful Data Curation Stabilizes In-context Learning},
  author={Chang, Ting-Yun and Jia, Robin},
  booktitle={EMNLP},
  url={https://arxiv.org/abs/2212.10378},
  year={2022}
}

@inproceedings{agrawal2022context,
  title={In-context examples selection for machine translation},
  author={Agrawal, Sweta and Zhou, Chunting and Lewis, Mike and Zettlemoyer, Luke and Ghazvininejad, Marjan},
  url={https://arxiv.org/abs/2212.02437},
  booktitle={ACL},
  year={2023}
}

@inproceedings{webson2021prompt,
  title={Do prompt-based models really understand the meaning of their prompts?},
  author={Webson, Albert and Pavlick, Ellie},
  booktitle={NAACL},
  year={2022},
  url={https://arxiv.org/abs/2109.01247}
}


@article{chen2022relation,
  title={On the relation between sensitivity and accuracy in in-context learning},
  author={Chen, Yanda and Zhao, Chen and Yu, Zhou and McKeown, Kathleen and He, He},
  journal={arXiv:2209.07661},
  url={https://arxiv.org/abs/2209.07661},
  year={2022}
}

% Meta-ICL
@inproceedings{chen2021meta,
  title={Meta-learning via language model in-context tuning},
  author={Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
  booktitle={ACL},
  url={https://arxiv.org/abs/2110.07814},
  year={2022}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv:2212.08073},
  url={https://arxiv.org/abs/2212.08073},
  year={2022}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv:2211.09110},
  url={https://arxiv.org/abs/2211.09110},
  year={2022}
}

@inproceedings{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  booktitle={ICLR},
  url={https://arxiv.org/abs/2112.10510},
  year={2022}
}

@article{chan2022transformers,
  title={Transformers generalize differently from information stored in context vs in weights},
  author={Chan, Stephanie CY and Dasgupta, Ishita and Kim, Junkyung and Kumaran, Dharshan and Lampinen, Andrew K and Hill, Felix},
  journal={arXiv:2210.05675},
  url={https://arxiv.org/abs/2210.05675},
  year={2022}
}

@inproceedings{dasgupta2022distinguishing,
  title={Distinguishing rule and exemplar-based generalization in learning systems},
  author={Dasgupta, Ishita and Grant, Erin and Griffiths, Tom},
  booktitle={ICML},
  year={2022},
  url={https://arxiv.org/abs/2110.04328},
}

@article{wies2023learnability,
  title={The learnability of in-context learning},
  author={Wies, Noam and Levine, Yoav and Shashua, Amnon},
  journal={arXiv:2303.07895},
  url={https://arxiv.org/abs/2303.07895},
  year={2023}
}


@inproceedings{kudo2018sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "EMNLP",
    url={https://arxiv.org/abs/1808.06226},
    year = "2018",
}



@article{ziegler2019fine,
  title={Fine-tuning language models from human preferences},
  author={Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  journal={arXiv:1909.08593},
  year={2019},
  url={https://arxiv.org/abs/1909.08593},
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={NeurIPS},
  url={https://arxiv.org/abs/2203.02155},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv:2204.05862},
  url={https://arxiv.org/abs/2204.05862},
  year={2022}
}


@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={NeurIPS},
  url={https://arxiv.org/abs/1706.03741},
  year={2017}
}

@misc{openai2023,
  author = {OpenAi},
  title = {GPT-4 API general availability and deprecation of older models in the Completions API
},
  url= {https://openai.com/blog/gpt-4-api-general-availability},
  year = {2023}, 
  note = "[Online; accessed 10-July-2023]"
}

@article{stamatatos2009survey,
  title={A survey of modern authorship attribution methods},
  author={Stamatatos, Efstathios},
  journal={American Society for information Science and Technology},
  year={2009},
  url={https://onlinelibrary.wiley.com/doi/10.1002/asi.21001}
}

@article{radford2018improving,
  title={Improving language understanding with unsupervised learning},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  journal={Technical report, OpenAI},
  url={https://openai.com/research/language-unsupervised},
}

@inproceedings{liu2018generating,
  title={Generating wikipedia by summarizing long sequences},
  author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  booktitle={ICLR},
  year={2018},
  url={https://arxiv.org/abs/1801.10198}
}

@article{williams1989learning,
  title={A learning algorithm for continually running fully recurrent neural networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural computation},
  year={1989},
  url={https://ieeexplore.ieee.org/document/6795228},
}


@article{der2009aleatory,
  title={Aleatory or epistemic? Does it matter?},
  author={Der Kiureghian, Armen and Ditlevsen, Ove},
  journal={Structural safety},
  year={2009},
  url={https://doi.org/10.1016/j.strusafe.2008.06.020},
}


@article{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	journal = {NeurIPs},
	author = {Kendall, Alex and Gal, Yarin},
	year = {2017},
        url={https://arxiv.org/abs/1703.04977},
}


@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={EMNLP},
  year={2013},
  url={https://aclanthology.org/D13-1170/}
}

@book{murphy2022probabilistic,
  title={Probabilistic machine learning: an introduction},
  author={Murphy, Kevin P},
  year={2022},
  publisher={MIT press},
  url={https://probml.github.io/pml-book/book1.html}
}

@article{fong2020marginal,
  title={On the marginal likelihood and cross-validation},
  author={Fong, Edwin and Holmes, Chris C},
  journal={Biometrika},
  year={2020},
  url={https://arxiv.org/abs/1905.08737},
}

@inproceedings{akyurek2023what,
title={What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={ICLR},
year={2023},
url={https://arxiv.org/abs/2211.15661}
}


@inproceedings{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  booktitle={NAACL},
  year={2022},
  url={https://arxiv.org/abs/2110.15943},
}

@article{kirsch2022general,
  title={General-purpose in-context learning by meta-learning transformers},
  author={Kirsch, Louis and Harrison, James and Sohl-Dickstein, Jascha and Metz, Luke},
  journal={arXiv:2212.04458},
  year={2022},
  url={https://arxiv.org/abs/2212.04458},
}


@article{chan2022data,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  journal={NeurIPS},
  url={https://arxiv.org/abs/2205.05055},
  year={2022}
}


@article{garnelo2018neural,
  title={Neural processes},
  author={Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
  journal={arXiv:1807.01622},
  url={https://arxiv.org/abs/1807.01622},
  year={2018}
}

@inproceedings{kossen2021self,
 author = {Kossen, Jannik and Band, Neil and Lyle, Clare and Gomez, Aidan N and Rainforth, Tom and Gal, Yarin},
 booktitle = {NeurIPS},
 title = {Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning},
 year = {2021},
 url={https://arxiv.org/abs/2106.02584},
}


@inproceedings{gordon2018meta,
  title={Meta-learning probabilistic inference for prediction},
  author={Gordon, Jonathan and Bronskill, John and Bauer, Matthias and Nowozin, Sebastian and Turner, Richard E},
  url={https://arxiv.org/abs/1805.09921},
  booktitle={ICLR},
  year={2019}
}

@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv:2207.05221},
  url={https://arxiv.org/abs/2207.05221},
  year={2022}
}

@article{lin2022teaching,
  title={Teaching models to express their uncertainty in words},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={TMLR},
  year={2023},
  url={https://arxiv.org/abs/2205.14334},
}

@article{gonen2022demystifying,
  title={Demystifying prompts in language models via perplexity estimation},
  author={Gonen, Hila and Iyer, Srini and Blevins, Terra and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv:2212.04037},
  year={2022},
  url={https://arxiv.org/abs/2212.04037}
}

@article{wu2023reasoning,
  title={Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks},
  author={Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Aky{\"u}rek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  journal={arXiv:2307.02477},
  url={https://arxiv.org/abs/2307.02477},
  year={2023}
}

@inproceedings{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  booktitle={EMNLP},
  url={https://arxiv.org/abs/2202.07206},
  year={2022}
}

@article{Malo2014GoodDO,
  title={Good debt or bad debt: Detecting semantic orientations in economic texts},
  author={P. Malo and A. Sinha and P. Korhonen and J. Wallenius and P. Takala},
  journal={Journal of the Association for Information Science and Technology},
  year={2014},
  url={https://arxiv.org/abs/1307.5336},
}



@inproceedings{wang2012baselines,
  title={Baselines and bigrams: Simple, good sentiment and topic classification},
  author={Wang, Sida I and Manning, Christopher D},
  booktitle={ACL},
  year={2012},
  url={https://dl.acm.org/doi/10.5555/2390665.2390688},
}

@inproceedings{gibert2018hate,
    title = "{Hate Speech Dataset from a White Supremacy Forum}",
    author = "de Gibert, Ona  and
      Perez, Naiara  and
      Garc{\'\i}a-Pablos, Aitor  and
      Cuadros, Montse",
    booktitle = "ACL Workshop on Abusive Language Online ({ALW}2)",
    year = "2018",
    url = "https://www.aclweb.org/anthology/W18-5102",
}

@inproceedings{Zhang2015CharacterlevelCN,
  title={Character-level Convolutional Networks for Text Classification},
  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  booktitle={NeurIPS},
  year={2015},
  url={https://arxiv.org/abs/1509.01626},
}



@article{mccreery2020effective,
      title={Effective Transfer Learning for Identifying Similar Questions: Matching User Questions to COVID-19 FAQs}, 
      author={Clara H. McCreery and Namit Katariya and Anitha Kannan and Manish Chablani and Xavier Amatriain},
      year={2020},
      journal={arXiv:2008.13546},
      url={https://arxiv.org/abs/2008.13546}
}


@inproceedings{wang2019glue,
  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  booktitle={ICLR},
  url={https://arxiv.org/abs/1804.07461},
  year={2019}
}



@inproceedings{dolan2005automatically,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, Bill and Brockett, Chris},
  booktitle={Third International Workshop on Paraphrasing (IWP2005)},
  year={2005},
  url={https://aclanthology.org/I05-5002/},
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012},
  url={https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf},
}

@inproceedings{dagan2005pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine learning challenges workshop},
  year={2005},
  url={https://link.springer.com/chapter/10.1007/11736790_9},
}


@inproceedings{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  url={https://arxiv.org/abs/1910.03771},
  booktitle={EMNLP: System Demonstrations},
  year={2020}
}

@inproceedings{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {NeurIPS},
year = {2019},
url = {https://pytorch.org/},
}


@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv:2303.03846},
  url={https://arxiv.org/abs/2303.03846},
  year={2023}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv:2210.11416},
  url={https://arxiv.org/abs/2210.11416},
  year={2022}
}

@inproceedings{pan2023incontext,
  title={What In-Context Learning "Learns" In-Context: Disentangling Task Recognition and Task Learning}, 
  author={Jane Pan and Tianyu Gao and Howard Chen and Danqi Chen},
  url={https://arxiv.org/abs/2305.09731},
  booktitle={ACL},
  year={2023}
}


@inproceedings{yoo2022ground,
  title={Ground-truth labels matter: A deeper look into input-label demonstrations},
  author={Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-goo and Kim, Taeuk},
  url={https://arxiv.org/abs/2205.12685},
  booktitle={EMNLP},
  year={2022}
}


@inproceedings{si2023measuring,
  title={Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations},
  author={Si, Chenglei and Friedman, Dan and Joshi, Nitish and Feng, Shi and Chen, Danqi and He, He},
  url={https://arxiv.org/abs/2305.13299},
  booktitle={ACL},
  year={2023}
}

@article{han2023context,
  title={In-Context Learning of Large Language Models Explained as Kernel Regression},
  author={Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  journal={arXiv:2305.12766},
  url={https://arxiv.org/abs/2305.12766},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv:2307.09288},
  url={https://arxiv.org/abs/2307.09288},
  year={2023}
}

@inproceedings{garcia2023unreasonable,
  title={The unreasonable effectiveness of few-shot learning for machine translation},
  author={Garcia, Xavier and Bansal, Yamini and Cherry, Colin and Foster, George and Krikun, Maxim and Johnson, Melvin and Firat, Orhan},
  booktitle={ICML},
  year={2023},
  url={https://arxiv.org/abs/2302.01398},
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv:2112.11446},
  url={https://arxiv.org/abs/2112.11446},
  year={2021}
}


@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  year={2023},
  url={https://arxiv.org/abs/2107.13586},
}


@inproceedings{webson2021prompt,
  title={Do prompt-based models really understand the meaning of their prompts?},
  author={Webson, Albert and Pavlick, Ellie},
  booktitle={NAACL},
  url={https://arxiv.org/abs/2109.01247},
  year={2022}
}


@inproceedings{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  booktitle={ACL},
  year={2021},
  url={https://arxiv.org/abs/2012.15723},
}
