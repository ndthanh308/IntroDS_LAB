
\documentclass{article} %
\usepackage{iclr2024_conference,times}

\input{math_commands.tex}
\setlength {\marginparwidth }{3.2cm}

\usepackage[color=lightgray]{todonotes}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=[rgb]{0.00, 0.45, 0.70},
    linkcolor=[rgb]{0.01, 0.62, 0.45},
    urlcolor=[rgb]{0.80, 0.47, 0.74},
    }

\usepackage{url}
\usepackage{booktabs}       %
\usepackage[english]{babel}

\usepackage{csquotes}


\DeclareQuoteStyle[american]{english}
    {\textquoteleft}
    {\textquoteright}
    {\textquoteleft}
    {\textquoteright}

\usepackage[capitalize]{cleveref}
\newcommand{\creflastconjunction}{, and\nobreakspace}


\title{In-Context Learning Learns Label Relationships but Is Not Conventional Learning}



\author{%
  Jannik Kossen$^{1}$\thanks{Correspondence to jannik.kossen@cs.ox.ac.uk.\, \textsuperscript{$\Delta$} Equal advising.}
  \phantom{123} \phantom{123} \phantom{123} \phantom{123}
  Tom Rainforth$^{2}$\textsuperscript{$\Delta$}
  \phantom{123} \phantom{123} \phantom{123} \phantom{123}
  Yarin Gal$^{1}$\textsuperscript{$\Delta$}
  \\[1.5em]
  $^1$ OATML, Department of Computer Science, University of Oxford \\
  $^2$ Department of Statistics, University of Oxford \\
}


\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \nabla \or \Delta \or *\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother


\author{%
  Jannik Kossen$^{1}$\thanks{Correspondence to jannik.kossen@cs.ox.ac.uk.\, \textsuperscript{$\Delta$} Equal advising.}
  \phantom{123} \phantom{123} \phantom{123} \phantom{123}
  Yarin Gal$^{1}$\textsuperscript{$\Delta$}
  \phantom{123} \phantom{123} \phantom{123} \phantom{123}
  Tom Rainforth$^{2}$\textsuperscript{$\Delta$}
  \\[1.5em]
  $^1$ OATML, Department of Computer Science, University of Oxford \\
  $^2$ Department of Statistics, University of Oxford \\
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %

\newcommand{\jk}[1]{{\color{green} [\textbf{jk}: #1]}}

\usepackage{enumitem}
\setlist{nosep}
\usepackage{siunitx}

\usepackage{printlen}
\graphicspath{{figs/}}
\usepackage{placeins}
\usepackage{xcolor}         %
\definecolor{pal0}{rgb}{0.00, 0.45, 0.70}
\definecolor{pal1}{rgb}{0.87, 0.56, 0.02}
\definecolor{pal2}{rgb}{0.01, 0.62, 0.45}
\definecolor{pal3}{rgb}{0.84, 0.37, 0.00}
\definecolor{pal4}{rgb}{0.80, 0.47, 0.74}
\definecolor{pal5}{rgb}{0.65, 0.34, 0.16}
\definecolor{pal6}{rgb}{0.97, 0.51, 0.75}
\definecolor{pal7}{rgb}{0.58, 0.58, 0.58}
\definecolor{pal8}{rgb}{0.87, 0.87, 0.00}
\definecolor{pal10}{rgb}{0.34, 0.71, 0.91}
\definecolor{pal9}{rgb}{0.93, 0.88, 0.20}

\definecolor{palbg}{rgb}{0.91, 0.94, 0.99}
\usepackage{rotating}

\usepackage{pdflscape}

\usepackage{tcolorbox}
\usepackage{xstring}
\usepackage{etoolbox}

\def\forlistlooptwo#1#2#3{%
    \ifboolexpr{test{\IfSubStr{#2}{,}} and test{\IfSubStr{#3}{,}}}{%
        \forlistlooptwohelper{#1}#2;#3;%
    }{%
        \ifboolexpr{test{\notblank{#2}} and test{\notblank{#3}}}{%
            #1{#2}{#3}%
        }{}%
    }%
}
\def\forlistlooptwohelper#1#2,#3;#4,#5;{%
    #1{#2}{#4}%
    \forlistlooptwo{#1}{#3}{#5}%
}

\usepackage{setspace}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage[normalem]{ulem}


\begin{document}
\fancypagestyle{lscapedplain}{%
  \fancyhf{}
  \fancyfoot{}
}


\maketitle 

\begin{abstract}
The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input--label relationship in the context.
However, there is currently no consensus about \emph{how} this in-context learning (ICL) ability of LLMs works.
For example, while \citet{xie2021explanation} liken ICL to a general-purpose learning algorithm, \citet{min2022rethinking} argue ICL does not even learn label relationships from in-context examples.
In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations.
To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided.
Our experiments show that ICL predictions almost always depend on in-context labels, and that ICL can learn truly novel tasks in-context.
However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data, and, further, that ICL does not consider all in-context information equally.
\end{abstract}





\vspace{-4mm}
\section{Introduction}
\label{sec:intro}
\vspace{-1mm}
\citet{brown2020language} have shown that Large Language Models (LLMs) \citep{radford2019language,chowdhery2022palm,hoffmann2022training,zhang2022opt} can perform so-called \emph{in-context learning} (ICL) of supervised tasks.
In contrast to standard in-weights learning, e.g.~gradient-based finetuning of model parameters, ICL requires no parameter updates.
Instead, examples of the input--label relationship of the downstream task are simply prepended to the query for which the LLM predicts.
This is sometimes also referred to as \emph{few-shot ICL} to differentiate from other ICL variants that do not use example demonstrations \citep{liu2023pre}.
Few-shot ICL is widely used, e.g.~in all LLM publications cited above, to improve predictions across a variety of established NLP tasks, such as sentiment or document classification, question answering, or natural language inference.












However, there is currently no consensus on \emph{why} ICL improves predictions, with prior work presenting a large variety of often contradictory perspectives.
For example, \citet{brown2020language} highlight similarities between the behavior of ICL and conventional learning algorithms, such as improvements with model size and number of examples.
Since then, some have argued that ICL works because it implements conventional, general-purpose learning algorithms such as Bayesian inference or gradient descent \citep{xie2021explanation,huszar2023implicit,hahn2023theory,jiang2023latent,zhang2023and,von2023transformers,akyurek2023what,han2023context}.
In contrast, others have highlighted practical shortcomings of ICL, suggesting ICL does not really `learn' from examples in the way one expects \citep{liu2021makes,lu2021fantastically,zhao2021calibrate,chen2022relation,agrawal2022context,chang2022careful,razeghi2022impact,li2023finding,wei2023larger}.
In particular, \citet{min2022rethinking} claim their \enquote{findings suggest that [LLMs] do not learn new tasks at test time} in the sense of \enquote{capturing the input-label correspondence}.
Clearly, these claims, if true, are not compatible with the behavior we would expect from a conventional learning algorithm.










% Figure environment removed



\looseness=-1
In this paper, we address the pressing need to obtain an improved understanding of how information given in-context affects ICL predictions.
Concretely, we study a set of questions that encode our beliefs about how ICL should behave if it incorporates label information like a conventional learner:
(1)~Does ICL take the input--label relationship of the in-context examples into account when predicting for test queries?
(2) Is ICL powerful enough to overcome prediction preferences originating from pre-training?
(3) Does ICL treat all information provided in-context equally?
To study these questions rigorously, we rephrase these them as null hypotheses that we study empirically.
Our results yield an improved understanding of the similarities and differences between ICL and conventional learners.
This is important, given that ICL is often used as a convenient replacement for the latter.
















Unlike prior work, we study in detail how ICL predictions evolve as an increasing number of examples are provided, from no examples at all up to the maximum possible, across a range of LLMs and tasks.
We further show that using probabilistic metrics better highlights the resulting ICL dynamics, often revealing large changes in the confidence of ICL predictions, even when accuracy metrics barely change at all.
These measures ensure we obtain a comprehensive picture of ICL behavior.




\looseness=-1
In our experiments, we first examine if ICL predictions depend on the labels of in-context examples by studying how probabilistic metrics react to randomized in-context labels (\S\ref{sec:random}, \cref{fig:random_teaser_LLaMa-2-70B}).
Further, we study ICL on a truly novel task the LLM \emph{cannot} know from pre-training (\S\ref{sec:author_id}).
Both experiments show that ICL typically considers in-context label relations.
We then investigate if ICL is powerful enough to overcome prediction preferences learned from pre-training data (\S\ref{sec:flipped_arbitrary}).
In our experiments, we find this is typically not the case as ICL performance plateaus if label relations oppose pre-training preference.
Further, while additional prompting can improve ICL here, we ultimately do not find prompts that lead to the desired behavior.
Finally, we study if ICL treats all information provided in-context equally (\S\ref{sec:time_series}).
This is important when the context contains diverging information about a label relationship.
By modifying label relations during ICL, we find it does not treat all in-context information equally, and, instead, ICL preferentially makes use of information closer to the query.



\looseness=-1
In summary, our results suggest a new middle ground regarding the capabilities and limitations of ICL.
While ICL can learn from label information, it does so differently than conventional learners.
Our findings thus contribute to a better understanding of information processing in ICL, which, in turn, is crucial to our ability to deploy LLMs safely and effectively.
For example, \citet{bai2022constitutional} suggest to use ICL for alignment, which relies on ICL being able to sufficiently adjust LLM behavior.















\vspace{-3mm}
\section{Background}
\label{sec:background}
\vspace{-2mm}

A large and growing body of prior work studies ICL.
We here highlight those studies that are most relevant to our motivation, and discuss other related work later in \S\ref{sec:related_work}.

\looseness=-1
Since its introduction by \citet{brown2020language}, few-shot ICL has become an integral part of LLM evaluations:
e.g.~many recent publications rely on few-shot ICL tasks, such as the popular HELM benchmark \citep{liang2022holistic}, to evaluate their LLMs \citep{chowdhery2022palm,hoffmann2022training}.

\looseness=-1
In the wake of ICL's success, follow up work has speculated if ICL implements a general purpose learning algorithm such as gradient descent \citep{von2023transformers} or Bayesian inference \citep{xie2021explanation}.
This line of work expresses the sentiment that ICL captures not just how much LLMs have learned during pre-training but, rather, how much LLMs have \emph{learned how to learn} novel supervised tasks in-context.
However, so far arguments have been largely theoretical, lacking solid experimental evidence in actual LLMs \citep{zhang2023and,huszar2023implicit,wies2023learnability,jiang2023latent}.


\looseness=-1
Conversely, a variety of studies have highlighted unexpected shortcomings of ICL.
For example, ICL can be sensitive to the formatting \citep{min2022rethinking} or order \citep{lu2021fantastically} of the in-context learning examples.
Further, LLMs prefer to predict labels that are common in the pre-training data \citep{zhao2021calibrate}, they can predict drastically different for similar prompts \citep{chen2022relation}, and they rely on task formulations similar to those observed in the pre-training data \citep{wu2023reasoning}.





In particular, \citet{min2022rethinking} claim that ICL does not learn label relationships from in-context examples and that ICL \enquote{performance drops only marginally when labels in the demonstrations are replaced by random labels}.
Further, they suggest that, instead of learning input--label relationships, ICL only works because the model learns about the general label space, the formatting of the examples, and their input distribution.
They assert that ICL does \enquote{not learn new tasks at test time} and that \enquote{ground truth demonstrations are in fact not required} in many common scenarios.
In the first part of our evaluation, we will revisit and ultimately disagree with these claims.



\vspace{-3mm}
\section{Null Hypotheses on How ICL Incorporates Label Information}
\label{sec:hypotheses}
\vspace{-2mm}

It is important we obtain a better understanding of how ICL uses information about the input--label relationship provided in-context.
We therefore study how far analogies between ICL and conventional learning algorithms extend.
If ICL does not behave like a conventional learning algorithm, we should significantly scale back expectations of what it can achieve.
For example, if ICL cannot sufficiently adjust LLM predictions, it may be inappropriate to use it for LLM alignment.


\looseness=-1
By \emph{learning algorithm}, we here refer to an algorithm that predicts the label of a test input given a training set of input--label pairs.
Although we have an intuitive notion of what a \emph{conventional} learning algorithm is, providing a precise definition is difficult.
However, we can list properties that popular machine learning algorithms tend to abide by.
If ICL does not conform to these properties, we should not think of it as a conventional learning algorithm.
As a basis for our investigation, we construct three null hypotheses that we will subsequently try to falsify.
Our first hypothesis follows from the notion that conventional learners make use of the conditional distribution of labels given inputs.






















 \emph{\textbf{Null Hypothesis 1 (NH1)}: ICL predictions are independent of the conditional label distribution of the examples given in-context.}

We will investigate NH1 in multiple ways.
In \S\ref{sec:random}, we revisit and refine the randomized in-context label experiment of \citet{min2022rethinking}:
in addition to revising their results for point predictions, we propose the use of \emph{probabilistic metrics} to study if label randomization really does not affect ICL predictive beliefs.
Then, in \S\ref{sec:author_id}, we study ICL on a novel task that we create and which ICL can \emph{only} solve by learning a novel label relationship that it cannot know from pre-training.


Next, we study \emph{how} ICL incorporates label information.
The pre-trained model already contains information towards many NLP tasks: even without any ICL, predictions are significantly better than random guessing.
This raises the question of how information given in-context interacts with this \emph{pre-training preference}.
In typical applications of conventional learners, we would expect that predictions eventually follow the label relation of the training examples when provided with sufficient data.
A learner that does not conform to this is inherently limited in what it can learn.
With NH2, we study if ICL conforms to these intuitions and can overcome the initial pre-training preference.



\emph{\textbf{Null Hypothesis 2 (NH2)}: ICL can overcome the zero-shot prediction preferences of the pre-trained model.}


In \S\ref{sec:flipped_arbitrary}, we modify in-context label relationships and study how this changes ICL predictions.
If NH2 is true, ICL should eventually predict according to any label relation given in-context.






























\looseness=-1
Our last hypothesis relies on the following typical property of conventional learning algorithms: they will consider all information in the examples equally.
If a dataset contains multiple sources of information about a label relationship, e.g.~the dataset itself is a union of multiple datasets, then all information is considered equally by the learner.
NH3 investigates if this is holds true for ICL as well.





\emph{\textbf{Null Hypothesis 3 (NH3)}: ICL considers all information given in-context equally.}


In \S\ref{sec:time_series}, we study non-stationary input distributions for which label relations \emph{change} during ICL. If NH3 is true, ICL predictions should not depend on the order in which we present label relations.








\vspace{-1mm}
\section{Experimental Setup \& ICL Training Dynamics}
\label{sec:dynamics}
\vspace{-2mm}
% Figure environment removed


\looseness=-1
We here detail our experimental setup for the subsequent evaluation of few-shot ICL behavior.


\textbf{Models \& Tasks.}
We employ LLMs from the LLaMa-2 \citep{touvron2023llama2}, LLaMa \citep{touvron2023LLaMa}, and Falcon \citep{falcon2023} families due to their strong performance and open source nature.
We evaluate on SST-2, Subjective (Subj.), Financial Phrasebank (FP), Hate Speech (HS), AG News (AGN), MQP, MRPC, RTE, and WNLI.
We provide citations for all tasks in \S\ref{sec:dataset_citations}.


\looseness=-1
\textbf{Context Size.}
We always report few-shot ICL performance across \emph{all possible} numbers of in-context demonstrations, i.e.~from zero-shot performance up to the maximum number of examples within the LLMs' input token limit.
This is in contrast to prior work, which often evaluates few-shot ICL at only a few context set sizes, and allows us to obtain a comprehensive picture of ICL \emph{`training dynamics'}.



\textbf{Computationally Cheap ICL Evaluations.}
We propose a novel evaluation strategy that obtains ICL predictions at all possible numbers of in-context demonstrations without incurring any additional cost.
Concretely, we exploit the fact that each forward pass through the model gives not just a prediction for the next token, but rather, the predicted probabilities for \emph{each} input token (given all preceding tokens).
By extracting those token predictions that correspond to labels of in-context examples, we obtain few-shot ICL predictions at all in-context dataset sizes with each forward pass.
We refer to \S\ref{sec:eval_approach} for a formalization of few-shot ICL and further description of our evaluation strategy.


\looseness=-1
\textbf{Evaluation Metrics.}
We evaluate few-shot ICL performance in terms of accuracy ($\uparrow$) and (average) log likelihood ($\uparrow$) of label predictions.
We also report entropy, which, while not a performance metric, is useful for understanding how much predicted probabilities are spread over classes.
We average metrics over sets of in-context examples drawn randomly and without replacement from the training set, and we compare to a guessing baseline that predicts with probabilities equal to class frequencies.


\looseness=-1
\textbf{Default Training Dynamics.}
Before modifying label relationships in the following sections, \cref{fig:default_falcon_sst2} shows standard few-shot ICL training dynamics for Falcon models on SST-2.
We observe reasonable behavior for all models:
as more in-context examples are observed, accuracies and log likelihoods increase, while entropies decrease.
Notably, log likelihoods and entropies show in-context \emph{learning} more clearly: predicted probabilities continue to improve at larger context sizes, whereas accuracies saturate quickly.
Differences between models are more noticeable for probabilistic metrics, too: entropies reveal that larger or instruction-tuned Falcon models predict with higher certainty on SST-2.
Similar findings also hold for LLaMa and LLaMA-2 models, for which we provide results in \cref{fig:learning_curve_sst2}.






\vspace{-1mm}
\section{Do ICL Predictions Depend on In-Context Labels?}
\label{sec:random}
\vspace{-2mm}
We now study the null hypotheses (NH) formulated in \S\ref{sec:hypotheses}, starting with NH1, which states that ICL predictions are independent of the conditional label distribution of the in-context examples.
To this end, we first revisit the experiments of \citet{min2022rethinking}, replacing all labels of in-context examples with labels drawn randomly from the training set of the task.
If NH1 is true, then accuracy, log-likelihood, and entropy should be identical for the randomized and standard label scenario.
We note that, while we believe the results of this experiment are already sufficient to reject NH1, the experiments in \S\ref{sec:author_id}â€“\S\ref{sec:time_series} will provide additional strong evidence for this conclusion.








\textbf{Observations \& Discussion.}
We evaluate NH1 across all our models, tasks, and metrics, computing full ICL training curves as introduced in \S\ref{sec:dynamics}.
\Cref{fig:random_teaser_LLaMa-2-70B} shows log likelihoods for LLaMa-2-70B for a selection of tasks, and \cref{fig:random_falcon_sst2} shows all metrics for all Falcon models on SST-2.
We observe significant differences in ICL behavior for the default and randomized label scenario.
As the context size grows, likelihoods eventually degrade significantly for randomized labels.
In \cref{fig:random_falcon_sst2}, we can further see entropies increase when randomizing labels.
This is reasonable from a probabilistic learning perspective: as noisy labels are observed, estimates of uncertainty will typically increase.
While differences are large for probabilistic log likelihood and entropy, they can be harder to spot for accuracy.
In \cref{fig:default_falcon_sst2}, only Falcon-40B experiences a sizeable accuracy decrease.
(For LLaMa(-2) models, accuracy decreases more frequently and can approach guessing level, cf.~\cref{fig:random_multi_LLaMa-2-7B,fig:random_multi_LLaMa-2-13B,fig:random_multi_LLaMa-2-70B,fig:random_multi_LLaMa-7B,fig:random_multi_LLaMa-13B,fig:random_multi_LLaMa-65B,fig:random_multi_LLaMa-7B,fig:random_multi_LLaMa-13B,fig:random_multi_LLaMa-65B}.)







We provide results for label randomization across all our models, tasks, and metrics: we invite the reader to view the full set of training curves in \S\ref{sec:extended_results} and provide a summary of the results in \cref{tab:random_summary_main}.
It shows the average difference in log likelihoods between the default and randomized labels at the maximum number of demonstrations for each task and model.
We gray out entries where ICL on default labels does not outperform the guessing baseline as we are only interested in studying label randomization when ICL works in the first place.
When default label performance is better than random (black entries), differences are almost always significantly positive (bold entries), indicating ICL performs worse for randomized labels.
\emph{\textbf{%
Based on these results, we reject NH1 that ICL predictions do not depend on the conditional label distribution of in-context examples.}}


% Figure environment removed



\begin{table}[t]
\vspace{-2mm}
\caption{%
    \looseness=-1
    Average differences between ICL log likelihoods for default and randomized labels.
    Bold entries indicate differences are statistically significant.
    We can disregard lightgray entries: for them, default ICL performance is not significantly better than a random guessing baseline.
    Whenever default ICL outperforms the baseline, ICL almost always performs significantly worse (positive differences) for random labels.
    Averages over \num{500} runs at max.~context size, standard errors in \cref{tab:random_summary_app}.
}
\label{tab:random_summary_main}
\centering
\vspace{-2mm}
\begin{adjustbox}{max width=1\textwidth}
\begin{tabular}{lrrrrrrrrr}
\toprule
$\Delta$ Log Likelihood & SST-2 & Subj & FP & HS & AGN & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{0.42}$ & $\bm{0.39}$ & $\bm{0.57}$ & $\bm{0.18}$ & $\bm{0.53}$ & \textcolor{lightgray}{$\bm{0.03}$} & \textcolor{lightgray}{$0.02$} & $\bm{0.03}$ & \textcolor{lightgray}{$0.02$} \\
LLaMa-2 13B & $\bm{0.41}$ & $\bm{0.62}$ & $\bm{0.49}$ & $\bm{0.24}$ & $\bm{0.81}$ & $\bm{0.04}$ & \textcolor{lightgray}{$0.01$} & $\bm{0.06}$ & \textcolor{lightgray}{$0.02$} \\
LLaMa-2 70B & $\bm{0.51}$ & $\bm{0.53}$ & $\bm{0.57}$ & $\bm{0.34}$ & $\bm{0.80}$ & $\bm{0.29}$ & $\bm{0.04}$ & $\bm{0.22}$ & $\bm{0.18}$ \\
Falcon 7B & $\bm{0.20}$ & $\bm{0.19}$ & $\bm{0.25}$ & $\bm{0.06}$ & $\bm{0.31}$ & \textcolor{lightgray}{$0.01$} & \textcolor{lightgray}{$0.01$} & \textcolor{lightgray}{$-0.01$} & \textcolor{lightgray}{$0.01$} \\
Falcon 7B Instr. & $\bm{0.13}$ & $\bm{0.08}$ & $\bm{0.11}$ & $\bm{0.03}$ & $\bm{0.15}$ & \textcolor{lightgray}{$0.03$} & \textcolor{lightgray}{$0.02$} & \textcolor{lightgray}{$-0.00$} & \textcolor{lightgray}{$0.00$} \\
Falcon 40B & $\bm{0.34}$ & $\bm{0.35}$ & $\bm{0.31}$ & $\bm{0.18}$ & $\bm{0.90}$ & $\bm{0.06}$ & \textcolor{lightgray}{$0.01$} & $0.01$ & \textcolor{lightgray}{$0.02$} \\
Falcon 40B Instr. & $\bm{0.25}$ & $\bm{0.37}$ & $\bm{0.27}$ & $0.02$ & $\bm{0.77}$ & $\bm{0.06}$ & \textcolor{lightgray}{$0.02$} & $0.02$ & \textcolor{lightgray}{$\bm{0.04}$} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-4mm}
\end{table}




\looseness=-1
Notably, \cref{tab:random_summary_main} shows that LLaMa-2-70B, our largest and most capable model, always performs worse under label randomization.
This suggests the importance of labels in ICL will increase as models become more powerful in the future.
However, performance often degrades significantly even for smaller models, although they struggle to reach better than random performance on the entailment tasks MQP, MRPC, RTE, and WNLI.
Occasionally, likelihoods improve despite random labels, e.g.~for the small Falcon models in \cref{fig:random_falcon_sst2}.
Following \citet{pan2023incontext,min2022rethinking}, we attribute this to ICL `recognizing', rather than learning, the task from the random label demonstrations.
However, we note that, even here, there are significant performance gaps to the default scenario.
We conclude that label randomization adversely affecting ICL predictions is the rule not the exception.














\looseness=-1
\textbf{Discussion of \citet{min2022rethinking}}.
Lastly, we discuss possible reasons for why \citet{min2022rethinking} arrive at the conclusion that label randomization only \enquote{barely hurts} ICL performance:
(1) They do not study probabilistic metrics, which are more sensitive to randomization.
(2) They use a fixed ICL dataset size of \num{16}, but effects of random labels increase with growing contexts.
(3) Only one model they study has more than 20B parameters (GPT-3), but we observe that larger models react more to randomization.
(\citet{pan2023incontext} also observe this, cf.~\S\ref{sec:related_work}.)
(4) On some tasks, performance for \citet{min2022rethinking} could be close to random guessing, where label randomization has less of an effect.


\vspace{-2mm}
\section{Can ICL Learn Truly Novel Label Relationships?}
\label{sec:author_id}
\vspace{-1mm}
\looseness=-1
The results of \S\ref{sec:random} show that ICL predictions \emph{do} depend on the label relationship of in-context examples.
Here, we explore the \emph{extent} to which ICL can extract label information from the context.
Concretely, we study if LLMs can learn \emph{truly novel} label relationships in-context.
To do this, we create a task that is guaranteed to not appear in the pre-training data.
The task needs to be distinct from established NLP tasks, for which the pre-training data could be contaminated and for which, often, strong zero-shot performance shows the model has learned the task, perhaps implicitly, during pre-training.














Specifically, we create an authorship identification \citep{stamatatos2009survey} dataset from private messages between two authors of this paper.
The task is to identify the author corresponding to a given message.
As messages stem from private communication, they are guaranteed to not be part of the pre-training corpus.
For ICL to succeed here, it needs to learn the novel input--label relationship provided in-context: while the LLM could have some general notion of authorship identification tasks, the specific input--label relationship is definitely novel, as the authors' private writing styles cannot be known to the LLM.
We give further details on the task in \S\ref{sec:author_details}.

% Figure environment removed

\looseness=-1

\looseness=-1
\textbf{Observations \& Discussion.}
\Cref{fig:author_id} shows that ICL for LLaMa-2 models succeeds at learning the author identification task.
Accuracies and log likelihoods increase, agreeing with expectations about conventional learning.
Larger models outperform smaller ones, but all models quickly outperform the random guessing baseline.
This also holds true for LLaMa and Falcon models, for which we additionally provide results in \cref{fig:author_id_all_models}.
We conclude that \textbf{\emph{LLMs can learn truly novel tasks in-context}}, correctly inferring the label relation from examples.
These results also strongly support our previous rejection of NH1 as, clearly, ICL predictions must depend on labels to learn the novel task.



\vspace{-2mm}
\section{Can ICL Overcome Pre-Training Preference?}
\label{sec:flipped_arbitrary}
\vspace{-1mm}

With NH2, we explore how in-context label information trades off against the LLM's \emph{pre-training preference}, i.e.~its zero-shot predictions based on label relationships inferred from pre-training and stored in the model parameters.
Often, pre-training preference and in-context label relationships agree: e.g.~in \cref{fig:random_falcon_sst2}, performance is high zero-shot and then improves with ICL.
To test NH2 if ICL can overcome pre-training preference, we create scenarios where pre-training preference and in-context observations are not aligned.
We then study if ICL behavior is compatible with fully overcoming pre-training preference as we would expect from a conventional learner.



 


\looseness=-1
Concretely, we use replacement label relationships when constructing the in-context examples.
(1)~We flip the default labels, e.g.~(negative, positive) get mapped to (positive, negative) for SST-2.
(2) We study arbitrary labels, e.g.~(negative, positive) become (A, B) or (B, A)---we deliberately choose \emph{arbitrary} labels here such that the LLM should not have a significant preference for assigning them to positive or negative.
We then evaluate ICL performance for predicting the \emph{replacement label relationship}, e.g.~the flipped labels in (1).
Note that, we rely on the flipped label experiments to evaluate NH2; results on arbitrary labels serve to complete the picture, and we discuss them later.









% Figure environment removed

\begin{table}[t]
\caption{%
    \looseness=-1
    Average differences between ICL entropies for default and flipped labels.
    Bold entries indicate differences are statistically significant.
    Again, we disregard entries for which default ICL performance is not significantly better than the guessing baseline (lightgray entries).
    When default ICL outperforms the baseline, ICL entropies are almost always significantly different between scenarios.
    We average \num{100} runs, report results at maximum context size, and show standard errors in \cref{tab:flipped_summary_app}.
}
\label{tab:flipped_summary_main}
\centering
\vspace{-.75mm}
\begin{adjustbox}{max width=1\textwidth}
\begin{tabular}{lrrrrrrrrr}
\toprule
$\Delta$ Entropy & SST-2 & Subj & FP & HS & AGN & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{-0.52}$ & $0.01$ & $\bm{-0.40}$ & $\bm{-0.10}$ & $\bm{-0.75}$ & \textcolor{lightgray}{$-0.00$} & \textcolor{lightgray}{$\bm{0.05}$} & $\bm{-0.03}$ & \textcolor{lightgray}{$-0.01$} \\
LLaMa-2 13B & $\bm{-0.48}$ & $\bm{-0.06}$ & $\bm{-0.47}$ & $\bm{-0.19}$ & $\bm{-0.95}$ & $\bm{-0.03}$ & \textcolor{lightgray}{$\bm{-0.07}$} & $\bm{-0.11}$ & \textcolor{lightgray}{$\bm{-0.07}$} \\
LLaMa-2 70B & $\bm{-0.17}$ & $\bm{-0.10}$ & $\bm{-0.40}$ & $\bm{-0.26}$ & $\bm{-1.00}$ & $\bm{-0.24}$ & $\bm{-0.15}$ & $\bm{-0.26}$ & $\bm{-0.21}$ \\
Falcon 7B & $\bm{-0.28}$ & $\bm{-0.12}$ & $-0.02$ & $\bm{-0.06}$ & $\bm{-0.52}$ & \textcolor{lightgray}{$0.00$} & \textcolor{lightgray}{$0.00$} & \textcolor{lightgray}{$-0.00$} & \textcolor{lightgray}{$-0.01$} \\
Falcon 7B Instr. & $\bm{-0.37}$ & $\bm{-0.07}$ & $\bm{-0.33}$ & $\bm{-0.05}$ & $\bm{-0.22}$ & \textcolor{lightgray}{$\bm{-0.02}$} & \textcolor{lightgray}{$\bm{0.13}$} & \textcolor{lightgray}{$0.01$} & \textcolor{lightgray}{$0.00$} \\
Falcon 40B & $\bm{-0.39}$ & $\bm{-0.23}$ & $\bm{-0.42}$ & $\bm{-0.19}$ & $\bm{-0.90}$ & $-0.00$ & \textcolor{lightgray}{$\bm{-0.10}$} & $-0.02$ & \textcolor{lightgray}{$-0.00$} \\
Falcon 40B Instr. & $\bm{-0.48}$ & $\bm{-0.16}$ & $\bm{-0.43}$ & $\bm{-0.31}$ & $\bm{-0.92}$ & $\bm{-0.10}$ & \textcolor{lightgray}{$-0.02$} & $\bm{-0.06}$ & $-0.01$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-3mm}
\end{table}




\looseness=-1
\textbf{Observations \& Discussion.}
We evaluate NH2 across all our models, tasks, and metrics.
\Cref{fig:flipped} shows results for a selection of large models and datasets.
Evidently, the LLMs can, to some extent, learn to predict the flipped label relationships against pre-training preference.
Accuracies on flipped labels reach levels significantly better than random guessing.
However, in particular for entropies, there is a consistent gap between the default and flipped label scenarios: ICL predictions on flipped labels are much less certain.
Importantly, given the plateauing behavior, this gap is unlikely to disappear with additional in-context observations.
(Practically speaking, we cannot actually add any additional examples as input size is maximal already and will deteriorate when exceeding the LLMs' input token limit; this is itself a limitation of ICL compared to conventional learning.)
It seems that label relationships inferred from pre-training have a permanent effect that cannot be overcome through in-context observations.
This does not agree with conventional learning: predictions on flipped labels should continue to improve as observations continue to contradict pre-training preference.



\looseness=-1
Crucially, we observe this behavior across models and tasks.
We encourage the reader to view the full set of training curves in \S\ref{sec:extended_results}.
\Cref{tab:flipped_summary_main} provides a summary of the results, showing differences in entropy between default and flipped label scenarios at maximum context size, highlighting statistical significance in bold and graying out entries where ICL fails on default labels.
Across the board, we again observe that predictions on flipped labels plateau: a significant gap between predictions on default and flipped labels remains, even at maximum input size.
For the models we study, \textbf{\emph{we reject NH2 that ICL can overcome prediction preferences from pre-training.}}
Again, the results here strongly support our previous rejection of NH1, as clearly, predictions change for replacement labels.













\Cref{fig:flipped} also shows that for replacement labels (A, B) and (B, A) both directions are similarly easy for ICL to learn. This indicates the LLM truly has not learned a preference for them during pre-training, agreeing with our intuition. Further, learning arbitrary replacement labels is slower than 
\begin{wrapfigure}{r}{0.38\textwidth}
  \centering
  % Figure removed
    \captionsetup{name={Fig.}}
  \vspace{-3mm}
    \caption{\looseness=-1%
    Prompts yield limited gains.
    }
  \label{fig:prompted_sneak}
  \vspace{-7mm}
\end{wrapfigure}
learning from the aligned default labels but faster than learning flipped labels, which does agree with intuitions about the behavior of inductive biases.




\textbf{Can Prompting Help?} 
We further study if specific prompts, i.e.~instructions that inform the LLMs of the flipped labels, can improve ICL predictions.
We find that, while some prompts initially can help the model predict on flipped labels, eventually, prompts no longer improve predictions.
\Cref{fig:prompted_sneak} gives an example of this, and we study prompting across more models and tasks in \S\ref{sec:prompted}.























% Figure environment removed

\vspace{-1mm}
\section{How Does ICL Aggregate In-Context Information?}
\label{sec:time_series}
\vspace{-2mm}
With NH3, we study if ICL considers all in-context information equally.
We have just seen that ICL does not treat pre-training preference equivalently to in-context label information.
However, it is similarly important to understand how ICL treats different sources of purely in-context information.

To test NH3, we change the label relationship \emph{during} in-context learning in three different scenarios.
(D $\rightarrow$ F): after $N$ observations of the \underline{\textbf{d}}efault label relation we \underline{\textbf{f}}lip the label relation for all following observations, e.g.~from (negative, positive) to (positive, negative) for SST-2.
(F $\rightarrow$ D): we now start with $N$ flipped label observations and then expose the model to default labels.
(Alternate F $\leftrightarrow$ D): we alternate between the default and flipped labels after \emph{each} observation.
For all three setups, after $2N$ observations, the model has observed the same number of the flipped and default label examples.
If NH3 is true, ICL should treat all observed label relations equally, no matter their position in the input.
This means, predictions should be the same for all three scenarios after $2N$ observations.










\textbf{Observations \& Discussion.}
\Cref{fig:time_series} shows accuracies for a selection of models and tasks, and we report full probabilistic metrics for all tasks and model combinations for which ICL was able to learn the flipped relationships well in 
\cref{fig:time_series_appendix_sst2,fig:time_series_appendix_subj,fig:time_series_appendix_financial_phrasebank,fig:time_series_appendix_hate_speech,fig:time_series_appendix_ag_news}.
We observe that, across almost all tasks and model combinations, predictions are significantly different between the three setups after observing the same number of examples of both label relationships (after $2N$ total observations, red dashed line in the figure).
\textbf{\emph{We thus reject NH3 that ICL treats all information provided in-context equivalently.}}

\looseness=-1
After the changepoint $N$, predictions immediately begin to adjust to the new label relationship.
In particular, after $2N$ observations, the (F $\rightarrow$ D) setup has a bias for predicting according to the default label relationship, while the (D $\rightarrow$ F) setup has a bias for predicting the flipped label relationship.
In other words, LLMs prefer to use information that is \emph{closer} to the query, instead of considering all available information equally.
Our finding is distinct from \citet{zhao2021calibrate}, who observe that ICL preferentially predicts labels that appear frequently near the query for a single \emph{fixed} label relation.
Lastly, we note once more that the results here also strongly support our previous rejection of NH1.





\vspace{-1mm}
\section{Discussion \& Limitations}
\vspace{-2mm}
\looseness=-1
\textbf{Alignment.} 
For alignment of LLMs, it is crucial to understand how pre-training preference and inputs trade-off, as well as how different parts of the input, such as a context string and user input, interact and influence predictions.
Our results suggest prompt-based alignment \citep{bai2022constitutional} may struggle to overwrite pre-training preference and could itself easily be overcome by future user input.






\textbf{Do Labels Always Matter?}
It is plausible that labels matter less for other NLP tasks such as question answering, where in-context examples may provide limited information towards the answer of the query question.
However, for the randomized label experiment, capable LLMs might still identify that the provided in-context answers are random and imitate this in their predictions.


\looseness=-1
\textbf{Limitations.}
We focus on few-shot ICL tasks where evaluation is based on logits and not free-form generation.
We do this mostly to avoid complications around evaluating free-form generation tasks and have no reason to believe our results should not transfer to this setting.
Further, our experiments do not cover RLHF-finetuned LLMs \citep{christiano2017deep,ziegler2019fine,ouyang2022training}.
We also do not evaluate on closed source models due to concerns around reproducibility.



\vspace{-1mm}
\section{Related Work}
\vspace{-2mm}
\label{sec:related_work}


\looseness=-1
Some recent work has studied the effect of labels in ICL.
\citet{yoo2022ground} also revisit label randomization and find significant variance across tasks and models.
\citet{pan2023incontext} further separate ICL into label-independent and -dependent learning, which they study by replacing labels with arbitrary tokens.
\citet{wei2023larger} find that smaller or instruction-tuned models are less capable when performing ICL with replacement labels.
Similar to \citet{min2022rethinking}, the above studies do not consider probabilistic metrics or full ICL training curves, and thus can underestimate changes in ICL predictions for modified labels.
For example, \citet{pan2023incontext} find that the gap between random and default labels is \enquote{insignificant} for small models, which our results, in particular for probabilistic metrics, contradict, cf.~\S\ref{sec:random}.
Further, \citet{wei2023larger} claim that \enquote{large models can override prior knowledge from pretraining [\dots] in-context} and \enquote{small models do not change their predictions when seeing flipped labels}, which is not supported by our results in \S\ref{sec:flipped_arbitrary}.


More generally, ICL has been the subject of many recent studies.
For example, \citet{min2021metaicl} fine-tune language models to improve ICL, \citet{si2023measuring} measure the inductive bias of ICL predictions,
\citet{chan2022transformers,dasgupta2022distinguishing} study differences between in-weights and in-context generalization, and \citet{chang2022careful,liu2021makes,zhang2022active} observe that the selection of examples affect ICL predictions significantly.
In this paper, we emphasize a probabilistic treatment of ICL predictions.
Uncertainty in LLMs has previously been studied, e.g.~by \citet{kadavath2022language,lin2022teaching,bai2022training,gonen2022demystifying}.
On non-language tasks, \citet{kirsch2022general,chan2022data} study properties that lead to the emergence of ICL.
Also related are \citet{garnelo2018neural,kossen2021self, gordon2018meta}, who propose deep non-parametric models on non-language tasks. Unlike ICL, they can guarantee invariance to example-order or closely approximate Bayesian predictive distributions \citep{muller2021transformers}.




\vspace{-2mm}
\section{Conclusions}
\vspace{-1mm}
In this paper, we have investigated how the conditional label distribution of in-context examples affects ICL predictions.
To ensure our conclusions represent ICL behavior well, we have studied ICL across all possible in-context dataset sizes and considered probabilistic aspects of ICL predictions.
In some sense, we have shown that ICL is both better and worse than expected.
On the one hand, our results demonstrate that, against expectations set by prior work, ICL does incorporate in-context label information and can even learn truly novel tasks in-context.
On the other hand, we have shown that analogies between ICL and conventional learning algorithms fall short in a variety of ways.
In particular, label relationships inferred from pre-training have a lasting effect that cannot be surmounted by in-context observations.
Additional prompting can improve but likely not overcome this deficiency.
Further, ICL does not treat all information provided in-context equally and preferentially makes use of label information that appears closer to the query.








\subsubsection*{Reproducibility Statement}
We discuss the details of our experimental evaluation in \S\ref{sec:details}.
We provide the code to reproduce our results at the following repository: \href{https://github.com/jlko/in\_context\_learning}{github.com/jlko/in\_context\_learning}.


\FloatBarrier
\bibliography{iclr2024_conference}
\bibliographystyle{iclr2024_conference}
\newpage
\FloatBarrier
\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}
\vspace{-2mm}
\section{Can Prompts Help ICL Learn Flipped Label Relationships?}
\label{sec:prompted}
\vspace{-1mm}

% Figure environment removed




In this section, we explore if \emph{prompts} can be used to overcome the plateauing ICL performance when default labels are flipped.
Before the in-context examples, we insert prompt strings that inform the LLM of the flipped label relationship in some fashion, and should thus help the LLM adjust to it during ICL.
These prompts could fundamentally change few-shot ICL behavior.
In fact, one can think of the in-context learner as the union of LLM and prompt, where so far the prompt was simply left empty.
There could exist prompts that help ICL learn the flipped label relationship better than without them, or as well as in the default scenario.
Note that, regardless of the outcome here, NH2 remains rejected as ICL should not need to rely on a prompt to correctly consider in-context observations.
Nevertheless, for the `prompted few-shot ICL' setup, NH2 should be reconsidered.




We explore the following three prompts: `In the following ...', \textcolor{pal4}{(Instruct Prompt)} `...negative means positive and positive means negative' (here for SST-2 and adapted to other tasks), \textcolor{pal2}{(Ignore Prompt)} `...ignore all prior knowledge', and \textcolor{pal9}{(Invert Prompt)} `...flip the meaning for all answers'.




\looseness=-1
\textbf{Observations \& Discussions.}
\Cref{fig:prompted_main} gives results for the prompted few-shot ICL setup for LLaMa-65B and Falcon-40B on SST-2.
\Cref{fig:prompted_appendix_sst2,fig:prompted_appendix_subj,fig:prompted_appendix_financial_phrasebank,fig:prompted_appendix_hate_speech,fig:prompted_appendix_ag_news,fig:prompted_appendix_medical_questions_pairs,fig:prompted_appendix_mrpc,fig:prompted_appendix_rte,fig:prompted_appendix_wnli} give results for our largest models across all tasks. 
However, prompting is most successful for the scenarios in \cref{fig:prompted_main}, making this the most interesting result to study NH2.
In particular, prompting has a surprisingly weak effect for LLaMa-2-70B.
In \cref{fig:prompted_main} we observe that prompts, in particular the \textcolor{pal4}{instruct} and \textcolor{pal9}{invert} prompts, can help improve ICL performance.
However, it seems that the positive impact from prompting is restricted to an initial boost at small in-context datasets sizes.
We then sometimes observe a `dip' in performance, which could indicate ICL forgetting about the prompt.
At large context sizes, none of our prompts have any advantage, and flipped label performance again plateaus short of performance for the default label setup.
\textbf{\emph{Therefore, we reject NH2 for the prompted ICL variations that we study.}}


\looseness=-1
It is possible that there exist prompts---that we have not found---for which we cannot reject NH2.
However, we are sceptical these prompts exist for the models we study, as their behavior at large context sizes is strikingly similar across all prompts we investigate.
For more capable LLMs, we suspect it may be possible to obtain a zero-shot performance on the flipped scenario that is equal to the zero-shot of the default scenario, i.e.~the prompt leads to the LLM perfectly flipping all its zero-shot predictions.
However, we are unsure if, in addition to flipping zero-shot predictions, such prompts would also improve \emph{ICL} on flipped label observations to be as good as in the default scenario.


\section{Evaluation Approach for Cheap In-Context Learning Dynamics}
\label{sec:eval_approach}

In this section, we suggest a---to the best of our knowledge---novel way of evaluating ICL that gives performance metrics at all in-context dataset sizes in a single forward pass without incurring additional cost.
We start by introducing the notation necessary to formalize few-shot ICL in LLMs.

\textbf{Dataset to Input String.}
The few-shot task is defined by a dataset $\mathcal{D} = \{(S_i, Y_i)\}_{i=1}^N$, where $S_i\in \mathcal{T}^{d_{S_i}}$ are input sentences from which to predict the associated labels $Y_i\in\mathcal{T}^{d_{y_i}}$, and $\mathcal{T}^{v}$ are text strings of length $v$.
A \emph{verbalizer} $V(S,Y)$ takes a sentence--label pair and maps it to an \emph{example}, e.g.~the sequence `I am happy' and label `positive' are verbalized as `Sentence: I am happy\textbackslash n Label: positive\textbackslash n'.
We also define a \emph{query verbalizer} $V_q(S)$ that maps a test query $S$ to a query example, e.g.~`I am sad' is mapped to `Sentence: I am sad\textbackslash n Label:', such that the next-token prediction of an LLM will be encouraged to predict the label for the query. 
We apply the verbalizer to the entire dataset set and concatenate its output to obtain the \emph{context} $\mathcal{C} = \oplus_{i=1}^N  V(S_i, Y_i)$.
Finally, we concatenate context $\mathcal{C}$ and verbalized query $V_q(S)$, where $S$ is a sentence drawn from a separate test set, to obtain the input to the language model, $I = \mathcal{C} \oplus V_q(S) \in \mathcal{T}^{d_I}$.

\textbf{Input String to Tokens.}
The input $I$ is \emph{tokenized} before it can be processed by the language model.
The tokenizer, $T(I)=(X_1, \dots, X_M)$, maps an input sequence $I$ to a sequence of integers, or tokens, $X_i\in (1, \dots, D)$, where $D$ is the vocabulary size, i.e.~the number of unique tokens.
We keep track of which token positions correspond to labels, $\mathcal{L} =(l_1, \dots, l_N)$, e.g.~the indices of the tokens immediately following the string `Label:' in the above example.

\textbf{Tokens to Predictions.}
In the following, we use capital letters to denote random variables and lower-case letters for their realizations.
Here, we describe the behavior of \emph{decoder-only} language models \citep{liu2018generating,radford2018improving}, a popular architecture choice for LLMs.
Given the observed sequence of input tokens $(X_1=x_1, \dots, X_M=x_M)$, a single forward pass through the language model gives an estimate of the \emph{joint probability},
\begin{align}
     p(X_1=x_1) \cdot p(X_2=x_2 \mid X_1=x_1) \cdot ... \cdot p(X_M=x_M\mid X_1=x_1, \dots, X_{M-1}=x_{M-1}). \label{eq:joint_raw}
\end{align}
We highglight that \cref{eq:joint_raw} gives the joint probability \emph{at the observed outcomes}: we obtain $M$ `one-step ahead' predictions, each conditioned only on observed outcomes and not on model predictions.
\Cref{eq:joint_raw} is a common objective in LLM training, where `the joint probability the model assigns to the observations' is sometimes referred to as \emph{teacher forcing} \citep{williams1989learning}.

At test time, LLMs are usually iteratively conditioned on their own \emph{predictions}, generating novel outputs via multiple forward passes, i.e.~one first samples $\hat{x}_{M}\sim p(X_{M}|\dots)$, and then $\hat{x}_{M+1}\sim p(X_{M+1} \mid \dots, X_{M}=\hat{x}_{M})$, and so on.
We here use `$\dots$' to stand in for any additional tokens also conditioned on, e.g. $(x_1, \dots, x_{M-1})$.
One usually ignores all other terms of the joint here---the predictions for $(X_1, \dots, X_{M-1})$ that are generated in each forward pass---as only the last term $p(X_M\mid \dots)$ is needed to sample the next token, i.e.~the label in standard few-shot ICL applications.


\textbf{Single-Forward Pass ICL Training Dynamics.}
We now explain our approach for efficient evaluation of ICL training dynamics.
Given input tokens $(X_1, \dots, X_M)$ for the few-shot ICL setup described above, we first select those terms from \cref{eq:joint_raw} that correspond to label token predictions,
\begin{align}
     \prod\nolimits_{i=1}^N p(X_{l_i}=x_{l_i} \mid X_1=x_1, \dots, X_{m < l_i} = x_{m < l_i}). \label{eq:joint_intermediate}
\end{align}
For each term, the model predicts a distribution over the entire token vocabulary, i.e.~$p(X_{l_i}\mid \dots)$ is a categorical distribution, $p = (p_1, \dots, p_D) = \mathbb{R}^D$, which is then evaluated at the observed tokens in \cref{eq:joint_intermediate}.
We can transform this into a prediction over only the few-shot task label $Y$ by selecting the indices of the categorical distribution which correspond to the tokenized labels and then renormalizing, $p(Y) = (p_{t_1}, \dots, p_{t_C}|\dots)$, where $C$ is the number of unique labels which are encoded to tokens $(t_1, \dots, t_C)$.
With this, we can rewrite \cref{eq:joint_intermediate} as the joint probability the model assigns to the sequence of sentence--label pairs $\{S_i=s_i,Y_i=y_i\}_{i=1}^N$,
\begin{multline}
    p(Y_1 = y_1 \mid S_1=s_1) \cdot p(Y_2=y_2 \mid S_1=s_1, Y_1=y_1, S_2=s_2) \cdot \dots \\
    \cdot p(Y_N=y_N \mid S_1=s_1, Y_1=y_1, \dots, S_{N-1}=s_{N-1}, Y_{N-1}=y_{N-1}, S_N=s_N) \label{eq:joint_label}.
\end{multline}
Note how, because the joint is evaluated at the observations, its individual terms are always conditioned on the true labels and not previous predictions.
This allow us to cheaply compute the \emph{training dynamics} of ICL as a function of increasing in-context dataset size.
With each forward pass, we obtain the individual terms of \cref{eq:joint_label}, which are the few-shot ICL predictions at all possible context dataset sizes, $i=(1, \dots, N)$.
In contrast, in standard few-shot ICL evaluations, each forward pass only yields predictions for a single test query, neglecting the information the joint contains about the first $N-1$ label predictions.
There may be interesting applications of \cref{eq:joint_label} to model selection, as the quantity has links to both Bayesian evidence \citep{murphy2022probabilistic} and cross-validation \citep{fong2020marginal}, although we do not explore this any further in this paper.

\textbf{Multi-Token Labels.}
So far, we have assumed that each label string is encoded as a single token.
However, our approach can also be applied if some or all labels are encoded as multiple tokens.
In essence, we continue to measure only the probability the model assigns to the first token of each label, making the (fairly harmless) assumption that the first (or only) token that each label is encoded to is unique among labels.
We believe this is justified, as, given the first token for a label, the model should near-deterministically predict the remaining tokens, i.e.~all the predictive information is contained in the first token the model predicts for a label.
For example, for the Subjectivity dataset, the label `objective' is encoded by the LLaMa tokenizer as a single token but the label `subjective' is encoded as two tokens, [subject, ive].
We only use the probability assigned to [subject] to assign probabilities to `subjective', and ignore any predictions for [ive].
However, if the model successfully accommodates the pattern of the in-context example labels, we would expect probabilities for [ive] following [subject] to be close to \num{1} always.

For LLaMa-7B on Subjectivity, we have investigated the above assumption empirically.
After the first observation of the `subjectivity' label in-context, the probability of predicting [ive] after observing [subject] are $0.9998 \pm 0.0003$ for the following \num{12} instances of the `subjectivity' label, with probabilities normalized over \emph{all} tokens of the vocabulary here.
In other words, we can safely evaluate the performance of the LLaMa model from its predictions of only the [subject] token, even though the full label is split over two tokens [subject, ive].



\section{Authorship Identification Task}
\label{sec:author_details}
We here give details on our novel authorship identification task.

\textbf{Data Collection \& Processing.}
We extract the last \num{151} messages sent between two authors of this paper on the Slack messaging platform.
If multiple messages are sent in a row by one author, these count as multiple messages.
We filter out \num{42} messages that were of low quality: URLs, article summaries, missed call notifications, and meeting notes.
This leaves us with \num{58} and \num{51} messages per author.
We set the maximum message length to be \num{200} and truncate any messages longer than that.
Before truncation, the longest message was \num{579} characters long.
The median message length is \num{68} before and after truncation, mean and standard deviation shrink from $100 \pm 98$ to $88 \pm 65$.
For use in ICL, we treat this dataset as we would any other and present messages in random order.

\textbf{Data Release.}
For now, we have decided to not make this dataset public for two reasons: (1) It contains genuinely private communication, and (2) releasing the data would mean that future LLMs might be trained on it, so we could no longer use it to test their ability to learn truly novel label relationships in-context.








\section{Dataset Citations}
\label{sec:dataset_citations}
We evaluate on SST-2 \citep{socher2013recursive}, Subjective \citep{wang2012baselines}, Financial Phrasebank \citep{Malo2014GoodDO}, Hate Speech \citep{gibert2018hate}, AG News \citep{Zhang2015CharacterlevelCN}, Medical Questions Pairs (MQP) \citep{mccreery2020effective}, as well as Microsoft Research Paraphrase Corpus (MRPC) \citep{dolan2005automatically},  Recognizing Textual Entailment (RTE) \citep{dagan2005pascal}, and Winograd Schema Challenge (WNLI) \citep{levesque2012winograd} from GLUE \citep{wang2019glue}.

\newpage
\section{Experiment Details}
\label{sec:details}


Below we give additional details on our experimental evaluation.



\textbf{Guessing Baseline.}
In our experiments, we frequently display a `\textcolor{pal7}{guessing} based on class frequencies' baseline as a grey-dashed line.
This baseline presents an \emph{informed guess} that relies only on knowing the class frequencies of the task and makes the exact same prediction for each input datapoint.
We here explain how we compute this baseline for accuracy, entropy, and log likelihood.
We are given a classification task with $C$ classes which appear with frequencies $p=[p_1, \dots, p_C]$ in the training set.
The baseline always predicts $p=[p_1, \dots, p_C]$, i.e.~it predicts the class frequencies.
For accuracy, it thus always predicts the majority class $c^* = \arg\max_k p_k$, which leads to accuracy $p_{c^*}$.
Further, the baseline prediction leads to a log likelihood of $\sum_k p_k \log p_k$ and an entropy of $-\sum_k p_k \log p_k$ under the training data distribution.

\looseness=-1
\textbf{Class Flipping.}
While most of our tasks are \emph{binary} classification, Financial Phrasebank and AG News are not.
For these datasets, when `flipping' labels in \S\ref{sec:flipped_arbitrary}, \S\ref{sec:prompted}, and \S\ref{sec:time_series}, we actually rotate labels instead, i.e.~we reassign labels $y$ as $y \leftarrow (y + 1) \mod C$, where $C$ is the number of classes.
For AG News, ['world', 'sports', 'business', 'science and technology'] get mapped to ['sports', 'business', 'science and technology', 'world']. 
For Financial Phrasebank, ['negative', 'neutral', 'positive'] get mapped to ['neutral', 'positive', 'negative'].
Note that, for Financial Phrasebank, rotating the labels is harder than naively inverting the label order, as rotating does not leave the meaning of the `neutral' label unchanged.

\textbf{In-Context Example Formatting.}
We use the following simple templates to format the in-context examples.
For SST-2, Subjectivity, Financial Phrasebank, Hate Speech, and our author identification task, we use the following line of Python code to format each input example: \\
{\footnotesize \texttt{f"Sentence: '\{sentence\}'\textbackslash nAnswer: \{label\}\textbackslash n\textbackslash n"}}.
\newline
For MRPC, WNLI, and RTE, we format instances with \newline
 {\footnotesize \texttt{f"Sentence 1: '\{sentence1\}'\textbackslash nSentence 2: '\{sentence2\}'\textbackslash nAnswer: \{label\}\textbackslash n\textbackslash n"}}.\newline
For MQP, we use \newline
 {\footnotesize \texttt{f"Question 1: '\{sentence1\}'\textbackslash nQuestion 2: '\{sentence2\}'\textbackslash nAnswer: \{label\}\textbackslash n\textbackslash n"}}.

\textbf{Implementation.}
We rely on the Hugging Face Python library \citep{wolf2019huggingface} and PyTorch \citep{NEURIPS2019_9015} to implement the experiments of this paper.
We use half-precision floating-point numbers for LLaMa-65B and LLaMa-2-70B, and we use 8bit-quantization for all other models, which we have found to not affect performance notably.
In \cref{fig:quantization_error}, we illustrate this by showing the difference between 8 Bit quantization and full 32 Bit precision for default ICL and ICL with label randomization for LLaMa-2-7B on the Subjectivity dataset: there is no significant loss of precision or change in behavior from 8 Bit quantization.

\textbf{Datasets.} We use Hugging Face Spaces to access all tasks considered in this paper.
For Hate Speech, we select the first \num{1000} examples with labels \num{0} and \num{1}, skipping datapoints with labels \num{2} and \num{3}.
We do not use custom processing for any other dataset.

% Figure environment removed







\textbf{Whitespace Tokenization.}
To evaluate few-shot ICL performance as introduced in \S\ref{sec:dynamics}, we need to identify the tokens that individual task labels are encoded to.
We here detail how to achieve this at the example of the SST-2 label `positive'.
In particular, we highlight the, perhaps unexpected, effects of whitespaces on input tokenization.
These details are important and, if not considered correctly, can degrade performance significantly.


For Falcon models, the tokenizer encodes `\texttt{\textcolor{pal1}{Answer:}}' as \texttt{[\textcolor{pal1}{20309, 37}]}, `\texttt{\textcolor{pal1}{Answer:}-}' as \texttt{[\textcolor{pal1}{20309, 37}, 204]}, and `\texttt{\textcolor{pal1}{Answer:}\textcolor{pal0}{-positive}}' as \texttt{[\textcolor{pal1}{20309, 37}, \textcolor{pal0}{3508}]}.
We here use dashes `\texttt{-}' instead of whitespaces for improved legibility.
Clearly, the relevant token for the label `positive' is \texttt{[\textcolor{pal0}{3508}]}.
Note how the token \texttt{[204]} for the trailing whitespace disappears again after appending the label.
Further, just encoding `\texttt{positive}' without a preceding whitespace gives \texttt{[\textcolor{pal2}{28265}]}.
However, this token does not appear in the input, where the label is preceded by a whitespace---we should use \texttt{[\textcolor{pal0}{3508}]} to measure ICL performance instead.

The LLaMa and LLaMa-2 tokenizer encodes `\texttt{\textcolor{pal1}{Answer:}}' as \texttt{[\textcolor{pal1}{673, 29901}]}, `\texttt{\textcolor{pal1}{Answer:}-}' as \texttt{[\textcolor{pal1}{673, 29901}, 29871]}, and `\texttt{\textcolor{pal1}{Answer:}\textcolor{pal0}{-positive}}' as \texttt{[\textcolor{pal1}{673, 29901}, \textcolor{pal0}{6374}]}.
Thus, the relevant token for the label `positive' is \texttt{[\textcolor{pal0}{6374}]}.
In contrast to the Falcon tokenizer, just encoding `\texttt{positive}' without a preceding whitespace also gives \texttt{[\textcolor{pal0}{6374}]}.

Lastly, similar caveats apply to the classic evaluation procedure for few-shot ICL, where we only evaluate the prediction for a single test query at the end of the input.
Here, it is crucially important that we do not end inputs with a trailing whitespace.
As we have seen above, for both LLaMa and Falcon tokenizers, the trailing whitespace leads to the generation of an extra token that is not present when encoding complete in-context examples, as the whitespace would usually be included in the label prediction itself.
This change in tokenization between in-context examples and test query can adversely affect ICL performance.














\textbf{Statistical Significance.}
In \cref{tab:random_summary_main,tab:flipped_summary_main,tab:random_summary_app,tab:flipped_summary_app} we bold differences if they are statistically significant at a \SI{95}{\percent} level.
Concretely, we compute if the absolute average differences are larger than \num{1.96} times the standard error.
Similarly, when deciding if default label performance is significantly better than random guessing performance, we check if mean performance plus \num{1.645} times the standard error is larger than the guessing baseline across accuracy and log likelihood.


\textbf{Maximum Context Dataset Size.}
For each task, we create in-context datasets by sub-sampling from the training set of the task.
Falcon and LLaMa support input sizes up to \num{2048} tokens, and LLaMa-2 supports up to \num{4096} input tokens.
For all models, performance will degrade if the input size exceeds this limit.
This caps the number of in-context examples we can include for each task.
For tasks where the individual input sentences are longer, we will be able to include fewer examples in-context.
Across all in-context datasets that we sample for a task, we compute the minimum number of in-context examples needed to exceed the token limit of \num{2048} or \num{4096}.
This is the maximum in-context dataset size for which we report results for that task.
We list these numbers in \cref{tab:max_context_size}
We find that, usually, the Falcon tokenizer is more efficient than the LLaMa-tokenizer, allowing us to study larger in-context dataset sizes.

\begin{table}[t]
    \caption{%
    Maximum number of in-context examples we consider for each model-task combination.
    Below, we shorten AG News as AGN, Hate Speech as HS, and Financial Phrasebank as FP.
    }
    \label{tab:max_context_size}
    \centering
    \begin{tabular}{lrrrrrrrrr}
    \toprule
     & SST-2 & Subj & FP & HS & AGN & MQP & MRPC & RTE & WNLI \\
    \midrule
    LLaMa-2 & 140 & 79 & 73 & 76 & 45 & 47 & 40 & 28 & 57 \\
    LLaMa & 66 & 37 & 33 & 26 & 21 & 21 & 20 & 13 & 27 \\
    Falcon & 67 & 39 & 37 & 28 & 25 & 23 & 21 & 15 & 30 \\
    \bottomrule
    \end{tabular}
    \vspace{-3mm}
\end{table}


\textbf{Calibration.}
We do not calibrate predicted probabilities by first dividing them by a `prior' probability and then renormalizing as suggested by \citet{zhao2021calibrate}.
We have found this rarely improves, and sometimes degrades predictions, cf.~\cref{fig:random_multi_calibrate_llama-65b}.
We observe this happening in particular for tasks where labels are encoded as multiple tokens.
\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}








\section{Extended Results}
\label{sec:extended_results}

\textbf{\Cref{sec:dynamics} -- Training Dynamics}: \Cref{fig:learning_curve_sst2} shows few-shot ICL training dynamics on SST-2 for a selection of models at different parameter counts.


\textbf{\Cref{sec:random} -- Label Randomization}:
\Cref{fig:random_sst2} gives results for randomized labels for all models on SST2.
\Cref{fig:random_multi_LLaMa-2-7B,fig:random_multi_LLaMa-2-13B,fig:random_multi_LLaMa-2-70B,fig:random_multi_LLaMa-7B,fig:random_multi_LLaMa-13B,fig:random_multi_LLaMa-65B,fig:random_multi_Falcon-7B,fig:random_multi_Falcon-7B-Instruct,fig:random_multi_Falcon-40B,fig:random_multi_Falcon-40B-Instruct} give results for all tasks and models comparing ICL with randomized labels to the default label setup.
\Cref{tab:random_summary_app} gives full summary statistics across all models, tasks, and metrics for the label randomization experiment.


\textbf{\Cref{sec:author_id}  -- Author ID Task}: \Cref{fig:author_id_all_models} gives few-shot ICL results for all models on our novel authorship identification task.

\textbf{\Cref{sec:flipped_arbitrary} -- Flipped Labels}: \Cref{fig:flipped_app_llama_2_sst2,fig:flipped_app_sst2,fig:flipped_app_llama_2_subj,fig:flipped_app_subj,fig:flipped_app_llama_2_financial_phrasebank,fig:flipped_app_financial_phrasebank,fig:flipped_app_llama_2_hate_speech,fig:flipped_app_hate_speech,fig:flipped_app_llama_2_ag_news,fig:flipped_app_ag_news,fig:flipped_app_llama_2_medical_questions_pairs,fig:flipped_app_medical_questions_pairs,fig:flipped_app_llama_2_mrpc,fig:flipped_app_mrpc,fig:flipped_app_llama_2_rte,fig:flipped_app_rte,fig:flipped_app_llama_2_wnli,fig:flipped_app_wnli} give results for all tasks and models for the modified label relationship experiments.
We also report performance for additional replacement labels across task and models.
\Cref{tab:flipped_summary_app} gives full summary statistics across all models, tasks, and metrics for the difference between default and flipped label performance.


\textbf{\Cref{sec:time_series} -- Dynamic Label Flipping}: In \cref{fig:time_series_appendix_sst2,fig:time_series_appendix_subj,fig:time_series_appendix_financial_phrasebank,fig:time_series_appendix_hate_speech,fig:time_series_appendix_ag_news}, we give results for the experiments investigating NH3 for all large models on tasks where label flipping gave strong performance in \S\ref{sec:flipped_arbitrary}.
For LLaMa-2-70B on Hate Speech in \cref{fig:time_series_appendix_hate_speech}, metrics appear very similar initially.
However, when exploring additional changepoints in \cref{fig:reflip_hate_speech_llama_2_70b}, we do find significant differences in predictions.


\textbf{\Cref{sec:prompted} -- Prompting with Flipped Labels}: \Cref{fig:prompted_appendix_sst2,fig:prompted_appendix_subj,fig:prompted_appendix_financial_phrasebank,fig:prompted_appendix_hate_speech,fig:prompted_appendix_ag_news,fig:prompted_appendix_medical_questions_pairs,fig:prompted_appendix_mrpc,fig:prompted_appendix_rte,fig:prompted_appendix_wnli} give results for all tasks and models for the prompted few-shot ICL setup.

















% Figure environment removed



% Figure environment removed


\def\plotRandomization#1#2{%
    \begin{landscape}
        \thispagestyle{empty}
        % Figure environment removed
    \end{landscape}
}%
\forlistlooptwo{\plotRandomization}{%
    LLaMa-2-7B,LLaMa-2-13B,LLaMa-2-70B,LLaMa-7B,LLaMa-13B,LLaMa-65B,Falcon-7B,Falcon-7B-Instruct,Falcon-40B,Falcon-40B-Instruct}{%
    Llama-2-7b-8bit,Llama-2-13b-8bit,Llama-2-70B,llama-7b-8bit,llama-13b-8bit,llama-65B,falcon-7b,falcon-7b-instruct,falcon-40b,falcon-40b-instruct}%


\begin{landscape}
    \thispagestyle{empty}
\begin{table}
    \caption{%
    Full summary statistics for the randomization experiment of \S\ref{sec:random}.
    We strongly encourage readers to also view the full training curves across all possible context sizes in \S\ref{sec:extended_results}.
    Across all metrics, here show the average difference between the default and randomized label scenario.
    We compute `Metric(Default) - Metric(Random)', such that positive accuracy/log likelihood differences indicate that ICL performs worse with randomized labels.
    We compute differences at the maximum context size for each task-model combination (cf.\S\ref{sec:details} and \cref{tab:max_context_size}).
    We display experiments where ICL accuracies or log likelihoods on the default labels do not significantly exceed random guessing performance in \textcolor{lightgray}{lightgray}.
    Across models, and metrics, model performance is usually significantly worse when labels are randomized and default performance exceeds random guessing.
    We display mean differences and standard errors over \num{500} runs. We bold entries for which mean differences are statistically significant.
    }
    \label{tab:random_summary_app}
    \centering
\begin{adjustbox}{max width=9.00177in}
\begin{tabular}{lrrrrrrrrr}
\toprule
\textbf{$\Delta$ Log Lik.} & SST-2 & Subj & F. Phraseb. & Hate Speech & AG News & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{0.42 \pm 0.02}$ & $\bm{0.39 \pm 0.02}$ & $\bm{0.57 \pm 0.02}$ & $\bm{0.18 \pm 0.01}$ & $\bm{0.53 \pm 0.04}$ & \textcolor{lightgray}{$\bm{0.03 \pm 0.01}$} & \textcolor{lightgray}{$0.02 \pm 0.01$} & $\bm{0.03 \pm 0.01}$ & \textcolor{lightgray}{$0.02 \pm 0.01$} \\
LLaMa-2 13B & $\bm{0.41 \pm 0.02}$ & $\bm{0.62 \pm 0.03}$ & $\bm{0.49 \pm 0.03}$ & $\bm{0.24 \pm 0.02}$ & $\bm{0.81 \pm 0.04}$ & $\bm{0.04 \pm 0.01}$ & \textcolor{lightgray}{$0.01 \pm 0.01$} & $\bm{0.06 \pm 0.02}$ & \textcolor{lightgray}{$0.02 \pm 0.03$} \\
LLaMa-2 70B & $\bm{0.51 \pm 0.03}$ & $\bm{0.53 \pm 0.02}$ & $\bm{0.57 \pm 0.02}$ & $\bm{0.34 \pm 0.02}$ & $\bm{0.80 \pm 0.03}$ & $\bm{0.29 \pm 0.02}$ & $\bm{0.04 \pm 0.02}$ & $\bm{0.22 \pm 0.02}$ & $\bm{0.18 \pm 0.02}$ \\
LLaMa 7B & $\bm{0.39 \pm 0.03}$ & $\bm{0.42 \pm 0.03}$ & $\bm{0.36 \pm 0.02}$ & $\bm{0.15 \pm 0.02}$ & $\bm{0.30 \pm 0.03}$ & $\bm{0.03 \pm 0.01}$ & \textcolor{lightgray}{$0.00 \pm 0.01$} & $0.03 \pm 0.02$ & \textcolor{lightgray}{$0.01 \pm 0.02$} \\
LLaMa 13B & $\bm{0.44 \pm 0.03}$ & $\bm{0.45 \pm 0.02}$ & $\bm{0.37 \pm 0.02}$ & $\bm{0.16 \pm 0.02}$ & $\bm{0.32 \pm 0.03}$ & $\bm{0.04 \pm 0.02}$ & \textcolor{lightgray}{$-0.01 \pm 0.02$} & $\bm{0.05 \pm 0.02}$ & \textcolor{lightgray}{$0.02 \pm 0.02$} \\
LLaMa 65B & $\bm{0.55 \pm 0.02}$ & $\bm{0.45 \pm 0.02}$ & $\bm{0.49 \pm 0.02}$ & $\bm{0.23 \pm 0.02}$ & $\bm{0.88 \pm 0.04}$ & $\bm{0.14 \pm 0.02}$ & $0.01 \pm 0.01$ & $\bm{0.12 \pm 0.02}$ & $\bm{0.08 \pm 0.02}$ \\
Falcon 7B & $\bm{0.20 \pm 0.01}$ & $\bm{0.19 \pm 0.01}$ & $\bm{0.25 \pm 0.01}$ & $\bm{0.06 \pm 0.01}$ & $\bm{0.31 \pm 0.03}$ & \textcolor{lightgray}{$0.01 \pm 0.02$} & \textcolor{lightgray}{$0.01 \pm 0.02$} & \textcolor{lightgray}{$-0.01 \pm 0.02$} & \textcolor{lightgray}{$0.01 \pm 0.02$} \\
Falcon 7B Instr. & $\bm{0.13 \pm 0.01}$ & $\bm{0.08 \pm 0.01}$ & $\bm{0.11 \pm 0.01}$ & $\bm{0.03 \pm 0.02}$ & $\bm{0.15 \pm 0.02}$ & \textcolor{lightgray}{$0.03 \pm 0.02$} & \textcolor{lightgray}{$0.02 \pm 0.03$} & \textcolor{lightgray}{$-0.00 \pm 0.02$} & \textcolor{lightgray}{$0.00 \pm 0.02$} \\
Falcon 40B & $\bm{0.34 \pm 0.02}$ & $\bm{0.35 \pm 0.02}$ & $\bm{0.31 \pm 0.02}$ & $\bm{0.18 \pm 0.02}$ & $\bm{0.90 \pm 0.04}$ & $\bm{0.06 \pm 0.02}$ & \textcolor{lightgray}{$0.01 \pm 0.02$} & $0.01 \pm 0.02$ & \textcolor{lightgray}{$0.02 \pm 0.02$} \\
Falcon 40B Instr. & $\bm{0.25 \pm 0.02}$ & $\bm{0.37 \pm 0.03}$ & $\bm{0.27 \pm 0.02}$ & $0.02 \pm 0.03$ & $\bm{0.77 \pm 0.04}$ & $\bm{0.06 \pm 0.02}$ & \textcolor{lightgray}{$0.02 \pm 0.02$} & $0.02 \pm 0.02$ & \textcolor{lightgray}{$\bm{0.04 \pm 0.02}$} \\
\bottomrule
\\
\toprule
\textbf{$\Delta$ Accuracy}& SST-2 & Subj & F. Phraseb. & Hate Speech & AG News & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{28.4 \pm 2.1}$ & $\bm{39.8 \pm 2.4}$ & $\bm{30.4 \pm 2.4}$ & $\bm{16.2 \pm 2.3}$ & $\bm{13.4 \pm 2.2}$ & \textcolor{lightgray}{$3.8 \pm 2.2$} & \textcolor{lightgray}{$5.2 \pm 2.8$} & $4.0 \pm 2.4$ & \textcolor{lightgray}{$3.0 \pm 2.5$} \\
LLaMa-2 13B & $\bm{19.8 \pm 2.0}$ & $\bm{46.0 \pm 2.4}$ & $\bm{24.4 \pm 2.4}$ & $\bm{19.0 \pm 2.3}$ & $\bm{34.6 \pm 2.3}$ & $3.2 \pm 2.5$ & \textcolor{lightgray}{$\bm{2.8 \pm 1.4}$} & $1.6 \pm 1.9$ & \textcolor{lightgray}{$3.6 \pm 2.3$} \\
LLaMa-2 70B & $\bm{34.8 \pm 2.3}$ & $\bm{29.8 \pm 2.1}$ & $\bm{27.0 \pm 2.2}$ & $\bm{28.6 \pm 2.5}$ & $\bm{24.4 \pm 2.1}$ & $\bm{19.6 \pm 2.5}$ & $\bm{3.6 \pm 1.7}$ & $\bm{14.0 \pm 2.0}$ & $\bm{13.6 \pm 2.5}$ \\
LLaMa 7B & $\bm{24.6 \pm 2.2}$ & $\bm{36.4 \pm 2.5}$ & $\bm{19.8 \pm 2.1}$ & $\bm{10.8 \pm 2.2}$ & $\bm{11.2 \pm 2.0}$ & $4.6 \pm 2.8$ & \textcolor{lightgray}{$-0.2 \pm 1.9$} & $2.8 \pm 2.6$ & \textcolor{lightgray}{$1.0 \pm 2.0$} \\
LLaMa 13B & $\bm{31.4 \pm 2.2}$ & $\bm{42.4 \pm 2.5}$ & $\bm{26.4 \pm 2.7}$ & $\bm{13.8 \pm 2.0}$ & $\bm{9.2 \pm 1.8}$ & $4.0 \pm 2.6$ & \textcolor{lightgray}{$-2.6 \pm 2.4$} & $2.0 \pm 2.4$ & \textcolor{lightgray}{$3.0 \pm 2.5$} \\
LLaMa 65B & $\bm{47.4 \pm 2.5}$ & $\bm{34.4 \pm 2.3}$ & $\bm{18.4 \pm 2.1}$ & $\bm{13.6 \pm 2.2}$ & $\bm{39.2 \pm 2.7}$ & $\bm{8.4 \pm 2.0}$ & $2.0 \pm 1.4$ & $\bm{7.6 \pm 1.8}$ & $\bm{4.6 \pm 2.1}$ \\
Falcon 7B & $\bm{6.8 \pm 1.6}$ & $\bm{14.4 \pm 2.0}$ & $\bm{24.0 \pm 2.4}$ & $\bm{7.6 \pm 1.8}$ & $\bm{13.2 \pm 2.0}$ & \textcolor{lightgray}{$3.0 \pm 2.2$} & \textcolor{lightgray}{$2.6 \pm 2.4$} & \textcolor{lightgray}{$-1.4 \pm 2.9$} & \textcolor{lightgray}{$0.0 \pm 2.1$} \\
Falcon 7B Instr. & $\bm{3.6 \pm 1.4}$ & $\bm{7.6 \pm 1.8}$ & $\bm{5.2 \pm 1.5}$ & $2.0 \pm 2.0$ & $\bm{8.2 \pm 1.9}$ & \textcolor{lightgray}{$1.6 \pm 2.6$} & \textcolor{lightgray}{$3.0 \pm 2.9$} & \textcolor{lightgray}{$-1.0 \pm 2.3$} & \textcolor{lightgray}{$2.8 \pm 1.8$} \\
Falcon 40B & $\bm{20.2 \pm 2.1}$ & $\bm{22.2 \pm 2.1}$ & $\bm{10.0 \pm 1.8}$ & $\bm{10.6 \pm 2.2}$ & $\bm{49.4 \pm 2.4}$ & $\bm{11.2 \pm 2.4}$ & \textcolor{lightgray}{$-0.4 \pm 1.1$} & $0.4 \pm 1.7$ & \textcolor{lightgray}{$1.2 \pm 2.2$} \\
Falcon 40B Instr. & $\bm{6.8 \pm 1.4}$ & $\bm{21.8 \pm 2.2}$ & $\bm{10.4 \pm 1.8}$ & $0.8 \pm 1.6$ & $\bm{40.4 \pm 2.4}$ & $\bm{4.2 \pm 2.0}$ & \textcolor{lightgray}{$3.4 \pm 2.3$} & $3.2 \pm 1.9$ & \textcolor{lightgray}{$3.6 \pm 2.2$} \\
\bottomrule
\\
\toprule
\textbf{$\Delta$ Entropy} & SST-2 & Subj & F. Phraseb. & Hate Speech & AG News & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{-0.465 \pm 0.007}$ & $\bm{-0.177 \pm 0.008}$ & $\bm{-0.529 \pm 0.012}$ & $\bm{-0.099 \pm 0.006}$ & $\bm{-0.910 \pm 0.014}$ & \textcolor{lightgray}{$\bm{-0.004 \pm 0.002}$} & \textcolor{lightgray}{$\bm{-0.005 \pm 0.002}$} & $\bm{-0.014 \pm 0.004}$ & \textcolor{lightgray}{$0.004 \pm 0.003$} \\
LLaMa-2 13B & $\bm{-0.499 \pm 0.008}$ & $\bm{-0.429 \pm 0.009}$ & $\bm{-0.477 \pm 0.013}$ & $\bm{-0.204 \pm 0.008}$ & $\bm{-1.009 \pm 0.013}$ & $\bm{-0.018 \pm 0.004}$ & \textcolor{lightgray}{$\bm{-0.021 \pm 0.004}$} & $\bm{-0.079 \pm 0.006}$ & \textcolor{lightgray}{$\bm{-0.058 \pm 0.006}$} \\
LLaMa-2 70B & $\bm{-0.564 \pm 0.007}$ & $\bm{-0.500 \pm 0.007}$ & $\bm{-0.563 \pm 0.010}$ & $\bm{-0.313 \pm 0.010}$ & $\bm{-1.046 \pm 0.011}$ & $\bm{-0.242 \pm 0.009}$ & $\bm{-0.074 \pm 0.006}$ & $\bm{-0.246 \pm 0.009}$ & $\bm{-0.220 \pm 0.008}$ \\
LLaMa 7B & $\bm{-0.426 \pm 0.009}$ & $\bm{-0.153 \pm 0.009}$ & $\bm{-0.225 \pm 0.011}$ & $\bm{-0.103 \pm 0.007}$ & $\bm{-0.497 \pm 0.013}$ & $-0.002 \pm 0.001$ & \textcolor{lightgray}{$-0.001 \pm 0.003$} & $-0.002 \pm 0.004$ & \textcolor{lightgray}{$-0.001 \pm 0.004$} \\
LLaMa 13B & $\bm{-0.376 \pm 0.009}$ & $\bm{-0.193 \pm 0.009}$ & $\bm{-0.218 \pm 0.009}$ & $\bm{-0.112 \pm 0.007}$ & $\bm{-0.567 \pm 0.015}$ & $-0.008 \pm 0.004$ & \textcolor{lightgray}{$\bm{-0.014 \pm 0.005}$} & $\bm{-0.012 \pm 0.004}$ & \textcolor{lightgray}{$0.001 \pm 0.004$} \\
LLaMa 65B & $\bm{-0.422 \pm 0.009}$ & $\bm{-0.283 \pm 0.008}$ & $\bm{-0.502 \pm 0.011}$ & $\bm{-0.273 \pm 0.009}$ & $\bm{-1.032 \pm 0.013}$ & $\bm{-0.100 \pm 0.007}$ & $\bm{-0.040 \pm 0.005}$ & $\bm{-0.128 \pm 0.008}$ & $\bm{-0.153 \pm 0.008}$ \\
Falcon 7B & $\bm{-0.196 \pm 0.007}$ & $\bm{-0.109 \pm 0.006}$ & $\bm{-0.099 \pm 0.006}$ & $\bm{-0.046 \pm 0.005}$ & $\bm{-0.428 \pm 0.015}$ & \textcolor{lightgray}{$-0.000 \pm 0.004$} & \textcolor{lightgray}{$0.003 \pm 0.005$} & \textcolor{lightgray}{$-0.002 \pm 0.004$} & \textcolor{lightgray}{$-0.002 \pm 0.004$} \\
Falcon 7B Instr. & $\bm{-0.207 \pm 0.007}$ & $\bm{-0.038 \pm 0.004}$ & $\bm{-0.115 \pm 0.007}$ & $\bm{-0.041 \pm 0.006}$ & $\bm{-0.200 \pm 0.011}$ & \textcolor{lightgray}{$0.000 \pm 0.004$} & \textcolor{lightgray}{$0.007 \pm 0.006$} & \textcolor{lightgray}{$0.003 \pm 0.006$} & \textcolor{lightgray}{$-0.003 \pm 0.006$} \\
Falcon 40B & $\bm{-0.382 \pm 0.009}$ & $\bm{-0.248 \pm 0.009}$ & $\bm{-0.358 \pm 0.010}$ & $\bm{-0.175 \pm 0.008}$ & $\bm{-0.923 \pm 0.015}$ & $-0.002 \pm 0.004$ & \textcolor{lightgray}{$0.001 \pm 0.006$} & $-0.008 \pm 0.005$ & \textcolor{lightgray}{$-0.001 \pm 0.004$} \\
Falcon 40B Instr. & $\bm{-0.396 \pm 0.008}$ & $\bm{-0.177 \pm 0.009}$ & $\bm{-0.313 \pm 0.010}$ & $\bm{-0.202 \pm 0.009}$ & $\bm{-0.922 \pm 0.015}$ & $\bm{-0.039 \pm 0.006}$ & \textcolor{lightgray}{$-0.013 \pm 0.007$} & $\bm{-0.033 \pm 0.006}$ & \textcolor{lightgray}{$\bm{-0.012 \pm 0.005}$} \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
\end{landscape}



% Figure environment removed




\def\plotFlipped#1#2{%
% Figure environment removed

% Figure environment removed
}%
\forlistlooptwo{\plotFlipped}{%
    sst2,subj,financial_phrasebank,hate_speech,ag_news,medical_questions_pairs,mrpc,rte,wnli}{%
    SST-2,Subjectivity,Financial Phrasebank,Hate Speech,AG News,Medical Questions Pairs,MRPC,RTE,WNLI}%


\begin{landscape}
    \thispagestyle{empty}
\begin{table}
    \caption{%
    Full summary statistics for the flipped label experiments of \S\ref{sec:flipped_arbitrary}.
    We strongly encourage readers to also view the full training curves across all possible context sizes and adittional replacement labels in \S\ref{sec:extended_results}.
    Across all metrics, here show the average difference between the default and flipped label scenario.
    We compute `Metric(Default) - Metric(Flipped)', such that positive accuracy/log likelihood differences indicate that ICL performs worse with flipped labels.
    We compute differences at the maximum context size for each task-model combination (cf.\S\ref{sec:details} and \cref{tab:max_context_size}).
    We display experiments where ICL accuracies or log likelihoods on the default labels do not significantly exceed random guessing performance in \textcolor{lightgray}{lightgray}.
    Across models, and metrics, model performance is usually significantly worse when labels are flipped (and default performance exceeds random guessing).
    In particular for entropies, we observe significant differences between ICL predictions.
    We display mean differences and standard errors over \num{100} runs. We bold entries for which mean differences are statistically significant.
    }
    \label{tab:flipped_summary_app}
    \centering
\begin{adjustbox}{max width=9.00177in}
\begin{tabular}{lrrrrrrrrr}
\toprule
\textbf{$\Delta$ Log Lik.} & SST-2 & Subj & F. Phraseb. & Hate Speech & AG News & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{0.41 \pm 0.02}$ & $-0.04 \pm 0.02$ & $\bm{0.36 \pm 0.03}$ & $\bm{0.26 \pm 0.01}$ & $\bm{0.87 \pm 0.03}$ & \textcolor{lightgray}{$0.01 \pm 0.00$} & \textcolor{lightgray}{$-0.02 \pm 0.01$} & $\bm{0.22 \pm 0.01}$ & \textcolor{lightgray}{$-0.00 \pm 0.01$} \\
LLaMa-2 13B & $\bm{0.31 \pm 0.02}$ & $0.03 \pm 0.02$ & $\bm{0.30 \pm 0.02}$ & $\bm{0.26 \pm 0.02}$ & $\bm{0.90 \pm 0.03}$ & $\bm{0.15 \pm 0.01}$ & \textcolor{lightgray}{$\bm{0.15 \pm 0.01}$} & $\bm{0.38 \pm 0.02}$ & \textcolor{lightgray}{$\bm{0.20 \pm 0.02}$} \\
LLaMa-2 70B & $\bm{0.08 \pm 0.02}$ & $\bm{0.08 \pm 0.02}$ & $\bm{0.35 \pm 0.03}$ & $\bm{0.18 \pm 0.02}$ & $\bm{0.90 \pm 0.02}$ & $\bm{0.24 \pm 0.02}$ & $\bm{0.15 \pm 0.02}$ & $\bm{0.21 \pm 0.02}$ & $0.08 \pm 0.02$ \\
LLaMa 7B & $\bm{0.36 \pm 0.02}$ & $\bm{0.13 \pm 0.02}$ & $\bm{0.53 \pm 0.02}$ & $\bm{0.31 \pm 0.02}$ & $\bm{1.06 \pm 0.03}$ & \textcolor{lightgray}{$0.05 \pm 0.00$} & \textcolor{lightgray}{$\bm{0.06 \pm 0.01}$} & \textcolor{lightgray}{$\bm{0.14 \pm 0.01}$} & \textcolor{lightgray}{$-0.03 \pm 0.01$} \\
LLaMa 13B & $\bm{0.40 \pm 0.02}$ & $\bm{0.09 \pm 0.02}$ & $\bm{0.51 \pm 0.02}$ & $\bm{0.33 \pm 0.02}$ & $\bm{1.18 \pm 0.03}$ & $\bm{0.16 \pm 0.01}$ & \textcolor{lightgray}{$\bm{0.16 \pm 0.01}$} & \textcolor{lightgray}{$\bm{0.22 \pm 0.01}$} & \textcolor{lightgray}{$0.05 \pm 0.01$} \\
LLaMa 65B & $\bm{0.33 \pm 0.02}$ & $\bm{0.16 \pm 0.02}$ & $\bm{0.63 \pm 0.02}$ & $\bm{0.19 \pm 0.02}$ & $\bm{1.02 \pm 0.03}$ & $\bm{0.39 \pm 0.02}$ & $\bm{0.31 \pm 0.02}$ & $\bm{0.41 \pm 0.02}$ & $\bm{0.20 \pm 0.02}$ \\
Falcon 7B & $\bm{0.48 \pm 0.02}$ & $\bm{0.37 \pm 0.01}$ & $\bm{0.58 \pm 0.02}$ & $\bm{0.19 \pm 0.01}$ & $\bm{1.38 \pm 0.04}$ & \textcolor{lightgray}{$0.05 \pm 0.01$} & \textcolor{lightgray}{$0.02 \pm 0.01$} & \textcolor{lightgray}{$0.05 \pm 0.01$} & \textcolor{lightgray}{$0.12 \pm 0.01$} \\
Falcon 7B Instr. & $\bm{0.70 \pm 0.02}$ & $\bm{0.48 \pm 0.01}$ & $\bm{0.77 \pm 0.02}$ & $\bm{0.52 \pm 0.02}$ & $\bm{2.29 \pm 0.03}$ & \textcolor{lightgray}{$-0.07 \pm 0.01$} & \textcolor{lightgray}{$0.01 \pm 0.02$} & \textcolor{lightgray}{$0.15 \pm 0.02$} & \textcolor{lightgray}{$\bm{0.22 \pm 0.02}$} \\
Falcon 40B & $\bm{0.33 \pm 0.02}$ & $\bm{0.25 \pm 0.02}$ & $\bm{0.74 \pm 0.03}$ & $\bm{0.27 \pm 0.02}$ & $\bm{1.00 \pm 0.03}$ & $\bm{0.20 \pm 0.01}$ & \textcolor{lightgray}{$\bm{0.19 \pm 0.01}$} & $\bm{0.39 \pm 0.01}$ & \textcolor{lightgray}{$0.09 \pm 0.01$} \\
Falcon 40B Instr. & $\bm{0.45 \pm 0.02}$ & $\bm{0.37 \pm 0.02}$ & $\bm{0.94 \pm 0.03}$ & $\bm{0.68 \pm 0.02}$ & $\bm{0.89 \pm 0.03}$ & $\bm{0.48 \pm 0.02}$ & \textcolor{lightgray}{$\bm{0.15 \pm 0.01}$} & $\bm{0.70 \pm 0.02}$ & $\bm{0.19 \pm 0.01}$ \\
\bottomrule
\\
\toprule
\textbf{$\Delta$ Accuracy}& SST-2 & Subj & F. Phraseb. & Hate Speech & AG News & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{34.0 \pm 5.3}$ & $-3.0 \pm 4.3$ & $\bm{15.0 \pm 4.3}$ & $\bm{26.0 \pm 6.9}$ & $\bm{47.0 \pm 5.6}$ & \textcolor{lightgray}{$1.0 \pm 8.4$} & \textcolor{lightgray}{$-5.0 \pm 5.9$} & $\bm{32.0 \pm 7.5}$ & \textcolor{lightgray}{$1.0 \pm 7.3$} \\
LLaMa-2 13B & $\bm{14.0 \pm 4.0}$ & $1.0 \pm 1.0$ & $\bm{12.0 \pm 3.8}$ & $\bm{18.0 \pm 5.4}$ & $\bm{37.0 \pm 5.8}$ & $\bm{22.1 \pm 7.1}$ & \textcolor{lightgray}{$\bm{20.0 \pm 5.3}$} & $\bm{36.0 \pm 7.0}$ & \textcolor{lightgray}{$\bm{27.0 \pm 6.5}$} \\
LLaMa-2 70B & $2.0 \pm 1.4$ & $1.0 \pm 2.2$ & $\bm{13.0 \pm 3.4}$ & $0.0 \pm 3.2$ & $\bm{51.0 \pm 5.6}$ & $\bm{11.0 \pm 4.7}$ & $\bm{20.0 \pm 5.5}$ & $7.0 \pm 5.0$ & $5.0 \pm 3.8$ \\
LLaMa 7B & $\bm{22.0 \pm 4.8}$ & $3.0 \pm 4.8$ & $\bm{36.0 \pm 5.9}$ & $\bm{27.0 \pm 6.6}$ & $\bm{64.0 \pm 6.4}$ & \textcolor{lightgray}{$6.0 \pm 6.6$} & \textcolor{lightgray}{$\bm{13.0 \pm 6.6}$} & \textcolor{lightgray}{$\bm{19.0 \pm 7.0}$} & \textcolor{lightgray}{$0.0 \pm 8.6$} \\
LLaMa 13B & $\bm{26.0 \pm 5.8}$ & $\bm{7.0 \pm 3.5}$ & $\bm{26.0 \pm 5.8}$ & $\bm{36.0 \pm 5.9}$ & $\bm{55.0 \pm 6.2}$ & $\bm{16.0 \pm 6.7}$ & \textcolor{lightgray}{$\bm{12.0 \pm 4.5}$} & \textcolor{lightgray}{$\bm{28.0 \pm 7.5}$} & \textcolor{lightgray}{$6.0 \pm 6.9$} \\
LLaMa 65B & $\bm{15.0 \pm 4.3}$ & $\bm{8.0 \pm 3.7}$ & $\bm{27.0 \pm 5.3}$ & $\bm{14.0 \pm 5.8}$ & $\bm{50.0 \pm 5.7}$ & $\bm{29.0 \pm 7.4}$ & $\bm{41.0 \pm 6.3}$ & $\bm{46.0 \pm 6.7}$ & $\bm{30.0 \pm 6.1}$ \\
Falcon 7B & $\bm{49.0 \pm 5.9}$ & $\bm{39.0 \pm 4.9}$ & $\bm{34.0 \pm 6.7}$ & $\bm{20.0 \pm 8.4}$ & $\bm{63.0 \pm 6.7}$ & \textcolor{lightgray}{$13.0 \pm 8.2$} & \textcolor{lightgray}{$7.0 \pm 4.5$} & \textcolor{lightgray}{$10.0 \pm 6.1$} & \textcolor{lightgray}{$12.0 \pm 8.3$} \\
Falcon 7B Instr. & $\bm{63.0 \pm 5.8}$ & $\bm{46.0 \pm 7.0}$ & $\bm{48.0 \pm 6.6}$ & $\bm{42.0 \pm 7.4}$ & $\bm{71.0 \pm 5.2}$ & \textcolor{lightgray}{$-10.0 \pm 7.1$} & \textcolor{lightgray}{$-9.0 \pm 6.3$} & \textcolor{lightgray}{$11.0 \pm 7.6$} & \textcolor{lightgray}{$\bm{17.0 \pm 8.5}$} \\
Falcon 40B & $\bm{15.0 \pm 4.3}$ & $\bm{17.0 \pm 4.5}$ & $\bm{36.0 \pm 5.9}$ & $\bm{20.0 \pm 6.8}$ & $\bm{58.0 \pm 6.5}$ & $\bm{28.0 \pm 6.8}$ & \textcolor{lightgray}{$\bm{32.0 \pm 6.8}$} & $\bm{25.0 \pm 8.3}$ & \textcolor{lightgray}{$\bm{16.0 \pm 7.8}$} \\
Falcon 40B Instr. & $\bm{29.0 \pm 5.3}$ & $\bm{30.0 \pm 5.2}$ & $\bm{45.0 \pm 5.5}$ & $\bm{53.0 \pm 6.8}$ & $\bm{49.0 \pm 6.9}$ & $\bm{45.0 \pm 7.0}$ & \textcolor{lightgray}{$11.0 \pm 6.3$} & $\bm{48.0 \pm 7.0}$ & $\bm{22.0 \pm 7.6}$ \\
\bottomrule
\\
\toprule
\textbf{$\Delta$ Entropy} & SST-2 & Subj & F. Phraseb. & Hate Speech & AG News & MQP & MRPC & RTE & WNLI \\
\midrule
LLaMa-2 7B & $\bm{-0.519 \pm 0.022}$ & $0.010 \pm 0.017$ & $\bm{-0.397 \pm 0.026}$ & $\bm{-0.100 \pm 0.013}$ & $\bm{-0.749 \pm 0.030}$ & \textcolor{lightgray}{$-0.003 \pm 0.005$} & \textcolor{lightgray}{$\bm{0.049 \pm 0.005}$} & $\bm{-0.027 \pm 0.008}$ & \textcolor{lightgray}{$-0.006 \pm 0.008$} \\
LLaMa-2 13B & $\bm{-0.479 \pm 0.019}$ & $\bm{-0.058 \pm 0.015}$ & $\bm{-0.475 \pm 0.024}$ & $\bm{-0.194 \pm 0.019}$ & $\bm{-0.953 \pm 0.028}$ & $\bm{-0.027 \pm 0.010}$ & \textcolor{lightgray}{$\bm{-0.067 \pm 0.011}$} & $\bm{-0.106 \pm 0.018}$ & \textcolor{lightgray}{$\bm{-0.073 \pm 0.017}$} \\
LLaMa-2 70B & $\bm{-0.167 \pm 0.019}$ & $\bm{-0.100 \pm 0.015}$ & $\bm{-0.398 \pm 0.029}$ & $\bm{-0.255 \pm 0.018}$ & $\bm{-0.995 \pm 0.024}$ & $\bm{-0.243 \pm 0.019}$ & $\bm{-0.150 \pm 0.015}$ & $\bm{-0.258 \pm 0.019}$ & $\bm{-0.209 \pm 0.017}$ \\
LLaMa 7B & $\bm{-0.434 \pm 0.020}$ & $-0.017 \pm 0.023$ & $\bm{-0.355 \pm 0.023}$ & $\bm{-0.159 \pm 0.019}$ & $\bm{-0.645 \pm 0.033}$ & \textcolor{lightgray}{$0.007 \pm 0.003$} & \textcolor{lightgray}{$\bm{-0.032 \pm 0.005}$} & \textcolor{lightgray}{$-0.005 \pm 0.008$} & \textcolor{lightgray}{$-0.014 \pm 0.010$} \\
LLaMa 13B & $\bm{-0.424 \pm 0.023}$ & $-0.005 \pm 0.020$ & $\bm{-0.237 \pm 0.022}$ & $\bm{-0.151 \pm 0.018}$ & $\bm{-0.731 \pm 0.033}$ & $0.008 \pm 0.009$ & \textcolor{lightgray}{$0.019 \pm 0.012$} & \textcolor{lightgray}{$-0.010 \pm 0.008$} & \textcolor{lightgray}{$-0.009 \pm 0.009$} \\
LLaMa 65B & $\bm{-0.345 \pm 0.019}$ & $\bm{-0.140 \pm 0.020}$ & $\bm{-0.528 \pm 0.024}$ & $\bm{-0.273 \pm 0.020}$ & $\bm{-1.041 \pm 0.029}$ & $\bm{-0.128 \pm 0.019}$ & $\bm{-0.141 \pm 0.015}$ & $\bm{-0.205 \pm 0.020}$ & $\bm{-0.199 \pm 0.019}$ \\
Falcon 7B & $\bm{-0.276 \pm 0.019}$ & $\bm{-0.118 \pm 0.014}$ & $-0.022 \pm 0.018$ & $\bm{-0.062 \pm 0.013}$ & $\bm{-0.518 \pm 0.037}$ & \textcolor{lightgray}{$0.002 \pm 0.009$} & \textcolor{lightgray}{$0.005 \pm 0.006$} & \textcolor{lightgray}{$-0.004 \pm 0.010$} & \textcolor{lightgray}{$-0.015 \pm 0.013$} \\
Falcon 7B Instr. & $\bm{-0.367 \pm 0.020}$ & $\bm{-0.068 \pm 0.012}$ & $\bm{-0.327 \pm 0.021}$ & $\bm{-0.052 \pm 0.016}$ & $\bm{-0.220 \pm 0.028}$ & \textcolor{lightgray}{$\bm{-0.024 \pm 0.009}$} & \textcolor{lightgray}{$\bm{0.127 \pm 0.015}$} & \textcolor{lightgray}{$0.005 \pm 0.015$} & \textcolor{lightgray}{$0.001 \pm 0.020$} \\
Falcon 40B & $\bm{-0.392 \pm 0.020}$ & $\bm{-0.233 \pm 0.022}$ & $\bm{-0.420 \pm 0.027}$ & $\bm{-0.190 \pm 0.022}$ & $\bm{-0.904 \pm 0.033}$ & $-0.003 \pm 0.010$ & \textcolor{lightgray}{$\bm{-0.103 \pm 0.012}$} & $-0.018 \pm 0.014$ & \textcolor{lightgray}{$-0.001 \pm 0.013$} \\
Falcon 40B Instr. & $\bm{-0.483 \pm 0.020}$ & $\bm{-0.160 \pm 0.024}$ & $\bm{-0.430 \pm 0.028}$ & $\bm{-0.313 \pm 0.024}$ & $\bm{-0.916 \pm 0.033}$ & $\bm{-0.098 \pm 0.017}$ & \textcolor{lightgray}{$-0.017 \pm 0.014$} & $\bm{-0.063 \pm 0.017}$ & $-0.013 \pm 0.013$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}
\end{landscape}






\def\plotTimeSeries#1#2{%
% Figure environment removed
}
\forlistlooptwo{\plotTimeSeries}{%
    sst2,subj,financial_phrasebank,hate_speech,ag_news}{%
    SST-2,Subjectivity,Financial Phrasebank,Hate Speech,AG News}%

% Figure environment removed


\def\plotPrompted#1#2{%
% Figure environment removed
}
\forlistlooptwo{\plotPrompted}{%
    sst2,subj,financial_phrasebank,hate_speech,ag_news,medical_questions_pairs,mrpc,rte,wnli}{%
    SST-2,Subjectivity,Financial Phrasebank,Hate Speech,AG News,Medical Questions Pairs,MRPC,RTE,WNLI}%


\end{document}
