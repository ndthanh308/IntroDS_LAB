\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2023)Agrawal, Zhou, Lewis, Zettlemoyer, and
  Ghazvininejad]{agrawal2022context}
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan
  Ghazvininejad.
\newblock In-context examples selection for machine translation.
\newblock In \emph{ACL}, 2023.
\newblock URL \url{https://arxiv.org/abs/2212.02437}.

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{akyurek2023what}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In \emph{ICLR}, 2023.
\newblock URL \url{https://arxiv.org/abs/2211.15661}.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,
  DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv:2204.05862}, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2204.05862}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,
  Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv:2212.08073}, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2212.08073}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{NeurIPS}, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Chan et~al.(2022{\natexlab{a}})Chan, Santoro, Lampinen, Wang, Singh,
  Richemond, McClelland, and Hill]{chan2022data}
Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre
  Richemond, James McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock \emph{NeurIPS}, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2205.05055}.

\bibitem[Chan et~al.(2022{\natexlab{b}})Chan, Dasgupta, Kim, Kumaran, Lampinen,
  and Hill]{chan2022transformers}
Stephanie~CY Chan, Ishita Dasgupta, Junkyung Kim, Dharshan Kumaran, Andrew~K
  Lampinen, and Felix Hill.
\newblock Transformers generalize differently from information stored in
  context vs in weights.
\newblock \emph{arXiv:2210.05675}, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2210.05675}.

\bibitem[Chang \& Jia(2022)Chang and Jia]{chang2022careful}
Ting-Yun Chang and Robin Jia.
\newblock Careful data curation stabilizes in-context learning.
\newblock In \emph{EMNLP}, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.10378}.

\bibitem[Chen et~al.(2022)Chen, Zhao, Yu, McKeown, and He]{chen2022relation}
Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He~He.
\newblock On the relation between sensitivity and accuracy in in-context
  learning.
\newblock \emph{arXiv:2209.07661}, 2022.
\newblock URL \url{https://arxiv.org/abs/2209.07661}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv:2204.02311}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{NeurIPS}, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.03741}.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine learning challenges workshop}, 2005.
\newblock URL \url{https://link.springer.com/chapter/10.1007/11736790_9}.

\bibitem[Dasgupta et~al.(2022)Dasgupta, Grant, and
  Griffiths]{dasgupta2022distinguishing}
Ishita Dasgupta, Erin Grant, and Tom Griffiths.
\newblock Distinguishing rule and exemplar-based generalization in learning
  systems.
\newblock In \emph{ICML}, 2022.
\newblock URL \url{https://arxiv.org/abs/2110.04328}.

\bibitem[de~Gibert et~al.(2018)de~Gibert, Perez, Garc{\'\i}a-Pablos, and
  Cuadros]{gibert2018hate}
Ona de~Gibert, Naiara Perez, Aitor Garc{\'\i}a-Pablos, and Montse Cuadros.
\newblock {Hate Speech Dataset from a White Supremacy Forum}.
\newblock In \emph{ACL Workshop on Abusive Language Online ({ALW}2)}, 2018.
\newblock URL \url{https://www.aclweb.org/anthology/W18-5102}.

\bibitem[Dolan \& Brockett(2005)Dolan and Brockett]{dolan2005automatically}
Bill Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Third International Workshop on Paraphrasing (IWP2005)},
  2005.
\newblock URL \url{https://aclanthology.org/I05-5002/}.

\bibitem[Fong \& Holmes(2020)Fong and Holmes]{fong2020marginal}
Edwin Fong and Chris~C Holmes.
\newblock On the marginal likelihood and cross-validation.
\newblock \emph{Biometrika}, 2020.
\newblock URL \url{https://arxiv.org/abs/1905.08737}.

\bibitem[Garnelo et~al.(2018)Garnelo, Schwarz, Rosenbaum, Viola, Rezende,
  Eslami, and Teh]{garnelo2018neural}
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo~J Rezende,
  SM~Eslami, and Yee~Whye Teh.
\newblock Neural processes.
\newblock \emph{arXiv:1807.01622}, 2018.
\newblock URL \url{https://arxiv.org/abs/1807.01622}.

\bibitem[Gonen et~al.(2022)Gonen, Iyer, Blevins, Smith, and
  Zettlemoyer]{gonen2022demystifying}
Hila Gonen, Srini Iyer, Terra Blevins, Noah~A Smith, and Luke Zettlemoyer.
\newblock Demystifying prompts in language models via perplexity estimation.
\newblock \emph{arXiv:2212.04037}, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.04037}.

\bibitem[Gordon et~al.(2019)Gordon, Bronskill, Bauer, Nowozin, and
  Turner]{gordon2018meta}
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and
  Richard~E Turner.
\newblock Meta-learning probabilistic inference for prediction.
\newblock In \emph{ICLR}, 2019.
\newblock URL \url{https://arxiv.org/abs/1805.09921}.

\bibitem[Hahn \& Goyal(2023)Hahn and Goyal]{hahn2023theory}
Michael Hahn and Navin Goyal.
\newblock A theory of emergent in-context learning as implicit structure
  induction.
\newblock \emph{arXiv:2303.07971}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.07971}.

\bibitem[Han et~al.(2023)Han, Wang, Zhao, and Ji]{han2023context}
Chi Han, Ziqi Wang, Han Zhao, and Heng Ji.
\newblock In-context learning of large language models explained as kernel
  regression.
\newblock \emph{arXiv:2305.12766}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.12766}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock In \emph{NeurIPS}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Huszár(2023)]{huszar2023implicit}
Ferenc Huszár.
\newblock Implicit bayesian inference in large language models, 2023.
\newblock URL
  \url{https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/}.
\newblock [Online; accessed 10-July-2023].

\bibitem[Jiang(2023)]{jiang2023latent}
Hui Jiang.
\newblock A latent space theory for emergent abilities in large language
  models.
\newblock \emph{arXiv:2304.09960}, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.09960}.

\bibitem[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain,
  Perez, Schiefer, Dodds, DasSarma, Tran-Johnson, et~al.]{kadavath2022language}
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan
  Perez, Nicholas Schiefer, Zac~Hatfield Dodds, Nova DasSarma, Eli
  Tran-Johnson, et~al.
\newblock Language models (mostly) know what they know.
\newblock \emph{arXiv:2207.05221}, 2022.
\newblock URL \url{https://arxiv.org/abs/2207.05221}.

\bibitem[Kirsch et~al.(2022)Kirsch, Harrison, Sohl-Dickstein, and
  Metz]{kirsch2022general}
Louis Kirsch, James Harrison, Jascha Sohl-Dickstein, and Luke Metz.
\newblock General-purpose in-context learning by meta-learning transformers.
\newblock \emph{arXiv:2212.04458}, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.04458}.

\bibitem[Kossen et~al.(2021)Kossen, Band, Lyle, Gomez, Rainforth, and
  Gal]{kossen2021self}
Jannik Kossen, Neil Band, Clare Lyle, Aidan~N Gomez, Tom Rainforth, and Yarin
  Gal.
\newblock Self-attention between datapoints: Going beyond individual
  input-output pairs in deep learning.
\newblock In \emph{NeurIPS}, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.02584}.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and
  Morgenstern]{levesque2012winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In \emph{Thirteenth international conference on the principles of
  knowledge representation and reasoning}, 2012.
\newblock URL \url{https://cdn.aaai.org/ocs/4492/4492-21843-1-PB.pdf}.

\bibitem[Li \& Qiu(2023)Li and Qiu]{li2023finding}
Xiaonan Li and Xipeng Qiu.
\newblock Finding supporting examples for in-context learning.
\newblock \emph{arXiv:2302.13539}, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.13539}.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
  Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
  et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv:2211.09110}, 2022.
\newblock URL \url{https://arxiv.org/abs/2211.09110}.

\bibitem[Lin et~al.(2023)Lin, Hilton, and Evans]{lin2022teaching}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Teaching models to express their uncertainty in words.
\newblock \emph{TMLR}, 2023.
\newblock URL \url{https://arxiv.org/abs/2205.14334}.

\bibitem[Liu et~al.(2022)Liu, Shen, Zhang, Dolan, Carin, and
  Chen]{liu2021makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu
  Chen.
\newblock What makes good in-context examples for gpt-$3$?
\newblock In \emph{ACL}, 2022.
\newblock URL \url{https://arxiv.org/abs/2101.06804}.

\bibitem[Liu et~al.(2023)Liu, Yuan, Fu, Jiang, Hayashi, and Neubig]{liu2023pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{ACM Computing Surveys}, 2023.
\newblock URL \url{https://arxiv.org/abs/2107.13586}.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{liu2018generating}
Peter~J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer.
\newblock Generating wikipedia by summarizing long sequences.
\newblock In \emph{ICLR}, 2018.
\newblock URL \url{https://arxiv.org/abs/1801.10198}.

\bibitem[Lu et~al.(2022)Lu, Bartolo, Moore, Riedel, and
  Stenetorp]{lu2021fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock In \emph{ACL}, 2022.
\newblock URL \url{https://arxiv.org/abs/2104.08786}.

\bibitem[Malo et~al.(2014)Malo, Sinha, Korhonen, Wallenius, and
  Takala]{Malo2014GoodDO}
P.~Malo, A.~Sinha, P.~Korhonen, J.~Wallenius, and P.~Takala.
\newblock Good debt or bad debt: Detecting semantic orientations in economic
  texts.
\newblock \emph{Journal of the Association for Information Science and
  Technology}, 2014.
\newblock URL \url{https://arxiv.org/abs/1307.5336}.

\bibitem[McCreery et~al.(2020)McCreery, Katariya, Kannan, Chablani, and
  Amatriain]{mccreery2020effective}
Clara~H. McCreery, Namit Katariya, Anitha Kannan, Manish Chablani, and Xavier
  Amatriain.
\newblock Effective transfer learning for identifying similar questions:
  Matching user questions to covid-19 faqs.
\newblock \emph{arXiv:2008.13546}, 2020.
\newblock URL \url{https://arxiv.org/abs/2008.13546}.

\bibitem[Min et~al.(2022{\natexlab{a}})Min, Lewis, Zettlemoyer, and
  Hajishirzi]{min2021metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock Metaicl: Learning to learn in context.
\newblock In \emph{NAACL}, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2110.15943}.

\bibitem[Min et~al.(2022{\natexlab{b}})Min, Lyu, Holtzman, Artetxe, Lewis,
  Hajishirzi, and Zettlemoyer]{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock In \emph{ACL}, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2202.12837}.

\bibitem[M{\"u}ller et~al.(2022)M{\"u}ller, Hollmann, Arango, Grabocka, and
  Hutter]{muller2021transformers}
Samuel M{\"u}ller, Noah Hollmann, Sebastian~Pineda Arango, Josif Grabocka, and
  Frank Hutter.
\newblock Transformers can do bayesian inference.
\newblock In \emph{ICLR}, 2022.
\newblock URL \url{https://arxiv.org/abs/2112.10510}.

\bibitem[Murphy(2022)]{murphy2022probabilistic}
Kevin~P Murphy.
\newblock \emph{Probabilistic machine learning: an introduction}.
\newblock MIT press, 2022.
\newblock URL \url{https://probml.github.io/pml-book/book1.html}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{NeurIPS}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.02155}.

\bibitem[Pan et~al.(2023)Pan, Gao, Chen, and Chen]{pan2023incontext}
Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen.
\newblock What in-context learning "learns" in-context: Disentangling task
  recognition and task learning.
\newblock In \emph{ACL}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.09731}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, 2019.
\newblock URL \url{https://pytorch.org/}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding with unsupervised learning.
\newblock \emph{Technical report, OpenAI}, 2018.
\newblock URL \url{https://openai.com/research/language-unsupervised}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 2019.
\newblock URL
  \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[Razeghi et~al.(2022)Razeghi, Logan~IV, Gardner, and
  Singh]{razeghi2022impact}
Yasaman Razeghi, Robert~L Logan~IV, Matt Gardner, and Sameer Singh.
\newblock Impact of pretraining term frequencies on few-shot reasoning.
\newblock In \emph{EMNLP}, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.07206}.

\bibitem[Si et~al.(2023)Si, Friedman, Joshi, Feng, Chen, and
  He]{si2023measuring}
Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He~He.
\newblock Measuring inductive biases of in-context learning with underspecified
  demonstrations.
\newblock In \emph{ACL}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.13299}.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{EMNLP}, 2013.
\newblock URL \url{https://aclanthology.org/D13-1170/}.

\bibitem[Stamatatos(2009)]{stamatatos2009survey}
Efstathios Stamatatos.
\newblock A survey of modern authorship attribution methods.
\newblock \emph{American Society for information Science and Technology}, 2009.
\newblock URL \url{https://onlinelibrary.wiley.com/doi/10.1002/asi.21001}.

\bibitem[TII(2023)]{falcon2023}
Technology Innovation~Institute TII.
\newblock Falcon llm, 2023.
\newblock URL \url{https://falconllm.tii.ae/}.
\newblock [Online; accessed 10-July-2023].

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023LLaMa}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv:2302.13971}, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,
  et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv:2307.09288}, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2307.09288}.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2023transformers}
Johannes Von~Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{ICML}, 2023.
\newblock URL \url{https://arxiv.org/abs/2212.07677}.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{ICLR}, 2019.
\newblock URL \url{https://arxiv.org/abs/1804.07461}.

\bibitem[Wang \& Manning(2012)Wang and Manning]{wang2012baselines}
Sida~I Wang and Christopher~D Manning.
\newblock Baselines and bigrams: Simple, good sentiment and topic
  classification.
\newblock In \emph{ACL}, 2012.
\newblock URL \url{https://dl.acm.org/doi/10.5555/2390665.2390688}.

\bibitem[Wei et~al.(2023)Wei, Wei, Tay, Tran, Webson, Lu, Chen, Liu, Huang,
  Zhou, et~al.]{wei2023larger}
Jerry Wei, Jason Wei, Yi~Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun
  Chen, Hanxiao Liu, Da~Huang, Denny Zhou, et~al.
\newblock Larger language models do in-context learning differently.
\newblock \emph{arXiv:2303.03846}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.03846}.

\bibitem[Wies et~al.(2023)Wies, Levine, and Shashua]{wies2023learnability}
Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock The learnability of in-context learning.
\newblock \emph{arXiv:2303.07895}, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.07895}.

\bibitem[Williams \& Zipser(1989)Williams and Zipser]{williams1989learning}
Ronald~J Williams and David Zipser.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural computation}, 1989.
\newblock URL \url{https://ieeexplore.ieee.org/document/6795228}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2019huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock In \emph{EMNLP: System Demonstrations}, 2020.
\newblock URL \url{https://arxiv.org/abs/1910.03771}.

\bibitem[Wu et~al.(2023)Wu, Qiu, Ross, Aky{\"u}rek, Chen, Wang, Kim, Andreas,
  and Kim]{wu2023reasoning}
Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky{\"u}rek, Boyuan Chen, Bailin
  Wang, Najoung Kim, Jacob Andreas, and Yoon Kim.
\newblock Reasoning or reciting? exploring the capabilities and limitations of
  language models through counterfactual tasks.
\newblock \emph{arXiv:2307.02477}, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.02477}.

\bibitem[Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma]{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In \emph{ICLR}, 2022.
\newblock URL \url{https://arxiv.org/abs/2111.02080}.

\bibitem[Yoo et~al.(2022)Yoo, Kim, Kim, Cho, Jo, Lee, Lee, and
  Kim]{yoo2022ground}
Kang~Min Yoo, Junyeob Kim, Hyuhng~Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo
  Lee, Sang-goo Lee, and Taeuk Kim.
\newblock Ground-truth labels matter: A deeper look into input-label
  demonstrations.
\newblock In \emph{EMNLP}, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.12685}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Roller, Goyal, Artetxe, Chen,
  Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv:2205.01068}, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2205.01068}.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{Zhang2015CharacterlevelCN}
Xiang Zhang, Junbo~Jake Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{NeurIPS}, 2015.
\newblock URL \url{https://arxiv.org/abs/1509.01626}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Feng, and Tan]{zhang2022active}
Yiming Zhang, Shi Feng, and Chenhao Tan.
\newblock Active example selection for in-context learning.
\newblock In \emph{EMNLP}, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2211.04486}.

\bibitem[Zhang et~al.(2023)Zhang, Zhang, Yang, and Wang]{zhang2023and}
Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang.
\newblock What and how does in-context learning learn? bayesian model
  averaging, parameterization, and generalization.
\newblock \emph{arXiv:2305.19420}, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.19420}.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh]{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In \emph{ICML}, 2021.
\newblock URL \url{https://arxiv.org/abs/2102.09690}.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving]{ziegler2019fine}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario
  Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv:1909.08593}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.08593}.

\end{thebibliography}
