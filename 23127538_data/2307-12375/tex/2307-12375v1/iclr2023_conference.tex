
\documentclass{article} %
\usepackage{iclr2023_conference,times}

\input{math_commands.tex}
\setlength {\marginparwidth }{3.2cm}

\usepackage[color=lightgray]{todonotes}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=[rgb]{0.00, 0.45, 0.70},
    linkcolor=[rgb]{0.01, 0.62, 0.45},
    urlcolor=[rgb]{0.80, 0.47, 0.74},
    }

\usepackage{url}

\usepackage{cleveref}

\title{In-Context Learning in Large Language \\ Models Learns Label Relationships \\ but Is Not Conventional Learning}







\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \nabla \or \Delta \or *\or \dagger\or \ddagger\or
   \mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
   \or \ddagger\ddagger \else\@ctrerr\fi}}
\makeatother


\author{%
  Jannik Kossen$^{1}$\thanks{Correspondence to jannik.kossen@cs.ox.ac.uk.\, \textsuperscript{$\Delta$} Equal advising.}
  \phantom{123} \phantom{123} \phantom{123} \phantom{123}
  Tom Rainforth$^{2}$\textsuperscript{$\Delta$}
  \phantom{123} \phantom{123} \phantom{123} \phantom{123}
  Yarin Gal$^{1}$\textsuperscript{$\Delta$}
  \\[1.5em]
  $^1$ OATML, Department of Computer Science, University of Oxford \\
  $^2$ Department of Statistics, University of Oxford \\
}





\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy %

\newcommand{\jk}[1]{{\color{green} [\textbf{jk}: #1]}}

\usepackage{enumitem}
\setlist{nosep}
\usepackage{siunitx}

\usepackage{printlen}
\graphicspath{{figs/}}
\usepackage{placeins}
\usepackage{xcolor}         %
\definecolor{pal0}{rgb}{0.00, 0.45, 0.70}
\definecolor{pal1}{rgb}{0.87, 0.56, 0.02}
\definecolor{pal2}{rgb}{0.01, 0.62, 0.45}
\definecolor{pal3}{rgb}{0.84, 0.37, 0.00}
\definecolor{pal4}{rgb}{0.80, 0.47, 0.74}
\definecolor{pal5}{rgb}{0.65, 0.34, 0.16}
\definecolor{pal6}{rgb}{0.97, 0.51, 0.75}
\definecolor{pal7}{rgb}{0.58, 0.58, 0.58}
\definecolor{pal8}{rgb}{0.87, 0.87, 0.00}
\definecolor{pal10}{rgb}{0.34, 0.71, 0.91}
\definecolor{pal9}{rgb}{0.93, 0.88, 0.20}

\definecolor{palbg}{rgb}{0.91, 0.94, 0.99}
\usepackage{rotating}

\usepackage{pdflscape}

\usepackage{tcolorbox}

\begin{document}
\fancypagestyle{lscapedplain}{%
  \fancyhf{}
  \fancyfoot{}
}


\maketitle 

\begin{abstract}
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input--label relationship in the context.
However, there is currently no consensus about \emph{how} this in-context learning (ICL) ability of LLMs works:
for example, while \citet{xie2021explanation} liken ICL to a general-purpose learning algorithm, \citet{min2022rethinking} argue ICL does not even learn label relationships from in-context examples.
In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input--label examples provided in-context, and (3) how ICL aggregates label information across in-context examples.
Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally.
Our results give insights into understanding and aligning LLM behavior.
\end{abstract}


\vspace{-2mm}
\section{Introduction}
\label{sec:intro}
\looseness=-1
Large Language Models (LLMs) have achieved impressive performance on a wide variety of natural language processing (NLP) tasks \citep{radford2019language,chowdhery2022palm,zhang2022opt,hoffmann2022training,touvron2023LLaMa}.
\citet{brown2020language} have shown that LLMs can perform so-called \emph{in-context learning} (ICL) of supervised tasks.
In contrast to standard in-weights learning, e.g.~gradient-based finetuning of model parameters, in-context learning requires no gradients or parameter updates.
Instead, a \emph{context} string is prepended to the test query for which the LLM predicts.
For \emph{zero-shot ICL}, the context is a \emph{prompt}, which is usually an instruction explaining the task to the model.
We focus on \emph{few-shot ICL}, where the context contains \emph{examples} of the input--label relationship of the downstream task.
Few-shot ICL can significantly improve the accuracy of LLM predictions and has been used successfully across a variety of established NLP tasks, such as sentiment or document classification, question answering, or natural language inference, cf.~\citet{liang2022holistic}.


However, there is currently no consensus in the literature about \emph{why} ICL improves predictions.
In their seminal work, \citet{brown2020language} highlight similarities between the behavior of ICL and conventional learning algorithms, such as improvements with model size and number of examples.
Since then, some have argued that ICL works because it implements general-purpose learning mechanisms such as Bayesian inference or gradient descent \citep{xie2021explanation,huszar2023implicit,hahn2023theory,jiang2023latent,zhang2023and,von2023transformers,akyurek2023what}.
In contrast, other studies have highlighted practical shortcomings of ICL, suggesting ICL does not really `learn' from examples in the way one expects \citep{liu2021makes,lu2021fantastically,zhao2021calibrate,chen2022relation,agrawal2022context,chang2022careful,min2022rethinking,razeghi2022impact,li2023finding}.
In particular, \citet{min2022rethinking} claim their `findings suggest that [LLMs] do not learn new tasks at test time' in the sense of `capturing the input-label correspondence'.






% Figure environment removed


There is thus a clear need for an improved understanding of how information given in-context affects ICL predictions.
Better characterizing ICL behavior is crucial to our ability to deploy LLMs safely and effectively.
In this paper, we address the following pressing questions on the capabilities of ICL:
(1) Does ICL take the input--label relationship of the in-context examples into account when predicting for test queries?
(2) Does ICL treat the input--label relationships inferred from model pre-training the same as input--label relationships provided in-context?
(3) How does ICL aggregate information across in-context examples, particularly if there are multiple sources of information?

\looseness=-1
In our paper, we first review the often contradictory observations on ICL in prior work (\S\ref{sec:background}).
We then rephrase the above questions about ICL behavior as null hypotheses (\S\ref{sec:hypotheses}) that we study empirically in the following set of experiments:

\begin{itemize}[leftmargin=*]
    \item We first study if ICL predictions depend on the label relationship of in-context examples by randomizing the labels given in-context.
    Unlike \citet{min2022rethinking}, we study \emph{probabilistic} aspects of ICL predictions (\S\ref{sec:random}, \cref{fig:random_teaser_LLaMa-65b}). 
    Further, we study ICL on a truly novel task the LLM \emph{cannot} know from pre-training (\S\ref{sec:author_id}). Both experiments suggest that ICL usually considers the label relationships of in-context examples.
    We discuss why our findings differ from those of \citet{min2022rethinking}.

    \item We then study if ICL treats label relationships learned in pre-training equivalently to label information provided in-context.
    Concretely, we modify in-context label relationships to differ to those from pre-training (\S\ref{sec:flipped_arbitrary}).
 Our results suggest ICL does not consider the two sources of label information equally.
    In particular, in-context examples can never fully overcome pre-training preference.
    Further, we study if additional prompting can improve ICL here (\S\ref{sec:prompted}).
    We find that, while prompting can help, we ultimately do not find prompts which lead to the desired behavior.

    \looseness=-1
    \item The context itself might contain multiple sources of information towards the label relationship.
    We study if ICL treats these sources equally by modifying the label relationship midway through ICL (\S\ref{sec:time_series}).
    Our results suggest that ICL does not consider all in-context information equally and instead behaves more like a time series model that quickly adapts to changes in the observed label relation.
\end{itemize}
The findings in this paper are relevant beyond direct applications of few-shot ICL: for example, techniques in LLM alignment \citep{bai2022constitutional} rely on ICL being sufficiently robust and capable to mitigate any undesirable behavior arising from model pre-training or user input.

\vspace{-1mm}
\section{Background}
\label{sec:background}
\vspace{-1mm}

A large and growing body of prior work studies ICL.
We here highlight work most relevant to this paper, and discuss other related work in \S\ref{sec:related_work}.
The GPT-3 paper introduced ICL \citep{brown2020language} and mostly highlights similarities to more expensive gradient-descent based finetuning.
For example, they demonstrate ICL performance steadily increasing with the number of in-context examples and model size.
Since then, few-shot ICL has become an integral part of LLM evaluations, e.g.~the popular HELM benchmark \citep{liang2022holistic} is entirely based around few-shot evaluations on supervised downstream tasks, and many recent publications rely on few-shot ICL to evaluate their LLMs \citep{brown2020language,touvron2023LLaMa,hoffmann2022training,chowdhery2022palm}.

In the wake of ICL's empirical success, follow-up works have speculated if ICL implements a general purpose learning algorithm such as gradient descent \citep{von2023transformers} or Bayesian inference \citep{xie2021explanation}.
This line of work expresses the sentiment that ICL captures not just how much LLMs have learned during pre-training but, rather, how much LLMs have \emph{learned how to learn novel tasks in-context}.
However, little experimental support for these hypotheses in actual LLMs exists and arguments remains largely theoretical with strong assumption \citep{zhang2023and,huszar2023implicit,wies2023learnability,jiang2023latent}.

In contrast to these works, a variety of studies have since highlighted shortcomings of ICL that traditional gradient-based in-weights learning is not, or less, affected by.
ICL can be sensitive to the formatting \citep{min2022rethinking} or order \citep{lu2021fantastically} of the in-context learning examples.
Further, LLMs prefer to predict labels that are common in the pre-training data \citep{zhao2021calibrate}, predict drastically different for seemingly similar prompts \citep{chen2022relation}, or rely on task formulations similar to those observed in pre-training data \citep{wu2023reasoning}.

In particular, \citet{min2022rethinking} claim that ICL does not learn label relationships from in-context examples because they observe that ICL `performance drops only marginally when labels in the demonstrations are replaced by random labels'.
Further, they suggest that, instead of learning input--label relationships, ICL only works because the model learns about the general label space, the formatting of the examples, and their input distribution.
In the first part of our experimental evaluation, we will revisit and ultimately disagree with their claims that ICL does `not learn new tasks at test time' and that `ground truth demonstrations are in fact not required'.
\vspace{-2mm}
       \section{Null Hypotheses on How ICL Incorporates Label Information}
\label{sec:hypotheses}
\vspace{-2mm}
Given the popularity of ICL and the ongoing debate regarding its abilities, obtaining a better understanding of how ICL makes use of the information provided in-context about the input--label relationship is a crucially important and pressing matter.
On the one hand, the results of \citet{min2022rethinking} are plausible: if zero-shot performance is strong already, then why would the model need to learn label relationships in-context.
On the other hand, these results, if always true, have far reaching ramifications: if ICL cannot be compared to learning algorithms, we should significantly scale back expectations of what can be achieved by it.
In this paper, we therefore present a thorough empirical study that seeks to understand how far analogies between ICL and learning algorithms extend.

To be precise, by \emph{learning algorithm} we will refer to an algorithm that, given a training set of input--label pairs, makes a prediction for the label of a test input.
For simplicity, we always assume the learning algorithm is \emph{probabilistic}, i.e.~it predicts a probability distribution over labels.
Further, we will use \emph{information-theoretic} language as a helpful tool to describe model behavior: for example, the training set contains \emph{information} towards the label relationship.





\looseness=-1
We reformulate our questions about ICL as null hypotheses derived from basic assumptions about how learning algorithms \emph{should} or \emph{should not} behave.
These assumptions focus on properties shared by many learning algorithms, e.g.~neural networks trained with mini-batch gradient descent.
If ICL implements a conventional learning algorithm, it should conform to these properties.
Our first assumption about conventional learning algorithms is that they learn the conditional distribution of labels given inputs from the examples provided.
From this, we derive our first null hypothesis.

\emph{\textbf{Null Hypothesis 1 (NH1)}: ICL predictions are independent of the conditional label distribution of the in-context examples.}

We will address this hypothesis in two ways:
In \S\ref{sec:random}, we first revisit and refine the randomized in-context label experiment of \citet{min2022rethinking}:
in addition to revising their results for point predictions, we propose the use of \emph{probabilistic metrics} to study if label randomization really does not affect ICL predictive beliefs.
Then, in \S\ref{sec:author_id}, we study ICL on a novel task that we create and which ICL can \emph{only} solve by learning a novel label relationship that it cannot know from pre-training.




With the next two hypotheses, we study \emph{how} ICL incorporates label information.
First, we investigate how label relationships learned during pre-training interact with information provided by in-context examples.
In conventional learning algorithms, inductive biases, for example an explicit Bayesian prior, guide predictions in the absence of relevant training data.
Once the model has observed training data relevant to a particular test input, these data should increasingly dominate predictions.
With null hypothesis two, we focus on the following characteristic shared across many conventional learning algorithms: as the amount of relevant training data increases, inductive biases become less relevant and training data become more relevant for prediction.



\emph{\textbf{Null Hypothesis 2 (NH2)}: ICL predictions consider label information provided by in-context examples equivalently to label information observed during model pre-training.}


In other words, with NH2 we study how ICL predictions adjust in light of information provided by in-context observations.
Often, the LLM pre-training data contains some information towards a specific ICL task: even without any in-context examples, i.e.~in the \emph{zero-shot} setting, predictive performance is significantly better than random guessing.
This zero-shot \emph{pre-training preference} should act like an inductive bias, guiding predictions only when there are no relevant in-context examples.
In \S\ref{sec:flipped_arbitrary}, we study how ICL predictions adjust if we modify the label relationships of in-context examples to differ from pre-training preference.
If NH2 is true, then ICL should increasingly predict according to the (modified) label relationship of the in-context examples.
In \S\ref{sec:prompted}, we additionally study if prompts can be used to improve ICL behavior for modified label relationships.
Falsification of NH2 would illustrate one way in which ICL and conventional learning algorithms differ.



Usually, learning algorithms consider all information provided by the examples equally: if a dataset contains multiple sources of information about a label relationship, e.g.~the dataset itself is a union of multiple datasets, then all datasets are considered equally by the learning algorithm.
However, this is not always the case: for example, in time series models a (possibly implicit) temporal dimension affects predictions.
With null hypothesis three, we seek to improve our understanding of how ICL incorporates different sources of in-context information.



\emph{\textbf{Null Hypothesis 3 (NH3)}: ICL considers all information given in-context equally.}

We test NH3 in \S\ref{sec:time_series} by studying \emph{non-stationary} input distributions for which label relationships change throughout ICL.
This is distinct from prior work such as \citet{lu2021fantastically}, who observe that ICL is sensitive to the order of in-context examples of a \emph{single fixed} label relationship.
We instead study how ICL learns label relationships if the context contains multiple, possibly conflicting sources of information towards the label relationship.






\section{Experimental Setup \& ICL Training Dynamics}
\label{sec:dynamics}


\looseness=-1
We here detail our experimental setup for the subsequent evaluation of few-shot ICL performance.


\textbf{Models \& Tasks.}
We use LLMs from the LLaMa \citep{touvron2023LLaMa} and Falcon \citep{falcon2023} families due to their strong performance and open source nature.
We evaluate on SST-2, Subjective, Financial Phrasebank, Hate Speech, AG News, MQP, MRPC, RTE, and WNLI, cf.~\S\ref{sec:details} for citations.

\textbf{In-Context Dataset Size.}
We always report few-shot ICL performance at all possible in-context dataset sizes, i.e.~from zero-shot performance without any examples up to the maximum number of examples the LLMs can accommodate.
This allows us to study the \emph{training dynamics} of ICL, i.e.~the progression of ICL predictions with increasing number of in-context examples.
In contrast, prior work often evaluates few-shot ICL at only a few context sizes to reduce computational cost.



\textbf{Computationally Cheap Few-Shot ICL Evaluations.}
It turns out, we can obtain ICL predictions at \emph{all possible} context sizes without incurring any additional cost.
We achieve this by exploiting the fact that each forward pass through the model gives not just a prediction for the next token, but rather, the predicted probabilities for \emph{each} input token (given all tokens preceding it).
By extracting those token predictions that correspond to labels of in-context examples, we can obtain few-shot ICL predictions at all in-context dataset sizes with each forward pass.
We refer to \S\ref{sec:eval_approach} for a formalization of few-shot ICL and further description of our computationally cheap evaluation approach.


\textbf{Evaluation Metrics.}
We evaluate few-shot ICL performance in terms of accuracy ($\uparrow$) and (average) log likelihood ($\uparrow$).
We also report entropy ($\downarrow$), which is useful for understanding how much predicted probabilities are spread across classes.
While lower entropies are often desirable, entropies alone are not suitable for performance evaluation: as they can be low or high for correct or incorrect predictions, we should always consider entropies together with accuracy or log likelihood.
We additionally sometimes display a \emph{guessing} baseline, that predicts probabilities equal to the training set class frequencies of a task, cf.~\S\ref{sec:details}.
We report average metrics, randomizing both the in-context examples and their order by sampling from the larger training set.


\textbf{Default Training Dynamics.}
Before modifying label relationships in the following sections, \cref{fig:dynamics} shows standard few-shot ICL training dynamics on SST-2.
We observe reasonable behavior for all models:
as more in-context examples are observed, accuracies and log likelihoods increase, while entropies decrease.
Notably, log likelihoods and entropies show in-context \emph{learning} more clearly: they are more sensitive to changes in the predicted probabilities and show predictions continue to improve at larger context sizes, whereas accuracies saturate quickly.
Differences between models are larger for probabilistic metrics, too: while all Falcon models achieve similar accuracy, log likelihoods and entropies reveal that larger or instruction-tuned models predict with higher certainty.
% Figure environment removed





\vspace{-1mm}
\section{Does ICL Really Ignore Random Labels?}
\label{sec:random}
\vspace{-1mm}
We now study the null hypotheses (NH) formulated in \S\ref{sec:hypotheses}, starting with NH1.
NH1 states that ICL predictions are independent of the conditional label distribution of the in-context examples.
If NH1 were true, as suggested by \citet{min2022rethinking}, we would need to significantly scale back expectations about the capabilities of few-shot ICL in LLMs.
We study NH1 by replacing all labels of in-context examples with labels drawn randomly from the training set of the task.
If NH1 is true, then accuracy, log-likelihood, and entropy should be identical for the randomized and standard label scenario.



\textbf{Observations \& Discussion.}
\Cref{fig:random_multi_LLaMa-65b} shows ICL training dynamics for the randomized label scenario for LLaMa-65B on all tasks, while \cref{fig:random} gives performance for all models on SST-2.
In \S\ref{sec:extended_results}, we provide results for all remaining task-model combinations.
Across all models and tasks where performance is better than random guessing, we observe that LLMs react with lower log-likelihood and increased predictive entropy to the random label scenario.
This is reasonable from the perspective of probabilistic learning: as the noisy in-context labels are observed, estimates of the aleatoric uncertainty associated with the task should increase \citep{der2009aleatory,kendall_what_2017}.
Logically speaking, if ICL reacts to labels when they are randomized, it must also incorporate label information when they are not; otherwise, it could not know when labels are randomized.
\emph{\textbf{%
We reject NH1 that ICL predictions do not depend on the conditional label distribution of in-context examples.}}

\looseness=-1
In some instances, e.g.~LLaMa models in \cref{fig:random}, we even observe accuracies \emph{decreasing} for randomized labels.
However, in other cases, e.g.~all Falcon variants except Falcon-40B in \cref{fig:random}, we, similar to \citet{min2022rethinking}, observe that accuracies do not change significantly under label randomization, although we note that probabilistic metrics still reveal significant differences between scenarios.

We also sometimes observe log likelihoods improving despite randomized labels, although there is still a significant gap to the default label scenario, e.g.~for LLaMa-65B on Hate Speech in \cref{fig:random_multi_LLaMa-65b} or 



\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}

% Figure environment removed

the smaller Falcon models on SST-2 in \cref{fig:random}.
We speculate that, in line with \citet{min2022rethinking}, this corresponds to the model learning about aspects of the few-shot task other than the label distribution.

\looseness=-1
Lastly, in particular for weaker models, we do find tasks where there is no difference between default and randomized labels, e.g.~for LLaMa-7B in \cref{fig:random_multi_LLaMa-7b}.
In these cases, performance is often close to random guessing.
It is plausible that ICL does not suffer from randomized labels here because it also does not successfully learn the task in the default label scenario in the first place.
However, to reject NH1, it is sufficient to find a single instance in which it does not hold---and we here find many.



In particular for the large LLaMa-65B and Falcon-40B models, our observations agree with the behavior of conventional learning algorithms and indicate that ICL often behaves much more sensibly than prior work suggests.
We conclude that it is usually not without consequence to use random labels for ICL, in particular if one cares about the predicted probabilities.



\textbf{Differences to \citet{min2022rethinking}}.
Lastly, we discuss possible reasons for why \citet{min2022rethinking} arrive at the conclusion that label randomization only `barely hurts' ICL performance:
(1) They do not study probabilistic metrics, which we have shown to be more sensitive to randomized labels.
(2) They use a fixed ICL dataset size of \num{16}, but effects of random labels increase with in-context dataset size.
(3) They study only one model with more than 20B parameters (GPT-3), but we observe that larger models are more sensitive to randomized labels.
(4) On some datasets, ICL performance for \citet{min2022rethinking} could be close to random guessing, where we observe that label randomization has less of an effect.
Lastly, we note that we \emph{do} find evidence supporting their other claim that LLMs can benefit from learning about non-label information, such as task formatting, in few-shot ICL.


\section{Can ICL Learn Truly New Label Relationships?}
\label{sec:author_id}
\looseness=-1
\Cref{sec:random}  has shown that ICL predictions \emph{do} depend on the label relationship of in-context examples.
Next, we explore the \emph{extent} to which ICL can extract label information from the context.
Concretely, we study if LLMs can learn \emph{truly novel} label relationships in-context by creating our own task, avoiding existing NLP tasks, and ensuring the following desiderata are met:
the task \emph{must} not be contained in the pre-training corpus, and further, we want to avoid tasks that the model could learn implicitly during pre-training: e.g.~SST-2 data might not be part of the pre-training corpus directly, but clearly the LLM already learns how to assign sentiment labels given its strong zero-shot performance.

Specifically, we create an authorship identification \citep{stamatatos2009survey} dataset from private messages between two authors of this paper.
The task is to identify the author corresponding to a given message.
As messages stem from private communication, they are guaranteed to not be part of the pre-training corpus.
We give further details on the task in \S\ref{sec:author_details}.
For ICL to succeed here, it needs to learn the novel input--label relationship provided in-context: while the LLM could have some general notion of authorship identification tasks, unlike for our SST-2 example, the specific input--label relationship is novel, as the authors' private writing styles cannot be known to the LLM.

% Figure environment removed

\looseness=-1

\looseness=-1
\textbf{Observations \& Discussion.}
\Cref{fig:author_id} shows ICL succeeds at learning the author identification task.
Accuracies and log-likelihoods increase while entropies decrease: ICL behavior agrees with our expectations about probabilistic learning algorithm.
An mild exception here is Falcon-7B-Instruct, for which accuracies are better than random guessing but lower than elsewhere.
These results show that \textbf{\emph{LLMs can learn truly novel tasks in-context}} by inferring a label relationship from in-context examples.
Together with \S\ref{sec:random}, we conclude that ICL can and does incorporate in-context label information.
\section{How Do Pre-training Preference and In-Context Information 
 Interact?}
\label{sec:flipped_arbitrary}
With NH2, we explore how in-context label information trades off against the LLM's \emph{pre-training preference}, i.e.~its zero-shot predictions based on label relationships inferred from pre-training and stored in the model parameters.
Pre-training preference and in-context label relationships are often aligned: e.g.~in \cref{fig:dynamics}, performance is high zero-shot and then improves with few-shot ICL.
To study NH2, we create scenarios where pre-training preference and in-context observations are not aligned, allowing us to test if ICL conforms to our assumptions about conventional learning algorithms.

\looseness=-1
Instead of using the default labels, we use replacement labels when constructing the in-context dataset:
(1) We flip the default labels, e.g.~(positive, negative) for SST-2.
(2) We study additional word-pairs and their flipped versions as replacement labels, e.g.~(A, B) and (B, A).
We deliberately choose arbitrary replacement labels for (2) such that the LLM should not have a preference for assigning either label to positive or negative.
We then evaluate few-shot ICL performance for predicting the replacement label relationship.
This is different to another experiment of \citet{min2022rethinking}, who replace in-context labels randomly with arbitrary words and then continue to evaluate performance on the default labels.
In contrast, our setup \emph{consistently} modifies in-context label information which allows us to study ICL behavior when the context is misaligned with pre-training preference.


If NH2 is true, ICL should increasingly predict according to the label relationship of the in-context examples---regardless of whether in-context labels are unmodified or follow one of the replacement configurations.
If ICL conforms to our assumptions about learning algorithms, eventually, in-context information about the label relationship should overcome pre-training preference.
It should, eventually, not matter which word-pairs are used as labels.
Because there are practical limits to the maximum size of the context datasets (\S\ref{sec:details}), we do not necessarily expect to see performance reach the same level across label configurations.
However, we do expect to see training curves that are compatible with ICL eventually reaching equal performance across label configurations.


% Figure environment removed


\textbf{Observations \& Discussion.}
\looseness=-1
\Cref{fig:flipped} shows results on SST-2 and Hate Speech for LLaMa-65B and Falcon-40B.
We first study behavior of the flipped versions of the default labels (dashed blue lines).
Evidently, the models can, to an extent, learn flipped label relationships in-context.
However, performance on flipped labels \emph{plateaus} below the performance obtained for the default labels.
The difference is particularly large for the predicted entropies.
It seems unlikely that additional in-context observations could improve predictions to be on par with the default label scenario.
Instead, it seems that label relationships inferred from pre-training have a permanent effect that cannot be overcome through in-context observations.
This does not agree with NH2: predictions should continue to improve as observations continue to contradict pre-training preference.
\textbf{\emph{We reject NH2 that ICL predictions consider label information in-context equivalently to label information inferred from pre-training.}}

We further observe that for replacement labels (A, B) and (B, A) both label directions are often similarly easy for ICL to learn.
This indicates the LLM truly has not learned a preference for them during pre-training.
This makes sense intuitively, as the letters A and B should not be consistently associated with any of the task labels in the pre-training data.
Across the board, we observe that learning arbitrary replacement labels is slower than learning from the aligned default labels but faster than learning flipped default labels, which agrees with our intuitions about inductive biases in conventional learning algorithms.
However, we observe differences between LLaMa-65B and Falcon-40B: for LLaMa-65B, few-shot ICL with arbitrary labels continues to improve or even reaches default label performance, while for Falcon-40B, we do observe performance plateauing for arbitrary labels.

\looseness=-1
In \S\ref{sec:extended_results}, we report results across all tasks and models and for additional replacement labels.
Note that not all experiments are suitable for investigating NH2: we should only consider task and model combinations where performance on the default label relationship well exceeds random guessing and shows plateauing behavior, which is not always the case.
For example for Subjectivity (\cref{fig:flipped_app_subj}), we are limited by the input token limit and cannot reach plateauing performance for the default label setup---and thus cannot investigate NH2.
However, to reject NH2 it is sufficient to show ICL failing to conform to the hypothesis at least once.
And further, we do not find \emph{any} instances of the model reaching the same performance for the default and flipped label relationship when default performance exceeds random guessing, further supporting our decision to reject NH2.
Lastly, we observe that smaller LLMs do not learn non-default label relationships well and that instruction-tuned Falcon-40B performs worse than vanilla Falcon-40B for flipped default labels, suggesting that small model size and instruction-tuning may decrease ICL ability.
















% Figure environment removed

\vspace{-1mm}
\section{Can Prompts Help ICL Learn Flipped Label Relationships?}
\label{sec:prompted}
\vspace{-1mm}
In this section, we explore if \emph{prompts} can be used to overcome the plateauing ICL performance when default labels are flipped.
Before the in-context examples, we insert prompt strings that inform the LLM of the flipped label relationship in some fashion, and should thus help the LLM adjust to it during ICL.
These prompts could fundamentally change few-shot ICL behavior.
In fact, one can think of the in-context learner as the union of LLM and prompt, where so far the prompt was simply left empty.
There could exist prompts that help ICL learn the flipped label relationship better than without them, or as well as in the default scenario.
Note that, regardless of the outcome here, NH2 remains rejected as ICL should not need to rely on a prompt to correctly consider in-context observations.
Nevertheless, for the `prompted few-shot ICL' setup, NH2 should be reconsidered.



We explore the following three prompts: `In the following ...', \textcolor{pal4}{(Instruct Prompt)} `...negative means positive and positive means negative' (here for SST-2 and adapted to other tasks), \textcolor{pal2}{(Ignore Prompt)} `...ignore all prior knowledge', and \textcolor{pal9}{(Invert Prompt)} `...flip the meaning for all answers'.




\textbf{Observations \& Discussions.}
\Cref{fig:prompted_main} gives results for the prompted few-shot ICL setup for LLaMa-65B and Falcon-40B on SST-2, and we give results for our largest models across all tasks in \S\ref{sec:extended_results}.
We observe that prompts, in particular the \textcolor{pal4}{instruct} and \textcolor{pal9}{invert} prompts, can help improve ICL performance.
However, it seems that the positive impact from prompting is restricted to an initial boost at small in-context datasets sizes.
We then sometimes observe a `dip' in performance, which could indicate ICL forgetting about the prompt.
At large context sizes, none of our prompts have any advantage, and flipped label performance again plateaus short of performance for the default label setup.
\textbf{\emph{Therefore, we reject NH2 for the prompted ICL variations that we study.}}

\looseness=-1
It is possible that there exist prompts---that we have not found---for which we cannot reject NH2.
However, we are sceptical these prompts exist for the models we study, as their behavior at large context sizes is strikingly similar across all prompts we investigate.
For more capable LLMs, we suspect it may be possible to obtain a zero-shot performance on the flipped scenario that is equal to the zero-shot of the default scenario, i.e.~the prompt leads to the LLM perfectly flipping all its zero-shot predictions.
However, we are unsure if, in addition to flipping zero-shot predictions, such prompts would also improve \emph{ICL} on flipped label observations to be as good as in the default scenario.



\section{How Does ICL Aggregate In-Context Information?}
\label{sec:time_series}
With NH3, we study if ICL considers all in-context information equally.
In \S\ref{sec:flipped_arbitrary}, we have already seen that ICL does not conform to our expectations when it comes to reconciling pre-training preference and in-context samples.
However, it is similarly important to understand how ICL treats different sources of purely in-context information.
To investigate NH3, we change the label relationship \emph{during} in-context learning: after some observations of the default label relationship we flip labels for all following observations, e.g.~from (negative, positive) to (positive, negative) for SST-2.
% Figure environment removed


\looseness=-1
If NH3 is true, ICL should treat all observed examples equally, no matter their position in the input.
After flipping the label relation at the changepoint, entropies should increase to reflect the uncertainty about the label relationship introduced by the flipped label observations.
In contrast, accuracies should remain mostly unaffected, at least for a while: after observing many examples for which the default label relationship holds (which further aligns with pre-training preference), a few observations of the flipped label relationship typically will not change the argmax of the probabilities.





\textbf{Observations \& Discussion.}
\Cref{fig:time_series} shows results for LLaMa-65B on SST-2, and we report results for additional models, task, and setups in \S\ref{sec:extended_results}, where we exclude tasks and model combinations for which ICL was unable to learn the flipped relationships in \S\ref{sec:flipped_arbitrary}.
We observe that accuracies decrease steadily when label relations are flipped.
Further, accuracies for LLaMa-65B drop below \SI{50}{\percent} on the default label relationship after observing fewer flipped than default datapoints.
This does not conform to our expectations about conventional learning algorithms expressed by NH3.
It seems, ICL behaves more like a time series model, where token positions are the time dimension: at the changepoint, ICL begins to switch to that new label relationship.
In other words, LLMs may prefer to use information that is \emph{closer} to the query, instead of considering all available information equally.
Our results indicate that rejection of NH3 may be appropriate.
This would further be supported by \citet{zhao2021calibrate}, who find ICL prefers to predict \emph{labels}---note that we study \emph{label relationships} instead---that appear more frequently near the query.



\looseness=-1
Even if the behavior of ICL here does not conform to the assumption of order invariance, it might not be unreasonable.
Distribution shifts are a common appearance in practice, and it may actually be desirable for the model to quickly accommodate to any shifts.
Our experiments are not meant to be derisive of ICL behavior, but rather highlight its quirks and features.

\vspace{-1mm}
\section{Discussion \& Limitations}
\textbf{Alignment.} We believe that our results are worth considering from an alignment perspective.
For alignment of LLMs, it is crucial to understand how (a) pre-training preference and inputs trade-off, as well as (b) how different parts of the input, such as a context string and user input, interact and influence predictions.
If our results transfer to the alignment setting, they would suggest it is likely that prompt-based alignment, as pursued e.g.~by \citet{bai2022constitutional}, could be more easily overwritten by user input and is less likely to overwrite pre-training preference than in-weights alignment.


\textbf{Do labels always matter?}
We have shown that, for our selection of tasks, in-context labels are important to ICL predictions.
However, it is plausible that for other ICL tasks, labels matter less.
For example, for question answering tasks, the answers to the questions in-context will usually not be informative to the answer of the query question.
Therefore, ICL might be less sensitive to the provided in-context labels here.
However, for the randomized label experiment, capable LLMs might still realize that the provided in-context answers are random and imitate this by predicting a random answer for the query, too, or at least, by predicting with increased uncertainty.


\looseness=-1
\textbf{Limitations.}
We focus on few-shot ICL tasks where evaluation is based on logits and not free-form generations.
We do this mostly to avoid complications around evaluating free-form generation tasks, however, we have no reason to believe our results should not transfer to this setting.
Further, we also do not study RLHF-finetuned LLMs \citep{christiano2017deep,ziegler2019fine,ouyang2022training,bai2022training} due to a lack of availability of open source RLHF-finetuned models.
Lastly, we do not evaluate on closed source models due to concerns around reproducibility.



\vspace{-1mm}
\section{Related Work}
\label{sec:related_work}

\looseness=-1
In addition to prior work mentioned in \S\ref{sec:intro} and \S\ref{sec:background}, in-context learning has garnered significant attention.
For example, \citet{min2021metaicl} fine-tune language models to improve ICL abilities,
\citet{chan2022transformers,dasgupta2022distinguishing} study differences in generalization when LLMs predict in-weights or in-context, and \citet{chang2022careful,liu2021makes,zhang2022active} observe that, similar to standard machine learning models, the selection of examples affect ICL predictions significantly.
In this paper, we emphasize a probabilistic treatment of LLM predictions, where both entropy and log-likelihood give information about the certainty with which the model predicts.
Uncertainty in LLMs has previously been studied, for example, by \citet{kadavath2022language,lin2022teaching,bai2022training,gonen2022demystifying}.
On non-language tasks, \citet{kirsch2022general,chan2022data} study properties of the data and model that lead to the emergence of ICL.
Also related are \citet{garnelo2018neural,kossen2021self, gordon2018meta}, who propose deep architectures for non-parametric prediction on non-language tasks. Unlike ICL in LLMs, the proposed models can guarantee invariance to example-order or closely approximate Bayesian predictives \citep{muller2021transformers}.




\section{Conclusions}
In this paper, we have investigated the impact the conditional label distribution of in-context examples has on ICL predictions.
In some sense, we have shown that ICL is both better and worse than expected.
On the one hand, our results demonstrate that, against expectations set by prior work, ICL can and does incorporate in-context label information.
On the other hand, we have shown that analogies between ICL and conventional learning algorithms fall short in a variety of ways.
In particular, label relationships inferred from pre-training have a lasting effect that cannot be surmounted by in-context observations.
Additional prompting can improve, but likely not completely overcome, this deficiency.
Further, it ICL may behave more like a time series model that prefers to predict the label relationship nearest to the query, instead of treating all in-context label information equally.
We encourage other researchers, particularly those from the alignment community, to consider potential implications of our results to their field.


\FloatBarrier
\bibliography{iclr2023_conference}
\bibliographystyle{iclr2023_conference}
\FloatBarrier
\appendix
\counterwithin{figure}{section}
\counterwithin{table}{section}


\section{Evaluation Approach for Cheap In-Context Learning Dynamics}
\label{sec:eval_approach}

In this section, we suggest a---to the best of our knowledge---novel way of evaluating ICL that gives performance metrics at all in-context dataset sizes in a single forward pass without incurring additional cost.
We start by introducing the notation necessary to formalize few-shot ICL in LLMs.

\textbf{Dataset to Input String.}
The few-shot task is defined by a dataset $\mathcal{D} = \{(S_i, Y_i)\}_{i=1}^N$, where $S_i\in \mathcal{T}^{d_{S_i}}$ are input sentences from which to predict the associated labels $Y_i\in\mathcal{T}^{d_{y_i}}$, and $\mathcal{T}^{v}$ are text strings of length $v$.
A \emph{verbalizer} $V(S,Y)$ takes a sentence--label pair and maps it to an \emph{example}, e.g.~the sequence `I am happy' and label `positive' are verbalized as `Sentence: I am happy\textbackslash n Label: positive\textbackslash n'.
We also define a \emph{query verbalizer} $V_q(S)$ that maps a test query $S$ to a query example, e.g.~`I am sad' is mapped to `Sentence: I am sad\textbackslash n Label:', such that the next-token prediction of an LLM will be encouraged to predict the label for the query. 
We apply the verbalizer to the entire dataset set and concatenate its output to obtain the \emph{context} $\mathcal{C} = \oplus_{i=1}^N  V(S_i, Y_i)$.
Finally, we concatenate context $\mathcal{C}$ and verbalized query $V_q(S)$, where $S$ is a sentence drawn from a separate test set, to obtain the input to the language model, $I = \mathcal{C} \oplus V_q(S) \in \mathcal{T}^{d_I}$.

\textbf{Input String to Tokens.}
The input $I$ is \emph{tokenized} before it can be processed by the language model.
The tokenizer, $T(I)=(X_1, \dots, X_M)$, maps an input sequence $I$ to a sequence of integers, or tokens, $X_i\in (1, \dots, D)$, where $D$ is the vocabulary size, i.e.~the number of unique tokens.
We keep track of which token positions correspond to labels, $\mathcal{L} =(l_1, \dots, l_N)$, e.g.~the indices of the tokens immediately following the string `Label:' in the above example.

\textbf{Tokens to Predictions.}
In the following, we use capital letters to denote random variables and lower-case letters for their realizations.
Here, we describe the behavior of \emph{decoder-only} language models \citep{liu2018generating,radford2018improving}, a popular architecture choice for LLMs.
Given the observed sequence of input tokens $(X_1=x_1, \dots, X_M=x_M)$, a single forward pass through the language model gives an estimate of the \emph{joint probability},
\begin{align}
     p(X_1=x_1) \cdot p(X_2=x_2 \mid X_1=x_1) \cdot ... \cdot p(X_M=x_M\mid X_1=x_1, \dots, X_{M-1}=x_{M-1}). \label{eq:joint_raw}
\end{align}
We highglight that \cref{eq:joint_raw} gives the joint probability \emph{at the observed outcomes}: we obtain $M$ `one-step ahead' predictions, each conditioned only on observed outcomes and not on model predictions.
\Cref{eq:joint_raw} is a common objective in LLM training, where `the joint probability the model assigns to the observations' is sometimes referred to as \emph{teacher forcing} \citep{williams1989learning}.

At test time, LLMs are usually iteratively conditioned on their own \emph{predictions}, generating novel outputs via multiple forward passes, i.e.~one first samples $\hat{x}_{M}\sim p(X_{M}|\dots)$, and then $\hat{x}_{M+1}\sim p(X_{M+1} \mid \dots, X_{M}=\hat{x}_{M})$, and so on.
We here use `$\dots$' to stand in for any additional tokens also conditioned on, e.g. $(x_1, \dots, x_{M-1})$.
One usually ignores all other terms of the joint here---the predictions for $(X_1, \dots, X_{M-1})$ that are generated in each forward pass---as only the last term $p(X_M\mid \dots)$ is needed to sample the next token, i.e.~the label in standard few-shot ICL applications.


\textbf{Single-Forward Pass ICL Training Dynamics.}
We now explain our approach for efficient evaluation of ICL training dynamics.
Given input tokens $(X_1, \dots, X_M)$ for the few-shot ICL setup described above, we first select those terms from \cref{eq:joint_raw} that correspond to label token predictions,
\begin{align}
     \prod\nolimits_{i=1}^N p(X_{l_i}=x_{l_i} \mid X_1=x_1, \dots, X_{m < l_i} = x_{m < l_i}). \label{eq:joint_intermediate}
\end{align}
For each term, the model predicts a distribution over the entire token vocabulary, i.e.~$p(X_{l_i}\mid \dots)$ is a categorical distribution, $p = (p_1, \dots, p_D) = \mathbb{R}^D$, which is then evaluated at the observed tokens in \cref{eq:joint_intermediate}.
We can transform this into a prediction over only the few-shot task label $Y$ by selecting the indices of the categorical distribution which correspond to the tokenized labels and then renormalizing, $p(Y) = (p_{t_1}, \dots, p_{t_C}|\dots)$, where $C$ is the number of unique labels which are encoded to tokens $(t_1, \dots, t_C)$.
With this, we can rewrite \cref{eq:joint_intermediate} as the joint probability the model assigns to the sequence of sentence--label pairs $\{S_i=s_i,Y_i=y_i\}_{i=1}^N$,
\begin{multline}
    p(Y_1 = y_1 \mid S_1=s_1) \cdot p(Y_2=y_2 \mid S_1=s_1, Y_1=y_1, S_2=s_2) \cdot \dots \\
    \cdot p(Y_N=y_N \mid S_1=s_1, Y_1=y_1, \dots, S_{N-1}=s_{N-1}, Y_{N-1}=y_{N-1}, S_N=s_N) \label{eq:joint_label}.
\end{multline}
Note how, because the joint is evaluated at the observations, its individual terms are always conditioned on the true labels and not previous predictions.
This allow us to cheaply compute the \emph{training dynamics} of ICL as a function of increasing in-context dataset size.
With each forward pass, we obtain the individual terms of \cref{eq:joint_label}, which are the few-shot ICL predictions at all possible context dataset sizes, $i=(1, \dots, N)$.
In contrast, in standard few-shot ICL evaluations, each forward pass only yields predictions for a single test query, neglecting the information the joint contains about the first $N-1$ label predictions.
There may be interesting applications of \cref{eq:joint_label} to model selection, as the quantity has links to both Bayesian evidence \citep{murphy2022probabilistic} and cross-validation \citep{fong2020marginal}, although we do not explore this any further in this paper.

\textbf{Multi-Token Labels.}
So far, we have assumed that each label string is encoded as a single token.
However, our approach can also be applied if some or all labels are encoded as multiple tokens.
In essence, we continue to measure only the probability the model assigns to the first token of each label, making the (fairly harmless) assumption that the first (or only) token that each label is encoded to is unique among labels.
We believe this is justified, as, given the first token for a label, the model should near-deterministically predict the remaining tokens, i.e.~all the predictive information is contained in the first token the model predicts for a label.
For example, for the Subjectivity dataset, the label `objective' is encoded by the LLaMa tokenizer as a single token but the label `subjective' is encoded as two tokens, [subject, ive].
We only use the probability assigned to [subject] to assign probabilities to `subjective', and ignore any predictions for [ive].
However, if the model successfully accommodates the pattern of the in-context example labels, we would expect probabilities for [ive] following [subject] to be close to \num{1} always.

For LLaMa-7B on Subjectivity, we have investigated the above assumption empirically.
After the first observation of the `subjectivity' label in-context, the probability of predicting [ive] after observing [subject] are $0.9998 \pm 0.0003$ for the following \num{12} instances of the `subjectivity' label, with probabilities normalized over \emph{all} tokens of the vocabulary here.
In other words, we can safely evaluate the performance of the LLaMa model from its predictions of only the [subject] token, even though the full label is split over two tokens [subject, ive].



\section{Authorship Identification Task}
\label{sec:author_details}
We here give details on our novel authorship identification task.

\textbf{Data Collection \& Processing.}
We extract the last \num{151} messages sent between two authors of this paper on the Slack messaging platform.
If multiple messages are sent in a row by one author, these count as multiple messages.
We filter out \num{42} messages that were of low quality: URLs, article summaries, missed call notifications, and meeting notes.
This leaves us with \num{58} and \num{51} messages per author.
We set the maximum message length to be \num{200} and truncate any messages longer than that.
Before truncation, the longest message was \num{579} characters long.
The median message length is \num{68} before and after truncation, mean and standard deviation shrink from $100 \pm 98$ to $88 \pm 65$.
For use in ICL, we treat this dataset as we would any other and present messages in random order.

\textbf{Data Release.}
For now, we have decided to not make this dataset public for two reasons: (1) It contains genuinely private communication, and (2) releasing the data would mean that future LLMs might be trained on it, so we could no longer use it to test their ability to learn truly novel label relationships in-context.

\section{Experiment Details}
\label{sec:details}

Below we give additional details on our experimental evaluation.

\textbf{Guessing Baseline.}
In our experiments, we frequently display a `\textcolor{pal7}{guessing} based on class frequencies' baseline as a grey-dashed line.
This baseline presents an \emph{informed guess} that relies only on knowing the class frequencies of the task and makes the exact same prediction for each input datapoint.
We here explain how we compute this baseline for accuracy, entropy, and log-likelihood.
We are given a classification task with $C$ classes which appear with frequencies $p=[p_1, \dots, p_C]$ in the training set.
The baseline always predicts $p=[p_1, \dots, p_C]$, i.e.~it predicts the class frequencies.
For accuracy, it thus always predicts the majority class $c^* = \arg\max_k p_k$, which leads to accuracy $p_{c^*}$.
Further, the baseline prediction leads to a log likelihood of $\sum_k p_k \log p_k$ and an entropy of $-\sum_k p_k \log p_k$ under the training data distribution.

\looseness=-1
\textbf{Class Flipping.}
While most of our tasks are \emph{binary} classification, Financial Phrasebank and AG News are not.
For these datasets, when `flipping' labels in \S\ref{sec:flipped_arbitrary}, \S\ref{sec:prompted}, and \S\ref{sec:time_series}, we actually rotate labels instead, i.e.~we reassign labels $y$ as $y \leftarrow (y + 1) \mod C$, where $C$ is the number of classes.
For AG News, ['world', 'sports', 'business', 'science and technology'] get mapped to ['sports', 'business', 'science and technology', 'world']. 
For Financial Phrasebank, ['negative', 'neutral', 'positive'] get mapped to ['neutral', 'positive', 'negative'].
Note that, for Financial Phrasebank, rotating the labels is harder than naively inverting the label order, as rotating does not leave the meaning of the `neutral' label unchanged.

\textbf{In-Context Example Formatting.}
We use the following simple templates to format the in-context examples.
For SST-2, Subjectivity, Financial Phrasebank, Hate Speech, and our author identification task, we use the following line of Python code to format each input example: \\
{\footnotesize \texttt{f"Sentence: '\{sentence\}'\textbackslash nAnswer: \{label\}\textbackslash n\textbackslash n"}}.
\newline
For MRPC, WNLI, and RTE, we format instances with \newline
 {\footnotesize \texttt{f"Sentence 1: '\{sentence1\}'\textbackslash nSentence 2: '\{sentence2\}'\textbackslash nAnswer: \{label\}\textbackslash n\textbackslash n"}}.\newline
For MQP, we use \newline
 {\footnotesize \texttt{f"Question 1: '\{sentence1\}'\textbackslash nQuestion 2: '\{sentence2\}'\textbackslash nAnswer: \{label\}\textbackslash n\textbackslash n"}}.

\textbf{Implementation.}
We rely on the Hugging Face Python library \citep{wolf2019huggingface} and PyTorch \citep{NEURIPS2019_9015} to implement the experiments of this paper.
We use half-precision floating-point numbers for LLaMa-65B and 8bit-quantization for all other models, which we have found to not affect performance notably.

\textbf{Datasets.}
We evaluate on SST-2 \citep{socher2013recursive}, Subjective \citep{wang2012baselines}, Financial Phrasebank \citep{Malo2014GoodDO}, Hate Speech \citep{gibert2018hate}, AG News \citep{Zhang2015CharacterlevelCN}, Medical Questions Pairs (MQP) \citep{mccreery2020effective}, as well as Microsoft Research Paraphrase Corpus (MRPC) \citep{dolan2005automatically},  Recognizing Textual Entailment (RTE) \citep{dagan2005pascal}, and Winograd Schema Challenge (WNLI) \citep{levesque2012winograd} from GLUE \citep{wang2019glue}.
We use Hugging Face Spaces to access all tasks considered in this paper.
For Hate Speech, we select the first \num{1000} examples with labels \num{0} and \num{1}, skipping datapoints with labels \num{2} and \num{3}.
We do not use custom processing for any other dataset.

\textbf{Whitespace Tokenization.}
The Falcon and LLaMa model families treat the tokenization of whitespaces differently.
These details can be important for ICL, leading to significant performance degradation if not considered correctly.
We need to ensure that the token sequences in-context are also the token sequences the model can predict for the queries.
Note that for our evaluation approach in \S\ref{sec:eval_approach}, this is less of a problem, as there is no separation between queries and in-context training points: each datapoint is used both as query (given all examples preceding it) and as an in-context training example (by all datapoints following it) in each forward pass.

For example, for the SST-2 label `positive', the LLaMa tokenizer encodes \texttt{`positive`} as \texttt{[\textcolor{pal0}{6374}]} and \texttt{` positive`} as \texttt{[29871, \textcolor{pal0}{6374}]}, adding a whitespace token.
However, the Falcon tokenizer encodes \texttt{`positive`} as \texttt{[\textcolor{pal2}{28265}]} and \texttt{` positive`} as \texttt{[\textcolor{pal1}{3508}]}, assigning a different unique token for the `whitespace + word' combination.
The string \texttt{`Answer: positive`} is encoded as  \texttt{[673, 29901, \textcolor{pal0}{6374}]} by the LLaMa tokenizer and as \texttt{[20309, 37, \textcolor{pal1}{3508}]} by the Falcon tokenizer.
While the Falcon tokenizer uses different tokens for words preceded by whitespaces, the LLaMa tokenizer adds whitespaces implicitly.
Thus, for Falcon models, the relevant label token belongs to \texttt{` positive`} and for LLaMa models to \texttt{`positive`}.

Further, for both models, if we want the model to predict the label for a query input, we need to end our input with \texttt{`Answer:`} not \texttt{`Answer: `}.
For the Falcon models, the whitespace is part of the relevant token, and for LLaMa models, the whitespace gets added implicitly.
If we would use \texttt{`Answer: `} instead, we would end up with an additional whitespace token in the input that is not part of the tokenized in-context example demonstrations.
The sequence of tokens for the query predictions then \emph{cannot} match the sequence of in-context examples, which can degrade ICL performance.


\textbf{Maximum Context Dataset Size.}
For each task, we create in-context datasets by sub-sampling from the training set of the task.
Falcon and LLaMa models support input sizes up to \num{2048} tokens and performance will degrade if the input size exceeds this limit.
This caps the number of in-context examples we can include for each task.
For tasks where the individual input sentences are longer, we will be able to include fewer examples in-context.
Across all in-context datasets that we sample for a task, we compute the minimum number of in-context examples needed to exceed the \num{2048} token limit.
This is the maximum in-context dataset size for which we report results for that task.
We find that, usually, the Falcon tokenizer is more efficient than the LLaMa-tokenizer, allowing us to study larger in-context dataset sizes.


\textbf{Calibration.}
We do not calibrate predicted probabilities by first dividing them by a `prior' probability and then renormalizing as suggested by \citet{zhao2021calibrate}.
We have found this rarely improves, and sometimes significantly degrades predictions, cf.~\cref{fig:random_multi_calibrate_llama-65b}, in particular, when labels are encoded as multiple tokens.





\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}


\section{Extended Results}
\label{sec:extended_results}

\textbf{\Cref{sec:random}}: \Cref{fig:random_multi_LLaMa-7b,fig:random_multi_LLaMa-13b,fig:random_multi_LLaMa-65b2,fig:random_multi_falcon-7b,fig:random_multi_falcon-7b-instruct,fig:random_multi_falcon-40b,fig:random_multi_falcon-40b-instruct} give results for all tasks and models comparing ICL with randomized labels to the default label setup.

\textbf{\Cref{sec:flipped_arbitrary}}: \Cref{fig:flipped_app_sst2,fig:flipped_app_subj,fig:flipped_app_financial_phrasebank,fig:flipped_app_hate_speech,fig:flipped_app_ag_news,fig:flipped_app_medical_questions_pairs,fig:flipped_app_mrpc,fig:flipped_app_rte,fig:flipped_app_wnli} give results for all tasks and models for the modified label relationship experiments.

\textbf{\Cref{sec:prompted}}: \Cref{fig:prompted_appendix_sst2,fig:prompted_appendix_subj,fig:prompted_appendix_financial_phrasebank,fig:prompted_appendix_hate_speech,fig:prompted_appendix_ag_news,fig:prompted_appendix_medical_questions_pairs,fig:prompted_appendix_mrpc,fig:prompted_appendix_rte,fig:prompted_appendix_wnli} give results for all tasks and models for the prompted few-shot ICL setup.


\textbf{\Cref{sec:time_series}}: In \cref{fig:time_series_appendix_sst2,fig:time_series_appendix_subj,fig:time_series_appendix_financial_phrasebank,fig:time_series_appendix_hate_speech,fig:time_series_appendix_ag_news}, we give results for the label changepoint experiments for LLaMa-65b, Falcon-40, and Falcon-40B-Instruct on tasks for which models could learn the flipped relationship in \S\ref{sec:prompted}.


In \cref{fig:time_series_LLaMa-65b,fig:time_series_falcon-40b,fig:time_series_falcon-40b-instruct}, we further present a variant of the changepoint experiment where we additionally investigate \emph{starting} with the \emph{flipped} label relationship and then changing to the default label relationship at the changepoint after $n=15$ observed examples.
In \cref{fig:time_series_flip_after_40_LLaMa-65b,fig:time_series_flip_after_40_falcon-40b,fig:time_series_flip_after_40_falcon-40b-instruct}, we further ablate inverting the label relationship after $n=40$ in both directions.
We perform these experiments for LLaMa-65B, Falcon-40B and Falcon-40B-Instruct on SST-2.
They exhibit the same problematic behavior as our main experiments in \S\ref{sec:time_series}.

In \cref{fig:time_series_random_first_40_LLaMa-65b,fig:time_series_random_first_40_falcon-40b,fig:time_series_random_first_40_falcon-40b-instruct} we additionally present results for a variation of the experiments in \S\ref{sec:time_series}: we randomize the labels for the first $n=35$ in-context observations of SST-2 (as in \S\ref{sec:random}), and then switch to either the default or the flipped label setup.
We observe that pre-training preference continues to affect model learning, with models still learning the default label relationship much quicker than the flipped one.



\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}
\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}
\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}
\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}
\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}

\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}
\begin{landscape}
    \thispagestyle{empty}
    % Figure environment removed
\end{landscape}


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed
% Figure environment removed

% Figure environment removed

% Figure environment removed
% Figure environment removed

\end{document}

