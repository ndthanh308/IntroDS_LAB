@article{kurniawati2021partially,
  title={Partially observable markov decision processes (pomdps) and robotics},
  author={Kurniawati, Hanna},
  journal={arXiv preprint arXiv:2107.07599},
  year={2021}
}

@article{hernandez2019survey,
  title={A survey and critique of multiagent deep reinforcement learning},
  author={Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={33},
  number={6},
  pages={750--797},
  year={2019},
  publisher={Springer}
}

@article{bakker2001reinforcement,
  title={Reinforcement learning with long short-term memory},
  author={Bakker, Bram},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@article{harmon1996multi,
  title={Multi-player residual advantage learning with general function approximation},
  author={Harmon, Mance E and Baird III, Leemon C}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@inproceedings{chung2015gated,
  title={Gated feedback recurrent neural networks},
  author={Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={2067--2075},
  year={2015},
  organization={PMLR}
}
@inproceedings{hausknecht2015deep,
  title={Deep recurrent q-learning for partially observable mdps},
  author={Hausknecht, Matthew and Stone, Peter},
  booktitle={2015 aaai fall symposium series},
  year={2015}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{heess2015memory,
  title={Memory-based control with recurrent neural networks},
  author={Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David},
  journal={arXiv preprint arXiv:1512.04455},
  year={2015}
}
@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}
@inproceedings{meng2021memory,
  title={Memory-based deep reinforcement learning for pomdps},
  author={Meng, Lingheng and Gorbet, Rob and Kuli{\'c}, Dana},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5619--5626},
  year={2021},
  organization={IEEE}
}
@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{gaudet2019adaptive,
  title={Adaptive guidance with reinforcement meta-learning},
  author={Gaudet, Brian and Linares, Richard},
  journal={arXiv preprint arXiv:1901.04473},
  year={2019}
}
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}
@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}
@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={279--292},
  year={1992},
  publisher={Springer}
}
@book{van2011insights,
  title={Insights in reinforcement learning: formal analysis and empirical evaluation of temporal-difference learning algorithms},
  author={Van Hasselt, Hado Philip and others},
  year={2011},
  publisher={Utrecht University}
}
@article{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}
@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}
@book{ravichandiran2019hands,
  title={Hands-On Deep Learning Algorithms with Python: Master deep learning algorithms with extensive math by implementing them using TensorFlow},
  author={Ravichandiran, Sudharsan},
  year={2019},
  publisher={Packt Publishing Ltd}
}
@misc{OpenAI-Gym,
  title = {{OpenAI Gym}},
  howpublished = {\url{https://gymnasium.farama.org/}},
  note = {Accessed: 2023-01-14}
}

@phdthesis{hauskrecht1997planning,
  title={Planning and control in stochastic domains with imperfect information},
  author={Hauskrecht, Milos},
  year={1997},
  school={Massachusetts Institute of Technology}
}

@article{hauskrecht2000value,
  title={Value-function approximations for partially observable Markov decision processes},
  author={Hauskrecht, Milos},
  journal={Journal of artificial intelligence research},
  volume={13},
  pages={33--94},
  year={2000}
}
@article{how2016dynamic,
  title={Dynamic Output-Feedback Control Architectures Matter [Focus on Education]},
  author={How, Jonathan P},
  journal={IEEE Control Systems Magazine},
  volume={36},
  number={6},
  pages={88--117},
  year={2016},
  publisher={IEEE}
}
@book{bertsekas2012dynamic,
  title={Dynamic programming and optimal control: Volume I},
  author={Bertsekas, Dimitri},
  volume={1},
  year={2012},
  publisher={Athena scientific}
}

@inproceedings{mcclement2022meta,
  title={Meta-Reinforcement Learning for Adaptive Control of Second Order Systems},
  author={McClement, Daniel G and Lawrence, Nathan P and Forbes, Michael G and Loewen, Philip D and Backstr{\"o}m, Johan U and Gopaluni, R Bhushan},
  booktitle={2022 IEEE International Symposium on Advanced Control of Industrial Processes (AdCONIP)},
  pages={78--83},
  year={2022},
  organization={IEEE}
}

@misc{chris1995neural,
  title={Neural networks for pattern recognition. Advanced texts in econometrics},
  author={Chris Bishop, J and Bishop, C and Hinton, G and Bishop, P},
  year={1995},
  publisher={Oxford: Clarendon Press}
}

@article{botvinick2019reinforcement,
  title={Reinforcement learning, fast and slow},
  author={Botvinick, Matthew and Ritter, Sam and Wang, Jane X and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
  journal={Trends in cognitive sciences},
  volume={23},
  number={5},
  pages={408--422},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{janner2021offline,
  title={Offline reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1273--1286},
  year={2021}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}
@inproceedings{hasani2021liquid,
  title={Liquid time-constant networks},
  author={Hasani, Ramin and Lechner, Mathias and Amini, Alexander and Rus, Daniela and Grosu, Radu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={7657--7666},
  year={2021}
}
@inproceedings{shen2021igibson,
  title={iGibson 1.0: A simulation environment for interactive tasks in large realistic scenes},
  author={Shen, Bokui and Xia, Fei and Li, Chengshu and Mart{\'\i}n-Mart{\'\i}n, Roberto and Fan, Linxi and Wang, Guanzhi and P{\'e}rez-Dâ€™Arpino, Claudia and Buch, Shyamal and Srivastava, Sanjana and Tchapmi, Lyne and others},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={7520--7527},
  year={2021},
  organization={IEEE}
}
@article{freeman2021brax,
  title={Brax--A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  author={Freeman, C Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  journal={arXiv preprint arXiv:2106.13281},
  year={2021}
}
@article{truong2022rethinking,
  title={Rethinking sim2real: Lower fidelity simulation leads to higher sim2real transfer in navigation},
  author={Truong, Joanne and Rudolph, Max and Yokoyama, Naoki and Chernova, Sonia and Batra, Dhruv and Rai, Akshara},
  journal={arXiv preprint arXiv:2207.10821},
  year={2022}
}
@article{miki2022learning,
  title={Learning robust perceptive locomotion for quadrupedal robots in the wild},
  author={Miki, Takahiro and Lee, Joonho and Hwangbo, Jemin and Wellhausen, Lorenz and Koltun, Vladlen and Hutter, Marco},
  journal={Science Robotics},
  volume={7},
  number={62},
  pages={eabk2822},
  year={2022},
  publisher={American Association for the Advancement of Science}
}
@article{li2015recurrent,
  title={Recurrent reinforcement learning: a hybrid approach},
  author={Li, Xiujun and Li, Lihong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Deng, Li and He, Ji},
  journal={arXiv preprint arXiv:1509.03044},
  year={2015}
}
@article{singla2019memory,
  title={Memory-based deep reinforcement learning for obstacle avoidance in UAV with limited environment knowledge},
  author={Singla, Abhik and Padakandla, Sindhu and Bhatnagar, Shalabh},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={22},
  number={1},
  pages={107--118},
  year={2019},
  publisher={IEEE}
}

@article{pearl2000models,
  title={Models, reasoning and inference},
  author={Pearl, Judea and others},
  journal={Cambridge, UK: CambridgeUniversityPress},
  volume={19},
  number={2},
  year={2000}
}

@article{gasse2021causal,
  title={Causal reinforcement learning using observational and interventional data},
  author={Gasse, Maxime and Grasset, Damien and Gaudron, Guillaume and Oudeyer, Pierre-Yves},
  journal={arXiv preprint arXiv:2106.14421},
  year={2021}
}

@article{malikopoulos2022separation,
  title={On Separation Between Learning and Control in Partially Observed Markov Decision Processes},
  author={Malikopoulos, Andreas A},
  journal={arXiv preprint arXiv:2211.14972},
  year={2022}
}
