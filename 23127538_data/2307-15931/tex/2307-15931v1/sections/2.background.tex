\section{Preliminary}\label{background}
The neural network and algorithm related to this work is introduced in this section. Further basics of RL can be found in \cite{sutton2018reinforcement}.
% the problem, use example
% \subsection{Reinforcement learning}

% Reinforcement learning (RL) is one of the machine learning problems. In RL, the agent is trained to make the decision $a_t$ at each time step $t$ when its state $S_t$ is given as observation $O_t$ from the environment. The learning is based on the immediate reward signal $R_t$ from the interaction between agent and environment as shown in Fig.\ref{fig:RL_concept}. Reward $R_t$ is normally a scalar feedback signal which represents the evaluation of the action $A_t$ when the agent is in the state of $S_t$. The objective of the system is to maximize the expected cumulative reward. Since the goal is the maximization of the total future reward, the agent can select the action in the dynamic environment. 
% % Figure environment removed
% The policy is one of the conceptual components of RL. At each time step, the agent decides its action based on the policy $\pi$. The policy maps from observation to probabilities of the next action, i.e., $\pi(a\vert s)$.
% Value functions are also one of the other components of RL. The value function is an estimation of the future reward and it provides the estimated evaluation of being in the state, i.e., state-value function (\ref{equ:value_fn}), or taking action with the state, i.e., action-value function(\ref{equ:actionvalue_fn}). In value functions, the discounted cumulative future reward, return $G_t$, from the current time step is defined under the policy $\pi$.
% \begin{equation}\label{equ:value_fn}
%     v_\pi(s) = \mathbb{E}_\pi[G_t\vert S_t = s] = \mathbb{E}_\pi\left[ \sum^{\infty}_{k=0} r^kR_{t+k+1}\vert S_t = s\right]
% \end{equation}
% \begin{equation}\label{equ:actionvalue_fn}
%     q_\pi(s, a) = \mathbb{E}_\pi[G_t\vert S_t = s, A_t = a] = \mathbb{E}_\pi\left[ \sum^{\infty}_{k=0} r^kR_{t+k+1}\vert S_t = s, A_t = a\right]
% \end{equation}
% Also, in the form of the Bellman expectation equation, these can be decomposed as 
% \begin{equation}\label{equ:value_fn2}
%     v_\pi(s) = \mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})\vert S_t = s]
% \end{equation}
% \begin{equation}\label{equ:actionvalue_fn2}
%     q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})\vert S_t = s,A_t = a]
% \end{equation}
% Solving the MDP in RL is eventually finding a policy that achieves the best return during the episode. The optimal policies $\pi_*$ have the optimal state-value function $v_*(s)$ or optimal action-value function $q_*(s,a)$ and they are defined as 
% \begin{equation}\label{equ:value_fn3}
%     v_*(s) = \max_\pi v_\pi(s) 
% \end{equation}
% for all $s \in \mathcal{S}$
% \begin{equation}\label{equ:actionvalue_fn3}
%     q_*(s, a) = \max_\pi q_\pi(s,a)  
% \end{equation}
% for all $s \in \mathcal{S}$ and $a \in \mathcal{A(s)}$. Particularly, the Bellman optimality equation for $q_*$ is described as  
% \begin{equation}\label{equ:actionvalue_fn4}
% \begin{split}
%     q_*(s, a)& = \mathbb{E}\left[R_{t+1}+\gamma \max_{a'}q_*(S_{t+1},a')\vert S_t=s, A_t = a \right] \\
%     &= \sum_{s', r}p(s', r\vert s, a)\left[ r+\gamma \max_{a'}q_*(s',a') \right].
% \end{split}
% \end{equation}

 
% \subsection{Partially Observable Markov Decision Process}
% In the form of RL, the history of information is retained from sequential observations, actions and rewards. The information accumulates as time elapses. When a current state succeeds in all relevant information from the history, the state signal is called Markov property \cite{sutton2018reinforcement}. If the state signal meets the Markov property, the probability of the dynamics of the environment can be represented as (\ref{equ:markov}) 
% \begin{equation}\label{equ:markov}
%   \begin{array}{l}
%     p(s', r\vert s, a) = Pr \left\{ S_{t+1}=s', R_{t+1}=r \vert S_0, A_0, R_1,\dots ,S_t, A_t\right\}
%     = Pr\left\{ S_{t+1}=s', R_{t+1}=r \vert S_t, A_t\right\}
%   \end{array}
% \end{equation}
% The tasks of RL are assumed to satisfy Markov property and this decision making process is called Markov Decision Process (MDP). The model, as a component of RL, is the representation of the environment for the agent. From this environment's dynamics with Markov property, the model can be described with the state-transition probability (\ref{equ:statetransition}) and reward expectation for state-action pairs (\ref{equ:expectation}). 
% \begin{equation}\label{equ:statetransition}
%     p(s'\vert s, a) =  Pr[S_{t+1}=s' \vert S_t = s, A_t = a] = \sum_{r\in \mathcal{R}}p(s', r\vert s, a)
% \end{equation}
% \begin{equation}\label{equ:expectation}
%     r(s,a) = \mathbb{E}[R_{t+1}\vert S_t = s, A_t = a]  = \sum_{r\in \mathcal{R}}r\sum_{s'\in \mathcal{S}}p(s', r\vert s, a)  
% \end{equation}
% However, the observation for the tasks in the real world often contains noise and disturbance. Sometimes the full state of the system is not observable. The decision process in the condition where the agent has no access to the true system state from observation ($s_t \neq o_t$) is called POMDP. In this condition, the agent is required to learn to construct internal state representation or requires an external state estimator to map its action from the available information. For Q-value representation in POMDP, $Q(o_t,a_t)$ is no longer equal to $Q(s_t,a_t)$. One of the effective approaches is to use historical information through using Recurrent Neural Networks (RNN) \cite{hernandez2019survey}. Further details of the examples which apply RNN in RL to solve POMDP can be found in Section \ref{relatedWork}. 

\subsection{Recurrent Neural Network}
% figure and algorithm of LSTM
% BPTT
% overfitting
 RNN is nowadays a widely used technique, yet it is worth briefly revisiting the concept since later discussion and algorithm development require the detail of RNN. RNN is a suitable learning network to deal with sequential data as input. Fig. \ref{fig:FNNRNN} contrasts the difference between Feedforward Neural Networks (FNN) and RNN. Unlike FNN, RNN carries the context of sequential observation and make predictions. On the other hand, FNN treats every input independently from other sequentially provided inputs and it returns outputs only depending on the current neural state \cite{ravichandiran2019hands}. For example, the text generation application requires the prediction of the next word coming based on not only the last word but also the previous words. RNN can predict output as a function of the current input and the inherited hidden states that capture contextual information from all of the previous inputs based on its trajectory. As a deep neural network application, RNN is widely applied for cases where sequential data is involved in the prediction (e.g. audio speech, time series prediction). 
% Figure environment removed
In Fig. \ref{fig:FNNRNN}, the hidden state $h_{t-1}$ from the hidden layer is carried to generate output data $y_{t}$ at the output layer when new input data $x_{t}$ enters in RNN. In case of RL, input data $x_{t}$ can be direct observation $o_t$ or output from the previous layer originated from $o_t$. The original RNN suffered from a vanishing gradient problem where the gradient becomes extremely small and has no effect on backpropagation through time (BPTT). To address this issue, advanced recurrent networks, such as Long Short-Term Memory (LSTM) network \cite{hochreiter1997long} and Gated Recurrent Unit (GRU) network \cite{chung2015gated}, with the gating network were developed. Fig. \ref{fig:LSTM} shows the simplified schematic of LSTM cell. LSTM consists of three gates named input gate, output gate and forget gate and inherits hidden states $h_t$ and cell states $c_t$ to make an estimation for the next time step. More specifically, the input gate decides what information should be stored in the cell state $c_t$ based on the current input and inherited hidden states. If the value is closer to $1$, the particular information generated from input data $x_t$ and previous hidden states $h_{t-1}$ has more weight to update the cell state. Also, forget gate selects how much information from the previous cell states should be forgotten or kept. If the values are closer to $0$, less information from the previous cell states $c_{t-1}$ is kept in the updated cell states $c_t$. By using these two gates, the network can selectively process the most informative (related) data in the input sequence. The output gate finally weights the information to be taken from the cell state as an output. GRU also has a similar gating mechanism. Due to these gates, LSTM and GRU can effectively carry the information and have less chance to face the gradient vanishing problem. In this work, LSTM is utilized in one of the layers of the neural networks to approach POMDP. The network is trained via BPTT, which goes back through the entire sequence used to generate the output. 
% The LSTM cell has three gates: input gate, forget gate, and output gate (Fig. \ref{fig:LSTM}). Specifically, the input gate decides how much new data to add to the memory cell based on the current input and inherited hidden states. If the values are closer to $0$, the new information is taken less. Also, forget gate selects how much information from the previous cell states should be forgotten or kept. If the values are closer to $0$, less information is retained. By using these two gates, the network can selectively process the most informative (related) data in the input sequence. 

% LSTM consists of three gates named input gate, output gate and forget gate and inherits hidden states $h_t$ and cell states $c_t$ to make an estimation for the next time step. More specifically, the role of the forget gate is to decide what information should be removed from the cell state. the input gate is in charge of selecting information to be stored in the cell states from inherited hidden states and new input data. The output gate finally weights the information to be taken from the cell state as an output. 


% Figure environment removed

\subsection{Twin Delayed Deep Deterministic policy gradient algorithm}
% figure
The developments in this work are based on Twin Delayed Deep Deterministic policy gradient algorithm (TD3) \cite{fujimoto2018addressing}. The schematics of TD3 are illustrated in Fig. \ref{fig:TD3}. $o_t$, $a_t$, $r_t$ and $a'_t$ in the figure represent observation, action, reward and action generated by target actor network at time step $t$. DDPG \cite{lillicrap2015continuous} showed great performance with an efficient number of iterations. However, the issue of Q-learning overestimation remained. In \cite{fujimoto2018addressing}, this issue was investigated and the extended algorithm was presented as TD3. Like in the Double Q-learning algorithm  \cite{hasselt2010double}, TD3 has a double critic network. Each critic network has a target critic network and actor network also consists of the target network and behavior network. Thanks to several modifications, TD3 showed better performance in achievable total reward compared to the other latest deep RL algorithms. Although the original TD3 might suffer from performance degradation, there are only few developments for TD3 to solve POMDP.
% Figure environment removed

\subsection{LSTM-TD3}
In \cite{meng2021memory}, the LSTM layer was implemented in the TD3 algorithm which is named LSTM-TD3. LSTM-TD3 has LSTM layers in actor and critic networks. The past sequential information is processed through LSTM layer. Processing sequence in an entire episode is time-consuming to train the network. Hence, in LSTM-TD3, the networks process partial trajectory instead of entire trajectory up to the current time step by setting a specific sequence length $l$ to generate the action from the policy and to roll out the trajectory during the update as a hyper-parameter. This method to train a network with the partial history of a specific length is also applied in \cite{hausknecht2015deep} as described in Section \ref{relatedWork}. \\
Another aspect to be mentioned is that LSTM-TD3 has a double input channel as network architecture as shown in Fig. \ref{fig:lstmtd3}. 
% Figure environment removed
The input data for the first input channel is sequential past information $[o_{t-l:t-1}, a_{t-l:t-1}]$ from the memory and the second input channel takes the current information $[o_{t} (, a_{t})]$. The expression $o_{t-l:t-1}$ and $a_{t-l:t-1}$ describes the sequential observation and action between time step $t-l$ and $t-1$ respectively. This structure was selected in order to prioritize the most recent observation since it is considered that the recent observation is more valuable in decision-making than earlier observations. Note that this hypothesis is questionable because it might not be true that the latest observation is more informative to reveal the true state of the system. This will be discussed in detail in Section~\ref{structure}. LSTM-TD3 \cite{meng2021memory} was tested for robot locomotion tasks in open AI gym simulation environment \cite{OpenAI-Gym} with several types of POMDP such as removing one of the state elements from the observation and adding random noise to observation. Especially in the scenario where one or all of the observation is lost with probability $p$, LSTM-TD3 showed greater performance compared to original TD3 and other Deep RL algorithms.

