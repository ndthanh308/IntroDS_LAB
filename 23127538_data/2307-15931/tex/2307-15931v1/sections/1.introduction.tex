\section{Introduction}\label{intro}
% describe the problem
% list of contribution
% Robustness of optimality is worked on. 
Machine learning (ML) techniques are widely applied for control tasks. Data-driven approach aided by ML algorithms could be beneficial since it does not necessarily require a system model. More specifically, recent developments in deep reinforcement learning (RL) techniques have been adapted to generate near-optimal actions in the given state of the agent in the continuous action and state space. The latest development of deep RL has been successfully tested in many complex simulation environments from the standardized framework in Open AI gym \cite{OpenAI-Gym}. \\
However, it is not guaranteed that the RL algorithm showing good performance in the simulation environment will perform as intended in the real world.
Although most of the RL algorithms have been built upon the assumption of the Markov Decision Process (MDP), this assumption is not valid in many real-world applications. The information collected from the sensors does not fully describe the true and full state of the agent due to the limited measurement, noise, and disturbances. This partially observable and non-deterministic scenario is categorized as Partially Observable Markov Decision Process (POMDP) \cite{kurniawati2021partially}. In the MDP condition, it is possible to map the action based on the observation which is equal to the states of the system.  However, in POMDP, the agent has no access to the true system state and the agent only receives the observation which is generated from the latent state of the system \cite{hernandez2019survey}. In this case, the agent is required to construct its state representation from the available information to restore the MDP condition. This state representation can be described in the same state space as the systems but also it can be in the encoded internal state space. \\
One of the approaches to address POMDP is using sequential information which the agent has experienced on its trajectory. By processing the sequential information, the agent could generate the internal state representation and restore the MDP condition. The recurrent neural networks or other advanced gated recurrent neural networks such as Long-Short Time Memory (LSTM) and Gated Recurrent Unit (GRU) can process the sequential data by carrying hidden states. These networks are implemented RL and they have shown improved performances in various POMDP environments \cite{miki2022learning,li2015recurrent,hausknecht2015deep,singla2019memory}. However, most of the development only utilizes sequential observation and not uses all available information. There are also other recent approaches to work on time series data in this context e.g., transformer \cite{janner2021offline,chen2021decision} and Liquid Time-constant Network \cite{hasani2021liquid}. \\
Although many algorithms have been developed to deal with POMDP, further investigation and development are required regarding the robustness under disturbances. Very little research has studied effects of selection of information, length of the utilized action/observation sequence, and network structure on the robustness. Also, the analysis of the disturbance characters is lacking although algorithms are tested in POMDP environments. Robustness is an important element in practical applications. Although the policy is optimized in certain conditions, the solution does not necessarily remain optimal once the configuration of the environment varies. The recent discussion of Sim2Real transfer is more focused on increasing the fidelity of the simulator \cite{shen2021igibson,freeman2021brax} but it is not possible to model the real world perfectly \cite{truong2022rethinking}. Therefore it is important to increase the robustness of the trained system.\\
The robust system with generalizability can keep the performance well in any condition once it is trained. In an effort to build such generalizability into the system, we consider the robustness of optimality under the specified non-static environment in this work. Namely, the networks are evaluated based on the achieved total reward per episode in POMDP environment. The robust agent, in this study, should dynamically adjust the policy in the environment with disturbances. \\ 
Firstly, the related background knowledge is briefly explained in Section \ref{background}. Secondly, we discuss the importance of including action sequences in the POMDP framework in Section \ref{keyidea}. Then, we evaluate one of the existing algorithms which has the LSTM layer and discuss the effect of the sequential information component and the length of time window to look back on different types of disturbances in Section \ref{actionInclusion}. Section \ref{structure} proposes a new structure of the network to improve the existing algorithm. With this structure, the sequential information is processed individually in the actor and critic network. Also, in Section \ref{structure}, we discuss a different methodology to share the internal state representation between both networks.
Finally, all developed algorithms are compared and evaluated in Section \ref{discussion}. The contribution of this work can be summarised as follows:
\begin{enumerate}
  \item We explain how the selection of information matters in solving POMDP. Past action sequences can help to increase the robustness of optimality due to their causality in the environment (Section \ref{keyidea}). The effect of information on the robustness is identified and discussed (Section \ref{actionInclusion}).  
  \item The network structure to attend to the sequential data is discussed carefully in Section \ref{structure}. By following the evolution of internal state construction, we show that it is more reasonable to treat the past and current information as one sequence instead of processing them independently.
  \item When LSTM cells are applied in the network, the sequential information is carried by hidden states $h_t$ and cell states $c_t$. We propose a new algorithm to train the critic network based on the generated hidden states from the actor network (Section \ref{structure}). The evaluation of the proposed methodology shows the possibility to utilize them as the initial state of LSTM cells in training. This approach makes it possible to avoid reprocessing the sequence at each network during the training.
\end{enumerate}
% Toward the implementation of reinforcement learning into real-world applications, it is inevitable to challenge solving POMDP. While our proposed method is tested in a simple environment, we hope this work demonstrates the contribution of the choice of information and the process to deal with them. 
