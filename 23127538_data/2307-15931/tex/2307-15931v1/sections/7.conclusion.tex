\section{Conclusion}\label{conclusion}
In this paper, the TD3 algorithm has been extended with the LSTM network to solve POMDP. When the disturbance is dynamically changing, it is beneficial to have a policy that dynamically adapts to the new environments. In this context, the inclusion of only observation sequences is not sufficient to construct internal state representation. When the policy is separated into estimation and control parts, belief states are generated from the sequence of state and action in POMDP. This framework is implicitly applicable for the end-to-end model-free RL. We have presented that adding an action sequence can improve the robustness of the achieved performance without the separation of the policy into these two components. The structure to treat the sequence was also discussed. The results showed that it is better to process the past sequence and the current information in the same channel. Also, we proposed a new method called H-TD3 which can significantly reduce the computational time by using the representation of the trajectory information encoded in the actor network for initialisation of the critic network. \\
For our future work, the generalizability should be further investigated. It was understood that the trained role of the network would differ depending on the characteristics of the disturbances. Developing an algorithm which covers both types of disturbances would take the RL application closer to real-world implementation.