\section{Dynamic RL}\label{keyidea}
% Explain it as if you were speaking to someone using a whiteboard 
% Conveying the intuition is primary, not secondary  
% Once your reader has the intuition, she can follow the details (but not vice versa) 
% Even if she skips the details, she still takes away something valuable

% causual and probablistic
% in POMDP, we consider causuality as seen in causal reinforcement learning
% we need to do something on s (future is fine) just past
% in dynamic programing, I and sufficient statics
% to update, baye's are used
% in model-based, learnig and traing a seperated 
% in model free, from experience directly but still use the concept internally
% robustly estimate s without using model improve the performance with noisy data
% if so, it is naturally to think action helps to estimate s (4a)
% and all should be treated as a sequence not seperated (4b)
% if the sequence is processed independently, est s is independent representation
% it could be possible to share (4c) and it might be more efficient

The challenge of POMDP in this study is that we have no access to the true state of the system $s_t$ due to the disturbances in observations. In our scenarios, disturbances are realized by stochastic noise or dynamic noise, or by hiding one of the elements of the observation. The details of disturbances are defined in Section \ref{actionInclusion}. If $s_t$ can be estimated or represented in an internally encoded format, it is possible to restore the MDP condition. Then, an already established RL algorithm can be utilized to identify the (sub-)optimal policy. Therefore, recovering the MDP condition through certain architecture design choices such as the definition of input/output variables should be important for the policy robustness. Hence our focus is to develop a RL agent which can dynamically adapt to the observation containing the disturbance with their own dynamics. To make a dynamic RL agent, the system should be able to generate the internal state representation by assessing causal information from the trajectory.

\subsection{Causality and Statistics}
% The dichotomy of statistics and causality are emphasized in \cite{pearl2000models}. A statistical parameter is defined in terms of a joint probability distribution of observed variables. A statistical assumption is restricted by any constraint on a joint distribution of observed variables, e.g., MDP. On the other hand, a causal parameter is defined in terms of a causal model which is formulated as \ref{equ:causalmodel} 
% \begin{equation}\label{equ:causalmodel}
%     x_i = f_i(pa_i, u_i), \quad i=1, \dots, n,
% \end{equation}
% where $pa_i$ is connotation parents which is the set of variables directly determining the value of $X_i$, and $U_i$\footnote{Capital letters (e.g., $X, U)$ are used for variable names and lowercase letters are used as a symbolic representation of specific value.} represents errors due to unconsidered factors. 
The dichotomy of statistics and causality are emphasized in \cite{pearl2000models}. A statistical model works on the joint probability of observed variables. A causal model, on the other hand, assumes that certain variables are unobserved. The causal analysis enables inference of dynamics of events under changing conditions, not only the likelihood of events under static conditions. \\
In MDP, the static transition model is available (e.g., $P(o_{t+1}(=s_{t+1})\vert o_t(=s_{t}), a_t)$). However, in POMDP with non-static disturbances, the model of the world dynamically changes. Therefore, it requires an additional mechanism to identify the causal effect and recognize the pattern based on available information in the past and current. 

\subsection{Analysis of disturbance}
When the decision-making is performed based on information on the trajectory, the causal system is illustrated in Fig.\ref{fig:causal}. In this work, the considered disturbances can be categorized into two. The first case is where the disturbance can be described by a dynamic model (Fig. \ref{fig:calsual_diagram}). Therefore, the trained agent may be able to learn a model comprising the system and the disturbance. The second case is where the disturbance does not have a temporal correlation between them and cannot be modeled (Fig. \ref{fig:calsual_diagram2}). In this case, the agent should learn to eliminate the effect of the disturbance to reveal the true state.
The trained agent in the environment of which category described in Fig. \ref{fig:calsual_diagram} might show generalizability in the environment where the existing disturbance has temporal correlation but not the other one. This will be demonstrated in Section \ref{discussion}.

% Figure environment removed

\subsection{POMDP formulation}
In general, the POMDP model is defined as 6-tuple $\langle \mathcal{S, A, O, T, Z, R} \rangle$ \cite{kurniawati2021partially}, where 
\begin{itemize}
    \item $\mathcal{S}$ is the state space,
    \item $\mathcal{A}$ is the action space,
    \item $\mathcal{O}$ is the observation space,
    \item $\mathcal{T}(s_t,a_t,s_{t+1})$ is the state transition which is a conditional probability distribution $P(s_{t+1}\vert s_{t}, a_{t})$,
    \item $\mathcal{Z}(s_{t+1}, a_t, o_{t+1})$ is the observation function which is a conditional probability distribution $P(o_{t+1}\vert s_{t+1}, a_{t})$,
    \item $\mathcal{R}$ is an immediate reward function which represents $\mathcal{R}=\mathbb{E}\left[r_{t+1}\vert s_t, a_t \right]$.
\end{itemize}
When observations are corrupted by additive dynamic disturbances, the observation function $\mathcal{Z}$ is not static. Namely, the observation function can be described as $P(o_{t+1}\vert s_{t+1}, a_{t}, d_{t})$. In addition, if the disturbance is modeled and it is independent of the states and action, a new disturbance transition function, which represents a conditional probability distribution could be introduced.

\subsection{Information states, belief states in POMDP}
In the context of dynamic programming, the optimal controller in POMDP can be separated into two parts; $(a)$ an estimator and $(b)$ an actuator \cite{bertsekas2012dynamic,kurniawati2021partially}. 
The information available at time $t$ is called complete information state $I_t^C$ \cite{hauskrecht1997planning} which consists of all historical observation and action i.e., $I^C_t=(o_{0:t}, a_{0:t-1})$, where $o_{0:t} = o_0, \dots , o_t$ and $a_{0:t-1} =  a_0 ,\dots, a_{t-1}$. In the case of POMDP, the estimator could make the conditional expectation $\hat s_t  = \mathbb{E}(s_t\vert I^C_t)$. The challenge of utilizing the complete information states is that the size of the data expands as time elapses. To resolve this issue, quantities known as sufficient statistics are normally considered. The sufficient statistics has a smaller dimension and preserves the essential content of $I^C_t$. The sufficient static is generated at the estimator and the actuator generates control inputs to the system based on the sufficient static. In the standard POMDP model, a belief state $b_t$ is commonly used as sufficient statistics. The belief state assigns conditional probabilities of every possible state \cite{hauskrecht2000value}.
% One of the quantities used as sufficient statics is the conditional probability distribution $P_{s_{t}\vert I_{t}}$. With this sufficient statics, the estimator generates the probability distribution $P_{s_t\vert I_t}$ $(t=0,1,\dots T-1)$ which is recursively updated based on the measurement $o_t$ and the control $a_{t-1}$ at time step $t$ as:
% \[
% P_{s_{t}\vert I_{t}}=\Phi(P_{s_{t-1}\vert I_{t-1}}, a_{t-1}, o_{t}).
% \]
% The actuator generates control inputs to the system based on the probability distribution $P_{s_t\vert I_t}$. When the finite-state system is considered, the optimal policy controls the vector of the conditional probabilities which is:
% \[
% p_t = (p^1_t, \dots , p^n_t)
% \]
% where 
% \[
% p^i_t = P(x_t=i\vert I_{t}) , \quad i=1,\dots n.
% \]
% $p_k$ is generally referred to as the belief state.
As illustrated in \cite{kurniawati2021partially}, the policy of the agent receives the belief states to make optimal decisions.
The belief states could be recursively updated via Bayesian inference. 
The update of belief states can be described as (\ref{equ:beliefupdate}).
\begin{flalign}\label{equ:beliefupdate}
    % b(s_{t+1}) = P(s_{t+1}\vert o_t, a_t, b(s_t)) &= \frac{P(o_t\vert s_{t+1}, a_t, b_t)P(s_{t+1}\vert a_t, b_t)}{P(o_t \vert a_t, b_t)}\\ \nonumber
    %  &= \frac{\mathcal{Z}(s', a, o)\sum_{s'\in \mathcal{S}}\mathcal{T}(s,a,s')b(s)}
    %  {\sum_{s''\in \mathcal{S}}\mathcal{Z}(s'', a, o)\sum_{{s}\in S} \mathcal{T}(s,a_k,s'')b(s)}
    b_{t+1}(s_{t+1}) = P(s_{t+1}\vert o_t, a_t, b_t) 
    &= \frac{P(o_t\vert s_{t+1}, a_t, b_t)P(s_{t+1}\vert a_t, b_t)}{P(o_t \vert a_t, b_t)} \\ \nonumber
     &= \frac{\mathcal{Z}(s_{t+1}, a_t, o_t)\sum_{s_t\in \mathcal{S}}\mathcal{T}(s_t,a_t,s_{t+1})b_t(s_t)}
     {\sum_{s_{t+1}\in S}\mathcal{Z}(s_{t+1}, a_t, o_t)\sum_{{s_t}\in S} \mathcal{T}(s_t,a_t,s_{t+1})b_t(s_t)}
\end{flalign}
% I have to say by action has also is taken in the transition model and to estimate s in the influence of d, just considering hist(o) is not enough probablistically. and it will help to idendify transition model which dynamically changed becasuse of the disturbance. 


\subsection{Identification of transition model}
The objective of RL is to maximize the future cumulative reward $G_t$ by learning policy $\pi$. In model-based RL, it is solved by decomposing the problem into learning and planning. From learning, the transition model is explicitly obtained from experience, and the value function is established based on the learned model in planning. The agent is directly trained from the experience in model-free RL. The transition model and its update operation formulated in (\ref{equ:beliefupdate}) is not required. However, the concept itself is still applicable since it is established based on it as described in Section \ref{background}. In model-free RL, the model is internally generated based on the collected data directly. Action is the element of the transition model and, therefore, it is part of a recursive update. Thus, it is clear that taking action sequences in model-free RL will help to improve the robustness of optimality in POMDP.\\ 
In either case of using complete information state $I^C_t$ or belief state $b_t$, identification of the dynamic transition model relies not on only the historical information of observation, but also that of action, since both have causal connections on the current observation as depicted in Fig. \ref{fig:calsual_diagram}. As we are considering the environment where the observation is disturbed, our discussion is focused on generating internal representations of $s_t$ which is symbolized by $s^{*}_t$ in the rest of this paper. The key idea is to formulate a structure of an RL agent which can dynamically adapt the state representation $s^{*}_t$ on given disturbances.
% % Figure environment removed



% \subsection{LSTM to reflect the sequence on the internal representation $s^{*}_t$}
\subsection{LSTM to reflect the sequence on the internal representation}
When the model of the system is not available, $s^{*}_t$ could be generated via RNN or enhanced RNN by processing the sequential input. $s^{*}_t$ could contain the identified information regarding the pattern of the dynamic model of the system and the disturbance or distilled information which is less affected by the disturbance. Most of the existing RL algorithms with RNNs only utilized the sequential information of observation and there are only very few cases taking action sequence as input. We suggest utilizing the sequence of action based on the above discussion in this study. Therefore, the effect of sequential action inclusion with the LSTM-TD3 algorithm \cite{meng2021memory} is investigated in the next section. 


% The probability distribution over trajectories $\tau$ in POMDP can be described as as \ref{equ:pdf_POMDP} \cite{gasse2021causal}.
% \begin{equation}\label{equ:pdf_POMDP}
%     P(\tau) = \sum_{s_0 \rightarrow \vert \tau \vert}^{\mathcal{S}^{\vert \tau \vert +1}} P_{init}(s_0)P_{obs}(o_0\vert s_0) \prod _{t=0}^{\vert \tau \vert -1} \pi (a_t\vert h_t) P_{trans} (s_{t+1} \vert s_t, a_t) P_{obs} (o_{t+1} \vert s_{t+1}),
% \end{equation}
% where $\tau$ is complete trajectory $\tau = (o_0, a_0, \dots, o_{H})$ with finite horizon $H$, $P_{init}(s_0)$ is initial state distribution, $P_{trans}(s_{t+1}\vert s_t, a_t)$ is state transition distribution, $P_{obs}(o_t\vert s_t)$ is observation distribution, $h_t$ is history at time $t$ with $h_t=(o_0, a_0, \dots ,o_t)$ and $\pi(a_t\vert h_t)$ is a stochastic policy.
% Then POMDP is a tuple $M = (\mathcal{S, O, A}, P_{init}, P_{obs}, P_{trans}, r)$ where states $s \in \mathcal{S}$, observations $o \in \mathcal{O}$, actions $a \in A$.

% in POMDP, we don't know s. we need to know s based on s. so that cumulative reward is estimated accurately. so that the actor can be trained 
% we want to avoid modeling but still internal state representation is needed


% action should be considered if not what happen?
% logic to propaget state estimation from the sequence 
% introducing complet and sufficient static and then belief state
% the doubt of structure in LSTM_TD3 
% whatelse could be done? learning from hidden states
% future is fine, we are talking about the current states

% \subsection{Dynamic programming for POMDP}

% \subsection{Causality}
% % model-based and model-free

% %  define robustness
% \subsection{Action inclusion (information)} 
% In model-based RL, learning planning 
% However, model-free the agent learns from experiment directly and MDP is not explicitly considered. 
% However it still assumes MDP 

% \subsection{How we should treat}
% % structure 
% % hidden states
