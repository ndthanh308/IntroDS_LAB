% \section{Shared $s^*$}\label{learningfromh}
% % the sequential information is carried (h c). can we consider them as sufficient static?
% % H-TD3 BPTT and H-TD3 BPTT NLC
% % In Section \ref{structure}, the structure to process ...
% % The question to be answered in this section is "".
% % One of the answer would be ...
% % share the s* between actor and critic
% The previously discussed algorithms generate $s^*_t$ in actor and critic networks independently by taking complete information at each network. Particularly, we have introduced the interpretation where $s^*_t$ is produced in the LSTM layer at actor network. The research question to be investigated in this section is this whether or not  necessary to have a different representation of $s^*_t$ between both networks? If $s^*_t$ generated from the actor network can be shared with the critic network, it will reduce computational costs drastically. Because TD3 has double critics as seen in Fig. \ref{fig:TD3}, $I^C_{t-l:t}$ is processed four times at each critic network during the update. In this section, a new algorithm, named H-TD3 (hidden-state-based TD3) is introduced. In the H-TD3 algorithm, the critic networks are trained based on the shared $s^*$ from the behavior actor network during the exploration.\\
% % can I cite the first version here to reference other structures?
% \subsection{H-TD3 algorithm}
% Here, we consider the same network structure with LSTM-TD3\textsubscript{1ha1hc} which has a LSTM layer with a single input channel for the critic network as it is illustrated on the right side of Fig. \ref{fig:htd3structure}. The main diagram of Fig. \ref{fig:htd3structure} describes the data flow of the H-TD3 algorithm. During exploration, the behavior actor and environment interact and the collected data is stored in the replay buffer. In addition to the normal data set, the hidden state and the cell state of the LSTM cell are stored. These states are generated at the behavior actor network when LSTM processed the sequence of $a_{t-l-1:t-2}$ and $o_{t-l:t-1}$. During the training, the critic network is trained based on this tuple. Critic networks take $a_t$ and $o_t$ as input data as the original TD3 algorithm. However, the LSTM cell is initialized with the stored $(h_{t-l:t-1}, c_{t-l:t-1})$. In this way, input data $a_t$ and $o_t$ is treated as if a continuous sequence of $a_{t-l-1:t-2}$ and $o_{t-l:t-1}$ without repeating the entire trajectory between $t-l$ to $t$. In the same way, at the target critics, the target Q-values are calculated based on input data $a'_{t+1}$ and $o_{t+1}$ with initialized LSTM state with $(h_{t-l+1:t}, c_{t-l+1:t})$. The critic network is required to learn parameters based on $(a_t, o_t, h_{t-l:t-1}, c_{t-l:t-1})$. The criticism of this approach may be the dismissed action $a_{t-1}$ and $a_{t}$ at critic and target critic networks respectively. However, if the hidden states $(h_{t-l:t}, c_{t-l:t})$ of LSTM after processing all sequence $a_{t-l-1:t-1}$ and $o_{t-l:t}$ are used to initialize critic networks, $o_{t}$ is counted twice and it would break the sequence at the critic. Another possibility could be to limit the input just to $a_{t}$, but changing the character of input data to LSTM for the given $(h_{t-l:t}, c_{t-l:t})$ in critics would add difficulty in learning. Actually, we observed the degradation of performance in both cases compared to using  $(h_{t-l:t-1}, c_{t-l:t-1})$.\\
% Considering the developed LSTM-TD3\textsubscript{1ha2hc} and LSTM-TD3\textsubscript{1ha1hc}, the output data from LSTM in actor network is actually the last hidden states $h_t$ after processing $l$ length of the sequential input. After the LSTM layer, $h_t$ is carried to the following last two layers. Hence, the most simplest method for the critic networks to learn based on the shared $s^*$ would be taking $h_t$ as input data at the critic network in addition to the current observation. However, this operation would extend the input dimension excessively as the dimension of the hidden state is usually much larger than that of the input. It increases the difficulty to train the network and the risk of failure of learning as known as the "curse of dimensionality". Training network with larger input dimensions increases the number of parameters drastically, leading to the much greater computational complexity of training. Also, ending up with over-fitting is a common issue. Therefore, it is more scalable to share the hidden and cell states produced in the actor network for to initialize the hidden state of the critic network as in the proposed H-TD3 algorithm.

% % Figure environment removed
