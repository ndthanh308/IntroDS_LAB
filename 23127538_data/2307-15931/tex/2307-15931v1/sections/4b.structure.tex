\section{Structure}\label{structure}
% discussion on the structure with one head lstm-td3 and 1ha2hc and lstm-td3
% dataset
In Section \ref{actionInclusion}, we demonstrated that including the action sequence can improve the robustness of learning.
In this section, let us discuss the structure. 
\subsection{Interpretation of LSTM-TD3}
The LSTM-TD3 has two input channels in both actor and citric networks. 
They could be interpreted that the first input channel to generate $s^*_{t-1}$ from complete information $I^C_{t-l:t}$ taken from the memory. Then, the $s^*_{t-1}$ is updated to $s^*_{t}$ by combining the current observation information from the second channel at the actor. Our hypothesis is that it might not be necessary to have a double-headed structure as presented in Fig. \ref{fig:lstmtd3} because the information up to the current time step can be treated as one sequence according to the generation of the information states.

\subsection{Modified LSTM-TD3}
Based on this hypothesis, this paper proposes an actor network with a single input channel as illustrated in Fig. \ref{fig:structure}. This input channel processes $I^C_{t-l:t} =(o_{t-l}, \dots , o_t, a_{t-l-1} ,\dots, a_{t-1})$. Internal state representation $s^*_t$ is generated in the process of the LSTM layer and action is decided based on $s^*_t$ by the succeeding layers. For the critic network, two types of structure are considered. LSTM-TD3\textsubscript{1ha2hc} (1 headed actor and 2 headed critic) in Fig. \ref{fig:1ha2hc} has a clearer interpretation for $s^*_t$ to the information states. $s^*_t$ is generated in the same way as actor network at the first input channel and it is concatenated with the processed $a_t$ at another input channel. The last two layers can be understood as $Q(s^*_t, a'_t)$. On the other hand, LSTM-TD3\textsubscript{1ha1hc} has single input channel and modified complete information  $I^{C'}_{t-l:t} =(o_{t-1}, \dots , o_t, a_{t-l} ,\dots, a_{t})$ is taken as input. $I^{C'}_{t-l:t}$ differs from the generation of $s^*_t$ based on the Bayesian inference and hence, the output from the LSTM layer is described as $sa_t$ in the figure. However, the definition of action value function $Q$ is the expected return at $s_t$, taking action $a_t$ when following policy $\pi$. When the critic network is given with the trajectory, $I^{C'}_{t-l:t}$, which is the combination of $o_t$ and $a_t$ from $\pi$, it should be able to still generate the expected return for $\pi$.  \\
Unlike the original LSTM-TD3, LSTM-TD3\textsubscript{1ha2hc} and LSTM-TD3\textsubscript{1ha1hc} do not have an explicit intention to prioritize the current information. These proposed algorithms follow the update of the belief state and let LSTM to evaluate the importance of the data. Moreover, since the system model does not change in any time step, all data in a sequence should be treated in the same path. Training the agent with separate sequences might be more complicated because the data is processed differently and combined which might be unnecessary according to the belief state update. Therefore, LSTM-TD3\textsubscript{1ha2hc} and LSTM-TD3\textsubscript{1ha1hc} are expected to show better robustness.  

% Figure environment removed

\subsection{H-TD3 algorithm}
The previously discussed algorithms generate $s^*_t$ in actor and critic networks independently by taking complete information at each network. Particularly, we have introduced the interpretation where $s^*_t$ is produced in the LSTM layer at actor network. The research question to be also investigated is whether or not it is necessary to have a different representation of $s^*_t$ between the two networks. If $s^*_t$ generated from the actor network can be shared with the critic network, it will drastically reduce the computational costs. Because TD3 has double critics as seen in Fig. \ref{fig:TD3}, $I^C_{t-l:t}$ is processed four times at each critic network during the update. Hence we propose, a new algorithm, named H-TD3 (hidden-state-based TD3) in which the critic networks are trained based on the shared $s^*$ from the behavior actor network during the exploration.\\
Here, we consider the same network structure with LSTM-TD3\textsubscript{1ha1hc} which has a LSTM layer with a single input channel for the critic network as it is illustrated on the right side of Fig. \ref{fig:htd3structure}. The main diagram of Fig. \ref{fig:htd3structure} describes the data flow of the H-TD3 algorithm. During exploration, the behavior actor and environment interact and the collected data is stored in the replay buffer. In addition to the normal data set, the hidden state and the cell state of the LSTM cell are stored. These states are generated at the behavior actor network when LSTM processed the sequence of $a_{t-l-1:t-2}$ and $o_{t-l:t-1}$. During the training, the critic network is trained based on this tuple. Critic networks take $a_t$ and $o_t$ as input data as the original TD3 algorithm. However, the LSTM cell is initialized with the stored $(h_{t-l:t-1}, c_{t-l:t-1})$. In this way, input data $a_t$ and $o_t$ is treated as if a continuous sequence of $a_{t-l-1:t-2}$ and $o_{t-l:t-1}$ without repeating the entire trajectory between $t-l$ to $t$. In the same way, at the target critics, the target Q-values are calculated based on input data $a'_{t+1}$ and $o_{t+1}$ with initialized LSTM state with $(h_{t-l+1:t}, c_{t-l+1:t})$. The critic network is required to learn parameters based on $(a_t, o_t, h_{t-l:t-1}, c_{t-l:t-1})$. The criticism of this approach may be the dismissed action $a_{t-1}$ and $a_{t}$ at critic and target critic networks respectively. However, if the hidden states $(h_{t-l:t}, c_{t-l:t})$ of LSTM after processing all sequence $a_{t-l-1:t-1}$ and $o_{t-l:t}$ are used to initialize critic networks, $o_{t}$ is counted twice and it would break the sequence at the critic. Another possibility could be to limit the input just to $a_{t}$, but changing the character of input data to LSTM for the given $(h_{t-l:t}, c_{t-l:t})$ in critics would add difficulty in learning. During the development, we observed the degradation of performance in both cases compared to using  $(h_{t-l:t-1}, c_{t-l:t-1})$.\\
Considering the developed LSTM-TD3\textsubscript{1ha2hc} and LSTM-TD3\textsubscript{1ha1hc}, the output data from LSTM in actor network is the last hidden states $h_t$ after processing $l$ length of the sequential input. After the LSTM layer, $h_t$ is carried to the following last two layers. Hence, the simplest method for the critic networks to learn based on the shared $s^*$ would be taking $h_t$ as input data at the critic network in addition to the current observation. However, this operation would extend the input dimension excessively as the dimension of the hidden state is usually much larger than that of the input. It increases the difficulty to train the network and the risk of failure of learning as known as the "curse of dimensionality". Training network with larger input dimensions increases the number of parameters drastically, leading to the much greater computational complexity of training. Also, ending up with over-fitting is a common issue. Therefore, it is more scalable to share the hidden and cell states produced in the actor network to initialize the hidden state of the critic network as in the proposed H-TD3 algorithm.

% Figure environment removed
