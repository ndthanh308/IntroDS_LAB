\section{Related Work}\label{relatedWork}
\subsection{Utilizing RNN in RL}
There are many existing developments that utilize RNN for RL algorithms to solve POMDP. A few examples are reviewed in this section.
\paragraph{RL-LSTM}
The utilization of LSTM in RL has been discussed for a long time. In the earlier development, RL-LSTM \cite{bakker2001reinforcement} was presented with a simple grid T-maze experiment using Advantage($\lambda$) learning \cite{harmon1996multi}. In their T-maze experiment, the agent is required to remember the first observation in the entire episode to reach the goal with optimal action. It was shown that RL-LSTM could effectively perform in this non-Markovian scenario. In the recent context of RL, the deep neural network is coupled with the gated recurrent neural network to address the challenge of solving POMDP. 
\paragraph{Deep Recurrent Q-learning (DRQL)}
In Deep Recurrent Q-learning (DRQL) \cite{hausknecht2015deep}, LSTM cells were added to the network by replacing the first dense layer after a convolutional network of DQN \cite{mnih2015human} in discrete action space. In their work, two types of updates were considered; Bootstrapped Sequential Updates and Bootstrapped Random Updates. The former randomly selects episodes and selected full episodes are replayed from the first step to the end. Hence, the hidden state of LSTM is carried out without initialization throughout one episode. The latter case randomly selects the point of the episode and unrolls the experience for certain time steps. Bootstrapped Sequential Updates can capture longer sequential information but it violates the random sampling policy of DQN. Bootstrapped Random Updates can keep the random sampling but captures the information in a shorter time scale since it requires hidden states of LSTM to be initialized at the beginning of each step. Both types showed similar performance in their investigation, and Bootstrapped Random Updates was finally applied for further discussion because it takes a shorter time to train. 
% DRQL was evaluated in the modified Pong game environment from Atari 2600 game. The game was modified such that the agent has access to the screen with the probability of $0.5$ to create the POMDP condition. The agent took 10 frames of the game screens as input. 
In their work, the past action information was not considered, instead, integrated visual information from the console screen is received as input. 
\paragraph{Recurrent DPG (RDPG)} 
Heess et al. \cite{heess2015memory} developed Recurrent DPG (RDPG) by extending DPG \cite{silver2014deterministic} with LSTM in actor-critic style by defining actor and critic as $\mu^\theta(h_t)$ and $Q^\mu(h_t,a_t)$ in terms of observation-action history $h_t = (o_1,a_1,o_2,a_2,\dots,o_{t-1},a_{t-1}, o_t)$. $\mu^\theta(h_t)$ describes a deterministic policy $\mu$ with parameters $\theta$. LSTM is used to summarise the history to make it scalable in POMDP. Similar to Bootstrapped Sequential Updates in DRQL, during the update, a minibatch of $N$ episodes is sampled to construct histories $h_t^i$ ($i=1\dots N$), and the target values are computed for each sampled episode through BPTT. It was tested in several environments but the most related ones are "the under-actuated pendulum" and "cartpole swing up" examples. In the experiments, the velocity-related information was removed from the observation. RDPG could learn the tasks which DDPG could not perform.
 
\subsection{Separating between estimation and control}
It is also a common approach to separate the estimation and control strategy using information state in reinforcement learning for POMDP condition \cite{malikopoulos2022separation,gasse2021causal}. The information states are updated based on Bayesian inference. Then, the control strategies are learned by taking the information state. Unlike our discussed methods, this approach explicitly generates the information state and policy takes estimated states. It supports the idea to consider past action information and also processes the sequence up to current information in the same stream as discussed in Section \ref{structure}.