\section{Action inclusion}\label{actionInclusion}
% result of action inclusion for LSTM-TD3
% construction of internal belief state 

% structure figure
% environment
% experiment result

 % In the next section, we will investigate the effective information and length of history to solve POMDP when observation is disturbed.
In the previous section, we have discussed that the belief state evolves based on the observation and action on the trajectory. In this section, the impact of the inclusion of the action sequence is demonstrated using LSTM-TD3. Also, the effect of the length of the sequence for the individual disturbance condition is investigated. 

\subsection{Length of sequence}
% we can set sequence length. 
% in some sense, we are filtering data already 
% the advantage of full episode
% - no need to know the length
% the disadvantage
% - takes time 
% - non iid 
% - random sampling
% the advantage of partial episode
% - faster 
% - iid
% the disadvantage
% - need to be long enough to capture the characteristics of the environment
% what we want to focus in this section
% - test with disturbances which has different stochastic characters and try to find the right length of the history.
When the partial trajectory is taken as the input sequence, the selection of its length $l$ can be a design parameter as in LSTM-TD3. In general, RL algorithms with a RNN layer require repeating either entire or partial episodes during the update in order to train the network via Back Propagation Through Time (BPTT). When entire episodes are repeated in training, it can be considered that the agent is trained using complete information $I^C$. As stated in Section \ref{keyidea}, the size of $I^C$ increases as time elapses. Thus, repeating entire trajectories to update the network is a time-consuming process. In case of the on-policy algorithm with RNN, utilizing $I^C$ might be a more natural approach since it follows the same trajectory during the training. Several works have explored the methods combining RNN with on-policy algorithms (e.g. with PPO\cite{gaudet2019adaptive} and A3C \cite{mnih2016asynchronous}). However on-policy algorithms are much less sample-efficient compared to off-policy algorithms. \\
For state value function based algorithms, parameter sharing is a common technique where the actor and critic share the same network parameters and only the last layer of each network is different. However, this technique is not as common for action value function based algorithms like TD3 since the parameter sets are different between actor and critic networks. When parameter sharing is not used, the same trajectory needs to be repeated multiple times to update multiple networks.\\
If the whole episode is repeated, the benefit provided by random sampling can be lost in the experience replay which is the mechanism for improving data efficiency and stability. Experience replay effectively minimizes the correlations between samples \cite{lillicrap2015continuous} so that the data distribution becomes closer to independent and identically distributed (i.i.d.) condition which is preferable in RL. In \cite{heess2015memory}, it is concerned that if the whole trajectory is sampled together, the bias in the learned policy might become severer in the update because the state distribution under the current policy no longer corresponds to the one from the replay buffer.\\
If a few steps of sequential data are randomly sampled as a unit, the sample correlation issue could be mitigated. This approach captures a shorter history of fixed length in the environment. LSTM-TD3 offers the opportunity to define the length of the sequence. The agent should be provided with enough length of sequential data so that it can capture the characteristics of the system and the disturbance. In the experiment in this section, the relationship between several characteristics of the disturbance and the length of sequence that is provided to the network is investigated.

\subsection{Experiment}\label{experiment}
To validate the discussion in Section \ref{keyidea} and investigate the effect of including action sequence on the different types of disturbances, we compare learning trajectories of the LSTM-TD3 algorithm are compared between with or without the past action sequence in the first input channel (see Fig. \ref{fig:lstmtd3}) in classic control "Pendulum" environment from OpenAI gym. In this environment, the observation consists of three elements $\left[x,y,\dot\theta \right]$, where $x$ and $y$ represent the position of the free end of the pendulum and $\dot\theta$ is its angular velocity. The action is defined as the torque to be added and the agent learns to keep the pendulum upright from a random initial condition. The definition of the "Pendulum-v0" environment is summarized in Table \ref{tab:Pendulum-v0}.\\
\begin{table}[h]
\begin{center}
\caption{"Pendulum-v0" environment definition} \label{tab:Pendulum-v0}
\begin{tabular}{ |c|c|c|c|  }
\hline 
 action & torque & min $-2.0$ & max $2.0$ \\
\hline
\multirow{3}{*}{observation} & $x=\cos{\theta}$ & min $-1.0$ & max $1.0$ \\
& $y=\sin{\theta}$ & min $-1.0$ & max $1.0$ \\
& angular velocity & min $-8.0$ & max $8.0$\\
\hline
reward  &\multicolumn{3}{|c|}{$r = -(\theta^2 + 0.1\Dot{\theta}^2 + 0.001 (torque)^2)$} \\
\hline
termination  &\multicolumn{3}{|c|}{at $200$ time steps} \\
\hline    
\end{tabular}
\end{center}
\end{table}
The motivation for this work is to develop an algorithm which robustly provides optimality under the presence of disturbance. As summarised in Table \ref{tab:scenario}, we classified disturbance under condition into five types: “temporal bias”, “temporal sinusoidal wave”, "random sinusoidal wave", “noise” and “hidden” conditions. In the “temporal bias” and the “temporal sinusoidal wave” cases, the observation of $x$ and $y$ are disturbed for specific lengths and times at random time steps by additive constant biases and sinusoidal waves. In the "random sinusoidal wave" case, the sinusoidal wave is randomly configured at each episode and all three elements are disturbed. In the “noise” case, zero-mean Gaussian noise with standard deviation $\sigma$, is added to each element of the observation during the entire episode. In the “hidden” condition, $\dot\theta$ is removed from the observation during the whole episode and the dimension of the observation becomes two. Scenarios 1,2, and 3 in Table \ref{tab:scenario} are the cases where the disturbance has its dynamics and patterns could be identified from the observation. The other remaining two cases have no dynamics in the disturbance. The networks should rather work on recovering the dynamics of the system by eliminating the effect of the disturbance from the observation than establishing a model comprising the system and disturbance.

\begin{table}[h]
\begin{center}
\caption{Test scenario} \label{tab:scenario}
% \begin{tabular}{p{2cm} p{2cm} p{11cm}}
\begin{tabular}{ccc}
\hline \hline
Scenario & Name & Description\\
\hline
1 & temporal bias & \makecell{Disturbance defined by $A\in[0.5,1.0]$ appearing\\ randomly is added to observation in $x$ and $y$ \\lasting $3$ time steps for $10$ times per episode.}\\
\hline
2 & temporal sinusoidal wave & \makecell{Disturbance defined by $A\sin{2\pi t/T}$ ($A=1$ and $T=70$) \\which starts appearing at randomly chosen time steps for\\ the period of $T$, is added to observation $x$ and $y$ .}\\
\hline
3 & random sinusoidal wave & \makecell{Disturbance randomly defined in every episode by $A\sin{2\pi t/T}$ \\ ($ 0.5\leq A \leq 2$ and 
 $10\leq T\leq 100$) which starts appearing at random\\ time step for the period of $T$, is added to all 3 observation elements.}\\
\hline
4 & noise & \makecell{Noise defined by zero-mean normal distribution with standard \\ deviation $\sigma  \in[0.5,1.0]$ is constantly added to \\ all 3 observation elements.}\\
\hline
5 & hidden & Angular velocity $\dot\theta$ is removed from observation.\\
\hline\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Results and analysis}
Fig. \ref{fig:action_MDP} to Fig. \ref{fig:action_hidden} show normalized learning trajectories after 5 trials of the LSTM-TD3 algorithm in the discussed scenarios. The orange lines, labeled "w/o action", show the case where the sequential action information is excluded. Namely, the first input channel of actor and critic networks process only sequential $l-1$ length of historical observation $o_{t-l:t-1}(=o_{t-l}, \dots,o_{t-1})$. The blue lines, labeled "w/ action", show the case which has historical action $a_{t-l:t-1}$ as additional information. The columns represent the length of the history to be processed. Fig. \ref{fig:action_MDP} and Fig. \ref{fig:action_bias} illustrate the results of the MDP case and the "temporal bias" case respectively. In the MDP case, LSTM-TD3 with or without action performs the same. Also, the final performance at the end of training in the "temporal bias" case did not show obvious differences. In this condition, a constant value is added for three time steps that randomly start appearing 10 times. Since the observation is biased by a constant value, the observation is still almost unique. Therefore, it can be considered to be similar to the MDP condition. \\
Fig. \ref{fig:action_sin} shows the results of the "temporal sinusoidal wave" scenario. As $l$ gets longer, the robustness improves. The wavelength of the defined sinusoidal wave is $70$. Longer $l$ helps to identify the system dynamics and the disturbance dynamics. Also, the case with action sequence included as the input to the agent achieved higher total rewards. The result indicates how the choice of information affects the robustness. \\
% As the amplitude or wavelength of additive sinusoidal wave increases, the performance of the agent becomes worse. The derivative of the additive sinusoidal wave is $2\pi\frac{A}{T} \sin{(2\pi \frac{t}{T}+\phi)}$. So, the derivative does not explain this symptom. The learning trajectory in the change of amplitude seems the agent is still progressing. In contrast, the case in the change of wavelength is not. When the amplitude is large, the value range of data is increased. Therefore, it would require more time to reach the best solution in the given environment. When the wavelength is larger, the reachable performance can be fundamentally lower because the period when observation is disturbed is longer. This would explain why a longer time window tends to provide better performance in this case but not the change in amplitude.\\
Fig. \ref{fig:action_sin4} represents the learning trajectory of the "random sinusoidal wave" scenario. Establishing the observation function in this scenario is much harder as compared to the "temporal sinusoidal wave" due to the infinite variety of sinusoidal waves. It requires the agent to dynamically adapt to the environment based on causality of action and observation and hence, the network with action sequence reaches better optimality. \\
% By combing the observations from two figures, it could be analyzed that the agent experience difficulty when the derivative of observation of each time step is larger. 
Fig. \ref{fig:action_noise} illustrates the learning trajectory under the "noise" condition. This scenario is more challenging than the "temporal sinusoidal wave" since there are no specific dynamics in the disturbance and all of the elements of the observations are constantly disturbed. Although the network considering sequential action information performs better, the performance degrades with the larger variance of the noise. The performance improves with longer $l$. Longer sequential data contributes to extracting the system state transition characteristic and eliminating the effect of the noise in LSTM. 
% The LSTM cell has three gates: input gate, forget gate, and output gate (Fig. \ref{fig:LSTM}). Specifically, the input gate decides how much new data to add to the memory cell based on the current input and inherited hidden states. If the values are closer to $0$, the new information is taken less. Also, forget gate selects how much information from the previous cell states should be forgotten or kept. If the values are closer to $0$, less information is retained. By using these two gates, the network can selectively process the most informative (related) data in the input sequence. 
In the white noise case, some data randomly contain less noise. By focusing on these data and putting less focus on more noisy data, the underlying pattern can be restored. When the length of the input sequence is longer, it has more chance to have informative data which is less affected by noise. In addition, as more episodes are experienced, degradation of achieved total reward was observed in the "noise" scenario. This would be due to the occurrence of overfitting. It could be prevented by changing the network configuration, e.g., reducing the number of parameters, and adding dropout layers.\\ 
Fig. \ref{fig:action_hidden} presents the result of the "hidden" scenario. The final performance is the same regardless of the action inclusion and length of the sequence. In this environment, only current observation $o_t$ and one step before $o_{t-1}$ should be sufficient to restore $\dot \theta$ since $\theta$ is updated as $\theta(k+1) \leftarrow \theta (k) + \dot\theta (k+1)dt$. The formula only requires the previous observation to restore $\dot{\theta}$ from the equation and therefore the performance is not affected by $l$. However, the past action information is making the learning process more efficient. The network does not have this mentioned interpretation and the reward signal is mixed with $\theta$, $\dot \theta$, and torque. The inclusion of action information could make learning efficient because the network learns to process the dynamics under more specific conditions. The most reasonable network structure to include this interpretation might rather just consist of fully connected layers by having the input of $\left[x_t, y_t, x_{t-1}, y_{t-1} \right]$ because the sequential process is not required.\\ 
% Figure environment removed
% Figure environment removed
% Figure environment removed
% % Figure environment removed
% % Figure environment removed
% Figure environment removed

% Figure environment removed
% Figure environment removed



% \subsection{Longer time window for pendulum}
% From the previous discussion, longer $l$ seems to be safer option when the characteristic of the disturbance is unknown. However, as the length of time window becomes longer, there might be issues as listed below. 
% \begin{itemize}
%     \item longer to iterate
%     \item less random sampling
%     \item back propagation become more specific about the trajectory and more separation from the recorded trajectory and current policy distribution exists
% \end{itemize}
% To examine the effect of longer $l$, the further simulation was performed. Only the results of "noise" and "hidden" cases are shown in Fig. \ref{fig:lbw_long} since no issue was found for "MDP", "temp bias" and "temporal sinusoidal case".  From this analysis, extending time window carelessly can end up with long learning and inadequate performance at all.  
