\section{Appendix}
% \subsection{Reinforcement learning}
% Reinforcement learning (RL) is one of the machine learning problems. In RL, the agent is trained to make the decision $a_t$ at each time step $t$ when its state $S_t$ is given as observation $O_t$ from the environment. The learning is based on the immediate reward signal $R_t$ from the interaction between agent and environment as shown in Fig.\ref{fig:RL_concept}. Reward $R_t$ is normally a scalar feedback signal which represents the evaluation of the action $A_t$ when the agent is in the state of $S_t$. The objective of the system is to maximize the expected cumulative reward. Since the goal is the maximization of the total future reward, the agent can select the action in the dynamic environment. 
% % Figure environment removed
% The policy is one of the conceptual components of RL. At each time step, the agent decides its action based on the policy $\pi$. The policy maps from observation to probabilities of the next action, i.e., $\pi(a\vert s)$.
% Value functions are also one of the other components of RL. The value function is an estimation of the future reward and it provides the estimated evaluation of being in the state, i.e., state-value function (\ref{equ:value_fn}), or taking action with the state, i.e., action-value function(\ref{equ:actionvalue_fn}). In value functions, the discounted cumulative future reward, return $G_t$, from the current time step is defined under the policy $\pi$.
% \begin{equation}\label{equ:value_fn}
%     v_\pi(s) = \mathbb{E}_\pi[G_t\vert S_t = s] = \mathbb{E}_\pi\left[ \sum^{\infty}_{k=0} r^kR_{t+k+1}\vert S_t = s\right]
% \end{equation}
% \begin{equation}\label{equ:actionvalue_fn}
%     q_\pi(s, a) = \mathbb{E}_\pi[G_t\vert S_t = s, A_t = a] = \mathbb{E}_\pi\left[ \sum^{\infty}_{k=0} r^kR_{t+k+1}\vert S_t = s, A_t = a\right]
% \end{equation}
% Also, in the form of the Bellman expectation equation, these can be decomposed as 
% \begin{equation}\label{equ:value_fn2}
%     v_\pi(s) = \mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})\vert S_t = s]
% \end{equation}
% \begin{equation}\label{equ:actionvalue_fn2}
%     q_\pi(s, a) = \mathbb{E}_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})\vert S_t = s,A_t = a]
% \end{equation}
% Solving the MDP in RL is eventually finding a policy that achieves the best return during the episode. The optimal policies $\pi_*$ have the optimal state-value function $v_*(s)$ or optimal action-value function $q_*(s,a)$ and they are defined as 
% \begin{equation}\label{equ:value_fn3}
%     v_*(s) = \max_\pi v_\pi(s) 
% \end{equation}
% for all $s \in \mathcal{S}$
% \begin{equation}\label{equ:actionvalue_fn3}
%     q_*(s, a) = \max_\pi q_\pi(s,a)  
% \end{equation}
% for all $s \in \mathcal{S}$ and $a \in \mathcal{A(s)}$. Particularly, the Bellman optimality equation for $q_*$ is described as  
% \begin{equation}\label{equ:actionvalue_fn4}
% \begin{split}
%     q_*(s, a)& = \mathbb{E}\left[R_{t+1}+\gamma \max_{a'}q_*(S_{t+1},a')\vert S_t=s, A_t = a \right] \\
%     &= \sum_{s', r}p(s', r\vert s, a)\left[ r+\gamma \max_{a'}q_*(s',a') \right].
% \end{split}
% \end{equation}

 
% \subsection{Partially Observable Markov Decision Process}
% In the form of RL, the history of information is retained from sequential observations, actions and rewards. The information accumulates as time elapses. When a current state succeeds in all relevant information from the history, the state signal is called Markov property \cite{sutton2018reinforcement}. If the state signal meets the Markov property, the probability of the dynamics of the environment can be represented as (\ref{equ:markov}) 
% \begin{equation}\label{equ:markov}
%   \begin{array}{l}
%     p(s', r\vert s, a) = Pr \left\{ S_{t+1}=s', R_{t+1}=r \vert S_0, A_0, R_1,\dots ,S_t, A_t\right\}
%     = Pr\left\{ S_{t+1}=s', R_{t+1}=r \vert S_t, A_t\right\}
%   \end{array}
% \end{equation}
% The tasks of RL are assumed to satisfy Markov property and this decision making process is called Markov Decision Process (MDP). The model, as a component of RL, is the representation of the environment for the agent. From this environment's dynamics with Markov property, the model can be described with the state-transition probability (\ref{equ:statetransition}) and reward expectation for state-action pairs (\ref{equ:expectation}). 
% \begin{equation}\label{equ:statetransition}
%     p(s'\vert s, a) =  Pr[S_{t+1}=s' \vert S_t = s, A_t = a] = \sum_{r\in \mathcal{R}}p(s', r\vert s, a)
% \end{equation}
% \begin{equation}\label{equ:expectation}
%     r(s,a) = \mathbb{E}[R_{t+1}\vert S_t = s, A_t = a]  = \sum_{r\in \mathcal{R}}r\sum_{s'\in \mathcal{S}}p(s', r\vert s, a)  
% \end{equation}
% However, the observation for the tasks in the real world often contains noise and disturbance. Sometimes the full state of the system is not observable. The decision process in the condition where the agent has no access to the true system state from observation ($s_t \neq o_t$) is called POMDP. In this condition, the agent is required to learn to construct internal state representation or requires an external state estimator to map its action from the available information. For Q-value representation in POMDP, $Q(o_t,a_t)$ is no longer equal to $Q(s_t,a_t)$. One of the effective approaches is to use historical information through using Recurrent Neural Networks (RNN) \cite{hernandez2019survey}. Further details of the examples which apply RNN in RL to solve POMDP can be found in Section \ref{relatedWork}. 


\subsection{Network structure}
\begin{table}[H]
\begin{center}
\caption{Structure} \label{tab:structure}
% \begin{tabular}{p{2cm} p{2cm} p{11cm}}
\begin{tabular}{ccc}
\hline \hline
Algorithm & Actor network & Critic network\\
\hline
LSTM-TD3 & \makecell{Lin(obs dim+act dim, 128), Lin(obs dim, 128)\\ReLU, ReLU\\LSTM(128,128), N/A\\Lin(256,128)\\ReLU\\Lin(128,1)\\ tanh} & \makecell{Lin(obs dim+act dim, 128), Lin(obs dim+act dim, 128)\\ReLU, ReLU\\LSTM(128,128), N/A\\Lin(256,128)\\ReLU\\Lin(128,1)} \\
\hline
LSTM-TD3$_{1ha1hc}$ &  \makecell{Lin(obs dim+act dim, 128)\\ReLU\\LSTM(128,128)\\Lin(128,128)\\ReLU\\Lin(128,1)\\ tanh}  &  \makecell{Lin(obs dim+act dim, 128)\\ReLU\\LSTM(128,128)\\Lin(128,128)\\ReLU\\Lin(128,1)} \\
\hline
LSTM-TD3$_{1ha2hc}$ & \makecell{Lin(obs dim+act dim, 128)\\ReLU\\LSTM(128,128)\\Lin(128,128)\\ReLU\\Lin(128,1)\\ tanh}  &  \makecell{Lin(obs dim+act dim, 128), Lin(act dim, 128) \\ReLU, ReLU\\LSTM(128,128), N/A\\Lin(256,128)\\ReLU\\Lin(128,1)} \\
\hline
H-TD3 &  \makecell{Lin(obs dim+act dim, 128)\\ReLU\\LSTM(128,128)\\Lin(128,128)\\ReLU\\Lin(128,1)\\ tanh}  &\makecell{Lin(obs dim+act dim, 128)\\ReLU\\LSTM(128,128)\\Lin(128,128)\\ReLU\\Lin(128,1)} \\
\hline
TD3 & \makecell{Lin(obs dim, 128)\\ReLU\\Lin(128,128)\\ReLU\\Lin(128,1)\\ tanh} & \makecell{Lin(obs dim+act dim, 128)\\ReLU\\Lin(128,128)\\ReLU\\Lin(128,1)}\\
\hline\hline
Lin: linear layer
\end{tabular}
\end{center}
\end{table}

\subsection{The number of parameters}
\begin{table}[H]
\begin{center}
\caption{The parameter of numbers (obs dim$=3$, act dim$=1$)} \label{tab:structure}
% \begin{tabular}{p{2cm} p{2cm} p{11cm}}
\begin{tabular}{cc}
\hline \hline
Algorithm & The number of parameters (actor, critic) \\
\hline
LSTM-TD3 &166273, 166401 \\
\hline
LSTM-TD3$_{1ha1hc}$ & 149377, 149377\\
\hline
LSTM-TD3$_{1ha2hc}$ &149377, 166017\\
\hline
H-TD3 &149377, 149377 \\
\hline
TD3 & 17153, 17281 \\
\hline\hline
\end{tabular}
\end{center}
\end{table}