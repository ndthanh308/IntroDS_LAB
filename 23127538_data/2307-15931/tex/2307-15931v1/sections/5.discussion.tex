\section{Experiment}\label{discussion}
% structure matters
% runtime
% htd3 other approach
\subsection{Simulation set-up}
In this section, the algorithms developed in Section \ref{structure} are tested in the "Pendulum" environment with disturbance conditions discussed in Section \ref{actionInclusion}. The applied disturbance conditions are as follows:
\begin{itemize}
    \item "temporal sinusoidal wave" with $\sin\frac{2\pi}{70}t$
    \item "random sinusoidal wave"
    \item A zero mean Gaussian "noise" with $\sigma=0.5$
    \item "hidden"
\end{itemize}
\subsection{Results}
In Fig. \ref{fig:all_traj}, the learning trajectory of the proposed algorithms is depicted. The trajectories are averaged after 5 trials. The time window $l$ is selected to be $1,3,6,10,20$. In the array of figures, each row represents a disturbance condition and each column represents an algorithm. The algorithms compared are original LSTM-TD3 without past action sequence, LSTM-TD3 with past action sequence, LSTM-TD3$_{1ha1hc}$, LSTM-TD3$_{1ha2hc}$, H-TD3 and original TD3 from the left column. The disturbances are "temporal sinusoidal wave", "random sinusoidal wave", "noise" and "hidden" from the top. The number of the iteration varies depending on the disturbance conditions. \\
Fig. \ref{fig:endperformance} shows the normalized total reward of the last ten records by taking algorithm types on the x-axis. Each figure describes the result of "temporal sinusoidal wave", "random sinusoidal wave", "noise" and "hidden" conditions respectively. The lines are drawn to compare the effect of the time window $l$.
% Figure environment removed

% Figure environment removed

\paragraph{Overall analysis}
Compared with TD3, any algorithms with LSTM layer showed better performance in the tested POMDP conditions. Especially, in the "noise" and the "hidden" cases, TD3 failed to learn. The "noise" environment generates the reward given the noisy observation. Without having a sequence, it is difficult to learn since the observation values are not unique to the reward signal. Also, the "hidden" without LSTM layer cannot recover information related to angular velocity which is part of the reward function.  \\
As observed in Section \ref{actionInclusion}, all algorithms with action sequences performed better than LSTM-TD3 without an action sequence. The midfield LSTM-TD3 (LSTM-TD3$_{1ha1hc}$, LSTM-TD3$_{1ha2hc}$) showed generally better performance compared with the original LSTM-TD3 with action sequence. From the results, it can be understood that it is better to treat the past sequence and current data as one sequence, unlike LSTM-TD3. LSTM-TD3$_{1ha1hc}$ had the best robustness in the optimality of all algorithms. The simplicity of the structure could have helped the network to learn. H-TD3 algorithm presented a comparable result with LSTM-TD3 with action case except for the "noise" case. It proves that it is possible to learn from shared hidden states seen in H-TD3 results. However, H-TD3 showed a slower learning trajectory for a given number of iterations compared to the others in Fig. \ref{fig:all_traj} due to the critic's dependency on the actor network to generate the hidden states and cell states. Interestingly, the results of the "noise" case in H-TD3 were significantly degraded and mostly worse than LSTM-TD3 without action sequence. In the H-TD3 algorithm, $a_{t-1}$ is not included in the information. The effect of lack of this information could become prominent in some conditions as in the "noise" case.
% \paragraph{Structure analysis}
% In this section, the results of LSTM-TD3$_{1ha1hc}$, LSTM-TD3$_{1ha2hc}$ 
% are focused to discuss about the structure in comparison with original LSTM-TD3. \\
% In "temporal sinusoidal wave" case, the performance of modified LSTM-TD3 algorithms showed better performance. Unlike LSTM-TD3 and TD3$_{1ha2hc}$, TD3$_{1ha1hc}$ with all time window $l$ showed very similar trajectory although the larger $l$ is slightly better. However TD3$_{1ha2hc}$ recorded better reachable episode reward except for $l=1$. Secondly, in "noise" case, it is noticeable that the result of $l=3$ in TD3$_{1ha1hc}$ is worse than LSTM-TD3 and LSTM-TD3$_{1ha2hc}$. The result of LSTM-TD3 with $l=3$ is comparable with LSTM-TD3$_{1ha2hc}$. Therefore this degradation is not rooted to the timing alignment of the data-set. The cause would be treating the current action $a_t$ separately from the sequence. This would explain why TD3$_{1ha2hc}$ is generaly better than LSTM-TD3$_{1ha1hc}$. Thirdly, LSTM-TD3$_{1ha1hc}$, LSTM-TD3$_{1ha2hc}$ showed less variance between $l$ in "hidden" case. Finally, "random sinusoidal wave" case represents the largest difference between original LSTM-TD3 and the modified algorithms in structure. The dynamical internal observation function is constructed by changing the structure to process past sequence and current information. 


% \paragraph{H-TD3 analysis}
% H-TD3 algorithm showed the comparable performance with LSTM-TD3$_{1ha1hc}$ and LSTM-TD3$_{1ha2hc}$ once it is converged. Interestingly, the "noise" case is exceptional. In H-TD3 algorithm $a_{t-1}$ is not included in the information. The effect of this lack could become prominent in the most difficult conditions which is "noise" case.  

\paragraph{Computational time and size}
Let us examine the computational time for training the network. Fig.~\ref{fig:time} shows the comparison of the training time by taking LSTM-TD3 with $l=1$ as the benchmark.
Due to the added operations in the code, LSTM-TD3$_{1ha1hc}$ and LSTM-TD3$_{1ha2hc}$ have a larger iteration time. The code is not optimized and there is room for efficiency in computation. 
As shown in the figure, the H-TD3 offers a much shorter time per iteration and is less affected by the length $l$. It is because it does not require repeating the trajectory in critic networks.

% Figure environment removed

\paragraph{Generalisability}
So far, the networks have been evaluated in the same environment where they were trained. However, in practice, the agent may experience different types of disturbances from the trained environment. For this reason, we tested how the networks trained in the "random sinusoidal wave" environment perform in "bias" and "noise" cases and also newly introduced environments: "combinational sinusoidal wave" and "damped sinusoidal wave". In the "combinational sinusoidal wave", two randomly configured sinusoidal waves with the same definition of "random sinusoidal wave" simultaneously appear, i.e., disturbance $= A_1\sin{2\pi t/T_1}+A_2\sin{2\pi t/T_2}$. On the other hand, the "combinational sinusoidal wave" adds damped sinusoidal waves defined by $e^{-(t-t_0)/T} A\sin{2\pi (t-t_0)/T}$, on all $3$ observation elements. The time steps to initiate damped sinusoidal wave $t_0$ are randomly selected and this disturbance continues till the end of the episode. The "bias" cases with the amplitude $1.0$ and the "noise" cases with $\sigma=1.0$ are also selected in this experiment. The normalized total reward of the episode after $1000$ time of trials is illustrated in Fig.~\ref{fig:gene}. The "comb. sin" and "damped sin" represent "combinational sinusoidal wave" and "damped sinusoidal wave" respectively. On the first column, the result of the "random sinusoidal wave" is shown as a reference. The trained network could perform except in "noise" disturbance. The dynamic model of disturbance exists in "combinational sinusoidal wave", "damped sinusoidal wave" and "bias" cases. As discussed in Section~\ref{keyidea}, the required behavior is different between the "noise" case and the other three cases. This result reinforces the hypothesis that the network trained in "random sinusoidal wave" utilizes the dynamic model of the disturbance and the system dynamic model and is compatible with the environment under the existence of temporally correlated disturbance.
% The selected types of disturbance in the test environments are the "bias" cases with the amplitude $0.5$ and $1.0$ and the "noise" cases with $\sigma=0.5$ and $\sigma=1.0$. By comparing with the results in Fig. \ref{fig:action_bias} when the network is trained in the same condition as the test environment, the trained network in the "random sinusoidal wave" environment performed well in two "bias" cases when the time window $l$ is long enough. However, the algorithm trained in the "random sinusoidal wave" environment did not show a good generalisability to the "noise" condition. Interestingly, performance in the "noise "condition of the TD3 algorithm has been improved by training in the "random sinusoidal wave" environment by comparing with the result shown in Fig. \ref{fig:endperformance}. The behavior of the gates in the LSTM cells for the "noise" case could differ from the other conditions. The other disturbances have non-random dynamics. Hence, the gate would keep the observations to generate the dynamics of the environment from the observations. Namely, all of the observations could contribute to generating the output. On the other hand, the "noise" case does not have specific dynamics. As we discussed in Section \ref{actionInclusion}, the forget gate would more actively select the informative data to make decisions in this case. Therefore, the algorithms with the LSTM layer did not show good generalisability in this test.

% Figure environment removed

% The advantage of 1ha1hc is less parameters (lighter network)
% overall comparison
% critic analysis from learned policy
% show sin4 result which requires more dynamic process to say action inclusion helps to identify the dynamic transition 
% In this section, all proposed algorithms are compared.

% The criticism of H-TD3 BPTT NLC is the input dimension of critic expands a lot. Then training the NN is much more difficult.

% Training neural networks can become more difficult as the input dimension increases due to a phenomenon known as the "curse of dimensionality." Here are a few reasons why this happens:

% Increased computational complexity: As the input dimension increases, the number of parameters in the neural network also increases, making the computational complexity of training the network much higher.

% Sparsity of data: With higher input dimensions, the data becomes more sparse, meaning that the amount of data needed to accurately represent the input space increases. This can make it harder for the neural network to learn patterns and relationships in the data.

% Overfitting: As the number of input dimensions increases, the network may become more prone to overfitting the training data, meaning that it learns to model the noise in the data rather than the underlying patterns. This can lead to poor performance on new data.

% Gradient vanishing/exploding: As the number of layers and neurons increase in a neural network, the gradients used to update the weights during training can become either too small (vanishing gradients) or too large (exploding gradients), making it difficult to train the network effectively.

% While training a neural network with high-dimensional inputs can be challenging, training a network with large dimensions in the hidden layer is generally not as difficult. This is because the hidden layer is responsible for transforming the input data into a lower-dimensional feature space that can be more easily processed by the output layer.

% Furthermore, the number of parameters in the hidden layer is typically much smaller than the number of parameters in the input layer, which makes the optimization problem easier to solve. This is because the input layer usually has a direct connection to every input feature, resulting in a large number of connections and parameters to learn. On the other hand, the hidden layer often has fewer connections and parameters, which makes it easier to optimize.

% \subsection{lerning from hidden states}
% % discuss about the other methods and cite the paper.
% flex and block (no BPTT) are other ways but not as good as the others becasue of BPTT 
