\begin{thebibliography}{10}

\bibitem{OpenAI-Gym}
{OpenAI Gym}.
\newblock \url{https://gymnasium.farama.org/}.
\newblock Accessed: 2023-01-14.

\bibitem{kurniawati2021partially}
Hanna Kurniawati.
\newblock Partially observable markov decision processes (pomdps) and robotics.
\newblock {\em arXiv preprint arXiv:2107.07599}, 2021.

\bibitem{hernandez2019survey}
Pablo Hernandez-Leal, Bilal Kartal, and Matthew~E Taylor.
\newblock A survey and critique of multiagent deep reinforcement learning.
\newblock {\em Autonomous Agents and Multi-Agent Systems}, 33(6):750--797,
  2019.

\bibitem{miki2022learning}
Takahiro Miki, Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun,
  and Marco Hutter.
\newblock Learning robust perceptive locomotion for quadrupedal robots in the
  wild.
\newblock {\em Science Robotics}, 7(62):eabk2822, 2022.

\bibitem{li2015recurrent}
Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li~Deng, and
  Ji~He.
\newblock Recurrent reinforcement learning: a hybrid approach.
\newblock {\em arXiv preprint arXiv:1509.03044}, 2015.

\bibitem{hausknecht2015deep}
Matthew Hausknecht and Peter Stone.
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock In {\em 2015 aaai fall symposium series}, 2015.

\bibitem{singla2019memory}
Abhik Singla, Sindhu Padakandla, and Shalabh Bhatnagar.
\newblock Memory-based deep reinforcement learning for obstacle avoidance in
  uav with limited environment knowledge.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems},
  22(1):107--118, 2019.

\bibitem{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock {\em Advances in neural information processing systems},
  34:1273--1286, 2021.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems},
  34:15084--15097, 2021.

\bibitem{hasani2021liquid}
Ramin Hasani, Mathias Lechner, Alexander Amini, Daniela Rus, and Radu Grosu.
\newblock Liquid time-constant networks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 7657--7666, 2021.

\bibitem{shen2021igibson}
Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart{\'\i}n-Mart{\'\i}n, Linxi Fan,
  Guanzhi Wang, Claudia P{\'e}rez-Dâ€™Arpino, Shyamal Buch, Sanjana Srivastava,
  Lyne Tchapmi, et~al.
\newblock igibson 1.0: A simulation environment for interactive tasks in large
  realistic scenes.
\newblock In {\em 2021 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 7520--7527. IEEE, 2021.

\bibitem{freeman2021brax}
C~Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and
  Olivier Bachem.
\newblock Brax--a differentiable physics engine for large scale rigid body
  simulation.
\newblock {\em arXiv preprint arXiv:2106.13281}, 2021.

\bibitem{truong2022rethinking}
Joanne Truong, Max Rudolph, Naoki Yokoyama, Sonia Chernova, Dhruv Batra, and
  Akshara Rai.
\newblock Rethinking sim2real: Lower fidelity simulation leads to higher
  sim2real transfer in navigation.
\newblock {\em arXiv preprint arXiv:2207.10821}, 2022.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{ravichandiran2019hands}
Sudharsan Ravichandiran.
\newblock {\em Hands-On Deep Learning Algorithms with Python: Master deep
  learning algorithms with extensive math by implementing them using
  TensorFlow}.
\newblock Packt Publishing Ltd, 2019.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{chung2015gated}
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio.
\newblock Gated feedback recurrent neural networks.
\newblock In {\em International conference on machine learning}, pages
  2067--2075. PMLR, 2015.

\bibitem{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International conference on machine learning}, pages
  1587--1596. PMLR, 2018.

\bibitem{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{hasselt2010double}
Hado Hasselt.
\newblock Double q-learning.
\newblock {\em Advances in neural information processing systems}, 23, 2010.

\bibitem{meng2021memory}
Lingheng Meng, Rob Gorbet, and Dana Kuli{\'c}.
\newblock Memory-based deep reinforcement learning for pomdps.
\newblock In {\em 2021 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 5619--5626. IEEE, 2021.

\bibitem{pearl2000models}
Judea Pearl et~al.
\newblock Models, reasoning and inference.
\newblock {\em Cambridge, UK: CambridgeUniversityPress}, 19(2), 2000.

\bibitem{bertsekas2012dynamic}
Dimitri Bertsekas.
\newblock {\em Dynamic programming and optimal control: Volume I}, volume~1.
\newblock Athena scientific, 2012.

\bibitem{hauskrecht1997planning}
Milos Hauskrecht.
\newblock {\em Planning and control in stochastic domains with imperfect
  information}.
\newblock PhD thesis, Massachusetts Institute of Technology, 1997.

\bibitem{hauskrecht2000value}
Milos Hauskrecht.
\newblock Value-function approximations for partially observable markov
  decision processes.
\newblock {\em Journal of artificial intelligence research}, 13:33--94, 2000.

\bibitem{gaudet2019adaptive}
Brian Gaudet and Richard Linares.
\newblock Adaptive guidance with reinforcement meta-learning.
\newblock {\em arXiv preprint arXiv:1901.04473}, 2019.

\bibitem{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937. PMLR, 2016.

\bibitem{heess2015memory}
Nicolas Heess, Jonathan~J Hunt, Timothy~P Lillicrap, and David Silver.
\newblock Memory-based control with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1512.04455}, 2015.

\bibitem{bakker2001reinforcement}
Bram Bakker.
\newblock Reinforcement learning with long short-term memory.
\newblock {\em Advances in neural information processing systems}, 14, 2001.

\bibitem{harmon1996multi}
Mance~E Harmon and Leemon~C Baird~III.
\newblock Multi-player residual advantage learning with general function
  approximation.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em International conference on machine learning}, pages
  387--395. PMLR, 2014.

\bibitem{malikopoulos2022separation}
Andreas~A Malikopoulos.
\newblock On separation between learning and control in partially observed
  markov decision processes.
\newblock {\em arXiv preprint arXiv:2211.14972}, 2022.

\bibitem{gasse2021causal}
Maxime Gasse, Damien Grasset, Guillaume Gaudron, and Pierre-Yves Oudeyer.
\newblock Causal reinforcement learning using observational and interventional
  data.
\newblock {\em arXiv preprint arXiv:2106.14421}, 2021.

\end{thebibliography}
