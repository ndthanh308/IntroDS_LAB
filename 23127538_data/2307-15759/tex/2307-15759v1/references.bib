@Article{mat2vec,
author={Tshitoyan, Vahe
and Dagdelen, John
and Weston, Leigh
and Dunn, Alexander
and Rong, Ziqin
and Kononova, Olga
and Persson, Kristin A.
and Ceder, Gerbrand
and Jain, Anubhav},
title={Unsupervised word embeddings capture latent knowledge from materials science literature},
journal={Nature},
year={2019},
month={Jul},
day={01},
volume={571},
number={7763},
pages={95-98},
abstract={The overwhelming majority of scientific knowledge is published as text, which is difficult to analyse by either traditional statistical analysis or modern machine learning methods. By contrast, the main source of machine-interpretable data for the materials research community has come from structured property databases1,2, which encompass only a small fraction of the knowledge present in the research literature. Beyond property values, publications contain valuable knowledge regarding the connections and relationships between data items as interpreted by the authors. To improve the identification and use of this knowledge, several studies have focused on the retrieval of information from scientific literature using supervised natural language processing3--10, which requires large hand-labelled datasets for training. Here we show that materials science knowledge present in the published literature can be efficiently encoded as information-dense word embeddings11--13 (vector representations of words) without human labelling or supervision. Without any explicit insertion of chemical knowledge, these embeddings capture complex materials science concepts such as the underlying structure of the periodic table and structure--property relationships in materials. Furthermore, we demonstrate that an unsupervised method can recommend materials for functional applications several years before their discovery. This suggests that latent knowledge regarding future discoveries is to a large extent embedded in past publications. Our findings highlight the possibility of extracting knowledge and relationships from the massive body of scientific literature in a collective manner, and point towards a generalized approach to the mining of scientific literature.},
issn={1476-4687},
doi={10.1038/s41586-019-1335-8},
url={https://doi.org/10.1038/s41586-019-1335-8}
}


@Article{Kim2017,
author={Kim, Edward
and Huang, Kevin
and Tomala, Alex
and Matthews, Sara
and Strubell, Emma
and Saunders, Adam
and McCallum, Andrew
and Olivetti, Elsa},
title={Machine-learned and codified synthesis parameters of oxide materials},
journal={Scientific Data},
year={2017},
month={Sep},
day={12},
volume={4},
number={1},
pages={170127},
abstract={Predictive materials design has rapidly accelerated in recent years with the advent of large-scale resources, such as materials structure and property databases generated by ab initio computations. In the absence of analogous ab initio frameworks for materials synthesis, high-throughput and machine learning techniques have recently been harnessed to generate synthesis strategies for select materials of interest. Still, a community-accessible, autonomously-compiled synthesis planning resource which spans across materials systems has not yet been developed. In this work, we present a collection of aggregated synthesis parameters computed using the text contained within over 640,000 journal articles using state-of-the-art natural language processing and machine learning techniques. We provide a dataset of synthesis parameters, compiled autonomously across 30 different oxide systems, in a format optimized for planning novel syntheses of materials.},
issn={2052-4463},
doi={10.1038/sdata.2017.127},
url={https://doi.org/10.1038/sdata.2017.127}
}


@article{word2vec,
  added-at = {2013-10-23T23:02:12.000+0200},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  biburl = {https://www.bibsonomy.org/bibtex/29665b85e8756834ac29fcbd2c6ad0837/wool},
  ee = {http://arxiv.org/abs/1301.3781},
  interhash = {e92df552b17e9f952226a893b84ad739},
  intrahash = {9665b85e8756834ac29fcbd2c6ad0837},
  journal = {CoRR},
  keywords = {nlp},
  timestamp = {2013-10-23T23:02:12.000+0200},
  title = {Efficient Estimation of Word Representations in Vector Space},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1301.html#abs-1301-3781},
  volume = {abs/1301.3781},
  year = 2013
}


@inproceedings{gensim,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      note={\url{http://is.muni.cz/publication/884893/en}},
      language={English}
}


@inproceedings{BERT,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{olivetti2020data,
  title={Data-driven materials research enabled by natural language processing and information extraction},
  author={Olivetti, Elsa A and Cole, Jacqueline M and Kim, Edward and Kononova, Olga and Ceder, Gerbrand and Han, Thomas Yong-Jin and Hiszpanski, Anna M},
  journal={Applied Physics Reviews},
  volume={7},
  number={4},
  year={2020},
  publisher={AIP Publishing}
}

@article{trewartha2022quantifying,
  title={Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science},
  author={Trewartha, Amalie and Walker, Nicholas and Huo, Haoyan and Lee, Sanghoon and Cruse, Kevin and Dagdelen, John and Dunn, Alexander and Persson, Kristin A and Ceder, Gerbrand and Jain, Anubhav},
  journal={Patterns},
  volume={3},
  number={4},
  year={2022},
  publisher={Elsevier}
}

@misc{bran2023chemcrow,
      title={ChemCrow: Augmenting large-language models with chemistry tools}, 
      author={Andres M Bran and Sam Cox and Andrew D White and Philippe Schwaller},
      year={2023},
      eprint={2304.05376},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph}
}

@misc{skreta2023errors,
      title={Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting}, 
      author={Marta Skreta and Naruki Yoshikawa and Sebastian Arellano-Rubach and Zhi Ji and Lasse Bjørn Kristensen and Kourosh Darvish and Alán Aspuru-Guzik and Florian Shkurti and Animesh Garg},
      year={2023},
      eprint={2303.14100},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@article{lee2023natural,
  title={Natural Language Processing Techniques for Advancing Materials Discovery: A Short Review},
  author={Lee, Joo Hyuk and Lee, Myeonghun and Min, Kyoungmin},
  journal={International Journal of Precision Engineering and Manufacturing-Green Technology},
  pages={1--13},
  year={2023},
  publisher={Springer}
}

@article{pyzer2022accelerating,
  title={Accelerating materials discovery using artificial intelligence, high performance computing and robotics},
  author={Pyzer-Knapp, Edward O and Pitera, Jed W and Staar, Peter WJ and Takeda, Seiji and Laino, Teodoro and Sanders, Daniel P and Sexton, James and Smith, John R and Curioni, Alessandro},
  journal={npj Computational Materials},
  volume={8},
  number={1},
  pages={84},
  year={2022},
  publisher={Nature Publishing Group UK London}
}