\begin{thebibliography}{10}

\bibitem{BERT}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{olivetti2020data}
Elsa~A Olivetti, Jacqueline~M Cole, Edward Kim, Olga Kononova, Gerbrand Ceder,
  Thomas Yong-Jin Han, and Anna~M Hiszpanski.
\newblock Data-driven materials research enabled by natural language processing
  and information extraction.
\newblock {\em Applied Physics Reviews}, 7(4), 2020.

\bibitem{lee2023natural}
Joo~Hyuk Lee, Myeonghun Lee, and Kyoungmin Min.
\newblock Natural language processing techniques for advancing materials
  discovery: A short review.
\newblock {\em International Journal of Precision Engineering and
  Manufacturing-Green Technology}, pages 1--13, 2023.

\bibitem{pyzer2022accelerating}
Edward~O Pyzer-Knapp, Jed~W Pitera, Peter~WJ Staar, Seiji Takeda, Teodoro
  Laino, Daniel~P Sanders, James Sexton, John~R Smith, and Alessandro Curioni.
\newblock Accelerating materials discovery using artificial intelligence, high
  performance computing and robotics.
\newblock {\em npj Computational Materials}, 8(1):84, 2022.

\bibitem{Kim2017}
Edward Kim, Kevin Huang, Alex Tomala, Sara Matthews, Emma Strubell, Adam
  Saunders, Andrew McCallum, and Elsa Olivetti.
\newblock Machine-learned and codified synthesis parameters of oxide materials.
\newblock {\em Scientific Data}, 4(1):170127, Sep 2017.

\bibitem{mat2vec}
Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga
  Kononova, Kristin~A. Persson, Gerbrand Ceder, and Anubhav Jain.
\newblock Unsupervised word embeddings capture latent knowledge from materials
  science literature.
\newblock {\em Nature}, 571(7763):95--98, Jul 2019.

\bibitem{gensim}
Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka.
\newblock {Software Framework for Topic Modelling with Large Corpora}.
\newblock In {\em {Proceedings of the LREC 2010 Workshop on New Challenges for
  NLP Frameworks}}, pages 45--50, Valletta, Malta, May 2010. ELRA.
\newblock \url{http://is.muni.cz/publication/884893/en}.

\bibitem{word2vec}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock {\em CoRR}, abs/1301.3781, 2013.

\bibitem{trewartha2022quantifying}
Amalie Trewartha, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John
  Dagdelen, Alexander Dunn, Kristin~A Persson, Gerbrand Ceder, and Anubhav
  Jain.
\newblock Quantifying the advantage of domain-specific pre-training on named
  entity recognition tasks in materials science.
\newblock {\em Patterns}, 3(4), 2022.

\bibitem{bran2023chemcrow}
Andres~M Bran, Sam Cox, Andrew~D White, and Philippe Schwaller.
\newblock Chemcrow: Augmenting large-language models with chemistry tools,
  2023.

\bibitem{skreta2023errors}
Marta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse~Bjørn
  Kristensen, Kourosh Darvish, Alán Aspuru-Guzik, Florian Shkurti, and Animesh
  Garg.
\newblock Errors are useful prompts: Instruction guided task programming with
  verifier-assisted iterative prompting, 2023.

\end{thebibliography}
