\subsection{Background: Convergent Robust Push-Sum}
In \cite{UnreliableConvergence}, we proposed the {\em Convergent Robust Push-Sum}, described in Algorithm \ref{alg:ps convergence rate}. It works for a single network, i.e., $M=1$ and no parameter server. For ease of exposition, we present Algorithm \ref{alg:ps convergence rate} for time-invariant graphs. 

%In our proposed algorithm, we are able to use the existing analysis in literature to obtain the convergence rate.
%
\begin{algorithm}
\caption{Convergent Robust Push-Sum}
\label{alg:ps convergence rate}
{\em Initialization}: For each agent $j\in \calV$, set $z_j[0]=w_j\in \reals^d$, $m_j[0]=1\in \reals,$ $\sigma_j[0]={\bf 0}\in \reals^d$, $\tilde{\sigma}_j[0]=0\in \reals$, and $\rho_{j^{\prime}j}[0]={\bf 0}\in \reals^d$, $\tilde{\rho}_{j^{\prime}j}[0]=0\in \reals$ for each incoming link, i.e., $j^{\prime}\in \calI_j$.

\For{$t\ge 1$}
{
$\sigma^{+}_j[t]  \gets  \sigma_j[t-1] + \frac{z_j[t-1]}{d_j+1}$,
$\tilde{\sigma}^{+}_j[t]  \gets  \tilde{\sigma}_j[t-1] + \frac{m_j[t-1]}{d_j+1}$\;

Broadcast $\pth{\sigma^{+}_j[t], \tilde{\sigma}^{+}_j[t]}$ to outgoing neighbors\;

\For {each incoming link $(j^{\prime}, j)$}
{\eIf{message $\pth{\sigma^{+}_{j^{\prime}}[t], \tilde{\sigma}^{+}_{j^{\prime}}[t]}$ is received}
{$\rho_{j^{\prime}j}[t] \gets \sigma^{+}_{j^{\prime}}[t]$, ~~ $\tilde{\rho}_{j^{\prime}j}[t] \gets \tilde{\sigma}^{+}_{j^{\prime}}[t]$\;}
{ $\rho_{j^{\prime}j}[t] \gets \rho_{j^{\prime}j}[t-1]$, ~~$\tilde{\rho}_{j^{\prime}j}[t] \gets \tilde{\rho}_{j^{\prime}j}[t-1]$\;}
%
%
$ z_j^{+}[t] \gets \frac{z_j[t-1]}{d_j+1} +  \sum_{j^{\prime}\in \calI_j} \pth{\rho_{j^{\prime}j}[t] - \rho_{j^{\prime}j}[t-1]}$, 
$m_j^{+}[t]  \gets \frac{m_j[t-1]}{d_j+1} + \sum_{j^{\prime}\in \calI_j} \pth{\tilde{\rho}_{j^{\prime}j}[t] -\tilde{\rho}_{j^{\prime}j}[t-1]}$.
}

$\sigma_j[t]  \gets  \sigma^{+}_j[t] + \frac{z_j^{+}[t]}{d_j+1}$,
$\tilde{\sigma}_j[t]  \gets  \tilde{\sigma}^{+}_j[t] + \frac{m_j^{+}[t]}{d_j+1}$,
$z_j[t]  \gets \frac{z_j^+[t]}{d_j+1}$,
$m_j[t] \gets \frac{m_j^+[t]}{d_j+1}$.
}
\end{algorithm}
% 


We use $\sigma^{+}_j[t]$, $\tilde{\sigma}^{+}_j[t]$, $z_j^+[t]$, and $m_j^{+}[t]$ to emphasize the fact that they are intermediate values of the corresponding quantities in iteration $t$.  

In each iteration of our Algorithm \ref{alg:ps convergence rate}, the cumulative transmitted value and weight $(\sigma_j, \tilde{\sigma}_j)$, and the local value and weight $(z_j, m_j)$ are updated twice. As mentioned before, with only the first update, the dynamics in the system are not stable enough, as the two iterates ``kept" by the virtual agents are reset to zero periodically and unexpectedly. This ``reset" is prevented by the second update, in which  %in our Algorithm \ref{alg:ps convergence rate}. 
%Intuitively speaking, in the second update, 
each agent pushes nonzero ``mass" to the virtual agents on its outgoing links. As a result of this, the two iterates ``kept" by a virtual agent will never be zero at the end of an iteration.
%\subsection{Augmented Graph}
%We consider the augmented graph of $G(\calV, \calE)$ and the associated matrix representation proposed in \cite{vaidya2012robust}.
%
\begin{definition}[Augmented Graph] \cite{vaidya2012robust}
Given a graph $G(\calV, \calE)$, the augmented graph $G^a(\calV^a, \calE^a)$ is constructed as: % follows \cite{vaidya2012robust}:
\begin{enumerate}
\item $\calV^a=\calV\cup \calE$:  $|\calE|$ virtual agents are introduced, each of which represents a link in $G(\calV, \calE)$. 
Let $n_{j^{\prime}j}$ be the virtual agent corresponding to edge $(j^{\prime},j)$.
\item $\calE^a \triangleq \calE \cup \sth{\pth{j^{\prime}, n_{j^{\prime}j}}, (n_{j^{\prime}j}, j), ~ \forall ~ (j^{\prime}, j)\in \calE}$. 
%$\calE\subseteq \calE^a$: the edge set in $G^a(\calV^a, \calE^a)$ preserves the topology of $G(\calV, \calE)$;
%\item Additionally, auxiliary edges are introduced: each auxiliary agent $n_{ij}$ has one incoming neighbor -- agent $i$ -- and one outgoing neighbor -- agent $j$.
%for each $k\in \calV$ and $(i,j)\in \calE$, $\pth{k, (i,j)}\in \calE^a$ if and only if $k=i$, and $\pth{(i,j),k}\in \calE^a$ if and only if $j=k$;
%\item  each auxiliary agent has a self-loop, i.e., $\pth{(i,j), (i,j)}\in \calE^a$.
\end{enumerate}
\end{definition}



\subsection{Convergent Robust Push-Sum on Hierarchical FL}
\label{subsec: push-sum hierarchical FL}
% 
% 
We first consider the scenario where each network has a designated agent that can exchange messages with the parameter server. Let $i_0$ denote the designated agent of network $i$. 

The algorithm for hierarchical setting is: 
In each iteration, each agent in a network perform the convergent robust push-sum update in Algorithm \ref{alg:ps convergence rate}. Every other $\Gamma$ iterations, the designated agent in each network wakes up and pushes 1/2 of its local estimate and local mass to the PS. The PS computes the received average estimate and mass, and sends the averages back to each designated agent. Each designated agent then updates its local estimates and mass as ones pushed back from the PS.  



\begin{algorithm}
\caption{Hierarchical Push-sum}
\label{alg:push-sum hierarchical FL}
{\em Initialization}: \\
For each sub-network $i=1, \cdots, M$: $z_j^i[0]=w_j^i\in \reals^d$, $m^i_j[0]=1\in \reals,$ $\sigma_j^i[0]={\bf 0}\in \reals^d$, $\tilde{\sigma}_j^i[0]=0\in \reals$, and $\rho_{j^{\prime}j}[0]={\bf 0}\in \reals^d$, $\tilde{\rho}_{j^{\prime}j}[0]=0\in \reals$ for each incoming link, i.e., $j^{\prime} \in \calI_j^i$.

 
\vskip \baselineskip 

\begin{center}
    On each client $j\in \calV_i$: 
\end{center}
\For{$t\ge 1$}
{
$\sigma_j^{i+}[t]  \gets  \sigma_j^i[t-1] + \frac{z_j^i[t-1]}{d_j^i[t]+1}$,
$\tilde{\sigma}_j^{i+}[t] \gets \tilde{\sigma}_j^i[t-1] + \frac{m_j^i[t-1]}{d_j^i[t]+1}$\;

Broadcast $\pth{\sigma^{i+}_j[t], \tilde{\sigma}^{i+}_j[t]}$ to outgoing neighbors\;

\For {each incoming link $(j^{\prime},j)\in \calE_i$}
{\eIf{message $\pth{\sigma^{i+}_{j^{\prime}}[t], \tilde{\sigma}^{i+}_{j^{\prime}}[t]}$ is received}
{$\rho^i_{j^{\prime}j}[t] \gets \sigma^{i+}_{j^{\prime}}[t]$, ~~ $\tilde{\rho}^i_{j^{\prime}j}[t] \gets \tilde{\sigma}^{i+}_{j^{\prime}}[t]$\;}
{ $\rho^i_{j^{\prime}j}[t] \gets \rho^i_{j^{\prime}j}[t-1]$, ~~$\tilde{\rho}^i_{j^{\prime}j}[t] \gets \tilde{\rho}^i_{j^{\prime}j}[t-1]$\;}
%
%
$ z_j^{i+}[t] \gets \frac{z_j^{i}[t-1]}{d_j^i[t]+1} +  \sum_{j^{\prime}\in \calI_j^i} \pth{\rho_{j^{\prime}j}[t] - \rho_{j^{\prime}j}[t-1]}$, 
$m_j^{i+}[t]  \gets \frac{m_j^i[t-1]}{d_j^i[t]+1} + \sum_{j^{\prime}\in \calI_j^i} \pth{\tilde{\rho}_{j^{\prime}j}[t] -\tilde{\rho}_{j^{\prime}j}[t-1]}$.
}

$\sigma^i_j[t]  \gets  \sigma^{i+}_j[t] + \frac{z_j^{i+}[t]}{d_j^i[t]+1}$,
$\tilde{\sigma}^i_j[t]  \gets  \tilde{\sigma}^{i+}_j[t] + \frac{m_j^{i+}[t]}{d_j^i[t]+1}$,
$z_j^i[t]  \gets \frac{z_j^{i+}[t]}{d_j^i[t]+1}$,
$m_j^i[t] \gets \frac{m_j^{i+}[t]}{d_j^i[t]+1}$\; 
% 
}

% \begin{center}
%     On each designated agent: 
% \end{center}
\If{$j$ is a designated agent of network $S_i$}
{
\If{$t\mod \Gamma =0$}
{
%$\sigma_j^i[t^+] \gets \sigma_j^i[t] + \frac{1}{2}z_j^i[t]$\; 
%$\tilde{\sigma}_j^i[t^+] \gets \tilde{\sigma}_j^i[t] + \frac{1}{2}m_j^i[t]$\; 
Send $\frac{1}{2}z_j^i[t]$ and $\frac{1}{2}m_j^i[t]$ to the PS\; 

Upon receiving messages from the PS {\bf do} \\
update  
$z_j^{i}[t]\gets \frac{1}{2}z_j^{i}[t] + \frac{1}{2M}\sum_{i=1}^M z_{i_0}^i[t]$\; 
$m_j^{i}[t]\gets \frac{1}{2}m_j^{i}[t] + \frac{1}{2M}\sum_{i=1}^M m_{i_0}^i[t]$\;
}
} 

\begin{center}
    On the PS: 
\end{center}
\If{$t\mod \Gamma =0$}
{
Wait to receive $z_{i_0}^i[t]$ and $m_{i_0}^i[t]$ from each designated agent of the $M$ networks\; 

Compute and send $\frac{1}{M}\sum_{i=1}^M \frac{1}{2}z_{i_0}^i[t]$ and $\frac{1}{M}\sum_{i=1}^M \frac{1}{2}m_{i_0}^i[t]$ to all designated agents $i_0$ for $i=1, \cdots, M$. 
} 

% \begin{center}
%     On each designated agent: 
% \end{center}
% \If{$t\mod \Gamma =0$}
% {
% Upon receiving messages from the PS: updates 
% $z_i[t]\gets \frac{1}{2}z_i[t^+] + \frac{1}{2M}\sum_{i=1}^M z_{i_0}^i[t^+]$\; 
% $m_i[t]\gets \frac{1}{2}m_i[t^+] + \frac{1}{2M}\sum_{i=1}^M m_{i_0}^i[t^+]$\;

% }

\end{algorithm}


\ls{Here, we need to state the assumptions on the connectivity of the networks. Please refer \cite{spiridonoff2020robust} to find the formal statements of the assumptions. 
}
\begin{assumption}
\label{ass: connectivity}
Each network $(\calV_i, \calE_i)$ is strongly connected for $i=1, \cdots, M$. 
\end{assumption}

\begin{definition}
\label{def: diameter}
For any network $S_i = G\pth{\calV_i, \calE_i}$ where $i=\in [M]$, its diameter, denoted by $D_i$, is the length $\max_{(u,v)\in \calV_i}d(u,v)$ of the ``longest shortest path'' between any two vertices $(u,v)$, where $d(u,v)$ is the length of the shortest path from vertex $u$ to vertex $v$. 
\end{definition}


\begin{theorem}
\label{rps convergence rate}
Let $D^* := \max_{i\in [M]}D_i$. 
Choose $\Gamma = BD^*$.  
Suppose that Assumptions \ref{ass: link reliability} and \ref{ass: connectivity} hold, and that $t\ge 2\Gamma$. 
Under Algorithm \ref{alg:ps convergence rate}, at each sub-network $i$, 
at each agent $i\in \calV$, it holds that 
\begin{align*}
\norm{\frac{z_j^i[t]}{m_j^i[t]} -\frac{1}{N}\sum_{i=1}^M \sum_{j=1}^{n_i} w_j^i} \le \frac{4M^2\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}\norm{w_{j^{\prime}}^i}}{\pth{\min_{i\in [M]}\beta_i}^{2D^*B}\sum_{i=1}^M n_i}  \min\{1, \gamma^{\lfloor t/2\Gamma \rfloor - 1}\}, 
\end{align*} 
where $\beta_i \triangleq \frac{1}{\max_{j\in \calV_i} (d_j^{i}+1)^2}$ and $\gamma := 1-\frac{1}{4M^2} \pth{\min_{i\in [M]}\beta_i}^{2D^*B}$.  
\end{theorem}
% 
The proof of Theorem \ref{rps convergence rate} relies on a compact matrix representation of the dynamics of $z$ and $m$. 

\subsection{Matrix Representation: }
\label{subsec: matrix representation}
% 
% 
Without loss of generality, we index the nodes in the $M$ networks from $1$ to $N:=\sum_{i=1}^M n_i$. 
\ls{Draw a figure to illustrate the adjacency matrix of the multiple networks.}

% 
% 
We follow our prior work \cite{UnreliableConvergence} on the matrix construction for a single network. 


Fix a network $S_i$. Fix $t$ be arbitrary iteration such that $t\mod \Gamma \not=0$. 

For each link $(j,j^{\prime})\in\calE_i[t]$, and $t\ge 1$, define %the indicator variable 
$\sfB^i_{(j,j^{\prime})}[t]$ as 
\begin{align}
\sfB^i_{(j,j^{\prime})}[t] \triangleq \left\{
 \begin{tabular}{ll}
 1, ~ if link $(j,j^{\prime})\in \calE_i[t]$ and is reliable at time $t$; \\
 0, ~ otherwise.  \label{indicator_var}
 \end{tabular}
 \right.
\end{align}
Recall that $z_j^i$ and $m_j^i$ are the value and weight for $j\in \calV^i$. % =\{1, \cdots, n\}$. 
For each $(j,j^{\prime})\in \calE_i[t]$, we define $z^i_{n_{j^{\prime}j}}$ and $m^i_{n_{j^{\prime}j}}$ as
\begin{align}
z^i_{n_{j^{\prime}j}}[t]~\triangleq ~\sigma^i_{j^{\prime}}[t]-\rho^i_{j^{\prime}j}[t], ~\text{and} %~  \label{z virtual}\\
~ m^i_{n_{j^{\prime}j}}[t]~\triangleq ~\tilde{\sigma}^i_{j^{\prime}}[t]-\tilde{\rho}^i_{j^{\prime}j}[t], \label{w virtual}
\end{align}
with $z^i_{n_{j^{\prime}j}}[0]={\bf 0}\in \reals^d$ and $m^i_{n_{j^{\prime}j}}[0]=0\in \reals$.
Intuitively, $z^i_{n_{j^{\prime}j}}[t]$ and $m^i_{n_{j^{\prime}j}}[t]$ are the value and weight that agent $j^{\prime}$ tries to send to agent $j$ not not successfully be delivered. 


The evolution of $z$ and $m$ can be described in a matrix form. 
% the following matrix form
% \begin{align}
% z_i[t] &= \sum_{j} z_j[t-1] {\bf M}_{j,i}[t],\label{matrix update 1}\\
% w_i[t] &= \sum_{j} w_j[t-1] {\bf M}_{j,i}[t], \label{matrix update 2}
% \end{align}
% where the transition matrix ${\bf M}[t]$ (specified later) is a function of the indicator variables defined in  \eqref{indicator_var}.
Since the update of value $z$ and weight $m$ are identical, %for ease of exposition, 
henceforth, we focus on the value sequence $z$.
% 
From Algorithm \ref{alg:ps convergence rate}, we know
\begin{align}
\rho^i_{j^{\prime}j}[t]&=\sfB^i_{(j^{\prime},j)}[t]\sigma^{i+}_{j^{\prime}}[t]+(1-\sfB^i_{(j^{\prime}, j)}[t])\rho^i_{j^{\prime}j}[t-1] \label{e_rho_reform 1}. %\\
%\tilde{\rho}_{j^{\prime}j}[t]&=\sfB_{(j,j^{\prime})}[t]\tilde{\sigma}^+_j[t]+(1-\sfB_{(j,j^{\prime})}[t])\tilde{\rho}_{j^{\prime}j}[t-1].\label{e_rho_reform 2}
\end{align}
%
%
%
By \eqref{indicator_var}, \eqref{w virtual} and \eqref{e_rho_reform 1},  for each $j\in \calV_i$, the update of $z_j^i$ is% and $w_i$ are
\begin{align*}
%\label{z[t]_reform 1}
%\begin{cases}
z_{j}^{i+}[t]&=\frac{z_{j}^i[t-1]}{d_j^i[t]+1} + \sum_{j^{\prime}\in\calI^i_j[t]}\sfB^i_{(j^{\prime}, j)}[t]\left(\frac{z^i_{j^{\prime}}[t-1]}{d_{j^{\prime}}^i[t]+1}+z_{n^i_{j^{\prime}j}}[t-1]\right),\\
\text{and} \qquad z_{j}^i[t] &= \frac{z^{i+}_{j}[t]}{d_j^i[t]+1}.
%\end{cases}
\end{align*}
Thus,
\begin{align*}%\label{matrix zi new}
z_j^i[t] %& = \frac{z_i^+[t]}{d_i^o+1} \\% = \frac{1}{d_i^o+1} \frac{z_{i}[t-1]}{d_i^o+1}\\
%\nonumber
%&\quad + \frac{1}{d_i^o+1}\sum_{j\in\calI_i}\sfB_{(j,j^{\prime})}[t]\left(\frac{z_j[t-1]}{d_j^{o}+1}+z_{n_{j^{\prime}j}}[t-1]\right)\\
%\nonumber
= \frac{z_j^i[t-1]}{\pth{d_j^i[t]+1}^2}  + \sum_{j^{\prime}\in \calI^i_j[t]} \frac{\sfB^i_{(j^{\prime}, j)}[t]}{\pth{d_j^i[t]+1}\pth{d_{j^{\prime}}^i[t]+1}} z^i_{j^{\prime}}[t-1]  
%\nonumber
 + \sum_{j^{\prime}\in \calI^i_j[t]} \frac{\sfB^i_{(j^{\prime}, j)}[t]}{d_j^i[t]+1} z^i_{n_{j^{\prime}j}}[t-1].
\end{align*}
%
%\begin{align}
%\label{z[t]_reform 2}
%\begin{cases}
%w^+_{i}[t]&=\frac{w_{i}[t-1]}{d_i^o+1} + \sum_{j\in\calI_i}\sfB_{(j,j^{\prime})}[t]\left(\frac{w_j[t-1]}{d_j^{o}+1}+w_{\phi(j,j^{\prime})}[t-1]\right),\\
%w_{i}[t] &= \frac{w^+_{i}[t]}{d_i^o+1}.
%\end{cases}
%\end{align}
Similarly, we get
\begin{align*}%\label{matrix z edge new}
%\nonumber
&z^i_{n_{j^{\prime}j}}[t]= \pth{\frac{1}{\pth{d_{j^{\prime}}^i[t]+1}^2} + \frac{1- \sfB^i_{(j^{\prime}, j)}[t]}{d_{j^{\prime}}^i[t]+1}} z^i_{j^{\prime}}[t-1]  \\
%\nonumber
&\quad + \sum_{k\in \calI^i_{j^{\prime}}[t]} \frac{\sfB^i_{(k,j^{\prime})}[t]}{\pth{d_k^i[t]+1}\pth{d_{j^{\prime}}^i[t]+1}} z^i_k[t-1] \\ 
%\nonumber
&\quad + \sum_{k\in \calI^i_{j^{\prime}}[t]} \frac{\sfB^i_{(k,j^{\prime})}[t]}{d_{j^{\prime}}^i[t]+1} z^i_{n_{kj^{\prime}}}[t-1] + \pth{1-\sfB^i_{(j^{\prime},j)}[t]} z^i_{n_{j^{\prime}j}}[t-1].
\end{align*}

Recall that $N = \sum_{i=1}^m n_i$ is the total number of agents in the multi-networks. Let $m_i : = |\calE_i|$ denote the number of edges in network $S_i$. Let $\tilde{N} := \sum_{i=1}^m (n_i+m_i)$. 
Thus, we construct a matrix ${\bf M}[t]\in \reals^{N\times N}$ as follows: Let $j$ be an arbitrary agent in network $S_i$. We have 
%such that %with the following structure:  %For $i, j\in \calV=\{1, \cdots, n\}$ and $k^{\prime}\in \{n+1, \cdots, m\}$ such that $(i,j)\in \calE$ and $k^{\prime}=\phi((i,j))$, let
\begin{align*}
%\nonumber
&{\bf M}_{j,j}[t]\triangleq \frac{1}{\pth{d_j^{i}[t]+1}^2}, \quad 
%\nonumber
{\bf M}_{j,j^{\prime}}[t]\triangleq \frac{\sfB^i_{(j^{\prime}, j)}[t]}{\pth{d_j^{i}[t]+1}\pth{d_{j^{\prime}}^i[t]+1}}, ~ \forall ~ j^{\prime}\in \calI_j^i[t],  
%\nonumber
\quad 
{\bf M}_{j, n_{j^{\prime}j}}[t]\triangleq \frac{\sfB^i_{(j^{\prime}, j)}[t]}{d_j^{i}[t]+1}, ~ \forall ~ j^{\prime} \in \calI^i_j[t], \\
%\nonumber
&{\bf M}_{n_{j^{\prime}j}, j^{\prime}}[t] \triangleq \frac{1}{\pth{d_{j^{\prime}}^i[t]+1}^2} + \frac{1- \sfB^i_{(j^{\prime}, j)}[t]}{d_{j^{\prime}}^i[t]+1}, 
\qquad \qquad \qquad \quad {\bf M}_{n_{j^{\prime}j}, k}[t] = \frac{\sfB^i_{(k,j^{\prime})}[t]}{\pth{d_k^i[t]+1}\pth{d_{j^{\prime}}^i[t]+1}}, ~~~ \forall ~ k \in \calI^i_{j^{\prime}}[t],\\
%\nonumber
&{\bf M}_{n_{j^{\prime}j}, n_{kj^{\prime}}}[t] \triangleq \frac{\sfB^i_{(k,j^{\prime})}[t]}{d_{j^{\prime}}^{i}[t]+1},~~ \forall ~ k\in \calI^i_{j^{\prime}}[t];   \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad {\bf M}_{n_{j^{\prime}j}n_{j^{\prime}j}}[t] \triangleq 1- \sfB^i_{(j^{\prime}, j)}[t].
%\label{matrix con 2}
\end{align*}
and any other entry in ${\bf M}[t]$ be zero.
%
%
It is easy to check that the obtained matrix ${\bf M}[t]$ is column stochastic, and that 
\begin{align*}
    \bm{z}[t] = \pth{\bm{M}[t] \otimes \bm{I}}\bm{z}[t-1], ~~~ \forall ~ t \mod \Gamma \not=0, 
\end{align*}
where $\bm{z}[t]\in \reals^{\tilde{N}d}$ is the vector that stacks all the local $z$'s. 


\subsubsection{Hierarchical networks}
\label{subsub: Hierarchical networks}
% 
% 
Fix $t$ be arbitrary iteration such that $t\mod \Gamma =0$. We construct matrix $\bm{M}$ in two steps. We let $\bar{\bm{M}}$ denote the matrix constructed the same way as above. Let $\bm{F}\in \reals^{\sum_{i=1}^M n_i \times \sum_{i=1}^M n_i}$ be the matrix that captures the mass push among the designated agents under the coordination of the parameter server. Specifically, 
\begin{align*}
\bm{F}_{j_0,j_0} &= \frac{M+1}{2M} ~~~~ \text{for each designated agent } j_0; \\
\bm{F}_{j_0,j^{\prime}_0} & = \frac{1}{2M} ~~~~ \text{for any pair of distinct designated agents } j_0, j^{\prime}_0,
\end{align*}
with all the other entries being zeros. Henceforth, we refer to matrix $\bm{F}$ as hierarchical fusion matrix. 
Clearly, $\bm{F}$ is a doubly-stochastic matrix. Hence, we define $\bm{M}$ as 
\begin{align}
\label{eq: fusion iteration}
\bm{M}[t] =\bm{F} \bar{\bm{M}}[t]. 
\end{align}
%
%
\ls{Moving to randomly chosen agents communicating with the PS, the fusion matrix $\bm{F}$ becomes time-varying. We might be able to use some gossiping procedure analysis techniques to derive the convergence rate. }
It is easy to see that the dynamics of $\bm{z}[t]$ for $t\mod \Gamma =0$ also obey 
\begin{align*}
    \bm{z}[t] = \pth{\bm{M}[t] \otimes \bm{I}}\bm{z}[t-1]. 
\end{align*} 


Thus, we have 
\begin{align*}
\bm{z}[t] & = \pth{\bm{M}[t] \otimes \bm{I}}\pth{\bm{M}[t-1] \otimes \bm{I}} \bm{z}[t-2] \\
& = \pth{\bm{M}[t] \otimes \bm{I}}\pth{\bm{M}[t-1] \otimes \bm{I}} \cdots \pth{\bm{M}[1] \otimes \bm{I}} \bm{z}[0]. 
\end{align*} 
That is, the evolution of $z$ is controlled by the matrix product $\bm{M}[t]\bm{M}[t-1]\cdots \bm{M}[1]$. 
In general, let ${\bf \Psi}(r,t)$ be the product of $t-r+1$ matrices
\begin{align*}
{\bf \Psi}(r,t)&\triangleq \prod_{\tau=r}^t\, {\bf M}^{\top}[\tau]={\bf M}^{\top}[r] {\bf M}^{\top}[r+1]\cdots {\bf M}^{\top}[t],
\end{align*}
where $r\le t$ with ${\bf \Psi}(t+1,t)\triangleq {\bf I}$ by convention, i.e., ${\bf \Psi}(r,t) = \pth{\bm{M}[t]\bm{M}[t-1]\cdots \bm{M}[1]}^{\top}$. 
% 
Notably, ${\bf M}^{\top}[\tau]$ is row-stochastic for each $\tau$ of interest. 
%Thus,
%\begin{align*}
%{\bf z}[t] &= {\bf z}[0] {\bf M}[1] \cdots {\bf M}[t] =  {\bf z}[0] {\bf \Psi}(1,t), \\
%{\bf w}[t] &={\bf w}[0] {\bf \Psi}(1,t).
%\end{align*}
%
% 
Without loss of generality, let us fix an arbitrary bijection   
%one-to-one mapping 
between $\{N+1, \cdots, \tilde{N}\}$ and $(j,j^{\prime})\in \calE_i$ for $i=1, \cdots, M$. For $j\in \calV_i$, we have 
%Thus, for each non-virtual agent $i\in \calV =\{1, \cdots, n\}$, we have
\begin{align}
%\nonumber
% original
% z_i[t] %&= \sum_{j=1}^m z_j[t-1] {\bf M}_{j^{\prime}j}[t]\\
% & = \sum_{j=1}^{m} z_j[0] {\bf \Psi}_{j^{\prime}j}(1,t) = \sum_{j=1}^{n} y_j {\bf \Psi}_{j^{\prime}j}(1,t),
z_j^{i}[t] %&= \sum_{j=1}^m z_j[t-1] {\bf M}_{j^{\prime}j}[t]\\
& = \sum_{j^{\prime}=1}^{\tilde{N}} z_{j^{\prime}}[0] {\bf \Psi}_{j^{\prime}j}(1,t) = \sum_{j^{\prime}=1}^{\tilde{N}} w_{j^{\prime}}^i {\bf \Psi}_{j^{\prime}j}(1,t),
%\nonumber
%&=\sum_{j=1}^m \pth{\sum_{k=1}^m z_k[t-2] {\bf M}_{kj}[t-1]+g_j[t-2]} {\bf M}_{j^{\prime}j}[t]\\
%\nonumber
%&=\sum_{j=1}^m \sum_{k=1}^m z_k[t-2]{\bf M}_{kj}[t-1]{\bf M}_{j^{\prime}j}[t]+\sum_{j=1}^mg_j[t-2] {\bf M}_{j^{\prime}j}[t]\\
%\nonumber
%&=\sum_{k=1}^m z_k[t-2] \pth{\sum_{j=1}^m  {\bf M}_{kj}[t-1]{\bf M}_{j^{\prime}j}[t]}+\sum_{j=1}^mg_j[t-2] {\bf M}_{j^{\prime}j}[t]\\
%\nonumber
%&=\sum_{k=1}^m z_k[t-2]{\bf \Psi}_{ki}(t-1, t)+\sum_{j=1}^mg_j[t-2] {\bf M}_{j^{\prime}j}[t]\\
%&=\sum_{r=0}^{t-1} \sum_{j=1}^m g_j[r]{\bf \Psi}_{j^{\prime}j}(r+2,t).
\label{evoz}
\end{align}
where the last equality holds due to $z_{j^{\prime}}[0] =w_j^i$ for $j\in \calV_i$ and $z_{j^{\prime}}[0]=0$ when $j^{\prime}$ corresponds to an edge, i.e., $j^{\prime}\notin \cup_{i=1}^M \calV_i$.
%
% Similar to \eqref{evoz}, for the weight evolution, for each $i\in \{1, \cdots, m\}$, we have
% \begin{align}
% w_j^{i}[t]%= \sum_{j=1}^m w_j[0] {\bf \Psi}_{j^{\prime}j}(1,t)
% =\sum_{j^{\prime}=1}^{n_i} w_{j^{\prime}}^{i}[0] {\bf \Psi}_{j^{\prime}j}(1,t),
% \label{evow}
% \end{align}
%where the last equality holds due to $w_i[0]=1$ for $i\in \calV$, and $w_i[0]=0$ otherwise.
%
%We investigate the convergence behavior of ${\bf \Psi}(r,t)$ (where $r\le t$) 
%Using ergodic coefficients \cite{Hajnal58},and some celebrated results obtained by Hajnal \cite{Hajnal58}, we show the following thoerem. 
\ls{Theorem statement to be revised. }


%
% 
% \section{Proof of Theorem \ref{rps convergence rate}}
% \label{app: proof of thm1}
We investigate the convergence behavior of ${\bf \Psi}(r,t)$ (where $r\le t$) using ergodic coefficients and some celebrated results obtained by Hajnal \cite{Hajnal58}.

Given a row stochastic matrix ${\bf A}$,
 coefficients of  ergodicity   $\delta({\bf A})$ and $\lambda({\bf A})$ are defined as:
\begin{align}
\delta({\bf A}) & \triangleq   \max_j ~ \max_{i_1,i_2}~\left | {\bf A}_{i_1 j}-{\bf A}_{i_2 j}\right |, \label{e_delta} \\
\lambda({\bf A}) & \triangleq   1 - \min_{i_1,i_2} \sum_j \min\{{\bf A}_{i_1 j}, {\bf A}_{i_2 j}\}. \label{e_lambda}
\end{align}
Informally speaking, the coefficients of ergodicity defined in \eqref{e_delta} and \eqref{e_lambda} characterize the ``difference" between any pair of rows of the given row-stochastic matrix ${\bf A}$. It is easy to see that  $0\leq \delta({\bf A}) \leq 1$, $0\leq \lambda({\bf A}) \leq 1$, and that the rows of ${\bf A}$ are identical if and only if $\delta({\bf A})=0=\lambda({\bf A})$. In addition, the ergodic coefficients $\delta(\cdot)$ and $\lambda(\cdot)$ have the following connection.
%
%The next result from \cite{Hajnal58} establishes a relation between the coefficient of ergodicity $\delta(\cdot)$ of a product of row stochastic matrices, and the coefficients of ergodicity $\lambda(\cdot)$ of the individual matrices defining the product.

\begin{proposition}\cite{Hajnal58}
\label{prop: claim_delta}
For any $p$ square row stochastic matrices ${\bf Q}[1],{\bf Q}[2],\dots {\bf Q}[p]$, it holds that
\begin{align}
\delta({\bf Q}[1]{\bf Q}[2]\ldots {\bf Q}[p]) ~\leq ~
 \Pi_{k=1}^p ~ \lambda({\bf Q}[k]).
\end{align}
\end{proposition}

Proposition \ref{prop: claim_delta} implies that if $\lambda({\bf Q}[k])\leq 1-c$ for some $c>0$ and for all $1\le k\le p$, then $\delta({\bf Q}[1],{\bf Q}[2]\cdots {\bf Q}[p])$ goes to zero exponentially fast as $p$ increases.

% 




% Next we show that, for sufficiently large $t$, it holds that $\lambda({\bf \Psi}(1,t))\le 1-\beta^{nB}$, where $\beta \triangleq \frac{1}{\max_{i\in \calV} (d_i^{o}+1)^2}$. 
% %
% Notably, when $t\mod \Gamma \not=0$, the matrix $\bm{M}^{\top}[t]$ is a block stochastic matrix with $M$ blocks, one for each network.  \ls{Provide an example of three sub-networks (with uneven sizes) assuming $B=1$ here. }
\begin{lemma}
\label{lm: block lower bound}
Suppose that $\Gamma \ge t-r+1\ge D_iB$ and $B\ge 1$. Then every entry in block $i$ of the matrix product ${\bf \Psi}(r, t)$ is lower bounded by $\beta_i^{D_iB}$, 
where $\beta_i = \frac{1}{\max_{j\in \calV_i} (d_j^{i}+1)^2}$ as per Theorem \ref{rps convergence rate}. 
\end{lemma}
% 
% 
\begin{proof}
Observe that 
\[
{\bf \Psi}(r, t) = {\bf M}^{\top}[r] {\bf M}^{\top}[r+1]\cdots {\bf M}^{\top}[t] = {\bf M}^{\top}[r] \cdots {\bf M}^{\top}[t-D_iB] {\bf M}^{\top}[t-D_iB+1] \cdots {\bf M}^{\top}[t].  
\]
Since $S^i$ is strongly connected and each link is reliable at least once during $B$ consecutive iterations, it is true that every entry in the $i$-th block of matrix product ${\bf M}^{\top}[t-D_iB+1] \cdots {\bf M}^{\top}[t]$ is lower bounded by $\beta_i^{D_iB}$. 

Moreover, each of the remained matrices in ${\bf \Psi}(r, t)$ is row-stochastic. Hence, every entry in block $i$ of the matrix product ${\bf \Psi}(r, t)$ is lower bounded by $\beta_i^{D_iB}$, proving the lemma. 
\end{proof}


\begin{lemma}
\label{lm: entire matrix lower bound}
Let $D^* := \max_{i\in [M]}D_i$. 
Choose $\Gamma = BD^*$. 
Suppose that $t-r+1\ge 2\Gamma$. Then every entry of the matrix product ${\bf \Psi}(r, t)$ is lower bounded by $\frac{1}{4M^2}\pth{\min_{i\in [M]}\beta_i}^{2D^*B}$. 
\end{lemma}
% 
%
\begin{proof}
By Lemma \ref{lm: block lower bound}, the construction of the fusion matrix $\bm{F}$, and the existence of self-loops. During consecutive $2\Gamma$ iterations, from any vertex $j^i$, we can reach any other vertex in the hierachical FL system. 
Thus, let $j^i$ and $j^{i^{\prime}}$ be two arbitrary vertices in subnetworks $i$ and $i^{\prime}$ respectively. It holds that 
\begin{align*}
 {\bf \Psi}_{j^i, j^{i^{\prime}}}(t-2\Gamma+1, t) &\ge   \frac{1}{2M} \beta_{i^{\prime}}^{D_{i^{\prime}}B}\frac{1}{2M} \beta_i^{D_iB} \ge \frac{1}{2M}\pth{\min_{i\in [M]}\beta_i}^{D^*B} \\
 & \ge \frac{1}{4M^2}\pth{\min_{i\in [M]}\beta_i}^{2D^*B}. 
\end{align*}

Since each matrix in the product ${\bf \Psi}(r, t)$ is row-stochastic, we conclude that every entry of the matrix product ${\bf \Psi}(r, t)$ is lower bounded by $\frac{1}{4M^2}\pth{\min_{i\in [M]}\beta_i}^{2D^*B}$. 

\end{proof}
% 
% 
\begin{lemma}
\label{lm: matrix block rate}
For $r\le t$ such that $\lfloor t/2\Gamma \rfloor - \lceil r/2\Gamma \rceil\ge 0$, it holds that $\delta\pth{{\bf \Psi}(r, t)}\le \gamma^{\lfloor t/2\Gamma \rfloor - \lceil r/2\Gamma \rceil},$
where $\gamma = 1-\frac{1}{4M^2} \pth{\min_{i\in [M]}\beta_i}^{2D^*B}$ as per Theorem \ref{rps convergence rate}. .
%\[\delta\pth{{\bf \Psi}(r, t)}\le \gamma^{\lfloor\frac{t-r}{2nB}\rfloor},~\text{for} r\le t.\]
\end{lemma}
{\red Henceforth, for ease of exposition, we adopt the simplification that 
\[
\lfloor t/2\Gamma \rfloor - \lceil r/2\Gamma \rceil =  (t-r)/2\Gamma. 
\]
In fact, such simplification does not affect the order of convergence rate. 
Exact expression can be recovered via following our current analysis while a straightforward bookkeeping of the floor and ceiling in the calculation.  
}
\begin{proof}
The following rewriting holds: 
\begin{align*}
{\bf \Psi}(r, t) & = {\bf \Psi}(r, \Gamma \lceil r/\Gamma\rceil) {\bf \Psi}(\Gamma\lceil r/\Gamma \rceil +1, \Gamma\lfloor t/\Gamma \rfloor) {\bf \Psi}(\Gamma\lfloor t/\Gamma \rfloor +1, \, t)\\
& = {\bf \Psi}(r, \Gamma \lceil r/\Gamma\rceil)\pth{ \prod_{k=\lceil r/\Gamma \rceil}^{\lfloor t/\Gamma \rfloor-1} {\bf \Psi}(k\Gamma+1, (k+1)\Gamma)}{\bf \Psi}(\Gamma\lfloor t/\Gamma \rfloor +1, \, t). 
\end{align*}

By Proposition \ref{prop: claim_delta}, we have 
\begin{align*}
\delta\pth{{\bf \Psi}(r, t)} &\le \lambda\pth{{\bf \Psi}(r, 2\Gamma \lceil r/2\Gamma\rceil)} \pth{\prod_{k=\lceil r/2\Gamma \rceil}^{\lfloor t/2\Gamma \rfloor-1} \lambda\pth{{\bf \Psi}(2k\Gamma+1, 2(k+1)\Gamma)}}\lambda\pth{{\bf \Psi}(2\Gamma\lfloor t/2\Gamma \rfloor +1, \, t)}  \\
& \le \prod_{k=\lceil r/2\Gamma \rceil}^{\lfloor t/2\Gamma \rfloor-1} \lambda\pth{{\bf \Psi}(2k\Gamma+1, 2(k+1)\Gamma)} \\
& \le \gamma^{\lfloor t/2\Gamma \rfloor - \lceil r/2\Gamma \rceil}, 
\end{align*}
where $\gamma := 1-\frac{1}{4M^2} \pth{\min_{i\in [M]}\beta_i}^{2D^*B}$. 
\end{proof}





\begin{theorem}
\label{rps convergence rate 2}
Under Algorithm \ref{alg:ps convergence rate}, at each agent $j\in \calV_i$ where $i=1, \cdots, M$, 
\begin{align*}
\norm{\frac{z_j^i[t]}{m_j^i[t]} -\frac{1}{N}\sum_{i=1}^M\sum_{j=1}^{n_i} w_j^i} \le  \frac{4M^2\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}\norm{w_{j^{\prime}}^i}}{\pth{\min_{i\in [M]}\beta_i}^{2D^*B}N}  \min\{1, \gamma^{\lfloor t/2\Gamma \rfloor - 1}\},
\end{align*}
where $\norm{\cdot}$ is the $\ell_2$ norm. 
\end{theorem}
\begin{proof}
% 
\begin{align*}
&\norm{\frac{z_j^i[t]}{m_j^i[t]} -\frac{1}{N}\sum_{i=1}^M\sum_{j=1}^{n_i} w_{j^{\prime}}^i}
= \norm{\frac{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}w_{j^{\prime}}^i{\bf \Psi}_{j^{\prime},j}(1,t)}{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)} -\frac{1}{N}\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i} w_{j^{\prime}}^i}\\  
& = \norm{\frac{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}w_{j^{\prime}}^i{\bf \Psi}_{j^{\prime},j}(1,t) 
- \sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i} w_{j^{\prime}}^i \sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)}{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)}} \\
& = \frac{\norm{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}w_{j^{\prime}}^i{\bf \Psi}_{j^{\prime},j}(1,t) 
- \sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i} w_{j^{\prime}}^i \sum_{i=1}^M\sum_{k=1}^{n_i}{\bf \Psi}_{k,j}(1,t)}}{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)} \\
& = \frac{\norm{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}w_{j^{\prime}}^iN{\bf \Psi}_{j^{\prime},j}(1,t) 
- \sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i} w_{j^{\prime}}^i \sum_{i=1}^M\sum_{k=1}^{n_i}{\bf \Psi}_{k,j}(1,t)}}{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)} \\
& = \frac{\norm{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}w_{j^{\prime}}^i\sum_{i=1}^M\sum_{k=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t) 
- \sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i} w_{j^{\prime}}^i \sum_{i=1}^M\sum_{k=1}^{n_i}{\bf \Psi}_{k,j}(1,t)}}{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)} \\
& = \frac{\norm{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}w_{j^{\prime}}^i\sum_{i=1}^M\sum_{k=1}^{n_i}\pth{{\bf \Psi}_{j^{\prime},j}(1,t) -{\bf \Psi}_{k,j}(1,t)}}}{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)} \\
& \le \frac{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}\norm{w_{j^{\prime}}^i}\sum_{i=1}^M\sum_{k=1}^{n_i}\abth{{\bf \Psi}_{j^{\prime},j}(1,t) -{\bf \Psi}_{k,j}(1,t)}}{N\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)}\\
& \le \frac{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}\norm{w_{j^{\prime}}^i}\delta\pth{{\bf \Psi}(1, t)}}{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)}. 
\end{align*}

By Lemma \ref{lm: matrix block rate}, we have 
\[
\delta\pth{{\bf \Psi}(1, t)}\le \min{1, \gamma^{\lfloor t/2\Gamma \rfloor - \lceil 1/2\Gamma \rceil}} = \min\{1, \gamma^{\lfloor t/2\Gamma \rfloor - 1}\}.
\]
% 
By Lemma \ref{lm: entire matrix lower bound}, it holds that 
\[
{\bf \Psi}_{j^{\prime},j}(1,t) \ge \frac{1}{4M^2}\pth{\min_{i\in [M]}\beta_i}^{2D^*B}. 
\]
Thus, 
\begin{align*}
 \frac{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}\norm{w_{j^{\prime}}^i}\delta\pth{{\bf \Psi}(1, t)}}{\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}{\bf \Psi}_{j^{\prime},j}(1,t)} \le \frac{4M^2\sum_{i=1}^M\sum_{j^{\prime}=1}^{n_i}\norm{w_{j^{\prime}}^i}}{\pth{\min_{i\in [M]}\beta_i}^{2D^*B}N}  \min\{1, \gamma^{\lfloor t/2\Gamma \rfloor - 1}\}. 
\end{align*}
\end{proof}

