For all experiments, the input point cloud is cropped at [($0, 51.2$), ($-25.6, 25.6$), ($-5, 10$)] meters along the $x$, $y$, $z$ axes, and a pillar grid size of $0.2m\times0.2m$ is used. For the traversability prediction network, the maximum number of points per pillar is set to ${N=20}$, and the channels of sparse and dense pillar features are set to ${C=128}$ and ${D=64}$, respectively. Each encoder and decoder has five layers, each consisting of a max pooling or transposed convolution layer and two $3 \times 3$ convolution layers, with a ReLU and a BatchNorm layer in the middle.

Our model is trained for $60$ epochs using the Adam optimizer with the outer loop learning rate of $\beta=3e^{-4}$ and a batch size of $16$. Each trajectory data within a batch comprises \textit{meta-train} data for the inner loop and \textit{meta-eval} data for the outer loop. Each meta-train data is composed of eight LiDAR point clouds and the ground-truth traversability costs, which are generated from the trajectory of the previous $M=8$ seconds from a reference time. The meta-eval data also consists of eight point clouds and the ground truth, but they are generated based on the trajectory of the upcoming $K=8$ seconds.  All network parameters are subject to adaptation and are updated $N_A=3$ times at a learning rate for inner loops of $\alpha=1e^{-4}$. During training, random horizontal flipping is applied with a probability of $50\%$, random rotation along the z-axis is applied between ($-\frac{\pi}{4},\frac{\pi}{4}$) radians, and random translation is applied $(-5, 5)$ meters in the $x$ and $y$ axes.