\documentclass{article}



\usepackage[preprint]{neurips_data_2023}




\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{xcolor}         %
\usepackage{multirow}


\usepackage{shortcuts}
\newcommand{\kw}[1]{\noindent{\textcolor{purple}{\{{\bf KW:} \em{#1}\}}}}
\newcommand{\yl}[1]{\noindent{\textcolor{red}{\{{\bf YL:} \em{#1}\}}}}
\newcommand{\jx}[1]{\noindent{\textcolor{cyan}{\{{\bf JX:} \em{#1}\}}}}
\newcommand{\ws}[1]{\noindent{\textcolor{orange}{\{{\bf WS:} \em{#1}\}}}}

\newcommand{\env}{\textsc{JoinGym}}
\newcommand{\Ntables}{N_{\text{tables}}}
\newcommand{\Ncols}{N_{\text{cols}}}
\newcommand{\sel}{\textrm{Sel}}

\title{JoinGym: An Efficient Query Optimization\\Environment for Reinforcement Learning}


\author{%
  \textbf{Kaiwen Wang}$^{*}$ \quad \textbf{Junxiong Wang}$^{*}$ \quad \textbf{Yueying Li} \quad \textbf{Nathan Kallus} \\
  \textbf{Immanuel Trummer} \quad \textbf{Wen Sun} \\
\\
\texttt{\{kw437,~jw2544,~yl3469,~kallus,~it224,~ws455\}@cornell.edu}
}

\begin{document}

\maketitle
\def\thefootnote{*}\footnotetext{Equal contribution.}\def\thefootnote{\arabic{footnote}}

\begin{abstract}
  In this paper, we present \textsc{JoinGym}, an efficient and lightweight query optimization environment for reinforcement learning (RL). 
  Join order selection (JOS) is a classic NP-hard combinatorial optimization problem from database query optimization and can serve as a practical testbed for the generalization capabilities of RL algorithms. 
  We describe how to formulate each of the left-deep and bushy variants of the JOS problem as a Markov Decision Process (MDP), and we provide an implementation adhering to the standard Gymnasium API.
  We highlight that our implementation \textsc{JoinGym} is completely based on offline traces of all possible joins, which enables RL practitioners to easily and quickly test their methods on a realistic data management problem without needing to setup any systems. 
  Moreover, we also provide all possible join traces on $3300$ novel SQL queries generated from the IMDB dataset.
  Upon benchmarking popular RL algorithms, we find that at least one method can obtain near-optimal performance on train-set queries but their performance degrades by several orders of magnitude on test-set queries. This gap motivates further research for RL algorithms that generalize well in multi-task combinatorial optimization problems.
\end{abstract}

\section{Introduction}
Deep reinforcement learning (RL) has achieved many successes, including video games \citep{bellemare2013arcade,cobbe2020leveraging}, robotic simulators \citep{tassa2018deepmind} and even combinatorial problems such as boolean satisfiability \citep{kurin2020can} and the traveling salesman problem \citep{miki2018applying}.
In database query optimization (DBQO), join order selection (JOS; a.k.a. join order optimization or access path selection) is the problem of finding a query execution plan with minimal cost.
Given its practical importance, this NP-hard combinatorial optimization problem has seen substantial interest and approaches based both on combinatorial optimization \citep{selinger1979access, chandra1980structure, vardi1982complexity, cosmadakis1988decidable} and RL \citep{marcus2019neo,yang2022balsa,marcus2018deep,krishnan2018learning}.
In this work, we present \env{}, an easy-to-use and lightweight
JOS environment
to evaluate the genearlization capabilities of RL models in realistic settings and to facilitate research on RL approaches to JOS and to solving combinatorial problems more generally.
\env{} supports both the left-deep and bushy variants of the JOS problem, and it also has a toggle for disabling Cartesian products.
These four choices give researchers the flexibility of changing the problem difficulty and the size of the search space.


The main advantages of \env{} and our contributions are as follows.
First, \env{} is easy to setup (simply import \env{} and call \texttt{gym.make()}) and is lightweight (can perform thousands of steps per second). 
Previous RL environments for systems such as Park \citep{mao2019park} require installing and setting up a database management system (DBMS), which is not ideal for researchers who want to quickly test their algorithms.
Moreover, Park's database environment actually performs live database queries, which can be time-consuming (a bad join order can take days to execute) and expensive (compute nodes can cost thousands of dollars), slowing down the RL experimentation process.
In contrast, \env{} uses an offline dataset of pre-recorded intermediate result (IR) cardinalities and simply performs lookups when rolling out a trajectory. While collecting the offline dataset is expensive, it only needs to be done once and there are already available datasets such as the Join Order Benchmark (JOB) \citep{leis2015good}, which contains $113$ queries. 
\env{} focuses on the IR cardinalities since it is an accurate proxy of execution time both empirically \citep{leis2015good} and theoretically \citep{cluet1995complexity}, and predicting IR cardinalities is the key challenge of DBQO \citep{Yang:EECS-2022-194}. 
Thus, by using offline IR cardinality data, \env{} is convenient and lightweight while capturing the key essence of the JOS problem.


Second, \env{} provides more diverse and challenging queries that complement JOB, the \emph{de facto} standard for testing JOS algorithms.
The primary drawback of the JOB dataset is that it only contains $113$ queries, which may not be enough at capturing the generalization challenges of JOS. 
Using $33$ query templates from JOB, we collected $100$ randomly generated offline traces for each template, resulting in $3300$ new queries that reflect diverse user movie interests.
Our dataset is challenging because the optimal plan for queries from the same template can vary largely due to different selectivities of the base tables, and these additional queries can serve as a better testbed for the generalization capabilities of RL algorithms. 
Curiously, we find that our additional queries may contain more unmeasured confounding than the JOB dataset, which adds another layer of complexity that was previously unexplored for combinatorial optimization with RL. 

Third, we extensively benchmark both online and offline RL algorithms on all four variants of \env{}.
We find that while RL algorithms can typically learn a near-optimal join policy on the training set, their performance often degrades by an order of magnitude on the testing set of queries. We note that the learned policies are still better than popular non-RL heuristic approaches from modern database systems. 
This shows that generalization in multi-task RL remains a challenge for deploying existing algorithms to real-world combinatorial optimization problems, and developing better multi-task algorithms for combinatorial optimization is a promising direction for future work. 
\emph{We will make all the queries and code available soon.}











\section{Related Works}

\textbf{RL Environments for Systems. } 
\citet{mao2019park} proposed the Park platform for experimenting RL in a wide range of computer systems tasks, including the JOS problem. 
While many of its environments are easy to use, their JOS environment requires pre-installing complex database management systems and executes queries online, which prohibits rapid development of RL algorithms. 
Moreover, Park also implements its own abstraction which is not compatible with the standard Gymnasium API, and its open-source repository does not include the implementation of RL algorithms. There are also other RL environments for systems, \eg, AutoCat to simulate cache-timing attacks \citep{10070947}. 
We focus on the JOS problem and provide an easy-to-use and efficient environment with a simulator. 

\textbf{Join Order Selection. } The applications of database systems are ubiquitous, \eg, search engines and online transactions, which crucially hinge on the ability to execute queries efficiently and at a massive scale \citep{selinger1979access, chaudhuri1998overview}. 
The majority of research in query optimization focuses on optimizing the selection join order \citep{leis2015good} since it can result in many orders of magnitude differences in execution time for the same query. Other optimizations (e.g., selecting indexes, materialized views, etc.) often use query optimization as a sub-function. If query optimization fails to provide accurate plans, it can cause confusion for other auto-tuning methods \citep{marcus2021bao,gunasekaran2023deep}. 

Modern database systems simply assume the correlation of data distribution to estimate the cardinality of intermediate results, thus leading to query plan which performs many orders of magnitude worse compared with optimal plans. Yet, ``join order selection'' optimization is still an unsolved problem \citep{lohman2014query}. Recently, \citet{yang2022balsa,marcus2019neo,krishnan2018learning} leverage deep RL models to address this problem. However, most of them are tested on the JOB, which consists of only 33 query templates and 113 queries. In JOB, queries from the same template have very similar optimal query plans, which makes generalization much easier.  











\textbf{Environments for multi-task RL and generalization. }
CoinRun \citep{cobbe2019quantifying} and Procgen \citep{cobbe2020leveraging} use procedural generation to create video games similar to Atari but can be generated with different seeds to test the generalization capabilities of RL algorithms.
For more real-world applications, Meta-World \citep{yu2020meta} is a multi-task and meta-learning benchmark for robotics, consisting of fifty distinct robotic manipulation tasks. FinRL-Meta \citep{liu2022finrl} is such a benchmark for trading and finance. It contains three market environments, \ie, stock trading, portfolio allocation, and crypto trading, and two data sources, \ie, Yahoo Finance and WRDS.
Our work, \env{}, is a multi-task benchmark for JOS with applications in database systems, where each query can be viewed as a separate task. 
















\section{Database Query Optimization Preliminaries}

A database consists of $\Ntables$ tables, $\texttt{DB} = \braces{T_1,T_2,\dots,T_{\Ntables}}$, 
where each table $T_i$ has a set of $\Ncols(T_i)$ columns, $\text{Cols}(T_i) = \braces{C_{i,1},C_{i,2},\dots,C_{i,\Ncols(T_i)}}$.
For the purposes of the DBQO, a SQL query, $q = (I,U,J)$, can be abstracted into three parts. 
First, $I=\{i_1,\dots,i_{|I|}\}\subset [\Ntables]$ specifies the tables $\texttt{DB}[I]$ needed for executing this query.
Second, $U=\braces{u_i}_{i\in I}$ is a set of unary \emph{filter predicates} such that for each $i\in I$, a filtered table $\wt T_i = u_i(T_i)$ 
is produced from keeping the rows of $T_i$ that satisfy the filter predicate $u_i$. 
The fraction of rows in $T_i$ that satisfy $u_i$ is defined as the \emph{selectivity}, $\sel_i = \nicefrac{|\wt T_i|}{|T_i|} \in [0,1]$.
Third, $J=\{P_{i_1i_2}\}_{i_1\neq i_2\in I}$ is a set of binary \emph{join predicates}, each specifying which columns should have matching values between any two tables. 

Given two tables $R$ and $S$ and join predicates $P\subset [\Ncols(R)]\times [\Ncols(S)]$, define their binary join as
\begin{align}
R\Join_P S
= \braces{ r \cup s \mid r \in R, s \in S, r_a = s_b\; \forall (a,b)\in P }, \label{eq:def-join}
\end{align}
where $r\cup s$ means concatenating rows $r$ and $s$, and $r_a = s_b$ stipulates that the $a$-th column of $r$ matches the $b$-th column of $s$ in value. 
Letting $\overline P=\{(b,a):(a,b)\in P\}$, we restrict $P_{i_1i_2}=\overline{P_{i_2i_1}}$.

The \emph{result} of $q$ is $\tilde T_{i_1}\Join_{P_{i_1,i_2}\cup\cdots\cup P_{i_1,i_{|I|}}}(\tilde T_{i_2}\Join_{P_{i_2,i_3}\cup\cdots\cup P_{i_2,i_{|I|}}}\cdots(\tilde T_{i_{|I|-1}}\Join_{P_{i_{|I|-1},i_{|I|}}}\tilde T_{i_{|I|}}))$. 
However, there are different ways to execute the same query as different sequences of binary joins.
For example, if $q=(\{1,2,3,4\},\{u_1,u_2,u_3,u_4\},\{P_{1,2},P_{1,3},\dots\})$, two different plans to execute the same query would be $\tilde T_1\Join_{P_{1,2}\cup P_{1,3}\cup P_{1,4}} (\tilde T_2 \Join_{P_{2,3}\cup P_{2,4}} (\tilde T_3\Join_{P_{3,4}}\tilde T_4))$ and $(\tilde T_1\Join_{P_{1,3}} \tilde T_3)\Join_{P_{1,2}\cup P_{1,4}\cup P_{3,2}\cup P_{3,4}} (\tilde T_2\Join_{P_{2,4}} \tilde T_4)$. The former involves the \emph{intermediate results} (IRs) $\mathrm{IR}_1=\tilde T_3\Join_{P_{3,4}}\tilde T_4$ and $\mathrm{IR}_2=\tilde T_2 \Join_{P_{2,3}\cup P_{2,4}} \mathrm{IR}_1$, while the latter involves the IRs $\mathrm{IR}_1=\tilde T_1\Join_{P_{1,3}} \tilde T_3$ and $\mathrm{IR}_2=\tilde T_2\Join_{P_{2,4}} \tilde T_4$.
The IRs in each of these plans can have drastically different cardinalities and the plans can therefore have drastically different run-times and computational costs \citep{ramakrishnan2003database}.
The IR cardinality generally depends on the selectivity of base tables and the correlation of joined columns. 
We define JOS as the problem of minimizing the total size of all IRs involved in a join order plan over all feasible join order plans.

\subsection{Left-Deep vs. Bushy Plans}
Every join order can be represented as a binary tree, where its leaves are the (filtered) base tables $\wt T_i$ and each node represents the IR from joining its two children using the predicates of .
The database literature distinguishes between two types of join plans: \emph{bushy} and \emph{left-deep} \citep{leis2015good}.
As the name suggests, left-deep plans only consider left-deep binary trees, which only cumulatively joins base tables with the IR of cumulative joins so far.
In contrast, bushy plans allow for any possible binary tree; \eg, joining two non-base-table IRs is allowed in bushy plans but disallowed in left-deep plans. 
With $|I|$ tables, there are $|I|!$ possible left-deep plans and $|I|!C_{|I|-1}$ bushy plans, where the $k$-th Catalan number $C_{k}$ is the number of unlabeled binary trees with $k+1$ leafs.
Thus, with either plan type, JOS has a super-exponential search space (see the middle of \cref{fig:mdp_statistics}), and, as \citet{ibaraki1984optimal} showed, is NP-hard. 
While left-deep plans are usually sufficient for obtaining a fast enough join plan, in some cases, bushy plans can yield join plans with smaller cumulative IR sizes and run-time at the cost of a much larger search space \citep{leis2015good}. 

\subsection{Disabling Cartesian Products}\label{sec:disable-cp}
Disabling (\ie, avoiding) \emph{Cartesian products} (CPs) has been a popular heuristic in most modern database systems to trade off optimality for a smaller search space.
CPs are expensive join operations where no constraints are placed on the column values, \ie, the CP between $R$ and $S$ is $R\Join_{\emptyset} S = \braces{r\cup s\mid r\in R, s\in S}$, which always has
cardinality equal to the product of the two source tables' cardinalities.
CP is indeed the most expensive binary join, but CPs as IRs can actually sometimes lead to smaller cumulative costs \citep{vance1996rapid}.
For example, it may be favorable to take the CP between two small tables before joining with a large table. 
Many database systems myopically avoid CPs, which further reduces the search space \citep{ramakrishnan2003database}.

\subsection{The Role of Generalization in Query Optimization}
In real database applications, developers usually create \emph{query templates} for each workload, such that each user search executes an instance of the template with potentially different filter predicates that reflect the user's interests. See \cref{fig:query_plan} (a) \& (b) for an example query template and its instantiations. 
A query's template determines its (final) query graph and different query templates may often share common subgraphs. By comprehending the underlying structure of these query graphs, deep learning models can \emph{generalize} across different templates to enhance future query execution planning; for example in \cref{fig:query_plan}, the optimal join plan for (b) is a sub-tree of the optimal plan for (c).
However, while queries with the same template have a common query graph, optimal join orders can nonetheless vary significantly due to the different filter conditions of the query, \eg, \cref{fig:query_plan} (a) \& (b) share the same template (and hence query graph) but have different optimal join orders.
Thus, the key challenge in learning query optimization is to learn correct query instances to mimic based on filter predicates and query graph structures for both unseen query instances (\ie, generalization within a template) and even unseen query templates (\ie, generalization across templates) at test time.






% Figure environment removed




\section{\env{}: A Database Query Optimization Environment}
We first formulate JOS as a Contextual Markov Decision Process (CMDP) \citep{modi2018markov} and describe how we encode the context and observation, which we implement with \env{}.

\subsection{Contextual MDP Formulation}
Selecting the join order naturally fits into the RL framework, since it is a sequence of actions (joins) and per-step costs (IR sizes) with the goal of minimizing the cumulative cost. We now formalize this problem as CMDP, consisting of context space $\Xcal$, state space $\Scal$, finite action space $\Acal$ of size $A$, horizon $H$, transition kernels $P_h(s'\mid s,a)$, and context-dependent reward functions $r_h(s,a; x)$, for $s,a,x\in\Scal\times\Acal\times\Xcal$. 
Each trajectory corresponds to a plan for a query $q=(I,U,J)$ and is rolled out as follows (and summarized in \cref{tab:compare-left-bushy}). First, the context $x$ is an encoding of the query $q$, and is fixed throughout the trajectory. 
At time $h\in[H]$, the state $s_h$ represents the partial join plan that has been executed so far, and the action $a_h$ specifies the join operation to perform at time $h$. 
For bushy plans, $a_h$ can be any valid edge in the join graph; for left-deep plans, $a_h$ is simply the $h$-th table to add to the left-deep tree.
Given $s_h,a_h$, the partial join plan representation is updated.
Given $x,s_h,a_h$, the agent receives a cost $c_h$ equal to the size of the IR from the join specified by $a_h$. We define the reward $r_h\propto\nicefrac{C^\star_{\text{plan\_type}}}{H}-c_h$ as the negative of the step-wise instantaneous regret, where $C^\star_{\text{method}}$ is the minimum cumulative cost, for $\text{plan\_type}\in \braces{ \text{bushy}, \text{left-deep} }$. 
For numerical stability, we both normalize our reward and clip each $c_h$ by $100 C^\star_{\text{plan\_type}}$, which is a query-dependent constant.
While each $r_h$ can be either negative or positive, the cumulative reward is non-positive, with zero cumulative reward being optimal. 
This procedure iterates until all queried tables are joined.
For bushy plans, the horizon is the number of joins, \ie, $H=|J|=|I|-1$; for left-deep plans, the $a_1$ corresponds to staging the first table, which does not perform any joins, and so $r_1 = 0$ and $H=|J|+1=|I|$. 


We now make a few remarks about this CMDP.
First, unlike the general CMDP of \citet{modi2018markov}, our transition kernel is independent of the context.
Second, since the agent cannot choose tables or edges that have already been joined in previous steps and edges between different base tables in two IRs are equivalent actions, the set of available actions is shrinking, \ie, $\Acal=\Acal_1\supset\Acal_2\supset\dots\supset\Acal_H$. We handle this by action masking \citep{huang2022closer}, where we appropriately constrain the agent's update and action selection rules so that the agent can only take the available actions at each step.\footnote{Another approach would be to give a large cost for choosing unavailable actions, but this would require the agent to learn to avoid such actions. We avoid this unnecessary learning with action masking.}
The third characteristic is that transition and reward dynamics are \emph{deterministic}; there is however stochasticity in the context, as queries may be sampled from some distribution. 
Finally, like \citet{yang2022balsa}, we assume the DB content is kept static.
However, in real applications, tables may be updated in ways that change their data distributions.
Robust RL \citep{kallus2022doubly,panaganti2022robust} or transfer RL \citep{finn2017model,agarwal2022provable} may be used to adapt and quickly recover from these distribution shifts.

% Figure environment removed

\textbf{Statistics of the CMDP. } 
\cref{fig:mdp_statistics} illustrates the statistics of \env{}. The left figure displays the distribution of the number of join tables in \env{}. More than half of the queries join at least nine tables. The middle figure illustrates the exponential growth of two different search spaces (\ie, left-deep search space and bushy search space) as the number of join tables increases. The bushy search space exhibits even faster growth compared to the left-deep plans. In our most challenging query template, search space exceeds $10^{15}$ for left-deep plans and surpasses $10^{23}$ for bushy plans. The right figure shows the sorted cumulative IR sizes of different join orders for the same representative query. The left-most point is the optimal plan. The sharp jump in costs from the optimal illustrates the significant challenge of \env{}.


\subsection{Context and State Encoding} \label{sec:state-encoding}

\label{sec:joingym}
% Figure environment removed 

Given a query $q$, we represent the context as $x = \prns{ v^{\sel}(q), v^{\text{goal}}(q)}$ as the concatenation of two components that are static throughout the trajectory.
The first part is the \emph{selectivity encoding}, a vector $v^{\sel}(q)\in [0,1]^{\Ntables}$ where the $t$-th entry is the selectivity of $u_t$ if $t\in I$, and $0$ otherwise:
\begin{align}
    &\forall t\in[\Ntables]: v^{\sel}(q)[t] = 
    \begin{cases}
        \sel_t, &\text{ if } t\in I, \\
        0, &\text{ o/w }.
            \end{cases}\notag %
\end{align}
The second part is the \emph{query encoding}, a binary vector $v^{\text{goal}}(q)\in\braces{0,1}^{\Ncols}$ (abusing notation, we define $\Ncols = \sum_{T}\Ncols(T)$) representing which columns need to be joined in this query; the $c$-th entry is $1$ if column $c$ appeared in any join predicate, and $0$ otherwise:
\begin{align}
    &\forall c\in[\Ncols]: v^{\text{goal}}(q)[c] = \I{ \exists (R,S,P)\in J, \exists p\in P: c=p[0]\vee c=p[1] }. \notag%
\end{align}

At time $h\in[H]$, we represent the state $s_h$ as the as \emph{partial plan encoding} $v^{\text{pp}}_h$, which represents the joins specified by prior actions $a_{1:h-1}$. Two examples are given in \cref{fig:trajectories}(d). 
The partial plan encoding is a vector $v^{\text{pp}}_h\in\braces{-1,0,1,2,\dots}^{\Ncols}$ where the $c$-th entry is positive if column $c$ has already been joined, $-1$ if the table of column $c$ has been joined or selected but column $c$ has not been joined yet, and $0$ otherwise.
If column $c$ has already been joined, the $c$-th entry will be the index of its join-tree in the forest; in left-deep plans, the index will always be $1$ since there is always only one join-tree, but in bushy plans with more than one tree, the index can be larger than $1$.
It is clearly important to mark joined columns (with positive numbers) since the policy needs to know which columns have been joined to choose the next action. 
In addition, we also mark unjoined columns belonging to joined or selected tables with the value $-1$. For example, in left-deep plans, we must be able to tell which table was selected by $a_1$ at $h=2$, even though said table has only been ``staged'' but not joined.
Beyond this special case, another use-case of the $-1$-marking is that it signals marked columns as part of potentially small IR; perhaps the rows of the table has been filtered from a prior join and it is better to join with this table rather than an unjoined base table.
We highlight that the partial plan encoding only logs which columns have been joined/staged rather than the current join tree, because future costs only depend on the current IRs rather than the tree structure.



We end this section with a caveat: our CMDP is strictly speaking a partially observable MDP (POMDP) because the state contains no information about the data of each table, which can influence the IR sizes and hence the reward function. In fact, almost all modern database systems are oblivious to the data distribution and often assume that data in columns are uniformly distributed.
While it is unclear what is the best way to compress tables into meaningful embeddings for JOS, our environment can be easily adapted to handle new query encodings.



\begin{table*}[t]
\centering
\def\arraystretch{1.5}%
\footnotesize
\begin{tabular} {|l|c|c|} 
\hline
Components & Left-deep \env{} & Bushy \env{} \\
\hline
Context $x$ & \multicolumn{2}{c|}{ Query encoding described in \cref{sec:state-encoding}. } \\
\hline
State $s_h$ & \multicolumn{2}{c|}{ Partial plan encoding described in \cref{sec:state-encoding}. } \\
\hline
Action $a_h$ & Table to join, from Discrete($\Ntables$) & Edge to join, from Discrete($\binom{\Ntables}{2}$) \\
\hline
Reward $r_h$ & \multicolumn{2}{c}{ Negative step-wise regret: $r_h\propto\nicefrac{C^\star_{\text{plan\_type}}}{H}-c_h$ for $\text{plan\_type}\in \braces{ \text{left-deep},\text{bushy} }$ } \vline \\
\hline
Transition $P_h$ & \multicolumn{2}{c}{ Deterministic transition of dynamic state features, described in \cref{sec:state-encoding}. } \vline \\
\hline
Horizon $H$ & $|I|$ & $|I|-1$ \\
\hline
\end{tabular}
\caption{Components of \env{} for deciding the join order of a single query $q=(I,U,J)$. Between the left-deep and bushy variants, the key difference is the actions (table vs edge). }
\label{tab:compare-left-bushy}
\vspace{-0.15in}
\end{table*}


\subsection{\env{} API}
\env{} adheres to the standard Gymnasium API \citep{gymnasium}, a maintained fork of OpenAI Gym \citep{brockman2016openai}. 
The left-deep and bushy variants are registered under the environment-ids \texttt{join\_optimization\_left-v0} and \texttt{join\_optimization\_bushy-v0} respectively and can be instantiated with \texttt{env = gym.make(env\_id, disable\_cartesian\_product)}, where \texttt{disable\_cartesian\_product} is a Boolean flag for the ``disable CP'' heuristic (described in \cref{sec:disable-cp}).
Following the Gymnasium API, \env{} implements the following two methods that operationalize the abstract MDP described in \cref{sec:joingym}.
\begin{enumerate}[label=(\roman*),leftmargin=0.7cm]
    \item \texttt{state, info = env.reset(options=\{query\_id=x\})}: reset the trajectory to represent the query with id \texttt{x}, and observe the initial state.
    \item \texttt{next\_state, reward, done, \_, info = env.step(action)}: perform join specified by \texttt{action}, and observe the next state, reward, and \texttt{done} which is \texttt{True} if and only if all tables of the current query are joined. 
    Our environment does not truncate so we ignore the fourth output.
\end{enumerate}
Above, \texttt{state} \& \texttt{next\_state} are encoded as in \cref{sec:state-encoding}.
Moreover, \texttt{info[``action\_mask'']} is a multi-hot encoding (\ie, MultiBinary) of the possible actions $\Acal_h$ at the current step. The RL algorithm should make use of this information to learn and act only from valid actions. 
In our code, we provide examples on how to do this for all algorithms tested in \cref{sec:online-exps}.


\section{Benchmarking Online and Offline Reinforcement Learning}\label{sec:online-exps}
In this section, we benchmark some standard RL algorithms on \env{} to explore several capabilities in combinatorial optimization and in generalization.

\textbf{Environment, Dataset and Algorithms: }
We experimented on both left-deep and bushy variants of \env{}, as well as disabling and enabling CPs. We tested on queries from the JOB and from the new query dataset that we collected.
The JOB only has $113$ queries, so we picked one from each of the $33$ query templates for validation and testing, and the $47$ remaining queries were used for training. Our new dataset contains $100$ queries for each of the $33$ query templates, and we picked $15/3/2$ random queries from each template for our training/validation/testing query sets. 
We benchmarked four different RL algorithms: (i) an off-policy Q-learning algorithm Deep Q-Network (DQN) \citep{mnih2015human}; (ii-iii) two off-policy actor-critic algorithms, Twin Delayed Deep Deterministic policy gradient (TD3) \citep{fujimoto2018addressing} and Soft Actor-Critic (SAC) \citep{haarnoja2018soft}; and (iv) an on-policy actor-critic algorithm Proximal Policy Optimization (PPO) \citep{schulman2017proximal}.
For DQN, we conducted an ablation with the Double DQN \citep{van2016deep}. 
For (i-iii), we conducted ablations with standard replay buffer (RB) and prioritized experience replay (PER) \citep{schaul2015prioritized}.\footnote{All algorithms were trained on one million environment steps with the training queries. In \cref{tab:new-data-results}, we report the best-performing policy's cumulative cost multiple with respect to the average validation set performance.} Due to space constraints, we present offline RL results in the appendix. 

\textbf{Evaluation Metric: } For a single query, the \emph{cumulative cost multiple} is defined as the cumulative IR size of the policy divided by the optimal cumulative IR size for the query under the specified join plan type; lower is better and we report the average performance over the train/val/test sets in \cref{tab:new-data-results}.
\cref{fig:learning_curve} shows the learning curve of the best-performing PPO run for left-deep plans with CPs disabled on our new data. We will report a table with standard error and discuss our hyperparameter choices in the appendix. 

\textbf{Discussions: }
\textbf{1)} When CPs are disabled, the search space is smaller and most algorithms have better performance than when CPs are enabled (which has the largest search space). 
\textbf{2)} Algorithms tend to perform better in the left-deep environment. In every setting except ``enable CP \& bushy'', there is at least one algorithm that attains near-optimal training set performance. 
\textbf{3)} In the JOB dataset, the test set's cost is $2\text{-}3\times$ that of the validation set. However, we find that generalization is worse in our new dataset, where the ``disable CP \& bushy'' and ``enable CP \& left'' settings have test set performance $100\times$ that of the validation. 
This suggests that our new dataset is harder for RL algorithms as the partial observability issue may be more pronounced; the unmeasured confounder may be mild in JOB since queries with the same template tend to have similar optimal plans, while this is not the case in our dataset since we generated queries randomly.


\begin{table*}[!t]
\centering
\def\arraystretch{1.2}%
\scriptsize
\begin{tabular}{|l|l|l|r|r|r|r|r|r|r|r|r|}
\hline
 \multicolumn{3}{|c|}{\textbf{JOB Data}} & \multicolumn{2}{|c|}{DQN} & \multicolumn{2}{c|}{DDQN} & \multicolumn{2}{c|}{TD3} & \multicolumn{2}{c|}{SAC} & PPO \\
\hline
 &  &  & RB & PER & RB & PER & RB & PER & RB & PER &  \\
\hline
\multirow[c]{6}{*}{\rotatebox{90}{disable CP}} & \multirow[c]{3}{*}{\rotatebox{90}{bushy}} & trn & 1.8 & 1.8 & 2.9 & 1.9 & 2.4 & 1.7 & 1.6e+02 & 7.6 & \bfseries \color{violet} 1.2 \\
\cline{3-12}
 &  & val & 25 & 26 & 38 & 68 & 24 & 23 & 1.5e+02 & 69 & \bfseries \color{violet} 16 \\
\cline{3-12}
 &  & tst & 4.3e+02 & \bfseries \color{violet} 41 & 4.3e+02 & 2e+02 & 4.2e+02 & 2.9e+02 & 5.6e+02 & 4.6e+02 & 1.5e+02 \\
\cline{2-12} \cline{3-12}
 & \multirow[c]{3}{*}{\rotatebox{90}{left}} & trn & 1.9 & 4.4 & \bfseries \color{violet} 1.8 & 2.1 & 4.4 & 6.1 & 2.8 & 3.6 & 2.8 \\
\cline{3-12}
 &  & val & 23 & 42 & \bfseries \color{violet} 12 & 18 & 25 & 24 & 16 & 61 & 15 \\
\cline{3-12}
 &  & tst & 55 & 44 & \bfseries \color{violet} 18 & 60 & 25 & 27 & 1.7e+03 & 70 & 1.7e+03 \\
\cline{1-12} \cline{2-12} \cline{3-12}
\multirow[c]{6}{*}{\rotatebox{90}{enable CP}} & \multirow[c]{3}{*}{\rotatebox{90}{bushy}} & trn & 1.4e+02 & 7.3e+08 & 2.7e+10 & 3e+04 & 15 & 1.6e+02 & 1.3e+07 & 7.1e+08 & \bfseries \color{violet} 2.7 \\
\cline{3-12}
 &  & val & 1.9e+12 & 1.8e+12 & 2.9e+10 & 4.9e+06 & 3.4e+02 & 4.6e+02 & 1.9e+07 & 2.7e+09 & \bfseries \color{violet} 1.2e+02 \\
\cline{3-12}
 &  & tst & 1.5e+15 & 1.3e+12 & 1.5e+15 & 3.8e+06 & 5.3e+02 & 7.4e+02 & 1.6e+07 & 1.6e+16 & \bfseries \color{violet} 2.3e+02 \\
\cline{2-12} \cline{3-12}
 & \multirow[c]{3}{*}{\rotatebox{90}{left}} & trn & \bfseries \color{violet} 1.6 & 4 & 2.9 & 3.3 & 12 & 15 & 3.3 & 2.9 & 6.8e+02 \\
\cline{3-12}
 &  & val & 5.1e+03 & 5.2e+03 & 76 & 21 & 5.3e+03 & 1.1e+02 & 88 & \bfseries \color{violet} 9.7 & 5.1e+02 \\
\cline{3-12}
 &  & tst & 6e+08 & 6e+08 & 1.8e+03 & \bfseries \color{violet} 27 & 6e+08 & 6.7e+02 & 1.8e+03 & 1.6e+03 & 2.9e+03 \\
\cline{1-12} \cline{2-12} \cline{3-12}
\hline
\end{tabular}
\begin{tabular}{|l|l|l|r|r|r|r|r|r|r|r|r|}
\hline
 \multicolumn{3}{|c|}{\textbf{Our New Data}}  & \multicolumn{2}{c|}{DQN} & \multicolumn{2}{c|}{DDQN} & \multicolumn{2}{c|}{TD3} & \multicolumn{2}{c|}{SAC} & PPO \\
\hline
 &  &  & RB & PER & RB & PER & RB & PER & RB & PER &  \\
\hline
\multirow[c]{6}{*}{\rotatebox{90}{disable CP}} & \multirow[c]{3}{*}{\rotatebox{90}{bushy}} & trn & \bfseries \color{violet} 1.1 & 1.3 & 1.2 & 9.6 & 1.3e+02 & 1.5e+02 & 1.6 & 1.4 & 73 \\
\cline{3-12}
 &  & val & 30 & 3.7e+02 & 37 & 1e+02 & 2.4e+02 & 3e+03 & \bfseries \color{violet} 28 & 2.4e+03 & 3.3e+03 \\
\cline{3-12}
 &  & tst & 1.1e+05 & 1.4e+05 & 6.4e+02 & 5.9e+02 & 5.5e+03 & 8.5e+03 & \bfseries \color{violet} 5.3e+02 & 9.7e+03 & 3e+03 \\
\cline{2-12} \cline{3-12}
 & \multirow[c]{3}{*}{\rotatebox{90}{left}} & trn & 1.3 & \bfseries \color{violet} 1 & 1.2 & 1 & 4e+02 & 4.5e+02 & 3.2 & 3.9 & 2.4 \\
\cline{3-12}
 &  & val & 65 & 2.3e+02 & 53 & 1.6e+02 & 4.1e+02 & 4.3e+02 & 53 & 32 & \bfseries \color{violet} 29 \\
\cline{3-12}
 &  & tst & 6.9e+02 & 2.6e+02 & 6.6e+02 & 2.8e+02 & 4.2e+02 & 1e+03 & 6.8e+02 & 7.7e+03 & \bfseries \color{violet} 62 \\
\cline{1-12} \cline{2-12} \cline{3-12}
\multirow[c]{6}{*}{\rotatebox{90}{enable CP}} & \multirow[c]{3}{*}{\rotatebox{90}{bushy}} & trn & 6.2e+10 & 4.2e+09 & 3.4e+09 & 9.4e+16 & 2e+05 & 6.6e+03 & 9e+31 & 1.4e+34 & \bfseries \color{violet} 1.4e+03 \\
\cline{3-12}
 &  & val & 6.3e+23 & 6.3e+20 & 6e+24 & 1.1e+21 & 1.7e+05 & 1.1e+05 & 9.8e+31 & 6.7e+34 & \bfseries \color{violet} 3.4e+04 \\
\cline{3-12}
 &  & tst & inf & 2.1e+21 & 4.9e+33 & 8.8e+23 & 1.6e+05 & 1e+05 & 8.3e+32 & 6.2e+34 & \bfseries \color{violet} 3.3e+04 \\
\cline{2-12} \cline{3-12}
 & \multirow[c]{3}{*}{\rotatebox{90}{left}} & trn & \bfseries \color{violet} 1 & 1.9 & 2.7e+02 & 1.3 & 2.6e+05 & 6.9e+04 & 1.5 & 5 & 1.1 \\
\cline{3-12}
 &  & val & 1.4e+04 & 2.4e+03 & 6.6e+03 & 3.6e+04 & 2.2e+05 & 7.3e+04 & 3.3e+03 & \bfseries \color{violet} 41 & 5.4e+03 \\
\cline{3-12}
 &  & tst & 1.9e+04 & 8.3e+04 & 6.5e+03 & 3.3e+04 & 2e+05 & 7e+04 & 1.8e+04 & \bfseries \color{violet} 6.6e+02 & 4.9e+03 \\
\cline{1-12} \cline{2-12} \cline{3-12}
\hline
\end{tabular}
\caption{Each algorithm corresponds to a column of \emph{cumulative cost multiples} (lower is better) averaged over the training (trn), validation (val) or testing (tst) query sets and over four possible environments. We use RB for ``replay buffer'' and PER for ``prioritized experience replay''.}
\label{tab:new-data-results}
\vspace{-0.15in}
\end{table*}

% Figure environment removed

\textbf{Future Directions:}
For RL, we've identified a gap in performance between the validation set and test set, and it would be promising to develop RL algorithms that scale to multi-task combinatorial optimization tasks and can generalize across query templates. Also, in our experiments, we looked at the average performance amongst queries, but practitioners may care more about the quantile or conditional value-at-risk (CVaR) of the cost multiple; hence, applying and developing risk-sensitive RL methods \citep{ma2021conservative,wang2023near} to system applications can be a promising direction. 
For DBQO, a promising avenue is to develop better table encodings, which can resolve the partial observability issues of applying RL-based methods.




\newpage
\bibliographystyle{plainnat}
\bibliography{main}
\appendix


\newpage







\newpage
\appendix
\onecolumn


\begin{center}\LARGE
\textbf{Appendices}
\end{center}

\section{List of Abbreviations and Notations}

{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
    \centering
      \caption{List of Abbreviations} \vspace{0.3cm}
    \begin{tabular}{l|l}
    JOS & Join Order Selection \\
    DB & Database \\
    DBQO & Database Query Optimization \\
    IR & Intermediate result table (from a join) \\
    CP & Cartesian product, \ie, join without filter \\
    JOB & Join Order Benchmark \citep{leis2015good}\\
    \end{tabular}
    \label{tab:abbrev}
\end{table}
}


{\renewcommand{\arraystretch}{1.2}%
\begin{table}[h!]
    \centering
      \caption{List of Notations} \vspace{0.3cm}
    \begin{tabular}{l|l}
    $\Ntables$ & Number of tables in the DB \\
    $\Ncols(T)$ & Number of columns in table $T$ \\
    $\Ncols$ & Total number of columns amongst all tables in DB \\
    $A \Join_P B$ & Binary join operator, defined in \cref{eq:def-join} \\
    \end{tabular}
    \label{tab:notation}
\end{table}
}

\newpage
\section{Benchmarking Offline RL}

\begin{table*}[!h]
\centering
\def\arraystretch{1.2}%
\scriptsize
\begin{tabular}{|l|l|l|r|r|r|r|r|r|r|r|r|r|}
\hline
 \multicolumn{3}{|c|}{\textbf{JOB Data}} & \multicolumn{2}{|c|}{DQN} & \multicolumn{2}{c|}{DDQN} & \multicolumn{2}{c|}{BC} & \multicolumn{2}{c|}{BCQ} & \multicolumn{2}{c|}{CQL} \\
\hline
 &  &  & Median & STD & Median & STD & Median & STD & Median & STD &  Median & STD \\
\hline
\multirow[c]{3}{*}{\rotatebox{90}{\tiny{disable CP}}} & \multirow[c]{3}{*}{\rotatebox{90}{left}} & trn & \bfseries \color{violet} 3.22 & 1.9e6 & 1471  & 3.4e10 & 1.6e5 & 7.6e+11 &  3.1e3 & 2.7e13  &  79 &\ 4.1e12\\
\cline{3-13}
 &  & tst & \bfseries \color{violet}  3.19 & 1.9e6 & 1470 & 3.3e10 & 8.0e4 & 7.3e12 & 9908 & 2.6e13 & 76 & 3.9e12\\
\cline{3-13}
 &  & val & \bfseries \color{violet} 3.21 &  2.0e6 & 1.0e3 & 3.4e10 & 6.4e5 & 7.5e12 & 1.5e5 & 2.9e13 & 81 & 4.1e12\\
\cline{1-13} \cline{1-13}
\end{tabular}
\caption{Each algorithm corresponds to a column of \emph{cumulative cost multiples} (lower is better) averaged over the training (trn), validation (val) or testing (tst) query sets and over the left deep trees environment. }
\label{tab:offline-results}
\vspace{-0.15in}
\end{table*}


\paragraph{Experimental Setup}
We collect offline datasets with the following search heuristics: adaptive, dphyp, genetic, goo, goodp, goodp2, gooikkbz, ikkbz, ikkbzbushy, minsel, quickpick, simplification \citep{neumann2018adaptive}. 
We highlight that these heuristics are \emph{search} heuristics, which always require (estimated) IR cardinalities to search through. For our experiments, we gave these heuristics access to the ground-truth IR cardinalities in the training. In practice, one can take traces from existing DBMS such as PostgreSQL.
For each heuristic, we collected $1000$ trajectories across different queries. We partition the dataset for training, evaluation and testing similarly as in online. 

\paragraph{Discussions} 
\cref{tab:offline-results} summarizes our offline results. We find that DQN has the best performance in terms of median and the validation/testing results are even better than online. 
CQL also obtains reasonable performance, but all other methods seem to have relatively poor median performance even on the training set. It's worth noting that all methods seem to have a heavy tail performance distribution (over queries), as shown by the large standard deviations. In later sections of the appendix, we see this is the case for online RL as well. This heavy-tail distribution of returns motivates applying risk-sensitive RL methods to \env{} for future work.


We also tested on some other offline algorithms, such as SAC, and it is hard to converge hence we didn't report the results. We observe that the TD error is increasing, although the Q value functions, actors and critics are learning. Making too many TD updates to the Q-function in offline deep RL is known to sometimes lead to performance degradation and unlearning, we can use regularization to address the issue~\citep{kumar2021dr3}. 



\subsection{Hyperparameters for Offline RL Algorithms}
We disabled CP for all cases, and did a hyperparameter search with grid search / bayesien optimization strategies on different algorithms. The final parameters we used for evaluation is shown below in Table~\ref{tab:bc} to Table~\ref{tab:ddqn}. 

\subsubsection{Batch-Constrained Q-learning}



\begin{table}[H]
    \centering
    \caption{Hyperparameter of Batch-Constrained Q-learning algorithm (BCQ).}
    \begin{tabular}{l|l}
        \toprule
        Hyperparameter & Value \\
        \midrule 
        Learning rate & $6.25 \times 10^{-5}$ \\
        Optimizer & Adam ($\beta = (0.95, 0.999)$) \\
        Batch size & 32 \\
        Number of critics & 6 \\
        Discount factor & 0.99 \\
        Target network synchronization coefficiency & 0.005 \\
        Action flexibility & 0.3 \\ 
        Gamma & 0.99 \\
        \bottomrule
    \end{tabular}
\end{table}


\subsection{Behavior Cloning}
\begin{table}[H]
    \centering
    \caption{Hyperparameter of Behavior Cloning (BC).}
    \begin{tabular}{l|l}
        \toprule
        Hyperparameter & Value \\
        \midrule 
        Learning rate & 0.001 \\
        Optimizer & Adam ($\beta = (0.9, 0.999)$) \\
        Batch size & 100 \\
        Beta & 0.5 \\
        \bottomrule
    \end{tabular} \label{tab:bc}
\end{table}

\subsection{Conservative Q-Learning}



\begin{table}[H]
    \centering
    \caption{Hyperparameter of Conservative Q-Learning (CQL).}
    \begin{tabular}{l|l}
        \toprule
        Hyperparameter & Value \\
        \midrule
        Actor learning rate & $3 \times 10^{-4}$ \\
        Critic learning rate & $3 \times 10^{-4}$ \\
        Learning rate for temperature parameter of SAC & $1 \times 10^{-4}$ \\
        Learning rate for alpha & $1 \times 10^{-4}$ \\
        Batch size & 256 \\
        N-step TD calculation & 1 \\
        Discount factor & 0.99 \\
        Target network synchronization coefficiency & 0.005 \\
        The number of Q functions for ensemble & 2 \\

        Initial temperature value & 1.0 \\
        Initial alpha value & 1.0 \\
        Threshold value & 10.0 \\
        Constant weight to scale conservative loss & 5.0 \\
        The number of sampled actions to compute & 10 \\ 
        \bottomrule
    \end{tabular}
\end{table}


\subsection{DQN}
\begin{table}[H]
    \centering
    \caption{Hyperparameter of DQN.}
    \begin{tabular}{l|l}
        \toprule
        Hyperparameter & Value \\
        \midrule 
        Learning rate & 6.25e-4 \\
        Batch size & 32 \\
        target\_update\_interval  & 8000 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Double DQN}


\begin{table}[H]
    \centering
    \caption{Hyperparameter of DDQN.}
    \begin{tabular}{l|l}
        \toprule
        Hyperparameter & Value \\
        \midrule 
        Learning rate & 6.25e-4 \\
        Batch size & 32 \\
        target\_update\_interval  & 8000 \\
        \bottomrule
    \end{tabular} \label{tab:ddqn}
\end{table}

\newpage
\section{Additional Online RL Results}
In this section, we show the learning curves of the best performing policy in each of the possible settings of $\{\text{disable CP}, \text{enable CP}\} \times \{\text{left}, \text{bushy}\}$, on our new dataset. %
For each setting, we also show the distribution of cost multiples over the training, validation and testing query sets.
The heavy-tail distributions suggest that risk-sensitive approaches may have better outcomes than traditional RL methods.

\subsection{Best RL for Disable CP and Bushy Plans}

% Figure environment removed


\newpage
\subsection{Best RL for Disable CP and Left-deep Plans}

% Figure environment removed



\subsection{Best RL for Enable CP and Bushy Plans}

% Figure environment removed


\newpage
\subsection{Best RL for Enable CP and Left-deep Plans}

% Figure environment removed


\subsection{Heuristic Method}
We also plot results for a heuristic \texttt{ikkbzbushy} which builds join selectivity estimates based on the training data and uses dynamic programming (DP) to compute the best bushy plan according to the estimated IR cardinalities (based on the join selectivity estimates). Similar with RL agents which learns in training queries, we build selectivity estimation of join predicates using training queries. We follow the most classic approach \citep{ramakrishnan2003database} for estimating selectivities of each join pair $R\Join_P S$ using $\sel_i(R\Join_P S) = \frac{|R\Join_P S|}{|R| |S|}$ and take the average among all queries as the final selectivities. And in the validate and test, we estimate the IR size using $\sel_i(R\Join_P S) |R| |S|$. \texttt{ikkbzbushy} can guarantee that the final plan has the smallest estimation cost.

So amongst all heuristics that use the same estimated IR cardinalities, this approach is the best possible heuristic since it uses the plan with the smallest estimated cost. However, since the selectivity estimation step is biased, the final performance is very poor, and worse than the RL-based approaches.
It's worth mentioning that DP-based approaches can take hours on med-large size queries since the problem space grows exponentially. In contrast, RL methods are much faster to run. 

\clearpage

% Figure environment removed








\end{document}
