\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{amsfonts,epsfig}
\usepackage[margin=1in]{geometry}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[hyphens]{url}
\usepackage{microtype}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{tabularx}
\usepackage{pifont} 
\usepackage{xcolor}
\definecolor{darkblue}{rgb}{0,0.08,0.45}
\definecolor{darkred}{RGB}{139,0,0}
\definecolor{Darkblue}{RGB}{0,0,139}
\definecolor{forestgreen}{RGB}{34,139,34}
\definecolor{darkgreen}{RGB}{0,100,0}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{stackengine}
\usetikzlibrary{shapes,arrows,chains}
\usetikzlibrary[calc]
\usetikzlibrary{bayesnet}
\tikzset{>=latex}
\newcommand{\cmark}{\ding{51}}
\newcommand{\xmark}{\ding{55}}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	citecolor=black,
	filecolor=black,
	urlcolor=black,
}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\newcommand{\betav}{\boldsymbol{\beta}}
\newcommand{\zetav}{\boldsymbol{\zeta}}
\newcommand{\lambdav}{\boldsymbol{\lambda}}
\newcommand{\yv}{\mathbf{y}}
\newcommand{\xv}{\mathbf{x}}
\newcommand{\zv}{\mathbf{z}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\pv}{\mathbf{p}}
\newcommand{\Xm}{\mathbf{X}}
\newcommand{\Dm}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\Um}{\mathbf{U}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\Wm}{\mathbf{W}}
\newcommand{\Am}{\mathbf{A}}
\newcommand{\Bm}{\mathbf{B}}
\newcommand{\lmin}{\lambda_{\min}}
\newcommand{\lmobs}[1]{\lambda^{\mathrm{obs}}_{#1}(\thetav,\overline{\yv})}
\newcommand{\yp}[1]{\yv^{(#1)}}
\newcommand{\ld}{\frac{\partial}{\partial\lambda}}
\newcommand{\muv}{\boldsymbol{\mu}}
\newcommand{\thetav}{\boldsymbol{\theta}}
\newcommand{\sigmav}{\boldsymbol{\sigma}}
\newcommand{\gammav}{\boldsymbol{\gamma}}
\newcommand{\tauv}{\boldsymbol{\tau}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\HH}{\mathbf{H}}
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\II}{\boldsymbol{\mathcal{I}}}
\newcommand{\JJ}{\boldsymbol{\mathcal{J}}}
\newcommand{\NI}{\overline{\II}}
\newcommand{\covm}{\boldsymbol{\Sigma}}
\newcommand{\yrep}{\mathbf{y}_{\mathrm{rep}}}
\newcommand{\tr}{\mathrm{tr}}
\renewcommand{\b}{\mathrm{base}}
\renewcommand{\Im}{\mathbf{\mathsf{I}}}
\newcommand{\MI}{\mathbf{I}}
\newcommand{\ev}{\mathbf{e}}
\newcommand{\Thetav}{\boldsymbol{\Theta}}
\newcommand{\thetat}{\widetilde{\thetav}}
\newcommand{\covt}{\widetilde{\covm}}
\newcommand{\Hm}{\mathbf{H}}
\newcommand{\Iobs}{\mathbf{I}_{\mathrm{obs}}(\overline{\yv},\thetat)}

\newtheorem{heuristic}{Heuristic}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\begin{document}
\title{Identifiability and Falsifiability: Two Challenges for Bayesian Model Expansion}
\author{Collin Cademartori}

\maketitle

\abstract{We study the identifiability of model parameters and falsifiability of model predictions under conditions of model expansion in a Bayesian setting. We present results and examples suggesting a tendency for identifiability and falsifiability to decrease in this context and for the severity of these problems to trade-off against one another. Additionally, we present two extended examples that demonstrate how these difficulties can be partially overcome by inferential methods that leverage the joint structure of the posterior distribution.}

\section{Introduction}\label{sec:introduction}

In this work we connect the process of (Bayesian) model expansion to two challenges for the interpretation and evaluation of statistical models, namely:

\begin{itemize}
\item \textbf{identifiability} - the ability of the model to support sufficiently precise inferences about parameters of interest, and
\item \textbf{falsifiability} - the readiness of the model to reveal deficiencies in its fit to the observed data.
\end{itemize}

These general concepts can be made precise in different ways. In practice, poor identifiability can manifest as marginal posterior distributions that are too wide to support substantively interesting conclusions about quantities of interest. Likewise, poor falsifiability can result in reduced power for tests of model fitness compared to nearby alternatives. In the Bayesian context, we will argue that identifiability can be quantified using the mutual information $\MI\left(\thetav,\yv\right)$, and that falsifiability can be quantified by the conditional mutual information $\MI\left( \thetav,\yrep\mid\yv \right)$ - quantities from information theory which we will discuss in more detail in the following sections. 

Model criticism has long been recognized as an essential component of applied statistical workflow, and this process commonly creates a need to expand our models to capture a more diverse collection of data behaviors \cite{BoxsLoop,BayesWorkflow1,BayesWorkflow2}. However, this process is not without challenges, as higher dimensional models can exhibit more complex posterior distributions which frustrate simple conclusions. Our main result quantifies two of these challenges by showing that under appropriate classes of model expansion, there exist bounds on these quantities which exhibit two important properties: (i) a bias towards reducing both falsifiability \textit{and} identifiability as the dimension of the expanded model grows and (ii) a tradeoff whereby model expansions which avoid reducing one of identifiability and falsifiability are in some sense more likely to reduce the other.

While we do not expect the behavior of bounds (which may be quite loose) to translate directly or universally to individual models and datasets, these two properties of our bounds qualitatively match the patterns that we observe in both simple cases where direct calculations are possible and in more complex examples with simulated and real data. We thus view the main contribution of our result as conceptually uniting and generalizing patterns observed in particular cases. For example:
\begin{enumerate}
\item The literature on Bayesian sparse regression has demonstrated that the identification problems inherent to high-dimensional regression problems can often be alleviated by imposing certain hierarchical priors on the coefficients (e.g. horseshoe or normal scale-mixture priors) \cite{RegHorseshoe,SpikeSlab,BayesShrink}. Critically, these priors work by encoding dependence between the coefficients, and thus do not require the addition of prior information marginally to be effective. The tradeoff between identifiability and falsifiability we observe in our main result breaks down when the prior encodes enough dependence between the parameters. This suggests that the kind of dependence encoding that can resolve identification problems in regression models may be a good strategy for addressing identifiability deficits more generally.
\item The posterior predictive $p$-value has been criticized in the model checking literature as being conservative or under-powered \cite{BayarriBerger00,RobinsEtAl00,PostPPharma}. Because these criticisms have hinged on frequency properties of the $p$-value, some Bayesians have responded by pointing out that the posterior predictive $p$-value is interpretable without reference to its distribution under frequentist replications. We will argue that our notion of falsifiability is directly linked to a general concept of power which does not require any reference to frequenctist considerations. Our main result suggests that the risk of such conservaitity problems is directly linked to the process of model expansion, but it also motivates a generlization of the posterior predictive $p$-value which we show is capable of resolving some of the practical problems caused by these problems.
\end{enumerate}

Our overall conclusion is thus both negative and positive. On on hand, we believe that our main result suggests a real tension exists between some of the basic goals of applied modeling in the context of iterative model expansion. On the other hand, we do not believe our result militates against successful model expansion in general. Rather, by quantifying some features of this tradeoff, our result points towards possible tools which we believe can form the basis of an expansion-ready statistical methodology.

We illustrate the basic shape of this tradeoff with an extremely simple regression example to establish intuition. Suppose we have only two observations $(y_1,y_2)$ and known measurement variance $\sigma^2 = 1$. In our first model, we have one predictor $\xv_1 = (0,1)$ with coefficient $\beta_1$. Assigning a normal prior, the resulting model is
 \begin{equation}
   \label{eq:simple_reg_1}
   y_j \mid\beta_1 \sim \mathrm{normal}\left( \beta_1x_{1j},1 \right)\text{ for } j=1,2, \quad \beta_1\sim\mathrm{normal}\left( 0,\sigma_b \right),
 \end{equation}
 where the hyperparameter $\sigma_b$ is taken large so that the prior is weakly informative. We then expand this model by adding a second predictor $\xv_2$ with $\|\xv_2\|=1$ and with coefficient $\beta_2$. Assuming $\beta_2$ is a priori independent of $\beta_1$ and assigning an identical marginal prior, we get
 \begin{equation}
   \label{eq:simple_reg_2}
   y_j\mid \beta_1,\beta_2 \sim \mathrm{normal}\left( \beta_1x_{1j}+\beta_2x_{2j},1 \right)\text{ for } j=1,2,\quad \beta_1,\beta_2\stackrel{iid}{\sim}\mathrm{normal}\left( 0,\sigma_b \right).
 \end{equation}
 We consider five levels of nonnegative correlation between the predictors $\xv_1$ and $\xv_2$, which are plotted in the top row of Figure \ref{fig:simple_reg_example}. To assess the effect of adding a second predictor on identifiability, we compare the marginal posterior of $\beta_1$ to its prior, plotted in the second row of Figure \ref{fig:simple_reg_example}.

As we would expect, as predictor correlation increases, the identification of the coefficient $\beta_1$ decreases.

It is less obvious how we should assess the falsifiability of the model. We argue in Section \ref{sec:underpowered_check_theory} that falsifiability is connected to a measure of posterior confidence about the true data generating process, expressed as a distribution over independent, replicated data $\yrep$.

In particular, we will argue that falsifiability tends to decrease as the sampling distributions $p(\yrep\mid\betav)$ and the posterior predictive distribution $p(\yrep\mid\yv) = \int p\left( \yrep\mid\betav \right)p\left( \betav\mid\yv \right)d\thetav$ become more dissimilar.

The third row of Figure \ref{fig:simple_reg_example} partially visualizes this by plotting the sampling distributions for a replicated first observation $y^{\mathrm{rep}}_1$ at the posterior means $\overline{\betav}$ with the corresponding posterior predictive distributions.

As the correlation between the predictors decreases, the distributions $p(y_1^{\mathrm{rep}}\mid\overline{\betav})$ and $p(y_1^{\mathrm{rep}}\mid\yv)$ become less similar.

 % Figure environment removed
 
 The relationships displayed in the highlighted panels correspond to the relationships between the corresponding distributions that occur in the single-predictor model \eqref{eq:simple_reg_1}. These highlighted panels also correspond to the best-case behavior, showing that the expanded model can only perform worse than the base model on either metric. In fact, we further see that these behaviors are inversely correlated among the expanded models, i.e. the most precise marginal inference occurs when the sampling and posterior predictive distributions are most dissimilar and vice versa. We will find evidence of similar phenomena more generally in the next sections.
 
 \subsection{Outline}\label{subsec:outline}

 In Sections \ref{sec:weak_id_theory} and \ref{sec:underpowered_check_theory}, we separately connect weakening of identifiability and falsifiability to some general conditions of model expansion . Section \ref{sec:id_check_connection} presents our main result connecting identifiability and falsifiability, showing that expansions which decrease the severity of one challenge have an increased risk of worsening the other (as seen in the above regression example). In Sections \ref{sec:weak_id_practice} and \ref{sec:underpowered_check_practice}, we demonstrate through two examples how richer inferences which leverage the full joint structure of the posterior distribution can alleviate some of the difficulties imposed by poor identifiability and falsifiability. 

 \subsection{Contributions}\label{subsec:contributions}

 Our main result, Theorem \ref{thm:strict_tradeoff} uses a generalizaton of the Bayesian Cramer-Rao bound \cite{LogSobolevBayes} and matrix concentration inequalities \cite{MatrixConcentration} to establish bounds on the mutual and conditional mutual information which exhibit both a tradeoff and dimension dependence. We sketch the result here. Suppose that we have a base model and expasion thereof denoted $p_{\b}$ and $p$ respectively, both defined over common data $\yv$ and with some shared parameters $\thetav$ (a notion which we will define unambiguously in the next section).

 \begin{theorem}\label{thm:informal}
   Let $\boldsymbol{\iota}_\b$ and $\boldsymbol{\iota}$ be the eigivenvalues of the expected Fisher information matrices for $\thetav$ in the base and expanded models $p_{\b}$ and $p$, respectively. Furthermore let $\MI_{\b}$ and $\MI$ denote the (conditional) mutual information in the base and expanded models. Finally, let $d$ be the dimension of $\thetav$ and $d^{\mathrm{exp}} \geq d$ the dimension of the parameter space for the expanded model. Then, under technical conditions given in the statement of Theorem \ref{thm:strict_tradeoff}, we have
       \begin{equation}
    \underbrace{\MI_{\b}\left(\yv,\thetav\right)}_{\substack{\text{base model} \\ \text{identifiability of $\thetav$}}} \leq \Psi_i\left(\boldsymbol{\iota}_{\b}\right), \qquad \underbrace{\MI\left(\yv,\thetav\right)}_{\substack{\text{expanded model} \\ \text{identifiability of $\thetav$}}} \leq \Psi_i\left(\boldsymbol{\iota}\right) - \Delta_i
  \end{equation}

   \begin{equation}
     \underbrace{\MI_{\b}\left(\yrep,\thetav\mid\yv \right)}_{\substack{\text{base model} \\ \text{falsifiability}}} \geq  c_d\Psi_f\left( \boldsymbol{\iota}_{\b} \right) \qquad\underbrace{\MI\left(\yrep,\thetav\mid\yv \right)}_{\substack{\text{expanded model} \\ \text{falsifiability}}} \geq  c_{d^{\mathrm{exp}}}\left[\Psi_f\left( \boldsymbol{\iota} \right) + \Delta_f\right],
  \end{equation}
where $\Psi_i,\Psi_f$ are increasing in each of the components of the vector argument, $\Delta_m,\Delta_c \geq 0$ are terms which tend to increase in magnitude with $d^{\mathrm{exp}}$, and $c_d, c_{d^{\mathrm{exp}}}$ are constants depending only on $d$ and $d^{\mathrm{exp}}$ respectively.
\end{theorem}

These inequalities are given from the adverse directions, in the sense that smaller mutual information and larger conditional mutual information are associated with reduced identifiability and falsifiability respectively. The dimensional dependence enters through the $\Delta$ terms, which push the bounds in the corresponding adverse directions. The tradeoff between these bounds occurs through the $\Psi$ terms. For instance, if the components of $\boldsymbol{\iota}$ are all smaller than $\boldsymbol{\iota}_{\b}$, then our mutual information bound will decrease in passing to the expanded model. In the reverse case, the conditional mutual information bound will increase.

In conjunction with numerous examples, this result suggests that at least one of reduced identifiability and reduced falsifiability should be expected in the process of iterative model expansion, which is the first major contribution of this work. The generality of this phenomenon motivates considering methods of inference and model checking which can cope with these conditions by extracting as much useful information from our models as possible.

Our second contribution is to demonstrate methods of utilizing the joint structure of the posterior in practice which may allow these challenges to be partially overcome in many cases. We show in an extended example how the dependence structure of the posterior can contain significant, practically useful information even when the marginal inferences are too weak to support strong conclusions about individual parameters. And in the context of model checking, we provide an extension of the traditional posterior predictive $p$-value, which we validate in a real data example, and which we argue is often more useful and easily applied than previous solutions designed to resolve the posterior predictive $p$-value's claimed power deficiencies.
 
 \subsection{Related Work}\label{subsec:related}
 Recently, statistical workflow has enjoyed increased attention as a discrete topic in statistics. This literature has sought to provide a consistent framework and practical advice for each step of a statistical analysis, including the process of model expansion (see, e.g. \cite{BayesWorkflow1,BayesWorkflow2,BayesWorkflow3}).
 Here we seek to complement this perspective by studying model expansion as a distinct regime. 
To this end, our main result provides interpretable bounds on the mutual and conditional mutual information, the former of which depends critically on Theorem 2 in \cite{LogSobolevBayes}.

 Outside of the context of model expansion, the problem of weak/non-identification has been extensively studied in the classical and Bayesian contexts. In the Bayesian setting, methods of detecting and dealing with identification problems have been studied in, e.g. \cite{BayesLearning,DataCloning}. Whereas these methods have usually been tied to particular (classes of) models, we study this problem in a general setting of model expansion.

 As we will argue in Section \ref{sec:underpowered_check_theory}, problems of falsifiability are directly connected to debates over the power and conservativity of the posterior predictive $p$-value. Various forms of this problem have been described, and possible solutions have been proposed in \cite{BayarriBerger99,BayarriBerger00,RobinsEtAl00,PostPPharma}. We propose another possible solution - conditional $p$-values - which differ from these previous proposals both in their goal and method of use, and we will argue that our approach is more practically applicable in many cases.

 Our approach to studying the problems of identifiability and falsifiability follows many previous successes in using information-theoretic tools to understand and quantify model behaviors in great generality. We enumerate a few connections of particular note:
 \begin{enumerate}
 \item We quantify identifiability by thinking of the information entropy of a posterior as representing our uncertainty about parameters of interest. This representation of uncertainty as entropy can be traced back to Jaynes, who used it to justify the use of maximum entropy posterior distributions \cite{MaxEntropyPriors}.
 \item Information-theoretic criteria have long been used to evaluate the predictive performance of models \cite{LooWAIC}. In the Bayesian context, the expected log predictive density (ELPD) has been used as a flexible and model-specific objective for model evaluation and comparison \cite{LOO}. When our data consists of a scalar quantity $y$, and the model is correctly specified, the ELPD can be given as   
   \begin{equation*}
     \mathrm{D}\left( p(y\mid\thetav^*) \;\vert\vert\; p(y\mid y^{\mathrm{rep}}) \right) + C,
   \end{equation*}
   where $p(\yv\mid\thetav^*)$ is the true data generating process, $C$ is a constant depending only on this true distribution, and $y^{\mathrm{rep}}\sim p(y\mid\thetav)$ is an independent replication of the data. If we substitute the true value $\thetav^*$ with an average over the posterior $p(\thetav\mid\yv)$, then the first term recovers the conditional mutual information $\MI\left( \thetav,\yrep\mid\yv \right)$ which we relate to the falsifiability of the model.
 \item The Rashomon effect, first described by Breiman in \cite{TwoCultures}, is a phenomenon whereby many models can achieve similar overall loss yet provide very different point predictions. In our work, we find that our concept of falsifiability is also threatened by the multiplicity of plausible sampling distributions in a model. And indeed, our conditional mutual information rests on a conceptually simiar KL divergence as a recently proposed metric for quantifying the Rashomon effect, the Rashomon capacity \cite{RashomonCapacity}.
 \item Mutual information-based quantities have also been deployed to bound measures of other adverse model behaviors, particular bias and generalization error \cite{GeneralizationError1,GeneralizationError2}.
 \end{enumerate}
 
 \section{Weak Identifiability and Model Expansion}\label{sec:weak_id_theory}

 We start by defining the types of model expansions to which our results will apply. We will write $p_{\b}(\yv,\thetav)$ for some base model defined over data $\yv\in\RR^n$ and parameters $\thetav\in\RR^d$. We then consider certain expansions of this base model defined as follows.

\begin{definition}[Model Expansion]
  A model $p(\yv,\thetav,\lambdav)$ defined with additional parameter $\lambdav\in\overline\RR^k$ is an expansion of $p_{\b}(\yv,\thetav)$ if
\begin{equation}
  \label{eq:expansion_def}
  p_{\b}(\yv,\thetav) = p(\yv,\thetav\mid\lambdav_0) \text{ for } \lambdav_0\in\overline{\RR}^k,
\end{equation}

where $\overline{\RR}=[-\infty,\infty]$.
\end{definition}
This framework includes many common examples of model expansion:

\begin{itemize}
\item Let $p_{\b}(\yv,\thetav)$ be a generalized linear model with response vector $\yv$ and parameters $\thetav$ including the coefficients and any additional parameters. Adding a new predictor and coefficient $\lambda$ with independent prior is then an expansion since $p_{\b}(\yv,\thetav)=p(\yv,\thetav\mid\lambda=0)$.
\item Let $p_{\b}(\yv,\thetav)$ be an exchangeable Poisson model over the data $y_i$ with $\theta$ the Poisson rate. We can extend this with an overdispersion parameter $\lambda$ (with independent prior). This is commonly modeled with a negative binomial distribution
  \[
    p(y\mid\theta,\lambda) = \binom{y + \lambda - 1}{y}\left( \frac{\theta}{\theta+\lambda} \right)^y\left( \frac{\lambda}{\theta+\lambda} \right)^\lambda.
  \]
  Since this reduces to the Poisson as $\lambda\to\infty$, we have that $p_{\b}(\yv,\thetav)=p(\yv,\thetav\mid\lambda=\infty)$, so this is again an expansion.
\end{itemize}

\subsection{Weak Identification and Marginal Entropy}\label{subsec:weak_id_entropy}

We now formalize our notion of identification using the information entropy. First we establish some notation. For a joint model $q(\thetav,\yv)$, the (differential) entropy of $q(\thetav)$ is denoted $h_{q(\thetav)}(\thetav)$, and the conditional entropy of $\thetav$ given $\yv$ is $h_{q(\thetav,\yv)}\left( \thetav\mid\yv \right)$. The mutual information (\textsf{mi}) is denoted $\MI_q\left( \thetav,\yv \right)$, which will at times be extended to a conditional mutual information (\textsf{cmi}), denoted by $\MI_q\left( \thetav,\yv\mid \xv \right)$, when the joint model extends over an additional quantity $\xv$. When distributions are clear from context, we may drop subscripts from entropies and mutual informations, writing e.g. $h(\thetav)$ and $\MI\left( \thetav,\yv \right)$.  The reader who is unfamiliar with information theory may consult Appendix \ref{app:it_overview} for definitions of these quantities and statements of the basic results that we use. With these definitions, we can now give quantitative operational definitions of our notions of weak marginal identification for arbitrary subsets of $\thetav$.

\begin{definition}[$\epsilon$-Weak Identification]\label{def:weak_operational}
  Let $I\subset [d]$. We say for any $\epsilon > 0$ that $\thetav_I = \left( \theta_i \right)_{i\in I}$ is $\epsilon$-weakly identified for data $\yv$ if
  \begin{equation}\label{eq:epsilon_id}
  h_{p(\thetav_I\mid\yv)}\left(\thetav_I\right) > h_{p(\thetav_I)}\left( \thetav_I \right) - \epsilon.
  \end{equation}
  For $p_1\left( \thetav,\yv \right)$ and $p_2(\thetav,\yv)$, $\thetav_I$ is more weakly identified in $p_2$ than $p_1$ if $h_{p_2(\thetav_I\mid\yv)}\left(\thetav_I \right)>h_{p_1(\thetav_I\mid\yv)}\left(\thetav_I \right)$.
 \end{definition}

We also define weak identification for entire models (regardless of data $\yv$) by averaging over the prior predictive distribution.

\begin{definition}[$\epsilon$-Weakly Identifiable Model]\label{def:weak_model}
  Let $I\subset [d]$. We say $\thetav_I$ is $\epsilon$-weakly identifiable in $p(\thetav,\yv)$ if
\begin{equation}\label{eq:epsilon_id_model}
  h\left( \thetav\mid\yv \right) > h\left(\thetav \right)-\epsilon,
\end{equation}
  or, equivalently, if $\MI(\thetav,\yv) < \epsilon$.
\end{definition}
Henceforth, we will leave the $\epsilon$-dependence of this definition implicit and simply say that a parameter is weakly identified if it is $\epsilon$-weakly identified for an appropriate value of $\epsilon$ (which will usually be given by domain understanding).

This operational definition of weak identification can only be interpreted relative to the prior. In many cases, this is a natural quantity to focus on (e.g. when we are concerned with the cost-benefit tradeoffs of data collection or the contribution of a research finding to existing knowledge). However, if we expand a model by adding prior information about $\thetav$, then it possible for both the posterior entropy of $\thetav$ and the mutual information to decrease. In other words, the identification relative to the prior may decrease while the posterior becomes more concentrated. This divergence between absolute and relative notions of identification can be avoided if we exclude from consideration expansions which decrease $h(\thetav)$. 

We can now show that certain model expansions tend to weaken identification in the above sense. If $p(\thetav,\lambdav,\yv)$ is an expansion of $p_{\b}(\thetav,\yv)$, then we have the following decomposition of the mutual information:
\begin{equation}
  \label{eq:id_weak_id_mi}
  \MI_{p(\thetav,\lambdav,\yv)}(\thetav,\yv) = \MI_{p_{\b}(\thetav,\yv)}(\thetav,\yv) + \Delta_I^{\mathrm{exp}} + \Delta^{\mathrm{post}}_I,
\end{equation}
where we define
\begin{align*}
  \Delta_I^{\mathrm{exp}} &= \MI_{p(\thetav,\lambdav,\yv)}\left( \thetav,\yv\mid\lambdav \right) - \MI_{p(\thetav,\lambdav,\yv)}\left( \thetav,\yv\mid\lambdav_0 \right)\\
  \Delta^{\mathrm{post}}_I &= \MI_{p(\thetav,\lambdav,\yv)}\left(\lambdav,\thetav\right) - \MI_{p(\thetav,\lambdav,\yv)}(\lambdav,\thetav\mid\yv).
\end{align*}

The $\Delta_I^{\mathrm{exp}}$ term is the difference in amount of information $\yv$ provides about $\thetav$ given $\lambdav$ and given $\lambdav_0$, averaging $\lambdav$ over the expanded model. The $\Delta^{\mathrm{post}}_I$ term is the difference in the amount of information $\lambdav$ provides about $\thetav$ before and after observing the data $\yv$. We regard this as a measure of the a priori informativeness of $\lambdav$ about $\thetav$, which is justified by the fact that we have, $\Delta^{\mathrm{post}}_I \geq -\MI\left( \lambdav,\thetav\mid\yv \right)$ with equality if and only if $\thetav$ and $\lambdav$ are independent in the expanded model (i.e. if $p(\thetav,\lambdav) = p(\thetav)p(\lambdav)$). When $\Delta^{\mathrm{post}}_I < 0$, \eqref{eq:id_weak_id_mi} shows that the expanded model is biased towards weaker identification of $\thetav$ compared to the base model.

We can also use the decomposition \eqref{eq:id_weak_id_mi} to define a concept which will be useful in the next sections. We say the parameter $\lambdav$ \textbf{dilutes} the effect of shared parameter $\thetav$ if $h_{p(\thetav,\yv\mid\lambdav)}\left(\thetav\mid\yv\right)$ is larger than $h_{p_{\b}(\thetav,\yv)}(\thetav\mid\yv)$ on average over $p(\lambdav)$. If this relationship is reversed, we say that $\lambdav$ \textbf{concentrates} the effect of $\thetav$. In the case that $p(\thetav\mid\lambdav) = p_{\b}(\thetav)$ for all $\lambdav$, dilution and concentration are equivalent to $\Delta_I^{\mathrm{exp}}<0$ and $\Delta_I^{\mathrm{exp}}>0$ respectively.

\subsection{The Relation to Marginal Fisher Information}

For model $q(\yv,\thetav)$, the observed and Fisher information matrices are defined as
\begin{gather}
  \label{eq:observed_info}
  [\JJ_q(\yv,\thetav)]_{ij} = -\frac{\partial^2}{\partial\theta_i\partial\theta_j}\log q(\yv\mid\thetav) \text{ for all } 1\leq i,j\leq d,\nonumber\\
  \II_q(\thetav) = \E_{q(\yv\mid\thetav)}\JJ_q(\yv,\thetav).
\end{gather}
We drop the subscript when the model is clear from context. We now state a bound on the mutual information in terms of the Fisher information, which follows directly from Theorem 2 of Aras et al. \cite{LogSobolevBayes}.

\begin{theorem}[Mutual Information Upper Bound]\label{thm:mi_ub}
  Let $q(\yv,\thetav)$ be a model such that the prior $p(\thetav)$ is log-concave with covariance matrix $\covm$, then
  \begin{equation}
    \label{eq:mi_fisher_ub_statement}
    \MI(\thetav,\yv) \leq d\psi\left( \frac{1}{d}\tr\left(\E_{q(\thetav)} \covm^{1/2}\II(\thetav)\covm^{1/2} \right) \right),
  \end{equation}
  where $\psi(x)$ is the concave increasing function given by
  \[
    \psi(x) = \begin{cases}\sqrt{x}, & 0\leq x\leq 1\\ 1+\frac{1}{2}\log(x), & x > 1  \end{cases}.
  \]
\end{theorem}

Now let $v_{\mathrm{pr}}$ be the maximum eigenvalue of $\covm$ (the covariance matrix over just the $\thetav$ parameters). Then we also clearly have
\begin{equation}\label{eq:weaker_mi}
  \MI(\thetav,\yv) \leq d\psi\left( \frac{v_{\mathrm{pr}}}{d}\E_{q(\thetav)}\tr\left(\II(\thetav) \right) \right).
\end{equation}
If $v_{\mathrm{pr}}$ differs between a base model and expanded model, then we can rescale the prior over $\thetav$ in the expanded model so that they are equal. The only possible difficulty is that we may no longer have a $\lambdav_0$ for which $p_{\b}\left( \thetav \right)=p\left( \thetav\mid\lambdav_0 \right)$. Such situations can always be resolved however by passing to a larger model which includes a prior scale hyperparameter for $\thetav$ in $\lambdav$. With such a hyperparameter, we always have the ability to set both the marginal prior scale and $\lambdav_0$-conditional prior scale for $\thetav$ independently, allowing equality of $v_{\mathrm{pr}}$ and preservation of the model expansion property.

We also note that rescaling $\thetav$ leaves $\MI(\thetav,\yv)$ unchanged since the mutual information is invariant to all inveritble transformations of $\thetav$ and $\yv$ separately. Thus, with loss of little generality, we henceforth assume that $v_{\mathrm{pr}}=1$ for all models. The weaker bound \eqref{eq:weaker_mi} will be useful for comparisons to other quantities in the next sections, and for deriving the following further upper bound, which applies more directly to model expansions, is easier to compute, and mirrors the relation \eqref{eq:id_weak_id_mi}.

\begin{theorem}\label{thm:marginal_trace_bound} Define the partial Hessian with respect to $\lambdav$ as $\left[\Hm\left( \lambdav;\thetav,\yv \right)\right]_{jk} = -\frac{\partial^2}{\partial\lambda_j\partial\lambda_k}\log p(\yv,\thetav,\lambdav)$
  for $1\leq j,k\leq m$. Then under the regularity conditions in Appendix \ref{app:trace_bound},  
  \begin{equation}
    \label{eq:ex_spectrum}
    \E\tr\II(\thetav) \leq \sum_{j=1}^d\left[ \E\left\lbrace- \frac{\partial^2}{\partial\theta_j^2}\log p(\yv\mid\thetav,\lambdav)\right\rbrace + \Delta_j \right],
  \end{equation}
  where we define
  \[
    \Delta_j = \E\left\lbrace -\frac{\partial^2}{\partial\theta_i^2}\log p\left(\lambdav\mid\thetav \right)  \right\rbrace - \frac{\left[ \sum_{j=1}^m\E\frac{\partial}{\partial\lambda_j}\frac{\partial}{\partial\theta_i}\log p(\yv\mid\thetav,\lambdav)\right]^2}{\E\|\Hm\left( \lambdav;\thetav,\yv \right)\|_{\mathrm{op}}}.
  \]
\end{theorem}
The $\Delta_j$ terms compare prior- and likelihood-based measures of dependence between $\thetav$ and $\lambdav$ and are thus analogous to the $\Delta^{\mathrm{post}}$ term in \eqref{eq:id_weak_id_mi}. As with the $\Delta^{\mathrm{post}}$ term, we have $\Delta_j\leq 0$ when $\thetav$ and $\lambdav$ are independent under the prior. In particular, when $p(\thetav,\lambdav,\yv)$ is an expansion of some base model and when $\sum_{i=1}^d \Delta_j < 0$, \eqref{eq:ex_spectrum} again exhibits a downward bias on the Fisher information of the expanded model compared to the base model. We now work through two simple examples to illustrate.

\begin{enumerate}
\item Take a linear regression model $p_{\b}$ with response $\yv\in\mathbb{R}^n$, predictors $\Xm\in\mathbb{R}^{n\times m}$, coefficients $\betav\in\mathbb{R}^m$, intercept $\alpha$, and log noise variance $\tau$:
  \[
    \left( 2\pi\exp\left( \tau \right) \right)^{-n/2}\exp\left[ -\left( \yv - \Xm\betav-\alpha\mathbf{1} \right)^T\left( \yv-\Xm\betav -\alpha\mathbf{1}\right)/2\exp(\tau) \right].
  \]
  We consider an expansion $p$ with additional predictor $\zv\in\mathbb{R}^n$ and coefficient $\lambda$. Suppose the coefficients are assigned independent priors, and let $\thetav = (\sigma,\beta_1,\ldots,\beta_m,\alpha)$.
  
  We assume without loss of generality that all predictors are centered as this does not affect the posterior entropy. We then find for all $1\leq j\leq m$,
  \begin{gather*}
    -\E_p\frac{\partial^2}{\partial\beta_j^2}\log p(\yv\mid \lambda,\thetav) = \E_{p(\tau)}\left\lbrace\frac{n\mathrm{var}\left( \xv_j \right)}{\exp(\tau)}\right\rbrace=-\E_{p_{\b}}\frac{\partial^2}{\partial\beta_j^2}\log p_{\b}(\yv\mid\thetav), \\
    -\E_{p}\frac{\partial^2}{\partial\tau}\log p(\yv\mid \lambda,\thetav)=\frac{n}{2}= -\E_{p_{\b}}\frac{\partial^2}{\partial\tau^2}\log p_{\b}(\yv\mid \thetav), \text{ and }\\
    -\E_{p}\frac{\partial^2}{\partial\alpha^2}\log p(\yv\mid \lambda,\thetav) = \E_{p(\tau)}\left\lbrace\frac{n}{\exp(\tau)}\right\rbrace=-\E_{p_{\b}}\frac{\partial^2}{\partial\alpha^2}\log p_{\b}(\yv\mid \lambda,\thetav)
   \end{gather*}
   These computations show that the first term in \eqref{eq:ex_spectrum} is just $\tr\left( \E\II_{p_{\b}}(\thetav) \right)$. 
  Assuming $\zv$ is also centered, computing the second term in \eqref{eq:ex_spectrum} gives that
  \[
    \E_{p_{\b}}\mathrm{Tr}\left( \boldsymbol{\mathcal{I}}_{p_{\b}} \right) - \E_p\mathrm{Tr}\left( \boldsymbol{\mathcal{I}}_p \right)\geq n^2\E_{p(\tau)}\left\lbrace\frac{1}{\exp(\tau)}\right\rbrace\sum_{j=1}^m\left[\frac{\mathrm{cov}\left( \xv_j,\zv \right)}{\mathrm{var}\left( \zv \right)}\right]^2,
  \]
  which reflects the familiar fact that the identifiability of regression models is reduced by significant correlation between predictors.
\item Next consider an exchangeable Poisson base model with likelihood
  \[
     \exp\left(\mu n\overline{\yv}-n\exp(\mu)\right)\Big/ \left( y_1! \times y_2!\times\cdots\times y_n! \right).
  \]
  This can be expanded to a negative binomial model with likelihood
  \[
     \left[\prod_{i=1}^n\frac{\Gamma\left( y_i+\exp(\lambda) \right)}{\Gamma\left( y_i+1 \right)\Gamma\left( \exp(\lambda) \right)}\right]\left( \frac{\exp(\mu)}{\exp(\mu)+\exp(\lambda)} \right)^{\sum_{i=1}^n y_i}\left( \frac{\exp(\lambda)}{\exp(\mu)+\exp(\lambda)} \right)^{n\exp(\lambda)}.
  \]
  This converges to the Poisson density as $\lambda\to\infty$, so the expanded model is an expansion of the Poisson model. Next observe that the second derivatives with respect to $\mu$ are given by
  \[
    -\frac{\partial^2}{\partial\mu^2}\log p\left( \yv\mid\mu,\lambda \right) = n\exp\left( \mu \right)\left[ 1 - \frac{\exp(\mu)}{\exp(\mu)+\exp(\lambda)} \right]\left[ \frac{\overline{y} + \exp(\lambda)}{\exp(\mu)+\exp(\lambda)} \right],
  \]
  which has expected value $n\exp\left( \mu \right)\left[ 1 - \frac{\exp(\mu)}{\exp(\mu)+\exp(\lambda)} \right]$ under $p(\yv\mid\mu,\lambda)$.
  With this we can show using \eqref{eq:ex_spectrum} that the Fisher information trace must fall in passing from the base to the expanded model:
  \[
    \E\mathrm{Tr}\left( \boldsymbol{\mathcal{I}}_p \right) \leq \E\left\lbrace n\exp\left( \mu \right)\left[ \frac{\exp(\lambda)}{\exp(\mu)+\exp(\lambda)} \right]\right\rbrace < \E\left\lbrace n\exp(\mu)\right\rbrace = \E\mathrm{Tr}\left( \boldsymbol{\mathcal{I}}_{p_{\b}} \right),
  \]
\end{enumerate}

The results of Theorems \ref{thm:mi_ub} and \ref{thm:marginal_trace_bound} both connect Bayesian and classical notions of identification and show that the marginal \textsf{mi} is controlled by a quantity that is often easily approximated before fitting the expanded model. This latter property may be useful when posterior sampling is slow, providing an indication of difficult posterior geometry before it frustrates the sampling algorithm.

The results of this section, particularly \eqref{eq:ex_spectrum}, suggest that dilution of $\thetav$ by $\lambdav$ may be heuristically indicated by a positive difference:
  \[
    \boldsymbol{\Delta}_{\text{dilute}} = \E_{p(\thetav)p(\lambdav)}\left\lbrace\II_{\b}\left( \thetav \right) - \II(\thetav\mid\lambdav)\right\rbrace,
  \]
  where $\II_{\b}$ is the Fisher information of the base model, and $\II(\thetav\mid\lambdav)$ is the Fisher information of the expanded model conditional on $\lambdav$, i.e. the principal submatrix of the full Fisher information matrix $\II(\thetav,\lambdav)$ obtained by deleting those twos and columns involving derivatives with respect to the components of $\lambdav$. We will also say that the effect of $\lambdav$ is \textbf{totally diluting/concentrating} of $\thetav$ if $\boldsymbol{\Delta}_{\text{dilute}}$ is positive/negative semidefinite, respectively.

\section{Weak Falsifiability and Model Expansion}\label{sec:underpowered_check_theory}

We now turn to the behavior of the posterior predictive distribution (\textsf{ppd}) $p(\yrep\mid\yv)$ under model expansion. For joint model $p(\yv,\thetav)$, the \textsf{ppd} is
\begin{equation}
  \label{eq:ppd}
  p(\yrep\mid\yv) = \int p(\yrep\mid\thetav)p(\thetav\mid\yv)d\thetav.
\end{equation}

Comparisons between posterior predictive samples and observed data are commonly used to check Bayesian models. It is often convenient to formalize these checks as posterior predictive $p$-values (\textsf{\textsf{ppp-v}}s).

\begin{definition}[Posterior Predictive $p$-Value]
  For observed data $\yv$, joint model $p(\yv,\thetav)$, and real-valued test statistic $T$, the right-tailed \textsf{ppp-v} for $T$ is
  \begin{equation}
    \label{eq:pppv_def}
    p_T = \int_{\{T(\yrep)\geq T(\yv)\}}p(\yrep\mid\yv)d\yrep.
  \end{equation}
  The left-tailed and two-tailed $p$-values are defined analogously.
\end{definition}

It will be useful to set \textsf{ppp-v}s in a general framework of model evaluations.
\begin{definition}[Data Distribution Evaluation]
  Given $p(\yv,\thetav)$, let $\mathcal{Y}$ be the (common) support of the densities $p(\yv\mid\thetav)$, $P(\mathcal{Y})$ be the space of all densities on $\mathcal{Y}$, and $\mathcal{E}\subset\RR$. Then for any data $\yv$, an evaluation is a (measurable) map
  \[
    e_{\yv}:P(\mathcal{Y}) \to \mathcal{E}
  \]
\end{definition}
For posterior sample $\{\thetav_{(s)}\}_{s=1}^S$ and evaluation $e_{\yv}$, the modeler has data
\begin{equation}\label{eq:check_data}
  \left\lbrace \thetav_{(s)},e_{\yv}\left( p(\yv\mid\thetav_{(s)}) \right) \right\rbrace_{s=1}^S.
\end{equation}
with which to evaluate the model. Since \eqref{eq:check_data} can be complex and high-dimensional, it may not proffer easy conclusions about overall model fitness. Posterior predictive checks solve this by providing simple summaries of \eqref{eq:check_data}. For statistic $T$, we define conditional \textsf{ppp-v}s $p_T(\thetav)$ as the evaluations $e_{\yv}\left( p(\cdot\mid\thetav) \right)$ for the map $q(\cdot) \to \int_{\{T(\yrep)\geq T(\yv)\}} q(\yrep)d\yrep$. The usual \textsf{ppp-v} is then just the average:
\begin{equation}\label{eq:pppv_reduction}
  p_T = \int e_{\yv}\left( p(\cdot\mid \thetav) \right) p (\thetav\mid\yv)d\thetav,
\end{equation}
which is naturally estimated by $\frac{1}{S} \sum_{s=1}^Se_{\yv}\left( p(\cdot\mid \thetav_{(s)}) \right)$. Thus, the \textsf{ppp-v} may be limited if relevant information in \eqref{eq:check_data} is lost in \eqref{eq:pppv_reduction}.
If $p(\yv\mid\thetav_{(s)})=p(\yv\mid\thetav_{(t)})$ for all $\yv$ and $1\leq s,t\leq S$, the \textsf{ppd} reduces to this one distribution, and no information is lost in \eqref{eq:pppv_reduction}. But generally the \textsf{ppd} will not be able to totally summarize all of the sampling distributions which are plausible under the posterior.

We quantify this loss of information with the Kullback-Leibler (KL) divergence, which is given for densities $p$ and $q$ over common support as $D\left( p(\yv) \vert\vert q(\yv) \right) = \E_{p(\yv)}\log\left( \frac{p(\yv)}{q(\yv)} \right)$.
This is a measure of discrepancy between distributions, and with it, we define a metric for the average discrepancy between distributions $p(\yrep\mid\thetav)$ drawn from the posterior and the \textsf{ppd}.

\begin{definition}[Posterior Sampling Divergence]
  For data $\yv$ and model $p(\thetav,\yv)$, the posterior sampling divergence is
  \begin{equation}
    \label{eq:psd}
    \mathsf{psd}\left( \yv \right) = \E_{p(\thetav\mid\yv)}D\left( p(\yrep\mid\thetav) \vert\vert p(\yrep\mid\yv) \right).
  \end{equation}
\end{definition}

Using the Donsker-Varadhan representation and Jensen's inequality, we get
\[
  \mathsf{psd}(\yv)\leq \E_{p(\thetav\mid\yv)}\left\lbrace \sup_{T:\mathcal{Y}\to\mathbb{R}}\left\lvert \E_{p(\yrep\mid\thetav)}T\left( \yrep \right) - \E_{p(\yrep\mid\yv)} T\left( \yrep \right) \right\rvert  \right\rbrace
\]

% Figure environment removed

In words, the \textsf{psd} lower bounds the degree to which typical sampling distributions $p(\yrep\mid\thetav)$ (with respect to the posterior) and the \textsf{ppd} can be distinguished by a statistic $T$. A large \textsf{psd} thus indicates increased risk of information loss when using a \textsf{ppp-v} compared to \eqref{eq:check_data}. An example shows how this information loss can be relevant for \textit{practical} model evaluation by making it difficult to falsify the model using a \textsf{ppp-v}. Let $\yv=(-10,10)$, with model
\begin{equation}
  \label{eq:bad_model}
  y_1,y_2\stackrel{iid}{\sim} \mathsf{student\text{-}t}\left( \theta,1,10 \right), \qquad \theta\sim \mathsf{uniform}\left( -15,15 \right),
\end{equation}
where $\mathsf{student\text{-}t}(\mu,\sigma,d)$ is the t distribution with location $\mu$, scale $\sigma$, and $d$ degrees of freedom.
This results in a multimodal posterior, plotted in the left panel of Figure \ref{fig:bad_model_posterior}. The right panel plots joint samples from the \textsf{ppd}, which is also bimodal despite the unimodality of the individual sampling distributions.

Consider the test statistics $T_1(y_1,y_2)=-y_1$ and $T_2(y_1,y_2)=y_2$, and let $p_{T_1}(\theta)$ and $p_{T_2}(\theta)$ be the corresponding conditional \textsf{ppp-v}s for $p(y_1,y_2\mid\theta)$. Figure \ref{fig:cond_pval_ex} plots these against $\theta$. The \textsf{ppp-v}s $p_{T_1}$ and $p_{T_2}$ are $\approx 0.165$, above usual thresholds for rejection and thus insufficient for falsification. However, the conditional $p$-values are vanishingly small over the bulk of the posterior support, suggesting that the model may be improved by introducing a scale parameter, or allowing the means to differ for the two observations, for example.

This example points towards a notion of power which does not make reference to the frequency properties of the \textsf{ppp-v}. Specifically, we will consider a model assessment to be underpowered if there is additional data (e.g. that contained in \eqref{eq:check_data}) which would lead us to consider the model fitness deficient (with respect to the data feature we are testing) despite the particular model assessment passing (i.e. indicating acceptable compatibility between data feature and model). Therefore, in light of the above, we view an increasing \textsf{psd} as increasing the risk of our chosen model assessments suffering power deficits. In these cases, we have to work harder to find strong evidence for the falsity of the model (e.g. by examining the conditional $p$-value plots in Figure \ref{fig:cond_pval_ex}), and in this sense the model exhibits weaker falsifiability.

% Figure environment removed

\subsection{Posterior Sampling Divergence and Model Expansion}

Since $\yv$ and $\yrep$ are conditionally independent given $\thetav$, it follows that
\begin{equation}
  \label{eq:psd_mi_id}
  \E_{p(\yv)}\mathsf{psd}\left( \yv \right) =\MI_{p(\thetav,\yv,\yrep)}\left( \thetav,\yrep\mid\yv \right).
\end{equation}

Then for base model $p_{\b}(\thetav,\yv)$ and expansion $p(\thetav,\lambdav,\yv)$, define
\[
\Delta_I = \E_{p(\lambdav)}\left[ \MI_{p(\thetav,\yrep,\yv\mid\lambdav)}(\thetav,\yrep\mid\yv) - \MI_{p(\thetav,\yrep,\yv\mid\lambdav_0)}(\thetav,\yrep\mid\yv) \right].
\]

Using the chain rule for \textsf{cmi}, we have that
\begin{equation}
  \label{eq:it_pval_power}
  \MI_{p}\left( (\thetav,\lambdav),\yrep\mid\yv \right)= \MI_{p_{\b}}\left( \thetav,\yrep\mid\yv \right) +\Delta_I + \MI_{p}\left(\lambdav,\yrep\mid\yv  \right).
\end{equation}

As before, the nonnegative term $\MI_{p}\left(\lambdav,\yrep\mid\yv  \right)$ creates an upward bias for the overall \textsf{cmi}.
In some simple examples, the \textsf{cmi} can be computed exactly:

\begin{enumerate}
\item For base model $y\mid\theta\sim \mathrm{normal}\left( \theta,1 \right)$ and $\theta\sim\mathrm{normal}\left( 0,1 \right)$, we get
  \[
    \MI(\thetav,\yrep\mid\yv) = h\left( \thetav\mid\yv \right) - h\left( \thetav\mid\yv,\yrep \right) = \log\left( 3/2 \right)/2.
  \]
  Now we add a redundant location parameter, so $\thetav=(\theta_1,\theta_2)$, and
  \[
    y\mid\thetav \sim\mathrm{normal}\left((\theta_1+\theta_2)/\sqrt{2}, 1\right), \qquad \theta_1,\theta_2\stackrel{iid}{\sim}\mathrm{normal}\left( 0,1 \right).
  \]
  After an invertible reparametrization $(\mu_1,\mu_2) = \phi(\theta_1,\theta_2)$, this model is
  \[
     y\mid\thetav \sim\mathrm{normal}\left(\mu_1, 1\right), \qquad \mu_1,\mu_2\stackrel{iid}{\sim}\mathrm{normal}\left( 0,1 \right).
   \]
   By invariance of the \textsf{cmi} under invertible reparametrization, we have
   \begin{equation*}
     \MI(\thetav,\yrep\mid\yv) = \MI(\muv,\yrep\mid\yv)=h\left( \mu_1\mid\yv \right)  - h\left( \mu_1\mid\yv,\yrep \right) = \log\left(3/2 \right)/2.
   \end{equation*}
 \item Now consider a normal location model with data $\yv\in\RR^{2n}$ for $n\geq 1$:
   \[
     \yv_i\stackrel{iid}{\sim}\mathrm{normal}\left( \theta,1 \right) \text{ for } 1\leq i\leq 2n, \qquad\theta\sim\mathrm{normal}\left( 0,1 \right).
   \]
   We expand this model by dividing $\yv$ as $\yv = \left(\yv^1,\yv^2  \right)$ with $\yv^1,\yv^2\in\RR^n$ and introducing separate means $\theta_1,\theta_2$, arriving at:
   \[
     \yv^j_i \stackrel{iid}{\sim}\mathrm{normal}\left( \theta_j,1 \right) \text{ for } 1\leq i\leq n \text{ and } j=1,2,\qquad \theta_1,\theta_2\stackrel{iid}{\sim}\mathrm{normal}\left( 0,1 \right).
   \]
   Now the \textsf{cmi} of the base model is $\mathrm{CMI}_{\mathrm{base}}(n) = \frac{1}{2}\log\left( \frac{4n+1}{2n+1} \right)$, whereas the \textsf{cmi} of the expanded model is $\mathrm{CMI}_{\mathrm{exp}}(n)=\log\left( \frac{2n+1}{n+1} \right)$. Figure \ref{fig:CMI_ex2} plots $(\mathrm{CMI}_{\mathrm{exp}}(n)-\mathrm{CMI}_{\mathrm{base}}(n))/\mathrm{CMI}_{\mathrm{base}}(n)$ against $n$. Clearly $\mathrm{CMI}_{\mathrm{exp}}(n) > \mathrm{CMI}_{\mathrm{base}}(n)$ for all $n$, and $\mathrm{CMI}_{\mathrm{exp}}(n)\to 2\mathrm{CMI}_{\mathrm{base}}(n)$ as $n\to\infty$.

   % Figure environment removed
   The change in \textsf{cmi} can be separated into two pieces. First, by splitting $\yv$, we reduce the data we have to estimate each of the means $\theta_1$ and $\theta_2$. This is reflected in the inequality $\log\left( \frac{4n+1}{2n+1} \right) > \log\left( \frac{2n+1}{n+1} \right)$. But parametrizing with two independent means adds a degree of freedom in the sampling distribution of the expanded model, doubling the constant factor, which dominates the comparison. However, the latter effect will not always determine the change in \textsf{cmi} between models, as the next example shows.
 \item We take the base model from the last example with $\yv\in\RR^n$ and expand it by adding a precision parameter and using a jointly normal-gamma prior:
   \[
     \yv_i\stackrel{iid}{\sim}\mathrm{normal}\left( \theta_1,\theta_2^{-1/2} \right) \text{ for } 1\leq i\leq n, \qquad (\theta_1,\theta_2)\sim\mathrm{NG}\left( 0,\mu_{\theta_2^{-1}},2,\mu_{\theta_2^{-1}} \right)
   \]
   Here, $\mu_{\theta_2^{-1}}>0$ is the prior mean of the variance $\theta_2^{-1}$. The marginal prior on $\theta_1$ is $\mathrm{normal}\left( 0,1 \right)$, matching the prior in the base model. Figure \ref{fig:CMI_ex3} shows estimated percentage changes in \textsf{cmi} from base to expanded model against $n$ for a range of noise levels $r=\mu_{\theta_2^{-1}}/n$.

   As before, increasing $n$ makes it easier to distinguish sampling distributions. Similarly, the added degree of freedom introduced by the precision parameter pushes the \textsf{cmi} larger. Hence, the majority of points in Figure \ref{fig:CMI_ex3} lie above $0$. However, unlike the last example, the \textsf{cmi} can decrease in the expanded model if $r$ is sufficiently large. This is because large values of $r$ create priors that favor sampling distributions with large scales that are correspondingly harder to distinguish. Nevertheless, the effect of the added degree of freedom dominates this comparison. For example, for $n=2$, the noise level in the base model is $r=0.5$. Unless the prior average noise level in the expanded model is more than double that of the base model (i.e. $r=1$), we can see that the \textsf{cmi} will increase.
   % Figure environment removed
 \item We now vary the prior scale in the model of the first example. Specifically, take $y\mid\theta\sim\mathrm{normal}\left( \theta,1 \right)$ and $\theta\sim\mathrm{normal}\left( 0,\sigma_p \right)$. This has \textsf{cmi} given by
   \begin{equation*}
     \frac{1}{2}\log\left( \frac{2\sigma^2_p + 1}{\sigma^2_p + 1} \right).
   \end{equation*}
   This is increasing in $\sigma_p$, and converges to $0$ as $\sigma_p\to 0$ and to $\frac{1}{2}\log(2)$ as $\sigma_p\to\infty$. In this case, there is no change in data size or degrees of freedom in the likelihood, and the \textsf{cmi} changes only because of the prior.
\end{enumerate}

In these examples we derived simple expressions for the \textsf{cmi} that depended on sample size, sampling variance, and prior variance. In most cases, these expressions increased to a finite upper bound in relevant limits, despite the fact that the \textsf{cmi} is unbounded above in general. The following lower bound in terms of the Fisher information demonstrates that this self-limiting behavior, as well as the dominance of the parameter dimension in driving increases the \textsf{cmi}, is not limited to these simple examples.

\begin{theorem}[Conditional Mutual Information Lower Bound]\label{thm:cmi_lb}
  For $M\geq 1$, define the $M$-replicated model:
  \[
    p\left( \yv^{(1)},\ldots,\yv^{(M)},\thetav \right) = p(\thetav)\prod_{i=1}^Mp\left( \yv^{(i)}\mid\thetav \right).
  \]
  Suppose for $M$ sufficiently large, we have that
  \begin{itemize}
  \item the posterior distributions $p\left(\thetav\mid\yv^{(1)},\ldots,\yv^{(M)}\right)$ are normal,
  \item the observed information matrix of $p(\yv,\thetav)$ is $\gamma$-subexponential for some $\gamma >0$ (i.e. the observed information does not have heavy tails),
  \item $\E\lambda_d(\covm)$, $\E\lambda^{-1}_1(\covm)$, $\E\lambda^2_d\left( \II(\thetav) \right)$, and $\E\lambda_1^{-1}(\II(\thetav)))$ are bounded by some $B>0$ where $\covm = \mathrm{Cov}\left( \thetav\mid\yp{1},\ldots,\yp{M} \right)$ (i.e. the posterior covariance and Fisher information are neither too small nor too large on average).
  \end{itemize}
  Then for $C$ a constant depending on $\gamma$ and $B$, we have
  \begin{equation}
    \label{eq:mi_fisher_lb_statement}
    \MI(\thetav,\yrep\mid\yv) \geq \frac{C}{\log d}\tr\left( \E_{p(\thetav,\yv)}\covm^{1/2}_{\yv}\II(\thetav)\covm_{\yv}^{1/2} \right),
  \end{equation}
 \end{theorem}

 \textit{Remarks:}
 \begin{itemize}
 \item The assumption that $\E\lambda^{-1}_1\left( \II(\thetav) \right) < B$ rules out singular models with $\lambda_1(\II(\thetav))=0$. However, such models can often be reparametrized as $\thetav' = \Psi(\thetav)$ using some $\Psi:\RR^d\to\RR^r$ with $r<d$ such that $\II(\thetav')$ is nonsingular. Applying the result to such a parametrization gives a lower bound for the original \textsf{cmi} $\MI(\thetav,\yrep\mid\yv)$.
 \item Normality of $p\left(\thetav\mid\yv^{(1)},\ldots,\yv^{(M)}\right)$ for $M\geq 1$ sufficiently large is almost certainly not satisfied unless it is satisfied for $M=1$. Nevertheless, if the Bernstein-von-Mises theorem holds, we would expect $p\left(\thetav\mid\yv^{(1)},\ldots,\yv^{(M)}\right)$ to be nearly normal for large $M$ even if it is far from normal for $M=1$. We thus conjecture a similar bound for more general posteriors.
 \end{itemize}

We note that our bound depends on the parameter dimension through the number of terms in the trace, and the other influences on the \textsf{cmi} observed in the above examples enter through the magnitudes of these terms. the self-limiting phenomenon can be seen in this bound through this multiplication of covariance and information matrices. For instance, in a Bernstein-von-Mises type limit, the posterior concentrates around the true parameter $\thetav_0$, we get $\covm_{\yv}\approx\II(\thetav_0)$, and the bound becomes $Cd/\log d$, which is independent of all non-dimensional factors.

 The dependence on the model dimension in this bound comes directly through the dimensions of the covariance and information matrices. The sampling and prior effects appear through the individual eigenvalues of the posterior-normalized Fisher information matrix. We also note that the self-limiting phenomenon can be seen in this bound through this multiplication of covariance and information matrices. For instance, in a Bernstein-von-Mises type limit, the posterior concentrates around the true parameter $\thetav_0$, we get $\covm_{\yv}\approx\II(\thetav_0)$, and the bound becomes $Cd/\log d$. 

\section{Marginal Entropy and Sampling Divergence}\label{sec:id_check_connection}

Let $p_{\b}(\yv,\thetav)$ and $p(\yv,\thetav,\lambdav)$ be a base model and expansion. Consider the following two extreme scenarios:
\begin{itemize}
\item Let $p_{\b}(\yv\mid\thetav) = q(\yv)$ for a density $q$. Then the likelihood is constant, and $h_{p_{\b}}(\thetav) = h_{p_{\b}}(\thetav\mid\yv,\yrep)$ for all $\yv$ and $\yrep$. It follows immediately that $\MI(\thetav,\yv)=\MI(\thetav,\yrep\mid\yv)=0$, and so any nontrivial expansion of $p_{\b}$ with $\thetav$ and $\lambdav$ independent must decrease the marginal posterior entropy of $\thetav$ and increase the \textsf{cmi}.
\item Let $p_{\b}(\yv\mid\theta) = \mathrm{normal}(\yv\mid \theta,1)$, and then take the expansion $p(\yv\mid\theta,\lambda) = p_{\b}(\yv\mid\theta+\lambda)$ with priors $\theta\sim\mathrm{normal}(0,\sigma_{\theta}^2)$ and $\lambda\sim\mathrm{normal}\left( 0,\sigma_{\lambda}^2 \right)$. We then have for the expanded model that
  \[
    \MI(\thetav,\yv) = \frac{1}{2}\log\left( 1 + \frac{\sigma^2_{\theta}}{1+\sigma^2_{\lambda}}\right), \quad \MI\left( (\thetav,\lambdav),\yrep\mid\yv \right) = \frac{1}{2}\log\left( \frac{1+2(\sigma^2_{\theta}+\sigma^2_{\lambda})}{1 + \sigma^2_{\theta} + \sigma^2_{\lambda}} \right).
  \]
  Similarly, the \textsf{cmi} in the base model is
  \[
    \MI\left( \thetav,\yrep\mid\yv \right) = \frac{1}{2}\log\left( \frac{1+2\sigma^2_{\theta}}{1+\sigma^2_{\theta}} \right).
  \]
  Both \textsf{cmi} expressions are bounded above by $\frac{1}{2}\log(2)$. Furthermore, by taking $\sigma^2_{\theta}$ large, we can ensure that the \textsf{cmi} for the base model is arbitrarily close to this limit. Then the \textsf{cmi} can increase only negligibly in the expanded model regardless of $\sigma_{\lambda}^2$. However, fixing $\sigma^2_{\theta}$, the \textsf{mi} in the expanded model tends to $0$ as $\sigma^2_{\lambda}\to\infty$. 
\end{itemize}

In each case, one of the models has singular Fisher information, and either the \textsf{mi} or \textsf{cmi} degrades (i.e. decreases or increases respectively) while the other changes negligibly. When both the base model and expanded model have nonsingular Fisher information, we can show that in certain cases there is a strict trade-off asymptotically between worsening (i.e. decreasing) \textsf{mi} and worsening (i.e. increasing) \textsf{cmi}.

\begin{theorem}\label{thm:strict_tradeoff}
  Let $p\left( \thetav,\lambdav,\yv \right)$ be an expansion of $p_{\b}(\thetav,\yv)$, and let  $\{\iota_i\}_{i=1}^d$ and $\{\iota^{\mathrm{cond}}_i\}_{i=1}^d$ be the eigenvalues of $\E \II_{\b}(\thetav)$ and $\E\II(\thetav\mid\lambdav)$ respectively.
  
  Furthermore, suppose that the conditions of Theorems \ref{thm:mi_ub}, \ref{thm:marginal_trace_bound}, and \ref{thm:cmi_lb} hold. In particular this requires that:
  \begin{enumerate}
  \item the marginal priors $p_{\b}(\thetav)$ and $p(\thetav)$ are log concave.
  \item the posteriors $p_{\b}(\thetav\mid \yv_M)$ and $p(\thetav,\lambdav\mid\yv_M)$ are normal for all $\yv_M$ for $M$ large enough, where $\yv_M$ is a vector of $M$ i.i.d. replicated draws from $p_{\b}(\yv\mid\thetav)$ and $p(\yv\mid\thetav,\lambdav)$ respectively,
  \item the expected spectra of the Fisher information matrices $\II_{\b}(\thetav)$ and $\II(\thetav,\lambdav)$ are bounded above and below by universal constants, and similarly for the expected spectra of the posterior covariance matrices,
  \item and a few other regularity conditions on the smoothness of the relevant densities and tails of the observed information.
  \end{enumerate}
  Additionally, we further assume that the distributions of the Fisher information matrices and posterior covariance matrices are not too skewed in the sense of Lemma \ref{lem:exp_exchange_lb} in Appendix \ref{app:heuristic_connection}. This ensures that the (random) Fisher information and covariance matrices are sufficiently well-summarized by their means.

  Under these conditions, for increasing functions $\psi_1$ and $\psi_2$, we have the following inequalities:
  \begin{equation}\label{eq:tradeoff_base_bounds}
    \MI_{\b}\left(\yv,\thetav\right) \leq \psi_1\left(\sum_{j=1}^d \iota_j\right), \qquad\MI_{\b}\left(\yrep,\thetav\mid\yv \right) \geq  \frac{1}{\log d}\sum_{i=j}^d\psi_2\left(\iota_j\right)
  \end{equation}
  \begin{gather}\label{eq:tradeoff_exp_bounds}
    \MI\left(\yv,\thetav\right) \leq \psi_1\left(\sum_{j=1}^d \iota^{\mathrm{cond}}_j\right) - \Delta_i,\\
    \MI\left(\yrep,(\thetav,\lambdav)\mid\yv \right) \geq \frac{1}{\log d^{\mathrm{exp}}}\left[\sum_{j=1}^d\psi_2\left(\iota^{\mathrm{cond}}_j\right) + \Delta_f\right],
  \end{gather}
  where $\Delta_f\geq 0$, and $\Delta_i\geq 0$ so long as knowledge of $\yv$ does not decrease the information that $\lambdav$ provides about $\thetav$ in the sense that $\sum_{j=1}^d \Delta_j \leq 0$ (where the $\Delta_j$ are defined in Theorem \ref{thm:marginal_trace_bound}).
  Furthermore, if $p$ is a totally diluting expansion of $p_{\b}$, then we have
  \begin{equation}\label{eq:tradeoff_diluting}
    \psi_i\left(\sum_{j=1}^d \iota^{\mathrm{cond}}_j\right)\leq \psi_i\left(\sum_{j=1}^d \iota_j\right),
  \end{equation}
  and if $p$ is a totally nondiluting expansion of $p$, then we have
  \begin{equation}\label{eq:tradeoff_nondiluting}
    \sum_{j=1}^d\psi_f\left(\iota^{\mathrm{cond}}_j\right) \geq \sum_{j=1}^d\psi_f\left(\iota_j\right).
  \end{equation}
\end{theorem}
\begin{proof}
  See Appendix \ref{app:heuristic_connection}.
\end{proof}
This result substantially generalizes the pattern we observed in our introductory regression example, where predictor correlation structures that offered better identification created greater posterior uncertainty about the sampling distribution and vice versa. We can interpret this theorem in a positive and negative light. Negatively, when our prior information is relatively unstructured, the process of model expansion may force us to confront either weakly identified marginal inferences or a large posterior sampling divergence. Positively, a prior with sufficient dependence between the parameters may allow us to avoid these difficulties, even if this prior is weak in the sense of carrying relatively little marginal information about any particular parameter. In the next sections, we explore examples where these two difficulties occur and demonstrate how they can be partially overcome with sufficiently rich posterior summaries. 

\section{Example: Inference Under Poor Identification}\label{sec:weak_id_practice}

The next two sections explore methodological implications of the above results with concrete examples. Here we consider an example in which we get `stuck' between an implausibly simple base model and an expanded model where the parameter of interest is too weakly identified to allow strong conclusions. We then show how the expanded model can still support nontrivial inferences which, in this case, inform how we should collect future data. In so doing, we hope to demonstrate (i) how the concerns raised in Sections \ref{sec:weak_id_theory} and \ref{sec:id_check_connection} motivate looking beyond standard marginal posterior summaries, and (ii) that weak identification need not be an inferential dead end for a statistical analysis.

\subsection{Two Models for Grouped Data}

We first define three simulated data sets, each generated by drawing random samples of $M$ measurements from each of $L$ subpopulations. We take $M=2$ and $L=20$ for all data sets, but vary the ratio of within- and between-subpopulation variances between them. Appendix \ref{app:id_sim_details} provides a complete description of the data generating process. Figure \ref{fig:id_data_plot} plots the data, with a row for each subpopulation, a column for each data set, and dots for the individual measurements.

% Figure environment removed

The unobserved grand (i.e. superpopulation-level) mean will be our quantity of interest.
We also take more positive values of the grand mean to represent ``better'' outcomes (i.e. more desirable from the researcher's standpoint). We think about the identification of the grand mean in two ways:
\begin{enumerate}
\item In terms of the reduction in entropy from marginal prior to posterior or the ratio of their standard deviations (which is just a monotonic transform of the former when both distributions are normal). This notion is general, but being unit-free, it is somewhat unnatural for drawing practical conclusions.
\item The posterior probability that the grand mean is below a threshold of practical significance. Specifically, we consider the grand mean to be practically significant only if it exceeds $1$.
\end{enumerate}

While these notions are distinct, greater entropy will tend to be associated with larger probability of a not-practically-significant effect (so long as the prior is sufficiently constraining in the extremes).

\subsubsection{An Oversimplified Initial Model}

In our base model, we assume the subpopulation-level distributions are identical (so the subpopulation means equal the grand mean). Letting $\yv_{ml}$ be the data with $1\leq m\leq M$ indexing measurements and $1\leq l\leq L$ indexing subpopulations, the sampling distribution is $\yv_{ml}\stackrel{iid}{\sim} \mathrm{normal}\left( \mu,\sigma_{*} \right)$.

Here $\mu$ represents the grand mean, to which we assign prior $\mathrm{normal}\left( 0,\left\lvert \mu_{0}\right\rvert \right)$, where $\mu_0$ is the ``true'' value used in simulating the three data sets. We treat $\sigma_{*}$ as a hyperparameter which we set to a prior guess.

Figure \ref{fig:base_model_post} shows histograms of draws from the posteriors for each data set along with the prior density. For each data set, the posterior standard deviation is approximately a quarter of the prior standard deviation, and the probabilities of a practically insignificant effect (i.e. $\mu < 1$) are between $2\%$ and $10\%$. Overall, we have improved on our prior knowledge, but the posterior cannot fully rule out the possibility of an insignificant effect.

% Figure environment removed

\subsubsection{A More Plausible Expanded Model}

Two features of the base model stand out for criticism:
\begin{enumerate}
\item We usually cannot confidently guess the scale $\sigma_*$ with just prior information.
\item If we regard the subpopulations as distinct for data collection purposes, then we likely have reason to believe their distributions could be distinct.
\end{enumerate}

Our expanded model thus adds a parameter for the subpopulation scale and allows the subpopulations to have distinct means (drawn from some superpopulation). In symbols,
\begin{equation}
  \label{eq:weak_id_exp_model}
  \begin{gathered}
  \yv_{ml} \stackrel{iid}{\sim} \mathrm{normal}\left(\theta_l,\sigma \right), \qquad \theta_l \stackrel{iid}{\sim}\mathrm{normal}\left(\mu,\tau\right),\\
  \mu\sim\mathrm{normal}\left(0,\lvert \mu_0 \rvert\right), \qquad\sigma \sim \mathrm{gamma}\left(a_\sigma,b_\sigma\right),\qquad  \tau\sim\mathrm{gamma}\left(a_{\tau},b_{\tau}\right).
  \end{gathered}
\end{equation}
In the above, $a_{\sigma},b_{\sigma},a_{\tau},b_{\tau}$ are hyperparameters of the model, and $\mu_0$ is unchanged from the base model. The hyperparameters $a_{\sigma}$ and $b_{\sigma}$ are taken such that the prior mode for $\sigma$ equals our prior guess $\sigma_{*}$. The prior on $\mu$ is unchanged from the base model, and $\mu$ again corresponds to the grand mean. Setting $\sigma=\sigma_{*}$ and $\tau=0$ recovers the base model as a special case.

Figure \ref{fig:exp_model_post} shows the posterior inferences for $\mu$ in the expanded model. In the first and third data sets, the posterior standard deviation is $\approx 1/3$ larger than in the base model. For the second data set, it is more than twice as large as in the base model. In all cases, the standard deviation is at least a third that of the prior. Similarly, the probabilities of a practically insignificant effect are all at least three times larger than in the base model, ranging from $\approx 6\%$ in the best case to over $30\%$.

% Figure environment removed

We might now return to the base model or focus on subpopulation means to get better identification. However, the former is inadvisable since the base model was implausible, and the latter will likely fail since the subpopulation sample sizes are tiny. A less convenient but much safer approach is simply to gather more data. In fact, inferences from the expanded model can yield relevant information for future data collection despite the weak identification.

\subsection{A Bootstrap Comparison of Two Sampling Schemes}

It has been recognized that bootstrapping new datasets, fitting a Bayesian model to each, and then aggregating the results can provide more information than the fit to the observed data alone \cite{BayesBag}. This idea has been used, e.g., for model criticism and selection tasks \cite{BayesBagSelect,BayesBagCrit}. Here, we demonstrate that a similar procedure with a Bayesian parametric bootstrap of future data can help us learn about how to sample such data despite the underwhelming marginal identification of the posterior.

First, we must identify the candidate sampling schemes. We may sample the same subpopulations that we sampled originally, or we may sample from new subpopulations, or some combination of these. If the within-subpopulation variance is low, then sampling the same subpopulations may yield little improvement for identification. If instead the between-subpopulation variance is low, sampling the same subpopulations may provide enough information about the individual $\theta_l$ to identify $\mu$ well. The issue may be further complicated by cost differences. For our purposes, we assume that a sample from a new subpopulation is four times the cost of a sample from an existing subpopulation.

To simplify our comparison, we focus on whether it is more efficient to sample \textit{exclusively} from existing subpopulations or from new subpopulations. To answer this, we use the joint posterior over parameters and replicated data to simulate sampling new data with these two schemes. We then compare posteriors on $\mu$ after refitting the model to these enlarged data sets. Replicating many times then allows us to assess the relative risks of each approach. Figure \ref{fig:diagram_boot} diagrams these resampling schemes with full pseudocode in Appendix \ref{app:id_sim_details}.
% Figure environment removed
We take $M^{\mathrm{new}}=8$ additional samples from each of the $L=20$ existing subpopulations in the first scheme and $M=2$ samples from $L^{\mathrm{new}}=20$ new subpopulations in the second scheme. These are performed at equivalent overall cost, hence the difference in total sample size. We replicate each scheme $R=500$ times. We also repeat this process with the prior replacing the posterior at each step in order to demonstrate that the posterior inferences differ from prior inferences despite the marginal weak identification.

For each of the $r=1,\ldots,R$ replications and each method, define the ratio $\rho_{r} = \sigma^{(r)}/\sigma_{\mathrm{obs}}$,
where $\sigma^{(r)}$ is the posterior standard deviation of $\mu$ for the $r^\mathrm{th}$ expanded data set simulated from the given method, and where $\sigma_{\mathrm{obs}}$ is the posterior standard deviation of $\mu$ given just the observed data set. Table \ref{fig:boot_results} presents the averages of these ratios over all replications $\overline{\rho}=\frac{1}{R}\sum_{r=1}^R\rho_{r}$ for each method and each of our observed data sets.

Despite weak identification, the posterior and prior columns differ, indicating that our simulated data reflects information learned from the observed data. Furthermore, the average improvement to identification for each scheme depends substantially on the data set, with each scheme winning in one data set and tying in the third.

Figure \ref{fig:boot_compare} gives a finer-grained comparison, plotting histograms of the $\rho_{r}$ for the two schemes. While the first data set yields a tie in the comparison of averages, the distribution of $\rho_r$ is wider when sampling the same subpopulations than when sampling new subpopulations. This suggests a risk-reward trade-off: the latter scheme gives a more predictable reduction in uncertainty while the former carries the possibility of a greater reduction. Together this demonstrates that we can get nontrivial inferences which depend strongly on the particular data observed despite weak identification.

% Figure environment removed

% Figure environment removed

\section{Example: Conditional Model Checking}\label{sec:underpowered_check_practice}

We next consider a model where the conditional \textsf{ppp-v}s contain substantially more information than the marginal $p$-value (i.e. the regular \textsf{ppp-v}) for a relevant test statistic. This conditional information motivates a modification which further improves model fitness and resolves a problem that was masked by the marginal $p$-value.

We take as our base model the election forecasting model of \cite{ElectionForecast}. We pay particular attention to herding, a process in which pollsters systematically augment their raw data in such a way that their published poll numbers are closer to the existing consensus of recent polls than they would otherwise be. That the variance among presidential election polls fell well below the minimal expected variance (assuming independence between polls) just before the election has been used as evidence for the existence of herding in 2016 \cite{PollError}. As this model does not explicitly account for herding, this will be starting point of our model checking.

\subsection{The Base Forecasting Model and a Check for Herding}
A more complete discussion of the model specification is given in Appendix \ref{app:forecasting_spec}, and full details can be found in \cite{ElectionForecast}. The primary purpose of the model is to infer the level of support for the Democratic candidate over time and across states. This level of support is represented by a matrix parameter $\muv\in\RR^{S\times T}$ with rows representing the $S=51$ states (including Washington DC) and columns representing the $T$ days from the start of measurement until election day. This parameter is assigned a time series prior:
\begin{equation}\label{eq:dem_support_prior}
  \muv_{t}\mid\muv_{t+1} \sim\mathrm{normal}\left( \muv_{t+1}, \covm^{\mu} \right) \text{ for } 1\leq t\leq T-1, \text{ and } \muv_T\sim \mathrm{normal}\left( \mathbf{m}^{\mathrm{f}},\mathbf{S}^{\mathrm{f}} \right).
\end{equation}
Here $\covm^{\mu}\in\RR^{S\times S}$ is a hyperparameter encoding correlation between states and variation over time, constructed using demographic data, polling from previous elections, and domain knowledge. Likewise, $\mathbf{m}^f\in\RR^S$ and $\mathbf{S}^f\in\RR^{S\times S}$ are hyperparameters set using a `fundamentals forecast' derived from variables known in political science to be good predictors of U.S. election outcomes.

Results of state and national polls are modeled with a binomial distribution that combines $\muv$ with terms representing sources of polling bias. Letting $i=1,\ldots,N_{\mathrm{state}}$ index state polls and $y_i$ denote the number of respondents supporting the Democratic candidate out of $n_i$ respondents in poll $i$, we have
\begin{equation}
  \label{eq:poll_data_model}
  y_i \sim \mathrm{binomial}\left(\mathsf{logit}^{-1}\left( \muv_{s_i,t_i}+\beta_i\right), n_i \right),
\end{equation}
where $s_i$, $t_i$ denote the state and day for poll $i$ and $\beta_i$ models various sources of bias (see Appendix \ref{app:forecasting_spec} for further discussion).

National polls are modeled similarly, except state-level terms including $\muv_{st}$ are averaged with weights accounting for each state's share of the national vote in the previous election.

Since most national polls lie in $(0.4,0.6)$ in any close race, the binomial sampling model \eqref{eq:poll_data_model} effectively places a lower bound of $\sqrt{0.6\times 0.4/n_i}\approx 0.49/\sqrt{n_i}$ on the standard deviation of national poll $i$. Thus, this model will be incompatible with sufficiently low poll variance over any interval with enough published polls. Figure \ref{fig:ppc_base} compares the observed variation in the last ten days with posterior predictive replications and shows that the observed polling variance is substantially smaller than in posterior simulations.

% Figure environment removed
\subsection{A Simple Model of Herding}
By allowing dependence among polls, adding a herding mechanism may resolve this tension between model and data. First define $\theta^{\mathrm{state}}_i = \mathsf{logit}^{-1}\left( \muv_{s_i,t_i}+\beta_{i}\right)$ and $\theta^{\mathrm{nat}}_j = \mathsf{logit}^{-1}\left( \overline{\mu}_{t_j}+\overline{\beta}_j\right)$ for $1\leq i\leq N^{\mathrm{state}}$ and $1\leq j\leq N^{\mathrm{nat}}$, where $\overline{\mu}_{t_j}$ and $\overline{\beta}_j$ are weighted averages of the means and biases over the states. The $\theta$ represent the expected average support for the Democratic candidate in random samples drawn from the sampling frame of the corresponding poll. We model the sampling process of national poll $j$ as follows.
\begin{enumerate}
\item The pollster samples their sampling frame, observing unherded result:
  \[
    y_j^{\mathrm{nat}}\mid\theta^{\mathrm{nat}}_j \sim\mathrm{binomial}\left( \theta_j^{\mathrm{nat}}, n_j^{\mathrm{nat}} \right).
  \]
\item The pollster calculates a herding target $\mu^{\mathrm{herd}}_j$ representing their quantification of the consensus of polls to which they compare their $y^{\mathrm{nat}}_j$.
\item The pollster herds $y_j^{\mathrm{nat}}$ by some fraction $\lambda_j^{\mathrm{herd}}\in (0,1)$ towards $\mu^{\mathrm{herd}}_j$. They publish this herded result:
  \[
    p_j^{\mathrm{nat}} = \left( 1-\lambda^{\mathrm{herd}}_j \right)\frac{y_j^{\mathrm{nat}}}{n^{\mathrm{nat}}_j} + \lambda^{\mathrm{nat}}_j\mu_j^{\mathrm{herd}}.
  \]
\end{enumerate}

To simplify the implementation of this scheme, we use the normal approximation to the binomial, arriving at
\begin{align}
  \label{eq:herd_sampling}
 \frac{y_j^{\mathrm{nat}}}{n^{\mathrm{nat}}_j} \;\Bigg\vert\; \theta^{\mathrm{nat}}_j &\sim\mathrm{normal}\left( \theta_j^{\mathrm{nat}},\sqrt{\theta_j^{\mathrm{nat}}\left( 1-\theta_j^{\mathrm{nat}} \right) \Big/ n^{\mathrm{nat}}_j} \right),\nonumber\\
  p_j^{\mathrm{nat}}\mid\theta_j^{\mathrm{nat}},\mu^{\mathrm{herd}}_j,\lambda_j^{\mathrm{herd}} &\sim \mathrm{normal}\Big(  \left( 1-\lambda^{\mathrm{herd}}_j \right)\theta^{\mathrm{nat}}_j + \lambda^{\mathrm{nat}}_j\mu_j^{\mathrm{herd}},\\
  &\hspace{2.1cm} \left( 1-\lambda^{\mathrm{herd}}_j \right)\sqrt{\theta_j^{\mathrm{nat}}\left( 1-\theta_j^{\mathrm{nat}} \right)\Big/ n^{\mathrm{nat}}_j}\Big).
\end{align}

The herding model for the $1\leq j\leq N^{\mathrm{state}}$ polls of states $1\leq s\leq 51$ uses analogous targets $\mu^{\mathrm{herd}}_{s_j,j}$ and percentages $\lambda^{\mathrm{herd}}_{s_j,j}$, where $s_j$ is the state in which poll $j$ was conducted. Now let $m^{\mathrm{nat}}$, $s^{\mathrm{nat}}$, $m^{\mathrm{state}}_s$ and $s^{\mathrm{state}}_s$ be the sample averages and standard deviations of the national polls and polls of state $1\leq s\leq 51$ respectively. We set the following priors for these parameters.
\begin{equation}
  \label{eq:bad_model_priors_mu}
  \mu^{\mathrm{herd}}_i \stackrel{iid}{\sim} \mathrm{normal}\left( m^{\mathrm{nat}},\frac{2}{3}s^{\mathrm{nat}} \right),\quad \mu^{\mathrm{herd}}_{s_j,j} \stackrel{iid}{\sim} \mathrm{normal}\left( m^{\mathrm{state}}_{s_j}, \frac{2}{3}s^{\mathrm{state}}_{s_j} \right).
\end{equation}
These essentially just constrain the herding targets to lie in $\pm 2$ standard deviations of the overall polling means, keeping them away from the extremes of our data. Next let $\mathcal{C}=\{1\leq i\leq N^{\mathrm{nat}}\mid t_i \geq T-10\}$ be the indices of national polls conducted in the last ten days prior to the election. We then set priors:
\begin{align}
  \lambda^{\mathrm{nat}}_i,\lambda^{\mathrm{state}}_{j,s_j}&\stackrel{iid}{\sim} \mathrm{Beta}\left(1 ,9 \right),  \text{ for all } 1\leq j\leq N^{\mathrm{state}} \text{ and } i\in \mathcal{C}^c,\nonumber\\
  \lambda^{\mathrm{nat}}_i\Bigg\vert\mu^{\mathrm{last}}_{\lambda},k^{\mathrm{last}}_{\lambda} &\stackrel{iid}{\sim} \mathrm{Beta}\left( \mu^{\mathrm{last}}_{\lambda}k^{\mathrm{last}}_{\lambda},\left( 1-\mu^{\mathrm{last}}_{\lambda} \right)k^{\mathrm{last}}_{\lambda} \right),  \text{ for all } i\in\mathcal{C},\label{eq:bad_model_priors_lambda}
\end{align}
where we use hyperpriors $\mu^{\mathrm{last}}_{\lambda}\sim\mathrm{uniform}\left[ 0,1 \right]$ and $k^{\mathrm{last}}_{\lambda}\sim\mathrm{normal}\left( 200,120 \right)$. We also constrain the $\lambda$ explicitly to $[0,0.9]$ since values too close to $1$ create adverse geometry that frustrates the sampler, and since our prior beliefs rules out such values. The hierarchical prior on the last ten days of national herding parameters serves two purposes. Since the polls in this period are our primary evidence for herding, inference for $\mu^{\mathrm{last}}_{\lambda}$ and $k^{\mathrm{last}}_{\lambda}$ is of substantive interest. Also, since data is especially dense in this period, we can better estimate the herding parameters for these polls (and can thus afford the weaker marginal priors implied by the hierarchical structure).

Figure \ref{fig:ppc_bad} displays the posterior predictive check of polling variation in the last ten days for the expanded model. While the observed variation is still relatively small, it is no longer implausible. We may now be tempted to stop and declare our model good enough, as it is not obvious what further improvements to make. We can, however, extract more information than is revealed by this (marginal) posterior predictive check.

% Figure environment removed

\subsection{A More Informative Conditional Model Check}

Recall that the conditional posterior predictive $p$-values (c\textsf{ppp-v}s) are defined for test statistic $T$ as $p_T(\thetav) = \E_{p(\yrep\mid\thetav)}\mathbbm{1}\left\lbrace T(\yrep) \geq T(\yv) \right\rbrace$.
The top panel of Figure \ref{fig:cppc_bad} plots the c\textsf{ppp-v}s with $T$ equal to the standard deviation of the last ten days of national polls against posterior samples of $\mu^{\mathrm{last}}_{\lambda}$, the average herding percentage in the last ten days of polling. The marginal distributions displayed on the axes show that the distribution of c\textsf{ppp-v}s has a heavy right tail with the preponderance of sampled $p$-values less than the average of $0.05$. Furthermore, the fit of the model appears much better for larger values of $\mu^{\mathrm{last}}_{\lambda}$, suggesting that model fitness may improve if the posterior favored more herding. But it is unclear \textit{how} we should achieve this as using a more informative prior to favor higher values of $\mu^{\mathrm{last}}_{\lambda}$ would be inconsistent with our (lack of) prior knowledge.
% Figure environment removed
Another comparison tells us more. The bottom panel of Figure \ref{fig:cppc_bad} plots the same cccp-vs against the standard deviation of the herding targets $\mu^{\mathrm{herd}}_i$ in the last ten days before the election. Less variability in the herding targets is associated with better fit, indicating another avenue for improvement.

By leveraging only the gross features of the poll results, the prior on $\mu^{\mathrm{herd}}_i$ was designed to be weakly informative. However, this prior only enforces that $\mu^{\mathrm{herd}}_i$ not be extreme compared to all polls in the series. But clearly $\mu^{\mathrm{herd}}_i$ should also be non-extreme compared to polls around the \textit{specific} time that poll $i$ was conducted. Including this prior information should reduce the posterior standard deviation of herding targets and thus hopefully improve the fit.

\subsection{Using the Conditional Check to Improve the Model}

To construct a new prior on the targets $\mu^{\mathrm{herd}}_i$, we calculate the trailing averages
\begin{equation}
  \label{eq:trailing_avg}
  c^i_d = \frac{1}{\lvert \mathcal{C}_{t_i}^d \rvert}\sum_{k\in\mathcal{C}^i_d}\frac{y_k^{\mathrm{nat}}}{n_k^{\mathrm{nat}}}
\end{equation}
for $d=2,\ldots,7$ days before each national poll $i$, where $\mathcal{C}^d_t = \left\lbrace 1\leq i\leq N^{\mathrm{nat}}\mid t-d \leq t_i\leq t\right\rbrace$.

We then define the average and spread of these:
\begin{equation}
  \label{eq:herd_prior_hyperparams}
  m_c^i = \frac{1}{6}\sum_{d=2}^7 c^i_d, \text{ and } s_c^i = \sqrt{\frac{1}{6}\sum_{d=2}^7 \left(c^i_d-m_c^i\right)^2} + 0.0002.
\end{equation}
The addition of a small constant $2^{-4}$ ensures the spread cannot shrink to $0$ when data is sparse. We then define the improved prior $\mu_i^{\mathrm{herd}} \sim \mathrm{normal}\left( m_c^i,s_c^i \right)$, with priors for the state herding targets constructed similarly from the state-level time series. This prior now encodes the idea that $\mu_i^{\mathrm{herd}}$ should look like a consensus of \textit{recent} polls.

% Figure environment removed

Figure \ref{fig:cppc_better} displays the same pair of conditional posterior predictive checks using this more informative prior. The (marginal) \textsf{ppp-v} is nearly $0.3$, and the unpleasant `spike and slab' shape of the c\textsf{ppp-v}s that appeared in Figure \ref{fig:cppc_bad} has been attenuated. Furthermore, we no longer find intervals of large posterior probability where the c\textsf{ppp-v}s are vanishingly small in either plot. Thus, the marginal check looks good, and the conditional check no longer indicates obvious directions for improvement.

The improvement in c\textsf{ppp-v}s versus the standard deviation of $\mu_i^{\mathrm{herd}}$ is expected, but the improvement in the comparison with the mean herding percentage (achieved by higher mean herding in the posterior) may be somewhat surprising. The source of the improvement is revealed in Figure \ref{fig:herd_percent_target}, which displays a strong negative association between the standard deviations of the $\mu_i^{\mathrm{herd}}$ and the $\mu^{\mathrm{last}}_{\lambda}$ in the last ten days for the first expanded model. Thus, solving the problem for the herding targets also solved the herding percentage problem for free.

% Figure environment removed

This example shows that the concern of Section \ref{sec:underpowered_check_theory} that marginalized model checks like the \textsf{ppp-v} could obscure information useful for assessing model fitness is more than purely theoretical. More positively, it also clearly demonstrates how the c\textsf{ppp-v} can be a powerful tool for motivating specific model improvements, especially when the marginal \textsf{ppp-v} is neither implausibly small nor reassuringly large.

\section{Conclusions} \label{sec:discussion}

When constructing a model for a given data analysis, a statistician should balance various desiderata, including:
\begin{itemize}
\item model predictions compatible with what is known about the world;
\item inferences sufficiently well identified to support nontrivial conclusions;
\item model checks powerful enough to reveal frictions between model and data.
\end{itemize}

When model checks reveal deficiencies, the first item is no longer satisfied, and a better model should be sought. In practice, this is often an expansion of the previous model. If such an expansions are not accompanied by sufficiently strong prior information (in the form of prior dependence of parameters, not the marginal scales), then our results demonstrate that a tension may easily arise within these three goals as the model dimension grows. Insofar as the first desideratum is most essential, this motivates methods that can extract useful information even when one of the latter two desiderata are not satisfied. One avenue is to pursue richer inferential summaries. As demonstrated by the last two examples, the full posterior often contains such rich inferential data. This data is both accessible (by looking beyond e.g. marginal means, standard deviations, and $p$-values) and capable of supporting nontrivial conclusions (which are concealed by the common marginal summaries).

Many directions for future work remain. Determining whether the trade-off observed in Lemma \ref{lem:heuristic_connection}, which depended on Fisher information-based bounds, could be strengthened (e.g. to a relation between information theoretic quantities directly) would be of particular interest. 
We would also like to have better tools for extracting joint inferences from the posterior. For example, the posterior predictive resampling we performed in Section \ref{sec:weak_id_practice} could become prohibitively expensive in large models, and methods to rapidly approximate such results would be useful. More broadly, there is at the time of writing no canonical method for estimating conditional analogs of the usual marginal summaries (e.g. means and variances), which would aid the study of joint inferences in practice. 

\nocite{LogSobolevBayes}
\nocite{BayarriBerger99}
\nocite{BayarriBerger00}
\nocite{RobinsEtAl00}
\nocite{EntropyLowerBound}
\nocite{PostDiscrep}
\nocite{PostP}
\nocite{PostPPharma}
\nocite{BayesWorkflow1}
\nocite{BayesWorkflow2}
\nocite{DataCloning}
\nocite{BayesLearning}
\nocite{ElectionForecast}
\nocite{PollError}
\bibliographystyle{plain} 
\bibliography{model-expansion-bayes}

\appendix

\section{Basic Quantities and Relations from Information Theory}\label{app:it_overview}
In this section, we provide statements of the basic results from information theory that we make use of throughout this paper. Proofs of these results can be found in any introductory course on information theory. We state all results in terms of conditional entropies and mutual informations when appropriate since these contain the non-conditional statements as special cases. First we review relevant definitions.
\begin{definition}[Basic Quantities of Information Theory]
  Let $q(\thetav,\yv)$ be some joint model. Then the entropy of $\thetav$ is defined as
  \begin{equation}
    \label{eq:entropy_def}
    h_{q(\thetav)}(\thetav) = -\E_{q(\thetav)}\log q(\thetav).
  \end{equation}
  The conditional entropy of $\thetav$ given $\yv$ is just the average entropy of the conditional distributions:
  \begin{equation}
    \label{eq:cond_entropy_def}
    h_{q(\thetav,\yv)}\left( \thetav\mid\yv \right) = \E_{q(\yv)}h_{q(\thetav\mid\yv)}\left( \thetav \right) = -\E_{q(\thetav,\yv)}\log q(\thetav\mid\yv).
  \end{equation}
  The mutual information between $\thetav$ and $\yv$ is the amount by which entropy is expected to decrease after conditioning $\yv$:
  \begin{equation}
    \label{eq:mi_def}
    \MI_q\left( \thetav,\yv \right) = h_{q(\thetav)}\left( \thetav \right) - h_{q(\thetav,\yv)}\left( \thetav\mid\yv \right).
  \end{equation}
  Finally, if we extend our joint model to $q(\thetav,\yv,\xv)$ where $\xv$ is any additional quantity, then the conditional mutual information given $\xv$ is just the difference of the corresponding conditional entropies:
  \begin{equation}
    \label{eq:cmi_def}
    \MI_q\left( \thetav,\yv\mid\xv \right) = h_{q(\thetav,\xv)}\left( \thetav\mid\xv \right) - h_{q(\thetav,\yv,\xv)}\left( \thetav\mid\yv,\xv \right).
  \end{equation}
\end{definition}

The first important result allows us to break up an entropy or mutual information expression additive over the components of vector arguments.
\begin{lemma}[Chain Rule for Entropy and Mutual Information]\label{lem:it_chain}
  Let $q(\xv,\yv,\zv)$ be a joint model and suppose that $\xv$ can be partitioned into sub-vectors $\left( \xv_1,\ldots,\xv_m \right)$ for some $m\geq 1$. Then we have that
  \[
    h_q\left( \xv\mid\yv \right) = \sum_{i=1}^m h\left( \xv_i\mid \xv_{< i},\yv \right),
  \]
  where $\xv_{<i}=\left( \xv_1,\ldots,\xv_{i-1} \right)$ for $i\geq 2$, and $\xv_{<1} = \{\}$. Furthermore, we have that
  \[
    \MI_q\left( \yv,\xv\mid\zv \right) = \sum_{i=1}^m \MI\left( \yv,\xv_i\mid \zv,\xv_{<i} \right).
  \]
\end{lemma}

Next, it can be useful to express the (conditional) mutual information in terms of the KL divergence. This can be done in two different ways.
\begin{lemma}[Mutual Information as KL Divergence]\label{lem:it_div}
  Let $q(\xv,\yv,\zv)$ be a joint model. Then we have
  \[
    \MI(\yv,\xv\mid\zv) = \E_{q(\zv)}D\left( q(\xv,\yv\mid\zv) || q(\xv\mid\zv) q(\yv\mid\zv) \right) = \E_{q(\yv,\zv)} D\left( q(\xv\mid\yv,\zv) || q(\xv\mid\zv) \right).
  \]
\end{lemma}

It is of fundamental importance that the KL divergence is always nonnegative, which follows by an application of Jensen's inequality.
\begin{lemma}[Nonnegativity of the KL Divergence]
  For any densities $p(\yv)$ and $q(\yv)$, we have
  \[
    D\left( p(\yv) || q(\yv) \right)\geq 0
  \]
  with equality if and only if $p(\yv)=q(\yv)$ $p$-almost surely.
\end{lemma}
This immediately implies nonnegativity of the mutual information, and in turn the fact that
\[
  h_{q(\xv,\yv)}\left(\yv\mid\xv  \right)\leq h_{q(\yv)}\left( \yv \right)
\]
for any joint distribution $q(\xv,\yv)$.

It is often useful to know how these quantities operate under certain transformations of the random quantities in terms of which they are defined. This is characterized by the following result.
\begin{lemma}[Entropy and Mutual Information Under Transformation]
  Let $\Am\in\RR^{d\times d}$ be any invertible matrix and let $\yv^\prime=\Am\yv$. Then we have
  \[
    h_{q(\yv^\prime)}\left( \yv^\prime \right) = h_{q(\yv)}\left( \yv \right) + \log \left\lvert \det\Am \right\rvert.
  \]
  Furthermore, if $\yv^\prime = \yv + c$ for any $c\in\RR$, then $h\left( \yv \right)=h\left( \yv^\prime \right)$. Thus, the entropy is invariant under translations and orthogonal transformations. The mutual information satisfies the stronger property of invariance under arbitrary smooth reparametrizations of the individual arguments. Specifically, let $\phi,\psi$ be smooth, invertible maps, and define $\yv^\prime=\phi(\yv)$ and $\xv^\prime = \psi(\xv)$. Then we have that
  \[
    \MI(\yv^\prime,\xv^\prime\mid\zv) = \MI(\yv,\xv\mid\zv).
  \]
\end{lemma}

The general behavior of the mutual information under potentially noninvertible transformations is characterized by the data processing inequality.
\begin{lemma}[Data Processing Inquality]
  Let $q(\xv,\yv,\zv)$ be any distribution, and suppose that $\xv$ and $\zv$ are conditionally independent given $\yv$. Then we have that
  \[
    \MI(\xv,\yv) \geq \MI(\xv,\zv).
  \]
  In particular, the above inequality holds if $\zv = \psi(\yv)$ for any function $\psi$.
\end{lemma}

Finally, certain distributions maximize the entropy under certain conditions. For our purposes, it suffices to note that normal distributions on $\RR^d$ maximize the entropy among all distributions with fixed covariance matrix and support equal to $\RR^d$.
\begin{lemma}[Maximum Entropy of Normal]
  Let $q(\yv)$ be any probability distribution supported on $\RR^d$, and let $p(\yv)$ be a normal distribution with any mean and covariance matrix equal to the covariance $\covm_q$ of $q(\yv)$. Then we have
  \[
    h_{q(\yv)}\left( \yv \right)\leq h_{p(\yv)}\left( \yv \right) = \frac{1}{2}\log\left( \det\left( 2\pi e \covm_q \right) \right).
  \]
\end{lemma}

\section{Weak Identification Simulation Details}\label{app:id_sim_details}
% Figure environment removed

We simulated our three data sets in Section \ref{sec:weak_id_practice} from the following model.
\begin{equation}
  \label{eq:weak_id_exp_model}
  \begin{gathered}
  \yv_{ml}\mid \thetav \stackrel{iid}{\sim} \mathrm{normal}\left(\theta_l,\sigma_* \right), \qquad \thetav_l \stackrel{iid}{\sim}\mathrm{normal}\left(\mu_*,\tau_*\right),\\
  \end{gathered}
\end{equation}
The $\sigma_*$, $\mu_*$, and $\tau_*$ are all hyperparameters. We fixed $\mu_*$ and $\tau_*$ for all data sets, but we varied $\sigma_*$ from $\tau_*/2$ to $2\tau_*$. For our purposes, the key differentiator of the data sets is the ratio of the variance of the subpopulation-level (sample) means to the overall (sample) variance. When this statistic is closer to $1$, the between-subpopulation variation swamps the within-population variation, and the reverse is true when this statistic is close to $0$. For the three resulting data sets, this statistic was approximately $0.45$, $0.6$, and $0.95$ respectively. The complete data sets along with the R and Stan code used to generate and analyze them are available on the companion github repository \url{https://github.com/collin-cademartori/BayesianModelExpansionPaper/}.


Algorithms \ref{alg:pseudo_boot1} and \ref{alg:pseudo_boot2} give detailed pseudo-code for the two resampling procedures we used to simulate the process of sampling data from (a) the same subpopulations from which the original data was sampled and (b) new subpopulations within the larger superpopulation. Each of these algorithms was run with $R=500$ replication of the resampling scheme, $S=2000$ samples drawn from each of the resulting posterior distributions, and with $M^{\mathrm{new}} = 8$ and $L^{\mathrm{new}} = 20$ respectively.

\section{Details of the Election Forecasting Model}\label{app:forecasting_spec}

The bias terms $\beta_i$ in \eqref{eq:poll_data_model} are decomposed into several further terms which are designed to capture the following sources of polling bias:
\begin{itemize}
\item Pollster-level ``house'' effects, i.e. the observed phenomenon that most pollsters exhibit a nonzero, temporally stable bias toward one party or the other.
\item Poll mode effects reflecting the observed phenomenon that the averages of phone- and internet-based polls tend to differ.
\item Poll population effects reflecting the observed phenomenon that the averages of ``registered voter'' and ``likely voter'' polls tend to differ.
\item Partisanship nonresponse effects that account for the fact that differential rates of response between members of the two parties can alter poll results. This term is only included for polls that do not attempt to adjust their results for the partisan composition of their sample. Unlike the previous terms, this term varies through time (and is assigned a time-series prior) in order to account for the fact that differential partisan nonresponse has been observed to vary over the course of an election cycle.
\item State-level measurement error effects reflecting the observation that polls of some states have historically been more accurate than others.
\item Poll-level random measurement error, i.e. a catch-all term representing any additional variance in the average support for the Democrat among the polls' sampling frames unaccounted for by the above terms.
\end{itemize}

A complete description of the polling model specification can be found in \cite{ElectionForecast}, and the code and data for the original model can be accessed at \url{https://github.com/TheEconomist/us-potus-model}. The code for our expanded models and our conditional posterior checks can be found at \url{https://github.com/collin-cademartori/BayesianModelExpansionPaper}.

\section{Conditional Mutual Information Bounds}\label{app:MI_bounds}
In this section we state and prove Theorem \ref{thm:cmi_lb}, our lower bound on the conditional mutual information. To establish the lower bound, we will need to use the following key fact on the monotonicity of the conditional mutual information under conditioning on successive replications.

\begin{lemma}[Decreasing Conditional Mutual Information]\label{lem:DMI}
  Let $p(\thetav,\yv)$ be a joint model of parameters and data. For any $M,R\geq 1$, let
  \[
    (\yv^{(1)},\ldots,\yv^{(M+R)})\stackrel{iid}{\sim} p(\yv\mid\thetav).
  \]
  Then we have that $\MI(\thetav,\yv^{(M)}\mid\yv,\yv^{(1)},\ldots,\yv^{(M-1)})$ is decreasing in $M$. Furthermore, we have that
  \[
    \MI(\thetav,\yv^{(1)}\mid\yv) \geq \frac{1}{M}\MI(\thetav,(\yv^{(1)},\ldots,\yv^{(M)})\mid\yv^{(M+1)},\ldots\yv^{(M+R)})
  \]
  for all $M\geq 1$.
\end{lemma}

\begin{proof}
  Letting $\overline{\yv}_M = \left(\yv^{(1)},\ldots,\yv^{(M)}\right)$ and $\widetilde{\yv}_R=\left(\yv^{(M+1)},\ldots,\yv^{(M+R)}\right)$, we have that
  \begin{align}
    \MI\left( \thetav, \overline{\yv}_M\mid\widetilde{\yv}_R \right) &= h\left(\overline{\yv}_M\mid\widetilde{\yv}_R\right) - h\left(\overline{\yv}_M\mid\widetilde{\yv}_R,\thetav\right)\nonumber\\
                                                                     &= h\left(\overline{\yv}_M\mid\widetilde{\yv}_R\right) - h\left(\overline{\yv}_M\mid\thetav\right)\label{eq:MIB_1}\\
                                                                     &\leq h\left(\overline{\yv}_M\mid\yv^{(M+1)}\right) - h\left(\overline{\yv}_M\mid\thetav\right)\label{eq:MIB_2}\\
                                                                     &= h\left(\overline{\yv}_M\mid\yv^{(M+1)}\right) - h\left(\overline{\yv}_M\mid\thetav,\yv^{(M+1)}\right)\label{eq:MIB_3}\\
    &= \MI\left(\thetav,\overline{\yv}_M\mid \yv^{(M+1)}\right),\nonumber
  \end{align}
  where \eqref{eq:MIB_1} follows from the conditional independence of the $\yv^{(i)}$ given $\thetav$, \eqref{eq:MIB_2} follows from the fact that conditioning decreases entropy, and \eqref{eq:MIB_3} again follows in the same way as \eqref{eq:MIB_1}.

  Now, by the chain rule for mutual information and the same argument as above, we can bound this latter conditional mutual information as
  \begin{align*}
    \MI\left(\thetav,\overline{\yv}_M\mid \yv^{(M+1)}\right)&=\sum_{j=1}^M \MI\left( \thetav,\yv^{(j)}\mid \yv^{(M+1)},\overline{\yv}_{j-1} \right)\\
                                                            &\leq \sum_{j=1}^M\MI\left( \thetav,\yv^{(j)}\mid\yv^{(M+1)} \right)\\
    &=M\MI\left( \thetav,\yv^{(1)}\mid\yv \right),
  \end{align*}
  where $\overline{\yv}_0=\left\lbrace\right\rbrace$, and where the last inequality follows from the fact that the distributions of $\left( \thetav,\yv^{(j)},\yv^{(M+1)} \right)$ are all equal to the distribution of $\left( \thetav,\yv^{(1)},\yv \right)$. Now dividing both sides by $M$ completes the proof.
\end{proof}

Before stating our main lemma, we first give a definition that extends the notion of a subexponential distribution to random symmetric matrices.

\begin{definition}[Subexponential Random Matrix]\label{def:subexponential}
  A random symmetric matrix $\mathbf{M}$ is said to be $(\alpha,\beta)$-subexponential if, for all $k\geq 2$, we have
  \[
    \E\mathbf{M}^k \prec \alpha\frac{k!}{2}\beta^{k-2}\mathbf{I},
  \]
  where the inequality denotes the Loewner order.
\end{definition}

We will also say that a random matrix is $\gamma$-subexponential if it is $(\alpha,\beta)$-subexponential for some $\alpha,\beta$ such that $\gamma=\alpha+\beta$.


\begin{lemma}[Fisher Information Lower Bound]
  For $M,R\geq 1$, define the $(M+R)$-replicated model:
  \[
    p\left( \yv^{(1)},\ldots,\yv^{(M+R)},\thetav \right) = p(\thetav)\prod_{i=1}^{M+R}p\left( \yv^{(i)}\mid\thetav \right).
  \]
  Suppose for $R$ sufficiently large, we have that
  \begin{itemize}
  \item the posterior distributions $p\left(\thetav\mid\yv^{(1)},\ldots,\yv^{(R)}\right)$ are normal,
  \item the observed information matrix of $p(\yv,\thetav)$ is $(\alpha,\beta)$-subexponential for some $\alpha,\beta >0$ (i.e. the observed information does not have heavy tails),
  \item $\E\lambda_d(\covm)$, $\E\lambda^{-1}_1(\covm)$, $\E\lambda^2_d\left( \II(\thetav) \right)$, and $\E\lambda_1^{-1}(\II(\thetav)))$ are bounded by some $B>0$ where $\covm = \mathrm{Cov}\left( \thetav\mid\yp{1},\ldots,\yp{M} \right)$ (i.e. the posterior covariance and Fisher information are neither too small nor too large on average).
  \end{itemize}
  Then for $C$ a constant depending on $\gamma=\alpha+\beta$ and $B$, we have for $M$ sufficiently large that
  \begin{equation}
    \label{eq:mi_fisher_lb_statement}
    \frac{1}{M}\MI(\thetav,\overline{\yv}_M\mid\widetilde{\yv}_R) \geq \frac{C}{\log d}\tr\left( \E_{p(\thetav,\yv)}\covm^{1/2}_{\overline{\yv}_R}\II(\thetav)\covm_{\overline{\yv}_R}^{1/2} \right).
  \end{equation}
\end{lemma}

We give the proof in the case $R=1$ and write $\yv$ for $\yv^{(M+1)}$ for simplicity, but the proof for $R>1$ follows in the exact same way with $\left(\yp{M+1},\ldots,\yp{M+R}\right)$ replacing $\yv$ throughout.

\begin{proof}
  By the invariance of the mutual information under invertible transformations, if we define $\thetat = \covm^{-1/2}_{\yv}\thetav$, we get
  \begin{equation}
    \label{eq:cmi_fish_1}
    \MI(\thetav,(\yp{1},\ldots,\yp{M})\mid\yv) = \MI(\thetat,(\yp{1},\ldots,\yp{M})\mid\yv) \geq \E\log\left[ \det\covt_{M}^{-1} \right],
  \end{equation}

  where the last inequality follows by expressing the conditional mutual information as the entropy difference $h\left(\thetat\mid\yv  \right) - h\left( \thetat\mid \yv,\yp{1},\ldots,\yp{M}\right)$, plugging in the expressions for the entropy of multivariate normal distributions, and simplifying.
  
  Now for $1\leq i\leq d$, let $\lambda^d_i(\cdot):H_d\to\mathbb{R}$ be the map from $d\times d$ Hermitian matrices to their $i^{\mathrm{th}}$ eigenvalue under increasing order. We will usually write $\lambda_i$ for $\lambda_i^d$ when the matrix dimension is clear from context. We will henceforth write $\covt^{-1}_M$ for $\covt_{\yv,\yp{1},\ldots,\yp{M}}^{-1}$ for convenience.

  Furthermore, define $\Iobs = \frac{1}{M}\sum_{i=1}^M\Hm_{\phi(\thetat\mid\yp{i})}(\thetat)$ where $\phi(\thetat\mid\yp{i}) = -\log p(\yp{i}\mid\thetat)$, and
  \begin{equation}
    \label{eq:cmi_fish_7}
    \mathcal{E}_{\thetat} = \left\lbrace \left\|\Iobs - \II(\thetat)\right\|_{\mathrm{op}} < \frac{\delta}{2} \right\rbrace.
 \end{equation}
  Finally, define the event
  \[
    \mathcal{G} = \left\lbrace \lambda_1\left( \II(\thetav) \right) \geq \delta  \right\rbrace.
  \]
  The proof will now proceed by analyzing the decomposition
  \begin{align}
    \E\log\left[\det\covt_{M}^{-1} \right] &= \sum_{i=1}^d \E \left[ \log\left( \lambda_i\left( \covt^{-1}_M \right)\right)\mathbbm{1}_{\mathcal{E}_{\thetat}\cap \mathcal{G}}\right] + \sum_{i=1}^d \E\left[ \log\left( \lambda_i\left( \covt^{-1}_M \right)\right)\mathbbm{1}_{\mathcal{E}^c_{\thetat}\cap\mathcal{G}}\right]\nonumber\\
                                           &\hspace{1cm} +\sum_{i=1}^d \E\left[ \log\left( \lambda_i\left( \covt^{-1}_M \right)\right)\mathbbm{1}_{\mathcal{G}^c}\right],\label{eq:cmi_decomp}
  \end{align}
  where, on the right hand side, we now take the expectations over $\left( \yp{0},\yp{1},\ldots,\yp{M},\thetat \right)$. Label these terms T1 - T3.

  \textbf{Lower bound for T3.}

  First we note that
  \begin{align*}
    \E\left[ \log\left( \lambda_i\left( \covt^{-1}_M \right)\right)\mathbbm{1}_{\mathcal{G}^c}\right] &\geq \E\left[ \log\left( \lambda_1\left( \covt^{-1}_M \right)\right)\mathbbm{1}_{\mathcal{G}^c}\right]\\
                                                                                                      &= -\E\left[ \log\left( \lambda_1\left( \covt_M \right)\right)\mathbbm{1}_{\{\lambda_1^{-1}\left(\II(\thetav)\right) > \delta^{-1}\}}\right]\\
                                                                                                      &\geq - \sqrt{\E\log\left(\lambda_1\left( \covt_M \right)\right)^2}\sqrt{\P\left( \lambda_1^{-1}\left(\II(\thetav)\right) \geq \delta^{-1} \right)}\\
                                                                                                      &\geq -\delta^{1/2}\sqrt{\E\log\left(\lambda_1\left( \covt_M \right)\right)^2\E\lambda^{-1}_1\left(\II(\thetav)\right)}\\
                                                                                                      &\geq -\delta^{1/2}\sqrt{\left(\E\lambda_1\left( \covt_M \right)+\E\lambda^{-1}_1\left( \covt_M \right)\right)\E\lambda^{-1}_1\left(\II(\thetav)\right)}\\
    &\geq -\sqrt{2}\delta^{1/2}B.
  \end{align*}
  So T3 is lower bounded by $-\sqrt{2}d\delta^{1/2}B$.
  
  \textbf{Lower Bound for T2.}

  We now lower bound T2. First observe that
  \begin{align*}
    \E\left[ \log\left( \lambda_i\left( \covt^{-1}_M \right)\right)\mathbbm{1}_{\mathcal{E}^c_{\thetat}\cap \mathcal{G}}\right]&\geq \E\left[ \log\left( \lambda_1\left( \covt^{-1}_M \right)\right)\mathbbm{1}_{\mathcal{E}^c_{\thetat}\cap \mathcal{G}}\right]\\
                                                                                                                                                   &= -\E\left[ \log\left( \lambda_1\left( \covt_M \right)\right)\mathbbm{1}_{\mathcal{E}^c_{\thetat}\cap \mathcal{G}}\right]\\
                                                                                                                                                   &\geq -\sqrt{\E\left[ \log\left( \lambda_1\left( \covt_M \right)\right)^2 \right]}\sqrt{\P\left( \mathcal{E}_{\thetat}^c\cap\mathcal{G} \right)}
  \end{align*}  

  Now we observe that we can upper bound the probability factor as
  \[
    \P\left( \mathcal{E}_{\thetat}^c\cap\mathcal{G} \right) =\E_{p(\thetav,\yv)}\P\left( \mathcal{E}_{\thetat}^c\cap \mathcal{G}\mid\thetat,\yv \right)\leq \E_{p(\thetav,\yv)}\P\left( \mathcal{E}_{\thetat}^c \mid\thetat,\yv\right).
  \]
  Since we assume that the observed information matrices $\Hm_{\phi(\thetat\mid\yp{i})}(\thetat)$ are sub-exponential in the sense of \eqref{def:subexponential}, we can apply the matrix Bernstein inequality conditional on $(\thetat,\yv)$ with rank of the Fisher information $\II(\thetat)$ to see that, if $\delta \leq 1$,
  \begin{equation}
    \label{eq:cmi_fish_13}
    \P\left( \mathcal{E}_{\thetat}^c \mid\thetat\right)\leq 2d \exp\left( -M\frac{\delta^2}{4\alpha + 2\beta\delta} \right) \leq 2d\exp\left( -M\frac{\delta^2}{4\gamma} \right),
  \end{equation}
  where $\alpha,\beta$ are the sub-exponential parameters controlling the tails of the observed information, $\gamma = \alpha + \beta$. 
  Since the right hand side is free of $(\thetat,\yv)$, we obtain the same upper bound for the marginal probability $\P(\mathcal{E}_{\thetat}^c\cap\mathcal{G})$. This gives an overall bound for T2 of $-\sqrt{2}Bd\exp\left( -M\frac{\delta^2}{8\gamma} \right)$.
  
  
  \textbf{Lower Bound for T1.}
  
  Now, for any Gaussian distribution, the Hessian of the potential function is exactly the precision matrix. Combining this with the fact that derivatives of the log normalizing constant vanish and the fact that $\mathrm{Cov}\left( \thetat\mid\yv \right)=\Im$, we have that, for all $\thetat\in\RR^{d}$,
  \begin{align}
    \label{eq:cmi_fish_4}
    \covt^{-1}_{\yp{0},\ldots,\yp{M}} &= \Hm_{-\log p(\thetat\mid\yp{0},\ldots,\yp{M})}(\thetat)\nonumber\\
                                      &= \Hm_{-\log(\thetat\mid\yv)}(\thetat) + \sum_{i=1}^M\Hm_{\phi(\thetat\mid\yp{i})}(\thetat)\nonumber\\
    &= \Im + \sum_{i=1}^M\Hm_{\phi(\thetat\mid\yp{i})}(\thetat).
  \end{align}

  Now note that
  \begin{equation}
    \label{eq:cmi_fish_5}
    \lambda_i\left(  \Im + \sum_{i=1}^M\Hm_{\phi(\thetat\mid\yp{i})}(\thetat) \right) = 1 + \lambda_i\left(\sum_{i=1}^M\Hm_{\phi(\thetat\mid\yp{i})}(\thetat) \right)
  \end{equation}
  Combining the observations of \eqref{eq:cmi_fish_5} and \eqref{eq:cmi_fish_4} with the identity \eqref{eq:cmi_fish_1}, we obtain
  \begin{align}
    \MI(\thetat,\yp{1},\ldots,\yp{M}\mid\yv) &= \frac{1}{2}\E\sum_{i=1}^d\log\left( 1 + \lambda_i\left(\sum_{i=1}^M\Hm_{\phi(\thetat\mid\yp{i})}(\thetat) \right) \right)\nonumber\\
    &=\frac{1}{2}\sum_{i=1}^d\E\log\left( 1 + M\lambda_i\left(\Iobs \right) \right)\label{eq:cmi_fish_6}.
  \end{align}

  Now Weyl's inequalities imply that
  \begin{equation}
    \label{eq:cmi_fish_9}
    \left\lvert \lambda_i\left( \Iobs \right) - \lambda_i\left( \II(\thetat) \right) \right\rvert \leq \left\lvert \Iobs - \II(\thetat)\right\rvert_{\mathrm{op}}.
  \end{equation}
  Using this, we have for all $1\leq i\leq M$ that
  \begin{align}
    \label{eq:cmi_fish_8}
    \E_{p(\overline{\yv}\mid\thetat,\yv)}&\left[\log\left( 1 + M\lambda_i\left(\Iobs \right) \right)\mathbbm{1}_{\mathcal{E}_{\thetat}\cap\mathcal{G}}\right]\nonumber\\
    &\hspace {1cm}\geq \log\left( 1 + \frac{M}{2}\lambda_i\left(\II(\thetat)\right) \right)\mathbbm{1}_{\mathcal{G}}(\thetat)\left( 1-\P_{p(\overline{\yv}\mid\thetat,\yv)}(\mathcal{E}_{\thetat}^c) \right)
  \end{align}
  where the expectations are over $\overline{\yv}=\left( \yp{1},\ldots\yp{M} \right)$ conditional on $(\thetat,\yv)$.

  Now combining \eqref{eq:cmi_fish_8}, and \eqref{eq:cmi_fish_6}, and letting
  \[
    M = 8k\frac{\gamma}{\delta^2}\log\left(2d\right),
  \]
  we obtain that
  \begin{align}
    \MI&(\thetat,\yp{1},\ldots,\yp{M}\mid\yv)\nonumber\\
    &\hspace{1cm}\geq\frac{1}{4}\sum_{i=1}^d\E_{p(\thetat,\yv)}\left[\log\left( 1 + \frac{4k\gamma}{\delta^2}\lambda_i\left(\II(\thetat)\right)\right)\mathbbm{1}_{\mathcal{G}}(\thetat)\right] - cd\left[ \delta^{1/2} + 2^{-k} \right].\label{eq:cmi_fish_12}
  \end{align}

  If we now define the function
  \begin{equation}
    \label{eq:double_trunc}
    \sigma^{\tau}(x) = \min\left(x,\tau\right)
  \end{equation}
  to be the $\tau$-truncation of $x$, then we have for $\delta^{-1}$ and $\tau$ sufficiently large that the first term above is lower bounded by
  \begin{align*}
    \label{eq:linear_lb}
    \E_{p(\thetat,\yv)}&\sum_{i=1}^d\log\left[  1 + \frac{4k\gamma}{\delta^2} \lambda_i\left( \II(\thetat) \right)\right]\mathbbm{1}_{\mathcal{G}}(\thetat)\\
    &\geq\E_{p(\thetat,\yv)}\sum_{i=1}^d\log\left[  1 + \frac{4k\gamma}{\delta^2} \lambda_i\left( \II(\thetat) \right)\right]\mathbbm{1}_{\{\delta < \lambda_1(\II(\thetat))\leq\lambda_d(\II(\thetat)) < \tau\}}\\
                       &\geq\frac{\log\left( 1+4k\gamma\tau/\delta^2 \right)}{\tau}\sum_{i=1}^d \E_{p(\thetat,\yv)}\lambda_i\left( \II(\thetat) \right)\mathbbm{1}_{\{\delta < \lambda_1(\II(\thetat))\leq\lambda_d(\II(\thetat)) < \tau\}}\\
    &\geq\frac{1}{\tau}\sum_{i=1}^d\E_{p(\thetat,\yv)} \lambda_i\left( \II(\thetat) \right)\mathbbm{1}_{\{\delta < \lambda_1(\II(\thetat))\leq\lambda_d(\II(\thetat)) < \tau\}},                                                                                                      
  \end{align*}

  where we have used the fact that $\log(1+x)\geq \frac{\log(1+c)}{c}x$ over the interval $[0,c]$ for any $c\geq 0$. Now we note that
  \begin{align*}
    \E_{p(\thetat,\yv)} &\lambda_i\left( \II(\thetat) \right)\mathbbm{1}_{\{\delta < \lambda_1(\II(\thetat))\leq\lambda_d(\II(\thetat)) < \tau\}}\geq \E_{p(\thetat,\yv)} \lambda_i\left( \II(\thetat) \right)\\
    &\hspace{1cm}-\E_{p(\thetat,\yv)} \lambda_i\left( \II(\thetat) \right)\mathbbm{1}_{\{ \lambda_1(\II(\thetat))\leq \delta\}}-\E_{p(\thetat,\yv)} \lambda_i\left( \II(\thetat) \right)\mathbbm{1}_{\{\lambda_d(\II(\thetat)) \geq \tau\}}.
  \end{align*}
  Now by applying Holder's inequality and Markov's inequality, the second to last expectation can be upper bounded by
  \[
    \sqrt{\E_{p(\thetat,\yv)} \lambda^2_d\left( \II(\thetat) \right)}\sqrt{\E_{p(\thetat,\yv)} \lambda^{-1}_1\left( \II(\thetat) \right)}\delta^{-1/2}\leq B\delta^{1/2}.
  \]
  Similarly, the last expectation is upper bounded by $B\tau^{-1/2}$. Thus, the terms of our lower bound have the form
  \[
    \frac{1}{\tau}\E_{p(\thetat,\yv)} \lambda_i\left( \II(\thetat) \right) - C\left( \tau^{-3/2} + \delta^{1/2} + 2^{-k}\right),
    \]
    where we can take $C=\max(c,B)$. By Jensen's inequality, we also have a lower bound $\E_{p(\thetat,\yv)} \lambda_i\geq B^{-1}$. Taking $\tau$, $\delta^{-1}$, and $k$ sufficiently large, we can ensure that the second term above is bounded by $B^{-1}/2\tau$, which yields an overall lower bound of
    \[
      \frac{1}{2B\tau}\sum_{i=1}^d \E_{p(\thetat)}\lambda_i\left( \II(\thetat) \right)
    \]
    Putting this all together, we get that
  \begin{equation}
    \frac{1}{M}\MI(\thetat,\yp{1},\ldots,\yp{M}\mid\yv) \geq \frac{\delta^2}{16Bk\tau\gamma\log(2d)}\tr\left[ \E\II(\thetat) \right],
  \end{equation}
  which concludes the proof upon noting that
  \[
    \II(\thetat) = \covm^{1/2}_{\yv}\II(\theta)\covm^{1/2}_{\yv}.
  \]
\end{proof}

\section{Proof of Theorem \ref{thm:marginal_trace_bound}}\label{app:trace_bound}
The key idea in this proof is to rearrange the logarithm, derivative, and expectation operations to obtain a more tractable expression for the marginal score. First, we need a version of the Cramer-Rao bound that relates the resulting expression to the desired derivatives.

\begin{lemma}[Cramer-Rao Lower Bound]
  Let $\phi=-\log p(\xv)$ be a differentiable density with $\xv\in\RR^d$, and let $f(\xv)$ be a differentiable function. Furthermore, assume that for any $\thetav\in \RR^d$ fixed, $\lim_{\|\xv\|\to\infty} f(\xv)p(\xv-\thetav) = 0$.

  Then we have the following inequality:
  \[
    \mathrm{Var}\left( f(\xv) \right) \geq \left[ \E \nabla f(\xv) \right]^T\left[ \E\mathbf{H}_\phi(\xv) \right]^{-1}\left[ \E \nabla f(\xv) \right].
  \]
\end{lemma}

\begin{proof}
  Assume without loss of generality that $p(\xv)$ has mean $\mathbf{0}$. Let $p_{\thetav}(\xv) = p(\xv-\thetav)$, and consider the (biased) estimator of $\left( f(\thetav),0,\ldots,0 \right)\in\RR^d$ given by $T(\xv)=\left(f(\xv),0,\ldots,0\right)$. We note that, under our assumptions, we have for $1\leq i\leq d$ that
  \[
    \frac{\partial}{\partial\theta_i}\E_{\thetav} f(\xv) = \E_{\thetav}\frac{\partial}{\partial x_i}f(\xv),
  \]
  and that
  \[
    \II(\thetav) = \E\mathbf{H}_{\phi}(\xv-\thetav),
  \]
  where the Hessian is respect to the components of $\xv$. Now the Cramer-Rao lower bound for $T(\xv)$ is just
  \[
    \mathrm{Cov}_{\thetav}\left( T(\xv) \right) \geq \left[\mathbf{J}_{\E_{\thetav} T(\xv)}(\thetav)\right]\II(\thetav)^{-1} \left[\mathbf{J}_{\E_{\thetav} T(\xv)}(\thetav)\right]^T.
  \]
  Multiplying on the left and right by $\ev_1^T$ and $\ev_1$ respectively preserves the inequality (by definition of Loewner order), and so evaluating this multiplication and taking $\thetav = \mathbf{0}$ yields
  \[
    \mathrm{Var}\left( f(\xv) \right) \geq \left[ \E \nabla f(\xv) \right]^T\left[ \E\mathbf{H}_\phi(\xv) \right]^{-1}\left[ \E \nabla f(\xv) \right],
  \]
  as claimed.
\end{proof}

For Theorem \ref{thm:marginal_trace_bound}, we need the following regularity conditions:
\begin{enumerate}
\item $\frac{\partial}{\partial\theta_i}\int p\left( \xv,\lambdav\mid\thetav \right)d\lambdav =\int\frac{\partial}{\partial\theta_i} p\left( \xv,\lambdav\mid\thetav \right)d\lambdav $ for all $1\leq i\leq d$.
\item $\mathrm{Var}_{p\left( \lambda\mid\xv,\thetav \right)}\left(\frac{\partial}{\partial\theta_i}\log p\left( \xv\mid\thetav,\lambdav\right)  \right)<\infty$ for all $1\leq i\leq d$.
\item For any fixed $\lambdav^*\in\RR^m$, almost every $\left( \xv,\thetav \right)$, and all $1\leq i \leq d$,
  \[
    \lim_{\|\lambdav\|\to\infty}p(\lambdav-\lambdav^*\mid\thetav,\xv)\left[ \frac{\partial}{\partial\theta_i}\log p\left( \xv\mid\thetav,\lambdav\right) \right]=0.
  \]
\end{enumerate}
We can now proceed with the proof.
\begin{proof}
    The $i^{\mathrm{th}}$ term of $\E\mathrm{Tr}\left(\boldsymbol{\mathcal{I}} \right)$ is just
    \begin{equation}\label{eq:trace_term}
      \E_{p(\xv,\thetav)}\left\lbrace \left( \frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav) \right)^2  \right\rbrace
    \end{equation}
    We can then rewrite
    \begin{align}
      \frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav) &= \frac{1}{p(\xv\mid\thetav)}\frac{\partial}{\partial\theta_i}\int p(\xv,\lambdav\mid\thetav)d\lambdav\nonumber\\
                                                              &= \int\frac{p(\xv,\lambdav\mid\thetav)}{p(\xv\mid\thetav)}\frac{\partial}{\partial\theta_i}\log p(\xv,\lambdav\mid\thetav)d\lambdav\label{eq:ps_id1}\\
                                                              &= \int p(\lambdav\mid\xv,\thetav)\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)d\lambdav\label{eq:ps_id2}\\
      &=\E_{p(\lambdav\mid\xv,\thetav)}\left\lbrace\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav) \right\rbrace\nonumber
    \end{align}
    where \eqref{eq:ps_id1} follows by exchanging the integral and derivative and using the expression for the logarithmic derivative, and \eqref{eq:ps_id2} follows from the fact that $p(\lambdav\mid\thetav)=p(\lambdav)$ by assumption. Plugging this into the above and using the definition of the variance, we obtain the identity
    \begin{align}
      \E_{p(\xv,\thetav)}&\left\lbrace \left( \E_{p(\lambdav\mid\xv,\thetav)}\left\lbrace\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav) \right\rbrace \right)^2  \right\rbrace\nonumber\\
                         &= \E_{p(\xv,\thetav)}\left\lbrace \E_{p(\lambdav\mid\xv,\thetav)}\left\lbrace\left(\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)\right)^2\right\rbrace - \mathrm{Var}_{p(\lambdav\mid\xv,\thetav)}\left\lbrace \frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav) \right\rbrace\right\rbrace\nonumber\\
      &=\E_{p(\xv,\thetav,\lambdav)}\left\lbrace\left(\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)\right)^2\right\rbrace - \E_{p(\xv,\thetav)}\left\lbrace \mathrm{Var}_{p(\lambdav\mid\xv,\thetav)}\left\lbrace \frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav) \right\rbrace\right\rbrace\label{eq:trace_var_decomp}
    \end{align}
    Now the usual Fisher information identity gives us that
    \[
      \E_{p(\xv,\thetav,\lambdav)}\left\lbrace\left(\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)\right)^2\right\rbrace=\E_{p(\xv,\thetav,\lambdav)}\left\lbrace-\frac{\partial^2}{\partial\theta^2_i}\log p(\xv\mid\thetav,\lambdav)\right\rbrace.
    \]
    On the other hand, using the Cramer-Rao inequality above, we get that
    \begin{equation}\label{eq:var_lb_cr}
      \mathrm{Var}_{p(\lambdav\mid\xv,\thetav)}\left\lbrace \frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav) \right\rbrace\geq \frac{\left[\E_{p(\lambdav\mid\xv,\thetav)}\sum_{j=1}^m\frac{\partial}{\partial\lambda_j}\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)  \right]^2}{\|\E_{p(\lambdav\mid\xv,\thetav)}\Hm\left( \lambdav;\thetav,\xv \right)\|_{\mathrm{op}}}
    \end{equation}
    Plugging this in, the right-hand term in \eqref{eq:trace_var_decomp} can be bounded by
    \begin{align*}
      \E_{p(\xv,\thetav)}&\left\lbrace \mathrm{Var}_{p(\lambdav\mid\xv,\thetav)}\left\lbrace \frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav) \right\rbrace\right\rbrace\\
      &\geq \E_{p(\xv,\thetav)}\left\lbrace \left[\frac{\E_{p(\lambdav\mid\thetav,\xv)}\sum_{j=1}^m\frac{\partial}{\partial\lambda_j}\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)}{\|\E_{p(\lambdav\mid\thetav,\xv)}\Hm(\lambdav;\thetav,\xv)\|^{1/2}_{\mathrm{op}}}  \right]^2 \right\rbrace\\
      &\geq  \left[\sum_{j=1}^m \E_{p(\xv,\thetav,\lambdav)}\left\lbrace\frac{\frac{\partial}{\partial\lambda_j}\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)}{\|\E_{p(\lambdav\mid\thetav,\xv)}\Hm(\lambdav;\thetav,\xv)\|^{1/2}_{\mathrm{op}}}  \right\rbrace\right]^2, \\
    \end{align*}
    where the second inequality follows from Jensen's inequality applied to the outer expectation and the square. Combining this with the above and summing completes the proof of the stronger inequality.

    To prove the weaker bound, we apply the reverse Holder's inequality to the expectation of \eqref{eq:var_lb_cr} under $p(\xv,\thetav)$ with $p=2$ to get the lower bound
    \begin{align*}
      \E_{p(\xv,\thetav)}&\frac{\left[\E_{p(\lambdav\mid\xv,\thetav)}\sum_{j=1}^m\frac{\partial}{\partial\lambda_j}\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)  \right]^2}{\| \E_{p(\lambdav\mid\xv,\thetav)}\Hm\left( \lambdav;\thetav,\xv \right)\|_{\mathrm{op}}}\\                                                                   &\geq \frac{\left[\E_{p(\xv,\thetav)}\left\lvert  \E_{p(\lambdav\mid\xv,\thetav)}\sum_{j=1}^m\frac{\partial}{\partial\lambda_j}\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav) \right\rvert\right]^2}{\E_{p(\xv,\thetav)}\| \E_{p(\lambdav\mid\xv,\thetav)}\Hm\left( \lambdav;\thetav,\xv \right)\|_{\mathrm{op}}}\\
      &\geq \frac{\left[ \sum_{j=1}^m\E_{p(\xv,\thetav,\lambdav)}\frac{\partial}{\partial\lambda_j}\frac{\partial}{\partial\theta_i}\log p(\xv\mid\thetav,\lambdav)\right]^2}{\E_{p(\xv,\thetav,\lambdav)}\| \Hm\left( \lambdav;\thetav,\xv \right)\|_{\mathrm{op}}},\\
    \end{align*}
    completing the proof.
  \end{proof}

\section{Proof of Theorem \ref{thm:strict_tradeoff}}\label{app:heuristic_connection}

We first prove a pair of lemmas that give conditions under which we can further lower bound the \textsf{cmi} in terms of just the Fisher information. The key idea here is to exploit the Cramer-Rao bound to relate the posterior covariance matrix that appears in Theorem \ref{thm:cmi_lb} to the Fisher information. The first step is to determine conditions under which we can get a lower bound on the \textsf{cmi} with the matrix product and expectation operations interchanged.
 To illustrate the idea, observe that for $d=1$, we have that
 \begin{align*}
   \E\tr\left(\covm_{\overline{\yv}}\II(\theta)\right) &= \E[\mathrm{Var}(\theta\mid\overline{\yv})\II(\theta)]\\
                                                        &= \mathrm{Cov}\left(  \mathrm{Var}(\theta\mid\overline{\yv}),\II(\theta)\right) + \E\mathrm{Var}(\theta\mid\overline{\yv})\E\II(\theta)\\
   &\geq \E\mathrm{Var}(\theta\mid\overline{\yv})\E\II(\theta) - \sqrt{\mathrm{Var}\left[\mathrm{Var}(\theta\mid\overline{\yv})\right]\mathrm{Var}\left[ \II(\theta) \right]}.
 \end{align*}
 Thus, in the one-dimensional case, if $\sqrt{\mathrm{Var}\left[\mathrm{Var}(\theta\mid\overline{\yv})\right]} < \delta\E\left[\mathrm{Var}(\theta\mid\overline{\yv})\right]$ and $\sqrt{\mathrm{Var}\left[ \II(\theta) \right]}< \delta \E\left[ \II(\theta) \right]$ for some $\delta\in (0,1)$, then we have that
 \[
   \E\tr\left(\covm_{\overline{\yv}}\II(\theta)\right) \geq (1-\delta^2)\E\mathrm{Var}(\theta\mid\overline{\yv})\E\II(\theta) = (1-\delta^2)\tr\left( \E\covm_{\yv}\E\II(\theta) \right),
 \]
 For a positive random variable $X$, the requirement that $\sqrt{\mathrm{Var}(X)}< \delta \E X$ is not a restriction on the variance $X$ in an absolute sense, since the condition can be satisfied for distributions with arbitrarily large variances so long as the mean is correspondingly large. Rather, we argue that this is naturally seen as a condition on the skewness of $X$. Indeed, for positive random variables $X$ with finite second moment, we have in general that
 \[
\P\left( X\geq (k+1)\sqrt{\E X^2} \right)\leq \frac{1}{k^2} \quad\text{and}\quad \P\left( X \geq k\E X \right)\leq \frac{1}{k}.
\]
The above variance-mean inequality implies that
\[
\P\left( X \geq (k+1)\E X \right)\leq \frac{\delta^2}{k^2}.
\]
Since $\P(X\geq k\E X)$ is large when the distribution of $X$ is skewed to the right, this variance-mean inequality primarily functions to limit the skew of the distribution of $X$. This skewness condition can be naturally generalized to the matrix case with $d\geq 1$, yielding the following lemma.

\begin{lemma}\label{lem:exp_exchange_lb}
  Under the conditions of Theorem \ref{thm:cmi_lb} and Lemma \ref{lem:heuristic_connection}, with relevant definitions taken from the same, we have that if
  \[
\sqrt{\mathrm{Var}(\II(\thetav))} < \delta\lambda_{\min}\left( \E \II(\thetav) \right)\quad \text{and}\quad \sqrt{\mathrm{Var}(\covm_{\overline{\yv}})} < \delta\lambda_{\min}\left( \E \covm_{\overline{\yv}} \right),
\]
for some $\delta\in \Big[0,2^{-1/2}\Big)$, then we have that
\[
  \tr\left( \E\covm_{\overline{\yv}}\II(\thetav) \right) \geq (1-\delta^2)\tr\left( \E\covm_{\overline{\yv}}\E\II(\thetav) \right)
\]

\end{lemma}

\begin{proof}
  Our proof is given in terms of general positive-definite matrices $\Am,\Bm\in\RR^{d\times d}$. First observe that
\begin{equation*}
  \E\tr\left( (\Am- \E\Am)(\Bm-\E\Bm) \right)= \tr\left( \E\Am\Bm \right) - \tr\left( \E\Am\E\Bm \right).
\end{equation*}
Therefore, it follows directly from von Neumann's trace inequalities that
\begin{align*}
  \lvert \tr\left( \E\Am\Bm \right) - \tr\left( \E\Am\E\Bm \right) \rvert &= \lvert \E\tr\left( (\Am- \E\Am)(\Bm-\E\Bm) \right)\rvert\\
                                                                          &\leq \E\lvert \tr\left( (\Am- \E\Am)(\Bm-\E\Bm) \right)\rvert\\
                                                                          &\leq d\left[ \E\lvert\lvert \Am-\E\Am \rvert\rvert_{\mathrm{op}}\lvert\lvert\Bm-\E \Bm\rvert\rvert_{\mathrm{op}} \right]\\
                                                                          &=d\Big[ \mathrm{Cov}\left(\lvert\lvert \Am-\E\Am \rvert\rvert_{\mathrm{op}},\lvert\lvert\Bm-\E \Bm\rvert\rvert_{\mathrm{op}} \right)\\  &\hspace{1cm}+ \E\lvert\lvert \Am-\E\Am \rvert\rvert_{\mathrm{op}}\E\lvert\lvert\Bm-\E \Bm\rvert\rvert_{\mathrm{op}}\Big]\\
                                                                          &\leq d\Big[ \sqrt{\mathrm{Var}\left(\lvert\lvert \Am-\E\Am \rvert\rvert_{\mathrm{op}}\right)}\sqrt{\mathrm{Var}\left(\lvert\lvert\Bm-\E \Bm\rvert\rvert_{\mathrm{op}}\right)}\\  &\hspace{1cm}+ \E\lvert\lvert \Am-\E\Am \rvert\rvert_{\mathrm{op}}\E\lvert\lvert\Bm-\E \Bm\rvert\rvert_{\mathrm{op}}\Big]\\
  &\leq 2d\sqrt{\E\lvert\lvert \Am-\E\Am \rvert\rvert_{\mathrm{op}}^2 \E\lvert\lvert \Bm-\E\Bm \rvert\rvert_{\mathrm{op}}^2}.
\end{align*}
It also follows from von Neumann's trace inequalities that
\[
  \tr\left( \E\Am\E\Bm \right) \geq d\lambda_{\min}\left( \E \Am \right)\lambda_{\min}\left( \E\Bm \right).
\]
Letting $\mathrm{Var}\left( \Am \right) = \E\lvert\lvert \Am-\E\Am \rvert\rvert_{\mathrm{op}}^2$, it follows that if there is a $\delta^2\in \left( 0,1 \right)$ such that
\[
\sqrt{\mathrm{Var}(\Am)}\sqrt{\mathrm{Var}(\Bm)} < \frac{\delta}{2}\lambda_{\min}\left( \E \Am \right)\lambda_{\min}\left( \E\Bm \right),
\]
then we have that $\tr\left( \E\Am\Bm \right) \geq (1-\delta^2)\tr\left( \E\Am\E\Bm \right)$. Of course, it further suffices that there is some $\delta\in\left( 0,\frac{1}{\sqrt{2}} \right)$ such that
\[
\sqrt{\mathrm{Var}(\Am)} < \delta\lambda_{\min}\left( \E \Am \right)\quad \text{and}\quad \sqrt{\mathrm{Var}(\Bm)} < \delta\lambda_{\min}\left( \E \Bm \right),
\]
which is guaranteed by our skewness conditions.  
\end{proof}

Relative to the one-dimensional case, we note two clear defects of this result. Specifically, the requirement that $\delta < 2^{-1/2}$ rather than $\delta <1$ and the fact that the variance is given in terms of the maximum singular value whereas the corresponding mean matrix is measured in terms of its minimal singular value make this bound more stringent than in the scalar case. Nevertheless, the qualitative requirement is essentially the same - that the distributions of the spectra of the posterior covariance and Fisher information are not too skewed.

Now we can use this lemma along with the Cramer-Rao bound to get a lower bound in terms of (only) the expected Fisher information.
\begin{lemma}\label{lem:heuristic_connection}
  If $\E_{p(\thetav)}\II(\thetav)$ has spectrum $\{\iota_i\}_{i=1}^d$ and $p(\thetav)=\mathrm{normal}(\thetav\vert\muv,\Im)$, then
  \begin{equation}
    \label{eq:conj_trace_lb}
    \tr\left( [\E\covm_{\yv}]^{1/2} [\E\II(\thetav)] [\E\covm_{\yv}]^{1/2} \right) \geq \sum_{i=1}^d \frac{\iota_i}{1+\iota_i},
  \end{equation}
\end{lemma}

\begin{proof}
We give the proof for general $R\geq 1$.  Let $\mathbf{J}_{\thetav,\yv}$ be the observed information matrix. Then we note that, by the Cramer-Rao bound, Jensen's inequality, and our assumptions on the prior, we have that
  \begin{align*}
    \E\covm_{\overline{\yv}_R} &\geq \E_{p(\overline{\yv})}\left[ \E_{p(\thetav\mid\overline{\yv})}\left\lbrace \sum_{j=1}^R\mathbf{J}_{\thetav,\yv_j} + \mathbf{I}\right\rbrace \right]^{-1} \\
    &\geq \left[ \E_{p(\thetav,\overline{\yv})}\left\lbrace \sum_{j=1}^R\mathbf{J}_{\thetav,\yv_j} + \mathbf{I}\right\rbrace \right]^{-1}\\
    &= \left[ R\E_{p(\thetav)}\II(\thetav) + \mathbf{I} \right]^{-1},
  \end{align*}
  where the inequalities represent the Loewner partial ordering of PSD matrices whereby $\Am\geq \Bm$ iff $\Am-\Bm$ is PSD. Using the fact that the trace is symmetric and Loewner order is preserved under conjugation by any other positive definite matrix, we have that
  \begin{align*}
    \tr\left([\E\covm_{\overline{\yv}}]^{1/2} [\E\II(\thetav)] [\E\covm_{\overline{\yv}}]^{1/2}  \right) &= \tr\left( [\E\II(\thetav)]^{1/2} [R\E\covm_{\overline{\yv}}] [\E\II(\thetav)]^{1/2} \right) \\
                                                                                   &\geq \tr\left([\E\II(\thetav)]^{1/2} \left[ \E\II(\thetav) + \mathbf{I} \right]^{-1} [\E\II(\thetav)]^{1/2}  \right)\\
    &= \tr\left([\E\II(\thetav)] \left[ R\E\II(\thetav) + \mathbf{I} \right]^{-1} \right).
  \end{align*}
  Now, writing the spectral decomposition of $\E\II(\thetav)$ as $\Um^T\boldsymbol{\Lambda}\Um$, where $\boldsymbol{\Lambda} = \mathrm{diag}\left( \lambda_1,\ldots,\lambda_d \right)$, and substituting this into the above, we get that
  \begin{align*}
    \tr\left([\E\II(\thetav)] \left[ R\E\II(\thetav) + \mathbf{I} \right]^{-1} \right) &= \tr\left( \Um^T\boldsymbol{\Lambda}\left[ R\boldsymbol{\Lambda} + \mathbf{I} \right]^{-1}\Um \right)\\
                                                                                       &= \tr\left( \boldsymbol{\Lambda}\left[ R\boldsymbol{\Lambda} + \mathbf{I} \right]^{-1} \right)\\
    &= \sum_{i=1}^d\frac{\lambda_i}{1+R\lambda_i},
   \end{align*}
   which completes the proof.
 \end{proof} 

We make a few observations about this result:
\begin{itemize}
\item Since the trace in \eqref{eq:conj_trace_lb} is invariant under orthogonal transformations of $\thetav$, the unit covariance assumption only imposes that the parameters have unit prior scale, which can always be achieved by rescaling.
\item The normality assumption may be relaxed by replacing the identity matrix by the Hessian $\mathbf{H}$ of $-\log p(\thetav)$ in the proof. If $\E_{p(\thetav)}\mathbf{H}\prec c\mathbf{I}$, then we get
  \[
    \tr\left( [\E\covm_{\yv}]^{1/2} [\E\II(\thetav)] [\E\covm_{\yv}]^{1/2} \right) \geq \sum_{i=1}^d \frac{\iota_i}{c+\iota_i}.
  \]
\item If $R>1$, we get a similar lower bound with terms $\iota_i/\left( 1 + R\iota_i \right)$.
\end{itemize}

We also note that this lower bound shares many features with the \textsf{cmi}: (i) a dimension dependence through the sum of $d$ terms, (ii) dependence on the likelihood curvature through the eigenvalues $\iota_i$, and (iii) a self-limiting behavior since increasing $\iota_i$ are offset by the decreasing curvature of $\frac{x}{1+x}$.

Finally we can prove Theorem \ref{thm:strict_tradeoff}.

\begin{proof}
  
  The first inequality of \eqref{eq:tradeoff_base_bounds} follows directly from Theorem \ref{thm:mi_ub} taking $\psi_1=d\psi(d^{-1} \cdot)$ (noting that $d$ is a constant here as it is the dimension of the shared parameters $\theta$). Likewise, the first inequality of \eqref{eq:tradeoff_exp_bounds} follows from Theorem \ref{thm:mi_ub} and Theorem \ref{thm:marginal_trace_bound} with the same $\psi_1$ and letting $\Delta_m = \sum_{i=1}^{d}\Delta_i$. Taking $\psi_2(x) = (1-\delta^2)C\frac{x}{1+Rx}$, the second inequality of \eqref{eq:tradeoff_base_bounds} follows from Theorem \ref{thm:cmi_lb} along with Lemmas \ref{lem:exp_exchange_lb} and \ref{lem:heuristic_connection}. Letting $\{\iota^{\mathrm{exp}}_i\}_{i=1}^{d^{\mathrm{exp}}}$ be the eigenvalues of $\E\II\left( \thetav,\lambdav \right)$, the same argument gives the lower bound
  \[
    \MI\left(\yrep,(\thetav,\lambdav)\mid\yv \right) \geq \sum_{i=1}^d\psi_2\left(\iota^{\mathrm{exp}}_i\right).
  \]
  To get the bound in terms of the $\iota_i^{\mathrm{cond}}$, we observe that
  \[
    \sum_{i=1}^d\psi_2\left( \iota_i^{\mathrm{cond}} \right) \leq \sum_{i=1+d^{\mathrm{exp}}-d}^{d^{\mathrm{exp}}}\psi_2\left( \iota_i^{\mathrm{exp}} \right) \leq \sum_{i=1}^{d^{\mathrm{exp}}}\psi_2\left( \iota_i^{\mathrm{exp}} \right),
  \]
  where the first inequality follows from the eigenvalue interlacing theorem and the fact that $\psi_2$ is increasing. Thus, the second bound in \eqref{eq:tradeoff_exp_bounds} follows with
  \[
    \Delta_c = \sum_{i=1}^{d^{\mathrm{exp}}}\psi_2\left( \iota_i^{\mathrm{exp}} \right)-\sum_{i=1}^d\psi_2\left( \iota_i^{\mathrm{cond}} \right).
  \]
  Finally, the inequalites \eqref{eq:tradeoff_diluting} and \eqref{eq:tradeoff_nondiluting} follow immediately from the above and the definition of a totally (non)diluting expansion.
\end{proof}
 
 \section{Connections with Power or Conservativity of the Posterior Predictive $p$-Value}
 
The phenomenon exhibited in Sections \ref{sec:underpowered_check_theory} and \ref{sec:underpowered_check_practice} whereby the posterior predictive $p$-value appears inappropriately large has long been observed in the literature. Indeed, Meng showed in \cite{PostP} that the distribution of the posterior predictive $p$-value under the prior predictive distribution $p(\yv)$ is dominated in convex order by the uniform distribution. This suggests that, in the majority of cases, for any test statistic $T$, we will have
\begin{equation}
  \label{eq:p_val_under}
  \int_{\{p_T(\yrep)\leq p_T(\yv)\}}p(\yrep)d\yrep < p_T(\yv).
\end{equation}
In other words, if the data are actually drawn from the model, then the probability of observing a posterior predictive $p$-value as small as what we do observe is usually less than the $p$-value itself. This inequality has led to a characterization of the posterior predictive $p$-value as conservative. More specifically, the above inequality has led to two related arguments against the \textsf{ppp-v}:  

\begin{itemize}
\item If we don't know the distribution of the $p$-value (e.g. if it is not \textit{calibrated} to be uniform), then we cannot interpret the $p$-value (see, e.g., \cite{RobinsEtAl00}).
\item Because the $p$-value is conservative, it will likely have low power against any alternative at conventional rejection thresholds (see, e.g., \cite{PostPPharma},\cite{BayarriBerger00}).
\end{itemize}

In response to the first concern, it has been argued that the posterior predictive $p$-value is directly interpretable as a tail probability, and thus does not need to be compared to any reference distribution that defines its frequency properties \cite{GelmanInterpretability}. In response to the second concern, it has been pointed out that rigid reject-or-not-reject decision rules (in terms of which power is traditionally defined) are inappropriate in contexts where (a) model usefulness is more important than strict model correctness and (b) the modeler is more interested in progressively improving a model than in rejecting it outright \cite{GelmanInterpretability}.

To draw out this contrast a bit, an oracle that can tell you with perfect accuracy whether a model is the true data generating process or not would have known reference distribution under the proposed model and $100\%$ power under any alternative. However, if this oracle can only provide a yes or no answer (as in reject-or-not testing procedures), then it will not provide any information which can be used in practice to improve the model. Thus, at their extremes, these two goals (determining the truth of a model and iteratively improving a model) can become completely decoupled.

It may seem, therefore, that if we focus our attention on iterative model improvement, then the above critiques of the posterior predictive $p$-value may be disregarded. However, even if we are focused narrowly on model improvement, the arguments of Sections \ref{sec:underpowered_check_theory} and \ref{sec:underpowered_check_practice} demonstrated that we can gain information relevant to this goal by disaggregating traditional posterior predictive checks (e.g. by evaluating the fitness of each sampling distribution separately and studying the joint posterior distribution of these evaluations with the model parameters). In other words, we argue that the underlying difficulties with the posterior predictive distribution as the basis of model assessment persist outside of the framework of decision rules and reference distributions and in fact extend to the framework of iterative model expansion.

We have also demonstrated that under commonly observed conditions, sufficient model expansion will usually increase the posterior sampling divergence. Consequently, we believe that this issue should be of more than just theoretical interest for the applied modeler.

\subsection{Comparison of the Conditional and Partial $p$-Values}

We turn now to a comparison between our proposed conditional $p$-value and another proposed modification of the posterior predictive $p$-value, the partial posterior predictive $p$-value (henceforth partial $p$-value) developed in \cite{BayarriBerger99}. Bayarri and Berger advocate for the partial $p$-value in \cite{BayarriBerger00} partly on the grounds that it is expected to be less conservative than the traditional \textsf{ppp-v}, and so it is instructive to compare our proposal and the partial $p$-value on the basis of their usefulness in practical problems.

The partial $p$-value is motivated by the idea that the conservativity of the posterior predictive $p$-value is a consequence of its double use of data - first to fit the posterior, then to form the test statistic. The partial $p$-value removes the influence of the test statistic on the posterior, resulting in the following definition:
\begin{equation}
  \label{eq:partial_pval}
  p^{\mathrm{part}}_T = \int_{\{T(\yrep) \geq T(\yv)\}} p\left( \yrep\mid\yv\setminus T(\yv) \right)d\yrep,
\end{equation}
where $T(\cdot)$ is any test statistic, and where we define the partial posterior predictive and partial posterior distributions as
\begin{align}
  \label{eq:partial_post}
  p\left( \yrep\mid\yv\setminus T(\yv) \right) &= \int p(\yrep\mid\thetav)p(\thetav\mid\yv\setminus T(\yv))d\thetav,\nonumber\\
  p\left( \thetav\mid \yv\setminus T(\yv) \right) &\propto \frac{p(\yv\mid\thetav)}{p(T(\yv)\mid\thetav)}p(\thetav).
\end{align}

By ensuring that the information contained in $T(\yv)$ is not accounted for in the posterior distribution, the partial $p$-value removes the double use of data present in the posterior predictive $p$-value. Furthermore, it has been shown in \cite{RobinsEtAl00} that the partial $p$-value has an asymptotically uniform distribution in the limit of large i.i.d. samples under common regularity conditions, thus eliminating the conservativity problem in this limit.

Given that the conditional $p$-values can only be evaluated as a distribution rather than a single numeric metric, it is unclear how exactly to extend the notion of conservativity to apply to the conditional $p$-value. While we have shown the conditional $p$-value to be ``more powerful'' in a certain general sense of containing more information (and hence giving us more opportunities to ``reject'' a base model), it certainly cannot meet the interpretational demand of being asymptotically uniform in any sense.

The partial $p$-value similarly improves over the posterior predictive $p$-value by potentially making us aware of problems with our model that would otherwise go unnoticed due to the double use of data in the posterior predictive $p$-value. However, the partial $p$-value also comes with a number of significant drawbacks:
\begin{itemize}
\item If the density $p(T(\yv)\mid\thetav)$ is difficult to compute (which we expect to be true in all but the simplest models and for all but the simplest test statistics), then the partial $p$-value may also be difficult to compute. In particular, there is no general computational scheme for estimating $p_T^{\mathrm{part}}$.
\item The partial $p$-value can most directly be seen as an evaluation of the partial posterior $p(\thetav\mid\yv\setminus T(\yv))$ rather than $p(\thetav\mid\yv)$, which can create interpretational difficulties. For example, if $T$ is itself a sufficient statistic for $\thetav$, then the partial posterior is just the prior, and the partial $p$-value reduces to the prior predictive $p$-value. But, as has been noted elsewhere (e.g. \cite{PostDiscrep}), the prior predictive $p$-value is undefined when the prior is improper and can be inapproriate as a model check insofar as it ignores the structure of the likelihood entirely.
\item Even if we are confident that a small partial $p$-value is revealing a problem of the fit of our model to our data, it provides no direct clues for how we might modify the model in order to improve the fitness to the chosen test statistic.
\end{itemize}

These three difficulties stand in contrast to the conditional $p$-value, which can be computed directly and generally, comes with an unambiguous interpretation, and can provide clues as to how to improve a model through the joint structure between parameters and conditional $p$-values under the posterior distribution. Furthermore, these difficulties are not unique to the partial $p$-value. Other alternative $p$-values (e.g. the $U$-conditional $p$-value) also face these same issues.

A further problem for the partial $p$-value that frustrates its use even on its own terms is that the asymptotic uniformity of its distribution under the proposed model only holds for large i.i.d. samples. Since the model expansion process leads us towards a large parameter limit rather than a large data limit, it is not clear that the asymptotic uniformity can be expected to be a good approximation to the true distribution of the partial $p$-value as we build larger models.

Thus, if we move from the question of how to determine whether a proposed model is the true data generating process to the question of how to build better models that capture more of the relevant patterns in our data, then we believe that the conditional $p$-value is a richer and more practical tool than $p$-values focused narrowly on approximate uniformity of their distributions. Furthermore, by revealing information about model fitness which may otherwise be obscured by averaging over the posterior, the conditional $p$-value may also eliminate the worst effects of conservativity and low ``power''.

\end{document}
