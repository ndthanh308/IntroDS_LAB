\section{Our Algorithms}\label{sec:algos}

Now we give formal descriptions of the algorithms we study.
We defer to \cite{moitra2022provably} for descriptions of ZAMinfluence, \textsc{PartitionAndApprox}, and \textsc{NetApprox}.

\subsection{Mathematical Programs for Robustness Auditing}\label{ssec:gurobi}
Moitra and Rohatgi observe that a fractional version of robustness auditing for linear regression with $n$ samples in $d$ dimensions can be cast as a mathematical program with $n$ fractional $[0,1]$ variables, $d$ real-valued variables, bilinear constraints which are linear in the binary and real-valued variables respectively, and a linear objective function.  Effectively, they consider a version of the problem in which weights do not have to be binary but may instead be chosen as fractional values and the last OLS coefficient $\beta_d$ is constrained to be 0, and write it as follows

\begin{equation*}
\begin{aligned}
&&&n\quad-\quad\max_{\boldsymbol{\beta}\in \mathrm{R}^{d-1}, \boldsymbol{w}\in[0,1]^n} \quad |\boldsymbol{w}|_1  \\
&&&\text{s.t. } %\quad \forall i\in\{1,\ldots,n\}: r_i = \sum_{j=2}^d X_{j,i}\beta_j - y_i \\
%&&&\quad 
\sum_{i=1}^nw_iX_{i,j'}\left(\sum_{j=1}^{d-1} X_{i,j}\beta_j - Y_i\right) = 0 \quad \forall j'\in[d]%\\ 
% &&&
% \qquad\beta_d=0\\
\end{aligned}
\end{equation*}

The $d$ bilinear constraints together enforce that the gradient of the OLS squared-error is $0$ at $\beta$ for the regression instance specified by the weights $w$ -- i.e, that $\beta$ is an OLS regressor for this weighted regression problem.
The constraints implicitly ensure that $\beta_d$, the last coefficient of $\beta$, is $0$, because the residual term $\sum_{j=1}^{d-1} X_{i,j} \beta_j - Y_i$ does not include any $\beta_d$ term.

The previous mathematical program solves a fractionally relaxed version of the stability problem. In this relaxed variant it is guaranteed that an optimal solution would set $\beta_d=0$.  For the integral problem we need to include an explicit variable for $\beta_d$; we can then write
\begin{equation*}
\begin{aligned}
&&&n\quad-\quad\max_{\boldsymbol{\beta}\in \mathrm{R}^{d}, \boldsymbol{w}\in\{0,1\}^n, \boldsymbol{r}\in \mathrm{R}^n} \quad |\boldsymbol{w}|_1  \\
&&&\text{s.t. } %\quad \forall i\in\{1,\ldots,n\}: r_i = \sum_{j=2}^d X_{j,i}\beta_j - y_i \\
%&&&\quad 
\sum_{i=1}^nw_iX_{i,j'}\left(\sum_{j=1}^d X_{i,j}\beta_j - Y_i\right) = 0 \quad \forall j'\in[d]\\ 
&&&
\qquad\beta_d\leq 0\\
\end{aligned}
\end{equation*}

Though these problems are nonconvex, with the terms $w_i\beta_j$ appearing in the constraints, they can be solved using exact solver methods, including those supported from Gurobi 9.0 onwards \cite{achterberg2020s}. In particular, these methods apply a globally optimal spatial branch-and-bound method which recursively partitions the feasible region into subdomains and invokes McCormick inequalities to obtain lower and upper bounds within each subdomain. We refer the reader to \cite{belotti2013mixed} for an overview of the general theory underlying these methods.

\paragraph{Implementation details.} 
Given unconstrained runtime, Gurobi is guaranteed to solve both the fractional and the integral quadratically constrained optimization problems. In practice, we find on all of our instances that Gurobi identifies good solutions much quicker for the fractional problem (for low-dimensional problems this entails provably small error, for high-dimensional problems the best heuristic solutions we can identify).  In all of our instances the fractional solution can be easily rounded to an integral one by just rounding every weight that is strictly smaller than 1 (with some numerical tolerance) to 0. We then run OLS on the subset of samples given by the rounded weights to confirm that the result has a negative final coefficient $\beta_d$. Alternatively, one can warm-start Gurobi on the integer-constraint instance using the rounded weights; however, we have found no instances where this gives improved solutions.

Directly running Gurobi on the integer-constrained instance, without a warm start obtained by rounding a fractional solution, often shows significantly worse performance (e.g., on Eubank and Fresh's data we can identify an upper bound of 28, by rounding the fractional solution, within seconds, whereas the integer-constrained optimization takes more than 30 minutes to identify an upper bound of 187).

On one of the Microcredit instances \cite{angelucci2015microcredit}, we identified an idiosyncratic behavior of the Gurobi solver: it returns an incorrect (claimed optimal) upper bound of $|\boldsymbol{w}|_1=0$ when solving the fractional problem. However, with the added constraint $|\boldsymbol{w}|_1\geq1$, Gurobi solves the fractional instance optimally. That constraint affects the performance on other instances, so we only include it when not including it leads to an incorrect upper bound of 0.

% \begin{equation*}
% \begin{aligned}
% &&&\max_{\boldsymbol{\beta}\in \mathrm{R}^{d-1}, \boldsymbol{w}\in[0,1]^n, \boldsymbol{r}\in \mathrm{R}^n} \quad |\boldsymbol{w}|_1  \\
% &&&\text{s.t. } \quad r_j = \sum_i w_jX_{j,i}\beta_{j,i}-y_i \quad\forall j \\
% &&&\quad \sum_{j=1}^n\left(w_jX_{j,i}r_{j}\right) = 0 \quad \forall k\in[d]\\ 
% &&&
% \qquad\beta_i=0\\
% \end{aligned}
% \end{equation*}

% $(X,y) = (X_1,y_1),\ldots,(X_n,y_n) \in \R^{d+1}$ and a coordinate $i \in [d]$:
% \[
    
%   \text{Stability}(X,y) = \min \, |S| \, \text{ such that } \, \text{sign}(\beta^{OLS, \, [n] \setminus S}_i) \neq \text{sign}(\beta^{OLS, [n]}_i)
% \]
% where $\beta^{OLS, \, [n]\setminus S}_i$ 



\subsection{Nearly-Linear Time Algorithm for Single Binary Treatment Variable}\label{ssec:binary_treatment}
For the simplest regression problems, with a single binary treatment variable and a real-valued outcome, we show that Stability is computable in time $O(n \log n)$.
The algorithm below assumes that the OLS regression for the input $(X,Y)$ has positive slope; otherwise replace each $Y_i$ by $-Y_i$.

The simple insight behind the algorithm is that if each $X_i \in \{0,1\}$ and we commit to removing $k \leq n$ samples, then the best subset of samples to remove to minimize the slope of the regression line consists of samples with $X_i = 0$ and minimum $Y_i$s, and samples with $X_i = 1$ and maximum $Y_i$s.



\begin{algorithm}
\caption{Exact Algorithm For Auditing Binary 2D Regression}
\label{alg:binary}
\begin{algorithmic}[1]
\Procedure{RobustnessAuditBinary}{$X,Y$}
    \State $n \gets \text{length}(X)$
    \State Let $Y^i = \{Y_j | X_j = i\}$ for $i \in \{0, 1\}$, sort $Y^0$ in decreasing, $Y^1$ in increasing order
    \State Set  $S^0_\ell$ as cumulative sums of the first $\ell$ terms of  $Y^0$ for $\ell\in 1,\ldots,|Y^0|$
    \State Set  $S^1_\ell$ as cumulative sums of the first $\ell$ terms of  $Y^1$ for $\ell\in 1,\ldots,|Y^1|$

    % \For{each suffix $S$ of $Y^0$ and $Y^1$}
    %     \Comment{Execute in $O(n)$ time by memoizing intermediate sums}
    %     \State $M_{S} \gets \sum_{i \in S} + (1, X_{i}) (1, X_{i})^\top$
    %     \State $V_{S} \gets \sum_{i \in S} X_i Y_i$
    % \EndFor
    \State $lower, upper\gets 0, n$ \Comment{$lower$ is too small and $upper$ is sufficient to flip sign}
    % \State Perform binary search on $k \in \{0, \ldots, n\}$ to find minimum number of points to remove
    \While{\textsc{True}}
        \State $\textsc{Flag}\gets\textsc{False}$; \quad $k\gets \lfloor (lower+upper)/2\rfloor$
        \For{$\ell \gets \max\{0,n-k-|Y^1|\}$ to $\min\{|Y^0|,n-k\}$} \Comment{Iterate over number of 0s to drop}
        \State \textbf{if} $-(n-k-\ell)*S^0_\ell + \ell* S^1_{n-k-\ell}\leq 0$ \textbf{do} $\textsc{Flag}\gets\textsc{True}$
        \EndFor
        \State \textbf{if} $lower=k=upper-1$ and \textsc{Flag}: \quad \Return $k$
        \State \textbf{if} $lower=k=upper-1$ and \textsc{not Flag}: \quad \Return $k+1$
        \State \textbf{if} $\textsc{Flag}$: $upper\gets k$;\quad \textbf{else}: $lower\gets k$
    \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}

We capture correctness of Algorithm~\ref{alg:binary} in the following theorem.

\begin{theorem}
  Given $X_1,\ldots,X_n \in \{0,1\}$ and $Y_1,\ldots,Y_n \in \R$, Algorithm~\ref{alg:binary} outputs $\text{Stability}(X,Y)$ in time $O(n \log n)$.
\end{theorem}
\begin{proof}
  To prove correctness of Algorithm~\ref{alg:binary}, we need to show two things.
  First, to correctly apply binary search, we need to show a monotonicity property: if there is a subset of $k$ samples which we can remove to change the OLS slope then for every $k' > k$ there is also such a subset of $k'$ samples.
  Second, we need to show correctness of the greedy step: if there is a subset of $k$ samples which change the sign of the slope of the OLS regression line when removed, then there exists such a subset which, for some $\ell \leq k$, removes the $\ell$ samples such that $X_i = 0$ with least $Y_i$ and the $k-\ell$ samples with $X_i = 1$ and greatest $Y_i$.

  For both these goals, let $S_1 = \{i \in [n] \, : \, X_i = 1\}$ and consider some subset $U \subseteq [n]$ with $|U| = n-k$.
  We derive an explicit formula for the slope of the OLS line on the dataset $\{(X_i,Y_i)\}_{i \in U}$.
  \begin{align*}
  \beta & = \Paren{\sum_{i \in U} (1,X_i) (1,X_i)^\top}^{-1} \sum_{i \in U} (1,X_i) \cdot Y_i \\
  & = \Paren{
  \begin{matrix} |U| & |U \cap S_1| \\
  |U \cap S_1| & |U \cap S_1|
  \end{matrix} }^{-1} \cdot \Paren{\begin{matrix} \sum_{i \in U} Y_i \\ \sum_{i \in U} X_i Y_i \end{matrix}} \\
  & = \frac 1 { |U| |U \cap S_1| - |U \cap S_1|^2} \cdot \Paren{\begin{matrix} |U \cap S_1|  & -|U \cap S_1| \\ -|U \cap S_1| & |U| \end{matrix} } \cdot \Paren{\begin{matrix} \sum_{i \in U} Y_i \\ \sum_{i \in U} X_i Y_i \end{matrix}} \, .
  \end{align*}
  The sign of the second coordinate of $\beta$ (which gives the slope) is the same as the sign of
  \[
  - |U \cap S_1| \cdot \sum_{i \in U} Y_i + |U| \cdot \sum_{i \in U \cap S_1} Y_i = - |U \cap S_1| \cdot \sum_{i \in U\cap S_0} Y_i + |U\cap S_0|\sum_{i \in U\cap S_1} Y_i
  \]
  Among all $U$ with a given $|U|$ and $|U \cap S_1|$, this expression is clearly minimized by minimizing $\sum_{i \in U \cap S_1} Y_i$ and maximizing $\sum_{i \in U \cap S_0} Y_i$.
  This establishes correctness of the greedy step.

  Now suppose that the OLS slope on $U$ is non-positive; we need to show that the same holds for some $U'$ with $|U'| = |U| - 1$, to establish correctness of binary search.
  By the above, we can assume
  \[
  -|U \cap S_1| \cdot \sum_{i \in U} Y_i + |U| \cdot \sum_{i \in U \cap S_1} Y_i \leq 0
  \]
  which rearranges to
  \[
  \frac{\sum_{i \in U \cap S_1} Y_i}{|U \cap S_1|} \leq \frac{\sum_{i \in U \cap S_0} Y_i} {|U \cap S_0|} \, .
  \]
  This is clearly preserved by removing from $U$ either one of $i^* = \arg \max_{i \in U \cap S_1} Y_i$ and $j^* = \arg \min_{j \in U \cap S_0} Y_j$.
\end{proof}

\subsection{Nearly-Linear Time Algorithm for Difference-in-Differences}\label{ssec:diffs}
We study the following difference-in-differences regression setting.
$N$ individuals in two groups, treatment and non-treatment, each report two responses, $Y_{i,\text{before}},Y_{i,\text{after}} \in \R$.
Here ``before'' and ``after'' correspond, respectively, to before and after the time at which the treatment group is treated.
The difference-in-differences linear model is then
\[
Y = \beta_0 + \beta_{1} \cdot \text{time} + \beta_{2} \cdot \text{treatment} + \beta_{3} \cdot \text{time} \times \text{treatment} \,
\]
where ``time'', ``treatment'' assume values in $\{0,1\}$, and the coefficient of interest is $\beta_3$.
We study Stability with respect to the removal of individuals from the dataset -- note that removing an individual corresponds to removing two data points, ``before'' and ``after''.

%The key mathematical reformulation which leads to our algorithm is captured in the 
The following lemma motivates our algorithm; it is based on a standard closed-form expression of the difference-in-difference regressor and is proved for completeness in Section~\ref{sec:proof-lem-diff-in-diffs}. 

\begin{lemma}
\label{lem:diff-in-diffs-reformulation}
    Let $Y_{1,\text{before}},Y_{1,\text{after}},\ldots,Y_{N,\text{before}},Y_{N,\text{after}} \in \R$ be a difference-in-differences dataset with $N$ individuals of which a subset $T \subseteq [N]$ are treated.
    For a subset $U \subseteq [N]$ of individuals, the coefficient $\beta_3^U$ of difference-in-differences on the dataset $U$ has the same sign as
    \[
    \E_{i \sim U \cap T} (Y_{i,\text{after}} - Y_{i,\text{before}} ) -  
    \E_{i \sim U} (Y_{i,\text{after}} - Y_{i,\text{before}}) \, .
    \]
\end{lemma}

Now we can state our algorithm.
The algorithm below assumes that $\beta_3$ on the whole dataset is nonnegative; otherwise simply negate all the $Y$s.


\begin{algorithm}
\caption{Exact Algorithm For Auditing Difference-in-Differences}
\label{alg:diff-in-diffs}
\begin{algorithmic}[1]
\Procedure{RobustnessAuditDiffInDiffs}{$(Y_{1,\text{before}}, Y_{1,\text{after}}),\ldots, (Y_{N,\text{before}}, Y_{N,\text{after}})$, $T \subseteq [N]$}
    \State $\delta_i \gets Y_{i,\text{after}} - Y_{i,\text{before}}\quad\forall i \in [N]$;\qquad $\Delta_{T} \gets \{\delta_i \, : \, i \in T\}$; \qquad $\Delta_{\overline{T}} = \{ \delta_i \, : \, i \in [N] \setminus T\}$.
    \State Sort $\Delta_T$ in increasing and $\Delta_{\overline{T}}$ in decreasing order
    \State Store partial sums $S^T_\ell$ defined as the sum of the first $\ell$ terms of $\Delta_T$ for $\ell\leq |T|$ 
    \State Store partial sum $S^{\overline{T}}_\ell$ defined as the sum of the first $\ell$ terms of $\Delta_{\overline{T}}$ for $\ell\leq N-|T|$
    % \State Sort $\Delta_{\overline{T}}$ in decreasing order and store all partial sums $\sum_{i \leq m} \Delta_{\overline{T}}(i)$ for $m \leq N - |T|$.
    % \State $n \gets \text{length}(X)$
    % \State Let $Y^i = \{Y_j | X_j = i\}$ for $i \in \{0, 1\}$, sort $Y^0$ in decreasing, $Y^1$ in increasing order
    % \State Set  $S^0_\ell$ as cumulative sums of the first $\ell$ terms of  $Y^0$ for $\ell\in 1,\ldots,|Y^0|$
    % \State Set  $S^1_\ell$ as cumulative sums of the first $\ell$ terms of  $Y^1$ for $\ell\in 1,\ldots,|Y^1|$

    % \For{each suffix $S$ of $Y^0$ and $Y^1$}
    %     \Comment{Execute in $O(n)$ time by memoizing intermediate sums}
    %     \State $M_{S} \gets \sum_{i \in S} + (1, X_{i}) (1, X_{i})^\top$
    %     \State $V_{S} \gets \sum_{i \in S} X_i Y_i$
    % \EndFor
    \State $lower, upper\gets 0, N$ \Comment{$lower$ is too small and $upper$ is sufficient to flip sign}
    % \State Perform binary search on $k \in \{0, \ldots, n\}$ to find minimum number of points to remove
    \While{\textsc{True}}
        \State $\textsc{Flag}\gets\textsc{False}$; \quad $k\gets \lfloor (lower+upper)/2\rfloor$
        \For{$\ell \gets \max\{0,k-|T|-N+1\}$ to $\min\{|T|-1,k\}$} \Comment{Iterate over \# of treated to remove}
        \State \textbf{if} $\frac{S_{|\Delta_T|-\ell}^{T}}{|T|-\ell}
        -\frac{S_{|\Delta_T|-\ell}^{T}+S_{|\Delta_{\overline{T}}|-(k-\ell)}}{N-k}
        \leq 0$ \textbf{do} $\textsc{Flag}\gets\textsc{True}$
        \EndFor
        \State \textbf{if} $lower=k=upper-1$ and \textsc{Flag}: \quad \Return $k$
        \State \textbf{if} $lower=k=upper-1$ and \textsc{not Flag}: \quad \Return $k+1$
        \State \textbf{if} $\textsc{Flag}$: $upper\gets k$;\quad \textbf{else}: $lower\gets k$
    \EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}


We show:
\begin{theorem}
    Algorithm~\ref{alg:diff-in-diffs} runs in $O(N\log N)$-time algorithm, taking $N$ individuals $\{(Y_{i,\text{before}}, Y_{i,\text{after}})\}_{i \in N}$, divided into treated and untreated subgroups, and returns the size of a minimum-size set $U \subseteq [N]$ such that the sign of $\beta_3$ for the dataset $U$ differs from the sign of $\beta_3$ on the dataset $[N]$.
\end{theorem}

\begin{proof}
    Running time is straightforward: lines 4,5 each take $O(N \log N)$ time to sort and $O(N)$ time for the partial sums.
    Then binary search requires $O(\log N)$ rounds, each requiring $O(N)$ times through the for loop on line 8; each execution of the loop is constant time using the stored partial sums.

    For correctness, we need to argue two things.
    First, we must show that if there is a subset of $k$ individuals which can be removed to make $\beta_3$ negative, then there is also such a subset of $k+1$ individuals -- this gives correctness of binary search.
    Second, we must show that if there is such a subset of $k$ individuals, then for some $\ell \leq k$ it can be found by removing the $\ell$ treated individuals with greatest $\delta_i$ and the $k-\ell$ non-treated individuals with least $\delta_i$.

    We start with monotonicity.
    Suppose that some subset of individuals $U \subseteq [N]$ has non-positive $\beta_3$.
    Then by Lemma~\ref{lem:diff-in-diffs-reformulation}, $\E_{i \sim U \cap T} \delta_i \leq \E_{i \sim U} \delta_i$.
    If some non-treated individual $j \in U$ has $\delta_j \leq \E_{i \sim U} \delta_i$, then we can remove $j$ from $U$ and maintain the inequality $\E_{i \sim (U \setminus \{j\}) \cap T} \delta_i \leq \E_{i \sim (U \setminus \{j\})} \delta_i$.

    Otherwise, every non-treated individual $j \in U$ has $\delta_j > \E_{i \sim U} \delta_i$.
    Now we remove any non-treated individual $j$ and obtain
    \[
    \E_{i \sim U \setminus \{j\}} \delta_i
    = \frac{|U \cap T|}{|U|-1} \cdot \E_{i \sim U \cap T} \delta_i + \frac{|U \cap \overline{T}|-1}{|U|-1} \cdot \E_{i \sim U \cap \overline{T} \setminus \{j \}} \delta_i
    \geq \frac{|U \cap T|}{|U|-1} \cdot \E_{i \sim U \cap T} \delta_i + \frac{|U \cap \overline{T}|-1}{|U|-1} \cdot \E_{i \sim U} \delta_i 
    \]
    where for the last inequality we used that every nontreated $j \in U$ has $\delta_j > \E_{i \sim U} \delta_i$.
    By hypothesis, $\E_{i \sim U} \delta_i \geq \E_{i \sim U \cap T} \delta_i$, so overall we got $\E_{i \sim U \setminus \{j\}} \delta_i \geq \E_{i \sim U \cap T} \delta_i$ as desired.

    Correctness of the greedy step is clear from Lemma~\ref{lem:diff-in-diffs-reformulation}.
\end{proof}



\subsubsection{Proof of Lemma~\ref{lem:diff-in-diffs-reformulation}}
\label{sec:proof-lem-diff-in-diffs}
We turn to the proof of Lemma~\ref{lem:diff-in-diffs-reformulation}, starting with some setup.

We reformulate difference-in-differences as an OLS regression problems with the usual $(X_i,Y_i)$ pairs.
Each individual contributes two vectors $X_{i,\text{before}}, X_{i,\text{after}} \in \{0,1\}^4$, where the first coordinate corresponds to the intercept of the regression line and the remaining three coordinates are time, treatment, and time $\times$ treatment, respectively. That is,
\begin{align*}
& X_{i,\text{before}}(0) = 1\\
& X_{i,\text{before}}(1) = 0\\
& X_{i,\text{before}}(2) = 0 \text{ if $i$ is not treated and otherwise } 1\\
& X_{i,\text{before}}(3) = 0\\
& X_{i,\text{after}}(0) = 1\\
& X_{i,\text{after}}(1) = 1\\
& X_{i,\text{after}}(2) = 0 \text{ if $i$ is not treated and otherwise } 1\\
& X_{i,\text{after}}(3) = 0 \text{ if $i$ is not treated and otherwise } 1 \, .
\end{align*}

We need one definition:
\begin{definition}
  Let $n \in \N$ and $s \in \N$ with $s \leq n$.
  The $(n,s)$-diff-in-diff covariance matrix is:
  \[
  \Sigma_{n,s} = \left ( \begin{matrix}
      n          & n/2 & s          & s/2 \\
      n/2 & n/2 & s/2 & s/2 \\
      s          & s/2 & s          & s/2 \\
      s/2 & s/2 & s/2 & s/2
  \end{matrix} \right ) \, .
  \]
  Note that for a diff-in-diff dataset with $n/2$ individuals and hence $n$ samples $X_i$, where $s/2$ of those individuals are in the treatment group, $\Sigma_{n,s} = \sum_{i \leq n} X_i X_i^\top$.
\end{definition}

The following is easy to check in e.g. Mathematica:

\begin{fact} 
\[
\det \Sigma_{n,s} \cdot \Sigma_{n,s}^{-1} =
\frac 1 8 \cdot \left(
\begin{array}{cccc}
 s^2 (n-s) & s^2 (s-n) & s^2 (s-n) & s^2 (n-s) \\
 s^2 (s-n) & 2 s^2 (n-s) & s^2 (n-s) & 2 s^2 (s-n) \\
 s^2 (s-n) & s^2 (n-s) & n s (n-s) & n s (s-n) \\
 s^2 (n-s) & 2 s^2 (s-n) & n s (s-n) & 2 n s (n-s) \\
\end{array}
\right)
\]
\end{fact}

\begin{proof}[Proof of Lemma~\ref{lem:diff-in-diffs-reformulation}]
Let $U \subseteq [N]$ be a subset of individuals in a diff-in-diffs dataset, where $T \subseteq [N]$ are the treated individuals and where $|U| = m/2$ and $U$ contains $s/2$ treatment individuals.
Then
\[
  \beta^U = \Sigma_{m,s}^{-1} \cdot \sum_{i \in U} X_{i,\text{before}} Y_{i,\text{before}} + X_{i,\text{after}} Y_{i,\text{after}}
\]

Since $\Sigma_{m,s} \succeq 0$ and hence $\det \Sigma_{m,s} \geq 0$, the sign of $\beta^U_3$ is the same as the sign of
\[
(s^2 (m-s), 2s^2 (s-m), ms(s-m), 2ms(m-s))^\top \sum_{i \in U} X_{i,\text{before}} Y_{i,\text{before}} + X_{i,\text{after}} Y_{i,\text{after}} \, ,
\]
which, since $s \geq 0$ and $m-s \geq 0$, has the same sign as
\[
(s, -2s, -m, 2m)^\top \sum_{i \in U} X_{i,\text{before}} Y_{i,\text{before}} + X_{i,\text{after}} Y_{i,\text{after}} \, .
\]
Dividing by $sm$, applying the definition of $X_{i,\text{before}}$ and $X_{i,\text{after}}$, and simplifying, this has the same sign as
\[
\E_{i \sim U} (Y_{i,\text{before}} + Y_{i,\text{after}} ) - 2 \E_{i \sim U} Y_{i,\text{after}} - \E_{i \sim U \cap T} (Y_{i,\text{before}} + Y_{i,\text{after}} ) + 2 \E_{i \sim U \cap T} Y_{i,\text{after}} \, ,
\]
which rearranges to the conclusion of the lemma.
\end{proof}

\subsection{Spectral Algorithm}\label{ssec:spectral}
In this section we describe and analyze our spectral robustness auditor.

\begin{algorithm}
\caption{Spectral Robustness Auditing}
\label{alg:spectral}
\begin{algorithmic}[1]
\Procedure{RobustnessAuditSpectral}{$X,Y$}
    \State $n \gets \text{len}(X)$
    \State $\beta \gets \text{OLS}(X,Y)$
    \State $M_1 \gets $ a $d \times n$ matrix where $i$-th column is $X_i \cdot (\iprod{X_i,\beta} - y_i)$
    \State $\Sigma \gets \tfrac 1 n \sum_{i \leq n} X_i X_i^\top$
    \State $C_1 \gets \|\Sigma^{-1/2} M_1\| / \sqrt{n}$ (maximum singular value of $\Sigma^{1/2} M_1 / \sqrt{n}$)
    \State $M_2 \gets $ a $d^2 \times n$ matrix where the $i$-th column is $\Sigma^{-1/2}X_i \otimes \Sigma^{-1/2} X_i$.
    \State $\Phi \gets $ a $d^2$-length vector where $\Phi_{i,i} = 1$ and $\Phi_{i,j} = 0$ if $i \neq j$.
    \State $W \gets (\tfrac 3 {2+d})^{1/2} \cdot \tfrac{\Phi \Phi^\top}{d} + (\tfrac 3 2)^{1/2} \cdot (I - \tfrac{\Phi \Phi^\top}{d})$.
    \State $C_2 \gets \|W M_2  / \sqrt{n}\|$ (maximum singular value of $W M_2 / \sqrt{n}$)
    \State $\varepsilon \gets \frac {\beta_i^2}{C_1 \cdot \sqrt{\Sigma^{-1}_{i,i}} + C_2 |\beta_i|}$
    
    \Return $\varepsilon$
\EndProcedure
\end{algorithmic}
\end{algorithm}


The key lemma is the following one, which is explicit to varying degrees in prior works such as \cite{klivans2018efficient,bakshi2021robust}.
We provide a short proof for completeness.

\begin{lemma}[Implicit in \cite{bakshi2021robust}]
\label{lem:spectral-correct-main}
  Let $X_1,\ldots,X_n \in \R^d, y_1,\ldots,y_n \in \R$ and let $\beta$ be the solution to OLS on $\{(X_i,y_i)\}_{i \in [n]}$.
  Let $\Sigma = \tfrac 1 n \sum_{i \leq n} X_i X_i^\top$.
  Let $C_1,C_2 \geq 0$ satisfy the following inequalities for every $v \in \R^d$:
  \begin{align*}
      & \frac 1 n \sum_{i \leq n} \iprod{X_i,v}^2 (\iprod{X_i,\beta} - y_i)^2 \leq C_1 \cdot \iprod{v, \Sigma v} \\
      & \frac 1 n \sum_{i \leq n} \iprod{X_i,v}^4 \leq C_2 \cdot \iprod{v, \Sigma v}^2 \, .
  \end{align*}
  Let $S \subseteq [n]$ and let $\beta_S$ be the solution to OLS on $\{(X_i,y_i)\}_{i \in S}$.
  Then
  \[
    \frac{n- |S|}{n} \geq \frac{(\beta_S - \beta)_1^2}{ \left( \sqrt{C_1} \|\Sigma^{-1/2} e_1\| + |(\beta_S - \beta)_1|\sqrt{C_2} \right)^2} \, ,
  \]
  where $(\beta_S - \beta)_1$ is the first coordinate of the vector $(\beta_S - \beta)$.
\end{lemma}

This allows us to prove:

\begin{theorem}
    Algorithm~\ref{alg:spectral} returns a valid lower bound on $\text{Stability}(X,Y)$ using $O(1)$ top singular values of matrices of dimension at most $n \times d^2$, a single $d \times d$ matrix inverse, and additional running time $O(nd^2)$.
\end{theorem}

\begin{proof}
In light of Lemma~\ref{lem:spectral-correct-main}, to prove correctness of Algorithm~\ref{alg:spectral} we just need to show that $C_1$ and $C_2$ as computed in Algorithm~\ref{alg:spectral} satisfy the hypotheses of Lemma~\ref{lem:spectral-correct-main}.
For $C_1$ this is clear by construction.

For $C_2$, we first observe that by replacing $v$ with $\Sigma^{-1/2} v$ we can just as well prove that for all $v \in \R^d$,
\[
\frac 1 n \sum_{i \leq n} \iprod{\Sigma^{-1/2} X_i, v}^4 \leq C_2 \|v\|^4 = C_2 \cdot (v \otimes v)^\top (\tfrac 2 3 I + \tfrac 1 {3} \Phi \Phi^\top) (v \otimes v) \, .
\]
Simple linear algebra shows that the matrix $W$ in Algorithm~\ref{alg:spectral} is exactly $(\tfrac 2 3 I + \tfrac 1 {3} \Phi \Phi^\top)^{-1/2}$.
So replacing $v \otimes v$ with $W^{-1/2} (v \otimes v)$ shows that $\|W M_2 / \sqrt{n}\|$ is a valid choice for $C_2$.
This proves correctness of Algorithm~\ref{alg:spectral}.
The running time is clear from inspection.
\end{proof}

\subsubsection{Proof of Lemma~\ref{lem:spectral-correct-main}}
To prove the lemma we need the following claim, which says $\tfrac 1 n \sum_{i \in S} X_i X_i^\top$ isn't too different from $\Sigma$.

\begin{claim}
\label{clm:covariance-lb}
  Let $X_1,\ldots,X_n,y_1,\ldots,y_n, \Sigma, S$, and $C_2$ be as in Lemma~\ref{lem:spectral-correct-main}.
  Let $\Sigma_S = \tfrac 1 n \sum_{i \in S} X_i X_i^\top$.
  Then 
  \[
  \Sigma_S \succeq \Paren{1 - \sqrt{C_2 \cdot \tfrac{n-|S|}{n}}} \cdot \Sigma \, .
  \]
\end{claim}
\begin{proof}[Proof of Claim~\ref{clm:covariance-lb}]
  Let $v \in \R^d$.
  We have
  \[
  \frac 1 n \sum_{i \in \overline{S}} \iprod{X_i,v}^2 = \frac 1 n \sum_{i \leq n} 1(i \notin S) \cdot \iprod{X_i,v}^2 \leq \sqrt{\frac 1 n \sum_{i \leq n} 1(i \notin S)} \cdot \sqrt{\frac 1 n \sum_{i \leq n} \iprod{X_i,v}^4}
  \leq \sqrt{\frac{n - |S|}{n}} \cdot \sqrt{C_2} \cdot \iprod{v, \Sigma v} \, ,
  \]
  where the inequality is Cauchy-Schwarz.
  So,
  \[
  \iprod{v, \Sigma_S v} = \frac 1 n \sum_{i \in S} \iprod{X_i,v}^2 = \iprod{v, \Sigma v} - \frac 1 n \sum_{i \in \overline{S}} \iprod{X_i,v}^2 \geq \Paren{1 - \sqrt{C_2 \cdot \frac{n - |S|}{n}}} \cdot \iprod{v, \Sigma v} \, ,
  \]
  which is what we wanted to show.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:spectral-correct-main}]
  Let $\Sigma_S = \tfrac 1 n \sum_{i \in S} X_i X_i^\top$.
  We start by bounding $\|\Sigma_S^{1/2} (\beta_S - \beta)\|^2$:
  \begin{align*}
    \|\Sigma_S^{1/2} (\beta_S - \beta)\|^2 & = \iprod{\beta_S - \beta, \Paren{\frac 1 n \sum_{i \in S} X_i X_i^\top } (\beta_S - \beta) } \\
    & = \iprod{\beta_S - \beta, \frac 1 n \sum_{i \in S} X_i (\iprod{X_i, \beta_S} - y_i) - \frac 1 n \sum_{i \in S} X_i (\iprod{X_i, \beta} - y_i)} \text{ by adding and subtracting $X_i y_i$} \\
    & = \iprod{\beta_S - \beta, - \frac 1 n \sum_{i \in S} X_i (\iprod{X_i, \beta} - y_i)} \text{ since $\beta_S$ minimizes $\sum_{i \in S} (\iprod{X_i, \beta_S} - y_i)^2$.} \\
    & = \iprod{\beta_S - \beta, - \frac 1 n \sum_{i \leq n} X_i (\iprod{X_i, \beta} - y_i) + \frac 1 n \sum_{i \in \overline{S}} X_i (\iprod{X_i, \beta} - y_i) } \\
    & = \iprod{\beta_S - \beta, \frac 1 n \sum_{i \in \overline{S}} X_i (\iprod{X_i, \beta} - y_i)} \text{ since $\beta$ minimizes $\sum_{i \leq n} (\iprod{X_i, \beta} - y_i)^2$.}
  \end{align*}
  The last expression above we can bound via Cauchy-Schwarz.
  It is equal to
  \begin{align*}
  \frac 1 n \sum_{i \leq n} 1(i \notin S) \cdot \iprod{\beta_S - \beta, X_i} \cdot (\iprod{X_i,\beta} - y_i)  & \leq \sqrt{\frac{n-|S|}{n}} \cdot \sqrt{\frac 1 n \sum_{i \leq n} \iprod{\beta_S - \beta,X_i}^2 (\iprod{X_i, \beta} - y_i)^2 } \\
  & \leq \sqrt{\frac{n - |S|}{n}} \cdot \sqrt{C_1} \cdot \sqrt{ \iprod{\beta_S - \beta, \Sigma (\beta_S - \beta)}}\\
  & = \sqrt{\frac{n - |S|}{n}} \cdot \sqrt{C_1} \cdot \|\Sigma^{1/2} (\beta_S - \beta)\| \, ,
  \end{align*}
  where the second inequality uses that $\frac 1 n \sum_{i \leq n} \iprod{X_i,v}^2 (\iprod{X_i,\beta} - y_i)^2 \leq C_1 \cdot \iprod{v, \Sigma v}$ for every $v\in\R^d$.
  Overall, we have obtained
  \begin{align}
      \label{eq:spectral-1}
      \| \Sigma_S^{1/2} (\beta_S - \beta)\|^2 \leq \sqrt{\frac{n-|S|}{n}} \cdot \sqrt{C_1} \cdot \|\Sigma^{1/2} (\beta_S - \beta)\| \, .
  \end{align}
  On the other hand, using Claim~\ref{clm:covariance-lb}, we have
  \begin{align}
  \label{eq:spectral-2}
  \|\Sigma_S^{1/2} (\beta_S - \beta)\|^2 \geq \Paren{ 1 - \sqrt{C_2 \cdot \frac{n - |S|}{n}}} \cdot \|\Sigma^{1/2} (\beta_S - \beta)\|^2 \, 
  \end{align}
  So, putting together \eqref{eq:spectral-1} and \eqref{eq:spectral-2} and dividing both sides by $\left(1 - \sqrt{C_2 \cdot \tfrac{n - |S|}{n}}\right) \cdot \|\Sigma^{1/2} (\beta_S - \beta)\|$,
  \[
  \|\Sigma^{1/2} (\beta_S - \beta)\| \leq \sqrt{\frac{n - |S|}{n}} \cdot \sqrt{C_1} \cdot \frac 1 {1 - \sqrt{C_2 \cdot \frac{n - |S|}{n}}} \, .
  \]
  Finally, the first coordinate of $(\beta_S - \beta)$ is
  \[
  |(\beta_S - \beta)_1| = \left | \iprod{ \Sigma^{-1/2} e_1, \Sigma^{1/2} (\beta_S - \beta)} \right | \leq \|\Sigma^{-1/2} e_1\| \cdot \|\Sigma^{1/2} (\beta_S - \beta)\|
  \]
  where $e_1$ is the first standard basis vector. With $\e = \frac{n - |S|}{n}$, we have obtained
  \[
  |(\beta_S - \beta)_1 | \leq \|\Sigma^{-1/2} e_1\| \cdot \sqrt{\e} \cdot \sqrt{C_1} \cdot \frac{1}{1 - \sqrt{C_2 \e}} \, .
  \]
  Solving for $\e$, we get
  \[
  \e \geq \frac{(\beta_S - \beta)_1^2}{\left(\sqrt{C_1} \|\Sigma^{-1/2} e_1\| + |(\beta_S - \beta)_1|\sqrt{C_2}\right)^2} \, . \qedhere
  \]
\end{proof}

