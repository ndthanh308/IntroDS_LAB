\section{Introduction}
Recently, Broderick, Giordano, and Meager \cite{broderick2020automatic} identified a striking pattern of non-robustness in several high-quality and large-scale econometric studies, ranging from the effects of healthcare enrollment (Oregon Medicaid Study \cite{finkelstein2012oregon}) to the effects of microcredit in developing economies \cite{angelucci2015microcredit, augsburg2015impacts,attanasio2015impacts,banerjee2015miracle,crepon2015estimating,karlan2011microcredit,tarozzi2015impacts}.
Key conclusions of the studies -- e.g. that the effect of a treatment on some outcome is positive and statistically significant -- change when the statistical analyses are re-run with a small but carefully-chosen subset of the sample is removed.
Often, dropping less than $1\%$ of the observations, or even just a single one, suffices.
This phenomenon appears even when the authors of the original studies have run careful robustness checks and removed outliers,
and it appears in the simplest settings, such as regression of a single binary treatment variable against a single real-valued outcome \cite{broderick2020automatic}.
Subsequent work by Kuschnig, Zens, and Crespo Cuaresma \cite{kuschnig2021hidden} reinforces this theme.

This leads to significant concerns about generalization and replicability.
Large studies necessarily use data collection methods which deviate from ``truly'' random sampling -- they use surveys and public records, they impute missing data, etc.
Although study designers devote significant effort to correcting for these imperfections e.g., by re-weighing subpopulations, such correction schemes may still yield samples whose distribution deviates by a few percent from ``truly random'' draws from the underlying population.
Furthermore, even the most careful randomized controlled trials may in other ways draw samples from a population which differs from the population to which the conclusions purportedly apply -- for example, a drug trial conducted in Boston whose results will be used to support nation-wide use of a drug, or the impact of a policy trialed in one country that may be used to inform its use in another.

A false discovery arising because of a few-percent deviation between the sample and underlying population would induce a small, highly influential set of samples -- thus, finding such small influential sets is a potential avenue to detecting such false discoveries. 
This is not the only reason a small influential set could arise, however -- another possibility is that the sample is indeed reflective of the population, and the effect detected in the sample is driven by a small segment of the population.
In this case, too, the small influential set (or knowledge that there is none) should be of significant interest to the researcher.
For further statistical interpretation of small influential sets of samples, see
\cite{broderick2020automatic,kuschnig2021hidden,moitra2022provably} for more discussion.


\paragraph{Robustness Auditing}
Assuming that we are interested in knowing whether the conclusion of a study is non-robust to the removal of a few samples, how can we find the offending samples?
Conversely, how can we be sure that such samples do \emph{not} exist?
Following Moitra and Rohatgi \cite{moitra2022provably}, we call this type of algorithmic task \emph{robustness auditing}.
We focus on least-squares linear regression, due to its simplicity and widespread use.

Concretely, we study algorithms to compute or approximate the following quantity, given a dataset $(X,y) = (X_1,y_1),\ldots,(X_n,y_n) \in \R^{d+1}$ and a coordinate $i \in [d]$:
\[
  \text{Stability}(X,y) = \min \, |S| \, \text{ such that } \, \text{sign}(\beta^{OLS, \, [n] \setminus S}_i) \neq \text{sign}(\beta^{OLS, [n]}_i)
\]
where $\beta^{OLS, \, [n]\setminus S}_i$ denotes the $i$-th coordinate of the ordinary least squares regression vector using the dataset $\{(X_i,y_i)\}_{[n] \setminus S}$.
A naive algorithm to decide whether $\text{Stability}(X,y) \leq t$ is to run one instance of linear regression for each $S \subseteq [n]$ with $|S| \leq t$. 
But this is computationally intractable even for moderate values of $n$ and $t$.
Moitra and Rohatgi \cite{moitra2022provably} show that this intractability is, to some extent, inherent: under standard computational complexity hypotheses, any algorithm that provably computes $\text{Stability}(X,y)$\footnote{Moitra and Rohatgi actually study a \emph{fractional} variant of Stability, where samples can be re-weighted instead of completely removed.} for any $d$-dimensional $X,y$ has worst-case running time at least $n^{\Omega(d)}$.

However, worst-case computational intractability does not preclude the existence of algorithms which compute or approximate Stability well for many instances of linear regression encountered in practice.
In this work we design several algorithms which can perform two related tasks: 
\begin{enumerate}
    \item produce a small set $S$, thus providing an \emph{upper bound} on $\text{Stability}(X,y)$, and
    \item provide a \emph{lower bound} on $\text{Stability}(X,y)$ which is valid on a per-dataset basis -- i.e. which holds without any statistical assumptions on $X,y$.
\end{enumerate}
We highlight that lower bounds on Stability are crucial for robustness auditing.
Optimization methods that approximate $\text{Stability}(X,y)$ by finding some subset of samples $S$ whose removal changes $\text{sign}(\beta_i)$ with no guarantee that it is the smallest one %can 
provide only upper bounds.
This is because given such a subset, there is no way to know if an even smaller set might exist, meaning that algorithms which provide only upper bounds on Stability have limited utility for checking robustness.



\paragraph{Our Contributions}
We design and implement several algorithms for robustness auditing and study their performance empirically on a range of regression problems drawn from recent publications in economics and political science, as well as standard testbed datasets such as Boston Housing \cite{harrison1978hedonic}.
We focus largely on simple and well-studied algorithmic ideas, to lay a foundation for future innovation by establishing performance baselines.
We provide implementations of these algorithms in an accompanying Python software package, \texttt{auditor\_tools}.\footnote{\url{https://github.com/df365/robustness_auditing/tree/main}}


Our main thesis is that standard algorithmic techniques actually provide significantly better than state-of-the-art performance for robustness auditing.
Moreover, the performance of some of these approaches is good enough for mainstream adoption, at least in simple-enough regression problems.
In more detail, we study:
\begin{itemize}
    \item An mixed integer quadratically constrained optimization approach to approximate (or sometimes exactly compute) Stability, using off-the-shelf optimization software (Gurobi \cite{achterberg2020s}). 
    On laptop hardware, this approach scales to regression problems with $10-100$ dimensions and $\approx 10^4$ samples, or $100-1000$ dimensions and $\approx 1000$ samples.
    It provides matching upper and lower bounds on Stability for numerous regression problems drawn from recent publications and standard testbed datasets where prior methods either provide no lower bound or have large gaps between upper and lower bounds.

    \item Efficient greedy algorithms to compute Stability exactly in two simple but common special cases of linear regression: regression of a real-valued outcome against a binary treatment variable and difference-in-difference estimation.
    These algorithms only apply to these special cases but always provide matching upper and lower bounds on Stability and easily scale to datasets with millions of samples.
\end{itemize}

Using these algorithms, we audit several linear regression datasets drawn from prominent econometric studies which prior work investigated using the algorithm of \cite{broderick2020automatic}, ZAMinfluence. We find in these examples that ZAMinfluence frequently fails to find the smallest possible set of samples $S$ to change the sign of an OLS regression coordinate. (Moitra and Rohatgi observe a similar phenomenon in the Boston Housing dataset \cite{moitra2022provably}.)

In some cases \cite{martinez2022much,eubank2022enfranchisement}, published in leading journals in economics and political science, the authors report the number of samples returned by ZAMinfluence as the minimum needed to change the outcome of their study, as evidence of robustness of their results.
That is, they treat the ZAMinfluence \emph{upper bound} on Stability as if it were a \emph{lower bound}, although ZAMinfluence comes with no such guarantee.
We invalidate such claims by finding smaller subsets, thus  highlighting the importance of lower bounds on Stability.
On datasets of small dimension (two or three), our algorithms frequently provide matching lower and upper bounds.

Overall, our results suggest that greedy methods and mixed integer quadratically constrained optimization offer a useful approach to robustness auditing for many regression problems encountered in practice.
However, there is room for improvement, especially on regression problems with more than two or three dimensions: on many such datasets drawn from econometric studies, none of our algorithms, or those in prior work, provide any nontrivial lower bounds on stability (in a reasonable amount of computation time).

In fact, it is easy to construct simple synthetic datasets with $100$ samples in four or more dimensions with Gaussian covariates for which no prior algorithm, nor any of the above, offer any nontrivial lower bound on Stability (in a reasonable amount of computation time).
Following recent theoretical developments in algorithmic robust statistics \cite{klivans2018efficient,bakshi2021robust}, our third contribution shows empirically that this is not due to inherent computational intractability.
We implement:
\begin{itemize}
    \item A spectral algorithm (i.e, based on eigenvalues and eigenvectors) which gives nontrivial lower bounds on Stability for synthetic datasets with tens of thousands of samples and four (or more) dimensions. In these settings, we did not plant any outliers and consequently the resulting regressions are expected to be stable under the removal of a sizable fraction of the samples.  Nonetheless, the mixed integer optimization approach provides no lower bound greater 0,  whereas \cite{moitra2022provably} does not run (in reasonable time) even in dimension 4.
    This shows that there is room for improvement beyond baseline approaches for robustness auditing. However, as our spectral algorithm is heavily tailored to synethic datasets, it does not improve over our baseline methods on any of the real-world data we study.
\end{itemize}

To encourage future work, we summarize the limitations of the algorithms we study in several ``challenge datasets,''.
These are datasets where our algorithms and those of prior work leave large gaps between upper and lower bounds on Stability; new algorithms which shrink or close these gaps would thus represent progress on robustness auditing for linear regression.


\subsection{Related Work}
We are aware of three prior works which attempt to compute or approximate $\text{Stability}(X,y)$.

\paragraph{ZAMinfluence and refinements} ZAMinfluence \cite{broderick2020automatic} 
finds small subsets of samples to drop based on the classical notion of influence functions.\footnote{Essentially, it removes those samples which have the greatest effect on the fitted parameter of interest when they are infinitesimally down-weighted, as measured by differentiation with respect to the weight assigned to that sample.}
It is computationally lightweight and applicable well beyond linear regression.
Its authors demonstrate that it finds small, high-influence subsets in several large-scale econometric studies.
ZAMinfluence has already seen significant adoption: since its initial release 2021, several studies in economics, finance, and political science have used it to perform robustness checks, ensuring that it \emph{doesn't} find a small subset of samples which can be dropped to change the study outcome \cite{eubank2022enfranchisement,martinez2022much,finger2022adoption,turnbull2022mobilising,falck2022systematic}.

However, ZAMinfluence only provides upper bounds on $\text{Stability}(X,y)$.
Moitra and Rohatgi show in the context of the Boston Housing dataset that these upper bounds frequently are not tight.
We show in this work that this non-tightness extends to multiple cases where ZAMinfluence has been used as a (purported) robustness check.

Kuschnig, Zens, and Crespo Cuaresma \cite{kuschnig2021hidden} experiment with several refinements of ZAMinfluence, mainly involving removing the most influential sample one at a time and then recomputing all influences.
This amounts to a greedy heuristic for approximating Stability.
They show that this heuristic finds better upper bounds on Stability than ZAMinfluence does in several examples.


\paragraph{\textsc{PartitionAndApprox} and \textsc{NetApprox}}
Moitra and Rohatgi \cite{moitra2022provably} propose two algorithms to approximate a fractional variant of $\text{Stability}(X,y)$ (meaning they search for a set of $[0,1]$-valued \emph{weights} rather than a subset $S$, and consider the resulting weighted OLS solution).
They prove strong theoretical guarantees for their algorithms -- in particular, under relatively weak assumptions on $X,y$, they can compute the fractional stability, up to error $\eps n$, in time roughly $(n/\eps)^{d + O(1)}$.

Moitra and Rohatgi implement modified variants of \textsc{PartitionAndApprox} and \textsc{NetApprox} for which their provable guarantees no longer apply, but which still give, for any given $X,y$, valid upper and lower bounds on the fractional stability.
They demonstrate that these lower bounds are nontrivial -- for a majority of two-dimensional regression problems drawn from the Boston Housing dataset \cite{harrison1978hedonic} their upper and lower bounds are within a factor of two.

\textsc{PartitionAndApprox} and \textsc{NetApprox} thus offer a potentially useful robustness audit, but from a practical standpoint there are still significant drawbacks.
First, their running times scales poorly with dimension.
(Indeed, \cite{moitra2022provably} show that this is inherent for any algorithm which provably computes Stability for any $X,y$.)
Consequently, Moitra and Rohatgi do not obtain nontrivial stability bounds on any regression problem of dimension larger than three.
Second, their upper and lower bounds are still far from tight for the majority of regression problems they draw from Boston Housing (e.g., their bounds are not within 1\% of each other on 92\% of the instances). 
By contrast, we solve $94\%$ of these problems with gaps of less than $1\%$.

\paragraph{Additional Prior Work}
Measures of the influence of individual samples on a linear regression have been extensively studied in statistics.
This literature is too broad to fully survey here; see e.g. \cite{chatterjee1986influential} and references therein for discussion of classical literature.
Determining what a fitted model would do in the absence of a subset of data has also been of recent interest in machine learning \cite{ilyas2022datamodels,yang2023many}. 


%\Snote{is this true?}\Dnote{yes.}

\subsection{Results}
We now briefly summarize the upper and lower bounds we obtain on Stability for real-world and synthetic datasets, and to what extent these improve on prior work.
We report full results in Section~\ref{sec:experiments}, where we run every algorithm that we study on every dataset, except where (a) the algorithm only works on a special case of linear regression which the dataset doesn't fit, or (b) the algorithm requires too much time to return results (see discussion in Section ~\ref{ssec:setup}) , as for \textsc{PartitionAndApprox}/\textsc{NetApprox} on datasets of dimension $3$ or larger.

\paragraph{Microcredit}
Meager \cite{meager2022aggregating} surveys seven randomized control trials involving availability of microcredit loans in developing countries.
Each involves a single regression of one binary treatment variable and a real-valued outcome, typically with thousands or tens of thousands of samples.
They are among the original datasets investigated using ZAMinfluence \cite{broderick2020automatic}.
Both our Gurobi-based approach and a simple greedy algorithm exactly solve Stability for all these studies (that is, they obtain matching lower and upper bounds).
In several cases our upper bounds improve on those obtained via ZAMinfluence,  and we provide the first lower bounds for these datasets.

\paragraph{Incarceration}
Eubank and Fresh \cite{eubank2022enfranchisement} investigate the effect of the end of Jim Crow on incarceration rates of Black people in the American South.
The resulting linear regression is 48-dimensional with 504 samples.
Using ZAMinfluence, Eubank and Fresh report that at least 19\% of their data would need to be removed to change the outcome of their study.
Our Gurobi-based method identifies a subset of $< 6\%$ of the data which has this effect.
None of our algorithms find any nontrivial lower bound on Stability for these data.


\paragraph{GDP and Democracy}
Martinez \cite{martinez2022much} investigates the effect of political freedom on national reports of economic growth.
The resulting linear regression is 211-dimensional, with 3895 samples.
\cite{martinez2022much} reports that ZAMinfluence needs to remove at least $5\%$ of the data to change the study's outcome; the authors run some heuristic tests to try and see if this $5\%$ can be reduced to $1\%$ and report that it likely cannot.
However, using Gurobi we find a subset of $\approx 3\%$ of the sample which can be removed to change the outcome of the study.
None of our algoirthms provide any nontrival lower bound on Stability for these data.

\paragraph{Boston Housing}
Boston Housing \cite{harrison1978hedonic} is a standard benchmark dataset for machine learning.\footnote{Some ethical issues surrounding the Boston Housing data have emerged in recent years \cite{fairlearn_boston_housing_data}. We report no conclusions or predictions made using these data, only Stability of some regression problems drawn from them, in order to compare our algorithms with \cite{moitra2022provably}.}
Moitra and Rohatgi test \textsc{PartitionAndApprox} and \textsc{NetApprox} on numerous regression problems with two-dimensional covariates drawn from Boston Housing.
On nearly all of these regression problems, Gurobi finds upper and lower bounds on Stability with significantly smaller gap (typically less than $1\%$).
On a few examples, Moitra and Rohatgi's algorithm obtains tighter bounds; in practice one could run both algorithms and report the tighter bound.

\paragraph{Minimum Wage}
Card and Krueger \cite{card1993minimum} study the effect of minimum wage on fast-food employment.
The resulting data and analysis have become a textbook example of the difference-in-differences method (indeed, our analysis is based on a CSV prepared by \cite{bauer2020causal}).
We design a simple greedy method which can exactly compute Stability for such difference-in-differences regressions; it scales easily to the 384 observation pairs in Card and Krueger's dataset.

\paragraph{Synthetic}
We expose an Achilles heel of all previously-discussed algorithms for robustness auditing of linear regression: none can provide nontrivial lower bounds on Stability for simple synthetic datasets with very robust linear trends.
Concretely, we generate $1000$ i.i.d. samples $X_1,\ldots,X_{1000} \in \R^4$ from $\mathcal{N}(0,I)$ and we let $Y_i = \beta^\top X_i + \eps_i$, where $\eps_i \sim \mathcal{N}(0,1)$ and $\beta = (1,1,1,1)$.
Using Gurobi we can identify a subset of ?? samples to remove, but we obtain no nontrivial lower bound.
However, our spectral algorithm supplies a lower bound showing that at least $10 \%$ of the data must be removed to change the sign of $\beta_1$.