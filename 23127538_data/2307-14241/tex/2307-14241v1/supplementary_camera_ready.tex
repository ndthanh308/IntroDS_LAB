





\documentclass[sn-mathphys]{sn-jnl}%


\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{array}
\usepackage{tabularx}
\usepackage{comment}
\usepackage{float}

\newcommand{\lennart}[1]{\textcolor{orange}{\emph{Lennart:~{#1}}}}
\newcommand{\toby}[1]{\textbf{\textcolor{blue}{Toby: #1}}}
\newcommand{\tony}[1]{\textbf{\textcolor{green}{Tony: #1}}}


\jyear{2022}%

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%
\newtheorem{proposition}[theorem]{Proposition}%

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom


\begin{document}

\author{\fnm{Lennart} \sur{Bastian$^*$}}
\author{\fnm{Tony Danjun} \sur{Wang$^*$}}
\author{\fnm{Tobias} \sur{Czempiel}}
\author{\fnm{Benjamin} \sur{Busam}}
\author{\fnm{Nassir} \sur{Navab}}

\affil{\orgdiv{Computer Aided Medical Procedures}, \orgname{Technical University Munich}, \orgaddress{\city{Munich},  \country{Germany}}}
\affil{\small{\\$^*$ Equal Contribution. \qquad Contact: \{first\}.\{last\}@tum.de}}

\title[DisguisOR]{DisguisOR -- Supplementary Materials}

\maketitle

\section{Methods}

\subsection{Obstruction Detection}
In~\autoref{fig:reprojection}, we detail how we handle obstructions that occur in the back-projection of the 3D face mesh. If an object is positioned between the face and a camera then the calculated distance between the camera and face in 3D will be larger than the distance inferred from the depth.
Specifically, we obtain the 2D coordinates of 20 fixed vertices of the 3D face mesh for each camera view. 
Using the depth map, we calculate the corresponding 3D coordinates using the depth values of each 2D coordinate. 
If the distance between a 3D coordinate inferred from the depth map and the corresponding actual 3D coordinate of a vertex is similar, we conclude the face is visible for the camera in question.

% Figure environment removed

% Figure environment removed

\subsection{Dataset Curation}
In~\autoref{fig:annotation_examples}, we provide a set of examples of the ground truth annotations in our dataset. It denotes the annotation bounding boxes of obstructed faces and cases where no face information is visible.

\subsection{3D Key-Point Smoothing}
To generate less noisy and more robust keypoints we perform a 3D keypoint smoothing and tracking over an entire sequence before fitting the SMPL mesh onto the 3D keypoints. 
The smoothing process helps to diminish outliers caused by incorrect triangulations during the 3D human pose estimation stage and to interpolate missed human poses.
This allows for a more reliable human mesh regression onto the 3D human poses.
Furthermore, the tracking enables anonymizing each person with the same face texture in every frame. 

\subsection{Rendering: Poisson Image Editing}
We further highlight the effect of different blending parameters of the poisson image editing image harmonization~(\autoref{fig:alpha_scores}).
As an additional robustness towards privacy preservation, one may also opt to blur in the source image prior to blending~(\autoref{fig:poisson_blurring}) or avoid blending altogether ~(\autoref{fig:alpha_scores} Alpha: 1.0).
% Figure environment removed

% Figure environment removed


\section{Results and Discussion}

\subsection{Face Localization}
In Tables~\ref{tbl:easy_eval_dataset_results},~\ref{tbl:medium_eval_dataset_results} and~\ref{tbl:hard_eval_dataset_results}, we present the precision (P), recall (R), F1-score (F1), and number of annotated faces of each individual camera and scenario.
The precision of DisguisOR is lower for some scenarios than that of DSFD, notably in the surgical cameras (SC1 and SC2). 
This is mainly due to the hardware constraints of the Kinect camera system, which does not generate a corresponding depth value for each pixel.
Cropping to the depth field of view and downsizing the color image to the resolution of the depth camera would mitigate these issues and likely favor DisguisOR -- however, we perform inference for all methods at native resolution of 2048x1536.
While the self-supervised domain adapation (SSDA)~\cite{issenhuth_face_detection_2018} with DSFD increased the recall significantly, especially in the surgical cameras, it also decreased the precision, resulting in a lower F1-score.

\begin{table}[h]
\centering
    \caption[]{Comparison of precision (P), recall (R) and F1-score (F1) at IOU@0.4 of DSFD~\cite{dsfd}, the Self-Supervised Domain Adaption (SSDA) method of~\cite{issenhuth_face_detection_2018}, and DisguisOR on the \textbf{Easy Scenario} across all cameras. The last column depicts the number of ground truth faces in each camera view
        \label{tbl:easy_eval_dataset_results}}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lccc|ccc|ccc|c}
            \hline
            \  & \multicolumn{3}{c|}{DSFD~\cite{dsfd}} & \multicolumn{3}{c|}{SSDA~\cite{issenhuth_face_detection_2018}} & \multicolumn{3}{c|}{DisguisOR} & \multicolumn{1}{c}{No. Faces}\\
                 & P & R & F1 & P & R & F1 & P & R & F1 &  \\ \hline
            SC1  & 90.5 & 91.9 & 91.2 & 44.8 & 98.4 & 61.6 & 83.5 & 91.1 & 87.1 & 384 \\ \hline
            SC2  & 87.2 & 87.8 & 87.5 & 71.6 & 91.1 & 80.2 & 75.7 & 86.5 & 80.7 & 327 \\ \hline
            WFC1 & 69.3 & 86.8 & 77.1 & 51.0 & 91.1 & 65.4 & 61.8 & 84.2 & 71.3 & 190 \\ \hline
            WFC2 & 97.8 & 98.3 & 98.1 & 62.2 & 97.6 & 76.0 & 87.3 & 92.2 & 89.7 & 409 \\ \hline
            Avg. & 86.2 & 91.2 & 88.5 & 57.4 & 94.6 & 70.8 & 77.1 & 88.5 & 82.2 & 327 \\ \hline
        \end{tabular*}
\end{table}

\begin{table}[h]
\centering
    \caption[]{Comparison of precision (P), recall (R) and F1-score (F1) at IOU@0.4 of DSFD~\cite{dsfd}, the Self-Supervised Domain Adaption (SSDA) method of~\cite{issenhuth_face_detection_2018}, and DisguisOR on the \textbf{Medium Scenario} across all cameras. The last column depicts the number of ground truth faces in each camera view
        \label{tbl:medium_eval_dataset_results}}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lccc|ccc|ccc|c}
            \hline
            \  & \multicolumn{3}{c|}{DSFD~\cite{dsfd}} & \multicolumn{3}{c|}{SSDA~\cite{issenhuth_face_detection_2018}} & \multicolumn{3}{c|}{DisguisOR} & \multicolumn{1}{c}{No. Faces} \\
                 & P & R & F1 & P & R & F1 & P & R & F1 &  \\ \hline
            SC1  & 97.5 & 84.2 & 90.4 & 71.9 & 93.4 & 81.3 & 87.4 & 97.3 & 92.1 & 558  \\ \hline
            SC2  & 63.4 & 37.9 & 47.4 & 29.6 & 53.5 & 38.1 & 35.1 & 78.5 & 48.5 & 256  \\ \hline
            WFC1 & 87.2 & 96.2 & 91.5 & 67.1 & 88.3 & 76.3 & 71.7 & 79.8 & 75.5 & 426  \\ \hline
            WFC2 & 94.0 & 96.0 & 95.0 & 94.0 & 95.4 & 94.7 & 93.3 & 92.3 & 92.8 & 1077 \\ \hline
            Avg. & 85.5 & 78.6 & 81.1 & 65.7 & 82.7 & 72.6 & 71.9 & 87.0 & 77.2 & 579  \\ \hline
        \end{tabular*}
\end{table}

\begin{table}[h]
\centering
    \caption[]{Comparison of precision (P), recall (R) and F1-score (F1) at IOU@0.4 of DSFD~\cite{dsfd}, the Self-Supervised Domain Adaption (SSDA) method of~\cite{issenhuth_face_detection_2018}, and DisguisOR on the \textbf{Hard Scenario} across all cameras. The last column depicts the number of ground truth faces in each camera view
        \label{tbl:hard_eval_dataset_results}}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lccc|ccc|ccc|c}
            \hline
            \  & \multicolumn{3}{c|}{DSFD~\cite{dsfd}} & \multicolumn{3}{c|}{SSDA~\cite{issenhuth_face_detection_2018}} & \multicolumn{3}{c|}{DisguisOR} & \multicolumn{1}{c}{No. Faces} \\
                 & P & R & F1 & P & R & F1 & P & R & F1 &  \\ \hline
            SC1  & 88.2 & 16.9 & 28.4 & 10.3 & 52.8 & 17.2 & 19.6 & 97.8 & 32.7 & 89  \\ \hline
            SC2  & 88.3 & 55.2 & 67.9 & 89.6 & 74.6 & 81.4 & 62.3 & 74.1 & 67.7 & 232 \\ \hline
            WFC1 & 76.2 & 87.1 & 81.3 & 73.5 & 56.4 & 63.8 & 51.1 & 79.7 & 62.3 & 202 \\ \hline
            WFC2 & 99.2 & 95.3 & 97.2 & 73.2 & 88.5 & 80.1 & 93.8 & 90.8 & 92.3 & 763 \\ \hline
            Avg. & 88.0 & 63.6 & 68.7 & 61.7 & 68.1 & 60.7 & 56.7 & 85.6 & 63.7 & 321 \\ \hline
        \end{tabular*}
\end{table}

\subsubsection{Different Confidence Thresholds}

\autoref{tbl:conf_thresh_dataset_results} depicts the precision, recall and F1-scores of DSFD~\cite{dsfd} using different confidence thresholds. 
The default confidence threshold is 0.5. 
We average each metric across all four cameras, and present the results for each scenario.
Lowering the confidence scores increases the recall, but at a significant cost of precision. 
Even at lower confidence thresholds, DSFD does not attain the recall of DisguisOR.
Furthermore, errant detections cover large swaths of the images (\autoref{fig:conf_threshold}).
Subsequent anonymization s.pdf could impact the image integrity and affect downstream tasks.

\begin{table}[h]
    \centering
        \caption[]{Comparison of precision (P), recall (R) and F1-score (F1) at IOU@0.4 of DSFD~\cite{dsfd} with different confidence thresholds and DisguisOR on all scenarios. P, R and F1 are averaged across all cameras}
        \label{tbl:conf_thresh_dataset_results}
        \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lccc|ccc|ccc} \hline
            \  & \multicolumn{3}{c|}{DSFD~\cite{dsfd} @ 0.1} & \multicolumn{3}{c|}{DSFD~\cite{dsfd} @ 0.025} & \multicolumn{3}{c}{DisguisOR} \\
                  & P & R & F1 & P & R & F1 & P & R & F1  \\ \hline
            Easy    & 73.3 & 94.8 & 81.7 & 12.5 & 96.5 & 20.6 & 77.1 & 88.5 & 82.2 \\ \hline
            Medium  & 71.2 & 84.0 & 76.4 & 26.9 & 88.4 & 38.3 & 71.9 & 87.0 & 77.2 \\ \hline
            Hard    & 64.1 & 69.6 & 64.8 & 23.6 & 73.6 & 28.9 & 56.7 & 85.6 & 63.7 \\ \hline
        \end{tabular*}
\end{table}

% Figure environment removed

% Figure environment removed

\pagebreak
\subsection{Experiments on Downstream Tasks}

\noindent\textbf{Face Detection.}
To evaluate the preservation of image integrity, we compare our anonymization method with the three conventional anonymization methods (blurring, pixelization and blackening) and DeepPrivacy~\cite{deepprivacy} on downstream face detection.
This experiment is designed to measure the degree to which an anonymization method creates unnatural alterations, which would lead to errant predictions from an existing method.
We compare the performance of the pre-trained DSFD~\cite{dsfd} model on images from all three scenarios anonymized through several methods.
For a fair comparison between the two methods, we only consider face detections made by both DeepPrivacy and DisguisOR.
We report the percentage of average precision (AP) at an intersection over union of greater than 0.4 (IOU@0.4) retained with respect to the original unaltered images.

\noindent\textbf{Human Pose Estimation.}
In order to further demonstrate the image quality preservation of our method, we evaluate the widely used AlphaPose~\cite{alphapose} human pose estimator, pre-trained on COCO~\cite{coco_dataset}, to understand the effect of unnatural image artifacts on human pose estimation.
We generate pseudo-ground truth by projecting the 3D joint positions of each generated SMPL mesh into each camera view. 
In accordance with previously established works~\cite{issenhuth_face_detection_2018}, we use the percentage of correct keypoints (PCK) metric to measure how many keypoints were accurately predicted within a threshold.
DeepPrivacy is omitted from this experiment to avoid an unfair bias in favor of DisguisOR due to how pseudo-ground truth is generated.

\section{Results.}

\noindent\textbf{Face Detection.}
\autoref{tbl:effect_anonymization_face_detection} depicts the downstream face detection performance of the face detector DSFD~\cite{dsfd} on images anonymized through various obfuscation methods. 
We report the percentage AP retained compared to the AP achieved on the original unaltered images. 
The results show that blackening the face area removes a considerable amount of information, making it prohibitively difficult for a method to localize a face. Blurring or pixelization is less detrimental to the detection methods' performance.
Nevertheless, the detection results of both methods are severely impacted by the blurring of face regions. 
This becomes even more evident in difficult scenarios. 
For all three conventional anonymization approaches, the performance drop is severe, and the AP for the face detection task is too low for most use cases.
In contrast, our proposed method generates faces that the detection methods can identify with high accuracy.

In the easy scenario, we observe a decrease of merely 2\% in DSFD's face detection AP, highlighting the preservation qualities of our method.
DeepPrivacy is able to retain more AP in the medium and hard scenarios, most likely since it generates synthetic faces with distinct facial features.
Using a maskless face texture further increases the retained AP by enabling face detectors to rely on more facial elements, such as the nose and mouth.
However, this comes at a cost of image quality, as apparent by the experiments in the main paper.
These results indicate that the realistic faces generated by DisguisOR could mitigate costly annotation and retraining due to an inferior anonymization method.
Furthermore, they also indicate that a given template and source image harmonization may be more suitable for a certain application.

\begin{table}[]
    \centering
    \caption[Face Detection on Anonymized Images]{Percentage of face detection AP of DSFD~\cite{dsfd} at IOU@0.4 for differently anonymized images with the AP on original images as baseline. Masked Texture denotes that blended templates contain medical masks (see~\autoref{fig:alpha_scores}), whereas Maskless Texture denotes that blended templates do not contain medical masks
    \label{tbl:effect_anonymization_face_detection}}
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}l|c|c|c}
        \toprule
        Anonymization Method & Easy Scenario & Medium Scenario & Hard Scenario \\
        \midrule
        Blackening & 12.2 & 10.9 & 9.1 \\
        Gaussian Blur & 18.4 & 19.3 & 7.8 \\
        Pixelization & 72.4 & 60.6 & 63.1\\
        DeepPrivacy~\cite{deepprivacy} & 99.1 & \textbf{98.9} & \textbf{94.2} \\
        DisguisOR (Masked Texture) & 98.2 & 85.3 & 74.7 \\
        DisguisOR (Maskless Texture) & \textbf{100.0} & 92.8 & 82.8  \\
        \bottomrule
    \end{tabular*}
\end{table}

\noindent\textbf{Human Pose Estimation.} In~\autoref{tbl:effect_anonymization_pose_estimation} we report the PCK of the human pose estimator AlphaPose~\cite{alphapose} on differently anonymized frames.
Human pose estimators are less susceptible to limited modifications of the face area. Therefore, the deviation of the PCKs is less severe.
Blurring, pixelization, and blackening of the faces can confuse the methods resulting in a decreased performance. 
It is interesting to see that the anonymization limited to the region of the head still has a measurable negative impact on the joint detection of the hip. 
This again highlights the importance of retaining the information and the need to anonymize without severe information loss. 
The PCK on images anonymized by our method is the highest, demonstrating the realism of our method.

\begin{table}[!hbt]
    \centering
    \caption[Human Pose Estimation on Anonymized Images]{PCKh@0.5 results for AlphaPose~\cite{alphapose} on differently anonymized images. Images are taken from all scenarios across all cameras
        \label{tbl:effect_anonymization_pose_estimation}}
        \resizebox{\textwidth}{!}{
    \begin{tabular}{@{\extracolsep{\fill}}l|ccccccc|c}
        \toprule
        Anonymization Method & Head & Shoulder & Elbow & Wrist & Hip & Knee & Ankle & Avg. \\ \midrule
        Blackening & 58.8 & 76.1 & 71.6 & 63.7 & 66.7 & 31.4 & 25.7 & 56.4 \\
        Pixelization & 71.1 & 79.0 & 74.5 & 65.9 & 72.2 & 32.5 & 26.1 & 61.4 \\
        Gaussian Blur & 72.6 & 81.5 & 76.8 & 67.3 & 74.2 & \textbf{34.1} & 26.3 & 63.0 \\
        DisguisOR & \textbf{78.2} & \textbf{82.0} & \textbf{77.4} & \textbf{67.4} & \textbf{76.2} & 33.9 & \textbf{27.0} & \textbf{65.0} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\section{Appendix}
For reproducibility, we provide the GitHub repository link and python version for each method that we have used in this paper.
All computations were performed on a computer with 64GB of RAM and an NVIDIA GeForce RTX 2080 Ti.
DeepPrivacy needed approximately 2.44s for anonymizing each frame (i.e., 4 images),while DisguisOR needed approximately 6.47s for each frame (see~\autoref{tbl:disguisor_runtime}). The majority of this time is spent on point cloud alignment and rendering, which could be made more efficient. More specifically, it needed 0.52s for 2D human pose estimation, 0.23 for 3D human pose estimation, 0.62s for human mesh estimation and 3.74s for registration and 1.36s rendering.
The memory footprint of DeepPrivacy reached around 4.6GB, while DisguisOR used approximately 6.5GB.

\begin{table}[h!]
    \centering
    \caption{The runtime for each frame (i.e., 4 images) of DisguisOR split into respective stages
    \label{tbl:disguisor_runtime}}
    \begin{tabular}{l|c}
        \toprule
        Stage & Runtime (seconds) \\
        \midrule
        2D Human Pose Estimation & 0.52 \\  
        3D Human Pose Esimtation & 0.23  \\
        Human Mesh Estimation & 0.62 \\
        Point Cloud Registration & 3.74 \\
        Rendering & 1.36 \\
        \midrule
        Total & 6.47 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{itemize}
    \item DisguisOR  \url{{https://github.com/wngTn/disguisor}} (Python 3.7.2)
    \begin{itemize}
    \item DEKR~\cite{dekr} \url{https://github.com/HRNet/DEKR} (Python 3.7.2)
    \item VoxelPose~\cite{voxelpose} \url{https://github.com/microsoft/voxelpose-pytorch} (Python 3.10.2) 
    \item EasyMocap~\cite{easymocap} \url{https://github.com/zju3dv/EasyMocap} (Python 3.6.2)
    \item SMPL~\cite{smpl} \url{https://smpl.is.tue.mpg.de}
    \end{itemize}
    \item Evaluation code versions
    \begin{itemize}
    \item DSFD~\cite{dsfd} \url{https://github.com/hukkelas/DSFD-Pytorch-Inference} (Python 3.8.13)
    \item DeepPrivacy~\cite{deepprivacy} \url{https://github.com/hukkelas/DeepPrivacy} (Python 3.8.2)
    \item AlphaPose~\cite{alphapose} \url{https://github.com/MVIG-SJTU/AlphaPose} (Python 3.7.2)
    \end{itemize}
\end{itemize}

\bibliography{bibliography} %


\end{document}
