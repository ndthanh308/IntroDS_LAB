{
  "title": "zkDL: Efficient Zero-Knowledge Proofs of Deep Learning Training",
  "authors": [
    "Haochen Sun",
    "Tonghe Bai",
    "Jason Li",
    "Hongyang Zhang"
  ],
  "submission_date": "2023-07-30T16:41:13+00:00",
  "revised_dates": [
    "2023-12-05T19:42:53+00:00"
  ],
  "abstract": "The recent advancements in deep learning have brought about significant changes in various aspects of people's lives. Meanwhile, these rapid developments have raised concerns about the legitimacy of the training process of deep neural networks. To protect the intellectual properties of AI developers, directly examining the training process by accessing the model parameters and training data is often prohibited for verifiers.\n  In response to this challenge, we present zero-knowledge deep learning (zkDL), an efficient zero-knowledge proof for deep learning training. To address the long-standing challenge of verifiable computations of non-linearities in deep learning training, we introduce zkReLU, a specialized proof for the ReLU activation and its backpropagation. zkReLU turns the disadvantage of non-arithmetic relations into an advantage, leading to the creation of FAC4DNN, our specialized arithmetic circuit design for modelling neural networks. This design aggregates the proofs over different layers and training steps, without being constrained by their sequential order in the training process.\n  With our new CUDA implementation that achieves full compatibility with the tensor structures and the aggregated proof design, zkDL enables the generation of complete and sound proofs in less than a second per batch update for an 8-layer neural network with 10M parameters and a batch size of 64, while provably ensuring the privacy of data and model parameters. To our best knowledge, we are not aware of any existing work on zero-knowledge proof of deep learning training that is scalable to million-size networks.",
  "categories": [
    "cs.LG",
    "cs.CR"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.16273",
  "pdf_url": "https://arxiv.org/pdf/2307.16273v2",
  "comment": "16 pages",
  "num_versions": null,
  "size_before_bytes": 5123849,
  "size_after_bytes": 1788711
}