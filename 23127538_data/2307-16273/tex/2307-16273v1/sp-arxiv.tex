%% bare_conf_compsoc.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference,compsoc]{IEEEtran}
% Some/most Computer Society conferences require the compsoc mode option,
% but others may want the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference,compsoc]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath

% to be able to draw some self-contained figs
\usepackage{tikz}
\usepackage{amsmath,amsfonts,amssymb,amsthm}

%% Additional packages
\usepackage{algorithm,algpseudocode}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{fontawesome5,pifont} % load Twemoji font
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{environ}
\usepackage{resizegather}
\usepackage{mdframed}
\usepackage{pgf-pie}
\usepackage{wrapfig}

\usepackage{url}
\def\UrlBreaks{\do\/\do-}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}

%\usepackage{url}
%\def\UrlBreaks{\do-\do\_}

%% Self-defined environments
\newtheorem{Thm}{Theorem}[section]
\newtheorem{Lem}[Thm]{Lemma}
\newtheorem{Prop}[Thm]{Proposition}
\newtheorem{Cor}[Thm]{Corollary}
\newtheorem{Rem}[Thm]{Remark}
\newtheorem{Def}[Thm]{Definition}
\newtheorem{Exm}[Thm]{Example}

\newcounter{protocol}
\makeatletter
\newenvironment{protocol}[1][htb]{%
    \let\c@algorithm\c@protocol
    \renewcommand{\ALG@name}{Protocol}% Update algorithm name
    \begin{algorithm}[#1]%
    }{\end{algorithm}
}
\makeatother

%% Self-defined shortcuts
\newcommand{\ceil}[1]{\left\lceil#1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor#1 \right\rfloor}
\newcommand{\round}[1]{\left\lfloor#1 \right\rceil}
\newcommand{\abs}[1]{\left|#1 \right|}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\innerproduct}[2]{\left\langle#1, #2\right\rangle}
\renewcommand{\vec}[1]{\overrightarrow{#1}}
\newcommand{\mat}[1]{\begin{pmatrix}#1 \\ \end{pmatrix}}
\newcommand{\me}[2]{\widetilde{#1}\left(#2\right)}

\renewcommand{\bf}[1]{\ifmmode\mathbf{#1}\else\textbf{#1}\fi}
\newcommand{\bs}[1]{\boldsymbol{#1}}

\renewcommand{\tt}[1]{\ensuremath{\texttt{#1}}}
\newcommand{\com}[1]{\ensuremath{\tt{com}_{#1}}}
\newcommand{\poly}[1]{\ensuremath{\tt{poly}\left({#1}\right)}}
\newcommand{\negl}[1]{\ensuremath{\tt{negl}\left({#1}\right)}}
\newcommand{\qeq}{\stackrel{?}{=}}

\def\e{\epsilon}
\def\g{\gamma}
\def\D{\mathcal{D}}
\def\N{\mathcal{N}}
\def\F{\mathbb{F}}
\def\G{\mathbb{G}}

\def\1{\mathbbm{1}}

\newcommand{\case}[2][lllllllllllllllllllllllllllllllllllll]{\left\{\begin{array}{#1}#2 \\ \end{array}\right.}
\newcommand{\Eq}[1]{\begin{align}#1\end{align}} %Equations with numberings%
\newcommand{\Eqn}[1]{\begin{align*}#1\end{align*}}  % Equations without numberings%

% \NewEnviron{resizedalign}{%
%   \par\noindent\adjustbox{max width=\linewidth}{%
%     \begin{minipage}{\linewidth}
%     \begin{align}
%       \BODY
%     \end{align}
%     \end{minipage}%
%   }\par%
% }


%% comments
\newcommand{\hy}[1]{\textcolor{red}{(hy: #1)}}
\newcommand{\hc}[1]{\textcolor{blue}{(hc: #1)}}


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed

Therefore, with randomly and independently chosen $u_\text{relu}''\sim \F$ and $\mathbf{u}_\text{stack} \sim \F^{\ceil{\log_2 (L-1)}}$, the trainer can batch up the proofs for $\mathbf{A}^{(\ell)}$ and $\mathbf{G}_\mathbf{Z}^{(\ell)}$, which connects the proofs of the arithmetic components and the validity check of auxiliary inputs seamlessly, by running the sumcheck protocol on \eqref{eq:connect-sumcheck}: \begin{align}
    \label{eq:connect-sumcheck}
    ~&(1-u_\text{relu}'')\sum_{\ell = 1}^{L-1}\me{\beta}{\mathbf{u}_\text{stack}, \ell - 1}\me{\mathbf{A}^{(\ell)}}{\mathbf{u}_{\mathbf{A}}} + \nonumber\\ 
    ~&+u_\text{relu}''\sum_{\ell = 1}^{L-1}\me{\beta}{\mathbf{u}_\text{stack}, \ell - 1}\me{\mathbf{G}_\mathbf{Z}^{(\ell)}}{\mathbf{u}_{\mathbf{G}_\mathbf{Z}}}\nonumber\\
    =&(1-u_\text{relu}'')\me{\mathbf{A}}{\mathbf{u}_\text{stack}, \mathbf{u}_{\mathbf{A}}} + u_\text{relu}''\me{\mathbf{G_Z}}{\mathbf{u}_\text{stack}, \mathbf{u_{G_Z}}}\nonumber\\
    =&\left(1-\sum_{\mathbf{b}\in \{0,1\}^{\log_2 D}}\me{\mathbf{B}_{Q-1}}{\mathbf{u}_\text{stack}, \mathbf{b}}\right)\nonumber\\
    ~&\left((1-u_\text{relu}'')\me{\mathbf{A}}{\mathbf{u}_\text{stack}, \mathbf{b}}\me{\beta}{\mathbf{u_A}, \mathbf{b}}\right.\nonumber\\
    ~&\left.+u_\text{relu}''\me{\mathbf{G_Z}}{\mathbf{u}_\text{stack}, \mathbf{b}}\me{\beta}{\mathbf{u_{G_Z}}, \mathbf{b}}\right),
\end{align} where the indices $\ell-1$ are in the form of binary representation.

The sumcheck protocol on \eqref{eq:connect-sumcheck} outputs the claims on $\me{\mathbf{B}_{Q-1}}{\mathbf{u}_\text{stack}, \mathbf{u}_\text{new}}, \me{\mathbf{A}}{\mathbf{u}_\text{stack}, \mathbf{u}_\text{new}}, \me{\mathbf{G_Z}}{\mathbf{u}_\text{stack}, \mathbf{u}_\text{new}}$, which is the evaluations at the same random point, and can be verified with respect to the Pedersen commitments of these tensors. By concatenating $u_\text{relu}''$, $\mathbf{u}_\text{stack}$, and $\mathbf{u}_\text{new}$, we get the $\mathbf{u}_\text{relu}$ that is needed for the validity check of the auxiliary inputs as described in Section \ref{sec:validity}. The full zkDL protocol between the trainer $\mathcal{T}$ and trusted verifier $\mathcal{V}$, on the committed data $\mathbf{X}, \mathbf{y}$, model parameters $\mathbf{W}$, gradients $\mathbf{G}_\mathbf{W}$ and auxiliary inputs $\mathbf{aux}$ (denoting the commitments collectively as $\com{\,} \gets \tt{Commit}\left(\mathbf{X}\|\mathbf{y}\|\mathbf{W}\|\mathbf{G}_\mathbf{W}\|\mathbf{aux}\right)$ for simplicity), is summarized as Protocol \ref{protocol:zkdl}.

\begin{protocol} 
    \caption{zkDL}
    \label{protocol:zkdl}
    \begin{algorithmic}[1]
    \Require $\mathcal{T}$ knows $\mathbf{X}, \mathbf{y}, \mathbf{W}, \mathbf{G}_\mathbf{W}, \mathbf{aux}$, $\mathcal{V}$ knows $\com{\,}$
    \State $\mathcal{T}$ and $\mathcal{V}$ run Protocol \ref{protocol:zkrelu-setup} to set-up zkReLU
     \For{Layers $\ell\gets 1, 2, \dots, L$}
        \State $\mathcal{T}$ and $\mathcal{V}$ run the GKR protocol on layer $\ell$ \label{alg-line:layers-gkr} 
     \EndFor
     \State \label{alg-line:connect-sumcheck}Run the sumcheck protocol on \eqref{eq:connect-sumcheck}
     \State $\mathcal{T}$ and $\mathcal{V}$ each run Algorithm \ref{algorithm:transform-commitment}
     \State $\mathcal{T}$ and $\mathcal{V}$ run the inner-product proof on \eqref{eq:main-inner-prod} \label{alg-line:aivp}
\end{algorithmic}
\end{protocol}

Protocol \ref{protocol:zkdl} achieves perfect completeness and near-certain soundness, ensuring that the proof's acceptance by the trusted verifier closely aligns with the prover's adherence to the prescribed training logic. These two properties are formally stated in Theorems \ref{thm:completeness} and \ref{thm:soundness}, respectively.

\begin{Thm}[Completeness]\label{thm:completeness}
    Consider a neural network with ReLU activation function, such that ReLU and its backpropagation are the only non-arithmetic operation. Assume that the unscaled $\mathbf{Z}^{(\ell)}$ and $\mathbf{G}_\mathbf{A}^{(\ell)}$ are $(Q+R)$-bit integers for each layer $\ell$. If the trainer satisfies the following conditions:

    \begin{itemize}
        \item Correctly decomposes $\mathbf{Z}^{(\ell)}$ and $\mathbf{G}_\mathbf{A}^{(\ell)}$ as specified in \eqref{eq:zkrelu-Z} and \eqref{eq:zkrelu-GA}, respectively.
        \item Computes $\mathbf{A}^{(\ell)}$ and $\mathbf{G}_\mathbf{Z}^{(\ell)}$ as specified in \eqref{eq:zkrelu-A} and \eqref{eq:zkrelu-GZ}, respectively.
        \item Performs all arithmetic operations correctly.
        \item Fully adheres to Protocol \ref{protocol:zkdl}.
    \end{itemize}

    Then, the proof generated by the trainer will be accepted by the trusted verifier with probability 1, achieving perfect completeness.
\end{Thm}

The correctness of Theorem \ref{thm:completeness} is primarily attributed to the design of zkDL and the underlying zkReLU, as outlined in Appendix \ref{appendix:zkrelu}. However, from a practical perspective, the definition of soundness should not solely focus on the arithmetic relations and validity of the auxiliary inputs. Instead, it should capture the implied correctness of the ReLU's forward and backward propagations, which constitute the ultimate objective of zkDL.

\begin{Thm}[Soundness] \label{thm:soundness}
    Assume the same neural network architecture as in Theorem \ref{thm:completeness} and that $2^{Q+R} \ll |\F|$. If $\lambda$ is the security parameter of the Pedersen commitment scheme, and the size of the model and data are both polynomially bounded by $\lambda$, then with probability $1-\negl{\lambda}$, the following holds: If a proof in Protocol \ref{protocol:zkdl} is accepted by the trusted verifier, it implies that:

    \begin{itemize}
        \item All arithmetic relations involved in the forward and backward propagations hold.
        \item The forward and backward propagation of the ReLU activation for each layer $\ell$ is correctly computed as:
        \begin{align}
            \label{eq:relu-forward-soundness}\mathbf{A}^{(\ell)} &= \mathbb{I}\left\{\left\lfloor\frac{\mathbf{Z}^{(\ell)}}{\gamma}\right\rceil \geq 0\right\}\odot \left\lfloor\frac{\mathbf{Z}^{(\ell)}}{\gamma}\right\rceil, \\
            \label{eq:relu-backward-soundness}\mathbf{G}_\mathbf{Z}^{(\ell)} &= \mathbb{I}\left\{\left\lfloor\frac{\mathbf{Z}^{(\ell)}}{\gamma}\right\rceil \geq 0\right\}\odot \left\lfloor\frac{\mathbf{G_A}^{(\ell)}}{\gamma}\right\rceil.
        \end{align}
    \end{itemize}
\end{Thm}

\begin{proof}[Proof of Theorem \ref{thm:soundness}]
    The execution of the GKR protocol within each layer in Line \ref{alg-line:layers-gkr} and the sumcheck protocol between the layers and the auxiliary inputs guarantees that the soundness error of all arithmetic relations is bounded by $\negl{\lambda}$.

    Therefore, the remaining goal is to demonstrate that a valid proof implies the correct execution of the forward and backward propagations with a probability of $1-\negl{\lambda}$. This ensures that Equations \eqref{eq:relu-forward-soundness} and \eqref{eq:relu-backward-soundness} hold for each layer $\ell$. For simplicity, we will omit the layer index $\ell$ in the following analysis.

    Consider the decomposition of $\mathbf{Z}$ based on Equation \eqref{eq:zkrelu-Z}. According to Theorem \ref{thm:main-inner-prod}, with a probability of $1-\negl{\lambda}$, the acceptance of the auxiliary input validity proof implies that $\mathbf{Z}''\in [0, 2^{Q-1})^D$, $\mathbf{B}_{Q-1} \in \{0, 1\}^D$, and $\mathbf{R_Z}\in [-2^{R-1}, 2^{R-1})^D$. Now, suppose there exists another triplet $({\mathbf{Z}''}^\star, \mathbf{B}_{Q-1}^\star, \mathbf{R}_\mathbf{Z}^\star)$ with the same range as $(\mathbf{Z}'', \mathbf{B}_{Q-1}, \mathbf{R_Z})$, and it is also a valid decomposition of $\mathbf{Z}$ based on Equation \eqref{eq:zkrelu-Z}. Then, we have:
    \[
    2^R({\mathbf{Z}''}^\star - \mathbf{Z}'') + (\mathbf{R}_\mathbf{Z}^\star - \mathbf{R}_\mathbf{Z}) = 2^{Q+R-1}(\mathbf{B}_{Q-1}^\star - \mathbf{B}_{Q-1}).
    \]
    
    By considering the range requirements of the auxiliary inputs, we observe that the left-hand side of the equation lies within the range $\left[-2^{Q+R-1}+1, 2^{Q+R-1}-1\right]$, while the right-hand side takes values in $\{-2^{Q+R-1}, 0, 2^{Q+R-1}\}$. Given the assumption that $2^{Q+R} \ll |\F|$, the intersection of these two ranges is $0$, which implies that $\mathbf{B}_{Q-1}^\star = \mathbf{B}_{Q-1}$. Furthermore, since $2^R({\mathbf{Z}''}^\star - \mathbf{Z}'')$ is a multiple of $2^R$ and $\mathbf{R}_\mathbf{Z}^\star - \mathbf{R}_\mathbf{Z}$ is bounded within $[-2^R + 1, 2^R - 1]$, both values must be $0$ in order to satisfy the summation equation. Therefore, the decomposition of $\mathbf{Z}$ as shown in Equation \eqref{eq:zkrelu-Z} is unique. Similarly, the decomposition of $\mathbf{G_A}$ as shown in Equation \eqref{eq:zkrelu-GA} is also unique.

    It is worth noting that by setting $\mathbf{Z}' \gets \round{\frac{\mathbf{Z}}{\gamma}}$, $\mathbf{R_Z}\gets \mathbf{Z} - 2^R\mathbf{Z}'$, $\mathbf{Z}'' \gets \mathbf{Z}' - 2^R\mathbb{I}\left\{\mathbf{Z}' < 0\right\}$, and $\mathbf{B}_{Q-1} = \mathbb{I}\left\{\mathbf{Z}' < 0\right\}$, we satisfy the validity of the decomposition given by Equation \eqref{eq:zkrelu-Z}, as well as the range requirements of $\mathbf{Z}''$, $\mathbf{B}_{Q-1}$, and $\mathbf{R_Z}$. Similarly, by setting $\mathbf{G}_\mathbf{A}'\gets \round{\frac{\mathbf{G}_\mathbf{A}}{\gamma}}$ and $\mathbf{R_{G_A}} \gets \mathbf{G_A} - 2^R\mathbf{G}_\mathbf{A}'$, we satisfy the validity of the decomposition given by Equation \eqref{eq:zkrelu-GA}, as well as the range requirements of $\mathbf{G}_\mathbf{A}'$ and $\mathbf{R_{G_A}}$. Therefore, with a probability of $1-\negl{\lambda}$, these form the unique decomposition that is necessary for generating a valid proof.

    Also, note that with a probability of $1-\negl{\lambda}$, the arithmetic relations given by Equations \eqref{eq:zkrelu-A} and \eqref{eq:zkrelu-GZ} hold. Therefore, combining these with the unique values of the decomposition, we have:
    \begin{align*}
    \mathbf{A} &= \left(\mathbf{Z}' - 2^R\mathbb{I}\left\{\mathbf{Z}' < 0\right\}\right)\odot\mathbb{I}\left\{\mathbf{Z}' \geq 0\right\} = \mathbf{Z}'\odot\mathbb{I}\left\{\mathbf{Z}' \geq 0\right\},\\
    \mathbf{G}_\mathbf{Z} &= \mathbf{G}_\mathbf{A}'\odot \mathbb{I}\left\{\mathbf{Z}' \geq 0\right\},
    \end{align*}
    which are equivalent to Equations \eqref{eq:relu-forward-soundness} and \eqref{eq:relu-backward-soundness}. This completes the proof of soundness with a probability of $1-\negl{\lambda}$.
\end{proof}

In addition to fulfilling the completeness and soundness requirements, the zkDL protocol also guarantees zero-knowledge, ensuring that it reveals no information about the training set and model parameters. This property is formalized in Theorem \ref{thm:zero-knowledge}, and the proof is outlined in Appendix \ref{appendix:zkrelu}.

\begin{Thm}[Zero-knowledge]\label{thm:zero-knowledge}
    Assuming the usage of the zero-knowledge Pedersen commitment scheme and the zero-knowledge variant of the GKR protocol \cite{DBLP:journals/eccc/ChiesaFS17, libra, orion}, Protocol \ref{protocol:zkdl} is zero-knowledge. In particular, for a security parameter $\lambda$ and any probabilistic polynomial (PPT) algorithm $\mathcal{A}$, there exists a simulator $\mathcal{S} = (\mathcal{S}_1, \mathcal{S}_2)$ such that the following two views are indistinguishable by $\mathcal{A}$ when provided with the public parameter $\texttt{pp}$ (corresponding to the generators used in the commitment scheme) as input.
    \begin{mdframed}[backgroundcolor=white,linewidth=1pt,roundcorner=5pt]
    \noindent\textbf{Real:}
      \begin{algorithmic}[1]
        \State $\com{\,}\gets \tt{Commit}\left(\mathbf{X} \|\mathbf{y}\|\mathbf{W}\|\mathbf{G}_\mathbf{W}\|\mathbf{aux}; \tt{pp}\right)$ 
        \State $\pi\gets \innerproduct{\mathcal{T}}{\mathcal{A}}.\tt{zkDL}(\tt{com}; \tt{pp})$
        \State \Return $\com{\,}, \pi$
      \end{algorithmic}
    \end{mdframed}
    %\hy{define this notation?}\hc{I am working on this, the notations in the previous parts need to be fixed to be compatible}

    \begin{mdframed}[backgroundcolor=white,linewidth=1pt,roundcorner=5pt]
    \noindent\textbf{Ideal:}
      \begin{algorithmic}[1]
        \State $\com{\,}\gets \mathcal{S}_1\left(1^\lambda; \tt{pp}\right)$
        \State $\pi\gets \innerproduct{\mathcal{S}_2}{\mathcal{A}}(\com{\,}; \tt{pp})$, given oracle access to $C$
        \State \Return $\com{\,}, \pi$
      \end{algorithmic}
    \end{mdframed}
    In the given context, $\texttt{Commit}\left(\mathbf{X} \|\mathbf{y} \|\mathbf{W} \|\mathbf{G}_\mathbf{W} \|\mathbf{aux}; \texttt{pp}\right)$ represents the process of making commitments for the data, model parameters, gradients, and auxiliary inputs using the generators contained in $\texttt{pp}$. The event $C = C^a\land C^v$ denotes the occurrence of two conditions: the satisfaction of all arithmetic relations ($C^a$) and the fulfillment of the value requirements for all auxiliary inputs ($C^v$). 
\end{Thm}
%\hy{do we want to move it?}\hc{TODO:}

\begin{Exm}[FCNN]\label{exm:fcnn} We examine a fully connected neural network (FCNN) comprising $L$ layers, assuming inputs and outputs of dimension $d$ and employing the square loss. Consider a batch of data points denoted as $(\mathbf{X} = \mathbf{A}^{(0)}, \mathbf{Y})$. In this context, we establish the following requirements: \begin{align}
    \label{eq:fcnn-Z}\mathbf{Z}^{(\ell)} &= \mathbf{A}^{(\ell - 1)}\mathbf{W}^{(\ell)}, & 1\leq \ell \leq L,\\
    \label{eq:fcnn-A}\mathbf{A}^{(\ell)} &= (\mathbf{1}-\mathbf{B}_{Q-1}^{(\ell)}) \odot {\mathbf{Z}^{(\ell)}}'', & 1\leq \ell \leq L-1,& ~\\
    \label{eq:fcnn-GZ-last}\mathbf{G}_{\mathbf{Z}}^{(L)} &= {\mathbf{Z}^{(L)}}' - \mathbf{Y},& ~\\
    \label{eq:fcnn-GA}\mathbf{G}_{\mathbf{A}}^{(\ell)} &= \mathbf{G}_\mathbf{Z}^{(\ell + 1)}{\mathbf{W}^{(\ell + 1)}}^\top, & 1 \leq \ell \leq L-1, \\
    \label{eq:fcnn-GW}\mathbf{G}_\mathbf{W}^{(\ell)} &= {\mathbf{G}_\mathbf{Z}^{(\ell)}}^\top {\mathbf{A}^{(\ell-1)}}, & 1\leq \ell \leq L,\\
    \label{eq:fcnn-GZ}\mathbf{G}_\mathbf{Z}^{(\ell)} &= (\mathbf{1}-\mathbf{B}_{Q-1}^{(\ell)}) \odot {\mathbf{G}_\mathbf{A}^{(\ell)}}', & 1 \leq \ell \leq L-1.
\end{align}
Here, the prover sends the commitments of auxiliary inputs ${\mathbf{Z}^{(\ell)}}''$, $\mathbf{B}_{Q-1}^{(\ell)}$, $\mathbf{R}_\mathbf{Z}^{(\ell)}$, ${\mathbf{G}_\mathbf{A}^{(\ell)}}'$, $\mathbf{R}_{\mathbf{G}_\mathbf{A}}^{(\ell)}$ for each layer $1\leq \ell \leq L$ to the verifier, along with the commitments of the weights and data. The homomorphism of the commitments allows for direct verification of the equality in \eqref{eq:fcnn-GZ-last}. Furthermore, the equality checks in \eqref{eq:fcnn-Z}, \eqref{eq:fcnn-GA}, and \eqref{eq:fcnn-GW} can be performed using the sumcheck for matrix products. Notably, since the tensors involved in these operations are either committed values or $\mathbf{A}^{(\ell)}$ and $\mathbf{G}_\mathbf{Z}^{(\ell)}$, and assuming the layers have the same dimension or zero-padding is applied otherwise, the equality check for multiple layers can be batched using random linear combination, resulting in a reduced proof size by a factor of $L$. Additionally, batching these proofs simplifies the correctness verification, as it relies on claims of random point evaluation on the stacked tensors $\mathbf{G}_\mathbf{Z}$ and $\mathbf{G}_\mathbf{A}$ and reduces to the claims involving $\me{\mathbf{B}_{Q-1}}{\mathbf{u}_\text{relu}}$, $\me{\mathbf{Z}''}{\mathbf{u}_\text{relu}}$, and $\me{\mathbf{G}_\mathbf{A}'}{\mathbf{u}_\text{relu}}$ using \eqref{eq:fcnn-A} and \eqref{eq:fcnn-GZ}.


    
\end{Exm}


\subsection{Overhead analysis}

The zkReLU protocol, along with its compatible arithmetic circuit design, significantly reduces the computational overhead for the trainer in terms of both the time and space complexity of the proof. Table \ref{tab:advantage} provides an overview of the asymptotic advantages gained in proof time and sizes when compared with the naive bit decomposition and conventional sequential proofs that follow the ordering of the layers in the neural network.

\medskip
\noindent\textbf{Proving time. }The non-arithmetic operations, including ReLU, have been the dominant factor contributing to the time complexity of zero-knowledge proofs for machine learning. In Section \ref{sec:validity}, the validity proof requires $O(DQ)$ group and field operations for each layer, which is asymptotically equivalent to the computation time needed for representing valid auxiliary inputs using bits. Specifically, the transformations of the auxiliary inputs and their commitments into a single inner product require $O(DQ)$ time, as well as the proof of the inner product itself. %Compared with the computation of ReLU which takes $O(D)$ operations, the overhead is 

In contrast, straightforwardly performing the sumcheck protocol between the auxiliary inputs and their bit decompositions would require $\Omega(D^2Q)$ operations per layer \cite{gkr, DBLP:conf/ccs/ZhangLWZSXZ21, libra, orion} on the equation: \begin{gather}
    \me{\mathbf{aux}}{\mathbf{u}} = \sum_{i\in \{0,1\}^{\log_2D}}\sum_{j\in \{0,1\}^{\log_2D}}\sum_{k\in\{0,1\}^{\log_2Q}}\me{\beta}{\mathbf{u}, i}\me{\mathbf{add}}{i,j,k}\me{\mathbf{B}}{j, k}2^k,
\end{gather} where $\widetilde{\mathbf{B}}$ represents the multilinear extension of bit decomposition of the auxiliary input $\mathbf{aux}$, and $\widetilde{\mathbf{add}}$ corresponds to the multilinear extension of wiring predicates used for the proofs on general arithmetic circuits. Therefore, an improvement factor of $O(D)$ is achieved for each ReLU activation. Additionally, due to the design of the circuit that incurs high parallelizability, the proof time on the entire forward and backward propagation process can be further compressed. 

\medskip
\noindent\textbf{Parallelization. }The proof of the training process based on zkReLU offers a higher degree of parallelization compared to the training process itself. As described in Section \ref{sec:integration}, the order of proof generation is NOT bound by the precedence relationships of the neural network layers. Instead, as illustrated in Figure \ref{fig:order}, the trainer needs to prove \emph{independently for each layer} \textbf{a)} the arithmetic relations within the layer, \textbf{b)} the arithmetic relations between the layer and the auxiliary inputs, and \textbf{c)} the validity of the auxiliary inputs. Therefore, each of the three steps can be run on all layers \emph{in parallel}, without being subject to the ordering of the layers in the neural network. As a result, the total proving time can be reduced by a factor of $O\left(L\right)$, while only adding an $O(\log L)$ overhead to combine the proofs for all layers, which is typically dominated by the $O(DQ)$ per-layer proving time.

Given that deep learning computations are commonly performed in highly parallelized environments, harnessing a significant degree of parallelization can effectively leverage the available computational resources and substantially diminish the additional overhead imposed on the original training time. Additionally, parallelization not only avoids incurring a trade-off between proof time and proof sizes but also \emph{reduces} the proof size.

% Figure environment removed

%\hy{Wouldn't this lead to more commitments that the trainer needs to make and lead to an increase of the proof size, compared to the case when the circuit is connected? Why is the disconnected circuit your unique weapon?}\hc{explanation added}



\medskip
\noindent\textbf{Proof size. }By leveraging the independent nature of different layers in the circuit, the proofs of arithmetic operations in different layers can be batched using a random linear combination. Additionally, the committed auxiliary inputs and outputs from different layers can be stacked together and committed as a single tensor. 

Therefore, by using the same randomness for all layers in each step of the proof (as illustrated in Figure \ref{fig:order}), the proofs for all layers can be compressed into one, which only introduces an additive overhead of $O(\log L)$ to the proof size of a single layer, where $L$ represents the total number of layers. In contrast, a serially generated proof concatenates the proofs of each layer, resulting in a linear growth of proof size in the number of layers multiplied by the proof size of a single layer. Typically, the proof size of a single layer is $O\left(\log (DQ)\right)$, where $D$ represents the dimensionality of the auxiliary inputs in a single layer and $Q$ represents the maximum number of bits for the values of the auxiliary inputs.

\subsection{Dealing with training data} \label{sec:data}
Upon verification of the training logic based on zkReLU, the trusted verifier provides an endorsement for the committed model parameters and datasets. The committed model parameters can be utilized for zero-knowledge verifiable inferences, while the authenticity of the training data can also be verified using the commitment of the data points.

Since each data point is typically involved in the training loop multiple times and may be assigned to different batches in different epochs, it is necessary to individually commit the data points before the training begins. In each step of the zkReLU protocol, a claim must be proven regarding the random-point evaluation of the multilinear extension $\me{\mathbf{X}}{\mathbf{u}, \mathbf{v}}$. Here, $\mathbf{X} = (\mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_{B-1})$ represents the data batch with a batch size of $B$ and dimension $d$. For convenience, we assume both $B$ and $d$ are powers of 2. Additionally, $\mathbf{u}\sim \F^{\log_2B}$ and $\mathbf{v}\sim \F^{\log_2d}$ represent the randomness generated during the execution of the zkReLU protocol.

To achieve this, the data points are separately committed using the Pedersen commitment scheme, resulting in commitments $\com{\mathbf{x}_0}, \com{\mathbf{x}_1}, \dots, \com{\mathbf{x}_{B-1}}$. Both the trainer and trusted verifier can then compute the commitment of $\me{\mathbf{X}}{\mathbf{u}, \cdot}\in \F^d$ as follows: \[
\com{\me{\mathbf{X}}{\mathbf{u}, \cdot}} := \prod_{i\in \left\{0,1\right\}^{\log_2B}}\com{\mathbf{x}_i}^{\me{\beta}{\mathbf{u}, i}}.
\]

Subsequently, the trainer proceeds to perform a proof of opening directly on $\com{\me{\mathbf{X}}{\mathbf{u}, \cdot}}$ in order to demonstrate the claim regarding $\me{\mathbf{X}}{\mathbf{u}, \mathbf{v}}$.

In addition to the endorsement by the trusted verifier, the trainer constructs a Merkle tree on all data points. This Merkle tree can be checked by the trusted verifier and enables data copyright owners to inquire about the membership status of their data points in the training set. The identification of data points is accomplished using Pedersen hash functions, assuming a hash output bit length of $k$, and employing deterministic Pedersen commitments with the randomness set to 0, as described in Section \ref{sec:pedersen}.

The Merkle tree is constructed based on a complete binary tree with a height of $k$, ensuring that each data point is stored in a leaf node identified by its hash, presented in the form of a bit string. When a data point is queried, the trainer can provide a zero-knowledge proof of membership or non-membership. This proof takes the form of a path from the leaf node identified by (a prefix of) the hash of the queried data point to the root of the Merkle tree \cite{DBLP:conf/focs/MicaliRK03}. Further details regarding the proof of membership or non-membership are provided in the appendix.

\section{Experiments} \label{sec:experiments}

\begin{table}[htbp]
\caption{Proving times (s) and proof sizes (kB) of zkReLU and Sum-Check Bit-Decomposition (SC-BD) on a full-connected network of $L=2$ layers. Runs exceeding the time limit are marked as $>10^3$ (s). \#param and \#aux represent the number of parameters and auxiliary inputs in each run. The table records the performance of proof generation of whole networks in one batch update.}
\label{tab:2lp-results}
\resizebox{1.0\linewidth}{!}{%
\begin{tabular}{cccccccc}
\hline
\multirow{2}{*}{Width} & \multirow{2}{*}{\# param}        & \multirow{2}{*}{BS} & \multirow{2}{*}{\# aux} & \multicolumn{2}{c}{\textbf{zkReLU (ours)}} & \multicolumn{2}{c}{SC-BD} \\
                       &                                   &                     &                         & time (s)    & size (kB)    & time (s)          & size (kB)  \\ \hline
\multirow{4}{*}{64}    & \multirow{4}{*}{$8.3$K} & 16                  & $5.1 \times 10^3$       & 0.033       & 9.4          & 12                & 14         \\
                       &                                   & 32                  & $1.0 \times 10^4$       & 0.068       & 10           & 44                & 15         \\
                       &                                   & 64                  & $2.0 \times 10^4$       & 0.13        & 11           & $2.1\times 10^2$  & 16         \\
                       &                                   & 128                 & $4.1 \times 10^4$       & 0.27        & 12           & $>10^3$           & 17         \\ \hline
\multirow{4}{*}{256}   & \multirow{4}{*}{$130$K} & 16                  & $2.0 \times 10^4$       & 0.14        & 11           & $2.0\times10^2$   & 16         \\
                       &                                   & 32                  & $4.1 \times 10^4$       & 0.50        & 12           & $>10^3$           & 17         \\
                       &                                   & 64                  & $8.2 \times 10^4$       & 0.57        & 13           & $>10^3$           & 18         \\
                       &                                   & 128                 & $1.6 \times 10^5$       & 1.2         & 14           & $>10^3$           & 20         \\ \hline
\multirow{4}{*}{1,024}  & \multirow{4}{*}{$2.1$M} & 16                  & $8.2 \times 10^4$       & 0.80        & 13           & $>10^3$           & 18         \\
                       &                                   & 32                  & $1.6 \times 10^5$       & 1.4         & 14           & $>10^3$           & 20         \\
                       &                                   & 64                  & $3.3 \times 10^5$       & 2.8         & 14           & $>10^3$           & 21         \\
                       &                                   & 128                 & $6.5 \times 10^5$       & 6.0         & 15           & $>10^3$           & 22         \\ \hline
\multirow{4}{*}{4,096}  & \multirow{4}{*}{$33$M} & 16                  & $3.3 \times 10^5$       & 3.9         & 14           & $>10^3$           & 21         \\
                       &                                   & 32                  & $6.5 \times 10^5$       & 6.6         & 15           & $>10^3$           & 22         \\
                       &                                   & 64                  & $1.3 \times 10^6$       & 11          & 16           & $>10^3$           & 23         \\
                       &                                   & 128                 & $2.6 \times 10^6$       & 25          & 17           & $>10^3$           & 24         \\ \hline
\end{tabular}
}

\end{table}

% Figure environment removed

\begin{table*}[htbp]
    \caption{Proof size (i.e., the number of hash values released by the model trainer) and verification time (in milliseconds) of proof of (non-)membership for different positivity ratios with various hash functions. The second column refers to the number of queried data to be verified w.r.t. their training membership. The third column shows the tree construction time $t_\tt{tree}$ (in seconds).}
    \resizebox{1.0\textwidth}{!}{%
    \begin{tabular}{ccccccccccccc}
        \hline
        \multirow{3}{*}{hash}        & \multirow{3}{*}{\# data} & \multirow{3}{*}{$t_\tt{tree}$ (s)} & \multicolumn{10}{c}{Positivity ratio}                                                                                    \\ 
                                     &                             &                                    & \multicolumn{2}{c}{0} & \multicolumn{2}{c}{0.1} & \multicolumn{2}{c}{0.5} & \multicolumn{2}{c}{0.9} & \multicolumn{2}{c}{1} \\ \cline{4-13} 
                                     & & & size (\#) & time (ms) & size (\#) & time (ms) & size (\#) & time (ms) & size (\#) & time (ms) & size (\#) & time (ms)\\ 
                                     \hline\hline
        \multirow{3}{*}{\tt{md5}}    & 10                          & \multirow{3}{*}{174}               & 148 & 0.84 & 260 & 4.6 & 697 & 12 & 1,136 & 19 & 1,244 & 22 \\
                                     & 100                         &                                    & 1,059 & 5.9 & 2,168 & 37 & 6,632 & 110 & 11,042 & 200 & 12,163 & 220 \\
                                     & 1,000                        &                                    & 7,148 & 48 & 18,248 & 350 & 62,565 & 1,300 & 107,094 & 2,200 & 118,180 & 2,300 \\
                                     \hline
        \multirow{3}{*}{\tt{sha1}}   & 10                          & \multirow{3}{*}{256}               & 136 & 0.79 & 284 & 5.9 & 854 & 17 & 1,419 & 29 & 1,564 & 32 \\
                                     & 100                         &                                    & 1,033 & 5.7 & 2,481 & 54 & 8,196 & 170 & 13,905 & 320 & 15,333 & 370 \\
                                     & 1,000                        &                                    & 6,995 & 45 & 21,312 & 530 & 78,583 & 2,900 & 135,775 & 4,600 & 150,122 & 6,000 \\
                                     \hline
        \multirow{3}{*}{\tt{sha256}} & 10                          & \multirow{3}{*}{602}               & 147 & 0.99 & 388 & 13 & 1,342 & 41 & 2,288 & 71 & 2,530 & 79 \\
                                     & 100                         &                                    & 1,036 & 6.3 & 3,436 & 100 & 12,987 & 460 & 22,575 & 780 & 24,962 & 870 \\
                                     & 1,000                        &                                    & 7,163 & 53 & 31,055 & 1,100 & 126,617 & 7,100 & 222,259 & 15,000 & 246,158 & 17,000 \\ \hline
    \end{tabular}
    }
    \label{tab:membership-exp}
    \flushleft{
    \rule{0in}{1.2em}{\footnotesize \textsuperscript{\textdagger} zkDL achieves 100\% membership inference accuracy in all above experiments, in contrast to a maximum of 63.7\% with MIA \cite{mia5}.
    }
    }
\end{table*}

In this section, we present the experimental evaluations of zkDL conducted on a Linux server equipped with 2 AMD EPYC 7532 CPUs, each with 32 cores and 256 GB of RAM. The experiments were performed using the CIFAR-10 dataset, with a dimension of 3,072, padded to 4,096 as a power of 2. The neural network architecture and circuit design described in Example \ref{exm:fcnn} were utilized for the experiments. To control the rounding errors caused by quantization, a scaling factor of $2^{16}$ was applied, and all real values involved in the computation were assumed to fall within the range of $[-2^{15}, 2^{15})$, making them amenable to scaling as 32-bit integers. Notably, no overflow issues were encountered during the experiments. We employed the MCL library \cite{mcl} to handle finite fields and elliptic curves, while the XTensor library \cite{xtensor} was utilized for tensor-based operations involved in deep learning and their associated proofs.

Prior works on zero-knowledge verifiable inference, whether bit-decomposition-based or lookup-table-based \cite{zen, zkcnn, zkml}, lack sufficient techniques to generalize the verifiability to the training phase, especially concerning the handling of ReLU. As a result, we follow the approach of pioneering works on zero-knowledge verifiable training \cite{veriml, unlearning} and implement the bit-decomposition-based handling of ReLU on the general-purpose ZKP backends, serving as our baseline for comparison.

\subsection{Experiments on zkReLU} \label{sec:experiments-zkrelu}

To evaluate the effectiveness of the zkReLU protocol, we initially focus on 2-layer perceptrons, which include only one ReLU activation. We vary the width (number of neurons in each layer) and the batch size (BS), recording the per-batch proving time and proof size in Table \ref{tab:2lp-results}. Additionally, we compare the proving time and sizes of zkReLU with the naive bit-decomposition (BD) approach using the sumcheck protocol, which represents how ReLU is handled in general-purpose zero-knowledge proof (ZKP) backends. A proving time limit of $10^3$ seconds is set for each experiment run. In the case of a timeout, the proof size is derived analytically without matching it with the experimental results for a sanity check.


In Table \ref{tab:2lp-results}, it can be observed that the proof time and proof sizes both exhibit a feasible rate of growth with respect to the increasing width and batch sizes. This scalability allows the proof of training logic based on zkReLU to be applied to models with 33 millions parameters within a running time of less than a half minute. In contrast, the na\"ive bit-decomposition (BD)-based method experiences unrealistic running times, except for small models and batch sizes. This highlights the limitation of using general-purpose zero-knowledge proof (ZKP) backends as a black box for deep learning proofs. It emphasizes the need for specifically tailored ZKP schemes to effectively prove the correctness of deep learning execution, particularly during the training process.

Additionally, we conduct experiments to further investigate the advantage of the high degree of parallelization in the circuit design compatible with zkReLU. We focus on the proof of training logic on multi-layer perceptrons with varying depths. We compare the optimized running times and proof sizes of parallel proofs, where the same randomness is applied to each layer, against those of conventional sequential proofs that align with the layer structure assumed in previous works \cite{unlearning, veriml, zkml} and formalized by Liu et al. in 2021 \cite{zkcnn}. The results of these experiments are presented in Figure \ref{fig:mlp-results}.

The parallel generation of proofs in zkReLU offers a significant advantage over conventional sequential proofs, as the proving time is no longer subject to linear increase with respect to the depth of the network. This parallelization enables the proof of the entire forward and backward process to be completed within tens of seconds, even for a 16-layer perceptron with over 200M parameters. Additionally, the proof size is efficiently controlled under 30 kB, compared to the larger sizes of up to 200-300 kB that occur with increasing depth. By harnessing the parallel computational resources available for deep learning, the proof generation based on zkReLU-compatible circuits achieves more favourable proof times and sizes, representing a significant step forward in the practical application of zero-knowledge proofs for deep learning in the AI industry.

\subsection{Additional experiments on the training data}


To evaluate the proof of (non-)membership described in Section \ref{sec:data}, we implemented the Merkle tree on the CIFAR-10 training set using three different hash functions: $\tt{md5}, \tt{sha1}, \tt{sha256}$. We conducted experiments with varying query sizes and positivity ratios (the ratio of positive data points, i.e., members of the training set, in the query set) and recorded the results.

Table \ref{tab:membership-exp} illustrates that Merkle tree construction times, proof sizes, and verification times increase with the query size and output length of the hash. Notably, queries with a larger positivity ratio exhibit increased complexity. However, when data copyrights are not violated (positivity ratio is 0), it only takes 0.05 milliseconds on average for a copyright owner to confirm that a data point they own is not in the training set. This constitutes a significant improvement over na\"ively loading and scanning the entire committed dataset, which takes 14 seconds. 

Furthermore, the proof of (non-)membership achieves 100\% accuracy due to the correctness and soundness of the Merkle tree. No data point was found for which the trainer can lie about its membership in the training dataset. This represents a substantial improvement over membership inference attacks \cite{mia9}, which achieve only 59.0\% to 63.7\% accuracy on the same dataset (CIFAR-10). These results underscore the crucial role of zkDL in providing rigorous guarantees of data legitimacy in deep learning.



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle






% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%% Figure environment removed

% Note that the IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command,
% and the \label for the overall figure must come after \caption.
% \hfil is used as a separator to get equal spacing.
% Watch out that the combined width of all the subfigures on a 
% line do not exceed the text width or a line break will occur.
%
%% Figure environment removed
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.




\section{Conclusion}
This paper presents zkDL, a novel solution to the challenge of zero-knowledge verifiable training for neural networks. Specifically focusing on the ReLU activation function, zkDL addresses the historical incompatibility with general-purpose ZKP backends. The protocol's design includes multiple efficient and mutually compatible ZKP systems tailored for deep learning training, as well as a novel modeling scheme of neural networks as arithmetic circuits, enabling a high degree of proof parallelization. These advancements significantly reduce time and communication costs associated with proving legitimate execution of deep learning training, effectively resolving legitimacy issues surrounding trained neural networks. Looking ahead, further theoretical development and practical implementations of efficient ZKP systems tailored for deep learning are anticipated to establish zkDL as a powerful tool safeguarding the AI industry's development.

% \ifCLASSOPTIONcompsoc
%   % The Computer Society usually uses the plural form
%   \section*{Acknowledgments}
% \else
%   % regular IEEE prefers the singular form
%   \section*{Acknowledgment}
% \fi
% The authors would like to thank...



% use section* for acknowledgment









% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
\clearpage
\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{reference}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

\appendices 
\section{zkReLU} \label{appendix:zkrelu}

\begin{proof}[Proof sketch of Theorem \ref{thm:completeness}]
    By the perfect completeness of the GKR protocol and the underlying sumcheck protocol, the trusted verifier accepts the interactive proofs in Lines \ref{alg-line:layers-gkr} and \ref{alg-line:connect-sumcheck} with probability 1, assuming the correct execution of all arithmetic operations within each layer and the Hadamard products in Equations \eqref{eq:zkrelu-A} and \eqref{eq:zkrelu-GZ}.

    Moreover, given the assumption that $\mathbf{Z}^{(\ell)}$ and $\mathbf{G}_\mathbf{A}^{(\ell)}$ are $(Q+R)$-bit integers, the decomposition based on Equations \eqref{eq:zkrelu-Z} and \eqref{eq:zkrelu-GA} ensures that the auxiliary inputs fall within their respective ranges. By correctly computing $\mathbf{B}_k$ and $\mathbf{B}_k'$, the trainer establishes the equalities in Equations \eqref{eq:range-valid}, \eqref{eq:bit-valid-diff}, \eqref{eq:bit-valid-prod}, and the inner product in Equation \eqref{eq:main-inner-prod}. Therefore, based on the perfect completeness of the inner-product proof \cite{bulletproof}, the trusted verifier accepts the validity of the auxiliary inputs with probability 1 in Line \ref{alg-line:aivp}.
\end{proof}

\begin{proof}[Proof sketch of Theorem \ref{thm:zero-knowledge}] 
   First, consider the GKR protocol applied to all the arithmetic operations involving $\mathbf{X}$, $\mathbf{y}$, $\mathbf{W}$, $\mathbf{G}_\mathbf{W}$, and $\mathbf{aux}$. By assuming the zero-knowledge properties of the commitment scheme and the GKR protocol, there exist simulators $\mathcal{S}^a = (\mathcal{S}_1^a, \mathcal{S}_2^a)$ such that $\mathcal{S}_1^a$ simulates the generation process of the commitments and $\mathcal{S}_2^a$ simulates the GKR-based proof of the arithmetic circuits, given oracle access to $C^a$.

    On the other hand, the auxiliary input validity proofs involve the zero-knowledge commitments of additional inputs, $\com{\mathbf{B}}^\text{ip}$ and $\com{\mathbf{B}_{Q-1}}^\text{ip}$ as defined in Protocol \ref{protocol:zkrelu-setup}. These commitments are subsequently transformed in Algorithm \ref{algorithm:transform-commitment} to perform the zero-knowledge inner-product proof. Therefore, there exists another simulator $\mathcal{S}^v = (\mathcal{S}^v_1, \mathcal{S}^v_2)$ such that $\mathcal{S}^v_1$ simulates the generation of the commitments and $\mathcal{S}^v_2$ simulates the inner-product proof, given oracle access to $C^v$. Thus, with $\mathcal{S}_1 = \mathcal{S}_1^a$ and $\mathcal{S}_2 = (\mathcal{S}_1^v, \mathcal{S}_2^a, \mathcal{S}_2^v)$, the ideal view is indistinguishable from the real view.
\end{proof}

\section{Proof of (non-)membership} \label{appendix:membership}
%\ref{sec:membership}
This section presents the proof of (non-)membership based on a Merkle tree that allows data copyright owners to query the membership of their data points in the dataset. After verifying the proofs of data quality and training logic, the trusted verifier requests the trainer to submit the root value of the Merkle tree, which the trusted verifier also computes itself using the commitments of all data points. If the two computed root values match, the trusted verifier endorses the private dataset and model parameters by publishing a digital signature on the committed model parameters and the root value of the Merkle tree. This signature enables the proof of (non-)membership. Upon the query on a data point, in the form of its commitment, the trainer computes the path in the Merkle tree to the root as the proof. The data copyright owner checks whether the reconstructed root value matches the one signed by the trusted verifier, providing a cost-effective and privacy-preserving way of verifying data ownership.

%the trainer and trusted verifier agree on the value of the Merkle tree root computed from the commitments of the datasets, thereby convincing the data providers that the model has not been secretly trained on other datasets \hy{how?}.

Using the Merkle tree reduces the size of the proof and limits the information leaked about data points that are not being queried. However, the original version of the Merkle tree cannot be directly applied since it cannot handle the proof of non-membership, where a data point is excluded from the training set as required. To address this, we introduce a variant of the Merkle tree that supports both the proof of membership and non-membership.

To construct the Merkle tree, we assume the existence of a collision-resistant hash function $\tt{hash}: \{0,1\}^*\to \{0,1\}^k$. The model trainer begins by computing the hash of the commitment of each data point: $\{\tt{hash}(\com{d}): d \in \D\}$.

For analysis purposes, we consider the complete binary tree $\mathcal{T}_k$ with depth $k$ (which is not required to be implemented). The nodes of depth $i$ ($0\leq i \leq k$) in $\mathcal{T}_k$ can be identified with bit strings of length $i$. The root of $\mathcal{T}_k$ is identified with the empty string $\epsilon$, and each of the $2^k$ leaves of $\mathcal{T}_k$ is identified with a bit string of length $k$. For each leaf node $b_1b_2\dots b_k$, we define $\tt{Path}_k(b_1b_2\dots b_k)$ as the path from the node to the root, i.e., $ b_1b_2\dots b_k \to b_1b_2\dots b_{k-1} \to \dots \to b_1b_2 \to  b_1 \to \tt{root}$, where the $\tt{root}$ is represented by the empty string. We also define $H_\D:= \{\tt{hash}(\com{d})\}_{d\in \D}$ as the collection of hashes of the data points. Due to the collision-resistance property, there is a bijection between $H_\D$ and $\D$ with an overwhelming probability.


We define the subtree corresponding to the training set $\D$ as the union of the paths from the nodes to the root in $H_\D$, i.e., $\tt{Tree}(H_\D):=\bigcup_{h\in H_\D}\tt{Path}_k(h)$; and its frontier as the nodes that are not in $\tt{Tree}(H_\D)$, but have its parent in $\tt{Tree}(H_\D)$, i.e., \[\tt{Frontier}(H_\D):= \left\{vb: vb\notin \tt{Tree}(H_\D), v\in \tt{Tree}(H_\D)\right\}\] (where $b\in \{0, 1\}$ is a bit). Then, we define another subtree $T_\D:= \tt{Tree}(H_\D)\cup \tt{Frontier}(H_\D)$ where each node in $T_\D$ either is a leaf node of $T_\D$ or has two children in $T_\D$, such that $\tt{Frontier}(H_\D)\cup H_\D$ is exactly the set of leaf nodes of $T_\D$. 

Then, the model trainer constructs the Merkle tree based on $T_\D$ by assigning values to its nodes. We denote the value assignment as $\tt{Val}_{T_\D}(\cdot)$, such that the leaf nodes of $T_\D$ are first assigned as follows:

\begin{itemize}
    \item Each $h_d = \tt{hash}(\com{d})\in H_\D$ is assigned value $\tt{Val}_{T_\D}(h_d) \gets \com{d}$;
    \item Each $v\in \tt{Frontier}(\D)$ is assigned value $\tt{Val}_{T_\D}(v) \gets\epsilon$;
\end{itemize}

Then, by executing Algorithm \ref{algorithm:merkle} as $h_\D\gets \tt{MerkleTree}(H_\D\cup \tt{Frontier}(H_\D), \tt{Val}_{T_\D})$, the value of each remaining node $v$ is assigned such that $\tt{Val}_{T_\D}(v) = \tt{hash}(\tt{Val}_{T_\D}(v0), \tt{Val}_{T_\D}(v1))$. The model owner only publishes the root value of $T_\D$, denoted by $h_\D$, which equals $\tt{Val}_{T_\D}(\tt{root})$.

\begin{algorithm}\caption{(Re-)construction of Merkle tree: $\tt{MerkleTree}(S, \tt{Val})$}\label{algorithm:merkle}
    \begin{algorithmic}[1]
        \Require A set of leaf nodes $S$ identified by their binary representation, and the value of each node $\tt{Val}: S\to \{0,1\}^*$
        \Procedure{MerkleTree}{$S, \tt{Val}$}
            \State $\tt{depth}\gets \max\{\tt{length}(s): s\in S\}$
            \For{$k\gets \tt{depth}, \tt{depth}-1, \dots 1$}
                \State $S_k\gets \{s\in S: \tt{length}(s)=k\}$
                \State $S_{k-1}'\gets \{v: v0, v1 \in S_k\}$
                \If{$2\abs{S_{k-1}'}\neq \left|S_k\right|$} 
                    \State Abort since exists $v\in S_k$ whose sibling is not in $S_k$
                \EndIf
                \State $S\gets S\sqcup S'_{k-1}$ \Comment{Must be disjoint union, abort otherwise}
                \For {$v\in S_{k-1}'$} \Comment{Recursively compute the hashes}
                    \State $\tt{Val}(v)\gets \tt{hash}(\tt{Val}(v0), \tt{Val}(v1))$
                \EndFor
                
            \EndFor
            \State $\{\tt{root}\}\gets S_0'$ \Comment{By execution, $\abs{S_0'} = 1$}
            \State \Return $\tt{Val}(\tt{root})$
        \EndProcedure
        
    \end{algorithmic}
\end{algorithm}

% Figure environment removed

An example of the Merkle tree with $k=3$ is shown in Figure \ref{fig:merkle-tree-example}. The hashes of the data points are $H_\D=\{000, 010, 011\}$ (marked in green), which store the values of the commitments of the corresponding data points. The frontier $\tt{Frontier}(H_\D)$ is marked in red, with each node storing the value of $\epsilon$. The yellow-coloured nodes are the rest of the Merkle tree $T_\D$, with each node storing the hash value of its two children. The uncolored vertices are not part of $T_\D$.

 When queried about any data point $d$, the model trainer can compute a proof of its (non-)membership in $\D$ by proving the (non-)membership of $h_d:=\tt{hash}(\tt{Commit}(d))$ (note that $\tt{Commit}(d)$ is assumed to be deterministic \cite{kzg}) in $H_\D$ using the Merkle tree $T_\D$. To prove a data point $d\in \D$, the model trainer needs to show that $h_d$ is a leaf of $T_\D$, by reconstructing the path from $h_d$ to the root in $T_\D$ and show that the value of the reconstructed root matches $h_\D$. On the other hand, to prove a data point $d\notin \D$ (with is equivalent to $h_d\notin H_\D$ with overwhelming probability), the model trainer needs to show that there exists an element of the $\tt{Frontier}(H_\D)$, $s$, that is a prefix of $h_d$ or $h_d$ itself (denoted by $s\preceq h_d$), and reconstruct the path from $s$ to the root with the correct reconstructed root value. In general, to accommodate queries for multiple data points $E$, we define $H_E:= \left\{\tt{hash}(\tt{Commit}(d)): d\in E\right\}$, which can be agreed upon by both parties. The model trainer can then prove the training set (non-)membership of each data point in $E$ using Protocol \ref{protocol:membership-prove}:

\begin{protocol}
    \caption{Trainers generates the proof of (non-)membership}\label{protocol:membership-prove}
    \begin{algorithmic}[1]
        \Require $H_E$: the hashed commitments of the queried data points 
        \State $H_E^\tt{inc}, H_E^\tt{exc}\gets H_\D \cap H_E, H_E \backslash H_\D$ \Comment{Hashes of the data points included and excluded from the training set}
        \State $F^\tt{exc}\gets \left\{s\in \tt{Frontier}(H_\D):\exists h\in H_E^\tt{exc}, s\preceq h \right\}$
        \State $F_E\gets \tt{Frontier}(H_E^\tt{inc}\cup F^\tt{exc})$
        \State $\pi^{\tt{mem}} \gets \left(F^\tt{exc}, \tt{Frontier}(F_E)\right)$
        \State \Return $H_E^\tt{inc}, H_E^\tt{exc}, \pi^{\tt{mem}}$
    \end{algorithmic}
\end{protocol} 

Note that the output of Protocol \ref{protocol:membership-prove} includes the values of the nodes in $\tt{Tree}(H_E^\tt{inc})$ and $F^\tt{exc}$, as well as frontier of the union of these two sets, $\tt{Frontier}(H_E^\tt{inc}\cup F^\tt{exc})$. This allows the data copyright owner to reconstruct the Merkle tree and compute its root. Specifically, let $h_E\gets \tt{MerkleTree}(H_E^\tt{inc}\cup F^\tt{exc}\cup \tt{Frontier}(H_E^\tt{inc}\cup F^\tt{exc}), \tt{Val}_E)$, where $\tt{Val}_E$ is the restriction of $\tt{Val}_{T_\D}$ onto $H_E^\tt{inc}\cup F^\tt{exc}\cup \tt{Frontier}(H_E^\tt{inc}\cup F^\tt{exc})$ since the model trainer only needs to release the values on these nodes. Then, the data copyright owner can check the validity of the proof by verifying that the reconstructed root value $h_E$ matches that received from the model trainer $h_\D$ as Protocol \ref{protocol:membership-verify}.

\begin{protocol}
    \caption{Data copyright owner verifies the proof of (non-)membership}\label{protocol:membership-verify}
    \begin{algorithmic}[1]
        \Require The queried data points $E$ (identified by their hashes $H_E$); root value of the training set Merkle tree $h_\D$; the hashed commitments of the queried data points $H_E$; output of Protocol \ref{protocol:membership-prove}, $H_E^\tt{inc}, H_E^\tt{exc}, (F^\tt{exc}, \tt{Frontier}(F_E)) = \pi^{\tt{mem}}$ sent from the trainer; the released node values $\tt{Val}_E$.
        \State Check $H_E \qeq H_E^\tt{inc} \sqcup H_E^\tt{exc}$ \Comment{Disjoint union, reject otherwise}
        \State Check $\tt{Val}(v) \qeq \epsilon$ for each $s\in F^\tt{exc}$
        \State Check $\stackrel{?}{\exists} s\in F^\tt{exc}$ such that $s\preceq h$ for each $h\in H_E^\tt{exc}$
        \State Check $h_\D \qeq \tt{MerkleTree}(H_E^\tt{inc}\cup F^\tt{exc}\cup \tt{Frontier}(H_E^\tt{inc}\cup F^\tt{exc}), \tt{Val}_E)$ 
        \State \Return \tt{accept} if all checks pass, otherwise \tt{reject}.
    \end{algorithmic}
\end{protocol}
%\vspace{-1 em}

For example, in Figure \ref{fig:merkle-tree-example}, let $H_E = \{000, 001, 011, 101\}$ be the queried set. The model trainer computes $H_E^\tt{inc}=\{000, 011\}$ and $H_E^\tt{exc}=\{001, 101\}$, and therefore $F^\tt{exc} = \{001, 1\}$. Therefore, the corresponding $\tt{Frontier}(H_E^\tt{exc}\cup F^\tt{exc}) = \{010\}$. Then, based on the released values of these nodes, the value of the root can be recovered by reconstructing the Merkle tree and then checked with the published value $h_\D$ to verify the proof.

\begin{Thm}[Data Membership]\label{thm:dm}%\hc{TODO: what is input}
     Consider the scenario when a data copyright owner queries the trainer on a batch of his/her data points $E$, in the following steps: 
    \begin{itemize}
        \item The data copyright owner sends the hashes of the queried data points $H_E$ to the trainer;
        \item The trainer sends $H_E^\tt{inc}$ and $H_E^\tt{exc}$ (the hashes of the queried data points that the trainer claims to be included in and excluded from the training set $\D$, respectively) to the data copyright owner, along with the proof $\pi^\tt{mem}$.
        \item The data copyright owner checks the validity of the proof using Protocol \ref{protocol:membership-verify}. 
    \end{itemize}
    
    With probability $1-\negl{\lambda}$, the followings hold:

    \begin{itemize}
        \item If the trainer fully adheres to Protocol \ref{protocol:membership-prove} to compute $H_E^\tt{inc}, H_E^\tt{exc}$ and $\pi^\tt{mem}$, the data copyright owner \tt{accepts} the membership result  returned from the trainer.
        \item If the trainer lies about the training set membership of any queried data point, i.e., $H_E^\tt{inc} \neq H_\D\cap H_E$ or $H_E^\tt{exc}\neq H_E\backslash H_\D$, the data copyright owner \tt{rejects}  the membership result returned from the trainer.
    \end{itemize}
\end{Thm}

\begin{proof}[Proof sketch of Theorem \ref{thm:dm}]
    
    If Protocol \ref{protocol:membership-prove} has been followed by the trainer, then the execution can be represented as $H_E = H_E^\tt{inc} \sqcup H_E^\tt{exc}$. Additionally, $F^\tt{exc}$ is a subset of $\tt{Frontier}(H_\D)$, such that $\tt{Val}(v) = \e$ for each $v\in F^\tt{exc}$, and for each $h\in H_E^\tt{exc}$, there exists $s\in F^\tt{exc}$ such that $s\preceq h$. Furthermore, since $\tt{Val}_E$ and $\tt{Val}_{T_\D}$ coincide for each sent node, the value of the root node recovered from the proof matches that of $h_\D$. As a result, all checks have passed, and the data copyright owner can accept the proof through Protocol \ref{protocol:membership-verify}.

    Consider the case when the trainer outputs $H_E^\tt{inc} \neq H_\D\cap H_E$ or $H_E^\tt{exc}\neq H_\D\backslash H_E$. To pass Protocol \ref{protocol:membership-verify}, it must hold that $H_E = H_E^\tt{inc} \sqcup H_E^\tt{exc}$. Therefore, there is either $h\notin H_\D$ such that $h\in H_E^\tt{inc}$, or $h'\in H_\D$ such that $h\in H_E^\tt{exc}$. However, in either case, the trainer needs to compute a valid path in the Merkle tree from $h$ (or one of the predecessors of $h'$) to the root and match the value of $h_\D$ at the root. Since the hash function used is non-invertible and collision-resistant, the polynomial-time trainer can only succeed with negligible probability. Therefore, the data copyright owner rejects the proof with probability at least $1-\negl{\lambda}$.

    % Without loss of generality, we assume that the query from the data copyright owner contains one data point (i.e., $|H_E| = 1$, denoting the hash of the data point as $h_E$). The proof for general case (i.e., $|H_E| > 1$) can be derived 
\end{proof}

%\section{xxxx}


% that's all folks
\end{document}


