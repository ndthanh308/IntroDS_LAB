\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}

\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{xcolor}
\newtheorem{definition}{Definition}%
%% as per the requirement new theorem styles can be included as shown below
\usepackage{tabularx}
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\usepackage{booktabs}
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\usepackage{multirow,makecell,array}
\usepackage{threeparttable}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{float}
\usepackage{csquotes}
\usepackage{colortbl}
\usepackage{ulem}
\usepackage{orcidlink}
% \usepackage{subfloat}
\usepackage{arydshln}
\setlength\dashlinedash{0.5pt}
\setlength\dashlinegap{1.5pt}
\setlength\arrayrulewidth{0.3pt}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{Towards Head Computed Tomography Image Reconstruction Standardization with Deep Learning Assisted Automatic Detection}

\author{Bowen Zheng~\orcidlink{0009-0000-3167-4399}, Chenxi Huang~\orcidlink{0000-0003-2863-6089}, Yuemei Luo~\orcidlink{0000-0001-8741-449X}
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
Three-dimensional (3D) reconstruction of head Computed Tomography (CT) images elucidates the intricate spatial relationships of tissue structures, thereby assisting in accurate diagnosis. Nonetheless, securing an optimal head CT scan without deviation is challenging in clinical settings, owing to poor positioning by technicians, patient's physical constraints, or CT scanner tilt angle restrictions. Manual formatting and reconstruction not only introduce subjectivity but also strain time and labor resources. To address these issues, we propose an efficient automatic head CT images 3D reconstruction method, improving accuracy and repeatability, as well as diminishing manual intervention. Our approach employs a deep learning-based object detection algorithm, identifying and evaluating orbitomeatal line landmarks to automatically reformat the images prior to reconstruction. Given the dearth of existing evaluations of object detection algorithms in the context of head CT images, we compared ten methods from both theoretical and experimental perspectives. By exploring their precision, efficiency, and robustness, we singled out the lightweight YOLOv8 as the aptest algorithm for our task, with an mAP of 92.91\% and impressive robustness against class imbalance. Our qualitative evaluation of standardized reconstruction results demonstrates the clinical practicability and validity of our method.
\end{abstract}

\begin{IEEEkeywords}
head computed tomography images, three-dimensional reconstruction, object detection
\end{IEEEkeywords}

\section{Introduction}
Three-dimensional (3D) reconstruction of computed tomography images has become an indispensable instrument in a broad spectrum of clinical tasks, particularly over the last few years\cite{bb1}\cite{bb3}\cite{bb4}. Specifically, head CT scans hold a pivotal role, revealing key details about structures such as the brain, skull, sinuses, and various soft tissues. This provides crucial insights instrumental in diagnosing and treatments for a wide range of conditions, from traumas and tumors to strokes and sinusitis\cite{bb5}\cite{bb6}. However, manual interpretation of these scans often proves time-consuming and error-prone. This is primarily attributed to the intricate details and subtle variations that can signify different pathological conditions\cite{bb7}\cite{bb8}. 

Employing computer 3D reconstruction software is possible to transform a sequence of CT axial images into a comprehensive three-dimensional model. This process transforms the series of two-dimensional scans into an intuitive representation of the spatial relationships among tissue structures. The outcome facilitates precise surgical planning and patient-specific treatment strategies, a significant enhancement in medical procedures. The current quality control standards for head CT images take the orbitomeatal line as the baseline for scans. This line is drawn from the center of the external auditory canal (EAC) to the outer canthus of the ipsilateral eye\cite{bb9}.

Nonetheless, it is often challenging to achieve ideal head CT images free of lateral deviations and based on the orbitomeatal line. Factors such as poor positioning by technicians, patient's physical constraints, and restrictions in the tilt angle of the CT scanner contribute to this challenge. As a result, radiologic technologists frequently manually reformat axial images using thin-layer data post-CT examination. Unfortunately, this process may precipitate deviations in the reconstruction baseline as a consequence of the operator’s subjective judgment, thereby affecting the accuracy of reconstruction outcomes. Additionally, manual reformatting and the subsequent 3D reconstruction require a significant investment in terms of time and labor.

Deep learning has emerged as a vital player in fields such as object detection, medical image segmentation\cite{bb10}\cite{bb11}\cite{bb34}, diagnostics\cite{bb12}\cite{bb13}, and predictive analytics\cite{bb14}\cite{bb15}\cite{bb35}. Previous research proposed a semi-automatic multiplanar reconstruction method\cite{bb16}. This method mandates manually setting five head landmarks on axial images to identify the orbitomeatal line, facilitating 3D and multiplanar reconstruction. However, it heightens the labor intensity given the necessity of manual landmark identification. 

Another approach utilizes an object detection algorithm to automatically reformat head CT images based on the orbitomeatal line\cite{bb17}. Despite its benefits, this method mainly focuses on the automatic reformatting of axial head CT images, with limited involvement in 3D reconstruction. Moreover, the You Only Look Once (YOLO) model used in this object detection algorithm exhibits an accuracy of only 0.68, suggesting substantial potential for improvement in detection accuracy.

In response to these challenges, we introduce a robust and trustworthy automated 3D reconstruction method, a solution that could substantially enhance diagnostic precision. The primary contributions of this paper are as follows.

\begin{enumerate}
    \item We have proposed an automatic 3D reconstruction method for head CT images, utilizing automatically reformatted CT images. This method not only promises more accurate and repeatable results, mitigating potential inconsistencies arising from varying technicians' manual reconstruction judgments, but also curtails the need for manual input, yielding considerable savings in time and labor.
    \item Capitalizing on a deep learning-based object detection algorithm, we have devised a streamlined process for 3D reconstruction. This algorithm identifies and evaluates orbitomeatal line landmarks, typically manually annotated by radiologists, to automate image reformatting. This process significantly reduces labor expenditure associated with image reformatting.
    \item We conducted an exhaustive comparative analysis of ten deep learning-based object detection methods, focusing on their accuracy, efficiency, and robustness in the context of automatic reformatting of CT images. Grounded in both theoretical and experimental insights, we discern the most effective and efficient algorithm for this particular task.
\end{enumerate}

% Figure environment removed

\begin{table*}[htbp]
\caption{high-level overview of object detection algorithms}
\renewcommand{\arraystretch}{1.2}
\begin{center}
\begin{tabular}{cccccc}
\hline
\cline{1-5}\rule{0pt}{10pt}
    \textbf{Algorithm} & \textbf{Algorithm Complexity} & \textbf{Architecture Complexity} & \textbf{Training Complexity} & \textbf{Inference Speed} &  \textbf{Robustness to Dataset Shift} \\
\midrule
   DETR  & High & High &	High &	Moderate & High	\\

    EfficientDet  & Moderate & High & Moderate & High & Moderate\\

    Faster R-CNN & High & High  & High &	Low & High\\

    RetinaNet & Moderate & Moderate & Moderate & High & Moderate\\
    
    YOLO  & Low & Low	 & Low &	Very high & Low\\
    \bottomrule
\end{tabular}
\label{tab1}
\end{center}
\end{table*}

\section{Materials and methods}
\subsection{Datasets}
The dataset we utilized comprises 140 consecutive non-contrast head CT scans gathered in January 2021. These scans were acquired utilizing a 128-detector row CT scanner (SOMATOM Definition AS+, Siemens, Erlangen, Germany), with specific scanning parameters: tube voltage set to 100kV, tube current at 447mA, a field-of-view of 222mm, and a reconstruction thickness of 1mm. The demographic distribution within these 140 cases was balanced, with approximately 49\% (68 cases) being male. Age distribution within the dataset varied, ranging from 18 to 94 years.

On average, each case comprised 139 slices. Following the crucial steps of annotation and data preprocessing, a total of 619 annotated images were generated from all the slices. These images were then categorized into a training set, a validation set, and a test set following an 8:1:1 ratio. This distribution resulted in a training set consisting of 495 images, a validation set composed of 62 images, and a test set encompassing another 62 images. This well-organized dataset serves as the foundation for the proposed automated 3D reconstruction method, facilitating its validation and evaluation.

\subsection{Proposed approach overview}

Our proposed approach hinges on standardizing the CT slice data via 3D rotation prior to reconstruction. This standardization process first calls for the identification of four landmarks that determine the orbitomeatal line: bilateral eyes and bilateral external auditory canals. As depicted in Fig. \ref{f1}, we employ a deep learning-based object detection algorithm to automatically identify these four pivotal landmarks. In every slice of each sequence, the object detection algorithm is applied to spot all possible landmarks. These potential landmarks are then assessed to ascertain the most suitable ones for the respective sequence. Utilizing these landmarks, we calculate rotation angles, subsequently performing a 3D rotation for image reformatting. Ultimately, we execute the 3D reconstruction of the reformatted DICOM sequences.

To actualize automated landmark detection, we harness a supervised object detection model, which is trained on preprocessed data. Prior to training, we carry out preprocessing on the raw dataset. The preprocessed data are manually annotated by radiological technologists utilizing the annotation tool LabelImg. This labeled dataset is subsequently partitioned into a training set and a validation set, enabling the trained model to be deployed for potential landmark detection.

The exploration of the performance of object detection algorithms in the context of head CT images remains largely untapped. With the intent of identifying the most suitable algorithm for this task, we selected ten deep learning-based object detection algorithms for a comparative analysis of both theoretical and experimental performance. As delineated in Table \ref{tab1}, we provide a high-level overview evaluating various aspects: Algorithm Complexity, Model Architecture Complexity, Training Procedure Complexity, Inference Speed, and Robustness to Dataset Shift. This systematic examination paves the way for an informed choice of the best-suited algorithm for the task at hand.

\begin{table*}[htbp]
\caption{10 object detection algorithms structures}
\renewcommand{\arraystretch}{1.2}
\begin{center}
\begin{tabular}{cccccc}
\hline
\cline{1-5}\rule{0pt}{10pt}
    \textbf{Algorithm} & \textbf{Stages} & \textbf{Backbone} & \textbf{Neck} & \textbf{Bounding box loss} & \textbf{Classification loss}\\
\midrule
   DETR\cite{bb22}  & Two & ResNet & - & L1 loss + GIoU loss	& Bipartite matching loss\\

    EfficientDet\cite{bb23}  & Two & EfficientNet & - & Smooth L1 loss & Focal loss\\

    Faster R-CNN\cite{bb24} & Two & ResNet  & - &	Smooth L1 loss & Log loss\\
    RetinaNet\cite{bb25} & One & ResNet  & - & Smooth L1 loss & Focal loss\\
    YOLOv3\cite{bb26}  & One & Darknet-53	& FPN &	Sum of squared error loss & Binary cross-entropy\\
    YOLOv4\cite{bb27} & One & CSPDarknet-53	& PANet + SSP &	CIoU loss & Binary cross-entropy\\
    YOLOv5\cite{bb28} & One & CSPDarknet-53	& PANet &	CIoU loss + Focal loss & Binary cross-entropy with Logits loss\\
    YOLOX\cite{bb29} & One & CSPDarknet-53	& PANet &	CIoU loss + Focal loss & Binary cross-entropy with Logits loss\\
    YOLOv7\cite{bb30} & One & E-ELAN	& ELAN-neck &	CIoU loss + Focal loss & Binary cross-entropy with Logits loss\\
    YOLOv8\cite{bb31} & One & CSPDarknet53 with C2f	& PANet &	CIoU loss + Distribution focal loss & VariFocal Loss\\
    \bottomrule
\end{tabular}
\label{tab2}
\end{center}
\end{table*}

\subsection{Data preprocessing}
The data preprocessing involves a transition of the original DICOM data into JPEG format images, while removing the overlays. This conversion maintains the original DICOM size of 512x512 pixels. The processed images possess a resolution of 96dpi, along with an 8-bit depth and a tri-channel color scheme.

% Figure environment removed
\subsection{Potential landmarks detection}

Our deep learning-based object detection algorithm identifies four pivotal landmarks: the bilateral eyes and bilateral external auditory canals. We explored ten object detection models to aid in potential landmark detection. These models' architectures, which encompass their stages, backbone, neck, bounding box loss, and classification loss, are displayed in Table \ref{tab2}. Notably, the YOLO series models, functioning as one-stage detectors, confer the benefit of high computational speed and low computational load, albeit with a trade-off in varying accuracy degrees.

The training generally encompasses two stages: the freezing stage and the unfreezing stage. In the freezing stage, the model's backbone is immobilized, the feature extraction network remains static, and the overall network is fine-tuned. This stage offers benefits stemming from its lower computational resource demand and faster training speed. In contrast, during the unfreezing stage, the model's backbone is not frozen, leading to changes in the feature extraction network, implying that all network parameters can undergo modifications. The process initiates with 50 epochs of freezing training, subsequently advancing to unfreezing training. It is noteworthy that the batch size during the unfreezing phase is halved compared to the freezing phase.

The parameters used during training are outlined in Table \ref{tab3}. Specifically for the DETR model, we employ only the adamw optimizer to expedite training. In contrast, other models utilize both the SGD and Adam optimizers. Considering that SGD necessitates a more extended period to converge, a larger total number of epochs is set. Conversely, Adam can operate with a relatively smaller number of total epochs. Bearing in mind that training a network from scratch might yield unsatisfactory results as a result of exceedingly random weights and unremarkable feature extraction, we fine-tune each model using officially trained weights on representative large-scale datasets in the field of image recognition, such as COCO\cite{bb32}, ImageNet\cite{bb33}.

\begin{table*}[htbp]
\caption{10 object detection algorithms parameters}
\centering
\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{CCCCCC}
\hline
\cline{1-5}\rule{0pt}{10pt}
    \textbf{Algorithm}  &\textbf{Optimizer} &\textbf{Total Epoch} & \textbf{Initial learning rate} & \textbf{Weight decay} & \textbf{Freezing batch size}\\ [0.5ex]
\hline
\vspace{0.5ex}\\ [-2ex]
   DETR  & Adamw & 300 & 0.0001 & 0.0001 & 8\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{EfficientDet}  & Adam & 100 & 0.0003 & 0 & \multirow{2}{*}{8}\\
    & SGD & 300 & 0.01 & 0.0005\\ [0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{Faster R-CNN}  & Adam & 100 & 0.0001 & 0 &\multirow{2}{*}{4}\\
    & SGD & 300 & 0.01 & 0.0001\\ [0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{RetinaNet}  & Adam & 100 & 0.0001 & 0 & \multirow{2}{*}{8}\\
    & SGD & 300 & 0.01 & 0.0001\\[0.5ex]  
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{YOLOv3}  & Adam & 100 & 0.001 & 0  & \multirow{2}{*}{16}\\
    & SGD & 300 & 0.01 & 0.0005\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{YOLOv4}  & Adam & 100 & 0.001 & 0 & \multirow{2}{*}{8}\\
    & SGD & 300 & 0.01 & 0.0005\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{YOLOv5}  & Adam & 100 & 0.001 & 0 & \multirow{2}{*}{16}\\
    & SGD & 300 & 0.01 & 0.0005\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{YOLOX}  & Adam & 100 & 0.001 & 0 & \multirow{2}{*}{16}\\
    & SGD & 300 & 0.01 & 0.0005\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{YOLOv7}  & Adam & 100 & 0.001 & 0 & \multirow{2}{*}{8}\\
    & SGD & 300 & 0.01 & 0.0005\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{YOLOv8}  & Adam & 100 & 0.001 & 0 & \multirow{2}{*}{32}\\
    & SGD & 300 & 0.01 & 0.0005\\[0.5ex] 

\bottomrule
\end{tabularx}
\label{tab3}
\end{table*}

\subsection{Landmark identification}

In every case, the head CT image, based on its volumetric data, produces a series of potential orbitomeatal line landmark groups $G_i$, where $i\in {1,2,\cdots, N }$. Each of these groups consists of four types of landmarks: the bilateral eyes and the bilateral external auditory canals. Every landmark is equipped with a confidence score and a square bounding box, centered around the (x, y) coordinate.

To ascertain the relative $z$ position for each of the four types of landmarks, we rely on the confidence score of the bounding boxes, as expressed in the following equation:
\begin{equation}
z=\arg \max _i\left(C_i\right)
\end{equation}
Where $C_i$ stands for the bounding box's confidence score pertaining to the orbitomeatal line landmark group indexed as $i$.

Fig. \ref{f2} illustrates that, for each landmark type, the bounding box with the highest confidence score is chosen among the candidate bounding boxes. Its index serves as the relative z-coordinate. This establishes the detection box of the three-dimensional coordinates (x, y, z) as the orbitomeatal line landmark.

% Figure environment removed

\subsection{Head CTs reformatting}
To reformat CT images, we conduct a three-dimensional rotation using Euler angles. These angles comprise a set of three distinct parameters, determining the position of a rigid body rotating around a fixed point. We compute these Euler angles based on landmarks from the bilateral eyes and external auditory canals.

Specifically, we establish a reference coordinate system, with the x-axis perpendicular to the sagittal plane, the y-axis perpendicular to the coronal plane, and the z-axis perpendicular to the axial plane.

In this system, the roll angle signifies the rotation around the x-axis. It represents the angle between the rotation vector on the sagittal plane and the y-axis. The pitch angle indicates rotation around the y-axis, i.e., the angle between the rotation vector on the coronal plane and the x-axis. The yaw angle refers to the rotation around the z-axis, or the angle between the rotation vector on the axial plane and the x-axis, as Fig. \ref{f3} illustrates.

Let the left eye landmark coordinates be $(\mathbf{x}_{Left-eye}, \mathbf{y}_{Left-eye}, \mathbf{z}_{Left-eye})$ and the right eye landmark coordinates be $(\mathbf{x}_{Right-eye}, \mathbf{y}_{Right -eye}, \mathbf{z}_{Right-eye}$). The coordinates for the left external auditory canal landmark are $(\mathbf{x}_{Left-EAC}, \mathbf{y}_{Left-EAC}, \mathbf{z}_{Left-EAC})$, and those for the right external auditory canal landmark are $(\mathbf{x}_{Right-EAC}, \mathbf{y}_{Right -EAC}, \mathbf{z}_{Right-EAC})$. Based on these landmarks, we calculate the roll angle $\boldsymbol{r}$, opting for the smaller angle value from both sides.

The roll angle is calculated as follows:

\begin{equation}
\begin{aligned}
\boldsymbol{r} = 
\min\biggl(&\arctan \left(\frac{\mathbf{z}_{Left-eye }-\mathbf{z}_{Left-EAC }}{\mathbf{y}_{Left-eye }-\mathbf{y}_{Left-EAC }}\right), \\
&\arctan \left(\frac{\mathbf{z}_{Right-eye }-\mathbf{z}_{Right-EAC }}{\mathbf{y}_{Right-eye }-\mathbf{y}_{Right-EAC }}\right)\biggr)
\end{aligned}
\end{equation}


Subsequently, we compute the pitch angle $\boldsymbol{p}$ based on the bilateral orbital and external auditory canal landmarks, again taking the smaller angle value.

The pitch angle is calculated as follows:
\begin{equation}
\begin{aligned}
\boldsymbol{p} = 
\min\biggl(&\arctan \left(\frac{\mathbf{z}_{Left-eye }-\mathbf{z}_{Right-eye }}{\mathbf{x}_{Left-eye }-\mathbf{x}_{Right-eye }}\right), \\
&\arctan \left(\frac{\mathbf{z}_{Left-EAC }-\mathbf{z}_{Right-EAC }}{\mathbf{x}_{Left-EAC  }-\mathbf{x}_{Right-EAC }}\right)\biggr)
\end{aligned}
\end{equation}

Finally, we calculate the yaw angle $\boldsymbol{y}$ based on the bilateral orbital landmarks:
\begin{equation}
\begin{aligned}
\boldsymbol{y} = 
\arctan \left(\frac{\mathbf{y}_{Left-eye }-\mathbf{y}_{Right-eye }}{\mathbf{x}_{Left-eye }-\mathbf{x}_{Right-eye }}\right)
\end{aligned}
\end{equation}

From the above calculations, we derive three rotation angles, based on which we carry out a three-dimensional rotation to obtain the reformatted DICOM format head CT images. This rotation process can be implemented with the SimpleITK library in Python, or alternatively, DICOM files can be read using the pydicom package in Python, with the rotation applied through library Scikit-image.

\subsection{Standardized reconstruction}
We undertake the three-dimensional reconstruction of the reformatted DICOM sequence utilizing the Visualization Toolkit (VTK). This task can be carried out and directly visualized with a medical image analysis platform, 3D Slicer. 

\section{Experiment and Results}\label{sec3}

\subsection{Experiment details}
The experiments were conducted in an environment comprising an Intel i7-13700k 3.4GHz CPU, an Nvidia RTX 4090 GPU with 24GB memory, and 64GB system memory. The software setup included Python 3.7, PyTorch 1.9.3, CUDA 11.6, and CUDNN 8.3.0. We employed SimpleITK 2.2.1 for 3D rotations and VTK 9.1.0 for 3D reconstructions.
\begin{table*}[htbp]
\caption{10 object detection algorithms average precision}
\centering
\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{CCCCCC}
\hline
\cline{1-5}\rule{0pt}{10pt}
    \textbf{Algorithm} & \textbf{Left EAC AP} & \textbf{Right EAC AP} & \textbf{Left Eyes AP} & \textbf{Right Eyes AP} & \textbf{mAP}\\
\midrule
   DETR  & 0.4934 & 0.4939 & 0.4902 & 0.5686	& 0.5115\\

    EfficientDet  & \textbf{0.9881} & \underline{0.7767} & 0.9380 & 0.9211 & \underline{0.9060}\\

    Faster R-CNN & 0.5151 & 0.6435  & 0.5704 &	0.4974 & 0.5566\\
    RetinaNet & 0.7325 & 0.5416  & 0.9724 & 0.8961 & 0.7857\\
    YOLOv3  & 0.7914 & 0.7531	& 0.9429 &	0.9125 & 0.8500\\
    YOLOv4 & 0.7091 & 0.5821	& 0.6487 &	0.5898 & 0.6324\\
    YOLOv5 & 0.5916 & 0.5750	& 0.7415 &	0.6435 & 0.6379\\
    YOLOX & 0.8458 & 0.7460	& \underline{0.9533} &	\underline{0.9376} & 0.8707\\
    YOLOv7& 0.8644 & 0.7109	& 0.9080 &	0.8857 & 0.8423\\
    YOLOv8& \underline{0.9395} & \textbf{0.8651}	& \textbf{0.9754} &	\textbf{0.9366} & \textbf{0.9291}\\
    \bottomrule
\end{tabularx}
\label{tab4}
\end{table*}


\begin{table*}[htbp]
\caption{10 object detection algorithms efficiency}
\centering
\renewcommand{\arraystretch}{1.2}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{CCCCCC}
\hline
\cline{1-5}\rule{0pt}{10pt}
    \textbf{Algorithm} & \textbf{mAP} & \textbf{GFLOPS} & \textbf{Total Parameters} & $\mathbf{PEI}$ & $\mathbf{CPEI}$\\
\midrule
   DETR & 0.5115 & 47.6260 & 36.4700 M & 0.0107 & 0.0140 \\
EfficientDet & \underline{0.9060} & \textbf{4.7560} & \textbf{3.8300 M} & \textbf{0.1905} & \textbf{0.2366} \\
Faster R-CNN & 0.5566 & 308.8130 & 136.7500 M & 0.0018 & 0.0041 \\
RetinaNet & 0.7857 & 105.3140 & 36.3920 M & 0.0075 & 0.0216 \\
YOLOv3 & 0.8500 & 99.3990 & 61.5400 M & 0.0086 & 0.0138 \\
YOLOv4 & 0.6324 & 90.8490 & 63.9540 M & 0.0070 & 0.0099 \\
YOLOv5 & 0.6379 & 73.3510 & 46.6480 M & 0.0087 & 0.0137 \\
YOLOX & 0.8707 & 99.6380 & 54.1500 M & 0.0087 & 0.0161 \\
YOLOv7 & 0.8423 & 67.3050 & 37.2110 M & 0.0125 & 0.0226 \\
YOLOv8 & \textbf{0.9291} & \underline{18.3380} & \underline{11.1370} M & \underline{0.0507} & \underline{0.0834} \\
    \bottomrule
\end{tabularx}
\label{tab5}
\end{table*}

\subsection{Evaluation metrics}
We assess model performance based on factors such as accuracy, convergence, computational efficiency, and memory footprint. For accuracy, we employ metrics like mean Average Precision (mAP), F1 score, Precision, and Recall. The mAP, a prevalent metric in object detection tasks, is the mean of precision scores at varying recall levels, offering a single-figure measure of quality across these levels. We also appraise the Average Precision (AP) for the four distinct landmark classes.

As for convergence, we compute the loss on both the validation and the test sets. Computational efficiency is measured using GLOPS (Giga Floating Point Operations Per Second), denoting the number of billion floating-point operations a system performs each second. With regard to memory usage, we assess the total parameter count of each model. This count indicates the potential memory requirement of the model, as each parameter denotes a value that the model must store and update during training.

To holistically observe model performance concerning accuracy, efficiency, and memory usage, we propose two indexes: the Precision Efficiency Index (PEI) and the Computational Precision Efficiency Index (CPEI):

\begin{equation}
PEI = 
\frac{mAP}{Total\ parameters}
\end{equation}
\begin{equation}
CPEI = 
\frac{mAP}{GFLOPS}
\end{equation}

In relative terms, PEI conveys the mAP per parameter present in the model, while CPEI denotes the mAP achieved per unit of computational effort.


\subsection{Detection model evaluation}



% Figure environment removed

% Figure environment removed

% Figure environment removed

In our experiment, the Adam optimizer outperformed SGD in terms of convergence speed, requiring 100 epochs of training compared to SGD's 300, with training data recorded every 10 epochs. 

From the data shown in Table \ref{tab4}, YOLOv8 exhibited the highest accuracy among the 10 object detection models, achieving a mAP of 0.9291. EfficientDet was another top-performer with an mAP of 0.9060. Each model demonstrated distinct advantages. YOLOv8 was notably robust against class imbalance, securing the highest AP within its class for right external auditory canal, left eye, and right eye. From an efficiency perspective, EfficientDet outshined all models, boasting the highest PEI and CPEI, as depicted in Table \ref{tab5}. This implies that EfficientDet maximizes potential with equivalent computational resources and usage. YOLOv8 ranked second in these aspects, offering an excellent choice for lightweight, high-performance detection. However, robustness remains paramount in clinical practice. Considering both precision and robustness, YOLOv8 emerged as the best choice for this task, with its slight efficiency sacrifice deemed acceptable.

It is worth noting that EfficientDet's impressive accuracy relies heavily on extensive iterative epochs. As shown in Fig. \ref{fig4b}, when trained with SGD, EfficientDet required 150 epochs to reach superior performance, whereas YOLOX, its main competitor, required only 100 epochs. EfficientDet was also observed to be sensitive to the optimizer. After 100 epochs with Adam, EfficientDet's accuracy fell behind the other nine models, with an mAP below 0.3 (Fig. \ref{fig4a}). However, its performance dramatically improved under SGD training. Both observations underscore EfficientDet's slow convergence, which is further demonstrated in Fig. \ref{fig5} and \ref{fig6}, where it was the slowest to converge among the ten models. This underscores the potential of simple models with multiple epochs in such object detection tasks.

Other noteworthy models include YOLOX, YOLOv7, and YOLOv3, which all showed good performance with mAPs around 0.85. Given the simplicity of the YOLO architecture, these three models are also suitable for similar tasks. On the other hand, DETR and Faster R-CNN exhibited lower precision, making them unsuitable for this detection task.

As depicted in Fig. \ref{fig7}, EfficientDet and YOLOv8 demonstrated superior precision across different levels of recall. Particularly when detecting samples with fewer bilateral EAC landmarks, both models managed to uphold precision while increasing recall (Fig. \ref{fig7c} and Fig. \ref{fig7d}).

In terms of F1 scores, YOLOv8 consistently performed well across all four landmark detections. As shown in Fig. \ref{fig8}, it achieved peak F1 scores at a lower threshold, denoting high F1 scores without a stringent threshold. None of the models exhibited consistent performance across thresholds for bilateral EACs, likely due to smaller sample sizes. Conversely, for bilateral eyes where the sample size was larger, YOLOv3, YOLOX, and YOLOv8 maintained high F1 scores across a wide threshold range, reflecting more robust models that perform well irrespective of the precise threshold. These models also demonstrated comparable precision performance for bilateral eyes (Fig. \ref{fig9c} and Fig. \ref{fig9d}).

% Figure environment removed


% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed



Specifically, we observed the AP and F1 score results of different methods when the score threshold was set to 0.5, as shown in Fig. \ref{f11}. Unexpectedly, YOLOv5 exhibits very low F1 scores across all classes, indicating a discrepancy between the model's precision and recall. This could suggest that while the model may be correctly identifying a reasonable number of objects, it could also be missing many objects or marking too many false positives, thereby leading to low F1 scores.

Moreover, there are isolated instances where the F1 score exceeds the AP. This could imply that while the model's precision and recall are balanced at the specific decision threshold used for the F1 score, the model's precision may vary more across all recall levels. This suggests that the model's performance might not be consistently good for different decision thresholds.

\subsection{Qualitative evaluation of Reconstruction}

A trio of experienced radiology specialists individually scrutinized both the non-standardized and standardized reconstruction outcomes, aiming to evaluate the quality of the reconstruction images. The assessment was performed with a five-tier grading scheme (1 - Subpar, 2 – Mediocre, 3 – Average, 4 – Superior, 5 – Excellent). The evaluation criteria encompassed three key elements: structural fidelity, absence of distortion, and consistency in representation across diverse viewpoints. More specifically, the experts considered whether the holistic architecture and shape of the original head were retained in the reconstructed outcomes, whether the form, contours, and features remained identifiable without distortion, and whether the reconstituted outcomes depicted a uniform interpretation of the original head, regardless of the viewpoint. To quantify the distinction between the non-standardized and standardized reconstruction outcomes, a Wilcoxon signed-rank test was employed, with the threshold of significance defined as $P <$ 0.05.

\begin{table}[tbp]
  \centering
  \setlength{\tabcolsep}{8pt}
  \caption{Qualitative evaluation of Reconstruction}
  \label{tab6}
  \begin{tabular}{ccccccc}
    \toprule
    \multirow{2}{*}{Observer} & \multirow{2}{*}{Reconstruction}& \multicolumn{5}{c}{Score}\\
    \cline{3-7}\rule{0pt}{10pt}
    & & 1 & 2 & 3 & 4 & 5\\
    \midrule
   \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{Observer1}  & Non-standardized & 12 & 11 & 9 & 13 & 7 \\
    & Standardized & 4 & 5 & 11 & 14 & 18\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{Observer2}  & Non-standardized & 10 & 10 & 9 & 12 & 11 \\
    & Standardized & 2 & 4 & 10 & 15 & 21\\[0.5ex] 
    \hdashline
    \vspace{0.5ex}\\ [-2ex]
    \multirow{2}{*}{Observer3}  & Non-standardized & 12 & 11 & 6 & 10 & 13 \\
    & Standardized & 2 & 2 & 10 & 17 & 21\\[0.5ex] 
    
    \bottomrule
 \end{tabular}
\end{table}

Table \ref{tab6} shows the score distributions for both non-standardized and standardized reconstruction results. Observers 1, 2, and 3 yielded average scores with associated standard deviations for the non-standardized reconstruction results of 3.4 ± 1.0, 3.5 ± 1.0, and 3.5 ± 1.0, respectively. Meanwhile, the standardized reconstruction results correspondingly elicited scores of 4.0 ± 1.0, 4.1 ± 1.0, and 4.1 ± 1.0, each manifesting a statistically significant discrepancy at $P <$ 0.001. Among the standardized reconstruction outcomes, the quantity of cases that were assigned scores of 3 or higher, thereby being classified as clinically viable, were 43 (82.7\%), 46 (88.5\%), and 48 (92.3\%) for observers 1, 2, and 3, respectively.

\subsection{Discussion}

The standardized three-dimensional reconstruction of head CT scans holds tremendous utility for various post-processing applications. These include tasks such as the segmentation of reconstructed results to isolate structures such as the brain and skull\cite{bb18}, the extraction of specific features or landmarks from the three-dimensional reconstruction—identifying specific anatomical structures or lesions to facilitate further analysis or clinical decision-making\cite{bb19}. Furthermore, they aid in the execution of quantitative measurements on the 3D reconstruction, including determining the volume of a tumor or quantifying the length and angle of certain anatomical structures\cite{bb20}, and embarking on 3D medical image registration\cite{bb21}, which aligns two or more 3D images within a shared spatial domain. The overarching objective of these applications is to bolster the precision of clinical diagnostic outcomes.

Given the comparable workflow and grayscale image format, our automated standardized 3D reconstruction strategy, driven by deep learning, should be readily applicable to other anatomical structures, such as limbs and the chest. This flexibility attests to the versatility of our methodology, although some degree of fine-tuning or transfer learning may be necessary to optimize performance. Furthermore, alternative image reconstruction modalities, such as Magnetic Resonance Imaging (MRI), could potentially be incorporated.

Despite these advancements, our work is subject to certain limitations:

Primarily, while our standardized reconstruction gained favorable subjective outcomes in the qualitative evaluation, indicative of its utility in a clinical setting, these results are somewhat empirical. It is advisable to implement quantitative metrics to assess the loss of detail in the standardized reconstruction induced by interpolation in the reconstruction process. By treating non-standardized reconstruction performed on original CT images as the ground-truth, metrics such as Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Dice Similarity Coefficient (DSC), or Jaccard Index could provide a more nuanced comparison between non-standardized and standardized reconstruction.

Secondarily, to fully gauge the robustness of our approach across varied clinical contexts and wider patient demographics, it is essential to augment the diversity of our dataset. The existing variety in terms of age, pathology, data acquisition equipment, and other variables may not adequately reflect the broader population. This potential limitation could compromise the general applicability of our model across diverse patient populations and clinical settings. Consequently, future endeavors should strive to corroborate the proposed approach with more heterogenous datasets.


\section{Conclusion}\label{sec4}

In this paper, we presented a robust and efficient automated method for standardized three-dimensional reconstruction of head CT images using a deep learning-based object detection algorithm. Our solution seamlessly identifies and assesses landmarks for image reformatting, reducing inconsistencies and time demands associated with manual processes.
Through a detailed analysis of ten object detection algorithms, we identified YOLOv8 as the most fitting choice for our task, based on reliability and efficiency.
Standardized reconstruction results further confirmed our method's clinical relevance and validity. Our innovative fusion of deep learning and radiology illuminated through this work not only holds promise for boosted diagnostic efficiency but also underscores the transformative potential of AI-driven healthcare solutions.




% \section*{Acknowledgments}
% This research has received no external funding.

%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}
 
 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%


% \begin{thebibliography}{1}
\bibliographystyle{IEEEtran}
\bibliography{reference.bib}
% \bibitem{ref1}
% {\it{Mathematics Into Type}}. American Mathematical Society. [Online]. Available: https://www.ams.org/arc/styleguide/mit-2.pdf

% \bibitem{ref2}
% T. W. Chaundy, P. R. Barrett and C. Batey, {\it{The Printing of Mathematics}}. London, U.K., Oxford Univ. Press, 1954.

% \bibitem{ref3}
% F. Mittelbach and M. Goossens, {\it{The \LaTeX Companion}}, 2nd ed. Boston, MA, USA: Pearson, 2004.

% \bibitem{ref4}
% G. Gr\"atzer, {\it{More Math Into LaTeX}}, New York, NY, USA: Springer, 2007.

% \bibitem{ref5}M. Letourneau and J. W. Sharp, {\it{AMS-StyleGuide-online.pdf,}} American Mathematical Society, Providence, RI, USA, [Online]. Available: http://www.ams.org/arc/styleguide/index.html

% \bibitem{ref6}
% H. Sira-Ramirez, ``On the sliding mode control of nonlinear systems,'' \textit{Syst. Control Lett.}, vol. 19, pp. 303--312, 1992.

% \bibitem{ref7}
% A. Levant, ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proc. 45th IEEE Conf. Decis.
% Control}, San Diego, CA, USA, 2006, pp. 5585--5590. DOI: 10.1109/CDC.2006.377165.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          

% \bibitem{ref8}
% M. Fliess, C. Join, and H. Sira-Ramirez, ``Non-linear estimation is easy,'' \textit{Int. J. Model., Ident. Control}, vol. 4, no. 1, pp. 12--27, 2008.

% \bibitem{ref9}
% R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez, ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proc. Amer. Control Conf.}, Chicago, IL, USA,
% 2000, pp. 2245--2249.

% \end{thebibliography}



\end{document}


