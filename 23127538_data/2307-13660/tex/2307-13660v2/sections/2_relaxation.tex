\section{Relaxing the Gromov--Hausdorff distance}
\label{relaxation}

\subsection{Matrix reformulation}
Let $X = \{x_1, \ldots, x_n\}$ and $Y = \{y_1, \ldots, y_m\}$ be metric spaces whose finite cardinalities satisfy $n \geq m > 1$, and let $\XX \in \R^{n\times n}$ and $\YY \in \R^{m\times m}$ denote their corresponding distance matrices. Recall that the Gromov--Hausdorff distance between $X$ and $Y$ can be formulated as a combinatorial minimization over the bi-directional mapping pairs $(f,g)$
$$\dGH(X, Y) = \frac{1}{2}\min_{\substack{f:X\to Y,\\ g:Y\to X}} \dis\left(\left\{\left(x, f(x)\right): x \in X\right\} \cup \left\{\left(g(y), y\right): y \in Y\right\}\right),\quad\cite{kalton1999distances}$$ where the distortion of some relation $R \subseteq X \times Y$ is defined as the largest absolute difference in distances incurred by it: $$\dis R \defeq \max_{\subalign{(x, y),(x', y') \in R}} \left|d_X(x, x') - d_Y(y, y')\right|.$$

For some $f:X\to Y$, consider its ``one-hot encoded'' matrix representation $\FF \in \{0, 1\}^{n \times m}$ s.t. $F_{ij} = \begin{cases}
    1, & f(x_i) = y_j\\
    0, & \text{otherwise}
\end{cases}$ and note that $\left(\FF\YY\FF^T\right)_{ij} = d_Y\left(f(x_i), f(x_j)\right)$. It follows that $$\max_{x,x' \in X} \Big|d_X(x, x') - d_Y\left(f(x), f(x')\right)\Big| = \big\|\XX - \FF\YY\FF^T\big\|_\infty.$$
From the analogous construction of $\GG \in \{0, 1\}^{m \times n}$ for an arbitrary $g: Y \to X$, $$\max_{y,y' \in Y} \left|d_X\left(g(y), g(y')\right) - d_Y(y, y')\right| = \big\|\YY - \GG\XX\GG^T\big\|_\infty.$$
Because $(\XX\GG^T)_{ij} = d_X\left(g(y_j), x_i\right)$ and $(\FF\YY)_{ij} = d_Y\left(f(x_i), y_j\right)$, $$\max_{x \in X, y \in Y} \left|d_X\left(x, g(y)\right) - d_Y\left(f(x), y\right)\right| = \big\|\XX\GG^T - \FF\YY\big\|_\infty.$$

Denote $R = R(f, g) \defeq \left\{\left(x, f(x)\right): x \in X\right\} \cup \left\{\left(g(y), y\right): y \in Y\right\}$. Any pair $(x, y), (x', y') \in R$ satisfies
\begin{align*}
\left|d_X(x, x') - d_Y(y, y')\right| \in \bigg\{&\left|d_X(x, x') - d_Y\left(f(x), f(x')\right)\right|,\\
&\left|d_X\left(g(y), g(y')\right) - d_Y(y, y')\right|,\\
&\left|d_X\left(x, g(y)\right) - d_Y\left(f(x), y\right)\right|,\\
&\left|d_X\left(x', g(y')\right) - d_Y\left(f(x'), y'\right)\right|\bigg\},
\end{align*}
% Arranging the three matrices inside $\|\cdot\|_\infty$ as blocks of an $(n+m)\times(n+m)$ matrix gives
and therefore
\begin{align*}
    \dis R &= \max\bigg\{\begin{aligned}[t]
        &\max_{x,x' \in X} \left|d_X(x, x') - d_Y\left(f(x), f(x')\right)\right|, \\ &\max_{y,y' \in Y} \left|d_X\left(g(y), g(y')\right) - d_Y(y, y')\right|,\\ &\max_{x \in X, y \in Y} \left|d_X\left(x, g(y)\right) - d_Y\left(f(x), y\right)\right|\bigg\}
    \end{aligned}  \\
    &= \left\|\begin{bmatrix}\XX - \FF\YY\FF^T & \XX\GG^T - \FF\YY \\ \GG\XX - \YY\FF^T & \YY - \GG\XX\GG^T\end{bmatrix}\right\|_\infty \\
    &= \big\|\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T\big\|_\infty,
\end{align*}
where $\RR \defeq \begin{bmatrix}\FF & \\ &\GG\end{bmatrix}$ is a matrix representation of $R$ and $\VV \defeq \begin{bmatrix}\XX & \\ &\YY\end{bmatrix}$, $\UU \defeq \begin{bmatrix}\YY & \\ &\XX\end{bmatrix}$, $\WW \defeq \begin{bmatrix}& \YY \\ \XX&\end{bmatrix}$. We note that the presence of redundant block $\GG\XX-\YY\FF^T$ in the distance difference matrix $\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T$ is motivated by symmetry of the latter.

By construction, $\RR \in \{0, 1\}^{(n+m)\times(n+m)}$ is row-stochastic and has $m\times m$ and $n\times n$ blocks of zeros in the upper right and lower left, respectively. Let $\RRR \subset \{0, 1\}^{(n+m)\times(n+m)}$ denote the set of all such matrices, which is in a 1-to-1 correspondence with the mapping pairs $(X\to Y)\times(Y\to X)$. We will write $\dis \RR \defeq \left\|\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T\right\|_\infty$ to denote the matrix-based formulation of distortion, and assume that the distinction between $\dis:\RRR\to \R$ and $\dis:\mathscr{P}(X\times Y)\to\R$ is clear from the context. Then an equivalent formulation of the Gromov--Hausdorff distance can be given as
\begin{equation*}
    \label{eqn:matrix_reformulation}
    \dGH(X, Y) = \frac{1}{2}\min_{\RR \in \RRR} \dis \RR.
    \tag{$\star$}
\end{equation*}

\subsection{Relaxing the objective}
The $\infty$-norm in $\dis \RR$ deprives the (otherwise quadratic) objective of (\ref{eqn:matrix_reformulation}) of its differentiability. A standard trope in smooth relaxations of the maximum function is to involve the sum of exponents of its arguments. It turns out that, using a sufficiently large base for exponentiation, we can construct a smooth relaxation of (\ref{eqn:matrix_reformulation}) that is minimized only by solutions to (\ref{eqn:matrix_reformulation}).

Let $\Delta \defeq \left\{\left|d_X(x, x') - d_Y(y, y')\right|: x, x' \in X, y, y' \in Y\right\}$, and note that $\dis \RR \in \Delta$ for any $\RR \in \RRR$.
% \begin{definition}
The \textit{distortion gap} between $X$ and $Y$ %for a pair of finite metric spaces $X, Y$
is then defined as $$\rho = \rho(X, Y) \defeq \min \left\{|\delta - \delta'|: \delta, \delta' \in \Delta, \delta \neq \delta'\right\}.$$ Trivially, $\dis \RR \neq \dis \RR'$ for some $\RR, \RR' \in \RRR$ implies that $|\dis \RR - \dis \RR'| \geq \rho$, which provides justification for the name.
% \end{definition}

\begin{restatable}{theorem}{cthreshold}
    \label{thm:c_threshold}
    Let $c \geq \left(\frac{(n+m)^2-n-m}{2}\right)^{1/\rho}$. Then
    \begin{align*}
    \argmin_{\RR\in\RRR}\left\|c^{\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T} + c^{\RR\UU\RR^T - \VV + \WW^T\RR^T - \RR\WW}\right\|_1 \subseteq \argmin_{\RR\in\RRR}\dis\RR,
    \end{align*}
    where the exponentials are taken entry-wise.

\end{restatable}


Theorem \ref{thm:c_threshold} is based on the idea that a decrease in $\left\|\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T\right\|_\infty$, the largest magnitude in the distance difference matrix, must decrease the above 1-norm relaxation even when it leads to increasing the magnitudes of all other distance differences from zero to the new maximum. In practice, however, suboptimal choices of $\RR \in \RRR$ do not tend to align the distances in $X$ and $Y$ better than solutions to (\ref{eqn:matrix_reformulation}), and therefore much smaller values of $c$ than $\left(\frac{(n+m)^2-n-m}{2}\right)^{1/\rho}$ can satisfy the statement of Theorem \ref{thm:c_threshold}.

Note that $c$ and $c^{-1}$ behave identically in the 1-norm relaxation, which means that both $c \in (0, 1]$ and $c \in [1, \infty)$ can be considered for analogous results. For simplicity, we focus on the latter option and assume $c \geq 1$ throughout this work.

Recall that the two parts of the distance difference matrix $\VV - \RR\UU\RR^T$ and $\RR\WW - \WW^T\RR^T$ have complementary block sparsity: the former contains $n\times m$ zeros in the upper right and $m\times n$ zeros in the lower left, while the latter has $n\times n$ zeros in the upper left and $m\times m$ zeros in the lower right. It follows that
\begin{align*}
    &\left\|c^{\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T} + c^{\RR\UU\RR^T - \VV + \WW^T\RR^T - \RR\WW}\right\|_1 \\ &\hspace{2cm}= \left\|c^{\VV - \RR\UU\RR^T} + c^{\RR\WW - \WW^T\RR^T} - c^\mathbf{0} + c^{\RR\UU\RR^T - \VV} + c^{\WW^T\RR^T - \RR\WW} - c^\mathbf{0}\right\|_1 \\ &\hspace{2cm}= \left\|c^{\VV - \RR\UU\RR^T}\right\|_1 + \left\|c^{\RR\UU\RR^T - \VV}\right\|_1 + \left\|c^{\RR\WW - \WW^T\RR^T}\right\|_1 + \left\|c^{\WW^T\RR^T - \RR\WW}\right\|_1 - 2(n+m)^2.
\end{align*}

Leveraging the structure of $\RR$ and subsequently applying the trace trick gives
\begin{align*}
&\left\|c^{\VV - \RR\UU\RR^T}\right\|_1 = \left\langle c^{\VV}, c^{-\RR\UU\RR^T}\right\rangle = \left\langle c^{\VV}, \RR c^{-\UU}\RR^T\right\rangle = \left\langle \RR , c^{\VV}\RR c^{-\UU}\right\rangle
\end{align*}
and
\begin{align*}
&\left\|c^{\RR\WW - \WW^T\RR^T}\right\|_1 = \left\langle c^{\RR\WW}, c^{-\WW^T\RR^T}\right\rangle = \left\langle \RR c^{\WW},
\big(\RR c^{-\WW}\big)^T\right\rangle = \left\langle \RR , \big(c^{\WW}\RR c^{-\WW}\big)^T\right\rangle,
\end{align*}
as well as $\left\|c^{\RR\UU\RR^T - \VV}\right\|_1 = \Big\langle \RR, c^{-\VV}\RR c^{\UU}\Big\rangle$ and $\left\|c^{\WW^T\RR^T - \RR\WW}\right\|_1 = \left\langle \RR , \left(c^{-\WW}\RR c^{\WW}\right)^T\right\rangle$. Combining the four equations casts the 1-norm relaxation of (\ref{eqn:matrix_reformulation}) as a quadratic minimization
%with optimality guarantees on its solutions for large enough $c$:
\begin{align*}
    \label{eqn:quadratic_objective}
    \min_{\RR \in \RRR} \obj(\RR) \defeq \left\langle \RR, c^{\VV}\RR c^{-\UU} + c^{-\VV}\RR c^{\UU} + \big(c^{\WW}\RR c^{-\WW} + c^{-\WW}\RR c^{\WW}\big)^T\right\rangle.
    \tag{$\star\star$}
\end{align*}


\subsection{Relaxing the domain}
In order to enable first-order methods for solving (\ref{eqn:quadratic_objective}), its objective $\obj$ needs to be considered over a continuous domain. A common approach in combinatorial optimization is to relax the discrete domain to its convex hull, employ a gradient-based algorithm on the convex region, and project the resulting solution back onto the original domain.
% The quadratic formulation enables first-order methods to find local minima if $\obj$ is extended to a continuous domain. A common practice is to relax a discrete domain to its convex hull, and to project the found solution back according to some choice of matrix distance.
The convex hull of $\RRR$, henceworth denoted as $\SSS$, is the set of all $(n+m)\times(n+m)$ row-stochastic matrices with $m\times m$ and $n\times n$ blocks of zeros in the upper right and lower left, respectively.
% the so-called $\SSS$ comprised of $(n+m)\times(n+m)$ row-stochastic matrices with, referred to as the \textit{bi-mapping polytope} .
This is a direct consequence of generalizing the Birkhoff-von Neumann theorem to the row-stochastic matrices, which was done e.g. in \cite{gubin2008subgraph} and \cite{cao2022centrosymmetric}. Because the points of $\SSS$ represent bi-directional pairs of generalized (or ``soft'') mappings, we refer to it as the \textit{bi-mapping polytope}.

Importantly, projecting a solution to the continuous relaxation 
% \begin{equation*}
%     \label{eqn:continuous_optimization}
%     \min_{\SS \in \SSS} \obj(\SS) \defeq \left\langle \SS, c^{\VV}\SS c^{-\UU} + c^{-\VV}\SS c^{\UU} + \left(c^{\WW}\SS c^{-\WW} + c^{-\WW}\SS c^{\WW}\right)^T\right\rangle
%     \tag{$\star\star$$\star$}
% \end{equation*}
$\min_{\SS \in \SSS}\obj(\SS)$
back onto the vertices $\RRR$ always yields a solution to (\ref{eqn:quadratic_objective}). Note that the faces of $\SSS$ can be characterized similarly to those of the $(n+m)$-th Birkhoff polytope, see e.g. \cite{paffenholz2013faces}. Specifically, every face of $\SSS$ corresponds to a (possibly empty) set of forbidden assignments between the points of $X$ and $Y$ and is comprised by the convex combinations of all the compliant mapping pairs. We write it formally as
\begin{restatable}{lemma}{polytopegeometry}
    \label{lem:polytope_geometry}
    Any face $\Phi$ of the bi-mapping polytope $\SSS$ is characterized by an index set $(\mathcal{I}, \mathcal{J}) \subset \{1, \ldots, n+m\}\times\{1,\ldots,n+m\}$ s.t. $\Phi = \left\{\SS \in \SSS: S_{ij} = 0 \quad\forall (i,j)\in (\mathcal{I}, \mathcal{J})\right\}$.
\end{restatable}
Using the above result, we can show that any face of $\SSS$ containing a solution on its interior must be a part of the solution set:
\begin{restatable}{theorem}{solutiononinterior}
\label{thm:solution_on_interior}
Let $\SS^* \in \argmin_{\SS \in \SSS}\obj(\SS)$ and $\Phi$ denote the face of $\SSS$ s.t. $\SS^* \in \Phi \setminus \partial\Phi$. Then $$\Phi \subseteq \argmin_{\SS \in \SSS}\obj(\SS).$$
\end{restatable}

In particular, Theorem \ref{thm:solution_on_interior} implies that $\obj$ attains its minimum on the vertices of $\SSS$.
%(and, as a consequence, that relaxing $\RRR$ to its convex hull $\SSS$ does not change the minimum of $\obj$).
In order to guarantee that the projection of $\argmin_{\SS \in \SSS} \obj(\SS)$ onto $\RRR$ is included in $\argmin_{\RR \in \RRR} \obj(\RR)$, it remains to show that the nearest vertices to any $\SS \in\SSS$ belong to the same faces as $\SS$.
%for any face $\Phi$ and point $\SS \in \Phi$, all the nearest vertices to $\SS$ are in $\Phi$.
\begin{restatable}{theorem}{voronoiface}
    \label{thm:voronoi_face}
    Let $\Phi$ be a face of $\SSS$. For any $\SS \in \Phi$, $$\argmin_{\RR \in \RRR}\|\SS - \RR\|_2 \subseteq \Phi.$$
\end{restatable}

Searching over the bi-mapping polytope $\SSS$ is a key difference between our approach and other relaxations of the Gromov--Hausdorff distance constrained to the Birkhoff polytope. The resulting continuous relaxation %(\ref{eqn:continuous_optimization})
\begin{equation*}
    \label{eqn:continuous_optimization}
    \min_{\SS \in \SSS} \obj(\SS) \defeq \left\langle \SS, c^{\VV}\SS c^{-\UU} + c^{-\VV}\SS c^{\UU} + \big(c^{\WW}\SS c^{-\WW} + c^{-\WW}\SS c^{\WW}\big)^T\right\rangle
    \tag{$\star\star$$\star$}
\end{equation*}
%along with the optimality guarantees for its solutions
whose solutions provably deliver the Gromov--Hausdorff distance
is the main theoretical result of this work.

%which can be proved as a modification to the Birkhoff-von Neumann theorem for row-stochastic matrices.
% \begin{theorem}
%     \label{thm:convex_hull}
%     The convex hull of $\RRR$ is $\SSS$.
% \end{theorem}
'