\section{Solving the relaxation}
\label{minimization}
\subsection{The Frank--Wolfe algorithm}
(\ref{eqn:continuous_optimization}) is an indefinite %(see Section \ref{convexity})
quadratic minimization with affine constraints and a Lipschitz gradient $$\nabla\obj(\SS) = 2\left(c^{\UU}\SS c^{-\VV} + c^{-\UU}\SS c^{\VV} + \big(c^{\WW}\SS c^{-\WW} + c^{-\WW}\SS c^{\WW}\big)^T\right).$$ While finding its global minimum remains an NP-hard problem, approximate solutions can be efficiently obtained by the Frank--Wolfe algorithm \cite{frank1956algorithm}, also known as conditional gradient descent. The iterative algorithm starts at some $\SS_0 \in \SSS$. At every iteration, it finds the descent direction as a point in $\SSS$ that minimizes the cosine similarity with the gradient at the current point $\SS_i$,
$$\DD_i \in \argmin_{\SS \in \SSS}\left\langle \SS, \nabla\obj(\SS_i)\right\rangle.$$
The descent direction $\DD_i$ chosen by the algorithm is always a vertex of $\SSS$ (note that the minimum must be attained at some vertex due to the linearity of the problem).
The algorithm then finds a point on the line segment $\overline{\SS_i\DD_i}$ that minimizes $\obj$, $$\SS_{i+1} \in \argmin_{\gamma \in [0, 1]} \obj\left(\gamma\SS_i + (1-\gamma)\DD_i\right),$$
which concludes the $i$-th iteration. The algorithm's convergence is measured as the \textit{Frank--Wolfe gap} $\left\langle \SS_i - \DD_i, \nabla \obj(\SS_i) \right\rangle \geq 0$, which is zero if and only if $\SS_i$ is a stationary point. 
The algorithm terminates when the Frank--Wolfe gap becomes sufficiently small (or after reaching the iteration limit). It takes the Frank--Wolfe algorithm $O(\epsilon^{-2})$ iterations to approach a stationary point with the gap of $\epsilon$ \cite{lacoste2016convergence}, each iteration requiring $O(n^3)$ time.
% \begin{theorem}
%     \label{thm:time_complexity}
%     One Frank--Wolfe iteration for solving (\ref{eqn:continuous_optimization}) takes $O(n^3)$ time.
% \end{theorem}

\subsection{Stationary points}
The structure of $\SSS$ helps characterize the stationary points of (\ref{eqn:continuous_optimization}). The following result suggests that the trend of saddle point prevalence in high-dimensional non-convex optimization \cite{dauphin2014identifying} is unlikely to manifest in (\ref{eqn:continuous_optimization}) for a broad class of metric spaces (e.g. random metric spaces from \cite{vershik2004random}).
\begin{restatable}{theorem}{oneminimizer}
    \label{thm:one_minimizer}
    Let the distances stored in $\XX$ be realized by continuous random variables $\mathrm{D}_1, \ldots, \mathrm{D}_{\frac{n(n-1)}{2}}$ such that any $\mathrm{D}_i$ restricted to any permissible realization of the rest of the variables $\left\{\mathrm{D}_j=d_j:j\neq i\right\}$ has support of non-zero measure. If $c > 1$, then the linear minimization from the first-order necessary optimality condition for any $\SS^*\in\SSS$ almost surely has a single solution:
    $$\mathbb{P}_\XX\left[\Big|\argmin_{\SS\in\SSS} \left\langle \SS, \nabla\obj(\SS^*) \right\rangle\Big| > 1\right] = 0.$$
    %and any permissible realization of all but one of them $\{X_j=x_j:j\neq i\}$ the support of $X_i|x_1,\ldots,x_{i-1},x_{i+1},\ldots,x_\frac{n(n-1)}{2}$ has positive measure.
\end{restatable}

% $\big|\argmin_{\SS\in\SSS} \langle \SS, \nabla\obj(\SS^*) \rangle$ combined with the first-order condition $\SS^* \in \argmin$ immediately fullfills the second-order condition as it pertains to the points in $\argmin _{\SS\in\SSS} \langle \SS, \nabla\obj(\SS^*) \rangle\setminus \{\SS^*\} = \emptyset.$

For any $\SS^* \in \SSS$, the probability distribution of the distances in $\XX$ induces a random matrix $\nabla\obj(\SS^*)$ and a Bernoulli variable representing whether $\SS^*$ is a stationary point of (\ref{eqn:continuous_optimization}). Under the assumption of independence of these two random objects, Theorem \ref{thm:one_minimizer} entails that any stationary point $\SS^*$ almost surely satisfies its second-order sufficient optimality condition as the latter ends up being imposed on the empty set $\argmin _{\SS\in\SSS} \left\langle \SS, \nabla\obj(\SS^*) \right\rangle\setminus \left\{\SS^*\right\}.$ In practice, it means that the approximate solutions to (\ref{eqn:continuous_optimization}) recovered by the Frank--Wolfe algorithm are likely to be points of local minima and not saddle points.

\subsection{Extent of non-convexity}
\label{convexity}
The Hessian of $\obj$ is a constant $(n+m)^2\times(n+m)^2$ matrix $$\HH \defeq \nabla^2\obj(\cdot) = 2\left(c^{\UU}\otimes c^{-\VV} + c^{-\UU}\otimes c^{\VV} + \left(c^{\WW}\otimes c^{-\WW^T} + c^{-\WW}\otimes c^{\WW^T}\right)\KK\right),$$ where $\otimes$ is the Kronecker product and $\KK \in \{0, 1\}^{(n+m)^2\times(n+m)^2}$ denotes the commutation matrix. %Let $\Lambda \subset \R$ denote the multi-set of its $(n+m)^2$ eigenvalues.
One approach to navigating the non-convexity of $\obj$ is to consider the space spanned by the eigenvectors corresponding to negative eigenvalues of $\HH$. However, doing so has $O(n^6)$-time and $O(n^4)$-memory complexity and is impractical for most cases. As a cheaper substitute, we assess the extent of non-convexity of $\obj$ using the normalized nuclear norm-induced distance from $\HH$ to the set of positive semidefinite matrices $\mathcal{M}^+$: $$\nconv(\obj) \defeq \inf_{\mathbf{M} \in \mathcal{M}^+} \frac{\|\HH - \mathbf{M}\|_*}{\|\HH\|_*} = \frac{\lambda^-}{\lambda^+ + \lambda^-} \in [0, 1],$$ where $\|\cdot\|_*$ denotes the nuclear norm and $\lambda^-$ and $\lambda^+$ are the total magnitudes of respectively negative and positive eigenvalues of $\HH$ \cite{davydov2019searching}. In particular, this distance is equal to 0 (or 1) if and only if the function is convex (or concave).

Notice that $\lambda^+ - \lambda^- = \tr\HH = 8(n+m)^2$ due to the zero main diagonals of $\VV$ and $\UU$ and
\begin{align*}
    \tr\left(\left(c^\WW \otimes c^{-\WW^T}\right)\KK\right) = \left\langle \KK,  c^\WW \otimes c^{-\WW^T} \right\rangle 
    = \sum_{i,j=1}^{n+m} c^{W_{ij}}c^{-W_{ij}}
    = (n+m)^2
    = \tr\left(\left(c^{-\WW} \otimes c^{\WW^T}\right)\KK\right),
\end{align*}
and therefore $$\nconv(\obj) = \frac{\lambda^+ - 8(n+m)^2}{2\lambda^+ - 8(n+m)^2}\in \left[0, \frac{1}{2}\right).$$
%  = 1 - \frac{1}{2 - \frac{8(n+m)^2}{\lambda^+}} 
Let $\lmax > 0$ denote the dominant eigenvalue of $\HH$ as per the Perron--Frobenius theorem. In the degenerate case of $c=1$ this is the only non-zero eigenvalue of the rank-1 Hessian $\HH=8\left(\mathbf{1}\mathbf{1}^T\right)$, which makes $\obj$ convex (though uninformative). %so $\lambda^+ = 8(n+m)^2=\lmax$.
%the problem becomes convex because its rank-$1$ Hessian with all entries equaling 8 rhas $\lambda^-=0$ and $\lambda^+=8(n+m)^2$.
On the other hand, the Courant--Fischer theorem yields $$\lmax \geq \frac{\mathbf{1}^T\HH\mathbf{1}}{\mathbf{1}^T\mathbf{1}} = \frac{\|\HH\|_1}{(n+m)^2} \xrightarrow[c \to \infty]{} \infty,$$
and
$\lambda^+ \geq \lmax$
then implies
$\nconv(\obj) \xrightarrow[c \to \infty]{} \frac{1}{2}.$
%$\lambda^+ \geq \lmax \geq \frac{\|\HH\|_1}{(n+m)^2} \to \infty$ as $c \to \infty$, and thus $\lim_{c \to \infty} \nconv = \frac{1}{2}$.
Therefore, the landscape of (\ref{eqn:continuous_optimization}) starts flat at $c=1$ and becomes increasingly non-convex as $c$ grows. The following result provides a tractable (specifically, $O(n^2)$-time and -space) bound on its non-convexity based on the value of $c$.
%to $(n^2-n)^{\frac{1}{\epsilon}}$, the largest value of interest in the context of finding the GH distance.


\begin{restatable}{theorem}{nonconvexity}
    \label{thm:nonconvexity}
    Let $\alpha \in \left[0, \frac{1}{2}\right)$ and $c \geq 1$ satisfy 
    $$\frac{2\alpha}{\frac{1}{2}-\alpha} = \frac{\sqrt{16(n+m)^4 + \left(c^{\dmax} + c^{-\dmax} - 2\right)p_{\max} - \frac{16}{(n+m)^4}\left\|c^{\WW}\right\|_1^2\left\|c^{-\WW}\right\|_1^2}}{n+m},$$
    %$$1-2\alpha = \frac{n+m}{\sqrt{16(n+m)^4 + !!p_{\max}(2\cosh(\dmax \ln c) - 2) - \frac{16}{(n+m)^2}\|c^\WW\|_1^2\|c^{-\WW}\|_1^2} + n+m},$$
    where $\dmax \defeq \max\{\diam X, \diam Y\}$ % = \|\WW\|_\infty$
    and $p_{\max} \defeq \left(\frac{\left(2\sqrt{2} + 4\right)(n+m)^2\sqrt{(n+m)^2\|\WW\|_2^2 - \|\WW\|_1^2}}{\dmax}+6\right)$. Then $\nconv(\obj) \leq \alpha$.% \quad \forall c \in [1, c_*].$$
\end{restatable}

% Negative eigenvalues correspond to dimensions in which $u$ is concave, so everywhere is a saddle point. So that 

% \subsection{Warm starts}
% The Frank--Wolfe algorithm converges to a stationary point that is fully determined by its starting point $\SS_0$. This induces a view of $\SSS$ as partitioned into the \textit{basins of attraction} %(see e.g. \cite{traonmilin2020basins, asenjo2013visualizing, tsang2018basin})
% of the stationary points of $\obj$. It follows that the chance of recovering a (global) solution to (\ref{eqn:continuous_optimization}) from a randomly sampled $\SS_0 \in \SSS$ is proportional to the total volume of the solution set's basins of attraction. Intuitively, this volume is expected to decrease as the value of $c$ and therefore $\nconv(\obj)$ increases. This presents a trade-off between the probability of a stationary point recovered by the Frank--Wolfe algorithm being a solution to (\ref{eqn:continuous_optimization}) and the likelihood of such a solution delivering the GH distance. To address this issue, we propose solving a sequence of (\ref{eqn:continuous_optimization}) for the increasing values $c_1 < c_2 < \ldots$, using each approximate solution produced by the Frank--Wolfe algorithm as a warm start in the subsequent minimization. The idea is illustrated !!!!
% This approach is based on the assumption that for a sufficiently small ratio $\frac{c_{k+1}}{c_k}$ a solution to (\ref{eqn:continuous_optimization}) for $c_k$ is more likely to be in the basin of attraction of some solution for $c_{k+1}$ than a randomly chosen point in $\SSS$. We compare its efficiency against solving the minimization for a single value of $c$ in the following section.


% Warm starts. Want upper bound of $c$ as a function of convexity.
% Notice that lambda- is bounded by ...sum lambda squared - lambda max squared $\|\HH\|_2^2 - \|\HH\|_1^2$. (*)
% It follows then that $conv = 1$ for $c = 1$ and $conv â€”> \frac{1}{2}$ for $c \to \inf$. 

% One way to do it is to use (*), but this will require $O(n^4)$ enumerations of the Hessian. Instead, we rely on the following result allowing for constructing an upper bound for $c$ in $O(n^2)$.

% We can express the relationship between $c$ and the degree of convexity as $$$$
% This allows for deriving $c$ for the desired degree of convexity of the landscape, and therefore for constructing a progression of minimization problems each using the previous output as a starting point. Additional bounds on $\lambda_{\max}$ and $\sum{\lambda_i^2}$ to avoid $O(n^4)$ enumerations of the Hessian are discussed in next section.


% $$\lambda_{\max} \geq \frac{8}{(n+m)^2}\sum c**\mathbf{C}\sum c**-\mathbf{C} \geq \frac{8}{(n+m)^2}$$ (the last transition is by Karamata/Jensen inequality: $\sum_1^n c**s_i \geq n\sum c**\frac{s_i}{n}$)

% Remark [IN THE PROOF!]: a tighter $O(n^2)$-time bound can be given for $\lambda_{\max}$ as $\sum\HH = 8\sumc^\mathbf{C}\sumc^{-\mathbf{C}}$. However, this would prevent from analytically solving for the upper bound of $c$ as a function of desired convexity, as the resulting inequality will be a polynomial of $O(n^4)$ degree. It would still be possible to solve using e.g. Newthon's method.

% SAME AS ABOVE REMARK. If using first LB for $\lambda_{\max}$, Newthon's method because of many power of $c$ in the equation. If using second LB, can solve directly.