\section{Numerical experiments}
\label{numerical}
We demonstrate the method's performance using our implementation from a Python package \texttt{dgh}.
Given a choice of $c$ and a budget of Frank--Wolfe iterations, it starts from a random $\SS_0 \in \SSS$ and iterates until the Frank--Wolfe gap is at most $10^{-8}$ (or there are no iterations left), after which the last $\SS_i$ is projected to the nearest $\RR \in \RRR$. The cycle repeats until the budget is depleted, after which the smallest found $\frac{1}{2}\dis \RR$ is returned as an estimate and upper bound of the Gromov--Hausdoff distance (we note that this approach is easily parallelizable as there is no interdependency between the random restarts).
In a bid to save on redundant computations, \texttt{dgh} also compares every new best $\frac{1}{2}\dis \RR$ against the trivial lower bound $$\dGH(X, Y) \geq \frac{1}{2}\max\left\{\left|\diam X - \diam Y\right|, \left|\rad X - \rad Y\right|\right\}\quad\cite{memoli2012some}$$ and terminates whenever they match, which means that the Gromov--Hausdorff distance was found exactly. Prior to the computations, \texttt{dgh} normalizes all distances in the input metric spaces so that $\max\{\diam X, \diam Y\} = 1$ to avoid floating-point arithmetic overflow, and scales the resulting $\dis\RR$ back afterwards. %Every time a smaller $\dis\RR$ is obtained, \texttt{dgh} compares it to the trivial lower bound of the Gromov--Hausdorff distance $$\dGH(X, Y) \geq \max\left\{\right\}$$ and terminates if they match
%--- note that this is equivalent to solving (\ref{eqn:continuous_optimization}) for $c^{\dmax}$ and does not affect the optimization landscape for the GH distance.

In the following, we describe our numerical experiments on synthetic and model metric spaces. All computations were performed on a standard 2016 Intel i7-7500U processor.
% !!!!
% beside 4.3, which due to the large volume of computation was performed on the supercomputer Falcon 

% We implement our approach to computing the GH distance as a Python package \texttt{dgh}. Given $c > 1$ and the budget of Frank--Wolfe iterations, our implementation solves a sequence of (\ref{eqn:continuous_optimization}) for $c_1 = c$ and $c_{k+1} = 1 + 10(c_k - 1)$ using the Frank--Wolfe algorithm with warm starts. The starting point for the first minimization in a sequence is selected from $\SSS$ uniformly at random. For every $c_k$, the minimization stops when the Frank--Wolfe gap gets below $10^{-8}$ or when no iterations are left in the budget. Once $c_{k+1}$ exceeds $10^8$, the (approximate) solution for $c_k$ is stored and the sequence is restarted until the iteration budget is depleted. The final solution is selected for the smallest distortion of its projection onto $\RRR$ which delivers an upper bound of the GH distance. All distances in the input metric spaces are scaled by $\dmax$ (so that $\max\{\diam X, \diam Y\} = 1$) prior to the computations to avoid floating-point arithmetic overflow --- note that this is equivalent to solving (\ref{eqn:continuous_optimization}) for $c^{\dmax}$ and does not affect the optimization landscape for the GH distance.

% Next, we describe numerical experiments on synthetic, real-world, and theoretical metric spaces demonstrating the performance and applicability of our approach.

\subsection{Synthetic metric spaces}

We evaluate the speed and accuracy of \texttt{dgh} on synthetic point clouds and metric graphs by computing the Gromov--Hausdorff distance from each space to its isometric copy. A point cloud is generated by uniformly sampling $n=200$ points from the unit cube in $\R^3$ and taking the Euclidean distance between them. A graph is repeatedly generated according to the Erd\H{o}s-R\'{e}nyi model with $n=200$ vertices and the edge probability of $p=0.05$ until it is connected, and then endowed with the shortest path metric. We generate 100 point clouds and 100 metric graphs, and run the experiment on each metric space for $c=1+10^{-5},1+10^{-3},\ldots,1+10^{19}$ and with the budgets of 100 and 1000 iterations. Note that (the matrix representation of) a mapping pair $(f, g)$ is a solution to (\ref{eqn:quadratic_objective}) if and only if $f=g^{-1}$ is an isometry, which corresponds to an all-zeros distance difference matrix. As a consequence, the solution set of the relaxation coincides with the original solutions to the Gromov--Hausdorff distance (\ref{eqn:matrix_reformulation}) for any $c>1$. 
% However, increasing the value of $c$ can affect the distances from local minima of (\ref{eqn:continuous_optimization}) to the set of isometries. 

% By the same logic used in the proof og Theorem 1, increasing the value of $c$ within a certain range penalizes large-magnitude entries in the corresponding ``soft'' distance difference matrices, thus making local minima better approximations of an isometry in the $\infty$-norm sense and potentially bringing them closer to the original solutions on the polytope $\SSS$

% However, the value of $c$ can affect the distances from local minima of (\ref{eqn:continuous_optimization}) to the set of isometries, and therefore the likelihood of an approximate solution recovered by \texttt{dgh} to deliver the (zero) Gromov--Hausdorff distance. %We expect the dependency of this likelihood on c to follow the logic behind Theorem 1: incerases in some range, then only decreases because benefits are gone but non-convexity grows


%, and therefore the projection of the solution set of (\ref{eqn:continuous_optimization}) onto $\RRR$

\input{figures/performance}

Figure \ref{fig:performance} shows the percentage of experiments in which the Gromov--Hausdorff distance was found exactly and the average compute time required by \texttt{dgh}, separately for each metric space type and iteration budget combination. 
%Predictably, the accuracy of the algorithm vanishes once $c$ becomes too small or too large. %While solving 
%(\ref{eqn:continuous_optimization}) for $c \leq 1 + 10^-5$ for the generated spaces becomes prohibitively slow due to the machine precision issues, the poor fit of the relaxation at some $c=1+\varepsilon$ for finding the Gromov--Hausdorff distance can be inferred from the fact that it is continuous in c and flat at c=1
Predictably, the accuracy of the algorithm on either metric space type decreases once $c$ becomes large enough. The initial positive trend may be explained by the associated reduction of maximal distance misalignment in local minima on $\SSS$ bringing them closer to isometries, which in turn increases the chance to find one by projecting an approximate solution of (\ref{eqn:continuous_optimization}) onto $\RRR$. Similarly to the logic underlying Theorem \ref{thm:c_threshold}, we expect the scale of $c$ on which the above effect is manifested to be inversely proportional to the distortion gap. Note that the distortion gap on metric graphs, $\rho=1$ due to their integer distances, is orders of magnitude bigger than on point clouds even after rescaling by their diameters --- $O(\log{n})$ for the Erd\H{o}s-R\'{e}nyi graph and $\sqrt{3}$ for the unit cube. For rescaled inputs, the average distortion gap is $0.24$ in metric graphs and $4.6 \times 10^{-17}$ in point clouds. This difference likely accounts for the contrasting optimal scales of the parameter $c$ in each case.

% The average distortion gap for the rescaled inputs is $0.24$ for the metric graphs and $4.6\cdot10^{-17}$ for the point clouds, which may explain the contrast between their optimal scales of $c$.% for the two metric space types.


% For each collection of metric spaces $X_1, \ldots, X_{100}$, we compute (an upper bound of) the GH distance between $X_i$ and $\pi(X_i)$ for some isometry $\pi$ for every $i=1,\ldots,100$. In addition, we compare the warm start approach implemented in \texttt{dgh} to the vanilla Frank--Wolfe algorithm for a fixed value of $c$ with the same iteration budget. Similarly to the warm start approach, the algorithm restarts from a randomly sampled point after converging and the best solution is selected after the iteration budget is depleted. Both warm start and vanilla approaches are tried for different values of $c \in \{1 + 10^p: p = -2, -3, \ldots, 7\}$ and given the same iteration budget of 100 iterations every time. !!!! Performance metrics

% Beside the warm start approach implemented in \texttt{dgh}, we additionally 

% \subsection{Classification on biological data}
\subsection{Model spaces}
\label{spheres}
% Recall that any compact metric space $X$ admits a finite approximation of arbitrary precision $\varepsilon$ in the form of its epsilon-net
Recall that a compact metric space $X$ contains its finite $\varepsilon$-net $X_\varepsilon$ for any $\varepsilon > 0$. Because $$\left|\dGH(X,Y) - \dGH(X_\varepsilon,Y_\varepsilon)\right| \leq \epsilon,\quad\cite{oles2022lipschitz}$$ such an approximation enables numerical estimation of the Gromov--Hausdorff distance between infinite $X,Y$ to an arbitrary precision. This can be particularly useful for estimating the distance between model metric spaces. We demonstrate it by refining an upper bound of the Gromov--hausdorff distance between the unit circle $S^1 \in \R^2$ and the upper hemisphere $H^2 \subset \R^3$ of the unit sphere, established to be strictly below $\frac{\sqrt{3}}{2}$ in \cite{lim2021gromov}.

To generate an $\varepsilon$-net of $H^2$, we use a slight modification of the regular construction described in \cite{deserno2004generate}.
%on the upper hemisphere $\left\{(\theta, \phi): \theta \in [0, \frac{\pi}{2}], \phi \in [0, 2\pi]\right\}$.
Given some small $\delta$, we consider evenly spaced polar angles $\theta_i$ covering the hemisphere range $\left[0, \frac{\pi}{2}\right]$ so that the geodesic distance from any $\mathbf{x} = (\theta, \phi) \in H^2$ to the nearest $\yy = (\theta_i, \phi)$ is $|\theta - \theta_i| \leq \frac{\delta}{2}$. The law of cosines then bounds the Eucledian distance between $\xx$ and $\yy$ by
\begin{align*}
    \|\xx - \yy\|^2 &= 2 - 2\cos(\theta - \theta_i)\\
    &\leq 2\left(1-\cos\frac{\delta}{2}\right).
\end{align*}
%$\forall \theta \in [0, \frac{\pi}{2}] \quad \min_i |\theta - \theta_i| \leq \frac{\delta}{2}$.
For each $\theta_i$, we choose evenly spaced azimuthal angles $\phi_j(\theta_i)$ so that %$\forall \phi \in [0, 2\pi] \quad \min_j |\phi - \theta_i| \leq \frac{\delta}{2}$
the geodesic distance between any $\mathbf{y} = (\theta_i, \phi)$ and the nearest $\mathbf{z} = \left(\theta_i, \phi_j(\theta_i)\right)$ is at most $\frac{\delta}{2}$. Because $\yy$ and $\zz$ are on a circle of radius $\sin\theta_i$, this implies $\left|\phi - \phi_j(\theta_i)\right| \leq \frac{\delta}{2\sin\theta_i}$ and therefore 
\begin{align*}
    \|\yy - \zz\|^2 &= 2\sin^2\theta_i - 2\sin^2\theta_i \cos\left(\phi - \phi_j(\theta_i)\right) \\
    &\leq 2\sin^2\theta_i\left(1-\cos\frac{\delta}{2\sin\theta_i}\right).
\end{align*}
%, and place a point at each $(\theta_i, \phi_j(\theta_i))$ to construct the $\varepsilon$-net $H^2_\varepsilon$.
The $\varepsilon$-net $H^2_\varepsilon$ is comprised of the points at $\left(\theta_i, \phi_j(\theta_i)\right)$ for every $i,j$ pair. Its covering radius $\varepsilon$ can be bounded based on
\begin{align*}
    \left|\cos\angle\xx\yy\zz\right| &= \left| \langle \xx - \yy, \yy - \zz\rangle\right| \\
    &= \sin\theta_i\left|\sin\theta_i - \sin\theta\right|\left(\cos\left(\phi - \phi_j(\theta_i)\right)\right) \\
    &\leq \sin\theta_i\left(\sin\theta_i\left(1-\cos\frac{\delta}{2}\right)+\cos\theta_i\sin\frac{\delta}{2}\right)\left(1 - \cos\frac{\delta}{2\sin\theta_i}\right),
\end{align*}
which entails
\begin{align*}
    \varepsilon^2 &\leq \sup_{\xx}\|\xx-\zz\|^2 \\
    &\leq \sup_{\xx} \left(\|\xx - \yy\|^2 + \|\yy - \zz\|^2 + 2\|\xx - \yy\|\|\yy - \zz\|\left|\cos{\angle \xx\yy\zz}\right|\right) \\
    &\leq 2\left(1-\cos\frac{\delta}{2}\right) + 2 \max_i \sin^2\theta_i \left(1 - \cos\frac{\delta}{2\sin\theta_i}\right)\Bigg[1 + \\&\hspace{2cm}2\sqrt{1-\cos\frac{\delta}{2}}\sqrt{1 - \cos\frac{\delta}{2\sin\theta_i}}\left(\sin\theta_i\left(1-\cos\frac{\delta}{2}\right)+\cos\theta_i\sin\frac{\delta}{2}\right)\Bigg].
\end{align*}

By letting $\delta=\frac{\sqrt{2\pi}}{15}$ in the above, we construct $H^2_\varepsilon$ of 168 points and $\varepsilon \approx 0.1385$. We match this covering radius on $S^1$ by constructing its $\varepsilon$-net $S^1_\varepsilon$ as a regular lattice of 23 points. Setting $c=10^7$ and running \texttt{dgh} with a permissive iteration budget for about 16 minutes yields a mapping pair delivering $\dGH(S^1_\varepsilon, H^2_\varepsilon) < 0.6913$, shown in Figure \ref{fig:spheres}. It follows that $$\dGH\left(S^1, H^2\right) \leq \dGH\left(S^1_\varepsilon, H^2_\varepsilon\right) + \varepsilon < 0.8298,$$ a refinement over $\frac{\sqrt{3}}{2} \approx 0.8660.$ Trivially, this bound holds for both the open and the closed hemispheres as well as for the ``helmet'' of $S^2$ --- its upper hemisphere that contains $(0, \phi) \in S^2$ if and only if $\phi \in \left[0, \pi\right)$, a construction from \cite{lim2021gromov} facilitating antipode-preserving mappings.

\input{figures/spheres}

% \subsection{Object recognition}
% To test the efficiency of estimating the Gromov--Hausdorff distance for object recognition, we apply our algorithm towards ModelNet10. \cite{wu20153d} The ModelNet10 dataset is comprised of 4,899 volumetric shapes from 10 classes (e.g. bathtubs, chairs, or monitors), and split 80/20 into the training and the testing sets. 

% Within the training data, we aim to infer the class of a shape as the class of its nearest neighbor from the entire testing set. We measure dissimilarity of a pair of shapes by bounding the Gromov--Hausdorff distance between the two 3-dimensional point clouds sampled from their 2-dimensional boundaries. Specifically, we sample 500 points from each shape and run the algorithm with  $c=10^7$ and the iteration budget of 100. Because classifying one shape relies on computing its Gromov--Hausdorff distance to the 908 elements of the testing set, we constrain our scope to 1000 training data objects obtained by randomly selecting 100 shapes from each class. The subsequent $0.908\times10^6$ computations of the Gromov--Hausdorff distance are distributed over 15,000 CPU cores (2016 Intel Xeon E5-2695v4) of Falcon supercomputer.

% Our algorithm achieves the classification accuracy of 90,0\%. The resulting confusion matrix is shown on Figure \ref{fig:confusion matrix}. 