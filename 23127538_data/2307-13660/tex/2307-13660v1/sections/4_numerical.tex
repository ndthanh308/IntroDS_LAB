\section{Numerical experiments}
\label{numerical}
We demonstrate the method's performance using our implementation from a Python package \texttt{dgh}.
Given a choice of $c$ and a budget of Frank--Wolfe iterations, it starts from a random $\SS_0 \in \SSS$ and iterates until the Frank--Wolfe gap is at most $10^{-8}$ (or there are no iterations left), after which the last $\SS_i$ is projected to the nearest $\RR$. The cycle repeats until the budget is depleted, after which the smallest found $\frac{1}{2}\dis \RR$ is returned as an estimate and upper bound of the Gromov--Hausdoff distance (we note that this approach is easily parallelizable as there is no interdependency between the random restarts).
In a bid to save on redundant computations, \texttt{dgh} also compares every new best $\frac{1}{2}\dis \RR$ against the trivial lower bound $$\dGH(X, Y) \geq \frac{1}{2}\max\left\{\left|\diam X - \diam Y\right|, \left|\rad X - \rad Y\right|\right\}\quad\cite{memoli2012some}$$ and terminates whenever they match, which means that the Gromov--Hausdorff distance was found exactly. Prior to the computations, \texttt{dgh} normalizes all distances in the input metric spaces so that $\max\{\diam X, \diam Y\} = 1$ to avoid floating-point arithmetic overflow, and scales the resulting $\dis\RR$ back afterwards. %Every time a smaller $\dis\RR$ is obtained, \texttt{dgh} compares it to the trivial lower bound of the Gromov--Hausdorff distance $$\dGH(X, Y) \geq \max\left\{\right\}$$ and terminates if they match
%--- note that this is equivalent to solving (\ref{eqn:continuous_optimization}) for $c^{\dmax}$ and does not affect the optimization landscape for the GH distance.

In the following, we describe our numerical experiments on synthetic and model geometric spaces. All computations were performed on a standard 2016 Intel i7-7500U processor.

% We implement our approach to computing the GH distance as a Python package \texttt{dgh}. Given $c > 1$ and the budget of Frank--Wolfe iterations, our implementation solves a sequence of (\ref{eqn:continuous_optimization}) for $c_1 = c$ and $c_{k+1} = 1 + 10(c_k - 1)$ using the Frank--Wolfe algorithm with warm starts. The starting point for the first minimization in a sequence is selected from $\SSS$ uniformly at random. For every $c_k$, the minimization stops when the Frank--Wolfe gap gets below $10^{-8}$ or when no iterations are left in the budget. Once $c_{k+1}$ exceeds $10^8$, the (approximate) solution for $c_k$ is stored and the sequence is restarted until the iteration budget is depleted. The final solution is selected for the smallest distortion of its projection onto $\RRR$ which delivers an upper bound of the GH distance. All distances in the input metric spaces are scaled by $\dmax$ (so that $\max\{\diam X, \diam Y\} = 1$) prior to the computations to avoid floating-point arithmetic overflow --- note that this is equivalent to solving (\ref{eqn:continuous_optimization}) for $c^{\dmax}$ and does not affect the optimization landscape for the GH distance.

% Next, we describe numerical experiments on synthetic, real-world, and theoretical metric spaces demonstrating the performance and applicability of our approach.

\subsection{Benchmarking on synthetic spaces}

We evaluate the speed and accuracy of \texttt{dgh} on synthetic point clouds and graphs by computing the Gromov--Hausdorff distance from each space to its isometric copy. A point cloud is generated by uniformly sampling $n=200$ points from the unit cube in $\R^3$ and taking the Euclidean distance between them. A graph is generated according to the Erd\H{o}s-R\'{e}nyi model with $n=200$ vertices and the edge probability of $p=0.05$ until it is connected, and then endowed with the shortest path metric. We generate 100 point clouds and 100 graphs, and run an experiment on each metric space for $c=1+10^{-4},1+10^{-3},\ldots,1+10^8$ and with the budgets of 100 and 1000 iterations. For each metric space type, we show the percent of experiments where the (zero) Gromov--Hausdorff distance was found exactly and the average time taken by \texttt{dgh} on Figure \ref{fig:performance}.
%, and another 100 graphs for $p=0.1$.
%We provide each experiment with the budgets of 100 and 1000 iterations and replicate it for every $c=1+10^{-4},1+10^{-3},\ldots,1+10^8$.

\input{figures/performance}

Interestingly, the method behaves very differently on the two metric space types. While the accuracy on graphs drops drastically as the value of $c$ grows, this trend is somewhat reversed for point clouds. We hypothesize that the large distortion gap $\rho=1$ in graphs relative to their $O(\log{n})$ diameter %($\rho=1$ due to their integer distances)
allows the solutions to be preserved by (\ref{eqn:continuous_optimization}) even for small $c$,  %(see Theorem \ref{thm:c_threshold})
whose near-convex optimization landscape enables their efficient recovery.
While the non-convexity of the optimization landscape growing together with $c$ is expected to drive the accuracy down for any metric space, this phenomenon may be outweighed by the solution-preserving effects of larger values of $c$ on point clouds.

% At the same time, the non-convexity of the optimization landscape may be a factor behind the worsened accuracy on graphs for larger values of $c$. An analogous dynamic for point clouds may be obscured

% Every experiment is replicated for the values of $c$ in $\{1+10^p: p = -4, \ldots, 8\}$ and the budgets of 100 and 1000 iterations.
% %provided with a budget of 100 and, separately, 1000 iterations every time.
% Figure \ref{fig:performance} aggregates the resulting performance measurements across the 100 spaces for each experimental setup.

% for each metric space type and the value of $c$.

% space c exact dGH time restarts


% For each collection of metric spaces $X_1, \ldots, X_{100}$, we compute (an upper bound of) the GH distance between $X_i$ and $\pi(X_i)$ for some isometry $\pi$ for every $i=1,\ldots,100$. In addition, we compare the warm start approach implemented in \texttt{dgh} to the vanilla Frank--Wolfe algorithm for a fixed value of $c$ with the same iteration budget. Similarly to the warm start approach, the algorithm restarts from a randomly sampled point after converging and the best solution is selected after the iteration budget is depleted. Both warm start and vanilla approaches are tried for different values of $c \in \{1 + 10^p: p = -2, -3, \ldots, 7\}$ and given the same iteration budget of 100 iterations every time. !!!! Performance metrics

% Beside the warm start approach implemented in \texttt{dgh}, we additionally 

% \subsection{Classification on biological data}
\subsection{Bounding the distance between model spaces}
\label{spheres}
% Recall that any compact metric space $X$ admits a finite approximation of arbitrary precision $\varepsilon$ in the form of its epsilon-net
Recall that a compact metric space $X$ contains its finite $\varepsilon$-net $X_\varepsilon$ for any $\varepsilon > 0$. Because $$\left|\dGH(X,Y) - \dGH(X_\varepsilon,Y_\varepsilon)\right| \leq \epsilon,\quad\cite{oles2022lipschitz}$$ such a finite approximation enables numerical estimation of the Gromov--Hausdorff distance between infinite $X,Y$ to an arbitrary precision. This can be particularly useful for estimating the distance between model metric spaces. We demonstrate it by refining an upper bound of the Gromov--hausdorff distance between the unit circle $S^1 \in \R^2$ and the upper hemisphere $H^2 \subset \R^3$ of the unit sphere, established to be strictly below $\frac{\sqrt{3}}{2}$ in \cite{lim2021gromov}.

To generate an $\varepsilon$-net of $H^2$, we use a slight modification of the regular construction described in \cite{deserno2004generate}.
%on the upper hemisphere $\left\{(\theta, \phi): \theta \in [0, \frac{\pi}{2}], \phi \in [0, 2\pi]\right\}$.
Given some small $\delta$, we consider evenly spaced polar angles $\theta_i$ covering the hemisphere range $\left[0, \frac{\pi}{2}\right]$ so that the geodesic distance from any $\mathbf{x} = (\theta, \phi) \in H^2$ to the nearest $\yy = (\theta_i, \phi\}$ is $|\theta - \theta_i| \leq \frac{\delta}{2}$. The law of cosines then bounds the Eucledian distance between $\xx$ and $\yy$ by
\begin{align*}
    \|\xx - \yy\|^2 &= 2 - 2\cos(\theta - \theta_i)\\
    &\leq 2\left(1-\cos\frac{\delta}{2}\right).
\end{align*}
%$\forall \theta \in [0, \frac{\pi}{2}] \quad \min_i |\theta - \theta_i| \leq \frac{\delta}{2}$.
For each $\theta_i$, we choose evenly spaced azimuthal angles $\phi_j(\theta_i)$ so that %$\forall \phi \in [0, 2\pi] \quad \min_j |\phi - \theta_i| \leq \frac{\delta}{2}$
the geodesic distance between any $\mathbf{y} = (\theta_i, \phi)$ and the nearest $\mathbf{z} = \left(\theta_i, \phi_j(\theta_i)\right)$ is at most $\frac{\delta}{2}$. Because $\yy$ and $\zz$ are on a circle of radius $\sin\theta_i$, this implies $\left|\phi - \phi_j(\theta_i)\right| \leq \frac{\delta}{2\sin\theta_i}$ and therefore 
\begin{align*}
    \|\yy - \zz\|^2 &= 2\sin^2\theta_i - 2\sin^2\theta_i \cos\left(\phi - \phi_j(\theta_i)\right) \\
    &\leq 2\sin^2\theta_i\left(1-\cos\frac{\delta}{2\sin\theta_i}\right).
\end{align*}
%, and place a point at each $(\theta_i, \phi_j(\theta_i))$ to construct the $\varepsilon$-net $H^2_\varepsilon$.
The $\varepsilon$-net $H^2_\varepsilon$ is comprised of the points at $\left(\theta_i, \phi_j(\theta_i)\right)$ for every $i,j$ pair. Its covering radius $\varepsilon$ can be bounded using
\begin{align*}
    \left|\cos\angle\xx\yy\zz\right| &= \left| \langle \xx - \yy, \yy - \zz\rangle\right| \\
    &= \sin\theta_i\left|\sin\theta_i - \sin\theta\right|\left(\cos\left(\phi - \phi_j(\theta_i)\right)\right) \\
    &\leq \sin\theta_i\left(\sin\theta_i\left(1-\cos\frac{\delta}{2}\right)+\cos\theta_i\sin\frac{\delta}{2}\right)\left(1 - \cos\frac{\delta}{2\sin\theta_i}\right),
\end{align*}
which entails
\begin{align*}
    \varepsilon^2 &\leq \sup_{\xx}\|\xx-\zz\|^2 \\
    &\leq \sup_{\xx} \left(\|\xx - \yy\|^2 + \|\yy - \zz\|^2 + 2\|\xx - \yy\|\|\yy - \zz\|\left|\cos{\angle \xx\yy\zz}\right|\right) \\
    &\leq 2\left(1-\cos\frac{\delta}{2}\right) + 2 \max_i \sin^2\theta_i \left(1 - \cos\frac{\delta}{2\sin\theta_i}\right)\Bigg[1 + \\&\hspace{2cm}2\sqrt{1-\cos\frac{\delta}{2}}\sqrt{1 - \cos\frac{\delta}{2\sin\theta_i}}\left(\sin\theta_i\left(1-\cos\frac{\delta}{2}\right)+\cos\theta_i\sin\frac{\delta}{2}\right)\Bigg].
\end{align*}

By letting $\delta=\frac{\sqrt{2\pi}}{15}$ in the above, we construct $H^2_\varepsilon$ of 168 points and $\varepsilon \approx 0.1385$. We match this covering radius on $S^1$ by constructing its $\varepsilon$-net $S^1_\varepsilon$ as a regular lattice of 23 points. Setting $c=10^7$ and running \texttt{dgh} with a permissive iteration budget for about 16 minutes yields a mapping pair delivering $\dGH(S^1_\varepsilon, H^2_\varepsilon) < 0.6913$, shown in Figure \ref{fig:spheres}. It follows that $$\dGH\left(S^1, H^2\right) \leq \dGH\left(S^1_\varepsilon, H^2_\varepsilon\right) + \varepsilon < 0.8298,$$ a refinement over $\frac{\sqrt{3}}{2} \approx 0.8660.$ We note that this bound holds for both the open and closed hemispheres as well as for the ``helmet'' of $S^2$ --- its upper hemisphere that contains $(0, \phi) \in S^2$ if and only if $\phi \in \left[0, \pi\right)$, a construction from \cite{lim2021gromov} facilitating antipode-preserving mappings.

\input{figures/spheres}