\section{Proofs}
\label{proofs}
\cthreshold*
\begin{proof}
    Recall that $\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T$ is by construction symmetric with zero main diagonal, and $\dis \RR$ is the largest magnitude among its entries. Let     $\RR, \RR' \in \RRR$ be such that $\dis \RR > \dis \RR'$ and therefore $\dis \RR \geq \dis\RR' + \rho$.
    % Let $R, R' \subseteq X \times Y$ be mapping pair-induced relations s.t. $\dis R > \dis R'$ with their matrix representations denoted as $\RR, \RR' \in \RRR$, respectively. By construction, $\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T$ is symmetric, has zero main diagonal, and only contains entries from $[0, \dis R]$ with at least one entry equal to $\dis R$.
    Because $a \mapsto c^a + c^{-a}$ is convex and attains its minimum at $a = 0$,
    \begin{align*}
        \left\|c^{\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T} + c^{\RR\UU\RR^T - \VV + \WW^T\RR^T - \RR\WW}\right\|_1 &\geq 2\left(c^{\dis \RR} + c^{-\dis \RR} + (n+m)^2-2\right) \\
        &> 2\left(c^{\dis \RR} + (n+m)^2 - 2\right).
    \end{align*}
    At the same time,
    \begin{align*}
        \hspace{-.5cm}\left\|c^{\VV - \RR'\UU\RR'^T + \RR'\WW - \WW^T\RR'^T} + c^{\RR'\UU\RR'^T - \VV + \WW^T\RR'^T - \RR'\WW}\right\|_1 &\leq \left((n+m)^2 - n - m\right)\left(c^{\dis \RR'} + c^{-\dis \RR'}\right) + 2(n+m) \\ &< \left((n+m)^2-n-m\right)c^{\dis \RR'} + (n+m)^2+n+m.
    \end{align*}
    % By the definition of distortion gap, $\dis R' \leq \dis R - \epsilon$ and therefore
    Then
    \begin{align*}
        &\left\|c^{\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T} + c^{\RR\UU\RR^T - \VV + \WW^T\RR^T - \RR\WW}\right\|_1 - \left\|c^{\VV - \RR'\UU\RR'^T + \RR'\WW - \WW^T\RR'^T} + c^{\RR'\UU\RR'^T - \VV + \WW^T\RR'^T - \RR'\WW}\right\|_1 \\
        &\quad> 2c^{\dis \RR} - \left((n+m)^2-n-m\right)c^{\dis \RR'} + (n+m)^2-n-m-4 \\
        &\quad\geq \left(2c^\rho - (n+m)^2-n-m\right)c^{\dis \RR'}  + (n+m)^2-n-m-4 \\
        &\quad\geq (n+m)^2-n-m-4 \\
        &\quad> 0.
    \end{align*}
    It follows that the 1-norm relaxation $\left\|c^{\VV - \RR\UU\RR^T + \RR\WW - \WW^T\RR^T} + c^{\RR\UU\RR^T - \VV + \WW^T\RR^T - \RR\WW}\right\|_1$ monotonically decreases with $\dis \RR$.
\end{proof}

% Proof of Theorem \ref{thm:convex_hull}:
% \begin{proof}
%     A generalization of the Birkhoff-von Neumann theorem for row-stochastic matrices states than the set of row-stochastic matrices is the convex hull of the set of binary row-stochastic matrices, see e.g. \cite{gubin2008subgraph} or \cite{cao2022centrosymmetric}. Trivially then, the Cartesian product of the $n\times m$ and $m \times n$ row-stochastic matrices is the convex hull of the Cartesian product of their binary counterparts. Replacing every matrix pair with their direct sum in both sets concludes the proof.
% \end{proof}

\polytopegeometry*
\begin{proof}
    The bi-mapping polytope $\SSS$ is defined by the $2nm$ inequalities of the form $S_{ij} \geq 0$, $n^2+m^2$ equality constraints of the form $S_{ij} = 0$, and the requirement of unit row sums $\SS\bf{1} = \bf{1}$. In particular, the facets of $\SSS$ lie in the hyperplanes $S_{ij} = 0$ for $(i,j) \in \left(\{1,\ldots,n\}\times \{1,\ldots,m\}\right) \cup \left(\{n+1,\ldots,n+m\}\times \{m+1,\ldots,m+n\}\right)$. Because every face is the intersection of a set of facets, $\Phi$ is given by the collective indices of zero entries describing the corresponding hyperplanes.
\end{proof}

\solutiononinterior*
\begin{proof}
The statement trivially holds for $\SS^* \in \RRR$ (recall that a vertex is its own interior). Otherwise, $\exists h,k,l$ s.t. $0 < S^*_{hk}, S^*_{hl} < 1$, which means that $\SS^*$ lies on the open segment whose endpoints $\SS', \SS''$ are given by $S'_{ij} = \begin{cases}
    S^*_{hk} + S^*_{hl} & \text{if $(i,j) = (h,k)$} \\
    0 & \text{if $(i,j) = (h,l)$}\\
    S^*_{ij} & \text{otherwise}
\end{cases}$, $S''_{ij} =
\begin{cases}
    0 & \text{if $(i,j) = (h,k)$} \\
    S^*_{hk} + S^*_{hl} & \text{if $(i,j) = (h,l)$}\\
    S^*_{ij} & \text{otherwise}
\end{cases}$. By applying the trace trick, we derive the following identity:
\begin{align*}
    \left\langle \SS', \grad \obj(\SS'') \right\rangle &= 2\left\langle \SS', c^\VV\SS'' c^{-\UU} + c^{-\VV}\SS''c^\UU + \big(c^\WW\SS'' c^{-\WW} + c^{-\WW}\SS'' c^\WW\big)^T \right\rangle \\
    &= 2\left\langle \SS'', c^\VV\SS' c^{-\UU} + c^{-\VV}\SS'c^\UU + \big(c^{-\WW}\SS' c^{\WW} + c^{\WW}\SS' c^{-\WW}\big)^T \right\rangle \\
    &= \left\langle \SS'', \grad \obj(\SS') \right\rangle.
\end{align*}
Denoting $\alpha \defeq \frac{S^*_{hk}}{S^*_{hk} + S^*_{hl}} \in (0, 1)$, we get
\begin{align*}
    \obj(\SS^*) &= \obj\left(\alpha \SS' + (1-\alpha)\SS''\right) \\
    &= \Big\langle \alpha \SS' + (1-\alpha)\SS'', \frac{1}{2}\grad\obj\left(\alpha \SS' + (1-\alpha)\SS''\right)\Big\rangle \\
    &= \alpha^2\obj(\SS') + \alpha(1-\alpha)\left\langle \SS'', \grad \obj(\SS') \right\rangle + (1-\alpha)^2\obj(\SS'') \\
    &\leq (1 - 2\alpha(1-\alpha)\obj(\SS^*) + \alpha(1-\alpha)\left\langle \SS'', \grad \obj(\SS') \right\rangle,
\end{align*} which implies $$\left\langle \SS'', \grad \obj(\SS') \right\rangle \leq 2 \obj(\SS^*) \leq \left\langle \SS', \grad \obj(\SS')\right\rangle$$ and therefore
\begin{equation*}
    \grad\obj(\SS')_{hl} \leq \grad\obj(\SS')_{hk}. \tag{1}
\end{equation*}
By the analogous reasoning using $\left\langle \SS', \grad \obj(\SS'') \right\rangle$ in place of $\left\langle \SS'', \grad \obj(\SS') \right\rangle$,
\begin{equation*}
    \grad\obj(\SS'')_{hk} \leq \grad\obj(\SS'')_{hl}.    \tag{2}
\end{equation*}
At the same time, %$\SS' - \SS'' = \begin{bmatrix}    \ddots & &  \end{bmatrix}$

\begin{align*}
    \grad\obj(\SS')_{ij} - \grad\obj(\SS'')_{ij} &= \grad\obj(\SS' - \SS'')_{ij} \\
    &= 2\big(c^\VV(\SS'-\SS'')c^{-\UU} + c^{-\VV}(\SS'-\SS'')c^\UU\big)_{ij} \\
    &\hspace{2.5cm}+ \big(c^\WW(\SS'-\SS'')c^{-\WW} + c^{-\WW}(\SS'-\SS'')c^\WW\big)_{ji} \\
    &= 2\begin{aligned}[t](S^*_{hk} + S^*_{hl})&\big(c^{V_{ih}}(c^{-U_{kj}} - c^{-U_{lj}}) + c^{-V_{ih}}(c^{U_{kj}} - c^{U_{lj}}) \\ &+ c^{W_{jh}}(c^{-W_{ki}} - c^{-W_{li}}) + c^{-W_{jh}}(c^{W_{ki}} - c^{W_{li}})\big),
    \end{aligned}
\end{align*}
and in particular
\begin{equation*}
\begin{aligned}
    \grad\obj(\SS')_{hk} - \grad\obj(\SS'')_{hk} &= 2(S^*_{hk} + S^*_{hl})\big(2 - c^{-U_{lk}} - c^{U_{lk}} + 2 - c^{W_{kh}}c^{-W_{lh}} - c^{-W_{kh}}c^{W_{lh}}\big) \\ &\leq 0,
\end{aligned} \tag{3}
\end{equation*}
\begin{equation*}
\begin{aligned}
        \grad\obj(\SS')_{hl} - \grad\obj(\SS'')_{hl} &= 2(S^*_{hk} + S^*_{hl})\big(c^{-U_{kl}} + c^{U_{kl}} - 2 + c^{W_{lh}}c^{-W_{kh}} + c^{-W_{lh}}c^{W_{kh}} - 2\big) \\ &\geq 0. 
\end{aligned}\tag{4}
\end{equation*}
Combining inequalities (1), (2), (3), and (4) gives $\grad\obj(\SS')_{hk} = \grad\obj(\SS')_{hl}$. Then $$\left\langle \SS'', \grad \obj(\SS')\right\rangle = \left\langle \SS', \grad \obj(\SS') \right\rangle = 2 \obj(\SS^*)$$ and therefore $\SS' \in \argmin_{\SS \in \SSS}\obj(\SS)$.

We showed that for any $(h, k)$ s.t. $0 < S^*_{hk} < 1$ there exists a solution $\SS' \in \SSS$ s.t. $S'_{ij} = 0$ whenever $S^*_{ij}=0$ or $(i,j)=(h,k)$. By Lemma \ref{lem:polytope_geometry}, it means that every facet of $\Phi$ contains a solution on its interior. Recursively repeating this argument establishes the existence of a solution in the interior of every non-empty face of $\Phi$, which also implies that every vertex of $\Phi$ is a solution.

Since $\obj$ is quadratic, attaining the same value at any three points on the same line renders the function constant on the entire line. In particular, any face $\Psi$ is included in $\argmin_{\SS\in\SSS}\obj(\SS)$ provided that $$\partial\Psi \cup \{\SS^\mathrm{o}\} \subseteq \argmin_{\SS\in\SSS}\obj(\SS)$$ for some interior point $\SS^\mathrm{o} \in \Psi\setminus\partial\Psi$. It follows that any positive-dimension face of $\Phi$ must be a part of the solution set if all the faces of $\Phi$ of dimension smaller by 1 are. Starting from the vertices of $\Phi$ and applying induction then yields $\Phi \subseteq \argmin_{\SS\in\SSS}\obj(\SS)$.
\end{proof}

\voronoiface*
\begin{proof}
    Note that $$\|\SS - \RR\|_2^2 = \|\SS\|_2^2 - 2\langle \SS, \RR \rangle + n+m\quad\forall\RR \in \RRR$$ and therefore $$\proj_\RRR \SS \in \argmin_{\RR \in \RRR}\,\|\SS - \RR\|_2 = \argmax_{\RR \in \RRR} \;\langle \SS, \RR \rangle.$$
    Let $s_i$ denote the maximum entry in the $i$-th row of $\SS$ for $i=1,\ldots,n+m$. Then $$\max_{\RR \in \RRR} \;\langle \SS, \RR \rangle = \sum_{i=1}^n s_i,$$ which means that any $\RR^* \in \argmax_{\RR \in \RRR} \;\langle \SS, \RR \rangle$ can have $R^*_{ij} \neq 0$ only if $S_{ij} = s_i$. Because $\SS$ is a convex combination of the vertices of $\Phi$, $S_{ij} = s_i > 0$ in turn implies the existence of $\RR \in \Phi \cap \RRR$ s.t. $R_{ij} \neq 0$. By Lemma \ref{lem:polytope_geometry}, the index of every non-zero entry of $\RR^*$ is not contained in the index set of zero entries characterizing $\Phi$, and therefore $\RR^* \in \Phi$.
\end{proof}

% \guarantees*
% \begin{proof}
%     By ..., the solution set of $\min_{\SS\in\SSS}\obj(\SS)$ is closed under inclusion 
%     By the corollary to Theorem \ref{thm:c_threshold}, $$\argmin_{\RR\in\RRR} \obj(\RRR) \subseteq \argmin_{\RR\in\RRR}\dis\RR.$$ It remains
% \end{proof}
%     \label{thm:guarantees}
%     Let $\proj_\RRR: \SSS \to \RRR$ denote the projection of the bi-mapping polytope onto the set of its vertices. If $c \geq \left(\frac{(n+m)^2-n-m}{2}\right)^{1/\rho}$, then $$\proj_\RRR \left(\argmin_{\SS\in\SSS} \obj(\SSS)\right) \subseteq \argmin_{\RR\in\RRR}\dis\RR.$$

% Proof of Theorem \ref{thm:time_complexity}:
% \begin{proof}
%     The time complexity of a single Frank--Wolfe iteration is defined by the calculations of gradient at the current point, the direction of smallest correlation with it, and the step size to take along said direction. Computing the gradient $\grad \obj(\SS_i)$ is comprised of multiplying and summing $(n+m)\times(n+m)$ matrices which has the time complexity of $O(n^3)$. Finding the descent direction $\DD_i$ amounts to solving $\min_{\RR\in\RRR}\langle \RR, \grad\obj(\SS_i) \rangle$, which can be done in $O(n^2)$ time by locating the smallest entry in each row of $\grad\obj(\SS_i)$. Finally, the quadratic minimization step $\min_{\gamma \in [0, 1]} \obj(\gamma\SS_i + (1-\gamma)\DD_i)$ admits a closed-form solution that requires $O(n^3)$-time computations including of $\ob2j(\DD_i)$.
% \end{proof}

\oneminimizer*
\begin{proof}
    $\left|\argmin_{\SS\in\SSS} \left\langle \SS, \grad\obj(\SS^*) \right\rangle\right| = 1$ if and only if the smallest entry in each row of $\grad\obj(\SS^*)$ is unique.  In order for $\grad\obj(\SS^*)_{hk} = \grad\obj(\SS^*)_{hl}$ to hold for some $h$ and $k\neq l$, the realizations $d_1, \ldots, d_\frac{n(n-1)}{2}$ of the distances in $X$ must satisfy
    $$\sum_{i=1}^{n+m}\sum_{j=1}^{n+m} \left(c^{U_{hi}}S_{ij}c^{-V_{jk}} + c^{-U_{hi}}S_{ij}c^{V_{jk}} +c^{-W_{kj}}S_{ji}c^{W_{ih}} + c^{W_{kj}}S_{ji}c^{-W_{ih}}\right) = 0,$$
    which can be rewritten as $$\sum_{i=0}^\frac{n(n-1)}{2}\sum_{j=0}^\frac{n(n-1)}{2}a_{ij}c^{d_i - d_j} = 0$$ for some $a_{ij} \in \R$ and $d_0 \defeq 0$. Because the left-hand side can be cast as a generalized polynomial through a change of variables, it must have a finite number of solutions. The probability of the distances in $X$ to form a particular solution $d_1^*, \ldots, d_\frac{n(n-1)}{2}^*$ is
    \begin{align*}
        &\mathbb{P}_\XX\left[\mathrm{D}_1=d^*_1,\ldots,\mathrm{D}_\frac{n(n-1)}{2}=d_\frac{n(n-1)}{2}^*\right] \\ &\hspace{2cm} = \mathbb{P}_\XX\left[\mathrm{D}_1=d^*_1|\mathrm{D}_2=d_2^*,\ldots,\mathrm{D}_\frac{n(n-1)}{2}=d_\frac{n(n-1)}{2}^*\right]\mathbb{P}_\XX\left[\mathrm{D}_2=d_2^*,\ldots,\mathrm{D}_\frac{n(n-1)}{2}=d_\frac{n(n-1)}{2}^*\right] \\
        &\hspace{2cm} = 0\cdot\mathbb{P}_\XX\left[\mathrm{D}_2=d_2^*,\ldots,\mathrm{D}_\frac{n(n-1)}{2}=d_\frac{n(n-1)}{2}^*\right]
        \\
        &\hspace{2cm} = 0,
    \end{align*} and therefore
    $\mathbb{P}_\XX\left[\grad\obj(\SS^*)_{hk} = \grad\obj(\SS^*)_{hl}\right] = 0.$
    Then
    \begin{align*}
        \mathbb{P}_\XX\left[\Big|\argmin_{\SS\in\SSS} \langle \SS, \grad\obj(\SS^*) \rangle\Big| > 1\right] &\leq \sum_{h=1}^{n+m}\sum_{k=1}^{n+m}\sum_{l=k+1}^{n+m} \mathbb{P}_\XX\left[\grad\obj(\SS^*)_{hk} = \grad\obj(\SS^*)_{hl}\right] \\ &= 0.
    \end{align*}
\end{proof}

\nonconvexity*
\begin{proof}
    Let $\lmax = \lambda_1 \geq \ldots \geq \lambda_{(n+m)^2}$ denote the eigenvalues of $\HH$. We already established using the Courant-Fischer theorem that
    \begin{align*}
        \lambda_1 &\geq \frac{\|\HH\|_1}{(n+m)^2} \\
        &= \frac{2}{(n+m)^2}(\big\|c^\UU\big\|_1\big\|c^{-\VV}\big\|_1 + \big\|c^{-\UU}\big\|_1\big\|c^{\VV}\big\|_1 + \big\|c^\WW\big\|_1\big\|c^{-\WW^T}\big\|_1 + \big\|c^{-\WW}\big\|_1\big\|c^{\WW^T}\big\|_1) \\
        &= \frac{8}{(n+m)^2}\big\|c^\WW\big\|_1\big\|c^{-\WW}\big\|_1.
    \end{align*}
    To bound $\lambda_{-}\defeq\sum_{\lambda_i < 0}|\lambda_i|$ from above%the total magnitude of eigenvalues
    , we will first introduce two new notations for convenience. For any two matrices $\mathbf{A}$ and $\mathbf{B}$, let $\mathbf{A} \ominus \mathbf{B}$ denote a matrix operation analogous to the Kronecker product but with subtraction in place of multiplication. In addition, define $\ch:\R\to\R$ as $\ch(a) \defeq c^a + c^{-a}$ and its entrywise counterpart for matrices $\ch:\R^{p\times q}\to\R^{p\times q}$, so that we are able to compactly write
    \begin{align*}
    \frac{1}{2}\HH &= \ch(\UU\ominus\VV) + \ch(\WW\ominus\WW^T)\KK \\
    &= \ch(\UU\ominus\VV) + \ch\big((\WW\ominus\WW^T)\KK\big).
    \end{align*}
    Note that $\big\langle \ch(\mathbf{A}), \ch(\mathbf{B}) \big\rangle = \big\|\ch(\mathbf{A}+\mathbf{B})\big\|_1 + \big\|\ch(\mathbf{A}-\mathbf{B})\big\|_1$. Furthermore, if $\|\mathbf{A}\|_1$ and $\|\mathbf{A}\|_\infty$ are fixed, $\big\|\ch(\mathbf{A})\big\|_1$ is maximized by the highest possible count of entries of $\mathbf{A}$ equal to $\big\|\mathbf{A}\big\|_\infty$ due to the superadditivity of $\ch$ (see also a proof based on Lagrange multipliers under ``\href{https://math.stackexchange.com/questions/1355638/upper-bound-on-sum-of-exponential-functions}{upper bound on sum of exponential functions}'' on Mathematics Stack Exchange). As a consequence, for $\mathbf{A} \in \R^{p\times q}$
    \begin{align*}
        \big\|\ch(\mathbf{A})\big\|_1 &\leq \left\lceil\frac{\|\mathbf{A}\|_1}{\|\mathbf{A}\|_\infty}\right\rceil\ch\big(\|\mathbf{A}\|_\infty\big) + \left\lfloor pq - \frac{\|\mathbf{A}\|_1}{\|\mathbf{A}\|_\infty}\right\rfloor \ch(0)\\
        &\leq \left(\frac{\|\mathbf{A}\|_1}{\|\mathbf{A}\|_\infty} + 1\right)\left(\ch\big(\|\mathbf{A}\|_\infty\big) - 2\right) + 2pq \\
        &\leq \left(\sqrt{pq}\frac{\|\mathbf{A}\|_2}{\|\mathbf{A}\|_\infty} + 1\right)\left(\ch\big(\|\mathbf{A}\|_\infty\big) - 2\right) + 2pq.
    \end{align*}
    The looser bound in terms of the Frobenius norm allows for its tractable computation when $\mathbf{A} = \mathbf{B}\ominus\mathbf{C}$ for some $\mathbf{B} \in \R^{p\times q}, \mathbf{C} \in \R^{r \times s}$ with non-negative entries, as then
    $$\|\mathbf{A}\|_2^2 = \|\mathbf{B}\ominus\mathbf{C}\|_2^2 = pq\|\mathbf{B}\|_2^2 + rs\|\mathbf{C}\|_2^2 - 2\|\mathbf{B}\|_1\|\mathbf{C}\|_1.$$
    
    Next, we bound the eigenvalues' total magnitude from above. Using the symmetry of $\HH$,
    \begin{align*}
        \hspace{-1.5cm}\frac{1}{4}\sum_{i=1}^{(n+m)^2}\lambda_i^2 &= \left\|\frac{1}{2}\HH\right\|_2^2 \\
        &= \big\|\ch(\UU\ominus\VV)\big\|_2^2 + \big\|\ch(\WW\ominus\WW^T)\big\|_2^2 + 2\Big\langle \ch(\UU\ominus\VV), \ch\big((\WW\ominus\WW^T)\KK\big)\Big\rangle \\
        &= \big\|\ch(2\UU\ominus2\VV)\big\|_1 + \big\|\ch(2\WW\ominus2\WW^T)\big\|_1 + 4(n+m)^4
        \\&\hspace{1cm}+ 
        % \begin{aligned}
        2\Big\|\ch\big(\UU\ominus\VV + (\WW\ominus\WW^T)\KK\big)\Big\|_1 +2\Big\|\ch\big(\UU\ominus\VV - (\WW\ominus\WW^T)\KK\big)\Big\|_  1 \\
        %&< ((n+m)^2\frac{\|\UU\ominus\VV\|_2 + \|\WW\ominus\WW^T\|_2}{\dmax}+2)(\ch(2\dmax) - 2) + 4(n+m)^4 + 4(n+m)^2 \\
        %&\hspace{1cm}+ ((n+m)^2\frac{\|\UU\ominus\VV + (\WW\ominus\WW^T)\KK)\|_2 + \|\ldots\|_2}{\dmax} + 4)(\ch(2\dmax) - 2) + 8(n+m)^4 \\
        &\leq \Bigg((n+m)^2\frac{\|\UU\ominus\VV\|_2 + \|\WW\ominus\WW^T\|_2 + \big\|\UU\ominus\VV + (\WW\ominus\WW^T)\KK\big\|_2 + \big\|\UU\ominus\VV - (\WW\ominus\WW^T)\KK\big\|_2}{\dmax}
        \\
        &\hspace{1cm}+6\Bigg)\big(\ch(2\dmax) - 2\big) + 16(n+m)^4.
        % \end{aligned}
        % &= 4(\|c^\WW \otimes c^{-\WW^T} + c^{-\WW} \otimes c^{\WW^T}\|_2^2 + \|c^\UU \otimes c^{-\VV} + c^{-\UU} \otimes c^{\VV}\|_2^2 \\ &\hspace{2cm} +  \langle(c^\WW \otimes c^{-\WW^T} + c^{-\WW} \otimes c^{\WW^T})\KK, c^\UU \otimes c^{-\VV} + c^{-\UU} \otimes c^{\VV}\rangle) \\
        % &= 4(\sum_{i,j,h,k=1}^{n+m}((c^{W_{ij}-W_{hk}} + c^{W_{hk}-W_{ij}})^2 + (c^{U_{ij}-V_{hk}} + c^{V_{hk}-U_{ij}})^2) + ).
    \end{align*}
    From
    \begin{align*}
        \big\|\UU\ominus\VV \pm (\WW\ominus\WW^T)\KK\big\|_2 &= \sqrt{\|\UU\ominus\VV\|_2^2 + \|\WW\ominus\WW^T\|_2^2 \pm 2\big\langle(\WW\ominus\WW^T)\KK, \UU\ominus\VV\big\rangle}
    \end{align*}
    and \begin{align*}
        \big\langle(\WW\ominus\WW^T)\KK, \UU\ominus\VV\big\rangle &= \sum_{i,j,h,k=1}^{n+m}(W_{ik} - W_{jh})(U_{ij} -V_{hk}) \\ &=\sum_{i,j,h,k=1}^{n+m}(W_{ik}U_{ij} - W_{jh}U_{ji} - W_{ik}V_{hk} + W_{jh}V_{kh}) \\
        &= (n+m)\sum_{i,j,k=1}^{n+m}(W_{ij}U_{ik} - W_{ij}U_{ik} + W_{ji}V_{ki} - W_{ji}V_{ki}) \\
        &= 0,
    \end{align*}
    it follows that
    \begin{align*}
        \frac{1}{4}\sum_{i=1}^{(n+m)^2}\lambda_i^2
        &\leq \Bigg((n+m)^2\frac{\|\UU\ominus\VV\|_2 + \|\WW\ominus\WW^T\|_2 + 2\sqrt{\|\UU\ominus\VV\|_2^2 + \|\WW\ominus\WW^T\|_2^2}}{\dmax}
        \\
        &\hspace{1cm}+6\Bigg)\big(\ch(2\dmax) - 2\big) + 16(n+m)^4 \\
        &= \left((n+m)^2\frac{(2\sqrt{2} + 4)\sqrt{(n+m)^2\|\WW\|_2^2 - \|\WW\|_1^2}}{\dmax}+6\right)\big(\ch(2\dmax) - 2\big) + 16(n+m)^4 \\
        &= \pmax\big(\ch(2\dmax) - 2\big) + 16(n+m)^4
    \end{align*}
    and therefore
    \begin{align*}
        \lambda_- &\leq \sum_{i=2}^{(n+m)^2}|\lambda_i| \\
        &\leq \sqrt{\big((n+m)^2 - 1\big)\left(-\lambda_1^2 + \sum_{i=1}^{(n+m)^2}\lambda_i^2\right)} \\
        &\leq 2(n+m)\sqrt{16(n+m)^4 + \pmax\big(\ch(2\dmax) -2\big) - \frac{16}{(n+m)^4}\big\|c^\WW\big\|_1^2\big\|c^{-\WW}\big\|_1^2} \\
        &= 4(n+m)^2\frac{\alpha}{\frac{1}{2} - \alpha}.
    \end{align*}
    Finally,
    \begin{align*}
        \nconv(\sigma) &= \frac{\lambda_-}{2\lambda_- + 8(n+m)^2} \\
        &= \frac{1}{2} - \frac{2(n+m)^2}{\lambda_- + 4(n+m)^2} \\
        &\leq \frac{1}{2} - \frac{1}{\frac{2\alpha}{\nicefrac{1}{2}-\alpha} + 2} \\
        &= \alpha.
    \end{align*}
\end{proof}
