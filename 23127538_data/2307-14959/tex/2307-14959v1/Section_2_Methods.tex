




\section{Methodology} 

\Cref{fig:attrib_imb} shows the overview of our Fed-MAS framework. Each local client is provided with a publicly self-supervised pre-trained model (e.g., MoCo-RN50~\cite{He2020MomentumCF_moco}) that is not involved in the training or communication process of the federated learning framework. Consequently, these pre-trained models do not increase communication costs while ensuring that each client can access the same consistent pre-trained model. With $n$ local clients and one global server, Fed-MAS performs the following steps in each round: (1) Each client receives the global model to measure its global class-aware divergence, $w_k$, and update its local model; (2) Each client trains its local model while estimating its class-aware divergence, $\hat{w}_k$; (3) Each client corrects $\hat{w}_k$ with ${w_k}$ to generate a rescue scalar, $RF$;
(4) Client uploads the parameters of its local model and $RF$ to the server;
(5) The server applies our proposed MAS to aggregate a new model from the parameters of the received client models, weighted by $RF$;


\subsection{Class Aware Global Observation via Self-Supervised Priors}


In highly imbalanced medical image datasets, both extreme class imbalance and inter-client intra-class variations can lead to client drift. Due to the decentralization of data, estimating the global intra-class attribute distribution in medical imaging within the FL framework is a challenge that is yet to be explored.



At the beginning of each round in the FL process, each client receives the model from the global server $\theta_{global}$. We study locally the distance between the distribution of the self-supervised pre-trained model, $f_{\xi}$, and $\theta_{glob}$ over each client's local data.

Given an input image $x$, we feed $x$ to the local feature encoder $g$ to generate a representation $z=g_\theta(x)$. This representation is then fed to an $MLP$ projector to generate a projection $y=MLP_\theta(z)$ in a space comparable with the self-supervised model. From the same discriminative pre-trained model in all clients, we can get a target  representation $y' = f_\xi(x)$, where both $y$ and $y'$ are L2 normalized. We can measure the distribution difference using mean squared error as:
\begin{equation}
    \mathcal{L}^\theta_{f} = 2 - 2\cdot{\langle y,  y' \rangle}
    \cdot
    \label{eq:cosine-loss}
\end{equation}

From~\cref{eq:cosine-loss}, we can generate a class-aware distance for class $k$ with $M_{k}$ total samples as:
\begin{equation}
   \mathcal{L}^\theta_{k} =\frac{1}{M_{k}}\sum\limits_{i = 1}^{M_{k}}  \mathcal{L}_{f}^\theta(x_{k,i}).
   \label{eq:loss_summation_over_theta}
\end{equation}
We define $w_k  = \mathcal{L}_{k}^{\theta_{glob}}$. 
The factor $w_k$ can help to capture the distance in distribution between the global server and the self-supervised model on each client's local data. This divergence can provide insights into the sensitivity of the global model, $\theta_{glob}$, in effectively capturing the specific class attribute in each client's local data. 
A high $w_{k}$ indicates the failure of $\theta_{glob}$ in capturing a local class $k$. In~\cref{fig:RF} (a), we can see that $w_{k}$ is inversely proportional to the global class distribution, even if the local client distribution is not necessarily the same.

% Figure environment removed 

\subsection{State Estimation via Knowledge Distillation}
While $w_k$ provides class-aware global divergence measurement with the same consistent local frozen self-supervised model, a client receives the global model, $\theta_{glob}$, and takes subsequent optimization steps for $E$ local epochs with uncertainty to generate $\theta'_{c}$. Hence, the client's drift from the global model is hideous after its uncertain optimization.

With a running average, a client can provide a class-aware divergence likelihood $\hat{w}_{k}$, where $\hat{w}_{k} =\sum\limits_{e = 1}^{E}\mathcal{L}^{\theta'_{c}}_{k}$. The factor $\hat{w}_{k}$ can help to capture how far the client drifted from $f_{\xi}$ since the global measurement, $w_{k}$, was taken. A client can then correct this estimation, $\hat{w}_{k}$, with the global observation, $w_{k}$, to generate a posterior rescue factor, $RF$, in every round.
\begin{equation}
    RF=  \sum\limits_{k=1}^{K}  w_{k} \hat{w}_{k} . \label{eq:rescue_factor}
\end{equation}


A higher $RF$ indicates that the client has information that the global model has not appropriately captured.


To train the projector ${MLP_\theta(\cdot)}$, we propose to minimize~\cref{eq:cosine-loss} along with the local balanced risk minimization~\cite{Ren2020balms} to 
minimize a total loss $\mathcal{L}_{total}$ concerning $\theta$ only as: 
\begin{equation}
\mathcal{L}_{total} = \mathcal{L}_{sup}+ \lambda_{f} ~\mathcal{L}_{f},
\label{eq:loss_total}
\end{equation}
where $\mathcal{L}_{sup}$ refers to the original supervised loss and $\lambda_{f}$ as a weighting factor. 
This can be seen as restricting the client optimization direction. However,
the self-supervised model ensures clients align with a common reference distribution and possess implicit regularization capabilities for minority classes through generalizable features~\cite{Elbatel2023FoProKDFP}.











\subsection{Model Aggregation via Self-Supervised Posteriors}
Inspired by the fact that client-specific models should contribute more to the global server to capture local variance, we propose a novel model aggregation via the corrected self-supervised posteriors (MAS) \marwan{should the corrected value be posterior not prior?}. We use our proposed $RF$ to indicate client-specific models that should contribute more to the global model than client-generic models to capture their attribute-class variations. While our proposed $RF$ can be used for biased client selection~\cite{loss_selection}, we use it to aggregate a global model. Instead of aggregating based on the weighted samples as in FedAvg~\cite{McMahan2017CommunicationEfficientLO_fedavg}, we propose to weight the global model, $\theta_{glob}$, based on the $RF$ value as follows:
\begin{equation}
    \bar{RF}_c ={\frac{RF_{c}}{\sum\limits_{j}RF_{j}}} \; \text{, and} \; 
    {\theta^{r+1}_{glob} = \sum\limits_{c = 1}^C  \bar{RF}_c {\theta'_{c}}}.
    \label{eq:tempavg} 
\end{equation}
% \vspace{-5pt}
% \begin{equation}
%     \theta^{r+1}_{glob} = \sum\limits_{c = 1}^C{\frac{RF_{c}}{\sum\limits_{j}RF_{j}}  {\theta_{c}}}
%     \label{eq:tempavg}
% \end{equation}
For instance, Client 3,4,5 in~\cref{fig:RF} (b) have mostly minority classes and contribute the most to $\theta_{glob}$ in~\cref{fig:RF} (c). Morever, in~\cref{fig:RF_isic} (a) Client ISIC-3 have mostly underrepresented attribute and contributes the most in ~\cref{fig:RF_isic} (b). Additionally, we show in~\cref{fig:RF} (d) and ~\cref{fig:RF_isic} (c) that the rescue factor for all clients is decreasing throughout rounds. This highlights the ability of MAS to accommodate different clients. (See~\cref{algo:fedfree} in Appendix). 


