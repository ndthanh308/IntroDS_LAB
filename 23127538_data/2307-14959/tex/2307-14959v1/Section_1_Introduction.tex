
% Figure environment removed

\section{Introduction} 


Federated learning (FL) has emerged as a way to train models with decentralized data while preserving privacy. Due to the inherent nature of data heterogeneity in medical imaging, training in a decentralized manner exhibits performance degradation compared to centralized training. With FedAvg~\cite{McMahan2017CommunicationEfficientLO_fedavg} as the main baseline, multiple works proposed to improve the model's generic performance under data decentralization~\cite{li2021model_moon,LiSZSTS20_fedprox,mendieta2022local_fedalign}. These methods have been successful in achieving positive results, assuming a balanced global data distribution. However, they struggle to address extreme data heterogeneity, especially in highly imbalanced medical datasets. There have been some methods proposed to address the imbalanced setting~\cite{Mu2021FedProcPC,10.1007/978-3-030-87199-4_31_fedirm}. Nevertheless, these methods shared local features among clients, which may raise privacy concerns.

Label distribution skewness has been studied in the context of FL~\cite{pmlr-v162-zhang22p_fedlc}. FedLC~\cite{pmlr-v162-zhang22p_fedlc}, inspired by LDAM~\cite{cao2019learning_LDAM}, showed promising results by adjusting the local client class distribution. Additionally, multiple works proposed to tackle the issue of highly skewed label distribution (i.e. long-tailed) by decoupling the classifier and the feature extractor~\cite{ijcai2022p308_creff,Wicaksana2023FCATL_taming,Chen2022TowardsFL_hualiang_LT}. The rationale behind these methods is rooted in the understanding that the classifier is the bottleneck for majority class bias~\cite{Kang2020Decoupling}. 
For instance, CReFF~\cite{ijcai2022p308_creff} retrained a balanced classifier on the server by leveraging federated features. A notable limitation of classifier re-training is its inability to address the intra-class attribute imbalance. Recently,~\cite{10.1007/978-3-031-20053-3_41_IF_dual} showed that training with imaging data with high attribute imbalance impedes representation learning by exacerbating the intra-class variations. In FL, the issue of intra-class imbalance is critical when dealing with highly imbalanced medical imaging datasets. As depicted in~\cref{fig:attrib_imb} (a), different skin tones can arise across different clients for the same class~\cite{10.1007/978-3-031-16852-9_1_skin_tone_dart}. For gastrointestinal recognition depicted in~\cref{fig:attrib_imb} (b), different findings can arise in different clients for the same class~\cite{Borgli2019HyperKvasirAC}. Hence, the challenge of an unbiased robust global model that takes into account both the attribute and class imbalance still remains. More recently, FedCE~\cite{jiang2023fedce} showed promising results by calculating a fair client contribution estimation in gradient and data space for medical image segmentation; Nevertheless, it relies on local validation samples, which may not adequately represent attribute imbalance and rare diseases in highly imbalanced medical image datasets.

Publicly available pre-trained models, such as MoCo-V2~\cite{He2020MomentumCF_moco} that were trained without any labels using a large set of naturals images, have been utilized with their batch statistics in calculating image priors~\cite{Hatamizadeh2022GradViTGI} and have been utilized with their generalizable representation to improve the performance in highly imbalanced medical imaging tasks~\cite{ding2022free}. In this paper, we leverage these pre-trained models locally to propose Fed-MAS as a novel approach to incorporate the client's local variations with consistent self-supervised priors, estimating client contributing ratios toward an unbiased robust global model.



