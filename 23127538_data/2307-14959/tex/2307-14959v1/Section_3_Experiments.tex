
% Figure environment removed 
\section{Experiments}

\noindent{\textbf{Dataset.}}~\textbf{HyperKvasir}~\cite{Borgli2019HyperKvasirAC} is a long-tailed (LT) dataset of 10,662 gastrointestinal tract images with 23 classes from different anatomical and pathological landmarks and findings. We divide the 23 classes into Head ($\textgreater$ 700 images per class), Medium (70 $\sim$ 700 images per class), and Tail ($\textless$ 70 images per class) with respect to their class counts. Additionally, we partition the data across eight clients with IID (similar label distributions) and non-IID (heterogeneous partition with Dirichlet distribution). \textbf{ISIC}~\cite{Combalia2019BCN20000DL_isic2019} is a highly imbalanced dataset of skin lesion images with 8 classes that exhibits skin-tone attribute imbalance~\cite{10.1007/978-3-031-16852-9_1_skin_tone_dart}. For instance, melanoma incidence is lower in quantity and higher in mortality rates in black patients than in others~\cite{Collins2011RacialDI}. We partition the dataset on four clients based on two attributes, light and dark skin tones, with~\cite{10.1007/978-3-031-16852-9_1_skin_tone_dart} labeling. Additionally, we split the data between the four clients for training, validation, and testing with 70\%, 15\%, and 15\%, respectively. We also benchmark Fed-MAS over Flamby-ISIC split~\cite{NEURIPS2022_232eee8e_flamby} with six different hospitals with stratified 5-fold cross-validation.
 

% \noindent \textbf{ISIC Attribute Split} To investigate the inter-client intra-class variations within a specific attribute, we conduct our study on the ISIC dataset using the skin color attribute. This choice aims to replicate real-world attributes and their distributions.



\noindent\textbf{Implementation Details.}
For both datasets, we use resnet-18~\cite{7780459_resnet} as the local target model. For the long-tailed HyperKvasir dataset, we employ an SGD optimizer and a cosine annealing scheduler~\cite{loshchilov2017sgdr_cosine} with a maximum learning rate of 0.1. For ISIC, we employ Adam optimizer with the 3e-4 learning rate. Additionally, we employ balanced risk minimization~\cite{Ren2020balms} and train methods for 200 communication rounds with 10 local epochs. We set $\lambda_{f}$ to 3 and provide an ablation in~\cref{apx:tbl:free_ablation} in Appendix.

\input{Section_3_sota_table}

\noindent \textbf{Evaluation Metrics.} We evaluate the model performance of the global model in this paper. To assess the unequal treatment of each class in HyperKvasir, we report the top-1 accuracy on shot-based division (head, medium, tail) and their average results denoted as ``All'' as existing works~\cite{10.1007/978-3-031-16437-8_44_monash_sampling_LT}. Following prior work~\cite{10.1007/978-3-030-87240-3_31_balanced_mixup,10.1007/978-3-031-17027-0_3_chest_LT_benchmark,reinke2022metrics}, we also report the Balanced Accuracy ``B-Acc'', which calculates the average per-class accuracy and is resistant to class imbalance. As the test set of HyperKvasir contains only 12 classes, we follow previous work~\cite{10.1007/978-3-030-87240-3_31_balanced_mixup} to assess the model performance with a stratified 5-fold cross-validation. To evaluate the performance of attributes in ISIC-FL, we report the ``B-Acc'' separately for each attribute (``Light'', ``Dark'') and the average of these scores ``Avg''. Additionally, we report the overall ``B-Acc'' across all attributes and distributions.


\subsection{Performance on the HyperKvasir}

We compare our methods with FL-methods~\cite{McMahan2017CommunicationEfficientLO_fedavg,LiSZSTS20_fedprox,li2021model_moon}, LT-integrated FL methods~\cite{cao2019learning_LDAM,Ren2020balms}, and label-skew FL methods~\cite{pmlr-v162-zhang22p_fedlc,ijcai2022p308_creff}



\noindent \textbf{FL-Methods}~\cite{McMahan2017CommunicationEfficientLO_fedavg,LiSZSTS20_fedprox,li2021model_moon}. 
One simple solution for federated learning with highly imbalanced medical data is to apply existing FL methods to our setting directly. 
To this end, we compare our methods with state-of-the-art FL methods, including FedAvg~\cite{McMahan2017CommunicationEfficientLO_fedavg}, FedProx~\cite{LiSZSTS20_fedprox}, and MOON~\cite{li2021model_moon}, under the same setting. As shown in~\Cref{tbl:sota}, we find that our method outperforms the best existing FL method MOON by 2.85\% and 5.68\% on ``B-acc'' in both IID and non-IID settings, respectively. Notably, our Fed-MAS achieves similar results with MOON~\cite{li2021model_moon} on the ``Head'' while reaching large improvements on the ``Tail'' (11.9\% on iid and 10.71\% on non-iid), showing that our Fed-MAS can tackle LT distribution under FL more effectively. 
The limited results could be attributed to the use of local empirical risk minimization in MOON~\cite{li2021model_moon}. However, even when we applied a balanced risk minimization~\cite{Ren2020balms} in MOON, our method still outperformed it (60.69\% vs. 62.08\% on ``B-acc'' for IID); see results in~\Cref{apx:tbl:FL_BRM} in Appendix. 





\noindent \textbf{LT integrated FL methods}~\cite{cao2019learning_LDAM,Ren2020balms}. To design FL methods for local clients with long-tailed distribution, a straightforward idea is to directly use LT methods in each local client and then use an FL framework such as FedAvg to obtain the final results. In this regard, we implement LDAM-DRW~\cite{cao2019learning_LDAM} and BSM~\cite{Ren2020balms} into the FedAvg framework and rename them as LDAM-FL and BSM-FL respectively.
From Table~\ref{tbl:sota}, we can notice the LT methods utilizing  an FL framework have produced limited results on ``Tail'' primarily due to the extreme client drifting phenomenon.
Please note that Fed-MAS does not focus on designing any specific long-tailed training for each local client. Instead, MAS enables the global server to effectively aggregate the model parameters from long-tailed distributed local clients.
As a result, our Fed-MAS can successfully capture the ``Tail'' with a 6.84\%  accuracy gain on IID with lower variance than the best-performing LT method BSM-FL~\cite{Ren2020balms}. Notably, our method consistently outperforms the best-performing LT method on the ``B-acc'' with a lower variance (improvement of 1.8\% on IID and 2.46\% on non-IID). 
 

\noindent \textbf{Label-Skew FL} We compare our method with the state-of-the-art label-skew FL method, FedLC~\cite{pmlr-v162-zhang22p_fedlc}, and the highly labeled skew (i.e. long-tailed) FL method, CReFF~\cite{ijcai2022p308_creff}. CReFF, as proposed by~\cite{ijcai2022p308_creff}, involves a method of re-training the classifier by utilizing learnable features on the server at each communication round, holding an equal treatment of all clients' models. However, this technique fails to accommodate inter-client intra-class variations which could arise.
From~\Cref{tbl:sota}, we can notice that FedAvg with local LT such as BSM-FL~\cite{Ren2020balms} can outperform CReFF~\cite{ijcai2022p308_creff} on the HyperKvasir dataset in both IID and non-IDD by 2.4\% and 1.8\% on ``B-acc'', respectively. 
Our comparative analysis illustrates that Fed-MAS consistently outperforms CReFF in both IID and non-IID by 4.2\% and 4.27\% on ``B-acc'', respectively, by incorporating the client's local variations with MAS. FedLC~\cite{pmlr-v162-zhang22p_fedlc} proposes a loss function to address label distribution skewness by locally calibrating logits and reducing local bias in learning. Their modification yields compelling performance. Nevertheless, our method surpasses them in both IID and non-IID, achieving improvements of 1.0\% and 1.83\% on ``B-Acc'', respectively. Remarkably, our method effectively captures the tail classes with reduced variance in both IID and non-IID, exhibiting improvements of 8.5\% and 9.6\%, respectively, while experiencing only a minor drop in performance for the head classes (96.5\% vs 94.3\% for IID and 95.8\% vs 93.0\% for non-IID).


% \noindent \textbf{Fed-LT methods}~\cite{ijcai2022p308_creff} We compare our method with the state-of-the-art Fed-LT method CReFF~\cite{ijcai2022p308_creff}.

% drifting issues with our proposed FLKD (Fast and Convenient Local Model) and DLMA (Robust Estimation for the Global Model) innovations. 


\begin{table}[t!]
\centering
\caption{Ablation of minimizing~\cref{eq:cosine-loss} (KD) and MAS on HyperKvasir non-IID}
\setlength{\tabcolsep}{12pt}
\resizebox{0.8\columnwidth}{!}{
\begin{tabular}{c|c|c|ccc}
\toprule
\multirow{2}{*}{} &\multirow{2}{*}{KD}& \multirow{2}{*}{MAS} & \multicolumn{3}{c}{Metrics}    \\  
\cline{4-6}& & & All (\%)&B-acc (\%) &p-value
   \\ 
   \hline
BSM-FL~\cite{Ren2020balms} (Baseline) & $\times$&$\times$
&58.24 $\pm$ 1.2&
59.15 $\pm$ 1.3 & \textemdash
\\
~\cite{Ren2020balms} w/ KD & \checkmark&$\times$
&59.26 $\pm$ 1.2
& 60.19 $\pm$ 1.1& $<$0.001
\\
\textbf{Fed-MAS}&\checkmark& \checkmark 
&\textbf{60.57 $\pm$ 1.1}
&\textbf{61.61 $\pm$ 1.0} & $<$0.001
\\
\bottomrule
\end{tabular}}
\label{tbl:ablation_dlma_flkd}
% \vspace{-0.5em}
\end{table}

\noindent \textbf{Effectiveness of KD and MAS} 
As shown in~\Cref{tbl:ablation_dlma_flkd}, minimizing~\cref{eq:cosine-loss} (KD) can enhance the ``All'' and ``B-Acc'' via 1.02\% and 1.04\% due to the implicit regularization of MoCo-V2 on the tail classes for extreme imbalance datasets. 
With both KD and MAS, the performance is further improved to the best via 2.33\% and 2.46\% on ``All'' and ``B-Acc'', respectively. MAS utilizes unbiased frozen generalizable representations to incorporate the inter-client intra-class characteristics in FL and combine them with the drifting belief. This combination helps in capturing client-specific models in the aggregation step.

\begin{table}[t]
  \centering
  \caption{Experimental Results on ISIC-FL. Results are averaged over 5 folds.}
  \setlength{\tabcolsep}{12pt}
  \label{tbl:sota_results_isic_fl}
  \resizebox{0.9\columnwidth}{!}{
  \begin{tabular}{l|cc|cc|c}
    \hline
    \multirow{3}{*}{Method}
     & \multicolumn{4}{c|}{\textbf{Attribute Setting (ours)}} & {Flamby-ISIC~\cite{NEURIPS2022_232eee8e_flamby}}
     \\ \cline{2-6}
     & Light &Dark&Avg&B-Acc &B-Acc
     \\
    \hline
    
%   &\multicolumn{5}{c}{w/o Weight Initialization}
%     \\
% \hline

% FedLC~\cite{pmlr-v162-zhang22p_fedlc} & 59.53 $\pm$ 2.7&54.21 $\pm$ 4.3&56.87 $\pm$ 2.2 &59.13 $\pm$ 2.4 
% & 61.26 $\pm$ 3.0
% \\
% BSM-FL~\cite{Ren2020balms} (Baseline) 
% &62.04 $\pm$ 1.7&59.59 $\pm$ 8.4&60.82 $\pm$ 3.7 &62.47 $\pm$ 1.5 

% & 65.89 $\pm$ 2.9
% \\

% ~\cite{Ren2020balms} {w/ KD}& 
% 62.75 $\pm$ 3.2&59.27 $\pm$ 8.7&61.01 $\pm$ 3.8 &63.13 $\pm$ 2.7 
% & 0
% \\

% \textbf{Fed-Mas (ours)}&
% 62.08 $\pm$ 2.2&62.06 $\pm$ 7.2&\textbf{62.07 $\pm$ 3.5 }&\textbf{63.32 $\pm$ 2.4}
% & 0
% \\
\hline
  & \multicolumn{5}{c}{With ImageNet Weight Initialization}
    \\
    \hline
FedLC~\cite{pmlr-v162-zhang22p_fedlc} & 71.11 $\pm$ 1.8&73.64 $\pm$ 6.6&72.38 $\pm$ 2.9 &71.63 $\pm$ 1.6 &
76.54 $\pm$ 2.6
\\
BSM-FL~\cite{Ren2020balms} (Baseline) &
73.88 $\pm$ 1.4&74.78 $\pm$ 5.4&74.33 $\pm$ 2.5 &74.49 $\pm$ 1.4 &
78.19 $\pm$ 1.8
\\
~\cite{Ren2020balms} {w/ KD}
&73.87 $\pm$ 1.6&72.44 $\pm$ 5.9&73.16 $\pm$ 3.0 &74.09 $\pm$ 1.5 &
79.17 $\pm$ 2.1
\\
\textbf{Fed-Mas (ours)}
&73.43 $\pm$ 1.6&77.0 $\pm$ 6.6&\textbf{75.21 $\pm$ 2.9} &\textbf{74.61 $\pm$ 1.4} &
\textbf{80.87 $\pm$ 2.2}
\\

% Mas-Inv (ours) &75.02 $\pm$ 1.9&77.06 $\pm$ 6.0&76.04 $\pm$ 3.0 &76.04 $\pm$ 1.9 \\

\bottomrule
  \end{tabular}
  }
    
    % \vspace{-0.5em}
\end{table}

\subsection{Performance on ISIC-FL}
We evaluate the best-performing and competitive methods with the ISIC-FL dataset to shorten the benchmark. While previous studies neglect weight initialization to provide better convergence analysis as pre-trained weights are architecture dependent. Recently,~\cite{nguyen2023where_to_begin} and~\cite{chen2023on_pre_trained_fl_1} studied the impact of pre-training initialization on reducing the data and system heterogeneity in FL. We present in~\Cref{tbl:sota_results_isic_fl} the results of the most competitive methods 
with weight initialization
on the ISIC-FL attribute setting.
% (See~\cref{tblxxx} in Appendix w/o weight initalization).
FedLC~\cite{pmlr-v162-zhang22p_fedlc} demonstrates compelling performance to address label skewness in Hyperkvasir-FL. Nevertheless, it falls short in accommodating attribute heterogeneity in ISIC-FL due to its local learning focus. Our method consistently outperforms FedLC~\cite{pmlr-v162-zhang22p_fedlc} with a notable improvement of 2.8\% and 3.0\% in terms of the averaged balanced accuracies ``Avg'' and balanced accuracy ``B-acc'' respectively. 
% when clients' model weights are not available. 
% When the client's model weights are available and initialized with ImageNet weights, the improvements are x.x\% and x.x\% on the ``Avg'' 
% and ``B-acc'' respectively. 
Compared to the baseline~\cite{Ren2020balms}, Fed-MAS notably captured the underrepresented attribute with 2.2\% on the ``B-acc'' of the ``Dark Attribute'' with a minimal drop of 0.5\% on the ``B-acc'' of the ``Light Attribute'', balancing the intra-class attribute characteristics in FL. On the highly heterogeneous Flamby-ISIC split resembling six hospitals, Fed-MAS outperform FedLC and the baseline on the ``B-acc'' with 4.33\% and 2.68\%, respectively.

% Our method
% when clients' models are not available, and x.x\% and  x.x \% on ``Avg'' and ``B-Acc'' respectively when client's model weights are available. 

\subsection{Privacy Concerns}
Similarly to traditional FL methods~\cite{McMahan2017CommunicationEfficientLO_fedavg,li2021model_moon,LiSZSTS20_fedprox}, Fed-MAS shares the model weights with an additional scalar, $RF$, which protects data privacy by not revealing input data or label distribution. The \textit{scalar}, $RF$, is calculated in the output feature space, safeguarding the input data distribution. Moreover, $RF$ poses uncertainty in approximating the client's label distribution as it can be influenced by diverse attributes in the majority class or a common attribute in the minority class.
