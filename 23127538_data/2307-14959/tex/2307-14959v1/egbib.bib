@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})
@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})
@String(ICML = {ICML})

@String(PMLR = {Proceedings of Machine Learning Research

})

@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{cui2019class_cb_loss,
  title={Class-balanced loss based on effective number of samples},
  author={Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9268--9277},
  year={2019}
}
@article{ZHANG202336_bkd,
title = {Balanced knowledge distillation for long-tailed learning},
journal = {Neurocomputing},
volume = {527},
pages = {36-46},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.01.063},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223000711},
author = {Shaoyu Zhang and Chen Chen and Xiyuan Hu and Silong Peng},
keywords = {Long-tailed learning, Knowledge distillation, Vision and text classification},
abstract = {Deep models trained on long-tailed datasets exhibit unsatisfactory performance on tail classes. Existing methods usually modify the classification loss to increase the learning focus on tail classes, which unexpectedly sacrifice the performance on head classes. In fact, this scheme leads to a contradiction between the two goals of long-tailed learning, i.e., learning generalizable representations and facilitating learning for tail classes. In this work, we explore knowledge distillation in long-tailed scenarios and propose a novel distillation framework, named Balanced Knowledge Distillation (BKD), to disentangle the contradiction between the two goals and achieve both simultaneously. Specifically, given a teacher model, we train the student model by minimizing the combination of an instance-balanced classification loss and a class-balanced distillation loss. The former benefits from the sample diversity and learns generalizable representation, while the latter considers the class priors and facilitates learning for tail classes. We conduct extensive experiments on several long-tailed benchmark datasets and demonstrate that the proposed BKD is an effective knowledge distillation framework in long-tailed scenarios, as well as a competitive method for long-tailed learning. Our source code is available: https://github.com/EricZsy/BalancedKnowledgeDistillation.}
}

@article{fang2021contrastive_cmi,
  title={Contrastive Model Inversion for Data-Free Knowledge Distillation},
  author={Fang, Gongfan and Song, Jie and Wang, Xinchao and Shen, Chengchao and Wang, Xingen and Song, Mingli},
  journal={arXiv preprint arXiv:2105.08584},
  year={2021}
}
@article{Wang2023FVPFV,
  title={FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation},
  author={Yan Wang and Jian Cheng and Yixin Chen and Shuai Shao and Lanyun Zhu and Zhenzhou Wu and T. Liu and Haogang Zhu},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.13672}
}
@INPROCEEDINGS{9157759_loss_bn,
  author={Ye, Jingwen and Ji, Yixin and Wang, Xinchao and Gao, Xin and Song, Mingli},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN}, 
  year={2020},
  volume={},
  number={},
  pages={12513-12522},
  doi={10.1109/CVPR42600.2020.01253}}
@article{YANG2022102457,
title = {Source free domain adaptation for medical image segmentation with fourier style mining},
journal = {Medical Image Analysis},
volume = {79},
pages = {102457},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102457},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001049},
author = {Chen Yang and Xiaoqing Guo and Zhen Chen and Yixuan Yuan},
keywords = {Source Free Domain Adaptation, Fourier Style Mining, Contrastive Domain Distillation, Consistency Learning},
abstract = {Unsupervised domain adaptation (UDA) aims to exploit the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled target domain. Existing UDA techniques typically assume that samples from source and target domains are freely accessible during the training. However, it may be impractical to access source images due to privacy concerns, especially in medical imaging scenarios with the patient information. To tackle this issue, we devise a novel source free domain adaptation framework with fourier style mining, where only a well-trained source segmentation model is available for the adaptation to the target domain. Our framework is composed of two stages: a generation stage and an adaptation stage. In the generation stage, we design a Fourier Style Mining (FSM) generator to inverse source-like images through statistic information of the pretrained source model and mutual Fourier Transform. These generated source-like images can provide source data distribution and benefit the domain alignment. In the adaptation stage, we design a Contrastive Domain Distillation (CDD) module to achieve feature-level adaptation, including a domain distillation loss to transfer relation knowledge and a domain contrastive loss to narrow down the domain gap by a self-supervised paradigm. Besides, a Compact-Aware Domain Consistency (CADC) module is proposed to enhance consistency learning by filtering out noisy pseudo labels with shape compactness metric, thus achieving output-level adaptation. Extensive experiments on cross-device and cross-centre datasets are conducted for polyp and prostate segmentation, and our method delivers impressive performance compared with state-of-the-art domain adaptation methods. The source code is available at https://github.com/CityU-AIM-Group/SFDA-FSM.}
}
@inproceedings{
yu2023tuning,
title={Tuning Frequency Bias in Neural Network Training with Nonuniform Data},
author={Annan Yu and Yunan Yang and Alex Townsend},
booktitle=ICLR,
year={2023},
url={https://openreview.net/forum?id=oLIZ2jGTiv}
}
@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@article{hu2022prosfda,
  title={ProSFDA: Prompt Learning based Source-free Domain Adaptation for Medical Image Segmentation},
  author={Hu, Shishuai and Liao, Zehui and Xia, Yong},
  journal={arXiv preprint arXiv:2211.11514},
  year={2022}
}

@inproceedings{Chen2022MaxStyle,
  title="{MaxStyle}: Adversarial Style Composition for Robust Medical Image Segmentation",
  author        = "Chen, Chen and Li, Zeju and Ouyang, Cheng and Sinclair, Matthew  
                   and Bai, Wenjia and Rueckert, Daniel",
  booktitle     =  {MICCAI},
  year          =  {2022},
  eprint        = "2206.01737"
}

@inproceedings{
kim2023domain,
title={Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach},
author={Minyoung Kim and Da Li and Timothy Hospedales},
booktitle=ICLR,
year={2023},
url={https://openreview.net/forum?id=7IG0wsTND7w}
}
@article{YANG2022102457_fourier_mining,
title = {Source free domain adaptation for medical image segmentation with fourier style mining},
journal = {Medical Image Analysis},
volume = {79},
pages = {102457},
year = {2022},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2022.102457},
url = {https://www.sciencedirect.com/science/article/pii/S1361841522001049},
author = {Chen Yang and Xiaoqing Guo and Zhen Chen and Yixuan Yuan},
keywords = {Source Free Domain Adaptation, Fourier Style Mining, Contrastive Domain Distillation, Consistency Learning},
abstract = {Unsupervised domain adaptation (UDA) aims to exploit the knowledge learned from a labeled source dataset to solve similar tasks in a new unlabeled target domain. Existing UDA techniques typically assume that samples from source and target domains are freely accessible during the training. However, it may be impractical to access source images due to privacy concerns, especially in medical imaging scenarios with the patient information. To tackle this issue, we devise a novel source free domain adaptation framework with fourier style mining, where only a well-trained source segmentation model is available for the adaptation to the target domain. Our framework is composed of two stages: a generation stage and an adaptation stage. In the generation stage, we design a Fourier Style Mining (FSM) generator to inverse source-like images through statistic information of the pretrained source model and mutual Fourier Transform. These generated source-like images can provide source data distribution and benefit the domain alignment. In the adaptation stage, we design a Contrastive Domain Distillation (CDD) module to achieve feature-level adaptation, including a domain distillation loss to transfer relation knowledge and a domain contrastive loss to narrow down the domain gap by a self-supervised paradigm. Besides, a Compact-Aware Domain Consistency (CADC) module is proposed to enhance consistency learning by filtering out noisy pseudo labels with shape compactness metric, thus achieving output-level adaptation. Extensive experiments on cross-device and cross-centre datasets are conducted for polyp and prostate segmentation, and our method delivers impressive performance compared with state-of-the-art domain adaptation methods. The source code is available at https://github.com/CityU-AIM-Group/SFDA-FSM.}
}
@inproceedings{chen2022simkd,
  title={Knowledge Distillation with the Reused Teacher Classifier},
  author={Chen, Defang and Mei, Jian-Ping and Zhang, Hailin and Wang, Can and Feng, Yan and Chen, Chun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11933--11942},
  year={2022}
}

@inproceedings{
dong2023lpt,
title={{LPT}: Long-tailed Prompt Tuning  for Image Classification},
author={Bowen Dong and Pan Zhou and Shuicheng Yan and Wangmeng Zuo},
booktitle=ICLR,
year={2023},
url={https://openreview.net/forum?id=8pOVAeo8ie}
}
@inproceedings{jia2022vpt,
  title={Visual Prompt Tuning},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2022}
}

@article{huang2021rda,
  title={RDA: Robust Domain Adaptation via Fourier Adversarial Attacking},
  author={Huang, Jiaxing and Guan, Dayan and Xiao, Aoran and Lu, Shijian},
  journal={arXiv preprint arXiv:2106.02874},
  year={2021}
}

@inproceedings{
kumar2022finetuning_LP_FT,
title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
author={Ananya Kumar and Aditi Raghunathan and Robbie Matthew Jones and Tengyu Ma and Percy Liang},
booktitle=ICLR,
year={2022},
url={https://openreview.net/forum?id=UYneFzXSJWh}
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}


@InProceedings{loss_selection,
  title = 	 { Towards Understanding Biased Client Selection in Federated Learning },
  author =       {Jee Cho, Yae and Wang, Jianyu and Joshi, Gauri},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {10351--10375},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 PMLR,
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  abstract = 	 { Federated learning is a distributed optimization paradigm that enables a large number of resource-limited client nodes to cooperatively train a model without data sharing. Previous works analyzed the convergence of federated learning by accounting of data heterogeneity, communication/computation limitations, and partial client participation. However, most assume unbiased client participation, where clients are selected such that the aggregated model update is unbiased. In our work, we present the convergence analysis of federated learning with biased client selection and quantify how the bias affects convergence speed. We show that biasing client selection towards clients with higher local loss yields faster error convergence. From this insight, we propose Power-of-Choice, a communication- and computation-efficient client selection framework that flexibly spans the trade-off between convergence speed and solution bias. Extensive experiments demonstrate that Power-of-Choice can converge up to 3 times faster and give 10\% higher test accuracy than the baseline random selection. }
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}
@article{Doersch2015UnsupervisedVR,
  title={Unsupervised Visual Representation Learning by Context Prediction},
  author={Carl Doersch and Abhinav Kumar Gupta and Alexei A. Efros},
  journal=ICCV,
  year={2015},
  pages={1422-1430}
}
@inproceedings{Chen2020_SIMCLR,
author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
title = {A Simple Framework for Contrastive Learning of Visual Representations},
year = {2020},
publisher = {JMLR.org},
abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by Sim-CLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100 times fewer labels.},
booktitle = ICML,
articleno = {149},
numpages = {11},
series = ICML
}

@inproceedings{Vincent2008ExtractingAC_autoencoder1,
  title={Extracting and composing robust features with denoising autoencoders},
  author={Pascal Vincent and H. Larochelle and Yoshua Bengio and Pierre-Antoine Manzagol},
  booktitle= ICML,
  year={2008}
}


@article{He2020MomentumCF_moco,
  title={Momentum Contrast for Unsupervised Visual Representation Learning},
  author={Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross B. Girshick},
  journal=CVPR,
  year={2020},
  pages={9726-9735}
}
@article{Wu2022CommunicationefficientFL_fedkd,
  title={Communication-efficient federated learning via knowledge distillation},
  author={Chuhan Wu and Fangzhao Wu and Ruixuan Liu and Lingjuan Lyu and Yongfeng Huang and Xing Xie},
  journal={Nature Communications},
  year={2022},
  volume={13}
}
        
@inproceedings{kolesnikov2019revisiting,
  title={Revisiting self-supervised visual representation learning},
  author={Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
  booktitle=CVPR,
  pages={1920--1929},
  year={2019}
}
@inproceedings{Radford2021LearningTV_clip,
  title={Learning Transferable Visual Models From Natural Language Supervision},
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  booktitle=ICML,
  year={2021}
}
@inproceedings{NEURIPS2020_18df51b9_feddf,
 author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U and Jaggi, Martin},
 booktitle = NIPS,
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {2351--2363},
 publisher = {Curran Associates, Inc.},
 title = {Ensemble Distillation for Robust Model Fusion in Federated Learning},
 volume = {33},
 year = {2020}
}
@misc{
pereyra2017regularizing,
title={Regularizing Neural Networks by Penalizing Confident Output Distributions},
author={Gabriel Pereyra and George Tucker and Jan Chorowski and Lukasz Kaiser and Geoffrey Hinton},
year={2017},
url={https://openreview.net/forum?id=HkCjNI5ex}
}
@INPROCEEDINGS{9825928_balance_fl,
  author={Shuai, Xian and Shen, Yulin and Jiang, Siyang and Zhao, Zhihe and Yan, Zhenyu and Xing, Guoliang},
  booktitle={2022 21st ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)}, 
  title={BalanceFL: Addressing Class Imbalance in Long-Tail Federated Learning}, 
  year={2022},
  volume={},
  number={},
  pages={271-284},
  doi={10.1109/IPSN54338.2022.00029}}

@InProceedings{Huang_2022_CVPR_fccl,
    author    = {Huang, Wenke and Ye, Mang and Du, Bo},
    title     = {Learn From Others and Be Yourself in Heterogeneous Federated Learning},
    booktitle = CVPR,
    month     = {June},
    year      = {2022},
    pages     = {10143-10153}
}

@article{Seo2020FederatedKD_feddistill,
  title={Federated Knowledge Distillation},
  author={Hyowoon Seo and Jihong Park and Seungeun Oh and Mehdi Bennis and Seong-Lyun Kim},
  journal={CoRR},
  year={2020},
  volume={abs/2011.02367}
}

@article{Hjelm2019LearningDR_con,
  title={Learning deep representations by mutual information estimation and maximization},
  author={R. Devon Hjelm and Alex Fedorov and Samuel Lavoie-Marchildon and Karan Grewal and Adam Trischler and Yoshua Bengio},
  journal={CoRR},
  year={2019},
  volume={abs/1808.06670}
}
@inproceedings{Noroozi2016UnsupervisedLO_jigsaw,
  title={Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles},
  author={Mehdi Noroozi and Paolo Favaro},
  booktitle=ECCV,
  year={2016}
}
@article{Pathak2016ContextEF_inpaint,
  title={Context Encoders: Feature Learning by Inpainting},
  author={Deepak Pathak and Philipp Kr{\"a}henb{\"u}hl and Jeff Donahue and Trevor Darrell and Alexei A. Efros},
  journal=CVPR,
  year={2016},
  pages={2536-2544}
}

@article{Zhu2021DataFreeKD_FedGen,
  title={Data-Free Knowledge Distillation for Heterogeneous Federated Learning},
  author={Zhuangdi Zhu and Junyuan Hong and Jiayu Zhou},
  journal=PMLR,
  year={2021},
  volume={139},
  pages={
          12878-12889
        }
}


@inproceedings{
li2021fedbn,
title={Fed{\{}BN{\}}: Federated Learning on Non-{\{}IID{\}} Features via Local Batch Normalization},
author={Xiaoxiao Li and Meirui Jiang and Xiaofei Zhang and Michael Kamp and Qi Dou},
booktitle=ICLR,
year={2021},
url={https://openreview.net/pdf?id=6YEQUn0QICG}
}

@article{Li2023NoFO_fed_lt_noisy,
  title={No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier},
  author={Zexi Li and Xinyi Shang and Rui He and Tao Lin and Chao Wu},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.10058}
}

@InProceedings{Xu_2021_CVPR,
    author    = {Xu, Qinwei and Zhang, Ruipeng and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
    title     = {A Fourier-Based Framework for Domain Generalization},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {14383-14392}
}

@inproceedings{
chen2023on_pre_trained_fl_1,
title={On the Importance and Applicability of Pre-Training for Federated Learning},
author={Hong-You Chen and Cheng-Hao Tu and Ziwei Li and Han Wei Shen and Wei-Lun Chao},
booktitle=ICLR,
year={2023}
}

@inproceedings{
nguyen2023where_to_begin,
title={Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning},
author={John Nguyen and Jianyu Wang and Kshitiz Malik and Maziar Sanjabi and Michael Rabbat},
booktitle=ICLR,
year={2023},
}

@InProceedings{10.1007/978-3-031-16852-9_1_skin_tone_dart,
author="Bevan, Peter J.
and Atapour-Abarghouei, Amir",
editor="Kamnitsas, Konstantinos
and Koch, Lisa
and Islam, Mobarakol
and Xu, Ziyue
and Cardoso, Jorge
and Dou, Qi
and Rieke, Nicola
and Tsaftaris, Sotirios",
title="Detecting Melanoma Fairly: Skin Tone Detection and Debiasing for Skin Lesion Classification",
booktitle="Domain Adaptation and Representation Transfer",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--11",
abstract="Convolutional Neural Networks have demonstrated human-level performance in the classification of melanoma and other skin lesions, but evident performance disparities between differing skin tones should be addressed before widespread deployment. In this work, we propose an efficient yet effective algorithm for automatically labelling the skin tone of lesion images, and use this to annotate the benchmark ISIC dataset. We subsequently use these automated labels as the target for two leading bias `unlearning' techniques towards mitigating skin tone bias. Our experimental results provide evidence that our skin tone detection algorithm outperforms existing solutions and that `unlearning' skin tone may improve generalisation and can reduce the performance disparity between melanoma detection in lighter and darker skin tones.",
isbn="978-3-031-16852-9"
}


@InProceedings{Chen_2021_ICCV_APR,
    author    = {Chen, Guangyao and Peng, Peixi and Ma, Li and Li, Jia and Du, Lin and Tian, Yonghong},
    title     = {Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {458-467}
}

@article{Oquab2023DINOv2LR,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Maxime Oquab and Timoth'ee Darcet and Th'eo Moutakanni and Huy Q. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russ Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael G. Rabbat and Vasu Sharma and Gabriel Synnaeve and Huijiao Xu and Herv{\'e} J{\'e}gou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.07193}
}
@article{Yang2023IntegratingL0_fed_lt_decoupled,
  title={Integrating Local Real Data with Global Gradient Prototypes for Classifier Re-Balancing in Federated Long-Tailed Learning},
  author={Wenkai Yang and Deli Chen and Hao Zhao and Fandong Meng and Jie Zhou and Xu Sun},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.10394}
}
@inproceedings{Wu2022FederatedLW_fedrare,
  title={Federated Learning with Imbalanced and Agglomerated Data Distribution for Medical Image Classification},
  author={Nannan Wu and Li Yu and Xin Yang and Kwang-Ting Cheng and Zengqiang Yan},
  year={2022}
}

@inproceedings{fedSSL_intro,
author = {van Berlo, Bram and Saeed, Aaqib and Ozcelebi, Tanir},
title = {Towards Federated Unsupervised Representation Learning},
year = {2020},
isbn = {9781450371322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378679.3394530},
doi = {10.1145/3378679.3394530},
abstract = {Making deep learning models efficient at inferring nowadays requires training with an extensive number of labeled data that are gathered in a centralized system. However, gathering labeled data is an expensive and time-consuming process, centralized systems cannot aggregate an ever-increasing amount of data and aggregating user data is raising privacy concerns. Federated learning solves data volume and privacy issues by leaving user data on devices, but is limited to use cases where labeled data can be generated from user interaction. Unsupervised representation learning reduces the amount of labeled data required for model training, but previous work is limited to centralized systems. This work introduces federated unsupervised representation learning, a novel software architecture that uses unsupervised representation learning to pre-train deep neural networks using unlabeled data in a federated setting. Pre-trained networks can be used to extract discriminative features. The features help learn a down-stream task of interest with a reduced amount of labeled data. Based on representation performance experiments with human activity detection it is recommended to pre-train with unlabeled data originating from more users performing a bigger set of activities compared to data used with the down-stream task of interest. As a result, competitive or superior performance compared to supervised deep learning is achieved.},
booktitle = {Proceedings of the Third ACM International Workshop on Edge Systems, Analytics and Networking},
pages = {31–36},
numpages = {6},
keywords = {unsupervised representation learning, human activity detection, federated learning, self-supervised learning, deep learning},
location = {Heraklion, Greece},
series = {EdgeSys '20}
}


@InProceedings{LubanaTKDM22_orchestra,
  title = 	 {Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering},
  author =       {Lubana, Ekdeep and Tang, Chi Ian and Kawsar, Fahim and Dick, Robert and Mathur, Akhil},
  booktitle = 	ICML,
  pages = 	 {14461--14484},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 PMLR,
  month = 	 {17--23 Jul},
  publisher =    PMLR,
  pdf = 	 {https://proceedings.mlr.press/v162/lubana22a/lubana22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/lubana22a.html},
  abstract = 	 {Federated learning is generally used in tasks where labels are readily available (e.g., next word prediction). Relaxing this constraint requires design of unsupervised learning techniques that can support desirable properties for federated training: robustness to statistical/systems heterogeneity, scalability with number of participants, and communication efficiency. Prior work on this topic has focused on directly extending centralized self-supervised learning techniques, which are not designed to have the properties listed above. To address this situation, we propose Orchestra, a novel unsupervised federated learning technique that exploits the federation’s hierarchy to orchestrate a distributed clustering task and enforce a globally consistent partitioning of clients’ data into discriminable clusters. We show the algorithmic pipeline in Orchestra guarantees good generalization performance under a linear probe, allowing it to outperform alternative techniques in a broad range of conditions, including variation in heterogeneity, number of clients, participation ratio, and local epochs.}
}

@article{Lin2017FocalLF,
  title={Focal Loss for Dense Object Detection},
  author={Tsung-Yi Lin and Priya Goyal and Ross B. Girshick and Kaiming He and Piotr Doll{\'a}r},
  journal=ICCV,
  year={2017},
  pages={2999-3007}
}
@article{Zhao2018FederatedLW_LTS,
  title={Federated Learning with Non-IID Data},
  author={Yue Zhao and Meng Li and Liangzhen Lai and Naveen Suda and Damon Civin and Vikas Chandra},
  journal={CoRR},
  year={2018},
  volume={abs/1806.00582}
}
@inproceedings{10.5555/3495724.3496362_LTS,
author = {Wang, Jianyu and Liu, Qinghua and Liang, Hao and Joshi, Gauri and Poor, H. Vincent},
title = {Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {In federated learning, heterogeneity in the clients' local datasets and computation speeds results in large variations in the number of local updates performed by each client in each communication round. Naive weighted aggregation of such models causes objective inconsistency, that is, the global model converges to a stationary point of a mismatched objective function which can be arbitrarily different from the true objective. This paper provides a general framework to analyze the convergence of heterogeneous federated optimization algorithms. It subsumes previously proposed methods such as FedAvg and FedProx, and provides the first principled understanding of the solution bias and the convergence slowdown due to objective inconsistency. Using insights from this analysis, we propose FedNova, a normalized averaging method that eliminates objective inconsistency while preserving fast error convergence.},
booktitle = NIPS,
articleno = {638},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@inproceedings{10.5555/3495724.3495922_GA,
author = {Lin, Tao and Kong, Lingjing and Stich, Sebastian U. and Jaggi, Martin},
title = {Ensemble Distillation for Robust Model Fusion in Federated Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Federated Learning (FL) is a machine learning setting where many devices collab-oratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios.In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far.},
booktitle = NIPS,
articleno = {198},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}
@INPROCEEDINGS{7780459_resnet,  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  booktitle=CVPR,   title={Deep Residual Learning for Image Recognition},   year={2016},  volume={},  number={},  pages={770-778}}

@article{Krizhevsky09learningmultiple_cifar,
  added-at = {2021-01-21T03:01:11.000+0100},
  author = {Krizhevsky, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/2fe5248afe57647d9c85c50a98a12145c/s364315},
  interhash = {cc2d42f2b7ef6a4e76e47d1a50c8cd86},
  intrahash = {fe5248afe57647d9c85c50a98a12145c},
  keywords = {},
  pages = {32--33},
  timestamp = {2021-01-21T03:01:11.000+0100},
  title = {Learning Multiple Layers of Features from Tiny Images},
  year = 2009
}


@inproceedings{sohn2016improved,
  title={Improved deep metric learning with multi-class n-pair loss objective},
  author={Sohn, Kihyuk},
  booktitle=NIPS,
  pages={1857--1865},
  year={2016}
}
@article{He2020FedMLAR,
  title={FedML: A Research Library and Benchmark for Federated Machine Learning},
  author={Chaoyang He and Songze Li and Jinhyun So and Mi Zhang and Hongyi Wang and Xiaoyang Wang and Praneeth Vepakomma and Abhishek Singh and Han Qiu and Li Shen and Peilin Zhao and Yan Kang and Yang Liu and Ramesh Raskar and Qiang Yang and Murali Annavaram and Salman Avestimehr},
  journal={CoRR},
  year={2020},
  volume={abs/2007.13518}
}
@inproceedings{ding2022free,
  author    = {Xinpeng Ding and
               Ziwei Liu and
               Xiaomeng Li},
  editor    = {Linwei Wang and
               Qi Dou and
               P. Thomas Fletcher and
               Stefanie Speidel and
               Shuo Li},
  title     = {Free Lunch for Surgical Video Understanding by Distilling Self-supervisions},
  booktitle = {MICCAI 2022},
  series    = {Lecture Notes in Computer Science},
  volume    = {13437},
  pages     = {365--375},
  publisher = {Springer},
  year      = {2022},
  timestamp = {Tue, 13 Dec 2022 14:39:06 +0100},
  biburl    = {https://dblp.org/rec/conf/miccai/DingLL22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{
loshchilov2017sgdr_cosine,
title={{SGDR}: Stochastic Gradient Descent with Warm Restarts},
author={Ilya Loshchilov and Frank Hutter},
booktitle=ICLR,
year={2017}
}

@incollection{NEURIPS2019_9015_pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{49350_noniid_setting,
  title={Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification},
  author={Tzu-Ming Harry Hsu and Qi and Matthew Brown},
  journal   = {CoRR},
  year={2019},
  volume={abs/1909.06335}
}

@InProceedings{Xu_2022_CVPR_fedsm,
    author    = {Xu, An and Li, Wenqi and Guo, Pengfei and Yang, Dong and Roth, Holger R. and Hatamizadeh, Ali and Zhao, Can and Xu, Daguang and Huang, Heng and Xu, Ziyue},
    title     = {Closing the Generalization Gap of Cross-Silo Federated Medical Image Segmentation},
    booktitle = CVPR,
    month     = {June},
    year      = {2022},
    pages     = {20866-20875}
}
@inproceedings{
zhang2018mixup,
title={mixup: Beyond Empirical Risk Minimization},
author={Hongyi Zhang and Moustapha Cisse and Yann N. Dauphin and David Lopez-Paz},
booktitle=ICLR,
year={2018},
url={https://openreview.net/forum?id=r1Ddp1-Rb},
}
@inproceedings{Huang2016DeepNW_stochdepth,
  title={Deep Networks with Stochastic Depth},
  author={Gao Huang and Yu Sun and Zhuang Liu and Daniel Sedra and Kilian Q. Weinberger},
  booktitle=ECCV,
  year={2016}
}
@inproceedings{10.5555/3495724.3496172_LTS,
author = {Yuan, Honglin and Ma, Tengyu},
title = {Federated Accelerated Stochastic Gradient Descent},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose Federated Accelerated Stochastic Gradient Descent (FEDAC), a principled acceleration of Federated Averaging (FEDAVG, also known as Local SGD) for distributed optimization. FEDAC is the first provable acceleration of FEDAVG that improves convergence speed and communication efficiency on various types of convex functions. For example, for strongly convex and smooth functions, when using M workers, the previous state-of-the-art FEDAVG analysis can achieve a linear speedup in M if given \~{O}(M) rounds of synchronization, whereas FEDAC only requires \~{O}(M1/3) rounds. Moreover, we prove stronger guarantees for FEDAC when the objectives are third-order smooth. Our technique is based on a potential-based perturbed iterate analysis, a novel stability analysis of generalized accelerated SGD, and a strategic tradeoff between acceleration and stability.},
booktitle = NIPS,
articleno = {448},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@inproceedings{NEURIPS2020_a3a3e8b3_gradaug,
 author = {Yang, Taojiannan and Zhu, Sijie and Chen, Chen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {14207--14218},
 publisher = {Curran Associates, Inc.},
 title = {GradAug: A New Regularization Method for Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/a3a3e8b30dd6eadfc78c77bb2b8e6b60-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{mendieta2022local_fedalign,
  title={Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning},
  author={Mendieta, Matias and Yang, Taojiannan and Wang, Pu and others},
  booktitle=CVPR,
  pages={8397--8406},
  year={2022}
}
@inproceedings{
gao2022federated_FedDC,
title={FedDC: Federated Learning with Non-IID Data via Local Drift Decoupling and Correction},
author={Liang Gao and Huazhu Fu and Li Li and Yingwen Chen and Ming Xu and Cheng-Zhong Xu},
booktitle=CVPR,
year={2022}
}
@inproceedings{li2021model_moon,
      title={Model-Contrastive Federated Learning}, 
      author={Qinbin Li and Bingsheng He and Dawn Song},
      booktitle=CVPR,
      year={2021},
}

@InProceedings{KarimireddyKMRS20_Scaffold,
  title = 	 {{SCAFFOLD}: Stochastic Controlled Averaging for Federated Learning},
  author =       {Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle = 	 ICML,
  pages = 	 {5132--5143},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 PMLR,
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/karimireddy20a/karimireddy20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/karimireddy20a.html},
  abstract = 	 {Federated learning is a key scenario in modern large-scale machine learning where the data remains distributed over a large number of clients and the task is to learn a centralized model without transmitting the client data. The standard optimization algorithm used in this setting is Federated Averaging (FedAvg) due to its low communication cost. We obtain a tight characterization of the convergence of FedAvg and prove that heterogeneity (non-iid-ness) in the client’s data results in a ‘drift’ in the local updates resulting in poor performance. As a solution, we propose a new algorithm (SCAFFOLD) which uses control variates (variance reduction) to correct for the ‘client drift’. We prove that SCAFFOLD requires significantly fewer communication rounds and is not affected by data heterogeneity or client sampling. Further, we show that (for quadratics) SCAFFOLD can take advantage of similarity in the client’s data yielding even faster convergence. The latter is the first result to quantify the usefulness of local-steps in distributed optimization.}
}

@inproceedings{Vanhaesebrouck2017DecentralizedCL_context,
  title={Decentralized Collaborative Learning of Personalized Models over Networks},
  author={Paul Vanhaesebrouck and Aur{\'e}lien Bellet and Marc Tommasi},
  booktitle={AISTATS},
  year={2017}
}
@inproceedings{10.5555/3495724.3497520_pFedME_local,
author = {Dinh, Canh T. and Tran, Nguyen H. and Nguyen, Tuan Dung},
title = {Personalized Federated Learning with Moreau Envelopes},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Federated learning (FL) is a decentralized and privacy-preserving machine learning technique in which a group of clients collaborate with a server to learn a global model without sharing clients' data. One challenge associated with FL is statistical diversity among clients, which restricts the global model from delivering good performance on each client's task. To address this, we propose an algorithm for personalized FL (pFedMe) using Moreau envelopes as clients' regularized loss functions, which help decouple personalized model optimization from the global model learning in a bi-level problem stylized for personalized FL. Theoretically, we show that pFedMe's convergence rate is state-of-the-art: achieving quadratic speedup for strongly convex and sublinear speedup of order 2/3 for smooth non-convex objectives. Experimentally, we verify that pFedMe excels at empirical performance compared with the vanilla FedAvg and Per-FedAvg, a meta-learning based personalized FL algorithm.},
booktitle = NIPS,
articleno = {1796},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}
@inproceedings{NIPS2017_6211080f_MTL,
 author = {Smith, Virginia and Chiang, Chao-Kai and Sanjabi, Maziar and Talwalkar, Ameet S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Federated Multi-Task Learning},
 url = {https://proceedings.neurips.cc/paper/2017/file/6211080fa89981f66b1a0c9d55c61d0f-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{zhang2020personalized_fedfomo_colab,
  title={Personalized federated learning with first order model optimization},
  author={Zhang, Michael and Sapra, Karan and Fidler, Sanja and Yeung, Serena and Alvarez, Jose M},
  journal={CoRR},
  year={2020}
}
@inproceedings{
zhang2021parameterized_kt_pfl_colab,
title={Parameterized Knowledge Transfer for Personalized Federated Learning},
author={Jie Zhang and Song Guo and Xiaosong Ma and Haozhao Wang and Wenchao Xu and Feijie Wu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=_89s8ViNwwj}
}

@InProceedings{10.1007/978-3-031-17027-0_3_chest_LT_benchmark,
author="Holste, Gregory
and Wang, Song
and Jiang, Ziyu
and Shen, Thomas C.
and Shih, George
and Summers, Ronald M.
and Peng, Yifan
and Wang, Zhangyang",
editor="Nguyen, Hien V.
and Huang, Sharon X.
and Xue, Yuan",
title="Long-Tailed Classification ofÂ Thorax Diseases onÂ Chest X-Ray: A New Benchmark Study",
booktitle="Data Augmentation, Labelling, and Imperfections",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="22--32",
abstract="Imaging exams, such as chest radiography, will yield a small set of common findings and a much larger set of uncommon findings. While a trained radiologist can learn the visual presentation of rare conditions by studying a few representative examples, teaching a machine to learn from such a ``long-tailed'' distribution is much more difficult, as standard methods would be easily biased toward the most frequent classes. In this paper, we present a comprehensive benchmark study of the long-tailed learning problem in the specific domain of thorax diseases on chest X-rays. We focus on learning from naturally distributed chest X-ray data, optimizing classification accuracy over not only the common ``head'' classes, but also the rare yet critical ``tail'' classes. To accomplish this, we introduce a challenging new long-tailed chest X-ray benchmark to facilitate research on developing long-tailed learning methods for medical image classification. The benchmark consists of two chest X-ray datasets for 19- and 20-way thorax disease classification, containing classes with as many as 53,000 and as few as 7 labeled training images. We evaluate both standard and state-of-the-art long-tailed learning methods on this new benchmark, analyzing which aspects of these methods are most beneficial for long-tailed medical image classification and summarizing insights for future algorithm design. The datasets, trained models, and code are available at https://github.com/VITA-Group/LongTailCXR.",
isbn="978-3-031-17027-0"
}


@InProceedings{pmlr-v139-collins21a_param_comp,
  title = 	 {Exploiting Shared Representations for Personalized Federated Learning},
  author =       {Collins, Liam and Hassani, Hamed and Mokhtari, Aryan and Shakkottai, Sanjay},
  booktitle = 	 ICML,
  pages = 	 {2089--2099},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 PMLR,
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/collins21a/collins21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/collins21a.html},
  abstract = 	 {Deep neural networks have shown the ability to extract universal feature representations from data such as images and text that have been useful for a variety of learning tasks. However, the fruits of representation learning have yet to be fully-realized in federated settings. Although data in federated settings is often non-i.i.d. across clients, the success of centralized deep learning suggests that data often shares a global {\em feature representation}, while the statistical heterogeneity across clients or tasks is concentrated in the {\em labels}. Based on this intuition, we propose a novel federated learning framework and algorithm for learning a shared data representation across clients and unique local heads for each client. Our algorithm harnesses the distributed computational power across clients to perform many local-updates with respect to the low-dimensional local parameters for every update of the representation. We prove that this method obtains linear convergence to the ground-truth representation with near-optimal sample complexity in a linear setting, demonstrating that it can efficiently reduce the problem dimension for each client. Further, we provide extensive experimental results demonstrating the improvement of our method over alternative personalized federated learning approaches in heterogeneous settings.}
}


@inproceedings{NEURIPS2020_24389bfe_meta,
 author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {3557--3568},
 publisher = {Curran Associates, Inc.},
 title = {Personalized Federated Learning with Theoretical Guarantees: A Model-Agnostic Meta-Learning Approach},
 url = {https://proceedings.neurips.cc/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{Chen2018FederatedMF_meta,
  title={Federated Meta-Learning for Recommendation},
  author={Fei Chen and Zhenhua Dong and Zhenguo Li and Xiuqiang He},
  journal={CoRR},
  year={2018},
  volume={abs/1802.07876}
}
@article{Jiang2019ImprovingFL_meta,
  title={Improving Federated Learning Personalization via Model Agnostic Meta Learning},
  author={Yihan Jiang and Jakub Konecn{\'y} and Keith Rush and Sreeram Kannan},
  journal={CoRR},
  year={2019},
  volume={abs/1909.12488}
}
@InProceedings{Li_2022_CVPR_mm,
    author    = {Li, Shuangtong and Zhou, Tianyi and Tian, Xinmei and Tao, Dacheng},
    title     = {Learning To Collaborate in Decentralized Learning of Personalized Models},
    booktitle = CVPR,
    month     = {June},
    year      = {2022},
    pages     = {9766-9775}
}
@InProceedings{Ma_2022_CVPR_mm,
    author    = {Ma, Xiaosong and Zhang, Jie and Guo, Song and Xu, Wenchao},
    title     = {Layer-Wised Model Aggregation for Personalized Federated Learning},
    booktitle = CVPR,
    month     = {June},
    year      = {2022},
    pages     = {10092-10101}
}

@article{Wang2019FederatedEO_context,
  title={Federated Evaluation of On-device Personalization},
  author={Kangkang Wang and Rajiv Mathews and Chlo{\'e} Kiddon and Hubert Eichner and Franccoise Beaufays and Daniel Ramage},
  journal={CoRR},
  year={2019},
  volume={abs/1910.10252}
}
@article{Hard2018FederatedLF_context,
  title={Federated Learning for Mobile Keyboard Prediction},
  author={Andrew Hard and Kanishka Rao and Rajiv Mathews and Françoise Beaufays and Sean Augenstein and Hubert Eichner and Chlo{\'e} Kiddon and Daniel Ramage},
  journal={CoRR},
  year={2018},
  volume={abs/1811.03604}
}
@article{Deng2020AdaptivePF_mm_apfl,
  title={Adaptive Personalized Federated Learning},
  author={Yuyang Deng and Mohammad Mahdi Kamani and Mehrdad Mahdavi},
  journal={CoRR},
  year={2020},
  volume={abs/2003.13461}
}

@article{Hanzely2020FederatedLO_mm,
  title={Federated Learning of a Mixture of Global and Local Models},
  author={Filip Hanzely and Peter Richt{\'a}rik},
  journal={CoRR},
  year={2020},
  volume={abs/2002.05516}
}
@article{Mansour2020ThreeAF_mm,
  title={Three Approaches for Personalization with Applications to Federated Learning},
  author={Y. Mansour and Mehryar Mohri and Jae Ro and Ananda Theertha Suresh},
  journal={CoRR},
  year={2020},
  volume={abs/2002.10619}
}
@article{Arivazhagan2019FederatedLW,
  title={Federated Learning with Personalization Layers_fedper},
  author={Manoj Ghuhan Arivazhagan and V. Aggarwal and Aaditya Singh and Sunav Choudhary},
  journal={CoRR},
  year={2019},
  volume={abs/1912.00818}
}
@inproceedings{LiSZSTS20_fedprox,
 author = {Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
 pages = {429--450},
 title = {Federated Optimization in Heterogeneous Networks},
 volume = {2},
 year = {2020}
}


@inproceedings{
acar2021federated_FedDyn,
title={Federated Learning Based on Dynamic Regularization},
author={Durmus Alp Emre Acar and Yue Zhao and Ramon Matas and Matthew Mattina and Paul Whatmough and Venkatesh Saligrama},
booktitle=ICLR,
year={2021},
url={https://openreview.net/forum?id=B7v4QMR6Z9w}
}
@inproceedings{10.5555/3524938.3525559_LTS,
author = {Malinovsky, Grigory and Kovalev, Dmitry and Gasanov, Elnur and Condat, Laurent and Richt\'{a}rik, Peter},
title = {From Local SGD to Local Fixed-Point Methods for Federated Learning},
year = {2020},
publisher = {JMLR.org},
abstract = {Most algorithms for solving optimization problems or finding saddle points of convex-concave functions are fixed-point algorithms. In this work we consider the generic problem of finding a fixed point of an average of operators, or an approximation thereof, in a distributed setting. Our work is motivated by the needs of federated learning. In this context, each local operator models the computations done locally on a mobile device. We investigate two strategies to achieve such a consensus: one based on a fixed number of local steps, and the other based on randomized computations. In both cases, the goal is to limit communication of the locally-computed variables, which is often the bottleneck in distributed frameworks. We perform convergence analysis of both methods and conduct a number of experiments highlighting the benefits of our approach.},
booktitle = ICML,
articleno = {621},
numpages = {10},
series = ICML
}
@article{Hsu2019MeasuringTE_GA,
  title={Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification},
  author={Tzu-Ming Harry Hsu and Qi and Matthew Brown},
  journal={CoRR},
  year={2019},
  volume={abs/1909.06335}
}
@inproceedings{
reddi2021adaptive_GA,
title={Adaptive Federated Optimization},
author={Sashank J. Reddi and Zachary Charles and Manzil Zaheer and Zachary Garrett and Keith Rush and Jakub Kone{\v{c}}n{\'y} and Sanjiv Kumar and Hugh Brendan McMahan},
booktitle=ICLR,
year={2021},
url={https://openreview.net/forum?id=LkFG3lB13U5}
}
@inproceedings{
reinke2022metrics,
title={Metrics Reloaded - A new recommendation framework for biomedical image analysis validation},
author={Annika Reinke and Evangelia Christodoulou and Ben Glocker and others},
booktitle={Medical Imaging with Deep Learning},
year={2022}
}
@InProceedings{10.1007/978-3-030-87240-3_31_balanced_mixup,
author="Galdran, Adrian
and Carneiro, Gustavo
and Gonz{\'a}lez Ballester, Miguel A.",
editor="de Bruijne, Marleen
and Cattin, Philippe C.
and Cotin, St{\'e}phane
and Padoy, Nicolas
and Speidel, Stefanie
and Zheng, Yefeng
and Essert, Caroline",
title="Balanced-MixUp for Highly Imbalanced Medical Image Classification",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="323--333",
abstract="Highly imbalanced datasets are ubiquitous in medical image classification problems. In such problems, it is often the case that rare classes associated to less prevalent diseases are severely under-represented in labeled databases, typically resulting in poor performance of machine learning algorithms due to overfitting in the learning process. In this paper, we propose a novel mechanism for sampling training data based on the popular MixUp regularization technique, which we refer to as Balanced-MixUp. In short, Balanced-MixUp simultaneously performs regular (i.e., instance-based) and balanced (i.e., class-based) sampling of the training data. The resulting two sets of samples are then mixed-upÂ to create a more balanced training distribution from which a neural network can effectively learn without incurring in heavily under-fitting the minority classes. We experiment with a highly imbalanced dataset of retinal images (55K samples, 5 classes) and a long-tail dataset of gastro-intestinal video frames (10K images, 23 classes), using two CNNs of varying representation capabilities. Experimental results demonstrate that applying Balanced-MixUp outperforms other conventional sampling schemes and loss functions specifically designed to deal with imbalanced data. Code is released at https://github.com/agaldran/balanced{\_}mixup",
isbn="978-3-030-87240-3"
}


@article{Borgli2019HyperKvasirAC,
  title={HyperKvasir, a comprehensive multi-class image and video dataset for gastrointestinal endoscopy},
  author={Hanna Borgli and Vajira Lasantha Thambawita and Pia Helen Smedsrud and S. Hicks and Debesh Jha and Sigrun Losada Eskeland and Kristin Ranheim Randel and Konstantin Pogorelov and Mathias Lux and Duc Tien Dang Nguyen and Dag Johansen and Carsten Griwodz and H{\aa}kon Kvale Stensland and Enrique Garcia-Ceja and Peter Thelin Schmidt and Hugo Lewi Hammer and M. Riegler and P. Halvorsen and Thomas de Lange},
  journal={Scientific Data},
  year={2019},
  volume={7}
}


@article{Elbatel2023FoProKDFP,
  title={FoPro-KD: Fourier Prompted Effective Knowledge Distillation for Long-Tailed Medical Image Recognition},
  author={Marawan Elbatel and Robert Martí and X. Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.17421}
}

@InProceedings{10.1007/978-3-030-87237-3_1_monash_BKD_local,
author="Ju, Lie
and Wang, Xin
and Wang, Lin
and Liu, Tongliang
and Zhao, Xin
and Drummond, Tom
and Mahapatra, Dwarikanath
and Ge, Zongyuan",
editor="de Bruijne, Marleen
and Cattin, Philippe C.
and Cotin, St{\'e}phane
and Padoy, Nicolas
and Speidel, Stefanie
and Zheng, Yefeng
and Essert, Caroline",
title="Relational Subsets Knowledge Distillation for Long-Tailed Retinal Diseases Recognition",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2021",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="3--12",
abstract="In the real world, medical datasets often exhibit a long-tailed data distribution (i.e., a few classes occupy most of the data, while most classes have rarely few samples), which results in a challenging imbalance learning scenario. For example, there are estimated more than 40 different kinds of retinal diseases with variable morbidity, however with more than 30+ conditions are very rare from the global patient cohorts, which results in a typical long-tailed learning problem for deep learning-based screening models. In this study, we propose class subset learning by dividing the long-tailed data into multiple class subsets according to prior knowledge, such as regions and phenotype information. It enforces the model to focus on learning the subset-specific knowledge. More specifically, there are some relational classes that reside in the fixed retinal regions, or some common pathological features are observed in both the majority and minority conditions. With those subsets learnt teacher models, then we are able to distil the multiple teacher models into a unified model with weighted knowledge distillation loss. The proposed framework proved to be effective for the long-tailed retinal diseases recognition task. The experimental results on two different datasets demonstrate that our method is flexible and can be easily plugged into many other state-of-the-art techniques with significant improvements.",
isbn="978-3-030-87237-3"
}


@InProceedings{10.1007/978-3-031-16437-8_44_monash_sampling_LT,
author="Ju, Lie
and Wu, Yicheng
and Wang, Lin
and Yu, Zhen
and Zhao, Xin
and Wang, Xin
and Bonnington, Paul
and Ge, Zongyuan",
editor="Wang, Linwei
and Dou, Qi
and Fletcher, P. Thomas
and Speidel, Stefanie
and Li, Shuo",
title="Flexible Sampling for Long-Tailed Skin Lesion Classification",
booktitle="MICCAI 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="462--471",
abstract="Most of the medical tasks naturally exhibit a long-tailed distribution due to the complex patient-level conditions and the existence of rare diseases. Existing long-tailed learning methods usually treat each class equally to re-balance the long-tailed distribution. However, considering that some challenging classes may present diverse intra-class distributions, re-balancing all classes equally may lead to a significant performance drop. To address this, in this paper, we propose a curriculum learning-based framework called Flexible Sampling for the long-tailed skin lesion classification task. Specifically, we initially sample a subset of training data as anchor points based on the individual class prototypes. Then, these anchor points are used to pre-train an inference model to evaluate the per-class learning difficulty. Finally, we use a curriculum sampling module to dynamically query new samples from the rest training samples with the learning difficulty-aware sampling probability. We evaluated our model against several state-of-the-art methods on the ISIC dataset. The results with two long-tailed settings have demonstrated the superiority of our proposed training strategy, which achieves a new benchmark for long-tailed skin lesion classification.",
isbn="978-3-031-16437-8"
}


@inproceedings{
Wang2020Federated_GA,
title={Federated Learning with Matched Averaging},
author={Hongyi Wang and Mikhail Yurochkin and Yuekai Sun and Dimitris Papailiopoulos and Yasaman Khazaeni},
booktitle=ICLR,
year={2020},
url={https://openreview.net/forum?id=BkluqlSFDS}
}
@inproceedings{Yurochkin2019BayesianNF_GA,
  title={Bayesian Nonparametric Federated Learning of Neural Networks},
  author={Mikhail Yurochkin and Mayank Agarwal and Soumya Shubhra Ghosh and Kristjan H. Greenewald and Trong Nghia Hoang and Yasaman Khazaeni},
  booktitle=ICML,
  year={2019}
}
@inproceedings{
chen2021_fedbe,
title={Fed{\{}BE{\}}: Making Bayesian Model Ensemble Applicable to Federated Learning},
author={Hong-You Chen and Wei-Lun Chao},
booktitle=ICLR,
year={2021},
url={https://openreview.net/forum?id=dgtpE6gKjHn}
}
@inproceedings{McMahan2017CommunicationEfficientLO_fedavg,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={H. B. McMahan and Eider Moore and Daniel Ramage and Seth Hampson and Blaise Ag{\"u}era y Arcas},
  booktitle={AISTATS},
  year={2017}
}

@inproceedings{
chen2022on_fedrod,
title={On Bridging Generic and Personalized Federated Learning for Image Classification},
author={Hong-You Chen and Wei-Lun Chao},
booktitle=ICLR,
year={2022},
url={https://openreview.net/forum?id=I1hQbx10Kxn}
}

@inproceedings{
Zhuang2022DivergenceawareFS_fedema,
title={Divergence-aware Federated Self-Supervised Learning},
author={Weiming Zhuang and Yonggang Wen and Shuai Zhang},
booktitle=ICLR,
year={2022},
url={https://openreview.net/forum?id=oVE1z8NlNe}
}

@article{Zhuang2021CollaborativeUV_FedU,
  title={Collaborative Unsupervised Visual Representation Learning from Decentralized Data},
  author={Weiming Zhuang and Xin Gan and Yonggang Wen and Shuai Zhang and Shuai Yi},
  journal=ICCV,
  year={2021},
  pages={4892-4901}
}
@article{Zhang2020FederatedUR_FedCA,
  title={Federated Unsupervised Representation Learning},
  author={Fengda Zhang and Kun Kuang and Zhaoyang You and Tao Shen and Jun Xiao and Yin Zhang and Chao Wu and Yueting Zhuang and Xiaolin Li},
  journal={CoRR},
  year={2020},
  volume={abs/2010.08982}
}
@inproceedings{BYOL,
author = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R\'{e}mi and Valko, Michal},
title = {Bootstrap Your Own Latent a New Approach to Self-Supervised Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3% top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and 79.6% with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
booktitle = NIPS,
articleno = {1786},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}



@InProceedings{Chen_2021_CVPR_siam,
    author    = {Chen, Xinlei and He, Kaiming},
    title     = {Exploring Simple Siamese Representation Learning},
    booktitle = CVPR,
    month     = {June},
    year      = {2021},
    pages     = {15750-15758}
}
@inproceedings{Goodfellow2014GenerativeAN,
  title={Generative Adversarial Nets},
  author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron C. Courville and Yoshua Bengio},
  booktitle={NIPS},
  year={2014}
}
@inproceedings{
    Ren2020balms,
    title={Balanced Meta-Softmax for Long-Tailed Visual Recognition},
    author={Jiawei Ren and Cunjun Yu and Shunan Sheng and Xiao Ma and Haiyu Zhao and Shuai Yi and Hongsheng Li},
    booktitle={Proceedings of Neural Information Processing Systems(NeurIPS)},
    month = {Dec},
    year={2020}
}

@article{Kobayashi2021GroupSL_group_softmax,
  title={Group Softmax Loss with Discriminative Feature Grouping},
  author={Takumi Kobayashi},
  journal={2021 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2021},
  pages={2614-2623}
}
@inproceedings{10.1007/978-3-031-20053-3_41_IF_dual,
author = {Tang, Kaihua and Tao, Mingyuan and Qi, Jiaxin and Liu, Zhenguang and Zhang, Hanwang},
title = {Invariant Feature Learning For Generalized Long-Tailed Classification},
year = {2022},  
booktitle = ECCV,
pages = {709–726},
numpages = {18},
keywords = {Data imbalance, Generalized Long-Tailed classification},
location = {Tel Aviv, Israel}
}



@inproceedings{Zhao2021WellclassifiedEA_decouple_problem,
  title={Well-classified Examples are Underestimated in Classification with Deep Neural Networks},
  author={Guangxiang Zhao and Wenkai Yang and Xuancheng Ren and Lei Li and Xu Sun},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2021}
}


@inproceedings{cao2019learning_LDAM,
  title={Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss},
  author={Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
  booktitle=NIPS,
  year={2019}
}

@inproceedings{ Kang2020Decoupling, title={Decoupling Representation and Classifier for Long-Tailed Recognition}, author={Bingyi Kang and Saining Xie and Marcus Rohrbach and Zhicheng Yan and Albert Gordo and Jiashi Feng and Yannis Kalantidis}, booktitle=ICLR, year={2020}, 
}

@InProceedings{10.1007/978-3-030-87199-4_31_fedirm,
author="Liu, Quande
and Yang, Hongzheng
and Dou, Qi
and Heng, Pheng-Ann",
editor="de Bruijne, Marleen
and Cattin, Philippe C.
and Cotin, St{\'e}phane
and Padoy, Nicolas
and Speidel, Stefanie
and Zheng, Yefeng
and Essert, Caroline",
title="Federated Semi-supervised Medical Image Classification via Inter-client Relation Matching",
booktitle="MICCAI 2021",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="325--335",
abstract="Federated learning (FL) has emerged with increasing popularity to collaborate distributed medical institutions for training deep networks. However, despite existing FL algorithms only allow the supervised training setting, most hospitals in realistic usually cannot afford the intricate data labeling due to absence of budget or expertise. This paper studies a practical yet challenging FL problem, named Federated Semi-supervised Learning (FSSL), which aims to learn a federated model by jointly utilizing the data from both labeled and unlabeled clients (i.e., hospitals). We present a novel approach for this problem, which improves over traditional consistency regularization mechanism with a new inter-client relation matching scheme. The proposed learning scheme explicitly connects the learning across labeled and unlabeled clients by aligning their extracted disease relationships, thereby mitigating the deficiency of task knowledge at unlabeled clients and promoting discriminative information from unlabeled samples. We validate our method on two large-scale medical image classification datasets. The effectiveness of our method has been demonstrated with the clear improvements over state-of-the-arts as well as the thorough ablation analysis on both tasks (Code will be made available at https://github.com/liuquande/FedIRM).",
isbn="978-3-030-87199-4"
}


@article{Mu2021FedProcPC,
  title={FedProc: Prototypical Contrastive Federated Learning on Non-IID data},
  author={Xutong Mu and Yulong Shen and Ke Cheng and Xueli Geng and Jiaxuan Fu and Tao Zhang and Zhiwei Zhang},
  journal={Future Gener. Comput. Syst.},
  year={2021},
  volume={143},
  pages={93-104}
}

@article{Wicaksana2023FCATL_taming,
  title={FCA: Taming Long-tailed Federated Medical Image Classification by Classifier Anchoring},
  author={Jeffry Wicaksana and Zengqiang Yan and Kwang-Ting Cheng},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.00738}
}

@InProceedings{pmlr-v162-zhang22p_fedlc,
  title = 	 {Federated Learning with Label Distribution Skew via Logits Calibration},
  author =       {Zhang, Jie and Li, Zhiqi and others},
  pages = 	 {26311--26329},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  month = 	 {17--23 Jul},
  publisher =    PMLR,
  abstract = 	 {Traditional federated optimization methods perform poorly with heterogeneous data (i.e. , accuracy reduction), especially for highly skewed data. In this paper, we investigate the label distribution skew in FL, where the distribution of labels varies across clients. First, we investigate the label distribution skew from a statistical view. We demonstrate both theoretically and empirically that previous methods based on softmax cross-entropy are not suitable, which can result in local models heavily overfitting to minority classes and missing classes. Additionally, we theoretically introduce a deviation bound to measure the deviation of the gradient after local update. At last, we propose FedLC (\textbf{Fed}erated learning via \textbf{L}ogits \textbf{C}alibration), which calibrates the logits before softmax cross-entropy according to the probability of occurrence of each class. FedLC applies a fine-grained calibrated cross-entropy loss to local update by adding a pairwise label margin. Extensive experiments on federated datasets and real-world datasets demonstrate that FedLC leads to a more accurate global model and much improved performance. Furthermore, integrating other FL methods into our approach can further enhance the performance of the global model.}
}

@inproceedings{
oh2022fedbabu,
title={Fed{BABU}: Toward Enhanced Representation for Federated Image Classification},
author={Jaehoon Oh and SangMook Kim and Se-Young Yun},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=HuaYQfggn5u}
}
@article{Chen2022TowardsFL_hualiang_LT,
  title={Towards Federated Long-Tailed Learning},
  author={Zihan Chen and Songshan Liu and Hualiang Wang and Howard H. Yang and Tony Q. S. Quek and Zuozhu Liu},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.14988}
}
@inproceedings{ijcai2022p308_creff,
  title     = {Federated Learning on Heterogeneous and Long-Tailed Data via Classifier Re-Training with Federated Features},
  author    = {Shang, Xinyi and Lu, Yang and Huang, Gang and Wang, Hanzi},
  booktitle = IJCAI,
  editor    = {Lud De Raedt},
  pages     = {2218--2224},
  year      = {2022},
  month     = {7}}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{Chen_Mei_Zhang_Wang_Wang_Feng_Chen_2021, title={Cross-Layer Distillation with Semantic Calibration}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/16865}, DOI={10.1609/aaai.v35i8.16865}, abstractNote={Recently proposed knowledge distillation approaches based on feature-map transfer validate that intermediate layers of a teacher model can serve as effective targets for training a student model to obtain better generalization ability. Existing studies mainly focus on particular representation forms for knowledge transfer between manually specified pairs of teacher-student intermediate layers. However, semantics of intermediate layers may vary in different networks and manual association of layers might lead to negative regularization caused by semantic mismatch between certain teacher-student layer pairs. To address this problem, we propose Semantic Calibration for Cross-layer Knowledge Distillation (SemCKD), which automatically assigns proper target layers of the teacher model for each student layer with an attention mechanism. With a learned attention distribution, each student layer distills knowledge contained in multiple layers rather than a single fixed intermediate layer from the teacher model for appropriate cross-layer supervision in training. Consistent improvements over state-of-the-art approaches are observed in extensive experiments with various network architectures for teacher and student models, demonstrating the effectiveness and flexibility of the proposed attention based soft layer association mechanism for cross-layer distillation.}, number={8}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chen, Defang and Mei, Jian-Ping and Zhang, Yuan and Wang, Can and Wang, Zhe and Feng, Yan and Chen, Chun}, year={2021}, month={May}, pages={7028-7036} }


@misc{mirza2014conditional,
  abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to
train generative models. In this work we introduce the conditional version of
generative adversarial nets, which can be constructed by simply feeding the
data, y, we wish to condition on to both the generator and discriminator. We
show that this model can generate MNIST digits conditioned on class labels. We
also illustrate how this model could be used to learn a multi-modal model, and
provide preliminary examples of an application to image tagging in which we
demonstrate how this approach can generate descriptive tags which are not part
of training labels.},
  added-at = {2017-10-04T17:14:40.000+0200},
  author = {Mirza, Mehdi and Osindero, Simon},
  biburl = {https://www.bibsonomy.org/bibtex/2a4426d639ebb30270839ad347bcfb999/achakraborty},
  description = {Conditional Generative Adversarial Nets},
  interhash = {efbbaeaebb1ea8d88264d258624d364c},
  intrahash = {a4426d639ebb30270839ad347bcfb999},
  keywords = {2014 GAN deep-learning machine-learning neural-networks},
  note = {cite arxiv:1411.1784},
  timestamp = {2017-10-04T17:14:40.000+0200},
  title = {Conditional Generative Adversarial Nets},
  url = {http://arxiv.org/abs/1411.1784},
  year = 2014
}


@inproceedings{bai2022improving_vision_freq,
  title={Improving Vision Transformers by Revisiting High-frequency Components},
  author={Bai, Jiawang and Yuan, Li and Xia, Shu-Tao and Yan, Shuicheng and Li, Zhifeng and Liu, Wei},
  booktitle={European Conference on Computer Vision},
  year={2022}
}


@article{Combalia2019BCN20000DL_isic2019,
  title={BCN20000: Dermoscopic Lesions in the Wild},
  author={Marc Combalia and Noel C. F. Codella and Veronica M Rotemberg and Brian Helba and Ver{\'o}nica Vilaplana and Ofer Reiter and Allan C. Halpern and Susana Puig and Josep Malvehy},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.02288}
}
@article{Tschandl2018TheHD_ham2018,
  title={The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions},
  author={Philipp Tschandl and Cliff Rosendahl and Harald Kittler},
  journal={Scientific Data},
  year={2018},
  volume={5}
}


@article{Makino2020DifferencesBH,
  title={Differences between human and machine perception in medical diagnosis},
  author={Taro Makino and Stanislaw Jastrzebski and Witold Oleszkiewicz and Celin Chacko and Robin Ehrenpreis and Naziya Samreen and Chloe Chhor and Eric Kim and Jiyon Lee and Kristine Pysarenko and Beatriu Reig and Hildegard Toth and Divya Awal and Linda Du and Alice Kim and James Park and Daniel K. Sodickson and Laura Heacock and Linda Moy and Kyunghyun Cho and Krzysztof J. Geras},
  journal={Scientific Reports},
  year={2020},
  volume={12}
}

@article{Collins2011RacialDI,
  title={Racial Differences in Survival after Surgical Treatment for Melanoma},
  author={Karen Kadela Collins and Ryan C. Fields and Dadrie F. Baptiste and Yichuan Liu and Jeffrey F. Moley and Donna B. Jeffe},
  journal={Annals of Surgical Oncology},
  year={2011},
  volume={18},
  pages={2925-2936}
}
@inproceedings{jiang2023fedce,
  title={Fair Federated Medical Image Segmentation via Client Contribution Estimation},
  author={Jiang, Meirui and Roth, Holger R and Li, Wenqi and Yang, Dong and Zhao, Can and Nath, Vishwesh and Xu, Daguang and Dou, Qi and Xu, Ziyue},
  booktitle=CVPR,
  year={2023}
}

@article{Hatamizadeh2022GradViTGI,
  title={GradViT: Gradient Inversion of Vision Transformers},
  author={Ali Hatamizadeh and Hongxu Yin and Holger R. Roth and Wenqi Li and Jan Kautz and Daguang Xu and Pavlo Molchanov},
  journal={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
  pages={10011-10020}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{NEURIPS2022_232eee8e_flamby,
 author = {Ogier du Terrail, Jean and Ayed, Samy-Safwan and others},
 booktitle = NIPS,
 pages = {5315--5334},
 publisher = {Curran Associates, Inc.},
 title = {FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings},
 volume = {35},
 year = {2022}
}
