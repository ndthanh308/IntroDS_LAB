\documentclass[preprint,12pt,sort&compress]{elsarticle}
\usepackage{lineno}
\usepackage{amsmath,bm, amssymb,amsthm,mathrsfs}
\usepackage{mathtools,booktabs}
\usepackage{MnSymbol}
%\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{epstopdf}
\usepackage{float}
\usepackage{epsfig}
\usepackage[caption=false]{subfig}
\usepackage[svgnames]{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{listings}
\usepackage{multicol}
\usepackage{accents}
\usepackage{csquotes}
\usepackage{multirow}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newdefinition{rmk}{Remark}
\newdefinition{res}{Result}
\newproof{pf}{Proof}
\newproof{pot}{Proof of Theorem \ref{thm2}}
\usepackage{amssymb}


\journal{}

\begin{document}

\begin{frontmatter}

\title{Robust Inference of One-Shot Device testing data with Competing Risk under Lindley Lifetime Distribution with an application to SEER Pancreas Cancer Data }
\tnoteref{label1}
 \author{Shanya Baghel$^a$}
 \author{Shuvashree Mondal$^b$\corref{cor1}}
 \ead{shuvasri29@iitism.ac.in}
 \cortext[cor1]{}
 \affiliation{organization={Department of Mathematics and Computing, Indian Institute of Technology (Indian School of Mines) Dhanbad},
             %addressline={},
             %city={},
             postcode={826004},
             state={Jharkhand},
             country={India}}
 %\fntext[label3]{}


\author{}

 \affiliation{organization={Department of Mathematics and Computing, Indian Institute of Technology (Indian School of Mines) Dhanbad},
             %addressline={},
             %city={},
             postcode={826004},
             state={Jharkhand},
             country={India}}

\begin{abstract}
This article aims at the lifetime prognosis of one-shot devices subject to competing causes of failure.  Based on the failure count data recorded across several inspection times, statistical inference of the lifetime distribution is studied under the assumption of Lindley distribution.  In the presence of outliers in the data set, the conventional maximum likelihood method or Bayesian estimation may fail to provide a good estimate.  Therefore, robust estimation based on the weighted minimum density power divergence method is applied both in classical and  Bayesian frameworks.  Thereafter, the robustness behaviour of the estimators is studied through influence function analysis.  Further, in density power divergence based estimation, we propose an optimization criterion for finding the tuning parameter which brings a trade-off between robustness and efficiency in estimation.  The article also analyses when the cause of failure is missing for some of the devices.  The analytical development has been restudied through a simulation study and a real data analysis where the data is extracted from the SEER database. 
\end{abstract}

%%Graphical abstract
%\begin{graphicalabstract}
%% Figure removed
%\end{graphicalabstract}

%%Research highlights
\begin{highlights}
\item Robust classical inference based on weighted density power divergence
\item Robust Bayesian inference based on weighted density power divergence
\item Study of robustness through influence function
\item A new data-driven approach for optimal tuning parameter
\item Missing cause of failure analysis
\item Application to SEER Pancreas Cancer Data
\end{highlights}

\begin{keyword}
Density Power Divergence \sep Influence function\sep Lindley Distribution \sep Missing Cause of Failure\sep One shot devices\sep Optimal tuning parameter \sep Robust Bayes estimation 
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code
\MSC 62F10\sep 62F12\sep 62NO2.
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

 %\linenumbers

%% main text
\section{Introduction}
\label{sec1}
\noindent The prevalence of one-shot devices can be seen in various fields of human inventions.    These devices remain torpid until activated and are immediately destroyed after use.  The exact failure time of such devices is not observable in most instances.  Therefore, the observation is limited to recording whether the device failed before or after a specified time, leading to dichotomous data.  Several studies are devoted to the lifetime study of highly reliable one-shot devices under the accelerated life testing (ALT) setup.  

\noindent Fan et al. \cite{f2009} applied one-shot device testing data for the analysis of highly reliable electro-explosive devices using the Bayesian approach with an exponential, normal and beta prior distribution.  Balakrishnan and Ling \cite{bnl2012} were engaged in the classical inference of the lifetime distribution of one-shot devices with constant stress ALT under exponential lifetime distribution and compared it with the Bayesian framework developed by Fan et al. \cite{f2009}.  Balakrishnan and Ling \cite{bl2012} extended this work \cite{bnl2012} for multiple stress models.  Under constant stress ALT, Balakrishnan and Ling \cite{bl2014} assumed gamma lifetime distribution and drew classical inference in context one-shot devices data analysis.

\noindent There are many possible causes that can result in the failure of any one-shot device.  For example, a fuse can be failed due to high temperature, humidity, voltage or overloading.  The competing risk analysis confides in identifying the particular cause that caused the ultimate failure of a device.  In survival analysis where one-shot device data corresponds to current-status data, there may be multiple causes of death in which there is one cause which occurs earliest and ultimately results in the demise of an individual.  Crowder \cite{cr2001} thoroughly studied the statistical inference of classical competing risks.  Several studies under competing risk setup were conducted thereafter \cite{zhang2016statistical,dutta2023inference,wang2018inference,bai2020inference}.  In the context of one-shot devices, Balakrishnan et al. \cite{bet2015,bnsl2015} drew classical inference with constant stress competing risk set up under exponential and Weibull lifetime distributions, respectively.  Balakrishnan et al. \cite{bsl2015} also went through the Bayesian approach for competing risk analysis of one-shot devices under exponential lifetime distribution.

 \noindent Most of the work is focused on exponential, gamma and Weibull as lifetime distributions for one-shot device data in the literature.  In this work, we come up with the study of statistical inference of the lifetime distribution of one-shot devices under independent competing risk setup where the lifetime under each risk follows two-parameter Lindley distribution.  Lindley distribution was introduced by Lindley \cite{lind1958}, \cite{lind1965} to analyse lifetime data, especially to study stress-strength reliability.  Ghitany et al. \cite{ghit2008} showed with evidence that the Lindley distribution offers better modelling than exponential distribution in terms of mathematical properties like mode, coefficient of variation, skewness, kurtosis and hazard rate shape.  Lindley distribution has broad applicability in analysing failure time data.  Mazucheli and Achcar \cite{m2011} used it as an alternative to exponential or Weibull lifetime data under the competing risk setup.  


 \noindent 
 In reality, it is quite likely to get data with outliers.  The conventional inferential methods designed for ideal situations fail here, and robust inference methods are needed.  In the literature, Basu et al. \cite{basu1998} introduced the density power divergence-based robust method of estimation.  In one-shot devices data analysis,  Balakrishnan et al. \cite{bp(2019), bcmp2019, blcamapa2020} carried the density power divergence based robust inference under gamma, Weibull and exponential lifetime models.  It has been observed that very few works on competing risk with robust estimation are available in the literature.  Only recently, Balakrishnan et al. \cite{pardo2023},  Balakrishnan and Castilla \cite{balla2023} developed robust inference under competing risk setup for one-shot devices.  Further, it has been observed through extensive literature surveys that robust Bayes estimation has yet to be studied in the context of one-shot devices.  The methodology of drawing a robust inference in the Bayesian model was first introduced by Hooker and Vidyashankar \cite{hook2014}.  Further, Ghosh and Basu \cite{g2016} developed robust Bayes estimation where the likelihood function has been substituted by the density power divergence measure in the posterior density function.
 

 \noindent The present article focuses on developing a robust estimation method in classical and Bayesian frameworks where the failure of one-shot devices is subject to independent competing risks with an interval monitoring setup under Lindley lifetime distribution.  The estimation procedure is based on the density power divergence measure between the true and assumed lifetime distributions.  Certain weights are associated with the density power divergence measure and thereafter, a weighted minimum density power divergence estimator (WMDPDE) is obtained.  Further, we study the asymptotic behaviour of the robust WMDPD estimator under this setup.  With the availability of prior information, Bayesian inference is quite essential.  In this work, following the idea by Ghosh and Basu \cite{g2016}, a weighted robust Bayes estimation (WRBE) is proposed where the robustified posterior density was developed on the exponential form of the maximiser equation based on density power divergence measure.  The Normal distribution and Dirichlet distribution are taken as priors which are based on the data.  Further, the robustness of the derived estimators is studied through the influence function.  The numerical study is conducted through simulation experiments and real data analysis to assess the performances of the derived methods.


 \noindent It is to be noted that in the estimation procedure based on the density power divergence method, the tuning parameter plays a pivotal role in bringing a balance between robustness and efficiency.  Warwick and Jones \cite{w2005} introduced a data-driven algorithm for choosing the optimal tuning parameter, which was further studied and modified by several authors (\cite{gbas2015,bas2021,cas2022}).  In this work, we propose an approach aiming to increase the robustness as well as precision of the estimation.  In that direction, a numerical study is also presented.

 \noindent The data analysis is conducted using information extracted from the database named ``Incidence $\cdot$SEER Research Data, 8 Registries, Nov 2021 Sub (1975-2019)" \cite{seer} which is maintained by National Cancer Institute (Surveillance, Epidemiology, and End Results Program (SEER)).  The data is focused only on individuals diagnosed with pancreatic cancer from 2008-2010.  The layout of data is given in Table \eqref{t7}.  It was also observed that for some of the patients, the demise of the patient was recorded, but the cause of death was missing.  This is frequently followed in real life, where the cause of the failure remains unspecified.  Lu and Liang \cite{lu2008} studied that analysis using only known causes of failures would lead to biased results.  To avoid this, the missing cause of failure analysis has been conducted separately in this work and the layout of the missing cause of failure data is given in Table \eqref{t10}.   This real data set is analysed by exploiting the developed methods and the results are discussed thoroughly.

 
 \noindent The rest of the article comprises sections divided as follows.  Section \ref{sec2} is concerned with model building and computation of maximum likelihood estimation.  Section \ref{sec3} focuses on deriving a robust WMDPD estimator and studying its asymptotic property.  The robust Bayes method of estimation is introduced in section \ref{sec4}.  In section \ref{sec5}, we study the robustness properties of the estimators through the influence function.  Section \ref{sec6} is devoted to the missing cause of failure analysis.  Section \ref{sec7} deals with the numerical study of the theoretical results developed in previous sections through a simulation study followed by the finding of optimal tuning parameter and data analysis.  Concluding remarks are given in Section \ref{sec8}.


\section{Model Description}\label{sec2}

\noindent This section is concerned with the model formulation for the lifetime data analysis of one-shot devices and the development of Maximum Likelihood Estimates (MLE). 

\noindent One-shot devices are studied under accelerated life testing (ALT), where $G$ devices are distributed into $I$ independent groups.  For $i=1,2,\dots,I,$ let the number of devices in $ith$ group be $g_i$ where $G=g_1+g_2+\dots+g_I$.  These devices are subject to $J$ type of stress factors, quantified by $s_{ij}$ ; $j=1,\dots, J$, where $s_{i0}=1$.   The failure mechanism is subject to R independent competing causes.  Let, $n_{ilr}$ denote the number of failures in the $ith$ group due to cause $r$ for $r=1,2,\dots,R,$ between the inspection times $(\tau_{i(l-1)},\tau_{il}]$  where $l=1,2,\dots,L$, and $\tau_{i0}=0.$   Therefore, the number of devices that survive after inspection time $\tau_{iL}$ for the $ith$ group is $k_i=g_i-\sum_{l=1}^{L}\sum_{r=1}^{R}n_{ilr}$.  Table \eqref{t1} shows the layout in tabular form.
\begin{table}[htb!]
	\caption{Model layout}
	\label{t1}
	\begin{center}	
 {\scalebox{0.88}{
		\begin{tabular}{cccccccccc} 
			\hline
			\textbf{Groups}&\textbf{Devices}&\multicolumn{4}{c}{\textbf{Co-factors}} &\multicolumn{4}{c}{\textbf{Inspection}} \\\cline{3-6}
			& &\textbf{Stress 1} &\textbf{Stress 2} &\dots & \textbf{Stress J}  &\multicolumn{4}{c}{\textbf{Times}}\\\hline 
		1&$g_1$  &$s_{11}$ &$s_{12}$ & \dots &$s_{1J}$&$\tau_{11}$ &$\tau_{12}$ &\dots &$\tau_{1L}$ \\
		2&$g_2$  &$s_{21}$ & $s_{22}$&\dots &$s_{2J}$&$\tau_{21}$ &$\tau_{22}$ &\dots &$\tau_{2L}$ \\
		\vdots&\vdots &\vdots &\vdots &$\udots$ & \vdots&\vdots & \vdots& $\udots$&\vdots \\
		i&$g_i$ &$s_{i1}$ &$s_{i2}$ &\dots &$s_{iJ}$ &$\tau_{i1}$ &$\tau_{i2}$ &\dots &$\tau_{iL}$ \\
		\vdots&\vdots &\vdots &\vdots &$\udots$ &\vdots &\vdots & \vdots& \vdots&\vdots \\
		I&$g_I$  &$s_{I1}$ & $s_{I2}$&\dots &$s_{IJ}$&$\tau_{I1}$ &$\tau_{I2}$ & &$\tau_{IL}$ \\
		\hline
		\textbf{Groups}&\textbf{Survivals} &\multicolumn{8}{c}{\textbf{Failures}}\\\cline{7-10}
		\hline
	   1 &$k_1$ & \multicolumn{3}{c}{$(n_{111},\dots,n_{11R})$}  &\multicolumn{1}{c}{\dots}  &\multicolumn{4}{c}{$(n_{1L1},\dots,n_{1LR})$} \\
	    2 &$k_2$ & \multicolumn{3}{c}{$(n_{211},\dots,n_{21R})$}  &\multicolumn{1}{c}{\dots}  &\multicolumn{4}{c}{$(n_{2L1},\dots,n_{2LR})$} \\
	     \vdots &\vdots & \multicolumn{3}{c}{\vdots}  &\multicolumn{1}{c}{$\udots$}  &\multicolumn{4}{c}{\vdots} \\
	     i &$k_i$ & \multicolumn{3}{c}{$(n_{i11},\dots,n_{i1R})$}  &\multicolumn{1}{c}{\dots}  &\multicolumn{4}{c}{$(n_{iL1},\dots,n_{iLR})$} \\
	     \vdots &\vdots & \multicolumn{3}{c}{\vdots}  &\multicolumn{1}{c}{$\udots$}  &\multicolumn{4}{c}{\vdots} \\
	  I &$k_I$ & \multicolumn{3}{c}{$(n_{I11},\dots,n_{I1R})$}  &\multicolumn{1}{c}{\dots}  &\multicolumn{4}{c}{$(n_{IL1},\dots,n_{ILR})$} \\  
	   \hline
		\end{tabular}}}
	\end{center}
\end{table}

\noindent In $i$th group for $i=1,2,\dots,I,$ the failure time due to $r$th competing cause for $r=1,\ldots,R,$ is denoted by the random variable $T_{ir},$ 
  which is assumed to follow two-parameter Lindley distribution with shape parameter $\alpha_{ir}$ and scale parameter $\theta_{ir}$.  The cumulative distribution function and probability density function of $T_{ir}$ are as follows. 
\begin{align*}
F_{ir}(t)&=1-\left(\frac{1+\alpha_{ir}\theta_{ir}+\theta_{ir} t}{\alpha_{ir}\theta_{ir}+1}\right)e^{-\theta_{ir} t}\;;\;(t,\theta_{ir})>0,\alpha_{ir}+\theta_{ir}>0,\\
f_{ir}(t)&=\frac{\theta_{ir}^2}{\alpha_{ir}\theta_{ir}+1}\left(\alpha_{ir}+t\right)e^{-\theta_{ir} t}\;;\;(t,\theta_{ir})>0,\alpha_{ir}+\theta_{ir}>0.
\end{align*}


\noindent Both shape and scale parameters are related to stress factors in log-linear form as
\begin{equation*}
    \alpha_{ir}=exp\left\{\sum_{j=0}^{J}a_{rj} s_{ij}\right\},\;
    \theta_{ir}=exp\left\{\sum_{j=0}^{J}b_{rj} s_{ij}\right\}\; ; \; r=1,2,\dots,R.
\end{equation*}
For the computational conveniences, two competing causes of failure and one stress factor are considered without the additive constant and denote $s_{i1}=s_i.$   Hence, $\bm{\Lambda}=\{( a_{1},b_{1}, a_{2}, b_{2} )\}^{'}$ are the model parameters to be estimated.

\noindent The failure probabilities $p_{il1}$, $p_{il2}$ due to cause 1, cause 2, respectively, in the interval $(\tau_{i(l-1)}, \tau_{il} )$ for $l=1,\ldots, L$ and the survival probability are obtained as follows.
\begin{align*}
   p_{il1} &=P(\tau_{i(l-1)}<T_{i1}\leq\tau_{il},T_{i2}>T_{i1})\\ 
   p_{il2}&=P(\tau_{i(l-1)}<T_{i2}\leq\tau_{il},T_{i1}>T_{i2})\\
   p_{i0}&=P(T_{il1g}>\tau_{iL},T_{il2g}>\tau_{iL}).
\end{align*}


\begin{res}\label{res1}
Under the assumption of Lindley distribution, 
\begin{align}
p_{il1}&=\frac{\theta_{i1}^2\theta_i^{-3}A^{(p_1)}_i}{\prod_{r=1}^{2}(\alpha_{ir}\theta_{ir}+1)}\;;\;
p_{il2}=\frac{\theta_{i2}^2\theta_i^{-3}A^{(p_2)}_i}{\prod_{r=1}^{2}(\alpha_{ir}\theta_{ir}+1)};\notag\\
p_{i0}&=\prod_{r=1}^{2}\left(\frac{1+\alpha_{ir}\theta_{ir}+\tau_{iL}\theta_{ir}}{\alpha_{ir}\theta_{ir}+1}\right)e^{-\tau_{iL}\theta_{ir}}.\label{1}
\end{align}
\end{res}
\begin{proof}
	The proof of the result and description of notations are given in the appendix.
\end{proof}

\noindent  Based on the observed failure count data, the Likelihood function can be obtained as,
\begin{equation}
    L(\bm{\bm{\Lambda}})\propto\prod_{i=1}^{I}\left[(p_{i0})^{k_i}\prod_{l=1}^{L}\prod_{r=1}^{2}(p_{ilr})^{n_{ilr}}\right].
\end{equation}
Therefore, the log-likelihood function is given as,
\begin{equation}
    ln\,L(\bm{\bm{\Lambda}})\propto\sum_{i=1}^{I}\left[k_i\, ln\,(p_{i0})]+\sum_{l=1}^{L}\sum_{r=1}^{2}n_{ilr}\, ln\,(p_{ilr})\right] . \label{3}
\end{equation}
Hence MLE of $\bm{\bm{\Lambda}},$ denoted by $\;\hat{\bm{\bm{\Lambda}}}=\{\hat{a}_1,\hat{b}_1,\hat{a}_2, \hat{b}_2\}$ would be derived as 
\begin{equation}
	\hat{\bm{\bm{\Lambda}}}=arg\mathop{max}_{\bm{\bm{\Lambda}}}ln L(\bm{\bm{\Lambda}}) . \label{4}
\end{equation}
provided $\sum_{i=1}^{I}\sum_{l=1}^{L}\sum_{r=1}^{2}n_{ilr}>0.$\\

\begin{res}\label{res2}
The set of estimating equations for obtaining MLE is given as follows:
\begin{equation}
    \sum_{i=1}^{I}s_i \bm{\Theta}_i\left[\frac{k_i}{p_{i0}} B_{i0}^{(\bm{\bm{\Lambda}})} C_i^{(p_0)}+\sum_{l=1}^{L}\sum_{r=1}^{2}\frac{n_{ilr}}{p_{ilr}} B_{ilr}^{(\bm{\bm{\Lambda}})} C_i^{(p_r)}\right]=\mathbf{0_4} .\label{5}
\end{equation}
\end{res}
\begin{proof}
The proof of the result and description of notations are given in the appendix.
\end{proof}
\noindent In the presence of outliers in the data set, MLE cannot provide a valid estimated value.  Therefore, some robust estimation method needs to be developed.

\section{Robust point estimation }\label{sec3}

\noindent In this section, a robust estimation method based on the density power divergence (DPD) measure will be discussed.  The DPD-based estimation was proposed by Basu et al. \cite{basu1998}.  Under the assumption that two probability distributions $F$ and $G$ having probability densities $f$ and $g$ respectively, the DPD measure between $g$ and $f$ is given as,
\begin{equation*}
D_{\gamma}(g, f)=\int \Bigg\{f^{1+\gamma}(t)-\left(1+\frac{1}{\gamma}\right) g(t)\,f^{\gamma}(t) +\left(\frac{1}{\gamma}\right)g^{1+\gamma}(t)\Bigg\}~dt\,;\quad 0<\gamma\leq 1.
\end{equation*}
where $\gamma$ is termed as the tuning parameter.
 
\noindent For one-shot device data, the DPD measure is computed between empirical and theoretical probability distributions.  The empirical failure probability due to two different causes and empirical survival probability is defined as,
 \begin{equation}
   \left(\hat{q}_{il1}, \hat{q}_{il2}, \hat{q}_{i0}\right)=\left(\frac{n_{il1}}{g_i}, \frac{n_{il2}}{g_i}, \frac{k_i}{g_i}\right)\,;\;i=1,2,\dots,I\,;l=1,2,\dots,L. \label{6} 
 \end{equation}
where the theoretical failure probabilities and survival probabilities are given by equation \eqref{1}.  Applying weights proportional to the size of groups, the weighted DPD (WDPD) measure with the weights, $w_i=\frac{g_i}{G}$ combining all the I groups is obtained as,
\begin{align}
     D^w_{\gamma}(\bm{\bm{\Lambda}})=\sum_{i=1}^{I}\frac{g_i}{G}&\left[\left\{(p_{i0})^{\gamma+1}+\sum_{l=1}^{L}\sum_{r=1}^{2}(p_{ilr})^{\gamma+1}\right\}\right.-\frac{\gamma+1}{\gamma}\bigg\{(\hat{q}_{i0}p_{i0}^{\gamma})+\Bigg.\notag\\
    &\left.\sum_{l=1}^{L}\sum_{r=1}^{2}\hat{q}_{ilr}(p_{ilr})^{\gamma}\right\}+\left.\frac{1}{\gamma}\left\{(\hat{q}_{i0})^{\gamma+1}+\sum_{l=1}^{L}\sum_{r=1}^{2}(\hat{q}_{ilr})^{\gamma+1}\right\}\right] .\label{7}
\end{align}  
When $\gamma$ tends to $ 0,$ $D^w_{\gamma}(\bm{\bm{\Lambda}})$ will converge to weighted Kullback-Leibler (KL) divergence measure $D_{KL}(\bm{\bm{\Lambda}})$ where
\begin{align*}
    D_{KL}(\bm{\bm{\Lambda}})&=\sum_{i=1}^{I}\frac{g_i}{G}\hat{q}_{i0}\,ln\left\{\frac{\hat{q}_{i0}}{p_{i0}}\right\}+\sum_{l=1}^{L}\sum_{r=1}^{2}\hat{q}_{ilr}\,ln\left\{\frac{\hat{q}_{ilr}}{p_{ilr}}\right\} .
\end{align*}

\noindent DPD measure can also be written in the form of likelihood function as follows:
\begin{equation*}
\mathop{lim}_{\gamma \to 0^{+}}D^w_{\gamma}(\bm{\bm{\Lambda}})
		=c-\frac{1}{G}\;ln L(\bm{\bm{\Lambda}}) .
\end{equation*}
where $c$ contains the terms independent of parameters.\\
\noindent The weighted minimum density power divergence estimators (WMDPDE) for estimating $\bm{\bm{\Lambda}}$ can be obtained by minimizing the WDPD measure as follows:
	\begin{equation}
		\hat{\bm{\bm{\Lambda}}}_{\gamma}^w=arg\mathop{min}_{\bm{\bm{\Lambda}}}D^w_{\gamma}(\bm{\bm{\Lambda}}) .\label{8}
	\end{equation}
	
\noindent Note that, the tuning parameter $\gamma$ plays a role in bringing a balance between robustness and efficiency in this estimation.


	\begin{res}\label{res3}
The set of estimating equations for obtaining WMDPDEs is given as follows:
	\begin{equation}
	    \sum_{i=1}^{I}g_i\,s_i\bm{\Theta}_i\left[Q_iB^{(\bm{\bm{\Lambda}})}_{i0}C_i^{(p_0)}+\sum_{l=1}^{L}\sum_{r=1}^{2}Q_{ilr}B^{(\bm{\bm{\Lambda}})}_{ilr}C_i^{(p_r)}\right]=\mathbf{0_4} .\label{9}
	\end{equation}
	\end{res}
	\begin{proof}
The proof of the theorem and description of notations are given in the appendix.
	\end{proof}
 \noindent As it is seen in the Results \eqref{res2} and \eqref{res3}, the explicit form of the MLEs and WMDPDEs could not be found.  Hence, the Co-ordinate Descent algorithm is used to obtain the estimates.  The steps of the algorithm are given in Algorithm \eqref{alg1}
 \begin{algorithm}[htb!]
\caption{{Coordinate-Descent Algorithm}}\label{alg1}
\begin{itemize}
\item Chose the initial values $\bm{\Lambda}_0=(a^0_1,b_1^0,a^0_2,b_2^0)$.
\item At the $m+1^{th}$ iteration, the estimate of the parameters can be derived as \\
    $
        a^{(m+1)}_1= a^{(m)}_1-\alpha\frac{\partial D(a_1^{(m)}, b_1^{(m)}, a_2^{(m)}, b_2^{(m)})}{\partial a_1}\\
         b^{(m+1)}_1= b^{(m)}_1-\alpha\frac{\partial D(a_1^{(m+1)}, b_1^{(m)}, a_2^{(m)}, b_2^{(m)})}{\partial b_1}\\
        a^{(m+1)}_2= a^{(m)}_2-\alpha\frac{\partial D(a_1^{(m+1)}, b_1^{(m+1)}, a_2^{(m)}, b_2^{(m)})}{\partial a_2}\\
         b^{(m+1)}_2= b^{(m)}_2-\alpha\frac{\partial D(a_1^{(m+1)}, b_1^{(m+1)}, a_2^{(m+1)}, b_2^{(m)})}{\partial b_2}\\
   $  
where $D=-ln\,L(\bm{\Lambda})$ for MLE and $D=D^w_{\gamma}(\bm{\Lambda})$ for WMDPDE and $\alpha$ is the learning rate.
\item Continue the process until \{$max\vert \bm{\Lambda}^{(m+1)}-\bm{\Lambda}^{(m)}\vert\,,\,max\vert D(\bm{\Lambda}^{(m+1)})-D(\bm{\Lambda}^{(m)})\vert<c$\} where $c$ is a predefined threshold value. 
\end{itemize}
\end{algorithm}

\noindent To study the asymptotic behaviour of WMDPDE, the following theorem is presented, which is based on the idea of Calvino et al.\cite{cal2021}.
\begin{thm}\label{thm1}
Let $\bm{\Lambda}^0$ be the true value of the parameter $\bm{\Lambda}$.  The asymptotic distribution of WMDPDE of $\bm{\Lambda}$, $\hat{\bm{\Lambda}}_{\gamma}$, is given by,
	\begin{equation}
		\sqrt{G}(\hat{\bm{\Lambda}}_{\gamma}-\bm{\Lambda}^0)\xrightarrow[G\to \infty]{\mathscr{L}} N\left(\mathbf{0_4},Q_{\gamma}^{-1}(\bm{\Lambda}^0)R_{\gamma}(\bm{\Lambda}^0)Q_{\gamma}^{-1}(\bm{\Lambda}^0)\right)\label{10}
	\end{equation}
\end{thm}
	\begin{proof}
	The proof of the theorem and description of notations are given in the appendix.
	\end{proof}

\section{Robust Bayes Method of Estimation}\label{sec4}
\noindent Bayesian inference is of paramount interest when some prior information is available about the model parameters.  Conventional Bayes estimation based on likelihood-based posterior is quite popular because of several optimal properties.  But the major drawback is that it can not produce a good estimated value when data come with contamination.  In the literature, the non-robustness problem is tried to be solved by replacing the likelihood function in the posterior by some robust loss function and the derived posterior is called a pseudo posterior.  Readers may see  Greco et al. \cite{gre:2008},  ChÃ©rief et al.  \cite{cher:2020}, Jewson et al. \cite{jew:2018}, Nakagawa and Hashimoto \cite{nak:2020} in this reference.
Inspired by Ghosh and Basu \cite{g2016}, a robust Bayesian estimation based on the power divergence measure is proposed here for the reliability analysis of a one-shot device with a competing risk interval monitoring set-up.  The following developments are done in that direction.\\
\noindent Define, 
\begin{flalign}
B^{w}_{\gamma}(\bm{\Lambda})=\sum_{i=1}^{I}\frac{g_i}{G}&\left[\frac{1}{\gamma}\left\{(\hat{q}_{i0}p_{i0}^{\gamma})+\sum_{l=1}^{L}\sum_{r=1}^{2}\hat{q}_{ilr}(p_{ilr})^{\gamma}\right\}\right.\notag\\
&\qquad\qquad\left.-\frac{1}{\gamma+1}\left\{(p_{i0})^{\gamma+1}+\sum_{l=1}^{L}\sum_{r=1}^{2}(p_{ilr})^{\gamma+1}\right\}\right].
&&
\end{flalign}
where WMDPDE with $\gamma>0$ is the maximizer of $B^{w}_{\gamma}(\bm{\Lambda})$.
Therefore, the weighted robust posterior density, a pseudo posterior can be defined as follows,
\begin{equation}
\pi^w_{\gamma}(\bm{\Lambda}\vert data)=\frac{\exp\left(B^{w}_{\gamma}(\bm{\Lambda})\right)\pi(\bm{\Lambda})}{\int \exp\left(B^{w}_{\gamma}(\bm{\Lambda})\right)\pi(\bm{\Lambda})\,d\bm{\Lambda}}.\label{12}
\end{equation}
Here, $\pi(\bm{\Lambda})$ is the joint prior density, and $\pi^w_{\gamma}(\bm{\Lambda}\vert data)$ is the proper density for $\gamma\geq 0$.
For $\gamma \rightarrow 0$, the robust pseudo posterior will converge to the conventional likelihood-based posterior density.  For any loss function $L(.,.),$ the Bayes estimator can be obtained as 
$$arg  \min_t \int L(\bm{\Lambda}, t) \pi^w_{\gamma}(\bm{\Lambda}\vert data) d \bm{\Lambda}.$$
For the squared error loss function, the Weighted Robust Bayes Estimator (WRBE) can be obtained as
\begin{equation}
\hat{\bm{\Lambda}}^{w}_{b \gamma}=\int \bm{\Lambda}\pi^w_{\gamma}(\bm{\Lambda}\vert data)\,d\bm{\Lambda}.
\end{equation}

\subsection{Choice of Priors}

\noindent In Bayesian inference choice of prior governs the estimation.  In this section, we mention few such prior choices.  For the model parameters $a_1, a_2, b_1, b_2,$ the interpretation of prior choice is not so meaningful.  Following the idea of Fan et al. \cite{f2009}, the prior information on $p_{ilr}$'s is considered.  For further development, we need the emperical estimates of $p_{ilr}$'s given in \eqref{6}.  But to avoid the zero-frequency situation, we follow the idea of Lee et al.(1985) \cite{le1985} and define,
\begin{equation} (\tilde{q}_{i0}, \tilde{q}_{il1}, \tilde{q}_{il2})=\left(\frac{k_i+1}{g_i+3}, \frac{n_{il1}+1}{g_i+3}, \frac{n_{il2}+1}{g_i+3}\right) \label{21}.
\end{equation}

\subsubsection{ Normal Prior based on data}

\noindent Define, error $\{e_{ilr} \}$'s as the difference between empirical estimates and the true probabilities as follows. 
\begin{equation}
\tilde{q}_{ilr}=p_{ilr}+e_{ilr}\;;\quad i=1,2,\dots,I\;;\;l=1,2,\dots,L\;;\;r=1,2.
\end{equation}
with the assumption that $e_{ilr}$ are independent $N(0,\sigma^2)$ variables.  Therefore, the conditional likelihood function as the prior distribution of $\bm{\Lambda}$ given $\sigma^2$ can be obtained as follows.
\begin{equation}
L(\bm{\Lambda}\vert \sigma^2)\propto \prod_{i=1}^{I}\prod_{l=1}^{L}\prod_{r=1}^{2}\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(p_{ilr}-\tilde{q}_{ilr})^2\right\} .
\end{equation}
Using non-informative prior of $\sigma^2$,
$\pi(\sigma^2)\propto \frac{1}{\sigma^2},$
the joint prior density of $\bm{\Lambda}$ is given as follows.
\begin{equation*}
\pi^{(1)}(\bm{\Lambda})\propto \int_{0}^{\infty}L(\bm{\Lambda}\vert \bm{\tau}, \bm{s},\sigma^2)\pi(\sigma^2) d\sigma^2\propto \left\{\sum_{i=1}^{I}\sum_{l=1}^{L}\sum_{r=1}^{2}(p_{ilr}-\tilde{q}_{ilr})^2\right\}^{-IL}.
\end{equation*}
Then by \eqref{12}, posterior density would be given as follows.
\begin{equation}
\pi^{(1)}_{\gamma}(\bm{\Lambda}\vert data)\propto \exp\left(B^w_{\gamma}(\bm{\Lambda})\right)\left\{\sum_{i=1}^{I}\sum_{l=1}^{L}\sum_{r=1}^{2}(p_{ilr}-\tilde{q}_{ilr})^2\right\}^{-IL}.
\end{equation}



\subsubsection{Dirichlet Prior based on data}

\noindent If a parameter can be interpreted as some probability, beta prior is a very natural choice.  Extending this idea, a Dirichlet prior is considered for the failure and survival probabilities as follows. 
\begin{equation*}
\pi_{i}^{(2)}=\frac{p_{i0}^{\beta_{i0}-1}\prod_{l=1}^{L}\prod_{r=1}^{2}p_{ilr}^{\beta_{ilr}-1}}{\bm{B}(\bm{\beta_i})}.
\end{equation*}
where, $\beta_{i0},\beta_{ilr}>0$ for $i=1,\ldots, I$ $l=1,\ldots, L$, $r=1,2$ and $$\bm{B}(\bm{\beta_i})=\frac{\Gamma \beta_{i0}\prod_{l=1}^{L}\prod_{r=1}^{2}\Gamma\beta_{ilr}}{\Gamma \left(\beta_{i0}+\sum_{l=1}^{L}\sum_{r=1}^{2}\beta_{ilr}\right)}.$$
The hyper-parameters are chosen equating the expectation of the failure and survival probabilities with their empirical estimates and equating variance as a known constant.  Therefore, we get
\begin{equation}
E(p_{i0})=\frac{\beta_{i0}}{\beta_{i0}+\sum_{l=1}^{L}\sum_{r=1}^{2}\beta_{ilr}}=\tilde{q}_{i0}\;,\; E(p_{ilr})=\frac{\beta_{ilr}}{\beta_{i0}+\sum_{l=1}^{L}\sum_{r=1}^{2}\beta_{ilr}}=\tilde{q}_{ilr}\;;\;r=1,2\label{18}
\end{equation}
\begin{equation}
Var(p_{i0})=\frac{\beta_{i0}\left(\sum_{r=1}^{2}\beta_{ilr}\right)}{\left(\beta_{i0}+\sum_{l=1}^{L}\sum_{r=1}^{2}\beta_{ilr}\right)^2\left(\beta_{i0}+\sum_{l=1}^{L}\sum_{r=1}^{2}\beta_{ilr}+1\right)}=\sigma^2_{(p)}.\label{19}
\end{equation}
where, $\sigma^2_{(p)}$ is assumed to be a prefixed quantity.  By equations \eqref{18} and \eqref{19} the estimates of hyper-parameters are,
\begin{align*}
\hat{\beta}_{il1}&=\tilde{q}_{il1}\left\{\frac{\tilde{q}_{i0}(1-\tilde{q}_{i0})}{\sigma^2_{(p)}}-1\right\}\; ,\quad \hat{\beta}_{il2}=\tilde{q}_{il2}\left\{\frac{\tilde{q}_{i0}(1-\tilde{q}_{i0})}{\sigma^2_{(p)}}-1\right\}\quad \text{and}\\
\hat{\beta}_{i0}&=\left\{\frac{\tilde{q}_{i0}(1-\tilde{q}_{i0})}{\sigma^2_{(p)}}-1\right\}-\sum_{l=1}^{L}\sum_{r=1}^{2}\hat{\beta}_{ilr}.
\end{align*}
Then by \eqref{12}, the joint posterior density would be given as follows.
\begin{equation}
\pi^{(2)}_{\gamma}(\bm{\Lambda}\vert data)\propto \exp\left(B^w_{\gamma}(\bm{\Lambda})\right)\prod_{i=1}^{I}\left[p_{i0}^{\hat{\beta}_{i0}-1}\prod_{l=1}^{L}\prod_{r=1}^{2}\left\{p_{ilr}^{\hat{\beta}_{ilr}-1}\right\}\right]\label{20}.
\end{equation}

\noindent Under both the prior assumption, the Bayes estimate cannot be obtained in closed form.  Hence we rely on the Metropolis-Hastings (MH) \cite{h1970} algorithm. The steps of the algorithm is given in algorithm \eqref{alg2}. 
\begin{algorithm}[htb!]
\caption{{Metropolis-Hastings (MH) Algorithm}}\label{alg2}
\begin{itemize}
\item Chose the initial value $\bm{\Lambda}_0=(a^0_1,b_1^0,a^0_2,b_2^0)$.
\item For $i=1,2,\dots,N;$ generate a value $\bm{\Lambda}^*=(a^*_1,b_1^*,a^*_2,b_2^*)$ from proposal distribution $\pi^*_{\gamma}({\cdot}\vert \bm{\Lambda}_{i-1} ).$ where $\bm{\Lambda}_{i-1}=(a^{i-1}_1,b_1^{i-1},a^{i-1}_2,b_2^{i-1}).$
\item Obtain acceptance probability $\alpha=min\left\{1,\frac{\pi^*_{\gamma}(\bm{\Lambda}_{i-1}\vert\bm{\Lambda}^{*})\pi^{(k)}_{\gamma}(\bm{\Lambda}^{*}\vert data)}{\pi^*_{\gamma}(\bm{\Lambda}^{*}\vert \bm{\Lambda}_{i-1})\pi^{(k)}_{\gamma}(\bm{\Lambda}_{i-1} \vert data)}\right\}$ for $k=1,2.$
\item Generate a random number $u\sim U(0,1)$:\\
If $u<\alpha$, set $\bm{\Lambda}_i=\bm{\Lambda}^{*},$ else $\bm{\Lambda}_i=\bm{\Lambda}_{i-1}$ where $\bm{\Lambda}_i=(a^i_1,b_1^i,a^i_2,b_2^i).$ 
\end{itemize}
\end{algorithm}
\noindent Note that, for each of the $a_1, b_1, a_2, b_2, $ the proposal distribution is chosen as
$$\pi_{\gamma}(\cdot| y)\sim \delta\, U(y -a, y +a)+(1-\delta)\, N(y,\sigma^2)\;;\; 0<\delta < 1.$$ where $U(y-a, y+a)$ denotes a uniform distribution with range $( y-a,y +a)$
and $N(y,\sigma^2)$ is the normal distribution with mean y and variance $\sigma^2$.  The mixture of Uniform and Normal distribution is taken so that a high acceptance rate can be achieved. For estimation purposes, a sequence of random variables is generated through the MH algorithm.  Let the total number of generated values be $N$.  First say $N_0$ iterative values are removed for burn-in and after $N_0^{th}$ values onwards, every $m^{th}$ value is taken to reduce auto-correlation among generated values.  In the simulation study, the specific values of $N$, $N_0$ and $m$ are taken for the analysis.  A total of say $N^{'}$ values for each of $a_1,b_1,a_2,b_2$ are finally obtained.  Based on these obtained values, the Bayes estimates and the highest posterior density credible intervals (HPD CRI) of the model parameters can be approximated by using algorithm \eqref{alg3}.
 \begin{algorithm}[htb!]
\caption{{Bayes Estimates and HPD Credible Intervals}}\label{alg3}
\begin{itemize}
\item The approximated Bayes estimate of $a_1$ can be obtained by $\frac{1}{N^{'}}\sum_{i=1}^{N^{'}}a^i_1$.


\item For $100(1-\xi)\%$ CRI of $a_1$:\\
Sort $a^i_1$'s in ascending order to obtain $(a^{(1)}_1, a^{(2)}_1, \ldots, a^{(N^{'})}_1)$ and 
$(a^{(j)}_1,a^{(j+[N^{'}(1-\xi)])}_1)$ for $ j=1,\dots,[N^{'}\xi]$ is the $100(1-\xi)\%$ credible inervals .
\item The $100(1-\xi)\%$ HPD CRI is $(a^{(j^*)}_1,a^{(j^*+[N^{'}(1-\xi)])}_1)$ such that 
$(a^{(j^*)}_1,a^{(j^*+[N^{'}(1-\xi)])}_1)\leq (a^{(j)}_1,a^{(j+[N^{'}(1-\xi)])}_1); j=1,\dots,[N^{'}\xi]$.\\

Similar way, the Bayes estimates and the CRIs of $b_1, a_2, b_2$ can be obtained. 
\end{itemize}
\end{algorithm}

\section{Property of Robustness}{}\label{sec5}

\noindent In previous sections we have derived the estimators of the model parameters using the robust estimation method.  In this section robustness of those estimators will be studied through the Influence Function (IF) \cite{hm1986}.  If $S(M)$ denotes the functional of any estimator from the true distribution M, then the IF is defined as follows.

\begin{equation*}
IF(t;S,M)=\lim_{\epsilon \to 0} \frac{S(M_{\epsilon}) - S(M) }{\epsilon} =\left. \frac{\partial S(M_{\epsilon}) }{\partial \epsilon} \right\vert_{\epsilon =0}.
\end{equation*}
where $M_{\epsilon}=(1-\epsilon)M+\epsilon\Delta_{t}$, $\epsilon (0 < \epsilon < 1)$ being the proportion and $\Delta_t$ being the degenerate
distribution at the point t.
\subsection{Influence Function of WMDPDE}
\noindent Let M be the true distribution from which data is generated.  If $S_{\gamma}(M)$ be the functional of WMDPDE $\hat{\bm{\Lambda}}_{\gamma}$, then $S_{\gamma}(M)$ is the value of $\bm{\Lambda}$ which minimizes,
\begin{align*}
\sum_{i=1}^{I}\frac{g_i}{G}\left[\left\{p_{i0}^{\gamma+1}+\sum_{l=1}^{L}\sum_{r=1}^{2}p^{\gamma+1}_{ilr}\right\}-\frac{\gamma+1}{\gamma}\left\{\left(\int_{I_{i0}}^{}d M\right)p^{\gamma}_{i0}+\sum_{l=1}^{L}\sum_{r=1}^{2}\left(\int_{I_{ilr}}^{}d M\right)p^{\gamma}_{ilr}\right\}\right].
\end{align*}
where,
$\bm{t}=(t_1, t_2)$ with $t_1,t_2 \in R$ and $\bm{t} \in I_{il1}\implies (\tau_{i(l-1)}<t_1\leq \tau_{il}, t_2>t_1) $; $\bm{t}\in I_{il2}\implies (\tau_{i(l-1)}<t_2\leq \tau_{il}, t_1>t_2)$; $\bm{t}\in I_{i0}\implies (t_1>\tau_{il}, t_2>\tau_{il})$.  
\begin{res}\label{res4}
The influence function of $\hat{\bm{\Lambda}}_{\gamma}$  based on all I groups is given as follows.
\begin{align*}
    \bm{IF(t;S_{\gamma},F_{\Lambda})}=Q_{\gamma}(\bm{\Lambda})^{-1}\sum_{i=1}^{I}\frac{g_i}{G}&\left[\left\{[\delta_{I_{i0}}(t)-p_{i0}]p_{i0}^{\gamma-1}\,\frac{\partial(p_{i0})}{\partial\Lambda}\right\}\right.\\
   &\left.+\left\{\sum_{l=1}^{L}\sum_{r=1}^{2}[\delta_{I_{ilr}}(t)-p_{ilr}]p^{\gamma-1}_{ilr}\,\frac{\partial(p_{ilr})}{\partial\Lambda}\right\}\right].
\end{align*}
\end{res}
\begin{proof}
The proof of the theorem and description of notations are given in the appendix.
\end{proof}

\subsection{Influence Function of WRBE}

\noindent The robustness property corresponding to robust Bayes estimators $\hat{\bm{\Lambda}}^w_{b \gamma}$ using IF \cite{g2016} is studied through Bayes functional, which is given as follows concerning squared error loss function.
\begin{equation*}
T^{(\gamma)}_G(M)=\frac{\int \bm{\Lambda}\exp\left\{ B^w_{\gamma}(\bm{\Lambda};M,F_{\bm{\Lambda}})\right\}\pi(\bm{\Lambda})d\bm{\Lambda}}{\int\exp\left\{B^w_{\gamma}(\bm{\Lambda};M,F_{\bm{\Lambda}})\right\}\pi(\bm{\Lambda})d\bm{\Lambda}}.
\end{equation*}
\begin{res}\label{res5}
The influence function of $\hat{\bm{\Lambda}}^w_{ b\gamma}$,  based on all I groups, is given by the following expression.
\begin{equation*}
\bm{IF(\bm{t};T_G^{(\gamma)},F_{\bm{\Lambda}})}=Cov_{(p)}\left(\bm{\Lambda},X_{\gamma}(\bm{\Lambda};\bm{t},f_{\bm{\Lambda}})\right).
\end{equation*}
\end{res}


\begin{proof}
The proof of the theorem and description of notations are given in the appendix.
\end{proof}
\section{Missing Cause of Failure Analysis}\label{sec6}

\noindent There are plenty of incidents where a device fails, but the reason for that failure cannot be identified.  In such a situation, the unknown cause of failure is said to be masked or missing.  In this section, we develop the previous estimation methods when cause of some of the failures are missing or unidentified. 

\noindent Let us denote the number of failures of the $i^{th}$ group due to missing cause by $n_{im}$.  Then, total number of devices under observation in the $ith$ group is $g_i=k_i+n_{il1}+n_{il2}+n_{im}$ and $G=g_1+g_2+\dots+g_I$.  Therefore, the updated probabilities are described as follows,
\begin{flalign*}
   & P(\{\tau_{i(l-1)}<min(T_{i1},T_{i2})\leq\tau_{il}\}\cap \{missing\})=(1-p_{i0})p_{im}\\
   &P(\{\tau_{i(l-1)}<T_{i1}\leq\tau_{il},T_{i2}>T_{i1}\}\cap \{not\; missing\})=p_{il1}(1-p_{im})\\
   &P(\{\tau_{i(l-1)}<T_{i2}\leq\tau_{il},T_{i1}>T_{i2}\}\cap \{not\; missing\})=p_{il2}(1-p_{im})
    &&
\end{flalign*}
where $p_{im}$ denotes the probability of failure due to missing cause and $p_{i0}, p_{il1}, p_{il2}$ are described in Result \eqref{res1}.  Therefore, in presence of some failures due to unidentified cause, the likelihood function becomes,
\begin{equation*}
    L_m\propto \prod_{i=1}^{I}\left[p_{i0}^{k_i}\big((1-p_{i0})p_{im}\big)^{n_{im}}\prod_{l=1}^{L}\prod_{r=1}^{2}\big(p_{ilr}(1-p_{im})\big)^{n_{ilr}}\right].
\end{equation*}
The weighted density power divergence measure can be obtained as,
\begin{align*}
 D_{\gamma}^{w}(\bm{\Lambda})_m&=\sum_{i=1}^{I}\frac{g_i}{G}\left[\left\{p_{i0}^{\gamma+1}+\{(1-p_{i0})p_{im})\}^{\gamma+1}+(1-p_{im})^{\gamma+1}\sum_{l=1}^{L}\sum_{r=1}^{2}p_{ilr}^{\gamma+1}\right\}\right.\\
 &\qquad-\frac{\gamma+1}{\gamma}\left\{\hat{q}_{i0}p^{\gamma}_{i0}+\{(1-p_{i0})p_{im}\}^{\gamma}\hat{q}_{im}+(1-p_{im})^{\gamma}\sum_{l=1}^{L}\sum_{r=1}^{2}\hat{q}_{ilr}p_{ilr}^{\gamma}\right\}\\
 &\left.\qquad+\frac{1}{\gamma}\left\{\hat{q}_{i0}^{\gamma+1}+\hat{q}^{\gamma+1}_{mi}+\sum_{l=1}^{L}\sum_{r=1}^{2}\hat{q}^{\gamma+1}_{ilr}\right\}\right].
\end{align*}
where $\hat{q}_{im}=\frac{n_{im}}{g_i}$ is the empirical failure probability for missing cause.  

\noindent Therefore, the estimating equation for the WMDPDEs can be obtained as,
\begin{flalign*}
&\sum_{i=1}^{I}g_i\,s_i\bm{\Theta}_i\left[B_{i0}^{(\bm{\Lambda})}C_i^{(p_0)}Q_i^{(m)}+(1-p_{im})^{\gamma}\sum_{l=1}^{L}\sum_{r=1}^{2}B_{ilr}^{(\bm{\Lambda})}C_i^{(p_r)}Q_{ilr}\right]=\mathbf{0}_4,\\
&\sum_{i=1}^{I}g_i\left[p_{im}^{\gamma-1}\left\{(1-p_{i0})^{\gamma+1}p_{im}-p_{i0}^{\gamma}\hat{q}_{im}\right\}-(1-p_{im})^{\gamma-1}\right.\\
&\qquad\qquad\qquad\qquad\qquad\quad\left.\left\{\sum_{l=1}^{L}\sum_{r=1}^{2}p_{ilr}^{\gamma}\left((1-p_{im})p_{ilr}-\hat{q}_{ilr}\right)\right\}\right]=0 .
&&
\end{flalign*}
where,
\begin{flalign*}
Q_i^{(m)} &= p_{i0}^{\gamma-1}(p_{i0}-q_{i0})+p_{im}^{\gamma-1}(1-p_{i0})^{\gamma-1}\left\{p_{im}(1-p_{i0})-\hat{q}_{im}\right\},\\
Q_{ilr}&=p_{ilr}^{\gamma-1}(p_{ilr}-\hat{q}_{ilr}).
&&
\end{flalign*}
Also, the weighted robust Bayes estimator concerning the missing cause of failure can be modified to,
\begin{equation*}
 \hat{\bm{\Lambda}}^{w}_{(im)}=\int \bm{\Lambda}\pi^w_{(im)}(\bm{\Lambda}\vert \bm{\tau},\bm{s})\,d\bm{\Lambda}.
\end{equation*}
where,
\begin{equation*}
\pi^w_{(im)}(\bm{\Lambda}\vert \bm{\tau},\bm{s})=\frac{\exp\left(B^{w}_{\gamma}(\bm{\Lambda})_{(im)}\right)\pi(\bm{\Lambda})}{\int \exp\left(B^{w}_{\gamma}(\bm{\Lambda})_{(im)}\right)\pi(\bm{\Lambda})\,d\bm{\Lambda}},
\end{equation*}
\begin{flalign*}
B^{w}_{\gamma}(\bm{\Lambda})_{(im)}&=\sum_{i=1}^{I}\frac{g_i}{G}\left[\frac{1}{\gamma}\left\{\hat{q}_{i0}p^{\gamma}_{i0}+\{(1-p_{i0})p_{im}\}^{\gamma}\hat{q}_{im}+(1-p_{im})^{\gamma}\sum_{l=1}^{L}\sum_{r=1}^{2}\hat{q}_{ilr}p_{ilr}^{\gamma}\right\}\right.\\
 &\qquad \left.-\frac{1}{\gamma+1}\left\{p_{i0}^{\gamma+1}+\{(1-p_{i0})p_{im})\}^{\gamma+1}+(1-p_{im})^{\gamma+1}\sum_{l=1}^{L}\sum_{r=1}^{2}p_{ilr}^{\gamma+1}\right\}\right].
 &&
\end{flalign*}
The rest of the analysis of the missing causes of failure is  similar to the analysis based on the methods developed in the previous sections. Hence it is omitted here.


\section{Numerical Study}\label{sec7}

\noindent In this section, the performance of the theoretical results developed in previous sections has been assessed numerically through simulation experiments and real data analysis.

\subsection{Simulation Analysis}
\noindent For simulation purposes, 75 one-shot devices following Lindley Lifetime distribution are put to the accelerated life testing experiment under two independent competing causes of failures.
\begin{table}[htb!]
	\caption{Model layout for simulation study}
	\label{t2}
	\begin{center}		
		\begin{tabular}{cccccc} 
			\hline
			\textbf{Groups}&\textbf{Devices}&\textbf{Stress Levels} &\multicolumn{3}{c}{\textbf{Inspection Times}} \\\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-6}
			 1 & 20 & 0.1 & 0.5 & 1.0 & 2.0 \\
			 2 & 25 &0.2 & 1.0 & 2.0 & 3.0  \\
			 3 & 30 &0.3 & 0.5 & 1.0 & 2.0  \\
		\hline
		\end{tabular}
	\end{center}
\end{table}
These devices are divided into three independent groups and are subject to three stress levels.  The layout for the simulation experiment is given in Table \eqref{t2}. 
\begin{table}[htb!]
\caption{Model parameter values for simulation experiment}
\label{t3}
\begin{center}		
\begin{tabular}{ccccccccc} 
\hline
\multirow{2}{*}{$\bm{\bm{\Lambda}}$}&\multicolumn{4}{c}{\textbf{Pure Data}}&\multicolumn{4}{c}{\textbf{Contaminated Data}} \\ \cmidrule(lr){2-5}\cmidrule(lr){6-9}
&\multicolumn{2}{c}{\textbf{Cause 1}}&\multicolumn{2}{c}{\textbf{Cause 2}}&\multicolumn{2}{c}{\textbf{Cause 1}}&\multicolumn{2}{c}{\textbf{Cause 2}}\\\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
&$\bm{a}_1$&$\bm{b}_1$&$\bm{a}_2$&$\bm{b}_2$&$\bm{a}_1$&$\bm{b}_1$&$\bm{a}_2$&$\bm{b}_2$\\
\hline
$\bm{\bm{\Lambda}}_1$ & 1 & -3 & -2 & 2 & 1.9 & -3.9 & -2.9 & 2.9 \\
$\bm{\bm{\Lambda}}_2$ & 1.5 & -3.5 & -2  & 1.5  & 2.0  & -4.0 & -2.5  & 2.4  \\
\hline
		\end{tabular}
	\end{center}
\end{table}
\noindent Two sets of model parameters are taken for studying the performance of MLE, WMDPDE, conventional Bayes Estimate (BE) and WRBE.  Two different contamination schemes are considered to analyse the robustness of WMDPDE and WRBE over MLE and BE.  These are given in Table \eqref{t3}.
\begin{table}[htb!]
	\caption{Bias of MLE and WMDPDE}
	\label{t4}
	\begin{center}
   {\scalebox{0.9}{
		\begin{tabular}{lcccccccc} 
  \hline
\multirow{2}{*}{}&\multicolumn{4}{c}{\textbf{Pure Data}}&\multicolumn{4}{c}{\textbf{Contaminated Data}}\\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
& $\hat{a}_1$ & $\hat{b}_1$ & $\hat{a}_2$ & $\hat{b}_2$&  $\tilde{a}_1$ & $\tilde{b}_1$ & $\tilde{a}_2$ & $\tilde{b}_2$\\
\hline
$\bm{\Lambda}=\bm{\Lambda}_1$\\
\textbf{MLE} &0.1726  &0.2069 &-0.0996  &0.4728&0.3512  &1.2796&-0.1552 & 2.0770 \\
\multicolumn{2}{l}{\textbf{WMDPDE}}\\
$\mathbf{\gamma=0.2}$ &0.2579 & 0.1849 & -0.1363 & 0.6768 &0.2340 & 0.1993 & -0.1462 & 0.7139 \\
$\mathbf{\gamma=0.4}$ &0.1830 & 0.1010 &-0.1000&  0.4058&0.1646& 0.1138& -0.0679 & 0.5573\\
$\mathbf{\gamma=0.6}$&0.1568 & -0.0916 &-0.0886 &  0.1259 &0.3278& -0.2082& -0.1457&  0.5272  \\
$\mathbf{\gamma=0.8}$ & 0.2857 &-0.2785 & -0.1837 & 0.1244 & 0.2114& -0.1456& -0.1130&  0.2656 \\
$\mathbf{\gamma=1.0}$ &0.1465 & -0.1182 & -0.0851 & 0.1114 & 0.2392 & -0.2147 &-0.1398 & 0.1530 \\
\hline
$\bm{\Lambda}=\bm{\Lambda}_2$\\
\textbf{MLE} &0.1328  &0.1780 &-0.0086  &0.8102 & 0.5767 & 1.2133 & -0.0249 & 1.0538 \\
\multicolumn{2}{l}{\textbf{WMDPDE}}\\
$\mathbf{\gamma=0.2}$& 0.1114 &0.2941& 0.0131 &0.3607 &  0.1071 & 0.3833 & 0.0209 & 0.4418  \\
$\mathbf{\gamma=0.4}$ & 0.1444 & 0.4051 & 0.0271 & 0.3207 & 0.1641 & 0.4177 & 0.0215 & 0.4162\\
$\mathbf{\gamma=0.6}$ & 0.0999 &0.1478& 0.0570& 0.1654 &0.1110 &0.2828 &0.0166& 0.2692  \\
$\mathbf{\gamma=0.8}$& 0.1265 & 0.0180 &-0.0213&  0.2151& 0.1092& 0.1318& 0.0100 &0.3278  \\
$\mathbf{\gamma=1.0}$ &0.1322& -0.0411& -0.0279&  0.1378&  0.1195 & -0.0422 &-0.0068 & 0.2256  \\
\hline
\end{tabular}}}
\end{center}
\end{table}

\begin{table}[!htbp]
	\caption{Bias of BE and WRBE}
	\label{t5}
	\begin{center}	
 {\scalebox{0.9}{
		\begin{tabular}{lcccccccc} 
  \hline
\multirow{2}{*}{}&\multicolumn{4}{c}{\textbf{Pure Data}}&\multicolumn{4}{c}{\textbf{Contaminated Data}}\\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
& $\hat{a}_1$ & $\hat{b}_1$ & $\hat{a}_2$ & $\hat{b}_2$&  $\tilde{a}_1$ & $\tilde{b}_1$ & $\tilde{a}_2$ & $\tilde{b}_2$\\
\hline
		\multicolumn{9}{c}{\textbf{Normal Prior}}\\
\hline
$\bm{\Lambda}=\bm{\Lambda}_1$\\
\textbf{BE}  &-0.0102 &-0.0203 &-0.0082 & 0.0455 &  -0.2560 &-0.4366&  0.0586 &  0.5795 \\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$ & -0.0137 & -0.0983 & -0.0808  & 0.1417 & -0.0127 & -0.0977 & -0.0922 & 0.1514 \\
$\mathbf{\gamma=0.4}$  & -0.0158  & -0.0982 & -0.0894 & 0.1405 & -0.0155 & -0.1021 & -0.0876 & 0.1432\\
$\mathbf{\gamma=0.6}$ &-0.0140 & -0.0991 & -0.0870 & 0.1407 & -0.0133 & -0.1055 &-0.0796 &  0.1376\\
$\mathbf{\gamma=0.8}$ & -0.0161 & -0.0915 & -0.0913 & 0.1374 & -0.0138 & -0.1063 & -0.0870 & 0.1380\\
$\mathbf{\gamma=1.0}$ & -0.0037 &-0.1006 &-0.0865 & 0.1376 &-0.0148  &-0.1001 &-0.0801  &0.1489 \\
\hline
$\bm{\Lambda}=\bm{\Lambda}_2$\\
\textbf{BE} &-0.1024 & -0.0353 &  0.0155 & 0.0437 & -0.3555 & -0.2476 & 0.0534 & 0.5532\\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$& -0.1032 & -0.1049&-0.0886  & 0.1371 &-0.0981 & -0.0995 & -0.0849 & 0.1443 \\
$\mathbf{\gamma=0.4}$  & -0.1002 & -0.0972 & -0.0821 & 0.1379 & -0.0995 & -0.1002 & -0.0846 & 0.1407\\
$\mathbf{\gamma=0.6}$ & -0.0979 & -0.1055 & -0.0897 & 0.1467 & -0.1049 & -0.1004 & -0.0882 & 0.1508  \\
$\mathbf{\gamma=0.8}$ & -0.1024 & -0.1037 & -0.0896 & 0.1305 &-0.1021& -0.1003&-0.0839&0.1484 \\
$\mathbf{\gamma=1.0}$ &-0.1055 & -0.1022 & -0.0790 &  0.1450 & -0.0934 & -0.0944 & -0.0815 & 0.1455 \\
\hline
\multicolumn{9}{c}{\textbf{Dirichlet Prior}}\\
\hline
$\bm{\Lambda}=\bm{\Lambda}_1$\\
\textbf{BE}&0.0671 & -0.0341 & -0.0114 & 0.0151  & 0.4903 & -0.8309 &-0.0806 & 0.4389  \\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$ & 0.0848 & -0.1418& -0.0964&  0.1214 &   0.0873&-0.1311& -0.0711 & 0.2039    \\
$\mathbf{\gamma=0.4}$  & 0.0863 &-0.1400 &-0.0947  & 0.1351  & 0.0912 & -0.1305 & -0.0641 &  0.2092\\
$\mathbf{\gamma=0.6}$& 0.0854 &-0.1459  &-0.0867  & 0.1367  &0.0882 &-0.1339& -0.0693 & 0.2092 \\
$\mathbf{\gamma=0.8}$ & 0.0859 & -0.1416 & -0.0912 & 0.1333 & 0.0918 & -0.1347 & -0.0736 & 0.2087 \\
$\mathbf{\gamma=1.0}$  & 0.0794 & -0.1417 & -0.0963  &0.1351 &0.0934 & -0.1354 &-0.0671 & 0.2025\\
\hline
$\bm{\Lambda}=\bm{\Lambda}_2$\\
\textbf{BE} & 0.0519 & 0.0266 & -0.0080 &-0.0104 & 0.7799 & 0.9063 & 0.0412 &-0.8227  \\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$&0.1087& -0.0912 &-0.0466&  0.0470& 0.0908 &-0.1270 &-0.0441&  0.0516 \\
$\mathbf{\gamma=0.4}$ & 0.1096 & -0.0890 & -0.0473 & 0.0514 & 0.0946 &-0.1373 &-0.0432  &0.0527\\
$\mathbf{\gamma=0.6}$ & 0.1090 &-0.0920 &-0.0412  &0.0465 &0.0969 &-0.1330&-0.0493&0.0557 \\
$\mathbf{\gamma=0.8}$ & 0.1183 &-0.0905 &-0.0462  & 0.0478 &0.0986&-0.1351&-0.0451&0.0524 \\
$\mathbf{\gamma=1.0}$ &0.1064  &-0.0806 & -0.0614 &  0.0467 & 0.0962 &-0.1274 &-0.0408 & 0.0523\\
\hline
\end{tabular}}}
\end{center}
\end{table}
\noindent Robustness behaviour can be observed through the study of biases of the estimators.  Hence, the Bias of MLE and WMDPDE is obtained through Monte Carlo simulation based on 1000 generations where MLE and WMDPDE are obtained using the Co-ordinate Descent Algorithm described in Algorithm \eqref{alg1}. The threshold value c is taken as $c=0.0001$, and the learning rate $\alpha$ is taken in the range $\alpha=(0.001,0.009)$.  The outcomes are reported in Table \eqref{t4}  for different schemes both in pure and contaminated cases.  \\
\noindent For the Bayesian estimation, the MH algorithm described in Algorithm \eqref{alg2} is used to generate a sample from the posterior distribution.  In proposal distribution, $\delta$ is set as 0.8 where we take $a=0.002$ and $\sigma^2=0.002.$  A sequence of 20,000 values is generated applying the MH algorithm and the first 2000 data points are discarded for burn-in, after which every $150^{th}$ sample is taken to reduce the auto-correlation.  Thus 121 samples are finally obtained whose average is the posterior estimate.  This process is repeated for 700 generations to approximate the biases of BE and WRBE based on both Normal and Dirichlet priors.   For Dirichlet prior, we choose $\sigma^2_p=0.005$.  The outcomes are reported in Table \eqref{t5}. 

\noindent From Table \eqref{t4}, it is evident that the bias of MLE is less than that of WMDPDE for pure data.  When data is contaminated, WMDPDE performs better than MLE as the bias of WMDPDE is less than that of MLE.  Also, the change of bias for WMDPDE is lesser than that of MLE changing from pure to contamination scheme.  Table \eqref{t5} shows that the bias of BE is less than that of WRBE in the pure data scheme.  But after contamination, there is no significant increase in the bias of WRBE, but the bias of BE is increased.  Thus, from Tables \eqref{t4} and \eqref{t5}, it can be concluded that WMDPDE and WRBE are robust estimators.  It is also observed for the simulation scheme 1 ($\bm{\Lambda}=\bm{\Lambda}_1$), BE and WRBE based on normal prior have less bias than that of Dirichlet prior and vice versa for the simulation scheme 2 ($\bm{\Lambda}=\bm{\Lambda}_2$).  Hence the choice of prior depends on the situation at hand.  Observing the figures in Tables \eqref{t4} and \eqref{t5}, it can be concluded that among the four estimation methods; MLE, WMDPDE, BE, and WRBE; WRBE has the smallest bias compared to other estimates whereas WMDPDE and WRBE are robust estimators.  Hence if prior information is available, WRBE is the best choice, but if it is not possible to get the prior knowledge, one can rely on WMDPDE for robust estimation purposes.
\begin{table}[htb!]
	\caption{Optimal value of tuning parameter}
	\label{t6}
	\begin{center}		
		\begin{tabular}{cccccc} 
			\hline
			\textbf{Tuning}&\multicolumn{4}{c}{\textbf{Estimates}}&\multirow{2}*{$\Phi_{\gamma}(\hat{\bm{\Lambda}})$}  \\
  \textbf{parameter}&$\hat{a}_1$&$\hat{b}_1$&$\hat{a}_2$&$\hat{b}_2$& \\
  \hline
  0.1 & 0.01530 &-0.11309 & 0.01961 &0.30967&0.83786 \\
  0.2 &  0.01150 & -0.33993 & 0.01100 &0.06346 &0.74776\\
  0.3 &  0.01069& -0.38879& 0.01015& 0.03626&0.70562\\
  0.4 & 0.01444& -0.38966& 0.01011& 0.03579& 0.68433  \\
  0.5 &  0.01274& -0.39045& 0.01008& 0.03535& 0.65819 \\
  0.6 &0.01157& -0.39118& 0.01006& 0.03495& 0.62569 \\
  0.7 & 0.01079& -0.39184& 0.01004& 0.03458&0.58414\\
  0.8 &  0.01027& -0.39246& 0.01003& 0.03424& 0.52944\\
  0.9 &  0.00943& -0.39302& 0.01002& 0.03393&0.45566\\
  1.0 & 0.00973& -0.39354& 0.01001& 0.03364& 0.35466\\
		\hline
		\end{tabular}
	\end{center}
\end{table}


\subsection{Optimal choice of tuning parameter}
\noindent As seen in section \ref{sec3}, the DPD measure based estimation depends on the choice of tuning parameter $\gamma$.  Hence it becomes necessary to find the optimal value for tuning the parameter concerning the criteria of interest.  This work considers a data-driven approach in finding optimal tuning parameter aiming to increase the robustness along with the precision of the estimation.  Therefore, the objective function defined in \eqref{tune} is to be minimised.
\begin{equation}
\Phi_{\gamma}(\bm{\Lambda})=C_1\,D^w_{\gamma}({\bm{\Lambda}})+C_2\,\vert V\vert \label{tune}
\end{equation}
where, $D^w_{\gamma}$ is defined in \eqref{7}, $\vert V\vert =det\left[Q_{\gamma}^{-1}({\bm{\Lambda}})R_{\gamma}({\bm{\Lambda}})Q_{\gamma}^{-1}({\bm{\Lambda}})\right]$ and $C_1, C_2$ are predefined positive weight values with $C_1+C_2=1$.  The goal is to find the tuning parameter that minimises the WDPD measure between the empirical and assumed theoretical distributions and the determinant of the variance-covariance matrix of the estimate of the parameter for the given choices of tuning parameters.  

\noindent Here we perform some numerical experiments in search of optimal tuning parameter under the set-up given in Table \eqref{t2} with true parameter value $\bm{\Lambda}=(0.01,-0.4,0.01,0.03)^{'}$.  For different values of $\gamma$, WMDPDE are obtained and thereafter $\Phi_{\gamma}(\hat{\bm{\Lambda}})$ is calculated with $C_1=C_2=0.5.$  Those results are reported in Table \eqref{t6}.  From this Table, it is observed that $\gamma=1.0$ is the optimal tuning parameter in this investigated case.
\begin{table}[htb!]
	\caption{Layout of the Data}
	\label{t7}
	\begin{center}	
 {\scalebox{0.9}{
		\begin{tabular}{ccccccc} 
			\hline
		\textbf{Groups}&\textbf{Diagnosed}&\textbf{Median age} &\multicolumn{4}{c}{\textbf{Inspection Times}}\\
			&\textbf{Patients} &\textbf{at diagnosis} &\multicolumn{4}{c}{\textbf{(Months)}}\\ \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-7}
1 & 0342 & 45.76 & 1 & 6 & 12 & 20 \\
2 & 1259 & 55.43 & 1 & 6 & 12 & 20 \\
3 & 2179 & 64.58 & 1 & 6 & 12 & 20 \\
	\hline
	\textbf{Groups}&\multicolumn{2}{c}{\textbf{Survived}} &\multicolumn{4}{c}{\textbf{Deaths}}\\
	&\multicolumn{2}{c}{\textbf{Patients}} &\multicolumn{4}{c}{\textbf{(Cancer, Other)}}\\ \cmidrule(lr){1-1}\cmidrule(lr){2-3}\cmidrule(lr){4-7}
1 & \multicolumn{2}{c}{066} &(018,04) & (108,02) & (082,03) & (057,02) \\
2 & \multicolumn{2}{c}{247} & {(101,14)} & (421,22)  & (268,17)  & (163,06)\\
3 & \multicolumn{2}{c}{392} & {(219,19)} & {(748,31)}  & (426,16) & (312,16) \\
	\hline
		\end{tabular}}}
	\end{center}
\end{table}

\begin{table}[htb!]
	\caption{Estimates of parameters and Bootstrap Bias and RMSE of MLE and WMDPDE for real data}
	\label{t8}
	\begin{center}	
 {\scalebox{0.85}{
		\begin{tabular}{lcccccccc} 
  \hline
 &\multicolumn{4}{c}{\textbf{Estimates}}&\multicolumn{4}{c}{\textbf{Bootstrap Bias(RMSE)}}\\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
&$\hat{\bm{a}}_1$&$\hat{\bm{b}}_1$&$\hat{\bm{a}}_2$&$\hat{\bm{b}}_2$&$\hat{\bm{a}}_1$&$\hat{\bm{b}}_1$&$\hat{\bm{a}}_2$&$\hat{\bm{b}}_2$\\
\hline
\textbf{MLE} & 0.0613&  0.1604&  0.4116& -0.3044 & 0.0939 & 0.0102 &  0.0119 & -0.0863 \\
& & & &  & (0.1125) & (0.0368) &  (0.01719) & (0.1253)\\
\multicolumn{2}{l}{\textbf{WMDPDE}}\\
$\mathbf{\gamma=0.2}$&0.0772&  0.1680&  0.4147 &  -0.3196 & 0.0889  &  0.0134&  0.0148 & -0.0994\\
& & & &  & (0.0890) & (0.0270) &  (0.0187) & (0.0947)\\
$\mathbf{\gamma=0.4}$& 0.0613 &  0.1676 &  0.4136 & -0.3784&  0.0419&  0.0119 & 0.0137 &-0.0580 \\
& & & &  & (0.0890) & (0.0270) &  (0.0187) & (0.0954)\\
$\mathbf{\gamma=0.6}$& 0.0330&  0.1681&  0.4066& -0.3464& 0.0135 &  0.0114 & 0.0066 & -0.0260  \\
& & & & & (0.0301) & (0.0244) &  (0.0037) & (0.0152)\\
$\mathbf{\gamma=0.8}$ & 0.0205&  0.1629&  0.4124& -0.3221& 0.0085 & 0.0057 & 0.0049 &-0.0018 \\
& & & &  & (0.0016) & (0.0017) &  (0.0011) & (0.0015)\\
$\mathbf{\gamma=1.0}$& 0.0195& 0.1620&  0.4167 & -0.3210 &  0.0016 & 0.0041  &0.0017 &-0.0076 \\
& & & & & (0.0010) & (0.0012) &  (0.0035) & (0.0016)\\
\hline
\end{tabular}}}
\end{center}
\end{table}


\begin{table}[htb!]
	\caption{Bootstrap Bias and RMSE of BE and WRBE for real data}
	\label{t9}
	\begin{center}	
 {\scalebox{0.85}{
		\begin{tabular}{lcccccccc} 
  \hline
 &\multicolumn{4}{c}{\textbf{Estimates}}&\multicolumn{4}{c}{\textbf{Bootstrap Bias(RMSE)}}\\\cmidrule(lr){2-5}\cmidrule(lr){6-9}
&$\hat{\bm{a}}_1$&$\hat{\bm{b}}_1$&$\hat{\bm{a}}_2$&$\hat{\bm{b}}_2$&$\hat{\bm{a}}_1$&$\hat{\bm{b}}_1$&$\hat{\bm{a}}_2$&$\hat{\bm{b}}_2$\\
\hline   
\multicolumn{5}{c}{\textbf{Normal Prior}}\\
\hline
\textbf{BE}&  0.0120&  0.1559&  0.3988& -0.3221   & -0.0415& 0.0069& 0.0085& 0.0142 \\
& & & & & (0.0734) & (0.0094) &  (0.0102) & (0.0437)\\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$& 0.0237& 0.1631 &  0.3947  & -0.3151  & -0.0019&0.0076 &-0.0032 &-0.0138  \\
& & & &  & (0.0072) & (0.0083) &  (0.0077) & (0.0080)\\
$\mathbf{\gamma=0.4}$  & 0.0186&  0.1547 &  0.3938  & -0.3233 & -0.0013& -0.0054& -0.0061& -0.0032\\
& & & &  & (0.0072) & (0.0071) &  (0.0097) & (0.0101)\\
$\mathbf{\gamma=0.6}$& 0.0285& 0.1657 & 0.3995& -0.3297&0.0083 &  0.0058& -0.0004& -0.0098 \\
& & & &  & (0.0061) & (0.0104) &  (0.0083) & (0.0105)\\
$\mathbf{\gamma=0.8}$ & 0.0157 & 0.1622 & 0.4074 & -0.3235  & -0.0042&  0.0018&  0.0072& -0.0033 \\
& & & &  & (0.0059) & (0.0064) &  (0.0112) & (0.0073)\\
$\mathbf{\gamma=1.0}$ & 0.0258 & 0.1518 & 0.4007 & -0.3165&  -0.0017&  -0.0067&  0.0004&  -0.0005 \\
& & & &  & (0.0063) & (0.0091) &  (0.0060) & (0.0062)\\
\hline
\multicolumn{5}{c}{\textbf{Dirichlet Prior}}\\
\hline
\textbf{BE} & 0.0281&  0.1683&  0.4055& -0.3275 & 0.0075&  0.0074&  0.0088& 0.0417 \\
& & & &  & (0.0204) & (0.0095) &  (0.0624) & (0.0734)\\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$ & 0.0219& 0.1626&  0.4011& -0.3163 & 0.0013&  0.0027& -0.0003&  0.0017\\
& & & &  & (0.0062) & (0.0074) &  (0.0079) & (0.0065)\\
$\mathbf{\gamma=0.4}$& 0.0200&0.1483&  0.3951& -0.3175  & -0.0001& -0.0105& -0.0054&  0.0002 \\
& & & &  & (0.0079) & (0.0096) &  (0.0087) & (0.0086)\\
$\mathbf{\gamma=0.6}$& 0.0161& 0.1542&  0.4012  &-0.3049 & -0.0038& -0.0049&  0.0003&  0.0130 \\
& & & &  & (0.0209) & (0.0075) &  (0.0062) & (0.0070)\\
$\mathbf{\gamma=0.8}$& 0.0225&  0.1589 &  0.4061 &-0.3317 & 0.0021& -0.0008&  0.0053& -0.0133 \\
& & & &  & (0.0041) & (0.0062) &  (0.0092) & (0.0069)\\
$\mathbf{\gamma=1.0}$& 0.0174& 0.1724 & 0.3990  & -0.3268  &  -0.0019&  -0.0047& -0.0008&  -0.0049 \\
& & & &  & (0.0063) & (0.0077) &  (0.0061) & (0.0079)\\
\hline
\end{tabular}}}
\end{center}
\end{table}



\subsection{Data Analysis}
\noindent For the real data analysis purposes, data is adopted from the database named ``Incidence$\cdot$SEER Research Data, 8 Registries, Nov 2021 Sub (1975-2019)"\cite{seer} which is recorded by National Cancer Institute (Surveillance, Epidemiology, and End Results Program (SEER)). 
 The data is extracted from patients between the periods 2008-2010 who were diagnosed with Pancreas cancer.  Further, data on 418 patients is omitted because of the unknown status of their death, cause of death or survival months.  Death due to pancreas cancer is taken as competing cause 1, and death due to other causes is taken as competing cause 2.  For this analysis, patients are divided into three age-groups $(40-49)$, $(50-59)$ and $(60-69)$ years.  Their median age at diagnosis is taken as the stress levels.  Stress levels are multiplied by $0.1$ for computational ease.  The patients are observed at the interval of $(1, 6, 12, 20)$ months for each of the groups which are converted to years by dividing it by $12$.  Number of deaths are recorded for each time interval and the number of surviving patients is noted.  The data layout is described in Table \eqref{t7}.
\begin{table}[htb!]
	\caption{95\% HPD CRI of the parameter estimates for real data}
	\label{t10}
	\begin{center}	
  {\scalebox{0.93}{
		\begin{tabular}{lcccccccc} 
  \hline
&\multicolumn{2}{c}{$\hat{\bm{a}}_1$} & \multicolumn{2}{c}{$\hat{\bm{b}}_1$} &\multicolumn{2}{c}{ $\hat{\bm{a}}_2$} & \multicolumn{2}{c}{$\hat{\bm{b}}_2$}  \\\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
&\textbf{LL} &\textbf{UL}&\textbf{LL} &\textbf{UL}&\textbf{LL} &\textbf{UL}&\textbf{LL} &\textbf{UL}\\
\hline
\multicolumn{9}{c}{\textbf{Normal Prior}}\\
\hline
\textbf{BE} & 0.0072& 0.0179 &0.1466& 0.1664& 0.3953 & 0.4009 & -0.3271 &-0.3162\\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$ & 0.0174& 0.0292 & 0.1590& 0.1667& 0.3902& 0.3998 & -0.3241 & -0.3041\\
$\mathbf{\gamma=0.4}$& 0.0143& 0.0246 & 0.1520 & 0.1571 & 0.3877& 0.4019 & -0.3262& -0.3194\\
$\mathbf{\gamma=0.6}$& 0.0189& 0.0346&0.1602 & 0.1718& 0.3939& 0.4072 & -0.3344 & -0.3204\\
$\mathbf{\gamma=0.8}$& 0.0112 & 0.0215 & 0.1538 & 0.1711 &0.3999& 0.4130& -0.3269 & -0.3194\\
$\mathbf{\gamma=1.0}$& 0.0223& 0.0289 &  0.1476& 0.1561& 0.3964& 0.4074 & -0.3217 & -0.3124  \\
\hline
\multicolumn{9}{c}{\textbf{Dirichlet Prior}}\\
\hline
\textbf{BE} & 0.0228& 0.0334&  0.1595& 0.1762&0.3985& 0.4130 & -0.3323 & -0.3230\\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$ & 0.0185& 0.0254 & 0.1585& 0.1668 & 0.3974 & 0.4032 & -0.3205 & -0.3122\\
$\mathbf{\gamma=0.4}$& 0.0124& 0.0265 & 0.1437& 0.1521 & 0.3902& 0.4006 & -0.3216& -0.3108\\
$\mathbf{\gamma=0.6}$& 0.0117 & 0.0220 & 0.1511& 0.1588 & 0.3947& 0.4091 & -0.3126& -0.2982\\
$\mathbf{\gamma=0.8}$& 0.0190 &0.0258 & 0.1544 &0.1631 & 0.4015& 0.4105& -0.3460 & -0.3153 \\
$\mathbf{\gamma=1.0}$& 0.0122& 0.0235 & 0.1651 & 0.1796 & 0.3946 & 0.4033 & -0.3335 & -0.3176\\
\hline
\end{tabular}}}
\end{center}
\end{table}

\noindent To ensure that the two-parameter Lindley distribution can be fitted to the data, a bootstrap-based goodness of fit test is performed. The distance-based test statistic is given as, 
$$
T=\sum_{i=1}^{I}\left\{\left\mid\frac{k_i-\hat{k}_i}{\hat{k}_i}\right\mid+\sum_{l=1}^{L}\sum_{r=1}^{2}\left\mid \frac{n_{ilr}-\hat{n}_{ilr}}{\hat{n}_{ilr}}\right\mid\right\}.
$$
where $\hat{k_i}$ and $\hat{n}_{ilr}$ are the expected number of survival and failures, respectively.  MLE given in Table \eqref{t8} is used to obtain the expected number of deaths and survivals.  The value of the test statistic came out to be $6.6594$, and the corresponding approximate p-value is $0.792$, which strongly satisfies the assumption of Lindley distribution as the lifetime distribution.  

\noindent For estimation purposes, the initial values are chosen as $a_1=0.02, b_1=0.16, a_2=0.40, b_2=-0.32$, which are found through the grid-search procedure.  The estimates based on MLE and WMDPDE with their bootstrap bias and root mean square of errors (RMSE) are reported in Table \eqref{t8}.  From this Table, it can be observed that as the tuning parameter increases, the bias of estimates decreases.  Also, the estimates based on BE and WRBE with their bootstrap bias and RMSE are given in Table \eqref{t9}.  From this Table, it can be observed that bias and RMSE based on Dirichlet prior is less than the corresponding bias and RMSE based on Normal prior in general.  From Tables \eqref{t8} and \eqref{t9} it can be concluded that WRBE based on Dirichlet prior is desirable among the four methods of estimation for this data set-up.  Finally, The 95\% HPD CRI of the parameters are reported in Table \eqref{t10} where LL indicates the lower limit and UL indicates the upper limit of the HPD CRI.  


\subsection{Missing Cause of Failure Data Analysis}


\noindent When the death of patients is recorded, but the observer misses the cause of their death, missing cause of failure analysis is applied.  Here the missing cause of failure probability (MCFP) is considered constant for all the independent groups and time intervals.
\begin{table}[htb!]
	\caption{Count Data Details (missing cause of failure)}
	\label{t11}
	\begin{center}	
 {\scalebox{0.85}{
		\begin{tabular}{ccccccc} 
			\hline
			\textbf{Groups}&\textbf{Diagnosed}&\textbf{Median age} &\multicolumn{4}{c}{\textbf{Inspection Times}}\\
			&\textbf{Patients} &\textbf{at diagnosis} &\multicolumn{4}{c}{\textbf{(Months)}}\\ \cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-7}
1 & 0348 & 45.76 & 1 & 6 & 12 & 20 \\
2 & 1276 & 55.41 & 1 & 6 & 12 & 20\\
3 & 2198 & 64.59 & 1 & 6 & 12 & 20\\
	\hline
	\textbf{Groups}&\textbf{Survived} & \textbf{Deaths} &\multicolumn{4}{c}{\textbf{Deaths}}\\
	&\textbf{Patients} &\textbf{(Missing Cause)}&\multicolumn{4}{c}{\textbf{(Cancer, Other)}}\\\cmidrule(lr){1-1}\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-7} 
1 & 066 & 06  &(018,04) & (108,02) & (082,03) & (057,02)  \\
2 & 247 & 17 & {(101,14)} & (421,22)  & (268,17)  & (163,06) \\
3 & 392 & 19 & {(219,19)} & {(748,31)}  & (426,16) & (312,16)  \\
	\hline
		\end{tabular}}}
	\end{center}
\end{table}
For the study, 42 patients are considered from the previously omitted 418 patients in the SEER dataset whose cause of death could not be recorded.  The updated layout of the data is provided in Table \eqref{t11}.  For estimation purposes, the initial values taken here are $a_1= 0.001, b_1=0.400, a_2=0.050, b_2=-0.320, p_m= 0.10$, found through extensive grid search.  The test statistic's value for verifying the fitness of Lindley lifetime distribution is obtained as $16.6354$, and the corresponding approximate p-value came out to be $0.267$, which suggests that the distribution can be fitted to the given data.  The estimates of the parameters are shown in Table \eqref{t12}.  The corresponding bootstrap bias with RMSE is presented in Tables \eqref{t13} and \eqref{t14}.  By closely observing the Tables \eqref{t13} and \eqref{t14}, it can be concluded that estimates based on the Normal prior may be chosen over the other estimates in terms of bias and RMSE.

\begin{table}[htb!]
\caption{Estimate of parameters for real data (with Missing Cause of failure)}
	\label{t12}
	\begin{center}	
	 \begin{minipage}{\textwidth}
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccccc@{\extracolsep{\fill}}} 
  \hline
  &\multicolumn{5}{c}{\textbf{Estimates}}\\\cmidrule(lr){2-6}
&$\hat{\bm{a}}_1$&$\hat{\bm{b}}_1$&$\hat{\bm{a}}_2$&$\hat{\bm{b}}_2$&$\hat{\bm{p}}_m$\\
\hline
\textbf{MLE} &0.0033 &0.0703 &0.3886 &-0.3173 &0.0956 \\
\multicolumn{2}{l}{\textbf{WMDPDE}}\\
$\mathbf{\gamma=0.2}$  &0.0069 &0.0574 &0.3967 &-0.3191 &0.0987  \\
$\mathbf{\gamma=0.4}$&0.0033 &0.0554 &0.3979 &-0.3195 &0.0992\\   
$\mathbf{\gamma=0.6}$ &0.0091 &0.0541 &0.3985 &-0.3197 &0.0996    \\   
$\mathbf{\gamma=0.8}$&0.0046 &0.0830 &0.3989 &-0.3194 & 0.0997 \\  
$\mathbf{\gamma=1.0}$ &0.0023 &0.0775 &0.3991 &-0.3197 &0.0998  \\   
\hline   
\multicolumn{6}{c}{\textbf{Normal Prior}}\\
\hline
\textbf{BE}&0.0030 &0.0550 & 0.4269 &-0.3181 &0.0950   \\    
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$& 0.0041&0.0482 &0.4262 &-0.3176 & 0.1017 \\   
$\mathbf{\gamma=0.4}$& 0.0012&0.0526&0.3990 &-0.3212 &0.0973\\     
$\mathbf{\gamma=0.6}$ &0.0071&0.0585 &0.3846 &-0.3173&0.1003 \\ 
$\mathbf{\gamma=0.8}$& 0.0065&0.0497&0.3907 &-0.3128 & 0.0993  \\  
$\mathbf{\gamma=1.0}$& 0.0041&0.0482&0.4262  &-0.3176&0.1017 \\    
\hline
\multicolumn{6}{c}{\textbf{Dirichlet Prior}}\\
\hline
\textbf{BE}& 0.0030&0.0480&0.3592&-0.3171 &0.1006  \\   
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$ & 0.0070&0.0501&0.4022&-0.3211&0.1024 \\ 
$\mathbf{\gamma=0.4}$&0.0034&0.0438& 0.3977 &-0.3197&0.1094\\ 
$\mathbf{\gamma=0.6}$&0.0052 & 0.0526&0.3818 &-0.3260 & 0.0989   \\  
$\mathbf{\gamma=0.8}$ & 0.0023&0.0467 &0.3872&-0.3211 &0.1006 \\ 
$\mathbf{\gamma=1.0}$&  0.0037&0.0481& 0.3722&-0.3166 &0.0957\\  
\hline
\end{tabular*}
\end{minipage}
\end{center}
\end{table}

\begin{table}[htb!]
\caption{Bootstrap Bias and RMSE of MLE and WMDPDE for real data (with Missing Cause of failure)}
	\label{t13}
	\begin{center}	
		 \begin{minipage}{\textwidth}
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccccc@{\extracolsep{\fill}}} 
  \hline
  &\multicolumn{5}{c}{\textbf{Bootstrap Bias (RMSE)}}\\\cmidrule(lr){2-6}
&$\hat{\bm{a}}_1$&$\hat{\bm{b}}_1$&$\hat{\bm{a}}_2$&$\hat{\bm{b}}_2$&$\hat{\bm{p}}_m$\\
\hline
\textbf{MLE} &0.0276  &0.1398  &-0.0473   &0.0030  &-0.0037 \\
& (0.0282) & (0.3296) &  (0.2101) & (0.0046)  & (0.0064)\\
\multicolumn{2}{l}{\textbf{WMDPDE}}\\
$\mathbf{\gamma=0.2}$ &0.0010 & 0.0072 & 0.0002 &0.0012 &-0.0009 \\
& (0.0445) & (0.3387) &  (0.2266) & (0.0039)  & (0.0045)\\
$\mathbf{\gamma=0.4}$& 0.0050    & 0.0065  &-0.0091   &0.0066  &-0.0005\\  
& (0.0220) & (0.3478) &  (0.2844) & (0.0015)  & (0.0010)\\
$\mathbf{\gamma=0.6}$  &  0.0011 &0.0059 &-0.0015 &0.0011 &-0.0006     \\ 
& (0.0113) & (0.3484) &  (0.2945) & (0.0054)  & (0.0026)\\
$\mathbf{\gamma=0.8}$& 0.0054 &0.0484  & -0.0013  & 0.0007 &-0.0003 \\  
& (0.0040) & (0.3486) & (0.3015)  & (0.0005)  & (0.0028)\\
$\mathbf{\gamma=1.0}$  &0.0022 & 0.0428 & -0.0011&0.0004& -0.0002  \\
& (0.0016) & (0.3488) & (0.3071)  & (0.0004)  &(0.0023) \\
\hline
\end{tabular*}
\end{minipage}
\end{center}
\end{table}

\begin{table}[htb!]
\caption{Bootstrap Bias and RMSE of BE and WRBE for real data (with Missing Cause of failure)}
	\label{t14}
	\begin{center}	
		 \begin{minipage}{\textwidth}
	\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}ccccccc@{\extracolsep{\fill}}}
  \hline
  &\multicolumn{5}{c}{\textbf{Bootstrap Bias (RMSE)}}\\\cmidrule(lr){2-6}
&$\hat{\bm{a}}_1$&$\hat{\bm{b}}_1$&$\hat{\bm{a}}_2$&$\hat{\bm{b}}_2$&$\hat{\bm{p}}_m$\\
\hline   
\multicolumn{6}{c}{\textbf{Normal Prior}}\\
\hline
\textbf{BE} & 0.0023 &0.0049&  0.0259 & 0.0042&-0.0050  \\ 
& (0.0043) & (0.0038) & (0.0023) & (0.0056)  & (0.0041)\\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$& 0.0038 &-0.0016 &0.0273&0.0024&0.0017 \\ 
& (0.0045) & (0.0037) & (0.0362)  &(0.0050)  & (0.0063)\\
$\mathbf{\gamma=0.4}$&0.0002 &0.0024& -0.0017&-0.0011&-0.0026\\
& (0.0010) & (0.0057) & (0.0121)  &(0.0108)  &(0.0039) \\
$\mathbf{\gamma=0.6}$  &0.0062&0.0086&-0.0160 & 0.0027&0.0005 \\ 
& (0.0046) &(0.0041)  & (0.0359)  & (0.0046) & (0.0118) \\
$\mathbf{\gamma=0.8}$&-0.0033 &-0.0003&-0.0105&0.0072 & -0.0007  \\
& (0.0041) & (0.0055) &  (0.0358)  &(0.0057)  &(0.0019) \\
$\mathbf{\gamma=1.0}$ &0.0032 &-0.0016 &0.0274&0.0024&0.0017\\
& (0.0074) & (0.0036) &(0.0372)   & (0.0055)  &(0.0037) \\
\hline
\multicolumn{6}{c}{\textbf{Dirichlet Prior}}\\
\hline
\textbf{BE}&0.0018&-0.0017&-0.0411 &0.0028&0.0006  \\
&(0.0041)  &(0.0043)  & (0.0548)  & (0.0038) & (0.0039)\\
\multicolumn{2}{l}{\textbf{WRBE}}\\
$\mathbf{\gamma=0.2}$  &0.0061 &0.0017& 0.0012&-0.0011&0.0025\\ 
& (0.0043) & (0.0038) & (0.0228)  &(0.0056)  & (0.0041)\\
$\mathbf{\gamma=0.4}$ &0.0024   &-0.0061 & -0.0032 & 0.0004  & 0.0091\\ 
& (0.0042) & (0.0091) & (0.0036)  &(0.0038)  & (0.0058)\\
$\mathbf{\gamma=0.6}$ &0.0043  & 0.0026& -0.0185  & -0.0061  &-0.0096 \\ 
& (0.0056)  & (0.0054) & (0.0037)  & (0.0054) &(0.0061) \\
$\mathbf{\gamma=0.8}$ &0.0010  &  -0.0028& -0.0126 &-0.0012 & 0.0008 \\
&(0.0095)  & (0.0050) & (0.0509)   &(0.0043)  &(0.0062) \\
$\mathbf{\gamma=1.0}$&-0.0044  &-0.0018 &-0.0259   & 0.0034&-0.0046\\ 
&  (0.0038)  & (0.0052) & (0.0371)  & (0.0049)  & (0.0044)\\
\hline
\end{tabular*}
\end{minipage}
\end{center}
\end{table}

\section{Conclusion}\label{sec8}
\noindent This work is focused on the development of density power divergence based robust method of estimation both in classical and Bayesian framework under two independent competing causes of failures formulated by Lindley lifetime distribution in the context of one-shot device data analysis.  Through extensive simulation experiments,  the robustness of the weighted minimum density power divergence estimator and weighted robust Bayes estimator has been proved over the conventional maximum likelihood estimator and Bayesian estimator, respectively.  It has also been found that when data is contaminated, the bias of the weighted robust Bayes estimator is the least among the four methods of estimation.  But when prior information is not possible to obtain, one can rely on a weighted minimum density power divergence estimator.  For both the robust estimators, we have derived the influence function which measures the robustness analytically.  As the tuning parameter plays a crucial role in estimation purposes, we have proposed an approach for finding out the optimal value of it aiming to increase robustness and precision in estimation.  Some numerical experiments have been done in this regard.  Further, the developments of the theoretical results have been extended to the situation when the cause of some of the failures remains unidentified or missing.  Finally, the SEER-Pancreas cancer data set has been taken for real-life lifetime data analysis to establish the utility of the theoretical results explained in this work.

\noindent The model analyzed here can be implemented assuming other lifetime distributions.  This study can be extended to the situation of dependent competing risks.  Robust testing of hypotheses in the Bayesian framework can also be developed.  Efforts in this direction are in the pipeline and we are optimistic about reporting these findings soon.

\section*{Declarations of interest}
\noindent none.

\section*{Funding}
\noindent This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.

\textcolor{black}{
\section*{Data availability}
\noindent {\footnotesize Surveillance, Epidemiology, and End Results (SEER) Program (www.seer.cancer.gov) SEER*Stat Database: Incidence - SEER Research Data, 8 Registries, Nov 2021 Sub (1975-2019) - Linked To County Attributes - Time Dependent (1990-2019) Income/Rurality, 1969-2020 Counties, National Cancer Institute, DCCPS, Surveillance Research Program, released April 2022, based on the November 2021 submission.}}

\appendix
\section{Proof of Result \eqref{res1}}
\begin{flalign*}
p_{il1}&=\int\limits_{\tau_{i(l-1)}}^{\tau_{il}}\int\limits_{t_1}^{\infty}f_{i1}(t_1;\alpha_{i1},\theta_{i1})f_{i2}(t_2;\alpha_{i2},\theta_{i2})\,dt_1 dt_2=\frac{\theta_{i1}^2\theta_i^{-3}A^{(p_1)}_i}{\prod_{r=1}^{2}(\alpha_{ir}\theta_{ir}+1)}\\
p_{il2}&=\int\limits_{\tau_{i(l-1)}}^{\tau_{il}}\int\limits_{t_2}^{\infty}f_{i2}(t_2;\alpha_{i2},\theta_{i2})f_{i1}(t_1;\alpha_{i1},\theta_{i1})\,dt_2 dt_1=\frac{\theta_{i2}^2\theta_i^{-3}A^{(p_2)}_i}{\prod_{r=1}^{2}(\alpha_{ir}\theta_{ir}+1)}\\
p_{i0}&=P(T_{il1g}>\tau_{iL},T_{il2g}>\tau_{iL})=\prod_{r=1}^{2}\left(\frac{1+\alpha_{ir}\theta_{ir}+\tau_{iL}\theta_{ir}}{\alpha_{ir}\theta_{ir}+1}\right)e^{-\tau_{iL}\theta_{ir}}
&&
\end{flalign*}
where,
\begin{flalign*}
A^{(p_1)}_i&=\bigg\{A_i(\alpha_{i1},\theta_{i2}) E_i^{(0)}+A_{i(l-1)}(\theta_{i2})-A_{il}(\theta_{i2})\bigg\}\\
 E_i^{(\eta)}&=\tau_{i(l-1)}^{\eta}e^{-\tau_{i(l-1)}\theta_i}-\tau_{il}^{\eta}e^{-\tau_{il}\theta_i}\\
A^{(p_2)}_i&=\bigg\{A_i(\alpha_{i2},\theta_{i1}) E_i^{(0)}+A_{i(l-1)}(\theta_{i1})-A_{il}(\theta_{i1})\bigg\}\quad;\quad\theta_i=\theta_{i1}+\theta_{i2}\\
A_i(\alpha,\theta)&=\theta_i\{1+\theta_i(\alpha+\theta\,\alpha_{i1}\alpha_{i2})\}\\
    A_{i(\mu)}(\theta)&=e^{-\tau_{i(\mu)}\theta_i}[\tau_{i(\mu)}\{\theta_i^2+\theta(1+\theta_i)(2+\alpha_i\theta_i)\}+\theta_i^2\theta\tau_{i(\mu)}^2]
    &&
\end{flalign*}

\section{Proof of Result \eqref{res2}}
\noindent The set of estimating equations for MLEs is obtained by,
\begin{flalign*}
 \sum_{i=1}^{I}\left[k_i\frac{\partial(ln(p_{i0}))}{\partial\bm{\bm{\Lambda}}}+\sum_{l=1}^{L}\sum_{r=1}^{2}n_{ilr}\frac{\partial(ln(p_{ilr}))}{\partial\bm{\bm{\Lambda}}}\right]=\mathbf{0_4}
&&
\end{flalign*}
Here,
\begin{flalign*}
\frac{\partial(ln(p_{i0}))}{\partial\bm{\bm{\Lambda}}}&=\frac{1}{p_{i0}}\,s_i \bm{\Theta}_i B_{i0}^{(\bm{\bm{\Lambda}})} C_i^{(p_0)}\quad \text{and} \quad \frac{\partial(ln(p_{ilr}))}{\partial\bm{\bm{\Lambda}}}=\frac{1}{p_{ilr}}s_i \bm{\Theta}_i B_{ilr}^{(\bm{\bm{\Lambda}})}C_i^{(p_r)}\\
\text{where}\quad  \bm{\Theta}_i&=(\alpha_{i1},\theta_{i1},\alpha_{i2},\theta_{i2})^{'}\;,\; \bm{\bm{\Lambda}}=(a_1,b_1,a_2,b_2)^{'}\;,\;\alpha_i=\alpha_{i1}+\alpha_{i2}
&&
\end{flalign*}
By utilizing the notations from proof of Result \eqref{res1},
\begin{flalign*}
 C_i^{(p_0)}&=\frac{e^{-\tau_{iL}\theta_i}}{\prod_{r=1}^{2}(\alpha_{ir}\theta_{ir}+1)}\;,\quad C_i^{(p_r)}=\frac{\theta_{ir}^2\theta_i^{-3}}{\prod_{r=1}^{2}(\alpha_{ir}\theta_{ir}+1)}\;;\quad  B_{i0}^{(a_1)}= B_i^{(1)} \tau_{iL}\theta_{i1}^2 \\
 B_{i0}^{(a_2)}&= B_i^{(2)} \tau_{iL}\theta_{i2}^2\;;\; B_i^{(1)}=\frac{1+\alpha_{i2}\theta_{i2}+\tau_{iL}\theta_{i2}}{\alpha_{i1}\theta_{i1}+1}\;,\;B_i^{(2)}=\frac{1+\alpha_{i1}\theta_{i1}+\tau_{iL}\theta_{i1}}{\alpha_{i2}\theta_{i2}+1}\\
B_{i0}^{(b_1)}&=B_i^{(1)}[B(\alpha_{i1},\theta_{i1})]\quad;\quad B_{i0}^{(b_2)}=B_i^{(2)}[B(\alpha_{i2},\theta_{i2})]\\  
B(\alpha,\theta)&=\left[\{(\alpha\theta+1)(\alpha+\tau_{iL})\}-\left\{(1+\alpha\theta+\theta\tau_{iL})(\alpha\theta+\alpha+1)\right\}\right]
 &&
\end{flalign*}
\begin{flalign*}
 B_{il1}^{(a_1)}&=\theta_i[G_i^{(1)}(\theta_{i2},0)+G_i^{(2)}(\theta_{i2},1)]-A_i^{(p_1)}(\alpha_{i1}\theta_{i1}+1)^{-1} \\
B_{il2}^{(a_2)}&=\theta_i[G_i^{(1)}(\theta_{i1},0)+G_i^{(2)}(\theta_{i1},1)]-A_i^{(p_2)}(\alpha_{i2}\theta_{i2}+1)^{-1} \\
B_{il1}^{(a_2)}&=\theta_i[G_i^{(3)}(\alpha_{i1},\theta_{i2},0)+G_i^{(4)}(\theta_{i2},1)]-A_i^{(p_1)}(\alpha_{i2}\theta_{i2}+1)^{-1}\\
B_{il2}^{(a_1)}&=\theta_i[G_i^{(3)}(\alpha_{i2},\theta_{i1},0)+G_i^{(4)}(\theta_{i1},1)]-A_i^{(p_2)}(\alpha_{i1}\theta_{i1}+1)^{-1}\\
B_{il1}^{(b_1)}&= G_i^{(5)}(\alpha_{i1},\theta_{i1},0)+G_i^{(6)}(\alpha_{i1},\theta_{i2},1)-G_i^{(7)}(\theta_{i2},2)\\
&\qquad-G_i^{(8)}(\theta_{i2},3)+A^{(p_1)}_iG_i^{(9)}(\alpha_{i1},\theta_{i1})\\
B_{il2}^{(b_2)}&= G_i^{(5)}(\alpha_{i2},\theta_{i2},0)+G_i^{(6)}(\alpha_{i2},\theta_{i1},1)-G_i^{(7)}(\theta_{i1},2)\\&\qquad
-G_i^{(8)}(\theta_{i1},3)+A^{(p_2)}_iG_i^{(9)}(\alpha_{i2},\theta_{i2})\\
B_{il1}^{(b_2)}&= G_i^{(10)}(\alpha_{i2},\theta_{i2},0)+G_i^{(11)}(\alpha_{i1},\theta_{i2},1)-G_i^{(12)}(\theta_{i2},2)\\&\qquad
-G_i^{(6)}(\alpha_{i2},\theta_{i2},3)+A^{(p_1)}_iG_i^{(13)}(\alpha_{i2},\theta_{i2})\\
B_{il2}^{(b_1)}&= G_i^{(10)}(\alpha_{i1},\theta_{i1},0)+G_i^{(11)}(\alpha_{i2},\theta_{i1},1)-G_i^{(12)}(\theta_{i1},2)\\&\qquad
-G_i^{(6)}(\alpha_{i1},\theta_{i1},3)+A^{(p_2)}_iG_i^{(13)}(\alpha_{i1},\theta_{i1})
 &&
\end{flalign*}
\begin{flalign*}
G^{(1)}_i(\theta,\eta)&=\theta_i(1+\alpha_i\theta)E^{(\eta)}_i\;;\;G^{(2)}_i(\theta,\eta)=\theta(1+\theta_i)E^{(\eta)}_i\\
G_i^{(3)}(\alpha,\theta,\eta)&=\alpha\theta\theta_i E^{(\eta)}_i\;;\;
G^{(4)}_i(\theta,\eta)=\theta(1+\theta_i)E^{(\eta)}_i\\
G_i^{(5)}(\alpha,\theta,\eta)&=\{1+2\theta_i(\alpha+\theta\alpha_{i1}\alpha_{i2})\}E^{(\eta)}_i\\
G_i^{(6)}(\alpha,\theta,\eta)&=\{2\theta_i+\theta(2+\alpha_i+2\alpha_i\theta_i)-A_i(\alpha,\theta)\}E^{(\eta)}_i\\
G_i^{(7)}(\theta,\eta)&=\{\theta^2_i+\theta(2+\alpha_i\theta_i(1+\theta_i))\}E^{(\eta)}_i\;;\;G_i^{(8)}(\theta,\eta)=\theta\theta^2_iE^{(\eta)}_i\\
G_i^{(9)}(\alpha,\theta)&=\left\{\frac{2\theta^{-1}+\alpha-3\theta^{-1}(\alpha\theta+1)}{\alpha\theta+1}\right\}\\
G_i^{(10)}(\alpha,\theta,\eta)&=\{1+\theta_i\alpha_i(2+\alpha(\theta_i+2\theta))\}E^{(\eta)}_i\\
G_i^{(11)}(\alpha,\theta,\eta)&=\{2\theta_i+\theta(2+\alpha_i+2\alpha_i\theta_i)+(1+\theta_i)(2+\alpha_i\theta_i)-A_i(\alpha,\theta)\}E^{(\eta)}_i\\
G_i^{(12)}(\theta,\eta)&=\theta\{2+\alpha_i\theta_i(1+\theta_i)\}E^{(\eta)}_i\;;\;G_i^{(13)}(\alpha,\theta)=\frac{3\theta^{-1}_i(\alpha\theta+1)+\alpha}{\alpha\theta+1}
&&
\end{flalign*}
Therefore, the following equations are obtained whose solution is the MLEs.
\begin{equation*}
    \sum_{i=1}^{I}s_i\bm{\Theta}_i\left[\frac{k_i}{p_{i0}}B_{i0}^{(\bm{\bm{\Lambda}})}C_i^{(p_0)}+\sum_{l=1}^{L}\sum_{r=1}^{2}\frac{n_{ilr}}{p_{ilr}}B_{ilr}^{(\bm{\bm{\Lambda}})}C_i^{(p_r)}\right]=\mathbf{0_4}
\end{equation*}

\section{Proof of Theorem \eqref{thm1}}
\noindent To obtain the alternative expression for the WDPD measure, we denote failure probabilities and survival probability for $i^{th}$ group as,
\begin{multicols}{4}
\noindent $p_{1i1}=P_{i1}$\\
$p_{2i1}=P_{i2}$\\
$p_{1i2}=P_{i3}$\\
$p_{2i2}=P_{i4}$\\
$\dots\;\dots$\\
$p_{il1}=P_{i(M-2)}$\\
$p_{il2}=P_{i(M-1)}$\\
$p_{i0}=P_{iM}$
\end{multicols}
\noindent We have, $h=2(l-1)+r$ and $M=2L+1$ i.e. $p_{ilr}=P_{i[2(l-1)+r]}$.  For the $i^{th}$ group, the number of failures between the interval $(\tau_{i(l-1)}-\tau_{il})$ is denoted as, $N_{il}=n_{il1}+n_{il2}$.  Again, we define, $X_{ui}=(X_{ui1},X_{ui2},\dots,X_{uiM})\sim MN(1,\undertilde{P_i})$; where, $\undertilde{P_i}=(P_{i1},P_{i2},\dots,P_{iM})$.  Therefore, $N_{ih}=\sum_{u_i=1}^{g_i}X_{u_{i}h}$.  Hence, WDPD measure in equation \eqref{7} can be rewritten as, 
\begin{flalign}
H^w_{\gamma}(\bm{\Lambda})&=\sum_{i=1}^{I}\frac{g_i}{G}\left[\sum_{h=1}^{M}(P_{ih})^{\gamma+1}-\frac{\gamma+1}{\gamma}\sum_{h=1}^{M}\frac{N_{ih}}{g_i}(P_{ih})^{\gamma}+\frac{1}{\gamma}\sum_{h=1}^{M}\left(\frac{N_{ih}}{g_i}\right)^{\gamma+1}\right] .\label{28}
\end{flalign}
Based on Calvino et al.\cite{cal2021}, the proof of the theorem proceeds as follows.\\
From equation \eqref{28} WDPD measure ignoring the terms independent of parameters is given as,
\begin{flalign*}
H_g(\gamma)&=\sum_{i=1}^{I}\frac{g_i}{G}\left[\sum_{h=1}^{M}(P_{ih})^{\gamma+1}-\frac{\gamma+1}{\gamma}\sum_{h=1}^{M}\frac{N_{ih}}{g_i}(P_{ih})^{\gamma}\right]=\sum_{i=1}^{I}\frac{g_i}{G}\left[\frac{1}{g_i}\sum_{u_i=1}^{g_i}V_{u_i\gamma}(\bm{\Lambda})\right] .
&&
\end{flalign*}
where, $V_{u_i\gamma}(\bm{\Lambda})=\left\{\sum_{h=1}^{M}P_{ih}^{\gamma+1}-\frac{\gamma+1}{\gamma}\sum_{h=1}^{M}X_{u_ih}P_{ih}^{\gamma}\right\}.$
\begin{flalign*}
\text{Let,}\;H_{g\bm{\Lambda}}&=\frac{\partial (H_g(\gamma))}{\partial\bm{\Lambda}}=\sum_{i=1}^{I}\frac{g_i}{G}\left[\frac{1}{g_i}\sum_{u_i=1}^{g_i}\frac{\partial(V_{u_i\gamma}(\bm{\Lambda}))}{\partial\bm{\Lambda}}\right]
&&
\end{flalign*}
For the $i^{th}$ group,
\begin{flalign*}
\frac{\partial(V_{u_i\gamma}(\bm{\Lambda}))}{\partial\bm{\Lambda}}
&=\sum_{h=1}^{M}(\gamma+1)P_{ih}^{\gamma}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}}-(\gamma+1)\sum_{h=1}^{M}X_{u_ih}P_{ih}^{\gamma-1}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}}
&&
\end{flalign*}
Here, we get,
\begin{flalign*}
E\left(\frac{\partial(V_{u_i\gamma}(\bm{\Lambda}))}{\partial\bm{\Lambda}}\right)&=0\;,\;
Var\left(\frac{\partial(V_{u_i\gamma}(\bm{\Lambda}))}{\partial\bm{\Lambda}}\right)=Var(Y)\;;\; Y=\frac{\partial(V_{u_i\gamma}(\bm{\Lambda}))}{\partial\bm{\Lambda}}
&&
\end{flalign*}
\begin{flalign*}
Var(Y)&=(\gamma+1)^2Var\left(\sum_{h=1}^{M}X_{u_ih}P_{ih}^{\gamma-1}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}}\right)\\
&=(\gamma+1)^2\left[\sum_{h=1}^{M}P_{ih}^{2(\gamma-1)}\left(\frac{\partial(P_{ih})}{\partial\bm{\Lambda}}\right)^2P_{ih}(1-P_{ih})\right.\\
&\qquad\qquad\quad\left.-2\sum_{(h_1,h_2)}P_{ih_1}^{\gamma}P_{ih_2}^{\gamma}\right.\left.\frac{\partial(P_{ih_1})}{\partial\bm{\Lambda}}\frac{\partial(P_{ih_2})}{\partial\bm{\Lambda}}\right]
&&
\end{flalign*}
(Since for multinomial distribution,\\ $Var(X_{u_ih})=P_{ih}(1-P_{ih})$ and $Cov(X_{u_ih_1},X_{u_ih_2})=-P_{ih_1}P_{ih_2}$)
\begin{flalign*}
Cov(Y_j,Y_f)&=(\gamma+1)^2\left[\sum_{h=1}^{M}P_{ih}^{2(\gamma-1)}\left(\frac{\partial(P_{ih_1})}{\partial\bm{\Lambda}_j}\right)\left(\frac{\partial(P_{ih_2})}{\partial\bm{\Lambda}_f}\right)P_{ih}(1-P_{ih})\right.\\
&\qquad\qquad\quad\left.-2\sum_{(h_1,h_2)}P_{ih_1}^{\gamma}P_{ih_2}^{\gamma}\left(\frac{\partial(P_{ih_1})}{\partial\bm{\Lambda}_j}\right)\left(\frac{\partial(P_{ih_2})}{\partial\bm{\Lambda}_f}\right)\right]
&&
\end{flalign*}
Therefore, $Cov\left(\frac{\partial(V_{u_i\gamma})}{\partial\bm{\Lambda}}(\bm{\Lambda})\right)=(\gamma+1)^2R_{i\gamma}(\bm{\Lambda})\;;$ where variance term is \\$R_{i\gamma}(\bm{\Lambda})_{jj}=\frac{Var(Y)}{(\gamma+1)^2}$ and covariance term is $R_{i\gamma}(\bm{\Lambda})_{jf}=\frac{Cov(Y_j,Y_f)}{(\gamma+1)^2}$.
Hence,
\begin{flalign*}
&Y=\frac{\partial(V_{u_i\gamma}(\bm{\Lambda}))}{\partial\bm{\Lambda}}\sim N\left(\mathbf{0},(\gamma+1)^2R_{i\gamma}(\bm{\Lambda})\right)\\
&\implies \frac{1}{g_i}\sum_{u_i=1}^{g_i}\frac{\partial(V_{u_i\gamma}(\bm{\Lambda}))}{\partial\bm{\Lambda}}\sim N\left(\bm{0},\frac{(\gamma+1)^2}{g_i}R_{i\gamma}(\bm{\Lambda})\right)\\
&H_{g\bm{\Lambda}}=\frac{\partial(H_g(\gamma))}{\partial\bm{\Lambda}}\sim N\left(\bm{0},\frac{(\gamma+1)^2}{G^2}\sum_{i=1}^{I}g_i R_{i\gamma}(\bm{\Lambda})\right).
&&
\end{flalign*}
Now, we define, $T_{\gamma}=-\sqrt{G}H_{g\bm{\Lambda}}=-\sqrt{G}\frac{\partial(H_g(\gamma))}{\partial\bm{\Lambda}}$
\begin{flalign}
\implies T_{\gamma}\sim N\left(\bm{0},(\gamma+1)^2\sum_{i=1}^{I}\frac{g_i}{G}R_{i\gamma}(\bm{\Lambda})\right)\label{29}
&&
\end{flalign}
\begin{flalign*}
\text{For,}\;\frac{\partial^2V_{u_i\gamma}(\bm{\Lambda})}{\partial\bm{\Lambda}_f\partial\bm{\Lambda}_j}&=\left[\sum_{h=1}^{M}\left\{(\gamma+1)\gamma P_{ih}^{\gamma-1}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_j}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_f}+(\gamma+1)P_{ih}^{\gamma}\,\frac{\partial^2(P_{ih})}{\partial\bm{\Lambda}_j\partial\bm{\Lambda}_f}\right\}\right]\\
&\qquad-\left[(\gamma+1)\sum_{h=1}^{M}X_{u_ih}
(\gamma-1)P_{ih}^{\gamma-2}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_j}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_f}\right]\\
&\qquad-\left[(\gamma+1)\sum_{h=1}^{M}X_{u_ih}P_{ih}^{\gamma-1}\frac{\partial^2(P_{ih})}{\partial\bm{\Lambda}_j\partial\bm{\Lambda}_f}\right]
&&
\end{flalign*}
Since, $\frac{1}{k_i}\sum_{u_i=1}^{g_i}X_{u_ih}\xrightarrow{p} P_{ih}$. \\ Therefore, $\frac{1}{g_i}\sum_{u_i=1}^{g_i}\frac{\partial^2V_{u_i\gamma}(\bm{\Lambda})}{\partial\bm{\Lambda}_f\partial\bm{\Lambda}_j}\xrightarrow{p}(\gamma+1)\sum_{h=1}^{M}P_{ih}^{\gamma-1}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_j}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_f}$.
\begin{flalign*}
\text{Hence,}\quad & \frac{\partial(H_{g\bm{\Lambda}})}{\partial\bm{\Lambda}_f}=\frac{\partial^2(H_g(\gamma))}{\partial\bm{\Lambda}_f\partial\bm{\Lambda}_j}=\xrightarrow{p}\sum_{i=1}^{I}\frac{g_i}{G}(\gamma+1)\sum_{h=1}^{M}P_{ih}^{\gamma-1}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_j}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_f}
&&
\end{flalign*}
Consider $\bm{\Lambda}^{0}$ to be the true value of parameters, then using the Taylor series expansion ignoring the higher order terms,
\begin{flalign}
&H_{g\bm{\Lambda}}=H_{g\bm{\Lambda}^0}+\sum_{f=1}^{4}\left.\frac{\partial (H_{g\bm{\Lambda}})}{\partial\bm{\Lambda}_f}\right\vert_{\bm{\Lambda=\Lambda_0}}(\hat{\bm{\Lambda}}_f-\hat{\bm{\Lambda}}_f^0)+\frac{1}{2}\sum_{j=1}^{4}\sum_{f=1}^{4}\left.\frac{\partial^2 (H_{g\bm{\Lambda}})}{\partial\bm{\Lambda}_j\partial\bm{\Lambda}_f}\right\vert_{\bm{\Lambda=\Lambda_0}}(\hat{\bm{\Lambda}}_j-\hat{\bm{\Lambda}}^0_j)(\hat{\bm{\Lambda}}_f-\hat{\bm{\Lambda}}^0_f)\notag\\
&\text{Since,}\quad H_{g\hat{\bm{\Lambda}}_{\gamma}}=\bm{0}\quad \text{and therefore,}\notag\\
& -\sqrt{G}H_{g\bm{\Lambda}^0}=\sqrt{G}\sum_{f=1}^{4}(\hat{\bm{\Lambda}}_f-\hat{\bm{\Lambda}}^0_f)\left[\left.\frac{\partial (H_{g\bm{\Lambda}})}{\partial\bm{\Lambda}_f}\right\vert_{\bm{\Lambda=\Lambda_0}}+\frac{1}{2}\sum_{j=1}^{4}\sum_{f=1}^{4}\left.\frac{\partial^2 (H_{g\bm{\Lambda}})}{\partial\bm{\Lambda}_j\partial\bm{\Lambda}_f}\right\vert_{\bm{\Lambda=\Lambda_0}}(\hat{\bm{\Lambda}}_j-\hat{\bm{\Lambda}}^0_j)\right]\label{30}
&&
\end{flalign}
Then,
\begin{flalign}
A_{(j,f)}&\xrightarrow{p}\sum_{i=1}^{I}\frac{g_i}{G}(\gamma+1)\sum_{h=1}^{M}P_{ih}^{\gamma-1}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_j}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_f}\;,\;\text{where}\,\;\frac{g_i}{G}\;\text{is finite.}\notag\\
\implies A_{\gamma}&\xrightarrow{p}(\gamma+1)Q_{\gamma}(\bm{\Lambda}^0)\,\;\text{when}\;g_i\to \infty.\label{31}
&&
\end{flalign}
where, $A_{\gamma}$ is the $4\times 4$ matrix with (j,f)th elements as $A_{(j,f)}$ where $A_{(j,f)}=\left.\frac{\partial (H_{g\bm{\Lambda}})}{\partial\bm{\Lambda}_f}\right\vert_{\bm{\Lambda=\Lambda_0}}+\frac{1}{2}\sum_{j=1}^{4}\sum_{f=1}^{4}\left.\frac{\partial^2 (H_{g\bm{\Lambda}})}{\partial\bm{\Lambda}_j\partial\bm{\Lambda}_f}\right\vert_{\bm{\Lambda=\Lambda_0}}(\hat{\bm{\Lambda}}_j-\hat{\bm{\Lambda}}^0_j)$ and\\
$Q_{\gamma}(\bm{\Lambda}^0)=\left[\left(\sum_{i=1}^{I}\frac{g_i}{G}\sum_{h=1}^{M}P_{ih}^{\gamma-1}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_j}\frac{\partial(P_{ih})}{\partial\bm{\Lambda}_f}\right)_{j,f}\right]$.  Let, $Z_f=\sqrt{G}(\hat{\bm{\Lambda}}_f-\bm{\Lambda}^0_f)$.  Then by equation \eqref{31} in equation \eqref{30},
\begin{flalign*}
&-\sqrt{G}H_{g\bm{\Lambda}^0}=Z_fA_{(j,f)}\implies T_{\gamma}=Z_{\gamma}A_{\gamma} \quad(\text{from equation}
\;\eqref{29})
\implies Z_{\gamma}=A_{\gamma}^{-1}T_{\gamma}\\
&\sqrt{G}(\hat{\bm{\Lambda}}_{\gamma}-\bm{\Lambda}^0)=A_{\gamma}^{-1}T_{\gamma}\sim N\left(\bm{0},Q_{\gamma}(\bm{\Lambda}^0)^{-1}R_{\gamma}(\bm{\Lambda}^0)Q_{\gamma}(\bm{\Lambda}^0)^{-1}\right)\;;\;R_{\gamma}(\bm{\Lambda}^0)=\sum_{i=1}^{I}\frac{g_i}{G}R_{i\gamma}(\bm{\Lambda}).
&&
\end{flalign*}

\section{Proof of Result \eqref{res4}}
\noindent The functional $S_{\gamma}(M)$ satisfies,
\begin{flalign*}
\sum_{i=1}^{I}\frac{g_i}{G}&\left[\left\{p^{\gamma}_{i0}\frac{\partial(p_{i0})}{\partial\bm{\Lambda}}+\sum_{l=1}^{L}\sum_{r=1}^{2}p^{\gamma}_{ilr}\frac{\partial(p_{ilr})}{\partial\bm{\Lambda}}\right\}\right.\\
&\left.-\left\{\left(\int_{I_{i0}}^{}dM\right)p^{\gamma-1}_{i0}\frac{\partial(p_{i0})}{\partial\bm{\Lambda}}+\sum_{l=1}^{L}\sum_{r=1}^{2}\left(\int_{I_{ilr}}^{}dM\right)p^{\gamma-1}_{ilr}\frac{\partial(p_{ilr})}{\partial\bm{\Lambda}}\right\}\right]=0
&&
\end{flalign*}
Replacing $M$ by $M=(1-\epsilon)F_{\bm{\Lambda}}+(\epsilon)\delta_{I_A}(\bm{t})$ and differentiating with respect to $\epsilon$ at $\epsilon\to 0^{+}$ we get the desired result.\\ 
where,
$\delta_{I_A}(\bm{t}) = 
\begin{cases}
1 \quad \text{if} \  \bm{t} \in I_A\\
0 \quad \text{otherwise}.\\
\end{cases}
$ \\

\section{Proof of Result \eqref{res5}}
\begin{flalign*}
IF(t;T^{(\gamma)}_G,F_{\bm{\Lambda}})&=\left.\frac{\partial}{\partial\epsilon}T^{(\gamma)}_G(M_{\epsilon})\right\vert_{\epsilon\to 0^{+}}=Cov_{(p)}\left(\bm{\Lambda},X_{\gamma}(\bm{\Lambda};\bm{t},f_{\bm{\Lambda}})\right)
&&
\end{flalign*}
where, $Cov_{(p)}()$ is the covariance for posterior distribution and
\begin{flalign*}
X_{\gamma}(\bm{\Lambda};t,f_{\bm{\Lambda}})&=\left.\frac{\partial }{\partial\epsilon}\{B^w_{\gamma}(\bm{\Lambda};M_{\epsilon},F_{\bm{\Lambda}})\}\right\vert_{\epsilon\to 0^{+}}\\
&=\frac{1}{\gamma}\sum_{i=1}^{I}\frac{g_i}{G}\left[\bigg\{\big(\delta_{I_{i0}}(t)-p_{i0}\big)p_{i0}^{\gamma}\bigg\}+\left\{\sum_{l=1}^{L}\sum_{r=1}^{2}\big(\delta_{I_{ilr}}(t)-p_{ilr}\big)p^{\gamma}_{ilr}\right\}\right].
&&
\end{flalign*}

\bibliographystyle{elsarticle-num} 
 \bibliography{cas-refs}
\end{document}

\endinput
%%
%% End of file `elsarticle-template-num-names.tex'.

