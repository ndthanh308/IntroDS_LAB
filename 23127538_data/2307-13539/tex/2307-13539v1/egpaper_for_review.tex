\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{tabularray}
\usepackage{array}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\usepackage[normalem]{ulem}
\usepackage{etoc}

\def\NB#1{{\color{red}{\bf [NB:} {\it{#1}}{\bf ]}}}
\def\JW#1{{\color{blue}{\bf [JL:} {\it{#1}}{\bf ]}}}
\def\RK#1{{\color{orange}{\bf [RK:} {\it{#1}}{\bf ]}}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{4163} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}



%%%%%%%%% TITLE
\title{Model Calibration in Dense Classification with Adaptive Label Perturbation}

\author{Jiawei Liu \;\, Changkun Ye \;\, Shan Wang \;\, Ruikai Cui \;\, Jing Zhang \;\, Kaihao Zhang \;\, Nick Barnes
\\
The Australian National University\\
% Institution1 address\\
% {\tt\small \{jiawei.liu3, changkun.ye, shan.wang, ruikai.cui, jing.zhang, kaihao.zhang, nick.barnes\}@anu.edu.au}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi


%%%%%%%%% ABSTRACT
% \begin{abstract}
% For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration degree, we propose a Self-Calibrating Binary Cross Entropy (SC-BCE) loss that unifies a range of label augmentation techniques, including Stochastic Label Perturbation (SLP) which draws from a Bernoulli distribution with label perturbation probability to augment the groundtruth label with a pre-selected label perturbation strength. Further, we assume heteroscedastic noises in training data and propose an Adaptive Stochastic Label Augmentation (ASLP) to learn an unique label perturbation probability for each training sample. Adaptive Stochastic Label Augmentation can (1) follow Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy in respect of missing information while preserving classification accuracy on known data as a conservative solution, denoted as $\text{ASLP}_{\text{MEI}}$, or (2) specifically improve model calibration degree by minimising the gap between the prediction accuracy and expected confidence of target training label, named as $\text{ASLP}_{\text{MC}}$. Extensive results demonstrate that $\text{ASLP}_{\text{MC}}$ significantly improves calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data.
% \end{abstract}

\begin{abstract}
For safety-related applications, it is crucial to produce trustworthy deep neural networks whose prediction is associated with confidence that can represent the likelihood of correctness for subsequent decision-making. Existing dense binary classification models are prone to being over-confident. To improve model calibration, we propose Adaptive Stochastic Label Perturbation (ASLP) which learns a unique label perturbation level for each training image. ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel), and label smoothing, to correct calibration while maintaining classification rates. ASLP follows Maximum Entropy Inference of classic statistical mechanics to maximise prediction entropy with respect to missing information. It performs this while: (1) preserving classification accuracy on known data as a conservative solution, or (2) specifically improves model calibration degree by minimising the gap between the prediction accuracy and expected confidence of the target training label. Extensive results demonstrate that $\text{ASLP}$ can significantly improve calibration degrees of dense binary classification models on both in-distribution and out-of-distribution data. The code is available on \url{https://github.com/Carlisle-Liu/ASLP}.
\end{abstract}


\etocdepthtag.toc{mtchapter}
\etocsettagdepth{mtchapter}{subsection}
\etocsettagdepth{mtappendix}{none}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:introduction}
Binary segmentation aims to differentiate foreground areas from the background in images. Its tasks include Salient Object Detection~\cite{ucnet++}, Camouflaged Object Detection~\cite{camouflaged_obj}, Smoke Detection~\cite{yan2022transmission}, \etc. Performance in these tasks has been significantly advanced using the strong representation powers of Deep Neural Networks (DNNs). However, with complex structures and a tremendous number of parameters, DNNs are prone to over-fitting to training data and producing over-confident predictions in the real world \cite{guo2017calibration}. Such issues can render the model predictions unreliable in decision making or utilisation in downstream tasks. 
% \NB{later in intro - too out of place in this paragraph} \sout{We choose to verify our method on Salient Object Detection task and corroborate the results with extra experiments on Camouflaged Object Detection and Smoke Detection.}

% Figure environment removed



Recently, a growing body of literature has been proposed to address model mis-calibration problems in DNNs. They can be roughly categorised as: (1) post-hoc operations, such as temperature scaling \cite{guo2017calibration}, Platt scaling \cite{platt1999probabilistic}, \etc, (2) training objective approaches \cite{karandikar2021soft}, like MMCE \cite{MMCE}, soft calibration objective \cite{karandikar2021soft}, focal loss \cite{focal_loss,ghosh2022adafocal}, and (3) data/label augmentation techniques, \eg label smoothing \cite{muller2019does} and mixup \cite{zhang2018mixup}. We propose an Adaptive Label Perturbation which learns a unique label perturbation level for each training image. As illustrated in Fig.~\ref{fig:intro_figure}, training with Adaptive Stochastic Label Perturbation, a form of ALP, can effectively moderate incorrect predictions and highlight them with high entropy values.

% We propose a Self-Calibrating Binary Cross Entropy loss that unifies a range of label augmentation techniques, including Label Smoothing \cite{szegedy2016rethinking} and DisturbLabel \cite{Xie_2016_CVPR}. Our proposed Stochastic Label Perturbation (SLP) follows a Bernoulli distribution with label perturbation probability $\alpha$ to draw a perturbed label with label perturbation strength $\beta$ equivalent to label smoothing, and $1 - \alpha$ chance to adopt the original groundtruth label. 


% We propose an Adaptive Label Perturbation (ALP) which learns a unique label perturbation level for each training image (heteroscedastic). As illustrated in Fig.~\ref{fig:intro_figure}, training with Adaptive Stochastic Label Perturbation, a form of ALP, can effectively moderate incorrect predictions and highlight them with high entropy values.


% The proposed Adaptive Label Perturbation (ALP) can approximate the Maximum Entropy Inference \cite{jaynes1957information} to maximise the prediction entropy while preserving the ideal dense classification performance on known data. This represents a conservative solution that adopts classification accuracy as proxy on known data and assumes maximum disorder on unknown data. 
% % We present an alternative Adaptive Label Perturbation ($\text{ALP}_{\text{MC}}$) solution that maximises the model calibration by using a calibration regulariser which constrains the expected confidence of individual supervision signal from dropping below the ideal accuracy on validation set.
% We also present an alternative ALP solution that, instead, takes model calibration degree as proxy on known data, using a calibration regulariser which constrains the expected confidence of individual supervision signal from dropping below the ideal accuracy on validation set.
% This effectively minimises the gap between the distributions of prediction confidence and prediction accuracy, which is the source of model mis-calibration.


Adaptive Label Perturbation employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss, which unifies label perturbation processes including stochastic approaches (like DisturbLabel \cite{Xie_2016_CVPR}), and label smoothing \cite{szegedy2016rethinking} to correct calibration while maintaining classification accuracy.
% \RK{why name it classification rates rather than accuracy/performance/capability/quality/precision/correctness}. 
SC-BCE loss is equivalent to 
% \NB{can be transformed is how you describe the proof, but that just shows that it is that}
a factored combination of (i) a BCE loss \wrt groundtruth label, and (ii) a BCE loss \wrt a uniform binary categorical distribution.
% and model prediction distribution, 
The former enhances dense binary classification performance and the latter improves the model calibration degree. 
% We empirically find that, for each label perturbation probability, there are a wide range of label perturbation probabilities that can reduce model over-confidence without negatively impacting the model's classification accuracy \NB{Why introduce this here - what is your point?}.
Our method can be connected to Maximum Entropy Inference \cite{jaynes1957information} of classic statistical mechanics, to maximise prediction entropy with respect to missing information while preserving the classification accuracy on known data.



% Our proposed Self-Calibrating Binary Cross Entropy loss is equivalent to 
% % \NB{can be transformed is how you describe the proof, but that just shows that it is that}
% a factored combination of (i) a
% regular Binary Cross Entropy loss, and (ii) the cross entropy between a uniform binary categorical distribution and model prediction distribution, where the former enhances the dense binary classification performance and the latter improves the model calibration degree. 
% % We empirically find that, for each label perturbation probability, there are a wide range of label perturbation probabilities that can reduce model over-confidence without negatively impacting the model's classification accuracy \NB{Why introduce this here - what is your point?}.
% Our method can be connected to Maximum Entropy Inference \cite{jaynes1957information} of classic statistical mechanics, to maximise prediction entropy with respect to missing information while preserving the classification accuracy on known data.


% Model calibration methods usually assume a validation set to tune their proposed parameters. Temperature Scaling (TS) \cite{guo2017calibration} employs a temperature hyperparameter (T), tuned on a validation set, to soften the model prediction confidence without changing classification results.
% % and so reduces the over-confidence without changing the classification results. 
% It assumes homoscedastic noise and uses a single temperature for the entire distribution. AdaFocal \cite{ghosh2022adafocal} assumes that, when evaluating the model calibration degree with bin-based expected calibration error \cite{guo2017calibration}, training and validation samples falling into the same bin are associated. Thus, applying bin-wise confidence adjustment on training samples can lead to improved model calibration on validation set. 
% % Differently from these methods, we propose an Adaptive Stochastic Label Perturbation that assumes heteroscedastic noise in training set and learns an unique label perturbation probability for individual training sample.
% Differently from these methods, we propose an Adaptive Label Perturbation that assumes heteroscedastic noise in the training set and learns an unique label perturbation probability for SLP or label perturbation strength for LS on individual training sample.

The proposed Adaptive Label Perturbation (ALP) can approximate Maximum Entropy Inference \cite{jaynes1957information} to maximise prediction entropy while preserving the ideal dense classification performance on known data. This represents a conservative solution that adopts classification accuracy as a proxy for known data and assumes maximum disorder on unknown data. 
% We present an alternative Adaptive Label Perturbation ($\text{ALP}_{\text{MC}}$) solution that maximises the model calibration by using a calibration regulariser which constrains the expected confidence of individual supervision signal from dropping below the ideal accuracy on validation set.
We also present an alternative ALP solution that, instead, takes model calibration degree as a proxy for known data, using a calibration regulariser which constrains the expected confidence of individual supervision signal to not  drop below the ideal accuracy on the validation set.
This effectively minimises the gap between the distributions of prediction confidence and prediction accuracy, which is the source of model mis-calibration.


% We propose Adaptive Label Perturbation ($\text{ALP}_{\text{MEI}}$), which can approximate the Maximum Entropy Inference \cite{jaynes1957information} to maximise the prediction entropy while preserving the ideal dense classification performance on known data. This represents a conservative solution that adopts classification accuracy as proxy on known data and assumes maximum disorder on unknown data. 
% % We present an alternative Adaptive Label Perturbation ($\text{ALP}_{\text{MC}}$) solution that maximises the model calibration by using a calibration regulariser which constrains the expected confidence of individual supervision signal from dropping below the ideal accuracy on validation set.
% We also present an alternative Adaptive Label Perturbation ($\text{ALP}_{\text{MC}}$) solution that, instead, takes model calibration degree as proxy on known data, using a calibration regulariser which constrains the expected confidence of individual supervision signal from dropping below the ideal accuracy on validation set.
% This effectively minimises the gap between the distributions of prediction confidence and prediction accuracy, which is the source of model mis-calibration.
% % \NB{Thinking about this the point is a bit more subtle. We are still doing max entropy, just not with respect to classification. Is the regularizer something we can justify as about known?}

Our contributions can be summarised as: 
% (i) We present a Self-Calibrating Binary Cross Entropy loss that is enabled by a Stochastic Label Perturbation technique, 
(i) We propose Adaptive Stochastic Label Perturbation that 
% \sout{assumes heteroscedastic confidence on the training set to} 
learns a sample-wise label perturbation level to improve model calibration; 
(ii) We present a Self-Calibrating Binary Cross Entropy loss that unifies label perturbation processes including stochastic approaches and label smoothing;
(iii) Following Maximum Entropy Inference \cite{jaynes1957information}
% \sout{ of classic statistical mechanics}, 
we show that Adaptive Stochastic Label Perturbation ($\text{ASLP}_{\text{MEI}}$), can maximise the prediction entropy while preserving the ideal dense classification accuracy, and 
(iv) We present an alternative Adaptive Stochastic Label Perturbation ($\text{ASLP}_{\text{MC}}$) solution to maximise model calibration degree, which achieves state-of-the-art performance in terms of model calibration degree on both in-distribution and out-of-distribution data. We thoroughly evaluate our method on Salient Object Detection and demonstrate its effectiveness for Camouflaged Object Detection, Smoke Detection and Semantic Segmentation.

% We verify our method on Salient Object detection, Camouflaged Object detection, Smoke detection, and semantic segmentation.


\section{Related Works}
\label{sec:related_works}
\noindent{\bf Model Calibration:}
Model calibration methods can be roughly divided into three categories of approach: (1) post hoc processing (2) training object, and (3) input/label augmentation. The first category utilises a validation set to align the prediction confidence distribution with prediction accuracy distribution. It includes histogram binning \cite{zadrozny2001obtaining}, isotropic regression \cite{zadrozny2002transforming}, Platt scaling \cite{platt1999probabilistic,niculescu2005predicting}, Bayesian binning into quantiles \cite{naeini2015obtaining}, Dirichlet scaling \cite{kull2019beyond}, mix-n-match \cite{zhang2020mix} and temperature scaling \cite{guo2017calibration,yu2022robust}. 

The second category focuses on designing training objectives that (in)directly improve  model calibration degree. Some methods address the in-continuity of expected calibration error, a widely adopted model calibration measure, and propose trainable calibration objectives like maximum mean calibration error \cite{MMCE}, soft calibration objective \cite{karandikar2021soft}. Other works discover that certain existing training objects are beneficial to model calibration, \eg Brier loss \cite{brier1950verification,degroot1983comparison}, confidence penalty \cite{pereyra2017regularizing} and focal loss \cite{focal_loss,ghosh2022adafocal}.

The third category employs data or label augmentation techniques to regularise the prediction confidence distribution. Mixup \cite{zhang2018mixup,thulasidasan2019mixup} explores the neighbourhood of training data through random interpolation of input images and associated labels to improve model calibration degree. Label smoothing \cite{muller2019does} augments the one-hot training labels with softer versions to prevent the model being over-confident.

\noindent{\bf Salient Object Detection:}
Inspired by pioneering work \cite{itti1998model}, traditional Saliency Object Detection (SOD) methods rely on various heuristic priors with handcraft features to explore low-level cures \cite{achanta2009frequency,jiang2013salient,jiang2013submodular,liu2010learning,wang2016kernelized}. However, these methods cannot cope with complex scenes because of the limited representation ability of handcrafted features \cite{borji2019salient}.
%
% To break the bottleneck of traditional methods, various deep learning based SOD methods make use of the powerful capability of neural networks and achieve great improvement in performance. 
%
Recently, deep learning based SOD methods broke the bottleneck of traditional methods due to the powerful capability of neural networks, achieving improvemed performance \cite{chen2018reverse,ji2020context,qin2019basnet,xu2021locate,liu2018picanet,wang2016saliency,wang2019iterative}. 
% For a more in-depth analysis of SOD, we point the reader to in the survey paper \cite{wang2021salient}.
Early deep SOD methods use multi-layer perception to predict a map with a pixel-wise score for each image \cite{wei2012geodesic,cheng2014global}. These approaches rely on fixed fully connected layers and thus severely limit the ability of spatial information extraction. Later methods address this issue via using fully convolutional networks (FCNs) \cite{long2015fully}.

Most contemporary SOD methods are designed based on FCNs with various schemes to improve performance. One of the most popular strategies is to fuse multi-scale information extracted from different layers and aggregate them in the network \cite{zhang2018bi,wu2019stacked,gao2020highly,pang2020multi,zhao2020suppress,zhang2020weakly}. 
% Typically, these methods are built on an encoder-decoder framework, in which the features from different layers of encoders are aggregated to the outputs of decoders \cite{zhang2017amulet,hou2017deeply,wang2018salient,wang2019iterative,zhao2019egnet,zhou2020interactive}. 
Attention modules are also applied to capture powerful multi-scale features via exploring relationship between local and global information \cite{liu2018picanet,piao2019depth,zhao2019pyramid,hu2020sac}.
Training SOD networks using auxiliary supervision is also a popular strategy \cite{wei2020label,wei2020label,tang2021disentangled}. For example, the body map and detail map are utilized by \cite{wei2020label} to help the network focus on center areas and edges, respectively. Skeleton \cite{liu2020dynamic} and uncertainty \cite{tang2021disentangled} are applied to the training processing due to their important roles in taking photos.

\noindent{\bf Noisy Label:}
Noisy labels refer to incorrect ground truth classes/values in classification/regression tasks. They arise from data collection or annotation processes, and exist commonly in real-world datasets \cite{algan2021image}. Efforts are put to identify the noisy labels and exclude them from network training in various computer vision tasks, deeming their incorporation as harmful. \cite{zhang2020learning} proposes a framework that learns from noisy labels, being a collection of predictions from classic SOD methods. The framework approximates the noise distribution in order to recover clean labels for model training. 

Differently from data augmentation techniques that are applied simultaneously to training samples and corresponding labels to generate more training data, one may artificially corrupt the label. We refer to this category of approaches as {\em label perturbation}, which includes label smoothing approaches 
% (soften the label) 
\cite{szegedy2016rethinking}, and DisturbLabel, Xie \etal \cite{Xie_2016_CVPR}. In image classification, Xie \etal \cite{Xie_2016_CVPR} shows that randomly replacing training labels with a prior distribution leads to a regularising effect, preventing overfitting. Our work is different from \cite{Xie_2016_CVPR} by employing labels corrupted to different scales to enhance the model calibration degrees for both in-distribution and out-of-distribution data. Further, in performing this, we assume noise that varies with different samples, making our method more adaptable.

% \sout{
% Little interest has been paid to study the effect of artificially corrupted label. This should be distinguished from data augmentation techniques that are applied simultaneously to training samples and their corresponding labels in order to generate more training data. In artificially corrupted labels, augmentations are solely applied to labels without modifying the corresponding training sample, \eg label smoothing \cite{szegedy2016rethinking}.
% Xie \etal \cite{Xie_2016_CVPR} shows that, in image classification, randomly replacing training labels with a prior distribution brings regularising effect on the network, preventing it from overfitting. Our work is different from \cite{Xie_2016_CVPR} by employing labels corrupted to different scales to enhance the model calibration degrees for both in-distribution and out-of-distribution data. Further, in performing this, we assume heteroscedastic noise that. varies with different samples, making our method more adaptable.}


% Figure environment removed


\section{Proposed Method}
% \label{sec:method}
% In this section, 
We first lay out the task setting in Sec.~\ref{sub_sec:task_setting}. Then we introduce our proposed Self-Calibrating Binary Cross Entropy loss in Sec.~\ref{sub_sec:sc_bce_loss} and prove its connection to Maximum Entropy Inference \cite{jaynes1957information} in Sec.~\ref{sub_sec:maximum_entropy_inference}. Lastly, we detail our major contribution - Adaptive Label Perturbation in Sec.~\ref{sub_sec:adaptive_stochastic_label_perturbation}.


\subsection{Task Setting}
\label{sub_sec:task_setting}
Binary segmentation problems aim to differentiate between foreground object(s) and background. They can be formulated as a pixel-wise binary classification problem. Given an independent and identically distributed (i.i.d) training dataset $\mathcal{D}_{\text{TR}} = \{x_{i}, y_{i}\}_{i=1}^{N}$ drawn from an unknown joint distribution of training images and groundtruth labels $P(\mathcal{X}, \mathcal{Y})$, a neural network model parameterised by $\theta$
is employed to predict labels for an input image $x \in \mathcal{X}$: $f_{\theta}(x) \in (0, 1)^{1 \times H \times W}$. We use $\hat{y}$ and $P_{\hat{y}}$ to denote the winning class and its associated probability respectively. 
% See Appendix.~\ref{A_sub_sec:Model_Implementation} for more details. 
% \RK{seems abrupt, I do not expect any details when understanding the content} 
The groundtruth label $y \in \{0, 1\}^{1 \times H \times W}$  represents the foreground pixels with \enquote{1} and background with \enquote{0}. In the following equations, we omit the spatial indexes $H$ and $W$ for simplicity.
A perfectly calibrated model has $\mathrm{P}(\hat{y} = y | P_{\hat{y}}) = P_{\hat{y}}, \, \forall P_{\hat{y}} \in (0, 1)$. 
% \RK{I don't know what is c upon the previous reading, and why c is the conditioing term}
That is, in the entire range of prediction probabilities, prediction with probability $P_{\hat{y}}$ has exactly $P_{\hat{y}}$ chance to be correct. 
% \RK{I understand what does model calibration means until I read here. Maybe explain this before would be better}
The calibration degree of a model $f_{\theta}(\cdot)$ over a distribution $\mathcal{D}$ is quantified with Expected Calibration Error (ECE), defined as $\mathbb{E}_{f_{\theta}(x)} [ | \mathrm{P}(\hat{y} = y | f_{\theta}(x)) - f_{\theta}(x) | ]$. 

% ECE is generally approximated with bin-based methods \cite{guo2017calibration}. \RK{Is this necessary, I cannot get any infomation as I don't known bin-based method}

% , such as iuoEqual-Width ECE ($\text{ECE}_{\text{EW}}$) \cite{guo2017calibration}, Equal-Mass ECE ($\text{ECE}_{\text{EM}}$) \cite{nguyen-oconnor-2015-posterior}, \etc.


\subsection{Self-Calibrating Binary Cross Entropy Loss}
\label{sub_sec:sc_bce_loss}
We propose a Self-Calibrating Binary Cross Entropy (SC-BCE) loss that unifies Label Smoothing \cite{szegedy2016rethinking}, DisturbLabel \cite{Xie_2016_CVPR} and Stochastic Label Perturbation as:
\begin{equation}
\begin{aligned}
    & \mathcal{L}_{\text{SC-BCE}}(\theta, X, Y, \alpha, \beta) \\ 
    = & \mathbb{E}_{x,y \in X,Y} \Bigl[ (1 - Z_{t}(x,y)) \cdot \mathcal{L}_{\text{BCE}}(\theta, x, y) \\
    & \qquad \qquad + Z_{t}(x,y) \cdot \mathcal{L}_{\text{BCE}}(\theta, x, p(y, \beta)) \Bigr]\\
    & \beta \in [0, 2], \quad \alpha \in [0, \frac{1}{\beta}),
\end{aligned}
\label{eq:SC-BCE_loss_definition}
\end{equation}
where $Z_{t}(x,y) \sim B(1, \alpha)$ follows a Bernoulli distribution with $\alpha$ probability to be $1$ and $1 - \alpha$ chance to be $0$, $t$ denotes the training epoch, $\alpha \in [0, \frac{1}{\beta})$ and $\beta \in [0, 2]$ are Label Perturbation Probability (LPP) and Label Perturbation Strength (LPS) respectively, 
% $P$ is a perturbed label defined as: $p(y, \beta) = (1 - \beta) \cdot y + \frac{\beta}{2}, \beta \in [0, 2]$
$p(y, \beta) = (1 - \beta) \cdot y + \frac{\beta}{2}, \beta \in [0, 2]$ is a perturbed label
and $\mathcal{L}_{\text{BCE}}(\theta, x, y)$\footnote{$\mathcal{L}_{\text{BCE}}(\theta, x, y) = -y \cdot \log(f_{\theta}(x)) - (1 - y) \cdot \log(1 - f_{\theta}(x))$} is a Binary Cross Entropy (BCE) loss computed for training pair $(x, y)$. For $\alpha=1$, the label perturbation equation follows the label smoothing equation for a binary label \cite{szegedy2016rethinking}. In the proposed SC-BCE loss, different label perturbations can be applied by setting (i) Label Smoothing \cite{szegedy2016rethinking}: $\alpha = 1$ and $\beta \in [0, 1)$, (2) DisturbLabel: $\beta = 1$ and $\alpha \in (0, 1)$, and (3) Stochastic Label Perturbation (SLP): $\beta \in (0, 2]$ and $\alpha \in (0, \frac{1}{\beta})$. For example, Hard Inversion (HI) that inverts the label category as shown in Fig.~\ref{fig:ASLP_ECE_overview}, can be stochastically applied by setting $\beta = 2$ and $\alpha \in (0, \frac{1}{2})$. 
% A regular BCE loss \wrt groundtruth label can also be recovered by setting both $\alpha = 0$ and $\beta = 0$.


In the implementation of SLP, the supervision for an individual training image in each epoch is sampled by drawing from a Bernoulli distribution. That is, the individual supervision can take the form of the groundtruth label or perturbed label in each training iteration. The overall function of SLP can be connected to that of a smoothed label by taking expectation of the Bernoulli variable: $\mathbb{E}_{Z_{t}}[(1 - Z_{t}(y)) \cdot Y + Z_{t}(y) \cdot p(y, \beta)], \, \forall y \in Y$. 
% Such implementation is too expensive to materialise in model training \NB{Not sure what this implementation is proposed - converge it each time?}. 
Taking the expectation over the Bernoulli variation in each iteration is too expensive to implement in model training.
Instead, following \cite{Xie_2016_CVPR}, the expectation of stochastically perturbed label is approximated by taking expectation across training epochs: $\mathbb{E}_{t \in T}[(1 - Z_{t}(y)) \cdot Y + Z_{t}(y) \cdot p(y, \beta)], \, \forall y \in Y$, where $T$ is the total number of training epochs and $Z_{t}(y)$ is a variable drawn from a Bernoulli distribution for the $\text{t}^{\text{th}}$ epoch.


% \begin{equation}
% p(y, \beta) = (1 - \beta) \cdot y + \frac{\beta}{2}, \beta \in [0, 2]
% \end{equation}
% \NB{Then explain the inverted label option, and the option of setting $\alpha$ to zero, and $\beta$ small so that there is no stochastic - just label perturbation in usual way.}
% $\alpha \in [0, \frac{1}{\beta})$ and $\beta \in [0, 2]$ are Label Perturbation Probability (LPP) and Label Perturbation Strength (LPS) respectively, 

% $\mathcal{L}_{\text{BCE}}(\theta, x, y)$\footnote{$\mathcal{L}_{\text{BCE}}(\theta, x, y) = -y \cdot \log(f_{\theta}(x)) - (1 - y) \cdot \log(1 - f_{\theta}(x))$} is a Binary Cross Entropy (BCE) loss computed for training pair $(x, y)$. SC-BCE computes the loss by taking expectation over training samples with stochastically perturbed labels with $\alpha \neq 0, \beta = (0, 2]$ or smoothed labels with $\alpha = 1, \beta = (0, 1]$ or original groundtruth labels with $\alpha = 0, \beta = 0$ (See Appendix.~\ref{A_sec:SC-BCE_loss_derivation} for connection to LS).

% The label perturbation equation follows the typical Label Smoothing (LS) function $LS(Y, \sigma) = (1 - \sigma) \cdot Y + \frac{\sigma}{K}$ \cite{muller2019does,lukasik2020does,xu2020towards,zhang2021delving} where the class number is set to $K = 2$ for a binary task and $\sigma$ is the smoothing strength. However, different from LS that limits $\sigma \in [0, 1)$, we allow the label perturbation to adopt more aggressive techniques $\beta \in [0, 2]$ by perturbing the label stochastically. As illustrated in Fig.~\ref{fig:ASLP_ECE_overview}, setting $\beta = 2$ produces an inverted groundtruth label. Despite the aggressively perturbed labels being seemingly incorrect, we show that stochastically substituting the groundtruth label with such aggressively perturbed labels as supervisions with a proper probability during training are beneficial to calibrating dense binary classification models.
% We can also recover a regular Binary Cross Entropy (BCE) loss by setting $\alpha = 0, \beta = 0$ and a BCE w/ LS loss by setting $\alpha = 1$ and $\beta \in (0, 1)$ (See Appendix.~\ref{A_sec:SC-BCE_loss_derivation} for connection to LS). 
% Overall, we restrict the product $\alpha \beta \in [0, 1)$ so that the expectation of the augmented label does not differ from the original groundtruth label in terms of categories. 




\subsection{Maximum Entropy Inference}
\label{sub_sec:maximum_entropy_inference}
Maximum Entropy Inference (MEI), assuming minimum distribution commitment in respect to missing information, was initially proposed by Jaynes \cite{jaynes1957information}. That is, the probability distribution should have maximum Shannon entropy subject to the partially available information.
% that is given.
Thus, in the complete absence of information, Shannon entropy for a binary prediction defined in Eq.~\eqref{eq:entropy_definition} should be maximised:
\begin{equation}
\begin{aligned}
    \mathbb{H}(f_{\theta}(X)) = \mathbb{E}_{x \in X} \Bigl[&- f_{\theta}(x) \cdot \log f_{\theta}(x) -\\
    & \bigl( 1 - f_{\theta}(x) \bigr) \cdot \log \bigl(1 - f_{\theta}(x) \bigr) \Bigr]
\label{eq:entropy_definition}
\end{aligned}
\end{equation}
% \NB{I think this should be the entropy between the network $f_{\theta}$ and the label - perhaps y. You are proposing below to set the label to 0.5 for max entropy. I don't think them both being $f_{\theta}$ makes sense here?} \JW{Entropy between network $f_{\theta}$ and the label becomes BCE loss. It should only associated with model prediction. The previous definition was to cater also for the cross entropy between uniform binary categorical distribution and model distribution $\mathbb{H}(\frac{1}{2}, f_{\theta}(X))$. Now I've redefined $\mathbb{H}(\frac{1}{2}, f_{\theta}(X))$ as $\mathcal{L}_{BCE}(U, f_{\theta}(X))$}
For salient object detection and other binary segmentation problems,
% (\ie foreground background segmentation), 
maximising 
% \sout{the Shannon entropy defined in} 
Eq.~\eqref{eq:entropy_definition} can be achieved with a binary uniform categorical distribution.


The proposed SC-BCE loss can be transformed into a factored combination of a BCE loss \wrt groundtruth label (the constraints of the data) and a BCE loss \wrt a binary uniform categorical distribution (See Appendix~\ref{A_sec:connection_between_SC-BCE_and_MEI} for derivation) as:
\begin{equation}
\begin{aligned}
    % \mathcal{L}_{\text{SC-BCE}}(\theta, X, Y, \alpha, \beta) = & (1 - \beta  Z) \cdot \mathcal{L}_{\text{BCE}}(\theta, X, Y)\\
    % & + \beta Z \cdot \mathcal{H}(U(X), f_{\theta}(X)),
    &\mathcal{L}_{\text{SC-BCE}}(\theta, X, Y, \alpha, \beta)\\ 
    = & \mathbb{E}_{x,y \in X,Y} \Bigl[ (1 - \beta  Z_{t}(x, y)) \cdot \mathcal{L}_{\text{BCE}}(\theta, x, y)\\
    & \qquad \qquad + \beta Z_{t}(x, y) \cdot \mathcal{L}_{\text{BCE}}(\theta, x, u) \Bigr],
\label{eq:overall_training_loss_definition}
\end{aligned}
\end{equation}
where $Z_{t}(x,y) \sim B(1, \alpha)$, $u$ is a binary uniform categorical distribution, and minimising the second term pushes the prediction distribution towards a uniform binary categorical distribution, equivalently maximising the inference entropy. Therefore, our proposed SC-BCE loss, a combination of a regular BCE loss and a BCE loss with a perturbed label, effectively performs a type of
% \sout{ term, is comparable to the} 
MEI.
% \sout{ of classic statistical mechanics}.
That is, the regular BCE loss component improves the model's binary classification accuracy in the presence of information
% \sout{. In addition, we maximise the} 
while the perturbed label maximizes prediction entropy with respect to missing information in order to close the gap between the available training data and the entire data distribution.
% from which the training data are drawn.

% That is, in the presence of information, we hold a regular BCE loss to improve model's binary classification accuracy, and in respect to missing information, being the gap between the available training data and the entire data distribution from which the training data are drawn, maximises the inference entropy.
% \NB{break last sentence into 2}

% \NB{Following}
% \sout{The most conservative approach, following} the constrained optimisation of MEI \cite{jaynes1957information} \sout{, is to} finds the largest product $\alpha \cdot \beta$ which achieves the ideal model's binary classification accuracy without any form of label augmentation. 
% This assumes that we have maximum entropy
% \NB{for all data not constrained by a given classification rate, which is a conservative assumption.}
% \sout{where data is unavailable, which may not always be the case.}
% \NB{However, we find that this leads to an under-confident model.}
% \sout{The conservative approach is found to render the model under-confident.} To produce a \sout{more} \NB{better} calibrated model, a small value for the product of label perturbation probability and strength is preferred, \eg $\alpha \cdot \beta = 0.02$. 


\subsection{Adaptive Label Perturbation}
\label{sub_sec:adaptive_stochastic_label_perturbation}
Stochastic Label Perturbation (SLP) uses a single label perturbation probability and perturbation strength for the entire training dataset.
% \sout{, with a selected label perturbation strength}.
% \RK{the term perturbation may not be sufficiently explain in previous section. I am not sure about the exact operation}
% , given a selected label augmentation strength, for the entire training dataset. 
However, this approach cannot adapt to predictive error that varies for different input images. To address this, we propose an Adaptive Stochastic Label Augmentation (ASLP) method 
% with a learning rule to 
to adjust the label augmentation probability for individual training samples. That is we allow the variable in Eq.~\eqref{eq:SC-BCE_loss_definition} to be drawn from a per training image Bernoulli distribution with sample-specific label perturbation probability as:
\begin{equation}
\begin{aligned}
Z_{t}(x, y) \sim B(1, \alpha_{x, y}), \, \forall x, y \in X, Y
\end{aligned}
\label{eq:sample_specific_Bernoulli_variable}
\end{equation}
where $\alpha_{x, y}$ is the label perturbation probability for sample (image-label) pair $(x, y)$. Initially, we set all label perturbation probabilities to $\{\alpha_{i} = 0\}_{i=1}^{N}$ and train a model with a regular BCE loss without label augmentation techniques, which is equivalent to $\mathcal{L}_{\text{SC-BCE}}(\theta, X, Y, \alpha=0, \beta=0)$. The trained model has weight $\theta_{\text{lm}}$ and its accuracy on the validation set, $\mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}})$, is held as an ideal performance. Subsequently, we select a label perturbation technique and continue to train the model with SC-BCE loss with a learning rule to update the label perturbation probability for individual training samples.


We propose the learning rule for $\alpha$, ($\text{ASLP}_{\text{MEI}}$) to approximate maximum entropy inference. The rule has two components: (1) $\nabla_{\alpha_{i}} = (2 / \beta) \cdot \partial \mathbb{E}_{Z_{t}}(x_{i}, y_{i}) \bigl[ \mathcal{L}_{\text{SC-BCE}}(X, Y, \theta, \{\alpha_{i}\}_{i=1}^{N}, \beta) \bigr] / \partial \alpha_{i}$ is the derivative of the expectation of SC-BCE over the Bernoulli variable \wrt $\alpha_{i}$. We divide this by $\beta / 2$ to ensure that different perturbation techniques (varying $\beta$ values) have the same convergence speed (See derivation in Appendix~\ref{A_sec:Grad_Alpha_Derivation}),
% \begin{equation}
%     \text{Grad-}\alpha_{i} = \frac{\partial \mathbb{E}_{Z_{t}(x_{i}, y_{i})} \bigl[ \mathcal{L}_{\text{SC-BCE}}(X, Y, \theta, \{\alpha_{i}\}_{i=1}^{N}, \beta) \bigr]}{\beta \cdot \partial \alpha_{i}},
% \label{Eq:Grad_alpha}
% \end{equation}
and (2) Accuracy Regularization to encourage maintenance of prediction accuracy. The rule is:
% $\text{Grad-}\alpha_{i} = \partial \mathbb{E}_{Z_{t}}(x_{i}, y_{i}) \bigl[ \mathcal{L}_{\text{SC-BCE}}(X, Y, \theta, \{\alpha_{i}\}_{i=1}^{N}, \beta) \bigr] / \beta \cdot \partial \alpha_{i}$ the derivative of the expectation of SC-BCE over the Bernoulli variable \wrt $\alpha_{i}$, and divided by $\beta$; and Accuracy Regularization to encourage maintenance of prediction accuracy. The rule is:
% \begin{equation}
% \begin{aligned}
%     \alpha_{i}^{n + 1} 
%     = \; & \alpha_{i}^{n} + \eta \cdot \biggl( \underbrace{ \frac{2 \cdot \bigl( \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)) - \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) \bigr) }{\beta}}_{\text{Grad-}\alpha_{i}}\\
%     & + \lambda \cdot \underbrace{\min \Bigl(\frac{\mathbb{A}(\theta, \mathcal{D}_{\text{VAL}}) - \mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}})}{\mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}}) }, 0\Bigr)}_{\text{Accuracy Regularisation}} \biggr)\\
%     &\text{for} \quad i = 1, \dots, N,
% \label{eq:ASLP_MEI_Learning_Rule}
% \end{aligned}
% \end{equation}
% \NB{I think you need to explain where the divided by $\beta$ came from} \JW{I direct them to Appendix A.3 for the explanation. Otherwise it takes a lot of space.}
% \begin{equation}
% \begin{aligned}
%     \alpha_{i}^{n + 1} 
%     = \; & \alpha_{i}^{n} + \eta \cdot (\nabla_{\alpha_{i}}
%     + \lambda \cdot \text{Reg}), \ \ \ \text{for} \ i = 1, \dots, N,
% \label{eq:ASLP_MEI_Learning_Rule}
% \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
    & \alpha_{i}^{n + 1} 
    = \; \alpha_{i}^{n} + \eta \cdot (\nabla_{\alpha_{i}}
    + \lambda \cdot \text{Reg}_A), \ \ \ \text{for} \ i = 1, \dots, N,\\
    &\nabla_{\alpha_{i}} = \frac{2 \cdot \bigl( \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)) - \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) \bigr) }{\beta}, \\
    & \text{Reg}_{\text{A}} = \min \Bigl(\frac{\mathbb{A}(\theta, \mathcal{D}_{\text{VAL}}) - \mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}})}{\mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}}) }, 0\Bigr),
\label{eq:ASLP_MEI_Learning_Rule}
\end{aligned}
\end{equation}
% where
% \begin{align}
%     \nabla_{\alpha_{i}} & = \frac{2 \cdot \bigl( \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)) - \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) \bigr) }{\beta}, \\
%     \text{Reg} & = \min \Bigl(\frac{\mathbb{A}(\theta, \mathcal{D}_{\text{VAL}}) - \mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}})}{\mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}}) }, 0\Bigr).
% \end{align}
% \RK{chaneg to The $\eta$ and $\lambda$ are hyperparameters controlling}
where $\eta$ and $\lambda$ are hyperparameters controlling the updating pace of label perturbation probability and the regularisation strength respectively, $\mathbb{A}(\theta, \mathcal{D}_{\text{VAL}})$ and $\mathbb{A}(\theta_{lm}, and \mathcal{D}_{\text{VAL}})$ denote the current and ideal accuracy on the validation set separately.
$\nabla_{\alpha_{i}}$ 
% (See derivation in Appendix.~\ref{A_sec:Grad_Alpha_Derivation})
% is the gradient of Eq.~\eqref{eq:overall_training_loss_definition} \wrt label perturbation probability, which 
aims to increase label perturbation probability to confident and correct samples and otherwise for incorrectly classified samples. For example, it returns a large positive value for correct predictions with small BCE loss value \wrt to groundtruth label $y_{i}$ and large BCE loss value \wrt to perturbed label $p(y_{i}, \beta)$.
% should have a positive gradient with larger values corresponding to higher confidences as illustrated in Fig.~\ref{fig:ASLP_ECE_overview}. 
% \NB{Dividing by $\beta$ has no impact in the stochastic case where $\beta$ is close to 1. When $\beta$ is far from 1 (e.g.), label smoothing, it removes the effect of a factor of $\beta$ in the derivative which otherwise slows convergence (see  
% Appendix.~\ref{A_sec:Grad_Alpha_Derivation}.)}
The \enquote{Accuracy Regularisation} ($\text{Reg}_{\text{A}}$) is designed to reduce the overall perturbation probability if the accuracy on the validation set reduces to be below the local minima. It returns 0 if there is no accuracy drop on the validation set and a large decrease will overwhelm the $\nabla_{\alpha_{i}}$ value and reduce the sample label perturbation probability.
Intuitively, $\text{ASLP}_{\text{MEI}}$ aims to construct a model that preserves the ideal classification accuracy while otherwise maximising the entropy \cite{jaynes1957information}. Note that which particular examples are classified correctly are able to change, but the accuracy is constrained to remain the same. Intuitively, having a model that better captures ignorance may lead to changes in the treatment of test examples that are distant from training distribution. Note, however that adopting classification accuracy as proxy for known data and otherwise maximizing entropy is a conservative strategy 
% that assumes maximum disorder\NB{ other than preserving accuracy} 
and we find that it results in the model being significantly under-confident.

The model mis-calibration arises from the distribution mismatch between prediction confidence and prediction accuracy \cite{focal_loss}. We offer an alternative model that uses the model calibration as proxy for known data and maximises the prediction entropy in respect to unknown data $\text{ASLP}_{\text{MC}}$.
% aims to optimise explicitly for model calibration, $\text{ASLP}_{\text{MC}}$. 
The learning rule replaces the \enquote{Accuracy Regularisation} in Eq.~\eqref{eq:ASLP_MEI_Learning_Rule} with a \enquote{Calibration Regularisation} ($\text{Reg}_{\text{C}}$) as:
\begin{equation}
    \text{Reg}_{\text{C}} = \min \Bigl( \bigl(\mathrm{1} - \frac{\beta \cdot \alpha_{i}^{n}}{2} \bigr) - \mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}}), 0\Bigr), 
\label{eq:Calibration_Regularisation_for_ASLP_MC}
\end{equation}
where $\mathrm{1} - (\beta \cdot \alpha_{i}^{n} / 2)$ denotes the expected confidence of  the perturbed label (Derivation in Appendix~\ref{A_sec:confidence_of_ecpectation_of_augmented_label}). For example, a foreground label \enquote{1} with 5\% chance of being inverted to \enquote{0} has an expected confidence of 0.95.
\enquote{Calibration Regularisation} constrains the expected confidence of perturbed label of each sample to not drop below the ideal classification accuracy on validation set, preventing the model from becoming under-confident. Note that we can also have an updating rule $\text{ALS}_{\text{MC}}$ to learn per-image label perturbation strength (adaptive $\beta$ and fixed $\alpha = 1$) (See Appendix~\ref{A_sec:Adaptive_Label_Smoothing}).




% \begin{algorithm}
% \caption{Adaptive Stochastic Label Perturbation}
% \label{alg:ASLP}
% \textbf{Input:} \\
% % Model Parameter: $f_{\theta_{\text{inter}}}(\cdot)$;
% % \Comment{Model optimised with a BCE loss (Eq.~\ref{eq:BCE_loss_definition}) has converged to a local minima.}\\
% Training and Validation Dataset: $\mathcal{D}_{\text{TR}} = \{x_{i}, y_{i}\}_{i=1}^{N_{\text{TR}}}$, $\mathcal{D}_{\text{VAL}} = \{x_{i}, y_{i}\}_{i=1}^{N_{\text{VAL}}}$; \\
% Training Epoch: $E_{1}$, $E_{2}$; \\
% Hyper-parameter: $\beta$, $\eta$, $\lambda$\\
% \textbf{Output:} \\
% Final Model Parameter: $f_{\theta_{\text{final}}}(\cdot)$; \\
% \begin{algorithmic}[1]
%     \Function{ASLP}{$\theta, \mathcal{D}(x, y), \alpha_{E_{1}}, \beta$}
%     \For{$e \gets 1$ to $E_{1} - 1$}
%         \State Train the model $f_{\theta_{e}}(\cdot)$ with $\mathcal{L}_{\text{BCE}}(\theta, X_{\text{TR}}, Y_{\text{TR}})$;
%     \EndFor
%     \State Initialise $\{\alpha^{e}_{i} = 0\}_{i=1}^{ N_{\text{TR}} }$;
%     \For{$e \gets E_{1}$ to $E_{1} + E_{2}$}
%         \State Train the model with $\mathcal{L}_{\text{SC-BCE}}(\theta, X, T(Y, \{\alpha_{i}^{e}\}_{i=1}^{ N_{\text{TR}} }, \beta))$;
%         \For{$i \gets 1$ to $| \mathcal{D}_{\text{TR}} |$}
%             \State Update $\alpha_{i}^{e + 1}$ with learning rules $\text{ASLP}_{\text{MEI}}$ or $\text{ASLP}_{\text{MC}}$ (Eq.~\eqref{eq:ASLP_MEI_Learning_Rule} and Eq.~\eqref{eq:Calibration_Regularisation_for_ASLP_MC});
%         \EndFor
%     \EndFor
%     \EndFunction
% \end{algorithmic}
% \end{algorithm}

\section{Experiments and Results}
\label{sec:experiments_and_results}
We verify the proposed method primarily on Salient Object Detection and also implement it for Camouflaged Object Detection, Smoke Detection and Semantic Segmentation tasks and report their results in the Appendices.
\subsection{Implementation Details}
\label{sub_sec:implementation_details}
\noindent\textbf{Evaluation Metrics:}
We use Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) \cite{guo2017calibration} and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) \cite{thulasidasan2019mixup} with 10 bins ($B = 10$) to evaluate the model calibration degrees. 
% We report these methods with different numbers of bins ($B = \{15, 30, 50\}$) in Appendix.~\ref{A_sec:TEXTURE100_Samples}. 
Additionally, we adopt $\text{ECE}_{\text{EM}}$ \cite{nguyen-oconnor-2015-posterior}, $\text{ECE}_{\text{DEBIAS}}$ \cite{kumar2019verified} and $\text{ECE}_{\text{SWEEP}}$ \cite{roelofs2022mitigating} to corroborate with the results of $\text{ECE}_{\text{EW}}$ (See Appendix~\ref{A_sec:model_calibration_benchmark_with_ECE_EM_SWEEP_DEBIAS}).

\noindent\textbf{Datasets:}
The proposed methods are trained with the DUTS-TR \cite{DUTS-TE} training dataset. It is divided into a training set $|\mathcal{D}_{\text{TR}}| = 9,553$ and validation set $|\mathcal{D}_{\text{VAL}}| = 1,000$. We use six testing datasets, including DUTS-TE \cite{DUTS-TE}, DUT-OMRON \cite{DUT-OMRON}, SOD \cite{SOD}, PASCAL-S \cite{PASCAL-S}, ECSSD \cite{ECSSD}, HKU-IS \cite{HKU-IS}, to evaluate the model calibration degree.
% of the proposed and compared models.


\begin{table*}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Salient object detection model calibration degree benchmark. Results are evaluated in with $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ with 10 bins (units in \%).
% and {\color{red} red} and {\color{blue} blue} indicate the best and the second-best performance respectively.) 
See Appendix~\ref{A_sec:model_calibration_benchmark_with_ECE_EM_SWEEP_DEBIAS} for evaluations with $\text{ECE}_{\text{EM}}$ \cite{nguyen-oconnor-2015-posterior}, $\text{ECE}_{\text{DEBIAS}}$ \cite{kumar2019verified} and $\text{ECE}_{\text{SWEEP}}$ \cite{roelofs2022mitigating}. 
% See Appendix~\ref{A_sec:SOD_Calibration_Degree_Benchmark_15_Bins} for evaluations with $\text{ECE}_{\text{EW}}$ using 15 bins.
}
\vspace{-2pt}
\begin{tabular}{cl|c|cc|cc|cc|cc|cc|cc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}} & {\multirow{2}{*}{Year}} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& & & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ \\
\midrule
\multirow{18}{*}{\parbox{1.0cm}{SOD \\ Methods}} 
& MSRNet \cite{li2017instance} & 2017 & 2.57 & 2.34 & 3.32 & 3.16 & 3.44 & 3.23 & 6.42 & 6.14 & 0.97 & 0.94 & 0.92 & 0.87\\
& SRM \cite{Wang_2017_ICCV} & 2017 & 4.02 & 3.72 & 4.19 & 3.96 & 4.88 & 4.59 & 9.93 & 9.58 & 2.53 & 2.35 & 1.86 & 1.72\\
& Amulet \cite{zhang2017amulet} & 2017 & 5.67 & 5.28 & 5.84 & 5.49 & 5.76 & 5.43 & 10.03 & 9.59 & 2.56 & 2.42 & 1.98 & 1.87\\
& BMPM \cite{zhang2018bi} & 2018 & 3.74 & 3.52 & 4.52 & 4.37 & 4.88 & 4.68 & 8.16 & 7.93 & 1.95 & 1.89 & 1.58 & 1.53 \\
& DGRL \cite{wang2018detect} & 2018 & 4.12 & 3.86 & 4.41 & 4.21 & 5.01 & 4.77 & 8.44 & 8.20 & 2.13 & 2.02 & 1.63 & 1.53\\
& PAGR \cite{zhang2018progressive} & 2018 & 4.04 & 3.79 & 5.14 & 4.96 & 5.64 & 5.37 & 12.17 & 11.87 & 2.84 & 2.70 & 1.62 & 1.54\\
& PiCANet \cite{liu2018picanet} & 2018 & 5.12 & 4.90 & 4.84 & 4.70 & 8.14 & 7.92 & 10.50 & 10.30 & 3.48 & 3.39 & 2.55 & 2.47\\
& CPD \cite{wu2019cascaded} & 2019 & 3.97 & 3.78 & 4.20 & 4.06 & 5.37 & 5.17 & 9.65 & 9.39 & 2.29 & 2.19 & 1.99 & 1.90\\
& BASNet \cite{qin2019basnet} & 2019 & 5.00 & 4.86 & 4.93 & 4.83 & 6.50 & 6.36 & 10.40 & 10.27 & 2.74 & 2.70 & 2.30 & 2.26\\
& EGNet \cite{zhao2019egnet} & 2019 & 3.33 & 3.14 & 3.66 & 3.50 & 5.42 & 5.19 & 8.04 & 7.79 & 1.98 & 1.88 & 1.47 & 1.40\\
& AFNet \cite{feng2019attentive} & 2019 & 3.95 & 3.74 & 4.25 & 4.09 & 5.06 & 4.84 & 8.15 & 8.02 & 2.38 & 2.27 & 1.87 & 1.78\\
& PoolNet \cite{Liu21PamiPoolNet} & 2019 & 3.33 & 3.12 & 3.86 & 3.70  & 5.32 & 5.07 & 8.14 & 7.87 & 2.00 & 1.90 & 1.82 & 1.75\\
& GCPANet \cite{chen2020global} & 2020 & 3.18 & 2.99 & 3.99 & 3.84 & 4.16 & 3.97 & 7.05 & 6.88 & 1.61 & 1.54 & 1.27 & 1.21\\
& MINet \cite{pang2020multi} & 2020 & 3.65 & 3.48 & 4.45 & 4.29 & 4.94 & 4.75 & 8.01 & 7.89 & 2.13 & 2.03 & 1.74 & 1.65 \\
& $\text{F}^{3}\text{Met}$ \cite{wei2020f3net} & 2020 & 3.67 & 3.50 & 4.25 & 4.10 & 4.85 & 4.67 & 7.95 & 7.78 & 2.26 & 2.16 & 1.92 & 1.83 \\
& EBMGSOD \cite{jing_ebm_sod21} & 2021 & 3.45 & 3.29 & 4.11 & 3.95 & 4.79 & 4.61 & 7.48 & 7.30 & 2.14 & 2.05 & 1.79 & 1.70\\
& ICON \cite{zhuge2021salient} & 2021 & 2.89 & 2.76 & 3.84 & 3.71 & 4.08 & 3.95 & 6.70 & 6.55 & 1.56 & 1.49 & 1.38 & 1.32\\
& PFSNet \cite{ma2021pyramidal} & 2021 & 2.94 & 2.72 & 3.95 & 3.81 & 4.45 & 4.27 & 7.59 & 7.39 & 2.41 & 2.25 & 2.06 & 1.96\\
& EDN \cite{wu2022edn} & 2022 & 3.62 & 3.47 & 4.02 & 3.90 & 4.89 & 4.74 & 8.81 & 8.66 & 2.20 & 2.13 & 1.65 & 1.58\\
\hline
\multirow{7}{*}{\parbox{1.0cm}{Model \\ Calibration \\ Methods}}
& Brier Loss \cite{brier1950verification} & 1950 & 2.77 & 2.58 & 3.55 & 3.38 & 3.90 & 3.70 & 6.40 & 6.16 & 1.37 & 1.30 & 1.04 & 0.99 \\
& Temperature Scaling \cite{guo2017calibration} & 2017 & 2.53 & 2.34 & 3.18 & 3.03 & 3.56 & 3.36 & 6.32 & 6.05 & 0.96 & 0.93 & 0.83 & 0.70\\
& MMCE \cite{MMCE} & 2018 & 2.86 & 2.67 & 3.56 & 3.41 & 4.00 & 3.81 & 6.85 & 6.63 & 1.41 & 1.35 & 1.18 & 1.13\\
& Label Smoothing \cite{muller2019does} & 2019 & 2.00 & 1.79 & 2.89 & 2.71 & 3.04 & 2.83 & 5.97 & 5.69 & {\color{blue} 0.83} & 0.68 & {\color{blue} 0.82} & 0.47\\
& Mixup \cite{thulasidasan2019mixup} & 2019 & 2.45 & 2.25 & 3.41 & 3.23 & 3.13 & 2.99 & {\color{blue} 5.82} & 5.70 & 1.41 & {\color{blue} 0.18} & 3.83 & 0.05\\
& Focal Loss \cite{focal_loss} & 2020 & 2.25 & 2.08 & 3.10 & 2.82 & 3.40 & 3.13 & 6.21 & 5.98 & 1.41 & 1.03 & 1.24 & 0.77\\
& AdaFocal \cite{ghosh2022adafocal} & 2022 & {\color{blue}1.61} & 1.41 & {\color{blue}2.31} & 1.84 & {\color{blue}2.53} & 2.27 & 5.88 & 5.47 & 1.63 & 0.79 & 1.35 & 0.52\\
\hline
\multirow{2}{*}{\parbox{1.0cm}{Our \\ Methods}}
& $\text{ASLP}_{\text{MC}}$  & 2023 & {\color{red} \textbf{1.40}} & {\color{blue} 1.22} & {\color{red} \textbf{1.99}} & {\color{blue} 1.83} & {\color{red} \textbf{2.31}} & {\color{blue} 2.10} & {\color{red} \textbf{5.50}} & {\color{blue} 5.17} & {\color{red} \textbf{0.48}} &  0.20 & {\color{red} \textbf{0.79}} & {\color{blue} 0.17}\\
& $\text{ASLP}_{\text{MEI}}$ & 2023 & 27.9 & {\color{red} \textbf{0.01}} & 26.0 & {\color{red} \textbf{0.00}} & 26.1 & {\color{red} \textbf{0.00}} & 22.4 & {\color{red} \textbf{0.00}} & 29.9 & {\color{red} \textbf{0.00}} & 30.5 & {\color{red} \textbf{0.00}}\\
\bottomrule
\end{tabular}
\label{tab:SOD_Calibration_Degree_Benchmark}
\end{table*}



\noindent \textbf{Compared Methods:}
We compare with both SOD models and model calibration methods
% , implemented on our baseline model, 
in terms of model calibration degrees. The SOD models include: MSRNet \cite{li2017instance}, SRM \cite{Wang_2017_ICCV}, Amulet \cite{zhang2017amulet}, BMPM \cite{zhang2018bi}, DGRL \cite{wang2018detect}, PAGR \cite{zhang2018progressive}, PiCANet \cite{liu2018picanet}, CPD \cite{wu2019cascaded}, BASNet \cite{qin2019basnet}, EGNet \cite{zhao2019egnet}, AFNet \cite{feng2019attentive}, PoolNet \cite{Liu21PamiPoolNet}, GCPANet \cite{chen2020global}, MINet \cite{pang2020multi}, $\text{F}^{3}\text{Met}$ \cite{wei2020f3net}, EBMGSOD \cite{jing_ebm_sod21}, ICON \cite{zhuge2021salient}, EDN \cite{wu2022edn}. We evaluate ECE on their published prediction results, or results produced with their released model weights. We also compare with model calibration methods include: Temperature Scaling (TS) \cite{guo2017calibration}, Brier Loss \cite{brier1950verification}, MMCE \cite{MMCE}, Label Smoothing \cite{muller2019does}, Mixup \cite{thulasidasan2019mixup}, Focal Loss \cite{focal_loss} and AdaFocal \cite{ghosh2022adafocal} implemented on our baseline model.

\noindent\textbf{Baseline Model Structure:}
We implement our method in the Pytorch framework. Our model has a simple U-Net structure, comprising of a ResNet50 encoder \cite{he2016deep} and a decoder, where the former is initialised with ImageNet-pretrained weights and the later by default. We also experiment with VGG16 \cite{simonyan2014very} and Swin transformer \cite{liu2021swin} encoders and report their results in Appendix~\ref{A_sec:Experiments_with_Additional_Backbones}.

\noindent\textbf{Label Perturbation Techniques:}
We experiment with four different label perturbation strategies: (1) Hard Inversion $T(Y, \alpha, \beta=1.0)$, (2) Soft Inversion (SI) $T(Y, \alpha, \beta=0.75)$, (3) Moderation (M) $T(Y, \alpha, \beta=0.5)$ and Dynamic Moderation (DM) $T(Y, \alpha, \beta=0.5)$ with additional 
% \NB{what is ex Gaussian?} 
Gaussian noise. See Appendix~\ref{A_sub_sec:SLP_Implementation} for implementation details.


\noindent\textbf{Training Details:} 
Each model is trained with an Adam optimiser for 30 epochs. The learning rate is initialised to $2.5 \times 10^{-5}$, and decays by a factor of 0.9 for each epoch after the $10^{\text{th}}$ epoch. All training images are scaled to $384 \times 384$. Basic data augmentation techniques, including random flipping, random translation and random cropping, are applied.

\noindent\textbf{Hyperparameters:}
The reported model calibration results associated with ASLP are obtained by setting $\eta = 0.002$ and $\lambda = 2,000$. 
% Their selections are studied in Sec.~\ref{sub_sub_sec:hyperparameters}.
We set the number of bins to $B = 10$ for ECE and OE evaluation metrics. 
% We also report $\text{ECE}_{\text{EW}}$ results with $B = 15$ in Appendix.~\ref{A_sec:SOD_Calibration_Degree_Benchmark_15_Bins}.




% Figure environment removed



\subsection{Model Calibration Degree Performance}
Tab.~\ref{tab:SOD_Calibration_Degree_Benchmark} presents the calibration degree of existing SOD models, existing model calibration methods and our proposed technique on the six SOD testing datasets. Our proposed $\text{ASLP}_{\text{MC}}$, designed to optimise the model calibration degree, achieves the best ECE performances on all testing datasets. 
% However, clear improvements of $\text{ASLP}_{\text{MC}}$ over Label Smoothing \cite{muller2019does} can be observed on DUTS-TE, DUT-OMRON, PASCAL-S, SOD and ECSSD datasets. 
% AdaFocal \cite{ghosh2022adafocal} has the overall closest model calibration performance in terms of ECE scores. But our $\text{ASLP}_{\text{MC}}$ still outperforms Adafocal by 10\% or more on the testing datasets.
In addition, $\text{ASLP}_{\text{MC}}$ also obtains the second-best OE performances on all six testing datasets, outperformed only by our $\text{ASLP}_{\text{MEI}}$.
% , which is a conservative solution that tends to produce significantly under-confident predictions. 
% successfully reduces the OE scores to 0 on most of the testing datasets (5 out of 6). 
% Compared to existing SOD models, $\text{ASLP}_{\text{MC}}$ has significant gains in both ECE and OE scores.
On the other hand, $\text{ASLP}_{\text{MEI}}$, though almost eliminates the over-confidence issue completely, is significantly mis-calibrated on the six testing datasets. This can be attributed to it being significantly under-confident rather than over-confident. The observed performances of $\text{ASLP}_{\text{MEI}}$ are in accordance with its design - assuming minimum distribution commitment with respect to missing information. That is, in the presence of limited training data, to maximise the prediction entropy while maintaining the prediction accuracy for in-distribution data. 
% In the following section, we would see that the design of $\text{ASLP}_{\text{MEI}}$ is beneficial on the out-of-distribution data in terms of both prediction accuracy and model calibration degree.

% Fig.~\ref{fig:joint_distribution_plot} presents the joint distribution of prediction confidence and prediction accuracy \cite{thulasidasan2019mixup} of existing SOD methods, model calibration methods and our proposed technique on DUTS-TE dataset (See Appendix.~\ref{A_sec:More_Joint_Plot} for other testing datasets and other methods). 
% Existing SOD methods produce extremely confident predictions whose confidence scores are nearly 100\% for the majority of samples. On the contrary, the prediction accuracy is on average lower than prediction confidence, resulting in the model being over-confident. On the other hand, existing model calibration methods are generally more calibrated than the SOD methods which in general do not strive to improve model calibration degree.
% % This, coupling with impecfect prediction accuracy, results in over-confident model (Joint distribution to the right-bottom side of the oracle line). On the other hand, 
% % existing calibration methods are generally more calibrated than the SOD methods which do not pay specific attention to model calibration. 
Fig.~\ref{fig:joint_distribution_plot} presents the joint distribution of prediction confidence and prediction accuracy 
% \cite{thulasidasan2019mixup} 
of some of the best calibrated methods and our proposed technique on DUTS-TE dataset 
% \NB{do you mean the best or most accurately calibrated methods - the word most may be ambiguous here} 
(See Appendix~\ref{A_sec:More_Joint_Plot} for other testing datasets and other methods). Existing SOD methods produce extremely confident predictions whose confidence scores are nearly 100\% for the majority of samples. On the contrary, the prediction accuracy is on average lower than prediction confidence, resulting in the model being over-confident. On the other hand, existing model calibration methods are generally more calibrated than the SOD methods which in general do not strive to improve model calibration degree.

Our proposed $\text{ASLP}_{\text{MC}}$ produces the most calibrated model whose joint distribution is closer to the oracle than those of the compared calibration methods and SOD methods. AdaFocal \cite{ghosh2022adafocal} produces the second most calibrated model on DUTS-TE. However, the majority of their joint distribution ({\color{blue} blue} high density area) is slightly to the right bottom of the oracle line, making their model slightly less calibrated. Whereas the high density area of our joint distribution is well aligned with the oracle line, showing $\text{ASLP}_{\text{MC}}$ is more calibrated. Despite the small difference on the plot, $\text{ASLP}_{\text{MC}}$ improves over AdaFocal by more than 10\% in terms of ECE scores on DUTS-TE dataset.
We can also observe that $\text{ASLP}_{\text{MEI}}$ is significantly under-confident for in-distribution data with its joint distribution being at the top-left side of the oracle line. Its prediction confidences are limited to between 70\% and 80\% while the prediction accuracies are generally above 90\%.
%\NB{I think this visualization can be improved. The text on the graphs and numbers are way to small, while they don't need to be. Because all the information is concentrated into the top right hand corner, you really can't see anything until you really zoom. For me the difference between all was hardly visible from normal size, and the difference for AdaFocal and ASLP_{ECE} was not apparent until I had it full width on a 38 '' monitor. Iwonder if you could zoom the top right
%}

% \JW{I'll redo the visualisation and increase the font size and zoom into the upper right corner}
%\NB{Sounds good. You'll also have to instruct the reader on how to read these diagrams a bit. Some in caption, some in Supp. The key point for these is really the symmetry of the top right I guess?}
% \JW{Calibrated model has symmetric prediction confidence distribution (top horizontal plot) and prediction accuracy distribution (right vertical plot). The resultant joint distribution of a calibrated model should be along the red line (oracle - perfectly calibrated model).  I'll add these to the caption.}
%\NB{For this bit, for AdaFocal, it seems slightly better centred for some of the red line that us - around 0.8 we seem more accurate than confident? But I guess this is not much of the data? } \JW{Yes, that's just a small proportion of data}
% \JW{I'll also add that in the joint distribution, darker blue indicates higher population, which needs to be taken account into account. On the contrary, lighter blue indicates very few data}
%\NB{Is that there now - I can't see it on ASLP_{ECE}? That is do you show it now - or is that what you will add?} 
% \JW{I've shown it, but its difference is very small. If you zoom into them, you may see that the darkest blue area, ASLP_ECE is nicely on the red line and Adafocal is slightly off to the right bottom. That;s the most of the data}
%\NB{could you increase the contrast?}
% \JW{I think that's manageable. I'll redo the plot. Maybe use a different colour for high density instead of same colour, different intensity}
%\NB{OK - see what you can do.}
% \NB{remember to add some description of how to read this. I think the story holds, but you need to take the reader on the journey for this one} 
% \JW{OK}

% Figure environment removed

\subsection{Model Calibration Degrees on Out-of-Distribution Dataset}
We compare our proposed method with existing model calibration methods in terms of model calibration degrees on Out-of-Distribution data. We consider texture images, where salient objects are completely absent, as OoD samples for the SOD task. 
% Thus, we collect a dataset consisting of 116 texture images, named TEXTURE100 \NB{why do you do your own one? There are texture datasets?} \JW{I did not realise. I'll run them on the DTD real quick} (see Appendix.~? for examples),
We use Describable Texture Dataset \cite{cimpoi2014describing} to evaluate the model calibration degrees on OoD samples.
% in terms of model calibration degree. 
Fig.~\ref{fig:joint_plot_texture100} shows the joint distribution of prediction confidence and prediction accuracy of various model calibration methods and our proposed techniques. It can be seen that the baseline model produces extremely confident predictions for OoD data. However, its accuracy is only 41.88\%, 
% \sout{which is even} 
worse 
% \sout{to the accuracy of a prior distribution}
than a uniform prior in a binary classification task. We also observe that Temperature Scaling does not calibrate the model under data distribution shift in accordance with literature \cite{ovadia2019can,focal_loss}. Our $\text{ASLP}_{\text{MC}}$, being the most calibrated for in-distribution data, is also more calibrated on the OoD samples than the existing model calibration methods by a large margin as shown in Tab.~\ref{tab:TEXTURE100_quantitative}. On the other hand, our $\text{ASLP}_{\text{MEI}}$ is more successful in handling OoD data. It is the most calibrated on OoD data, with a larger proportion of the distribution aligned with the oracle line. As shown in Tab.~\ref{tab:TEXTURE100_quantitative}, it outperforms existing model calibration methods in terms of both ECE and OE by significant margins. This can be attributed to its minimum distribution assumption in the presence of limited training data. 


\begin{table}[tbh!]
    \centering
    \scriptsize
    \renewcommand{\arraystretch}{1.2}
    \renewcommand{\tabcolsep}{1.0mm}
    \caption{Model calibration methods and our $\text{ASLP}_{\text{MC}}$ and $\text{ASLP}_{\text{MEI}}$ are evaluated on the Out-of-Distribution dataset, Describable Texture Dataset \cite{cimpoi2014describing}, in terms of 
    $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ with 10 bins,
    and Accuracy (ACC).}
    \vspace{-8pt}
    \begin{tabular}{L{0.32\linewidth}|C{0.15\linewidth}C{0.15\linewidth}C{0.15\linewidth}}
    \toprule
    {\multirow{2}{*}{\parbox{3.6cm}{Method}}} & \multicolumn{3}{c}{Evaluation (\%)} \\
    & ECE $\downarrow$ & OE $\downarrow$ & ACC $\uparrow$ \\
    \midrule
    Baseline & 52.36 & 51.05 & 41.88\\
    \hline
    Brier Loss \cite{brier1950verification} & 38.85 & 37.18 & 53.62\\
    Temperature Scaling \cite{guo2017calibration} & 51.95 & 50.46 & 41.59\\
    Label Smoothing \cite{muller2019does} & 37.22 & 35.48 & 55.41\\
    MMCE \cite{MMCE} & 40.64 & 39.67 & 54.39\\
    Mixup \cite{thulasidasan2019mixup} & 31.07 & 29.10 & 58.71\\
    Focal Loss \cite{focal_loss} & 40.01 & 38.43 & 49.71\\
    AdaFocal \cite{ghosh2022adafocal} & 27.55 & 25.07 & 55.39\\
    \hline
    $\text{ASLP}_{\text{MC}}$ & 18.31 & 16.37 & 61.93\\
    $\text{ASLP}_{\text{MEI}}$ & \textbf{13.43} & \textbf{8.40} & \textbf{62.47}\\
    \bottomrule
    \end{tabular}
    \label{tab:TEXTURE100_quantitative}
    \vspace{-10pt}
\end{table}



\begin{table*}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.1}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Ablation: Effect of Stochastic Label Perturbation (SLP) and Adaptive Stochastic Label Perturbation (ASLP) with different label perturbation techniques on the model calibration degrees evaluated on Expected Calibration Error (ECE) and Over-confidence Error (OE). The proposed ASLP is generalised to an Adaptive Label Smoothing (ALS) technique that adaptively tunes the label softening scale ($\beta_{\text{ada}}$).}
\vspace{-8pt}
\begin{tabular}{l|ccc|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& $\alpha$ & $\beta$ & e & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ \\
\midrule
Baseline (\enquote{B}) & 0 & 0 & 0 & 3.48 & 3.29 & 4.17 & 4.02 & 4.60 & 4.41 & 7.42 & 7.17 & 1.93 & 1.86 & 1.64 & 1.59\\
\hline
$\text{SLP}_{\text{HI}}^{\alpha=0.01}$ & 0.01 & 1.0 & \xmark & 2.21 & 1.84 & 2.96 & 2.78 & 3.11 & 2.82 & 6.09 & 5.80 & 1.03 & 0.68 & 1.01 & 0.53\\
$\text{SLP}_{\text{SI}}^{\alpha=0.02}$ & 0.02 & 0.75 & \xmark & 2.25 & 2.05 & 3.00 & 2.82 & 3.05 & 2.83 & 6.40 & 6.09 & 0.93 & 0.84 & 0.87 & 0.60\\
$\text{SLP}_{\text{M}}^{\alpha=0.03}$ & 0.03 & 0.5 & \xmark & 2.24 & 2.03 & 3.17 & 2.97 & 3.41 & 3.20 & 6.26 & 5.97 & 0.83 & 0.77 & 0.96 & 0.81 \\
$\text{SLP}_{\text{DM}}^{\alpha=0.03}$ & 0.03 & 0.5 & \cmark & 2.29 & 2.09 & 3.00 & 2.83 & 3.47 & 3.24 & 6.72 & 6.43 & 1.13 & 1.04 & 0.96 & 0.80\\
$\text{LS}^{\beta=0.03}$ & 1.0 & 0.03 & \xmark & 2.20 & 1.99 & 3.09 & 2.91 & 3.24 & 3.03 & 6.27 & 5.99 & 1.03 & 0.78 & 0.92 & 0.67\\
\hline
$\text{ASLP}_{\text{MC}}^{\text{HI}}$ & $\alpha_{\text{ada}}$ & 1.0 & \xmark & 1.40 & 1.22 & 1.99 & 1.83 & 2.31 & 2.10 & 5.50 & 5.17 & 0.48 &  0.20 & 0.79 & 0.17\\
$\text{ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 1.51 & 1.29 & 2.14 & 1.95 & 2.29 & 2.07 & 5.12 & 4.80 & 0.61 & 0.34 & 0.84 & 0.22\\
$\text{ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \xmark & 1.47 & 1.27 & 1.87 & 1.80 & 2.37 & 2.13 & 5.63 & 5.29 & 0.51 & 0.23 & 0.80 & 0.20\\
$\text{ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \cmark & 1.64 & 1.20 & 1.94 & 1.75 & 2.03 & 1.81 & 4.14 & 3.84 & 0.80 & 0.42 & 0.87 & 0.42 \\
$\text{ALS}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 1.46 & 1.25 & 2.07 & 1.87 & 2.30 & 2.10 & 5.44 & 5.18 & 0.61 & 0.25 & 0.81 & 0.32\\
\bottomrule
\end{tabular}
\label{tab:Ablation_Study}
\vspace{-10pt}
\end{table*}



\subsection{Discussion}
\noindent\textbf{Adaptive Stochastic Label Perturbation:}
We study the effect of ASLP on ECE and OE and present the experimental results in Tab.~\ref{tab:Ablation_Study}. It shows that ASLPs significantly outperforms the baseline model, \enquote{B}, which does not adopt any model calibration measures. In addition, ASLPs also outperforms their static counterparts (SLPs) which use a single $\alpha$ for the entire dataset. This can be attributed to the approach modelling variance of noise with input image.

% We study the effect of SLP and ASLP on ECE and OE and present the experimental results in Tab.~\ref{tab:Ablation_Study}. It shows that, under a small label perturbation probability, the four proposed label perturbation techniques can alleviate the model over-confidence issues over the baseline model, \enquote{B}, on the six testing datasets. They also achieve similar results with the Label Smoothing (LS) technique \cite{muller2019does}, setting $\beta = 0.03$. SLPs, which have a static label perturbation probability, are outperformed by their adaptive versions (ASLP), which tunes a label perturbation probability per image. This shows that the stochastic label perturbation approach can reliably improve model calibration without introducing additional variables (only one hyperparameter with set values that shows improvement over a range of small values). \NB{Maybe move this point and state it more efficiently with the hyperparameter dicussion}
% % On DUTS-TE, ASLPs with different label perturbation techniques achieve respective improvements, measured as ECE reduction, of 25 - 36\% over SLPS and 52 - 59\% over the baseline model \enquote{B}. It demonstrates the benefit of per-sample perturbation parameter. It is noteworthy that both SLPs and ASLPs enhance the model calibration degree without sacrificing the prediction accuracy (See Appendix Sec.~\ref{A_sub_sec:SLP_Classification_Performance}, \ref{A_sec:Generalisation_to_SOD_Models}).

\noindent\textbf{Generalisation of Adaptive Label Smoothing:}
We generalise the proposed ASLP to label smoothing, developing an Adaptive Label Smoothing (ALS) that fixes the label perturbation probability to 100\%, akin to the label smoothing technique \cite{muller2019does}, and tunes a smoothing factor for each training sample. As shown in Tab.~\ref{tab:Ablation_Study}, ALS effectively reduces the ECE and OE scores over its static version $\text{LS}^{\beta=0.03}$, and achieves similar performances with $\text{ASLP}_{\text{MC}}$ approaches on the six testing datasets. It manifests that our proposed ASLP can be generalised onto other label perturbation techniques as a measure to calibrate the SOD models.

\noindent\textbf{Compatibility with SOTA SOD Models}
We retrain EBMGSOD \cite{jing_ebm_sod21}, ICON \cite{zhuge2021salient} and EDN \cite{wu2022edn} with the proposed $\text{ASLP}_{\text{MC}}$ and find significant improvements in terms of model calibration degrees without compromising their classification abilities (See Appendix~\ref{A_sec:Generalisation_to_SOD_Models}).

\noindent\textbf{Compatibility with Different Backbones:} We demonstrate that our proposed method is also compatible with VGG16 \cite{simonyan2014very} and Swin transformer \cite{liu2021swin} backbones. See Appendix~\ref{A_sec:Experiments_with_Additional_Backbones} for details.

\noindent\textbf{Effectiveness in Other Dense Binary Classification Tasks:} Experiments on Smoke Detection \cite{yan2022transmission} and Camouflaged Object Detection \cite{fan2020camouflaged} demonstrate that our method can be generalised to dense binary classification tasks to improve model calibration degrees. See Appendix~\ref{A_sec:Additional_Experiments_on_Dense_Binary_Classification_Tasks}.

\noindent\textbf{Generalisation to Multi-Class Segmentation task:} Experiments on Semantic Segmentation \cite{everingham2010pascal} demonstrate that our method can also be generalised to dense multi-class classification tasks. See Appendix~\ref{A_sec:Additional_Experiments_on_Semantic_Segmentation}.

\subsection{Hyperparameters}
\noindent\textbf{Static Stochastic Label Perturbation:} 
Tab.~\ref{tab:Ablation_Study} shows that, under a small label perturbation probability, the four label perturbation techniques can alleviate the model over-confidence issues of the baseline model, \enquote{B}, on the six testing datasets. They also achieve similar results to Label Smoothing \cite{muller2019does}, setting $\beta = 0.03$ and $\alpha = 1$. 
% SLPs, which have a static label perturbation probability, are outperformed by their adaptive versions (ASLP), which tunes a label perturbation probability per image. This shows that the stochastic label perturbation approach can reliably improve model calibration without introducing additional variables (only one hyperparameter with set values that shows improvement over a range of small values). \NB{Maybe move this point and state it more efficiently with the hyperparameter dicussion}
% \sout{With an increasing label perturbation probability, ECE scores of the 4 SLPs tend to reduce to a critical point before climbing. This is caused by model transitioning from being over-confident to under-confident. This is evidenced in the OE scores which keep decreasing to 0 when the label perturbation probability continues to increase. (See Appendix.~\ref{A_sub_Sec:SLP_Model_Calibration}). }
Each SLP has a wide range of effective label perturbation probabilities that improves model calibrations (See Appendix~\ref{A_sub_Sec:SLP_Model_Calibration} Tab.~\ref{A_tab:Static_SLP_Effective_Alpha_Range}), and these improvements do not sacrifice the model's classification performance (see Appendix~\ref{A_sub_sec:SLP_Classification_Performance}).
% \NB{I can't see this decrease behaviour? Its certainly not apparent in Tab 3. That they are stable over a range of small values that improves over baseline is clear.}  \JW{In Appendix.~J3, left figures have a horizontal line indicating the ECE of baseline model. When label perturbation probabilities are too high, model's ECE shoot over this line. This is a decrease in model calibration degrees.}
Larger values of the label perturbation probability eventually lead to increasing ECE scores as the model transitions to being under-confident (see Appendix~\ref{A_sub_Sec:SLP_Model_Calibration}).

\noindent\textbf{Updating Rate $\eta$:} 
% Fig.~\ref{sub_fig:ablation_study_on_learning_rate_eta} illustrates that 
$\text{ASLP}_{\text{MC}}$ models trained with $\eta \in [0.0002, 0.005]$ are generally stable, producing similar calibration degrees and classification performances. Values smaller than 0.001 require longer training and high values lead to sub-optimal results (See Appendix~\ref{A_sec:Hyperparameters}).

\noindent\textbf{Regularisation Strength $\lambda$:} 
% As shown in Fig.~\ref{sub_fig:ablation_study_on_regularisation_strength_lambda}, 
$\lambda$ spanning between 500 and 10,000 are optimal. 
Very high values for $\lambda$ can lead to oscillation resulting in poor performance (See Appendix~\ref{A_sec:Hyperparameters}).

% Overly high values can often default the updating process for sample-wise label perturbation probability $\{\alpha_{i}\}_{1}^{N}$, resulting in sub-optimal performances. \NB{I don't follow this point} \JW{When regularisation returns a negative value, a large $\lambda$ value would enlarge this value and drops the $\alpha$ to negative value, we clip $\alpha \in [0, \frac{1}{\beta}$ so that would default the learnt $\alpha$ to 0.}


\section{Conclusion}
\label{sec:conclusion}
This work first introduces a Self-Calibrating Binary Cross Entropy loss that unifies label perturbation processes including stochastic approaches and label smoothing to improve model calibration 
% degree 
while preserving classification accuracy. We further propose an Adaptive Stochastic Label Perturbation that learns a unique label perturbation level for individual training image. Following Maximum Entropy Inference, ASLP adopts classification / calibration as proxy for known data and maximises the prediction entropy with respect to missing data. The proposed $\text{ASLP}_{\text{MC}}$ improves model calibration degrees on both in-distribution samples and out-of-distribution samples, without negatively impacting classification performance. The approach can be easily applied to different models, which we demonstrate with several SOTA models. It is also demonstrated to be effective on a semantic segmentation task and other binary tasks.

\small{\vspace{.1in}\noindent\textbf{Acknowledgments.}\quad
This research was in-part supported by the ANU-Optus Bushfire Research Center of Excellence.}


{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\clearpage
\onecolumn
\appendix
\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsection}
\tableofcontents
\clearpage

\input{Appendix}

\end{document}