

\section{Derivation}
\subsection{Self-Calibration Binary Cross Entropy (SC-BCE) Loss}
\label{A_sec:SC-BCE_loss_derivation}

We show that our SC-BCE loss is close to label smoothing in binary classification. Label smoothing, as defined in Eq.~\eqref{A_eq:label_smoothing}, is a typical data augmentation that softens the training supervision signals \cite{muller2019does,lukasik2020does,xu2020towards,zhang2021delving}.
\begin{equation}
    S(Y, \sigma) = \text{LS}(Y, \sigma) = (1 - \sigma) Y + \frac{\sigma}{K}, \quad \forall y \in Y.
    \label{A_eq:label_smoothing}
\end{equation}
where $\sigma$ is the label smoothing strength hyperparameter and $K$ is the number of classes, thus is set to $K = 2$ for a binary task. For image label pairs $X,Y\sim P$, the BCE loss with label smoothing takes the form:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\text{BCE}}(\theta, X, S(Y, \sigma)) =  \mathbb{E}_{x,y \in X,Y} \biggl[ - \Bigl((1 - \sigma)y + \frac{\sigma}{2} \Bigr) \log f_{\theta}(x) - \Bigl(1 - \bigl((1 - \sigma)y + \frac{\sigma}{2} \bigr)\Bigr) \log (1 - f_{\theta}(x) ) \biggr].
\end{aligned}
\end{equation}
% \NB{That is not a straightforward equation.  Label smoothing implies that we replace Y with $(1-\alpha) Y + \alpha / 2$. This equation proposes that that is the same as adding alpha times the negative BCE loss.  This is not a simple fact. You would have to show this. I would have guessed it waas more straightforward to take the expectation over the training set of the  SC-BCE loss and show that it reduces to the $\alpha/2$ form.
% }
\noindent

On the other hand, our proposed SC-BCE loss, taking expectation over the Bernoulli variable $Z_{t}(x, y)$, can be written as:
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_{\text{SC-BCE}}(X,Y;\theta, \alpha, \beta) = & (1-Z)\mathcal{L}_{\text{BCE}}(X,Y;\theta) + Z \mathcal{L}_{\text{BCE}}(X, P(Y, \beta), \theta)\\
%     = & (1-Z)\mathcal{L}_{\text{BCE}}(X,Y;\theta) + Z \Bigg[(1-\frac{\beta}{2})\mathcal{L}_{\text{BCE}}(X,Y;\theta) + \frac{\beta}{2}\mathcal{L}_{\text{BCE}}(X,1-Y;\theta)\Bigg] \\
% \end{aligned}
% \end{equation}
\begin{equation}
\begin{aligned}
\mathbb{E}_{Z_{t}} \biggl[ \mathcal{L}_{\text{SC-BCE}}(\theta, X,Y, \alpha, \beta) \biggr] = & \mathbb{E}_{Z_{t}} \biggl[ (1-Z_{t})\mathcal{L}_{\text{BCE}}(X,Y;\theta) + Z_{t} \mathcal{L}_{\text{BCE}}(X, P(Y, \beta), \theta) \biggr]\\
= & (1 - \alpha)\mathcal{L}_{\text{BCE}}(X,Y;\theta) + \alpha  \mathcal{L}_{\text{BCE}}(X, P(Y, \beta), \theta) \\
% = & \mathbb{E}_{X} \Bigl[ - \bigl( (1 - \alpha)y + \alpha p \bigr) \log f_{\theta}(x) - \bigl( 1 - (1 - \alpha)y + (- \alpha + \alpha) - \alpha p \bigr) \log (1 - f_{\theta}(x)) \Bigr]\\
= & \mathbb{E}_{x,y \in X, Y} \Bigl[ - \bigl( (1 - \alpha)y + \alpha p \bigr) \log f_{\theta}(x) - \bigl( 1 - (1 - \alpha)y - \alpha p \bigr) \log (1 - f_{\theta}(x)) \Bigr]\\
\end{aligned}
\end{equation}
Substitute: $p(Y, \beta) = (1 - \beta) \cdot y + \frac{\beta}{2}$, then we have:
\begin{equation}
\begin{aligned}
% & \mathbb{E}_{X} \Bigl[ - \bigl( (1 - \alpha)y + \alpha p \bigr) \log f_{\theta}(x) - \bigl( 1 - (1 - \alpha)y + - \alpha p \bigr) \log (1 - f_{\theta}(x)) \Bigr]\\
& \mathbb{E}_{x,y \in X,Y} \Bigl[ - \bigl( (1 - \alpha)y + \alpha p \bigr) \log f_{\theta}(x) - \bigl( 1 - (1 - \alpha)y - \alpha p \bigr) \log (1 - f_{\theta}(x)) \Bigr]\\
= & \mathbb{E}_{x,y \in X, Y} \biggl[ - \Bigl( (1 - \alpha)y + \alpha \bigl( (1 - \beta)y + \frac{\beta}{2} \bigr) \Bigr) \log f_{\theta}(x) - \bigg( 1 - \Bigl( (1 - \alpha)y + \alpha \bigl( (1 - \beta)y + \frac{\beta}{2} \Bigr) \bigg) \log(1 - f_{\theta}(x)) \biggr]\\
= & \mathbb{E}_{x,y \in X, Y} \biggl[ - \Bigl((1 - \alpha \beta)y + \frac{\alpha \beta}{2} \Bigr) \log f_{\theta}(x) - \Bigl(1 - \bigl((1 - \alpha \beta)y + \frac{\alpha \beta}{2} \bigr)\Bigr) \log (1 - f_{\theta}(x) ) \biggr]\\
= & \mathcal{L}_{\text{bce}}(\theta, X, S(Y, \alpha \beta)),
\end{aligned}
\end{equation}
where we let $\alpha \beta = \sigma$ to show that the expectation of SC-BCE loss over with a stochastically perturbed label over a Bernoulli variable is equivalent to a BCE loss with a smoothed label.

% \NB{Should we connect the dots that $K=2$ for binary, so the difference is the scaling of the first term of Eqn 9}

% \NB{Actually, this is missing a step. You need to do this over the expecation I think.}

\subsection{Connection between SC-BCE and Maximum Entropy Inference}
\label{A_sec:connection_between_SC-BCE_and_MEI}

We prove that the SC-BCE loss maximises prediction entropy as well as minimising cross entropy between the prediction distribution and groundtruth distribution. Given the SC-BCE loss written as:
% \NB{I think you need to state here why you are setting beta in P to 2}
\begin{equation}
\begin{aligned}
    % \mathcal{L}_{\text{SC-BCE}}(X,Y;\theta) = & (1-Z_{t}) \mathcal{L}_{\text{BCE}}(X,Y;\theta) + Z_{t} \mathcal{L}_{p}(X,Y;\theta)\\
    % = & (1-Z_{t})\mathcal{L}_{\text{BCE}}(X,Y;\theta) + Z_{t} \Bigg[(1-\frac{\beta}{2})\mathcal{L}_{\text{BCE}}(X,Y;\theta) + \frac{\beta}{2}\mathcal{L}_{\text{BCE}}(X,1-Y;\theta)\Bigg] \\
    % = & (1 - \beta Z_{t})\mathcal{L}_{\text{BCE}}(X,Y;\theta) + \frac{\beta Z_{t}}{2}\big[\mathcal{L}_{\text{BCE}}(X,1-Y;\theta) + \mathcal{L}_{\text{BCE}}(X,Y;\theta)\big]
    \mathcal{L}_{\text{SC-BCE}}(\theta, X,Y,\alpha, \beta) = & (1-Z_{t}) \mathcal{L}_{\text{BCE}}(\theta, X, Y) + Z_{t} \mathcal{L}_{\text{BCE}}(\theta, X, P(Y, \beta))\\
    = & (1-Z_{t})\mathcal{L}_{\text{BCE}}(\theta, X,Y) + Z_{t} \Bigg[(1-\frac{\beta}{2})\mathcal{L}_{\text{BCE}}(\theta,X,Y) + \frac{\beta}{2}\mathcal{L}_{\text{BCE}}(\theta,X,P(Y, 2))\Bigg] \\
    = & (1 - \beta Z_{t})\mathcal{L}_{\text{BCE}}(\theta, X,Y) + \frac{\beta Z_{t}}{2}\big[\mathcal{L}_{\text{BCE}}(\theta, X, P(Y, 2)) + \mathcal{L}_{\text{BCE}}(\theta,X,Y)\big]
\end{aligned}
\label{A_eq:Expanded_SC-BCE_Loss}
\end{equation}
where the first term includes a regular BCE loss $\mathcal{L}_{\text{BCE}}(\theta,X,Y)$ with random weight $1 - \beta Z_{t}$ and $P(Y, 2)$ represents an inverted label. Aside from the coefficient $Z\beta/2$, the second term can be expanded as a simpler form without label $Y$ by collecting the Y terms:

\begin{equation}
% \label{Aeq:cy-max-H1}
\begin{aligned}
     %\mathcal{L}_{\text{BCE}}(X,1-Y;\theta) + \mathcal{L}_{\text{BCE}}(X,Y;\theta) = & - \Big[(1 - Y)\log f_{\theta}(X) + Y\log (1 - f_{\theta}(X))\Big] - \\
      %& \Big[Y\log f_{\theta}(X) + (1 - Y)\log (1 - f_{\theta}(X))\Big] \\
    %  = & - \Big[\log f_{\theta}(X) + \log (1 - f_{\theta}(X))\Big] \\
    %  = & - 2 \left[\frac{1}{2}\log f_{\theta}(X) + \frac{1}{2}\log (1 - f_{\theta}(X))\right] \\
    %  = & 2 \mathcal{H}(1/2, f_{\theta}(X)) \\
    \mathcal{L}_{\text{BCE}}(\theta,X,P(Y,2)) + \mathcal{L}_{\text{BCE}}(\theta,X,Y) = & - \mathbb{E}_{x,y \in X,Y}\Big[(1 - y)\log f_{\theta}(x) + y\log (1 - f_{\theta}(x))\Big]\\
    & - \mathbb{E}_{x,y \in X, Y}\Big[y\log f_{\theta}(x) + (1 - y)\log (1 - f_{\theta}(x))\Big] \\
    = & - \mathbb{E}_{x \in X} \Big[\log f_{\theta}(x) + \log (1 - f_{\theta}(x))\Big] \\
    = & 2 \cdot \mathbb{E}_{x \in X} \left[ - \frac{1}{2}\log f_{\theta}(X) - \frac{1}{2}\log (1 - f_{\theta}(X))\right] \\
    = & 2 \cdot \mathcal{L}_{\text{BCE}}(\theta, X, U)\\
\end{aligned}
\label{A_eq:Expanded_Second_Term_of_SC-BCE_Loss}
\end{equation}
where $U$ is a uniform binary categorical distribution.
% where $\mathcal{H}(p,q) = -p \cdot \log q - (1 - p) \cdot \log (1 - q)$ denotes the Cross Entropy between two binary categorical distribution $[p, 1-p]$ and $[q, 1-q]$.
% \NB{Maybe just connect the final equation with a back sub so that you have exactly (4) here.}
% \NB{But as per my modification to Eqn 2, going back to the Jaynes paper, entropy is the expecation of the value function. I think you are missing steps here related to the expectation.}
Substituting Eq.~\eqref{A_eq:Expanded_Second_Term_of_SC-BCE_Loss} into Eq.~\ref{A_eq:Expanded_SC-BCE_Loss} yields:
\begin{equation}
    \mathcal{L}_{\text{SC-BCE}}(\theta,X,Y,\alpha, \beta) = (1 - \beta Z_{t}) \cdot \mathcal{L}_{\text{BCE}}(\theta,X,Y) + \beta Z_{t} \cdot \mathcal{L}_{\text{BCE}}(\theta, X, U)
\end{equation}




\subsection{Derivation of Grad-$\alpha$}
\label{A_sec:Grad_Alpha_Derivation}
We start with the SC-BCE loss with sample-wise Bernoulli variable on a finite training dataset $\mathcal{D}_{\text{TR}} = \{x_{i}, y_{i}\}_{i=1}^{N}$ as:
% \begin{equation}
%     \mathcal{L}_{\text{SC-BCE}}(\theta, X, Y, \alpha, \beta) = (1 - \beta  Z_{t}) \cdot \mathcal{L}_{\text{BCE}}(\theta, X, Y) + \beta Z_{t} \cdot \mathcal{H}(U(X), f_{\theta}(X)).
% \label{eq:overall_training_loss_definition}
% \end{equation}
\begin{equation}
    % \mathcal{L}_{\text{SC-BCE}}(\theta, X, Y, \alpha, \beta) = \sum_{i=1}^{N} (1 - \beta  Z_{t}(x_{i}, y_{i})) \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \beta Z_{t}(x_{i}, y_{i}) \cdot \mathcal{H}(U(x_{i}), f_{\theta}(x_{i})).
    \mathcal{L}_{\text{SC-BCE}}(\theta, X, Y, \alpha, \beta) = \sum_{i=1}^{N} (1 -  Z_{t}(x_{i}, y_{i})) \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + Z_{t}(x_{i}, y_{i}) \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)).
\end{equation}
where the variable is drawn from sample-specific Bernoulli distributions: $Z_{t}(x_{i}, y_{i}) \sim B(1, \alpha_{i}), \, i = 1, \dots, N$. Further, we take expectation over the Bernoulli variable for each individual training sample to recover:
\begin{equation}
\begin{aligned}
    % & \sum_{i=1}^{N} \mathbb{E}_{Z_{t}(x_{i}, y_{i})} \Bigl[ (1 - \beta  Z_{t}(x_{i}, y_{i})) \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \beta Z_{t}(x_{i}, y_{i}) \cdot \mathcal{H}(U(x_{i}), f_{\theta}(x_{i})) \Bigr]\\
    % =& \sum_{i=1}^{N} (1 - \alpha_{i} \beta) \cdot  \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \alpha_{i} \beta \cdot \mathcal{H}(U(x_{i}), f_{\theta}(x_{i}))
    & \sum_{i=1}^{N} \mathbb{E}_{Z_{t}(x_{i}, y_{i})} \Bigl[ (1 - Z_{t}(x_{i}, y_{i})) \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + Z_{t}(x_{i}, y_{i}) \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)) \Bigr]\\
    =& \sum_{i=1}^{N} (1 - \alpha_{i}) \cdot  \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \alpha_{i} \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)).
\end{aligned}
\label{A_eq:Expected_SC-BCE}
\end{equation}
We further differentiate the above equation \wrt sample-specific label perturbation probability $\alpha_{i}, \, i = 1, \dots, N$ to obtain:
\begin{equation}
\begin{aligned}
    \frac{\partial \sum_{i=1}^{N} (1 - \alpha_{i}) \cdot  \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \alpha_{i} \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta))}{\partial \alpha_{i}} =& - \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)),\\
    & \text{for} \quad i = 1, \dots, N,
\end{aligned}
\label{eq:unnormalised_Grad_alpha}
\end{equation} 
Performing gradient descent according to this gradient will lead to an optimal value for $\alpha$ with the regularization term.
We find Eq.~\ref{eq:unnormalised_Grad_alpha} (Unnormalised $\nabla_{\alpha_{i}}$) favours perturbation methods with higher perturbation strength $\beta$, leading them to to converge faster. 
This is because label perturbation techniques with higher strengths,  $\beta$, by definition have lower label perturbation probabilities, $\alpha$, overall to achieve optimal model calibration degrees whereas unnormalised Grad-$\alpha$ agnostic to label perturbation strength.
% caused by computing a larger label perturbation probability updating gradient for larger perturbation strength value whereas stronger perturbation techniques usually require smaller label perturbation probabilities to achieve desired model calibration degrees. 
As illustrated in Fig.~\ref{fig:convergence_speed_of_grad_alpha}, with unnormalised Grad-$\alpha$, Hard Inversion (HI) with the largest perturbation strength $\beta = 2$ converges with only 5 epochs of ASLP training whereas it takes Moderation (M) and Dynamic Moderation (DM) with moderate perturbation strength ($\beta = 1$) around 11 epochs to converge. 
% Figure environment removed

We propose a normalised version that allows ASLP under different perturbation strengths $\beta \in (0, 2]$ to converge equally fast. The unnormalised version (Eq.~\ref{eq:unnormalised_Grad_alpha}) is divided by $\beta / 2$ and the normalised $\nabla_{\alpha_{i}}$ is as:
\begin{equation}
    \nabla_{\alpha_{i}} = \frac{2 \cdot \Bigl(\mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta)) \Bigr)}{\beta}, \qquad i = 1, \dots, N
\end{equation}
Fig.~\ref{fig:convergence_speed_of_grad_alpha} illustrates that ASLP with different perturbation strengths with normalised $\nabla_{\alpha_{i}}$ can converge equally fast.



\subsection{Confidence of the Expectation of Stochastically Perturbed Label}
\label{A_sec:confidence_of_ecpectation_of_augmented_label}
We define the expectation of the stochastically perturbed label as:
\begin{equation}
    \mathbb{E}_{Z_{t}} \Bigl[ (1 - Z_{t}) \cdot Y + Z_{t} \cdot P(Y, \beta) \Bigr] = (1 - \alpha \beta) \cdot Y + \frac{\alpha \beta}{2},
\label{eq:expectation_of_stochastically_perturbed_label}
\end{equation}
where we require $\beta \in [0, 2]$ and $\alpha \in [0, \frac{1}{\beta})$. The resultant product is $\alpha \beta \in [0, 1)$. The expected confidence of perturbed label is:
\begin{equation}
\begin{aligned}
    C\Biggl( \mathbb{E}_{Z_{t}} \Bigl[ (1 - Z_{t}) \cdot Y + Z_{t} \cdot P(Y, \beta) \Bigr] \Biggr) =& \Bigl| (1 - \alpha \beta) \cdot Y + \frac{\alpha \beta}{2} - 0.5 \Bigr| + 0.5 \\
    = & 1 - \frac{\alpha \beta}{2}, \; \forall Y = \{0, 1\}
\end{aligned}
\end{equation}


\subsection{Adaptive Label Smoothing (ALS)}
\label{A_sec:Adaptive_Label_Smoothing}
Adaptive Label Smoothing (ASL) applies Label Smoothing with per-image label perturbation strength ($\alpha = 1$ and $\{\beta_{i}\}_{i=1}^{N}$). Similar to the derivation of $\nabla_{\alpha_{i}}$, we differentiate Eq.~\eqref{A_eq:Expected_SC-BCE} \wrt image-specific label perturbation strength as:
\begin{equation}
\begin{aligned}
    % \text{Grad-}\beta = &\frac{\partial \sum_{i=1}^{N} (1 - 1 \cdot \beta_{i}) \cdot  \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + 1 \cdot \beta_{i} \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta_{i}))}{\partial \beta_{i}}\\
    % =& - \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta_{i})), \qquad \text{for} \quad i = 1, \dots, N,
    \nabla_{\beta_{i}} = &\frac{\partial \sum_{i=1}^{N} (1 - 1 \cdot \beta_{i}) \cdot  \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + 1 \cdot \beta_{i} \cdot \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta_{i}))}{\partial \beta_{i}}\\
    =& - \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) + \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta_{i})), \qquad \text{for} \quad i = 1, \dots, N,
\end{aligned}
\label{eq:Unnormalised_Grad_Beta}
\end{equation}
The updating rule ($\text{ALS}_{\text{MC}}$) that incorporates adaptive label smoothing to maximise model calibration is formulated as:
\begin{equation}
\begin{aligned}
    \beta_{i}^{n + 1} 
    = \; & \beta_{i}^{n} + \eta \cdot \biggl( \underbrace{ \mathcal{L}_{\text{BCE}}(\theta, x_{i}, p(y_{i}, \beta_{i})) - \mathcal{L}_{\text{BCE}}(\theta, x_{i}, y_{i}) }_{\nabla_{\beta_{i}}} + \lambda \cdot \underbrace{\min \Bigl( \bigl(\mathrm{1} - \frac{1 \cdot \beta_{i}}{2} \bigr) - \mathbb{A}(\theta_{lm}, \mathcal{D}_{\text{VAL}}), 0\Bigr)}_{\text{Reg}_{\text{C}}} \biggr)\\
    &\text{for} \quad i = 1, \dots, N,
\end{aligned}
\end{equation}


\clearpage




\section{Implementations}
\subsection{Model}
\label{A_sub_sec:Model_Implementation}
Our model adopts a simple U-Net \cite{ronneberger2015u} structure consisting of an encoder and a decoder. Feature maps $\{F_{i} \in i \cdot C \times \frac{H}{i \cdot 8} \times \frac{W}{i \cdot 8}\}_{i=1}^{4}$ are extracted by the encoder, where $C = 256$ and $i$ indexes from low level to high level with an increasing value. 

The model outputs pixel-wise logits $\sigma(x_{i}) \in (- \infty, \infty)^{1 \times H \times W}, \; i = 1, \dots, N$ where $N$ is the total number of samples, which is further processed with a Sigmoid function to produce the prediction probability as:
\begin{equation}
\begin{aligned}
    f_{\theta}(x_{i}) &= \text{Sigmoid} (\sigma(x_{i})) = \frac{1}{1 + e^{- \sigma(x_{i})}},    \quad i = 1, \dots, N.
\end{aligned}
\end{equation}
The prediction probability after the Sigmoid function is in the range $f_{\theta}(x) \in (0, 1)^{1 \times H \times W}$. The predicted label is \enquote{foreground} (Labeled as \enquote{1}) if the prediction probability is larger than 0.5 and is \enquote{background} (labeled as \enquote{0}) otherwise as:
\begin{equation}
    \hat{y}_{i} = \mathbbm{1} (f_{\theta}(x_{i}) > 0.5), \quad i = 1, \dots, N.
\end{equation}
The probability of predicted label $\hat{y}$, also known as the winning class, is:
\begin{equation}
    P_{\hat{y}_{i}} = |f_{\theta}(x_{i}) - 0.5| + 0.5, \quad i = 1, \dots, N.
\end{equation}

\subsection{Evaluation Metrics - Model Calibration Degree}
\label{A_sec:Evaluation_Metrics_Model_Calibration_Degree}
% \subsubsection{Model Calibration Degree}
% We use Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) \cite{guo2017calibration} and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) \cite{thulasidasan2019mixup} to evaluate the model calibration degrees which are reported in the Main Paper. $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ are defined as:
% \begin{equation}
% \begin{aligned}
%     &\text{ECE}_{\text{EW}} = \sum_{i=1}^{M} \frac{|B_{i}|}{\mathcal{|D|}} \cdot | C_{i} - A_{i} |,\\
%     &\text{OE}_{\text{EW}} = \sum_{i=1}^{M} \frac{|B_{i}|}{\mathcal{|D|}} \cdot \mathbbm{1} (C_{i} > A_{i}) \cdot | C_{i} - A_{i} |,
% \end{aligned}
% \end{equation}
% where $M$ is the total number of bins, $B_{i}$ and $\mathcal{D}$ denote the size of the $i^{\text{th}}$ bin and the dataset respectively, $C_{i} = \frac{1}{|B_{i}|} \sum_{j \in B_{i}} P_{\hat{y}_{j}}$ is the mean prediction confidence of the $i^{\text{th}}$ bin, and $A_{i} = \frac{1}{|B_{i}|} \sum_{j \in B_{i}} \mathbbm{1}(\hat{y}_{i} = y_{i})$ is the mean accuracy of the $i^{\text{th}}$ bin. $\text{ECE}_{\text{EW}}$ has fixed-width bins, with the range $\Bigl[ \frac{i}{M}, \frac{i + 1}{M} \Bigr), \; i = 0, \dots, M - 1$ for the $i^{\text{th}}$ bin. 

% Additionally, we adopt $\text{ECE}_{\text{EM}}$ \cite{nguyen-oconnor-2015-posterior}, $\text{ECE}_{\text{DEBIAS}}$ \cite{kumar2019verified} and $\text{ECE}_{\text{SWEEP}}$ \cite{roelofs2022mitigating} to corroborate with the results of $\text{ECE}_{\text{EW}}$. We also adapt Over-confidence Error to the binning schemes of $\text{ECE}_{\text{EM}}$ and $\text{ECE}_{\text{SWEEP}}$ to produce $\text{OE}_{\text{EM}}$ and $\text{OE}_{\text{SWEEP}}$ respectively.

\subsubsection{Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) \cite{guo2017calibration}}
\begin{equation}
    \text{ECE}_{\text{EW}} = \sum_{i=1}^{M} \frac{|B_{i}|}{\mathcal{|D|}} | C_{i} - A_{i} |,
\end{equation}
where $M$ is the total number of bins, $B_{i}$ and $\mathcal{D}$ denote the size of the $i^{\text{th}}$ bin and the dataset respectively, $C_{i} = \frac{1}{|B_{i}|} \sum_{j \in B_{i}} P_{\hat{y}_{j}}$ is the mean prediction confidence of the $i^{\text{th}}$ bin, and $A_{i} = \frac{1}{|B_{i}|} \sum_{j \in B_{i}} \mathbbm{1}(\hat{y}_{i} == y_{i})$ is the mean accuracy of the $i^{\text{th}}$ bin. $\text{ECE}_{\text{EW}}$ has fixed-width bins, with the range $\Bigl[ \frac{i}{M}, \frac{i + 1}{M} \Bigr), \; i = 0, \dots, M - 1$ for the $i^{\text{th}}$ bin.

\subsubsection{Equal-Mass Expected Calibration Error ($\text{ECE}_{\text{EM}}$) \cite{nguyen-oconnor-2015-posterior}}
\begin{equation}
    \text{ECE}_{\text{EW}} = \sum_{i=1}^{M} \frac{|B_{i}|}{\mathcal{|D|}} \cdot | C_{i} - A_{i} |, \qquad \text{where} \; |B_{j}| = |B_{k}|, \forall j, k \in [1, M].
\end{equation}
Equal-Mass Expected Calibration Error ($\text{ECE}_{\text{EM}}$) is different from Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) by constraining all bins to have equal size.

\subsubsection{SWEEP Expected Calibration Error ($\text{ECE}_{\text{SWEEP}}$) \cite{roelofs2022mitigating}}
\begin{equation}
    \text{ECE}_{\text{SWEEP}} = (\sum_{i = 1}^{b^{*}} \frac{|B_{i}|}{\mathcal{|D|}} | C_{i} - A_{i} \cdot |^{p})^{\frac{1}{p}}, \qquad \text{where} \; b^{*} = \max(b | 1 \leq b \leq n, \forall b' \leq b^{*}, A_{1} \leq \dots \leq A_{b'})
\end{equation}
$p$ is a hyperparameter that is set to $p = 1$ and $n$ is the largest bin number to be tested which we set to $n = 100$. $\text{ECE}_{\text{SWEEP}}$ follows $\text{ECE}_{\text{EM}}$ to constrain equal-size bins. $\text{ECE}_{\text{SWEEP}}$ starts with bin number $B = 1$ and keeps increasing the bin number until monotony in bin accuracy breaks.


\subsubsection{DEBIAS Expected Calibration Error ($\text{ECE}_{\text{DEBIAS}}$) \cite{kumar2019verified}}
\begin{equation}
    \text{ECE}_{\text{DEBIAS}} = \sum_{i=1}^{M} \frac{|B_{i}|}{\mathcal{|D|}} \Bigl[ (C_{i} - A_{i})^{2} - \frac{A_{i} \cdot (1 - A_{i})}{|B_{i}| - 1} \Bigr]
\end{equation}
DEBIAS Expected Calibration Error ($\text{ECE}_{\text{DEBIAS}}$) adopts equal-width bins.

\subsubsection{Over-confidence Error (OE)}
\begin{equation}
    \text{OE} = \sum_{i=1}^{M} \frac{|B_{i}|}{|\mathcal{D}|} \cdot \mathbbm{1}(C_{i} > A_{i}) \cdot | C_{i} - A_{i} |,
\end{equation}
We adapt OE to different binning schemes of $\text{ECE}_{\text{EW}}$, $\text{ECE}_{\text{EM}}$, $\text{ECE}_{\text{SWEEP}}$ to produce $\text{OE}_{\text{EW}}$, $\text{OE}_{\text{EM}}$, $\text{OE}_{\text{SWEEP}}$ respectively.


% \subsection{Evaluation Metrics - Dense Classification}
\subsection{Evaluation Metrics - Dense Classification}
\subsubsection{Prediction Accuracy}
The model prediction accuracy is computed as:
\begin{equation}
    % \mathbb{A}(\theta, \mathcal{D}_\text{VAL}) = \frac{1}{N_{\text{VAL}}} \sum_{i = 1}^{N_{\text{VAL}}} \mathbbm{1} (\hat{y}_{i} = y_{i}),
    \mathbb{A}(\theta, \mathcal{D}) = \frac{1}{N \times H \times W} \sum_{i = 1}^{N} \sum_{j = 1}^{H} \sum_{k = 1}^{W} \mathbbm{1} (\hat{y}_{i}^{j,k} = y_{i}^{j,k}),
\end{equation}
% where $\mathcal{D}_{\text{VAL}} = \{x_{i}, y_{i}\}_{i=1}^{N_{\text{VAL}}}$ denotes the validation set the $N_{\text{VAL}}$ is the size of validation set. 
where $\mathcal{D} = \{x_{i}, y_{i}\}_{i=1}^{N}$ denotes the dataset with $N$ samples, $H$ and $W$ is the height and the width of sample respectively. 

% Similarly the prediction accuracy on Out-of-Distribution dataset is computed with $\mathbb{A}(\theta, \mathcal{D}_{\text{OoD}})$, where we use texture images from Describable Texture Dataset \cite{cimpoi2014describing} as OoD samples.

\subsubsection{F-measure}
F-measure is computed as:
\begin{equation}
    F_{\xi} = \frac{(1 + \xi^{2}) \times \text{Precision} \times \text{Recall}}{\xi^{2} \times \text{Precision} + \text{Recall}},
\label{A_eq:F_measure}
\end{equation}
where $\xi$ is a hyperparameter. We follow previous works \cite{wu2022edn,liu2018picanet,zhang2017amulet,Liu21PamiPoolNet} to set $\xi^{2} = 0.3$. We report the maximum F-measure which selects the best results computed with various binarising threshold.

\subsubsection{E-measure}
Enhanced-alignment measure (E-measure) \cite{fan2018enhanced} is computed as:
\begin{equation}
\begin{aligned}
    Q_{FM} &= \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \phi_{FM}(i, j), \quad \text{where}\\
    \phi_{FM} &= f(\xi_{FM}) = \frac{1}{4}(1 + \xi_{FM})^{2},\\
    \xi_{FM} &= \frac{2 \cdot \varphi_{GT} \circ \varphi_{FM}}{\varphi_{GT} \circ \varphi_{GT} + \varphi_{FM} \circ \varphi_{FM}},\\
    \varphi_{I} &= I - \mu_{i} \cdot A,
\end{aligned}
\end{equation}
where $I \in (0, 1)$ is a dense binary prediction map with mean value $\mu_{I}$, $A$ is an one matrix whose dimension matches that of $I$, $\varphi_{GT}$ and $\varphi_{FM}$ denote groundtruth map and model prediction respectively, $H$ and $W$ is image height and width. Maximum E-measure replaces the mean value with a range of binarising thresholds and report the highest result.


% We use F-measure and E-measure \cite{fan2018enhanced} to evaluate the dense binary classification performances. These results are reported in Appendix.~\ref{A_tab:Generalisation_to_SOD_Methods_Classification_Results} to prove our proposed methods improve the model calibration degrees without negatively impacting the dense binary classification accuracies.


\subsection{Datasets}
\noindent\textbf{DUTS-TR \cite{DUTS-TE}:} is commonly used training dataset for Salient Object Detection task. It consists of 10,553 pairs of image and pixel-wise annotations. We take a subset consisting 1,000 training samples as a validation set and uses the remaining 9,553 samples for training.

\noindent\textbf{DUTS-TE \cite{DUTS-TE}:} is a testing dataset consisting of 5,019 images. Both DUTS-TE and DUTS-TR belong to the DUTS dataset.

\noindent\textbf{DUT-OMRON \cite{DUT-OMRON}:} consists of 5,168 testing images, each of which includes at least one structurally complex foreground object(s).

\noindent\textbf{PASCAL-S \cite{PASCAL-S}:} contains 850 testing samples that are obtained from PASCAL-VOC dataset, which is designed for semantic segmentation task.

\noindent\textbf{SOD \cite{SOD}:} includes 300 testing images of a wide variety of natural scenes.

\noindent\textbf{ECSSD \cite{ECSSD}: } has 1,000 semantically meaningful images for testing.

\noindent\textbf{HKU-IS \cite{HKU-IS}:} is comprised of 4,447 testing images, each having multiple foreground objects.

\noindent\textbf{Describable Texture Dataset (DTD) \cite{cimpoi2014describing}:} contains 5,640 real-world texture images. These images are grouped into 47 categories described by adjectives such as \enquote{\textit{grooved}}, \enquote{\textit{woven}}, \enquote{\textit{matted}}. Some texture images have a distinct region that could be considered to be salient. We selectively choose only 500 texture images that have no obvious salient object 
% They are from the following categories: \enquote{\textit{blotchy}}, \enquote{\textit{braided}}, \enquote{\textit{bumpy}}, 
% \enquote{\textit{chequered}}, \enquote{\textit{cracked}}, \enquote{\textit{crosshatched}}, \enquote{\textit{fibrous}}, \enquote{\textit{flecked}}, \enquote{\textit{grooved}}, \enquote{\textit{knitted}}, \enquote{\textit{marbled}}, \enquote{\textit{matted}}, \enquote{\textit{pitted}}, \enquote{\textit{porous}}, \enquote{\textit{smeared}}, \enquote{\textit{woven}}, \enquote{\textit{wrinkled}}, 
and show some examples in Fig.~\ref{fig:DTD_Samples}. We consider the selected texture images an Out-of-Distribution samples for salient object detection. The complete collection of the 500 selected texture images are presented in Fig.~\ref{A_fig:Full_Texture_Image_Collection} at the end of the Appendix.

% Figure environment removed


\clearpage

% \section{TEXTURE100 Dataset}
% \label{A_sec:TEXTURE100_Samples}
% Fig.~\ref{fig:TEXTURE100_Samples} presents some texture images from the collected TEXTURE100 dataset.
% % Figure environment removed

% \clearpage



\section{Model Calibration Benchmark with $\text{ECE}_{\text{EM}}$, $\text{ECE}_{\text{SWEEP}}$ and $\text{ECE}_{\text{DEBIAS}}$}
\label{A_sec:model_calibration_benchmark_with_ECE_EM_SWEEP_DEBIAS}
We present the model calibration degrees of existing SOD methods, model calibration methods and our proposed methods evaluated in terms of: (i) Equal-Mass Expected Calibration Error $\text{ECE}_{\text{EM}}$ and Equal-Mass Over-confidence Error $\text{OE}_{\text{EM}}$ in Tab.~\ref{A_tab:ECE_EM_Benchmark}, (ii) $\text{ECE}_{\text{SWEEP}}$ and $\text{OE}_{\text{EM}}$ in Tab.~\ref{A_tab:ECE_SWEEP_Benchmark}, and (iii) $\text{ECE}_{\text{DEBIAS}}$ in Tab.~\ref{A_tab:ECE_DEBIAS_Benchmark}. Our proposed method, $\text{ASLP}_{\text{MC}}$, still outperforms existing salient object detection and model calibration methods with these model calibration evaluation metrics.

\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.0mm}
\caption{Salient object detection model calibration degree benchmark evaluated with $\text{ECE}_{\text{EM}}$ (\%) and $\text{OE}_{\text{EM}}$ (\%). We set the number of bins to $B = 10$. (values are shown in \% and {\color{red} red} and {\color{blue} blue} indicate the best and the second-best performance respectively.)}
\begin{tabular}{cl|c|cc|cc|cc|cc|cc|cc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}} & {\multirow{2}{*}{Year}} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& & & $\text{ECE}_{\text{EM}} \downarrow$ & $\text{OE}_{\text{EM}} \downarrow$ & $\text{ECE}_{\text{EM}} \downarrow$ & $\text{OE}_{\text{EM}} \downarrow$ & $\text{ECE}_{\text{EM}} \downarrow$ & $\text{OE}_{\text{EM}} \downarrow$ & $\text{ECE}_{\text{EM}} \downarrow$ & $\text{OE}_{\text{EM}} \downarrow$ & $\text{ECE}_{\text{EM}} \downarrow$ & $\text{OE}_{\text{EM}} \downarrow$ & $\text{ECE}_{\text{EM}} \downarrow$ & $\text{OE}_{\text{EM}} \downarrow$ \\
\midrule
\multirow{18}{*}{\parbox{1.0cm}{SOD \\ Methods}} 
& MSRNet \cite{li2017instance} & 2017 & 3.35 & 3.03 & 3.64 & 3.40 & 4.23 & 3.93 & 5.52 & 5.13 & 1.12 & 1.08 & 1.05 & 0.96\\
& SRM \cite{Wang_2017_ICCV} & 2017 & 4.45 & 4.05 & 4.10 & 3.78 & 4.92 & 4.53 & 7.69 & 7.22 & 2.81 & 2.57 & 2.20 & 2.00\\
& Amulet \cite{zhang2017amulet} & 2017 & 5.63 & 5.10 & 5.46 & 4.98 & 5.69 & 5.23 & 8.24 & 7.63 & 2.64 & 2.45 & 2.09 & 1.94\\
& BMPM \cite{zhang2018bi} & 2018 & 3.47 & 3.21 & 4.52 & 4.18 & 4.77 & 4.57 & 8.00 & 7.88 & 1.89 & 1.83 & 1.55 & 1.50 \\
& DGRL \cite{wang2018detect} & 2018 & 4.42 & 4.04 & 3.87 & 3.57 & 4.91 & 4.57 & 5.69 & 5.35 & 2.23 & 2.07 & 1.69 & 1.53\\
& PAGR \cite{zhang2018progressive} & 2018 & 4.00 & 3.63 & 3.28 & 3.00 & 5.06 & 4.67 & 7.60 & 7.14 & 2.49 & 2.29 & 1.40 & 1.25\\
& PiCANet \cite{liu2018picanet} & 2018 & 5.37 & 4.98 & 5.17 & 4.82 & 5.78 & 5.41 & 8.75 & 8.33 & 2.67 & 2.44 & 2.31 & 2.09\\
& CPD \cite{wu2019cascaded} & 2019 & 3.10 & 2.90 & 3.62 & 3.42 & 4.11 & 3.86 & 6.76 & 6.42 & 2.07 & 1.94 & 1.81 & 1.70\\
& BASNet \cite{qin2019basnet} & 2019 & 6.07 & 5.85 & 6.15 & 5.96 & 5.72 & 5.48 & 5.07 & 4.88 & 2.12 & 2.04 & 2.36 & 2.28\\
& EGNet \cite{zhao2019egnet} & 2019 & 3.54 & 3.29 & 3.55 & 3.33 & 4.92 & 4.61 & 6.42 & 6.07 & 1.96 & 1.84 & 1.64 & 1.55\\
& AFNet \cite{feng2019attentive} & 2019 & 3.58 & 3.33 & 3.02 & 2.81 & 4.08 & 3.79 & 6.65 & 6.14 & 2.19 & 2.04 & 1.78 & 1.66\\
& PoolNet \cite{Liu21PamiPoolNet} & 2019 & 3.80 & 3.52 & 3.53 & 3.30  & 5.44 & 5.09 & 6.87 & 6.49 & 2.18 & 2.04 & 1.61 & 1.52\\
& GCPANet \cite{chen2020global} & 2020 & 4.40 & 4.12 & 4.84 & 4.61 & 4.92 & 4.64 & 4.20 & 3.94 & 1.87 & 1.76 & 1.54 & 1.47\\
& MINet \cite{pang2020multi} & 2020 & 5.02 & 4.76 & 5.40 & 5.13 & 6.17 & 5.86 & 8.29 & 8.01 & 2.84 & 2.67 & 2.31 & 2.17 \\
& $\text{F}^{3}\text{Met}$ \cite{wei2020f3net} & 2020 & 3.47 & 3.26 & 3.88 & 3.68 & 4.56 & 4.32 & 7.34 & 6.95 & 2.45 & 2.31 & 1.91 & 1.80 \\
& EBMGSOD \cite{jing_ebm_sod21} & 2021 & 3.64 & 3.41 & 3.78 & 3.55 & 4.79 & 4.52 & 5.83 & 5.56 & 2.30 & 2.15 & 1.85 & 1.72\\
& ICON \cite{zhuge2021salient} & 2021 & 2.40 & 2.26 & 2.95 & 2.81 & 3.45 & 3.29 & 4.27 & 4.09 & 1.34 & 1.25 & 1.23 & 1.16\\
& PFSNet \cite{ma2021pyramidal} & 2021 & 3.07 & 2.84 & 3.44 & 3.16 & 4.99 & 4.64 & 5.82 & 5.48 & 2.43 & 2.17 & 2.87 & 2.70\\
& EDN \cite{wu2022edn} & 2022 & 3.89 & 3.68 & 4.35 & 4.18 & 4.62 & 4.41 & 4.02 & 3.85 & 1.60 & 1.52 & 1.34 & 1.26\\
\hline
\multirow{7}{*}{\parbox{1.0cm}{Model \\ Calibration \\ Methods}}
& Brier Loss \cite{brier1950verification} & 1950 & 2.78 & 2.61 & 3.55 & 3.40 & 3.90 & 3.72 & 6.40 & 6.18 & 1.34 & 1.31 & 1.04 & 1.00 \\
& TS \cite{guo2017calibration} & 2017 & 2.77 & 2.60 & 3.44 & 3.30 & 3.85 & 3.67 & 6.64 & 6.40 & 1.21 & 1.17 & 0.95 & 0.91\\
& MMCE \cite{MMCE} & 2018 & 2.86 & 2.69 & 3.56 & 3.42 & 4.07 & 3.89 & 6.85 & 6.63 & \color{blue}{1.41} & 1.35 & 1.18 & 1.13\\
& LS \cite{muller2019does} & 2019 & 2.74 & 2.10 & 3.51 & 2.81 & 3.97 & 3.35 & 4.50 & 4.10 & 1.50 & 0.99 & 1.44 & 0.84\\
& Mixup \cite{thulasidasan2019mixup} & 2019 & 3.00 & 2.73 & 3.40 & 3.13 & \color{blue}{2.14} & 0.59 & 4.94 & 4.62 & 1.86 & 0.45 & 4.94 & 0.20\\
& Focal Loss \cite{focal_loss} & 2020 & 2.15 & 2.03 & 2.69 & 2.38 & 2.95 & 2.70 & 4.61 & 4.38 & 1.57 & 1.16 & 1.29 & 0.87\\
& AdaFocal \cite{ghosh2022adafocal} & 2022 & \color{blue}{1.74} & 1.50 & \color{blue}{1.96} & 1.45 & 2.45 & 2.02 & \color{blue}{3.88} & 3.09 & 1.79 & 0.74 & 1.45 & 0.44\\
\hline
\multirow{2}{*}{\parbox{1.0cm}{Our \\ Methods}}
& $\text{ASLP}_{\text{ECE}}$  & 2023 & \color{red}{\textbf{1.53}} & \color{blue}{1.41} & \color{red}{\textbf{1.72}} & \color{blue}{1.43} & \color{red}{\textbf{1.58}} & \color{blue}{1.55} & \color{red}{\textbf{2.30}} & \color{blue}{1.66} & \color{red}{\textbf{0.71}} & \color{blue}{0.35} & \color{red}{\textbf{0.84}} & \color{blue}{0.19}\\
& $\text{ASLP}_{\text{MEI}}$ & 2023 & 21.00 & \color{red}{\textbf{0.08}} & 20.24 & \color{red}{\textbf{0.00}} & 19.89 & \color{red}{\textbf{0.00}} & 18.14 & \color{red}{\textbf{0.00}} & 22.15 & \color{red}{\textbf{0.00}} & 22.58 & \color{red}{\textbf{0.00}}\\
\bottomrule
\end{tabular}
\label{A_tab:ECE_EM_Benchmark}
\end{table}


\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.0mm}
\caption{Salient object detection model calibration degree benchmark evaluated with $\text{ECE}_{\text{SWEEP}}$ (\%) and $\text{OE}_{\text{SWEEP}}$ (\%). The number of bins for each evaluation is selected to ensure a monotonically increasing accuracy in the bins \cite{roelofs2022mitigating} (values are shown in \% and {\color{red} red} and {\color{blue} blue} indicate the best and the second-best performance respectively.)}
\begin{tabular}{cl|c|cc|cc|cc|cc|cc|cc}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}} & {\multirow{2}{*}{Year}} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& & & $\text{ECE}_{\text{SW}} \downarrow$ & $\text{OE}_{\text{SW}} \downarrow$ & $\text{ECE}_{\text{SW}} \downarrow$ & $\text{OE}_{\text{SW}} \downarrow$ & $\text{ECE}_{\text{SW}} \downarrow$ & $\text{OE}_{\text{SW}} \downarrow$ & $\text{ECE}_{\text{SW}} \downarrow$ & $\text{OE}_{\text{SW}} \downarrow$ & $\text{ECE}_{\text{SW}} \downarrow$ & $\text{OE}_{\text{SW}} \downarrow$ & $\text{ECE}_{\text{SW}} \downarrow$ & $\text{OE}_{\text{SW}} \downarrow$ \\
\midrule
\multirow{18}{*}{\parbox{1.0cm}{SOD \\ Methods}} 
& MSRNet \cite{li2017instance} & 2017 & 3.16 & 2.85 & 4.10 & 3.86 & 4.09 & 3.85 & 5.30 & 5.05 & 1.04 & 1.00 & 1.01 & 0.94\\
& SRM \cite{Wang_2017_ICCV} & 2017 & 4.66 & 4.32 & 4.92 & 4.61 & 5.77 & 5.43 & 8.04 & 7.56 & 2.98 & 2.74 & 2.12 & 1.95\\
& Amulet \cite{zhang2017amulet} & 2017 & 6.52 & 6.04 & 7.31 & 6.85 & 6.50 & 6.08 & 8.47 & 7.88 & 2.17 & 2.06 & 2.47 & 2.32\\
& BMPM \cite{zhang2018bi} & 2018 & 4.77 & 4.38 & 4.27 & 3.98 & 6.13 & 5.74 & 8.74 & 8.31 & 2.09 & 1.72 & 2.03 & 1.85 \\
& DGRL \cite{wang2018detect} & 2018 & 4.51 & 4.30 & 3.98 & 3.81 & 4.61 & 4.46 & 5.23 & 4.89 & 1.98 & 1.84 & 1.88 & 1.73\\
& PAGR \cite{zhang2018progressive} & 2018 & 4.40 & 4.07 & 5.20 & 5.26 & 5.71 & 5.44 & 12.07 & 11.45 & 2.80 & 2.62 & 1.58 & 1.50\\
& PiCANet \cite{liu2018picanet} & 2018 & 4.81 & 4.52 & 4.17 & 3.86 & 5.34 & 4.91 & 7.71 & -7.27 & 2.75 & 2.46 & 2.08 & 1.89\\
& CPD \cite{wu2019cascaded} & 2019 & 4.00 & 3.80 & 4.45 & 4.33 & 4.76 & 4.58 & 6.98 & 6.65 & 2.29 & 2.16 & 2.26 & 2.15\\
& BASNet \cite{qin2019basnet} & 2019 & 7.17 & 6.94 & 7.10 & 6.91 & 7.70 & 7.48 & 7.84 & 7.74 & 2.14 & 2.11 & 2.59 & 2.51\\
& EGNet \cite{zhao2019egnet} & 2019 & 3.91 & 3.68 & 4.29 & 4.08 & 4.75 & 4.55 & 5.89 & 5.56 & 1.84 & 1.71 & 1.29 & 1.23\\
& AFNet \cite{feng2019attentive} & 2019 & 4.31 & 4.06 & 4.48 & 4.27 & 4.56 & 4.49 & 6.79 & 6.24 & 2.21 & 2.06 & 2.06 & 1.95\\
& PoolNet \cite{Liu21PamiPoolNet} & 2019 & 3.58 & 3.36 & 4.30 & 4.10 & 6.09 & 5.75 & 6.72 & 5.75 & 1.98 & 1.85 & 1.53 & 1.45\\
& GCPANet \cite{chen2020global} & 2020 & 4.45 & 4.18 & 5.26 & 5.04 & 5.01 & 4.75 & 5.74 & 5.60 & 1.63 & 1.52 & 1.58 & 1.51\\
& MINet \cite{pang2020multi} & 2020 & 4.97 & 4.69 & 6.03 & 5.77 & 6.97 & 6.67 & 8.17 & 7.97 & 1.99 & 1.93 & 1.48 & 1.45 \\
& $\text{F}^{3}\text{Met}$ \cite{wei2020f3net} & 2020 & 3.29 & 3.15 & 4.56 & 4.36 & 4.26 & 4.10 & 7.74 & 7.29 & 2.20 & 2.08 & 2.29 & 2.17 \\
& EBMGSOD \cite{jing_ebm_sod21} & 2021 & 4.32 & 4.10 & 5.03 & 4.81 & 4.40 & 4.29 & 5.46 & 5.18 & 2.53 & 2.39 & 2.30 & 2.17\\
& ICON \cite{zhuge2021salient} & 2021 & 2.64 & 2.54 & 4.16 & 4.02 & 3.93 & 3.90 & 5.13 & 5.01 & 1.32 & 1.24 & 1.20 & 1.14\\
& PFSNet \cite{ma2021pyramidal} & 2021 & 4.89 & 4.79 & 5.89 & 5.61 & 7.73 & 7.54 & 10.74 & 10.45 & 2.31 & 2.28 & 2.21 & 2.19\\
& EDN \cite{wu2022edn} & 2022 & 4.28 & 4.07 & 4.78 & 4.60 & 5.10 & 4.92 & 5.63 & 5.55 & 1.48 & 1.45 & 1.54 & 1.45\\
\hline
\multirow{7}{*}{\parbox{1.0cm}{Model \\ Calibration \\ Methods}} 
& Brier Loss \cite{brier1950verification} & 1950 & 3.43 & 3.17 & 4.39 & 4.15 & 4.44 & 4.22 & 5.03 & 4.22 & 1.48 & 1.38 & 1.21 & 1.15 \\
& TS \cite{guo2017calibration} & 2017 & 3.30 & 3.03 & 4.12 & 3.91 & 3.48 & 3.30 & 5.33 & 4.97 & 1.29 & 1.22 & 1.13 & 1.08\\
& MMCE \cite{MMCE} & 2018 & 3.44 & 3.20 & 4.38 & 4.17 & 3.66 & 3.48 & 5.55 & 5.19 & 1.40 & 1.31 & 1.36 & 1.29\\
& LS \cite{muller2019does} & 2019 & 2.97 & 2.92 & 3.88 & 3.81 & 4.08 & 4.99 & 5.67 & 5.42 & 1.46 & 1.27 & 1.32 & 0.99\\
& Mixup \cite{thulasidasan2019mixup} & 2019 & 3.01 & 2.76 & 4.47 & 4.21 & \color{blue}{1.84} & \color{blue}{1.26} & 5.26 & 4.99 & 1.28 & 1.11 & 1.73 & 1.48\\
& Focal Loss \cite{focal_loss} & 2020 & 2.23 & 2.14 & 3.73 & 3.43 & 3.03 & 2.93 & 4.77 & 4.59 & 1.30 & 1.16 & 1.40 & 1.08\\
& AdaFocal \cite{ghosh2022adafocal} & 2022 & \color{blue}{1.79} & 1.60 & \color{blue}{2.44} & 2.08 & 1.88 & 1.78 & \color{blue}{4.16} & 3.46 & \color{blue}{1.16} & 0.97 & \color{blue}{1.03} & 0.86\\
\hline
\multirow{2}{*}{\parbox{1.0cm}{Our \\ Methods}}
& $\text{ASLP}_{\text{ECE}}$  & 2023 & \color{red}{\textbf{1.37}} & \color{blue}{1.21} & \color{red}{\textbf{1.67}} & \color{blue}{1.33} & \color{red}{\textbf{1.77}} & 1.51 & \color{red}{\textbf{2.73}} & \color{blue}{2.41} & \color{red}{\textbf{0.97}} & \color{blue}{0.61} & \color{red}{\textbf{0.89}} & \color{blue}{0.41}\\
& $\text{ASLP}_{\text{MEI}}$ & 2023 & 20.78 & \color{red}{\textbf{0.00}} & 19.64 & \color{red}{\textbf{0.00}} & 19.74 & \color{red}{\textbf{0.00}} & 17.35 & \color{red}{\textbf{0.00}} & 22.47 & \color{red}{\textbf{0.00}} & 22.90 & \color{red}{\textbf{0.00}}\\
\bottomrule
\end{tabular}
\label{A_tab:ECE_SWEEP_Benchmark}
\end{table}




\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.8mm}
\caption{Salient object detection model calibration degree benchmark evaluated with $\text{ECE}_{\text{DEBIAS}}$ \cite{kumar2019verified}. We set he number of bins to $B = 10$. (values are shown in \% and {\color{red} red} and {\color{blue} blue} indicate the best and the second-best performance respectively.)}
\begin{tabular}{cl|c|c|c|c|c|c|c}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{Methods}} & {\multirow{2}{*}{Year}} & 
\multicolumn{6}{c}{$\text{ECE}_{\text{DEBIAS}} (\%) \downarrow$}\\
& & & DUTS-TE \cite{DUTS-TE} & DUT-OMRON \cite{DUT-OMRON} & PASCAL-S \cite{PASCAL-S} & SOD \cite{SOD} & ECSSD \cite{ECSSD} & HKU-IS \cite{HKU-IS}\\
\midrule
\multirow{18}{*}{\parbox{1.0cm}{SOD \\ Methods}} 
& MSRNet \cite{li2017instance} & 2017 & 0.167 & 0.188 & 0.235 & 0.524 & 0.020 & 0.015\\
& SRM \cite{Wang_2017_ICCV} & 2017 & 0.419 & 0.358 & 0.436 & 1.221 & 0.186 & 0.110\\
& Amulet \cite{zhang2017amulet} & 2017 & 0.553 & 0.536 & 0.508 & 1.165 & 0.235 & 0.079\\
& BMPM \cite{zhang2018bi} & 2018 & 0.471 & 0.378 & 0.440 & 1.175 & 0.191 & 0.134 \\
& DGRL \cite{wang2018detect} & 2018 & 0.420 & 0.370 & 0.430 & 0.807 & 0.096 & 0.072\\
& PAGR \cite{zhang2018progressive} & 2018 & 0.340 & 0.418 & 0.470 & 1.568 & 0.137 & 0.053\\
& PiCANet \cite{liu2018picanet} & 2018 & 0.456 & 0.359 & 0.461 & 0.985 & 0.175 & 0.124\\
& CPD \cite{wu2019cascaded} & 2019 & 0.390 & 0.353 & 0.567 & 1.233 & 0.145 & 0.109\\
& BASNet \cite{qin2019basnet} & 2019 & 0.544 & 0.536 & 0.683 & 1.190 & 0.138 & 0.127\\
& EGNet \cite{zhao2019egnet} & 2019 & 0.318 & 0.304 & 0.576 & 0.860 & 0.109 & 0.066\\
& AFNet \cite{feng2019attentive} & 2019 & 0.381 & 0.348 & 0.471 & 0.934 & 0.132 & 0.091\\
& PoolNet \cite{Liu21PamiPoolNet} & 2019 & 0.335 & 0.326 & 0.612 & 0.907 & 0.107 & 0.055\\
& GCPANet \cite{chen2020global} & 2020 & 0.388 & 0.318 & 0.372 & 0.569 & 0.068 & 0.043\\
& MINet \cite{pang2020multi} & 2020 & 0.448 & 0.505 & 0.606 & 1.041 & 0.172 & 0.142\\
& $\text{F}^{3}\text{Met}$ \cite{wei2020f3net} & 2020 & 0.457 & 0.468 & 0.556 & 0.816 & 0.193 & 0.167\\
& EBMGSOD \cite{jing_ebm_sod21} & 2021 & 0.374 & 0.406 & 0.508 & 0.733 & 0.154 & 0.130\\
& ICON \cite{zhuge2021salient} & 2021 & 0.306 & 0.390 & 0.382 & 0.607 & 0.098 & 0.101\\
& PFSNet \cite{ma2021pyramidal} & 2021 & 0.323 & 0.339 & 0.539 & 0.594 & 0.588 & 0.435\\
& EDN \cite{wu2022edn} & 2022 & 0.285 & 0.281 & 0.407 & 0.745 & 0.068 & 0.061\\
\hline
\multirow{7}{*}{\parbox{1.0cm}{Model \\ Calibration \\ Methods}} 
& Brier Loss \cite{brier1950verification} & 1950 & 0.241 & 0.265 & 0.330 & 0.572 & 0.051 & 0.035\\
& TS \cite{guo2017calibration} & 2017 & 0.230 & 0.246 & 0.338 & 0.631 & 0.040 & 0.024\\
& MMCE \cite{MMCE} & 2018 & 0.250 & 0.269 & 0.378 & 0.752 & 0.054 & 0.039\\
& LS \cite{muller2019does} & 2019 & 0.218 & 0.241 & 0.303 & 0.570 & 0.047 & 0.034\\
& Mixup \cite{thulasidasan2019mixup} & 2019 & 0.143 & 0.211 & 0.110 & 0.423 & 0.078 & 0.482\\
& Focal Loss \cite{focal_loss} & 2020 & 0.135 & 0.193 & 0.262 & 0.518 & 0.070 & 0.061\\
& AdaFocal \cite{ghosh2022adafocal} & 2022 & \color{blue}{0.069} & 0.133 & 0.103 & 0.383 & 0.108 & 0.102\\
\hline
\multirow{2}{*}{\parbox{1.0cm}{Our \\ Methods}}
& $\text{ASLP}_{\text{ECE}}$  & 2023 & \color{red}{\textbf{0.056}} & \color{red}{\textbf{0.103}} & \color{red}{\textbf{0.061}} & \color{red}{\textbf{0.083}} & \color{red}{\textbf{0.024}} & \color{red}{\textbf{0.027}}\\
& $\text{ASLP}_{\text{MEI}}$ & 2023 & 4.565 & 4.027 & 4.079 & 3.112 & 5.095 & 5.301\\
\bottomrule
\end{tabular}
\label{A_tab:ECE_DEBIAS_Benchmark}
\end{table}


\clearpage


% \section{Model Calibration Benchmark Using $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ with Different Number of Bins.}
% \label{A_sec:SOD_Calibration_Degree_Benchmark_15_Bins}
% Tab.~\ref{tab:SOD_Calibration_Degree_Benchmark_15_Bins} presents model calibration degrees of existing SOD models, model calibration methods and our proposed technique evaluated in terms of $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ with 15 bins. Overall, the results are similar to the evaluation using $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ with 100 bins as shown in Tab.~\ref{tab:SOD_Calibration_Degree_Benchmark}. This is attributed to that confidences of the majority of model predictions are close to 1. Thus changing the binning intervals do not significantly change the binning scheme for these predictions. The exception is $\text{ASLP}_{\text{MEI}}$ whose prediction confidences are significantly moderated. Its $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ scores demonstrate larger difference when using different number of bins.

% % These predictions are categorised into the last bin which spans $[0.99, 1.00)$ when using 100 bins. In case of using 15 bins, these predictions are similarly categorised into the last bin which spans $[0.93, 1.00)$. 

% \begin{table*}[htb!]
% \centering
% \scriptsize
% \renewcommand{\arraystretch}{1.2}
% \renewcommand{\tabcolsep}{1.5mm}
% \caption{Salient object detection model calibration degree benchmark. Results are evaluated in with $\text{ECE}_{\text{EW}}$ and $\text{OE}_{\text{EW}}$ with 15 bins (units in \%).
% % and {\color{red} red} and {\color{blue} blue} indicate the best and the second-best performance respectively.) 
% % See Appendix.~\ref{A_sec:model_calibration_benchmark_with_ECE_EM_SWEEP_DEBIAS} for evaluations with $\text{ECE}_{\text{EM}}$ \cite{nguyen-oconnor-2015-posterior}, $\text{ECE}_{\text{DEBIAS}}$ \cite{kumar2019verified} and $\text{ECE}_{\text{SWEEP}}$ \cite{roelofs2022mitigating}.
% }
% \begin{tabular}{cl|c|cc|cc|cc|cc|cc|cc}
% \toprule
% \multicolumn{2}{c|}{\multirow{2}{*}{Methods}} & {\multirow{2}{*}{Year}} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
% & & & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ \\
% \midrule
% \multirow{18}{*}{\parbox{1.0cm}{SOD \\ Methods}} 
% & MSRNet \cite{li2017instance} & 2017 & 2.51 & 2.31 & 3.24 & 3.09 & 3.33 & 3.16 & 6.21 & 5.97 & 0.96 & 0.93 & 0.89 & 0.85\\
% & SRM \cite{Wang_2017_ICCV} & 2017 & 3.93 & 3.89 & 4.74 & 4.48 & 9.67 & 9.35 & 2.47 & 2.32 & 1.82 & 1.69\\
% & Amulet \cite{zhang2017amulet} & 2017 & 5.50 & 5.15 & 5.65 & 5.34 & 5.57 & 5.28 & 9.67 & 9.27 & 2.50 & 2.38 & 1.93 & 1.83\\
% & BMPM \cite{zhang2018bi} & 2018 & 3.67 & 3.45 & 4.45 & 4.30 & 4.81 & 4.62 & 8.03 & 7.84 & 1.92 & 1.87 & 1.55 & 1.51 \\
% & DGRL \cite{wang2018detect} & 2018 & 4.05 & 3.81 & 4.33 & 4.15 & 4.90 & 4.69 & 8.26 & 8.04 & 2.09 & 1.99 & 1.60 & 1.51\\
% & PAGR \cite{zhang2018progressive} & 2018 & 3.96 & 3.74 & 5.05 & 4.89 & 5.49 & 5.26 & 11.90 & 11.61 & 2.77 & 2.65 & 1.59 & 1.51\\
% & PiCANet \cite{liu2018picanet} & 2018 & 5.05 & 4.84 & 4.78 & 4.65 & 8.03 & 7.82 & 10.36 & 10.15 & 3.42 & 3.34 & 2.51 & 2.44\\
% & CPD \cite{wu2019cascaded} & 2019 & 3.92 & 3.76 & 4.15 & 4.02 & 5.29 & 5.12 & 9.51 & 9.28 & 2.27 & 2.18 & 1.97 & 1.89\\
% & BASNet \cite{qin2019basnet} & 2019 & 4.96 & 4.84 & 4.90 & 4.81 & 6.44 & 6.33 & 10.30 & 10.19 & 2.72 & 2.69 & 2.29 & 2.25\\
% & EGNet \cite{zhao2019egnet} & 2019 & 3.29 & 3.12 & 3.61 & 3.47 & 5.33 & 5.13 & 7.90 & 7.68 & 1.95 & 1.87 & 1.45 & 1.39\\
% & AFNet \cite{feng2019attentive} & 2019 & 3.90 & 3.71 & 4.20 & 4.05 & 4.97 & 4.78 & 8.06 & 7.97 & 2.35 & 2.25 & 1.85 & 1.76\\
% & PoolNet \cite{Liu21PamiPoolNet} & 2019 & 3.28 & 3.10 & 3.80 & 3.66  & 5.22 & 5.00 & 7.98 & 7.75 & 1.97 & 1.89 & 1.80 & 1.74\\
% & GCPANet \cite{chen2020global} & 2020 & 3.14 & 2.97 & 3.93 & 3.80 & 4.08 & 3.93 & 6.94 & 6.78 & 1.59 & 1.52 & 1.25 & 1.20\\
% & MINet \cite{pang2020multi} & 2020 & 3.62 & 3.47 & 4.40 & 4.27 & 4.88 & 4.72 & 7.86 & 7.75 & 2.11 & 2.03 & 1.72 & 1.64 \\
% & $\text{F}^{3}\text{Met}$ \cite{wei2020f3net} & 2020 & 3.63 & 3.49 & 4.21 & 4.09 & 4.80 & 4.65 & 7.86 & 7.70 & 2.24 & 2.16 & 1.90 & 1.83 \\
% & EBMGSOD \cite{jing_ebm_sod21} & 2021 & 3.42 & 3.28 & 4.06 & 3.93 & 4.73 & 4.58 & 7.37 & 7.22 & 2.12 & 2.04 & 1.78 & 1.70\\
% & ICON \cite{zhuge2021salient} & 2021 & 2.87 & 2.76 & 3.80 & 3.70 & 4.04 & 3.93 & 6.63 & 6.50 & 1.54 & 1.48 & 1.37 & 1.32\\
% & PFSNet \cite{ma2021pyramidal} & 2021 & 2.92 & 2.72 & 3.93 & 3.80 & 4.41 & 4.25 & 7.49 & 7.33 & 2.38 & 2.24 & 2.04 & 1.95\\
% & EDN \cite{wu2022edn} & 2022 & 3.59 & 3.46 & 3.99 & 3.89 & 4.85 & 4.72 & 8.68 & 8.56 & 2.17 & 2.11 & 1.63 & 1.58\\
% \hline
% \multirow{7}{*}{\parbox{1.0cm}{Model \\ Calibration \\ Methods}}
% & Brier Loss \cite{brier1950verification} & 1950 & 2.75 & 2.56 & 3.52 & 3.37 & 3.86 & 3.67 & 6.30 & 6.08 & 1.35 & 1.29 & 1.04 & 0.99 \\
% & Temperature Scaling \cite{guo2017calibration} & 2017 & 2.52 & 2.33 & 3.14 & 3.00 & 3.54 & 3.35 & 6.27 & 6.03 & 0.95 & 0.93 & 0.82 & 0.70\\
% & MMCE \cite{MMCE} & 2018 & 2.85 & 2.69 & 3.55 & 3.42 & 4.07 & 3.89 & 6.74 & 6.57 & 1.40 & 1.34 & 1.16 & 1.12\\
% & Label Smoothing \cite{muller2019does} & 2019 & 2.00 & 1.81 & 2.89 & 2.72 & 3.04 & 2.85 & 5.97 & 5.71 & {\color{blue} 0.83} & 0.68 & {\color{blue} 0.83} & 0.48\\
% & Mixup \cite{thulasidasan2019mixup} & 2019 & 2.44 & 2.27 & 3.42 & 3.26 & 3.12 & 2.99 & {\color{blue} 5.84} & 5.72 & 1.31 & {\color{blue} 0.26} & 3.72 & 0.00\\
% & Focal Loss \cite{focal_loss} & 2020 & 2.30 & 2.14 & 3.16 & 2.85 & 3.45 & 3.16 & 6.19 & 5.99 & 1.43 & 1.03 & 1.26 & 0.78\\
% & AdaFocal \cite{ghosh2022adafocal} & 2022 & {\color{blue}1.62} & 1.42 & {\color{blue}2.32} & 1.85 & {\color{blue}2.43} & 2.18 & 5.81 & 5.43 & 1.62 & 0.79 & 1.35 & 0.52\\
% \hline
% \multirow{2}{*}{\parbox{1.0cm}{Our \\ Methods}}
% & $\text{ASLP}_{\text{MC}}$  & 2023 & {\color{red} \textbf{1.41}} & {\color{blue} 1.22} & {\color{red} \textbf{1.98}} & {\color{blue} 1.82} & {\color{red} \textbf{2.27}} & {\color{blue} 2.09} & {\color{red} \textbf{5.51}} & {\color{blue} 5.18} & {\color{red} \textbf{0.50}} &  0.21 & {\color{red} \textbf{0.77}} & {\color{blue} 0.16}\\
% & $\text{ASLP}_{\text{MEI}}$ & 2023 & 20.8 & {\color{red} \textbf{0.05}} & 19.4 & {\color{red} \textbf{0.00}} & 19.5 & {\color{red} \textbf{0.00}} & 16.7 & {\color{red} \textbf{0.00}} & 22.2 & {\color{red} \textbf{0.00}} & 22.7 & {\color{red} \textbf{0.00}}\\
% \bottomrule
% \end{tabular}
% \label{tab:SOD_Calibration_Degree_Benchmark_15_Bins}
% \end{table*}


% \clearpage



\section{Joint Distribution of Prediction Confidence and Prediction Accuracy on 6 Testing Datasets}
\label{A_sec:More_Joint_Plot}
Fig.~\ref{fig:more_joint_plot} presents the joint distribution of prediction confidence and prediction accuracy of our methods, existing model calibration methods and some of the salient object detection models on the six SOD testing datasets.


% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

\clearpage

\section{Generalisation to Existing SOD Methods}
\label{A_sec:Generalisation_to_SOD_Models}
We study the compatibility of the proposed updating rule $\text{ASLP}_{\text{MC}}$ with some of the existing state-of-the-art SOD models, including EBMGSOD \cite{jing_ebm_sod21}, ICON \cite{zhuge2021salient}, and EDN \cite{wu2022edn}, and present the model calibration results in Tab.~\ref{A_tab:Generalisation_to_SOD_Methods_Calibration_Results}. We implement the $\text{ASLP}_{\text{MC}}$ with the Hard Inversion (HI) label perturbation technique. The results demonstrate that our proposed method is readily compatible with existing SOD methods to improve their respective model calibration degrees. Further, we find that incorporation of our proposed $\text{ASLP}_{\text{MC}}$ into the training of existing SOD models do not negatively impact their classification performances as demonstrated in Tab.~\ref{A_tab:Generalisation_to_SOD_Methods_Classification_Results}. 
% On the contrary, incorporation of $\text{ASLP}_{\text{MC}}$ slightly improves the dense classification performance.


\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.0mm}
\caption{The model calibration degrees of existing Salient Object Detection models with or without the proposed Adaptive Label Augmentation are evaluated in terms of Equal-Width Expected Calibration Error, $\text{ECE}_{\text{EW}}$, and Equal-Width Over-confidence Error, $\text{OE}_{\text{EW}}$, with 10 bins ($B = 10$).}
\begin{tabular}{l|c|c|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & {\multirow{2}{*}{Year}} & {\multirow{2}{*}{$\text{ASLP}_{\text{MC}}$}} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& & & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ \\
\midrule
EBMGSOD \cite{jing_ebm_sod21} & 2021 & \xmark & 3.45 & 3.29 & 4.11 & 3.95 & 4.79 & 4.61 & 7.48 & 7.30 & 2.14 & 2.05 & 1.79 & 1.70\\
ICON \cite{zhuge2021salient} & 2021 & \xmark & 2.89 & 2.76 & 3.84 & 3.71 & 4.08 & 3.95 & 6.70 & 6.55 & 1.56 & 1.49 & 1.38 & 1.32\\
% PFSNet \cite{ma2021pyramidal} & 2021 & \xmark & 2.74 & 2.72 & 2.78 & 2.76 & 3.30 & 3.27 & 6.42 & 6.39 & 2.41 & 2.39 & 2.06 & 2.05\\
EDN \cite{wu2022edn} & 2022 & \xmark & 3.62 & 3.47 & 4.02 & 3.90 & 4.89 & 4.74 & 8.81 & 8.66 & 2.20 & 2.13 & 1.65 & 1.58\\
\hline
EBMGSOD & 2021 & \cmark & 1.60 & 1.34 & 1.91 & 1.74 & 2.45 & 2.23 & 5.48 & 5.21 & 0.77 & 0.47 & 0.75 & 0.22\\
ICON & 2021 & \cmark & 1.28 & 1.05 & 1.88 & 1.67 & 2.45 & 2.17 & 5.17 & 4.91 & 1.25 & 0.07 & 1.10 & 0.05\\
% PFSNet & 2021 & \cmark & 1.35 & 1.14 & 2.78 & 2.76 & 3.30 & 3.27 & 6.42 & 6.39 & 2.41 & 2.39 & 2.06 & 2.05\\
EDN & 2022 & \cmark & 2.02 & 1.77 & 2.23 & 2.03 & 2.74 & 2.55 & 6.77 & 6.46 & 0.82 & 0.52 & 0.71 & 0.35\\
\bottomrule
\end{tabular}
\label{A_tab:Generalisation_to_SOD_Methods_Calibration_Results}
\end{table}





\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.0mm}
\caption{The dense classification accuracy of Salient Object Detection models with or without the proposed Adaptive Label Augmentation is evaluated with maximum F-measure and maximum E-measure \cite{fan2018enhanced}.}
\begin{tabular}{l|c|c|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & {\multirow{2}{*}{Year}} & {\multirow{2}{*}{$\text{ASLP}_{\text{MC}}$}} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& & & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ \\
\midrule
EBMGSOD \cite{jing_ebm_sod21} & 2021 & \xmark & 0.850 & 0.927 & 0.762 & 0.867 & 0.830 & 0.896 & 0.834 & 0.800 & 0.914 & 0.944 & 0.906 & 0.952\\
ICON \cite{zhuge2021salient} & 2021 & \xmark & 0.860 & 0.924 & 0.773 & 0.876 & 0.850 & 0.899 & 0.815 & 0.854 & 0.933 & 0.954 & 0.919 & 0.953\\
% PFSNet \cite{ma2021pyramidal} & 2021 & \xmark & 0.898 & 0.954 & 0.823 &  \\
EDN \cite{wu2022edn} & 2022 & \xmark & 0.893 & 0.949 & 0.821 & 0.900 & 0.879 & 0.920 & 0.840 & 0.860 & 0.950 & 0.969 & 0.940 & 0.970\\
\hline
EBMGSOD & 2021 & \cmark & 0.853 & 0.930 & 0.767 & 0.871 & 0.841 & 0.901 & 0.839 & 0.807 & 0.923 & 0.946 & 0.912 & 0.956\\
ICON & 2021 & \cmark & 0.864 & 0.929 & 0.776 & 0.877 & 0.857 & 0.904 & 0.819 & 0.855 & 0.940 & 0.959 & 0.926 & 0.959\\
% PFSNet & 2021 & \cmark & \\
EDN & 2022 & \cmark & 0.898 & 0.954 & 0.824 & 0.901 & 0.880 & 0.923 & 0.848 & 0.866 & 0.952 & 0.971 & 0.942 & 0.972\\
\bottomrule
\end{tabular}
\label{A_tab:Generalisation_to_SOD_Methods_Classification_Results}
\end{table}

\clearpage

\section{Experiments on Additional Dense Classification Tasks}
\label{A_sec:Additional_Experiments_on_Dense_Binary_Classification_Tasks}
\subsection{Camouflaged Object Detection}
We train our model on the COD10K training set \cite{fan2020camouflaged} which consists of 6,000 training samples. We partition it into a training set of 5,400 samples and a validation set of 600 samples. Four testing datasets, including the COD10K testing set \cite{fan2020camouflaged}, NC4K \cite{lv2021simultaneously}, CAMO \cite{le2019anabranch} and CHAMELEON \cite{skurowski2018animal}, are used to evaluate the model calibration degree and dense binary classification accuracy. We train the models for 50 epochs and the rest of settings follow those in Salient Object Detection.

We apply the proposed $\text{ASLP}_{\text{MC}}$ with Hard Inversion (HI) and Soft Inversion (SI) label perturbation techniques and $\text{ALS}_{\text{MC}}$ to improve the model calibration degrees with four label perturbation techniques and report the results in Tab.~\ref{tab:Model_Calibration_on_Camouflaged_Object_Detection}. It can be observed that both ASLP with various label perturbation techniques and ALS can also significantly improve model calibration degrees in Camouflaged Object Detection models. Further, we show that the improvements in model calibration degree are achieved without negatively impacting the classification accuracy as shown in Tab.~\ref{tab:Classification_Accuracy_on_Camouflaged_Object_Detection}.

\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Application Adaptive Stochastic Label Perturbation (ASLP) with different label perturbation techniques in Camouflaged Object Detection task. The model calibration degrees are evaluated with Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 10 bins. Results are presented in (\%).}
\begin{tabular}{l|ccc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{2}{c|}{COD10K \cite{fan2020camouflaged}} & \multicolumn{2}{c|}{NC4K \cite{lv2021simultaneously}} & \multicolumn{2}{c|}{CHAMELEON \cite{skurowski2018animal}} & \multicolumn{2}{c}{CAMO \cite{le2019anabranch}} \\
& $\alpha$ & $\beta$ & e & 
$\text{ECE}_{\text{EW}} \downarrow$ & $\text{OE}_{\text{EW}} \downarrow$ & $\text{ECE}_{\text{EW}} \downarrow$ & $\text{OE}_{\text{EW}} \downarrow$ & $\text{ECE}_{\text{EW}} \downarrow$ & $\text{OE}_{\text{EW}} \downarrow$ & $\text{ECE}_{\text{EW}} \downarrow$ & $\text{OE}_{\text{EW}} \downarrow$ \\
\midrule
Baseline (\enquote{COD-B}) & 0 & 0 & \xmark & 1.65 & 1.55 & 2.75 & 2.60 & 0.63 & 0.57 & 3.62 & 3.46\\
\hline
$\text{COD-ASLP}_{\text{MC}}^{\text{HI}}$ & 
$\alpha_{\text{ada}}$ & 1.0 & \xmark & 1.06 & 0.81 & 1.67 & 1.51 & 0.43 & 0.12 & 2.00 & 1.80\\
$\text{COD-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 1.05 & 0.80 & 1.72 & 1.55 & 0.44 & 0.21 & 2.03 & 1.85\\
$\text{COD-ALS}_{\text{MC}}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 1.03 & 0.76 & 1.69 & 1.53 & 0.45 & 0.28 & 1.98 & 1.81\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \xmark & 1.03 & 0.76 & 1.69 & 1.53 & 0.45 & 0.28 & 1.98 & 1.81\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \cmark & 1.03 & 0.84 & 1.72 & 1.52 & 0.42 & 0.16 & 2.07 & 1.87 \\
\bottomrule
\end{tabular}
\label{tab:Model_Calibration_on_Camouflaged_Object_Detection}
\end{table}

\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Application Adaptive Stochastic Label Perturbation (ASLP) with different label perturbation techniques in the Camouflaged Object Detection task. The dense classification accuracy is evaluated with maximum F-measure and maximum E-measure \cite{fan2018enhanced}.}
\begin{tabular}{l|ccc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{2}{c|}{COD10K \cite{fan2020camouflaged}} & \multicolumn{2}{c|}{NC4K \cite{lv2021simultaneously}} & \multicolumn{2}{c|}{CHAMELEON \cite{skurowski2018animal}} & \multicolumn{2}{c}{CAMO \cite{le2019anabranch}} \\
& $\alpha$ & $\beta$ & e & 
$F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ \\
\midrule
Baseline (\enquote{COD-B}) & 0 & 0 & \xmark & 0.715 & 0.886 & 0.803 & 0.902 & 0.843 & 0.940 & 0.749 & 0.855\\
\hline
$\text{COD-ASLP}_{\text{MC}}^{\text{HI}}$ & 
$\alpha_{\text{ada}}$ & 1.0 & \xmark & 0.716 & 0.886 & 0.803 & 0.902 & 0.845 & 0.942 & 0.756 & 0.861\\
$\text{COD-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 0.716 & 0.887 & 0.802 & 0.904 & 0.844 & 0.943 & 0.759 & 0.867\\
$\text{COD-ALS}_{\text{MC}}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 0.717 & 0.887 & 0.804 & 0.905 & 0.845 & 0.941 & 0.767 & 0.868\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \xmark & 0.717 & 0.887 & 0.804 & 0.905 & 0.845 & 0.941 & 0.767 & 0.868\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \cmark & 0.715 & 0.886 & 0.806 & 0.905 & 0.844 & 0.945 & 0.761 & 0.865 \\
\bottomrule
\end{tabular}
\label{tab:Classification_Accuracy_on_Camouflaged_Object_Detection}
\end{table}


\begin{table}[h!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Application Adaptive Stochastic Label Perturbation (ASLP) with different label perturbation techniques in the Smoke Detection (SD) task. Model calibration degree is evaluated with Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 10 bins. Dense classification accuracy is evaluated with maximum F-measure and maximum E-measure \cite{fan2018enhanced}.}
\begin{tabular}{l|ccc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{4}{c}{SMOKE5K \cite{fan2020camouflaged}} \\
% \cmidrule{5-8}\\
& $\alpha$ & $\beta$ & e & 
$\text{ECE}_{\text{EW}} (\%) \downarrow$ & $\text{OE}_{\text{EW}} (\%) \downarrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ \\
\midrule
Baseline (\enquote{SD-B}) & 0 & 0 & \xmark & 0.164 & 0.154 & 0.763 & 0.930 \\
\hline
$\text{SD-ASLP}_{\text{MC}}^{\text{HI}}$ & 
$\alpha_{\text{ada}}$ & 1.0 & \xmark & 0.071 & 0.063 & 0.763 & 0.930\\
$\text{SD-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 0.076 & 0.072 & 0.765 & 0.932\\
$\text{SD-ALS}_{\text{MC}}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 0.079 & 0.072 & 0.764 & 0.930\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \xmark &  & 0.079 & 0.072 & 0.764 & 0.930\\\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \cmark & 0.089 & 0.077 & 0.765 & 0.930 \\
\bottomrule
\end{tabular}
\label{tab:Smoke_Detection_Results}
\end{table}



\subsection{Smoke Detection}
We train our model on the SMOKE5K training set \cite{yan2022transmission} which consists of 4,600 training samples of real smoke. We partition it into a training set of 4,200 samples and a validation set of 400 samples. SMOKE5K testing set, comprising of 400 real-smoke images, is used to evaluate model calibration degree and dense binary classification accuracy. 

We apply the proposed $\text{ASLP}_{\text{MC}}$ with Hard Inversion (HI) and Soft Inversion (SI) label perturbation techniques and $\text{ALS}_{\text{MC}}$ to improve the model calibration degrees and report the results in Tab.~\ref{tab:Smoke_Detection_Results}. It can be observed that both $\text{ASLP}_{\text{MC}}$ with different label perturbation techniques and $\text{ALS}_{\text{MC}}$ can significantly improve model calibration degrees in Smoke Detection models, despite the baseline model already achieving higher calibration degrees compared with baseline models in Salient Object Detection and Camouflaged Object Detection. We can observe that our proposed methods still achieve improvements in model calibration degree without negatively impacting the classification accuracy.



% \clearpage


\section{Experiments on Additional Dense Multi-Class Classification Task - Semantic Segmentation}
\label{A_sec:Additional_Experiments_on_Semantic_Segmentation}
We evaluate our proposed methods on the PASCAL VOC 2012 segmentation dataset \cite{everingham2010pascal} which has 20 foreground categories and 1 background category. The official split has 1,464, 1,449, and 1,456 samples in training, validation and testing sets respectively. Following previous work \cite{chen2017deeplab}, we use an augmented training set comprising of 10,582 samples, provided by \cite{hariharan2011semantic}, for model training. As we do not have access to the groundtruth of \enquote{official testing set} whose evaluation is server-based, we adopt the \enquote{official validation set} as \enquote{our testing set} to evaluate the model calibration degrees and segmentation accuracies. Similar to our implementation in dense binary classification tasks, we partition the augmented training set into \enquote{our training set} of 9,582 images and \enquote{our validation set} of 1,000 images. 

% We adopt DeepLabv3+ \cite{chen2017deeplab} with ResNet50 backbone as our baseline model (\enquote{SS-B}) and apply the proposed $\text{ASLP}_{\text{MC}}$ with with Hard Inversion (HI) and Soft Inversion (SI) label perturbation techniques and $\text{ALS}_{\text{MC}}$ to improve the model calibration degrees. We report model calibration results evaluated in terms of Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 100 bins in Tab.~\ref{tab:Semantic_Segmentation_Results}. 

We adopt DeepLabv3+ \cite{chen2017deeplab} with a ResNet50 backbone as our baseline model (\enquote{SS-B}) and apply the proposed $\text{ASLP}_{\text{MC}}$ with with the Hard Inversion (HI) label perturbation technique and $\text{ALS}_{\text{MC}}$ to improve the model calibration degrees. We report model calibration results evaluated in terms of Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 10 bins in Tab.~\ref{tab:Semantic_Segmentation_Results}. 


\begin{table}[h!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Application Adaptive Stochastic Label Perturbation (ASLP) with different label perturbation techniques in a Semantic Segmentation (SS) task. Model calibration degree is evaluated with Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 10 bins. Segmentation accuracy is evaluated with Intersection-over-Union (IoU) \cite{chen2017deeplab}.}
\begin{tabular}{l|ccc|cc|c}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{3}{c}{PASCAL VOC 2012 \cite{everingham2010pascal}} \\
% \cmidrule{5-8}\\
& $\alpha$ & $\beta$ & e & 
$\text{ECE}_{\text{EW}} (\%) \downarrow$ & $\text{OE}_{\text{EW}} (\%) \downarrow$ & IoU (\%) $\uparrow$ \\
\midrule
Baseline (\enquote{SS-B}) & 0 & 0 & \xmark & 6.29 & 5.37 & 71.2 \\
\hline
$\text{SS-ASLP}_{\text{MC}}^{\text{HI}}$ & $\alpha_{\text{ada}}$ & 1.0 & \xmark & 4.05 & 3.13 & 71.3\\
% $\text{SS-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 3.98 & 3.15 & 75.2\\
$\text{SS-ALS}_{\text{MC}}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 4.10 & 3.24 & 71.5\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \xmark &  & 0.079 & 0.072 & 0.764 & 0.930\\\\
% $\text{COD-ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \cmark & 0.089 & 0.077 & 0.765 & 0.930 \\
\bottomrule
\end{tabular}
\label{tab:Semantic_Segmentation_Results}
\end{table}





\clearpage


\section{Static Stochastic Label Perturbation}
\subsection{Implementation}
\label{A_sub_sec:SLP_Implementation}
We implement four static stochastic label perturbation techniques each of which have a single label perturbation probability $\alpha$ for the entire training dataset. Their details are as below:
\begin{itemize}
    \item \textbf{Hard Inversion (HI)} produces the perturbed label by inverting the groundtruth label with $p = \text{LP}(y, 2) = 1 - y$. Intuitively, it switches the label category from \enquote{salient} to \enquote{non-salient} and vice versa. The label perturbation probability is limited to $\alpha \in [0, 0.5)$ to avoid learning a complete opposite task (non-salient background detection).
    
    \item \textbf{Soft Inversion (SI)} inverts the label category and softens the target with  $p = \text{LP}(y, 0.75) = -0.5y + 0.75$. Similarly, the label perturbation probability is limited to $p \in [0, \frac{1}{1.5})$ to prevent from learning a complete opposite task.
    
    \item \textbf{Moderation (M)} transforms groundtruth label into a prior distribution on the two classes (salient foreground object v.s. non-salient background), as $p = \text{LP}(y, 0.5) = 0.5$. The label perturbation probability is in the range $\alpha \in [0, 1)$.
    
    \item \textbf{Dynamic Moderation (DM)} introduces additional stochasticity on top of the \textbf{Moderation} method by adding an additional noise sampled from a truncated normal distribution\footnote{Truncated normal distribution $\mathcal{N}_{a,b}(\mu, \sigma)$, where $a$ and $b$ indicate the bound, $\mu$ is the mean and $\sigma$ is the variance.}: $p = \text{LP}(y, 0.5) + e = 0.5 + e, \,\, e \sim \mathcal{N}_{-0.5, 0.5}(0, 1)$. The label perturbation probability is in the range $\alpha \in [0, 1)$.
\end{itemize}

\subsection{Effect of Static Stochastic Label Perturbation Techniques on Model Calibration Degrees}
\label{A_sub_Sec:SLP_Model_Calibration}

Fig.~\ref{fig:Static_SLP_Model_Calibration} presents model calibration degrees, evaluated in terms of Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 100 bins ($B = 100$), of various static stochastic label perturbation techniques, in which a unique label perturbation probability $\alpha$ is set for all samples throughout the training. We can observe that, with an increasing label perturbation probability, ECE scores tend to reduce to a critical points before climbing. This is caused by the model transitioning from being over-confident to under-confident. This is evidenced in the OE scores which keep decreasing until 0 when the label perturbation probability increases. Further, \enquote{HI} has the steepest change in terms of both ECE and OE scores. This rate can be related to the product of label perturbation probability and strength $\alpha \beta$. We also find a dampening effect of additional stochasticity at high label perturbation probability range ($\alpha \in [0.4, 0.6]$) where \enquote{DM} is consistently less under-confident than \enquote{M}.

\begin{table}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.0mm}
\caption{Effect label perturbation probability range (\%) for different static stochastic label perturbation techniques to reduce the Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) scores on the six testing datasets.}
\begin{tabular}{l|c|c|c|c|c|c}
\toprule
Static SLP Technique & DUTS-TE \cite{DUTS-TE} & DUT-OMRON \cite{DUT-OMRON} & PASCAL-S \cite{PASCAL-S} & SOD \cite{SOD} & ECSSD \cite{ECSSD} & HKU-IS \cite{HKU-IS}\\
\midrule
Hard Inversion (HI)     & 0 - 5\% & 0 - 3\% & 0 - 5\% & 0 - 10\% & 0 - 1\% & 0 - 1\%\\
Soft Inversion (SI)     & 0 - 5\% & 0 - 5\% & 0 - 5\% & 0 - 10\% & 0 - 2\% & 0 - 2\%\\
Moderation (M)          & 0 - 5\% & 0 - 5\% & 0 - 5\% & 0 - 20\% & 0 - 3\% & 0 - 3\%\\
Dynamic Moderation (DM) & 0 - 5\% & 0 - 5\% & 0 - 5\% & 0 - 20\% & 0 - 3\% & 0 - 3\%\\
\bottomrule
\end{tabular}
\label{A_tab:Static_SLP_Effective_Alpha_Range}
\end{table}

The effective label perturbation probability range for each static SLP technique on the six testing datasets is summarised in Tab.~\ref{A_tab:Static_SLP_Effective_Alpha_Range}. In general, the static SLPs have a wide range of effective label perturbation probability leading to reduced ECE scores compared to the baseline. The widest effective label perturbation probability range is found on the SOD dataset, with 0 - 10\% for \enquote{HI} and \enquote{SI} and 0 - 20\% for \enquote{M} and \enquote{DM}. This can be attributed to the baseline model being the most mis-calibrated on the SOD dataset, thus stronger label augmentation measures are required to transform the model from being over-confident to being under-confident. On the other hand, the baseline model is the most calibrated on the ECSSD and the HKU-IS datasets, indicating a small gap between the prediction confidence and prediction accuracy distributions. That leaves little space for label augmentation techniques to reduce the prediction confidence in order to match the prediction accuracy.



\subsection{Effect of Static Stochastic Label Perturbation Techniques on Dense Binary Classification Performance}
\label{A_sub_sec:SLP_Classification_Performance}
We present the dense binary classification performance, evaluated in terms of maximum F measure, of various static stochastic label perturbation techniques in Fig.~\ref{fig:Static_SLP_F_Measure}. It can be observed that in the effective label perturbation probability range for respective static SLP techniques, the dense binary classification performances are not negatively impacted. The performance drop is observed when the product $\alpha \beta$ is too high, \eg $\alpha \in [0.2, 0.3]$ for \enquote{HI}, $\alpha = 0.4$ for \enquote{SI}, and $\alpha = 0.6$ for \enquote{DM}. Overall, incorporation of static SLP techniques, with an effective label perturbation probability, can achieve improved model calibration degrees without sacrificing the dense bianry classification performance.



% Figure environment removed

% Figure environment removed




% Figure environment removed


\clearpage



\section{Experiments on Salient Object Detection with Additional Backbones}
\label{A_sec:Experiments_with_Additional_Backbones}
Experiments with additional backbones, VGG16 and Swin Transformer, are carried out on Salient Object Detection. We replace the ResNet50 backbone of the baseline model with VGG16 and Swin Transformer in respective experiments. We apply the proposed $\text{ASLP}_{\text{MC}}$ with with Hard Inversion (HI) and Soft Inversion (SI) label perturbation techniques and $\text{ALS}_{\text{MC}}$ to improve the model calibration degrees with respective backbones. 


\begin{table*}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Model calibration degrees with Swin transformer \cite{liu2021swin} backbone. Results are evaluated with Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 10 bins (units in (\%)).
% Effect of Stochastic Label Perturbation Adaptive Stochastic Label Perturbation (ASLP) with different label perturbation techniques and Adaptive Label Smoothing (ALS) on the model calibration degrees evaluated on Expected Calibration Error (ECE) and Over-confidence Error (OE) of baseline model that uses the Swin transformer \cite{liu2021swin} as its encoder.
% % The proposed ASLP is generalised to an Adaptive Label Smoothing (ALS) technique that adaptively tunes the label softening scale ($\beta_{\text{ada}}$).
}
\begin{tabular}{l|ccc|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& $\alpha$ & $\beta$ & e & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ \\
\midrule
Baseline (\enquote{Swin-B}) & 0 & 0 & 0 & 2.41 & 2.23 & 3.29 & 3.15 & 3.35 & 3.19 & 6.23 & 6.05 & 1.02 & 0.97 & 0.87 & 0.82\\
% \hline
% $\text{SLP}_{\text{HI}}^{\alpha=0.01}$ & 0.01 & 1.0 & \xmark & 2.21 & 1.84 & 2.96 & 2.78 & 3.11 & 2.82 & 6.09 & 5.80 & 1.03 & 0.68 & 1.01 & 0.53\\
% $\text{SLP}_{\text{SI}}^{\alpha=0.02}$ & 0.02 & 0.75 & \xmark & 2.25 & 2.05 & 3.00 & 2.82 & 3.05 & 2.83 & 6.40 & 6.09 & 0.93 & 0.84 & 0.87 & 0.60\\
% $\text{SLP}_{\text{M}}^{\alpha=0.03}$ & 0.03 & 0.5 & \xmark & 2.24 & 2.03 & 3.17 & 2.97 & 3.41 & 3.20 & 6.26 & 5.97 & 0.83 & 0.77 & 0.96 & 0.81 \\
% $\text{SLP}_{\text{DM}}^{\alpha=0.03}$ & 0.03 & 0.5 & \cmark & 2.29 & 2.09 & 3.00 & 2.83 & 3.47 & 3.24 & 6.72 & 6.43 & 1.13 & 1.04 & 0.86 & 0.80\\
% $\text{LS}^{\beta=0.03}$ & 1.0 & 0.03 & \xmark & 2.20 & 1.99 & 3.09 & 2.91 & 3.24 & 3.03 & 6.27 & 5.99 & 1.03 & 0.78 & 0.92 & 0.67\\
\hline
$\text{Swin-ASLP}_{\text{MC}}^{\text{HI}}$ & $\alpha_{\text{ada}}$ & 1.0 & \xmark & 1.44 & 1.21 & 1.73 & 1.59 & 1.74 & 1.57 & 5.08 & 4.85 & 0.57 &  0.30 & 0.81 & 0.23\\
$\text{Swin-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 1.48 & 1.14 & 1.63 & 1.49 & 1.80 & 1.52 & 5.14 & 4.93 & 0.64 & 0.38 & 0.80 & 0.24\\
% $\text{ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \xmark & 1.36 & 1.19 & 1.70 & 1.57 & 1.76 & 1.55 & 5.06 & 4.92 & 0.52 & 0.24 & 0.73 & 0.20\\
% $\text{ASLP}_{\text{MC}}^{\text{M}}$ & $\alpha_{\text{ada}}$ & 0.5 & \cmark & 1.38 & 1.14 & 1.78 & 1.62 & 1.83 & 1.52 & 5.07 & 4.81 & 0.52 & 0.34 & 0.80 & 0.26 \\
$\text{Swin-ALS}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 1.44 & 1.14 & 1.76 & 1.57 & 1.69 & 1.55 & 5.17 & 4.82 & 0.54 & 0.36 & 0.77 & 0.24\\
\bottomrule
\end{tabular}
\label{tab:Swin_Transformer_Calibration_Degrees}
\end{table*}


\begin{table*}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Dense classification accuracy with Swin transformer \cite{liu2021swin} backbone. Results are evaluated with maximum F-measure and maximum E-measure \cite{fan2018enhanced}.
}
\begin{tabular}{l|ccc|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& $\alpha$ & $\beta$ & e & 
$F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ \\
\midrule
Baseline (\enquote{Swin-B}) & 0 & 0 & 0 & 0.894 & 0.949 & 0.804 & 0.890 & 0.877 & 0.920 & 0.858 & 0.878 & 0.948 & 0.969 & 0.939 & 0.969\\
\hline
$\text{Swin-ASLP}_{\text{MC}}^{\text{HI}}$ & $\alpha_{\text{ada}}$ & 1.0 & \xmark & 0.895 & 0.953 & 0.808 & 0.892 & 0.881 & 0.924 & 0.959 & 0.879 & 0.950 & 0.969 & 0.938 & 0.969\\
$\text{Swin-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 0.895 & 0.952 & 0.805 & 0.893 & 9,880 & 0.922 & 0.857 & 0.882 & 0.950 & 0.969 & 0.939 & 0.970\\
$\text{Swin-ALS}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 0.895 & 0.952 & 0.804 & 0.892 & 0.879 & 0.920 & 0.859 & 0.879 & 0.948 & 0.969 & 0.939 & 0.970\\
\bottomrule
\end{tabular}
\label{tab:Swin_Transformer_Claasification_Accuracy}
\end{table*}


\begin{table*}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Model calibration degrees with VGG16 \cite{simonyan2014very} backbone. Results are evaluated with Equal-Width Expected Calibration Error ($\text{ECE}_{\text{EW}}$) and Equal-Width Over-confidence Error ($\text{OE}_{\text{EW}}$) with 10 bins (units in (\%)).
% Effect of Stochastic Label Perturbation Adaptive Stochastic Label Perturbation (ASLP) with different label perturbation techniques and Adaptive Label Smoothing (ALS) on the model calibration degrees evaluated on Expected Calibration Error (ECE) and Over-confidence Error (OE) of baseline model that uses the Swin transformer \cite{liu2021swin} as its encoder.
% % The proposed ASLP is generalised to an Adaptive Label Smoothing (ALS) technique that adaptively tunes the label softening scale ($\beta_{\text{ada}}$).
}
\begin{tabular}{l|ccc|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& $\alpha$ & $\beta$ & e & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ & $\text{ECE} \downarrow$ & $\text{OE} \downarrow$ \\
\midrule
Baseline (\enquote{VGG-B}) & 0 & 0 & 0 & 3.46 & 3.23 & 4.12 & 3.92 & 4.40 & 4.17 & 7.87 & 7.60 & 2.02 & 1.91 & 1.51 & 1.44\\
\hline
$\text{VGG-ASLP}_{\text{MC}}^{\text{HI}}$ & $\alpha_{\text{ada}}$ & 1.0 & \xmark & 1.44 & 1.28 & 1.91 & 1.82 & 2.40 & 2.16 & 5.44 & 5.08 & 0.57 & 0.21 & 0.84 & 0.16\\
$\text{VGG-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 1.47 & 1.23 & 2.05 & 1.81 & 2.34 & 2.15 & 5.54 & 5.22 & 0.51 & 0.21 & 0.88 & 0.19\\
$\text{VGG-ALS}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 1.48 & 1.31 & 1.99 & 1.76 & 2.33 & 2.04 & 5.53 & 5.14 & 0.45 & 0.29 & 0.82 & 0.13\\
\bottomrule
\end{tabular}
\label{tab:VGG16_Calibration_Degrees}
\end{table*}


\begin{table*}[htb!]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.2}
\renewcommand{\tabcolsep}{1.5mm}
\caption{Dense classification accuracy with VGG16 \cite{simonyan2014very} backbone. Results are evaluated with maximum F-measure and maximum E-measure \cite{fan2018enhanced}.
}
\begin{tabular}{l|ccc|cc|cc|cc|cc|cc|cc}
\toprule
\multirow{2}{*}{Methods} & \multicolumn{3}{c|}{Perturbation Params} & \multicolumn{2}{c|}{DUTS-TE \cite{DUTS-TE}} & \multicolumn{2}{c|}{DUT-OMRON \cite{DUT-OMRON}} & \multicolumn{2}{c|}{PASCAL-S \cite{PASCAL-S}} & \multicolumn{2}{c|}{SOD \cite{SOD}} & \multicolumn{2}{c|}{ECSSD \cite{ECSSD}} & \multicolumn{2}{c}{HKU-IS \cite{HKU-IS}}\\
& $\alpha$ & $\beta$ & e & 
$F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ & $F_{\text{max}} \uparrow$ & $E_{\text{max}} \uparrow$ \\
\midrule
Baseline (\enquote{VGG-B}) & 0 & 0 & 0 & 0.838 & 0.912 & 0.741 & 0.851 & 0.844 & 0.895 & 0.810 & 0.851 & 0.921 & 0.944 & 0.913 & 0.950\\
\hline
$\text{VGG-ASLP}_{\text{MC}}^{\text{HI}}$ & $\alpha_{\text{ada}}$ & 1.0 & \xmark & 0.844 & 0.916 & 0.746 & 0.857 & 0.844 & 0.896 & 0.812 & 0.851 & 0.921 & 0.944 & 0.913 & 0.951\\
$\text{VGG-ASLP}_{\text{MC}}^{\text{SI}}$ & $\alpha_{\text{ada}}$ & 0.75 & \xmark & 0.845 & 0.916 & 0.747 & 0.855 & 0.846 & 0.895 & 0.810 & 0.851 & 0.921 & 0.944 & 0.916 & 0.953\\
$\text{VGG-ALS}$ & 1.0 & $\beta_{\text{ada}}$ & \xmark & 0.843 & 0.914 & 0.745 & 0.857 & 0.848 & 0.898 & 0.811 & 0.852 & 0.921 & 0.945 & 0.913 & 0.952\\
\bottomrule
\end{tabular}
\label{tab:VGG16_Claasification_Accuracy}
\end{table*}


\clearpage

\section{Hyperparameters}
\label{A_sec:Hyperparameters}

% Figure environment removed

\section{Training and Inference Time}
In SOD, the training of ASLP on DUTS-TR requires 2.5 hours, which is 0.2 hours longer (or $\sim8.7$\% more) than training the base model (2.3 hours). The inference speed of ASLP on the six SOD testing datasets averages: 53.40 samples per second, which is the same as that of the base model because of the same network architecture. Both training and inference time are evaluated on a single Geforce RTX 3090 GPU.

% The necessity of evaluating the accuracy on the validation set does increase the training time. However, the increase is small w.r.t.~the baseline, In SOD, the training of ASLP on DUTS-TR requires 2.5 hours, which is 0.2 hours longer (or $\sim8.7$\% more) than training the base model (2.3 hours). The inference speed of ASLP on the six SOD testing datasets averages: 53.40 samples per second, which is the same as that of the base model because of the same network architecture.




\clearpage


\section{500 Texture Images from Describable Texture Dataset}

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% \section{Self-Calibration Binary Cross Entropy (SC-BCE) Loss}
% \label{A_sec:SC-BCE_loss_derivation}

% We show that our SC-BCE loss is close to label smoothing in binary classification. Label smoothing, as defined in Eq.~\eqref{A_eq:label_smoothing}, is a typical data augmentation that softens the training supervision signals \cite{muller2019does,lukasik2020does,xu2020towards,zhang2021delving}.
% \begin{equation}
%     S = \text{LS}(Y, \sigma) = (1 - \sigma) Y + \frac{\sigma}{K}, \quad \forall y \in Y.
%     \label{A_eq:label_smoothing}
% \end{equation}
% For image label pairs $X,Y\sim P$, the BCE loss with label smoothing takes the form:
% \begin{equation}
%     \mathcal{L}_{\text{L-BCE}}(X, Y;\theta) = (1 - \sigma) \mathcal{L}_{\text{BCE}} (X, Y;\theta) + \sigma \mathcal{L}_{\text{BCE}} (X, 1 - Y;\theta)
% \end{equation}
% \NB{That is not a straightforward equation.  Label smoothing implies that we replace Y with $(1-\alpha) Y + \alpha / 2$. This equation proposes that that is the same as adding alpha times the negative BCE loss.  This is not a simple fact. You would have to show this. I would have guessed it waas more straightforward to take the expectation over the training set of the  SC-BCE loss and show that it reduces to the $\alpha/2$ form.
% }
% \noindent
% where $\sigma$ is the label smoothing strength hyperparameter, which is usually set to a small number, \eg 0.01.

% On the other hand, our SC-BCE loss can be written as:
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_{\text{SC-BCE}}(X,Y;\theta, \alpha, \beta) = & (1-Z)\mathcal{L}_{\text{BCE}}(X,Y;\theta) + Z \mathcal{L}_{\text{BCE}}(X, P(Y, \beta), \theta)\\
%     = & (1-Z)\mathcal{L}_{\text{BCE}}(X,Y;\theta) + Z \Bigg[(1-\frac{\beta}{2})\mathcal{L}_{\text{BCE}}(X,Y;\theta) + \frac{\beta}{2}\mathcal{L}_{\text{BCE}}(X,1-Y;\theta)\Bigg] \\
% \end{aligned}
% \end{equation}
% where $P(Y, \beta) = (1 - \beta) \cdot Y + \frac{\beta}{2}$. Since $Z\sim B(1,\alpha)$ follows a Bernoulli distribution, the SC-BCE loss can be seen as with probability $\alpha$ having label smoothing $\sigma=\beta/2$ and no label smoothing otherwise.

% \NB{Should we connect the dots that $K=2$ for binary, so the difference is the scaling of the first term of Eqn 9}

% \NB{Actually, this is missing a step. You need to do this over the expecation I think.}



