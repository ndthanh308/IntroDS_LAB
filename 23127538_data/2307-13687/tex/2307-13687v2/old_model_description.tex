\textcolor{red}{The rest of this section is old text. I believe it describes Song's first paper on how they implemented noise-conditional diffusion models NCDM. This is different from the score-based SDE approach in the repo we are using.}

% Our dataset consisted of 50,000 training images and 10,000 validation images from PILArNet. Each image corresponds to the 2D projection of a single interaction event within a simulated LArTPC detector. The images are 64x64 pixels with single-channel intensities. According to SSNet these events were approximately 70\% tracks and 30\% showers. 
% % Why 70/30 split? Explain in introduction? 

We assume* our training image data is i.i.d. sampled from some larger distribution $f_{all}(\mathbf{x})$ of all possible particle events images. Our goal is then to map the space and sample new particle events. The training data distribution $f_{train}(\mathbf{x})$ can be converted to a normalized probability density function  $p_{train}(\mathbf{x}) = \frac{e^{f_{train}(\mathbf{x})}}{Z_\theta}$. To avoid the issue of normalization we map this space using a gradient field dubbed the score function: 
\begin{equation}
\nabla_{\mathbf{x}} \log p_{train}(\mathbf{x}) = - \nabla_{\mathbf{x}} f_{train}(\mathbf{x}) \approx s_\theta(\mathbf{x}, \sigma)
\end{equation}

However, these gradients are difficult to calculate directly, so instead, we approximate them through a neutral network $s_\theta$ using score matching [A. Hyv\"arinen (2005)]. This is a method of comparing the expected l2 norm (Euclidian distance) between the training data score function and approximation from the neural network. 
\begin{equation}
\frac{1}{2}\mathbb{E}_{p_{data}(\mathbf{x})} \Big[ ||s_\theta(\mathbf{x}, \sigma)
- \nabla_{\mathbf{x}} \log p_{data}(\mathbf{x})||_2^2  \Big]
\end{equation}
This is commonly referred to as the Fisher Divergence. It can be shown [Song, Ermon] that this equation is equivalent to this equation: 
\begin{equation}
\mathbb{E}_{p_{data}(\mathbf{x})} \Big[ 
\mathrm{tr}(\nabla_{\mathbf{x}} s_\theta(\mathbf{x}, \sigma))
+ \frac{1}{2}||s_\theta(\mathbf{x}, \sigma)||_2^2  \Big]
\end{equation}
This gives us the benefit of much easier calculation. However the high dimensionality of image data and the required depth of the neural network that even this is still too inefficient. 

Finally, we turn to sliced score matching. Here we project the score functions onto 1D space to compare. By necessity, a matching score function must agree across all possible projections. 

Can be calculated using a constant number of backpropagations [Martens, Sutskever, Swersky (2012)]. This is the loss used  when training. This is also the Sliced Fisher Divergence. 
\begin{equation}
\mathbb{E}_{p_{data}(\mathbf{x})} \Big[ 
\mathbf{v}^\intercal \nabla^2_{\mathbf{x}} \log p_\theta(\mathbf{x}) \mathbf{v}
+ \frac{1}{2} (\mathbf{v}^\intercal \nabla_{\mathbf{x}} \log p_\theta(\mathbf{x}) )^2  \Big] + \mathrm{const}
\end{equation}
Sliced score matching projects the scores onto 1D space and compares them. We do this for many random project vectors $\mathbf{v}$ to get an accurate expectation value. 
% https://www.jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf
% https://arxiv.org/abs/1206.6464


Despite the versatility of score matching, the approximated score function is inaccurate in  low-density regions, i.e. regions lacking training images. These regions comprise the majority of our $\mathbb{R}^{64\times64}$ pixel space due to the manifold hypothesis [Belkin, Niyogi (2004)], which states that high-dimensional real-world datasets generally exist on a lower-dimensional manifold. This is especially true of our dataset due to the inherent sparsity of LArTPC event images. We resolve this problem by applying Gaussian noise to the images before determining the score function. This diffuses the training data to fill the low-density regions in pixel space. However, images generated using this score function would produce noisy images. Fortunately, we can resolve this dilemma by applying an annealed version of Langevin dynamics during the generation phase.
% Is it clear why the score function generates noisy images? 
% Manifold Hypothesis: https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf

Langevin dynamics [] is a Monte Carlo simulation method whereby a 'particle' travels along the score vectors to local minimums. This can be thought of as stochastic gradient descent. In the annealed version we simply pause after each step and replace the score function learned from less  noise. This means we must learn and remember several instances of the score function $s_{\theta}(\mathbf{x},\sigma)$, dependent on the standard deviation $\sigma$ of the Gaussian noise applied to the training images. 

By gradually decreasing the noise we can maintain a continuous path of accurate score functions for the traversal through the pixel space. The 'particle' is initialized randomly, so we do not have to concern ourselves with it starting in a high-density region according to the manifold hypothesis. 

Decreasing noise scales. 

Thus we can maintain an accurate score function for the entirety of the traversal through the pixel space. This process is repeated for each sampling, wherein the final location of the 'particle' corresponds to a newly generated image. 



