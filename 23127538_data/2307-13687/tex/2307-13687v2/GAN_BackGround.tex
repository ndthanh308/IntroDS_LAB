\subsection{Background on Generative Models} 
\label{sec:back}


There are two ways to specify a probability distribution, viz., explicit vs implicit. 
In explicit models, a parametric form of distribution is specified, say $\mathsf{P}_{\bm{X}}(\bm{x}; \Theta)$, where $\Theta$ is set of parameters. 
In implicit models, the main idea is that if a random variable $\bm{X}$ has distribution $\mathsf{P}$\footnote{We make no notational distinction between specifying density vs mass function vs cumulative distributions - it is unambiguously inferred from the context.}, this distribution is implicitly specified via a transformation. 
That is, $\bm{X} = G(\bm{Z})$ where $G$ is a map and $\bm{Z} \sim \mathsf{P}_{\bm{Z}}$.
Given $G, \mathsf{P}_{\bm{Z}}$ it is possible to \textit{compute} $\mathsf{P}_{\bm{X}}$, but it is hard when $G$ is complex, say a deep neural network, and especially when $\mathsf{P}_{\bm{Z}}$ is assumed to be \textit{simple} and lower-dimensional compared to $\bm{X}$. On the other hand, this allows one to generate IID \textit{samples} from $\mathsf{P}_{\bm{X}}$ via IID samples from $P_{\bm{Z}}$. 
Hence the name generative modeling. To simplify exposition, as it is standard in related literature, the distribution associated with $G(\bm{Z})$ when $\bm{Z} \sim \mathsf{P}_{\bm{Z}}$ is denoted $G_{\#}\mathsf{P}_{\bm{Z}}$ - and is referred to as the \textit{pushforward} of $\mathsf{P}_{\bm{Z}}$ under $G$. 

Popular explicit models are Normalizing Flows (NF)~(\cite{NF_Review, papamakarios2019normalizing}) or the pixel-CNN models~(\cite{van2016conditional, salimans2017pixelcnn++}). 
These approaches provide a computable characterization of density of $\bm{X}$ starting with the density of $\bm{Z}$, and hence provide a mechanism to generate data as well as data likelihoods. 
However, these approaches may not be suitable when the true density has low intrinsic dimensions.
% \textcolor{red}{Need a few lines here about NAF etc. And also mention pixel CNN as well in a concrete manner as a class of parameterized auto-regressive models for generating the data.}\\
% However, the working assumption in most of these numerically feasible explicit models that the true data distribution is absolutely continuous with respect to the Lebesgue measure in ambient dimensions - in other words the distribution is not concentrated on a thin set, which may not be true for images, such as LArTPC images, which tend to concentrate on low dimensional manifolds. 

The Variational Auto Encoders (VAE)~(\cite{kingma2019introduction}) and its many variants, provide a generative model for the data via a latent space distribution (typically in low dimensions) and the learned decoder. For ease of exposition we consider the following set up from~\cite{bousquet2017optimal}. 
Let $\mathsf{P}_{\bm{Z}}$ be a latent distribution that we want to \textit{enforce} and let $E,D$ be two mappings, $E: \mathcal{X} \rightarrow \mathcal{Z}$ - called the encoder and $D: \mathcal{Z} \rightarrow \mathcal{X}$ called the decoder. Given this set-up, a variation of VAE attempts to learn $E,D$ via,
\begin{align}
    \arg \min_{E,D} [\mathbb{E}_{\bm{X}} \ell(\bm{X}, D\circ E(\bm{X}))] + \lambda KL(E_{\#} \mathsf{P}_{\bm{X}}|| \mathsf{P}_{\bm{Z}}).
\end{align}
Here $KL$ denotes the Kullback-Liebler (KL) divergence~(\cite{cover1999elements}).
A generative model is then $D_{\#} \mathsf{P}_{\bm{Z}}$, i.e. $G = D$ from the VAE. One can have other discrepancy measures to enforce the prior on the latent and in addition impose other regularizations that are application specific.

% - governing factors being how one can best approx. these from data and variance in these estimates. 
% Starting with two units, a probabilistic encoder characterized by $P^{(E)}_{\bm{Z}|\bm{X}}$ and a probabilistic decoder characterized by $P^{(D)}_{\bm{\hat{X}}|\bm{Z}}$, and a given \textit{prior} $\mathsf{P}_{\bm{Z}}$ on the latent, the VAE can be seen to solve for - \textcolor{red}{Need to find a clear exposition without too much details...}
% \begin{align}
%     \min_{E,D} \mathbb{E}_{\bm{Z}, \bm{X}} [KL(P^{(E)}_{\bm{Z}|\bm{X}} ||\mathsf{P}_{\bm{Z}}) + \lambda \log (P^{(D)}_{\bm{\hat{X}}|\bm{Z}})]
% \end{align}

In \cite{goodfellow2014generative}, an approach termed as Generative Adversarial Networks (GANs) was proposed. 
In its original formulation, one considers two maps, the generator map $G$ and an \textit{adversary} map $A: \mathcal{X} \rightarrow [0,1]$. 
Essentially $A$ evaluates the likelihood that data, whether real or fake comes from the right distribution. 
Then one solves for,
\begin{align}
    \min_{G} \max_{A \in \mathcal{A}} \mathbb{E}_{\bm{X}, \bm{Z}}\left[\log A(\bm{X}) + \log ((1 - A\circ G(\bm{Z})) \right], 
\end{align}
where $\mathcal{A}$ is a given class of functions, typically parameterized via deep neural networks. 
% From the point of view of game theory, in the above set-up, the Generator $G$ plays first and therefore the Adversary $A$ is given more power.
% Note that one cannot generally swap the min and the max and expect the same behavior. This original formulation faced a lot of challenges both numerically and technically, especially when the data was multimodal and/or concentrates on a low-dimensional manifold in high ambient dimensions, e.g. images. 
To alleviate some issues with this seminal approach, GANs based on Integral Probability Metrics (IPMs) were proposed, e.g. Wasserstein-1 GANs~(\cite{arjovsky2017wasserstein, gulrajani2017improved}) and Maximum Mean Discrepancy GAN~(\cite{li2017mmd}). 
A general IPM based GAN can be written as, 
\begin{align}
    \min_{G} \underset{IPM(G_\#\mathsf{P}_{\bm{Z}}, \mathsf{P}_{\bm{X}}, \mathcal{A})}{\underbrace{\max_{A: A \in \mathcal{A}} \mathbb{E}_{\bm{Z}} [A \circ G(\bm{Z})] - \mathbb{E}_{\bm{X}} [A(\bm{X})]}},
\end{align}
where the adversary is restricted to a (symmetric) class of functions $\mathcal{A}$ and typically this class is approximated via neural networks. For e.g. in MMD GANs these are given by the unit ball of a Reproducing Kernel Hilbert Space (RKHS), in Wasserstein-GANs these correspond to the class of Lipschitz-1 functions. 

% The min-max optimization problem over large data sets is still challenging and, training and regularization of GANs still remains an important topic of investigation. 

It is \emph{quite natural to couple the autoencoder approach with the implicit GAN-like approach or the explicit NAF-like approach}. This is the approach that is undertaken in many recent papers, see e.g.~(\cite{an2019ae, van2017neural}) and we will employ one such approach called as VQ-VAE proposed in~\cite{van2017neural}. In particular, given an AE with with encoder-decoder $E,D$, combined with learning a generative $G$ model for the latent space vectors, one can learn an overall generative model as the composition $D\circ G(\bm{Z})$, $\bm{Z} \sim \mathsf{P}(E(\bm{X}))$. \textcolor{red}{Quite intriguingly, VQ-VAE, one actually learns a hybrid model - The pixel CNN is an \textit{explicit model}, while the Decoder part is \textit{implicit}. Although we don't explicitly show it here, in our experience this hybrid approach was the most successful among the ones we tried so far.}