\section{Additional details on Methods}

\subsection{VQ-VAE Implementation Details}
\label{sec:vqvae_details}

\quad The VQVAE's structure is best explained as a two-part network. 
The first network is much like a classical autoencoder, the caveat being that vector quantization is applied to the latent codes, 
and that extra losses are computed to optimize the quantization. 
The second network is an autoregressive generative model that generates latent vectors one component at a time.\par
For the encoder and decoder, we use convolutional and deconvolutional neural networks respectively. 
The CNN's are arranged into blocks. Each block contains a convolutional layer with ReLU activation, a batch-normalization layer, and finally another ReLU activation. Our experience has been that ReLU-BN-ReLU blocks produce sets of quantization vectors that lead to more realistic generation (compared to just BN-ReLU). This should be investigated in future work.
In the convolutional encoder, all blocks except the last downsample the input image by a factor of two. 
In the deconvolutional decoder, all blocks except the first upsample the image by a factor of two.
Encoder outputs are fed through a vector quantization layer before being passed to the decoder. \par
For the autoregressive model used to model the distribution of quantization vectors over the latent vector,
we employ a masked and gated PixelCNN model. 


% Figure environment removed

\subsection{PixelCNN Implementation Details}

Our PixelCNN network is composed of six gated, masked, convolutional blocks. 
Each block is composed of a horizontal and vertical stack. 
Each stack is composed of masked convolution, a gating layer, and a residual layer. 
Information from the vertical stack is passed to the horizontal stack. 
For the vertical stack, gating occurs after the residual layer, 
while in the horizontal stack gating occurs first. \par
The convolutional blocks are followed by two convolutional layers with an amount of output filters equal to the number of quantization vectors. 
The activations are passed into a Softmax function to approximate the likelihood of each quantization vector at that component of the latent code.\par Our VQVAE implementation is based on the works of Ken Leidal and Amelie Royer, 
which can be found at \url{https://github.com/kkleidal/GatedPixelCNNPyTorch} 
and \url{https://github.com/ameroyer/ameroyer.github.io} respectively.

% Figure environment removed


% % Figure environment removed


\subsection{Training Data Details}
\label{subsec:genmodel_training_data}

We used publicly available examples of LArTPC images produced by
the DeepLearnPhysics collaboration~\cite{dlpdata}. 
Among the available data from this source,
we used the 50k single-particle image data set to train the VQ-VAE.
We use the separate 40K single-particle image 
data set to test the reconstructions of the VQ-VAE.
Images in both the train and test set consist of a 256x256 image~\cite{dlpsingle}.
The particle generated for each image is chosen among five species: $e^-$, $\gamma$, $\mu^-$, $\pi^+$, and protons ($p$).
The momentum of each particle is chosen from a uniform distribution between the following ranges 
($e^-$) 35.5 to 800 MeV/c, 
($\gamma$) 35 to 800 MeV/c, 
($\mu^-$) 90 to 800 MeV/c,
($\pi^+$) 105 to 800 MeV/c, and
($p$) 105 to 800 MeV/c.
The particle is simulated inside a large volume of argon.
It is propagated via Geant4. Afterwards,  a 2.56 m$^3$ box is chosen in 3D that
maximizes the particle's trajectory within the volume and recorded in the file.
The 2D images are created as 2D projections ($xy$, $yz$, $zx$) of the 3D charge depositions.
All images included in the data set are guaranteed to have 2D projection images with at least 10 non-zero pixels.
We use the $zx$ projections.

\subsection{Generator Model Training Procedure Details}

% \textcolor{red}{Needs meta parameter details.}


\begin{table}[h!]
\centering
    \begin{tabular}{c c c c c}
    Num. Quantization Vectors  & Quant. Vector Dim.  & Enc. Filters & Dec. Filters.  & Batch Size\\
    \hline
    512 & 8 & [16, 32] & [32, 16] & 512
\end{tabular}
\caption{VQVAE Meta Parameters}
\label{tab:VQVAE Metaparameters}
\end{table}
\begin{table}[h!]
\centering
    \begin{tabular}{c c c}
    Num. PixelCNN Blocks & Num. Filters per Block &  Batch Size\\
    \hline
    6 & 128 & 512
\end{tabular}
\caption{PixelCNN Meta Parameters}
\end{table}

We train the network in two parts. 
First, the autoencoder is trained on a set of fifty thousand, 64x64, single channel LArTPC images. 
Training is conducted with a  batch size of 512 images and with the Adam optimizer initialized with a learning rate of 3e-4.\par
For a single training loop, three losses are computed. 
First is the mean squared error between the reconstructed image and the true image. 
This loss is propagated through the decoder and encoder. 
Because the vector quantization process contains an argmin operation, 
which gradient can not pass through, 
gradients are copied from the beginning of the first layer of the decoder to the last layer of the encoder.\par
Bypassing the quantization layer means that the codebook must be learned independently of the reconstruction error. 
We use an L2 loss between the encoder's unquantized outputs and quantized outputs (with a stop-gradient operation applied to the unquantized outputs) to learn the quantization vectors. 
To ensure that the encoder commits to a specific set of quantization vectors,
a second L2 loss is introduced between the unquantized outputs and the quantized outputs, 
with the stop-gradient being applied this time to the quantized outputs.\par
The PixelCNN network is trained using the cross entropy loss between the quantization vector predictions and the true quantization vector. We use the Adam optimizer and initialize it with a learning rate of 1e-3.




% \section{Validation of PixelCNN training}

% \textcolor{red}{If we are going to have this section, this would contain Paul's  likelihood distribution comparisons.}

% % Figure environment removed

\subsection{Training of the Track/Shower labeling model}
\label{sec:ssnet_details}

The quality of the generated images is quantified using a convolutional
semantic segmentation network (SSNet) modeled after the network described in
~\cite{abratenko2020semantic}.
The architecture of the track/shower semantic segmentation network
is the same in~\cite{abratenko2020semantic} with the one exception being that
dense convolutions are used instead of sparse submanifold convolutions.
The network is structured as a U-Net with ResNet layers.
Four pairs of down-sampling and up-sampling layers are combined
before a convolution layer outputs three classes: background, shower, and track. 

The images used to train the track/shower network
is related to the data used to train the generative network.
The two data sets were created using the same traditional simulation chain.
The track/shower data set is different
because it contains the pixel-wise truth labels.
The training sample consisted of 15k 256x256 examples.
A test set with 10k was used to monitor the training of the network for over-fitting.
The network was trained using Adam with a batch size of 16, momentum of 0.9, and a weight-decay of 1.0e-4.
The network was trained for 20 epochs with a starting learning rate of 1.0e-3.
The learning rate was cut in half every 4 epochs.
No significant difference in train versus test set loss and accuracy was seen, 
so the last saved checkpoint is used for the studies in this work.

Data augmentation techniques were applied to the training set. 
This included flipping the image along the horizontal and vertical axis, 
transposing the image, and scaling the pixel values across an individual 
images by a random value between 0.90 and 1.1 drawn uniformly.
The pixel-wise classification accuracy after training was 98.5\% per pixel.


\section{Additional SSNet Study}

% \subsection{SSNet output versus PixelCNN training epoch}

As described in the main text, the output of SSNet is
used as a metric to  evaluate and compare image quality.
One validation study we did for this metric was to compare the SSNet output
on generated samples produced by the same model after several epochs of training.
Our expectation is that the comparison to the output on test images should
improve with increasing epoch.
Figure~\ref{fig:ssnet_vs_epoch} shows two distributions. 
The first is a distribution of the number of pixels labeled as shower or track per image.
The second is a distribution of the class score for pixels above threshold.
The first tells us how often pixels within patches with shower-like or track-like features are produced.
The second tells us how confident the network is that the pixel is part
of a region that matches track or shower features.
Both plots compare distributions computed for generated images for three successive
PixelCNN training checkpoints to the distribution 
computed over test images (i.e. not generated). 
We find that the generated distributions better match the test distribution
as the model is trained longer.
In order to provide a score to see improvement, we calculate the KL divergence between the generated distributions and the test distribution.

% Figure environment removed

\begin{table}[]
%% Updated with final_k512_p1-p4
\centering
    \begin{tabular}{c|c c c c}
    Checkpoint  & Num. Shower Pix.  & Shower Score & Num. Track Pix.  & Track Score\\
    \hline
    \#1: 100 epochs  & 0.36 & 0.22 & 0.41 & 0.21 \\
    \#2: 200 epochs  & 0.42 & 0.19 & 0.22 & 0.19\\
    \#3: 300 epochs  & 0.37 & 0.17 & 0.22 & 0.17\\
    \#4: 400 epochs  & 0.24 & 0.13 & 0.18 & 0.13
\end{tabular}
\caption{KL-divergence between SSNet output distributions calculated 
from generated images versus test images. 
Smaller values are better.
This table shows the metric for successive PixelCNN training checkpoints.}
\label{tab:ssnet_score_vs_epoch}
\end{table}


% \subsection{SSNet output versus number of VQ-VAE code-vectors, $k$}

% We compared the image quality for those generated using a VQ-VAE with
% two different values for the number of quantized vectors, $k$: 256 and 512.
% KL-divergence of the $k=512$ models were lower for the different SSNet distributions we checked. 
% For the number of track pixel distribution, the KL-divergence was
% 0.18 and 0.21, for the $k=512$ model and $k=256$ model, respectively. 
% For the number of shower pixel distribution, 0.24 versus 0.55;
% for the track score  distribution, 0.13 versus 0.22; 
% and for the shower score distribution, 0.13 versus 0.23.


% % Figure environment removed

% We performed an experiment to 

\section{Publicly available code, models, and generated image sets}
\label{sec:publiccode}

The code implementing all the models discussed here can be found on github at \texttt{https://github.com/NuTufts/LArTPC-VQVAE}. 
Model weights for the VQ-VAE, PixelCNN,
and track/shower semantic segmentation network
will be uploaded to Zenodo. 
A sample of generated images are also provided on Zenodo.
% The VQ-VAE encoder and decoder weights are at X.
% The PixelCNN model weights are at X.
% Finally, we provide a 10k-image sample of generated images.
% The generated sample can be downloaded at X.

% The URL is not given here in order to preserve blindness. at \texttt{github.com/NuTufts/LARTPC-VQVAE}.

\section{Additional Example Images}
\label{sec:additional_examples}

In this section, we provide additional image samples to view.

% \afterpage{%
% % Figure environment removed
% \clearpage
% }

% \afterpage{%
% % Figure environment removed
% \clearpage
% }

\afterpage{%
% Figure environment removed
\clearpage
}

\afterpage{%
% Figure environment removed
\clearpage
}

%\DeclareCaptionLabelSeparator{}{}

% The \nocite command causes all entries in a bibliography to be printed out
% whether or not they are actually referenced in the text. This is appropriate
% for the sample file to show the different styles of references, but authors
% most likely will not want to use it.
%\nocite{*}
