\documentclass[%
 reprint,
%superscriptaddress,
%groupedaddress,
%unsortedaddress,
%runinaddress,
%frontmatterverbose, 
%preprint,
%preprintnumbers,
%nofootinbib,
%nobibnotes,
%bibnotes,
 amsmath,amssymb,
 aps,
%pra,
%prb,
%rmp,
%prstab,
%prstper,
%floatfix,
nofootinbib, %Fix footnotes 
]{revtex4-2}

%\usepackage{iclr2021_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn, xcolor}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage{subcaption}
\usepackage{float}
\usepackage{makecell}
\usepackage{afterpage}
\usepackage{tabularx}
\usepackage{comment}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{accents}
% \usepackage[numbers]{natbib}

\usepackage{balance}

\makeatletter
\DeclareRobustCommand{\cev}[1]{%
  \mathpalette\do@cev{#1}%
}
\newcommand{\do@cev}[2]{%
  \fix@cev{#1}{+}%
  \reflectbox{$\m@th#1\vec{\reflectbox{$\fix@cev{#1}{-}\m@th#1#2\fix@cev{#1}{+}$}}$}%
  \fix@cev{#1}{-}%
}
\newcommand{\fix@cev}[2]{%
  \ifx#1\displaystyle
    \mkern#23mu
  \else
    \ifx#1\textstyle
      \mkern#23mu
    \else
      \ifx#1\scriptstyle
        \mkern#22mu
      \else
        \mkern#22mu
      \fi
    \fi
  \fi
}

\makeatother

% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% tables
\usepackage{array} 
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}

\usepackage{xcolor}
\usepackage{bbm}
% Comments and revisions
\newif\ifcomments
\commentstrue
% \commentsfalse
\ifcomments
    \usepackage{ulem}
    \normalem
    \def\twcomment#1{{$\!$\color{magenta} [TW: #1]}}
    \def\twedit#1#2{{\color{red}{\sout{#1}}}{{\color[rgb]{0.0,0.5,1.0}#2}}}
%
    \def\sacomment#1{{$\!$\color{blue} [SA: #1]}}
    \def\saedit#1#2{{\color{red}{\sout{#1}}}{{\color[rgb]{0.5,0.5,1.0}#2}}}
%
    \def\zicomment#1{{$\!$\color{teal} [ZI: #1]}}
    \def\ziedit#1#2{{\color{red}{\sout{#1}}}{{\color{teal}#2}}}
%    
\else 
    \def\twcomment#1{}
    \def\twedit#1#2{#2}
%    
    \def\sacomment#1{}
    \def\saedit#1#2{#2}
%    
    \def\zicomment#1{}
    \def\ziedit#1#2{#2}
\fi


\begin{document}

% ----------------------------


\title{Score-based Diffusion Models for Generating Liquid Argon Time Projection Chamber Images}% Force line breaks with \\
%\thanks{A footnote to the article title}%

\author{Zeviel Imani}
  \email{zeviel.imani@tufts.edu}
\author{Taritree Wongjirad}
  \email{taritree.wonjirad@tufts.edu}
\affiliation{Department of Physics and Astronomy, Tufts University, Medford, Massachusetts\\
The NSF AI Institute for Artificial Intelligence and Fundamental Interactions\\
}

\author{Shuchin Aeron}
\email{shuchin.aeron@tufts.edu}
\affiliation{
Department of Electrical and Computer Engineering, Tufts University, Medford, Massachusetts\\
The NSF AI Institute for Artificial Intelligence and Fundamental Interactions\\
}

% ----------------------------

\begin{abstract}
% We show for the first time, high-fidelity generation of LArTPC-like data using a generative neural network.
% This demonstrates that methods developed for natural images do transfer to LArTPC-produced images which in contrast to natural images are globally sparse, but locally dense.
% % This opens the door to future opportunities in novel data-driven \sacomment{shall we clarify here that this analysis is the physics analysis?} analysis methods that make use of such models.
% %\twcomment{I have ideas, such as the iterative evaluation described in the VQ-VAE paper or systematic image mods to interrogate classifiers, but I think they'd be considered a little unfamiliar, so I suggest we drop this line. There is the use of the generative model as a fast surrogate for traditional sims.}
% We present the method we employ, which is a variant of score-based generative diffusion models. We evaluate the fidelity of the generated images using several different approaches that include using a variant of measures used to evaluate natural images, comparisons between high-dimensional distributions, and comparisons relevant to LArTPC experiments.
For the first time, we show high-fidelity generation of LArTPC-like data using a generative neural network. 
This demonstrates that methods developed for natural images do transfer to LArTPC-produced images, which, in contrast to natural images, are globally sparse but locally dense. 
We present the score-based diffusion method employed. 
We evaluate the fidelity of the generated images using several quality metrics, including modified measures used to evaluate natural images, comparisons between high-dimensional distributions, and comparisons relevant to LArTPC experiments.
\end{abstract}

\maketitle

% ----------------------------


\section{Introduction}

%% The point of the introduction is to tell us why the readers should read this paper.
%% What did we do? 
%% Why did we do it? 
%% Why they should care?
%% - Make a generative network for small lartpc images using diffusion networks
%% - lartpcs play an important role in neutrino physics. 
%% - gen networks open a way towards generating latpc images that avoids the large time simulating events.

The Liquid Argon Time Projection Chambers (LArTPC)~\cite{rubbia1977liquid,chen1976p496} is a particle detector technology utilized in several current and future neutrino experiments~\cite{amerio2004design,anderson2012argoneut,acciarri2017design,acciarri2015proposal,abi2018dune}.
Their wide use in experimental neutrino physics derives from their ability to scale to sizes with length dimensions of tens of meters %their versatility 
while still being able to resolve the three-dimensional location of charged particle trajectories to the several-millimeter scale.
% This fine-detail of neutrino interactions that can be captured by LArTPCs is what is required of experiments in order to push the

The active portion of a LArTPC consists of a time-projection chamber (TPC) wherein ionization electrons created by charged particles are drifted toward charge-sensitive devices via an electric field created across the TPC volume.
The charge-sensitive devices include planes of sense wires or readouts capable of recording the 2D locations of ionization~\cite{dwyer2018larpix,asaadi2020first}. 
Modeling the processes through which the ionization eventually leads to waveforms in the readout electronics is an important and challenging task~\cite{adams2018ionization}.  
One challenge is that the model can have long execution times. For example, simulating an event in the MicroBooNE LArTPC can take upwards of 10-15 minutes per event when including trajectories from cosmic ray particles and particles produced by neutrino interactions.
In this paper, we provide proof-of-principle experiments that demonstrate how diffusion models (DM), a class of machine learning algorithms, can serve as an alternative to a full detector simulation.

% Related work paragraph (add more citations?) 
Currently, DMs are attracting a lot of attention due to their ability to generate novel images with high fidelity and large variance.
These models are being used for a large variety of image types, including natural images~\cite{rombach2022high}, paintings~\cite{yi2021exploring}, or drawings~\cite{peng2023difffacesketch}.
When asked to generate novel images, they produce samples that match closely to the style and subjects of the images from the training set.
They also produce samples that vary over the different semantic content included within the training set.
In HEP, diffusion models have primarily been used in collider physics for jet unfolding ~\cite{leigh2023pc, diefenbacher2023improving, butter2023jet}, and calorimeter data generation ~\cite{vinicius2022Calorimeter, buhmann2023caloclouds, acosta2023comparison}. 
This is not intended as an exhaustive list. 
Given their versatility DMs will likely find use in many other applications. 

In our work, we explore how well DMs can capture the features, 
both at high and low distance scales, 
found within images containing particle trajectories.
Such images are very different from those for which DMs were developed. %TODO: how so? 
Furthermore, the way we measure the quality of the images produced by DMs for LArTPCs
will differ from those for natural images.
In this work, we develop methods of quantification we believe are most relevant to
LArTPC images.

What we find is that the score-based method we adapted works well without the need to fine-tune meta-parameters. 
Parameters include those that define the neural network used in DMs 
and those that control aspects of the training.
Qualitatively, the images produced by the DM are quite similar to the training images and are 
a definite improvement over images produced by our past attempt using another algorithm known as a Vector-Quantized Variational Autoencoder~\cite{lutkus2022towards}.
As a first proof-of-principle exploration, 
we only train and generate relatively small images, i.e., 64x64 pixels, 
in the context of current LArTPC experiments, 
which can produce images on the scale of 1000x1000 pixels.
Our aim is to investigate if DMs can generate images of high enough quality such that DMs may one day serve as surrogate models for LArTPC detector simulations.
% Examples of both training and generated images are shown in Figure~\ref{fig:compare_training_gen}.
% DMs capable of producing high-fidelity LArTPC images also provide new approaches toward the analysis of LArTPC images.
 


 % Figure environment removed


\section{Score-based Generative Models}

The goal of generative models is to learn a probability distribution given samples (data) and to produce, i.e., generate new samples from this learned distribution. In most cases, especially when using deep networks, the model learned is an \emph{implicit} one, in that an explicit characterization of the distribution is not sought, only a means to efficiently sample. For a survey of various methods, we refer the reader to \cite{bond2021deep, yang2023diffusion}.


In our case, the data are (portions of) images that come from a LArTPC detector. In this context, we note the following points. 

\begin{itemize}
    \item Compared to the popular image datasets, the LArTPC images are \emph{extremely} sparse, and thus the latent dimensionality of the data manifold is potentially much smaller.
    Thus, the models we construct must be able to produce samples from this low-dimensional manifold where our data resides.
    \item  Furthermore, the frequency at which different semantic content, i.e., different modes of the distribution, appears must mimic that in the reference dataset.
    For example, in the context of LArTPC images, one would want to sample images that depict low-angle scattering by a muon at a much lower rate than scattering examples that reverse the direction of the trajectory as that physical scattering rate is much lower.
    \item We might also expect that the generative model can reproduce the distribution of underlying particle momenta used to prepare the reference data set.
\end{itemize}
%TODO: make bullet points more concise 

Motivated by the recent success of the Score-based Generative Models (SGM) in  \cite{song2021scorebased} that are part of a wider family of so-called diffusion models \cite{yang2023diffusion}, in this paper, we employ them for learning a generative model for the LArTPC data and evaluate its performance. On top of the empirical success, several recent works such as \cite{chen2023improved, chen2022sampling, de2022convergence} have shown that given an accurate score estimator under relatively mild conditions, SGM can provably learn to generate high-dimensional \emph{multi-modal} distributions even when the data is supported on low-dimensional manifold, a scenario that has plagued past efforts in generative modeling which are usually prone to suffering from mode collapse. However, apart from the requirement to learn good score estimators in high dimensions, diffusion models come with additional computational complexity, most notably in generating samples from the learned model \cite{zhang2022fast}. 
Below we begin by providing a brief overview of the SGMs, primarily taken from the seminal paper \cite{song2021scorebased}. 

Most generative models involve learning a transformation or a sequence of transformations, i.e., a process, between easy-to-sample distribution, e.g., the normal distribution and the data distribution.  For diffusion models, one connects the data images to the images sampled from the normal distribution
through a diffusion process where at each time step, an increasing amount of noise is added to the data images. The SGM learns to undo this diffusion process, i.e., reverse the forward noising process in a principled way. This reverse process can then be used to generate samples from the manifold of images implied by the reference data set.

To be precise, following \cite{song2021scorebased} forward diffusion process of noising the images is captured by the following Stochastic Differential Equation (SDE) starting at time $t=0$ with initial condition $\vec{\bm{X}}_0 \sim$ data to time $t= T$:
\begin{equation}
    \mathrm{d}\vec{\bm{X}}_t = \mathbf{f}(\vec{\bm{X}}_t,t)\mathrm{d}t + g(t)\mathrm{d}{\bm{W}}_t.
\end{equation}
Here $\vec{\bm{X}}_t \in \mathbb{R}^d$ is a vector-valued random variable containing the pixel values on the $d$ pixels of the image at time $t$. 
% In other words, $\mathbf{x} \in \mathbb{R}^d$ with $d$ as the number of pixels in the image. 
$\mathbf{f}(\vec{\bm{X}}_t,t)$ and $g(t)$ are referred to as the drift and diffusion functions, respectively, of the SDE, and $\vec{\bm{W}}_t$ is a standard Wiener process (aka Brownian motion) independent of $\vec{\bm{X}}_t$.

The key idea behind SGM is based on results that under mild conditions the reverse process defined by $\cev{\bm{X}}_{s} = \vec{\bm{X}}_{T-t}$~\footnote{In our notation, bold-face variables represent vector quantities while the vector arrow indicates the whether the time variable is for the forward or backward process.} is also a diffusion process~\cite{anderson1982reverse, Conforti_2022}.
Figure~\ref{fig:full_process} provides a pictorial illustration of the forward (data-to-noise) and backward (noise-to-data) processes.
The reverse SDE is given by the following equation for $s \in [0, T]$ (which, following \cite{chen2023improved}, we write in more transparent forward way):
\begin{multline}
   \label{eq:gen_sde}
    \mathrm{d}\cev{\bm{X}}_{s} = [ -\mathbf{f}(\cev{\bm{X}}_{s},T - t) +
    g^2(T - t)\nabla \log p(\cev{\bm{X}}_{s}) ]\mathrm{d}s \\
    + g(T -t)\mathrm{d}{\bm{W}}_s,
\end{multline}
where ${\bm{W}}_s$ is a standard Wiener process and $p(\cev{\bm{X}}_s)$ is the probability density function for the (backward) process at time $s \in [0, T]$. Note that $p(\cev{\bm{X}}_s) = p(\vec{\bm{X}}_{T-t})$. 

Given the parameters of the forward diffusion process, namely the drift and diffusion functions, realizing the reverse diffusion requires an additional quantity $\nabla \log p(\vec{\bm{X}}_t)$, referred to as the score function \cite{hyvarinen2005estimation}. While we do not have access to this directly (since we only have access to samples from the data distribution) this can be estimated via a time-dependent score estimator denoted, $\mathbf{s}_{\bm{\theta}}(\vec{\bm{X}}_t,t)$ that is parameterized via a deep network with parameters $\bm{\theta}$ -- hence the name Score-based Generative Model (SGM). In particular, in \cite{song2021scorebased} one solves for:
% \begin{widetext}
% \begin{equation}
%     \label{eq:lossfn}
%     \bm{\theta}^* = \argmin_{\bm\theta} \int_{0}^{T} \Bigl\{ \lambda(t) \mathbb{E}_{\vec{\bm{X}}_0} \mathbb{E}_{\vec{\bm{X}}_t|\vec{\bm{X}}_0} \Big[ \big{|}\big{|} \mathbf{s}_{\bm{\theta}}(\vec{\bm{X}}_t,t) - \nabla \log p(\vec{\bm{X}}_t \, | \, \vec{\bm{X}}_0 ) \big{|}\big{|}^2_2 \Big] \Bigr\} dt.
% \end{equation}
% \end{widetext}
\begin{multline}
    \label{eq:lossfn}
    \bm{\theta}^* = \argmin_{\bm\theta} \int_{0}^{T} \Bigl\{ \lambda(t) \mathbb{E}_{\vec{\bm{X}}_0} \mathbb{E}_{\vec{\bm{X}}_t|\vec{\bm{X}}_0} \Big[ \\
    \big{|}\big{|} \mathbf{s}_{\bm{\theta}}(\vec{\bm{X}}_t,t) - 
    \nabla \log p(\vec{\bm{X}}_t \, | \, \vec{\bm{X}}_0 ) \big{|}\big{|}^2_2 \Big] \Bigr\} dt.
\end{multline}

The expectation with respect to $\vec{\bm{X}}_0$ in \eqref{eq:lossfn} can be approximated with respect to the empirical distribution of $\vec{\bm{X}}_0$, which is precisely supported on the given dataset of images. For the inner expectation, for specific choices of $f(\bm{x}, t)$ and $g(t)$, one can explicitly write down the \emph{closed} form of $\log p(\vec{\bm{X}}_t|\vec{\bm{X}}_0)$. The outermost expectation in \eqref{eq:lossfn} is with respect to a distribution over the time window $[0, T]$ and $\lambda(t)$ is a positive weight function that can be used to normalize or modulate the relative importance of scores estimated at different times. Further details of the choice of the forward SDE, the choice of $\bm{\theta}$ for the score estimator, as well as time discretization of the forward and reverse SDEs and weighting, is discussed in section \ref{sec:training_SGM}.

% \saedit{Specifically,} in order to train for the parameters of the (score) model, we have the network predict the score for a batch of examples images that have undergone the forward diffusion process. When the batch of images is constructed,
% the amount of time, $t$, each image has undergone the diffusion process 
% is chosen between an interval at random.
% Using such example images, we can optimize for the values of the model parameters such that
% \begin{widetext}
% \begin{equation}
%     \label{eq:lossfn}
%     \bm{\theta}^* = \argmin_{\bm\theta} \mathbb{E}_{t} \Bigl\{ \lambda(t) \mathbb{E}_{\mathbf{x}(0)} \mathbb{E}_{\mathbf{x}(t)|\mathbf{x}(0)} \Big[ \big{|}\big{|} \mathbf{s}_{\bm{\theta}}(\mathbf{x}(t),t) - \nabla_{\mathbf{x}(t)} \log p_{0t}(\mathbf{x}(t) \, | \, \mathbf{x}(0) ) \big{|}\big{|}^2_2 \Big] \Bigr\}.
% \end{equation}
% \end{widetext}
% In practice, the above mean-squared error between the true and predicted score serves as the loss function we must compute per training iteration.
% The mean is taken over the sample of times, $t$, the training images, $\mathbf{x}(0)$, and the noise-added images, $\mathbf{x}(t)$.
% The function, $\lambda(t)$, is a positive weight function that can be used to normalize or modulate the relative importance of scores estimated at different times.
% Because we know the perturbation to the original images from the forward diffusion process for each image in the training, the true score for each image in the batch, $\nabla_{\mathbf{x}(t)}\log p_{0t}(\mathbf{x}(t) \,| \, \mathbf{x}(0))$, can be calculated and compared to the predicted score.

%% This is where I think what I am describing is located: https://github.com/NuTufts/score_sde/blob/0acb9e0ea3b8cccd935068cd9c657318fbc6ce4c/losses.py#L65





\section{Data Employed}

We construct 64x64 crops of particle trajectories from images found in the public PILArNet 2D dataset~\cite{adams2020pilarnet}.
This is a public dataset of LArTPC-like 2D images and 3D voxelized data.
The data is made using \textsc{Geant4} to simulate the transport of one or more particles within a volume of liquid argon.
The location of ionization created by the particles is recorded within the volume.
It is important to note that the 2D images are not made using a full detector simulation
that models signals on a readout wire plane such as those found in current LArTPC experiments.
We are not aware of such a model that is available outside of one of the existing LArTPC collaborations.
Instead, images are made by simply projecting the 3D locations of ionizations deposited within a volume onto the XY, XZ, and YZ planes.
However, a simple model of charge diffusion is applied as the ionization deposits are projected to the planes.
This increases the longitudinal and transverse spatial distribution of the deposits.
We use the XZ projections in our studies.

The particles transported to make the images are chosen at random from
electrons, photons, muons, protons, and charged pions.
The starting location of the particles are chosen at random uniformly in the volume.
The momentum of the particles generated are sampled using a uniform distribution whose bounds are particle dependent. Further details on this can be found in Ref.~\cite{adams2018ionization}.
We do not make any cuts on the data set using particle types or momenta.
% TODO: Generated phrasing 

From the projected images, 64x64 crops are made.  
The locations of crops are chosen to ensure a minimum number of pixels 
above a certain threshold in order to
avoid empty images.
The pixels are then whitened.
A common bias value is subtracted from all the pixels across the data set and then scaled to keep values between -1 and 1. %Zev: Normalized or scaled? Normalized to be between 0 and 255? 
Finally, the values were rescaled between 0 and 255 and converted to png files for convenience. 
We prepared a total of 50,000 images for the training dataset. 
An additional 10,000 images were reserved as a validation sample.
% The images were then compiled into a TensorFlow dataset. 
% Our prepared datasets are available on Zenodo~\footnote{\url{https://zenodo.org/record/8300355}} in order for others to reproduce our results and compare results. 
We have made our datasets publicly available on  Zenodo\footnote{\url{https://zenodo.org/record/8300355}} for reproducibility and to encourage comparisons with alternate methods. 
% Details of the cropping algorithm, normalization, and preparation are recorded in Appendix~\ref{appendix:dataprep}.

% Zev: I think this is a sufficient explanation of our dataset, so an appendix reference is not necessary. Also, I don't know the specifics of the data.


\section{Training the Model}
\label{sec:training_SGM}

The choice of forward SDE: Within the SGM framework, one can make different choices for the form of the drift and diffusion functions. In this work, we used what is called a variance preserving SDE (VPSDE) in \cite{song2021scorebased} whose SDE is
\begin{equation}
\label{eq:vpsde}
    \mathrm{d} \vec{\bm{X}}_t = -\frac{1}{2}\beta(t) \vec{\bm{X}}_t \mathrm{d}t + \sqrt{\beta(t)}\mathrm{d}\vec{\bm{W}}_t.
\end{equation}
The function $\beta(t)$ is a time-varying function used to control the amount of noise added at time $t$.

We again follow~\cite{song2021scorebased} and use a linear function in $t$ that varies between configurable bounds, $\beta_{min}$ and $\beta_{max}$:
\begin{equation}
\label{eq:beta}
    \beta(t) = \frac{\beta_{max}-\beta_{min}}{T}t + \beta_{min}. 
\end{equation}
For our choice of values for $\beta_{max}$ and $\beta_{min}$ (and other parameters),
please refer to Table~\ref{tab:configs}.
The function, $\beta(t)$, through its appearance in the drift term of Eq.~\ref{eq:vpsde}, serves to modulate the rate of decay of the mean expected pixel values to zero.
Given some total time interval, $T$, and sufficiently large enough $\beta(t)$ values, this process transforms an initial sample of $\bm{X}_0 \thicksim p_{\textrm{data}}$, drawn from the distribution of data images, into, $\bm{X}_T \thicksim \mathcal{N}(0,\sigma^2\mathbf{I})$. In other words, the pixels at time $T$ are assumed to have become uncorrelated. 
% \sacomment{Need to figure out if this $\sigma^2 = 1$ or not.} I believe it is.
% \saedit{now consistent with a set of samples drawn from the distribution $\bm{X} \thicksim \mathcal{N}(0,\sigma^2\mathbf{I})$}{}.
Here, $\sigma^2$ is the variance of the normal distribution and is assumed to be common between all pixels in the images. 
%These assumptions for the covariance are used in the implementation we employ. 
% However, it has been shown that these assumptions can be relaxed and instead customized or learned. \sacomment{Are we trying to say here that for some finite $T$ it will be the case that the final distribution may not be normal, but we will assume that is the case?}
% \twcomment{I recall seeing work where the covariance matrix is learned. I think that included possible correlations. But if I cannot find the citation, we should remove it.}

% As such we can think of our data as being time-dependent $x(t)$ where $t \in [0,T)$. 
Training our model requires learning the score $\nabla_{\vec{\bm{X}}_t} \log p(\vec{\bm{X}}_t)$ as a function of time.
We use batch stochastic gradient descent to optimize $\bm{s}_{\bm{\theta}}$ according to Eq.~\ref{eq:lossfn}.
For each training iteration, we sample a batch of $N$ (unperturbed) training images, $\vec{\bm{X}}_0$.
For each $\vec{\bm{X}}_0$ in the batch, a random time is sampled from a uniform distribution over the interval $[0,T]$.
Both $\vec{\bm{X}}_0$ and the time, $t$, are used to sample a training example, $\vec{\bm{X}}_t$, through the following transition kernel:
\begin{equation}
\label{eq:apply_noise}
    % \vec{\bm{X}}_t = \vec{\bm{X}}_0 e^{-\frac{1}{4}t^2\int^t_0\beta(s)ds}e^{\frac{1}{2}t\beta(0)} 
    % + \sqrt{1-e^{-\frac{1}{4}t^2\int^t_0\beta(s)ds}e^{\frac{1}{2}t\beta(0)}} * \mathcal{N}(0,1)
    p(\vec{\bm{X}}_t | \vec{\bm{X}}_0) = 
    \mathcal{N}( \vec{\bm{X}}_0 e^{-\frac{1}{2}\int^t_0\beta(s)ds}, \bm{I}-\bm{I}e^{-\int^t_0\beta(s)ds}).
    % Equation 29 in paper 
\end{equation}
Note that the kernel's specific expression is dictated by the choice of a VPSDE as the diffusion process.
By choosing a large $\beta_{max}$, the kernel is driven more quickly towards $\mathcal{N}(\bm{0},\bm{I})$.

The batch of perturbed images, along with the sampled batch of times, $t$, are passed into the score function estimator, $\bm{s}_{\bm{\theta}}(\vec{\bm{X}}_t, t)$, implemented as a convolution neural network (CNN). 
We follow the work of Song et al.~\cite{song2021scorebased} for the CNN architecture and use their Noise Conditional Score Network (NCSN). 
We chose to keep many of the default configurations used in that work
for modeling the CIFAR-10 dataset~\cite{krizhevsky2009learning}.
The full list of parameters can be found in appendix \ref{appendix:traingen_details}, table \ref{tab:configs} with our modifications highlighted.

The implementation of the loss function we use to train $\bm{s}_{\bm{\theta}}(\vec{\bm{X}}_t, \vec{t})$ follows directly from Eq.~\ref{eq:lossfn} and Eq.~\ref{eq:apply_noise}. 
Because of the marginal probability, $p(\vec{\bm{X}}_t|\vec{\bm{X}}_0)$ is Gaussian,
the gradient of the log-marginal probability has a closed-form.
Therefore, the \emph{empirical} loss calculated for a training batch of images is
% Therefore, the average loss for a batch of training images, given our implementation choices, is \sacomment{Need to check the eq. below - the right hand side should be summed over the samples $N$? In our notation, bold-face capital $\bm{X}$ are random variables and lowercase $\bm{x}$ are an instance of the random variable. Also need to specify $\lambda(t)$? See table 1 in \url{https://arxiv.org/pdf/2101.09258.pdf} and check with @Zev's code next week.}
\begin{equation}
    \label{eq:loss_in_practice}
    \mathcal{L}(\bm{\theta}) 
    = \frac{1}{N}\sum^{N}_{i}|| \bm{s}_{\bm{\theta}}(\vec{\bm{X}}^{i}_{t^{i}}, t^{i}) 
    - \frac{-(\vec{\bm{X}}^{i}_{t^{i}}-\vec{\bm{\mu}}^{i}_{t^i})}{\vec{\bm{\sigma^2}}^i_{t^i}} ||^2_2,
\end{equation}
where the sum is over a batch with $N$ samples, indexed by $i$, in the training batch. 
Each item in the batch uses a different, $t^i$, sampled from a uniform distribution over the interval $t \in (0,T]$.
The values of $\vec{\bm{\mu}}_t$ and $\vec{\bm{\sigma^2}}_t$ are mean and variance in the Normal distribution defined in Eq.~\ref{eq:apply_noise}.
In the training configuration we used, $\lambda(t)$ from Eq.~\ref{eq:lossfn} is chosen to be simply $1$.
% TODO: Double check notation t^i 

We train the model for a designated number of steps, 
periodically saving checkpoints for later use in generating images. 
We will instead refer to training epochs from here onwards, where one epoch is defined as 
the number of training iterations to process the entire training dataset. 
% the number of training iterations needed to process a total number of images equal to the number of images in the training dataset. 
Since our batch size, $N$, is 128 images and our training dataset is 50,000 images, one epoch is equivalent to 391 training iterations. 
We trained the model for a maximum of 300 epochs. 

The losses for the training and validation sets versus epoch for the final training run are shown in Figure~\ref{fig:loss}.
The similar loss between the training and validation set as a function of epoch suggests that there is no significant over-fitting of $s_\theta$.
As will be discussed in Sec.~\ref{sec:results}, we observe that the model does not improve appreciably past epoch 50 through several metrics for evaluating the quality of generated images.
The time to train a model for 50 epochs took approximately 6 hours using two NVidia Titian 2080 RTX GPUs.

% Figure environment removed





\section{Generating Images}

After training the model, we were able to produce generated images from any of the pre-designated checkpoints. 
Generating 50,000 images from any epoch took approximately 40 hours using two NVidia Titian 280 RTX GPUs.
This method can generate any number of images, we produced 50,000 to match the number of training images for easier comparison. 

The generation process, generally, requires implementing a numeral approximation of the reverse diffusion SDE given in Eq.~\ref{eq:gen_sde}. 
We employed a version of the Euler-Maruyama in the code repository of Song et al.~\cite{song2021scorebased}.
Additional approaches were available within the repository. These approaches include alternate numerical solvers and the use of ``corrector" functions, which aim to modify the sampled images at each iteration to be more like the training data.
Exploration of more efficient and accurate SDE methods is still an active area of research for generative models.
We leave studies for optimizing LArTPC image generation for future work.

Generating a sample image under our choice of VPSDE and numerical SDE predictor involves iterating over $M$ constant time steps of $\Delta t=-(T-\epsilon)/(M-1)$ from time $T$ to $\epsilon=1.0\times10^{-3}$.
A non-zero $\epsilon$ is used to avoid numerical instabilities.
During each iteration, $i$, going in reverse order from $i=M$ to $i=1$, 
the sample image, $\vec{\bm{X}}_{t_i}$, 
and predicted mean pixel values $\vec{\bm{\mu}}_{t_i}$ at $t_i=T+(i-M)\Delta t$ are updated using
\begin{equation}
\label{eq:gen_concrete_vpsde_update}
\begin{split}
   \vec{\bm{\mu}}_{t_{i-1}} & =  \vec{\bm{X}}_{t_i} + [-\frac{1}{2}\beta(t_i)\vec{\bm{X}}_{t_i} -\beta(t_i)\bm{s}_{\bm{\theta}}(\vec{\bm{X}}_{t_i},t_i)] \Delta t \\
   \vec{\bm{X}}_{t_{i-1}} & = \vec{\bm{\mu}}_{t_{i-1}} + z\sqrt{-\beta(t_i)\Delta t},
\end{split}
\end{equation}
where $\bm{z}\sim\mathcal{N}(\bm{0},\bm{I})$, and $\beta(t)$ is again defined by Eq.~\ref{eq:beta}.
The pixel values for the prior (noise) image, $\vec{\bm{X}}_{t_M=T}$, are sampled from $\mathcal{N}(\bm{0},\bm{I})$.

We perform a minimal amount of post-processing on the generated images.
By default, the images are normalized to have pixel values ranging from 0 to 255. 
We apply a threshold to each image pixel such that all pixel values below 5 are set to 0. 
This ensures a constant background necessary for the pixel-level sensitivity of our quality metrics, namely the SSNet labels.
A similar threshold is applied to the wire plane waveforms from the LArTPC used in the MicroBooNE experiment~\cite{adams2018ionization}.
We believe this is an artifact of the diffusion process struggling to achieve the large homogeneous regions of zero values needed for these sparse LArTPC images. 

Per our loss curve in Figure \ref{fig:loss}, we can see that the network rapidly improves and then plateaus. 
We have selected three key epochs: 10, 50, and 150, for which we test the quality of generated images. 
We will demonstrate that 10 epochs is insufficient training and creates low-fidelity images. 
After 50 epochs, our generated samples are of high fidelity according to our metrics in the results section. 
Extending training to 150 epochs and beyond produces a minimal improvement in the loss and is not worth the extra computational effort. 
As such, we make the case that training for 50 epochs is sufficient. 
Our various quality metrics will further support this choice of training time.

 % Figure environment removed


\section{Evaluating Quality}

We find qualitatively that the images generated by the network are very similar to those in the training set when compared by simple visual inspection. 
In order to quantify this a little further, we conducted a small sample "Turing test" where participants were asked to differentiate true LArTPC (training) images from our generated images by eye. 
We had 11 participants of varying expertise look at 100 randomly selected images generated from epoch 50. 
Only a single participant achieved a statistically significant 64\% accuracy.
Everyone else was within one standard deviation of random chance. 
All the accuracies and corresponding p-values can be found in \ref{fig:Turing}. 
This small-sample experiment suggests that the generated images are at least superficially very similar to the LArTPC images.

% Figure environment removed

While such a qualitative study helps us confirm our belief that the generated model produces high-fidelity images, quantitative measures are desired.
How to measure the quality of images is still an area of open research for natural images.
In addition to how well the images match, we also want to know if the variation in the type of images are similar.
To measure image quality, we have conducted several studies that aim to measure how alike the images are at both large and small scales.

For small scales, we are interested in whether the generated images exhibit the distribution of patterns of pixel values in a small patch.
To measure this, we propose to compare the output of a semantic segmentation network, referred to as SSNet~\cite{collaboration2019deep,abratenko2021semantic}, that has been used by LArTPC experiments as part of their reconstruction and analysis chain~\cite{abratenko2022search}.
The pattern of ionization left behind by charged particles can be classified broadly into two classes: track and shower. "Tracks" are nearly straight lines resulting from more massive charged particles, such as protons, pions, and muons. 
Electrons and photons typically instigate an electromagnetic cascade, resulting in a pattern of deposited charge referred to as a "shower". 
The semantic segmentation network we employ is trained to label individual pixels in an image as either having been created by a particle whose trajectories are track-like or shower-like. 
In order to judge the fidelity of generated images, we compare the distribution of shower and track scores along with the frequency of track-like pixels and shower pixels in the generated and real data set.

%% These details are best left to the methods type section
% Chen et al: https://inspirehep.net/files/df1edf05d33a55edbb411bdebb802f58
% Rubbia: https://cds.cern.ch/record/117852/files/CERN-EP-INT-77-8.pdf

For larger scales, we use measures that come from the study of high-dimensional distributions along with reconstructed quantities inspired by physics that are extracted from the images.
For high-dimensional probability distributions, we use the measures: Wasserstein-1 distance, Sinkhorn Divergence, and Maximal Mean Discrepancy (MMD).
These measures attempt to quantify how different the images are at the distribution level.
We were also able to approximate an FID distance metric. %TODO:  phrasing? 
For the physics-inspired metrics, we measure quantities that reflect the type of information experiments will want to extract about the particles that ultimately make the images.
This includes a measure of the length and width of track-like trajectories and the energy of shower trajectories.
We use SSNet and simple clustering techniques to identify track-like and shower-like trajectories.
The entire image is then classified as either track or shower based on the majority of pixel labels.
We employ a simple principle-component analysis of the spatial pixel distribution for tracks to measure the length and width. 
Ultimately, the most relevant measure for asking if the data produced by a generative network is sufficient is its impact on physics analyses.
The data we worked with does not lend itself to such analyses and so we leave that to future work to address this.

% Figure environment removed


\section{Results}
\label{sec:results}

The method for determining the quality of LArTPC-like images is still an open question. 
Here, we report on four proposed approaches to quantifying generated LArTPC image quality: 
SSNet pixel-labeling, high dimensional goodness of fit, modified FID score, and physics-based analyses.
%This is more than our previous work using a VQ-VAE approach (see appendix), which was quickly found to be lacking. 

\subsection{SSNet Results}

Our first external metric for gauging the quality of generated images uses our pre-trained semantic segmentation network (SSNet). 
For every image, SSNet labels each pixel as either background, track, or shower, as shown in Figure \ref{fig:SSNet_images}. 
We then compare the distribution of these pixel labels and their associated certainty against our validation dataset.
Per our loss curve (and high dimensional comparison), we have selected our three key epochs: 10, 50, and 150 to analyze in figure \ref{fig:SSNet_hists}.
We compare these distributions against our validation dataset. 
As expected, we can clearly see that 10 epochs of training is insufficient. 
Meanwhile, generated images produced using weights from epochs 50 and 150 both have nearly identical distributions to each other and the validation dataset.
This is in agreement with our prediction that there is little, if any, improvement from training beyond 50 epochs. 

% Our semantic segmentation network (SSNet) provides pixel-level labeling as either background, track, or shower, as shown in Figure \ref{fig:SSNet_images}. 
% The entire image is then classified as either track or shower based on the majority of pixel labels. 
% After inputting our generated images, we can compare the distribution of these pixel labels and their associated certainty against our validation dataset.
% Per our loss curve (and high dimensional comparison), we have selected our three key epochs: 10, 50, and 150 to analyze in figure \ref{fig:SSNet_hists}.
% As expected, we see that at epoch 10, the output of SSNet for both generated track and shower images differ substantially from the output of SSNet run on the validation sample. 
% Meanwhile, generated images produced using weights from epochs 50 and 150 both have nearly identical distributions to each other and the validation dataset.

% Our first external metric for gauging the quality of generated images makes use of SSNet. 
% This semantic segmentation network labels pixels as background, track, or shower. 
% The most simple analysis is counting the number of pixels each time. 
% We can also look at the probability scores of each of the labels. 
% These histograms are shown below in Figure~\ref{fig:SSNet_hists}, where we plotted epochs 10, 50, and 150 and compared them against the validation dataset. 
% We can clearly see that 10 epochs of training is insufficient. 
% By epoch 50, the histograms are in very close agreement with the validation dataset. 
% We see little, if any, improvement by training to 150 epochs. 

\subsection{Model Validation Via Comparing High-dimensional Distributions}

Seeking to refine our model selection we propose to compare the pixel-space distributions, i.e., the distributions in the $64^2$ dimension space, of the training, validation, and generated data via three high-dimensional goodness-of-fit (GoF) measures: Maximum Mean Discrepancy (MMD) \cite{JMLR:v13:gretton12a},  Sinkhorn Divergence (SD) \cite{genevay2018learning, mena2019statistical},  and the Wasserstein-1 distance \cite{Sriperumbudur_2012}. 
Wasserstein distances, also referred to as the optimal transport and earth mover's distance, are based on defining distances based on the minimal cost of coupling via a joint distribution with the given distributions as fixed marginals. 
In some cases, this coupling corresponds to a map, a special type of coupling, which in turn can be interpreted as a transport of mass reshaping one distribution into another. 
MMD distances belong to a family of distances called integral probability metrics, which are defined as a maximum absolute difference, i.e., discrepancy, between the expected value of a class of functions under the two distributions. 
Notably, Wasserstein-1 distance is also an integral probability metric. 
Sinkhorn divergence, while not strictly a distance, interpolates between Wasserstein and MMD distances and is numerically faster to compute as well as has faster rates of statistical estimation \cite{pmlr-v89-genevay19a, mena2019statistical}. 

These distribution level comparisons are shown in Figure~\ref{fig:high_dim_GoF}. 
For MMD, the minimum distance is from epoch 50. 
For both SD and Wasserstein-1, epoch 50 is the second minimum after epoch 20. 
The reason for the minimum at 20 epochs is unclear and warrants further exploration in later work. 
This is the case when comparing against both the validation and training datasets. 
We observe that all distance metrics increase slightly with longer training, potentially indicating overfitting. 
However, the variations within this plateau region (starting at 20 epochs) are relatively small compared to the reduction in value from the beginning of training. 
This behavior is similar to the loss curve, supporting our selection of key epochs and reinforcing our choice of epoch 50 as striking the right balance of performance versus training time.


% Using these metrics for model selection via validation, from Figure~\ref{fig:high_dim_GoF}, we pick epoch 50. By this point in the training, the loss no longer decreases significantly. 
% We observe the same trend for the SSNet-FID measure.
% Epoch 50 also corresponds to the epoch where the MMD between the training and validation datasets is the smallest. 
% We also observe that both the MMD and SD rise slightly with longer training potentially indicating overfitting. 
% However, we note that with the MMD and SD, the level of increase with higher epochs is relatively small compared to the reduction in value from the beginning of training. 
% We find it difficult to determine how those levels of change in the MMD and SD correspond to image quality as other metrics described below 

% that training for $50$ epochs produces the best results. As predicted by the loss function, longer training times produces comparable or even slightly worse outcomes.  


% Figure environment removed


% Figure environment removed

\subsection{Via Inception Score and SSNet} 

A common metric for image generation is the Fr√©chet inception distance (FID)~\cite{FIDpaper}. 
This is calculated by getting the deepest layer activations of the classifier model for a dataset, fitting the activations to a multivariate Gaussian, and then finding the 2-Wasserstein distance between the training and generated datasets. 
Typically, the activations come from the deepest layer (pool3) of Google's Inception v3~\cite{szegedy2016rethinking}. 
However, we do not have a well-defined classifier for LArTPC type images. 
Instead, we use SSNet and get the activations of one of the deepest convolution layers (double\_resnet[2]) as an 8,192 parameter vector and conduct the prescribed calculations. 
The architecture of SSNet features skip connections, and alternate layer choices quickly become computationally unwieldy. 
Despite this, we believe our SSNet-FID is analogous to traditional FID.
This SSNet-FID metric, shown in Figure \ref{fig:FID}, supports our previous analyses; the model rapidly improves before performance plateaus with an inflection point around epoch 50. 
The best image activations, i.e., closest to the training and validation datasets, are produced by epoch 150, but the differences within the plateau are minimal. 
A table of SSNet-FID values can be found in Appendix~\ref{appendix:eval_details}. 


% Figure environment removed


\subsection{Physics-based Comparisons}

An important characteristic of  LArTPC images is the fact that they encode underlying physical processes. 
As such, we have devised a few simple tests to verify that the generated images follow the expected realistic behavior of particle interactions. 
Perhaps the most relevant feature is the energy the particle deposits within the detector. 
We begin by separating the generated images as either track or shower according to the dominant SSNet pixel labeling. 
The different physical processes of track and showers require separate methods of analysis.  

For track-like events, energy is deposited as it moves through the detector medium (liquid Argon), so we can simply find the length of the track to get an approximate measure of the energy. 
For this calculation, we used DBScan (eps=3) to remove background noise and keep only the largest cluster of pixels.
We then use a convex hull algorithm to determine the distance between the two farthest points on the track. 
The majority of track images contain a single particle track.
However, many events have more than one track. 
This is from interaction vertices producing multiple track-producing particles or occasionally overlapping events; an example is shown in Figure \ref{fig:example_tracks}. 
Note that the length of these multiple-track events is the longest straight line distance between two points, \emph{not} a trace of the entire event.  
We apply this calculation across our key epoch generated images and LArTPC validation dataset to provide a consistent metric for comparison. 
We then bin these values into a histogram in Figure \ref{fig:track_length} displaying the fraction of images for each length. 
We can see that epoch 10 is unable to capture the shape of this distribution and vastly overproduces tracks of the maximum length.
This is expected as our training metrics suggested that epoch 10 was insufficiently trained, and the images produced are still noisy. 
Epoch 50 and 150 fare much better, and it is difficult to determine which more closely matches the validation distribution. 
To aid our differentiation, we have calculated a total chi-squared comparing each generated histogram to the validation histogram, recording in Table \ref{tab:chi-squared}. 
We find that epoch 50 is a closer match.
The histograms are not a perfect match, but we believe them to be reasonably close enough to justify the functionality of this method of generation. 

% Figure environment removed

We also measured the width of the track by finding the maximum distance along the secondary axis of a principle component analysis applied to the largest cluster of track pixels. 
We are applying the same DBScan clustering algorithm as with the track length analysis. 
We are interested in the track width because it correlates with several physical processes within the detector.
First, in the context of our images, where track trajectories are made by muons, pions, and protons,
the track width is sensitive to the frequency of secondary interactions that can occur with pions and protons, which scatter off nuclei in the detector. In theory, it is also sensitive to the shape and frequency of delta-ray production from muons. The delta rays manifest as shower features in the images and, in principle, should not be included in the track cluster pixels. However, in practice, the delta ray showers are often small enough to be tagged by the SSNet as track pixels and are then seen as track pixel bumps along the track.
Second, the width of tracks as seen in LArTPCs reflects detector physics. This includes diffusion of the ionization cloud as it drifts to the charge-sensitive electronics.  It is also sensitive to the details of how the ionization induces a signal onto the charge-sensitive devices such as wires or pixel-based collection electrodes.

As before, we have plotted our track width histogram in Figure \ref{fig:track_width}. 
We see that the validation distribution is primarily thin widths (53\% of tracks are $\leq5$ pixels) and then exponentially decays with a long tail.
These thin widths come from straight-line single-track images. 
Wider tracks are caused by track curvature (from processes like multiple-Coulomb scattering) and tracks with secondary interactions. 
All of which can vary slightly if background noise causes bumps or wiggles in the track. 
Figure \ref{fig:example_tracks} shows an example track image from the validation dataset that has multiple tracks and some minor background interference. 
Looking back at our track width histogram, we can compare our key epochs to the validation distribution. 
We find that epoch 10 is unable to reproduce the proper distribution shape and significantly overproduces extremely wide tracks, as evidenced by the final overflow bin. 
A few epoch 10 tracks are so poor quality that the image is classified as entirely noise, in which case the track is reported as having zero width.
The width distributions for epochs 50 and 150 are a closer fit to the validation dataset.
It is interesting that the large-width tail of the distribution matches fairly well, suggesting that secondary interactions are reproduced fairly well.
However, there are still noticeable differences in the first few bins. 
Disentangling the cause of this difference is reserved for future work where one would want an image generation apparatus capable of better manipulating the activation of physics processes such as delta-ray production, ionization discussion, or the physics of the readout.
Using the chi-squared values in Table \ref{tab:chi-squared}, we find that for the track width distribution agreement, the time epoch 150 is a significantly closer fit to our validation dataset. 
This suggests that longer training might benefit the learning of the more subtle physics that influences this image quality measure.


% Figure environment removed

% Figure environment removed

The energy of shower-like events is proportional to the amount of charge they deposit through scattering processes. 
Since our shower images are dominated by these charged particle scattering interactions, we find the energy by summing all the pixel intensities in the image. 
We have plotted a histogram showing the amount of charge deposited per image in Figure \ref{fig:shower_charge}. 
We find that the epoch 10 charge distribution does not resemble the shape of the validation dataset. 
The generated images contain significant background noise, an indication that 10 epochs is insufficient training. 
The surplus in activated pixels correlated to excessively high charge measurements, which we have condensed to a final overflow bin. 
Epochs 50 and 150 both more closely resemble the validation dataset but are not perfect.
Visually, it is difficult to determine which charge distribution is more accurate. 
We can use the mean as a comparison shorthand. 
The shower charge distribution for the LArTPC validation dataset has a mean of 15134, whereas epoch 50 had a mean of 14861 and epoch 150 of 15577. 
This puts epoch 50 as the closer match, off by only 273 compared to epoch 150's difference of 716. 
The mean of a distribution is a simple metric, so we also used a chi-squared test like with the track distributions. 
Per Table \ref{tab:chi-squared},  epoch 50 has a slightly better chi-squared value. 
Both these heuristics agree that epoch 50 produces at least marginally better shower images than our other key epochs. 
% LArTPC Val: mean = 15134, variance = $3.29 \times 10^7$
% Epoch 10: mean = 44624, variance = $1.08 \times 10^9$ 
% Epcoh 50: mean = 14861, variance = $2.96 \times 10^7$
% Epoch 150: mean = 15577, variance = $2.96 \times 10^7$

\begin{table}[h]
\begin{tabular}{|C{2cm}|C{1.8cm}|C{1.8cm}|C{1.8cm}|}
\hline
$\chi^2$ &
  \begin{tabular}[c]{@{}c@{}} Track \\ Length \end{tabular} &
  \begin{tabular}[c]{@{}c@{}} Track \\ Width \end{tabular} &
  \begin{tabular}[c]{@{}c@{}} Shower \\ Charge \end{tabular} \\ \hline
Epoch 10  & 269624 & 77244 & 1405 \\ \hline
Epoch 50  & 113   & 399    & 63   \\ \hline
Epoch 150 & 128   & 166    & 70   \\ \hline
\end{tabular}
\caption{Total chi-squared values comparing binned histograms of our generated key epochs against the LArTPC validation set.} 
\label{tab:chi-squared}
\end{table}


% Figure environment removed

Another metric to observe is the makeup of our generated dataset broken down by track and shower images. 
We find that both our training and validation datasets contain 69\% track and 31\% shower images. 
The breakdown of our generated epochs can be inferred from the number of events displayed on the legends in our track length and shower charge plots. 
For convenience, we have calculated the percentages in Table \ref{tab:track_shower_percentages}.
We see that epoch 10 is unable to capture this rate. 
Epochs 50 and 150 are within a few percent of the expected rate, but both slightly overproduce shower events.
Images generated from epoch 50 are produced closer to the true ratio of track and shower images than our other key epochs. 

\begin{table}[]
\begin{tabular}{|c|c|c|}
\hline
            & Track \% & Shower \% \\ \hline
Training    & 69.2   & 30.8    \\ \hline
Validation  & 69.0   & 31.0    \\ \hline
Epoch 10    & 46.3   & 53.7    \\ \hline
Epoch 50    & 68.4   & 31.6    \\ \hline
Epoch 150   & 67.9   & 32.1    \\ \hline
\end{tabular}
\caption{Breakdown of track and shower rates. Images are classified as "track-like" or "shower-like" based on the majority pixel label according to SSNet. The convergence of the track versus shower fraction in the generated image data set produced at the different epochs towards the validation set is a measure of the model's fidelity in reproducing the original data set.}
\label{tab:track_shower_percentages}
\end{table}

\subsection{Checking for Mode Collapse}

% Figure environment removed

A major concern for any method of image generation is mode collapse; the created images need to replicate the characteristics of the training data but still be distinct and unique. 
We want our generated images to sample from the entire space of valid LArTPC images according to the probability distribution $p(\vec{\bm{X}}_0)$ of our training data. 
An example in our context is that we would want the generator to produce tracks smoothly over a range of lengths and angles and not just generate lengths or angles closely around the neighborhood of examples in the training set. % TODO: Rephrase 
A useful heuristic is to find the nearest neighbors for a generated image, i.e., the images from the training data that are most similar to a generated image. 
For a single generated image, we calculate its Euclidian distance (L2 norm) from every image in the LArTPC training dataset. 
The (five) images with the smallest distances are defined to be the nearest neighbors. 
This measure of this distance describes a quantitative measure of uniqueness, and qualitatively we can tell that the generated image is unlike the training images. 
We can repeat this procedure to compare a single generated image against the set of all generated images to verify that we are reasonably sampling from the space of images. 
The results of this process can be seen in~\ref{fig:l2_two_nearest_neighbors}.
Several additional examples of nearest neighbors can be found in Appendix~\ref{appendix:moreimages_mode_check}, Figure \ref{fig:l2_neighbors}. 

The use of Euclidian distance when computing distance is standard for typical image datasets such as CIFAR-10 and CelebA. 
However, our dataset of particle trajectories is unlike these standard datasets due to their extreme sparsity and underlying semantic content.
Recent research suggests that Earth (or Energy) Mover's Distance (EMD) is a more meaningful metric for collider-based events \cite{thaler2019physicsemd}. 
As such, we have also found the nearest neighbors using EMD instead of Euclidian distance. 
Examples of this can be found in Appendix~\ref{appendix:moreimages_mode_check}, Figure \ref{fig:emd_neighbors}. 

% TODO: expand this section
In both measures of nearest neighbors, we find minimum distances significantly greater than zero, implying that the generated image is unique. 
Visually, we can tell that the nearest neighbors are different images. 
This is particularly evident in shower-like events where the overall orientation of the shower and its density are similar, but the patterns within the shower envelope vary noticeably.
For tracks, the trajectories are very similar, but we do see variation in the frequency and placement of high energy deposits and delta ray features.
Interestingly, the measured distances between the training images and the generated images are similar.
The distance between showers is also larger than with tracks, as one might expect, given the higher inherent variation in the shower process.
One avenue of research is to understand this metric in more detail to both better understand how to quantify potential mode collapse and also for understanding particle trajectories in LArTPCs.
In summary, we believe this qualitative analysis supports the conclusion that the generated images are not mere reproductions of the training images.
Combining this evidence with the reproduction of various distributions, we conclude that the model does not exhibit significant mode collapse, but is instead generating examples with variations similar to that of the training sample.

\subsection{Comparison to Another Model}

As far as we know, the only past attempt at generative models for LArTPCs was 
our previous attempts using Vector-Quantized Variational Autoencoder (VQ-VAE).
At the time, the generated images from this approach began to resemble the patterns seen in LArTPCs, a first compared to past unpublished attempts with GANs and OT approaches. 
However, despite the progress, the generated images were visibly different than the training dataset. 
The study using VQ-VAEs was presented in~\cite{lutkus2022towards} and introduced the use of SSNet outputs to quantify the similarity of generated images to the training sample.
The SSNet measures aligned with observations of a model approaching LArTPC features but not yet closely reproducing them. 
In order to facilitate a comparison to the performance of our diffusion model, we reproduce the SSNet measures and measure the quality of the VQ-VAE images using the approaches presented above.
The full results of this analysis can be found in Appendix \ref{appendix:VQ-VAE}.

\subsection{Discussion}

How best to determine the quality of LArTPC-like images remains an open question -- just as is the case for natural images. 
We have proposed several approaches which aim at different views of how to compare the images.
This includes the direct comparison of the images using statistical techniques developed for comparing high-dimensional probability distributions, feature analysis through a domain-relevant CNN (SSNet),
and through the physical quantities extracted from the images.
We find that the VPSDE diffusion model implemented here produces generated images very similar to the validation set.
These measures also largely improve over training time.

One limitation of our quality measures is that they still do not address what we ultimately want to know: would an analysis that attempted to make a physical statement be significantly biased by the use of data created by a generative diffusion model?
Our ability to answer this question is limited by our choice of dataset, which used cropped images that did not make an attempt to capture a full particle trajectory.
This reflects the goal of this work, which was to explore if diffusion models could mimic the collection of features found in LArTPC images.
This study provides encouraging evidence to this narrower question, opening the path towards developing generative models that can potentially assist physics analyses in various ways, thus making the larger question now relevant.

Future directions for our work would aim at addressing the impact on physics analyses but will also need to explore the large space of choices in the implementation of diffusion models.
One direction would be to implement diffusion models for various particle types conditioned on momentum.
Implementation of these would make it possible to generate a collection of images that could be translated and combined to form images representing the final state particles from neutrino interactions.
Particle-level or interaction-level images could then be passed into existing reconstruction and selection algorithms to quantify the amount of bias introduced into those analyses.
One could also explore the similarity of generated images to existing data sets isolated from experiments such as Proto-DUNE and MicroBooNE.
In order to be useful, such models will have to be capable of generating much larger images than the 64x64 images explored here.

For model choice explorations, there are a large number of directions for future work to investigate.
One area of active research for natural images is to improve the generation method of these models, often by looking to reduce generation time.
These improvements in generation times may or may not have image quality trade-offs.
There are also additional choices in the framework of the diffusion SDE process that
center around the choice of drift and diffusion functions, $\bm{f}$ and $\bm{g}$.
Another area of active research is into the theoretical connections of diffusion models to optimal transport~\cite{khrulkov2022understanding} and Schrodinger Bridge formalisms~\cite{de2021diffusion},
which may one day guide the choice of drift and diffusion functions for specific goals of the model.
Interestingly, recent work has explored the possibility of extending generative models past the use of SDEs describing diffusion and instead using SDEs that implement other physical processes~\cite{liu2023genphys}.

In order to support efforts to reproduce and compare against our efforts, 
we will prepare a repository of our code, the weights of our generative model, 
and copies of our training, validation, and generated data sets. 
We provide the weights for the final training run at 50 epochs. 
These materials will be stored on Zenodo~\footnote{\url{https://zenodo.org/record/8300355}}.
Scripts and code for training, generating, and calculating our evaluation metrics will be provided on GitHub~\footnote{\url{https://github.com/NuTufts/Score_Based_Diffusion_for_LArTPC_Images}}.

\section{Conclusions}

In this work, we show that a choice of implementation for a score-matching diffusion model is able to learn and generate novel images containing features that would be observed in data from LArTPCs.
We have proposed metrics for future work to determine the model selection and to quantify how well the generated images match the original data set. 
According to these metrics, this is the first example of LArTPC experiments where images produced by a generative model are shown to be very similar to the training set. 
The slight but significant deviation in the generated images is seen in the distribution of narrow track widths which indicates further work to fully capture the physical processes that influence that aspect of the trajectories.
However, we believe our results are an important proof-of-principle demonstration that establishes the groundwork to further pursue the use of score-matching diffusion models for LArTPC experiments.

% We first had to select an appropriate amount of training, which is another challenging task. 
% From the loss curve in Figure \ref{fig:loss} we found that our diffusion model quickly reached a plateau of performance, with epoch 50 being the approximate inflection point. 
% Our direct comparison of the high-dimensional distributions using MMD and Sinkhorn Divergence supported this idea of a plateau as seen in Figure \ref{fig:high_dim_GoF}. 
% Finally, our SSNet-FID \ref{fig:FID} has a similar shape, with 50 as the inflection point. 
% Here we do see minimal improvement from further training beyond 50 epochs.
% From these metrics, we selected three key epochs to analyze further:
% \begin{itemize}
%     \item Epoch 10: insufficient training demonstrates the poor initial performance of our diffusion model
%     \item Epoch 50: the inflection point provided promising results with minimal training
%     \item Epoch 150: further training yielded few improvements according to our metrics
% \end{itemize}
% From this we conclude epoch 50 to be the sweet spot, providing a nice balance in performance and training time. 
% Depending on the quality metric, further training might even decrease the model's performance. 
% We then developed several quality metrics to compare our key epochs against a validation dataset of LArTPC images. 

% Our first test was applying SSNet to provide a semantic analysis of the fine structure of our images through the pixel label histograms of Figure \ref{fig:SSNet_hists}. 
% As expected, epoch 10 deviated significantly from the validation distribution across all the SSNet histograms.  
% We found that epochs 50 and 150 are nearly indistinguishable from each other and the LArTPC validation dataset.  
% This was the extent of our previous analysis using VQ-VAE to generate LArTPC images \cite{lutkus2022towards}, further details of which can be found in Appendix \ref{appendix:VQ-VAE}. 
% We needed higher fidelity quality metrics for LArTPC-like images. 

% We developed three physics-informed metrics: track length, track width, and shower charge. 
% For all of these metrics, epoch 10 was unsuccessful in capturing the shape of the distribution and overproduced extreme cases dominating the overflow bin of our histograms. 
% For epochs 50 and 150 we can visually differentiate them, but determining which is a closer match to the validation dataset is difficult. 
% We rely on a chi-squared test for comparing the histograms, which is shown in Table \ref{tab:chi-squared}. 
% Track length and shower charge both favor epoch 50 as generating marginally better images, while track width favors epoch 150. 

% Across all our metrics we find a large increase in performance going from epochs 10 to epoch 50, but minor mixed effects from further training to epoch 150. 
% Only track width is the only metric and SSNet-FID that favors epoch 150 over epoch 50. 
% It is not clear why this is the case.
% Our chosen quality heuristics are not definitive and judging quality for LArTPC-like images remains an open question beyond the scope of this analysis. 
% From our analyses, we find that epoch 10 is worse by every metric, and epoch 150 does not provide enough marginal benefit to justify the significant increase in training time. 
% This provides sufficient justification to select epoch 50 as the best model. 

% Some high-dimensional measures seem to get worse over increased training.
% It does not seem obvious if this is a sign of over-fitting as the physics and SSNet measures do not get appreciably worse.
 
% Also worth noting that the generated images do not experience mode collapse.


\subsubsection*{Acknowledgments} 

% Hack to fix formatting 
\vspace{-2mm}

% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

This material is based upon work supported 
by the U.S. Department of Energy (DOE)
and the National Science Foundation (NSF).
T.W. was supported by the U.S. DOE, 
Office of High Energy Physics under Grant No. DE-SC0007866. 
S.A. was funded by the NSF under CAREER Award No. CCF:1553075.
This work was also supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, \href{http://iaifi.org/}{http://iaifi.org/)}).

\bibliography{mybib}
\bibliographystyle{unsrtnat}

\clearpage

\appendix

\section{Additional Details for Training and Generating Images}
\label{appendix:traingen_details}

The realization of a generative diffusion model involves many possible choices.
In order to facilitate reproduction and exploration in future work, we document all of the parameters used in the work presented in Table \ref{tab:configs}
These parameters are specific to the code repository by Song et al.~\cite{song2021scorebased}, which we used for training and generation.
We left the majority of parameters unchanged from the default CIFAR-10 configurations. 
We leave the exploration of hyperparameters turning to later work.

\section{Additional Details for Our Evaluation Metrics}
\label{appendix:eval_details}

% The code used in our various quality metrics is available on GitHub and the associated datasets on Zenodo.
% We encourage others to reproduce and expand upon our work. 
Our high-dimensional goodness of fit tests required specifying parameters. 
For MMD, we used a mixed kernel with sigma values: $2^{10}, 2^{11}, 2^{12}, 2^{13}, 2^{14}, \text{and } 2^{15}$.
We found that smaller sigma values had less ability to differentiate between epochs. 
Larger sigma values vertically scaled the distances without affecting the shape of the distribution. 
For Sinkhorn Divergence, we used eps = 1, alternate eps values primarily affected vertical scaling.

Table~\ref{appendix:SSNet-FID_values} provides values for future comparisons of the SSNet-FID score images generated after training for a different number of epochs.

% Hack to manually break column 
\vfill\break 

\section{Additional Images for Similarity Measures Used for Studying Potential Mode Collapse}
\label{appendix:moreimages_mode_check}

Figures \ref{fig:l2_neighbors} and \ref{fig:emd_neighbors} each provide five examples of nearest neighbors for a generated image in the training and generated image datasets. The distances are calculated using l2 Euclidean norm and Earth Mover's Distance (EMD), respectively. 

% Figures \ref{fig:training_image_page} and \ref{fig:generated_image_page} show a random sampling of images from our training dataset and generated images from epoch 50, respectively.

\section{Additional Uncurated Examples of Training and Generated Images}
\label{appendix:moretrain_gen_images}

Figures~\ref{fig:training_image_page} and Figures~\ref{fig:generated_image_page} include more examples of training and generated images, respectively.


\section{VQ-VAE Comparison}
\label{appendix:VQ-VAE}

We revisited our previous attempt \cite{lutkus2022towards} using a vector quantized variational autoencoder (VQ-VAE) model to generate LArTPC images. 
We generated 50,000 new images to compare performance against our diffusion model and ran the images through SSNet and our physics-based metrics.  
These results can be found in Figures~\ref{fig:SSNet_hists_vqvae} and~\ref{fig:physics_vqvae}, respectively.
While VQ-VAE performs better than expected from our previous analysis, it is still noticeably worse than epochs 50 and 150 across all our metrics. 

% \section{Data Preparation Details}
% \label{appendix:dataprep}

% \section{Additional Details and Images for Similarity Measures Used for Studying Potential Mode Collapse}
% \label{appendix:moreimages_mode_check}

% \twcomment{You also need details on parameter choices, if any, for L2 and EMD calculations.}


\clearpage

\begin{table*}[h]
\begin{minipage}{0.4\textwidth}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{\bf{default\_particle\_configs.py}} \\ \hline
\multicolumn{2}{|c|}{Training}  \\ \hline
batch size  & 128  \\ \hline
n iters & 4000  \\ \hline
\bf{snapshot freq} & \bf{400}  \\ \hline
\bf{log freq} & \bf{100}  \\ \hline
eval freq & 100  \\ \hline
snapshot freq for preemption & 10000  \\ \hline
likelihood weighting & FALSE  \\ \hline
continuous training  & TRUE  \\ \hline
reduce mean & FALSE  \\ \hline
\multicolumn{2}{|c|}{Sampling}  \\ \hline
n steps each & 1  \\ \hline
noise removal & TRUE  \\ \hline
Probability flow & FALSE  \\ \hline
snr  & 0.16  \\ \hline  
\multicolumn{2}{|c|}{Evaluation}  \\ \hline
\bf{batch size}  & 1\bf{28}  \\ \hline
\bf{enable sampling} & \bf{TRUE}  \\ \hline
num samples & 50048  \\ \hline
\bf{enable loss} & \bf{FALSE}   \\ \hline
enable bpd & FALSE  \\ \hline
\multicolumn{2}{|c|}{Data}  \\ \hline
random flip & TRUE  \\ \hline
centered & FALSE  \\ \hline
uniform dequantizations & FALSE  \\ \hline
num channels & 3  \\ \hline
\multicolumn{2}{|c|}{Model}  \\ \hline
sigma min & 0.01  \\ \hline
sigma max & 50  \\ \hline
num scales & 1000  \\ \hline
beta min & 20  \\ \hline
beta max & 20  \\ \hline
dropout & 0.1  \\ \hline
embedding type & fourier  \\ \hline
\multicolumn{2}{|c|}{Optimization}  \\ \hline
weight decay & 0  \\ \hline
optimizer & Adam  \\ \hline
learning rate & 0.0002  \\ \hline
beta1 & 0.9  \\ \hline %Initial beta? 
eps & 0.00000001  \\ \hline
warmup & 5000  \\ \hline
grad clip & 1  \\ \hline
\end{tabular}
\label{tab:configs1}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{\bf{larcv\_png64\_ncsnpp\_continuous.py}} \\ \hline
\multicolumn{2}{|c|}{Training}  \\ \hline
sde & vpsde  \\ \hline
continuous & TRUE  \\ \hline
reduce  mean & TRUE  \\ \hline
\multicolumn{2}{|c|}{Sampling}  \\ \hline
method & pc  \\ \hline
predictor & euler maruyama  \\ \hline
corrector & none  \\ \hline
\multicolumn{2}{|c|}{Data}  \\ \hline
\bf{dataset} & \bf{larcv\_png64} \\ \hline
centered & TRUE  \\ \hline
\multicolumn{2}{|c|}{Model}  \\ \hline
\bf{image size} & \bf{64}  \\ \hline
name & ncsnpp  \\ \hline
scale by sigma & FALSE  \\ \hline
ema rate & 0.9999  \\ \hline
normalization & GroupNorm  \\ \hline
nonlinearity & swish  \\ \hline
nf & 128  \\ \hline
ch mult & (1, 2, 2, 2)  \\ \hline
num res blocks & 4  \\ \hline
atten resolution & (16,)  \\ \hline
resamp with conv & TRUE  \\ \hline
conditional & TRUE  \\ \hline
fir & TRUE  \\ \hline
fir kernel & [1, 3, 3, 1]  \\ \hline
skip rescale & TRUE  \\ \hline
resblock type & biggan  \\ \hline
progressive & none  \\ \hline
progressive input & residual  \\ \hline
progressive combine & sum  \\ \hline
attention type & ddpm  \\ \hline
embedding type & positional  \\ \hline
init scale & 0  \\ \hline
fourier scale & 16  \\ \hline
conv size  & 3  \\ \hline
\end{tabular}
\end{minipage}
\caption{List of all parameters used in our score model. Bolded values differ from the CIFAR-10 defaults. Variable parameters such as the number of training iterations and specification of evaluation checkpoint have been omitted. Note we moved the dataset name to the specific configuration file (larcv\_png64...). }
\label{tab:configs}
\end{table*}

\begin{table*}[h]
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
Epochs     & 10     & 20    & 30   & 40   & 50   & 60   & 100  & 150  & 300  \\ \hline
Training   & 121.63 & 18.96 & 10.2 & 9.15 & 8.94 & 8.99 & 8.95 & 8.75 & 8.79 \\ \hline
Validation & 121.38 & 18.31 & 10.2 & 9.21 & 9.09 & 8.87 & 8.94 & 8.71 & 8.84 \\ \hline
\end{tabular}
\caption{SSNet-FID Values for all generated epochs.}
\label{appendix:SSNet-FID_values}
\end{table*}


% Figure environment removed

% Figure environment removed



% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed



% \newpage



% \newpage



% \clearpage

% % ONE COLUMN FOR FOLLOW APPENDICES THAT SERVE AS CODE DOCUMENTATION NOTES
% \onecolumngrid

% \section{REMOVEME: documenting how the loss is calculated}
% \label{sec:lossnote}

% The loss function used to train the score function is generated from various configuration inputs (such as the choice of SDE).
% The generating function, \texttt{get\_sde\_loss\_fn} for the loss function is found at \href{https://github.com/NuTufts/score_sde/blob/main/losses.py#L65}\texttt{losses.py:65}. It returns the following loss function.

% \begin{verbatim}
%   def loss_fn(model, batch):
%     score_fn = mutils.get_score_fn(sde, model, train=train, continuous=continuous)
%     t = torch.rand(batch.shape[0], device=batch.device) * (sde.T - eps) + eps
%     z = torch.randn_like(batch)
%     mean, std = sde.marginal_prob(batch, t)
%     perturbed_data = mean + std[:, None, None, None] * z
%     score = score_fn(perturbed_data, t)

%     if not likelihood_weighting:
%       losses = torch.square(score * std[:, None, None, None] + z)
%       losses = reduce_op(losses.reshape(losses.shape[0], -1), dim=-1)
%       # Note: reduce_mean = False so reduce_op = 0.5 * torch.sum() 
%       loss = torch.mean(losses) 
% \end{verbatim}

% \noindent
% The mean and std define the marginal probability $p(\vec{\bm{X}}_t|\vec{\bm{X}}_0)$ which by construction follows a Normal distribution.  These values are made by the function \texttt{VPSDE::marginal\_prob} found at \texttt{sde\_lib.py:141}:

% \begin{verbatim}
%     log_mean_coeff = -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0
%     mean = torch.exp(log_mean_coeff[:, None, None, None]) * x
%     std = torch.sqrt(1. - torch.exp(2. * log_mean_coeff))
% \end{verbatim}

% The score function (from utils.py:144)
% \begin{verbatim}
%     def score_fn(x, t):
%         labels = t * 999
%         score = model_fn(x, labels)
%         std = sde.marginal_prob(torch.zeros_like(x), t)[1]
%         score = -score / std[:, None, None, None]
%      return score
% \end{verbatim}
% The model function calls the neural network defined in \texttt{ncsnpp.py} (through \texttt{utils.py:108}). 
% \\

% Let's confirm what the loss expression is driving $s$ to output.
% Let $\vec{\bm{mu}}_t$ be mean and $\vec{\bm{\sigma}}_t$ by std. Loss is zero when:
% \begin{equation}
% \begin{split}
% 0 & = \bm{s}*\vec{\bm{\sigma}}_t+\bm{z}  \\
% \bm{s} & = \bm{z}/\vec{\bm{\sigma}}_t \\
% \end{split}
% \end{equation}
% The mean is based on the original pixel values, $\vec{\bm{X}}_0$:
% \begin{equation}
% \vec{\bm{\mu}}_t = \vec{\bm{X}}_0 e^{-\frac{1}{4} (\beta_1-\beta_0)t^2 - \frac{1}{2}  \beta_0 t}
% \end{equation}
% We can replace the specific instance of $\bm{z}$ that we have sampled during our training iteration using the expression for creating the perturbed data, $\vec{\bm{X}}_t$.
% \begin{equation}
% \begin{split}
% \vec{\bm{X}}_t & = \vec{\bm{\mu}}_t + \vec{\bm{\sigma}}_t * \bm{z} \\
% \bm{z} & = (\vec{\bm{X}}_t-\vec{\bm{\mu}}_t)/\vec{\bm{\sigma}}_t
% \end{split}
% \end{equation}
% So
% \begin{equation}
% \begin{split}
% \bm{s} & = (\vec{\bm{X}}_t-\vec{\bm{\mu}}_t)/\vec{\bm{\sigma}}_t^2
% \end{split}
% \end{equation}
% This is the closed-form expression for the score function, $\nabla_{\vec{\bm{X}}_t} p( \vec{\bm{X}}_t | \vec{\bm{X}}_0 )$, when we assume a Gaussian marginal probability.
% We can interpret $(\vec{\bm{X}}_t-\vec{\bm{\mu}}_t)$ as the expected change in the pixel value after the diffusion process runs for some time $t$.  
% It is normalized by the expected variance at $t$, $\vec{\bm{\sigma}}_t^2$.


% \section{REMOVE ME: documentation for Predictor}

% The construction of the sampling procedure is defined in \texttt{sampling.py:80} by function \texttt{get\_sampling\_fn}.

% The core sampling loop is implemented in \texttt{sampling.py:390} in
% \begin{verbatim}
%     with torch.no_grad():
%       # Initial sample                                                                                                           
%       x = sde.prior_sampling(shape).to(device)
%       timesteps = torch.linspace(sde.T, eps, sde.N, device=device)

%       for i in range(sde.N):
%         t = timesteps[i]
%         vec_t = torch.ones(shape[0], device=t.device) * t
%         x, x_mean = corrector_update_fn(x, vec_t, model=model)
%         x, x_mean = predictor_update_fn(x, vec_t, model=model)

%       return inverse_scaler(x_mean if denoise else x), sde.N * (n_steps + 1)    
% \end{verbatim}

% \noindent
% The update function in the iterative loop is implemented below:


% \begin{verbatim}
% class EulerMaruyamaPredictor(Predictor):
%   def __init__(self, sde, score_fn, probability_flow=False):
%     super().__init__(sde, score_fn, probability_flow)

%   def update_fn(self, x, t):
%     dt = -1. / self.rsde.N
%     z = torch.randn_like(x)
%     drift, diffusion = self.rsde.sde(x, t)
%     x_mean = x + drift * dt
%     x = x_mean + diffusion[:, None, None, None] * np.sqrt(-dt) * z
%     return x, x_mean
% \end{verbatim}

% \noindent
% \texttt{rsde.sde} in the update function is defined in the base class Predictor which is inherited by the class EulerMaruyamaPredictor. The \texttt{rse.sde} function is then: 

% \begin{verbatim}
%       def sde(self, x, t):
%         """Create the drift and diffusion functions for the reverse SDE/ODE."""
%         drift, diffusion = sde_fn(x, t)
%         score = score_fn(x, t)
%         drift = drift - diffusion[:, None, None, None] ** 2 * score * (0.5 if self.probability_flow else 1.)
%         # Set the diffusion function to zero for ODEs.                                                                                                                                        
%         diffusion = 0. if self.probability_flow else diffusion
%         return drift, diffusion
% \end{verbatim}

% \noindent
% Here, the form of sde\_fn comes from the same forward sde function from the VPSDE class.

% \begin{verbatim}
%   def sde(self, x, t):
%     beta_t = self.beta_0 + t * (self.beta_1 - self.beta_0)
%     drift = -0.5 * beta_t[:, None, None, None] * x
%     diffusion = torch.sqrt(beta_t)
%     return drift, diffusion
% \end{verbatim}

% \noindent
% And score\_fn is the neural network.

% So basically, we start at t=$T$ and an $\vec{\bm{X}}_T=\cev{\bm{X}}_0$. 
% The latter is sampled from the normal distribution. 
% At each time step, we first set $t_{next}$ to $t_{next} = t + dt$. ($dt$, as defined, is negative.) 
% Then we sample the next state, $\vec{\bm{X}}_{t_{next}}$, where $t_{next}=t_{i-1}$. 
% We repeat for the number of time steps until $t=0=t_{i=0}$. 
% We return the final generated, $\vec{\bm{X}}_0=\cev{\bm{X}}_T$.

\end{document}
