\section{Introduction}
\label{section:intro}
% 可以展开的东西
% STGNN的描述
% STGNN的问题
% Long-Term Traffic Forecasting的难点

% \textbf{Backgrounds.}
% Therefore, traffic forecasting has always been a hot topic of research in academia and industry.
{\color{black}
Traffic forecasting aims at predicting future traffic conditions (\eg traffic speed or flow) based on historical traffic conditions observed by sensors.
% 
With the development of Intelligent Transportation Systems~(ITS), traffic forecasting fuels a wide range of services related to traffic scheduling, public safety, \etc~\cite{2022D2STGNN, JGY1}.
% 
For example, predicting long-term traffic changes (\eg 1 day) is valuable for people to plan their route in advance to avoid possible traffic congestion.

In general, traffic data consists of multiple time series, where each time series records traffic conditions observed by sensors deployed on a road network.
% 
A critical property of traffic data is that there exist strong correlations between time series owing to the connection of road networks~\cite{2018DCRNN}.
% 
To make accurate traffic forecasting, state-of-the-art proposals~\cite{2018DCRNN,2019GWNet,2022D2STGNN, JGY2} usually adopt Spatial-Temporal Graph Neural Networks (STGNNs), which model the correlations between time series based on Graph Convolution Networks~(GCNs)~\cite{2016GCN,2017GCN,2018DCRNN}.
However, there is no free lunch. 
Graph convolution brings significant {\color{black}improvements in performance and complexity at the same time. }
Computational complexity usually increases linearly or quadratically with the length and number of time series~\cite{2022STEP}.
Therefore, it is difficult for STGNNs to scale to long-term historical traffic data, let alone predict long-term future traffic conditions.
In fact, most existing works focus on short-term traffic prediction, \eg predicting future 12 time steps~(1 hour in commonly used datasets).
% Although some recent works~\cite{2022STEP} attempt to use longer-term historical data, they are still designed for short-term forecasting.
Such an inability to make long-term traffic forecasting limits the practicality of these models.

% Figure environment removed

In this paper, we focus on long-term traffic forecasting, \eg predicting a future day.
% 
{\color{black}
Except for the correlations between time series, we argue that the long-term traffic forecasting task has its own uniqueness.}
% 
In the following, we discuss them in detail to motivate model design.
% 
An example of traffic data is shown in Figure \ref{figure:intro}(a).
% 
On the one hand, when observing from a global perspective, traffic data usually exhibit regular changes, \eg daily periodicity.
% 
On the other hand, local details are also extremely important for traffic forecasting. 
For example, we must capture the rapidly decreasing traffic {\color{black}changes} when daily traffic congestion occurs.
% 
{\color{black}
To capture these different patterns, we argue that exploiting multi-scale representations of traffic data is the key challenge of accurate long-term traffic forecasting.}
% Based on the above observations, we argue that the bottleneck of long-term traffic forecasting is exploiting multi-scale representations of traffic data.
% 
Specifically, smaller-scale and larger-scale representations are extracted based on smaller and larger receptive fields, respectively.
% , respectively.
The former is usually semantically weak but fine-grained, which facilitates the prediction of local details, \eg rapid changes during traffic congestion.
In contrast, the latter is coarse-grained but semantically strong, which is helpful in predicting global changes, \eg daily periodicity. 
% 
An illustration is shown in Figure \ref{figure:intro}(b). 
The prediction based on large-scale features captures daily periodicity but misses local details, which can be fixed by further incorporating small-scale features.

However, it is a challenging task to exploit multi-scale representations of traffic data.
% 
We discuss it from two aspects: \textit{generating} and \textit{utilizing} multi-scale representations.
% of traffic data.
% 
\textit{On the one hand}, most existing models cannot generate multi-scale representations of traffic data.
% 
State-of-the-art models for long-sequence time series forecasting~\cite{2021Informer} mainly adopt Transformer architecture to capture the long-term dependencies based on self-attention mechanisms~\cite{2017Transformer}.
% 
However, standard self-attention naturally has a global receptive field and thus can only generate representations on a fixed scale.
% which is unsuitable for long-term traffic forecasting.
% 
\textit{On the other hand}, utilizing multi-scale representations for traffic forecasting is also a challenging task, as it usually requires a specific decoder.
% to effectively use multi-scale features.
% 
For example, in computer vision tasks like object detection and semantic segmentation, researchers designed decoders such as FPN~\cite{2017FPN} and U-Net~\cite{2015UNet} to utilize the multi-scale representations extracted by the pre-trained CNN~\cite{2016ResNet} encoder.
% 
{\color{black}
These architectures usually require pixel alignment of input and output images.
% the pixels of the input and output images are aligned. 
However, the historical and future sequences in traffic \textit{forecasting} problems are not the same sequences, \ie not aligned, making existing approaches~\cite{2017FPN,2015UNet} inapplicable.}
% However, unlike computer vision tasks, where the pixels of the input and output images are usually aligned, the history and future sequences in the traffic forecasting problem are not aligned, making existing approaches~\cite{2017FPN,2015UNet} inapplicable.

Based on the above discussion, we summarize three challenges that the desired long-term traffic forecasting model should address.
{\color{black}
% 
First, it must \textit{efficiently} model the correlations between multiple long-term time series.
% 
Second, it should \textit{generate} multi-scale representations of traffic data by an encoder.
% 
Third, it should include a decoder for traffic forecasting tasks to effectively \textit{utilize} the multi-scale representations generated by the encoder.}

To address the above challenges, we propose a novel \textit{Hierarchical U-net TransFormer} (named HUTFormer).
As shown in Figure \ref{fig:main_arch}, HUTFormer is a two-stage model consisting of a hierarchical encoder and a hierarchical decoder, forming an inverted U-shaped structure.
% 
{\color{black}
\textit{To address the efficiency problem}}, HUTFormer designs an efficient input embedding strategy, which employs segment embedding and spatial-temporal positional encoding to significantly reduce the complexity of modeling multiple long-term time series in both temporal and spatial dimensions.
% 
% The HUTForm encoder proposes a windowed converter layer to limit the received field, and then designs segments to cooperate as a pooling layer to extract features at larger scales.
{\color{black}
\textit{To generate multi-scale representations}}, the HUTFormer encoder {\color{black}proposes} a window Transformer layer to limit the receptive field, and then {\color{black}designs} segment merging as a pooling layer to extract larger-scale features.
Thus, lower layers of the encoder focus on {\color{black}smaller-scale} features, while higher layers generate {\color{black}larger-scale} features.
Then, HUTFormer makes an intermediate prediction based on the top-level representations.
% 
{\color{black}
\textit{To utilize multi-scale representations}, the HUTFormer decoder proposes a cross-scale attention mechanism to address the misalignment issue}, which retrieves information for each segment of the intermediate prediction from multi-scale representations, thus enabling the fine-tuning of the intermediate prediction.
% 
By exploiting the multi-scale representations of traffic data, HUTFormer is capable of making accurate long-term traffic forecasting. 
The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item To our best knowledge, this is the first attempt to study long-term traffic forecasting. 
    % We analyze its unique challenges, and then propose a novel \textit{Hierarchical U-net TransFormer}~(HUTFormer) to efficiently and effectively exploit the multi-scale representations of traffic data.
     We {\color{black}reveal} its unique challenges in exploiting multi-scale representations of traffic data, and propose a novel \textit{Hierarchical U-net TransFormer}~(HUTFormer) to address them.
    \item 
    We {\color{black}propose} window self-attention and cross-scale attention mechanisms to generate and utilize multi-scale representations effectively. 
    In addition, to address complexity issues, we design an input embedding strategy that includes segment embedding and spatial-temporal positional encoding.
    \item Extensive experiments on four traffic datasets show that the proposed HUTFormer significantly outperforms state-of-the-art traffic forecasting and long-sequence time series forecasting baselines, and effectively exploits the multi-scale representations of traffic data.
\end{itemize}
}

% \subsection{Backgrounds}
% What is traffic forecasting and what is its role? (Why we need traffic forecasting)
% \subsection{Data Modeling and Problem Definition}
% How to define the traffic forecasting problem, and how to model the traffic data.
% \subsection{Related Work}
% How do previous works do and why they can not address long-term traffic forecasting?
% Why do we need long-term traffic forecasting?
% \subsection{Motivation and Challenges}
% The unique challenges compared to short-term traffic forecasting
% \subsection{Methods}
% What do we do to solve these challenges?
% \subsection{Experiments}
% Experiments
% \subsection{Contributions}
% We are the first ones to study/focuse on the long-term traffic forecasting.
% We propose a hierarchical unet transformer to solove xxx and archive long-term traffic forecasting.
% Experiments.
