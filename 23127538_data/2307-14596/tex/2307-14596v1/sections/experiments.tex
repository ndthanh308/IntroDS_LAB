% % % % % % % % % % % % % % % % % % % % % % % % 
%   实验设置：
%           数据集描述
%           基线模型描述
%           超参数设置
%   主实验结果：
%           实验设置
%           长序列预测模型的效果
%           时空图神经网络的效果
%           我们的效果
%           原因分析，引出下面的消融实验
%   消融实验：
%           目标1：验证层次化多尺度表征的有效性
%           1. 验证Decoder的作用：Decoder有用
%               1.1：使用Concat来替代Decoder
%               1.2：去除Decoder
%           2. 验证Encoder中层次化表征的作用：层次化多尺度表征有用
%               2.1 基于1.2，去除Segment Merging和Window Transformer Layer，和1.2对比。
%           目标2:验证Input Embedding策略的有效性
%           3. 验证ST-PE的作用
%               3.1 使用Learnable Positional Encoding替代STPE
%               3.2 使用GCN来替代
%           4. 验证Segment Embedding的作用
%               4.1 去除Segment Embedding （OOM）
%           目标3:验证训练策略的有效性
%           5. 验证训练策略
%               5.1 End2End
%               5.2 2Stage但是不固定Encoder
%   可视化实验：
%           1. 预测结果可视化
%               1.1 HUTFormer的结果可视化
%               1.2 没有Hierarchical Multi-Scale Feature的可视化（即2.1）
%           2. 位置编码可视化
%   超参数实验：
%           1. 针对Input Embeddin的
%               1.1 Patch Size
%               1.2 Encoder Patch Dim
%               1.3 Decoder Patch Dim
%           2. 针对Encoder的
%               2.1 Window Size
% % % % % % % % % % % % % % % % % % % % % % % % 


\section{Experiments}
\label{sec:exp}
In this section, we conduct extensive experiments on four real-world traffic datasets to validate the effectiveness of HUTFormer for long-term traffic forecasting.
First, we introduce the experimental settings, including datasets, baselines, and implementation details.
Then, we compare HUTFormer with other state-of-the-art traffic forecasting baselines and long-sequence time series forecasting baselines.
Furthermore, we conduct more experiments to evaluate the impact of important components and strategies, including the effectiveness of the hierarchical U-net structure, the input embedding strategy, and the two-stage training strategy.
% More details, such as baseline descriptions, optimization settings, and more visualizations, can be found in the Appendix.
% The code of can be found in this repository\footnote{\url{}}.
% Besides, you can also find experimental results on more datasets.

\subsection{Experimental Setting}
\noindent
\textbf{Datasets.}
We conduct experiments on four commonly used traffic datasets, including two traffic speed datasets~(METR-LA and PEMS-BAY) and two traffic flow datasets~(PEMS04 and PEMS08).
The statistical information is summarized in Table \ref{tab:datasets}.
% Details are neglect
% We omit their details for simplicity.

\begin{itemize}
    \item \textbf{METR-LA} is a traffic speed dataset collected from loop-detectors located on the LA County road network~\cite{METR-LA}. 
    It contains data of 207 selected sensors over a period of 4 months from Mar to Jun in 2012~\cite{2018DCRNN}. 
    The traffic information is recorded at the rate of every 5 minutes, and the total number of time slices is 34,272.
    \item \textbf{PEMS-BAY} is a traffic speed dataset collected from California Transportation Agencies (CalTrans) Performance Measurement System (PeMS)~\cite{PEMS-BAY}.
    It contains data of 325 sensors in the Bay Area over a period of 6 months from Jan 1st 2017 to May 31th 2017~\cite{2018DCRNN}.
    The traffic information is recorded at the rate of every 5 minutes, and the total number of time slices is 52,116.
    \item \textbf{PEMS04} is a traffic flow dataset also collected from CalTrans PeMS.
    It contains data of 307 sensors over a period of 2 months from Jan 1st 2018 to Feb 28th 2018~\cite{2019ASTGCN}.
    The traffic information is recorded at the rate of every 5 minutes, and the total number of time slices is 16,992.
    \item \color{black}{\textbf{PEMS08} is a public traffic flow dataset collected from CalTrans PeMS. 
    Specifically, PEMS08 contains data of 170 sensors in San Bernardino over a period of 2 months from July 1st 2018 to Aug 31th 2018~\cite{2019ASTGCN}.
    The traffic information is recorded at the rate of every 5 minutes, and the total number of time slices is 17,856.}
\end{itemize}

\begin{table}
% \setlength{\abovecaptionskip}{0.cm}
% \setlength{\belowcaptionskip}{-0.cm}
\caption{Statistics of datasets.}
\label{tab:datasets}
\scalebox{1}{
  \begin{tabular}{c|c|c|c|c}
    \toprule
    \textbf{Type} &\textbf{Dataset} & \textbf{\# Samples} & \textbf{\# Sensors} & \textbf{Sample Rate}\\
    \midrule
    \multirow{2}*{\textbf{Speed}} &
    \textbf{METR-LA} & 34272  & 207 & 5 minutes\\
    & \textbf{PEMS-BAY} & 52116  & 325 & 5 minutes\\
    % \hline
    \midrule
    % \cline
    \multirow{2}*{\textbf{Flow}}  & 
    \textbf{PEMS04} & 16992 & 307 & 5 minutes\\
    & \textbf{PEMS08} & 17856 & 170 & 5 minutes\\
    \bottomrule
  \end{tabular}
  }
\end{table}

\noindent
\textbf{Baselines.}
% We select a wealth of state-of-the-art baselines of both traffic forecasting~\cite{2018DCRNN} and long-sequence forecasting~\cite{2021Informer}.
On the one hand, 
% for the traffic forecasting baselines, we select state-of-the-art STGNNs, 
we select six traffic forecasting baselines, including 
\begin{itemize}
    \item \textbf{DCRNN}~\cite{2018DCRNN}~(Diffusion Convolutional Recurrent Neural Network) is one of the earliest works for STGNN-based traffic forecasting, which replaces the fully connected layer in GRU~\cite{2014GRU} by diffusion convolutional layer to form a Diffusion Convolutional Gated Recurrent Unit (DCGRU). 
    \item \textbf{Graph WaveNet}~\cite{2019GWNet} is a traffic forecasting model, which stacks gated temporal convolutional layer and GCN layer by layer to jointly capture the spatial and temporal dependencies.
    \item \textbf{MTGNN}~\cite{2020MTGNN} is a traffic forecasting model, which extends Graph WaveNet through the mix-hop propagation layer in the spatial module, the dilated inception layer in the temporal module, and a delicate graph learning layer.
    \item \textbf{STID}~\cite{2022STID} is a simple but effective baseline for traffic forecasting, which identifies the indistinguishability of samples in both spatial and temporal dimensions as a key bottleneck, and addresses the indistinguishability by attaching spatial and temporal identities.
    \item \textbf{STEP}~\cite{2022STEP} is a traffic forecasting model, which enhances existing STGNNs with the help of a time series pre-training model.
    It significantly extends the length of historical data.
    \item \textbf{D$^2$STGNN}~\cite{2022D2STGNN}: D$^2$STGNN is a state-of-the-art traffic forecasting model, which identifies the diffusion process and inherent process in traffic data, and further decouples them for better modeling. 
\end{itemize}

% 
On the other hand, 
we also select six long-sequence forecasting baselines, 
including 
\begin{itemize}
    \item \textbf{HI}~\cite{2020HI}: HI~(Historical Inertia) is a basic baseline for long-sequence time series forecasting problems, which  directly takes the most recent time steps in the input as output.
    \item \textbf{DLinear}~\cite{2023DLinear}: DLinear is a simple but effective long-sequence time series forecasting model, which decomposes the time series into a trend and a remainder series and employs two one-layer linear networks to model these two series.
    \item \textbf{Informer}~\cite{2021Informer}: Informer is a model for long-sequence time series forecasting, which designs a ProbSparse self-attention mechanism and distilling operation to handle the challenges of the quadratic complexity in the standard Transformer.
    Also, it carefully designs a generative decoder to alleviate the limitation of standard encoder-decoder architecture.
    \item \textbf{Autoformer}~\cite{2021AutoFormer}: Autoformer is a model for long-sequence time series forecasting, which is proposed as a decomposition architecture by embedding the series decomposition block as an inner operator. Besides, it designs an efficient Auto-Correlation mechanism to conduct dependencies discovery and information aggregation at the series level.
    \item \textbf{FEDformer}~\cite{2022FEDFormer}: FEDformer~\cite{2022FEDFormer} is a frequency-enhance Transformer for long-sequence time series forecasting problems. It proposes an attention mechanism with low-rank approximation in frequency and a mixture of experts decomposition to control the distribution shifting.
    \item \textbf{Pyraformer}~\cite{2022Pyraformer}: Pyraformer~\cite{2022Pyraformer}
    is a pyramidal attention-based model for long-sequence time series forecasting.
    Pyramidal attention can effectively describe short and long temporal dependencies with low time and space complexity.
    % By adjusting some hyper-parameters, Pyraformer can achieve the theoretical O(L) complexity and O(1) maximum signal traversing path length.
    
\end{itemize}

\noindent
\textbf{Metrics.}
In this paper, we evaluate the performances of all baselines by Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) metrics. 
First, the MAE metric reflects the absolute prediction error, but is affected by the units of the dataset. 
For example, traffic speed datasets usually take values between 0km/h and 70km/h, while traffic flow datasets usually take values between 0 and hundreds.
Thus, we also adopt MAPE, which can eliminate the impact of data units and reflects the relative error, helping to understand the accuracy more intuitively.


\noindent
\textbf{Implementation.}
For all datasets, we use historical $T_p=288$ time steps~(\ie one day) to predict future $T_f=288$ time steps.
For HUTFormer, we set the segment length $L$ to 12, and the number of segments $P=24$~($L\times P=288)$.
We set the window size to $3$.
We set the hidden dimension of temporal embedding $\textbf{T}^{TiD}$ to 8, while others $d$ to 32.
% We set all the hidden dimensions $d$ to 32, except for the 
% the hidden dimension $d$ to 32.
The depth of HUTFormer is set to $4$.
For baselines, we adopt the default settings in their paper.
Moreover, as discussed before, STGNNs can not directly handle the long-term traffic forecasting task due to their high complexity.
Therefore, we first apply the segment embedding to reduce the length of input tokens for them\footnote{
Methods implemented with segment embeddings are marked with $*$.}.
% Baselines and HUTFormer are compared based on a unified pipeline to ensure the fairness of the results.
% More details can be found in our code repository.


% \section{More Experiments Details}
% \subsection

\noindent\textbf{Optimization Settings.}
For both encoding and decoding stages, we apply the optimization settings in Tabel \ref{tab:opt_enc}.
Specifically, we adopt Adam as our optimizer, and set learning rate and weight decay to $0.0005$ and $0.0001$, respectively.
The batch size is set to 64.
In addition, we use a learning rate scheduler, MultiStepLR, which adjusts the learning rate at epochs 1, 40, 80, and 120 with gamma 0.5.
Moreover, the gradient clip is set to 5.
All the experiments in Section \ref{sec:exp} are running on an Intel(R) Xeon(R) Gold 5217 CPU @ 3.00GHz, 128G RAM computing server, equipped with RTX 3090 graphics cards.

\begin{table}[h]
\caption{Optimization settings.}
\label{tab:opt_enc}
\centering  
\begin{tabular}{p{3cm}|p{4cm}}
\toprule
config  & value \\
\midrule
optimizer & Adam\\
learning rate & 0.0005\\
batch size & 64\\
weight decay & 0.0001\\
learning rate schedule & MultiStepLR\\
milestones & [1, 40, 80, 120]\\
gamma & 0.5\\
gradient clip & 5\\
\bottomrule
\end{tabular}
\end{table}
\input{tables/traffic_speed.tex}



% STGNN
% BasicTS
% Hyper-Parameters
\subsection{Main Results}
\label{section:main_results}
\noindent
\textbf{Settings.} 
% As mentioned before, we use historical $T_p=288$ time steps~(\ie one day) to predict future $T_f=288$ time steps.
We follow the dataset division in previous works.
Specifically, for traffic speed datasets (METR-LA and PEMS-BAY), we use 70\%, 10\%, and 20\% of the data for training, validating, and testing, respectively.
For traffic flow datasets (PEMS04 and PEMS08), we use 60\%, 20\%, and 20\% of data for training, validating, and testing, respectively.
We compare the performance at 1, 4, 8, 12, 16, and 24 hours~(horizon 12, 48, 96, 144, 192, and 288) of forecasting on the MAE and MAPE metrics.

\noindent
\textbf{Results.}
% 综述
The results of traffic speed and traffic flow forecasting are shown in Table \ref{tab:traffic_speed} and \ref{tab:traffic_flow}, respectively.
In general, HUTFormer consistently outperforms all baselines, indicating its effectiveness.


% 长序列模型，例如AutoFormer、FEDFormer等在交通预测问题上的表现并不好。
% 主要原因是他们的模型不符合交通数据的独特性。
Long-sequence forecasting models do not perform well on traffic forecasting tasks.
We conjecture that the main reason is that these models do not fit the characteristics of traffic data.
% 一方面，交通数据和其他时间序列数据的关键不同在于，时间序列之间有很强的关联。
\textit{First}, there exist strong correlations between the time series of traffic data.
% 
% 这是由于路网的限制，传感器记录的取值之间通常有很强的联系。例如相邻的传感器可能会更加相似，或者地理功能区域相似的传感器会更加相似【ST-MetaNet】。
For example, due to the constraint of road networks, time series from adjacent sensors or from similar geographical functional areas may be more similar~\cite{2019STMetaNet}.
Understanding and exploiting the correlations between time series is essential for traffic forecasting.
However, long-sequence forecasting models are usually not concerned with such spatial dependencies.
% 另一方面，长期交通预测要求对全局和局部依赖的联合利用。然而，长序列预测模型主要聚焦计算全局的依赖，例如通过优化普通self-attention的效率、在series级别上计算auto correlation。
\textit{Second}, as discussed in Section \ref{section:intro}, the long-term traffic forecasting task requires exploiting multi-scale representations to capture the complex dynamics of traffic data. 
However, most long-term sequence forecasting models mainly focus on capturing global dependencies based on self-attention mechanisms.
For example, Informer~\cite{2021Informer} optimizes the efficiency of the original self-attention mechanism through the ProbSparse mechanism.
Autoformer~\cite{2021AutoFormer} conducts the dependencies discovery at the series level.
They can not generate and utilize multi-scale representations of traffic data.
In summary, the above-mentioned uniqueness of long-term traffic forecasting tasks significantly affects the effectiveness of long-sequence forecasting models.

Compared to long-sequence forecasting models, 
traffic forecasting models achieve better performance.
This is mainly because they model correlations between time series with the help of graph convolution.
Most of them~\cite{2018DCRNN,2019GWNet,2020MTGNN,2022STEP, 2022D2STGNN} utilize diffusion convolution, a variant of graph convolution, to model the diffusion process at each time step.
However, there is no free lunch.
The graph convolution brings a high complexity~\cite{2022STEP}.
As mentioned earlier, we had to implement these models with the segment embedding in HUTFormer to reduce the length of input tokens to make them runnable.
Kindly note that although the latest baseline STEP~\cite{2022STEP} can handle long-term historical data, it still requires a downstream STGNN as the backend, which can only make short-term future predictions.
In summary, these models only focus on short-term traffic forecasting and do not consider the uniqueness of long-term traffic forecasting, \ie exploiting multi-scale representations.

Compared to all baselines, HUTFormer achieves state-of-the-art performances by sufficiently addressing the issues of long-term traffic forecasting tasks.
Specifically, on the one hand, HUTFormer efficiently handles the correlations between long-term time series with spatial-temporal positional encoding and segment embedding.
On the other hand, HUTFormer effectively generates and utilizes multi-scale representations based on the hierarchical U-net.

\input{tables/traffic_flow.tex}

\input{tables/more_datasets}


\subsection{Efficiency}
\label{appendix:efficiency}
In this section, we conduct more experiments to evaluate the efficiency of the HUTFormer variants in Section \ref{section:ablation}.
We conduct experiments with a single NVIDIA V100 graphics card with 32GB memory, and report the GPU memory usage and running time.
Specifically, for the two-stage training variants, we report the largest GPU memory usage of the two stages and report the sum of the running time in the encoding and decoding stages.
We conduct experiments on the METR-LA dataset.

% Figure environment removed

The results are shown in Figure \ref{figure:appendix_efficency}.
First, we can see that removing the segment embedding~(\ie \textit{w/o SE}) will significantly increase the computational complexity, and require more GPU memory.
Second, compared with applying GCN, HUTFormer is more efficient and effective by leveraging the spatial-temporal positional encoding, which does not increase much complexity.


\subsection{Generalization}
The ability of HUTFormer to generate and utilize multi-scale features should also be effective in many non-traffic data, since the multi-scale features widely exists in many domains. In order to verify the generalization of HUTFormer, in this part, we compare HUTFormer with more latest Transformer-based long time series forecasting models~\cite{2023Crossformer, Triformer} based on three commonly used long-sequence prediction datasets, ETTh1, ETTm1, and Weather. The details of Crossformer~\cite{2023Crossformer} and Triformer~\cite{Triformer} as well as the three datasets are neglected for simplicity. Interest readers can refer to their papers~\cite{2023Crossformer, Triformer}.
We use the same setting as the other datasets in our paper.
As shown in the table \ref{tab:more_datasets}, HUTFormer still outperforms these models on these datasets, which further verifies the effectiveness and generalization of HUTFormer.

\subsection{Ablation Study}
\label{section:ablation}

% \input{tables/ablation_uni.tex}
\input{tables/ablation_metrla.tex}

In this subsection, we conduct more experiments to evaluate the impact of some important components and strategies.
Specifically, we evaluate from three aspects, including the effectiveness of the hierarchical U-net structure, the input embedding strategy, and the two-stage training strategy.
Due to space limitations, we only present the results on METR-LA datasets in Table \ref{tab:ablation}.


The hierarchical U-net structure is designed to generate and exploit multi-scale features.
Specifically, the encoder combines window self-attention and segment merging to generate multi-scale features, while the decoder primarily utilizes extracted features based on cross-scale attention.
% 
Therefore, to evaluate their effectiveness, we set up three variants.
{\color{black}
First, we replace the decoder with a simple concatenation, named $HUTFormer\ concat$.
The concatenation of features from different scales naturally preserves all information.
Second, we set $HUTFormer\ w/o\ decoder$ to remove the decoder and use the intermediate prediction as the final prediction.
The above two variants are used to demonstrate that exploiting multi-scale features is a non-trivial challenge and our hierarchical decoder is effective.}
% 
Third, we set $HUTFormer\ w/o\ hierarchy$ to further remove segment merging and replace the window Transformer layer with a standard Transformer layer, to evaluate the effectiveness of hierarchical multi-scale representations.
% their effectiveness.
As shown in Table4 \ref{tab:ablation}, 
$HUTFormer$ significantly outperforms $HUTFormer\ concat$ and $HUTFormer\ w/o\ decoder$, which shows that it is not an easy task to utilize the multi-scale features, and also validates the effectiveness of our decoder.
In addition, the results of $HUTFormer\ w/o\ hierarchy$ show that hierarchical multi-scale features are crucial for accurate long-term traffic forecasting.
The above results show that generating and utilizing hierarchical multi-scale features is important, and the designed hierarchical U-net structure is effective.
% hierarchical encoder and decoder we designed are effective.



% Figure environment removed

{\color{black}
The input embedding strategy aims to address the complexity issue from both spatial and temporal dimensions.
Specifically, it consists of a Segment Embedding~(SE) and a Spatial-Temporal Positional Encoding~(ST-PE).}
To verify their effectiveness, we set up three variants.
First, we set $HUTFormer\ w/o\ ST\text{-}PE$, which replaces the ST-PE with standard learnable positional encoding.
Second, we set $HUTFormer\ GCN$, which replaces the spatial embeddings in ST-PE with graph convolution~\cite{2019GWNet}.
Third, we remove the segment embedding to get $HUTFormer\ w/o\ SE$.
As shown in Table \ref{tab:ablation}, 
without ST-PE, 
the performance of HUTFormer decreases significantly.
This is because modeling the correlations between time series is the basis of traffic forecasting.
% This is because without modeling the correlation between time series, models can only 
In addition, we can see that the ST-PE strategy is significantly better than performing graph convolution, indicating the superiority of ST-PE.
Moreover, removing segment embedding not only leads to a significant decrease in performance but also increases the complexity due to the increased sequence length.
{\color{black}
These results indicate the effectiveness of the spatial-temporal positional encoding and segment merging.}


% The results of $HUTFormer\ GCN$ is similar to original $HUTFormer$. 
% However, we find that the memory usage is twice that of HUTFormer.


Finally, we evaluate the two-stage training strategy of HUTFormer.
To this end, we set two variants.
First, we set $HUTFormer$ $end2end$, which trains the HUTFormer in an end-to-end strategy.
Second, we set $HUTFormer\ w/o\ fix$, which does not fix the parameter of the encoder when training the decoder.
% 
The results in Table \ref{tab:ablation} show that either the end-to-end strategy or the strategy without fixing the encoder leads to insufficient optimization and significant performance degradation.
% 
In addition, both strategies require more memory.
In contrast, our two-stage strategy achieves the best performance and efficiency simultaneously.



\subsection{Hyper-parameter Study}
\label{sec:hyper}
% In this subsection, we conduct experiments to study the impact of two key hyper-parameters: segment size $L$ and hidden dimension $d$. 
% We conduct experiments on the METR-LA dataset and report the MAE at horizon 288.
% % 
% As shown in Figure \ref{figure:vis_hyper}(a), the segment size $L=12$ achieves the best performance, mainly because smaller segments cannot provide robust semantics, while larger segments ignore more local details.
% In addition, we can see that as the segment size increases, the encoder runs faster~(seconds/epoch).
% Kindly note that changing the segment size also changes the depth of the HUTFormer to ensure that the receptive field covers the entire sequence.
% % 
% The impact of the hidden dimension $d$ is shown in Figure \ref{figure:vis_hyper}(b).
% With the increase of $d$, the performance first rises and then begins to drop slowly. 
% The reason is that too small hidden dimensions are insufficient to encode complex dynamics in traffic data, while larger dimensions might introduce more redundancy.
In this subsection, we conduct experiments to study the impact of two key hyper-parameters: segment size and window size.
We conduct experiments on the METR-LA dataset and report the MAE at horizon 288.
% 
Moreover, we report the training speed of the encoder, since these hyper-parameters mainly affect the encoder.
% 
As shown in Figure \ref{figure:vis_hyper}(a), the segment size $L=12$ achieves the best performance. Smaller segments cannot provide robust semantics, while larger segments ignore more local details.
% 
In addition, we can see that as the segment size increases, the encoder runs faster~(seconds/epoch).
Kindly note that changing the segment size may change the depth of the HUTFormer to ensure that the receptive field covers the entire sequence.
% 
The impact of the window size is shown in Figure \ref{figure:vis_hyper}(b), where larger window sizes perform worse.
This is because the ability to extract multi-scale features is weakened as the window size increases.
Moreover, the efficiency of HUTFormer will also decrease~\cite{2021SwinTransformer} on larger window sizes.

% Figure environment removed

\subsection{Visualization}
\subsubsection{Spatial-Temporal Positional Encoding}
To further understand the HUTFormer in modeling the correlations between multiple time series in traffic data, we analyze the spatial-temporal positional encoding layer.
% 
Modeling correlations between multiple time series have been widely discussed in multivariate time series forecasting~\cite{2019GWNet, 2020MTGNN,2022STEP}.
% 
Previous works usually utilize Graph Convolution Networks~(GCN), which conduct message passing in a pre-defined graph.
GCN is a powerful model, but it has high complexity of $\mathcal{O}(N^2)$.
Very recent works, STID~\cite{2022STID} and ST-Norm~\cite{2021STNorm}, identify that graph convolution in multivariate time series forecasting is essentially used for addressing the indistinguishability of samples on the spatial dimension.
Based on such an observation, STID proposes a simple but effective baseline of attaching spatial and temporal identities, achieving a similar performance of GCN but high efficiency.
The Spatial-Temporal Positional Encoding~(ST-PE) is designed based on such an idea~\cite{2022STID}.

The ST-PE contains three learnable positional embeddings, $\mathbf{E}\in\mathbb{R}^{N\times d}$, 
$\mathbf{T}^{TiD}\in\mathbb{R}^{N_D\times d}$ and $\mathbf{T}^{DiW}\in\mathbb{R}^{N_W\times d}$, 
where $N$ is the number of time series, $N_D$ is the number of time slots of a day~(determined by the sensor's sampling frequency), and $N_W=7$ is the number of days in a week. 
We utilize t-SNE [14] to visualize these three embedding matrices.
Kindly note that $\mathbf{T}^{DiW}$ only have $7$ embeddings, which is significantly less than the hidden dimension $32$, making it hard to get correct visualizations.
Therefore, we additionally train a HUTFormer with the embedding size of  $\mathbf{T}^{DiW}$ to 2 to get a more accurate visualization.
% , and change the addition-based positional encoding in Eq. (\ref{eq:stpe}) to concatenation-based positional encoding~\cite{2022STID} with a linear reduction layer, to get a more accurate visualization.

The results are shown in Figure \ref{figure:vis_stid}.
First, as shown in Figure \ref{figure:vis_stid}(a), the spatial embeddings are likely to cluster.
% 
For example, traffic conditions observed by sensors that are connected or have similar geographical functionality are more likely to be similar.
However, it is not as apparent as in the results in STID~\cite{2022STID}.
% 
We conjecture this is because the impact of the indistinguishability of the samples becomes weaker as the length of the historical data increases.
Second, Figure \ref{figure:vis_stid}(b) shows the embeddings of 288 time slots, where the daily periodicity is very obvious.
% 
Third, Figure \ref{figure:vis_stid}(c) visualizes the embeddings of each day in a week, where weekdays are closer and weekends' are different.


\subsubsection{Prediction Visualization}
In order to further intuitively evaluate HUTFormer, in this subsection, we visualize the prediction of HUTFormer and other baselines on the METR-LA dataset.
Specifically, we select sensor 12 and displayed its data from June 05th, 2012 to June 06th, 2012 (located in the test dataset). 
The forecasting results of Autoformer~\cite{2021AutoFormer}, Graph WaveNet~\cite{2019GWNet}, HUTFormer w/o hierarchy, and HUTFormer are shown in Figure \ref{figure:vis}.
First, Autoformer performs the worst, since it does not fit the characteristics of traffic data as discussed in Section \ref{section:main_results}.
Graph WaveNet and HUTFormer w/o hierarchy perform well on smooth periods but fail to accurately capture the sudden change when traffic congestion occurs.
In contrast, benefiting from exploiting multi-scale representations of traffic data, HUTFormer performs better in both smooth periods and sudden changes.
% 
% More prediction visualizations can be found in Appendix \ref{appendix:vis}.

% Figure environment removed
