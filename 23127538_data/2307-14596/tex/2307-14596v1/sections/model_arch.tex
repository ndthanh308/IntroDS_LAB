\section{Model Architecture}

% Figure environment removed

\subsection{Overview}
\label{sec:methods:overview}
As illustrated in Figure \ref{fig:main_arch}, HUTFormer is based on a hierarchical U-net structure to \textit{generate} and \textit{utilize} multi-scale representations of traffic data.
In this subsection, we intuitively discuss each component of HUTFormer and its two-stage training strategy.

% First, we elaborate on the framework of HUTFormer.
% 
% Hierarchical Encoder
% \noindent
% \textbf{Hierarchical encoder.} 
First, we discuss the hierarchical encoder.
% 
The window Transformer layer is the basis for generating multi-scale representations, {\color{black}which calculates self-attention within a small window to limit the receptive field.}
% 
Then, segment merging acts as a pooling layer, reducing the sequence length to produce larger-scale representations.
% 
By combining them, lower layers can focus on smaller-scale features while higher layers focus on larger-scale features, thus successfully generating multi-scale features.
% 
Then, an intermediate prediction is made based on the top-layer representations.
However, considering that the top-layer features are semantically strong but coarse-grained, 
% the intermediate prediction can basically predict global changes but may fail to capture rapidly changing local details, \eg the red line in Figure \ref{figure:intro}(b).
the intermediate prediction {\color{black}may fail to capture rapidly changing local details, \eg the red line in Figure \ref{figure:intro}(b).}
% 
% , while essentially capturing global changes, may fail in periods of rapid changes, \eg the green line in Figure \ref{figure:intro}(b).
% Hierarchical Decoder

{\color{black}To address the above problem}, the hierarchical decoder aims to fine-tune the intermediate prediction by incorporating multi-scale representations.
% 
% However, as discussed in Section \ref{section:intro}, the history and future sequence in traffic forecasting tasks are not aligned.
% % 
% Thus, the feature sequences extracted by the encoder and the decoder can not be directly superimposed as regular u-net structures~\cite{2015UNet, 2021SwinUNet} do.
% 
U-net~\cite{2015UNet,2021SwinUNet} is a popular structure for utilizing multi-scale representations, especially in computer vision tasks~(\eg semantic segmentation).
In these tasks, the pixels of the input and target images are aligned, \ie models operate on the same image.
% In these tasks, the pixels of input and target image are aligned, \ie models operating on the same image.
% the input and output image are basically the same image, \ie the pixels of input and output image are aligned.
However, for traffic \textit{forecasting} tasks, 
the input and output sequences are {\color{black}not the same sequence, \ie} not aligned.
Thus, the representations generated by the encoder and the decoder cannot be directly superimposed as regular U-net structures~\cite{2015UNet, 2021SwinUNet} do for computer vision tasks.
% 
To this end, we design a cross-scale Transformer layer, which uses the representations from the decoder as \textit{queries} and the multi-scale features from the encoder as \textit{keys} and \textit{values} to retrieve information.
% 
Such a top-down pathway and lateral connects help to combine the multi-scale representations, thus enhancing the prediction accuracy.

% Efficiency Issue
In addition, HUTFormer addresses complexity issues based on an efficient input embedding strategy, which consists of segment embedding and spatial-temporal positional encoding.
% 
On the one hand, segment embedding {\color{black}reduces complexity from the temporal dimension by using} time series segments as basic input tokens.
This simple operation has significant benefits in both reducing the length of the input sequence and providing more robust semantics~\cite{2022STEP}.
% 
On the other hand, spatial-temporal positional encoding is designed to replace the standard positional encoding~\cite{2017Transformer,2021ViT} in Transformer. 
More importantly, it efficiently models the correlations among time series from the perspective of solving the indistinguishability of samples~\cite{2022STID, 2021STNorm}, 
avoiding the {\color{black}high complexity of conducting graph convolution~\cite{2019GWNet, 2018DCRNN} in the spatial dimension}.
% 
% In addition, HUTFormer reduces complexity issues from temporal and spatial dimensions based on an efficient input embedding strategy, which consists of segment embedding and spatial-temporal positional encoding.
% Overall, HUTFormer is significantly more efficient than previous works that conduct graph convolution at each time steps.
% Overall, HUTFORER works much more efficiently than previous graph convolution on each time slice.

% Finally, we propose the training strategy.
% Specifically, HUTFormer applies a two-stage training strategy.
Finally, we propose the training strategy: a two-stage strategy.
The first stage aims to train the hierarchical encoder based on the Mean Absolute Error~(MAE) between the intermediate prediction and ground truth.
In the second stage, we only train the decoder, while the parameters of the encoder are fixed to act as the multi-scale feature extractor.
% 
The reason for adopting the two-stage strategy is that traffic forecasting tasks are different from tasks that employ an end-to-end strategy (\eg semantic segmentation~\cite{2015UNet, 2021SwinUNet} and object detection~\cite{2017FPN} in computer vision).
%
Specifically, in computer vision tasks, pre-trained vision models (\eg pre-trained ResNet~\cite{2016ResNet}) usually serve as the backbone to extract multi-scale features~\cite{2017FPN}.
However, there is no pre-trained model for time series that can extract multi-scale features.
Therefore, optimizing the feature extractor~(\ie the encoder) and downstream networks~(\ie the decoder) in an end-to-end fashion may be insufficient.
The experimental results in Section \ref{section:ablation} also verify this hypothesis.
% 
Next, we introduce each component in detail.

\subsection{Input Embedding}

\textbf{Segment embedding.}
Most previous works usually use single data points as the basic input units.
However, isolated points of time series usually give less semantics~\cite{2022STEP} and are more easily affected by noise.
Therefore, HUTFormer adopts segment embedding, \ie dividing the input sequence into several segments to get the input tokens.
Specifically, given the time series $\mathbf{X}^i\in\mathbb{R}^{T \times C}$ from sensor $i$, HUTFormer divides it into $P$ non-overlapping segments of length $L$, \ie $T=P*L$. We denote the $j$th segment as $\mathbf{X}^i_j\in\mathbb{R}^{LC}$.
Then, we conduct the input embedding layer based on these segments: 
\begin{equation}
\mathbf{S}_j^i=\mathbf{W}\cdot\mathbf{X}^i_j+\mathbf{b},
\end{equation}
where $\mathbf{S}_j^i\in\mathbb{R}^{d}$ is the embedding of segments $j$ of the time series from sensor $i$, and $d$ is the hidden dimension.
$\mathbf{W}\in\mathbb{R}^{d\times (LC)}$ and $\mathbf{b}\in\mathbb{R}^{d}$ are learnable parameters shared by all segments.

In summary, applying segment embedding brings two benefits.
First, it provides more robust semantics. 
Second, it significantly reduces the sequence length to reduce computational complexity.

\noindent
\textbf{Spatial-temporal positional encoding.}
In this paper, we propose to replace the standard positional encoding in Transformer-based networks~\cite{2017Transformer, 2021ViT} with Spatial-Temporal Positional Encoding~(ST-PE).
Specifically, given the segment embedding $\mathbf{S}_j^i\in\mathbb{R}^{d}$ of segments $j$ from time series $i$, ST-PE conduct positional encoding on the spatial and temporal dimensions simultaneously:
\begin{equation}
    \mathbf{U}_j^i = \text{Linear}(\mathbf{S}_j^i \parallel \mathbf{E}^i \parallel \mathbf{T}_j^{TiD} \parallel \mathbf{T}_j^{DiW}).
    \label{eq:stpe}
\end{equation}
On the spatial dimension, we define the spatial positional embeddings $\mathbf{E}\in\mathbb{R}^{N\times d_1}$, where $N$ is the number of time series~(\ie sensors), and $d_1$ is the hidden dimension.
On the temporal dimension, we define two semantic positional embeddings,
$\mathbf{T}^{TiD}\in\mathbb{R}^{N_D\times d_2}$ and $\mathbf{T}^{DiW}\in\mathbb{R}^{N_W\times d_3}$, 
where $N_D$ is the number of time slots of a day~(determined by the sensor's sampling frequency) and $N_W=7$ is the number of days in a week. The temporal embeddings are thus shared among slots for the same time of the day and the same day of the week.
Semantic temporal positional embeddings are helpful since traffic systems usually reflect the periodicity of human society.
In addition, kindly note that all other baseline models~\cite{2018DCRNN, 2019GWNet, 2022D2STGNN, 2021AutoFormer, 2022FEDFormer, 2022Pyraformer} also use such temporal features, so there is no unfairness.
$\text{Linear}(\cdot)$ is a linear layer to reduce the hidden dimension.
$\mathbf{E}$, $\mathbf{T}^{TiD}$, and $\mathbf{T}^{DiW}$ are trainable parameters.

Embedding $\mathbf{E}$ is vital for reducing the complexity of modeling the spatial correlations between time series.
% 
This is because attaching spatial embeddings plays a similar role to GCN in terms of solving the indistinguishability of samples~\cite{2022STID}, but with two primary advantages. 
% 
% This is because attaching the spatial embeddings plays a similar role to GCN from the perspective of solving the indistinguishability of samples~\cite{2022STID, 2021STNorm}, but with two primary advantages.
On the one hand, it is more efficient than GCNs, which usually have $\mathcal{O}(N^2)$ complexity.
On the other hand, it does not generate many additional network parameters than approaches based on variable-specific modeling~\cite{2020AGCRN, 2022TriFormer}.

\subsection{Hierarchical Encoder}
\textbf{Window Transformer Layer.}
Standard Transformer Layers~\cite{2017Transformer} are designed based on the multi-head self-attention mechanism.
As shown in Figure \ref{fig:w-msa}(a), it computes the attention among all input tokens.
Therefore, each layer of the Transformer Layer has an infinite receptive field, and many works~\cite{2021Informer, 2021AutoFormer, 2022FEDFormer, 2022Pyraformer} try to capture long-term dependencies based on such a feature.

% Figure environment removed

However, the infinite receptive field makes the standard Transformer layers unable to generate multi-scale features~\cite{2021SwinTransformer}.
Inspired by recent development in computer vision~\cite{2021SwinTransformer}, we apply the window self-attention in HUTFormer to extract the hierarchical multi-scale features.
An example of window self-attention with windows size 2 is shown in Figure \ref{fig:w-msa}(b).
Window self-attention forces calculating attention inside non-overlapping windows, thereby limiting the size of the receptive field.
By replacing multi-head self-attention in standard Transformer layers~\cite{2017Transformer} with the Window Multi-head Self-Attention~(W-MSA), we present the window Transformer layer:
\begin{equation}
    \begin{aligned}
    \mathbf{H}^{in'} & = \text{W-MSA}(\text{LN}(\mathbf{H}^{in})) + \mathbf{H}^{in}, \\
    \mathbf{H}^{out} & = \text{MLP}(\text{LN}(\mathbf{H}^{in'})) + \mathbf{H}^{in'},
    \end{aligned}
\end{equation}
where $\text{LN}(\cdot)$ is the layer normalization, and $\text{MLP}(\cdot)$ is the multi-layer perceptron. $\mathbf{H}^{in}\in\mathbb{R}^{P\times d}$ and $\mathbf{H}^{out}\in\mathbb{R}^{P\times d}$ are the input and output sequences. $P$ is the sequence length, and $d$ is the hidden dimension.
By limiting the receptive field size, the window transformer layer is the basis for extracting multi-scale features.

\noindent
\textbf{Segment Merging.} To generate hierarchical multi-scale representations, we adopt segment merging, which reduces the number of tokens and increases the number of hidden dimensions as the network gets deeper. 
As illustrated in Figure \ref{fig:segment_merging}, segment merging divides the token series into non-overlapping groups of size 2, and concatenates the features within each group.

% Figure environment removed


By combining the segment merging and window transformer layer, we get the basic block of the hierarchical encoder~(\ie the blue block in Figure \ref{fig:main_arch}).
Assuming $(\mathbf{H}^i)^{l}_{enc}\in\mathbb{R}^{P^l\times d^l}$ is the representation of time series $i$ after block $l$~($l\geq 1$) of the encoder, the $(l+1)$th block is computed as:
\begin{equation}
    \begin{aligned}
    (\mathbf{H}^i)^{l'}_{enc} &= \text{SegmentMerging}((\mathbf{H}^i)^{l}_{enc}),\\
    (\mathbf{H}^i)^{l+1}_{enc} &= \text{WindowTransformer}((\mathbf{H}^i)^{(l')}_{enc}),
    \end{aligned}
\end{equation}
where $(\mathbf{H}^i)^{l+1}_{enc}\in\mathbb{R}^{P^{l+1}\times d^{l+1}}$ is the representation of time series $i$ after block $l+1$ of the encoder. 
$P^{l+1}=\frac{P^l}{2}$ is the number of tokens after $(l+1)$th layer, and $d^{l+1}=2d^{l+1}$ is the hidden dimension.

\noindent
\textbf{Prediction Layer.}
Assuming there are $S$ blocks in the encoder, HUTFormer makes an intermediate prediction with a linear layer:
\begin{equation}
    \hat{\mathbf{Y}}^i_{enc} = \text{Linear}(\mathop{\parallel}\limits_{j=1}^{P^S}(\mathbf{H}^i_j)^{S}_{enc}),
\end{equation}
where $P^S$ is the number of tokens after the $S$th block.
$\hat{\mathbf{Y}}^i\in\mathbb{R}^{T_f\times C}$ is the prediction of time series $i$.
Considering the prediction from all $N$ time series, $\hat{\mathcal{Y}}^{enc}\in\mathbb{R}^{T_f\times N\times C}$, we compute the Mean Absolute Error~(MAE) as regression loss to train the hierarchical encoder:
\begin{equation}
    \mathcal{L}_{enc} = \frac{1}{T_f N C}\sum_{j=1}^{T_f}\sum_{i=1}^{N}\sum_{k=1}^{C}|\hat{\mathcal{Y}}_{ijk}^{enc} - \mathcal{Y}_{ijk}|.
\end{equation}


\subsection{Hierarchical Decoder}
\textbf{Cross-Scale Transformer Layer.}
The hierarchical decoder aims to effectively utilize the multi-scale features, to fine-tune each segment of the intermediate prediction.
However, as discussed in Section \ref{sec:methods:overview}, the history and future sequence in traffic forecasting tasks are not aligned, making the feature sequences extracted by the encoder and the decoder cannot be directly superimposed.
Therefore, we design a cross-scale attention mechanism to select and incorporate multi-scale features.
Different from self-attention, cross-scale attention utilizes the representations of the decoder as \textit{queries} to retrieve the multi-scale features from the encoder.
For brevity, we denote $\mathbf{H}_{enc}\in\mathbb{R}^{P_{enc}\times d_{enc}}$ as the representation from the encoder and $\mathbf{H}_{dec}\in\mathbb{R}^{P_{dec}\times d_{dec}}$ as the corresponding representation from the decoder.
Then, the Cross-scale Attention~(CA) is computed as:
\begin{equation}
    \begin{aligned}
    \text{CA}(\mathbf{H}_{enc}, \mathbf{H}_{dec})&=\text{Softmax}(\frac{\mathbf{H}_{dec}(\mathbf{H}_{enc}^{'})^T}{\sqrt{d_{dec}}})\mathbf{H}_{enc}^{'},\\
\mathbf{H}_{enc}^{'}&=\text{Linear}(\mathbf{H}_{enc}),
    \end{aligned}
\end{equation}
The $\text{Linear}(\cdot)$ layer is used to transform the hidden dimension from $d_{enc}$ to $d_{dec}$.
By replacing the multi-head self-attention with Multi-head Cross-scale Attention~(MCA), we present the cross-scale Transformer layer as:
\begin{equation}
    \begin{aligned}
    \mathbf{H}^{in'}_{dec} & = \text{MCA}(\text{LN}(\mathbf{H}^{in}_{enc}, \mathbf{H}^{in}_{dec})) + \mathbf{H}^{in}_{dec}, \\
    \mathbf{H}^{out}_{dec} & = \text{MLP}(\text{LN}(\mathbf{H}^{in'}_{dec})) + \mathbf{H}^{in'}_{dec},
    \end{aligned}
\end{equation}
where $\mathbf{H}^{in}_{enc}$ is the multi-scale feature from the encoder, and $\mathbf{H}^{in}_{dec}$ is the input feature from the decoder.
$\mathbf{H}^{out}_{dec}$ is the output of the cross-scale Transformer layer.

\noindent
\textbf{Prediction Layer.} Assuming $(\mathbf{H}^i_j)^S_{dec}\in\mathbb{R}^{d_{dec}}$ is the representation of the decoder's last block~(\ie the $S$th block) for $j$th segment of $i$th time series, HUTFormer makes the final prediction for each segment with a shared linear layer:
\begin{equation}
    (\hat{\mathbf{Y}}^i_j)_{dec} = \text{Linear}((\mathbf{H}^i_j)^{S}_{dec}),
\end{equation}
where $(\hat{\mathbf{Y}}^i_j)_{dec}\in\mathbb{R}^{LC}$ is the final prediction of segment $j$ of time series $i$.
Similar to the encoder, we consider the prediction from all $P_{dec}$ segments~($P_{dec}\times L=T_f$) of all $N$ time series, $\hat{\mathcal{Y}}^{dec}\in\mathbb{R}^{T_f\times N\times C}$, and compute the MAE loss to train the hierarchical decoder:
\begin{equation}
    \mathcal{L}_{dec} = \frac{1}{T_f N C}\sum_{j=1}^{T_f}\sum_{i=1}^{N}\sum_{k=1}^{C}|\hat{\mathcal{Y}}_{ijk}^{dec} - \mathcal{Y}_{ijk}|.
\end{equation}
Kindly note that the parameters of the encoder are fixed during this stage to serve as a pre-training model for extracting robust hierarchical multi-scale representations of traffic data.