\section{Related Work}
\subsection{Traffic Forecasting}
Previous traffic forecasting studies usually fall into two categories, \ie knowledge-driven~(\eg queuing theory~\cite{cascetta2013transportation}) and early data-driven~(\eg sequential models~\cite{kumar2015short, 2014GRU, FC-LSTM, 2016TCN}).
However, these methods usually ignore the correlation between time series and the high non-linearity of time series~\cite{2022D2STGNN}, which severely limits the effectiveness of these methods.
To this end, Spatial-Temporal Graph Neural Networks (STGNNs) were proposed recently~\cite{2018DCRNN} to model the complex spatial-temporal correlations in traffic data.
Specifically, STGNNs combine Graph Neural Networks~(GNNs)~\cite{2017GCN,2016GCN} and sequential models~(\eg CNN~\cite{2016TCN} or RNN~\cite{2014GRU}), to model the complex spatial-temporal correlation in traffic data.
For example, DCRNN~\cite{2018DCRNN}, ST-MetaNet~\cite{2019STMetaNet}, AGCRN~\cite{2020AGCRN}, \etc.~\cite{2021GTS,2018STGCN,2021DGCRN}, are RNN-based methods, which combine GNN with recurrent neural networks.
Graph WaveNet~\cite{2019GWNet}, MTGNN~\cite{2020MTGNN}, STGCN~\cite{2018STGCN}, and StemGNN~\cite{2020StemGNN} are CNN-based methods~\cite{2021DMSTGCN}, which usually combines GNN with the Temporal Convolution Network (TCN~\cite{2016TCN}).
Moreover, the attention mechanism is also a widely used technique in STGNNs~\cite{2020GMAN, 2020STGNN, 2021ASTGNN,2019ASTGCN,2020STGRET}.
% 
Although STGNNs have achieved significant progress, their complexity is high.
Specifically, their complexity usually increases linearly or quadratically with the length and the number of time series~\cite{2022STEP}, since they need to deal with both temporal and spatial dependency at every step.
Thus, most of them focus on short-term traffic forecasting based on short-term history data, \eg predicting future 1-hour traffic conditions based on 1-hour historical data.
A recent work STEP~\cite{2022STEP} attempts to address this issue based on the time series pre-training model.
It significantly enhances STGNN's ability to exploit long-term historical data.
However, STEP requires a downstream STGNN as the backbone, which still focuses on short-term traffic forecasting.

Although STGNN-based traffic forecasting has made significant progress, these models only focus on short-term traffic forecasting, and cannot handle long-term traffic forecasting.
On the one hand, due to the high complexity, most of them can not handle long-term history data, let alone predict long-term future traffic conditions.
On the other hand, apart from efficiency issues, long-term traffic forecasting also has its unique challenges, which require exploiting multi-scale representations of traffic data to capture the complex long-term traffic dynamics.


% 以前的交通工作分为基于知识（例如排队论）的和基于数据的（例如ARIMA）。
% 然而，在空间维度，交通数据中的时间序列之间存在着强相关，在时间维度，交通数据的演化通常是复杂的非线性。
% 使得这些方法在现实世界的数据集上表现不佳。
% 幸运地是，深度学习技术的进展有效地缓解了上述问题。
% 其中，时空图神经网络是目前最强大的模型。
% 总的来说，STGNN结合了图卷积和序列模型来建模交通数据中的时空依赖。
% 一方面，图卷积非常适合出了时间序列之间的非欧关系。
% 另一方面，序列模型，例如GRU、LSTM、TCN等可以用来捕捉空间依赖。
% 例如，DCRNN、GWNet。
% 然而，STGNN的复杂度通常和XXX成平方或者正比例关系。这使得他们无法扩展到长期历史数据中，更无法进一步作出预测。
% 最新的方法STEP尝试通过预训练模型的帮助来建模长期历史数据。
% 然而，它需要一个下游的骨干模型，which仍然专注于短期预测任务。

% 总的来说，基于时空图神经网络的模型仅专注于短期交通预测，忽略了长期交通预测及其独特挑战的挑战。
% 和现有工作不同的是，HUTFormer是首次尝试聚焦于长期交通预测任务，实现了准确的长期交通预测。
% 还没有引用的STGNN：
% STSGCN、STFGCN


\subsection{Long-Sequence Time Series Forecasting}
% 长序列预测模型是的一个研究热点。
% 他们的主要目的标是通过建模长期的历史序列来做出长期的未来预测。
% 例如，Informer首次将Self-Attention引入时间序列预测，利用XXX解决了XXX问题。
% 基于Informer，后续的许多模型被提了出来。
% Autoformer
% FEDFormer
% Linear
% 总的来说，长序列预测模型的主要注意力放在高效地建模长期历史序列上。
% 然而，长序列预测模型并不是专门为交通预测任务设计，无法有效地建模交通数据。
% 我们从两方面进行说明。
% 首先，交通数据的一大特点是时间序列之间存在强依赖，which是一个交通序列预测中一个非常重要的瓶颈。然而，长序列预测模型通常不关注这样的空间关联。这使得他们的有效性大打折扣。
% 其次，如Section1中讨论的，长期交通预测问题要求对局部和全局动态性的全局建模，which要求模型能够利用多尺度特征。
% 然而，长序列预测模型通常依赖自注意力机制，which不能不显式地产生多尺度特征。
Recently, long-sequence time series forecasting~\cite{2021Informer} has received much attention~\cite{2021Informer,2021AutoFormer,2022FEDFormer,2022Pyraformer,2023DLinear, 2019LogTrans, 2020ReFormer}.
% 2020ReFormer
They aim to make long-term future predictions by modeling long-term historical sequences.
For example, Informer~\cite{2021Informer} proposes a ProbSparse self-attention mechanism to replace the standard self-attention, which enhances the predictive ability of the standard Transformer in the long-sequence forecasting problem.
Autoformer~\cite{2021AutoFormer} designs an efficient Auto-Correlation mechanism to conduct dependencies discovery and information aggregation at the series level.
DLinear~\cite{2023DLinear} rethinks Transformer-based techniques and proposes a simple linear model based on decomposition to achieve better accuracy.

Although these models have made considerable progress in long-term time series forecasting, they are not designed for traffic data, which significantly affects their effectiveness in traffic forecasting problems.
We discuss it from two aspects.
First of all, there are strong correlations between multiple time series in traffic data, which is an important bottleneck in traffic forecasting~\cite{2018DCRNN}.
However, long-sequence time series forecasting models usually do not pay attention to such spatial correlations.
Second, as discussed in Section \ref{section:intro}, long-term traffic forecasting requires exploiting multi-scale representations of traffic data to capture the complex long-term traffic dynamics.
However, long-sequence forecasting models usually rely on the self-attention mechanism and its variants, which can not explicitly generate multi-scale features.



% XFormers
\label{sec:ltsf}