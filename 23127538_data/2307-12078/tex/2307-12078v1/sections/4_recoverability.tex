
\section{Conditions for Recoverability}
\label{sec:recoverability}
In this section, we derive conditions on the sensor network $\mathcal G$, its configuration $\bold p$, and the error vector $\bold x$, under which one can uniquely and exactly recover $\bold x$ by solving either the $l_2/l_0$ minimization problem P1, or the $l_2/l_q$ minimization problem P2 (where $0<q\leq 1$). The noise-free case is discussed first, which gives us qualitative conditions for recoverability, whereas Section \ref{sec:robustness} incorporates quantitative considerations such as measurement noise.

\subsection{$l_2/l_0$ Minimization}
From the literature on compressive sensing, we have the following lemma about the recoverability of $\bold x$ using the $l_2/l_0$ minimization problem P1.

\begin{lemma}[$l_2/l_0$ Recoverability \cite{afdideh2016recovery}]
Let $\bold x$ be a solution to the $l_2/l_0$ minimization problem P1. Then, $\bold x$ is the unique solution to P1 if
\begin{equation}
    \|\bold x\|_{2,0} < \hspace{1pt}\frac{1}{2}\hspace{3pt}{
            \min \left\lbrace \|\bold v\|_{2,0} \hspace{1pt}\big\vert\hspace{1pt}
                 \bold v \in \ker(\bold R), \bold v\neq \bold0
                 \right\rbrace
        }
\label{eq:block-spark}
\end{equation}
\label{lemma:l0_recoverability}
\end{lemma}

Note that Lemma \ref{lemma:l0_recoverability} establishes the uniqueness of the solution of P1 in spite of $\bold R$ having a non-trivial null space.
The quantity $\min \left\lbrace \|\bold v\|_{2,0} \hspace{1pt}\big\vert\hspace{1pt}
                 \bold v \in \ker(\bold R), \bold v\neq \bold0
                 \right\rbrace$ 
                 is called the \textit{block spark} of the matrix $\bold R$ \cite{afdideh2016recovery}.
Although it is difficult to compute in general, we can characterize the block spark of a well-studied class of rigidity matrices, namely those that have maximal rank, using the following lemmas. In what follows, given two subspaces $V_1, V_2\subseteq \mathbb R^n$, $V_1+V_2$ refers to the sum $\{\bold v_1+\bold v_2\big|\bold v_1\in V_1, \bold v_2\in V_2\}$ which is also a subspace. Given integers $n$ and $k$, $\binom{n}{k}$ denotes the corresponding binomial coefficient.

\begin{lemma}[Null Space of $\bold R_D$]
For $d=2$ or $3$, let $S_d$ be the space of all $d\times d$ real skew-symmetric matrices. We have 
\begin{equation}
\left\lbrace \bold 1_{|\mathcal V|}\otimes \bold q 
\hspace{1pt}\big|\hspace{1pt}
\bold q \in \mathbb R^d \right\rbrace
+ \left\lbrace(\bold I_{|\mathcal V|} \otimes \bold S)\bold p
\hspace{1pt}\big|\hspace{1pt}
\bold S \in S_d\right\rbrace 
\subseteq \ker\left(\bold R_D\right)
\label{eq:rD_null_space}
\end{equation}
As a consequence, $\rank(\bold R_D)\leq d|\mathcal V| - \binom{d+1}{2}$. \vspace{2pt}
\label{lemma:distance_rigidity_properties}
\end{lemma}
\begin{proof}
Consider the case of $d=3$.
The first summand on the left hand side of (\ref{eq:rD_null_space}) contains block vectors of the form
\begin{equation}
\bold v = \begin{bmatrix}
\bold q^\top & \bold q^\top & \dots & \bold q^\top    
\end{bmatrix}^\top
\end{equation}
for some vector $\bold q \in \mathbb R^3$.
It can be checked that, for block vectors of this form,  $\bold R_D \bold v = \bold 0$.
% \begin{equation}
% \bold R_D \bold v = (\bold p[i] - \bold p[j])^\top (\bold v[i] - \bold v[j]) = \bold 0
% \end{equation}
Focusing on the second summand of (\ref{eq:rD_null_space}), we can observe that $S_3$ is a vector space, as it is closed under the scalar multiplication and matrix addition operations.
Consider the following basis for $S_3$:
\begin{equation}
    % \mathcal S = \vspan
    \left\lbrace
    \begin{bmatrix*}[r] 0 & 1 & 0\\ -1 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix*},\begin{bmatrix*}[r] 0 & 0 & 1\\ 0 & 0 & 0 \\ -1 & 0 & 0 \end{bmatrix*},\begin{bmatrix*}[r] 0 & 0 & 0\\ 0 & 0 & -1 \\ 0 & 1 & 0 \end{bmatrix*}
    \right\rbrace
\end{equation}
Multiplying each basis element with the corresponding block of $\bold p$, we see that $\left\lbrace
(\bold I_{|\mathcal V|} \otimes \bold S)\bold p
\hspace{1pt}\big|\hspace{1pt}
\bold S \in  S_3\right\rbrace$ 
is composed of block vectors $\bold v$ that have the following property: 
for some $a,b,c \in \mathbb R$, 
\begin{equation}
    \bold v [i] = a
    \begin{bmatrix*}[r]
        \bold p[i]_y \\ -\bold p[i]_x \\  0\quad
    \end{bmatrix*} + b \begin{bmatrix*}[r]
        \bold p[i]_z \\  0\quad \\ -\bold p[i]_x 
    \end{bmatrix*} + c\begin{bmatrix*}[r]
         0\quad \\ -\bold p[i]_z \\ \bold p[i]_y
    \end{bmatrix*}\ \forall i\in\mathcal V
\end{equation}
% \begin{equation}
%     \bold v[i] \in \vspan \left\lbrace
%     \begin{bmatrix*}
%         \bold p[i]_y \\ -\bold p[i]_x \\ \bold 0\quad
%     \end{bmatrix*}, \begin{bmatrix*}[r]
%         \bold p[i]_z \\ \bold 0\quad \\ -\bold p[i]_x 
%     \end{bmatrix*}, \begin{bmatrix*}[r]
%         \bold 0\quad \\ -\bold p[i]_z \\ \bold p[i]_y
%     \end{bmatrix*}
% \right\rbrace
% \end{equation}
Once again, it can be verified that $\bold R_D \bold v = \bold 0$.
A similar reasoning applies to the $d=2$ case as well.
Finally, the condition on the rank follows from the dimension of the null space in either case, which is $\binom{d+1}{2}$.
\end{proof}
\begin{remark}
The intuition for Lemma \ref{lemma:distance_rigidity_properties} is that the isometries (i.e., rigid motions) of $\mathbb R^d$ preserve the distances between the sensor network agents. Of these isometries, there are $d$ translations and $\binom{d}{2}$ rotations, corresponding to the first and second summand of the left hand side of (\ref{eq:rD_null_space}), respectively, which add up to the $\binom{d+1}{2}$ dimensions of $\ker(\bold R_D)$. Furthermore, the skew-symmetric matrices $S_d$ make up the Lie algebra $\frak{s0}(d)$ corresponding to the infinitesimal rotations in $\mathbb R^d$, which explains their relevance to distance rigidity.
\label{rem:R_D_nullspace}
\end{remark}

The null space of $\bold R_D$ was characterized in \cite{trinh2016further} and \cite{decent_dist_rigidity_2015} for the cases of $d=2$ and $3$, respectively, whereas the characterization given in Lemma \ref{lemma:distance_rigidity_properties} is a generalization which  holds true in arbitrary dimensions, i.e., $d\geq 1$. An equivalent result about the bearing rigidity matrix, $\bold R_B$, is as follows.
\begin{lemma}[Null Space of $\bold R_B$\protect{\cite[Lemma 3]{Zhao_Zelazo_2016}}]
    For $d=2$ or $3$, we have
    \begin{equation}
    \left\lbrace \bold 1_{|\mathcal V|}\otimes \bold q 
\hspace{1pt}\big|\hspace{1pt}
\bold q \in \mathbb R^d \right\rbrace+\vspan\big(\{\bold p\}\big)\subseteq \ker\left(\bold R_B\right),
    \label{eq:rb_nullspace}
    \end{equation}
    and consequently, $\rank(\bold R_B)\leq d|\mathcal V| - d - 1$.
    \label{lemma:bearing_rigidity_properties}
\end{lemma}
\begin{proof}
    The proof is similar to that of Lemma \ref{lemma:distance_rigidity_properties}; it
    involves checking that the vectors in each summand of (\ref{eq:rb_nullspace}) vanish under multiplication with $\bold R_B$.
\end{proof}
\begin{remark}
    The second summand in (\ref{eq:rb_nullspace}) corresponds to scaling (expansion and shrinking) of $\mathbb R^d$, which preserves the orientations of the vectors $\bold p[i] - \bold p[j]$, $\forall i,j\in \mathcal V$, and therefore preserves the inter-agent bearing measurements.
    \label{rem:R_B_nullspace}
\end{remark}

Thus, given a sensor network with $|\mathcal V|$ agents, $\bold R$ has a maximal rank (across all possible values of $\mathcal E$ and $\bold p$) that is strictly less than the number of columns in $\bold R$. A sensor network configuration is said to be \textit{infinitesimally rigid} in dimension $d$ when the corresponding rigidity matrix has maximal rank, in which case, the symbol `$\subseteq$' in (\ref{eq:rD_null_space}) and (\ref{eq:rb_nullspace}) become equalities \cite{decent_dist_rigidity_2015,zhao2019bearing}.
% \footnote{In the references, infinitesimal rigidity is presented as a lemma or a theorem instead. We present it as a definition here so as to omit any extraneous details from the paper.}
% \begin{remark}
% Infinitesimal rigidity is related to the observability of the sensor network configuration \cite{Zhao_Zelazo_2016}, and 
There is extensive literature discussing how infinitesimal rigidity can be maintained in a distributed manner \cite{Olfati-Saber_Murray_2002,decent_dist_rigidity_2015,bearing_rigidity_maintenance_2017}. It can also be guaranteed with high probability in random sensor network configurations (modeled as random geometric graphs) based on the spatial density and sensing radius of the agents \cite{Eren_rigidity_randomness_2004}. 
% \end{remark}

We now state the condition for $l_2/l_0$ recoverability (i.e., the ability to recover the localization error vector $\bold x$ uniquely by solving the $l_2/l_0$ minimization problem P1) for infinitesimally rigid sensor network configurations.
\begin{theorem}
Suppose $\bold R$ has maximal rank. Then, a solution $\bold x$ of the $l_2/l_0$ minimization problem P1 is the unique solution to P1 if 
\begin{equation}
\|\bold x\|_{2,0}<\frac{1}{2}\left(|\mathcal V|-\tilde s\right), 
\end{equation}
where $\tilde s$ is determined as follows:
\begin{enumerate}
    \item if $\bold R=\bold R_D$ and $d=2$, then $\tilde s=1$
    \item if $\bold R=\bold R_D$ and $d=3$, then $\tilde s$ is equal to the maximum number of colinear points in $\{\bold p[i]\}_{i\in \mathcal V}$
    \item if $\bold R=\bold R_B$ and $d=2$ or $3$, then $\tilde s=1$
\end{enumerate}
\label{theorem:l0_recoverability}
\end{theorem}
\begin{proof}
% This is a straightforward application of Lemma \ref{lemma:l0_recoverability}, so we provide a sketch of the proof. 
We provide a sketch of the proof.
% Recall that $|\mathcal D|$ is equal to $\|\bold x\|_{2,0}$, the block sparsity of $\bold x$. 
First, we need to show that
\begin{equation}
     \hspace{3pt}{
            \min \left\lbrace \|\bold v\|_{2,0} \hspace{1pt}\big\vert\hspace{1pt}
                 \bold v \in \ker(\bold R), \bold v\neq \bold0
                 \right\rbrace
        } = |\mathcal V| - \tilde s \hspace{3pt}
\end{equation}
which can be verified on a case-by-case basis using Lemmas \ref{lemma:distance_rigidity_properties} and \ref{lemma:bearing_rigidity_properties}. Thereafter, Lemma \ref{lemma:l0_recoverability} establishes that $\bold x$ is the unique solution to P1 in each case.

While the proof can be completed algebraically,
the following is an intuitive explanation:
$\tilde s$ is equal to the number of fixed points of the corresponding transformations of $\mathbb R^d$ (see Remarks \ref{rem:R_D_nullspace} and \ref{rem:R_B_nullspace}), which is $1$ for $2$D rotations, the number of points lying on the axis of rotation for $3$D rotations, and $1$ for scaling; these correspond to cases $1$, $2$ and $3$ of the theorem, respectively.
\end{proof}
% \begin{remark}
% In Theorem \ref{lemma:l0_recoverability}, . Furthermore, we observe that  For, e.g., the fixed points of a rotation of $\mathbb R^3$ is the line (i.e., the axis of rotation), which leads to the second result above.
% \end{remark}

% The configuration $\bold p$ is said to be \textit{generic} if there are no algebraic coincidences between the agents' positions, i.e., $\{\bold p[i]\}_{i\in\mathcal V}$ do not satisfy a polynomial equation with rational coefficients \cite{LovÃ¡sz_genericrigidity_1982}. For generic configurations, infinitesimal rigidity in $2$D is purely a property of the sensor network connectivity $\mathcal E$, and does not depend on the specific configuration itself.

Recall that $|\mathcal D|$ is equal to $\|\bold x\|_{2,0}$, the block sparsity of $\bold x$.  Hence, in each of the cases considered in Theorem \ref{theorem:l0_recoverability},
roughly up to half of the agents in $\mathcal V$ can have localization errors before the $l_2/l_0$ minimization approach fails to recover the error vector $\bold x$ uniquely.
Note that the existing literature on rigidity theory assumes that the identities of the anchors (and consequently, the support of $\bold x$) is known \cite{zhao2019bearing}, whereas Theorem \ref{theorem:l0_recoverability} pertains to the more general case where the support of $\bold x$ may not be known.
% As expected, dropping this assumption reduces the ability to recover $\bold x$.
% Since we dropped the assumption that the sparsity pattern of $\bold x$, 
% This corresponds to the intuition that if more than half of the agents have deviated systematically, then the smaller set of agents are classified into $\mathcal D$ instead.

\subsection{$l_2/l_q$ Recoverability}
We now turn our attention to the $l_2/l_q$ minimization P2, for $0<q\leq 1$, where $q=1$ is the most attractive approach due to its computational ease.
% \red{distance and bearing rigidity maintenance, resp. \cite{decent_dist_rigidity_2015, bearing_rigidity_maintenance_2017}\\
% Once identified, olfati's work allows us to 'patch' holds in graphs \cite{Olfati-Saber_Murray_2002}
% }
A necessary and sufficient condition for $l_2/l_q$ minimization to uniquely recover a block sparse solution is given via the $l_q$ block null space property (NSP) \cite{latushkin_null_2015}, which is defined as follows.
% \begin{definition}[$l_q$ Block Null Space Property (NSP) \protect{}]
A matrix $\bold A\in\mathbb R^{m\times dn}$ is said to satisfy the $l_q$ block NSP of order $s$ if, for all subsets $\mathcal S\subseteq \{1, 2, \dots, n\}$ with $|\mathcal S|\leq s$, we have
% there exists a constant $0<\tau<1$ such that
\begin{equation}
\|\bold v_{\mathcal S}\|_{2,q} < \|\bold v_{\mathcal S^\complement}\|_{2,q},\quad \forall\hspace{1pt}\bold v\in \ker(\bold A)\backslash \mathcal \{\bold 0\}
\label{eq:block_nsp_condition}
\end{equation}
\begin{lemma}
 [$l_2/l_q$ Recoverability]
Let $\bold x$ be a solution to the $l_2/l_q$ minimization problem P2, where $0<q\leq 1$. Then, $\bold x$ is the unique solution to P2
if and only if $\bold R$ satisfies the $l_q$ block NSP of order $\|\bold x\|_{2,0}$.
\label{lemma:lq_recoverability}
\end{lemma}
\begin{proof}
The case for $q=1$ was shown in \cite[Thm. 2]{Stojnic_Parvaresh_Hassibi_2009} and the case for $0<q<1$ was shown in \cite[Thm. 26]{latushkin_null_2015}.
\end{proof}
Using Lemma \ref{lemma:lq_recoverability}, we have the following result for the localization error identification problem in the case of $\bold R = \bold R_D$. 
% Here, `{$\bold R_D$ has maximal rank}' may be interpreted as `{the sensor network is infinitesimally rigid}'.
\begin{theorem}
Suppose $\bold R_D$ has maximal rank and $d=2$. Then,
a solution $\bold x$ of the $l_2/l_q$ minimization problem P2, with $0<q\leq 1$ and $\bold R=\bold R_D$, is the unique solution to P2 if and only if
\begin{equation}
   \sum_{i\in\mathcal S}\| \bold p[i] - \bold c\|^q_{2} < \sum_{i\in\mathcal S^\complement}\| \bold p[i] - \bold c\|^q_{2}
    \label{eq:point_l_q_condition}
\end{equation}
for all $\bold c\in \mathbb R^2$ and for all subsets $\mathcal S\subseteq \{1, 2, \dots, |\mathcal V|\}$ with $|\mathcal S|\leq \|\bold x\|_{2,0}$.
\label{theorem:lq_recoverability}
\end{theorem}
\begin{proof} Using Lemma \ref{lemma:distance_rigidity_properties}, we see that any vector $\bold v\in\ker(\bold R_D)$ is a block vector having the form $\bold v[i] = \tilde{\bold c} + k\hspace{1pt}\bold S\hspace{1pt}\bold p [i]$,
where 
\[\bold S=\begin{bmatrix*}[r]
    0 & 1 \\ -1 & 0
\end{bmatrix*},
\]
$\tilde {\bold c} \in \mathbb R^2$, and $k\in\mathbb R$. Note that $\tilde {\bold c}$ and $k$ have the same value in each block of $\bold v$.
% (the degrees of freedom in choosing $\tilde {\bold c}$ and $k$ agrees with the dimension of $\ker(\bold R_D)$, which is $3$) 
Using these facts in (\ref{eq:block_nsp_condition}) gives us the condition
\begin{equation}
    \left(\sum_{i\in \mathcal S}\|\tilde {\bold c} + k\hspace{1pt}\bold S\hspace{1pt}\bold p[i] \|^q_2\right)^{1/q} < \left(\sum_{i\in \mathcal S^\complement}\|\tilde {\bold c}+k\hspace{1pt}\bold S\hspace{1pt}\bold p[i]\|^q_2\right)^{1/q}
    \label{eq:thm_lq_before_c}
\end{equation}
Noting that $\bold S$ is an orthogonal matrix and dividing both sides by $k$, we have
\begin{equation}
   \left(\sum_{i\in \mathcal S}\|\tfrac{1}{k}\bold S^\top\tilde {\bold c} + \bold p[i] \|^q_2\right)^{1/q} < \left(\sum_{i\in \mathcal S^\complement}\|\tfrac{1}{k}\bold S^\top\tilde {\bold c} + \bold p[i]\|^q_2\right)^{1/q}
   \label{eq:thm_lq_after_c}
\end{equation}
Finally, we set $\bold c = - \tfrac{1}{k}\bold S^\top\tilde {\bold c}$ and use the operation $({}\cdot{})^q$ on both sides
to obtain (\ref{eq:point_l_q_condition}).
\end{proof}

To see the relevance of Theorem \ref{theorem:lq_recoverability}, consider $q=1$. Equation (\ref{eq:point_l_q_condition}) can be rewritten as
\begin{equation}
   \sum_{i\in\mathcal S}\| \bold p[i] - \bold c\|_{2} < \frac{1}{2}\sum_{i\in\mathcal V}\| \bold p[i] - \bold c\|_{2}
    \label{eq:point_l_1_condition}
\end{equation}
where for an arbitrary point $\bold c \in \mathbb R^2$, the left hand side is maximized by the set $\mathcal S$ corresponding to the positions of the $|\mathcal S|$ agents that are furthest from $\bold c$.
Thus, inequality (\ref{eq:point_l_1_condition}) is saying that the sum of distances of the $|\mathcal S|$ furthest agents from $\bold c$ accounts for less than half of the total sum of distances from $\bold c$, 
i.e., the agents in $\mathcal S^\complement$ are not clustered together near $\bold c$. 
% This condition is stronger than $l_2/l_0$ recoverability, since some of the agents' positions may be nearly coincident, giving us $s=0$ in the worst case.

As inequality (\ref{eq:point_l_1_condition}) must hold for all $\mathcal S\subseteq \mathcal V$ such that $|\mathcal S|\leq \|\bold x\|_{2,0}$, there is a maximal value for $\|\bold x\|_{2,0}$ such that inequality (\ref{eq:point_l_1_condition}) is satisfied by a given sensor network configuration, which is the maximum number of localization errors that can be uniquely recovered through $l_2/l_1$ minimization. We can generally expect this number to increase with the size of the network, since as the size of the sensor network increases, so does the right hand side of (\ref{eq:point_l_1_condition}). This indicates that up to a fraction of the agents in $\mathcal V$ can have localization errors before $l_2/l_q$ recovery fails to recover $\bold x$. A similar result can be shown for the case of $d=3$.
\begin{theorem}
The main claim of Theorem \ref{theorem:lq_recoverability} holds for $3$-dimensional sensor networks (i.e., $d=3$) if and only if inequality (\ref{eq:point_l_q_condition}) is satisfied by each of the $2$D configurations obtained by projecting $\{\bold p[i]\}_{i\in \mathcal V}$ onto an arbitrary plane in $\mathbb R^3$.
\label{theorem:3d_lq_recovery}
\end{theorem}
\begin{proof}
For $d=3$, a null vector $\bold v \in \ker(\bold R_D)$ has the form $\bold v[i] = \tilde{\bold c} + k\hspace{1pt} {\bold S}\hspace{1pt}\bold p [i]$, where $k\in \mathbb R$ and $ {\bold S}$ is a $3\times 3$ skew-symmetric matrix. We use the result from 
% \cite[Prop. 5]{skewSymmProp5} 
\cite[Sec. 7.3]{hogben2013handbook}
that any $3\times 3$ skew symmetric matrix $\bold S$ can be expressed as \mbox{$\bold S = l\hspace{1pt}\bold Q \bold \Lambda\bold Q^\top$},
% \footnote{A simple proof of this fact is that $\bold Q$ is a $3$D rotation which has $2$ degrees of freedom. Along with the freedom in choosing $l$, $l\hspace{1pt}\bold Q \bold \Lambda\bold Q^\top$ has $3$ degrees of freedom, same as the $3$ dimensions of $\bold S$.}
where
\begin{equation}
  \bold \Lambda =  \begin{bmatrix*}[r]
0 & 1 & 0\\
-1 & 0 & 0\\
0 & 0 & 0
    \end{bmatrix*},
\end{equation}
$l\in \mathbb R$, and $\bold Q$ is an orthogonal matrix. Thus, $\bold v$ has the form $\bold v[i] = \tilde{\bold c} + kl\hspace{1pt} {\bold Q}{\bold \Lambda}{\bold Q}^\top\bold p [i]$.
Let $\tilde {\bold p}[i]=\bold Q^\top\bold p[i]$, such that $\tilde {\bold p}[i]$  denotes the agents' positions in a rotated coordinate frame. Following again the steps between (\ref{eq:thm_lq_before_c}) and (\ref{eq:thm_lq_after_c}), we have the condition
\begin{equation}
       \sum_{i\in \mathcal S}\|  \bold \Lambda \tilde{ \bold p}[i] - \bold c \|^q_2 < \sum_{i\in \mathcal S^\complement}\| \bold \Lambda \tilde{ \bold p}[i] - \bold c\|^q_2
\end{equation}
where $\bold c = - \frac{1}{kl}\bold Q^\top \tilde {\bold c}$. Observe that $\bold \Lambda$ projects (then rotates) the points onto a plane, as $
\bold \Lambda \tilde {\bold p}[i] = \left[\tilde{\bold p}[i]_y\ -\tilde{\bold p}[i]_x\  0\right]^\top$. To complete the argument, observe that the plane being projected onto is determined by $\bold Q$, which can be chosen arbitrarily.
\end{proof}

In the case of bearing-based localization error recovery, the conditions for $d=2$ and $3$ are similar to that of Theorem \ref{theorem:lq_recoverability}. This is related to the fact that the dimension of the bearing measurements (which are represented by unit vectors) equals the dimension of the space, $d$, as opposed to distance measurements which are scalar-valued irrespective of $d$.
\begin{proposition}
Suppose $\bold R_B$ has maximal rank and $d=2$. Then,
a solution $\bold x$ of the $l_2/l_q$ minimization problem P2, with $0<q\leq 1$ and $\bold R=\bold R_B$, is the unique solution to P2 if and only if
\begin{equation}
   \sum_{i\in\mathcal S}\| \bold p[i] - \bold c\|^q_{2} < \sum_{i\in\mathcal S^\complement}\| \bold p[i] - \bold c\|^q_{2}
    \label{eq:point_l_q_condition_Bearing}
\end{equation}
for all $\bold c\in \mathbb R^d$ and for all subsets $\mathcal S\subseteq \{1, 2, \dots, |\mathcal V|\}$ with $|\mathcal S|\leq \|\bold x\|_{2,0}$.
\label{thm:l_q_recoverability_Bearing}
\end{proposition}
\begin{proof}
Observe that a vector $\bold v \in \ker (\bold R_B)$ has the form $\bold v[i] = \tilde {\bold c} + k \hspace{1pt}\bold p [i]$, where $\tilde {\bold c} \in \mathbb R^d$ and $k \in \mathbb R$.
Thereafter, the proof follows similarly to those of Theorems \ref{theorem:lq_recoverability} and \ref{theorem:3d_lq_recovery}.
\end{proof}

Thus, following a reasoning similar to the one we used for distance-based localization error recovery, we observe that $l_2/l_1$ minimization can be used to recover the localization errors by processing the inter-agent bearing measurements as well, as long as the number of localization errors is not too large (i.e., beyond the maximal value of $|\mathcal S|$ for which inequality (\ref{eq:point_l_q_condition_Bearing}) holds).
% \begin{remark}
% In the literature, infinitesimal rigidity is usually stated as a theorem and not a definition, but we have omitted any details which are not relevant to this paper.
% \end{remark}