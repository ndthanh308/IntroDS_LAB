\section{Block-Sparse Optimization Problem}%
\label{sec:optimization}
\noindent
Consider the residual vector $\bold z$, defined as 
\begin{equation}
    \bold z \coloneqq \bold y - \bold \Phi(\bold {\hat p}) 
\end{equation}
Using (\ref{eq:edge_measurement_block}) and (\ref{eq:taylor_series}), we have
\begin{align}
\bold z = \bold R (\bold p - \bold {\hat p}) +\bold e &= \bold R \bold x + \bold e
\end{align}
where the notation $\bold R$ is introduced for brevity;
it denotes either the distance or bearing rigidity matrix ($\bold R_D(\bold p)$ or $\bold R_B(\bold p)$, respectively) depending on the context. 

% \subsection{$l_2/l_0$ Minimization}
A naive approach for recovering the localization errors in the noise-free case (i.e., the case where $\bold e=\bold 0$) is to solve the system of linear equations
$\bold z = \bold R\hat{\bold x}$ for $\hat{\bold x}$. However, there are two issues with this approach:
\begin{itemize}
    \item \textit{Non-Uniqueness of the Solution}: Both the distance and bearing rigidity matrices \big($\bold R_D$ and $\bold R_B$, respectively\big) have non-trivial null spaces (as shown in Section \ref{sec:recoverability}), irrespective of the configuration and connectivity of the sensor network. Thus, the system of linear equations $\bold z = \bold R\hat{\bold x}$ does not have a unique solution; it is said to be underdetermined.
    \item \textit{Non-Sparsity of the Solution}: For $0<s<|\mathcal V|$, the set of all block $s$-sparse vectors in $\mathbb R^{d|\mathcal V|}$ is a union of lower-dimensional subspaces, each having the dimension $ds$ \cite{Eldar_Mishali_2009}. Consequently, the set of block $s$-sparse vectors has a measure of $0$ in the ambient space $\mathbb R^{d|\mathcal V|}$, meaning that general iterative algorithms used to solve the foregoing system of equations will yield non-sparse solutions with overwhelming probability. This is equivalent to the classification of all the agents in $\mathcal V$ as having localization errors (i.e., $\mathcal D = \mathcal V$), which may be undesirable.
\end{itemize}
In sensor network applications where it is more likely that $|\mathcal D|<|\mathcal V|$, under certain conditions on the block-sparsity of $\bold x$, it is possible to uniquely recover it by solving the following optimization problem: 
\begin{align*}
\begin{array}{rclc}
   \textsc{P1:}  &  &
\begin{array}{rl}
\underset{\hat{\bold x}}{\textnormal{minimize}}\quad  
&\| \hat{\bold x}\|_{2,0}  \\
\textnormal{subject to} \quad
& \bold z = \bold R\hat{\bold x}
\end{array} & \qquad \qquad
\end{array}
\end{align*}
which we call the (constrained) $l_2/l_0$ minimization problem. The function $\|\cdot\|_{2,0}$ is not convex, however, and $l_2/l_0$ minimization is known to be a computationally hard problem \cite{robust_NSP_2017}.
Therefore, it is desirable to study the recoverability of $\bold x$ using the following relaxed $l_2/l_q$ minimization problem, where $0<q\leq1$:
\begin{align*}
\begin{array}{rclc}
   \textsc{P2:}  &  &
\begin{array}{rl}
\underset{\hat{\bold x}}{\textnormal{minimize}}\quad  
&\| \hat{\bold x}\|_{2,q}  \\
\textnormal{subject to} \quad
& \bold z = \bold R\hat{\bold x}
\end{array} & \qquad \qquad
\end{array}
\end{align*}
In particular, $l_2/l_1$ minimization is a convex optimization problem and can be solved using second-order cone programming (SOCP) solvers \cite{wang_wang_xu_2013}, block variants of basis pursuit and matching pursuit solvers \cite{efficient_block_sparse_2010, giaralis_bomp_2021, Block_COSAMP_2019}, as well as distributed algorithms \cite{mota2011basis}. The nonlinearity and noise in the measurement model can also be accommodated by using basis pursuit denoising \cite{Eldar_Mishali_2009}. This makes the $l_2/l_q$ minimization problem (\textsc{P2}) an attractive alternative to the $l_2/l_0$ version (\textsc{P1}). 
Moreover, it has been noted that $l_2/l_q$ optimization outperforms $l_q$ optimization when the solution is known to be block sparse \cite{Eldar_Mishali_2009,efficient_block_sparse_2010}. 
% Additionally, $l_2/l_q$ minimization with $0<q<1$ further outperforms $l_2/l_1$ minimization in terms of promoting block-sparsity of the solution \cite{wang_wang_xu_2013}.
% In part, this is because if a generic vector $\bold x$ is block $s$-sparse, then it is $ds$-sparse, and
% the space of $ds$-sparse vectors 
% % is is a union of $\binom{d|\mathcal V|}{ds}$ $ds$ dimensional subspaces, whereas the space of block $s$-sparse vectors is a union of $\binom{|\mathcal V|}{s}$ $ds$-dimensional subspaces.
% is much larger than the space of block $s$-sparse vectors.