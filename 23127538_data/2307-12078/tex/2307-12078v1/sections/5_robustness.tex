\section{Robustness Guarantees}%
\label{sec:robustness}
In this section, we discuss the robustness of the $l_2/l_q$ minimization method subject to practical considerations such as measurement noise and linearization error. 
\subsection{Measurement Noise}
Suppose $\bold z = \bold R\bold x+\bold e$, where the measurement noise vector $\bold e$ is bounded as $\|\bold e\|_2\leq \epsilon$ with $\epsilon > 0$, then we are interested in solving the following robust $l_2/l_q$ minimization problem:
\begin{align*}
\begin{array}{rclc}
   \textsc{P3:}  &  &
\begin{array}{rl}
\underset{\hat{\bold x}}{\textnormal{minimize}}\quad  
&\| \hat{\bold x}\|_{2,q}  \\
\textnormal{subject to} \quad
& \|\bold z - \bold R \hat{\bold x}\|_2\leq \epsilon
\end{array} & \qquad \qquad
\end{array}
\end{align*}
where $\epsilon$ can be interpreted as the slackness of the constraint, which is introduced to accommodate the measurement noise.
In the case of $q=1$, the problem P3 is related to the basis pursuit denoising problem \cite{gill2011crowd_bpdn}.
Denote the solution of P3 as $\bold x^*$ which (due to the measurement noise) may be different from the vector $ {\bold x}$ that generated the observed measurements.
Nevertheless, we have the following relationship between $\bold x$ and $\bold x^*$.
\begin{lemma}[Robust $l_2/l_q$ Recoverability \protect{\cite[Cor. 3]{robust_NSP_2017}}]
Let $s>0$.
Suppose for all sets $\mathcal S\subseteq \{1, 2, \dots, n\}$ with $|\mathcal S|\leq s$, there exist constants $0<\tau < 1$ and $\gamma >0$ such that
\begin{equation}
\|\bold v_{\mathcal S}\|_{2,q} \leq \tau\|\bold v_{\mathcal S^\complement}\|_{2,q} + \gamma \| \bold R \bold v\|_2
\label{eq:robust_lq_cond}
\end{equation}
for all $\bold v\in \mathbb R^{d|\mathcal V|}$. Then, a solution $\bold x^*$ of Problem P3 approximates $ {\bold x}$ with the error
\begin{equation}
    \| \bold x - \bold x^*\|_{2,q} \leq C_{q, \tau}\sigma _{q,s}( \bold x) + C_{q,\tau,\gamma}\epsilon,
    \label{eq:robust_lq_ineq}
\end{equation}
\begin{align}
&\textit{where} \nonumber \\
& C_{q, \tau}=2^{\sfrac{2}{q}-1}\left(\frac{1+\tau^q}{1-\tau^q}\right)^{\sfrac{1}{q}};\   C_{q,\tau,\gamma}=2^{\sfrac{2}{q}}\left(\frac{1}{1-\tau^q}\right)^{\sfrac{1}{q}}\gamma\ ;\nonumber\\
&
% \textit{and}\qquad \quad       \sigma_s(\bold x)_{2,q} = \inf_{\left\lbrace \hspace{1pt}\bold z\hspace{1pt}\vert\hspace{1pt}{}\|\bold z\|_{2,0}\leq s \right\rbrace } \|\bold x - \bold z\|_{2,q}
\textit{and}\quad       \sigma_{q,s}(\bold x) = \inf \left\lbrace  \|\bold x - \bold z\|_{2,q} 
\hspace{1pt}\big\vert\hspace{1pt}
\bold z \in \mathbb R^{d|\mathcal V|}, \|\bold z\|_{2,0}\leq s  \right\rbrace.
\nonumber
\end{align}
\label{lem:robust_lq}
\end{lemma}
% If $s \geq \|\bold x\|_{2,0}$, then $\sigma_s(\bold x)_{2,q}=0$; the first term on the right hand side of (\ref{eq:robust_lq_ineq}) vanishes.
% % In addition, $\epsilon=0$ and (\ref{eq:robust_lq_ineq}) establishes that $\bold x = \bold x^*$. 
% In the presence of measurement noise, we have $\epsilon > 0$, which contributes to the second term on the right hand side of (\ref{eq:robust_lq_ineq}).
Using Lemma \ref{lem:robust_lq}, we can show that $l_2/l_1$ recoverability in the noise-free case (which was discussed in the previous section) also guarantees a certain degree of robustness in the presence of noise.
\begin{theorem}
Suppose that $\bold R$ satisfies the $l_1$ block NSP of order $s$. 
Then, there exist constants $0<\tau<1$ and $0<\gamma<\infty$ such that any solution $\bold x^*$ of the robust $l_2/l_q$ minimization problem P3, with $q=1$, is related to $\bold x$ via inequality (\ref{eq:robust_lq_ineq}).
\label{thm:l1_robust}
\end{theorem}
\begin{proof}
The proof is given in the appendix.
\end{proof}
\noindent
A crucial observation from this proof is that the constant $\gamma$ in (\ref{eq:robust_lq_cond}) (which represents the sensitivity of the approximation error to measurement noise) is inversely proportional to the smallest non-zero eigenvalue of $\bold R^T \bold R$, which is denoted as $\tilde \lambda(\bold R^T\bold R)$. In the literature, $\tilde \lambda(\bold R^T\bold R)$ is called as the \textit{worst-case rigidity index} and is considered as a quantitative measure of the rigidity of the configuration \cite{zhu2009stiffness, trinh2016further, jordan2022rigidity}. By using distributed control algorithms to maximize the worst-case rigidity index of a sensor network configuration, the robustness of the  $l_2/l_1$ minimization approach to measurement noise can be enhanced \cite{decent_dist_rigidity_2015, bearing_rigidity_maintenance_2017}.

 \subsection{Imperfect Position Estimates}
In this subsection, we address the case where the agents in $\mathcal D^\complement$ have obtained imperfect estimates of their own positions due to the uncertainty in their dynamical and measurement models. In this case,
we have $\mathbf x [i]\neq{\bold 0}\ \forall i \in \mathcal V$, i.e., $\bold x$ is not a block-sparse vector. As each agent in $\mathcal V$ has a localization error, we consider the problem of identifying the \textit{significant localization errors} instead which are defined as follows. Given $\kappa > 0$, agent $i\in \mathcal V$ is said to have a significant localization error if
$\|\bold x[i]\|_2 > \kappa$. Let ${\mathcal D} \subseteq \mathcal V$ be the set of agents which have significant localization errors.

While the noise in the inter-agent measurements contributes to the second term on the right hand side of inequality (\ref{eq:robust_lq_ineq}), the error in the position estimates of the agents in $ {\mathcal D}^\complement$, $\bold x_{ {\mathcal D}^\complement}$, contributes to the term $\sigma_{q,s}(\bold x)$. Suppose the sensor network configuration satisfies the $l_1$ block NSP of order $s$, such that $s \geq | {\mathcal D}|$, then we have the following upper bound on $\sigma_{1,s}(\bold x)$:

\begin{equation}
\sigma_{1,s}(\bold x) \leq 
% 2\left(\frac{1+\tau}{1-\tau}\right)
% \Big(|\mathcal V|-s\Big) \|\bold x_{\mathcal D^\complement}\|_{2,\infty} =
\big(|\mathcal V|-s\big) \kappa
\end{equation}
which follows from the observation that the infimum in the expression for $\sigma_{q,s}(\bold x)$ is attained only if $\bold z[i]=\bold x[i] \Leftrightarrow i\in  {\mathcal D}$. Thus, the approximation error increases linearly with respect to the number of agents in the sensor network.

% where $\|\bold x_{\mathcal D^\complement}\|_{2,\infty}$ is the maximum estimation noise among the nominal agents, i.e., those in $\mathcal D^\complement$.

Interestingly, the presence of randomness in the entries of $\bold p$ and $\bold R$ is beneficial for the theoretical analysis of rigidity;
a random set of positions, $\{\bold p[i]\}_{i\in\mathcal V}$, is said to be \textit{generic}, which means that there is no algebraic dependence between the positions of the agents. Generic configurations play an important role in rigidity theory as it is easier to ensure rigidity in generic configurations (see \cite{Eren_rigidity_randomness_2004,bearing_rigidity_maintenance_2017}).
Moreover, with probability one, no more than $2$ points in $\{\bold p[i]\}_{i\in\mathcal V}$ can be colinear in a random configuration. Thus, a stronger guarantee of $l_2/l_0$ recoverability can be established for random configurations using Theorem \ref{theorem:l0_recoverability}. This may be compared to the role of random matrices in compressive sensing theory, which are used to guarantee recoverability properties with probability one \cite{Eldar_Mishali_2009}.

\subsection{Linearization Error}

In practice, the linearization error introduced by using $\bold R(\hat {\bold p})$ in place of the nonlinear measurement model can undermine the performance of our approach. This problem can be addressed by using an iterative approach to solve the robust $l_2/l_q$ minimization problem P3, using the idea of \textit{bootstrapping}.
% Moreover, the Jacobian matrix $\bold R(\bold p)$ is not known.
Suppose $\bold x_k$ is the estimate of $\bold x$ at a given iteration of the convex optimization routine, we can use $\bold R(\hat {\bold p} + \bold x_k)$ in place of $\bold R(\bold p)$ as a bootstrapped estimate of the rigidity matrix, based on the observation that $\hat {\bold p} + \bold x_k$ is closer to $\bold p$ than $\hat {\bold p}$ is.
% , since $\hat {\bold p}+\bold x_k$ is likely to be closer to the true value of  $\bold p$ than $\hat {\bold p}$.

With this motivation, we propose an algorithm based on sequential convex programming (SCP) (given in Algorithm \ref{alg:scp}) to iteratively reconstruct $\bold x$, following the methodology of \cite{zillober2004very}. In each iteration, the SCP algorithm updates its estimate of the rigidity matrix, and then
solves the corresponding robust $l_2/l_1$ minimization problem using a convex optimization solver. The stopping condition for the SCP algorithm, given in step $5$ of Algorithm \ref{alg:scp}, is based on the following proposition. It shows that in a sensor network where $\bold \Phi(\hat{\bold p}) \neq \bold \Phi({\bold p})$ (i.e., the inter-agent measurement residuals are non-zero) due to localization errors, Algorithm \ref{alg:scp} recovers a sparse vector $\bold x^*$ which serves as an \textit{explanation} for the observed measurement residuals.
% (which can be compared to \cite[Prop. 4.2.1]{bertsekas}).

\begin{proposition}
Consider the noise-free case (i.e., $\bold e=\bold 0$) of the localization error recovery problem. Suppose in Algorithm $\ref{alg:scp}$, we have $\|\tilde {\bold x}_k\|_{2} \rightarrow 0$ as $\epsilon_{k} \rightarrow 0$, then
\begin{equation}
    \lim_{\epsilon_k \rightarrow 0} \big(\bold \Phi (\hat {\bold p} + \bold x_k)\big) = \bold \Phi (\bold p)
    \label{eq:local_convergence}
\end{equation}
% where the convergence is in the norm $\|{}\cdot{}\|_2$.
\end{proposition}
\begin{proof}
Since $\|\tilde {\bold x}_k\|_{2} \rightarrow 0$, there exists some $\bold x^*\in \mathbb R^{d|\mathcal V|}$ such that $\lim_{\epsilon_k \rightarrow 0}({\bold x}_k) = \bold x^*$, giving us
\begin{equation}
 % \quad\text{and}\quad 
\lim_{\epsilon_k \rightarrow 0} \left(\bold z_{k}\right) = \bold y- \bold \Phi(\hat{\bold p}+\bold x^*)
\label{eq:z_limit}
\end{equation}
Moreover, as $\tilde {\bold x}_k$ is a solution to the optimization problem in step $4$ of Algorithm \ref{alg:scp}, we have
\begin{equation}
    \lim_{\epsilon_k\rightarrow 0}\left( \bold z_{k} -  \bold R_{k} \tilde {\bold x}_k\right) = \bold 0 
    \label{eq:final_prop_limiteq}
    \end{equation}
Using the definition of $\bold z_k$ and substituting $\bold y$ with $\bold \Phi(\bold p)$, we can rewrite (\ref{eq:final_prop_limiteq}) as
    \begin{equation}
    \lim_{\epsilon_k \rightarrow 0} \Big( \bold \Phi (\bold p) - \bold \Phi (\hat {\bold p} + \bold x_k) - \nabla \bold \Phi (\hat {\bold p} + \bold x_k)\tilde {\bold x}_k \Big)= \bold 0
    \label{eq:prop_local_convergence_lasteqn}
\end{equation}
Since each of the summands has a limit (due to the continuity of $\bold \Phi$ and $\nabla\bold \Phi$), we can use the fact that $\tilde {\bold x}_k \rightarrow \bold 0$ in (\ref{eq:prop_local_convergence_lasteqn}) to obtain (\ref{eq:local_convergence}).
\end{proof}

Thus, Algorithm \ref{alg:scp} is able to uniquely recover both the set of incorrectly localized agents as well as their corrected positions. We use Algorithm \ref{alg:scp} to numerically demonstrate the proposed approach in the next section, showing that it recovers $\bold x$ accurately in as few as $4$ SCP iterations.

\begin{algorithm}
\caption{Sparse Localization Error Recovery using SCP}
\begin{algorithmic}[1]
\vspace{2pt}
\REQUIRE 
% $\mathcal G$, 
 $\hat{\bold p}$, $\bold y$, $N_{\text{Iterations}}\geq 1$, initial constraint slackness $\epsilon_0\geq 0$, slackness reduction factor $0<\rho\leq1$, and tolerance $\delta \geq 0$.\\
\vspace{3pt} 
\STATE $\bold x_0 \leftarrow \bold 0$
\FOR{$k=1,\dots,N_{\text{Iterations}}$}
\STATE Construct the residual vector and rigidity matrix: 
\[\bold z_{k-1} = \bold y - \bold \Phi (\hat {\bold p} + \bold x_{k-1})\] 
\[\bold R_{k-1} = \nabla \bold \Phi(\hat {\bold p} + \bold x_{k-1})\]
\STATE Update the estimated error vector using \[\bold x_k \leftarrow \bold x_{k-1} + \tilde {\bold x}_{k-1},\]
where $\tilde {\bold x}_{k-1}$ is the solution to the following convex optimization problem:
\begin{align*}
\begin{array}{rl}
\underset{\bold x}{\textnormal{minimise}}\quad  
&\|\bold x_{k-1} + \bold x\|_{2,1}  \\
\textnormal{subject to} \quad
& \|\bold z_{k-1} - \bold R_{k-1}\bold x\|_2 \leq \epsilon_{k-1}
\end{array}
\end{align*}
\IF {$\|\tilde {\bold x}_{k-1}\|_2<\delta$}
\STATE \textbf{break}
\ELSE 
\STATE Tighten the constraint: $\epsilon_k \leftarrow \rho\hspace{1pt}\epsilon_{k-1}$
\ENDIF
\ENDFOR
\STATE $\bold x^* \leftarrow \bold x_k$
\RETURN The recovered localization error vector, $\bold x^*$.
\end{algorithmic}
\label{alg:scp}
\end{algorithm}