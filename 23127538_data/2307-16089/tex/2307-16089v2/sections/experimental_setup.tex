\section{Experimental Setup}
\label{sec:experimental_setup}

We compare \manner{} against state-of-the-art NNRs on a range of single- and multi-aspect recommendation tasks. We experiment with two aspects: \textit{topical categories} (\textit{ctg}) and news \textit{sentiment} (\textit{snt}).

\vspace{1.4mm}
\noindent\textbf{Baselines.} 
We evaluate several NNRs trained on classification objectives. We follow \citet{wu2021empowering} and replace the original NEs of all baselines that do not use PLMs (instead, contextualizing word embeddings with convolution or self-attention layers) with the same PLM used in \manner{}.\footnote{The only exception is the final text embedding, where \citet{wu2021empowering} pool tokens with an attention network.} % instead of using the \texttt{[CLS]} embedding.}
%
We include two models optimized purely for content personalization: (1) NRMS \cite{wu2019nrms}, and (2) MINER \cite{li2022miner}. We further evaluate seven NNRs that inject aspect information. Thereof, five incorporate \textit{topical categories}, i.e., (3) NAML \cite{wu2019naml}, (4) LSTUR \cite{an2019lstur}, (5) MINS \cite{wang2022news}, (6) CAUM \cite{qi2022news}, (7) TANR \cite{wu2019tanr}, and two the news \textit{sentiment}: (8) SentiRec \cite{wu2020sentirec}, and (9) SentiDebias \cite{wu2022removing}. For more details, see Appendix \ref{sec:appendix_baselines}. 

\vspace{1.4mm}
\noindent\textbf{Data.}
%
We carry out the evaluation on two prominent monolingual news recommendation benchmarks: MINDlarge (denoted MIND) \cite{wu2020mind} with news in English and Adressa-1 week \cite{gulla2017adressa} (denoted Adressa) with Norwegian news. We provide further details about dataset usage and statistics in Appendix \ref{sec:appendix_datasets}. 
%
As Adressa contains no disambiguated named entities, we use only the news title as input to \manner{}' NE, while on MIND we use all news features as NE input.


\vspace{1.4mm}
\noindent\textbf{Evaluation Metrics.}
We report performance with AUC, MRR, nDCG@k ($k \hspace{-0.2em} = \hspace{-0.2em} \{5, 10\}$). We measure aspect-based diversity of recommendations at position $k$ as the normalized entropy of the distribution of aspect $A_p$'s values in the recommendation list: 
%
\begin{equation}
\small
    D_{A_p}@k \hspace{-0.2em} = \hspace{-0.2em} - \sum_{j \in A_p} \frac{p(j) \log p(j)}{\log(|A_p|)}
\end{equation}
where $A_p \hspace{-0.2em} \in \hspace{-0.2em} \{ctg, snt\}$, and $|A_p|$ is the number of $A_p$ classes.
%
If aspect-based personalization is successful, aspect $A_p$'s distribution in the recommendations should be similar to its distribution in the user history. We evaluate personalization with the generalized Jaccard similarity \cite{bonnici2020kullback}:
% , a robust measure of similarity between two probability distributions 
%
\begin{equation}
\small
    \mathit{PS}_{A_p}@k \hspace{-0.2em} = \hspace{-0.2em} \frac{\sum_{j=1}^{|A_p|} \min(\mathcal{R}_j, \mathcal{H}_j)}{\sum_{j=1}^{|A_p|} \max(\mathcal{R}_j, \mathcal{H}_j)},
\end{equation}
where $R_j$ and $H_j$ represent the probability of a news with class $j$ of $A_p$ to be contained in the recommendations list $R$, and, respectively, in the user history $H$.
%
As all metrics are bounded to $[0, 1]$, we measure the trade-off between content-based personalization (nDCG@$k$) and either aspect-based diversity $D_{A_p}@k$ or aspect-based personalization $\mathit{PS}_{A_p}@k$ with the harmonic mean. We denote this T\textsubscript{A\textsubscript{p}}@$k$ for single-aspect. For multi-aspect evaluation, i.e., when ranking for content-personalization by diversifying simultaneously over topics and sentiment, we adopt as evaluation metric the harmonic mean between nDCG@$k$, D\textsubscript{ctg}@$k$ (topical category), and D\textsubscript{snt}@$k$ (sentiment), denoted 
T\textsubscript{all}@$k$. 

\vspace{1.4mm}
\noindent\textbf{Training Details.}
%
We use RoBERTa Base \cite{liu2019roberta} and NB-BERT Base \cite{kummervold2021operationalizing,nielsen2023scandeval} in experiments on MIND and Adressa, respectively. 
%
We set the maximum history length to 50. We tune the main hyperparameters of all NNRs. We train all models with mixed precision, the Adam optimizer \cite{kingma2014adam}, the learning rate of 1e-5 on MIND, 1e-6 on Adressa, and 1e-6 for the sentiment\,\texttt{A-Module} on both datasets.
%%
In \texttt{A-Module} training, we sample 20 instances per class,\footnote{For $M$ class instances, we obtain $\frac{M^2-M}{2}$ positive pairs for that class for SCL. 
% Preliminary experiments with larger $M$ did not yield improvements; we thus kept $M=20$ to reduce the computational footprint of our experiments.
} while in \texttt{CR-Module} training we sample four negatives per positive example. We find the optimal temperature of 0.36 on MIND, and 0.14 on Adressa, for the \texttt{CR-Module}, and of 0.9 for all \texttt{A-Modules} on both datasets.
%
We train all baselines and the \texttt{CR-Module} for 5 epochs on MIND and 20 epochs on Adressa, with a batch size of 8. We train each \texttt{A-Module} for 100 epochs, with the batch size of 60 for sentiment and 360 for topics. We repeat runs five times with different seeds and report averages and standard deviations for all metrics. We refer to Appendices \ref{sec:appendix_model_parameters} - \ref{sec:appendix_hyperparameters} for further details about model sizes and hyperparameters. 

