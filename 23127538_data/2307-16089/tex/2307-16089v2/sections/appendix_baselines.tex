\section{Baselines}
\label{sec:appendix_baselines}

We compare \manner{} against the following content-based NNRs, in which we replace the original NEs of all baselines that do not use PLMs with the same PLM employed in \manner{}:
\begin{enumerate}
    \item NRMS \cite{wu2019nrms} encodes only the news title, and learns user representations with an encoder combining multi-head self-attention and additive attention. 
    \item MINER \cite{li2022miner} employs a poly attention scheme to extract multiple user interest vectors for the users' representations using additive attention layers.
    \item NAML \cite{wu2019naml} leverages information about topical categories, in addition to textual content from the news title and abstract, as input to the NE. It learns user representations from the clicked news embeddings with a user encoder based on additive attention.
    \item LSTUR \cite{an2019lstur} also incorporates category information in the NE, next to title and abstract. It learns user representations via recurrent neural networks, and differentiates between short-term user preferences encoded with a GRU \cite{cho2014learning}, and long-term embeddings, consisting of a randomly initialized and fine-tuned component.
    \item MINS \cite{wang2022news} embeds both textual features (i.e, title, abstract), as well as categories. It employs a UE which combines multi-head self-attention, multi-channel GRU-based recurrent network, and additive attention to generate user embeddings.
    \item CAUM \cite{qi2022news} leverages not only titles and corresponding named entities, but also topical categories as input to the NE. In contrast to the other baselines, it combines a candidate-aware self-attention network with a candidate-aware convolutional neural network to learn candidate-aware user representations.
    \item TANR \cite{wu2019tanr} injects category information by jointly optimizing the NE for content-based personalization and topic classification. Its UE is analogous to that of NAML.
    \item SentiRec \cite{wu2020sentirec} adds regularization for sentiment diversity to its primary content personalization objective, and encodes users similarly to NRMS.
    \item SentiDebias \cite{wu2022removing} uses sentiment-debiasing based on adversarial learning to reduce the NNR's sentiment bias (originating from the user data) and generate sentiment-diverse recommendations.
\end{enumerate}
