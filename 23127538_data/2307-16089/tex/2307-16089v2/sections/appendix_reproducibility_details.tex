\section{Reproducibility Details}
\label{sec:appendx_reproducibility}

\subsection{Model Parameters.}
\label{sec:appendix_model_parameters}
Table \ref{tab:model_parameters} lists the number of model parameters, in millions, for both datasets. 

\subsection{Hyperparameters and Implementation}
\label{sec:appendix_hyperparameters}

\vspace{1.4mm}
\noindent\textbf{Hyperparameter Optimization.}
We use RoBERTa Base \cite{liu2019roberta} and NB-BERT Base \cite{kummervold2021operationalizing,nielsen2023scandeval} as the backbone PLMs of all models, in experiments on MIND and Adressa, respectively. In both cases, we fine-tune only the PLM's last four layers.\footnote{In preliminary results, we did not see significant differences between full fine-tuning of all layers and fine-tuning only the last four layers. In the interest of computational efficiency, we thus froze the first eight layers of the transformer.}
In the cross-lingual transfer experiments from \$\ref{sec:xlt}, we fine-tune all of the 6 layers of DistilBERT.
%
We use 100-dimensional TransE embeddings \cite{bordes2013translating} pretrained on Wikidata as input to the entity encoder in the NE of the knowledge-aware NNRs. We perform hyperparameter tuning on the main hyperparameters of \manner{} and the baselines using grid search. Table \ref{tab:hyperparameters} lists the search spaces for all the optimized hyperparameters, as well as the best values. We repeat each experiment five times with the seeds ($\{42, 43, 44, 45, 46\}$) set with PyTorch Lightning's \texttt{seed\_everything}.
%
\input{tables/model_parameters}
%
\input{tables/hyperparameters}
%

\vspace{1.4mm}
\noindent\textbf{Code.} We train \manner{}, as well as all the baselines, using the implementations provided in the NewsRecLib library \cite{iana-etal-2023-newsreclib}.\footnote{\href{https://github.com/andreeaiana/newsreclib}{https://github.com/andreeaiana/newsreclib}}

\vspace{1.4mm}
\noindent\textbf{Infrastructure and Compute.}
We conduct all experiments on a cluster with virtual machines. We train \manner{} on both datasets, as well as the baselines on MIND, on a single NVIDIA A100 40GB GPU. We train the baselines on Adressa on a single NVIDIA Tesla V100 32GB GPU. 

 