\section{Additional Results}
\label{sec:appendix_additional_results}

\subsection{Content Personalization}
\label{sec:appendix_content_personalization}

Fig.~\ref{fig:ablation_features} shows \manner{}'s performance on MIND for different inputs to the NE. Even the \texttt{CR-Module} exposed to titles only (i.e., no abstract or entity information) outperforms all of the baselines on content recommendation.
Fig. \ref{fig:ablation_training} illustrates \manner{}'s performance for alternative architecture designs and training objectives (cf. \$\ref{sec:content_personalization}).\footnote{For brevity, we report results on MIND; findings on Adressa exhibit identical trends.}
%
% Figure environment removed

\subsection{Single-Aspect Customization}
\label{sec:appendix_single_apsect_customization}

% Figure environment removed
%
Figure \ref{fig:single_aspect_results_addresa} explores the trade-off between content and aspect diversification, and respectively, personalization tasks for different values of $\lambda$\textsubscript{ctg} and $\lambda$\textsubscript{snt} on the Adressa dataset. 
%
Fig. \ref{fig:tsne_embeddings_adressa} shows the 2-dimensional t-SNE visualizations \cite{van2008visualizing} of the news embeddings produced with aspect-specialized NEs trained on Adressa. 

\subsection{Multi-Aspect Customization}
\label{sec:appendix_multi_apsect_customization}

Fig. \ref{fig:multi_aspect_results_addressa} explores the trade-off between content personalization and multi-aspect diversification on Adressa.

%
% Figure environment removed

% Figure environment removed   

\subsection{Cross-Lingual Transfer}
\label{sec:appendix_xlt}

% Figure environment removed

Fig. \ref{fig:xlt_adressa_mind_pers} summarizes the \texttt{XLT} results for single-aspect personalization on the target-language dataset MIND, whereas Fig. \ref{fig:xlt_mind_adressa} shows the analogous \texttt{XLT} results for single-aspect diversification and personalization, respectively, on the target-language dataset Adressa.

% Figure environment removed

\subsection{Time Complexity Analysis}
\label{sec:appendix_time}

Table \ref{tab:inference_time} shows the average inference time for the entire MIND (365,201 impressions), and respectively, Adressa (146,284 impressions) test sets. Note that runtimes heavily depend on the computing infrastructure used, as well as parallel usage of the infrastructure for other tasks, as experiments are conducted on a HPC cluster. We highlight that \manner{} achieves a much lower inference time than the other NNRs.

\input{tables/inference_time}