\section{Experimental Setup}
\label{sec:experimental_setup}

We compare \manner{} against state-of-the-art NNRs on a range of single- and multi-aspect recommendation tasks. We experiment with two aspects: \textit{topical categories} (\textit{ctg}) and news \textit{sentiment} (\textit{snt}).

\vspace{1.4mm}
\noindent\textbf{Baselines.} 
We evaluate several NNRs trained on classification objectives. We follow \citet{wu2021empowering} and replace the original NEs of all baselines that do not use PLMs (instead, contextualizing word embeddings with convolution or self-attention layers) with the same PLM used in \manner{}.\footnote{The only exception is the final text embedding, where \citet{wu2021empowering} pool tokens with an attention network.} 
%
We include two models optimized purely for content personalization: (1) NRMS \cite{wu2019nrms}, and (2) MINER \cite{li2022miner}. We further evaluate seven NNRs that inject aspect information. Thereof, five incorporate \textit{topical categories}, i.e., (3) NAML \cite{wu2019naml}, (4) LSTUR \cite{an2019lstur}, (5) MINS \cite{wang2022news}, (6) CAUM \cite{qi2022news}, (7) TANR \cite{wu2019tanr}, and two the news \textit{sentiment}: (8) SentiRec \cite{wu2020sentirec}, and (9) SentiDebias \cite{wu2022removing}. 

\vspace{1.4mm}
\noindent\textbf{Data.}
%
We carry out the evaluation on two prominent monolingual news recommendation benchmarks: MINDlarge (denoted MIND) \cite{wu2020mind} with news in English and Adressa-1 week \cite{gulla2017adressa} (denoted Adressa) with Norwegian news. 
% 
Since \citet{wu2020mind} do not release test labels for MIND, we use the provided validation portion for testing, and split the respective training set into temporally disjoint training (first four days of data) and validation portions (the last day). Following established practices on splitting the Adressa dataset \cite{hu2020graph,xu2023group}, we use the data of the first five days to construct user histories and the clicks of the sixth day to build the training dataset. We randomly sample 20\% of the last day's clicks to create the validation set, and treat the remaining samples of the last day as the test set.\footnote{Note that during validation and testing, we reconstruct user histories with all the samples of the first six days of data.} Since Adressa contains only positive samples (i.e., no data about users' seen but not clicked news), we randomly sample 20 news as negatives for each clicked article to build impressions following \citet{yi2021efficient}.\footnote{Table \ref{tab:datasets} summarizes the datasets' statistics.}
%
As Adressa contains no disambiguated named entities, we use only the news title as input to \manner{}' NE, while on MIND we use all news features as NE input.

Regarding aspects, the topical category annotations are provided in both datasets. As no sentiment labels exist in neither MIND nor Adressa, we use a multilingual XLM-RoBERTa Base model \cite{conneau2020unsupervised} trained on tweets and fine-tuned for sentiment analysis \cite{barbieri2022xlm} to classify news into three classes: positive (pos), neutral, and negative (neg). We compute real-valued scores using the model's confidence scores $s_i$ for class $i$, and the predicted sentiment class label $\hat{l}$ as follows:
%
\begin{equation}
\small
    s_{sent}  \hspace{-0.2em} =  \hspace{-0.2em}
        \begin{cases}
            (+1) \times s_{pos} \text{, if } \hat{l} \hspace{-0.2em} =  \hspace{-0.2em} pos \\  
            (-1) \times s_{neg} \text{, if } \hat{l} \hspace{-0.2em} =  \hspace{-0.2em} neg \\  
            (1 - s_{neutral})  \hspace{-0.2em} \times \hspace{-0.2em} (s_{pos} - s_{neg}) \text{, otherwise} \\  
        \end{cases}
\end{equation}
%

\vspace{1.4mm}
\noindent\textbf{Evaluation Metrics.}
We report performance with AUC, MRR, nDCG@k ($k \hspace{-0.2em} = \hspace{-0.2em} \{5, 10\}$). We measure aspect-based diversity of recommendations at position $k$ as the normalized entropy of the distribution of aspect $A_p$'s values in the recommendation list: 
%
\begin{equation}
\small
    D_{A_p}@k \hspace{-0.2em} = \hspace{-0.2em} - \sum_{j \in A_p} \frac{p(j) \log p(j)}{\log(|A_p|)}
\end{equation}
where $A_p \hspace{-0.2em} \in \hspace{-0.2em} \{ctg, snt\}$, and $|A_p|$ is the number of $A_p$ classes.
%
If aspect-based personalization is successful, aspect $A_p$'s distribution in the recommendations should be similar to its distribution in the user history. We evaluate personalization with the generalized Jaccard similarity \cite{bonnici2020kullback}:
%
\begin{equation}
\small
    \mathit{PS}_{A_p}@k \hspace{-0.2em} = \hspace{-0.2em} \frac{\sum_{j=1}^{|A_p|} \min(\mathcal{R}_j, \mathcal{H}_j)}{\sum_{j=1}^{|A_p|} \max(\mathcal{R}_j, \mathcal{H}_j)},
\end{equation}
where $R_j$ and $H_j$ represent the probability of a news with class $j$ of $A_p$ to be contained in the recommendations list $R$, and, respectively, in the user history $H$.
%
As all metrics are bounded to $[0, 1]$, we measure the trade-off between content-based personalization (nDCG@$k$) and either aspect-based diversity $D_{A_p}@k$ or aspect-based personalization $\mathit{PS}_{A_p}@k$ with the harmonic mean. We denote this T\textsubscript{A\textsubscript{p}}@$k$ for single-aspect. For multi-aspect evaluation, i.e., when ranking for content-personalization by diversifying simultaneously over topics and sentiment, we adopt as evaluation metric the harmonic mean between nDCG@$k$, D\textsubscript{ctg}@$k$ (topical category), and D\textsubscript{snt}@$k$ (sentiment), denoted 
T\textsubscript{all}@$k$. 

\vspace{1.4mm}
\noindent\textbf{Training Details.}
%
We use RoBERTa Base \cite{liu2019roberta} and NB-BERT Base \cite{kummervold2021operationalizing,nielsen2023scandeval} in experiments on MIND and Adressa, respectively. 
%
We set the maximum history length to 50. We tune the main hyperparameters of all NNRs. We train all models with mixed precision, the Adam optimizer \cite{kingma2014adam}, the learning rate of 1e-5 on MIND, 1e-6 on Adressa, and 1e-6 for the sentiment\,\texttt{A-Module} on both datasets.
%%
In \texttt{A-Module} training, we sample 20 instances per class,\footnote{For $M$ class instances, we obtain $\frac{M^2-M}{2}$ positive pairs for that class for SCL. 
% Preliminary experiments with larger $M$ did not yield improvements; we thus kept $M=20$ to reduce the computational footprint of our experiments.
} while in \texttt{CR-Module} training we sample four negatives per positive example. We find the optimal temperature of 0.36 on MIND, and 0.14 on Adressa, for the \texttt{CR-Module}, and of 0.9 for all \texttt{A-Modules} on both datasets.
%
We train all baselines and the \texttt{CR-Module} for 5 epochs on MIND and 20 epochs on Adressa, with a batch size of 8. We train each \texttt{A-Module} for 100 epochs, with the batch size of 60 for sentiment and 360 for topics. We repeat runs five times with different seeds and report averages and standard deviations for all metrics. We refer to Appendices \ref{sec:appendix_model_parameters} - \ref{sec:appendix_hyperparameters} for further details about model sizes and hyperparameters. 

