\section*{Limitations}
\label{sec:limitations}

\manner{} targets exclusively content-based neural news recommendation and leverages solely textual features. In practice, recommender systems may incorporate content features from various other modalities (e.g., image, video), as well as similarities between users in a collaborative filtering manner. We leave the extension of \manner{} with multi-modal content for future work.

\manner{} independently handles each aspect and aggregates them by weighting the aspect-specific similarities. While it could be argued that direct interactions between different aspects might improve the recommendation performance, training a separate A-Module for each aspect is exactly what drives \manner's flexibility. The A-modules allow the user to arbitrarily define the preferences for any concrete recommendation, by defining how much diversification or personalization is desired over each aspect. As illustrated by the results of our experiments, \manner{} outperforms all the state-of-the-art systems, including the ones where additional aspects are directly integrated in the training objective or in the news encoder. MANNeR is thus, besides being drastically more flexible (as it supports arbitrary recommendation objectives at inference time), also more performant, despite the fact that no interactions exist between the aspect modules at training time.    


Our framework fully fine-tunes a PLM per aspect-specific module (either for content-relevance in the \texttt{CR-Module} or for aspect similarity in the \texttt{A-Module}). As all modules share the same PLM as backbone, parameter efficient fine-tuning (PEFT), e.g.  LoRA \cite{hu2021lora}, would bypass the need to repeatedly load the entire PLM per module into memory. PEFT has been shown to closely meet the performance of full fine-tuning. This represents a key advantage for deploying \manner{} in real-world applications. We however fully fine-tuned models to avoid PEFT as a confounding factor in our experiments. We further chose base-sized PLMs as the backbone of the NE in all models due to computational constraints. While in fine-tuning they remain competitive to large language models (LLMs), the latter may capture richer semantics, which can prove particularly useful for \texttt{XLT} applications. With a PEFT approach, \manner{} could easily leverage LLMs without a corresponding increase in computational resources.

Lastly, there exist varied approaches for measuring the descriptive \cite{castells2021novelty} and normative \cite{vrijenhoek2023radio} diversity of recommendations. While some of these metrics can be tailored to support arbitrary aspects (i.e., to measure the diversity of recommendations w.r.t. to a particular categorical feature), we opted to quantify aspect-based diversity as generally as possible, leveraging only the distribution of an aspect's values in the recommendation list. We leave exploration of further diversity metrics to future work.

