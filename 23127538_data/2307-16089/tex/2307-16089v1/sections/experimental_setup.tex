\section{Experimental Setup}
\label{sec:experimental_setup}

We compare \manner{} against state-of-the-art NNRs on a range of single- and multi-aspect recommendation tasks. We experiment with two aspects: \textit{topical categories} and \textit{sentiment} of news.

\rparagraph{Baselines.} 
We evaluate several state-of-the-art NNRs, trained by minimizing the classification-oriented cross-entropy loss. For fair comparison, following \citet{wu2021empowering}, we replace the original NEs of all baseline models that do not use PLMs (which range from aggregations of word embeddings, over convolutional networks to multi-head self-attention) with the same PLM we employ in \manner{}.\footnote{The only exception is the final text embedding, where \citet{wu2021empowering} pool subword tokens with an attention network instead of using the \texttt{[CLS]} embedding.}
With that modification, we evaluate six NNRs optimized purely for content personalization: (1) \textit{NAML} \cite{wu2019naml}, (2) \textit{NRMS} \cite{wu2019nrms}, (3) \textit{LSTUR}\footnote{We evaluate two variants: LSTUR\textsubscript{ini} and LSTUR\textsubscript{con}, which mutually differ in their user embeddings. We refer the reader to \citet{an2019lstur} for details.} \cite{an2019lstur}, (4) MINS \cite{wang2022news}, (5) CAUM \cite{qi2022news}, and (6) MINER\footnote{We evaluate three variants: MINER\textsubscript{max}, MINER\textsubscript{mean}, and MINER\textsubscript{weighted}, which differ in the aggregation strategy for the final user click score. We refer the reader to \citet{li2022miner} for details.} \cite{li2022miner}; and three NNRs trained with aspect-informed objectives: (7) \textit{TANR} \cite{wu2019tanr} injects information on topical categories by jointly optimizing for content-based personalization and topic classification, (8) \textit{SentiRec} \cite{wu2020sentirec} adds regularization for sentiment diversity to its primary content personalization objective, and (9) SentiDebias \cite{wu2022removing} uses sentiment-debiasing based on adversarial learning to reduce the NNR's sentiment bias (originating from the user data) and generate sentiment-diverse recommendations.

% \vspace{1.4mm}
% \noindent\textbf{Data.}
\rparagraph{Data.}
%
We carry out the evaluation on two prominent monolingual news recommendation benchmarks: MINDlarge (denoted MIND) \cite{wu2020mind} with news in English and Adressa-1 week \cite{gulla2017adressa} (denoted Adressa) with Norwegian news. 
%%
Since \citet{wu2020mind} do not release test labels for MIND, we use the provided validation portion for testing, and split the respective training set into temporally disjoint training (first four days of data) and validation portions (the last day). Following established practices on splitting the Adressa dataset \cite{hu2020graph,xu2023group}, we use the data of the first five days to construct the user history and the clicks of the sixth day to build the training dataset. We randomly sample 20\% of the clicks of the last day to create the validation set, and treat the remaining samples of the last day as the test set. Note that during validation and testing, we reconstruct the user histories using all the samples of the first six days of data. Since the Adressa dataset contains only positive samples (i.e., it includes no data about news seen but not clicked by the user) \cite{gulla2017adressa}, we randomly sample 20 news as negatives for each clicked article in order to build impressions following \citet{yi2021efficient}. Table \ref{tab:mind_dataset} summarizes the statistics of the two datasets. 

Regarding aspects, the topical category annotations are already provided in both datasets. As no sentiment labels exist in neither MIND nor Adressa, we use a multilingual XLM-RoBERTa Base model \cite{conneau2020unsupervised} trained on tweets and fine-tuned for sentiment analysis \cite{barbieri2022xlm} to classify news into one of three classes: positive (pos), neutral, and negative (neg). We compute real-valued scores using the model's confidence scores as follows:
%
\begin{equation}
\small
    s_{sent} = 
        \begin{cases}
            (+1) \times s_{pos} \text{, if } \hat{l} = pos \\  
            (-1) \times s_{neg} \text{, if } \hat{l} = neg \\  
            (1 - s_{neutral}) \times (s_{pos} - s_{neg}) \text{, otherwise} \\  
        \end{cases}
\end{equation}
%
where $s_i$ denotes the model's score for class $i$, and $\hat{l}$ is the predicted sentiment class label.

Adressa contains no disambiguated named entities, so we only use news title as input to NE: this model variant is denoted \mannertitle{}. On MIND, we train and evaluate both the ``full'' \manner{} (i.e., with title, abstract and corresponding entities as NE input) and \mannertitle{}.

\input{tables/datasets}

% \vspace{1.4mm}
% \noindent\textbf{Evaluation Metrics.}
\rparagraph{Evaluation Metrics.}
In line with prior work, we report the models' performance in terms of four metrics: AUC, MRR, nDCG@5, and nDCG@10. Additionally, we measure aspect-based diversity of recommendations at position $k$ as the normalized entropy of the distribution of the aspect $A_p$'s values in the recommendation list: 
%
\begin{equation}
    D_{A_p}@k = - \sum_{j \in A_p} \frac{p(j) \log p(j)}{\log(|A_p|)}
\end{equation}
where $A_p \in \{ctg, snt\}$, $|A_p|$ denotes the number of classes of aspect $A_p$, and \textit{ctg} and \textit{snt} stand for \textit{topical category} and \textit{sentiment}, respectively.

If aspect-based personalization is successful, the aspect $A_p$'s distribution in the recommendations should be similar to its distribution in the user history. We thus opt to evaluate personalization with the generalized Jaccard similarity, a robust measure of similarity between two probability distributions \cite{bonnici2020kullback}: 
%
\begin{equation}
    \mathit{PS}_{A_p}@k = \frac{\sum_{j=1}^{|A_p|} \min(\mathcal{R}_j, \mathcal{H}_j)}{\sum_{j=1}^{|A_p|} \max(\mathcal{R}_j, \mathcal{H}_j)},
\end{equation}
where $R_j$ and $H_j$ represent the probability of a news with class $j$ of aspect $A_p$ to be contained in the recommendations list $\mathcal{R}$, and, respectively, in the user history $\mathcal{H}$.

Lastly, since all metrics are bounded to the $[0, 1]$ range, we measure the trade-off between content-based personalization (nDCG@$k$) and either aspect-based diversity $D_{A_p}@k$ or aspect-based personalization $\mathit{PS}_{A_p}@k$ by combining them with the harmonic mean. We denote this with T\textsubscript{A\textsubscript{p}}@$k$ for single-aspect. For multi-aspect evaluation, i.e., when we rank for content-personalization by diversifying simultaneously over topics and sentiment, we adopt as the evaluation metric the harmonic mean between nDCG@$k$, D\textsubscript{ctg}@$k$ (topical category), and D\textsubscript{snt}@$k$ (sentiment), denoted with T\textsubscript{all}@$k$. 
%%%
%%% Could stay, but can also be removed in the interest of space
%%%
Note that, while we use an unweighted harmonic mean to quantify the trade-off between recommendation and aspect-based customization (i.e. we equally weigh all the aspects), specific use-cases in which some aspect is more important than others could, more generally, combine the aspect scores with a weighted (harmonic) mean.

\rparagraph{Implementation and Optimization Details.}
We use RoBERTa Base \cite{liu2019roberta} (a monolingual English PLM) and NB-BERT Base \cite{kummervold2021operationalizing,nielsen2023scandeval} (a monolingual Norwegian PLM) in experiments on MIND and Adressa, respectively. In both cases, our training fine-tunes only the last four layers of the PLM.\footnote{In preliminary results, we did not see significant differences between full fine-tuning of all layers and fine-tuning only the last four layers. In the interest of computational efficiency, we thus froze the first eight layers of the transformer.}
%%
We report the number of parameters for each model and dataset in Table \ref{tab:model_parameters}.\footnote{For LSTUR, with its user embedding matrix, the number of parameters depends on the training data size.} We use 100-dimensional TransE embeddings \cite{bordes2013translating} pretrained on Wikidata as input to the entity encoder in NE. We set the maximum history length to 50 and set all other model-specific hyperparameters for baselines to optimal values reported in the respective papers. We train all models with mixed precision, and optimize with the Adam algorithm \cite{kingma2014adam}, with the learning rate set to 1e-5 on MIND and 1e-6 on Adressa.\footnote{For the sentiment\,\texttt{A-Module}: learning rate = 1e-6 for both datasets.}
%%
In \texttt{A-Module} training, we sample 20 instances per class,\footnote{For $M$ class instances, we obtain $\frac{M^2-M}{2}$ positive pairs for that class for SCL. Preliminary experiments with larger $M$ did not yield improvements; we thus kept $M=20$ to reduce the computational footprint of our experiments.} whereas in the \texttt{CR-Module} training we sample four negatives per positive example. We search for the optimal \texttt{SCL} temperature $\tau$ using the validation portions of the two dataset variants, sweeping the intervals $[0.1, 0.4]$ with a step of 0.02 for the \texttt{CR-Module}, and $[0.1, 0.9]$ with a step of 0.05 for the \texttt{A-Module}, respectively. For fair comparison, we train all the baseline NNRs under the same training budget as for the \texttt{CR-Module} in \manner{}: for 5 epochs on MIND and 20 epochs on Adressa, with a batch size of 8. We train each \texttt{A-Module} for 100 epochs, with the batch size of 60 (3 classes) for sentiment and 360 for topics (18 topics). We run each experiment five times with different random seeds and report averages and standard deviations for all metrics. We trained each model on one NVIDIA Tesla\,A100\,GPU (40GB memory).\footnote{Our implementation is publicly available at \url{https://github.com/andreeaiana/manner}}
% \footnote{Our implementation will be publicly available on GitHub upon acceptance.} 

\input{tables/model_parameters}
