\section{Results and Discussion}
\label{sec:rsults_discussion}

We first discuss the content personalization performance of \manner{} in comparison to state-of-the-art NNRs. We then analyze its capability for single- and multi-aspect (i) diversification and (ii) personalization. In the latter (i.e., aspect customization experiments), we treat \manner{}, and respectively \mannertitle{}, with just the \texttt{CR-Module} (i.e. $\lambda_{A_p} = 0$) as (another) baseline. Lastly, we evaluate \manner{}'s ability to re-use pretrained aspect-specific modules in cross-lingual transfer experiments. 

\subsection{Content Personalization}
\label{subsec:results_recommendation}

\input{tables/recommendation_results}

Table \ref{tab:results_recommendation} summarizes the results on content personalization for \manner{} and the baselines.
Since the task does not require any aspect-based customization, we evaluate the \manner{} variant that uses only its CR-Module at inference time (i.e., $\lambda = 0$).
%%
\manner{} consistently outperforms all state-of-the-art NNRs in terms of both classification and ranking metrics on both datasets. Given that \manner{}'s \texttt{CR-Module} derives the user embedding by merely averaging clicked news embeddings, these results question the need for complex parameterized UEs, present in all the baselines, in line with the findings of \citet{iana2023simplifying}. 
% \manner{} also seems to be sample efficient: while the other NNR models clearly benefit from larger training datasets (MINDlarge\,vs.\,MINDsmall), \manner{}'s performance is almost the same (marginally better for MINDlarge). This suggests that our lightweight \texttt{CR-Module} is especially suitable for data-lean settings.  

% \vspace{1.4mm}
% \noindent\textbf{Ablations.}
\rparagraph{Ablations.}
%%
We next ablate the \texttt{CR-Module}'s content personalization performance along two dimensions: Fig.~\ref{fig:ablation_features} shows \manner{}'s performance on the MIND dataset for different inputs to the NE; \ref{fig:ablation_training} shows performance for alternative architecture designs and training objectives\footnote{For brevity, we report these results only on MIND; results for Adressa exhibit the same trends.}.  
%%%
Fig.~\ref{fig:ablation_features} reveals that all groups of features -- news abstract, as well as entities in both title and abstract -- contribute to the overall performance. We note, however, that even the \texttt{CR-Module} that was exposed only to raw title text (i.e., no abstract text nor entity information) outperforms all state-of-the-art NNRs on content recommendation (cf.~Table \ref{tab:results_recommendation}).

Figure \ref{fig:ablation_training} compares the CR-Module (as detailed in \S\ref{sec:methodology}) against two alternative design choices, namely when we replace: (1) the simple averaging of clicked news embeddings with a parameterized (i.e., trainable) user encoder, namely an additive attention layer; and (2) our metric-based SCL training objective with the standard cross-entropy classification loss. Results justify both of our design decisions: (1) the fact that a trainable UE leads to performance loss strengthens our skepticism for the need for elaborate UE components \cite{iana2023simplifying}, commonly used in NNR; (2) replacing SCL with the standard cross-entropy loss results in an even more prominent performance loss: this is in line with findings from other similarity-oriented NLP tasks in which contrastive objectives led to better class separation \cite{reimers2019sentence,tunstallefficient}. 

% Figure environment removed

\input{tables/diversification_results}

\subsection{Single-aspect Customization}

% Figure environment removed
%
% Figure environment removed


\paragraph{Diversification.}
Table~\ref{tab:results_diversification} summarizes the results on aspect diversification tasks (i.e. for topical categories and sentiment). We report the performance for content personalization (nDCG), single-aspect diversification (D\textsubscript{ctg} for topical category, D\textsubscript{snt} for sentiment), single-aspect harmonic mean between content personalization and aspect diversification (T\textsubscript{ctg} for topical category, T\textsubscript{snt} for sentiment, respectively), and the multi-aspect harmonic mean between nDCG, D\textsubscript{ctg}, and D\textsubscript{snt} (T\textsubscript{all}).\footnote{For brevity, we report metrics only for $k=10$; results for $k=5$ exhibit the same trends.}
%%%
All baseline models (baseline NNRs and \manner{}'s \texttt{CR-Module}, without aspect diversification) obtain similar diversification scores (D\textsubscript{ctg} and D\textsubscript{snt}). 
%
% 
The sentiment-aware SentiRec-PLM, with an explicit auxiliary sentiment diversification objective, yields the highest sentiment diversity (D\textsubscript{snt}) on Adressa; this comes at the expense of content personalization quality (lowest nDCG). %Somewhat surprisingly, its sentiment diversification is not better than that of other models o
On MIND, SentiDebias-PLM -- another sentiment-specific model -- achieves the highest sentiment diversity, but also exhibits the lowest content personalization performance. Overall, the results in Table \ref{tab:results_diversification} point to a trade-off between content personalization and aspectual diversity: models with higher D\textsubscript{A\textsubscript{p}}@$k$ tend to have a lower nDCG@$k$. 

Unlike all other models, \manner{} can trade content personalization for diversity (and vice-versa) with different values of the aspect coefficients $\lambda$. 
We analyze this trade-off for different values of $\lambda$\textsubscript{ctg} and $\lambda$\textsubscript{snt}.  %%% 
Figures \ref{fig:single_aspect_div_mind} and \ref{fig:single_aspect_div_adressa} illustrate the \manner{} performance in single-aspect diversification tasks for different values of $\lambda$ on MIND and Adressa, respectively. 
%%%
The steady drop in nDCG together with the steady increase in D\textsubscript{A\textsubscript{p}} indeed indicate the existence of a trade-off between content personalization and aspect diversification. For topical categories we observe a steeper decline in content personalization quality with improved diversification than for sentiment. Sentiment diversity reaches peak performance for $\lambda_{snt}=-0.4$ on MIND and $\lambda_{snt}=-0.6$ on Adressa, whereas category diversity continues to increase all the way to $\lambda_{ctg}=-0.9$ on MIND and $\lambda_{ctg}=-1.0$ on Adressa. Intuitively, content-based personalization is more aligned with the topical than with the sentiment consistency of recommendations. The best trade-off (i.e., maximal performance according to T\textsubscript{A\textsubscript{p}}@10), is achieved with $\lambda_{ctg}=-0.2$ and $\lambda_{ctg}=-0.3$ for topics on MIND, and Adressa, respectively, and $\lambda_{snt}=-0.2$ for sentiment on both datasets.  

We attribute these effects to the representation spaces of our \texttt{A-Modules} (for categories and for sentiment). We thus further investigate the aspect-specific representation spaces of the two \texttt{A-Modules}. Fig. \ref{fig:tsne_embeddings} shows the 2-dimensional t-SNE visualizations \cite{van2008visualizing} of the news embeddings produced with aspect-specialized encoders trained on the two datasets: (a) and (c) for topical categories;  and (b) and (d) for sentiment. The results confirm our hypothesis: the latent representation space of the encoder was reshaped to group instances from the same classes.
% We observe exactly what we hoped that the metric-based SCL objective would achieve: the latent representation space of the encoder was reshaped to group instances from the same classes. 
%
The separation of classes, however, is less prominent for representation spaces of the encoders trained on Adressa than for those learned on MIND (e.g., the effect is stronger on the category-shaped embedding space). We believe that this is because Adressa has 10 times fewer news than MIND (and contrastive learning, naturally, benefits from more news pairs), with over half of the topical categories in Adressa represented with fewer than 100 examples. 
%Two factors that influence the quality of the reshaped representation spaces warrant a mindful interpretation of the results. First, training with a contrastive objective benefits from larger training datasets (and Adressa-1 week has approximately one tenth of the news in MINDlarge). Second, over half of the categories from Adressa have fewer than 100 examples, and are thus underrepresented in the training data. 

% Figure environment removed

\input{tables/personalization_results}

% \vspace{1.4mm}
% \noindent\textbf{Personalization.}
\rparagraph{Personalization.}
Table \ref{tab:results_personalization} displays the results on aspect personalization tasks. Similar to the diversification tasks, we report the performance for content personalization (nDCG) along with the aspect-personalization scores (PS\textsubscript{ctg} for topics, PS\textsubscript{snt} for sentiment).   
The results generally mirror those from diversification. Surprisingly, we find that TANR -- a model trained with 
%injects category information in the recommendations by means of 
an auxiliary topic classification task -- underperforms NAML and LSTUR\textsubscript{ini}, both of which add topical categories to the set of NE input features, in category personalization (PS\textsubscript{ctg}) on MIND and Adressa, respectively. %Adressa, both of which use topical categories as additional input features to the NE. 
\manner{}'s \texttt{CR-Module} alone (i.e., without any aspect customization) yields competitive category personalization performance. 
%We observe that without any additional aspects incorporated in the model, . 
We believe that this is because (1) \manner{}'s \texttt{CR-Module} is best in content personalization and (2) category personalization is well-aligned with content personalization (i.e., news with similar content tend to belong to the same category). 
%
Figures \ref{fig:single_aspect_pers_mind} and \ref{fig:single_aspect_pers_adressa} explore the trade-off between content personalization and aspect personalization, for different (in this case, positive) values of $\lambda$ on both datasets. 
The best topical category personalization (PS\textsubscript{ctg}), obtained for $\lambda_{ctg}>0.7$ on MIND and $\lambda_{ctg}>0.4$ on Adressa, comes at the (small) expense for content personalization (small nDCG drop): with too much weight on the category similarity of news, the impact of content relevance becomes diluted. Increased sentiment personalization, on the other hand, is much more detrimental to content personalization. We find this to be intuitive: users do not really choose what to read based on sentiment. Tailoring recommendations according to the sentiment of previously clicked news thus leads to many more content-irrelevant recommendations. 

% Figure environment removed


\subsection{Multi-aspect Customization} 

\sparagraph{Diversification.}
Fig. \ref{fig:multi_aspect_div_mind} explores the trade-off between content personalization and multi-aspect diversification, i.e. diversifying simultaneously over both topical categories and sentiments, for different values of the aspect coefficients $\lambda_{ctg}$ and $\lambda_{snt}$, respectively.\footnotemark{}
%
We report the multi-aspect harmonic mean (T\textsubscript{all}) between nDCG, D\textsubscript{ctg}, and D\textsubscript{snt}. 
%
We achieve the highest T\textsubscript{all} for $\lambda_{ctg}=-0.2$ and $\lambda_{snt}=-0.25$ on MIND. In line with results on single-aspect diversification, we observe that improving diversity in terms of topical categories rather than sentiments has a more negative effect on content personalization quality, i.e. steeper decline in T\textsubscript{all}. Overall, the results in Fig. \ref{fig:multi_aspect_div_mind} confirm that \manner{} can generalize to diversify for multiple aspects at once by weighting individual aspect relevance scores less than in the single-aspect task. This can be explained by the fact that weighting several aspects higher at the same time acts as a double discounting for content personalization, diluting content relevance disproportionately.


% \vspace{1.4mm}
% \noindent\textbf{Personalization.}
\rparagraph{Personalization.}
%
Fig. \ref{fig:multi_aspect_pers_mind} shows the results of multi-aspect personalization.\footnotemark[\value{footnote}]\footnotetext{For brevity, we report results only on the MIND dataset; findings on Adressa exhibit the same patterns.} 
%
Similar to multi-aspect diversification, we report the trade-off T\textsubscript{all} between content personalization (nDCG) and topic and sentiment personalization, PS\textsubscript{ctg} and PS\textsubscript{snt}. On MIND, we achieve the best multi-aspect trade-off (i.e., the best T\textsubscript{all}) for $\lambda_{ctg}=0.5$ and $\lambda_{snt}=0.2$ on MIND. Stronger enforcing of alignment of candidate news with user's history is needed for topical categories than for sentiment (i.e., ($\lambda_{ctg}$ > $\lambda_{snt}$)). This is because sentiment exhibits low variance within topical categories (e.g., \textit{politics} news are mostly negative) and enforcing categorical personalization thus partly also achieves sentiment personalization. 
%%%%%%%%%%%%%%%
%% GG: I did not understand this below at all. As far as I can see, the optimal lambda_sent in single-aspect personalization is also 0.2 like here in multi-aspect personalization 
%%%%%%%%%%%%%%%
%In this scenario, we weight the sentiment similarity of news higher than in the single-aspect personalization task. In other words, we find that higher levels of categorical personalization support a small degree of sentiment personalization without sacrificing content personalization. This suggests that sentiments are less diverse within particular categories than between different categories.

% Figure environment removed


\subsection{Cross-Lingual Transfer}

Lastly, we analyze the transferability of \mannertitle{} across datasets and languages in single-aspect customization experiments.\footnote{We evaluate only \mannertitle{}, since the full version of \manner{} which requires news abstracts and disambiguated named entities cannot be trained on Adressa.} 

\rparagraph{Experimental Setup.}
%
Concretely, we compare the \mannertitle{}, with i.e., \texttt{CR-Module} and \texttt{A-Module} trained on the train portions of the English MIND dataset against the variants in which \texttt{CR-Module} and/or \texttt{A-Module} are trained on the Norwegian Adressa dataset and transferred to English for inference on the test portion of MIND. 
%language as they are evaluated on) evaluate the performance of \mannertitle{} fully trained and evaluated on the target-language dataset against three versions of the model which differ in the source-language dataset used for training its \texttt{CR-Module} and \texttt{A-Module}: (i) \mannertitle{} fully trained on the source-language dataset, (ii) \mannertitle{} with the \texttt{CR-Module} trained on the target-language and the \texttt{A-Module} on the source-language dataset, and (iii) \mannertitle{} with the \texttt{CR-Module} trained on the source-language and the \texttt{A-Module} trained on the target-language dataset.
%
To enable cross-lingual transfer, in these experiments we replace the monolingual PLMs used in \mannertitle{}'s NE with a multilingual DistilBERT Base model \cite{sanh2019distilbert}.\footnote{In these cross-lingual transfer experiments, we fine-tune all 6 PLM's layers.}
%
We report the performance for content personalization (nDCG), single-aspect diversification (D\textsubscript{ctg} for topic, D\textsubscript{snt} for sentiment), and single-aspect personalization (PS\textsubscript{ctg} and PS\textsubscript{snt}).

\rparagraph{Results and Discussion.}
%
Figure \ref{fig:cross_lingual_transfer_results} summarizes the cross-lingual transfer results for single-aspect diversification and personalization. 
%(MIND as target dataset, i.e., English as the target language) and Adressa as the source-language dataset (i.e., Norwegian as the source language).\footnote{For brevity, we report results with MIND as the target dataset; results on Adressa as the target dataset and MIND as the source dataset exhibit similar patterns.}
%
Expectedly, \mannertitle{} trained fully on Adressa (i.e., in Norwegian) suffers from a large drop in content personalization (ranking) performance, compared to the counterpart trained on MIND (i.e., in English). 
%coupled with a small gain in diversity, due to the previously discussed inverse correlation between content-based recommendation and diversity (i.e., worse ranking implies more diverse recommendations). 
In contrast, transferring only the \texttt{A-Module}, i.e., training the \texttt{CR-Module} on MIND and the \texttt{A-Module} (for topics and sentiment) on Adressa), 
%trained on the source-language dataset with the \texttt{CR-Module} trained on the target-language dataset 
yields performance comparable to that of complete in-language training (i.e., both \texttt{CR-Module} and \texttt{A-Module} trained on MIND). 
%performs comparably to the \mannertitle{} trained completely on the target-language dataset. 
This is particularly the case for the sentiment \texttt{A-Module}, since the sentiment labels between the languages/datasets are more aligned than those for topical categories. 
%More importantly, we find that the more aligned the classes of an aspect $A_p$ are between the source- and the target-language datasets (i.e., as it is the case for the sentiment aspect), the more negligible are the differences in performance between the models using \texttt{A-Module} trained on the target-language and the source-language, respectively.  
%
%Moreover, our findings reveal similar patterns for single-aspect personalization.
% Specifically, replacing the \texttt{A-Module} trained on the target-language dataset with one trained on another source-language dataset exhibits competitive performance in terms of both content-based recommendation, as well as aspect-based personalization. In comparison to aspect diversification, in this setting, the aspect-based customization performance achieved by all the four model variants is nearly identical. 
%
This is encouraging, as it suggests 
%Overall, these results suggest 
that \manner{} has the ability to ``plug-in'' \texttt{A-Modules} trained on datasets in languages different that the target language, when the aspectual labels are not available for the target language. We highlight that the cross-lingual \texttt{A-Module} transfer is successful despite the fact that we transfer from a smaller dataset to a larger one (i.e., Adressa to MIND) and from a less resourced language to a more resourced one (i.e., Norwegian to English).
%, but also vice-versa. 
%
This suggests that, coupled with multilingual PLMs, \manner{} can be used for effective news recommendation in lower-resource languages, where training data and aspectual labels are scarce. Furthermore, the results suggest that the \texttt{A-Modules} could be trained on general-purpose classification datasets (e.g. topic or sentiment classification datasets). 
%, as long as there is a class overlap between the source and target setups.
