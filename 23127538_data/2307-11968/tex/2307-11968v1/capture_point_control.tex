To achieve locomotion over complex terrain, one approach is to track a reference capture point location, $\mathbf{\xi}_r$, which comes from Eq. \ref{eqn:com_trajectory}. 
This can be done through momentum shaping with an ICP based on LIPM dynamics:
\begin{equation}
\dot{\mathbf{l}}_d = m \left( \omega^2 \left( \mathbf{x} - \mathbf{r}_{\text{ecmp},d}\right) + \mathbf{g} \right),
\label{eqn:momentum_law}
\end{equation}
where $\dot{\mathbf{l}}_d$ that is the net linear momentum rate objective and $\mathbf{r}_{\text{ecmp},d}$ is the desired eCMP position from the controller. 
The use of the eCMP allows the robot to use both linear  and angular momentum for balance (the ``ankle" and ``hip" strategies, respectively) \cite{englsberger2017smooth}.
The challenge is to balance the use of these tasks. 
We also specifically want to decouple this from the step adjustment mechanism, such that step adjustment does its best to maintain balance, and ICP control does its best to maintain tracking.

At the highest level, we can slightly redefine a standard proportional ICP feedback controller \cite{hopkins2014humanoid, griffin2017walking} as
\begin{equation}
\mathbf{r}_{\text{ecmp},d} = \mathbf{k}_p \left( \mathbf{\xi} - \mathbf{\xi}_r \right) + \mathbf{r}_{\text{ecmp},r}, \ \ \ \mathbf{r}_{\text{ecmp},r} = \mathbf{r}_{\text{cop},r} + \mathbf{\kappa}_r,
\end{equation}
where $\mathbf{r}_{\text{cop},r}$ and $\mathbf{r}_{\text{ecmp},r}$ are the reference CoP and eCMP positions values from  Eq. \ref{eqn:com_trajectory} and $\kappa_r$ is the reference difference between the two. 
This feedback task can be written as
\begin{equation}
        \mathbf{\delta} + \mathbf{\kappa} = \mathbf{k}_p \mathbf{\xi}_e,
        \label{eqn:feedback_task}
\end{equation}
where $\delta$ and $\kappa$ encode the CoP and eCMP feedback, respectively, and $\mathbf{\xi}_e = \mathbf{\xi} - \mathbf{\xi}_r$.
This definition results in the optimal controller output, $\mathbf{r}_{\text{cop,d}} = \mathbf{r}_{\text{cop,r}} + \mathbf{\delta}^*$ and $\mathbf{r}_{\text{ecmp,d}} = \mathbf{r}_{\text{cop,d}} + \mathbf{\kappa}^*$.



%% Figure environment removed


 
While these feedback terms can be found directly from Eq. \ref{eqn:feedback_task}, this does not provide a mechanism to balance the use of the CoP and eCMP. 
To do so, we define a basic QP as:
\begin{equation}
\begin{aligned}
    \min_{\kappa, \delta} \quad &  \left\|  \mathbf{\delta} + \mathbf{\kappa} - \mathbf{k}_p \mathbf{\xi}_e \right\|_{Q_e} + \left\|  \left( \mathbf{\delta} + \mathbf{\kappa}\right)^T \mathbf{k}_p \mathbf{\xi}_e \right\|_{Q_\perp} + \\
    & \left\| \delta \right\|_{R_\delta} + \left\| \kappa - \kappa_r \right\|_{R_\kappa} + \left\|  \mathbf{\delta} + \mathbf{\kappa} - \mathbf{\delta}_p - \mathbf{\kappa}_p \right\|_{R_p} 
    \\
\text{s.t.} \quad & \mathbf{A}_{\text{foot}} \left( \mathbf{r}_{\text{cop,r}} + \delta \right) \le \mathbf{b}_{\text{foot}}, \\
& \kappa_{\text{min}} \le \kappa \le \kappa_{\text{max}}.
\label{eqn:qp}
\end{aligned}
\end{equation}
The $Q_e$ task tries to achieve the desired feedback magnitude, the $Q_\perp$ task forces the feedback to be along the appropriate direction, $R_\delta$ and $R_\kappa$ regularize the solution, and $R_p$ penalizes deviations from the previous solution $\delta_p$ and $\kappa_p$ for consistency.
The first constraint forces the output CoP to lie within the support polygon using a half-space formulation, while the second constraint bounds the  angular momentum.

% Figure environment removed

The output regulation $R_p$ is a distinct advantage of the QP-based approach. 
While feedback can be filtered using  direct rate limits or low-pass filters to the output on the feedback controller in Eq. \ref{eqn:feedback_task}, this prohibits rapid changes of force distribution for stability in the face of a sudden constraint change or disturbance.
Penalizing changes in feedback as a cost in the QP, instead, allows leveraging large feedback when necessary but avoiding small changes when not.

Once the desired eCMP is obtained, we can compute $\dot{\mathbf{l}}_d$ using Eq. \ref{eqn:momentum_law}. 
This is then supplied as a task to a whole-body controller along with other motion tasks \cite{Koolen_2016}.
In this case, a desired angular momentum rate is encoded entirely in the $\dot{\mathbf{l}}_d$ task and the other spatial acceleration tasks.
The whole-body controller balances the trade off between the necessary angular momentum rate to achieve $\dot{\mathbf{l}}_d$ and these other accelerations, allowing for perfect tracking if there are no redundant commands during nominal walking, and generating motions like pitching the torso and windmilling the arms when necessary to achieve $\dot{\mathbf{l}}_d$.

