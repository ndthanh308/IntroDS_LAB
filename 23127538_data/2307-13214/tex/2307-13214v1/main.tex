\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithm,tabularx} 
\usepackage{algorithmicx}
\usepackage{times}
\usepackage{array}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{varwidth}% http://ctan.org/pkg/varwidth
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{multirow} %tabllular
\usepackage{booktabs}
\usepackage{epsfig}
\usepackage{xcolor}
\usepackage[noend]{algpseudocode}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021


\begin{document}

\title{A Sample Article Using IEEEtran.cls\\ for IEEE Journals and Transactions}

\title{FedMEKT: Distillation-based Embedding Knowledge Transfer for Multimodal Federated Learning}
\author{~Huy~Q.~Le,~Minh~N.~H.~Nguyen,\IEEEmembership{~Member,~IEEE},~Chu~Myaet~Thwal, Yu Qiao, Chaoning Zhang, and Choong Seon Hong,\IEEEmembership{~Senior~Member,~IEEE}.
        % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}

        \IEEEcompsocitemizethanks{	
           \IEEEcompsocthanksitem Huy~Q.~Le,~Chu~Myaet~Thwal,~and~C.~S.~Hong are with the Department of Computer Science and Engineering, School of Computing, Kyung Hee University, Yongin-si 17104, South Korea. (email: \{quanghuy69, chumyaet, cshong\}@khu.ac.kr).
            \IEEEcompsocthanksitem Yu~Qiao and Chaoning Zhang is with the Department of Artificial Intelligence, School of Computing, Kyung Hee University, Yongin-si 17104, South Korea. (email: qiaoyu@khu.ac.kr; chaoningzhang1990@gmail.com).
           \IEEEcompsocthanksitem M.~N.~H.~Nguyen is with Digital Science and Technology Institute, The University of Danangâ€”Vietnam-Korea University of Information and Communication Technology, Da Nang, Vietnam (email: nhnminh@vku.udn.vn).
                                                        }
        }

% The paper headers
% \markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
% {Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
   Federated learning (FL) enables a decentralized machine learning paradigm for multiple clients to collaboratively train a generalized global model without sharing their private data. Most existing works simply propose typical FL systems for single-modal data, thus limiting its potential on exploiting valuable multimodal data for future personalized applications. Furthermore, the majority of FL approaches still rely on the labeled data at the client side, which is limited in real-world applications due to the inability of self-annotation from users. In light of these limitations, we propose a novel multimodal FL framework that employs a semi-supervised learning approach to leverage the representations from different modalities. %Specifically, we develop the multimodal embedding knowledge transfer mechanism in federated learning, namely, FedMEKT, which enables the fused multimodal knowledge exchange between server and clients using a small multimodal proxy dataset.
  Bringing this concept into a system, we develop a distillation-based multimodal embedding knowledge transfer mechanism, namely FedMEKT, which allows the server and clients to exchange the joint knowledge of their learning models extracted from a small multimodal proxy dataset.
   Our FedMEKT iteratively updates the generalized global encoders with the joint embedding knowledge from the participating clients. Thereby, to address the modality discrepancy and labeled data constraint in existing FL systems, our proposed FedMEKT comprises local multimodal autoencoder learning, generalized multimodal autoencoder construction, and generalized classifier learning. Through extensive experiments on three multimodal human activity recognition datasets, we demonstrate that FedMEKT achieves superior global encoder performance on linear evaluation and guarantees user privacy for personal data and model parameters while demanding less communication cost than other baselines. %Moreover, FedMEKT demands far less communication overhead compared to other parameter-aggregation baselines in the training process. 
\end{abstract}

\begin{IEEEkeywords}
Multimodal learning, Federated learning, Representation learning, Semi-supervised learning.
\end{IEEEkeywords}

\section{Introduction}

With the rapid emergence of technologies, artificial intelligence (AI) has made significant progress in a variety of applications such as virtual assistants, e-commerce, healthcare, and recommendation systems~\cite{pawar2020explainable,lugano2017virtual,rahayu2022systematic,bawack2022artificial}, which in turn demand a massive amount of personal data from end-users. Consequently, data privacy and security become great barriers in centralized machine learning systems where the server collects personal data and trains a generic model for data-intensive applications~\cite{liu2021machine}. To tackle these challenges, federated learning (FL) has been proposed as a decentralized machine learning paradigm that aggregates model parameters from multiple clients for collaborative training without sharing their private data, thus protecting sensitive user information \cite{mcmahan2017communication,li2020federated,kairouz2021advances}. It has been an engaging and demanding research area, demonstrating promising results in various domains like smart cities~\cite{zheng2022applications,ramu2022federated,pandya2023federated} and healthcare~\cite{xu2021federated,antunes2022federated,nguyen2022federated}.

Despite many advantages in preserving privacy, existing FL methods consider a scenario where clients hold only single-modal data~\cite{mcmahan2017communication,li2020federated}, restricting the use of \textit{multimodal data} in various equipment. Recent works on deep multimodal learning demonstrate that common features from multimodal data assure more accurate and robust performance than single-modal data in many applications, such as text and image for language translation \cite{rajendran2015bridge}, audio and video for emotion recognition~\cite{liang2018computational}, and different sensor data in healthcare \cite{garcia2018mental}. As a result, multimodal data provides more generalized capability to design an FL framework that uses multimodal client data generated from multiple sources and devices, such as smartphones and smartwatches. To leverage the benefits of multimodal data for decentralized machine learning systems, there have been some prior works in multimodal FL. The authors in~\cite{xiong2022unified} designed the co-attention layer to fuse representations from different modalities and obtain the generalized features to train personalized models. This method requires all users to have labeled data from all modalities, which means that users have to annotate the data. However, in the real world, this could be a cost hindrance for users to collect the labeled data from different modalities, such as various types of sensor data~\cite{alonso2015challenges}. \textit{Label annotation} poses a significant challenge in real-world applications, particularly in healthcare, where certain sites may lack the resources or incentives for extensive data labeling. One possible solution for saving the annotation cost is to design the multimodal FL under the semi-supervised setting where clients own private unlabeled data, and the server holds labeled data for the global downstream tasks. To deal with the labeled data constraint of local clients in multimodal FL,~\cite{zhao2022multimodal} proposed a framework that works under the semi-supervised setting using multiple autoencoders for different modalities. This work applies the mechanism of the conventional FedAvg framework~\cite{mcmahan2017communication} to construct the global multimodal encoders for supervised downstream tasks by aggregating model parameters from local autoencoders. However, these methods rely on aggregating model parameters from skewed private data on the server using an averaging scheme adopted from FedAvg \cite{mcmahan2017communication}. This can lead to a decrease in the generalization ability of the global model due to statistical heterogeneity \cite{li2019fedmd, deng2020adaptive} and require a large \textit{communication overhead}. To handle the communication cost in multimodal FL, we develop a knowledge transfer mechanism that operates on both sides of the system, leveraging the joint knowledge of the learning models rather than relying on model parameters.

In this paper, we design a novel multimodal FL framework under a semi-supervised setting to tackle the limitations of existing multimodal FL works \cite{zhao2022multimodal,xiong2022unified} and resolve the labeled data constraint for FL clients while saving communication overhead. Thus, we propose FedMEKT, a novel semi-supervised learning-based multimodal embedding knowledge transfer mechanism that exploits joint multimodal knowledge from unlabeled data of participating clients using proxy data to achieve the generalized encoder. Acknowledging CreamFL~\cite{yu2023multimodal} as the first knowledge distillation-based multimodal federated learning (FL) framework, we distinguish our approach in several ways. Firstly, our approach operates in a semi-supervised setting, while CreamFL utilizes a supervised setting. Secondly, we propose a knowledge transfer scheme that leverages \textit{joint embedding knowledge} and performs distillation based on this joint representation. In contrast, CreamFL adopts a contrastive-based method for representation aggregation and incorporates separate representations for each modality during the local training process. These differences highlight the unique characteristics of our approach over CreamFL. To this end, we construct the FedMEKT algorithm for multimodal FL by utilizing the split multimodal autoencoder backbone from~\cite{ngiam2011multimodal}. This facilitates communication between the server and clients by leveraging a small multimodal proxy dataset. Inspired by the effectiveness of joint embeddings generated from multimodal data, we raise the following research question: \textit{``how to effectively obtain joint embeddings for knowledge transfer between the server and multimodal heterogeneous clients?"} 


To answer this question, we introduce a fusion layer to combine the knowledge generated from different modalities into joint embedding knowledge. Moreover, instead of updating the global model by aggregating local model parameters, our FedMEKT updates the generalized autoencoder via the joint embedding knowledge from participating clients. %Note that different from previous works \cite{liang2020think, liu2020federated} that generate the representation from private raw data, which may violate the privacy preservation in FL, our work adopts a multimodal proxy dataset to extract the personal knowledge that could not be reconstructed back to the original data, thus ensuring the privacy for local clients.
Unlike previous works such as \cite{liang2020think, liu2020federated}, which generate representations directly from private raw data and may compromise privacy in FL, our approach leverages a multimodal proxy dataset. By extracting personal knowledge that cannot be reverse-engineered to reconstruct the original data, our method ensures privacy for local clients while preserving the efficacy of knowledge transfer. Through extensive experiments, we demonstrate that our proposed scheme can achieve a significant performance gain and save communication costs compared to other methods on supervised tasks. Our main contributions are:
\begin{itemize}
     \item %We propose a knowledge transfer method for multimodal FL that can be specified as upstream and downstream mechanisms, and develop a novel multimodal FL framework FedMEKT that allows joint embedding knowledge exchange between the server and clients.
     We propose a novel upstream and downstream knowledge transfer method for multimodal FL, called FedMEKT. This method %incorporates upstream and downstream mechanisms, allowing for
     enables the exchange of joint multimodal embedding knowledge between the server and clients via a small proxy data %We propose %the downstream and upstream multimodal embedding knowledge transfer mechanisms between server and clients 
    considering the following problems: 1) \textit{local multimodal autoencoder learning} to update local multimodal encoders with the global multimodal knowledge received from the server, 2) \textit{generalized multimodal autoencoder construction} to aggregate the knowledge sent from heterogeneous client encoders to global encoders, and 3) \textit{generalized classifier learning} to train the classifier for global downstream tasks. 
    \item To facilitate the aggregation of joint multimodal embedding knowledge, we employ a fusion layer and establish knowledge exchange between the server and clients, without the need for parameter exchange.
    \item We validate the proposed method by conducting extensive experiments over three multimodal human activity recognition datasets. Our scheme achieves superior performance in global downstream tasks while requiring far less communication overhead than other baselines.

\end{itemize}
%------------------------------------------------------------------------

% Figure environment removed

\section{Related Work}

\paragraph{Semi-supervised Learning}  Semi-supervised learning (SSL) has been applied in various machine learning tasks to leverage the unlabeled data for solving the labeling cost issue, considering a scenario where the system holds both unlabeled and labeled data \cite{zhu2009introduction}. Pseudo-labeling \cite{lee2013pseudo} has become one of the popular trends in SSL, which generates the pseudo-label for unlabeled dataset and compute the loss function based on the sum of original and pseudo-label loss functions. The combination of pseudo-label and consistency regularization has been widely applied in SSL with many state-of-the-art methods, such as UDA \cite{xie2020unsupervised}, FixMatch~\cite{sohn2020fixmatch}, and MixMatch~\cite{berthelot2019mixmatch}. In our work, we consider a semi-supervised scenario in FL setting where private unlabeled data is provided by users, and labeled data is utilized in training classifiers to perform the downstream tasks. 

\paragraph{Federated Learning based Knowledge Distillation}  Knowledge distillation (KD) \cite{bucilua2006model,ba2013deep} has become a promising technique that helps FL to solve heterogeneity issues. KD generally provides knowledge communication between global and local models instead of exchanging model parameters. The authors in \cite{jeong2018communication,nguyen2022cdkt} applied KD in FL to minimize the communication overhead by using the distillation regularizer between student and teacher logits. The clients update their local models to conduct the averaged global predictions on the server. FedDF \cite{lin2020ensemble} first obtains a global model by aggregating the local model parameters and then updates it again by performing ensemble distillation from all client teacher models. Unlike FedDF, which still uses the model parameters exchange between the clients and the server, KT-pFL \cite{zhang2021parameterized} formulates the personalized knowledge transfer for personalized FL leveraging the proxy data to update the local soft predictions. However, most of these schemes apply KD in traditional FL with single-modal labeled data while we integrate the knowledge transfer scheme with the multimodal FL in this work.

\paragraph{Multimodal Learning}  Multimodal learning has attracted lots of attention in recent years. The multimodal deep learning systems enable leveraging data from multiple modalities, such as image \cite{bain2021frozen,ye2019cross}, video \cite{nagrani2021attention,bain2021frozen}, sensors \cite{islam2022mumu,caesar2020nuscenes}, etc., hence providing better performance than single-modal data. One of the first designs in multimodal deep learning was fusion \cite{potamianos2003recent,wollmer2010context} that fuses representations from different layers of multiple modalities in various ways, such as concatenation, multiplication, or weighted sum. However, these works still face misalignment in different fusion levels. In recent years, researchers have proposed different model architectures for multimodal applications such as co-attention \cite{lu2016hierarchical} for VQA tasks \cite{kumar2020mcqa}, and various types of transformers for language-video tasks~\cite{sun2019videobert}. In terms of the encoder-decoder framework, \cite{ngiam2011multimodal} proposed the autoencoders for audio and visual data. Another popular approach is DCCA \cite{andrew2013deep}, which is a combination of canonical correlation analysis (CCA) and autoencoders to fuse multimodal representations in the feature subspace. 

Emerging multimodal FL works \cite{xiong2022unified,zhao2022multimodal} motivate us to initiate the novel design of a knowledge transfer scheme in multimodal FL. In this work, the generalized encoders could alternatively be built from personalized encoders by the joint multimodal embedding knowledge transfer. As a result, the well-generalized encoder helps to strengthen the performance of downstream supervised tasks. In the following section, we present our proposed Federated Multimodal Embedding Knowledge Transfer scheme and algorithm.
%------------------------------------------------------------------------
% Figure environment removed
\section{FedMEKT: Federated Multimodal Embedding Knowledge Transfer}
\subsection{Preliminary}
A recent multimodal FedAvg scheme \cite{zhao2022multimodal} exploits multimodal data in FL by simply aggregating the parameters from local autoencoders trained on different data modalities. After the local training process with a private unlabeled dataset $D_k =\{\textbf{x}^M_k\}$ in client $k$, the global autoencoder is updated based on the aggregated model parameters from both unimodal and multimodal client models. The multimodal client models are given more weights than unimodal client models to align the representations from different modalities. Given two modalities $A$ and $B$, the corresponding split autoencoders, $(f_{A},g_{A})$ and $(f_{B},g_{B})$, are used to minimize the distance between the original input data and the reconstructed data, which is referred as the reconstruction loss. Therefore, the goal of MM-FedAvg is to learn the split global autoencoder model that minimizes the total loss function over the entire unlabeled dataset from $K$ number of clients. Using the similar setting in this work, we denote $n_A=\sum_{k\in{m_{A}, m_{AB}}}{n_k}$,  $n_B=\sum_{k\in{m_{B}, m_{AB}}}{n_k}$ as the total number of data samples for the modality $A$ and $B$, respectively, where $m$ denotes the set of clients in each modality (i.e., client with modality $m_{AB}$ holds both data from modality $A$ and $B$), and $n_k$ is the number of data samples for client $k$. We consider clients hold multimodal data, such as different types of data (e.g., accelerometer data Acce and gyroscope data Gyro). The split autoencoder model is defined with $f$ and $g$ as the encoder and decoder for modality $M$, respectively. In particular, learning objective functions for the global autoencoders $A$ and $B$ can be defined as:

\begin{equation}
\begin{split}
     \min_{f_{A},g_{A}}{\mathcal{L}}_{s}(f_{A},g_{A})= \sum_{k\in{m_{A}}}{\frac{n_{k}}{n_{A}}}&{\mathcal{L}}_{k}(f_{A},g_{A}) \\
     &+\alpha \, \sum_{k\in{m_{AB}}}{\frac{n_{k}}{n_{A}}}{\mathcal{L}}_{k}(f_{A},g_{A}),  
    % \text{where}  {\ell}^{}_{k}(f_{A},g_{A})=\min_{f_{A},g_{A}}{\ell}^{}_{A}(x_A,x_A') +{\ell}_{A}(x_B, x_B')
\end{split}
\end{equation}

\begin{equation}
\begin{split}
       \min_{f_{B},g_{B}}{\mathcal{L}}^{}_{s}(f_{B},g_{B})= \sum_{k\in{m_{B}}}{\frac{n_{k}}{n_{B}}}&{\mathcal{L}}_{k}(f_{B},g_{B}) \\
       &+\alpha \, \sum_{k\in{m_{AB}}}{\frac{n_{k}}{n_{B}}}{\mathcal{L}}_{k}(f_{B},g_{B}),  
    % \text{where}  {\ell}^{}_{k}(f_{A},g_{A})=\min_{f_{A},g_{A}}{\ell}^{}_{A}(x_A,x_A') +{\ell}_{A}(x_B, x_B')
\end{split}
\end{equation}
%
where ${\mathcal{L}}^{}_{k}(f_{A},g_{A})$ and ${\mathcal{L}}^{}_{k}(f_{B},g_{B})$ are loss functions of split autoencoder for each modality $A$ and $B$. Specifically, loss functions at client $k$ with two modalities can be defined as:

\begin{equation}
    {\mathcal{L}}^{}_{k}(f_{A},g_{A})= \min_{f_{A},g_{A}}{\ell}^{}_{A}(x^A,x^{A'}) +{\ell}_{B}(x^B, x^{B'}), 
\end{equation}

\begin{equation}
    {\mathcal{L}}^{}_{k}(f_{B},g_{B})= \min_{f_{B},g_{B}}{\ell}^{}_{A}(x^A,x^{A'}) +{\ell}_{B}(x^B, x^{B'}),
\end{equation}
%
where $\ell_A$ and $\ell_B$ are reconstruction losses \cite{zhao2022multimodal} (e.g., MSE loss) while $x^{A'}$ and $x^{B'}$ denote the reconstructed outputs of two modalities A and B, respectively.

Different from existing multimodal FL methods, which aggregate the parameters from clients in each communication round to train the global model, we deploy the multimodal embedding knowledge transfer mechanism to transfer the local embedding knowledge from all multimodal clients to collectively build the generalized global encoder model $f_M$ which can generate more powerful representations for downstream tasks. In this work, we consider each client $k$ has either a multimodal dataset $D_k=\{\textbf{x}^A_k,\textbf{x}^B_k\}$ or a unimodal dataset $D_k=\{\textbf{x}^A_k\}$ or $D_k=\{\textbf{x}^B_k\}$, where multimodal clients own different types of sensor data (e.g., accelerometer data Acce and gyroscope data Gyro) or sensor and visual data at the same time (e.g., RGB images and accelerometer data Acce). Then, we create a small unlabeled proxy dataset $D_r=\{\textbf{x}^A_r,\textbf{x}^B_r\}$ that all clients can access and provide their embedding knowledge, as shown in Fig.~\ref{systemmodel}. The global encoder model shares its generalized joint embedding knowledge with all clients by upstream transfer and receives local joint embedding knowledge from clients through a downstream transfer mechanism using proxy data. We assume this proxy dataset has a small size and contains unlabeled data from all modalities that the system can collect easily. Proxy data could be collected from the service provider which is out-of-distribution from all clients' local data, and does not capture the full statistics of local data. It works as the bridge to deliver the knowledge from clients to servers and vice versa. In this design, the FL framework could guarantee better data privacy and protect model parameters compared to schemes that require parameter exchange between the server and clients. Fig.~\ref{systemmodel} shows an overview of our FedMEKT framework on UR Fall Detection dataset~\cite{kwolek2014human}. Primarily, we design the embedding knowledge transfer in multimodal FL for FedMEKT by formulating three general problems: \textit{local multimodal autoencoder learning}, \textit{generalized multimodal autoencoder construction},  and \textit{generalized classifier learning} in the following subsections. An illustration of our proposed FedMEKT scheme is shown in Fig.~\ref{system_training_step}.


\subsection{Local Multimodal Autoencoder Learning}
Our architecture aims to address the challenge of modality discrepancy among multiple clients on the local side. Since each client may have different modalities, designing a framework that can effectively learn the common features from multimodal data across all clients is crucial. To achieve this, we design the distillation-based multimodal knowledge transfer mechanism for local multimodal autoencoder learning on the client side, enabling the local models to improve the generalization capabilities by mimicking the global embedding knowledge $e^{h}_{s}$ received from the server during downstream multimodal embedding knowledge transfer. The embedding knowledge is generated from the encoder $f$ of modality $M$  (i.e., $e^M=f(x_r^M)$), and it could be extracted from different hidden layers of the encoder (i.e., $e^{Mh}$), where $h$ is the number of hidden layers of the encoder for the modality $M$. The embedding knowledge from modalities A and B are $e^{Ah}_k$ and $e^{Bh}_k$, respectively. To obtain the \textit{joint embedding knowledge}, we design the fusion layer to concatenate the knowledge extracted from two modalities A and B, i.e., $e^h_k=[e^{Ah}_k\mathbin\Vert e^{Bh}_k]$. The fusion layer has the same dimensions as the hidden layers of the encoder to store the joint embedding knowledge. In the local learning problem, the client models update depending on private unlabeled data $D_k$ with the local reconstruction loss and proxy data $D_r$ with embedding knowledge transfer loss ${\ell}_{EKD}$. In particular, we construct the local multimodal loss function for each device $k$ on both modalities A and B as follows:
\begin{algorithm}[t]
\footnotesize
\caption{FedMEKT Algorithm} 
\label{mainalg}
\begin{algorithmic}[1]
    \State \textbf{Input:} $T, R, N, P  $, $\eta_1$, $\eta_2$, $\eta_3$
    \For{ $t=0,\dots,T-1$}
\State -- \textit{\textbf{Client Execution}} --
    % \State \begin{varwidth}{\linewidth}
    
    \State \textbf{Local Multimodal Autoencoder Learning:}
    Clients receive  global joint embedding knowledge ${e}^{h}_{s}$ from the server 
    % \For{device $n=1,\dots,N  $}
    \For { $n=0,\dots,N-1$}
        % \State \begin{varwidth}[t]{0.85\linewidth}
        % We loop for each batch of private and proxy data (i.e., $S_k$ and $S_r$): \end{varwidth}
        \For {$S_k$ $\subseteq$~$D_k$, $S_r$ $\subseteq$~$D_r$ } 
            \begin{flalign*}
                &&&&&&{w}^{t}_{k} = &{w}^{t}_{k}-\eta_1\nabla \mathcal{L}_c^k  &&\text{\Comment{local model parameters ${w}_{k}$ update }}
            \end{flalign*}
         \State $\mathcal{L}_c^k$ is computed via Eq.~\ref{eq:totalloss-local} %\Comment{Local Autoencoder Update}
    \EndFor
    \EndFor
    \State \begin{varwidth}[t]{1.\linewidth}
    \end{varwidth}
    Device $k$ sends the joint embedding knowledge $e^{h}_{k}$ to the server using proxy data $D_r$ 
    % \end{varwidth}
    % \EndFor
    \State -- \textit{\textbf{Server Execution}} --
    %   \State \begin{varwidth}{\linewidth}
    \State \textbf{Generalized Multimodal Autoencoder Construction:} %Server distributes $e^h_s$ to all clients
    \For { $r=0,\dots,R-1$}
      \For {$S_r$ $\subseteq$~$D_r$ } 
      \begin{align*}
       &&&&&&{w}^{t}_{s}= &{w}^{t}_{s}-\eta_2\nabla\mathcal{L}_s  && \text{\Comment{global model parameters $w_s$ update}}
      \end{align*}
      \State $\mathcal{L}_s$ is computed via Eq.~\ref{eq:totalloss-server} %\Comment{Global Autoencoder Update}
     \EndFor
     \EndFor
    \State \textbf{Generalized Classifier Learning:} Global classifier receives $h_s^M$ from global encoders $\forall{M\in\{A,B\}}$
    \For { $p=0,\dots,P-1$}
      \For {$S_L$ $\subseteq$~$D_L$ } 
        \begin{align*}
              &&&&&&{w}^{t,M}_{gc}= &{w}^{t,M}_{gc}-\eta_3\nabla\mathcal{L}_{gc}   &&\text{\Comment{global classifier $w_{gc}$ update}}
        \end{align*}
      \State $\mathcal{L}_{gc}$ is computed via Eq.~\ref{eq:globalclassifier-mmEKT}%\Comment{Global Classifier Update} 
      \EndFor
    \EndFor 
    \EndFor
\end{algorithmic}
\end{algorithm}	
% \begin{equation}
%     {\ell}^{k}_{A}= {\ell}_{A}(x^A_k,x^{A'}_k|D_k) +{\ell}_{B}(x^B_k,x^{B'}_k|D_k)+\alpha \, {\ell}_{EKD}(e^{h}_{k},e^{h}_{s}|D_r)
%      \label{eq:ondeviceA-mmEKT}
% \end{equation}

\begin{equation}
\begin{split}
        {\mathcal{L}}^{k}_{A}= {\ell}_{A}(x^A_k,g_A(h_A)|D_k) &+{\ell}_{B}(x^B_k,g_B(h_A)|D_k) \\
        &+\gamma \, {\ell}_{EKD}(e^{h}_{k},e^{h}_{s}|D_r),
     \label{eq:ondeviceA-mmEKT}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
  {\mathcal{L}}^{k}_{B}= {\ell}_{A}(x^A_k,g_A(h_B)|D_k) &+{\ell}_{B}(x^B_k,g_B(h_B)|D_k) \\
  &+\gamma \, {\ell}_{EKD}(e^{h}_{k},e^{h}_{s}|D_r),
     \label{eq:ondeviceB-mmEKT}
\end{split}
\end{equation}
%
where $h_A=f_A(x^A_k)$, $h_B=f_B(x^B_k)$, and $\gamma$ is the parameter to manipulate the trade-off between the local reconstruction loss and the embedding knowledge transfer regularizer. Hence, the $\ell_{EKD}$ denotes the joint embedding knowledge transfer regularizer (e.g., Kullback-Leibler (KL) Divergence loss) for both modalities A and B. By mimicking the generalized encoder via minimizing the $\ell_{EKD}$, the local model could enhance the generalization of local multimodal encoder models and avoid biases when training on the skewed private dataset. If client $k$ holds the unimodal data of modality A or B, it is only necessary to compute the autoencoder loss function for that particular modality. The overall training loss $\mathcal{L}^k_c$ for the autoencoders from two modalities for each client $k$ is given in Eq.~\ref{eq:totalloss-local}. Different from MM-FedAvg \cite{zhao2022multimodal} method that updates the autoencoder of modality A and B separately, we apply the synchronous update for the entire autoencoder from both modalities. 
\begin{equation}
     \mathcal{L}^k_c=\mathcal{L}^k_A+\mathcal{L}^k_B.
     \label{eq:totalloss-local}
\end{equation}



\subsection{Generalized Multimodal Autoencoder Construction}
% To solve the generalized multimodal autoencoder construction problem, we utilize the proxy data $D_r$ to collect the embedding knowledge from all multimodal devices to build the global autoencoder model. On the server, we conduct the modality-averaging mechanism of the local embedding knowledge based on the modality label. Accordingly, we gather the embedding knowledge from same modality of all clients and then perform the averaging operation to obtain the collective knowledge of each modality for the global model. Subsequently, we design the generalized multimodal autoencoder construction problem for the split autoencoder with two modalities using embedding knowledge distillation (EKD) \cite{}method as follow:
%  We define that the embedding knowledge is generated from the encoder $f$ of modality $M$  (i.e., $e^M=f(x_r^M)$) and it could be extracted from different hidden layers of the encoder (i.e., $e^{M_h}$), where $h$ is the number of hidden layers of the encoder for the modality $M$.
On the server side, we target to perform joint knowledge aggregation in order to capture the biased features of the local models and learn a generalized global encoder for downstream tasks. To solve the generalized multimodal autoencoder construction problem, we utilize the proxy data $D_r$ to generate the global multimodal embedding knowledge from the current global model and collect the multimodal embedding knowledge from local clients to perform the upstream multimodal embedding knowledge transfer. Firstly, we concatenate the global embedding knowledge from different modalities to achieve the joint global embedding knowledge $e^h_s$. %For the local embedding knowledge, we first conduct the modality-averaging mechanism of the local embedding knowledge based on the modality label. 
Accordingly, we gather the embedding knowledge from the same modality of all clients and then perform the averaging operation to obtain the collaborative embedding knowledge of each modality, which are $\sum_{k\in{K}}{\frac{1}{K}}e^{Ah}_{k}$ and $\sum_{k\in{K}}{\frac{1}{K}}e^{Bh}_{k}$. Hence, similar to the client side, we concatenate the \textit{collaborative embedding knowledge} from two modalities, A and B, by using the fusion layer and achieve the joint local embedding knowledge, which is $\sum_{k\in{K}}{\frac{1}{K}}e^{h}_{k}=[\sum_{k\in{K}}{\frac{1}{K}}e^{Ah}_{k}\mathbin\Vert \sum_{k\in{K}}{\frac{1}{K}}e^{Bh}_{k}]$. The global autoencoder model is updated with reconstruction loss and global embedding knowledge transfer loss using proxy data $D_r$. Subsequently, we design the generalized multimodal learning loss for two modalities as follows:
 

\begin{equation}
\begin{split}
       {\mathcal{L}}^s_{A}= {\ell}_{A}(x^A_r,g_A(h_A)|D_r) & +{\ell}_{B}(x^B_r,g_B(h_A)|D_r) \\
       &+\beta \, {\ell}_{EKD}\bigg(e^{h}_{s},\sum_{k\in{K}}{\frac{1}{K}}e^{h}_{k}|D_r\bigg),
       \label{eq:globalA-mmEKT}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
    {\mathcal{L}}^s_{B}= {\ell}_{A}(x^A_r,g_A(h_B)|D_r)& +{\ell}_{B}(x^B_r,g_B(h_B)|D_r) \\
    &+\beta \, {\ell}_{EKD}\bigg(e^{h}_{s},\sum_{k\in{K}}{\frac{1}{K}}e^{h}_{k}|D_r\bigg),
        \label{eq:globalB-mmEKT}
\end{split}
\end{equation}
%
where $h_A=f_A(x^A_r)$, $h_B=f_B(x^B_r)$, and $\beta$ is the parameter to control the trade-off between reconstruction loss of proxy data and embedding knowledge transfer regularizer. The embedding knowledge transfer regularizer (i.e., $\ell_{EKD}$) attempts to close the gap between the multimodal embedding knowledge of the server and joint embedding knowledge from multimodal clients.  Same as the client side, we leverage the synchronous update for the generalized multimodal autoencoder model. The total loss for global multimodal autoencoder construction is denoted as follows: 
\begin{equation}
     \mathcal{L}_s=\mathcal{L}^s_A+\mathcal{L}^s_B.
     \label{eq:totalloss-server}
\end{equation}

\subsection{Generalized Classifier Learning}
On the server, we attach the global classifier $\phi_{gc}$ to generalized encoder part of the global autoencoder model to perform the supervised learning task by using multimodal labeled dataset $D_L$. The classifier training process helps to update solely the generalized classifier for classification downstream tasks. Hence, we use cross-entropy loss to learn the generalized multimodal classifier:

\begin{equation}
    {\mathcal{L}}_{gc}= {\mathcal{L}}_{CE}(z^M_s|D_L), \; \forall{M\in\{A,B\}}, 
    \label{eq:globalclassifier-mmEKT}
\end{equation}
%
where ${\mathcal{L}}_{CE}$ is the cross-entropy loss and $h^{M}_{s}=f_{M}(x^{M}_L)$, $z^M_s=\phi_{gc}(h^{M}_{s})$ are the representations from the global encoder and the outcome of the global classifier, respectively. 
\subsection{FedMEKT Algorithm}
Turning this multimodal FL scheme into reality, we develop the FedMEKT algorithm (Alg.~\ref{mainalg}) to perform the embedding knowledge transfer between the server and multimodal clients. At the beginning of each communication round, the server randomly selects a subset of clients from the total of $K$ multimodal clients to participate in the local training, and the global autoencoder model broadcasts the generalized embedding knowledge to all selected clients. Each client performs $N$ local training steps with its private multimodal data and proxy dataset in the local knowledge transfer problem~(\ref{eq:totalloss-local}) and outputs the local embedding knowledge using proxy data $D_r$, then sends it to the server. The generalized autoencoder model is constructed on the server side by solving the generalized multimodal learning problem~(\ref{eq:totalloss-server}). Then, the server uses the updated global encoder to extract the multimodal representations of input data in the labeled dataset $D_L$ to train the classifier for the supervised learning task (\ref{eq:globalclassifier-mmEKT}). 
% \begin{table*}[]
% \footnotesize
% \begin{center}
% \begin{tabular}{|c|lc|c|c|c|c|c|}
% \hline
% Dataset                & \multicolumn{2}{c|}{Modality}                                                                      & MM-FedAvg~\cite{zhao2022multimodal} & MM-FedProx~\cite{li2020federated} & MM-MOON~\cite{li2021model} & FedMEKT-S & FedMEKT-C       \\ \hline 
% \multirow{6}{*}{mHealth~\cite{banos2014mhealthdroid}} & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Acce-Gyro\end{tabular}}}  & Acce  & 63.83     & 67.86      & 64.47   & 64.13   & \textbf{68.28} \\ \cline{3-3} 
%                          & \multicolumn{1}{l|}{}                                                                      & Gyro  & 63.62     & 64.36      & 64.12   & 62.10   & \textbf{64.60} \\ \cline{2-3} 
%                          & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Acce-Mage\end{tabular}}}  & Acce  & 69.99     & 70.00      & 69.71   & 70.15   & \textbf{71.16} \\ \cline{3-3} 
%                          & \multicolumn{1}{l|}{}                                                                      & Mage  & 68.49     & 69.21      & 68.79   & 70.12   & \textbf{71.13} \\ \cline{2-3} 
%                          & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Gyro-Mage\end{tabular}}}  & Gyro  & 65.43     & 65.37      & 65.90    & 64.47   & \textbf{67.10} \\ \cline{3-3} 
%                          & \multicolumn{1}{l|}{}                                                                      & Mage  & 68.28     & 68.75      & 68.13   & 67.82   & \textbf{69.02} \\ \hline
% \multirow{6}{*}{UR Fall~\cite{kwolek2014human}} & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Acce-RGB\end{tabular}}}   & Acce  & 61.70     & 65.66      & 65.89   & 69.32   & \textbf{70.66} \\ \cline{3-3} 
%                          & \multicolumn{1}{l|}{}                                                                      & RGB   & 57.88     & 59.26      & 62.00   & 60.21   & \textbf{66.70} \\ \cline{2-3} 
%                          & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Acce-Depth\end{tabular}}} & Acce  & 67.24     & 68.79      & 68.08   & 69.25   & \textbf{72.68} \\ \cline{3-3} 
%                          & \multicolumn{1}{l|}{}                                                                      & Depth & 60.76     & 68.08      & 68.16   & 65.85   & \textbf{75.22} \\ \cline{2-3} 
%                          & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}RGB-Depth\end{tabular}}}  & RGB   & 69.88     & 75.60      & 73.38   & 73.81   & \textbf{77.87} \\ \cline{3-3} 
%                          & \multicolumn{1}{l|}{}                                                                      & Depth & 67.61     & 70.04      & 66.48   & 70.33   & \textbf{70.57} \\ \hline
% \multirow{2}{*}{Opp~\cite{chavarriaga2013opportunity}}     & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Acce-Gyro\end{tabular}}}  & Acce  & 71.75     & 72.25      & 72.96   & 72.03   & \textbf{73.50} \\ \cline{3-3} 
%                          & \multicolumn{1}{l|}{}                                                                      & Gyro  & 72.08     & 71.43      & 72.09   & 72.09   & \textbf{72.15} \\ \hline
% \end{tabular}
% \end{center}
% \caption{The comparison of average accuracy on different datasets with multimodal clients.  }
% \label{multimodal_table}
% \end{table*}





\section{Experimental Results}
% \subsection{Experiment Details}
% \subsubsection{Dataset}

\begin{table}[h]
\caption{Detail Architecture.}
\begin{center}
\begin{tabular}{ccccc}
\hline
\textbf{Dataset} & \textbf{Modality}                                          & \textbf{X size}                                     & \textbf{$h_1$ size} & \textbf{$h_2$ size} \\ \hline
mHealth~\cite{banos2014mhealthdroid}          & \begin{tabular}[c]{@{}c@{}}Acce\\ Gyro\\ Mage\end{tabular} & \begin{tabular}[c]{@{}c@{}}9\\ 6\\ 6\end{tabular}   & 4                & 24               \\ \hline
UR Fall~\cite{kwolek2014human}          & \begin{tabular}[c]{@{}c@{}}Acce\\ RGB\\ Depth\end{tabular} & \begin{tabular}[c]{@{}c@{}}3\\ 512\\ 8\end{tabular} & 2,4              & 32               \\ \hline
Opp~\cite{chavarriaga2013opportunity}              & \begin{tabular}[c]{@{}c@{}}Acce\\ Gyro\end{tabular}        & \begin{tabular}[c]{@{}c@{}}24\\ 15\end{tabular}     & 10               & 24               \\ \hline
\end{tabular}

\label{detai_model}
\end{center}
\end{table}

\begin{table*}[h]
\caption{Overall information of the multimodal autoencoder architecture used in our experiments.}
\begin{center}
\begin{tabular}{@{}ll|lll@{}}
\toprule
\multicolumn{2}{c|}{\multirow{2}{*}{\textbf{Components}}} & \multicolumn{3}{c}{\multirow{2}{*}{\textbf{Layer Detail}}} \\
\multicolumn{2}{c|}{}                                     & \multicolumn{3}{c}{}                                             \\ \midrule
\multicolumn{2}{l|}{\multirow{2}{*}{Encoder A}}           & \multicolumn{3}{l}{(lstm): LSTM(input\_layer A, hidden\_layer 1A, batch\_first=True)}       \\
\multicolumn{2}{l|}{}                                     & \multicolumn{3}{l}{(lstm2): LSTM(hidden\_layer 1A, hidden\_layer 2A, batch\_first=True)}      \\ \midrule
\multicolumn{2}{l|}{\multirow{2}{*}{Decoder A}}           & \multicolumn{3}{l}{(lstm): LSTM(hidden\_layer 2A, hidden\_layer 1A, batch\_first=True)}       \\
\multicolumn{2}{l|}{}                                     & \multicolumn{3}{l}{(lstm2): LSTM(hidden\_layer 1A, output\_layer A, batch\_first=True)}      \\ \midrule
\multicolumn{2}{l|}{\multirow{2}{*}{Encoder B}}           & \multicolumn{3}{l}{(lstm): LSTM(input\_layer B, hidden\_layer 1B, batch\_first=True)}     \\
\multicolumn{2}{l|}{}                                     & \multicolumn{3}{l}{(lstm2): LSTM(hidden\_layer 1B, hidden\_layer 2B, batch\_first=True)}      \\ \midrule
\multicolumn{2}{l|}{\multirow{2}{*}{Decoder B}}           & \multicolumn{3}{l}{(lstm): LSTM(hidden\_layer 2B, hidden\_layer 1B, batch\_first=True)}       \\
\multicolumn{2}{l|}{}                                     & \multicolumn{3}{l}{(lstm2): LSTM(hidden\_layer 1B, output\_layer B, batch\_first=True)} \\ \bottomrule  
\end{tabular} 
\label{table:model_architecture}
\end{center}
\end{table*}


\begin{table*}[t]
\caption{The comparison of average accuracy on different datasets with multimodal clients.  }
\begin{center}
\resizebox{17cm}{!}{%
\begin{tabular}{|c|cc|c|c|c|c|c|c|}
\hline
Datasets                 & \multicolumn{2}{c|}{Modality}                            & MM-FedAvg~\cite{zhao2022multimodal} & MM-FedProx~\cite{li2020federated} & MM-MOON~\cite{li2021model} & CreamFL~\cite{yu2023multimodal}         & FedMEKT-S & FedMEKT-C      \\ \hline
\multirow{6}{*}{mHealth~\cite{banos2014mhealthdroid}} & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Gyro}}  & Acce  & 63.83     & 67.86      & 64.47   & \textbf{68.58}          & 64.13     & 68.28 \\ 
                         & \multicolumn{1}{c|}{}                            & Gyro  & 63.62     & 64.36      & 64.12   & 64.34          & 62.10     & \textbf{64.60} \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Mage}}  & Acce  & 69.99     & 70.00      & 69.71   & 69.51          & 70.15     & \textbf{71.16} \\ 
                         & \multicolumn{1}{c|}{}                            & Mage  & 68.49     & 69.21      & 68.79   & 69.18          & 70.12     & \textbf{71.13} \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{Gyro-Mage}}  & Gyro  & 65.43     & 65.37      & 65.90   & 66.05          & 64.47     & \textbf{67.10} \\ 
                         & \multicolumn{1}{c|}{}                            & Mage  & 68.28     & 68.75      & 68.13   & 68.00          & 67.82     & \textbf{69.02} \\ \hline
\multirow{6}{*}{UR-Fall~\cite{kwolek2014human}} & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & 61.70     & 65.66      & 65.89   & 66.82          & 69.32     & \textbf{70.66} \\  
                         & \multicolumn{1}{c|}{}                            & RGB   & 57.88     & 59.26      & 62.00   & 62.34          & 60.21     & \textbf{66.70} \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & 67.24     & 68.79      & 68.08   & 71.61          & 69.25     & \textbf{72.68} \\  
                         & \multicolumn{1}{c|}{}                            & Depth & 60.76     & 68.08      & 68.16   & \textbf{75.33} & 65.85     & 75.22          \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & 69.88     & 75.60      & 73.38   & \textbf{78.57} & 73.81     & 77.87          \\ 
                         & \multicolumn{1}{c|}{}                            & Depth & 67.61     & 70.04      & 66.48   & 69.18          & 70.33     & \textbf{70.57} \\ \hline
\multirow{2}{*}{Opp~\cite{chavarriaga2013opportunity}}     & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Gyro}}  & Acce  & 71.75     & 72.25      & 72.96   & 73.33          & 72.03     & \textbf{73.50} \\ 
                         & \multicolumn{1}{c|}{}                            & Gyro  & 72.08     & 71.43      & 72.09   & 72.10          & 72.09     & \textbf{72.15} \\ \hline
\end{tabular}
}%
\end{center}
\label{multimodal_table}
\end{table*}


% \begin{table*}[]
% \footnotesize
% \begin{center}
% \begin{tabular}{|c|lc|c|c|c|c|c|}
% \hline
% Dataset                 & \multicolumn{2}{c|}{Modality}                                                                      & MM-FedAvg~\cite{zhao2022multimodal}      & MM-FedProx~\cite{li2020federated} & MM-MOON~\cite{li2021model} & FedMEKT-S      & FedMEKT-C      \\ \hline 
% \multirow{6}{*}{mHealth~\cite{banos2014mhealthdroid}} & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Acce-Gyro\end{tabular}}}  & Acce  & 64.19          & 65.27      & 67.97   & 64.32          & \textbf{68.37} \\ \cline{3-3} 
%                          & \multicolumn{1}{c|}{}                                                                      & Gyro  & 63.50          & 64.37      & 64.76   &64.06                & \textbf{65.19}      \\ \cline{2-3} 
%                          & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Acce-Mage\end{tabular}}}  & Acce  & 68.41 & 66.29      & 66.33   & 67.96          & \textbf{68.71}          \\ \cline{3-3} 
%                          & \multicolumn{1}{c|}{}                                                                      & Mage  & 68.88          & 67.76      & 68.67   & 69.11          & \textbf{70.68} \\ \cline{2-3} 
%                          & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Gyro-Mage\end{tabular}}}  & Gyro  & 65.57          & 66.30      & 64.65   & 65.09          & \textbf{67.38} \\ \cline{3-3} 
%                          & \multicolumn{1}{c|}{}                                                                      & Mage  & 68.14          & 68.59      & 68.50   & \textbf{69.68} & 69.03          \\ \hline
% \multirow{6}{*}{UR Fall~\cite{kwolek2014human}} & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Acce-RGB\end{tabular}}}   & Acce  & 73.60          & 69.63      & 64.61   & 67.69          & \textbf{73.92} \\ \cline{3-3} 
%                          & \multicolumn{1}{c|}{}                                                                      & RGB   & 61.61          & 62.45      & 64.66   & 60.42          & \textbf{68.18} \\ \cline{2-3} 
%                          & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Acce-Depth\end{tabular}}} & Acce  & 62.18          & 73.45      & 70.74   & 68.88          & \textbf{73.70} \\ \cline{3-3} 
%                          & \multicolumn{1}{c|}{}                                                                      & Depth & 71.38          & 68.82      & 71.91   & 66.87          & \textbf{71.97} \\ \cline{2-3} 
%                          & \multicolumn{1}{c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}RGB-Depth\end{tabular}}}  & RGB   & 66.99          & 74.27      & 77.72   & 75.75          & \textbf{80.13} \\ \cline{3-3} 
%                          & \multicolumn{1}{c|}{}                                                                      & Depth & 66.21          & 67.48      & 70.97   & 72.48          & \textbf{74.10} \\ \hline
% \multirow{2}{*}{Opp~\cite{chavarriaga2013opportunity}}     & \multicolumn{1}{l|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Acce-Gyro\end{tabular}}}  & Acce  & 70.59          & 71.81      & 70.84   & 72.14          & \textbf{73.61} \\ \cline{3-3} 
%                          & \multicolumn{1}{c|}{}                                                                      & Gyro  & 71.12          & 72.10      & 72.09   & 72.09          & \textbf{72.13} \\ \hline
% \end{tabular}
% \end{center}
% \caption{The comparison of average accuracy on different datasets with mixed clients. }
% \label{mixed_table}
% \end{table*}


\begin{table*}[t]
\caption{The comparison of average accuracy on different datasets with mixed clients. }
% \footnotesize
\begin{center}
\resizebox{17cm}{!}{%
\begin{tabular}{|c|cc|c|c|c|c|c|c|}
\hline
Datasets                 & \multicolumn{2}{c|}{Modality}                            & MM-FedAvg~\cite{zhao2022multimodal} & MM-FedProx~\cite{li2020federated} & MM-MOON~\cite{li2021model} & CreamFL~\cite{yu2023multimodal}        & FedMEKT-S      & FedMEKT-C      \\ \hline
\multirow{6}{*}{mHealth~\cite{banos2014mhealthdroid}} & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Gyro}}  & Acce  & 64.19     & 65.27      & 67.97   & 66.89          & 64.32          & \textbf{68.37} \\ 
                         & \multicolumn{1}{c|}{}                            & Gyro  & 63.50     & 64.37      & 64.76   & 64.72          & 64.06          & \textbf{65.19} \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Mage}}  & Acce  & 68.41     & 66.29      & 66.33   & 67.78          & 67.96          & \textbf{68.71} \\  
                         & \multicolumn{1}{c|}{}                            & Mage  & 68.88     & 67.76      & 68.67   & 69.52          & 69.11          & \textbf{70.68} \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{Gyro-Mage}}  & Gyro  & 65.57     & 66.30      & 64.65   & 66.68          & 65.09          & \textbf{67.38} \\ 
                         & \multicolumn{1}{c|}{}                            & Mage  & 68.14     & 68.59      & 68.50   & 69.52          & \textbf{69.68} & 69.03          \\ \hline
\multirow{6}{*}{UR-Fall~\cite{kwolek2014human}} & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & 73.60     & 69.63      & 64.61   & 69.72          & 67.69          & \textbf{73.92} \\  
                         & \multicolumn{1}{c|}{}                            & RGB   & 61.61     & 62.45      & 64.66   & \textbf{68.28} & 60.42          & 68.18          \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & 62.18     & 73.45      & 70.74   & \textbf{75.24} & 68.88          & 73.70          \\ 
                         & \multicolumn{1}{c|}{}                            & Depth & 71.38     & 68.82      & 71.91   & \textbf{72.73} & 66.87          & 71.97          \\ \cline{2-3} 
                         & \multicolumn{1}{c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & 66.99     & 74.27      & 77.72   & 76.51          & 75.75          & \textbf{80.13} \\ 
                         & \multicolumn{1}{c|}{}                            & Depth & 66.21     & 67.48      & 70.97   & 68.38          & 72.48          & \textbf{74.10} \\ \hline
\multirow{2}{*}{Opp~\cite{chavarriaga2013opportunity}}     & \multicolumn{1}{c|}{\multirow{2}{*}{Acce-Gyro}}  & Acce  & 70.59     & 71.81      & 70.84   & 72.33          & 72.14          & \textbf{73.61} \\ 
                         & \multicolumn{1}{c|}{}                            & Gyro  & 71.12     & 72.10      & 72.09   & 72.11          & 72.09          & \textbf{72.13} \\ \hline
\end{tabular}
}%
\end{center}
\label{mixed_table}
\end{table*}
\subsection{Experimental Setup}
\subsubsection{Datasets} In this section, we evaluate the efficiency of the FedMEKT algorithm using three multimodal human activity recognition (HAR) datasets: mHealth~\cite{banos2014mhealthdroid}, UR Fall Detection~\cite{kwolek2014human}, and Opportunity (Opp)~\cite{chavarriaga2013opportunity}. To conduct the experiments, we randomly select $10$ clients from a total of $30$ clients in each round. We follow the experimental setup from MM-FedAvg~\cite{zhao2022multimodal} to generate the training and test data for the federated systems for all three datasets. For the proxy dataset of each dataset, we generate a subset of total data with a size approximately equal to the testing data. For supervised training, we randomly sample labeled data from the training dataset. In each dataset, we use two modalities as the input modalities for the training process. %More details about the datasets are provided in the Appendix.
\paragraph{mHealth~\cite{banos2014mhealthdroid}}This dataset consists of the on-body sensor records from $10$ participants doing $13$ daily living and exercise activities. In this dataset, we consider three modalities, accelerometer (Acce), gyroscope (Gyro), and magnetometer (Mage), in our experiments. We randomly use data from one participant as testing data, another participant's data as proxy data, and the rest as training data.
\paragraph{UR Fall Detection~\cite{kwolek2014human}}This dataset contains $70$ video clips of human activities with $3$ classes. We consider $3$ modalities RGB camera (RGB), depth camera (Depth), and sensory data of each video frame from accelerometers (Acce) in our experiments. We follow the setup from MM-FedAvg~\cite{zhao2022multimodal} to generate RGB data. We randomly sample $1/10$ of data as testing data, $1/10$ of data as proxy data, and the rest as training data. 
\paragraph{Opportunity~\cite{chavarriaga2013opportunity}}This is the human activity recognition dataset that consists of the on-body sensor records from $4$ participants doing kitchen activities. For each participant, they recorded five different runs. In this dataset, we utilize 18 kitchen activities as $18$ classes and consider two modalities, accelerometers (Acce) and gyroscope (Gyro) for our experiments. We use runs $4$ and $5$ from participants $2$ and $3$ as the testing data, runs $4$ and $5$ from participants $1$ and $4$ as proxy data, and the rest for training data, respectively. 
\subsubsection{Model Architecture and Baselines}
FedMEKT comprises two versions, FedMEKT-C and FedMEKT-S, with FedMEKT-C being the primary algorithm that utilizes joint multimodal knowledge for knowledge transfer. In contrast, FedMEKT-S employs split knowledge from each modality, updating autoencoders of modalities A and B separately with the respective modality's split knowledge instead of concatenating the knowledge of different modalities from all clients as FedMEKT-C does. In Tables~\ref{detai_model} and~\ref{table:model_architecture}, we provide a summary of the autoencoder architecture that we employed in this paper. We follow the approach taken by Zhao et al. \cite{zhao2022multimodal} and provide the model size for each dataset. In contrast to the prior work, we use two hidden layers as primary layers and extract knowledge from those layers for both upstream and downstream transfer mechanisms. This allows us to effectively leverage the learned representations for a wide range of tasks. In this study, we compared our proposed method, FedMEKT, with several baselines, including MM-FedAvg~\cite{zhao2022multimodal}, CreamFL~\cite{yu2023multimodal} and multimodal versions of state-of-the-art approaches, i.e., FedProx \cite{li2020federated} and MOON \cite{li2021model}. 


\subsubsection{Implementation Details} The FedMEKT algorithm was developed using the Pytorch library \cite{paszke2019pytorch} and experiments were simulated on a server with one NVIDIA GeForce GTX-1080 Ti GPU using CUDA version 11.2 and an Intel Core i7-7700K 4.20GHz CPU with sufficient memory for model training. The knowledge transfer scheme utilized LSTM \cite{hochreiter1997long} autoencoders with 2 LSTM layers, and knowledge was extracted from 2 hidden layers. For downstream tasks, the global classifier was implemented as a two-layer perceptron with ReLU~\cite{nair2010rectified} activation function. 

\subsubsection{Evaluation Metrics} We evaluate the performance of the global encoder by extracting the representations for downstream tasks. Specifically, we freeze the parameters of the global encoder and train a linear classifier on the extracted representations. We report the average $F_1$ score in last 10 communication rounds as our evaluation metric.




\subsection{Experimental Results}

\subsubsection{Performance Comparison}

\paragraph{Multimodal Clients} Table~\ref{multimodal_table} presents the performance comparison of our FedMEKT algorithm and other baselines on three multimodal human activity recognition datasets with $30$ multimodal clients. As the results show, FedMEKT-C achieves the highest test accuracy in most cases on the mHealth dataset with approximately $\textbf{0.5-2\%}$ improvement over other baselines. Similarly, on the UR Fall Detection dataset, FedMEKT-C also outperforms other baselines with $\textbf{1-2\%}$ improvement in most modalities combinations. While our method may not surpass CreamFL in certain combination cases, we still achieve competitive results. On the Opp dataset, FedMEKT-C obtains the highest accuracy of $\textbf{73.50\%}$ followed by CreamFL with the test accuracy of $\textbf{73.33\%}$ on the Acce downstream task. We observe that FedMEKT-S and MM-FedProx also achieve relatively high test accuracies in some cases but still slightly worse than FedMEKT-C. Overall, the results demonstrate that our FedMEKT method, especially the FedMEKT-C variant, outperforms all baselines across all modalities combinations and datasets, indicating its effectiveness in addressing the challenges of multimodal federated learning. %We claim that FedMEKT-C has the best performance in most cases as the global model can adaptively learn better from the common representations derived from the joint knowledge of all clients than from learning from the split knowledge. 
In the majority of cases, FedMEKT-C demonstrates superior performance as the global model effectively learns from the joint embedding knowledge of all clients, encompassing common representations, compared to learning from the split knowledge.

\begin{table}[t]
\caption{The comparison of average accuracy of FedMEKT under different proxy data size on UR Fall Detection Dataset~\cite{kwolek2014human} with multimodal clients. }
% \footnotesize
\begin{center}
\begin{tabular}{cc|ccc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{ Modality    }}             & \multicolumn{3}{c|}{Size of Proxy Data $D_r$}                                        \\ \cline{3-5}
\multicolumn{2}{|c|}{}                                    & \multicolumn{1}{c}{10\%}   & \multicolumn{1}{c}{50\%}            & 100\%  \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & \multicolumn{1}{c}{58.05} & \multicolumn{1}{c}{61.16}          & \textbf{70.66} \\ 
\multicolumn{1}{|c|}{}                            & RGB   & \multicolumn{1}{c} {58.27} & \multicolumn{1}{c}{60.43}          & \textbf{66.70} \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & \multicolumn{1}{c}{59.27} & \multicolumn{1}{c}{64.39}          & \textbf{72.68} \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{58.22} & \multicolumn{1}{c}{62.93}          & \textbf{75.22} \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & \multicolumn{1}{c}{67.43} & \multicolumn{1}{c}{72.99}          & \textbf{77.87} \\
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{56.88} & \multicolumn{1}{c}{\textbf{72.55}} & 70.57    \\ \hline
\end{tabular}
\label{multimodal_proxy_data}
\end{center}
\end{table}

\begin{table}[t]
\caption{The comparison of average accuracy of FedMEKT under different proxy data size on UR Fall Detection Dataset~\cite{kwolek2014human} with mixed clients. }
% \footnotesize
\begin{center}
\begin{tabular}{cc|ccc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{ Modality    }}             & \multicolumn{3}{c|}{Size of Proxy Data $D_r$}                                        \\ \cline{3-5}
\multicolumn{2}{|c|}{}                                    & \multicolumn{1}{c}{10\%}   & \multicolumn{1}{c}{50\%}            & 100\%  \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & \multicolumn{1}{c}{63.40} & \multicolumn{1}{c}{65.02}          & \textbf{73.92} \\ 
\multicolumn{1}{|c|}{}                            & RGB   & \multicolumn{1}{c} {58.22} & \multicolumn{1}{c}{60.53}          & \textbf{68.18} \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & \multicolumn{1}{c}{58.43} & \multicolumn{1}{c}{61.95}          & \textbf{73.70} \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{58.22} & \multicolumn{1}{c}{62.88}          & \textbf{71.97} \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & \multicolumn{1}{c}{62.89} & \multicolumn{1}{c}{67.92}          & \textbf{80.13} \\
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{58.16} & \multicolumn{1}{c}{60.15} & \textbf{74.10}    \\ \hline
\end{tabular}
\label{mixed_proxy_data}
\end{center}
\end{table}



\paragraph{Mixed Clients} Apart from the multimodal clients setting, we extend the experiment to include a mixed clients setting with both unimodal and multimodal clients. To conduct this experiment, we include $10$ multimodal clients, $10$ unimodal clients for modality A, $10$ unimodal clients for modality B, and randomly select $10$ clients from a total of $30$ mixed clients in each communication round. Following \cite{zhao2022multimodal}, parameter-based methods such as MM-FedAvg, MM-FedProx, and MM-MOON assigned higher weights (set to 100) to multimodal clients in model aggregation. For FedMEKT variants like FedMEKT-C and FedMEKT-S with unimodal clients, we generate the knowledge from the encoder of the corresponding modality and obtain the collaborative knowledge of each modality by averaging the knowledge from both multimodal and unimodal clients. Table~\ref{mixed_table} shows the performance comparison of our methods with other baselines on downstream tasks in mixed clients setting. For most modalities combinations of three datasets, FedMEKT-C outperforms other baselines on downstream tasks performance. Exceptions include the Mage task in the Gyro-Mage combination of the mHealth dataset and certain combination cases in the UR Fall Detection dataset, where FedMEKT-S and CreamFL show slightly better performance. Nonetheless, we still achieve competitive results in these cases. From our observation, FedMEKT outperforms other baselines with $\textbf{1-3\%}$ in most combination cases, and some cases can improve the performance of downstream tasks when working on mixed clients setting. Overall, from the results in Table~\ref{mixed_table}, we claim that our proposed method can work well in various practical scenarios and outperforms both multimodal versions of other classical FL methods and state-of-the-art methods such as MM-FedAvg and CreamFL.



\begin{table}[t]
\caption{The comparison of average accuracy of FedMEKT under different EKT Steps on UR Fall Detection Dataset~\cite{kwolek2014human} with multimodal clients. }
\begin{center}
\begin{tabular}{|cc|ccc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Modality}}                   & \multicolumn{3}{c|}{\# of EKT Steps R}                                   \\ \cline{3-5} 
\multicolumn{2}{|c|}{}                                    & \multicolumn{1}{c}{1}     & \multicolumn{1}{c}{2}              & 3     \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & \multicolumn{1}{c}{53.73} & \multicolumn{1}{c}{\textbf{70.66}} & 66.28 \\  
\multicolumn{1}{|c|}{}                            & RGB   & \multicolumn{1}{c}{58.25} & \multicolumn{1}{c}{\textbf{66.70}} & 58.72 \\ \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & \multicolumn{1}{c}{55.28} & \multicolumn{1}{c}{\textbf{72.68}} & 65.34 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{58.73} & \multicolumn{1}{c}{\textbf{75.22}} & 65.35 \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & \multicolumn{1}{c}{72.96} & \multicolumn{1}{c}{\textbf{77.87}} & 63.26 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{58.21} & \multicolumn{1}{c}{\textbf{70.57}} & 67.86 \\ \hline
\end{tabular}
\label{Rstep_multimodal}
\end{center}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[t]
\caption{The comparison of average accuracy of FedMEKT under different EKT Steps on UR Fall Detection Dataset~\cite{kwolek2014human} with mixed clients. }
\begin{center}
\begin{tabular}{|cc|ccc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Modality}}                   & \multicolumn{3}{c|}{\# of EKT Steps R}                                   \\ \cline{3-5} 
\multicolumn{2}{|c|}{}                                    & \multicolumn{1}{c}{1}     & \multicolumn{1}{c}{2}              & 3     \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & \multicolumn{1}{c}{58.21} & \multicolumn{1}{c}{\textbf{73.92}} & 72.22 \\  
\multicolumn{1}{|c|}{}                            & RGB   & \multicolumn{1}{c}{58.23} & \multicolumn{1}{c}{\textbf{68.18}} & 59.86 \\ \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & \multicolumn{1}{c}{49.00} & \multicolumn{1}{c}{\textbf{73.70}} & 56.67 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{56.97} & \multicolumn{1}{c}{\textbf{71.97}} & 64.64 \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & \multicolumn{1}{c}{71.88} & \multicolumn{1}{c}{\textbf{80.13}} & 63.92 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{58.24} & \multicolumn{1}{c}{\textbf{74.10}} & 60.76 \\ \hline
\end{tabular}
\label{Rstep_mixed}
\end{center}
\end{table}

% Figure environment removed
\begin{table}[t]
\caption{The comparison of average accuracy of FedMEKT under different local epochs on UR Fall Detection Dataset~\cite{kwolek2014human} with multimodal clients. }
\begin{center}
\begin{tabular}{|cc|ccc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Modality}}                   & \multicolumn{3}{c|}{\# of Local Epochs N}                                   \\ \cline{3-5} 
\multicolumn{2}{|c|}{}                                    & \multicolumn{1}{c}{1}     & \multicolumn{1}{c}{2}              & 3     \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & \multicolumn{1}{c}{56.71} & \multicolumn{1}{c}{\textbf{70.66}} & 68.03 \\  
\multicolumn{1}{|c|}{}                            & RGB   & \multicolumn{1}{c}{58.23} & \multicolumn{1}{c}{\textbf{66.70}} & 63.93 \\ \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & \multicolumn{1}{c}{58.96} & \multicolumn{1}{c}{\textbf{72.68}} & 62.26 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{59.66} & \multicolumn{1}{c}{\textbf{75.22}} & 67.67 \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & \multicolumn{1}{c}{67.30} & \multicolumn{1}{c}{\textbf{77.87}} & 69.37 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{68.48} & \multicolumn{1}{c}{\textbf{70.57}} & 61.60 \\ \hline
\end{tabular}
\label{Nstep_multimodal}
\end{center}
\end{table}

\begin{table}[t]
\caption{The comparison of average accuracy of FedMEKT under different local epochs on UR Fall Detection Dataset~\cite{kwolek2014human} with mixed clients. }
\begin{center}
\begin{tabular}{|cc|ccc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{Modality}}                   & \multicolumn{3}{c|}{\# of Local Epochs N}                                   \\ \cline{3-5} 
\multicolumn{2}{|c|}{}                                    & \multicolumn{1}{c}{1}     & \multicolumn{1}{c}{2}              & 3     \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-RGB}}   & Acce  & \multicolumn{1}{c}{67.76} & \multicolumn{1}{c}{\textbf{73.92}} & 58.25 \\  
\multicolumn{1}{|c|}{}                            & RGB   & \multicolumn{1}{c}{60.00} & \multicolumn{1}{c}{\textbf{68.18}} & 65.14 \\ \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{Acce-Depth}} & Acce  & \multicolumn{1}{c}{57.60} & \multicolumn{1}{c}{\textbf{73.70}} & 61.42 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{63.65} & \multicolumn{1}{c}{71.97} & \textbf{74.58} \\  \cline{1-2}
\multicolumn{1}{|c|}{\multirow{2}{*}{RGB-Depth}}  & RGB   & \multicolumn{1}{c}{65.94} & \multicolumn{1}{c}{\textbf{80.13}} & 72.64 \\  
\multicolumn{1}{|c|}{}                            & Depth & \multicolumn{1}{c}{58.63} & \multicolumn{1}{c}{\textbf{74.10}} & 66.81 \\ \hline
\end{tabular}
\label{Nstep_mixed}
\end{center}
\end{table}
\begin{table}[t]
\caption{Results on ablation studies with multimodal clients.}
\begin{center}
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multirow{2}{*}{Methods}                                           & \multicolumn{2}{c|}{Acce-RGB}                        & \multicolumn{2}{c|}{Acce-Depth}                      & \multicolumn{2}{c|}{RGB-Depth}                       \\ \cline{2-7} 
                                                                   & \multicolumn{1}{c|}{Acce}           & RGB            & \multicolumn{1}{c|}{Acce}           & Depth          & \multicolumn{1}{c|}{RGB}            & Depth          \\ \hline
FedMEKT                                                            & \multicolumn{1}{c|}{\textbf{70.66}} & \textbf{66.70} & \multicolumn{1}{c|}{\textbf{72.68}} & \textbf{75.22} & \multicolumn{1}{c|}{\textbf{77.87}} & \textbf{70.57} \\ \hline
\begin{tabular}[c]{@{}c@{}}FedMEKT\\ (w/o local EKD)\end{tabular}  & \multicolumn{1}{c|}{67.32}          & 58.21          & \multicolumn{1}{c|}{68.02}          & 74.25          & \multicolumn{1}{c|}{63.78}          & 58.21          \\ \hline
\begin{tabular}[c]{@{}c@{}}FedMEKT\\ (w/o global EKD)\end{tabular} & \multicolumn{1}{c|}{69.36}          & 58.20          & \multicolumn{1}{c|}{72.30}          & 66.06          & \multicolumn{1}{c|}{45.63}          & 58.46          \\ \hline
\end{tabular}
\label{ablation_multimodal}
\end{center}
\end{table}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[!]
\caption{Results on ablation studies with mixed clients.}
\begin{center}
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multirow{2}{*}{Methods}                                           & \multicolumn{2}{c|}{Acce-RGB}                        & \multicolumn{2}{c|}{Acce-Depth}                      & \multicolumn{2}{c|}{RGB-Depth}                       \\ \cline{2-7} 
                                                                   & \multicolumn{1}{c|}{Acce}           & RGB            & \multicolumn{1}{c|}{Acce}           & Depth          & \multicolumn{1}{c|}{RGB}            & Depth          \\ \hline
FedMEKT                                                            & \multicolumn{1}{c|}{\textbf{73.92}} & \textbf{68.18} & \multicolumn{1}{c|}{\textbf{73.70}} & \textbf{71.97} & \multicolumn{1}{c|}{\textbf{80.13}} & \textbf{74.10} \\ \hline
\begin{tabular}[c]{@{}c@{}}FedMEKT\\ (w/o local EKD)\end{tabular}  & \multicolumn{1}{c|}{70.38}          & 59.08          & \multicolumn{1}{c|}{66.48}          & 67.05          & \multicolumn{1}{c|}{71.12}          & 60.17          \\ \hline
\begin{tabular}[c]{@{}c@{}}FedMEKT\\ (w/o global EKD)\end{tabular} & \multicolumn{1}{c|}{61.59}          & 60.36          & \multicolumn{1}{c|}{73.61}          & 67.99          & \multicolumn{1}{c|}{45.70}          & 58.22          \\ \hline
\end{tabular}
\label{ablation_mixed}
\end{center}
\end{table}

\subsubsection{Efficiency Evaluation}
Inspired by previous work~\cite{zhang2021parameterized}, we evaluate the communication cost by recording three criteria: proxy data and knowledge size for distillation-based methods, and model parameters for model parameter-based methods. As shown in Fig~\ref{cost}, our proposed distillation-based FedMEKT with the knowledge transfer scheme can save more communication cost than other parameter-based methods.


\subsubsection{Effect of Proxy Dataset}
In this experiment, we evaluate the performance of our proposed method which utilizes proxy data for exchanging knowledge between the server and clients. We assess the impact of varying the size of the proxy dataset, $D_r$, on the FedMEKT-C algorithm with different client settings. Tables~\ref{multimodal_proxy_data} and~\ref{mixed_proxy_data} demonstrate the effect of the proxy data size on the algorithm's performance. The results indicate that increasing the size of the proxy data $D_r$ enhances the performance in most of the scenarios. In our experiment on the UR Fall Detection Dataset~\cite{kwolek2014human}, we set the default size of the proxy dataset to $1000$ samples, which corresponds to $100\%$ of $D_r$ and $1/10$ of the total data.
\subsubsection{Effect of Hyperparameters}
\paragraph{Effect of Knowledge Transfer Step R} In this experiment, we compare the performance of FedMEKT on the UR Fall Detection dataset~\cite{kwolek2014human} under different settings of embedding knowledge transfer (EKT) steps. The number of local epochs is set to $2$. As shown in Tables~\ref{Rstep_multimodal} and~\ref{Rstep_mixed}, we find that the value $R=2$ consistently yields the highest performance across all scenarios, outperforming other values.


\paragraph{Effect of Local Epoch N} Similarly, we also investigate the behavior of the local epoch $N$ on the UR Fall Detection dataset~\cite{kwolek2014human}, as shown in Tables~\ref{Nstep_multimodal} and~\ref{Nstep_mixed}, with the number of knowledge transfer steps fixed at $2$. Our findings indicate that achieving optimal performance requires a moderate number of local epochs, as a large value of $N$ does not always result in better performance.


\paragraph{Other Hyperparameter Tuning} We tune the hyperparameters $\gamma$ and $\beta$ for FedMEKT, selecting from the range of \{0.01, 0.1\} for each modality combination, and report the best result. For MM-FedProx and MM-MOON, we adopt the values from the original works~\cite{li2020federated,li2021model} and tune the hyperparameter $\mu$ for the proximal term and the contrastive loss. We use the Adam optimizer with a learning rate in the range between \{0.001, 0.01\} for all approaches and report the best result.

\subsubsection{Ablation Studies}
In this experiment, we investigate the impact of EKD regularizer on both the server and client sides. We conduct experiments on UR Fall Detection~\cite{kwolek2014human} under different scenarios, as shown in Table~\ref{ablation_multimodal} and \ref{ablation_mixed}, by removing the EKD regularizer on local and global sides, respectively. Our results demonstrate that FedMEKT can significantly benefit from the EKD regularizer on both sides of the framework. In all use cases, we observe the improved performance with the EKD regularizer present on both sides.

\section{Discussion}
In this section, we explore the strengths and limitations of our method in comparison to MM-FedAvg and other related works. Firstly, FedMEKT offers several advantages, including communication cost savings by avoiding parameter aggregation and achieving higher performance in multimodal FL. By utilizing joint embedding knowledge exchange instead of parameter exchange, FedMEKT significantly reduces the communication cost required between the server and clients. Furthermore, FedMEKT demonstrates superior performance compared to other baselines. The joint embedding knowledge transfer mechanism allows the global model to capture the distinctive features of multimodal data from all local models, leading to enhanced generalization capabilities and improved accuracy in downstream tasks. However, it is essential to acknowledge that FedMEKT also has limitations, such as the requirement of a proxy dataset, which can be cumbersome for tasks involving large amounts of data. In scenarios where the sample number is extremely large, and the task complexity is high, both FedMEKT and other baseline methods would still incur a high communication cost. Future research could explore strategies to address this limitation and optimize communication efficiency in multimodal FL.

\section{Conclusion}
In this work, we proposed a novel multimodal federated learning framework under the semi-supervised setting by developing the joint embedding knowledge transfer scheme. Through extensive simulations, our proposed FedMEKT obtains a more stable and better performance in downstream tasks than other baselines without exchanging model parameters. This results in saving communication cost, particularly in models with millions of parameters, enhancing privacy protection. %Future research will extend this work to include other tasks and additional modalities, enabling the deployment of federated learning in personalized applications.

% \section*{Acknowledgments}
% This should be a simple paragraph before the References to thank those individuals and institutions who have supported your work on this article.



{\appendix[Formulation for Algorithms Under Multimodal FL Framework]
Inspired by other state-of-the-art FL methods such as FedProx~\cite{li2020federated} and MOON~\cite{li2021model}, we reproduce several algorithms under multimodal federated learning framework. For each algorithm, we present its mathematical formulation.
\subsection{MM-FedProx}
Similar to MM-FedAvg~\cite{zhao2022multimodal}, the learning objective functions for global autoencoders A and B can be defined as:
\begin{equation}
\begin{split}
     \min_{f_{A},g_{A}}{\mathcal{L}}_{s}(f_{A},g_{A})= \sum_{k\in{m_{A}}}{\frac{n_{k}}{n_{A}}}&{\mathcal{L}}_{k}(f_{A},g_{A}) \\
     &+\alpha \, \sum_{k\in{m_{AB}}}{\frac{n_{k}}{n_{A}}}{\mathcal{L}}_{k}(f_{A},g_{A}),  
    % \text{where}  {\ell}^{}_{k}(f_{A},g_{A})=\min_{f_{A},g_{A}}{\ell}^{}_{A}(x_A,x_A') +{\ell}_{A}(x_B, x_B')
\end{split}
\end{equation}

\begin{equation}
\begin{split}
       \min_{f_{B},g_{B}}{\mathcal{L}}^{}_{s}(f_{B},g_{B})= \sum_{k\in{m_{B}}}{\frac{n_{k}}{n_{B}}}&{\mathcal{L}}_{k}(f_{B},g_{B}) \\
       &+\alpha \, \sum_{k\in{m_{AB}}}{\frac{n_{k}}{n_{B}}}{\mathcal{L}}_{k}(f_{B},g_{B}),  
    % \text{where}  {\ell}^{}_{k}(f_{A},g_{A})=\min_{f_{A},g_{A}}{\ell}^{}_{A}(x_A,x_A') +{\ell}_{A}(x_B, x_B')
\end{split}
\end{equation}
%
where ${\mathcal{L}}^{}_{k}(f_{A},g_{A})$ and ${\mathcal{L}}^{}_{k}(f_{B},g_{B})$ are loss functions of split autoencoder for each modality $A$ and $B$. Specifically, loss functions at client $k$ for MM-FedProx can be defined as:
\begin{equation}
\begin{split}
    {\mathcal{L}}^{}_{k}(f_{A},g_{A})= \min_{f_{A},g_{A}}{\ell}^{}_{A}(x^A,x^{A'}) +{\ell}_{B}(x^B, x^{B'})
    +\mu\ell_{proxA}, 
\end{split}
\end{equation}

\begin{equation}
\begin{split}
    {\mathcal{L}}^{}_{k}(f_{B},g_{B})= \min_{f_{B},g_{B}}{\ell}^{}_{A}(x^A,x^{A'}) +{\ell}_{B}(x^B, x^{B'}) 
    +\mu\ell_{proxB},
\end{split}
\end{equation}
%
where $\ell_A$ and $\ell_B$ are reconstruction losses \cite{zhao2022multimodal} (e.g., MSE loss), $\ell_{proxA}=\left \| w_s^A - w_k^A \right \|^{2}$, $\ell_{proxB}=\left \| w_s^B - w_k^B \right \|^{2}$ while $x^{A'}$ and $x^{B'}$ denote the reconstructed outputs of two modalities A and B, respectively.
\subsection{MM-MOON}
Following the MM-FedAvg~\cite{zhao2022multimodal} mechanism, MM-MOON assigned higher weights (set to 100) to multimodal clients in model aggreagation. The learning objective functions for global autoencoders A and B can be defined as:
\begin{equation}
\begin{split}
     \min_{f_{A},g_{A}}{\mathcal{L}}_{s}(f_{A},g_{A})= \sum_{k\in{m_{A}}}{\frac{n_{k}}{n_{A}}}&{\mathcal{L}}_{k}(f_{A},g_{A}) \\
     &+\alpha \, \sum_{k\in{m_{AB}}}{\frac{n_{k}}{n_{A}}}{\mathcal{L}}_{k}(f_{A},g_{A}),  
    % \text{where}  {\ell}^{}_{k}(f_{A},g_{A})=\min_{f_{A},g_{A}}{\ell}^{}_{A}(x_A,x_A') +{\ell}_{A}(x_B, x_B')
\end{split}
\end{equation}

\begin{equation}
\begin{split}
       \min_{f_{B},g_{B}}{\mathcal{L}}^{}_{s}(f_{B},g_{B})= \sum_{k\in{m_{B}}}{\frac{n_{k}}{n_{B}}}&{\mathcal{L}}_{k}(f_{B},g_{B}) \\
       &+\alpha \, \sum_{k\in{m_{AB}}}{\frac{n_{k}}{n_{B}}}{\mathcal{L}}_{k}(f_{B},g_{B}),  
    % \text{where}  {\ell}^{}_{k}(f_{A},g_{A})=\min_{f_{A},g_{A}}{\ell}^{}_{A}(x_A,x_A') +{\ell}_{A}(x_B, x_B')
\end{split}
\end{equation}
%
where ${\mathcal{L}}^{}_{k}(f_{A},g_{A})$ and ${\mathcal{L}}^{}_{k}(f_{B},g_{B})$ are loss functions of split autoencoder for each modality $A$ and $B$. Specifically, loss functions at client $k$ for MM-MOON can be defined as:
\begin{equation}
\begin{split}
    &{\mathcal{L}}^{}_{k}(f_{A},g_{A})= \min_{f_{A},g_{A}}{\ell}^{}_{A}(x^A,x^{A'}) +{\ell}_{B}(x^B, x^{B'})
    +\mu\ell_{conA},
\end{split}
\end{equation}

\begin{equation}
\begin{split}
    {\mathcal{L}}^{}_{k}(f_{B},g_{B})= \min_{f_{B},g_{B}}{\ell}^{}_{A}(x^A,x^{A'}) +{\ell}_{B}(x^B, x^{B'}) 
    +\mu\ell_{conB},
\end{split}
\end{equation}
%
where $\ell_A$ and $\ell_B$ are reconstruction losses \cite{zhao2022multimodal} (e.g., MSE loss), $\ell_{conA}$ and $\ell_{conB}$ are the model-contrastive loss for modality A and B, respectively. The model-contrastive loss for two modalities are denoted as:
{
\small
\begin{equation}
% \begin{aligned}
    \ell_{conA}  = -\log{\frac{\exp(\frac{\textrm{sim}(z_A, z_{globA})}{\tau})}{\exp(\frac{\textrm{sim}(z_A, z_{globA})}{\tau}) + \sum_{i=1}^k\exp(\frac{\textrm{sim}(z_A, z_{prevA}^i)}{\tau})}}
% \end{aligned}
\end{equation}
}

{
\small
\begin{equation}
% \begin{aligned}
    \ell_{conB}  = -\log{\frac{\exp(\frac{\textrm{sim}(z_B, z_{globB})}{\tau})}{\exp(\frac{\textrm{sim}(z_B, z_{globB})}{\tau}) + \sum_{i=1}^k\exp(\frac{\textrm{sim}(z_B, z_{prevB}^i)}{\tau})}}
% \end{aligned}
\end{equation}
}
%
where $\tau$ denotes a temperature parameter. We follows the original paper MOON~\cite{li2021model} for setting $\tau$.}



%{\appendices
%\section*{Proof of the First Zonklar Equation}
%Appendix one text goes here.
% You can choose not to have a title for an appendix if you want by leaving the argument blank
%\section*{Proof of the Second Zonklar Equation}
%Appendix two text goes here.}




 
 % argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%

\bibliographystyle{IEEEtran}
% \bibliography{mybib}

% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{pawar2020explainable}
U.~Pawar, D.~Oâ€™Shea, S.~Rea, and R.~Oâ€™Reilly, ``Explainable ai in
  healthcare,'' in \emph{2020 International Conference on Cyber Situational
  Awareness, Data Analytics and Assessment (CyberSA)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2020, pp. 1--2.

\bibitem{lugano2017virtual}
G.~Lugano, ``Virtual assistants and self-driving cars,'' in \emph{2017 15th
  International Conference on ITS Telecommunications (ITST)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2017, pp. 1--5.

\bibitem{rahayu2022systematic}
N.~W. Rahayu, R.~Ferdiana, and S.~S. Kusumawardani, ``A systematic review of
  ontology use in e-learning recommender system,'' \emph{Computers and
  Education: Artificial Intelligence}, p. 100047, 2022.

\bibitem{bawack2022artificial}
R.~E. Bawack, S.~F. Wamba, K.~D.~A. Carillo, and S.~Akter, ``Artificial
  intelligence in e-commerce: a bibliometric study and literature review,''
  \emph{Electronic markets}, vol.~32, no.~1, pp. 297--338, 2022.

\bibitem{liu2021machine}
B.~Liu, M.~Ding, S.~Shaham, W.~Rahayu, F.~Farokhi, and Z.~Lin, ``When machine
  learning meets privacy: A survey and outlook,'' \emph{ACM Computing Surveys
  (CSUR)}, vol.~54, no.~2, pp. 1--36, 2021.

\bibitem{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas,
  ``Communication-efficient learning of deep networks from decentralized
  data,'' in \emph{Artificial intelligence and statistics}.\hskip 1em plus
  0.5em minus 0.4em\relax PMLR, 2017, pp. 1273--1282.

\bibitem{li2020federated}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith,
  ``Federated optimization in heterogeneous networks,'' \emph{Proceedings of
  Machine Learning and Systems}, vol.~2, pp. 429--450, 2020.

\bibitem{kairouz2021advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings \emph{et~al.}, ``Advances
  and open problems in federated learning,'' \emph{Foundations and
  Trends{\textregistered} in Machine Learning}, vol.~14, no. 1--2, pp. 1--210,
  2021.

\bibitem{zheng2022applications}
Z.~Zheng, Y.~Zhou, Y.~Sun, Z.~Wang, B.~Liu, and K.~Li, ``Applications of
  federated learning in smart cities: recent advances, taxonomy, and open
  challenges,'' \emph{Connection Science}, vol.~34, no.~1, pp. 1--28, 2022.

\bibitem{ramu2022federated}
S.~P. Ramu, P.~Boopalan, Q.-V. Pham, P.~K.~R. Maddikunta, T.~Huynh-The,
  M.~Alazab, T.~T. Nguyen, and T.~R. Gadekallu, ``Federated learning enabled
  digital twins for smart cities: Concepts, recent advances, and future
  directions,'' \emph{Sustainable Cities and Society}, vol.~79, p. 103663,
  2022.

\bibitem{pandya2023federated}
S.~Pandya, G.~Srivastava, R.~Jhaveri, M.~R. Babu, S.~Bhattacharya, P.~K.~R.
  Maddikunta, S.~Mastorakis, M.~J. Piran, and T.~R. Gadekallu, ``Federated
  learning for smart cities: A comprehensive survey,'' \emph{Sustainable Energy
  Technologies and Assessments}, vol.~55, p. 102987, 2023.

\bibitem{xu2021federated}
J.~Xu, B.~S. Glicksberg, C.~Su, P.~Walker, J.~Bian, and F.~Wang, ``Federated
  learning for healthcare informatics,'' \emph{Journal of Healthcare
  Informatics Research}, vol.~5, pp. 1--19, 2021.

\bibitem{antunes2022federated}
R.~S. Antunes, C.~Andr{\'e}~da Costa, A.~K{\"u}derle, I.~A. Yari, and
  B.~Eskofier, ``Federated learning for healthcare: Systematic review and
  architecture proposal,'' \emph{ACM Transactions on Intelligent Systems and
  Technology (TIST)}, vol.~13, no.~4, pp. 1--23, 2022.

\bibitem{nguyen2022federated}
D.~C. Nguyen, Q.-V. Pham, P.~N. Pathirana, M.~Ding, A.~Seneviratne, Z.~Lin,
  O.~Dobre, and W.-J. Hwang, ``Federated learning for smart healthcare: A
  survey,'' \emph{ACM Computing Surveys (CSUR)}, vol.~55, no.~3, pp. 1--37,
  2022.

\bibitem{rajendran2015bridge}
J.~Rajendran, M.~M. Khapra, S.~Chandar, and B.~Ravindran, ``Bridge
  correlational neural networks for multilingual multimodal representation
  learning,'' \emph{arXiv preprint arXiv:1510.03519}, 2015.

\bibitem{liang2018computational}
P.~P. Liang, R.~Salakhutdinov, and L.-P. Morency, ``Computational modeling of
  human multimodal language: The mosei dataset and interpretable dynamic
  fusion,'' in \emph{First Workshop and Grand Challenge on Computational
  Modeling of Human Multimodal Language}, 2018.

\bibitem{garcia2018mental}
E.~Garcia-Ceja, M.~Riegler, T.~Nordgreen, P.~Jakobsen, K.~J. Oedegaard, and
  J.~T{\o}rresen, ``Mental health monitoring with multimodal sensing and
  machine learning: A survey,'' \emph{Pervasive and Mobile Computing}, vol.~51,
  pp. 1--26, 2018.

\bibitem{xiong2022unified}
B.~Xiong, X.~Yang, F.~Qi, and C.~Xu, ``A unified framework for multi-modal
  federated learning,'' \emph{Neurocomputing}, vol. 480, pp. 110--118, 2022.

\bibitem{alonso2015challenges}
O.~Alonso, ``Challenges with label quality for supervised learning,''
  \emph{Journal of Data and Information Quality (JDIQ)}, vol.~6, no.~1, pp.
  1--3, 2015.

\bibitem{zhao2022multimodal}
Y.~Zhao, P.~Barnaghi, and H.~Haddadi, ``Multimodal federated learning on iot
  data,'' in \emph{2022 IEEE/ACM Seventh International Conference on
  Internet-of-Things Design and Implementation (IoTDI)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2022, pp. 43--54.

\bibitem{li2019fedmd}
D.~Li and J.~Wang, ``Fedmd: Heterogenous federated learning via model
  distillation,'' \emph{arXiv preprint arXiv:1910.03581}, 2019.

\bibitem{deng2020adaptive}
Y.~Deng, M.~M. Kamani, and M.~Mahdavi, ``Adaptive personalized federated
  learning,'' \emph{arXiv preprint arXiv:2003.13461}, 2020.

\bibitem{yu2023multimodal}
Q.~Yu, Y.~Liu, Y.~Wang, K.~Xu, and J.~Liu, ``Multimodal federated learning via
  contrastive representation ensemble,'' \emph{arXiv preprint
  arXiv:2302.08888}, 2023.

\bibitem{ngiam2011multimodal}
J.~Ngiam, A.~Khosla, M.~Kim, J.~Nam, H.~Lee, and A.~Y. Ng, ``Multimodal deep
  learning,'' in \emph{ICML}, 2011.

\bibitem{liang2020think}
P.~P. Liang, T.~Liu, L.~Ziyin, N.~B. Allen, R.~P. Auerbach, D.~Brent,
  R.~Salakhutdinov, and L.-P. Morency, ``Think locally, act globally: Federated
  learning with local and global representations,'' \emph{arXiv preprint
  arXiv:2001.01523}, 2020.

\bibitem{liu2020federated}
F.~Liu, X.~Wu, S.~Ge, W.~Fan, and Y.~Zou, ``Federated learning for
  vision-and-language grounding problems,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~34, no.~07, 2020, pp.
  11\,572--11\,579.

\bibitem{kwolek2014human}
B.~Kwolek and M.~Kepski, ``Human fall detection on embedded platform using
  depth maps and wireless accelerometer,'' \emph{Computer methods and programs
  in biomedicine}, vol. 117, no.~3, pp. 489--501, 2014.

\bibitem{zhu2009introduction}
X.~Zhu and A.~B. Goldberg, ``Introduction to semi-supervised learning,''
  \emph{Synthesis lectures on artificial intelligence and machine learning},
  vol.~3, no.~1, pp. 1--130, 2009.

\bibitem{lee2013pseudo}
D.-H. Lee \emph{et~al.}, ``Pseudo-label: The simple and efficient
  semi-supervised learning method for deep neural networks,'' in \emph{Workshop
  on challenges in representation learning, ICML}, vol.~3, no.~2, 2013, p. 896.

\bibitem{xie2020unsupervised}
Q.~Xie, Z.~Dai, E.~Hovy, T.~Luong, and Q.~Le, ``Unsupervised data augmentation
  for consistency training,'' \emph{Advances in Neural Information Processing
  Systems}, vol.~33, pp. 6256--6268, 2020.

\bibitem{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, N.~Carlini, Z.~Zhang, H.~Zhang, C.~A. Raffel, E.~D.
  Cubuk, A.~Kurakin, and C.-L. Li, ``Fixmatch: Simplifying semi-supervised
  learning with consistency and confidence,'' \emph{Advances in neural
  information processing systems}, vol.~33, pp. 596--608, 2020.

\bibitem{berthelot2019mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~A.
  Raffel, ``Mixmatch: A holistic approach to semi-supervised learning,''
  \emph{Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{bucilua2006model}
C.~BuciluÇŽ, R.~Caruana, and A.~Niculescu-Mizil, ``Model compression,'' in
  \emph{Proceedings of the 12th ACM SIGKDD international conference on
  Knowledge discovery and data mining}, 2006, pp. 535--541.

\bibitem{ba2013deep}
J.~Ba and R.~Caruana, ``Do deep nets really need to be deep?'' in
  \emph{Advances in Neural Information Processing Systems}, vol.~27, 2014.

\bibitem{jeong2018communication}
E.~Jeong, S.~Oh, H.~Kim, J.~Park, M.~Bennis, and S.-L. Kim,
  ``Communication-efficient on-device machine learning: Federated distillation
  and augmentation under non-iid private data,'' \emph{arXiv preprint
  arXiv:1811.11479}, 2018.

\bibitem{nguyen2022cdkt}
M.~N. Nguyen, H.~Q. Le, S.~R. Pandey, and C.~S. Hong, ``Cdkt-fl: Cross-device
  knowledge transfer using proxy dataset in federated learning,'' \emph{arXiv
  preprint arXiv:2204.01542}, 2022.

\bibitem{lin2020ensemble}
T.~Lin, L.~Kong, S.~U. Stich, and M.~Jaggi, ``Ensemble distillation for robust
  model fusion in federated learning,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~33, pp. 2351--2363, 2020.

\bibitem{zhang2021parameterized}
J.~Zhang, S.~Guo, X.~Ma, H.~Wang, W.~Xu, and F.~Wu, ``Parameterized knowledge
  transfer for personalized federated learning,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~34, pp. 10\,092--10\,104, 2021.

\bibitem{bain2021frozen}
M.~Bain, A.~Nagrani, G.~Varol, and A.~Zisserman, ``Frozen in time: A joint
  video and image encoder for end-to-end retrieval,'' in \emph{Proceedings of
  the IEEE/CVF International Conference on Computer Vision}, 2021, pp.
  1728--1738.

\bibitem{ye2019cross}
L.~Ye, M.~Rochan, Z.~Liu, and Y.~Wang, ``Cross-modal self-attention network for
  referring image segmentation,'' in \emph{Proceedings of the IEEE/CVF
  conference on computer vision and pattern recognition}, 2019, pp.
  10\,502--10\,511.

\bibitem{nagrani2021attention}
A.~Nagrani, S.~Yang, A.~Arnab, A.~Jansen, C.~Schmid, and C.~Sun, ``Attention
  bottlenecks for multimodal fusion,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~34, pp. 14\,200--14\,213, 2021.

\bibitem{islam2022mumu}
M.~M. Islam and T.~Iqbal, ``Mumu: Cooperative multitask learning-based guided
  multimodal fusion,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, vol.~36, no.~1, 2022, pp. 1043--1051.

\bibitem{caesar2020nuscenes}
H.~Caesar, V.~Bankiti, A.~H. Lang, S.~Vora, V.~E. Liong, Q.~Xu, A.~Krishnan,
  Y.~Pan, G.~Baldan, and O.~Beijbom, ``nuscenes: A multimodal dataset for
  autonomous driving,'' in \emph{Proceedings of the IEEE/CVF conference on
  computer vision and pattern recognition}, 2020, pp. 11\,621--11\,631.

\bibitem{potamianos2003recent}
G.~Potamianos, C.~Neti, G.~Gravier, A.~Garg, and A.~W. Senior, ``Recent
  advances in the automatic recognition of audiovisual speech,''
  \emph{Proceedings of the IEEE}, vol.~91, no.~9, pp. 1306--1326, 2003.

\bibitem{wollmer2010context}
M.~W{\"o}llmer, A.~Metallinou, F.~Eyben, B.~Schuller, and S.~Narayanan,
  ``Context-sensitive multimodal emotion recognition from speech and facial
  expression using bidirectional lstm modeling,'' 2010.

\bibitem{lu2016hierarchical}
J.~Lu, J.~Yang, D.~Batra, and D.~Parikh, ``Hierarchical question-image
  co-attention for visual question answering,'' \emph{Advances in neural
  information processing systems}, vol.~29, 2016.

\bibitem{kumar2020mcqa}
A.~Kumar, T.~Mittal, and D.~Manocha, ``Mcqa: Multimodal co-attention based
  network for question answering,'' \emph{arXiv preprint arXiv:2004.12238},
  2020.

\bibitem{sun2019videobert}
C.~Sun, A.~Myers, C.~Vondrick, K.~Murphy, and C.~Schmid, ``Videobert: A joint
  model for video and language representation learning,'' in \emph{Proceedings
  of the IEEE/CVF International Conference on Computer Vision}, 2019, pp.
  7464--7473.

\bibitem{andrew2013deep}
G.~Andrew, R.~Arora, J.~Bilmes, and K.~Livescu, ``Deep canonical correlation
  analysis,'' in \emph{International conference on machine learning}.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 2013, pp. 1247--1255.

\bibitem{banos2014mhealthdroid}
O.~Banos, R.~Garcia, J.~A. Holgado-Terriza, M.~Damas, H.~Pomares, I.~Rojas,
  A.~Saez, and C.~Villalonga, ``mhealthdroid: a novel framework for agile
  development of mobile health applications,'' in \emph{International workshop
  on ambient assisted living}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2014, pp. 91--98.

\bibitem{chavarriaga2013opportunity}
R.~Chavarriaga, H.~Sagha, A.~Calatroni, S.~T. Digumarti, G.~Tr{\"o}ster,
  J.~d.~R. Mill{\'a}n, and D.~Roggen, ``The opportunity challenge: A benchmark
  database for on-body sensor-based activity recognition,'' \emph{Pattern
  Recognition Letters}, vol.~34, no.~15, pp. 2033--2042, 2013.

\bibitem{li2021model}
Q.~Li, B.~He, and D.~Song, ``Model-contrastive federated learning,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2021, pp. 10\,713--10\,722.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative
  style, high-performance deep learning library,'' \emph{Advances in neural
  information processing systems}, vol.~32, 2019.

\bibitem{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' \emph{Neural
  computation}, vol.~9, no.~8, pp. 1735--1780, 1997.

\bibitem{nair2010rectified}
V.~Nair and G.~E. Hinton, ``Rectified linear units improve restricted boltzmann
  machines,'' in \emph{Proceedings of the 27th international conference on
  machine learning (ICML-10)}, 2010, pp. 807--814.

\end{thebibliography}

% \begin{thebibliography}{1}
% \bibliographystyle{IEEEtran}

% % \bibitem{ref1}
% % {\it{Mathematics Into Type}}. American Mathematical Society. [Online]. Available: https://www.ams.org/arc/styleguide/mit-2.pdf

% % \bibitem{ref2}
% % T. W. Chaundy, P. R. Barrett and C. Batey, {\it{The Printing of Mathematics}}. London, U.K., Oxford Univ. Press, 1954.

% % \bibitem{ref3}
% % F. Mittelbach and M. Goossens, {\it{The \LaTeX Companion}}, 2nd ed. Boston, MA, USA: Pearson, 2004.

% % \bibitem{ref4}
% % G. Gr\"atzer, {\it{More Math Into LaTeX}}, New York, NY, USA: Springer, 2007.

% % \bibitem{ref5}M. Letourneau and J. W. Sharp, {\it{AMS-StyleGuide-online.pdf,}} American Mathematical Society, Providence, RI, USA, [Online]. Available: http://www.ams.org/arc/styleguide/index.html

% % \bibitem{ref6}
% % H. Sira-Ramirez, ``On the sliding mode control of nonlinear systems,'' \textit{Syst. Control Lett.}, vol. 19, pp. 303--312, 1992.

% % \bibitem{ref7}
% % A. Levant, ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proc. 45th IEEE Conf. Decis.
% % Control}, San Diego, CA, USA, 2006, pp. 5585--5590. DOI: 10.1109/CDC.2006.377165.

% % \bibitem{ref8}
% % M. Fliess, C. Join, and H. Sira-Ramirez, ``Non-linear estimation is easy,'' \textit{Int. J. Model., Ident. Control}, vol. 4, no. 1, pp. 12--27, 2008.

% % \bibitem{ref9}
% % R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez, ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proc. Amer. Control Conf.}, Chicago, IL, USA,
% % 2000, pp. 2245--2249.

% \end{thebibliography}


% \newpage

% % \section{Biography Section}
% % If you have an EPS/PDF photo (graphicx package needed), extra braces are
% %  needed around the contents of the optional argument to biography to prevent
% %  the LaTeX parser from getting confused when it sees the complicated
% %  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
% %  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
% %  simpler here.)
 
% \vspace{11pt}

% % \bf{If you include a photo:}\vspace{-33pt}

% \begin{IEEEbiography}[{% Figure removed}]
% {\bf Huy Q. Le} received the B.E. degree in Control Engineering Automation from the Ho Chi Minh City University of Technology, Ho Chi Minh City, Vietnam, in 2019. He is currently pursuing a Ph.D. degree in Computer Science at Kyung Hee University, South Korea. His research interests include machine learning, federated learning, self-supervised learning, and multimodal learning.
% \end{IEEEbiography}


% \begin{IEEEbiography}[{% Figure removed}]
% 	{\bf Minh N. H. Nguyen}  (M'20)
% 	(M'20) received the BE degree in Computer Science and Engineering from Ho Chi Minh City University of Technology, Vietnam, in 2013 and the Ph.D. degree in Computer Science and Engineering from Kyung Hee University, Seoul, South Korea, in 2020. He is currently working as a lecturer and researcher at The University of Danang â€“ Vietnam - Korea University of Information and Communication Technology, Vietnam. He received the best KHU Ph.D. thesis award in engineering in 2020. He is in charge of the Research Program at Digital Science and Technology Institute, VKU. He participated as Track Chairs for ATC 2023 Conference and CITA2023. His research interests include wireless communications, mobile edge computing, federated learning, natural language processing, and computer vision.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{% Figure removed}]
% 	{\bf Chu Myaet Thwal} received the B.E degree in Computer Engineering and Information Technology from Yangon Technological University, Myanmar in 2019. She is currently pursuing the Ph.D. degree in Computer Science and Engineering at Kyung Hee University, South Korea. Her research interests include Deep Learning, Artificial Intelligence, and Federated Learning.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{% Figure removed}]
% 	{\bf Yu Qiao}  received his B.E. degree in Internet of Things Engineering and M.E. degree in Computer Science and Technology from Nanjing University of Information Science and Technology (NUIST), Nanjing, China, in 2016 and 2019, respectively. He is currently pursuing the Ph.D. degree at the Department of Artificial Intelligence at Kyung Hee University (KHU), South Korea. Before his Ph.D., he also served as a Camera Software Engineer, Intermediate Software Engineer, Spreadtrum Communications (UNISOC), Shanghai, China, from 2019 to 2022. His interests include machine learning, federated learning, self-supervised learning, and distributed edge intelligence.
% \end{IEEEbiography}

% \begin{IEEEbiography}[{% Figure removed}]
% 	{\bf Chaoning Zhang}  received the B.E. and M.E. degree in Electrical Engineering from Harbin Institute of Technology, China, in 2012 and 2015, respectively, and the Ph.D. degree from KAIST in 2018. Since 2019, He has been an assistant professor at the department of Artificial Intelligence, School of Computing, Kyung Hee University. Prior to that, he worked as a post-doctoral researcher at KAIST. His research interests include but are not limited to adversarial machine learning and self-supervised learning for addressing model robustness and data efficiency issues in computer vision and beyond. 
% \end{IEEEbiography}

% \begin{IEEEbiography}[{% Figure removed}]
% 	{\bf Choong Seon Hong}
% 	(Sâ€™95-Mâ€™97-SMâ€™11) received the B.S. and M.S. degrees in electronic engineering from Kyung Hee University, Seoul, South Korea, in 1983 and 1985, respectively, and the Ph.D. degree from Keio University, Tokyo, Japan, in 1997. In 1988, he joined KT, Gyeonggi-do, South Korea, where he was involved in broadband networks as a member of the Technical Staff. Since 1993, he has been with Keio University. He was with the Telecommunications Network Laboratory, KT, as a Senior Member of Technical Staff and as the Director of the Networking Research Team until 1999. Since 1999, he has been a Professor with the Department of Computer Science and Engineering, Kyung Hee University. His research interests include future Internet, intelligent edge computing, network management, and network security. Dr. Hong is a member of the Association for Computing Machinery (ACM), the Institute of Electronics, Information and Communication Engineers (IEICE), the Information Processing Society of Japan (IPSJ), the Korean Institute of Information Scientists and Engineers (KIISE), the Korean Institute of Communications and Information Sciences (KICS), the Korean Information Processing Society (KIPS), and the Open Standards and ICT Association (OSIA). He has served as the General Chair, the TPC Chair/Member, or an Organizing Committee Member of international conferences, such as the Network Operations and Management Symposium (NOMS), International Symposium on Integrated Network Management (IM), Asia-Pacific Network Operations and Management Symposium (APNOMS), End-to-End Monitoring Techniques and Services (E2EMON), IEEE Consumer Communications and Networking Conference (CCNC), Assurance in Distributed Systems and Networks (ADSN), International Conference on Parallel Processing (ICPP), Data Integration and Mining (DIM), World Conference on Information Security Applications (WISA), Broadband Convergence Network (BcN), Telecommunication Information Networking Architecture (TINA), International Symposium on Applications and the Internet (SAINT), and International Conference on Information Networking (ICOIN). He was an Associate Editor of the IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT and the IEEE JOURNAL OF COMMUNICATIONS AND NETWORKS and an Associate Editor for the International Journal of Network Management and an Associate Technical Editor of the IEEE Communications Magazine. He currently serves as an Associate Editor for the International Journal of Network Management and Future Internet Journal.			
% \end{IEEEbiography}

\vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




\vfill

\end{document}


