%File: formatting-instructions-latex-2024.tex
%release 2024.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai24}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}

%
% These are are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2024.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai24.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{WC-SBERT: Zero-Shot Text Classification via SBERT with Self-Training for Wikipedia Categories}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\thanks{With help from the AAAI Publications Committee.}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    Te-Yu Chi\equalcontrib\textsuperscript{\rm 1},
    Yu-Meng Tang\equalcontrib\textsuperscript{\rm 1,\rm 2},
    Chia-Wen Lu\equalcontrib\textsuperscript{\rm 1},
    Qiu-Xia Zhang\equalcontrib\textsuperscript{\rm 1},
    Jyh-Shing Roger Jang\textsuperscript{\rm 1}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}National Taiwan University\\
    \textsuperscript{\rm 2}Tongji University\\
    d09922009@ntu.edu.tw, 
    tonmoregulus@gmail.com,
    b07610058@ntu.edu.tw,
    r10922164@ntu.edu.tw,
    jang@mirlab.org
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2}, 
    % J. Scott Penberthy\textsuperscript{\rm 3}, 
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    % 1900 Embarcadero Road, Suite 101\\
    % Palo Alto, California 94303-3310 USA\\
    % email address must be in roman text type, not monospace or sans serif
    % proceedings-questions@aaai.org
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1,\rm 2},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

\begin{abstract}
In the research domain of Natural Language Processing (NLP), the problems of intent and topic classification have consistently been fundamental and significant. These issues demand that machines comprehend the contextual meaning of a sentence, thereby addressing various language-related problems such as Question-Answering (QA) and Multi-dialogue. In many classification tasks, it is customary to rely on training datasets to enhance the model's accuracy. However, in practical business applications, it is sometimes challenging to gather a sufficient amount of data for training purposes.

Our research focuses on solving the zero-shot text classification problem in NLP, with a particular emphasis on innovative self-training strategies. To achieve this objective, we propose a novel self-training strategy that uses labels rather than text for training, significantly reducing the model's training time. Specifically, we use categories from Wikipedia as our training set and leverage the SBERT pre-trained model to establish positive correlations between pairs of categories within the same text, facilitating associative training.

For new test datasets, we have improved the original self-training approach, eliminating the need for prior training and testing data from each target dataset. Instead, we adopt Wikipedia as a unified training dataset to better approximate the zero-shot scenario. This modification allows for rapid fine-tuning and inference across different datasets, greatly reducing the time required for self-training.

Our experimental results demonstrate that this method can adapt the model to the target dataset within minutes. Compared to other BERT-based transformer models, our approach significantly reduces the amount of training data by training only on labels, not the actual text, and greatly improves training efficiency by utilizing a unified training set. Additionally, our method achieves state-of-the-art results on both the Yahoo Topic and AG News datasets.
\end{abstract}

\section{Introduction}

With the rapid advancement of Natural Language Processing (NLP) technologies, zero-shot text classification has become a hot research topic in this field. Zero-shot text classification is a unique text classification task, distinguished by its ability to classify text into predefined categories without any prior labeled data. This problem holds significant research value in NLP as it not only copes with known categories but also deals with newly appearing categories and texts from unknown domains, showcasing a remarkable generalization capability.

Despite the recent success achieved by \citet{selftrain} in using a self-training approach, wherein prediction results were used as labels on an unlabeled dataset, and these pseudo-labels were subsequently used to train the model through multiple iterations, this method still leaves room for further improvement and optimization.

This paper presents a novel self-training strategy, which focuses on training on labels rather than texts. This method not only reduces model inference time but also improves training efficiency. Our proposed strategy utilizes the SBERT \citep{reimers-2019-sentence-bert} model for sentence-level vector representation to train on labels by establishing their positive associations, rather than training on texts. During the self-training phase, we can quickly perform a large number of inferences, greatly reducing the inference time.

Our method was validated on the Yahoo Topic and AG News datasets and compared with existing self-training methods. Our experimental results show that by self-training on labels, our method can adjust a model to a target dataset in a matter of minutes, and compared to other BERT-based transformer models, our method can drastically reduce the volume of training data, thereby enhancing training efficiency.

We also compared our methods with the popular GPT and GPT fine-tune methods in the experiments. Directly using GPT for text classification is cost-effective but only provides average accuracy. On the other hand, fine-tuning on the superior Curie model provides preliminary accuracy close to the state of the art, but it's expensive and unsuitable for inference on large-scale datasets. In contrast, our model is more effective and requires fewer computational resources.

Therefore, our model has several contributions:

\begin{enumerate}
  \item It achieves state-of-the-art results on the Yahoo Topic and AG News datasets.
  \item Compared to self-training methods by \citeauthor{selftrain}, it does not require preliminary self-training on the target dataset's training set. Instead, it uses Wikipedia data for self-training. This allows universal application across datasets, greatly reducing self-training inference time by encoding Wikipedia data in advance and storing the embeddings in H5 format.
  \item Compared to other BERT-based transformer models that primarily train on texts, our study trains only on categories (labels), significantly reducing the amount of training data, thereby enhancing training efficiency and reducing computational resource consumption.
  
\end{enumerate}

The remaining part of this paper will further detail our self-training strategy, including the selection of the SBERT model, why we view Wikipedia page categories as labels, and how we fine-tune the model using the self-training strategy. Moreover, we will explain how we applied this method to our experiments, designed them to effectively validate our approach, and discuss our research findings and potential applications and limitations of our method. In summary, we hope this study can provide a novel, effective solution to the zero-shot intent classification problem. We also aim to stimulate more discussion and research on this topic.


\section{Related work}
Zero-shot text classification is a text classification task that has the characteristic of classifying text into predefined classes without prior labeled data. We focus on open-domain zero-shot text classification, which has better generalization capability compared to specific domains. It can not only classify text into known classes but also handle new classes and texts from unknown domains. \citet{chang2008} conducted the initial research on this task, referred to as "dataless classification" at that time. It is a method that relies solely on general knowledge for classification and does not require any domain-specific data. Since then, this task has gained significant attention.

With the advancement of deep neural networks, text classification has experienced a significant shift towards the use of pre-trained language models (PLMs) (\citeauthor{plm1}, \citeyear{plm1}; \citeauthor{plm2}, \citeyear{plm2}; \citeauthor{plm3}, \citeyear{plm3}). The first-generation PLMs, such as Word2Vec \citep{word2vec} and GloVe \citep{glove}, relied on word embeddings and typically classified the text based on word similarity. While these models were effective in capturing the semantic meaning of words and sentences, they lacked the ability to understand complex linguistic concepts and contextual information. In contrast, the second-generation PLMs, such as BERT \citep{bert2019}, GPT-2 \citep{glove}, and RoBERTa \citep{roberta}, are based on contextual embeddings. These models become mainstream techniques in text classification since they can capture the semantic meaning of words in different contexts and can be fine-tuned for this NLP task.

Recently, \citet{tewiki} as well as \citet{chu2020} both utilized Wikipedia data as training data to construct BERT-based classifiers. This choice is due to the vast amount of article data available in Wikipedia, which covers a wide range of general knowledge. Therefore, it is considered highly suitable for open-domain zero-shot text classification tasks. The datasets constructed by \citeauthor{chu2020} and \citeauthor{tewiki} contain 5.75 million documents and 1.19 million categories, and 3.3 million documents and 674 top-level categories, respectively. Compared to their works, we select Wikipedia dataset from Huggingface as data source. The dataset stands out for its substantial size, comprehensive coverage, and meticulous categorization. With over 6 million documents and more than 1.5 million categories, it has shown better performance on popular text classification datasets in our experiments.

Self-training is one of the commonly used techniques in the fields of semi-supervised \citep{semi} and unsupervised learning \citep{unsuper}. It is characterized by utilizing model predictions to expand the training dataset and iteratively improve the model through self-training. In recent studies (\citeauthor{zeroshot1}, \citeyear{zeroshot1}; \citeauthor{zeroshot2}, \citeyear{zeroshot2}; \citeauthor{zeroshot3}, \citeyear{zeroshot3}; \citeauthor{selftrain}, \citeyear{selftrain}) about self-training in zero-shot text classification, \citet{selftrain} overall achieved the best performance. Self-training involves pseudo-labeling the training and test sets of the target dataset. Subsequently, the model undergoes multiple iterations of training using these pseudo-labels to enhance its understanding of the dataset. While this approach has indeed demonstrated state-of-the-art results on multiple datasets, it requires prior examination of the data in each individual target dataset. In certain commercial contexts, this may not accurately represent the absence of zero-shot data, necessitating additional computational resources to train the model on different datasets. This study aims to improve the self-training approach in terms of dataset selection and performance.

\section{Methodology and approach}
Previous research has extensively investigated various methods and models for zero-shot intent classification problems. These approaches encompass BERT-based \citep{tewiki} methods that involve training on text and categories, as well as the utilization of generative models such as GPT \citep{gpt-zero} and T5 \citep{t5-zero} to guide the generation of desired answers. Among these approaches, the commonly adopted strategy involves training BERT on text and categories and utilizing entailment to ascertain the relationship between hypotheses and premises, classifying them as entailment, neutral, or contradiction. However, utilizing this approach requires training on the given text, which can be time-consuming.

Recent studies have employed a self-training approach \citep{selftrain}, wherein the original model undergoes iterative inference on the training set of the target dataset. Data instances surpassing a predetermined confidence threshold are then employed as training data for subsequent fine-tuning. Although this approach attains state-of-the-art performance in intent classification, it presents two challenges:

\begin{itemize}
    \item Acquiring pre-training data from diverse target datasets for the purpose of fine-tuning.
    \item Addressing the first challenge, performing inference on different datasets necessitates relatively more time for processing encoding embeddings. Notably, embedding processing constitutes one of the most time-consuming steps in BERT processing.
\end{itemize}

To overcome these aforementioned challenges, this study introduces a methodology that achieves state-of-the-art performance in zero-shot classification while resolving the aforementioned issues. The subsequent section outlines the methodology employed in this research.

\subsection{SBERT}
SBERT (\textit{Sentence-BERT}) \citep{reimers-2019-sentence-bert} is a method based on BERT (\textit{Bidirectional Encoder Representations from Transformers}) that calculates sentence-level vector representations. It utilizes a siamese architecture consisting of two BERT networks with shared weights, where each network processes an input sentence. These two BERT networks learn representations of the two sentences through parameter sharing, enabling comparison of sentence similarity using methods such as cosine similarity. Furthermore, in the fine-tuning process of SBERT for different downstream tasks, the choice of the loss function is crucial. In this study, we employ the multiple ranking loss (MNR Loss), which is particularly suitable when the training dataset contains only positive pairs. The MNR Loss brings positive data closer together in the vector space and pushes negative data further apart, thus forming clusters of similar sentences. Leveraging the architecture and training/inference optimizations of SBERT compared to traditional BERT, this study utilizes the pre-trained model \textit{all-mpnet-base-v2} from SBERT as the primary base pre-trained model.

We introduces a novel training approach for BERT-based models, shifting from the conventional training method to an association training based on model labels (categories). The purpose of the basic model fine-tuning stage is to fine-tune the SBERT-based model using the Wikipedia dataset to construct training data and obtain a generalized base model. Based on the provided URLs in the dataset, we identify the corresponding category for each data entry's text. Each data entry in the dataset is defined by three components: id \(i\), text \(t_i\), and the set \(C_i\) of categories to which the text belongs. Assume that $C_i = \left\{c_1, c_2,...,c_{n_i}  \right\}$, $n_i$ represents the size of the category set for the \(i^{th}\) data entry. We use each category in the set $C_i$ for pairwise combinations. For each data entry, we can obtain \(C^{n_i}_{2}\) pairs of classes as training data. The set \(P_i\) comprises pairs of classes for each data entry. Its definition is shown below. We select all \(P_i\) where $n_i \geq 2$ to construct the initial training set and perform fine-tuning on the pre-trained model. 

\begin{quote}
    $P_i = \left\{(c_j, c_k) | c_j, c_k \in C_i, c_j \neq c_k \right\}$
\end{quote}

Regarding the dataset, we utilize the Wikipedia dataset from Huggingface and assume that the content of each wiki, along with its associated categories, exhibits positive correlations based on their respective wiki page IDs. As a result, we modify the training approach of most BERT-based models for the given text and adopt associated training using model labels (categories). Since the length of labels is significantly shorter compared to the content of the text, this reduces training costs. We use the pre-trained SBERT model as the base model and treat the categories of each wiki page in \textit{wiki-cate} as labels. We combine these labels in pairs (non-repetitive combinations using Python's \textit{itertools.combinations}) and convert them into the SBERT training data format (\textit{InputExample}) to create the training dataset \textit{train-samples}. Pseudo code is shown below:

\begin{listing}[h]%
\caption{Pseudo code of generate SBERT training samples}
\label{lst:listing}%
\begin{lstlisting}[language=Python]
import itertools
from sentence_transformers import InputExample

train_samples = []

for data in wiki_cate:
    categories = data["categories"]
    for pair in list(itertools.combinations(categories, 2)):
        train_samples.append(InputExample(texts=[pair[0], pair[1]]))
\end{lstlisting}
\end{listing}

Through SBERT, we train on the aforementioned train-samples to obtain a general purpose foundational model called \textit{wiki-cate} SBERT model: \textit{WC-SBERT}.

\subsection{Wiki text embeddings}
% For different target tasks, we also adopt the concept of self-training. However, the difference lies in that we do not use the training dataset of the specific target task for self-training. Instead, we utilize the original 6.4 million Wikipedia texts for inference and fine-tuning. Since there is no need to obtain specific training sets for individual tasks, in this study, we encode the entire Wikipedia text (taking the first 200 words of each text) using the aforementioned WC-SBERT. We then pre-store all the embeddings in an h5 file called "wiki-text-embeddings.h5" using the Python library h5py.

% In various downstream target tasks, we only need to acquire the labels for the target task and retrieve the embeddings from the h5 file to perform similarity comparisons.

After the aforementioned data processing, the size of the dataset \(D\) is 6,458,670. We utilize the general base model WC-SBERT to encode the first 200 words of the text in each data point from \(D\). The resulting embeddings are stored in an h5 file for subsequent fine-tuning.

\subsection{Self-training}
% Relying on SBERT enables rapid similarity inference (within seconds) for each million data point. This allows for quick retrieval of similarity scores between the 6.4 million wiki data and their corresponding labels. Similarly, the categories of wiki pages with similarity scores above a threshold are selected as data for each iteration. Fine-tuning is then performed based on the original base model to adapt it to downstream tasks for evaluation. The fine-tuning process for downstream tasks can be completed within approximately 5 minutes, significantly reducing the time required during the self-training phase.

% By following the aforementioned methodology, we can simultaneously address the concerns associated with self-training, eliminate the need for pre-examining target training datasets, and enhance the efficiency of each iteration in self-training. By training solely based on labels, this approach reduces training costs compared to traditional BERT-based models trained on the given text. Furthermore, it achieves state-of-the-art performance on the target dataset.

In the self-training fine-tune stage, we define the target label set as \(L\). Initially, we use WC-SBERT to calculate the similarity between each text and all elements \(l\) in the label set \(L\). Then, for each text \(t_i\), we select the label \(l_j\) from \(L\) that has the highest similarity score with \(t_i\) to obtain the similarity score $similarity (t_i, l_j)$ between \(t_i\) and \(l_j\), and compare the similarity between them. Next, we compare the similarity with the set threshold. If the similarity is greater than the threshold, we pair all elements ($c_1, c_2,..., c_{n_i}$) of the class set \(C_i\) of \(t_i\) with the target label \(l_j\) to get $(c_1, l_j), (c_2, l_j),..., (c_{n_i}, l_j)$, all text categories-target label pairs are used as training data for the fine-tuning stage. Since we have already encoded the text and stored it in an h5 file in the previous step, during the similarity calculation stage, we only need to calculate the embeddings of the target labels, and the text embeddings can be directly retrieved from the h5 file. With the advantage of SBERT's fast similarity inference, we can quickly obtain $similarity (t_i, l_j)$ for each million data points.

% Figure environment removed

As shown in the Figure \ref{self-train_fine-tine}, we use the method described above to obtain the training data and fine-tune the original model \(M_{base}\), resulting in model \(M_1\). We then continue to use the same method, using \(M_1\) to calculate similarity scores and filter out the data for further fine-tuning of \(M_{base}\), obtaining model \(M_2\). We conduct this process iteratively, until we obtain the final model \(M_{final}\).


% \section{Experimental setup}
\section{Dataset}
\subsection{Training dataset}
This study utilized the train set of Wikipedia dataset from Huggingface as the training set for the pre-trained model. The dataset consists of a total of 6,417,006 records, with fields including wiki page ID, URL, title, and text. Since this dataset does not include the categories for each page, a separate web scraping program was designed in this study to retrieve the categories for each page. A total of 1,563,193 categories were collected, and the combination of these categories with the original dataset was named the \textit{wiki-cate} dataset. Here are some examples of the data:

\begin{listing}[h]%
\caption{Wikipedia sample data format}
\label{lst-wiki-data-format}%
\begin{lstlisting}
"id": 12,
"url": "https://en.wikipedia.org/wiki/Anarchism",
"title": "Anarchism",
"text": "Anarchism is a political philosophy and movement...",
"categories": ["Anarchism", "Anti-capitalism", "Anti-fascism", "..."]
\end{lstlisting}
\end{listing}

\subsection{Evaluation dataset (target dataset)}
\begin{itemize}
    \item \textbf{AG News (AG's News Corpus)}: The AG News dataset \citep{yahoo-agnews} consists of news titles and descriptions, categorized into four classes: \textit{World}, \textit{Sports}, \textit{Business}, and \textit{Sci/Tech}. This study utilizes the AG News dataset from Huggingface, with a total of 7,600 data points in the test set. \textit{Sci/Tech} is further divided into two classes (\textit{Science} and \textit{Technology}) for classification purposes.
    \item \textbf{Yahoo! Answers}: The Yahoo! Answers dataset \citep{yahoo-agnews} mainly comprises questions and answers from the Yahoo! platform. In this study, Yahoo! Answers dataset from Huggingface is employed, containing 60,000 data points in the test set. There are ten main categories for the topics: \textit{Society \& Culture}, \textit{Science \& Mathematics}, \textit{Health}, \textit{Education \& Reference}, \textit{Computers \& Internet}, \textit{Sports}, \textit{Business \& Finance}, \textit{Entertainment \& Music}, \textit{Family \& Relationships}, and \textit{Politics \& Government}. Each label is individually treated as a single label for classification (splitted by the '\&' character), and the output is mapped back to the original label. 
    \item \textbf{DBpedia}: The DBpedia dataset \citep{dbpedia} is derived from structured information extracted from Wikipedia. In this study, the DBpedia dataset \textit{dbpedia\_14} from Huggingface is used as the experimental subject, consisting of 70,000 data points in the test set. It includes 14 distinct categories, namely \textit{Company}, \textit{EducationInstitution}, \textit{Artist}, \textit{Athlete}, \textit{OfficeHolder}, \textit{MeanOfTransportation}, \textit{Building}, \textit{NaturalPlace}, \textit{Village}, \textit{Animal}, \textit{Plant}, \textit{Album}, \textit{Film}, and \textit{WrittenWork}. We will split the partial classification into words by separating them with a space as input: \textit{EducationInstitution} to \textit{Education institution}, \textit{OfficeHolder} to \textit{Office holder}, \textit{MeanOfTransportation} to \textit{Mean of transportation}, \textit{NaturalPlace} to \textit{Nature place} and \textit{WrittenWork} to \textit{Written work}.
    \label{dbpedia}
\end{itemize}

In this study, the labels of the test dataset are pre-combined with prompts to perform actual classification. Table \ref{table-ds-prompt} shows the prompts for each target dataset.

\begin{table}[h]
\centering
%\resizebox{.95\columnwidth}{!}{
\begin{tabular}{l|l}
    \hline
    \textbf{Dataset} & \textbf{Prompt} \\
    \hline
    AG News & This topic is talk about \{label\}.\\
    Yahoo! Answers & This topic is talk about \{label\}.\\
    DBpedia & This sentence is belong to \{label\}.\\
    \hline
\end{tabular}
\caption{\textbf{Prompts used in the target datasets.}}
\label{table-ds-prompt}
\end{table}



\section{Experiments}
This study prioritizes conducting relevant experiments using the AG News dataset as the target dataset. The findings and techniques are then applied to other target datasets, such as Yahoo topic and DBpedia, for evaluation purposes.

\subsection{Implementation Details}
The parameters used for SBERT experiments are as follows:

\begin{itemize}
    \item Pre-trained model: all-mpnet-base-v2
    \item Train batch size: 128
    \item Sequence length (token length): 128
    \item Epoch: 1
    \item Loss function: Multiple Negative Ranking (MNR) Loss
\end{itemize}

\subsection{Prompt evaluation}
In recent research, Prompt tuning \citep{schick2020exploiting} has shown to significantly improve the accuracy of zero-shot or few-shot classification. We aim to investigate whether Prompt tuning can also enhance the similarity matching of the SBERT-based sentence similarity model, WC-SBERT, which is trained solely on labels. In our experiments, we compare the performance of WC-SBERT using a simple hard prompt \citep{liu2022p} ("This topic is talk about [\textit{Label}]") with that of using only the label for comparison. Figure \ref{fig-prompt-evaluation} shows the evaluation results of WC-SBERT on the 7,600 test set of AG News dataset.

% Figure environment removed

\subsection{Query label mapping evaluation}
WC-SBERT is pre-trained using wiki categories as labels. We are particularly interested in whether direct comparison of the target test set's queries and labels should be conducted, or if an intermediate step is required. This intermediate step involves mapping the queries to the closest category within the wiki categories and then performing the category-label matching. Intuitively, the presence of seen categories during WC-SBERT's training process should potentially improve accuracy.

In the experiment, we compare each text (query) in the AG News test set with all wiki categories using cosine similarity. We obtain the category closest to this query as input (for example, query: \textit{Fears for T N pension after talks Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.} corresponds to category: \textit{Transport and General Workers' Union}). We then perform a second round of cosine similarity between this category and the labels that should be output for the test. The experimental results can be referred to the Figure \ref{fig-label-mapping-evaluation}.

% Figure environment removed

\subsection{Self-training}
The self-training experiments can be divided into three parts to assess the impact of their configurations on accuracy:

\begin{itemize}
    \item \(i\): The number of iterations
    \item \(m\): The choice of the model for fine-tuning
    \item \(t\): The setting of the similarity score threshold
\end{itemize}

This study conducted relevant experiments on the AG News test set. Detailed results can be referred to Figure \ref{fig-self-training}.

% Figure environment removed

\subsection{Self-training performance comparison}

Continuing from the previous experiment, Self-training relies on iterative training to increase accuracy. The iterative process consists of two main parts: inference on the target training and testing sets, and fine-tuning based on the inference results. We conducted the experiments using WC-SBERT on different datasets and reproduced the current best model in \citet{selftrain} approach. The experiment involved 2 rounds of fine-tuning and 3 rounds of inference to compare the time cost of inference and fine-tuning between the two methods, thereby evaluating their performance.

To ensure the experiment's fairness, all tests were conducted using the same hardware environment with the following specifications: CPU: Intel(R) Xeon(R) Gold 6154 (8 cores), GPU: NVIDIA Tesla V100 * 2, RAM: 128 GB, OS: Ubuntu 20.04 LTS. The experimental results are presented in Table \ref{speed-comparison}.

\subsection{GPT experiment}

We have also used the currently popular GPT-3.5 model and the GPT-4+fine-tune model provided by the API to test the effects of zero-shot text classification. Compared to the GPT-4 model, the GPT-3.5 model is more affordable, allowing us to test on several datasets. For GPT-3.5, the prompt template we used is as follows:

\begin{listing}[h]%
\caption{Prompt sample with GPT 3.5}
\label{gpt-prompt-listing}%
\begin{lstlisting}
There are 10 classes below:
    Social & Culture
    Science & Mathematics
    Health
    ...
This text is belong to which class: 
Which celebrity smokes the most weed? snoop dog of course
\end{lstlisting}
\end{listing}


Using the aforementioned prompt for inference on the GPT-3.5 model, the results are presented in the experiment result section. 

Moreover, we also tried using the GPT-4 + fine-tune provided by OpenAI API to address the zero-shot text classification problem. However, due to its high cost, we only used 6,700 Wikipedia entries (split into 5,700 training data and 1,000 validation data) to fine-tune the GPT-4 model. We then randomly selected 300 entries from the Yahoo topic as a test set. If we fine-tuned the GPT-4 model with all 5,700 training data entries at once, the resulting test accuracy was 0.55, far from state of the art. However, OpenAI API also offers the feature to fine-tune an already fine-tuned model, meaning you can fine-tune a model a second time with a smaller dataset. By dividing the 5,700 entries into 5,000 for the first round of fine-tuning and 700 for the second round, we preliminarily achieved a test accuracy of 0.62. Of course, due to the high cost of using the Curie model, we could only preliminarily test on a few hundred entries. This GPT fine-tuning approach is also challenging to apply to large-scale datasets for zero-shot text classification.

\subsection{Experiment results}
From the above experiments, several inferences can be drawn:
\begin{itemize}
    \item SBERT-based similarity models, when combined with labels as the pre-training and fine-tuning objective, still show sensitivity to the use of prompts. Using prompts leads to better accuracy.
    \item From the label mapping experiment, we have shattered conventional intuition. Directly comparing the query with the target dataset's labels using SBERT's similarity yields better results than mapping the query first.
    \item Self-training effectively improves accuracy within 1 to 2 iterations.
    \item Compared to the current best model in Gera et al. (2022), WC-SBERT demonstrates significant improvements in both inference and fine-tuning performance. During the inference phase, WC-SBERT achieves a remarkable reduction in time across different datasets, ranging from 10 to 40 times faster. In the fine-tuning phase, Self-Gera only utilizes 800 samples from the target set, whereas WC-SBERT employs 6.45 million samples from WIKI for fine-tuning. Despite the difference in dataset sizes, WC-SBERT's overall fine-tuning time remains lower than that of Self-Gera, resulting in a performance boost of 5,000 to 9,700 times.
    \item GPT-3.5 model does not achieve state-of-the-art performance on several datasets, shows poor results, and has higher costs compared to other models. Currently, it is not suitable for use in zero-shot text classification problems.
\end{itemize}

Based on the above inferences, we adopt this framework as our foundational model architecture and conduct experiments on different target datasets as shown in the Table \ref{table-exp-result}.

% \subsubsection{Speed Comparison}
\begin{table*}[h]
\scriptsize
\centering
\begin{tabular}{l|ccc|ccc|ccc}
\hline
    \textbf{Dataset} & \multicolumn{3}{c}{\textbf{AG News}}    & \multicolumn{3}{c}{\textbf{DBpedia}} & \multicolumn{3}{c}{\textbf{Yahoo}} \\
\hline
    Model & \multicolumn{1}{c}{Self-Gera} & \multicolumn{1}{c}{WC-SBERT} & \multicolumn{1}{c}{time ratio} & \multicolumn{1}{c}{Self-Gera} & \multicolumn{1}{c}{WC-SBERT} & \multicolumn{1}{c}{time ratio} & \multicolumn{1}{c}{Self-Gera} & \multicolumn{1}{c}{WC-SBERT} & \multicolumn{1}{c}{time ratio}  \\
\hline
Inference (\(i\)=0) & 204 & 20 & 10.2 & 7939 & 304 & 26.12 & 13201                & 300 & 44.00 \\
Inference (\(i\)=1) & 207 & 20 & 10.35 & 8077 & 299 & 27.01 & 11525               & 300 & 38.42 \\
Inference (\(i\)=2) & 206 & 20 & 10.3 & 8197 & 300 & 27.32 & 11493                & 300 & 38.31 \\
Total time (sec.) & 617 & 59 & 10.46 & 24213 & 904 & 26.78 &              36219 & 901 & 40.20 \\
\#sample      & 7600 & 7600 & - & 70000 & 70000 & - & 58966                 & 60000 & - \\
Avg. time (sec.)  & 0.0271 & 0.0026 & \textbf{10.42} & 0.1153                   & 0.0043 & \textbf{26.79} & 0.2047                            & 0.0050 & \textbf{40.92} \\
\hline
Fine-tune (\(i\)=1) & 110 & 91 & 1.21 & 395 & 110 & 3.60 & 221                    & 133 & 1.66 \\
Fine-tune (\(i\)=2) & 106 & 87 & 1.22 & 391 & 124 & 3.15 & 218                    & 134 & 1.63 \\
Total time (sec.)  & 216 & 179 & 1.21 & 785 & 234 & 3.35 & 438                   & 267 & 1.64 \\
\#sample       & 800 & 6458670 & - & 2800 & 6458670 & - 
               & 2000 & 6458670 & - \\
Avg. time (sec./100 samples)   & 13.5039 & 0.0014 & \textbf{9766.34} 
               & 14.0262 & 0.0018 & \textbf{7738.85} 
               & 10.9610 & 0.0021 & \textbf{5219.52} \\
\hline
\end{tabular}
\caption{\textbf{Self-training speed comparison between WC-SBERT and Self-Gera.} We compare the training and inference time between our WC-SBERT and self-training model in \citet{selftrain}. The backbone of the former model is \textit{all-mpnet-base-v2}, while the latter model uses \textit{Narsil/deberta-large-mnli-zero-cls} as its backbone.  \(i\) denotes iteration of self-training, "Inference Avg. time" represents the average time spent per sample data, "Fine-tune Avg. time" represents the average time spent per 100 sample data, and "time ratio" is the time of Self-Gera divided by the time of WC-SBERT. The numerical values presented in the table are rounded to the nearest integer to show the measurement errors.}
\label{speed-comparison}
\end{table*}


\begin{table*}[h]
\scriptsize
\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l}
\hline
\textbf{Label index}&0&1&2&3&4&5&6&7&8&9&10&11&12&13&\(A.\) \\
\hline
0 (Company)&3115&80&52&28&23&517&118&60&21&12&113&579&186&96&62.3 \\
1 (EducationInstitution)&14&4768&55&11&7&2&27&16&64&6&7&6&14&3&95.36 \\
2 (Artist)&38&14&1348&11&18&0&38&18&14&25&12&2254&446&764&26.96 \\
3 (Athlete)&7&0&4&4950&1&0&4&1&2&3&0&23&4&1&99.00 \\
4 (OfficeHolder)&260&117&44&54&3757&21&245&56&239&51&14&8&32&102&75.14 \\
5 (MeanOfTransportation)&1088&0&1&17&0&3381&16&480&8&6&1&1&1&0&67.62 \\
6 (Building)&79&239&89&5&3&33&2347&791&1376&11&10&2&9&6&46.94 \\
7 (NaturalPlace)&0&0&0&0&0&2&1&4976&21&0&0&0&0&0&99.52 \\
8 (Village)&0&2&0&0&0&0&0&363&4635&0&0&0&0&0&92.70 \\
9 (Animal)&0&0&0&17&0&0&0&67&0&1972&2944&0&0&0&39.44 \\
10 (Plant)&8&0&0&0&0&1&0&6&0&3&4979&0&3&0&99.58 \\
11 (Album)&0&0&1&1&0&0&0&0&0&0&0&4996&1&1&99.92 \\
12 (Film)&2&0&4&1&0&0&1&3&3&5&0&442&4538&1&90.76 \\
13 (WrittenWork)&264&53&273&5&61&15&91&34&112&208&118&275&1293&2198&43.96 \\
\hline
\end{tabular}
%}
\caption{ \textbf{Error analysis of DBpedia (without description prompts).} The predicted labels by our WC-SBERT are presented as columns, while the actual labels are presented in rows. The rightmost column displays the test accuracy for each label. Results with desciption prompts is shows in Table \ref{table-dbpedia-confusion-matrix-2} in the appendix.}
\label{table-dbpedia-confusion-matrix-1}
\end{table*}




\begin{table}[h]
\small
\centering
\begin{tabular}{l|l|l|l}
\hline
\textbf{Dataset} & \textbf{AG News} & \textbf{Yahoo} & \textbf{DBpedia} \\
\hline
WC-SBERT & \textbf{0.815} & \textbf{0.627} & 0.7481 \\
& (\(i\)=2, \(t\)=0.8) & (\(i\)=1, \(t\)=0.8) & (\(i\)=1, \(t\)=0.7) \\
Wiki-Ding & 0.796 & 0.573 & 0.902 \\
Self-Gera & 0.814 & 0.62 & \textbf{0.945} \\
GPT 3.5 & 0.71 & 0.597 & N/A \\
\hline
\end{tabular}
\caption{\textbf{Evaluation results of different target datasets with prompts.} The numbers denote test accuracy of our model and three baselines: \citet{tewiki}, \citet{selftrain}, and GPT3.5. Due to the cost of GPT-3.5, we do not perform GPT-3.5 inference on the DBPedia test set, which consists of a total of 70,000 samples.}
\label{table-exp-result}
\end{table}


\section{Discussion}

Based on the experimental results, it can be observed that WC-SBERT achieves state-of-the-art performance in zero-shot topic intent classification tasks such as AG News and Yahoo! topic, while significantly reducing the training time required for self-training. However, when facing the DBpedia task, it did not perform well, achieving only 0.74 accuracy. Therefore, we conducted further error analysis specifically for DBpedia in an attempt to understand the reasons affecting accuracy.

Table \ref{table-dbpedia-confusion-matrix-1} shows that certain labels of DBpedia are incorrectly classified. For example, although there should be 5,000 instances classified as \textit{Animal} in the actual data, the prediction results include 2,944 instances misclassified as \textit{Plant}. We believe this situation may be due to the presence of ambiguous definitions in some labels. Based on the definitions and content of the dataset, \textit{Artist} can encompass professionals in various fields, including painters, musicians, and producers. \textit{Album} refers to music-related albums, and \textit{Film} may cover music and other audiovisual content as well. It is understandable that confusion and ambiguity may arise from the label definitions.

In earlier experiments, we realized the importance of prompts on SBERT's performance in classification tasks. Therefore, we defined specific prompt formats for each label in DBpedia: "This {description} described in this content is {label}." We added explicit keywords and self-defined descriptions for each label, aiming to enhance discriminability by distinguishing the characteristics of each label. For example, the description of \textit{Athelete} is "This person who plays sport described in this content is an athlete." Other prompts with descriptions of DBpedia can be referred to Table \ref{table-descri} in the appendix. Using the updated prompts, we performed inference with the same model again, and the accuracy increased from 0.7423 to 0.8746. Although there is still a slight difference in accuracy compared to results achieved by \citeauthor{selftrain}, they requires access to the training and test datasets for downstream fine-tuning tasks. In contrast, our study only use data from Wikipedia, resulting in an improvement of nearly 20\% in accuracy compared to the original 0.74. Based on the results of the error analysis, in addition to SBERT's sensitivity to prompts, the predictive ability for classification tasks can also be enhanced by adding keywords and descriptions to the labels.

We also employ description prompts on both AG News and Yahoo! Topics using the same approach. We incorporated specific descriptions for a few categories. Based on previous experiments, we understand that SBERT utilizes contextual understanding to compare the similarity or distance between two sentences. In an attempt to increase the distinction between two ambiguous categories, we injected reverse implications by using the term "not." For example, in the case of AG News, we modified the original prompt to "This topic is talk about World, not Business." All prompts are presented in Table \ref{table-prompt-agnews} and \ref{table-prompt-yahoo} in the appendix. The accuracy improved as shown in Table \ref{table-prompt-result}; however, further manual adjustments are needed to refine the prompt definition and descriptions for better effectiveness. One of our future works is to explore how to automatically generate appropriate prompts for different datasets.

\begin{table}[h]
\centering
%\resizebox{.95\columnwidth}{!}{
\begin{tabular}{l|l|l|l}
\hline
\textbf{Dataset} & \textbf{AG News} & \textbf{Yahoo} & \textbf{DBpedia} \\
\hline
Default prompt & 0.815 & 0.627 & 0.7481 \\
Description prompt & \textbf{0.837} & \textbf{0.634} & \textbf{0.8746} \\
\hline
\end{tabular}
\caption{\textbf{Test set accuracy of the default and description prompts.}}
\label{table-prompt-result}
\end{table}

\section{Conclusion}
This paper introduces a novel model for zero-shot text classification that is adaptable to open-domain datasets. Traditional methods typically involve extensive training on large amounts of text. In our approach, we propose WC-SBERT, a combination of SBERT with the Wikipedia dataset, and utilize self-training to train the Wikipedia categories. This approach significantly improves training efficiency and reduces computing time. Our experimental results demonstrate that WC-SBERT substantially decreases the training and inference time per data point and achieves state-of-the-art performance on popular text classification datasets, such as AG News and Yahoo.

% \section{Copyright}
% All papers submitted for publication by AAAI Press must be accompanied by a valid signed copyright form. They must also contain the AAAI copyright notice at the bottom of the first page of the paper. There are no exceptions to these requirements. If you fail to provide us with a signed copyright form or disable the copyright notice, we will be unable to publish your paper. There are \textbf{no exceptions} to this policy. You will find a PDF version of the AAAI copyright form in the AAAI AuthorKit. Please see the specific instructions for your conference for submission details.

% \section{Formatting Requirements in Brief}
% We need source and PDF files that can be used in a variety of ways and can be output on a variety of devices. The design and appearance of the paper is strictly governed by the aaai style file (aaai24.sty).
% \textbf{You must not make any changes to the aaai style file, nor use any commands, packages, style files, or macros within your own paper that alter that design, including, but not limited to spacing, floats, margins, fonts, font size, and appearance.} AAAI imposes requirements on your source and PDF files that must be followed. Most of these requirements are based on our efforts to standardize conference manuscript properties and layout. All papers submitted to AAAI for publication will be recompiled for standardization purposes. Consequently, every paper submission must comply with the following requirements:

% \begin{itemize}
% \item Your .tex file must compile in PDF\LaTeX{} --- (you may not include .ps or .eps figure files.)
% \item All fonts must be embedded in the PDF file --- including your figures.
% \item Modifications to the style file, whether directly or via commands in your document may not ever be made, most especially when made in an effort to avoid extra page charges or make your paper fit in a specific number of pages.
% \item No type 3 fonts may be used (even in illustrations).
% \item You may not alter the spacing above and below captions, figures, headings, and subheadings.
% \item You may not alter the font sizes of text elements, footnotes, heading elements, captions, or title information (for references and mathematics, please see the limited exceptions provided herein).
% \item You may not alter the line spacing of text.
% \item Your title must follow Title Case capitalization rules (not sentence case).
% \item \LaTeX{} documents must use the Times or Nimbus font package (you may not use Computer Modern for the text of your paper).
% \item No \LaTeX{} 209 documents may be used or submitted.
% \item Your source must not require use of fonts for non-Roman alphabets within the text itself. If your paper includes symbols in other languages (such as, but not limited to, Arabic, Chinese, Hebrew, Japanese, Thai, Russian and other Cyrillic languages), you must restrict their use to bit-mapped figures. Fonts that require non-English language support (CID and Identity-H) must be converted to outlines or 300 dpi bitmap or removed from the document (even if they are in a graphics file embedded in the document).
% \item Two-column format in AAAI style is required for all papers.
% \item The paper size for final submission must be US letter without exception.
% \item The source file must exactly match the PDF.
% \item The document margins may not be exceeded (no overfull boxes).
% \item The number of pages and the file size must be as specified for your event.
% \item No document may be password protected.
% \item Neither the PDFs nor the source may contain any embedded links or bookmarks (no hyperref or navigator packages).
% \item Your source and PDF must not have any page numbers, footers, or headers (no pagestyle commands).
% \item Your PDF must be compatible with Acrobat 5 or higher.
% \item Your \LaTeX{} source file (excluding references) must consist of a \textbf{single} file (use of the ``input" command is not allowed.
% \item Your graphics must be sized appropriately outside of \LaTeX{} (do not use the ``clip" or ``trim'' command) .
% \end{itemize}

% If you do not follow these requirements, your paper will be returned to you to correct the deficiencies.

% \section{What Files to Submit}
% You must submit the following items to ensure that your paper is published:
% \begin{itemize}
% \item A fully-compliant PDF file.
% \item Your \LaTeX{} source file submitted as a \textbf{single} .tex file (do not use the ``input" command to include sections of your paper --- every section must be in the single source file). (The only allowable exception is .bib file, which should be included separately).
% \item The bibliography (.bib) file(s).
% \item Your source must compile on our system, which includes only standard \LaTeX{} 2020 TeXLive support files.
% \item Only the graphics files used in compiling paper.
% \item The \LaTeX{}-generated files (e.g. .aux,  .bbl file, PDF, etc.).
% \end{itemize}

% Your \LaTeX{} source will be reviewed and recompiled on our system (if it does not compile, your paper will be returned to you. \textbf{Do not submit your source in multiple text files.} Your single \LaTeX{} source file must include all your text, your bibliography (formatted using aaai24.bst), and any custom macros.

% Your files should work without any supporting files (other than the program itself) on any computer with a standard \LaTeX{} distribution.

% \textbf{Do not send files that are not actually used in the paper.} Avoid including any files not needed for compiling your paper, including, for example, this instructions file, unused graphics files, style files, additional material sent for the purpose of the paper review, intermediate build files and so forth.
% \textbf{Obsolete style files.} The commands for some common packages (such as some used for algorithms), may have changed. Please be certain that you are not compiling your paper using old or obsolete style files.

% \textbf{Final Archive.} Place your source files in a single archive which should be compressed using .zip. The final file size may not exceed 10 MB.
% Name your source file with the last (family) name of the first author, even if that is not you.


% \section{Using \LaTeX{} to Format Your Paper}

% The latest version of the AAAI style file is available on AAAI's website. Download this file and place it in the \TeX\ search path. Placing it in the same directory as the paper should also work. You must download the latest version of the complete AAAI Author Kit so that you will have the latest instruction set and style file.

% \subsection{Document Preamble}

% In the \LaTeX{} source for your paper, you \textbf{must} place the following lines as shown in the example in this subsection. This command set-up is for three authors. Add or subtract author and address lines as necessary, and uncomment the portions that apply to you. In most instances, this is all you need to do to format your paper in the Times font. The helvet package will cause Helvetica to be used for sans serif. These files are part of the PSNFSS2e package, which is freely available from many Internet sites (and is often part of a standard installation).

% Leave the setcounter for section number depth commented out and set at 0 unless you want to add section numbers to your paper. If you do add section numbers, you must uncomment this line and change the number to 1 (for section numbers), or 2 (for section and subsection numbers). The style file will not work properly with numbering of subsubsections, so do not use a number higher than 2.

% \subsubsection{The Following Must Appear in Your Preamble}
% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% \documentclass[letterpaper]{article}
% % DO NOT CHANGE THIS
% \usepackage[submission]{aaai24} % DO NOT CHANGE THIS
% \usepackage{times} % DO NOT CHANGE THIS
% \usepackage{helvet} % DO NOT CHANGE THIS
% \usepackage{courier} % DO NOT CHANGE THIS
% \usepackage[hyphens]{url} % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS
% \urlstyle{rm} % DO NOT CHANGE THIS
% \def\UrlFont{\rm} % DO NOT CHANGE THIS
% \usepackage{graphicx}  % DO NOT CHANGE THIS
% \usepackage{natbib}  % DO NOT CHANGE THIS
% \usepackage{caption}  % DO NOT CHANGE THIS
% \frenchspacing % DO NOT CHANGE THIS
% \setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
% \setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
% %
% % Keep the \pdfinfo as shown here. There's no need
% % for you to add the /Title and /Author tags.
% \pdfinfo{
% /TemplateVersion (2024.1)
% }
% \end{verbatim}\end{scriptsize}
% \end{quote}

% \subsection{Preparing Your Paper}

% After the preamble above, you should prepare your paper as follows:
% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% \begin{document}
% \maketitle
% \begin{abstract}
% %...
% \end{abstract}\end{verbatim}\end{scriptsize}
% \end{quote}

% \noindent You should then continue with the body of your paper. Your paper must conclude with the references, which should be inserted as follows:
% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% % References and End of Paper
% % These lines must be placed at the end of your paper
% \bibliography{Bibliography-File}
% \end{document}
% \end{verbatim}\end{scriptsize}
% \end{quote}


% \begin{quote}
% \begin{scriptsize}\begin{verbatim}
% \begin{document}\\
% \maketitle\\
% ...\\
% \bibliography{Bibliography-File}\\
% \end{document}\\
% \end{verbatim}\end{scriptsize}
% \end{quote}

% \subsection{Commands and Packages That May Not Be Used}
% \begin{table*}[t]
% \centering

% \begin{tabular}{l|l|l|l}
% \textbackslash abovecaption &
% \textbackslash abovedisplay &
% \textbackslash addevensidemargin &
% \textbackslash addsidemargin \\
% \textbackslash addtolength &
% \textbackslash baselinestretch &
% \textbackslash belowcaption &
% \textbackslash belowdisplay \\
% \textbackslash break &
% \textbackslash clearpage &
% \textbackslash clip &
% \textbackslash columnsep \\
% \textbackslash float &
% \textbackslash input &
% \textbackslash input &
% \textbackslash linespread \\
% \textbackslash newpage &
% \textbackslash pagebreak &
% \textbackslash renewcommand &
% \textbackslash setlength \\
% \textbackslash text height &
% \textbackslash tiny &
% \textbackslash top margin &
% \textbackslash trim \\
% \textbackslash vskip\{- &
% \textbackslash vspace\{- \\
% \end{tabular}
% %}
% \caption{Commands that must not be used}
% \label{table1}
% \end{table*}

% \begin{table}[t]
% \centering
% %\resizebox{.95\columnwidth}{!}{
% \begin{tabular}{l|l|l|l}
%     authblk & babel & cjk & dvips \\
%     epsf & epsfig & euler & float \\
%     fullpage & geometry & graphics & hyperref \\
%     layout & linespread & lmodern & maltepaper \\
%     navigator & pdfcomment & pgfplots & psfig \\
%     pstricks & t1enc & titlesec & tocbind \\
%     ulem
% \end{tabular}
% \caption{LaTeX style packages that must not be used.}
% \label{table2}
% \end{table}

% There are a number of packages, commands, scripts, and macros that are incompatable with aaai24.sty. The common ones are listed in tables \ref{table1} and \ref{table2}. Generally, if a command, package, script, or macro alters floats, margins, fonts, sizing, linespacing, or the presentation of the references and citations, it is unacceptable. Note that negative vskip and vspace may not be used except in certain rare occurances, and may never be used around tables, figures, captions, sections, subsections, subsubsections, or references.


% \subsection{Page Breaks}
% For your final camera ready copy, you must not use any page break commands. References must flow directly after the text without breaks. Note that some conferences require references to be on a separate page during the review process. AAAI Press, however, does not require this condition for the final paper.


% \subsection{Paper Size, Margins, and Column Width}
% Papers must be formatted to print in two-column format on 8.5 x 11 inch US letter-sized paper. The margins must be exactly as follows:
% \begin{itemize}
% \item Top margin: .75 inches
% \item Left margin: .75 inches
% \item Right margin: .75 inches
% \item Bottom margin: 1.25 inches
% \end{itemize}


% The default paper size in most installations of \LaTeX{} is A4. However, because we require that your electronic paper be formatted in US letter size, the preamble we have provided includes commands that alter the default to US letter size. Please note that using any other package to alter page size (such as, but not limited to the Geometry package) will result in your final paper being returned to you for correction.


% \subsubsection{Column Width and Margins.}
% To ensure maximum readability, your paper must include two columns. Each column should be 3.3 inches wide (slightly more than 3.25 inches), with a .375 inch (.952 cm) gutter of white space between the two columns. The aaai24.sty file will automatically create these columns for you.

% \subsection{Overlength Papers}
% If your paper is too long and you resort to formatting tricks to make it fit, it is quite likely that it will be returned to you. The best way to retain readability if the paper is overlength is to cut text, figures, or tables. There are a few acceptable ways to reduce paper size that don't affect readability. First, turn on \textbackslash frenchspacing, which will reduce the space after periods. Next, move all your figures and tables to the top of the page. Consider removing less important portions of a figure. If you use \textbackslash centering instead of \textbackslash begin\{center\} in your figure environment, you can also buy some space. For mathematical environments, you may reduce fontsize {\bf but not below 6.5 point}.


% Commands that alter page layout are forbidden. These include \textbackslash columnsep,  \textbackslash float, \textbackslash topmargin, \textbackslash topskip, \textbackslash textheight, \textbackslash textwidth, \textbackslash oddsidemargin, and \textbackslash evensizemargin (this list is not exhaustive). If you alter page layout, you will be required to pay the page fee. Other commands that are questionable and may cause your paper to be rejected include \textbackslash parindent, and \textbackslash parskip. Commands that alter the space between sections are forbidden. The title sec package is not allowed. Regardless of the above, if your paper is obviously ``squeezed" it is not going to to be accepted. Options for reducing the length of a paper include reducing the size of your graphics, cutting text, or paying the extra page charge (if it is offered).


% \subsection{Type Font and Size}
% Your paper must be formatted in Times Roman or Nimbus. We will not accept papers formatted using Computer Modern or Palatino or some other font as the text or heading typeface. Sans serif, when used, should be Courier. Use Symbol or Lucida or Computer Modern for \textit{mathematics only. }

% Do not use type 3 fonts for any portion of your paper, including graphics. Type 3 bitmapped fonts are designed for fixed resolution printers. Most print at 300 dpi even if the printer resolution is 1200 dpi or higher. They also often cause high resolution imagesetter devices to crash. Consequently, AAAI will not accept electronic files containing obsolete type 3 fonts. Files containing those fonts (even in graphics) will be rejected. (Authors using blackboard symbols must avoid packages that use type 3 fonts.)

% Fortunately, there are effective workarounds that will prevent your file from embedding type 3 bitmapped fonts. The easiest workaround is to use the required times, helvet, and courier packages with \LaTeX{}2e. (Note that papers formatted in this way will still use Computer Modern for the mathematics. To make the math look good, you'll either have to use Symbol or Lucida, or you will need to install type 1 Computer Modern fonts --- for more on these fonts, see the section ``Obtaining Type 1 Computer Modern.")

% If you are unsure if your paper contains type 3 fonts, view the PDF in Acrobat Reader. The Properties/Fonts window will display the font name, font type, and encoding properties of all the fonts in the document. If you are unsure if your graphics contain type 3 fonts (and they are PostScript or encapsulated PostScript documents), create PDF versions of them, and consult the properties window in Acrobat Reader.

% The default size for your type must be ten-point with twelve-point leading (line spacing). Start all pages (except the first) directly under the top margin. (See the next section for instructions on formatting the title page.) Indent ten points when beginning a new paragraph, unless the paragraph begins directly below a heading or subheading.


% \subsubsection{Obtaining Type 1 Computer Modern for \LaTeX{}.}

% If you use Computer Modern for the mathematics in your paper (you cannot use it for the text) you may need to download type 1 Computer fonts. They are available without charge from the American Mathematical Society:
% http://www.ams.org/tex/type1-fonts.html.

% \subsubsection{Nonroman Fonts.}
% If your paper includes symbols in other languages (such as, but not limited to, Arabic, Chinese, Hebrew, Japanese, Thai, Russian and other Cyrillic languages), you must restrict their use to bit-mapped figures.

% \subsection{Title and Authors}
% Your title must appear centered over both text columns in sixteen-point bold type (twenty-four point leading). The title must be written in Title Case according to the Chicago Manual of Style rules. The rules are a bit involved, but in general verbs (including short verbs like be, is, using, and go), nouns, adverbs, adjectives, and pronouns should be capitalized, (including both words in hyphenated terms), while articles, conjunctions, and prepositions are lower case unless they directly follow a colon or long dash. You can use the online tool \url{https://titlecaseconverter.com/} to double-check the proper capitalization (select the "Chicago" style and mark the "Show explanations" checkbox).

% Author's names should appear below the title of the paper, centered in twelve-point type (with fifteen point leading), along with affiliation(s) and complete address(es) (including electronic mail address if available) in nine-point roman type (the twelve point leading). You should begin the two-column format when you come to the abstract.

% \subsubsection{Formatting Author Information.}
% Author information has to be set according to the following specification depending if you have one or more than one affiliation. You may not use a table nor may you employ the \textbackslash authorblk.sty package. For one or several authors from the same institution, please separate them with commas and write all affiliation directly below (one affiliation per line) using the macros \textbackslash author and \textbackslash affiliations:

% \begin{quote}\begin{scriptsize}\begin{verbatim}
% \author{
%     Author 1, ..., Author n\\
% }
% \affiliations {
%     Address line\\
%     ... \\
%     Address line\\
% }
% \end{verbatim}\end{scriptsize}\end{quote}


% \noindent For authors from different institutions, use \textbackslash textsuperscript \{\textbackslash rm x \} to match authors and affiliations. Notice that there should not be any spaces between the author name (or comma following it) and the superscript.

% \begin{quote}\begin{scriptsize}\begin{verbatim}
% \author{
%     AuthorOne,\equalcontrib\textsuperscript{\rm 1,\rm2}
%     AuthorTwo,\equalcontrib\textsuperscript{\rm 2}
%     AuthorThree,\textsuperscript{\rm 3}\\
%     AuthorFour,\textsuperscript{\rm 4}
%     AuthorFive \textsuperscript{\rm 5}}
% }
% \affiliations {
%     \textsuperscript{\rm 1}AffiliationOne,\\
%     \textsuperscript{\rm 2}AffiliationTwo,\\
%     \textsuperscript{\rm 3}AffiliationThree,\\
%     \textsuperscript{\rm 4}AffiliationFour,\\
%     \textsuperscript{\rm 5}AffiliationFive\\
%     \{email, email\}@affiliation.com,
%     email@affiliation.com,
%     email@affiliation.com,
%     email@affiliation.com
% }
% \end{verbatim}\end{scriptsize}\end{quote}

% You can indicate that some authors contributed equally using the \textbackslash equalcontrib command. This will add a marker after the author names and a footnote on the first page.

% Note that you may want to  break the author list for better visualization. You can achieve this using a simple line break (\textbackslash  \textbackslash).

% \subsection{\LaTeX{} Copyright Notice}
% The copyright notice automatically appears if you use aaai24.sty. It has been hardcoded and may not be disabled.

% \subsection{Credits}
% Any credits to a sponsoring agency should appear in the acknowledgments section, unless the agency requires different placement. If it is necessary to include this information on the front page, use
% \textbackslash thanks in either the \textbackslash author or \textbackslash title commands.
% For example:
% \begin{quote}
% \begin{small}
% \textbackslash title\{Very Important Results in AI\textbackslash thanks\{This work is
%  supported by everybody.\}\}
% \end{small}
% \end{quote}
% Multiple \textbackslash thanks commands can be given. Each will result in a separate footnote indication in the author or title with the corresponding text at the botton of the first column of the document. Note that the \textbackslash thanks command is fragile. You will need to use \textbackslash protect.

% Please do not include \textbackslash pubnote commands in your document.

% \subsection{Abstract}
% Follow the example commands in this document for creation of your abstract. The command \textbackslash begin\{abstract\} will automatically indent the text block. Please do not indent it further. {Do not include references in your abstract!}

% \subsection{Page Numbers}

% Do not print any page numbers on your paper. The use of \textbackslash pagestyle is forbidden.

% \subsection{Text}
% The main body of the paper must be formatted in black, ten-point Times Roman with twelve-point leading (line spacing). You may not reduce font size or the linespacing. Commands that alter font size or line spacing (including, but not limited to baselinestretch, baselineshift, linespread, and others) are expressly forbidden. In addition, you may not use color in the text.

% \subsection{Citations}
% Citations within the text should include the author's last name and year, for example (Newell 1980). Append lower-case letters to the year in cases of ambiguity. Multiple authors should be treated as follows: (Feigenbaum and Engelmore 1988) or (Ford, Hayes, and Glymour 1992). In the case of four or more authors, list only the first author, followed by et al. (Ford et al. 1997).

% \subsection{Extracts}
% Long quotations and extracts should be indented ten points from the left and right margins.

% \begin{quote}
% This is an example of an extract or quotation. Note the indent on both sides. Quotation marks are not necessary if you offset the text in a block like this, and properly identify and cite the quotation in the text.

% \end{quote}

% \subsection{Footnotes}
% Use footnotes judiciously, taking into account that they interrupt the reading of the text. When required, they should be consecutively numbered throughout with superscript Arabic numbers. Footnotes should appear at the bottom of the page, separated from the text by a blank line space and a thin, half-point rule.

% \subsection{Headings and Sections}
% When necessary, headings should be used to separate major sections of your paper. Remember, you are writing a short paper, not a lengthy book! An overabundance of headings will tend to make your paper look more like an outline than a paper. The aaai24.sty package will create headings for you. Do not alter their size nor their spacing above or below.

% \subsubsection{Section Numbers.}
% The use of section numbers in AAAI Press papers is optional. To use section numbers in \LaTeX{}, uncomment the setcounter line in your document preamble and change the 0 to a 1. Section numbers should not be used in short poster papers and/or extended abstracts.

% \subsubsection{Section Headings.}
% Sections should be arranged and headed as follows:
% \begin{enumerate}
% \item Main content sections
% \item Appendices (optional)
% \item Ethical Statement (optional, unnumbered)
% \item Acknowledgements (optional, unnumbered)
% \item References (unnumbered)
% \end{enumerate}

% \subsubsection{Appendices.}
% Any appendices must appear after the main content. If your main sections are numbered, appendix sections must use letters instead of arabic numerals. In \LaTeX{} you can use the \texttt{\textbackslash appendix} command to achieve this effect and then use \texttt{\textbackslash section\{Heading\}} normally for your appendix sections.

% \subsubsection{Ethical Statement.}
% You can write a statement about the potential ethical impact of your work, including its broad societal implications, both positive and negative. If included, such statement must be written in an unnumbered section titled \emph{Ethical Statement}.

% \subsubsection{Acknowledgments.}
% The acknowledgments section, if included, appears right before the references and is headed ``Acknowledgments". It must not be numbered even if other sections are (use \texttt{\textbackslash section*\{Acknowledgements\}} in \LaTeX{}). This section includes acknowledgments of help from associates and colleagues, credits to sponsoring agencies, financial support, and permission to publish. Please acknowledge other contributors, grant support, and so forth, in this section. Do not put acknowledgments in a footnote on the first page. If your grant agency requires acknowledgment of the grant on page 1, limit the footnote to the required statement, and put the remaining acknowledgments at the back. Please try to limit acknowledgments to no more than three sentences.

% \subsubsection{References.}
% The references section should be labeled ``References" and must appear at the very end of the paper (don't end the paper with references, and then put a figure by itself on the last page). A sample list of references is given later on in these instructions. Please use a consistent format for references. Poorly prepared or sloppy references reflect badly on the quality of your paper and your research. Please prepare complete and accurate citations.

% \subsection{Illustrations and  Figures}

% % Figure environment removed

% % Figure environment removed

% % Using the \centering command instead of \begin{center} ... \end{center} will save space
% % Positioning your figure at the top of the page will save space and make the paper more readable
% % Using 0.95\columnwidth in conjunction with the


% Your paper must compile in PDF\LaTeX{}. Consequently, all your figures must be .jpg, .png, or .pdf. You may not use the .gif (the resolution is too low), .ps, or .eps file format for your figures.

% Figures, drawings, tables, and photographs should be placed throughout the paper on the page (or the subsequent page) where they are first discussed. Do not group them together at the end of the paper. If placed at the top of the paper, illustrations may run across both columns. Figures must not invade the top, bottom, or side margin areas. Figures must be inserted using the \textbackslash usepackage\{graphicx\}. Number figures sequentially, for example, figure 1, and so on. Do not use minipage to group figures.

% If you normally create your figures using pgfplots, please create the figures first, and then import them as pdfs with proper bounding boxes, as the bounding and trim boxes created by pfgplots are fragile and not valid.

% When you include your figures, you must crop them \textbf{outside} of \LaTeX{}. The command \textbackslash includegraphics*[clip=true, viewport 0 0 10 10]{...} might result in a PDF that looks great, but the image is \textbf{not really cropped.} The full image can reappear (and obscure whatever it is overlapping) when page numbers are applied or color space is standardized. Figures \ref{fig1}, and \ref{fig2} display some unwanted results that often occur.

% If your paper includes illustrations that are not compatible with PDF\TeX{} (such as .eps or .ps documents), you will need to convert them. The epstopdf package will usually work for eps files. You will need to convert your ps files to PDF in either case.

% \subsubsection {Figure Captions.}The illustration number and caption must appear \textit{under} the illustration. Labels and other text with the actual illustration must be at least nine-point type. However, the font and size of figure captions must be 10 point roman. Do not make them smaller, bold, or italic. (Individual words may be italicized if the context requires differentiation.)

% \subsection{Tables}

% Tables should be presented in 10 point roman type. If necessary, they may be altered to 9 point type. You may not use any commands that further reduce point size below nine points. Tables that do not fit in a single column must be placed across double columns. If your table won't fit within the margins even when spanning both columns, you must split it. Do not use minipage to group tables.

% \subsubsection {Table Captions.} The number and caption for your table must appear \textit{under} (not above) the table.  Additionally, the font and size of table captions must be 10 point roman and must be placed beneath the figure. Do not make them smaller, bold, or italic. (Individual words may be italicized if the context requires differentiation.)



% \subsubsection{Low-Resolution Bitmaps.}
% You may not use low-resolution (such as 72 dpi) screen-dumps and GIF files---these files contain so few pixels that they are always blurry, and illegible when printed. If they are color, they will become an indecipherable mess when converted to black and white. This is always the case with gif files, which should never be used. The resolution of screen dumps can be increased by reducing the print size of the original file while retaining the same number of pixels. You can also enlarge files by manipulating them in software such as PhotoShop. Your figures should be 300 dpi when incorporated into your document.

% \subsubsection{\LaTeX{} Overflow.}
% \LaTeX{} users please beware: \LaTeX{} will sometimes put portions of the figure or table or an equation in the margin. If this happens, you need to make the figure or table span both columns. If absolutely necessary, you may reduce the figure, or reformat the equation, or reconfigure the table.{ \bf Check your log file!} You must fix any overflow into the margin (that means no overfull boxes in \LaTeX{}). \textbf{Nothing is permitted to intrude into the margin or gutter.}


% \subsubsection{Using Color.}
% Use of color is restricted to figures only. It must be WACG 2.0 compliant. (That is, the contrast ratio must be greater than 4.5:1 no matter the font size.) It must be CMYK, NOT RGB. It may never be used for any portion of the text of your paper. The archival version of your paper will be printed in black and white and grayscale. The web version must be readable by persons with disabilities. Consequently, because conversion to grayscale can cause undesirable effects (red changes to black, yellow can disappear, and so forth), we strongly suggest you avoid placing color figures in your document. If you do include color figures, you must (1) use the CMYK (not RGB) colorspace and (2) be mindful of readers who may happen to have trouble distinguishing colors. Your paper must be decipherable without using color for distinction.

% \subsubsection{Drawings.}
% We suggest you use computer drawing software (such as Adobe Illustrator or, (if unavoidable), the drawing tools in Microsoft Word) to create your illustrations. Do not use Microsoft Publisher. These illustrations will look best if all line widths are uniform (half- to two-point in size), and you do not create labels over shaded areas. Shading should be 133 lines per inch if possible. Use Times Roman or Helvetica for all figure call-outs. \textbf{Do not use hairline width lines} --- be sure that the stroke width of all lines is at least .5 pt. Zero point lines will print on a laser printer, but will completely disappear on the high-resolution devices used by our printers.

% \subsubsection{Photographs and Images.}
% Photographs and other images should be in grayscale (color photographs will not reproduce well; for example, red tones will reproduce as black, yellow may turn to white, and so forth) and set to a minimum of 300 dpi. Do not prescreen images.

% \subsubsection{Resizing Graphics.}
% Resize your graphics \textbf{before} you include them with LaTeX. You may \textbf{not} use trim or clip options as part of your \textbackslash includegraphics command. Resize the media box of your PDF using a graphics program instead.

% \subsubsection{Fonts in Your Illustrations.}
% You must embed all fonts in your graphics before including them in your LaTeX document.

% \subsubsection{Algorithms.}
% Algorithms and/or programs are a special kind of figures. Like all illustrations, they should appear floated to the top (preferably) or bottom of the page. However, their caption should appear in the header, left-justified and enclosed between horizontal lines, as shown in Algorithm~\ref{alg:algorithm}. The algorithm body should be terminated with another horizontal line. It is up to the authors to decide whether to show line numbers or not, how to format comments, etc.

% In \LaTeX{} algorithms may be typeset using the {\tt algorithm} and {\tt algorithmic} packages, but you can also use one of the many other packages for the task.

% \begin{algorithm}[tb]
% \caption{Example algorithm}
% \label{alg:algorithm}
% \textbf{Input}: Your algorithm's input\\
% \textbf{Parameter}: Optional list of parameters\\
% \textbf{Output}: Your algorithm's output
% \begin{algorithmic}[1] %[1] enables line numbers
% \STATE Let $t=0$.
% \WHILE{condition}
% \STATE Do some action.
% \IF {conditional}
% \STATE Perform task A.
% \ELSE
% \STATE Perform task B.
% \ENDIF
% \ENDWHILE
% \STATE \textbf{return} solution
% \end{algorithmic}
% \end{algorithm}

% \subsubsection{Listings.}
% Listings are much like algorithms and programs. They should also appear floated to the top (preferably) or bottom of the page. Listing captions should appear in the header, left-justified and enclosed between horizontal lines as shown in Listing~\ref{lst:listing}. Terminate the body with another horizontal line and avoid any background color. Line numbers, if included, must appear within the text column.

% \begin{listing}[tb]%
% \caption{Example listing {\tt quicksort.hs}}%
% \label{lst:listing}%
% \begin{lstlisting}[language=Haskell]
% quicksort :: Ord a => [a] -> [a]
% quicksort []     = []
% quicksort (p:xs) = (quicksort lesser) ++ [p] ++ (quicksort greater)
% 	where
% 		lesser  = filter (< p) xs
% 		greater = filter (>= p) xs
% \end{lstlisting}
% \end{listing}

% \subsection{References}
% The AAAI style includes a set of definitions for use in formatting references with BibTeX. These definitions make the bibliography style fairly close to the ones  specified in the Reference Examples appendix below. To use these definitions, you also need the BibTeX style file ``aaai24.bst," available in the AAAI Author Kit on the AAAI web site. Then, at the end of your paper but before \textbackslash end{document}, you need to put the following lines:

% \begin{quote}
% \begin{small}
% \textbackslash bibliography\{bibfile1,bibfile2,...\}
% \end{small}
% \end{quote}

% Please note that the aaai24.sty class already sets the bibliographystyle for you, so you do not have to place any \textbackslash bibliographystyle command in the document yourselves. The aaai24.sty file is incompatible with the hyperref and navigator packages. If you use either, your references will be garbled and your paper will be returned to you.

% References may be the same size as surrounding text. However, in this section (only), you may reduce the size to \textbackslash small if your paper exceeds the allowable number of pages. Making it any smaller than 9 point with 10 point linespacing, however, is not allowed. A more precise and exact method of reducing the size of your references minimally is by means of the following command: \begin{quote}
% \textbackslash fontsize\{9.8pt\}\{10.8pt\}
% \textbackslash selectfont\end{quote}

% \noindent You must reduce the size equally for both font size and line spacing, and may not reduce the size beyond \{9.0pt\}\{10.0pt\}.

% The list of files in the \textbackslash bibliography command should be the names of your BibTeX source files (that is, the .bib files referenced in your paper).

% The following commands are available for your use in citing references:
% \begin{quote}
% {\em \textbackslash cite:} Cites the given reference(s) with a full citation. This appears as ``(Author Year)'' for one reference, or ``(Author Year; Author Year)'' for multiple references.\smallskip\\
% {\em \textbackslash shortcite:} Cites the given reference(s) with just the year. This appears as ``(Year)'' for one reference, or ``(Year; Year)'' for multiple references.\smallskip\\
% {\em \textbackslash citeauthor:} Cites the given reference(s) with just the author name(s) and no parentheses.\smallskip\\
% {\em \textbackslash citeyear:} Cites the given reference(s) with just the date(s) and no parentheses.
% \end{quote}
% You may also use any of the \emph{natbib} citation commands.


% \section{Proofreading Your PDF}
% Please check all the pages of your PDF file. The most commonly forgotten element is the acknowledgements --- especially the correct grant number. Authors also commonly forget to add the metadata to the source, use the wrong reference style file, or don't follow the capitalization rules or comma placement for their author-title information properly. A final common problem is text (expecially equations) that runs into the margin. You will need to fix these common errors before submitting your file.

% \section{Improperly Formatted Files }
% In the past, AAAI has corrected improperly formatted files submitted by the authors. Unfortunately, this has become an increasingly burdensome expense that we can no longer absorb). Consequently, if your file is improperly formatted, it will be returned to you for correction.

% \section{Naming Your Electronic File}
% We require that you name your \LaTeX{} source file with the last name (family name) of the first author so that it can easily be differentiated from other submissions. Complete file-naming instructions will be provided to you in the submission instructions.

% \section{Submitting Your Electronic Files to AAAI}
% Instructions on paper submittal will be provided to you in your acceptance letter.

% \section{Inquiries}
% If you have any questions about the preparation or submission of your paper as instructed in this document, please contact AAAI Press at the address given below. If you have technical questions about implementation of the aaai style file, please contact an expert at your site. We do not provide technical support for \LaTeX{} or any other software package. To avoid problems, please keep your paper simple, and do not incorporate complicated macros and style files.

% \begin{quote}
% \noindent AAAI Press\\
% 1900 Embarcadero Road, Suite 101\\
% Palo Alto, California 94303-3310 USA\\
% \textit{Telephone:} (650) 328-3123\\
% \textit{E-mail:} See the submission instructions for your particular conference or event.
% \end{quote}

% \section{Additional Resources}
% \LaTeX{} is a difficult program to master. If you've used that software, and this document didn't help or some items were not explained clearly, we recommend you read Michael Shell's excellent document (testflow doc.txt V1.0a 2002/08/13) about obtaining correct PS/PDF output on \LaTeX{} systems. (It was written for another purpose, but it has general application as well). It is available at www.ctan.org in the tex-archive.

% \appendix
% \section{Reference Examples}
% \label{sec:reference_examples}

% \nobibliography*
% Formatted bibliographies should look like the following examples. You should use BibTeX to generate the references. Missing fields are unacceptable when compiling references, and usually indicate that you are using the wrong type of entry (BibTeX class).

% \paragraph{Book with multiple authors~\nocite{em:86}} Use the \texttt{@book} class.\\[.2em]
% \bibentry{em:86}.

% \paragraph{Journal and magazine articles~\nocite{r:80, hcr:83}} Use the \texttt{@article} class.\\[.2em]
% \bibentry{r:80}.\\[.2em]
% \bibentry{hcr:83}.

% \paragraph{Proceedings paper published by a society, press or publisher~\nocite{c:83, c:84}} Use the \texttt{@inproceedings} class. You may abbreviate the \emph{booktitle} field, but make sure that the conference edition is clear.\\[.2em]
% \bibentry{c:84}.\\[.2em]
% \bibentry{c:83}.

% \paragraph{University technical report~\nocite{r:86}} Use the \texttt{@techreport} class.\\[.2em]
% \bibentry{r:86}.

% \paragraph{Dissertation or thesis~\nocite{c:79}} Use the \texttt{@phdthesis} class.\\[.2em]
% \bibentry{c:79}.

% \paragraph{Forthcoming publication~\nocite{c:21}} Use the \texttt{@misc} class with a \texttt{note="Forthcoming"} annotation.
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   note="Forthcoming",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:21}.

% \paragraph{ArXiv paper~\nocite{c:22}} Fetch the BibTeX entry from the "Export Bibtex Citation" link in the arXiv website. Notice it uses the \texttt{@misc} class instead of the \texttt{@article} one, and that it includes the \texttt{eprint} and \texttt{archivePrefix} keys.
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   eprint="xxxx.yyyy",
%   archivePrefix="arXiv",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:22}.

% \paragraph{Website or online resource~\nocite{c:23}} Use the \texttt{@misc} class. Add the url in the \texttt{howpublished} field and the date of access in the \texttt{note} field:
% \begin{quote}
% \begin{footnotesize}
% \begin{verbatim}
% @misc(key,
%   [...]
%   howpublished="\url{http://...}",
%   note="Accessed: YYYY-mm-dd",
% )
% \end{verbatim}
% \end{footnotesize}
% \end{quote}
% \bibentry{c:23}.

% For the most up to date version of the AAAI reference style, please consult the \textit{AI Magazine} Author Guidelines at \url{https://aaai.org/ojs/index.php/aimagazine/about/submissions#authorGuidelines}

% \section{Acknowledgments}
% AAAI is especially grateful to Peter Patel Schneider for his work in implementing the original aaai.sty file, liberally using the ideas of other style hackers, including Barbara Beeton. We also acknowledge with thanks the work of George Ferguson for his guide to using the style and BibTeX files --- which has been incorporated into this document --- and Hans Guesgen, who provided several timely modifications, as well as the many others who have, from time to time, sent in suggestions on improvements to the AAAI style. We are especially grateful to Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan for the improvements to the Bib\TeX{} and \LaTeX{} files made in 2020.

% The preparation of the \LaTeX{} and Bib\TeX{} files that implement these instructions was supported by Schlumberger Palo Alto Research, AT\&T Bell Laboratories, Morgan Kaufmann Publishers, The Live Oak Press, LLC, and AAAI Press. Bibliography style changes were added by Sunil Issar. \verb+\+pubnote was added by J. Scott Penberthy. George Ferguson added support for printing the AAAI copyright slug. Additional changes to aaai24.sty and aaai24.bst have been made by Francisco Cruz, Marc Pujol-Gonzalez, and Mico Loretan.

% \bigskip
% \noindent Thank you for reading these instructions carefully. We look forward to receiving your electronic files!

\bibliography{aaai24}

\clearpage
\section{Appendix}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
\hline
\textbf{Label} & \textbf{Description prompt} \\
\hline
World&This topic is talk about World not Business \\
Sports&This topic is talk about Sports \\
Business&This topic is talk about Science not World \\
Science&This topic is talk about Science \\
Technology&This topic is talk about Technology \\
\hline
\end{tabular}
\caption{\textbf{Description prompts of labels on AG News.}}
\label{table-prompt-agnews}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|l}
\hline
\textbf{Label} & \textbf{Description prompt} \\
\hline
Society&This topic is talk about Society not Family or Relationships \\
Education&This topic is talk about Education not Science or Mathematics \\
\hline
\end{tabular}
\caption{\textbf{Description prompts of labels on Yahoo! Topic.} We only provided descriptions for a few categories in our analysis.}
\label{table-prompt-yahoo}
\end{table}

\begin{table*}[h]
\small
\centering
\begin{tabular}{l|l}
\hline
\textbf{Label} & \textbf{Description prompt} \\
\hline
Company&This topic is describing this company \\
EducationInstitution&This school, university described in this content is an education institution \\
Artist&This musician, painter, singer, writer, author described in this content is an artist \\
Athlete&This person who plays sport described in this content is an athlete \\
OfficeHolder&This person who holds a position or office in a government described in this content is an officeholder \\
MeanOfTransportation&This vehicles, ridden, trains and other conveyances described in this content is transportation \\
Building&This man-made structure described in this content is a building \\
NaturalPlace&
\begin{tabular}[c]{@{}l@{}}This natural landforms, bodies of water, vegetation, rocks, forests, rivers, lakes, mountains, oceans, grasslands \\ described in this content is a natural place\end{tabular} \\
Village&This town, small settlement or community described in this content is a village \\
Animal&This organism described in this content is an animal \\
Plant&This organism described in this content is a plant \\
Album&This music or recorded tracks described in this content is an album \\
Film&This movie described in this content is a film \\
WrittenWork&This books, essays, poems or literatures described in this content is a written work \\
\hline
\end{tabular}
\caption{\textbf{Description prompts of labels on DBpedia.}}
\label{table-descri}
\end{table*}

\begin{table*}[h]
\scriptsize
\centering
\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l|l|l|l|l}
\hline
\textbf{Label index}&0&1&2&3&4&5&6&7&8&9&10&11&12&13&\(A.\) \\
\hline
0 (Company)&3096&71&87&6&19&607&224&18&6&15&60&464&213&114&61.92 \\
1 (EducationInstitution)&28&4804&58&1&6&1&51&3&31&2&3&0&9&3&96.08 \\
2 (Artist)&61&22&2862&5&37&1&113&3&0&10&4&710&235&937&57.24 \\
3 (Athlete)&8&1&26&4941&4&4&7&0&0&1&0&1&6&1&98.82 \\
4 (OfficeHolder)&100&92&80&17&4441&10&157&2&16&8&1&2&5&69&88.82 \\
5 (MeanOfTransportation)&75&0&0&0&0&4853&58&8&0&3&0&1&1&1&97.06 \\
6 (Building)&91&208&35&0&2&55&4262&110&207&8&1&1&7&13&85.24 \\
7 (NaturalPlace)&0&0&1&0&0&0&45&4853&101&0&0&0&0&0&97.06 \\
8 (Village)&0&5&1&0&0&0&12&92&4890&0&0&0&0&0&97.80 \\
9 (Animal)&0&0&2&6&0&0&0&186&0&4423&382&0&0&1&88.46 \\
10 (Plant)&8&0&0&0&0&2&1&46&0&11&4931&0&1&0&98.62 \\
11 (Album)&0&0&5&1&0&0&0&0&0&0&0&4992&1&1&99.84 \\
12 (Film)&4&0&26&0&0&2&3&0&0&2&0&326&4629&8&92.58 \\
13 (WrittenWork)&584&50&87&0&9&14&32&12&16&144&29&132&645&3246&64.92 \\
\hline
\end{tabular}
%}
\caption{\textbf{Error analysis of DBpedia (with description prompt).} The predicted labels are presented as columns, while the actual labels are presented in rows. The rightmost column displays the test accuracy for each label.}
\label{table-dbpedia-confusion-matrix-2}
\end{table*}

\end{document}
