%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Template for USENIX papers.
%
% History:
%
% - TEMPLATE for Usenix papers, specifically to meet requirements of
%   USENIX '05. originally a template for producing IEEE-format
%   articles using LaTeX. written by Matthew Ward, CS Department,
%   Worcester Polytechnic Institute. adapted by David Beazley for his
%   excellent SWIG paper in Proceedings, Tcl 96. turned into a
%   smartass generic template by De Clarke, with thanks to both the
%   above pioneers. Use at your own risk. Complaints to /dev/null.
%   Make it two column with no page numbering, default is 10 point.
%
% - Munged by Fred Douglis <douglis@research.att.com> 10/97 to
%   separate the .sty file from the LaTeX source template, so that
%   people can more easily include the .sty file into an existing
%   document. Also changed to more closely follow the style guidelines
%   as represented by the Word sample file.
%
% - Note that since 2010, USENIX does not require endnotes. If you
%   want foot of page notes, don't include the endnotes package in the
%   usepackage command, below.
% - This version uses the latex2e styles, not the very ancient 2.09
%   stuff.
%
% - Updated July 2018: Text block size changed from 6.5" to 7"
%
% - Updated Dec 2018 for ATC'19:
%
%   * Revised text to pass HotCRP's auto-formatting check, with
%     hotcrp.settings.submission_form.body_font_size=10pt, and
%     hotcrp.settings.submission_form.line_height=12pt
%
%   * Switched from \endnote-s to \footnote-s to match Usenix's policy.
%
%   * \section* => \begin{abstract} ... \end{abstract}
%
%   * Make template self-contained in terms of bibtex entires, to allow
%     this file to be compiled. (And changing refs style to 'plain'.)
%
%   * Make template self-contained in terms of figures, to
%     allow this file to be compiled. 
%
%   * Added packages for hyperref, embedding fonts, and improving
%     appearance.
%   
%   * Removed outdated text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}

\usepackage{graphicx}
\usepackage{soul,enumitem}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{subcaption}


\newcommand{\zt}{\texttt{zns-tools}}
\newcommand{\ioc}{\texttt{ioctl()}}

%-------------------------------------------------------------------------------
\begin{document}
%-------------------------------------------------------------------------------

%don't want date printed
\date{}

% make title bold and 14 pt font (Latex default is non-bold, 16 pt)
\title{Understanding (Un)Written Contracts of NVMe ZNS Devices with \zt{}}

%for single author (just remove % characters)
\author{
{\rm Nick Tehrany, Krijn Doekemeijer, and Animesh Trivedi}\\
VU, Amsterdam
} 

\maketitle

%-------------------------------------------------------------------------------
\begin{abstract}
%-------------------------------------------------------------------------------
Operational and performance characteristics of flash SSDs have long been associated with a set of Unwritten Contracts due to their hidden, complex internals and lack of control from the host software stack. These unwritten contracts govern how data should be stored, accessed, and garbage collected. The emergence of Zoned Namespace (ZNS) flash devices with their open and standardized interface allows us to write these unwritten contracts for the storage stack. However, even with a standardized storage-host interface, due to the lack of appropriate end-to-end operational data collection tools, the quantification and reasoning of such contracts remain a challenge. 
In this paper, we propose \texttt{zns.tools}, an open-source framework for end-to-end event and metadata collection, analysis, and visualization for the ZNS SSDs contract analysis. We showcase how \texttt{zns.tools} can be used to understand how the combination of RocksDB with the F2FS file system interacts with the underlying storage. Our tools are available openly at \url{https://github.com/stonet-research/zns-tools}. 
\end{abstract}


\section{Introduction}
The emergence of flash solid state drives (SSDs) has resulted in the redesigning of the host interfaces~\cite{2022-nvme-spec,2012-nvme-vs-ahci}, the block layer~\cite{2013-systor-mq,2019-atc-aync}, I/O schedulers~\cite{2019-atc-mqfq,2021-fast-d2fq}, file systems~\cite{2015-fast-f2fs}, applications~\cite{2021-FAST-rocksdb-flash}, and distributed systems~\cite{2012-nsdi-corfu}. 
Despite the aforementioned significant end-to-end changes to leverage the characteristics of flash storage, the reasoning and analysis of modern flash SSDs are governed by many \textit{``unwritten contracts''}~\cite{2017-eurosys-unwritten} regarding how user data should be stored, grouped, accessed, and deleted. In parts such expectations are considered unwritten as SSDs themselves are complex with many internal hidden details (flash chip sizes, layout, garbage collection (GC) units, buffer sizes)~\cite{2022-systor-ssd-study}. Furthermore, the complex architecture of the layered modern storage stack makes the end-to-end reasoning about the unwritten \mbox{contracts} challenging. 



% Figure environment removed

\textbf{NVMe Zone Namespace Devices and \st{Un}Written \mbox{Contracts}:} To address the challenges of complexity, researchers have advocated for more open interfaces~\cite{2017-fast-openchannel-ssd,2017-systor-autostream,2014-asplos-sdf}, which has culminated in the Zoned Namespace (ZNS) specification~\cite{2021-atc-zns,2021-wd-zns,2021-hotos-zone}. Briefly, a ZNS device divides its storage capacity into multiple \textit{zones} that can only be sequentially written (or appended to), thus closely mimicking how flash chips internally work. Data can be read from anywhere either sequentially or randomly. I/O on different zones is isolated and done in parallel, thus a zone represents a unit of parallelism. Beyond these basic read/write operations, ZNS devices have a number of unique flash/ZNS management-related commands. Of particular interest is the \textit{reset} command with zones, which explicitly resets a zone to be written again. In case there was any live data, it is the responsibility of the host software (file system, application) to ensure that the data is copied to a new zone before resetting the old zone. The significant advantage of such a design is that it clearly identifies the unit of garbage collection (zone) and the timing when to reset (under the control of the host software), both of these details are intricately complex to reverse engineer~\cite{2022-systor-ssd-study,2023-cluster-zns-study}. Hence, with ZNS devices, previously ``unwritten contracts'' are now standardized \textit{``written contracts''}, which forces us to re-think the operational design of our storage stacks~\cite{2021-hotos-zone}. 


\textbf{Reasoning about the Written ZNS Contracts with Applications in a Layered Storage Stack:} ZNS software support in the storage stack is still under development. There are multiple ways through which ZNS contracts can be extended to an application in an end-to-end manner. Figure~\ref{fig:zns-software-support} shows such possibilities where ZNS software support is needed at the (a) block-level, I/O scheduler support with mq-deadline~\cite{2023-zns-sched}; or (b) file system level, currently F2FS and Btrfs support ZNS~\cite{2020-btrfs-zns}; or (c) application-level direct support, e.g., ZenFS customized file system backend for RocksDB~\cite{2021-atc-zns}. In such a layered architecture, it is challenging to ensure and cross-check if the ZNS contracts are extended and respected by all layers due to the \textit{semantic gap} between the layers. For example, the POSIX \texttt{fadvise} and \texttt{fctnl} calls are \textit{hints} to a file system that the file system is free to ignore. Similarly, 
 \mbox{ZNS}/flash file systems themselves have complex bookkeeping operations (garbage collection, log writing, fragmentation) where previously respected hints can be transparently overwritten by a file system without the application's knowledge (see below two examples in contract violation examples). Hence, in this work, we argue, \textit{there is a need to systematically investigate if and how the new written ZNS contracts are extended to applications}.  


%F2FS, a ZNS-supporting file system, reclassifies a file segment hotness class (hot, warm, cold) after a GC cycle without consulting the application. This reclassification can lead to data grouping which an application may have expected to be kept separate in different zones, thus leading to a contract violation. 

% Figure environment removed


\textbf{\zt{}: A Framework to Study Written ZNS Contracts:} In this work, we propose a set of open-sourced tools called \zt{} for the ZNS software stack. Put together, these tools allow us to collect operational data and events across the stack in a programmed manner. The collected data is then analyzed and visualized to identify various contract violations. 
More specifically, we are interested in understanding the following previously ``unwritten contracts'' and how they are (not) followed across the layers: (1) ``Request Scale'' with ``Locality'': how data placement decisions are made (that result in large I/O, and parallel requests with minimal FTL overheads in ZNS SSDs) and how such allocations look on ZNS SSDs; (2) ``Grouping by death time'': how data grouping decisions are made to ensure minimal overheads from GC-related data movements and overheads; and (3) ``Uniform Data Lifetime'': how a ZNS device's lifetime is managed by generating and deleting data with a uniform lifetime, and subsequently explicitly resetting a zone in a ZNS SSD that (indirectly) controls wear-leveling on the device. 


\textbf{Example: ZNS Contract violation with RocksDB and F2FS:}
\label{sec:introduction-contract-violation}
Using \zt{}, we identify two such violations about data grouping in the state-of-the-practice combination of RocksDB on F2FS with a ZNS SSD. 
The first issue is F2FS-specific. F2FS reclassifies the hotness (hot, warm, cold) of a data block after a round of GC cycle, where the GC always moves data blocks to cold data. This reclassification allows previously segregated data blocks under different hotness classes to be co-located in a new segment together, thus violating hotness hints from RocksDB. 
The second issue is RocksDB and F2FS specific. RocksDB writes SSTable to a file system in two passes, raw data (the table) and a small footer (less than a page). 
After writing the raw SSTable data to hot or cold data, depending on the level of the SSTable , RocksDB synchronizes the file causing F2FS to flush the data to the ZNS device. Upon completion of the flush, RocksDB writes the footer of the SSTable, containing a checksum, which is synchronized to the storage. Due to only a single page for the footer being dirty in the page cache, which is below the minimum threshold for F2FS of 16 pages, F2FS sets an inode flag to indicate the data as being hot. As a result, F2FS writes the footer page to the hot data segment, violating the contract for the SSTables by ignoring any hints for the SSTable writes.
%After writing the SST data, RocksDB synchronizes the file causing F2FS to flush the data to the ZNS device. Upon completion of the flush, RocksDB writes a 1 page footer (4KiB) for the SSTable, containing a checksum, which is synchronized to the storage. Due to only a single page for the footer being dirty in page cache,  which is below a minimum threshold for F2FS of 16 pages, F2FS sets an inode flag to indicate the data as being hot. As a result, F2FS writes the page to the hot data, violating the contract for the SSTables, by ignoring the SSTable write hint.
%After writing the raw data RocksDB flushes the file system, leaving only the footer in the page cache. If the number of dirty pages in page cache is below a threshold of 16 pages, F2FS classifies the data as hot, by setting a hot inode flag. As a result, F2FS treats the small write hot data, which causes contract violation for SSTables. Data is no longer grouped by lifetime, its footers are incorrectly identified, and is no longer co-located with the file data. 
Both of these incidents are examples of violating the ``Grouping by death time'' contract. 


In this work we propose a collection of \zt{} that combined constitute a framework that allows parts-by-parts building an end-to-end understanding of how data is stored and managed in a layered storage stack on top of ZNS devices. The primary goal of the zns-tools framework is to allow building a complete picture of events across the layers that influence data placement and movement decisions. \cref{fig:rocksdb-timeline} shows on such end-to-end example generated from our framework.  Our key contributions in this work include the following: 
\begin{itemize}%[leftmargin=10pt,itemindent=0em,nolistsep]
    \item Making a case for building end-to-end operational data analysis tools to reason about previously unwritten, but now written ZNS SSDs storage contracts for applications. 
    \item \zt{}, an end-to-end framework that provides support for collecting, analyzing, and visualizing ZNS contracts across the layered storage stack. 
    \item Specific demonstration of the framework's capability on F2FS with RocksDB applications for an end-to-end visualization of operational analytics. \cref{fig:rocksdb-timeline} shows one such end-to-end example generated from our framework.   
    \item \texttt{zns-tools} are open-sourced and available here: \url{https://github.com/stonet-research/zns-tools}. This paper is available under CC-BY 4.0 License.
\end{itemize}


\section{Design of \zt{} Framework}
\zt{} is a new framework for ZNS SSDs that is designed to collect operational data about the ZNS storage stack to identify contract violations. It comes with tools that aid in extracting and visualizing: (\textbf{C1}) where is user data stored on ZNS SSDs; (\textbf{C2}) how much I/O is triggered to each zone; (\textbf{C3}) which zones are subjected to GC resets from the software stack and how data migrates across zones because of GC. Currently, there are four tools in the framework, \texttt{zns.fiemap}, \texttt{zns.segmap}, \texttt{zns.imap}, and \texttt{zns.trace}.
They are visualized in \cref{fig:zns-tools}, and their exact bash command and outputs are shown in our GitHub repository. 

%n total there are four tools in the framework, in this section we will discuss four of them. 
%We will explain their merit, how they function, and how they can identify if ZNS contracts are violated.

%Nick I have rewritten this paragraph above here ^
% \zt{} is a new framework for ZNS that is motivated by the emerging opportunity of ZNS devices, and allows understanding ZNS data storage and management throughout the layered storage stack. A fundamental benefit of ZNS devices, which prior standardized flash devices lack, is the exposing of the physical erase unit of the flash storage (zones being mapped to one/multiple flash blocks). With such knowledge, data placement and storage management can logically be broken down into the physical erase units, by identifying and understanding the data grouping within individual zones. In this section, we present four of the five tools introduced in the \zt{} framework, their design and implementation, and the adaptability of additional applications and file systems for ZNS. One tool (\texttt{zns.fpbench}) provides a F2FS benchmarking framework, similar to the fio tool.
\subsection{zns.fiemap}
\texttt{zns.fiemap} (ZNS file-mapping) is a tool that is designed to extract placement information from the stack to identify how the ``Locality'' contract is followed. The placement of files and data within modern SSDs is not static, and constantly changes in response to application or file system level events. 
Applications can typically provide allocation/location hints (by means of data temperature), but it is ultimately the file system that decides about the final data storage location. ZNS SSDs require any live data to be copied during GC, hence, the data location changes. Furthermore, any log-structured file system has its own out-of-place update mechanisms. With ZNS SSDs and log-structured file system, the location of the data is thus dynamic and changes constantly. The repeated execution of \texttt{zns.fiemap} traces the file data movement in this dynamic environment to identify how data from different files are grouped in zones. 
Furthermore, while moving file extents, if the ``Locality'' rule is not followed, that leads to file fragmentation with holes that are known to cause severe performance degradation~\cite{2021-Park-FragPicker,f2fs_defrag_tool,2020-date-ars,2018-Kadekodi-Geriatrix,2016-hotstorage-fs-frag-mobile}. Holes violate the ``Request Scale'' contract that recommends large sequential I/O requests. In ZNS, how a file is stored among zones also determines the amount of chip-level parallelism, a large file I/O can extract. Hence, \texttt{zns.fiemap} provides detailed information about the on-device zone-level placement of files within various file systems (addressing \texttt{C1},\texttt{C3}).

\texttt{zns.fiemap} retrieves file location mappings from the Linux kernel using the \ioc{} syscall with the \texttt{FIEMAP} flag. 
Support for \texttt{FIEMAP} is not necessary for file systems for POSIX compliance, however, all currently available file systems with ZNS support (F2FS and Btrfs) support this flag. With \texttt{FIEMAP}, file systems implement the tracking of extents, representing ranges of physically contiguous data for a file, which are returned to the \ioc{} caller. By iterating over the logical range of a file, \texttt{zns.fiemap} retrieves data mappings of all the extents for the particular file. 
The collected extents for the targeted file are mapped to their respective zone(s) containing the file's data using their logical address ranges. For example, on a ZNS SSD with a zone size of 1MiB, logical addresses between [0, 1MiB) fall within the first zone, [1MiB, 2MiB) on the second zone, and so on. The zone size and zone size ranges can be queried with the ZNS device using a zone management command. With all information about file extents, their addresses, address-to-zone mappings, \texttt{zns.fiemap} reports a detailed profile of the extent distribution (min, max, percentiles), hole statistics, and zone-level placement information. 
\cref{fig:zns-tools} (a) illustrates the operational aspect of \texttt{zns.fiemap}, retrieving the extent mappings of File \texttt{A} using \ioc{} with the \texttt{FIEMAP} flag, followed by mapping the three file extents to zones 1 and 2.


%In addition to file-to-zone mapping, \texttt{zns.fiemap} reports file fragmentation statistics with the number of holes between extents. The presence of holes, and fragmentation has been shown to deteriorate performance even on SSDs~\cite{2021-Park-FragPicker,f2fs_defrag_tool,2020-date-ars,2018-Kadekodi-Geriatrix,2016-hotstorage-fs-frag-mobile} and violates the "Request Scale" contract that asks for large sequential I/O requests. Zone distribution also determines the amount of chip-level parallelism a file I/O will leverage. 
%As a final result, this tool presents a detailed breakdown of which file range is stored on which page address inside which zones with fragment, holes information.  
%file fragmentation statistics are collected. These statistics include the presence of \textit{holes} between extents, where other or invalid data is residing, resulting in the data of the file being fragmented. Reducing file fragmentation is a key challenge to avoid turning performance of sequential reads into random reads~\cite{chen2009understanding,2021-Park-FragPicker}, and maintain intra-file data grouping. 
%\cref{fig:zns-tools} (a) illustrates the operational aspect of \texttt{zns.fiemap}, retrieving the extent mappings of File \texttt{A} using \ioc{} with the \texttt{FIEMAP} flag, followed by mapping the three file extents to zones 1 and 2.

% % Figure environment removed

\subsection{\texttt{zns.segmap} and \texttt{zns.imap}}
\texttt{zns.segmap} and \texttt{zns.imap} are tools that collect and quantify metadata around the ``Grouping by death time'' contract. This contract is implemented within a file system that uses application-level lifetime hints (\texttt{RWH\_WRITE\_LIFE\_*} flags with the \texttt{fcntl} call), or its own data segregation policies based on data access heatmaps. It recommends that any data that is about to get deleted, or over-written, should be stored within a single GC unit. With this design, once data is invalidated, the GC unit can be simply reset without having to copy any live data, thus reducing GC overheads. Naturally, an accurate grouping requires coordinated efforts from the application, as well as the file system, over a lifetime of the data/file(s). Furthermore, such groupings should be constantly evaluated due to the dynamic nature of data placement with GC events within the file system. 


\texttt{zns.segmap} and \texttt{zns.imap} are tools to extract grouping information for file data and metadata for F2FS. F2FS does heat-based data grouping in \textit{segments}. A file can have its data stored in multiple segments, and a single segment can contain data from multiple files. By default, F2FS uses 3 classes of classification (hot, warm, cold) on two types of data, file data, and file metadata (inodes). \texttt{zns.segmap} extracts the file-to-segment mappings using \texttt{zns.fiemap}, and reads the segment hotness classification from \texttt{procfs}\footnote{\url{/proc/fs/f2fs/nvme0n1/segment\_info}}. 
To locate the inode in F2FS, \texttt{zns.imap} firstly reads the F2FS superblock, which is written at a particular offset within the storage device, followed by parsing the superblock to identify the location of the node address table (NAT), where F2FS stores the block addresses of inodes, and the last checkpoint. Traversing the NAT, \texttt{zns.imap} retrieves the block address for the file inode to look up, followed by issuing a single read request to retrieve the inode. The retrieved inode is mapped to the zone in which it is contained. 
With these tools, we report for each F2FS segment, its hotness classification, number of file extents, the segment-to-zone mapping, and the inode-to-zone mapping. With this information, we can stitch a complete timeline of user data as it ages in the storage system (\cref{fig:rocksdb-timeline}). Though these tools are F2FS-specific, they can be extended to other file systems that do active data segregation. With F2FS, this information is available in the \texttt{/proc} file system\footnote{We also tried Brtfs with ZNS, but it was unstable.}. 
The file system-related metadata is also available as a part of low-level libraries (such as libext2fs or libf2fs) or can be generated~\cite{210526}.  
\cref{fig:zns-tools} (b) and (c) illustrate the design of \texttt{zns.segmap} and \texttt{zns.imap} tools. They retrieve extents for various files (\ioc{} call) and inodes (FS metadata walks) from F2FS, and retrieve F2FS segment lifetime classifications and the inode location. 




%After retrieving the extents of all files, the extents are sorted by their physical block addresses, as extents for different files may reside in the same zone. After sorting extents, each is mapped to the respective F2FS segment it is contained in, followed by collecting the F2FS segment information. Furthermore, during the process of mapping extents to segments, a number of statistics is collected, including the total number of segments and zones that file data spans across, and counters on the number of segments for each lifetime classifications that the file data occupies. Information on the lifetime classification of file data allows identifying the data grouping within F2FS, and understanding the data movement among the lifetime classifications, in addition to physical data movement on the ZNS device. This information is essential for identifying the movement of file data among the different lifetime classifications, as a result of F2FS garbage collection reclassifying all data as cold data. 
%This tool help us collect operational data for "Group by Death time" contract.

%\cref{fig:zns-tools} (b) illustrates the design of \texttt{zns.segmap}, retrieving extents for various files from F2FS using the \ioc{} call, and retrieving lifetime classifications from F2FS with \texttt{procfs} F2FS segments. 
%As \texttt{zns.segmap} is intended for mapping of file extents to F2FS segments, the design is primarily focused on F2FS. However, it has additional support for Btrfs, where retrieved extents for all files are only mapped to the ZNS zones without ...? 

% \cref{fig:zns-segmap-example} illustrates an example output of the file mappings after a RocksDB generated workload. The depicted information showcases the data placement of several files, where the \texttt{LOG} file is mapped into 7 different extents, all within 1 hot segment. Similarly, the output shows the mapping of several SSTFiles, which are mapped largely mapped to cold segments, however have one hot segment each. Furthermore, for each of the files, the extent that is in the hot data segment (e.g., extent 3 for \texttt{000038.sst} in segment 13,342) is 4KiB in size~\footnote{8 blocks (from size: 0x8 field) with 512B LBA format. 8*512B=4,096B}. Using this information, we identify that RocksDB writes the SSTFile data under a lifetime hint, based on the level of the SST, which is flushed to the storage, followed by a footer, containing checksum information for ensuring consistency, that flushed to storage independently. The footer is small in size, occupying 1 4KiB block, resulting in F2FS reclassifying it to hot data. In F2FS, if the number of dirty pages when flushing to storage is less than 16, an inode flag is set to identify it as being hot, based on which the data is written to the current hot segment.

% % Figure environment removed

%\subsection{\texttt{zns.imap}}
%The purpose of \texttt{zns.imap} tool is to identify contract violation of ... 
%It does so by tracking and identifying information stored specifically in an inode of a file. For this purpose, this tool is file system specific, currently only supporting F2FS which has the most mature ZNS SSD integration. 
%To locate the inode in F2FS, \texttt{zns.imap} firstly reads the F2FS superblock, which is written %at a particular offset within the storage device, followed by parsing the superblock to identify the location of the node address table (NAT), where F2FS stores the block addresses of inodes, and the last checkpoint. Traversing the NAT, \texttt{zns.imap} retrieves the block address for the file inode to look up, followed by issuing a single read request to retrieve the inode. The retrieved inode is mapped to the zone in which it is contained, similar to the prior \texttt{zns.fiemap} and \texttt{zns.segmap} tools, followed by printing a report on the full contents of the inode. \cref{fig:zns-tools} (c) illustrates the inode information retrieval process of \texttt{zns.imap}, firstly reading the F2FS superblock and checkpoint, followed by traversing the NAT to identify the block address of the inode. With the block address, the inode is read and mapped to the zone(s) it is contained in. 



% \cref{fig:zns-imap-example} shows an example output for a mapped inode of the RocksDB \texttt{LOG} file, where furthermore the contents of the superblock and the latest checkpoint are reported.

% % Figure environment removed

\subsection{\texttt{zns.trace}}
% Figure environment removed

The \texttt{zns.trace} tool identifies performance-critical ``Request Scale'' and ``Uniform Data Lifetime'' contracts by tracking ZNS command calls. \texttt{zns.trace} is a combination of a BPFtrace~\cite{bpftrace} script and a plotting tool that traces zone-level ZNS write, append, read, and reset operations (addressing \texttt{C2}). Tracing such data is useful for investigating the access patterns to the underlying storage device independently of file system or application workloads. 



\texttt{zns.trace} utilizes BPFtrace~\cite{bpftrace}, which inserts probes into the Linux kernel functions, that upon being triggered (i.e., the function being called) initiate data collection. The tracing script captures the NVMe I/O event with the command I/O sizes (their histogram) and zone reset calls, and maps the events to their corresponding zone(s). By analyzing the function arguments, the script identifies the type of operation, and further extracts data fields based on the request size. Importantly, the tracing supports ZNS devices in VMs (apart from just the host), where the zone reset command sets the function argument for the type of command to \texttt{REQ\_OP\_DRV\_OUT}, indicating the host driver (e.g., vfio-pci when using NVMe passthrough to the VM) is responsible for the request. This case requires to furthermore analyze the NVMe command and its zone management command field on the type of zone management action (e.g., close, finish, reset, open, offline). \texttt{zns.trace} relies on inserting kernel probes to parse I/O functions, and is therefore currently limited to kernel-based ZNS tracing. However, ongoing work is extending the tracing framework for user-level (e.g., SPDK) I/O and broader zone management request tracing. 

% The user can decide when to stop the script by hand (SIGINT). After completion the data is logged to files for further data processing and plots are generated for the data. By default heatmaps are generated for writes/appends, reads, and resets. The tool is at the moment limited to capturing events in the kernel, and can as a result not be used for tracing user-level requests (e.g. SPDK). 

%Upon termination of the tracing scripts, all collected data is written to a file, followed by post-processing to generate visuals of the various collected information. A fundamental benefit of ZNS devices is the representation of zones, allowing collected data to be represented on the basis of a zone. Therefore, the \texttt{zns.trace} tool generates numerous heat maps on the various collected I/O request types and zone management operations.

To illustrate the utility of \texttt{zns.trace} we run an identical workload on a number of database/file system and quantify the workload reset profiles. The number of resets is directly linked to the number of reset cycles that an SSD can undergo before exhausting flash chips. The ``Uniform Data Lifetime'' contract recommends generating data with a uniform lifetime to evenly spread the device wear. 
We run experiments on an emulated NVMe ZNS device (QEMU v6.0.0) with 64 zones of size 64MiB (4GiB in total). Our workload is a YCSB workload-A (50\% update, 50\% read)~\cite{2010-socc-ycsb} on RocksDB~\cite{rocksdb-git}(7.4.3), MongoDB~\cite{2018-springer-mongodb}(6.06), and PostgreSQL~\cite{momjian2001postgresql}(9.6.24) as KV backend targets with F2FS as file system. \cref{fig:heatmap} shows our results. Here each cell represents a zone, and the color indicates the total number of resets issued to the zone (since startup). Blue squares indicate a zone to which no reset commands were issued at all. There are two interesting observations. Firstly, for an identical workload, the three KV backends show vastly different profiles. 
We can see that for this test configuration PostgreSQL leads to significantly more resets. Additionally, we can identify that one of the bottom left zones (zone 2) is in all three cases heavily utilized. This zone corresponds to a warm node zone initialized by F2FS, where the inodes of files are written.
Secondly, file system aging has a significant impact on the reset profile as shown by \cref{fig:heatmap} (d) which has an aged F2FS (10$\times$ iterations). 
Both of these results make a case for a systematic study of how zone resets are called or consequently, how data grouping by uniform lifetime or death time is done. 




% Tool versions used
    % QEMU version: v6.0.0
    % OS: linux 5.19.0 kernel and Ubuntu 22.04
    % bpftrace: v0.15.0-43-g7667
    % rocksdb: v7.4.3
    % mongodb: v6.06
    % postgresql: 9.6.24


\subsection{RocksDB on F2FS with ZNS Timeline}
To demonstrate an end-to-end utility of \zt{}, we build an end-to-end data placement visualization using RocksDB on F2FS. Using the RocksDB's db\_bench benchmark with workloads \textit{fillrandom} and \textit{overwrite} fills the F2FS file system on the ZNS device. Minor modifications (less than 100 LOCs) to the RocksDB source code are made to trace the activity of operations, and retrieve the data mapping of the generated files. On each compaction or flush operation, RocksDB calls the \zt{} to map all its files. 
\cref{fig:rocksdb-timeline} illustrates this end-to-end timeline (automatically generated from traces) with a few salient events across the layers (applications, file systems, and ZNS device) over the lifetime of an SSTable. The timeline starts at time 0, where data is flushed from memory to level-0 (abbreviated L0) as SSTable 31. The file is stored in the orange zone classified as WARM. Subsequently, F2FS issues a series of reset calls on several zones. The next logical event is the compaction of file 31 with 33 and 34 to generate file 37 on L2\footnote{internal trivial promotion events such as moving 31 from L0 to L1 are skipped in the visualization for the sake of clarity.}. While file 37 is written as WARM file, old files in prior zones are deleted (pink lines). In the next round, file 37 is picked up for compaction with 35 and 38 files to generate two L2 files (41, and 42). 
Such a timeline visualization gives an understanding of how the different RocksDB operation interaction with F2FS affects the utilization of the ZNS storage space, file classification, and data movement over time, thus making it easy to spot contract violations visually. 


% Figure environment removed


\section{Related Work}
Flash SSDs with their complex internal logic and \textit{``unwritten contracts''}~\cite{2017-eurosys-unwritten} have also been studied in detail for performance and operation characterizations~\cite{2021-tos-ssd-blackbox,2018-ieee-ssd-bottleneck-analysis,2010-mascots-ssd-blackbox-modeling,2011-mascots-ssd-modeling,2013-sigmetrics-ssd-expectations,2022-systor-ssd-study} and with the impact of GC operations~\cite{2018-tos-ibm-flashsystems,2019-tom-d-choice-gc,2021-sigmetric-ssd-management,2012-sigmetrics-ssd-perf-anamoly,2013-sigmetrics-gc-modeling}. 
There is a rich history of collecting file system traces, analyzing, and replaying them for understanding the impact of optimizations~\cite{2013-Jeong-AndroStep,1985-sosp-bsd-trace, 2003-lisa-nfs, 2000-atc-fs-comparison, 2004-fast-tracefs,DBLP:conf/fast/TarasovKMHPKZ12}. Much of these works only focus on basic read/write interfaces that are sufficient for HDDs, but not SSDs. 
None of these tools track device management-related operations (reset, open, finish, close), which are now a critical part of ZNS devices. The libzbd ZNS library also support a basic GUI visualization tool for zone states~\cite{2023-libzbd}, however, it lacks any application or file system level information. 
Beyond these tools, the eBPF-based BCC framework has emerged as the de-facto API for writing complex, end-to-end tracking frameworks~\cite{2023-bcc}, which we also leverage. 
Similar to \zt{}, IOScope uses eBPF-assisted file offset-based I/O tracing~\cite{2018-Saif-IOscope}. However, its tracing is limited to the files (at the VFS level), and does not connect the file to its location, which can change based on the file system and application level operations. Several block-level tracking tools exist (BCC's biotop and biosnoop, DTraces's iosnoop), however, they do not link block-level I/O back to the file system or applications. MapFS is a file system interface that presents low-level details from file to storage mappings for applications to manipulate data-heavy operations via light-weight metadata operations~\cite{2011-hotstorage-mapfs}. 
Prabhakaran et al.~\cite{2005-atc-stp} introduce techniques to study file system behavior with semantic knowledge of events and on-disk data structure layouts. \zt{} extends such motivation to include workloads with the new ZNS management operations as well.  
HintStore is a flexible framework that is designed to explore the effectiveness of hints with heterogeneous storage~\cite{2022-Ge-HintStor}. \zt{} analysis captures the after-effect of the hints, and its collected data can be used to verify if hints are implemented in an end-to-end manner. 
In a distributed setting, systems like Wintermute~\cite{2020-hpdc-wintermute}, Apollo~\cite{2021-hpdc-apollo}, and Beacon~\cite{2019-nsdi-io-supercomputer} provide a distributed framework for operational data collection and analysis. 
In comparison to these works, the focus of \zt{} is on collecting, analyzing, and visualizing written ZNS contracts across multiple storage stack layers to reason about previously unwritten SSD storage contracts. 


%Collection of traces to study storage systems has a long history. Ousterhoust et al. ~\cite{1985-sosp-bsd-trace} present one of the early results from trace collection, operational data analystics, and simulator based trace replay to study the impact of caches on file system performance.  


%Ellard and Seltzer make a case for decoupling NFS trace collection from analysis~\cite{2003-lisa-nfs}. 


%~\cite{2000-atc-fs-comparison}  does trace collection from various deployment modes, file systems and operating systems for a given workload. 
%Geriatrix~\cite{2018-atc-Geriatrix} discusses the impact of aging on file system performance. Counter stacks do 
%File system traces~\cite{2004-fast-tracefs}, 
%Workload characterization~\cite{2014-osdi-counterstack}, 

\section{Conclusion and Ongoing Work}
In this work, we have presented a case to systematically reason about previously ``\textit{unwritten contracts}'' for flash SSDs on recently emerged ZNS flash SSD devices. Due to the unique I/O and management interface of ZNS SSDs, they make such unwritten contracts, written by putting them under the control of the host storage stack. To investigate the effectiveness of ZNS contracts, we have developed and presented a set of \zt{} to collect, analyze, and visualize ZNS operational data. 
We are working on tracing events from PostgreSQL, Aerospike, and scientific workloads on top of BrtFS also, to develop a broader framework in which such end-to-end contract analysis can be done. 


\textbf{Acknowledgment:} This work in-parts is supported by the donations from Western Digital. 


%-------------------------------------------------------------------------------
\section*{Availability}
%-------------------------------------------------------------------------------

\url{https://github.com/stonet-research/zns-tools}. 

%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{main}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks
