\section{Method}

% Figure environment removed

The overall framework of the proposed method is shown in Figure~\ref{fig:fig3}, and pseudo-code of PromptStyler is described in Algorithm~\ref{alg:pseudocode_prompt}.
Our method learns style word vectors to represent a variety of styles in a hyperspherical joint vision-language space (\eg, CLIP~\cite{radford2021clip} latent space).
After learning those style word vectors, we train a linear classifier using synthesized style-content features produced by a pre-trained text encoder $T(\cdot)$.
At inference time, a pre-trained image encoder $I(\cdot)$ extracts image features from input images, which are fed as input to the trained linear classifier.
Thanks to the cross-modal transferability phenomenon of the joint vision-language space~\cite{zhang2023diagnosing},
this classifier could produce class scores using the image features.
Note that we exploit CLIP as our large-scale vision-language model;
its image encoder and text encoder are frozen in our entire framework.

\subsection{Prompt-driven style generation}
\label{main:main_3_1}
%
An input text prompt is converted to several tokens via a tokenization process, and then such tokens are replaced by their corresponding word vectors via a word lookup process.
In \mbox{PromptStyler}, a pseudo-word $\boldsymbol{S}_{i}$ in a prompt is a placeholder which is replaced by a style word vector $\mathbf{s}_{i} \in \mathbb R^{D}$ during the word lookup process.
Note that three kinds of prompts are used in the proposed method:
a style prompt $\mathcal{P}_{i}^{\,\mathrm{style}}$ (``a $\boldsymbol{S}_{i}$ style of a"), a content prompt $\mathcal{P}_{m}^{\,\mathrm{content}}$ (``[class]$_{m}$"), and a style-content prompt $\mathcal{P}_{i}^{\,\mathrm{style}} \circ \mathcal{P}_{m}^{\,\mathrm{content}}$ (``a $\boldsymbol{S}_{i}$ style of a [class]$_{m}"$).
$\boldsymbol{S}_{i}$ indicates the placeholder for $i$-th style word vector and [class]$_{m}$ denotes $m$-th class name.

Suppose we want to generate $K$ different styles in a joint vision-language space.
In this case, the proposed method needs to learn $K$ style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$, where each $\mathbf{s}_{i}$ is randomly initialized at the beginning.
To effectively simulate various distribution shifts in the joint vision-language space,
those style word vectors need to be diverse while not distorting content information when they are exploited in style-content prompts.
There are two possible design choices for learning such word vectors: 
(1) learning each style word vector $\mathbf{s}_{i}$ in a sequential manner, or (2) learning all style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$ in a parallel manner.
We choose the former, since it takes much less memory during training.
Please refer to the supplementary material (Section \redcolornumber{A.2}) for the empirical justification of our design choice.

\noindent \textbf{Style diversity loss.}
%
To maximize the diversity of $K$ styles in a hyperspherical joint vision-language space,
we sequentially learn style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$ in such a way that $i$-th style feature $T(\mathcal{P}_{i}^{\,\mathrm{style}}) \in \mathbb R^{C}$ produced by $i$-th style word vector  $\mathbf{s}_{i}$ is orthogonal to $\{T(\mathcal{P}_{j}^{\,\mathrm{style}})\}^{i-1}_{j=1}$ produced by previously learned style word vectors $\{\mathbf{s}_{j}\}^{i-1}_{j=1}$.
Regarding this, the style diversity loss $\mathcal{L}_{\mathrm{style}}$ for learning $i$-th style word vector $\mathbf{s}_{i}$ is computed by
%
\begin{align}
    \label{eq:loss_style}
    \mathcal{L}_{\mathrm{style}} &= \frac{1}{i-1} \sum^{i-1}_{j=1} \left| \frac{T(\mathcal{P}_{i}^{\,\mathrm{style}})}{\Vert T(\mathcal{P}_{i}^{\,\mathrm{style}}) \Vert_{\scriptscriptstyle{2}}} \bigcdot \frac{T(\mathcal{P}_{j}^{\,\mathrm{style}})}{\Vert T(\mathcal{P}_{j}^{\,\mathrm{style}}) \Vert_{\scriptscriptstyle{2}}} \right| \;.
\end{align}
%
This style loss $\mathcal{L}_{\mathrm{style}}$ aims to minimize the absolute value of the cosine similarity between $i$-th style feature and each of the existing style features.
When the value of this loss becomes zero, it satisfies the orthogonality between $i$-th style feature and all the existing style features.

\noindent \textbf{Content consistency loss.}
%
Learning the style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$ only using the style diversity loss sometimes leads to undesirable outcome, 
since a learned style $\mathbf{s}_{i}$ could substantially distort content information when used to generate a style-content feature $T(\mathcal{P}_{i}^{\,\mathrm{style}} \circ \mathcal{P}_{m}^{\,\mathrm{content}}) \in \mathbb R^{C}$.
To alleviate this problem, we encourage the content information in the style-content feature to be consistent with its corresponding content feature $T(\mathcal{P}_{m}^{\,\mathrm{content}}) \in \mathbb R^{C}$ while learning each $i$-th style word vector  $\mathbf{s}_{i}$.
Specifically, each style-content feature synthesized via $i$-th style word vector $\mathbf{s}_{i}$ should have the highest cosine similarity score with its corresponding content feature.
For $i$-th style word vector $\mathbf{s}_{i}$, a cosine similarity score $z_{imn}$ between a style-content feature with $m$-th class name and a content feature with $n$-th class name is computed by
%
\begin{align}
    \label{eq:logit_content}
    z_{imn} &= \frac{T(\mathcal{P}_{i}^{\,\mathrm{style}} \circ \mathcal{P}_{m}^{\,\mathrm{content}})}{\Vert T(\mathcal{P}_{i}^{\,\mathrm{style}} \circ \mathcal{P}_{m}^{\,\mathrm{content}}) \Vert_{\scriptscriptstyle{2}}} \bigcdot \frac{T(\mathcal{P}_{n}^{\,\mathrm{content}})}{\Vert T(\mathcal{P}_{n}^{\,\mathrm{content}}) \Vert_{\scriptscriptstyle{2}}} \;.
\end{align}
%
Using cosine similarity scores between style-content features and content features, the content consistency loss $\mathcal{L}_{\mathrm{content}}$ for learning $i$-th style word vector $\mathbf{s}_{i}$ is computed by 
%
\begin{align}
    \label{eq:loss_content}
    \mathcal{L}_{\mathrm{content}} &= -\frac{1}{N} \sum^{N}_{m=1} \log \left( \frac{\mathrm{exp}(z_{imm})}{\sum^{N}_{n=1}\mathrm{exp}(z_{imn})} \right) ,
\end{align}
%
where $N$ denotes the number of classes pre-defined in the target task.
This content loss $\mathcal{L}_{\mathrm{content}}$ is a contrastive loss which encourages each style-content feature to be located closer to its corresponding content feature so that it forces each $i$-th style word vector $\mathbf{s}_{i}$ to preserve content information when used to synthesize style-content features.

\noindent \textbf{Total prompt loss.}
%
\mbox{PromptStyler} learns $K$ style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$ in a sequential manner, where each $i$-th style word vector $\mathbf{s}_{i}$ is learned 
using both $\mathcal{L}_{\mathrm{style}}$ (Eq.\,(\ref{eq:loss_style})) and $\mathcal{L}_{\mathrm{content}}$ (Eq.\,(\ref{eq:loss_content})).
In the proposed method, the total loss $\mathcal{L}_{\mathrm{prompt}}$ for learning $i$-th style word vector is computed by
%
\begin{align}
    \label{eq:loss_prompt}
    \mathcal{L}_{\mathrm{prompt}} &= \mathcal{L}_{\mathrm{style}} + \mathcal{L}_{\mathrm{content}} \;.
\end{align}
%
Using this prompt loss $\mathcal{L}_{\mathrm{prompt}}$, we train $i$-th style word vector $\mathbf{s}_{i}$ for $L$ training iterations.

\subsection{Training a linear classifier using diverse styles}
%
After learning $K$ style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$, we generate $KN$ style-content features for training a linear classifier.
To be specific, we synthesize those features using the learned $K$ styles and pre-defined $N$ classes via the text encoder $T(\cdot)$.
The linear classifier is trained by a classification loss using $\ell_2$-normalized style-content features and their class labels;
each class label is the class name used to generate each style-content feature.
To effectively leverage the hyperspherical joint vision-language space,
we adopt ArcFace~\cite{ArcFace} loss as our classification loss $\mathcal{L}_{\mathrm{class}}$.
Note that ArcFace loss is an angular Softmax loss which computes the cosine similarities between classifier input features and classifier weights with an additive angular margin penalty between classes.
This angular margin penalty allows for more discriminative predictions by pushing features from different classes further apart.
Thanks to the property, this angular Softmax loss has been widely used in visual recognition tasks~\cite{ArcFace_app1,ArcFace_app2,ArcFace_app3,ArcFace_app4,ArcFace_app5}.

\begin{algorithm}[t!]
    \caption{\mbox{PromptStyler}}
    \label{alg:pseudocode_prompt}
    \textbf{Requirement:} pre-trained text encoder $T(\cdot)$, pre-defined $N$ \hspace*{\algorithmicindent}\hspace*{\algorithmicindent}\;\;\;\;\;\;\;\;\;\;\,\,class names in the target task \\
    %
    \textbf{Input:} number of style word vectors $K$, number of training \hspace*{\algorithmicindent}\hspace*{\algorithmicindent}iterations $L$ \\
    %
    \textbf{Output:}
    $KN$ style-content features
    %
    \begin{algorithmic}[1]
        \Statex \commentcolor{\# randomly initialize style word vectors}
        
        \State 
        $\{\mathbf{s}_{i}\}^{K}_{i=1} \leftarrow \mathtt{random\_initialize}(\{\mathbf{s}_{i}\}^{K}_{i=1})$
        
        \Statex \commentcolor{\# sequentially learn $K$ style word vectors}
        
        \For {$i=1,2,\ldots,K$} 
        
            \Statex \commentcolor{\;\;\;\;\;\,\# $L$ training iterations for learning each word vector}
            
            \For {$\mathrm{iteration}=1,2,\ldots,L$} 
                
                \Statex \commentcolor{\;\;\;\;\;\;\;\;\;\;\,\,\# compute $\mathcal{L}_{\mathrm{style}}$ using $T(\cdot)$ and word vectors}
                
                \State $\mathcal{L}_{\mathrm{style}} \leftarrow \mathtt{style\_diversity\_loss}(\mathbf{s}_{i}, \{\mathbf{s}_{j}\}^{i-1}_{j=1})$
                
                \Statex \commentcolor{\;\;\;\;\;\;\;\;\;\;\,\,\# compute $\mathcal{L}_{\mathrm{content}}$ using $T(\cdot)$ and a word vector}
                
                \State $\mathcal{L}_{\mathrm{content}} \leftarrow \mathtt{content\_consistency\_loss}(\mathbf{s}_{i})$
                
                \State
                $\mathcal{L}_{\mathrm{prompt}} \leftarrow
                \mathcal{L}_{\mathrm{style}} + \mathcal{L}_{\mathrm{content}}$ 
                
                \State
                Update $\mathbf{s}_{i}$ using $\mathcal{L}_{\mathrm{prompt}}$ by gradient descent
                
            \EndFor
            
        \EndFor
        \State
        Synthesize $KN$ style-content features using the learned $K$ style word vectors and $N$ class names via $T(\cdot)$ 
    \end{algorithmic} 
    %
\end{algorithm} 


\subsection{Inference using the trained classifier}
%
The trained classifier is used with a pre-trained image encoder $I(\cdot)$ at inference time.
Given an input image $\mathbf{x}$, the image encoder extracts its image feature $I(\mathbf{x}) \in \mathbb{R}^{C}$, which is mapped to the hyperspherical joint vision-language space by $\ell_2$ normalization.
Then, the trained classifier produces class scores using the $\ell_2$-normalized image feature.
Note that the text encoder $T(\cdot)$ is not used at inference time, while the image encoder $I(\cdot)$ is only exploited at inference time.
%