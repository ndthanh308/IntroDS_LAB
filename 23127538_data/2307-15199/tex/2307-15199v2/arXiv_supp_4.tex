\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand\thefigure{D\arabic{figure}}
\renewcommand\thetable{D\arabic{table}}

\section{Discussion}
\label{supp:supp_4}
%
PromptStyler aims to improve model's generalization capability by simulating various distribution shifts in the latent space of a large-scale pre-trained model.
To achieve this goal, our method leverages a joint vision-language space where text features could effectively represent their relevant image features.
It does not mean that image and text features should be perfectly interchangeable in the joint vision-language space;
a recent study has demonstrated the modality gap phenomenon of this joint space~\cite{liang2022mind}.
However, thanks to the cross-modal transferability in the joint vision-language space~\cite{zhang2023diagnosing},
the proposed method could still be effective,
\ie, we could consider text features as proxies for image features while training a linear classifier (Fig.~\redcolornumber{3} of the main paper).

When our method is implemented with CLIP~\cite{radford2021clip} and we adopt ArcFace~\cite{ArcFace} as our classification loss $\mathcal{L}_{\mathrm{class}}$, there is another interesting interpretation of the proposed method.
As described in Section~\redcolornumber{A.1},
CLIP text encoder synthesizes classifier weights using class names for zero-shot inference and then it computes cosine similarity scores between the classifier weights and input image features.
Similarly, our method computes cosine similarity scores between classifier weights of the trained classifier (Fig.~\redcolornumber{3} of the main paper) and input image features.
From this perspective,
the proposed method improves the decision boundary of the synthesized classifier used in zero-shot CLIP by generating diverse style-content features and then training a linear classifier using the style-content features. 
In other words, 
the trained classifier could be considered as an improved version of the synthesized classifier used in zero-shot CLIP.
%