\section{Experiments}
%
For more comprehensive understanding, please refer to the supplementary material (Section \redcolornumber{B} and \redcolornumber{D}).

\subsection{Evaluation datasets}
%
The proposed method does not require any actual data for training.
To analyze its generalization capability,
four domain generalization benchmarks are used for evaluation:
\textbf{PACS}~\cite{PACSdataset} (4 domains and 7 classes), \textbf{VLCS}~\cite{VLCSdataset} (4 domains and 5 classes), \textbf{OfficeHome}~\cite{OfficeHomedataset} (4 domains and 65 classes) and \textbf{DomainNet}~\cite{DomainNetdataset} (6 domains and 345 classes).
On these benchmarks, we repeat each experiment three times using different random seeds and report average top-1 classification accuracies with standard errors.
Unlike the \textit{leave-one-domain-out cross-validation} evaluation protocol~\cite{gulrajani2021domainbed}, we do not exploit any source domain data for training.

\subsection{Implementation details}
%
PromptStyler is implemented and trained with the same configuration regardless of the evaluation datasets.
Training takes about $30$ minutes using a single RTX 3090 GPU.

\noindent \textbf{Architecture.}
% 
We choose CLIP~\cite{radford2021clip} as our large-scale pre-trained vision-language model, and use the publicly available pre-trained model.\footnote{\url{https://github.com/openai/CLIP}}
The text encoder $T(\cdot)$ used in training is Transformer~\cite{vaswani2017attention} and the image encoder $I(\cdot)$ used at inference is ResNet-50~\cite{resnet} as default setting in experiments; 
our method is also implemented with ViT-B/16~\cite{dosovitskiy2021an} or ViT-L/14~\cite{dosovitskiy2021an} for further evaluations as shown in Table~\ref{table:main_result}.
Note that text and image encoders are derived from the same CLIP model and frozen in the entire pipeline.
The dimension of each text feature or image feature is $C=1024$ when our method is implemented with ResNet-50, while $C=512$ in the case of ViT-B/16 and $C=768$ in the case of ViT-L/14.


\noindent \textbf{Learning style word vectors.}
% 
We follow prompt learning methods~\cite{CoOp,CoCoOp} when learning the word vectors.
Using a zero-mean Gaussian distribution with $0.02$ standard deviation, 
we randomly initialize $K$ style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$, where $K=80$. 
The dimension of each style word vector is $D=512$ when the proposed method is implemented with ResNet-50~\cite{resnet} or ViT-B/16~\cite{dosovitskiy2021an}, while $D=768$ in the case of ViT-L/14~\cite{dosovitskiy2021an}.
Each $i$-th style word vector $\mathbf{s}_{i}$ is trained by the prompt loss $\mathcal{L}_{\mathrm{prompt}}$ for $L=100$ training iterations using the SGD optimizer with $0.002$ learning rate and $0.9$ momentum.
The number of classes $N$ is pre-defined by each target task definition, \eg, $N=345$ for DomainNet~\cite{DomainNetdataset}.

\noindent \textbf{Training a linear classifier.}
%
The classifier is trained for $50$ epochs using the SGD optimizer with $0.005$ learning rate, $0.9$ momentum, and a batch size of $128$.
In ArcFace~\cite{ArcFace} loss, its scaling factor is set to $5$ with $0.5$ angular margin.

\noindent \textbf{Inference.}
%
Input images are pre-processed in the same way with the CLIP model; resized to $224\times224$ and normalized.


\begin{table*}[!t]
    \centering
    \resizebox{\textwidth}{!}{
        % 
        \begin{tabular}{lccccccc|c}
        %
        \Xhline{2\arrayrulewidth}
        % 
        \multicolumn{1}{c}{}
        & \multicolumn{2}{c}{Configuration}
        & \multicolumn{1}{c}{}
        & \multicolumn{5}{c}{Accuracy (\%)}
        \\
        \cline{2-3}
        \cline{5-9}
        % 

        \vspace{-0.8mm}
        & Source & Domain & \;
        & & & &
        &
        \\
        Method 
        & Domain & Description &
        & \normalsize{P}\small{ACS} & \normalsize{V}\small{LCS} & \normalsize{O}\small{fficeHome} & \normalsize{D}\small{omainNet} 
        & \normalsize{A}\small{vg.}
        \\
        \hline
        \multicolumn{9}{c}{\textit{ResNet-50~\cite{resnet} with pre-trained weights on ImageNet~\cite{deng2009imagenet}}}
        \\
        % 
        \hline
        %
            DANN~\cite{DANN}
            & \ding{51} & \textbf{--} &
            & 83.6\scriptsize{$\pm{0.4}$} & 78.6\scriptsize{$\pm{0.4}$} & 65.9\scriptsize{$\pm{0.6}$} & 38.3\scriptsize{$\pm{0.1}$} 
            & 66.6
        \\
            RSC~\cite{RSC}
            & \ding{51} & \textbf{--} &
            & 85.2\scriptsize{$\pm{0.9}$} & 77.1\scriptsize{$\pm{0.5}$} & 65.5\scriptsize{$\pm{0.9}$} & 38.9\scriptsize{$\pm{0.5}$} 
            & 66.7
        \\
            MLDG~\cite{Li2018Learning}
            & \ding{51} & \textbf{--} &
            & 84.9\scriptsize{$\pm{1.0}$} & 77.2\scriptsize{$\pm{0.4}$} & 66.8\scriptsize{$\pm{0.6}$} & 41.2\scriptsize{$\pm{0.1}$} 
            & 67.5
        \\
            SagNet~\cite{SagNet}
            & \ding{51} & \textbf{--} &
            & \textbf{86.3}\scriptsize{$\pm{0.2}$} & 77.8\scriptsize{$\pm{0.5}$} & 68.1\scriptsize{$\pm{0.1}$} & 40.3\scriptsize{$\pm{0.1}$} 
            & 68.1
        \\
            SelfReg~\cite{SelfReg}
            & \ding{51} & \textbf{--} &
            & 85.6\scriptsize{$\pm{0.4}$} & 77.8\scriptsize{$\pm{0.9}$} & 67.9\scriptsize{$\pm{0.7}$} & 42.8\scriptsize{$\pm{0.0}$} 
            & 68.5
        \\
            GVRT~\cite{Min2022Grounding}
            & \ding{51} & \textbf{--} &
            & 85.1\scriptsize{$\pm{0.3}$} & \textbf{79.0}\scriptsize{$\pm{0.2}$} & 70.1\scriptsize{$\pm{0.1}$} & 44.1\scriptsize{$\pm{0.1}$} 
            & 69.6
        \\
            MIRO~\cite{cha2022miro}
            & \ding{51} & \textbf{--} &
            & 85.4\scriptsize{$\pm{0.4}$} & \textbf{79.0}\scriptsize{$\pm{0.0}$} & \textbf{70.5}\scriptsize{$\pm{0.4}$} & \textbf{44.3}\scriptsize{$\pm{0.2}$}
            & \textbf{69.8}
        \\
        \hhline{-|-|-|-|-|-|-|-|-|}
        \multicolumn{9}{c}{\textit{ResNet-50~\cite{resnet} with pre-trained weights from CLIP~\cite{radford2021clip}}}
        \\
        \hhline{-|-|-|-|-|-|-|-|-|}
            ZS-CLIP (C)~\cite{radford2021clip}\;\;\;\;
            & \textbf{--} & \textbf{--} &
            & 90.6\scriptsize{$\pm{0.0}$} & 76.0\scriptsize{$\pm{0.0}$} & 68.6\scriptsize{$\pm{0.0}$} & 45.6\scriptsize{$\pm{0.0}$} 
            & 70.2
        \\
            CAD~\cite{CAD}
            & \ding{51} & \textbf{--} &
            & 90.0\scriptsize{$\pm{0.6}$} & 81.2\scriptsize{$\pm{0.6}$} & 70.5\scriptsize{$\pm{0.3}$} & 45.5\scriptsize{$\pm{2.1}$} 
            & 71.8
        \\
            ZS-CLIP (PC)~\cite{radford2021clip}
            & \textbf{--} & \ding{51} &
            & 90.7\scriptsize{$\pm{0.0}$} & 80.1\scriptsize{$\pm{0.0}$} & 72.0\scriptsize{$\pm{0.0}$} & 46.2\scriptsize{$\pm{0.0}$} 
            & 72.3
        \\
            \cellcolor{gray!9.0}\textbf{PromptStyler}
            & \cellcolor{gray!9.0}\textbf{--} & \cellcolor{gray!9.0}\textbf{--} & \cellcolor{gray!9.0} & \cellcolor{gray!9.0}\textbf{93.2}\scriptsize{$\pm{0.0}$} & \cellcolor{gray!9.0}\textbf{82.3}\scriptsize{$\pm{0.1}$} & \cellcolor{gray!9.0}\textbf{73.6}\scriptsize{$\pm{0.1}$} & \cellcolor{gray!9.0}\textbf{49.5}\scriptsize{$\pm{0.0}$} 
            & \cellcolor{gray!9.0}\textbf{74.7}
        \\
        %
        \hhline{-|-|-|-|-|-|-|-|-|}
        %
        \multicolumn{9}{c}{\textit{ViT-B\,/\,16~\cite{dosovitskiy2021an} with pre-trained weights from CLIP~\cite{radford2021clip}}}
        \\
        %
        \hhline{-|-|-|-|-|-|-|-|-|}
        %
            ZS-CLIP (C)~\cite{radford2021clip}
            & \textbf{--} & \textbf{--} &
            & 95.7\scriptsize{$\pm{0.0}$} & 76.4\scriptsize{$\pm{0.0}$} & 79.9\scriptsize{$\pm{0.0}$} & 57.8\scriptsize{$\pm{0.0}$} 
            & 77.5
        \\
            MIRO~\cite{cha2022miro}
            & \ding{51} & \textbf{--} &
            & 95.6 & 82.2 & 82.5 & 54.0 
            & 78.6
        \\
            ZS-CLIP (PC)~\cite{radford2021clip}
            & \textbf{--} & \ding{51} &
            & 96.1\scriptsize{$\pm{0.0}$} & 82.4\scriptsize{$\pm{0.0}$} & 82.3\scriptsize{$\pm{0.0}$} & 57.7\scriptsize{$\pm{0.0}$} 
            & 79.6
        \\
            \cellcolor{gray!9.0}\textbf{PromptStyler}
            & \cellcolor{gray!9.0}\textbf{--} & \cellcolor{gray!9.0}\textbf{--} & \cellcolor{gray!9.0} & \cellcolor{gray!9.0}\textbf{97.2}\scriptsize{$\pm{0.1}$} & \cellcolor{gray!9.0}\textbf{82.9}\scriptsize{$\pm{0.0}$} & \cellcolor{gray!9.0}\textbf{83.6}\scriptsize{$\pm{0.0}$} & \cellcolor{gray!9.0}\textbf{59.4}\scriptsize{$\pm{0.0}$} 
            & \cellcolor{gray!9.0}\textbf{80.8}
        \\
        %
        \hhline{-|-|-|-|-|-|-|-|-|}
        %
        \multicolumn{9}{c}{\textit{ViT-L\,/\,14~\cite{dosovitskiy2021an} with pre-trained weights from CLIP~\cite{radford2021clip}}}
        \\
        %
        \hhline{-|-|-|-|-|-|-|-|-|}
        %
            ZS-CLIP (C)~\cite{radford2021clip}
            & \textbf{--} & \textbf{--} &
            & 97.6\scriptsize{$\pm{0.0}$} & 77.5\scriptsize{$\pm{0.0}$} & 85.9\scriptsize{$\pm{0.0}$} & 63.3\scriptsize{$\pm{0.0}$} 
            & 81.1
        \\
            ZS-CLIP (PC)~\cite{radford2021clip}
            & \textbf{--} & \ding{51} &
            & 98.5\scriptsize{$\pm{0.0}$} & \textbf{82.4}\scriptsize{$\pm{0.0}$} & 86.9\scriptsize{$\pm{0.0}$} & 64.0\scriptsize{$\pm{0.0}$} 
            & 83.0
        \\            
            \cellcolor{gray!9.0}\textbf{PromptStyler}
            & \cellcolor{gray!9.0}\textbf{--} & \cellcolor{gray!9.0}\textbf{--} & \cellcolor{gray!9.0} & \cellcolor{gray!9.0}\textbf{98.6}\scriptsize{$\pm{0.0}$} & \cellcolor{gray!9.0}\textbf{82.4}\scriptsize{$\pm{0.2}$} & \cellcolor{gray!9.0}\textbf{89.1}\scriptsize{$\pm{0.0}$} & \cellcolor{gray!9.0}\textbf{65.5}\scriptsize{$\pm{0.0}$} 
            & \cellcolor{gray!9.0}\textbf{83.9}
        \\
        %
        \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \vspace{-2mm}
    %
    \caption{Comparison with the state-of-the-art domain generalization methods.
    ZS-CLIP (C) denotes zero-shot CLIP using ``[class]" as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using ``a photo of a [class]" as its text prompt.
    Note that \mbox{PromptStyler} does not exploit any source domain data and domain descriptions.}
    %
    \vspace{0.25mm}
    \label{table:main_result}
\end{table*}


\subsection{Evaluations}
%

\noindent \textbf{Main results.}
PromptStyler achieves the state of the art in every evaluation on PACS~\cite{PACSdataset}, VLCS~\cite{VLCSdataset}, OfficeHome~\cite{OfficeHomedataset} and DomainNet~\cite{DomainNetdataset} as shown in Table~\ref{table:main_result}.
Note that all existing methods utilize source domain data except for zero-shot CLIP~\cite{radford2021clip} in Table~\ref{table:main_result}.
Compared with zero-shot CLIP which generates each text feature using a domain-agnostic prompt (``[class]"),
PromptStyler largely outperforms its records in all evaluations.
Our method also shows higher accuracy compared with zero-shot CLIP which produces each text feature using a domain-specific prompt (``a photo of a [class]"), 
even though we do not exploit any domain descriptions.
These results confirm that the proposed method effectively improves the generalization capability of the chosen pre-trained model, \ie, CLIP, without using any images by simulating various distribution shifts via prompts in its latent space.

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lcccc}
        %
        \Xhline{2\arrayrulewidth}
        \multicolumn{1}{c}{}
        & \multicolumn{2}{c}{Inference Module}
        & \multicolumn{2}{c}{}
        \\
        \cline{2-3}
        \vspace{-0.5mm}
        & Image & Text & &
        \\
        Method 
        & Encoder & Encoder & \!\#\,Params\! & \!FPS\!
        \\
        %
        \hline
        %
        \multicolumn{5}{c}{\textit{\normalsize{O}\small{fficeHome} \normalsize{(65 classes)}}}
        \\
            \hhline{-|-|-|-|-|}
            ZS-CLIP~\cite{radford2021clip}
            & \ding{51} & \ding{51}
            & 102.0M
            & 1.6
        \\
            \cellcolor{gray!9.0}\textbf{PromptStyler}
            & \cellcolor{gray!9.0}\ding{51} & \cellcolor{gray!9.0}\textbf{--} 
            & \cellcolor{gray!9.0}\textbf{38.4M}
            & \cellcolor{gray!9.0}\textbf{72.9}
        \\
        %
        \hhline{-|-|-|-|-|}
        %
        \multicolumn{5}{c}{\textit{\normalsize{D}\small{omainNet} \normalsize{(345 classes)}}}
        \\
            %
            \hhline{-|-|-|-|-|}
            %
            ZS-CLIP~\cite{radford2021clip}
            & \ding{51} & \ding{51}
            & 102.0M
            & 0.3
        \\
            \cellcolor{gray!9.0}\textbf{PromptStyler}
            & \cellcolor{gray!9.0}\ding{51} & \cellcolor{gray!9.0}\textbf{--} 
            & \cellcolor{gray!9.0}\textbf{38.7M}
            & \cellcolor{gray!9.0}\textbf{72.9}
        \\
        %
        \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \vspace{-2mm}
    %
    \caption{The number of parameters and inference speed on OfficeHome~\cite{OfficeHomedataset} and DomainNet~\cite{DomainNetdataset} using ResNet-50~\cite{resnet} as an image encoder. 
    Note that CLIP~\cite{radford2021clip} text encoder needs to generate text features as many as the number of classes.}
    %
    \vspace{-5.5mm}
    \label{table:param_speed}
\end{table}

% Figure environment removed

% Figure environment removed

\noindent \textbf{Computational evaluations.}
%
In Table~\ref{table:param_speed}, we compare our PromptStyler and zero-shot CLIP~\cite{radford2021clip} in terms of the number of parameters and inference speed;
the inference speed was measured using a single RTX 3090 GPU with a batch size of $1$.
Note that we do not exploit a text encoder at inference time, 
which makes our model $\sim$2.6$\times$ smaller and $\sim$243$\times$ faster compared with CLIP.
Regarding the inference speed, the proposed model is about $45\times$ faster for the target task OfficeHome~\cite{OfficeHomedataset} ($65$ classes) and it is about $243\times$ faster for the target task DomainNet~\cite{DomainNetdataset} ($345$ classes).

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{cccccc|c}
        %
        \Xhline{2\arrayrulewidth}
        \multicolumn{2}{c}{}
        & \multicolumn{5}{c}{\small{Accuracy (\%)}}
        \\
        \cline{3-7}
        \!$\mathcal{L}_{\mathrm{style}}$\! & \!$\mathcal{L}_{\mathrm{content}}$\! 
        & \!\small{P}\scriptsize{ACS}\! & \!\small{V}\scriptsize{LCS}\! & \!\small{O}\scriptsize{fficeHome}\!\! & \!\small{D}\scriptsize{omainNet}\! & \!\small{A}\scriptsize{vg.}\!
        \\
        %
        \hline
            \!\textbf{--}\! & \!\textbf{--}\! 
            & \!92.6\! & \!78.3\! & \!72.2\!\! & \!48.0\! & \!72.8\!
        \\
            \!\ding{51}\! & \!\textbf{--}\! 
            & \!92.3\! & \!80.9\! & \!71.5\!\! & \!48.2\! & \!73.2\!
        \\
            \!\textbf{--}\! & \!\ding{51}\!
            & \!92.8\! & \!80.5\! & \!72.4\!\! & \!48.6\! & \!73.6\!
        \\
            \!\cellcolor{gray!9.0}\ding{51}\! & \!\cellcolor{gray!9.0}\ding{51}\!
            & \cellcolor{gray!9.0}\!\textbf{93.2}\! & \cellcolor{gray!9.0}\!\textbf{82.3}\! & \cellcolor{gray!9.0}\!\textbf{73.6}\!\! & \cellcolor{gray!9.0}\!\textbf{49.5}\! & \cellcolor{gray!9.0}\!\textbf{74.7}\!
        \\
        %
        \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \vspace{-2mm}
    %
    \caption{Ablation study on the style diversity loss $\mathcal{L}_{\mathrm{style}}$ and content consistency loss $\mathcal{L}_{\mathrm{content}}$ used in the prompt loss.}
    %
    \vspace{-2.5mm}
    \label{table:ablation_loss}
\end{table}

\noindent \textbf{t-SNE visualization results.}
% 
In Figure~\ref{fig:fig4}, we qualitatively evaluate style-content features synthesized for the target task VLCS~\cite{VLCSdataset} (5 classes) using t-SNE~\cite{tSNE} visualization.
As shown in Figure~\ref{fig:fig4}(c), PromptStyler generates a variety of styles while not distorting content information;
style-content features obtained from the same class name share similar semantics with diverse variations.
This result confirms that we could effectively simulate various distribution shifts in the latent space of a large-scale vision-language model by synthesizing diverse styles via learnable style word vectors.

\noindent \textbf{Text-to-Image synthesis results.}
%
In Figure~\ref{fig:fig5}, we visualize style-content features (from ``a $\boldsymbol{S_{*}}$ style of a \textbf{cat}") via diffusers library.\footnote{\url{https://github.com/huggingface/diffusers}} 
These results are obtained with $6$ different style word vectors, 
where the word vectors are learned for the target task DomainNet~\cite{DomainNetdataset} using ViT-L/14~\cite{dosovitskiy2021an} model.

\subsection{More analyses}
\vspace{-1mm}

\noindent \textbf{Ablation study on the prompt loss.}
%
In Table~\ref{table:ablation_loss}, we evaluate the effects of $\mathcal{L}_{\mathrm{style}}$ and $\mathcal{L}_{\mathrm{content}}$ in $\mathcal{L}_{\mathrm{prompt}}$ used for learning style words.
Interestingly, our method also achieves state-of-the-art results even without using these losses, \ie, the proposed framework (Fig.~\ref{fig:fig3}) is substantially effective by itself.
Note that randomly initialized style word vectors are already diverse, and CLIP~\cite{radford2021clip} is already good at extracting correct content information from a style-content prompt even without training the word vectors using $\mathcal{L}_{\mathrm{content}}$.
When we learn style word vectors using $\mathcal{L}_{\mathrm{style}}$ without  $\mathcal{L}_{\mathrm{content}}$, 
style-content features obtained from different class names share more similar features than those from the same class name (Fig.~\ref{fig:fig4}(a)).
On the other hand, using $\mathcal{L}_{\mathrm{content}}$ without $\mathcal{L}_{\mathrm{style}}$ leads to less diverse style-content features (Fig.~\ref{fig:fig4}(b)).
When incorporating both losses,
we could generate diverse styles while not distorting content information (Fig.~\ref{fig:fig4}(c)).

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{ccccc|c}
        %
        \Xhline{2\arrayrulewidth}
        \multicolumn{1}{c}{}
        & \multicolumn{5}{c}{\small{Accuracy (\%)}}
        \\
        \cline{2-6}
        $\mathcal{L}_{\mathrm{class}}$
        & \!\small{P}\scriptsize{ACS}\! & \!\small{V}\scriptsize{LCS}\! & \!\small{O}\scriptsize{fficeHome}\!\! & \!\small{D}\scriptsize{omainNet}\! & \!\small{A}\scriptsize{vg.}\!
        \\
        %
        \hline
            \small{Softmax}
            & \!92.5\! & \!81.2\! & \!72.3\! & \!48.6\! & \!73.7\!
        \\
            \cellcolor{gray!9.0}\textbf{\small{ArcFace}}
            & \cellcolor{gray!9.0}\!\textbf{93.2}\! & \cellcolor{gray!9.0}\!\textbf{82.3}\! & \cellcolor{gray!9.0}\!\textbf{73.6}\! & \cellcolor{gray!9.0}\!\textbf{49.5}\! & \cellcolor{gray!9.0}\!\textbf{74.7}\!
        \\
        %
        \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \vspace{-2mm}
    %
    \caption{Ablation study on the classification loss $\mathcal{L}_{\mathrm{class}}$ used for training a linear classifier in the proposed framework.}
    %
    \vspace{-3.3mm}
    \label{table:ablation_classifier}
\end{table}

% Figure environment removed

% Figure environment removed

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccc}
        %
        \Xhline{2\arrayrulewidth}
        
        & \multicolumn{2}{c}{Configuration}
        & \multicolumn{1}{c}{}
        & \multicolumn{2}{c}{\!\!Accuracy (\%)\!\!}
        \\
        \cline{2-3}
        \cline{5-6}
        \vspace{-0.5mm}
        & \!\!Source\!\! & \!\!Domain\!\! &
        & \multicolumn{2}{c}{}
        \\
        Method
        & \!\!Domain\!\! & \!\!Description\!\! &
        & \multicolumn{2}{c}{\!\!\!\normalsize{T}\small{erra Incognita}\!\!\!}
        \\
        %
        \hline
        \multicolumn{6}{c}{\textit{ResNet-50~\cite{resnet} with pre-trained weights on ImageNet~\cite{deng2009imagenet}}}
        \\
        \hhline{-|-|-|-|-|-|}
            SelfReg~\cite{SelfReg}
            & \ding{51} & \textbf{--} &
            & \multicolumn{2}{c}{47.0\scriptsize{$\pm{0.3}$}}
        \\
            GVRT~\cite{Min2022Grounding}
            & \ding{51} & \textbf{--} &
            & \multicolumn{2}{c}{\textbf{48.0}\scriptsize{$\pm{0.2}$}}
        \\
        \hhline{-|-|-|-|-|-|}
        \multicolumn{6}{c}{\textit{ResNet-50~\cite{resnet} with pre-trained weights from CLIP~\cite{radford2021clip}}}
        \\
        \hhline{-|-|-|-|-|-|}
            ZS-CLIP (C)~\cite{radford2021clip}\!\!
            & \textbf{--} & \textbf{--} &
            & \multicolumn{2}{c}{19.5\scriptsize{$\pm{0.0}$}}
        \\
            ZS-CLIP (PC)~\cite{radford2021clip}\!\!
            & \textbf{--} & \ding{51} &
            & \multicolumn{2}{c}{23.8\scriptsize{$\pm{0.0}$}}
        \\
            \cellcolor{gray!9.0}\textbf{PromptStyler}\!\!
            & \cellcolor{gray!9.0}\textbf{--} 
            & \cellcolor{gray!9.0}\textbf{--} 
            & \cellcolor{gray!9.0} 
            & \multicolumn{2}{c}{\cellcolor{gray!9.0}\textbf{30.5}\scriptsize{$\pm{0.8}$}}
        \\
        %
        \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \vspace{-2.5mm}
    %
    \caption{Unsatisfactory results obtained from CLIP~\cite{radford2021clip} without using source domain data from Terra Incognita~\cite{TerraDataset}.}
    %
    \vspace{-4.5mm}
    \label{table:terra_limitation}
\end{table}

\noindent \textbf{Ablation study on the classification loss.}
%
In Table~\ref{table:ablation_classifier}, we evaluate the effects of the original Softmax loss and the angular Softmax loss (\ie, ArcFace~\cite{ArcFace}).
PromptStyler also achieves the state of the art using the original one, 
which validates that the performance improvement of our method mainly comes from the proposed framework (Fig.~\ref{fig:fig3}).
Note that the angular Softmax loss further improves its accuracy by leveraging the hyperspherical joint vision-language space.

\noindent \textbf{Effect of the number of styles.}
%
We evaluate our method with regard to the number of style word vectors $K$ as shown in Figure~\ref{fig:fig6}.
Interestingly, our PromptStyler outperforms CLIP~\cite{radford2021clip} using just $5$ styles. 
This evaluation shows that $20$ style word vectors are enough to achieve decent results.

\noindent \textbf{Effect of the number of iterations.}
%
We evaluate our method with regard to the number of training iterations $L$ for learning each style word vector as shown in Figure~\ref{fig:fig7}.
This evaluation shows that $20$ iterations are enough to achieve decent results.
%