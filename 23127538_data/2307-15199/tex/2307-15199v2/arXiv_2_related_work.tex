\section{Related Work}

\noindent \textbf{Domain Generalization.}
%
Model's generalization capability to arbitrary unseen domains is the key factor to successful deployment of neural networks in real-world applications,
since substantial distribution shifts between source and target domains could significantly degrade their performance ~\cite{recht2019ood,hendrycks2019ood}.
To this end, Domain Generalization (DG)~\cite{Muandet2013DG,Li2018Learning,Li2018Domain,gulrajani2021domainbed,wang2021dgsurvey,zhou2022dgsurvey,Min2022Grounding,SWAD,kim2022broad,LRDG2022,cha2022miro,frikha2022data} has been studied.
It assumes target domain data are not accessible while using data from source domains.
Generally speaking, existing DG methods could be divided into two categories: multi-source DG~\cite{Zhou2020Learning,Li2019Episodic,Carlucci2019Domain,Dou2019Domain,Matsuura2020Domain,Seo2020Learning,Mahajan2021Domain,zhou2021mixstyle,Xu2021FACT,Rame2022Fishr} and single-source DG~\cite{Wang2021Learning,Li2021Progressive,Qiao2020Learning,Fan2021Adversarially}.
Mostly, multi-source DG methods aim to learn domain-invariant features by exploiting available multiple source domains, and single-source DG methods also aim to learn such features by generating diverse domains based on a single domain and then exploiting the synthesized domains.

\noindent \textbf{Source-free Domain Generalization.}
%
In this setup, we are not able to access any source and target domains as summarized in Table~\ref{table:task_definition}.
Thus, source-free DG is much more challenging than multi-source and single-source DG. 
From the observation that 
synthesizing new domains from the given source domain could effectively improve model's generalization capability~\cite{Zhou2020Learning,zhou2020ddaig,Wang2021Learning,Li2021Progressive,kang2022styleneophile},
we also try to generate diverse domains but without using any source domains to deal with source-free DG.
By leveraging a large-scale pre-trained model which has already seen a great variety of domains,
our method could simulate various distribution shifts in the latent space of the large-scale model.
This approach has several advantages compared with existing DG methods;
source domain images are not required and there is no concern for catastrophic forgetting which might impede model's generalization capability.
Also, it would be immensely practical to exploit such a large-scale model for downstream visual recognition tasks, since we only need the task definition.

\noindent \textbf{Large-scale model in Domain Generalization.}
%
Recently, several DG methods~\cite{cha2022miro,CAD} exploit a large-scale pre-trained model (\eg, CLIP~\cite{radford2021clip}) to leverage its great generalization capability.
While training neural networks on available data, CAD~\cite{CAD} and MIRO~\cite{cha2022miro}
try to learn robust features using such a large-scale model.
Compared with them, the proposed method could learn domain-invariant features using a large-scale pre-trained model without requiring any actual data.

\noindent \textbf{Joint vision-language space.}
%
Large-scale vision-language models~\cite{ALIGN,radford2021clip,TCL} are trained with a great amount of image-text pairs, and achieve state-of-the-art results on downstream visual recognition tasks~\cite{CoOp,CoCoOp,ProDA,CLIPAdapter,TipAdapter}.
By leveraging their joint vision-language spaces, we could also effectively manipulate visual features via prompts~\cite{StyleGAN_NADA,StlyeCLIP,CLIPstyler,LADS}. 
Interestingly, Textual Inversion~\cite{gal2023textualinversion} shows that a learnable style word vector for a pseudo-word $\boldsymbol{S_{*}}$ could capture a shared style of images using CLIP~\cite{radford2021clip} with a prompt (``a painting in the style of $\boldsymbol{S_{*}}$").
From this observation, we argue that learnable style word vectors would be able to seek a variety of styles for simulating various distribution shifts in a joint vision-language space without using any images.
%