\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand\thefigure{A\arabic{figure}}
\renewcommand\thetable{A\arabic{table}}

\section{Method Details}
\label{supp:supp_1}
%
This section provides more details of the chosen vision-language model (Section~\redcolornumber{A.1}) and design choices for learning style word vectors (Section~\redcolornumber{A.2}).

\subsection{Large-scale vision-language model}
\label{supp:subsection_vlm}
%

We choose CLIP~\cite{radford2021clip} as our pre-trained vision-language model which is a large-scale model trained with 400 million image-text pairs.
Note that the proposed method is broadly applicable to the CLIP-like vision-language models~\cite{ALIGN,TCL} which also construct hyperspherical joint vision-language spaces using contrastive learning methods. 
Given a batch of image-text pairs, such models jointly train an image encoder and a text encoder considering similarity scores obtained from image-text pairings.

\noindent \textbf{Joint vision-language training.}
%
Suppose there is a batch of $M$ image-text pairs.
Among all possible $M \times M$ pairings,
the matched $M$ pairs are the positive pairs and the other $M^{2}-M$ pairs are the negative pairs.
CLIP~\cite{radford2021clip} is trained to maximize cosine similarities of image and text features from the positive $M$ pairs while minimizing the similarities of such features from the negative $M^{2}-M$ pairs.

\noindent \textbf{Image encoder.}
%
CLIP~\cite{radford2021clip} utilizes ResNet~\cite{resnet} or ViT~\cite{dosovitskiy2021an} as its image encoder.
Given an input image, the image encoder extracts its image feature.
After that, the image feature is mapped to a hyperspherical joint vision-language space by $\ell_{2}$ normalization.

\noindent \textbf{Text encoder.}
%
CLIP~\cite{radford2021clip} utilizes Transformer~\cite{vaswani2017attention} as its text encoder.
Given an input text prompt, it is converted to word vectors via a tokenization process and a word lookup procedure.
Using these word vectors, the text encoder generates a text feature which is then mapped to a hyperspherical joint vision-language space by $\ell_{2}$ normalization.

\noindent \textbf{Zero-shot inference.}
%
At inference time, zero-shot CLIP~\cite{radford2021clip} synthesizes classifier weights via the text encoder using $N$ class names pre-defined in the target task.
Given an input image, the image encoder extracts its image feature and the text encoder produces $N$ text features using the $N$ class names.
Then, it computes cosine similarity scores between the image feature and text features, and selects the class name which results in the highest similarity score as its classification output.

% Figure environment removed

\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand\thefigure{B\arabic{figure}}
\renewcommand\thetable{B\arabic{table}}

% Figure environment removed

\begin{table*}[!t]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lccccccc|c}
        %
        \Xhline{2\arrayrulewidth}
        %
        \multicolumn{1}{c}{}
        & \multicolumn{2}{c}{Configuration}
        & \multicolumn{1}{c}{}
        & \multicolumn{5}{c}{Accuracy (\%)}
        \\
        \cline{2-3}
        \cline{5-9}
        %
        \vspace{-0.8mm}
        & Source & Domain & \;
        & & & &
        &
        \\
        Method
        & Domain & Description &
        & \normalsize{L}\small{ocation100} & \normalsize{L}\small{ocation38} & \normalsize{L}\small{ocation43} & \normalsize{L}\small{ocation46} & \normalsize{A}\small{vg.}
        \\
        %
        \hline
        \multicolumn{9}{c}{\textit{ResNet-50~\cite{resnet} with pre-trained weights on ImageNet~\cite{deng2009imagenet}}}
        \\
        \hhline{-|-|-|-|-|-|-|-|-|}
            SelfReg~\cite{SelfReg}
            & \ding{51} & \textbf{--} &
            & 48.8\scriptsize{$\pm{0.9}$} & 41.3\scriptsize{$\pm{1.8}$} & 57.3\scriptsize{$\pm{0.7}$} & \textbf{40.6}\scriptsize{$\pm{0.9}$} & 47.0
        \\
            GVRT~\cite{Min2022Grounding}
            & \ding{51} & \textbf{--} &
            &  \textbf{53.9}\scriptsize{$\pm{1.3}$} & \textbf{41.8}\scriptsize{$\pm{1.2}$} & \textbf{58.2}\scriptsize{$\pm{0.9}$} & 38.0\scriptsize{$\pm{0.6}$} & \textbf{48.0}
        \\
        \hhline{-|-|-|-|-|-|-|-|-|}
        \multicolumn{9}{c}{\textit{ResNet-50~\cite{resnet} with pre-trained weights from CLIP~\cite{radford2021clip}}}
        \\
        \hhline{-|-|-|-|-|-|-|-|-|}
            ZS-CLIP (C)~\cite{radford2021clip}
            & \textbf{--} & \textbf{--} &
            & 8.4\scriptsize{$\pm{0.0}$} & 13.7\scriptsize{$\pm{0.0}$} & 32.5\scriptsize{$\pm{0.0}$} & 23.3\scriptsize{$\pm{0.0}$} & 19.5
        \\
            ZS-CLIP (PC)~\cite{radford2021clip}
            & \textbf{--} & \ding{51} &
            & 9.9\scriptsize{$\pm{0.0}$} & 28.3\scriptsize{$\pm{0.0}$} & 32.9\scriptsize{$\pm{0.0}$} & 24.0\scriptsize{$\pm{0.0}$} & 23.8
        \\
            \cellcolor{gray!9.0}\textbf{PromptStyler}\!\!
            & \cellcolor{gray!9.0}\textbf{--} 
            & \cellcolor{gray!9.0}\textbf{--} 
            & \cellcolor{gray!9.0} 
            & \cellcolor{gray!9.0}\textbf{13.8}\scriptsize{$\pm{1.7}$} & \cellcolor{gray!9.0}\textbf{39.8}\scriptsize{$\pm{1.3}$} & \cellcolor{gray!9.0}\textbf{38.0}\scriptsize{$\pm{0.4}$} & \cellcolor{gray!9.0}\textbf{30.3}\scriptsize{$\pm{0.3}$} & \cellcolor{gray!9.0}\textbf{30.5}
        \\
        %
        \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \vspace{-2mm}
    %
    \caption{Top-1 classification accuracy on the Terra Incognita~\cite{TerraDataset} dataset.
    Compared with existing domain generalization methods which utilize source domain data, zero-shot methods using CLIP~\cite{radford2021clip} show unsatisfactory results on this dataset.}
    %
    \label{table:supp_terra_limitation}
    \vspace{-1mm}
\end{table*}

\subsection{Empirical justification of our design choice}
\label{supp:subsection_design}
%
As described in Section~\redcolornumber{3.1} of the main paper,
there are two possible design choices for learning $K$ style word vectors: 
(1) learning each style word vector $\mathbf{s}_{i}$ in a sequential manner, or (2) learning all style word vectors $\{\mathbf{s}_{i}\}^{K}_{i=1}$ in a parallel manner.
We choose the former mainly due to its much less memory overhead.
As shown in Figure~\ref{fig:supp_design}, we could sequentially learn $\sim$$100$ style word vectors with $\sim$$4.2$ GB memory usage.
However, it is not possible to learn more than $21$ style word vectors in a parallel manner using a single RTX 3090 GPU (24 GB Memory) due to its large 
memory overhead.
In detail, learning $20$ and $21$ style word vectors takes $22.4$ GB and $23.5$ GB, respectively.
The large memory overhead caused by the parallel learning design substantially limits the number of learnable style word vectors. 

To be specific, PromptStyler with the parallel learning design needs to generate $K$ style features, $KN$ style-content features, and $N$ content features for learning $K$ style word vectors at the same time; 
these features are used to compute the style diversity loss $\mathcal{L}_{\mathrm{style}}$ and the content consistency loss $\mathcal{L}_{\mathrm{content}}$ for learning all the style word vectors in a parallel manner.
Note that the large memory overhead is mainly caused by the $KN$ style-content features.
Suppose we want to learn $80$ style word vectors for the target task OfficeHome~\cite{OfficeHomedataset} (65 classes).
Then, we need to synthesize $5200(=80 \times 65)$ style-content features.
Even worse, we need to generate $27600(=80 \times 345)$ style-content features for the target task DomainNet~\cite{DomainNetdataset} ($345$ classes).
On the other hand, PromptStyler with the sequential learning design only requires $i$ style features, $N$ style-content features, and $N$ content features for learning $i$-th style word vector, where $1 \leq i \leq K$.
For scalability, we chose the sequential learning design since it could handle a lot of learnable style word vectors and numerous classes in the target task.
%