\section{Analyses on Terra Incognita} 
\label{supp:supp_2}
%
\vspace{-2mm}
%
As described in Section~\redcolornumber{5} of the main paper, the quality of the latent space constructed by a large-scale pre-trained model significantly affects the effectiveness of PromptStyler.
To be specific, the proposed method depends on the quality of the joint vision-language space constructed by CLIP~\cite{radford2021clip}.
Although our method achieves state-of-the-art results on PACS~\cite{PACSdataset}, VLCS~\cite{VLCSdataset}, OfficeHome~\cite{OfficeHomedataset}, and DomainNet~\cite{DomainNetdataset}, 
its performance on Terra Incognita~\cite{TerraDataset} is not satisfactory.
This section provides more analyses on the dataset.

Table~\ref{table:supp_terra_limitation} shows that PromptStyler outperforms zero-shot CLIP~\cite{radford2021clip} for all domains in the Terra Incognita dataset~\cite{TerraDataset}. 
However, its accuracy on this dataset is lower compared with existing domain generalization methods~\cite{Min2022Grounding,SelfReg} which utilize several images from the dataset as their source domain data.
This unsatisfactory result might be due to the low accuracy of CLIP on the dataset.
We suspect that images in the Terra Incognita dataset (Fig.~\ref{fig:supp_terra_dataset}) might be significantly different from the domains that CLIP has observed.
The distribution shifts between CLIP training dataset and the Terra Incognita dataset might be extreme, and thus such distribution shifts could not be entirely covered by our method
which exploits CLIP latent space.
We hope this issue could be alleviated with the development of large-scale models.
%