\begin{abstract}
%
\vspace{-3mm}
In a joint vision-language space, a text feature (\eg, from ``a photo of a dog") could effectively represent its relevant image features (\eg, from dog photos). 
Inspired by this, we propose \textbf{PromptStyler} which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization.
Our method learns to generate a variety of style features (from ``a $\boldsymbol{S_{*}}$ style of a") via learnable style word vectors for pseudo-words $\boldsymbol{S_{*}}$. 
To ensure that learned styles do not distort content information, 
we force \mbox{style-content} features (from ``a $\boldsymbol{S_{*}}$ style of a [class]") to be located nearby their corresponding content features (from ``[class]") in the joint vision-language space. 
After learning style word vectors, we train a linear classifier using synthesized \mbox{style-content} features.
PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome and DomainNet, although it does not require any images and takes just $\sim$30 minutes for training using a single GPU. 
%
\end{abstract}
%