\vspace{-2mm}
%
\section{Introduction}
\vspace{-0.5mm}

% Figure environment removed

Deep neural networks are usually trained with the assumption that training and test data are independent and identically distributed,
which makes them vulnerable to substantial distribution shifts between training and test data~\cite{recht2019ood,hendrycks2019ood}.
This susceptibility is considered as one of the major obstacles to their deployment in real-world applications.
To enhance their robustness to such distribution shifts,
Domain Adaptation (DA)~\cite{ben2006da,sun2016coral,hoffman2018cycada,tzeng2017adversarial,peng2019moment,zhao2019da,Saito2019semi,lee2023surgical} has been studied; 
it aims at adapting neural networks to a target domain using target domain data available in training.
However, such a target domain is often latent in common training scenarios, which considerably limits the application of DA.
Recently, a body of research has addressed this limitation by Domain Generalization (DG)~\cite{Li2018Domain,zhou2021mixstyle,gulrajani2021domainbed,Li2018Learning,Carlucci2019Domain,cha2022miro,kim2022broad} that aims to improve model's generalization capability to any unseen domains.
It has been a common practice in DG to utilize multiple source domains for learning domain-invariant features~\cite{zhou2022dgsurvey,wang2021dgsurvey}, 
but it is unclear which source domains are ideal for DG, 
since arbitrary unseen domains should be addressed.
Furthermore, it is costly and sometimes even infeasible to collect and annotate large-scale multi-source domain data for training.

% Figure environment removed

We notice that a large-scale pre-trained model might have already observed a great variety of domains and thus can be used as an efficient proxy of actual multiple source domains.
From this perspective, we raised a question
\textit{``Could we further improve model's generalization capability by simulating various distribution shifts in the latent space of such a large-scale model without using any source domain data?"}
If this is possible, DG will become immensely practical by effectively and efficiently exploiting such a large-scale model. 
However, this approach is much more challenging since any actual data of source and target domains are not accessible but only the target task definition (\eg, class names) is given.

In this paper, we argue that large-scale vision-language models~\cite{ALIGN,radford2021clip,TCL} could shed light on this challenging \textit{source-free domain generalization}.
For example, CLIP~\cite{radford2021clip} constructs a hyperspherical joint vision-language space where a text feature (\eg, from ``a photo of a dog") could effectively represent its relevant image features (\eg, from dog photos) as conceptually illustrated in Figure~\ref{fig:fig1}(a).
This joint vision-language space can open up a totally new training paradigm, where we train a classifier using text features while running an inference with the classifier using image features.
This training procedure meets the necessary condition for the source-free domain generalization, 
\ie, source domain images are not required.
By leveraging such a joint vision-language space,
we would be able to simulate various distribution shifts via prompts without using any images.

We propose a prompt-driven style generation method, dubbed \textbf{PromptStyler}, which synthesizes diverse styles via learnable word vectors to simulate distribution shifts in a hyperspherical joint vision-language space.
PromptStyler is motivated by the observation that a shared style of images could characterize a domain~\cite{zhou2021mixstyle,kang2022styleneophile} and such a shared style could be captured by a learnable word vector for a pseudo-word $\boldsymbol{S_{*}}$ using CLIP~\cite{radford2021clip} with a prompt (``a painting in the style of $\boldsymbol{S_{*}}$")~\cite{gal2023textualinversion}.
As shown in Figure~\ref{fig:fig1}(b), our method learns a style word vector for $\boldsymbol{S_{*}}$ to represent each style.

To effectively simulate various distribution shifts, we try to maximize \textit{style diversity} as illustrated in Figure~\ref{fig:fig2}.
Specifically, our method encourages learnable style word vectors to result in orthogonal style features in the hyperspherical space,
where each style feature is obtained from a \textbf{style prompt} (``a $\boldsymbol{S_{*}}$ style of a") via a pre-trained text encoder.
To prevent learned styles from distorting content information, we also consider \textit{content consistency} as illustrated in Figure~\ref{fig:fig2}.
Each style-content feature obtained from a \textbf{\mbox{style-content} prompt} (``a $\boldsymbol{S_{*}}$ style of a [class]") is forced to be located closer to its corresponding content feature obtained from a \textbf{content prompt} (``[class]") than the other content features.

Learned style word vectors are used to synthesize style-content features for training a classifier; 
these synthesized features could simulate images of known contents with diverse unknown styles in the joint space.
These style-content features are fed as input to a linear classifier which is trained by a classification loss using contents (``[class]") as their class labels. 
At inference time, an image encoder extracts image features from input images, which are fed as input to the trained classifier.
Note that the text and image encoders are derived from the same pre-trained vision-language model (\eg, CLIP~\cite{radford2021clip}); the text encoder is only involved in training and the image encoder is only involved at inference time.

The proposed method achieves state-of-the-art results on PACS~\cite{PACSdataset}, VLCS~\cite{VLCSdataset}, OfficeHome~\cite{OfficeHomedataset} and DomainNet~\cite{DomainNetdataset} without using any actual data of source and target domains.
It takes just $\sim$30 minutes for the entire training using a single RTX 3090 GPU, and our model is $\sim$2.6$\times$ smaller and $\sim$243$\times$ faster at inference compared with CLIP~\cite{radford2021clip}.

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{cccc}
        %
        \Xhline{2\arrayrulewidth}
        Setup & Source
        & Target & Task Definition
        \\
        \hline
            DA & \ding{51}
            & \ding{51} & \ding{51} 
        \\
            DG & \ding{51}
            & \textbf{--}  & \ding{51} 
        \\
            Source-free DA & \textbf{--} 
            & \ding{51} & \ding{51} 
        \\
            \cellcolor{gray!9.0}\textbf{Source-free DG} & \cellcolor{gray!9.0}\textbf{--} 
            & \cellcolor{gray!9.0}\textbf{--} & \cellcolor{gray!9.0}\ding{51} 
        \\
        %
        \Xhline{2\arrayrulewidth}
    \end{tabular}}
    \vspace{-2mm}
    %
    \caption{
        Different requirements in each setup.
        Source-free DG only assumes the task definition (\ie, what should be predicted) without requiring source and target domain data.
    }
    %
    \vspace{-2.5mm}
    \label{table:task_definition}
\end{table}

Our contributions are summarized as follows:
%
\begin{itemize}
    \vspace{-2.5mm}
    \item This work is the first attempt to synthesize a variety of styles in a joint vision-language space via prompts to effectively tackle source-free domain generalization.
    %
    \vspace{-2.5mm}
    \item This paper proposes a novel method that effectively simulates images of known contents with diverse unknown styles in a joint vision-language space. 
    %
    \vspace{-2.5mm}
    \item PromptStyler achieves the state of the art on domain generalization benchmarks without using any images.
    %
\end{itemize}
%