\section{Background}\label{sec:prior-art}

\noindent \textbf{Learning deep representations in the context of gene sequence modeling.}
Recent studies~\cite{dnavec,DNABert,yang:2021:bioarxiv:logo} represent gene sequences as k-mers, using a learned dense representation from an adapted BERT model. Mock et al.~\cite{mock2021bertax} broadly adopt the DNABert architecture for the task of taxonomy classification and model gene sequences as 3-mers and also include next sentence prediction along with MLM training loss. 
 Instead of using k-mer representations, the \textit{BigBird} model~\cite{bigbird2020} trained a SentencePiece tokenizer on the Human Reference Genome and applied to the tasks of chromatin-profile prediction and promoter region prediction. 
However, the pretrained model weights of the \textit{BigBird} model for the genomics setting are not publicly available, and the alternative of training from scratch is prohibitively costly for such a large model.
Mo et al.~\cite{genebert2021} infuse domain knowledge into the model by proposing a multimodal pretraining setup comprising gene sequences and information on transcription factors and regions. The recent Enformer~\cite{enformer} utilizes a combination of convolutional and transformer layers to feed long sequences of one-hot encoded base pairs into the model. However, unlike gene sequence classification, which is the focus of this work, the Enformer model is used to predict gene expression tracks. 

\noindent \textbf{Random and PMI-masking for MLM training in NLP.} 
The initial work applied \emph{random token masking}, as performed by BERT \cite{bert}, where 15\% of the input tokens are chosen to be masked uniformly. Previous work has also investigated simultaneous masking of sequences of adjacent tokens which form either a whole word (\emph{whole word masking} \cite{senrich:2016:acl:wholewordmasking}) or an entity (\emph{entity masking} \cite{sun2019ernie}); the technique proves to be beneficial over randomly masking single, non-contiguous tokens. 
Joshi et al.~\cite{spanbert2020} propose \emph{random span masking} where random spans with lengths chosen from a geometric distribution are masked at random positions. This simple method outperforms the more involved entity masking approach~\cite{sun2019ernie}. When applied to gene sequences, especially entity and whole word masking, have the problem that there is no well-defined concept of entities or words in gene sequences. 
The \emph{PMI-masking} approach \cite{pmi-masking} builds on the ideas of the entity and span masking where they treat collocated n-grams with high Pointwise Mutual Information scores analogous to an entity and thereby mask these spans of tokens together. The authors show that masking PMI tokens (i) accelerates training while matching end-of-pretraining performance in roughly half the training steps and (ii) improves upon previous masking approaches at the end of pretraining. Sadeq et al.~\cite{sadeq-etal-2022-informask} develop the \textit{InforMask} masking strategy, where they measure the \textit{informative relevance} of a word as the sum of PMI values between a masked word and all unmasked words in the given sentence (higher values are prioritized for masking). However, the semantic equivalent of a sentence (in NLP) is not known in the case of gene sequence modeling.