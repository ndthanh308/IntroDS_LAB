\section{Introduction}

Computational analysis of genomics has revolutionized the field of medical science \cite{McGuire2020}, particularly with the advent of the \textit{Human Reference Genome} \cite{Schneider072116}. As seen in recent studies~\cite{enformer,genomics-survey}, deep learning has been applied to various genomic applications, such as protein structure analysis, gene expression data, and transcriptome analysis.
The state-of-the-art pretrained models (DNABert~\cite{DNABert} and LOGO~\cite{yang:2021:bioarxiv:logo}) in gene sequence classification tasks are widely used in literature~\cite{Badirli2021,genebert2021}. The input data for these tasks are often presented as a sequence of nucleotides. Each side of the double-helix DNA strand comprises the bases adenine (A), cytosine (C), guanine (G), and thymine (T). Similar to the NLP domain, the standard approach in gene sequence classification is to pretrain the transformer models by randomly masking (and predicting) tokens, the so-called masked language modeling objective (MLM). However, unlike words or sentences in languages, no clear semantically demarcated tokens are present within the gene sequence. Therefore, to come to a workable solution, researchers randomly select a sequence of $k$ nucleotides~\cite{DNABert,genebert2021}; for example, if $k$ = 3, the sequence $TCG$ can be selected from a  gene sequence  $\cdots GAT\textbf{TCG}ATGC\cdots$. 
However, the workable solution may be easy to guess if (say) GAT\textbf{TCG} is a very common sequence because \textbf{GAT} is still unmasked and frequently co-occurs with the masked \textbf{TCG} sequence. This, in turn, may decelerate the training process and increase the pretraining time; 
for example, DNABert trains for $25$ days on $8$ NVIDIA 2080Ti GPUs. A possible solution to reduce the instances of `easy' learning is to identify highly-correlated commonly occurring spans and mask them in their entirety so that the pretraining model does not consume precious computing cycles in predicting `easy' cases.

A systematic approach to identify correlated spans can be based on the principle of \textit{Pointwise Mutual Information} (PMI), which calculates the chance of a set of tokens spanning together vis-a-vis occurring independently; a high PMI score indicates a high correlation. In this work, we propose a novel masking algorithm, \genemask, for MLM training of gene sequences. We randomly identify positions in a gene sequence as mask centers and locally select the span around the mask center with the highest (modified) PMI to mask.  
However, PMI also favors rarely occurring correlated spans  (where individual tokens may have a low frequency of occurrence), which are not easy cases as the model has not frequently seen such patterns.
Hence we modify the PMI metric and impose a discounting factor to down-score rare correlated spans. 
Instead of randomly selecting tokens to mask, as done in DNABert (or LOGO), we use the PMI score to prioritize the \textit{relevant} tokens within a given gene sequence. In the case of \genemask, we randomly chose positions in a gene sequence and locally chose the span with the highest PMI to mask. This is in contrast to the strategy adopted in NLP literature by Levine et al.~\cite{pmi-masking}, where they use the PMI score to formulate an absolute importance notion and consequently create a fixed masking vocabulary. 

As current medical datasets often face data scarcity issues, the move towards personalized medicine requires models that perform well with limited training data at hand \cite{shaikhina:2017:aimed:SmallDataMLinMedicine,Hekler:2019:bmcmedicine:smallDataParadigm}. We, therefore, evaluate our proposed \genemask-based MLM training strategy in the low-resource (few-shot) setting. 

Our extensive experimentation shows that \genemask-based DNABert and LOGO improve over the standard random masking-guided DNABert and LOGO, respectively, in \textbf{few-shot settings} ($10, 50, 100, 500$ and $1000$ training data points per class) over four benchmark datasets of gene sequence classification (two on promoters - \textit{Prom-core, Prom-300}, one on enhancer - \textit{Cohn-enh} and one on splice sites - \textit{Splice-40}). We posit that \genemask{} helps incorporate non-trivial genetic knowledge because we observe that \genemask-based DNABert pretrained for \textbf{10K steps even outperforms original DNABert pretrained for 120K steps} for all few-shot settings in case of Prom-300 and Cohn-enh dataset. 
In addition, we perform \textbf{motif}\footnote{Sequence motifs are short, recurring patterns in DNA that are presumed to have a biological function~\cite{haeseleer2006}.}
\textbf{analysis} and observe a strong correlation between top-ranked PMI tokens and conserved DNA sequence motifs, providing a biological reason behind the performance improvement in \genemask-based DNABert. 
Finally, to alleviate the issue of the tremendous engineering effort needed to develop the experimental setup of gene sequence classification tasks, we make all the codes (including trained models), data, and appendix publicly available at \url{https://github.com/roysoumya/GeneMask}.