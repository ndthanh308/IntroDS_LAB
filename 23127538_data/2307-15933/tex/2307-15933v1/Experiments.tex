\section{Experimental Setup}\label{sec:experiments}
Here, we provide the dataset specifics, evaluation setup,  model training, and baseline model details.

\subsection{Datasets}\label{sec:datasets}
We use four benchmark datasets of gene sequence classification {(\small{\textsc{Prom-core, Prom-300, Splice-40, Cohn-enh}})} for evaluation purposes. The two datasets of promoter region prediction and splice site prediction are not directly  available and involve significant effort (including a paper implementation) for their construction. 

\noindent {\small{\textbf{\textsc{Promoter Region Prediction (Prom-core and Prom-300).}}}}
A \textit{promoter} is a DNA region typically located upstream of the gene, which is the site of transcription initiation (as defined by Zaheer et al.~\cite{bigbird2020}). 
\ul{The task is to classify a given DNA fragment as a promoter or non-promoter sequence}. 
 However, we follow the instructions of the DeePromoter~\cite{deepromoter2019} paper, including the negative data creation, since the authors do not provide the datasets. Thus, we obtained human TATA and non-TATA promoter data, i.e., including promoter sequences with and without a TATA box (a common promoter-related motif found between $-30$ to $-25$ bp (upstream) of a gene's transcription start site), from the Eukaryotic Promoter Database~\cite{epdnew2012}, using the website API of the \textit{EPD
selection tool}~\cite{EPD}. 
We extracted $-249$ to $+50$ bp sequences around TSS for the {\bf Prom-300 dataset} and $-34$ to $+35$ bp for the {\bf Prom-core dataset.} We perform the standard train-test split of $70\%$ and $30\%$, which leads to $53276$ and $5920$ data points, respectively. 

\noindent {\small{\textbf{\textsc{Splice Donor and Acceptor Site Prediction (Splice-40).}}}}
We followed the same strategy as prior works~\cite{DNABert,Wang2019} for dataset construction. We extract $40$~bp long sequences around the donor and acceptor sites of exons (randomly selected) as positive sequences. 
The exon's position information (chromosome number, start index, and end index) is obtained from the corresponding gene annotation file. \ul{The task is to classify a given DNA fragment as a donor, acceptor, or non-splice site (non-overlapping intermediate sequences between exons) sequence.} We perform the standard train-test split of $70-30\%$, which leads to $24300$ and $3000$ training and test data points, respectively. 

\noindent {\small{\textbf{\textsc{Enhancer Cohn Prediction (Cohn-enh).}}}}
An \textit{enhancer} is a DNA sequence that can bind specific proteins and increase the chance of transcription of a particular gene. \ul{Here, the input is a DNA sequence of $500$ bp in length and a binary classification task where the task is to classify a DNA fragment as an enhancer or non-enhancer sequence.}
This dataset has been adapted from Cohn et al.~\cite{cohn2018} and is made available as a benchmark dataset by Martinek et al.~\cite{martinek2022} in Github~\cite{gresova2022genomic}; we use the same train-test split that leads to $20843$ and $6948$ data points as train and test datasets, respectively. 

\subsection{Evaluation Setup}\label{sec:evaluation-setup}
We report the standard metrics of accuracy (used for performance comparison) and AUC (stands for Area Under the Receiver Operating Characteristic Curve) used for classification tasks where the class labels are balanced. 
We follow the standard evaluation setup used in the few-shot text classification setting~\cite{schick-schutze-2021-exploiting,schick-schutze-2021-just}. Thus, we assume not to have access to a validation dataset to optimize the hyperparameters and investigate the performance for different training set sizes (few-shot settings) $n= 10, 50, 100, 500,$ and $1000$, where $n$ denotes the number of training data points per class. 
We report the mean and standard deviation of accuracy and AUC by running the experiments \textbf{ten} times by randomly choosing seed and $n$ training (fine-tuning) data points per class in each run.
We report the statistical significance results based on paired t-test for the performance improvement by \genemaskbest{} over ORI 10K model. 

\subsection{Training Details}\label{sec:impl-details}

In this paper, we train all the pretrained models and their variants for only $10000$ steps, which takes about \textbf{2.5 days to complete for DNABert} and around \textbf{20 hours for LOGO} using four GTX 1080Ti 11GB GPUs.  We select such a setup for two reasons --- (i) to explore different pretrained model variants in a reasonable time because we observe that the perplexity of DNABert (SOTA model) has converged to a low score and is stable over the last $3000$ pretraining steps (see Figure~\ref{fig:perplexity-plot}).
(ii) As observed by Levine et al.~\cite{pmi-masking}, PMI masking learns fast and is thus quite efficient to reap the benefit even with fewer pretraining steps. The default number of warmup steps for DNABert is 10K steps out of 200K ($5\%$ of the maximum number of steps). Since we reduce the maximum pretraining steps limit from 200K to 10K, we set the number of warmup steps as $500$ ($5\%$ of $10000$ steps) to maintain the same ratio. 
We use the same values as the original SOTA models for the remaining hyperparameters 
(see section~\ref{sec:appendex-model-parameter} of Appendix to know more about training (including fine-tuning) details and hyperparameters). We observe in Figure~\ref{fig:perplexity-plot} that the perplexity scores of the baseline models are lower.  The same trend is also reported by Levine et al.~\cite{pmi-masking}. However, the single-token perplexity values are not comparable between models with different masking strategies and, thus, do not indicate downstream performance.

% Figure environment removed

\subsection{Baseline Models}\label{sec:baseline-models}
We evaluate our work on two gene transformer-based models - DNABert~\cite{DNABert} and LOGO~\cite{yang:2021:bioarxiv:logo}. To perform a fair comparison, we train the baseline models with the same hyperparameter settings as \genemask. These models follow \textbf{random masking} during the MLM training step instead of the proposed \genemask{} strategy.  We use the same model hyperparameters for pretraining \genemask{} and the baseline models to perform a fair comparison. 
 We will refer to the SOTA models that use random masking as the original SOTA model \textbf{(ORI)} without any PMI-guided masking.  We use the original DNABert model pretrained on 120K steps (\textbf{ORI 120K}) based on the pretrained model weights provided by ~\cite{DNABert} as a baseline model. 
 
\noindent\textbf{Fixed PMI-guided Masking Vocabulary (Gene-PMI-VOC)}: We undertook a significant effort to adapt the PMI-masking strategy in NLP proposed by Levine et al.~\cite{pmi-masking} to the genomic setting which is a contribution in itself. We create a PMI-masking vocabulary $\approx 10$ times the DNABert vocabulary size of $4101$ tokens. 
 We first select all possible k-mer sequences ($2 \leq k \leq 10$) whose frequency of occurrence is  $\geq 10000$. We then rank them using our proposed PMI metric and select the top $40000$ as the masking vocabulary. During masking, we randomly select the mask centers (nucleotides) and then use the masking vocabulary to tokenize the input gene sequence into PMI tokens. We mask an entire PMI token (2 to 10-mer) within which the selected mask center lies (based on tokenization); Algorithm~\ref{algo:genepmivoc} (appendix) describes our implementation in detail. 

 \noindent \textbf{Pretraining Model Variants of \genemask.} We explore two different pretraining setups --- (i) Half Gradient Accumulation (task-independent) and (ii) Gene Boundary-aware Pretraining (task-specific, boosts performance over Splice-40 dataset).

    \noindent \textbf{(i). Half Gradient Accumulation (HGA)}: The \textit{gradient accumulation steps}  parameter is halved, reducing it from $25$ (default DNABert configuration) to $12$ steps, and consequently, it reduces the effective batch size (it is the product of per GPU train batch size, GPU count and gradient accumulation steps) by $50\%$. This aims to reduce the \textit{generalization gap} issue that arises when the training batch size is too large~\cite{hoffer2017,keskar2017}. 
    
   \noindent \textbf{(ii). Gene Boundary-aware Pretraining (GB)}: In the pretraining data construction stage, as previously described in Section~\ref{sec:dnabert-config}, we do not consider gene boundaries. Thus, it may happen that the DNA segment spans across different genomic entities and, since such spans have no semantic underpinning, may introduce noise during the masked token prediction step of MLM training. We obtain the gene boundary information from the gene annotation file, corresponding to the same Human Reference Genome, as described in the pretraining data construction part of Section~\ref{sec:dnabert-config}. To limit a DNA segment within a single gene boundary, we use the minimum distance to the next gene boundary as an upper limit for determining the length of the DNA segment $L$; each DNA segment corresponds to a single data point in the pretraining dataset. 

\section{Experimental Results}
We compare the performance of our proposed \genemask{} strategy and random masking strategy-based SOTA models of DNABert and LOGO as shown in Table~\ref{tab:perf-compare-10K}. We observe that the performance improvement due to \genemask{} strategy over random masking strategy (ORI 10K model) is more prevalent in lower data settings (10, 50-shot), reduces at higher data settings of 500 and 1000-shot (except for Splice-40), and ceases to exist when trained on full datasets, as evident from Figure~\ref{fig:perf-compare-graph} of Appendix. We consider the two additional pretraining model variations for DNABert - 
\textit{Half Gradient Accumulation} and \textit{Gene Boundary-aware Pretraining} and along with the base model referring to the \genemask{} model having the same hyperparameters as the baseline models such as ORI 10K. We present the best performance among the three models, such as \genemask{}, \genemask{w/ HGA} and \genemask{w/ GB}] as \genemaskbest{} model. The \genemask{} model is also referred to as \genemask{} 10K to indicate the number of training steps. 

\begin{table*}[t]
    \footnotesize
    \centering
    \scalebox{0.8}{
    \addtolength{\tabcolsep}{-0.1em}
    \begin{tabular}{cccccccccc}
     \hline
        \textbf{Data} &  \textbf{Model}  & \multicolumn{2}{c}{\textbf{Prom-core}}& \multicolumn{2}{c}{\textbf{Prom-300}} & \multicolumn{2}{c}{\textbf{Cohn-enh}}& \multicolumn{2}{c}{\textbf{Splice-40}}   \\
        \textbf{per class}&\textbf{type}&\textbf{Accuracy}&\textbf{AUC}&\textbf{Accuracy}&\textbf{AUC}&\textbf{Accuracy}&\textbf{AUC}&\textbf{Accuracy}&\textbf{AUC} \\ \hline %\cline{2-8}
        &\multicolumn{9}{c}{\textbf{DNABert as Base model}} \\ 
        10& ORI 120K &\textbf{0.606 $\pm$ 0.045}&\textbf{0.661 $\pm$ 0.064}&$0.638 \pm 0.070$&$0.708 \pm 0.088$&$0.582 \pm 0.030$&$0.631 \pm 0.044$&$0.404 \pm 0.019$&$0.577 \pm 0.021$ \\
        
& Gene-PMI-VOC &$0.6 \pm 0.047$&$0.652 \pm 0.067$&\cellcolor{yellow} $0.653 \pm 0.065$&\cellcolor{yellow} $0.731 \pm 0.066$&\cellcolor{yellow} $0.596 \pm 0.048$& \cellcolor{yellow} $0.66 \pm 0.062$ &\textbf{0.412 $\pm$ 0.02} &\textbf{0.599 $\pm$ 0.02} \\
        &  ORI 10K &$0.586 \pm 0.051$&$0.641 \pm 0.082$& $0.601 \pm 0.065$ &$0.657 \pm 0.095$&$0.579 \pm 0.047$& $0.655 \pm 0.058$ &$0.409 \pm 0.017$&$0.588 \pm 0.018$\\
 
&  \cellcolor{lightgreen} \genemaskbest{} &\cellcolor{yellow} 0.602 $\pm$ 0.058&\cellcolor{yellow} 0.655 $\pm$ 0.078&\cellcolor{lightgreen} \textbf{0.676 $\pm$ 0.054}**&\cellcolor{lightgreen} \textbf{0.779 $\pm$ 0.074}**&\cellcolor{lightgreen} \textbf{0.622 $\pm$ 0.050}*&\cellcolor{lightgreen} \textbf{0.701 $\pm$ 0.038}* &\cellcolor{lightgreen} \textbf{0.412 $\pm$ 0.026}&\cellcolor{yellow} $0.591 \pm 0.036$\\ \cline{2-10}

50& ORI 120K &\textbf{0.687 $\pm$ 0.024}&\textbf{0.756 $\pm$ 0.03}&\cellcolor{yellow} $0.808 \pm 0.019$&$0.89 \pm 0.013$&$0.638 \pm 0.020$&$0.679 \pm 0.028$ &$0.472 \pm 0.048$&$0.651 \pm 0.049$ \\ 
& Gene-PMI-VOC  &$0.649 \pm 0.058$&$0.738 \pm 0.033$&$0.800 \pm 0.027$&\cellcolor{yellow} $0.893 \pm 0.02$&\cellcolor{yellow} $0.645 \pm 0.014$&\cellcolor{yellow} $0.706 \pm 0.016$ &\textbf{0.522 $\pm$ 0.071}&\textbf{0.703 $\pm$ 0.062}\\
&  ORI 10K &$0.653 \pm 0.058$&$0.718 \pm 0.064$&$0.789 \pm 0.059$&$0.882 \pm 0.04$&$0.634 \pm 0.031$&$0.689 \pm 0.044$ &$0.512 \pm 0.014$&$0.687 \pm 0.017$\\

 
&\cellcolor{lightgreen}  \genemaskbest{} &\cellcolor{yellow} 0.678 $\pm$ 0.026&\cellcolor{yellow} 0.744 $\pm$ 0.026&\cellcolor{lightgreen} \textbf{0.815 $\pm$ 0.02}&\cellcolor{lightgreen} \textbf{0.905 $\pm$ 0.013}&\cellcolor{lightgreen} \textbf{0.654 $\pm$ 0.017}*&\cellcolor{lightgreen} \textbf{0.713 $\pm$ 0.011}$\dagger$ &\cellcolor{yellow} $0.519 \pm 0.027$&\cellcolor{yellow} $0.696 \pm 0.023$\\ \cline{2-10}

100& ORI 120K &\textbf{0.712 $\pm$ 0.009}&\textbf{0.781 $\pm$ 0.012}&\cellcolor{yellow} $0.842 \pm 0.014$&$0.915 \pm 0.012$&\cellcolor{yellow} $0.669 \pm 0.017$&$0.736 \pm 0.022$ &$0.507 \pm 0.059$&$0.683 \pm 0.063$\\ 
& Gene-PMI-VOC  &$0.697 \pm 0.011$&$0.767 \pm 0.013$&$0.835 \pm 0.017$&$0.912 \pm 0.014$&$0.65 \pm 0.051$&\textbf{0.737 $\pm$ 0.011} &\textbf{0.605 $\pm$ 0.017}&\textbf{0.779 $\pm$ 0.015}\\ 

&  ORI 10K &$0.695 \pm 0.014$&$0.765 \pm 0.017$&\cellcolor{yellow} $0.842 \pm 0.018$&\textbf{0.923 $\pm$ 0.009}&$0.668 \pm 0.015$&$0.736 \pm 0.011$ &$0.543 \pm 0.027$&$0.72 \pm 0.024$\\


&\cellcolor{lightgreen} \genemaskbest{} &\cellcolor{yellow} 0.708 $\pm$ 0.013*&\cellcolor{yellow} 0.779 $\pm$ 0.015*&\cellcolor{lightgreen} \textbf{0.847 $\pm$ 0.029}& \cellcolor{yellow} 0.920 $\pm$ 0.020&\cellcolor{lightgreen} \textbf{0.67 $\pm$ 0.017}&\cellcolor{lightgreen} \textbf{0.737 $\pm$ 0.013} &\cellcolor{yellow} $0.577 \pm 0.019$**&\cellcolor{yellow} $0.751 \pm 0.017$**\\ \cline{2-10}

500& ORI 120K &$0.743 \pm 0.008$&$0.819 \pm 0.007$&$0.883 \pm 0.006$&$0.951 \pm 0.005$&\textbf{0.698 $\pm$ 0.009}&\textbf{0.776 $\pm$ 0.011} &$0.429 \pm 0.07$&$0.608 \pm 0.077$\\ 

& Gene-PMI-VOC &$0.738 \pm 0.021$&$0.82 \pm 0.017$&$0.884 \pm 0.004$&$0.948 \pm 0.008$&$0.692 \pm 0.011$&$0.759 \pm 0.011$&$0.639 \pm 0.025$&$0.809 \pm 0.021$ \\ 
&  ORI 10K &\cellcolor{yellow} $0.752 \pm 0.007$&\textbf{0.831 $\pm$ 0.003}&\cellcolor{yellow} $0.888 \pm 0.007$&\textbf{0.958 $\pm$ 0.002}&$0.696 \pm 0.009$&$0.767 \pm 0.008$&\cellcolor{yellow} $0.665 \pm 0.035$&\cellcolor{yellow} $0.834 \pm 0.022$ \\

&\cellcolor{lightgreen}  \genemaskbest{} &\cellcolor{lightgreen} \textbf{0.753 $\pm$ 0.005}&\cellcolor{lightgreen} \textbf{0.831 $\pm$ 0.003}&\cellcolor{lightgreen} \textbf{0.89 $\pm$ 0.006}&\cellcolor{yellow} $0.957 \pm 0.002$&\cellcolor{lightgreen} \textbf{0.698 $\pm$ 0.006}&\cellcolor{yellow} 0.771 $\pm$ 0.007 &\cellcolor{lightgreen} \textbf{0.692 $\pm$ 0.016}*&\cellcolor{lightgreen} \textbf{0.851 $\pm$ 0.01}$\dagger$\\ \cline{2-10}

1000& ORI 120K  &$0.758 \pm 0.006$&$0.835 \pm 0.004$&$0.895 \pm 0.005$&$0.957 \pm 0.005$&$0.700 \pm 0.009$&$0.769 \pm 0.009$ &$0.496 \pm 0.109$&$0.673 \pm 0.107$\\ 
& Gene-PMI-VOC &$0.759 \pm 0.007$&$0.834 \pm 0.006$&$0.895 \pm 0.004$&$0.96 \pm 0.004$&$0.698 \pm 0.007$&$0.766 \pm 0.009$ &$0.504 \pm 0.02$&$0.689 \pm 0.016$ \\ 
&ORI 10K &\cellcolor{yellow} $0.765 \pm 0.004$&\cellcolor{yellow} $0.839 \pm 0.005$&\textbf{0.901 $\pm$ 0.003}&\textbf{0.964 $\pm$ 0.002}&\cellcolor{yellow} $0.705 \pm 0.005$&\cellcolor{yellow} $0.776 \pm 0.006$ &\cellcolor{yellow} $0.651 \pm 0.019$&\cellcolor{yellow} $0.821 \pm 0.015$\\


&\cellcolor{lightgreen}  \genemaskbest{} &\cellcolor{lightgreen} \textbf{0.766 $\pm$ 0.007}&\cellcolor{lightgreen} \textbf{0.843 $\pm$ 0.006}&\cellcolor{yellow} $0.898 \pm 0.005$&\cellcolor{yellow} $0.962 \pm 0.002$&\cellcolor{lightgreen} \textbf{0.706 $\pm$ 0.005}&\cellcolor{lightgreen} \textbf{0.778 $\pm$ 0.006} &\cellcolor{lightgreen} \textbf{0.709 $\pm$ 0.012}**&\cellcolor{lightgreen} \textbf{0.865 $\pm$ 0.008}** \\ \hline 

&\multicolumn{9}{c}{\textbf{LOGO as Base model}} \\ 
10& ORI 10K&$0.506 \pm 0.017$&$0.557 \pm 0.064$&$0.502 \pm 0.005$&$0.557 \pm 0.028$&$0.53 \pm 0.043$&$0.599 \pm 0.067$ &$0.333 \pm 0.012$&$0.565 \pm 0.022$\\
&\cellcolor{lightgreen} \genemask{} 10K &\cellcolor{lightgreen} \textbf{0.54 $\pm$ 0.049}$\dagger$&\cellcolor{lightgreen} \textbf{0.582 $\pm$ 0.088}$\dagger$&\cellcolor{lightgreen} \textbf{0.519 $\pm$ 0.04}&\cellcolor{lightgreen} \textbf{0.594 $\pm$ 0.086}&\cellcolor{lightgreen} \textbf{0.553 $\pm$ 0.063}&\cellcolor{lightgreen} \textbf{0.649 $\pm$ 0.067}* &\cellcolor{lightgreen} \textbf{0.353 $\pm$ 0.017}*&\cellcolor{lightgreen} \textbf{0.558 $\pm$ 0.028}\\ 


50& ORI 10K&$0.565 \pm 0.048$&$0.635 \pm 0.037$&$0.618 \pm 0.037$&$0.663 \pm 0.05$&$0.627 \pm 0.007$&$0.676 \pm 0.006$ &$0.438 \pm 0.019$&$0.61 \pm 0.021$\\
&\cellcolor{lightgreen} \genemask{} 10K &\cellcolor{lightgreen} \textbf{0.611 $\pm$ 0.04}**&\cellcolor{lightgreen} \textbf{0.676 $\pm$ 0.017}**&\cellcolor{lightgreen} \textbf{0.668 $\pm$ 0.014}**&\cellcolor{lightgreen} \textbf{0.733 $\pm$ 0.022}**&\cellcolor{lightgreen} \textbf{0.635 $\pm$ 0.006}$\dagger$&\cellcolor{lightgreen} \textbf{0.692 $\pm$ 0.009}** &\cellcolor{lightgreen} \textbf{0.465 $\pm$ 0.018}**&\cellcolor{lightgreen} \textbf{0.636 $\pm$ 0.022}*\\ 


100& ORI 10K&$0.628 \pm 0.014$&$0.677 \pm 0.017$&$0.646 \pm 0.033$&$0.694 \pm 0.039$&$0.638 \pm 0.008$&$0.691 \pm $ 0.011&$0.447 \pm 0.049$&$0.624 \pm 0.043$\\
&\cellcolor{lightgreen} \genemask{} 10K &\cellcolor{lightgreen} \textbf{0.644 $\pm$ 0.008}*&\cellcolor{lightgreen} \textbf{0.695 $\pm$ 0.012}*&\cellcolor{lightgreen} \textbf{0.705 $\pm$ 0.023}**&\cellcolor{lightgreen} \textbf{0.783 $\pm$ 0.023}**&\cellcolor{lightgreen} \textbf{0.639 $\pm$ 0.006}&\cellcolor{lightgreen} \textbf{0.695 $\pm$ 0.008} &\cellcolor{lightgreen} \textbf{0.465 $\pm$ 0.021}&\cellcolor{lightgreen} \textbf{0.651 $\pm$ 0.02}$\dagger$\\ %\cline{2-8}

500& ORI 10K&\textbf{0.693 $\pm$ 0.010}&\textbf{0.754 $\pm$ 0.007}&$0.751 \pm 0.034$&$0.817 \pm 0.037$&$0.629 \pm 0.006$&$0.679 \pm 0.007$&$0.355 \pm 0.027$&$0.629 \pm 0.014$ \\
&\cellcolor{lightgreen} \genemask{} 10K &\cellcolor{lightgreen} $0.69 \pm 0.006$&\cellcolor{lightgreen} $0.748 \pm 0.009$&\cellcolor{lightgreen} \textbf{0.835 $\pm$ 0.008}**&\cellcolor{lightgreen} \textbf{0.907 $\pm$ 0.008}**&\cellcolor{lightgreen} \textbf{0.648 $\pm$ 0.004}**&\cellcolor{lightgreen} \textbf{0.709 $\pm$ 0.04}**&\cellcolor{lightgreen} \textbf{0.443 $\pm$ 0.025}**&\cellcolor{lightgreen}  \textbf{0.644 $\pm$ 0.015}* \\ %\cline{2-8}

1000& ORI 10K &$0.705 \pm 0.014$&$0.769 \pm 0.013$&$0.808 \pm 0.015$&$0.884 \pm 0.016$&$0.652 \pm 0.008$& $0.709 \pm 0.008$ & $0.504 \pm 0.02$ & $0.689 \pm 0.016$ \\
&\cellcolor{lightgreen} \genemask{} 10K &\cellcolor{lightgreen} \textbf{0.723 $\pm$ 0.005}&\cellcolor{lightgreen} \textbf{0.785 $\pm$ 0.006}&\cellcolor{lightgreen} \textbf{0.855 $\pm$ 0.004}**&\cellcolor{lightgreen} \textbf{0.925 $\pm$ 0.004}**&\cellcolor{lightgreen} \textbf{0.660 $\pm$ 0.004}*&\cellcolor{lightgreen} \textbf{0.727 $\pm$ 0.004}** &\cellcolor{lightgreen} \textbf{0.507 $\pm$ 0.011}&\cellcolor{lightgreen} \textbf{0.701 $\pm$ 0.007}$\dagger$\\ \hline %\cline{2-8}
    \end{tabular}
    }
    \caption{Performance comparison for gene sequence classification tasks where all values are rounded to 3 decimal places. The best-performing performance value is written in \textbf{bold}, and the second-best-performing value is highlighted in yellow. \textit{ORI} refers to the SOTA model with random masking. Paired t-test was conducted to check whether the performance improvement of \genemask-based models over ORI 10K model is statistically significant or not; ** for $p$-value < 0.01, * for $p$ < 0.05, and $\dagger$ for p-value < 0.1 }
    \label{tab:perf-compare-10K}
\end{table*}
\raggedbottom

\noindent \textbf{Performance comparison among different masking strategies.} In the case of \textit{DNABert}, \genemaskbest{} \ul{mostly (two out of three times) outperforms all the baseline models across the four datasets of Prom-core, Prom-300, Cohn-enh, and Splice-40}, and in the remaining case where it does not achieve the best performance, it is \ul{always the second-best ranked model} (see Table~\ref{tab:perf-compare-10K}, the second best-performing model is highlighted in yellow). 
We observe that the task-specific performance improvement between \genemaskbest{}{} and random masking (ORI 10K model variant) in \textit{DNABert}, in terms of average accuracy over all few-shot settings for each task, is the highest for Splice-40 followed by Prom-300. Since the \genemask{} (w/ GB) pretraining model variant is explicitly targeted for Splice-40, it highlights the effectiveness of such task-specific interventions.

 In the case of \textit{LOGO}, we do not explore different pretraining model variants and instead compare between ORI 10K and \genemask{} 10K model (both of them have exactly the same model hyperparameters). We observe that \genemask{} 10K (base model) \ul{outperforms random masking-based SOTA models (ORI) in all settings across four benchmark tasks} as shown in Table~\ref{tab:perf-percent-fewshot-ori10K}. 
The highest percentage improvement in terms of average accuracy over all few-shot settings is Splice-40 with $8.32\%$, followed by Prom-300, Prom-core, and Cohn-enh at $7.67\%$, $3.72\%$ and $1.95\%$ respectively. However, we observe the \ul{performance improvement of} \genemask{} \ul{to be higher for LOGO as compared to DNABert for all tasks except the Cohn-enh task}. 

\begin{table}
    \centering
    \begin{tabular}{cccccc}
    \hline
        k-shot & 10 & 50 & 100 & 500 & 1000 \\ \hline
        DNABert &  $2.94\%$ & $0.93\%$ & $0.73\%$ & $0.40\%$ & $1.85\%$\\
        LOGO &  $4.92\%$ & $5.87\%$ & $3.90\%$ & $7.74\%$ & $2.85\%$\\\hline
        
    \end{tabular}
    \caption{Percentage improvement in average accuracy over four datasets of \genemask{} 10K over ORI 10K model. }
    \label{tab:perf-percent-fewshot-ori10K}
\end{table}

\textbf{\genemask{} 10K versus ORI 10K model.}
 In the case of DNABert and LOGO, ORI-10K almost always performs much inferior to any variant of \genemask, although trained for the same number of epochs. Table~\ref{tab:perf-percent-fewshot-ori10K} shows the percentage improvement in average accuracy across four datasets of \genemask{} 10K over the ORI 10K model for various $k$-shot setups. It indicates that \genemask{}-based masking strategy is more beneficial for lightweight models like LOGO; heavier models like DNABert might automatically learn a certain amount of span correlations (like PMI) information, thus diminishing the independent impact of \genemask. 
 From now onwards, we will discuss only DNABert-based models and compare the performance of \genemaskbest{} with the baseline models.  

\noindent \textbf{ \genemaskbest{} versus Gene-PMI-VOC model.} 
We observe that \genemaskbest{} \ul{almost consistently outperforms the Gene-PMI-VOC model, except for the 10, 50, and 100-shot settings of the Splice-40 task}. We further observe that the performance of \ul{Gene-PMI-VOC is quite unstable as it even performs poorer than the ORI 10K model in most cases} (12 out of 20 DNABert settings in Table~\ref{tab:perf-compare-10K}). A possible reason for such inconsistent performance is its sensitivity to the (arbitrary) size of its masking vocabulary, created using a PMI score to formulate an absolute importance notion. However, determining the optimal vocabulary size in a principled manner is difficult due to the absence of human-understandable semantics in genomics (like words and phrases in NLP). A suboptimal choice may lead to difficulty in guaranteeing diverse masking patterns, thus ushering in an inefficient MLM training regime. 

\begin{table}
    \centering
    \begin{tabular}{cccccc}
    \hline
        k-shot & 10 & 50 & 100 & 500 & 1000 \\ \hline
        ORI 120K & $3.58\%$ & $2.46\%$ & $2.64\%$ & $10.17\%$ & $8.15\%$ \\ \hline
    \end{tabular}
    \caption{Percentage improvement in average accuracy over four datasets of \genemaskbest{} over ORI 120K model in case of DNABert.}
    \label{tab:perf-percent-fewshot-ori120K}
\end{table}

\noindent \textbf{ \genemaskbest{} versus ORI-120K model.} Table~\ref{tab:perf-percent-fewshot-ori120K} shows the percentage improvement in average accuracy scores (over four datasets) of \genemaskbest{} over the original DNABert model trained for 120K steps (ORI 120K model). \genemaskbest{} improves decently over ORI 120K in low data settings (100-shot and below) but the performance rises in 500 and 1000-shot settings, which is contrary to our standard observation (all datasets except Splice-40). This happens due to the Splice-40 task, where the performance improvement due to \genemaskbest{} increases at higher data settings ($0.692$ versus $0.429$ and $0.709$ versus $0.496$ for \genemaskbest{} and ORI 120K model respectively) as \genemask{} (w/ GB) is effective at that setting. 
Another interesting observation is that \ul{ORI 120K model outperforms} \genemaskbest{} \ul{on shallow data settings (10, 50, and 100-shot) of the Prom-core task, whereas} \genemaskbest{} \ul{outperforms again in 500 and 1000-shot settings}. This may be because Prom-core is an easier task with a much shorter context (70 base pairs (bp) in length as compared to 300 bp for Prom-300 and 500 bp for Cohn-enh), whereby the ORI 120K model simply memorizes the gene sequence patterns instead of actually learning intrinsic (or extra) knowledge of gene sequences. Hence, the effect of memorization recedes in higher data settings (above 500-shot), as seen in Figure~\ref{fig:perf-compare-graph} of Appendix. The reason behind the improvement in the Splice-40 task at a higher $k$-shot is also similar. 

\noindent \textbf{Deconstructing \genemaskbest{}.}  We compare the performance of three model variants of \genemask{}. We observe from Table~\ref{tab:ablation-compressed} that \ul{half gradient accumulation (HGA) helps to improve the model performance in 10 and 50-shot} settings (10 and 50-shot for Prom-300, 50-shot for Cohn-enh, and 10-shot for Splice-40). It is empirically observed that if large batch size is used to train deep neural networks, the trained models appear to generalize poorly~\cite{hoffer2017,keskar2017}. HGA reduces the effective batch size by $50\%$ and mitigates the \textit{generalization gap} issue in such low data settings (10 and 50-shot). However, beyond the 100-shot setting, the impact is negative or marginal. We observe that when we consider \ul{gene boundaries during pretraining data construction (GB)}, the performance consistently improves for the Splice-40 task. The primary reason may be that the \ul{Splice-40 task involves identifying exon boundaries, which can benefit strongly from gene boundaries as side information}.  

\begin{table}[!ht]
    \footnotesize
    \centering
    \scalebox{0.72}{
    \addtolength{\tabcolsep}{-0.3em}
    \begin{tabular}{cccccc}
    \hline
        \textbf{k-shot} &  \textbf{Model Type}  & \textbf{Prom-core}& \textbf{Prom-300} & \textbf{Cohn-enh} & \textbf{Splice-40}  \\ \hline

10 & \cellcolor{lightgreen} \genemask&\cellcolor{lightgreen} \textbf{0.602 $\pm$ 0.058}&\cellcolor{lightgreen} $0.625 \pm 0.09$&\cellcolor{lightgreen} \textbf{0.622 $\pm$ 0.050}&\cellcolor{lightgreen} $0.392 \pm 0.02$\\

& \genemask w/ HGA &$0.585 \pm 0.065$&\textbf{0.676 $\pm$ 0.054}&$0.604 \pm 0.068$&\textbf{0.412 $\pm$ 0.026} \\

& \genemask w/ GB&$0.568 \pm 0.05$ &$0.593 \pm 0.038$&$0.602 \pm 0.043$&$0.398 \pm 0.015$ \\ 

50 & \cellcolor{lightgreen} \genemask &\cellcolor{lightgreen} \textbf{0.678 $\pm$ 0.026}&\cellcolor{lightgreen} $0.781 \pm 0.097$&\cellcolor{lightgreen} $0.648 \pm 0.016$&\cellcolor{lightgreen} $0.505 \pm 0.018$  \\ 

& \genemask w/ HGA &$0.677 \pm 0.033$&\textbf{0.815 $\pm$ 0.02}&\textbf{0.654 $\pm$ 0.017}&$0.448 \pm 0.095$ \\

&  \genemask w/ GB &$0.669 \pm 0.025$&$0.793 \pm 0.022$&$0.641 \pm 0.012$&\textbf{0.519 $\pm$ 0.027} \\ 

100 & \cellcolor{lightgreen} \genemask&\cellcolor{lightgreen} \textbf{0.708 $\pm$ 0.013}&\cellcolor{lightgreen} $0.843 \pm 0.027$&\cellcolor{lightgreen} $0.655 \pm 0.052$&\cellcolor{lightgreen} $0.561 \pm 0.027$ \\ 

& \genemask w/ HGA &$0.694 \pm 0.043$&\textbf{0.847 $\pm$ 0.029}&\textbf{0.67 $\pm$ 0.017} &$0.555 \pm 0.079$ \\
& \genemask w/ GB&$0.695 \pm 0.014$&$0.829 \pm 0.016$&$0.661 \pm 0.008$&\textbf{0.577 $\pm$ 0.019} \\ 

500 & \cellcolor{lightgreen} \genemask &\cellcolor{lightgreen} \textbf{0.753 $\pm$ 0.005}&\cellcolor{lightgreen} \textbf{0.89 $\pm$ 0.006}&\cellcolor{lightgreen} \textbf{0.698 $\pm$ 0.006} &\cellcolor{lightgreen} $0.669 \pm 0.017$ \\  

& \genemask w/ HGA &$0.75 \pm 0.008$&$0.887 \pm 0.004$&$0.674 \pm 0.058$ &$0.662 \pm 0.026$ \\

&  \genemask w/ GB&$0.746 \pm 0.006$&$0.877 \pm 0.005$&$0.675 \pm 0.005$&\textbf{0.692 $\pm$ 0.016} \\ 

1000&  \cellcolor{lightgreen} \genemask &\cellcolor{lightgreen} \textbf{0.766 $\pm$ 0.004}&\cellcolor{lightgreen} \textbf{0.898 $\pm$ 0.005}&\cellcolor{lightgreen} \textbf{0.706 $\pm$ 0.005}&\cellcolor{lightgreen} \textbf{0.709 $\pm$ 0.012} \\ 

& \genemask w/ HGA &\textbf{0.766 $\pm$ 0.007}&\textbf{0.898 $\pm$ 0.004}&$0.699 \pm 0.008$ &$0.69 \pm 0.025$ \\

& \genemask w/ GB&$0.756 \pm 0.005$&$0.889 \pm 0.005$&$0.688 \pm 0.004$&$0.693 \pm 0.02$ \\ \hline 
    \end{tabular}
    }
    \caption{Performance comparison in terms of accuracy among \genemask-guided DNABert model variants at $10000$ steps. \textit{\genemask} represents the hyperparameter settings of baseline models as ORI 10K model}
    \label{tab:ablation-compressed}
\end{table}
\raggedbottom



\section{Domain-specific Model Explainability}
\label{sec:motifs}
\begin{table}
    \centering
    \scalebox{0.9}{
    \begin{tabular}{cccc} 
    \hline
         Dataset&Motifs& Normalized PMI rank  \\
         &(Consensus Logo)& (out of 4096) \\ \hline
         Prom-core & n\textbf{TATAAA}r &242\\
         Cohn-enh & \textbf{GTGGCT}sw & 126 \\
         Prom-core, Cohn-enh&nCyy\textbf{CCTCC}n* &1, 11, 52, 175, 186 \\
         Prom-core & sCw\textbf{GCAGC}n & 259, 516, 540, 570, 628 \\
         Cohn-enh & ks\textbf{CTGGG}m &5, 17, 20, 21, 71 \\
         Cohn-enh & \textbf{TTTTTT}TTTn & 8 \\ \hline
    \end{tabular}
    }
    \caption{PMI-based rankings based on Normalized-PMI\textsubscript{n} score for motifs in finetuning datasets. A motif of length five matches as a sub-string to multiple 6-mers and thus mention the top five ranks}
    \label{tab:motif-6-mer-ranking}
\end{table}

Motifs are repetitive units within a \textit{Human Reference Genome}, having a certain biological significance. We check whether the correlated tokens identified by \genemask{}  are, in fact (part of) such units. We also perform a  preliminary evaluation on the task of identifying functional genetic variants in Section~\ref{sec:app-func-gene-variants}.

\noindent \textbf{Associating top-20 ranked 6-mers based on Normalized-PMI\textsubscript{k} score with conserved DNA sequence motifs.} 
We analyze whether \ul{highly ranked PMI tokens} resemble meaningful concepts by checking their \ul{overlap with known motifs}. We performed a Google search with the following query template: [``6-mer name'' \textit{DNA sequence motif}]. \textit{AATCTC} is a 6-mer, ``'' are used as a Google wildcard to indicate that the term \textit{AATCTC} must always be present in search results. We only considered the first page of Google results to determine whether a particular 6-mer is mentioned in biomedical literature.  Among the \textit{top 20 ranked} 6-mers, we observed that all except 2 (\textit{CCAGGC} - rank nine, \textit{GCCTGG} - rank ten) are indeed previously mentioned in the biomedical literature. Since the \genemask{} strategy favors the top-ranked PMI tokens that correlate well with the known DNA sequence motifs of enhancers and promoters, this knowledge translates to improved performance in low data settings (10, 50, and 100-shot) on the Prom-core, Prom-300, and Cohn-enh datasets. 

\noindent \textbf{PMI-based rankings capture motifs present in finetuning datasets.} We use the R package \textit{rGADEM}~\cite{rgadem} to perform de novo motif discovery and obtain a total of $12$ motifs from the Prom-core and Cohn-enh datasets, which are mostly concentrated around lengths 5, 6, and 7. We present our best matches and their corresponding ranks in the 6-mer PMI ranked list (\textit{RANK}) in Table~\ref{tab:motif-6-mer-ranking} (see Table~\ref{tab:appendix-motif-6-mer-ranking} for the complete list, their corresponding consensus logos in Figures~\ref{fig:consensus-logos-1} and~\ref{fig:consensus-logos-2}). 
We observe that most of the \ul{6-mers that match the discovered motifs are ranked very high}. We further observe that our \ul{top-1 ranked 6-mer} is present in motifs of both \ul{enhancers and promoters} based on de-novo motif discovery and is mentioned in previous biomedical literature~\cite{chow1991}. The TATA box, a well-known motif for promoters (row 1 of Table~\ref{tab:motif-6-mer-ranking}) is ranked at $242$; the best TATA box motif is \textit{TATATA} with a PMI rank of $15$. 