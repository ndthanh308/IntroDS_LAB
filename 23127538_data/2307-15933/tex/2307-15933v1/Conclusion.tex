\section{Conclusion}
In this paper, we develop a PMI- induced novel masking algorithm, \genemask{}, for MLM training of gene sequences which ensures a substantial speedup of 10x and performance improvement over the random masking strategy of SOTA models (DNABert and LOGO) in various few-shot settings. 
Further we showed that for a certain task (Splice-40), respecting gene boundaries during pretraining data construction improves the performance even further, confirming the basic thesis of the importance of incorporating domain-specific information during pretraining. This is further corroborated when we observe a strong correlation  between top-ranked NPMI tokens and conserved DNA sequence motifs, implying that \genemask{} is able to capture meaningful semantic structure in gene sequences. Finally, we must mention that gene sequence classification is a challenging problem and requires a tremendous engineering effort even to develop the experimental setup. Therefore, the codes (including trained models), datasets, and appendix are made publicly available at \url{https://github.com/roysoumya/GeneMask}. 
The elaborate setup developed will help us undertake several 
 \textbf{future work}, for example, we will explore time-variant MLM training strategies for gene sequences to achieve better few-shot performance, such as incorporating a decaying masking ratio, which showed good performance on GLUE and SQuaD datasets in NLP domain~\cite{yang2023learning}. 
 
 \ack Soumyadeep Roy is supported by the Institute Ph.D. Fellowship at the Indian Institute of Technology Kharagpur. Soumyadeep Roy and Niloy Ganguly were also affiliated with L3S Research Center, Germany while conducting this work. This research was funded by the Federal Ministry of Education and Research (BMBF), Germany, under the project LeibnizKILabor with grant No. 01DD20003.