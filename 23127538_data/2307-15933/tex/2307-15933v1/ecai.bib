@article {Schneider072116,
	author = {Schneider, Valerie A. and Graves-Lindsay, Tina and others},
	title = {Evaluation of GRCh38 and de novo haploid genome assemblies demonstrates the enduring quality of the reference assembly},
	elocation-id = {072116},
	year = {2016},
	doi = {10.1101/072116},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {The human reference genome assembly plays a central role in nearly all aspects of today{\textquoteright}s basic and clinical research. GRCh38 is the first coordinate-changing assembly update since 2009 and reflects the resolution of roughly 1000 issues and encompasses modifications ranging from thousands of single base changes to megabase-scale path reorganizations, gap closures and localization of previously orphaned sequences. We developed a new approach to sequence generation for targeted base updates and used data from new genome mapping technologies and single haplotype resources to identify and resolve larger assembly issues. For the first time, the reference assembly contains sequence-based representations for the centromeres. We also expanded the number of alternate loci to create a reference that provides a more robust representation of human population variation. We demonstrate that the updates render the reference an improved annotation substrate, alter read alignments in unchanged regions and impact variant interpretation at clinically relevant loci. We additionally evaluated a collection of new de novo long-read haploid assemblies and conclude that while the new assemblies compare favorably to the reference with respect to continuity, error rate, and gene completeness, the reference still provides the best representation for complex genomic regions and coding sequences. We assert that the collected updates in GRCh38 make the newer assembly a more robust substrate for comprehensive analyses that will promote our understanding of human biology and advance our efforts to improve health.},
	journal = {bioRxiv}
}

@Article{McGuire2020,
author={McGuire, Amy L.
and Gabriel, Stacey
and others},
title={The road ahead in genetics and genomics},
journal={Nature Reviews Genetics},
year={2020},
month={Oct},
day={01},
volume={21},
number={10},
pages={581-596},
abstract={In celebration of the 20th anniversary of Nature Reviews Genetics, we asked 12 leading researchers to reflect on the key challenges and opportunities faced by the field of genetics and genomics. Keeping their particular research area in mind, they take stock of the current state of play and emphasize the work that remains to be done over the next few years so that, ultimately, the benefits of genetic and genomic research can be felt by everyone.},
issn={1471-0064},
doi={10.1038/s41576-020-0272-6}
}

@inproceedings{glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  booktitle={ EMNLP},
  
  year={2014}
}

@article{lstm,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
}

@inproceedings{pmi-masking,
title={PMI-Masking: Principled masking of correlated spans},
author={Yoav Levine and Barak Lenz and others},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=3Aoft6NWFej}
}

@Article{enformer,
author={Avsec, {\v{Z}}iga
and Agarwal, Vikram
and others},
title={Effective gene expression prediction from sequence by integrating long-range interactions},
journal={Nature Methods},
year={2021},
month={Oct},
day={01},
volume={18},
number={10},
pages={1196-1203},
abstract={How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequences through the use of a deep learning architecture, called Enformer, that is able to integrate information from long-range interactions (up to 100{\thinspace}kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Furthermore, Enformer learned to predict enhancer--promoter interactions directly from the DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of human disease associations and provide a framework to interpret cis-regulatory evolution.},
issn={1548-7105},
doi={10.1038/s41592-021-01252-x},
url={https://doi.org/10.1038/s41592-021-01252-x}
}


@inproceedings{word2vec,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in NIPS},
  year={2013}
}
@inproceedings{baldi2012autoencoders,
  title={Autoencoders, unsupervised learning, and deep architectures},
  author={Baldi, Pierre},
  booktitle={ ICML workshop on unsupervised and transfer learning},
  year={2012}
}

@article{math-genomics,
author = {Athens, Josie and José, Marco},
year = {2005},
month = {04},
pages = {},
title = {Mathematical properties of DNA sequences from coding and noncoding regions},
volume = {51},
journal = {Revista Mexicana de Física}
}

@article{genomics-survey,
  title={Deep Learning for Genomics: A Concise Overview},
  author={Tianwei Yue and Haohan Wang},
  journal={ArXiv},
  year={2018},
  volume={abs/1802.00810}
}

@inproceedings{elmo,
  author={Peters, Matthew E. and  Neumann, Mark and others},
  title={Deep contextualized word representations},
  booktitle={Proc. of NAACL},
  year={2018}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{bert,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
}

@article{protvec,
   title={Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics},
   volume={10},
   ISSN={1932-6203},
   DOI={10.1371/journal.pone.0141287},
   number={11},
   journal={PLOS ONE},
   publisher={Public Library of Science (PLoS)},
   author={Asgari, Ehsaneddin and Mofrad, Mohammad R. K.},
   year={2015},
   month={Nov},
   pages={e0141287}
}

@article{DNABert,
    author = {Ji, Yanrong and Zhou, Zhihan and others},
    title = "{DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome}",
    journal = {Bioinformatics},
    volume = {37},
    number = {15},
    pages = {2112-2120},
    year = {2021},
    month = {02},
    abstract = "{Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btab083},
}


@article{rudin2019stop,
  title={Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author={Rudin, Cynthia},
  journal={Nature Machine Intelligence},
  volume={1},
  number={5},
  pages={206--215},
  year={2019},
  publisher={Nature Publishing Group}
}

@ARTICLE{deepromoter2019,
AUTHOR={Oubounyt, Mhaned and Louadi, Zakaria and Tayara, Hilal and Chong, Kil To},   
TITLE={DeePromoter: Robust Promoter Predictor Using Deep Learning},      
JOURNAL={Frontiers in Genetics},      
VOLUME={10},      
YEAR={2019},        
DOI={10.3389/fgene.2019.00286},      
ISSN={1664-8021},   
ABSTRACT={The promoter region is located near the transcription start sites and regulates transcription initiation of the gene by controlling the binding of RNA polymerase. Thus, promoter region recognition is an important area of interest in the field of bioinformatics. Numerous tools for promoter prediction were proposed. However, the reliability of these tools still needs to be improved. In this work, we propose a robust deep learning model, called DeePromoter, to analyze the characteristics of the short eukaryotic promoter sequences, and accurately recognize the human and mouse promoter sequences. DeePromoter combines a convolutional neural network (CNN) and a long short-term memory (LSTM). Additionally, instead of using non-promoter regions of the genome as a negative set, we derive a more challenging negative set from the promoter sequences. The proposed negative set reconstruction method improves the discrimination ability and significantly reduces the number of false positive predictions. Consequently, DeePromoter outperforms the previously proposed promoter prediction tools. In addition, a web-server for promoter prediction is developed based on the proposed methods and made available at <ext-link ext-link-type="uri" xlink:href="https://home.jbnu.ac.kr/NSCL/deepromoter.htm" xmlns:xlink="http://www.w3.org/1999/xlink">https://home.jbnu.ac.kr/NSCL/deepromoter.htm</ext-link>.}
}

@article{epdnew2012,
    author = {Dreos, René and Ambrosini, Giovanna and others},
    title = "{EPD and EPDnew, high-quality promoter resources in the next-generation sequencing era}",
    journal = {Nucleic Acids Research},
    volume = {41},
    number = {D1},
    pages = {D157-D164},
    year = {2012},
    month = {11},
    abstract = "{The Eukaryotic Promoter Database (EPD), available online at http://epd.vital-it.ch, is a collection of experimentally defined eukaryotic POL II promoters which has been maintained for more than 25 years. A promoter is represented by a single position in the genome, typically the major transcription start site (TSS). EPD primarily serves biologists interested in analysing the motif content, chromatin structure or DNA methylation status of co-regulated promoter subsets. Initially, promoter evidence came from TSS mapping experiments targeted at single genes and published in journal articles. Today, the TSS positions provided by EPD are inferred from next-generation sequencing data distributed in electronic form. Traditionally, EPD has been a high-quality database with low coverage. The focus of recent efforts has been to reach complete gene coverage for important model organisms. To this end, we introduced a new section called EPDnew, which is automatically assembled from multiple, carefully selected input datasets. As another novelty, we started to use chromatin signatures in addition to mRNA 5′tags to locate promoters of weekly expressed genes. Regarding user interfaces, we introduced a new promoter viewer which enables users to explore promoter-defining experimental evidence in a UCSC genome browser window.}",
    issn = {0305-1048},
    doi = {10.1093/nar/gks1233},
    url = {https://doi.org/10.1093/nar/gks1233},
    eprint = {https://academic.oup.com/nar/article-pdf/41/D1/D157/18788951/gks1233.pdf},
}

@inproceedings{bigbird2020,
 author = {Zaheer, Manzil and Guruganesh, Guru and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {17283--17297},
 title = {Big Bird: Transformers for Longer Sequences},
 url = {https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{arumae-etal-2020-empirical,
    title = "An Empirical Investigation Towards Efficient Multi-Domain Language Model Pre-training",
    author = "Arumae, Kristjan  and
      Sun, Qing  and
      Bhatia, Parminder",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    url = "https://aclanthology.org/2020.emnlp-main.394",
    doi = "10.18653/v1/2020.emnlp-main.394",
    pages = "4854--4864",
    abstract = "Pre-training large language models has become a standard in the natural language processing community. Such models are pre-trained on generic data (e.g. BookCorpus and English Wikipedia) and often fine-tuned on tasks in the same domain. However, in order to achieve state-of-the-art performance on out of domain tasks such as clinical named entity recognition and relation extraction, additional in domain pre-training is required. In practice, staged multi-domain pre-training presents performance deterioration in the form of catastrophic forgetting (CF) when evaluated on a generic benchmark such as GLUE. In this paper we conduct an empirical investigation into known methods to mitigate CF. We find that elastic weight consolidation provides best overall scores yielding only a 0.33{\%} drop in performance across seven generic tasks while remaining competitive in bio-medical tasks. Furthermore, we explore gradient and latent clustering based data selection techniques to improve coverage when using elastic weight consolidation and experience replay methods.",
}



@InProceedings{houlsby19a,
  title = 	 {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and others},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2790--2799},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/houlsby19a.html},
  abstract = 	 {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8\%$ of the performance of full fine-tuning, adding only $3.6\%$ parameters per task. By contrast, fine-tuning trains $100\%$ of the parameters per task.}
}


@inproceedings{kadapter2021,
  author    = {Ruize Wang and
               Duyu Tang and
               others},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters},
  booktitle = {Findings of the Association for Computational Linguistics: {ACL/IJCNLP}
               2021, Online Event, August 1-6, 2021},
  series    = {Findings of {ACL}},
  volume    = {{ACL/IJCNLP} 2021},
  pages     = {1405--1418},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  doi       = {10.18653/v1/2021.findings-acl.121},
}

@article {yang:2021:bioarxiv:logo,
    author = {Yang, Meng and Huang, Lichao and others},
    title = "{Integrating convolution and self-attention improves language model of human genome for interpreting non-coding regions at base-resolution}",
    journal = {Nucleic Acids Research},
    volume = {50},
    number = {14},
    pages = {e81-e81},
    year = {2022},
    month = {05},
    abstract = "{Interpretation of non-coding genome remains an unsolved challenge in human genetics due to impracticality of exhaustively annotating biochemically active elements in all conditions. Deep learning based computational approaches emerge recently to help interpret non-coding regions. Here, we present LOGO (Language of Genome), a self-attention based contextualized pre-trained language model containing only two self-attention layers with 1 million parameters as a substantially light architecture that applies self-supervision techniques to learn bidirectional representations of the unlabelled human reference genome. LOGO is then fine-tuned for sequence labelling task, and further extended to variant prioritization task via a special input encoding scheme of alternative alleles followed by adding a convolutional module. Experiments show that LOGO achieves 15\\% absolute improvement for promoter identification and up to 4.5\\% absolute improvement for enhancer-promoter interaction prediction. LOGO exhibits state-of-the-art multi-task predictive power on thousands of chromatin features with only 3\\% parameterization benchmarking against the fully supervised model, DeepSEA and 1\\% parameterization against a recent BERT-based DNA language model. For allelic-effect prediction, locality introduced by one dimensional convolution shows improved sensitivity and specificity for prioritizing non-coding variants associated with human diseases. In addition, we apply LOGO to interpret type 2 diabetes (T2D) GWAS signals and infer underlying regulatory mechanisms. We make a conceptual analogy between natural language and human genome and demonstrate LOGO is an accurate, fast, scalable, and robust framework to interpret non-coding regions for global sequence labeling as well as for variant prioritization at base-resolution.}",
    issn = {0305-1048},
    doi = {10.1093/nar/gkac326},
    url = {https://doi.org/10.1093/nar/gkac326},
    eprint = {https://academic.oup.com/nar/article-pdf/50/14/e81/45289746/gkac326\_supplemental\_file.pdf},
}

	@article{10.1093/nar/gkac326,
author = {Yang, Meng and Huang, Haiping and others},
	title = {Integrating convolution and self-attention improves language model of human genome for interpreting non-coding regions at base-resolution},
	year = {2021},
	doi = {10.1101/2021.09.06.459087},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Interpretation of non-coding genome remains an unsolved challenge in human genetics due to impracticality of exhaustively annotate biochemically active elements in all conditions. Deep learning based computational approaches emerge recently to help interpretating non-coding regions. Here we present LOGO (Language of Genome), a self-attention based contextualized pre-trained language model containing only 2 self-attention layers with 1 million parameters as a substantially light architecture that applies self-supervision techniques to learn bidirectional representations of unlabeled human reference genome. LOGO is then fine-tuned for sequence labelling task, and further extended to variant prioritization task via a special input encoding scheme of alternative alleles followed by adding a convolutional module. Experiments show that LOGO achieves 15\% absolute improvement for promoter identification and up to 4.5\% absolute improvement for enhancer-promoter interaction prediction. LOGO exhibits state-of-the-art multi-task predictive power on thousands of chromatin features with only 3\% parameterization benchmarking against fully supervised model, DeepSEA and 1\% parameterization against a recent BERT-based language model for human genome. For allelic-effect prediction, locality introduced by one dimensional convolution shows improved sensitivity and specificity for prioritizing non-coding variants associated with human diseases. In addition, we apply LOGO to interpret type 2 diabetes (T2D) GWAS signals and infer underlying regulatory mechanisms. We make a conceptual analogy between natural language and human genome and demonstrate LOGO is an accurate, fast, scalable, and robust framework to interpret non-coding regions for global sequence labeling as well as for variant prioritization at base-resolution.Competing Interest StatementF.M. declares the following competing interests: stock holdings in MGI.},
	URL = {https://www.biorxiv.org/content/early/2021/09/06/2021.09.06.459087},
	eprint = {https://www.biorxiv.org/content/early/2021/09/06/2021.09.06.459087.full.pdf},
	journal = {bioRxiv}
}

@article{Li2019,
    author = {Li, Wenran and Wong, Wing Hung and Jiang, Rui},
    title = "{DeepTACT: predicting 3D chromatin contacts via bootstrapping deep learning}",
    journal = {Nucleic Acids Research},
    volume = {47},
    number = {10},
    pages = {e60-e60},
    year = {2019},
    month = {03},
    abstract = "{Interactions between regulatory elements are of crucial importance for the understanding of transcriptional regulation and the interpretation of disease mechanisms. Hi-C technique has been developed for genome-wide detection of chromatin contacts. However, unless extremely deep sequencing is performed on a very large number of input cells, which is technically limited and expensive, current Hi-C experiments do not have high enough resolution to resolve contacts between regulatory elements. Here, we develop DeepTACT, a bootstrapping deep learning model, to integrate genome sequences and chromatin accessibility data for the prediction of chromatin contacts between regulatory elements. DeepTACT can infer not only promoter–enhancer interactions, but also promoter–promoter interactions. In tests based on promoter capture Hi-C data, DeepTACT shows better performance over existing methods. DeepTACT analysis also identifies a class of hub promoters, which are correlated with transcriptional activation across cell lines, enriched in housekeeping genes, functionally related to fundamental biological processes, and capable of reflecting cell similarity. Finally, the utility of chromatin contacts in the study of human diseases is illustrated by the association of IFNA2 to coronary artery disease via an integrative analysis of GWAS data and interactions predicted by DeepTACT.}",
    issn = {0305-1048},
    doi = {10.1093/nar/gkz167},
    url = {https://doi.org/10.1093/nar/gkz167},
    eprint = {https://academic.oup.com/nar/article-pdf/47/10/e60/28761812/gkz167.pdf},
}


@article{Zhou2015PredictingEO,
  title={Predicting effects of noncoding variants with deep learning–based sequence model},
  author={Jian Zhou and Olga G. Troyanskaya},
  journal={Nature Methods},
  year={2015},
  volume={12},
  pages={931-934}
}

@article{Khurana2016RoleON,
  title={Role of non-coding sequence variants in cancer},
  author={Ekta Khurana and Yao Fu and Dimple Chakravarty and Francesca Demichelis and Mark A. Rubin and Mark B. Gerstein},
  journal={Nature Reviews Genetics},
  year={2016},
  volume={17},
  pages={93-108}
}

@article{benson2018genbank,
  title={GenBank.},
  author={Benson, DA and Cavanaugh, M and others},
  journal={Nucleic Acids Research},
  volume={46},
  number={D1},
  pages={D41--D47},
  year={2018}
}

@article{whalen2021navigating,
  title={Navigating the pitfalls of applying machine learning in genomics},
  author={Whalen, Sean and Schreiber, Jacob and Noble, William S and Pollard, Katherine S},
  journal={Nature Reviews Genetics},
  pages={1--13},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{Wang2019,
author={Wang, Ruohan
and Wang, Zishuai
and others},
title={SpliceFinder: ab initio prediction of splice sites using convolutional neural network},
journal={BMC Bioinformatics},
year={2019},
month={Dec},
day={27},
volume={20},
number={23},
pages={652},
abstract={Identifying splice sites is a necessary step to analyze the location and structure of genes. Two dinucleotides, GT and AG, are highly frequent on splice sites, and many other patterns are also on splice sites with important biological functions. Meanwhile, the dinucleotides occur frequently at the sequences without splice sites, which makes the prediction prone to generate false positives. Most existing tools select all the sequences with the two dimers and then focus on distinguishing the true splice sites from those pseudo ones. Such an approach will lead to a decrease in false positives; however, it will result in non-canonical splice sites missing.},
issn={1471-2105},
doi={10.1186/s12859-019-3306-3}
}

@inproceedings{Badirli2021,
 author = {Badirli, Sarkhan and Akata, Zeynep and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {19352--19362},
 title = {Fine-Grained Zero-Shot Learning with DNA as Side Information},
 url = {https://proceedings.neurips.cc/paper/2021/file/a18630ab1c3b9f14454cf70dc7114834-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{mock2021bertax,
  title={BERTax: taxonomic classification of DNA sequences with Deep Neural Networks},
  author={Mock, Florian and Kretschmer, Fleming and others},
  journal={BioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}

@article{genebert2021,
  author    = {Shentong Mo and
               Xi Fu and
               others},
  title     = {Multi-modal Self-supervised Pre-training for Regulatory Genome Across
               Cell Types},
  journal   = {CoRR},
  volume    = {abs/2110.05231},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.05231},
  eprinttype = {arXiv},
  eprint    = {2110.05231},
  timestamp = {Mon, 25 Oct 2021 20:07:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-05231.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{yamada-etal-2020-luke,
    title = "{LUKE}: Deep Contextualized Entity Representations with Entity-aware Self-attention",
    author = "Yamada, Ikuya  and
      Asai, Akari  and
      Shindo, Hiroyuki  and
      Takeda, Hideaki  and
      Matsumoto, Yuji",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    url = "https://aclanthology.org/2020.emnlp-main.523",
    doi = "10.18653/v1/2020.emnlp-main.523",
    pages = "6442--6454",
    abstract = "Entity representations are useful in natural language tasks involving entities. In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer. The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them. Our model is trained using a new pretraining task based on the masked language model of BERT. The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia. We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores. The proposed model achieves impressive empirical performance on a wide range of entity-related tasks. In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering). Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.",
}


@inproceedings{pergola-etal-2021-boosting,
    title = "Boosting Low-Resource Biomedical {QA} via Entity-Aware Masking Strategies",
    author = "Pergola, Gabriele  and
      Kochkina, Elena  and
      Gui, Lin  and
      Liakata, Maria  and
      He, Yulan",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    url = "https://aclanthology.org/2021.eacl-main.169",
    doi = "10.18653/v1/2021.eacl-main.169",
    pages = "1977--1985",
    abstract = "Biomedical question-answering (QA) has gained increased attention for its capability to provide users with high-quality information from a vast scientific literature. Although an increasing number of biomedical QA datasets has been recently made available, those resources are still rather limited and expensive to produce; thus, transfer learning via pre-trained language models (LMs) has been shown as a promising approach to leverage existing general-purpose knowledge. However, fine-tuning these large models can be costly and time consuming and often yields limited benefits when adapting to specific themes of specialised domains, such as the COVID-19 literature. Therefore, to bootstrap further their domain adaptation, we propose a simple yet unexplored approach, which we call biomedical entity-aware masking (BEM) strategy, encouraging masked language models to learn entity-centric knowledge based on the pivotal entities characterizing the domain at hand, and employ those entities to drive the LM fine-tuning. The resulting strategy is a downstream process applicable to a wide variety of masked LMs, not requiring additional memory or components in the neural architectures. Experimental results show performance on par with the state-of-the-art models on several biomedical QA datasets.",
}


@misc{corby2020,
  doi = {10.48550/ARXIV.2007.00655},
  url = {https://arxiv.org/abs/2007.00655},
  author = {Rosset, Corby and Xiong, Chenyan and others},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Knowledge-Aware Language Model Pretraining},
  publisher = {arXiv},
  year = {2020},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{yamaguchi-etal-2021-frustratingly,
    title = "Frustratingly Simple Pretraining Alternatives to Masked Language Modeling",
    author = "Yamaguchi, Atsuki  and
      Chrysostomou, George  and
      others",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.249",
    doi = "10.18653/v1/2021.emnlp-main.249",
    pages = "3116--3125",
    abstract = "Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQUAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41{\%} of the BERT-BASE{'}s parameters, BERT-MEDIUM results in only a 1{\%} drop in GLUE scores with our best objective.",
}


@article{nguyen2016dna,
  title={DNA Sequence Classification by Convolutional Neural Network},
  author={Nguyen, Ngoc Giang and Tran, Vu Anh and others},
  journal={Journal of Biomedical Science and Engineering},
  volume={9},
  number={05},
  pages={280},
  year={2016},
  publisher={Scientific Research Publishing}
}


@inproceedings{mikolov2013,
  author    = {Tom{\'{a}}s Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{dnavec,
  author    = {Patrick Ng},
  title     = {dna2vec: Consistent vector representations of variable-length k-mers},
  journal   = {CoRR},
  volume    = {abs/1701.06279},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.06279},
  eprinttype = {arXiv},
  eprint    = {1701.06279},
  timestamp = {Mon, 13 Aug 2018 16:48:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Ng17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wwbert:github,
  title={Original BERT Github Repository},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  url={https://github.com/google-research/bert},
  year={2019}
}

@inproceedings{senrich:2016:acl:wholewordmasking,
  author    = {Rico Sennrich and
               Barry Haddow and
               others},
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2016, Volume
               1: Long Papers},
  year      = {2016},
  url       = {https://doi.org/10.18653/v1/p16-1162},
  doi       = {10.18653/v1/p16-1162},
  timestamp = {Fri, 06 Aug 2021 00:41:04 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/SennrichHB16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{visscher2017,
title = {10 Years of GWAS Discovery: Biology, Function, and Translation},
journal = {The American Journal of Human Genetics},
volume = {101},
number = {1},
pages = {5-22},
year = {2017},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2017.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0002929717302409},
author = {Peter M. Visscher and Naomi R. Wray and Qian Zhang and Pamela Sklar and Mark I. McCarthy and Matthew A. Brown and Jian Yang},
keywords = {genome-wide association study, SNP, obesity, auto-immune disease, schizophrenia, heritability},
abstract = {Application of the experimental design of genome-wide association studies (GWASs) is now 10 years old (young), and here we review the remarkable range of discoveries it has facilitated in population and complex-trait genetics, the biology of diseases, and translation toward new therapeutics. We predict the likely discoveries in the next 10 years, when GWASs will be based on millions of samples with array data imputed to a large fully sequenced reference panel and on hundreds of thousands of samples with whole-genome sequencing data.}
}

@article{kundaje2015integrative,
  title={Integrative analysis of 111 reference human epigenomes},
  author={Kundaje, Anshul and Meuleman, Wouter and others},
  journal={Nature},
  volume={518},
  number={7539},
  pages={317--330},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{encode2012integrated,
  title={An integrated encyclopedia of DNA elements in the human genome},
  author={ENCODE Project Consortium},
  journal={Nature},
  volume={489},
  number={7414},
  pages={57--74},
  year={2012},
  publisher={Nature Publishing Group UK London}
}



@article{spanbert2020,
    author = {Joshi, Mandar and Chen, Danqi and others},
    title = "{SpanBERT: Improving Pre-training by Representing and Predicting Spans}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    pages = {64-77},
    year = {2020},
    month = {01},
    abstract = "{We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6\\% and 88.7\\% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6\\% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00300},
    url = {https://doi.org/10.1162/tacl\_a\_00300},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00300/1923170/tacl\_a\_00300.pdf},
}

@article {cohn2018,
	author = {Cohn, Dikla and Zuk, Or and others},
	title = {Enhancer Identification using Transfer and Adversarial Deep Learning of DNA Sequences},
	elocation-id = {264200},
	year = {2018},
	doi = {10.1101/264200},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Enhancer sequences regulate the expression of genes from afar by providing a binding platform for transcription factors, often in a tissue-specific or context-specific manner. Despite their importance in health and disease, our understanding of these DNA sequences, and their regulatory grammar, is limited. This impairs our ability to identify new enhancers along the genome, or to understand the effect of enhancer mutations and their role in genetic diseases.We trained deep Convolutional Neural Networks (CNN) to identify enhancer sequences in multiple species. We used multiple biological datasets, including simulated sequences, in vivo binding data of single transcription factors and genome-wide chromatin maps of active enhancers in 17 mammalian species. Our deep networks obtained high classification accuracy by combining two training strategies: First, training on enhancers vs. non-enhancer background sequences, we identified short (1-4bp) low-complexity motifs. Second, by replacing the negative training set by adversarial k-order random shuffles of enhancer sequences (thus maintaining base composition while shuttering longer motifs, including transcription factor binding sites), we identified a set of biologically meaningful motifs, unique to enhancers. In addition, classification performance improved when combining positive data from all species together, showing a shared mammalian regulatory architecture.Our results demonstrate that design of adversarial training data, and transfer of learned parameters between networks trained on different species/datasets improve the overall performance and capture biologically meaningful information in the parameters of the learned network.Contact: or.zuk{at}mail.huji.ac.il, tommy{at}cs.huji.ac.il},
	URL = {https://www.biorxiv.org/content/early/2018/02/14/264200},
	eprint = {https://www.biorxiv.org/content/early/2018/02/14/264200.full.pdf},
	journal = {bioRxiv}
}


@article {martinek2022,
	author = {Martinek, Vlastimil and Cechak, David and others},
	title = {Fine-Tuning Transformers For Genomic Tasks},
	elocation-id = {2022.02.07.479412},
	year = {2022},
	doi = {10.1101/2022.02.07.479412},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Transformers are a type of neural network architecture that has been successfully used to achieve state-of-the-art performance in numerous natural language processing tasks. However, what about DNA, the language life written in the four-letter alphabet? In this paper, we review the current state of Transformers usage in genomics and molecular biology in general, introduce a collection of benchmark datasets for the classification of genomic sequences, and compare the performance of several model architectures on those benchmarks, including a BERT-like model for DNA sequences DNABERT as implemented in HuggingFace (armheb/DNA_bert_6 model). In particular, we explore the effect of pre-training on a large DNA corpus vs training from scratch (with randomized weights). The results presented here can be used for identification of functional elements in human and other genomes.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/02/10/2022.02.07.479412},
	eprint = {https://www.biorxiv.org/content/early/2022/02/10/2022.02.07.479412.full.pdf},
	journal = {bioRxiv}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{peters-etal-2019-knowledge,
    title = "Knowledge Enhanced Contextual Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      others",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    url = "https://aclanthology.org/D19-1005",
    doi = "10.18653/v1/D19-1005",
    pages = "43--54",
    abstract = "Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert{'}s runtime is comparable to BERT{'}s and it scales to large KBs.",
}

@inproceedings{van-de-cruys-2011-two,
    title = "Two Multivariate Generalizations of Pointwise Mutual Information",
    author = "Van de Cruys, Tim",
    booktitle = "Proceedings of the Workshop on Distributional Semantics and Compositionality",
    month = jun,
    year = "2011",
    url = "https://aclanthology.org/W11-1303",
    pages = "16--20",
}


@inproceedings{mosbach2021,
  author    = {Marius Mosbach and
               Maksym Andriushchenko and
               others},
  title     = {On the Stability of Fine-tuning {BERT:} Misconceptions, Explanations,
               and Strong Baselines},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=nzpLWnVAyah},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MosbachAK21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pantel2002,
author = {Pantel, Patrick and Lin, Dekang},
title = {Discovering Word Senses from Text},
year = {2002},
isbn = {158113567X},
doi = {10.1145/775047.775138},
abstract = {Inventories of manually compiled dictionaries usually serve as a source for word senses. However, they often include many rare senses while missing corpus/domain-specific senses. We present a clustering algorithm called CBC (Clustering By Committee) that automatically discovers word senses from text. It initially discovers a set of tight clusters called committees that are well scattered in the similarity space. The centroid of the members of a committee is used as the feature vector of the cluster. We proceed by assigning words to their most similar clusters. After assigning an element to a cluster, we remove their overlapping features from the element. This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses. Each cluster that a word belongs to represents one of its senses. We also present an evaluation methodology for automatically measuring the precision and recall of discovered senses.},
booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {613–619},
numpages = {7},
keywords = {clustering, word sense discovery, evaluation, machine learning},
location = {Edmonton, Alberta, Canada},
series = {KDD '02}
}

@article{sun2019ernie,
  title={Ernie: Enhanced representation through knowledge integration},
  author={Sun, Yu and Wang, Shuohuan and others},
  journal={arXiv preprint arXiv:1904.09223},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@Article{Liang2012,
author={Liang, Wang},
title={Segmenting DNA sequence into words based on statistical language model},
journal={Nature Precedings},
year={2012},
month={Feb},
day={27},
abstract={This paper presents a novel method to segment/decode DNA sequences based on n-gram statistical language model. Firstly, we find the length of most DNA ``words'' is 12 to 15 bps by analyzing the genomes of 12 model species. The bound of language entropy of DNA sequence is about 1.5674 bits. After building an n-gram biology languages model, we design an unsupervised `probability approach to word segmentation' method to segment the DNA sequences. The benchmark of segmenting method is also proposed. In cross segmenting test, we find different genomes may use the similar language, but belong to different branches, just like the English and French/Latin. We present some possible applications of this method at last.},
issn={1756-0357},
doi={10.1038/npre.2012.6939.1},
url={https://doi.org/10.1038/npre.2012.6939.1}
}


@inproceedings{kyono:2019;mlhc:MultiViewMammography,
  author    = {Trent Kyono and
               Fiona J. Gilbert and
               others},
  title     = {Multi-view Multi-task Learning for Improving Autonomous Mammogram
               Diagnosis},
  booktitle = {Proceedings of the Machine Learning for Healthcare Conference, {MLHC}
               2019},
  series    = {Proceedings of Machine Learning Research},
  volume    = {106},
  pages     = {571--591},
  publisher = {{PMLR}},
  year      = {2019},
  url       = {http://proceedings.mlr.press/v106/kyono19a.html},
  timestamp = {Fri, 27 Nov 2020 15:17:46 +0100},
  biburl    = {https://dblp.org/rec/conf/mlhc/KyonoGS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{shaikhina:2017:aimed:SmallDataMLinMedicine,
  author    = {Torgyn Shaikhina and
               Natasha A. Khovanova},
  title     = {Handling limited datasets with neural networks in medical applications:
               {A} small-data approach},
  journal   = {Artif. Intell. Medicine},
  volume    = {75},
  pages     = {51--63},
  year      = {2017},
  url       = {https://doi.org/10.1016/j.artmed.2016.12.003},
  doi       = {10.1016/j.artmed.2016.12.003},
  timestamp = {Wed, 27 Jul 2022 22:16:23 +0200},
  biburl    = {https://dblp.org/rec/journals/artmed/ShaikhinaK17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Hekler:2019:bmcmedicine:smallDataParadigm,
  title={Why we need a small data paradigm},
  author={Eric B. Hekler and Predrag V. Klasnja and others},
  journal={BMC Medicine},
  year={2019},
  volume={17}
}

@article{oei:2021:precisionmedicine:domainKnowledgePatientSimilarity,
    AUTHOR = {Oei, Ronald Wihal and Fang, Hao Sen Andrew and Tan, Wei-Ying and Hsu, Wynne and Lee, Mong-Li and Tan, Ngiap-Chuan},
    TITLE = {Using Domain Knowledge and Data-Driven Insights for Patient Similarity Analytics},
    JOURNAL = {Journal of Personalized Medicine},
    VOLUME = {11},
    YEAR = {2021},
    NUMBER = {8},
    ARTICLE-NUMBER = {699},
    URL = {https://www.mdpi.com/2075-4426/11/8/699},
    ISSN = {2075-4426},
    ABSTRACT = {Patient similarity analytics has emerged as an essential tool to identify cohorts of patients who have similar clinical characteristics to some specific patient of interest. In this study, we propose a patient similarity measure called D3K that incorporates domain knowledge and data-driven insights. Using the electronic health records (EHRs) of 169,434 patients with either diabetes, hypertension or dyslipidaemia (DHL), we construct patient feature vectors containing demographics, vital signs, laboratory test results, and prescribed medications. We discretize the variables of interest into various bins based on domain knowledge and make the patient similarity computation to be aligned with clinical guidelines. Key findings from this study are: (1) D3K outperforms baseline approaches in all seven sub-cohorts; (2) our domain knowledge-based binning strategy outperformed the traditional percentile-based binning in all seven sub-cohorts; (3) there is substantial agreement between D3K and physicians (κ = 0.746), indicating that D3K can be applied to facilitate shared decision making. This is the first study to use patient similarity analytics on a cardiometabolic syndrome-related dataset sourced from medical institutions in Singapore. We consider patient similarity among patient cohorts with the same medical conditions to develop localized models for personalized decision support to improve the outcomes of a target patient.},
    DOI = {10.3390/jpm11080699}
}

@inproceedings{schick-schutze-2021-exploiting,
    title = "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    url = "https://aclanthology.org/2021.eacl-main.20",
    doi = "10.18653/v1/2021.eacl-main.20",
    pages = "255--269",
    abstract = "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with {``}task descriptions{''} in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.",
}

@inproceedings{schick-schutze-2021-just,
    title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    url = "https://aclanthology.org/2021.naacl-main.185",
    doi = "10.18653/v1/2021.naacl-main.185",
    pages = "2339--2352",
    abstract = "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much {``}greener{''} in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
}

@inproceedings{zhang2021,
  author    = {Tianyi Zhang and
               Felix Wu and
               Arzoo Katiyar and
               Kilian Q. Weinberger and
               Yoav Artzi},
  title     = {Revisiting Few-sample {BERT} Fine-tuning},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=cO1IH43yUF},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/0007WKWA21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{keskar2017,
  author    = {Nitish Shirish Keskar and
               Dheevatsa Mudigere and
               others},
  title     = {On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,  Conference Track Proceedings},
  year      = {2017},
  url       = {https://openreview.net/forum?id=H1oyRlYgg},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KeskarMNST17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hoffer2017,
 author = {Hoffer, Elad and Hubara, Itay and others},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
 url = {https://proceedings.neurips.cc/paper/2017/file/a5e0ff62be0b08456fc7f1e88812af3d-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{helden2000,
    author = {Helden, Jacques van and Rios, Alma. F. and Collado-Vides, Julio},
    title = "{Discovering regulatory elements in non-coding sequences by analysis of spaced dyads}",
    journal = {Nucleic Acids Research},
    volume = {28},
    number = {8},
    pages = {1808-1818},
    year = {2000},
    month = {04},
    abstract = "{The application of microarray and related technologies is currently generating a systematic catalog of the transcriptional response of any single gene to a multiplicity of experimental conditions. Clustering genes according to the similarity of their transcriptional response provides a direct hint to the regulons of the different transcription factors, many of which have still not been characterized. We have developed a new method for deciphering the mechanism underlying the common transcriptional response of a set of genes, i.e. discovering cis-acting regulatory elements from a set of unaligned upstream sequences. This method, called dyad analysis, is based on the observation that many regulatory sites consist of a pair of highly conserved trinucleotides, spaced by a non-conserved region of fixed width. The approach is to count the number of occurrences of each possible spaced pair of trinucleotides, and to assess its statistical significance. The method is highly efficient in the detection of sites bound by C6 Zn2 binuclear cluster proteins, as well as other transcription factors. In addition, we show that the dyad and single-word analyses are efficient for the detection of regulatory patterns in gene clusters from DNA chip experiments. In combination, these programs should provide a fast and efficient way to discover new regulatory sites for as yet unknown transcription factors.}",
    issn = {0305-1048},
    doi = {10.1093/nar/28.8.1808},
    url = {https://doi.org/10.1093/nar/28.8.1808},
    eprint = {https://academic.oup.com/nar/article-pdf/28/8/1808/9905065/281808.pdf},
}

@Manual{rgadem,
    title = {rGADEM: de novo motif discovery},
    author = {Arnaud Droit and Raphael Gottardo and Gordon Robertson and Leiping Li},
    year = {2022},
    note = {R package version 2.44.1},
  }
  
@misc{human-reference,
    title= {Genome Reference Consortium Human Build 38 patch release 13 (GRCh38.p13)},
    author={Genome Reference Consortium},
    url= {https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39/},
    year= {2019},
}

@inproceedings{roy2021,
author = {Roy, Soumyadeep and Chakraborty, Sudip and Mandal, Aishik and Balde, Gunjan and Sharma, Prakhar and Natarajan, Anandhavelu and Khosla, Megha and Sural, Shamik and Ganguly, Niloy},
title = {Knowledge-Aware Neural Networks for Medical Forum Question Classification},
year = {2021},
doi = {10.1145/3459637.3482128},
abstract = {Online medical forums have become a predominant platform for answering health-related information needs of consumers. However, with a significant rise in the number of queries and the limited availability of experts, it is necessary to automatically classify medical queries based on a consumer's intention, so that these questions may be directed to the right set of medical experts. Here, we develop a novel medical knowledge-aware BERT-based model (MedBERT) that explicitly gives more weightage to medical concept-bearing words, and utilize domain-specific side information obtained from a popular medical knowledge base. We also contribute a multi-label dataset for the Medical Forum Question Classification (MFQC) task. MedBERT achieves state-of-the-art performance on two benchmark datasets and performs very well in low resource settings.},
booktitle = {Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management},
pages = {3398–3402},
numpages = {5},
keywords = {online health communities, clinical text classification},
location = {Virtual Event, Queensland, Australia},
series = {CIKM '21}
}

@article{schneider1990,
    author = {Schneider, Thomas D. and Stephens, R.Michael},
    title = "{Sequence logos: a new way to display consensus sequences}",
    journal = {Nucleic Acids Research},
    volume = {18},
    number = {20},
    pages = {6097-6100},
    year = {1990},
    month = {10},
    abstract = "{A graphical method is presented for displaying the patterns in a set of aligned sequences. The characters representing the sequence are stacked on top of each other for each position in the aligned sequences. The height of each letter is made proportional to Its frequency, and the letters are sorted so the most common one is on top. The height of the entire stack is then adjusted to signify the information content of the sequences at that position. From these ‘sequence logos’, one can determine not only the consensus sequence but also the relative frequency of bases and the information content (measured In bits) at every position in a site or sequence. The logo displays both significant residues and subtle sequence patterns.}",
    issn = {0305-1048},
    doi = {10.1093/nar/18.20.6097},
    url = {https://doi.org/10.1093/nar/18.20.6097},
    eprint = {https://academic.oup.com/nar/article-pdf/18/20/6097/3939026/18-20-6097.pdf},
}

@article{chow1991,
title = {Characterization of a novel liver-specific enhancer in the human prothrombin gene.},
journal = {Journal of Biological Chemistry},
volume = {266},
number = {28},
pages = {18927-18933},
year = {1991},
issn = {0021-9258},
doi = {https://doi.org/10.1016/S0021-9258(18)55152-8},
url = {https://www.sciencedirect.com/science/article/pii/S0021925818551528},
author = {B.K. Chow and V. Ting and others},
abstract = {The 5'-flanking sequence of the human prothrombin gene was isolated by screening a human liver phage library with a human prothrombin cDNA as a hybridization probe. A phage was identified that contained 3 kilobase pairs of DNA upstream of the initiator methionine codon. Primer extension studies showed that the major transcription initiation sites were located 23 and 36 base pairs upstream of the initiator codon. DNA sequences in the 5'-flanking region of the human prothrombin gene were then analyzed for cis-activating transcriptional activity by a transient expression system using the human growth hormone gene as the reporter gene. The chimeric expression vector was introduced into HepG2 cells, and secreted human growth hormone was monitored by using a radio-immunoassay. These studies showed that the 3-kilo-base pair fragment contained sequences that were sufficient for the initiation of transcription in HepG2 cells. Subsequent deletion studies showed that the 3-kilobase pair fragment contained two elements: a weak promoter in the region immediately upstream of the mRNA coding sequence and an enhancer located between nucleotides -860 and -940. The enhancer element was active at a distance and in either orientation. In addition, the enhancer was liver cell-specific and acted on heterologous promoters including the herpes simplex virus thymidine kinase promoter and the mouse metallothionein I promoter. Comparison of the nucleotide sequence of the enhancer with a DNA sequence data base showed the enhancer sequence to be unique. The enhancer sequence is flanked by an inverted repeat 5' CCTCCC 3' and contains a putative binding site for hepatic nuclear factor 1. Deoxyribonuclease I footprint analysis and linker scanning mutagenesis showed that the enhancer contains multiple protein binding motifs. Mutagenesis of the 3' boundary CCTCCC sequence eliminated the enhancer activity. Comparison with other liver genes showed the presence of the CCTCCC sequence in the hepatitis B virus enhancer, the alpha 1-antitrypsin promoter, and the fibrinogen beta-chain promoter, suggesting a functional role for this motif.}
}

@inproceedings{liu2020,
  author    = {Liyuan Liu and
               Haoming Jiang and
               others},
  title     = {On the Variance of the Adaptive Learning Rate and Beyond},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020},
  year      = {2020},
  url       = {https://openreview.net/forum?id=rkgz2aEKDr},
  timestamp = {Mon, 30 May 2022 13:48:58 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LiuJHCLG020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@InProceedings{xiong2020,
  title = 	 {On Layer Normalization in the Transformer Architecture},
  author =       {Xiong, Ruibin and Yang, Yunchang and others},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10524--10533},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/xiong20b.html},
  abstract = 	 {The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.}
}

@misc{EPD,
  key={Eukaryotic Promoter Database},
  title={EPD Selection Tool of EPD Eukaryotic Promoter Database},
  url={https://epd.epfl.ch/human/human_database.php?db=human},
  year={2022},
}

@misc{DNABert-Codebase,
  key={DNABert Github Repository},
  title={DNABert Github Repository},
  url={https://github.com/jerryji1993/DNABERT},
  year={2019},
}

@Article{haeseleer2006,
author={D'haeseleer, Patrik},
title={What are DNA sequence motifs?},
journal={Nature Biotechnology},
year={2006},
month={Apr},
day={01},
volume={24},
number={4},
pages={423-425},
abstract={Sequence motifs are becoming increasingly important in the analysis of gene regulation. How do we define sequence motifs, and why should we use sequence logos instead of consensus sequences to represent them? Do they have any relation with binding affinity? How do we search for new instances of a motif in this sea of DNA?},
issn={1546-1696},
doi={10.1038/nbt0406-423},
url={https://doi.org/10.1038/nbt0406-423}
}




@article{gresova2022genomic,
  title={Genomic Benchmarks: A Collection of Datasets for Genomic Sequence Classification},
  author={Gresova, Katarina and Martinek, Vlastimil and others},
  journal={bioRxiv},
  year={2022},
  publisher={Cold Spring Harbor Laboratory},
  url={https://www.biorxiv.org/content/10.1101/2022.06.08.495248}
}

@article{dbsnp2001,
    author = {Sherry, S. T. and Ward, M.-H. and others},
    title = "{dbSNP: the NCBI database of genetic variation}",
    journal = {Nucleic Acids Research},
    volume = {29},
    number = {1},
    pages = {308-311},
    year = {2001},
    month = {01},
    abstract = "{In response to a need for a general catalog of genome variation to address the large-scale sampling designs required by association studies, gene mapping and evolutionary biology, the National Center for Biotechnology Information (NCBI) has established the dbSNP database [S.T.Sherry, M.Ward and K.Sirotkin (1999) Genome Res., 9, 677–679]. Submissions to dbSNP will be integrated with other sources of information at NCBI such as GenBank, PubMed, LocusLink and the Human Genome Project data. The complete contents of dbSNP are available to the public at website: http://www.ncbi.nlm.nih.gov/SNP. The complete contents of dbSNP can also be downloaded in multiple formats via anonymous FTP at ftp://ncbi.nlm.nih.gov/snp/.}",
    issn = {0305-1048},
    doi = {10.1093/nar/29.1.308},
}

@article{clinvar2013,
    author = {Landrum, Melissa J. and Lee, Jennifer M. and others},
    title = "{ClinVar: public archive of relationships among sequence variation and human phenotype}",
    journal = {Nucleic Acids Research},
    volume = {42},
    number = {D1},
    pages = {D980-D985},
    year = {2013},
    month = {11},
    abstract = "{ClinVar (http://www.ncbi.nlm.nih.gov/clinvar/) provides a freely available archive of reports of relationships among medically important variants and phenotypes. ClinVar accessions submissions reporting human variation, interpretations of the relationship of that variation to human health and the evidence supporting each interpretation. The database is tightly coupled with dbSNP and dbVar, which maintain information about the location of variation on human assemblies. ClinVar is also based on the phenotypic descriptions maintained in MedGen (http://www.ncbi.nlm.nih.gov/medgen). Each ClinVar record represents the submitter, the variation and the phenotype, i.e., the unit that is assigned an accession of the format SCV000000000.0. The submitter can update the submission at any time, in which case a new version is assigned. To facilitate the evaluation of the medical importance of each variant, ClinVar aggregates submissions with the same variation/phenotype combination, adds value from other NCBI databases, assigns a distinct accession of the format RCV000000000.0 and reports if there are conflicting clinical interpretations. Data in ClinVar are available in multiple formats, including html, download as XML, VCF or tab-delimited subsets. Data from ClinVar are provided as annotation tracks on genomic RefSeqs and are used in tools such as Variation Reporter (http://www.ncbi.nlm.nih.gov/variation/tools/reporter), which reports what is known about variation based on user-supplied locations.}",
    issn = {0305-1048},
    doi = {10.1093/nar/gkt1113},
}

@inproceedings{sadeq-etal-2022-informask,
    title = "{I}nfor{M}ask: Unsupervised Informative Masking for Language Model Pretraining",
    author = "Sadeq, Nafis  and
      Xu, Canwen  and
      McAuley, Julian",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.395",
    pages = "5866--5878",
    abstract = "Masked language modeling is widely used for pretraining large language models for natural language understanding (NLU). However, random masking is suboptimal, allocating an equal masking rate for all tokens. In this paper, we propose InforMask, a new unsupervised masking strategy for training masked language models. InforMask exploits Pointwise Mutual Information (PMI) to select the most informative tokens to mask. We further propose two optimizations for InforMask to improve its efficiency. With a one-off preprocessing step, InforMask outperforms random masking and previously proposed masking strategies on the factual recall benchmark LAMA and the question answering benchmark SQuAD v1 and v2.",
}


@ARTICLE{Bouma2009mp,
  title     = "Normalized (pointwise) mutual information in collocation
               extraction",
  author    = "Bouma, Gerlof",
  journal   = "Proceedings of GSCL",
  publisher = "Potsdam",
  volume    =  30,
  pages     = "31--40",
  year      =  2009
}

@misc{yang2023learning,
      title={Learning Better Masking for Better Language Model Pre-training}, 
      author={Dongjie Yang and Zhuosheng Zhang and Hai Zhao},
      year={2023},
      eprint={2208.10806},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}