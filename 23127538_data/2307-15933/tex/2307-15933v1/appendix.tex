\section{Appendix}
In this section, we provide the supplementary material associated with the paper. 

\subsection{Ethics Statement}
The gene sequence data used for pretraining and finetuning is obtained from publicly available sources and can be obtained directly without signing any explicit data use agreement. The three benchmark datasets of Prom-core, Prom-300, and Cohn-enh are also used in previous studies for the task of gene sequence classification~\cite{bigbird2020,DNABert,martinek2022,deepromoter2019}. Our work does not involve patient-level data for the experiments.  We do not foresee any negative social impacts of this work, but of course, the accumulation of improvements in ML could be misused as it may give more power to nefarious agents.

\subsection{Background}
\noindent \textbf{Importance of understanding gene regulatory code.}
The long strands of DNA found in the human chromosomes can be classified into genes, and the genes, in turn, comprise \textit{coding} and \textit{non-coding} parts. A \textit{coding} part encapsulates the information required for converting the nucleotide to a \textit{protein}. These proteins are the building blocks of all tissues. These genes interact with the non-coding regions which perform gene regulation. \textit{Promoters/Enhancers} speed up the process of coding, \textit{inhibitors} slow down the reaction. These non-coding genes are called \textit{gene regulatory elements}\cite{genomics-survey}. The non-coding regions, accounting for over $98\%$ of the whole genome, implement significant yet largely unknown regulatory functions. Recent large consortia projects, including the ENCyclopedia of DNA Elements (ENCODE)~\cite{encode2012integrated}, Roadmap Epigenomics~\cite{kundaje2015integrative}, and the Genomics of Gene Regulation (GGR), have produced a large number of experimental mapping readouts to help annotate non-coding genome in specific tissues or cell-lines. On the other hand, Genome-wide association studies (GWAS) have discovered that the vast majority ($>90\%$) of associated genome loci for complex diseases and traits fall in non-coding regions~\cite{visscher2017}.

\noindent \textbf{Learning deep representations in the context of gene sequence modeling.}
 Nguyen et al.~\cite{nguyen2016dna} encode base pair triples as one-hot vectors to feed into convolutional neural networks for DNA sequence classification tasks, whereas Badirli et al.~\cite{Badirli2021} convert the DNA barcodes represented by nucleotide sequences into a vector embedding useful for the task of fine-grained species classification.

\noindent \textbf{MLM training.}
\cite{yamaguchi-etal-2021-frustratingly} explore alternative pretraining tasks compared to MLM, such as shuffled word detection, random word detection, manipulated word detection (Shuffle + Random), masked token type classification, and masked first character prediction. Here, we choose the original DNABert configuration of MLM without Next Sentence Prediction and experiment with multiple masked token selection strategies. 

\subsection{Methodology}

\subsubsection{Gene-PMI-VOC model: Limitations of Trivial Adaptation of PMI Masking to Gene Sequence}\label{sec:genepmivoc-details}

We initially developed the closest adaptation of the standard PMI masking~\cite{pmi-masking} approach used in NLP to the gene sequence classification setup, which involved significant human effort from our end. We consider it as a baseline model (Gene-PMI-VOC) for our experiments. We describe the working of the model in detail in Algorithm~\ref{algo:genepmivoc}. We next describe the various drawbacks of the \textit{Gene-PMI-VOC} model: 

\begin{itemize}
    \item  \textbf{Determining the masking vocabulary size:} Levine et al.~\cite{pmi-masking} use a small-scale evaluation of an n-gram's collocation quality as a function of its rank. They create an ad-hoc dataset composed of 1000 n-grams that they manually label as the collocated terms or not (for more details, please see Section A of the Appendix of~\cite{pmi-masking}). Creating such an ad-hoc dataset in the gene sequence modeling setting is difficult because: (i) Gene sequences are continuous, and such semantically-meaning unit (like words or tokens in NLP) is not available in the genomic domain. (ii) The manual annotation process will require medical domain experts' involvement and, thus, a significant human effort.

   \item The standard PMI formulation~\cite{pmi-masking} \textbf{favors tokens with a low frequency of occurrence}. This significantly reduces the efficacy of PMI masking and leads to the wastage of PMI masking-based pretraining steps because, during the MLM training stage, such high PMI tokens will rarely appear as masked tokens (given their low frequency of occurrence). 
    
\end{itemize}

\SetCommentSty{mycommfont}
\SetKwInput{KwInput}{Input}      % Set the Input
\SetKwInput{KwOutput}{Output}    % set the Output
\SetKwInput{KwData}{Initialization}

\LinesNumberedHidden

\begin{algorithm}[h]
\footnotesize
\caption{Gene-PMI-VOC Algorithm.}\label{algo:genepmivoc}

\DontPrintSemicolon \;
    \KwInput{DNA string made of 6-mer tokens with a maximum length of 510, $MaskVocab$: Fixed Masking Vocabulary composed of k-mers where $1 \leq k \leq 10$, $m$: MLM probability = $0.025$ }
    \KwOutput{$MaskTokenSet$: List of tokens to mask in the DNA string}
    \KwData{
    
    \tcp{P[i] denotes the PMI-token at $i$-th position in the DNA segment, DNA[i] denotes the $i$-th nucleotide, $p =$ maximum input sequence length * MLM probability $= 512 * 0.025 = 12.8$ on average, $MaskVocab$ size $= 40000$, $PmiTokenizedString$: stores the DNA input sequence tokenized by PMI tokenizer}

    $MaskTokenSet \gets \emptyset$

    $PmiTokenizedString$ $\gets \emptyset$
  
    }
	\SetKwFunction{FMain}{MapNucleotideToKmerTokens}
	\SetKwProg{Fn}{Function}{:}{}
	
	\Fn{\FMain{nucleotide position id $i$}}{
            
            $MappedTokens$ $\gets$ T[$j$],  $\forall(j)_{j=i - 2}^{i + 3}$ \;
			
			\KwRet\  $MappedTokens$ \; 
    }

    \textbf{Step 1:} Tokenize the input DNA string into PMI tokens obtained from the fixed vocabulary $MaskVocab$. Select the 10-mer from the start of the input DNA string $s$ and check whether it is in $MaskVocab$. If found true, add it to $PmiTokenizedString$, move to the end of the 10-mer, and repeat the process until the end of $s$ is reached. If found false, repeat the same thing with 9-mer, 8-mer, ..., and 1-mer consecutively until there is a positive match with $MaskVocab$.

    \textbf{Step 2:} Randomly select $p$ nucleotides as mask centers (MC) spread uniformly over the DNA string.
            
    \textbf{Step 3:}
    \For{each nucleotide in $p$ mask centers}
	{
            $PositionId$ := Token index of nucleotide on the DNA segment given as input
            
            $PmiNucleotides$ := Using $PmiTokenizedString$, identify the PMI gene sequence corresponding to the nucleotide at $PositionId$)

            \For{each nucleotide $d$ in $PmiNucleotides$}
            {
                $MaskTokenSet$ $\gets$ $MaskTokenSet$~$\cup$~ $MapNucleotideToKmerTokens$ ($d$)
                }
                     
	} 

     \KwRet\  $MaskTokenSet$ \;
\end{algorithm}
  

\begin{table}
\centering
\small
\begin{tabular}{cc}
\hline
Sequence Length  & Total tokens in vocabulary  \\ \hline
1  & 5  \\
2  & 17  \\
3  & 65  \\
4 & 212  \\
5  & 533  \\
6 & 1465    \\
7   & 3829   \\
8   & 10271   \\
9& 17537   \\
10    & 6071  \\ \hline
\end{tabular}
\caption{Masking vocabulary statistics of Gene-PMI-VOC baseline model}
\label{tab:pmi-vocab-dist}
\end{table}


\subsubsection{Normalized PMI\textsubscript{k} Metric Details.} Here, we provide more implementation details for the Normalized \textit{PMI\textsubscript{k}} (N-PMI) as defined in Equation~\ref{eq:norm-pmi}. We choose the minimum frequency of occurrence threshold $c$ as $101$, which puts a cut-off beyond the first quartile ($25$ percentile) of k-mer frequencies. The k-mer frequency distribution follows a long-tailed distribution; hence the number of tokens below 101 is very low. 

\subsection{Experimental Setup: Training Details}\label{sec:appendex-model-parameter}

\noindent \textbf{Finetuning parameter configuration.} The models are finetuned for $20$ epochs at a learning rate of $4e^{-4}$, warmup steps percentage of $10\%$, hidden dropout probability of $0.1$, weight decay as $0.01$, per GPU train batch size as $5$ and use the \textit{AdamW} optimizer for the 10, 50 and 100-shot setting. However, the performance drops in 500 and 1000-shot settings due to overfitting. Therefore, for the 500-shot settings and above, we use the same hyperparameters as the original DNABert paper --- a lower learning rate from $4e^{-4}$ to $5e^{-5}$ and a lower number of epochs from 20 to 5. Mosbach et al.~\cite{mosbach2021} observe that such a high number of fine-tuning epochs helps address random initialization issues in the low-resource settings (10, 50, and 100-shot). 

\begin{table}[!ht]
    \centering
    \begin{tabular}{ccc}
    \hline
        \textbf{Parameter} & \textbf{DNABert} & \textbf{LOGO} \\
        \hline
        Hidden Size & 768 & 256 \\
        Hidden Layers & 12 & 2 \\
        Attention Heads & 12 & 8 \\
       Per GPU train batch size & 10 & 5 \\
        Hidden Dropout Probability & 0.1 & 0 \\
        Attention Dropout Probability & 0.1 & 0 \\
        Intermediate Size & 3072 &  3072 \\
        Embedding Size & 512 & 512 \\ \hline
 %       Parameter Size & 100M & 1M \\ 
    \end{tabular}
    \caption{Difference between parameters of DNABert and LOGO}
    \label{tab:dna-logo-config}
\end{table}

\subsection{Experimental Results: Memorization versus Generalization}\label{sec:expt-results}

% Figure environment removed

%\subsubsection{Memorization versus Generalization.}
It was observed in Radford et al.~\cite{radford2019language} that data overlap between WebText training data and specific evaluation datasets like Winograd Schema Challenge, CoQA, and LAMBADA, provides a small but consistent improvement to model performance; this indicates that the model generalization results suffer from severe over-reporting. Instead of using a probabilistic data structure like Bloom filters as used by Radford et al.~\cite{radford2019language}, we use a deterministic method based on sets and hashing to compute the exact overlap between the pretraining data with task-specific test data and between task-specific train and task data. Specifically, we compute the percentage of $15$-grams (a subsequence of k-mers of length $15$) that are common between the dataset pairs. This leads to $185.4$ million unique $15$-grams for the pretraining data (based on the Human Reference Genome), which covers $17.26\%$ of all possible $15$-length combinations. The overlap results are presented in Table~\ref{tab:data-overlap}. 

\ul{We observe that the task-specific train and test data overlap is very low (median overlap of $\leq 5.2\%$) for all the datasets except for Prom-300 (median overlap of $31.9\%$).} To investigate the contribution to model performance due to memorization, we remove test data points with more than $50\%$ overlap or top five percentile data points in terms of the highest overlap ratio. We observe that for all tasks except Splice-40, there is a marginal drop in performance %(the highest percentage drop is for Splice-400, where the accuracy dropped from $0.809$ to $0.803$, equal to $0.74\%$ drop)
whereas, for Splice-40, performance improves marginally (please see the first and second rows of Table~\ref{tab:perf-compare-10K}). \ul{We thus conclude that although there is some overlap between the task-specific train and test dataset, it does not contribute much to model performance.} Thus, memorization in DNABert architecture does not lead to over-reporting of generalization performance for the models discussed in this paper. Furthermore, we evaluate only in few-shot settings, where only a small portion of the training dataset %(maximum $100$ data points per class)
is used for finetuning.

\begin{table}
    \centering
    \small
    \begin{tabular}{ccc} \hline
        Task&Training data & Pretraining data \\ \hline
         Prom-core&0.018 (0.255)& 0.218 (0.655) \\ 
         Prom-300&0.319 (0.621) & 0.274 (0.621) \\ 
         Splice-40& 0.0 (0.24) & 0.052 (0.723) \\
         Cohn-enh & 0.569 (0.956)& 0.583 (0.953)\\ \hline
        
    \end{tabular}
    \caption{Overlap ratio of test dataset with pretraining data and training data. The median is reported along with the $95$ percentile value in brackets}
    \label{tab:data-overlap}
\end{table}



\subsection{Domain-specific Model Explainability: PMI-based rankings capture motifs present in fine-tuning datasets}
As motif discovery is computationally expensive, we only provide a subset of the data: randomly sampled 1000 (prom300 - $300$ bp and enhancers-cohn $500$ bp) and $2000$ (prom-core - 70 bp) data points. We present the complete list of matches and their corresponding ranks in the 6-mer PMI ranked list (\textit{RANK}) in Table~\ref{tab:appendix-motif-6-mer-ranking}, their corresponding consensus logos in Figures~\ref{fig:consensus-logos-1} and~\ref{fig:consensus-logos-2}.

\begin{table}
    \centering
    \scalebox{0.7}{
    \begin{tabular}{ccc}\hline
         Dataset&Motifs& Normalized PMI rank \\ && (Top 5 for 5-mers) (out of 4096) \\ \hline
         Prom-core&nCyy\textbf{CCTCC}n* &1, 11, 52, 175, 186  \\
         Prom-core&sCs\textbf{CCGCC}sCCn & 103, 1181, 1678, 2205, 2534 \\
         Prom-core&sCw\textbf{GCAGC}n & 259, 516, 540, 570, 628  \\
         Prom-core&yy\textbf{TTTATA}n & 286  \\
         Prom-core&n\textbf{TATAAA}r &242 \\
         Prom-core&n\textbf{GAGGAGG}v & AGGAGG (rank 56), GAGGAG (rank 278) \\
         Prom-core&k\textbf{GCTGC}wGs & 260, 510, 555, 590, 639 \\
         Cohn-enh&ks\textbf{CTGGG}m &5, 17, 20, 21, 71 \\
         Cohn-enh&n\textbf{CCTGGCC}h & CCTGGC (rank 25), CTGGCC (rank 129)  \\
         Cohn-enh&yy\textbf{CCAG}r\textbf{G}n & 302, 593, 1247, 2778 \\
         Cohn-enh&\textbf{TTTTTT}TTTn & 8 \\
         Cohn-enh&\textbf{GTGGCT}sw & 126 \\
         \hline
    \end{tabular}}
    \caption{PMI-based rankings based on NPMI\textsubscript{k} 
    score for the motifs present in finetuning datasets. The motifs are of lengths 5, 6, or 7. For length 7, we mention two rankings considering two 6-length sub-motifs. A motif of length five matches as a sub-string to multiple 6-mers; we only mention the top five ranks for all such matches.}
    \label{tab:appendix-motif-6-mer-ranking}
\end{table}

% Figure environment removed

% Figure environment removed



\subsubsection{Analyzing the effect of functional genetic variants}\label{sec:app-func-gene-variants} 
Here, we aim to test the extent of domain-specific knowledge learned by the \genemask-guided models over the original SOTA model (ORI). Therefore, instead of the gene sequence classification task, we investigate the model performance for the task of identifying functional genetic variants. We thus reproduce the variant analysis conducted by~\cite{DNABert}, using dbSNP~\cite{dbsnp2001} and ClinVar~\cite{clinvar2013}, to compare the performance of the \genemask{} model with the original SOTA model (ORI), when both models are finetuned on 10-shot Prom-core dataset. 

 $400,000$ variants were retrieved from dbSNP, and the original authors constructed the corresponding genomic sequences (both original and mutated). When the original and mutated sequences offer significantly different prediction probabilities with respect to promoter prediction, the variant is queried in ClinVar and other similar databases to ascertain their importance. Analyzing these queries established that the variants identified by the fine-tuned promoter model have \ul{interpretable uses and links to diseases or other functional aspects}. As a result, DNABERT demonstrated its capability to \ul{capture and propose new and significant (disease-specific) variants in the future.} 

\noindent \textbf{Implementation Details.} Since we do not have access to the specific code used by the authors to evaluate the importance of a given variant using Clinvar, we instead compare our model with the ORI 120K model variant of DNABert, which is also fine-tuned on the 10-shot Prom-core dataset (ORI 120K performs best in Prom-core low-resource setting) and obtain the original weights used by the authors~\cite{DNABert-Codebase}.  The differences in promoter prediction probabilities for the dataset mentioned above ($400,000$ data points) are recorded. Then, the sequences are ranked by non-increasing order of the absolute difference value. Using the published values given by the authors for ORI 120K promoter setting as ground truth (i.e., a proxy to identify all possible important functional variants), we compare a model's ranked list of variants. At this point, we obtain an individual ranked list of data points for the ORI and PMI \textit{few-shot} models. 

\noindent \textbf{Discussion of Results.} The degree of overlap (intersection of the two lists) with the ground-truth ranked list for multiple top-N settings for the ORI 10K and \genemask{} 10K setting is provided in Figure~\ref{fig:variant-perf-comp}. We observe that \genemask-guided \ul{DNABert consistently reports higher overlap over the original model (ORI) in different top-N settings ($5000 \leq N \leq 50000$)}. Thus, we conclude that \genemask{} helps incorporate intrinsic (or relevant) genomics information into DNABert. 

 % Figure environment removed


