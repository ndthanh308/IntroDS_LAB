\section{Building blocks of SOTA models}\label{sec:dnabert-config}

We focus on the two SOTA transformer-based pretrained models named DNABert~\cite{DNABert} and LOGO~\cite{yang:2021:bioarxiv:logo} that are adapted to the gene sequence modeling domain. Through MLM pretraining, these models learn powerful contextual representations for DNA fragments utilizing abundant unlabeled data from the \textit{Human Reference Genome}, which contains around 3.2 billion base pairs over 24 chromosomes. 

\noindent \textbf{Implementation details of SOTA models and associated research challenges.} 
The tokenization of gene sequences and MLM training is performed based on the author's codebase~\cite{DNABert-Codebase}. %\footnote{\url{https://github.com/jerryji1993/DNABERT}}.
However, they do not provide the dataset for pretraining or finetuning (downstream) tasks. Therefore, we follow the author's description to construct the corresponding datasets, which is nontrivial. We explain the pretraining data creation process in this section and later describe the finetuning dataset creation process in Section~\ref{sec:datasets}. 
%For LOGO, we only utilize the model configurations provided by the author's codebase\footnote{\url{https://github.com/melobio/LOGO}} and use the same dataset and training setup as DNABert with the only difference that LOGO is a much lightweight model than DNABert (2 self-attention blocks versus 12 of DNABert). The remaining model differences are compared in Table~\ref{tab:dna-logo-config} of Appendix.

\noindent \textbf{Tokenization of gene sequences.} The gene sequence is first converted into a k-mer representation, which is commonly used in the literature~\cite{DNABert,dnavec}. The $k$-mer representation is a sliding window of length $k$. For example, \textit{AGCACGCAG} in 6-mer representation leads to 3 tokens - \textit{AGCACG}, \textit{GCACGA}, \textit{CACGAG}. The vocabulary comprises all combinations ($4^k$ length) and five special tokens - \textit{CLS, PAD, UNK, SEP, MASK}. According to the set-up chosen by SOTA models, we consider $k = 6$ for all the experiments, as it also provides a good trade-off between longer contextual information and manageable computational complexity~\cite{yang:2021:bioarxiv:logo}. %\tblu{because it is the longest length k-mer used by both the SOTA models and thus helps to incorporate richer contextual information.
As stated in the LOGO paper~\cite{yang:2021:bioarxiv:logo}, 6-mers incorporate richer contextual information while keeping the memory and computational complexity manageable.  %because of the increase in vocabulary size (four times the 6-mer vocabulary).}   %The token vocabulary size is $4^6+5=~4101$ in our case.~\cite{yang:2021:bioarxiv:logo} further supported such a k-mer representation from a biological standpoint, stating that each nucleotide is not independent such as codon rules in the coding region and regulatory motifs in the non-coding region. Therefore, instead of operating on base pairs, the DNA sequence is segmented into tokens further to increase the context length~\cite{bigbird2020,Liang2012}.  
% Figure environment removed

\noindent \textbf{Pretraining data preparation.} %Since the pretraining data is not provided with the author's codebase, we follow the implementation details mentioned in the paper to construct the pretraining dataset, which is a non-trivial task. 
We obtain the Human Reference Genome from the Genome Reference Consortium Human Build 38 patch release 13 (GRCh38.p13) FASTA file~\cite{human-reference} from the NCBI website. % with RefSeq assembly accession id as GCF\_000001405.39 from NCBI website\footnote{\url{https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39/}}.
It serves as a large-scale corpus of unlabeled gene sequence data, which we use for MLM training to obtain a contextual representation of the 6-mer tokens. We perform the following steps to convert the Human Reference Genome to a form that DNABert (or LOGO) can use to train with the MLM objective: \\
%\noteng{does not}\notesr{Corrected to change ours to a third person except for the pretraining data preparation part, highlighting the non-trivial nature of the SOTA model setup.}
    {\bf (a).} For each chromosome $c$ in the Human Reference Genome, we randomly choose the starting index between $1$ and $1000$~\cite{bigbird2020}. \\
%It is not clear in the DNABert documentation how to select the starting point for a given chromosome. Therefore, we follow the strategy proposed by~\cite{bigbird2020} where it is explicitly mentioned.
    {\bf (b).} Given the chromosome number and its starting index ($ST$), we next determine the length of the DNA segment $L$ as BERT has the limitation of accommodating a maximum of $512$ tokens. We select $L$ as $510$ for $50\%$ of the cases and a randomly selected length between $5$ and $510$ for the remaining $50\%$ of cases~\cite{DNABert}. \\
    {\bf (c).} We thus create a DNA segment comprising the base pairs between $ST$ and $ST+L$ of chromosome $c$, corresponding to \textit{data point in the pretraining dataset}. We filter out DNA segments that contain bases other than A, T, C, or G. 

    %\item We repeat the process by updating $ST$ as $ST+L$ till we reach the end of the chromosome.

\noindent \textbf{MLM training.} The SOTA models are trained with masked language modeling loss similar to  BERT~\cite{bert}. %with some key differences. First, the \textit{next sentence prediction} task is not used for pretraining; therefore, only the masked language modeling loss remains. Second, 
However, to mask a nucleotide, a contiguous sequence of tokens is masked to prevent information leakage, as each nucleotide is part of k consecutive k-mers. %\tblu{, as clearly depicted in Figure~\ref{fig:masking-kmer}}. 
More formally, say a nucleotide is represented as DNA[$i$], while a 6-mer token is represented as T[$i$], equivalent to \{DNA[$i$-2], DNA[$i$ - 1] $\cdots$ DNA[$i$ + 3]\}; then the tokens T[$j$],  $\forall(j)_{j=i - 2}^{i + 3}$ are masked. We enforce this mapping between a nucleotide and a 6-mer token in \genemask{} as the $MapNucleotideToKmerTokens$ function in Algorithm~\ref{algo:genemask}.
% overfitting because each token is in k-mer representation, which means that the previous token has ($k-1$) base pairs overlapping. 
%\noteng{refer to the picture here}\notesr{Added.}
Given that, $k=6$ and $15\%$ of tokens need to be masked~\cite{bert}, the MLM probability is set at: $15\% / 6$; that is, $2.5\%$ of the nucleotides are chosen for masking. (Since we are mainly working with 6-mer, unless mentioned explicitly T[$i$] = $T_6[i]$.)






\if{0}
DNABert is a BERT-based transformer model that is trained on gene sequences obtained from the Human Reference Genome, which corresponds to a long sequence of nucleotides (a combination of bases adenine (A), cytosine (C), guanine (G), and thymine (T)) for each chromosome. We use the Genome Reference Consortium Human Build 38 patch release 13 (GRCh38.p13) FASTA file with RefSeq assembly accession id as GCF\_000001405.39 from NCBI website\footnote{\url{https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39/}} for constructing the large gene sequence corpus to be used to perform unsupervised Masked Language Model (MLM) training. It is presented as a sequence of nucleotides. Each side of the double helix DNA strand comprises the bases adenine (A), cytosine (C), guanine (G), and thymine (T)).

We convert the obtained human reference genome into documents ($D$), where each document $d \in D$ is a sequence of sentences, where each sentence is a sequence of fragments of DNA.


DNABert first takes a set of sequences represented as k-mer tokens as input. 

Each sequence is then represented as a matrix M by embedding each token into a numerical vector. Specifically, DNABert encodes contextual information by performing the multi-head self attention mechanism. It adopts similar pretraining scheme like BERT with the following key differences: (a) next sentence prediction task is not used for pretraining, therefore only the masked language modeling loss remains (b) since each token is in k-mer representation, thus the previous token has ($k-1$) base pairs overlapping. Thus, we always mask a contiguous sequence of $k$ tokens while masking in order to prevent overfitting. %, which makes the mask prediction quite trivial. Thus the strategy to mask contiguous $k$ tokens is adopted.

\noindent \textbf{Pretraining Data Construction.} \tblu{ Following the pretraining data preparation strategy as outlined in DNABert, we obtain sequences of length 510 for 50\% of cases and a randomly selected length between 5 and 510 for the remaining half of the time.} 
 
\noindent \textbf{Tokenization of gene sequences.} \noteng{I think we dont have this. }\notesr{Removed gene boundary description}
%Extract gene annotations and adhere to gene boundaries during tokenization, instead of randomly selecting a gene sequence segment for tokenization. We first obtain the GeneBank annotations that provide the positions of all known genes in a human (around 27000), i.e, for each gene, we know its chromosome number, start and end index. 
The gene sequence is first converted into a k-mer representation, which is commonly used in literature~\cite{dnavec,DNABert}. ~\cite{Yang2021} further supported such a k-mer representation from a biological standpoint, stating that each nucleotide is not independent such as codon rules in coding region and regulatory motifs in non-coding region. $k$-mer representation is a sliding window of length $k$. For example, \textit{AGCACGCAG} in 6-mer representation leads to 3 tokens - \textit{AGCACG}, \textit{GCACGA}, and \textit{CACGCAG}. The vocabulary thus comprises of all combinations ($4^k$ length) and 5 special tokens - \textit{CLS, PAD, UNK, SEP, MASK}. We consider $k = 6$ for all the experiments, and interchangeably refer to it as $6-mer$.
%\noindent \textit{Justification:} \notesr{Please use LOGO paper part explaining k-mer usage --- I have mentioned some points based on the feedback I received from our talk} We use 6-mer as it captures longer context. \notejw{I wonder if it is the longer context or the larger vocabulary that is yielding better performance. I think I remember the differences between 3 and 6-mers to be quite small (?)}
%Prior works use 3, 4, 5, 6 -mers --- Prior feedback: multiples of 3 need to be used. --- LOGO paper intuition: each nucleotide is not independent such as codon rules in coding region and regulatory motifs in non-coding region --- Choice Implication: Bigger k leads to larger vocabulary size, thus require more model parameters, more memory usage and longer convergence time.
\fi