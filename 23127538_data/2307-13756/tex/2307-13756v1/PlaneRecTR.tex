\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}

\usepackage{bbm}
\usepackage{bbding}
\usepackage{subfigure}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{mathrsfs}
\usepackage[dvipsnames]{xcolor}
\newcommand{\compresslist}{%
 \setlength{\itemsep}{0pt}%
 \setlength{\parskip}{0pt}%
 \setlength{\parsep}{0pt}%
 }

\graphicspath{{images/}}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{PlaneRecTR: Unified Query learning for 3D Plane Recovery from a Single View}

\author{Jingjia Shi \quad
Shuaifeng Zhi\footnotemark[1] \quad
Kai Xu\footnotemark[1] \\
National University of Defense Technology, China\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}
\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi
\footnotetext[1]{Shuaifeng Zhi and Kai Xu are corresponding authors.}

%%%%%%%%% ABSTRACT
\begin{abstract}
   % Existing solutions to 3D plane recovery from a single image can usually be divided into several subtasks of plane detection, segmentation, parameter estimation and possibly depth estimation. Previous works tend to solve this task by either extending the RCNN-based segmentation framework or clustering learned dense pixel embedding. However, none of them tried to integrate above related tasks into a unified framework but treat these tasks separately, which we suspect is potentially a main source of performance limitation for existing approaches.
   % Existing solutions to 3D plane recovery from a single image can usually be divided into several steps of plane detection, segmentation, parameter estimation and possibly depth estimation.
   3D plane recovery from a single image can usually be divided into several subtasks of plane detection, segmentation, parameter estimation and possibly depth estimation.
   Previous works tend to solve this task by either extending the RCNN-based segmentation network or the dense pixel embedding-based clustering framework. However, none of them tried to integrate above related subtasks into a unified framework but treat them separately and sequentially, which we suspect is potentially a main source of performance limitation for existing approaches.
   Motivated by this finding and the success of query-based learning in enriching reasoning among semantic entities, in this paper, we propose PlaneRecTR, a Transformer-based architecture, which for the first time unifies all subtasks related to single-view plane recovery with a single compact model. Extensive quantitative and qualitative experiments demonstrate that our proposed unified learning achieves mutual benefits across subtasks, obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane datasets. Codes are available at \url{https://github.com/SJingjia/PlaneRecTR}.
\end{abstract}


%%%%%%%%% BODY TEXT
\section{Introduction}
Immersing in a virtual 3D world involves efficient reasoning about surrounding scenes, whose properties need to be frequently updated. Though tremendous efforts have been paid to create authentic 3D geometry from multi-view image observations or even a single image, a trade-off exists between the reconstruction quality and efficiency, depending on the underlying scene representations \cite{Davison:ARXIV2018,rosen2021advances,Zhi:etal:CVPR2019,Zhi:etal:ICCV2021,zhi2022ilabel}. 
3D maps composed of sparse primitives such as point clouds are light-weight to maintain but lack topological structures, while dense geometry like volumetric grids and meshes are computational intensive to acquire and maintain. In this spectrum, planar representation has been proven to be a reliable alternative, which is compact, efficient, expressive, and generalizable enough to be deployed ubiquitously as well. Therefore, in practice it would always be ideal to infer planar information purely from a video sequence, or even a single image.
Being a challenging and yet fundamentally ill-posed computer vision problem, single image plane recovery/reconstruction has been extensively researched and focused so far. 
% lines, segments -> line segments, vanishing points
There have been early attempts using image processing to extract low-level primitives, such as line segments and vanishing points, to extract planar structures from the input image \cite{delage2007automatic, lee2009geometric}.

% Figure environment removed

As Convolutional Neural Networks (CNNs) have become the major approach to  tackle vision problems in the past few years, their excelling in performance gradually spreads to the plane estimation task. Pioneering works such as PlaneNet \cite{Liu:etal:CVPR2018:Planenet} and PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} propose CNN-based efficient solutions to piece-wise planar structure recovery from a single image in a top-down manner. There have also been bottom-up solutions such as PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} and PlaneTR \cite{Tan:etal:ICCV2021:Planetr} which obtain plane-level masks by a post-clustering procedure on top of learned deep pixel-wise embedding space. Recently, Transformers \cite{Vaswani:etal:NIPS2017}, as another major type of fundamental vision models, have made great progress on various vision tasks \cite{wang2020axial, dosovitskiy2020image, xu2021vitae, touvron2021training, wang2022towards}. The success of vision Transformers does not merely comes from its realization of global/long-range interaction across images via attention mechanisms, 
another important aspect is the design of query-based set predictions, initially proposed in Detection Transformer (DETR) \cite{carion2020end} to enable reasoning between detected instances and global context. The introduction of query learning technique to vision Transformers has been further proven to be effective in several high-level vision tasks including instance and semantic segmentation \cite{cheng2021per, cheng2022masked, xu2022fashionformer}, video panoptic segmentation \cite{weber2021step, yuan2022polyphonicformer}, \etc.
PlaneTR \cite{Tan:etal:ICCV2021:Planetr} is an early attempt of using such ideas in single-view plane reconstruction. Motivated by the structured-guided learning,  PlaneTR integrate additional geometric cues like line segments into its training process, leading to state-of-the-art performance on the ScanNet dataset and unseen NYUv2-Plane dataset.

However, all of above mentioned single-view plane recovery methods somewhat dis-entangle the prediction of principle components required for plane reconstruction.
Specifically, PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} learns to predict plane offsets from a monocular depth prediction branch while other attributes such as plane masks and normals are estimated separately from colour images. PlaneTR \cite{Tan:etal:ICCV2021:Planetr} also leans to predict a monocular depth map apart from the Transformer module, which is later used for acquiring plane segmentation masks via associate embedding clustering \cite{Yu:etal:CVPR2019:PlaneAE}. Under such cases, these closely related prediction tasks are usually interleaved and we conjecture this could be one performance bottleneck for existing data-driven approaches.

Motivated by this finding, we seek to borrow recent advance in query-based learning and aim to design a single, compact and unified model to jointly learn all plane-related tasks. We expect such design would achieve a mutual benefits among tasks and improve the existing performance of single view plane reconstruction. Extensive experimental results on the ScanNet benchmark dataset show that \textit{without using neighbouring views} \cite{Liu:etal:CVPR2019:Planercnn} \textit{nor extra structural cues} \cite{Tan:etal:ICCV2021:Planetr} during training, our unified querying learning model, named PlaneRecTR, achieves new state-of-the-art performance with a concise structure. Additionally, we have also found such framework can well take the advancements in fundamental vision models and is able to further increase its performance given stronger backbones.

To summarize, our contribution are as follows:

\begin{itemize}\compresslist
    \item We proposed a first single unified framework to address the single view plane reconstruction task, where all related tasks including plane detection, plane parameter estimation, plane segmentation and depth prediction are jointly inferred in a multi-task manner motivated by query-based learning.
    \item Extensive numerical evaluation and visual comparisons demonstrate the benefits of our proposed unified query learning, which has been proven to takes full advantages of plane semantic and geometric cues to achieve mutual benefits.
    \item We have achieved state-of-the-art single image plane recovery performance on the public ScanNet and NYUv2-Plane datasets, surpassing existing CNN-based and Transformer-based baselines. In addition, we have shown that our model could keep benefiting from stronger backbone vision models.
    
    
\end{itemize}
%------------------------------------------------------------------------

\section{Related Work}

%-------------------------------------------------------------------------
\subsection{3D Plane Recovery from a Single Image}


Traditional methods for 3D plane recovery from a single image often rely on strong assumptions of scenes \cite{barinova2008fast, delage2007automatic, fouhey2014unfolding}(\eg, the Manhattan world assumption), or require manual extraction of primitives \cite{delage2007automatic, fouhey2014unfolding, lee2009geometric}, such as superpixels, vanishing points and line segments, which may not be applicable to actual scenes. In contrast, recent learning-based approaches have alleviated such limitations leveraging large-scale training corpus, which have witnessed exciting progress in single view plane recovery.

PlaneNet \cite{Liu:etal:CVPR2018:Planenet} is the first to propose an end-to-end learning framework for this task, it also generates a large dataset of planar depth maps by utilizing the ScanNet dataset \cite{Dai:etal:CVPR2017}. PlaneRecover \cite{yang2018recovering} presents an unsupervised learning approach that specifically targets outdoor scenes. Both PlaneNet and PlaneRecover can only predict a fixed number of planes. PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} tackles the limitation by using a proposal-based instance segmentation framework, \ie, Mask R-CNN \cite{He:etal:ICCV2017}.
 After plane detection and segmentation through Mask R-CNN, plane normal and image depth are then predicted, followed by plane offset calculation.
 It also proposes a segmentation refinement network and a warping loss to improve performance. 
In summary, recent proposal-based methods require multiple steps to successively tackle subtasks of 3D plane recovery including plane detection, plane segmentation, plane parameters estimation and depth estimation \cite{Liu:etal:CVPR2019:Planercnn}, therefore the overall pipeline are decoupled where planar attributes are usually inferred given predicted plane segmentation.

 
 On the other hand, PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} leverages a proposal-free instance segmentation approach, which used mean shift clustering to group embedding vectors within planar regions. PlaneTR \cite{Tan:etal:ICCV2021:Planetr} inherits the design of DETR \cite{carion2020end} to detect possible plane instances and estimate plane parameters in parallel, followed by plane segmentation generated by pixel clustering like PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE}. 
 Specifically, its Transformer branch only predicts instance-level plane information, thus post-processing like clustering is still required to carry out pixel-wise segmentation. The global depth is inferred by another convolution branch. 
 
 Therefore, existing advanced methods, whether based on direct CNN prediction or embedding clustering, still divide the whole 3D plane recovery task into several stages.

% Figure environment removed

%-------------------------------------------------------------------------
% \subsection{Transformers and DETR}
\subsection{Vision Transformers}
Transformer \cite{Vaswani:etal:NIPS2017} has recently become a popular fundamental vision model to solve computer vision problems \cite{wang2020axial, dosovitskiy2020image, xu2021vitae, touvron2021training, wang2022towards}. Researchers have employed vision Transformers to develop new methods, \eg, DETR \cite{carion2020end}, which treats object detection as a direct set prediction problem using a Transformer encoder-decoder architecture. DETR uses a fixed number of learned object queries to reason about object relations and global image context, enabling to output the final set of predictions in parallel.  Building on the success of DETR, Maskformer \cite{cheng2021per} and Mask2former \cite{cheng2022masked} have introduced a pixel-level decoder to solve both semantic-level and instance-level segmentation tasks in a unified manner. 
Besides, there have been other inspiring works \cite{yuan2022polyphonicformer, Huang_2022_CVPR:MonoDTR, he2023ssd:SSD-MonoDTR, wu2023monopgc:MonoGPC} to tackle different 3D vision tasks through vision Transformer. For example, Polyphonicformer \cite{yuan2022polyphonicformer} brings the merits of query-based reasoning into joint depth and panoptic rediction to solve Depth-aware Video Panoptic Segmentation (DVPS) task. MonoDTR \cite{Huang_2022_CVPR:MonoDTR} proposes a depth-aware Transformer network to integrates context and depth features for monocular 3D object detection. Our method is also a Transformer-based architecture and leverages query learning to unify all subtasks of plane reconstruction.

%------------------------------------------------------------------------

\section{Method}

In this section, we present architecture details and training configurations of proposed PlaneRecTR. We start by first introducing the overall architecture of PlaneRecTR in Section~\ref{sec:model}, and then describe the inference process of recovering 3D planes in Section \ref{sec:planeinference}. Finally, we discuss the training process and its objective functions in Section \ref{sec:training}.


%-------------------------------------------------------------------------
\subsection{Unified Query Learning for Plane Recovery}
\label{sec:model}


Inspired by the recent successes of DETR \cite{carion2020end} and Mask2Former \cite{cheng2022masked} in object detection and semantic segmentation, we find that, it is not impossible to tackle the challenging monocular planar reconstruction task using a \textit{single}, \textit{compact} and \textit{unified} framework, thanks to the merits of query-based reasoning to enable joint modeling of multiple tasks.

As shown in Figure \ref{fig:network_full}, our PlaneRecTR consists of three main modules: (1) A pixel-level module to learn dense pixel-wise deep embedding of the input colour image. (2) A Transformer-based unified query learning module to jointly predict four target properties for each of $N$ shared plane queries, including plane classification probability $p_i$, plane parameter $n_i$, mask embedding, and depth embedding ($i\in[1,2,..., N]$). Specifically, $p_i$ is the probability to judge whether the $i^\text{th}$ query corresponds to a plane or not; $n_i\doteq \tilde{n_i}/d_i \in \mathbb{R}^3$,  where $\tilde{n_i} \in \mathbb{R}^3$ is its plane normal and $d_i$ is the distance from the $i^\text{th}$ plane to camera center, \ie, offset. (3) A plane-level module to generate plane-level mask $m_i$ and plane-level depth $d_i$ through mask and depth embedding ($i\in[1,2,..., N]$). And then we remove non-plane query hypothesis while combining the remaining ones for final image-wise plane recovery.
These three modules will be described in details bellow.



\vspace{-10pt}
\paragraph{Pixel-Level Module.}
Given an image of size $H \times W$ as input, we use the pre-trained ResNet-50 \cite{He:etal:CVPR2016} as the backbone model to extract image feature maps. Similar to Mask2Former, a multi-scale convolutional pixel decoder is used to produce a set of dense feature maps with four scales, denoted as follows:
\begin{equation}
\begin{aligned}
\mathbb{F}&=\{F_{1}\in \mathbb{R}^{C_{1}\times H/32\times W/32}, F_{2}\in \mathbb{R}^{C_{2}\times H/16\times W/16}, \\
& F_{3}\in \mathbb{R}^{ C_{3}\times H/8\times W/8}, \mathcal{E}_\text{pixel}\in \mathbb{R}^{ C_{\mathcal E}\times H_{\mathcal E}\times W_{\mathcal E}}\}.
\end{aligned}
\end{equation}

Among these, the first three feature maps $\{F_{1}, F_{2}, F_{3}\}$ are fed to the Transformer module, while the last one $\mathcal{E}_\text{pixel}$, a dense per-pixel embedding of resolution $H_{\mathcal E}=H/4$ and $W_{\mathcal E}=W/4$, is used for computing plane-level binary masks and plane-level depths.
%
\vspace{-10pt}
\paragraph{Transformer Module.}
We use the Transformer decoder with masked attention proposed in \cite{cheng2022masked}, which computes $N$ plane-level embeddings from above mentioned multi-scale feature maps $\{F_{1}, F_{2}, F_{3}\}$ and $N$ learnable plane queries. The predicted embeddings are then independently projected to four target properties by four different multi-layer perceptrons (MLPs). Overall, the Transformer module predicts required planar attributes for all of $N$ embeddings (upper part in Figure \ref{fig:network_full}).

\vspace{-10pt}
\paragraph{Plane-Level Module.}
 As shown in upper right part of Figure \ref{fig:network_full}, we obtain a dense plane-level binary mask $m_i \in [0, 1]^{H \times W}$/depth prediction $d_i \in \mathbb{R}^{H \times W}$ by a dot product between the $i^\text{th}$ mask/depth embedding and the dense per-pixel embedding from previous two modules, respectively.
 
Please note that we have also investigated the idea of learning two individual pixel decoders for semantics and depth specific dense embeddings and mask predictions. However, we did not observe a clear performance improvement, and therefore stick to current set-up with a shared $\mathcal{E}_\text{pixel}$ for sake of efficiency.

We finally obtains $N$ plane-level predictions $\{(p_i,n_i, m_i, d_i)\}_{i=1}^N$ by the plane-level module, and each plane-level prediction contains all the information needed to recover a possible 3D plane.


%-------------------------------------------------------------------------
\subsection{3D Plane Recovery during Inference}
\label{sec:planeinference}

% sjj: argmin->argmax
In this section, we briefly discuss how to recover 3D planes from our plane-level predictions. For the $N$ plane-level predictions predicted by the network  $\{(p_i,n_i, m_i, d_i)\}_{i=1}^N$, we first drop non-plane components according to the plane classification probability $p_i$, leading to a subset of $K$ planes ($\ie, K\leq N$). 
% The final planar segmentation mask of the input image is obtained by, for each pixel position of planar region, the most probable plane index based on segmentation mask prediction, \ie, $\mathop{\arg\max}\limits_{i}\{m_i\}_{i=1}^{K}$.
For each pixel in the planar regions, we calculate $\mathop{\arg\max}\limits_{i}\{m_i\}_{i=1}^{K}$ to obtain the most likely plane index as the final global segmentation mask.


In this simple and efficient manner, we can use the network predictions to create 3D plane reconstruction of the input frame. Note that plane-level depths are not involved during inference and we use plane parameters to infer planar depths. 
We experimentally found that this design also leads to more structural and smooth geometric predictions than that relying on direct depth predictions.
%-------------------------------------------------------------------------
\subsection{Network Training}
\label{sec:training}
\paragraph{Plane-level Depth Training.}
Previous methods tend to predict a global depth by separate network branches to calculate planar offset \cite{Liu:etal:CVPR2019:Planercnn} or use depth as additional cues to formulate segmentation masks \cite{Yu:etal:CVPR2019:PlaneAE,Tan:etal:ICCV2021:Planetr}.
In contrast, our method tries to achieve mutual benefits between planar semantic and geometric reasoning, 
we leverage joint query learning to unify all components of plane recovery in a concise multi-task manner. As a result, we explicitly predict dense plane-level depths, binary-masks, plane probabilities and parameters from a shared feature space, which is refined and produced via attention mechanism of the Transformer.

% Therefore we additionally add plane-level Depth prediction Task in the training phase, and just add an MLP and dot product operation in the implementation.
\vspace{-10pt}
\paragraph{Bipartite Matching.}
During training, one important step is to build optimal correspondences between $N$ predicted planes and $M$ ground truth planes. 
% We achieve this following \cite{cheng2022masked} using bipartite matching by searching for a permutation $\hat{\sigma}$ (where $\sigma(i)$ indicates the matched index of the predict planes to the ground truth $\hat{s}_{p}^{i}$) by minimizing a matching defined cost function $D$:
Following bipartite matching of \cite{cheng2022masked}, we search for a permutation $\hat{\sigma}$ (where $\sigma(i)$ indicates the matched index of the predict planes to the ground truth $\hat{s}_{p}^{i}$) by minimizing a matching defined cost function $D$:
\begin{equation}
\label{eq: bi-matching}
\hat{\sigma}=\underset{\sigma}{\arg \min } \sum_{i=1}^{K} D\left(\hat{s}_{p}^{i}, s_p^{\sigma(i)}\right), 
\end{equation}
\vspace{-0.5cm}
\begin{align}
\label{eq: bi_matching2}
D=-p_{\sigma(i)}\left( \hat{p}_{i}\right)
&+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{n} \, L_{1}\left(\hat{{n}}_{i}, {n}_{\sigma(i)}\right) \nonumber\\
&+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{d} \, L_{1}\left(\hat{{d}}_{i}{m}_{\sigma(i)}, {d}_{\sigma(i)}\right)  \nonumber\\
&+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{ce} \, L_{ce}
\nonumber\\
&+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{dice} \, L_{dice},
\end{align}
where $(\hat{p}_i,\hat{n}_i, \hat{m}_i, \hat{d}_i)$ are the $i$-th ground-truth plane attributes; $\omega_{n}, \omega_{d}, \omega_{ce}$ and $\omega_{dice}$ are weighting terms and set to 1, 2, 5, 5, respectively. 
% Here we additional consider of influence of mask and depth quality using an $L_1$ depth loss and segmentation dice loss $L_{dice}$, respectively.
Here we additional consider the influence of mask and depth quality using a mask binary cross-entropy loss $L_{ce}$ \cite{cheng2022masked}, a mask dice loss $L_{dice}$ \cite{milletari2016v} and an $L_1$ depth loss, respectively.

\vspace{-10pt}
\paragraph{Loss Functions.}
After bipartite matching, the final training objectives, \ie, the total loss $L$ is formulated by four parts as follows:
\begin{equation}
% \small 
\mathcal{L} = 
\sum\limits_{i=1}^{M}\left( \mathcal{L}_{\text {cls }}^{(i)} + \mathcal{L}_{\text {param }}^{(i)} + \mathcal{L}_{\text{mask}}^{(i)} + \lambda\mathcal{L}_{\text {depth}}^{(i)} \right),
\end{equation}
where $\lambda$ is a weight to balance the magnitude for loss terms and is set to 2 in this paper. $\mathcal{L}_{\text {cls}}$ and $\mathcal{L}_{\text {param}}$ are a plane classification loss and a plane parameter loss, in a similar form to previous works \cite{Tan:etal:ICCV2021:Planetr}. 

Different from \cite{Tan:etal:ICCV2021:Planetr}, the left two loss terms in our paper $\mathcal{L}_{\text{mask}}$ and $\lambda\mathcal{L}_{\text {depth}}$ are designed to explicitly learn dense planar masks and depths. Specifically,
we have plane segmetation mask prediction loss as a combination of cross-entropy loss and dice loss:
\begin{align}
\label{eq:mask}
\mathcal{L}_{\text {mask }}^{(i)} =
& \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{ce} L_{ce} + \nonumber\\
& \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{dice} L_{dice},
\end{align}
where $\beta_{ce} = 5$ and $\beta_{dice} = 5$.
The depth prediction loss is in a typical $L_1$ form:
\begin{align}
\label{eq:mask}
\mathcal{L}_{\text {depth }}^{(i)} =
& \mathbbm{1}_{\{\hat{p}_{i}=1\}}  L_{1}\left(\hat{{d}}_{i}{m}_{\sigma(i)}, {d}_{\sigma(i)}\right).
\end{align}
%------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}

In this section, we conduct experiments to evaluate the performance of the proposed PlaneRecTR on two public datasets. Then, we ablate structural designs of PlaneRecTR to demonstrate that multiple subtasks of plane recovery can benefit each other through unified query learning.



% \subsection{Datasets and Evaluation Metrics}
% \label{sec:data_metrics}
\vspace{-10pt}
\paragraph{Datasets.} 
We train and evaluate PlaneRecTR on two popular benchmarks: ScanNet \cite{Dai:etal:CVPR2017} and NYUv2-Plane \cite{Silberman:etal:ECCV2012}. 

\textbf{ScanNet} dataset is a large-scale RGB-D video collection of 1,513 indoor scenes. We use piece-wise planar ground-truth  generated by PlaneNet \cite{Liu:etal:CVPR2018:Planenet}, which contains 50,000 training and 760 testing images with resolution 256 $\times$ 192. 

\textbf{NYUv2-Plane} dataset is a planar variant of the original NYUv2 dataset \cite{Silberman:etal:ECCV2012} provided by \cite{Liu:etal:CVPR2018:Planenet}. 
% Different from the original NYUv2 data (795 training and 654 testing images) of size 640x480, data of NYUv2-Plane are also at resolution 256x192.
It has 654 testing images with resolution 256 $\times$ 192. 
% Following \cite{Tan:etal:ICCV2021:Planetr, Liu:etal:CVPR2019:Planercnn},  
Please note that NYUv2-Plane is adopted to highlight generalization capability of plane recovery methods without any further fine-tuning.


%-------------------------------------------------------------------------
\vspace{-10pt}
\paragraph{Evaluation Metrics.}
For the entire task of 3D plane recovery, we adopt per-plane and per-pixel recalls to evaluate our PlaneRecTR and baselines, following \cite{Tan:etal:ICCV2021:Planetr, Liu:etal:CVPR2019:Planercnn}. The per-plane/pixel recall metrics is defined as the percentage of the correctly predicted ground truth planes/pixels. 
% A plane is considered being correctly predicted if its segmentation Intersection over Union (IoU) and depth (surface normal) error satisfy pre-defined thresholds. Specifically, the segmentation threshold of IoU is set to 0.5, while the error thresholds of depth and surface normal vary from 0.05m/2.5° to 0.6m/30°, with an increment of 0.05m/2.5°.
A plane is considered being correctly predicted if its segmentation Intersection over Union (IoU), depth normal and plane offset errors satisfy pre-defined thresholds. Specifically, the segmentation threshold of IoU is set to 0.5, while the error thresholds of depth/surface normal vary from 0.05m/2.5° to 0.6m/30°, with an increment of 0.05m/2.5°.

Additionally, for evaluating plane segmentation, we apply three popular metrics\cite{Arbelaez:etal:TPAMI2010, yang2018recovering}: variation of information (VI), rand index (RI) and segmentation covering (SC). For evaluating plane parameter, we obtained the best match by minimizing the $L_1$ cost between $K$ predicted plane parameters and $M$ ground truth plane parameters, and then separately compute the average errors of plane normal and offset. As to NYUv2-Plane dataset, depth accuracy are evaluated on structured planar regions to highlight plane recovery performance \cite{Liu:etal:CVPR2018:Planenet,Tan:etal:ICCV2021:Planetr}.

%-------------------------------------------------------------------------
\vspace{-10pt}
% \paragraph{Baseline Models.}
\paragraph{Network Variants.}
We note a few important architectural differences w.r.t. model designs and learning tasks. To reflect the contributions of our key design choices, we consider (1) explicit plane-level binary mask prediction (M), (2) plane-level depth prediction (D), (3) plane parameter estimation (P), and thus denote network varieties in Table \ref{tab:variants}.
% add hrnet: is same as planeTR
% \begin{itemize}
% \setlength\itemsep{0.03em}
%     \item \textbf{PlaneRecTR}: our complete model with ResNet-50 unifying plane classification and other three tasks (M, D and P);
%     \item  \textbf{PlaneRecTR (-D)}: the network is trained without explicitly predicting plane-level depth;
%     \item  \textbf{PlaneRecTR (-M-D)}: the network only predicts per-query confidence of being a plane and if so its plane parameters;
%     \item  \textbf{PlaneRecTR (-P-D)}: the network only learns for single-view plane segmentation;
%     \item \textbf{PlaneRecTR (HRNet-32)}: We replace our ResNet-50 backbone by the same HRNet-32 \cite{WangSCJDZLMTWLX19:HRNet} backbone as the most critical baseline PlaneTR \cite{Tan:etal:ICCV2021:Planetr}.
%     \item \textbf{PlaneRecTR (Swin-B)}: We replace our ResNet-50 backbone by a Swin-B model \cite{liu2021swin}.
% \end{itemize}


\begin{table}
% \small
% \renewcommand\arraystretch{1.1}
\centering
\resizebox{0.95\linewidth}{!}{ 
    % \begin{center}
    % \setlength{\tabcolsep}{1.3mm}f
    \begin{tabular}{c|cccc}
    \toprule
        % \multirow{2}{*}{Method\_Backbone}  &\multicolumn{6}{c}{(Planar Depth Accuracy)} \\ 
        Name & Backbone &  Task 'M' & Task 'P' & Task 'D' \\ 
        \hline
        PlaneRecTR          & ResNet-50 & $\checkmark$ & $\checkmark$ & $\checkmark$\\
        PlaneRecTR (-D)  & ResNet-50 & $\checkmark$ & $\checkmark$ & -\\
        PlaneRecTR (-M-D)  & ResNet-50 & - & $\checkmark$ & - \\
        PlaneRecTR (-P-D)  & ResNet-50 & \checkmark & - & - \\
        PlaneRecTR (HRNet-32)  & HRNet-32 \cite{WangSCJDZLMTWLX19:HRNet} & $\checkmark$ & $\checkmark$ & $\checkmark$\\
        PlaneRecTR (Swin-B)  & Swin-B \cite{liu2021swin} & $\checkmark$ & $\checkmark$ & $\checkmark$\\
        \bottomrule
    \end{tabular}
}
\vspace{5pt}
% \caption{Different baseline models.}%\vspace{-10pt}
\caption{Our PlaneRecTR variants during experiments. (`-X' indicates the network is trained without task `X')}

\vspace{-5pt}
\label{tab:variants}
\end{table}


%-------------------------------------------------------------------------
\subsection{Implementation Detail}
Our network is implemented with Detectron2 \cite{wu2019detectron2}. We train our network on the ScanNet training set with total of 34 epochs on a single NVIDIA TITAN V GPU. We use AdamW optimizer \cite{loshchilov2017decoupled} with an initial learning rate of $1 \times 10^{-4}$ and a weight decay of 0.05. The batch size is set to 16. 



% Figure environment removed

%-------------------------------------------------------------------------
\subsection{Results on the ScanNet Dataset}
% \vspace{-10pt}
\paragraph{Qualitative Results.}
In this section, we present qualitative results of our single-view plane reconstruction on a variety of unseen ScanNet scenes in both 2D and 3D domains. As can be seen in Figure \ref{fig:3DPlane}, our PlaneRecTR is able to predict accurate planar segmentation masks and plane parameters, as well as reasonable 3D plane reconstructions.

We further show detailed visual comparisons to PlaneTR, up-to-date the strongest single-view plane recovery approach, in Figure \ref{fig:plane_compare}. Although PlaneTR has integrated extra structural cues like line segments of scenes, it is exciting to see that our approach, mainly benefiting from the explicit joint modelling of 
% query learning of plane parameter, segmentation and depth,
plane geometry and segmentation, 
% is able to predict crisper plane segmentation masks (row 1-2 of Figure \ref{fig:plane_compare}) and discriminate con planes with similar normals (row 2 of Figure \ref{fig:plane_compare}), with more complete and holistic structures.
is able to predict crisper plane segmentation mask and discriminate planes with similar normals (rows 2-3, 5-6, 8-9 of Figure \ref{fig:plane_compare}), with more complete and holistic structures.


%-------------------------------------------------------------------------
\vspace{-10pt}
\paragraph{Quantitative Results.}
In this section, we conduct extensive quantitative evaluations towards previous state-of-the-art learning-based plane recovery methods: PlaneNet \cite{Liu:etal:CVPR2018:Planenet}, PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}, PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} and PlaneTR \cite{Tan:etal:ICCV2021:Planetr}. 


Like \cite{Tan:etal:ICCV2021:Planetr}, PlaneRCNN is shown here mainly as a reference because of its learning with a different ScanNet training set-up. We thus focus on numerical comparison towards other recent baselines. We use public implementation of PlaneTR \cite{Tan:etal:ICCV2021:Planetr} and its provided pre-trained weights to report corresponding performance. 

In terms of segmentation accuracy, Table \ref{tab:scannet_nyu_seg} shows that on challenging ScanNet we achieves a new state-of-the-art plane segmentation performance, outperforming leading PlaneTR with a relative large margin especially in the VI metric. To further demonstrate the flexibility of our method, we have shown improved versions of PlaneRecTR by replacing ResNet-50 backbone \cite{He:etal:CVPR2016} with the same HRNet-32 backbone \cite{WangSCJDZLMTWLX19:HRNet} as PlaneTR or a SwinTransformer-B model \cite{liu2021swin}. The performance gap widens and shows that our framework could benefit from ongoing research in developing more powerful fundamental vision models.

As to 
% geometry performance including plane parameter estimation and plane-level depth prediction,
performance of entire plane recovery task, 
we display per-pixel and per-plane recalls of depth and plane normal on the ScanNet dataset, respectively (see Figure \ref{fig:plane_recall}).
With varying thresholds from 0.05 to 0.6 meters and from 2.5 to 30 degrees for depth and normal evaluations, our method consistently outperform all baselines, indicating more precise predictions of plane parameters, segmentation mask and planar depths. We want to further highlight the significant improvement w.r.t. per-plane recall, our methods could efficiently discover structural planes of various scales, in contrast to \cite{Tan:etal:ICCV2021:Planetr} which tends to miss planes with small areas or sharing similar geometry (see Figure \ref{fig:plane_compare}).

We own the improvements to the explicit joint modelling of plane detection, plane parameter estimation, plane segmentation and depth prediction as a multi-task querying learning problem, and we will give detailed ablation studies in Section \ref{sec:ablation}.

% Figure environment removed


% Figure environment removed

















%-------------------------------------------------------------------------
\subsection{Results on the NYUv2-Plane Dataset}
% add  "See appendix for more evaluation metrics".
\begin{table}
% \small
% \renewcommand\arraystretch{1.1}
\centering
\resizebox{0.95\linewidth}{!}{ 
    % \begin{center}
    % \setlength{\tabcolsep}{1.3mm}f
    \begin{tabular}{c|ccc|ccc}
    \toprule
        \multirow{2}{*}{Method} & \multicolumn{3}{c|}{ScanNet} & \multicolumn{3}{c}{NYUv2-Plane} \\ 
                                & VI~$\downarrow$ & RI~$\uparrow$ & SC~$\uparrow$ & VI~$\downarrow$ & RI~$\uparrow$ & SC~$\uparrow$\\\midrule
        PlaneNet \cite{Liu:etal:CVPR2018:Planenet}        & 1.259 & 0.858 & 0.716 & 1.813 & 0.753 & 0.558 \\
        PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}      & 1.337 & 0.845 & 0.690 & 1.596 & 0.839 & 0.612 \\
        PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE}        & 1.025 & 0.907 & 0.791 & 1.393 & 0.887 & 0.681 \\
        PlaneTR \cite{Tan:etal:ICCV2021:Planetr}           & 0.767 & 0.925 & 0.838 & 1.163 & 0.891 & 0.712  \\ \bottomrule
        % PlaneMaskParamTR            & 0.694 & 0.937 & 0.855 & 1. & 0. &0. \\
        PlaneRecTR            & \textbf{0.698} & \textbf{0.936} & \textbf{0.854} & \textbf{1.130} & \textbf{0.905} & \textbf{0.722} \\
        PlaneRecTR (HRNet-32)
    %      & \textcolor[RGB]{0,153,153}{\textbf{0.679}} & 
    % \textcolor[RGB]{0,153,153}{\textbf{0.937}} & \textcolor[RGB]{0,153,153}{\textbf{0.857}}&  \textcolor[RGB]{0,153,153}{\textbf{1.049}} & \textcolor[RGB]{0,153,153}{\textbf{0.912}} & \textcolor[RGB]{0,153,153}{\textbf{0.738}} \\
    & {\textbf{0.679}} & 
    {\textbf{0.937}} & {\textbf{0.857}}&  {\textbf{1.049}} & {\textbf{0.912}} & {\textbf{0.738}} \\
        PlaneRecTR (Swin-B)            & \underline{\textbf{0.652}} & \underline{\textbf{0.943}} & \underline{\textbf{0.865}} & \underline{\textbf{1.045}} & \underline{\textbf{0.915}} & \underline{\textbf{0.745}}  \\
         \bottomrule
        % Ours-R101               & 0.793 & 0.923 & 0.832  \\
    \end{tabular}
}
    % \end{center}
    % \vspace{-8pt}
\vspace{5pt}  
\caption{Comparison of plane segmentation results on the ScanNet dataset and NYUv2-Plane dataset. }
\label{tab:scannet_nyu_seg}
\end{table}


\begin{table}
% \small
% \renewcommand\arraystretch{1.1}
\centering
\resizebox{0.95\linewidth}{!}{ 
    % \begin{center}
    % \setlength{\tabcolsep}{1.3mm}f
    \begin{tabular}{c|ccc|ccc}
    \toprule
        % \multirow{2}{*}{Method\_Backbone}  &\multicolumn{6}{c}{(Planar Depth Accuracy)} \\ 
        Method & Rel$\downarrow$ &  ${\text{log}_{10}}\downarrow$ & RMSE$\downarrow$ & $\delta_{1}\uparrow$ & $\delta_{2}\uparrow$ & $\delta_{3}\uparrow$ \\ 
        \hline
        % \textcolor{gray}{PlaneNet \cite{Liu:etal:CVPR2018:Planenet}}        & \textcolor{gray}{0.236} & \textcolor{gray}{0.124} & \textcolor{gray}{0.913} & \textcolor{gray}{53.0} & \textcolor{gray}{78.3} & \textcolor{gray}{90.4} \\
        % \textcolor{gray}{PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE}}      & \textcolor{gray}{0.205} & \textcolor{gray}{0.097} & \textcolor{gray}{0.820} & \textcolor{gray}{61.3} & \textcolor{gray}{87.2} & \textcolor{gray}{95.8}\\ 
        % \textcolor{gray}{PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}}        & \textcolor{gray}{\textbf{0.183}} & \textcolor{gray}{\textbf{0.076}} & \textcolor{gray}{\textbf{0.619}} & \textcolor{gray}{\textbf{71.8}} & \textcolor{gray}{\textbf{93.1}} & \textcolor{gray}{\textbf{98.3}} \\ 
        {PlaneNet \cite{Liu:etal:CVPR2018:Planenet}}        & {0.236} & {0.124} & {0.913} & {53.0} & {78.3} & {90.4} \\
        {PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE}}      & {0.205} & {0.097} & {0.820} & {61.3} & {87.2} & {95.8}\\ 
        {PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}}        & {\textbf{0.183}} & {\textbf{0.076}} & {\textbf{0.619}} & {\textbf{71.8}} & {\textbf{93.1}} & {\textbf{98.3}} \\ 
        

        {PlaneTR \cite{Tan:etal:ICCV2021:Planetr}}      & {0.199} & {0.100} & {0.700} & {59.6} & {86.6} & {96.3} \\ \hline
        %\hline

        PlaneRecTR         & 0.202 & 0.100  & 0.729 & 59.5 & 87.4 & 96.0 \\    
        {PlaneRecTR (HRNet-32)}               
        % & \textcolor[RGB]{0,153,153}{\textbf{0.178}} & \textcolor[RGB]{0,153,153}{\textbf{0.087}} & \textcolor[RGB]{0,153,153}{\textbf{0.635}} & \textcolor[RGB]{0,153,153}{\textbf{66.6}} & \textcolor[RGB]{0,153,153}{\textbf{91.1}} & \textcolor[RGB]{0,153,153}{\textbf{ 97.7}} \\ 
        & {\textbf{0.178}} & {\textbf{0.087}} & {\textbf{0.635}} & {\textbf{66.6}} & {\textbf{91.1}} & {\textbf{ 97.7}} \\ 
        PlaneRecTR (Swin-B)           & \underline{\textbf{0.157}} & \underline{\textbf{0.073}} & \underline{\textbf{0.547}} & \underline{\textbf{74.1}} & \underline{\textbf{94.2}} & \underline{\textbf{99.0}}  \\
         \bottomrule
        % Ours-R101               & 0.793 & 0.923 & 0.832  \\
    \end{tabular}
}
\vspace{5pt}
\caption{Depth accuracy comparison on the NYUv2 dataset. }
\vspace{-10pt}
\label{tab:nyu_depth}
\end{table}



NYUv2-Plane dataset is chosen here mainly to verify the generalization capability of our method on unseen novel indoor scenes. As shown in Table \ref{tab:scannet_nyu_seg}, our method still achieves leading plane segmentation accuracy in all metrics without any fine-tuning. Please check bottom part of Figure \ref{fig:plane_compare} for more detailed visual comparisons.
In terms of depth prediction task, we focus on pixel-wise depth accuracy in planar regions, thus we treat PlaneTR \cite{Tan:etal:ICCV2021:Planetr} as the targeting baseline for a fair comparison and others as reference.
Our base pipeline performs on-par with PlaneTR and outperforms PlaneNet \cite{Liu:etal:CVPR2018:Planenet} and PlanAE \cite{Yu:etal:CVPR2019:PlaneAE}. When our backbone is replaced from ResNet-50 \cite{He:etal:CVPR2016} to the same HRNet-32 \cite{WangSCJDZLMTWLX19:HRNet} as PlaneTR, our performance is significantly better than PlaneTR. PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}, with a ResNet-101 backbone, achieves better performance, partly caused by its utilization of neighbouring multi-view information and higher resolution images during training, while ours is trained with a single image without using extra cues \cite{Liu:etal:CVPR2019:Planercnn,Tan:etal:ICCV2021:Planetr}. 
Nevertheless, we have found that the benefit using more discriminative backbones (\eg, HRNet-32, Swin-B) also transfers to our generalization ability in novel scenes, resulting in a huge performance boost to reach leading planar depth accuracy (see bottom two rows of Table \ref{tab:nyu_depth}). We present more evaluation results on the NYUv2-Plane dataset in Appendix. 



%-------------------------------------------------------------------------
\subsection{Ablation Studies}
\label{sec:ablation}
In this section, we conduct detailed ablation studies on the ScanNet dataset and show the effectiveness of each key design choice in our method.
\vspace{-10pt}
\paragraph{Effects of Jointly Predicting Plane-Level Depths.}
One key difference to previous Transformer-based \cite{Tan:etal:ICCV2021:Planetr} and CNN-based baselines \cite{Liu:etal:CVPR2018:Planenet,Yu:etal:CVPR2019:PlaneAE,Liu:etal:CVPR2019:Planercnn} is that, instead of learning monocular (planar) depth prediction in an individual branch/network, we predict depth based on shared representations for segmentation mask prediction and plane parameters. It turns out that this simple design choice leads to promising performance increment, achieve mutual benefits among tasks. Bottom two rows of Table \ref{tab:all_ablation} show that augmenting depth prediction tasks to query learning affects per-pixel and per-plane recalls of depth and normal, plane parameter errors (normal and offset). It is worth to note the significant recall rates gain especially under the strict (lower) thresholds.  
\vspace{-10pt}
\paragraph{Effects of Predicting Dense Plane-Level Masks.}
Compared to our closest Transformer-based baseline PlaneTR, we also differs in getting plane-level masks via a dense prediction instead of post-clustering from dense embeddings and depth information. Rightmost two columns of Table \ref{tab:all_ablation} has shown that explicitly driving to learn per-plane masks significantly boost the accuracy of plane parameter estimation of our method, even when direct depth supervision is not available.

\begin{table}[t]
\centering
% \large
\LARGE
\resizebox{1.0\linewidth}{!}
{    \renewcommand{\arraystretch}{1.3} % Default value: 1
    \begin{tabular}{ccccccc}
    \toprule
        \multirow{3}{3cm}{\centering {}\\{\huge Method}} & \multicolumn{4}{c}{Per-Pixel/Per-Plane Recalls~$\uparrow$ } & \multicolumn{2}{c}{Plane Parameters}  \\ 
         \cmidrule(l{15pt}r{15pt}){2-5}
        &\multicolumn{2}{c}{Depth}&\multicolumn{2}{c}{Normal} & \multicolumn{2}{c}{Estimation Errors~$\downarrow$}\\
        \cmidrule(l{15pt}r{15pt}){2-3}\cmidrule(l{15pt}r{15pt}){4-5}\cmidrule(l{15pt}r{15pt}){6-7}

         & @0.10 m& @0.60 m& @$5^{\circ}$ & @$30^{\circ}$ & Normal (°) & Offset (mm)\\\midrule
         
        % PlaneTR &52.83/40.74  & 80.52/61.49 &59.45/43.14 & 80.25/60.68 &36.23/27.47 & 76.53/58.47 \\
        PlaneRecTR (-M-D) & - & - & - & - &  10.97 & 177.69\\
        PlaneRecTR(-D)&50.28/43.36  & 82.87/72.29 &61.42/47.80 & 83.44/71.11 & 10.27 (-0.70) & 166.85 (-10.84) \\
        PlaneRecTR& \textbf{53.07/45.07} & \textbf{83.60/72.84} & \textbf{62.75/48.48} & \textbf{83.85/71.33} & \textbf{10.23} (\textcolor{red}{-0.74}) & \textbf{165.13}(\textcolor{red}{-12.56}) \\\bottomrule
    \end{tabular}
}
\vspace{5pt}
\caption{Ablation studies of PlaneRecTR's unified settings on the ScanNet dataset.}
\vspace{-5pt}
\label{tab:all_ablation}
\end{table}
% \noindent \textbf{Plane segmentation and geometry prediction.}
% Figure environment removed


\vspace{-10pt}
\paragraph{Plane Segmentation and Geometry Prediction.}
We also investigate whether the powerful segmentation framework could further benefit from learning plane geometry (plane parameters and depths). The experiment results shows that there are marginal numerical gains in terms of plane segmentation task comparing our complete model to the variant PlaneRecTR (-P-D). Fortunately, we do observe clear qualitative improvements. As shown in Figure \ref{fig:ablation_mask_compare}, our complete model manages to distinguish detailed planar structures and even predict more fine-grained and sound plane segmentation than ground truth annotation, which, however, could possibly be penalized during quantitative evaluation (bottom row of Figure \ref{fig:ablation_mask_compare} where bedstead labelling is missing).



%------------------------------------------------------------------------
\section{Conclusion}
% We have shown that unifying plane related tasks including plane detection, plane parameter estimation, plane segmentation and depth estimation into a single Transformer-based module could achieves promising performance in the challenging single-view plane recovery task, in segmentation accuracy, depth/normal/offset recall rates. The simple yet effective design of joint modelling leads us to obtain SOTA performance on challenging ScanNet and NYUv2 datasets.
Our proposed PlaneRecTR unifies plane related tasks including plane detection, plane parameter estimation, plane segmentation and depth estimation into a single Transformer-based module, its learnable plane queries implement explicit joint modelling and mutual benefits of plane-level semantic and geometric information in challenging single-view plane recovery task. We have shown that the simple yet effective design leads to obtain SOTA performance on challenging ScanNet and NYUv2 datasets. Ablation studies and visual examples validate our motivation and key design choices. 
More importantly, we have observed that the continually developing backbone vision models could further push the boundary of our performance, allowing our framework to be updated with new finding in such areas with a plug-and-play manner. However, our method could only work on planar regions of a single image while it is more practical to build accurate planar reconstructions from a sparse set of views, possibly holistic geometric reasoning as well. We expect our framework to be extended to such cases where camera pose estimation could also be jointly optimized in a unified manner. We see this as an exciting venue to explore in the recent future.

\section{Acknowledgements}
% We thank the anonymous reviewers and AC for their valuable comments. We are grateful to authors of PlaneTR \cite{Tan:etal:ICCV2021:Planetr} for the helpful comments on evaluations. This work was supported in part by the National Key Research and Development Program of China (2018AAA0102200), NSFC (62325211, 62132021, 62201603) and Research Program of National University of Defense Technology (Grant No. ZK22-04).

We thank the anonymous reviewers and AC for their valuable comments, and authors of PlaneTR \cite{Tan:etal:ICCV2021:Planetr} for the helpful comments on evaluations. This work was supported in part by the National Key Research and Development Program of China (2018AAA0102200), NSFC (62325211, 62132021, 62201603) and Research Program of National University of Defense Technology (Grant No. ZK22-04).
{\small
% \bibliographystyle{ieee_fullname}
\bibliographystyle{ieee_fullname}
\bibliography{robotvision}
}


% \newpage
\clearpage
\section*{Appendix}
\appendix
\section{Supplement Video}

We encourage readers to also watch our \textit{supplement video} (\url{https://youtu.be/YBB7totHGJg}) which gives a more vivid illustration of our pipeline and shows 3D reconstruction results.

\section{Details of Training and Inference}
In the ScanNet experiments, we screened out 2307 images from the complete training set (50,000 images) with poor plane parameters annotations, while the testing set remained the same as the previous methods to fair comparison. During inference, we dropped non-plane predictions from a total of $N$ predicted plane probabilities. Sigmoid function is then performed on the remaining $K$ plane predictions to obtain plane-level soft masks, followed by an argmax operation along $K$ soft masks to obtain plane segmentation. During this process, if the peak soft mask value of certain pixels is below a pre-defined threshold (\ie, 0.01 in our implemtation), they are also regarded as non-plane regions. We find this could prevent the network from excess plane predictions on non-planar structures.




\section{More Evaluation Metrics on NYUv2-Plane}

As our method only estimates the depth of planar regions, for a fair comparison we also conduct evaluations using the same reconstructed metrics to ScanNet: per-pixel/plane recall rates of depth, normal, respectively. According to the similar definition in Evaluation Metrics of Section \ref{sec:experiments}, per-pixel/plane recall of plane offset is added and its threshold varies from 25mm to 300mm with an increment of 25mm (see Figure \ref{fig:nyu_recall_appendix} and Table \ref{tab:nyu_recall_sample_appendix} for details). Similarly, our method variants with all three backbones overall outperform PlaneTR \cite{Tan:etal:ICCV2021:Planetr} and we see a large performance boost of PlaneRecTR (HRNet-32/Swin-B) against the ResNet-50 counterpart. In addition, we show the average error statistics of plane parameters separately in Table \ref{tab:nyu_para_error_appendix}, we observe obvious accuracy improvement using HRNet-32/Swin-B backbone, especially on the offset estimation task on unseen NYUv2 data.



\begin{table}[h]
\centering
\resizebox{1.0\linewidth}{!}
{    \renewcommand{\arraystretch}{1.3} % Default value: 1
    \begin{tabular}{ccccccc}
    \toprule
        \multirow{3}{2cm}{\centering {}\\{\large Method}} & \multicolumn{6}{c}{Per-Pixel/Per-Plane Recalls~$\uparrow$}  \\ 
         \cmidrule(l{15pt}r{15pt}){2-7}
        &\multicolumn{2}{c}{Depth}&\multicolumn{2}{c}{Normal}&\multicolumn{2}{c}{Offset}\\
        \cmidrule(l{15pt}r{15pt}){2-3}\cmidrule(l{15pt}r{15pt}){4-5}
        \cmidrule(l{15pt}r{15pt}){6-7}
         & @0.10 m& @0.60 m& @$5^{\circ}$ & @$30^{\circ}$  & @50 mm& @300 mm \\\midrule
        PlaneTR \cite{Tan:etal:ICCV2021:Planetr} &7.08/5.07  & 41.98/27.10 & 20.08/11.69 & 52.08/32.85 &9.06/5.71 & 35.64/22.75 \\
        % PlaneRecTR& 7.72/6.48 & 44.44/35.70 & 14.43/10.56 & 55.99/42.24 &\textbf{10.30/6.97} & 38.51/28.69 \\
        % PlaneRecTR (HRNet)& 8.99/8.06 & 48.60/39.33 & 19.91/14.06 & 58.58/45.27 & 10.04/6.81 & 41.52/31.55 \\
        PlaneRecTR& \textbf{7.72/6.48} & \textbf{44.44/35.70} & 14.43/10.56 & \textbf{55.99/42.24} &\textbf{10.30/6.97} & \textbf{38.51/28.69} \\
        PlaneRecTR (HRNet-32) 
        & \textbf{\textcolor{red}{8.99/8.06}} & \textbf{\textcolor{red}{48.60/39.33}} & 
        19.91\textbf{\textcolor{red}{/14.06}} & 
        \textbf{\textcolor{red}{58.58/45.27}} & 10.04/6.81 & 
        \textbf{\textcolor{red}{41.52/31.55}} \\
        PlaneRecTR (Swin-B) &\underline{\textbf{10.58/9.36}}  & \underline{\textbf{54.06/42.40}} & \underline{\textbf{24.08/16.42}} & \underline{\textbf{59.92/45.68}} & 8.94/6.41 & \underline{\textbf{44.30/33.10}} \\
        \bottomrule
    \end{tabular}
}
\vspace{5pt}
\caption{Per-pixel and per-plane recalls comparison on the NYUv2-Plane dataset.}
\label{tab:nyu_recall_sample_appendix}
\end{table}

\begin{table}[h]
\centering
\resizebox{1.0\linewidth}{!}{
    \begin{tabular}{cccc}
        \toprule
        \multirow{2}*{Method} & \multicolumn{2}{c}{Plane Parameter Estimation Errors $\downarrow$}\\
        \cmidrule(l{5pt}r{5pt}){2-3}
        ~ & Normal (°) & Offset (mm)  \\
        \midrule
        PlaneTR \cite{Tan:etal:ICCV2021:Planetr} & 17.09 & 615.92  \\
        % PlaneRecTR & 15.98 & 611.82  \\
        % PlaneRecTR (HRNet) & {15.55}  & {577.28} \\
        % PlaneRecTR (Swin-B) & \textbf{15.08}  & \textbf{553.47}  \\
        PlaneRecTR & \textbf{15.98}(-1.11) & \textbf{611.82}(-4.10)  \\
        PlaneRecTR (HRNet-32) & \textbf{\textcolor{red}{15.55}}(-1.54)  & \textbf{\textcolor{red}{577.28}}(-38.64) \\
        PlaneRecTR (Swin-B) & \underline{\textbf{15.08}}(-2.01)  & \underline{\textbf{553.47}}(-62.45)  \\
        % PlaneRecTR-SwinB & 0 & 0  \\
        \bottomrule
    \end{tabular}
}
\vspace{5pt}
\caption{Plane parameters estimation comparison on the NYUv2-Plane dataset.} %\vspace{-10pt}
\label{tab:nyu_para_error_appendix}
\end{table}





% Figure environment removed


\section{More Details of Figure \ref{fig:plane_recall}}
Since the performance of PlaneRecTR and PlaneRecTR (HRNet-32) in Figure \ref{fig:plane_recall} overlaps,  we show the specific values of some methods under lower and higher thresholds respectively in Table \ref{tab:scannet_recall_sample_appendix}.
\begin{table}[h]
\centering
\resizebox{1.0\linewidth}{!}
{    \renewcommand{\arraystretch}{1.3} % Default value: 1
    \begin{tabular}{ccccccc}
    \toprule
        \multirow{3}{2cm}{\centering {}\\{\large Method}} & \multicolumn{6}{c}{Per-Pixel/Per-Plane Recalls~$\uparrow$}  \\ 
         \cmidrule(l{15pt}r{15pt}){2-7}
        &\multicolumn{2}{c}{Depth}&\multicolumn{2}{c}{Normal}&\multicolumn{2}{c}{Offset}\\
        \cmidrule(l{15pt}r{15pt}){2-3}\cmidrule(l{15pt}r{15pt}){4-5}
        \cmidrule(l{15pt}r{15pt}){6-7}
         & @0.10 m& @0.60 m& @$5^{\circ}$ & @$30^{\circ}$  & @50 mm& @300 mm \\\midrule
        % PlaneNet \cite{Liu:etal:CVPR2018:Planenet} \\
        % PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} & \\
        PlaneTR \cite{Tan:etal:ICCV2021:Planetr} & 52.89/40.76  & 80.52/61.49 & 59.45/43.14 & 80.25/60.68 & 20.22/27.47 & 76.53/58.47 \\
        % PlaneRecTR& 53.07/45.07 & 83.60/72.84 & 62.75/48.48 & 83.85/71.33 & 36.98/29.89 & 78.95/67.13 \\
        % PlaneRecTR (HRNet)& 54.32/47.36 & 83.73/73.67 & 62.79/49.07 & 83.81/71.92 & 38.80/32.30 & 79.61/68.60 \\
        PlaneRecTR& \textbf{53.07/45.07} & \textbf{83.60/72.84} & \textbf{62.75/48.48} & \textbf{83.85/71.33} & \textbf{36.98/29.89} & \textbf{78.95/67.13} \\
        PlaneRecTR (HRNet-32)& \textbf{\textcolor{red}{54.32/47.36}} & 
        \textbf{\textcolor{red}{83.73/73.67}} & \textbf{\textcolor{red}{62.79/49.07}} & 83.81\textbf{\textcolor{red}{/71.92}} & \textbf{\textcolor{red}{38.80/32.30}} & \textbf{\textcolor{red}{79.61/68.60}} \\
        PlaneRecTR (Swin-B) & \underline{\textbf{57.74/50.03}}  & \underline{\textbf{85.34/75.49}} & \underline{\textbf{67.43/52.02}} & \underline{\textbf{85.19/73.67}} & \underline{\textbf{40.56/33.24}} & \underline{\textbf{81.89/70.74}} \\
        \bottomrule
    \end{tabular}
}
\vspace{5pt}
\caption{Per-pixel and per-plane recalls comparison on the ScanNet dataset.}
\label{tab:scannet_recall_sample_appendix}
\end{table}

\section{A Visual Comparison of PlaneRecTR and PlaneRecTR (SwinB)}


In Figure \ref{fig:plane_compare_appendix} we show qualitative results of our proposed methods in details.
When using a powerful backbone model, the network is able to predict more accurate plane segmentation masks (row 2,7,8), discriminate confusing nearu-by planes (row 3), and avoid over segmentation (row 4).
In it also exciting to see that PlaneRecTR (Swin-B) even discover small planes which are missed by PlaneRecTR(row 1,5,6) and the ground truth depths (row 1,5).


% Figure environment removed


\section{3D Segmentation Visualization }
In Figure \ref{fig:3DMask} and Figure \ref{fig:3DMask_compare}, we add 3D segmentation masks visualization for Figure \ref{fig:3DPlane} and Figure \ref{fig:plane_compare} of the text in order to better display 3D plane recovery results.



\section{More Visualization Results of PlaneRecTR}
We display more visual comparisons of our method on both datasets in Figure \ref{fig:more_vis_appendix}.
% Figure environment removed


% Figure environment removed



% Figure environment removed









\end{document}



