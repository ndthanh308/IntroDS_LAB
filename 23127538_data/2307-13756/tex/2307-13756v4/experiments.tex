\section{Single View Experiments}
\label{sec:experiments}

In this section, we first perform experimental evaluations on the monocular 3D plane recovery task with two public datasets. Then, we ablate the structural designs of our intra-frame component \jiajia{(PlaneRecTR)} to demonstrate that multiple sub-tasks of plane recovery can benefit each other through unified query learning.

\paraspace
\ptitle{Datasets.} 
We train and evaluate \jiajia{the monocular} component on two popular benchmarks: \jiajia{(1)} \textbf{ScanNetv1} \cite{Dai:etal:CVPR2017} dataset, which is a large-scale RGB-D video collection of 1,513 indoor scenes. We use piece-wise planar ground-truth  generated by PlaneNet \cite{Liu:etal:CVPR2018:Planenet}, which contains 50,000 training and 760 testing images with resolution 256 $\times$ 192\jiajia{; (2)} \textbf{NYUv2-Plane} \cite{Silberman:etal:ECCV2012} dataset, which is a planar variant of the original NYUv2 dataset \cite{Silberman:etal:ECCV2012} provided by \cite{Liu:etal:CVPR2018:Planenet}. 
% Different from the original NYUv2 data (795 training and 654 testing images) of size 640x480, data of NYUv2-Plane are also at resolution 256x192.
It has 654 testing images with resolution 256 $\times$ 192. 
% Following \cite{Tan:etal:ICCV2021:Planetr, Liu:etal:CVPR2019:Planercnn},  
% Please note that NYUv2-Plane is adopted here to highlight the generalization capability of plane recovery methods without any further fine-tuning.


%-------------------------------------------------------------------------
\paraspace
\ptitle{Evaluation Metrics.}
For the entire task of 3D plane recovery, we adopt per-plane and per-pixel recalls \jiajia{for evaluation} following \cite{Tan:etal:ICCV2021:Planetr, Yu:etal:CVPR2019:PlaneAE}. The per-plane/pixel recall metric is defined as the percentage of the correctly predicted ground truth planes/pixels. 
% A plane is considered being correctly predicted if its segmentation Intersection over Union (IoU) and depth (surface normal) error satisfy pre-defined thresholds. Specifically, the segmentation threshold of IoU is set to 0.5, while the error thresholds of depth and surface normal vary from 0.05m/2.5° to 0.6m/30°, with an increment of 0.05m/2.5°.
A plane is considered to be correctly predicted if its segmentation Intersection over Union (IoU), depth, normal and plane offset errors satisfy pre-defined thresholds. Specifically, the \jiajia{IoU} threshold is set to 0.5, while the error thresholds of depth/surface normal vary from 0.05m/2.5° to 0.6m/30°, with an increment of 0.05m/2.5°.

To evaluate plane segmentation, we apply three popular metrics\cite{Arbelaez:etal:TPAMI2010, yang2018recovering}: variation of information (VI), rand index (RI) and segmentation covering (SC). For plane parameter, we \jiajia{find} the best match by minimizing the $L_1$ cost between $K$ predicted planes and $M$ ground truth planes, and then separately compute the average errors of plane normal and offset. As to the NYUv2-Plane dataset, depth accuracy is evaluated on structured planar regions.

\input{tables/planerectr_variants}
\input{figures/planerectr_scannet_3dres}


%-------------------------------------------------------------------------
\paraspace
\ptitle{Network Variants.}
%We note a few important architectural differences w.r.t. model designs and learning tasks. 
To reflect the contributions of our key design choices, we consider (1) explicit plane-level binary mask prediction (M), (2) plane-level depth prediction (D), (3) plane parameter estimation (P), and thus denote network varieties in Table \ref{tab:planerectr_variants}.



%-------------------------------------------------------------------------
\subsection{Implementation Detail}
\label{sec:monocular_imp_detail}
Our intra-frame component is implemented using Detectron2 \cite{wu2019detectron2}. We train it on the ScanNetv1 training set with a total of 34 epochs on a single NVIDIA TITAN V GPU. We use AdamW optimizer \cite{loshchilov2017decoupled} with an initial learning rate of $0.0001$ and a weight decay of $0.05$. The batch size is set to $16$.



%-------------------------------------------------------------------------
\subsection{Results on the ScanNetv1 Dataset}
% \paraspace
\ptitle{Qualitative Results.}
\jiajia{In Figure \ref{fig:planerectr_scannet_3dres}}, we present qualitative results of our single-view plane reconstruction on a variety of unseen ScanNetv1 scenes in both 2D and 3D domains. \jiajia{Our method} is able to predict accurate plane segmentation masks and plane parameters, as well as reasonable 3D plane reconstructions. We further show detailed visual comparisons to \jiajia{the leading} PlaneTR \cite{Tan:etal:ICCV2021:Planetr} in Figure \ref{fig:planerectr_compare_3dres}. Although PlaneTR integrates extra structural cues like line segments, it is exciting to see that our approach, mainly benefiting from joint modelling of 
% query learning of plane parameter, segmentation and depth,
plane geometry and segmentation, 
% is able to predict crisper plane segmentation masks (row 1-2 of Figure \ref{fig:planerectr_compare_3dres}) and discriminate con planes with similar normals (row 2 of Figure \ref{fig:planerectr_compare_3dres}), with more complete and holistic structures.
is able to predict crisper segmentation and discriminate planes sharing similar normals 
% (rows 2-3, 5-6, 8-9 of Figure \ref{fig:planerectr_compare_3dres})
(columns (b), (c))
, with more complete and holistic structures.

\input{tables/planerectr_scannet_nyu_seg}
\input{tables/planerectr_nyu_depth}
\input{figures/planerectr_compare_3dres}

\input{figures/planerectr_scannet_recall}
%-------------------------------------------------------------------------
\paraspace
\ptitle{Quantitative Results.}
We conduct extensive quantitative evaluations towards previous state-of-the-art learning-based plane recovery methods: PlaneNet \cite{Liu:etal:CVPR2018:Planenet}, PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}, PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} and PlaneTR \cite{Tan:etal:ICCV2021:Planetr}. Like \cite{Tan:etal:ICCV2021:Planetr}, PlaneRCNN is shown here mainly as a reference because of its learning with a different ScanNetv1 training set-up. 
%We thus focus on numerical comparison towards other recent baselines. 
We use public implementation of PlaneTR \cite{Tan:etal:ICCV2021:Planetr} and its provided pre-trained weights to report corresponding performance. 

In terms of segmentation accuracy, Table \ref{tab:planerectr_scannet_nyu_seg} shows that on the challenging ScanNetv1 we achieve a new state-of-the-art plane segmentation performance, outperforming \jiajia{the} leading PlaneTR with a relative large margin especially in the VI metric. To further demonstrate the flexibility of our method, we have shown improved versions of PlaneRecTR by replacing ResNet-50 backbone \cite{He:etal:CVPR2016} with the same HRNet-32 backbone \cite{WangSCJDZLMTWLX19:HRNet} as PlaneTR or a SwinTransformer-B model \cite{liu2021swin}. The performance gap widens and shows that our framework could benefit from ongoing research in developing more powerful fundamental vision models.

\input{figures/planerectr_backbones}

As to 
% geometry performance including plane parameter estimation and plane-level depth prediction,
performance of entire plane recovery task, 
we display per-pixel and per-plane recalls of depth and plane normal on the ScanNetv1 dataset, respectively (see Figure \ref{fig:planerectr_scannet_recall}).
With varying thresholds from 0.05 to 0.6 meters and from 2.5 to 30 degrees for depth and normal evaluations, our method consistently outperforms all baselines, indicating more precise predictions of plane parameters, segmentation mask and planar depths. We want to further highlight the significant improvement w.r.t. per-plane recall, our methods could efficiently discover structural planes of various scales, in contrast to \cite{Tan:etal:ICCV2021:Planetr} which tends to miss planes with small areas or sharing similar geometry (see Figure \ref{fig:planerectr_compare_3dres}).








%-------------------------------------------------------------------------
\subsection{Results on the NYUv2-Plane Dataset}


The NYUv2-Plane dataset is chosen here mainly to verify the generalization capability of our method on unseen novel scenes. As shown in Table \ref{tab:planerectr_scannet_nyu_seg}, our method still achieves leading plane segmentation accuracy in all metrics without any fine-tuning. Please check the bottom part of Figure \ref{fig:planerectr_compare_3dres} for more detailed visual comparisons.
% In terms of depth prediction task,
In Table \ref{tab:planerectr_nyu_depth},
we focus on pixel-wise depth accuracy in planar regions, thus we treat PlaneTR \cite{Tan:etal:ICCV2021:Planetr} as the targeting baseline for a fair comparison and others as reference.
Our base pipeline performs on-par with PlaneTR and outperforms PlaneNet \cite{Liu:etal:CVPR2018:Planenet} and PlanAE \cite{Yu:etal:CVPR2019:PlaneAE}. When our backbone is replaced from ResNet-50 \cite{He:etal:CVPR2016} to the same HRNet-32 \cite{WangSCJDZLMTWLX19:HRNet} as PlaneTR, our performance is significantly better than PlaneTR. PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}, with a ResNet-101 backbone, achieves better performance, partly caused by its utilization of neighbouring multi-view information and higher resolution images during training, while ours is trained with a single image without using extra cues \cite{Liu:etal:CVPR2019:Planercnn,Tan:etal:ICCV2021:Planetr}. 
Nevertheless, we have found that the benefit using more discriminative backbones (\eg, HRNet-32, Swin-B) also transfers to our generalization ability in novel scenes, resulting in a huge performance boost to reach a leading planar depth accuracy. 
%We present more evaluation results of 3D plane recovery on the NYUv2-Plane dataset in Appendix. 



\input{tables/planerectr_all_ablation}

\input{figures/ablation_planerectr_mask_compare}

%-------------------------------------------------------------------------
\subsection{Ablation Studies}
\label{sec:ablation}
\jiajia{We own the above improvements to the explicit joint modelling of plane geometry and segmentation via our unified query learning.} In this section, we conduct detailed ablation studies on the ScanNetv1 dataset and show the effectiveness of the key designs.

\paraspace
\ptitle{Effects of Jointly Predicting Plane-Level Depths.}
One key difference to previous Transformer-based \cite{Tan:etal:ICCV2021:Planetr} and CNN-based baselines \cite{Liu:etal:CVPR2018:Planenet,Yu:etal:CVPR2019:PlaneAE,Liu:etal:CVPR2019:Planercnn} is that, instead of learning monocular (planar) depth prediction in an individual branch, we predict plane-level depths based on shared representation for plane detection, segmentation and parameter prediction. It turns out that this simple design choice leads to promising performance increment, achieve mutual benefits among tasks. Bottom two rows of Table \ref{tab:planerectr_all_ablation} show that augmenting depth prediction task to query learning affects per-pixel and per-plane recalls of depth and normal, plane parameter errors. It is worth noting the significant gain in recall rates especially under the strict (lower) thresholds.  

\paraspace
\ptitle{Effects of Predicting Dense Plane-Level Masks.}
Compared to \jiajia{the} Transformer-based PlaneTR, we  differ in obtaining plane-level masks via a dense prediction instead of post-clustering from dense embeddings and depth information. Rightmost two columns of Table \ref{tab:planerectr_all_ablation} have shown that explicitly learning per-plane masks significantly boosts the accuracy of plane parameter estimation of our method, even when direct depth supervision is not available.





\paraspace
\ptitle{Plane Segmentation and Geometry Prediction.}
We also investigate whether the powerful segmentation framework could further benefit from learning plane geometry (plane parameters and depths). The experiment results show that there are marginal numerical gains \jiajia{for} plane segmentation \jiajia{when} comparing \jiajia{PlaneRecTR} to PlaneRecTR (-P-D). Fortunately, we do observe clear qualitative improvements. As shown in Figure \ref{fig:ablation_planerectr_mask_compare}, our complete model manages to distinguish detailed planar structures and even predict more fine-grained and sound plane segmentation than ground truth annotation, which, however, could possibly be penalized during quantitative evaluation (bottom row of Figure \ref{fig:ablation_planerectr_mask_compare} where bedstead labelling is missing).




\paraspace
\ptitle{Backbone Impacts.}
Like other vision frameworks, the backbone feature extractor also has an active role in the final performance of PlaneRecTR. We see this as an exciting advantage, since our concise architectural design continues to benefit from ongoing research in fundamental vision models. PlaneRecTR exhibits a stable performance increase and achieves superior SOTA performance when adopting more powerful backbones such as HRNet32 and SwinB, as shown in Table \ref{tab:planerectr_scannet_nyu_seg} and \ref{tab:planerectr_nyu_depth} \jiajia{as well as Figure \ref{fig:planerectr_scannet_recall} and \ref{fig:planeretr_backbones}}.
