%!TEX root = geotrans.tex

\section{Related Work}
\label{sec:related}
\subsection{3D Plane Recovery from a Single Image}
Recovering monocular planes enables 3D planar reconstruction and structural scene understanding. Traditional methods often rely on strong assumptions of scenes \cite{delage2007automatic, fouhey2014unfolding} (\eg, the Manhattan world assumption), or require manual extraction of primitives \cite{delage2007automatic,fouhey2014unfolding, lee2009geometric}, such as superpixels and line segments, which may not be applicable to complex real-world scenes. 

PlaneNet \cite{Liu:etal:CVPR2018:Planenet} is the first to propose an end-to-end learning framework for this task, it also releases a large dataset of planar depth maps utilizing the ScanNetv1 dataset \cite{Dai:etal:CVPR2017}. PlaneRecover \cite{yang2018recovering} presents an unsupervised learning approach that specifically targets outdoor scenes. However, both PlaneNet and PlaneRecover can only predict a fixed number of planes. PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} tackles this limitation and extracts an arbitrary number of planes with planar parameters and segmentation masks using a proposal-based instance segmentation framework, \ie, Mask R-CNN \cite{He:etal:ICCV2017}. It also proposes a segmentation refinement network as well as a warping loss between frames to improve performance.
These proposal-based methods require multiple steps to successively tackle sub-tasks of 3D plane recovery including plane detection, segmentation, parameter and depth estimations, \etc. 

On the other hand, PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} leverages a proposal-free instance segmentation approach, which uses mean shift clustering to group embedding vectors within planar regions. PlaneTR \cite{Tan:etal:ICCV2021:Planetr} inherits the design of DETR \cite{carion2020end} to concurrently detect plane instances and estimate plane parameters, followed by plane segmentations generated by a pixel clustering strategy like PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE}. 
Specifically, its Transformer branch only predicts instance-level plane information, thus post-processing like clustering is still required to carry out pixel-wise segmentation. The global depth is inferred by another convolution branch. 
 
Therefore, existing advanced methods, whether based on direct CNN prediction or embedding clustering, still divide the whole 3D plane recovery task into several steps. In contrast, our plane query learning offers a unified solution for the aforementioned sub-tasks within the intra-frame component and can be seamlessly extended to address inter-frame requirements in an end-to-end manner.

\subsection{3D Planar Reconstruction from Sparse Views}

Built upon advancement in monocular plane recovery, numerous works \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac} have emerged to address the challenging two-view planar reconstruction with unknown camera poses, aiming to build a coherent 3D planar reconstruction.

SparsePlanes \cite{jin2021sparseplanes} is the first learning-based approach for planar reconstruction and pose estimation from sparse views. 
\jiajia{With monocular planes derived from PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} and $1024$ pose hypotheses estimated from a dense pixel attention network, SparsePlanes employs them for a complex two-step optimization to calculate plane correspondences and a final pose.}
Furthermore, PlaneFormers \cite{agarwala2022planeformers} utilizes predicted monocular planes and top \jiajia{$9$} pose hypotheses from SparsePlanes as input, \jiajia{and replaces} the handcrafted optimization by \jiajia{$9$} learnable \jiajia{planeformer} modules, mitigating the \jiajia{intricate} optimization issue \cite{jin2021sparseplanes}.  
NOPE-SAC \cite{tan2023nopesac} improves monocular plane quality by replacing PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} with a modified PlaneTR \cite{Tan:etal:ICCV2021:Planetr}, and 
achieves an initial coarse pose from direct regression using the similar pixel attention \cite{jin2021sparseplanes}.
It also introduces differentiable optimal transport \cite{sarlin2020superglue} for plane matching and proposes one-plane pose hypotheses based on corresponding plane pairs, resolving conflicts between numerous pose hypotheses and limited 3D plane correspondences during SparsePlanes' pose refinement.

However, both PlaneFormers and the leading \jiajia{NOPE-SAC} still adopt a multi-stage pipeline derived from SparsePlanes, requiring bootstrapping from external initial pose and correspondence supervision. 
The reason behind this lies in the existence of a chicken-and-egg relationship between explicitly learning camera pose and plane correspondence.  In all previous methods \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}, the capability of plane embeddings learned by ground truth correspondence supervision is \jiajia{still} insufficient to accurately track the same plane instance under sparse views.  An additional model is necessary to provide the initial pose(s), so that monocular \jiajia{planes} could be merged under a unified coordinate system, thereby assisting the original plane embedding in enhancing matching accuracy and ultimately refining the initial pose.
Our proposed PlaneRecTR++ deviates from such dilemma using unified plane query learning, actively inferring plane estimations, correspondences, and camera pose in a single-stage framework, \textit{without relying on initial pose guidance and supervision for plane matching}.


\subsection{Correspondence and Camera Pose Estimation}

Camera pose estimation between adjacent images is a fundamental step in multi-view 3D reconstruction\cite{hartley2003multiple}. Early studies focused primarily on extracting sparse keypoint correspondences \cite{Lowe:IJCV2004, bay2008surf} to compute the essential matrix using the five-point solver \cite{nister2004fivepoint}. 
Subsequently, significant efforts have been devoted to learning-based approaches for robust keypoint detection \cite{detone2018superpoint, dusmanu2019d2net, tyszkiewicz2020disk} and matching \cite{sarlin2020superglue, sun2021loftr, zhou2020learn}. However, pose computed solely from the essential matrix lacks a real scale and causes further challenges in the sparse view setting, where only a limited number of correct correspondences could be found.

Moreover, existing methods for directly learning relative pose from images usually require concatenating pairwise frames \cite{en2018rpnet} or computing affinity volume\cite{cai2021extreme, jin2021sparseplanes, tan2023nopesac}, resulting in a significant computational burden. Additionally, these methods either rely on extensively overlapping images\cite{en2018rpnet, cai2021extreme}, or adapt to sparser views but yield limited precision, thus serving solely as an initial pose prior \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}.
Recently proposed Pose Vit \cite{rockwell20228posevit}, a Transformer structure with tokenized uniform image patches as well as their quadratic positional bias as input, attempts to learn proximal patch correspondence information using attention mechanism, ultimately allowing a direct regression of rotation and translation with scale in a wide baseline.  


Our approach draws inspiration from attention-based modules \cite{sarlin2020superglue, sun2021loftr,rockwell20228posevit}, but further extends the standard cross attention by a plane-aware attentive design. With our learned plane embeddings as the only input for pose prediction, our method implicitly learns genuine plane correspondences and is able to recover precise camera poses within a compact module.


