\input{figures/planerectr_network}

% \section{PlaneRecTR(Single View Plane Recovery)}
\section{Intra-Frame Plane Query Learning}
\label{sec:intra_frame_method}

In this section, we present the details of intra-frame plane query learning, \ie, PlaneRecTR. We start by first introducing its architecture in Section~\ref{sec:model}, and then discuss the training process and loss functions in Section \ref{sec:training}. Finally, we describe the inference process of recovering 3D planes from a single view in Section \ref{sec:planeinference}.


%-------------------------------------------------------------------------
% \subsection{Unified Query Learning for Plane Recovery}
% \subsection{PlaneRecTR Module}
\subsection{Transformer-based Unified Query Learning for Single-view Plane Recovery}
\label{sec:model}


Inspired by the successes of DETR \cite{carion2020end} and Mask2Former \cite{cheng2022masked} in object detection and segmentation, we find that, it is \jiajia{feasible} to tackle the challenging monocular planar reconstruction task using a \textit{single}, \textit{compact} and \textit{unified} framework, thanks to the merits of query-based reasoning \jiajia{for enabling} joint modeling of multiple tasks.

As shown in Figure \ref{fig:planerectr_network}, intra-frame plane query learning component consists of three main modules: (1) A pixel-level module to learn dense pixel-wise deep embedding of the input colour image. (2) A Transformer-based unified query learning module to jointly predict, for each of $N$ learnable plane queries, its corresponding plane embeddings $\mathcal{E}_\text{plane}$ as well as four target properties, including plane classification probability $p_i$, plane parameter $n_i$, mask embedding, and depth embedding ($i\in[1,2,..., N]$). Specifically, $p_i$ is the probability to judge whether the $i^\text{th}$ query corresponds to a plane or not; $n_i\doteq \tilde{n_i}/d_i \in \mathbb{R}^3$,  where $\tilde{n_i} \in \mathbb{R}^3$ is its plane normal and $d_i$ is the distance from the $i^\text{th}$ plane to camera center, \ie, offset. (3) A plane-level module to generate plane-level mask $m_i$ and plane-level depth $d_i$ through mask and depth embedding ($i\in[1,2,..., N]$). We then remove non-plane query hypothesis while combining the remaining ones for the final image-wise plane recovery. These three modules will be described in detail below.



\paraspace
\ptitle{Pixel-Level Module.}
Given an \jiajia{input} image of size $H \times W$, we use the pre-trained ResNet-50 \cite{He:etal:CVPR2016} as backbone to extract dense image feature maps, unless otherwise mentioned. \jiajia{Subsequently}, a multi-scale convolutional pixel decoder \jiajia{\cite{cheng2022masked}} is used to produce a set of dense feature maps with four scales, denoted as follows:
\begin{equation}
\begin{aligned}
\setlength{\abovedisplayskip}{3pt}
\setlength{\belowdisplayskip}{3pt}
\mathbb{F}&=\{F_{1}\in \mathbb{R}^{C_{1}\times H/32\times W/32}, F_{2}\in \mathbb{R}^{C_{2}\times H/16\times W/16}, \\
& F_{3}\in \mathbb{R}^{ C_{3}\times H/8\times W/8}, \mathcal{E}_\text{pixel}\in \mathbb{R}^{ C_{\mathcal E}\times H_{\mathcal E}\times W_{\mathcal E}}\},
\end{aligned}
\end{equation}
\jiajia{where $C_1$, $C_2$, $C_3$, $C_{\mathcal E}$ are feature dimensions.} The first three feature maps $\{F_{1}, F_{2}, F_{3}\}$ are fed to the Transformer module, while the last one $\mathcal{E}_\text{pixel}$, a dense per-pixel embedding of resolution $H_{\mathcal E}=H/4$ and $W_{\mathcal E}=W/4$, is exclusively used for computing plane-level binary masks and plane-level depths.

\paraspace
\ptitle{Transformer Module.}
We use the Transformer decoder with masked attention proposed in \cite{cheng2022masked}, which computes \textit{unified plane embeddings} $\mathcal{E}_\text{plane} \in \mathbb{R}^{N \times C_{\mathcal E}}$ from above mentioned multi-scale feature maps $\{F_{1}, F_{2}, F_{3}\}$ and $N$ learnable plane queries. The predicted $\mathcal{E}_\text{plane}$ are then independently projected to four target properties by four MLPs. Overall, the Transformer module predicts required planar attributes through $N$ plane queries. %(upper left part in Figure \ref{fig:planerectr_network}).

\paraspace
\ptitle{Plane-Level Module.}
 As shown in Figure \ref{fig:planerectr_network}, we obtain a dense plane-level binary mask $m_i \in [0, 1]^{H_{\mathcal E} \times W_{\mathcal E}}$/depth prediction $d_i \in \mathbb{R}^{H_{\mathcal E} \times W_{\mathcal E}}$ by a dot product between the $i^\text{th}$ mask/depth embedding and the dense per-pixel embedding $\mathcal{E}_\text{pixel}$ from previous two modules, respectively.
% Note that we have also investigated the idea of learning two individual pixel decoders respectively for semantic and depth embeddings.
% %, in order to separately predict planar masks and depths. 
% However, we did not observe a clear improvement, and therefore stick to current set-up with a shared $\mathcal{E}_\text{pixel}$ for the sake of efficiency and simplicity.
We finally obtain $N$ plane-level predictions $\{y_i=(p_i,n_i, m_i, d_i)\}_{i=1}^N$, \jiajia{each of which} contains all the necessary information to recover a possible 3D plane.


%-------------------------------------------------------------------------
\subsection{Training Objective and Configuration}
\label{sec:training}
% \paraspace
\ptitle{Plane-level Depth Training.}
Previous methods tend to predict a global image-wise depth by separate network branches to calculate plane offset \cite{Liu:etal:CVPR2019:Planercnn} or use depth as additional cues to formulate segmentation \cite{Yu:etal:CVPR2019:PlaneAE,Tan:etal:ICCV2021:Planetr}.
In contrast, our method tries to achieve mutual benefits between planar semantic and geometric reasoning, 
\jiajia{and leverages learnable plane queries} to unify all components of plane recovery in a concise multi-task manner. As a result, we explicitly predict dense plane-level depths, binary-masks, plane probabilities and parameters from a shared feature space, which is produced and refined via attention mechanism of the Transformer.

\paraspace
\ptitle{Bipartite Matching.}
During training, one important step is to build optimal correspondences between $N$ predicted planes and $M$ ground truth planes ($N \geq M$).
Following bipartite matching of \cite{cheng2022masked, Tan:etal:ICCV2021:Planetr}, we search for a permutation $\hat{\sigma}$ 
by minimizing a matching defined cost function $D$:
\begin{equation}
\label{eq: bi-matching}
\hat{\sigma}=\underset{\sigma}{\arg \min } \sum_{i=1}^{N} D\left(\hat{y}_{i}, y_{\sigma(i)}\right), 
\end{equation}
\vspace{-0.5cm}
\begin{align}
\label{eq: bi_matching2}
D= & \mathbbm{1}_{\{\hat{p}_{i}=1\}} \Big[
 -\omega_{1} \, p_{\sigma(i)}
+ \omega_{2} L_{1}\left(\hat{{n}}_{i}, {n}_{\sigma(i)}\right) \nonumber\\
& + \omega_{3} L_{1}\left(\hat{{d}}_{i}, {d}_{\sigma(i)}\hat{m}_{i}\right) + \omega_{4} L_{ce}  +  \omega_{5} L_{dice} \Big],
\end{align}
where $\hat{y}_{i} = (\hat{p}_i,\hat{n}_i, \hat{m}_i, \hat{d}_i)$ are the $i^\text{th}$ ground-truth plane attributes, we augment the ground truth instances with non-planes where $\hat{p}_{i}=0$ if $i\textgreater M$; $\sigma(i)$ indicates the matched index of the predicted planes to the ground truth $\hat{y}_{i}$; $\mathbbm{1}$ is an indicator function taking 1 if $\hat{p}_{i}=1$ is true and 0 otherwise; $\omega_{1}, \omega_{2}, \omega_{3}, \omega_{4}$ and $\omega_{5}$ are weighting terms and set to 2, 1, 2, 5, 5, respectively. 
Here we additionally consider the influence of mask and depth quality using a mask binary cross-entropy loss $L_{ce}$ \cite{cheng2022masked}, a mask dice loss $L_{dice}$ \cite{milletari2016v} and an $L_1$ depth loss, respectively.

\paraspace
\ptitle{Loss Functions.}
After bipartite matching, the final training objective $L$ is composed of \jiajia{the} following four parts:
% \vspace{-0.3cm}
\begin{equation}
\small 
\mathcal{L} = 
\sum\limits_{i=1}^{M}\left( \lambda\mathcal{L}_{\text {cls }}^{(i)} + \mathcal{L}_{\text {param }}^{(i)} + \mathcal{L}_{\text{mask}}^{(i)} + \lambda\mathcal{L}_{\text {depth}}^{(i)} \right),
\end{equation}
where $\lambda$ is a weighting factor and is set to 2 in this paper. $\mathcal{L}_{\text {cls}}$ and $\mathcal{L}_{\text {param}}$ are a plane classification loss and a plane parameter loss, in a similar form to previous work \cite{Tan:etal:ICCV2021:Planetr}.

However, different from PlaneTR \cite{Tan:etal:ICCV2021:Planetr}, the remaining two loss terms in our paper $\mathcal{L}_{\text{mask}}$ and $\mathcal{L}_{\text {depth}}$ are designed to explicitly learn dense planar masks and depths. Specifically,
we introduce the plane segmentation mask prediction loss, as a combination of a cross-entropy loss and a dice loss:
\begin{equation}
% \label{eq:mask}
\mathcal{L}_{\text {mask }}^{(i)} =
 \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{1} L_{ce} + \nonumber
 \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{2} L_{dice},
\end{equation}
where $\beta_{1}, \beta_{2} = 5$.
The depth loss is in a typical $L_1$ form, penalizing the discrepancy of depth value within planar regions:
\begin{equation}
% \label{eq:mask}
\mathcal{L}_{\text {depth }}^{(i)} =
 \mathbbm{1}_{\{\hat{p}_{i}=1\}}  L_{1}\left(\hat{{d}}_{i}, {d}_{\sigma(i)}\hat{m}_{i}\right),
\end{equation}

%-------------------------------------------------------------------------
\subsection{Inference Process of Monocular 3D Plane Recovery}
\label{sec:planeinference}


For the $N$ plane-level predictions $\{y_i\}_{i=1}^N$ predicted by the network, we first drop non-plane candidates according to the plane classification probability $p_i$, leading to a valid subset of $K$ planes (\ie, $K\leq N$). 
For each pixel within planar regions, we calculate 
the most likely plane index
$\mathop{\arg\max}\limits_{i}\{m_i\}_{i=1}^{K}$ to obtain the final image-wise segmentation mask.
%In this simple and efficient manner, we can use the network predictions to create 3D plane reconstruction of the input frame. 
Note that plane-level depths are not involved during inference and we use plane parameters and segmentation to infer planar depths. 
We experimentally found that this design also leads to more structural and smooth geometric predictions than that relying on direct depth predictions.