%!TEX root = geotrans.tex

\IEEEraisesectionheading{\section{Introduction}\label{sec:intro}}
\IEEEPARstart{I}{mmersing} in a virtual 3D world involves efficient reasoning of surrounding scenes, whose properties need to be frequently updated. Though tremendous efforts have been devoted to create authentic 3D geometry from multi-view image observations or even a single image, a trade-off still exists between reconstruction quality and efficiency, depending on the underlying scene representations. 
3D maps composed of sparse primitives such as point clouds are light-weight to maintain but lack topological structures, while dense geometry like volumetric grids and meshes are computationally intensive to acquire and maintain. In this spectrum, planar representation has been proven to be a reliable alternative, which is compact, efficient, expressive, and generalizable enough to be deployed ubiquitously as well. Therefore, in practice it would always be ideal to infer planar information purely from a video sequence, or even a single image.
Being a challenging and yet fundamentally ill-posed computer vision problem, single image plane recovery has been extensively researched and focused so far. 
% lines, segments -> line segments, vanishing points
Early attempts have been made using image processing techniques to extract low-level primitives such as line segments, vanishing points to extract planar structures from an input image \cite{delage2007automatic, lee2009geometric}. Furthermore, multi-view 3D plane reconstruction \cite{Salas-Moreno:etal:ISMAR2014,hsiao2018dense} are investigated where plane-related constraints are introduced to regularize both camera poses as well as dense geometry, \eg, the well-known Manhattan-world assumption \cite{li2023hong,zhou2024neural}.

\input{figures/teaser}

As Convolutional Neural Networks (CNNs) have become the mainstream paradigm to tackle computer vision problems in the past few years \cite{xia2022metalearning,xia2024blind}, their excellence in performance gradually spreads to plane estimation task. Pioneering works such as PlaneNet \cite{Liu:etal:CVPR2018:Planenet} and PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} propose an efficient solution of piece-wise planar structure recovery from a single image using CNNs in a top-down manner.  There have also been bottom-up solutions such as PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} and PlaneTR \cite{Tan:etal:ICCV2021:Planetr} which obtain plane-level masks by a post-clustering procedure on top of learned deep pixel-wise embeddings. Recently, as another emergent fundamental paradigm, Transformers \cite{Vaswani:etal:NIPS2017} have made great progress on a wide range of vision tasks \cite{wang2020axial, dosovitskiy2020image, wang2022towards}. The success of vision Transformers not only comes from its realization of global/long-range interaction across images via attention mechanisms, another important factor is the design of query-based set predictions, initially proposed in Detection Transformer (DETR) \cite{carion2020end} to enable reasoning between detected instances and their global context, which has been further proven to be particularly effective in high-level vision tasks such as instance and semantic segmentation \cite{cheng2022masked, xu2022fashionformer}, video panoptic segmentation \cite{weber2021step, yuan2022polyphonicformer}, \etc.
PlaneTR \cite{Tan:etal:ICCV2021:Planetr} is one early attempt of using such ideas in single-view image plane reconstruction. It is also inspired by structure-guided learning to integrate additional geometric cues like line segments during training, leading to state-of-the-art performance on the ScanNetv1 dataset and the unseen NYUv2-Plane dataset. Based on high-quality monocular plane predictions, following works have explored their extension to establish a plane-aware structure from motion framework, involving across-view plane correspondence learning and camera pose estimation \cite{jin2021sparseplanes,rockwell20228posevit,agarwala2022planeformers,tan2023nopesac}. 

However, all of the above mentioned single/multi-view plane recovery methods somewhat disentangle the prediction of principle components required for plane reconstruction.
\jiajia{For} single-view \jiajia{methods}, PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} learns to predict plane offset from a monocular depth prediction branch while other attributes such as plane mask and normal are estimated separately from colour images, PlaneTR \cite{Tan:etal:ICCV2021:Planetr} also predicts a monocular depth map apart from the Transformer module, which is later used for acquiring plane segmentation masks using clustering associate embedding \cite{Yu:etal:CVPR2019:PlaneAE}. 
For multi-view ones, SparsePlanes \cite{jin2021sparseplanes}, PlaneFormers \cite{agarwala2022planeformers} and NOPE-SAC \cite{tan2023nopesac} inherit the above multi-step methods as the monocular plane predictor and further adopt a two-stage framework. Initially, they compute planes of two frames in the same coordinate system (from the predicted initial pose). Subsequently, complex manual design optimization algorithms \cite{jin2021sparseplanes} or additional neural modules \cite{agarwala2022planeformers, tan2023nopesac} are employed to achieve plane correspondence and refine pose hypotheses by incorporating externally provided camera states and explicit correspondence ground truth.
Accordingly, these closely related prediction tasks are usually interleaved, and none of them successfully unifies multi-view plane recovery within a single compact model. We conjecture this could be one performance bottleneck for existing data-driven approaches.

Motivated by this finding, we seek to borrow recent advance in query learning and aim to design a single, compact and unified model to jointly learn all plane-related tasks, and we expect such design would achieve a mutual benefits among tasks and advance the existing performance of both monocular and multi-view plane segmentation and reconstruction. Extensive experiments results on the ScanNet, MatterPort3D and NYUv2-Plane benchmark datasets show that \textit{without using any external priors} \cite{Liu:etal:CVPR2019:Planercnn, Tan:etal:ICCV2021:Planetr, jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac} during training, our unified querying learning model, named PlaneRecTR++, achieves new state-of-the-art performance with a concise structure. Additionally, we have also found that such framework can implicitly discover the spatial plane correspondences to enable precise 3D plane reconstruction.

To summarize, our contributions are as follows:
\begin{itemize}\compresslist
    \item We propose a first single unified framework to address the challenging 3D plane recovery task, where all closely related sub-tasks are jointly optimized and inferred in a multi-task manner motivated by query-based learning.

    \item Without any external priors and supervisions other than input images, we propose a novel plane-aware attention structure to first tackle sparse view plane reconstruction in a purely end-to-end manner, producing robust cross-view plane correspondences and camera poses.
    
    \item  Our proposed method achieves significant gains in terms of model performance and compactness. Extensive numerical and visual comparisons on four public benchmark datasets demonstrate state-of-the-art performance of our proposed unified query learning, taking full advantages of plane-related cues to achieve mutual benefits.
\end{itemize}

A previous version of this work was published at ICCV2023 \cite{shi2023planerectr}. This paper extends the conference version with the following new contributions. First, to amplify the effectiveness of unified query learning in plane reconstruction, we extend PlaneRecTR \cite{shi2023planerectr} to jointly tackle multi-view plane recovery and camera pose estimation in a purely end-to-end manner, \ie PlaneRecTR++, enabling superior performance on the ScanNetv2 and MatterPort3D datasets. Second, we propose a plane-aware cross attention module to implicitly learn plane correspondences, achieving mutual benefits without requiring pose initialization and correspondence labelling. Third, we conduct extensive and comprehensive experiments with detailed ablation analysis to provide a thorough understanding of PlaneRecTR++. 
\jiajia{The plane embeddings guided by query learning from PlaneRecTR++ not only retain comparable single-view plane recovery capability to \cite{shi2023planerectr}, but also exhibit superior cross-view consistency, enabling precise plane matching and pose inference.}

