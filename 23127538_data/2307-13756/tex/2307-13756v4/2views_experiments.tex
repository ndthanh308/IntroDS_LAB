\section{Sparse View Experiments}
\label{sec:corrattn_experiments}

\paraspace
\ptitle{Datasets.}
We adopt the setup of various existing works \cite{jin2021sparseplanes, tan2023nopesac} to ensure a fair comparison, 
\jiajia{which includes two large-scale sparse view datasets as benchmarks.}

\textbf{ScanNetv2 Dataset.}
% We utilized the indoor ScanNetv2 \cite{Dai:etal:CVPR2017, Dai:scannetv2web} video dataset, which includes plane annotations generated by \cite{Liu:etal:CVPR2019:Planercnn}. 
We use a more challenging sparse view split of the ScanNetv2 dataset created by \cite{Dai:etal:CVPR2017,Dai:scannetv2web,Liu:etal:CVPR2019:Planercnn,tan2023nopesac}, consisting of 17,237/4,051 image pairs from 1,210/303 non-overlapping scenes for training/testing. 
% We denote it as ScanNetv2 to make a distinction between the ScanNetv1 in Section \ref{sec:experiments} for monocular plane recovery. 
Different to ScanNetv1 for monocular plane recovery, ScanNetv2 was proposed to contain a lower overlapping ratio between frames and more complex camera rotation distributions. The average overlap ratio of adjacent frames were 20.6\% and 18.6\% in the training and test sets, respectively. The image size is 640 × 480 for all baselines.

\textbf{MatterPort3D Dataset.}
In contrast to ScanNetv2, the sparse view version of the MatterPort3D dataset\cite{chang2017matterport3d} is created in a semi-simulated manner, whose RGB images are rendered from approximated 3D planar meshes during the generation process of plane annotations \cite{jin2021sparseplanes}, thereby mitigating real-world impact like lighting variation. 
However, these rendered RGB images consist solely of planes along with numerous small erroneous facets caused by approximated 3D planar meshes, increasing \jiajia{the difficulty for accurate plane detection. Therefore, evaluations on MatterPort3D may provide unfair advantages to all two-stage baselines \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac} with a dense pixel-based pose initialization, and cause challenges to plane-based methods (like ours). We have confirmed such findings in subsequent experiments and regard MatterPort3D mainly as a complement benchmark compared to ScanNetv2 dataset.} The training set and testing set consist of 31,932 and 7,996 image pairs, respectively, exhibiting an overlap ratio of about 21.0\% \cite{tan2023nopesac}. The image size is also kept to 640 × 480.

\paraspace
\ptitle{Evaluation Metrics.}
To evaluate relative camera pose, we use geodesic distance and euclidean distance to measure rotation and translation error, respectively. We also employ three popular statistical measurements: the median to reflect overall prediction accuracy, the mean to account for large outlier errors, and the percentage of errors below a certain threshold.

To assess the overall planar reconstruction performance, we initially present the predicted results obtained from monocular images on the above datasets, utilizing two metrics: plane segmentation and plane recovery recall from Section \ref{sec:experiments}. For the merged 3D reconstruction results derived from two adopted views, we employ the average precision (AP) metric \cite{jin2021sparseplanes}, treating each reconstructed 3D plane as a detection target for evaluation.    A true positive of 3D plane detection necessitates three conditions: (i) (Mask) an intersection-over-union value to the ground-truth mask $\geq 0.5$;    (ii) (Normal) arccos value between the predicted and ground truth normal $\leq \alpha$ where $\alpha \in \{30^{\circ}, 15^{\circ}, 5^{\circ}\}$; and (iii) (Offset) absolute difference between predicted and ground truth offset $\leq \beta$ where $\beta \in \{1\text{m}, 0.5\text{m}, 0.2\text{m}\}$.
To further assess the network's capability in implicitly learning plane-aware correspondences during reconstruction, Section \ref{sec:unified_plane_emb} presents the number of true positives (TP) and calculates precision, recall, and F-score for plane correspondences.

\subsection{Baselines and their Variants}
% \paraspace
% \ptitle{Baselines and their Improved Variants.}
3D planar reconstruction from sparse views demands the resolution of multiple sub-tasks, encompassing relative camera pose estimation, monocular planes recovery and multi-view plane matching.  We conducted a comparative analysis between representative baselines in terms of both overall results and intermediate outcomes.


\paraspace
\ptitle{Public Baselines} We compared our method with several state-of-the-art and leading sparse views planar reconstruction methods, including SparsePlanes \cite{jin2021sparseplanes}, PlaneFormers \cite{agarwala2022planeformers}, and NOPE-SAC \cite{tan2023nopesac} which are all multi-stage approaches. Specifically, all these baselines begin by estimating one or more initial camera poses from learned dense features of two input frames, and subsequently refine these camera poses using predicted planes via post optimization algorithms\cite{jin2021sparseplanes},  SIFT-like keypoints \cite{jin2021sparseplanes}, or separate network models\cite{agarwala2022planeformers, tan2023nopesac}.
Therefore, their primary contribution usually lies in an elaborated pose refinement module while treating monocular plane prediction and initial camera pose prediction as given knowledge from external and distinct modules. 
These facts are also our main differences by performing two-view planar reconstruction in a purely end-to-end manner from images without any external priors and matching supervision. 

As to the evaluation of camera pose estimation, we additionally consider two popular baselines SuperGlue \cite{sarlin2020superglue} and Pose ViT \cite{rockwell20228posevit}. SuperGlue is a competitive graph-based point matching network that leverages the estimated essential matrix with a RANSAC solver to obtain the relative camera pose. Therefore, SuperGlue inherits the intrinsic scale ambiguity of the Essential Matrix,  so we solely focus on comparing the rotation error during its evaluation. Pose Vit uses embeddings of tokenized image patches extracted from a ViT \cite{vit2021} and follows the principle of the Eight-Point Algorithm \cite{hartley1997defense} via another ViT module to directly estimate relative camera pose between two input images. Our proposed PlaneRecTR++ draws inspiration from its design philosophy but brings several important modifications to better fit unified query learning as well as end task. 
Specifically, we choose to use plane-aware embeddings instead of those from raw image patches,  even without requiring the Quadratic Position Encodings used in \cite{rockwell20228posevit}. Our method also introduces distinct differences in the attention structure, enhancing interpretability and yielding better performance.  

\input{tables/planerectr++_scannetv2_mp3d_seg}
\input{tables/planerectr++_scannetv2_mp3d_recall}

\paraspace
\ptitle{Improved Baseline Variants} The monocular plane predictor of SparsePlanes and PlaneFormers is based on PlaneRCNN\cite{Liu:etal:CVPR2019:Planercnn}, while \jiajia{NOPE-SAC} utilizes an improved version of the leading PlaneTR \cite{Tan:etal:ICCV2021:Planetr} (denoted as PlaneTRP) to recover monocular planes. Motivated by \cite{tan2023nopesac}, to make fair comparisons, we include improved baseline variants by replacing the PlaneRCNN backbone of SparsePlanes and PlaneFormers with a more powerful PlaneTRP module, termed SparsePlanes-TRP and PlaneFormers-TRP.


% ### Move paragraph below to single-view 7.2
% We also compare the monocular plane predictions of our PlaneRecTR++ with PlaneTRP \cite[nopsesac] for single view evaluation to isolate the contribution of our intra???-frame plane query learning. Among them, we present our monocular plane prediction results obtained exclusively during the monocular pretraining phase and after the joint training phase, respectively denoted as Ours(Monocular) and Ours.  
% In the appendix, similarly, we also use the plane predictions of our PlaneRecTR++ and input them to other baseline methods for experiments, but find no significant improvement in results.

\input{tables/pose_comparison}
\input{tables/2views_planerec_ap}

\paraspace
\ptitle{NOPE-SAC and its Variants} 
The NOPE-SAC framework comprises four distinct modules\jiajia{: (1) A} Monocular Plane Predictor (PlaneTRP); \jiajia{(2) An attention} network for \jiajia{dense pixel-based} pose initialization; \jiajia{(3) A supervised} differentiable plane matching module based on optimal transport; \jiajia{(4) A neural RANSAC network for plane-level pose refinement.}
To provide a more comprehensive comparison in pose estimation to the leading NOPE-SAC \cite{tan2023nopesac}, we further introduce its two variants to better isolate the contribution of key modules.

\textbf{NOPE-SAC Init.} refers to the camera pose initialization network to kick off its overall procedure, \ie, module (2) above, which relies solely on dense pixel features. We present its pose accuracy to indicate the quality of learned pose prior \cite{tan2023nopesac}, acting as a similar role to external pose predictor in \cite{jin2021sparseplanes, agarwala2022planeformers}. 

\textbf{NOPE-SAC Ref.} \jiajia{refers to NOPE-SAC without module (2) and mainly relies on the pose refinement module for pose estimation. NOPE-SAC Ref. performs plane matching without initial pose and directly estimates relative pose from sparser views through the neural RANSAC network. Specifically, NOPE-SAC Ref. sets the initial pose to an identity matrix and cuts off initial pose related geometric score function during plane matching, while maintaining the appearance score function intact. Despite retaining a multi-stage approach,  NOPE-SAC Ref. shares consensus with ours in that it relies solely on \textit{monocular plane features} for plane matching and pose regression from \textit{input sparse views}.}


In contrast to NOPE-SAC and other existing baselines, one thing worth highlighting is that our unified plane embedding demonstrates multi-view consistency and implicitly accomplishes plane matching, \textit{without any external supervision for plane matching or the need for initial pose assistance}. In Section \ref{sec:unified_plane_emb}, we compare our method with optimal transport (OT) within NOPE-SAC\cite{tan2023nopesac} requiring external priors.

\subsection{Implementation Detail}


The two-phase training process of sparse views reconstruction, as described in Section \ref{sec:overview}, begins with pre-training the model on the Scannetv2 and Matterport3d datasets following Section \ref{sec:monocular_imp_detail}. Subsequently, we employ nearly identical training configurations to jointly train the entire model with 42 epochs and a 10-fold reduction in loss for monocular planes.

\input{figures/2view_rec}

\subsection{Evaluation of Monocular Planes}
\label{sec:eval_mono}
In this section, we compare the monocular plane predictions to first highlight the effectiveness of our intra-frame plane query learning component after joint optimization. In Table \ref{tab:planerectr++_scannetv2_mp3d_seg} and Table \ref{tab:planerectr++_scannetv2_mp3d_recall}, we initially evaluated the performance for single-view plane segmentation and geometry, similar to Section \ref{sec:experiments}. 

On both datasets, our method demonstrated superior monocular plane prediction accuracy compared to the state-of-the-art PlaneTRP \cite{Tan:etal:ICCV2021:Planetr, tan2023nopesac}. Furthermore, as discussed in Section \ref{sec:corrattn_experiments}, the MatterPort3D dataset \cite{chang2017matterport3d} presents a greater challenge for plane prediction compared to the ScanNetv2 dataset, leading to a moderate performance degradation for all methods.




We also present our monocular plane prediction results exclusively from the monocular pre-training phase (\ie, without the joint optimization stage using pose losses), denoted as Ours (Monocular). There is almost indistinguishable difference in terms of the monocular prediction accuracy (rows 2-3 of Table \ref{tab:planerectr++_scannetv2_mp3d_seg} and Table \ref{tab:planerectr++_scannetv2_mp3d_recall}). This observation signifies the feasibility of our overall pipeline that even though our unified plane embeddings have incorporated evident multi-view consistency for pose estimation (Section \ref{sec:pose_evaluation}), there still remains a comparable ability for monocular plane prediction after the comprehensive joint training phase.




\input{figures/2view_rec_compare}



% In the appendix, similarly, we also use the plane predictions of our PlaneRecTR++ and input them to other baseline methods for experiments, but find no significant improvement in results.

\subsection{Relative Camera Pose Evaluation}
\label{sec:pose_evaluation}



\paraspace
\ptitle{Quantitative Results.} In Table \ref{tab:pose_comparison}, our PlaneRecTR++ demonstrates superior performance in camera pose \textit{across all metrics} on the realistic ScanNetv2 dataset compared to other methods. On the MatterPort3D dataset, our approach achieves overall comparable results to the \jiajia{leading NOPE-SAC}, exhibiting better translation accuracy while slightly lagging behind in rotation estimation.

\jiajia{The relatively poor rotation performance can primarily be attributed to the simulated characteristics of input images on the MatterPort3D dataset, which contains numerous erroneous tiny planes that challenge plane detection of all methods (see Section \ref{sec:eval_mono}) but enhance cross-view photometric consistency during rendering (see dataset introduction in Section \ref{sec:corrattn_experiments}).  
During inference on MatterPort3D, the dense pixel-based NOPE-SAC Init. and other pose initialization networks within baseline methods \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac} could provide a superior initial rotation, transforming the input sparse views into closer views and thus lowering the difficulty of following plane-level pose predictions.
} 

\jiajia{It is noteworthy that even on this challenging MatterPort3D dataset, our median rotation error remains smaller than that of NOPE-SAC, which indicates smaller typical prediction errors.}



\paraspace
\ptitle{Qualitative Results.} 
Figure \ref{fig:2view_rec} visually illustrates the relative camera pose estimates of our PlaneRecTR++ from two different viewpoints (last two columns) on the ScanNetV2 and MatterPort3D datasets.  Our method can accurately recover a precise relative camera pose from input sparse views, even in scenarios with extremely low image overlap (rows 3-8), without relying on initial pose estimation and explicit corresponding plane pairs.
Figure \ref{fig:2view_rec_compare} presents the predicted pose comparison of ours and NOPE-SAC\cite{tan2023nopesac}, showing that our method recovers a more accurate relative camera pose than the leading baseline.


\subsection{3D Planar Reconstruction Evaluation}


\paraspace
\ptitle{Quantitative Results.}
The numerical evaluation on the final 3D reconstruction is shown in Table \ref{tab:3DPlane}.  Our unified single-stage approach demonstrates superior performance compared to all other multi-stage methods, particularly exhibiting a significant enhancement in the reconstruction accuracy on the real-world dataset ScanNetv2.

\paraspace
\ptitle{Qualitative Results.} 
Figure \ref{fig:2view_rec} presents the visualization of the 3D plane reconstruction results achieved by PlaneRecTR++ on the ScanNetv2 \cite{Dai:etal:CVPR2017, Dai:scannetv2web} and the MatterPort3D \cite{chang2017matterport3d} datasets. Note that our model implicitly learns plane matching during pose inference, and we further extract and process the probability distributions of plane correspondences from our model (see 3rd column). \jiajia{Our method exhibits superior performance in plane matching and reconstruction even in the presence of extremely sparse views, and is capable of identifying and exploiting the discontinuous ground (rows 5,7,8).}  In Figure \ref{fig:2view_rec_compare}, a \jiajia{further} comparison between our proposed PlaneRecTR++ and the current state-of-the-art NOPE-SAC \cite{tan2023nopesac} is provided on both datasets, demonstrating that our method yields more precise plane reconstructions and recovers more accurate relative camera poses from sparse views.
Even in more challenging scenarios characterized by inconsistent brightness (column 1,3), confusion caused by symmetrical repetitive patterns (column 6) and so on, our method can implicitly acquire robust correspondences for reconstruction and pose recovery, which is outperforms NOPE-SAC that relies on initial pose prior and matching supervision.

\input{tables/pose_model_ablation}
\input{tables/corr_ablation}


\paraspace
\ptitle{Inference Time.} 
We calculate the average inference time of the latest neural methods for joint planar reconstruction and pose estimation on a NVIDIA TITAN V GPU. 
Our single-stage PlaneRecTR++ ($0.258$ s) achieves higher inference speed than previous multi-stage NOPE-SAC ($0.274$ s) and PlaneFormers ($4.257$ s), thanks to the exemption from external pose initialization modules \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac} and iterative refinement modules \cite{agarwala2022planeformers}.
\subsection{Ablation Studies of Model Designs}
\label{sec:sparseview_ablation}


We conducted extensive ablation studies to investigate the contributions of each design choice, particularly within our plane aware cross attention layer in Section \ref{sec:plane_cross_attention}. We focus on experimenting with the following two aspects: (1) cross embedding structure (CE) within the bilinear attention mechanism, and (2) the specialized design of query, key and value subdivision.





\paraspace
\ptitle{Cross Embeddings.}
Our method differs from Pose ViT \cite{rockwell20228posevit}, which also employs bilinear attention, in that we cross-place plane embeddings of different input images on both sides of the bilinear attention matrix, while Pose ViT places visual features and positional encodings of the same input image, which experimentally yields better results as shown in \cite{rockwell20228posevit}. We consider that our cross plane embeddings placement follows a more intuitive discipline and better performance. To validate our idea, we follow  \cite{rockwell20228posevit} and shift our plane aware cross attention layer's structure to the same embedding placement strategy.  The experimental findings (rows 1, 2 of Table \ref{tab:pose_model_ablation} and \ref{tab:corr_ablation}) show a significant degradation in performance without proposed cross embeddings set-up.

We believe the key reason for the differences between ours and Pose ViT lies in whether the network truly learns plane correspondences. In both methods, it is widely anticipated that the similarity attention matrix would serve as the function for the object assignment matrix. 
However, only our plane aware cross attention design empowers the network to effectively execute authentic plane-level matching, considering that it is less clear to generate plausible patch-wise correspondences of two sparse views.  Consequently, in our method, cross embedding placement naturally yields superior performance compared to Pose ViT, wherein this prerequisite is not met and may even impede efficient passing of visual features through cross-embedding attention.

\input{figures/heatmap}
\input{figures/multiview_3dres}

\paraspace
% \ptitle{One Piece Key/Query.}
\ptitle{Query, Key and Value Designs.} We show the effectiveness of our query, key and value designs by evaluating two model variants on pose, plane correspondences and reconstruction, respectively. 

In  Table \ref{tab:pose_model_ablation}, on both datasets, pose accuracy of PlaneRecTR++ with the unsplit query and key design is more precise when maintaining VNum. is 4 (rows 1, 3). In Figure \ref{subfig:qk1v4} and \ref{subfig:qkv4}, the highlighted areas of the plane correspondence attention matrix $\rm{C}(Q_i, K_j)$ using our unsplit key and query, align well with the ground truth correspondence. However, the distribution of 4 similarity attention matrices, each computed using one of the 4 split query and key segments, does not effectively capture the ground truth pattern. Though the combination of 4 similarity matrices can approximate actual plane correspondence distribution, it is still inferior to ours caused by more introduced noisy matches with high probabilities. 
We consider its potential in capturing real correspondences via a post-hoc evaluation, where we select the one head with the highest matching accuracy to ground truth and compare it with our method.  As presented in rows 1, 3 of two datasets in Table \ref{tab:corr_ablation},  even after carefully selecting the best possible matches from the split query and key pairs, the performance is still inferior to ours adopting unsplit query and key pairs.

Moreoever, in Table \ref{tab:pose_model_ablation} and \ref{tab:corr_ablation}, when the key and query are guaranteed to be complete, the accuracy of all metrics with VNum. $=4$ still surpasses that with VNum. $=1$ (rows 1, 4), indicating a positive contribution from the partition of value term.


On the whole, we have validated that our design not only retains the advantages of multi-head attention in standard Transformer, but also effectively captures the distribution of plane correspondence and further enhancing model performance.





% \paraspace
% \ptitle{Model Designs.}


% \paraspace
% \ptitle{Pose Regression}

% \subsection{Unified Plane Embedding Study}
\vspace{-0.1cm}
\subsection{Studies of Unified Plane Embedding}
\label{sec:unified_plane_emb}

During the inter-frame plane query learning stage, we have actually conducted several experiments to enhance the capability of input plane embedding with auxiliary knowledge. Such attempts include concatenating cosine positional encoding \cite{sun2021loftr}, quadratic positional encoding of plane center \cite{rockwell20228posevit}, plane parameter encoding or plane appearance embedding along with original plane embedding. We also explored to filter out plane embedding sequence using their plane probability $p_i$, or to incorporate several self-attention layers \cite{rockwell20228posevit, Vaswani:etal:NIPS2017} to promote contextual features, or to introduce an explicit view consistency loss of planes and pose during training.
%(see Appendix?).
However, none of these variants yielded any obvious improvement in the current model's performance.

It became evident that our unified plane embedding, achieved through a simple combination of intra-frame and inter-frame plane query learning, already encompassed adequate information to address the task of sparse views planar reconstruction.

\input{figures/tsne}
\input{tables/corr_comparison}

\paraspace
\ptitle{Consistent Planar Attributes across Frames.}
After the initial single view training and following comprehensive sparse view training, rows 2,3 of Table \ref{tab:planerectr++_scannetv2_mp3d_seg} and Table \ref{tab:planerectr++_scannetv2_mp3d_recall} exhibit comparable performance in monocular plane detection on two datasets.   In the rows 3,4 of Table \ref{tab:corr_compare}, the former relies solely on similar appearance features from a single view and achieves poor matching results, whereas the latter computes a reasonable and precise plane correspondence. In Figure \ref{fig:tsne}, despite only being trained on input sparse views, our unified query embeddings exhibit promising consistency across more frames without the need for ground truth correspondence supervision. \jiajia{Figure \ref{fig:multiview_rec} exhibits our qualitative results that conspicuously outperform the leading NOPE-SAC on multiple views ($\geq 3$ views).}



% \paraspace
% \ptitle{Monocular Visualization}

% poaitinal encoding, context???

\paraspace
\ptitle{Implicit Plane Matching.}
Compared with the differentiable optimal transport (OT) \cite{sarlin2020superglue} method from NOPE-SAC \cite{tan2023nopesac}, our approach owns the following advantages:
(1) Most importantly, we skip the requirement of pose initialization in all previous methods \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}. This means that there is no need for us to convert plane parameters into the same coordinate system before effectively utilizing planar geometry for matching.  We believe that this is a crucial factor for constructing a single-stage method.
(2) We do not require explicit supervision using \jiajia{the} ground truth correspondences. Instead, only through pose supervision, our carefully designed simple network structure actively learns multi-view consistency for plane embedding.  In a single forward pass, our method implicitly performs plane matching and probabilistically synthesizes pairwise plane features for pose prediction.  
It does not explicitly perform plane matching and input hard plane pairs to a pose refinement network \cite{jin2021sparseplanes, tan2023nopesac}.
(3) The correspondence attention matrix formed by our network can be processed using a simple MNN (maximum nearest neighbor) operation to obtain a hard plane assignment matrix.  The accuracy of this assignment matrix is comparable or even higher than previous methods with supervisions, as shown in Table \ref{tab:corr_compare}.

