\input{figures/crossplaneattn_network}
\section{Inter-Frame Plane Query Learning}
\label{sec:inter_frame_method}
%-------------------------------------------------------------------------
In this section, we \jiajia{present} how we extend our \jiajia{monocular} framework PlaneRecTR to a novel multi-view setup, while still retaining the virtue of query-based learning. 
We introduce an inter-frame query learning component on top of unified plane embeddings of per-frame. A plane aware cross attention layer is proposed to achieve inter-frame plane interactions, within which dual softmax \cite{sun2021loftr, rockwell20228posevit} and bilinear attention \cite{kim2018bilinear, rockwell20228posevit} mechanisms are used to align the intermediate attention structure with a plane correspondence matrix and enable plane-level feature fusion between views.  
Most importantly, we further modify the key, query, and value forms of standard multi-head attention \cite{Vaswani:etal:NIPS2017} to effectively utilize the complete representation of unified plane embeddings, which better accommodates multi-view plane properties without requiring any additional inputs such as position encoding \cite{sun2021loftr, rockwell20228posevit}. This simple adjustment guarantees that our attention structure truly accomplishes plane matching, allowing the network to spontaneously focus on genuine paired planes from two views.

As illustrated in Figure \ref{fig:crossplaneattn_network}, we employ two plane-aware cross attention layers and an MLP head to construct a simple and lightweight pose regression module. A plane-aware attention layer first takes two-view plane embeddings $\mathcal{E}_\text{plane}^1$ and $\mathcal{E}_\text{plane}^2$ as input, and actively learns their correspondences within the network. Subsequently, our model directly regresses a relative camera pose from probabilistic paired plane embeddings. Conceptually, this entire process aligns with the logical framework of solutions based on classical two-view geometry \cite{hartley2003multiple}, 
thereby offering enhanced interpretability. 


\subsection{Cross Attention for Unified Plane Embeddings}
\label{sec:plane_cross_attention}

\ptitle{Preliminaries: Standard Cross Attention.} 
The Transformer cross attention layer \cite{Vaswani:etal:NIPS2017} updates the input value term by mapping query from the $i$-th input and key-value pair from another $j$-th input through a weighted summation, typically using a scaled dot-product similarity function $\operatorname{S}(\cdot,\cdot)$. In addition, attention often employs a multi-head strategy \cite{Vaswani:etal:NIPS2017} where query, key, and value (denoted $Q_i$, $K_j$, $V_j$ 
$\in \mathbb{R}^{N \times C_{\mathcal E}}$, respectively)
are divided, along channel dimensions, into $N_h$ segments $\{q_i^h\}_{h=1}^{N_h}$, $\{k_j^h\}_{h=1}^{N_h}$, $\{v_j^h\}_{h=1}^{N_h} \in \mathbb{R}^{N \times \frac{C_{\mathcal E}}{N_h}}$, in order to enhance the expressiveness and diversity of results without incurring extra computational costs. 

The multi-head similarity function ($\operatorname{S}$) and  cross attention layer ($\operatorname{MCA}$) are defined in Equations \ref{eq:standard_crossattn1} and \ref{eq:standard_crossattn2}:
\begin{small}
\begin{equation}
% \setlength\abovedisplayskip{2pt}%shrink space
% \setlength\belowdisplayskip{3pt}
\label{eq:standard_crossattn1}
% \rm{Attention}(Q_i, K_j, V_j) = \rm{softmax}( \frac{Q_iK_j^T}{\sqrt{d_k}} )V_j
\operatorname{S}(q_i^h, k_j^h) =  \operatorname{softmax}\left(\frac{q_i^h{k_j^h}^T}{\sqrt{C_{\mathcal E}/N_h}}, 1\right),
\end{equation}
\end{small}
\begin{small}
\begin{align}
\label{eq:standard_crossattn2}
\operatorname{MCA}(Q_i, K_j, V_j) = \operatorname{Linear}(\operatorname{Concat}(\{\operatorname{S}(q_i^h, k_j^h)v_j^h\}_{h=1}^{N_h})),
\end{align}
\end{small}where $\operatorname{softmax}(\cdot, k)$ applies softmax operation across the $k$-th axis; $\operatorname{Linear}$ and $\operatorname{Concat}$ mean linear projection and channel-wise concatenation, respectively.
Our \jiajia{method} targets equations \ref{eq:standard_crossattn1} and \ref{eq:standard_crossattn2} for intuitive and efficient plane-specific modifications, achieving implicit plane matching and direct pose regression.

\paraspace
% \ptitle{Dual Softmax}
\ptitle{Plane Correspondence Probability Function.}
To overcome the limitations of the multi-stage two-view plane reconstruction paradigm \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}, here we specifically devise an inter-frame correspondence attention structure to learn reliable plane embeddings, which \jiajia{enables} the network to autonomously acquire probabilities of plane correspondence and conduct pose inference in a single forward pass. This design also eliminates the dependency on either ground truth correspondence supervision or initial pose.




Specifically, in contrast to the similarity function in Equation \ref{eq:standard_crossattn1},
%employed for calculating the similarity attention matrix in the standard cross-attention layer.   
we utilize a dual-softmax operation instead of a single softmax on the \emph{unsplit} query and key embeddings $Q_i, K_j$, aiming to keep integral embedding information when constructing plane-wise correspondence probability, as shown in Equation \ref{eq:dual_softmax} below.
\begin{equation}
\label{eq:dual_softmax}
% \setlength\abovedisplayskip{0.5pt}%shrink space
% \setlength\belowdisplayskip{0.5pt}
\small
\operatorname{C}(Q_i, K_j) =\operatorname{softmax}(\frac{Q_iK_j^T}{\sqrt{C_{\mathcal E}}} , 1) \odot \operatorname{softmax}(\frac{Q_iK_j^T}{\sqrt{C_{\mathcal E}}} , 2)\jiajia{,}
\end{equation}
We compute a 2D correspondence matrix $\operatorname{C}(Q_i, K_j)$,  where the element at the $m^{th}$ row and $n^{th}$ column $\operatorname{C}_{mn}(Q_i, K_j)$ denotes the probability that the $m^{th}$ plane embedding from the $i^{th}$ image $I_i$ corresponds to the $n^{th}$ plane embedding from  $j^{th}$ image $I_j$, indicating their likelihood of representing the same plane instance. For the task of two-view plane reconstruction, we stick to configurations of $\{i=1,j=2\}$ and $\{i=2,j=1\}$.


The noteworthy aspect lies in our simple modification to the input query and key's formats, which yield benefits that better align with the characteristics of plane instance. The key $K_j$ and query $Q_i$ in our model are derived from a linear mapping of the unified plane embeddings $\mathcal{E}_\text{plane}$ obtained by intra-frame query learning. We want to highlight the practical significance of an intact $\mathcal{E}_\text{plane}$ representing plane entities. Specifically, we preserve the integrity of $Q_i$ and $K_j$ rather than dividing them into multiple heads to fully leverage the representation power of unified plane embeddings encoding comprehensive information (geometry, appearance, location, context, etc.). This facilitates learning genuine correspondence distribution between planes instead of only abstractly capturing similarities among different subspace representations at various positions.

This simple design has been experimentally validated (Section \ref{sec:sparseview_ablation} and \ref{sec:unified_plane_emb}) to significantly increase the discriminative multi-view consistency of the unified plane embeddings $\mathcal{E}_\text{plane}$, which is of much higher quality than previous methods. Consequently, it facilitates the integration of real plane pairs' information without initial pose for direct precise pose estimation and enables explicit utilization in achieving accurate fusion of plane meshes from two views, thereby completing the overall reconstruction.

\paraspace
% \ptitle{Bilinear Cross Attention}
\ptitle{Inter-frame Plane Aware Cross Attention.}
\label{sec:bilinear_attntion}
The standard cross attention offers an efficient approach to selectively utilize one of the input plane sequences, but overlooks the interaction between two inputs. Therefore, we adopt bilinear attention \cite{kim2018bilinear, rockwell20228posevit} to incorporate planar information from both views through the plane correspondence probability distribution $\operatorname{C}(Q_i, K_j)$:
\begin{align}
\label{eq:bilinear_attentin}
% \setlength\abovedisplayskip{2pt}%shrink space
% \setlength\belowdisplayskip{2pt}
% \rm{BiAttention}(Q_i, K_j, V_i, V_j) =V_i^T \rm{dual\_softmax}( Q_i, K_j)V_j
\operatorname{PCA}(Q_i, K_j, V_i, V_j) = &\operatorname{Linear}(\operatorname{Concat}( \nonumber\\
& \{(v_i^h)^T \operatorname{C}(Q_i, K_j)v_j^h\}_{h=1}^{N_h} ))\jiajia{,}
\end{align}

Unlike the query and key terms, the value is still  divided into $N_h$ segments along the feature dimension, maintaining the advantages of multi-head attention. The value segments share the correspondence attention (Equation \ref{eq:dual_softmax}) from \emph{unsplit} query and key, allowing our model to selectively attend to actual corresponding plane embedding pairs across distinct sub-spaces.


As shown in Figure \ref{fig:crossplaneattn_network}, two parallel plane aware cross attention layers (Equation \ref{eq:bilinear_attentin}) are used to capture integrated features of the corresponding planes from $I_1$ to $I_2$ as well as from $I_2$ to $I_1$. It is worth noting that pose ViT \cite{rockwell20228posevit} employs the identical  features on both sides of bilinear attention matrix. However, we intuitively choose to cross-place embedding sequences of distinct images to model correspondences, thereby enhancing learning efficiency and yielding improved results. We will validate various design disparities through subsequent ablation studies in Section \ref{sec:sparseview_ablation}. 



\subsection{Pose Regression}
\label{sec:pose_loss}
Each plane aware cross attention layer ultimately outputs a feature map of size $N_h\times \frac{C_{\mathcal E}}{N_h} \times \frac{C_{\mathcal E}}{N_h}$. The corresponding plane feature maps from two parallel cross attention layers are concatenated and then mapped to a relative camera pose $T=(t, q) \in SE(3)$ using a simple MLP with two hidden layers. Here, $t \in \mathbb{R}^3$ represents translation in real units, and $q \in \mathbb{R}^4$ denotes a unit quaternion representing rotation transformation satisfying $\Vert q \Vert = 1 $.

We use lietorch \cite{teed2021lietorch} to calculate the geodesic distance $\mathcal{G} \in \mathbb{R}^6$ between the predicted pose $T$ and the ground truth pose $T^{\ast}$ as the loss for backpropagation:
\begin{equation}
% \setlength\abovedisplayskip{2pt}%shrink space
% \setlength\belowdisplayskip{2pt}
\mathcal{G} (T, T^{\ast}) = 
\operatorname{Log} ( T.\operatorname{inv()} \cdot  T^{\ast}),
\end{equation}
\begin{equation}
% \setlength\abovedisplayskip{2pt}%shrink space
% \setlength\belowdisplayskip{2pt}
\mathcal{L}_{pose} = \lambda_t \Vert \mathcal{G}_{1:3} \Vert + \lambda_q \Vert \mathcal{G}_{4:6} \Vert\jiajia{.}
\end{equation}
where $\lambda_t = 5$, $\lambda_q = 15$.
It should be noted that $\mathcal{L}_{pose}$ serves as the sole objective for our inter-frame component. Without requiring correspondence supervision, the proposed framework allows for the discovery of plane correspondence, and transforms abstract similarity attention distribution within network into a concrete probabilistic distribution of plane correspondences. Furthermore, the bilinear structure effectively utilizes integrated features of inter-frame planes, leading to accurate pose recovery without relying on external initial poses.


\subsection{Inferring Sparse Views Planar Reconstruction}
% Monocular planes recovery

% Plane matching

% Planar fusion
After intra-frame plane query learning, we can independently recover the 3D plane sets of two images in their respective camera coordinate systems using the corresponding unified plane embeddings as described in Section \ref{sec:model} and \ref{sec:planeinference}.

% Following inter-frame plane query learning, our unified plane embeddings capture multi-view consistency and the probability distribution computation for plane correspondence enables efficient plane matching within the attention layer.
During inference, we extract the learned correspondence matrix $\operatorname{C}(Q_i, K_j)$ from the network and filter out low-probability correspondences using a threshold $\theta$. We then employ the mutual nearest neighbor (MNN) criterion \cite{sun2021loftr} to obtain a hard assignment between the two plane sets. Like previous methods for sparse view planar reconstruction \cite{jin2021sparseplanes,agarwala2022planeformers,tan2023nopesac}, based on above estimated camera pose and plane matching results, we transform the plane attributes into canonical viewpoint for final reconstruction and evaluation.  Specifically, we merge normals, offsets, and textures of paired monocular 3D planes \cite{jin2021sparseplanes} to achieve a geometrically precise and smooth reconstruction using sparse views. Those paired planes whose deviations in merged normals or offsets exceed predefined thresholds are removed during inference. 
