
\section{Related Work}
\label{sec:related}
\subsection{3D Plane Recovery from a Single Image}
Recovering monocular planes empowers 3D planar reconstruction and structural scene understanding. Traditional methods for 3D plane recovery from a single image often rely on strong assumptions of scenes \cite{delage2007automatic, fouhey2014unfolding} (\eg, the Manhattan world assumption), or require manual extraction of primitives \cite{delage2007automatic,fouhey2014unfolding, lee2009geometric}, such as superpixels, points and line segments, which may not be applicable to complex actual scenes. 
% In contrast, recent learning-based approaches have alleviated such limitations leveraging large-scale training corpus, which have witnessed exciting progress so far. 

PlaneNet \cite{Liu:etal:CVPR2018:Planenet} is the first to propose an end-to-end learning framework for this task, it also releases a large dataset of planar depth maps utilizing the ScanNetv1 dataset \cite{Dai:etal:CVPR2017}. PlaneRecover \cite{yang2018recovering} presents an unsupervised learning approach that specifically targets outdoor scenes. However, both PlaneNet and PlaneRecover can only predict a fixed number of planes. PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} tackles this limitation and extracts an arbitrary number of planes with planar parameters and segmentation masks using a proposal-based instance segmentation framework, \ie, Mask R-CNN \cite{He:etal:ICCV2017}. It also proposes a segmentation refinement network as well as a warping loss between frames to improve performance.
% PlaneSegNet \cite{xie2021planesegnet} employs a fast single-stage CNN for plane segmentation, and later introduces an improved non-maximum suppression method for plane instance (FF-NMS) to improve localization and segmentation accuracy. 
These proposal-based methods require multiple steps to successively tackle sub-tasks of 3D plane recovery including plane detection, segmentation, parameter and depth estimations, \etc. 
% Therefore, the overall pipeline is decoupled where planar attributes are inferred given predicted plane segmentation.

On the other hand, PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE} leverages a proposal-free instance segmentation approach, which uses mean shift clustering to group embedding vectors within planar regions. PlaneTR \cite{Tan:etal:ICCV2021:Planetr} inherits the design of DETR \cite{carion2020end} to concurrently detect plane instances and estimate plane parameters, followed by plane segmentations generated by a pixel clustering strategy like PlaneAE \cite{Yu:etal:CVPR2019:PlaneAE}. 
Specifically, its Transformer branch only predicts instance-level plane information, thus post-processing like clustering is still required to carry out pixel-wise segmentation. The global depth is inferred by another convolution branch. 
 
Therefore, existing advanced methods, whether based on direct CNN prediction or embedding clustering, still divide the whole 3D plane recovery task into several steps. In contrast, our plane query learning-based approach offers a unified solution for the aforementioned sub-tasks within the intra-frame component and can be seamlessly extended to address inter-frame requirements in an end-to-end manner.

\subsection{3D Planar Reconstruction from Sparse Views}

Built upon advancement in monocular plane recovery, numerous works \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac} have emerged to address the challenging two-view planar reconstruction with unknown camera poses, aiming to build a coherent 3D planar reconstruction.
% advances the state-of-the-art performance in 3D reconstruction from sparse views and enables a single, coherent, global planar reconstruction.

SparsePlanes \cite{jin2021sparseplanes} is the first learning-based approach for planar reconstruction and relative camera pose estimation from sparse views. 
% It decomposes the whole problem into three interconnected tasks: per-frame plane reconstruction, inter-frame plane correspondence, and pose estimation, through separate modules in a multi-stage manner. 
Specifically, with monocular planes and their embeddings from PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn}, a dense pixel attention network is applied to estimate a number of $K$=1024 pose hypotheses, all of which are used for plane matching and a final pose via a complex two-step optimization. %Various strategies are required to converge from a massive amount of coarse pose hypotheses and to finally reach a few plane correspondences.
Furthermore, PlaneFormers \cite{agarwala2022planeformers} utilizes predicted monocular planes and top $h$=9 pose hypotheses from SparsePlanes \cite{jin2021sparseplanes} as input, but replace the intricate handcrafted optimization by $h$ leanable PlaneFormer modules, mitigating the complex optimization issue faced by \cite{jin2021sparseplanes}.  
% Despite of its improved performance compared to SparsePlanes, there is still ample room for enhancing model efficiency and effectiveness
% In the discrete optimization stage, SparsePlanes independently solves K matching problems by utilizing predicted planes, K poses and their probabilities to obtain optimal pose hypothesis and plane correspondences. In the continuous optimization stage, it further refines the planes and pose hypothesis through minimizing corresponding plane parameters and SIFT \cite{} keypoints.
% SparsePlanes integrates separate networks and various optimization algorithms and there remains ample scope for enhancing model efficiency and performance. 
% Each PlaneFormer receives tokens representing the plane-level features of both frames in each hypothesized global coordinate system, thereby introducing self-attention features among all planes in the two frames to directly predict plane correspondence and refined pose residuals.
% SparsePlanes necessitates various strategies to converge from a massive amount of rough pose hypotheses and to reach a few plane correspondences, leaving a challenging ill-posed optimization problem especially under the sparse view setting. 
% PlaneFormers directly leverages the powerful transformer framework to mitigate the complex optimization issue faced by SparsePlanes.  Despite of its improved performance compared to SparsePlanes, there is still ample room for enhancing model efficiency and effectiveness
NOPE-SAC \cite{tan2023nopesac} improves monocular plane quality by replacing PlaneRCNN \cite{Liu:etal:CVPR2019:Planercnn} with a modified PlaneTR \cite{Tan:etal:ICCV2021:Planetr}, and 
% achieves the final pose estimation through a direct regression from an external coarse camera pose.
achieves an initial coarse pose from direct regression using the similary pixel attention \cite{jin2021sparseplanes}.
It also introduces differentiable optimal transport \cite{sarlin2020superglue} for plane matching and proposes one-plane pose hypotheses based on corresponding plane pairs, effectively resolving conflicts between numerous pose hypotheses and limited 3D plane correspondences during SparsePlanes' pose refinement.

However, both PlaneFormers and the leading Nope-SAC still adopt a multi-stage pipeline derived from SparsePlanes, requiring bootstrapping from external priors like initial pose estimation and correspondence supervision. 
% Specifically, they employ an existing monocular plane predictor to estimate per-frame planes, incorporate an additional dense pixel-based pose initialization network for rough pose(s) estimation, and leverage the initial pose(s) for effective plane matching and plane-level pose refinement.
The reason behind this lies in the existence of a chicken-and-egg relationship between explicitly learning camera pose and plane correspondence.  In all previous methods \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}, the capability of plane embeddings learned by ground truth correspondence supervision is insufficient to accurately track the same plane instance under sparse views.  To address this issue, an additional neural model is necessary to provide the initial pose(s), so that monocular planar information could be merged under a unified coordinate system, thereby assisting the original plane embedding in enhancing matching accuracy and ultimately refining the initial pose.
Our proposed PlaneRecTR++ deviates from such dilemma using unified plane query learning, actively inferring plane estimations, correspondences, and camera pose in a single-stage framework, \textit{without relying on initial pose guidance and supervision for plane matching}.


\subsection{Correspondence and Camera Pose Estimation}

Camera pose estimation between adjacent images is a fundamental step in multi-view 3D reconstruction\cite{hartley2003multiple}. Early studies focused primarily on extracting sparse keypoint correspondences \cite{Lowe:IJCV2004, bay2008surf} to compute the essential matrix using the five-point solver \cite{nister2004fivepoint}. 
Subsequently, significant efforts have been devoted to learning-based approaches for robust keypoint detection \cite{detone2018superpoint, dusmanu2019d2net, tyszkiewicz2020disk} and matching \cite{sarlin2020superglue, sun2021loftr, zhou2020learn}. However, pose computed solely from the essential matrix lacks a real scaling factor and causes further challenges in the sparse view setting, where only a limited number of correct correspondences could be found.

% as well as a direct pose estimation aware of the absolute scale. These neural approaches, however, often rely on largely overlapping images and abundant pose annotations. In addition, the predicted camera pose of sparse view still lacks satisfying accuracy and is typically refined by various designed follow-up modules.
Moreover, existing methods for directly learning relative pose from images usually require concatenating pairwise frames \cite{en2018rpnet} or computing affinity volume\cite{cai2021extreme, jin2021sparseplanes, tan2023nopesac}, resulting in a significant computational burden. Additionally, these methods either rely on extensively overlapping images\cite{en2018rpnet, cai2021extreme}, or adapt to sparser views but yield limited precision, thus serving solely as an initial pose prior \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}.
% \cite{jin2021sparseplanes, tan2023nopesac} serves as the pose initialization module in the current multi-stage sparse view planar reconstruction methods. \cite{jin2021sparseplanes} transforms pose estimation into a classification problem by predicting probabilities for priori poses, \cite{tan2023nopesac} directly regresses a pose to reduce the number of pose hypotheses.  They can only estimate rough initial pose(s), which will be used as priority for correspondence prediction and depend on the correspondence to be further refined.
Recently proposed Pose Vit \cite{rockwell20228posevit}, a transformer structure with tokenized uniform image patches as well as their quadratic positional bias as input, attempts to learn proximal patch correspondence information using attention mechanism, ultimately allowing a direct regression of rotation and translation with scale in a wide baseline.  


Our approach draws inspiration from attention-based modules \cite{sarlin2020superglue, sun2021loftr,rockwell20228posevit}, but further empowers the standard cross attention by a plane-aware attentive design. With our learned plane embeddings as the only input for pose prediction, our method implicitly learns genuine plane correspondences and is able to recover precise camera poses within a compact module.

