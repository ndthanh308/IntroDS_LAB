\section{Sparse View Experiments}
\label{sec:corrattn_experiments}

\paraspace
\ptitle{Datasets.}
We adopt the setup of various existing works \cite{jin2021sparseplanes, tan2023nopesac} to ensure a fair comparison to published baselines, and evaluate on two large-scale sparse view datasets for simultaneous planar reconstruction and camera pose estimation.

\textbf{ScanNetv2 Dataset}
% We utilized the indoor ScanNetv2 \cite{Dai:etal:CVPR2017, Dai:scannetv2web} video dataset, which includes plane annotations generated by \cite{Liu:etal:CVPR2019:Planercnn}. 
We use a more challenging sparse view split of the ScanNetv2 dataset created by \cite{Dai:etal:CVPR2017,Dai:scannetv2web,Liu:etal:CVPR2019:Planercnn,tan2023nopesac}, consisting of 17,237/4,051 image pairs from 1,210/303 non-overlapping scenes for training/testing. 
% We denote it as ScanNetv2 to make a distinction between the ScanNetv1 in Section \ref{sec:experiments} for monocular plane recovery. 
Different to ScanNetv1 for monocular plane recovery, ScanNetv2 was proposed to contain a lower overlapping ratio between frames and more complex camera rotation distributions. The average overlap ratio of adjacent frames were 20.6\% and 18.6\% in the training and test sets, respectively. The image size is 640 × 480 for all baselines.
% Following the sparse view split by \cite{tan2023nopesac}, it includes 17,237/4,051 image pairs from 1,210/303 non-overlapping scenes for training/testing. We denote it as ScanNetv2 to distinguish it from ScanNetv1 in Section \\ref{sec:experiments}. ScanNetv2 has a lower frame overlap and more complex camera rotations. To ensure diversity, a frame interval of at least 20 frames in the training set and 40 frames in the test set was maintained. The average overlap of adjacent frames is 20.6\\% and 18.6\\% in the training and test sets, respectively. The image size is 640 × 480 for all baselines.
% To ensure sufficient diversity and avoid redundancy, a frame interval of at least 20 frames in the training set and 40 frames in the test set was kept between sampled image pairs.  Consequently, the average overlapping ratio of adjacent frames were 20.6\% and 18.6\% in the training and test sets, respectively. The image size is 640 × 480 for all baselines.

\textbf{MatterPort3D Dataset}
% 仿真数据集，有很多碎片
In contrast to ScanNetv2, the sparse view version of the MatterPort3D dataset\cite{chang2017matterport3d} is created in a semi-simulated manner, whose RGB images are rendered from approximated 3D planar meshes during the generation process of plane annotations \cite{jin2021sparseplanes}, thereby mitigating real-world impact like lighting variation. 
%However, it should be noted that the re-rendered RGB image comprises a scene entirely composed of planes, including numerous error facets resulting from the approximated 3D plane mesh. 
However, these rendered RGB images consist solely of planes along with numerous small erroneous facets caused by approximated 3D planar meshes, increasing the level of complexity for accurate plane detection. The training set and testing set consist of 31,932 and 7,996 image pairs, respectively, exhibiting an overlap ratio of about 21.0\% \cite{tan2023nopesac}. The image size is also kept to 640 × 480.

\paraspace
\ptitle{Evaluation Metrics.}
To evaluate relative camera pose, we use geodesic distance and euclidean distance to measure rotation and translation error, respectively. We also employ three popular statistical measurements: the median to reflect overall prediction accuracy, the mean to account for large outlier errors, and the percentage of errors below a certain threshold.

To assess the overall planar reconstruction performance, we initially present the predicted results obtained from monocular images on the above datasets, utilizing two metrics: plane segmentation and plane recovery recall from Section \ref{sec:experiments}. For the merged 3D reconstruction results derived from two adopted views, we employ the average precision (AP) metric \cite{jin2021sparseplanes}, treating each reconstructed 3D plane as a detection target for evaluation.    A true positive of 3D plane detection necessitates three conditions: (i) (Mask) an intersection-over-union value to the ground-truth mask $\geq 0.5$;    (ii) (Normal) arccos value between the predicted and ground truth normal $\leq \alpha$ where $\alpha \in \{30^{\circ}, 15^{\circ}, 5^{\circ}\}$; and (iii) (Offset) absolute difference between predicted and ground truth offset $\leq \beta$ where $\beta \in \{1\text{m}, 0.5\text{m}, 0.2\text{m}\}$.
To further assess the network's capability in implicitly learning plane-aware correspondences during reconstruction, Section \ref{sec:unified_plane_emb} presents the number of true positives (TP) and calculates precision, recall, and F-score for plane correspondences.

\subsection{Baselines and their Variants}
% \paraspace
% \ptitle{Baselines and their Improved Variants.}
3D planar reconstruction from sparse views demand the resolution of multiple sub-tasks, encompassing relative camera pose estimation, monocular planes recovery and multi-view plane matching.  We conducted a comparative analysis between representative baselines in terms of both overall results and intermediate outcomes.

\paraspace
\ptitle{Public Baselines} We compared our method with several state-of-the-art and leading sparse views planar reconstruction methods, including SparsePlanes \cite{jin2021sparseplanes}, PlaneFormers \cite{agarwala2022planeformers}, and NOPE-SAC \cite{tan2023nopesac} which are all multi-stage approaches. Specifically, all these baselines begin by estimating one or more initial camera poses from learned dense features of two input frames, and subsequently refine these camera poses using predicted planes via post optimization algorithms\cite{jin2021sparseplanes},  SIFT-like keypoints \cite{jin2021sparseplanes}, or separate network models\cite{agarwala2022planeformers, tan2023nopesac}.
Therefore, their primary contribution usually lies on elaborated pose refinement module while treating monocular plane prediction and initial camera pose prediction as given knowledge from external and distinct modules.
% In contrast to the end-to-end pipeline of our proposed PlaneRecTR++, their primary contribution focuses on pose refinement while treating monocular plane prediction as a distinct module. 
These facts are also our main differences by inferring two-view planar reconstruction in a purely end-to-end manner from images without any external priors and matching supervision. 

As to the evaluation of camera pose estimation, we additionally consider two popular baselines SuperGlue \cite{sarlin2020superglue} and Pose Vit \cite{rockwell20228posevit}. Superglue is a competitive graph-based point matching network that leverages the estimated essential matrix with a RANSAC solver to obtain the relative camera pose. Therefore, Superglue inherits the intrinsic scale ambiguity of the Essential Matrix,  so that we solely focus on comparing the rotation error during its evaluation. Pose Vit uses embeddings of tokenized image patches extracted from a ViT \cite{vit2021} and follows the principle of the Eight-Point Algorithm \cite{hartley1997defense} via another ViT module to directly estimate relative camera pose between two input images. Our proposed PlaneRecTR++ draws inspiration from its philosophy but brings several important modifications to better fit unified query learning as well as end task. 
Specifically, we choose to use plane-aware embeddings instead of those from raw image patches,  even without requiring Quadratic Position Encodings in \cite{rockwell20228posevit}. Our method also introduces distinct differences in the attention structure, enhancing interpretability and yielding better performance.  %Notably, as Pose Vit enables a scale estimation of pose, we thus evaluate its both rotation and translation accuracy.

\paraspace
\ptitle{Improved Baseline Variants} The monocular plane predictor of SparsePlanes and PlaneFormers are based on PlaneRCNN\cite{Liu:etal:CVPR2019:Planercnn}, while Nope-SAC utilizes an improved version of the leading PlaneTR \cite{Tan:etal:ICCV2021:Planetr} (denoted as PlaneTRP) to recover monocular planes. Motivated by \cite{tan2023nopesac}, to make fair comparisons, we include improved baseline variants by replacing the PlaneRCNN backbone of SparsePlanes and PlaneFormers with a more powerful PlaneTRP module, termed as SparsePlanes-TRP and PlaneFormers-TRP.



\paraspace
\ptitle{NOPE-SAC and its Variants} 
The NOPE-SAC framework comprises four distinct modules, namely (1) a Monocular Plane Predictor (PlaneTRP), (2) a network for pixel-level pose initialization, (3) a differentiable plane matching module based on optimal transport, and (4) a network for plane-level pose refinement.
To provide a more comprehensive comparison in pose estimation to the leading NOPE-SAC \cite{tan2023nopesac}, we further introduce its two variants to better isolate the contribution of key modules.

\textbf{NOPE-SAC Init.} refers to the camera pose initialization network to kick off its overall procedure, \ie, module (2) above, which is solely rely on dense pixel features. We present its pose accuracy to indicate the quality of learned pose prior \cite{tan2023nopesac}, acting as a similar role to external pose predictor in \cite{jin2021sparseplanes, agarwala2022planeformers}. 
% NOPE-SAC Init. extracts a dense 4D afﬁnity volume between two frames and directly estimates relative camera pose.

\textbf{NOPE-SAC Ref.} refers solely to the camera pose reﬁnement module of NOPE-SAC that produces the final camera pose given initial pose and plane correspondences.It eliminates the pose initialization network by cutting off pose related geometric score function during plane matching, while keeping the appearance score function intact. Despite retaining a multi-stage approach,  NOPE-SAC Ref. shares consensus with ours in that it relies solely on monocular plane features for plane matching and then regresses camera pose.



In contrast to NOPE-SAC and other existing baselines, one thing worth to highlight is that our unified plane embedding demonstrates multi-view consistency and implicitly accomplishes plane matching, \textit{without any external supervision for plane matching nor the need for initial pose assistance}. In Section \ref{sec:unified_plane_emb}, we compare our method with optimal transport (OT) within NOPE-SAC\cite{tan2023nopesac} requiring external priors.


\input{tables/planerectr++_scannetv2_mp3d_seg}
\input{tables/planerectr++_scannetv2_mp3d_recall}

\subsection{Implementation Detail}

The two-phase training process of sparse views reconstruction, as described in Section \ref{sec:overview}, begins with pre-training the model on the Scannetv2 and Matterport3d datasets following Section \ref{sec:monocular_imp_detail}. Subsequently, we employ nearly identical training configurations to jointly train the entire model with 42 epochs and a 10-fold reduction in loss for monocular planes.



\subsection{Evaluation of Monocular Planes}
\label{sec:eval_mono}
In this section, we compare the monocular plane predictions to first highlight the effectiveness of our intra-frame plane query learning component after joint optimization. In Table \ref{tab:planerectr++_scannetv2_mp3d_seg} and Table \ref{tab:planerectr++_scannetv2_mp3d_recall}, we initially evaluated the performance for single-view plane segmentation and geometry, similar to Section \ref{sec:experiments}. 

On both datasets, our method demonstrated superior monocular plane prediction accuracy compared to the state-of-the-art PlaneTRP \cite{Tan:etal:ICCV2021:Planetr, tan2023nopesac}. Furthermore, as discussed in Section \ref{sec:corrattn_experiments}, the MatterPort3D dataset \cite{chang2017matterport3d} presents a greater challenge for plane prediction compared to the ScanNetv2 dataset, leading to a moderate performance degradation for all methods.


\input{tables/pose_comparison}
\input{tables/2views_planerec_ap}

We also present our monocular plane prediction results exclusively from the monocular pre-training phase (\ie, without the joint optimization stage using pose losses), denoted as Ours (Monocular). There is almost indistinguishable difference in terms of the monocular prediction accuracy (rows 2-3 of Table \ref{tab:planerectr++_scannetv2_mp3d_seg} and Table \ref{tab:planerectr++_scannetv2_mp3d_recall}). This observation signifies the feasibility of our overall pipeline that even though our unified plane embeddings have incorporated evident multi-view consistency for pose estimation (Section \ref{sec:pose_evaluation}), there still remains a comparable ability for monocular plane prediction after the comprehensive joint training phase.

\input{figures/2view_rec}


\input{figures/2view_rec_compare}





\subsection{Relative Camera Pose Evaluation}
\label{sec:pose_evaluation}



\textbf{Quantitative Results.} As presented in Table \ref{tab:pose_comparison}, our PlaneRecTR++ demonstrates superior performance in estimating camera pose \textit{across all metrics} on the realistic ScanNetv2 dataset compared to other methods. On the MatterPort3D dataset, our approach achieves overall comparable results to the state-of-the-art Nope-SAC method, exhibiting better translation accuracy while slightly lagging behind in rotation estimation. 
This discrepancy can be attributed to the characteristics of input RGB images of MatterPort3D, which contains numerous erroneous tiny planes that challenge plane detection but enhance cross-view photometric consistency during rendering (Section \ref{sec:corrattn_experiments}), leading to a similar accuracy trend for ours on two datasets like above mentioned monocular predictions in Section \ref{sec:eval_mono}.

We have found that the multi-stage design of Nope-SAC \cite{tan2023nopesac} enables a robust pose estimation from both image-level and plane-level information, especially when it is difficult to conduct plane recognition and matching, whereas our end-to-end pipeline relies more heavily on plane-level embeddings and thus yields slightly lower precision estimates under such extreme scenarios. The difference observed in rotation error statistics reflects this phenomenon, with our median value being smaller than that of Nope-SAC, indicating smaller typical prediction errors and higher precision for most of the test set; meanwhile, the mean value indicates the existence of large outlier error values.

NOPE-SAC Init. and NOPE-SAC Ref., relying solely on image-level features and plane-level features, respectively, exhibit much poorer performance compared to our plane-based method. This also indicates that NOPE-SAC is highly dependent on external initial priors.



\textbf{Qualitative Results.} 
Figure \ref{fig:2view_rec} visually illustrates the relative camera pose estimates of our PlaneRecTR++ from two different viewpoints (last two columns) on the ScanNetV2 and MatterPort3D datasets.  Our method can accurately recover a precise relative camera pose from input sparse views, even in scenarios with extremely low image overlap (rows 3-8), without relying on initial pose estimation and explicit corresponding plane pairs.
Figure \ref{fig:2view_rec_compare} presents the predicted pose comparison of ours and NOPE-SAC\cite{tan2023nopesac}, showing that our method recovers a more accurate relative camera pose than the leading baseline.


\subsection{3D Planar Reconstruction Evaluation}


\textbf{Quantitative Results.}
% ！！！！！ 可以去掉其他两个精度的比较，保持方法SOTA？？？？？
The numerical evaluation on the final 3D reconstruction is shown in Table \ref{tab:3DPlane}.  Our unified single-stage approach demonstrates superior performance compared to all other multi-stage methods, particularly exhibiting a significant enhancement in the reconstruction accuracy on the real-world dataset ScanNetv2.

\textbf{Qualitative Results.} 
Figure \ref{fig:2view_rec} presents the visualization of the 3D plane reconstruction results achieved by PlaneRecTR++ on the ScanNetv2 \cite{Dai:etal:CVPR2017, Dai:scannetv2web} and the MatterPort3D \cite{chang2017matterport3d} datasets. Note that our model implicitly learns plane matching during pose inference, and we further extract and process the probability distributions of plane correspondences from our model (see 3rd column).   In Figure \ref{fig:2view_rec_compare}, a comparison between our proposed PlaneRecTR++ and the current state-of-the-art NOPE-SAC \cite{tan2023nopesac} is further provided on both datasets, demonstrating that our method yields more precise plane reconstructions and recovers more accurate relative camera poses from sparse views.
Even in more challenging scenarios characterized by inconsistent brightness (column 1,3), confusion caused by symmetrical repetitive patterns (column 6) and so on, our method can implicitly acquire robust correspondences for reconstruction and pose recovery, which is outperforms NOPE-SAC that relies on initial pose prior and matching supervision.


\vspace{-0.5cm}
\input{figures/heatmap}
\vspace{-0.5cm}

\input{tables/pose_model_ablation}
\input{tables/corr_ablation}
\subsection{Ablation Studies of Model Designs}
\label{sec:sparseview_ablation}


We conducted extensive ablation studies to investigate the contributions of each design choices, particularly within our plane aware cross attention layer in Section \ref{sec:plane_cross_attention}. We focus on experimenting with following two aspects: (1) cross embedding structure (CE) within the bilinear attention mechanism, and (2) the specialized design of query, key and value subdivision.





\paraspace
\ptitle{Cross Embeddings.}
Our method differs from Pose ViT \cite{rockwell20228posevit}, which also employs bilinear attention, in that we cross-place plane embeddings of different input images on both sides of the bilinear attention matrix, while Pose ViT place visual features and positional encodings of the same input image, which experimentally yields better results as shown in \cite{rockwell20228posevit}.We consider that our cross plane embeddings placement follows a more intuitive discipline and better performance. To validate our idea, we follow  \cite{rockwell20228posevit} and shift our plane aware cross attention layer's structure to the same embedding placement strategy.  The experimental findings (rows 1, 2 of Table \ref{tab:pose_model_ablation} and \ref{tab:corr_ablation}) witness a significant degradation in performance without proposed cross embeddings set-up.

We believe the key reason for the differences between ours and Pose ViT lies in whether the network truly learns plane correspondences. In both methods, it is widely anticipated that the similarity attention matrix would serve as the function for the object assignment matrix.
However, only our plane aware cross attention design empowers the network to effectively execute authentic plane-level matching, considering that it is less clear to generate plausible patch-wise correspondences of two sparse views.  Consequently, in our method, cross embedding placement naturally yields superior performance compared to Pose ViT, wherein this prerequisite is not met and may even impede efficient passing of visual features through cross-embedding attention.

\paraspace
% \ptitle{One Piece Key/Query.}
\ptitle{Query, Key and Value Designs.} We show the effectiveness of our query, key and value designs by evaluating two model variants on pose, plane correspondences and reconstruction, respectively. 


In  Table \ref{tab:pose_model_ablation}, on both datasets, pose accuracy of PlaneRecTR++ with the unsplit query and key design is more precise when maintaining VNum. is 4 (rows 1, 3). In Figure \ref{subfig:qk1v4} and \ref{subfig:qkv4}, the highlighted areas of the plane correspondence attention matrix $\rm{C}(Q_i, K_j)$ using our unsplit key and query, align well with the ground truth correspondence. However, the distribution of 4 similarity attention matrices, each computed using one of the 4 split query and key segments, does not effectively capture the ground truth pattern. Though the combination of 4 similarity matrices can approximate actual plane correspondence distribution, it is still inferior to ours caused by more introduced noisy matches with high probabilities. 
We consider its potential in capturing real correspondences via a post-hoc evaluation, where we select the one head with the highest matching accuracy to ground truth and compare it with our method.  As presented in rows 1, 3 of two datasets in Table \ref{tab:corr_ablation},  even after carefully selecting the best possible matches from the split query and key pairs, the performance is still inferior to ours adopting unsplit query and key pairs.

Moreoever, in Table \ref{tab:pose_model_ablation} and \ref{tab:corr_ablation}, when the key and query are guaranteed to be complete, the accuracy of all metrics with VNum. $=4$ still surpasses that with VNum. $=1$ (rows 1, 4), indicating a positive contribution from the partition of value term.


On the whole, we have validated that our design not only retain the advantages of multi-head attention in standard Transformer, but also effectively capturing the distribution of plane correspondence and further enhancing model performance.







\input{tables/corr_comparison}


\vspace{-0.3cm}
\subsection{Studies of Unified Plane Embedding}
\label{sec:unified_plane_emb}

During the inter-frame plane query learning stage, we have actually conducted several experiments to enhance the capability of input plane embedding with auxiliary knowledge. Such attempts include concatenating cosine positional encoding \cite{sun2021loftr}, quadratic positional encoding of plane center \cite{rockwell20228posevit}, plane parameter encoding or plane appearance embedding along with original plane embedding. We also explored to filter out plane embedding sequence using their plane probability $p_i$, or to incorporate several self-attention layers \cite{rockwell20228posevit, Vaswani:etal:NIPS2017} to promote contextual features, or to introduce an explicit view consistency loss of planes and pose during training.
%(see Appendix?).
However, none of these variants yielded any obvious improvement in the current model's performance.

It became evident that our unified plane embedding, achieved through a simple combination of intra-frame and inter-frame plane query learning, already encompassed adequate information to address the task of sparse views planar reconstruction.


\paraspace
\ptitle{Consistent Planar Attributes across Frames.}
After the initial single view training and following comprehensive sparse view training, rows 2,3 of Table \ref{tab:planerectr++_scannetv2_mp3d_seg} and Table \ref{tab:planerectr++_scannetv2_mp3d_recall} exhibits comparable performance in monocular plane detection on two datasets.   In the rows 3,4 of Table \ref{tab:corr_compare}, the former relies solely on similar appearance features from a single view and achieves poor matching results, whereas the latter computes a reasonable and precise plane correspondence. In Figure \ref{fig:tsne}, despite of only being trained on input sparse views, our unified query embeddings exhibit promising consistency across more frames without the need for ground truth correspondence supervision.

\input{figures/tsne}


\paraspace
\ptitle{Implicit Plane Matching.}
Compared with the differentiable optimal transport (OT) \cite{sarlin2020superglue} method from NOPE-SAC \cite{tan2023nopesac}, our approach owns the following advantages:
1) Most importantly, we skip the requirment of pose initialization in all previous methods \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}. This means that there is no need for us to convert plane parameters into the same coordinate system before  effectively utilize planar geometry for matching.  We believe that this is a crucial factor for constructing a single-stage method.
2) We do not require explicit supervision using ground truth correspondences. Instead, only through pose supervision, our carefully designed simple network structure actively learns multi-view consistency for plane embedding.  In a single forward pass, our method implicitly performs plane matching and probabilistically synthesizes pairwise plane features for pose prediction.  
It does not explicitly perform plane matching and input hard plane pairs to a pose refinement network \cite{jin2021sparseplanes, tan2023nopesac}.
3) The correspondence attention matrix formed by our network can be processed using a simple MNN (maximum nearest neighbor) operation to obtain a hard plane assignment matrix.  The accuracy of this assignment matrix is comparable or even higher than previous methods with supervisions, as shown in Tabel \ref{tab:corr_compare}

