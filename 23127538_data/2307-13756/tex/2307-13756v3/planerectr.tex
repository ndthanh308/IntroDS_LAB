\input{figures/planerectr_network}

% \section{PlaneRecTR(Single View Plane Recovery)}
\section{Intra-Frame Plane Query Learning}
\label{sec:intra_frame_method}

In this section, we present the details of intra-frame plane query learning, \ie, PlaneRecTR. We start by first introducing its architecture in Section~\ref{sec:model}, and then discuss the training process and loss functions in Section \ref{sec:training}. Finally, we describe the inference process of recovering 3D planes from a single view in Section \ref{sec:planeinference}.


%-------------------------------------------------------------------------
% \subsection{Unified Query Learning for Plane Recovery}
% \subsection{PlaneRecTR Module}
\subsection{Transformer-based Unified Query Learning for Single-view Plane Recovery}
\label{sec:model}


Inspired by the recent successes of DETR \cite{carion2020end} and Mask2Former \cite{cheng2022masked} in object detection and instance segmentation, we find that, it is not impossible to tackle the challenging monocular planar reconstruction task using a \textit{single}, \textit{compact} and \textit{unified} framework, thanks to the merits of query-based reasoning to enable joint modeling of multiple tasks.

As shown in Figure \ref{fig:planerectr_network}, intra-frame plane query learning component consists of three main modules: (1) A pixel-level module to learn dense pixel-wise deep embedding of the input colour image. (2) A Transformer-based unified query learning module to jointly predict, for each of $N$ learnable plane queries, its corresponding plane embeddings $\mathcal{E}_\text{plane}$ as well as four target properties, including plane classification probability $p_i$, plane parameter $n_i$, mask embedding, and depth embedding ($i\in[1,2,..., N]$). Specifically, $p_i$ is the probability to judge whether the $i^\text{th}$ query corresponds to a plane or not; $n_i\doteq \tilde{n_i}/d_i \in \mathbb{R}^3$,  where $\tilde{n_i} \in \mathbb{R}^3$ is its plane normal and $d_i$ is the distance from the $i^\text{th}$ plane to camera center, \ie, offset. (3) A plane-level module to generate plane-level mask $m_i$ and plane-level depth $d_i$ through mask and depth embedding ($i\in[1,2,..., N]$). We then remove non-plane query hypothesis while combining the remaining ones for the final image-wise plane recovery. These three modules will be described in details bellow.



\paraspace
\ptitle{Pixel-Level Module.}
Given an image of size $H \times W$ as input, we use the pre-trained ResNet-50 \cite{He:etal:CVPR2016} as the backbone model to extract dense image feature maps, unless otherwise mentioned. Similar to Mask2Former, a multi-scale convolutional pixel decoder is used to produce a set of dense feature maps with four scales, denoted as follows:
\begin{equation}
\begin{aligned}
\mathbb{F}&=\{F_{1}\in \mathbb{R}^{C_{1}\times H/32\times W/32}, F_{2}\in \mathbb{R}^{C_{2}\times H/16\times W/16}, \\
& F_{3}\in \mathbb{R}^{ C_{3}\times H/8\times W/8}, \mathcal{E}_\text{pixel}\in \mathbb{R}^{ C_{\mathcal E}\times H_{\mathcal E}\times W_{\mathcal E}}\}.
\end{aligned}
\end{equation}

Among these, the first three feature maps $\{F_{1}, F_{2}, F_{3}\}$ are fed to the Transformer module, while the last one $\mathcal{E}_\text{pixel}$, a dense per-pixel embedding of resolution $H_{\mathcal E}=H/4$ and $W_{\mathcal E}=W/4$, is exclusively used for computing plane-level binary masks and plane-level depths.

\paraspace
\ptitle{Transformer Module.}
We use the Transformer decoder with masked attention proposed in \cite{cheng2022masked}, which computes \textbf{unified plane embeddings} $\mathcal{E}_\text{plane} \in \mathbb{R}^{N \times C_{\mathcal E}}$ from above mentioned multi-scale feature maps $\{F_{1}, F_{2}, F_{3}\}$ and $N$ learnable plane queries. The predicted $\mathcal{E}_\text{plane}$ are then independently projected to four target properties by four different multi-layer perceptrons (MLPs). Overall, the Transformer module predicts required planar attributes through $N$ plane queries (upper left part in Figure \ref{fig:planerectr_network}).

\paraspace
\ptitle{Plane-Level Module.}
 As shown in the upper right part of Figure \ref{fig:planerectr_network}, we obtain a dense plane-level binary mask $m_i \in [0, 1]^{H_{\mathcal E} \times W_{\mathcal E}}$/depth prediction $d_i \in \mathbb{R}^{H_{\mathcal E} \times W_{\mathcal E}}$ by a dot product between the $i^\text{th}$ mask/depth embedding and the dense per-pixel embedding $\mathcal{E}_\text{pixel}$ from previous two modules, respectively.
 
Please note that we have also investigated the idea of learning two individual pixel decoders respectively for semantic and depth dense embeddings, in order to separately predict planar masks and depths. However, we did not observe a clear performance improvement, and therefore stick to current set-up with a shared $\mathcal{E}_\text{pixel}$ for the sake of efficiency and simplicity.

We finally obtain $N$ plane-level predictions $\{y_i=(p_i,n_i, m_i, d_i)\}_{i=1}^N$ by the plane-level module, and each plane-level prediction contains all the necessary information to recover a possible 3D plane.


%-------------------------------------------------------------------------
\subsection{Training Objective and Configuration}
\label{sec:training}
% \paraspace
\ptitle{Plane-level Depth Training.}
Previous methods tend to predict a global image-wise depth by separate network branches to calculate plane offset \cite{Liu:etal:CVPR2019:Planercnn} or use depth as additional cues to formulate segmentation masks \cite{Yu:etal:CVPR2019:PlaneAE,Tan:etal:ICCV2021:Planetr}.
In contrast, our method tries to achieve mutual benefits between planar semantic and geometric reasoning, 
we leverage joint query learning to unify all components of plane recovery in a concise multi-task manner. As a result, we explicitly predict dense plane-level depths, binary-masks, plane probabilities and parameters from a shared feature space, which is produced and refined via attention mechanism of the Transformer.

% Therefore we additionally add plane-level Depth prediction Task in the training phase, and just add an MLP and dot product operation in the implementation.
\paraspace
\ptitle{Bipartite Matching.}
During training, one important step is to build optimal correspondences between $N$ predicted planes and $M$ ground truth planes ($N \geq M$).
% We achieve this following \cite{cheng2022masked} using bipartite matching by searching for a permutation $\hat{\sigma}$ (where $\sigma(i)$ indicates the matched index of the predict planes to the ground truth $\hat{s}_{p}^{i}$) by minimizing a matching defined cost function $D$:
Following bipartite matching of \cite{cheng2022masked, Tan:etal:ICCV2021:Planetr}, we search for a permutation $\hat{\sigma}$ 
% (where $\sigma(i)$ indicates the matched index of the predict planes to the ground truth $\hat{s}_{p}^{i}$) 
by minimizing a matching defined cost function $D$:
\begin{equation}
\label{eq: bi-matching}
\hat{\sigma}=\underset{\sigma}{\arg \min } \sum_{i=1}^{N} D\left(\hat{y}_{i}, y_{\sigma(i)}\right), 
\end{equation}
\vspace{-0.5cm}
\begin{align}
\label{eq: bi_matching2}
% D=-\omega_{1} \, p_{\sigma(i)}\left( \hat{p}_{i}\right)
% &+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{2} \, L_{1}\left(\hat{{n}}_{i}, {n}_{\sigma(i)}\right) \nonumber\\
% &+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{3} \, L_{1}\left(\hat{{d}}_{i}, {d}_{\sigma(i)}\hat{m}_{i}\right)  \nonumber\\
% &+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{4} \, L_{ce}
% \nonumber\\
% &+ \mathbbm{1}_{\{\hat{p}_{i}=1\}}  \, \omega_{5} \, L_{dice},
% D=\mathbbm{1}_{\{\hat{p}_{i}=1\}} \left[& -\omega_{1} \, p_{\sigma(i)}
% + \omega_{2} L_{1}\left(\hat{{n}}_{i}, {n}_{\sigma(i)}\right) \nonumber \right. \\
% &+ \omega_{3} L_{1}\left(\hat{{d}}_{i}, {d}_{\sigma(i)}\hat{m}_{i}\right)  \nonumber\\
% &+ \omega_{4} L_{ce} \nonumber\\
% & \left. +  \omega_{5} L_{dice} \right],
% D=\mathbbm{1}_{\{\hat{p}_{i}=1\}} \Big[
% & -\omega_{1} \, p_{\sigma(i)}
% + \omega_{2} L_{1}\left(\hat{{n}}_{i}, {n}_{\sigma(i)}\right) \nonumber\\
% & + \omega_{3} L_{1}\left(\hat{{d}}_{i}, {d}_{\sigma(i)}\hat{m}_{i}\right) + \omega_{4} L_{ce}  +  \omega_{5} L_{dice} \Big],
D= & \mathbbm{1}_{\{\hat{p}_{i}=1\}} \Big[
 -\omega_{1} \, p_{\sigma(i)}
+ \omega_{2} L_{1}\left(\hat{{n}}_{i}, {n}_{\sigma(i)}\right) \nonumber\\
& + \omega_{3} L_{1}\left(\hat{{d}}_{i}, {d}_{\sigma(i)}\hat{m}_{i}\right) + \omega_{4} L_{ce}  +  \omega_{5} L_{dice} \Big],
\end{align}
where $\hat{y}_{i} = (\hat{p}_i,\hat{n}_i, \hat{m}_i, \hat{d}_i)$ are the $i^\text{th}$ ground-truth plane attributes, we augment the ground truth instances with non-planes where $\hat{p}_{i}=0$ if $i\textgreater M$; $\sigma(i)$ indicates the matched index of the predicted planes to the ground truth $\hat{y}_{i}$;$\mathbbm{1}$ is an indicator function taking 1 if $\hat{p}_{i}=1$ is true and 0 otherwise; $\omega_{1}, \omega_{2}, \omega_{3}, \omega_{4}$ and $\omega_{5}$ are weighting terms and set to 2, 1, 2, 5, 5, respectively. 
% Here we additional consider of influence of mask and depth quality using an $L_1$ depth loss and segmentation dice loss $L_{dice}$, respectively.
Here we additionally consider the influence of mask and depth quality using a mask binary cross-entropy loss $L_{ce}$ \cite{cheng2022masked}, a mask dice loss $L_{dice}$ \cite{milletari2016v} and an $L_1$ depth loss, respectively.

\paraspace
\ptitle{Loss Functions.}
After bipartite matching, the final training objectives $L$ is composed of following four parts:
\vspace{-0.3cm}
\begin{equation}
% \small 
\mathcal{L} = 
\sum\limits_{i=1}^{M}\left( \lambda\mathcal{L}_{\text {cls }}^{(i)} + \mathcal{L}_{\text {param }}^{(i)} + \mathcal{L}_{\text{mask}}^{(i)} + \lambda\mathcal{L}_{\text {depth}}^{(i)} \right),
\end{equation}
where $\lambda$ is a weighting factor and is set to 2 in this paper. $\mathcal{L}_{\text {cls}}$ and $\mathcal{L}_{\text {param}}$ are a plane classification loss and a plane parameter loss, in a similar form to previous work \cite{Tan:etal:ICCV2021:Planetr}.
% \begin{equation}
% \mathcal{L}_{\text {cls }}^{(i)} = -\log p_{{\hat{\sigma}}(i)}\left( \hat{p}_{i}\right).
% \end{equation}
% \begin{align}
% \label{eq:param}
% \mathcal{L}_{\text {parm }}^{(i)} =
% & \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,          L_{1}\left(\hat{{n}}_{i}, {n}_{{\hat{\sigma}}(i)}\right) + \nonumber\\
% & \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{cos} \left(1- \cos\left(\hat{{n}}_{i}, {n}_{{\hat{\sigma}}(i)}\right)\right) + \nonumber\\
% & \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{q} \sum\limits_{q\in Q_{i}} \| {n}_{{\hat{\sigma}}(i)}^{T} q - 1 \|,
% \end{align}
% where $Q_{i}$ is the set of 3D points calculated from pixels belonging to ground truth plane instance via the ground truth depth map. $\beta_{cos} = 5$ and $\beta_q = 2$ in this paper.

However, different from PlaneTR \cite{Tan:etal:ICCV2021:Planetr}, the left two loss terms in our paper $\mathcal{L}_{\text{mask}}$ and $\mathcal{L}_{\text {depth}}$ are designed to explicitly learn dense planar masks and depths. Specifically,
we introduce the plane segmentation mask prediction loss, as a combination of a cross-entropy loss and a dice loss:
% \begin{align}
% % \label{eq:mask}
% \mathcal{L}_{\text {mask }}^{(i)} =
% & \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{1} L_{ce} + \nonumber\\
% & \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{2} L_{dice},
% \end{align}
\begin{equation}
% \label{eq:mask}
\mathcal{L}_{\text {mask }}^{(i)} =
 \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{1} L_{ce} + \nonumber
 \mathbbm{1}_{\{\hat{p}_{i}=1\}}   \,  \beta_{2} L_{dice},
\end{equation}
where $\beta_{1} = 5$ and $\beta_{2} = 5$.
The depth prediction loss is in a typical $L_1$ form, penalizing the discrepancy of depth value within planar regions:
\begin{equation}
% \label{eq:mask}
\mathcal{L}_{\text {depth }}^{(i)} =
 \mathbbm{1}_{\{\hat{p}_{i}=1\}}  L_{1}\left(\hat{{d}}_{i}, {d}_{\sigma(i)}\hat{m}_{i}\right).
\end{equation}

%-------------------------------------------------------------------------
\subsection{Inference Process of Monocular 3D Plane Recovery}
\label{sec:planeinference}

% sjj: argmin->argmax
In this section, we briefly discuss how to recover 3D planes from our plane-level predictions. For the $N$ plane-level predictions $\{y_i\}_{i=1}^N$ predicted by the network, we first drop non-plane candidates according to the plane classification probability $p_i$, leading to a valid subset of $K$ planes (\ie, $K\leq N$). 
% The final planar segmentation mask of the input image is obtained by, for each pixel position of planar region, the most probable plane index based on segmentation mask prediction, \ie, $\mathop{\arg\max}\limits_{i}\{m_i\}_{i=1}^{K}$.
For each pixel within the planar regions, we calculate 
the most likely plane index
$\mathop{\arg\max}\limits_{i}\{m_i\}_{i=1}^{K}$ to obtain its mask id and thus obtain the final image-wise segmentation mask.



In this simple and efficient manner, we can use the network predictions to create 3D plane reconstruction of the input frame. Note that plane-level depths are not involved during inference and we use plane parameters and segmentation to infer planar depths. 
We experimentally found that this design also leads to more structural and smooth geometric predictions than that relying on direct depth predictions.