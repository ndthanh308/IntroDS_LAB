\input{figures/crossplaneattn_network}
\section{Inter-Frame Plane Query Learning}
\label{sec:inter_frame_method}
%-------------------------------------------------------------------------
In this section, we presented how we extend our single-view plane recovery framework PlaneRecTR to a novel multi-view setup, while still retaining the virtue of query-based learning.
% The unified plane embeddings learned from PlaneRecTR have encompassed comprehensive contextual information regarding 3D geometry, position, context, and appearance of each plane, which have enabled accurate reconstruction of real-scale 3D plane from a single view. To fully exploit the strengths of these embeddings, we aim to eliminate the external pose initialization in previous methods \cite{jin2021sparseplanes,agarwala2022planeformers,tan2023nopesac}, but directly learn precise camera pose from plane embedding sequences of input views. 
% To achieve this goal, we drew inspiration from \cite{} and have introduced dual softmax and bilinear attention mechanisms to modify the standard cross attention for our framework. Additionally, we have incorporated network designs that are more suitable for multi-view plane properties and eliminated redundant inputs such as position encoding. The introduced modifications in Section \ref{sec:modifications} confer the similarity attention matrix with the notion of (planar) correspondence in classical two view geometry \cite{}, endowing the pose regression network with enhanced interpretability.
We introduce an inter-frame query learning component on top of unified plane embeddings of per-frame. A plane aware cross attention layer is proposed to achieve inter-frame plane interactions, within which dual softmax \cite{sun2021loftr, rockwell20228posevit} and bilinear attention \cite{kim2018bilinear, rockwell20228posevit} mechanisms are used to align the intermediate attention structure with a 2D plane correspondence matrix and enable plane-level feature fusion between views.
%Such design guarantees that the attention procedure truly accomplishes plane matching, allowing the network to spontaneously focus on genuine paired planes from two views. In our implementation, we linearly project representative unified plane embeddings to motivate key, query, and value attributes to better accommodate multi-view plane properties while eliminating redundant inputs like position encoding \cite{sun2021loftr, rockwell20228posevit}.  
Most importantly, we further modify the key, query, and value forms of standard multi-head attention \cite{Vaswani:etal:NIPS2017} to effectively utilize the complete representation of unified plane embeddings, which better accommodates multi-view plane properties without requiring any additional inputs such as position encoding \cite{sun2021loftr, rockwell20228posevit}. This simple adjustment guarantees that our attention structure truly accomplishes plane matching, allowing the network to spontaneously focus on genuine paired planes from two views.

As illustrated in Figure \ref{fig:crossplaneattn_network}, we employ two plane-aware cross attention layers and an MLP head to construct a simple and lightweight pose regression module. A plane-aware attention layer first takes two-view plane embeddings $\mathcal{E}_\text{plane}^1$ and $\mathcal{E}_\text{plane}^2$ as input, and actively learn their correspondences within the network. Subsequently, our model directly regresses a relative camera pose $T=\{R,t\}$ from probabilistic paired plane embeddings. Conceptually, this entire process aligns with the logical framework of solutions based on classical two-view geometry \cite{hartley2003multiple},
%In addition, since plane-aware cross attention confers the similarity attention matrix with the notion of plane correspondence in the classical two-view geometry \cite{hartley2003multiple},
thereby offering enhanced interpretability. 


% Intuitively, a plane aware cross attention layer takes plane embeddings $\mathcal{E}_\text{plane}^1$ and $\mathcal{E}_\text{plane}^2$ as input and actively learn the correspondence between plane sequences in two images. Subsequently, our model directly regresses a relative camera pose $T=\{R,t\}$ from probabilistic paired plane embeddings. Conceptually, this entire process aligns with the logical framework of solutions based on two-view geometry \cite{}. 

%The estimated rotation matrix R consists of four elements, while t is represented by vectors, specifically expressing the relative transformation relationship from the camera coordinate system of image1 to that of image2


\subsection{Cross Attention for Unified Plane Embeddings}
\label{sec:plane_cross_attention}
% \subsection{}


% The attention function in transformer maps query and key-value pairs to an output, which is computed as a weighted sum of values. Each value's weight is determined by a similarity function between the query and the corresponding key. The scaled dot-product is commonly chosen as similarity function due to its faster computation speed and space efficiency.

% $Attention(Q, K, V) = softmax( QK^T / sqrt(d_k) )V$

% Cross attention is an attention mechanism that integrates two distinct embedding sequences. In Figure.\ref{fig:corrattn_standard_crossattn}(a), one of the embedding sequences serves as a query input, while the other generates key and value inputs. So cross attention facilitates the utilization of embedding sequence to guide the learning of another one.
% Multi-head attention structure in transformer is illustrated in Figure.\ref{fig:corrattn_standard_crossattn}(b). Key, query and value are typically partitioned into $h$ segments along the feature dimension using linear projections. The segments are then processed by $h$ parallel cross attention layers and ultimately merged to restore the original feature dimension. The utilization of multi-head cross attention enables the model to establish multiple sub-spaces, thereby facilitating the model's ability to concentrate on diverse aspects of features and acquire more comprehensive and informative knowledge without incurring additional computational costs.
% \paraspace
\ptitle{Preliminaries: Standard Cross Attention.}
% Standard Cross attention \cite{} commonly employs scaled dot products as the similarity function , enabling the utilization of one embedding sequences to guide the learning of another sequence:
% The attention layer in Transformer \cite{} maps query and key-value pairs to an updated output value in a from of weighted summation. The weight of each value is commonly determined by a scaled dot-product similarity function $\rm{S}$ between the query and the key. Cross attention is an attention mechanism that facilitates the utilization of embedding sequence to guide the learning of another one, where one of the embedding sequences serves as a query input $Q_i$, while the other generates key and value inputs $K_j$, $V_j$. Besides, cross attention is often built in a multi-head manner \cite{}, whose $Q_i$, $K_j$ and $V_j$ are typically partitioned into $n_h$ segments $\{q_i^h\}_{h=1}^{N_h}$, $\{k_j^h\}_{h=1}^{N_h}$, $\{v_j^h\}_{h=1}^{N_h}$ along the feature dimension using linear projections. The utilization of multi-head attention enables the model to concentrate on diverse aspects of features and acquire more comprehensive information without incurring additional computational costs.
% The aforementioned multi-head similarity function ($\rm{S}$) and the overall multi-head cross attention layer ($\rm{MCA}$) are represented by formula \ref{eq:standard_crossattn1} and \ref{eq:standard_crossattn2}, respectively.
The Transformer cross attention layer \cite{Vaswani:etal:NIPS2017} updates the input value term by mapping query from the $i$-th input and key-value pair from another $j$-th input through a weighted summation, typically using a scaled dot-product similarity function $\rm{S}(\cdot,\cdot)$. In addition, attention often employs a multi-head strategy \cite{Vaswani:etal:NIPS2017} where queries, keys, and values (denoted $Q_i$, $K_j$, $V_j$ 
$\in \mathbb{R}^{N \times C_{\mathcal E}}$, respectively)
are divided, along channel dimensions, into $N_h$ segments $\{q_i^h\}_{h=1}^{N_h}$, $\{k_j^h\}_{h=1}^{N_h}$, $\{v_j^h\}_{h=1}^{N_h} \in \mathbb{R}^{N \times \frac{C_{\mathcal E}}{N_h}}$, in order to enhance the expressiveness and diversity of results without incurring extra computational costs. 

The multi-head similarity function ($\rm{S}$) and  cross attention layer ($\rm{MCA}$) are defined in Equations \ref{eq:standard_crossattn1} and \ref{eq:standard_crossattn2}:
\begin{small}
\begin{equation}
\label{eq:standard_crossattn1}
% \rm{Attention}(Q_i, K_j, V_j) = \rm{softmax}( \frac{Q_iK_j^T}{\sqrt{d_k}} )V_j
\rm{S}(q_i^h, k_j^h) =  \rm{softmax}\left(\frac{q_i^h{k_j^h}^T}{\sqrt{C_{\mathcal E}/N_h}}, 1\right),
\end{equation}
\end{small}
\begin{small}
\begin{align}
\label{eq:standard_crossattn2}
\rm{MCA}(Q_i, K_j, V_j) = \rm{Linear}(\rm{Concat}(\{\rm{S}(q_i^h, k_j^h)v_j^h\}_{h=1}^{N_h})),
\end{align}
\end{small}where $\rm{softmax}(\cdot, k)$ applies softmax operation across the $k$-th axis; $\rm{Linear}$ and $\rm{Concat}$ mean linear projection and channel-wise concatenation, respectively.

% Where one of the embedding sequences serves as a query input $Q_i$, while the other generates key and value inputs $K_j$, $V_j$. Besides, cross attention in Transformer is built in a multi-head manner \cite{}, whose query $Q_i$, key $K_j$ and value $V_j$ are typically partitioned into $n_h$ segments $\{q_i^h\}_{h=1}^{N_h}$, $\{k_j^h\}_{h=1}^{N_h}$, $\{v_j^h\}_{h=1}^{N_h}$ along the feature dimension using linear projections. The utilization of multi-head attention enables the model to concentrate on diverse aspects of features and acquire more comprehensive information without incurring additional computational costs.
Our proposed model targets equations \ref{eq:standard_crossattn1} and \ref{eq:standard_crossattn2} for intuitive and efficient plane-specific modifications, achieving implicit plane matching and direct regression of precise pose.

\paraspace
% \ptitle{Dual Softmax}
\ptitle{Plane Correspondence Probability Function.}
% The process of inferring pose from plane instance sequences from two views is heavily reliant on the correspondence between the two sequences.    However, due to the similar texture and geometric characteristics of plane instances, there exists a chicken-and-egg relationship between explicitly learning pose and explicitly learning plane correspondence. In all previous methods, the representation ability of the plane embedding learned by GT correspondence supervision is insufficient to accurately track the same plane under different views. To address this problem, an additional neural model was introduced to calculate the initial pose and obtain plane information within a unified coordinate system to assists the original plane embedding in improving matching accuracy and ultimately refine the initial pose.
% To transcend the previous multi-stage pipeline paradigm \cite{} and facilitate end-to-end learning for enhanced efficiency and performance, We have specifically devised an inter-frame correspondence attention structure for robust unified plane embedding, enabling the network to autonomously acquire probabilities of plane correspondence needed for pose inference during a single forward pass. This eliminates the need for ground truth correspondence supervision and reliance on initial pose.
To overcome the limitations of multi-stage two-view plane reconstruction paradigm \cite{jin2021sparseplanes, agarwala2022planeformers, tan2023nopesac}, here we specifically devised an inter-frame correspondence attention structure to learn reliable plane embeddings, which enable the network to autonomously acquire probabilities of plane correspondence and conduct pose inference in a single forward pass. This design also eliminates the dependency on either ground truth correspondence supervision or initial pose.




% Considering that the key and query are derived from plane embedding sequences of different views in our model, the score matrix computed by scaled dot-product during cross attention exhibits similarities with the assignment matrix between planes.  we employ dual-softmax \cite{} instead of single softmax to process the score matrix, i.e., applying softmax on both dimensions of S to derive probabilities for soft mutual nearest neighbor matching:
Specifically, in contrast to the similarity function in Equation \ref{eq:standard_crossattn1},
%employed for calculating the similarity attention matrix in the standard cross-attention layer.   
we utilize a dual-softmax operation instead of a single softmax on the \emph{unsplit} query and key embeddings $Q_i, K_j$, aiming to keep integral embedding information when constructing plane-wise correspondence probability, as shown in Equation \ref{eq:dual_softmax} below.
\begin{align}
\label{eq:dual_softmax}
% \rm{dual\_softmax}(Q_i, K_j)
% &= \rm{softmax}(\frac{Q_iK_j^T}{\sqrt{d_k}} , 1) \odot \rm{softmax}(\frac{Q_iK_j^T}{\sqrt{d_k}} , 2)
\rm{C}(Q_i, K_j) =\rm{softmax}(\frac{Q_iK_j^T}{\sqrt{C_{\mathcal E}}} , 1) \odot \rm{softmax}(\frac{Q_iK_j^T}{\sqrt{C_{\mathcal E}}} , 2).
\end{align}
We compute a 2D correspondence matrix $\rm{C}(Q_i, K_j)$,  where the element at the $m^{th}$ row and $n^{th}$ column $\rm{C}_{mn}(Q_i, K_j)$ denotes the probability that the $m^{th}$ plane embedding from the $i^{th}$ image $I_i$ corresponds to the $n^{th}$ plane embedding from  $j^{th}$ image $I_j$, indicating their likelihood of representing the same plane instance. For the task of two-view plane reconstruction, we stick to configurations of $\{i=1,j=2\}$ and $\{i=2,j=1\}$.

% \begin{align}
% \label{eq:dual_softmax}
% % \rm{dual\_softmax}(Q_i, K_j)
% % &= \rm{softmax}(\frac{Q_iK_j^T}{\sqrt{d_k}} , 1) \odot \rm{softmax}(\frac{Q_iK_j^T}{\sqrt{d_k}} , 2)
% \rm{C}(Q_i, K_j) =\rm{softmax}(\frac{Q_iK_j^T}{\sqrt{C_{\mathcal E}}} , 1) \odot \rm{softmax}(\frac{Q_iK_j^T}{\sqrt{C_{\mathcal E}}} , 2),
% \end{align}

% where softmax(Â·, k) applies softmax across the k-th axis.
%After dual-softmax operation, the similarity attention matrix has the property that the sum of each row and column is less than or equal to 1, which is formally consistent with the plane assignment matrix.
The noteworthy aspect lies in our simple modification to the input query and key's formats, which yield benefits that better align with the characteristics of plane instance. The key $K_j$ and query $Q_i$ in our model are derived from a linear mapping of the unified plane embeddings $\mathcal{E}_\text{plane}$ obtained by intra-frame query learning. We want to highlight the practical significance of an intact $\mathcal{E}_\text{plane}$ representing plane entities. Specifically, we preserve the integrity of $Q_i$ and $K_j$ rather than dividing them into multiple heads to fully leverage the representation power of unified plane embeddings encoding comprehensive information (geometry, appearance, location, context, etc.). This facilitates learning genuine correspondence distribution between planes instead of only abstractly capturing similarities among different subspace representations at various positions.

This simple design has been experimentally validated (Section \ref{sec:sparseview_ablation} and \ref{sec:unified_plane_emb}) to significantly increase the discriminative multi-view consistency of the unified plane embeddings $\mathcal{E}_\text{plane}$, which is of much higher quality than previous methods. Consequently, it facilitates the integration of real plane pairs' information without initial pose for direct precise pose estimation and enables explicit utilization in achieving accurate fusion of plane meshes from two views, thereby completing the overall reconstruction.

\paraspace
% \ptitle{Bilinear Cross Attention}
\ptitle{Inter-frame Plane Aware Cross Attention.}
\label{sec:bilinear_attntion}
The standard cross attention offers an efficient approach to selectively utilize one of input plane embedding sequences, but overlooks the interaction between two inputs. Therefore, we adopt bilinear attention \cite{kim2018bilinear, rockwell20228posevit} to incorporate planar information from both views through the plane correspondence probability distribution $\rm{C}(Q_i, K_j)$:
\begin{align}
\label{eq:bilinear_attentin}
% \rm{BiAttention}(Q_i, K_j, V_i, V_j) =V_i^T \rm{dual\_softmax}( Q_i, K_j)V_j
\rm{PCA}(Q_i, K_j, V_i, V_j) = &\rm{Linear}(\rm{Concat}( \nonumber\\
& \{(v_i^h)^T \rm{C}(Q_i, K_j)v_j^h\}_{h=1}^{N_h} )).
\end{align}

Unlike query and key terms, the value continues to be divided into $N_h$ segments along the feature dimension, maintaining the advantages of multi-head attention. The value segment $v_j^h \in \frac{C_{\mathcal E}}{N_h}$ shares the correspondence attention (Equation \ref{eq:dual_softmax}) from \emph{unsplit} query and key, allowing our model to selectively attend to actual corresponding plane embedding pairs across distinct sub-spaces.


As shown in Figure \ref{fig:crossplaneattn_network}, two parallel plane aware cross attention (Equation \ref{eq:bilinear_attentin}) are parallelly employed to capture integrated features of the corresponding plane from $I_1$ to $I_2$ as well as from $I_2$ to $I_1$. It is worth note that pose ViT \cite{rockwell20228posevit} employs the same features on both sides of bilinear attention matrix. However, we intuitively choose to cross-place embedding sequences of distinct images to model correspondences, thereby enhancing learning efficiency and yielding improved results. We will validate various design disparities through subsequent ablation studies in Section \ref{sec:sparseview_ablation}. 



\subsection{Pose Regression}
\label{sec:pose_loss}
Each plane aware cross attention layer ultimately learns a feature map of size $n_h\times \frac{C_{\mathcal E}}{n_h} \times \frac{C_{\mathcal E}}{n_h}$. The corresponding plane feature maps from two parallel cross attention layers are concatenated and then mapped to a relative camera pose $T=(t, q) \in SE(3)$ using a simple MLP with two hidden layers. Here, $t \in \mathbb{R}^3$ represents translation in real units, and $q \in \mathbb{R}^4$ denotes a unit quaternion representing rotation transformation satisfying $\Vert q \Vert = 1 $.

We use lietorch\cite{teed2021lietorch} to calculate the geodesic distance $\mathcal{G} \in \mathbb{R}^6$ between the predicted pose $T$ and the ground truth pose $T^{\ast}$ as the loss for backpropagation:
\begin{equation}
\mathcal{G} (T, T^{\ast}) = 
\rm{Log} ( T.inv() \cdot  T^{\ast}),
\end{equation}
\begin{equation}
\mathcal{L}_{pose} = \lambda_t \Vert \mathcal{G}_{1:3} \Vert + \lambda_q \Vert \mathcal{G}_{4:6} \Vert,
\end{equation}
where $\lambda_t = 5$, $\lambda_q = 15$.
It should be noted that $\mathcal{L}_{pose}$ serves as the only objective for our inter-frame component. Without requiring correspondence supervision, the proposed framework allows for the discovery of plane correspondence, and transforms the abstract similarity attention distribution within network into a concrete probabilistic distribution of plane correspondences. Furthermore, the bilinear structure effectively utilizes integrated features of inter-frame planes, leading to accurate pose recovery without relying on external initial poses.

% This active learning process allows for the acquisition of plane correspondence within the network structure, transforming the abstract similarity attention distribution in the transformer into a concrete probabilistic distribution of plane correspondences.  Additionally, CorrAttn's bilinear structure effectively utilizes integrated features of inter-frame planes.  Ultimately, this module accurately recovers pose from the plane structure without relying on an initial pose as a hint.

\subsection{Inferring Sparse Views Planar Reconstruction}
% Monocular planes recovery

% Plane matching

% Planar fusion
After intra-frame plane query learning, we can independently recover the 3D plane sets of two images in their respective camera coordinate systems using the corresponding unified plane embeddings as described in Section \ref{sec:model} and \ref{sec:planeinference}.

% Following inter-frame plane query learning, our unified plane embeddings capture multi-view consistency and the probability distribution computation for plane correspondence enables efficient plane matching within the attention layer.
During inference, we extract the learned correspondence matrix $\rm{C}(Q_i, K_j)$ from the network and filter out low-probability correspondences using a threshold $\theta$. We then employ the mutual nearest neighbor (MNN) criteria \cite{sun2021loftr} to obtain a hard assignment of two plane sets. Like previous methods for sparse view planar reconstruction \cite{jin2021sparseplanes,agarwala2022planeformers,tan2023nopesac}, based on above estimated camera pose and plane matching results, we transform the plane attributes into canonical viewpoint for final reconstruction and evaluation.  Specifically, we merge normals, offsets, and textures of paired monocular 3D planes \cite{jin2021sparseplanes} to achieve a geometrically precise and smooth reconstruction using sparse views. Those paired planes whose deviations in merged normals or offsets exceed predefined thresholds are removed during inference. 
