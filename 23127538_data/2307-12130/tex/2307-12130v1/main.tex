\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{verbatim}
\usepackage{graphicx}
\graphicspath{{./figures/}}
\usepackage{balance}
\usepackage{hyperref}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}

%%% Definitions
\newcommand{\taucamera}{\href{https://www.flir.com/products/tau-2/}{\texttt{Tau2}}}
\newcommand{\blackbody}{\href{https://www.ci-systems.com/sr-800n-superior-accuracy-blackbody}{\texttt{SR-800N}}}
\newcommand{\scientificCamera}{\href{https://www.flir.com/products/a655sc/}{\texttt{A655sc}}}
\newcommand{\campbellCtrl}{\href{https://www.campbellsci.com/cr1000}{\texttt{Campbell CR1000}}}
\newcommand{\opPoint}{operating~point}
\newcommand{\Tfpa}{T_{amb}}
\newcommand{\tfpa}{t_{amb}}
\newcommand{\Tobj}{T_{obj}}
\newcommand{\tobj}{t_{obj}}
\newcommand{\tobjapprox}{\Tilde{t}_{obj}}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\mat}[1]{ \underline{\underline{#1}}}
\newcommand{\nChannels}{\delta}
\newcommand{\nLevels}{\alpha}
\newcommand{\mPoly}{M_{pl}}
\newcommand{\NumTempObj}{L_{obj}}
\newcommand{\mSpatial}{M_{sp}}
\newcommand{\mRadial}{M_{rad}}
\newcommand{\mFPA}{M_{amb}}
\newcommand{\NumFPA}{L_{FPA}}
\newcommand{\TambAsVec}{\left[\Tfpa[0],...,\Tfpa[\NumFPA]\right]}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}

\newcommand{\sizePatches}{0.195}
\newcommand{\figPathces}[1]{
        \subfloat{% Figure removed}
        \hfill
        \subfloat{% Figure removed}
        \hfill
        \subfloat{% Figure removed}
        \hfill
        \subfloat{% Figure removed}
        \hfill
        \subfloat{% Figure removed}
        % \hfill
        % \subfloat{% Figure removed}
}
\newcommand{\figPathcesCaption}[1]{
        \subfloat[Sample]{% Figure removed}
        \hfill
        \subfloat[Ground Truth]{% Figure removed}
        \hfill
        \subfloat[GxPD]{% Figure removed}
        \hfill
        \subfloat[He et al.~\cite{He2018}]{% Figure removed}
        \hfill
        \subfloat[ADMIRE~\cite{admire}]{% Figure removed}
        % \hfill
        % \subfloat[]{% Figure removed}
}


\begin{document}
\title{Improving temperature estimation in low-cost infrared cameras using deep neural networks}
\author{\href{mailto:navotoz@mail.tau.ac.il}{Navot Oz},
        \href{mailto:sochen@math.tau.ac.il}{Nir Sochen},
        \href{mailto:mend@eng.tau.ac.il}{David Mendelovich},
         and \href{mailto:iftach@volcani.agri.gov.il}{Iftach Klapp}

\thanks{\href{mailto:navotoz@mail.tau.ac.il}{Navot Oz}
        and \href{mailto:mend@eng.tau.ac.il}{David Mendelovich} are with the School of Electrical Engineering, Tel Aviv University, Tel Aviv 69978, Israel.}%
\thanks{\href{mailto:sochen@math.tau.ac.il}{Nir Sochen} is with the Department of Mathematics, Tel Aviv University, Tel Aviv 69978, Israel.}%
\thanks{\href{mailto:navotoz@mail.tau.ac.il}{Navot Oz} and \href{mailto:iftach@volcani.agri.gov.il}{Iftach Klapp} are with the Department of Sensing, Information and Mechanization engineering, Agricultural Research Organization, Volcani Institute, P.O. Box 15159, Rishon LeZion 7505101, Israel.}%
\thanks{This work was supported by the Israeli Ministry of Agriculture’s Kandel Program (grant number 20-12-0018).}}


\maketitle

\begin{abstract}
Low-cost thermal cameras are inaccurate (usually $\pm 3^\circ C$) and have space-variant nonuniformity across their detector. Both inaccuracy and nonuniformity are dependent on the ambient temperature of the camera. The main goal of this work was to improve the temperature accuracy of low-cost cameras and rectify the nonuniformity.

A nonuniformity simulator that accounts for the ambient temperature was developed. An end-to-end neural network that incorporates the ambient temperature at image acquisition was introduced. The neural network was trained with the simulated nonuniformity data to estimate the object's temperature and correct the nonuniformity, using only a single image and the ambient temperature measured by the camera itself. 
Results show that the proposed method lowered the mean temperature error by approximately $1^\circ C$ compared to previous works.
In addition, applying a physical constraint on the network lowered the error by an additional $4\%$.

The mean temperature error over an extensive validation dataset was $0.37^\circ C$. The method was verified on real data in the field and produced equivalent results.
\end{abstract}

\begin{IEEEkeywords}
Deep learning, Convolutional neural network (CNN), Calibration, Bolometer, Image processing, Space- and time-variant nonuniformity, Fixed-Pattern Noise (FPN)
\end{IEEEkeywords}

\section{Introduction}\label{sec:intro}
\IEEEPARstart{I}{nfrared} (IR) imagery in the $8_{\mu m}-14_{\mu m}$ atmospheric window measures the thermal radiation emitted from an object. IR imagery is extensively used for various applications, such as - military night vision \cite{bolometer_night_vision}, medical fever screening \cite{bolometer_medial_screening} and machinery fault diagnosis \cite{bolometer_fault_diagnosis}, among many others. One interesting utilization of such an imaging system is agriculture, because the temperature of a plant is important in deducing information on its well-being \cite{ir_agri_est_crop_water, ir_agri_dought}.

Low-cost IR cameras are usually uncooled, and rely on microbolometer arrays as sensors. The microbolometer array enables the construction of inexpensive IR cameras with low energy requirements. Unlike the photon-counting detector arrays (e.g., CMOS in the visible range), microbolometers measure changes in the electrical resistance caused by the incident thermal radiation originating from an object \cite{bolometer}. The thermal radiation heats each microbolometer to a temperature that depends on the scene, and each microbolometer in the array has a slightly different temperature depending on the observed scene and the incident angle of the radiation. The resistance of each microbolometer changes according to the scene temperature. The minuscule changes in the resistance of each microbolometer in the array are used to construct an image corresponding to the temperature of the observed scene.

However, microbolometer arrays are subject to space-variant nonuniformity and noise from various sources. 
The microbolometer array is uncooled, and so a prominent source of nonuniformity is thermal radiation emitted by the camera itself \cite{kruseIR}. 
Another parasitic thermal radiation source is the narcissus effect, where unfocused reflection of the detector returns from the optical surfaces \cite{infraredTheramlImaging}. The effect of the internal self-radiation (red lines) mixed with the incident thermal radiation from the scene (green lines) is schematically presented in Fig.~\ref{fig:camera_narcissus}. 

% Figure environment removed
% Figure environment removed

These parasitic effects are dependent on ambient temperatures, meaning that their influence on the measurements changes with ambient temperature of the camera. Fig.~\ref{fig:effect_narcissus} demonstrates the nonuniformity effect on an image of a uniform heat source (black body). Notice that the effect is also spatially variant.

Another source of nonuniformity is fixed-pattern noise (FPN). The readout circuitry of the microbolometer array is usually line-based (similar to charge coupled devices). Slight changes between readers on the same array can lead to considerable disparity between lines on the image \cite{Riou2004}.

Finally, the signal-to-noise ratio of the camera is often low due to readout and electronic noise~\cite{kruseIR}. These noises affect the minimum detectable change in scene temperature, known in the literature as noise equivalent differential temperature (NEDT). Lower NETD values are preferable, and the noise in the camera increases this value~\cite{Riou2004}.

The thermal radiation emitted by a body for all wavelengths can be found using the Stefan-Boltzmann law, whereby the emitted radiation can be approximated by the fourth power of the object's temperature~\cite{kruseIR}:
\begin{equation}\label{eq:stefanBoltzmann}
    L(T)\approx\epsilon\sigma T^4  \quad \left[   \frac{W}{m^2}   \right]
\end{equation}
where $T$ is the object's temperature, $\epsilon$ is a proportional constant and $\sigma$ is the Boltzmann constant.

In a small environment near a reference temperature $T_0$, the Stefan-Boltzmann law can be expanded by Taylor series:
\begin{equation}\label{eq:stefanBoltzmannTaylor}
\begin{split}
    L(T)&=\epsilon\cdot\sigma T^4=\epsilon\cdot\sigma(T_0+\Delta T)^4\\
    &\approx\epsilon\cdot\sigma(T_0)^4+4\epsilon\cdot\sigma(T_0)^3\Delta T
    \approx a_1\cdot\tobj + a_0
\end{split}
\end{equation}
where $a_0$, $a_1$ are the coefficients and $T_0$ is a reference temperature. $\Delta T$ was changed to $\tobj$ for brevity.

Equation~\ref{eq:stefanBoltzmannTaylor} demonstrates that the radiation can be approximated as linear in scene temperature for a small environment around a reference temperature. This result means that the incident thermal radiation on the sensor has a temperature-\textit{dependent} element and a temperature-\textit{independent} element.

The ambient temperature of the camera has a profound effect on the measurements that it produces. Fig.~\ref{fig:drift} demonstrates the drift in measurements caused by a change in ambient temperature. Thus, the model in Equation~\ref{eq:stefanBoltzmannTaylor} must also account for changes in ambient temperature. 
The linear approximation of the overall reading of the camera depends on both the ambient temperature and the object temperature:
\begin{equation}\label{eq:finalWithFPA}
    L(\tobj, \tfpa) = G(\tfpa)\cdot L(\tobj)+D(\tfpa)
\end{equation} where $\tfpa,\tobj$ are the ambient and object temperatures, respectively.

$G(\tfpa)$ and $D(\tfpa)$ in Eq.~\ref{eq:finalWithFPA} are polynomials of $\tfpa$. The polynomial model has been previously shown to be representative of the underlying physical thermal radiation model (e.g,\cite{Nugent2013,Tempelhahn2016}). For the remainder of this work, higher-order polynomials, mainly quadratic, will be used for approximations.

Separating the coefficients from the object temperature in Eq.~\ref{eq:finalWithFPA} is complicated when only the camera response is given \cite{Papini2018}. 
However, some mathematical functions can separate a product into a summation, such as the $\log()$ function \cite{math_handbook}. The existence of a separation function suggests the use of neural networks, which can approximate any function \cite{nn_alg_book}. Thus, this work attempts to develop an end-to-end neural network that represents the polynomial in Eq.~\ref{eq:finalWithFPA}. For completeness, a network with a linear physical constraint will also be developed and compared.

% Figure environment removed


\section{Prior work}\label{sec:prior}
\noindent Nonuniformity correction is an ongoing area of research. Different approaches are described in sections \ref{sec:prior:calib}-\ref{sec:prior:singleimage}.

\subsection{Calibration-based methods}\label{sec:prior:calib}
\noindent The process of calibration requires collecting data of a known heat source under different environmental conditions. This process is usually conducted with a scientifically calibrated blackbody in an environmental chamber. The data are used to find coefficients that solve an equation for the calibration. 

The baseline for the calibration methods is a one-point correction. These methods usually assume a known and constant gain across ambient temperatures, and only solve for the offset (e.g.,~\cite{Schulz1995}). The natural extension is the two-point correction where no assumption is made for the gain (e.g.,~\cite{Riou2004}). These early methods solved for the coefficients using a simple linear regression model.

Contemporary methods formulate the nonuniformity correction (NUC) as an inverse problem. 
Nugent et al.~\cite{Nugent2013} solved it as a least-squares problem, with the offset and gain modeled as polynomials of the object's temperature. 
In ref.~\cite{Nugent2014}, they used the internal shutter of the camera to periodically update the results of the calibration. 
Liang et al.~\cite{Liang2017} based the solution on interpolation of a predefined offset table for each ambient temperature, and the offset values for this table were found using a two-point correction.
Chang and Li~\cite{Chang2019Intergration} solved for both the ambient temperature and integration time of the camera.

Calibration-based methods produce good results but relay on the collection of extensive data. The data must be accurate and contain both varying object temperatures and ambient temperatures, calling for the use of scientific-grade equipment. Moreover, these methods are valid only for the camera used to collect the data, meaning that the data-collection process must be performed for every camera to be calibrated. Any attempt to apply the calibration data on another camera will be noisy and have noticeable FPN because the coefficients of the calibration will not be suitable between cameras.

\subsection{Scene-based methods}
\noindent Scene-based methods exploit redundant data in and between frames, rendering calibration unnecessary. 
The redundant data can be movement between frames, camera jitter between images, or a constraint on the dataset itself.

Most of these methods assume that the change in ambient temperature is slow, thus the gain and offset changes slowly, and both can be regarded as constant between frames. 
This assumption holds true, but only for a limited time and ambient temperature span.

Averbuch et al.~\cite{Averbuch2007} used the motion between frames. Consecutive frames were registered to add data on each pixel, and an inverse problem was solved to find the offset. The solution was updated using a Kalman filter.
Papini et al.~\cite{Papini2018} used pairs of blurred and sharp images to approximate the gain and offset.

These approaches offer good approximations for the temperatures but are expensive to calculate and require redundancy between frames.

\subsection{Single image-based methods}\label{sec:prior:singleimage}
\noindent The idea of this approach is to use only information that is already embedded in the frame itself.
Scribner et al.~\cite{nnOld} used a neural network to find offset and gain. The neural network acted as a locally adaptive filter on a small neighborhood.
Tendero and Gilles~\cite{admire} equalized the frame using the cumulative histogram of all of the columns in the frame, and then used the discrete cosine transform to denoise the results.

Recently, methods that utilize neural networks in general, and convolutional neural networks (CNN) in particular, have been suggested.
He et al.~\cite{He2018} suggested using a U-Net-type CNN trained end-to-end.
Jian et al.~\cite{Jian2018} filtered the frame with a bilateral filter to allow the network to concentrate only on the high-frequency information.
Chang et al.~\cite{Chang2019} shared multiscale information between layers of the network to improve the NUC results.
Saragadam et al.~\cite{Saragadam2021} used a neural network as prior information for solving an optimization problem. The input to the network was jittered frames of the same object. The physical constraint shown in Eq.~\ref{eq:stefanBoltzmannTaylor} was imposed as part of the optimization problem.

Jointly estimating accurate scene temperature and correcting nonuniformity using only a single image at different ambient temperatures, without the need to calibrate for each camera, has yet to be achieved.

This work aims to both estimate the scene temperature and correct the nonuniformity in frames captured by uncooled microbolometer-based cameras. We introduce a method of accurately estimating a space-variant nonuniformity model from measurements, which also accounts for the ambient temperature of the camera. The model utilizes prior knowledge on the physics of the domain, namely radial spatial dependence, which is incorporated into the mathematical modeling of the nonuniformity. The nonuniformity model is general and represents different cameras, unlike previous calibration methods. Thus model is used to train a CNN to correct the nonuniformity and produce accurate scene temperatures based only on a single frame and the ambient temperature of the camera. We also compare a neural network with a physical constraint based on Eq.~\ref{eq:finalWithFPA} to an end-to-end temperature-estimation network and show that the physical constraint improves performance by a small margin.
Finally, we demonstrate our method on real data collected with a low-cost uncooled microbolometer camera and compare it to measurements taken with a scientific-grade radiometric camera to show that the method indeed works and can provide generalizations.
To summarize the findings of this work:
\begin{enumerate}
    \item A method for jointly estimating scene temperature and perform nonuniformity correction with only a single gray-level frame and ambient temperature.
    \item Development of a nonuniformity simulator that uses physical prior knowledge of radial pixel dependence. The simulator is general and can faithfully represent multiple cameras and situations for a wide range of ambient temperatures.
    \item Elimination of the need to calibrate each camera separately.
    \item Investigation of the effect of the physical constraint introduced in Eq.~\ref{eq:stefanBoltzmannTaylor} on the scene temperature estimation.
\end{enumerate}

\section{Proposed method}\label{sec:method}
\noindent The described method is aimed at providing a single-image scene-based method for correcting the space-variant degradation and temperature drift in microbolometer arrays.

The proposed method is composed of four steps:
\begin{enumerate}[label=\Alph*.]
    \item Characterize the nonuniformity in an uncooled microbolometer thermal camera described by four steps (section~\ref{sec:method:characterize}).
    \begin{enumerate}[label=\arabic*)]
        \item Model the camera response to a set of object temperatures (\ref{sec:method:characterization:temperatureDependency}).
        \item Use the spatial dependency between pixels as a constraint (\ref{sec:method:characterization:spatialDependency}).
        \item Exploit symmetry around the middle of the frame (\ref{sec:method:characterization:axisSymmetry}).   
        \item Apply the method to new frames (\ref{sec:method:characterization:applyToNew}).
    \end{enumerate}
    \item Acquire a large dataset of accurate temperature maps.
    \item Create samples using the accurate temperature maps and the synthetic nonuniformity  (Alg.~\ref{alg:applyNonUniformity}).
    \item Train a CNN to perform NUC in a supervised manner (section~\ref{sec:method:net}).
\end{enumerate}

\subsection{Characterization of the nonuniformity}\label{sec:method:characterize}
\noindent In this work, we used the low-cost uncooled-microbolometer thermal camera FLIR \taucamera, because it allows access to raw measurements of thermal radiation.
To estimate the nonuniformity for various ambient temperatures, the camera was placed in an environmental chamber (Fig.~\ref{fig:envChamber}), and focused on a \blackbody\ blackbody, which served as the object of the setup. The \taucamera\ was set to $60$ frames per second (FPS). The camera output was set to radiation flux, so the raw measurement of each microbolometer is represented as a $14_{bit}$ integer. To acquire the rawest possible radiation flux and without any image processing, all the automatic image enhancements were disabled before each measurement (details are in a table in the supplementary materiel).
The equipment and \taucamera\ parameters are elaborated in section~\ref{sec:materials:equipment}.

% Figure environment removed

An extensive dataset of camera responses was collected, comprised of the camera response at a known object temperature for different ambient temperatures.
The camera response was measured for a series of \opPoint s denoted as $R(\tfpa, \tobj)_i$, where $\tfpa$ is the ambient temperature, $\tobj$ is the object temperature and $i\in[1,\hdots,N]$. The measurements were made on a predefined set of temperatures such that $\tfpa\in\Tfpa$ and $\tobj\in\Tobj$.
$N$ images were averaged for each \opPoint\ to lower the noise per pixel~$\left(\propto N^{-0.5}\right)$. 
The averaged images at an \opPoint\ are denoted as $\Bar{R}(\tfpa, \tobj)$.


\subsubsection{Object temperature dependence}\label{sec:method:characterization:temperatureDependency}
\noindent The camera response for a given object temperature and ambient temperature can be estimated as a polynomial for each pixel:
\begin{equation}\label{eq:estResponse}
    \Tilde{R}(\tfpa, \tobj)[x,y] = \sum_{m=0}^{\mPoly}{\Tilde{b}_{C}(\tfpa)[x,y]} \cdot \tobj^m[x,y]
\end{equation} where $\mPoly$ is the degree of the polynomial fit, and $b$ is the pixel-wise coefficient of the polynomial that depends on the ambient temperature.

To estimate the coefficient vector $\Tilde{b}_{C}$, first the dependence of the response on ambient temperature was determined and fitted.
The dependence was determined from the \opPoint measurements by estimating the polynomial coefficients for each $\tfpa$. A matrix of object temperatures at each \opPoint\ and a vector of camera responses is built for each $\tfpa$, and the polynomial coefficients are found \emph{per pixel} using Least Squares. We denote:
\begin{align*}
    \mat{A}_O[\tfpa] &= 
    \begin{bmatrix}
        \Tobj^0[1] & \dots & \Tobj^{\mPoly}[1] \\
        \vdots & \ddots  & \vdots \\
        \Tobj^0[\NumTempObj] &  \dots & \Tobj^{\mPoly}[\NumTempObj]
    \end{bmatrix}_{\NumTempObj\times \mPoly} \\
    \vec{b}_{C}[\tfpa] &= 
    \begin{bmatrix}
        c_0[\tfpa] \\ \vdots \\ c_{\mPoly}[\tfpa]
    \end{bmatrix}_{\mPoly\times1} \\ 
    \vec{R}[\tfpa] &=  
    \begin{bmatrix}
        R\left(\tfpa,\Tobj[0]\right)\\\vdots\\R\left(\tfpa,\Tobj[\NumTempObj]\right)
    \end{bmatrix}_{\NumTempObj\times1}
\end{align*} where $\mPoly$ is the degree of the polynomial to fit, and $\NumTempObj$ is the length of $\Tobj$. 
Then the values of $\vec{b}_{C}[\tfpa][x,y]$ are estimated by solving the inverse problem:
\begin{subequations} 
    \begin{align}
        \vec{R}[\tfpa][x,y] &= \mat{A}_O[\tfpa] \cdot \vec{b}_{C}[\tfpa][x,y] \rightarrow\\
        \vec{b}_{C}[\tfpa][x,y] &= \mat{A}_O^{+}[\tfpa]\cdot\vec{R}[\tfpa][x,y] \label{eq:LsSolutionFPA}
    \end{align}
\end{subequations}
where $\mat{A}_O^{+}$ is the Moore–Penrose inverse of $\mat{A}_O$. 

A set of coefficients $\vec{b}_{C}\nolinebreak\in\nolinebreak\mathcal{R}^{\mPoly}$ exists for each $\tfpa\nolinebreak\in\nolinebreak\Tfpa$. These coefficients are \textit{pixel-wise}, meaning there are $\mPoly$ coefficient maps with spatial dimensions of $h\nolinebreak\times\nolinebreak w$ for $h,w$ the dimensions of each image. These coefficient maps were filtered using a Gaussian filter with $\sigma=1$ to remove high-frequency noise stemming from dead pixels in the camera.

Fig.~\ref{fig:fitGl2BB} presents an example of the fitting results between the gray levels at an \opPoint\ an the real blackbody temperatures, as described in Eq.~\ref{eq:LsSolutionFPA}. The number of coefficients was chosen empirically as $\mPoly=3$. The fit provides a good estimation to the data ($R^2\geq0.99$).

% Figure environment removed%

Fig.~\ref{fig:coefObjTemp} is a scheme of the coefficients for a given object temperature as described in Eq.~\ref{eq:LsSolutionFPA}. Each coefficient map is two-dimensional.

% Figure environment removed

The measurements are expected to be symmetrical around the middle of the image~\cite{Tempelhahn2016}, but practical effects can create skew. The skewing limits the usability of the model because the skewed model does not accurately depict a general symmetrical case.
The effect of the skewing on real data can be seen in Fig.~\ref{fig:skew}a.

% Figure environment removed%

\subsubsection{Spatial dependence}\label{sec:method:characterization:spatialDependency}
\noindent So far, the polynomial dependence of the camera's readings on $\tobj,\tfpa$\ were found for each pixel. To overcome the skewing, the coefficients from Eq.~\ref{eq:LsSolutionFPA} are fitted to a spatial function. The spatial fitting is performed separately on each set of coefficients~$\vec{b}_{C}[m],\forall m\in[0,...,\mPoly]$.
The spatial fitting is performed twice: for a quadratic polynomial and for a high-degree polynomial. The coefficients that have the most profound effect on the skewing are the linear and quadratic coefficients. The ideal form of nonuniformity is expected to be axis-symmetric, and a quadratic function can be viewed as a low-frequency distortion of this symmetry. Thus, subtraction of the polynomials up to the quadratic coefficient removes the low-frequency distortion and alleviates the skew. An example of the skewing effect on real data and fitting results can be seen in Fig.~\ref{fig:skew}.

Under these assumptions, we intend to find a skew-less axis-symmetric polynomial approximation of the measurements. This will be achieved by first fitting the results to a spatial function, and then fitting again to a radial function.

The first step, fitting the coefficients of the camera response to a spatial function, exploits the correlation between neighboring pixels. The spatial fitting reduces the number of coefficients considerably, from $\propto h\times s$ - the number of pixels - to $\propto\mSpatial$ - the number of coefficients in the spatial fit where $\mSpatial<<h\times w$.

To fit to a spatial function, we first define two matrices of dimensions $h\times w$. The matrices are built from vectors in the range $[-0.5,0.5]$, in $\mat{H}$ as columns and in $\mat{W}$ as rows:
$$\mat{W} = \begin{bmatrix} 
                -0.5&\ldots&0.5 \\
                \vdots & \ddots & \vdots \\
                -0.5&\ldots&0.5 
            \end{bmatrix}_{h,w},
\quad \mat{H}=\mat{W}^T $$

The spatial fit is defined as:
\begin{subequations} \label{eq:spatialFit}
    \begin{align}
        K[\tobj]&=\sum_{q=0}^{\mSpatial}\sum_{z=0}^{\mSpatial}  \vec{\beta}_{C}[\tfpa][q,z]\cdot\mat{H}^{q}\cdot \mat{W}^{z} \\
        \vec{\beta}_{C}[\tfpa]&=\argmin_{\vec{\beta}_{C}[\tfpa]}\left(\vec{b}_{C}[\tfpa] - K[\tobj]\right)
    \end{align}
\end{subequations} where $\mSpatial$ is the number of coefficients in the spatial fit. The powers $q,z$ are applied respectively to matrices $\mat{H},\mat{W}$ element-wise.

We define $\vec{\beta}^C_{C}$ as the quadratic fit with $\mSpatial\nolinebreak=\nolinebreak2$, $\vec{\beta}^F_{C}$ the fine fit with $\mSpatial\nolinebreak>>\nolinebreak2$, and $\vec{\beta}^S_{C}$ the final skew-less fit:
\begin{equation}\label{eq:spatialFitSubtraction}
    \vec{\beta}^S_{C}[\tfpa] =
    \begin{cases}
        \text{Mean}(\vec{\beta}^C_{C}[q,z],\vec{\beta}^F_{C}[q,z]),&q=z=0\\
        \vec{\beta}^F_{C}[q,z]-\vec{\beta}^C_{C}[q,z],&\forall q,z\neq0
    \end{cases}
\end{equation}
Notice that $\vec{\beta}^S_{C}[\tfpa]\in\mathcal{R}^{\mSpatial\times\mSpatial}$. The bias coefficient is averaged between the fits. Empirically, this is found to produce better results. 

Fig.~\ref{fig:spatialFitSideView} shows a horizontal cross-sectional view of Fig.~\ref{fig:skew}, along with the results of the spatial fitting in Eq.~\ref{eq:spatialFitSubtraction}. The cross-section of the measurements, fine fit, quadratic fit and subtraction fitting are presented. The subtraction fitting is calculated by subtracting the quadratic polynomial from the fine polynomial. The number of coefficients for the fine fit were set to $\mSpatial=15$. The final fit does indeed alleviate the skewing, while remaining faithful to the measurements.

% Figure environment removed%


\subsubsection{Axis-symmetric fitting}\label{sec:method:characterization:axisSymmetry}
\noindent To exploit the radial symmetry around the middle of the image, the spatial fit results of Eq.~\ref{eq:spatialFitSubtraction} are fitted to a radial kernel:
\begin{subequations}\label{eq:fpaFit}
    \begin{align}
        \mat{P}&= \sqrt{\mat{H}^2 + \mat{W}^2},\quad\quad \mat{P}\in\mathcal{R}^{h\times w} \\
        J[\tfpa] &= \sum_{r=0}^{\mRadial} \vec{\mathcal{B}}_{C}[\tfpa][r]\cdot\mat{P}^{r} \\
        \vec{\mathcal{B}}_{C}[\tfpa]&=\argmin_{\vec{\mathcal{B}}_{C}[\tfpa]}\left(\vec{\beta}^S_{C}[\tfpa] - J[\tfpa]\right) \label{eq:spatialRadiiFit}
    \end{align}
\end{subequations} 
where $\mRadial$ is the number of coefficients in the radial fit and $\vec{\mathcal{B}}_{C}[r]\nolinebreak\in\nolinebreak\mathcal{R}^{\mRadial}$ are the radial fitting coefficients.

Notice that for each ambient temperature $\tfpa\in\TambAsVec$ in the discrete set of measurements, there is a unique vector of radial coefficients $\vec{\mathcal{B}}_{C}[\tfpa]$. The last step in the estimation process is to express a polynomial approximation of the radial coefficients by $\tfpa$; specifically, to find $\vec{\mathcal{B}}(t)$ in the continuous range $t\in\TambAsVec$.

The result of Eq.~\ref{eq:fpaFit} should output the following approximation: $$\Tilde{\mathcal{B}}_{C}(\tfpa) \approx \sum_{m=0} ^{\mFPA}{\Gamma[m] \cdot \tfpa^m}$$
where $\Gamma$ are the coefficients of the ambient temperature polynomial, $\mFPA$ is the degree of the polynomial and $\NumFPA$ is the length of $\Tfpa$. 
Notice that the $\Gamma$ coefficients are not dependent on the spatial dimension. The approximated coefficients $\Tilde{\mathcal{B}}_{C}(\tfpa)$ will be used with $\mat{P}$ the radial kernel, to approximate the spatial nonuniformity.

To solve Eq.~\ref{eq:fpaFit}, we make several denotations. $\mat{A}_C$ is a matrix containing the powers of all $\tfpa\in\TambAsVec$:
\begin{equation}
        \mat{A}_C = 
        \begin{bmatrix}
            \Tfpa^0[1] & \cdot & \Tfpa^{\mFPA}[1] \\
            \vdots & \cdot   & \vdots \\
            \Tfpa^0[\NumFPA] &  \cdot & \Tfpa^{\mFPA}[\NumFPA]
        \end{bmatrix}_{\NumFPA\times \mFPA}
\end{equation}

$\vec{\mathcal{B}}_C\{m\}$ is a vector of the $m$th coefficient in $\vec{\mathcal{B}}_{C}$ for all $\tfpa\in\TambAsVec$:
\begin{equation}
        \vec{\mathcal{B}}_C[r] = 
        \begin{bmatrix}
            \vec{\mathcal{B}}_{C}\left[\Tfpa[0]\right][r]\\
            \vdots \\
            \vec{\mathcal{B}}_{C}\left[\Tfpa[\NumFPA]\right][r]
        \end{bmatrix},\forall r\in[0,...,\mRadial]
\end{equation}

$\mat{R}_{C}$ is a matrix of all the $\vec{\mathcal{B}}_{C}\{m\}\quad\forall m\in[0,...,\mRadial]$:
\begin{equation}
        \mat{R}_{C} = 
        \begin{bmatrix}
            \vec{\mathcal{B}}_C[0] & \hdots &\vec{\mathcal{B}}_C[\mRadial]
        \end{bmatrix}_{\NumFPA\times \mRadial}    
\end{equation}

$\Gamma$, the radial coefficient matrix, is comprised of $\gamma_{i,j}$ coefficients:
\begin{equation}
        \Gamma = 
        \begin{bmatrix}
            \gamma_{0,0} & \hdots & \gamma_{0,\mRadial} \\
            \vdots & \ddots & \vdots \\ 
            \gamma_{\mFPA,0} & \hdots & \gamma_{\mFPA,\mRadial} 
        \end{bmatrix}_{\mFPA\times \mRadial} \label{eq:fitRadiiFPA:coef}    
\end{equation}

Then the values of $\Gamma$ can be found by solving the inverse problem:
\begin{equation}\label{eq:fitRadiiFPA}
    \mat{R}_C = \mat{A}_C \cdot \Gamma \quad \rightarrow\quad
    \Gamma = \mat{A}_C^{+}\cdot\mat{R}_C 
\end{equation}
$\mat{A}_C^{+}$ is the Moore–Penrose inverse of $\mat{A}_C$. 

Fig.~\ref{fig:coefRadial} is a scheme of the radial coefficients being used to approximate the coefficients $\Tilde{b}_{C}(\tfpa)$. The entire estimation algorithm is depicted in Alg.~\ref{alg:estimateNonUniformity}.

% Figure environment removed

To summarize the entire characterization process, we now describe how the nonuniformity model can be applied to a temperature map.
First, the radial coefficients are estimated for a given ambient temperature $\tfpa$:
\begin{equation}\label{eq:estimationOfMeas:sp}
    \Tilde{\mathcal{B}}_{C}(\tfpa) = \sum_{m=0} ^{\mFPA}{\Gamma[m] \cdot \tfpa^m}
\end{equation}

Next, the pixel-wise coefficients are found from the radial coefficients $\Tilde{\mathcal{B}}_{C}(\tfpa)$ and the radial kernel $\mat{P}$:
\begin{equation} \label{eq:estimationOfMeas:fpa}
    \Tilde{b}_{C}(\tfpa)[x,y] = \sum_{r=0}^{\mRadial}{\Tilde{\mathcal{B}}_{C}(\tfpa)[r]\cdot P^r}
\end{equation} Notice that the coefficients $\Tilde{b}_{C}(\tfpa)$ are pixel-wise and that for $P^r$, the power $r$ is applied element-wise on the matrix $P$.

Lastly, the estimated response of the camera $\Tilde{R}(\tfpa, \tobj)$ is approximated using the given object temperature and the pixel-wise coefficients $\Tilde{b}_{C}(\tfpa)$. The equation is stated at the beginning of section~\ref{sec:method:characterization:temperatureDependency}, as Eq.~\ref{eq:estResponse}:
$$ \Tilde{R}(\tfpa, \tobj)[x,y] = \sum_{m=0}^{\mPoly}{\Tilde{b}_{C}(\tfpa)[x,y]} \cdot \tobj^m[x,y] $$
where $\tobj$ can have dimensions similar to a frame. The power $m$ is applied element-wise.


\subsubsection{Applying the camera simulator to new data}\label{sec:method:characterization:applyToNew}
\noindent The process of applying the nonuniformity to a new measurement is described in Alg.~\ref{alg:applyNonUniformity}.

The characterization process produces synthetic nonuniformity maps. 
The process also handles error in the measured data, such as skewing around the middle of the image.

The degradation maps enable training different supervised algorithms for the nonuniformity correction. The maps are noise-less and perfectly symmetrical around the middle of the image. Different augmentations can be applied by the user to simulate noises of varying degrees, directional heating on the camera which results in skewing, and FPN on the frames. These augmentations increase generalization, making it possible to perform NUC on different cameras with the same training. Details on the exact augmentations performed in this work are given in section~\ref{sec:method:preprocessing}.

\begin{algorithm}
    \SetAlgoLined
    \KwData{Images of \blackbody\ blackbody at different \opPoint s.}
    \KwIn{$\mathbf{\mPoly}$ is the degree of the polynomial of the object's temperature.\newline
          $\mathbf{\mFPA}$ is the degree of the coarse spatial fit.\newline
          $\mathbf{M_F}$ is the degree of the fine spatial fit.\newline
          $\mathbf{\mRadial}$ is the degree of the radial fit.\newline
          $\mathbf{\mFPA}$ is the degree of the camera temperature fit for the radial coefficients.}
    \KwOut{The $\tfpa$-dependent spatial nonuniformity coefficients $\Gamma\in\mathcal{R}^{M_{C}\times M_{r}}$.}
    \For{$\tfpa\in \Tfpa  $}{
        $\vec{b}_{C}[\tfpa][x,y]\longleftarrow$ Eq.~\ref{eq:LsSolutionFPA}, $\forall x,y\in$ image}
    \For{$m\in[0,...,\mPoly]$}{
        \For{$\tfpa\in \Tfpa  $}{
            Gaussian filter on $\vec{b}_{C}[\tfpa][m]$\\
            $\vec{\beta}^C_{C}[\tfpa][m]\longleftarrow$ Quadratic spatial fit (Eq.~\ref{eq:spatialFit})\\
            $\vec{\beta}^F_{C}[\tfpa][m]\longleftarrow$ Fine spatial fit (Eq.~\ref{eq:spatialFit})\\
            $\vec{\beta}^S_{C}[\tfpa][m]\longleftarrow$ Subtract the spatial fit (Eq.~\ref{eq:spatialFitSubtraction})\\
            $\vec{\mathcal{B}}_{C}[\tfpa][m]\longleftarrow$ Radial spatial fit (Eq.~\ref{eq:spatialRadiiFit})}
        $\Gamma[m]\longleftarrow$ Fit radial coefficients to $\tfpa$ (Eq.~\ref{eq:fitRadiiFPA})}
    \caption{Estimation of nonuniformity maps.}
    \label{alg:estimateNonUniformity}
\end{algorithm}

\begin{algorithm}
    \SetAlgoLined
    \KwIn{$T$ is an accurate temperature map of arbitrary dimensions and $\tfpa$ to simulate the camera temperature.}
    \KwOut{$\Hat{R}(\tfpa, T)$ a gray-level map simulation at temperature $\tfpa$.}
    $\Tilde{R}(\tfpa, \tobj)\longleftarrow\mat{0}_{h\times w}$\\
    \For{$m\in[0,...,\mPoly]$}{
        $\Tilde{\mathcal{B}}_{C}(\tfpa)[m]\longleftarrow$  Calc. radial coefficients (Eq.~\ref{eq:estimationOfMeas:sp})\\
        $\Tilde{b}_{C}(\tfpa)[m]\longleftarrow$ Calc. per-pixel coefficients using the radial coefficients (Eq.~\ref{eq:estimationOfMeas:fpa})\\
        \tcp{Multiplication and power of matrices are applied element-wise.}
        $\Tilde{R}(\tfpa, \tobj)\longleftarrow\Tilde{R}(\tfpa, \tobj)+\Tilde{b}_{C}(\tfpa)[m]\cdot \tobj^m$}
    \caption{Transform an accurate temperature map of arbitrary dimensions into a nonuniform radiation flux.}
    \label{alg:applyNonUniformity}
\end{algorithm}


\subsection{Network}\label{sec:method:net}
% Figure environment removed
\noindent The network is based on the U-Net~\cite{unet} architecture and is presented in Fig.~\ref{fig:net}. 

The nonuniformity is space-variant (section~\ref{sec:intro}), so the network needs a large receptive field to correct the entire image. The U-Net architecture is a tradeoff between the receptive field of the network and the computational requirements. $\nLevels$ is the coefficient of the spatial resolution and $\nChannels$ is the coefficient for the number of channels. At each level, the spatial resolution decreases by $2^\nLevels$\ and the number of channels increases by $2^\nChannels$. The deeper levels of the network increase the receptive field because each convolution kernel affects a larger area in the original frame. The computation requirements decrease by a factor of 2 for each encoder level~\cite{Oz2020}.

The network shown in Fig.~\ref{fig:net} operates as an end-to-end function:
\begin{equation}\label{eq:netEnd2End}
    \tobjapprox = F(I(\tobj), \tfpa)
\end{equation} where $I(\tfpa)$ is a gray-level map taken at known ambient temperature $\tfpa$, and $F$ is the output of the network blocks. We name this configuration \textbf{E2E} (end to end).

Equation~\ref{eq:stefanBoltzmannTaylor} shows that the radiance is a linear function of the scene temperature, and Eq.~\ref{eq:finalWithFPA} shows a linear dependence on the ambient temperature. To plug this prior knowledge into the network, the final block in the U-NET was replaced with two blocks of the same configuration. Both blocks have the same input, which is the output of the layer before the split. 
These blocks extract the estimated object temperature $\tobjapprox$ from the linear approximation of the radiance shown in Eq.~\ref{eq:finalWithFPA}:
\begin{equation}
\begin{split}\label{eq:netPysical}
    I(\tfpa) &= G(\tfpa)\cdot\tobj+D(\tfpa) \longrightarrow \\
    \tobjapprox &= \mathcal{G} \cdot I(\tfpa) + \mathcal{D}
\end{split}
\end{equation}
where $I(\tfpa)$ is the input to the network, and $\mathcal{G}\approx\frac{1}{G}, \mathcal{D}\approx\frac{D}{G}$ and the outputs of the respective blocks. We name this configuration \textbf{GxPD} (Gx + D).

The effects of both networks are elaborated in section~\ref{sec:experiments}.


\subsection{Loss functions}
\noindent The loss function is comprised of a fidelity term, a structural term, and a noise-reduction term. The fidelity term is the mean absolute error (MAE) which is robust to outliers \cite{anwar2020}, applied on the difference between the accurate temperature map $T$ and the output of the network $\Hat{T}$:
\begin{equation}\label{eq:lossFid}
    \mathcal{L}_{Fid} = \frac{1}{h\cdot w}\sum_{i,j}\left|\Bar{T}_{i,j} - \Hat{T}_{i,j}\right|
\end{equation} 
where $h,w$ are height and width respectively.

The structural term measures the dissimilarity index, based on the structural similarity metric (SSIM). The SSIM is aimed at providing a good metric for the human visual-perception system. Use of the DSSIM method has been shown to improve network performance in image-restoration tasks \cite{ssimLoss2017}. It is calculated as:
\begin{equation}\label{eq:lossSSIM}
    \mathcal{L}_{DSSIM} = \frac{1-\text{SSIM}(\Bar{T},\Hat{T})}{2}
\end{equation}

The noise-reduction term is total variation loss \cite{totalvariation}. The underlying assumption is that the sum of absolute gradients for noisy images is higher than for clean images:
\begin{equation}
    \mathcal{L}_{TV}(\Hat{T}) = \frac{1}{h\cdot w}\sum_{i,j}\left|\Hat{T}_{i,j+1}-\Hat{T}_{i,j}\right|+\left|\Hat{T}_{i+1,j}-\Hat{T}_{i,j}\right|
\end{equation} where $i,j$ denotes the pixel position.

The overall loss term for the network training is:
\begin{equation}\label{eq:loss}
    \mathcal{L} = \mathcal{L}_{Fid} + \beta\cdot\mathcal{L}_{DSSIM} + \gamma\cdot\mathcal{L}_{TV}
\end{equation}
where $\beta, \gamma$ are the hyperparameters that balance the loss terms.


\subsection{Preprocessing}\label{sec:method:preprocessing}
\noindent The input to the network is a gray-level map created from an accurate temperature map using the synthetic nonuniformity as described in Alg.~\ref{alg:applyNonUniformity}. The input to the network can be described as:
\begin{equation}\label{eq:netInput}
    I(\tfpa) = \Hat{R}(\tfpa, \tobj) + \mathcal{N}(0, \sigma^2)
\end{equation}
where $\Hat{R}(\tfpa, \tobj)$ is the synthetic gray-level map (Alg.~\ref{alg:applyNonUniformity}), and $\mathcal{N}$ is the additive Gaussian noise.

The input of the network is a frame representing the radiation flux measured by the microbolometer. These are represented by 14-bit gray-levels. To normalize them to the range of [0,1], the maximal and minimal values of gray-levels in the entire training and validation sets were obtained, and all inputs were normalized by:
\begin{equation}
    \Bar{I}(\tfpa) = \frac{I(\tfpa) - I_\text{min}}{I_\text{max}-I_\text{min}}
\end{equation}
where $\Bar{I}(\tfpa)$ is the normalized input and $I_\text{min},I_\text{max}$ are the minimal and maximal gray-levels over the datasets.

The accurate temperature maps must also be normalized to the range [0,1]. Again, the maximal and minimal temperatures were found over all datasets and both the output of the network and the original accurate temperature maps were normalized:
\begin{equation}
    \Bar{T} = \frac{T - T_\text{min}}{T_\text{max}-T_\text{min}}
\end{equation}
where $\Bar{T}$ is the normalized accurate temperature map and $T_\text{min},T_\text{max}$ are the minimal and maximal temperatures over all datasets.

Augmentations were applied during training and validation to enrich the dataset further. These included cropping to $256\times256$ pixels, random horizontal and vertical flips, and $90^\circ$ rotations. 

Random Gaussian noise with $\sigma^2=5_{GL}$ and FPN were generated for each frame. FPN was generated as:
\begin{equation}
    M_{FPN} = \begin{bmatrix}
        1 \\ \vdots  \\ 1
    \end{bmatrix}_{h\times1}\cdot
    \left(\begin{bmatrix}
        \mathcal{U}[v_{\min}, v_{\max}] \\ \vdots  \\ \mathcal{U}[v_{\min}, v_{\max}]
    \end{bmatrix}^T\right)_{1\times w}
\end{equation} where $\mathcal{U}$ is uniform distribution. $v_{\min}, v_{\max}$ were chosen as $v_{\min}=0.9, v_{\max}=1$. 

During training, all augmentations were generated and applied randomly, i.e, random cropping and flipping, and randomly generated noise and FPN.
During validation, the cropping was a $256\times 256$ pixels rectangle around the center of the frame, to make the validation process deterministic.
Moreover, the Gaussian noise and FPN were generated once for each frame and used throughout the entire validation process. This was done to allow a fair comparison between experiments.

To construct the input of the network, first cropping and flipping augmentations were applied to a temperature map $T$. Second, a random $\tfpa$ was generated and used with the augmented temperature map in Eq.~\ref{eq:estimationOfMeas:fpa} to obtain a simulated camera response $\Tilde{R}(\tfpa, T)$. Then, normalization was applied to this simulated response to get $\Bar{I}(\tfpa)$. The last step was to apply the Gaussian noise $\left(\mathcal{N}(1, \sigma^2)\right)$ and FPN $\left(M_{FPN}\right)$ to the normalized simulated camera response:
\begin{equation}
    I^{in}_{\tfpa} = \mathcal{N}(1, \sigma^2) \otimes M_{FPN} \otimes \Bar{I}(\tfpa)
\end{equation} where $I^{in}_{\tfpa}$ is the normalized gray-level input to the network and $\otimes$ is the element-wise multiplication. 

\subsection{Training details}\label{sec:experiments:training}
\noindent The network was trained using the ADAM optimizer \cite{adamOpt2015} with a learning rate of $10^{-4}$. The learning rate was halved on a validation loss plateau of more than 3 epochs. The network was run for a 100 epochs, but early stopping was applied for a validation loss plateau of 8 epochs. 
The weights were initialized using the orthogonal scheme \cite{orthoInit2014} with a scaling of $50^{-2}$.
The training was run on a single Nvidia 2080Ti. The network was written in Python3.8 using Pytorch 1.10.  The hyperparameters of the network are given in a table at the supplementary material.
The optimal hyper-parameters by optimizing on the average MAE (Eq.~\ref{eq:lossFid}) of the validation sets.

The convergence results for the validation MAE of the E2E and GxPD network is found in the supplementary materiel.

\section{Experimental results}\label{sec:experiments}
\noindent The methods described in section~\ref{sec:method} were used to estimate temperature maps and correct nonuniformity in microbolometer-based thermal cameras. The presented experiments are organized as follows: 
\begin{enumerate}
    \item The data and equipment used to develop the proposed method.
    \item The results for characterization of the nonuniformity as presented in section~\ref{sec:method:characterize}.
    \item The results of the NUC performed by the network, including the effect of the physical constraint.
    \item The results of the NUC performed by the network on real data.
\end{enumerate}

\subsection{Data}\label{sec:materials:data}
\noindent The data for the environmental chamber were measured at ambient temperatures $\Tfpa$= \{27, 31, 37.2, 38.9, 40.4, 41.5, 43.6, 44.7, 46.2, 46.8, 48, 50.8\}$^\circ C$. The \blackbody\ temperatures at each \opPoint\ were $\Tobj$= \{20, 25, 30, 35, 40, 45, 50, 55, 60\}$^\circ C$.

Noise variance was determined from the environmental chamber measurements:
\begin{equation}\label{eq:varNoise}
    \sigma^2[\tfpa,\tobj] = \frac{1}{h\cdot w}\sum_{i=0}^h\sum_{j=0}^w\text{Var}(R[\tfpa,\tobj][i,j])_N
\end{equation}
As seen in Eq.~\ref{eq:varNoise}, the noise variance  used as input to train the network in Eq.~\ref{eq:netInput} was the average over the spatial dimension of the variance map obtained from $N$ images. The effects of $\tfpa,\tobj$ on $\sigma^2$ were found to be negligible, so the average of all $\sigma^2$ was $\sigma^2=5$ gray levels.

As for the training of the network, the datasets were temperature maps collected using a FLIR \scientificCamera\ camera, which is a scientific-level radiometric camera. The \scientificCamera\ accuracy is only $2\%$ of the temperature range in each frame.

The training dataset was $12,897$ frames. The validation set was comprised of $4,723$ frames. All frames were of different agricultural fields in Israel, taken from an unmanned aerial vehicle (UAV) flying $70_m-100_m$ above the ground.
Only sharp frames were used, hand-picked by a human user.

The validation sets were captured at the same locations as the training sets, but on different days. This validation procedure was chosen to eliminate data leakage between the training and validation sets, so that the metrics represent the ability of the network to generalize to different data. 
The training and validation dataset split remained the same for all training schemes, to allow a fair comparison between different experiments.

\subsection{Equipment}\label{sec:materials:equipment}
\noindent The environmental chamber used for the characterization process (section~\ref{sec:method:characterize}) was designed and built at the Agricultural Research Organization, Volcani Institute. A cooking oven was adapted by controlling the heating element with a \campbellCtrl\ controller. A PID control loop was implemented on the \campbellCtrl\ to achieve a stable ambient temperature for the camera inside the oven. A schematic of the environmental chamber is presented in Fig.~\ref{fig:envChamber}.

The \campbellCtrl, \taucamera\ and \blackbody\ were all controlled via Python3.8 from a Linux Ubuntu 20.04 computer.

The camera was calibrated using FLIR ThermalResearch v2.1. The configuration of the camera can be seen in a table at the supplementary material and information on the various functions can be found in Tau2 Quark Software IDD.

\subsection{Camera characterization}
The number of coefficients for the radial fit (Eq.~\ref{eq:spatialRadiiFit}) was set to $\mRadial=8$. The number of coefficients for the FPA fit of the radial coefficients (Eq.~\ref{eq:fitRadiiFPA}) was set to $\mFPA=3$. These values were chosen empirically.

The results of the nonuniformity characterization process described in section~\ref{sec:method:characterize} as summarized by Alg.~\ref{alg:estimateNonUniformity} are shown in Fig.~\ref{fig:fitRes}. 
Four examples from different \opPoint s are shown. These results illustrate that the fitting is both valid and corrects the skew in the measurements.

% Figure environment removed

\subsection{Nonuniformity correction}\label{sec:results:nuc}
% Figure environment removed

A visual comparison of NUC between the proposed method and other methods is presented in Fig.~\ref{fig:results:zoomin}. The left-most figure is the input to the network. The patch in the red square is zoomed-in and presented for GxPD, He et al.~\cite{He2018} and ADMIRE~\cite{admire}. 
Observing the results, He et al.~\cite{He2018} does not thoroughly removes the NUC, and ADMIRE~\cite{admire} increases noise and adds surplus edges and details thus limiting the fidelity of its estimation. GxPD appears similar to the ground truth data.
More visual results are in figures S17-S29 in the \emph{supplementary material}.

A side-view of the results of the temperature estimation can be seen in Fig.~\ref{fig:results:plots}. These figures contain the real temperature, and the estimations made by the results of the E2E network and the physically constrained network GxPD. As can be seen, both estimations are accurate and both network configurations are similar. The input to the network cannot be displayed with the plots, because it is in gray levels, whereas the network output temperatures are in $^\circ C$.

Table~\ref{tab:resultsWithWithoutAmb} compares the metrics of the estimations between the different configurations and compares them to He et al~\cite{He2018}. The latter results were retrained on the same data using the training scheme suggested by those authors~\cite{He2018}. For a fair comparison, we constrained our network to the same depth and number of filters as He et al~\cite{He2018}.
The results of the E2E network without the ambient temperature are also compared. The metrics in the table are an average of the metrics from all validation sets.
Although ADMIRE~\cite{admire} is compared visually in Fig.~\ref{fig:results:zoomin}, its metrics cannot be compared in the table because the method does not estimate the temperature, only corrects nonuniformity.

As can be seen in Table~\ref{tab:resultsWithWithoutAmb}, the ambient temperature significantly improves the performance of the network. The results for the E2E and GxPD are similar, with the latter showing a marginal advantage. The similar performance between the E2E and GxPD networks can be explained by the expressive power of the neural network. The network in E2E can intrinsically represent the GxPD network~\cite{nn_alg_book}. Having said that, the MAE in GxPD is lower by $4\%$ in comparison to E2E, meaning that the physical constraint still has a measurable effect on the results.

\begin{table}[ht]
    \centering
    \caption{Estimation results for the different configurations (end-to-end (E2E), with and without (w/o) $\tfpa$; and physically constrained (GxPD).}
    \begin{tabular}{|c|c|c|c|}
    \hline
    Network & MAE [$^\circ C$] & PSNR [dB] & SSIM \\
    \hline
    He et al. \cite{He2018} & 1.31 & 34.46 & 0.92 \\
    \hline 
    E2E w/o $\tfpa$ &  0.48 & 43.25 & 0.99 \\
    \hline
    E2E with $\tfpa$ &  0.42 & 44.50 & 0.99 \\
    \hline
    GxPD & 0.37 & 45.43 & 0.99 \\
    \hline
    \end{tabular}
    \label{tab:resultsWithWithoutAmb}
\end{table}
% Figure environment removed

\subsection{Real data}\label{sec:experiments:realdata}
% Figure environment removed
% Figure environment removed%
\noindent We captured the same scene with an accurate \scientificCamera\ scientific-level radiometric camera and with the \taucamera\ camera. The \scientificCamera\ outputs a temperature map and the \taucamera\ outputs gray levels corresponding to the radiation flux. The camera used for capturing these images was different from the one used for the calibration process.

The ambient temperature and emissivity of the \scientificCamera\ were tuned using an accurate temperature sensor placed in the scene. The scenes were registered by hand-picking correspondence points and performing a homography with OpenCV V4.5.4.

Five results are presented in Fig.~\ref{fig:results:realData}, another result is presented in Fig.~\ref{fig:results:realData:building} and six more are presented in figures S1-S6 in the supplementary material. The gray scales are the temperatures taken using the \scientificCamera. The blue patches in the frames are the per-pixel differences between the temperatures and the results of GxPD.
The numbers in white are the MAE between GxPD and the temperature map. We used the GxPD method because its MAE results were significantly better.
The two uppermost figures are cars taken at the morning. The hot areas with high errors stem from direct sunlight hitting the metal and glass surfaces of the cars. the next two figures are buildings captured from a great distance. The last figure is a tree from a distance. 
Part of the error stems from registration errors between the two cameras, or from moving objects during acquisition (e.g, leaves in the lowest figure).

The range of the MAE is $0.15^\circ C-0.93^\circ C$.
This small error in temperature estimation is of the same order as the accuracy of the scientific \scientificCamera. 
This accurate result was achieved without any thermographic corrections or NUC from the \taucamera, only the radiation flux as gray levels. The exact configuration can be seen in the supplementary materiel.
These results are also on-par with the results on the validation set (Table~\ref{tab:resultsWithWithoutAmb}) and with the visual results (Fig.~\ref{fig:results:zoomin}).

\section{Conclusion}\label{sec:conclusion}
\noindent A method to characterize the physical behavior of a system was demonstrated (section~\ref{sec:method:characterize}). The characterization process allowed for supervised training of a deep learning network (section~\ref{sec:method:net}). The temperature estimation performed by the network can be generalized to real data and different cameras (section~\ref{sec:experiments:realdata}). This allows for a faster NUC process that only requires a single collection of calibration data.

We also showed that the ambient temperature of the camera has a significant effect on the accuracy of the temperature estimation.

The proposed method (E2E) shows a significant improvement of roughly $1^\circ C$ compared to previous works~\cite{He2018}, producing a MAE of only $0.42^\circ C$. This error was lowered even more by imposing a physical constraint (GxPD). 

The physically constrained GxPD network achieved better results than the E2E network. This result suggests that the non-linearity of the neural network is capable of decomposing the dual dependency of measurements in $\tobj$\ and $\tfpa$. We leave this aspect to future work.

The results show good agreement between the image estimation and the ground truth on simulated data with mean temperature error of $0.37^\circ C$, as well as on real-world experimental data with mean temperature error ranging in $0.15^\circ C-0.93^\circ C$.


\section*{Acknowledgments}
\noindent The authors thank Dr. Yaffit Cohen and Dr. Eitan Goldstein for the UAV data used in this work; 
and Moti Barak, Lavi Rosenfeld and Liad Reshef for the design and construction of the environmental chamber.


\section*{Disclosures}
\noindent The authors declare no conflicts of interest.

\balance    % both columns will be of equal height on the last page
\bibliographystyle{IEEEtran}
\bibliography{biblography}

\end{document}
