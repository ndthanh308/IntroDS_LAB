\let\oldyear\year
\documentclass{ieeeaccess}
\let\setyear\year
\let\year\oldyear
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{interval}

\usepackage{scalerel}
\usepackage{tikz}

\NewSpotColorSpace{PANTONE}
 \AddSpotColor{PANTONE} {PANTONE3015C} {PANTONE\SpotSpace 3015\SpotSpace C} {1 0.3 0 0.2}
 \SetPageColorSpace{PANTONE}
\usetikzlibrary{svg.path}
\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                 svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                 svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
 \begin{tikzpicture}[yscale=-1,transform shape]
 \pic{orcidlogo};
 \end{tikzpicture}
 }{|}}}}
\usepackage{hyperref} %←- Load after everything else
\hypersetup{
 colorlinks=false,
 linkbordercolor=white,
 urlbordercolor=white,
pdfborder={0 0 0}
}

% compiler error without following two lines
\usepackage{caption,setspace}
\captionsetup{font={sf,small,stretch=0.80},labelfont={bf,color=accessblue}}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\history{Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.}
\doi{10.1109/ACCESS.2017.DOI}


\title{Big Data -- Supply Chain Management Framework for Forecasting: Data Preprocessing and Machine Learning Techniques}

%%%%%%%%%%%%%%%%%%%%

\author{\uppercase{Md Abrar Jahin\authorrefmark{\orcidicon{0000-0002-1623-3859}\,1},} 
\uppercase{Md Sakib Hossain Shovon\authorrefmark{\orcidicon{0000-0002-5013-8556}\,2}, Jungpil Shin\authorrefmark{\orcidicon{0000-0002-7476-2468}\,3},}
\IEEEmembership{Senior Member, IEEE},
\uppercase{Istiyaque Ahmed Ridoy\authorrefmark{\orcidicon{0009-0001-7835-5835}\,4}, Yoichi Tomioka\authorrefmark{\orcidicon{0000-0003-3509-6607}\,3},}
\IEEEmembership{Member, IEEE},
\uppercase{and M. F. Mridha\authorrefmark{\orcidicon{0000-0001-5738-1631}\,2},}
\IEEEmembership{Senior Member, IEEE}
}


\address[1]{Department of Industrial Engineering and Management, Khulna University of Engineering and Technology (KUET), Khulna 9203, Bangladesh}
\address[2]{Department of Computer Science, American International University-Bangladesh, Dhaka 1229, Bangladesh}
\address[3]{Department of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu 965-8580, Japan}
\address[4]{Institute of Business Administration, University of Dhaka, Dhaka, Bangladesh}


\tfootnote{This work was supported by the Competitive Research Fund of The University of Aizu, Japan.}

\markboth
{M.A. Jahin \headeretal: Big Data -- Supply Chain Management Framework for Forecasting}
{M.A. Jahin \headeretal: Big Data -- Supply Chain Management Framework for Forecasting}

\corresp{Corresponding author: Jungpil Shin (e-mail: jpshin@u-aizu.ac.jp).}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
This article intends to systematically identify and comparatively analyze state-of-the-art supply chain (SC) forecasting strategies and technologies. A novel framework has been proposed incorporating Big Data Analytics in SC Management (problem identification, data sources, exploratory data analysis, machine-learning model training, hyperparameter tuning, performance evaluation, and optimization), forecasting effects on human-workforce, inventory, and overall SC. Initially, the need to collect data according to SC strategy and how to collect them has been discussed. The article discusses the need for different types of forecasting according to the period or SC objective. The SC KPIs and the error-measurement systems have been recommended to optimize the top-performing model. The adverse effects of phantom inventory on forecasting and the dependence of managerial decisions on the SC KPIs for determining model performance parameters and improving operations management, transparency, and planning efficiency have been illustrated. The cyclic connection within the framework introduces preprocessing optimization based on the post-process KPIs, optimizing the overall control process (inventory management, workforce determination, cost, production and capacity planning). The contribution of this research lies in the standard SC process framework proposal, recommended forecasting data analysis, forecasting effects on SC performance, machine learning algorithms optimization followed, and in shedding light on future research.
\end{abstract}

\begin{keywords}
Data analysis; Decision making; Demand forecasting; Hyperparameter tuning; Literature review; Supply chain performance
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
\PARstart{T}{he} supply chain (SC) has evolved sufficiently over the past years to discover new methods and techniques for solving SCM problems. The SC can develop its configuration based on its control, coordination, and management \cite{maccarthy2016}. The advent of Big Data (BD) brings one such change. Like other fields, BD can be utilized to improve decision-making reprocesses and alter business models through multiple resources, tools, and applications \cite{waller2013}. Therefore, SC and BD usages are connected to help one another. Although the concepts for SCM are already well-developed, it is possible to improve further; recent researches on enhancing efficiency through collaboration \cite{schliephake2009}, usage of RFID and intelligent goods \cite{holmqvist2006} are two examples of such innovations that improved SCM processes. Newer technologies are further enabling the discovery of innovative strategies for solving SC problems. Big Data Analytics (BDA) is one such disruptive innovation. Although BD has been present for a long time, the approaches to making sense of BD are comparatively new, and such systems have not been wholly integrated into other branches of knowledge \cite{yu2018}. We identified the absence of data usage and relevant processes in SC directly as a major problem that needs to be addressed.

BD has similarly grown popular over the years. After academic and technical publications first mentioned such technological developments, it has drawn the attention of various people, including literary scholars, corporate leaders, and government officials \cite{hazen2018}. The most recognizable feature of BD is probably its size or the amount of data stored. Beyond this feature of volume, BD is recognizable by the greater variety in data types, high data collection and analysis speed or velocity, veracity dealing necessity, random variabilities in data, and the growth in value as more data are analyzed \cite{gandomi2015}. Simply having access to BD is not helpful; data analytics is a must to create value or gather information out of the enormous collection of data. Where analytical methods are applied to BD, it is called BDA. While there are vast applications of BDA, its role in improving the SC process is notable.


\subsection{Research gaps}
We found potential research gaps combining the preprocessing for machine learning (ML) forecasting, control process (SC processes where BDA is helpful), and post-process (for the evaluation of forecasting model). Although there is fragmented research on these particular topics (“SC forecasting model performance” or “the application of BDA on SC” or “ML forecasting techniques with BDA implementation” or “BD driven SC performance evaluation”), the necessity to form a cyclic connection among these three processes led us to the development of this article. 

\subsection{Research objectives}
The primary objective of this article is to shed light on the potential BDA investigations on SCM studies, what significant contributions BDA has made to the efficient use of ML forecasting in SC processes, what preprocessing and post-processing SC forecasting techniques have been robustly developed so far, and are currently in use. Mainly, the forecasting techniques in an SC setting have been discussed from the perspective of BDA. This research aims to continually enhance the performance of a forecasting model incorporating a sustainable circular BDA-SCM framework that can drive future research using business intelligence and value theory as theoretical approaches. Systematic Literature Review (SLR) was used to perform this research, which is an approach used to locate, evaluate, and interpret what relevant research has been done on a specific issue or topic by sketching out and analyzing the current intellectual landscape \cite{tranfield2003}. The review paper attempts to combine the applications of BDA and ML forecasting in SCM by seeking the solutions to the following research questions (RQs) for guiding the study's development to accomplish its overall objective:
\begin{itemize}
    \item RQ1: What are the efficient steps to formulate an ML Forecasting model to predict the SC factors?
    \item RQ2: How can the forecasting, SC decision-making, and performance measurement processes be connected, tracked, and optimized in cyclic order?
    \item RQ3: How can forecasting affect SC performance, and which ML forecasting models are relevant to SC forecasting?
\end{itemize}

The novelty of this paper lies in several key aspects:

\begin{itemize}
    \item The introduction of a comprehensive BDA-SCM framework that provides a holistic view of SC forecasting and highlights interconnections between processes, offering a novel perspective.
    
    \item The integration of ML techniques within SCM for forecasting purposes presents novel approaches to enhance accuracy and effectiveness.
    
    \item Addressing the issue of phantom inventory, providing insights and potential solutions to improve inventory management practices and forecasting precision.
    
    \item Exploring the connection between accurate forecasting and SC performance, offering a novel perspective on leveraging forecasting models for optimization.

    \item Conducting a comprehensive survey of 152 papers spanning several decades, providing a unique and valuable contribution to the field and consolidating a vast body of literature.

    Including papers from such a broad timeframe allows for identifying trends, shifts in methodologies, and key milestones in the field. This comprehensive survey sets this paper apart from other SLR review papers that have not undertaken such an extensive examination of the literature. The insights gained from this extensive survey enhance the robustness and reliability of the conclusions drawn in the paper.
    
\end{itemize}
By incorporating these novel aspects, the paper contributes to the existing body of knowledge in the field of BD - SCM Framework for Forecasting, offering new insights, methodologies, and recommendations for future research and practical implementation.


\section{Research procedures}
\subsection{Planning the review}
In this article, the BDA-SCM cyclic framework was developed initially incorporating pre-process, control process, and post-process phases. Each phase was illustrated utilizing the most relevant selected works of literature. For forecasting purposes, pre-process recommendations include a step-by-step approach to forecasting and BDA best practices to facilitate comprehensive demand forecasting considering state-of-the-art technologies and relevant research. In the control process, how SC factors and forecasting affect workforce efficiency have been discussed. The post-process portion of the managerial decision-making process explains how managers use the KPI and optimization of the forecasting model to choose the appropriate metrics and insights.
\subsection{Conducting the review}
\subsubsection{Search strategy}
This SLR aimed to provide a comprehensive and objective evaluation of the existing research until 2023 on BDA-SCM, including an investigation and analysis of various SC forecasting problems and BDA innovations, strategies, and techniques. Major academic databases, including Google Scholar and Science Direct, were searched to minimize bias and ensure the inclusion of a broad range of relevant sources and content. Only English articles published in peer-reviewed journals in the fields of Computer Science, Business, Management and Accounting, Engineering, and Decision Sciences were included. Figure \ref{fig1} shows the PRISMA flow diagram for the systematic review process, which includes the number of articles identified, screened, and included in the analysis.
A combination of keywords and subject headings related to the topic of interest was used to develop the search strategy. The search strings were limited to the title, abstract, and keywords fields and included the following terms:
\begin{itemize}
    \item (“Data Analytics” OR “Big Data” OR “Data Analysis”) AND (“Supply Chain Management”) AND (“Forecasting”) 
    \item (“Data Preprocessing” OR “Data Wrangling” OR “Supply Chain Data Analysis”)
    \item (“Supply Chain Forecasting” OR “Demand Forecasting”)
    \item (“Warehouse” OR “Inventory”) AND (“Workforce” OR “Human”) AND (“Forecasting”)
    \item (“Supply Chain Performance” OR “Supply Chain KPI” OR “Supply Chain Monitoring”)
    \item (“Forecasting KPI” OR “Forecasting Error Measurement” OR “Forecasting Performance”)
    \item (“Forecasting Model” OR “Time-series Forecasting”)
\end{itemize}

\subsubsection{Selection strategy}
To ensure that the selected papers were empirically sound and conceptually relevant to BDA-SCM-related research advances, the relevance of each publication was assessed. Articles were considered more relevant if the search terms appeared in the title, abstract, keywords, and throughout the text. The identified papers were critically analyzed, particularly in terms of the relevant sections that mentioned BDA-SCM. This approach drew from relevant views on SCM-forecasting challenges and BDA techniques and helped to achieve the research review goals.
The remaining articles were then assessed to verify that they provided the necessary research perspective and empirical data to meet the review's objectives. Finally, to ensure that the selected articles aligned with the review goals, we conducted a rigorous alignment process, comparing the articles to the research review objectives. Only articles that met all of the selection criteria were included in the final review.

% Figure environment removed


\section{BDA-SCM framework }
Figure \ref{fig2} mainly consists of the use and cyclic flow of data in SC. It only includes the SC parts where BDA may be involved. Figure \ref{fig3} complements Figure \ref{fig2} by mentioning the methods for cleaning, exploring, and analyzing data properly. It includes feature engineering techniques to select only the most relevant and unique features from which ML algorithms can learn efficiently. Finally, Figure \ref{fig4} is a proposed method for data splitting, model training, hyperparameter optimization, cross-validation, testing, and evaluating errors to perfect the forecasting methods mentioned in Figure \ref{fig2}.

% Figure environment removed

% Figure environment removed

% Figure environment removed


\section{Pre-process}
\subsection{Identifying business problems}
At the outset, the type of data that needs to be collected, stored, analyzed, and interpreted is selected based on SC strategies. \cite{wieland2013} categorized SC strategies based on risk and impact, such as robustness for low-impact high-risk, agility for low-risk high-impact, rigidity for low-impact low-risk, and resilience for high-impact high-risk decisions. The strategies can be adopted by varying levels of responsiveness and efficiency. Responsiveness has been a critical factor in gaining a competitive advantage, and it depends on the deviations in demands and a company's capability to respond to such deviations. An increase in responsiveness decreases efficiency, and vice versa \cite{minnich2006}. The responsiveness level affects product volume, order fulfillment rate, workforce, manufacturing capacity, warehouse capacity, transportation carriers, product mix, supplier’s product mix, inbound and outbound logistics, etc. \cite{thatte2013}. Therefore, whether data needs to be collected should be decided based on responsiveness, as different sets of data by allocating are required to boost the SC efficiency and responsiveness. Furthermore, the frequency of data analyses would also depend on the responsiveness level of the company. In short, the factors that may dictate the sort of forecasting required for a business include the context of forecasting, the types of data available, the required level of accuracy, the length of the forecasting period, the time available for each forecast, and the value addition made through the forecast \cite{chambers1971}.

\subsection{Identifying data sources}
Once the data that needs to be gathered is selected, identifying the sources is essential. Determining variables is required for timely forecasts to bring helpful information \cite{duarte2006}. Moreover, a conclusion may not be based on a single type of data; the initial conclusion can be validated based on multiple data types. \cite{varela2014} mentioned 56 different data sources for four main SCM levers, procurement, warehouse operations, marketing, and transportation, as leveraging various data sources allows finding actionable insights quickly; some of the more relevant data sources have been listed below:

\begin{enumerate}
  \item Transportation
  \item Barcode systems
  \item Demand chain
  \item CRM Transaction data
  \item BOMs
  \item Customer surveys
  \item Blogs and news
  \item Demand Forecasts
  \item Procurement
  \item Delivery times and terms
  \item Invoice data
  \item ERP Transaction data
  \item GPS-enabled BD telematics
  \item Product reviews
  \item Competitor pricing
  \item Inventory costs
  \item Customer Location and Channel
  \item Traffic density
  \item Email records
  \item Crowd-based Pickup and Delivery
  \item Equipment or asset data
  \item Intelligent Transport Systems
  \item EDI purchase orders
  \item Warehouse operations
  \item Logistics Network Topology
  \item In-transit Inventory
  \item SRM Transaction data
  \item Transportation Costs
  \item Warehouse Costs
  \item Pricing and margin data
  \item RFID
  \item Origination and destination (OND)
  \item Local and global events
  \item Supplier current capacity \& customers
  \item Sales history
  \item Weather data
  \item SKU level
  \item Supplier financial performance information
  \item Raw material pricing volatility
  \item On-Shelf-Availability
  \item P2P (Procure-to-Pay)
  \item Product traceability \& monitoring system
\end{enumerate}


\subsection{Data preprocessing and feature engineering (FE)}
\subsubsection{Duplicates removal}
It is problematic to waste space and runtime with duplicate rows. Duplicate rows create incoherence, and the ML model fails to learn new information. Because of input mistakes, changes in some feature values (e.g., the identifier value) may generate duplicate rows that will be deemed distinct by the machine. Using data preprocessing libraries in Python and R, it is easier to drop the duplicates or substitute them with relevant values. Nevertheless, the main challenge is the identification of factors on which the duplicates should be removed. Of the number of methods invented to remove duplicates, we review the following:

\textit{Bayesian:} The Fellegi-Sunter-algorithm is the most commonly used model in probabilistic approaches because of its Bayesian nature \cite{fellegi1969}. The Bayes Decision Rule is a common approach \cite{elmagarmid2007}. A Bayesian inference difficulty may develop when the probability density of a unique row differs from a duplicate record and the functions are known. Neural network (NN) algorithms are more accurate without the Fellegi-Sunter-algorithm if the data are adequately described or labeled \cite{wilson2011}.

\textit{Partitioning Methods:} Clustering methods identify and drop duplicates utilizing graph partitioning approaches \cite{singla2004}. However, \cite{hassanzadeh2009} compared 12 clustering methods and found that the popular sophisticated algorithms provided lower accuracy, first suggesting that Markov Clustering is a more scalable, accurate, and efficient algorithm.

\textit{Aggregate fitting:} CART \cite{cochinwala2001} and SVM \cite{joachims1999} aggregate fitting results for various row features. SVM is highly memory efficient and works well with lots of dimensions. However, it does not work well with large datasets or data with overlapping classes. CART is intuitive and easily used. The problem is that data is classified based on the sample and may not apply to larger datasets.

\textit{Others:} Bootstrapping clusters \cite{verykios2000} or hierarchical graph structures encode the features as non-matchable binary features creating dual probability densities rather than probabilistic distribution modeling for the inspected quantities \cite{ravikumar2012}. Bootstrapping clusters are used for unsupervised data. Simple techniques have long been studied, such as utilizing distance measurements to identify duplication \cite{monge1996}. Weighted transformations also occur in literature \cite{dey1998}. Additional methods, like ranking the most same-type weighted rows comparable to those provided, are also utilized to identify the least duplicated rows \cite{guha2004}.


\subsubsection{Dealing with categorical features}
One-Hot, Ordinal, Helmert, Polynomial, and Binary encoders are outperformed with a 95\% accuracy by Sum, and Backward Difference encoders are preferred for prediction jobs \cite{potdar2017}. \cite{lopez2020} presented a generic Information-based encoder that transforms mixed-type features into numeric ones while maintaining the dataset’s original dimension, with better accuracy than One-Hot and Feature-Hashing. \cite{garnier2019} demonstrated that Ordinal-encoder (straightforward and convenient to execute but incorporates a sequence of features) outperformed Hashing (introduces limited features and moderately ignores the feature sequence); One-hot-encoding generates a massive number of features and forces the use of a very simplified regression analysis. To train residual features from time categorical variables derived from variable time stamps, DeepGB neural network with embedding layers may be used, which are necessary to learn multiple time series at once to encode categorical features in a lower dimension or by embedding their IDs and retrieve helpful information \cite{karingula2021}.

\subsubsection{Data scaling}
Normalization is valuable when using ANN, clustering techniques, or classification software. The learning phase may be accelerated by normalizing the data features in tanning faces for backpropagation NN methods.

\textit{Min-max normalization:} The scaling of $b$ values of a numerical feature $F$ to a defined range represented by $[\text{new-min}_F, \text{new-max}_F]$ is termed min-max normalization. To acquire the new value, the following equation is applied in $b$ to produce a changed value $b'$:

\begin{equation}
\small
b' = \frac{{(b - min_F)}}{{max_F - min_F}} \cdot (new - max_F - new - min_F) + new - min_F
\label{eq1}
\end{equation} where $max_F$ and $min_F$ mean the maximum and minimum feature values, respectively. In normalization, $[new-min_F, new-max_F] = [0,1]$ or $[-1,1]$ are the usual intervals \cite{bib5}.

Datasets prepared for use with distance-based learning methods commonly use this normalization technique. The features having a significant $max_F-min_F$ difference will be prevented from dominating the distance computation by applying a normalization to rescale the data to the same value ranges, and it will not be able to distort the learning process by assigning the older features much weight. It is also known to help ANNs learn faster by allowing the weights to converge more quickly. 

\textit{Z-score normalization:} Min-max normalization is not practicable if the minimum and maximum values are not provided. Even when these values are known, the existence of outliers might cause the min-max normalization to be skewed by clustering the values and restricting the computational accuracy available to represent them.

\begin{equation}
b' =  \frac{(b - \bar{x})}{s_x}
\end{equation}
where $\bar{x}$ is the sample mean.
\begin{equation}
\bar{x} = \frac{1}{n} \sum_{i=1}^{n} b_i
\end{equation}
Moreover, $s_x$ is the mean absolute deviation of $x$ \cite{han2012}.
\begin{equation}
s_x  =  \frac{1}{n} \sum_{i=1}^{n}(b_i-\bar{x})
\end{equation}

\textit{Decimal scaling normalization:} Normalising the numerical feature values by relocating the decimal-point by 10th power divisions so that the $\text{$highest$ $absolute$ $value$}<1$ after transformation is a simple method for minimizing the absolute feature values.
\begin{equation}
b' =  \frac{b}{10^k}
\end{equation}
where $k$ is an integer (the lowest), such that $new-max_F<1$.


\subsubsection{Data transformation}
Data transformation can create new features, also known as changing features, where mathematical formulae derived from business models or pure mathematical formulae are used to integrate the raw input features. Linear, quadratic, polynomial, non-polynomial, rank and Box-Cox transformations are a few to name among different existing transformation techniques. 

Normalizations may not be sufficient in research experiments, and full automation to fit the data and optimize the resulting model. Combining the data embedded in several features may be advantageous in some circumstances. Linear transformation based on simple algebraic operations is a basic approach that may be utilized for this goal. A quadratic transformation can occur when a newly introduced feature is formed using the expressions in quadratic form. Using the fundamental features of the dataset, quadratic modifications can assist us in uncovering information that is not directly there. Transformation approximation using polynomials could be implemented by brute force exploration with one unit at a time when no expert assistance can tell us which transformation and features to employ. The transformation of the rank approach is recommended for identical training and test data or a complete dataset for DA and cluster analysis model development \cite{refaat2010}.

Nonparametric approaches are not recommended to be introduced into traditional statistics courses using rank transformation because it inhibits how widely the nonparametric technique may be used, which is unnecessary. Another misperception is that the nonparametric technique is utilized chiefly for hypothesis testing. This entirely obscures the superior theoretical and conceptual flexibility of many nonparametric methods.

\cite{spitzer1978} studied the limited sample aspects of the estimated parameters using the Box-Cox transformation. Under the premise of approximating normalcy, the technique worked well. The outputs were utterly impartial for forecasting, and their differences were surprisingly small. Asymptotic variances and stability features of Box-Cox estimates in the linear model were examined by \cite{bickel1981}. In the case of unknown transformation parameters, linear regression models with minor to intermediate error variances showed much higher asymptotic variances than known ones. Furthermore, they observed that Box-Cox approaches perform inconsistently in models with minor to intermediate residual variance.

\subsubsection{Filtering extreme outliers}
The most often recommended approach in the literature of outlier identification and repair is via filtering.

\textit{Outlier detection:} Statistical methods for detecting outliers include box plots, scatter plots, z-scores, and IQR (Interquartile Range) scores. Normal distribution empirical relations should be followed for outliers where the values are $<\mu$ - $3\sigma$ or $>\mu+3\sigma$ for normal distribution, where $\sigma$ and $\mu$ are the standard deviation and mean of a particular feature. IQR proximity rule should be used in which outliers are $<(Q1$ – $1.5\times IQR)$ or $>(Q1 + 1.5\times IQR)$ for skewed distribution. For other distributions, a percentile-based approach should be used in which values that are distant from the 99 percentile and $<1$ percentile are regarded as outliers.

\textit{Outlier treatment:}\\
Trimming: Outliers from the dataset are removed, and it’s not recommended.\\
Capping: In capping, if the values are greater or less than an established threshold, those are deemed outliers, and the outlier numbers in the dataset determine the capping number.\\
Treat outliers as MVs: Outliers may be handled as the MVs are treated.\\
Outlier Removal Clustering (ORC) eliminates outliers in each loop, a modification of K-Means Clustering. ORC efficiently removes outliers from clusters. The parameters should be adjusted appropriately as the model precision changes depending on the dataset. ORC guarantees that the centroid computation is not biased by locations distant from the k-clusters.

\subsubsection{Dealing with missing values (MVs)}
In SC Data Analysis, one of the preprocessing techniques, Imputation, is adopted to overcome the drawbacks of MVs. The most straightforward approach to drop the rows having MVs is if a comparatively small fraction of observations is present and the analysis of all rows is not substantially skewed in interpretation \cite{little1987}. \cite{barnard1999} showed that MVs are generally connected with three sorts of issues:
\begin{enumerate}
    \item Inefficiency
    \item Difficulties in managing and interpreting data
    \item Skewness because of discrepancies between perfect and missing data.
\end{enumerate}

When it comes to MV therapy, there are generally three options \cite{farhangfar2007}:
\begin{enumerate}
    \item First, eliminate all instances that have MVs in their features. Thus, removing features with higher-than-normal MV levels falls within this area.
    \item When estimating the model parameters for a whole dataset, another way is to employ maximum likelihood processes, using the obtained model parameters for imputation via sampling.
    \item Finally, MV imputation is a group of processes focused on substituting predicted MVs for existing ones. Most of the time, the features in a data set are interdependent. As a result, MVs may be calculated by identifying correlations among features.
\end{enumerate}

\textit{Common approaches:} To keep the MVs unchanged, known as Do Not Impute (DNI), is the most straightforward approach where if the baseline MVs strategies are available, the algorithm must employ them. When many rows include MVs and using DNI would lead to an irrelevant, inaccurate, and small dataset, then MVs are commonly substituted by the universal-most-frequent feature value for nominal features and the universal mean value for quantitative features \cite{grzymala2005}. \cite{batista2003} showed another process utilizing Hot Deck that partitions the complete dataset into clusters, links each row with a cluster, and fills up the MVs, where any complete row from the cluster can be utilized. The imputation of Cold Deck is identical to the hot deck, except the dataset cannot be the existing dataset. They demonstrated that the MVs imputation based on the KNN might beat the internal techniques assessing C4.5 and CN2 to handle MVs and exceed the imputation method of mean or mode, which is widely intended to treat MVs.

\textit{Maximum likelihood imputation methods:} Assume for $n$ independent rows $(i=1,…,n)$, there are $k$ variables $(y_i1,y_i2,…,y_ik)$ with no missing data. The maximum likelihood function is:
\begin{equation}
\small
    L = \prod_{i=1}^n f_i(y_{i1}, y_{i2}, \ldots, y_{ik}; \theta)
\label{eq6}
\end{equation}

Assume that $y_1$ and $y_2$ have MVs that fulfill the Missing at Random (MAR)-assumption for a specific row $i$. The combined probability for that observation is the chance of witnessing the remaining features, $y_{i3}$ $ $ through $y_{ik}$. If $y_1$ and $y_2$ are two discrete features, this is the aforementioned combined probability multiplied by all potential values of the two features with MVs:
\begin{equation}
\small
f_i^* (y_{i3},\ldots,y_{ik};\theta) = \prod_{y_1}\prod_{y_2}f_i(y_{i1},\ldots,y_{ik};\theta)
\label{eq7}
\end{equation}

For continuous MVs,
\begin{equation}
\small
f_i^*(y_{i3}, \ldots, y_{ik}; \theta) = \int_{y_1} \int_{y_2} (y_{i1}, y_{i2}, \ldots, y_{ik}) \, dy_2 \, dy_1
\label{eq8}
\end{equation}

The multiplication of probabilities for all the rows is the overall likelihood. If there are $q$ rows with full data and $p-q$ rows with MVs on $y_1$ and $y_2$ features, then the ML function becomes:
\begin{equation}
\small
L = \prod_{i=1}^q f_i(y_{i1}, y_{i2}, \ldots, y_{ik}; \theta) \prod_{i=q+1}^p f_i^*(y_{i3}, \ldots, y_{ik}; \theta)
\label{eq9}
\end{equation}

\cite{bib2} narrowed down the following imputation options using non-parametric statistical testing:
\begin{itemize}
    \item Row elimination (IM) and no imputation (DNI) methods are outperformed by imputation techniques that fill in the MVs.
    \item No single-size/generic imputation method works for all regressors or classifiers.
\end{itemize}

CMC and EC methods are proposed that yield a lower noise ratio for Wilson and balance the mean MI difference. The proposed imputation approaches focused on classification techniques, including Rule Induction Learning Models: FKMI, Black Boxes Methods: EC, and Lazy Learning (LL) models: MC.

\subsubsection{Binning}
In this method, a continuous variable is converted into a group of intervals. Each interval can then be treated as a ‘bin,’ with the option of enforcing an order dependent on the data's subsequent processing. While smoothing, each bin's min and max values are calculated as bin borders. Then, for each value, the nearest border value is substituted. Typically, the smoothing effect increases with bin width. If the bin widths are identical, binning may be used as a discretization method by substituting mean or median for bin value. It is possible to create hierarchical ideas by iterating over this procedure indefinitely. It's unsupervised since class labels are not used, and bin numbers are specified by the user.

\subsubsection{Deep-Learning (DL) based FE}
A multi-filter NN (MFNN) end-to-end model was developed for multivariate financial time-series FE and classification-based forecasting utilizing DL techniques \cite{peng2005}. Their proposal MFNN was 15.41\% higher than the best result (Logistic Regression) of traditional ML models and 22.41\% higher than the statistical approach (Linear Regression) in terms of returns.

\subsection{Exploratory data analysis (EDA) \& data reduction (DR)}
In this process, the target or dependent column and independent features are obtained. The DR, EDA, and clustering techniques reduce runtime and space during the deep-learning modeling phase. DR can be employed to decrease the size of a dataset while still keeping the data’s original integrity. In our framework, we suggest performing Feature Selection and Feature Extraction simultaneously after selecting the target column and finding redundant features; then, Discretisation may be performed if necessary. Then the dataset will be ready for further analysis and model training.

\subsubsection{Identifying redundant features}
Feature Redundancy lengthens the modeling time of ML algorithms and leads to model overfitting. Feature redundancy arises from the possibility of derivation from another feature or set of features. The following techniques may be adopted to handle redundancy:

\textit{Covariance and correlation:} In statistics, covariance refers to the amount that two features or factors change in tandem whose value lies in the $(-\infty, +\infty)$ range. Positive covariance indicates they move in the same direction. Negative covariance means that any features are greater than the mean, and others are less than vice-versa. Zero covariance means features may be independent under a certain hypothesis. 
On the other hand, correlation analysis is a widely used dimensionless measurement ranging from $-1$ to $+1$ to discover redundancies in numerical features that evaluate and quantify the relationship intensity. The features are positively correlated for correlation values greater than zero (0); for zero, they are independent; for less than zero, they are negatively correlated \cite{bib5}. Covariance and correlation are directly proportional to each other. 
In numeric feature selection, correlation is better to use, as correlation analysis is scaled [-1, 1], but the covariance range is indefinite $(-\infty, +\infty)$. We should choose correlation for better interpretation. Changes in location, size, or scale have no effect on correlation. However, both of them are limited to only being able to identify linearity.

\textit{$\chi^2$ correlation:} The $\chi^2$ (Chi-Square) test is often used when dealing with nominal features and finite value-set. We can use the $\chi^2$ test to see whether there is any link between the values of two nominal features, where a probability table with joint events is established. If $\alpha$ (significance level) is less than the estimated one (or the $\chi^2$-value (calculated) > table value), the null hypothesis gets discarded, and the two features can be said to be correlated statistically \cite{bib5}.
SC Analysts must remember that the $\chi^2$-test does not tell much about the strength of the relationship between two features. The $\chi^2$-test offers advantages such as resilience regarding data distribution, computational simplicity, extensive information produced from the test, utilization for investigations where parametric criteria cannot be satisfied, and scalability in processing data from two and multiple-group research. The drawbacks are sample size constraints and difficulty in comprehension when there are many (>20) features.


\subsubsection{Feature selection(FS)}
The reasons for conducting FS may include removing unnecessary data, enhancing forecasting accuracy, reducing data cost and model complexity, and improving training efficiencies such as reductions in space needs and computational costs \cite{saeys2007}. FS approaches, despite their widespread use, have several drawbacks:

\begin{itemize}
    \item Training data size significantly impacts the subsets produced by many FS models (particularly those created using wrapper-based techniques). If the training data is limited, then the feature subsets retrieved will be limited, resulting in the loss of key variables.
    \item Because the target feature is connected with many independent features, and their removal would adversely influence learning accuracy, reducing high-dimensional data to a limited range of features is not always possible.
    \item When dealing with huge datasets, a reverse elimination approach takes too long since the algorithm must make judgments based on enormous amounts of data in the early stages.
    \item In certain circumstances, FS results will still include significant important features which may obstruct using complicated training strategies.
\end{itemize}

\textit{Leading methods:} In order to create FS techniques by combining a feature evaluation score and a cutting criterion, \cite{arauzoazofra2011} recommended that functions based on information principle produce better accuracy, not suggesting any universal cutting condition. However, those independent of the metric perform best, and outcomes differ across models, and for each kind of model, wrapper techniques were recommended to avoid this effect. 

\cite{bolon2013} investigated nine feature selectors running across 11 simulated datasets to examine the methodologies in the context of a growing number of unnecessary features, noise in the data, redundancy, correlation between attributes, and the ratio of observations to features. ReliefF proved to be the best alternative regardless of the specifics of the data, and it is a filter with a cheap computational expense. Wrapper techniques have proven to be an intriguing choice in specific disciplines if they can be used with the same classifiers and consider the greater computing costs. Extensive theoretical research has been conducted on the Relief and its variants, showing that they are resilient, noise-resistant, and can decrease their space-time complexity in parallel \cite{robnik2003}. 

Since the emergence of rough sets in pattern recognition, several FS techniques have based their criteria for assessing reductions and approximations based on this idea \cite{swiniarski2003}. Because total searches of substantial datasets are impossible, stochastic methods based on meta-heuristics and approximate assessment criteria have also been explored. \cite{wang2007} utilized particle swarm optimization for this job. Features are discontinuous, making it challenging to pick approximately set-based characteristics in the literature. Rough set-based feature selection's key drawback is the constraining condition that all values be discrete, for which issue, a fuzzy rough FS method (FRFS) was suggested \cite{jensen2007}.

When data is vast, messy, blended with categorical and numerical variables, and may have dynamic effects requiring sophisticated models, the synthesis of forecasting analytics in the form of ensembles can create a compressed sample of non-redundant features \cite{tuv2009}. There are four phases to the technique suggested here: identifying relevant features, computing masking scores, removing the masked factors, and generating residuals for progressive modification. The Random Forest ensemble is considered in all four stages.

Two problems arose simultaneously with the growth of highly-dimensional data: FS is essential in every training, and the accuracy and robustness of the FS algorithms may be ignored. \cite{rodriguez2010} discussed the FS reduction job introducing the Quadratic Programming FS (QPFS), which utilizes the Nyströn-approximation-matrix diagonalization method for large datasets. mRMR and ReliefF were outperformed using Pearson's correlation coefficient and MI. A local learning-based approach may be beneficial when assessing many irrelevant attributes and complicated data ranges \cite{sun2010}. The impacts of high-dimensional datasets may be mitigated by pre-processing the feature ranking procedure to exclude class-dependent density-based features \cite{javed2012}. To scale any method in significant data issues demands cutting-edge distributed-computing frameworks like MapReduce and Message Passing Interface (MPI) \cite{zhao2013}.

We can use supervised FS if the data has class labels; else, unsupervised FS is the best option. This approach generally maximizes clustering efficiency or the FS based on correlation, feature dependency, and priority. The primary premise is to eliminate features that bring almost no value beyond what is already provided by the existing features in the system. \cite{mitra2002} suggested using feature dependency/similarity to reduce redundancy without needing a search procedure. An information compression metric called the maximum information compression index governs the clustering partitioning process, which uses features as the measure of similarity. Forward orthogonal search (FOS) is another unsupervised FS approach that aims to maximize the total reliance on the data to find relevant features \cite{wei2007}. Without compromising performance in clustering, the unsupervised FS used the Random Cluster Ensemble framework to compress the set of features by roughly 1/100 of its initial dimensions \cite{elghazel2015}. When compared to well-known classifications, precision/recall analyses revealed that feature weighting was highly successful in discovering the most suitable clusters \cite{modha2003}.

\subsubsection{Feature extraction}

Feature extraction accelerates the ML algorithm’s execution, optimizes raw data quality, boosts the algorithm’s efficiency, and simplifies the interpretation of the findings.

Principal component analysis (PCA):  It targets to analyze a collection of features’ variance-covariance patterns employing a few linear combinations and seeks the optimal $k$ number of $n-dimensional$ orthogonal vectors for data description, where $k \geq n$. Accounting for the most critical percentage of the discrepancy in the original dataset, the principal component (the first derived feature) is produced in decreasing order of contribution. Typically, for containing $\geq$ 95\% variance, just the top few principal components are retained. PCA is beneficial when many independent variables correlate with one another \cite{johnson2007}. Principal-component is quick and comprehensive and ensures a solution is found for all datasets \cite{bib3}.

\textit{Factor analysis:} The fundamental concept underlying component analysis identifies a collection of influencing factors to restore the current features through a series of linear adjustments on the components. It is a method that finds out the range of factors along with their associated loadings, providing the features as well as the mean of the features \cite{johnson2007}. The factor models can be solved by (1) the Maximum-likelihood method and (2) the Principal-component method. Maximum likelihood presupposes actual data following a normal distribution and is computationally costly. 
The comparative differences between PCA and factor analysis are:
\begin{enumerate}
    \item Factor analysis, unlike PCA, implies a basic structure that connects the factors to the empirical observations.
    \item A three-factor system is substantially different from a two-factor system in factor analysis; however, in PCA, the two initial principal components stay the same when employing a third component.
    \item PCA is simple and quick. There are several methods for doing the computations in factor analyses, some of which are complex and tedious. 
    \item Using a sequence of linear transformations, PCA attempts to spin the original features’ axis. Again, factor analysis generates a new range of features to demonstrate the observed covariances and correlations.
\end{enumerate}
	
\textit{Multidimensional scaling (MDS):} MDS may be used in SCM to estimate the map depicting transportation distances between or within inventories using the distance matrix. The result is skewed owing to the disparity between calculated distances and the actual distances between inventories lying in a straight line. The map is typically centered on the origin and expanded to cover considerable distances. However, the answer may be found in any rotation.
\textit{Locally linear embedding (LLE):} With LLE, local linear fits are used to restore universal nonlinear configuration \cite{roweis2000}. All points are a linear weighted sum of their neighbors if adequate data is available. It is the basic notion behind the manifold approximation algorithm. For the LLE algorithm, the geometric principle is all that is required. LLE's advantages are that local minima are not involved in optimizations and have only two parameters. The embedded space has a universal coordinate system and preserves the local geometry of high-dimensional data. LLE also has several inherent shortcomings, which are stated as follows:
\begin{itemize}
    \item LLE generates folds and nonhomogeneous warps when the dataset is small or the points are irregularly measured.
    \item Noise significantly affects LLE, which causes embedding derivation errors.
    \item Short circuits may develop during the neighbors since the query typically uses Euclidean distance.
    \item Poor eigenproblems may arise.
    \item If two high-dimensional space observations differ, LLE cannot assure that their corresponding low-dimensional space instances also differ.
    \item LLE's embedding findings are extremely susceptible to its two system parameters: the number of clusters of each instance and regularisation.
    \item LLE presupposes that complete data exists on a unified surface and is unsupervised, but that does not happen for multi-labeled classification tasks.
    \item It is unclear how to assess the new sample data points because LLE does not provide a parameterized function that reflects between high-dimensional space and low-dimensional manifold. 
\end{itemize}


\subsubsection{Cardinality reduction}
The merging of two or more nominal or ordinal variables into a single unique category is called cardinality reduction. It is challenging to manage nominal features with a large number of groups. Converting high cardinal variables into binary variables provides many new variables, mostly zeroes. However, if utilized without conversion with models like Decision Tree that can accept them, there are risks of model over-fitting. So, decreasing the number of groups should be considered \cite{bib3}.

\subsubsection{Discretization}
The discretization method turns the numerical data into qualitative data, i.e., quantitative features, into discontinuous or nominal features that provide a non-overlapping segmentation of a linear system. Discretization can decrease data since it converts data to a much smaller sub-ensemble of discrete values from an enormous range of numerical values. Numerical features should be discretized as real-world dataset features are generally continuous, whereas most of the current ML algorithms can only be trained by utilizing nominal features in categorical data \cite{bib4}. 

Discretization generally involves four steps: (1) Continuous feature values to be discretized need sorting, (2) identifying a breakpoint or nearby intervals for joining, (3) dividing or combining continuous value ranges based on specific criteria, and (4) stopping this process at definite value.

MVD and UCP are promising approaches that are not supervised and helpful to apply to various ML issues other than the classification under adverse circumstances. They generalized a subset of the top global discretizers based on a compromise between UCPD, FUSInter, Distance, MDLP, and Chi2 as the ranges and accuracy \cite{bib4}. The possibility of utilizing multivariate discretization features may be investigated since parallel computers are becoming strong. Chi2 may delete redundant features, and Contrast or ID3 (dynamic discretization methods) may be addressed to integrate discretization into a learning process \cite{liu2002}.

\subsection{Forecasting}
Forecasting can be called predicting or estimating a value from the future \cite{armstrong2001}. Forecasting in a business-like SC performance is vital for suppliers that do forecast more than those that do not \cite{forslund2007}. Mainly three types of forecasting are done based on the length of the forecast: operational forecasting for short-term operational activities that range from hours to a few weeks, tactical forecasting for a moderate duration to support tactical planning that ranges from months to a few years, and strategic forecasting which is aligned with long-term goals to make strategic decisions \cite{lapide1999}. Furthermore, the frequency of a type of forecasting that is done is dependent on the length of the forecast. Long-term forecasts are rarely done, whereas operational forecasts may be required frequently. The different forecasts deal with different uncertainties. Long-term forecasts deal with raw material cost fluctuations, final product price changes, seasonal variations in demand, and changes in production rate in the long term. In contrast, short-term uncertainties are concerned with variations in daily processes, order cancellations, random failures in production, etc. \cite{gupta2003}. 

While forecasting is practical, forecasting correctly with more accuracy is even more helpful. Demand forecasting that allows anticipating sales in the forecasted period helps minimize overproduction and overstock \cite{chambers1971}. 

\subsubsection{Types}
Although forecasting techniques have evolved, forecasting techniques may be divided into three main categories: qualitative techniques, which deal with qualitative data or information to forecast; time series analysis and projection, which are related to historical data and patterns arising from them; causal models, where along with the historical data, special events and their relation with system elements are also considered \cite{chambers1971}. The qualitative technique is not much related to BD and data analytics; the other two are. Even so, qualitative data can be used to adjust forecasting models toward incredible accuracy. \cite{kuo2016} displayed one such example: Qualitative data can be used through fuzzy NNs combined with quantitative data for training the model. Nevertheless, accurate forecasts cannot be done based on qualitative data only. Time-series analysis is pretty straightforward, especially with the recent advancement of statistical tools. However, the role of such forecasts is to reduce errors in the forecast by minimizing the deviations at each point. Therefore, they do not consider special occasions such as promotions where sales are more remarkable than usual \cite{debaets2018}. This flaw brings us to causal models or models that consist of probabilities of forecasting accuracy, the effect of outside interventions, and the interrelation of different types of variables in the model \cite{hitchcock2023}. 

With the evolution of knowledge, different techniques for forecasting have emerged, and new classifications to understand them. \cite{buchatskaya2015} classified the different techniques into two broad groups of Intuitive and Formalized methods and divided Formalized methods further into Mathematical, System-structural, Associated, and Advanced information methods. 

\subsubsection{Model fit and train}
The dataset can be randomly split into the train, validation, and test sets for unbiased evaluation with new data to evaluate predictive performance with data different from training data. The best approach would be to split a dataset by a date feature. The most recent samples can be utilized for validation and testing. The primary concept is to choose a sample subset that accurately reflects the model data. 

Two factors determine the proportions of these three sets: the number of data samples and training models. Some models require significant training data; therefore, the model should be tuned for more extensive training sets in this scenario. Models with fewer hyperparameters will be easier to validate and tune, allowing a small validation set size. An extensive validation set will be beneficial if the model contains considerably more significant hyperparameters. There will be no requirement for a validation set if the model has no hyper-parameters or is challenging to adjust.

When using k-fold CV, the train-test dataset splitting is repeated for k-times, with each new set being given a shot at becoming the hold-out set. Time-series data cannot be used with k-fold CV directly since they believe that there lies no connection between the rows and that they are all separate instances. For time-series data, instances' time horizon prevents arbitrarily dividing them into clusters. Instead, data should be segmented, and the chronological sequence of instances maintained. The term backtesting is used in time-series forecasting to describe the technique of evaluating models using past data. In meteorology, this is regarded as 'hindcasting' rather than 'forecasting.'

\subsubsection{Hyperparameter tuning}
Optimizing performance requires tuning hyperparameters automatically by Automated ML (AutoML). Hyperparameters are available in most ML systems. Hyperparameter adjustment has the most influence in optimizing, regularising, and architecting NNs. Common use cases of Automatic Hyperparameter Optimization (HPO) include:
\begin{itemize}
    \item ML, specially AutoML, will require less manual effort
    \item ML algorithms' efficiency (by customizing them to the task at hand) has improved, resulting in the new high state-of-the-art for significant ML standards in research findings \cite{snoek2012}
    \item It enhances the opportunity of reproducing the ML process
    \item It allows the fair comparison of methods with the same type of tuning.
\end{itemize}
One issue with HPO is that a particular configuration does not work well for all datasets \cite{kohavi1995}. These days, optimizing hyperparameters above the default parameters supplied by standard ML packages is increasingly acknowledged.

To assess a lower-cost optimization model, the authors proposed Bayesian Optimization and Hyperband (BOHB) as an efficient, flexible, stable, and parallelizable default HPO technique \cite{feurer2019}. However, if all hyperparameters are valid and just a few function evaluations are available, the (Spearmint) Gaussian technique is recommended \cite{snoek2012}. To solve restricted optimization issues in vast areas, they suggested RandomForest-based Tree Parzen Estimator (TPE) or Sequential Model-based Algorithm Configuration (SMAC) and Covariance Matrix Adaptation - Evolution Strategy (CMA-ES). Genetic approaches were initially used for adjusting two hyperparameters of RBF-SVM $C$ and $\mu$ faster than GridSearch for better forecasting accuracy \cite{chen2004}. CMA-ES was initially utilized for the optimization of hyperparameters to optimize hyperparameters of SVM $C$ and $\alpha$, (for all input sizes) the kernel scale of length $l_i$, and the whole matrix of spin and scaling \cite{friedrichs2005}. CMA-ES has lately proved a perfect solution for Parallelized HPO, superior to current Bayesian heuristics while optimizing 19 deep-NN hyperparameters on parallel 30 GPUs \cite{loshchilov2016}. A Gaussian online approach incorporated EI to tune the SVM hyperparameters, attaining factor 100 (regression, three hyperparameters) and 10 (classification, two hyperparameters) speedups against GridSearch \cite{frohlich2005}.  A robust, adaptable, and analogous combination of Hyperband and Bayesian optimization was introduced that significantly surpassed both BlackBox and Hyperband optimization for a broad variety of issues, along with SVM adjustment, different types of NNs, and reinforced ML algorithms \cite{falkner2018}. As early as 2002, ancient ML models offered GridSearch for hyperparameter optimization \cite{john1994, michie1995}. PatternSearch and GDFS (Greedy Depth-First Search) were the first dynamic optimization techniques for HPO, with GDFS outperforming GridSearch. Particle Swarm Model Selection (PSMS) handles conditional configuration space with a customized particle swarm optimizer. Modified Ensembling was added to PSMS to prevent overfitting and integrate the better methods from many generations \cite{escalante2010}. In addition, to maximize pipeline architecture and solely utilize Particle Swarm Optimization for every pipeline hyperparameter, PSMS was modified to utilize a genetic optimization algorithm \cite{sun2013}. For the hyperparameter adjustment of deep neural, \cite{bergstra2011} utilized Bayesian optimization, outperforming random searching and manual. In addition, TPE generated better output than a Gaussian approach considering the mechanism. Random forest TPE and Bayesian optimization have also succeeded in searching for combined neural and HPO \cite{bergstra2013}. 
We suggest a unique manual approach that might be helpful in general cases:
\begin{itemize}
    \item If there are many hyperparameters, the CV score can be evaluated for the first hyperparameter. After that, such a hyperparameter value should be selected to avoid overfitting and lower accuracy. After setting that hyperparameter, the next hyperparameter should be evaluated by iterating a similar process one by one. The HPO algorithm should be chosen based on the hyperparameter type. 
    \item If there are less than or equal to two hyperparameters, the desired HPO approach can be used directly.
\end{itemize}


\subsubsection{Model evaluation}
In our framework, we suggest model fitting and training on our analysis-ready training data using default parameters and then move to the next step of tuning the hyperparameters. If the model accuracy deteriorates, it is not the feature's fault; instead, we should focus on the HPO of the models. After HPO, the top-performing models can be easily chosen based on the elimination process, but the model-overfitting issue should be considered. The model can be evaluated with the predicted sales against the actual sales after at least one month in the initial operating period. 

\subsubsection{Top forecasting models}
Table \ref{forecasting_models} provides a list of time-series demand forecasting models that have been used in our reviewed literature. Table \ref{ml--models} provides a comprehensive overview of the most recently proposed ML models in different forecasting applications and the corresponding performance metrics evaluated in each literature. Considering accuracy and precision in forecasting the future time-series lags, the ARIMA model outperformed the AR (AutoRegressive), MA (moving average), and SES (Simple Exponential Smoothing) models. The empirical research reported that long short-term memory (LSTM) enhanced forecasting by 85\% when evaluated by comparison to ARIMA (traditional-based model). Furthermore, the number of epochs (training times) did not influence the forecasting model’s performance, which showed genuinely random behavior \cite{siami2018}.

\begin{table*}[!ht]
  \centering
  \caption{List of Time-Series Demand Forecasting Models Used in Literature Review}
  \label{forecasting_models}
  \begin{tabular}{|l|l|}
    \hline
    \textbf{Model Name} & \textbf{Citation} \\
    \hline
    ARIMA & \cite{yue2007} \\
    \hline
    LSTM & \cite{yue2007}, \cite{li2023} \\
    \hline
    RBFNN & \cite{yue2007} \\
    \hline
    Winter models with SVM & \cite{yue2007} \\
    \hline
    Adaptive Network & \cite{efendigil2009} \\
    \hline
    Fuzzy reasoning strategy and ANN & \cite{efendigil2009} \\
    \hline
    GAD & \cite{li2018} \\
    \hline
    SHEnSVM & \cite{yue2010} \\
    \hline
    Deep--Learning (DL) technique & \cite{kilimci2019} \\
    \hline
    SVR & \cite{ribeiro2022} \\
    \hline
    RF & \cite{ribeiro2022}, \cite{mitra2022} \\
    \hline
    XGBoost & \cite{gurnani2017}, \cite{ribeiro2022} \\
    \hline
    Ada Boost & \cite{shukla2022}, \cite{mitra2022} \\
    \hline
    Random Forest & \cite{shukla2022}, \cite{mitra2022}, \cite{hamdan2023} \\
    \hline
    MLP & \cite{shukla2022}, \cite{zohdi2022} \\
    \hline
    CNN--LSTM & \cite{nithin2022}, \cite{sandhya2022} \\
    \hline
    GRU & \cite{hu2023} \\
    \hline
    EGD--SNet & \cite{mehmood2022} \\
    \hline
    LSTM, BiLSTM, GRU & \cite{bassiouni2023} \\
    \hline
    Temporal Convolutional Network & \cite{bassiouni2023} \\
    \hline
    Swish Activation & \cite{nithin2022} \\
    \hline
    Shallow NN & \cite{rathipriya2023} \\
    \hline
    DNN & \cite{rathipriya2023} \\
    \hline
    Extreme Learning Machine (ELM) & \cite{chaudhuri2022} \\
    \hline
    Adaptive Neuro--Fuzzy Inference System (ANFIS) & \cite{hamdan2023} \\
    \hline
    SARIMAX & \cite{hamdan2023} \\
    \hline
    Prophet & \cite{hamdan2023}, \cite{mitra2022} \\
    \hline
    RF--XGBoost--LR & \cite{mitra2022} \\
    \hline
    RNN & \cite{sandhya2022} \\
    \hline
    GRU & \cite{sandhya2022} \\
    \hline
    XGBoost--LSTM & \cite{wei2021} \\
    \hline
    FB--Prophet & \cite{jha2021} \\
    \hline
    XGBoost--LightGBM & \cite{yang2021} \\
    \hline
    M--GAN--XGBoost & \cite{wang2021} \\
    \hline
    AUG--NN & \cite{javeri2021} \\
    \hline
  \end{tabular}
\end{table*}


For the demand forecasting procedure, \cite{yue2007} evaluated statistical models, RBFNN (Radial Basis Function NNs), and winter models with SVM. According to their conclusion, the efficiency of SVM outperforms other algorithms by about a mean MAPE outcomes threshold of 7.7\%. \cite{efendigil2009} demonstrated a new AI-utilized forecasting approach evaluating a fuzzy reasoning strategy and ANN based on the Adaptive Network to handle the demand containing inadequate knowledge. During testing, they obtained MAPE: 18\% on average for some products. For unpredictable customer demands, neural methods have provided a robust forecasting strategy in a multi-level SC framework. A Greedy Aggregation Decomposition (GAD) approach is a generic approach to self-development in a discontinuous time-series forecasting method that considers double-based causes of variation, addressing a practical discontinuous issue of forecasting demand \cite{li2018}. With a limited dataset, they outperformed SBA, Croston’s method, TSB, MA-7, SES, MAPA, MA-3, ADIDA, iADIDA, N-7 with a MAPE accuracy rate of 5.9\%. \cite{yue2010} offered the SHEnSVM (Selective \& Heterogeneous Ensemble of SVMs) model for sales forecasts. Individual SVMs were trained using samples produced by the bootstrap method, and grid search created parameters, as stated by the specified model. The optimum specific combo strategy was found using a genetic algorithm. They claimed a 10\% increment using the SVM algorithm \& a 64\% on average enhancement in MAPE. The authors used beer data from three product variants in their tests. \cite{kilimci2019} integrated Deep-Learning (DL) technique, the SVR algorithm, and best-performing time-series analytic models utilizing the boosting ensemble approach to demand forecasting systems. Their DL implementation in the new integration strategy (MAPE: 24.7\%) lowers mean forecasting error in the SC, outperforming both the conventional best-performing forecasting model (MAPE: 42.4\%) and the unique integration strategy without DL (MAPE: 25.8\%). XGBoost, ARIMA, and Snaive STL decomposition have outperformed solo and hybrid models and the modeling mix and have provided the best forecasting accuracy \cite{gurnani2017}.

\cite{li2023} used Facebook Prophet (FB-Prophet) and ANNs to forecast lithium mineral resource prices in China. Quality and quantity of lithium data, network architecture, and activation functions significantly impacted the performance of an ANN forecasting model. Overfitting can occur when an ANN model is too closely tailored to the training dataset, and regularization and early halting strategies can enhance the model's performance. The FB-Prophet model, which uses a decomposable time-series model, can effectively forecast data with fewer value matrices, handle missing values, and practice adjustments. \cite{hu2023} created recurrent NN (RNN), LSTM, and gated recurrent unit (GRU) models to forecast the demand for U.S. influenza vaccinations, with data from 1980 to 2011 serving as the training set and data from 2012 to 2020 serving as the testing set. The prediction models may be scalable because there was no overfitting between the expected and actual numbers. The error comparisons demonstrated that GRU is more precise than LSTM and RNN in predicting vaccination demand. Energy Generation and Demand Forecasting Search Net (EGD-SNet), a framework that can anticipate energy production, demand, and temperature across various areas, was reported in the study of \cite{mehmood2022}. Together with the 

10 most popular ML regressors, the EGD-SNet framework includes 11 dimensionality reduction techniques and 13 alternative feature selection algorithms. By locating the best hyperparameters, it employs Particle Swarm Optimizer (PSO) to train regressors intelligently. Also, it can create an end-to-end pipeline by selecting the right regressor, feature, and dimensionality reduction methodologies to accurately anticipate energy generation or demand for a specific geographical data set, depending on the features of the data. \cite{bassiouni2023} implemented many DL methods, including data collecting, de-noising or pre-processing, feature extraction, and classification stages. Two primary DL models determine feature extraction. The first variation used three RNN structures: LSTM, BiLSTM, and GRU. The second variant used the temporal convolutional network (TCN). They used SoftMax, RT, RF, KNN, ANN, and SVM classifiers for an online dataset. TCN predicts COVID-19-restricted shipping risk almost 100\% accurately.

As the daily fish demand forecasting models for grocery merchants to reduce food waste and enhance sustainable SCs, \cite{migueis2022} investigated LSTM, Feedforward NNs, Support Vector Regression, RF, and a Holt-Winters statistical model. The findings showed that the LSTM model provided the best outcomes in terms of root mean squared error (27.82), mean absolute error (20.63), and mean positive error (17.86). \cite{haider2022} forecasted solar Global Horizontal Irradiance using statistical and Deep Learning architectures, which aids grid management and power distribution and highlights Pakistan's solar power potential in addressing global climate change. They employed SARIMAX, Prophet, LSTM, convolutional NN (CNN), and ANN statistical approaches. Error measures like $R^2$, MAE, MSE, and RMSE were used to evaluate each model's performance. They concluded that SARIMAX and Prophet are ideal for long-term forecasts, whereas ANN, CNN, and LSTM are best for short-term forecasts. \cite{shukla2022} found that the optimum model for every participant in the supply chain across all three inventory replenishment strategies is a stacked ensemble model consisting of XG Boost, Ada Boost, and Random Forest. According to a methodology for comparing forecasting methods developed by \cite{singha2022}, the MLP method has a little edge over the CNN, LSTM, and CNN-LSTM approaches. \cite{vithitsoontorn2022} utilized data from the last five years to estimate demand for eight dairy products from five dairy production facilities using a direct multistep prediction method. ARIMA works effectively on a narrow subset of unpredictable series, whereas LSTMs excel at anticipating seasonal patterns. It outperforms ARIMA for trends. Monthly data decreased model training error.

For the purpose of forecasting daily energy use, \cite{ribeiro2022} investigated the effectiveness of three machine learning models, SVR, RF, and XGBoost; three deep learning models, RNNs, LSTM, and GRU; and ARIMA. For both very short-term load forecasting (VSTLF) and short-term load forecasting (STLF), the suggested XGBoost models beat competing models; the ARIMA model did the worst. \cite{elmir2023} presented a smart platform for data-driven blood bank management that forecasts blood demand and balances blood collection and distribution based on optimal blood inventory management to avoid blood wastage and shortage. This improves blood quality and quantity, increasing blood collection by 11\% and reducing blood waste by 20\%. Balancing blood collection and distribution based on good blood inventory management and arranging blood donation sessions to avoid cancellations may lower inventory levels. \cite{nithin2022} proposed a CNN-LSTM model with Swish Activation to estimate a store's supply based on prior sale. This outperforms Rectified Linear Unit, the most effective activation function (ReLU). They forecasted sales using Multilayer Perceptron, LSTM cells, and CNNs. CNN-LSTM Model has a reduced RMSE, according to the experiment. Pharmaceutical businesses can use Shallow NN and DNN demand forecasting models for eight anatomical treatment chemical thematic drug groups \cite{rathipriya2023}. Shallow NN models performed well for five of eight medication categories, while the ARIMA model performed best for the other three.

\cite{chaudhuri2022} introduced an extreme learning machine (ELM) model using the Harris Hawks optimization (HHO) method to estimate e-commerce product demand. In forecasting product demand for the next three months, the ELM-HHO model outperformed the statistical ARIMA (7,1,0) model by 62.73\%, the NN-based GRU model by 40.73\%, the LSTM model by 34.05\%, the traditional non-optimized ELM model with 100 hidden nodes by 27.16\%, and the ELM-BO model by 11.63\%. \cite{hamdan2023} developed a novel ML forecasting approach by merging adaptive neuro-fuzzy inference system (ANFIS) and time-series data features to forecast real-time e-order arrivals in distribution hubs, helping third-party logistics providers better manage hourly-based e-order arrival rates. ELM, GB, KNN, MLP, and DT were five ML algorithms used by \cite{zohdi2022} to forecast demand in a business based on Black Friday customer information. According to the results, MLP, ELM, GB, KNN, and DT were the top algorithms in terms of MSE, while ELM, MLP, GB, DT, and KNN had the greatest performances in terms of MAE. Moreover, ELM had a higher $R^2$ value of 0.6365, whereas DT had a lower value (0.4877). \cite{mitra2022} compared RF, XGBoost, gradient boosting, AdaBoost, and ANN algorithms to a hybrid (RF-XGBoost-LR) model for retail chain sales forecasting. A US retail company's weekly sales data was used to analyze estimates based on factors like temperature and shop size. The hybrid RF-XGBoost-LR outperformed other models in many criteria. RNN and LSTM were used by \cite{sandhya2022} to improve stock price prediction. The memory cell, a computer that replaces artificial neurons, is buried in the network. The study increased epochs and load sizes to improve precision. Time and lot size boost prediction accuracy in this work. The test data predicts the specified technique, which yields more accurate outcomes. The proposed method forecasts stock markets more accurately.

\begin{table*}[!ht]

\caption{The Most Recent (2022--2023) ML Models for Forecasting Applications in SCM}
\label{ml--models}
\centering
\begin{tabular}{|p{2cm}|p{4cm}|p{3cm}|p{4cm}|}
\hline
\textbf{Study} & \textbf{ML Models} & \textbf{Performance Metrics} & \textbf{Application/Domain} \\
\hline
\cite{bassiouni2023} & EGD--SNet Framework: 10 ML regressors & Accuracy, RMSE, MAE & Energy generation and demand forecasting \\
\hline
\cite{migueis2022} & LSTM, Feedforward NNs, Support Vector Regression, RF, Holt--Winters model & RMSE, MAE, Mean positive error & Daily fish demand forecasting for grocery merchants \\
\hline
\cite{haider2022} & SARIMAX, Prophet, LSTM, CNN, ANN & $R^2$, MAE, MSE, RMSE & Solar Global Horizontal Irradiance forecasting \\
\hline
\cite{shukla2022} & Stacked ensemble model (XG Boost, Ada Boost, Random Forest) & Not specified & Supply chain inventory replenishment \\
\hline
\cite{singha2022} & MLP, CNN, LSTM, CNN--LSTM & Not specified & Forecasting method comparison \\
\hline
\cite{vithitsoontorn2022} & ARIMA, LSTM & Not specified & Dairy product demand forecasting \\
\hline
\cite{ribeiro2022} & SVR, RF, XGBoost, RNNs, LSTM, GRU, ARIMA & Not specified & Daily energy use forecasting \\
\hline
\cite{elmir2023} & Smart platform utilizing ML models & Not specified & Blood bank management and demand forecasting \\
\hline
\cite{nithin2022} & CNN--LSTM model with Swish Activation & RMSE & Store supply forecasting based on prior sale \\
\hline
\cite{rathipriya2023} & Shallow NN, DNN, ARIMA & Not specified & Pharmaceutical demand forecasting \\
\hline
\cite{chaudhuri2022} & Extreme Learning Machine (ELM) with Harris Hawks optimization (HHO) & Not specified & E--commerce product demand forecasting \\
\hline
\cite{hamdan2023} & Adaptive Neuro--Fuzzy Inference System (ANFIS) & Not specified & Real--time e--order arrivals forecasting \\
\hline
\cite{zohdi2022} & ELM, GB, KNN, MLP, DT & MSE, MAE, $R^2$& Business demand forecasting based on Black Friday customer information \\
\hline
\cite{mitra2022} & RF, XGBoost, Gradient boosting, AdaBoost, ANN, Hybrid RF--XGBoost--LR & Not specified & Retail chain sales forecasting \\
\hline
\cite{sandhya2022} & RNN, LSTM & Not specified & Stock price prediction \\
\hline
\end{tabular}

\end{table*}


Based on the mentioned studies, we suggest considering the following recently best-performing hybrid time-series demand forecasting ML models: 1. XGBoost-LSTM \cite{wei2021} 2. FB-Prophet \cite{jha2021} 3. XGBoost-LightGBM \cite{yang2021} 4. M-GAN-XGBoost \cite{wang2021} 5. SARIMA, integrated AttConvLSTM, and FB-Prophet \cite{wan2021} 6. AUG-NN \cite{javeri2021}.

When selecting the primary top-forecasting model, it is recommended to consider the best cross-validation (CV) score, minimum runtime, and space consumption as criteria for evaluation.


\section{Control-process}
Actual processes do not always go as predicted. There are variabilities in performances emerging from changing levels of efficiency. In ideal cases, the workforce and existing capacity can achieve the goal as planned. However, performance levels are inconsistent with humans \cite{rabbitt2001} and vary with other factors such as WIP inventory, machine utilization, product mix, and queueing system \cite{wu2005}.  Such fluctuations in efficiency cannot be predicted accurately. Hence, they must be recognized in time, and appropriate measures to meet the requirements must be taken. Responsive to the SC process's randomness may consist of three steps: capturing or recording data simultaneously with the SC activity, comparing the recorded data with the standard, and adjusting capacity to meet the short-term goal. Data can be used to determine the optimal decision for the changing suppliers, changing price levels, the competitiveness of the competitors, and monitoring performance during the process \cite{elgendy2014}. Furthermore, during a discrepancy in planned and actual output levels, the root cause can be identified using BDA \cite{russom2011}. \cite{bagshaw2017} mentioned other benefits of BD on workforce scheduling, production efficiency, employee productivity, capacity utilization, flexibility, and lead time reduction.

\subsection{Information flow}
Forecasting decisions affect further SC planning. As such, information flows across multiple phases of the SC process. Logistics superiority and better stock level synchronization are possible through a flow of demand information from downstream members to the upstream ones and the flow of production plan and delivery information from the upstream members to the downstream ones \cite{vanpoucke2009}. Like the three types of forecasting, there are three types of decisions in SC, strategic, operational, and tactical \cite{strack2010}. Recent studies have shown how the findings from one level can affect other decisions and limit the number of options for subsequent decisions \cite{berg1999}.

Through forecasting, it is possible to attain efficiency by properly allocating resources in different areas such as workforce, capacity, inventory management, etc. There are need-based, supply-based, and demand-based models for forecasting and planning in such areas of SCM \cite{safarishahrbijari2018}. Each method requires some sort of information flow. The forecasted amount can be used to estimate the number of dependent inventory demands. The final product’s demand indirectly affects the required workforce, capacity, warehouse planning, and lower costs through optimization.

\subsection{Production efficiency}
Having real-time data on production boosts production efficiency. Firms can manage order processing across SCs and companies while decreasing errors and waste inside manufacturing facilities by incorporating real-time data into SC operations \cite{waller2013}. This efficiency is further enhanced when data from suppliers and distributors are available. Through close connections and sharing information with SC partners, data-driven SCs may also affect manufacturing and operations processes through increased efficiency in product development, product design, quality improvement, and balance between capacity and demand \cite{sanders2016}. Additionally, data integration in the SC has been found to aid in developing production strategies and the timely delivery of products and services \cite{droge2012}.

\subsection{Employee productivity}
In general, there is either an excess of the workforce or a shortage in the production process; the question is how to reduce the inefficiency. Under variable output requirements, workforce scheduling without data analysis entails investing in cross-job training to enable workers to be more productive and efficient in their work. However, this reduces performance as time is spent on upskilling or reskilling.

Data-driven decision-making, or the forecasting of required outputs to estimate the required workforce, is an excellent way to minimize such costs of hiring and laying-off by adequately scheduling the workforce \cite{bagshaw2017}. Through proper scheduling, the workforce from idle time can be shifted for workdays requiring extra hours and thereby balanced. \cite{noack2008} showed that work pressure could be balanced with reduced slack time and workforce through different heuristic algorithms, with each algorithm performing well in different areas of efficiency. 

\subsection{Inventory management}
Reducing costs from inventory can cut the overall cost of the business. Different models have been created to minimize costs and maximize profits that aid with material planning mechanisms, stock-out predictions, inventory level predictions, and many more \cite{hajek2020}. Inventory costs can be lowered at sourcing, transportation, and holding levels, optimizing inventory decisions \cite{sanders2016}.

\subsection{Role of data by time frame}
Although BDA can make SC processes efficient, it cannot be done with the same forecast. Capacity planning or storage size falls under long-term strategic decisions requiring long-term forecasts or aggregated short-term forecasts. Contrarily, production plans may be short-term operational decisions requiring short-term predictions. \cite{andrawis2011} stated how predictions could be derived from the aggregation of short-term, disaggregation of long-term, or a co-integration of both kinds of forecasts. Hence, on the one hand, separate forecasts can be produced. Conversely, forecasts can be derived from other forecasts to maintain relevance.

\section{Post-process}
\subsection{Supply chain performance}
Once an SC process is completed, the performance needs to be reviewed to identify the gaps in planning models. Performance measurement is defined as quantifying actions across two fundamental dimensions: effectiveness and efficiency \cite{neely1995}. Performance measurement is essential to control the output; without it, no person or machine can be held liable for subpar performance, and problems will be harder to identify and solve. Performance measurement helps with information for management feedback, decision-making, monitoring performance, diagnosing problems, motivating people, identifying potentials of a decision, measuring success or failure, reviewing and adjusting business strategies, specifying company goals, and much more \cite{chan2003}. \cite{ho2007} offered a comprehensive methodology considering all three SC system stages, including ERP-based SC performance. To comprehend whether network scanning and embeddedness are linked to SC performance, Bernardes and Zsidisin (2008, 209) studied the correlation between SCM strategy and network scanning and embeddedness concepts.

Immediately after the tasks are completed, the performance data must be recorded. For data collection, the performance metrics are first to be identified, just as those found for business evaluations \cite{bittencourt2005}. \cite{gunasekaran2001} mentioned plan success, source optimization, production efficiency, delivery performance, and customer support-relation and satisfaction, each having multiple performance metrics under them. The performance level found afterward can be of three types below average, average, and above-average \cite{asrol2021}. The actions followed after such a finding is different in each case. When a below-average performance is observed, managers can either look for anomalies in the system or review whether the goals set were too high to achieve.

Conversely, an above-average performance requires rechecking the goals so that optimization of resources is possible. In order to meet the goals-setting theory stated by \cite{locke2006}, these changes may be adjusted to suit. Operational benefits such as performance monitoring, objective setting, management, transparency, and planning functions can be improved with the assistance of BDA and performance metrics derived from them through the use of predictive KPIs, dashboards, and scorecards by the SC operational managers within the organization \cite{elgendy2014}.

Besides managerial decision-making, the performance data are crucial to modifying existing forecasting models. Under a considerable deviation of performance, the data received from this level needs to be sent back to the forecasting stage for tweaking the forecasting model to higher perfection. The performance metrics can thus act as an indicator of forecasting model errors. 

\subsection{Forecasting error measurement}
Our proposed cyclic framework is evaluated against predicted sales when the actual sales data is available or using the hold-out set. Nevertheless, the hold-out set might not be perfect for real-world scenarios, so we encourage a cyclic and continuous development process from real-sales data evaluation insights. We encourage a cyclic and continuous development process from real-sales data evaluation insights. A few evaluation metrics can be used for the post-process evaluation to fine-tune the forecasting model in the preprocessing phase. Assume test data with m periods, $t=1,\ldots,m$. The difference between forecasted sales $f_t$ and actual sales $y_t$ at a period $t$ can be referred to as the forecasting error $e_t=y_t-f_t$.

\subsubsection{Mean Absolute Error}
\begin{equation}
\small
    MAE = \frac{1}{m} \sum_{t=1}^m |e_t | = mean(|e_t |)
\end{equation}

MAE is very straightforward and relatively simple to explain, and scale dependence is its disadvantage.


\subsubsection{Mean Absolute Percentage Error}
\begin{equation}
\small
	 MAPE = \frac{1}{m} \sum_{t=1}^m|\frac{e_t}{y_t}\times100\\%
\end{equation}

MAPE is perhaps the most often utilized error indicator for business forecasting because of its comprehensiveness. However, despite the term ‘Percentage,’ the MAPE value may be higher than 100\\%. The rows equal to 0 causes problems since the fraction’s denominator cannot be filled in. MAPE is an appropriate metric when dealing with intermittent demand. Asymmetry is its major drawback as it penalizes overfitting more than underfitting, leading to probable skewness.


\subsubsection{Mean Squared Error}
\begin{equation}
\small
	MSE = \frac{1}{m} \sum_{t=1}^m {e_t}^2
\end{equation}

Compared to RMSE, MSE takes less runtime and is more flexible. However, we might not interpret MSE as the actual sales because the error is squared.


\subsubsection{Root Mean Squared Error}
\begin{equation}
\small
	 RMSE=\sqrt{\frac{1}{m} \sum_{t=1}^m{e_t}^2} =\sqrt{mean(|{e_t}^2|)}
\end{equation}

Two consequences occur by performing the dual transformation in RMSE: more weight is placed on more significant errors, and positive and negative errors cannot cancel one another out since they are all transformed into positives. 


\subsubsection{Mean Absolute Scaled Error}
\begin{equation}
\small
	MASE=\frac{\frac{1}{J} \sum_{j}|e_j|}{\frac{1}{T-1}\sum_{t=2}^T|y_t-y_(t-1)|}\text{, for non-seasonal time-series}
\end{equation}

\begin{equation}
\small
	MASE=\frac{\frac{1}{J} \sum_{j}|e_j|}{\frac{1}{T-m}\sum_{t=m+1}^T|y_t-y_(t-m)|}\text{, for seasonal time-series}
\end{equation}

With MAE, outliers are protected; with RMSE, we are assured of an impartial prediction. SC Analysts need to analyze MAE and see whether it results in a significant bias, and therefore they should utilize RMSE. In situations when there are many outliers in the dataset, MAE may help correct the skewed prediction. 


\subsubsection{Tracking Signal}
The tracking signal is the way to verify if the forecasting method currently in use is the correct one. A tracking signal that changes according to the forecast bias shows bias in the prediction model. It is often employed when the forecasting model’s validity is questionable.
\begin{equation}
\small
	\text{Algebraic sum of forecast error} = \sum_{t=1}^m|e_t|
\end{equation}
\begin{equation}
	\text{Tracking Signal} = \frac{\text{Algebraic sum of forecast error}}{\text{Mean Absolute Error}}
\end{equation}

A rule of thumb holds that the technique employed for forecasting is accurate when the tracking signal is within $-4$ to $+4$.


\subsection{Phantom inventory}
Research has shown that imprecise perpetual inventories (PIs) are overestimated approximately 50\% of the time; that is, PI displays higher stock than that is present in the shop, called a phantom inventory. The most severe issue in a phantom inventory is unavailability – the system considers it has an adequate inventory and does not order a replenishment. The recognized reasons for phantom inventory are \cite{waller2006}:
\begin{itemize}
    \item Stolen goods, defective products that are not reported
    \item Cashier mistakes
    \item A shop may get deliveries from the distribution center (products that should have but were not received)
    \item Returned goods that should update the system are sometimes wrong.
\end{itemize}

To resolve the stock inconsistency, businesses may perform a bunch of tasks \cite{kang2005}:
\begin{itemize}
    \item The supply of safety may be raised. The enhanced security inventory aims to mitigate inventory problems by having ‘excess’ inventories at hand. RFID may allow the costs of storing this additional and redundant inventory to be reduced.
    \item The business may often conduct manual inventory numbers. Physical inventory audits may interrupt storage, are extremely expensive, and differ in precision – improved RFID precision may be an affordable option.
    \item The business may construct a continuous decrease equivalent to the total inventory loss that one believes takes place to balance the phantom inventory. The issue is that the precise inventory loss is not known. The visibility provided by RFID may be more accurate than conventional stock loss techniques.
    \item The business may attempt to minimize mistakes by improving inventory management, decreasing fraud, etc. 
\end{itemize}

Inventory precision is a determining factor for forecasting, procurement, and replenishment quality, where inventory records are used as input. Inaccurate demand forecasting due to phantom inventory (overstated PI) may be improved by including RFID in the process \cite{hardgrave2009}.



\section{Challenges}
This section aims to provide a comprehensive overview of the challenges encountered during the review of 152 articles from 1969 to 2023 in the field of BDA-SCM Framework for Forecasting, with a specific focus on data preprocessing and ML techniques. The challenges identified herein will serve as a valuable resource for future researchers, enabling them to address and overcome these obstacles, ultimately advancing the domain and contributing to its growth and development.
\subsubsection{Data Quality and Reliability}
One of the critical challenges observed in the reviewed literature is the issue of data quality and reliability. Many studies acknowledged the presence of incomplete, inconsistent, and erroneous data within supply chain datasets. Future research efforts should focus on developing robust data cleansing, integration, and quality assurance techniques to enhance the reliability and accuracy of the forecasting models.
\subsubsection{Scalability and Performance}
With the exponential growth of data in SCM, scalability and performance have become significant challenges. The reviewed articles often lacked details on how their proposed techniques would scale up to handle large-scale datasets or real-time processing requirements. Future researchers should explore scalable algorithms, distributed computing frameworks, and parallel processing techniques to ensure the effectiveness and efficiency of forecasting models.
\subsubsection{Variety and Complexity of Data Sources}
The diverse range of data sources, such as structured, unstructured, and semi-structured data, presents challenges in data preprocessing and feature extraction. The reviewed literature indicated limited exploration of techniques for effectively handling data variety and complexity. Future research should focus on developing innovative methods for integrating and analyzing heterogeneous data sources to extract meaningful insights for accurate forecasting.
\subsubsection{Feature Engineering and Selection}
Effective FE and FS play a crucial role in the performance of ML models. However, the surveyed literature lacked comprehensive approaches to identify the most relevant features for forecasting within the supply chain context. Future researchers should investigate advanced FE techniques, including automated FS, dimensionality reduction, and feature representation, to improve forecasting accuracy.
\subsubsection{Model Interpretability and Explainability}
The black-box nature of some ML models limits their interpretability and hampers decision-making processes. The surveyed literature revealed a lack of emphasis on model interpretability, hindering the wider adoption of forecasting techniques in SCM. Future research should focus on developing transparent and interpretable models that provide explanations for their predictions, enabling practitioners to understand and trust the results.
\subsubsection{Real-Time Data Processing and Analysis}
SCM requires real-time monitoring and decision-making capabilities. However, the surveyed literature demonstrated a limited exploration of real-time data processing and analysis techniques for forecasting purposes. Future research efforts should concentrate on developing real-time forecasting frameworks that leverage stream processing, online learning, and adaptive algorithms to handle dynamic and time-sensitive supply chain scenarios.
\subsubsection{Privacy and Security Concerns}
The integration of big data in SCM raises concerns regarding data privacy and security. The surveyed articles paid limited attention to these challenges, and there is a lack of comprehensive approaches to ensure the privacy and security of sensitive supply chain data. Future researchers should focus on developing robust privacy-preserving and secure ML techniques to safeguard data while maintaining the accuracy and efficiency of forecasting models.
\subsubsection{Integration of Domain Knowledge}
SCM involves complex domain-specific knowledge, including industry-specific constraints, regulations, and contextual factors. The reviewed literature showed a limited integration of such domain knowledge into the forecasting frameworks. Future research should emphasize the incorporation of domain expertise and contextual information to enhance the relevance and accuracy of forecasting models within the supply chain domain.
\subsubsection{Lack of Benchmark Datasets and Evaluation Metrics}
The absence of standardized benchmark datasets and evaluation metrics hinders the comparison and reproducibility of forecasting techniques. The reviewed articles often utilized different datasets and evaluation metrics, making it challenging to assess the performance of various models. Future researchers should strive to establish benchmark datasets and evaluation protocols specific to supply chain forecasting, enabling fair comparisons and facilitating advancements in the field.

By overcoming these challenges through innovative techniques and methodologies, researchers can contribute to the advancement of this field, leading to more accurate, scalable, and interpretable forecasting models for SCM.


\section{Conclusions}
This study has successfully addressed the challenges of SC forecasting and presented cutting-edge technological solutions to overcome them. It has introduced a comprehensive BDA-SCM framework encompassing three essential SC processes: Pre-Process, Control-Process, and Post-Process. The key findings and contributions of this study can be summarized as follows:
\begin{enumerate}
    \item Pre-Process: In the pre-processing stage of SC forecasting, the significance of accurate data aligned with SC objectives was emphasized. The study provided recommendations for SC analysts, including the utilization of exploratory data analysis, FE, hyperparameter tuning, and recent ML model training approaches to improve forecasting accuracy. However, it is essential to note that further research is needed to explore advanced techniques for data cleansing, integration, and quality assurance to ensure reliable and high-quality input data.
    \item Control-Process: The study discussed how BD could facilitate efficient managerial decision-making in various areas of SCM, such as production and capacity planning, workforce requirements, and inventory management. Leveraging insights from forecasted data allows decision-makers to optimize SC operations and resource allocation. However, future research should focus on developing real-time decision support systems that can integrate and analyze large-scale data streams to enable timely and effective decision-making.
    \item Post-Process: The post-process section emphasized SC performance measurement and the role of BDA in optimizing model predictions. By analyzing performance metrics and leveraging BDA techniques, SC practitioners can identify areas for improvement and refine their forecasting models accordingly. Future research efforts should focus on developing comprehensive performance measurement frameworks specific to SC forecasting, including quantitative and qualitative metrics, on enabling more accurate evaluation and comparison of forecasting models.

    The accuracy of inventory records is a crucial determinant for forecasting, procurement, and replenishment quality, as these records serve as inputs for these processes. Inaccurate demand forecasting resulting from phantom inventory (overstated PI) can be mitigated through the inclusion of RFID technology in the inventory management process, as evidenced by previous studies. Future research in this area should explore advanced techniques and methodologies for addressing phantom inventory, incorporating emerging technologies, and developing comprehensive frameworks for inventory management and forecasting to enhance the overall performance of supply chain operations.

    
\end{enumerate}

This study has successfully addressed the research questions posed:

RQ1: The study has identified and outlined efficient steps to formulate a machine learning (ML) forecasting model for predicting supply chain (SC) factors. Recommendations for accurate data preprocessing, feature engineering, hyperparameter tuning, and advanced ML model training approaches have been provided to enhance the accuracy of SC forecasting models.

RQ2: The study has emphasized the importance of connecting, tracking, and optimizing the forecasting, SC decision-making, and performance measurement processes in a cyclic order. The proposed BDA-SCM framework encompasses the Pre-Process, Control-Process, and Post-Process stages, providing guidance on integrating these processes to optimize SC operations and resource allocation.

RQ3: The study has explored the impact of forecasting on SC performance and identified relevant ML forecasting models for SC forecasting. The connection between accurate forecasting and improved SC performance has been highlighted, with recommendations for performance measurement and the utilization of BDA techniques to optimize model predictions.

By successfully addressing these research questions, this study contributes to the advancement of the field by providing insights into efficient ML modeling steps, the integration of forecasting and SC decision-making processes, and the relevance of ML forecasting models for SC forecasting. Future research should build upon these findings to further enhance the understanding and implementation of BDA in SCM.

This SLR followed a rigorous and objective evaluation approach, utilizing predefined criteria and conducting a comprehensive search to ensure a comprehensive understanding of the BDA-SCM domain. However, it is crucial to acknowledge the limitations of this study. The limitations include the availability of relevant literature, potential publication bias, and the dynamic nature of the BDA-SCM field. Future research should aim to address these gaps by conducting further empirical studies, developing benchmark datasets, and exploring emerging technologies and methodologies to advance the understanding and implementation of BDA in SCM.

By considering and addressing the aforementioned challenges and limitations, future researchers can build upon the findings of this study and contribute to the advancement of the BDA-SCM domain.


\begin{thebibliography}{00}

\bibitem{thatte2013}
A. A. Thatte, S. S. Rao, and T. S. Ragu-Nathan, ``Impact Of SCM Practices Of A Firm On Supply Chain Responsiveness And Competitive Advantage Of A Firm,'' \textit{JABR}, vol. 29, no. 2, Art. no. 2, Feb. 2013. doi: \href{https://doi.org/10.19030/jabr.v29i2.7653}{10.19030/jabr.v29i2.7653}.

\bibitem{arauzoazofra2011}
A. Arauzo-Azofra, J. L. Aznarte, and J. M. Benítez, ``Empirical study of feature selection methods based on individual feature evaluation for classification problems,'' \textit{Expert Systems with Applications}, vol. 38, no. 7, pp. 8170–8177, Jul. 2011. doi: \href{https://doi.org/10.1016/j.eswa.2010.12.160}{10.1016/j.eswa.2010.12.160}.

\bibitem{monge1996}
A. E. Monge and C. P. Elkan, ``The field matching problem: Algorithms and applications,'' in \textit{Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, in KDD’96}. Portland, Oregon: AAAI Press, Aug. 1996, pp. 267–270.

\bibitem{farhangfar2007}
A. Farhangfar, L. A. Kurgan, and W. Pedrycz, ``A Novel Framework for Imputation of Missing Values in Databases,'' \textit{IEEE Trans. Syst., Man, Cybern. A}, vol. 37, no. 5, pp. 692–709, Sep. 2007. doi: \href{https://doi.org/10.1109/TSMCA.2007.902631}{10.1109/TSMCA.2007.902631}.

\bibitem{gandomi2015}
A. Gandomi and M. Haider, ``Beyond the hype: Big data concepts, methods, and analytics,'' \textit{International Journal of Information Management}, vol. 35, no. 2, pp. 137–144, Apr. 2015. doi: \href{https://doi.org/10.1016/j.ijinfomgt.2014.10.007}{10.1016/j.ijinfomgt.2014.10.007}.

\bibitem{gunasekaran2001}
A. Gunasekaran, C. Patel, and E. Tirtiroglu, ``Performance measures and metrics in a supply chain environment,'' \textit{International Journal of Operations \& Production Management}, vol. 21, no. 1/2, pp. 71–87, Jan. 2001. doi: \href{https://doi.org/10.1108/01443570110358468}{10.1108/01443570110358468}.

\bibitem{gupta2003}
A. Gupta and C. D. Maranas, ``Managing demand uncertainty in supply chain planning,'' \textit{Computers \& Chemical Engineering}, vol. 27, no. 8–9, pp. 1219–1227, Sep. 2003. doi: \href{https://doi.org/10.1016/S0098-1354(03)00048-6}{10.1016/S0098-1354(03)00048-6}.

\bibitem{elmagarmid2007}
A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios, ``Duplicate Record Detection: A Survey,'' \textit{IEEE Trans. Knowl. Data Eng.}, vol. 19, no. 1, pp. 1–16, Jan. 2007. doi: \href{https://doi.org/10.1109/TKDE.2007.250581}{10.1109/TKDE.2007.250581}.

\bibitem{kalousis2007}
A. Kalousis, J. Prados, and M. Hilario, ``Stability of feature selection algorithms: a study on high-dimensional spaces,'' \textit{Knowl Inf Syst}, vol. 12, no. 1, pp. 95–116, May 2007. doi: \href{https://doi.org/10.1007/s10115-006-0040-8}{10.1007/s10115-006-0040-8}.

\bibitem{ribeiro2022}
A. M. N. C. Ribeiro, P. R. X. Do Carmo, P. T. Endo, P. Rosati, and T. Lynn, ``Short-and Very Short-Term Firm-Level Load Forecasting for Warehouses: A Comparison of Machine Learning and Deep Learning Models,'' \textit{Energies}, vol. 15, no. 3, p. 750, 2022, doi: \href{https://doi.org/10.3390/en15030750}{10.3390/en15030750}.

\bibitem{neely1995}
A. Neely, M. Gregory, and K. Platts, ``Performance measurement system design: A literature review and research agenda,'' \textit{International Journal of Operations \& Production Management}, vol. 15, no. 4, pp. 80–116, Jan. 1995. doi: \href{https://doi.org/10.1108/01443579510083622}{10.1108/01443579510083622}.
    
\bibitem{safarishahrbijari2018}
A. Safarishahrbijari, ``Workforce forecasting models: A systematic review,'' \textit{Journal of Forecasting}, vol. 37, no. 7, pp. 739–753, 2018. doi: \href{https://doi.org/10.1002/for.2541}{10.1002/for.2541}.

\bibitem{wieland2013}
A. Wieland, ``Selecting the right supply chain based on risks,'' \textit{Journal of Manufacturing Technology Management}, vol. 24, no. 5, pp. 652–668, Jan. 2013. doi: \href{https://doi.org/10.1108/17410381311327954}{10.1108/17410381311327954}.

\bibitem{hardgrave2009}
B. C. Hardgrave, J. Aloysius, and S. Goyal, ``Does RFID improve inventory accuracy? A preliminary analysis,'' \textit{International Journal of RF Technologies: Research and Applications}, vol. 1, no. 1, pp. 44–56, Mar. 2009. doi: \href{https://doi.org/10.1080/17545730802338333}{10.1080/17545730802338333}.

\bibitem{jha2021}
B. Kumar Jha and S. Pande, ``Time Series Forecasting Model for Supermarket Sales using FB-Prophet,'' in \textit{2021 5th International Conference on Computing Methodologies and Communication (ICCMC)}, Erode, India: IEEE, Apr. 2021, pp. 547–554. doi: \href{https://doi.org/10.1109/ICCMC51019.2021.9418033}{10.1109/ICCMC51019.2021.9418033}.

\bibitem{maccarthy2016}
B. L. MacCarthy, C. Blome, J. Olhager, J. S. Srai, and X. Zhao, "Supply chain evolution – theory, concepts and science," \textit{International Journal of Operations \& Production Management}, vol. 36, no. 12, pp. 1696–1718, Jan. 2016. doi: \href{https://doi.org/10.1108/IJOPM-02-2016-0080}{10.1108/IJOPM-02-2016-0080}.

\bibitem{hazen2018}
B. T. Hazen, J. B. Skipper, C. A. Boone, and R. R. Hill, ``Back in business: operations research in support of big data analytics for operations and supply chain management,'' \textit{Ann Oper Res}, vol. 270, no. 1–2, pp. 201–211, Nov. 2018. doi: \href{https://doi.org/10.1007/s10479-016-2226-0}{10.1007/s10479-016-2226-0}.

\bibitem{cornelis2010}
C. Cornelis, R. Jensen, G. Hurtado, and D. Śle¸zak, ``Attribute selection with fuzzy decision reducts,'' \textit{Information Sciences}, vol. 180, no. 2, pp. 209–224, Jan. 2010. doi: \href{https://doi.org/10.1016/j.ins.2009.09.008}{10.1016/j.ins.2009.09.008}.

\bibitem{droge2012}
C. Droge, S. K. Vickery, and M. A. Jacobs, ``Does supply chain integration mediate the relationships between product/process strategy and service performance? An empirical study,'' \textit{International Journal of Production Economics}, vol. 137, no. 2, pp. 250–262, Jun. 2012. doi: \href{https://doi.org/10.1016/j.ijpe.2012.02.005}{10.1016/j.ijpe.2012.02.005}.

\bibitem{duarte2006}
C. Duarte and F. Cardoso, ``The Use of Qualitative Information for Forecasting Exports,'' \textit{Economic Bulletin and Financial Stability Report Articles and Banco de Portugal Economic Studies}, 2006. Accessed: Jun. 19, 2023. [Online]. Available: \url{https://ideas.repec.org//a/ptu/bdpart/b200615.html}

\bibitem{hitchcock2023}
C. Hitchcock, ``Causal Models,'' in \textit{The Stanford Encyclopedia of Philosophy}, E. N. Zalta and U. Nodelman, Eds., Spring 2023. Metaphysics Research Lab, Stanford University, 2023. Accessed: Jun. 24, 2023. [Online]. Available: \url{https://plato.stanford.edu/archives/spr2023/entries/causal-models/}

\bibitem{li2018}
C. Li and A. Lim, ``A greedy aggregation–decomposition method for intermittent demand forecasting in fashion retailing,'' \textit{European Journal of Operational Research}, vol. 269, no. 3, pp. 860–869, Sep. 2018. doi: \href{https://doi.org/10.1016/j.ejor.2018.02.029}{10.1016/j.ejor.2018.02.029}.

\bibitem{ho2007}
C.-J. Ho, ``Measuring system performance of an ERP-based supply chain,'' \textit{International Journal of Production Research}, vol. 45, no. 6, pp. 1255–1277, Mar. 2007. doi: \href{https://doi.org/10.1080/00207540600635235}{10.1080/00207540600635235}.

\bibitem{vithitsoontorn2022}
C. Vithitsoontorn and P. Chongstitvatana, ``Demand Forecasting in Production Planning for Dairy Products Using Machine Learning and Statistical Method,'' \textit{Proceedings of the 2022 International Electrical Engineering Congress, iEECON 2022}, no. 1, pp. 1–4, 2022, doi: \href{https://doi.org/10.1109/iEECON53204.2022.9741683}{10.1109/iEECON53204.2022.9741683}.

\bibitem{dey1998}
D. Dey, S. Sarkar, and P. De, ``Entity matching in heterogeneous databases: a distance-based decision model,'' in \textit{Proceedings of the Thirty-First Hawaii International Conference on System Sciences}, Kohala Coast, HI, USA: IEEE Comput. Soc, 1998, pp. 305–313. doi: \href{https://doi.org/10.1109/HICSS.1998.649225}{10.1109/HICSS.1998.649225}.

\bibitem{grewal2017}
D. Grewal, A. L. Roggeveen, and J. Nordfält, ``The Future of Retailing,'' \textit{Journal of Retailing}, vol. 93, no. 1, pp. 1–6, Mar. 2017. doi: \href{https://doi.org/10.1016/j.jretai.2016.12.008}{10.1016/j.jretai.2016.12.008}.

\bibitem{michie1995}
D. Michie, D. J. Spiegelhalter, C. C. Taylor, and J. Campbell, Eds., \textit{Machine learning, neural and statistical classification}. USA: Ellis Horwood, 1995.

\bibitem{minnich2006}
D. Minnich and F. H. Maier, ``Supply Chain Responsiveness and Efficiency – Complementing or Contradicting Each Other?,'' 2006. Accessed: Jun. 19, 2023. [Online]. Available: \url{https://www.semanticscholar.org/paper/Supply-Chain-Responsiveness-and-Efficiency-%E2%80%93-or-Minnich-Maier/ac570a977dba95a48d073b43b0ef1d9e82d06604}

\bibitem{noack2008}
D. Noack and O. Rose, ``A simulation based optimization algorithm for slack reduction and workforce scheduling,'' in \textit{2008 Winter Simulation Conference}, Dec. 2008, pp. 1989–1994. doi: \href{https://doi.org/10.1109/WSC.2008.4736293}{10.1109/WSC.2008.4736293}.

\bibitem{wilson2011}
D. R. Wilson, ``Beyond probabilistic record linkage: Using neural networks and complex features to improve genealogical record linkage,'' in \textit{The 2011 International Joint Conference on Neural Networks}, Jul. 2011, pp. 9–14. doi: \href{https://doi.org/10.1109/IJCNN.2011.6033192}{10.1109/IJCNN.2011.6033192}.

\bibitem{modha2003}
D. S. Modha and W. S. Spangler, ``Feature Weighting in k-Means Clustering,'' \textit{Machine Learning}, vol. 52, no. 3, pp. 217–237, Sep. 2003. doi: \href{https://doi.org/10.1023/A:1024016609528}{10.1023/A:1024016609528}.

\bibitem{singha2022}
D. Singha and C. Panse, ``Application of different Machine Learning models for Supply Chain Demand Forecasting: Comparative Analysis,'' \textit{Proceedings of 2nd International Conference on Innovative Practices in Technology and Management, ICIPTM 2022}, vol. 2, pp. 312–318, 2022, doi: \href{https://doi.org/10.1109/ICIPTM54933.2022.9753864}{10.1109/ICIPTM54933.2022.9753864}.

\bibitem{tranfield2003}
D. Tranfield, D. Denyer, and P. Smart, ``Towards a Methodology for Developing Evidence-Informed Management Knowledge by Means of Systematic Review,'' \textit{British Journal of Management}, vol. 14, no. 3, pp. 207–222, 2003. doi: \href{https://doi.org/10.1111/1467-8551.00375}{10.1111/1467-8551.00375}.

\bibitem{locke2006}
E. A. Locke and G. P. Latham, ``New Directions in Goal-Setting Theory,'' \textit{Curr Dir Psychol Sci}, vol. 15, no. 5, pp. 265–268, Oct. 2006. doi: \href{https://doi.org/10.1111/j.1467-8721.2006.00449.x}{10.1111/j.1467-8721.2006.00449.x}.

\bibitem{bernardes2008}
E. S. Bernardes and G. A. Zsidisin, ``An examination of strategic supply management benefits and performance implications,'' \textit{Journal of Purchasing and Supply Management}, vol. 14, no. 4, pp. 209–219, Dec. 2008. doi: \href{https://doi.org/10.1016/j.pursup.2008.06.004}{10.1016/j.pursup.2008.06.004}.

\bibitem{tuv2009}
E. Tuv, A. Borisov, G. Runger, and K. Torkkola, "Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination," \textit{J. Mach. Learn. Res.}, vol. 10, pp. 1341–1366, Dec. 2009.

\bibitem{vanpoucke2009}
E. Vanpoucke, K. K. Boyer, and A. Vereecke, ``Supply chain information flow strategies: an empirical taxonomy,'' \textit{International Journal of Operations \& Production Management}, vol. 29, no. 12, pp. 1213–1241, Jan. 2009. doi: \href{https://doi.org/10.1108/01443570911005974}{10.1108/01443570911005974}.

\bibitem{bittencourt2005}
F. Bittencourt and R. J. Rabelo, ``A Systematic Approach for VE Partners Selection Using the SCOR Model and the AHP Method,'' in \textit{Collaborative Networks and Their Breeding Environments}, L. M. Camarinha-Matos, H. Afsarmanesh, and A. Ortiz, Eds., in IFIP — The International Federation for Information Processing, vol. 186. New York: Springer-Verlag, 2005, pp. 99–108. doi: \href{https://doi.org/10.1007/0-387-29360-4_10}{10.1007/0-387-29360-4\_10}.

\bibitem{friedrichs2005}
F. Friedrichs and C. Igel, ``Evolutionary tuning of multiple SVM parameters,'' \textit{Neurocomputing}, vol. 64, pp. 107–117, Mar. 2005. doi: \href{https://doi.org/10.1016/j.neucom.2004.11.022}{10.1016/j.neucom.2004.11.022}.

\bibitem{honghai2005}
F. Honghai, C. Guoshun, Y. Cheng, Y. Bingru, and C. Yumei, ``A SVM Regression Based Approach to Filling in Missing Values,'' in \textit{Knowledge-Based Intelligent Information and Engineering Systems}, R. Khosla, R. J. Howlett, and L. C. Jain, Eds., in Lecture Notes in Computer Science, vol. 3683. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 581–587. doi: \href{https://doi.org/10.1007/11553939_83}{10.1007/11553939\_83}.

\bibitem{mehmood2022}
F. Mehmood, M. U. Ghani, H. Ghafoor, R. Shahzadi, M. N. Asim, and W. Mahmood, ``EGD-SNet: A computational search engine for predicting an end-to-end machine learning pipeline for Energy Generation \& Demand Forecasting,'' \textit{Applied Energy}, vol. 324, no. March, p. 119754, 2022, doi: \href{https://doi.org/10.1016/j.apenergy.2022.119754}{10.1016/j.apenergy.2022.119754}.

\bibitem{chan2003}
F. T. S. Chan, ``Performance Measurement in a Supply Chain,'' \textit{The International Journal of Advanced Manufacturing Technology}, vol. 21, no. 7, pp. 534–548, May 2003. doi: \href{https://doi.org/10.1007/s001700300063}{10.1007/s001700300063}.

\bibitem{brown2012}
G. Brown, A. Pocock, M.-J. Zhao, and M. Luján, ``Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection,'' \textit{Journal of Machine Learning Research}, vol. 13, no. 2, pp. 27–66, 2012.

\bibitem{batista2003}
G. E. A. P. A. Batista and M. C. Monard, ``An analysis of four missing data treatment methods for supervised learning,'' \textit{Applied Artificial Intelligence}, vol. 17, no. 5–6, pp. 519–533, May 2003. doi: \href{https://doi.org/10.1080/713827181}{10.1080/713827181}.

\bibitem{john1994}
G. H. John, ``Cross-Validated C4.5: Using Error Estimation for Automatic Parameter Selection,'' Stanford University, Stanford, CA, USA, 1994.

\bibitem{strack2010}
G. Strack and Y. Pochet, ``An integrated model for warehouse and inventory planning,'' \textit{European Journal of Operational Research}, vol. 204, no. 1, pp. 35–50, Jul. 2010. doi: \href{https://doi.org/10.1016/j.ejor.2009.09.006}{10.1016/j.ejor.2009.09.006}.

\bibitem{elghazel2015}
H. Elghazel and A. Aussem, ``Unsupervised feature selection with ensemble learning,'' \textit{Mach Learn}, vol. 98, no. 1–2, pp. 157–180, Jan. 2015. doi: \href{https://doi.org/10.1007/s10994-013-5337-8}{10.1007/s10994-013-5337-8}.

\bibitem{forslund2007}
H. Forslund and P. Jonsson, ``The impact of forecast information quality on supply chain performance,'' \textit{International Journal of Operations \& Production Management}, vol. 27, no. 1, pp. 90–107, Jan. 2007. doi: \href{https://doi.org/10.1108/01443570710714556}{10.1108/01443570710714556}.

\bibitem{frohlich2005}
H. Frohlich and A. Zell, ``Efficient parameter selection for support vector machines in classification and regression via model-based global optimization,'' in \textit{Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.}, Montreal, QC, Canada: IEEE, 2005, pp. 1431–1436. doi: \href{https://doi.org/10.1109/IJCNN.2005.1556085}{10.1109/IJCNN.2005.1556085}.

\bibitem{hu2023}
H. Hu, J. Xu, M. Liu, and M. K. Lim, ``Vaccine supply chain management: An intelligent system utilizing blockchain, IoT and machine learning,'' \textit{Journal of Business Research}, vol. 156, no. December 2022, p. 113480, 2023, doi: \href{https://doi.org/10.1016/j.jbusres.2022.113480}{10.1016/j.jbusres.2022.113480}.

\bibitem{escalante2010}
H. J. Escalante, M. Montes, and E. Sucar, ``Ensemble particle swarm model selection,'' in \textit{The 2010 International Joint Conference on Neural Networks (IJCNN)}, Barcelona, Spain: IEEE, Jul. 2010, pp. 1–8. doi: \href{https://doi.org/10.1109/IJCNN.2010.5596915}{10.1109/IJCNN.2010.5596915}.

\bibitem{liu2002}
H. Liu, F. Hussain, C. L. Tan, and M. Dash, ``Discretization: An Enabling Technique,'' \textit{Data Mining and Knowledge Discovery}, vol. 6, no. 4, pp. 393–423, Oct. 2002. doi: \href{https://doi.org/10.1023/A:1016304305535}{10.1023/A:1016304305535}.

\bibitem{liu2009}
H. Liu, J. Sun, L. Liu, and H. Zhang, ``Feature selection with dynamic mutual information,'' \textit{Pattern Recognition}, vol. 42, no. 7, pp. 1330–1339, Jul. 2009. doi: \href{https://doi.org/10.1016/j.patcog.2008.10.028}{10.1016/j.patcog.2008.10.028}.

\bibitem{peng2005}
H. Peng, F. Long, and C. Ding, ``Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy,'' \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 27, no. 8, pp. 1226–1238, Aug. 2005. doi: \href{https://doi.org/10.1109/TPAMI.2005.159}{10.1109/TPAMI.2005.159}.

\bibitem{wei2021}
H. Wei and Q. Zeng, ``Research on sales Forecast based on XGBoost-LSTM algorithm Model,'' \textit{J. Phys.: Conf. Ser.}, vol. 1754, no. 1, p. 012191, Feb. 2021. doi: \href{https://doi.org/10.1088/1742-6596/1754/1/012191}{10.1088/1742-6596/1754/1/012191}.

\bibitem{wei2007}
H. Wei and S. A. Billings, ``Feature Subset Selection and Ranking for Data Dimensionality Reduction,'' \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 29, no. 1, pp. 162–166, Jan. 2007. doi: \href{https://doi.org/10.1109/TPAMI.2007.250607}{10.1109/TPAMI.2007.250607}.

\bibitem{hamdan2023}
I. K. A. Hamdan, W. Aziguli, D. Zhang, and E. Sumarliah, ``Machine learning in supply chain: prediction of real-time e-order arrivals using ANFIS,'' \textit{International Journal of System Assurance Engineering and Management}, vol. 14, no. s1, pp. 549–568, 2023, doi: \href{https://doi.org/10.1007/s13198-022-01851-7}{10.1007/s13198-022-01851-7}.

\bibitem{lopez2020}
I. Lopez-Arevalo, E. Aldana-Bobadilla, A. Molina-Villegas, H. Galeana-Zapién, V. Muñiz-Sanchez, and S. Gausin-Valle, ``A Memory-Efficient Encoding Method for Processing Mixed-Type Data on Machine Learning,'' \textit{Entropy}, vol. 22, no. 12, Art. no. 12, Dec. 2020. doi: \href{https://doi.org/10.3390/e22121391}{10.3390/e22121391}.

\bibitem{loshchilov2016}
I. Loshchilov and F. Hutter, ``CMA-ES for Hyperparameter Optimization of Deep Neural Networks,'' \textit{arXiv}, Apr. 25, 2016. doi: \href{https://doi.org/10.48550/arXiv.1604.07269}{10.48550/arXiv.1604.07269}.

\bibitem{fellegi1969}
I. P. Fellegi and A. B. Sunter, ``A Theory for Record Linkage,'' \textit{Journal of the American Statistical Association}, vol. 64, no. 328, pp. 1183–1210, Dec. 1969. doi: \href{https://doi.org/10.1080/01621459.1969.10501049}{10.1080/01621459.1969.10501049}.

\bibitem{rodriguez2010}
I. Rodriguez-Lujan, R. Huerta, C. Elkan, and C. S. Cruz, ``Quadratic Programming Feature Selection,'' \textit{J. Mach. Learn. Res.}, vol. 11, pp. 1491–1516, Aug. 2010.

\bibitem{varela2014}
I. Varela Rozados and B. Tjahjono, \textit{Big Data Analytics in Supply Chain Management: Trends and Related Research}, 2014. doi: \href{https://doi.org/10.13140/RG.2.1.4935.2563}{10.13140/RG.2.1.4935.2563}.

\bibitem{javeri2021}
I. Y. Javeri, M. Toutiaee, I. B. Arpinar, T. W. Miller, and J. A. Miller, ``Improving Neural Networks for Time Series Forecasting using Data Augmentation and AutoML,'' \textit{arXiv}, May 07, 2021. Accessed: Jun. 19, 2023. [Online]. Available: \url{http://arxiv.org/abs/2103.01992}.

\bibitem{barnard1999}
J. Barnard and X.-L. Meng, ``Applications of multiple imputation in medical studies: from AIDS to NHANES,'' \textit{Stat Methods Med Res}, vol. 8, no. 1, pp. 17–36, Feb. 1999. doi: \href{https://doi.org/10.1177/096228029900800103}{10.1177/096228029900800103}.

\bibitem{bergstra2013}
J. Bergstra, D. Yamins, and D. Cox, ``Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures,'' in \textit{Proceedings of the 30th International Conference on Machine Learning}, PMLR, Feb. 2013, pp. 115–123. Accessed: Jun. 19, 2023. [Online]. Available: \url{https://proceedings.mlr.press/v28/bergstra13.html}

\bibitem{bergstra2011}
J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, ``Algorithms for Hyper-Parameter Optimization,'' in \textit{Advances in Neural Information Processing Systems}, Curran Associates, Inc., 2011. Accessed: Jun. 19, 2023. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html}

\bibitem{chambers1971}
J. C. Chambers, S. K. Mullick, and D. D. Smith, ``How to Choose the Right Forecasting Technique,'' \textit{Harvard Business Review}, Jul. 01, 1971. Accessed: Jun. 19, 2023. [Online]. Available: \url{https://hbr.org/1971/07/how-to-choose-the-right-forecasting-technique}

\bibitem{han2012}
J. Han, M. Kamber, and J. Pei, ``3 - Data Preprocessing,'' in \textit{Data Mining} (Third Edition), J. Han, M. Kamber, and J. Pei, Eds., in The Morgan Kaufmann Series in Data Management Systems. Boston: Morgan Kaufmann, 2012, pp. 83–124. doi: \href{https://doi.org/10.1016/B978-0-12-381479-1.00003-4}{10.1016/B978-0-12-381479-1.00003-4}.

\bibitem{spitzer1978}
J. J. Spitzer, "A Monte Carlo Investigation of the Box-Cox Transformation in Small Samples," \textit{Journal of the American Statistical Association}, vol. 73, no. 363, pp. 488–495, 1978, doi: \href{https://doi.org/10.2307/2286587}{10.2307/2286587}.

\bibitem{berg1999}
J. P. van den BERG, ``A literature survey on planning and control of warehousing systems,'' \textit{IIE Transactions}, vol. 31, no. 8, pp. 751–762, Aug. 1999. doi: \href{https://doi.org/10.1080/07408179908969874}{10.1080/07408179908969874}.

\bibitem{armstrong2001}
J. S. Armstrong, Ed., \textit{Principles of Forecasting: A Handbook for Researchers and Practitioners}, vol. 30. in International Series in Operations Research \& Management Science, vol. 30. Boston, MA: Springer US, 2001. doi: \href{https://doi.org/10.1007/978-0-306-47630-3}{10.1007/978-0-306-47630-3}.

\bibitem{snoek2012}
J. Snoek, H. Larochelle, and R. P. Adams, ``Practical Bayesian Optimization of Machine Learning Algorithms,'' in \textit{Advances in Neural Information Processing Systems}, Curran Associates, Inc., 2012. Accessed: Jun. 19, 2023. [Online]. Available: \url{https://proceedings.neurips.cc/paper/2012/hash/05311655a15b75fab86956663e1819cd-Abstract.html}

\bibitem{grzymala2005}
J. W. Grzymala-Busse, L. K. Goodwin, W. J. Grzymala-Busse, and X. Zheng, ``Handling Missing Attribute Values in Preterm Birth Data Sets,'' in \textit{Rough Sets, Fuzzy Sets, Data Mining, and Granular Computing}, D. Ślęzak, J. Yao, J. F. Peters, W. Ziarko, and X. Hu, Eds., in Lecture Notes in Computer Science, vol. 3642. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 342–351. doi: \href{https://doi.org/10.1007/11548706_36}{10.1007/11548706\_36}.

\bibitem{bagshaw2017}
K. B. Bagshaw, ``WORKFORCE BIG DATA ANALYTICS AND PRODUCTION EFFICIENCY: A Manager’s Guide,'' \textit{ABR}, vol. 5, no. 7, Jul. 2017. doi: \href{https://doi.org/10.14738/abr.57.3168}{10.14738/abr.57.3168}.

\bibitem{chaudhuri2022}
K. D. Chaudhuri and B. Alkan, ``A hybrid extreme learning machine model with harris hawks optimisation algorithm: an optimised model for product demand forecasting applications,'' \textit{Applied Intelligence}, vol. 52, no. 10, pp. 11489–11505, 2022, doi: \href{https://doi.org/10.1007/s10489-022-03251-7}{10.1007/s10489-022-03251-7}.

\bibitem{javed2012}
K. Javed, H. A. Babri, and M. Saeed, ``Feature Selection Based on Class-Dependent Densities for High-Dimensional Binary Data,'' \textit{IEEE Trans. Knowl. Data Eng.}, vol. 24, no. 3, pp. 465–477, Mar. 2012. doi: \href{https://doi.org/10.1109/TKDE.2010.263}{10.1109/TKDE.2010.263}.

\bibitem{potdar2017}
K. Potdar, T. S. Pardawala, and C. D. Pai, ``A Comparative Study of Categorical Variable Encoding Techniques for Neural Network Classifiers,'' \textit{International Journal of Computer Applications}, vol. 175, no. 4, pp. 7–9, Oct. 2017.

\bibitem{schliephake2009}
K. Schliephake, G. Stevens, and S. Clay, ``Making resources work more efficiently – the importance of supply chain partnerships,'' \textit{Journal of Cleaner Production}, vol. 17, no. 14, pp. 1257–1263, Sep. 2009. doi: \href{https://doi.org/10.1016/j.jclepro.2009.03.020}{10.1016/j.jclepro.2009.03.020}.

\bibitem{wu2005}
K. Wu, ``An examination of variability and its basic properties for a factory,'' \textit{IEEE Transactions on Semiconductor Manufacturing}, vol. 18, no. 1, pp. 214–221, Feb. 2005. doi: \href{https://doi.org/10.1109/TSM.2004.840525}{10.1109/TSM.2004.840525}.

\bibitem{lapide1999}
L. Lapide, ``New Developments in Business Forecasting,'' \textit{The Journal of Business Forecasting Methods \& Systems}, vol. 17, no. 2, pp. 28–29, 1999.

\bibitem{yue2010}
L. Yue, L. Zhenjiang, Y. Yafeng, T. Zaixia, G. Junjun, and Z. Bofeng, ``Selective and Heterogeneous SVM Ensemble for Demand Forecasting,'' in \textit{2010 10th IEEE International Conference on Computer and Information Technology}, Jun. 2010, pp. 1519–1524. doi: \href{https://doi.org/10.1109/CIT.2010.270}{10.1109/CIT.2010.270}.

\bibitem{yue2007}
L. Yue, Y. Yafeng, G. Junjun, and T. Chongli, ``Demand Forecasting by Using Support Vector Machine,'' in \textit{Third International Conference on Natural Computation (ICNC 2007)}, Aug. 2007, pp. 272–276. doi: \href{https://doi.org/10.1109/ICNC.2007.324}{10.1109/ICNC.2007.324}.

\bibitem{waller2013}
M. A. Waller and S. E. Fawcett, ``Data Science, Predictive Analytics, and Big Data: A Revolution That Will Transform Supply Chain Design and Management,'' \textit{Journal of Business Logistics}, vol. 34, no. 2, pp. 77–84, 2013. doi: \href{https://doi.org/10.1111/jbl.12010}{10.1111/jbl.12010}.

\bibitem{waller2006}
M. A. Waller, H. Nachtmann, and J. Hunter, ``Measuring the impact of inaccurate inventory information on a retail outlet,'' \textit{The International Journal of Logistics Management}, vol. 17, no. 3, pp. 355–376, Jan. 2006. doi: \href{https://doi.org/10.1108/09574090610717527}{10.1108/09574090610717527}.

\bibitem{asrol2021}
M. Asrol, M. Marimin, M. Machfud, M. Yani, and E. Taira, ``Risk Management for Improving Supply Chain Performance of Sugarcane Agroindustry,'' \textit{iems}, vol. 20, no. 1, pp. 9–26, Mar. 2021. doi: \href{https://doi.org/10.7232/iems.2021.20.1.9}{10.7232/iems.2021.20.1.9}.

\bibitem{cochinwala2001}
M. Cochinwala, V. Kurien, G. Lalk, and D. Shasha, ``Efficient data reconciliation,'' \textit{Information Sciences}, vol. 137, no. 1–4, pp. 1–15, Sep. 2001. doi: \href{https://doi.org/10.1016/S0020-0255(00)00070-0}{10.1016/S0020-0255(00)00070-0}.

\bibitem{feurer2019}
M. Feurer and F. Hutter, ``Hyperparameter Optimization,'' in \textit{Automated Machine Learning}, F. Hutter, L. Kotthoff, and J. Vanschoren, Eds., in \textit{The Springer Series on Challenges in Machine Learning}. Cham: Springer International Publishing, 2019, pp. 3–33. doi: \href{https://doi.org/10.1007/978-3-030-05318-5_1}{10.1007/978-3-030-05318-5\_1}.

\bibitem{gurnani2017}
M. Gurnani, Y. Korke, P. Shah, S. Udmale, V. Sambhe, and S. Bhirud, ``Forecasting of sales by using fusion of machine learning techniques,'' in \textit{2017 International Conference on Data Management, Analytics and Innovation (ICDMAI)}, Pune: IEEE, Feb. 2017, pp. 93–101. doi: \href{https://doi.org/10.1109/ICDMAI.2017.8073492}{10.1109/ICDMAI.2017.8073492}.

\bibitem{holmqvist2006}
M. Holmqvist and G. Stefansson, "‘SMART GOODS’ AND MOBILE RFID A CASE WITH INNOVATION FROM VOLVO," \textit{Journal of Business Logistics}, vol. 27, no. 2, pp. 251–272, Sep. 2006. doi: \href{https://doi.org/10.1002/j.2158-1592.2006.tb00225.x}{10.1002/j.2158-1592.2006.tb00225.x}.

\bibitem{bassiouni2023}
M. M. Bassiouni, R. K. Chakrabortty, O. K. Hussain, and H. F. Rahman, ``Advanced deep learning approaches to predict supply chain risks under COVID-19 restrictions,'' \textit{Expert Systems with Applications}, vol. 211, no. August 2022, p. 118604, 2023, doi: \href{https://doi.org/10.1016/j.eswa.2022.118604}{10.1016/j.eswa.2022.118604}.

\bibitem{refaat2010}
M. Refaat, \textit{Data Preparation for Data Mining Using SAS}. Elsevier, 2010.

\bibitem{robnik2003}
M. Robnik-Šikonja and I. Kononenko, "Theoretical and Empirical Analysis of ReliefF and RReliefF," \textit{Machine Learning}, vol. 53, no. 1, pp. 23–69, Oct. 2003. doi: \href{https://doi.org/10.1023/A:1025667309714}{10.1023/A:1025667309714}.

\bibitem{zohdi2022}
M. Zohdi, M. Rafiee, V. Kayvanfar, and A. Salamiraad, ``Demand forecasting based machine learning algorithms on customer information: an applied approach,'' \textit{International Journal of Information Technology (Singapore)}, vol. 14, no. 4, pp. 1937–1947, 2022, doi: \href{https://doi.org/10.1007/s41870-022-00875-3}{10.1007/s41870-022-00875-3}.

\bibitem{mitra2022} 
A. Mitra, A. Jain, A. Kishore, and P. Kumar, ``A Comparative Study of Demand Forecasting Models for a Multi-Channel Retail Company: A Novel Hybrid Machine Learning Approach,'' \textit{Oper. Res. Forum}, vol. 3, no. 4, p. 58, Sep. 2022. doi: \href{https://doi.org/10.1007/s43069-022-00166-4}{10.1007/s43069-022-00166-4}.

\bibitem{elgendy2014}
N. Elgendy and A. Elragal, "Big Data Analytics: A Literature Review Paper," in \textit{Advances in Data Mining. Applications and Theoretical Aspects}, P. Perner, Ed., in \textit{Lecture Notes in Computer Science}, vol. 8557. Cham: Springer International Publishing, 2014, pp. 214–227. doi: \href{https://doi.org/10.1007/978-3-319-08976-8_16}{10.1007/978-3-319-08976-8\_16}.

\bibitem{kwak2002a}
N. Kwak and Chong-Ho Choi, "Input feature selection by mutual information based on Parzen window," \textit{IEEE Trans. Pattern Anal. Machine Intell.}, vol. 24, no. 12, pp. 1667–1671, Dec. 2002. doi: \href{https://doi.org/10.1109/TPAMI.2002.1114861}{10.1109/TPAMI.2002.1114861}.

\bibitem{kwak2002b}
N. Kwak and Chong-Ho Choi, "Input feature selection for classification problems," \textit{IEEE Trans. Neural Netw.}, vol. 13, no. 1, pp. 143–159, Jan. 2002. doi: \href{https://doi.org/10.1109/72.977291}{10.1109/72.977291}.

\bibitem{sanders2016}
N. R. Sanders, "How to Use Big Data to Drive Your Supply Chain," \textit{California Management Review}, vol. 58, no. 3, pp. 26–48, May 2016. doi: \href{https://doi.org/10.1525/cmr.2016.58.3.26}{10.1525/cmr.2016.58.3.26}.

\bibitem{hassanzadeh2009}
O. Hassanzadeh, F. Chiang, H. C. Lee, and R. J. Miller, "Framework for evaluating clustering algorithms in duplicate detection," \textit{Proc. VLDB Endow.}, vol. 2, no. 1, pp. 1282–1293, Aug. 2009. doi: \href{https://doi.org/10.14778/1687627.1687771}{10.14778/1687627.1687771}.

\bibitem{estevez2009}
P. A. Estevez, M. Tesmer, C. A. Perez, and J. M. Zurada, "Normalized Mutual Information Feature Selection," \textit{IEEE Trans. Neural Netw.}, vol. 20, no. 2, pp. 189–201, Feb. 2009. doi: \href{https://doi.org/10.1109/TNN.2008.2005601}{10.1109/TNN.2008.2005601}.

\bibitem{hajek2020}
P. Hajek and M. Z. Abedin, "A Profit Function-Maximizing Inventory Backorder Prediction System Using Big Data Analytics," \textit{IEEE Access}, vol. 8, pp. 58982–58994, 2020. doi: \href{https://doi.org/10.1109/ACCESS.2020.2983118}{10.1109/ACCESS.2020.2983118}.

\bibitem{bickel1981}
P. J. Bickel and K. A. Doksum, "An Analysis of Transformations Revisited," \textit{Journal of the American Statistical Association}, vol. 76, no. 374, pp. 296–311, Jun. 1981, doi: \href{https://doi.org/10.1080/01621459.1981.10477649}{10.1080/01621459.1981.10477649}.

\bibitem{mitra2002}
P. Mitra, C. A. Murthy, and S. K. Pal, "Unsupervised feature selection using feature similarity," \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 24, no. 3, pp. 301–312, Mar. 2002. doi: \href{https://doi.org/10.1109/34.990133}{10.1109/34.990133}.

\bibitem{rabbitt2001}
P. Rabbitt, P. Osman, B. Moore, and B. Stollery, "There are stable individual differences in performance variability, both from moment to moment and from day to day," \textit{The Quarterly Journal of Experimental Psychology Section A}, vol. 54, no. 4, pp. 981–1003, Nov. 2001. doi: \href{https://doi.org/10.1080/713756013}{10.1080/713756013}.

\bibitem{ravikumar2012}
P. Ravikumar and W. Cohen, "A Hierarchical Graphical Model for Record Linkage." \textit{arXiv}, Jul. 12, 2012. doi: \href{https://doi.org/10.48550/arXiv.1207.4180}{10.48550/arXiv.1207.4180}.

\bibitem{russom2011}
P. Russom and others, "Big data analytics," \textit{TDWI best practices report, fourth quarter}, vol. 19, no. 4, pp. 1–34, 2011.

\bibitem{sandhya2022}
P. Sandhya, R. Bandi, and D. D. Himabindu, ``Stock Price Prediction using Recurrent Neural Network and LSTM,'' \textit{Proceedings - 6th International Conference on Computing Methodologies and Communication, ICCMC 2022}, no. Iccmc, pp. 1723–1728, 2022, doi: \href{https://doi.org/10.1109/ICCMC53470.2022.9753764}{10.1109/ICCMC53470.2022.9753764}.

\bibitem{singla2004}
P. Singla and P. Domingos, "Multi-relational record linkage," in \textit{Proc. KDD-2004 Workshop Multi-Relational Data Mining}, 2004, pp. 31–48.

\bibitem{chen2004}
Peng-Wei Chen, Jung-Ying Wang, and Hahn-Ming Lee, "Model selection of SVMs using GA approach," in \textit{2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)}, Budapest, Hungary: IEEE, 2004, pp. 2035–2040. doi: \href{https://doi.org/10.1109/IJCNN.2004.1380929}{10.1109/IJCNN.2004.1380929}.

\bibitem{sun2013}
Q. Sun, B. Pfahringer, and M. Mayo, "Towards a Framework for Designing Full Model Selection and Optimization Systems," in \textit{Multiple Classifier Systems}, Z.-H. Zhou, F. Roli, and J. Kittler, Eds., in \textit{Lecture Notes in Computer Science}. Berlin, Heidelberg: Springer, 2013, pp. 259–270. doi: \href{https://doi.org/10.1007/978-3-642-38067-9_23}{10.1007/978-3-642-38067-9\_23}.

\bibitem{johnson2007}
R. A. Johnson and D. W. Wichern, \textit{Applied Multivariate Statistical Analysis}, 6th edition. Upper Saddle River, N.J: Pearson, 2007.

\bibitem{garnier2019}
R. Garnier and A. Belletoile, "A multi-series framework for demand forecasts in E-commerce." \textit{arXiv}, May 31, 2019. doi: \href{https://doi.org/10.48550/arXiv.1905.13614}{10.48550/arXiv.1905.13614}.

\bibitem{little1987}
R. J. A. Little and D. B. Rubin, \textit{Statistical Analysis With Missing Data}. Wiley, 1987.

\bibitem{kuo2016}
R. J. Kuo, Y. S. Tseng, and Z.-Y. Chen, "Integration of fuzzy neural network and artificial immune system-based back-propagation neural network for sales forecasting using qualitative and quantitative data," \textit{J Intell Manuf}, vol. 27, no. 6, pp. 1191–1207, Dec. 2016. doi: \href{https://doi.org/10.1007/s10845-014-0944-1}{10.1007/s10845-014-0944-1}.

\bibitem{jensen2007}
R. Jensen and Q. Shen, "Fuzzy-Rough Sets Assisted Attribute Selection," \textit{IEEE Trans. Fuzzy Syst.}, vol. 15, no. 1, pp. 73–89, Feb. 2007. doi: \href{https://doi.org/10.1109/TFUZZ.2006.889761}{10.1109/TFUZZ.2006.889761}.

\bibitem{kohavi1995}
R. Kohavi and G. H. John, "Automatic Parameter Selection by Minimizing Estimated Error," in \textit{Machine Learning Proceedings 1995}, A. Prieditis and S. Russell, Eds., San Francisco (CA): Morgan Kaufmann, 1995, pp. 304–312. doi: \href{https://doi.org/10.1016/B978-1-55860-377-6.50045-1}{10.1016/B978-1-55860-377-6.50045-1}.

\bibitem{durbin1975}
R. P. Durbin, "Letter: Acid secretion by gastric mucous membrane," \textit{Am J Physiol}, vol. 229, no. 6, p. 1726, Dec. 1975. doi: \href{https://doi.org/10.1152/ajplegacy.1975.229.6.1726}{10.1152/ajplegacy.1975.229.6.1726}.

\bibitem{andrawis2011}
R. R. Andrawis, A. F. Atiya, and H. El-Shishiny, "Combination of long term and short term forecasts, with application to tourism demand forecasting," \textit{International Journal of Forecasting}, vol. 27, no. 3, pp. 870–886, Jul. 2011. doi: \href{https://doi.org/10.1016/j.ijforecast.2010.05.019}{10.1016/j.ijforecast.2010.05.019}.

\bibitem{rathipriya2023}
R. Rathipriya, A. A. Abdul Rahman, S. Dhamodharavadhani, A. Meero, and G. Yoganandan, ``Demand forecasting model for time-series pharmaceutical data using shallow and deep neural network model,'' \textit{Neural Computing and Applications}, vol. 35, no. 2, pp. 1945–1957, 2023, doi: \href{https://doi.org/10.1007/s00521-022-07889-9}{10.1007/s00521-022-07889-9}.

\bibitem{swiniarski2003}
R. W. Swiniarski and A. Skowron, "Rough set methods in feature selection and recognition," \textit{Pattern Recognition Letters}, vol. 24, no. 6, pp. 833–849, Mar. 2003. doi: \href{https://doi.org/10.1016/S0167-8655(02)00196-4}{10.1016/S0167-8655(02)00196-4}.

\bibitem{haider2022}
S. A. Haider, M. Sajid, H. Sajid, E. Uddin, and Y. Ayaz, ``Deep learning and statistical methods for short- and long-term solar irradiance forecasting for Islamabad,'' \textit{Renewable Energy}, vol. 198, no. February, pp. 51–60, 2022, doi: \href{https://doi.org/10.1016/j.renene.2022.07.136}{10.1016/j.renene.2022.07.136}.

\bibitem{debaets2018}
S. De Baets and N. Harvey, "Forecasting from time series subject to sporadic perturbations: Effectiveness of different types of forecasting support," \textit{International Journal of Forecasting}, vol. 34, no. 2, pp. 163–180, Apr. 2018. doi: \href{https://doi.org/10.1016/j.ijforecast.2017.09.007}{10.1016/j.ijforecast.2017.09.007}.

\bibitem{falkner2018}
S. Falkner, A. Klein, and F. Hutter, "BOHB: Robust and Efficient Hyperparameter Optimization at Scale," in \textit{Proceedings of the 35th International Conference on Machine Learning}, PMLR, Jul. 2018, pp. 1437–1446. Accessed: Jun. 19, 2023. [Online]. Available: \url{https://proceedings.mlr.press/v80/falkner18a.html}.

\bibitem{bib5}
S. García, J. Luengo, and F. Herrera, "Data Preparation Basic Models," in \textit{Data Preprocessing in Data Mining}, in Intelligent Systems Reference Library, vol. 72. Cham: Springer International Publishing, 2015, pp. 39–57. doi: \href{https://doi.org/10.1007/978-3-319-10247-4_3}{10.1007/978-3-319-10247-4\_3}.

\bibitem{bib2}
S. García, J. Luengo, and F. Herrera, "Data Reduction," in \textit{Data Preprocessing in Data Mining}, in Intelligent Systems Reference Library, vol. 72. Cham: Springer International Publishing, 2015, pp. 147–162. doi: \href{https://doi.org/10.1007/978-3-319-10247-4_6}{10.1007/978-3-319-10247-4\_6}.

\bibitem{bib3}
S. García, J. Luengo, and F. Herrera, "Dealing with Missing Values," in \textit{Data Preprocessing in Data Mining}, in Intelligent Systems Reference Library, vol. 72. Cham: Springer International Publishing, 2015, pp. 59–105. doi: \href{https://doi.org/10.1007/978-3-319-10247-4_4}{10.1007/978-3-319-10247-4\_4}.

\bibitem{bib4}
S. García, J. Luengo, and F. Herrera, "Discretization," in \textit{Data Preprocessing in Data Mining}, in Intelligent Systems Reference Library, vol. 72. Cham: Springer International Publishing, 2015, pp. 245–283. doi: \href{https://doi.org/10.1007/978-3-319-10247-4_9}{10.1007/978-3-319-10247-4\_9}.

\bibitem{guha2004}
S. Guha, N. Koudas, A. Marathe, and D. Srivastava, "Merging the results of approximate match operations," in \textit{Proceedings of the Thirtieth international conference on Very large data bases - Volume 30}, in VLDB '04. Toronto, Canada: VLDB Endowment, Aug. 2004, pp. 636–647.

\bibitem{karingula2021}
S. R. Karingula et al., "Boosted Embeddings for Time Series Forecasting." \textit{arXiv}, Jul. 11, 2021. doi: \href{https://doi.org/10.48550/arXiv.2104.04781}{10.48550/arXiv.2104.04781}.

\bibitem{nithin2022}
S. S. J. Nithin, T. Rajasekar, S. Jayanthy, K. Karthik, and R. R. Rithick, ``Retail Demand Forecasting using CNN-LSTM Model,'' \textit{Proceedings of the International Conference on Electronics and Renewable Systems, ICEARS 2022}, no. Icears, pp. 1751–1756, 2022, doi: \href{https://doi.org/10.1109/ICEARS53579.2022.9752283}{10.1109/ICEARS53579.2022.9752283}.

\bibitem{shukla2022}
S. Shukla and V. M. Pillai, ``Stockout Prediction in Multi Echelon Supply Chain using Machine Learning Algorithms,'' \textit{Proceedings of the 2nd Indian International Conference on Industrial Engineering and Operations Management Warangal, Telangana, India}, pp. 1258–1270, 2022.

\bibitem{siami2018}
S. Siami-Namini, N. Tavakoli, and A. Siami Namin, "A Comparison of ARIMA and LSTM in Forecasting Time Series," in \textit{2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)}, Dec. 2018, pp. 1394–1401. doi: \href{https://doi.org/10.1109/ICMLA.2018.00227}{10.1109/ICMLA.2018.00227}.

\bibitem{roweis2000}
S. T. Roweis and L. K. Saul, "Nonlinear Dimensionality Reduction by Locally Linear Embedding," \textit{Science}, vol. 290, no. 5500, pp. 2323–2326, Dec. 2000. doi: \href{https://doi.org/10.1126/science.290.5500.2323}{10.1126/science.290.5500.2323}.

\bibitem{wang2021}
S. Wang and Y. Yang, "M-GAN-XGBOOST model for sales prediction and precision marketing strategy making of each product in online stores," \textit{Data Technologies and Applications}, vol. 55, no. 5, pp. 749–770, Jan. 2021. doi: \href{https://doi.org/10.1108/DTA-11-2020-0286}{10.1108/DTA-11-2020-0286}.

\bibitem{efendigil2009}
T. Efendigil, S. Önüt, and C. Kahraman, "A decision support system for demand forecasting with artificial neural networks and neuro-fuzzy models: A comparative analysis," \textit{Expert Systems with Applications}, vol. 36, no. 3, pp. 6697–6707, Apr. 2009. doi: \href{https://doi.org/10.1016/j.eswa.2008.08.058}{10.1016/j.eswa.2008.08.058}.

\bibitem{joachims1999}
T. Joachims, "Making large-scale support vector machine learning practical," in \textit{Advances in kernel methods: support vector learning}, Cambridge, MA, USA: MIT Press, 1999, pp. 169–184.

\bibitem{bolon2013}
V. Bolón-Canedo, N. Sánchez-Maroño, and A. Alonso-Betanzos, "A review of feature selection methods on synthetic data," \textit{Knowl Inf Syst}, vol. 34, no. 3, pp. 483–519, Mar. 2013. doi: \href{https://doi.org/10.1007/s10115-012-0487-8}{10.1007/s10115-012-0487-8}.

\bibitem{buchatskaya2015}
V. Buchatskaya, "Forecasting Methods Classification and its Applicability," \textit{IJST}, vol. 8, no. 1, pp. 1–8, Jan. 2015. doi: \href{https://doi.org/10.17485/ijst/2015/v8i30/84224}{10.17485/ijst/2015/v8i30/84224}.

\bibitem{migueis2022}
V. L. Miguéis, A. Pereira, J. Pereira, and G. Figueira, ``Reducing fresh fish waste while ensuring availability: Demand forecast using censored data and machine learning,'' \textit{Journal of Cleaner Production}, vol. 359, no. March, 2022, doi: \href{https://doi.org/10.1016/j.jclepro.2022.131852}{10.1016/j.jclepro.2022.131852}.

\bibitem{verykios2000}
V. S. Verykios, A. K. Elmagarmid, and E. N. Houstis, "Automating the approximate record-matching process," \textit{Information Sciences}, vol. 126, no. 1, pp. 83–98, Jul. 2000. doi: \href{https://doi.org/10.1016/S0020-0255(00)00013-X}{10.1016/S0020-0255(00)00013-X}.

\bibitem{elmir2023}
W. Ben Elmir, A. Hemmak, and B. Senouci, ``Smart Platform for Data Blood Bank Management: Forecasting Demand in Blood Supply Chain Using Machine Learning,'' \textit{Information (Switzerland)}, vol. 14, no. 1, pp. 1–24, 2023, doi: \href{https://doi.org/10.3390/info14010031}{10.3390/info14010031}.

\bibitem{winkler1997}
W. Winkler, "Improved Decision Rules In The Fellegi-Sunter Model Of Record Linkage," \textit{Proceedings of the Section on Survey Research Methods, American Statistical Association}, vol. 1, Sep. 1997.

\bibitem{yu2018}
W. Yu, R. Chavez, M. A. Jacobs, and M. Feng, "Data-driven supply chain capabilities and performance: A resource-based view," \textit{Transportation Research Part E: Logistics and Transportation Review}, vol. 114, pp. 371–385, Jun. 2018. doi: \href{https://doi.org/10.1016/j.tre.2017.04.002}{10.1016/j.tre.2017.04.002}.

\bibitem{li2023}
X. Li, T. Sengupta, K. Si Mohammed, and F. Jamaani, ``Forecasting the lithium mineral resources prices in China: Evidence with Facebook Prophet (Fb-P) and Artificial Neural Networks (ANN) methods,'' \textit{Resources Policy}, vol. 82, no. April, p. 103580, 2023, doi: \href{https://doi.org/10.1016/j.resourpol.2023.103580}{10.1016/j.resourpol.2023.103580}.

\bibitem{wang2007}
X. Wang, J. Yang, X. Teng, W. Xia, and R. Jensen, "Feature selection based on rough sets and particle swarm optimization," \textit{Pattern Recognition Letters}, vol. 28, no. 4, pp. 459–471, Mar. 2007. doi: \href{https://doi.org/10.1016/j.patrec.2006.09.003}{10.1016/j.patrec.2006.09.003}.

\bibitem{kang2005}
Y. Kang and S. B. Gershwin, "Information inaccuracy in inventory systems: stock loss and stockout," \textit{IIE Transactions}, vol. 37, no. 9, pp. 843–859, Sep. 2005. doi: \href{https://doi.org/10.1080/07408170590969861}{10.1080/07408170590969861}.

\bibitem{saeys2007}
Y. Saeys, I. Inza, and P. Larrañaga, "A review of feature selection techniques in bioinformatics," \textit{Bioinformatics}, vol. 23, no. 19, pp. 2507–2517, Oct. 2007. doi: \href{https://doi.org/10.1093/bioinformatics/btm344}{10.1093/bioinformatics/btm344}.

\bibitem{sun2010}
Y. Sun, S. Todorovic, and S. Goodison, "Local-Learning-Based Feature Selection for High-Dimensional Data Analysis," \textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 32, no. 9, pp. 1610–1626, Sep. 2010. doi: \href{https://doi.org/10.1109/TPAMI.2009.190}{10.1109/TPAMI.2009.190}.

\bibitem{wan2021}
Y. Wan, Y. Chen, C. Yan, and B. Zhang, "Similarity-based sales forecasting using improved ConvLSTM and prophet," \textit{Intelligent Data Analysis}, vol. 25, no. 2, pp. 383–396, Jan. 2021. doi: \href{https://doi.org/10.3233/IDA-205103}{10.3233/IDA-205103}.

\bibitem{yang2021}
Y. Yang, Y. Wu, P. Wang, and X. Jiali, "Stock Price Prediction Based on XGBoost and LightGBM," \textit{E3S Web Conf.}, vol. 275, p. 01040, 2021. doi: \href{https://doi.org/10.1051/e3sconf/202127501040}{10.1051/e3sconf/202127501040}.

\bibitem{kilimci2019}
Z. H. Kilimci et al., "An Improved Demand Forecasting Model Using Deep Learning Approach and Proposed Decision Integration Strategy for Supply Chain," \textit{Complexity}, vol. 2019, pp. 1–15, Mar. 2019. doi: \href{https://doi.org/10.1155/2019/9067367}{10.1155/2019/9067367}.

\bibitem{zhao2013}
Z. Zhao, R. Zhang, J. Cox, D. Duling, and W. Sarle, "Massively parallel feature selection: an approach based on variance preservation," \textit{Mach Learn}, vol. 92, no. 1, pp. 195–220, Jul. 2013. doi: \href{https://doi.org/10.1007/s10994-013-5373-4}{10.1007/s10994-013-5373-4}.


\end{thebibliography}


\vspace{10cm}

\begin{IEEEbiography}[{% Figure removed}]{Md Abrar Jahin,} CSCA\texttrademark\ (Certified Supply Chain Analyst), is an ambitious and dedicated undergraduate student currently pursuing a B.Sc. in industrial and production engineering at Khulna University of Engineering \& Technology (KUET) in Bangladesh. With a commitment to academic excellence, Abrar has consistently demonstrated his passion for learning and achieving remarkable milestones.

His exceptional leadership skills and innovative thinking led him to win the "Smart Roads – Winter Road Maintenance Hackathon 2021" and subsequently earn a three-week research internship at UiT -- The Arctic University of Norway. During this internship, he actively contributed to the DIT4BEARs Smart Roads project and conducted research in the field of Machine Learning. In 2021, he received a fully funded scholarship as a 'Research Intern' at Okinawa Institute of Science and Technology Graduate University (OIST) in Japan. Under the supervision of Prof. Miller in the Physics and Biology Unit, he focused on investigating perfectly conserved sequences in humans and other species. This valuable experience further enhanced his research skills and deepened his understanding of comparative genomics. Building on his previous achievements, he was offered another opportunity in 2023 as a 'Visiting Research Student' at OIST. He is actively collaborating with Prof. Miller, Dr. Biller, Dr. Zifcakova, Mr. Parker, and Dr. Pnini, delving into the evolutionary dynamics of strongly conserved sequences in vertebrates and insects.

Mr. Jahin's research interests span a wide range of disciplines, including Sustainable Supply Chain Management (SCM), Human-Computer Interaction (HCI), Quantum Computing, Deep Learning (DL), Comparative Genomics, Explainable AI (XAI), AI for Business, Natural Language Processing (NLP), Image Processing, Reinforcement Learning (RL), and Business Analytics. With his exceptional academic background, international research experiences, and diverse research interests, Abrar is poised to make significant contributions to the fields of engineering, genomics, and technology-driven solutions for sustainable supply chains.
\end{IEEEbiography}

\begin{IEEEbiography}[{% Figure removed}]{MD. SAKIB HOSSAIN SHOVON} received a B.Sc. degree in computer science and engineering (CSE) with a major in information systems from American International University--Bangladesh (AIUB) in 2023. 

He was a Research Assistant (RA) at AIUB (May 2022 -- August 2022). He works at Advanced Machine Intelligence Research Lab (AMIRL) as a Research Coordinator \& Lead Research Assistant (July 2022 -- Continuing). In addition, he is a trainee Machine Learning (ML) Engineer under the Government Edge Project associated with HeadBlocks (28th May 2023 -- ongoing). 

Mr. Shovon has published many articles in prestigious journals. His research interests include Classical Machine Learning (CML), Quantum Machine Learning (QML), Federated Machine Learning (FML), Reinforcement Learning (RL), Computer Vision (CV), Natural Language Processing (NLP), Recurrent Neural Network (RNN) and Explainable AI (XAI).
\end{IEEEbiography}

\begin{IEEEbiography}[{% Figure removed}]{JUNGPIL SHIN} (Senior Member, IEEE) received the B.Sc. degree in computer science and statistics and the M.Sc. degree in computer science from Pusan National University, South Korea, in 1990 and 1994, respectively, and the Ph.D. degree
in computer science and communication engineering from Kyushu University, Japan, in 1999.

He was an Associate Professor, a Senior Associate Professor, and a Full Professor at the School of Computer Science and Engineering, The University of Aizu, Japan, in 1999, 2004, and 2019, respectively. He has co-authored more than 300 published papers for widely cited journals and conferences. His research interests include pattern recognition, image processing, computer vision, machine learning, human-computer interaction, non-touch interfaces, human gesture recognition, automatic control, Parkinson’s disease diagnosis, ADHD diagnosis, user authentication, machine intelligence, handwriting analysis, recognition, and synthesis. 

Prof. Shin is a member of ACM, IEICE, IPSJ, KISS, and KIPS. He received a scholarship from the Japanese Government (MEXT). He served as a program chair and as a program committee member for numerous international conferences. He serves as an Editor for IEEE journals, \textit{Sensors} (MDPI), \textit{Electronics} (MDPI), and Tech Science. He serves as a reviewer for several major IEEE and SCI journals.
\end{IEEEbiography}

\begin{IEEEbiography}[{% Figure removed}]{Istiyaque Ahmed Ridoy,} CSCA\texttrademark\ (Certified Supply Chain Analyst), was born in Dhaka, Bangladesh in 2000. He completed his BBA from the Institute of Business Administration, University of Dhaka, in 2023. Additionally, he is completing his Master of Data Science and Analytics from East West University. 

Mr. Ridoy worked as a BI Analyst at GoZayaan from 2021 to 2022. He is currently working in the same company as a Data Analyst. His research interest includes Supply Chain, Data Analytics, and Statistics.

\end{IEEEbiography}

\begin{IEEEbiography}
[{% Figure removed}]{YOICHI TOMIOKA} (Member, IEEE) received the B.E., M.E., and D.E. degrees from the Tokyo Institute of Technology, Tokyo, Japan, in 2005, 2006, and 2009, respectively. 

He was a Research Associate with the Tokyo Institute of Technology until 2009. He was an Assistant Professor with the Division of Advanced Electrical and Electronics Engineering, Tokyo University of Agriculture and Technology until 2015. He was an Associate Professor at the School of Computer Science and Engineering, The University of Aizu until 2018, where he has been a Senior Associate Professor since 2019. 

His research interests include image processing, hardware acceleration, high-performance computing, electrical design automation, and combinational algorithms.
\end{IEEEbiography}


\begin{IEEEbiography}[{% Figure removed}]{Dr. M. F. Mridha} (Senior Member, IEEE) is currently working as an Associate Professor in the Department of Computer Science, American International University-Bangladesh (AIUB). Before that, he worked as an Associate Professor and Chairman in the Department of Computer Science and Engineering, at Bangladesh University of Business and Technology (BUBT). He also worked as a CSE department faculty member at the University of Asia Pacific and as a graduate head from 2012 to 2019. He received his Ph.D. in AI/ML from Jahangirnagar University in the year 2017. 

His research experience in academia and industry results in over 120 journal and conference publications. His research work contributed to the reputed \textit{Journal of Scientific Reports} (Nature), \textit{Knowledge-Based Systems}, \textit{Artificial Intelligence Review}, IEEE Access, \textit{Sensors, Cancers, Biology,} and \textit{Applied Sciences}, etc. His research interests include artificial intelligence (AI), machine learning, deep learning, and natural language processing (NLP). For more than 10 (Ten) years, he has been with the masters and undergraduate students as a supervisor of their thesis work. His research interests include artificial intelligence (AI), machine learning, natural language processing (NLP), big data analysis, etc. 

Dr. Mridha has served as a program committee member in several international conferences/workshops. He served as an Academic editor of several journals, including \textit{PLOS ONE} Journal. He has served as a reviewer of reputed journals like IEEE Transactions on Neural Networks,  IEEE Access, \textit{Knowledge-based System, Expert Systems, Bioinformatics, Springer Nature, MDPI,} etc. and conferences like ICCIT, HONET, ICIEV, IJCCI, ICAEE, ICCAIE, ICSIPA, SCORED, ISIEA, APACE, ICOS, ISCAIE, BEIAC, ISWTA, IC3e, ISWTA, CoAST, icIVPR, ICSCT, 3ICT, DATA21, etc.
\end{IEEEbiography}


\EOD

\end{document}
