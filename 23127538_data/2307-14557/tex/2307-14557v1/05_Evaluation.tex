%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation}
\label{sec:evaluation}

In this section, we present the evaluation of \name. We begin by discussing our implementation setup and evaluation tools and follow with a comparison with both the CPU-based solutions as well as other hardware accelerators. We will quantitatively assess the performance benefits from \name. We then evaluate our bit mapping technique, with a focus on energy and area savings. Then we study the throughput per area performance of \name, demonstrating its superior performance over other SOTA CIM accelerators. Finally, we assess the scalability of \name and highlight its versatility in handling diverse polynomial degrees and bitwidths.

\subsection{Implementation Setup}
To verify the functionality of \name and evaluate performance characteristics such as latency, energy, and area, we have assembled a comprehensive evaluation framework. 

This framework considers the simulation of hardware components, including the modular reduction unit, shift-adders, accumulators and XBA arrays. We implemented the reduction unit, shift-adder, and accumulator using RTL, coded in Verilog and evaluated the energy consumption and area of these components using the RTL synthesis tool Cadence Encounter, paired with the 45nm CMOS predictive technology model (PTM) \cite{cao2011predictive}. We used Neurosim ~\cite{lu2021neurosim} to estimate the latency, energy, and area of the ReRAM-based XBAs, as well as successive-approximation-register (SAR) ADCs assuming the same 45nm technology node. The size of each XBA is 128 rows $\times$ 128 columns and one ADC is shared by 8 columns. 


We then incorporated the aftermentioned simulation-based results into our Python-based cycle-accurate simulator. This simulator tracks the pipeline stages for a given PMM operation and computes the cycle count and total energy consumption by emulating the operations of each hardware component on a cycle-by-cycle basis. This evaluation framework allows us to generate a holistic and precise assessment of the overall performance of a PMM in \name.


\begin{table*}[b]
%\vspace*{-2mm}
\centering
\caption{Comparison between \name and other SOTA solutions on a 256-point polynomial. Technology size: 45nm}
\label{tab:results}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c c}
\hline 
\hline
& \multicolumn{2}{c|}{PMM Solutions} & \multicolumn{4}{c|}{NTT Solutions (CIM)} & \multicolumn{3}{c}{NTT Solutions (Non-CIM)} \\\hline

Design   & \textbf{\name} & \textbf{CPU} & \textbf{RMNTT} & \textbf{BPNTT} & \textbf{MENTT}& \textbf{CryptoPIM} & \textbf{FPGA}& \textbf{LEIA} & \textbf{Sapphire}\\ 
\hline
Device   &   ReRAM & CMOS & ReRAM & SRAM & SRAM & ReRAM & CMOS & CMOS  & CMOS\\ 
Frequency (MHz) & 400 & 2.5k & 400 & 3.8K & 218 & 909 & 164 & 267 & 64\\
Bit width & 16 & 16 & 14 & 16 & 14 & 16 & 16 & 14 & 14\\
\hline
Area ($mm^2$) & \textbf{0.27}  & - & $0.76^{*}$ & 0.063 & 0.173 & 0.152 & - & 1.77 & 0.354 \\
Latency ($us$) & \textbf{0.32} & 56 & 0.44 & 61.9 & 15.9 & 68.7 & 24.3 & 0.6 & 20.1\\
Energy ($nJ$) & 308.07 & - & 429.91 & 69.4 & 47.8 & 2.6k & 3.1k & 44.1 & 236.3\\
Throughput\$ (KOP$/s$) & $3.1k$ & - & $2.2k$  & $258.6$ & $62.8$ & $553.3$ & $41.2$ & $1.7k$ & $49.7$   \\
Throughput/Area\$ (KOP$/s/mm^2$) & \textbf{11.4k} & - & $2.9k$  & $4.1k$ & $364$ & $3.6k$ & - & $940.6$ & $140.1$  \\
\hline
\hline
  \multicolumn{11}{@{}p{\linewidth}@{}}{\small * We estimate the area for RMNTT based on the information reported in the paper. We utilize the same XBA area and peripheral components as \name for the sake of comparison.} \\
  \multicolumn{11}{@{}p{\linewidth}@{}}{\small \$ We evaluate the throughput based on the type of operations performed in the corresponding accelerators. We report the throughput of PMM for both X-Poly and CPU as well as the throughput of NTT for other accelerators as reported in the literature.}
\end{tabular}

%\vspace*{-5mm}
\end{table*} 
 
\subsection{Comparison with SOTA Solutions}
\subsubsection{Comparison with CPU} We first compared our \name implementation with a CPU implementation that performs PMM with a SOTA C++ library (Number Theory Library version 11.5.1 \cite{ntl}). An Intel(R) Xeon(R) CPU E5-2680 v3 operating at 2.50GHz was used for the CPU implementation. The results are shown in Table~\ref{tab:results} (col 3). The latency of the \name design is 200$\times$ better than the CPU implementation. Performance enhancement is primarily due to the parallel compute capability and fast multiplication inherent in the XBAs in our CIM-based architecture, allowing for a much more efficient PMM execution.

\subsubsection{Comparison with other accelerators} Next, we compared \name with other SOTA accelerators. As current accelerators for PMM only use NTT, we compare our approach to SOTA accelerators that support NTT given a polynomial degree of 256. That said, NTT solutions require additional multiplications and the INTT to obtain final PMM results. Compared to \name, this may increase overall latency and energy consumption by 2$\times$. Moreover, with \name, we can generate PMM results in a single step without the need for additional multiplication or INTT.

\textbf{XBA solutions:} We first compared our implementation with other CIM solutions, specifically with ReRAM implementations. \edit{We scaled the latency and energy of~\cite{rmntt} to 45nm for a fair comparison to \name, following the methodology outlined in~\cite{bpntt}. Given that the study in~\cite{rmntt} did not provide area results, we carried out an estimation using the mapping methodology introduced in their publication. We assume the same area for XBAs and peripheral components as \name.}  Although our design exhibited similar latency to RMNTT, our improved mapping technique results in a significantly reduced area. That is mainly because we reduce the footprint of shift-adders to just 20\% of the original area by using the proposed BM technique, thereby leading to a 3.9$\times$ improvement in the throughput-per-area ratio. %Our improved mapping technique allows us to utilize the available resources more efficiently, thus reducing the number of computational elements required. 

\textbf{Compute in SRAM solutions:} We also compared our implementation with in-SRAM solutions. The \name approach also improves throughput and throughput-per-area. Again, this can be attributed to both the parallel computing capability and the fast multiplication feature of our XBA-type mapping technique, which enables us to perform multiple computations simultaneously. More specifically, throughput is improved by 11$\times$, and throughput-per-area is improved by up to 3$\times$.

\textbf{Non-CIM solutions:} Finally, we compared \name with non-CIM solutions. The \name design is advantageous as it can store entire polynomial coefficients inside the XBAs. This feature eliminated the need for frequent access to on-chip memory for coefficients in long polynomials, which reduces data movement between the computing unit and the on-chip memory. This results in a reduction in both latency and energy consumption. Overall, \name outperformed ASIC and FPGA solutions in terms of throughput and energy efficiency. Compared to SOTA FPGA implementations, \name can achieve a remarkable 75$\times$ throughput improvement. When compared against SOTA ASIC implementations, \name can achieve a 2$\times$ throughput improvement.

\subsection{Bit Mapping Study}
We now evaluate the energy and area benefits of the proposed BM technique, discussed in Sec.\ref{sec:design}. To evaluate performance, we consider two scenarios: (1) conventional mapping as the implementation of RMNTT, the SOTA XBA-based NTT accelerator, and (2) our proposed BM technique. Fig.\ref{fig:MappingBenefit} illustrates the shift-adder area/energy and ADC area/energy given various polynomial degrees for each mapping. Results suggest that due to the large polynomial degrees and high bitwidths associated with the PMM, the peripherals (such as the shift-adders) in the design with conventional mapping consume a significant proportion of the energy and area. Moreover, this escalates with polynomial degrees. However, our proposed BM technique decreases the area for shift-add operations by 80\%, leading to an additional 3$\times$ reduction in overall area. Moreover, compared to conventional mapping, our design has lower latency and energy consumption. 

Fig.~\ref{fig:breakdown} illustrates the area and energy breakdown of our proposed design. This analysis further reveals that the majority of the energy consumption and area is spent on ADC operations, with the proposed mapping technique reducing the energy and area consumption for other peripherals significantly. 

% Figure environment removed

% Figure environment removed

\subsection{Throughput per Area Study}
Table~\ref{tab:results} shows that the XBA-based solutions (\name and \cite{rmntt}) achieve higher throughput but require a larger area than the in-SRAM solution~\cite{bpntt}. This is due to the inherent design of XBA-based solutions: they require a more expansive area to accommodate an increase in both polynomial degree and bitwidth~\cite{rmntt}. In-SRAM solutions can support larger parameter sizes within a similar area. However, this is accompanied by a substantial reduction in throughput. However, \name reduces XBA area  while maintaining its high throughput. 

To further understand the trade-off between throughput and area, we conducted an analysis of the throughput per area performance and compare the results with other SOTA CIM solutions. We consider a range of polynomial degrees and bitwidths, to generate a comprehensive perspective regarding the strengths of our design.

Fig.~\ref{fig:throughputarea} illustrates the throughput per area performance of our design, as well as the SOTA XBA design in \cite{rmntt} and the in-SRAM design in \cite{bpntt}. Our results show that \name can achieve significantly better throughput-per-area performance than both of these solutions, even as the parameter size increases. This highlights how \name  can lead to decreased area consumption of the XBA-based solution without compromising  the throughput. \eat{Our new data mapping technique reduces the peripheral cost and high memory density of the emerging ReRAM device.}

% Figure environment removed


\eat{
% Figure environment removed}

% Figure environment removed

\subsection{Scalibility of \name}
\label{sec:memory_utilized}

Modern applications like HE in privacy-preserving machine learning often choose polynomials with a large degree and bitwidth\cite{HE_F1,GAZELLE}. Storing these entirely within XBAs demands a high number of arrays, leading to significant area usage and energy consumption. \eat{As shown in Fig.~\ref{fig:latency}, we observed an increasing trend in area and energy as we increased both polynomial degree and bitwidth in our design.}

Our polynomial mapping scheme (Sec~\ref{sec:poly_mapping}) allows us to reuse arrays. This enables us to employ a smaller number of XBAs to accommodate larger polynomials. However, reusing XBAs could potentially affect our design's latency. To address this, we conducted an experiment where given a fixed number of XBA arrays, we assessed the capability of \name to adapt to various polynomial degrees and bitwidths. The objective here was to determine how we could optimize  \name to maximize design throughput under different polynomial degrees and bitwidths constraints. 


The left graph in Fig.~\ref{fig:throughput} depicts the maximum throughput of \name using different numbers of XBAs. We considered polynomial degrees ranging from 256 to 2048, with a fixed bitwidth of 16. The right graph demonstrates the maximum throughput for different bitwidths ranging from 8 to 64, while maintaining a constant polynomial degree of 512. Our experiments highlight that our design is capable of managing a wide array of polynomial degrees and bitwidths while maintaining a fixed number of XBAs. As anticipated, higher degrees and bitwidths require longer computation times due to the necessity for reuse of the same arrays within the pipeline. By modifying the number of XBAs, we can manage the balance between area and throughput. Overall, our design showcases robust scalability, effectively adapting to a broad spectrum of polynomial degrees and bitwidths.



