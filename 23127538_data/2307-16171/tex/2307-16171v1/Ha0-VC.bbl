% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{qian2019autovc}
K.~Qian, Y.~Zhang, S.~Chang, X.~Yang, and M.~Hasegawa-Johnson, ``{AutoVC:
  Zero-shot Voice Style Transfer with Only Autoencoder Loss},'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2019, pp. 5210--5219.

\bibitem{yang22f_interspeech}
S.~Yang, M.~Tantrawenith, H.~Zhuang, Z.~Wu, A.~Sun, J.~Wang, N.~Cheng, H.~Tang,
  X.~Zhao, J.~Wang, and H.~Meng, ``{Speech Representation Disentanglement with
  Adversarial Mutual Information Learning for One-shot Voice Conversion},'' in
  \emph{Proc. Interspeech 2022}, 2022, pp. 2553--2557.

\bibitem{lee2022duration}
S.-H. Lee, H.-R. Noh, W.-J. Nam, and S.-W. Lee, ``{Duration Controllable Voice
  Conversion via Phoneme-Based Information Bottleneck},'' \emph{IEEE/ACM
  Transactions on Audio, Speech, and Language Processing}, vol.~30, pp.
  1173--1183, 2022.

\bibitem{bilinski22_interspeech}
P.~Bilinski, T.~Merritt, A.~Ezzerg, K.~Pokora, S.~Cygert, K.~Yanagisawa,
  R.~Barra-Chicote, and D.~Korzekwa, ``{Creating New Voices using Normalizing
  Flows},'' in \emph{Proc. Interspeech 2022}, 2022, pp. 2958--2962.

\bibitem{choi2023dddm}
H.-Y. Choi, S.-H. Lee, and S.-W. Lee, ``{DDDM-VC: Decoupled Denoising Diffusion
  Models with Disentangled Representation and Prior Mixup for Verified Robust
  Voice Conversion},'' \emph{arXiv preprint arXiv:2305.15816}, 2023.

\bibitem{kong2020hifi}
J.~Kong, J.~Kim, and J.~Bae, ``{HiFi-GAN: Generative Adversarial Networks for
  Efficient and High Fidelity Speech Synthesis},'' \emph{Advances in Neural
  Information Processing Systems}, vol.~33, pp. 17\,022--17\,033, 2020.

\bibitem{kim21f_interspeech}
J.-H. Kim, S.-H. Lee, J.-H. Lee, and S.-W. Lee, ``Fre-{GAN}: {A}dversarial
  {F}requency-{C}onsistent {A}udio {S}ynthesis,'' in \emph{Interspeech}, 2021.

\bibitem{jang21_interspeech}
W.~Jang, D.~Lim, J.~Yoon, B.~Kim, and J.~Kim, ``{UnivNet: A Neural Vocoder with
  Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform
  Generation},'' in \emph{Proc. Interspeech 2021}, 2021, pp. 2207--2211.

\bibitem{koizumi22_interspeech}
Y.~Koizumi, H.~Zen, K.~Yatabe, N.~Chen, and M.~Bacchiani, ``{SpecGrad:
  {D}iffusion {P}robabilistic {M}odel {B}ased {N}eural {V}ocoder with
  {A}daptive {N}oise {S}pectral {S}haping},'' in \emph{Interspeech}, 2022.

\bibitem{yoneyama2023source}
R.~Yoneyama, Y.-C. Wu, and T.~Toda, ``{Source-Filter HiFi-GAN: Fast and Pitch
  Controllable High-Fidelity Neural Vocoder},'' in \emph{ICASSP 2023-2023 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp. 1--5.

\bibitem{polyak21_interspeech}
A.~Polyak, Y.~Adi, J.~Copet, E.~Kharitonov, K.~Lakhotia, W.-N. Hsu, A.~Mohamed,
  and E.~Dupoux, ``{Speech Resynthesis from Discrete Disentangled
  Self-Supervised Representations},'' in \emph{Interspeech}, 2021.

\bibitem{kim2021conditional}
J.~Kim, J.~Kong, and J.~Son, ``{Conditional Variational Autoencoder with
  Adversarial Learning for End-to-End Text-to-Speech},'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2021, pp. 5530--5540.

\bibitem{liu22c_interspeech}
Y.~Liu, R.~Xue, L.~He, X.~Tan, and S.~Zhao, ``{DelightfulTTS 2: End-to-End
  Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders},'' in
  \emph{Proc. Interspeech 2022}, 2022, pp. 1581--1585.

\bibitem{choi2023nansy}
H.-S. Choi, J.~Yang, J.~Lee, and H.~Kim, ``{NANSY}++: Unified voice synthesis
  with neural analysis and synthesis,'' in \emph{The Eleventh International
  Conference on Learning Representations}, 2023.

\bibitem{qian2020f0}
K.~Qian, Z.~Jin, M.~Hasegawa-Johnson, and G.~J. Mysore, ``F0-consistent
  many-to-many non-parallel voice conversion via conditional autoencoder,'' in
  \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, 2020, pp. 6284--6288.

\bibitem{maimon2022speaking}
G.~Maimon and Y.~Adi, ``{Speaking Style Conversion With Discrete
  Self-Supervised Units},'' \emph{arXiv preprint arXiv:2212.09730}, 2022.

\bibitem{choi2021neural}
H.-S. Choi, J.~Lee, W.~Kim, J.~Lee, H.~Heo, and K.~Lee, ``{Neural Analysis and
  Synthesis: Reconstructing Speech from Self-Supervised Representations},''
  \emph{Advances in Neural Information Processing Systems}, vol.~34, pp.
  16\,251--16\,265, 2021.

\bibitem{lee2022hierspeech}
S.-H. Lee, S.-B. Kim, J.-H. Lee, E.~Song, M.-J. Hwang, and S.-W. Lee,
  ``{HierSpeech: Bridging the Gap between Text and Speech by Hierarchical
  Variational Inference using Self-supervised Representations for Speech
  Synthesis},'' in \emph{Advances in Neural Information Processing Systems},
  2022.

\bibitem{popov2022diffusionbased}
V.~Popov, I.~Vovk, V.~Gogoryan, T.~Sadekova, M.~S. Kudinov, and J.~Wei,
  ``{Diffusion-Based Voice Conversion with Fast Maximum Likelihood Sampling
  Scheme},'' in \emph{International Conference on Learning Representations},
  2022.

\bibitem{sadekova22_interspeech}
T.~Sadekova, V.~Gogoryan, I.~Vovk, V.~Popov, M.~Kudinov, and J.~Wei, ``{A
  Unified System for Voice Cloning and Voice Conversion through Diffusion
  Probabilistic Modeling},'' in \emph{Proc. Interspeech 2022}, 2022, pp.
  3003--3007.

\bibitem{mcauliffe2017montreal}
M.~McAuliffe, M.~Socolof, S.~Mihuc, M.~Wagner, and M.~Sonderegger, ``{Montreal
  Forced Aligner: Trainable Text-Speech Alignment Using Kaldi},'' in
  \emph{Interspeech}, vol. 2017, 2017, pp. 498--502.

\bibitem{min2021meta}
D.~Min, D.~B. Lee, E.~Yang, and S.~J. Hwang, ``{Meta-StyleSpeech: Multi-Speaker
  Adaptive Text-to-Speech Generation},'' in \emph{International Conference on
  Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  7748--7759.

\bibitem{lee2021multi}
S.-H. Lee, H.-W. Yoon, H.-R. Noh, J.-H. Kim, and S.-W. Lee,
  ``{Multi-SpectroGAN: High-Diversity and High-Fidelity Spectrogram Generation
  with Adversarial Style Combination for Speech Synthesis},'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~35, no.~14, 2021, pp. 13\,198--13\,206.

\bibitem{chung21_interspeech}
H.~Chung, S.-H. Lee, and S.-W. Lee, ``{Reinforce-Aligner: Reinforcement
  Alignment Search for Robust End-to-End Text-to-Speech},'' in \emph{Proc.
  Interspeech 2021}, 2021, pp. 3635--3639.

\bibitem{defossez2022high}
A.~D{\'e}fossez, J.~Copet, G.~Synnaeve, and Y.~Adi, ``{High Fidelity Neural
  Audio Compression},'' \emph{arXiv preprint arXiv:2210.13438}, 2022.

\bibitem{ren2022prosospeech}
Y.~Ren, M.~Lei, Z.~Huang, S.~Zhang, Q.~Chen, Z.~Yan, and Z.~Zhao,
  ``{ProsoSpeech: Enhancing Prosody With Quantized Vector Pre-training in
  Text-to-Speech},'' in \emph{ICASSP 2022-2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2022, pp. 7577--7581.

\bibitem{kim2022guided2}
S.~Kim, H.~Kim, and S.~Yoon, ``{Guided-TTS 2: A Diffusion Model for
  High-quality Adaptive Text-to-Speech with Untranscribed Data},'' \emph{arXiv
  preprint arXiv:2205.15370}, 2022.

\bibitem{zen2019libritts}
H.~Zen, V.~Dang, R.~Clark, Y.~Zhang, R.~J. Weiss, Y.~Jia, Z.~Chen, and Y.~Wu,
  ``{LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech},'' 2019,
  pp. 1526--1530.

\bibitem{veaux2017superseded}
C.~Veaux, J.~Yamagishi, K.~MacDonald \emph{et~al.}, ``{Superseded-cstr vctk
  corpus: English multi-speaker corpus for cstr voice cloning toolkit},'' 2017.

\bibitem{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter, ``{Decoupled Weight Decay Regularization},'' in
  \emph{International Conference on Learning Representations}, 2019.

\bibitem{lee2021voicemixer}
S.-H. Lee, J.-H. Kim, H.~Chung, and S.-W. Lee, ``{VoiceMixer: Adversarial Voice
  Style Mixup},'' \emph{Advances in Neural Information Processing Systems},
  vol.~34, pp. 294--308, 2021.

\bibitem{casanova2022yourtts}
E.~Casanova, J.~Weber, C.~D. Shulby, A.~C. Junior, E.~G{\"o}lge, and M.~A.
  Ponti, ``{YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice
  Conversion for everyone},'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022, pp. 2709--2720.

\bibitem{radford2022robust}
A.~Radford, J.~W. Kim, T.~Xu, G.~Brockman, C.~McLeavey, and I.~Sutskever,
  ``{Robust Speech Recognition via Large-scale Weak Supervision},'' \emph{arXiv
  preprint arXiv:2212.04356}, 2022.

\bibitem{kwon2021ins}
Y.~Kwon, H.~S. Heo, B.-J. Lee, and J.~S. Chung, ``{The ins and outs of speaker
  recognition: lessons from VoxSRC 2020},'' in \emph{ICASSP 2020-2020 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}, 2021.

\end{thebibliography}
