% ****** Start of file aipsamp.tex ******
%
%   This file is part of the AIP files in the AIP distribution for REVTeX 4.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 American Institute of Physics.
%
%   See the AIP README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex  aipsamp
%  2)  bibtex aipsamp
%  3)  latex  aipsamp
%  4)  latex  aipsamp
%
% Use this file as a source of example code for your aip document.
% Use the file aiptemplate.tex as a template for your document.
\documentclass[%
 aip,
 jmp,%
 amsmath,amssymb,
%preprint,%
 reprint,%
author-year,%
%author-numerical,%
]{revtex4-2}
\usepackage{lineno}
\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[]{float}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{comment}



%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
\usepackage[table]{xcolor}

\def\marg{1ex}

\renewcommand\familydefault{cmss}
\newcommand{\popI}{\mathcal{P}_1}
\newcommand{\popII}{\mathcal{P}_2}
\newcommand{\NI}{\mathcal{N}_1}
\newcommand{\NII}{\mathcal{N}_2}
\newcommand{\PI}{\text{P}_1}
\newcommand{\PII}{\text{P}_2}
\newcommand{\T}{\mathcal{T}}
\newcommand{\rh}{\nu_\text{h}}
\newcommand{\rl}{\nu_{\ell}}
\newcommand{\rhI}{\nu_\text{h,1}}
\newcommand{\rlI}{\nu_{\ell,1}}
\newcommand{\rhII}{\nu_\text{h,2}}
\newcommand{\rlII}{\nu_{\ell,2}}
\newcommand{\rt}{\nu_\text{t}}
\newcommand{\rtI}{\nu_\text{t,1}}
\newcommand{\rtII}{\nu_\text{t,2}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Wb}{\mathcal{W}_\text{b}}
\newcommand{\Wc}{\mathcal{W}_\text{c}}
\newcommand{\SII}{\mathcal{S}_\text{2}}
\newcommand{\Sb}{\mathcal{S}_\text{b}}
\newcommand{\varSb}{\sigma^{2}_\text{b}}


\newcommand{\ms}{\,\text{ms}}
\newcommand{\MOhm}{\,\text{M}\Omega}
\newcommand{\mV}{\,\text{mV}}
\newcommand{\nS}{\,\text{nS}}
\newcommand{\pA}{\,\text{pA}}
\newcommand{\pF}{\,\text{pF}}
\newcommand{\sps}{\,\text{spikes/s}}


\begin{document}
%\linenumbers



\preprint{AIP/123-QED}

\title{A theoretical framework for learning through structural plasticity}
%\thanks{Footnote to title of article.}

\author{Gianmarco Tiddia}
\email{gianmarco.tiddia@dsf.unica.it}
\affiliation{Department of Physics, University of Cagliari, Italy}
\affiliation{Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Cagliari, Cagliari, Italy}
\author{Luca Sergi}%
\affiliation{Department of Physics, University of Cagliari, Italy}

\author{Bruno Golosio}
\affiliation{Department of Physics, University of Cagliari, Italy}
\affiliation{Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Cagliari, Cagliari, Italy}


\date{9 August 2023}

\begin{abstract}
A growing body of research indicates that structural plasticity mechanisms are crucial for learning and memory consolidation.
Starting from a simple phenomenological model, we exploit a mean-field approach to develop a theoretical framework of learning through this kind of plasticity, capable of taking into account several features of the connectivity and pattern of activity of biological neural networks, including probability distributions of neuron firing rates, selectivity of the responses of single neurons to multiple stimuli, probabilistic connection rules and noisy stimuli. More importantly, it describes the effects of consolidation, pruning and reorganization of synaptic connections. This framework will be used to compute the values of some relevant quantities used to characterize the learning and memory capabilities of the neuronal network in a training and validation procedure as the number of training patterns and other model parameters vary. The results will then be compared with those obtained through simulations with firing-rate-based neuronal network models.
\end{abstract}

\keywords{computational neuroscience, structural plasticity, memory, firing rate models}%Use showkeys class option if keyword
                              %display desired
\maketitle

\section{\label{sec:introduction} Introduction}
Together with temporary and reversible changes of synaptic efficacy such as short and long-term plasticity mechanisms, structural changes in the synaptic morphology of the network are fundamental mechanisms that take place in healthy brains. These changes occurs at longer time scales than the short or long-term mechanisms mentioned above, and consist in the consolidation, creation of new synapses or erasure of synapses that have not been consolidated. This type of synaptic plasticity, which is called \textit{structural plasticity}, can be spontaneous, but also experience-based \citep{Butz2009}, and it has a key role in the stabilization of new concepts that has to be kept in memory after learning \citep{Fu2011}.\\
Indeed, it is known that neurotransmitters can be neurotrophic factors, i.e. participate in growth or suppression of dendritic spines, synapses, axon and dendrites \citep{Mattson1988, Lamprecht2004, Richards2005}. Thus, structural plasticity is a neural-activity-driven mechanism, which can increase or decrease the number of synapses. Such modifications are then flanked by an homeostatic kind of structural plasticity, which has a balancing effect achieved with adding or removing synapses, as described in \citep{Fauth2016}.\\
Moreover, the number of synapses in the brain can change over time. In \cite{Huttenlocher1979} it is shown that synaptic density reaches the highest values at $1$-$2$ years age, it drops during adolescence and stabilizes between age $16$-$72$, followed by a slight decline. However, although synaptic density remains approximately stable during adulthood, rewiring of network connections occurs as well in order to efficiently store new memories. The increase of connections in high activity regions (and vice versa), together with the rearrangement of synapses leads to a fine-tuning of the brain's circuits \citep{sakai2020}, because on one hand some synapses can be strengthened through long-term potentiation (LTP) and new connections can be formed next to the already potentiated ones to further enhance synaptic transmission, on the other hand when the presynaptic and postsynaptic neuron activities have low correlation their connection are likely to be removed. The latter process is called synaptic pruning and it is considered essential for optimizing activity propagation and memory capacity\citep{Chklovskii2004, Knoblauch2014, Knoblauch2016}.
Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia \citep{Bourgeron2009,Moyer2015}, leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects\citep{Hutsler2010,Pagani2021,Glantz2000}.\\
Regarding computational modelling, in the last decades computational neuroscience has mostly focused on plasticity models that involve strengthening or weakening of existing synapses, like spike timing-dependent plasticity (STDP) \citep{Gutig2003} or short-term plasticity (STP) \citep{tsodyks1998} and on their role in long-term, short-term, working memory and learning \citep{Song2000, Bi2001, ThaCo, Mongillo2008, Tiddia2022_WM}. Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. \cite{Knoblauch2014} and \cite{Knoblauch2016} describe a model of structural plasticity based on "effectual connectivity", defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network's connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas changes only related to the synaptic weight are described by the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity and it is possible to explain some cognitive mechanism such as the spacing effect \citep{Knoblauch2014}.\\
\cite{Spiess2016} simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus making the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented.\\
Some new insights about the importance of synaptic pruning are also shown in \cite{Navlakha2015}, in which different pruning rates were studied suggesting that a slowly decreasing rate of pruning over time leads to more efficient network architectures.\\
As discussed above, the biochemical and biophysical mechanisms underlying structural plasticity are extremely complex and only partially understood to date. For this reason, rather than attempting to build a biologically detailed model, this work exploits a relatively simple phenomenological model, including both the activity-driven and the homeostatic contributions; despite the lower complexity, this model accounts for the effects of structural plasticity in terms of the consolidation of synaptic connections between neurons with a high activity correlation as well as those of pruning and rewiring the connections for which this correlation is lower. This approach is also justified by the requirement for a simple and effective computational model suitable for simulating networks with a relatively large number of neurons and connections and for representing learning processes with sizable numbers of training and validation patterns.
This model will then serve as the foundation for the creation of a mean-field-based theoretical framework for learning through synaptic plasticity capable of accounting for a variety of biological network properties. This framework will be used in a training and validation procedure to characterize learning and memory capacity of plastic neuronal networks as the number of training patterns and other model parameters vary. The results will then be compared with those obtained through simulations based on firing-rate-based neuronal networks.
The model considers two populations of neurons, $\popI$ and $\popII$, with synaptic connections directed from the first to the second population.
During the training phase, $\popI$ receives an input stimulus, while $\popII$ receives a contextual stimulus.
The model assumes that synaptic consolidation is a probabilistic process driven by pre- and postsynaptic spiking activity. For each pattern given in input to the model, only a small fraction of neurons contribute to this process.
Connection consolidation is complemented by a rewiring process: connection pruning, which eliminates unconsolidated connections, and creation of new connections, which restores network balance.
This framework does not specifically refer to a particular region of the brain; rather, it is built on characteristics that are ubiquitous for several brain areas.\\
The proposed approach is capable of accounting for different probabilistic connection rules, firing rate probability distributions, presence of noise in stimuli, thus providing a general framework to study the impact of structural plasticity in learning on large-scale neuronal network models.

\section{\label{sec:methods} Materials and Methods}
Here we introduce the theoretical framework for the structural plasticity model and the firing-rate-based network model used to validate it and estimate its learning capability. Firstly, we present the general model of two neuron populations, and secondly the theoretical framework is divided into two possible approaches: a simple, discrete rate model in which neurons can only assume two possible firing rates values and a more realistic continuous rate model in which firing rates follow a continuous probability distribution. Finally, we introduce the computational simulation framework.

\subsection{\label{subseq:rate_model} Two neuron populations model}
The neuronal network model used in this work consists of two neuron populations, $\popI$ and $\popII$, consisting of $10^5$ neurons each. This number corresponds roughly to the number of neurons in $1 \text{ mm}^2$ of cerebral cortex. The exchange of information between the two populations takes place through the connections from the population $\popI$ to the population $\popII$, which in the model are on average $5\cdot 10^3 $ per neuron of $\popII$, for a total of $5\cdot 10^8 $ connections. Each connection has an initial synaptic weight of $\Wb$. The first population ($\popI$) receives an input stimulus (e.g., visual) mimicked by the activation of the neurons with a firing rate pattern associated to it.
During the training stage, in addition to the signal from $\popI$, the second population receives another stimulus (e.g., auditory), that we identify as a \textit{contextual stimulus}. Figure \ref{fig:net_scheme} depicts a simple scheme of the network. 
Structural plasticity is modelled following the categories proposed by \cite{Fauth2016}, i.e., activity-dependent and homeostatic. The firing rate patterns of the two populations have a role in the activity-dependent structural plasticity.
The consolidation of a synaptic connection occurs when 
the firing rates of both the presynaptic and the postsynaptic neurons are concurrently above a certain threshold (the definition of which will vary depending on the model).
The synaptic weight of a consolidated connection increases from $\Wb$ to a value $\Wc > \Wb$. 
Once a synaptic connection has been consolidated, it will maintain the synaptic weight $\Wc$, without the possibility of returning to the initial state $\Wb$. This approach is a computationally-effective way of representing the biological structural changes that make a consolidated connection strong and durable. The sets of input and contextual patterns used for network training are independent firing-rate patterns of the two populations randomly generated from predefined firing rate probability distributions, which can be either discrete or continuous. The training process is performed using $\T$ independent input patterns, together with the corresponding contextual stimuli. A diagram of the training and validation processes is shown in Figure \ref{fig:net_scheme}. During training, when both input and contextual stimulus are used, a fraction of the neurons in the population $\popII$ will assume a rate above threshold.
These neurons, called {\it coding}, or {\it selective} neurons, play a vital role in input coding. The existence of neurons showing selective firing rate in response to specific stimuli is largely confirmed by experimental results. The average input signal to these neurons will be called $\langle \SII \rangle$. The non-selective neurons of $\popII$ will instead be called {\it non-coding} or {\it background} neurons, and their average input signal will be indicated with $\langle \Sb \rangle$.
The proposed model accounts for the ability of the network to learn the association between input patterns and the corresponding contextual stimuli.

% Figure environment removed

We may distinguish two different types of models based on the potential values that the neuron firing rate can assume:
\begin{itemize}
     \item \textbf{Discrete model}
     the rates of the neurons can assume only two discrete values $\rl$ (low rate) and $\rh$ (high rate);
     \item \textbf{Continuous model}
     the rate of neurons can assume values from a continuous probability distribution.
\end{itemize}
In this work, in agreement with the experimental observations, a log-normal distribution of the firing rate has been used for the continuous model
\citep{Roxin2011}. In the next section we will derive the mean-field equations for these two models, which are summarized in the following tables.


\begin{table}[H]
%\nolinenumbers
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{\textbf{Summary}}\\
  \hline 
  \textbf{Populations} & $\popI$, $\popII$ \\
  \hline 
  \textbf{Connectivity} & sparse random connectivity\\
  \hline 
  \textbf{Neurons} & firing-rate-based models of point-like neurons
  \\
  \hline 
  \textbf{Synapses} & structural plasticity \\
  \hline 
  \textbf{Input} & firing rate pattern extracted from a discrete or continuous probability distribution  \\
  \hline 
\end{tabular}
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.4\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.4\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{\textbf{Populations}}\\
  \hline 
  \textbf{Name} & \textbf{Elements} & \textbf{Size}\\
  \hline 
  $\popI$  & point-like neurons & $\NI$\\
  \hline 
  $\popII$  & point-like neurons & $\NII$\\
  \hline 
\end{tabular}
\caption{Description of the network model (continued on next page).}
\label{tab:model_description}
\end{table}
\addtocounter{table}{-1}
\begin{table}[H]
%\nolinenumbers
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{\textbf{Neuron }}\\
  \hline 
  \textbf{Type} & firing-rate-based neuron model\\
  \hline 
  \textbf{Firing rate distribution} & 
    \begin{itemize}
    \item discrete model: the firing rates can assume only two discrete values $\rh$ (high rate) and $\rl$ (low rate);
    \item continuous model: the firing rates can assume values from a continuous probability distribution; this work uses a lognormal distribution
    \end{itemize}\\
  \hline 
\end{tabular}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% 
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Connectivity}
  }\\
  \hline 
  \textbf{Source} & \textbf{Target} & \textbf{Pattern}\\
  \hline
  $\popI$ & $\popII$ & %
                      \begin{itemize}
                      \item random, independent; in-degrees (i.e., incoming connections) can be homogeneous, with a fixed number of $\C$ connections per neuron of $\popII$, or driven by a Poisson distribution;
                      \item synaptic weights are $\Wb$ for unconsolidated connections and $\Wc$ for consolidated ones, with $\Wc>\Wb$;
                      \item multiple connections between the same couple of presinaptic and postsinaptic neurons (``multapses'') are allowed by default, but they can be disabled.
                      \end{itemize}\\
  % \hline
  % all & all & no self-connections (``autapses''), multiple connections (``multapses'') are permitted\\
  \hline

\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 

  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Synapse}
  }\\
  \hline 
  \textbf{Type} & structural plasticity\\
  \hline 
  \textbf{Description} & initial synaptic weight are set to $\Wb$ for all the instantiated connections;
  when a training pattern is used, considering a connection between a $\popI$ neuron $i$ and a $\popII$ neuron $j$:
                         \begin{itemize}
                            \item (discrete model) $\Wb \rightarrow \Wc$ if $\nu_\text{i}=\nu_\text{j}=\rh$
                            \item (continuous model) $\Wb \rightarrow \Wc$ if $\nu_\text{i}>\rtI$ and $\nu_\text{j}>\rtII$
                         \end{itemize}
                        Once a connection is consolidated, it cannot return to the initial weight.
  \\
  \hline 
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 

  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Connection rewiring}
  }\\
  \hline 
  \textbf{Description} & periodically, unconsolidated connections are pruned, and new connections are created: if $h$ is the number of consolidated incoming connections of a neuron of $\popII$, $\C - h$ new connections will be created, were $\C$ is a fixed number if the fixed-indegree connection rule is used, while it is extracted from a Poisson distribution if the Poisson-indegree rule is selected; in both cases, the presynaptic neurons are randomly extracted from $\popI$.
  \\
  \hline 
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Input stimulus}
  }\\
\hline 
\textbf{Description} & firing rate pattern of the neurons of $\popI$ selected from the training or from the test set.\\
\hline 
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Contextual stimulus}
  }\\
\hline 
\textbf{Description} & firing rate pattern of the neurons of $\popII$ selected from the training set; used only in the training phase.\\
\hline 
\end{tabular}
\caption{Description of the network model (continued on next page).}
\end{table}
\addtocounter{table}{-1}
\begin{table}[H]
%\nolinenumbers
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Train set}
  }\\
\hline 
\textbf{Type} & set of $T$ independent firing-rate patterns of the neurons of $\popI$ (input stimulus) and $\popII$ (contextual stimulus) \\
\hline 
  \textbf{Description} &
  each pattern is randomly generated from predefined firing rate probability distributions, which can be either discrete or continuous:
    \begin{itemize}
    \item discrete model: the firing rates can assume only two discrete values $\rh$ (high rate) and $\rl$ (low rate) with probabilities $p_1$ and $q_1 = 1 - p_1$ in the $\popI$ population, $p_2$ and $q_2 = 1 - p_2$ in the $\popII$ population.  
    \item continuous model: the firing rates can assume values on the basis of a continuous probability distribution; this work uses a lognormal distribution.
    \end{itemize}\\
\hline
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Test set}
  }\\
\hline 
\textbf{Type} & set of $V$ firing-rate patterns of the neurons of $\popI$ (input stimulus) \\
\hline 
  \textbf{Description} &
  each pattern is randomly extracted from the train set and eventually altered by adding noise from a predefined distribution:
    \begin{itemize}
    \item discrete model: the pattern is left unchanged;
    \item continuous model: the firing rate of each neuron is modified by adding a random deviation extracted from a predefined probability distribution; in this work we used a truncated Gaussian distribution. 
    \end{itemize}\\
\hline
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\caption{Description of the network model (continued).}
\end{table}

\def\widthA{0.1}
\def\widthB{0.25}
\def\widthC{0.65}
\begin{table}[H]
%\nolinenumbers
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline 
    \multicolumn{3}{|>{\columncolor{white}}c|}{\textbf{Network and connectivity}}\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    \hline 
    $\NI$ & $100000$ & number of neurons of $\popI$\\
    \hline 
    $\NII$ & $100000$ & number of neurons of $\popII$\\
    \hline 
    $\C$ & $5000$ & number of connection in-degrees per neuron of $\popII$\\
    \hline 
    $\T$ & variable & number of training patterns\\
    \hline 
    $s$ & 100 & connection rewiring step\\
    \hline 
    \end{tabular}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline
    \multicolumn{3}{|>{\columncolor{white}}c|}{
    \textbf{Neuron}
    }\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    \hline 
    $\rl$ & $2.0\sps$ & low firing rate\\
    \hline 
    $\rh$ & $50\sps$ & high firing rate\\
    \hline 
    \end{tabular}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline
    \multicolumn{3}{|>{\columncolor{white}}c|}{
    \textbf{Synapse}
    }\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    %%%%%%%%%%%%%%%%%%%%
    %% transmission
    \hline
    $\Wb$ & $0.1$\,pA & baseline synaptic weight\\
    \hline
    $\Wc$ & $1$\,pA & consolidated synaptic weight \\
    \hline
    \end{tabular}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline
    \multicolumn{3}{|>{\columncolor{white}}c|}{
    \textbf{Stimulus}
    }\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    %%%%%%%%%%%%%%%%%%%%
    %% transmission
    \hline
    $p_1$ & $0.001$ & probability for a neuron of $\popI$ of having high rate when an input stimulus is injected\\
    \hline
    $p_2$ & $0.001$ & probability for a neuron of $\popII$ of having high rate when a contextual stimulus is injected \\
    \hline
  \end{tabular}
  \caption{Model parameters.}
  \label{tab:model_parameters}
\end{table}

\subsubsection{\label{subsubseq:rate-discr} Discrete rate model}
As previously mentioned, in this model the input and contextual stimuli are represented by discrete firing-rate patterns of the two populations, $\popI$ and $\popII$ respectively, in which the firing rate of each neuron can assume only two possible values, $\rh$ (high rate ) or $\rl$ (low rate). Each training example consists of two patterns, one representing the input stimulus to the population $\popI$,
the other representing the contextual stimulus to the population $\popII$. The pattern representing the input stimulus is generated by randomly extracting the firing rate of each neuron of $\popI$ from the two values, $\rh$ and $\rl$, with probabilities $p_1$ and
$q_1 = 1 – p_1$, respectively. The corresponding pattern for the contextual stimulus is generated in a similar way, extracting the values of the firing rates of the $\popII$ neurons, $\rh$ or $\rl$, with probabilities $p_2$ and
$q_2 = 1 – p_2$, respectively.
The mean number of high-rate neurons in the two populations will be:
\begin{equation}
\label{eq:Nh}
\begin{split}
     N_{h,1}&=\NI p_1 \\
     N_{h,2}&=\NII p_2
     \end{split}
\end{equation}
where $\NI$ and $\NII$ indicate the number of neurons of $\popI$ and $\popII$, respectively.
A connection will be consolidated in a training example if both the presynaptic and the postsynaptic neuron assume a high firing rate $\rh$.
The probability that a generic connection is consolidated in a single example is $p_1 p_2$, and thus the probability that it is not consolidated after $\T$ training examples is
$(1-p_1p_2)^{\T}$.
The probability $p$ that a connection is consolidated in at least one of the $\T$ training examples is given by the complement of the previous expression:
\begin{equation}
\label{eq:p}
     p= 1-(1-p_1 p_2)^\T
\end{equation}
The product of this expression by the number of incoming connections per neuron $\C$ gives us the average number of consolidated connections per neuron:
\begin{equation}
\label{eq:av_k}
     \langle k \rangle=\C \Bigl[ 1 - (1-p_1p_2)^{\T} \Bigr] = \C p
\end{equation}
For each neuron we will therefore have on average $\langle k \rangle$ consolidated connections with synaptic weight $\Wc$ and $\C- \langle k \rangle$ unconsolidated connections with synaptic weight $\Wb$.

The test set consists of $V$ firing-rate patterns of the neurons of $\popI$, randomly extracted from the $\T$ input patterns of the train set. In the discrete rate model, the pattern are unaltered, thus each input patter of the test set is identical to an input pattern of the train set.
The contextual stimuli are not used in the validation phase.

The average rate of the neurons in the population  $\popI$ is
\begin{equation}
\label{eq:av_r}
    \langle \nu \rangle = p_1\rh + (1-p_1)\rl
\end{equation}
The input signal targeting a non-coding neuron of $\popII$ (i.e., a background neuron) is equal to the weighted sum of the signals coming from the $\C$ connections:
\begin{equation}
\label{eq:Sb}
   \Sb = \Wc \sum_{i=1}^{ k}\nu_i +\Wb \sum_{i=1}^{\C- k } \xi_i
\end{equation}
where $\C$ is the number of incoming connections, $k$ is the number of consolidated connections, $\nu_i$ are the firing rates of the neurons connected to the consolidated connections and $\xi_i$ are the firing rates of the neurons connected to the unconsolidated connections.
From the linearity of $\Sb$ with respect to $\nu_i$ and $\xi_i$ and from the fact that the rates of presynaptic neurons have the same mean value $\langle \nu \rangle$, it follows that
\begin{equation}
\label{eq:av_sb}
    \langle \Sb \rangle = [ \Wc \langle k \rangle 
    +  \Wb (\C - \langle k \rangle) ] \langle \nu \rangle
\end{equation}
In this equation, we can clearly observe two distinct contributions: one related to consolidated connections, which depends on the mean value $\langle k \rangle$, and the other related to unconsolidated connections, which depends on $\C - \langle k \rangle$.
From this result we can now calculate the variance on the background signal, which is defined as:
\begin{equation}
     \sigma_{b}^{2}=\langle (\Sb-\langle \Sb \rangle)^2 \rangle
\end{equation}
Using Equations \ref{eq:Sb} and \ref{eq:av_sb}, we can compute the variance as:
\begin{equation}\label{eq:discrprelim}
    \sigma_{b}^{2}= \langle \Bigl[ \Wc \sum_{i=1}^{k}\nu_i +\Wb \sum_{i=1}^{\C-k} \xi_i 
    - [ \Wc \langle k \rangle 
    +  \Wb (\C - \langle k \rangle) ] \langle \nu \rangle \Bigr]^2 \rangle
\end{equation}
Taking advantage of the equality $\langle k \rangle=k+(\langle k \rangle-k)$, we can rewrite:
\begin{equation}
   \Wc \langle k \rangle
    + \Wb (\C - \langle k \rangle)
    = \Wc k + \Wc (\langle k \rangle - k) + \Wb \Bigl[(\C-k) + (k-\langle k \rangle ) \Bigr] =
\end{equation}
\begin{equation*}
     =\Wc k + \Wb (\C- k) + \Wc (\langle k \rangle - k)  + \Wb (k - \langle k \rangle)
\end{equation*}
Inserting this last expression in Equation \ref{eq:discrprelim} and rewriting the terms with the multiplicative factors $k$ and $\C - k$ with summations, such as for example $\Wc k \langle \nu \rangle = \Wc \sum_{i=1}^{k} \langle \nu \rangle$, we obtain:
\begin{equation}
\label{eq:var_sb2}
     \sigma_{b}^2 = \langle \Bigl[ \Wc \sum_{i=1}^k (\nu_i - \langle \nu \rangle) +\Wb \sum_{i=1}^ {\C-k} (\xi_i - \langle \nu \rangle) + (k-\langle k \rangle)(\Wc-\Wb)\langle \nu \rangle \Bigr]^2 \rangle
\end{equation}
Taking into account that the mixed terms go to zero since $\sum_{i} \langle(x_i - \langle x \rangle) \rangle=0$, setting $\sum_{i} (x_i - \langle x \rangle)^ 2=\sigma^{2}_{x}$, we will have that:
\begin{equation}
\label{variance_sigmab}
     \sigma_{b}^2= \Bigl[\Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2
\end{equation}
where $\sigma_{k}^2 = \langle (k- \langle k\rangle )^2 \rangle$.
In the previous formula we note two contributions depending respectively on the variance of the firing rate and on the variance of the number of consolidated connections.
The value of the variance of $k$ is not shown here, but is derived in Appendix \ref{app:var_k}, whereas the variance of the rate is, by definition, $\sigma^2_{\nu}=\langle\nu^2\rangle - \langle\nu\rangle^2$. \\
Now we estimate the average input to a coding neuron of $\popII$. The neuron receives signals from neurons of $\popI$ coming from both consolidated and unconsolidated connections.
If $\C$ is the number of incoming connections, the average number of high-rate presynaptic neurons will be $p_1 \C$, while those with low rate will be on average $\C' = \C (1 - p_1)$.
Since the input pattern used for validation is identical to the corresponding training pattern, the $p_1 \C$ connections coming from high rate neurons will certainly be consolidated.
The remaining $\C'$ connections come from neurons of $\popI$ at low rate, however they may have been consolidated in other training examples.
The average number of consolidated connections
from low-rate neurons can be calculated using Equation \ref{eq:av_k}:
\begin{equation}
\label{eq:av_k1}
    \langle k' \rangle = \C' p = \C(1-p_1) p = \langle k \rangle (1 - p_1)
\end{equation}
Summing up, the average value of $\SII$ is
\begin{equation}
\label{eq:s2}
\begin{split}
    \langle \SII \rangle &= \Wc \C p_1 \rh + \Wc \langle k' \rangle \rl + \Wb (\C' - \langle k' \rangle) \rl =\\
    &= \Wc \C p_1 \rh + \Wc \langle k \rangle (1-p_1) \rl + \Wb (\C - \langle k \rangle) (1-p_1) \rl =\\
    &= \Wc \C p_1 \rh + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb  \Bigr] (1-p_1)\rl
\end{split}
\end{equation}

The previous formula does not consider the rewiring of the connections; the effect of rewiring will be described in Section \ref{subseq:rewiring}, where we will derive the expression of $\SII$ that takes it into account.
We identify this case as "with rewiring" to distinguish it from the case in which unconsolidated connections are not pruned and rewired. Indeed, this distinction is useful to estimate the contribution of this mechanism on the input signal on coding neurons of $\popII$.

Now, it is possible to obtain the signal-difference-to-noise-ratio (SDNR) using the formula
\begin{equation}
\label{eq:SDNR}
    \text{SDNR} = \dfrac{|\langle \SII \rangle - \langle \Sb \rangle |}{\sigma_b}
\end{equation}

In this work the SDNR is calculated on the input signal to the coding and non-coding neurons of the $\popII$ population due to the connections coming from the $\popII$ population, rather than on the firing rates of the $\popII$ neurons.
This choice is justified by the need to evaluate the memory capacity associated with the plasticity of the connections from $\popI$ to $\popII$.
In general, in addition to the signal from the $\popI$ population, the $\popII$ neurons will receive other excitatory and inhibitory signals in input.
In rate-based models, the response of neurons to the overall input signal
is generally described by an activation function that expresses the firing rate as a function of that signal.
A common choice is the threshold-linear (or ReLU) function

\begin{equation}
\label{eq:ReLu}
\Phi(x) = \alpha \text{max}\{ 0, x\}
\end{equation}

where $\alpha$ is a multiplicative coefficient. With this choice,
the average rates of coding and non-coding neurons of $\popII$ can be written as

\begin{equation}
\begin{split}
\langle\nu_\text{c}\rangle &=
    \Phi(\langle\SII\rangle + \langle S_\text{o}\rangle - S_\text{thresh}) \\
\langle\nu_\text{nc}\rangle &=
    \Phi(\langle\Sb\rangle + \langle S_\text{o}\rangle - S_\text{thresh})
\end {split}
\end{equation}

where $\langle S_\text{o}\rangle$ is the average input signal from (excitatory and/or inhibitory) neuron populations
different from $\popI$ and $S_\text{thresh}$ is the activation threshold.
Assuming that the total input signal is above the threshold for both coding and non-coding neurons, the average rates will be linear functions of the input signals:

\begin{equation}
\begin{split}
\langle\nu_\text{c}\rangle &=
    \alpha(\langle\SII\rangle + \langle S_\text{o}\rangle - S_\text{thresh}) \\
\langle\nu_\text{nc}\rangle &=
    \alpha(\langle\Sb\rangle + \langle S_\text{o}\rangle - S_\text{thresh})
\end {split}
\end{equation}

while the variance of the non-coding neuron rate will be

\begin{equation}
\sigma_\text{nc}^2 = \alpha^2 (\sigma_\text{b}^2 + \sigma_\text{o}^2)
\end{equation}

The SDNR calculated on the rate will therefore be

\begin{equation}
\label{eq:SDNRnu}
    \text{SDNR}_\nu = 
    \dfrac{|\langle\nu_\text{c}\rangle - \langle\nu_\text{nc}\rangle|}
    {\sigma_\text{nc}} =
        \dfrac{|\langle \SII \rangle - \langle \Sb \rangle |}
        {\sqrt{\sigma_\text{b}^2 + \sigma_\text{o}^2}}
\end{equation}

which has an expression similar to that reported in Equation \ref{eq:SDNR}, with the only difference that there is an additional contribution to the noise due to the signal coming from other populations.

It should also be noted that the definitions of SDNR reported in Equations \ref{eq:SDNR} and \ref{eq:SDNRnu} refer to the mean signal difference between single coding and non-coding neurons. However, the overall memory capacity of the synaptic connections between the two populations can be best quantified by the SDNR evaluated on the total input signal to coding neurons and to an equivalent number of non-coding neurons.
Calling $N_{h,2}$ the mean number of coding neurons in the population $\popII$, we can define

\begin{equation}
\label{eq:SDNRpop}
\text{SDNR}_\text{pop} = 
    \dfrac{|
    N_{h,2}
    \langle \SII \rangle - N_{h,2}
    \langle \Sb \rangle |}
        {\sqrt{N_{h,2}}
        \sigma_\text{b}} =
    \dfrac{\sqrt{p_2 \NII}
    |\langle \SII \rangle -
    \langle \Sb \rangle |}
        {\sigma_\text{b}}
\end{equation}

where we used Equation \ref{eq:Nh} and $\sqrt{N_{h,2}}\sigma_\text{b}$ is the standard deviation of the total input signal to $N_{h,2}$ non-coding neurons.
Thus, $\text{SDNR}_\text{pop}$ scales with the square root of $p_2 N_2$.

Table \ref{tab:discrete_model} summarizes the equations of the discrete rate model.

\begin{table}[H]
%\nolinenumbers
    \centering
    \renewcommand\arraystretch{1.5}
    \begin{tabular}{
  |@{\hspace*{\marg}}m{0.3\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.6\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Discrete rate model}
  }\\
  \hline 
  \textbf{Name} & \textbf{Symbol} & \textbf{Equation}\\
  \hline
  Rate mean & $\langle \nu \rangle$ & $p_1\rh + (1-p_1)\rl$\\
    \hline
  Rate variance & $\sigma_{\nu}^2$ & $\Big( p_1\rh^2 + (1-p_1)\rl^2 \Big)-\Big(p_1\rh + (1-p_1)\rl\Big)^2$\\
  \hline
  Average background signal & $\langle \Sb \rangle$ & $\langle k \rangle \Wc \langle \nu \rangle + (\C - \langle k \rangle) \Wb \langle \nu \rangle$\\
  \hline
  Variance of background signal & $\varSb$ & $\Bigl[\Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2$\\
  \hline
  Average coding-neuron signal (without rewiring) & $\langle \SII \rangle$ &$\Wc \C p_1 \rh + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb  \Bigr] (1-p_1)\rl$\\
  \hline
\end{tabular}
    \caption{Summary of the equations for the discrete model.}
    \label{tab:discrete_model}
\end{table}


\subsubsection{\label{subsubseq:rate-cont} Continuous model}
As mentioned earlier, in this model the firing rate patterns are generated from a continuous probability distribution, $\rho(\nu)$.
The distinction between high-rate and low-rate neurons
is based on two rate thresholds, $\rtI$ for the population $\popI$ and $\rtII$ for the population $\popII$. The values of these thresholds are related to the fraction of neurons above the threshold for the two populations, $p_1$ and $p_2$, respectively, by the equations:
\begin{equation}
    \begin{split}
    p_1 &= \int_{\rtI}^{\infty} \rho (\nu) d\nu\\
    p_2 &= \int_{\rtII}^{\infty} \rho (\nu) d\nu\\
    \end{split}
\end{equation}
The average rates of the neurons of $\popI$ below and above threshold,
$\langle \rlI \rangle$ and $\langle \rhI \rangle$,
can be computed from the firing rate distribution as:
\begin{equation}
    \label{eq:av_rate_cont}
    \begin{split}
        \langle \rlI \rangle &= \int_{0}^{\rtI} \nu \rho (\nu) d\nu \Big/ \int_{0}^{\rtI} \rho (\nu) d\nu = \frac{1}{q_1}\int_{0}^{\rtI} \nu \rho (\nu) d\nu\\
        \langle \rhI \rangle &= \int_{\rtI}^{\infty} \nu \rho (\nu) d\nu \Big/ \int_{\rtI}^{\infty} \rho (\nu) d\nu = \frac{1}{p_1} \int_{\rtI}^{\infty} \nu \rho (\nu) d\nu
    \end{split}
\end{equation}
where $q_1 = 1- p_1$.
Similar equations can be used to compute
$\langle \rlII \rangle$ and $\langle \rhII \rangle$.
From these equations
the average firing rate can be expressed as
\begin{equation}
    \langle \nu \rangle= 
    \int_{0}^{\infty} \nu \rho (\nu) d\nu
    = q_1 \langle \rlI \rangle + p_1 \langle \rhI \rangle 
    = q_2 \langle \rlII \rangle + p_2 \langle \rhII \rangle 
\end{equation}
In a training example, a connection will be consolidated if both the presynaptic and the postsynaptic neuron assume a firing rate above threshold.
Figure \ref{fig:lognormal} depicts an example of firing rate distribution, with the threshold and the average values of low and high firing rate.

% Figure environment removed

As in the discrete model, the test set consists of $V$ firing-rate patterns of the neurons of $\popI$, randomly extracted from the $\T$ input patterns of the train set.
Here we consider the case where the patterns are unchanged, thus each input patter of the test set is identical to an input pattern of the train set. In a later section we will discuss the effect of altering these patterns by adding noise.
To estimate the values of $\langle\SII\rangle$, $\langle\Sb\rangle$ and $\varSb$ we proceed in a similar way as for the discrete model.
First of all we calculate the input signal to a generic non-coding neuron of $\popII$.
Let $\C$ be the number of incoming connections to this neuron,
$P(k)$ the probability that $k$ of these connections are consolidated, $\nu_1, \dotsi, \nu_k$ the firing rates of the
presynaptic neurons of the consolidated connections
and $\xi_1, \dotsi, \xi_{\C - k}$ the firing rates of the
presynaptic neurons of the unconsolidated connections.
The probability of having $k$ consolidated connections and rates in the range $(\nu_1, \nu_1+d\nu_1), \dotsi, (\nu_k, \nu_k + d\nu_k)$, $(\xi_1, \xi_1+d\xi_1), \dotsi, (\xi_{\C-k}, \xi_{\C-k} + d\xi_{\C-k})$ is $P(k)\rho (\nu_1)\dotsi \rho (\nu_k) \rho (\xi_1) \dotsi \rho (\xi_{\C-k})
d\nu_1 \dotsi d\nu_k d\xi_1 \dotsi d\xi_{\C-k}$.
To calculate the average background signal we should average the expression of $\Sb$, given by equation \ref{eq:Sb}, over all the possible values of $k$ and of the firing rates, thus 
\begin{equation}
\label{eq:av_sb_cont}
\begin{split}
    \langle \Sb \rangle &= \sum_{k} P(k) \int d\nu_1 \dotsi \int d\nu_k \int d\xi_1 \dotsi \int d\xi_{\C - k} \rho (\nu_1) \dotsi \rho (\nu_k) \rho (\xi_1) \dotsi \rho (\xi_{\C - k}) \cdot \\
    &\cdot\Bigl[ \Wc (\nu_1 + \dotsi + \nu_k) + \Wb (\xi_1 + \dotsi + \xi_{\C-k})\Bigr] = \\
    &= \sum_{k} P(k) \Bigl[ \Wc k \langle \nu \rangle + \Wb (\C-k) \langle \nu \rangle \Bigr]
    = [ \Wc \langle k \rangle 
    +  \Wb (\C - \langle k \rangle) ] \langle \nu \rangle
\end{split}
\end{equation}

where we used the fact that $\int \nu \rho(\nu) d\nu = \int \xi \rho(\xi) d\xi = \langle \nu \rangle$. Note that the result obtained in Equation \ref{eq:av_sb_cont} is the same as the one obtained for the discrete model (see Equation \ref{eq:av_sb}).

The variance of the background signal can be similarly derived:
\begin{equation}
\label{eq:var_sb_cont}
\begin{split}
    \varSb &=\langle(\Sb - \langle \Sb \rangle)^2\rangle = \sum_k P(k) \int d\nu_1  \dotsi \int d\nu_k \int d\xi_1 \dotsi \int d\xi_{\C-k} \rho(\nu_1) \dotsi \rho(\xi_{\C-k}) \cdot \\
    &\cdot \Bigl[ \Wc \sum_{i=1}^{k}\nu_i
    +\Wb \sum_{i=1}^{\C-k} \xi_i 
    - [ \Wc \langle k \rangle 
    +  \Wb (\C - \langle k \rangle) ] \langle \nu \rangle \Bigr]^2 = \\
    &= \sum_k P(k) \int d\nu_1 \dotsi \int d\xi_{\C-k} \rho(\nu_1) \dotsi \rho(\xi_{\C-k})
    \Bigl[ \Wc \sum_{i=1}^k (\nu_i - \langle \nu \rangle) +\Wb \sum_{i=1}^ {\C-k} (\xi_i - \langle \nu \rangle) + \\ 
    & + (k-\langle k \rangle)(\Wc-\Wb)\langle \nu \rangle \Bigr]^2
\end{split}
\end{equation}
    
Where in the last line we used the substitution $\langle k \rangle=k+(\langle k \rangle - k)$ as done for Equation \ref{eq:var_sb2}.
The mixed terms of the equation above are null because $\int \rho (x) (x-\langle x \rangle)dx = 0$, ergo we can write the variance of the background signal as follows:

\begin{equation}
\label{eq:var_sb2_cont}
\begin{split}
    \varSb &= \sum_k P(k) \int d\nu_1 \dotsi \int d\xi_{\C-k} \rho(\nu_1) \dotsi \rho(\xi_{\C-k}) \Bigl[ \Wc^2 k \langle (\nu - \langle \nu \rangle)^2 \rangle + \Wb^2 (\C - k) \langle (\nu - \langle \nu \rangle)^2 \rangle +\\
    & + (\Wc - \Wb)^2 (k - \langle k \rangle)^2 \langle \nu \rangle ^2 \Bigr] = \Bigl[ \Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2
\end{split}
\end{equation}
The variance of $k$ has the same expression as for the discrete case, and is derived in  Appendix \ref{app:var_k}.
$\SII$ can be derived similarly to the discrete case as well.
We have a contribution from $p_1\C$ neurons of $\popI$ at high rate connected by consolidated connections.
The remaining $\C'= \C - p_1\C$ neurons have a low rate
and can be connected either by unconsolidated connections or by connections that have been consolidated in input patterns different from the current one.
Calling $\langle k' \rangle$ the average number of 
consolidated connections outgoing from the $\C'$ low-rate neurons and using the definition of $\langle \rl \rangle$ and $\langle \rh \rangle$ given by Equation \ref{eq:av_rate_cont} we can write
\begin{equation}
\label{eq:s2_cont}
\begin{split}
    \langle \SII \rangle &= \Wc p_1 \C \langle \rh \rangle + (\Wc - \Wb) \langle k' \rangle \langle \rl \rangle + \Wb \C(1-p_1)\langle \rl \rangle =\\
    &= \Wc p_1 \C \langle \rh \rangle + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb \Bigr] (1-p_1) \langle \rl \rangle
\end{split}
\end{equation}
where we used the expression of $\langle k' \rangle$
from Equation \ref{eq:av_k1}.
As can be seen, Equation \ref{eq:s2_cont} differs from \ref{eq:s2} only because the rates are not discrete but can assume continuous values, thus the discrete values
$\rh$ and $\rl$ are replaced by the averages
$\langle \rh \rangle$ and $\langle \rl \rangle$.
The modified formula of $\SII$ that takes into account
connection rewiring will be derived in section \ref{subseq:rewiring}.

In this work we use a log-normal distribution of the firing rates for the continuous model.
Indeed, it is known that rate distribution in the cortex are long-tailed and skewed with a log-normal shape 
\citep{Roxin2011}.
The log-normal distribution is a continuous probability distribution of a random variable $\nu$ whose logarithm 
$\ln(\nu)$ is normally distributed.
The probability density function of this distribution is
\begin{equation}
\label{eq:lognormal}
 \rho_\text{LN}(\nu) = \frac{1}{\sqrt{2\pi} \sigma \nu} \cdot \exp\Bigl( -\frac{(\ln(\nu)-\mu)^2}{2\sigma^2}\Bigr)
\end{equation}
where $\mu$ and $\sigma$ are the mean and standard deviation of $\ln(\nu)$.
The expressions of the mean value and standard deviation of $\rho_\text{LN}(\nu)$ will be derived in Appendix \ref{app:distr_fr}.

\subsubsection{\label{subsubseq:poiss-indegree} Poisson distribution of incoming connections per neuron}

Hitherto we considered a model in which each neuron of $\popII$ has a fixed number of incoming connections, i.e., a fixed in-degree, $\C$. However, a more general and more realistic approach would consider $\C$ as variable across the neurons of $\popII$
according to an appropriate probability distribution
$P(\C)$.
In this work we will focus on the case where the number of incoming connections follows a Poisson distribution (i.e. a Poisson-indegree connection rule), however the approach we will present can be easily extended to other distributions.
The values of $\langle \SII \rangle$ and $\langle \Sb \rangle$, previously averaged over the rate $\nu$ and the number of consolidated connections $k$, should be also averaged over the number of incoming connections, so that

\begin{equation}
    \begin{split}
        \langle \langle \Sb \rangle_{\nu,k} \rangle_{\C} &= \sum_{c} P(\C) \langle \Sb \rangle_{\nu,k}\\
        \langle \langle \SII \rangle_{\nu,k} \rangle_{\C} &= \sum_{\C} P(\C) \langle \SII \rangle _{\nu,k}
    \end{split}
    \label{eq:averaging_over_C}
\end{equation}

where $\langle \SII \rangle_{\nu,k}$ is given by Equation \ref{eq:s2_cont} and $\langle \Sb \rangle _{\nu,k}$ is given by Equation \ref{eq:av_sb_cont}. Since these equations are linear in $\C$ and since $\sum_{\C} \C P(\C) = \langle\C\rangle$, Equations \ref{eq:s2_cont} and \ref{eq:av_sb_cont} would show $\langle\C\rangle$ instead of $\C$ when averaged over the number of incoming connections per neuron.\\
The variance can be obtained from the equation: 
\begin{equation}
    \text{Var}(\langle \langle \Sb \rangle_{\nu,k}\rangle_{\C})= \sigma^2_{\nu,k,\C} =\langle\langle\Sb^2\rangle_{\nu,k}\rangle_{\C}-\langle\langle\Sb\rangle_{\nu,k}\rangle_{\C}^2
\end{equation}
Knowing that $\langle \varSb\rangle_{\C}=\langle \langle
\Sb^2\rangle_{\nu_k}-\langle\Sb\rangle_{\nu,k}^2\rangle_{\C}
=\langle\langle\Sb^2\rangle_{\nu,k}\rangle_{\C}-\langle\langle\Sb\rangle_{\nu,k}^2\rangle_{\C}$ and that $\langle k \rangle = p\C$ we can write
\begin{equation}
\begin{split}
    \sigma^2_{\nu,k,\C} &= \langle\varSb\rangle_{\C} + \langle\langle\Sb\rangle_{\nu,k}^2\rangle_{\C}-\langle\langle\Sb\rangle_{\nu,k}\rangle_{\C}^2=\\
    &= \langle\varSb\rangle_{\C} + \Bigl\{ \langle\nu\rangle \Bigl[ \Wb + p(\Wc-\Wb) \Bigr] \Bigr\}^2 \Bigl[ \langle\C^2\rangle - \langle\C\rangle^2 \Bigr] =\\
    &= \langle\varSb\rangle_{\C} + \langle\nu\rangle^2 \Bigl[ \Wb + p(\Wc-\Wb) \Bigr]^2 \sigma_{\C}^2
\end{split}
\label{eq:var_sb_c_variable}
\end{equation}

This equation is also valid for the discrete model.

\subsubsection{\label{subseq:rewiring} Connection Rewiring}
In the proposed approach, rewiring is implemented by periodically pruning unconsolidated connections and creating new ones. These procedures are performed with a fixed step on the number of training examples, which we will call {\it rewiring step}, denoted by the letter $s$.
The creation of the new connections is made in such a way as to keep the distribution of the number of incoming connections per neuron unchanged.
If $h$ is the number of consolidated incoming connections of a neuron of $\popII$, after pruning all the unconsolidated connections, $\C - h$ new connections will be created. $\C$ is a fixed number if the fixed-indegree connection rule is used, while it is extracted from a Poisson distribution if the Poisson-indegree rule is selected; in both cases, the presynaptic neurons are randomly extracted from $\popI$. For this reason, rewiring leaves the expressions of the background signal and of the variance on this signal unchanged, while, as we will see, it modifies the input signal to coding neurons.
A diagram of the rewiring process is shown in Figure \ref{fig:rewiring}, which illustrates the activity of a high-rate neuron of $\popII$ and of the presynaptic neurons of its incoming connections in a training example, and the effect of connection rewiring. 

% Figure environment removed

The average number of incoming connections that are consolidated in the current example (blue lines) is equal to the average number of high-rate presynaptic neurons,
$p_1 \C$.
The average number of incoming connections that are consolidated in other examples after the entire training,
$\langle k' \rangle$, is given by equation \ref{eq:av_k1}:
\begin{equation}
    \langle k' \rangle = p \C(1-p_1)
\end{equation}
Let $t$ be the next training index for which rewiring will be applied, and $k'_t$ the number of connections from low-rate neurons that are consolidated before $t$ (green lines in the figure).
These connections will not be affected by rewiring, so even in the test phase with the same input pattern they will have low-rate presynaptic neurons.
The average value of $k'_t$ is
\begin{equation}
\label{eq:avg_k1t}
\langle k'_t \rangle = p_t \C ( 1 - p_1 )
\end{equation}
where $p_t$ is given by an expression analogous to the one obtained for $p$ (Equation \ref{eq:p})
\begin{equation}
\label{eq:pt}
p_t = 1 - (1 - p_1 p_2)^t
\end{equation}
On the other hand, there will be $k' - k'_t$ connections displaced by rewiring and consolidated in training examples of index greater than $t$.
Putting all the contributions together, we obtain the following expression for $\SII$:

\begin{equation}
\label{eq:s2_cont_rewiring}
\begin{split}
    \langle \SII \rangle &=
    p_1 \C \Wc \langle \rh \rangle +
    ( \langle k' \rangle - \langle k'_t \rangle )
    \Wc \langle \nu \rangle
    + \langle k'_t \rangle \Wc \langle \rl \rangle
    + \Wb (\C - p_1 \C - \langle k' \rangle)
    \langle \nu \rangle =\\
    &= p_1 \C \Wc \langle \rh \rangle +
    \langle k' \rangle \Wc \langle \nu \rangle +
    \Wb [ \C (1 - p_1) - \langle k' \rangle ]
    \langle \nu \rangle
    - \langle k'_t \rangle \Wc
    ( \langle \nu \rangle - \langle \rl \rangle )
\end{split}
\end{equation}
To obtain the average value of $\SII$ over all examples, $\langle k'_t \rangle$ must be averaged over all values of the index $t$ for which rewiring is done.
This calculation is shown in Appendix \ref{app:avg_k1t}.
Equation \ref{eq:s2_cont_rewiring} is also valid in the case for which neurons can assume discrete values of firing rate. In that case, $\langle \rl \rangle$, $\langle \rh \rangle$ and $\langle \nu \rangle$ have to be replaced with the discrete values $\rl$, $\rh$ and the average rate $\langle \nu \rangle$ shown in Table \ref{tab:discrete_model}.

\subsubsection{\label{subseq:noise} Introduction of noise into input patterns}
In a realistic learning model, the test patterns will never be exactly the same as the training ones. The ability of a learning model to generalize is linked to the ability to recognize which training pattern or patterns are most similar to a given test pattern, according to appropriate metrics.
To study the generalization capacity of the model proposed in this work, the test input patterns were generated starting from the corresponding training input patterns by adding
noise, which is represented by a deviation extracted from a given probability distribution with assigned standard deviation.
In Appendix \ref{app:noise} we describe the effect that a noise with a truncated Gaussian distribution has on the firing rates and on the variables $\Sb$, $\SII$, $\varSb$ and SDNR, and we derive the modified equations.

\subsubsection{\label{subseq:summary} Summary of theoretical model equations}
Table \ref{tab:continuous_model} summarizes the equations of the continuous rate model in case of fixed number of incoming connections per neuron. As mentioned before, when the value of $\C$ follows a Poisson distribution, $\langle\Sb\rangle$ and $\langle\SII\rangle$ are given by the same expression obtained for the fixed in-degree connection rule with $\C$ replaced by $\langle\C\rangle$. The variance of the background signal has an additional term in that case and is given by Equation \ref{eq:var_sb_c_variable}.

\begin{table}[h]
%\nolinenumbers
    \centering
    \renewcommand\arraystretch{1.5}
    \begin{tabular}{
  |@{\hspace*{\marg}}m{0.3\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.6\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Continuous rate model}
  }\\
  \hline 
  \textbf{Name} & \textbf{Symbol} & \textbf{Equation}\\
  \hline
  Rate distribution & $\rho(\nu)$ & $\frac{1}{\sqrt{2\pi} \sigma \nu} \cdot \exp\Bigl( -\frac{(\ln(\nu)-\mu)^2}{2\sigma^2}\Bigr)$ \\
  \hline
  Mean of the normal distribution of $\ln(\nu)$ & $\mu$ & $\ln(\langle \nu \rangle) - \frac{\sigma^2}{2}$\\
  \hline
  Standard deviation of the normal distribution of $\ln(\nu)$ & $\sigma$ & $\text{erf}^{-1}(q_1) - \text{erf}^{-1}\Big( \frac{q_1 \langle \rl \rangle}{\langle \nu \rangle} \Big)$\\
  \hline
  Rate threshold & $\rt$ & $\exp \Big(\text{erf}^{-1}(q_1)\sigma + \mu\Big)$\\
  \hline
  Average high rate & $\langle \rh \rangle$ & $\frac{1}{p_1} \int_{\rt}^{\infty} \nu \rho (\nu) d\nu$\\
  \hline
  Average low rate & $\langle \rl \rangle$ & $\frac{1}{q_1}\int_{0}^{\rt} \nu \rho (\nu) d\nu$\\
  \hline
  Average rate & $\langle \nu \rangle$ & $q_1 \langle \rl \rangle + p_1 \langle \rh \rangle $\\
    \hline
  Rate standard deviation & $\sigma_{\nu}^2$ & $\Big( e^{\sigma^2} - 1 \Big)e^{2\mu + \sigma^2}$\\
  \hline
  Average background signal & $\langle \Sb \rangle$ & $(\Wc - \Wb) \langle k \rangle \langle \nu \rangle + \Wb \C \langle \nu \rangle$\\
  \hline
  Variance of background signal & $\varSb$ & $\Bigl[ \Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2$\\
  \hline
  Average coding-neuron signal (without rewiring) & $\langle \SII \rangle$ & $\Wc p_1 \C \langle \rh \rangle + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb \Bigr] (1-p_1) \langle \rl \rangle$ \\
  \hline
  Average coding-neuron signal (with rewiring) & $\langle \SII \rangle$ & $\Wc p_1 \C \langle \rh \rangle + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb \Bigr] (1-p_1) \langle \nu \rangle - \langle k'_t \rangle \Wc
    ( \langle \nu \rangle - \langle \rl \rangle )$ \\
  \hline
  
\end{tabular}
    \caption{Summary of the equations for the continuous model. Equations shown here refers to the case in which the number of incoming connections per neuron $\C$ is constant.}
    \label{tab:continuous_model}
\end{table}

\subsection{Computational simulations of the model}
The validation of the equations derived in the previous sections was done through simulations with firing-rate-based neuronal network models.
The code of the simulator was written in the C++
programming language GCC (\url{https://github.com/gcc-mirror/gcc}) (version 10.2.0) and with the GSL (\url{https://www.gnu.org/software/gsl/}) (version 2.7) scientific libraries.
The simulations have been performed using the supercomputers Galileo 100 and JUSUF \cite{VonStVieth2021}.
The networks used for the simulations are generated according to the selected connection rule. In particular, in the case of the fixed-indegree rule, $\C$ incoming connections are created for each neuron of the $\popII$ population, where $\C$ has a fixed value.
In the case of the Poisson-indegree rule, for each neuron of the population $\popII$ the number of incoming connections $C$ is extracted from a Poisson distribution with mean $\langle C \rangle$.
In both cases, the indexes of the presynaptic neurons are randomly extracted on the $\popI$ population. 
The connection weights are initially set to the
baseline value, $\Wb$.
Each training input patterns of the discrete model is generated by extracting, for each neuron of $\popI$, a random number $r$ from a uniform distribution in the interval $[0, 1]$; if $r<p_1$ the rate of the neuron is set to the high level, $\rh$, otherwise it is set to $\rl$.
An analogous procedure is used to generate the corresponding contextual stimulus pattern on the neurons of the population $\popII$.
A connection is consolidated in a training example if both the presynaptic and the postsynaptic neuron are in the high-rate level, $\rh$.
In the continuous case, the firing rates of the input pattern and those of the contextual pattern are extracted from a log-normal distribution. In this case, a connection is consolidated if the firing rates of both the presynaptic and postsynaptic neurons are above the thresholds $\nu_{t,1}$ and $\nu_{t,2}$, respectively.
Connection rewiring is performed every $s$ training steps, as described in section \ref{subseq:rewiring}.
The test set is generated by randomly extracting
$V$ input patterns from the train set.
In the discrete case, the patterns are not modified.
In the continuous case, the patterns of the test set are altered by adding noise extracted from a truncated Gaussian distribution.

To estimate $\langle\Sb\rangle$ and $\langle\SII\rangle$ we compute the input of each $\popII$ neuron as the sum of the rate of the presynaptic neurons of its incoming connections multiplied by the synaptic weights (i.e., $\Wc$ or $\Wb$). 
The variance $\varSb$ is evaluated by the formula:

\begin{equation}
    \varSb = \langle\Sb^2\rangle - \langle\Sb\rangle^2
\end{equation}

where the mean values are calculated over the input signals to all non-coding neurons of $\popII$.

\section{\label{sec:results} Results}
This section compares the results of the simulations of the firing rate model with the theoretical predictions
described in the previous section. Since we proposed several versions of the model, with different features implemented, the section is divided into different parts that sum up the main characteristics of the model.
We present the results of the approaches employing discrete and continuous values for neuron firing rates, comparing the theoretical values of the average input signal to background neurons $\langle\Sb\rangle$,
the average input signal to coding neurons $\langle\SII\rangle$, the variance of the background input signal $\varSb$ and the signal-difference-to-noise ratio
with the values obtained from the simulations.
This way, we are able to assess the capacity of the population $\popII$, and thus of the network, to recognize
a pattern memorized in the training phase.
Here, we present simulation results with a Poisson-driven number of incoming connections, with $\langle\C\rangle=5000$. We opted for such an approach since it is more realistic for biological neural circuits with respect to a fixed amount of connections per neuron. Each simulation is repeated $10$ times using a different seed for random number generation to ensure the robustness of the simulation results. The values shown in the plots are a result of averaging over the different seeds.

\subsection{Comparison between continuous and discrete firing rate}
As previously mentioned, the main difference in calculating
$\langle\Sb\rangle$ and $\langle\SII\rangle$ with the continuous rate model versus the discrete model is that the discrete values of $\nu_l$ and $\nu_h$ are replaced, respectively, by the average values of the rate below and above threshold, calculated on the continuous probability distribution.
On the other hand, the variance of the background signal differs in the two models, because it depends on the variance of the rate, $\sigma^2_{\nu}$, which is different in the two cases.\\
The first study we present is oriented towards the estimation of these parameters as a function of the number of training patterns $\T$.
As the number of training patterns increases, so does the number of patterns encoded by each individual neuron.
Since $p_2$ is the probability that a neuron of $\popII$ is in a high-rate level for a single training pattern, on average such neuron will encode $p_2 \T$ patterns of the entire training set.
This multiple selectivity of individual neurons is also present in biological neural networks, in which the same neuron can be selective for several stimuli \citep{Rigotti2013}.\\
The test set consists of $V = 1000$ input patterns, generated as described in the Materials and Methods section.
Thus, the simulation outcome used for our analysis is an average over the entire test set of the $\Sb$, $\SII$, $\varSb$ and SDNR values obtained for each test pattern.
Figure \ref{fig:discr_vs_contin} shows the comparison of the simulation outcomes using discrete and continuous rate values.

% Figure environment removed

We can see that the curves of $\langle\Sb\rangle$ and $\langle\SII\rangle$ obtained from the simulations using the continuous firing rate distribution are superimposed
on those obtained using the discrete model; this is due to the fact that the the choice of the threshold on the log-normal distribution is done so that the average values for low and high rate, $\langle \nu_l \rangle$ and 
$\langle \nu_h \rangle$, correspond to the values adopted in the discrete rate model.
On the other hand, the variance of the background signal $\varSb$ differs in the two models, because it depends on the variance of the firing rate,
$\sigma_\nu$, which is different in the two cases.
This leads also to the different behavior of the SDNR.

\subsection{Comparison between theoretical predictions
and simulation results}
The results shown in the previous section are obtained from simulations. This section presents a comparison between simulation results and theoretical expectations.
To provide a quantitative estimation of the discrepancy between the theoretical predictions and the simulations, we evaluate their relative error, using the theoretical values as a reference.\\
As described previously, the test input patterns used in the continuous rate model are altered from the corresponding training input patterns by adding noise extracted from a truncated Gaussian distribution, with assigned standard deviation. In this section, we present simulation results and comparisons with theoretical predictions for standard deviation values ranging from $1$\,Hz to $5$\,Hz.
These values are relatively high in relation to the average firing rate of $\popI$, which is slightly higher than $2$\,Hz.\\
Figure \ref{fig:relative-error} shows the curves obtained for the continuous rate model using different values for the standard deviation of the noise, together with the relative error with respect to theoretical predictions.
It can be observed that the curves obtained from the simulations are compatible with the theoretical ones for all the noise levels.

% Figure environment removed

Regarding $\langle\Sb\rangle$ and $\langle\SII\rangle$, the curves corresponding to different noise levels appear perfectly superimposed.
This is due to the fact that the noise is driven by a distribution with zero mean, and thus the addition of noise to the quantities represented in the curves does not alter their average (see Appendix \ref{app:noise} for the details). Regarding $\varSb$, the values corresponding to
different noise levels
differ from each other and increase with the standard deviation of the noise, in agreement with the theoretical model.\\
The relative error between simulation results and theoretical prediction is quite small: for $\langle\Sb\rangle$ and $\langle\SII\rangle$ the errors span between $0.01$\% and $0.4$\%, whereas $\varSb$ shows a relative error of around $1$\% for all the simulations performed with different number of training patterns.


The addition of noise with fluctuations greater than or comparable to the average firing rate can
produce negative rate values for a fraction of the neurons.
Considering that negative rate values are not physically possible, this behavior can be corrected in the simulations by simply  replacing negative values of the firing rates with zero, i.e. saturating negative rates
to zero.
This correction is equivalent to having a different noise distribution, with an average value greater than zero.
However, the current theoretical model is not able to take this effect into account. Since negative values are replaced by zeros,
we would expect the average values of $\Sb$ and $\SII$ evaluated by the simulations that exploit saturation to be greater than the values
predicted by the theoretical model.
Figure \ref{fig:relative-error-saturation} shows the behavior of the model with this correction on the neurons firing rate.

% Figure environment removed

As can be seen from the figure, the discrepancies between simulations and theoretical predictions are much higher and can arrive to $40$\%. This is due to the fact that the current theoretical framework is not able to take this correction into account.
However it should be considered, as discussed above, that the considered noise levels are relatively high when compared with the average rate used in these simulations.
Indeed, a different choice for the values of $\langle \rl \rangle$ and $\langle \rh \rangle$ (and thus a different average rate of the whole distribution) would have an impact on the discrepancies shown here.
In particular, a higher average rate would strongly diminish the amount of neurons having
negative firing rate as a result of the noise addition.

The relative error of $\varSb$ is greater than that shown for $\langle \Sb \rangle$ and $\langle \SII \rangle$ in Figure \ref{fig:relative-error}; this is due to a simplification used in the theoretical model to derive the expression of the variance.
The values of $\Sb$ from which we compute the variance are obtained by incoming connections 
from neurons of $\popI$, but since connections are created randomly, different neurons of the $\popII$ population may have presynaptic neurons in common, and therefore their input signals are correlated.
The theoretical model does not take this correlation into account.
The average number of presynaptic neurons in common to two arbitrary neurons of
$\popII$ depends on the total number of neurons of $\popI$ and on the number of incoming connections per neuron of $\popII$.
Calling $\NI=\mathcal{N}$, we can state that the bias due to this simplification becomes more and more relevant when the ratio $\mathcal{C}/\mathcal{N}$ increases.
In order to estimate this bias as a function of the $\mathcal{C}/\mathcal{N}$ ratio,
we performed a series of simulations with a fixed
number of training patterns, $\T=5000$, changing the $\mathcal{C}/\mathcal{N}$ ratio.
Figure \ref{fig:c_n_comparison} shows the results of this analysis.

% Figure environment removed

As can be seen in the right panel of Figure \ref{fig:c_n_comparison}, a greater value of $\mathcal{C}/\mathcal{N}$ leads to a higher discrepancy between theoretical prediction and simulation. However, such a ratio, for natural density circuits in the brain, is very far from values of $\mathcal{C}/\mathcal{N}$ near unity. Indeed, a plausible value of the ratio would be less than $0.1$, resulting in negligible relative errors.


\subsection{Impact of synaptic rewiring}
In the simulations discussed so far, the rewiring mechanism was always performed with
a rewiring step $s=100$.
This means that every $100$ training patterns,
all the unconsolidated connections are removed, and new connections are created.
This operation represents the effect of homeostatic structural plasticity, that aims at keeping the network balanced by reorganizing connections, while activity-dependent structural plasticity focuses on the consolidation of connections.\\
To motivate the choice of this step for connection rewiring, we show here the results for networks trained for $\T=5000$ patterns with a different rewiring step $s$.
We also show the results of a simulation that does not perform rewiring, in order to highlight
the different behavior of a network that combines connection consolidation with periodic rewiring and that of a network that exploits only connection consolidation.
Figure \ref{fig:t_study} shows the results obtained by these simulations using different rewiring intervals. 

% Figure environment removed

As can be noticed, the values of $\Sb$, $\SII$, $\varSb$ and SDNR do not change significantly as the rewiring step varies.
This means that the value of the step $s$ chosen for the connection rewiring has no impact in the results of the simulations.
On the other hand, significant differences emerge when comparing the results of simulations
with or without connection rewiring; it can be observed that the signal-difference-to-noise ratio has a lower value  when the rewiring is disabled. This confirms that connection rewiring grants a higher capability of recognising an input pattern among the several patterns
for which the network was trained.\\
We also applied a similar protocol for simulations enabling or disabling connection rewiring as a function of the number of training patterns $\T$. The results are shown in Figure \ref{fig:rew_vs_norew}.\\
We can say that the performance of the model are improved when connection rewiring is enabled, and the relative difference between a rewired or just consolidated connectivity increases when increasing the number of training patterns.\\
The effect of rewiring can become more relevant when a greater number of connections is consolidated at every step (i.e., with greater values of $p_1$ and $p_2$).
Furthermore, the importance of the rewiring mechanisms can significantly change 
when the average number of connections is not constant, but increases or decreases as a result of rewiring itself. This aspect will be explored in a future work.

% Figure environment removed


\section{\label{sec:discussion} Discussion}
In the previous section the predictions of the theoretical framework, based on a mean-field approach, have been compared with the results of simulations performed using firing-rate-based neuronal networks.
This comparison shows that the proposed framework is able to accurately predict the values of various quantities relevant for assessing learning and memory capacity in the presence of structural plasticity mechanisms, taking into account numerous characteristics of biological neuronal networks.
The rates of neurons in the training and test patterns can be distributed according to an arbitrary probability distribution. In this work two cases have been considered, a simplified one in which the rates can assume only two values, high rate and low rate, and a more realistic one in which the rates follow a log-normal distribution in agreement with \citep{Roxin2011}.
Connectivity between regions can be achieved through different connection rules, with fixed number of connections per target neuron or, more realistically,
with a random connectivity in which  the number of incoming connections of each target neuron follows a Poisson distribution.\\
Since the biochemical and biophysical mechanisms underlying structural plasticity are multiple and extremely complex, we opted for a phenomenological approach to capture their main aspects: 
a simple model of structural plasticity has been exploited, able to to represent plasticity processes driven by neuronal activity as well as mechanisms which leads to homeostasis, in agreement with the work of \cite{Fauth2016} which divides structural plasticity mechanisms in these two categories.
Structural plasticity driven by neuronal activity is achieved through the consolidation of synapses connecting neurons that are concurrently in a high-rate level.
This process can be triggered by other forms of plasticity that modify synaptic efficacy, such as STDP, followed by 
mechanisms involving citoarchitectural changes, such as the creation of novel connections next to the already existing ones.\\
The homeostatic form of structural plasticity  involves a balance between pruning connections that are not utilized over time and creation of novel connections.
This is achieved in the simulations through a periodic connection rewiring, which consists of a removal of unconsolidated connections followed by creation of new connections.
The results show that connection rewiring leads to an increase in SDNR, conducting to an higher capability of recognizing the input patterns when this mechanism is enabled.\\
In order to evaluate the generalization capability of the framework with the continuous firing-rate distribution model, the test patterns were generated by altering the training input patterns through the addition of noise from a given probability distribution and assigned standard deviation. In particular, in this work a truncated Gaussian distribution has been used for the noise.
The results of the simulations are compatible with the theoretical predictions, with differences in the order of $1-2$\%, which is a remarkable result for our purpose.\\
Such an approach can lead to a fraction of the neurons with a negative firing rate as a result of noise addition.
Since negative firing rates are not physically possible, a more realistic model would apply a saturation to zero for these rates.
Saturation can be activated in the neuronal network simulations, however the current version of the theoretical framework is unable to account for it.
This leads to a discrepancy between the simulations with saturation turned on and the theoretical predictions, which grows as the noise increases and becomes relevant when the noise gets significantly greater than the average firing rate.
Future work should be devoted to the development of a theoretical model capable of taking into account the saturation of negative firing rates. \\
Another limitation of the framework comes from a simplification in the calculation of the background signal variance, which does not take into account the correlations between the contributions of presynaptic neurons to the input signals to distinct neurons of the target population.
However, the results show that the impact of this simplifications is very small, at most in the order of a few percent.\\
The theoretical model can be surely extended. It can potentially provide a powerful tool to describe the impact of structural plasticity in cognitive processes such as learning in a large-scale model of the cortex with natural density and plausible characteristics. For instance, the consolidation mechanism can be probability-driven, with a probability depending on the rate of pre- and postsynaptic neurons. This would replace the current deterministic mechanism that requires a firing rate threshold to be exceeded from both the neurons to have synaptic consolidation. Moreover, the probability could depend on other variables not necessarily related to the firing rate.
In particular, it has been hypothesized that plasticity  mechanisms may also depend on the bursting activity of neurons \cite{Butts2007, Payeur2021}.
The consolidation probability of a connection could therefore depend, in addition to the firing rate of the presynaptic and postsynaptic neurons, also to their bursting activity.\\
In this work we have considered the connections between two distinct populations $\popI$ and $\popII$.
However, the proposed framework also lends itself to the study of structural plasticity in the self-connections of a neuron population.
An extension of the model could include recurrent connections in $\popII$, together with an inhibitory population in order to have a more realistic architecture of excitatory and inhibitory neurons.
Indeed, it is known that the mechanisms of competition through lateral inhibition play a key role in biological learning \citep{Coultrip1992}. 
In this extended model, the theoretical framework can allow to obtain the differential equations governing the dynamics of the activity of the population $\popII$ and the dependence of the coefficients of these equations on the number of training patterns and on the other model parameters \citep{Sergi2023}. Such an extension is currently under development and it will be subject of a future work.\\
Another extension of the model could describe more in detail the mechanisms of synaptic pruning and rewiring. Indeed, connection rewiring as intended in the current model preserves the total number of connections over time, which is a typical behavior of a healthy adult brain \citep{Huttenlocher1979}. However, to shed light of the importance of these mechanism in neurological disorders, or to perform studies focused on this mechanism in different life stages, this mechanism should be extended to enable different "speed" for the processes embedded in structural plasticity.\\
Moreover, it would be interesting to expand this work through simulations of spiking neural networks, to study learning through structural plasticity in more detail. Indeed, simulators such as NEST \citep{NEST} and its GPU implementation \citep{Tiddia2022} can lead to fast and efficient simulations of large-scale models on supercomputer clusters.\\
In conclusion, this work intend to provide a sufficiently general theoretical framework for learning through structural plasticity. This framework is able to describe synaptic consolidation, pruning and rewiring, and includes several features that can be added with a modular fashion.
The validation has been performed through simulations with firing-rate-based neuronal network models, showing a remarkable compatibility between the results of the simulations and theoretical predictions.





\section*{\label{sec:contributions} Author contributions}
% Ideas; formulation or evolution of overarching research goals and aims.
\textbf{Conceptualization:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Management activities to annotate (produce metadata), scrub data and maintain research data (including software code, where it is necessary for interpreting the data itself) for initial use and later reuse.
\textbf{Data curation:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Application of statistical, mathematical, computational, or other formal techniques to analyze or synthesize study data.
\textbf{Formal analysis:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Acquisition of the financial support for the project leading to this publication.
\textbf{Funding acquisition:} Bruno Golosio

% Conducting a research and investigation process, specifically performing the experiments, or data/evidence collection.
\textbf{Investigation:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Development or design of methodology; creation of models
\textbf{Methodology:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Management and coordination responsibility for the research activity planning and execution.
\textbf{Project administration:} Bruno Golosio

% Provision of study materials, reagents, materials, patients, laboratory samples, animals, instrumentation, computing resources, or other analysis tools.
\textbf{Resources:} Bruno Golosio and Gianmarco Tiddia

%  	Programming, software development; designing computer programs; implementation of the computer code and supporting algorithms; testing of existing code components.
\textbf{Software:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

%  	Oversight and leadership responsibility for the research activity planning and execution, including mentorship external to the core team.
\textbf{Supervision:} Bruno Golosio

%  	Verification, whether as a part of the activity or separate, of the overall replication/reproducibility of results/experiments and other research outputs.
\textbf{Validation:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

%  	Preparation, creation and/or presentation of the published work, specifically visualization/data presentation.
\textbf{Visualization:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Creation and/or presentation of the published work, specifically writing the initial draft (including substantive translation).
\textbf{Writing - original draft:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Preparation, creation and/or presentation of the published work by those from the original research group, specifically critical review, commentary or revision – including pre- or post-publication stages.
\textbf{Writing - review and editing:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi


\section*{\label{sec:funding} Funding}
This study was supported by the European Union's Horizon 2020 Framework Programme for Research and Innovation under Specific Grant Agreements No. 945539 (Human Brain Project SGA3), No. 785907 (Human Brain Project SGA2), and by the Italian Ministry of University and Research (MUR) Piano Nazionale di Ripresa e Resilienza (PNRR), project e.INS Ecosystem of Innovation for Next Generation Sardinia – spoke 10 - CUP F53C22000430001 – MUR code: ECS00000038.
We acknowledge the use of Fenix Infrastructure resources, which are partially funded from the European Union's Horizon 2020 research and innovation programme through the ICEI project under the grant agreement No. 800858.\\


\section*{\label{sec:data_av_statement} Data availability statement}
All the simulation code needed to reproduce the results reported in this work, together with the related documentation, are freely available in the GitHub repository \url{https://github.com/gmtiddia/structural_plasticity}.


\section*{\label{sec:acknowledgements} Acknowledgements}
We thank Prof. Dr. Paolo Ruggerone for the fruitful discussion on structural plasticity mechanisms.



\nocite{*}
\section{Bibliography}
\bibliography{bibliography}

\appendix

\section{Log-normal distribution of the firing rate}
\label{app:distr_fr}
The theoretical treatment of the continuous model proposed in this work is valid for a generic firing rate probability distribution. However, the model validation presented in the result section is focused on a log-normal distribution, which is a continuous probability distribution of a random variable $\nu$ whose logarithm 
$\ln(\nu)$ is normally distributed.
The probability density function of this distribution is
\begin{equation}
\label{eq:lognormal_app}
 \rho_\text{LN}(\nu) = \frac{1}{\sqrt{2\pi} \sigma \nu} \cdot \exp\Bigl( -\frac{(\ln(\nu)-\mu)^2}{2\sigma^2}\Bigr)
\end{equation}
where $\mu$ and $\sigma$ are the mean and standard deviation of $\ln(\nu)$.
Expanding Equation \ref{eq:av_rate_cont} using Equation \ref{eq:lognormal_app} we have

\begin{equation}
\begin{split}
    \langle \rl \rangle &=\frac{1}{q_1}\int_{-\infty}^{y_t} \nu(y)G_{\sigma, \mu}(y) dy=\frac{1}{q_1}\int_{-\infty}^{y_t} e^y \frac{1}{\sqrt{2\pi \sigma^2}}e^{\frac{-(y-\mu)^2}{2\sigma^2} } dy\\
    \langle \rh \rangle &= \frac{1}{p_1}\int_{y_{t}}^\infty \nu(y) G_{\sigma, \mu}(y) dy= \frac{1}{p_1}\int_{y_{t}}^\infty e^y \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-(y-\mu)^2}{2\sigma^2} } dy 
\end{split}
\label{eq:av_rate_cont2}
\end{equation}

where $y$ is a variable representing the logarithm of the firing rate, $y=\ln(\nu)$, and follows a normal distribution $G_{\sigma, \mu}(y)$, while $y_t$ represents the value linked to the threshold value on the rate $\nu_t$ ($y_t=\ln(\nu_t)$).

In the logarithmic representation the area of the portion of the Gaussian $G_{\sigma, \mu}(y)$ having $y<y_t$ corresponds to the probability that a neuron has a low rate, $q_1 $. Therefore we can write:

\begin{equation}
    q_1= \int_{-\infty}^{y_t} G_{\sigma, \mu}(y) dy= \frac{1}{2}+ \int_{\mu}^{y_t} \frac{1}{\sqrt{2 \pi \sigma^2}} e^{- \frac{(y-\mu)^2}{2 \sigma^2}} dy
\end{equation}

Substituting $x= \frac{y-\mu}{\sqrt{2} \sigma}$ we obtain:

\begin{equation}
    q_1=\frac{1}{2}+ \int_{0}^{\frac{y_t-\mu}{\sqrt{2} \sigma}} \frac{1}{\sqrt{\pi}} e^{-x^2} dx = \frac{1}{2}+\frac{1}{2} \text{erf}(\frac{y_t-\mu}{\sqrt{2}\sigma})
\end{equation}

where with $\text{erf}(x)$ we indicate the error function, defined as:

\begin{equation}
    \text{erf}(x)= \frac{2}{\sqrt{\pi}} \int_{0}^x e^{-t^2} dt
\end{equation}

and then:

\begin{equation}
 %\frac{y_t-\mu}{\sqrt{2}\sigma} = \text{erf}^{-1}(2q_1-1) 
 y_t=\mu+\sqrt{2}\sigma \text{erf}^{-1}(2q_1-1)
\end{equation}

where $\text{erf}^{-1}$ is the inverse of the $\text{erf}$ function. By substituting $z=y-\mu$ we can rewrite $\nu_h$ from Equation \ref{eq:av_rate_cont2} as:

\begin{equation}
\begin{split}
 \langle \nu_h \rangle=&  
  \frac{1}{p_1}\int_{y_{t}-\mu}^\infty  \frac{1}{\sqrt{2\pi \sigma^2}} e^{z+\mu-\frac{z^2}{2\sigma^2} } dz =   \frac{1}{p_1} \frac{e^\mu}{\sqrt{2\pi \sigma^2}}  \int_{y_{t}-\mu}^\infty e^{-\frac{z^2 -2\sigma^2z}{2\sigma^2} } dz= \\
 =&\frac{1}{p_1} \frac{e^\mu}{\sqrt{2\pi \sigma^2}}  \int_{y_{t}-\mu}^\infty e^{-\frac{(z -\sigma^2)^2-\sigma^4}{2\sigma^2} } dz =
     \frac{1}{\sqrt{2\pi}}\frac{e^{\mu+\frac{\sigma^2}{2}}}{\sigma p_1} \int_{y_{t} -\mu}^\infty e^{\frac{-(z-\sigma^2)^2}{2 \sigma^2}}dz
    \end{split}
\end{equation}

Making a further substitution $\xi=\frac{z-\sigma^2}{ \sqrt{2} \sigma}$ finally we find:

\begin{equation}
\begin{split}
    \langle \nu_h \rangle&= \frac{1}{\sqrt{\pi}} \frac{e^{\mu + \frac{1}{2}\sigma^2}}{p_1} \int_{\frac{y_{t} -\mu-\sigma^2}{\sqrt{2}\sigma}}^\infty e^{-\xi^2}d\xi=\\
     &= \frac{1}{\sqrt{\pi}} \frac{e^{\mu + \frac{1}{2}\sigma^2}}{p_1} \Bigl(\int_{0}^\infty e^{-\xi^2}d\xi- \int_{0}^{\frac{y_{t} -\mu-\sigma^2}{\sqrt{2}\sigma}} e^{-\xi^2}d\xi \Bigr)=\\
     &=\frac{e^{\mu + \frac{1}{2}\sigma^2}}{p_1}\Bigl[\frac{1}{2}- \frac{1}{2} \text{erf} \Bigl(\frac{y_{t} -\mu-\sigma^2}{\sqrt{2}\sigma} \Bigr) \Bigr]=\\
     &=\frac{\langle \nu \rangle }{2 p_1}\Bigl[1- \text{erf} \Bigl(\text{erf}^{-1}(2q_1-1) -\frac{\sigma}{\sqrt{2}}\Bigr) \Bigr]
    \end{split}
\end{equation}

where with $\langle \nu \rangle$ 
we indicate the average rate, which for the log-normal distribution is given by the known expression

\begin{equation}
\label{eq:avg_nu_lognormal}
\langle \nu \rangle = e^{\mu + \frac{1}{2}\sigma^2}
\end{equation}

With similar steps we obtain the expression of
$\langle \nu_l \rangle$:

\begin{equation}
    \langle \nu_l \rangle= \frac{\langle \nu \rangle }
    {2 q_1}\Bigl[1- \text{erf} \Bigl(\text{erf}^{-1}(2p_1-1) -\frac{\sigma}{\sqrt{2}}\Bigr) \Bigr] 
\end{equation}

From these two equations we can finally derive the relationships between $\sigma$, $q_1$, $\nu$ and $\nu_h$ or $\nu_l$ respectively:

\begin{equation}
    \sigma= \sqrt{2} \Bigl[ \text{erf}^{-1} ( 2 q_1 - 1) -
    \text{erf}^{-1} \Bigl( 1 -
    {\frac{2 p_1 \langle \nu_h \rangle}
    {\langle \nu \rangle}} \Bigr) \Bigr]
\end{equation}

\begin{equation}
    \sigma= \sqrt{2} \Bigl[ \text{erf}^{-1} ( 2 p_1 - 1) -
    \text{erf}^{-1} \Bigl( 1 -
    {\frac{2 q_1 \langle \nu_l \rangle}
    {\langle \nu \rangle}} \Bigr) \Bigr]
\end{equation}


Using the equation \ref{eq:avg_nu_lognormal} we can rewrite $\mu$ as:

\begin{equation}
    \mu= \ln(\langle \nu \rangle) -\frac{\sigma^2}{2}
\end{equation}

The average rate $\langle \nu \rangle$
can also be expressed as a function of
$\langle \rh \rangle$ and $\langle \rl \rangle$:

\begin{equation}
    \langle \nu \rangle = p_1 \langle \rh \rangle +
    q_1 \langle \rl \rangle
\end{equation}

The latter equations allow us to express the parameters of the log-normal distribution $\sigma$ and $\mu$ as a function of the parameters of the model,
$p_1$,  $\langle \rh \rangle$ and $\langle \rl \rangle$.


\section{Estimation of the variance of $k$}
\label{app:var_k}
In this appendix we will compute the variance on the number of consolidated connections in input to a neuron of $\popII$ (i.e., $\sigma^2_{k}$) which, as we have seen previously, enters the formula for the variance on the background signal. For the calculation we will use the table below which represents the two states, rate high (1) or rate low (0), for a single neuron of the population $\popII$ and for the presynaptic neurons of its input connections in a complete simulation over $\T$ patterns.

\begin{table}[H]
%\nolinenumbers
\centering
\resizebox{8cm}{3cm}{%
\begin{tabular}{|| c | c | c | c| c | c | c | c | c ||} 
 \hline
 t & $\mathcal{O}$ & $\mathcal{I}_0$ & $\mathcal{I}_1$ & .... & $\mathcal{I}_{k-1}$ & $\mathcal{I}_{k}$ & .... & $\mathcal{I}_{\C-1}$\\ [0.5ex] 
\hline\hline 
 0 & 1& $x_{00}$& & &  $x_{k-1,0}$& 0& & 0   \\
\hline 
 1 & 1&.. & & & ..&0 & & 0   \\ 
 \hline
 2 & 1&.. & & & ..&0 & & 0 \\
\hline
 .. & .. & ..& & & ..& .. & & ..\\
\hline
 m-1 & 1& $x_{0m}$ & & &  $x_{k-1,m-1}$&0 & & 0\\

\hline
.. &0 & & & & & & & \\
\hline
.. & ..& & & & & & & \\

 \hline
$\T-1$ &0 & & & & & & & \\ [1ex] 
\hline
\end{tabular}}
\vspace{2mm}
    \caption{Table representing
    %the rate in input to a single neuron of $\popII$. 
    the two states rate high (1) or rate low (0) for a single neuron of the population $\popII$ and for the presynaptic neurons of its input connections in a complete simulation.    
    Each row represents a training pattern, with index ranging from 0 to $\T-1$. 
    The first two columns represent the training pattern index $t$ and the rate level $\mathcal{O}$, high or low, of the $\popII$ neuron. 
    The other columns $\mathcal{I}_j$ represent the rate level, high or low, of the presynaptic neurons connected to the neuron of $\popII$ through its $C$ incoming connections. The entries for rate levels can be 0 or 1 for low rate and for high rate respectively; in case of continuous distribution of the rate, the two levels correspond to a rate over or under the threshold $\nu_{\text{t}}$. 
    The table shows the case in which the $\PII$ neuron is in the high-rate level for the first $m$ examples and in the low-rate level for $\T -m$ examples, while the last $\C - k$ presynaptic neurons
    are in the low-rate level for the first $m$ examples.
}
    \label{fig:rate_matrix}
\end{table}

Given the scheme of Table \ref{fig:rate_matrix}, we call:
\begin{itemize}
    \item $p_1$: probability that a neuron of $\popI$ is in the high-rate level, i.e. probability that a cell of a column $\mathcal{I}_j$ is equal to one;
    \item $p_2$: probability that the neuron of $\popII$ is in the high-rate level for a given example, i.e., probability that a cell of the column $\mathcal{O}$ is equal to one;
    \item $p_2^m$: probability that the neuron of $\popII$ is in the high-rate level for the first $m$ patterns;
    \item $(1-p_2)^{\T - m}$: probability that the neuron of $\popII$ is in the low-rate level for the remaining $\T - m$ patterns;
    \item $(1-p_1)^m$: probability that a neuron of $\popI$ is in the low-rate level for the first $m$ patterns;
    \item $1-(1-p_1)^m$: probability that a neuron of $\popI$ is in the high-rate level for at least one pattern out of the first $m$;
    \item $[1-(1-p_1)^m]^{k}$: probability that every neuron of $\popI$ of the columns
    $\mathcal{I}_0$, ...., $\mathcal{I}_{k-1}$
    is above threshold for at least one pattern among the first $m$;
    \item $(1-p_1)^{m(\C-k)}$: probability that every neuron of $\popI$ of the last $\C - k$ columns is below threshold for the first $m$ patterns.
\end{itemize}

Now we can combine all these results to calculate the probability that one neuron of $\popII$ and $k$ presynaptic neurons of its input connections are at the high level for $m$ generic patterns (i.e., not necessarily the first $m$). To do this we have to take into account that the neuron of $\popII$ will not necessarily be at the high level in the first $m$ examples and that the neurons of $\popI$ at the high level will not necessarily be the first $k$ ( as in the case shown in the table). 
For this we have to use binomial coefficients that will take into account all possible combinations
in the choice of $m$ patterns out of all possible $\T$ patterns and in the choice of $k$ presynaptic neurons out of a total of $\C$ connections:

\begin{equation}
\label{eq:Q_mk}
    Q(m,k) = \binom{\T}{m} p_{2}^m (1-p_2)^{\T-m}
    \binom{\C}{k}  \hspace{2mm} [1-(1-p_1)^m]^k  \hspace{2mm} (1-p_1)^{m(\C-k)} 
\end{equation}

The probability that $k$ connections of a generic neuron of $\popII$ are consolidated can be calculated by adding $Q(m,k)$ over all possible values of $m$:

\begin{equation}
    P(k) = \sum_{m=0}^\T Q(m,k)
\end{equation}

and the average number of consolidated connections can be calculated as:

\begin{equation}
\begin{split}
    \langle k \rangle &= \sum_{m,k}  k Q(m,k) = \sum_m \binom{\T}{m} p_{2}^m (1-p_2)^{\T-m} \sum_{k=0}^{\C} k \binom{\C}{k} (1-q_1^m)^k q_1^{m (\C-k)} =\\
    &= \sum_m \binom{\T}{m} p_{2}^m (1-p_2)^{\T-m} \C(1-q_1^m)
    =\\
    &= \C \Bigl[\hspace{0.2cm}\sum_m \binom{\T}{m} p_{2}^m (1-p_2)^{\T-m}
    - \sum_m \binom{\T}{m} (p_2 q_1)^m (1-p_2)^{\T-m} \Bigr]
    =\\
    &= \C \Bigl[\hspace{0.2cm}
    1 - \sum_m \binom{\T}{m} (p_2 - p_1 p_2)^m (1-p_2)^{\T-m} \Bigr]
    \end{split}
\end{equation}

where $q_1 = 1 - p_1$ and we have used the formula for the mean value of a binomial distribution:

\begin{equation}
\sum_{k=0}^{n} k \binom{n}{k} p^k (1-p)^{n-k}= np
\end{equation}

Using the relationship:

\begin{equation}
    (a+b)^n=\sum_{k=0}^n \binom{n}{k} a^k b^{n-k}
\end{equation}

we can get the expression of $\langle k \rangle$:

\begin{equation}
\label{eq:avg_k}
    \langle k \rangle = \C [1-(1-p_1 p_2)^\T]
\end{equation}

To calculate $\sigma_{k}^2$ we must also calculate $\langle k^2 \rangle$:

\begin{equation}
\begin{split}
    \langle k^2 \rangle= \sum_{m,n} Q(m,k) k^2 \hspace{2.5cm}\\
    \langle k^2 \rangle= \sum_m \binom{\T}{m} p_{2}^m (1-p_2)^{T-m} \cdot \sum_{k=0}^C \binom{\C}{k} (1-q_1^m)^k q_1^{m (C-k)} k^2
    \end{split}
\end{equation}

After some calculations, analogous to the case of
$\langle k \rangle $, we obtain the following formula:

\begin{equation}
\label{eq:avg_k2}
    \langle k^2 \rangle= C(C-1) [1+ p_1 p_2 (p_1 - 2)]^T
    - C (2C - 1) (1 - p_1 p_2)^T + C^2
\end{equation}

Finally, the variance can be calculated from equations
\ref{eq:avg_k} and \ref{eq:avg_k2} as

\begin{equation}
    \sigma_{k}^{2} = \langle k^2 \rangle 
    - \langle k \rangle^2 
\end{equation}

\section{Noise addition during test phase}
\label{app:noise}
As anticipated in section \ref{subseq:noise}
in order to assess the generalization capacity of the model proposed in this work, the test input patterns were generated starting from the corresponding training input patterns by adding noise with a given probability distribution.
More specifically, each test pattern is generated by adding to the rate of the corresponding training pattern the contribution of a further extraction from
a truncated Gaussian distribution
$G(\eta)_{\mu_{\text{T}}, \sigma_{\text{T}}}$.
 Therefore the single neuron rate in a test pattern will be given by the following formula:

\begin{equation}
    \nu_{\text{tot}} = \nu + \eta
\end{equation}

where $\eta$ is a rate driven by the distribution $G(\eta)_{\mu_{\text{T}}, \sigma_{\text{T}}}$.
The input signal to a neuron of the population $\popII$ can be expressed as the scalar product between the vector 
$\vec{\mathcal{W}}$ of the weights and the vector
$\vec{\nu}_{\text{tot}}$ of the rates of the presynaptic neurons:

\begin{equation}
    \vec{\mathcal{W}}\cdot\vec{\nu}_{\text{tot}} = \vec{\mathcal{W}}\cdot\vec{\nu} + \vec{\mathcal{W}}\cdot\vec{\eta}
\end{equation}
Since the noise distribution has zero mean, 
its contribution to the average values of the signals
in input to the coding and non-coding neurons,
$\langle\SII\rangle$ and $\langle\Sb\rangle$, will be zero.
On the other hand, it will affect the variance
of the background signal, $\varSb$.
Since $\nu$ and $\eta$ are independent and random variables, the overall variance will be equal to the sum of the variance in the absence of noise $\varSb$ 
(see Equation \ref{eq:var_sb2_cont})
plus the variance due to noise.
Thus
\begin{equation}
\label{eq:noisy_varSb}
    \sigma^{*2}_{\text{b}} =
    \varSb +
    \langle k \rangle
  (\Wc \sigma_{\eta})^2
  + (C - \langle k \rangle)
    (\Wb \sigma_{\eta})^2 =
    \varSb + \sigma^2_{\eta} \C \Bigl[ p \Wc^2 + (1-p)\Wb^2 \Bigr]
\end{equation}
where
$\sigma^2_{\eta} = \sigma^2_{\text{T}}$
is the variance of $G(\eta)_{\mu_{\text{T}}, \sigma_{\text{T}}}$.
Truncating the Gaussian distribution in the symmetric interval $[-2\sigma, 2\sigma]$, the mean is zero, whereas the variance is

\begin{equation}
    \sigma^{2}_{\text{T}}= \sigma^2 \Bigl[1- \frac{4\cdot e^{-2}}{\sqrt{2\pi}\text{erf}(\sqrt{2})} \Bigr]
\end{equation}

\section{Calculation of the mean value of $k'_t$
over the rewiring steps}
\label{app:avg_k1t}
In section 2 we obtained the expression of $\SII$ in the presence of rewiring and we observed that this depends on the parameter $k'_t$, given by (Equation \ref{eq:avg_k1t}):

\begin{equation}
\label{eq:avg_k1t1}
\langle k'_t \rangle = p_t \C ( 1 - p_1 )
\end{equation}

 where, according to Eq. \ref{eq:pt}, $p_t$ is given by

\begin{equation}
p_t = 1 - (1 - p_1 p_2)^t
\end{equation}

In order to calculate the mean value of $\SII$ for all patterns, $k'_t$ should be averaged over all the values of the training index $t$ for which the rewiring is performed, i.e.

\begin{equation}
\label{eq:t_rewiring}
t = s i \qquad i = 0, \dots , \frac{\T}{s}
\end{equation}

where $s$ is the rewiring step and for simplicity we assume that $\T$ is a multiple of $s$ and that there is a final rewiring after the last training step.
The average of $p_t$ over the rewiring values of $t$ is

\begin{equation}
\langle p_t \rangle =
\frac{\sum_{i=0}^{\T/s} 1 - [(1 - p_1 p_2)^s]^i }
{\frac{\T}{s} + 1} =
1 - \frac{\sum_{i=0}^{\T/s} [(1 - p_1 p_2)^s]^i }
{\frac{\T}{s} + 1}
= 1 - \frac{b s}{\T + s}
\end{equation}

where we introduced a parameter $b$ defined as

\begin{equation}
b = \sum_{i=0}^{T/s} [(1 - p_1 p_2)^s]^i
= \frac{1 - [(1 - p_1 p_2)^s]^{T/s + 1}}
{1 - (1 - p_1 p_2)^s}
= \frac{1 - (1 - p_1 p_2)^{T + s}}
{1 - (1 - p_1 p_2)^s}
\end{equation}


\end{document}



