@article {Roxin2011,
	author = {Alex Roxin and Nicolas Brunel and David Hansel and Gianluigi Mongillo and Carl van Vreeswijk},
	title = {On the Distribution of Firing Rates in Networks of Cortical Neurons},
	volume = {31},
	number = {45},
	pages = {16217--16226},
	year = {2011},
	doi = {10.1523/JNEUROSCI.1677-11.2011},
	publisher = {Society for Neuroscience},
	abstract = {The distribution of in vivo average firing rates within local cortical networks has been reported to be highly skewed and long tailed. The distribution of average single-cell inputs, conversely, is expected to be Gaussian by the central limit theorem. This raises the issue of how a skewed distribution of firing rates might result from a symmetric distribution of inputs. We argue that skewed rate distributions are a signature of the nonlinearity of the in vivo f{\textendash}I curve. During in vivo conditions, ongoing synaptic activity produces significant fluctuations in the membrane potential of neurons, resulting in an expansive nonlinearity of the f{\textendash}I curve for low and moderate inputs. Here, we investigate the effects of single-cell and network parameters on the shape of the f{\textendash}I curve and, by extension, on the distribution of firing rates in randomly connected networks.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/31/45/16217},
	eprint = {https://www.jneurosci.org/content/31/45/16217.full.pdf},
	journal = {Journal of Neuroscience}
}

@book{gsl,
author = {Gough, Brian},
title = {GNU Scientific Library Reference Manual - Third Edition},
year = {2009},
isbn = {0954612078},
publisher = {Network Theory Ltd.},
edition = {3rd},
abstract = {The GNU Scientific Library (GSL) is a free numerical library for C and C++ programmers. It provides over 1,000 routines for solving mathematical problems in science and engineering. Written by the developers of GSL this reference manual is the definitive guide to the library. The GNU Scientific Library is free software, distributed under the GNU General Public License (GPL). All the money raised from the sale of this book supports the development of the GNU Scientific Library. This is the third edition of the manual, and corresponds to version 1.12 of the library (updated January 2009).}
}

@article{VonStVieth2021,
  doi = {10.17815/jlsrf-7-179},
  url = {https://doi.org/10.17815/jlsrf-7-179},
  year = {2021},
  month = oct,
  publisher = {Forschungszentrum Julich,  Zentralbibliothek},
  volume = {7},
  author = {Benedikt Von St. Vieth},
  title = {{JUSUF}: Modular Tier-2 Supercomputing and Cloud Infrastructure at J\"{u}lich Supercomputing Centre},
  journal = {Journal of large-scale research facilities {JLSRF}}
}

@article{sakai2020,
author = {Jill Sakai },
title = {How synaptic pruning shapes neural wiring during development and, possibly, in disease},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {28},
pages = {16096-16099},
year = {2020},
doi = {10.1073/pnas.2010281117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2010281117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2010281117}}

@article{Lamprecht2004,
  doi = {10.1038/nrn1301},
  url = {https://doi.org/10.1038/nrn1301},
  year = {2004},
  month = jan,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {5},
  number = {1},
  pages = {45--54},
  author = {Raphael Lamprecht and Joseph LeDoux},
  title = {Structural plasticity and memory},
  journal = {Nature Reviews Neuroscience}
}

@article{Huttenlocher1979,
title = {Synaptic density in human frontal cortex — Developmental changes and effects of aging},
journal = {Brain Research},
volume = {163},
number = {2},
pages = {195-205},
year = {1979},
issn = {0006-8993},
doi = {https://doi.org/10.1016/0006-8993(79)90349-4},
url = {https://www.sciencedirect.com/science/article/pii/0006899379903494},
author = {Huttenlocher, Peter R.},
abstract = {Density of synaptic profiles in layer 3 of middle frontal gyrus was quantitated in 21 normal human brains ranging from newborn to age 90 years. Synaptic profiles could be reliably demonstrated by the phosphotungstic acid method (Bloom and Aghajanian3) in tissue fixed up to 36 h postmortem. Synaptic density was constant throughout adult life (ages 16–72 years) with a mean of 11.05 × 108synapses/cu.mm ± 0.41 S.E.M.. There was a slight decline in synaptic density in brains of the aged (ages 74–90 years) with a mean of 9.56 × 108synapses/cu.mm ± 0.28S.E.M. in 4 samples (P < 0.05). Synaptic density in neonatal brains was already high—in the range seen in adults. However, synaptic morphology differed; immature profiles had an irregular presynaptic dense band instead of the separate presynaptic projections seen in mature synapses. Synaptic density increased during infancy, reaching a maximum at age 1–2 years which was about 50\% above the adult mean. The decline in synaptic density observed between ages 2–16 years was accompanied by a slight decrease in neuronal density. Human cerebral cortex is one of a number of neuronal systems in which loss of neurons and synapses appears to occur as a late developmental event.}
}

@article{Knoblauch2014,
    doi = {10.1371/journal.pone.0096485},
    author = {Knoblauch, Andreas AND Körner, Edgar AND Körner, Ursula AND Sommer, Friedrich T.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Structural Synaptic Plasticity Has High Memory Capacity and Can Explain Graded Amnesia, Catastrophic Forgetting, and the Spacing Effect},
    year = {2014},
    month = {05},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pone.0096485},
    pages = {1-19},
    abstract = {Although already William James and, more explicitly, Donald Hebb's theory of cell assemblies have suggested that activity-dependent rewiring of neuronal networks is the substrate of learning and memory, over the last six decades most theoretical work on memory has focused on plasticity of existing synapses in prewired networks. Research in the last decade has emphasized that structural modification of synaptic connectivity is common in the adult brain and tightly correlated with learning and memory. Here we present a parsimonious computational model for learning by structural plasticity. The basic modeling units are “potential synapses” defined as locations in the network where synapses can potentially grow to connect two neurons. This model generalizes well-known previous models for associative learning based on weight plasticity. Therefore, existing theory can be applied to analyze how many memories and how much information structural plasticity can store in a synapse. Surprisingly, we find that structural plasticity largely outperforms weight plasticity and can achieve a much higher storage capacity per synapse. The effect of structural plasticity on the structure of sparsely connected networks is quite intuitive: Structural plasticity increases the “effectual network connectivity”, that is, the network wiring that specifically supports storage and recall of the memories. Further, this model of structural plasticity produces gradients of effectual connectivity in the course of learning, thereby explaining various cognitive phenomena including graded amnesia, catastrophic forgetting, and the spacing effect.},
    number = {5},
}

@ARTICLE{Knoblauch2016,
AUTHOR={Knoblauch, Andreas and Sommer, Friedrich T.},
TITLE={Structural Plasticity, Effectual Connectivity, and Memory in Cortex},
JOURNAL={Frontiers in Neuroanatomy},
VOLUME={10},
YEAR={2016},
URL={https://www.frontiersin.org/articles/10.3389/fnana.2016.00063},
DOI={10.3389/fnana.2016.00063},
ISSN={1662-5129},
ABSTRACT={Learning and memory is commonly attributed to the modification of synaptic strengths in neuronal networks. More recent experiments have also revealed a major role of structural plasticity including elimination and regeneration of synapses, growth and retraction of dendritic spines, and remodeling of axons and dendrites. Here we work out the idea that one likely function of structural plasticity is to increase “effectual connectivity” in order to improve the capacity of sparsely connected networks to store Hebbian cell assemblies that are supposed to represent memories. For this we define effectual connectivity as the fraction of synaptically linked neuron pairs within a cell assembly representing a memory. We show by theory and numerical simulation the close links between effectual connectivity and both information storage capacity of neural networks and effective connectivity as commonly employed in functional brain imaging and connectome analysis. Then, by applying our model to a recently proposed memory model, we can give improved estimates on the number of cell assemblies that can be stored in a cortical macrocolumn assuming realistic connectivity. Finally, we derive a simplified model of structural plasticity to enable large scale simulation of memory phenomena, and apply our model to link ongoing adult structural plasticity to recent behavioral data on the spacing effect of learning.}
}

@article{Fu2011,
title = {Experience-dependent structural plasticity in the cortex},
journal = {Trends in Neurosciences},
volume = {34},
number = {4},
pages = {177-187},
year = {2011},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166223611000178},
author = {Min Fu and Yi Zuo},
abstract = {Synapses are the fundamental units of neuronal circuits. Synaptic plasticity can occur through changes in synaptic strength, as well as through the addition/removal of synapses. Two-photon microscopy in combination with fluorescence labeling offers a powerful tool to peek into the living brain and follow structural reorganization at individual synapses. Time-lapse imaging depicts a dynamic picture in which experience-dependent plasticity of synaptic structures varies between different cortical regions and layers, as well as between neuronal subtypes. Recent studies have demonstrated that the formation and elimination of synaptic structures happens rapidly in a subpopulation of cortical neurons during various sensorimotor learning experiences, and that stabilized synaptic structures are associated with long lasting memories for the task. Therefore, circuit plasticity, mediated by structural remodeling, provides an underlying mechanism for learning and memory.}
}

@article{Butz2009,
title = {Activity-dependent structural plasticity},
journal = {Brain Research Reviews},
volume = {60},
number = {2},
pages = {287-305},
year = {2009},
issn = {0165-0173},
doi = {https://doi.org/10.1016/j.brainresrev.2008.12.023},
url = {https://www.sciencedirect.com/science/article/pii/S0165017308001513},
author = {Markus Butz and Florentin Wörgötter and Arjen {van Ooyen}},
keywords = {Structural Plasticity, Lesion-induced plasticity, Experience-dependent plasticity, Synaptic rewiring, Homeostasis, Activity-dependent neurite outgrowth, Development},
abstract = {Plasticity in the brain reaches far beyond a mere changing of synaptic strengths. Recent time-lapse imaging in the living brain reveals ongoing structural plasticity by forming or breaking of synapses, motile spines, and re-routing of axonal branches in the developing and adult brain. Some forms of structural plasticity do not follow Hebbian- or anti-Hebbian paradigms of plasticity but rather appear to contribute to the homeostasis of network activity. Four decades of lesion studies have brought up a wealth of data on the mutual interdependence of neuronal activity, neurotransmitter release and neuronal morphogenesis and network formation. Here, we review these former studies on structural plasticity in the context of recent experimental studies. We compare spontaneous and experience-dependent structural plasticity with lesion-induced (reactive) structural plasticity that occurs during development and in the adult brain. Understanding the principles of neural network reorganization on a structural level is relevant for a deeper understanding of long-term memory formation as well as for the treatment of neurological diseases such as stroke.}
}

@article{Chklovskii2004,
  doi = {10.1038/nature03012},
  url = {https://doi.org/10.1038/nature03012},
  year = {2004},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {431},
  number = {7010},
  pages = {782--788},
  author = {D. B. Chklovskii and B. W. Mel and K. Svoboda},
  title = {Cortical rewiring and information storage},
  journal = {Nature}
}

@ARTICLE{Spiess2016,
AUTHOR={Spiess, Robin and George, Richard and Cook, Matthew and Diehl, Peter U.},
TITLE={Structural Plasticity Denoises Responses and Improves Learning Speed},
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={10},
YEAR={2016},  
URL={https://www.frontiersin.org/articles/10.3389/fncom.2016.00093},
DOI={10.3389/fncom.2016.00093},
ISSN={1662-5188},
ABSTRACT={Despite an abundance of computational models for learning of synaptic weights, there has been relatively little research on structural plasticity, i.e., the creation and elimination of synapses. Especially, it is not clear how structural plasticity works in concert with spike-timing-dependent plasticity (STDP) and what advantages their combination offers. Here we present a fairly large-scale functional model that uses leaky integrate-and-fire neurons, STDP, homeostasis, recurrent connections, and structural plasticity to learn the input encoding, the relation between inputs, and to infer missing inputs. Using this model, we compare the error and the amount of noise in the network's responses with and without structural plasticity and the influence of structural plasticity on the learning speed of the network. Using structural plasticity during learning shows good results for learning the representation of input values, i.e., structural plasticity strongly reduces the noise of the response by preventing spikes with a high error. For inferring missing inputs we see similar results, with responses having less noise if the network was trained using structural plasticity. Additionally, using structural plasticity with pruning significantly decreased the time to learn weights suitable for inference. Presumably, this is due to the clearer signal containing less spikes that misrepresent the desired value. Therefore, this work shows that structural plasticity is not only able to improve upon the performance using STDP without structural plasticity but also speeds up learning. Additionally, it addresses the practical problem of limited resources for connectivity that is not only apparent in the mammalian neocortex but also in computer hardware or neuromorphic (brain-inspired) hardware by efficiently pruning synapses without losing performance.}
}

@article{Navlakha2015,
    doi = {10.1371/journal.pcbi.1004347},
    author = {Navlakha, Saket AND Barth, Alison L. AND Bar-Joseph, Ziv},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Decreasing-Rate Pruning Optimizes the Construction of Efficient and Robust Distributed Networks},
    year = {2015},
    month = {07},
    volume = {11},
    url = {https://doi.org/10.1371/journal.pcbi.1004347},
    pages = {1-23},
    abstract = {Robust, efficient, and low-cost networks are advantageous in both biological and engineered systems. During neural network development in the brain, synapses are massively over-produced and then pruned-back over time. This strategy is not commonly used when designing engineered networks, since adding connections that will soon be removed is considered wasteful. Here, we show that for large distributed routing networks, network function is markedly enhanced by hyper-connectivity followed by aggressive pruning and that the global rate of pruning, a developmental parameter not previously studied by experimentalists, plays a critical role in optimizing network structure. We first used high-throughput image analysis techniques to quantify the rate of pruning in the mammalian neocortex across a broad developmental time window and found that the rate is decreasing over time. Based on these results, we analyzed a model of computational routing networks and show using both theoretical analysis and simulations that decreasing rates lead to more robust and efficient networks compared to other rates. We also present an application of this strategy to improve the distributed design of airline networks. Thus, inspiration from neural network formation suggests effective ways to design distributed networks across several domains.},
    number = {7},
}

@article{Moyer2015,
title = {Dendritic spine alterations in schizophrenia},
journal = {Neuroscience Letters},
volume = {601},
pages = {46-53},
year = {2015},
note = {Dendritic Spine Dysgenesis in Neuropsychiatric Disease},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2014.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S0304394014009203},
author = {Caitlin E. Moyer and Micah A. Shelton and Robert A. Sweet},
keywords = {Schizophrenia, Dendritic spine, Postmortem, Adolescence, MAP2, Kalirin},
abstract = {Schizophrenia is a chronic illness affecting approximately 0.5–1\% of the world’s population. The etiology of schizophrenia is complex, including multiple genes, and contributing environmental effects that adversely impact neurodevelopment. Nevertheless, a final common result, present in many subjects with schizophrenia, is impairment of pyramidal neuron dendritic morphology in multiple regions of the cerebral cortex. In this review, we summarize the evidence of reduced dendritic spine density and other dendritic abnormalities in schizophrenia, evaluate current data that informs the neurodevelopment timing of these impairments, and discuss what is known about possible upstream sources of dendritic spine loss in this illness.}
}

@article{Bourgeron2009,
title = {A synaptic trek to autism},
journal = {Current Opinion in Neurobiology},
volume = {19},
number = {2},
pages = {231-234},
year = {2009},
note = {Development},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2009.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438809000592},
author = {Thomas Bourgeron},
abstract = {Autism spectrum disorders (ASD) are diagnosed on the basis of three behavioral features namely deficits in social communication, absence or delay in language, and stereotypy. The susceptibility genes to ASD remain largely unknown, but two major pathways are emerging. Mutations in TSC1/TSC2, NF1, or PTEN activate the mTOR/PI3K pathway and lead to syndromic ASD with tuberous sclerosis, neurofibromatosis, or macrocephaly. Mutations in NLGN3/4, SHANK3, or NRXN1 alter synaptic function and lead to mental retardation, typical autism, or Asperger syndrome. The mTOR/PI3K pathway is associated with abnormal cellular/synaptic growth rate, whereas the NRXN–NLGN–SHANK pathway is associated with synaptogenesis and imbalance between excitatory and inhibitory currents. Taken together, these data strongly suggest that abnormal synaptic homeostasis represent a risk factor to ASD.}
}

@article{Hutsler2010,
title = {Increased dendritic spine densities on cortical projection neurons in autism spectrum disorders},
journal = {Brain Research},
volume = {1309},
pages = {83-94},
year = {2010},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2009.09.120},
url = {https://www.sciencedirect.com/science/article/pii/S0006899309023117},
author = {Jeffrey J. Hutsler and Hong Zhang},
keywords = {Cerebral cortex, Autistic disorder, Pyramidal cell, Dendritic spine, Neuroanatomy},
abstract = {Multiple types of indirect evidence have been used to support theories of altered cortical connectivity in autism spectrum disorders (ASD). In other developmental disorders reduced spine expression is commonly found, while conditions such as fragile X syndrome show increased spine densities. Despite its relevance to theories of altered cortical connectivity, synaptic spine expression has not been systematically explored in ASD. Here we examine dendritic spines on Golgi-impregnated cortical pyramidal cells in the cortex of ASD subjects and age-matched control cases. Pyramidal cells were studied within both the superficial and deep cortical layers of frontal, temporal, and parietal lobe regions. Relative to controls, spine densities were greater in ASD subjects. In analyses restricted to the apical dendrites of pyramidal cells, greater spine densities were found predominantly within layer II of each cortical location and within layer V of the temporal lobe. High spine densities were associated with decreased brain weights and were most commonly found in ASD subjects with lower levels of cognitive functioning. Greater spine densities in ASD subjects provide structural support for recent suggestions of connectional changes within the cerebral cortex that may result in altered cortical computations.}
}

@article{Pagani2021,
  doi = {10.1038/s41467-021-26131-z},
  url = {https://doi.org/10.1038/s41467-021-26131-z},
  year = {2021},
  month = oct,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {12},
  number = {1},
  author = {Marco Pagani and Noemi Barsotti and Alice Bertero and Stavros Trakoshis and Laura Ulysse and Andrea Locarno and Ieva Miseviciute and Alessia De Felice and Carola Canella and Kaustubh Supekar and Alberto Galbusera and Vinod Menon and Raffaella Tonini and Gustavo Deco and Michael V. Lombardo and Massimo Pasqualetti and Alessandro Gozzi},
  title = {{mTOR}-related synaptic pathology causes autism spectrum disorder-associated functional hyperconnectivity},
  journal = {Nature Communications}
}

@article{Glantz2000,
    author = {Glantz, Leisa A. and Lewis, David A.},
    title = "{Decreased Dendritic Spine Density on Prefrontal Cortical Pyramidal Neurons in Schizophrenia}",
    journal = {Archives of General Psychiatry},
    volume = {57},
    number = {1},
    pages = {65-73},
    year = {2000},
    month = {01},
    abstract = "{The pathophysiological characteristics of schizophrenia appear to involve altered synaptic connectivity in the dorsolateral prefrontal cortex. Given the central role that layer 3 pyramidal neurons play in corticocortical and thalamocortical connectivity, we hypothesized that the excitatory inputs to these neurons are altered in subjects with schizophrenia.To test this hypothesis, we determined the density of dendritic spines, markers of excitatory inputs, on the basilar dendrites of Golgi-impregnated pyramidal neurons in the superficial and deep portions of layer 3 in the dorsolateral prefrontal cortex (area 46) and in layer 3 of the primary visual cortex (area 17) of 15 schizophrenic subjects, 15 normal control subjects, and 15 nonschizophrenic subjects with a psychiatric illness (referred to as psychiatric subjects).There was a significant effect of diagnosis on spine density only for deep layer 3 pyramidal neurons in area 46 (P = .006). In the schizophrenic subjects, spine density on these neurons was decreased by 23\% and 16\% compared with the normal control (P = .004) and psychiatric (P = .08) subjects, respectively. In contrast, spine density on neurons in superficial layer 3 in area 46 (P = .09) or in area 17 (P = .08) did not significantly differ across the 3 subject groups. Furthermore, spine density on deep layer 3 neurons in area 46 did not significantly (P = .81) differ between psychiatric subjects treated with antipsychotic agents and normal controls.This region- and disease-specific decrease in dendritic spine density on dorsolateral prefrontal cortex layer 3 pyramidal cells is consistent with the hypothesis that the number of cortical and/or thalamic excitatory inputs to these neurons is altered in subjects with schizophrenia.Arch Gen Psychiatry. 2000;57:65-73-->}",
    issn = {0003-990X},
    doi = {10.1001/archpsyc.57.1.65},
    url = {https://doi.org/10.1001/archpsyc.57.1.65},
    eprint = {https://jamanetwork.com/journals/jamapsychiatry/articlepdf/481552/yoa9030.pdf},
}

@article{Gutig2003,
  doi = {10.1523/jneurosci.23-09-03697.2003},
  url = {https://doi.org/10.1523/jneurosci.23-09-03697.2003},
  year = {2003},
  month = may,
  publisher = {Society for Neuroscience},
  volume = {23},
  number = {9},
  pages = {3697--3714},
  author = {R. G\"{u}tig and R. Aharonov and S. Rotter and Haim Sompolinsky},
  title = {Learning Input Correlations through Nonlinear Temporally Asymmetric Hebbian Plasticity},
  journal = {The Journal of Neuroscience}
}

@article{tsodyks1998,
    author = {Tsodyks, Misha and Pawelzik, Klaus and Markram, Henry},
    title = "{Neural Networks with Dynamic Synapses}",
    journal = {Neural Computation},
    volume = {10},
    number = {4},
    pages = {821-835},
    year = {1998},
    month = {05},
    abstract = "{Transmission across neocortical synapses depends on the frequency of presynaptic activity (Thomson \\&amp; Deuchars, 1994). Interpyramidal synapses in layer V exhibit fast depression of synaptic transmission, while other types of synapses exhibit facilitation of transmission. To study the role of dynamic synapses in network computation, we propose a unified phenomenological model that allows computation of the postsynaptic current generated by both types of synapses when driven by an arbitrary pattern of action potential (AP) activity in a presynaptic population. Using this formalism, we analyze different regimes of synaptic transmission and demonstrate that dynamic synapses transmit different aspects of the presynaptic activity depending on the average presynaptic frequency. The model also allows for derivation of mean-field equations, which govern the activity of large, interconnected networks. We show that the dynamics of synaptic transmission results in complex sets of regular and irregular regimes of network activity.}",
    issn = {0899-7667},
    doi = {10.1162/089976698300017502},
    url = {https://doi.org/10.1162/089976698300017502},
    eprint = {https://direct.mit.edu/neco/article-pdf/10/4/821/813807/089976698300017502.pdf},
}

@article{Rigotti2013,
  doi = {10.1038/nature12160},
  url = {https://doi.org/10.1038/nature12160},
  year = {2013},
  month = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {497},
  number = {7451},
  pages = {585--590},
  author = {Mattia Rigotti and Omri Barak and Melissa R. Warden and Xiao-Jing Wang and Nathaniel D. Daw and Earl K. Miller and Stefano Fusi},
  title = {The importance of mixed selectivity in complex cognitive tasks},
  journal = {Nature}
}

@article{Coultrip1992,
title = {A cortical model of winner-take-all competition via lateral inhibition},
journal = {Neural Networks},
volume = {5},
number = {1},
pages = {47-54},
year = {1992},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80006-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005800061},
author = {Robert Coultrip and Richard Granger and Gary Lynch},
keywords = {Winner-take-all circuits, Lateral inhibition},
abstract = {Simulations were performed of physiological interactions among excitatory and inhibitory neurons in anatomically realistic local-circuit architectures modeled after hippocampal field CA1. The simulated circuitry consists of several excitatory neurons jointly innervating and receiving feedback from a common inhibitory interneuron. Excitatory cells in the simulation receive input during a cycle of naturally-occurring rhythmic activity (the hippocampal theta rhythm), and the neuron receiving the most input activation is the first to reach its spiking threshold. Spiking excites the inhibitory cell, which in turn prevents other cells from responding. The result is the natural generation of a simple competitive or “winner-take-all” (WTA) mechanism, allowing only the most strongly-activated cell in a group or “patch” to respond with spiking activity. Formal mathematical characterization of the mechanism reveals specific physiological characteristics of the input to the network, which enable it to closely approximate an ideal winner-take-all mechanism. Unlike other, more abstract WTA mechanisms that have been proposed, the parameters of this biologically-derived WTA mechanism can be directly related to specific physiological and anatomical features of particular cortical circuits.}
}


@article{Mattson1988,
  doi = {10.1016/0165-0173(88)90020-3},
  url = {https://doi.org/10.1016/0165-0173(88)90020-3},
  year = {1988},
  month = apr,
  publisher = {Elsevier {BV}},
  volume = {13},
  number = {2},
  pages = {179--212},
  author = {Mark P. Mattson},
  title = {Neurotransmitters in the regulation of neuronal cytoarchitecture},
  journal = {Brain Research Reviews}
}

@article{Richards2005,
  doi = {10.1073/pnas.0501881102},
  url = {https://doi.org/10.1073/pnas.0501881102},
  year = {2005},
  month = apr,
  publisher = {Proceedings of the National Academy of Sciences},
  volume = {102},
  number = {17},
  pages = {6166--6171},
  author = {David A. Richards and Jos{\'{e}} Maria Mateos and Sylvain Hugel and Vincenzo de Paola and Pico Caroni and Beat H. G\"{a}hwiler and R. Anne McKinney},
  title = {Glutamate induces the rapid formation of spine head protrusions in hippocampal slice cultures},
  journal = {Proceedings of the National Academy of Sciences}
}

@ARTICLE{Fauth2016,
AUTHOR={Fauth, Michael and Tetzlaff, Christian},
TITLE={Opposing Effects of Neuronal Activity on Structural Plasticity},
JOURNAL={Frontiers in Neuroanatomy},
VOLUME={10},
YEAR={2016},
URL={https://www.frontiersin.org/articles/10.3389/fnana.2016.00075},
DOI={10.3389/fnana.2016.00075},
ISSN={1662-5129},
ABSTRACT={The connectivity of the brain is continuously adjusted to new environmental influences by several activity-dependent adaptive processes. The most investigated adaptive mechanism is activity-dependent functional or synaptic plasticity regulating the transmission efficacy of existing synapses. Another important but less prominently discussed adaptive process is structural plasticity, which changes the connectivity by the formation and deletion of synapses. In this review, we show, based on experimental evidence, that structural plasticity can be classified similar to synaptic plasticity into two categories: (i) Hebbian structural plasticity, which leads to an increase (decrease) of the number of synapses during phases of high (low) neuronal activity and (ii) homeostatic structural plasticity, which balances these changes by removing and adding synapses. Furthermore, based on experimental and theoretical insights, we argue that each type of structural plasticity fulfills a different function. While Hebbian structural changes enhance memory lifetime, storage capacity, and memory robustness, homeostatic structural plasticity self-organizes the connectivity of the neural network to assure stability. However, the link between functional synaptic and structural plasticity as well as the detailed interactions between Hebbian and homeostatic structural plasticity are more complex. This implies even richer dynamics requiring further experimental and theoretical investigations.}
}

@ARTICLE{Tiddia2022,
AUTHOR={Tiddia, Gianmarco and Golosio, Bruno and Albers, Jasper and Senk, Johanna and Simula, Francesco and Pronold, Jari and Fanti, Viviana and Pastorelli, Elena and Paolucci, Pier Stanislao and van Albada, Sacha J.},
TITLE={Fast Simulation of a Multi-Area Spiking Network Model of Macaque Cortex on an MPI-GPU Cluster},
JOURNAL={Frontiers in Neuroinformatics},
VOLUME={16},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fninf.2022.883333},
DOI={10.3389/fninf.2022.883333},
ISSN={1662-5196},
ABSTRACT={Spiking neural network models are increasingly establishing themselves as an effective tool for simulating the dynamics of neuronal populations and for understanding the relationship between these dynamics and brain function. Furthermore, the continuous development of parallel computing technologies and the growing availability of computational resources are leading to an era of large-scale simulations capable of describing regions of the brain of ever larger dimensions at increasing detail. Recently, the possibility to use MPI-based parallel codes on GPU-equipped clusters to run such complex simulations has emerged, opening up novel paths to further speed-ups. NEST GPU is a GPU library written in CUDA-C/C++ for large-scale simulations of spiking neural networks, which was recently extended with a novel algorithm for remote spike communication through MPI on a GPU cluster. In this work we evaluate its performance on the simulation of a multi-area model of macaque vision-related cortex, made up of about 4 million neurons and 24 billion synapses and representing 32 mm<sup>2</sup> surface area of the macaque cortex. The outcome of the simulations is compared against that obtained using the well-known CPU-based spiking neural network simulator NEST on a high-performance computing cluster. The results show not only an optimal match with the NEST statistical measures of the neural activity in terms of three informative distributions, but also remarkable achievements in terms of simulation time per second of biological activity. Indeed, NEST GPU was able to simulate a second of biological time of the full-scale macaque cortex model in its metastable state 3.1× faster than NEST using 32 compute nodes equipped with an NVIDIA V100 GPU each. Using the same configuration, the ground state of the full-scale macaque cortex model was simulated 2.4× faster than NEST.}
}

@ARTICLE{Tiddia2022_WM,
AUTHOR={Tiddia, Gianmarco and Golosio, Bruno and Fanti, Viviana and Paolucci, Pier Stanislao},
TITLE={Simulations of working memory spiking networks driven by short-term plasticity},
JOURNAL={Frontiers in Integrative Neuroscience},
VOLUME={16},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fnint.2022.972055},
DOI={10.3389/fnint.2022.972055},
ISSN={1662-5145},
ABSTRACT={Working Memory (WM) is a cognitive mechanism that enables temporary holding and manipulation of information in the human brain. This mechanism is mainly characterized by a neuronal activity during which neuron populations are able to maintain an enhanced spiking activity after being triggered by a short external cue. In this study, we implement, using the NEST simulator, a spiking neural network model in which the WM activity is sustained by a mechanism of short-term synaptic facilitation related to presynaptic calcium kinetics. The model, which is characterized by leaky integrate-and-fire neurons with exponential postsynaptic currents, is able to autonomously show an activity regime in which the memory information can be stored in a synaptic form as a result of synaptic facilitation, with spiking activity functional to facilitation maintenance. The network is able to simultaneously keep multiple memories by showing an alternated synchronous activity which preserves the synaptic facilitation within the neuron populations holding memory information. The results shown in this study confirm that a WM mechanism can be sustained by synaptic facilitation.}
}

@ARTICLE{NEST,
  author  = {Marc-Oliver Gewaltig and Markus Diesmann},
  title   = {NEST (NEural Simulation Tool)},
  journal = {Scholarpedia},
  year    = {2007},
  volume  = {2},
  pages   = {1430},
  number  = {4}
}

@article{ThaCo,
    doi = {10.1371/journal.pcbi.1009045},
    author = {Golosio, Bruno AND De Luca, Chiara AND Capone, Cristiano AND Pastorelli, Elena AND Stegel, Giovanni AND Tiddia, Gianmarco AND De Bonis, Giulia AND Paolucci, Pier Stanislao},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Thalamo-cortical spiking model of incremental learning combining perception, context and NREM-sleep},
    year = {2021},
    month = {06},
    volume = {17},
    url = {https://doi.org/10.1371/journal.pcbi.1009045},
    pages = {1-26},
    abstract = {The brain exhibits capabilities of fast incremental learning from few noisy examples, as well as the ability to associate similar memories in autonomously-created categories and to combine contextual hints with sensory perceptions. Together with sleep, these mechanisms are thought to be key components of many high-level cognitive functions. Yet, little is known about the underlying processes and the specific roles of different brain states. In this work, we exploited the combination of context and perception in a thalamo-cortical model based on a soft winner-take-all circuit of excitatory and inhibitory spiking neurons. After calibrating this model to express awake and deep-sleep states with features comparable with biological measures, we demonstrate the model capability of fast incremental learning from few examples, its resilience when proposed with noisy perceptions and contextual signals, and an improvement in visual classification after sleep due to induced synaptic homeostasis and association of similar memories.},
    number = {6},

}

@article{Mongillo2008,
author = {Gianluigi Mongillo  and Omri Barak  and Misha Tsodyks },
title = {Synaptic Theory of Working Memory},
journal = {Science},
volume = {319},
number = {5869},
pages = {1543-1546},
year = {2008},
doi = {10.1126/science.1150769},
URL = {https://www.science.org/doi/abs/10.1126/science.1150769},
eprint = {https://www.science.org/doi/pdf/10.1126/science.1150769},
abstract = {It is usually assumed that enhanced spiking activity in the form of persistent reverberation for several seconds is the neural correlate of working memory. Here, we propose that working memory is sustained by calcium-mediated synaptic facilitation in the recurrent connections of neocortical networks. In this account, the presynaptic residual calcium is used as a buffer that is loaded, refreshed, and read out by spiking activity. Because of the long time constants of calcium kinetics, the refresh rate can be low, resulting in a mechanism that is metabolically efficient and robust. The duration and stability of working memory can be regulated by modulating the spontaneous activity in the network.}}

@article{Song2000,
  doi = {10.1038/78829},
  url = {https://doi.org/10.1038/78829},
  year = {2000},
  month = sep,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {3},
  number = {9},
  pages = {919--926},
  author = {Sen Song and Kenneth D. Miller and L. F. Abbott},
  title = {Competitive Hebbian learning through spike-timing-dependent synaptic plasticity},
  journal = {Nature Neuroscience}
}

@article{Bi2001,
  doi = {10.1146/annurev.neuro.24.1.139},
  url = {https://doi.org/10.1146/annurev.neuro.24.1.139},
  year = {2001},
  month = mar,
  publisher = {Annual Reviews},
  volume = {24},
  number = {1},
  pages = {139--166},
  author = {Guo-qiang Bi and Mu-ming Poo},
  title = {Synaptic Modification by Correlated Activity: Hebb{\textquotesingle}s Postulate Revisited},
  journal = {Annual Review of Neuroscience}
}

@mastersthesis{Sergi2023,
  author  = "Luca Sergi",
  title   = "Teoria di campo medio dell'apprendimento nei sistemi neurali biologici attraverso la plasticità sinaptica strutturale",
  school  = "Department of Physics, University of Cagliari, Italy",
  month   = "July",
  year    = "2023",
  type    = "Master's Thesis",
  note    = "",
  annote  = ""
}


@article{Butts2007,
  doi = {10.1371/journal.pbio.0050061},
  url = {https://doi.org/10.1371/journal.pbio.0050061},
  year = {2007},
  month = mar,
  publisher = {Public Library of Science ({PLoS})},
  volume = {5},
  number = {3},
  pages = {e61},
  author = {Daniel A Butts and Patrick O Kanold and Carla J Shatz},
  editor = {Charles F Stevens},
  title = {A Burst-Based {\textquotedblleft}Hebbian{\textquotedblright} Learning Rule at Retinogeniculate Synapses Links Retinal Waves to Activity-Dependent Refinement},
  journal = {{PLoS} Biology}
}

@article{Payeur2021,
  doi = {10.1038/s41593-021-00857-x},
  url = {https://doi.org/10.1038/s41593-021-00857-x},
  year = {2021},
  month = may,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {24},
  number = {7},
  pages = {1010--1019},
  author = {Alexandre Payeur and Jordan Guerguiev and Friedemann Zenke and Blake A. Richards and Richard Naud},
  title = {Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits},
  journal = {Nature Neuroscience}
}


@InProceedings{Capone2022,
  title = 	 {Burst-Dependent Plasticity and Dendritic Amplification Support Target-Based Learning and Hierarchical Imitation Learning},
  author =       {Capone, Cristiano and Lupo, Cosimo and Muratore, Paolo and Paolucci, Pier Stanislao},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2625--2637},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/capone22b/capone22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/capone22b.html},
  abstract = 	 {The brain can learn to solve a wide range of tasks with high temporal and energetic efficiency. However, most biological models are composed of simple single-compartment neurons and cannot achieve the state-of-the-art performances of artificial intelligence. We propose a multi-compartment model of pyramidal neuron, in which bursts and dendritic input segregation give the possibility to plausibly support a biological target-based learning. In target-based learning, the internal solution of a problem (a spatio-temporal pattern of bursts in our case) is suggested to the network, bypassing the problems of error backpropagation and credit assignment. Finally, we show that this neuronal architecture naturally supports the orchestration of “hierarchical imitation learning”, enabling the decomposition of challenging long-horizon decision-making tasks into simpler subtasks.}
}



@article{Carlu2020,
author = {Carlu, M. and Chehab, O. and Dalla Porta, L. and Depannemaecker, D. and H\'{e}ric\'{e}, C. and Jedynak, M. and K\"{o}ksal Ers\"{o}z, E. and Muratore, P. and Souihel, S. and Capone, C. and Zerlaut, Y. and Destexhe, A. and di Volo, M.},
title = {A mean-field approach to the dynamics of networks of complex neurons, from nonlinear Integrate-and-Fire to Hodgkin–Huxley models},
journal = {Journal of Neurophysiology},
volume = {123},
number = {3},
pages = {1042-1051},
year = {2020},
doi = {10.1152/jn.00399.2019},
note ={PMID: 31851573},
URL = {https://doi.org/10.1152/jn.00399.2019},
eprint = {https://doi.org/10.1152/jn.00399.2019},
abstract = { We present a mean-field formalism able to predict the collective dynamics of large networks of conductance-based interacting spiking neurons. We apply this formalism to several neuronal models, from the simplest Adaptive Exponential Integrate-and-Fire model to the more complex Hodgkin–Huxley and Morris–Lecar models. We show that the resulting mean-field models are capable of predicting the correct spontaneous activity of both excitatory and inhibitory neurons in asynchronous irregular regimes, typical of cortical dynamics. Moreover, it is possible to quantitatively predict the population response to external stimuli in the form of external spike trains. This mean-field formalism therefore provides a paradigm to bridge the scale between population dynamics and the microscopic complexity of the individual cells physiology.NEW \& NOTEWORTHY Population models are a powerful mathematical tool to study the dynamics of neuronal networks and to simulate the brain at macroscopic scales. We present a mean-field model capable of quantitatively predicting the temporal dynamics of a network of complex spiking neuronal models, from Integrate-and-Fire to Hodgkin–Huxley, thus linking population models to neurons electrophysiology. This opens a perspective on generating biologically realistic mean-field models from electrophysiological recordings. }
}

@article{Markram2015,
  doi = {10.1016/j.cell.2015.09.029},
  url = {https://doi.org/10.1016/j.cell.2015.09.029},
  year = {2015},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {163},
  number = {2},
  pages = {456--492},
  author = {Henry Markram and Eilif Muller and Srikanth Ramaswamy and Michael~W. Reimann and Marwan Abdellah and Carlos~Aguado Sanchez and Anastasia Ailamaki and Lidia Alonso-Nanclares and Nicolas Antille and Selim Arsever and Guy~Antoine~Atenekeng Kahou and Thomas~K. Berger and Ahmet Bilgili and Nenad Buncic and Athanassia Chalimourda and Giuseppe Chindemi and Jean-Denis Courcol and Fabien Delalondre and Vincent Delattre and Shaul Druckmann and Raphael Dumusc and James Dynes and Stefan Eilemann and Eyal Gal and Michael~Emiel Gevaert and Jean-Pierre Ghobril and Albert Gidon and Joe~W. Graham and Anirudh Gupta and Valentin Haenel and Etay Hay and Thomas Heinis and Juan~B. Hernando and Michael Hines and Lida Kanari and Daniel Keller and John Kenyon and Georges Khazen and Yihwa Kim and James~G. King and Zoltan Kisvarday and Pramod Kumbhar and S{\'{e}}bastien Lasserre and Jean-Vincent Le~B{\'{e}} and Bruno~R.C. Magalh{\~{a}}es and Angel Merch{\'{a}}n-P{\'{e}}rez and Julie Meystre and Benjamin~Roy Morrice and Jeffrey Muller and Alberto Mu{\~{n}}oz-C{\'{e}}spedes and Shruti Muralidhar and Keerthan Muthurasa and Daniel Nachbaur and Taylor~H. Newton and Max Nolte and Aleksandr Ovcharenko and Juan Palacios and Luis Pastor and Rodrigo Perin and Rajnish Ranjan and Imad Riachi and Jos{\'{e}}-Rodrigo Rodr{\'{\i}}guez and Juan~Luis Riquelme and Christian R\"{o}ssert and Konstantinos Sfyrakis and Ying Shi and Julian~C. Shillcock and Gilad Silberberg and Ricardo Silva and Farhan Tauheed and Martin Telefont and Maria Toledo-Rodriguez and Thomas Tr\"{a}nkler and Werner Van~Geit and Jafet~Villafranca D{\'{\i}}az and Richard Walker and Yun Wang and Stefano~M. Zaninetta and Javier DeFelipe and Sean~L. Hill and Idan Segev and Felix Sch\"{u}rmann},
  title = {Reconstruction and Simulation of Neocortical Microcircuitry},
  journal = {Cell}
}

@article{Wilson1972,
  doi = {10.1016/s0006-3495(72)86068-5},
  url = {https://doi.org/10.1016/s0006-3495(72)86068-5},
  year = {1972},
  month = jan,
  publisher = {Elsevier {BV}},
  volume = {12},
  number = {1},
  pages = {1--24},
  author = {Hugh R. Wilson and Jack D. Cowan},
  title = {Excitatory and Inhibitory Interactions in Localized Populations of Model Neurons},
  journal = {Biophysical Journal}
}


@article{Hopfield1984,
author = {J J Hopfield },
title = {Neurons with graded response have collective computational properties like those of two-state neurons.},
journal = {Proceedings of the National Academy of Sciences},
volume = {81},
number = {10},
pages = {3088-3092},
year = {1984},
doi = {10.1073/pnas.81.10.3088},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.81.10.3088},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.81.10.3088},
abstract = {A model for a large network of "neurons" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological "neurons." Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.}}


@article{SanzLeon2013,
  doi = {10.3389/fninf.2013.00010},
  url = {https://doi.org/10.3389/fninf.2013.00010},
  year = {2013},
  publisher = {Frontiers Media {SA}},
  volume = {7},
  author = {Paula Sanz Leon and Stuart A. Knock and M. Marmaduke Woodman and Lia Domide and Jochen Mersmann and Anthony R. McIntosh and Viktor Jirsa},
  title = {The Virtual Brain: a simulator of primate brain network dynamics},
  journal = {Frontiers in Neuroinformatics}
}


@article{Amit1997,
    author = {Amit, D J and Brunel, N},
    title = "{Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex.}",
    journal = {Cerebral Cortex},
    volume = {7},
    number = {3},
    pages = {237-252},
    year = {1997},
    month = {04},
    abstract = "{We investigate self-sustaining stable states (attractors) in networks of integrate-and-fire neurons. First, we study the stability of spontaneous activity in an unstructured network. It is shown that the stochastic background activity, of 1-5 spikes/s, is unstable if all neurons are excitatory. On the other hand, spontaneous activity becomes self-stabilizing in presence of local inhibition, given reasonable values of the parameters of the network. Second, in a network sustaining physiological spontaneous rates, we study the effect of learning in a local module, expressed in synaptic modifications in specific populations of synapses. We find that if the average synaptic potentiation (LTP) is too low, no stimulus specific activity manifests itself in the delay period. Instead, following the presentation and removal of any stimulus there is, in the local module, a delay activity in which all neurons selective (responding visually) to any of the stimuli presented for learning have rates which gradually increase with the amplitude of synaptic potentiation. When the average LTP increases beyond a critical value, specific local attractors (stable states) appear abruptly against the background of the global uniform spontaneous attractor. In this case the local module has two available types of collective delay activity: if the stimulus is unfamiliar, the activity is spontaneous; if it is similar to a learned stimulus, delay activity is selective. These new attractors reflect the synaptic structure developed during learning. In each of them a small population of neurons have elevated rates, which depend on the strength of LTP. The remaining neurons of the module have their activity at spontaneous rates. The predictions made in this paper could be checked by single unit recordings in delayed response experiments.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/7.3.237},
    url = {https://doi.org/10.1093/cercor/7.3.237},
    eprint = {https://academic.oup.com/cercor/article-pdf/7/3/237/9752573/070237.pdf},
}

@article{Renart2004,
  title={Mean-field theory of irregularly spiking neuronal populations and working memory in recurrent cortical networks},
  author={Renart, Alfonso and Brunel, Nicolas and Wang, Xiao-Jing},
  journal={Computational neuroscience: A comprehensive approach},
  pages={431--490},
  year={2004},
  publisher={Chapman \& Hall Boca Raton}
}


@article{diSanto2018,
author = {Serena di Santo  and Pablo Villegas  and Raffaella Burioni  and Miguel A. Muñoz },
title = {Landau–Ginzburg theory of cortex dynamics: Scale-free avalanches emerge at the edge of synchronization},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {7},
pages = {E1356-E1365},
year = {2018},
doi = {10.1073/pnas.1712989115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1712989115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1712989115},
abstract = {Understanding the origin, nature, and functional significance of complex patterns of neural activity, as recorded by diverse electrophysiological and neuroimaging techniques, is a central challenge in neuroscience. Such patterns include collective oscillations emerging out of neural synchronization as well as highly heterogeneous outbursts of activity interspersed by periods of quiescence, called “neuronal avalanches.” Much debate has been generated about the possible scale invariance or criticality of such avalanches and its relevance for brain function. Aimed at shedding light onto this, here we analyze the large-scale collective properties of the cortex by using a mesoscopic approach following the principle of parsimony of Landau–Ginzburg. Our model is similar to that of Wilson–Cowan for neural dynamics but crucially, includes stochasticity and space; synaptic plasticity and inhibition are considered as possible regulatory mechanisms. Detailed analyses uncover a phase diagram including down-state, synchronous, asynchronous, and up-state phases and reveal that empirical findings for neuronal avalanches are consistently reproduced by tuning our model to the edge of synchronization. This reveals that the putative criticality of cortical dynamics does not correspond to a quiescent-to-active phase transition as usually assumed in theoretical approaches but to a synchronization phase transition, at which incipient oscillations and scale-free avalanches coexist. Furthermore, our model also accounts for up and down states as they occur (e.g., during deep sleep). This approach constitutes a framework to rationalize the possible collective phases and phase transitions of cortical networks in simple terms, thus helping to shed light on basic aspects of brain functioning from a very broad perspective.}}


@article{Capone2019,
  title = {State-dependent mean-field formalism to model different activity states in conductance-based networks of spiking neurons},
  author = {Capone, Cristiano and di Volo, Matteo and Romagnoni, Alberto and Mattia, Maurizio and Destexhe, Alain},
  journal = {Phys. Rev. E},
  volume = {100},
  issue = {6},
  pages = {062413},
  numpages = {10},
  year = {2019},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.100.062413},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.100.062413}
}

@article{Tetzlaff2012,
  title = {Time scales of memory,  learning,  and plasticity},
  volume = {106},
  ISSN = {1432-0770},
  url = {http://dx.doi.org/10.1007/s00422-012-0529-z},
  DOI = {10.1007/s00422-012-0529-z},
  number = {11–12},
  journal = {Biological Cybernetics},
  publisher = {Springer Science and Business Media LLC},
  author = {Tetzlaff,  Christian and Kolodziejski,  Christoph and Markelic,  Irene and W\"{o}rg\"{o}tter,  Florentin},
  year = {2012},
  month = nov,
  pages = {715–726}
}

@article{Zito2002,
title = {Activity-Dependent Synaptogenesis in the Adult Mammalian Cortex},
journal = {Neuron},
volume = {35},
number = {6},
pages = {1015-1017},
year = {2002},
issn = {0896-6273},
doi = {https://doi.org/10.1016/S0896-6273(02)00903-0},
url = {https://www.sciencedirect.com/science/article/pii/S0896627302009030},
author = {Karen Zito and Karel Svoboda},
abstract = {Recent electron microscopic studies provide evidence that the adult cortex generates new synapses in response to sensory activity and that these structural changes can occur rapidly, within 24 hr of sensory stimulation. Together with progress imaging synapses in vivo, the stage appears set for advances in understanding the dynamics and mechanisms of experience-dependent synaptogenesis.}
}

@article{Pakkenberg2003,
title = {Aging and the human neocortex},
journal = {Experimental Gerontology},
volume = {38},
number = {1},
pages = {95-99},
year = {2003},
note = {Proceedings of the 6th International Symposium on the Neurobiology and Neuroendocrinology of Aging},
issn = {0531-5565},
doi = {https://doi.org/10.1016/S0531-5565(02)00151-1},
author = {Bente Pakkenberg and Dorte Pelvig and Lisbeth Marner and Mads J. Bundgaard and Hans Jørgen G. Gundersen and Jens R. Nyengaard and Lisbeth Regeur},
keywords = {Fibre length, Glial cells, Neocortical volume, Stereology, Synapses, Total numbers},
abstract = {Neurostereology has been applied to quantitative anatomical study of the human brain. Such studies have included the total neocortical number of neurons and glial cells, the estimated size distribution of neocortical neurons, the total myelinated fiber length in the brain white matter, the total number of synapses in the neocortex, and the effect of normal aging on these structural elements. The difference in total number of neurons was found to be less than 10% over the age range from 20 to 90 years, while the glial cell number in six elderly individuals, mean age 89.2 years, showed an average number of 36 billion glial cells, which was not statistically significantly different from the 39 billion glial cells in the neocortex of six young individuals with a mean age of 26.2 years. The total myelinated fiber length varied from 150,000 to 180,000km in young individuals and showed a large reduction as a function of age. The total number of synapses in the human neocortex is approximately 0.15×1015 (0.15 quadrillion). Although the effect of aging is seen in all estimated structural elements, the effect of sex is actually higher. The functional relevance of these differences in neuron numbers in both age and gender is not known.}
}

@article{Muller2002,
  title = {{LTP},  Memory and Structural Plasticity},
  volume = {2},
  ISSN = {1566-5240},
  DOI = {10.2174/1566524023362041},
  number = {7},
  journal = {Current Molecular Medicine},
  publisher = {Bentham Science Publishers Ltd.},
  author = {Muller,  Dominique and Nikonenko,  Irina and Jourdain,  Pascal and Alberi,  Stefano},
  year = {2002},
  month = nov,
  pages = {605–611}
}


@article{Yang2008,
  title = {Spine Expansion and Stabilization Associated with Long-Term Potentiation},
  volume = {28},
  ISSN = {1529-2401},
  url = {http://dx.doi.org/10.1523/JNEUROSCI.3998-07.2008},
  DOI = {10.1523/jneurosci.3998-07.2008},
  number = {22},
  journal = {The Journal of Neuroscience},
  publisher = {Society for Neuroscience},
  author = {Yang,  Yunlei and Wang,  Xiao-bin and Frerking,  Matthew and Zhou,  Qiang},
  year = {2008},
  month = may,
  pages = {5740–5751}
}


@article{Schultz:2007,
AUTHOR = {Schultz, S. R},
TITLE   = {{S}ignal-to-noise ratio in neuroscience},
YEAR    = {2007},
JOURNAL = {Scholarpedia},
VOLUME  = {2},
NUMBER  = {6},
PAGES   = {2046},
DOI     = {10.4249/scholarpedia.2046},
NOTE    = {revision \#137197}
}


@article{Goldin2001,
  title = {Functional Plasticity Triggers Formation and Pruning of Dendritic Spines in Cultured Hippocampal Networks},
  volume = {21},
  ISSN = {1529-2401},
  url = {http://dx.doi.org/10.1523/JNEUROSCI.21-01-00186.2001},
  DOI = {10.1523/jneurosci.21-01-00186.2001},
  number = {1},
  journal = {The Journal of Neuroscience},
  publisher = {Society for Neuroscience},
  author = {Goldin,  Miri and Segal,  Menahem and Avignone,  Elena},
  year = {2001},
  month = jan,
  pages = {186–193}
}


