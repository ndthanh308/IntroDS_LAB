% ****** Start of file aipsamp.tex ******
%
%   This file is part of the AIP files in the AIP distribution for REVTeX 4.
%   Version 4.2a of REVTeX, December 2014
%
%   Copyright (c) 2014 American Institute of Physics.
%
%   See the AIP README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.2
%
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex  aipsamp
%  2)  bibtex aipsamp
%  3)  latex  aipsamp
%  4)  latex  aipsamp
%
% Use this file as a source of example code for your aip document.
% Use the file aiptemplate.tex as a template for your document.
\documentclass[%
 aip,
 jmp,%
 amsmath,amssymb,
%preprint,%
 reprint,%
author-year,%
%author-numerical,%
]{revtex4-2}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
\usepackage[]{float}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools}




%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines
\usepackage[table]{xcolor}

\def\marg{1ex}

\renewcommand\familydefault{cmss}
\newcommand{\popI}{\mathcal{P}_1}
\newcommand{\popII}{\mathcal{P}_2}
\newcommand{\NI}{\mathcal{N}_1}
\newcommand{\NII}{\mathcal{N}_2}
\newcommand{\PI}{\text{P}_1}
\newcommand{\PII}{\text{P}_2}
\newcommand{\T}{\mathcal{T}}
\newcommand{\rh}{\nu_\text{h}}
\newcommand{\rl}{\nu_{\ell}}
\newcommand{\rt}{\nu_\text{t}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\Wb}{\mathcal{W}_\text{b}}
\newcommand{\Wc}{\mathcal{W}_\text{c}}
\newcommand{\SII}{\mathcal{S}_\text{2}}
\newcommand{\Sb}{\mathcal{S}_\text{b}}
\newcommand{\varSb}{\sigma^{2}_\text{b}}


\newcommand{\ms}{\,\text{ms}}
\newcommand{\MOhm}{\,\text{M}\Omega}
\newcommand{\mV}{\,\text{mV}}
\newcommand{\nS}{\,\text{nS}}
\newcommand{\pA}{\,\text{pA}}
\newcommand{\pF}{\,\text{pF}}
\newcommand{\sps}{\,\text{spikes/s}}


\begin{document}




\preprint{AIP/123-QED}

\title{A modular theoretical framework for learning through structural plasticity}
%\thanks{Footnote to title of article.}

\author{Gianmarco Tiddia}
\affiliation{Department of Physics, University of Cagliari, Italy}
\affiliation{Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Cagliari, Cagliari, Italy}
 %\altaffiliation[Also at ]{Physics Department, XYZ University.}%Lines break automatically or can be forced with \\
\author{Luca Sergi}%
\affiliation{Department of Physics, University of Cagliari, Italy}

\author{Bruno Golosio}
\email{golosio@unica.it}
\affiliation{Department of Physics, University of Cagliari, Italy}
\affiliation{Istituto Nazionale di Fisica Nucleare (INFN), Sezione di Cagliari, Cagliari, Italy}


\date{\today}

\begin{abstract}
It is known that, during learning, modifications in synaptic transmission and, eventually, structural changes of the connectivity take place in our brain. This can be achieved through a mechanism known as structural plasticity. In this work, starting from a simple phenomenological model, we exploit a mean-field approach to develop a modular theoretical framework of learning through this kind of plasticity, capable of taking into account several features of the connectivity and pattern of activity of biological neural networks, including probability distributions of neuron firing rates, selectivity of the responses of single neurons to multiple stimuli, probabilistic connection rules and noisy stimuli. More importantly, it describes the effects of consolidation, pruning and reorganization of synaptic connections. This framework will be used to compute the values of some relevant quantities used to characterize the learning and memory capabilities of the neuronal network in a training and validation procedure as the number of training patterns and other model parameters vary. The results will then be compared with those obtained through simulations with firing-rate-based neuronal network models.
\end{abstract}

\keywords{computational neuroscience, structural plasticity, memory, firing rate models}%Use showkeys class option if keyword
                              %display desired
\maketitle

\section{\label{sec:introduction} Introduction}
Together with temporary and reversible changes of synaptic efficacy such as short and long-term plasticity mechanisms, structural changes in the synaptic morphology of the network, are fundamental mechanisms that take place in healthy brains. These changes occurs at longer time scales than the short or long-term mechanisms mentioned above, and consist in the consolidation, creation of new synapses next to the consolidated ones or erasure of synapses that have not been consolidated. This type of synaptic plasticity is called \textit{structural plasticity} and has a key role in the stabilization of new concepts that has to be kept in memory after learning \citep{Fu2011}. Additionally, structural plasticity can be spontaneous, but also experience-based \citep{Butz2009}.\\
Indeed, it is known that neurotransmitters can be neurotrophic factors, i.e. participate in growth or suppression of dendritic spines, synapses, axon and dendrites outgrowth \citep{Mattson1988, Lamprecht2004, Richards2005}. Thus, structural plasticity mechanism is deeply connected with neural activity. This activity-driven mechanism, which can increase or decrease the number of synapses, is then flanked by an homeostatic mechanism of homeostatic structural plasticity, which has a balancing effect achieved with adding or removing synapses, as described in \citep{Fauth2016}.\\
Moreover, the number of synapses in the brain can change over time. In \cite{Huttenlocher1979} it is shown that synaptic density reaches the highest values at $1$-$2$ years age, it drops during adolescence and stabilizes between age $16$-$72$, followed by a slight decline. However, although synaptic density remains approximately stable during adulthood, rewiring of network connections occurs as well in order to efficiently store new memories. The increase of connections in high activity regions (and vice versa), together with the rearrangement of synapses leads to a fine-tuning of the brain's circuits \citep{sakai2020}, because on one hand some synapses can be strengthened through long-term potentiation (LTP) and new connections can be formed next to the already potentiated ones to further enhance synaptic transmission, on the other hand synapses that are weakened through long-term depression (LTD) can be removed. The latter process is called synaptic pruning and it has been proven to be fundamental for optimizing activity propagation and memory capacity\citep{Chklovskii2004, Knoblauch2014, Knoblauch2016}.\\
Furthermore, it is commonly believed that synaptic pruning and rewiring dysfunction are one of the neural correlate of developmental disorders such as autism or schizophrenia \citep{Bourgeron2009,Moyer2015}, leading to, respectively, an higher or lower synaptic density with respect to neurotypical subjects\citep{Hutsler2010,Pagani2021,Glantz2000}.\\
Regarding computational modelling, in the last decades computational neuroscience has mostly focused on models of plasticity that involve strengthening or weakening of existing synapses, like spike timing-dependent plasticity (STDP) \citep{Gutig2003} or short-term plasticity (STP) \citep{tsodyks1998} and their role in long-term, short-term and working memory \citep{ThaCo, Mongillo2008, Tiddia2022_WM}. Only in recent times computational models of structural plasticity and connectivity rearrangements during learning were developed, showing intriguing results. In \cite{Knoblauch2014} and \cite{Knoblauch2016} is described a model of structural plasticity based on "effectual connectivity", defined in these works as the fraction of synapses able to represent a memory stored in a network. By structural plasticity, effectual connectivity is improved, since synapses that do not code for the memory are moved in order to optimize network's connectivity. Their model defines synapses using a Markov model of three states: potential (i.e. not instantiated), instantiated but silent or instantiated and consolidated. Structural plasticity is thus related to the passage of the synapses from a potential state to an instantiated state (and vice versa), whereas weight plasticity is relate to the consolidation of the instantiated synapses. With such a model, it is possible to show that networks with structural plasticity have higher or comparable memory capacity to networks with dense connectivity. Moreover, such a model explains some cognitive mechanism such as the spacing effect.\\
In \cite{Spiess2016} is simulated a spiking neural network with structural plasticity and STDP, showing that structural plasticity reduces the amount of noise of the network after a learning process, thus makes the network able to have a clearer output. Furthermore, such a network with structural plasticity shows higher learning speed than the same network with only STDP implemented.\\
Some new insights about the importance of synaptic pruning are also shown in \cite{Navlakha2015}, in which different pruning rates were studied suggesting that a decreasing rate of pruning leads to more efficient network architectures.\\
In this work we present a phenomenological model of structural plasticity, integrated into a modular theoretical framework of learning driven from this kind of plasticity. We develop a firing-rate-based network model, validated through simulations, in order to evaluate the learning capability of such a network modelling both the activity-driven and the homeostatic structural plasticity.\\
The model considers two neuron populations connected to each other, with a population targeted by an input signal representing a pattern to be learned by the network. We refer to the population that receives the input as $\popI$, which projects its spiking activity to population $\popII$, also targeted by a contextual signal. Indeed, this framework does not refer to a specific region of the brain, it is based on properties that are ubiquitous for several brain areas. The neuron populations considered here can eventually be in the same or also in adjacent brain regions.\\
The model assumes that synaptic consolidation is a probabilistic process driven by pre- and postsynaptic spiking activity. For each pattern given in input to the model, only a small fraction of neurons contribute to connection consolidation. Connection consolidation is then followed by a mechanism of pruning of non-consolidated connections and rewiring to grant network balance.\\
Several connection rules can be applied to the neurons of the model, and noise can be added to have more realistic pattern acquisition. To improve the biological realism of the network we simulate neuron populations having a log-normal firing rate distribution \citep{Roxin2011}. The model is trained on a variable number of patterns to evaluate the capacity of the model to learn new patterns as a function of the mode parameters, such as the density of connections or the dimension of the neuron population. Thus, we propose a modular theoretical framework able to reproduce the behaviour of a simple network during a learning process driven by structural plasticity. The model is validated with simulations, providing a general model to study the impact of structural plasticity in learning on a large-scale network model.

\section{\label{sec:methods} Materials and Methods}
Here we introduce the theoretical framework for the structural plasticity model and the firing-rate-based network model used to validate it and estimate it's learning capability. Firstly, it is presented the general model of two neurons population, and secondly the theoretical framework is divided into two possible approaches: a simple, discrete rate model in which neurons can only have two possible firing rates and a more realistic continuous rate model in which neurons firing rate follow a log-normal distribution. Finally, we introduce the computational simulation framework.

\subsection{\label{subseq:rate_model} Two populations model}
The rate model presents two neuron populations of $10^5$ neurons each. The first population ($\popI$) receives a sensory input (e.g., visual) mimicked by the activation of a fraction of the neurons of the population. The second population of neurons ($\popII$) is connected with the first one and receives input from it together with another sensory input (e.g., auditory), that we identify as a \textit{contextual signal}. The two signals incoming on this population lead to a network training through a mechanism of synaptic consolidation, and thus make the network able to classify an input injected into the first population. The next figure depicts a simple scheme of the network.

% Figure environment removed

The network training is done by injecting these two input signals to the network populations, leading to a change in the firing rate for a subset of neurons of the two populations. Structural plasticity is modelled following the categories proposed by \cite{Fauth2016}, i.e., activity-dependent and homeostatic. The pattern of firing rates among the two populations have a role in the activity-dependent structural plasticity, so that neurons having an high firing rate connected with each other show connection consolidation, which takes into account the structural change that occur at the biological level which make a consolidated connection durable. In this model of structural plasticity, a consolidated connection can not return to the baseline synaptic strength. The homeostatic effect of structural plasticity is implemented, during training, through connection recombination for the non-consolidated connections, which represents the mechanisms of pruning and creation of new connections with the aim of keeping network balance. After training the network with a variable number of $\T$ patterns, one of the patterns will be injected again into the first population, with the aim of evaluating the signal incoming into the second population of neurons. We call the neurons of $\popII$ that code the input injected \textit{representative}, and we call the average input signal to these neurons $\langle \SII \rangle$. The other neurons of $\popII$ will receive a more general input, that we can identify as the background. We call these neurons \textit{background neurons} and we call the average input signal they receive $\langle\Sb\rangle$. We eventually estimate the contrast-to-noise ratio (CNR) between representative and background neurons to estimate the capacity of such a model of properly distinguish the coding signal and the non-coding signal.

We implement such a network model using two approaches based on the value of rate that neurons can show:
\begin{itemize}
    \item \textbf{discrete approach}; i.e., neurons can assume a high value of rate when targeted by an external input ($\rh$) or a low value of rate otherwise ($\rl$);
    \item \textbf{continuous approach}; i.e., neurons have a rate distribution $\rho$, which can have a low or high average rate.
\end{itemize}

This work will present both of the approaches and the following tables will describe the model in detail.
\begin{table}[H]
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{\textbf{Summary}}\\
  \hline 
  \textbf{Populations} & $\popI$, $\popII$ \\
  \hline 
  \textbf{Connectivity} & sparse random connectivity\\
  \hline 
  \textbf{Neurons} & firing-rate-based models of point-like neurons
  \\
  \hline 
  \textbf{Synapses} & structural plasticity \\
  \hline 
  \textbf{Input} & random choice of neurons that pass from the low rate state to the high rate state \\
  \hline 
\end{tabular}
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.4\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.4\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{\textbf{Populations}}\\
  \hline 
  \textbf{Name} & \textbf{Elements} & \textbf{Size}\\
  \hline 
  $\popI$  & point-like neurons & $\NI$\\
  \hline 
  $\popII$  & point-like neurons & $\NII$\\
  \hline 
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{\textbf{Neuron }}\\
  \hline 
  \textbf{Type} & firing-rate-based neuron model\\
  \hline 
  \textbf{Description} & 
    \begin{itemize}
        \item discrete approach: neurons assume a firing rate of $\rh$ when targeted by an external input, otherwise it assumes a rate $\rl$
        \item continuous approach: neurons assume a firing rate driven by a log-normal distribution. The distribution is associated with a threshold $\rt$ that identifies the low rate neurons ($\rho (r)<\rt$) and the high rate neurons ($\rho (r)>\rt$)
    \end{itemize}\\
  \hline 
\end{tabular}
\caption{Description of the network model (continued on next page).}
\label{tab:model_description}
\end{table}

\addtocounter{table}{-1}
\begin{table}[H]
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%% 
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Connectivity}
  }\\
  \hline 
  \textbf{Source} & \textbf{Target} & \textbf{Pattern}\\
  \hline
  $\popI$ & $\popII$ & %
                      \begin{itemize}
                      \item random, independent; in-degrees can be homogeneous or driven by a Poisson distribution
                      \item synaptic weights are $\Wb$ for non-consolidated connections and $\Wc$ for consolidated ones, with $\Wc>\Wb$
                      \item multiple connections (``multapses'') are allowed by default, but they can be disabled
                      \end{itemize}\\
  % \hline
  % all & all & no self-connections (``autapses''), multiple connections (``multapses'') are permitted\\
  \hline

\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 

  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Synapse}
  }\\
  \hline 
  \textbf{Type} & structural plasticity\\
  \hline 
  \textbf{Description} & initial synaptic weight is set to $\Wb$ for all the instantiated connections.
  When a training pattern occurs, considering a connection between a neuron $i$ from $\popI$ and a neuron $j$ from $\popII$:
                         \begin{itemize}
                            \item (discrete approach) $\Wb \rightarrow \Wc$ if $\nu_\text{i}=\nu_\text{j}=\rh$
                            \item (continuous approach) $\Wb \rightarrow \Wc$ if $\nu_\text{i}>\rt \land \nu_\text{j}>\rt$
                         \end{itemize}
                        if $\mathcal{W}_{\text{i,j}}=\Wc$ the connection can not be further consolidated, neither can return to $\Wb$. A flag can also activate synaptic pruning and rewiring, so that after a certain number $t$ of training patterns, non-consolidated in-degrees of each neuron of $\popII$ are moved to different presynaptic neurons chosen using a random uniform distribution $\mathcal{U}$: $\mathcal{W}_{i,j}\rightarrow\mathcal{W}_{i',j}$, with $i'=\mathcal{U}(\NI) \in [0, \NI-1]$
  \\
  \hline 
\end{tabular}
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Stimulus}
  }\\
\hline 
\textbf{Type} & random choice of rate for each neuron according to the rate approach adopted\\
\hline 
  \textbf{Description} & 
  \begin{itemize} 
      \item (discrete approach) when an input targets $\popI$ (or $\popII$), a fraction of $\PI$ (or $\PII$) neurons of the population moves from a low to a high value of firing rate. Thus, we have a probability $p_1=\PI/\NI$ (or $p_2=\PII/\NII$) for a neuron of the population $\popI$ (or $\popII$) to be high rate
      \item (continuous approach) the rate distribution is set to that, on average, $\PI$ neurons of $\popI$ and $\PII$ neurons of $\popII$ are high rate
  \end{itemize}\\
\hline
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\begin{tabular}{
  |@{\hspace*{\marg}}p{0.2\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}p{0.8\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{2}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Initial conditions}
  }\\
\hline 
\textbf{Type} & homogeneous initial firing rates and synaptic weights\\
\hline 
  \textbf{Description} &
  \begin{itemize}
    \item all synaptic weights are set to $\Wb$.
  \end{itemize}\\
  \hline 
\end{tabular}
%%%%%%%%%%%%%%%%%%%
\caption{Description of the network model (continued).}
\label{tab:model_description}
\end{table}

\def\widthA{0.1}
\def\widthB{0.25}
\def\widthC{0.65}
\begin{table}[H]
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline 
    \multicolumn{3}{|>{\columncolor{white}}c|}{\textbf{Network and connectivity}}\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    \hline 
    $\NI$ & $100000$ & number of neurons of $\popI$\\
    \hline 
    $\NII$ & $100000$ & number of neurons of $\popII$\\
    \hline 
    $\C$ & $5000$ & number of connection in-degrees per neuron of $\popII$\\
    \hline 
    $\T$ & variable & number of training patterns\\
    \hline 
    \end{tabular}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline
    \multicolumn{3}{|>{\columncolor{white}}c|}{
    \textbf{Neuron}
    }\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    \hline 
    $\rl$ & $2.0\sps$ & low firing rate\\
    \hline 
    $\rh$ & $50\sps$ & high firing rate\\
    \hline 
    \end{tabular}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline
    \multicolumn{3}{|>{\columncolor{white}}c|}{
    \textbf{Synapse}
    }\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    %%%%%%%%%%%%%%%%%%%%
    %% transmission
    \hline
    $\Wb$ & $0.1\pA$ & baseline synaptic weight\\
    \hline
    $\Wc$ & $10\pA$ & consolidated synaptic weight \\
    \hline
    \end{tabular}
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  \begin{tabular}{
    |@{\hspace*{\marg}}p{\widthA\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthB\textwidth-2.\marg}@{\hspace*{\marg}}
    |@{\hspace*{\marg}}p{\widthC\textwidth-2.\marg}@{\hspace*{\marg}}
    |}
    \hline
    \multicolumn{3}{|>{\columncolor{white}}c|}{
    \textbf{Stimulus}
    }\\
    \hline 
    \textbf{Name} & \textbf{Value } & \textbf{Description}\\
    %%%%%%%%%%%%%%%%%%%%
    %% transmission
    \hline
    $p_1$ & $0.001$ & probability for a neuron of $\popI$ of having high rate when an input is injected\\
    \hline
    $p_2$ & $0.001$ & probability for a neuron of $\popII$ of having high rate when an input is injected \\
    \hline
  \end{tabular}
  \caption{Model parameters.}
  \label{tab:model_parameters}
\end{table}

\subsubsection{\label{subsubseq:rate-discr} Discrete rate model}
%Discussione dettagliata del modello di rate con rate discreti
Here the rate model with discrete values of rate is presented. We refer to Tables \ref{tab:model_description} and \ref{tab:model_parameters} for the description of connectivity and network parameters. In this section we derive the estimation of the CNR, and thus the estimation of the signal in input from representative and background neurons of $\popII$. We name these signals $\SII$ and $\Sb$ respectively. Moreover, we derive the variance of the background signal $\varSb$.

First of all, we estimate the average number of connection in-degrees from $\popII$ that are consolidated after $\T$ training patterns. As mentioned in Table \ref{tab:model_description}, a connection will be consolidated if both the presynaptic and the postsynaptic neurons show a high firing rate $\rh$. The probability that a generic connection between $\popI$ and $\popII$ is consolidated after a training pattern is $p_1p_2$, and thus the probability for the connection to not be consolidated after $\T$ training patterns is $(1-p_1p_2)^{\T}$. Thus, the probability that, after $\T$ training patterns, the connection is consolidated for at least a training pattern is the complementary of the quantity written above. The product of this quantity by the total number of in-degrees per neuron gives us an estimation of the average amount of consolidated connections per neuron, since once that a connection is consolidated it can not return to a baseline synaptic weight:
\begin{equation}
\label{eq:av_k}
     \langle k \rangle=\C \Bigl[ 1 - (1-p_1p_2)^{\T} \Bigr] = \C p
\end{equation}
Thus, for each neuron we have $k$ consolidated connections and $\C - k$ non-consolidated connections with synaptic weights $\Wc$ and $\Wb$ respectively.
The average rate for $\popI$ can be defined as
\begin{equation}
\label{eq:av_r}
    \langle \nu \rangle = p_1\rh + (1-p_1)\rl
\end{equation}
ergo the average input signal targeting a generic neuron of $\popII$ (i.e., a background neuron) is
\begin{equation}
\label{eq:av_sb}
    \langle \Sb \rangle = \langle k \rangle \Wc \langle \nu \rangle + (\C - \langle k \rangle) \Wb \langle \nu \rangle
\end{equation}
Now we can estimate the variance of the background signal, which is defined as
\begin{equation}
\label{eq:var_sb}
        \varSb = \langle(\Sb - \langle \Sb \rangle)^2\rangle = \Bigl\langle \Bigl[ (\Wc \sum_{i=1}^{k} \nu_i + \Wb \sum_{i=1}^{\C-k} \nu_i) - \langle \Sb \rangle \Bigr]^2\Bigr\rangle
\end{equation}
Using Equation \ref{eq:av_sb} in Equation \ref{eq:var_sb} together with the substitution $\langle k \rangle=k+(\langle k \rangle - k)$, and considering that $k\langle \nu \rangle = \sum_{i=1}^{k} \langle \nu \rangle$ we can rewrite Equation \ref{eq:var_sb} as follows:
\begin{equation}
\label{eq:var_sb2}
    \begin{split}
        \varSb &= \Bigl\langle\Bigl[ \Wc\sum_{i=1}^{k}(\nu_i - \langle \nu \rangle) + \Wb \sum_{i=1}^{\C-k} (\nu_i - \langle \nu \rangle) + (\Wc - \Wb)(k-\langle k \rangle) \langle \nu \rangle \Bigr]^2\Bigr\rangle=\\
        &= \Wc^2\sum_{i=1}^{k}(\nu_i - \langle \nu \rangle)^2 + \Wb^2\sum_{i=1}^{\C-k}(\nu_i - \langle \nu \rangle)^2 + (\Wc - \Wb)^2 (k - \langle k \rangle)^2 \langle \nu \rangle ^2 = \\
        &= \Bigl[\Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2
    \end{split}
\end{equation}
where we have considered that $\sum_{i=1}^{N}(n_i - \langle n \rangle)=0$ and $\sum_{i=1}^{N}(n_i - \langle n \rangle)^2=\sigma_{n}^2$. The value of the variance of $k$ is not shown here, but is derived in Appendix \ref{app:var_k}, whereas the variance of the rate is, by definition, $\sigma^2_{\nu}=\langle\nu^2\rangle - \langle\nu\rangle^2$.

Now we estimate the average input from a representative neuron of $\popII$. The neuron receives signals from neurons of $\popI$ coming from both consolidated and non-consolidated connections; furthermore, not all the consolidated connections connect the neuron in exam with neurons of $\popI$ at high level of rate. This is due to the fact that the network has been trained for $\T$ different patterns, and a neuron of $\popII$ can represent more than one pattern. Thus, the neuron has in input at least an average amount of $p_1\C$ consolidated connections from neurons at high rate related to the pattern we are giving in input, and the rest of the $\C'=\C-p_1\C = \C(1-p_1)$ connections comes from neurons of $\popI$ at low rate, and some of them can be consolidated for other patterns. Hence, in this case the average number of consolidated connections is not simply $\langle k \rangle$, but we have an additional amount of $\langle k' \rangle$ consolidated connections among the $\C'$ connections which can be evaluated as in Equation \ref{eq:av_k}:
\begin{equation}
    \langle k' \rangle = \C' p = \C(1-p_1) p = \langle k \rangle (1 - p_1)
\end{equation}
Ergo, summing up, the estimation of $\SII$ is
\begin{equation}
\label{eq:s2}
\begin{split}
    \langle \SII \rangle &= \Wc p_1 \C \rh + \Wc \langle k' \rangle \rl + \Wb (\C' - \langle k' \rangle) \rl =\\
    &= \Wc p_1 \C \rh + \Wc \langle k \rangle (1-p_1) \rl + \Wb (\C - \langle k \rangle) (1-p_1) \rl =\\
    &= \Wc \C p_1 \rh + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb  \Bigr] (1-p_1)\rl
\end{split}
\end{equation}

Moreover, our model implements also synaptic pruning and rewiring, so that after a certain number of training patterns submitted to the network, the non-consolidated connections are reorganized, i.e. the index of the presynaptic neuron is changed by a random selection. This way, from a theoretical point of view, the value of rate associated to the in-degrees of each neuron of $\popII$ is $\nu$ and not $\rl$ because of the randomness of rewiring. In this case, Equation \ref{eq:s2} would become

\begin{equation}
\label{eq:s2_rewiring}
    \langle \SII \rangle = \Wc \C p_1 \rh + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb  \Bigr] (1-p_1)\nu
\end{equation}
From now on, we will identify this case as "with recombination" to distinguish it from the case in which non-consolidated connections are not pruned and rewired. Indeed, this distinction is useful to estimate the contribution of this mechanism on the input signal on coding neurons of $\popII$.

Now, it is possible to obtain the estimation of the CNR using the formula
\begin{equation}
\label{eq:cnr}
    \text{CNR} (p_1, p_2, \C, \rh, \rl, \T, \Wc, \Wb) = \dfrac{|\langle \SII \rangle - \langle \Sb \rangle |}{\sigma_b}
\end{equation}

In Table \ref{tab:discrete_model} are summarized the equations of the discrete rate model.

\begin{table}[H]
    \centering
    \renewcommand\arraystretch{1.5}
    \begin{tabular}{
  |@{\hspace*{\marg}}m{0.3\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.6\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Discrete rate model}
  }\\
  \hline 
  \textbf{Name} & \textbf{Symbol} & \textbf{Equation}\\
  \hline
  Rate mean & $\langle \nu \rangle$ & $p_1\rh + (1-p_1)\rl$\\
    \hline
  Rate variance & $\sigma_{\nu}^2$ & $\Big( p_1\rh^2 + (1-p_1)\rl^2 \Big)-\Big(p_1\rh + (1-p_1)\rl\Big)^2$\\
  \hline
  Average background signal & $\langle \Sb \rangle$ & $\langle k \rangle \Wc \langle \nu \rangle + (\C - \langle k \rangle) \Wb \langle \nu \rangle$\\
  \hline
  Variance of background signal & $\varSb$ & $\Bigl[\Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2$\\
  \hline
  Average coding-neuron signal (without recombination) & $\langle \SII \rangle$ &$\Wc \C p_1 \rh + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb  \Bigr] (1-p_1)\rl$\\
  \hline
  Average coding-neuron signal (with recombination) & $\langle \SII \rangle$ &$\Wc \C p_1 \rh + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb  \Bigr] (1-p_1)\langle \nu \rangle$\\
  \hline
  
\end{tabular}
    \caption{Summary of the equations for the discrete model.}
    \label{tab:discrete_model}
\end{table}


\subsubsection{\label{subsubseq:rate-cont} Continuous model}
\paragraph{The rate distribution}

The continuous model implements rates driven from a log-normal distribution. Indeed, it is known that rate distribution in the cortex are long-tailed and skewed with a log-normal shape \citep{Roxin2011}. Having such a distribution of rate for both the populations of the model, we distinguish neurons at "high" or "low" rate through a rate threshold $\rt$, so that during training if two neurons are connected to each other and are in the regime of high firing rate, their connection is consolidated. This approach needs thus to extract a rate distribution and chose a firing rate threshold so that the average of the part of the distribution above threshold is equal to $\rh$ and the below-threshold section of the distribution has average rate $\rl$. Figure \ref{fig:lognormal} depicts an example of firing rate distribution, with the threshold and the average values of low and high firing rate.

% Figure environment removed

The log-normal distribution is defined as
\begin{equation}
\label{eq:lognormal}
 \rho(\nu) = \frac{1}{\sqrt{2\pi} \sigma \nu} \cdot \exp\Bigl( -\frac{(\ln(\nu)-\mu)^2}{2\sigma^2}\Bigr)
\end{equation}

where $\mu$ and $\sigma$ represents the distribution parameters related to the mean and standard deviation of the normally distributed $\ln(\nu)$. These parameters can be linked to Equation \ref{eq:av_r} by defining $\langle \rl \rangle$ and $\langle \rh \rangle$ in a convenient way:

\begin{equation}
    \label{eq:av_rate_cont}
    \begin{split}
        \langle \rl \rangle &= \int_{0}^{\rt} \nu \rho (\nu) d\nu \Big/ \int_{0}^{\rt} \rho (\nu) d\nu = \frac{1}{q_1}\int_{0}^{\rt} \nu \rho (\nu) d\nu\\
        \langle \rh \rangle &= \int_{\rt}^{\infty} \nu \rho (\nu) d\nu \Big/ \int_{\rt}^{\infty} \rho (\nu) d\nu = \frac{1}{p_1} \int_{\rt}^{\infty} \nu \rho (\nu) d\nu
    \end{split}
\end{equation}
where $q_1 = 1- p_1$ and
\begin{equation}
    \langle \nu \rangle= q_1 \langle \rl \rangle + p_1 \langle \rh \rangle 
\end{equation}

The estimation of the mean and standard deviation of $\ln(\nu)$ as a function of the rate parameters and $p_1$ is discussed in Appendix \ref{app:distr_fr}.


\paragraph{Estimation of input signals}
To estimate the values of $\langle\SII\rangle$, $\langle\Sb\rangle$ and $\varSb$ we proceed similarly to the discrete model. We will call the neuron firing rate using $\nu$ and $\xi$ to distinguish the rate of the neurons connected with the $k$ consolidated connections and the ones connected through the $\C-k$ non consolidated connections. Indeed, $\nu$ and $\xi$ follow the same distribution, so at the end the notation will be simplified. However, it is useful to distinguish the rate of these neurons for a better understanding of the contributions to the input signals to neurons of $\popII$.

In particular, the probability of having $k$ consolidated connections and rates in the range $(\nu_1, \nu_1+d\nu_1), \dotsi, (\nu_k, \nu_k + d\nu_k)$ is $P(k)\rho (\nu_1)\dotsi \rho (\nu_k)d\nu_1 \dotsi d\nu_k$, where $P(k)$ is the probability of having $k$ consolidated connections. Considering also the non-consolidated connections with neurons having rate in the range $(\xi_1, \xi_1+d\xi_1), \dotsi, (\xi_{\C-k}, \xi_{\C-k} + d\xi_{\C-k})$ we have that each configuration of $\C$ connections and respective rates is represented by $P(k)\rho (\nu_1)\dotsi \rho (\nu_k)d\nu_1 \dotsi d\nu_k \rho (\xi_1) \dotsi \rho (\xi_{\C-k})d\xi_1 \dotsi d\xi_{\C-k}$. To calculate the average background signal we should average over all the possible values of $k$, and thus 

\begin{equation}
\label{eq:av_sb_cont}
\begin{split}
    \langle \Sb \rangle &= \sum_{k} P(k) \int d\nu_1 \dotsi \int d\nu_k \int d\xi_1 \dotsi \int d\xi_{\C - k} \rho (\nu_1) \dotsi \rho (\nu_k) \rho (\xi_1) \dotsi \rho (\nu_{\C - k}) \cdot \\
    &\cdot\Bigl[ \Wc (\nu_1 + \dotsi + \nu_k) + \Wb (\xi_1 + \dotsi + \xi_{\C-k})\Bigr] = \\
    &= \sum_{k} P(k) \Bigl[ \Wc k \langle \nu \rangle \Wb (\C-k) \langle \xi \rangle \Bigr] = (\Wc - \Wb) \langle k \rangle \langle \nu \rangle + \Wb \C \langle \nu \rangle
\end{split}
\end{equation}

where we used that $\int \rho(\nu) d\nu = \int \rho(\xi) d\xi = \langle \nu \rangle$. Note that the result obtained in Equation \ref{eq:av_sb_cont} is the same as the one obtained for the discrete approach (see Equation \ref{eq:av_sb}).
The variance of the background signal can be similarly derived:
\begin{equation}
\label{eq:var_sb_cont}
\begin{split}
    \varSb &=\langle(\Sb - \langle \Sb \rangle)^2\rangle = \sum_k P(k) \int d\nu_1  \dotsi \int d\nu_k \int d\xi_1 \dotsi \int d\xi_{\C-k} \rho(\nu_1) \dotsi \rho(\xi_{\C-k}) \cdot \\
    &\cdot \Bigl[ \Wc (\nu_1 + \dotsi + \nu_k) + \Wb (\xi_1 + \dotsi + \xi_{\C-k}) - \Wc \langle k \rangle \langle \nu \rangle - \Wb (\C- \langle k \rangle) \langle \nu \rangle \Bigr]^2=\\
    &= \sum_k P(k) \int d\nu_1 \dotsi \int d\xi_{\C-k} \rho(\nu_1) \dotsi \rho(\xi_{\C-k}) \Bigl[ \Wc (\nu_1 + \langle \nu \rangle + \dotsi + \nu_k + \langle \nu \rangle) + \\ 
    & + \Wb (\xi_1 + \langle \nu \rangle + \dotsi + \xi_{\C-k} + \langle \nu \rangle) + (\Wc - \Wb)(k - \langle k \rangle) \langle \nu \rangle \Bigr]^2
\end{split}
\end{equation}
Where in the last line we used the substitution $\langle k \rangle=k+(\langle k \rangle - k)$ as done for Equation \ref{eq:var_sb2}.
The non-quadratic terms of the equation above are null because $\int \rho (x) (x-\langle x \rangle)dx = \int \rho (x) x dx - \langle x \rangle \int \rho (x) dx = 0$, ergo we can write the variance of the background signal as follows:

\begin{equation}
\label{eq:var_sb2_cont}
\begin{split}
    \varSb &= \sum_k P(k) \int d\nu_1 \dotsi \int d\xi_{\C-k} \rho(\nu_1) \dotsi \rho(\xi_{\C-k}) \Bigl[ \Wc^2 k \langle (\nu - \langle \nu \rangle)^2 \rangle + \Wb^2 (\C - k) \langle (\nu - \langle \nu \rangle)^2 \rangle +\\
    & + (\Wc - \Wb)^2 (k - \langle k \rangle)^2 \langle \nu \rangle ^2 \Bigr] = \Bigl[ \Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2
\end{split}
\end{equation}

As for the discrete case, the value of the variance of $k$ is derived in Appendix \ref{app:var_k} and there is no difference in its derivation between the discrete and the continuous approach.

Regarding the estimation of $\langle \SII \rangle$ we can use the same conclusions driven in the discrete case. Indeed, we have a contribution from $p_1\C$ neurons of $\popI$ at high rate and consolidated connections, corresponding to the signal coming from the neurons of $\popI$ targeted by the external input. This means that the rest $\C'= \C - p_1\C$ neurons that have a low rate but can also be connected with consolidated connections (which are consolidated for other training patterns). Calling $P'(k')$ the probability of having $k'$ consolidated connections among the $\C'$ in-degrees and using the definition of $\langle \rl \rangle$ and $\langle \rh \rangle$ given by Equation \ref{eq:av_rate_cont} we can write
\begin{equation}
\label{eq:s2_cont}
\begin{split}
    \langle \SII \rangle &= \Wc p_1 \C \langle \rh \rangle + (\Wc - \Wb) \langle k' \rangle \langle \rl \rangle + \Wb \C(1-p_1)\langle \rl \rangle =\\
    &= \Wc p_1 \C \langle \rh \rangle + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb \Bigr] (1-p_1) \langle \rl \rangle
\end{split}
\end{equation}

As can be seen, Equation \ref{eq:s2_cont} differs from \ref{eq:s2} only because the rates are not discrete but can assume continuous values, thus $\rh$ and $\rl$ have been replaced by $\langle \rh \rangle$ and $\langle \rl \rangle$. Similar conclusions can be derived by comparing the estimation of $\langle\Sb\rangle$ and $\varSb$ as well. Indeed, we would expect that numerical simulation would differ only in the estimation of the variance of the firing rate. Moreover, as discussed for the discrete model, when we add rewiring of non-consolidated synapses Equation \ref{eq:s2_cont} becomes:

\begin{equation}
\label{eq:s2_rewiring_cont}
\begin{split}
    \langle \SII \rangle = \Wc p_1 \C \langle \rh \rangle + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb \Bigr] (1-p_1) \langle \nu \rangle
\end{split}
\end{equation}

Hitherto we considered a model in for which each neuron of $\popII$ has a constant amount of in-degrees, i.e., $\C$. However, a more general and more realistic approach would consider $\C$ as a variable across the neurons of $\popII$. Thus, we can consider the amount of in-degrees of each neuron driven by a Poisson distribution. This way, the values of $\langle \SII \rangle$ and $\langle \Sb \rangle$, previously averaged over the rate $\nu$ and the number of consilidated connections $k$, should be also averaged over the number of in-degrees, so that

\begin{equation}
    \begin{split}
        \langle \langle \Sb \rangle_{\nu,k} \rangle_{\C} &= \sum_{c} P(c) \langle \Sb \rangle_{\nu,k}\\
        \langle \langle \SII \rangle_{\nu,k} \rangle_{\C} &= \sum_{c} P(c) \langle \SII \rangle _{\nu,k}
    \end{split}
    \label{eq:averaging_over_C}
\end{equation}

where $\langle \SII \rangle_{\nu,k}$ is the same as Equation \ref{eq:s2_cont} and $\langle \Sb \rangle _{\nu,k}$ is the same as Equation \ref{eq:av_sb_cont}. Since these equations are linear in $\C$ and since $\sum_{c}P(c) = \langle\C\rangle$, Equations \ref{eq:s2_cont} and \ref{eq:av_sb_cont} would show $\langle\C\rangle$ instead of $\C$ when averaged over the number of in-degrees per neuron.\\
Regarding the standard deviation, we should determine 
\begin{equation}
    \text{Var}(\langle \langle \Sb \rangle_{\nu,k}\rangle_{\C})= \sigma^2_{\nu,k,\C} =\langle\langle\Sb^2\rangle_{\nu,k}\rangle_{\C}-\langle\langle\Sb\rangle_{\nu,k}\rangle_{\C}^2
\end{equation}
Knowing that $\langle \varSb\rangle_{\C}=\langle \langle\Sb^2\rangle_{\nu_k}-\langle\Sb\rangle_{\nu,k}^2\rangle=\langle\langle\Sb^2\rangle_{\nu,k}\rangle_{\C}-\langle\langle\Sb\rangle_{\nu,k}^2\rangle_{\C}$ and that $\langle k \rangle = p\C$ we can write
\begin{equation}
\begin{split}
    \sigma^2_{\nu,k,\C} &= \langle\varSb\rangle_{\C} + \langle\langle\Sb\rangle_{\nu,k}^2\rangle_{\C}-\langle\langle\Sb\rangle_{\nu,k}\rangle_{\C}^2=\\
    &= \langle\varSb\rangle_{\C} + \Bigl\{ \langle\nu\rangle \Bigl[ \Wb + p(\Wc-\Wb) \Bigr] \Bigr\}^2 \Bigl[ \langle\C^2\rangle_{\C} - \langle\C\rangle^2 \Bigr] =\\
    &= \langle\varSb\rangle_{\C} + \langle\nu\rangle^2 \Bigl[ \Wb + p(\Wc-\Wb) \Bigr]^2 \sigma_{\C}^2
\end{split}
\label{eq:var_sb_c_variable}
\end{equation}

This equation is also valid in case of discrete values of neuron firing rate.\\
In Table \ref{tab:continuous_model} are summarized the equations of the continuous rate model in case of fixed number of in-degrees per neuron. As mentioned before, when the value of $\C$ is driven by a Poisson distribution, we can use, for $\langle\Sb\rangle$ and $\langle\SII\rangle$, the same equations proposed with fixed in-degree but replacing $\C$ with $\langle\C\rangle$. For the variance of the background signal, we can refer to Equation \ref{eq:var_sb_c_variable}.

\begin{table}[H]
    \centering
    \renewcommand\arraystretch{1.5}
    \begin{tabular}{
  |@{\hspace*{\marg}}m{0.3\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.1\textwidth-2.\marg}@{\hspace*{\marg}}
  |@{\hspace*{\marg}}m{0.6\textwidth-2.\marg}@{\hspace*{\marg}}
  |}
  \hline 
  \multicolumn{3}{|>{\color{black}\columncolor{white}}c|}{
  \textbf{Continuous rate model}
  }\\
  \hline 
  \textbf{Name} & \textbf{Symbol} & \textbf{Equation}\\
  \hline
  Rate distribution & $\rho(\nu)$ & $\frac{1}{\sqrt{2\pi} \sigma \nu} \cdot \exp\Bigl( -\frac{(\ln(\nu)-\mu)^2}{2\sigma^2}\Bigr)$ \\
  \hline
  Mean of the normal distribution of $\ln(\nu)$ & $\mu$ & $\ln(\langle \nu \rangle) - \frac{\sigma^2}{2}$\\
  \hline
  Standard deviation of the normal distribution of $\ln(\nu)$ & $\sigma$ & $erf^{-1}(q_1) - erf^{-1}\Big( \frac{q_1 \langle \rl \rangle}{\langle \nu \rangle} \Big)$\\
  \hline
  Rate threshold & $\rt$ & $\exp \Big(erf^{-1}(q_1)\sigma + \mu\Big)$\\
  \hline
  Average high rate & $\langle \rh \rangle$ & $\frac{1}{p_1} \int_{\rt}^{\infty} \nu \rho (\nu) d\nu$\\
  \hline
  Average low rate & $\langle \rl \rangle$ & $\frac{1}{q_1}\int_{0}^{\rt} \nu \rho (\nu) d\nu$\\
  \hline
  Average rate & $\langle \nu \rangle$ & $q_1 \langle \rl \rangle + p_1 \langle \rh \rangle $\\
    \hline
  Rate standard deviation & $\sigma_{\nu}^2$ & $\Big( e^{\sigma^2} - 1 \Big)e^{2\mu + \sigma^2}$\\
  \hline
  Average background signal & $\langle \Sb \rangle$ & $(\Wc - \Wb) \langle k \rangle \langle \nu \rangle + \Wb \C \langle \nu \rangle$\\
  \hline
  Variance of background signal & $\varSb$ & $\Bigl[ \Wc^2 \langle k \rangle + \Wb^2 (\C - \langle k \rangle)\Bigr] \sigma_{\nu}^2 + (\Wc - \Wb)^2 \sigma_{k}^2 \langle \nu \rangle ^2$\\
  \hline
  Average coding-neuron signal (without recombination) & $\langle \SII \rangle$ & $\Wc p_1 \C \langle \rh \rangle + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb \Bigr] (1-p_1) \langle \rl \rangle$ \\
  \hline
  Average coding-neuron signal (with recombination) & $\langle \SII \rangle$ & $\Wc p_1 \C \langle \rh \rangle + \Bigl[ (\Wc - \Wb) \langle k \rangle + \C \Wb \Bigr] (1-p_1) \langle \nu \rangle$ \\
  \hline
  
\end{tabular}
    \caption{Summary of the equations for the continuous model. Equations shown here refers to the case in which the number of in-degrees per neuron $\C$ is constant.}
    \label{tab:continuous_model}
\end{table}

Indeed, to mimic the presentation of different instances of the same class when testing network performance, we also designed a noise addition to the neurons of population $\popI$. The derivation of the equations with noise addition is shown in Appendix \ref{app:noise}.

\subsection{Computational simulations of the model}
Together with the derivation of the values of $\langle\Sb\rangle$, $\langle\SII\rangle$ and $\varSb$ obtained using the equation described in the previous sections, we also performed some simulations of the network model to validate the results. In particular, we performed C++ simulations using GCC\footnote{\url{https://gcc.gnu.org/}} (version 10.2.0) and GSL\footnote{\url{https://www.gnu.org/software/gsl/}} (version 2.7) using the supercomputers Galileo 100 and JUSUF \cite{VonStVieth2021}. To estimate $\langle\Sb\rangle$ and $\langle\SII\rangle$ we compute the input incoming to each neuron of $\popII$ as the sum of the rate of the neurons of $\popI$ connected to the former multiplied by the value of the synaptic weight of the connection (i.e., $\Wc$ or $\Wb$). During training, in the discrete model, for each neuron is extracted a random number $n$; if $n<p_i$ ($i=1,2$) the neuron can be considered as a high rate neuron, otherwise it assumes a rate $\rl$. In the continuous case, a firing rate is extracted basing on the rate distribution and it is checked whether the rate is above or under the rate threshold $\rt$. These extraction are made for each of the $\T$ patterns.
After the training phase, some patterns are presented again to $\popI$ in sequence, with the aim of computing the signal in input to the neurons of $\popII$, ergo $\Sb$ and $\SII$. We usually obtain the estimation of $\varSb$ from the same simulations, but we identified a bias on this quantity, which has been evaluated. Indeed, during the simulation we measure the value of $\Sb$ for each of the neurons of $\popII$ that did not coded for that pattern during training and we obtain this value for every of the $\T$ patterns. Thus, the variance of that quantity is related to sets of background signals that are not independent from each other, which leads to a bias in the estimation of the variance of $\varSb$. 
%However, it is possible to estimate $\varSb$ using a separate script, in which we perform a training and we measure the value of $\Sb$ $N$ times in order to have a set of independent background signals from which we can compute the variance.


\section{\label{sec:results} Results}
This section compares the results of the simulations of the firing rate model and the theoretical estimations described in the previous section. Since we proposed several versions of the model, with different features implemented, the section is divided into different parts that sum up the main characteristics of the simulations. We present the results of the model employing discrete and continuous values for neurons firing rate, comparing the estimation of the background signal $\langle\Sb\rangle$, the one of the coding neurons $\langle\SII\rangle$, the variance of the background signal $\varSb$ and the contrast-to-noise ratio. This way, we are able to identify the capacity of the population $\popII$, and thus the network, of classifying a pattern correctly. Here, we present simulation results with a Poisson-driven number of in-degrees, with $\langle\C\rangle=5000$. We opted for such an approach since it is the most realistic one for biological neural circuits with respect to a fixed amount of connections per neuron. Each simulation is repeated $10$ times using a different seed for random number generation to ensure the robustness of the simulation results. The values shown in the plots are a result of an averaging over the different seeds. 

\subsection{Comparison between continuous and discrete firing rate}
Here is shown the comparison between the theoretical estimation and the firing rate simulations for the rate model employing discrete values for neuron rate or continuous values following a log-normal distribution. As mentioned in the previous section, the main difference for $\langle\Sb\rangle$ and $\langle\SII\rangle$ when using a continuous rate is that, instead of having a simple value for high or low rate, we have to define a threshold and average the rates. However, if the average estimation for the high and low rates are the same as the values used for the discrete model, the estimations for the aforementioned signals are identical despite the approach for defining the neurons firing rate. Indeed, the only difference lies in the estimation of the variance of the background signal, since the variance of the rate $\sigma^2_{\nu}$ changes.\\
The first study we perform is oriented to the estimation of these parameters as a function of the number of training patterns $\T$. Indeed, the larger the number of training patterns of the network is,
the larger is the number of patterns to be encoded by every single neuron on average. Since $p_1$ is the probability of being at high rate for a training pattern, on average a neuron encodes $p_1\NII\T$ patterns. This effect is also present in biological neural networks, in which the same neuron can contribute to several different tasks \citep{Rigotti2013}.\\
Moreover, since we tested the network trained for a different amount of patterns, in order to show a comparable outcome across the different simulations, we decide to perform the test phase using only a subset of $1000$ test patterns, no matter the value of $\T$ (in case of $\T>1000$, otherwise the network will be tested for all the patterns learned). Thus, the simulation outcome used for our analysis is an average over the $1000$ sets of $\Sb$, $\SII$, $\varSb$ and CNR obtained for each test pattern. 
Figure \ref{fig:discr_vs_contin} shows the comparison of the simulation outcomes using discrete and continuous rate values with their respective theoretical estimations. Additionally, for simulations having a continuous distribution of neuron rates, we also add the simulation outcomes when a noisy input is added. Noise is designed using a truncated Gaussian distribution with zero mean and standard deviation of $1$\,Hz truncated at $2\sigma$.\\
As can be seen from Figure \ref{fig:discr_vs_contin}, simulation results match the theoretical estimations, so that the lines are almost indistinguishable. Moreover, we can notice that the behavior and the values of $\langle\Sb\rangle$ and $\langle\SII\rangle$ for discrete and continuous rate simulation are very similar, but the $\varSb$ shows a significant difference in the values obtained from these simulations, as expected from the theoretical estimation. Furthermore, as indicated in Appendix \ref{app:noise}, we can notice that continuous rate simulations with noise implemented show the same estimation for the neuron signals with respect to a similar simulation with noise disabled. This happens because the noise has zero mean, however, the variance of the background signal shows some small differences, still predicted by the theoretical model.
\newpage
% Figure environment removed


\subsection{Discrepancy evaluation}
To provide a quantitative estimation of the fluctuations that can arise when comparing the simulation results and the theoretical model, we also evaluate their relative error, using the theoretical estimation as a reference. Figure \ref{fig:relative-error} shows the same values obtained for the continuous rate model in Figure \ref{fig:discr_vs_contin} together with the relative error with respect to theory. We only show the continuous model data since the discrepancies evaluated from the discrete rate model are totally similar (data not shown).

% Figure environment removed

As can be seen, the relative error is small indeed: for $\langle\Sb\rangle$ and $\langle\SII\rangle$ the errors span between $0.01$\% and $0.4$\%, whereas $\varSb$ shows a relative error that is totally negligible until the network is trained on $10^4$ patterns, and then increases up to $1$\%. These errors, for our purpose, are totally negligible. However, we identified the origin of the higher discrepancy for $\varSb$ in a bias on our simulations.\\
Indeed, the values of $\Sb$ we measure the variance from, are obtained by in-degrees from neurons of $\popI$, but since connections are created randomly, there is the possibility of having a neuron of $\popI$ targeting multiple neurons of $\popII$, leading to values of $\Sb$ that are not independent. This probability is a function of the total number of neurons of $\popI$ and the number of in-degrees per neuron of $\popII$, and calling $\NI=\NII=\mathcal{N}$, we can state that this bias becomes more and more relevant when the $\mathcal{C}/\mathcal{N}$ increases. To have an unbiased estimation of the $\varSb$ we should repeat a simulation multiple times measuring the value $\Sb$ from a single non-coding neuron of $\popII$, which can be extremely expensive from a computational point of view. We decided then to estimate this bias as a function of the $\mathcal{C}/\mathcal{N}$ ratio. We fixed the number of training patterns at $\T=1000$ and we changed the $\mathcal{C}/\mathcal{N}$ ratio using different values of $\mathcal{N}$. Figure \ref{fig:c_n_comparison} shows the results of this analysis.

% Figure environment removed

As can be seen in the right panel of Figure \ref{fig:c_n_comparison}, an higher value for $\mathcal{C}/\mathcal{N}$ leads to an higher discrepancy between theoretical estimation and simulation. However, such a ratio, for natural density circuits in human cortex, are very far from values of $\mathcal{C}/\mathcal{N}$ near unity. Indeed, a plausible value of the ratio would be less than $0.1$, resulting in negligible relative errors.\\
Moreover, having a neuron that can be active for more than a pattern is totally plausible in biological networks, so a different implementation of the connectivity would not reproduce a plausible network connectivity and, on the other hand, the design of a different protocol for the unbiased estimation of $\varSb$ would not be worthwhile because of the higher computational cost and the low entity of this bias in case of natural density networks (in our case we have $\C=5000$ and $\mathcal{N}=100000$, thus the ratio is 0.05). For this reason, we decided to calculate the variance of the background signal using the data of $\Sb$ extracted from the usual simulation, since the entity of this bias is negligible.

\subsection{Impact of synaptic recombination}
In the previous simulations, the recombination mechanism was always implemented employing a rewiring step of $100$ training patterns. This means that, every time $100$ training patterns are given in input and the consolidation of the connection has taken place, all the non-consolidated connections are randomly re-distributed across the populations of the model to mimic a mechanism of connection removal and creation. This mimics a mechanism of homeostatic structural plasticity, that aims at keeping the network balanced by reorganizing connections, while activity-dependent structural plasticity focuses on the consolidation of connections.\\
However, to motivate the choice of this step for connection recombination, we show here the results for networks trained for $5000$ patterns with a different recombination step $t$. We also show the result for a simulation that do not perform the recombination (i.e., $t=0$), in order to see the difference between the values of a network that recombines its connections or just consolidate a subset of them. Figure \ref{fig:t_study} shows the values obtained by this network using different recombination intervals. 

% Figure environment removed

As can be seen, the values obtained when connection recombination is enabled shows compatible fluctuation, ensuring that the value of step $t$ chosen for the connection recombination has no impact in the results of the simulation. Indeed, a significant difference emerges when comparing data with recombination enabled or disabled, which leads to a lower value of the contrast-to-noise ratio when the recombination is disabled. This confirms that a network enabling connection recombination grants an higher capability of recognising an input class among the several classes for which the network is trained.\\
We also applied a similar protocol of the one used in Figure \ref{fig:relative-error} for simulations enabling or disabling connection recombination as a function of the number of training patterns $\T$. The results are shown in Figure \ref{fig:rew_vs_norew}.

% Figure environment removed

We can say that the performance of the model are improved when connection recombination is enabled, and the relative difference between a recombined or just consolidated connectivity increases when increasing the number of training patterns.\\
Regarding the comparison with theoretical estimation and simulations when enabling or disabling rewiring, the current theoretical model shows values that are very near to both the versions, but the recombination step $t$ is not considered into the theoretical framework. Indeed, we suggest that the effect of recombination can become more and more relevant when a considerable number of connections can be consolidated at every step (i.e., with higher values of $p_1$ and $p_2$), so that the number of connections to be pruned and rewired is not approximately the total amount of connections instantiated. Moreover, we suggest that the importance of recombination mechanisms can significantly change when the number of connections after recombination increase or decrease in number. However, this aspect has not been explored in the current work.


\section{\label{sec:discussion} Discussion}
We present a modular theoretical framework able to evaluate the effect of structural plasticity during learning in a large-scale portion of the cortex using mean-field theory and simulations of firing-rate-based neuron models. The model we present here is a simple two population model connected as a feed-forward network. This work does not aim at a description of a specific brain region, we chose to develop a framework sufficiently general that considers common features of cortical circuits. For example, the average population firing rate when a neuron population reaches a stimulus is driven from a log-normal distributions, in agreement with \citep{Roxin2011}, even if a simpler version of the model implements a binary rate for neuron, with compatible results with respect to the latter. Moreover, connectivity between regions can be achieved through several connection rules, with fixed number of connections per neuron or, more realistically, from a random connectivity in which each neuron has a number of input driven from a Poisson distribution.\\
Structural plasticity mechanism is designed in order to represent both mechanisms driven by neuronal activity and mechanisms which leads to homeostasis, in agreement with the work of \cite{Fauth2016} which divides structural plasticity in these two categories. Indeed, the biochemical and biophysical mechanisms underlying structural plasticity are many and extremely complex. For this reason we opted for a phenomenological approach to capture the main aspects of these mechanisms. Neuronal activity-driven structural plasticity is achieved through consolidation of synapses connecting active neurons, which result in an increase of the synaptic efficacy. This can be due to mechanisms such as STDP but also to mechanisms involving citoarchitectural changes, such as the creation of novel connections next to the already existing ones.\\
Moreover, connections that are not utilized over time are likely to be pruned and, to grant network balance, novel connections are created in the network, which is related to homeostasis structural plasticity. This has been achieved in the simulations using a recombination step $t$, so that, during training, after $t$ training patterns are given to the network, recombination (i.e., pruning and rewiring) can take place. This simple phenomenological approach is able to underline some differences in the network contrast-to-noise ratio when a test pattern is given to the network, resulting in an higher capability of recognizing the input when this mechanism is enabled. We do not have experienced a performance difference when using a different recombination step. We also evaluated the capability of the model of recognising a learned input when noise is added, with results totally compatible with the theoretical estimation. Indeed, the relative error between theoretical estimation and simulations is in the order of $1-2$\%, which is a remarkable results for our purpose. Additionally, we identified a bias in the simulation protocol, but further estimation has revealed that for neural density networks the impact of such a bias can be considered negligible.\\
Indeed, the theoretical model can be surely extended. It can potentially provide a powerful tool to describe the impact of structural plasticity in cognitive processes such as learning in a large-scale model of the cortex with natural density and plausible characteristics. For instance, the consolidation mechanism can be probability-driven, with a probability depending on the rate of pre- and postsynaptic neurons. This would replace the current deterministic mechanism that requires a firing rate threshold to be exceeded from both the neurons to have synaptic consolidation. Moreover, the probability can also be related to other characteristics not necessarily related to the firing rate.\\
Furthermore, the model can be extended by adding recurrent connections to the population receiving inputs from the other one, together with an inhibitory population in order to have a more realistic architecture of excitatory and inhibitory neurons. Indeed. it is known that learning can be achieved in the cortex through a mechanism of competition through lateral inhibition \citep{Coultrip1992}. Such an extension of the model is currently under development and it will be object of a future work.\\
Another extension of the model, and in particular on the mechanism of structural plasticity, would be to mimic more in detail the mechanism of synaptic pruning and rewiring. Indeed, connection recombination as intended in the current model preserves the total number of connections over time, which is a typical behavior of a healthy adult brain \citep{Huttenlocher1979}. However, to shed light of the importance of these mechanism in neurological disorders, or to perform studies focused on this mechanism in other life stages, this mechanism should be extended to enable different "speed" for the processes that belong to the mechanism we call structural plasticity.\\
Moreover, it would be interesting to open to simulations of spiking neural networks, to study learning through structural plasticity in more detail. Indeed, simulators such as NEST \citep{NEST} and its GPU implementation \citep{Tiddia2022} can lead to fast and efficient simulations of large-scale models on supercomputer clusters.
In conclusion, such a work intend to provide a sufficiently general theoretical framework for learning through structural plasticity. The model is able to describe synaptic consolidation, pruning and rewiring, and includes several features that can be added with a modular fashion. The framework is validated through simulations of rate-based models, showing a remarkable compatibility between the results of simulations and theoretical estimations.





\section*{\label{sec:contributions} Author contributions}
% Ideas; formulation or evolution of overarching research goals and aims.
\textbf{Conceptualization:} Bruno Golosio

% Management activities to annotate (produce metadata), scrub data and maintain research data (including software code, where it is necessary for interpreting the data itself) for initial use and later reuse.
\textbf{Data curation:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Application of statistical, mathematical, computational, or other formal techniques to analyze or synthesize study data.
\textbf{Formal analysis:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Acquisition of the financial support for the project leading to this publication.
\textbf{Funding acquisition:} Bruno Golosio

% Conducting a research and investigation process, specifically performing the experiments, or data/evidence collection.
\textbf{Investigation:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Development or design of methodology; creation of models
\textbf{Methodology:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Management and coordination responsibility for the research activity planning and execution.
\textbf{Project administration:} Bruno Golosio

% Provision of study materials, reagents, materials, patients, laboratory samples, animals, instrumentation, computing resources, or other analysis tools.
\textbf{Resources:} Bruno Golosio and Gianmarco Tiddia

%  	Programming, software development; designing computer programs; implementation of the computer code and supporting algorithms; testing of existing code components.
\textbf{Software:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

%  	Oversight and leadership responsibility for the research activity planning and execution, including mentorship external to the core team.
\textbf{Supervision:} Bruno Golosio

%  	Verification, whether as a part of the activity or separate, of the overall replication/reproducibility of results/experiments and other research outputs.
\textbf{Validation:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

%  	Preparation, creation and/or presentation of the published work, specifically visualization/data presentation.
\textbf{Visualization:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Creation and/or presentation of the published work, specifically writing the initial draft (including substantive translation).
\textbf{Writing - original draft:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi

% Preparation, creation and/or presentation of the published work by those from the original research group, specifically critical review, commentary or revision  including pre- or post-publication stages.
\textbf{Writing - review and editing:} Bruno Golosio, Gianmarco Tiddia and Luca Sergi


%\section*{\label{sec:funding} Funding}

\section*{\label{sec:data_av_statement} Data availability statement}
All the simulation code needed to reproduce the results reported in this work, together with the related documentation, are freely available in the GitHub repository \url{https://github.com/gmtiddia/structural_plasticity}.

\section*{\label{sec:acknowledgements} Acknowledgements}
We acknowledge the use of Fenix Infrastructure resources, which are partially funded from the European Union's Horizon 2020 research and innovation programme through the ICEI project under the grant agreement No. 800858.




\nocite{*}
\section{Bibliography}
\bibliography{bibliography}% Produces the bibliography via BibTeX.




\appendix

\section{Firing rate distribution for the rate model}
\label{app:distr_fr}
Here the estimation for the mean and standard deviation of the function $y=\ln(\nu)$ is discussed.

Expanding Equation \ref{eq:av_rate_cont} using Equation \ref{eq:lognormal} we have

\begin{equation}
\begin{split}
    \langle \rl \rangle &=\frac{1}{q_1}\int_{-\infty}^{y_t} \nu(y)G_{\sigma, \mu}(y) dy=\frac{1}{q_1}\int_{-\infty}^{y_t} e^y \frac{1}{\sqrt{2\pi \sigma^2}}e^{\frac{-(y-\mu)^2}{2\sigma^2} } dy\\
    \langle \rh \rangle &= \frac{1}{p_1}\int_{y_{t}}^\infty \nu(y) G_{\sigma, \mu}(y) dy= \frac{1}{p_1}\int_{y_{t}}^\infty e^y \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-(y-\mu)^2}{2\sigma^2} } dy 
\end{split}
\label{eq:av_rate_cont2}
\end{equation}

where $y=\ln(\nu)$ and follows a normal distribution $G_{\sigma, \mu}(y)$. The firing rate threshold is represented by $y_t=\ln(\rt)$. Knowing that, by definition, $\int_{y_t}^{\infty} \rho(y)dy=p_1$, we have that
\begin{equation}
    y_t = erf^{-1}(q_1)\sigma + \mu
\end{equation}
with $erf(\cdot)$ error function defined as:
\begin{equation}
    erf(x)= \frac{1}{\sqrt{2\pi}} \int_x^{\infty} e^{-\frac{x^2}{2}} dx
\end{equation}
Starting from Equation \ref{eq:av_rate_cont2} and doing the substitutions $y-\mu = z$ and $(z-\sigma^2)/\sigma = w$ we can derive the following relation
\begin{equation}
    \langle \rh \rangle = \frac{\langle \nu \rangle}{p_1}\Bigl( 1 - erf(erf^{-1}(q_1) - \sigma) \Bigr) \Rightarrow \sigma = erf^{-1}(q_1) - erf^{-1}\Big( \frac{q_1 \langle \rl \rangle}{\langle \nu \rangle} \Big)
\end{equation}
Moreover, since by definition of the log-normal distribution $\rho (\nu)$, $\langle \nu \rangle = e^{\mu + \sigma^2 /2}$ we can easily derive that
\begin{equation}
    \mu = \ln(\langle \nu \rangle) - \frac{\sigma^2}{2}
\end{equation}
whereas the variance of the distribution is
\begin{equation}
    \sigma_{\nu}^2 = \Big( e^{\sigma^2} - 1 \Big)e^{2\mu + \sigma^2}
\end{equation}
These parameters, and thus the shape of the distribution, depend on the choice of the choice of the average values of high and low rate and the probability $p_1$ of having a neuron of $\popI$ at high rate when an input is injected.


\section{Estimation of the variance of $k$}
\label{app:var_k}
Here, we estimate the variance on the number of consolidated connections in input to a neuron of $\popII$ (i.e., $\sigma^2_{k}$). The following scheme supports for the calculation. It represents a complete simulation for a single neuron of $\popII$ over $\T$ patterns.

\begin{table}[H]
\centering
\resizebox{8cm}{3cm}{%
\begin{tabular}{|| c | c | c | c| c | c | c | c | c ||} 
 \hline
 t & $\mathcal{O}$ & $I_0$ & $I_1$ & .... & $I_{n-1}$ & $I_{n}$ & .... & $I_{\C-1}$\\ [0.5ex] 
\hline\hline 
 0 & 1& $x_{00}$& & &  $x_{n-1,0}$& 0& & 0   \\
\hline 
 1 & 1&.. & & & ..&0 & & 0   \\ 
 \hline
 2 & 1&.. & & & ..&0 & & 0 \\
\hline
 .. & .. & ..& & & ..& .. & & ..\\
\hline
 m-1 & 1& $x_{0m}$ & & &  $x_{n-1,m-1}$&0 & & 0\\

\hline
.. &0 & & & & & & & \\
\hline
.. & ..& & & & & & & \\

 \hline
$\T$ &0 & & & & & & & \\ [1ex] 
\hline
\end{tabular}}
\vspace{2mm}
    \caption{Scheme representing the rate in input to a single neuron of $\popII$. Each row represent a pattern, with id ranging from 0 to $\T-1$. Each column $\mathcal{I}_j$ represents the rate of the neuron of $\popI$ connected to the neuron of $\popII$ through its $j$-th connection. The first column (indicated with the letter $\mathcal{O}$) stores the rate of the neuron of $\popII$ given a determined pattern. Entries can be 0 or 1 basing on the fact that the neuron shows an high or a low rate (in case of continuous rate, it represents a rate over or under the threshold $\nu_{\text{t}}$). Indexes $m$ and $h$ represent, respectively, a training pattern and one of the $\C$ input connections to the neuron.}
    \label{fig:rate_matrix}
\end{table}

Given the scheme of Figure \ref{fig:rate_matrix}, we call:
\begin{itemize}
    \item $p_1$: probability that a neuron of $\popI$ is above threshold, i.e., the probability of having at least a cell of a column $\mathcal{I}_j$ equal to one
    \item $p_2$: probability that the neuron of $\popII$ is above threshold for a given example, i.e., the probability of having at least a cell of the column $\mathcal{O}$ equal to one
    \item $p_2^m$: probability of having the neuron of $\popII$ at high rate for the first $m$ patterns
    \item $(1-p_2)^{\T - m}$: probability of having the neuron of $\popII$ at low rate for the remaining $\T - m$ patterns
    \item $(1-p_1)^m$: probability of having one neuron of $\popI$ under threshold for the first $m$ patterns
    \item $1-(1-p_1)^m$: probability of having one neuron of $\popI$ above the threshold for the first $m$ patterns
    \item $(1-p_1)^{m(\C-h)}$: probability of having $\C-h$ neurons of $\popI$ under threshold for the first $m$ patterns
    \item $[1-(1-p_1)^m]^{h}$: probability of having $h$ neurons of $\popI$ above the threshold for the first $m$ patterns
\end{itemize}

Now we can combine all these results to calculate the probability that one neuron of population $\popII$ and $h$ neurons of the population $\popI$ are above threshold for $m$ generic patterns (i.e., not necessarily the first $m$):
\begin{equation}
\label{eq:Q_mh}
    Q(m,h) = \binom{h}{\C}  \hspace{2mm} [1-(1-p_1)^m]^h  \hspace{2mm} (1-p_1)^{m(\C-h)} \binom{m}{T} p_{2}^m (1-p_2)^{\T-m}
\end{equation}    
The previous formula corresponds to the probability that a synapse is consolidated (which in our models occurs when the rates of two connected neurons are above threshold). The two binomial coefficients are introduced to take into account all the possible combinations (in the choice of $m$ patterns over all the possible $\T$ patterns and in the choice of $h$ presynaptic neurons in a total of $\C$ connections). If we want to calculate the probability of consolidation over $m$ patterns, we have to sum $Q(m,h)$ over $m$:
\begin{equation}
    P(h) = \sum_{m=0}^T Q(m,t)
\end{equation}
If we define $\rho_m=(1-p_1)^m$ we can write the previous equation as:
\begin{equation}
\begin{split}
    \langle h \rangle &= \sum_{m,h}  h Q(m,h) = \sum_m \binom{m}{\T} p_{2}^m (1-p_2)^{\T-m} \sum_{h=0}^{\C} h \binom{h}{\C} (1-\rho_m)^h \rho_{m}^{\C-h} =\\
    &= \sum_m \binom{m}{\T} p_{2}^m (1-p_2)^{\T-m} \C(1-\rho_m)
    \end{split}
\end{equation}

Replacing again $\rho_m=(1-p_1)^m$ we obtain:
\begin{equation}
 P(h)=C \cdot[\hspace{0.2cm}\sum_m \binom{m}{T} p_{2}^m (1-p_2)^{T-m} - \sum_m \binom{m}{T} p_{2}^m (1-p_2)^{T-m} (1-p_1)^m \hspace{0.2cm}]
\end{equation}
the last equation is the difference of two terms which can be written as:

\begin{equation}
    (a+b)^n=\sum_{0}^n a^{n-k}b^{k}
\end{equation}
So, finally we get the expression of $\langle h \rangle$:

\begin{equation}
    \langle h \rangle = C\cdot[1-(1-p_1 p_2)]
\end{equation}
For the calculation of $sigma_{k}$ we need $\langle h^2 \rangle$:

\begin{equation}
\begin{split}
    \langle h^2 \rangle= \sum_{m,n} Q(m,h) h^2 \hspace{2.5cm}\\
    \langle h^2 \rangle= \sum_m \binom{m}{T} p_{2}^m (1-p_2)^{T-m} \cdot \sum_{0}^C \binom{h}{C} (1-\rho_m)^h \rho_{m}^{C-h} h^2
    \end{split}
\end{equation}
Applying the binomial theorem, we obtain the following expression:
\begin{equation}
    \langle h^2 \rangle= C(C-1)\cdot[1+p_1p_2\cdot(p_1-2)]^T-C\cdot(2C-1)\cdot(1-p_1p_2)^T+C^2
\end{equation}

\section{Noise addition during test phase}
\label{app:noise}
The network model described in this work performs the test by presenting to the network the same patterns shown during training. However, for a rigorous test of the model performance, test patterns should be different instances of the same classes for which the model is trained. Indeed, this can be achieved in this model by adding a noise to the $\popI$ input response in order to mimic a novel pattern of a certain class.
To this end, we add a Gaussian noise to the firing rate of each neuron of $\popI$. Given that each neuron has a rate $\nu$ driven from a distribution $p(\nu)$, we describe the total firing rate of the neuron as
\begin{equation}
    \nu_{\text{tot}} = \nu + \eta
\end{equation}
where $\eta$ is a rate driven from a truncated Gaussian distribution $G(\eta)_{\mu_{\text{T}}, \sigma_{\text{T}}}$. Hence, each neuron of $\popII$ receives
\begin{equation}
    \vec{\mathcal{W}}\cdot\vec{\nu_{\text{tot}}} = \vec{\mathcal{W}}\cdot\vec{\nu} + \vec{\mathcal{W}}\cdot\vec{\eta}
\end{equation}
where we use the vector symbol to indicate all the in-degrees of the generic neuron of $\popII$ in exam. Since the noise distribution has zero mean, averaging over the rate to compute the values $\langle\Sb\rangle$ and $\langle\SII\rangle$ would lead to put $\vec{\mathcal{W}}\cdot\vec{\eta}=0$, thus noise would give non-zero contribution only to $\varSb$.\\
The variance of the background signal thus would be the sum of $\varSb$ calculated without noise (see Equation \ref{eq:var_sb2_cont}) and the variance of the background due to the additional rate $\eta$, where $\langle \eta \rangle=0$. The simple sum can be done since noise and input rate are independent. Thus
\begin{equation}
\label{eq:noisy_varSb}
    \sigma^{*2}_{\text{b}} = \varSb + \sigma^2_{\eta} \C \Bigl[ p \Wc^2 + (1-p)\Wb^2 \Bigr]
\end{equation}
where $\sigma^2_{\eta}=\sigma^2_{\text{T}}$, ergo is the variance of the truncated Gaussian distribution $G(\eta)_{\mu_{\text{T}}, \sigma_{\text{T}}}$. To estimate the values of mean and standard deviation of the truncated distribution, we define $\phi(\eta)_{\mu, \sigma}$ as the standard normal distribution from which $G(\eta)_{\mu_{\text{T}}, \sigma_{\text{T}}}$ is derived. Truncating the distribution in the symmetric interval $[-2\sigma, 2\sigma]$, the mean of the truncated distribution is equal as the one of the standard normal distribution (which is zero), whereas the standard deviation is
\begin{equation}
    \sigma_{\text{T}}=\sigma \Bigl[ 1- \sqrt{\frac{2}{\pi}} \frac{e^{-2}}{\text{erf}(\sqrt{2})} \Bigr]^{1/2}
\end{equation}
For Equation \ref{eq:noisy_varSb} to be correct, the network should be trained using non-noisy input (i.e., having rates of $\popI$ driven only from the log-normal distribution). Alternatively, more training instances of the same class can be generated by adding a Gaussian noise, however, in this case the firing rate threshold used to distinguish an high rate to a low rate should change from $\nu_{\text{t}}$ to a value that take into account that a neuron with input rate $\nu_{\text{t}} - \eta_{\text{max}}+\epsilon$, with $\epsilon \ll 1$ and $\eta_{\text{max}}=\max\Bigl\{ G(\eta)_{\mu_{\text{T}}, \sigma_{\text{T}}}\Bigr\}$ could be considered high rate when noise is added. Thus, we define the new firing rate threshold as
\begin{equation}
    \nu_{\text{t}}' = \nu_{\text{t}} + \eta_{\text{max}}
\end{equation}




\end{document}




\end{document}
%
% ****** End of file aipsamp.tex ******