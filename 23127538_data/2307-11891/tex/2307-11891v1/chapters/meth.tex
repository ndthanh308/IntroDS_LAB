\section{Methods} 

The development and improvement of methodologies to detect GWs is a continually evolving field. Our research focuses on enhancing the accuracy and efficiency of the GravAD pipeline, a process that includes three primary stages: generating simulated signals, refining the optimisation strategy, and reducing the number of templates.

Each of these stages has the objective of yielding high Signal-to-Noise Ratios (SNRs). The SNR is how strong the GW signal is against a background of noise. This indicator alerts us to the presence of a detection \cite{snr}. Each iteration in GravAD's search aims to refine templates based on gradient information. This works by updating mass parameters fed into the waveform generator. For more information on how GravAD works, visit our previous publication \cite{Doyle_2023}.

\subsection{Simulated Signals Generation}

Simulated signals, defined as synthetic data crafted to mimic real GWs, prove instrumental in testing the effectiveness of the GravAD pipeline. They provide valuable insights into the accuracy of the algorithm by facilitating comparisons between known parameters and those estimated.

The python library, \texttt{ripple} \cite{ripple_gw}, is utilised to generate these simulated signals according to predetermined parameters, amid the interference of noise. This inclusion of noise is a methodological decision intended to assess the efficacy of GravAD when applied to realistic signals. Each simulated signal is created with a pair of masses. Masses from 20 to 100 (with a step of 10) are used for both the primary and secondary bodies involved in the simulated gravitational event. This consequently results in the generation of signals that correspond to the coalescence of two objects, one of which, for example, could possess a mass of 20 solar masses, while the other could potentially be as massive as 80 solar masses. For each pair of masses, the frequency domain waveform of the GW signal is generated using the \texttt{gen\_waveform} function from the GravAD library. This function takes as input the masses of the two bodies, a frequency series determined by a delta frequency/step size, and a set of parameters describing the spins, distance, and phase; however, these are all set to the same value as they have minimal impact on the search.

Once the waveform is generated, it is then transformed into a noisy signal in the frequency domain. This process is proceeded by adding a noise profile obtained from the Power Spectral Density (PSD) derived from the event 'GW150914' detected by the LIGO Hanford detector (H1).

The noisy waveform, $h_{noisy}(f)$ is given by: 

\begin{equation}
	h_{noisy}(f) = h(f) + N(f) 
\end{equation}

Where $h(f)$ is the generated waveform, $N(f)$ is the noise profile from the PSD.

This approach to generating and storing a wide range of simulated signals with varying mass parameters allows for robust testing of the GravAD pipeline under different signal scenarios.

\subsection{Optimisation Strategy Selection}

In our prior endeavours, we predominantly employed stochastic gradient descent (SGD) and simulated annealing (SA) in our pursuit of detecting GWs. In a bid to further refine our search capabilities, we incorporated the concept of momentum in our gradient computations, resulting in an enhancement in the form of Adaptive Moment Estimation (Adam).

The effectiveness of Adam can be attributed to its ability to combine the benefits of two extensions of SGD, specifically Root Mean Square Propagation (RMSProp) and Momentum. RMSProp employs a moving average of squared gradients to normalise the gradient, facilitating faster convergence and eliminating the risk of vanishing learning rates \cite{rmsprop}. On the other hand, Momentum takes into account past gradients to smooth out the update. Therefore, Adam, effectively mitigates the challenges of high variance in parameter updates, providing smoother convergence to optimal solutions.

Despite our expectations, the implemented Adam method did not prove as effective as alternative approaches. Our findings revealed that utilising solely the momentum aspect of Adam led us to locate the optimal template more swiftly than with the comprehensive Adam application. This adjustment, importantly, maintained a higher level of accuracy.

\subsubsection{Stochastic Gradient Descent}

Our gradient is calculated using AD, which simplifies things greatly. We take the derivative of the SNR calculation and combine this with a learning rate in order to climb to the top of the peak, essentially performing gradient ascent.

We can therefore create an updated mass parameter $\theta_i$:

\begin{equation}
	\theta_i = \alpha \cdot g_i
\end{equation}

where $i$ corresponds to the index of the parameter (in this case $i=0$, corresponding to the first gradient value), and with $\alpha$ as the learning rate and $g_i$ as the current gradient.

\subsubsection{Simulated Annealing}

Simulated annealing complements SGD by facilitating its escape from local maxima, enhancing the optimisation process. By enabling hill-climbing moves, which may temporarily worsen the objective function value, SA offers a mechanism to explore alternative solutions in pursuit of a global optimum \cite{SA}. In GravAD we use this mechanism in the form of a perturbation.

The perturbations are generated with a normal distribution and then scaled by the temperature. If $N(0,1)$ denotes a standard normal distribution (mean 0, variance 1), then the perturbations can be represented as:

\begin{equation}
	P_i = T_i \cdot N(0,1)
\end{equation}

with $T_i$ as the temperature.

The updated parameters are then calculated by adding the product of the learning rate and gradient (from SGD) to the perturbation (from SA). Given $\theta_i$ as the current mass (update parameter), $\alpha$ as the current learning rate, and $g_i$ as the current gradient, the new mass parameter can be calculated as:
\begin{equation}
	\theta_{i} = \theta_{i-1} + \left(\alpha \cdot g_i \right)+ P_i
\end{equation}

We commence with an initial temperature value of 1, which subsequently diminishes with each iteration due to the annealing rate of 0.99. The temperature parameter is updated by a straightforward multiplication of the current temperature with the annealing rate:

\begin{equation}
	T_{i+1} = T_i \cdot \gamma
\end{equation}

Where $T_{i+1}$is the new temperature and $\gamma$ is the annealing rate.

\subsubsection{Adam}
Adam's optimisation process can be expressed through the following equations, as outlined in\cite{adam}:

\begin{equation}
	m_i = \beta_1 \cdot m_{i-1} + (1 - \beta_1) \cdot g_i \label{eq:mo}
\end{equation}

where $m_i$ is the moving averages of the gradient, $g_i$ is the current gradient, and $\beta_1$ is the decay rate set to 0.9. We then calculate the gradient squared:

\begin{equation}
	v_i = \beta_2 \cdot v_{i-1} + (1 - \beta_2) \cdot g_i^2
\end{equation}

where $\beta_2$ is the decay rate set to 0.999. From here we can compute the bias-corrected estimates. Firstly:

\begin{equation}
	\hat{m_i} = \frac{m_i}{1 - \beta_1^i}
\end{equation}

then,

\begin{equation}
	\hat{v_i} = \frac{v_i}{1 - \beta_2^i}
\end{equation}

resulting in our updated parameter:

\begin{equation}
	\theta_i = \theta_{i-1} - \alpha \cdot \frac{\hat{m_i}}{\sqrt{\hat{v_i}} + \epsilon}
\end{equation}

 with $\alpha$ as the learning rate, and $\epsilon$ is a small constant to avoid division by zero.
  
 \subsection{Applying the Optimisations}
 
 When we combine the optimisations of SGD, SA, and Adam we get:
 
 \begin{equation}
 	\theta_i = \theta_{i-1} - \alpha \cdot \frac{\hat{m_i}}{\sqrt{\hat{v_i}} + \epsilon}  + P_i .
 \end{equation}
 
In practice, however, this method falls short in performance when compared to the likes of SGD and SA. Prompted by this discovery, we ventured into an alternative approach, using only a segment of the Adam optimiser: the momentum.
 
We apply equation \ref{eq:mo} in a straightforward manner. Subsequently, the updated parameter is determined by the sum of the product of the learning rate and the updated momentum (from the preceding step), and the perturbation (derived from SA). Given $\theta_i$ as the current parameter (in this context, mass1), $\alpha$ as the current learning rate, and $P_i$ as the current perturbation, the new parameter is computed as follows:
 
 \begin{equation}
 	\theta_i = \theta_{i-1} + \left(\alpha \cdot m_i\right) + P_i
 \end{equation}
 
The aforementioned equations illustrate the parameter update rules when utilising SGD with momentum, combined with SA. These rules demonstrate how the gradient information, momentum, and perturbations guide the search for optimal solutions within the parameter space.

The subsequent inclusion of momentum serves to further fine-tune this process. By factoring in the momentum of the gradient, GravAD facilitates swifter convergence and a more nuanced exploration of the parameter space. Consequently, the combination of these optimisation strategies not only accelerates the attainment of solutions but also boosts the likelihood of these solutions being near-optimal. This, in turn, amplifies our capabilities in GW detection.

\subsection{Template Reduction Technique}

GravAD implements an adaptive termination procedure to fine-tune its exploration of the parameter space, which is analogous to the callback function found in machine learning frameworks such as TensorFlow. This function is engineered to pinpoint suitable peak values in the SNR landscape, and prematurely terminate the search upon their discovery.

The mechanism functions by identifying occasions when the SNR surpasses a previously recorded peak. Upon identification, the algorithm records the new peak SNR and its corresponding iteration index. If, for instance, 5 iterations pass without topping the previously recorded peak SNR, the system makes a strategic manoeuvre to conclude the search prematurely.

The capability to adaptively terminate manifests as an efficient methodology for navigating the template space. This results in a substantial decrease in the number of templates required for the search. Consequently, it improves GravAD's proficiency to detect GWs rapidly.

For subsequent experiments, we selected a cutoff value of 2. This indicates that if the SNR does not improve within two additional iterations, the algorithm will cease the search. This value was chosen because it doesn't significantly degrade the SNR and produces results comparable to a cutoff value of 25. It also leads to a low average number of iterations. We arrived at these values and averages by using events: GW150914, GW151012, GW151226, GW170104, GW170729, GW170809, GW170814, GW170818, GW170823. The outcomes of these tests are visualised in Figure \ref{fig:cutoff}.

% Figure environment removed