\section{Deep learning-based imaging}
\label{sec:dl_model}

In this work, we perform the imaging of radio interferometer data using deep learning techniques available inside the Python package \texttt{radionets} \citep{radionets}.
Our approach differs from the conventional CLEAN algorithm \citep{clean} in that we reconstruct missing information directly in Fourier space.
In the reconstruction process, no iterative source model is formed. 
Consequently, no convolution with the clean beam of the observation is necessary.

% Figure environment removed

The reconstruction of the missing data in Fourier space is performed by the trained deep learning model.
The model exploits the data recorded during an observation and uses the contained information to estimate values for missing pixels with the help of convolutional layers \citep{Goodfellow_convs}.
We describe the underlying methodology in our previous publication \citet{Schmidt_2022}.
In the following, we discuss the changes we made to handle the simulated FIRST data.

\subsection{Model definition}

Our imaging architecture is based on residual blocks \citep{residual-nets, residual-block} integrated into a \texttt{SRResNet} architecture known from super-resolution problems \citep{superres}.
A detailed model description can be found in \cite{Schmidt_2022}.
We keep the original architecture except for increasing the number of residual blocks to sixteen. The reason for this is that as the complexity of the input images increases, as described in the previous sections, the depth and complexity of the network must also increase in order to obtain meaningful reconstructions.

\subsection{Model training}

Since our deep learning-based imaging is performed directly in Fourier space,
the deep learning model training is done in Fourier space as well.
The simulated visibilities, described in \autoref{sec:rime}, are used as input.
% Here, incomplete real and imaginary maps are passed to the model in parallel.
During the training process, the model learns to reconstruct missing information and generates complete real and imaginary maps as output.
\autoref{fig:train_overview} gives an overview of said training routine.

For model training, we use a data set consisting of \num{70000} real and imaginary images. \num{50000} of these images are used for the training of the neural network.
Additional \num{10000} validation maps help to check for over-training.
In \autoref{sec:evaluation}, the remaining \num{10000} images are used for evaluating the trained model.
In one training epoch all training and validation maps are passed through the network.
The loss function that quantifies the difference between the network's prediction and the simulated truth is chosen to be a split L1 loss \eqref{eq:l1}, which is defined as:

\begin{align}
    \mathrm{Loss} &= \mathrm{L1}\left(x_{\mathrm{real}}, y_{\mathrm{real}}\right) + \mathrm{L1}\left(x_{\mathrm{imag}}, y_{\mathrm{imag}}\right),  \label{eq:l1} \\
    \mathrm{with} &~~\mathrm{L1}(x, y) = \left|~x - y~\right|,
\end{align}
where $x$ is the predicted output from the network and $y$ is the true distribution.

We used the ADAM optimizer \citep{adam} to update the weights during training, which outperformed stochastic gradient decent (SGD) \citep{sgd} in convergence time at the cost of more readily learned parameters.
We trained the neural network model for \num{400} epochs with a batch size of \num{100}.
An adaptive learning rate is applied, starting with a learning rate of $2\cdot10^{-4}$ which peaks after 150 epochs at $1\cdot10^{-3}$.
Afterward, the learning rate steadily decrease to $6\cdot10^{-4}$ until the end of the training.
The duration of one epoch is \SI{185}{\second}, which resulted in a total training time of $\approx$ \SI{20.5}{\hour}, on the computing specifications summarized in \autoref{tab:computer-specifications}.
The reconstruction times of the neural network model are of the order of milliseconds.
For a $\SI{128}{\pixel} \times \SI{128}{\pixel}$ image the pure reconstruction time of the neural network is \SI{6.4\pm0.1}{\milli\second} evaluated 100 times on the same image.
As our approach has no iterative routines, the reconstruction time is independent of the $(u, v)$ coverage.
A reconstruction time comparison with established imaging software is given in \autoref{sec:times}.


\autoref{fig:train_loss} visualizes the curves for the training and validation loss.
Both, training and validation losses are steadily decreasing in both cases.
A gap between training and validation loss starts to form after the first $\sim10$ epochs.
However, this gap is not a sign of over-training, as the validation loss does not start to increase in later epochs.

We were able to smooth the loss curves by implementing a normalization based on all training images.
Here, the mean of all non-zero input values is subtracted from all non-zero pixels.
In the next step, the result is divided by the standard deviation of all non-zero values.
The complete normalization is defined as
\begin{align}
    x_\text{norm} = \frac{x - \mathrm{mean}(x)}{\mathrm{std}(x)},~~\text{for}~~x\neq0.
\end{align}

% Figure environment removed

% Figure environment removed

\subsection{Reconstruction}

In this section, we show how the deep learning model is able to reconstruct incomplete visibility data.
\autoref{fig:compl_reco} shows the reconstructed real and imaginary maps for an exemplary test source. The top row illustrates the reconstruction for the real part, while the bottom row is dedicated to the imaginary part.

The distributions shown in \autoref{fig:compl_reco} reveal a good agreement between reconstruction and simulated truth.
Especially for the real part, all structures in the center of the image are reconstructed well. For the imaginary part, the center is also well reconstructed, while the outer parts are predicted to be a constant. Since in the true image these parts are not significant, this is not a big effect on the reconstruction. The differences for, both, real and imaginary part are an order of magnitude lower than the flux density in the original image.

In \cite{Schmidt_2022}, we
used amplitude and phase instead of real and imaginary parts of the complex visibilities.
In the case of simple Gaussian sources, using amplitude and phase instead of real and imaginary parts had the advantage that the parameter space was smaller.
However, we noticed a sharp decrease in the phase reconstructions when switching to more realistic data.
Our experiments showed that in the case of FIRST simulations the reconstruction got much better with real and imaginary maps.  \autoref{fig:imag_vs_phase} shows that the reconstructions of the imaginary part are reasonably good up to the edges, while phase map reconstructions deviate more significantly from the true values in these areas.