\section{Diagnostics}
\label{sec:evaluation}

% Figure environment removed

% Figure environment removed

In this section, we quantify the reconstruction ability of our deep learning model.
%Evaluation marks the last part of our analysis chain, see \autoref{fig:analysis-chain}.
The diagnostics will be performed in image space, which means the reconstructed visibility distributions are processed by the inverse Fourier transformation before the evaluation techniques get applied on the reconstructed source distributions.
Furthermore, we compare our deep learning-based approach with the
Cotton-Schwab Clean algorithm \citep{schwab} implemented in
the established imaging software \texttt{WSCLEAN} \citep{wsclean}.
\autoref{tab:wsclean-params} summarizes the cleaning parameters, which were used for all cleaning tasks with \texttt{WSCLEAN} in this work. Due to the increasing complexity and computation times, we restricted ourselves to a small number of tuneable parameters. Therefore, we omit e.g. the multi-scale cleaning and use the conservative Cotton-Schwab clean algorithm.
For both \texttt{radionets} and \texttt{WSCLEAN}, the goal is that reconstructed and simulated radio galaxies do not deviate.

\begin{table}%[ht!]
    \centering
    \caption{Overview of the parameter settings used to create the clean images using \texttt{WSCLEAN}.}
    \begin{tabular}{lr}
    \toprule
    Parameter & Setting \\
    \midrule
    size & \SI{128}{px} \\
    scale & \SI{1.56}{asec} \\
    mgain & \num{0.8} \\
    gain & \num{0.01} \\
    niter & \num{1000000} \\
    auto-mask & \num{3} \\
    autothresh & \num{1} \\
    weight & uniform \\
    \bottomrule
    \end{tabular}
    \label{tab:wsclean-params}
\end{table}

\subsection{Comparison with \texttt{WSCLEAN}}
\label{sec:comparison_wsclean}

As a first evaluation, we perform a visual analysis of the inverse Fourier transformed visibility maps shown in \autoref{fig:compl_reco}. The corresponding source distributions in image space are shown in \autoref{fig:image-pred}.
In particular, we show the deep learning-based source reconstruction, the simulated source distribution and their difference. 
The mean values of the differences are an order of magnitude smaller than the peak flux densities, meaning that simulation and prediction only differ slightly.
\autoref{fig:reco_wsclean} shows the reconstruction for the same source using \texttt{WSCLEAN}.
The comparison between the reconstructed and the simulated source distribution reveals a smeared-out source structure in the case of \texttt{WSCLEAN}.
This smearing results from the convolution with the restoring beam
inside \texttt{WSCLEAN}'s cleaning routine.

To test the model's performance, we need to evaluate more than one image.
To this end, we have developed three diagnostic methods that focus on different aspects of a complete source reconstruction.
We also use an existing evaluation metric, the Multi Scale Structural Similarity Index Measure (MS-SSIM), to compare the similarity between reconstructed and simulate source images.
All four evaluation methods were applied to a distinctive test data set containing \num{10000} images.
The test images are the same for the \texttt{radionets} and the \texttt{WSCLEAN} evaluation.

First, we compare the reconstructed and simulated source areas. 
The necessary cut to distinguish source and background structures is \SI{10}{\percent} of the peak flux density.
Then, the area of every source component in the image is computed and summed up for both predicted and simulated source distribution. This is done by obtaining the contour levels using \texttt{matplotlib} and and relating them to the enclosed area using Leibniz' Sector formula. A more detailed explanation can be found in our previous work \citep{Schmidt_2022}.
\autoref{fig:hist-area} shows the area ratios for the \texttt{radionets} and the \texttt{WSCLEAN} reconstructions.
The optimal value of one is reached in cases where the source areas of, both, prediction and simulation are exactly the same.
Ratios below one indicate an underestimated source area, while ratios above one denote overestimated areas. 
As shown in the histogram, for the majority of test sources reconstructed and simulated source areas match well in the case of \texttt{radionets}, which is confirmed by the mean ratio of \num{0.971} and the standard deviation of \num{0.088}.
In the case of \texttt{WSCLEAN}, the source area is overestimated around \SI{20}{\percent} for most of the sources.
The reason for this overestimate is the beam smearing in \texttt{WSCLEAN}'s reconstruction routine.
After creating a point source model, the built model is 
convolved with the clean beam of the observation.
This procedure spreads the source emission to a larger area of the sky.
For a fair comparison between the \texttt{radionets} and \texttt{WSCLEAN} reconstruction, we take into account the smearing during our comparison with the simulated images.
We use the clean beam computed by \texttt{WSCLEAN} to smear the simulated source distribution before we calculate the ratios for \texttt{WSCLEAN}.

% Figure environment removed

The second method examines the peak flux densities of the predicted and simulated source distributions.
Again, the above threshold is applied to define the source.
Then, the ratio between predicted and simulated peak flux densities is calculated.
\autoref{fig:hist-peak} shows the values for the \texttt{radionets} and \texttt{WSCLEAN} reconstructions.
The optimal ratio is again one, while ratios above and below indicate over- and underestimates, respectively.
The distribution peaks around the 1 with a mean ratio of 0.998 and a standard deviation of 0.072 for \texttt{radionets}.
This underlines that the brightest spot, e.g. the core of the source, is reconstructed well.
Again, the comparison with \texttt{WSCLEAN} is a bit more complicated.
Beam smearing effects have to be taken into account before the peak flux density comparison with the simulated source distribution is possible.
While the simulated flux densities are in units of $\mathrm{Jy}\cdot\mathrm{px}^{-1}$, the \texttt{WSCLEAN} reconstructions are in units of $\mathrm{Jy}\cdot\mathrm{beam}^{-1}$.
We compute the beam area, divide it by the pixel area, and use the scaling factor to scale the flux densities of the \texttt{WSCLEAN} images so that they match the simulations.
For the beam area calculation, we use the expression

\begin{align}
    A_\text{beam} = \frac{2\,\pi \, b_\text{min} b_\text{maj}}{8 \, \ln(2)},
\end{align}
where $b_\text{min}$ is the full width at half maximum (FWHM) of the beam's minor axis and $b_\text{maj}$ the FWHM of the beam's major axis.
Even though the images have the same flux density units, the peak flux density of the \texttt{WSCLEAN} reconstruction is underestimated by a factor of two with a mean ratio of \num{0.481} and a standard deviation of {0.059}.
The beam smears the initial peak flux to neighboring pixels, which causes a decrease of the peak flux density in this individual pixel. 
Another effect to take into account is that the effective resolution for \texttt{radionets} is higher than for \texttt{WSCLEAN}, as can be seen in the right images of \autoref{fig:image-pred} and \autoref{fig:reco_wsclean}. Thus, the non-correctable beam smearing and the difference in resolution lead to the expected lower peak flux densities for \texttt{WSCLEAN}.
%For this reason, peak flux density underestimations are too strong for \texttt{WSCLEAN}, as the exact correction of beam smearing to neighboring pixels is not possible.

%The evaluation of the integrated peak flux densities reveals that the unit correction for \texttt{WSCLEAN} basically works.
For the third diagnostic method, we sum up all flux densities inside of the \SI{10}{\percent} peak flux density threshold to calculate the integrated flux density.
Then, we compute the ratio between the values for reconstruction and simulation.
\autoref{fig:hist-sum} shows the resulting ratios for \texttt{WSCLEAN} and \texttt{radionets}.
For both reconstruction techniques, the integrated peak flux densities match well for the complete test data set.
The good agreement is apparent in the mean and standard deviation of \num{0.976\pm0.053} for \texttt{radionets} and \num{0.975\pm0.069} for \texttt{WSCLEAN}.
In the case of the integrated flux density, the beam smearing of \texttt{WSCLEAN} plays no significant role anymore after transforming the images from units of $\mathrm{Jy}\cdot\mathrm{beam}^{-1}$ to units of $\mathrm{Jy}\cdot\mathrm{px}^{-1}$.

% To summarize the results of the first three evaluation methods, we showed that the trained model is able to reconstruct the source area, the brightest spot in the source and the integrated flux density really well, which are three main properties of a radio source. Thus, it can be safely said that the reconstructed and the true source correspond well.

% Figure environment removed

For a more image-based comparison of the simulated and the predicted source distributions, we use a criterion commonly used in the field of computer vision, the \textbf{M}ulti \textbf{S}cale \textbf{S}tructural \textbf{S}imilarity \textbf{I}ndex \textbf{M}easure (MS-SSIM) \citep{ms-ssim}. 
This metric is split into three distinctive parts, which are luminance, contrast and structure:

\begin{align}
    l(x, y) &= \frac{2 \mu_x \mu_y + C_1}{\mu_x^2 + \mu_y^2 + C_1} \label{eq:luminance}
\end{align}
\begin{align}
    c(x, y) &= \frac{2 \sigma_x \sigma_y + C_1}{\sigma_x^2 + \sigma_y^2 + C_1} \label{eq:contrast}
\end{align}
\begin{align}
    s(x, y) &= \frac{\sigma_{xy} + C_3}{\sigma_x \sigma_y + C_3} \label{eq:structure}
\end{align}
with $\mu$ as the mean, $\sigma$ as the standard deviation, $\sigma_{xy}$ as the covariance, $x$ and $y$ as the images to be compared and

\begin{equation}
    C_1 = (K_1\,L)^2, C_2 = (K_2 \, L)^2 \ \text{and} \ C_3 = C_2/2 \, .
\end{equation}
Here, $L$ refers to the dynamic range of the pixel values and $K_1 \ll 1$ and $K_2 \ll 1$ to small scalar constants that are used to correct for numerical instabilities in the denominator. The metric is then a combination of those three components \eqref{eq:luminance} to \eqref{eq:structure}:

\begin{equation}
    \mathrm{SSIM}(x, y) = \left[l_M(x, y)\right]^{\alpha_M} \prod_{j=1}^M \left[c_j(x, y)\right]^{\beta_j} \left[s_j(x, y)\right]^{\gamma_j} \, .
\end{equation}
This formula is an improvement over the predecessor because it opens up the possibility to use image details at different resolutions. The optimal value is 1, which can only be achieved if $x=y$.

As in the previous three evaluation methods, we reconstructed \num{10000} test sources with \texttt{WSCLEAN} and \texttt{radionets}.
Then, we computed the MS-SSIM between the reconstructed and simulated source distributions, see  
\autoref{fig:hist-msssim}.
In the case of \texttt{radionets}, the distribution peaks around the optimal value of one.
Further evidence for the good performance is the fact that the mean is \num{0.999} and the standard deviation is \num{0.002}.
In terms of computer-vision and image reconstruction tasks, the predicted and simulated source distributions are very similar with very few outliers.
This proves the robustness of our approach to model, both, noisy input images and more complex source structures, which is a major improvement over the Gaussian sources from our previous work. For \texttt{WSCLEAN}, the results are worse with a mean and standard deviation of \num{0.920(62)}. 
The effect is related to the beam smearing inside \texttt{WSCLEAN}'s cleaning routine.
The deviation of the MS-SSIM is consistent with the distributions shown in \autoref{fig:hist-area}.
Even though we apply the same clean beam to the simulated sources before calculating the MS-SSIM, the beam smearing effects cannot be completely corrected, which results in an source area overestimation around \SI{20}{\percent}.
When computing the mean structural similarities, this area over-estimations lead to deviations from the optimal value of one in the case of \texttt{WSCLEAN}. All mean values and standard deviations from the four methods are summarized in \autoref{tab:overview_eval}.

\begin{table}
    \centering
    \caption{Overview of the mean values and standard deviations from \autoref{fig:hist-area} to \autoref{fig:hist-msssim}.}
    \begin{tabular}{c|c|c}
    \toprule
    method & \texttt{radionets} & \texttt{WSCLEAN} \\
    \midrule
    Area &  \num{0.971(88)} & \num{1.200(319)}\\
    Peak flux &  \num{0.998(72)} & \num{0.481(59)}\\
    Integrated flux & \num{0.976(53)} & \num{0.975(69)}\\
    MS-SSIM & \num{0.999(2)} & \num{0.920(62)} \\
    \bottomrule
    \end{tabular}
    \label{tab:overview_eval}
\end{table}

In order to test the robustness of both approaches, we split the results of the  evaluation methods into the different SNRs of the input data.
The SNRs of the input data is quantified with the help of \texttt{WSCLEAN}.
We run \texttt{WSCLEAN} on the test data with the settings summarized in \autoref{tab:wsclean-params}, but the \texttt{auto-mask} option is set to \num{5}.
Afterwards, we calculate the the SNR as follows:

\begin{align}
    \mathrm{SNR} = \frac{\mathrm{max}(I_\text{clean})}{\mathrm{std}(I_\text{residual})}.
\end{align}
Here, $I_\text{clean}$ denotes the clean image and $I_\text{residual}$ represents the residual image.
The resulting SNR distribution is shown in \autoref{fig:hist_snr}.
\autoref{fig:vis_snr} shows input data with a SNR of \num{5}, \num{35}, and \num{70} for a visual analysis of the different noise levels.

We created seven SNR categories,
%starting from zero to ten with a step size of ten and finishing with all SNRs above sixty.
0 to 10, 10 to 20, and so on with the final category begin all SNR above sixty.
For the comparison between \texttt{radionets} and \texttt{WSCLEAN}, we compute the mean and standard deviation for each category and plot the standard deviation against the mean ratio.

\autoref{fig:mean-std-area} shows the means and standard deviations of the area ratios from \autoref{fig:hist-area} split into the different SNRs of the input visibility maps.
Both reconstruction techniques converge to the optimal mean ratio of one and the optimal standard deviation of zero for larger SNRs.
While \texttt{radionets} reconstructions underestimate the source areas for smaller SNRs, \texttt{WSCLEAN} overestimates the source areas for these cases.

The means and standard deviations of the peak flux density ratios from \autoref{fig:hist-peak} are shown in \autoref{fig:mean-std-peak}.
Analyzing the results for the different SNRs of the input visibilities reveals no significant improvements regarding the mean ratios.
The standard deviation is slightly improved with increasing SNR.
Peak flux densities for \texttt{WSCLEAN} are underestimated too much, as the exact correction of beam smearing to
neighboring pixels is not possible.

In \autoref{fig:mean-std-sum} the mean and standard deviations for the integrated flux density ratios are shown.
A significant improvement for means and standard deviations is evident for \texttt{radionets} and \texttt{WSCLEAN} reconstructions.
For both reconstruction techniques, the highest SNRs converge to the optimal mean of one and the optimal standard deviation of zero.

\autoref{fig:mean-std-msssim} shows the mean and standard deviation of the MS-SSIM split into the different SNRs of the input visibilities.
For the \texttt{radionets} reconstructions, the MS-SSIM does not significantly change for different SNRs, while the results improve with higher SNRs in the case of \texttt{WSCLEAN}.
The smaller MS-SSIM values for \texttt{WSCLEAN} originate from the smeared out source structures because of the clean beam of the observation.
The clean beam improves with higher SNRs.
Consequently, the smearing decreases, which results in an improved MS-SSIM.

A detailed summary of the corresponding values is given in \autoref{tab:snr} for \texttt{radionets} and in \autoref{tab:snr-wsclean} for \texttt{WSCLEAN}.

\subsection{Computing times}
\label{sec:times}

\begin{table}
    \centering
    \caption{Comparison of run-times to reconstruct simulated observations with \texttt{WSCLEAN} and \texttt{radionets}. Computing times are shown for a $\SI{128}{\pixel} \times \SI{128}{\pixel}$ image. For \texttt{radionets}, the run-times are split into pure deep learning reconstruction times (DL reco) and reconstruction times including data operations (DL reco + I/O).}
    \begin{tabular}{lc}
         & Computing times / s \\ \toprule
    \texttt{WSCLEAN}                    &  \num{4.52\pm0.23} \\
    \texttt{radionets}: DL reco + I/O   &  \num{2.01\pm0.03} \\
    \texttt{radionets}: DL reco         &  \num{0.04\pm0.01} \\ \bottomrule
    \end{tabular}
    {\parbox{3in}{\vspace{0.15cm}\footnotesize Note: The programs are run for 100 times on one image. The presented values are the mean run-times with standard deviations. Pure deep learning reconstruction times (DL reco) just include the time needed for applying the deep learning model. Evaluation times including data loading, model loading, and data saving (DL reco + I/O) are significantly longer as these data operation tasks are not yet optimized.}}
    \label{tab:run_times}
\end{table}

A major advantage of the deep learning-based analysis are the short computing times, after the deep learning model was trained.
While \texttt{WSCLEAN} requires around \SI{4.5}{\second} to process a $\SI{128}{\pixel} \times \SI{128}{\pixel}$ simulation, our \texttt{radionets} framework less than half this time with \SI{2.01}{\second} (DL reco + I/O).
This reconstruction time includes also data operations like data loading, model loading and saving the reconstruction.
Data operations are expected to be accelerated in the future, as these are not yet optimized.
The pure reconstruction times for the deep learning model (DL reco) are even faster as convolutional neural networks (CNN) are very efficient when applied to image data.
The neural network models needs around \SI{40}{\milli\second} to reconstruct a $\SI{128}{\pixel} \times \SI{128}{\pixel}$ simulation.
All computing times are summarized in \autoref{tab:run_times}.