% IEEE overleaf file sharing link: https://www.overleaf.com/2927533418rwqjdxrkbvwg
% Google Drive link: https://drive.google.com/drive/folders/1ym1dXhN-pd3BBht-_qdjLbf6LfKH09Ol?usp=sharing
% previous overleaf link: https://www.overleaf.com/2382424889svwbtcbbyjsr

\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{subcaption}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Towards Anatomy Education with Generative AI-based Virtual Assistants in Immersive Virtual Reality Environments*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
Anatomy education is essential to support medical students in understanding the morphology, location, and spatial relationships of anatomical structures. Virtual reality (VR) and interactive 3D visualization systems have been proposed to provide an engaging learning experience and environment. However, VR-based systems integrated with a generative artificial intelligence (AI) assistant for anatomy education are still underrepresented. 
This work presents a VR environment with a generative AI virtual assistant to support human anatomy education, allowing the user to communicate verbally with the virtual assistant. We aim to provide a more interactive, adaptive, and informative learning experience.
The proposed environment was assessed in a pilot user study (n\,=\,16) with a comparison of two configurations: \textit{avatar} and \textit{screen}-based virtual assistant. 
We observed no significant difference between the configurations and difficulty level in the task completion time and the number of interactions with the virtual assistant. 
However, there was a significant difference in the score between the difficulty level in the \textit{avatar} configuration. 
The results also provide insights into the usability, task load, and sense of presence in the virtual environment.  
Our proposed environment offers potential benefits and research directions for medical education, using generative AI to assist and enhance the learning experience. 
\end{abstract}

\begin{IEEEkeywords}
Virtual reality, anatomy education, virtual assistant, generative AI, human-computer interaction
\end{IEEEkeywords}

\section{Introduction} 

Medical anatomy education is one of the crucial aspects of medical training, involving learning the structures and functions of the anatomy in the human body. 
These skills are essential during the prerequisite of surgical procedures, and students should be aware of the variation in morphology and location of anatomical structures.
Traditionally, medical students learn human anatomy through textbooks, lectures, and dissection of cadavers. However, these approaches have several limitations, e.g., the lack of interactivity, the expense of cadavers, and ethical concerns associated with dissection \cite{PREIM2018132}.


In recent years, virtual reality (VR) has emerged as a viable alternative to traditional anatomy education approaches \cite{chen2017recent, chheang2021collaborative, preim2021virtual}. VR can enable students to immerse themselves in a virtual environment where they can interact with anatomy 3D models, making the learning experience more engaging and interactive. 
VR also allows students to perform virtual training in a safe environment without ethical concerns \cite{de2016virtual, chheang2020toward}.
One of the major challenges is the lack of personalized learning experiences. 
Most VR-based systems for anatomy education rely on pre-programmed scenarios that may not meet the individual learning needs of each student. 
Here, generative artificial intelligence (AI) support, such as \textit{ChatGPT}, can be beneficial \cite{li2023chatgpt}.
%This is where generative artificial intelligent (AI) assistance, e.g., ChatGPT, can be beneficial.

Compared to conventional virtual assistants that can be rigid based on predefined rules and templates, generative AI can produce natural and engaging dialogues that mimic human conversations. 
It can adapt to the context, level, and interest and provide personalized conversation and guidance. 
Furthermore, generative AI leverages large-scale data and information from various sources, including scientific articles, textbooks, and datasets, which allows it to generate rich and diverse content to enhance communication and understanding.
 
This work presents an immersive VR environment to support human anatomy education using generative AI conversational assistance (see \ref{fig:environment}).

% Figure environment removed
We integrated generative AI services (\textit{\textit{OpenAI, ChatGPT}}) into the VR environment, enhanced the virtual avatar representation and lip synchronization, and proposed a framework to tackle the verbal conversation between the user and the virtual assistant.
The proposed environment can offer students a more interactive, adaptive, and informative learning experience.
To assess the proposed environment, we developed two configurations: \textit{avatar}- and \textit{screen}-based virtual assistants and compared them in a pilot user study (n\,=\,16).
Our contributions are the following:
\begin{itemize}
    \setlength\itemsep{0em}
    \item An immersive VR environment to support human anatomy education, allowing users to communicate verbally and interact with generative AI-based virtual assistants.
    
    \item Results of a pilot user study (n\,=\,16) providing insights on user performance, usability, task load, and sense of presence in the VR environment.

    \item Exploratory analysis to identify potential features, benefits, limitations, and research directions.
    
\end{itemize}




\section{Related Work}



In this section, we provide an overview of prior research related to the use of VR and virtual assistant in anatomy education.



%%%%% More citations
\subsection{VR-based Anatomy Education Systems}


Traditional methods of learning medical anatomy can be enhanced by using different VR and augmented reality (AR) systems to provide a superior learning experience \cite{pedram2023toward, makinen2022user, bracq2021training, Juan2022Learning}. 
Kurt et al. \cite{KURT2013109} discuss different training methods available for medical anatomy. 
The training approach using cadavers has significant limitations due to the difficulty of obtaining the model, ethical issues, and the health risks involved. VR-based training replicating real-life situations, on the other hand, has gained popularity as an alternative approach.
The potential advantages of VR-based training include minimizing risks, reducing training time and cost, and enabling students to learn in an engaging environment.
A survey conducted by \cite{PREIM2018132} highlighted the importance of focusing on learning perspectives and emphasizing the importance of designing compelling learning scenarios and motivational strategies to support anatomy education.
Boomgaard et al. \cite{boomgaard2022novel} presented an approach for anatomy education by employing AR to enhance the visualization of anatomical structures, offering students an engaging and interactive learning experience. 
They concluded that using immersive technology in blended learning could improve the quality and impact of higher education and training, especially in under-resourced settings. 


% Figure environment removed

\cite{erolin2019using} presented a collection of 3D anatomical models to enhance anatomy education in VR. Their pilot results indicate that most participants found the models easy to interact with and expressed positive feedback. 
\cite{nakai2022anatomy} discussed the feasibility of holding anatomy lectures for medical students online in VR format. They developed the VR environment, allowing participants to interact and modify the anatomical structures of internal body organs. The results suggested potential advantages of the study, including access to multiple 3D models and real-time collaboration.
\cite{kurul2020alternative} conducted a study to investigate how immersive VR affects anatomy training in undergraduate physical therapy students. 
Their results reveal a difference between the VR and control groups and highlight the importance of VR can be used as an alternative method for anatomy training.
\cite{6918271} developed a VR environment to train students to recognize and comprehend anatomical structures and medical procedures.
Their system allows users to manipulate medical data, transforming it into 3D representations and adjusting the relative sizes of objects within the virtual environment. 
In a similar approach, 
Izard et al. \cite{Izard2016} conducted a study using cranium anatomy and yielded similar results, demonstrating the efficacy of VR in enhancing anatomical understanding.
Nakai et al. \cite{nakai_ana} conducted a lecture study for medical anatomy involving the nervous, musculoskeletal, and cardiovascular systems. 
The proposed format includes interactive elements that allow participants to closely analyze the human body and virtually interact with various organs.

\cite{Saalfeld_2020} introduced a tutoring system to improve medical students' learning experience in a shared virtual environment; thus, a teacher can help the student to learn the anatomy of the human skull. 
\cite{schott2021vr} proposed a VR/AR multi-user environment for liver anatomy education using clinical cases. The results demonstrate the potential benefits of the prototype to facilitate surgery education and support a range of training scenarios, from small learning groups to classroom-size settings. 

In addition to VR systems, AR also shows the potential to support anatomy learning environments and could serve as a supplemental teaching tool~\cite{bork2021effectiveness, minopoulos2023medical, richards2023student}. 
Results show that students' behavioral engagement was enhanced in student autonomy and time allocation towards task completion, specifically with viewing images and answering related questions.





% % Figure environment removed


\subsection{Virtual Assistant and Visual Embodiment}

Anatomy education in VR environments entails a complex and resource-intensive process. 
It may require a suitable environment and competent instructors to teach human anatomy effectively.
The psychological implications of virtual assistants have been the research subject for decades \cite{moro2017effectiveness, mao2021immersive, bernardo2017virtual, norouzi2018systematic}.
Kim et al.  \cite{8613756} conducted a study investigating different levels of embodiment in virtual assistants. They created an avatar equipped with a text-to-speech service and various animation movements, a human-in-the-loop mechanism with three different conditions: \textit{speech}, \textit{speech and gesturing}, and \textit{speech, gesturing, and locomotion}.
The study suggested that gesturing and locomotion in the avatar increased trust between the user and the interactive virtual assistant and increased usage for various tasks. 

Haesler et al. \cite{8699187} conducted a study using \textit{Amazon Alexa}, comparing a voice-only version with an embodied avatar version to perform simple everyday tasks.
The results of their study suggested participants preferred the embodied avatar over the voice-only version.
In a separate study, Kim et al. \cite{9089596} analyzed reducing task loads with the help of \textit{Amazon Alexa}. 
The experiment includes three conditions: the user working alone, a voice-only assistant, and an embodied assistant. 
The findings indicated that employing a voice assistant helped reduce the number of tasks; however, users still expressed uncertainty about tasks outside their visual range. % being completed by the voice-only assistant.
On the other hand, the embodied version instilled in users more confidence in completing tasks, fostering increased trust and collaboration between the assistant and the user \cite{yao2022virtual}.
% summary
These studies collectively indicate that users tend to place more trust in tasks they can visually confirm, highlighting the importance of visual embodiment in virtual assistants.

% comparision
Compared to the prior research, our VR environment offers a unique advantage by integrating a generative AI-based virtual assistant, \textit{OpenAI, ChatGPT}, as a companion to support learning human anatomy.  
The virtual assistant enables users to engage in verbal conversations and receive responses to their information queries, resulting in increased confidence and engagement.
Moreover, it can be used as a source of guidance and to provide users with detailed information. Hence, the user can seek clarification, ask follow-up questions, and receive personalized explanations to facilitate a deeper understanding of complex medical understanding.






\section{Materials and Methods}

In the following sections, we describe the participants, apparatus, study procedure, and study design. 
Our research questions include the following:
\begin{itemize}
    \setlength\itemsep{0em}
    \item [\textbf{RQ1}] How do configurations between \textit{avatar} and \textit{screen}-based virtual assistant influence the user performance for anatomy education?
    
    \item [\textbf{RQ2}] To what extent do subjective measures, such as usability, task load, and presence associate with configurations of the virtual assistant?
    
    \item [\textbf{RQ3}] What are the benefits, limitations, and potential research directions of using generative AI for anatomy education?
 
\end{itemize}

% Figure environment removed

\subsection{Participants}
%%\textcolor{red}{-- Rommy }

A priori power analysis was conducted to analyze the sample size with \textit{Analysis of Variance} (ANOVA) for interaction effects (repeated measures, within factors). We used a \textit{G*Power} statistical analysis software to calculate the effect size $\eta_{p}^{2}= 0.14$ for two factors, resulting in a sample size of 16~\cite{faul2007g}.

During participant recruitment, we had two pre-screening phases. %The first phase was included in the pre-questionnaire that participants had to fill out before scheduling a time to participate in the study. 
The first phase required all participants to fill out a screening questionnaire prior to scheduling a time to meet.
During this phase, participants were excluded if they self-reported vision or motion sickness concerns that would prevent them from interacting in the VR environment or were under 18. 
The second phase was a part of the training before the study began. During this phase, participants were excluded if they found themselves incapable of interacting with the environment or uncomfortable being in VR. 
20 participants registered for the study. Four participants were excluded: three were excluded during the first pre-screening phase (one reported vision issues and two reported motion sickness issues), and another had severe discomfort during the training in the VR environment. 
A total of 16 participants successfully participated in this study, and their demographic information is listed in \ref{tab:participants}.





\begin{table}[t]
%\centering
\caption{Participant background and characteristics ($n=16$).}
\label{tab:participants}
% \scriptsize
\resizebox{\columnwidth}{!}{
\begin{tabular}{p{0.5\columnwidth}p{0.2\columnwidth}p{0.2\columnwidth}} %p{0.3\columnwidth}p{0.1\columnwidth}p{0.1\columnwidth}} %p{0.5\columnwidth}p{0.2\columnwidth}p{0.15\columnwidth}
\hline
Characteristics  & Value  & Mean \\
\hline
Age   & [20 -- 35] & 26\,$\pm$\,4.97 \\
Gender                    &    &    \\
\hspace*{0.3cm} Male      & 6 & (37.50\%) \\
\hspace*{0.3cm} Female    & 10 & (62.50\%) \\
Education                &   &      \\
\hspace*{0.3cm} Bachelor's program  & 6 & (37.50\%) \\
\hspace*{0.3cm} Master's program & 3 & (18.75\%) \\
\hspace*{0.3cm} Doctoral program & 7 & (43.75\%) \\
% Video game experience    &   &      \\
% \hspace*{0.3cm}None      & 4 & (25.00\%) \\
% \hspace*{0.3cm}Several times a year & 7 & (43.75\%) \\
% \hspace*{0.3cm}Several times a week & 3 & (18.75\%) \\
% \hspace*{0.3cm}Daily     & 2 & (12.50\%) \\
Medical anatomy knowledge           &    &     \\
\hspace*{0.3cm}Not much   & 5 & (12.50\%) \\
\hspace*{0.3cm}Basic & 11 & (87.50\%) \\
Health centered classes           &    &     \\
\hspace*{0.3cm}No   & 8 & (50.00\%) \\
\hspace*{0.3cm}Yes  & 8 & (50.00\%) \\
VR experience            &    &     \\
\hspace*{0.3cm}Never used before   & 6 & (37.50\%) \\
\hspace*{0.3cm}Used a few times & 3 & (18.75\%) \\
\hspace*{0.3cm}Used several times   & 5 & (31.25\%) \\
\hspace*{0.3cm}Regular use   & 2 & (12.50\%) \\
Handedness           &    &     \\
\hspace*{0.3cm}Left   & 2 & (12.50\%) \\
\hspace*{0.3cm}Right   & 14 & (87.50\%) \\

\hline
\end{tabular}
}
\end{table}






\subsection{Apparatus}
%%\textcolor{red}{-- Vuthea}

An overview of the system architecture of the proposed VR environment for human anatomy education with generative AI virtual assistants is shown in \ref{fig:systemarchitecture}.
The VR environment was developed in \textit{Unity} game engine (version 2019.4.34f1). 
The 3D models of the living room and additional models from \textit{Sketchfab} were customized, and \textit{OpenHELP} organ models were used in the VR environment \cite{kenngott2015openhelp}. 
We used \textit{Valve Index} VR headset, controllers, lighthouses, and its components for VR setup.
\textit{Virtual Reality Toolkit} (VRTK) was used to develop basic interactions, including teleportation, grabbing objects, and interactions with user interface (UI).  
The UI for the VR quiz was developed with VR questionnaire toolkit~\cite{feick2020virtual}. 
We integrated \textit{ChatGPT} (OpenAI, USA) to provide services as an intelligent conversational agent for answering questions.
%For the virtual avatar presented in the user study, we used an AI-based library (Avatar SDK, Itseez3D Inc., USA) to create a photo-realistic model. 
We built the virtual avatar through an AI-Based library (Avatar SDK, Itseez3D Inc., USA) in order to simulate a photo-realistic model.
The virtual avatar was animated to provide gestures with facial expressions. Using Microsoft Azure Speech service to provide natural interaction and engagement, we used text-to-embodiment capabilities for text-to-speech and speech-to-text.


\subsection{Study Procedure}


An overview of the study procedure is shown in \ref{fig:studyprocedure}.
To account for learning curves, the conditions' order and difficulty level was counterbalanced.

During the study, we ran through the particulars of the study with each participant. We also made sure they understood the consent form before asking them to sign it. 
The participants had a chance to become familiarized with the VR headset and controllers and learn how to interact with the virtual assistant in the training environment before they began the study. 
They learned how to ask questions to the virtual assistant by holding down a button on the controller while asking their question, then releasing it to allow the system to process and answer their question. They also learned the difference between how the avatar and the text screen would answer their question. 
Once they felt comfortable using the technology, we began the study. 

\paragraph{\textbf{Tasks}}

To start, the participant was placed in a virtual living room environment with a virtual cadaver, a screen with a question with multiple choice answers, and their starting virtual assistant. They were able to interact with their environment by teleporting around the room, moving the screens around, and moving the organs within the cadaver. Once they were ready, we asked them to select the ``Next'' button so that they could get their first question. They were told that they could ask the virtual assistant as many questions as they wanted to figure out the answer to the multiple-choice question and that there was no pressure in getting the correct answer. 
The only condition was that they could not read out the entire question directly to the virtual assistant. 
Once they had an answer, they selected it from the multiple-choice options and then clicked the ``Submit'' button. They were then asked another question with a different difficulty level in the same virtual assistant configuration.



Once they finished with their first virtual assistant configuration, they were asked to take a break and answer a short mid-questionnaire on a computer. Then, they repeated the process of answering two questions with varying difficulty levels with the other virtual assistant configuration. Once they were done, we again asked them to take a break and answer a post-questionnaire. 
Lastly, they were given the option to ask any questions they had and to give us any feedback.

% % Figure environment removed

% % Figure environment removed

\subsection{Study Design}

Our study was designed as a $2\times2$ within-subject experiment to assess two ways of interaction with the virtual assistant. % (2 \textit{configuration} $\times$ 2 \textit{level of difficulty}). 
During the study, each participant was assigned a virtual assistant variant order. They were asked to answer two questions of each difficulty level for each configuration of the virtual assistant. The opportunity to start with either the \textit{avatar} or the \textit{screen} was counterbalanced between all the participants. 
This was to avoid bias due to the learning effect and to ensure that there was an equal number of participants performing each order. 


\subsubsection{Independent Variables}

The configuration of the virtual assistant and difficulty level were defined as independent variables.

\paragraph{\textbf{Configuration}} There are two types of the generative AI virtual assistant presented in the virtual environment: \textit{avatar} and \textit{screen} (see also~\ref{fig:environment}). 

\begin{itemize}
    \setlength\itemsep{0em}
    \item \textit{Avatar}: a photo-realistic avatar with audio and lip synchronization integrated with \textit{Microsoft Azure} text-to-speech and generative AI services for answering questions.
    
    \item \textit{Screen}: a screen that displays texts responded to by generative AI services with the question asked by the participant.  
    
\end{itemize}

\paragraph{\textbf{Level of Difficulty}} There are two levels of difficulty: \textit{easy} and \textit{hard}. They are multiple-choice questions that describe scenarios of patient situations. The participant can interact and request support from the virtual assistant to solve each question. We categorize Q1 and Q2 as \textit{easy}, and Q3 and Q4 as \textit{hard}.      

\begin{itemize}
    \setlength\itemsep{0em}
\item[Q1]  A 50-year-old male presents with difficulty breathing and chest pain. Imaging reveals an occlusion of the left anterior descending (LAD) coronary artery. Which of the following areas of the heart is most likely affected by the occlusion? (a) left atrium, (b) right atrium, (c) left ventricle, and (d) right ventricle.
    
\item[Q2] A patient presents stabbing pain in his left chest that radiates to his left arm. Which of the following anatomical structures is most likely affected? (a) left lung, (b) left ventricle of the heart, (c) spleen, and (d) left kidney.
    
\item[Q3] A patient presents with a sharp pain in the right upper quadrant of the abdomen, which shifts to the right shoulder. He reports feeling nauseous and having a low-grade fever. Other symptoms can include nausea and vomiting. The liver is located in the upper right side of the abdomen and produces bile, which is stored in the gallbladder. The pancreas is located behind the stomach and produces digestive enzymes and hormones, while the stomach is located in the upper left side of the abdomen and is responsible for digesting food. Which of the following structures is most likely affected? (a) gallbladder, (b) liver, (c) pancreas, and (d) stomach.    
    
\item[Q4] A patient presents with weakness and tingling in their hands and feet, along with difficulty maintaining balance. On physical examination, they have decreased reflexes and muscle weakness, tingling, and numbness in the hands and feet, as well as difficulty maintaining balance. Which of the following structures is most likely affected? (a) brainstem, (b) frontal lobe, (c) spinal cord, and (d) peripheral nerves.
 
\end{itemize}

\subsubsection{Dependent Variables}

During the study, we recorded the participant's chosen answer, task completion time, and number of interactions with the virtual assistant as the dependent variables. All this data was automatically stored in a \textit{CSV} file for analysis. %The study took about 30 to 40 minutes per participant. 

\begin{itemize}
    \setlength\itemsep{0em}
    \item \textit{Task Completion Time}: time taken from when they started the question until they submitted the answer.
    
    \item \textit{Number of Interactions}: the number of times the participant 
    % interacts and
    requests information from the virtual assistant. 

    \item \textit{Score}: whether their answer to the multiple choice question was correct or incorrect.

\end{itemize}



\begin{table*}[!t]
\centering
\caption{Summary of descriptive results of user performance.}
\label{tab:descriptiveResults}
\begin{tabular}{lrrr}
\hline
Variable & Task Completion Time (s) & Number of Interactions & Score \\
\hline
Avatar & 159.23 (149.25) [26.38] & 5.96 (11.87) [2.10] & 0.59 (0.49) [0.08]  \\
\hspace*{0.2cm} Easy & 117.74 (72.61) [18.15] & 5.31 (11.22) [2.80] & 0.75 (0.44) [0.11] \\
\hspace*{0.2cm} Hard & 
200.72 (192.59) [48.14] & 6.62 (12.83) [3.20] & 0.43 (0.51) [0.12]\\
Screen & 159.06 (152.94) [27.03] & 3.65 (2.74) [0.48] & 0.50 (0.50) [0.09] \\
\hspace*{0.2cm} Easy & 177.50 (200.57) [50.14] & 4.31 (3.36) [0.84] & 0.56 (0.51) [0.12] \\
\hspace*{0.2cm} Hard & 140.62 (85.96) [21.49] & 3.00 (1.82) [0.45] & 0.43 (0.51) [0.12] \\
\hline
\end{tabular} 
\\
\raggedright
\hspace*{3.5cm} 
\textit{All entities are in the format: mean value (standard deviation) [standard error].}
\end{table*}

% Figure environment removed


\subsubsection{Questionnaires}

Besides the performance data, we collected data from standardized questionnaires as subjective measures. Those questionnaires were designed and answered using the \textit{Qualtrics} survey platform.

\begin{itemize}
    \setlength\itemsep{0em}
    \item \textit{Usability}: we assessed the usability using the system usability scale (SUS) questionnaire \cite{Brooke1995SUS}, which includes ten questions with a 5-point Likert-scale from \textit{strongly disagree} to \textit{strongly agree}.
    The final score from SUS questionnaire was determined to a range between 0\,--\,100 (0\,--\,50: not acceptable, 51\,--\,67: poor, 68: okay, 69\,--\,80: good, 81\,--\,100: excellent) \cite{bangor2009determining}. 

    \item \textit{Task Load}: we used the \textit{NASA TLX} questionnaire to assess the subjective task load of the participants \cite{hart2006nasa}. This questionnaire contains six questions regarding
    mental demand, physical demand, temporal demand, performance, effort, and frustration. 
    
    \item \textit{Presence}: we assessed the sense of presence in the virtual environment using  \textit{igroup presence questionnaire} (IPQ) \cite{schubert2001experience, schwind2019using}. This questionnaire has 14 questions classified as general presence, spatial presence, involvement, and experienced realism. It is a 7-point Likert scale ranging from \textit{strongly disagree} to \textit{strongly agree}.   

    

\end{itemize}

\subsubsection{Semi-structured Interviews}

After completion of all the tasks, we asked the participant to collect qualitative feedback with the following questions:

\begin{itemize}
    \setlength\itemsep{0em}
    \item What is your feedback on the VR environment and configurations of the virtual assistant?
    \item Do you have any questions or suggestions?
\end{itemize}

% \subsubsection{Data Analysis}
% %%\textcolor{red}{-- Megha \& Rommy \& Vuthea}

% \textit{RStudio} with R for statistical computing was used for data analysis.
% A two-way analysis of variance (ANOVA) was conducted for dependent variables.


\begin{table}[!t]
\centering
\caption{Summary of statistical results ($p<.05$).}
\label{tab:anovaresults}
\resizebox{\columnwidth}{!}{
\begin{tabular}{lllrrcr}
\hline
Variable  & DFn & DFd  & F & p & Sig. & $\eta_{p}^{2}$  \\
\hline
Task Completion Time & & & & & & \\
\hspace*{0.2cm} Configuration & 1 & 15 & 0.0001 & 0.992 &  & \textless 0.0001%3.42$e^{-07}$ 
\\
\hspace*{0.2cm} Difficulty & 1 & 15 & 3.409 & 0.084 &  & 0.0062 \\
\hspace*{0.2cm} Configuration * Difficulty & 1 & 15 & 3.242 & 0.091 &  & 0.0408 \\
Number of Interactions & & & & & & \\
\hspace*{0.2cm} Configuration & 1 & 15 & 0.751 & 0.399 &  & 0.0183 \\
\hspace*{0.2cm} Difficulty & 1 & 15 & 0.000 & 1.000 &  & 0.000 \\
\hspace*{0.2cm} Configuration * Difficulty & 1 & 15 & 4.472 & 0.051 &  & 0.0059 \\
Score & & & & & & \\
\hspace*{0.2cm} Configuration & 1 & 15 & 1.000 & 0.333 &  & 0.0142 \\
\hspace*{0.2cm} Difficulty & 1 & 15 & 4.622 & 0.046 & * & 0.0731 \\
\hspace*{0.2cm} Configuration * Difficulty & 1 & 15 & 0.4128 & 0.530 &  & 0.0142 \\
\hline
\end{tabular}
}
\end{table}








\section{Results}

We describe the results for both descriptive and statistical analysis of user performance, questionnaire results, and general feedback in the following sections.

\subsection{User Performance Results (RQ1)}

We used \textit{RStudio} with R for computing the statistical analysis with a two-way ANOVA for dependent variables.
A summary of descriptive results for objective measures of user performance is shown in \ref{tab:descriptiveResults} and \ref{fig:userperformance}. 
The results of statistical analysis are listed in \ref{tab:anovaresults}.  

\subsubsection{Task Completion Time (TCT)}

We found no significant differences between the \textit{configuration}, \textit{level of difficulty}, and their interaction effect on the task completion time. 
%For \textit{level of difficulty}, \textit{Easy} questions were on average faster than \textit{Hard} in the \textit{avatar} configuration, while in the \textit{screen} configuration, they were vice versa.
\textit{Easy} questions were on average faster than \textit{Hard} in the \textit{avatar}
configuration. While in the \textit{screen} configuration, \textit{easy} questions were on average slower than \textit{hard}.
The results could show that the participants spend more time interacting with the \textit{avatar} in the \textit{hard} level, while it took more time to complete the tasks on the \textit{screen}, e.g., reading texts, in the \textit{easy} level. 
Descriptive results for total completion time, however, show a very small difference between \textit{avatar}: 159.23\,s (\textit{SD} = 149.25) and \textit{screen}: 159.06\,s (\textit{SD} = 152.94) configuration.
It indicates that both configurations are comparable to assist the user in solving the tasks.


\subsubsection{Number of Interactions}

Descriptive results show that all tasks in the \textit{avatar} require more interactions and requests with the virtual assistant, especially in the \textit{hard} level. Notably, participants interacted less in the \textit{hard} level in the \textit{screen} configuration.       
However, there were no significant differences between the \textit{configuration}, \textit{level of difficulty}, and their interaction effect on the number of interactions. 
The results could show that the user needs more clarification with verbal communication with the virtual avatar compared to the display screen.  

\subsubsection{Score}

The score of solving questions in the \textit{avatar} configuration was on average, higher than the \textit{screen}. However, there was no significant difference between the configurations.
For \textit{level of difficulty}, we found a significant difference between them (\textit{F}(1, 15) = 4.622, \textbf{\textit{p} = 0.046}, $\eta_{p}^{2}$ = 0.0731). We further analyzed the data with the pairwise t-test, and the results show a statistically significant difference between the \textit{easy} and \textit{hard} questions (\textit{t} = -1.78, \textit{df} = 15, \textbf{\textit{p} = 0.04}) in the \textit{avatar} configuration (see \ref{fig:userperformance}). 
It could show the participants who answered the questions in the \textit{easy} level with the \textit{avatar} configuration got higher scores and performed faster than the \textit{hard} questions.       


% Figure environment removed

% Figure environment removed



\subsection{Questionnaire Results (RQ2)}
%%\textcolor{red}{-- Megha \& Rommy \& Vuthea}

In the following sections, we describe the subjective results from the questionnaires. 

\subsubsection{Usability}

The results of usability using SUS questionnaire show an average score for \textit{avatar} (\textit{M} = 75.16, \textit{SD} = 10.55) and \textit{screen} (\textit{M} = 76.72, \textit{SD} = 13.53) (see \ref{fig:sus_tlx}). 
There was no significant difference between the main effect and their interaction effect.
The scores of both configurations higher than 68 are above the average \cite{bangor2009determining}, which could show their potential benefits in terms of usability.


\subsubsection{Task Load}

The subjective task load was assessed using an unweighted (raw) NASA-TLX questionnaire.  
Descriptive results are shown in \ref{fig:sus_tlx}.
We found no significant differences for all task load items between the configurations (i.e., mental demand, physical demand, temporal demand, performance, effort, and frustration). 
It can be seen that the highest scoring of them was mental demand: \textit{avatar} (\textit{M} = 43.75, \textit{SD} = 24.32), \textit{screen} (\textit{M} = 44.06, \textit{SD} = 24.71) closely followed by effort: \textit{avatar} (\textit{M} = 35.31, \textit{SD} = 18.83), \textit{screen} (\textit{M} = 37.50, \textit{SD} = 20.00). In contrast, physical demand: \textit{avatar} (\textit{M} = 14.06, \textit{SD} = 15.40), \textit{screen} (\textit{M} = 13.75, \textit{SD} = 12.04) was the lowest scoring.  
It could indicate that the proposed environment could impact the mental demand while the participant tried to interact with the virtual assistant to solve the questions. 
%%\textcolor{red}{Describe more on its items.}

\subsubsection{Presence}

The sense of presence in the immersive VR environment was assessed using an IPQ questionnaire. 
There were no significant differences among the configurations. Descriptive results show the average score for general presence: \textit{avatar} (\textit{M} = 5.25, \textit{SD} = 1.00), \textit{screen} (\textit{M} = 5.50, \textit{SD} = 0.89), 
spatial presence: \textit{avatar} (\textit{M} = 3.95, \textit{SD} = 0.36), \textit{screen} (\textit{M} = 3.86, \textit{SD} = 0.37), 
involvement: \textit{avatar} (\textit{M} = 4.27, \textit{SD} = 0.32), \textit{screen} (\textit{M} = 4.34, \textit{SD} = 0.35) , 
and experienced realism: \textit{avatar} (\textit{M} = 4.62, \textit{SD} = 0.46), \textit{screen} (\textit{M} = 4.70, \textit{SD} = 0.41)). 
For descriptive results, it could show that the general presence and experienced realism had the highest score, followed by involvement and spatial presence (see \ref{fig:ipq}).


\subsection{Qualitative Participant Feedback (RQ3)}

% \paragraph{Interacting with the Virtual Assistant}
All of the participants were %positive and 
able to interact with both configurations of the virtual assistant. 
Most participants found the \textit{avatar} was easy to interact with, while the \textit{screen} could provide a better clarification of the response. 
%understand ChatGPT's response on the screen and thought it was as easy to interact with as the avatar. 
%Most participants found the avatar easy to understand but even the two that didn't found it just as easy to interact with the avatar as the screen. Only one participant is an outlier in finding both just as easy to understand but found the avatar harder to interact with than the screen.

%\paragraph{Question Difficulty}

When asked, neither participants that had the \textit{screen} first or \textit{avatar} first had a consensus that a certain question was the most challenging. 
Instead, half the participants found all four questions challenging.
% When we look at the responses comparing the two configurations, however, within the \textit{screen}, half the participants found the first question harder, whereas within the avatar scenario half the participants found the two questions equal in difficulty.
%half the participants found the first question harder, whereas within the avatar scenario half the participants found the two questions equal in difficulty. 

%\subsubsection{Open Ended Feedback}
We had three participants give us open-ended feedback. Two of them commented on the difference between the two configurations. They both preferred the \textit{avatar} to the screen due to feeling more immersed. 
However, they commented that there were problems with the avatar's voice synchronization. One said that it was slow, the other said that it would sometimes have interference or be cut off. 
The third participant commented on the experience of interacting with the environment. They felt that they had to pre-plan their questions before asking because if they took their time asking the system would interrupt them with a response. They also mentioned that they would sometimes forget to let go of the talk button, and then the virtual assistant's response on the screen would be overridden by new text reacting to something they said.


\section{Discussion}

%Our main goal in this pilot study was to gather preliminary data on what things need to be considered when handling question answering in a virtual reality environment. 
The proposed VR environment was evaluated in a pilot study to gather preliminary data on what things need to be considered when handling question answering for human anatomy education in a VR environment.
%Our results show that at least for some more detail oriented questions the text screen virtual assistant was better able to communicate information. 
%This aligns with our original hypothesis since we expected some of ChatGPT's answers to be lengthy, resulting in being able to re-read sections being useful. While we could not see a clear user preference, we were able to gather insight into multiple other things to consider and what things to change going into a full study.
%%%%%%%%%%%%% 
%%%%% Discuss the results and correlate with the user feedback
% Though most of our results do not show significant findings, we do believe that they raise questions that can help set-up future studies. We posed three research questions when designing this study. 

%\paragraph{Effect of Virtual Assistant Configurations (RQ1)}

\textit{RQ1} concerned how the two virtual assistant configurations would impact user performance. We did find that participants scored significantly higher when answering the \textit{easy} as compared to \textit{hard} questions in the \textit{avatar} configuration. 
The fact that we found no significant difference in the \textit{screen} leads us to believe that participants might have had an easier time keeping track of the virtual assistant's answers for the \textit{hard} questions in the \textit{screen} than the \textit{avatar} configuration. 
This is corroborated by the fact that the participants had more interactions with the virtual assistant in the \textit{avatar} than the \textit{screen} with the \textit{hard} questions. Additionally, for user feedback, the participants did not have a clear preference for either the \textit{screen} or the \textit{avatar}. It could also indicate that they found benefits to both scenarios for different questions. As such, we need to gather more information on the benefits of the \textit{screen} compared to the \textit{avatar} configuration. Knowing what makes each helpful will allow us to combine the two configurations in a way that will prove best for user performance in anatomy education.

%\paragraph{Subjective Measures (RQ2)}

\textit{RQ2} pertained to the impact subjective measures had on the configuration of the virtual assistant. There was no clear significance regarding usability, task load, or presence. In general, it seemed like the participants were experiencing higher mental demand than physical demand. They also seemed to score higher for general presence than spatial presence.
These perceived preferences could be due to where the study took place, their experience level with virtual assistants and VR going in, or other things that could have kept them from forgetting their surroundings.
In terms of usability, both configurations could be classified as relatively easy to use. It could add to our belief that each configuration has clear potential benefits. Hence, combining both configurations could provide a full option for interacting with the environment.
%, and we need to gather more information on the screen scenario's benefits compared to the avatar scenario.


%\paragraph{Potential Benefits, Limitations, and Research Directions (RQ3)}

\textit{RQ3} concerned the limitations and potential research directions of using generative AI for anatomy education. 
Integrating generative AI virtual assistant into the virtual environment could adapt the user and provide personalized support \cite{sallam2023chatgpt}. 
Moreover, it could offer an engaging, immersive, and interactive learning experience, enhancing the learning process and motivation.
The generative AI-based virtual assistant can query a vast database of information and provide comprehensive information and resources according to the student's needs. 
However, the quality and accuracy of the responses could be another topic needed for further evaluation.


\paragraph{Limitations}
Our pilot study was conducted with 16 participants, which is still considered a small sample size.
Future work aims to conduct an extensive study with a high sample size and target the medical professions.
During the study, we noticed that the way a participant phrased a question could impact the response they received. 
It correlates with other studies showing that virtual assistant could be bias, particularly when asked to find a relationship between items \cite{bang2023multitask, alkaissi2023artificial}.
There are also concerns regarding the generation of inaccurate content, the lack of transparency and unclear information of the source of data used, and other related consequences \cite{sallam2023chatgpt}.
Future work should investigate the transcripts of the conversations and the accuracy of responses. 
Regarding the verbal conversation using speech-to-text, some of our participants had problems with being understood. It was a problem with participants with an accent and when they spoke too quickly. As a result, both these factors could have affected the participants' correctness scores and experience.

The participants' varying experience levels using VR and virtual assistant, e.g., \textit{ChatGPT}, were limitations. It was very clear from watching the participants who had used VR before versus the ones who had minimal to no experience that there was an effect on confidence and immersion. Participants with little experience were less likely to interact with the environment and look around, which we believe impacted how different they felt the two configurations were. Additionally, all but two participants treated the interactions like they would a search engine instead of taking advantage of the more conversational dialogue that the virtual assistant can maintain. We believe this is due to their inexperience with the generative AI and could have impacted their immersion and experience.


\paragraph{Future Work}
For future work, it would be interesting to further develop and integrate with visual input and output, e.g., similar to \textit{ChatGPT-4}, to provide an extensive learning environment and experience. Furthermore, providing tools to monitor learning progress and assessments could be beneficial. 
The avatar of the virtual assistant can be improved by incorporating advanced techniques such as visual/acoustic emotion recognition \cite{hartholt2019ubiquitous, canal2022survey}, gaze engagement tracking \cite{guo2023social}, and body gesture analysis \cite{bhattacharya2021text2gestures, li2022pose}. 
Additionally, it could be beneficial to study the learning effect with a collaborative VR environment, which allows multiple users to join in the same shared virtual environment \cite{scavarelli2021virtual, chheang2022towards, PREIM2023403}.


\section{Conclusion}

We have presented an immersive VR environment with a generative AI-based virtual assistant for human anatomy education.
The proposed environment was evaluated in a pilot user study (n = 16). 
The evaluation results show the effect of virtual assistant configurations influenced user performance. There were small differences between the \textit{avatar} and \textit{screen}-based virtual assistant in terms of task completion time and number of interactions. There was a significant difference between the difficulty level in the \textit{avatar}-based virtual assistant.  
Moreover, subjective measures were reported, including usability, task load, and sense of presence. 
Combining both configurations of virtual assistants could provide a comprehensive option to assist and enhance the learning experience. 
Additionally, the results provide insights into potential benefits, limitations, and research directions of using generative AI for education. 
The proposed environment offers interactive, adaptive, and informative learning for training and education.


%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}

% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.


%%
%% If your work has an appendix, this is the place to put it.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,bibfile}

\end{document}

%\textcolor{blue}{The results of this experiment indicate that participants achieved higher scores on knowledge-based questions when the cognitive complexity was low. This suggests that individuals are more effective in utilizing tools such as ChatGPT to obtain direct answers when the cognitive demands are less demanding. 
%%The reason for this phenomenon could potentially be attributed to the fact that knowledge-based inquiries typically possess explicit and tangible solutions, hence facilitating the capacity of a system such as ChatGPT to provide precise and expeditious replies.
%On the contrary, problems that involve analysis require deeper cognitive engagement, interpretation, and multifaceted comprehension and may not be readily resolved even with the aid of ChatGPT. 
%%Social facilitation/inhibition refers to performance enhancement of a simple or well-learned task, and performance impairment of a complex or novel task when completed in the presence of others. 
%Questions of this nature frequently require reasoning abilities resembling those of humans, nuanced judgment, and perhaps subjective interpretation. This observation underscores the significant correlation between the cognitive complexity of a question, as categorized according to Bloom's taxonomy, and the effectiveness of virtual educational technologies such as ChatGPT in supporting learners.}

%\textcolor{blue}{The result suggests that avatars may boost user engagement by providing a more relatable or familiar interface, as seen by the difference in interaction frequency between the avatar-using condition and the direct ChatGPT interface. Avatars' visual and audible presentations mayminimize cognitive load and improve information processing.}
%\textcolor{blue}{The difference in interaction frequency between the avatar-using condition and the direct ChatGPT interface suggests avatars can affect participant engagement and involvement by providing a more relatable or familiar interface, which may explain the reported increase in user engagement. Avatars can also reduce cognitive load by presenting information in visual and audio formats, making information processing more efficient.}
%This suggests how users interact with virtual surroundings, including avatars, might affect their experience and engagement. This also allows further research into optimizing these aspects to improve virtual learning and interactions.}