\section{An efficient variant of \alg for large action spaces}
\label{sec:appendix_distributed}
\subsection{The Distributed \alg (\subalg) algorithm}

For ease of readability in the pseudocode we pull-out some key functionality of the MW algorithm, which we put into a class called \mwu described in \cref{alg:mwu}. Each agent is an instance of this class, i.e., each agent maintains its own set of weights over its actions, and updates to these instances are coordinated by \subalg as described in \cref{alg:subalg}. Conceptually it can be thought of as $m$ instances of \alg, where the action spaces of other agents are absorbed into $\calA'$. While for presenting the algorithm we always decompose $\A$ as $\A_1 \times \dots \times \A_m$ and have each agent control the intervention on one node, one could in practice choose to decompose the action space in a different way. A simple example would be having agent one control $\A_1 \times \A_2$ and thus designing the intervention for both $X_1, X_2$

When computing the $\ucb$ for a single agent $i \in [m]$, we will find the following notation convenient. Let $\ait \in \A_{i}$ be the action chosen by agent $i$ at time $t$. $\a_{-i, t} \in \A_{-i} \subseteq \A$ are the actions chosen at time $t$ by all agents except agent $i$. Note that since the subspaces of the action space each agent controls are non-overlapping, $\A = \Ai \cup \A_{-i}$. When agent $i$ chooses actions in $\A_i$, for convenience we will from now on represent it as a vector in $\calA$ with $0$ at all indexes the agent cannot intervene. We do similarly for $\a_{-i,t}$. Then we use the notation $\ucb_t(\ait, \a_{-i,t}, \a_t') = \ucb_t(\ait + \a_{-j, t}, \a_t')$. 

% Figure environment removed

\begin{algorithm}[t]
    \caption{Multiplicative Weights Update (\mwu)}
    \label{alg:mwu}
    \begin{algorithmic}[1]  
        \Require set $\calA_i$ where $\abs{\calA_i} = K_i$, learning rate $\eta$
        \State Initialize $\bw^1 = \frac{1}{K_i}(1, \ldots, 1) \in \mathbb{R}^{K_i}$
        \Function{play\_action}{}
            \State $\bm{p} = \bm{w} \cdot 1/\left(\sum_{j=1}^{K_i} \bm{w}[j]\right)$ 
            \State $a \sim \bf{p}$
            \State \Return a \Comment{sample action}
        \EndFunction \\
        \Function{update}{$f(\cdot)$}
            \State $\bm{f} = \min(\bm{1}, [f(a)]_{a\in\calA_i}) \in \mathbb{R}^K$  \Comment{rewards vector}
            \State $\bf{w} = \bf{w} \cdot \exp{(\eta \bm{f})}$ \Comment{MW update}
            \State \Return
        \EndFunction
        
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
    \caption{Distributed Causal Bayesian Optimization Multiplicative Weights (\subalg)}
    \label{alg:subalg}
    \begin{algorithmic}[1]  
        \Require parameters $\tau, \{\beta_{t}\}_{t\geq1}, \G, \snoiserv$, kernel functions $k_{i}$ and prior means $\mu_{i,0} = 0$, $\forall i \in [m]$ 
        \State $\text{Algo}^i \leftarrow \mwu(\calA^i), \ i = 1, \dots , m$, \cref{alg:mwu} \ \Comment{initialize agents}
        \For{$t = 1, 2, \hdots$}
            \State $\text{Algo}_i.\text{PLAY\_ACTION}(),\  i = 1, \dots, m$ \Comment{sample actions and perform interventions}
            \State Observe samples $\{\zit, \sit, \ait'\}_{i=0}^m$
           \State Update posteriors $\{\mu_{i,t}(\cdot), \sigma^2_{i,t}(\cdot) \}_{i=0}^m$ 
           \For{$i \in 1, \dots, m$}
            \For{$a_i \in \calA_i$}
                \State Compute $\text{ucb}_{i, t}(\ai) = \ucb_t(\ai, \a_{-i, t}, \a_t')$ for $\ai \in \calA_i$  using \cref{alg:oracle}
            \EndFor
            \State $\text{Algo}_i.\text{UPDATE}(\text{ucb}_{i, t}(\cdot)), i= 1, \dots, m$
        \EndFor
        
        \EndFor 
    \end{algorithmic}
\end{algorithm}



\subsection{Approximation guarantees for monotone submodular rewards}

In this section we show that \subalg enjoys provable approximation guarantees in case of monotone and DR-submodular rewards. Both such notions are defined below. \looseness=-1

\begin{definition}[Monotone Function]
A function $f:\calA \subseteq \mathbb{R}^m \rightarrow \mathbb{R}$ is monotone if for $\bm{x} \leq \bm{y}$,
$$f(\bm{x}) \leq f(\bm{y}).$$
\end{definition}


\begin{definition}[DR-Submodularity, \citep{bian2017continuous}]
A function $f:\calA \subseteq \mathbb{R}^m \rightarrow \mathbb{R}$ is DR-submodular if, for all $\bm{x} \leq \bm{y}\in \calA$, $\forall i \in [m], \forall k \geq 0$ such that $(\bm{x} + k \bm{e}_i)$ and $(\bm{y} + k \bm{e}_i) \in \calA$, 
$$f(\bm{x} + k \bm{e}_i) - f(\bm{x}) \geq f(\bm{y} + k \bm{e}_i) - f(\bm{y}).$$
\end{definition}

DR-submodularity is a generalization of the more common notion of a submodular set function to continuous domains \cite{bach2019submodular}. 
For our analysis, in particular, we assume that for every $\at' \in \calA'$, the reward function $r(\at, \at')$ is monotone DR-submodular in $\at$.

We consider ACBO as a game played among all the distributed agents and the adversary.  Our guarantees are then based on the results of~\citet{sessa21online} and depend on the following notions of average and worst-case game curvature. 

\begin{definition}[Game Curvature]
    Consider a sequence of adversary actions $\a'_1, \dots, \a'_T$. We define the average and worst-case game curvature as
  $$c_{avg}(\{\at'\}_{t=1}^T) = 1 - \inf_i \frac{\sum_{t=1}^T [\nabla r(2 \a_{max}, \at')]_i}{\sum_{t=1}^T [\nabla r(\bm{0}, \at')]_i} \in [0,1],$$
    $$c_{wc}(\{\at'\}_{t=1}^T) = 1 - \inf_{t,i} \frac{[\nabla r(2 \a_{max} , \at')]_i}{[\nabla r(\bm{0}, \at')]_i} \in [0,1],$$
    where $\a_{max} = a_{max} \bm 1$ and $a_{max}$ is the largest intervention value a single agent can assign at any index. If $2 \a_{max}$ is outside the domain of $r$, then the definition can be applied to a monotone extension of $r$ over $\mathbb{R}^m$. A definition of game curvature for non-differentiable $r$ is given in the appendix of \citet{sessa21online}.  \looseness-1

\end{definition}

Curvature measures how close $r(\cdot, \a'_{:, t})$ is to being linear in the agents' actions, with $c_{avg} = c_{wc} = 0$ coinciding with a completely linear function. The closer $r(\cdot, \a'_{:, t})$ is to linear, generally the easier the function is to optimize in a distributed way, because a linear function is separable in its inputs. $c_{wc}$ is the worst-case curvature of $r(\cdot, \a'_{:, t})$ across all rounds $t$, while $c_{avg}$ is the average curvature across rounds. The curvature of $r(\cdot, \a'_{:, t})$ will change with $t$ because $\a'_{:, t}$ will change across rounds. 


We will without loss of generality assume that $r(\mathbf{0}, \a') = 0$ for all $\a'$. If this did not hold then $r(\mathbf{0}, \a')$ could simply be subtracted from all observations to make it true. 
Then, we can show the following. \looseness-1

\begin{theorem}
    \label{thm:2}
    Consider the setting of \cref{thm:1} but with the additional monotone submodularity and curvature assumptions made in this section. Assume $\abs{\calA_i} = K$ for all $i$. Then, if actions are played according to \subalg with $\beta_t = \mathcal{O}\left( \calB +\sqrt{\gamma_{t-1} + \log{(m/\delta)}}\right)$ and $\eta= \sqrt{8 \log(K)/T}$ then with probability at least $1-\delta$,
    \begin{align}
        \sum_{t=1}^T r(\a_t, \a_t') \geq \: & \alpha \cdot \textsc{OPT} 
        - m \cdot \mathcal{O}\left(\left(\calB + \sqrt{\gamma_T + \log(m/\delta)} \right)^{N+1} \degree^N L_{\sigma}^N L_f^N m \sqrt{T \gamma_T} \right) \\
        & - m \cdot\mathcal{O}\left(\sqrt{T\log{K}} + \sqrt{T\log(2m/\delta)}\right), \nonumber
    \end{align}
    with 
    $$\alpha = \max \left\{1 - c_{avg}(\{\at'\}_{t=1}^T), \left(1+c_{wc}(\{\at'\}_{t=1}^T)\right)^{-1} \right\}$$ 
    and $\textsc{OPT} = \max_{\a \in \calA} \sum_{t=1}^T r(\at, \at')$ is the expected reward achieved by playing the best fixed interventions in hindsight. $\calB$ and $\gamma_T$ are defined the same as in \cref{thm:1}.
\end{theorem}
\begin{proof}

We will find useful the notation of the regret of an individual agent $i\in[m]$. We will consider the regret of each agent to be not in terms of the reward $r$ but in terms of the feedback that agent receives: the UCB. We therefore define 
$$R^i(T) = \max_{a} \sum_{t=1}^T \ucb_t(a, \a_{-i, t}, \a'_t) - \sum_{t=1}^T\ucb_t(\ait, \a_{-i, t}, \a'_t).$$
It can be thought of as the regret of agent $i$ compared to the best fixed action in hindsight, given that the actions of all other agents are fixed, and the agent is trying to maximize the sum of UCBs. 

Using the above definitions, and provided that $\ucb_t(\cdot)$ are a valid upper confidence bound functions on the reward (according to \cref{lem:rew_confidence}), 
%a sequence of $\{\beta_t \}_{t=1}^T$ chosen according to \cref{lem:node_confidence}, and applying \cref{alg:mwu} with UCBs as the update target (like in \cref{alg:subalg}), 
we can directly apply \citep[Theorem~1]{sessa21online}. This shows that with probability $1-\frac{\delta}{2}$, the overall reward obtained by \subalg is lower bounded by 
$$\sum_{t=1}^T r(\a_t, \a_t') \geq \alpha \cdot \textsc{OPT} 
        - m \sum_{t=1}^T C_t\left(\frac{\delta}{2}\right)
        - \sum_{i=1}^m R^i(T),$$
where $\alpha$ is defined in~\Cref{thm:2} and $C_t(\delta/2)$ is as defined in \cref{lem:rew_confidence}. We note that \citet{sessa21online} stated their theorem for the case where the UCB was computed using a single GP model (our setting with only a reward node and parent actions), however the proof is identical when any method to compute the UCB is used with an accompanying confidence bound in the form of \cref{lem:rew_confidence}. 

Then, we can obtain the bound in the theorem statement by further bounding the agents' regrets $R^i(T)$. 
Indeed, because each agent updates its strategy (Line~10 in \cref{alg:subalg}) using the MW rule (\cref{alg:mwu}), we can bound each $R^i(T)$ with probability $1-\frac{\delta}{2m}$ using \cref{eq:event2}. We can also substitute in for $C_t(\delta/2)$ using \cref{lem:rew_confidence}. Via a union bound we get our final result with probability $1-\delta$.\looseness=-1 
\end{proof}


