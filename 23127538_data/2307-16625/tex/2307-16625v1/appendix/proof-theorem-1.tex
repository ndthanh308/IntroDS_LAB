\subsection{Proof of \cref{thm:1}}
\label{app:thm-1-proof}

We start by proving that there exists a sequence of $\beta_t$ that allow \cref{as:well_calibrated_model} to hold. 
\begin{lemma}[Node Confidence Lemma ]\label{lem:node_confidence}

Let $\mathcal{H}_{k_i}$ be a RKHS with underlying kernel function $k_i$. Consider an unknown function $f_i:\calZ_i \times \calA_i' \times \A_i' \rightarrow \calX_i$ in $\mathcal{H}_{k_i}$ such that $\| f\|_{k_i} \leq \calB_i$, and the sampling model $\sit = f(\zit, \ait, \ait') + \noise_t$ where $\noise_t$ is $\noisescale$-sub-Gaussian (with independence between times). By setting 
%$$ \beta_t = B + \frac{\rho}{\sqrt{d}}\sqrt{2 \left(\gamma_{t-1} + \log(2m/\delta) \right)} \, ,$$ 
$$ \beta_t = \calB_i + \noisescale\sqrt{2 \left(\gamma_{t-1} + \log(m/\delta) \right)} \, ,$$
the following holds with probability at least $1-\delta$: 
$$  | \mu_{t-1}(\zi, \ai, ai') - f_i(\zi, \ai, \ai') | \leq \beta_t \sigma_{i, t-1}(\zi, \ai, \ai') \: ,$$

$\forall \zi, \ai, \ai' \in \calZ_i' \times \calA_i \times  \calA_i', \quad \forall t \geq 1, \quad \forall i \in [m], \quad$
where $\mu_{t-1}(\cdot)$ and $\sigma_{t-1}(\cdot)$ are given in \cref{eq:mean_update}, \cref{eq:sigma_update} and $\gamma_{t-1}$ is the maximum information gain defined in~\eqref{mutual_info}.
\end{lemma}
\begin{proof}
Lemma~\ref{lem:node_confidence} follows directly from \citet[Lemma 1]{sessa2019no}
%\citet[Lemma 10]{chowdhury2019online} (concentration of mean transition function) 
after applying a union bound so that the statement holds for all $m$ GP models in our system.
\end{proof}
Now we give an upper and lower confidence bound for $r$ at each $t$. Note that since in our setup all $\omega_i$ are assumed bounded in $[0, 1]$, we can use $\noisescale=\frac{1}{4}$.

\begin{lemma}[Reward Confidence Lemma]\label{lem:rew_confidence}
Choose $\delta \in [0, 1]$ and then choose the sequence $\{\beta_t \}^T_{t=1}$ according to \cref{lem:node_confidence}.

$\forall \a , \ai \in \calA \times \calA'$ , $\forall t$ we have with probability $1-\delta$

$$\ucb_t(a, a') - C_t(\delta) \leq r(a, a') \leq \ucb_t(a, a_t')$$

where we define $C_t(\delta)$ as 
$$C_t(\delta) = L_{Y, t} \degree^N  \sqrt{m \E[\snoise]{ \sum_{i=0}^m \norm{\sigma_{i, t-1} (z_{i, t}, a_{i, t}, a_{i,t}')}^2 }} \, .$$

We define $L_{Y, t}=2\beta_t (1+L_f+2\beta_tL_{\sigma})^N$.
\end{lemma}
\begin{proof}
The RHS follows firstly from  \cref{as:well_calibrated_model}
meaning that with probability at least $1-\delta$, $f \in \{\tilde{f}_t\}$. The RHS then follows directly from the definition of $\ucb$ in \cref{eq:cucb}. 

The LHS follows directly from \citet[Lemma 4]{sussex2022model}. The addition of the adversary actions does not change the analysis in this lemma because for a given $t$, $\a_t'$ is fixed for both the true model $f$ and the model in $\{\tilde{f} \}$ that leads to the upper confidence bound. 
\end{proof}

Now we can prove the main theorem. Choose $\delta\in [0, 1]$ and then choose the sequence $\{\beta_t \}^T_{t=1}$ according to \cref{lem:node_confidence} so that \cref{as:well_calibrated_model} holds with probability $1-\frac{\delta}{2}$. First recall that regret is defined as

$$R(T) = \sum_{t=1}^T r(\bar{\a}, \a_t') -  \sum_{t=1}^T r(\a_t, \a_t')$$

where $\bar{\a} = \argmax \sum_{t=1}^T r(\bar{\a}, \a_t')$. Now we can say that with probability at least $1-\frac{\delta}{2}$

\begin{align}
R(T) &\leq \sum_{t=1}^T \min\{1, \ucb_t(\bar{\a}, \a_t')\} - \sum_{t=1}^T \left[ \ucb_t(\a_t, \a_t') - C_t \left( \delta/2 \right) \right]\\
&\leq \sum_{t=1}^T \min\{1, \ucb_t(\bar{\a}, \a_t')\} - \sum_{t=1}^T \ucb_t(\a_t, \a_t') +  \sum_{t=1}^T C_t\left(\delta/2\right) \label{eq:3_terms_ub}
\end{align}
where the first line follows from \cref{lem:rew_confidence}. 

Evaluating the last term we see

\begin{align}
    \sum_{t=1}^T C_t\left(\delta/2\right) 
    &\overset{\tiny \circled{1}}{=} \sqrt{T} \sqrt{\sum_{t=1}^T C_t\left(\delta/2\right)^2} \\
    &\overset{\tiny \circled{2}}{\leq} \sqrt{T} \sqrt{\sum_{t=1}^T L_{Y, t}^2 \degree^{2N} \E[\snoise]{ \sum_{i=0}^m \norm{\sigma_{i, t-1} (z_{i, t}, a_{i, t}, a_{i,t}')}^2}} \\
    &\overset{\tiny \circled{3}}{=} \calO \left( L_{Y, T} \degree^N \sqrt{T m \gamma_T} \right) .
\end{align}

$\tiny \circled{1}$ comes from AM-QM inequality and $\tiny \circled{2}$ comes from plugging in for $C_t(\frac{\delta}{2})$. Finally, $\tiny \circled{3}$ comes from $L_{T, t}$ being weakly increasing in $t$ and from using the same analysis as \cite[Theorem 1]{sussex2022model} to upper bound the sum of $\sigma$s with $\gamma_T$. 

Moreover, we can upper bound the first two terms of \cref{eq:3_terms_ub} using a standard regret bound for the MW update rule (Line~9 in \alg), e.g. from~\citep[Corollary~4.2]{cesa2006prediction}. Indeed, with probability $1-\frac{\delta}{2}$ it holds:

\begin{equation}
\sum_{t=1}^T \min\{1, \ucb(\bar{\a}, \a_t')\} - \sum_{t=1}^T \ucb(\a_t, \a_t') 
 = \calO \left( \sqrt{T \log \abs{\calA}} +  \sqrt{ T \log(2/\delta)} \right) . \label{eq:event2}
 \end{equation}


Using the union bound on the two different probabilistic events discussed so far (\cref{as:well_calibrated_model} and \cref{eq:event2}) we can say that with probability at least $1-\delta$

$$R(T) = \calO \left( \sqrt{T \log \abs{\calA}} +  \sqrt{ T \log(2/\delta)} + L_{Y, T} \degree^N \sqrt{T m \gamma_T} \right) \, . $$

Substituting in for $L_{Y, t}$ and $\beta_t$ gives the result of \cref{thm:1}.

\input{appendix/binary-tree-dag}

