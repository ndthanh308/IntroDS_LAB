\section{Experiments}
\label{app:experiments}

\subsection{Function Networks} 

We evaluate \alg on 8 environments that have been modified from existing function networks \cite{astudilloBayesianOptimizationFunction2021}. These have been previously used to study CBO algorithms \cite{sussex2022model}. We modify each environment in two ways (Perturb and Penny) in order to incorporate an adversary, resulting in 8 environments total. For all environments we tried to make the fewest modifications possible when incorporating the adversary in a way that made the game nontrivial while maintaining the spirit of the original function network.

Perturb environments allow the adversary to modify some of the input actions of the agents. Penny environments incorporate some element of the classic matching pennies game into the environment. If node $X_i$ has an adversary action parent, part of the mechanism for generating $X_i$ will involve multiplying another parent by the adversary action. Because the adversary can select negative actions (see below), this results in a dynamic similar to matching pennies. 

Throughout the setup and theory we assume that there is one action per node for simplicity. For many of the function networks experiments there may be $0$ or more than one action per node. Similar theoretical results for this case can also be shown using our analysis. 

In all environments we map the discrete action space $[0, K-1]$ to the continuous domain of the original function network. Using $a$ as the continuous action at a single node and $\da$ as the discrete action input at that node, we always use mapping $a = \left( \frac{\da}{K-1} - 0.5 \right) C_1 + C_2$, where $C_1, C_2$ defines some environment specific linear map that usually maps to the input space of the original function network in \citet{astudilloBayesianOptimizationFunction2021}. We use the same mapping for adversary actions in Perturb environments 

For the adversary's actions in Penny environments, we use a more complicated mapping from $\da'$ to $a'$ to ensure that the adversary normally cannot select $0$, which would result in a trivial game that the adversary can solve by always playing $\a' = \mathbf{0}$. If $K$ is even we use
$$a' = \left( \frac{\da'}{K-1} (1-2\epsilon) \epsilon - 0.5 \right) C_1 + C_2 ,$$
where $\epsilon = 0.05$. If $K$ is odd we use
$$a' = \left( \frac{\da'+0.5}{K-1} (1-2\epsilon)  \epsilon - 0.5 \right) C_1 + C_2,$$
where $\epsilon = 0.05$.

DAGs illustrating the casual structure of each environment are shown in \cref{fig:task_dags}. For some environments, we made the number of nodes smaller than the environment's counterpart in \citet{astudilloBayesianOptimizationFunction2021} for computational reasons. We give the full SCM for each environment at the end of this subsection. 

To select hyperparamaters for each method we perform a small hyperparameter sweep to select the best hyperparameters, before fixing the best hyperparameters and running 10 repeats on fresh seeds. On all repeats the agent is initialized with $2m+1$ samples of uniformly randomly sampled $\a$ and $\a'$, where $m$ is the number of action nodes. 

\input{appendix/task_dags}

\input{appendix/other_fn_plots}

\cref{fig:app_fn} shows the regret plots for environments not already shown in the main paper. As discussed, we find that standard non-adversarial BO algorithms often fail to achieve sublinear regret and that \alg is often the most sample efficient compared to \gpmw. Only on Ackley-Penny is \gpmw obtaining a lower regret. This can be understood by our theory. Ackley-Penny is not at all sparse. That is, $\degree \approx m$. Our \cref{thm:1} suggests that most improvement will come in high dimensional settings with sparse graphs. This is made clear by the superior performance of \alg on the Alpine2 environments. 

Here we systematically list the SCM for each environment. 

\paragraph{Dropwave-Penny} $\a \in [0, 2]^2$, $\a'\in[-1, 1]^1$. We have
$$X_0 = \norm{\a},$$
$$Y = \frac{\cos(3X_0)}{2+0.5X_0^2}a_0'.$$
This is the original Dropwave from \citet{astudilloBayesianOptimizationFunction2021} but with a modified input space and a matching pennies dynamic on the final node. 

\paragraph{Dropwave-Perturb} $\a \in [-10.24, 10.24]^2$, $\a'\in[-10.24/5, 10.24/5]^1$. Like many Perturb systems, the adversary has a smaller domain than the agent to prevent it from being too strong. We have
$$X_0 = \norm{\left(\begin{bmatrix}
           a_0 - a_0' \\
           a_1 
         \end{bmatrix} 
         \right)},$$

$$Y = \frac{\cos(3X_0)}{2+0.5X_0^2}.$$
This is the original Dropwave from \citet{astudilloBayesianOptimizationFunction2021} but with one of the actions being perturbed by the adversary. 

\paragraph{Alpine-Penny} $\a \in [0, 10]^4$, $\a'\in[1, 11]^1$. We have
$$X_0 = -\sqrt{a_0}\sin (a_0)$$
For nodes $X_i$ with an adversary parent we have
$$X_i = -\sqrt{a_i'}\sin (a_i') X_{i-1},$$
and for nodes influenced by the agent we have
$$X_i = -\sqrt{a_i}\sin (a_i) X_{i-1}.$$
This is the original Alpine2 from \citet{astudilloBayesianOptimizationFunction2021} but with an adversary controlling one of the nodes instead of the agent. We shift that adversary's action space so that they cannot $0$ the output with a fixed action. 

\paragraph{Alpine-Perturb} $\a \in [0, 10]^4$, $\a'\in[0, 2]^3$. Let $\bar{a}_i = a_i + a_i'$ for $i$ when $X_i$ has an adversarial action input, and $\bar{a}_i = a_i$ otherwise. We have
$$X_0 = -\sqrt{\bar{a}_0}\sin (\bar{a}_0),$$
$$X_i = -\sqrt{\bar{a}_i'}\sin (\bar{a}_i) X_{i-1}.$$

\paragraph{Rosenbrock-Penny} $\a \in [0, 1]^4$, $\a'\in[0,1]^2$. We have
$$X_0 = -100 (a_1-a_0^2)^2 - (1-a_0)^2 + 10,$$
$$X_i = \left( -100 (a_{i+1}-a_{i}^2)^2 - (1-a_i)^2 + 10 + X_{i-1} \right) \bar{a}_i',$$
where $\bar{a}_i' = 1$ if there is no adversary over node $i$ and otherwise $\bar{a}_i' = a_i'. $

\paragraph{Rosenbrock-Perturb} $\a \in [-2, 2]^4$, $\a'\in[-1,1]^2$. Let $\bar{a}_i = a_i + a_i'$ for $i$ when $X_i$ has an adversarial action input, and $\bar{a}_i = a_i$ otherwise. We have
$$X_0 = -100 (\bar{a}_1-\bar{a}_0^2)^2 - (1-\bar{a}_0)^2 + 10,$$
$$X_i = -100 (\bar{a}_{i+1}-\bar{a}_{i}^2)^2 - (1-\bar{a}_i)^2 + 10 + X_{i-1}.$$

\paragraph{Ackley-Penny} $\a \in [-2, 2]^4$, $\a'\in[-1, 1]^1$. Let $\bar{a}_i = a_i + a_i'$ for $i$ when $X_i$ has an adversarial action input, and $\bar{a}_i = a_i$ otherwise. We have
$$X_0 = \frac{1}{4} \sum_i \bar{a}_i^2,$$
$$X_1 = \frac{1}{4} \sum_i \cos(2 \pi \bar{a}_i),$$
$$Y = 20 \exp(-0.2\sqrt{X_0}) + e^{X_1}.$$

\paragraph{Ackley-Perturb} $\a \in [-2, 2]^4$, $\a'\in[-1, 1]^2$. We have
$$X_0 = \frac{1}{4} \sum_i a_i^2,$$
$$X_1 = \frac{1}{4} \sum_i \cos(2 \pi a_i),$$
$$Y = 20 a_0' \exp(-0.2\sqrt{X_0}) + e^{X_1}.$$
This is the original Ackley from \citet{astudilloBayesianOptimizationFunction2021} but with a matching pennies dynamic on the final node. 


\subsection{Shared Mobility System} 

We use the same SMS simulator as \citet{sessa21online} and thus we largely refer to this regarding simulator details, unless otherwise specified. The simulator uses real demand data amalgamated from several SMS sources in Louisville Kentucky \cite{LouisvilleKentuckyOpen}. We treat the system as a single SMS where all transport units are identical. A single trip taken in the Louisville data at a specific time is treated as a single unit of demand in the simulator. The demand is fulfilled if the location of the demand (where the trip started in the dataset) is within a certain distance (0.8km Euclidean distance) of a depot containing at least one bike. If the demand for a trip is satisfied, a single trip starting from the depot is counted and the bike is instantaneously transported to the nearest depot to the end location of the trip in the dataset. The simulator's use of real trip data means that the geographical and temporal distribution of demand, and its relation to the weather, is realistic.

The depots are not in the original trip data but constructed from the trip data using a similar procedure to \citet{sessa21online}. The start location for every trip taken over the year is clustered via k-means, and then clusters that are very close together are removed. This left 116 depots where bikes can be left. We consider a system with 40 bikes, which are distributed initially by 5 trucks that place all bikes in that truck at the same depot. 

We further allocate depots to regions. These are constructed by using trip data across the whole year, and using a heuristic that clusters depots into regions so that there is a low chance that any given trip starts in one region and ends in another. As shown in \cref{fig:subalgo} this leads to nearby depots often being in the same region, which is reasonable. We get $15$ regions $R_{1}$ to $R_{15}$.

Agent action $\a_i$ is a one-hot $116$-length vector for which depot truck $i$'s bikes are placed at. 

We obtain 3 measurements for $\a'_{t}$ at the end of each day $t$. This is the day's average temperature, rainfall, and total demand (including unmet demand). These are part of $\a'$ because they are out of our agent's control and not sampled \iid across days. The agent must adaptively respond to these observations over time. Weather data is the real weather from that day obtained from \citet{LocalWeatherForecast}.

Observations $\s_{i}^{r}$ give the number of bikes at day start in depot number $i$ within region $r$. $\s_{r}$ is the total fulfilled trips that started in region $r$. Reward $Y$ is then the total trips in a day. All observations are normalized to ensure they are fixed in $[0, 1]$. 

In \citet{sessa21online}, 2 separate GPs are used to model weekday and weekend demand. For simplicity we use a simulator that skips weekends, and therefore we don't need a separate model for the two types of day. No matter the algorithm used, the first $10$ days of the year use the \rand strategy to gather exploratory data to initialize the GP models for the $\s_{r}$. 

The graph used by \subalg is given in \cref{fig:subalgo}(b). The relationship between the bike allocations and bike distribution at day start $\{X^r_{:}\}_{r=R_1}^{R_{15}}$ is a fixed known function. The mechanism from the starting bike distribution in a region $r$ ($X^r_{:}$), adversary actions $\a'$ (weather and demand) and total trips in region $r$ over the day ($X_r$) is an unknown function that must be learnt for each $r$. The relationship between total trips $Y$ and its parents is known (sum over parents). For this kind of graph the output of the Causal UCB Oracle (\cref{alg:oracle}) will always set $\eta=\mathbf 1$, because more trips in any region results in more total trips. For computational efficiency we therefore implement the Causal UCB Oracle to set $\eta$ to $\mathbf 1$ instead of optimising over $\eta$.


