\section{Analysis}
\label{sec:analysis}
Here we analyse the theoretical performance of \alg and provide a bound on its regret as a function of the underlying causal graph. For our analysis, we make some additional technical assumptions. First,  we assume all $\fofi \in \fs$ are $L_f$-Lipschitz continuous. This follows directly from the regularity assumptions of \cref{sec:setup}. Second, we assume that $\forall i, t$, the functions $\mui$, $\sigmait$ are $L_\mu$,  $L_\sigma$ Lipschitz continuous. This holds if the RKHS of each $\fofi$ has a Lipschitz continuous kernel (see \citet{curiEfficientModelBasedReinforcement2020}, Appendix G). 
 
In addition, to show how the regret guarantee depends on the specific GP hyperparameters used, we use a notion of model complexity for each node $i$:
\begin{equation}
\label{eq:mutual_info_node}
\gamma_{i,T} := 
    \underset{\scriptstyle A_i\subset\ \{\Zi \times \Ai \times \Ai'\}^T \atop\scriptstyle}{\max} \; I(\bm x_{i, 1:T}, \fofi) 
\end{equation}
where $I$ is the mutual information and the observations $\s_{i, 1:T}$ implicitly depend on the GP inputs $A_i$. This is analogous to the maximum information gain used in the analysis of standard BO algorithms \cite{srinivas10}. We also define 
\begin{equation}
\label{mutual_info}
\gamma_T = \max_i \gamma_{i, T}
\end{equation}
as the worst-case maximum information gain across all nodes in the system. 

Finally, we define two properties of the causal graph structure that the regret guarantee will depend on. In the DAG $\G$ over nodes $\{X_i\}_{i=0}^m$, let $\degree$ denote the maximum number of parents of any variable in $\calG$: $\degree = \max_{i} \abs{\pa(i)}$. Then let $N$ denote the maximum distance from a root node to $X_m$: $N = \max_i \mathrm{dist}(X_i, X_m)$ where $\mathrm{dist}(\cdot, \cdot)$ is the number of edges in the longest path from a node $X_i$ to $X_m$. 
We can then prove the following theorem on the regret of \alg.

\begin{theorem}\label{thm:1}
Fix $\delta \in (0,1)$, if actions are played according to \alg with $\beta_t = \mathcal{O} \left(\calB + \sqrt{\gamma_{t-1} + \log(m/\delta)} \right)$ and $\tau = \sqrt{(8\log \abs{\calA})/T}$, then with probability at least $1- \delta$, 
\begin{equation*}
R(T) = \mathcal{O}\left(\sqrt{T \log \abs{\calA}} +  \sqrt{ T \log(2/\delta)} +  \left(\calB+  \sqrt{\gamma_{T} + \log(m/\delta)} \right)^{N+1} \degree^N L_{\sigma}^N L_f^N m \sqrt{T \gamma_T} \right) \,,
\end{equation*}
where $\mathcal{B} = \max_i \mathcal{B}_i$, $\gamma_T = \max \gamma_{i, t}$. That is, $\gamma_T$ is the worst-case maximium information gain of any of the GP models.
\end{theorem}

\cref{thm:1}, whose proof is deferred to the appendix, shows that \alg is no-regret for a variety of common kernel functions, for example linear and squared exponential kernels. This is because even when accounting for the dependence of $\gamma_T$ in $T$, the bound is still sublinear in $T$. We discuss the dependence of $\gamma_T$ on $T$ for specific kernels in \cref{app:particular_kernels}. 

\paragraph{Comparison with~\gpmw} We can use \cref{thm:1} to demonstrate that the use of graph structure in \alg results in a potentially exponential improvement in the rate of regret compared to \gpmw~\citep{sessa2019no} with respect to the number of action variables $m$. Consider the graph in \cref{fig:analysis}b (see Appendix) for illustration.
When all $X_i$ in \alg are modeled with squared exponential kernels, we have $\gamma_T = \mathcal O((\degree +2)(\log T)^{\degree + 3})$. This results in a cumulative regret that is exponential in $\degree$ and $N$. Instead, \gpmw uses a single high-dimensional GP (\cref{fig:analysis}a), implying $\gamma_T = \mathcal O((\log T)^m)$ for a squared exponential kernel.  
Note that $m \geq \degree + N$ and thus, for several common graphs, the exponential scaling in $N$ and $\degree$ could be significantly more favorable than the exponential scaling in $m$. Specifically for the binary tree of \cref{fig:analysis}b, where $N=\log(m)$ and $\degree=2$, the cumulative regret of \alg will have only \emph{polynomial} dependence on $m$. 

Furthermore, in addition to favorable scaling of the regret in $m$, the model class considered by \alg is considerably larger than that of \gpmw, because \alg can model systems where reward depends on actions according to a composition of GPs based on $\G$, rather than a single GP.\looseness=-1 