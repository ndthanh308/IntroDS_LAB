\section{Computational Considerations in Larger Action Spaces} \label{sec:distributed}

The computational complexity of \alg is dominated by the computation of the counterfactual expected reward estimate for each possible intervention $\a \in \A$ (Line 7 in~\cref{alg:model-based-cbo}). In many situations, even with a large number of action variables $m$, $\abs{\calA}$ may still be small and thus \alg 
 feasible to implement. 
%$\abs{\cal{A}}$ is small, \alg will not have high computational cost. 
A notable example is when there exist constraints on the possible interventions, such as only being able to intervene on at most a few nodes simultaneously. In the worst case, though, $\abs{\cal{A}}$ grows exponentially with $m$ and thus \alg may not be computationally feasible.
In this section we will show how prior knowledge of the problem structure can be used to modify \alg to be computationally efficient even in settings with huge action spaces. We note that the computational efficiency of \alg is not affected by the number of possible adversary interventions $\abs{\calA'}$ because these are only observed a-posteriori (in fact, $\calA'$ need not be finite).  

The general idea consists of decomposing \alg into a \emph{decentralized} algorithm, which we call \subalg, that orchestrates multiple sub-agents. First recall that $\calA$ can be decomposed into $m$ smaller action spaces so that $\calA = \calA_1 \times ... \times \calA_m$. We then consider $m$ independent agents where each agent $i$ performs \alg but only over the action space $\calA_i$. Importantly, the observations of the actions of the other agents are considered part of $\calA'$ for that agent. Moreover, all agents utilize the same calibrated model $\calM_t$ at each round. We provide full pseudo-code and theory in the appendix. Our approach is inspired by \citet{sessa21online} who propose a distributed analog of the \gpmw algorithm.\looseness=-1 

In~\cref{sec:appendix_distributed} we show that under specific assumptions on the reward function, \subalg provably enjoys an approximate version of the guarantees in~\cref{thm:1}.
Namely, we study a setting where $r$ is a \emph{submodular} and monotone increasing function of $a$ for any given set of adversary actions. Submodularity is a diminishing returns property (see formal definition in the appendix) widely exploited in a variety of domains to derive efficient methods with approximation guarantees, see e.g.~\cite{marden2013distributed, sessa21online, Paccagnan2022}. Similarly, we exploit submodularity in the overall reward's structure to parallelize the computation over the possible interventions in our causal graph. In our experiments, we study rebalancing an SMS where \alg is not applicable due to a combinatorial action space but \subalg is efficient and achieves good performance. \looseness-1