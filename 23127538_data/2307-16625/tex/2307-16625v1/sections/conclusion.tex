\section{Conclusion}
We introduce \alg, the first principled approach to causal Bayesian optimization in non-stationary and potentially multi-agent environments. We prove a sublinear regret guarantee for \alg and demonstrate a potentially exponential improvement in regret, in terms of the number of possible intervention targets, compared to state-of-the-art methods. %es that do not make use of a causal model for rewards. 
We further propose a distributed version of our approach, \subalg, that can scale to large action spaces and achieves approximate regret guarantees when rewards are monotone submodular. Empirically, our algorithms outperform existing non-causal and non-adversarial methods on synthetic function network tasks and on an SMS rebalancing simulator based on real data.

%\newpage
\section*{Acknowledgements}

We thank Lars Lorch for his feedback on the draft of this paper.

This research was supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, by the European Research Council (ERC) under the European Unionâ€™s Horizon grant 815943, and by
ELSA (European Lighthouse on Secure and Safe AI) funded by the European Union under grant agreement No. 101070617. 