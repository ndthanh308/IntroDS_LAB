\section{Method}

In this section, we introduce the methodology for the proposed \alg algorithm. 

\subsection{Calibrated models}
An important component of our approach is the use of calibrated uncertainty models to learn functions $\fs$, as done in~\citet{sussex2022model}. At the end of each round $t$, we use the dataset $\dataset_{t} = \{\ztt, \att, \att', \stt\}$ of past interventions to fit a separate model for every node in the system.
\alg can be applied with any set of models that have calibrated uncertainty. That is, for every node $i$ at time $t$ the model has a mean function $\muit$ and variance function $\sigmait$ (learnt from $\dataset_t$) that accurately capture any epistemic uncertainty in the true model.

\begin{assumption}[\textit{Calibrated model}]
    All statistical models are {\em calibrated} w.r.t. $\fs$, so that $\forall i, t$ there exists a sequence $\beta_t \in \R_{>0}$ such that, with probability at least $(1 - \delta)$,  for all $\zi, \ai, \ai' \in \Zi \times \Ai \times \Ai'$ we have $| \fofi(\zi, \ai, \ai') - \mu_{i, t-1}(\zi, \ai, \ai') | \leq \beta_{t} \sigma_{i, t-1}(\zi, \ai, \ai')$, element-wise.
    \label{as:well_calibrated_model}
\end{assumption}

If the models are calibrated, we can form confidence bounds that contain the true system model with high probability. This is done by combining separate confidence bounds for the mechanism at each node. At time $t$, the known set $\calM_t$ of statistically plausible functions $\tfs = \{\tfi\}_{i=0}^m$ is defined as: 
\begin{equation}
\label{eq:calibrated}
\begin{split}
    \calM_t = \bigg\{& \tfs = \{\tfi\}_{i=0}^m  ,\ \text{s.t.}\ \forall i: \ \tfi \in \mathcal{H}_{k_i}, \ \|\tfi\|_{k_i} \leq \mathcal{B}_i, 
    \text{and} \\
    &  \ \abs{\tfi(\zi, \ai, \ai') - \mu_{i, t-1}(\zi, \ai, \ai')} \leq \beta_{t} \sigma_{i,t-1}(\zi, \ai, \ai'), \ \forall \z_i \in \calZ_i, \ai \in \calA_i, \ai' \in \calA_i' \bigg\}.
\end{split}
\end{equation}
\paragraph{GP models} Gaussian Process (GP) models can be used to model epistemic uncertainty. These are the model class we study in our analysis (\cref{sec:analysis}), where we also give explicit forms for $\beta_t$ that satisfy \cref{as:well_calibrated_model}. 
For all $i \in [m]$, let $\mu_{i,0}$ and $\sigma_{i,0}^2$ denote the prior mean and variance functions for each $\fofi$, respectively. Since $\snoise_i$ is bounded, it is also subgaussian and we denote the variance by $\noisescalei^2$. The corresponding posterior GP mean and variance, denoted by $\mui,t$ and $\sigmait^2$ respectively, are computed based on the previous evaluations $\dataset_{t}$:
\begin{align}
\muit(\citt) &= \mathbf{k}_t(\citt)^\top (\mathbf{K}_t + \noisescalei^2 \mathbf{I}_t)^{-1} \sitt  \label{eq:mean_update}\\
\sigmait^2(\citt) &= k(\citt; \citt) -  \mathbf{k}_t(\citt)^\top (\mathbf{K}_t + \noisescalei^2 \mathbf{I}_t)^{-1} \mathbf{k}_t(\citt) \, , \label{eq:sigma_update}
%\vspace{-0.5em}
\end{align}
where $\citt = (\zitt, \aitt, \aitt')$, $\mathbf{k}_t(\citt) = [k(\c_{i, j}, \citt)]_{j=1}^t$, and $\mathbf{K}_t = [k(\c_{i,j}, \c_{i, j'})]_{j,j'}$ is the kernel matrix. 


\subsection{The \alg~algorithm}

\looseness-1
\begin{wrapfigure}{t}{.55\linewidth}
    %\center
    \vspace{-0.4in}
    \begin{minipage}{\linewidth}
    \begin{algorithm}[H]
        \caption{Causal Bayesian Optimization Multiplicative Weights (\alg)}
        \label{alg:model-based-cbo}
        \begin{algorithmic}[1]  
            \Require parameters $\tau, \{\beta_{t}\}_{t\geq1}, \G, \snoiserv$, kernel functions $k_{i}$ and prior means $\mu_{i,0} = 0$ $\forall i \in [m]$ 
            \State Initialize $\bw^1 = \frac{1}{\abs{\calA}}(1, \ldots, 1) \in \mathbb{R}^{\abs{\calA}}$
            \For{$t = 1, 2, \hdots$}
                    \State Sample $\a_t \sim \bw^t$
                    \State Observe samples $\{\zit, \sit, \ait'\}_{i=0}^m$
               \State Update posteriors $\{\muit(\cdot), \sigmait^2(\cdot) \}_{i=0}^m$ %and construct confidence sets $\{\tilde{\bm{f}}_t\}$
                \For{$\a \in \calA$}
                    \State Compute $\ucb_t(\a, \a_t')$ using \cref{alg:oracle}
                    \State $\hat{y}_{\a}^t = \min(1, \ucb_t(\a, \a_t'))$
                    \State $w_{\a}^{t+1} \propto w_{\a}^t \exp(\tau \cdot \hat{y}_{\a}^t)$
                \EndFor
            \EndFor 
        \end{algorithmic}
    \end{algorithm}
\end{minipage}
\par
\vspace{-0.15in}
\end{wrapfigure}

We can now present the proposed 
\looseness-1 \alg algorithm. Our approach is based upon the classic multiplicative weights method~\citep{littlestone1994weighted,freund1997decision}, widely used in adversarial online learning. Indeed, ACBO can be seen as a specific structured online learning problem. At time $t$, \alg maintains a weight $w_{\a}^{t}$ for every possible intervention $\a \in \calA$ such that $\sum_{\a} w_{\a}^{t} = 1$ and uses these to sample the chosen intervention, i.e., $\a_t \sim w_{\a}^{t}$. Contrary to standard CBO (where algorithms can choose actions deterministically), in adversarial environments such as ACBO randomization is necessary to achieve no-regret, see, e.g.,~\cite{cesa2006prediction}.

Then, \alg updates the weights at the end of each round based upon what action the adversary took $\a'_t$ and the observations $\s_t$. If the mechanism between actions, adversary actions, and rewards were to be completely known (i.e., the function $r(\cdot)$ in our setup), a standard MW strategy suggests updating the weight for every $\a$ according to
\begin{equation}
\label{eq:mw_update}
w_{\a}^{t+1} \propto w_{\a}^t \exp{} \left(\tau \cdot r(\a, \a_t') \right) \; ,  \nonumber
\end{equation}
where $\tau$ is a tunable learning rate. In particular, $r(\a, \a_t')$ is the counterfactual of what would have happened, in expectation over noise, had $\a_t'$ remained fixed but the algorithm selected $\a$ instead of $\a_t$. \looseness-1

However, in ACBO the system is unknown and thus such counterfactual information is not readily available. On the other hand, as outlined in the previous section, we can build and update calibrated models around the unknown mechanisms and then estimate counterfactual quantities from them.
Specifically, \alg utilizes the confidence set $\calM_t$ to compute an \emph{optimistic} estimate of $r(\a, \a_t')$: 
\begin{align} \label{eq:cucb}
    \ucb_t(\a, \a') &= \max_{\tfs \in \calM_t} \E[\snoise]{y \mid \tfs, \bm a, \a_t'}.
\end{align}
Given action $\a$, opponent action $\a_t'$ and confidence set $\calM_t$, $\ucb_t(\a, \a')$ represents the highest expected return among all system models in this confidence set. \alg uses such estimates to update the weights in place of the true but unknown counterfactuals $r(\a, \a_t')$.
Computing $\ucb_t(\a, \a')$ is challenging since our confidence set $\calM_t$ consists of a set of $m$ different models and one must propagate epistemic uncertainty through all models in the system, from actions to rewards.  
Because mechanisms can be non-monotonic and nonlinear, one cannot simply independently maximize the output of every mechanism. We thus defer this task to an algorithmic subroutine (denoted causal UCB oracle) which we describe in the next subsection.
\alg is summarized in \cref{alg:model-based-cbo}.

We note that \alg strictly generalizes the \textsc{GP-MW} algorithm of~\citet{sessa2019no}, which was first to propose combining MW with optimistic counterfactual reward estimates. However, they consider a trivial causal graph with only a target node, thus a \emph{single} GP model. For this simpler model one can compute $\ucb_t$ in closed-form but must ignore any causal structure in the reward model.  In \cref{sec:analysis} and in our experiments we show \alg can significantly outperform \gpmw both theoretically and experimentally.\looseness=-1 

\subsection{Causal UCB oracle}

\looseness-1 The problem in \cref{eq:cucb} is not amenable to commonly used optimization techniques, due to the maximization over a set of functions with bounded RKHS norm. 
Therefore, similar to \citet{sussex2022model} we make use of the reparameterization trick to write any $\tfi \in \tfs \in \calM_t$ using a function $\etai :\calZ_i \times \calA_i \times \calA_i' \rightarrow [-1, 1]$ as \looseness-1
\begin{equation}
\tfit(\zoi, \aoi, \aoi') = \mu_{i, t-1}(\zoi, \aoi, \aoi') + \beta_{t} \sigma_{i, t-1}(\zoi, \aoi, \aoi') \etai (\zoi, \aoi, \aoi'),
\label{eq:reparametrization}
\end{equation}
where $\soi = \tfi(\zoi, \aoi, \aoi') + \tilde{\noise}_i$ denotes observations from simulating actions in one of the plausible models, and not necessarily the true model. The validity of this reparameterization comes directly from the definition of $\calM_t$ in \cref{eq:calibrated} and the range of $\etai$. 

The reparametrization allows for rewriting $\ucb_t$ in terms of $\etas: \Z \times \A \times \A' \to [-1,1]^{|\X|}$: \looseness-1
\begin{equation}
\ucb_t(\a, \a') = \max_{\etas(\cdot)} \E[\snoise]{y \mid \tfs, \bm a, \a'_t}, \ \ \text{s.t.  } \tfs = \{\tfit\} \text{\ \  in \cref{eq:reparametrization}} . 
\label{eq:reparam_ucb}
\end{equation}

\begin{wrapfigure}{t}{.55\linewidth}
\vspace{-0.3in}
    %\center
    \begin{minipage}{\linewidth}
\begin{algorithm}[H]
    \caption{Causal UCB Oracle}
    \label{alg:oracle}
    \begin{algorithmic}[1]  
        \Require neural networks $\etas$, actions $\a, \a'$, model posteriors $\bmu, \bsigma$, parameter $\beta_t$, repeats $N_\text{rep}$.\looseness=-1
    \State Initialize $\textsc{Solutions} = \emptyset$
    \For{$j = 1, \hdots, N_\text{rep}$ }
        \State Randomly initialize weights of each $\eta_i \in \etas$
        \State $\ucb_{t,j} =  \max_{\etas (\cdot)} \mathbb E [y \mid \tfs, \bm a, \a']$ computed via stochastic gradient ascent on $\etas$
        \State $\textsc{Solutions} =\textsc{Solutions} \cup \{\ucb_{t,j}\}$.
    \EndFor \\
    \Return $\max(\textsc{Solutions})$
    \end{algorithmic}
\end{algorithm}
\end{minipage}
\par
\vspace{-0.2in}
\end{wrapfigure}

\looseness-1 In practice, we can parameterize $\etas$, for example with neural networks, and maximize this objective using stochastic gradient ascent, as described in \cref{alg:oracle}. The use of the reparameterization trick simplifies the optimization problem because we go from optimizing over functions with a tricky constraint ($\tfs \in \calM_t$) to a much simpler constraint ($\etas$ just needing output in $[0, 1]$). Since the optimization problem is still non-convex, we deploy multiple random re-initializations of the $\etas$ parameters. 
