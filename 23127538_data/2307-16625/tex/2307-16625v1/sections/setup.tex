\section{Background and Problem Statement}
\label{sec:setup}
We consider the problem of an agent interacting with an SCM for $T$ rounds in order to maximize the value of a reward variable. We start by introducing SCMs, the soft intervention model used in this work, and then define the adversarial sequential decision-making problem we study. In the following, we denote with $[m]$ the set of integers $\{0, \dots, m\}$. \looseness-1

\paragraph{Structural Causal Models}
Our SCM is described by a tuple $\langle \G,  Y, \bX, \fs, \snoiserv \rangle$ of the following elements: $\G$ is a \emph{known} DAG; $Y$ is the reward variable; $\bX = {\{X_i\}_{i=0}^{m-1}}$ is a set of observed scalar random variables; the set $\fs = \{\fofi\}_{i=0}^m$ defines the \emph{unknown} functional relations between these variables; and $\snoiserv = \{\snoiserv_i \}_{i=0}^{m}$ is a set of independent noise variables with zero-mean and known distribution. % \looseness-1
 We use the notation $Y$ and $X_m$ interchangeably and assume the elements of $\bX$ are topologically ordered, i.e., $X_0$ is a root and $X_m$ is a leaf.  We denote with $\pa_i \subset \{0, \dots, m\}$ the indices of the parents of the $i$th node, and use the notation $\bZi = \{ X_j\}_{j \in \pa_i}$ for the parents this node. We sometimes use $X_i$ to refer to both the $i$th node and the $i$th random variable. \looseness-1\looseness-1

Each $X_i$ is generated according to the function $\fofi: \calZ_i \rightarrow \calX_i$, taking the parent nodes $\bZi$ of $X_i$ as input: $\si =\fofi(\zi) + \noisei$, where lowercase denotes a realization of the corresponding random variable. The reward is a scalar $x_m \in [0,1]$ while observation $X_i$ is defined over a compact set $\si \in \calX_i \subset \R$, and its parents are defined over $\calZ_i = \prod_{j \in pa_i} \calX_j$ for $i\in [m-1]$.\footnote{Here we consider scalar observations for ease of presentation, but we note that the methodology and analysis can be easily extended to vector observations as in \citet{sussex2022model}}  \looseness-1

\paragraph{Interventions}

\looseness -1 In our setup, an agent and an adversary both perform \emph{interventions} on the SCM~\footnote{Our framework allows for there to be potentially multiple adversaries, but since we consider everything from a single player's perspective, it is sufficient to combine all the other agents into a single adversary.}. 
We consider a soft intervention model \citep{eberhardt2007interventions} where interventions are parameterized by controllable \emph{action variables}. A simple example of a soft intervention is a shift intervention, where actions affect their outputs additively \citep{zhang2021matching}.

First, consider the agent and its action variables $\bm a = {\{ \ai\}_{i=0}^{m}}$. Each action $a_i$ is a real number chosen from some finite set. That is, the space $\calA_i $  of action $a_i$ is   $\calA_i \subset \R_{[0, 1]}$ where $\abs{\calA_i} = K_i$  for some $K_i \in \nN$. Let $\calA$ be the space of all actions $\bm a = {\{ \ai\}_{i=0}^{m}}$. 
% Let $\calA$ be the space of all actions $\bm a = {\{ \ai\}_{i=0}^{m}}$.
We represent the actions as additional nodes in $\G$ (see \cref{fig:overview}): $\ai$ is a parent of only $X_i$, and hence an additional input to $\fofi$. Since $\fofi$ is unknown, the agent does not know apriori the functional effect of $\ai$ on $X_i$. Not intervening on a node $X_i$ can be considered equivalent to selecting $\ai = 0$. For nodes that cannot be intervened on by our agent, we set $K_i = 1$ and do not include the action in diagrams, meaning that without loss of generality we consider the number of action variables to be equal to the number of nodes $m$.
\footnote{There may be constraints on the actions our agent can take. We refer the reader to \citet{sussex2022model} for how our setup can be extended to handle constraints.}

For the adversary we consider the same intervention model but denote their actions by $\a'$ with each $\ai'$ defined over $\calA_i' \subset \R_{[0, 1]}$ where $\abs{\calA_i'} = K_i'$ and $K_i'$ is not necessarily equal to $K_i$. 

According to the causal graph, actions $\a, \a'$ induce a realization of the graph nodes: 
\begin{align}
\label{eq:groud_truth}
& \si = \fofi(\zi, \ai, \ai') + \noisei, \ \ \forall i \in [m].
\end{align}
 
If an index $i$ corresponds to a root node, the parent vector $\zi$ denotes an empty vector, and the output of $\fofi$ only depends on the actions.

\looseness-1

\paragraph{Problem statement}
Over multiple rounds, the agent and adversary intervene simultaneously on the SCM, with known DAG $\calG$ and fixed but unknown functions $\fs = \{\fofi\}_{i=1}^m$ with $\fofi: \calZ_i \times \A_i \times \A_i' \rightarrow \calX_i$. \looseness-1
At round $t$ the agent selects actions $\at = \{\ait\}_{i=0}^m$ and obtains observations $\st = \{\sit\}_{i=0}^m$, where we add an additional subscript to denote the round of interaction. When obtaining observations, the agent also observes what actions the adversary chose $\at' = \{\ait'\}_{i=0}^m$.  We assume the adversary does not have the power to know $\at$ when selecting $\at'$, but only has access to the history of interactions until round $t$. The agent obtains a reward given by \looseness-1
\begin{align}
\label{eq:groud_truth_target}
& y_t = f_m(\bm z_{m, t}, a_{m, t}, a_{m, t}') + \noise_{m, t},
\end{align}
which implicitly depends on the whole action vector $\at$ and adversary actions $\at'$. 

The agent's goal is to select a sequence of actions that maximizes their cumulative expected reward $\sum_{t=1}^T 
r(\at, \at')$ where $r(\at, \at') = \E{y_t\mid \at, \at'}$ and expectations are taken over $\snoise$ unless otherwise stated. The challenge for the agent lies in not knowing a-priori neither the causal model (i.e., the functions $\fs = \{\fofi\}_{i=1}^m$), nor the sequence of adversarial actions $\{\at'\}_{t=1}^{\cdots}$.

\paragraph{Performance metric} 

After $T$ timesteps, we can measure the performance of the agent via the notion of regret:
\begin{align}
    R(T) = \max_{\a \in \A} \sum_{t=1}^T r(\a, \at') - \sum_{t=1}^T r(\at, \at'),
    \label{eq:regret}
\end{align}
\ie, the difference between the best cumulative expected reward obtainable by playing a single fixed action if the adversary's action sequence and $\fs$ were known in hindsight, and the agent's cumulative expected reward. We seek to design algorithms for the agent that are \emph{no-regret}, meaning that $R(T)/T \rightarrow 0$ as $T\rightarrow \infty$, for any sequence $\at'$. We emphasize that while we use the term `adversary', our regret notion encompasses all strategies that the adversary could use to select actions. This might include cooperative agents or mechanism non-stationarities. \looseness -1


 For simplicity, we consider only adversary actions observed after the agent chooses actions. Our methods can be extended to also consider adversary actions observed \emph{before} the agent chooses actions, i.e., a \textit{context}. This results in learning a policy that returns actions depending on the context, rather than just learning a fixed action. This extension is straightforward and we briefly discuss it in~\Cref{app:contextual}. \looseness-1

\textbf{Regularity assumptions} We consider standard smoothness assumptions for the unknown functions $\fofi:\mathcal{S} \rightarrow \X_i$ defined over a compact domain $\mathcal{S}$ \citep{srinivas10}. In particular, for each node $i \in [m]$, we assume that $\fofi(\cdot)$ belongs to a reproducing kernel Hilbert space (RKHS) $\mathcal{H}_{k_i}$, a space of smooth functions defined on $\calS = \calZ_i \times \calA_i \times \calA_i'$.
This means that $\fofil \in \mathcal{H}_{k_i}$ is induced by a kernel function $k_i: \calS \times  \calS \rightarrow \mathbb{R}$. 
We also assume that $k_i(s,s') \leq 1$ for every $s, s' \in \calS$\footnote{This is known as the bounded variance property, and it holds for many common kernels.}. Moreover, the RKHS norm of $\fofi(\cdot)$ is assumed to be bounded $\|\fofi\|_{k_i} \leq \mathcal{B}_i$ for some fixed constant $\mathcal{B}_i>0$.  Finally, to ensure the compactness of the domains $\Z_i$, we assume that the noise $\snoise$ is bounded, i.e., $\noisei \in \left[-1,1\right]^{d}$. \looseness-1
