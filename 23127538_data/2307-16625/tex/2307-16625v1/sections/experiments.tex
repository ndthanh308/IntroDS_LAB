\section{Experiments}
\label{sec:experiments}
\looseness-1 We evaluate \alg and \subalg on various synthetic problems and a simulator of rebalancing an SMS based on real data. The goal of the experiments is to understand how the use of causal modelling and adaptability to external factors in \alg affects performance compared to other BO methods that are missing one or both of these components.
All methods are evaluated using 10 repeats, where we report mean and standard error for different time horizons. 



\subsection{Function networks}

% Figure environment removed

\paragraph{Networks}

We evaluate \alg on 8 diverse environments. As a base, we take 4 examples of function networks from \citet{astudilloBayesianOptimizationFunction2021}. Function networks is a noiseless CBO setup with no adversaries. To study an adversarial setup, we modify each environment by adding adversaries' inputs in 2 ways: Penny and Perturb. In Penny, an adversary can affect a key node with a dynamic similar to the classic matching pennies game. In Perturb, the adversary can perturb some of the agent's interventions. The exact way in which the adversary's actions affect the environment is unknown and the actions themselves can only be observed a-posteriori. In each round, the adversary has a 20\% chance to play randomly, and an 80\% chance to try and minimize the agent's reward, using full knowledge of the agent's strategy and the environment. The causal graph and structural equation model for each environment in given in Appendix~\ref{app:experiments}. 

\paragraph{Baselines}
We compare the performance of  \alg (\cref{alg:model-based-cbo}) with \gpmw \cite{sessa2019no} which does not exploit the causal structure. Moreover, 
to study the importance of adversarial methods, we additionally compare against non-adversarial baselines
\ucbalg \citep{srinivas10}, and \mcbo \citep{sussex2022model} which uses a similar causal modelling and optimism approach to \alg but cannot account for adversaries.

\paragraph{Results}
We give results from 3 of the 8 environments in \cref{fig:fn}. The rest can be found in Appendix~\ref{app:experiments}. 
In general across the 8 environments, \mcbo and \ucbalg obtain linear regret, while the regret of \gpmw and \alg grows sublinearly, consistent with \cref{thm:1}.
We observe that \alg has the strongest or joint-strongest performance on 7 out of the 8 environments. The setting where \alg was not strongest involves a dense graph where worst-case GP sparsity is the same as \gpmw, which is consistent with our theory. In settings such as Alpine where the graph is highly sparse, we observe the greatest improvements from using \alg. %In simple graphs such as Dropwave (\cref{fig:fn}c) we observe \alg and \gpmw to have more similar sample efficiencies. 

\subsection{Learning to rebalance Shared Mobility Systems (SMSs)}

We evaluate \subalg on the task of rebalancing an SMS, a setting introduced in \citet{sessa21online}. The goal is to allocate bikes to city locations (using relocation trucks, overnight) to maximize the number of bike trips in the subsequent day. The action space is combinatorial because each of the $5$ trucks can independently reallocate units to a new depot. This makes the approaches studied above computationally infeasible, so we deploy \subalg. We expect the reward $Y_t$ (total trips given unit allocation) to be monotone submodular because adding an extra unit to a depot should increase total trips but with decreasing marginal benefit, as discussed in \citet{sessa21online}. The goal is to compare \subalg to a distributed version of \gpmw and understand whether the use of a causal graph can improve sample efficiency.  

% Figure environment removed

\paragraph{Setup and trips simulator}
A simulator is constructed using historical data from an SMS in Louisville, KY \citep{LouisvilleKentuckyOpen}. Before each day $t$, all 40 bikes in the system are redistributed across 116 depots. This is done by $5$ trucks, each of which can redistribute 8 bikes at one depot, meaning a truck $i$'s action is $a_i \in [116]$.  
The demand for trips corresponds to real trip data from \citet{LouisvilleKentuckyOpen}. After each day, we observe weather and demand data from the previous day which are highly non-stationary and thus correspond to adversarial interventions $\a'$ according to our model. Our simulator is identical to the one of \citet{sessa2019no}, except we exclude weekends for simplicity. We give more details in Appendix~\ref{app:experiments}.\looseness=-1 

We compare three methods on the SMS simulator. First, in \rand each truck places its bikes at a depot chosen uniformly at random. Second, \dgpmw modifies \gpmw using the same ideas as those presented in \cref{sec:distributed}. It is a special case of \subalg but using the graph in \cref{fig:overview}(a). That is, a single GP is used to predict $Y_t$ given $\a_t, \a_t'$. Finally, we evaluate \subalg which utilizes a more structured causal graph exploiting the reward structure. Based on historical data, we cluster the depots into regions such that trips don't frequently occur across two different regions (e.g., when such regions are too far away). Then, our graph models the trips starting in each region only using bike allocations to depots in that region. $Y_t$ is computed by summing the trips across all regions (see \cref{fig:subalgo} (b) in the appendix for an illustration). This system model uses many low-dimensional GPs instead of a single high-dimensional GP as used in $\dgpmw$.

\paragraph{Results}
\cref{fig:sms} (a) displays the allocation strategy of \subalg (blue dots are proportional to the number of bikes allocated to each depot, averaged across the 200 days). We observe that \subalg learns the underlying demand patterns and allocates bikes in strategic areas where the demand (green dots) is higher. 
Moreover, in \cref{fig:sms} (b) we see that \subalg is significantly more sample efficient than \dgpmw. This improvement is largely attributed to early rounds, where \subalg learns the demand patterns much faster than \dgpmw due to learning smaller-dimensional GPs.\looseness=-1 
