\begin{abstract}
In Causal Bayesian Optimization (CBO), an agent intervenes on an unknown structural causal model to maximize a downstream reward variable. In this paper, we consider the generalization where 
other agents or external events also intervene on the system,
which is key for enabling adaptiveness to non-stationarities 
such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as {\em Adversarial Causal Bayesian Optimization (\acbo)} and introduce the first algorithm for \acbo with bounded regret: {\em Causal Bayesian Optimization with Multiplicative Weights} (\alg). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. 
We derive regret bounds for \alg that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards.
Empirically, \alg outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how \alg can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.\looseness=-1   
\end{abstract}
