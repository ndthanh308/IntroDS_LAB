\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
Abdolmaleki, A., Springenberg, J.~T., Tassa, Y., Munos, R., Heess, N., and
  Riedmiller, M.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Abramovich \& Persson(2016)Abramovich and Persson]{abramovich2016some}
Abramovich, S. and Persson, L.-E.
\newblock Some new estimates of the '{J}ensen gap'.
\newblock \emph{Journal of Inequalities and Applications}, 2016\penalty0
  (1):\penalty0 1--9, 2016.

\bibitem[Agarwal et~al.(2019)Agarwal, Jiang, Kakade, and
  Sun]{agarwal2019reinforcement}
Agarwal, A., Jiang, N., Kakade, S.~M., and Sun, W.
\newblock Reinforcement learning: Theory and algorithms.
\newblock \emph{CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep}, 2019.

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{an2021uncertainty}
An, G., Moon, S., Kim, J.-H., and Song, H.~O.
\newblock Uncertainty-based offline reinforcement learning with diversified
  {Q}-ensemble.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 7436--7447, 2021.

\bibitem[Bai et~al.(2022)Bai, Wang, Yang, Deng, Garg, Liu, and
  Wang]{bai2022pessimistic}
Bai, C., Wang, L., Yang, Z., Deng, Z.-H., Garg, A., Liu, P., and Wang, Z.
\newblock Pessimistic bootstrapping for uncertainty-driven offline
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Bakushinskii(1967)]{bakushinskii1967general}
Bakushinskii, A.~B.
\newblock A general method of constructing regularizing algorithms for a linear
  incorrect equation in hilbert space.
\newblock \emph{Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  7\penalty0 (3):\penalty0 672--677, 1967.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzev, and
  Rosasco]{bauer2007regularization}
Bauer, F., Pereverzev, S., and Rosasco, L.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.

\bibitem[Brandfonbrener et~al.(2021)Brandfonbrener, Whitney, Ranganath, and
  Bruna]{brandfonbrener2021offline}
Brandfonbrener, D., Whitney, W., Ranganath, R., and Bruna, J.
\newblock Offline {RL} without off-policy evaluation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4933--4946, 2021.

\bibitem[Buckman et~al.(2021)Buckman, Gelada, and
  Bellemare]{buckman2020importance}
Buckman, J., Gelada, C., and Bellemare, M.~G.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Calandra et~al.(2017)Calandra, Owens, Upadhyaya, Yuan, Lin, Adelson,
  and Levine]{calandra2017feeling}
Calandra, R., Owens, A., Upadhyaya, M., Yuan, W., Lin, J., Adelson, E.~H., and
  Levine, S.
\newblock The feeling of success: Does touch sensing help predict grasp
  outcomes?
\newblock In \emph{Conference on Robot Learning}, pp.\  314--323. PMLR, 2017.

\bibitem[Chebotar et~al.(2021)Chebotar, Hausman, Lu, Xiao, Kalashnikov, Varley,
  Irpan, Eysenbach, Julian, Finn, et~al.]{chebotar2021actionable}
Chebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J.,
  Irpan, A., Eysenbach, B., Julian, R.~C., Finn, C., et~al.
\newblock Actionable models: Unsupervised offline reinforcement learning of
  robotic skills.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1518--1528. PMLR, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Ding et~al.(2019)Ding, Florensa, Abbeel, and Phielipp]{ding2019goal}
Ding, Y., Florensa, C., Abbeel, P., and Phielipp, M.
\newblock Goal-conditioned imitation learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Emmons et~al.(2021)Emmons, Eysenbach, Kostrikov, and
  Levine]{emmons2021rvs}
Emmons, S., Eysenbach, B., Kostrikov, I., and Levine, S.
\newblock Rvs: What is essential for offline rl via supervised learning?
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Eysenbach et~al.(2020{\natexlab{a}})Eysenbach, Geng, Levine, and
  Salakhutdinov]{eysenbach2020rewriting}
Eysenbach, B., Geng, X., Levine, S., and Salakhutdinov, R.
\newblock Rewriting history with inverse rl: Hindsight inference for policy
  improvement.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{a}}.

\bibitem[Eysenbach et~al.(2020{\natexlab{b}})Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2020c}
Eysenbach, B., Salakhutdinov, R., and Levine, S.
\newblock C-learning: Learning to achieve goals via recursive classification.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Levine, and
  Salakhutdinov]{eysenbach2021replacing}
Eysenbach, B., Levine, S., and Salakhutdinov, R.
\newblock Replacing rewards with examples: Example-based policy search via
  recursive classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Eysenbach et~al.(2022)Eysenbach, Udatha, Salakhutdinov, and
  Levine]{eysenbach2022imitating}
Eysenbach, B., Udatha, S., Salakhutdinov, R.~R., and Levine, S.
\newblock Imitating past successes can be very suboptimal.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 6047--6059, 2022.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pp.\  49--58.
  PMLR, 2016.

\bibitem[Fleming(1990)]{fleming1990equivalence}
Fleming, H.~E.
\newblock Equivalence of regularization and truncated iteration in the solution
  of ill-posed image reconstruction problems.
\newblock \emph{Linear Algebra and its applications}, 130:\penalty0 133--150,
  1990.

\bibitem[Fu et~al.(2018)Fu, Singh, Ghosh, Yang, and Levine]{fu2018variational}
Fu, J., Singh, A., Ghosh, D., Yang, L., and Levine, S.
\newblock Variational inverse control with events: A general framework for
  data-driven reward definition.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 20132--20145, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pp.\
  1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International conference on machine learning}, pp.\
  2052--2062. PMLR, 2019.

\bibitem[Gao et~al.(2017)Gao, Sitharam, and Roitberg]{gao2017bounds}
Gao, X., Sitharam, M., and Roitberg, A.~E.
\newblock Bounds on the {J}ensen gap, and implications for mean-concentrated
  distributions.
\newblock \emph{arXiv preprint arXiv:1712.05267}, 2017.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized {M}arkov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2160--2169. PMLR, 2019.

\bibitem[Ghosh et~al.(2020)Ghosh, Gupta, Reddy, Fu, Devin, Eysenbach, and
  Levine]{ghosh2020learning}
Ghosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C.~M., Eysenbach, B., and
  Levine, S.
\newblock Learning to reach goals via iterated supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[G{\"u}l{\c{c}}ehre et~al.(2020)G{\"u}l{\c{c}}ehre, Wang, Novikov,
  Le~Paine, Colmenarejo, Zolna, Agarwal, Merel, Mankowitz, Paduraru,
  et~al.]{gulccehre2020rl}
G{\"u}l{\c{c}}ehre, {\c{C}}., Wang, Z., Novikov, A., Le~Paine, T., Colmenarejo,
  S.~G., Zolna, K., Agarwal, R., Merel, J., Mankowitz, D.~J., Paduraru, C.,
  et~al.
\newblock {RL} unplugged: Benchmarks for offline reinforcement learning.
\newblock 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Hatch et~al.(2022)Hatch, Yu, Rafailov, and Finn]{hatch2022example}
Hatch, K., Yu, T., Rafailov, R., and Finn, C.
\newblock Example-based offline reinforcement learning without rewards.
\newblock \emph{Proceedings of Machine Learning Research vol}, 144:\penalty0
  1--17, 2022.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and
  Van~Soest]{hazan2019provably}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2681--2691. PMLR, 2019.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth-Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli, Henderson, Novikov,
  Colmenarejo, Cabi, Gulcehre, Paine, Cowie, Wang, Piot, and
  de~Freitas]{hoffman2020acme}
Hoffman, M., Shahriari, B., Aslanides, J., Barth-Maron, G., Behbahani, F.,
  Norman, T., Abdolmaleki, A., Cassirer, A., Yang, F., Baumli, K., Henderson,
  S., Novikov, A., Colmenarejo, S.~G., Cabi, S., Gulcehre, C., Paine, T.~L.,
  Cowie, A., Wang, Z., Piot, B., and de~Freitas, N.
\newblock Acme: A research framework for distributed reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.00979}, 2020.

\bibitem[Huntley(1967)]{huntley1967dimensional}
Huntley, H.~E.
\newblock \emph{Dimensional analysis}.
\newblock Dover publications, 1967.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke,
  et~al.]{kalashnikov2018scalable}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, pp.\  651--673. PMLR, 2018.

\bibitem[Kalashnikov et~al.(2021)Kalashnikov, Varley, Chebotar, Swanson,
  Jonschkowski, Finn, Levine, and Hausman]{kalashnikov2021mt}
Kalashnikov, D., Varley, J., Chebotar, Y., Swanson, B., Jonschkowski, R., Finn,
  C., Levine, S., and Hausman, K.
\newblock Scaling up multi-task robotic reinforcement learning.
\newblock In \emph{5th Annual Conference on Robot Learning}, 2021.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit {Q}-learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kumar et~al.(2019{\natexlab{a}})Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Kumar et~al.(2019{\natexlab{b}})Kumar, Peng, and
  Levine]{kumar2019reward}
Kumar, A., Peng, X.~B., and Levine, S.
\newblock Reward-conditioned policies.
\newblock \emph{arXiv preprint arXiv:1912.13465}, 2019{\natexlab{b}}.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pp.\  45--73. Springer, 2012.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2020)Li, Pinto, and Abbeel]{li2020generalized}
Li, A., Pinto, L., and Abbeel, P.
\newblock Generalized hindsight for reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 7754--7767, 2020.

\bibitem[Lu et~al.(2021)Lu, Ball, Parker-Holder, Osborne, and
  Roberts]{lu2021revisiting}
Lu, C., Ball, P., Parker-Holder, J., Osborne, M., and Roberts, S.~J.
\newblock Revisiting design choices in offline model based reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lynch \& Sermanet(2021)Lynch and Sermanet]{lynch2021language}
Lynch, C. and Sermanet, P.
\newblock Language conditioned imitation learning over unstructured data.
\newblock \emph{Proceedings of Robotics: Science and Systems}, 2021.

\bibitem[Lyu et~al.(2022)Lyu, Ma, Yan, and Li]{lyu2022efficient}
Lyu, J., Ma, X., Yan, J., and Li, X.
\newblock Efficient continuous control with double actors and regularized
  critics.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  7655--7663, 2022.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{Mnih2013PlayingAW}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.~A.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{ArXiv}, abs/1312.5602, 2013.

\bibitem[Nachum et~al.(2019)Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019.

\bibitem[Nash(1951)]{nash1951non}
Nash, J.
\newblock Non-cooperative games.
\newblock \emph{Annals of mathematics}, pp.\  286--295, 1951.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Paine et~al.(2020)Paine, Paduraru, Michi, Gulcehre, Zolna, Novikov,
  Wang, and de~Freitas]{paine2020hyperparameter}
Paine, T.~L., Paduraru, C., Michi, A., Gulcehre, C., Zolna, K., Novikov, A.,
  Wang, Z., and de~Freitas, N.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.09055}, 2020.

\bibitem[Paster et~al.(2021)Paster, McIlraith, and Ba]{paster2020planning}
Paster, K., McIlraith, S.~A., and Ba, J.
\newblock Planning from pixels using inverse dynamics models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Peters \& Schaal(2007)Peters and Schaal]{peters2007reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  745--750, 2007.

\bibitem[Peters et~al.(2010)Peters, Mulling, and Altun]{peters2010relative}
Peters, J., Mulling, K., and Altun, Y.
\newblock Relative entropy policy search.
\newblock In \emph{Twenty-Fourth AAAI Conference on Artificial Intelligence},
  2010.

\bibitem[Pinto \& Gupta(2016)Pinto and Gupta]{pinto2016supersizing}
Pinto, L. and Gupta, A.
\newblock Supersizing self-supervision: Learning to grasp from 50k tries and
  700 robot hours.
\newblock In \emph{2016 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  3406--3413. IEEE, 2016.

\bibitem[Rezaeifar et~al.(2022)Rezaeifar, Dadashi, Vieillard, Hussenot, Bachem,
  Pietquin, and Geist]{rezaeifar2022offline}
Rezaeifar, S., Dadashi, R., Vieillard, N., Hussenot, L., Bachem, O., Pietquin,
  O., and Geist, M.
\newblock Offline reinforcement learning as anti-exploration.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  8106--8114, 2022.

\bibitem[Santos(1996)]{santos1996equivalence}
Santos, R.~J.
\newblock Equivalence of regularization and truncated iteration for general
  ill-posed problems.
\newblock \emph{Linear algebra and its applications}, 236:\penalty0 25--33,
  1996.

\bibitem[Savinov et~al.(2018)Savinov, Dosovitskiy, and Koltun]{savinov2018semi}
Savinov, N., Dosovitskiy, A., and Koltun, V.
\newblock Semi-parametric topological memory for navigation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Scherrer(2013)]{scherrer2013improved}
Scherrer, B.
\newblock Improved and generalized upper bounds on the complexity of policy
  iteration.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Siegel et~al.()Siegel, Springenberg, Berkenkamp, Abdolmaleki, Neunert,
  Lampe, Hafner, Heess, and Riedmiller]{siegel2020keep}
Siegel, N., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert, M.,
  Lampe, T., Hafner, R., Heess, N., and Riedmiller, M.
\newblock Keep doing what worked: Behavior modelling priors for offline
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[Singh et~al.(2019)Singh, Yang, Hartikainen, Finn, and
  Levine]{singh2019end}
Singh, A., Yang, L., Hartikainen, K., Finn, C., and Levine, S.
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock \emph{arXiv preprint arXiv:1904.07854}, 2019.

\bibitem[Srivastava et~al.(2019)Srivastava, Shyam, Mutz, Ja{\'s}kowski, and
  Schmidhuber]{srivastava2019training}
Srivastava, R.~K., Shyam, P., Mutz, F., Ja{\'s}kowski, W., and Schmidhuber, J.
\newblock Training agents using upside-down reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.02877}, 2019.

\bibitem[Sun et~al.(2019)Sun, Li, Liu, Zhou, and Lin]{sun2019policy}
Sun, H., Li, Z., Liu, X., Zhou, B., and Lin, D.
\newblock Policy continuation with hindsight inverse dynamics.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tung et~al.(2018)Tung, Harley, Huang, and Fragkiadaki]{tung2018reward}
Tung, H.-Y., Harley, A.~W., Huang, L.-K., and Fragkiadaki, K.
\newblock Reward learning from narrated demonstrations.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  7004--7013, 2018.

\bibitem[Vieillard et~al.(2020)Vieillard, Kozuno, Scherrer, Pietquin, Munos,
  and Geist]{vieillard2020leverage}
Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M.
\newblock Leverage the average: an analysis of kl regularization in
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12163--12174, 2020.

\bibitem[Villaflor et~al.(2020)Villaflor, Dolan, and
  Schneider]{villaflor2020fine}
Villaflor, A., Dolan, J., and Schneider, J.
\newblock Fine-tuning offline reinforcement learning with model-based policy
  optimization.
\newblock \emph{Offline Reinforcement Learning Workshop at Neural Information
  Processing Systems}, 2020.

\bibitem[Wahba(1987)]{wahba1987three}
Wahba, G.
\newblock Three topics in ill-posed problems.
\newblock In \emph{Inverse and ill-posed problems}, pp.\  37--51. Elsevier,
  1987.

\bibitem[Wang et~al.(2018)Wang, Xiong, Han, Liu, Zhang,
  et~al.]{wang2018exponentially}
Wang, Q., Xiong, J., Han, L., Liu, H., Zhang, T., et~al.
\newblock Exponentially weighted imitation learning for batched historical
  data.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Wang et~al.(2021)Wang, Wu, Salakhutdinov, and
  Kakade]{wang2021instabilities}
Wang, R., Wu, Y., Salakhutdinov, R., and Kakade, S.
\newblock Instabilities of offline rl with pre-trained neural representation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10948--10960. PMLR, 2021.

\bibitem[Wang et~al.(2020)Wang, Novikov, Zolna, Merel, Springenberg, Reed,
  Shahriari, Siegel, Gulcehre, Heess, et~al.]{wang2020critic}
Wang, Z., Novikov, A., Zolna, K., Merel, J.~S., Springenberg, J.~T., Reed,
  S.~E., Shahriari, B., Siegel, N., Gulcehre, C., Heess, N., et~al.
\newblock Critic regularized regression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7768--7778, 2020.

\bibitem[Wen et~al.(2021)Wen, Kumar, Gummadi, and
  Schuurmans]{wen2021characterizing}
Wen, J., Kumar, S., Gummadi, R., and Schuurmans, D.
\newblock Characterizing the gap between actor-critic and policy gradient.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11101--11111. PMLR, 2021.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Yang et~al.(2021)Yang, Fang, Han, Du, Luo, and Li]{Yang2021MHERMH}
Yang, R., Fang, M., Han, L., Du, Y., Luo, F., and Li, X.
\newblock {MHER}: Model-based hindsight experience replay.
\newblock \emph{ArXiv}, abs/2107.00306, 2021.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{yu2021combo}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 28954--28967, 2021.

\bibitem[Zhou et~al.(2021)Zhou, Bajracharya, and Held]{zhou2020plas}
Zhou, W., Bajracharya, S., and Held, D.
\newblock Plas: Latent action space for offline reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1719--1735. PMLR, 2021.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Ziebart, B.~D.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\bibitem[Zolna et~al.(2020)Zolna, Novikov, Konyushkova, Gulcehre, Wang, Aytar,
  Denil, de~Freitas, and Reed]{zolna2020offline}
Zolna, K., Novikov, A., Konyushkova, K., Gulcehre, C., Wang, Z., Aytar, Y.,
  Denil, M., de~Freitas, N., and Reed, S.
\newblock Offline learning from demonstrations and unlabeled experience.
\newblock \emph{arXiv preprint arXiv:2011.13885}, 2020.

\end{thebibliography}
