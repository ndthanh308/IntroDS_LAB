
\section{\ours Pseudocode}

We provide the complete pseudocode for \ours in Algorithm~\ref{alg:dp_wil}.

% Figure environment removed

\section{Hyperparameters}
\label{app:hparams}
\subsection{Error Budget Threshold}
The only hyperparameter we need for waypoint selection is $\eta$, the error threshold (Table~\ref{supp:table:waypoint_hyperparam}). The threshold $\eta$ is the same for all data sizes \{30, 50, 100, 200\} across all tasks on RoboMimic, i.e. $\eta=0.005$. We also use a consistent $\eta$ for both scripted data and human data on both tasks in the Bimanual Manipulation suite, i.e. $\eta=0.01$. Two out of three real-world tasks also use the same $\eta$; however, on the \textbf{Coffee Making} task, we opt for a lower $\eta$ to select more waypoints due to the high-precision nature of the task.


\input{table/hparam_waypoint}

\subsection{ACT in Bimanual Simulation Suite}
We use the same hyperparameters as the ACT paper \cite{zhao2023learning}, shown in Table~\ref{table:hparam_act}, except reducing the chunk size from $100$ to $50$.
Intuitively, as the length of trajectories reduces after running \ours, the chunk size can also be reduced to represent the same wall-clock time.

\begin{table*}[tbh!]
\centering
\setlength{\tabcolsep}{32pt}
\begin{tabular}{lll}
\toprule
\textbf{Hyperparameter} & \textbf{ACT} & \textbf{\ours + ACT} \\ \midrule
learning rate & 1e-5 & 1e-5 \\
batch size & 8 & 8 \\
\# encoder layers & 4 & 4 \\
\# decoder layers & 7 & 7 \\
feedforward dimension & 3200 & 3200 \\
hidden dimension & 512 & 512 \\
\# heads & 8 & 8 \\
chunk size & 100 & \textbf{50} \\
beta & 10 & 10 \\
dropout & 0.1 & 0.1 \\

\bottomrule
\end{tabular}
\caption{\small Hyperparameters of \ours+ACT and ACT. The only difference is the reduction in chunk size.}
\label{table:hparam_act}
\end{table*}



\subsection{Diffusion Policy in RoboMimic}
We use the exact same set of training hyperparameters as Diffusion Policy~\citep{chi2023diffusion} (Table~\ref{tab:hparam_transformer}). The only additional hyperparameter we added is the ``control multiplier'' (bottom row), which allows the low-level controller to take more steps to reach the target position at the inference time. This can be useful when predicted waypoints are far apart. 

\input{table/hparam_diffusion}


\subsection{A Guide to Hyperparameter Selection}

We suggest selecting an error threshold for new tasks based on a ratio of the number of waypoints to the average length of the trajectories. Our recommendation is to aim for a ratio of approximately 1:8, which can be automatically calculated using the waypoint generation script in our codebase. The ideal ratio may vary depending on the specific task and control frequency. Based on our empirical findings, a ratio between 1:5 and 1:15 tends to effectively reduce the policy horizon while still maintaining an accurate approximation of the trajectories.

For tasks involving real robots using ALOHA hardware \cite{zhao2023learning}, we advise turning on temporal ensembling (Sec~\ref{sec:ta}) to ensure smoother actions. Nonetheless, if the policy appears overly hesitant, two potential remedies are: (a) disabling temporal ensembling, and (b) increasing DT to emulate a blocking controller, where DT refers to the time interval between each update in a simulation or a control loop.


\section{Implementation and Experiment Details}
\label{app:implementation}
\subsection{Controller}
\label{sec:controller}
We use an Operation Space Controller (OSC) in RoboMimic, which allows position and orientation control of the robotâ€™s end-effector. It takes in the desired absolute position and orientation of the end-effector, and computes the necessary torques and velocities. 

We use the default joint position controller in the Bimanual Manipulation suite. On real-world tasks, we made no change to the controller except for the \textbf{Coffee Making} task, where we increased the step time from 0.02 to 0.1. This allows the controller to operate closer to a blocking controller, and execute low-level actions longer until reaching the desired joint position.

\subsection{Loss Function}
To determine the distance between potential waypoints and the ground truth trajectory, we project the ground truth state onto the linearly interpolated waypoint trajectory and compute the L2 distance for xyz position. For orientation, we convert the axis angles to quaternions and slerp two ground truth quaternions to determine the projection. Then we sum the position and orientation distances as the state loss. For the trajectory loss, we take a max over all states.


\subsection{Temporal Ensemble}
\label{sec:ta}

For all the ACT experiments, we adopt a \textit{temporal ensemble} technique as in the original paper~\cite{zhao2023learning}. Temporal ensembling is an approach to improve the smoothness of action chunking in robotic tasks. It queries the policy at each timestep, creating overlapping chunks and multiple predicted actions for each timestep. These predictions are then combined via a weighted average using an exponential weighting scheme, $w_i = \exp(-m \times i)$, that helps in smoothly incorporating new observations. This technique enhances the precision and smoothness of motion without any additional training cost, but requires extra computation during inference. We refer readers to \citet{zhao2023learning} for more details.


\subsection{Computation Cost}
Computing waypoints is inexpensive, especially compared to the training budget. The wall clock time for labeling one trajectory in \textbf{Lift} is 0.8 seconds on average.

\input{fig/robomimic}