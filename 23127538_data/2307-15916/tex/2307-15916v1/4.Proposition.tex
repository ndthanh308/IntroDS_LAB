\section{Our proposals}

%Spatial-based graph attention network (GAT)~\cite{velivckovic2018graph} can be adopted to encode the spatial relationships within the sensor network. As a weighted message-passing process, GAT is different from normal graph convolutional networks (GCNs)~\cite{zuo2023graph}. In GAT, the relations between nodes are not only decided by nodes' distance, but also by their inherent feature similarities. 
%Compared with spectral-based graph neural network [25, 26], GAT can adapt to various network structures.

In this paper, we adopt Graph WaveNet~\cite{wu2019graph} as the backbone model, which consists of $l$ Spatio-Temporal (ST) Blocks. However, our proposed EGAT can be integrated to any spatio-temporal models with adaptations on graph network layers. We employ Temporal Convolution Network (TCN) to encode the temporal dynamics of the AQIs. Specifically, as shown in Figure~\ref{fig:system_structure}, we designed an Expandable Graph Attention Network (EGAT) to learn from the data with evolving graph structures. 
%We adopt residual connections between the input and output of each ST block to avoid the gradient vanishing problem. 
The output forecasting layer takes skip connections on the output of the final ST Block and the hidden states after each TCN module for final predictions.
% Figure environment removed

%We consider two practical scenarios: i) new sensors are deployed, generating data with expanded structure for training; ii) for inference, we predict air qualities in areas W/o sensors.

\subsection{Temporal Dynamics with Temporal Convolution Network}
Compared to RNN-based approaches, Temporal Convolution Network (TCN)~\cite{wu2019graph} allows handling long-range sequences in a parallel manner, which is critical in industrial scenarios considering the model efficiency. %The output of the TCN's last layer is a representation that captures temporal dynamics in history. 

Given an input air quality sequence embedding $H$= $f_{linear}(\mathcal{X}) \in \mathbb{R}^{N \times d \times T}$, a filter $\mathcal{F}\in \mathbb{R}^{1 \times \mathrm{K}}$, $\mathrm{K}$ is the temporal filter size, $\mathrm{K}=2$ by default. The dilated causal convolution operation of $H$ with $\mathcal{F}$ at time $t$ is represented as:
\begin{equation}
\small
    H \star \mathcal{F}(t)= \textstyle\sum_{s=0}^{\mathrm{K}}\mathcal{F}(s) H(t-\textbf{d} \times s) \in \mathbb{R}^{N \times d\times T'}
\end{equation}
where $\star$ is the convolution operator, $\textbf{d}$ is the dilation factor, $d$ is the embedding size, $T'$ is the generated sequence length.
%, which equals to one on the last layer. 
We define the output of a gated TCN layer as:
\begin{equation}
\small
    \textbf{h} = tanh(W_{\mathcal{F}^{1}} \star H) \odot \sigma(W_{\mathcal{F}^{2}} \star H) \in \mathbb{R}^{N \times d \times T'}
\end{equation}
where $W_{\mathcal{F}^{1}}$, $W_{\mathcal{F}^{2}}$ are learnable parameters, $\odot$ is the element-wise multiplication operator, $\sigma(\cdot)$ denotes Sigmoid function.

\subsection{Expandable Graph Attention Networks (EGATs)}
%The spatial sensor nodes are deployed in an incremental way. How to inherit the knowledge from data collected in the old sensor network and combine it with the data collected from the new network shows a critical, and challenging technical gap in real-life scenarios. Bridging this gap allows industrial players to make use of existing data collection infrastructures as a basic staring point, to enrich and personalize the air quality monitoring and forecasting services with newly deployed sensors. 
Graph attention network (GAT)~\cite{velivckovic2018graph}, as a weighted message-passing process, models neighboring nodes' relationships via their inherent feature similarities.
%Compared with spectral-based graph neural network [25, 26], GAT can adapt to various network structures.
%The input of a graph attention layer is a set of node features at time $t$, 
Given a set of air pollution features at time $t$: $\textbf{h}(t)$ = $\{h_1, h_2, ..., h_N\}, h_i\in \mathbb{R}^{N \times d}$ as input of a graph attention layer, following~\cite{velivckovic2018graph}, we define the attention score between node $i$, $j$ as: 
%apply one learnable linear transformation to each interconnected node to obtain sufficient expressive power of the high-level features, the \textit{attention coefficients} $e_{i j}$ :
\begin{equation}\label{eq:attention_score}
\small
\alpha_{i j}=\frac{\exp 
\left(
\operatorname{a}\left(W h_i, W h_j\right)
\right)
}{\sum_{k \in \mathcal{N}_i} \exp \left(
\operatorname{a}\left(W h_i, W h_k\right)
\right)} 
\end{equation}

where $W \in \mathbb{R}^{d \times d'}$ is a weight matrix, $\operatorname{a}$ is the attentional mechanism as mentioned in~\cite{velivckovic2018graph}: $\mathbb{R}^{d'} \times \mathbb{R}^{d'} \to \mathbb{R} $, and $\mathcal{N}_i$ is a set of neighbor nodes of $v_i$.
A \textit{multi-head attention} with a nonlinearity $\sigma$ is employed to obtain abundant spatial representation of $v_i$ with features from its neighbor nodes $\mathcal{N}_i$:
\begin{equation}
\label{eq:multi_head_GAT}
\small
h_{i}'=\sigma\left(\frac{1}{K} \sum_{k=1}^K \sum_{j \in \mathcal{N}_i} \alpha_{i j} W^k h_j\right)
\end{equation}
Therefore, the GAT layer in $i$-th ST Block can be defined as:
\begin{equation}\label{eq:gat}
\small
H_{i+1} = \sigma \left(\frac{1}{K} \sum_{k=1}^K \mathcal{A} \textbf{h}_{i} W^k \right)
\end{equation}
where $\mathcal{A}$=$\{\alpha_{ij}\} \in \mathbb{R}^{N \times N}$, $H_{i+1} \in \mathbb{R}^{N \times d' \times T}$, $W^k \in \mathbb{R}^{d \times d'}$.


\iffalse 
%% Graph alignment (graph restruction)? L(G_dist, f(G_atten)), to check the graph construction methods using different node representations
\subsubsection{Graph Alignment}
In order to express the structural information of graphs and reduces structure deviation caused by different source data distribution, the graph is reconstructed by output embeddings for structure-aware learning. We predict the likelihood of an edge existing between nodes $v_i$ and $v_j$, by multiplying learned embeddings $h_i$ and $h_j$ as follows:
$$
p\left(a_{i j} \mid h_i, h_j \right)=\operatorname{sigmoid}\left(\left(h_i\right)^T, h_j\right) .
$$
As such, the graph $\mathbf{A}$ can be constructed as
$$
\mathbf{A}=\operatorname{sigmoid}\left[\left(H\right)^T \cdot H\right]
$$
where $(\cdot)^T$ is the transpose of the embedding matrix.
% 

Then, the re-construction loss can be defined as:
\begin{equation}
    L = | A - \mathbf{A}|
\end{equation}
 where $A$ is the original adjacency matrix, which can be basically represented by the distances between sensors.
\fi

When expanding the graph with new sensor nodes, we scale up the GAT layers on new nodes while conserving the information learned over the old ones. Basically, new nodes can be considered during both model's training and inference.

%We aim to build a model $f$, which is firstly trained over a dataset $\{\mathcal{X}_{\tau}\}$ on a sensor network $G_{\tau}$ = $\{\mathcal{V}_{\tau}, \mathcal{E}_{\tau}\}$, and can be incrementally trained over $\{\mathcal{X}_{\tau'}\}$ on an expanded network $G_{\tau'}$. 
% several cases for inference
%For inference, given a sequence $\mathcal{X} \in \mathbb{R}^{N_{\tau'} \times F \times T}$ and a sensor network $G_{\tau'}$, the model $f$ can predict the $AQI$ for the next $T_{p}$ time steps $\mathcal{Y}$=$\{y_t\}_{t = T+1}^{T+T_{p}} \in \mathbb{R}^{N\times T_{p}}$, where $N_{\tau'} \geq N_{\tau}$. 
%When $N_{\tau'} > N_{\tau}$, the model $f$ can predict the $AQI$ of any specific areas, without feeding the area's data for training.

\subsubsection{Expandable Graph Network Training}
We consider that the sensor network expands with the newly built infrastructures. The model learned from $G_{\tau}$ can be updated with recent data over $G_{\tau'}$ without re-training the model from scratch. 

From Equation \ref{eq:gat}, with new embeddings $\mathbf{h}_{\tau'}$$\in$$\mathbb{R}^{N_{\tau'} \times d \times T}$, the weight matrix $W^{k}$ stays unchanged; only the adjacency matrix requires updates: $\mathcal{A}_{\tau} \in \mathbb{R}^{N_{\tau} \times N_{\tau}}$ $\to$ $\mathcal{A}_{\tau'}\in \mathbb{R}^{N_{\tau'} \times N_{\tau'}}$.
%%
We re-define $\mathcal{N}_i$=$\{\mathcal{N}_{i,\tau}, \mathcal{N}_{i,\tau'}\}$ as the $k$ nearest neighbors of $v_{i}$, where $\mathcal{N}_{i,\tau}$ denotes neighbors from existing nodes, $\mathcal{N}_{i,\tau'}$ indicates those from newly added nodes. 
%%
Given a set of new sensors $\Delta \mathcal{V}_{\tau}$, we obtain new edge connections $\Delta \mathcal{E}_{\tau}$=$\{\mathcal{N}_{i}\}_{i=1}^{\Delta N}$, where $\Delta N$=$N_{\tau'}-N_{\tau}$, with $\mathcal{O}(N_{\tau'} \Delta N)$ time for distance computations.
According to Equation~\ref{eq:attention_score}, the attentional mechanism will apply to $\Delta \mathcal{E}_{\tau}$ with $\mathcal{O}(\Delta N k)$ time. Therefore, the attention score between node $i, j$ can be re-defined as:
\begin{equation}\label{eq:attention_score_new}
\footnotesize
\alpha_{i j}=\frac{\exp 
\left(
\operatorname{a}\left(W h_i, W h_j\right)
\right)
}{\sum\limits_{k \in \mathcal{N}_{i,\tau}} \! \exp \left(
\operatorname{a}\left(W h_i, W h_k\right)
\right)
\!+\!
\sum\limits_{k \in \mathcal{N}_{i,\tau'}} \! \exp \left(
\operatorname{a}\left(W h_i, W h_k\right)
\right)
} 
\end{equation}
In this manner, we can update the graph layer, i.e., $\mathcal{A}_{\tau'}$ incrementally by considering cached attention scores over $\mathcal{E}_{\tau}$, reducing the time complexity to $\mathcal{O}(N_{\tau'} \Delta N + \Delta N k)$. 
This is much faster than rebuilding the entire graph layer ($\mathcal{O}(N_{\tau'}^{2})$).
%In other words, when applying GAT, we need to re-calibrate the attention scores between the graph nodes. 
%Instead of rebuilding the model from scratch, the incremental learning process requires $\mathcal{O}(N'^{2}\text{-} N^{2})$ extra computation time, which calculates the relation scores between newly added and existing sensor nodes. In fact, it is not necessary because only the neighbors within the order $k$ are affected. %(cited from Wang et al., CIKM'20, Streaming GNNs via Continual Learning) An improved idea is to use breadth-first-search (BFS) to find all the L-order neighbors and calculate their scoring function, but the complexity is still related to the size of the neighborhood of Î”G. 
%Therefore, we generate a sub-graph by selecting neighbor nodes of newly added nodes on $\mathcal{G}$ within k-hops. This will greatly reduce the processing time to $\mathcal{O}((\Delta N k^{2})^2)$ by updating a sub-graph $\mathcal{G}_{sub}$ with $\Delta N$ nodes. % only use the new-sensor data to calculate the loss?
% Q1: how to compute the adjacency matrix? -> 
% Q2: attention and distanced-based adjacency matrix value
% Q3: why using GAT, but not a normal GCN layer?  
%% Answer to Q2: for training, we use attention-based (GAT), for inference, we use distance-based (GCN)
%% Answer to Q3: GAT can better capture the information in each node for enriching the node's embedding
% TODO: to clarify the time complexity for computing attention scores, formulate the complexity in math representation




\iffalse 
\subsubsection{Expandable Graph Network Inference}
During the inference step, the sensor network can be different from the existing one, with an expanded structure. The objective is to forecast the values in unseen sensors, which are not learned in the model. In this case, only the spatial location of the desired area is available. We aim to forecast the air quality of a specific area without any sensors. As the graph alignment allows constraining the graph structure to the distance-based graph topology, the air quality on unknown areas can be obtained by simply weighting the data from connected sensors. 
\begin{equation}
    Y_i = \sum_{j\in \mathcal{N}_i} a_{ij} Y_j, N_i = \{v_j | dist(v_i, v_j) < \varepsilon \}
\end{equation}
where $a_{ij} \in A$ is the reversed distance between $v_i$ and $v_j$, $\varepsilon$ is a threshold which decides the neighboring sensor nodes. 
% need to compare between i) model trained on new sensor data ii) model not trained on new sensor data  
% Q: do we have the data from new sensors? 1) without data -> more practical, can infer the air quality in any areas (worse performance); 2) with data -> re-train the model with updated sensor network (better performance)
\fi


% consider multiple scenarios for model inference
\subsubsection{Expandable Graph Network Inference}
When no sensors are installed in (unseen) areas, \textit{Spatial Smoothing} can be performed on the unseen node $v_i$. Based on its spatial location, we incorporate predictions from its neighbor nodes:
\begin{equation}
    Y_i = \sum_{j\in \mathcal{N}_i} a_{ij} Y_j, N_i = \{v_j | dist(v_i, v_j) < \varepsilon \}
\end{equation}
where $\mathcal{N}_i$ is the first-order neighbors of $v_i$ (excluding $v_i$, as the data on $v_i$ is unavailable), $a_{ij}=1-\frac{dist(v_{i}, v_{j})}{\sum_{k\in \mathcal{N}_{i}} dist(v_{i}, v_{k})}$ is the inverse Euclidean Distance (ED) between $v_i$ and $v_j$, $\varepsilon$ is a threshold which decides the neighboring sensor nodes. 

We propose a robust \textit{Spatial Representation Smoothing} technique that considers richer spatial relationships, in the embedding space, between unseen and existing nodes. Given an unseen node $v_i$, its embedding $h_i$ can be defined as follows:
\begin{equation}
\small
h_{i}=\sigma\left(\frac{1}{K} \sum_{k=1}^K \sum_{j \in \mathcal{N}_i} a_{i j} W^k h_j\right )
\end{equation}
where $a_{ij}$ is the inverse ED between $v_i$ and $v_j$, $W^{k}$ is the learned weights in each attention head as shown in Equation~\ref{eq:multi_head_GAT}.

    
% need to compare between i) model trained on new sensor data ii) model not trained on new sensor data  (inference W/o new sensor data) 

% Q: do we have the data from new sensors? 1) without data -> more practical, can infer the air quality in any areas; 2) with data -> sensors should be installed, may be more convincing for the paper/model performance; arguments for this point: no training effort, just forecast the air quality on newly installed sensors; 3) with data -> re-train the model with updated sensor network



\iffalse
\subsection{Dynamic Evolving Data Distributions}
% can be put in future work, as there is a huge effort to explore/collect the data with corresponding dynamic features 
The air quality data distribution evolves with time. Thus, a stable model can not adapt to the recent distribution, leading to deteriorated performance when forecasting real-time air quality. Re-training the model can be a simple solution, however, it requires a careful selection of the re-training period, a considerable re-training effort, and a non-ignorable re-training latency. 
%(To consider transfer learning or meta-learning)
\fi


\subsection{Output Forecasting Layer}
For final predictions, we take skip connections as shown in~\cite{wu2019graph} on the final ST Block's output and hidden states after each TCN.
The concatenated output features are defined as:
\begin{equation}
\small
    O = (\textbf{h}_{0} W^{0} + b^{0})\| ... \| (\textbf{h}_{l-1} W^{l-1} + b{l-1}) \ \| (\mathcal{H}_{l} W^{l} + b^{l})
\end{equation}
where $O \in \mathbb{R}^{N\times (l+1)d}$, $W_{s}^{i}$, $b_{s}^{i}$ are learnable parameters for the convolution layers. Two fully-connected layers are added to project the concatenated features into the desired dimension:
\begin{equation}
\small
    \hat{\mathcal{Y}} = (ReLU(OW_{fc}^{1} +  b_{fc}^{1})) W_{fc}^{2}  +  b_{fc}^{2} \in \mathbb{R}^{N\times T_{p}}
\end{equation}
where $W_{fc}^{1}$, $W_{fc}^{2}$, $b_{fc}^{1}$, $b_{fc}^{2}$ are learnable parameters. We use mean absolute error (MAE) \cite{wu2019graph} as loss function for training.

