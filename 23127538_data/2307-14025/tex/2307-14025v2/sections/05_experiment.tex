\section{Experiments}
%
We evaluate TR-MIL on different datasets: MIL benchmarks, synthetic MIL datasets, and a real-world biomedical dataset for anemia classification.
%
\subsection{MIL Benchmarks}
%
\begin{table*}[]
    \caption{Topological regularization improves the classification performance of RGMIL (SOTA) on MIL benchmarks. 
    Among previous methods, we specifically reimplemented RGMIL for our analysis. Other results (gray) are collected from papers proposed by \citet{ilse2018attention} (APMIL and GAPMIL), \citet{yan2018deep} (DPMIL), \citet{li2021dual} (DSMIL), \citet{huang2022bag} (BDRMIL), and \citet{du2023rgmil} (RGMIL).}
    \label{tab:benchmark}
    \renewcommand{\arraystretch}{1.2}
    \begin{tabularx}{\textwidth}{lXXXXX}
    \toprule
        Method               
        & MUSK1 
        & MUSK2
        & FOX 
        & TIGER
        & ELEPHANT
        \\
    \midrule
        \textcolor{gray}{APMIL (2018)}            
        & \textcolor{gray}{0.892$\pm$0.040}
        & \textcolor{gray}{0.858$\pm$0.048}
        & \textcolor{gray}{0.615$\pm$0.043}
        & \textcolor{gray}{0.839$\pm$0.022}
        & \textcolor{gray}{0.868$\pm$0.022}
        \\
        \textcolor{gray}{GAPMIL (2018)}        
        & \textcolor{gray}{0.900$\pm$0.050}
        & \textcolor{gray}{0.863$\pm$0.042}
        & \textcolor{gray}{0.603$\pm$0.029}
        & \textcolor{gray}{0.845$\pm$0.018}
        & \textcolor{gray}{0.857$\pm$0.027}
        \\
        \textcolor{gray}{DPMIL (2018)}              
        & \textcolor{gray}{0.907$\pm$0.036}
        & \textcolor{gray}{0.926$\pm$0.043}
        & \textcolor{gray}{0.655$\pm$0.052}
        & \textcolor{gray}{0.897$\pm$0.028}
        & \textcolor{gray}{0.894$\pm$0.030}
        \\
        \textcolor{gray}{DSMIL (2021)}     
        & \textcolor{gray}{0.932$\pm$0.023}
        & \textcolor{gray}{0.930$\pm$0.020}
        & \textcolor{gray}{0.729$\pm$0.018}
        & \textcolor{gray}{0.869$\pm$0.008}
        & \textcolor{gray}{0.925$\pm$0.007}
        \\
        \textcolor{gray}{BDRMIL (2022)}                 
        & \textcolor{gray}{0.926$\pm$0.079}
        & \textcolor{gray}{0.905$\pm$0.092}
        & \textcolor{gray}{0.629$\pm$0.110}
        & \textcolor{gray}{0.869$\pm$0.066}
        & \textcolor{gray}{0.908$\pm$0.054}
        \\
        \midrule
        RGMIL (2023)           
        & 0.940$\pm$0.070
        & 0.920$\pm$0.106
        & 0.714$\pm$0.107
        & 0.842$\pm$0.088
        & 0.915$\pm$0.042
        \\
        TR-RGMIL (ours)      
        & \textbf{0.946$\pm$0.078}
        & \textbf{0.970$\pm$0.042}
        & \textbf{0.747$\pm$0.054}
        & \textbf{0.961$\pm$0.040}
        & \textbf{0.941$\pm$0.054}   
        \\
        \hline
    \end{tabularx}
\end{table*}


We evaluate TR-MIL on five classic MIL benchmark datasets. 
%
These include three image-based datasets (FOX, TIGER, and ELEPHANT), each comprising $200$ bags, introduced by \citet{dietterich1997solving}. 
%
For these datasets, instead of actual images, we only have extracted features from tiled image patches (instances) representing parts of an image. 
%
Additionally, we employ MUSK1 and MUSK2 datasets, introduced by \citet{andrews2002support}, which contain data on $92$ and $102$ molecules, respectively. 
%
In these datasets, each molecule is represented by a bag of instances, with each instance corresponding to a different molecular conformation.
%
The number of instances per bag ranges from as few as $1$ to as many as $1044$, providing a comprehensive assessment of our model's adaptability and robustness across different scales of data representation.
%

We utilized an identical encoder architecture to clarify and ensure an equitable comparison with the existing state-of-the-art MIL method (RGMIL). 
%
This architecture includes $2$ linear layers with a ReLU activation, projecting input features into a $512$-dimensional space for both layers. 
%
The primary modification in our setup is integrating a topological signature calculator into the input and instance encoder.
%

The original RGMIL model used $231$ features for FOX, TIGER, and ELEPHANT datasets and $167$ features for MUSK1 and MUSK2 datasets, including a last feature representing the repeated label of the bag for each instance. 
%
However, in our re-implementation, we followed the standard benchmark settings of $230$ and $166$ features for the respective datasets to align with previous works and provide a comprehensive comparison (see Table \ref{table:bm_mil_model}).
%
We run both the RGMIL and TR-RGMIL models $5$ times, applying $10$-fold cross-validation and reporting the average optimal performance of the model during the training.
%
When using this instance feature vector, we observed a decline in RGMIL's performance, with TR-RGMIL still outperforming all other methods in all five standard MIL benchmarks (Table \ref{tab:benchmark}).

Our experiments reveal that the RGMIL model is prone to overfitting if no topological regularization is being used.
%
This phenomenon is characterized by the MIL classifier exhibiting its best performance during the initial epochs of training.
%
Topological regularization effectively mitigates this overfitting issue (Figure \ref{fig:Benchmarks_LC}). 
%






\subsection{Synthetic Datasets}
%
% Figure environment removed

% Figure environment removed

For evaluating the robustness of TR-MIL framework across varied MIL problem definitions, including instance image complexity, number of training bags, and bag sizes, we draw on the methods outlined by \citet{ilse2018attention}. 
%
To consider the inherent complexity of instance images, we create two synthetic datasets: the first comprising MNIST images as instances (MIL-MNIST), and the second bags of Fashion-MNIST images~\citep{xiao2017fashion} as instances (MIL-FashionMNIST), providing a more challenging scenario with complex visual data.  
%
In MIL-MNIST, the digit ``9'' is considered a positive instance, while all other digits are considered negative instances. 
%
In MIL-FashionMNIST, the label ``Dress'' is considered a positive instance, while other labels showcase negative instances. 
%
We construct distinct training datasets containing a total number of $10$, $14$, $20$, $50$, $100$, and $200$ bags to evaluate the influence of the quantity of training data. 
%
Additionally, we explore different amounts of instances per bag, sampling them from Gaussian distributions with mean and standard deviations defined as $(10, 2)$, $(50, 10)$, and $(100, 20)$, respectively. 
%
Positive bags are defined as those containing at least one positive instance, accounting for up to $20\%$ of the instances within the bag. 

\paragraph{Models.}
%
We use a deep instance encoder architecture introduced by \citet{ilse2018attention} (see Table \ref{table:cv_mil_model} for details).
%
It consists of two convolutional layers with a kernel size of $5$, a stride of $1$, and ReLU activation functions. 
%
These layers generate $20$ and $500$ feature maps, respectively. 
This is followed by a fully-connected layer. 
%
The output from this encoder is a $500$-dimensional feature vector, which then undergoes further processing in the aggregation function. 
%
The attention network comprises two linear layers, resulting in a final output dimension of $128$ followed by $1$.
%
The topological signature of input instances is calculated on image space and latent space, applying pixel-vise Euclidean distance of instance images and latent feature vectors (Figure \ref{fig:diagram}.

\paragraph{Results.}
%
We evaluate the effectiveness of topological regularization on three aggregation functions in MIL, max pooling, average pooling, attention-based pooling, which serves as the baseline for numerous studies in the field~\citep{ilse2018attention}, in addition to the regressor-guided pooling technique, recognized as the state-of-the-art~\citep{du2023rgmil}.
%
% For the evaluation metric, we choose the F1-score over accuracy because it provides more informative insights by simultaneously accounting for both false positives and false negatives.
%
We analyze the average F1-score and its standard deviation for different numbers of training bags (Figure \ref{fig:Mnist_Fmnist}) and bag sizes (Figure \ref{fig:Mnist_Fmnist_detailed}) over five runs on MIL-MNIST and MIL-FashionMNIST datasets. 
%
Without topological regularization, models trained with few training bags perform poorly, akin to random guessing, due to overfitting (Learning curves are shown in Figure \ref{fig:Mnist_LC}). 
%
Adding topological regularization provides a reasonable complexity for the encoder to resolve overfitting and lets the encoder learn a more meaningful latent representation of data.
%
Consequently, it improves the MIL model performance across both datasets.
%
Notably, topological regularization narrows the performance gap between basic aggregations of max pooling and average pooling compared to advanced techniques of attention and regressor-guided pooling.
%
This demonstrates the crucial role of accounting for a bag's topological structure in enhancing MIL classification, surpassing the impact of the aggregation function, as evidenced in our toy experiment.
%




\subsection{Anemia Classification}

\begin{table*}[t]
    \caption{Topological regularization improves classification performance for all pooling strategies for Anemia classification. We apply it to different MIL methods with average/max pooling, attention-based pooling~\cite{sadafi2020attention}, and anomaly-aware pooling~\cite{kazeminia2022anomaly}. Numbers show the average classification performance along with the standard deviation from  3 cross-validation and 3 runs. Best performance is indicated by bold text. Additionally, for each pooling method, we compare the classification performance without (\xmark) and with (\cmark) topological regularization, and the winner is underlined for clarity.}
    \label{tab:performance}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{lcccccccc}
        \toprule
        & \multicolumn{2}{c}{Average pooling}                                            
        & \multicolumn{2}{c}{Anomaly pooling}                        
        & \multicolumn{2}{c}{Attention pooling}
        & \multicolumn{2}{c}{Max pooling}                  
        \\ \midrule
        \multicolumn{1}{c}{\begin{tabular}[c]{@{}l@{}}Topological 
        \\regularization\end{tabular}} 
        & \xmark
        & \multicolumn{1}{c}{\cmark}                        
        & \xmark
        & \multicolumn{1}{c}{\cmark}               
        & \xmark
        & \multicolumn{1}{c}{\cmark}               
        & \xmark
        & \cmark               
        \\ \midrule
        \multicolumn{1}{l}{Accuracy}                                                              
        & \multicolumn{1}{c}{72.25$\pm$7.0} 
        & \multicolumn{1}{c}{{\ul \textbf{81.29$\pm$2.5}}} 
        & \multicolumn{1}{c}{77.85$\pm$3.7} 
        & \multicolumn{1}{c}{{\ul 79.50$\pm$1.2}} 
        & \multicolumn{1}{c}{73.72$\pm$3.8} 
        & \multicolumn{1}{c}{{\ul 77.76$\pm$1.6}} 
        & \multicolumn{1}{c}{64.33$\pm$5.8} 
        & {\ul 71.44$\pm$5.6} 
        \\
        \multicolumn{1}{l}{F1-Score}                                                              
        & \multicolumn{1}{c}{70.47$\pm$7.4} 
        & \multicolumn{1}{c}{{\ul \textbf{80.28$\pm$3.1}}} 
        & \multicolumn{1}{c}{76.69$\pm$4.0} 
        & \multicolumn{1}{c}{{\ul 77.01$\pm$1.8}} 
        & \multicolumn{1}{c}{72.38$\pm$3.8} 
        & \multicolumn{1}{c}{{\ul 74.69$\pm$1.6}} 
        & \multicolumn{1}{c}{62.96$\pm$5.0}   
        & {\ul 68.77$\pm$5.4} 
        \\
        \multicolumn{1}{l}{AUROC}                                                                 
        & \multicolumn{1}{c}{89.88$\pm$2.7} & \multicolumn{1}{c}{{\ul \textbf{93.72$\pm$4.4}}} 
        & \multicolumn{1}{c}{89.05$\pm$4.3} & \multicolumn{1}{c}{{\ul 90.89$\pm$2.5}} 
        & \multicolumn{1}{c}{91.58$\pm$3.0} & \multicolumn{1}{c}{{\ul 91.88$\pm$2.5}} 
        & \multicolumn{1}{c}{84.83$\pm$2.8} & {\ul 89.73$\pm$3.1} 
        \\
        \multicolumn{1}{l}{Recall}                                                                 
        & \multicolumn{1}{c}{59.68$\pm$7.8} & \multicolumn{1}{c}{{\ul \textbf{65.12$\pm$5.0}}} 
        & \multicolumn{1}{c}{63.24$\pm$3.2} & \multicolumn{1}{c}{{\ul 65.99$\pm$3.3}} 
        & \multicolumn{1}{c}{59.31$\pm$6.4} & \multicolumn{1}{c}{{\ul 60.42$\pm$2.3}} 
        & \multicolumn{1}{c}{52.77$\pm$8.6} & {\ul 53.75$\pm$4.7} 
        \\
        \multicolumn{1}{l}{Precision}                                                                 
        & \multicolumn{1}{c}{61.77$\pm$7.1} & \multicolumn{1}{c}{{\ul \textbf{79.06$\pm$12.0}}} 
        & \multicolumn{1}{c}{67.36$\pm$4.5} & \multicolumn{1}{c}{{\ul 69.67$\pm$4.6}} 
        & \multicolumn{1}{c}{64.89$\pm$4.9} & \multicolumn{1}{c}{{\ul 73.24$\pm$7.9}} 
        & \multicolumn{1}{c}{52.86$\pm$9.6} & {\ul 63.43$\pm$7.5} 
        \\ \bottomrule
    \end{tabular}%
    }
\end{table*}

The diagnosis of anemia relies on presence of the minority red blood cells in a patient's blood sample that shows morphological features associated with the disease. 
%
Anemia disorders lead to various aberrant shapes such as sickle-shaped (SCD), crumpled or perforated (thalassemia), star-shaped (Xero), or even spherical (HS) cells. 
%
These deformations can manifest with varying degrees of severity and in different proportions, while it is also possible for other cell types unrelated to anemia conditions to coexist.
%
Detecting the hallmark cells indicative of anemia poses a significant challenge due to substantial variability in expert opinions. 
%
% Furthermore, a few atypical cells do not indicate an anemia condition, complicating the diagnostic process.
%
This makes the \emph{manual} annotation of blood samples for supervised model training a laborious and costly endeavor~\citep{kazeminia2022anomaly}. 
%
Lacking cell-level annotations, MIL is used in this context by treating cells as instances and blood samples as bags, with anemia types assigned to each blood sample~\citep{lu2020clinical}.
%


This dataset consists of $521$ microscopy images of blood samples obtained from patients who underwent various treatments at different times. 
%
Each sample comprises $4$ to $12$ images, each containing $12$ to $45$ cells.
%
The data is distributed among five classes, i.e.,
%
\begin{inparaenum}[(i)]
\item Sickle Cell Disease (SCD) with $13$ patients and $170$ samples, 
\item Thalassemia with only $3$ patients and $25$ samples,
\item Hereditary Xerocytosis with $9$ patients and $56$ samples,
\item Hereditary Spherocytosis (HS) with $13$ patients and $89$, as well as
\item healthy control group consisting of $33$ individuals and $181$ samples.
\end{inparaenum}
%
Given the rarity of disease samples in anemia, the dataset for the $5$-class Anemia classification task is exhibiting data scarcity of training bags, making its classification challenging.
%
Following previous research, we implement a patient-centric approach by dividing the dataset into three equivalent folds. 
%
This division allocates two folds for training and reserves one for test. 
%

In this application, \citet{kazeminia2022anomaly} introduces the state-of-the-art MIL approach, introducing anomaly scores derived from the Mahalanobis distance to a Gaussian mixture model for detecting negative instances in anemia classification. 
%
However, the effectiveness of this method is limited by the encoder's capacity to map negative instances without a direct learning signal.
%

In addition to data scarcity, the anemia dataset introduces an inherent ambiguity within the dataset:
Blood samples may contain a low deformed cell ratio that falls below a specified threshold to identify a disorder.
%
Such data introduces a significant challenge for attention-based and anomaly-aware pooling techniques, as they do not conform to the fundamental assumptions of these mechanisms.
% 
Consequently, it is not just the presence of positive instances that is critical, but also their \emph{proportion} in the data. 
%
We anticipate that geometry and topology can capture this nuanced information, helping us overcome this challenge.
%
Consistent with prior experiments, for a fair comparison, we apply topological regularization to this architectures~\citep{kazeminia2022anomaly}.
%
In previous works, the instance encoder contains $3$ convolutions followed by $2$ ReLu and Tanh activation functions, followed by $2$ linear layers to obtain a latent representation of instances in a $500$-dimensional space. 
%
The instance encoder's input is $4 \times 4 \times 256$ features extracted by a frozen encoder trained in a cell segmentation network. 
%
However, in our experiments, we capture the topological signature of each bag directly from the image data space and the $500$-dimensional latent space. We posit that features extracted by the segmentation model, irrespective of the cell type, may lack crucial shape information and thus potentially manipulate the topology of the bag (see Table \ref{table:mil_rbc_model}).

Following common practice in medical and biomedical evaluations, we employ five standard evaluation metrics: Accuracy, F1-Score, Area Under the Receiver Operating Characteristic Curve (AUROC), Precision, and Recall.
%
All metrics are macro-weighted because of the dataset imbalance.

\paragraph{Results.}

% Figure environment removed

% Figure environment removed

%
\cref{tab:performance} shows that topological regularization improves the performance of MIL models using \emph{all} aggregation functions, resulting in higher mean performance and often resulting in reduced variance.
%
Notably, topologically regularized MIL with \emph{average pooling} surpasses other aggregation schemes.
%
This aligns with our findings from experiments on synthetic datasets, where we observe that topological regularization particularly narrowing the gap between performance of the MIL employing different aggregation functions.
%
The inherent ambiguity in the anemia dataset for MIL suggests that enhancing instance projection in latent space via average pooling is more effective than attention pooling, as it better captures the ratio of positive instances. 
%
Without topological regularization, scarce training data impede the instance encoder from generating meaningful, generalizable latent representations. 
%
However, integrating topological inductive bias into the latent space mitigates these challenges, significantly improving model performance.

\paragraph{Instance-level analysis.}
%
We evaluate the influence of topological regularization on the instance-level explanation of the anomaly-aware MIL approach. 
%
Figure \ref{fig:interpretation} shows anomaly scores achieved with and without topological regularization. 
%
Without topological regularization, we observe a notable inconsistency: the anomaly detector assigns different anomaly scores to visually similar instances.
%
This inconsistency is mitigated when topological regularization is applied.
%
This is an important aspect of our analysis, revealing a challenge in the model's ability to evaluate similar data points uniformly and demonstrating the effectiveness of topological regularization in enhancing the model's explainability.
%
Further illustrating this point, we visualize the distance matrix of instances within a bag in the input space and compare them with their corresponding matrices in the latent space in scenarios with and without topological regularization in Figure \ref{fig:dists_umaps}.
%
This figure shows that MIL with topological regularization better preserves the distances between bag instances in the latent space projection compared to anomaly-aware MIL without regularization.
%
The figure indicates that, without topological regularization, only a few instances are projected far from the majority, elucidating the observed inconsistency in anomaly scores for deformed shapes.
%

In addressing potential inquiries regarding our choice of topological regularization over a distance-preservation-based loss, it's noteworthy to emphasize the distinct advantages of our approach. 
%
Topological regularization loss is particularly robust against noise and highly effective in high-dimensional spaces. 
%
It exhibits scale invariance, a critical feature that enables the preservation of the distance pattern of instances within a bag. 
%
This level of distance pattern preservation might not be as effectively achieved with a regularization loss focused solely on distance preservation. 
%
This aspect underscores the strategic advantage of our chosen method, confirming the efficacy of topological regularization in maintaining the integrity of instance relationships in the latent space, thereby enhancing both the model's performance and its explainability.


