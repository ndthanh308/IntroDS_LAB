\section{Introduction}

Multiple Instance Learning~(MIL) is a variant of weakly-supervised learning that operates without annotations for individual data samples. 
%
In MIL, each \emph{bag}, i.e., a group of instances, is assigned a \emph{single} label~\citep{lu2020clinical}.
%
A bag is labeled positive if it contains at least one positive instance and negative otherwise.
%
MIL-based deep classifiers require substantial training data for optimal performance, primarily due to the complexities inherent to backpropagation and the challenge of addressing diversities within bags:
The loss signal must effectively navigate through the aggregation function that ensures precise training of the model to represent each instance.
%
% Without an adequate aggregation function and training data, the accurate representation of instances is challenging, given the wide-ranging diversity of positive and negative instances.

MIL classifiers are widely used for biomedical applications like pathology and hematology disease classification~\citep{shao2021transmil, li2021dual, zhang2022dtfd, chen2022scaling, wagner2023transformer, sadafi2020attention, kazeminia2022anomaly, hehr2023explainable}.
%
Here, representing individual instances properly is pivotal in interpreting the model's reliability, especially in clinical decision-making contexts.
%
Unfortunately, data scarcity is a common problem in biomedical scenarios, particularly when dealing with rare diseases like rare anemias. 
%
In such cases, the need for MIL-based training approaches that operate in the scarce-data regime is paramount.
%

% Recent advances in MIL models tend to concentrate on refining aggregation functions while largely overlooking the challenges associated with the extensive training data requirements. 
% %
% These challenges are particularly pronounced in high-stakes applications with constrained data availability, where instance heterogeneity and data selection bias are prevalent~\cite {liu2022distributionally}.
%
% `Latent heterogeneity' refers to the subtle yet critical, underlying differences in data, such as the varied presence of the same instance type that may exist in a bag, for instance. 
% %
% `Data selection bias' occurs when training data is not representative of the broader application context, such as in models trained primarily on a majority class, leading to poor generalization for minority classes.
% 
Improving MIL under data scarcity necessitates leveraging additional structure from data via inductive biases~\citep{goyal2022inductive}.
%
Being able to capture fundamental organizational principles of data at multiple scales, topological algorithms\footnote{
Despite their name, these algorithms also capture geometrical aspects of data, but we will refrain from writing \emph{geometrical-topological algorithms} for brevity.
} recently arose as a source of such inductive biases, permitting the integration into deep learning models~\citep{Hensel21}.
%
The primary appeal of such algorithms lies in their robustness to noise and perturbations, resulting in \emph{stable multi-scale representations}.
%
When a pronounced geometrical-topological signal is present in the data, these algorithms improve interpretability, generalizability, and predictive performance~\citep{Horn22a, Waibel22a}, even in the presence of \emph{singular structures}, which preclude the use of standard techniques~\citep{vonRohrscheidt23a}.
%

% Figure environment removed

We introduce \emph{Topologically-Regularized Multiple Instance Learning (TR-MIL)}, a data-centered solution to address the challenges of training MIL with scarce training data.
%
By leveraging multi-scale shape descriptors on the level of MIL bags, we develop a novel regularization scheme that ensures the preservation of crucial geometrical-topological information in the latent space of our model (see \cref{fig:diagram} for a schematic overview).
%
Our regularization method improves generalization performance, exhibiting higher accuracy and robustness, as well as improved adaptability to data-scarcity.
%
The \textbf{main contributions} of our work are:
%
\begin{compactitem}
    %
    \item We introduce TR-MIL, the first method to improve generalizability of MIL trained with scarce data.
    %
    \item We demonstrate that maintaining the topological bias inherent in a bag's data distribution enhances the performance of MIL classifiers trained with variant amounts of data.
    %
    \item TR-MIL adapts to any MIL model aggregation strategy.
    %
    \item TR-MIL outperforms state-of-the-art on MIL benchmarks.
    %
    \item TR-MIL outperforms the state-of-the-art on rare anemia classification.
    % %
    % \item We make our frameworkâ€™s code and synthetic data publicly available, promoting transparency and reproducibility in research.
    %
\end{compactitem}
