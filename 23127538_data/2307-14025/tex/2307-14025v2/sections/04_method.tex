\section{Methods}
\label{method}

Our approach treats each bag as a point cloud in a high-dimensional space whose geometrical-topological features should be adequately captured by the model.
%
Each instance influences the bag's `shape,' with positive instances notably altering its shape in comparison to the distribution of negative samples. 
%
We thus need a descriptor that captures the characteristics of a point cloud, while remaining stable to perturbations and invariant under transformations like translations and rotations that are irrelevant for determining the overall shape.
%
Persistent homology provides such a suitable descriptor; \citet{sheehy2014persistent} demonstrates that critical topological-geometrical features captured by persistent homology are approximately even under projections or embeddings of the data, making it highly robust.
%
The calculation of persistent homology only requires a choice of distance metric. 
%
While our framework remains agnostic to the specific choice of distance metric, we opted to use the per-pixel Euclidean distance here since it is straightforward to calculate.
%
This enables us to transform the point cloud of a bag $X_{b_m}$ into a distance matrix $A^{X_{b_m}}$~(see Figure \ref{fig:diagram}).
%
Next, we use the distance matrix $A$ representing the bag's point cloud and calculate its Vietoris-Rips complex, \( \text{VR}(A, \epsilon) \), where points are connected if they lie within a distance \( \epsilon \) of each other, i.e.,
%
\begin{equation}
    \label{eq:PH}
    \text{VR}(A, \epsilon) = \{ \sigma \subseteq A \mid \forall a_i, a_j \in \sigma, d(a_i, a_j) \leq \epsilon \}.
\end{equation}
Here, \( d(a_i, a_j) \) denotes the distance between points \( a_i \) and \( a_j \) in \(A\).
%
The topology of $\text{VR}$ changes as we vary \( \epsilon \).
%
Formally, this leads to a filtration of simplicial complexes $\{\text{VR}(A, \epsilon_0), \text{VR}(A, \epsilon_1), \ldots, \text{VR}(A, \epsilon_m) \}$, with an ordered sequence of distance thresholds $0 = \epsilon_0 < \epsilon_1 < \ldots < \epsilon_m$ (Figure \ref{fig:diagram}). 
%
Persistent homology tracks the `birth' and `death' of topological features across this sequence, represented in a persistence diagram by points \( (\beta_1, \beta_2) \), where \( \beta_1 = \epsilon_i \) and \( \beta_2 = \epsilon_j \).
%
This diagram constitutes a summary of the shape of each bag, measured by multi-scale topological features like connected components ($0$D), loops ($1$D), and voids ($2$D) (Figure \ref{fig:diagram}) and yields a set of $d$-dimensional persistence diagrams, described in the form of \emph{persistence pairings} $\pi^{X_{b_{m}}}$ of points in the input space, representing the bag's topological signature. 
%
While our method generalizes to features of arbitrary dimensions, we focus on connected components for computational considerations.

Our objective is to inject this signature as an inductive bias into the model to enhance its robustness and predictive performance.
%
Thus, we capture the distance matrix $A^{Z_{b_{m}}}$ and signature of the bag's point cloud in the latent space $\pi^{Z_{b_{m}}}$) and define a loss term to penalize the encoder $f_{\theta}$ for any inconsistency in preserving the bag's signature during projection from input to latent space.
%
To this end, we utilize the topological regularization loss proposed by \citet{moor2020topological}, which addresses the challenge of backpropagating through topological descriptors. 
%
This approach retains topological features in the input space as prominent features in the latent representation by defining a loss $L_{X_{b_m}\rightarrow Z_{b_m}}$.
%
To enhance stability in the model's outcomes, it simultaneously penalizes topological features within the latent space that lack corresponding importance in the input domain by $L_{Z_{b_m}\rightarrow X_{b_m}}$.
%
The final topological regularization loss is defined as
%
\begin{align}
	\label{topo_loss}L_\mathrm{topo} & := L_{X_{b_m}\rightarrow Z_{b_m}} + L_{Z_{b_m}\rightarrow X_{b_m}},\\
\shortintertext{where}
	\label{topo_loss_1}
 L_{X_{b_m}\rightarrow Z_{b_m}}\!\!&:= \frac{1}{2}\left \| A^{X_{b_m}}\left [ {\pi^{X_{b_m}}} \right ] -\!\! A^{Z_{b_m}}\left [ {\pi^{X_{b_m}}} \right ] \right \|^2\!\!,\\
 \shortintertext{and}
	\label{topo_loss_2}
 L_{Z_{b_m}\rightarrow X_{b_m}} & \!\!:= \frac{1}{2}\left \| A^{Z_{b_m}}\left [ {\pi^{Z_{b_m}}} \right ] - A^{X_{b_m}}\left [ {\pi^{Z_{b_m}}} \right ] \right \|^2\!\!,
 \end{align}
%
with $\pi^{X_{b_m}}$ and $\pi^{Z_{b_m}}$ denoting the persistence pairing of topological features in the input space and the latent space, respectively.
%

Our framework is flexible to integrate any aggregation function for representing the whole bag ${\zeta}_{b_{M}}$.
%
With this, the classifier head, incorporating a linear regressor and softmax functions, assigns the bag's label based on its refined representation.
%
Similar to standard MIL models, we train the MIL classification head using cross-entropy loss, computed based on the divergence between the predicted label of the bag and its corresponding ground truth label, thereby guiding the model towards accurate bag-level predictions. 
%
Our formulation also gives rise to a variant of a multi-classifier head approach like the auxiliary loss that \citet{sadafi2020attention} proposed.
%
The final loss of topologically-regularized MIL (TR-MIL) framework $L_\mathrm{total}$ is the weighted sum of the MIL classification loss $L_\mathrm{class}$ and topological regularization term $L_\mathrm{topo}$:
%
\begin{equation}
	\label{loss}
 L_\mathrm{total} = L_\mathrm{class}+\lambda L_\mathrm{topo},
\end{equation}
%
where $\lambda$ is a hyperparameter to adjust the influence of the topological regularization loss
(Appendix \ref{sec:arch_detals}) presents more details of MIL architecture examples.



% Figure environment removed

\paragraph{Complexity and Parameters.}
%
The computational complexity involved in calculating certain topological features aligns more closely with the rate of the inverse Ackermann function~\citep{cormen2022introduction}, which increases significantly slower compared to the rate of increasing $n$.
%
Therefore, the computational complexity of the topological signature calculation of a bag containing $n$ instances is dominated by the calculation of pairwise distances, i.e., $\mathcal{O}(n^2)$, considering that we only capture $0$-dimensional topological features.
%
The topological signature calculation does not introduce any additional learnable parameters, thereby keeping the model's parameter size unchanged. 
%
It merely introduces one topological regularization loss and one hyperparameter, denoted as $\lambda$.

\paragraph{Limitations.}
%
The primary limitation of our approach is that the calculation of topological features does \emph{not} exhibit favorable scaling parameters in case higher-order topological features are required.
%
While our implementation supports topological features of arbitrary dimension, their calculation scales progressively worse; connected components, i.e., $0$-dimensional features, can still be efficiently calculated~(see previous paragraph), but higher-order features may prove limiting. 
%
We plan on investigating mitigation strategies in future work, using, e.g., approximate filtrations~\citep{Sheehy13a} or distributed computations~\citep{wagner2021improving}.

\paragraph{Toy dataset.}
%
As an illustrative example, we consider a toy dataset, where negative instances are sampled from a $100$-dimensional random space and positive instances are sampled from the surface of a $100$-dimensional sphere as a known geometrical object.
%
To satisfy the positive bag definition of MIL, the sphere overlaps with the space of random negative instances.
% 
We consider a simple $2$-layer encoder projecting instances from the $100$-dimensional space to a visualizable $2$-dimensional representation (see Table \ref{table:toy_mil_model}).
%
We apply our framework utilizing regressor-guided aggregation~\citep{du2023rgmil} to the dataset.
%
We chose this aggregation function due to its demonstrated superiority in instance-level performance.
%
Figure \ref{fig:toy} illustrates the resulting instance and bag representations, contrasting the scenarios with and without topological regularization.
%
TR-RGMIL preserves the topology of positive instances, resembling a circle, as the expected 2D projection of a hypersphere.
% 
As a result, the aggregated bag's latent is more distinct, leading to a higher classification accuracy (RGMIL and TR-GMIL yield $0.55\pm0.05$ and $0.8\pm0.22$ accuracy,respectively, averaged in $5$ runs).
% 
