\section{Background}

\paragraph{MIL Architectures.}
MIL architectures typically comprise three key components: an instance encoder, an aggregation function, and a classifier head (Figure \ref{fig:diagram}).
%
Given a collection of bags ${b_{1}, \dots, b_{M}}$, each bag contains a set of instances, represented as $X_{b_m} := \{x_{1}, \dots x_{n}\}$ with $n$ denoting the number of instances in the bag.
%
An instance encoder $f_{\theta}$ with parameters $\theta$ transfers instance data into a latent space, yielding feature vectors $z_i:=f_{\theta}(x_i)$. 
%
The aggregation function then creates a global representation of a bag $\zeta_{b_{m}}$ from these embedded instances. 
%
Finally, this bag representation is passed through a classifier head, which predicts the overall label of the bag.

\paragraph{Geometry \& Topology.}
Our work is based on recent advances in topological machine learning~\citep{Hensel21}, a nascent field that aims to leverage geometry and topology from data to elicit improved representations.
%
We employ \emph{persistent homology}, a technique for calculating multi-scale geometrical-topological information from data~\cite{edelsbrunner2009computational}.
%
Persistent homology considers data to be a point cloud (Figure \ref{fig:diagram}, using a metric~(e.g., Euclidean distance) to assess its multi-scale shape information. 
%
This includes topological information like connected components, cycles, and higher-dimensional voids in addition to geometrical information like curvature or convexity~\citep{Bubenik20a, Turkes22a}.
%
Such information is collected in a set of \emph{persistence diagrams}, i.e., multi-scale topological descriptors.
%
These descriptors are calculated by approximating the data in terms of a simplicial complex, i.e., a generalized graph, typically based on distance functions like the Euclidean distance.
%
Recent work proved that persistent homology can be integrated with deep learning models, leading to a new class of hybrid models that are capable of capturing topological aspects of data.
%
Such models have shown exceptional performance as regularization terms in different applications~\citep{Chen19a,Vandaele22a,Waibel22a}.