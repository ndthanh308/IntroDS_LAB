\section{Experiments}

\paragraph{Data.}
We applied our method to microscopic images of blood samples from hereditary hemolytic anemia patients.
The dataset includes 3630 microscopic images of blood samples obtained from 71 patients who underwent various treatments at different times. 
The data is distributed among several classes: Sickle Cell Disease (SCD) with 13 patients and 170 samples; Thalassemia with 3 patients and 25 samples; Hereditary Xerocytosis with 9 patients and 56 samples; and Hereditary Spherocytosis (HS) with 13 patients and 89 samples, in addition to a healthy control group consisting of 33 individuals and 181 samples.
\paragraph{Training setting.}
We partitioned the dataset into three folds. 
We reserved one fold for testing during each experiment iteration while the others were employed for training. 
In exploring anomaly-aware MIL, we adopted the same configuration used in \cite{kazeminia2022anomaly}, which entailed recalibrating the Gaussian mixture model on the control distribution every five epochs.
We utilized the Adam optimizer with a learning rate of $5 \times 10^{-4}$ for optimization. 
We set the constraint values of $\beta = 0.95^{\mathrm{epoch}}$, and $\lambda = 5 \times 10^{-3}$.

\begin{table}[t]
    \caption{Topological regularization improves classification performance for all pooling strategies. We applied it on different MIL methods with average/max pooling, attention-based pooling~\cite{sadafi2020attention}, and anomaly-aware pooling~\cite{kazeminia2022anomaly}. Numbers show the average classification performance along with the standard deviation from  3 cross-validation and 3 runs. Best performance is indicated by bold text. Additionally, for each pooling method, we compared the classification performance without (\xmark) and with (\cmark) topological regularization, and the winner is underlined for clarity.}
    \label{tab:performance}
    \resizebox{\textwidth}{!}{%
    \renewcommand{\arraystretch}{1.2}
    \begin{tabular}{ccccccccc}
        \toprule
        & \multicolumn{2}{c}{Average pooling}                                            
        & \multicolumn{2}{c}{Anomaly pooling~\cite{kazeminia2022anomaly}}                            & \multicolumn{2}{c}{Attention pooling~\cite{sadafi2020attention}}                           & \multicolumn{2}{c}{Max pooling}                  
        \\ \midrule
        \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Topological 
        \\regularization\end{tabular}} 
        & \xmark                              & \multicolumn{1}{c}{\cmark}                        
        & \xmark                              & \multicolumn{1}{c}{\cmark}               
        & \xmark                              & \multicolumn{1}{c}{\cmark}               
        & \xmark                             & \cmark               
        \\ \midrule
        \multicolumn{1}{c}{Accuracy}                                                              & \multicolumn{1}{c}{72.25±7.0} & \multicolumn{1}{c}{{\ul \textbf{81.29±2.5}}} & \multicolumn{1}{c}{77.85±3.7} & \multicolumn{1}{c}{{\ul 79.50±1.2}} & \multicolumn{1}{c}{73.72±3.8} & \multicolumn{1}{c}{{\ul 77.76±1.6}} & \multicolumn{1}{c}{64.33±5.8} & {\ul 71.44±5.6} \\
        \multicolumn{1}{c}{F1-Score}                                                              & \multicolumn{1}{c}{70.47±7.4} & \multicolumn{1}{c}{{\ul \textbf{80.28±3.1}}} 
        & \multicolumn{1}{c}{76.69±4.0} & \multicolumn{1}{c}{{\ul 77.01±1.8}} 
        & \multicolumn{1}{c}{72.38±3.8} & \multicolumn{1}{c}{{\ul 74.69±1.6}} 
        & \multicolumn{1}{c}{62.96±5.0}   & {\ul 68.77±5.4} 
        \\
        \multicolumn{1}{c}{AUROC}                                                                 
        & \multicolumn{1}{c}{89.88±2.7} & \multicolumn{1}{c}{{\ul \textbf{93.72±4.4}}} 
        & \multicolumn{1}{c}{89.05±4.3} & \multicolumn{1}{c}{{\ul 90.89±2.5}} 
        & \multicolumn{1}{c}{91.58±3.0} & \multicolumn{1}{c}{{\ul 91.88±2.5}} 
        & \multicolumn{1}{c}{84.83±2.8} & {\ul 89.73±3.1} 
        \\ \bottomrule
    \end{tabular}%
    }
\end{table}
\paragraph{Evaluation.}
We employed three standard evaluation metrics: Accuracy, F1-Score (weighted macro), and Area Under the Receiver Operating Characteristic Curve (AUROC).
Our results (Table \ref{tab:performance}) show a significant impact of topology-based regularization on the performance of MIL baselines. This is not restricted to Max and Average pooling techniques but also applies to the attention-based and anomaly-aware pooling techniques proposed in previous state-of-the-art studies \cite{sadafi2020attention,kazeminia2022anomaly}.
Moreover, this substantial performance increase is not limited to the average value of results, as the proposed regularization method also reduces the error margin. 
These finding show the increased robustness afforded by our proposed regularization strategy.

Remarkably, the utilization of topological regularization in combination with average pooling in MIL showed a superior level of performance, which refers to the inherent ambiguity present within the explored dataset. 
Specifically, blood samples may contain a low ratio of deformed cells that falls below a specified threshold to identify a disorder.
Such data introduces a significant challenge for attention-based and anomaly-aware pooling techniques, which do not conform to the fundamental assumptions of these mechanisms. 
In contrast, the average pooling technique, which has previously struggled with issues of vanishing gradient and inadequate training, is poised to leverage a topologically structured latent space to identify instances relevant to the disorder and fine-tune its classifier weights to incorporate their respective ratios. 
However, average pooling provides less interpretability of classifier performance than the anomaly-aware pooling technique. 
This limitation makes it potentially less preferable for applications in the medical environment where accurate interpretation of classifier performance is crucial.

% Figure environment removed

We evaluated the effectiveness of our proposed method by conducting additional tests on the anomaly-aware MIL approach, which was the second-best-performing method in our experiments. 
Insufficient training data for the anomaly-aware MIL method may fail to detect anomalous cells and inconsistent assignment of anomaly scores.
Our topological regularizer resolves this issue by penalizing the mapping of topologically similar instances far apart from each other (Fig. \ref{fig:interpretation}).
%
To analyze the impact of regularization in more detail, we visualized the distance matrix heatmap for instances within a bag, following the convergence of anomaly-aware MIL, both with and without topological regularization (Fig. \ref{fig:dists_umaps}). 
Our findings demonstrate that incorporating topological regularization successfully distinguishes more anomalies by creating higher distances in the latent space and enhances the overlap between the distance patterns in the image and latent spaces. 
The observed performance improvement provides empirical evidence of the functionality of our proposed approach.

% Figure environment removed

\paragraph{CO2 Emission Related to Experiments.}
All experiments were conducted for 228 hours on institute's infrastructure with 0.432 kgCO$_2$eq/kWh carbon efficiency and A100 PCIe 40/80GB hardware (TDP 250W), resulting in 24.62 kgCO$_2$eq total emissions with no direct offset. Calculations utilized the \href{https://mlco2.github.io/impact#compute}{MachineLearning Impact calculator}~\cite{lacoste2019quantifying}.
