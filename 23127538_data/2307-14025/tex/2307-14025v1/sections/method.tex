\section{Method}
Our study presents a unified framework, incorporating four different Multiple Instance Learning (MIL) models, with the topological signature calculator playing a crucial role in summarizing the topological features of a bag (see Fig. \ref{fig:diagram}). 
The framework comprises various components, including an instance detector, deep encoder, topological signature calculator, pooling techniques, and classifier heads. 
The use of topological signatures is a critical aspect of our model, as we calculate them for both the instance images and the latent space.

% Figure environment removed

\paragraph{Input data.}
%
Our framework is tailored for analyzing microscopic images in a MIL setting, where bags ${B_{1}, ..., B_{M}}$ represent sets of blood sample images containing red blood cells as instances ${I_{1}, ..., I_{N}} \in B_{m\in M}$.
Notably, the framework is customized to meet the unique demands of this particular domain, where instances are distributed along independent spatial locations within the image.
This is different from cases where tiling techniques are employed to capture instances~\cite{shao2021transmil}.
In our scenario, a pre-trained object detector, such as a mask R-CNN~\cite{he2017mask}, is used to identify an instance and extract instance-level features ${H_{1}, ..., H_{N}}$ from the region of interest (see Fig.~\ref{fig:diagram}). 
Then an encoder is embedded to extract class-related features of instances and map them to the latent space ${F_{1}, ..., F_{N}}$. 

\paragraph{Topological features.} We calculate multi-scale topological features via persistent homology based on the Vietoris--Rips complex \cite{edelsbrunner2009computational} derived from the Euclidean distances of the latent space created from each bag. 
The Vietoris--Rips complex serves to characterize the multi-scale topological information of the encoded bags.
The outcome of this computation is a set of $d$-dimensional diagrams and pairings $\pi$ that signify the relevant point indices involved in the creation and destruction of topological features\footnote{%
  Technically, these indices can refer to arbitrary simplices in the Vietoris--Rips complex, but since we only consider lower-dimensional topological information, a mapping to individual data points is possible.
}.
Given that calculating high-dimensional topological features can significantly increase computation time, focusing on low-dimensional features is often more practical. 
Previous work showed that even $0$-dimensional features, i.e., \emph{connected components}, already contain a wealth of information that is crucial for data analysis~\cite{bubenik2015statistical,edelsbrunner2009computational}. 
Therefore, we proceed by analyzing only the $0$-dimensional topology of the object.
To obtain a topology-based regularization, we let ourselves be inspired by the method developed by Moor et al.~\cite{moor2020topological}, which addresses the challenge of backpropagating topology-based loss terms~(or topological information in general).
Specifically, this approach identifies the most salient topological features---represented by a pair of point indices---and subsequently maps them back to the distances in the space, represented as a matrix $A^{Z}$ and $A^{X}$ for the hidden and image spaces, respectively.
%
The corresponding topological features in both matrices are then compared to define a loss. Essentially, the loss measures to what extent topological features in one space are being preserved when mapping to a different space. 
To avoid biases originating from looking at a specific space, we measure the difference between topological features when mapping from the image space to the hidden space $L_{X\rightarrow Z}$, as well as their respective opposite direction $L_{Z\rightarrow X}$, i.e., mapping from the hidden space to the image space. 
%
Intuitively, our objective is to assess the dissimilarity between topological features in both spaces based on matrices of pairwise distances.
%
Therefore, the final topological loss $L_\mathrm{topo}$ is defined as:
%
\begin{equation}
	\label{topo_loss}
	L_\mathrm{topo} := L_{X\rightarrow Z} + L_{Z\rightarrow X},
\end{equation}
where
\begin{equation}
	\label{topo_loss_1}
	L_{X\rightarrow Z} := \frac{1}{2}\left \| A^{X}\left [ {\pi^{X}} \right ] - A^{Z}\left [ {\pi^{X}} \right ] \right \|^2 ,
\end{equation}
and
\begin{equation}
	\label{topo_loss_2}
	L_{Z\rightarrow X} := \frac{1}{2}\left \| A^{Z}\left [ {\pi^{Z}} \right ] - A^{X}\left [ {\pi^{Z}} \right ] \right \|^2 .
\end{equation}

\paragraph{Framework.}
%
The remaining part of our framework consists of the pooling strategy and classifier heads.
The pooling strategy aggregates predictions across instances within a bag by summarizing instance-level features into a bag-level representation. 
This allows models to consider the global features of a bag rather than just the individual instances. 
Our framework includes average pooling, max pooling, and an improved version of attention-based pooling \cite{sadafi2020attention} for limited training data.
Furthermore, we have included the anomaly-aware pooling technique Kazeminia et al.~\cite{kazeminia2022anomaly} proposed recently as a state-of-the-art (see Fig.~\ref{fig:diagram}).

\paragraph{Loss terms.}
%
We adopted a dual classifier head approach that comprised a bag classification head and a single-cell classification head. 
The bag classification head is trained using a cross-entropy loss function $L_\mathrm{MIL}$, calculated as the difference between the predicted bag label and the corresponding ground truth label for the bag. 
Conversely, the single-cell classification head is trained using a cross-entropy loss function $L_\mathrm{SIC}$ that utilizes the noisy labels of instances as the repeated labels of the bag for all instances. 
This approach allowed us to incorporate bag-level and instance-level information into our models and evaluate their contributions to classification performance.
The final classification loss $L_\mathrm{class}$ is defined as:
\begin{equation}
	\label{mil_loss}
	L_\mathrm{class} = (1-\beta) L_\mathrm{MIL} + \beta L_\mathrm{SIC},
\end{equation}
where $\beta = 0$ for average and max pooling strategies. However, for attention and anomaly-aware pooling techniques, it decreases relative to the epoch number.
The final loss $L_\mathrm{total}$ is the weighted sum of $L_\mathrm{class}$ and our topological regularization term $L_\mathrm{topo}$:
%
\begin{equation}
	\label{loss}
 L_\mathrm{total} = L_\mathrm{class}+\lambda L_\mathrm{topo},
\end{equation}
%
where $\lambda$ is a hyperparameter to adjust the influence of the topological loss. 