
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@inproceedings{43461,
title	= {Long-Short Term Memory Neural Network for Keyboard Gesture Recognition},
author	= {Ouais Alsharif and Tom Ouyang and Françoise Beaufays and Shumin Zhai and Thomas Breuel and Johan Schalkwyk},
year	= {2015},
booktitle	= {International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}

@techreport{li2017deep,
author = {Li, Jinyu},
title = {Deep Learning Acoustic Model in Microsoft Cortana Voice Assistant},
institution = {Microsoft},
year = {2017},
month = {November},
abstract = {Deep Learning Acoustic Modeling has been widely deployed to real-world speech recognition products and services that benefit millions of users. In this talk, I will first briefly describe selected developments and investigations at Microsoft to make deep learning networks more effective under production environment, with the focus on computational cost reduction, knowledge transfer and model robustness. Then, I will introduce our recent efforts in adapting models with unlabeled data and the work of separating speeches from multi-speakers.},
url = {https://www.microsoft.com/en-us/research/publication/deep-learning-acoustic-model-in-microsoft-cortana-voice-assistant/},
number = {MSR-TR-2017-60},
}

@inproceedings{Ying_2018,
	doi = {10.1145/3219819.3219890},
	url = {https://doi.org/10.1145\%2F3219819.3219890},
	year = 2018,
	month = {jul},
	publisher = {{ACM}},
	author = {Rex Ying and Ruining He and Kaifeng Chen and Pong Eksombatchai and William L. Hamilton and Jure Leskovec},
	title = {Graph Convolutional Neural Networks for Web-Scale Recommender Systems},
	booktitle = {Proceedings of the 24th {ACM} {SIGKDD} International Conference on Knowledge Discovery and Data Mining}
}

@inproceedings{heimerl2018interactive,
  title={Interactive analysis of word vector embeddings},
  author={Heimerl, Florian and Gleicher, Michael},
  booktitle={Computer Graphics Forum},
  volume={37},
  number={3},
  pages={253--265},
  year={2018},
  organization={Wiley Online Library}
}

@inproceedings{BansalXAITeamPerformance,
author = {Bansal, Gagan and Wu, Tongshuang and Zhou, Joyce and Fok, Raymond and Nushi, Besmira and Kamar, Ece and Ribeiro, Marco Tulio and Weld, Daniel},
title = {Does the Whole Exceed Its Parts? The Effect of AI Explanations on Complementary Team Performance},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445717},
doi = {10.1145/3411764.3445717},
abstract = {Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI’s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {81},
numpages = {16},
keywords = {Augmented intelligence, Human-AI teams, Explainable AI},
location = {Yokohama, Japan},
series = {CHI '21}
}

@article{BuccinaCFF,
author = {Bu\c{c}inca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z.},
title = {To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-Assisted Decision-Making},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449287},
doi = {10.1145/3449287},
abstract = {People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI's suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {apr},
articleno = {188},
numpages = {21},
keywords = {artificial intelligence, trust, cognition, explanations}
}

@ARTICLE{8667702,  author={Ji, Xiaonan and Shen, Han-Wei and Ritter, Alan and Machiraju, Raghu and Yen, Po-Yin},  journal={IEEE Transactions on Visualization and Computer Graphics},   title={Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection},   year={2019},  volume={25},  number={6},  pages={2181-2192},  doi={10.1109/TVCG.2019.2903946}}

@online{TensorflowGithub,
  author = {Tensorflow},
  title = {Tensorboard},
  year = {2022},
  url = {https://github.com/tensorflow/tensorboard},
  urldate = {2022-10-07}
}

@online{vraagvoordewetenschap,
    author = {Flanders Scientific Research Fund (FWO)},
    title = {Vraag voor de wetenschap - Vlaamse Wetenschapsagenda},
    year = {2018},
    url= {https://www.vraagvoordewetenschap.be/},
    urldate = {2022-10-07}
}

@online{bernerslee_2006,
    author = {Berners-Lee, Tim},
    title = {Linked Data},
    year = {2006},
    url= {https://www.w3.org/DesignIssues/LinkedData.html},
    urldate = {2022-10-07}
}

@online{bernerslee_1998,
    author = {Berners-Lee, Tim},
    title = {Semantic Web Road Map},
    year = {1998},
    url= {https://www.w3.org/DesignIssues/Semantic.html},
    urldate = {2022-10-07}
}

@online{vraagvoordewetenschap2,
    author = {Flanders Scientific Research Fund (FWO)},
    title = {Vraag voor de wetenschap - Vlaamse Wetenschapsagenda},
    year = {2018},
    url = {https://www.vraagvoordewetenschap.be/p/thema_zoek?c=&search=&order=vvdw_answered},
     urldate = {2022-10-07}
}

@online{bmbf2019,
    author = {Federal Ministry of Education and Research},
    title = {Richtlinie zur Förderung von partizipativen Wissenschaftskommunikationsprojekten im Wissenschaftsjahr 2022 – Nachgefragt!},
    year = {2019},
    url={https://www.bmbf.de/bmbf/shareddocs/bekanntmachungen/de/2019/12/2767_bekanntmachung.html},
    urldate = {2022-10-07}
}

@online{TFHUB_use,
author = {Google},
    title = {Tensorflow Hub - universal-sentence-encoder},
    year = {2022},
    url = {https://tfhub.dev/google/collections/universal-sentence-encoder/1},
    urldate = {2022-10-07}
}
@online{INSPIRE,
author = {European Commission},
title = {INSPIRE KNOWLEDGE BASE},
year = {2023},
url={https://inspire.ec.europa.eu/},
urldate = {2022-10-07}
}


@online{INSPIRE_codelist,
author = {European Commission},
title = {INSPIRE code list register},
url = {https://inspire.ec.europa.eu/codelist},
year = {2013},
urldate = {2022-10-07}
}

@online{INSPIRE_themes,
author = {European Commission},
title = {INSPIRE theme register},
url = {https://inspire.ec.europa.eu/theme},
year = {2013},
urldate = {2022-10-07}
}

@online{DCAT_themes,
author = {European Commission},
title = {RDF: Data Themes},
year = {2022},
urldate = {2022-10-07},
url = {http://publications.europa.eu/resource/authority/data-theme}
}

@online{DCAT,
author = {European Commision},
title = {DCAT Application Profile for data portals in Europe},
year = {2022},
url = {https://joinup.ec.europa.eu/collection/semantic-interoperability-community-semic/solution/dcat-application-profile-data-portals-europe/release/211},
urldate = {2022-10-07}
}

@inproceedings{10.5555/313559.313789,
author = {Yianilos, Peter N.},
title = {Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces},
year = {1993},
isbn = {0898713137},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
booktitle = {Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {311–321},
numpages = {11},
keywords = {clustering, metric space, randomized methods, nearest neighbor, computational geometry, pattern recognition, associative memory},
location = {Austin, Texas, USA},
series = {SODA '93}
}

@inproceedings{10.5555/3001460.3001507,
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\"{o}rg and Xu, Xiaowei},
title = {A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},
year = {1996},
publisher = {AAAI Press},
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
pages = {226–231},
numpages = {6},
keywords = {clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise, arbitrary shape of clusters},
location = {Portland, Oregon},
series = {KDD'96}
}

@online{TFHUB_nnlm,
author = {Google},
    title = {Tensorflow Hub - nnlm},
    year=2022,
    url={https://tfhub.dev/google/collections/nnlm/1},
    urldate = {2022-10-07}
}

@article{van2008visualizing,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}

@misc{yang2019multilingual,
      title={Multilingual Universal Sentence Encoder for Semantic Retrieval}, 
      author={Yinfei Yang and Daniel Cer and Amin Ahmad and Mandy Guo and Jax Law and Noah Constant and Gustavo Hernandez Abrego and Steve Yuan and Chris Tar and Yun-Hsuan Sung and Brian Strope and Ray Kurzweil},
      year={2019},
      eprint={1907.04307},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{10.5555/944919.944966,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {1137–1155},
numpages = {19}
}

@online{GoogleTranslate,
author = {Google},
    title = {Translation AI},
    year=2022,
    url={https://cloud.google.com/translate},
    urldate = {2022-10-07}
}

@article{DBLP:journals/corr/abs-1803-11175,
  author    = {Daniel Cer and
               Yinfei Yang and
               Sheng{-}yi Kong and
               Nan Hua and
               Nicole Limtiaco and
               Rhomni St. John and
               Noah Constant and
               Mario Guajardo{-}Cespedes and
               Steve Yuan and
               Chris Tar and
               Yun{-}Hsuan Sung and
               Brian Strope and
               Ray Kurzweil},
  title     = {Universal Sentence Encoder},
  journal   = {CoRR},
  volume    = {abs/1803.11175},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.11175},
  eprinttype = {arXiv},
  eprint    = {1803.11175},
  timestamp = {Mon, 13 Aug 2018 16:46:40 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-11175.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@online{OpenRefine,
    author = {Open Refine Developement Community},
    title = {Clustering Methods In-depth},
    year=2022,
    url={https://docs.openrefine.org/technical-reference/clustering-in-depth},
    urldate = {2022-10-07}
}

@online{bmbf2022,
author = {Federal Ministry of Education and Research},
    title = {Wissenschaftsjahr 2022 – Nachgefragt!},
url={https://www.wissenschaftsjahr.de/2022/fragen},
urldate = {2022-10-07}
}

@inproceedings{feinberg2014,
author = {Feinberg, Melanie and Carter, Daniel and Bullard, Julia},
title = {A Story without End: Writing the Residual into Descriptive Infrastructure},
year = {2014},
isbn = {9781450329026},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2598510.2598553},
doi = {10.1145/2598510.2598553},
booktitle = {Proceedings of the 2014 Conference on Designing Interactive Systems},
pages = {385–394},
numpages = {10},
keywords = {metadata, interaction criticism, critical design},
location = {Vancouver, BC, Canada},
series = {DIS '14}
}

@inproceedings{FeinbergDesignPerspectiveData2017,
author = {Feinberg, Melanie},
title = {A Design Perspective on Data},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025837},
doi = {10.1145/3025453.3025837},
abstract = {Empirical studies invariably show that data generation is situationally contingent and interpretively flexible, even when data is collected automatically. This essay situates data generation within a design perspective, demonstrating how data creation can be understood as a multilayered set of interlocking design activities. By showing how data is infused with design, this paper argues that any "use" of data represents a continuation of its design. We are always designers of data, never its mere appropriators.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {2952–2963},
numpages = {12},
keywords = {metadata, materiality, infrastructure, design, data},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{Sengers2006,
author = {Sengers, Phoebe and Gaver, Bill},
title = {Staying Open to Interpretation: Engaging Multiple Meanings in Design and Evaluation},
year = {2006},
isbn = {1595933670},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1142405.1142422},
doi = {10.1145/1142405.1142422},
abstract = {Human-Computer Interaction (HCI) often focuses on how designers can develop systems that convey a single, specific, clear interpretation of what they are for and how they should be used and experienced. New domains such as domestic and public environments, new influences from the arts and humanities, and new techniques in HCI itself are converging to suggest that multiple, potentially competing interpretations can fruitfully co-exist. In this paper, we lay out the contours of the new space opened by a focus on multiple interpretations, which may more fully address the complexity, dynamics and interplay of user, system, and designer interpretation. We document how design and evaluation strategies shift when we abandon the presumption that a specific, authoritative interpretation of the systems we build is necessary, possible or desirable.},
booktitle = {Proceedings of the 6th Conference on Designing Interactive Systems},
pages = {99–108},
numpages = {10},
keywords = {hermeneutics, interpretation, arts, evaluation, design, humanities},
location = {University Park, PA, USA},
series = {DIS '06}
}


@book{bowker_sorting_1999,
	address = {Cambridge, Mass.},
	author = {Bowker, Geoffrey C. and Star, Susan Leigh},
	isbn = {9780262269070 9780262024617},
	language = {eng},
	publisher = {MIT Press},
	series = {Inside technology},
	shorttitle = {Sorting things out},
	title = {Sorting things out: classification and its consequences},
	year = {1999}}

@inproceedings{blodgett2020-languageBias,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
}

@article{Terveen_1995,
author={Terveen, Loren G.}, 
title={Overview of human-computer collaboration}, 
journal={Knowledge-Based Systems}, 
collection={Human-computer collaboration}, 
series={Human-computer collaboration}, 
volume={8}, 
number={2}, 
year={1995}, 
month={Apr}, 
pages={67–81}, 
DOI={10.1016/0950-7051(95)98369-H}
}


@article{Shneiderman_Maes_1997, 
author={Shneiderman, Ben and Maes, Pattie}, 
title={Direct manipulation vs. interface agents}, 
journal={Interactions}, 
volume={4}, 
number={6}, 
year={1997}, 
month={Nov}, 
pages={42–61}, 
DOI={10.1145/267505.267514}
}

@inproceedings{Wang_HHColltoHumanAIColl,
author = {Wang, Dakuo and Churchill, Elizabeth and Maes, Pattie and Fan, Xiangmin and Shneiderman, Ben and Shi, Yuanchun and Wang, Qianying},
title = {From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3381069},
doi = {10.1145/3334480.3381069},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {explainable ai, trusted ai, human-ai collaboration, computer-supported corporative work, ai-powered healthcare, group collaboration, ai partner},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@article{mackeprang_sweet,
author = {Mackeprang, Maximilian and M\"{u}ller-Birn, Claudia and Stauss, Maximilian Timo},
title = {Discovering the Sweet Spot of Human-Computer Configurations: A Case Study in Information Extraction},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359297},
doi = {10.1145/3359297},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {195},
numpages = {30},
keywords = {human-computer collaboration, large scale ideation, semantic annotation}
}

 @article{Baumeretal:2020:Topicalizer,
 author={Baumer, Eric P S and Siedel, Drew and McDonnell, Lena and Zhong, Jiayun and Sittikul, Patricia and McGee, Micki}, title={Topicalizer: reframing core concepts in machine learning visualization by co-designing for interpretivist scholarship}, journal={Human–Computer Interaction}, volume={35}, number={5–6}, year={2020}, month={Apr}, publisher={Lawrence Erlbaum Associates, Inc.}, pages={452–480}, url={https://www.tandfonline.com/doi/full/10.1080/07370024.2020.1734460}, 
 DOI={10.1080/07370024.2020.1734460} 
 }
 
 @article{Jiang_serendipity,
author = {Jiang, Jialun Aaron and Wade, Kandrea and Fiesler, Casey and Brubaker, Jed R.},
title = {Supporting Serendipity: Opportunities and Challenges for Human-AI Collaboration in Qualitative Analysis},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {CSCW1},
url = {https://doi.org/10.1145/3449168},
doi = {10.1145/3449168},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = apr,
articleno = {94},
numpages = {23},
keywords = {interview, human-ai collaboration, ai, qualitative research}
}


@article{Carvalho_Interpretability,
	author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
	doi = {10.3390/electronics8080832},
	issn = {2079-9292},
	journal = {Electronics},
	number = {8},
	title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
	url = {https://www.mdpi.com/2079-9292/8/8/832},
	volume = {8},
	year = {2019},
	bdsk-url-1 = {https://www.mdpi.com/2079-9292/8/8/832},
	bdsk-url-2 = {https://doi.org/10.3390/electronics8080832}}
