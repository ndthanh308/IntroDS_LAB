\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=scriptsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{bbm}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{siunitx}
\usepackage{nicefrac}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{amsthm}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}
\usepackage{multicol}
% \usepackage[numbers]{natbib}
% updated with editorial comments 8/9/2021

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\tikzstyle{block} = [rectangle, fill, fill=teal!10, 
    text width=12em, text centered, minimum height=2em]
\tikzstyle{empty} = [rectangle, fill, fill=white, 
     text centered]
\tikzstyle{line} = [draw, -latex']
\usetikzlibrary{calc}
\usetikzlibrary{positioning,fit,arrows.meta,backgrounds,calc}
\usetikzlibrary{decorations.pathmorphing}

\tikzset{
    algorithm/.style={%
        draw, rounded corners,
        minimum width=0cm,
        minimum height=1cm,
        font={\fontsize{10pt}{12}\sffamily},
        fill=white,
        text=black,
        },
    ours/.style={
        draw, rounded corners,
        minimum width=0cm,
        minimum height=1cm,
        font={\fontsize{10pt}{12}\sffamily},
        fill=gray,
        text=white,
    }
    % >=LaTeX
}

\newcounter{experimentcounter}
\setcounter{experimentcounter}{1}

\newcommand{\normtwo}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\normone}[1]{\left\lVert#1\right\rVert_1}
\newcommand{\baskin}[1]{\textcolor{red}{(Baskin: #1)}}

\newcommand{\mNC}{\mathcal{NC}}
\newcommand{\mR}{\mathcal{R}} 
\newcommand{\mRx}[1]{\mathcal{R_{#1}}}
\newcommand{\mRxy}[2]{\mathcal{R}_{#1,#2}}
\newcommand{\mO}{\mathcal{O}}
\newcommand{\mC}{\mathcal{C}}
\newcommand{\mH}{\mathcal{H}}
\newcommand{\mW}{\mathcal{W}}
\newcommand{\mE}{\mathcal{E}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mA}{\mathcal{A}}
\newcommand{\mV}{\mathcal{V}}
\newcommand{\mB}{\mathcal{B}}
\newcommand{\mS}{\mathcal{S}}
\newcommand{\mP}{\mathcal{P}}
\newcommand{\mPx}[1]{\mathcal{P}_{#1}}
\newcommand{\mPxy}[2]{\mathcal{P}_{#1}^{#2}}
\newcommand{\mo}{\mathcal{o}}
\newcommand{\mQ}{\mathcal{Q}}
\newcommand{\mT}{\mathcal{T}}
\newcommand{\mF}{\mathcal{F}}
\newcommand{\mJ}{\mathcal{J}}
\newcommand{\ms}{\mathcal{s}}
\newcommand{\md}{\mathcal{d}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\mG}{\mathcal{G}}
\newcommand{\mI}{\mathcal{I}}
\newcommand{\DMI}{\mathcal{DMI}}

\newcommand{\fd}{\mathfrak{d}}
\newcommand{\fo}{\mathfrak{o}}
\newcommand{\fe}{\mathfrak{e}}
\newcommand{\fc}{\mathfrak{c}}
\newcommand{\fg}{\mathfrak{g}}
\newcommand{\fD}{\mathfrak{D}}
\newcommand{\fP}{\mathfrak{P}}
\newcommand{\fO}{\mathfrak{O}}

\newcommand{\vp}{\mathbf{p}}
\newcommand{\vI}{\mathbf{I}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vpxy}[2]{\mathbf{p}_{#1}^{#2}}
\newcommand{\vo}{\mathbf{o}}
\newcommand{\vox}[1]{\mathbf{o}_{#1}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vfx}[1]{\mathbf{f}_{#1}}
\newcommand{\vd}{\mathbf{d}}
\newcommand{\vdx}[1]{\mathbf{d}_{#1}}
\newcommand{\vfxy}[2]{\mathbf{f}_{#1}^{#2}}
\newcommand{\vfxyz}[3]{\mathbf{f}_{#1, #2}^#3}
\newcommand{\vn}{\mathbf{n}}
\newcommand{\vgammai}{\boldsymbol{\gamma_i}}
\newcommand{\vDelta}{\boldsymbol{\Delta}}
\newcommand{\vPi}{\boldsymbol{\Pi}}
\newcommand{\vpi}{\boldsymbol{\pi}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vq}{\mathbf{q}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vQ}{\mathbf{Q}}
\newcommand{\vB}{\mathbf{B}}
\newcommand{\vD}{\mathbf{D}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\vC}{\mathbf{C}}
\newcommand{\vc}{\mathbf{c}}
\newcommand{\vA}{\mathbf{A}}
\newcommand{\vone}{\mathbf{1}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\va}{\mathbf{a}}
\newcommand{\vb}{\mathbf{b}}
\newcommand{\vl}{\mathbf{l}}
\newcommand{\ve}{\mathbf{e}}
\newcommand{\vh}{\mathbf{h}}
\newcommand{\vhx}[1]{\mathbf{h}_{#1}}
\newcommand{\vhi}{\mathbf{h}_i}
\newcommand{\vlambdai}{\boldsymbol{\lambda}_i}
\newcommand{\vthetai}{\boldsymbol{\theta}_i}
\newcommand{\vtheta}{\boldsymbol{\theta}}
\newcommand{\vlambda}{\boldsymbol{\lambda}}
\newcommand{\vPx}[1]{\mathbf{P}_{#1}}
\newcommand{\vPxy}[2]{\mathbf{P}_{#1, #2}}
\newcommand{\vgxy}[2]{\mathbf{g}_{#1}^{#2}}
\newcommand{\vj}{\mathbf{j}}
\newcommand{\vjx}[1]{\mathbf{j}_{#1}}
% \newcommand{\vPxyz}[3]{\mathbf{P_{#1, #2, #3}}}
\newcommand{\vPxyzt}[4]{\mathbf{P}_{#1, #2, #3}^{#4}}
\newcommand{\vexyz}[3]{\mathbf{e}_{#1, #2}^{#3}}
\newcommand{\vs}{\mathbf{s}}
\newcommand{\vP}{\mathbf{P}}
\newcommand{\vr}{\mathbf{r}}

\newcommand{\etc}{\emph{etc.} } %\xspace}
\newcommand{\ie}{\emph{i.e.,} } %\xspace}
\newcommand{\eg}{\emph{e.g.,} } %\xspace}

\newcommand{\todo}[1]{{\color{red}TODO: #1}}


% \everypar{\looseness=-1}

\begin{document}

% \title{Probabilistic Trajectory Planning for Static and Interaction-aware Dynamic Obstacle Avoidance}
\title{DREAM: Decentralized Real-time Asynchronous Probabilistic Trajectory Planning for Collision-free Multi-Robot Navigation in Cluttered Environments}

\author{Bask{\i}n \c{S}enba\c{s}lar and
        Gaurav S. Sukhatme
        % <-this % stops a space
\thanks{Bask{\i}n \c{S}enba\c{s}lar (corresponding author) and Gaurav S. Sukhatme are with the Department of Computer Science, University of Southern California, Los Angeles, CA, USA. GSS holds concurrent appointments as a Professor at USC and as an Amazon Scholar. This paper describes work performed at USC and is not associated with Amazon. Email: \{baskin.senbaslar, gaurav\}@usc.edu.}% <-this % stops a space
}

% The paper headers
\markboth{}%
{}

\IEEEpubid{}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
% Collision-free mobile multi-robot navigation is challenging in cluttered environments with both static and dynamic obstacles.
% Dynamic obstacles may additionally be interactive, \ie their behavior varies according to the behavior of other entities.
% %The perception and prediction modules of robotic systems create probabilistic representations and predictions of such environments.
% We propose a novel prediction representation for interactive behaviors of dynamic obstacles.
% and a real-time trajectory planning algorithm that probabilistically avoids collisions with both static and interactive dynamic obstacles as well teammates, while producing dynamically feasible trajectories.
% During decision making, our planner simulates the interactive behavior of dynamic obstacles in response to the actions of the robot.
% We explicitly minimize collision probabilities with static and dynamic obstacles using a multi-objective search formulation and formulate a quadratic program to safely fit a smooth trajectory to the search result while attempting to preserve the collision probabilities computed during search.
% We evaluate our algorithm extensively in simulations to show its performance under different environments and configurations using 78000 randomly generated cases.
% We compare its performance to a state-of-the-art trajectory planning baseline for static and dynamic obstacle avoidance using 4500 randomly generated cases.
% We show that our algorithm achieves up to 3.8x success rate using as low as 0.18x time the baseline uses. 
% We implement our algorithm on physical quadrotors, and show its feasibility in the real world.

Collision-free navigation in cluttered environments with static and dynamic obstacles is essential for many multi-robot tasks.
Dynamic obstacles may also be interactive, \ie their behavior varies based on the behavior of other entities.
We propose a novel representation for interactive behavior of dynamic obstacles and a decentralized real-time multi-robot trajectory planning algorithm allowing inter-robot collision and static and dynamic obstacle avoidance.
Our planner simulates the behavior of dynamic obstacles during decision-making, accounting for interactivity.
We account for the perception inaccuracy of static and prediction inaccuracy of dynamic obstacles.
We handle asynchronous planning between teammates and message delays, drops, and re-orderings.
We evaluate our algorithm in simulations using 25400 random cases and compare it against three state-of-the-art baselines using 2100 random cases. 
Our algorithm achieves up to 1.68x success rate using as low as 0.28x time in single-robot, and up to 2.15x success rate using as low as 0.36x time in multi-robot cases compared to the best baseline.
We implement our planner on real quadrotors to show its real-world applicability.\looseness=-1
\end{abstract}

\begin{IEEEkeywords}
collision avoidance, multi-robot systems, motion and path planning, probabilistic trajectory planning
\end{IEEEkeywords}
\section*{Supplemental Video}
\begin{center}
\url{https://youtu.be/ct8okY5pmgI}
\end{center}

\section {Introduction}

Collision-free mobile robot navigation in cluttered environments is a foundational problem in settings such as autonomous driving~\cite{campbell2010autonomous}, autonomous last-mile delivery~\cite{li2020lastmile}, and warehouse automation~\cite{inam2018warehouse}.
In such environments, obstacles can be static or dynamic. Further, dynamic obstacles may be interactive, \ie changing their behavior according to the behavior of other entities.
There can be multiple mobile robots explicitly cooperating with each other to avoid collisions.
Here, we present DREAM: a \underline{d}ecentralized \underline{re}al-time \underline{a}synchronous probabilistic trajectory planning algorithm for \underline{m}ulti-robot teams (Fig.~\ref{Figure:ClutteredEnvironment}).\looseness=-1
% From here on, we describe our algorithm from the perspective of a single team member, which we refer to as \emph{the ego robot}.
% All robots in the team executes the same algorithm.

Each robot uses onboard sensing to perceive its environment and classifies objects into three sets: static obstacles, dynamic obstacles, and teammates.
It produces a probabilistic representation of static obstacles, in which each static obstacle has an existence probability.
Each robot uses an onboard prediction system to predict the behaviors of dynamic obstacles and assigns realization probabilities to each behavior.
The perception system provides the current shapes of teammates, i.e., we require geometry-only sensing for teammates and do not require estimation/communication of higher-order derivatives, e.g., velocities or accelerations.
Each robot computes discretized separating hyperplane trajectories (DSHTs)~\cite{senbaslar2022async} between itself and teammates, and uses DSHTs during decision-making for inter-teammate collision avoidance, allowing safe operation under asynchronous planning and imperfect communication.\looseness=-1

% Figure environment removed

Using these uncertain representations of static and dynamic obstacles and DSHTs for teammates, each robot generates dynamically feasible polynomial trajectories in real-time by primarily minimizing the probabilities of collisions with static and dynamic obstacles and DSHT violations, while minimizing distance, duration, rotation, and energy usage as secondary objectives using our planner.
A DSHT hyperplane is said to be violated when the robot is not fully contained in the safe side of the hyperplane.
During decision-making, we consider interactive behaviors of dynamic obstacles in response to robot actions.
The planner runs in a receding horizon fashion, in which the planned trajectory is executed for a short duration and a new trajectory is planned from scratch.
The planner can be guided with desired trajectories, therefore it can be used in conjunction with offline planners that perform longer horizon decision-making.\looseness=-1

DREAM utilizes a three-stage widely used pipeline~\cite{senbaslar2023rlss, senbaslar2021rlss, tordesillas2020mader,kondo2023rmader,liu2017planning, richter2013planning,chen2016planning,gao2018fast}, differing in specific operations from prior work at each stage:\looseness=-1
\begin{enumerate}
\item \textbf{Goal Selection:} Choose a goal position on the desired trajectory to plan to and the time at which the goal position should be (or should have been) reached,\looseness=-1
\item \textbf{Discrete Search:} Find a discrete spatiotemporal path to the goal that minimizes the probability of collision with static and dynamic obstacles, DSHT violations, and total duration, distance, and the number of rotations,\looseness=-1
\item \textbf{Trajectory Optimization:} Solve a quadratic program (QP) to safely fit a dynamically feasible trajectory to the discrete plan while preserving i) the collision probabilities computed and ii) DSHT elements not violated during search.\looseness=-1
\end{enumerate}

% % Figure environment removed

The contributions of our work are as follows:\looseness=-1
\begin{itemize}
    \item We define a representation for interactive behaviors of dynamic obstacles that can be used within a planner. We propose three model-based prediction algorithms to predict interactive behavior models of dynamic obstacles.\looseness=-1
    \item We propose a decentralized real-time trajectory planning algorithm for multi-robot navigation in cluttered environments that produces dynamically feasible trajectories avoiding static and (interactive) dynamic obstacles and teammates that plan asynchronously. \looseness=-1
    Our algorithm handles message delays, drops, and re-orderings between teammates.
    It explicitly accounts for sensing uncertainty with static obstacles and prediction uncertainty with dynamic obstacles.
    % \item We present a real-time trajectory planning algorithm that probabilistically avoids collisions against static and interactive dynamic obstacles, and produces dynamically feasible polynomial trajectories. 
    \item We evaluate our algorithm extensively in simulations to show its performance under different environments and configurations using 25400 randomly generated runs.
    We compare its performance to three state-of-the-art multi-robot navigation decision-making algorithms using 2100 randomly generated runs, and show that our algorithm achieves up to 1.68x success rate using as low as 0.28x time in the single-robot case, and 2.15x success rate using as low as 0.36x time in multi-robot scenarios compared to the best baseline.
    We implement our algorithm for physical quadrotors and show its feasibility in the real world.\looseness=-1
\end{itemize}


\section{Related Work}\label{Section:RelatedWork}

% \textbf{Single-robot polynomial trajectory generation for static obstacle avoidance:} Single robot collision-free polynomial trajectory generation for static obstacle avoidance has been extensively studied. 
% \cite{richter2013planning} uses RRT*~\cite{karaman2010rrtstar} to find a collision-free path in environments with only static obstacles, followed by solving a quadratic program to smoothen the path to a dynamically feasible piecewise polynomial trajectory. Polynomial spline trajectory generation based on local trajectory optimization in which collision avoidance against static obstacles is integrated into the cost function~\cite{oleynikova2016quad} is proposed, which makes safety a soft contraint.~\cite{chen2016planning} uses standard A* search where octrees~\cite{hornung2013octomap} are used for static obstacle representation to plan a kinematically safe path, and smooths it safely by containing the final plan in a safe navigation corridor (SNC) lying in the empty octree cells.
% Computing a collision-free discrete path using jump point search (JPS)~\cite{harabor2011jps} followed by SNC based polynomial optimization is also proposed~\cite{liu2017planning}. 
% \cite{usenko2017real} proposes a B-spline~\cite{piegl1996nurbs} generation algorithm using locally built maps, which are represented with 3D circular buffers.
% A piecewise B\'ezier curve~\cite{prautzsch2002bezier} based polynomial trajectory generation algorithm~\cite{gao2018fast} in which an initial collision-free trajectory is computed using fast marching~\cite{sethian1999fast}, followed by optimization within an SNC is also proposed.
% \cite{tordesillas2022faster} combines JPS with SNC construction and solves an optimization problem to compute a piecewise polynomial that allows navigation in unknown environments.\looseness=-1

\textbf{Static and dynamic obstacle avoidance and accounting for uncertainty:} Various approaches for avoiding static and dynamic obstacles and integrating uncertainty associated with several sources (\eg unmodeled system dynamics, state estimation inaccuracy, perception noise, or prediction inaccuracies) have been proposed. 
\cite{tordesillas2020mader} proposes a polynomial trajectory planner to avoid static obstacles and dynamic obstacles given their predicted trajectories along with a maximum prediction error. 
\cite{qi2023unstruc} combines motion primitive search with spline optimization for static and dynamic obstacle avoidance. Chance constrained RRT (CC-RRT)~\cite{luders2010chance} plans trajectories to avoid static and dynamic obstacles, limiting the probability of collisions under Gaussian system and prediction noise.
\cite{aoude2013probabilistically} performs trajectory prediction using Gaussian mixture models to estimate motion models of dynamic obstacles, and uses these models within an RRT variant to predict their trajectories as a set of trajectory particles within CC-RRT to compute and limit collision probabilities.
\cite{zhu2019chance} proposes a chance-constrained MPC formulation for static and dynamic obstacle avoidance where uncertainty stems from Gaussian system model and state estimation noise, and dynamic obstacle model noise where dynamic obstacles are modeled using constant velocities with Gaussian acceleration noise.
RAST~\cite{chen2023rast} is a risk-aware planner that does not require segmenting obstacles into static and dynamic, but uses a particle-based occupancy map in which each particle is associated with a predicted velocity; and~\cite{nair2022collision} an MPC-based collision avoidance method where uncertainty stems from system noise of the robot and prediction noise for dynamic obstacles.
\cite{janson2018monte} uses a Monte Carlo sampling to compute collision probabilities of trajectories under system uncertainty.\looseness=-1

Prior decentralized decision-making approaches have been proposed for the cooperative navigation of multiple robots, in which each robot computes its own plan, cooperating with others during decision-making for collision avoidance.
We classify them into two groups: short and medium horizon decision-making algorithms, where the algorithms in the former output a single action to execute, while the algorithm in the latter output medium horizon trajectories, e.g., trajectories that are $2-10$ seconds long, in a receding horizon fashion.
Our approach falls into the latter category.\looseness=-1

\textbf{Short horizon multi-robot decision making:}~\cite{van2011reciprocal} presents optimal reciprocal collision avoidance (ORCA), a velocity obstacle-based approach, which outputs velocity commands.
\cite{wang2017safety} utilizes safety barrier certificates (SBC) for collision avoidance, which outputs acceleration commands.
% ORCA and SBC utilize position and velocity sensing.
GLAS~\cite{riviere2020glas} combines a learned network trained to imitate a global planner~\cite{honig2018quadswarms} with a safety module to generate safe actions.
\cite{nn2021batra} proposes using a learned network to control the thrusts of quadrotor propellers.
Several approaches to solve multi-agent path finding problems on grids using learned networks with direction outputs have also been proposed~\cite{damani2020primal2,li2020gnn}.\looseness=-1

\textbf{Medium horizon multi-robot decision making:}
\cite{zhou2017bvc} utilizes buffered Voronoi cells (BVC) within a model predictive control (MPC) framework, where each robot stays within its cell in each planning iteration.
BVC requires position-only sensing and does not depend on inter-robot communication.
~\cite{wang2021dpmc} presents a distributed model predictive control (DMPC) scheme that requires full state sensing  between robots.
Utilizing a DMPC scheme with full plan communication is also proposed~\cite{luis2019dmpc,luis2020dmpc}.
Accounting for asynchronous planning between robots becomes essential when planning durations increase.
\cite{senbaslar2022async} introduces discretized separating hyperplane trajectories (DSHTs) as a constraint generation mechanism to account for asynchronous planning under imperfect inter-robot communication, and extends BVC planner with the DSHTs to adopt it to asynchronous planning scenarios.
Differential flatness~\cite{murray1995differential} of the underlying systems is utilized to plan in the output space instead of the input space by many planners, which allows planning continuous splines with limited derivative magnitudes to ensure dynamic feasibility.
RTE~\cite{senbaslar2018rte} uses buffered Voronoi cells in a spline optimization framework and combines the optimization with discrete planning to locally resolve deadlocks.
Obstacle avoidance is ensured using safe navigation corridors (SNC) during optimization.
RLSS~\cite{senbaslar2023rlss} uses support vector machines instead of Voronoi diagrams to support robots with any convex shape and ensures kinematic feasibility of the generated problem.
% It also uses SNCs for obstacle avoidance.
MADER~\cite{tordesillas2020mader} combines discrete planning with spline optimization, treating SNC constraints as decision variables in a non-linear optimization problem.
It explicitly accounts for asynchronous planning using communication, while assuming instantaneous perfect communication between robots.
RMADER~\cite{kondo2023rmader} extends MADER to handle communication delays with known bounds between teammates.
RSFC~\cite{park2020rsfc,park2021rsfc} plans for piecewise splines with B\'ezier curve~\cite{farouki2012bernstein} pieces where safety between robots is ensured by making sure that their relative trajectories are in a safe set, where trajectories between robots are shared with instantaneous perfect communication.
LSC~\cite{park2022lsc} extends RSFC by using linear safety constraints without slack variables, which may cause the final solution to be unsafe in RSFC.
Ego-swarm~\cite{zhou2021ego} formulate collision avoidance as a cost function, which they optimize using gradient-based local optimization.
TASC~\cite{toumieh2022tasc,toumieh2023tasc} uses SNCs, which it computes between the communicated plans of other robots and the last plan of the planning robot.
TASC accounts for bounded communication delays.\looseness=-1

\textbf{Prediction of dynamical systems:} Predicting future states of dynamical systems is studied extensively and many recent approaches have been developed in the autonomous vehicle domain. 
\cite{wiest2012pred} uses Gaussian mixture models to estimate a Gaussian distribution over the future states of a vehicle given its past states.
\cite{lee2017desire} predicts future trajectories of dynamic obstacles by learning a posterior distribution over future dynamic obstacle trajectories given past trajectories.
Multi-modal prediction for vehicles to tackle bias against unlikely future trajectories during training is also investigated~\cite{kim2022diverse}.
\cite{bartoli2018context} presents a method for human movement prediction using context information modeling human-human and human-static obstacle interactions.
\cite{zhou2023dyn} generates multi-modal pedestrian predictions utilizing and modeling social interactions between humans and human intentions. 
State-of-the-art approaches that predict future trajectories of dynamic obstacles given past observations, potentially in a multi-modal way, use relatively computationally heavy approaches making them hard to re-query to model interactivity between the robot and dynamic obstacles during decision-making. 
In this paper, we propose \emph{policies} that are fast to query as prediction outputs instead of future trajectories.
Policies model intentions of the dynamic obstacles (movement models) as well as the interaction between dynamic obstacles and the robot (interaction models) as vector fields of velocities.\looseness=-1

% In this sense, they resemble artificial potential fields describing the movement of objects in velocity space.

% In the most generic sense, artificial potential fields`\cite{khatib1986real} are functions from states to actions, describing the behavior of robots.
% and have been extensively studied in robot navigation; obstacles are modeled using repulsive fields and navigation goals are modeled using attractive fields.
% Navigation functions~\cite{rimon1990exact}, a special case of artificial potential fields, can solve the exact robot navigation problem with perfect information where obstacles are spherical; subsequent work has applied them to dynamic obstacle avoidance~\cite{ge2002dynamic}. When perfect information is not available, local minima may cause robots to deadlock. These situations can be alleviated by adding virtual obstacles~\cite{park2003new}, switching between fields~\cite{fedele2018obstacles}, or using simulated annealing~\cite{qidan2006sim}. While we do not use artificial potential fields for decision making of the robot, we use vector fields in velocity space to model dynamic obstacle movements, which can be considered artificial potential fields for a single-integrator system.
%Their application to robot navigation is an indication that they can model complex behaviors of dynamic objects.
%We use the vector fields to simulate the behavior of dynamic obstacles in response to the actions we plan for the robot.

\section{Problem Definition}\label{Section:Problem}

Consider a team of $\#^R$ robots.
Let $\mR_i^{robot}: \mathbb{R}^d \rightarrow P(\mathbb{R}^d)$ be the convex set-valued collision shape function of robot $i$, where $i \in \{1, \ldots, \#^R\}$ and $\mR_i^{robot}(\vp)$ is the space occupied by the robot when placed at position $\vp$.
Here, $d \in \{2,3\}$ is the ambient dimension that the robots operate in, and $P(\mathbb{R}^d)$ is the power set of $\mathbb{R}^d$.
We assume that the robots are rigid, and the collision shape functions are defined as $\mR_i^{robot}(\vp) = \mRxy{i}{\vzero}^{robot} \oplus \{\vp\}$ where $\mRxy{i}{\vzero}^{robot}$ is the shape of robot $i$ when placed at the origin $\vzero$ and $\oplus$ is the Minkowski sum operator.\looseness=-1
% \footnote{Note that the shape of the robot does not depend on the orientation, which means that either robot's orientation is fixed or its collision shape contains the robot in all orientations.}
% We assume that robot's collision shape is convex.

We assume that the robots are differentially flat~\cite{murray1995differential}, i.e., their states and inputs can be expressed in terms of their output trajectories and their finite derivatives, and the output trajectory is the Euclidean trajectory that the robot follows.
When a system is differentially flat, its dynamics can be accounted for by imposing output trajectory continuity up to the required degree of derivatives and imposing constraints on maximum derivative magnitudes.
Many existing systems like quadrotors~\cite{mellinger2011snap} or car-like robots~\cite{murray1993CarLike} are differentially flat.
Each robot $i$ requires output trajectory continuity up to degree $c_i$, and has maximum derivative magnitudes $\gamma_i^k$ for derivative degrees $k \in \{1, \ldots, K_i\}$.\looseness=-1

Each robot $i$ detects objects and classifies them into three sets: static obstacles $\mO_i$, dynamic obstacles $\mD_i$, and teammates $\mC_i$.
Static obstacles do not move.
Dynamic obstacles move with or without interaction with the robot.
Teammates are objects that navigate using our planner, i.e., the other robots.\looseness=-1
% We do not model interactions between dynamic obstacles, while in reality, this interaction may exist.
% All objects have convex shapes, which are sensed by the perception system.\footnote{If a sensed object has a non-convex shape, its convex hull can be used. In that case, our planner will avoid the convex hull of the object.}

Each static obstacle $j \in \mO_i$ has a convex shape $\mQ_{i, j} \subset \mathbb{R}^d$, and has an existence probability $p_{i, j}^{stat} \in [0, 1]$.
Many existing data structures including occupancy grids~\cite{homm2010efficient} and octrees~\cite{hornung2013octomap} support storing obstacles in this form.
Each perceived teammate $j\in \mC_i$ has a convex shape $\mS_{i, j}$ sensed by robot $i$.\looseness=-1

Each dynamic obstacle $j\in\mD_i$ is modeled using i) its current position $\vp^{dyn}_{i, j}$, ii) its convex set valued collision shape function $\mR_{i, j}^{dyn}: \mathbb{R}^d \rightarrow P(\mathbb{R}^d)$ where $\mR_{i,j}^{dyn}(\vp) = \mR_{i,j,\vzero}\oplus \{\vp\}$ and $\mR_{i, j, \vzero}$ is the shape of obstacle $j$ when placed at the origin, and iii) a probability distribution over its $\#_{i,j}^B$ predicted behavior models $\mB_{i, j, k}$, $k\in\{1, \ldots, \#_{i,j}^B\}$, where each behavior model is a 2-tuple $\mB_{i, j, k} = (\mM_{i, j, k}, \mI_{i, j, k})$ such that 
% $\vp_{\fD,i}$ is the current position of the dynamic obstacle, 
% $\mR_{\fD, i}: \mathbb{R}^d \rightarrow P(\mathbb{R})$ is the convex collision shape function of the dynamic obstacle, 
$\mM_{i, j, k}$ is the movement and $\mI_{i, j, k}$ is the interaction model of the dynamic obstacle.
$p^{dyn}_{i,j,k}$ is the probability that dynamic obstacle $j$ moves according to behavior model $\mB_{i, j, k}$ such that $\sum_{k=1}^{\#_{i,j}^B}p^{dyn}_{i, j, k} \leq 1$ for all $j \in \mD_i$.\looseness=-1

A movement model $\mM: \mathbb{R}^d \rightarrow \mathbb{R}^d$ is a function from the dynamic obstacle's position to its desired velocity, describing its intent.
An interaction model $\mI: \mathbb{R}^{4d} \rightarrow \mathbb{R}^d$ is a function describing robot-dynamic obstacle interaction of the form $\vv^{dyn} = \mI(\vp^{dyn}, \tilde{\vv}^{dyn}, \vp^{robot}, \vv^{robot})$.
Its arguments are 4 vectors: position $\vp^{dyn}$ of the dynamic obstacle, desired velocity $\tilde{\vv}^{dyn}$ of the dynamic obstacle (which is obtained from the movement model, i.e., $\tilde{\vv}^{dyn} = M(\vp^{dyn})$), and position $\vp^{robot}$ and velocity $\vv^{robot}$ of robot.
It outputs the velocity $\vv^{dyn}$ of the dynamic obstacle.
Notice that interaction models do not model interactions between multiple dynamic obstacles or interactions with multiple teammates, i.e., the velocity $\vv^{dyn}$ of a dynamic obstacle does not depend on the position or velocity of other dynamic obstacles or the other teammates from the perspective of a single teammate.
This is an accurate representation in sparse environments where moving objects are not in close proximity to each other but an inaccurate assumption in dense environments.
We choose to model interactions this way for computational efficiency as well as non-reliance on perfect communication: modeling interactions between multiple dynamic obstacles would result in a combinatorial explosion of possible dynamic obstacle behaviors since we support multiple hypotheses for each dynamic obstacle, and modeling interactions of dynamic obstacles with multiple teammates would require joint planning for all robots, requiring perfect communication\footnote{One could also define a single joint interaction model for all dynamic obstacles and perform non-probabilistic decision making with respect to dynamic obstacles if inter-dynamic obstacle interactions exist and single dynamic obstacle models are insufficient at describing the dynamic obstacle behaviors.}.
While using only position and velocity to model robot-dynamic obstacle interaction is an approximation of reality, we choose this model because of its simplicity.
This simplification allows us to use interaction models to update the behavior of dynamic obstacles during discrete search efficiently\footnote{During planning, we evaluate movement and interaction models sequentially to compute the velocity of dynamic obstacles.
One could also combine movement and interaction models and have a single function to describe the dynamic obstacle behavior for planning.
We choose to model them separately to allow separate predictions of these models.}.\looseness=-1

Each robot $i$ has a state estimator that estimates its output derivatives up to derivative degree $c_i$, where degree $0$ corresponds to position, degree $1$ corresponds to velocity, and so on.
If state estimation accuracy is low, the trajectories computed by the planner can be used to compute the expected derivatives in an open-loop fashion assuming perfect execution.
The $k^{th}$ derivative of robot $i$'s current position is denoted with $\vp^{self}_{i,k}$ where $k \in \{0, \ldots, c_i\}$.\looseness=-1
% $\{\vp_{i,0}, \ldots, \vp_{i, c_i}\}$ is the full current state of the robot.
% We do not utilize the noise associated with state estimation during planning.

Each robot $i$ is tasked with following a desired trajectory $\vd_i(t): [0, T_i] \rightarrow \mathbb{R}^d$ with duration $T_i$ without colliding with obstacles.
% We define $\vd_i(t) = \vd(T_i)\ \forall t > T_i$.
The desired trajectory $\vd_i(t)$ can be computed by a global planner using potentially incomplete prior knowledge about obstacles.
It does not need to be collision-free with respect to static or dynamic obstacles.
If no such global planner exists, it can be set to a straight line from a start position to a goal position.\looseness=-1

\section{Approach}

% % Figure environment removed

To follow the desired trajectories $\vd_i$ as closely as possible while avoiding collisions, we propose a decentralized real-time planner executed in a receding horizon fashion.\looseness=-1

\subsection{Discretized Separating Hyperplane Trajectories (DSHTs)}

We utilize DSHTs~\cite{senbaslar2022async} as constraints for inter-robot collision avoidance, which allows us to enforce safety when planning is asynchronous, i.e., robots start and end planning at different time points, and the communication medium is imperfect.
We briefly reiterate the theory behind DSHTs next.\looseness=-1

Let $\Omega$ be a commutative deterministic separating hyperplane computation algorithm:
it computes a separating hyperplane between two linearly separable sets, and each call to it with the same pair of arguments results in the same hyperplane. We use hard-margin support vector machines (SVM) as $\Omega$.\looseness=-1

Let $\vf_i(t):[0, T_{cur}] \rightarrow \mathbb{R}^d$ and $\vf_j(t):[0, T_{cur}]\rightarrow \mathbb{R}^d$ be the trajectories robots $i$ and $j$ executed from navigation start time $0$ to current time $T_{cur}$, respectively.
The separating hyperplane trajectory $\mH_{i,j}:[0, T_{cur}]\rightarrow \mH^d$ between robots $i$ and $j$ induced by $\Omega$ is defined as $\mH_{i,j}(t) = \Omega(\mR_i^{robot}(\vf_i(t)), \mR_j^{robot}(\vf_j(t)))$ where $\mH^d$ is the set of all hyperplanes in $\mathbb{R}^d$.\looseness=-1

Each robot $i$ stores a tail time point variable $T^{tail}_{i, j}\leq T_{cur}$ for each other robot $j$ denoting the time point after which the hyperplanes in $\mH_{i, j}$ should be used to constrain robot $i$'s plan against robot $j$.
If robot $i$ starts planning at $T_{cur}$, it uses all hyperplanes $\mH_{i, j}(t)$ where $t\in [T^{tail}_{i, j}, T_{cur}]$ to constrain itself against robot $j$ by enforcing its trajectory to be in the safe side of each hyperplane.
When robot $i$ successfully finishes a planning iteration that started at $T_{i, start}$, meaning that it is now constrained by hyperplanes from $T^{tail}_{i, j}$ to $T_{i, start}$ on $\mH_{i, j}$ against each other robot $j$, it broadcasts its identity $i$ and $T_{i, start}$.
Robots $j$ receiving the message update their tail time points against robot $i$ by setting $T^{tail}_{j, i}=T_{i, start}$, discarding constraints, and those that do not receive it do not update their tail points, over-constraining themselves against robot $i$.
As shown in~\cite{senbaslar2022async}, this constraint discarding and over-constraining mechanism ensures that active trajectories of each pair of robots share a constraining hyperplane at all times under asynchronous planning, message delays, drops and re-orderings.\looseness=-1

Let $\mH^{active}_{i,j} = \{\mH_{i,j}(t)\ |\ t\in [T^{tail}_{i,j}, T_{cur}]\}$ be the active set of separating hyperplanes of robot $i$ against robot $j$.
There are infinitely many hyperplanes in $\mH^{active}_{i,j}$ when $T^{tail}_{i,j} < T_{cur}$.
We sample hyperplanes in $\mH^{active}_{i, j}$ using a sampling step in the time domain shared among all teammates.
Let $\tilde{\mH}^{active}_{i, j}$, which is the active DSHT of robot $i$ against robot $j$, be the finite sampling of $\mH^{active}_{i, j}$, and $\tilde{\mH}^{active}_i = \{\mH \in\tilde{\mH}^{active}_{i, j}\ |\ j\in\{1, \ldots, \#^R\}\setminus \{i\}\}$ be the set of all hyperplanes that should constraint robot $i$ on a planning iteration that starts at time $T_{cur}$.
We use hyperplanes $\tilde{\mH}^{active}_i$ during planning to enforce safety against robot teammates.\looseness=-1

% % Figure environment removed

It is assumed that perception, prediction, and state estimation systems are executed independently from the planner and produce the information described in Sec.~\ref{Section:Problem}.
DSHT computation is done asynchronously and independently from the planner, maintaining tail time points $T_{i,j}$ and providing $\tilde{\mH}^{active}_i$ to the planner.
The inputs from these systems to the planner in robot $i$ are:\looseness=-1
\begin{itemize}
\item \textbf{Static obstacles}: Convex shapes $\mQ_{i, j}$ with their existence probabilities such that $p^{stat}_{i,j}$ is the probability that obstacle $j \in \mO_i$ exists.\looseness=-1
\item \textbf{Dynamic obstacles}: Set $\mD_i$ of dynamic obstacles where each dynamic obstacle $j\in \mD_i$ has the current position $\vp^{dyn}_{i,j}$, collision shape function $\mR^{dyn}_{i,j}$, and behavior models $\mB_{i, j, k}$ with corresponding realization probabilities $p^{dyn}_{i, j, k}$ where $k\in\{1,\ldots,\#^B_{i, j}\}$.\looseness=-1
\item \textbf{Active DSHTs}: Set $\tilde{H}_i^{active}$ of separating hyperplanes against all other robots.\looseness=-1
\item \textbf{Self state}: The state $\{\vp^{self}_{i,0}, \ldots, \vp^{self}_{i,c_i}\}$ of the robot.\looseness=-1
\end{itemize}

There are three stages of our algorithm: i) goal selection, which selects a goal position on the desired trajectory to plan to, ii) discrete search, which computes a spatiotemporal discrete path to the goal position, minimizing the probability of collision with two classes of obstacles, DSHT violations, distance, duration, and rotations using a multi-objective search method, and iii) trajectory optimization, which safely computes a dynamically feasible trajectory by smoothing the discrete path while preserving the collision probabilities computed and DSHT hyperplanes not violated during the search.
The planner might fail during trajectory optimization, the reasons for which are described in Sec.~\ref{Section:TrajectoryOptimization}.
If planning fails, the robot continues using its previous plan.\looseness=-1
% If planning succeeds, the robot replaces its plan with the new one.

\subsection{Goal Selection}\label{Section:GoalSelection}

% Figure environment removed

In the goal selection stage (Fig.~\ref{Figure:GoalSelection}), each robot $i$ chooses a goal position $\vg_i$ on the desired trajectory $\vd_i$ and the time $T_i'$ at which $\vg_i$ should be (or should have been) reached. This stage has two parameters: the desired time horizon $\tau_i$ and the static obstacle existence probability threshold $p_{i}^{min}$.\looseness=-1

First, the closest point $\vx$ on the desired trajectory $\vd_i$ to the robot's current position $\vp^{self}_{i, 0}$, is found by discretizing $\vd_i$. 
Let $\tilde{T}$ be the time point of $\vx$ on $\vd_i$, i.e., $\vx=\vd_i(\tilde{T})$.
Then, goal selection finds the smallest time point $T_i' \in [\min(\tilde{T} + \tau_i, T_i), T_i]$ on $\vd_i$ such that the robot is collision-free against static obstacles with existence probabilities at least $p^{min}_i$ when placed on $\vd_i(T_i')$ using collision checks with small increments in time.
The goal position $\vg_i$ is set to $\vd_i(T_i')$, and the time at which it should be (or should have been) reached is $T_i'$. We assume that the robot placed at $\vd_i(T_i)$ is collision-free; hence such a $T_i'$ always exists.\looseness=-1

The selected goal position $\vg_i$ and the time point $T_i'$ are used during the discrete search stage, which uses a goal-directed search algorithm.
Note that goal selection chooses the goal position on the desired trajectory without considering its reachability; the actual trajectory the robot follows is planned by the rest of the algorithm.\looseness=-1


\subsection{Discrete Search}\label{Section:DiscreteSearch}

In the discrete search stage, we plan a path to the goal position $\vg_i$ using cost algebraic $\text{A}^*$ search~\cite{edelkamp2005cost}.
Cost algebraic $\text{A}^*$ is a generalization of standard $\text{A}^*$ to a richer set of cost systems, namely cost algebras.
We compute six cost terms during the search, define the cost of an action as the vector of the computed cost terms, and order the cost vectors lexicographically, forming a cost algebra.
% Our cost vectors with lexicographical ordering form a cost algebra.
Cost algebraic $\text{A}^*$ finds an optimal action sequence according to the lexicographical ordering of our cost vectors. Here, we summarize the formalism of cost algebras from the original paper~\cite{edelkamp2005cost}.
The reader is advised to refer to the original paper for a detailed and complete description of concepts.\looseness=-1

\begin{definition}
Let $A$ be a set and $\times: A\times A \rightarrow A$ be a binary operator. A monoid is a tuple $(A, \times, \vzero)$  if the identity element $\vzero \in A$ exists, $\times$ is associative, and $A$ is closed under $\times$.\looseness=-1
\end{definition}

\begin{definition}
Let $A$ be a set. A relation $\preceq\ \subseteq A \times A$ is a total order if it is reflexive, anti-symmetric, transitive, and total.
The least operation $\sqcup$ gives the least element of the set according to a total order, i.e., $\sqcup A = c \in A$ such that $c \preceq a\ \forall a\in A$, and the greatest operation $\sqcap$ gives the greatest element of the set according to the total order, i.e., $\sqcap A = c\in A$ such that $a \preceq c\ \forall a \in A$.\looseness=-1
\end{definition}

\begin{definition}
A set $A$ is isotone if $a \preceq b$ implies both $a \times c \preceq b \times c$ and $c \times a \preceq c \times b$ for all $a,b,c\in A$.
$a \prec b$ is defined as $a \preceq b \wedge a \neq b$.
A set $A$ is strictly isotone if $a \prec b$ implies both $a \times c \prec b \times c$ and $c\times a \prec c \times b$ for all $a,b,c\in A, c\neq \vone$ where $\vone = \sqcap A$.\looseness=-1
\end{definition}

\begin{definition}
A cost algebra is a 6-tuple $(A, \sqcup, \times, \preceq, \vone, \vzero)$ such that $(A, \times, \vzero)$ is a monoid, $\preceq$ is a total order, $\sqcup$ is the least operation induced by $\preceq$, $\vone = \sqcap A$, and $\vzero = \sqcup A$, i.e. the identity element is the least element.\looseness=-1
\end{definition}

Intuitively, $A$ is the set of cost values, $\sqcup$ is the operation used to select the best among the values, $\times$ is the operation to cumulate the cost values, $\preceq$ is the operator to compare the cost values, $\vone$ is the greatest and $\vzero$ is the least cost value as well as the identity cost value under $\times$.\looseness=-1

% In this paper, we use two cost algebras, namely $(\mathbb{R}_{\geq 0} \cup \{\infty\}, min, +, \leq, \infty, 0)$, i.e. real number costs with and $(\mathbb{N}_{\geq 0} \cup \{\infty\}, min, +, \leq, \infty, 0)$. 

To support multiple objectives during search, the prioritized Cartesian product of cost algebras is defined as follows.\looseness=-1

\begin{definition}\label{Definition:PrioritizedCartesianProductOfCostAlgebras}
    The prioritized Cartesian product of cost algebras $C_1 = (A_1, \sqcup_1, \times_1, \preceq_1, \vone_1, \vzero_1)$ and $C_2 =  (A_2, \sqcup_2, \times_2, \preceq_2, \vone_2, \vzero_2)$, denoted by $C_1 \times_p C_2$ is a tuple $(A_1 \times A_2, \sqcup, \times, \preceq, (\vone_1, \vone_2), (\vzero_1, \vzero_2))$ where $(a_1, a_2) \times (b_1, b_2) = (a_1 \times_1 b_1, a_2 \times_2 b_2)$, $(a_1,a_2) \preceq (b_1,b_2)$ iff $a_1 \prec_1 b_1 \vee (a_1 = b_1 \wedge a_2 \preceq_2 b_2)$, and $\sqcup$ is induced by $\preceq$.\looseness=-1
\end{definition}

Note that, $\preceq$ in Def.~\ref{Definition:PrioritizedCartesianProductOfCostAlgebras} induces lexicographical ordering among cost algebras $C_1$ and $C_2$.\looseness=-1

\begin{proposition}\label{Proposition:CartesianProductOfCostAlgebras}
If $C_1$ and $C_2$ are cost algebras, and $C_1$ is strictly isotone, then $C_1 \times_p C_2$ is also a cost algebra.
If, in addition, $C_2$ is strictly isotone, $C_1 \times_p C_2$ is also strictly isotone.\looseness=-1
\begin{proof}
Given in~\cite{edelkamp2005cost}.
\end{proof}
\end{proposition}

Proposition~\ref{Proposition:CartesianProductOfCostAlgebras} allows one to take the Cartesian product of any number of strictly isotone cost algebras and end up with a strictly isotone cost algebra.\looseness=-1

Given a cost algebra $C = (A, \sqcup, \times, \preceq, \vone, \vzero)$, cost algebraic A* finds a lowest cost path according to $\sqcup$ between two nodes in a graph where edge costs are elements of set $A$, which are ordered according to $\preceq$ and combined with $\times$ where the lowest cost value is $\vzero$ and the largest cost value is $\vone$.
Cost algebraic A* uses a heuristic for each node of the graph, and cost algebraic A* with re-openings finds cost-optimal paths only if the heuristic is admissible.
An admissible heuristic for a node is a cost $h\in A$, which underestimates the cost of the lowest cost path from the node to the goal node according to $\preceq$.\looseness=-1

We conduct a multi-objective search, in which each individual cost term is a strictly isotone cost algebra, and optimize over their Cartesian product.
The individual cost terms are defined over two cost algebras, namely $(\mathbb{R}_{\geq 0} \cup \{\infty\}, min, +, \leq, \infty, 0)$, i.e., non-negative real number costs with standard addition and comparison, and $(\mathbb{N} \cup \{\infty\}, min, +, \leq, \infty, 0)$, natural numbers with standard addition and comparison, both of which are strictly isotone.
Therefore, any number of their Cartesian products are also cost algebras by Proposition~\ref{Proposition:CartesianProductOfCostAlgebras}.\looseness=-1

We explain the discrete search for an arbitrary robot $i$ in the team; each robot runs the same algorithm.
The planning horizon of the search is $\tau_i' = \max(\tilde{\tau}_i, T_i' - T_{cur}, \alpha_i\frac{\normtwo{\vp_{i,0}^{self} - \vg_i}}{\tilde{\gamma}_i^1})$ where $\tilde{\tau}_i$ is the minimum search horizon and $\tilde{\gamma}_i^1$ is the maximum speed parameter for the search stage. 
In other words, the planning horizon is set to the maximum of minimum search horizon, the time difference between the goal time point and the current time point, and a multiple of the minimum required time to reach the goal position $\vg_i$ from the current position $\vp_{i,0}^{self}$ applying maximum speed $\tilde{\gamma}^1_i$ where multiplier $\alpha_i \geq 1$.
The planning horizon $\tau_i'$ is used as a suggestion in the search and is exceeded if necessary, as explained later in this section.\looseness=-1

\textbf{States.} The states $x$ in our search formulation have six components: i) $x.\vp \in \mathbb{R}^d$ is the position of the state, ii) $x.\vDelta \in \{-1, 0, 1\}^d \setminus \{\vzero\}$ is the direction of the state on a grid oriented along robot's current velocity $\vp_{i,1}^{self}$ with a rotation matrix $R_{rot} \in SO(d)$ such that $R_{rot}(1, 0, \ldots, 0)^\top = \frac{\vp_{i, 1}^{self}}{\normtwo{\vp_{i,1}^{self}}}$, iii) $x.t \in [0, \infty)$ is the time of the state, iv) $x.\mO\subseteq\mO_i$ is the set of static obstacles that collide with the robot $i$ following the path from start state to $x$, v) $x.\mD$ is the set of dynamic obstacle behavior model--position pairs $(\mB_{i,j,k}, \vp_{i,j,k}^{dyn})$ such that dynamic obstacle $j$ moving according to $\mB_{i,j,k}$ does not collide the robot $i$ following the path from start state to $x$, and the dynamic obstacle ends up at position $\vp_{i,j,k}^{dyn}$, and vi) $x.\mH \subseteq \tilde{\mH}_{i}^{active}$ is the set of active DSHT hyperplanes that the robot $i$ violates following the path from start state to $x$.\looseness=-1

The start state of the search is $x^1$ with components $x^1.\vp = \vp_{i, 0}^{self}$, $x^1.\vDelta = (1, 0, \ldots, 0)^\top$, $x^1.t = 0$, $x^1.\mO$ are set of all obstacles that intersect with $\mR_i^{robot}(\vp_{i, 0}^{self})$, $x^1.\mD$ contains behavior model--position pairs $(\mB_{i, j, k}, \vp_{i,j}^{dyn})$ of dynamic obstacles $j$ that do not initially collide with robot, i.e. $\mR_i^{robot}(\vp_{i,0}^{self}) \cap \mR_{i,j}^{dyn}(\vp_{i,j}^{dyn}) = \emptyset$, one for each $k\in\{1,\ldots,\#^B_{i,j}\}$, and $x^1.\mH$ contains all hyperplanes in $\tilde{\mH}_i^{active}$ that the robot $i$ violates initially at $\vp_{i,0}^{self}$.
The goal states are all states $x^g$ with position $x^g.\vp = \vg_i$.\looseness=-1

\textbf{Actions.} There are three action types in our search. Let $x$ be the current state and $x^+$ be the state after applying an action.\looseness=-1

\begin{itemize}
\item FORWARD($s$, $t$) moves the current state $x$ to $x^+$ by applying constant speed $s$ along current direction $x.\vDelta$ for time $t$.
% It is only available if $x.t + t \leq \tau'$, i.e. the timepoint of the state after applying the action does not exceed planning horizon.
The state components change as follows.\looseness=-1

\begin{itemize}
    \item $x^+.\vp = x.\vp + R_{rot}\frac{x.\vDelta}{\normtwo{x.\vDelta}}st$
    \item $x^+.\vDelta = x.\vDelta$
    \item $x^+.t = x.t + t$.
    \item We compute static obstacles $\mO^+$ colliding with the robot with shape $\mR_i^{robot}$ travelling from $x.\vp$ to $x^+.\vp$ and set $x^+.\mO = x.\mO \cup \mO^+$.
    \item 
    % First, we initialize $x^+.\mD$ to $\emptyset$. 
    Let $(\mB_{i,j,k}, \vp_{i,j,k}^{dyn}) \in x.\mD$ be a dynamic obstacle behavior model--position pair that does not collide with the state sequence from the start state to $x$.
    Note that robot applies velocity $\vv =  \frac{x^+.\vp - x.\vp}{x^+.t - x.t}$ from state $x$ to $x^+$.
    We get the desired velocity $\tilde{\vv}_{i,j,k}^{dyn}$ of the dynamic obstacle at time $x.t$ using its movement model: $\tilde{\vv}_{i,j,k}^{dyn} = \mM_{i,j,k}(\vp_{i,j,k}^{dyn})$.
    The velocity $\vv_{i,j,k}^{dyn}$ of the dynamic obstacle can be computed using the interaction model: $\vv_{i,j,k}^{dyn} = \mI_{i,j,k}(\vp_{i,j,k}^{dyn}, \tilde{\vv}_{i,j,k}^{dyn}, x.\vp, \vv)$.
    We check whether the dynamic obstacle shape $\mR_{i,j}^{dyn}$ swept between $\vp_{i,j,k}^{dyn}$ and $\vp_{i,j,k}^{dyn} + \vv_{i,j,k}^{dyn} t$ collides with robot shape $\mR_i^{robot}$ swept between $x.\vp$ and $x^+.\vp$.
    If not, we add not colliding dynamic obstacle behavior model by $x^+.\mD = x^+.\mD\ \cup\ \{(\mB_{i,j,k}, \vp_{i,j,k}^{dyn} + \vv_{i,j,k}^{dyn} t)\}$.
    Otherwise, we discard the behavior model.
    % Note that the collision checking during this operation is conservative: we do not consider the time aspect of the collisions, but check if two swept geometries intersect.
    \item We compute the hyperplanes $\mH^+\subseteq \tilde{\mH}_i^{active}$ the robot $i$ violates at $x^+.\vp$, and set $x^+.\mH = x.\mH \cup \mH^+$.
\end{itemize}

\item ROTATE($\vDelta'$) changes the current state $x$ to $x^+$ by changing its direction to $\vDelta'$.
It is only available if $x.\vDelta \neq \vDelta'$.
The rotate action is added to penalize turns during search as discussed in the description of costs.
The state components remain the same except $x^+.\vDelta$ is set to  $\vDelta'$.\looseness=-1
% \begin{itemize}
%     \item $x^+.\vp = x.\vp$
%     \item $x^+.\vDelta = \vDelta'$
%     \item $x^+.t = x.t$
%     \item $x^+.\mO = x.\mO$
%     \item $x^+.\mD = x.\mD$
%     \item $x^+.\mH = x.\mH$
% \end{itemize}

\item REACHGOAL changes the current state $x$ to $x^+$ by connecting $x.\vp$ to the goal position $\vg_i$.
The remaining search horizon for the robot to reach its goal position is given by $\tau_i' - x.t$.
Recall that the maximum speed of the robot during the search is $\tilde{\gamma}^1_i$; hence
the robot needs at least $\frac{\normtwo{\vg_i - x.\vp}}{\tilde{\gamma}^1_i}$ seconds to reach the goal position from state $x$.
% REACHGOAL action is only available when $x.t + t' \leq \tau'$, i.e. robot can reach to the goal position within planning horizon when it moves at its maximum speed.
% We apply the smallest speed that ensures that the robot reaches its goal position within planning horizon by setting $x^+.t = \tau'$.
We set the duration of this REACHGOAL action to the maximum of these two values: $\max(\tau_i' - x.t, \frac{\normtwo{\vg_i - x.\vp}}{\tilde{\gamma}^1_i})$.
Therefore, the search horizon $\tau_i'$ is merely a suggestion during search and is exceeded whenever it is not dynamically feasible to reach the goal position within the search horizon.
The state components change as follows.\looseness=-1
\begin{itemize}
    \item $x^+.\vp = \vg_i$
    \item $x^+.\vDelta = x.\vDelta$
    \item $x^+.t = x.t + \max(\tau_i' - x.t, \frac{\normtwo{\vg_i - x.\vp}}{\tilde{\gamma}^1_i})$
    \item $x^+.\mO$, $x^+.\mD$, and $x^+.\mH$ are computed in the same way as FORWARD.
    % \item $x^+.\mD$ is computed in the same way as FORWARD.
    % \item $x^+.\mH$ is computed in the same way as FORWARD.
\end{itemize}
\end{itemize}
Note that we run interaction models only when a robot applies a time-changing action (FORWARD or REACHGOAL), which is an approximation of reality because dynamic objects can potentially change their velocities between robot actions.
We also conduct \emph{conservative collision checks} against dynamic obstacles because we do not include the time domain in the collision check.
This conservatism allows us to preserve collision probability upper bounds against dynamic obstacles during trajectory optimization as discussed in Sec.~\ref{Section:TrajectoryOptimization}.\looseness=-1

We compute the probability of not colliding with static obstacles and a lower bound on the probability of not colliding with dynamic obstacles for each state of the search tree recursively.
We interleave the computation of sets $x.\mO$ and $x.\mD$ with the probability computation.\looseness=-1

\subsubsection{Computing the Probability of Not Colliding with Static Obstacles} 

% Let $x^{1:n} = x^1, \ldots, x^n$ be a state sequence such that $x^l.t \geq x^{l-1}.t \wedge (x^l.t = x^{l-1}.t \implies x^l.\vp = x^{l-1}.\vp)\ \forall l \in \{2, \ldots n\}$. (Note that our actions result in state sequences in this form.)
Let $x^{1:n} = x^1, \ldots, x^n$ be a state sequence in the search tree.
Let $\mC_{s}(x^{l:m})$ be the proposition that is true if and only if the robot following timed path $(x^l.\vp, x^l.t), \ldots, (x^m.\vp, x^m.t)$ collides with any of the static obstacles in $\mO_i$, where $m\geq l$.
The event of not colliding with any of the static obstacles while following a prefix of state sequence $x^{1:n}$ admits a recursive definition: $\neg\ \mC_{s}(x^{1:l}) = \neg\ \mC_{s}(x^{1:m}) \bigwedge \neg\ \mC_{s}(x^{m:l})\ \forall l\in\{1,\ldots,n\}\ \forall m\in \{1, \ldots, l\}$.\looseness=-1

We compute the probability of not colliding with any of the static obstacles for each prefix of the state sequence $x^{1:n}$ during search and store this probability as metadata of each state.
Probability $p(\neg\ \mC_{s}(x^{1:l}))$ of not colliding with static obstacles $\mO_i$ while traversing the state sequence $x^{1:l}$ is given by:\looseness=-1
\begin{align*}
    p(&\neg\ \mC_{s}(x^{1:l})) = p(\neg\ \mC_{s}(x^{1:l-1}) \wedge \neg\ \mC_{s}(x^{l-1:l}))\\
    &= p(\neg\ \mC_{s}(x^{1:l-1})) p(\neg\ \mC_{s}(x^{l-1:l})\ |\ \neg\ \mC_{s}(x^{1:l-1}))
\end{align*}

The first term $p(\neg\ \mC_{s}(x^{1:l-1}))$ is the recursive term that can be obtained from the parent state during search.\looseness=-1

The second term $p(\neg\ \mC_{s}(x^{l-1:l}) | \neg\ \mC_{s}(x^{1:l-1}))$ is the conditional term that we compute during state expansion.
Let $\mO^{l:m}_i \subseteq \mO_i$ be the set of static obstacles that collide with robot $i$ traversing $x^{l:m}$ where $l \leq m$.
Given that the robot has not collided while traversing $x^{1:l-1}$ means that no static obstacle that collides with the robot while traversing $x^{1:l-1}$ exists.
Therefore, we compute the conditional probability as the probability that none of the obstacles in $\mO^{l-1:l}_i \setminus \mO^{1:l-1}_i$ exists as ones in $\mO^{l-1:l}_i \cap \mO^{1:l-1}_i$ do not exist as presumption.
We assume that static obstacles' non-existence events are independent.
Let $E(j)$ be the event that static obstacle $j\in\mO_i$ exists.
We have\looseness=-1
\begin{align*}
p(&\neg\ \mC_{s}(x^{l-1:l}) \ |\  \neg\ \mC_{s}(x^{1:l-1})) = p\left(\bigwedge_{j \in \mO^{l-1:l}_i \setminus \mO^{1:l-1}_i} \neg\ E(j)\right)\\
&= \prod_{j \in \mO^{l-1:l}_i \setminus \mO^{1:l-1}_i} p(\neg E(j)) = \prod_{j \in \mO^{l-1:l}_i \setminus \mO^{1:l-1}_i} (1-p_{i,j}^{stat})
\end{align*}

The key operation for computing  the conditional is computing the set $\mO^{l-1:l}_i \setminus \mO^{1:l-1}_i$.
We do this in a time-efficient way by using the fact that $x^l.\mO = \mO^{1:l}_i$.
During node expansion, we compute $\mO^{l-1:l}_i$ by querying the static obstacles for collisions against the region swept by $\mR_i^{robot}$ from position $x^{l-1}.\vp$ to $x^l.\vp$.
We obtain $\mO^{1:l-1}_i$ from the parent state's $x^{l-1}.\mO$.
The probability of not colliding is computed according to obstacles in $\mO^{l-1:l}_i\setminus \mO^{1:l-1}_i$ and $x^{l}.\mO$ is set to $\mO^{1:l}_i = \mO^{1:l-1}_i \cup \mO^{l-1:l}_i$ as described before.\looseness=-1

The recursive term $p(\neg\ \mC_{s}(x^{1:1}))$ is initialized for the start state $x^1$ by computing the non-existence probability of obstacles in $x^{1}.\mO$, i.e., $p(\neg\ \mC_{s}(x^{1:1})) = \prod_{j\in x^1.\mO}(1-p_{i,j}^{stat})$.\looseness=-1

\subsubsection{Computing a Lower Bound on the Probability of not Colliding with Dynamic Obstacles}
A lower bound on the probability of not colliding with dynamic obstacles is computed as follows.\looseness=-1

Let $C_d(x^{l:m})$ be the proposition, conditioned on the full state sequence $x^{1:n}$, that is true if and only if the robot following the $(x^l.\vp, x^l.t), \ldots, (x^m.\vp, x^m.t)$ portion of $x^{1:n}$ collides with any of the dynamic obstacles in $\mD_i$ where $l\leq m$.
Similar to the static obstacles, the event of not colliding with any of the dynamic obstacles while following a prefix of the path $x^{1:n}$ is recursive: $\neg\ C_d(x^{1:l}) = \neg\ C_d(x^{1:m}) \bigwedge \neg\ C_d(x^{m:l})\ \forall l\in\{1,\ldots,n\}\ \forall m\in \{1, \ldots, l\}$.\looseness=-1

The formulation of the probability of not colliding with dynamic obstacles is identical to that developed for static obstacles:
\begin{align*}
    p(&\neg\ \mC_d(x^{1:l})) = p(\neg\ \mC_d(x^{1:l-1}) \wedge \neg\ \mC_d(x^{l-1: l})) \\
    &= p(\neg\ \mC_d(x^{1:l-1}))p(\neg\ \mC_d(x^{l-1:l})\ |\ \neg\ \mC_d(x^{1:l-1}))
\end{align*}

The first term $p(\neg\ \mC_d(x^{1:l-1}))$ is the recursive term that can be can be obtained from the parent state during search.\looseness=-1

The second term $p(\neg\ \mC_d(x^{l-1:l})\ |\ \neg\ \mC_d(x^{1:l-1}))$ is the conditional term that is computed during state expansion.
Let $C_{d, j}(x^{l:m})$ be the proposition, conditioned on the full path $x^{1:n}$, that is true if and only if the robot following the $(x^l.\vp, x^l.t), \ldots, (x^m.\vp, x^m.t)$ portion of $x^{1:n}$ collides with dynamic obstacle $j\in\mD_i$ where $l\leq m$.
We assume independence between not colliding with different dynamic obstacles; hence, the conditional term simplifies as follows.\looseness=-1
\begin{align*}
    &p(\neg\ \mC_d(x^{l-1:l})\ |\ \neg\ \mC_d(x^{1:l-1})) \\
    &= p\left(\bigwedge_{j \in \mD_i} \neg \ \mC_{d, j}(x^{l-1:l})\ |\ \bigwedge_{j\in\mD_i} \neg\ \mC_{d, j}(x^{1:l-1})\right)\\
    &= \prod_{j \in \mD_i} p(\neg\ C_{d, j}(x^{l-1:l}))\ |\ \neg\ C_{d, j}(x^{1:l-1}))
\end{align*}

The computation of the terms $p(\neg\ C_{d, j}(x^{l-1:l})\ |\ \neg\ C_{d, j}(x^{1:l-1}))$ for each obstacle $j\in\mD_i$ is done by using $x^{l-1}.\mD$ and $x^{l}.\mD$.
Given that robot following states $x^{1:l-1}$ has not collided with dynamic obstacle $j$ means that no behavior model of $j$ that resulted in a collision while traversing $x^{1:l-1}$ is realized.
We store all not colliding dynamic obstacles behavior models in $x^{l-1}.\mD$.
Within these, all dynamic obstacle modes that do not collide with the robot while traversing from $x^{l-1}$ to $x^{l}$ are stored in $x^{l}.\mD$.
Let $x^l.\mD_j$ be the set of all behavior model indices of dynamic obstacle $j\in\mD_i$ that has not collided with $x^{1:l}$.
The probability that the robot does not collide with dynamic obstacle $j$ while traversing from $x^{l-1}$ to $x^{l}$ given that it has not collided with it while traveling from $x^1$ to $x^{l-1}$ is given by\looseness=-1
\begin{align*}
    p(\neg\ C_{d, j}(x^{l-1:l})\ |\ \neg\ C_{d,j}(x^{1:l-1})) &=\frac{\sum_{k \in x^l.\mD_j} p_{i,j,k}^{dyn}}{\sum_{k \in x^{l-1}.\mD_j}p_{i,j,k}^{dyn}}.
\end{align*}


% Figure environment removed

These computed probabilities for not colliding are lower bounds because \emph{collision checks against dynamic obstacles are done conservatively}, i.e., the time domain is not considered during sweep to sweep collision checks.
Conservative collision checks never miss collisions but may over-report them.\looseness=-1

\textbf{Costs.} 
% Let $p_{s}(x^l) = 1-p(\neg\ C_s(x^{1:l}))$ be the probability of collision with any of the static obstacle while traversing the state sequence $x^{1:l}$.
% Let $p_{d}(x^l) = 1-p(\neg\ C_d(x^{1:l}))$ be an upper bound for the probability of collision with any of the dynamic obstacles while traversing state sequence $x^{1:l}$.
Let $p_{s}(x^l) = 1-p(\neg\ C_s(x^{1:l}))$ be the probability of collision with any of the static obstacles and $p_{d}(x^l) = 1-p(\neg\ C_d(x^{1:l}))$ be an upper bound for the probability of collision with any of the dynamic obstacles while traversing state sequence $x^{1:l}$.
% Let $p_{d}(x^l) = 1-p(\neg\ C_d(x^{1:l}))$ be an upper bound for the probability of collision with any of the dynamic obstacles while traversing state sequence $x^{1:l}$.
% Note that both probabilities can be computed after computing no collision probabilities for each state $x^l$.
We define $P_{s}(t): [0, x^n.t] \rightarrow [0,1]$ of state sequence $x^{1:n}$ as the linear interpolation of $p_{s}$:
\begin{align*}
    P_{s}(t) &= 
    \begin{cases}
        \frac{x^2.t - t}{x^2.t-x^1.t}p_{s}(x^1) \\\ \ \ +\frac{t-x^1.t}{x^2.t-x^1.t}p_{s}(x^2) & x^1.t\leq t <x^2.t\\
        \ldots\ \\
        \frac{x^n.t - t}{x^n.t-x^{n-1}.t}p_{s}(x^{n-1}) \\\ \ \ +\frac{t-x^{n-1}.t}{x^n.t-x^{n-1}.t}p_{s}(x^n) & x^{n-1}.t\leq t \leq x^n.t
        \end{cases}
\end{align*}
We define $P_{d}(t): [0, x^n.t] \rightarrow [0,1]$ of a state sequence $x^{1:n}$ in a similar way using $p_{d}$.
We define $P_c(t):[0, x^n.t]\rightarrow[0, \infty)$ as the linear interpolation of the number of violated hyperplanes in active DSHTs of the state sequence $x^{1:n}$, i.e., the points $(x^1.t, |x^1.\mH|), \ldots, (x^n.t, |x^n.\mH|)$.\looseness=-1

% Let $T_{team}$ be the duration up to which teammate safety is enforced.
% We define teammate safety cost term $\mJ_{team}(x^l) = \int_0^{\min(x^l.t, T_{team})}P_c(\tau)d\tau$, in which violation cost accumulation is cut off at $T_{team}$.
% We use $H_{team}(x^l) = P_c(x^l.t)\max(0, \min(H_{duration}(x^l), T_{team} - x^l.t))$ as an admissible heuristic for $\mJ_{team}(x^l)$.

We associate six different cost terms to each state $x^l$ in state sequence $x^{1:n}$: i) $\mJ_{static}(x^l) \in [0, \infty)$ is the cumulative static obstacle collision probability defined as $\mJ_{static}(x^l) = \int_0^{x^l.t}P_s(\tau)d\tau$, ii) $\mJ_{dynamic}(x^l)\in [0, \infty)$ is the cumulative dynamic obstacle collision probability defined as $\mJ_{dynamic}(x^l) = \int_0^{x^l.t}P_d(\tau)d\tau$, iii) $\mJ_{team}(x^l) \in [0, \infty)$ is the cumulative number of violated active DSHT hyperplanes defined as $\mJ_{team}(x^l) = \int_0^{\min(x^l.t, T^{team}_i)}P_c(\tau)d\tau$, in which violation cost accumulation is cut off at $T_i^{team}$ parameter, iv) $\mJ_{distance}(x^l) \in [0, \infty)$ is the distance traveled from start state $x^1$ to state $x^l$, v) $\mJ_{duration}(x^l) \in [0, \infty)$ is the time elapsed from start state $x^1$ to state $x^l$, and vi) $\mJ_{rotation}(x^l) \in \mathbb{N}$ is the number of rotations from start state $x^1$ to state $x^l$.\looseness=-1

We cut off violation cost accumulation of DSHTs because of the conservative nature of using separating hyperplanes for teammate safety: they divide the space into two disjoint sets linearly without considering the robots' intents.
The robots need to be safe until the next successful planning iteration because of the receding horizon planning, and overly constraining a large portion of the plan at each planning iteration with conservative constraints decreases agility.
We investigate the effects of $T^{team}_i$ on navigation performance in Sec.~\ref{Section:TeammateSafetyEnforcementDurationEvaluation}.\looseness=-1

We compute the cost terms of the new state $x^+$ after applying actions to the current state $x$ as follows.
\begin{itemize}
    \item $\mJ_{static}(x^+) = \mJ_{static}(x) + \int_{x.t}^{x^+.t}P_s(\tau)\tau$
    \item $\mJ_{dynamic}(x^+) = \mJ_{dynamic}(x) + \int_{x.t}^{x^+.t}P_d(\tau)\tau$
    \item $\mJ_{team}(x^+) = \mJ_{team}(x) + \int_{\min(x.t, T_i^{team})}^{\min(x^+.t, T_i^{team})}P_c(\tau)d\tau$
    \item $\mJ_{distance}(x^+) = \mJ_{distance}(x) + \normtwo{x^+.\vp - x.\vp}$
    \item $\mJ_{duration}(x^+) = \mJ_{duration}(x) + (x^+.t - x.t)$
    % \item $\mJ_{rotation}(x^+) = \mJ_{rotation}(x) + (x.\vDelta \neq x^+.\vDelta\ ?\ 1\ :\ 0)$
    \item $\mJ_{rotation}(x^+) = \mJ_{rotation}(x) + \mathbbm{1}_{\neq}(x.\vDelta, x^+.\vDelta)$
\end{itemize}
where $\mathbbm{1}_{\neq}$ is the indicator function with value $1$ if its arguments are unequal, and $0$ otherwise.\looseness=-1

Lower cost (with respect to standard comparison operator $\leq$) is better in all cost terms.
All cost terms have the minimum cost of $0$ and upper bound cost of $\infty$.
All cost terms are additive using the standard addition operator $+$.
$\mJ_{static}, \mJ_{dynamic}, \mJ_{team}, \mJ_{distance}, $ and $\mJ_{duration}$ are cost algebras ($\mathbb{R}_{\geq 0} \cup \{\infty\}$, min, $+$, $\leq$, $\infty$, $0$) and $\mJ_{rotation}$ is cost algebra ($\mathbb{N} \cup \{\infty\}$, min, $+$, $\leq$, $\infty$, $0$), both of which are strictly isotone.
Therefore, their Cartesian product is also a cost algebra, which is what we optimize over.
% The cost algebra we are optimizing over is the Cartesian product of these cost algebras.
The cost $\mJ(x)$ of each state $x$ is:
\begin{align*}
    \mJ(x) = \begin{bmatrix}\mJ_{static}(x)\\ \mJ_{dynamic}(x)\\ \mJ_{team}(x) \\\mJ_{distance}(x)\\ \mJ_{duration}(x)\\ \mJ_{rotation}(x)\end{bmatrix}.
\end{align*} 
We order cost terms lexicographically as required by Cartesian product cost algebras.
A sample state sequence and computed costs are shown in Fig.~\ref{Figure:DiscreteSearch}.\looseness=-1

This induces an ordering between cost terms: we first minimize cumulative static obstacle collision probability, and among the states that minimize that, we minimize cumulative dynamic obstacle collision probability, and so on.
Hence, safety is the primary; distance, duration, and rotation optimality are the secondary concerns.
Out of safety with respect to static and dynamic obstacles and teammates, we prioritize static obstacles over dynamic obstacles, because static obstacles can be considered a special type of dynamic ones, i.e., with $\vzero$ velocity, and hence, prioritizing dynamic obstacles would make the static obstacle avoidance cost unnecessary.
This ordering allows us to optimize the special case first, and then attempt the harder one.
The reason we prioritize dynamic obstacles over teammates is the conservative nature of using DSHTs for teammates.
Violating a separating hyperplane does not necessarily result in a collision because each hyperplane divides the space into two between robots, and the robots occupy a very small portion of their side of each hyperplane.\looseness=-1

The heuristic $H(x)$ we use for each state $x$ during search is as follows.
\begin{align*}H(x) &= \begin{bmatrix}
H_{static}(x) \\
H_{dynamic}(x)\\
H_{team}(x)\\
H_{distance}(x)\\
H_{duration}(x)\\
H_{rotation}(x)
\end{bmatrix} \\
&= \begin{bmatrix}
% \int_{x.t}^{x.t + H_{duration}(x)} P_s(x.t)d\tau\\
P_s(x.t)H_{duration}(x)\\
% \int_{x.t}^{x.t + H_{duration}(x)} P_d(x.t)d\tau\\
P_d(x.t)H_{duration}(x)\\
P_c(x.t)\max(0, \min(H_{duration}(x), T_i^{team} - x.t))\\
\normtwo{x.\vp - \vg_i}\\
\max(\tau_i'-x.t, \frac{H_{distance}(x)}{\tilde{\gamma_1}})\\
0
\end{bmatrix}
\end{align*}
We first compute $H_{distance}(x)$, which we use in the computation of $H_{duration}(x)$.
Then, we use $H_{duration}(x)$ during the computation of $H_{static}(x)$, $H_{dynamic}(x)$, and $H_{team}(x)$.\looseness=-1

\begin{proposition}
    All individual heuristics are admissible.
    \begin{proof}
    
    \textbf{Admissibility of $\boldsymbol{H_{distance}}$:} $H_{distance}(x)$ is the Euclidean distance from $x.\vp$ to $\vg_i$, and never overestimates the true distance.\looseness=-1

    \textbf{Admissibility of $\boldsymbol{H_{duration}}$:} The goal position $\vg_i$ can be any position in $\mathbb{R}^d$, which is an uncountable set.
    The FORWARD and ROTATE actions can only move the robot to a discrete set of positions, which is countable, as any discrete subset of a Euclidean space is countable. 
    Therefore, the probability that the robot reaches $\vg_i$ by only executing FORWARD and ROTATE actions is zero.
    The robot cannot execute any action after REACHGOAL action in an optimal path to a goal state, because the REACHGOAL action already ends in the goal position and any subsequent actions would only increase the total cost.
    Hence, the last action in an optimal path to a goal state should be REACHGOAL.
    There are two cases to consider.\looseness=-1
    
    If the last action while arriving at state $x$ is REACHGOAL, $x.t \geq \tau_i'$ holds (as REACHGOAL enforces this, see the descriptions of actions). Since $x.\vp = \vg_i$, $H_{distance}(x) = 0$. Therefore, $H_{duration}(x) = \max(\tau_i' -x.t, \frac{H_{distance}(x)}{\tilde{\gamma}_i^1}) = 0$, which is trivially admissible, as $0$ is the lowest cost.\looseness=-1
    % in cost algebra $(\mathbb{R}_{\geq 0} \cup \{\infty\}, \min, +, \leq, \infty, 0)$.\looseness=-1

    If the last action while arriving at state $x$ is not REACHGOAL, the search should execute REACHGOAL action to reach to the goal position in the future, which enforces that goal position will not be reached before $\tau_i'$. 
    Also, since the maximum speed that can be executed during search is $\tilde{\gamma}_i^1$, robot needs at least $\frac{H_{distance}(x)}{\tilde{\gamma}_i^1}$ seconds to reach to the goal position as $H_{distance}(x)$ is an admissible heuristic for distance to goal position.
    Hence, $H_{duration}(x) = \max(\tau_i' - x.t, \frac{H_{distance}(x)}{\tilde{\gamma}_i^1})$ is admissible.\looseness=-1

    \textbf{Admissibility of $\boldsymbol{H_{static}}$ and $\boldsymbol{H_{dynamic}}$:}
    We prove the admissibility of $H_{static}$.
    Proof of admissibility of $H_{dynamic}$ follows identical steps.
    $P_s$ is a nondecreasing nonnegative function as it is the accumulation of linear interpolation of probabilities, which are defined over $[0, 1]$.
    % While state expansion, no collision probabilities can never increase, and hence, collision probabilities can never decrease.
    Therefore, $P_s(x.t) \leq P_s(t)$ for $t \geq x.t$ in an optimal path to a goal state traversing $x$.
    % Let $T^g(x)$ be the time point a goal state will be reached in an optimal state sequence from $x$.
    % $T^g(x)$ is unique for each state $x$, because the optimal cost to goal is unique, and $T^g(x) - x.t$ is a part of our cost vector as $\mJ_{duration}$.
    The robot needs at least $H_{duration}(x)$ seconds to reach a goal state from $x$, since $H_{duration}$ is an admissible heuristic.
    Let $T^g(x) \geq H_{duration}(x)$ be the actual duration needed to reach to a goal state from $x$ on an optimal path.
    The actual cumulative static obstacle collision probability to a goal on an optimal path from $x$ is $\int_{x.t}^{x.t + T^g(x)}P_s(\tau)d\tau$.
    We have
    \begin{align*}
        H_{static}(x) &= P_s(x.t)H_{duration}(x) \\
        &= \int_{x.t}^{x.t+H_{duration}(x)}P_s(x.t)d\tau \\
        &\leq \int_{x.t}^{x.t + T^g(x)}P_s(x.t)d\tau \leq \int_{x.t}^{x.t + T^g(x)}P_s(\tau)d\tau.
    \end{align*}
    In other words, $H_{static}(x)$ does not overestimate the true static obstacle cumulative collision probability from $x$ to a goal state.\looseness=-1
    % Since robot needs at least $H_{duration}(x)$ seconds to reach to a goal state, $P_s(x.t)H_{duration}(x)$ underestimates the true static, and $P_d(x.t)H_{duration}(x)$ underestimates the true dynamic obstacle cumulative collision probability from $x$ to a goal state.

    \textbf{Admissibility of $\boldsymbol{H_{team}}$:} 
    Let $x^{1:n}$ be an optimal state sequence from start state $x^1$ to a goal state $x^n$ traversing $x$.
    If $x.t \geq T_i^{team}$, $P_c(t)$ will not be accumulated in the future because of the cut-off.
    If $x.t \leq T_i^{team}$, $P_c(t)$ will be accumulated for a duration at least $\min(H_{duration}(x), T_i^{team} - x.t)$ because $H_{duration}$ never overestimates the true duration to a goal and accumulation is cut off at $T_i^{team}$.
    Therefore, $P_c(t)$ will be accumulated for at least $\max(0, \min(H_{duration}(x), T_i^{team} - x.t))$ after state $x$.
    Let $T^c(x) \geq \max(0, \min(H_{duration}(x), T_i^{team} - x.t))$ be the actual duration $P_c(t)$ will be accumulated after state $x$.
    The actual cumulative number of violated active DSHT hyperplanes is given by $\int_{x.t}^{x.t+T^c(x)}P_c(\tau)d\tau$.\looseness=-1
    
    $|x^l.\mH|\geq|x^{l-1}.\mH|$ for all $l\in\{2, \ldots, n\}$ because if a hyperplane is violated while traversing $x^1,\ldots, x^{l-1}$, it is also violated while traversing $x^1, \ldots, x^l$.
    Therefore, linear interpolation $P_c(t)$ of the number of violated hyperplanes is a nondecreasing function, i.e., $P_c(x.t) \leq P_c(t)\ \forall t\in[x.t, x^n.t]$.
    In addition, $P_c(t)$ is a nonnegative function as it is a linear interpolation of set cardinalities.
    Hence, we have
    \begin{align*}
        H_{team}&(x) = P_c(x.t) \times\\
        &\ \ \ \ \ \ \ \ \max(0, \min(H_{duration}(x), T_i^{team} - x.t))\\
        &=\int_{x.t}^{x.t + \max(0, \min(H_{duration}(x), T_i^{team} - x.t))}P_c(x.t)d\tau\\
        &\leq \int_{x.t}^{x.t+T^c(x)}P_c(x.t)d\tau \leq \int_{x.t}^{x.t+T^c(x)}P_c(\tau)d\tau.
    \end{align*}
    In other words, $H_{team}(x)$ never overestimates true accumulated $P_c(t)$ in an optimal path to the goal state $x^n$ from $x$.\looseness=-1

    \textbf{Admissibility of $\boldsymbol{H_{rotation}}$:} $H_{rotation} = 0$ is trivially admissible because $0$ is the lowest cost.
    
    \end{proof}
\end{proposition}

As each individual cost term is admissible, their Cartesian product is also admissible.
Hence, cost algebraic A* with re-openings minimizes $\mJ$ with the given heuristics $H$.\looseness=-1

% Since the cartesian product $\mJ(x)$ with lexicographical ordering is a cost algebra and all heuristics of individual cost terms are admissible, cost algebraic $A^*$ minimizes over this ordering.

\textbf{Time limited best effort search.} 
Finding the optimal state sequence induced by the costs $\mJ$ to the goal position $\vg_i$ can take a lot of time.
Because of this, each robot $i$ limits the duration of the search using a maximum search duration parameter $T^{search}_i$.
When the search gets cut off because of the time limit, we return the lowest cost state sequence to a goal state so far.
During node expansion, A* applies all actions to the state with the lowest cost.
One of those actions is always REACHGOAL.
Therefore, we connect all expanded states to the goal position using REACHGOAL action.
Hence, when the search is cut off, there are many candidate plans to the goal position, which are already sorted according to their costs by A*.\looseness=-1

We remove the states generated by ROTATE actions from the search result and provide the resulting sequence to the trajectory optimization stage.
Note that ROTATE changes only the direction $x.\vDelta$.
The trajectory optimization stage does not use $x.\vDelta$, therefore we remove repeated states for the input of trajectory optimization.\looseness=-1

\subsection{Trajectory Optimization}\label{Section:TrajectoryOptimization}

Let $x^1,\ldots, x^N$ be the state sequence provided to trajectory optimization.
In the trajectory optimization stage, each robot $i$ fits a B\'ezier curve $\vf_{i,l}(t): [0, T_{i,l}] \rightarrow \mathbb{R}^d$ of degree $h_{i,l}$ (which are parameters) where $T_{i,l} = x^{l+1}.t - x^l.t$ to each segment from $(x^l.t, x^l.\vp)$ to $(x^{l+1}.t, x^{l+1}.\vp)$ for all $l \in \{1, \ldots, N-1\}$ to compute a piecewise trajectory $\vf_i(t):[0, x^N.t]\rightarrow \mathbb{R}^d$ where each piece is the fitted B\'ezier curve, i.e.
\begin{align*}
    \vf_i(t) = \begin{cases}
        \vf_{i,1}(t - x^1.t) & x^1.t = 0 \leq t < x^2.t\\
        \ldots&\ \\
        \vf_{i,N-1}(t - x^{N-1}.t) & x^{N-1}.t\leq t \leq x^{N}.t
    \end{cases}
\end{align*}
A B\'ezier curve $\vf_{i,l}(t)$ of degree $h_{i,l}$ has $h_{i,l}+1$ control points $\vP_{i, l, 0}, \ldots, \vP_{i, l, h_{i,l}}$ and is defined as
\begin{align*}
    \vf_{i,l}(t) = \sum_{k = 0}^{h_{i,l}} \vP_{i, l, k}\binom{h_{i,l}}{k}\left(\frac{t}{T_{i,l}}\right)^k\left(1-\frac{t}{T_{i,l}}\right)^{h_{i,l} - k}
\end{align*}
% The degrees $h_{i,l}$ of B\'ezier pieces are parameters.\looseness=-1

B\'ezier curves are contained in the convex hull of their control points~\cite{farouki2012bernstein}.
This allows us to easily constrain B\'ezier curves to be inside convex sets by constraining their control points to be in the same convex sets.
Let $\mP_i = \{\vP_{i, l, k}\ |\ l\in\{1, \ldots, N-1\}, k\in \{0, \ldots, h_{i,l}\} \}$ be the set of the control points of all pieces of robot $i$.\looseness=-1

During the trajectory optimization stage, we construct a QP where decision variables are control points $\mP_i$ of the trajectory.\looseness=-1

We preserve the cumulative collision probabilities $P_{s}$ and $P_{d}$ and the cumulative number of violated active DSHT hyperplanes $P_{c}$ of the state sequence $x^{1:N}$, by ensuring that robot following $\vf_{i,l}(t)$ i) avoids the same static obstacles and dynamic obstacle behavior models robot traveling from $x^1$ to $x^{l+1}$ avoids and ii) does not violate any active DSHT hyperplane that the robot traveling from $x^{1}$ to $x^{l+1}$ does also not violate for all $l\in\{1,\ldots,N-1\}$.
The constraints generated for these are always feasible.\looseness=-1
% Note that we do not enforce constraints to $\vf_{i,l}(t)$ for static obstacles or dynamic obstacle behavior models that the robot avoids or the active DSHT hyperplanes that the robot does not violate while travelling from $x^l$ to $x^{l+1}$ but does not avoid/does violate while travelling from $x^1$ to $x^l$, because doing so would not decrease $P_s(x^{l+1})$, $P_d(x^{l+1})$, or $P_c(x^{l+1})$ as the robot has already collided with/violated those obstacles/active DSHT hyperplanes.
% This has the added benefit of decreasing the number of constraints of the optimization problem.\looseness=-1

In order to encourage dynamic obstacles to determine their behavior using the interaction models in the same way they determine it in response to $x^{1:N}$, we add cost terms that match the position and the velocity at the start of each B\'ezier piece $\vf_{i,l}(t)$ to the position and the velocity of the robot at $x^{l}$ for each $l\in\{1, \ldots, N-1\}$.\looseness=-1

\subsubsection{Constraints}
There are five types of constraints we impose on the trajectory, all of which are linear in control points $\mP_i$.

% % Figure environment removed

% Figure environment removed

\textbf{Static obstacle avoidance constraints.}
Let $j \in \mO_i \setminus x^{l+1}.\mO$ be a static obstacle that robot $i$ travelling from $x^1$ to $x^{l+1}$ avoids for an $l\in\{1, \ldots, N-1\}$.
% Robot avoids it while traversing it from $x^{i}$ to $x^{i+1}$ trivially.
Let $\zeta_{i,l}^{robot}$ be the space swept by the robot traveling the straight line from $x^l.\vp$ to $x^{l+1}.\vp$.
Since the shape of the robot is convex and it is swept along a straight line segment, $\zeta_{i,l}^{robot}$ is also convex~\cite{senbaslar2023rlss}.
Static obstacle $j$ is also convex by definition.
Since robot avoids $j$, $\mQ_{i,j} \cap \zeta_{i,l}^{robot} = \emptyset$.
Hence, they are linearly separable by the separating hyperplane theorem.
We compute the support vector machine (SVM) hyperplane between $\zeta_{i,l}^{robot}$ and $\mQ_{i,j}$, snap it to $\mQ_{i,j}$ by shifting it along its normal so that it touches $\mQ_{i,j}$, and shift it back to account for robot's collision shape $\mR_i^{robot}$ similarly to~\cite{senbaslar2023rlss} (Fig.~\ref{Figure:SeparatingHyperplanes}).
Let $\mH_{\zeta_{i,l}^{robot}, \mQ_{i,j}}$ be this hyperplane.
We constrain $\vf_{i,l}$ with $\mH_{\zeta_{i,l}^{robot}, \mQ_{i,j}}$ for it to avoid static obstacle $j$, which is a feasible linear constraint as shown in~\cite{senbaslar2023rlss}.\looseness=-1

These constraints enforce that robot traversing $\vf_{i,l}(t)$ avoids the same obstacles robot traversing from $x^1$ to $x^{l+1}$ avoids, not growing the set $x^{l+1}.\mO$ between $[x^l.t, x^{l+1}.t]\ \forall l\in\{1, \ldots, N-1\}$, and hence preserving $P_{s}(t)$ $\forall t\in[0, x^N.t]$.\looseness=-1

\textbf{Dynamic obstacle avoidance constraints.}
Let $(\mB_{i,j,k}, \vp_{i,j,k}^{dyn}) \in x^{l+1}.\mD$ be a dynamic obstacle behavior model--position pair that does not collide with robot travelling from $x^{1}$ to $x^{l+1}$ for an $l\in\{1, \ldots, N-1\}$.
$\mB_{i, j, k}$ should be in $x^l.\mD$ as well, because the behavior models in $x^{l+1}.\mD$ are a subset of behavior models in $x^l.\mD$ by definition.
Let $\tilde{\vp}_{i, j, k}^{dyn}$ be the position of the dynamic obstacle $j$ moving according to behavior model $\mB_{i,j,k}$ at state $x^l$.
Let $\zeta^{dyn}_{i, j, k, l}$ be the region swept by the dynamic object $j$ from  $\tilde{\vp}_{i, j, k}^{dyn}$ to $ \vp_{i,j,k}^{dyn}$.
During collision check of state expansion from $x^l$ to $x^{l+1}$, we check whether $\zeta^{dyn}_{i, j, k, l}$ intersects with $\zeta_{i,l}^{robot}$ and add the model to $x^{l+1}.\mD$ if they do not.
Since these sweeps are convex sets (because they are sweeps of convex sets along straight line segments), they are linearly separable.
We compute the SVM hyperplane between them, snap it to the region swept by dynamic obstacle and shift it back to account for the robot shape $\mR_i^{robot}$ (Fig.~\ref{Figure:SeparatingHyperplanes}).
Let $\mH_{\zeta_{i,l}^{robot}, \zeta^{dyn}_{i, j, k, l}}$ be this hyperplane.
We constrain $\vf_{i,l}$ with $\mH_{\zeta_{i,l}^{robot}, \zeta^{dyn}_{i, j, k, l}}$, which is a feasible linear constraint as shown in~\cite{senbaslar2023rlss}.\looseness=-1

These constraints enforce that robot traversing $\vf_{i,l}(t)$ avoids same dynamic obstacle behavior models robot travelling from $x^1$ to $x^{l+1}$ avoids, not shrinking the set $x^{l+1}.\mD\ \forall l\in\{1,\ldots, N-1\}$, and hence preserving $P_{d}(t)$ $\forall t\in[0, x^N.t]$.\looseness=-1

The reason we perform \emph{conservative collision checks} for dynamic obstacle avoidance during discrete search is to use the separating hyperplane theorem.
Without the conservative collision check, there is no proof of linear separability, and SVM computation might fail.\looseness=-1

\textbf{Teammate avoidance constraints.}
Let $\mH \in \tilde{\mH}^{active}_i\setminus x^{l+1}.\mH$ be an active DSHT hyperplane that is not violated while traversing states from $x^1$ to $x^{l+1}$.
If $x^l.t < T_i^{team}$, i.e., the segment from $x^l$ to $x^{l+1}$ is within the teammate safety enforcement period, we constrain $\vf_{i, l}$ with $\mH$ by shifting it to account for robot's collision shape, and enforcing $\vf_{i, l}$ to be in the safe side of the shifted hyperplane, which is a feasible constraint~\cite{senbaslar2023rlss}.
Otherwise, we do not constrain the piece $\vf_{i, l}$ with active DSHT hyperplanes.\looseness=-1

Within the safety enforcement period $T_i^{team}$, any $\vf_{i, l}$ does not violate any active DSHT hyperplane that is not violated while traversing the state sequence $x^{1:l+1}$, preserving the cardinality of sets $x^{l+1}.\mH$, and hence $P_c(t)$.\looseness=-1

\textbf{Continuity constraints.}
We enforce continuity up to the desired degree $c_i$ between pieces by
\begin{align*}
    \frac{d^k\vf_{i,l}(T_{i,l})}{dt^k} = \frac{d^k\vf_{i,l+1}(0)}{dt^k}\ &\forall l \in \{1,\ldots, N-2\}\\
    &\forall k\in\{0,\ldots,c_i\}.
\end{align*}

We enforce continuity up to desired degree $c_i$ between planning iterations by
\begin{align*}
    \frac{d^k\vf_{i}(0)}{dt^k} = \vp_{i,k}^{self}\ \forall k\in\{0, \ldots, c_i\}.
\end{align*}

\textbf{Dynamic limit constraints.}
% We enforce derivative magnitude limits $\gamma_i^k$ for all $k\in \{1, \ldots, K_i\}$.
Derivative of a B\'ezier curve is another B\'ezier curve with a lower degree, control points of which are linearly related to the control points of the original curve~\cite{farouki2012bernstein}.
Let $\mP_i^k = \vD^k(\mP_i)$ be the control points of the $k^{th}$ derivative of $\vf_i$, where $\vD^k$ is the linear transformation relating $\mP_i$ to $\mP_i^k$.
We enforce dynamic constraints uncoupled among dimensions by limiting maximum $k^{th}$ derivative magnitude in each dimension by $\frac{\gamma_i^k}{\sqrt{d}}$ so that they are linear.
Utilizing the convex hull property of B\'ezier curves, we enforce
\begin{align*}
 -\frac{\gamma_i^k}{\sqrt{d}} \preceq\vP \preceq \frac{\gamma_i^k}{\sqrt{d}}\ \forall\vP\in\mP_i^k   
\end{align*}
which limits $k^{th}$ derivative magnitude with $\gamma_i^k$ along the trajectory $\vf_i$ where $\preceq$ between a vector and a scalar is true if and only if all elements of the vector are less than or equal to the scalar.\looseness=-1

While collision avoidance constraints are always feasible, we do not have a general proof of feasibility for continuity and dynamic limit constraints, which may cause the planner to fail.
If the planner fails, the robot continues using the last successfully planned trajectory.\looseness=-1
% \textbf{Dynamic limit constraints.}
% To enforce dynamic limits during trajectory optimization, we linearly constrain the derivatives of the trajectory in each dimension independently on sampled points by
% \begin{align*}
%     -\frac{\gamma_k}{\sqrt{d}} \preceq \frac{d^k\vf(j\Delta t)}{dt^k} &\preceq \frac{\gamma_k}{\sqrt{d}}\\
%     &\forall j \in \{0, \ldots, \left\lfloor\frac{x^N.t}{\Delta t}\right\rfloor\}\ \forall k \in \{1,\ldots, K\}
% \end{align*}
% where $\Delta t$ is the sampling interval and $\preceq$ is the component-wise less than operator, such that all elements of a vector is less than (or more than, if the scalar is on the left) the given scalar.

% Note that these constraints only enforce dynamic limits at sampled points, and the planned trajectory may be dynamically infeasible between sampled points.
% We check for infeasibilities between sampled points during validity check.

\subsubsection{Objective Function}
We use a linear combination of three cost terms as our objective function, all of which are quadratic in control points $\mP_i$.\looseness=-1

\textbf{Energy term.}
We use the sum of integrated squared derivative magnitudes as a metric for energy usage similar to~\cite{senbaslar2023rlss, honig2018quadswarms, richter2013planning, senbaslar2018rte}. The energy usage cost term $\mJ_{energy}(\mP_i)$ is
\begin{align*}
\mJ_{energy}(\mP_i) = \sum_{\lambda_{i,k} \in \vlambda_i} \lambda_{i,k} \int_0^{x^N.t}\normtwo{\frac{d^k\vf_i(t)}{dt^k}}^2dt
\end{align*}
where $\lambda_{i,k}$s are parameters.\looseness=-1

\textbf{Position matching term.}
We add a position matching term $J_{position}(\mP_i)$ that penalizes distance between piece endpoints and state sequence positions $x^{2}.\vp, \ldots, x^{N}.\vp$.\looseness=-1

\begin{align*}
\mJ_{position}(\mP_i) = \sum_{l\in\{1,\ldots,N-1\}} \theta_{i,l}\normtwo{\vf_{i,l}(T_{i,l}) - x^{l+1}.\vp}^2
\end{align*}
where $\theta_{i,l}$s are weight parameters.\looseness=-1

\textbf{Velocity matching term.}
We add a velocity matching term $J_{velocity}$ that penalizes divergence from the velocities of the state sequence $x^{1:N}$ at piece start points.\looseness=-1

\begin{align*}
    \mJ_{velocity}(\mP_i) = \sum_{l\in\{1,\ldots,N-1\}} \beta_{i,l}\normtwo{\frac{d\vf_{i,l}(0)}{dt} - \frac{x^{l+1}.\vp - x^l.\vp}{x^{l+1}.t - x^l.t}}^2
\end{align*}
where $\beta_{i,l}$s are weight parameters.\looseness=-1

Position and velocity matching terms encourage matching the positions and velocities of the state sequence $x^{1:N}$. This causes dynamic obstacles to make similar interaction decisions against the robot following trajectory $\vf_i(t)$ to they do to the robot following the state sequence $x^{1:N}$.
One could also add constraints to the optimization problem to exactly match positions and velocities.
Adding position and velocity matching terms as constraints resulted in a high rate of optimization infeasibilities in our experiments.
Therefore, we choose to add them to the cost function of the optimization term in the final algorithm.\looseness=-1

\section{Evaluation}

We implement our algorithm in C++.
We use CPLEX 12.10 to solve the quadratic programs generated during the trajectory optimization stage, including the SVM problems.
We evaluate our planner's behavior in simulations in 3D.
% We explicitly simulate decentralized planning, noisy sensing, online prediction, and imperfect communication.
% We compute hard-margin SVMs between objects using SVM optimization code generated by CVXGEN~\cite{mattingley2012cvxgen}.
All simulation experiments are conducted on a computer with Intel(R) i7-8700K CPU @3.70GHz, running Ubuntu 20.04 as the operating system.
The planning pipeline is executed in a single core of the CPU in each planning iteration of each robot.
We compare our algorithm's performance with three state-of-the-art decentralized navigation decision-making algorithms, namely SBC~\cite{wang2017safety}, RLSS~\cite{senbaslar2023rlss}, and RMADER~\cite{kondo2023rmader}, in both single-robot and multi-robot scenarios in simulations and show that our algorithm achieves considerably higher success rate compared to the baselines.
We implement our algorithm for physical quadrotors and show its feasibility in the real world in single and multi-robot experiments.
The supplemental video includes recordings from both i) simulations, including some not covered in this paper, and ii) physical robot experiments.\looseness=-1

% We publish a supplemental video\footnote{\url{https://youtu.be/ct8okY5pmgI}} with our paper which contains recordings from i) simulation experiments, some of which are and others are not discussed in this paper, and ii) physical robot experiments.\looseness=-1

\subsection{Simulation Evaluation Setup}

\subsubsection{Obstacle Sensing}\label{Section:Mocking}
We use octrees~\cite{hornung2013octomap} to represent the static obstacles.
Each axis-aligned box with its stored existence probability is used as a static obstacle.
We model static obstacle sensing imperfections using three operations applied to the octree representation of the environment in static obstacle sensing uncertainty experiments (Sec.~\ref{Section:StatiObstacleSensingUncertainty}):
\begin{itemize}
\item \textbf{increaseUncertainty:} Increases the uncertainty of existing obstacles by moving their existence probabilities closer to $0.5$, by sampling a probability between the existence probability $p$ of an obstacle and $0.5$ uniformly.
% Let $p \in [0, 1]$ be the existence probability of an obstacle. If $p \leq 0.5$, we sample a probability in $[p, 0.5]$ uniformly and change $p$ to the random sample. Similarly, if $p > 0.5$, we sample a probability in $[0.5, p]$ and change $p$ to the sample.
\item \textbf{leakObstacles($\boldsymbol{p_{leak}}$):} Leaks each obstacle to a neighbouring region with probability $p_{leak}$.
% Let $p$ be the existence probability of an obstacle. We leak it to a neighbouring region randomly with probability $p_{leak}p$, and increase the existence probability of the neighbouring region by $p_{leak}p$ if the original obstacle is leaked.
\item \textbf{deleteObstacles:} Deletes obstacles randomly according to their non-existence probabilities.
% Let $p$ be the existence probability of an obstacle.
% We remove it with probability $1-p$.
\end{itemize}\looseness=-1

We model dynamic obstacle shapes $\mR_{i,j}^{dyn}$ as axis-aligned boxes.
For each robot, $i$, to simulate imperfect sensing of $\mR_{i,j}^{dyn}$, we inflate or deflate it in each axis randomly according to a one dimensional $0$ mean Gaussian noise with standard deviation $\sigma_i$ in experiments with dynamic obstacle sensing uncertainty (Sec.~\ref{Section:DynamicObstacleSensingUncertainty})\footnote{Note that, we do not explicitly account for the dynamic obstacle shape sensing uncertainty during planning, yet we still show our algorithm's performance under such uncertainty.}.
Each robot $i$ is assumed to be noisily sensing the position and velocity of each dynamic obstacle $j\in \mD_i$ according to a $2d$ dimensional $0$ mean Gaussian noise with positive definite covariance $\Sigma_i \in \mathbb{R}^{2d \times 2d}$.
The first $d$ terms of the noise are applied to the real position and the second $d$ terms of the noise are applied to the real velocity of the obstacle to compute sensed position and velocity at each simulation step.\looseness=-1
% Let $\vp_{i,j, hist}^{dyn}$ be the sensed position and $\vv_{i,j,hist}^{dyn}$ be the sensed velocity history of the dynamic obstacle $j$ by robot $i$.
% The uncertainty in sensing positions and velocities of dynamic obstacles creates two different uncertainties reflected to the planner: i) prediction inaccuracy increases (Sec.~\ref{Section:Prediction}), ii) current positions $\vp^{dyn}_{i,j}$ of dynamic obstacles provided to the planner becomes wrong.
% The planner does not explicitly model inaccuracy of individual predictors but it explicitly models uncertainty across models by assigning probabilities to them.
% Similarly, the planner does not explicitly model current position uncertainty of dynamic obstacles.
% Yet, we still show our algorithm's behavior under such uncertainties.\looseness=-1

\subsubsection{Predicting Behavior Models of Dynamic Obstacles}\label{Section:Prediction}
We introduce three simple model-based online dynamic obstacle behavior model prediction methods to use during evaluation\footnote{More sophisticated behavior prediction methods can be developed and integrated with our planner, which might potentially use domain knowledge about the environment objects exists or handle position and velocity sensing uncertainties explicitly.}.\looseness=-1

% Figure environment removed

Let $\vp^{dyn}$ be the position of a dynamic obstacle. We define three movement models:
\begin{itemize}
\item \textbf{Goal attractive movement model $\boldsymbol{\mM_g(\vp^{dyn} | \hat{\vg}, \hat{s})}$  (Fig.~\ref{Figure:GoalAttractive}):} Attracts the dynamic obstacle to the goal position $\hat{\vg}$ with desired speed $\hat{s}$.
The desired velocity $\tilde{\vv}^{dyn}$ of the dynamic obstacle is computed as $\tilde{\vv}^{dyn} = \mM_g(\vp^{dyn} | \hat{\vg}, \hat{s}) = \frac{\hat{\vg} - \vp^{dyn}}{\normtwo{\hat{\vg} - \vp^{dyn}}}\hat{s}$.
\item \textbf{Constant velocity movement model $\boldsymbol{\mM_c(\vp^{dyn} | \hat{\vv})}$ (Fig.~\ref{Figure:ConstantVelocity}):} Moves the dynamic obstacle with constant velocity $\hat{\vv}$.
The desired velocity $\tilde{\vv}^{dyn}$ of the dynamic obstacle is computed as $\tilde{\vv}^{dyn} = \mM_c(\vp^{dyn} | \hat{\vv}) = \hat{\vv}$.
\item \textbf{Rotating movement model $\boldsymbol{\mM_r(\vp^{dyn} | \hat{\vc}, \hat{s})}$ (Fig.~\ref{Figure:Rotating}):} Rotates the robot around the rotation center $\hat{\vc}$ with desired speed $\hat{s}$.
The desired velocity $\tilde{\vv}^{dyn}$ of the dynamic obstacle is computed as $\tilde{\vv}^{dyn} = \mM_r(\vp^{dyn} | \hat{\vc}, \hat{s}) = \frac{\vr}{\normtwo{\vr}}\hat{s}$ where $\vr\perp(\vp^{dyn}-\hat{\vc})$\footnote{During prediction, we assume that we have access to the algorithm computing $\vr$ from $\vp^{dyn}$ and $\hat{\vc}$ as there are infinitely many vectors perpendicular to $\vp^{dyn}-\vc$ when $d \geq 3$.}.
\end{itemize}
Let $\vp^{robot}$ be the current position and $\vv^{robot}$ be the current velocity of a robot.
We define one interaction model:
\begin{itemize}
\item \textbf{Repulsive interaction model $\boldsymbol{\mI_r(\vp^{dyn}, \tilde{\vv}^{dyn}, \vp^{robot}, \vv^{robot} | \hat{f})}$ (Fig.~\ref{Figure:Repulsive}):} Causes dynamic obstacle to be repulsed from the robot with repulsion strength $\hat{f}$.
The velocity of the dynamic obstacle is computed as $\vv^{dyn} = \mI_r(\vp^{dyn}, \tilde{\vv}^{dyn}, \vp^{robot}, \vv^{robot} | \hat{f}) = \tilde{\vv}^{dyn} + \frac{\left(\vp^{dyn} - \vp^{robot}\right)\hat{f}}{\normtwo{\vp^{dyn} - \vp^{robot}}^3}$.
The dynamic obstacle gets repulsed away from the robot linearly proportional to repulsion strength $\hat{f}$, and quadratically inversely proportional to the distance from the robot\footnote{Note that the interaction model we use does not utilize the velocity $\vv^{robot}$ of the robot, while our planner allows it. We choose to use this interaction model for easier online prediction of model parameters as our paper is not focused on prediction algorithms.}.
\end{itemize}

We propose three online prediction methods to predict the behavior models of dynamic obstacles from the sensed position and velocity histories of dynamic obstacles and the robot, one for each combination of movement and interaction models.
% We describe them for a dynamic obstacle robot pair.
Each robot runs the prediction algorithms for each dynamic obstacle.
Let $\vp^{robot}_{hist}$ be the position and $\vv_{hist}^{robot}$ be the velocity history of the robot collected synchronously with $\vp^{dyn}_{hist}$ and $\vv^{dyn}_{hist}$ for the dynamic obstacle.\looseness=-1

\paragraph{Goal attractive repulsive predictor}
Assuming the dynamic obstacle moves according to goal attractive movement model $\mM_g(\vp^{dyn} | \hat{\vg}, \hat{s})$ and repulsive interaction model $\mI_r(\vp^{dyn}, \tilde{\vv}^{dyn}, \vp^{robot}, \vv^{robot} |\hat{f})$, we estimate parameters $\hat{\vg}$, $\hat{s}$ and $\hat{f}$.\looseness=-1

We solve two consecutive quadratic programs (QP): i) one for goal $\hat{\vg}$ estimation, ii) one for desired speed $\hat{s}$ and repulsion strength $\hat{f}$ estimation\footnote{While joint estimation of $\hat{\vg}$, $\hat{s}$ and $\hat{f}$ would be more accurate, we choose to estimate the parameters in two steps so that the individual problems are QPs, and can be solved quickly.\looseness=-1}.\looseness=-1

\textbf{Goal estimation.} 
Let $\vp^{dyn}_{hist,k}$ and $\vv^{dyn}_{hist,k}$ be the $k$th elements of $\vp^{dyn}_{hist}$ and $\vv^{dyn}_{hist}$ respectively.
$\vp^{dyn}_{hist,k} + t_k\vv^{dyn}_{hist,k}, t_k \geq 0$ is the ray the dynamic obstacle would have followed if it did not change its velocity after the $k$th sample.
We estimate the goal position $\hat{\vg}$ of the dynamic obstacle by computing the point whose average squared distance to these rays 
% $\vp^{dyn}_{hist,k} + t_k\vv^{dyn}_{hist,k}, t_k \geq 0$ 
is minimal:
% We compute $\hat{\vg}$ as the point whose average distance to all these rays is minimum.
% The quadratic optimization problem we solve is as follows:
\begin{align*}
    \min_{\hat{\vg}, t_1, \ldots, t_K} &\frac{1}{K} \sum_{k=1}^{K} \normtwo{\vp^{dyn}_{hist,k} + t_k\vv^{dyn}_{hist,k} - \hat{\vg}}^2, \text{s.t.}\\
    &t_k \geq 0\ \forall k\in \{1, \ldots, K\}
\end{align*}
where $K$ is the number of recorded position/velocity pairs.\looseness=-1

% We use the estimated goal position $\hat{\vg}$, the position history $\vp_{hist}^{dyn}$ and the velocity history $\vv_{hist}^{dyn}$ of dynamic obstacle, and the position history $\vp^{robot}_{hist}$ and the velocity history $\vv^{robot}_{hist}$ of the robot to estimate the desired speed $\hat{s}$ and repulsion strength $\hat{f}$ of the dynamic obstacle.\looseness=-1
% Let $\vp^{robot}_{hist,k} \in \vp^{robot}_{hist}$ be the position, and $\vv^{robot}_{hist,k} \in \vv^{robot}_{hist}$ be the velocity of the robot at $k$th sample.\looseness=-1
\textbf{Desired speed and repulsion strength estimation.} Assuming the dynamic obstacle moves according to the goal attractive repulsive behavior model, its estimated velocity at step $k$ is:

\begin{align*}
    \hat{\vv}^{dyn}_k &= \mI_r(\vp^{dyn}_{hist,k}, \mM_g(\vp^{dyn}_{hist,k} | \hat{\vg}, \hat{s}), \vp^{robot}_{hist,k}, \vv^{robot}_{hist,k} | \hat{f})\\
    &= \frac{\hat{\vg} - \vp^{dyn}_{hist,k}}{\normtwo{\hat{\vg}-\vp^{dyn}_{hist,k}}}\hat{s} + \frac{\left(\vp^{dyn}_{hist,k} - \vp^{robot}_{hist,k}\right)\hat{f}}{\normtwo{\vp^{dyn}_{hist,k} - \vp^{robot}_{hist,k}}^3}
\end{align*}
We minimize the average squared distance between estimated and sensed dynamic obstacle velocities to estimate $\hat{s}$ and $\hat{f}$:
\begin{align*}
    \min_{\hat{s},\hat{f}} \frac{1}{K}\sum_{k=1}^K \normtwo{\hat{\vv}^{dyn}_k - \vv^{dyn}_{hist,k}}^2.
\end{align*}\looseness=-1

\paragraph{Constant velocity repulsive predictor}
Assuming the dynamic obstacle moves according to the constant velocity movement model $\mM_c(\vp^{dyn} | \hat{\vv})$ and repulsive interaction model $\mI_r(\vp^{dyn}, \tilde{\vv}^{dyn}, \vp^{robot}, \vv^{robot} |\hat{f})$, we estimate the parameters $\hat{\vv}$ and $\hat{f}$.\looseness=-1

We solve a single QP to estimate both parameters.
Assuming the dynamic obstacle moves according to the constant velocity repulsive behavior model, its estimated velocity at step $k$ is 
\begin{align*}
    \hat{\vv}^{dyn}_k &= \mI_r(\vp^{dyn}_{hist,k}, \mM_c(\vp^{dyn}_{hist,k} | \hat{\vv}), \vp^{robot}_{hist,k}, \vv^{robot}_{hist,k})\\
    &= \hat{\vv} + \frac{\left(\vp^{dyn}_{hist,k} - \vp^{robot}_{hist,k}\right)\hat{f}}{\normtwo{\vp^{dyn}_{hist,k} - \vp^{robot}_{hist,k}}^3}
\end{align*}
We minimize the average squared distance between estimated and sensed velocities to estimate $\hat{\vv}$ and $\hat{f}$:
\begin{align*}
    \min_{\hat{\vv},\hat{f}} \frac{1}{K}\sum_{k=1}^K \normtwo{\hat{\vv}^{dyn}_k - \vv^{dyn}_{hist,k}}^2.
\end{align*}\looseness=-1

\paragraph{Rotating repulsive predictor}
Assuming the dynamic obstacle moves according to the rotating movement model $\mM_r(\vp^{dyn} | \hat{\vc}, \hat{s})$ and repulsive interaction model $\mI_r(\vp^{dyn}, \tilde{\vv}^{dyn}, \vp^{robot}, \vv^{robot} |\hat{f})$, we estimate the parameters $\hat{\vc}$, $\hat{s}$ and $\hat{f}$.\looseness=-1

We solve two consecutive optimization problems: i) one for rotation center $\hat{\vc}$ estimation, ii) one for desired speed $\hat{s}$ and repulsion strength $\hat{f}$ estimation\footnote{
We separate estimation to two steps because of a similar reason with the goal attractive repulsive predictor.
The first problem is a linear program (LP) and the second one is a QP, both of which allowing fast online solutions.\looseness=-1}.\looseness=-1

\textbf{Rotation center estimation.}
We estimate $\hat{\vc}$ by minimizing averaged dot product between sensed velocities and the vectors pointing from the rotation center to the sensed positions.
The reasoning is that if the obstacle is rotating around a point $\hat{\vc}$, its velocity should always be perpendicular to the vector connecting $\hat{\vc}$ to its position.
The LP we solve is:
\begin{align*}
    \min_{\hat{\vc}}\frac{1}{K}\sum_{i=1}^K |\vv^{dyn}_{hist,k} \cdot (\vp^{dyn}_{hist,k} - \hat{\vc})|.
\end{align*}\looseness=-1

% We use the estimated rotation center $\hat{\vc}$, as well as position/velocity histories of the dynamic obstacle and the robot to estimate the desired speed $\hat{s}$ and repulsion strength $\hat{f}$ of the dynamic obstacle.\looseness=-1
\textbf{Desired speed and repulsion strength estimation.} Assuming the dynamic obstacle moves according to the rotating repulsive behavior model, its estimated velocity at step $k$ is 
\begin{align*}
    \hat{\vv}^{dyn}_k &= \mI_r(\vp^{dyn}_{hist,k}, \mM_r(\vp^{dyn}_{hist,k} | \hat{\vc}, \hat{s}), \vp^{robot}_{hist,k}, \vv^{robot}_{hist,k})\\
    &= \frac{\vr^{dyn}_k}{\normtwo{\vr^{dyn}_k}}\hat{s} + \frac{\left(\vp^{dyn}_{hist,k} - \vp^{robot}_{hist,k}\right)\hat{f}}{\normtwo{\vp^{dyn}_{hist,k} - \vp^{robot}_{hist,k}}^3}
\end{align*}
where $\vr^{dyn}_k$ is a perpendicular vector to $\vp^{dyn}_{hist,k} - \hat{\vc}$, computed with the known algorithm.
We minimize the average squared distance between estimated and sensed velocities to estimate $\hat{s}$ and $\hat{f}$:
\begin{align*}
    \min_{\hat{s},\hat{f}} \frac{1}{K}\sum_{k=1}^K \normtwo{\hat{\vv}^{dyn}_k - \vv^{dyn}_{hist,k}}^2.
\end{align*}\looseness=-1

\paragraph{Assigning probabilities to behavior models}
Each robot runs the three predictors for each dynamic obstacle.
For each predicted behavior model $(\mM_j, \mI_j)$, $j \in \{1,2,3\}$, we compute the average estimation error $E_j$ as the average $L^2$ norm between the predicted and the actual velocities:
\begin{align*}
    \frac{1}{K}\sum_{k=1}^K \lVert &\vv^{dyn}_{hist,k}  -\mI_j(\vp^{dyn}_{hist,k}, \mM_j(\vp^{dyn}_{hist,k}), \vp^{robot}_{hist,k}, \vv^{robot}_{hist,k})\rVert_2
\end{align*}
We compute the softmax of errors $E_j$ with base $b$ where $0 < b < 1$, and use them as probabilities, i.e., $p^{dyn}_{j} = \frac{b^{E_j}}{\sum_{k=1}^3 b^{E_k}}$.\looseness=-1

\subsubsection{Metrics}
We run each single robot simulation experiment $1000$ times and each multi-robot simulation experiment $100$ times in randomized environments in performance evaluation of our algorithm (Sec.~\ref{Section:PerfUnderDiffConfandEnv}).
In baseline comparisons, we run each single-robot simulation experiment $250$ times and each multi-robot simulation experiment $100$ times (Sec.~\ref{Section:BaselineComparison}).
In each experiment, the robots are tasked with navigating from their start to goal positions through an environment with static and dynamic obstacles.
There are nine metrics that we report for each experiment, averaged over all robots in all runs.\looseness=-1
\begin{itemize}
    \item \textbf{Success rate:} Ratio of robots that navigate from their start positions to their goal positions successfully without any collisions.
    \item \textbf{Collision rate:} Ratio of robots that collide with a static or a dynamic obstacle or a teammate at least once.
    \item \textbf{Deadlock rate:} Ratio of robots that deadlock, i.e., do not reach its goal position.
    \item \textbf{Static obstacle collision rate:} Ratio of robots that collide with a static obstacle at least once.
    \item \textbf{Dynamic obstacle collision rate:} Ratio of robots that collide with a dynamic obstacle at least once.
    \item \textbf{Teammate collision rate:} Ratio of robots that collide with a teammate at least once.
    \item \textbf{Average navigation duration:} Average time it takes for a robot to navigate from its start position to its goal position across no-deadlock no-collision robots.
    \item \textbf{Planning fail rate:} Ratio of failing planning iterations over all planning iterations of all robots in all runs.
    \item \textbf{Average planning duration:} Average planning duration over all planning iterations of all robots in all runs.
\end{itemize}\looseness=-1

\subsubsection{Fixed Parameters and Run Randomization}

Here, we describe fixed parameters across all experiments and the parameters that are randomized in all experiments.
Fixed parameters are shared by each robot $i$, and randomized parameters are randomized the same way for all robots $i$.\looseness=-1

% The safety distance $D$ of goal selection is set to $\SI{0}{m}$, i.e., we select goal positions arbitrarily close to static obstacles.
% The minimum static obstacle existence probability $p_i^{min}$ of goal selection is set to $0.1$.
% The desired time horizon $\tau_i$ of goal selection is set to $\SI{2.5}{s}$.

% The maximum speed $\tilde{\gamma}_i^{1}$ of search is set to $\SI{5.0}{\frac{m}{s}}$.
% The FORWARD actions of search are FORWARD($\SI{2.0}{\frac{m}{s}}$, $\SI{0.5}{s}$), FORWARD($\SI{3.5}{\frac{m}{s}}$, $\SI{0.5}{s}$), and FORWARD($\SI{4.5}{\frac{m}{s}}$, $\SI{0.5}{s}$).
% The time limit $T^{search}$ of search is set to $\SI{75}{ms}$.
% Minimum search planning horizon $\tilde{\tau}_i$ is set to $\SI{2.0}{s}$.
% Maximum speed duration multiplier $\alpha_i$ of search is set to $1.5$.
% The time limit $T^{search}_i$ is set to $\SI{75}{ms}$.\looseness=-1
% The degrees $h_{i,l}$ of B\'ezier curves of trajectory optimization are set to $13$ for all pieces.
% The sampling interval $\Delta t$ for dynamic constraints is set to $\SI{0.099}{s}$.
% The position and velocity matching cost weights $\theta_{i,l}$ and $\beta_{i,l}$ of trajectory optimization are set to $10, 20, 30$ for the first three pieces, and $40$ for the remaining pieces.
% The velocity matching cost weights $\beta_{i,l}$ of trajectory optimization are set to $10, 20, 30$ for the first three pieces, and $40$ for the remaining pieces.
% The energy term weights of trajectory optimization are set to $\lambda_{i,1} = 2.8$, $\lambda_{i,2} = 4.2$, $\lambda_{i,4} = 0.2$, and $0$ for all other degrees.
% We set $c_i=2$, i.e., enforce continuity up to acceleration, maximum velocity $\gamma_i^1=\SI{10}{\frac{m}{s}}$ and maximum acceleration $\gamma_i^2 = \SI{15}{\frac{m}{s^2}}$ for all robots $i$, unless explicitly stated otherwise.\looseness=-1
% We set maximum velocity $\gamma_i^1=\SI{10}{\frac{m}{s}}$ and maximum acceleration $\gamma_i^2 = \SI{15}{\frac{m}{s^2}}$ unless explicitly stated otherwise.\looseness=-1
\textbf{Fixed parameters.} We set $p_i^{min}=0.1$, $\tau_i=\SI{2.5}{s}$, $\tilde{\gamma}_i^1=\SI{5.0}{\frac{m}{s}}$, $\tilde{\tau}_i=\SI{2.0}{s}$, $\alpha_i=1.5$,  $T^{search}_i=\SI{75}{ms}$, $h_{i,l}=13$ for all $l$,  $\theta_{i,l}$ and $\beta_{i,l}$ $10, 20, 30$ for the first three pieces, and $40$ for the remaining pieces, $\lambda_{i,1} = 2.8$, $\lambda_{i,2} = 4.2$, $\lambda_{i,4} = 0.2$, and $\lambda_{i,l} = 0$ for all other degrees, $c_i=2$,  maximum velocity $\gamma_i^1=\SI{10}{\frac{m}{s}}$ and maximum acceleration $\gamma_i^2 = \SI{15}{\frac{m}{s^2}}$ for all robots $i$.
The FORWARD actions of search are ($\SI{2.0}{\frac{m}{s}}$, $\SI{0.5}{s}$), ($\SI{3.5}{\frac{m}{s}}$, $\SI{0.5}{s}$), and ($\SI{4.5}{\frac{m}{s}}$, $\SI{0.5}{s}$).\looseness=-1

The desired trajectory of each robot is set to the shortest path connecting its start position to the goal position, avoiding only the static obstacles.
The duration of the desired trajectory assumes the robot follows it at $\frac{1}{3}$ of its maximum speed $\tilde{\gamma}_i^1$ for search.\looseness=-1

In all runs of all experiments, robots navigate in random forest environments, i.e., static obstacles are tree-like objects.
The forest has $\SI{15}{m}$ radius, and trees are $\SI{6}{m}$ high and have a radius of $\SI{0.5}{m}$.
The forest is centered around the origin.
The octree structure has a resolution of $\SI{0.5}{m}$.
The density $\rho$ of the forest, i.e., the ratio of occupied cells in the octree within the forest, is set differently in each experiment.\looseness=-1

\textbf{Run randomization.}
We randomize the following parameters in all runs of each experiment in the same way.\looseness=-1

% Static obstacles
% [OK] Forest radius: Fixed
% [OK] Forest center: Fixed
% [OK] Forest tree radius: Fixed
% [OK] Forest tree height: Fixed
% [OK] Forest resolution: Fixed
% [OK] Forest density: Set by each experiment differently


% Dynamic obstacles
% [OK] Collision shape: always randomized
% [OK] Initial position: always randomized
% [OK] Movement model Type: always randomized
% [OK]-- Goal Attractive
% [OK] ---- Goal -- always randomized
% [OK]---- Desired speed -- Randomized differently in each experiment
% [OK]-- Rotating
% [OK] ---- Center -- always randomized
% [OK]---- Desired speed -- Randomized differently in each experiment
% [OK]-- Constant Velocity
% [OK]---- Velocity direction -- always randomized
% [OK]---- Velocity magnitude -- Randomized differently in each experiment
% [OK] Interaction Model Type: always REPULSIVE
% [OK]-- Repulsive
% [OK]---- Repulsion strength: Randomized differently in each experiment
% [OK]Decision making period: always randomized

\textit{Dynamic obstacle randomization.} 
We randomize the axis-aligned box collision shape of each dynamic obstacle by randomizing its size in each dimension uniformly in $[\SI{1}{m}, \SI{4}{m}]$. 
% The minimum size in any dimension is $\SI{1}{m}$, and maximum size in any dimension is $\SI{4}{m}$. 
The dynamic obstacle's initial position is uniformly sampled in the box $A$ with minimum corner $\begin{bmatrix}\SI{-12}{m} & \SI{-12}{m} & \SI{-2}{m}\end{bmatrix}^\top$ and maximum corner $\begin{bmatrix}\SI{12}{m} & \SI{12}{m} & \SI{6}{m}\end{bmatrix}^\top$.
We sample the movement model of the obstacle among goal attractive, constant velocity, and rotating models.
If the goal attractive movement model is sampled, we sample its goal position $\hat{\vg}$ uniformly in the box $A$.
If rotating model is sampled, we sample the rotation center $\hat{\vc}$ in the box with minimum corner $\begin{bmatrix}\SI{-0.5}{m} & \SI{-0.5}{m} & \SI{0.0}{m}\end{bmatrix}^\top$ and maximum corner $\begin{bmatrix}\SI{0.5}{m} & \SI{0.5}{m} & \SI{6.0}{m}\end{bmatrix}^\top$.
% We sample the speed $s$ of the dynamic obstacle uniformly in interval $[\SI{0.5}{\frac{m}{s}}, \SI{1.5}{\frac{m}{s}}]$.
The desired speed $\hat{s}$ of the obstacles is sampled uniformly in $[\SI{0.5}{\frac{m}{s}}, \SI{1.0}{\frac{m}{s}}]$.
If the constant velocity model is sampled, the velocity $\hat{\vv}$ is set by uniformly sampling a unit direction vector and multiplying it with $\hat{s}$.
The interaction model is always the repulsive model. 
The repulsion strength $\hat{f}$ is set/randomized differently in each experiment.\footnote{Note that, in reality, there is no necessity that the dynamic obstacles move according to the behavior models that we define.
The reason we choose to move dynamic obstacles according to these behavior models is so that at least one of our predictors assumes the correct model for the obstacle.
The prediction is still probabilistic in the sense that we generate three hypotheses for each dynamic obstacle and assign probabilities.\looseness=-1
% One would need to develop new predictors for dynamic obstacles following other behavior models.
}
Each dynamic obstacle changes its velocity every decision-making period, which is sampled uniformly from $[\SI{0.1}{s}, \SI{0.5}{s}]$.
Each dynamic obstacle runs its movement model to get its desired velocity and runs its interaction model for each robot, and executes the average velocity, i.e., dynamic obstacles interact with all robots, while an individual robot models only the interactions with itself.
The number of dynamic obstacles $\#^D$ is set differently in each experiment.\looseness=-1

% Robots
% [OK] Start position: always randomized
% [OK] Goal position: always randomized
% [OK] Collision shape: always randomized
% Goal selector:
% [OK]-- Desired time horizon: Randomized differently in each experiment
% [OK] -- Safety distance: Fixed
% [OK] -- Minimum static obstacle exietence probability: Fixed
% Validity checker:
% [OK]-- Maximum derivative magnitudes: Set differently in each experiment
% Trajectory optimizer:
% [OK]-- Continuity degree: Set differently in each experiment
% [OK] -- Bezier control points per piece: Fixed
% [OK] -- Position matching cost weights: Fixed
% [OK] -- Velocity matching cost weights: Fixed
% [OK] -- Integrated squared derivative magnitude weights: Fixed
% Search 
% [OK] -- Maximum speed: Fixed
% [OK] -- Speed duration set: Fixed
% [OK] -- Time limit: Fixed
% [OK] -- Smallest time horizon: Fixed
% [OK] -- Slack: Fixed
% [OK] Replanning period: Randomized differently in each experiment
% [OK] Dynamic obstacle position/velocity sensing noise: Set differently in each experiment
% [OK] Dynamic obstacle shape sensing noise: Set differently in each experiment

\textit{Robot randomization.}
We randomize the axis-aligned box collision shape of each robot by randomizing its size in each dimension uniformly in $[\SI{0.2}{m}, \SI{0.3}{m}]$.
We sample the replanning period of each robot uniformly in $[\SI{0.2}{s}, \SI{0.4}{s}]$.
Robot start positions are selected randomly around the forest on a circle with radius $\SI{21.5}{m}$ at height $\SI{2.5}{m}$.
They are placed on the circle with equal arc length distance between them.
The goal positions are set to the antipodal points of the start positions on the circle.
The number of robots $\#^R$ is set/randomized differently in each experiment.\looseness=-1
% Robots never collide with a static or a dynamic obstacle or each other initially because i) static obstacle forest has radius $\SI{15}{m}$, ii) dynamic obstacles are spawned in a box with corners $\begin{bmatrix}\SI{-12}{m} & \SI{-12}{m} & \SI{-2}{m}\end{bmatrix}^\top$ and $\begin{bmatrix}\SI{12}{m} & \SI{12}{m} & \SI{6}{m}\end{bmatrix}^\top$, and iii) .
% , which means that they can never collide with the robot even when both robot and dynamic obstacle have the maximum size.

% The dynamic obstacle position/velocity sensing noise covariance $\Sigma_i$, dynamic obstacle shape sensing noise standard deviation $\sigma_i$, and the number of robots $\#^R$ are set/randomized differently in each experiment.\looseness=-1

Sample environments with varying static obstacle densities and the number of dynamic obstacles are shown in Fig.~\ref{Figure:SampleEnvironments}.

% Figure environment removed

% \section{Evaluation (TO REMOVE)}\label{Section:Evaluation}

% We evaluate our algorithm's performance under different configurations and environments in simulations.
% We compare our algorithm's performance with MADER~\cite{tordesillas2020mader}.
% In addition, we implement our algorithm for physical quadrotors and show its feasibility in real world.

% We publish an accompanying video which can be found in \url{https://youtu.be/WRJrzIycwTg}.
% The video contains visualization videos from our experiments, as well as visualizations from more challenging cases outside of the ones described in this paper.
% Also, it contains recordings from our physical experiments.

\subsubsection{Simulating Communication Imperfections}\label{Section:SimulatingCommunicationImperfections}

Robots communicate with each other in order to update tail time points, whenever they successfully plan.
We simulate imperfections in the communication medium.
We model message delays as an exponential distribution with mean $\delta$ seconds and message drops with a Bernoulli distribution with drop probability $\kappa$ similar to~\cite{senbaslar2022async}.
% \begin{align*}
%     P_{delay}(x; \delta) &= \begin{cases} 
%       \delta e^{-\delta x}& x\geq 0 \\
%       0 & x < 0 
%    \end{cases}\\
%    P_{drop}(x; \kappa) &= \begin{cases} 
%       \kappa& x \text{ is drop} \\
%       1-\kappa & x \text{ is no-drop}.
%    \end{cases}
% \end{align*}
Each message a robot broadcasts is dropped with probability $\kappa$, and applied a delay sampled from the delay distribution if it is not dropped.
Message re-orderings are naturally introduced by random delays.
\looseness=-1

\subsection{Performance under Different Configurations and Environments in Simulations}\label{Section:PerfUnderDiffConfandEnv}

We evaluate the performance of DREAM when it is used in different environments and configurations.\looseness=-1
% In addition, we show its performance under different configurations.\looseness=-1

% \subsubsection{Desired Trajectory Quality}

% \begin{table}[t]
%     \centering
%     \caption{Effects of desired trajectory quality}
%     \vspace{-0.05in}
%     \label{Table:DesiredTrajectoryQuality}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline \textbf{Prior}&  w/o & with & w/o & with & w/o & with\\
%          $\boldsymbol{\rho}$ & 0.2 & 0.2 & 0.2 & 0.2 & 0.3& 0.3\\
%          $\boldsymbol{\#^D}$& 0 & 0 & 25 & 25& 50& 50\\
%         \hline 
%          \textbf{succ. rate} & {\color{red}0.945} & {\color{cyan}0.994} & {\color{red}0.885} & {\color{cyan}0.952} & {\color{red}0.596} & {\color{cyan}0.866}\\ 
%          \textbf{coll. rate} & {\color{red}0.016} & {\color{cyan}0.000} & {\color{red}0.094} & {\color{cyan}0.045} & {\color{red}0.319} & {\color{cyan}0.126}\\
%          \textbf{deadl. rate}& {\color{red}0.040} & {\color{cyan}0.006} & {\color{red}0.028} & {\color{cyan}0.004}& {\color{red}0.139} & {\color{cyan}0.011}\\
%          \textbf{s. coll. rate}& {\color{red}0.016} & {\color{cyan}0.000} & {\color{red}0.043} & {\color{cyan}0.001} & {\color{red}0.207} & {\color{cyan}0.015}\\
%          \textbf{d. coll. rate} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{red}0.062} & {\color{cyan}0.044} & {\color{red}0.217} & {\color{cyan}0.118} \\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & {\color{orange}27.27} & {\color{magenta}28.42} & {\color{orange}27.76} & {\color{magenta}28.51} & 31.19 & 30.86\\
%          \textbf{pl. fail rate}& {\color{red}0.060} & {\color{cyan}0.043}  & {\color{red}0.066} & {\color{cyan}0.052} & {\color{red}0.093} & {\color{cyan}0.076}\\
%          \textbf{avg. pl. dur. [ms]} & {\color{red}88.02} & {\color{cyan}41.14} & {\color{red}106.27} & {\color{cyan}61.42} & {\color{red}170.56} & {\color{cyan}90.41}\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.1in}
% \end{table}

% To evaluate the effects of desired trajectory quality to navigation performance, we compare two different approaches to compute desired trajectories in single-robot experiments.
% In the first, which we call the \emph{without prior} strategy, we set the desired trajectory of the robot to the straight line segment connecting its start position to its goal position.
% In the second, which we call the \emph{with prior} strategy, we set the desired trajectory of the robot to the shortest path between its start position to its goal position avoiding static obstacles.\looseness=-1

% During these experiments, we set $T^{search}_i = \SI{75}{ms}$, $\gamma^1_i = \SI{10}{\frac{m}{s}}$, $\gamma^1_i = \SI{15}{\frac{m}{s^2}}$.
% % The desired planning horizon $\tau_i$ is set to $\SI{2.5}{s}$.
% The desired speed $\hat{s}$ of dynamic obstacles is uniformly sampled in interval $[\SI{0.5}{\frac{m}{s}}, \SI{1.0}{\frac{m}{s}}]$, and repulsion strength $\hat{f}$ uniformly sampled in interval $[0.2, 0.5]$ for each obstacle.
% % Dynamic obstacle position/velocity sensing noise covariance $\Sigma_i$, and shape sensing noise standard deviation $\sigma_i$ are set to $0$, hence the robot senses dynamic obstacles perfectly.
% % We do not mock imperfect sensing of static obstacles, i.e., robot has access to the correct static obstacles in the environment.\looseness=-1

% The duration of the desired trajectory is set assuming that robot desires to follow it with $\frac{1}{3}$ with its maximum speed $\tilde{\gamma}_i^1$ for search.\looseness=-1

% We control the density $\rho$ of the forest, the number $\#^D$ of dynamic obstacles, and whether the desired trajectory is computed using with or without the knowledge of static obstacles a priori, and report the metrics.\looseness=-1

% The results are summarized in Table~\ref{Table:DesiredTrajectoryQuality}.
% With prior strategy results in higher success rates, lower collision rates, deadlock rate, planning failure rate and planning duration in all pairs of cases  ({\color{cyan}cyan} vs. {\color{red}red} values).
% Average navigation duration of without prior strategy is slightly lower than that of the with prior strategy ({\color{orange}orange} vs. {\color{magenta}magenta} values), which we attribute to the lower success rate of the without prior strategy, in which, hard navigation tasks are failed and the average navigation duration is computed over easier tasks. 
% These suggests the necessity of providing good desired trajectories.\looseness=-1

% In all following experiments, we use the \emph{with prior} strategy, and provide the desired trajectory by computing the shortest path avoiding only static obstacles and setting its duration assuming that robot desires to follow it with $\frac{1}{3}$ of its maximum speed $\tilde{\gamma}^1_i$ for search.\looseness=-1

% \subsubsection{Enforced Degree of Continuity}

% \begin{table}[t]
%     \centering
%     \caption{Effects of enforced continuity degree on navigation performance.}
%     \label{Table:Continuity}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline $\boldsymbol{c_i}$ & 0 & 1 & 2 & 3 & 4& 5\\
%         \hline 
%          \textbf{succ. rate} & {\color{red}0.980}  & {\color{red}0.986} & 0.971 & 0.930 & 0.893 & 0.830\\ 
%          \textbf{coll. rate} & 0.020 & 0.014 & 0.023 & 0.054  & 0.059 & 0.057\\
%          \textbf{deadl. rate}& 0.000 & 0.000 & 0.006 & 0.018  & 0.053 & 0.119 \\
%          \textbf{s. coll. rate}& 0.001 & 0.000 & 0.000 & 0.000 & 0.000 & 0.003 \\
%          \textbf{d. coll. rate} & 0.019 & 0.014 & 0.023  & 0.054 & 0.059 & 0.056 \\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & 30.87 & 29.23 & 28.57 & 28.50 & 28.31 & 28.29\\
%          \textbf{pl. fail rate}& {\color{cyan}0.001} & {\color{cyan}0.012} & {\color{cyan}0.041} & {\color{cyan}0.093} & {\color{cyan}0.121} & {\color{cyan}0.154}\\
%          \textbf{avg. pl. dur. [ms]} & 49.83 & 51.79 & 53.36  & 55.05 & 53.77 & 54.82\\
%          \hline
%     \end{tabular}}
% \end{table}

% The degree of continuity $c_i$ we enforce in the trajectory optimization stage determines the smoothness of the resulting trajectory.
% The search step enforces only position continuity and degree of continuity greater than that is enforced solely by the trajectory optimizer.
% % While we guarantee the feasibility of kinematic constraints, namely the position continuity and obstacle avoidance constraints, we cannot guarantee the feasibility of higher order continuity constraints.

% To evaluate the effects of enforced degree of continuity to navigation performance, we compare the navigation metrics when different degrees are enforced in single-robot experiments.

% During these experiments, we do not set any maximum derivative magnitudes to evaluate the effects of only the degree of continuity.
% We set $\tau_i = \SI{2.5}{s}$, $T^{search}_i = \SI{75}{ms}$, $\Sigma_i = \vzero$, $\sigma_i = 0$, $\rho = 0.2$, and $\#^D = 25$.
% $\hat{s}$ is sampled in interval $[\SI{0.5}{\frac{m}{s}}, \SI{1.0}{\frac{m}{s}}]$, and repulsion strength $\hat{f}$ is sampled in interval $[0.2, 0.5]$ uniformly for each obstacle.
% % Dynamic obstacle position/velocity sensing noise covariance $\Sigma$, and shape sensing noise standard deviation $\sigma$ are set to $0$, hence the robot senses dynamic obstacles perfectly.
% We do not mock sensing imperfections of static obstacles.
% % The density $\rho$ of the forest is set to $0.2$ and the number of dynamic obstacles $\#^D$ is set to $25$.

% We control the enforced degree of continuity $c_i$, and report the metrics.

% The results are summarized in Table~\ref{Table:Continuity}.
% In general, success rates decrease and deadlock and collision rates increase as the enforced degree of continuity increases.
% Interestingly, success rate from degree $0$ to degree $1$ increases ({\color{red}red} values in \textbf{succ. rate} row), which we adhere to the non-smooth changes in robot's position when $c_i=0$, causing hard to predict interactions between dynamic obstacles and the robot.
% Navigation duration is not significantly affected by the enforced degree of continuity.
% Planning failure rate increases with the enforced degree of continuity as seen in {\color{cyan}cyan} values in \textbf{pl. fail rate} row, which is the main cause of collision rate increase.
% Average planning duration is not significantly affected by the enforced degree of continuity, because the number of continuity constraints are insignificant compared to safety constraints.

% In the remaining experiments, we set $c_i=2$ for all robots $i$, i.e., enforce continuity up to acceleration, unless explicitly stated otherwise.

% \subsubsection{Dynamic Limits}


% \begin{table}[t]
%     \centering
%     \caption{Effects of dynamic limits}
%     \vspace{-0.05in}
%     \label{Table:DynamicLimits}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline$\boldsymbol{\gamma_i^1}[\frac{m}{s}]$ & $\infty$ & $15$ & $10$ & $10$ & $10$ & $10$  \\
%          $\boldsymbol{\gamma_i^2}[\frac{m}{s^s}]$ & $\infty$ & $20$ & $20$ & $15$ & $10$ & $7$   \\
%         \hline 
%          \textbf{succ. rate} & 0.977 & {\color{cyan}0.961} & {\color{cyan}0.961} & {\color{red}0.955} & {\color{red}0.897} & {\color{red}0.844}\\ 
%          \textbf{coll. rate} & 0.022 & 0.035 & 0.037 & 0.037 & 0.101  & 0.148 \\
%          \textbf{deadl. rate}& 0.001 & 0.004 &  0.002 & 0.008 & 0.002 & 0.019  \\
%          \textbf{s. coll. rate} & 0.002 & 0.002 & 0.000 & 0.000 & 0.007 & 0.013 \\
%          \textbf{d. coll. rate} & 0.021 & 0.033 & 0.037 & 0.037 & 0.095 & 0.144 \\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]}  &28.55 & 28.57 & 28.51 & 28.56 &   28.49 & 28.47 \\
%          \textbf{pl. fail rate} & 0.040 & 0.046 & 0.045 & 0.053  & 0.069  & 0.093 \\
%          \textbf{avg. pl. dur. [ms]} & 54.66 & 63.14 & 62.44 & 61.90 &   59.84 & 60.34\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.1in}
% \end{table}

% % The dynamic limits $\gamma_i^k$ of the robots are conservatively enforced during trajectory optimization.\looseness=-1

% The discrete search stage limits maximum speed of the resulting discrete plan by enforcing maximum speed $\tilde{\gamma}_i^1$ (which is set to $\SI{5}{\frac{m}{s}}$ during the experiments).
% While this encourages limiting the maximum speed of the resulting trajectory, trajectory optimization stage actually enforces the dynamic limits.\looseness=-1

% To evaluate the effects of different dynamic limits $\gamma_i^k$, we compare the navigation metrics when different dynamic limits are enforced in single-robot experiments.\looseness=-1

% % During these experiments, we do not set any maximum derivative magnitudes in validity check.
% During these experiments, we set $T^{search}_i = \SI{75}{ms}$, $\rho = 0.2$, and $\#^D = 25$.
% The desired speed $\hat{s}$ of dynamic obstacles is sampled in interval $[\SI{0.5}{\frac{m}{s}}, \SI{1.0}{\frac{m}{s}}]$, and repulsion strength $\hat{f}$ is sampled in interval $[0.2, 0.5]$ uniformly for each obstacle.
% % We do not mock sensing imperfections of static obstacles.\looseness=-1

% We control the maximum velocity $\gamma_i^1$ and maximum acceleration $\gamma_i^2$, and report the metrics.
% We do not decrease $\gamma_i^1$ below $\SI{10}{\frac{m}{s}}$, because search has a speed limit of $\tilde{\gamma}_i^1 = \SI{5}{\frac{m}{s}}$, and setting $\gamma_i^1 = \SI{10}{\frac{m}{s}}$ enforces $\frac{10}{\sqrt{3}}{\frac{m}{s}} \approx \SI{5.77}{\frac{m}{s}}$ speed limit in all dimensions.
% If a lower speed limit is required, maximum speed limit $\tilde{\gamma}_i^1$ of search should also be decreased.\looseness=-1

% The results are given in Table~\ref{Table:DynamicLimits}.
% Unsurprisingly, collision, deadlock and planning failure rates increase and success rate decreases as the dynamic limits get more constraining.
% Since the velocity is limited during the discrete search step explicitly, decreasing $\gamma_i^1$ has a relatively smaller effect on metrics ({\color{cyan}cyan} values in \textbf{succ. rate} row) compared to decreasing $\gamma_i^2$ ({\color{red}red} values in \textbf{succ. rate} row).\looseness=-1

% In the remaining experiments, we set $\gamma_i^1 = \SI{10}{\frac{m}{s}}$ and $\gamma_i^2 = \SI{15}{\frac{m}{s^2}}$ for all robots $i$, unless explicity stated otherwise.\looseness=-1


\subsubsection{Repulsive Dynamic Obstacle Interactivity}

\begin{table}[t]
    \centering
    \caption{Effects of repulsion strength}
    \vspace{-0.05in}
    \label{Table:RepulsionStrength}
    \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
         \hline $\hat{f}$ & {\color{cyan}$-0.5$} & $0$ & $0.5$ & $1.5$ & $3.0$ & $6.0$  \\
         \hline 
         \textbf{succ. rate} & 0.818 & 0.897 & 0.915 & 0.936 & 0.970 & 0.991\\ 
         \textbf{coll. rate} & 0.182 & 0.102  & 0.084 & 0.064 & 0.030  & 0.009\\
         \textbf{deadl. rate} & 0.001 & 0.004 & 0.002 & 0.000  &  0.000  & 0.000 \\
         {\color{lightgray}\textbf{s. coll. rate}} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} &   {\color{lightgray}0.000} &{\color{lightgray}0.000} \\
         \textbf{d. coll. rate}  & 0.182 & 0.102  & 0.084  & 0.064 &   0.030 & 0.009\\
          {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
         \textbf{avg. nav. dur. [s]}   & 26.71 & 26.72 & 26.71 & 26.71 & 26.69  & 26.68\\
         \textbf{pl. fail rate} & 0.029 & 0.030 & 0.026 & 0.022 & 0.019  & 0.014\\
         \textbf{avg. pl. dur. [ms]}  & 49.72 & 50.63 & 50.55  & 46.98  &  43.17 & 37.76\\
         \hline
    \end{tabular}}
    \vspace{-0.1in}
\end{table}

We experiment with different levels of repulsive interactivity of dynamic obstacles and we compare navigation metrics when dynamic obstacles use different repulsion strengths $\hat{f}$ in single-robot experiments.
During these experiments, we set $\rho_i = 0$, and $\#^D = 50$.\looseness=-1
% The desired speed $\hat{s}$ of dynamic obstacles is sampled in interval $[\SI{0.5}{\frac{m}{s}}, \SI{1.0}{\frac{m}{s}}]$ uniformly for each obstacle.

% We control the repulsion strength $\hat{f}$ and report the metrics.\looseness=-1

The results are summarized in Table~\ref{Table:RepulsionStrength}.
In general, as the repulsive interactivity increases, the collision and deadlock rates decrease, and the success rate increases.
In addition, the average planning duration decreases as the repulsive interactivity increases, because the problem gets easier for the robot if dynamic obstacles take some responsibility for collision avoidance even with a simple repulsion rule.
In experiment $1$, repulsion strength is set to $-0.5$ ({\color{cyan}cyan} value), causing dynamic obstacles to get attracted to the robot, i.e., they move towards the robot.
Even in that case, the robot can achieve a high success rate, i.e., 0.818, as it models dynamic obstacle interactivity.\looseness=-1

In the remaining experiments, we sample $\hat{f}$ in $[0.2, 0.5]$, unless explicitly stated otherwise.

% \subsubsection{Dynamic Obstacle Desired Speed}

% \begin{table}[t]
%     \centering
%     \caption{Effects of dynamic obstacle desired speed}
%     \vspace{-0.05in}
%     \label{Table:DesiredSpeed}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline $\hat{s} [\frac{m}{s}]$ & $0.5$ & $0.75$ & $1.0$ & $1.5$ & $2.5$ & $5.0$  \\
%          \hline 
%          \textbf{succ. rate} & 0.937 & 0.914 & 0.900 & 0.864 & 0.717 & 0.272\\ 
%          \textbf{coll. rate} & 0.062 & 0.086 & 0.099 & 0.136 & 0.283 & 0.728\\
%          \textbf{deadl. rate} & {\color{cyan}0.002} & {\color{cyan}0.002} & {\color{cyan}0.002} & {\color{cyan}0.001} & {\color{cyan}0.000} & {\color{cyan}0.000}\\
%          {\color{lightgray}\textbf{s. coll. rate}} & {\color{lightgray}0.000} &  {\color{lightgray}0.000}  &  {\color{lightgray}0.000} &   {\color{lightgray}0.000}  & {\color{lightgray}0.000} & {\color{lightgray}0.000}\\
%          \textbf{d. coll. rate}  & 0.062 & 0.086 &0.099  &  0.136& 0.283 & 0.728\\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & 26.68 & 26.74 & 26.72 & 26.67 & 26.68 & 27.36\\
%          \textbf{pl. fail rate} & 0.026 & 0.026 & 0.028 & {\color{orange}0.026} & {\color{orange}0.031} & {\color{orange}0.038}\\
%          \textbf{avg. pl. dur. [ms]} & {\color{red}48.76} & {\color{red}51.03} & {\color{red}48.94} & {\color{red}36.10} & {\color{red}30.83} & {\color{red}25.20}\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.2in}
% \end{table}

% To evaluate the behavior of our planner in environments with dynamic obstacles having different desired speeds, we control the desired speed $\hat{s}$ and report the metrics in single-robot experiments.\looseness=-1

% During these experiments, we set $\rho_i = 0$, $\#^D = 50$.

% The results are given in Table~\ref{Table:DesiredSpeed}.
% The collision rates increase and the success rate decreases as the desired speed of dynamic obstacles increase.
% Deadlock rates are close to $0$ in all cases ({\color{cyan}cyan} values in \textbf{deadl. rate} row), because there are no static obstacles in the environment and dynamic obstacles eventually move away from the robot.
% The average planning duration for the robot decreases as the desired speed increases as it can be seen in {\color{red}red} values in the \textbf{avg. pl. dur.} row.
% We attribute this to the dynamic obstacles with constant velocity.
% As the desired speed of the dynamic obstacles increase, constant velocity obstacles leave the environment faster, causing the robot to avoid lesser number of obstacles, albeit faster obstacles.
% The planning failure rate also increases with the increase in dynamic obstacle desired speeds as it can be seen in {\color{orange}orange} values in \textbf{pl. fail rate} row, because the robot has to take abrupt actions more frequently to avoid faster obstacles, causing the pipeline to fail because of dynamic limits of the robot.\looseness=-1

% In the remaining experiments we sample desired speed $\hat{s}$ uniformly in interval $[\SI{0.5}{\frac{m}{s}}, \SI{1.0}{\frac{m}{s}}]$.

% \subsubsection{Number of Dynamic Obstacles}

% \begin{table}[t]
%     \centering
%     \caption{Effects of number of dynamic obstacles}
%     \vspace{-0.05in}
%     \label{Table:NumberOfDynamicObstacles}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline $\#^D$ & $10$ & $25$ & $50$ & $75$ & $100$ & $200$  \\
%          \hline 
%          \textbf{succ. rate} & 0.997 & 0.973 & 0.897 & 0.838 & 0.747 & 0.402\\ 
%          \textbf{coll. rate} & 0.003 & 0.027 & 0.102 & 0.161 & 0.252 & 0.598\\
%          \textbf{deadl. rate} & 0.000 & 0.000 & 0.004 & 0.007 & 0.017 & 0.087\\
%          {\color{lightgray}\textbf{s. coll. rate}} & {\color{lightgray}0.000} & {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000}    & {\color{lightgray}0.000} & {\color{lightgray}0.000}\\
%          \textbf{d. coll. rate}  & 0.003 & 0.027 &  0.102  &  0.161    & 0.252 & 0.598\\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & 26.69 &  26.69 &  26.72 &  26.72   & {\color{red}26.76} & {\color{red}28.05}\\
%          \textbf{pl. fail rate} & 0.005 &  0.013  & 0.027  &    0.037 & 0.049  & 0.098\\
%          \textbf{avg. pl. dur. [ms]} & 17.88 &  30.72  &  50.59 &  65.90   & 76.68 & 104.70\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.1in}
% \end{table}


% To evaluate the behavior of our planner in environments with different number of dynamic obstacles, we control the number of dynamic obstacles $\#^D$, and report the metrics in single-robot experiments.

% During these experiments, we set $T^{search}_i = \SI{75}{ms}$, and $\rho = 0$. 

% The results are given in Table~\ref{Table:NumberOfDynamicObstacles}.
% Expectedly, as the number of dynamic obstacles increase, collision, deadlock, and planning failure rates increase, and the success rate decreases.
% Average navigation duration is not affected until the number of dynamic obstacles is increased from $100$ to $200$ as it can be seen in {\color{red}red} values in \textbf{avg. nav. dur} row.
% Even then, increase in navigation duration is small.\looseness=-1

% \subsubsection{Static Obstacle Density}

% \begin{table}[t]
%     \centering
%     \caption{Effects of static obstacle density to navigation performance.}
%     \label{Table:StaticObstacleDensity}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline $\rho$ & $0.1$ & $0.2$ & $0.3$ & $0.4$ & $0.5$ & $0.6$  \\
%          \hline 
%          \textbf{succ. rate} & 0.998 & 0.998 & 0.999 & 0.985 & 0.984 & 0.991\\ 
%          \textbf{coll. rate} & 0.000  & 0.000 & 0.001 & 0.004 & 0.005 & 0.000\\
%          \textbf{deadl. rate} & 0.002 & 0.002 & 0.000 & 0.012 & 0.013 & 0.009\\
%          \textbf{s. coll. rate} & 0.000 & 0.000 & 0.001 & 0.004 & 0.005 & 0.000\\
%          {\color{lightgray}\textbf{d. coll. rate}} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000}  & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000}\\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & 27.52 & 28.46 & 30.28 & 33.42 & 35.13 & 35.35\\
%          \textbf{pl. fail rate} & {\color{cyan}0.012} & {\color{cyan}0.039} & {\color{cyan}0.068} & {\color{cyan}0.063} & {\color{red}0.026} & {\color{red}0.013}\\
%          \textbf{avg. pl. dur. [ms]} &  {\color{cyan}34.33} &  {\color{cyan}41.73} &  {\color{cyan}46.73} &  {\color{cyan}50.12} & {\color{red}42.74} & {\color{red}38.98}\\
%          \hline
%     \end{tabular}}
% \end{table}

% We control the static obstacle density $\rho$ and report the metrics to evaluate the behavior of our planner in environments with different static obstacle densities in single-robot experiments.

% We set $\tau_i = \SI{2.5}{s}$, $T^{search}_i = \SI{75}{ms}$, $\Sigma_i = \vzero$, and $\sigma_i = 0$. We set number of dynamic obstacles $\#^D = 0$ to evaluate the affects of static obstacles only.
% Note that, during these experiments, desired trajectory is set using the \emph{with prior} strategy, meaning that the desired trajectory avoids all static obstacles.
% We do not mock sensing imperfections of static obstacles.

% The results are given in Table~\ref{Table:StaticObstacleDensity}.
% The effect of $\rho$ to success, collision, and deadlock rates is small.
% This stems from the fact that desired trajectory is already good, and our planner has to track it as close as possible.
% Average navigation duration increases with $\rho$ because the environment gets more complicated, causing the robot to traverse longer paths.
% From $\rho=0.1$ to $\rho=0.4$, planning failure rate and planning duration increase, while from $\rho = 0.4$ to $\rho = 0.6$, they decrease as it can be seen in {\color{cyan}cyan} and {\color{red}red} values in \textbf{pl. fail rate} and \textbf{avg. pl. dur} rows.
% The reason is the fact that as the environment density increases, it is less likely that the shortest path from the start position to the goal position goes through the forest, as there is no path through the forest.
% Instead, desired trajectory avoids the forest all together.
% This causes planning to become easier.


% \subsubsection{Time Limit of Search} 

% \begin{table}[t]
%     \centering
%     \caption{Effects of the time limit of search with dynamic limits}
%     \vspace{-0.05in}
%     \label{Table:TimeLimitOfSearch}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline $T^{search}_i [ms]$ & $75$ & $150$ & $300$ & $600$ & $1000$ & $2000$  \\
%          \hline 
%          \textbf{succ. rate} & 0.655 & {\color{cyan}0.731} & {\color{red}0.708} & {\color{red}0.730} & {\color{red}0.682} & {\color{red}0.700} \\ 
%          \textbf{coll. rate} & 0.343 & {\color{cyan}0.268} & {\color{red}0.289} & {\color{red}0.269} & {\color{red}0.315} & {\color{red}0.296}\\
%          \textbf{deadl. rate} & 0.013 & {\color{cyan}0.013} & {\color{red}0.014} & {\color{red}0.018} & {\color{red}0.026} & {\color{red}0.022}\\
%          \textbf{s. coll. rate} & 0.031 & 0.016 & 0.005 & 0.003 & 0.001 & 0.000\\
%          \textbf{d. coll. rate} & 0.339 & 0.268 & 0.288 & 0.269 & 0.315 & 0.296\\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & 29.15 & 29.11 & 29.05 & 28.81 & 420.97 & 28.83\\
%          \textbf{pl. fail rate} & {\color{orange}0.078} & {\color{orange}0.082} & {\color{orange}0.089} & {\color{orange}0.107} & {\color{orange}0.119} & {\color{orange}0.130}\\
%          \textbf{avg. pl. dur. [ms]} & 108.51 & 145.55 & 205.40 & 300.02 & 420.97 & 688.66\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.2in}
% \end{table}

% The time limit $T^{search}_i$ of the discrete search determines the number of states discovered and expanded during the cost algebraic A* search, directly affecting the quality of the resulting plan.
% To evaluate the affects of $T^{search}_i$, we run our algorithm with different values of it and report the metrics in single-robot experiments.
% During evaluations, we set $T^{search}_i$ to values from $\SI{75}{ms}$ to $\SI{2000}{ms}$.
% Note that, for high values of $T^{search}_i$, the planning is not real-time anymore.
% However, we still evaluate the hypothetical performance of our algorithm by freezing the simulation until the planning is done.\looseness=-1

% During these experiments we set $\rho = 0.2$, and $\#^D = 100$.
% % We do not mock sensing imperfections of static obstacles.

% First set of results are summarized in Table~\ref{Table:TimeLimitOfSearch}.
% Surprisingly, as $T^{search}_i$ increases, the collision and deadlock rates increase and the success rate decreases as it can be seen by comparing {\color{cyan}cyan} values to {\color{red}red} values, except for the change from $\SI{75}{ms}$ to $\SI{150}{ms}$.
% This is surprising because, discrete search theoretically produces no worse plans than before as the time allocated for it increases.
% The reason of this stems from the dynamic limits of the robot.
% We enforce continuity up to acceleration, maximum velocity $\SI{10}{\frac{m}{s}}$ and maximum acceleration $\SI{15}{\frac{m}{s^2}}$.
% Enforcing these during trajectory optimization becomes harder as the discrete plan gets more and more complicated, having more rotations and speed changes.
% Increasing the time allocated to discrete search makes it more likely to produce complicated plans.
% Increase in planning failure rates support this argument, as it can be seen in {\color{orange}orange} values in \textbf{pl. fail rate} column.\looseness=-1
% % As $T^{search}_i$ increases, planning failure rates also increase.\looseness=-1

% \begin{table}[t]
%     \centering
%     \caption{Effects of the time limit of search without dynamic limits}
%     \vspace{-0.05in}
%     \label{Table:NoDynTimeLimitOfSearch}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline $T^{search}_i [ms]$ & $75$ & $150$ & $300$ & $600$ & $1000$ & $2000$  \\
%          \hline 
%          \textbf{succ. rate} & {\color{cyan}0.828} & {\color{cyan}0.892} & {\color{cyan}0.922} &  {\color{cyan}0.934} & {\color{cyan}0.926} & {\color{cyan}0.933} \\ 
%          \textbf{coll. rate} & {\color{cyan}0.170} & {\color{cyan}0.107} & {\color{cyan}0.076} & {\color{cyan}0.061} & {\color{cyan}0.074} & {\color{cyan}0.066}\\
%          \textbf{deadl. rate} & {\color{cyan}0.003} & {\color{cyan}0.003} & {\color{cyan}0.002} & {\color{cyan}0.005} & {\color{cyan}0.000} & {\color{cyan}0.001}\\
%          \textbf{s. coll. rate} & 0.003 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
%          \textbf{d. coll. rate} & 0.169 & 0.107 & 0.076 & 0.061 & 0.074 & 0.066\\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & 29.80 & 29.90 & 29.69 & 29.50 & 29.47 & 29.54\\
%          \textbf{pl. fail rate} & {\color{red}0.015} & {\color{red}0.017} & {\color{red}0.024} & {\color{red}0.035} & {\color{red}0.043} & {\color{red}0.049}\\
%          \textbf{avg. pl. dur. [ms]} & 97.27 & 134.80 & 192.80 & 301.26 & 437.43 & 742.61\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.1in}
% \end{table}

% To further support this argument, we run another set of experiments in which we set $c_i = 1$, $\gamma^1_i = \infty$, and $\gamma^2_i = \infty$.
% The results are given in Table~\ref{Table:NoDynTimeLimitOfSearch}.
% This time, deadlock and collision rates decrease and the success rate increases as $T^{search}_i$ increases as it can be seen in {\color{cyan}cyan} values.
% Planning failure rate also increases ({\color{red}red} values in \textbf{pl. fail rate} row) but its effects are not detrimental to collision and deadlock rates like before.\looseness=-1

% In the remaining experiments, we set $T^{search}_i = \SI{75}{ms}$.


% \subsubsection{Desired Planning Horizon}

% \begin{table}[t]
%     \centering
%     \caption{Effects of the desired planning horizon to navigation performance.}
%     \vspace{-0.05in}
%     \label{Table:DesiredPlanningHorizon}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline $\tau_i [s]$ & $1.0$ & $2.5$ & $5.0$ & $7.5$ & $10.0$ & $20.0$  \\
%          \hline 
%          \textbf{succ. rate} & {\color{red}0.943} & {\color{cyan}0.948} & {\color{orange}0.935} & {\color{orange}0.924} & {\color{orange}0.906} & {\color{orange}0.883}\\ 
%          \textbf{coll. rate} & {\color{red}0.052} & {\color{cyan}0.046} & {\color{orange}0.061} & {\color{orange}0.066} & {\color{orange}0.074} & {\color{orange}0.093} \\
%          \textbf{deadl. rate} & {\color{red}0.008} & {\color{cyan}0.006} & {\color{orange}0.004} & {\color{orange}0.011} & {\color{orange}0.022} & {\color{orange}0.031}\\
%          \textbf{s. coll. rate} & 0.001 & 0.002 & 0.004 & 0.009 & 0.015 &0.049 \\
%          \textbf{d. coll. rate} & 0.052 & 0.045 & 0.058 & 0.058 & 0.064 & 0.058\\
%           {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
%          \textbf{avg. nav. dur. [s]} & {\color{red}29.52} & {\color{cyan}28.49}  & 28.42 & 28.33 & 28.32 &28.61 \\
%          \textbf{pl. fail rate} & 0.053 & 0.052 & 0.069 & 0.071 & 0.063 & 0.050\\
%          \textbf{avg. pl. dur. [ms]} & 58.71 &  62.55& 80.71 & 119.52 & 152.78 & 195.13\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.1in}
% \end{table}

% The desired planning horizon $\tau_i$ is used by the goal selection stage.
% The goal selection stage selects the position on the desired trajectory that is one planning horizon away from the current time as the goal position if robot is collision-free when placed on it.
% As $\tau_i$ increases, our planner tends to plan longer and longer trajectories.

% We evaluate the effects of $\tau_i$ to navigation performance by changing its value and reporting navigation metrics in single-robot experiments.
% In these experiments, we set $\Sigma_i = \vzero$, $\sigma_i = 0$, $\rho = 0.2$, and $\#^D = 25$.
% We do not mock sensing imperfections of static obstacles.

% The results are summarized in Table~\ref{Table:DesiredPlanningHorizon}.
% When $\tau_i$ is set to $\SI{1.0}{s}$, the robot tends to plan shorter trajectories, limiting its longer horizon decision making ability.
% Increasing $\tau_i$ to $\SI{2.5}{s}$, decreases collision and deadlock rates and increases the success rate, as well as the average navigation duration compared to $\tau_i=\SI{1.0}{s}$, which can be seen by comparing the {{\color{cyan}cyan} and {{\color{red}red} values.
% However, increasing $\tau_i$ more causes an increase in collision and deadlock rates and a decrease in the success rate, which can be seen in {\color{orange}orange} values.
% The reason is that increasing $\tau_i$ causes distance between the goal position and robot's current position to increase.
% As the distance to the goal position increases, the discrete search step needs more time to produce high quality discrete plans.
% Since $T^{search}_i$ is fixed during these experiments, increasing $\tau_i$ causes robot to collide or deadlock more.

% In the remaining experiments, we set $\tau_i = \SI{2.5}{s}$.

\subsubsection{Dynamic Obstacle Sensing Uncertainty}\label{Section:DynamicObstacleSensingUncertainty}

\begin{table}[t]
    \centering
    \caption{Effects of dynamic obstacle sensing uncertainty}
    \vspace{-0.05in}
    \label{Table:DynamicObstacleSensingUncertainty}
    \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
         \hline $\Sigma_i [\times I_{2d}]$ & $0.0$ & $0.1$ & $0.1$ & $0.2$ & $0.2$ & $0.5$  \\
         $\sigma$ & $0.0$ & $0.0$ & $0.1$ & $0.1$ & $0.2$ & $0.5$  \\
         \hline 
         \textbf{succ. rate} & 0.905 & 0.879 & 0.832 & 0.797 & {\color{cyan}0.661} & 0.410\\ 
         \textbf{coll. rate} &  0.094 & 0.119 &  0.168 & 0.203 & 0.339 &  0.590\\
         \textbf{deadl. rate} &  0.001& 0.003 & 0.002 & 0.001&0.004  & 0.009 \\
         {\color{lightgray}\textbf{s. coll. rate}} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000}  &  {\color{lightgray}0.000}\\
         \textbf{d. coll. rate}& 0.094 & 0.119 & 0.168 & 0.203& 0.339 &  0.590\\
          {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
         \textbf{avg. nav. dur. [s]} & 26.71 & 26.70 & 26.70 &26.71 &26.69  &  26.72\\
         \textbf{pl. fail rate}& 0.026 & 0.030 & 0.028 & 0.029& 0.031 & 0.037 \\
         \textbf{avg. pl. dur. [ms]}&  59.76& 55.53 & 54.54 & 54.83& 54.46 & 56.15 \\
         \hline
    \end{tabular}}
    \vspace{-0.2in}
\end{table}

The dynamic obstacle sensing uncertainty is modeled by i) applying a zero mean Gaussian with covariance $\Sigma_i$ to the sensed positions and velocities of dynamic obstacles and ii) randomly inflating dynamic obstacle shapes by a zero mean Gaussian with standard deviation $\sigma_i$.
These create three inaccuracies reflected to our planner: i) dynamic obstacle shapes provided to our planner become wrong, ii) prediction inaccuracy increases, and iii) current positions $\vp^{dyn}_{i,j}$ of dynamic obstacles provided to the planner become wrong.
Our planner models uncertainty across behavior models by using realization probabilities but does not explicitly account for the current position or shape uncertainty.\looseness=-1
% Note that our planner does not explicitly model dynamic obstacle shape sensing uncertainty.
% In addition, it does not explicitly model dynamic obstacle current position uncertainty.
% It also does not explicitly model uncertainty of predicted individual behavior models.
% It models uncertainty across behavior models by assigning probabilities to each of them.\looseness=-1

To evaluate the performance of DREAM under different levels of dynamic obstacle sensing uncertainty, we control $\Sigma_i$ and $\sigma_i$ and report the metrics in single-robot experiments.
During these experiments, we set $\rho = 0$ to evaluate the effects of dynamic obstacles only. 
We set $\#^D = 50$.\looseness=-1

The results are given in Table~\ref{Table:DynamicObstacleSensingUncertainty}.
$\Sigma_i$ is set to a constant multiple of identity matrix $I_{2d}$ of size $2d \times 2d$ in each experiments.
Expectedly, as the uncertainty increases, the success rate decreases.
An increase in collision and deadlock rates is also seen.
Similarly, the planning failure rate tends to increase as well.\looseness=-1

\begin{table}[t]
    \centering
    \caption{Effects of dynamic obstacle inflation under dynamic obstacle sensing uncertainty of $\Sigma_i = 0.2I_{2d}, \sigma_i = 0.2$.}
    \vspace{-0.05in}
    \label{Table:InflationUnderDynamicObstacleSensingUncertainty}
    \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
         \hline inflation $[m]$ & $0.2$ & $0.5$ & $1.0$ & $1.5$ & $2.0$ & $4.0$  \\
         \hline 
         \textbf{succ. rate} & {\color{cyan}0.779} & {\color{cyan}0.818} & 0.747 & 0.617 & 0.417 & {\color{red}0.057}\\ 
         \textbf{coll. rate} & 0.220 & 0.182 & 0.250 & 0.382 & 0.581 &  0.943\\
         \textbf{deadl. rate}& 0.003 & 0.001 &  0.005& 0.007 & 0.003 &  0.000\\
          {\color{lightgray}\textbf{s. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000}  &  {\color{lightgray}0.000} &   {\color{lightgray}0.000}\\
         \textbf{d. coll. rate} & 0.220 & 0.182 & 0.250 & 0.382 & 0.581 &  0.943\\
          {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
         \textbf{avg. nav. dur. [s]} & 26.70 & 26.72 & 26.93 & 27.16 & 27.79 & 26.81 \\
         \textbf{pl. fail rate} & 0.032 & 0.035 & 0.047 & 0.058 &0.067 &  0.021\\
         \textbf{avg. pl. dur. [ms]} & 57.80 &  63.014& 70.92 & 74.63 & 72.66 & 23.13 \\
         \hline
    \end{tabular}}
    \vspace{-0.1in}
\end{table}

One common approach to tackling unmodeled uncertainty for obstacle avoidance is artificially inflating the shapes of obstacles.
To show the effectiveness of this approach, we set $\Sigma_i = 0.2I_{2d}$ and $\sigma_i = 0.2$, and inflate the shapes of obstacles with different amounts during planning.
The results of these experiments are given in Table~\ref{Table:InflationUnderDynamicObstacleSensingUncertainty}.
% The inflation amount is the amount we inflate the shape of the dynamic obstacle in all axes.
Inflation clearly helps when done in reasonable amounts.
When inflation is set to $\SI{0.2}{m}$, success rate increases from $0.661$ as reported in Table~\ref{Table:DynamicObstacleSensingUncertainty} ({\color{cyan}cyan} value) to $0.779$.
It further increases to $0.818$ when the inflation amount is set to $\SI{0.5}{m}$, which can be seen in {\color{cyan}cyan} values in Table~\ref{Table:InflationUnderDynamicObstacleSensingUncertainty}.
However, as the inflation amount increases, metrics start to degrade as the planner becomes overly conservative.
Success rate decreases down to $0.057$ when the inflation amount is set to $\SI{4.0}{m}$ as it can be seen in the {\color{red}red} value in Table~\ref{Table:InflationUnderDynamicObstacleSensingUncertainty}.\looseness=-1
 
% In the remaining experiments, we set $\Sigma_i = \vzero$ and $\sigma_i = 0$.

\subsubsection{Static Obstacle Sensing Uncertainty}\label{Section:StatiObstacleSensingUncertainty}

\begin{table}
    \centering
    \caption{Effects of static obstacle sensing uncertainty}
    \vspace{-0.05in}
    \label{Table:StaticObstacleSensingUncertainty}
    
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
         \hline imperfections & $L(0.2)$ & $L(0.2)^2$ & $L(0.3)^2I$ &$L(0.3)^2ID$& $L(0.3)^2IDL(0.2)$ & $L(0.3)^2IDL(0.5)$   \\
         \hline 
         \textbf{succ. rate} & 0.994 & 0.992 & 0.986 & 0.956 & 0.971 & 0.949 \\
         \textbf{coll. rate} & {\color{cyan}0.001}  & {\color{cyan}0.003}&{\color{cyan}0.005} &  {\color{red}0.037}&{\color{orange}0.020} & {\color{magenta}0.039}\\
         \textbf{deadl. rate}&{\color{cyan}0.005} &{\color{cyan}0.005} &{\color{cyan}0.009} & 0.007& 0.009 & 0.015\\
         \textbf{s. coll. rate}&0.001 &0.003 & 0.005& 0.037&0.020 &0.039 \\
         {\color{lightgray}\textbf{d. coll. rate}}&{\color{lightgray}0.000} & {\color{lightgray}0.000}& {\color{lightgray}0.000}&{\color{lightgray}0.000} & {\color{lightgray}0.000}&{\color{lightgray}0.000} \\
          {\color{lightgray}\textbf{t. coll. rate}} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} &  {\color{lightgray}0.000} \\
         \textbf{avg. nav. dur. [s]}&27.49 & 27.53& 27.73&27.49 & 27.55& 28.18\\
         \textbf{pl. fail rate}&0.029 &0.041 & 0.053& 0.038 & 0.047 & 0.063\\
         \textbf{avg. pl. dur. [ms]}& 36.16& 41.76&53.39 &40.58 & 46.41 &67.65 \\
         \hline
    \end{tabular}}
    \vspace{-0.2in}
\end{table}

As we describe in Sec.~\ref{Section:Mocking}, we use three operations to model sensing imperfections of static obstacles: i) increaseUncertainty, ii) leakObstacles($p_{leak}$), and iii) deleteObstacles.
We evaluate the effects of static obstacle sensing uncertainty by applying a sequence of these operations to the octree representation of static obstacles, and provide the resulting octree to our planner in single-robot experiments.
Application of leakObstacles increases the density of the map, but the resulting map contains the original obstacles. 
increaseUncertainty does not change the density of the map but increases the uncertainty associated with the obstacles.
deleteObstacles decreases the density of the map, but the resulting map may not contain the original obstacles, leading to unsafe behavior. In these experiments, we set $\rho = 0.1$, and $\#^D = 0$.\looseness=-1

The results are given in Table~\ref{Table:StaticObstacleSensingUncertainty}.
In the imperfections row of the table, we use $L(p_{leak})$ as an abbreviation for leakObstacles($p_{leak}$), and $L(p_{leak})^n$ as an abbreviation for repeated application of leakObstacles to the octree.
We use $I$ for increaseIncertainty, and $D$ for deleteObstacles.
Leaking obstacles or increasing the uncertainty associated with them does not increase the collision and deadlock rates significantly as seen in the {\color{cyan}cyan} values.
The planning duration and failure rates increase as the number of obstacles increases.
Deleting obstacles causes a sudden jump of the collision rate as it can be seen between {\color{cyan}cyan} and {\color{red}red} values because the planner does not know about existing obstacles.
Leaking obstacles back with $p_{leak} = 0.2$ after deleting them decreases the collision rate back as it can be seen between {\color{red}red} and {\color{orange}orange} values.
However, leaking obstacles with high probability increases the collision rate back as it can be seen between {\color{orange}orange} and {\color{magenta}magenta} values.
This happens because the environments get significantly more complicated because the number of obstacles increases.
Environment complexity is also reflected in the increased deadlock rate.\looseness=-1

% In the remaining experiments, we do not mock static obstacle sensing imperfections.

% \subsubsection{Teammate Safety Enforcement Strategies}

% \begin{table*}
%     \centering
%     \vspace{0.1in}
%     \caption{Effects of teammate safety enforcement strategies under imperfect communication with or without dynamic objects}
%     \vspace{-0.05in}
%     \label{Table:SafetyEnforcement}
%     \resizebox{\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & $\delta [s]$ & $\kappa$ & $\#^D$ &  Safety & \textbf{succ. rate} & \textbf{coll. rate} & \textbf{deadl. rate} & \textbf{s. coll. rate} & \textbf{d. coll. rate} & \textbf{t. coll. rate} & \textbf{avg. nav. dur. [s]} & \textbf{pl. fail rate} & \textbf{avg. pl. dur. [ms]}\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$0.0$} & \multirow{2}{*}{$0.0$} & \multirow{2}{*}{$0$} & Constraint &$1.000$  &$0.000$& $0.000$& {\color{lightgray}$0.000$}&$0.000$&{\color{red}$0.000$}&$61.17$&$0.000$&$200.84$\\
%         &&&&Cost & $1.000$& $0.000$& $0.000$ & {\color{lightgray}$0.000$} &$0.000$&{\color{teal}$0.000$}&$60.45$&$<0.001$&$168.90$\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$0.0$} & \multirow{2}{*}{$0.0$} & \multirow{2}{*}{$50$} & Constraint & {\color{brown}$0.791$} & {\color{brown}$0.209$}& $0.001$ & {\color{lightgray}$0.000$} &{\color{brown}$0.209$}&{\color{red}$0.000$}&$44.12$&$0.007$&$260.44$\\
%         &&&&Cost &{\color{brown}$0.952$} &{\color{brown}$0.048$}&$0.000$&{\color{lightgray}$0.000$}&{\color{brown}$0.045$}&{\color{cyan}$0.004$}&$45.35$&$0.005$&$164.06$\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$0.25$} & \multirow{2}{*}{$0.1$} & \multirow{2}{*}{$0$} & Constraint &$1.000$  & $0.000$ & $0.000$ & {\color{lightgray}$0.000$} &$0.000$&{\color{red}$0.000$}&$75.46$&$0.000$&$258.24$\\
%         &&&&Cost &1.000 &$0.000$&$0.000$&{\color{lightgray}$0.000$}&$0.000$&{\color{teal}$0.000$}&65.97&$<0.001$&$249.36$\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$0.25$} & \multirow{2}{*}{$0.1$} & \multirow{2}{*}{$50$} & Constraint & {\color{brown}$0.798$} & {\color{brown}$0.202$} & $0.001$ & {\color{lightgray}$0.000$} &{\color{brown}$0.202$}&{\color{red}$0.000$}&$54.42$&$0.007$&$230.55$\\
%         &&&&Cost &{\color{brown}$0.956$}&{\color{brown}$0.044$}&$0.000$&{\color{lightgray}$0.000$}&{\color{brown}$0.043$}&{\color{cyan}$0.003$}&$50.18$&$0.006$&$208.89$\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$1.0$} & \multirow{2}{*}{$0.25$} & \multirow{2}{*}{$0$} & Constraint & $0.983$ & $0.000$ & $0.017$ & {\color{lightgray}$0.000$} &$0.000$&{\color{red}$0.000$}&$105.0$&$0.000$&{\color{orange}$486.86$}\\
%         &&&&Cost & $0.995$&$0.000$&$0.005$&{\color{lightgray}$0.000$}&$0.000$&{\color{teal}$0.000$}&$82.49$&$<0.001$&{\color{orange}$382.10$}\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$1.0$} & \multirow{2}{*}{$0.25$} & \multirow{2}{*}{$50$} & Constraint & {\color{brown}$0.709$} & {\color{brown}$0.291$} & $0.000$ & {\color{lightgray}$0.000$} &{\color{brown}$0.291$}&{\color{red}$0.000$}&$72.36$&$0.005$&{\color{orange}$360.38$}\\
%         &&&&Cost &{\color{brown}$0.923$}&{\color{brown}$0.076$}&$0.001$&{\color{lightgray}$0.000$}&{\color{brown}$0.072$}&{\color{cyan}$0.006$}&$59.21$&$0.008$&$297.25$\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$5.0$} & \multirow{2}{*}{$0.75$} & \multirow{2}{*}{$0$} & Constraint & $0.015$ &$0.000$&{\color{magenta}$0.985$}&{\color{lightgray}$0.000$}&$0.000$&{\color{red}$0.000$}&{\color{magenta}$141.22$}&$0.000$&{\color{orange}$1951.44$}\\
%         &&&&Cost &$0.490$&$0.000$&{\color{magenta}$0.510$}&{\color{lightgray}$0.000$}&$0.000$&{\color{teal}$0.000$}&{\color{magenta}$128.97$}&$<0.001$&{\color{orange}$1042.66$}\\
%         \hline \multirow{2}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{2}{*}{$5.0$} & \multirow{2}{*}{$0.75$} & \multirow{2}{*}{$50$} & Constraint &{\color{brown}$0.047$}&{\color{brown}$0.436$}&{\color{magenta}$0.875$}&{\color{lightgray}$0.000$}&{\color{brown}$0.436$}&{\color{red}$0.000$}&{\color{magenta}$114.52$}&$0.004$&{\color{orange}$1735.21$}\\
%         &&&&Cost &{\color{brown}$0.702$}&{\color{brown}$0.289$}&$0.013$&{\color{lightgray}$0.000$}&{\color{brown}$0.250$}&{\color{cyan}$0.067$}&$79.06$&$0.037$&{\color{orange}$768.56$}\\
%         % \hline \multirow{2}{*}{} & \multirow{2}{*}{$\infty$} & \multirow{2}{*}{--} & \multirow{2}{*}{$0$} & Constraint &  &  &  &  &&&&&\\
%         % &&&&Cost & &&&&&&&&\\
%         % \hline \multirow{2}{*}{} & \multirow{2}{*}{$\infty$} & \multirow{2}{*}{--} & \multirow{2}{*}{$50$} & Constraint &  &  &  &  &&&&&\\
%         % &&&&Cost & &&&&&&&&\\
%         \hline
%     \end{tabular}}
%     \vspace{-0.20in}
% \end{table*}

% During discrete search, we compute active DSHT violation cost term $\mJ_{team}$ for each state and minimize it as a part of our cost vector.
% An alternative way of ensuring teammate avoidance is using active DSHTs as constraints, and enforcing them by discarding any state that violates any of the hyperplanes during search.
% Using active DSHTs as constraints instead of a cost term \emph{ensures} teammate avoidance~\cite{senbaslar2022async}.
% However, we show that using DSHTs as constraints results in a conservative behavior when communication medium is imperfect, and using them as cost terms results in better performance in environments with dynamic obstacles or high message delays and drops. \looseness=-1

% In these experiments, we set static object density $\rho = 0$, and teammate safety enforcement duration $T^{team}_i = \infty$.
% The dynamic objects are not interactive, i.e., $\hat{f} = 0$.
% The number of robots is $\#^R = 16$.
% We control the average delay $\delta$, message drop probability $\kappa$, number of dynamic obstacles $\#^D$, and the safety enforcement strategy.
% The results are given in Table~\ref{Table:SafetyEnforcement}.\looseness=-1

% When the safety between teammates are enforced with constraints, robots never collide with each other as it can be seen in {\color{red}red} values in \textbf{t. coll. rate} column.
% This is the case because teammate constraint generation method we use makes sure that trajectories of each pair of robots share a constraining hyperplane at all times under asynchronous planning and communication imperfections.
% When these constraints are used as hard constraints throughout the trajectory, robots provably never collide~\cite{senbaslar2022async}.
% When teammate safety is enforced as a cost term, our algorithm can still achieve no collisions between teammates when there are no dynamic objects in the environment as it can be seen in {{\color{teal}teal} values in \textbf{t. coll. rate} column.
% However, when dynamic objects are present, teammates may collide with each other as it can be seen in {\color{cyan}cyan} values in \textbf{t. coll. rate}, since we prioritize dynamic obstacle avoidance to teammate avoidance during search.\looseness=-1

% As the average message delay and message drop rate increases, our algorithm becomes more and more conservative for teammate safety.
% When there are no dynamic obstacles in the environment or when the teammate safety is enforced as constraints, this causes conservative behavior, increasing average navigation duration as well as the deadlock rate as it can be seen in {\color{magenta}magenta} values in \textbf{deadl. rate} and \textbf{avg. nav. dur} columns.
% In addition, sizes of $\tilde{\mH}^{active}_i$ increases in these scenarios, causing the number of constraints to increase substiantially during trajectory optimization, slowing down the algorithm such that it is not real-time anymore as it can be seen in {\color{orange}orange} values in \textbf{avg. pl. dur.} column.\looseness=-1

% Even if enforcing teammate safety as constraints ensures collision avoidance between teammates, it results in a higher collision rate and a lower success rate compared to enforcing it with a cost term when dynamic objects are introduced to the environment as it can be seen in {\color{brown}brown} values in \textbf{succ. rate}, \textbf{coll. rate}, and \textbf{d. coll. rate.} columns.
% % The dynamic objects do not explicitly cooperate with teammates.
% Enforcing teammate safety using constraints causes robots to be conservative against teammates to ensure safety under asynchronous planning and imperfect communication.
% Since dynamic objects do not explicitly cooperate with the teammates, this causes teammates to collide with dynamic objects because the feasible set of plans is considerably smaller.
% Also, enforcing teammate safety using a cost term takes a lower planning duration as it can be seen in \textbf{avg. pl. dur.} column and robots reach to their goal positions faster as it can be seen in \textbf{avg. nav. dur.} column.\looseness=-1

% Therefore, using active DSHTs as a part of a cost term is a better choice as it results in a higher success rate, lower collision rate and lower navigation duration using less time.

\subsubsection{Imperfect Communication Medium}

\begin{table}
    \centering
    \caption{Effects of imperfect communication medium}
    \vspace{-0.05in}
    \label{Table:ImperfectCommunicationMedium}
    
    \resizebox{\linewidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
         \hline $\delta[s]$ & $0.0$ & $0.0$ & $0.25$ &$0.25$& $1.0$ & $1.0$ & $5.0$ & $5.0$   \\
         $\kappa$ & $0.0$ & $0.0$ & $0.1$ &$0.1$& $0.25$ & $0.25$ & $0.75$ & $0.75$   \\
         $\#^D$ & $0$ & $50$ & $0$ &$50$& $0$ & $50$ & $0$ & $50$   \\
         \hline 
         \textbf{succ. rate} & 1.000 & 0.952 & 1.000 & 0.956 & 0.995 & 0.923 & 0.490 & 0.702\\
         \textbf{coll. rate}& 0.000 & 0.048 & 0.000 & 0.044 & 0.000 & 0.076 & 0.000 & 0.289\\
         \textbf{deadl. rate} & 0.000 & 0.000 & 0.000 & 0.000 &0.005  &0.001 &{\color{magenta}0.510} & 0.013\\
         {\color{lightgray}\textbf{s. coll. rate}}& {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} \\
         \textbf{d. coll. rate}& 0.000 & 0.045 & 0.000 & 0.043 & 0.000 & 0.072& 0.000 & 0.250\\
          \textbf{t. coll. rate}& {\color{teal}0.000} & {\color{cyan}0.004} & {\color{teal}0.000} & {\color{cyan}0.003} & {\color{teal}0.000} & {\color{cyan}0.006} & {\color{teal}0.000} & 0.067\\
         \textbf{avg. nav. dur. [s]}& 60.45 & 45.35 & 65.97 & 50.18 & 82.49 & 59.21 & {\color{magenta}128.97} & 79.06\\
         \textbf{pl. fail rate}& $<$0.001 & 0.005 & $<$0.001 & 0.006 & $<$0.001 & 0.008 & $<$0.001 & 0.037\\
         \textbf{avg. pl. dur. [ms]}& 168.90 & 164.06 & 249.36 & {\color{red}208.89} & {\color{orange}382.10} & 297.25 & {\color{orange}1042.66} & {\color{orange}768.56}\\
         \hline
    \end{tabular}}
    \vspace{-0.2in}
\end{table}

We evaluate the performance of DREAM in multi-robot experiments with or without dynamic obstacles under different levels of communication imperfections. 
% During discrete search, we compute active DSHT violation cost term $\mJ_{team}$ for each state and minimize it as a part of our cost vector.
% An alternative way of ensuring teammate avoidance is using active DSHTs as constraints, and enforcing them by discarding any state that violates any of the hyperplanes during search.
% Using active DSHTs as constraints instead of a cost term \emph{ensures} teammate avoidance~\cite{senbaslar2022async}.
% However, we show that using DSHTs as constraints results in a conservative behavior when communication medium is imperfect, and using them as cost terms results in better performance in environments with dynamic obstacles or high message delays and drops. \looseness=-1
In these experiments, we set static object density $\rho = 0$, and teammate safety enforcement duration $T^{team}_i = \infty$, i.e., we enforce DSHTs for the full plans.
The obstacles are not interactive, i.e., $\hat{f} = 0$.
The number of robots is $\#^R = 16$.
We control the average delay $\delta$, message drop probability $\kappa$, and number of dynamic obstacles $\#^D$.
The results are given in Table~\ref{Table:ImperfectCommunicationMedium}.\looseness=-1

% When the safety between teammates are enforced with constraints, robots never collide with each other as it can be seen in {\color{red}red} values in \textbf{t. coll. rate} column.
% This is the case because teammate constraint generation method we use makes sure that trajectories of each pair of robots share a constraining hyperplane at all times under asynchronous planning and communication imperfections.
% When these constraints are used as hard constraints throughout the trajectory, robots provably never collide~\cite{senbaslar2022async}.
DREAM results in no collisions between teammates when there are no obstacles regardless of the imperfection amount of the communication medium as seen in {{\color{teal}teal} values.
However, when dynamic obstacles are present, teammates may collide with each other as it can be seen in {\color{cyan}cyan} values, since we prioritize dynamic obstacle avoidance to teammate avoidance.
As communication imperfections increase, DREAM becomes more and more conservative for teammate safety.
When there are no dynamic obstacles, this causes conservative behavior, increasing average navigation duration as well as the deadlock rate as it can be seen in {\color{magenta}magenta} values.
As cardinalities of $\tilde{\mH}^{active}_i$ increase in these scenarios, the number of constraints increase substantially in trajectory optimization, slowing down the algorithm such that it is not real-time anymore as it can be seen in {\color{orange}orange} values.
When obstacles are present, deadlocks occur less frequently as the primary objective becomes obstacle avoidance, causing robots to disperse in the environment, and hence, avoid teammates easily.\looseness=-1

% Even if enforcing teammate safety as constraints ensures collision avoidance between teammates, it results in a higher collision rate and a lower success rate compared to enforcing it with a cost term when dynamic objects are introduced to the environment as it can be seen in {\color{brown}brown} values in \textbf{succ. rate}, \textbf{coll. rate}, and \textbf{d. coll. rate.} columns.
% The dynamic objects do not explicitly cooperate with teammates.
% Enforcing teammate safety using constraints causes robots to be conservative against teammates to ensure safety under asynchronous planning and imperfect communication.
% Since dynamic objects do not explicitly cooperate with the teammates, this causes teammates to collide with dynamic objects because the feasible set of plans is considerably smaller.
% Also, enforcing teammate safety using a cost term takes a lower planning duration as it can be seen in \textbf{avg. pl. dur.} column and robots reach to their goal positions faster as it can be seen in \textbf{avg. nav. dur.} column.\looseness=-1

% Therefore, using active DSHTs as a part of a cost term is a better choice as it results in a higher success rate, lower collision rate and lower navigation duration using less time.


\subsubsection{Teammate Safety Enforcement Duration}\label{Section:TeammateSafetyEnforcementDurationEvaluation}

\begin{table}[t]
    \centering
    \caption{Effects of teammate safety duration}
    \vspace{-0.05in}
    \label{Table:SafetyEnforcementDuration}
    \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
         \hline $\boldsymbol{T_{team}}$ & $0.0$ & $0.5$ & $1.0$ & $1.5$ & $2.0$ & $2.5$\\
        \hline 
         \textbf{succ. rate} &0.300&0.748& {\color{red}0.964} &0.963&0.961&0.933\\ 
         \textbf{coll. rate} &0.700&0.252& 0.036&0.038&0.039&0.068\\ 
         \textbf{deadl. rate} &0.000&0.001& 0.000&0.000&0.000&0.000\\ 
         \textbf{s. coll. rate} &{\color{lightgray}0.000}&{\color{lightgray}0.000}&{\color{lightgray}0.000}&{\color{lightgray}0.000}&{\color{lightgray}0.000}&{\color{lightgray}0.000}\\ 
         \textbf{d. coll. rate} &0.068&0.098&0.036&0.038&0.036&0.058\\ 
         \textbf{t. coll. rate} &0.677&0.183&0.001&0.000&0.005&0.010\\ 
         \textbf{avg. nav. dur. [s]} &{\color{orange}27.57}&{\color{orange}30.55}&{\color{orange}37.60}&{\color{orange}44.71}&{\color{orange}51.01}&{\color{orange}49.38}\\ 
         \textbf{pl. fail rate} &0.011&0.025&0.008&0.005&0.005&0.006\\ 
         \textbf{avg. pl. dur. [ms]} &46.58&77.87&{\color{magenta}115.30}&153.47&195.93&205.35\\ 
         \hline
    \end{tabular}}
    \vspace{-0.2in}
\end{table}

% As we show it in the previous experiments, constraining for ensured teammate safety increases the collision rate because of the dynamic objects in the environment.
Enforcing teammate safety for the full trajectories causes planning to be not real-time and results in a high rate of deadlocks when communication imperfections are high. We investigate the effects of relaxing teammate constraints by controlling safety enforcement durations $T^{team}_i$.
In these experiments, we set $\rho = 0$, $\#^D=50$, $\hat{f}=0$, $\delta = \SI{0.25}{s}$, $\kappa = 0.1$, and $\#^R=16$.
Hence, these experiments can be compared with experiment $28$ in Table~\ref{Table:ImperfectCommunicationMedium}.
The results are given in Table~\ref{Table:SafetyEnforcementDuration}.\looseness=-1

The average navigation duration of the robots tends to increase as $T_i^{team}$ increases, because the planner becomes more and more conservative, as it can be seen in {\color{orange}orange} values.
% When teammate safety is not enforced, i.e. $T_{team}^i = 0$, success rate is $0.3$ as it is seen in {\color{brown}brown} value.
Setting $T_{team}^i=1.0$ results in the best success rate, $0.964$, as seen in the {\color{red}red} value.
The average planning duration decreases by $\approx 50\%$ when $T_{team}^i$ is set to $1.0$ compared to setting it $\infty$, greatly increasing the cases our algorithm is real-time as it can be seen in the {\color{magenta}magenta} value compared to the {\color{red}red} value in Table~\ref{Table:ImperfectCommunicationMedium}.
The success rate of $0.964$ is higher than that of experiment $28$ in Table~\ref{Table:ImperfectCommunicationMedium}, which is $0.956$.
In addition, the average navigation duration decreases to $37.60$ compared to $50.18$ in Table~\ref{Table:ImperfectCommunicationMedium}.
Relaxing safety with respect to teammates not only decreases planning and navigation durations but improves the success rate of the algorithm.\looseness=-1

In the remaining experiments, we set $T_i^{team} = 1.0$.

\subsection{Baseline Comparisons in Simulations}\label{Section:BaselineComparison}

% Many state-of-the-art decentralized multi-robot navigation decision making algorithms exist as discussed in Sec.~\ref{Section:RelatedWork}.
We summarize the collision avoidance and deadlock-free navigation comparisons between state-of-the-art decentralized navigation decision-making algorithms in Fig.~\ref{Figure:BaselineComparison}.
%where a directed edge means that the source algorithm is shown to be better than the destination algorithm in experiments with multiple robots, some of which containing static or dynamic obstacles and some simulating asynchronous planning and communication imperfections.
The comparison graph suggests that SBC~\cite{wang2017safety}, RLSS~\cite{senbaslar2023rlss}, RMADER~\cite{kondo2023rmader}, and TASC~\cite{toumieh2022tasc,toumieh2023tasc} are the best performing algorithms.
TASC and RMADER are shown to have a similar collision avoidance performance in~\cite{toumieh2023tasc}.
Therefore, we compare our algorithm DREAM to SBC, RLSS, and RMADER, and establish that it results in a better performance than them.
SBC is a short horizon algorithm: it computes the next safe acceleration to execute to drive the robot to its goal position each iteration.
RMADER, RLSS, and DREAM are medium horizon algorithms: they compute long trajectories, which they execute for a shorter duration in a receding horizon fashion.\looseness=-1

All of our baselines drive robots to their given goal positions.
We add support for desired trajectories by running our goal selection algorithm in every planning iteration and providing the selected goal position as intermediate goal positions.
% During our comparisons, we use \emph{with prior} strategy for all algorithms that the desired trajectories avoid the static obstacles in the environment.
\looseness=-1


% Figure environment removed

\subsubsection{Using RMADER in Comparisons}

RMADER is a real-time decentralized trajectory planning algorithm for static and dynamic obstacle and multi-robot collision avoidance.
It explicitly accounts for asynchronous planning between robots using communication and accounts for communication delays with known bounds.
It models dynamic obstacle movements using predicted trajectories.
It does not explicitly model robot--dynamic obstacle interactions.\looseness=-1
% It supports uncertainty associated with predicted trajectories using axis aligned boxes, which we call uncertainty boxes, such that it requires that the samples of the real trajectory dynamic obstacle is going to follow are contained in known bounding boxes around the samples of the predicted trajectory.\looseness=-1

Dynamic obstacles move according to movement and interaction models in our formulation.
We convert movement models to predicted trajectories by propagating dynamic obstacles' positions according to the desired velocities from the movement models.
Since the interactive behavior of dynamic obstacles depend on the trajectory that the robot is going to follow, which is computed by the planner, their effect on the future trajectories is unknown prior to planning.
Therefore, we do not use interactive obstacles during baseline comparisons.\looseness=-1

% When a dynamic obstacle is not interactive, e.g. repulsion strength is $0$ in a repulsive model, predicted trajectories are perfect if the decision making period of dynamic obstacles are known and movement model predictions are correct.

% RMADER enforces dynamic limits independently for each dimension similar to our planner.
% It creates outer polyhedral representations of dynamic obstacles' shapes inflated by the uncertainty boxes around the predicted trajectories, and avoids them during planning.
% Planning pipeline of MADER consists of a discrete search method followed by trajectory optimization similar to ours.
% The optimization problem is a non-linear program that is locally optimized using the discrete solution as initial guess.
% RMADER plans trajectories toward given goal positions.

We use the code of RMADER published by its authors~\cite{githubGitHubMitaclrmader} and integrate it to our simulation system.
% In RMADER's implementation, the robot is modeled as a sphere, while the algorithm can support any convex collision shape in theory.
% We implement our algorithm for box like collision shapes.
% To provide robot shapes we have to MADER, we compute the smallest spheres containing the boxes.
% In order not to cause MADER to be overly conservative, we change our robot shape randomization method to generate boxes that have the same sizes in all dimensions.
% We also sample robot sizes in interval $[\SI{0.1}{m}, \SI{0.2}{m}]$ instead of $[\SI{0.2}{m}, \SI{0.3}{m}]$ so that the robot can fit between static obstacles easily when its collision shape is modelled using bounding spheres. (Since resolution of octrees we generate is $\SI{0.5}{m}$, the smallest possible gap between static obstacles is $\SI{0.5}{m}$.) 
% The randomization of our runs are same as before except for these changes.
We set the desired maximum planning time of RMADER to $\SI{500}{ms}$ in each planning iteration, which it exceeds if needed, even when the simulated replanning period is smaller.
We freeze the environment until RMADER is done replanning to cancel the effects of exceeding the replanning period.
% Having lower limits on planning time, e.g. $\SI{350}{ms}$, decreases the performance of RMADER drastically ($\approx50\%$ more collision rate) in our experiments.
% Instead of generating $1000$ random environments for each experiment, we generate $250$ random environments while evaluating MADER because the MADER simulations are considerably slower than ours.

% \begin{table}[t]
%     \centering
%     \caption{Performance of MADER when it avoids most likely or all behavior models of dynamic obstacles.}
%     \label{Table:MADERAvoidAllOrNot}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{93} & \textbf{94} & \textbf{95} & \textbf{96} & \textbf{97} & \textbf{98}\\
%          \hline avoid & all & most l. & all & most l. & all & most l.  \\
%          $\rho$ & 0.1 & 0.1 & 0.2& 0.2 &0.2&0.2\\
%          $\#^D$ & 10 & 10 & 10& 10 & 25 & 25  \\
%          \hline 
%          \textbf{succ. rate} & 0.828 & 0.848 & 0.640 & 0.672 &0.352 & 0.380 \\
%          \textbf{coll. rate} & 0.160 & 0.112 & 0.200& 0.200 &0.472 & 0.520  \\
%          \textbf{deadl. rate} & 0.076 & 0.092 &0.276 & 0.256 &0.580 & 0.504 \\
%          \textbf{s. coll. rate} & 0.000 & 0.000 & 0.000& 0.000 &0.000 & 0.000  \\
%          \textbf{d. coll. rate} & 0.160 & 0.112 & 0.200 & 0.200 &0.472 & 0.520  \\
%          \textbf{avg. nav. dur. [s]} & 24.85 & 24.87 & 25.94& 25.99 &25.93 & 26.05 \\
%          \textbf{pl. fail rate} &  0.248 & 0.255 & 0.514& 0.486 &0.739 &  0.702  \\
%          \textbf{avg. pl. dur. [ms]}& 266.97 & 260.26 &398.53 & 387.06 &426.41 & 407.47  \\
%          \hline
%     \end{tabular}}
% \end{table}

% \paragraph{Avoiding All or Most Likely Behavior Models of Dynamic Obstacles with RMADER}\label{Section:RMADERAvoidAllOrNot}
Our prediction system generates three behavior models for each dynamic obstacle.
RMADER does not support multiple behavior hypotheses explicitly.
Therefore, it has the choice of avoiding the most likely or all behavior models of dynamic obstacles, modeling each behavior model as a separate obstacle.
We provide only the most likely behavior models to RMADER during evaluation because avoiding all behavior models resulted in highly conservative behavior.\looseness=-1
% Another option is to model the dynamic obstacle as a single obstacle and set the uncertainty box that it contains all predicted trajectories generated from each behavior model.
% This is an extremely conservative approach  because the predictions can be pointing to completely different directions in completely different speeds.
% Therefore, we do not evaluate this possibility.\looseness=-1

% To evaluate the performance of RMADER between these two choices, we run it in environments with different densities and compare the approaches in single-robot experiments.
% During these experiments, we set repulsion strength $\hat{f} = 0$ of dynamic obstacles.
% In addition, decision making period of dynamic obstacles are perfectly known, meaning that predicted trajectories are correct if predicted movement models are correct.
% We set the uncertainty box to a symmetric one with size $\SI{0.1}{m}$ in all dimensions to handle prediction inaccuracy of individual movement models.\looseness=-1

% \begin{table}[t]
%     \centering
%     \caption{Performance of RMADER when it avoids most likely or all behavior models of dynamic obstacles}
%     \label{Table:RMADERAvoidAllOrNot}
%     \vspace{-0.05in}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline avoid & all & most l. & all & most l. & all & most l.  \\
%          $\rho$ & 0.1 & 0.1 & 0.2& 0.2 &0.2&0.2\\
%          $\#^D$ & 10 & 10 & 10& 10 & 25 & 25  \\
%          \hline 
%          \textbf{succ. rate} & {\color{red}0.620} & {\color{cyan}0.644} & {\color{red}0.456} &  & {\color{red}0.244} & {\color{cyan}0.284}\\
%          \textbf{coll. rate} & 0.164 & 0.148 & 0.112 &  & 0.392 & 0.364\\
%          \textbf{deadl. rate} & 0.352 & 0.312 & 0.536 &  & 0.752 & 0.700\\
%          \textbf{s. coll. rate}  & 0.000 & 0.000 & 0.000 &  & 0.000 & 0.000\\
%          \textbf{d. coll. rate} & 0.164 & 0.148 & 0.112 &  & 0.392 & 0.364\\
%          \textbf{t. coll. rate}  & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000}\\
%          \textbf{avg. nav. dur. [s]}  &38.85 & 38.05 & 49.60 &  & 50.51 & 50.63\\
%          \textbf{pl. fail rate}  & 0.49 & 0.47 & 0.45 &  & 0.66 & 0.66\\
%          \textbf{avg. pl. dur. [ms]} & 424.31 & 418.82 & 803.79 &  & 772.14 & 844.01\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.1in}
% \end{table}

% The results are summarized in Table~\ref{Table:RMADERAvoidAllOrNot} for varying static obstacle density $\rho$ and number of dynamic obstacles $\#^D$.
% The success rates increase when RMADER avoids only the most likely behavior models of dynamic obstacles compared to avoiding all behavior models as it can be seen by comparing {\color{cyan}cyan} and {\color{red}red} values in \textbf{succ. rate} row.
% The conservatism added by avoiding all behavior models in order to increase safety has an inverse effect on the success rate because the feasible set of plans shrinks.\looseness=-1
% The success, collision, and deadlock rates in experiments $97$ and $98$ suggests that deadlocks and collisions occur in the same runs a lot.
% For instance, in experiment $84$, success rate is $0.380$, collision rate is $0.520$, and the deadlock rate is $0.504$, suggesting that $\approx 40\%$ of cases are cases in which robot deadlocks and collides, accounting for $\approx 78\%$ collision cases.
%We investigated the root causes of this issue by analyzing the behavior of MADER's official implementation while running the simulations and pinpointed the reasons.

% \begin{table}[t]
%     \centering
%     \caption{Performance of MADER when different sizes of prediction uncertainty boxes are used.}
%     \label{Table:MADERUncertaintyBox}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{85} & \textbf{86} & \textbf{87} & \textbf{88} & \textbf{89} & \textbf{90}\\
%          \hline uncer. b. size $[m]$ & $0.0$ &$0.2$& $0.5$ & $1.0$ & $2.0$ & $3.0$   \\
%          \hline 
%          \textbf{succ. rate} & 0.736 & 0.724 & 0.672 & 0.616 & 0.468 & 0.220\\
%          \textbf{coll. rate} & 0.248 & 0.236 & 0.272 & 0.276 & 0.268 & 0.296\\
%          \textbf{deadl. rate} & 0.124  & 0.152 & 0.188 & 0.284 & 0.452 & 0.724\\
%          \textbf{s. coll. rate} & 0.000 & 0.000 & 0.000  & 0.000 & 0.000 & 0.000 \\
%          \textbf{d. coll. rate} & 0.248 & 0.236 & 0.272  & 0.276 & 0.268 & 0.296\\
%          \textbf{avg. nav. dur. [s]} & 25.73 & 25.85 &26.04  & 26.99 & 28.38 & 30.01\\
%          \textbf{pl. fail rate} & 0.342 & 0.400 &  0.459 & 0.570 &  0.700 & 0.828\\
%          \textbf{avg. pl. dur. [ms]} & 388.50 & 384.76 & 390.22 & 390.69 & 383.81 & 395.57 \\
%          \hline
%     \end{tabular}}
% \end{table}

% \begin{table*}
%     \centering
%     \caption{Baseline comparison of our planner in single robot scenarios}
%     \label{Table:MADERVSOurs}
%     \resizebox{\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & $\rho$ & $\#^D$ & Alg. & \textbf{succ. rate} & \textbf{coll. rate} & \textbf{deadl. rate} & \textbf{s. coll. rate} & \textbf{d. coll. rate} & \textbf{avg. nav. dur. [s]} & \textbf{pl. fail rate} & \textbf{avg. pl. dur. [ms]}\\
%         \hline \multirow{2}{*}{\textbf{91}} & \multirow{2}{*}{$0.0$} & \multirow{2}{*}{$15$} & MADER & 0.980 & 0.020 & 0.008 & \textbf{0.000} & 0.020 &\textbf{24.10}&0.051&32.26\\
%         &&&Ours & \textbf{0.996} & \textbf{0.004} &\textbf{0.000}& \textbf{0.000}& \textbf{0.004}& 26.70& \textbf{0.009}& \textbf{21.05}\\
%         \hline \multirow{2}{*}{\textbf{92}} & \multirow{2}{*}{$0.1$} & \multirow{2}{*}{$15$} & MADER & 0.820 & 0.180 & 0.028 & \textbf{0.000} & 0.180 & \textbf{24.96} & 0.170 & 264.11\\
%         &&&Ours &\textbf{0.980}&\textbf{0.016}&\textbf{0.004}&\textbf{0.000}&\textbf{0.016}&27.41&\textbf{0.032}&\textbf{42.33}\\
%         \hline \multirow{2}{*}{\textbf{93}} & \multirow{2}{*}{$0.2$} & \multirow{2}{*}{$15$} & MADER &0.724&0.248&0.116&\textbf{0.000}&0.248&\textbf{25.86}&0.352&393.28\\
%         &&&Ours &\textbf{0.976}&\textbf{0.020}&\textbf{0.004}&0.004&\textbf{0.016}&28.28&\textbf{0.053}&\textbf{49.79}\\
%         \hline \multirow{2}{*}{\textbf{94}} & \multirow{2}{*}{$0.2$} & \multirow{2}{*}{$25$} & MADER &0.616&0.364&0.192&\textbf{0.000}&0.364&\textbf{25.76}&0.465&401.95\\
%         &&&Ours &\textbf{0.960}&\textbf{0.040}&\textbf{0.000}&\textbf{0.000}&\textbf{0.040}&28.24&\textbf{0.050}&\textbf{57.32}\\
%         \hline \multirow{2}{*}{\textbf{95}} & \multirow{2}{*}{$0.2$} & \multirow{2}{*}{$50$} & MADER &0.292&0.664&0.484&\textbf{0.000}&0.664&\textbf{25.82}&0.711&423.01\\
%         &&&Ours &\textbf{0.884}&\textbf{0.116}&\textbf{0.000}&0.008&\textbf{0.116}&28.32&\textbf{0.058}&\textbf{75.39}\\
%         \hline \multirow{2}{*}{\textbf{96}} & \multirow{2}{*}{$0.3$} & \multirow{2}{*}{$50$} & MADER &0.228&0.732&0.620&\textbf{0.000}&0.732&\textbf{27.32}&0.776&491.27\\
%         &&&Ours &\textbf{0.868}&\textbf{0.124}&\textbf{0.016}&0.020&\textbf{0.120}&29.81&\textbf{0.079}&\textbf{86.28}\\
%         \hline
%     \end{tabular}}
% \end{table*}

% \textbf{Why does MADER cause deadlocks?}
% MADER embraces planning failures by planning trajectories that allows robot to stop by ensuring that the planned trajectory has $0$ higher order derivatives at their endpoints in each planning iteration.
% Since it ensures that the planned trajectory will be collision-free with respect to the obstacles in the environment, this allows for a safe stop.
% If a robot stops because of planning failures, we observed that it deadlocks as well if the cause of planning failures are static obstacles.
% The reason of these failures is MADER's search algorithm. 
% During search, they plan for a B-Spline polynomial with a fixed number of control points, say $n$, that obeys dynamic limits of the robot and is safe against obstacles.
% To tackle high computation times of a complete search method, they limit the run time of the search and return the best solution found if the time limit is hit.
% But, this solution might have less than $n$ control points if the search is stopped prematurely.
% They append the last control point of the best path until there are $n$ control points for the initially guessed B-Spline.
% However, since the newly appended control points are not computed during search, there is no guarantee that the resulting B-Spline will be dynamically feasible or safe.
% In the official code, they check for the dynamic feasibility and the safety of the final plan.
% The planning iteration fails if dynamic feasibility or safety is not satisfied.
% When a robot stops close to a static obstacle, and the reason of high search time is the static obstacle that is in the close vicinity of the robot, planning fails in each following iteration, causing the deadlock.
% The high rate of planning failures can also be observed in Table~\ref{Table:MADERAvoidAllOrNot}.

% \textbf{Why does MADER cause deadlocks and collisions together?}
% MADER has a concept of a planning hypersphere $\mS$, that it computes trajectories within.
% It projects the goal positions to the planning hypersphere, and plans trajectories to the projected goal positions within the hypersphere.
% This limits the decision making horizon of MADER in time domain.
% When the decision making horizon is limited, MADER does not consider dynamic obstacles that are far away during decision making.
% In the official implementation, they omit dynamic obstacles that are more than $4$ times the radius of the planning hypersphere away from the robot during planning.
% Safety is ensured against dynamic obstacles that within this distance in each planning iteration.
% Therefore, when a robot safely stops, it is safe against close obstacles but not necessarily safe against obstacles that are far away. 
% When a robot deadlocks and the dynamic obstacles that are far away gets eventually close to the robot, collision occurs.
% This is the main reason we found causing deadlocks and collision together.

% \textbf{Suggestions for tackling MADER deadlocks.}
% As the deadlocks are the root cause of many collisions, and appending control points for prematurely stopped search is the main cause of deadlocks, we suggest two principled approaches to replace appending control points logic.
% \begin{itemize}
% \item \textbf{Not appending control points.} Instead of requiring a fixed number of control points, the algorithm can be adapted to work with any number of control points.
% This requires changing the trajectory optimizer to be able to optimize trajectories with any number control points.
% When the discrete search stops prematurely, it will return a B-Spline that is dynamically feasible and safe, and the optimizer will optimize it without requiring a fixed number of control points.
% \item \textbf{Knot insertion.} B-Splines support a unique operation called knot insertion that allows increasing the number of control points without changing the shape or parameterization of the trajectory~\cite{piegl1996nurbs}.
% Knot insertion will allow increasing the number of control points without violating the safety constraints or dynamic limits.
% \end{itemize}
% We do not evaluate the performance of these suggestions as it is outside the scope of our paper.

% In the remaining experiments, we provide the predictions generated from most likely behavior models of dynamic obstacles to RMADER because it results in a better performance.\looseness=-1

% \paragraph{Setting Uncertainty Box with RMADER}\label{Section:RMADERUncertaintyBox}

% Next, we simulate the behavior of RMADER with interactive obstacles, and evaluate the performance of it when uncertainty boxes with different sizes are used in single-robot experiments.\looseness=-1

% We set $\rho = 0.2$, and $\#^D = 15$ during these experiments.
% Repulsion strength $\hat{f}$ is sampled uniformly in interval $[0.2, 0.5]$ and the dynamic obstacle decision making period is sampled uniformly in interval $[\SI{0.1}{s}, \SI{0.5}{s}]$.
% These create prediction inaccuracies for RMADER stemming from three main reasons: i) trajectories generated from individual movement models are inaccurate because the dynamic obstacle decision making periods are unknown, ii) movement models do not explain the behavior of the dynamic obstacles because of interactivity, and iii) predicted behavior models are inaccurate because of the inherent inaccuracy of predictors listed in Sec.~\ref{Section:Prediction}.\looseness=-1

% \begin{table}[t]
%     \centering
%     \caption{Performance of RMADER when different sizes of prediction uncertainty boxes are used.}
%     \label{Table:RMADERUncertaintyBox}
%     \vspace{-0.05in}
%     \resizebox{\columnwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|}
%         \hline \textbf{Exp. $\boldsymbol{\#}$} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}} & \textbf{\theexperimentcounter\stepcounter{experimentcounter}}\\
%          \hline uncer. b. size $[m]$ & $0.0$ &$0.2$& $0.5$ & $1.0$ & $2.0$ & $3.0$   \\
%          \hline 
%          \textbf{succ. rate} & {\color{cyan}0.356} & {\color{cyan}0.336} & {\color{cyan}0.344} & {\color{cyan}0.224} & {\color{cyan}0.160} & {\color{cyan}0.088}\\
%          \textbf{coll. rate}  & {\color{red}0.216} & 0.240 & 0.224 & 0.196 & 0.268 & {\color{red}0.268}\\
%          \textbf{deadl. rate} & {\color{red}0.600} & 0.632 & 0.624 & 0.736 & 0.820 & {\color{red}0.896}\\
%          \textbf{s. coll. rate}  & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000\\
%          \textbf{d. coll. rate}  & 0.216 & 0.240 & 0.224 & 0.196 & 0.268 & 0.268\\
%          \textbf{t. coll. rate}  & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000} & {\color{lightgray}0.000}\\
%          \textbf{avg. nav. dur. [s]}  & 50.93 & 47.49 & 51.72 & 50.83 & 51.9775 & 55.10\\
%          \textbf{pl. fail rate}  & 0.59 & 0.61 & 0.61 & 0.65 & 0.71 & 0.77\\
%          \textbf{avg. pl. dur. [ms]}  & 895.91 & 895.14 & 895.98 & 906.55 & 875.33 & 862.13\\
%          \hline
%     \end{tabular}}
%     \vspace{-0.1in}
% \end{table}

% We set uncertainty boxes to axis aligned boxes with sizes from $\SI{0.0}{m}$ to $\SI{3.0}{m}$ in each axis. 
% The results are given in ~\ref{Table:RMADERUncertaintyBox}.
% The success rates decrease as the size of the uncertainty box increases ({\color{cyan}cyan} values).
% The increase in collision rate is smaller compared to the increase in deadlock rate ({\color{red}red} values).
% As the uncertainty box size increases, avoiding dynamic obstacles become harder, causing robot to divert from the desired trajectory more.
% As the robot diverts from the desired trajectory, the likelihood that it deadlocks because of static obstacles increases.
% Therefore, even if the uncertainty box handles uncertainty associated with dynamic obstacle predictions, the fact that RMADER is prone to deadlocks reverts the benefits of having uncertainty box.\looseness=-1

% In the remaining experiments, we set the uncertainty box size to $\SI{0.0}{m}$ for RMADER because it results in a better performance.\looseness=-1

\subsubsection{Using RLSS in Comparisons}

RLSS is a real-time decentralized trajectory planning algorithm for static obstacles and multi-robot collision avoidance.
It does not account for asynchronous planning between teammates.
It does not utilize communication and depends on position/shape sensing only.
When using RLSS in comparisons, we model dynamic obstacles as robots and provide their current positions and shapes to the planning algorithm.
% We integrate our goal selector to RLSS to provide intermediate goal positions.
We use our own implementation of RLSS during our comparisons.\looseness=-1

\subsubsection{Using SBC in Comparisons}

SBC is a safety barrier certificates-based safe controller for static and dynamic obstacle and multi-robot collision avoidance.
SBC runs at a high frequency ($>\SI{50}{Hz}$), therefore it does not need to account for asynchronous planning.
It does not utilize communication and depends on position, velocity, and shape sensing.
When simulating SBC, we do not use the predicted behavior models and feed the current positions and velocities of the dynamic obstacles to the algorithm, which assumes that the dynamic obstacles execute the same velocity for the short future ($<\SI{20}{ms}$).\looseness=-1

SBC models shapes of obstacles and robots as hyperspheres.
We provide shapes of objects to SBC as the smallest hyperspheres containing the axis-aligned boxes.
We sample robot sizes in the interval $[\SI{0.1}{m}, \SI{0.2}{m}]$ instead of $[\SI{0.2}{m}, \SI{0.3}{m}]$ in SBC runs so that the robot can fit between static obstacles easily when its collision shape is modeled using bounding hyperspheres. (Since the resolution of octrees we generate is $\SI{0.5}{m}$, the smallest possible gap between static obstacles is $\SI{0.5}{m}$.)
% We integrate our goal selector to SBC to provide intermediate goal positions.
We use our own implementation of SBC.\looseness=-1

\subsubsection{Single Robot Experiments}


\begin{table*}
    \centering
    \caption{Baseline comparisons in single robot scenarios}
    \vspace{-0.05in}
    \label{Table:SingleRobotComparisons}
    \resizebox{\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & $\rho$ & $\#^D$ & Alg. & \textbf{succ. rate} & \textbf{coll. rate} & \textbf{deadl. rate} & \textbf{s. coll. rate} & \textbf{d. coll. rate} & \textbf{t. coll. rate} & \textbf{avg. nav. dur. [s]} & \textbf{pl. fail rate} & \textbf{avg. pl. dur. [ms]}\\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.0$} & \multirow{4}{*}{$15$} & SBC & {\color{red}0.992} & 0.008 & 0.000 & {\color{lightgray}0.000} & 0.008 & {\color{lightgray}0.000} & 33.96 & 0.001 & 0.26\\
        &&&RMADER & 0.960 & 0.036 & 0.008 & {\color{lightgray}0.000} & 0.036  &  {\color{lightgray}0.000} & 25.31 & 0.100 & 19.24\\
        &&&RLSS & 0.776 & 0.224 & 0.012 & {\color{lightgray}0.000} & 0.224 & {\color{lightgray}0.000} & 26.80 & 0.052 & 6.69\\
        &&&DREAM & 0.984 & 0.016 & {\color{Dandelion}0.000} & {\color{lightgray}0.000} & 0.016 & {\color{lightgray}0.000} & 26.67 & 0.008 & 13.93\\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.1$} & \multirow{4}{*}{$15$} & 
        SBC & {\color{red}0.352} & 0.648 & {\color{ForestGreen}0.236} & {\color{cyan}0.648} & {\color{orange}0.004} & {\color{lightgray}0.000} & 34.46 & 0.006 & 0.73\\
        &&&RMADER & 0.756 &  0.096 &  {\color{ForestGreen}0.212} & {\color{magenta}0.000} & 0.096 & {\color{lightgray}0.000} & 38.17 & {\color{brown}0.313} & 341.14\\
        &&&RLSS & 0.816 & 0.184 & 0.008  & {\color{magenta}0.000} &  0.184 & {\color{lightgray}0.000} & 27.26 & 0.034 & 28.37\\
        &&&DREAM & 0.984 & 0.016 & {\color{Dandelion}0.000} & {\color{magenta}0.000} &0.016 & {\color{lightgray}0.000} & 27.47 & 0.025 &  21.09\\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.2$} & \multirow{4}{*}{$15$} & SBC & 0.072 & 0.928 & {\color{ForestGreen}0.564} & {\color{cyan}0.928} & {\color{orange}0.044}  & {\color{lightgray}0.000} & 35.25 & 0.011 & 1.47\\
        &&&RMADER&  0.456 & 0.204 & {\color{ForestGreen}0.516} & {\color{magenta}0.000} & 0.204 & {\color{lightgray}0.000} & 49.74 & {\color{brown}0.470} & 769.83\\
        &&&RLSS & 0.780 & 0.204 & 0.036 & {\color{magenta}0.000} & 0.204 &  {\color{lightgray}0.000}  &  28.36 & 0.088 & 48.07\\
        &&&DREAM & 0.988 & 0.012  & {\color{Dandelion}0.000} & {\color{magenta}0.000} & 0.012 & {\color{lightgray}0.000}  & 28.31 & 0.043 & 25.56\\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.2$} & \multirow{4}{*}{$25$} & SBC & 0.080 & 0.920 & {\color{ForestGreen}0.560}  & {\color{cyan}0.920}  & {\color{orange}0.068} & {\color{lightgray}0.000} & 37.09 & 0.019 & 1.47\\
        &&&RMADER & 0.400 & 0.264 & {\color{ForestGreen}0.568} & {\color{magenta}0.000} & 0.264 &  {\color{lightgray}0.000} &  50.77 & {\color{brown}0.535} & 751.45\\
        &&&RLSS & 0.752 & 0.228 & 0.044 & {\color{magenta}0.000} & 0.228 & {\color{lightgray}0.000}  & 28.66 & 0.095 & 35.59 \\
        &&&DREAM & 0.956 & 0.040  & {\color{Dandelion}0.008} & {\color{magenta}0.000} & 0.040 & {\color{lightgray}0.000} & 28.28 & 0.059 & 31.54\\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.2$} & \multirow{4}{*}{$50$} & SBC & 0.056 & 0.944 & {\color{ForestGreen}0.556} & {\color{cyan}0.944} & {\color{orange}0.144} &  {\color{lightgray}0.000} & 42.23 &  0.032 & 1.42\\
        &&&RMADER& 0.116 & 0.632 & {\color{ForestGreen}0.844} &  {\color{magenta}0.000} & 0.632 & {\color{lightgray}0.000}   & 49.79 & {\color{brown}0.755} & 890.89\\
        &&&RLSS & 0.536 & 0.448 & 0.064 & {\color{magenta}0.000} & 0.448 & {\color{lightgray}0.000}& 29.33 & 0.151 & 52.93\\
        &&&DREAM & 0.900 & 0.100 & {\color{Dandelion}0.008} & {\color{magenta}0.000} & 0.100 & {\color{lightgray}0.000} & 28.46 & 0.073 & 47.02 \\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.3$} & \multirow{4}{*}{$50$} & SBC & {\color{RedViolet}0.008} & 0.992 & {\color{ForestGreen}0.788} & {\color{cyan}0.992} & {\color{orange}0.212} & {\color{lightgray}0.000}  & 48.75 & 0.041 & 1.92\\
        &&&RMADER & {\color{RedViolet}0.092} & 0.536 & {\color{ForestGreen}0.904} & {\color{magenta}0.000} & 0.536 & {\color{lightgray}0.000} & 56.68 & {\color{brown}0.745} & 1217.24\\
        &&&RLSS & {\color{RedViolet}0.536} & 0.436 & {\color{RoyalPurple}0.160} & {\color{magenta}0.000} & 0.436 & {\color{lightgray}0.000} &  31.28 & 0.237 & 171.81\\
        &&&DREAM & {\color{RedViolet}0.884} & 0.116 & {\color{Dandelion}0.000} & {\color{magenta}0.000} & 0.116 & {\color{lightgray}0.000} & 30.25 & 0.080 & 48.10\\
        \hline
    \end{tabular}}
    \vspace{-0.1in}
\end{table*}

We compare DREAM with the baselines in environments with different static obstacle densities $\rho$ and number of dynamic obstacles $\#^D$ in single-robot experiments. The results are summarized in Table~\ref{Table:SingleRobotComparisons}.\looseness=-1

% During these experiments, the repulsion strength $f$ of dynamic obstacles is sampled in interval $[0.2, 0.5]$ uniformly, and the decision making period of dynamic obstacles are sampled in interval $[\SI{0.1}{s}, \SI{0.5}{s}]$ uniformly.

SBC's performance decreases sharply when static obstacles are introduced to the environment as can be seen in {\color{red}red} values in \textbf{succ. rate} column.
SBC mainly suffers from collisions with static obstacles compared to dynamic obstacles ({\color{cyan}cyan} vs {{\color{orange}orange} values).
All medium horizon algorithms can avoid static obstacles perfectly ({\color{magenta}magenta} values).\looseness=-1

SBC and RMADER result in a high deadlock rate as it can be seen in {\color{ForestGreen}green} values.
The reason SBC results in a high ratio of deadlocks is its short horizon decision-making setup.
Since it does not consider the longer horizon effects of the generated actions, as the environment density increases, it tends to deadlock.
RMADER results in a high ratio of planning failures as the density increases, as can be seen in {\color{brown}brown} values in the \textbf{pl. fail rate} column, which causes it to consume the generated plans and not be able to generate new ones, which results in deadlocks.
RLSS has better deadlock avoidance compared to SBC and RMADER, but it too results in deadlocks as the environment density increases, as can be seen in {\color{RoyalPurple}purple} value in \textbf{deadl. rate} column.
DREAM results in little to no deadlocks ({\color{Dandelion}yellow} values).\looseness=-1

In terms of the success rate, DREAM considerably improves the state-of-the-art, resulting in $\approx$110x improvement over SBC, $\approx$9.6x improvement over RMADER, and $\approx$1.65x improvement over RLSS in the hardest scenario ({\color{RedViolet}violet} values in \textbf{succ. rate} column}).\looseness=-1

% In general, our algorithm results in a considerably higher success rate, lower collision, deadlock, planning failure rates, and a lower average planning duration.
% MADER results in a lower navigation duration compared to our planner.
% We attribute the reason of this to our planner's behavior during the tail end of the navigation.
% When the robot is close to the goal position, the planning horizon $\tau'$ of the search is set to the minimum search horizon parameter $\tilde{\tau}$, which is set to $\SI{2.0}{s}$ in our experiments. 
% Therefore, the search always aims to reach to the goal position within at least $\SI{2.0}{s}$, causing the robot to slow down during the tail end of the navigation.
% MADER results in $0$ static obstacle collision rates in all cases, because it never generates a trajectory that collides with static obstacles.
% Since it also plans for stopping at the end of the trajectory, robot is either following a safe trajectory with respect to static obstacles or is stopped.
% On the other hand, our planner occasionally results in collisions with static obstacles because we have no such property.
% As we have discussed in Sec.~\ref{Section:RMADERAvoidAllOrNot}, MADER is prone to deadlocks and collisions against dynamic obstacles.

\subsubsection{Multi Robot Experiments}


\begin{table*}
    \centering
    \caption{Baseline comparisons in multi robot scenarios with $\delta=\SI{1}{s}$ average delay and $\kappa=0.25$ message drop probability for our algorithm and $\delta=\SI{1}{s}$ maximum delay and no message drops for RMADER. SBC and RLSS do not depend on communication.}
    \vspace{-0.05in}
    \label{Table:MultiRobotComparisons}
    \resizebox{\linewidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
        \hline \textbf{Exp. $\boldsymbol{\#}$} & $\rho$ & $\#^D$ & $\#^R$ & Alg. & \textbf{succ. rate} & \textbf{coll. rate} & \textbf{deadl. rate} & \textbf{s. coll. rate} & \textbf{d. coll. rate} & \textbf{t. coll. rate} & \textbf{avg. nav. dur. [s]} & \textbf{pl. fail rate} & \textbf{avg. pl. dur. [ms]}\\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.2$} & \multirow{4}{*}{$25$} & \multirow{4}{*}{$16$} & SBC  & {\color{red}0.059} & 0.941 & 0.483 & 0.941 & 0.126 & 0.004 & 38.82  & 0.021   & 1.58\\
        &&&&RMADER& 0.116 & 0.551 & 0.870 & {\color{orange}0.000} & 0.551 & 0.001 & 60.90 & {\color{magenta}0.932} & 813.71 \\
        &&&&RLSS & 0.488 & 0.508 & 0.039 & {\color{orange}0.000} & 0.428 & {\color{brown}0.134} & 34.95 & 0.259 & 97.78 \\
        &&&&DREAM & \textbf{0.960} & 0.036 & 0.005 & 0.003 & {\color{RedViolet}0.035} & 0.000 & 40.77 &  0.014 & 123.68 \\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.2$} & \multirow{4}{*}{$25$}& \multirow{4}{*}{$32$} & 
        SBC & {\color{red}0.061} & 0.938 & 0.450 & 0.937 & 0.121 & 0.003 & 39.13 & 0.018 & 1.62 \\
        &&&&RMADER & 0.112 & 0.520 & 0.868 & {\color{orange}$<$ 0.001} & 0.514 & 0.008 &  62.82 & {\color{magenta}0.923} & 912.37\\
        &&&&RLSS & 0.471 & 0.521 & 0.059 & {\color{orange}0.000} & 0.373 & {\color{brown}0.225} & 35.53 & 0.354  & 90.16\\
        &&&&DREAM & \textbf{0.841} & 0.097 & 0.069 & 0.006 & {\color{RedViolet}0.074} & 0.026 & 60.31  & 0.014 & 185.87\\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.3$} & \multirow{4}{*}{$25$}& \multirow{4}{*}{$16$} & SBC & {\color{red}0.006} & 0.994 & 0.691 & 0.994 &  0.184 & 0.001 & 42.68 & 0.028 & 1.78\\
        &&&&RMADER & 0.054 & 0.534 & {\color{cyan}0.935} & {\color{orange}0.000} & 0.533 & 0.003 & 68.67  & {\color{magenta}0.952} & 1457.58\\
        &&&&RLSS & 0.591 & 0.396 & 0.056 & {\color{orange}0.000} & 0.332 & {\color{brown}0.093} & 38.47 & 0.324  & 300.33 \\
        &&&&DREAM& \textbf{0.941} & 0.051 & 0.011 & 0.005 & {\color{RedViolet}0.044} & 0.003  & 42.53 & 0.020  & 120.33 \\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.3$} & \multirow{4}{*}{$25$}& \multirow{4}{*}{$32$} & SBC & {\color{red}0.006} & 0.993 & 0.669 & 0.993 & 0.191 & 0.006 & 40.40 & 0.031 & 1.79 \\
        &&&&RMADER & 0.056  & 0.484 & {\color{cyan}0.934} & {\color{orange}0.000} & 0.481 & 0.006 & 64.39   & {\color{magenta}0.889} & 1467.52\\
        &&&&RLSS & 0.503 & 0.486 & 0.064 & {\color{orange}0.000} & 0.343 & {\color{brown}0.209} & 39.10 & 0.440  & 282.33 \\
        &&&&DREAM & \textbf{0.782} & 0.108 & 0.127 & 0.014 & {\color{RedViolet}0.081} & 0.025 & 58.04 & 0.019  & 178.71 \\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.3$} & \multirow{4}{*}{$50$}& \multirow{4}{*}{$16$} & SBC & {\color{red}0.006} & 0.994 & 0.636 & 0.993 & 0.318 & 0.003 & 47.60 & 0.046  & 1.96\\
        &&&&RMADER & 0.041 & 0.673 & {\color{cyan}0.956} & {\color{orange}0.000} & 0.673 & 0.001 &  62.97  & {\color{magenta}0.920} & 1315.97\\
        &&&&RLSS & 0.402 &  0.592 & 0.073 & {\color{orange}0.000} & 0.533 & {\color{brown}0.116} &  39.15 & 0.375   & 354.34\\
        &&&&DREAM & \textbf{0.864} & 0.128 & 0.012 & 0.006 & {\color{RedViolet}0.122} & 0.008 & 43.30 & 0.023   & 128.87 \\
        \hline \multirow{4}{*}{\textbf{\theexperimentcounter\stepcounter{experimentcounter}}} & \multirow{4}{*}{$0.3$} & \multirow{4}{*}{$50$}& \multirow{4}{*}{$32$} & SBC & {\color{red}0.006} & 0.993 & 0.646 & 0.992 & 0.348 & 0.011 &  44.82 & 0.050 & 2.01\\
        &&&&RMADER & 0.022 & 0.701 & {\color{cyan}0.974} & {\color{orange}$<$0.001} & 0.699 & 0.005 & 63.12   & {\color{magenta}0.916}  &1109.09 \\
        &&&&RLSS & 0.357 & 0.634 & 0.098 & {\color{orange}0.000} & 0.540 & {\color{brown}0.197} & 39.77 & 0.475 & 325.69\\
        &&&&DREAM & \textbf{0.712} & 0.216 & 0.093 & 0.018 & {\color{RedViolet}0.183} &  0.046 & 59.46 & 0.028   & 189.06 \\
        \hline
    \end{tabular}}
    \vspace{-0.2in}
\end{table*}

We compare DREAM with the baselines in highly cluttered environments with different $\rho$, $\#^D$ and $\#^R$.
During these experiments, we simulate communication imperfections.
SBC and RLSS do not depend on communication and hence communication imperfections do not affect them.
RMADER accounts for message delays with known bounds.
DREAM accounts for message delays with unknown bounds as well as message drops.\looseness=-1

For DREAM, we introduce mean communication delay $\delta = {1}{s}$ and message drop probability $\kappa=0.25$.
Since RMADER does not account for message drops, we set $\kappa = 0.0$ for RMADER.
In addition, in RMADER, we bound communication delays with the mean $\delta$ by generating a random sample from the distribution and setting it to $\delta$ if it is more than $\delta$.
Therefore, DREAM runs in considerably more challenging environments during these experiments compared to RMADER.
RMADER has a \emph{delay check} phase to account for communication delays, which should run for at least the maximum communication delay. We set its duration to $\SI{1.1}{s}$.
The environments used are more challenging compared to single-robot experiments, as not only $\rho$ and $\#^D$ are high, but also multiple teammates navigate under asynchronous decision making and considerable communication imperfections.\looseness=-1

The results are summarized in Table~\ref{Table:MultiRobotComparisons}.
SBC is ineffective for navigating in environments with high clutter ({\color{red}red} values in \textbf{succ. rate} column).
Since the communication imperfections are high, RMADER results in conservative behavior, resulting in deadlocks.
Given that it already results in a considerable rate of deadlocks in single-robot scenarios, almost all robots using RMADER deadlock in the hardest scenarios ({\color{cyan}cyan} values in \textbf{deadl. rate} column).
The high planning failure rate of RMADER ({\color{magenta}magenta} values in \textbf{pl. fail rate} column) is the main cause of the deadlocks: once plans are consumed and planning fails, it keeps failing until the end of the simulation.\looseness=-1

Both RLSS and RMADER result in no collisions with static obstacles as they i) avoid static obstacles using hard constraints unlike DREAM, and ii) enforce static obstacle avoidance for the full horizon unlike SBC ({\color{orange}orange} values in \textbf{s. coll. rate} column).
DREAM results in the lowest dynamic obstacle avoidance rate ({\color{RedViolet}violet} values in \textbf{d. coll. rate} column).
RLSS results in high teammate collisions, because it is the only algorithm that is affected by asynchronous planning but does not account for it ({\color{brown}brown} values in \textbf{t. coll. rate} column).\looseness=-1

In terms of success rate, DREAM considerably improves the state-of-the-art, resulting in $\approx$156.8x improvement over SBC, $\approx$32.36x improvement over RMADER, $\approx$2.15x improvement over RLSS (\textbf{bold} values in \textbf{succ. rate} column).\looseness=-1

\subsection{Physical Robot Experiments}

% Figure environment removed


We implement and run DREAM for Crazyflie 2.1 quadrotors.
We use quadrotors as i) dynamic obstacles moving according to goal attractive, rotating, or constant velocity movement models and repulsive interaction model, ii) static obstacles, and iii) teammates navigating to their goal positions using our planner.
Each planning quadrotor runs the predictors in real time to generate a probability distribution over behavior models of each dynamic obstacle.
Then, it runs our planner to compute trajectories in real time.\looseness=-1

For obstacle and robot localization, we use VICON motion tracking system, and we manage the Crazyflies using Crazyswarm~\cite{preiss2017crazyswarm}.
We use Robot Operating System (ROS)~\cite{quigley09ros} as the underlying software system.
Predictors and our algorithm run on a separate process for each robot in a base station computer with Intel(R) Xeon(R) CPU E5-2630 v4 @2.20GHz CPU, running Ubuntu 20.04 as the operating system.\looseness=-1

Pictures from the physical experiments can be seen in Fig.~\ref{Figure:PhysicalExperiments}.
The recordings from our physical experiments can be found in our supplemental video.
Our physical experiments show the feasibility of running DREAM in real-time in the real world.
\looseness=-1


\section{Conclusion}
We present DREAM--a decentralized multi-robot real-time trajectory planning algorithm for mobile robots navigating in environments with static and interactive dynamic obstacles.
DREAM explicitly minimizes the probabilities of collision with static and dynamic obstacles and violations of discretized separating hyperplane trajectories with respect to teammates as well as distance, duration, and rotations using a multi-objective search method; and energy usage during optimization.
The behavior of dynamic obstacles is modeled using two vector fields, namely movement and interaction models.
% Movement models describe the intentions of the dynamic obstacles, while the interaction models describe the interaction of dynamic obstacles with the robot.
DREAM simulates the behaviors of dynamic obstacles during decision-making in response to the actions the planning robot is going to take using the interaction models.
We present three online model-based prediction algorithms to predict the behavior of dynamic obstacles and assign probabilities to them.
We extensively evaluate DREAM in different environments and configurations and compare with three state-of-the-art decentralized real-time multi-robot navigation decision-making methods. DREAM considerably improves the state-of-the-art, achieving up to 1.68x success rate using as low as 0.28x time in single-robot, and up to 2.15x success rate using as low as 0.36x time in multi-robot experiments compared to the best baseline.
We show its feasibility in the real-world by implementing and running it for quadrotors.\looseness=-1

Future work includes generalizing the search stage to other state/action spaces and modeling inter-dynamic obstacle interactions during decision-making.\looseness=-1


% \section*{Acknowledgement}
\bibliographystyle{IEEEtran}
\bibliography{bibliography}% common bib file


% \begin{IEEEbiography}[{% Figure removed}]{Bask{\i}n \c{S}enba\c{s}lar} received his B.S. degree in Computer Engineering in 2017 from Middle East Technical University, Ankara, Turkey. He received his M.S. degree in Computer Science in 2019 from University of Southern California, Los Angeles, USA as a Fulbright Grantee. In 2019, he started his PhD. in University of Southern California as a USC Annenberg Fellow. Currently, he is a member of the Robotic Embedded Systems Laboratory (RESL). His research interests include multi-robot aware control, trajectory planning and navigation pipelines. \end{IEEEbiography}

% \begin{IEEEbiography}[{% Figure removed}]{Gaurav S. Sukhatme} is Professor of Computer
% Science and Electrical Engineering and Gordon S.
% Marshall Chair in Engineering at the Viterbi School
% of Engineering at the University of Southern California (USC). He received his undergraduate education
% at IIT Bombay in Computer Science and Engineering, and M.S. and Ph.D. degrees in Computer
% Science from USC. He is the co-director of the USC
% Robotics Research Laboratory and the director of
% the USC Robotic Embedded Systems Laboratory
% which he founded in 2000. He is the Editor-in-Chief of Autonomous Robots and has served as
% Associate Editor of the IEEE Transactions on Robotics and Automation, the
% IEEE Transactions on Mobile Computing, and on the editorial board of IEEE
% Pervasive Computing. \end{IEEEbiography}

\end{document}


