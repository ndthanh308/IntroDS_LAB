\section{Introduction}\label{sec:intro}
\subsection{Motivation}
Many Internet applications are running on cloud environments and generating large-scale data, 
e.g., Facebook \cite{facebook}, Twitter \cite{twitter} and Google \cite{google}. For these Internet applications, analyzing high 
volume of data is one of the most important workloads. For example, Facebook and Twitter analyze users’ posts, 
users’ activity logs, systems’ logs to query trends, make advertising decisions, and check overall cluster health.
Since the results of data analytics queries are usually used for making important decisions 
that affect revenues and system health, the queries must be processed promptly without a performance bottleneck.

To meet the performance goals, data analytics systems may deploy \textit{redundant} compute resources,
e.g., virtual machines (VMs), \textit{a prior}.
While this approach is simple and works well, this will incur additional cost (\$) for idle VMs. 
To avoid cost for unused compute resources, many previous 
works \cite{cherrypick, vanir, selecta, optimuscloud, optimus, ernest, paris, juggler, rpss, crystalLp} 
focused on determining optimal configurations, e.g., the number of VM instances and their types, and storage types, 
by predicting required compute resources for workloads. With these systems, additional VMs can be deployed to 
handle incoming queries without the performance bottleneck and idle VMs can be terminated to reduce cost based on workloads,
i.e., scalable data analytics systems. These systems, however, may not handle the \textit{latency-sensitive} 
queries promptly due to the unavoidable overhead of 
VM, i.e., boot-up latency ($>$ 55 seconds) \cite{coldBoot, coldBoot2}. 
If queries cause peak workload due to a lack of compute resources, 
they must wait until additional VM instances are fully deployed to be processed.

Many recent works \cite{spock, splitserve, occupy, flint, pocket, locus, numpywren, spark_on_lambda}
focused on adopting a newly emerging compute resource, \textit{serverless} (SL), such as Apache OpenWhisk \cite{openwhisk}, 
AWS Lambda \cite{lambdaRef}, Azure Functions \cite{azure_functions}, and Google Functions \cite {cloudFunctionsRef}, 
for data analytics to avoid the cold-boot latency problem, i.e., serverless data analytics (SDA).
Since SL offers \textit{agility}, very small boot-up time ($<$ 100 ms), and a \textit{pure-pay-as-you-go} cost model\footnote{Most popular cloud providers charge for SL only when the code is executed at either 1 millisecond (AWS) or 100 millisecond (GCP) granularity.}, SDA systems can deploy SL instances\footnote{We use the term serverless instances to refer serverless code invocations.} immediately and handle incoming 
queries without overprovisioned VMs in a cost-efficient way. %and thus they can avoid cost for idle VMs. 
These SDA systems, unfortunately, may still encounter cost- and performance bottlenecks based on data analytic workloads 
because SL offers worse performance and more expensive cost than VM \cite{cocoa,serverless_limitations,SLvsVMLee}.

%as shown in recent works \cite{cocoa,serverless_limitations,SLvsVMLee}.

\begin{table}
	\centering
	\caption{Comparison between SL and VM with the same amount of compute resources (2 vCPU with 2 GB RAM)}
	%	Note, the local disk storage cost is not included in VM but 		it is negligible as we will show in Section \ref{sec:heterogenous_resources}.}
	\scalebox{0.7} {
		\begin{tabular}{|c||c|c|}
			\hline
			& SL & VM \\
			\hline
			\hline
			Agility (Boot latency)& \textbf{High ($<$ 100 ms)}  & Low ($>$ 55 seconds) \\
			\hline
			Performance & Varying based on memory size &  \textbf{Relatively constant}\\
			\hline
			Cost Efficiency & \begin{tabular}{c}  \textbf{High (Pure pay-as-you-go :}\\\textbf{only when executed)}\end{tabular} & \begin{tabular}{c} Low (Pay-as-you-go \\: when deployed)\end{tabular}\\
			\hline
			Unit Time Cost (\$) & \begin{tabular}{c} Expensive (up to 5.8X) \end{tabular} &  \textbf{Cheaper}\\
			\hline
			%Limitations  & \begin{tabular}{c} Network, Memory\\ Storage, Code size  \end{tabular} & \begin{tabular}{c} Boot up overhead \\ (cold-boot latency) \end{tabular}\\
			%\hline
		\end{tabular}
	}
	%\end{footnotesize} 
	\label{tab:compare}
	%\end{wraptable}
	%\vspace{-4mm}
\end{table}

%Table \ref{tab:compare} shows the performance and cost comparisons between SL and VM,
Table \ref{tab:compare} shows the comparisons between SL and VM,
which represents different cost-performance points.
While data analytics systems may choose either one based on their resource demands and goals,
it would be highly desirable for them to achieve composite benefits (\textbf{bold} in Table \ref{tab:compare}), 
i.e., agility and cost-efficiency from SL and better performance and cheaper cost from VM.
However, determining compute resources configurations, e.g., how many SL and VM instances, 
is challenging due to the complexities: 1) heterogeneous compute resource characteristics,
2) workload prediction (how long a query will be executed), 3) diverse cost-performance goals,
and 4) dynamics from workloads.
While some recent works \cite{cocoa, splitserve, spock, libra, perf-cost-ratio, robustScaling} tried 
to exploit SL 
and VM together but they could not address these challenges as they have focused on 
either simple workload (independent tasks) or simple assumption without workload prediction.
Thus, they may not work well for data analytics.  

In this paper, we introduce \textit{Smartpick}, a serverless-enabled data analytics system (SEDA), 
that helps data analytics applications achieve desired cost-performance goals by addressing aforementioned challenges.
To determine \textit{optimal} cloud configurations of SL and VM, % that meet cost-performance goals, 
Smartpick uses a machine learning technique, decision-tree based Random Forest (RF) coupled with Bayesian Optimizer (BO), 
that predicts data analytic workloads using historical information.
%Since exploiting SL and VM opens a rich cost-performance tradeoff space due to heterogeneous characteristics, Smartpick provides a \textit{knob} that allows applications to easily explore the space. 
Smartpick provides a \textit{knob} that allows applications to easily explore the cost-performance tradeoff space opened by exploiting SL and VM together.
Smartpick supports a simple but strong mechanism called \textit{relay-instances} to further improve performance with reduced cost.
To handle workload dynamics, Smartpick uses an event-driven approach that triggers a model retraining task to automatically evolve 
prediction models.
%In addition, Smartpick uses an event-driven approach which triggers a model retraining task to automatically evolve 
%prediction models. % in background.\right) 

A Smartpick prototype implementation was built on the Spark \cite{sparkRef}, so that Spark applications can easily utilize
our system by setting diverse Smartpick's properties \textit{without any modification}.
We evaluated Smartpick on live-testbeds, Amazon AWS and Google Cloud Platform (GCP), using well-known benchmarks: TPC-DS \cite{tpcds}, Word Count \cite{wordCountInHive}, and TPC-H \cite{tpch}. 
Evaluations show that Smartpick can accurately characterize the TPC-DS workload performance with accuracies of 97.05\% on AWS and 83.49\% on GCP. 
%\textbf{KO: these numbers are different from the abstraction} \textcolor{red}{ADM: corrected!}
%The assessments show that using the relay-instances mechanism results in similar/better performance at up to 50\% of cost reduction.
The experimental results show that Smartpick can reduce cost by up to 50\% without performance degradation by using the relay-instances mechanism.
%We can also observe that Smartpick allows applications to easily explore the cost-performance tradespace with a simple knob. Lastly, results confirm the prediction model's fast and reliable convergence to completely unknown queries workloads.
%We can observe that Smartpick allows applications to easily explore the cost-performance tradespace with a simple knob. Lastly, results confirm the prediction model's fast and reliable convergence to completely unknown queries workloads.
The results also confirm that Smartpick allows applications to easily explore the richer cost-performance tradeoff space with a simple knob and to handle workload dynamics by retraining the prediction model automatically. 

%The enhanced version of \textit{Smartpick} with relay-instances (that is, \textit{Smartpick-r}) 
%\textcolor{blue}{Please proofread below as I revised.} 

\begin{table}
	\centering
	\caption{Feature comparison with state-of-the-art. $\triangle$ indicates that metric is
		considered but with limitations}
	%	Note, the local disk storage cost is not included in VM but 		it is negligible as we will show in Section \ref{sec:heterogenous_resources}.}
	\scalebox{0.74} {
		\begin{tabular}{|c|c|c||c|}
			\hline
			& Cocoa & SplitServe & Smartpick \\
			\hline
			\hline
			Exploiting SL \& VM & \cmark  & \cmark & \cmark \\
			\hline
			Workload Prediction &  &  & \cmark \\
			\hline
			Handling Dynamics &  &  & \cmark \\
			\hline
			Segueing (Relay-instances) &   & $\triangle$ & \cmark \\
			\hline
			Cost-performance Tradeoff & $\triangle$ &  & \cmark \\
			\hline
			%			Optimization & VM \& SL  &  & VM \& SL \\
			%			\hline
			%Limitations  & \begin{tabular}{c} Network, Memory\\ Storage, Code size  \end{tabular} & \begin{tabular}{c} Boot up overhead \\ (cold-boot latency) \end{tabular}\\
			%\hline
		\end{tabular}
	}
	%\end{footnotesize} 
	\label{tab:compareStateOfArt}
	%\end{wraptable}
	%\vspace{-4mm}
\end{table}

\subsection{Research Contributions}
%Table \ref{tab:compareStateOfArt} shows comparison of Smartpick approach to that of Cocoa \cite{cocoa} and SplitServe \cite{splitserve}. 
Table \ref{tab:compareStateOfArt} compares Smartpick approach to two recent SEDA systems, i.e., Cocoa \cite{cocoa} and SplitServe \cite{splitserve}. %, that expoited SL and VM together. 
%While all of the previous techniques consider hybrid model of VMs and SLs, none of them consider an in-house workload prediction system that ensures dynamics (such as, new workloads and change in data sizes) are properly integrated into the system. 
While these systems utilize both SL and VM, they do not predict queries' workloads but just rely on external workload prediction systems \cite{cherrypick, vanir, selecta, optimuscloud, optimus, ernest, paris, juggler, rpss, crystalLp}.
However, these prediction systems may not work well in SEDA due to their SL-agnostic approach and workload dynamics, which significantly affect overall cost and performance.
Thus, we designed the workload prediction module to easily work with any SEDA system that needs performance prediction.
%they do not predict queries' workloads, which significantly affect overall cost and performance. 
%In addition, the support for relaying of workloads from SLs to VMs through segueing (which is similar to relay in our work) is somewhat overlooked. That is, Cocoa \cite{cocoa} does not consider this, while SplitServe \cite{splitserve} partially considers it through a static approach. In contrary, Smartpick supports dynamic segueing (or relay) that leads to significantly reduced costs. 
%due to the characteristics of SL as shown in Table \ref{tab:compare}.
Since using SL for a long time would incur additional cost without performance improvement \cite{cocoa, splitserve},
Smartpick judiciously and dynamically terminates SL instances using the mechanism called \textit{relay-instances}. 
While SplitServe \cite{splitserve} uses a similar technique called segueing,
they use a static approach, which leads to significant cost inflation.
%Moreover, Smartpick incorporates the two important characteristics of Cocoa \cite{cocoa}, that is, (1) combined optimization of VMs and SLs for resource determination, and (2) efficient navigation of cost-performance tradeoff. Thus, not only does Smartpick help in combining/enhancing the benefits of Cocoa \cite{cocoa} and SplitServe \cite{splitserve}, but it also incorporates several key considerations through an in-house workload prediction system.
%Smartpick offers a simple knob for applications to explore the cost-performance tradeoff. 
%For the cost-performance tradeoff, Smartpick determines optimal compute resource configurations using workload prediction techniques and a simple knob.
%Cocoa also offers a similar feature, they highly rely on the external prediction systems that may not work well in SEDA
%allows applications explore the space but they highly rely on the external prediction systems that may not work well in SEDA.
While Cocoa considers exploring the cost-performance tradeoff space like Smartpick, 
its performance is highly dependent on several static parameters that may be hard to tune in SEDA.

%To summarize, the key contributions of this paper are as follows:
To summarize, the research contributions are as follows:
\begin{itemize}[wide = 0pt, topsep=0pt]
	\renewcommand{\labelitemi}{\noindent$\bullet$}
	\item The design and implementation of Smartpick, the first scalable data analytics system 
	(to the best of our knowledge) that predicts data analytics workloads with consideration of 
	SL and VM together to determine optimal compute resource configurations. 
	%\item A simple but Efficient techniques of relaying workload to VMs for better cost-performance trade-off.
	%\item Enhanced support of handling alien queries by utilizing the already trained model.
	%\item Mechanisms for adapting the trained model to coarse-grained dynamics that arise in the system.
	\item Flexibility that allows \textit{unmodified data analytics applications and other SEDA systems} to reap the benefits.
	\item A simple way to easily explore the cost-performance tradeoff space using diverse mechanisms embedded within the workload prediction.
	%    fine-tuning client applications to reap the benefits of different instance 
	%families spanning across multiple providers.
	\item Event-driven re-training of the prediction model to handle workload dynamics, e.g., varying data size and new queries.
	\item Thoughtful empirical evaluations on AWS \cite{awsRef} and GCP \cite{gcpRef}, showing the efficacy of Smartpick. 
	%    better performance, improved reliability and optimized cost with almost no overhead to the underlying computations.
\end{itemize}

% talk about organization of paper
% \textcolor{blue}{KO:Can be removed if page issues}
%The remainder of this paper is organized as follows. We discuss the background for our work in Section \ref{back} followed by a detailed explanation of workload prediction scheme in Section \ref{workload}. Subsequently, in Section \ref{design} we present the system design and in Section \ref{impl} we highlight the architecture and implementation details. Next, in Section \ref{eval} we report the experimental evaluation of Smartpick prototype on the live test-beds. Thereafter, in Section \ref{relwork} we review the related work and finally conclude the paper in Section \ref{concl}.