\section{Evaluation} \label{sec:eval}
In this section, we present a detailed discussion of our evaluation to show the efficacy of Smartpick. 
\subsection{Experimental Setup}\label{subsec:setup}
\noindent\textbf{Compute resource setting}: We deployed Smartpick prototype implementation on 
live test-beds of AWS \cite{awsRef} (US East region) and GCP (US East region) \cite{gcpRef}.
On AWS, we use \textit{t3.xlarge} instance (4 vCPUs and 16 GB RAM) for the Spark master, Spark driver, and the external Redis server. For workers that are dynamically deployed at run-time, 
we use \textit{t3.small} instances (2 vCPUs and 2 GB RAM)
for VM and Lambda \cite{lambdaRef} 2 GB RAM for SL.
Note that each Lambda instance provides 2 vCPUs. 
That is, each VM and SL instance offer the same amount of
CPU cores and memory in our evaluation. On GCP, 
we use a similar compute resource setting to AWS, 
i.e., \textit{e2-standard-4} (4 vCPUs and 16 GB RAM) for the master, the driver, and the Redis server, and \textit{e2-small} (2 vCPUs and 2 GB RAM) and Function \cite{cloudFunctionsRef} with 2 GB RAM for workers. All experimental results are an average of 10 runs, plotted with 90\% confidence intervals.
For cost, we use cost information on AWS and GCP web pages for VMs and SLs. We consider storage cost, e.g., local disk storage of VM and external storage (Redis) instance for communication among SLs 
as explained in Section \ref{sec:impl}. 
We also consider burstable costs of \$0.05 per vCPU-hour as we use the \textit{t3} instance family. Note that burstable costs of GCP e2-small is free of charge, but users cannot control it.

\noindent\textbf{Applications}: For workloads to evaluate Smartpick, we use three popular benchmarks, TPC-DS \cite{tpcds}, TPC-H \cite{tpch}, and Word Count (WC) \cite{wordCountInHive}. 
TPC-DS suite comprises compute and I/O intensive workloads with a high number of dependent map and shuffle stages (6 $\sim$ 16). TPC-H benchmark has SQL-like query benchmarking (moderated compute and I/O) with a lesser sequence of stages (2 $\sim$ 6). Lastly, we use Word Count as a simple query with I/O requirement. 
%For data source, 100 GB data were generated and stored in both AWS S3 and Google storage to be used for TPC-DS, TPC-H, and WC queries. 
For input data, we generate 100 GB of data in both AWS S3 and Google storage for each benchmark. 
%Additionally, a separate 500 GB dataset of TPC-H was generated and stored on each of the cloud providers in order to demonstrate Smartpick's incorporation of dynamics.
%We mainly use TPC-DS workloads to build prediction
%models as explained in Section \ref{workloadpredImpl}.
%We use WC and TPC-H as alien queries to evaluate Smartpick's performance on completely new workloads.
While we observed similar patterns of results from these benchmarks, we mainly show the results from TPC-DS queries due to space constraints.
We use WC and TPC-H benchmarks as new queries to evaluate Smartpick's performance on workload dynamics. 
In addition, we generate separate 500 GB data for benchmarks to see how Smartpick reacts with changes to data size.

\noindent\textbf{Baselines}: 
%For the baselines, we compare Smartpick with different approaches. (1) First, we compare Smartpick and Smartpick-r (that is, Smartpick with relay) against VM-only and SL-only approaches. 
%Here, the resources determined by workload prediction module are eventually converted to either VM-only or SL-only models for comparison purposes. 
%(2) Next, we compare the proposed prototype against state-of-the-art techniques: Cocoa \cite{cocoa} and SplitServe \cite{splitserve}. 
%To achieve this, we obtained the source code of Cocoa and SplitServe, which were then integrated into Smartpick's implementation on Spark for seamless comparisons. 
%(3) Third, we further show the significance of precision in workload prediction module by inducing random errors (both positive and negative), followed by reporting of the observed behavior. 
%(4) Next, we evaluate Smartpick for efficient navigation of cost-performance tradeoff. 
%(5) This is followed by the evaluation of Smartpick under system dynamics (similar queries vs new workloads). 
%(6) Finally, we discuss the efficacy of Smartpick in enhancing the memory locality through additional configurations.
We compare Smartpick's hybrid approach with two extreme approaches, i.e., SL-only and VM-only. %First, we compare Smartpick and Smartpick-r (Smartpick with the relay-instances mechanism) against VM-only and SL-only approaches.
To mimic VM-only and SL-only approaches, we tweak Smartpick's workload prediction module to choose either SL-only or VM-only for comparison purposes.
For the baselines, we compare the Smartpick against two state-of-the-art serverless-enabled data analytics systems, Cocoa \cite{cocoa} and SplitServe \cite{splitserve}. 
Note that we obtained the source code of Cocoa and SplitServe and integrated them into Smartpick's implementation on Spark for seamless comparisons.

\noindent\textbf{Building Prediction Models}: 
%This is because they have representational workloads and were used in previous work Cocoa \cite{cocoa} (for comparison purposes). 
To train the prediction models, we run 20 randomly selected configurations of VMs and SLs 
for each of the 5 TPC-DS queries i.e., 11, 49, 68, 74, and 82, as representational workloads, short-, mid-, and long-running queries. 
We generate 1000 data samples, i.e., different SLs + VMs configurations, by the heuristic approach discussed in Section \ref{sec:impl}.
We use 800 samples to build prediction models and 200 samples to evaluate the accuracies of the models (Section \ref{subsec:workload}).
We build two prediction models, \textit{Smartpick} without relay-instances and \textit{Smartpick-r} with the relay-instances for comparison purpose.

\begin{table}[!t]
	\centering
	\caption{Performance comparison between GCP and AWS}
	\label{tab:compAWSGCP}
	\scalebox{0.65} {\begin{tabular}{|p{1.3cm}|p{2.3cm}|p{1.4cm}|p{1.2cm}|p{1.6cm}|p{1.4cm}|p{1.4cm}|}
			\hline
			\textbf{Provider}&\textbf{Cloud Storage (MiB/s)}&\textbf{VM I/O (writes/s)}&\textbf{VM I/O (reads/s)}&\textbf{Memory (1k-ops/s)}&\textbf{VM CPU (events/s)} & \textbf{SL CPU (events/s)} \\
			%\hline
			\hline
			AWS& 117.53& 771.06& 1156.59& 4675.66& 1109.07 & 811.13\\
			\hline
			GCP& 51.64& 764.14& 1146.21& 4182.49& 906.67 & 714.87\\
			\hline
	\end{tabular}}
	%\vspace{-4mm}
\end{table}



\noindent\textbf{Performance Comparison between AWS and GCP:} % Broad insights:}
%Before discussing about the results obtained for various scenarios, it is important to explain the outcomes of several benchmarking experiments, for compute resources, on AWS and GCP. 
To clearly understand the experimental results, we first describe the performance difference between AWS and GCP. 
%summarizes the observations for AWS S3 and Google Storage and VM. 
Table \ref{tab:compAWSGCP} shows benchmark results between AWS and GCP; S3 and Storage for cloud storage, t3.small and e2-small for VM,
and Lambda and Function for SL.
%. It also shows the SL performance comparison across both the cloud providers. 
Both of these VM and SL compute resources have 2 GB memory with dual vCPUs.
In order to collect the bandwidth information for Cloud Storage accesses, we upload a 1.5 GB text file onto AWS S3 and GCP Storage and then capture the time taken for download through a Python \cite{python} script. For the remaining measures, we use the Sysbench \cite{sysbench} with identical parameters on both the cloud providers. %\textbf{KO:How did you measure?} \textcolor{red}{ADM: modified text.}
The table shows that AWS S3 provides better data transfer rate (bandwidth), which can affect overall query performance as input data is read from these cloud storage.
For CPU performance on VM, i.e., I/O, Memory, and VM CPU, AWS offers better performance than GCP. We observe that there is no significant difference in the boot-up time of VM as both require 31 $\sim$ 32 seconds. Similarly, for CPU comparisons on SLs, AWS offers better performance than GCP.
%We also measure the boot-up time of VMs for both the cloud providers, but there is no significant difference; both showing values in the range 31-32 seconds. 
Additionally, SL workers on GCP \cite{cloudFunctionsRef} do not have ephemeral storage for source files other than the configured RAM \cite{gcpFunctionsVolInfo}, which further reduces the available memory for computation. 
%Thus, 
In summary, the query execution times in GCP are comparably higher than that in AWS, which offers better performance for cloud resources we used in our evaluation. 
%, which mainly stems from the AWS S3 and VM/SL performance different instances that offer better performance than GCP.
%across all the parameters while both VM instance types we used on AWS and GCP are burstable. 
%We ran similar experiments on AWS Lambda and GCP Function with 2 GB memory to gauge the compute performance of SLs. While Function \cite{cloudFunctionsRef} gave $84.05$ \textit{events per second} for maximum prime number threshold of $10,000$, AWS Lambda produced $76.87$ \textit{events per second} for the same workload. Thus, for SL compute resource, both the cloud providers have comparable performance.
%Lastly, we measure the VM's boot-up times of AWS and GCP, but we do not see any significant difference; both showing values in the range 31-32 seconds. To recapitulate, the query execution times in GCP are comparably higher in contrast to that in AWS and the difference mainly stems from AWS S3 and VM instances that offer better performance than GCP.
% Figure environment removed
% Figure environment removed
% Figure environment removed
%We also measure the boot-up time of VMs for both cloud providers, but there is no significant difference; both showing values in the range 31-32 seconds.
%For SL performance, both cloud providers offer similar performance.
%In summary, the query execution times in GCP are comparably higher in contrast to that in AWS and the difference mainly stems from AWS S3 and VM instances that offer better performance than GCP.
\subsection{Workload Prediction}\label{subsec:workload}
In this experiment, we show how accurately Smartpick and
Smartpick-r models predict given queries' workloads with the 
initial prediction models explained in Section \ref{subsec:setup}. We capture different key statistics from the model training phase. First, we see a reasonable Root Mean Squared Error (RMSE) for both the models, i.e., 
Smartpick and Smartpick-r. 
On AWS, we get RMSE scores of $6.2$ and $8.2$ respectively, 
where as on GCP, we get the same as $12.8$ and $7.59$ respectively. Based on the extensive statistical analysis, 
we take 2 times the standard error as an accurate enough prediction, since it considers both the directions of error (positive and negative) \cite{stderr}. 
Thus, we plot graphs for each of the above cases by 
considering the distance from truth values on the test dataset.

Figure \ref{fig:smartpickAcc-aws} shows the frequency of test samples (200/1000 in our experiments with an 80:20 hold-out split for training and testing respectively) at varying distances 
from the truth values in seconds. It is observed that for Smartpick on AWS, $98.5$\% of the predicted samples lie within $10$ seconds difference of the actual query execution times, 
which shows that the model yields accurate predictions \cite{stderr}. Likewise, Smartpick-r provides a prediction accuracy of $97.05$\% on AWS. 
%The figure also shows the results from GCP as well, 
Smartpick and Smartpick-r on GCP give prediction accuracies of $73.4$\% and $83.49$\%, respectively, which is due to higher query execution time on GCP that
incurs more variance. We assume that these results are reliable enough for prediction systems \cite{predAcc1, predAcc2, predAcc3, predAcc4}. 
Besides, the prediction model will become more accurate as Smartpick considers workload dynamics (Section \ref{wordCount}). 
%it is worth mentioning that Smartpick becomes more and more accurate as dynamics are incorporated into the system (Section \ref{wordCount}).
\subsection{Performance and Cost Comparisons}\label{subsec:eval_compare}
\subsubsection{Comparisons with other approaches}
\label{subsub:compare_other approach}
%\noindent{\textbf{Comparisons with other approaches}}:
In this experiment, we compare the performance of Smartpick and Smartpick-r to two baselines, i.e., VM-only and SL-only approaches. Note that the cost-performance knob ($\epsilon$) in this experiment is set to 0, i.e., the best performance. Figure \ref{fig:smartpickNAWS} shows the
results on AWS. Figure \ref{subfig:awsperformance}
and Figure \ref{subfig:awscost}
show query completion times and 
cost, respectively for five TPC-DS queries (11, 49,
68, 74, and 82) with 4 different approaches, i.e., VM-only, SL-only, Smartpick, and Smartpick-r. The results clearly show that both Smartpick models
achieve better performance to that of VM-only and SL-only approaches
with reduced cost. While we can see similar performance from Smartpick and Smartpick-r, Smartpick-r incurs less cost as expensive SLs
are terminated when corresponding VMs are ready, which shows the benefits of the \textit{relay-instances} mechanism. 
Figure \ref{subfig:awsacc} and Figure \ref{subfig:awsaccr} %(Smartpick-r)
show predicted and actual query completion times using Smartpick and Smartpick-r respectively. 
These figures show that Smartpick can predict given queries' execution times accurately.
% Figure environment removed
Figure \ref{fig:smartpickNGCP} shows the similar patterns of results on GCP with more variance than AWS due to the different performance characteristics as explained in Section \ref{subsec:setup}.
%The figure shows more variance compared to experiments on AWS due to the different characteristics of 
%cloud resources between AWS and GCP as explained in Section \ref{subsec:setup}. 
For query 49 on GCP, we see a slightly better performance/cost compared to other queries, 
which is due to the persistent behavior of workload and significantly lesser variance. 
%\textbf{KO: Work well Only for short-running queries on GCP?} \textcolor{red}{ADM: No. It works well for all given queries on GCP. For query 49, we explain its difference from that on AWS.}
The VM-only cost on GCP is lower than other approaches as the burstable feature is free of charge on GCP. 
%on GCP because of the fact that the burstable feature is free (no extra cost) and determined by the provider.
%\textbf{Nonetheless, the fact that we see consistent patterns for query completion times (across cloud providers and against the pre-defined baselines) shows great generality and reliability of Smartpick.}
%Since, Smartpick-r shows better/similar performance with reduced cost, henceforth we will present the evaluation results of Smartpick-r when compared to other systems and configurations. Thus, moving forward we use Smartpick to refer to Smartpick-r, unless otherwise mentioned.
Overall, Smartpick-r shows better/similar performance with reduced cost compared to other approaches. In the rest of experiments, we use Smartpick to refer to Smartpick-r, unless otherwise mentioned.

%\subsection{Comparisons with state-of-the-art systems} \label{perfSystems}
\subsubsection{Comparisons with State-of-the-art Systems}\label{subsubsec:stateArtComp}
\label{subsub:compare_state_of_art}
%\noindent{\textbf{Comparisons with State-of-the-art Systems}}
In this section, we compare Smartpick with state-of-the-art systems, i.e., Cocoa \cite{cocoa} and SplitServe \cite{splitserve}. 
%Both of these systems rely on external workload prediction scheme, and thus we plug-in the WP module (see Section \ref{workloadpredImpl}) into Cocoa and SplitServe respectively. Figure \ref{fig:smartpickSystems} shows the evaluation on AWS and GCP. We observe that SplitServe tends to give comparable query completion times, but at a high cost (VMs and SLs combined). This is because of the underlying design of SplitServe, which (1) spawns equal number of VMs and SLs, and (2) depends on static segueing. Similarly, Cocoa gives comparable query completion times, but tends to always favor SLs because of its optimization design (more number of SLs help the objective function in final resource determination). Thus, Smartpick with dynamic relay support provides acceptable query completion times and incurs much lower cost.
Since they rely on external workload prediction (WP) systems, we tweak our WP module to choose VM instead of SL + VM, 
and plug-in the module into Cocoa and SplitServe respectively as we discussed in Section \ref{sec:impl}.
Figure \ref{fig:smartpickSystems} shows the evaluation on AWS and GCP. 
We observe that SplitServe tends to give comparable query completion times, but at a high cost (VMs and SLs combined)
due to the underlying design of segueing, i.e., the same number of SL and VM, and static timeout threshold for SL, as we discussed in Section \ref{subsec:relay}.
%which (1) spawns equal number of VMs and SLs, and (2) depends on static segueing. 
Similarly, Cocoa gives comparable query completion times, but 
we see inflated costs for Cocoa as well.
This is because Cocoa tends to always favor SLs because of 
its dependency on other simply assumed static values, such as the execution time for each map/shuffle task, which significantly affects their decisions.
%Hence, we see inflated costs for Cocoa as well. %\textbf{KO: Not sure.} \textcolor{red}{ADM: rephrased to reflect what we discussed.}
Thus, Smartpick can offer better query completion times with much reduced cost than other systems.

%% Figure environment removed

% Figure environment removed

%\subsection{Precision of Workload Prediction module}
%Efficacy of workload prediction module is important for its applicability by other systems, such as Cocoa \cite{cocoa} and SplitServe \cite{splitserve}. Although the comparable performance metrics in Section \ref{perfSystems} does suggest high-level of reliability in WP module, we perform additional experiments to confirm this hypothesis.  Figure \ref{fig:err-aws} shows performance and cost metrics for query 11 on two kinds of error scenarios in AWS. Figure \ref{subfig:posErr} shows the behavior of Smartpick when positive errors of varying magnitudes (0\%, 5\% and 10\% respectively) are introduced into the prediction system. This leads to more resources per query and as result, we get very small improvements in latency by incurring higher costs. In contrast, Figure \ref{subfig:negErr} shows that with negative errors of varying magnitudes query latency increases slowly with cost value significantly reduced. We could see similar pattern of results for GCP and other queries, but do not include them here due to space restrictions. This experiment not only shows the efficacy of Smartpick's workload prediction, but also confirms the richer cost-performance tradeoff space which needs to be explored.

\subsection{Exploiting cost-performance tradeoff}\label{subsec:tradeoffspaceEval}
For applications that have a limited budget, achieving the target performance
goal with the minimum cost is an important task, as discussed in Section \ref{subsec:tradeoff}. 
In this experiment, we show how such applications
achieve their cost-performance goals using Smartpick's property 
\textit{compute.knob}. Additionally, systems, e.g., SplitServe \cite{splitserve} that did not account for cost-performance tradeoff, can also benefit from Smartpick's design. Figure \ref{fig:troff-aws} shows the behavior of Smartpick and SplitServe (for query 11) with different values of the newly introduced performance knob. As applications increase the value of this knob from 0.2 - 0.8, the cost reduces significantly by trading off the query latency, as discussed in Section \ref{subsec:tradeoff}. 
Figure \ref{subfig:trSS} also shows that other systems, e.g., SplitServe, can benefit from Smartpick's feature by exploring the cost-performance tradeoff space.
Note that we could see a similar pattern of results from other queries on AWS and GCP, but omitted to cite these results here due to space constraints.

\subsection{Handling Dynamics}\label{handlingDynamicsParent}
\subsubsection{New Queries from TPC-DS workload}\label{subsubsec:simCheckEvalChild} To see how Smartpick handles other 
queries of TPC-DS, we use the queries 2, 4, 18, 55, and 62,
as unknown queries to Smartpick that have similar workloads 
with the queries used for building prediction models. Figure \ref{fig:newTPCDS} shows the benefit from Similarity Checker (SC), which helps achieve the best query latency ($\epsilon=0$) at a reduced cost for all new queries. This highlights the significance of SC module for similar workloads, which was discussed in detail in Section \ref{sec:design}. %\textcolor{red}{ADM: updated figures and modified text!}

% Figure environment removed

% Figure environment removed

% Figure environment removed

\subsubsection{Handling new workloads and increase in size}\label{wordCount}
One of the key aspects of Smartpick is to handle new queries 
by retraining models with the characteristics of new workload. 
In this section, we use Word Count (WC) as a new workload to
Smartpick. Based on the early trials, we observe that same instance re-training leads to an overhead on the ongoing job (which is expected), and therefore, advocate the use of different instance re-training (unless required otherwise).
To trigger the model retraining, we set \textit{errorDifference.trigger} to 10. That is, if the difference between actual query execution time 
and predicted time is more than 10 seconds, then model retraining is triggered. When the new query is submitted at first, 
Similarity Checker is invoked for each unknown query and the job proceeds to termination based on the closest match 
as discussed in Section \ref{subsec:designDynamics}. Upon job termination, an independent monitor thread triggers background re-training if the difference in predicted and actual values is higher than the configured threshold (\textit{errorDifference.trigger}). Figure \ref{fig:wrdCnt} shows that Smartpick's prediction model quickly converges to new values by efficient (data-burst based) re-training, as discussed in Section \ref{subsec:designDynamics}.
%\textbf{KO: Why AWS show more fluctuation?} \textcolor{red}{ADM: The variation is within the tolerance limit of 10 seconds. It is more prominent on AWS because of scale, but otherwise the results are in acceptable range and demonstrate dynamics incorporation by Smartpick.}

Another important aspect of handling dynamics is the change in workload size. We follow the same set-up as above, but instead use TPC-H query 3 workload as an alien query. 
In addition, after 5 executions, we change the database to point to a larger size of 500 GB and clean the event logs for existing query. 
While such significant changes may be rare in real environment, the dataset size grows eventually and consistently with increasing use of the application.
% to see how Smartpick handles the data size changes. 
%This is because in real life, such sudden change in data size is rare, but eventually and consistently the dataset size grows with increasing use of the application. 
Figure \ref{fig:tpch} shows the results observed for query 3. Clearly, when the data size shoots up, 
Smartpick can capture this change and quickly converges to the actual execution times. 
This support of handling dynamics asynchronously and quickly makes Smartpick a robust application with enhanced reliability even in the presence of workload dynamics. Note that the larger spike in the case of GCP is because of the slowness of cloud resources (as discussed in Section \ref{subsec:setup}), which is further aggravated by the large input data size of 500 GB.
%\subsection{Enhancing Memory Locality}
%In this section, we discuss the behavior of Smartpick under enhanced configurations. It is worth mentioning here that t3 instance family does not have proportional increase in cost, and as a result hinders the memory locality enhancement. Thus on AWS \cite{awsRef}, we modify the configuration key \textit{smartpick.cloud.compute.instanceFamily} to "\textit{m5}". Setting this in Spark configuration alerts Smartpick to distribute the determined VM-resource cores among larger sized instances through the greedy approach discussed in Algorithm \ref{alg:coreAlloc}, thereby enhancing the data locality. Thus, we evaluate the performance-cost changes for various queries compared to the \textit{t3} family described before. Likewise, we change the desired instance family to \textit{c5} on AWS \cite{awsRef} and \textit{e2-standard} on Google Cloud \cite{gcpRef}, and record the findings. Table \ref{tab:yi-instFam} summarizes these results. Although fewer number of large-core instances lead to the reduced query completion time, they come with higher cost. For instance, \textit{query 82} on AWS - c5 \cite{awsRef} decreases the completion time by $31.59$\%, whereas it increases the cost by $55.83$\%.
%\textbf{KO: Determining right family and size jointly would be interesting future work.}
%
%\begin{table}
%	\centering
%	\caption{Yields from different instance families}
%	\label{tab:yi-instFam}
%	\scalebox{0.95} {\begin{tabular}{cccl}
%		\hline
%		Family&Query&Completion Time($\downarrow$)&Cost($\uparrow$)\\
%		\hline
%		\multirow{2}{*}{AWS - m5}&82 &23.84\% &19.38\%\\&68 &8.72\% &74.22\%\\
%		\hline
%		\multirow{2}{*}{AWS - c5}&82 &31.59\% &55.83\%\\&68 &16.92\% &72.4\%\\
%		\hline
%		\multirow{2}{*}{Google Cloud - e2}&82 &13.66\% &95.5\%\\&68 &64.94\% &92.33\%\\
%		\hline
%	\end{tabular}}
%	%\vspace{-4mm}
%\end{table}