\section{System Model and Motivation}\label{back}
\subsection{System Model}\label{subsec:sysmodel}
\noindent\textbf{Data center (DC) setting and compute resources:} We focus on a single DC environment in the public cloud, where the network is not a performance bottleneck \cite{irre}
and infinite compute resources, i.e., serverless (SL) and virtual machine (VM), are available.
Each compute resource has different characteristics in terms of performance, cost, and agility, as shown in Table \ref{tab:compare}.
%For example, recent work \cite{cocoa} showed that serverless provides 70\% performance compared to VM with 
%expensive cost (up to 5.8X) while offering high agility and cost-efficiency. 
Such compute resources heterogeneity opens a rich cost-performance tradeoff space that applications can explore based on their demands.
%need to rewrite by KO
While data within a DC can be accessed and processed without a performance bottleneck, achieving memory-locality is important for performance improvement \cite{irre}. 
Exploiting SL in data analytics requires external storage systems, e.g., Redis \cite{redis} or AWS S3 \cite{s3Ref}, due to its 
limitations, e.g., limited network and storage, which may incur performance overhead.
We assume that performance overhead from losing memory-locality is negligible as we target queries with several tens of seconds granularity. 
%We will discuss potential performance improvement by using larger VM size and direct SL communication in Section \ref{memLoc} and Section \ref{relwork} respectively.
We will discuss potential performance improvement with improved memory locality in Section  \ref{sec:related}.

\noindent\textbf{Data analytics applications:} %\subsection{Instance Types}
We consider data analytics applications that generate diverse classes of MapReduce-like queries, 
e.g., reporting, ad-hoc, iterative, and data mining, as classified in \cite{whytpcds, whytpcds2}.
These queries contain several map and reduce stages that  cannot start until 
all their dependencies are resolved, i.e., dependent tasks. 
These queries can be processed by de-facto distributed data processing frameworks, e.g., Hadoop \cite{hadoop} and Spark \cite{sparkRef}.
While reporting queries are somewhat predictable as they are regularly generated based on the schedule, i.e.,
recurring (\textit{static}) queries, the remaining classes of queries, especially ad-hoc queries, are impromptu and dynamically
constructed to answer immediate and specific questions, i.e., \textit{dynamic} queries.
In this work, we mainly consider dynamic queries that may cause peak workloads.
Applications may utilize infinite compute resources, e.g., redundant VM instances, 
to handle dynamic queries without the performance bottleneck, which incurs additional cost for under-utilized or idle compute resources \cite{underutil}.
We assume that they have limited operational budgets; thus, minimizing the cost of processing queries within their target performance goals is highly desirable.

\noindent\textbf{Data analytics system (DAS):}
%Thus, we consider DAS that utilizes \textbf{data locality-aware} scheduling. 
We assume that DAS deploys an optimal number of long-lived VM instances as \textit{static compute resources} 
to handle static queries using workload prediction tools or systems \cite{cherrypick, vanir, selecta, optimuscloud, optimus, ernest, paris, juggler, rpss, crystalLp}.
However, DAS may encounter a performance bottleneck due to peak workloads (lack of compute resources) 
caused by the dynamic queries, e.g., ad-hoc queries.
While DAS can deploy additional VM instances to handle the dynamic queries, applications may not achieve the desired performance goals due to unavoidable overhead of VM, i.e., cold boot-up latency ($>$ 55 seconds)  \cite{coldBoot, coldBoot2}. 
Instead, DAS may deploy SL instances to start processing queries immediately as done in previous works \cite{spock, splitserve, occupy, flint, pocket, locus, numpywren, spark_on_lambda}, 
i.e., serverless data analytics (SDA). 
However, based on query workloads, SDA may encounter the cost-bottleneck for little (or no) performance improvement \cite{cocoa}. 
%For example, SDA incurs 600\% additional cost without 
%performance improvement to process a query that takes 120 seconds \cite{cocoa}. 
To handle dynamic queries in a timely and cost-efficient way,
we consider DAS that uses a \textit{hybrid approach exploiting SL and VM together} 
to achieve composite benefits, i.e., agility and cost-efficiency from SL, and better performance and cheaper cost from VM.

\noindent\textbf{Determining optimal compute resource configuration problem:}
%\textcolor{blue}{Please proofread below as I revised.}
While recent works \cite{cocoa, splitserve, spock, libra, perf-cost-ratio, robustScaling} have introduced 
similar hybrid approaches, they adhere to simple assumptions or 
workloads, e.g., static parameters without workload prediction, 
dynamics-free prediction model, and independent tasks, 
which would not work well for serverless-enabled data analytics (SEDA). 
%fails to ensure reliability in the dynamically evolving cloud environment.
In this work, we focus on determining the optimal compute resource configurations, i.e., 
how many SL and VM instances need to dynamically be spawned to handle incoming queries.
However, this is challenging because many metrics must be considered, 
e.g., query workload estimations (prediction), diverse applications' cost-performance goals, 
and heterogeneous compute resource characteristics. 
To determine optimal configurations, diverse approaches have been introduced to 
build performance prediction models using historical data \cite{cherrypick, vanir, selecta, optimuscloud, optimus, ernest, paris, juggler, rpss, crystalLp}.
%e.g., latent factor collaborative filtering \cite{selecta}, 
%machine learning models \cite{paris}, online fitting \cite{optimus},
%Bayesian optimization \cite{cherrypick}, sampling \cite{ernest}, 
%and combining a series of techniques \cite{vanir}. 
Unfortunately, these systems do not consider SL, but only VM for compute resources and
thus do not work well for SEDA. %determining optimal configurations with SL. 
Furthermore, with a large search space for optimality, novel approaches are required to navigate the solution space efficiently and ensure acceptable overhead/cost for the decision-making.
%\textcolor{blue}{Please proofread below as I revised.}
In this work, we use a machine learning technique, decision-tree based Random Forest (RF), to predict data analytic workloads using historical information. 
To efficiently explore the large search space, we incorporate Bayesian Optimizer (BO) into our prediction model, i.e., RF + BO (Section \ref{sec:workload_prediction}).
%\textcolor{blue}{KO: introduce BO to reduce the navigating the space}
Given predicted workloads, we focus on minimizing cost while meeting target performance goals, i.e.,
exploring a cost-performance tradeoff space (Section \ref{subsec:tradeoff}). %, by exploiting serverless and VM together (Section \ref{design}).

\noindent\textbf{Dynamics:} 
%We assume that dynamics are norm in cloud environments. For example, applications may send new queries unknown to DAS at any time. In addition, data size can change as more data is aggregated as time goes by. To predict workload correctly, the prediction model must be updated by incorporating these changes.
We assume that applications may send new queries unknown to DAS at any time. In addition, data size can be changed as more data is aggregated. To predict workload correctly, the prediction model must be updated by incorporating these changes (Section \ref{subsec:dynamics}).

%\noindent\textbf{Relay-instances:}
%To reap the benefits from the hybrid approach, they should be used in coordination. 
%SLs can be invoked and used until when a query is completed, which may incur additional cost. 
%In this approach, SLs execute the query tasks until the VMs are ready to process the rest of the tasks.
%SL instances can be invoked and used until when a query is completed, which may incur additional cost as we discussed in Section \ref{sec:intro}.
%To avoid this, Smartpick uses a simple but efficient mechanism, \textit{relay-instances}, with which 
%the SL instances start running the tasks quickly, and will be terminated when corresponding VMs are ready for rest of the tasks, i.e., after VM's cold-boot time, 
%which can further improve performance with reduced cost (Section \ref{subsec:relay}).

% Figure environment removed

\subsection{Illustrative Example} \label{subsec:example}
Workloads in data analytics systems (DAS) have large variance on query completion times. This stems from the fact that each of them can have different query semantics and thus, dissimilar resource needs to process the given data. To account for such scenarios and to handle the incoming queries efficiently, we highlight the need for performance prediction through an interpretative example. Let's assume three classes of dynamic queries: short-, mid-, and long-running queries, that incur peak workload. 
These queries have 100 tasks (short), 250 tasks (mid), and 500 tasks (long) respectively. Since all static compute instances are busy handling regular queries, DAS needs to deploy additional compute resources to handle them. In this case, DAS must determine optimal configurations, i.e., \textit{how many SL and VM instances}, 
that meet the applications' cost-performance tradeoff preference.

%Clearly, 
DAS has three options to deploy compute resources: 1) SL instances only (SL-only), 
2) VM instances only (VM-only), and 3) both VMs and SLs (Hybrid). For the sake of comparison, we consider AWS t3.small instance (2 vCPUs and 2 GB memory)
and AWS Lambda with 2GB memory. Note that AWS Lambda (2 GB) offers 2 vCPUs for each invocation. We take cost information from AWS \cite{t3Price, lambdaPrice}. 
%and also include storage cost for AWS gp2 8 GB SSD whenever VM instances are involved and Redis \cite{redis} storage cost (on master VM instance) whenever SL instances are involved. 
We consider storage cost for each VM (gp2 8 GB) and Redis \cite{redis} (external) storage cost (on master VM instance) whenever SL instances are involved.
%The choice of t3 instance family is based on the assumption of CPU-intensive workload and thus, we add the burstable costs (\$0.05 per vCPU-hour) into our model. For performance of SL instances, we assume zero-boot latency and include 30\% 
%performance overhead to query completion time \cite{cocoa}. For VM, we added 55 seconds to query completion time due to the cold-boot overhead \cite{coldBoot, coldBoot2}. 
Note that we choose AWS t3 family for the same compute resources as SL instance, and we consider the burstable costs (\$0.05 per vCPU-hour) in our model. For the performance of SL instances, we assume zero-boot latency and include 30\% 
performance overhead to task execution time (based on experimental evidence as shown in Section \ref{subsec:setup}). For the VM-only approach, we added 55 seconds to the query completion time as the cold-boot overhead \cite{coldBoot, coldBoot2}. 


%\noindent\textbf{Determining Compute Resource Configuration Problem:}
% \begin{table}[!t]
% 	\centering
% 	\caption{Estimated query execution times and costs (per query) with different compute resources configurations} % $\uparrow$ indicates best performance, $\leftrightarrow$ indicates average performance and $\downarrow$ indicates worst performance.}
% 	\label{tab:cpt}
% 	\scalebox{0.9} {\begin{tabular}{|c|c|c|c|c|c|}
% 			\hline
% 			Tasks &  % & \multicolumn{1}{c|}{Start-latency (s)} 
% 			Approach & Num VM & Num SL & Time (s) & Cost (\cent) \\ \hline

% %			\multirow{3}{*}{100}&\textbf{SL-only ($\uparrow$)} &0 &5 &\textcolor{blue}{\textbf{54.99}} &\textcolor{blue}{\textbf{1.09}}\\ &VM-only ($\downarrow$) &5 &0 &97.3 &1.65\\ &Combined ($\leftrightarrow$) &1 &4 &60.32 &1.2\\ \hline

% %			\multirow{3}{*}{250}&SL-only ($\leftrightarrow$) &0 &5 &137.475 &2.73\\ &VM-only ($\downarrow$) &5 &0 &160.75 &2.72\\ &\textbf{Combined ($\uparrow$)} &1 &4 &\textcolor{blue}{\textbf{126.84}}&\textcolor{blue}{\textbf{2.53}}\\ \hline

% %			\multirow{3}{*}{500}&SL-only ($\downarrow$) &0 &5 &274.95 &5.5\\ &VM-only ($\leftrightarrow$) &5 &0 &266.5 &\textcolor{blue}{\textbf{4.51}}\\ &\textbf{Combined ($\uparrow$)} &1 &4 &\textcolor{blue}{\textbf{237.71}} &4.74\\ \hline

% 			\multirow{3}{*}{\begin{tabular}{@{}c@{}}100 \\ (Short)\end{tabular}} & SL-only &0 &5 &\textcolor{blue}{\textbf{54.9}} &\textcolor{blue}{\textbf{1.2}}\\ &VM-only &5 &0 &97.3 &1.6\\ &Hybrid &1 &4 &60.3 &1.3 \\ 
% 			\hline
% 			\multirow{3}{*}{\begin{tabular}{@{}c@{}}250 \\ (Mid)\end{tabular}} &SL-only  &0 &5 &137.5 &3\\ &VM-only &5 &0 &160.8 &2.7\\ &Hybrid &1 &4 &\textcolor{blue}{\textbf{126.8}}&\textcolor{blue}{\textbf{2.7}} \\ 
% 			\hline
% 			\multirow{3}{*}{\begin{tabular}{@{}c@{}}500 \\ (Long)\end{tabular}}&SL-only &0 &5 &275 &6\\ &VM-only  &5 &0 &266.5 &\textcolor{blue}{\textbf{4.5}}\\ & Hybrid &1 &4 &\textcolor{blue}{\textbf{237.7}} &5.1 \\ 
% 			\hline

% 	\end{tabular}}
% 	%\vspace{-4mm}
% \end{table}

%% Figure environment removed

Figure \ref{fig:allConfEx} presents the expected query execution time and cost when 
DAS applies different approaches for an incoming query, assuming that 5 instances (either SL, VM or combined) are the optimal number of CPU cores. Here (0,5) and (5,0) represent the two extremes of compute resources configuration, i.e., SL-only and VM-only approach, respectively.
%It is evident that for a query with 100 tasks, SL-only approach outperforms the other two with best performance of 54.99 seconds. 
For the short-query, the SL-only approach offers the best performance with reduced cost, thanks to the agility of SL. 
%However, for mid- and long- running queries (with 250 and 500 tasks respectively), a combined allocation of VMs and SLs leads to the best application performance with average cost. 
For mid- and long-queries, however, the SL-only approach inflates cost without performance improvement, while the hybrid approach leads to better performance with the average cost.
%Additionally, it is important to note that VM-only approach outperforms the SL-only approach for long-running query, 
%since the latter has better compute efficiency as discussed in Section \ref{intro}. 
Interestingly, the VM-only approach outperforms the SL-only approach for long-running query due to the heterogeneity between SL and VM, as discussed in Section \ref{sec:intro}. 
%Therefore, it is clearly noticeable that workload prediction scheme is extremely important in determining the optimal configuration of VMs and SLs for varying query classes. 
%Further, there lies a rich cost-performance tradeoff space (for mid- and long- running queries), which when explored within the resource allocation scheme, can benefit applications with varying levels of latency and/or budget requirements. 
%Figure \ref{fig:allConfEx} shows the composite interplay of performance and cost for several different VM/SL configurations (considered in the example above). 
The results clearly show that a workload prediction scheme is extremely important to determine the optimal configurations of VMs and SLs for varying query classes.
The results also indicate that there is a richer cost-performance tradeoff space based on the query workloads. 

\noindent\textbf{Relaying workload:} 
%We also conducted few trials for short-running and long-running queries and could see dominance of SL-only for the former and Hybrid/Relay for the latter. Therefore, it is clearly noticeable that workload prediction scheme is extremely important in determining the optimal configuration of VMs and SLs for varying query classes. Additionally, there is a rich cost-performance tradeoff space for mid- and long- running queries which needs careful exploration.
%Once the cold boot-up time of VMs has elapsed (roughly 55 seconds in the example), that is all VM instances are running and connected to the master, SL instances can be shut-down to further reduce the cost (relay approach). This relaying of workload, from SLs to VMs, can lead to further reduction in query latency with lower costs. For example, for a long running query (500 tasks) and relay instance allocation of 5 VMs and 5 SLs, the query completion time achieved is a significant low of 219.97 seconds with cost as low as 4.9 $\cent$. Interestingly, this cost is slightly more than that of VM-only allocation approach (3.19 $\cent$ in Table \ref{tab:cpt}), with notable reduction to query completion time. Thus, there is a great opportunity for finer (combined) resource allocations that could harness the benefits of relay by providing best latency at a miniature cost.
%To further performance improvement with reduced cost, assume that the relay-instances mechanism is applied to the example, which spawns 5 SLs and 5 VMs simultaneously.
%The SL instances will start the query quickly and be terminated when the corresponding 5 VMs instances are ready.
%With the mechanism, the query require 219.97 seconds with cost 4.5 $\cent$. 
%Thus, there is a great opportunity for finer (combined) resource allocations that could harness the benefits of relay by providing best latency at a miniature cost.
In the hybrid approach, SLs can be invoked and used until a query is completed, which may incur additional cost without performance improvement due to SL's characteristics, as discussed in Section \ref{subsec:sysmodel}.
To avoid this, SLs can be terminated when corresponding VM instances are ready to avoid cost inflation and performance degradation, i.e., \textit{relay-instances} mechanism. 
For example, for a long-running query (500 tasks), 5 SLs and 5 VMs can be allocated simultaneously. 
The 5 SLs start running the tasks quickly and will be terminated when the corresponding 5 VMs are ready for the rest of the tasks, i.e., after VM's cold-boot time.
This approach results in performance improvement to 198.8 seconds with a reduced cost of 5\textcent, which is a better approach than simply using SLs throughout the query execution. 
%and relay instance allocation of 5 VMs and 5 SLs, the query completion time achieved is a significant low of 219.97 seconds with cost as low as 4.9 $\cent$. 
We will discuss the relay-instances mechanism in Section \ref {subsec:relay} in detail.
%\textbf{KO: I want to discuss about the cost and performance tradeoff for the example tomorrow}. \textcolor{red}{ADM: added new chart and text for showing different VM/SL configurations and their benefits.}
%\begin{table}[!t]
%	\centering
%	\caption{Estimated query execution times and costs (per query of 200 tasks) with different compute resources configurations}
%	\label{tab:cpt}
%	\scalebox{0.88} {\begin{tabular}{|c|l|c|c|c|c|}
%		\hline
%		Approach &  % & \multicolumn{1}{c|}{Start-latency (s)} 
%		Num VM & Num SL & Pre-tasks & Time (s) & Cost (\cent) \\ \hline
%		
%		SL-only & 0 & 5 & 200 & 109.98 & 3.93 \\ \hline
%		
%		VM-only & 5 & 0 & 0 & 139.6 & 1.67 \\ \hline
%		
%		Hybrid & 4 & 1 & 22 & 147.34 & \textcolor{brown}{\textbf{2.74}} \\ \hline
%		
%		Hybrid & 1 & 4 & 88 & \textcolor{blue}{\textbf{104.67}} & 3.29 \\ \hline
%		
%		Hybrid & 2 & 3 & 66 & 117.45 & 3.19 \\ \hline
%		
%		Relay & 4 & 4 & 88 & 114.22 & 2.83 \\ \hline
%		
%		Relay & 5 & 5 & 110 & \textcolor{blue}{\textbf{93.07}} & 3.17 \\ \hline
%		
%		Relay & 3 & 2 & 44 & 164.98 & \textcolor{brown}{\textbf{2.31}} \\ \hline
%		
%	\end{tabular}}
%	\vspace{-4mm}
%\end{table}