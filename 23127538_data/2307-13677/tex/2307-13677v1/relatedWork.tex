\section{Related Work} \label{sec:related}
%\subsection{Distinctions from Similar Prior Work}\label{dist}
%\textbf{KO: Go to related work}

%\textcolor{blue}{KO:Related work must be high-level. Details only for your work not their works}
\noindent\textbf{Exploiting SL and VM together}:
%Here we explain the fundamental differences of \textit{Smartpick} when compared with few recent works. 
%\textcolor{blue}{Libra and cocoa are too long.}
LIBRA \cite{libra}, aims to reduce the cost of hybrid deployments by utilizing cost indifference point, though actual costs can vary depending on the granularity of estimated completion time, where Smartpick comes into play. Cocoa \cite{cocoa} depends on static parameters and does not support relaying of workloads from SLs to VMs, which results in inflated cost. %Moreover, it is not adaptive to coarse-grained dynamics and lacks the ability to address similar queries without additional overhead.%offloading requests to severless mode whenever the request volume is below the cost indifference point. However, actual costs at the query-level will depend on the granularity of estimated completion time, which is where Smartpick comes into play. Conversely, Cocoa \cite{cocoa} depends on static parameters and does not embed the underlying system metrics for decision-making. %Moreover, it is not adaptive to coarse-grained dynamics and lacks the ability to address similar queries without additional overhead. 
%In addition, it does not support relaying of workloads from SLs to VMs, which will result in a better overall performance. 
While SplitServe \cite{splitserve} incorporates segueing from SLs to VMs, it results in cost inflation due to its design. 
It also demands the end-user to employ a cost manager for determining the additional SL resources, which is burdensome work.
SplitServe \cite{splitserve}, MArk \cite{perf-cost-ratio}, FEAT \cite{robustScaling} and Spock \cite{slo-spock} aim at reactively launching the SL instances whenever free cores are unavailable. Conversely, Smartpick's resource determination scheme optimizes the choice of VMs and SLs together while meeting cost-performance goals.
%, thereby enhancing the overall performance at a reduced cost.%(that is, number of VMs and SLs) based on machine learning schemes on top of prolific system parameters and rich query characteristics.%Other works, such as MArk \cite{perf-cost-ratio}, 
%Costless \cite{costless}, multi-media processing and IoT applications focus on usability of serverless model to 


%% Figure environment removed

\noindent\textbf{Workload prediction for compute resource configurations:} %\textit{Smartpick's} decision-making relies on a light-weight workload prediction technique that has an average run-time of 1.5 - 2 seconds. Even at such fine-scaled intervals, the model is able to precisely determine the resource needs of incoming requests because it is founded on the rich set of underlying system metrics. For cases where the estimated prediction time is off by a small amount (stemming from cloud dynamics \cite{perfVar}), \textit{Smartpick} is still able to reap the benefits out of dynamically spawned VM and SL instances and this is due to the fact that (application) resource requirements are relative to the intrinsic system properties. 
%\textcolor{blue}{KO: Include all the previous works mentioned before. You don't need to jump into too detilas but focus on different.}
%\textcolor{blue}{KO: refer all the workload prediction paper throughout the paper. 
%Merge cite{cherrypick, vanir, selecta, optimuscloud, optimus, ernest, paris} in intro
%cite{ernest, optimus, cherrypick, optimuscloud, crystalLp, rpss} in related work.
%}
Numerous prior works \cite{ernest, vanir, selecta, paris, optimus, cherrypick, optimuscloud, crystalLp, rpss, juggler}
have proposed methodical workload prediction schemes that help determine resource configurations for VM-based workloads. Adding SLs to the supported compute instance types leads to a huge search space for optimality and thus, renders these techniques time-consuming and ineffective. Interestingly, PerfOrator \cite{PerfOrator} uses hardware-level statistics to build performance model of big data queries, whereas Smartpick requires no advance knowledge of hardware settings and even supports the hybrid model of SLs and VMs.
%A few other works, such as OptimusCloud \cite{optimuscloud} and CherryPick \cite{cherrypick}, tried to determine compute resource requirements based on Workload Prediction. However, they focused on only 1 compute instance type - that is, VMs. Adding SLs to the supported compute instance types, leads to a huge search space for optimality. To understand the benefits that \textit{Smartpick} has to offer, we tune OptimusCloud \cite{optimuscloud} and CherryPick \cite{cherrypick} to incorporate both VMs and SLs into resource determination scheme.

\noindent\textbf{Handling dynamics:} CherryPick \cite{cherrypick} relies solely on the BO model to incorporate cloud uncertainties into the decision-making. This works fine for VM instance families but is not well suited to the hybrid approach for ad-hoc alien queries. Jockey \cite{jockey}, Morpheus \cite{Morpheus}, and ARIA \cite{aria} dynamically tune resource allocations (based on historical data) to ensure time-critical jobs with stringent SLOs are provided with required compute resources. They are, however silent on types of compute resources and do not consider the cold boot-up time of VMs. Conversely, Optimus \cite{optimus} does not depend on the historical information and imposes a checkpoint-inspired technique to handle changes in parameter servers, which can lead to a huge overhead due to multiple restarts. Quasar \cite{Quasar} updates its (VM) resource allocation approach based on active monitoring and sensitivity of the application's performance. Smartpick, instead, can handle unknown requests by employing spatial cosine similarity and course-grained dynamics, as shown in Section \ref{subsec:designDynamics}.
%extracting query characteristics, such as, sub-queries and table meta-data. Despite having built on certain workloads, the proposed model is also able to handle unknown requests by extracting query characteristics, such as sub-queries and table meta-data. Further, it relies on spatial cosine similarity to ascertain closeness to a reference workload. Therefore, the assessment of alien queries on \textit{Smartpick} (Figure \ref{fig:alienQN}) demonstrate astounding performance. The model supports background re-training of workload prediction module in order to adapt to the changing system environments and/or workload traits. This is crucial to the on-boarding of new requests and improving the accuracies to desired levels, which is evident from the Word Count problem on \textit{Smartpick} (discussed in Section \ref{wordCount}). Besides, since \textit{Smartpick} supports batch-based triggers for coarse-grained dynamics, hence the overhead incurred in long run (for keeping the model up-to-date) is almost negligible. 
%\textcolor{blue}{KO: While it is okay to explain about your approach here, you need to compare yours with other previous work. Find how other works handle dynamics
%in serverless + Vm papers and workload prediction papers.}



%A few other works, such as OptimusCloud \cite{optimuscloud} and CherryPick \cite{cherrypick}, tried to determine compute resource requirements based on Workload Prediction. However, they focused on only 1 compute instance type - that is, VMs. Adding SLs to the supported compute instance types, leads to a huge search space for optimality. To understand the benefits that \textit{Smartpick} has to offer, we tune OptimusCloud \cite{optimuscloud} and CherryPick \cite{cherrypick} to incorporate both VMs and SLs into resource determination scheme. Simulation results are presented in Figure \ref{fig:allComp}, where the primary vertical axis gives performance-cost ratio \cite{perf-cost-ratio} of each model (scaled to a multiple of 100). Equation \ref{eq:perfCost} provides the way these values are computed and we take the mean of 10 such simulation runs before plotting the final results. \begin{equation}
%  PC_r = \frac{1/Time}{1+cost}
%  \label{eq:perfCost}
%\end{equation}It is evident that OptimusCloud \cite{optimuscloud} gives the worst $PC_r$ value because of the large overhead arising from search complexity. Moreover, CherryPick \cite{cherrypick} has better search complexity because of the surrogate design but incurs a higher cost from the projected execution runs on live VM and SL instances. Overall, we observed the best $PC_r$ values for \textit{Smartpick} since it not only reduces the search time complexity, but also incurs a lower cost from the enhanced \textit{RF+BO} approach.

\noindent\textbf{Enhancing memory locality}: 
%One of the significant weakness of serverless
%in data analytics is limited network, i.e., SLs cannot communicate with each other directly, 
%which makes data analytics systems hard to utilize serverless. 
%This is because data analytics workload requires to share intermediate data among workers. 
%Many previous works \cite{spock, splitserve, occupy, flint, pocket, locus, numpywren, spark_on_lambda} utilized external storage e.g., AWS S3 and Redis \cite{redis}, to overcome this limitation. 
%However, this approach may naturally cause performance degradation due to losing
%data (memory) locality. More importantly, external storage can
%be a single point of performance bottleneck. 
%While we did not encounter the performance bottleneck from Redis in our evaluation setting,
%if data size become larger and more compute resources are used, Redis would become
%a single point of performance bottleneck.
%%For example, AWS S3 performs poorly for small and frequent access data. 
%Some recent works \cite{punching, boxer, short-lived, fmi} showed that
%serverless instances can communicate with each other directly using diverse techniques, 
%e.g., tcp hole punching and socket-related library replacement. We expect that using these
%techniques would improve performance for short-running queries, so that more SLs have 
%chance to be used in our system. We plan to apply these techniques in Smartpick 
%for performance improvement without additional cost. 
Many serverless-enabled data analytics systems \cite{spock, splitserve, occupy, flint, pocket, locus, numpywren, spark_on_lambda}
have utilized external storage systems, such as Redis and AWS S3, to avoid SL's limitation, i.e., limited network. 
However, this may naturally cause performance degradation due to losing data (memory) locality.
Some recent works \cite{punching, boxer, short-lived, fmi} showed that
SL instances can communicate with each other directly using TCP hole punching and socket-related library replacement. 
We expect that using such techniques would improve performance for diverse queries, especially short-running queries.
We plan to apply these techniques in Smartpick for further performance improvement without additional cost.
To improve memory locality, we also consider using larger (expensive) VM instance types (and families).
We could observe that applications can improve performance with additional cost by using 
larger VM instance family, e.g., AWS c3, which opens another richer tradeoff space.
However, we omitted this result due to space constraints. 
%We plan to extend Smartpick to determine optimal instance type (and family) as future work. 
