%%\section{System Design} \label{design}
%\section{System Design} \label{design}
%In this section, we will present how Smartpick
%helps applications explore the cost-performance tradeoff space. 
%We also show how Smartpick handle workload dynamics using
%prediction model retraining. Lastly, we will show 
%how Smartpick enhances memory-locality with 
%a small number of larger instances to improve performance.

%\section{System Design} \label{design}
\section{Smartpick Overview} \label{sec:design}
In this section, we present an overview of Smartpick.

%, describing diverse mechanisms
%that helps applications 1) explore the cost-performance tradeoff space, 2) handle workload dynamics using
%prediction model retraining, and 3) achieve better performance with reduce cost. %enhances performance memory-locality with a small number of larger instances to improve performance.


\subsection{Smartpick Architecture \& Workflow}\label{subsec:workflowArch}
Figure \ref{fig:arch} shows the Smartpick architecture in which the numerical values show the order of query execution when a new query is sent to Smartpick.

% Figure environment removed

\begin{itemize}[wide = 0pt, topsep=0pt]
	\renewcommand{\labelitemi}{\noindent$\bullet$}
	\item The workload prediction (WP) component comprises two sub-modules, i.e., RF and BO, that work together to efficiently explore the large search space as discussed in Section \ref{sec:workload_prediction}.
	\item Similarity Checker (SC) parses the alien (unknown) queries for extracting meaningful information such as the number of tables, columns, and subqueries inferred in the request.
	\item Monitor and Feature Extraction (MFE) monitors job execution, and 
	maintains a trained RF model and query features.
	\item History Server (HS) captures and stores the metrics outlined in Table \ref{tab:fea-wp}. 
	\item Resource Manager (RM) spawns and manages SL and VM instances based on optimal compute resource configurations. 
	\item Background Re-train creates a new model when the current model is outdated due to workload dynamics. 
\end{itemize}	

\noindent\textbf{Workflow:} When a new query is received (step 0), \textit{Job Initializer} (JI) asks WP to determine the optimal number of VMs and SLs required for the job (step 1). 
To efficiently predict query workload, WP maintains a list of queries against which the current operating model is trained. 
If WP realizes that the incoming query is not in the queries list, i.e., unknown query, WP asks the SC
to find the closest query/workload (in step 2). To determine optimal configurations, WP needs a trained RF model and 
query features as inputs except for \textit{instances} and \textit{query-duration}, as explained in Section \ref{subsec:wrkPredDetails}.
WP acquires these inputs from MFE (step 3) that pulls historical data from the History Server (step 4).
When all the inputs are available (step 5), WP can determine the optimal number of SLs and VMs. If the cost-performance tradeoff knob ($\epsilon$)
is set to greater than 0, WP iterates the Estimated Time list (or $ET_l$) to find a configuration that meets 
the cost-performance goal as explained in Section \ref{subsec:tradeoff}. 
From our evaluation, WP can determine compute configuration asynchronously (without blocking the Spark \cite{sparkRef} execution flow) within 
1.5 seconds for a known query and less than 2.5 seconds 
for an unknown (alien) query. We assume that this overhead is ignorable 
as we consider queries that take several tens of seconds. 
WP returns the resource requirements of incoming query to JI (in step 6). 
JI asks RM to spawn VMs and SLs based on the determination (step 7). 
RM spawns the desired number of VMs/SLs on the chosen cloud provider (step 8), following which the query execution begins.
If the prediction error in query execution (examined by MFE on job completion in step 9) is higher than the threshold, the prediction model is retrained by Background Re-train. 

\subsection{Handling Dynamics}\label{subsec:designDynamics}
\label{subsec:dynamics}
Workload dynamics could occur due to several reasons. 
For example, data analytics applications may need to write new queries to meet their needs \cite{changingNeeds}.
In addition, applications on the cloud store data in enormous volumes for decision-making and health checks \cite{bigDataForAnalysis},
i.e., increased data size. Smartpick is designed to handle such dynamics automatically.

%Coarse-grained dynamics can arise due to several reasons. First, the dataset that the computing framework is referring to has increased. This is very common since applications on cloud store data in enormous volumes for decision-making and health checks \cite{bigDataForAnalysis}. Second, the workload or type of queries that the system handles have changed. This scenario is also the result of growing data sizes and rapidly changing application needs \cite{changingNeeds} that client applications must adhere to. 

\noindent\textbf{Similarity check for alien queries:} Determining compute resources for alien queries is challenging since the prediction model is completely unaware of their resource needs.
To make a reasonably accurate prediction for such unknown queries, 
Smartpick maintains the known queries' identifiers and their attributes, 
such as the number of tables, columns, subqueries, and map tasks. When queries are sent, Smartpick extracts these attributes from the incoming queries 
and computes the \textit{spatial cosine similarity} to search for the closest known-query identifier. This reference identifier, along with other inputs (as discussed in Section \ref{subsec:wrkPredDetails}), is then used to deduce the request's resource-needs. We will show that Smartpick with similarity 
can help achieve good performance with reduced cost for similar yet alien queries in Section \ref{subsubsec:simCheckEvalChild}.
%\textbf{KO: How this identifies used in workload prediction? Briefly explain this.} \textcolor{red}{ADM: modified text to reflect the same.}
%\textbf{KO: Refer evaluation showing its efficacy.} \textcolor{red}{ADM: updated!}

%The proposed model takes a passive approach for handling such queries by first computing the spatial cosine similarity on a set of pre-determined attributes - one of them being the number of map tasks received as input. 
%Along with other query-based characteristics, similarity scores are computed and the closest query identifier is returned for usage in resource determination. 
%Eventually, as sufficient system metrics are available for these queries and background re-training gets triggered, the model adapts itself to the new workloads by incorporating them into the learning dataset.

%Similarity Checker (SC) makes use of the sql-metadata library \cite{sql-metadata} to parse the alien queries, 
%which helps extract meaningful information such as the number of tables, columns and subqueries inferred in the request. 
%Next, a 4-dimensional list is computed having all of the above features (along with \textit{"number of map tasks"} - received as input from the WP module), followed by the determination of spatial cosine similarity with respect to the known queries that helps filter out the best match. Thus, the closest query identifier is returned to WP module, 
%which then uses it for deducing resource-needs of the request.


%\noindent\textbf{Adapting to coarse-grained Dynamics: } Coarse-grained dynamics can arise due to several reasons. First, the dataset that the computing framework is referring to has increased. This is very common since applications on cloud store data in enormous volumes for decision-making and health checks \cite{bigDataForAnalysis}. Second, the workload or type of queries that the system handles have changed. This scenario is also the result of growing data sizes and rapidly changing application needs \cite{changingNeeds} that client applications must adhere to. Consequently, there is a need for an efficient approach that can handle it without impacting the execution of ongoing jobs/tasks. Smartpick supports background re-training of the regressor model so that up-to-date system metrics are incorporated into the workload prediction scheme. In addition, this re-training needs to be highly configurable so that any application with specific needs can reap the maximum benefits out of it.

\noindent\textbf{Retraining prediction models:} %Adapting to workload dynamics:} 
%\textbf{KO: We can say something here in high-level, ``even with Simliarity checker, it would not work well when workload is totally different from the queires used for prediction. 
%Thus, we need to retrain the preidction model when the accuracy is very low. To achieve this, Smartpick monitor the differece between actual and predict execution time query. 
%If the differece is greator than specific threshold, Smartpick will spwawn a model retraining task to deal with the new queries.''
%Consequently, there is a need for an efficient approach that can handle dynamics without impacting the execution of ongoing jobs/tasks. 
%Smartpick supports background re-training of the regressor model so that up-to-date system metrics are incorporated into the workload prediction scheme. } 
While Similarity Checker works well for alike queries, it does not account for workloads that are completely different from the trained queries. Thus, in the event of new/changed workloads, that is, when the accuracy is below an acceptable threshold, we need to retrain the prediction model. To achieve this, Smartpick monitors the difference between actual- and predicted- query execution time. If the difference is greater than a specified threshold, then Smartpick will spawn an asynchronous model re-training task that will re-tune the prediction models (in background) for handling dynamics.
In addition, this re-training needs to be highly configurable so that any application with specific needs can reap the maximum benefits out of it. 
We will discuss these configurable options in detail in Section \ref{sec:impl}. 
%\textbf{The paragraph below is duplicated with the one in the implementation (Prediction model updates). Move this to implementation and merge them.}
%\textit{In addition, this re-training needs to be highly configurable so that any application with specific needs can reap the maximum benefits out of it. Accordingly, the re-training in Smartpick is offered in two different flavors, that is, same instance re-training (on the master) and different instance re-training. In same instance re-training, the model checks for the minimum RAM configured for this task. Irrespective of the ongoing jobs, if RAM is available, then it spawns a new sub-process for re-training and updates the referred model for Smartpick. In a scenario, where the minimum RAM configured by the user is not available for re-training, a new instance will be started and used for this purpose.}
\subsection{Relay Instances} \label{subsec:relay}
To reap the benefits from the hybrid approach, i.e., SL + VM, they should be used in coordination.
This is because utilizing SL instances until when a query is completed may incur an additional cost with little 
(or no) performance improvement due to SL's more expensive cost and worse performance than VM, as discussed in Section \ref{sec:intro}.
To avoid this, Smartpick uses a simple but efficient mechanism, \textit{relay-instances}, with which 
the SL instances start running the tasks quickly, and will be terminated when corresponding VMs are ready for the rest of the tasks.
%i.e., after VM's cold-boot time. %which can further improve performance with reduced cost (Section \ref{subsec:relay}).
That is, SLs are only used during the VM's cold-boot time, and then terminated 
to maximize the benefits of the hybrid approach, i.e., agility from SL and better performance with reduced cost from VM. Consequently, Smartpick's prediction model incorporates the relay-instances mechanism, and thus, the VM and SL resources determined (which may be unequal but optimal) account for these relaying workloads.
%\textbf{KO: we need to say our prediction model incorate with relay instance so optimal like ``Note that, our preidction model can be trained with consideration of relay-instances mecanism''} \textcolor{red}{ADM: edited!}
%This is achieved by task replacement and the overall process leads to improved performance at reduced cost. 
%Note that the relay instances is not beneficial for short-running queries as shown in Section \ref{subsec:example}.

SplitServe \cite{splitserve} offers a similar approach, called segueing. However, their approach 
relies on a static threshold to terminate SLs, which may be costly with limited performance improvement. 
In addition, they use the same numbers SL and VM, which may not be optimal for a query. 
For example, SLs can be idle during the static timeout in segueing, which inflates overall cost significantly with limited performance improvement.
We present the benefits of relay instances and cost-performance comparison between relay instances and segueing in Section \ref{subsec:eval_compare}.

%Although SLs provide better agility and cost efficiency than VMs they are low in performance and incur high unit time cost (as discussed in Section \ref{sec:intro}).
%SL instances can be invoked and used until when a query is completed, which may incur additional cost as we discussed in Section \ref{sec:intro}.

%Thus, Smartpick benefits by offering relay instances, where SLs are terminated as soon as VMs complete the cold-boot time.  %\textbf{KO:Write how relay instances work briefly and discuss the difference with seguing} %\textcolor{red}{ADM: added text to explain the differences briefly.}
%The SL instances will start the query quickly and be terminated when the corresponding 5 VMs instances are readsey.
%With the mechanism, the query require 219.97 seconds with cost 4.5 $\cent$. 
%Thus, there is a great opportunity for finer (combined) resource allocations that could harness the benefits of relay by providing best latency at a miniature cost.
%\subsection{Improving Memory Locality} \label{memLoc}
%While network is not a significant performance bottleneck, memory-locality is still important for performance \cite{irre}.
%In addition, having small number of larger VM instances (with higher cores per instance) results in increased data locality and therefore, faster query execution times. 
%We ran few trials on AWS \cite{awsRef}, and the results concur with the aforementioned expectation. Thus, although Smartpick model is intended to be trained on 1 instance family type, 
%Smartpick supports for using larger VM instances with more cores and memory. 
%We confirmed this is true by running few trials on AWS \cite{awsRef}. 
%Although prediction model is trained on on a single instance type, 
%Smartpick allows applications to use larger VM with more cores and memory to improve memory-locality, thus performance.
%This setting is preferable for latency-sensitive queries, where cost incurred for larger VMs is acceptable with respect to the application goals.
%If such scenario is desired, then Smartpick takes a greedy approach \cite{greedy}. Once, the optimal number of VMs are determined for a given job, the instance families are decided in a decreasing order of the available cores. Algorithm \ref{alg:coreAlloc} provides the pseudo-code that is followed for instance family determination.
%Algorithm \ref{alg:coreAlloc} provides the pseudo-code that is followed for instance family determination.
%Given the optimal number of VMs are determined for a given query, the instance families are decided in a decreasing order of the available cores. \textbf{KO: It may be needed to explain the algorithms in detail. (or more comments)} \textcolor{red}{ADM: i went through the algorithm again. I think the existing comments make sense and the pseudo-code is easy to follow.}
%
%
%\setlength{\textfloatsep}{10pt}
%\begin{algorithm}
%	%\caption{Determining instance family}\label{alg:coreAlloc}
%	\scriptsize
%	\caption{Determining smaller number of larger VM}\label{alg:coreAlloc}
%	\begin{algorithmic}[1]
%		\Require $n \geq 0$ \Comment{Number of cores}
%		\Ensure $n$ is even, else increment by 1 and update
%		\State $iF \gets instanceFamily$
%		\State $iFCoreMap$ \Comment{Sorted iFType-core map}
%		\State $instCntMap \gets \{\}$ \Comment{Instance count map}
%		\While{$n > 0$}
%		\If{$iF$ is default}
%		\If{$instCntMap$ contains default}
%		\State $currVal \gets instCntMap_{default}$ 
%		\State $instCntMap$.put(default, $currVal$ + 1)
%		\Else
%		\State $instCntMap$.put(default, 1)
%		\EndIf
%		\State $n \gets n - 2$
%		\Else
%%		\State $\{iFType, cores\} \gets \displaystyle\argmax_{cores} \| iFCoreMap_{iFType} \newline- n \leq 0 \|$
%		\State $\{iFType, cores\} \gets \displaystyle\argmax_{cores} \| iFCoreMap_{iFType} - n \leq 0 \|$
%		\If{$instCntMap$ contains iFType}
%		\State $currVal \gets instCntMap_{iFType}$ 
%		\State $instCntMap$.put(iFType, $currVal$ + 1)
%		\Else
%		\State $instCntMap$.put(iFType, 1)
%		\EndIf
%		\State $n \gets n - cores$
%		\EndIf
%		\EndWhile
%		\State \Return $instCntMap$
%	\end{algorithmic}
%\end{algorithm}
%%\vspace{-1mm}