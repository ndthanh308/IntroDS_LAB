{
  "title": "Graph-based Polyphonic Multitrack Music Generation",
  "authors": [
    "Emanuele Cosenza",
    "Andrea Valenti",
    "Davide Bacciu"
  ],
  "submission_date": "2023-07-27T15:18:50+00:00",
  "revised_dates": [],
  "abstract": "Graphs can be leveraged to model polyphonic multitrack symbolic music, where notes, chords and entire sections may be linked at different levels of the musical hierarchy by tonal and rhythmic relationships. Nonetheless, there is a lack of works that consider graph representations in the context of deep learning systems for music generation. This paper bridges this gap by introducing a novel graph representation for music and a deep Variational Autoencoder that generates the structure and the content of musical graphs separately, one after the other, with a hierarchical architecture that matches the structural priors of music. By separating the structure and content of musical graphs, it is possible to condition generation by specifying which instruments are played at certain times. This opens the door to a new form of human-computer interaction in the context of music co-creation. After training the model on existing MIDI datasets, the experiments show that the model is able to generate appealing short and long musical sequences and to realistically interpolate between them, producing music that is tonally and rhythmically consistent. Finally, the visualization of the embeddings shows that the model is able to organize its latent space in accordance with known musical concepts.",
  "categories": [
    "cs.SD",
    "cs.LG",
    "eess.AS"
  ],
  "primary_category": "cs.SD",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14928",
  "pdf_url": "https://arxiv.org/pdf/2307.14928v1",
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 2834004,
  "size_after_bytes": 116189
}