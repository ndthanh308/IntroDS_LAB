\section{Confident Simultaneous Feature Ranking}\label{sec:conf_rank}

In this section, we introduce our ranking method which is designed to rank FI values while taking into account the uncertainty associated with the post-hoc FI method and the sampling process. Using our base-to-global framework, we are able to quantify the uncertainty by calculating simultaneous CIs for the true ranks.

\subsection{Feature Ranking}

Our method uses pairwise hypothesis tests to estimate lower and upper bounds for the true rank of each feature.
For each feature pair $j,k$, we perform two one-sided hypothesis tests:
\begin{enumerate}
\item[(a)] A test of
$H^1_{jk}:  \trueimp_j < \trueimp_k \text{ versus } H_{jk}^0: \trueimp_j \geq \trueimp_k$;
\item[(b)] 
A test of $H^1_{kj}:  \trueimp_k < \trueimp_j \text{ versus } H_{kj}^0: \trueimp_k \geq \trueimp_j.$
\end{enumerate}

Each test is composed of a p-value $p_{jk} = pairCompare(\basematrix_j, \basematrix_k)$
and a significance level $\alpha \in (0,0.5]$; the test rejects $H_{jk}^0$ if $p_{jk}<\alpha$. 
The test is \emph{calibrated} if: $\mathbb{P}(p_{jk} \leq \alpha) \leq \alpha \quad \text{for any } \trueimp_j \geq \trueimp_k$, meaning that the probability of rejecting $H^0_{jk}$ when $H^1_{jk}$ is correct is bounded by $\alpha$. For the tests to be calibrated, they need to account for the possible dependence between $\basematrix_j$ and $\basematrix_k$. In our implementation, we use the paired-sample t-test (see Section \ref{subsec:implementation}).

There is a natural relation between the results of the one-sided hypothesis tests and the ranking of the global FI values. The rejection of a hypothesis $H_{jk}^0: \trueimp_j \geq \trueimp_k$, implies acceptance of a \emph{partial ranking} of the global FI values, namely $\trueimp_j < \trueimp_k$. This partial ranking limits the global ranking: the feature $j$ cannot be ranked highest ($u_j < p$), and the feature $k$ cannot be ranked lowest ($l_k > 1$). We combine many partial ranking statements to improve the bounds on the ranks.

Combining many probabilistic decisions comes at a price. Setting the tests' rejection threshold to $\alpha$ limits the marginal probability of making an error in each partial ranking to $\alpha$. However, when combined, the probability of making at least one error increases with the number of tests, and without proper adjustments it may greatly exceed $\alpha$. In the next subsection, we adjust the p-values to control this probability over multiple tests.

\subsection{Controlling Partial Ranking Error}

We define $\paranking$ as the set of partial rankings from all pairwise tests 
$\paranking = \{(j,k) : H_{jk}^0 \text{ was rejected} \}$. A partial ranking error is a pair $(j',k')\in \paranking$ for which $\trueimp_{j'}\geq \trueimp_{k'}$. 
For simultaneous CIs, we want to control the probability of error over all the partial rankings.

\begin{definition}(Family-Wise Error Rate)
The family-wise error rate (FWER) is controlled at probability level $\alpha$ on the set of partial rankings $\paranking$ if the probability of making any partial ranking error is less than $\alpha$:
$\mathbb{P}(\exists (j',k') \in \paranking:  \trueimp_{j'} \geq \trueimp_{k'}) \leq \alpha$.
\end{definition}

To control the FWER, we replace the original p-values with a set of adjusted p-values $\mathbf{p}^{adj} = FWERAdjust(\basematrix, pairCompare)$. After adjustment, the partial rankings are obtained by comparing $p_{jk}^{adj}$ and $p_{kj}^{adj}$ to the required FWER level $\alpha \,.$\footnote{In practice, when both tests use the same data and the threshold $\alpha$ is less than $0.5$, none or just one of the null hypotheses will be rejected (there will not be a case in which both of the null hypotheses are rejected).} Some examples of adjustment procedures in which the FWER is controlled are provided in Section \ref{subsec:implementation}.

\subsection{Confident Simultaneous Feature Ranking}\label{subsec:alg}

When the FWER is controlled for the partial rankings set, we can use the partial rankings set to derive simultaneous CIs for the true ranks:

\begin{theorem} \citep{al2022simultaneous}\label{thm}
Let $\paranking$ be the set of partial rankings with FWER control at level $\alpha$. For $j = 1,...,p$, define: 
\begin{align*}
    L_j = 1 + \#\{k : (k,j) \in D\}, \\
    U_j = p - \#\{k : (j,k) \in D\}.
\end{align*}
Then the sets $\{[L_j, U_j]$ for $j \in \features\}$ are $(1-\alpha)$ simultaneous CIs for the true ranks.
\end{theorem}

The construction naturally extends the definition of rank-set provided in Definition \ref{def:setrank}. The idea of the proof is that a coverage failure means that the set of true (one-sided) differences is smaller than the set of observed (one-sided) differences. This means that at least one partial ranking in $\paranking$ is false. Therefore the FWER upper bounds the probability of an error in the CIs (see proof in Appendix \ref{app:proof}). Our ranking method is based on ICRanks \citep{al2022simultaneous}; the way in which the proposed method differs from ICRanks is discussed in Section \ref{subsec:implementation}.

Algorithm \ref{alg} summarizes our method for constructing simultaneous CIs for the true ranks. The algorithm works directly on the base FI matrix without requiring access to the trained model, the FI method, or the explanation set. The main assumption is that our paired test is calibrated for the possible distributions of base FI values.

\begin{algorithm}
\caption{Simultaneous CIs for Ranks}\label{alg}
\begin{algorithmic}
\REQUIRE 
\STATE $\basematrix$: base FI matrix;
\STATE $1 - \alpha > 0$: level of confidence;
\STATE $pairCompare$: suitable paired test;
\STATE $FWERAdjust$: 
FWER adjustment procedure.


\FOR {$j,k \in \features, j \neq k$} {
\STATE    $p_{jk} \gets pairCompare(\basematrix_j, \basematrix_k)$.
}\ENDFOR

\STATE $\mathbf{p}^{adj} =  \gets FWERAdjust(\basematrix, pairCompare)$
\STATE $D \gets  \{(j,k) : p_{jk}^{adj} \leq \alpha\}$

\FOR {$j \in \features$} {
\STATE $L_j \gets 1 + \#\{k : (k,j) \in \paranking\}$
\STATE $U_j \gets p - \#\{k : (j,k) \in \paranking\}$
}\ENDFOR

\RETURN $[L_1, U_1], \ldots, [L_p, U_p]$.
\end{algorithmic}
\end{algorithm}

\subsection{Ranking Method Implementation}\label{subsec:implementation}

\paragraph{Paired Test} We use a parametric paired t-test to compute p-values for the pairs of base FI values.
Set $\mathbf{d} = \basematrix_j - \basematrix_k$ to be the vector of differences, and 
denote $\bar{d}$ as the sample average and $s_d$ as the sample standard deviation. Then the one-sided $\alpha$ level test rejects the null hypothesis if  $\bar{d}/ (s_d/\sqrt{n}) >  T_{n-1}(1-\alpha)$, where $T_{n-1}(1-\alpha)$ denotes the $1-\alpha$ quantile of student-t ($n-1$ df). The paired t-test is fairly robust to departures from a normal distribution \citep{posten1979robustness}.

\paragraph{Adjustment for Multiple Tests} We implement two sequential procedures to adjust (increase) the p-values:
\begin{itemize}
    \item Holm's procedure \citep{holm1979simple}. Assuming that the base FI values are normally distributed, the paired t-test is calibrated with this procedure. We implement Holm's procedure on the one-sided hypothesis tests, although for approximately normal data, the two-sided Holm would likely work well also. See \cite{shaffer1980control, shaffer1995multiple} on using Holm's procedure for pairwise tests.
    \item Min-P \citep{westfall1993resampling}. This procedure is based on bootstrapping. Therefore, no further assumptions are required.
\end{itemize}
The adjusted p-values are then compared to the predefined threshold of $\alpha$ (details on the procedures are provided in Appendix \ref{app:fwer}). With these procedures, if the p-values are calibrated, then the FWER for the rejected tests is controlled at the $\alpha$ level, regardless of the dependence.

\paragraph{Comparison to ICRanks} Similar to Algorithm \ref{alg}, ICRanks \citep{al2022simultaneous} is based on Tukey's correction \citep{tukey1953problem} in order to control the differences between ranks simultaneously. Tukey's correction is designed for normal and independent data, which are distributional assumptions that would usually not hold for FI values. In contrast, our algorithm applies a test to each feature pair separately, and the Holm's or the Min-P procedure is performed on the resulting p-values; therefore, it can be utilized with robust or nonparametric tests \citep{wilcox2011introduction}.


