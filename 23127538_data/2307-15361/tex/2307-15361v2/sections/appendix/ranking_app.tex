\section{Definitions and Proofs}\label{app:def_proof}

\subsection{Proof of Theorem \ref{thm}}
\label{app:proof}

In this section, we present the detailed proof of Theorem \ref{thm}.

Recall that $D$ is the set of partial rankings and that we assume that the probability of any error in $D$ is less than $\alpha$.
To prove the theorem, we first show that any error in coverage, i.e., a CI that does not cover the true rank, must be caused by at least one partial ranking error in $D$:

Suppose that there is a coverage error. Without loss of generality, assume that the coverage error occurs for feature 1: 
$$ [l_1, u_1] \nsubseteq [L_1,U_1].$$
The coverage error can be in one or both bounds:
\begin{enumerate}[label=(\arabic*)]
    \item $l_1 < L_1$,
    \item $u_1 > U_1$.
\end{enumerate}

If (1), then $L_1 > 1$, and there are $L_1-1>0$ pairs of the type $(1,k) \in D$, meaning that there are $L_1-1$ features with a significantly lower observed global FI than the observed global FI of feature 1.
However, according to Definition \ref{def:setrank}, $l_1-1 = \# \{k: \trueimp_1 > \trueimp_k\}$, meaning that there are only $l_1-1$ features with true global FI lower than the true global FI of feature 1.
Combining these two statements together, there must be at least one feature $k = 2,...,p$ for which 
$(1,k) \in D$ but $\trueimp_1 \ngtr \trueimp_k$, meaning that there is a partial ranking error in $D$.

If (2), then the set $\{k \in 2,...,p : (k,1) \in D\}$ is higher than the set $\{k: \trueimp_k>\trueimp_1\}$. Again, this would mean that for at least one value of $k$ there is a partial ranking error.

The event of at least one coverage error is contained in the event of obtaining a partial ranking error. Given that, the coverage error probability is bounded by $FWER = \alpha$.

\subsection{FWER Adjustment Procedures}\label{app:fwer}

Here, we provide details on the two sequential procedures that we use in our implementation. After adjustments, the p-values are compared to a chosen $\alpha$ level. Note that all p-values are inflated compared to their original level, making it less likely that the null hypothesis will be rejected. Furthermore, the p-values keep their relative order after adjustment. In the procedures below, this is governed by the $\max$ function, which assures that the order is maintained. The resulting process is sequential in that for a given level $\alpha$, after the first non-rejected value, all others would not be rejected. Let $p_1,...,p_K$ be a set of K p-values obtained by testing a family on null hypotheses $H_1^0, \ldots, H_0^K$; below we demonstrate how the two FWER adjustment procedures are used to calculate $p_1^{adg},...,p_K^{adg}$, a set of adjusted p-values.

\subsubsection{Holm's Procedure} 

We implement Holm's procedure \citep{holm1979simple} on one-sided hypothesis tests. The paired t-test is calibrated with this procedure for normally distributed base FI values.

Let $p_{(1)}\leq,...,\leq p_{(K)}\leq 1$ be the sorted set of p-values. Then:
\begin{align*}
    &p^{adj}_{(1)} = K\cdot p_{(1)}, \\
    &p^{adj}_{(2)} = \max\{p^{adj}_{(1)}, (K-1)p_{{(2)}}\}, \\
    &\ldots, \\
    &p^{adj}_{(k)} = \max\{p^{adj}_{(1)},...,p^{adj}_{(k-1)}, (K - (k-1))p_{(k)}\}, \\
    &\ldots, \\
    &p^{adj}_{(K)} = \max\{p^{adj}_{(1)},...,p^{adj}_{(K-1)}, p_{(K)}\}.
\end{align*}

\subsubsection{Min-P Procedure}

Holm's procedure is highly conservative, since it is valid regardless of the structure of dependence between the p-values. To improve it, \cite{westfall1993resampling} suggested the Min-P procedure. The idea is to use bootstrapping to model the structure of dependencies between p-values, obtain lower adjusted p-values, and reject more hypotheses. The details of the Min-P procedure described here are taken from \cite{efron2012large}. 

Here, we also start with the sorted set of p-values $p_{(1)}\leq,...,\leq p_{(K)}\leq 1$. Let $i_1, \ldots i_K$ indicate the corresponding original indices, $p_{(k)} = p_{i_k}$, and define $I_k=\{ i_k, i_{k+1}, \ldots, i_K \}$ and $\pi_k=\mathcal{P}_0 \{ \min_{j \in I_k} (P_j) \leq p_{(k)}\}$. Here, $(P_1, \ldots, P_K)$ indicates a hypothetical realization of the unordered p-values $p_1,...,p_K$ obtained under the complete null hypothesis, meaning all $H_k^0$s are true. 
The adjusted p-values are then defined by:
\begin{align*}
    &p^{adj}_j = \max_{k \leq j} \pi_k.
\end{align*}



