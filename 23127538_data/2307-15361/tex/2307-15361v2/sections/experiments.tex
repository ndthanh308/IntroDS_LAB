\section{Evaluation}\label{sec:experiments}

In this section, we evaluate our base-to-global framework and ranking method. We use synthetic data to assess our method's validity (simultaneous coverage) and efficiency. We analyze our ranking method by generating base FI values directly (Section \ref{sec:comparison}). We note that feature ranking is an interpretability step at the end of an ML task, as shown in Figure \ref{fig:process}; therefore, we simulate the entire process of training and explaining a model with simulated data (Section \ref{sec:shap_simulated}) and real data (Section \ref{sec:real}).

% Figure environment removed

\paragraph{Metrics} We use the metrics (ranking measures) suggested by \cite{al2022simultaneous} to define simultaneous coverage and efficiency:
\begin{itemize}
    \item \emph{Simultaneous coverage} -- the proportion of experiments where all true ranks are covered by their CIs: $one \text{ if all } \{\trueimp_j \in [L_j, U_j]\}, and zero \text{ otherwise} $.
    \item \emph{Efficiency} -- the average relative size of the CIs: $\frac{1}{p \cdot (p-1)}\sum_{j=1}^p (U_j - L_j)$.
\end{itemize}
Higher coverage and lower efficiency are better.


\subsection{Ranking Method Comparison}\label{sec:comparison}

\paragraph{Ranking Methods} We compare the ranking measures of four ranking methods: a naive ranking method based on empirical quantiles of bootstrap samples as a baseline (details are provided in Appendix \ref{app:naive}), ICRanks,\footnote{\href{https://cran.r-project.org/web/packages/ICRanks/ICRanks.pdf}{ICRanks package}} our ranking method with Holm's procedure, and our ranking method with the Min-P adjustment procedure.

We sample the base FI values from a multivariate-normal distribution $N_p(\mathbf{\mu}, \Sigma)$ with predetermined vector of means $\mathbf{\mu}$ and a covariance matrix $\Sigma$. The true global FI values are the means, and we control the correlation structure between the base FI values via the definition of the covariance matrix.

\paragraph{Vector of Means} The structure of the vector $\mathbf{\mu}$ is $\left(1^{\mu\text{-exponent}}, 2^{\mu\text{-exponent}}, \ldots, (p+1)^{\mu\text{-exponent}}\right)^T$, with $\mu$-exponent $\in [0.1, 0.25, 0.5]$. A lower value of $\mu$-exponent results in a more dense vector of means. Ties between the means are allowed.

\paragraph{Covariance Matrix} The covariance matrix structure is composed of a vector $\mathbf{\sigma}^2$ of the variances of the base FI values sampled from the re-scaled chi-squared distribution ($\chi_{(5)}^2 / 5$). The correlation matrix structure can be one of three structures: identity (no correlations), block-wise pairs, or equal correlations with $\rho \in [0.1, 0.5, 0.9]$. In addition, we vary the level of noise in the base FI values by scaling the vector $\mathbf{\sigma}$ by $\sigma$-factor $\in [0.2, 1, 5]$.

We analyze the ranking measures for multiple conditions of the vector of means ($\mathbf{\mu}$) and the correlation matrix ($\Sigma$) (a total of 486 conditions). The number of features $p$ is one of: $[10, 30, 50]$, and the number of base FI values $n$ is one of: $[100, 300, 1000]$. We sample 100 independent explanation sets for each configuration and report the average ranking measures across the repetitions. Below, we present the results for $p=30$, $\mu$-exponent=0.25, and equal correlations. Additional results are presented in Appendix \ref{app:mock}.

\paragraph{Simultaneous Coverage} In the naive ranking method, simultaneous coverage is not maintained in all conditions. All other methods maintain simultaneous coverage levels of almost 100\%; this indicates that they are overly conservative compared to the nominal required simultaneous coverage of $90\%$.

\paragraph{Efficiency} Without correlations, efficiency degrades as the $\sigma$-factor increases, with almost no difference observed between the methods (Figure \ref{fig:mock_exp_no_corr}). With correlations, as $\rho$ increases our method becomes more efficient than ICRanks, with the Min-P adjustment seen to be slightly more efficient than Holm's procedure. The gap between the methods increases as the $\sigma$-factor increases (Figure \ref{fig:mock_exp_corr}).

% Figure environment removed

% Figure environment removed


\subsection{SHAP Ranking Measures}\label{sec:shap_simulated}

Here, we simulate the entire ML process as described in Figure \ref{fig:process} with simulated data. We analyze the ranking measures and runtime of our ranking method with Min-P and Holm's procedures compared to ICRanks.

\paragraph{Data Generating Process (DGP)} We follow the DGP of \cite{ishwaran2019standard}. We sample a data matrix $X$ with independent uniformly distributed features and calculate $Y$ as a function of $X$ with noise. We define two functions:
\begin{enumerate}[label=(\Alph*)]
    \item $ \! 
    \begin{aligned}[t]
    y &= 10\sin{(\pi x_1x_2)} + 20(x_3 - 0.5)^2 + 10x_4 + 5x_5 + \epsilon; \\
    & \{x_j\} \sim U(0, 1); \, \epsilon \sim N(0, 1).
\end{aligned}
$
\item $ \! 
\begin{aligned}[t]
    y &= (x_1^2 + [x_2x_3 - (x_2x_4)^{-1}]^2)^{0.5} + \epsilon; \\
    & x_1 \sim U(0, 10), x_2 \sim U(\pi, 2\pi), x_4 \sim U(1, 5), \\
    & \text{all other features } \{x_j\} \sim U(0, 1); \, \epsilon \sim N(0, 1).
\end{aligned}
$
\end{enumerate}

The definitions of $(X,Y)$ are for $p=10$ features. We simulate a larger number of features by defining the functions for cycles of 10 features.  For example, in DGP-A, $X_{11} \sim U(0, 10)$ and is added to $Y$ as $X_{11}^2$.

For this simulation, we sample a large data matrix $X_{M \times p}$ ($M=500k$), calculate $Y$ as a function of $X$ with noise, and train a prediction model on $\sampleset_{train}=(X, Y)$. We calculate the global FI values $\imp_1, \ldots \imp_p$ based on a sufficiently large sample ($n=1M$), making it a low variance estimator of $\trueimp_1, \ldots \trueimp_p$ \citep{slack2021reliable}. We generate multiple simulated datasets, varying the number of features ($p$) and base FI values ($n$), the DGP, and the prediction models. We sample 100 independent explanation sets for each evaluation configuration to measure the ranking efficiency, simultaneous coverage, and runtime.

Below we present the results for the DGP-A with a random forest (RF) model \citep{breiman2001random} and DGP-B with an XGBoost (XGB) model \citep{chen2016xgboost}  (see Appendix \ref{app:shap_simulated} for the complete results). We use TreeSHAP \citep{lundberg2019Tree} to compute the base FI values, relying on the definition of base and global FI values presented in Section \ref{sec:framework} (Equation \ref{eq:local_shap}). To calculate the ranking measures, we repeatedly sample $\sampleset_{explain}$ independent of $\sampleset_{train}$.

\paragraph{Simultaneous Coverage} All of the examined methods maintain simultaneous coverage levels of almost 100\% in all simulated conditions. However when the base FI values have an extremely long tail, simultaneous coverage is not guaranteed (an example is provided in Appendix \ref{app:shap_simulated_non_normal}).

\paragraph{Efficiency} We can see that ICRanks is comparable to our ranking method. The ranking efficiency improves as $n$ increases (Figure \ref{fig:shap_results}). For the XGB model, we see that the efficiency of our method with the Min-P procedure is worse than that of our method with Holm's procedure for low $n$ values; the Min-P procedure recalibrates the p-values based on resampled data, which is an inefficient process when $n$ is low.

\paragraph{Runtime} We analyze the runtime of TreeSHAP (computation of base FI values) and the ranking times (ICRanks, Holm's procedure, and the Min-P procedure). The runtime of ICRanks and Holm's procedure is ten times faster than the runtime of TreeSHAP. The Min-P procedure requires B repetitions (bootstrap samples) of the pairwise tests, so the runtime increases with B. Details are provided in Appendix \ref{app:shap_simulated_runtime}.

% Figure environment removed

\subsection{Real Data Experiments}\label{sec:real}

\subsubsection{Ranking Stability}

We demonstrate the use of our ranking method and present the simultaneous CIs produced with the bike sharing dataset \citep{fanaee2014event}, the TreeSHAP FI method, and Holm's procedure. We create 60/40 train/test splits, fit an XGB regression model (default hyperparameters) to the training set ($R^2=0.98$), and evaluate the performance on the test set ($R^2=0.94$). Then, we calculate the base FI values for $n=50$ and $n=1000$ by sampling from the test set. Presenting the CIs for the ranks enables us to compare the stability for different sizes of $n$ (see Figure \ref{fig:bike_base_size}). The triangles within the CIs are the observed global FI values. The process of constructing the CIs for $n=50$ base FI values is described in Appendix \ref{app:full_example}.

% Figure environment removed

\subsubsection{Training Stability}

Our base-to-global framework can also be used to quantify the uncertainty in training stemming from the sampling of the training set. Here, we use the COMPAS dataset \citep{angwin2016machine}, an RF classification model (default hyperparameters), the PFI method, and the Min-P procedure. We define the base FI values as global PFI values. Each trained model produces a base FI vector $\basevalue_i$; the global FI values are obtained by resampling and training multiple equivalent models (with the same hyperparameters and size of $\sampleset_{train}$ ($M=3K$) and similar training accuracy ($0.883 \pm 0.005$)). We use the same explanation set ($N=600$) to calculate the importance values. Figure \ref{fig:compas_training} presents the true ranks' CIs for two values of $n$. The observed uncertainty in the ranking for $n=10$ indicates that the randomness in sampling can influence the learned mapping  between the features and the target variable.

% Figure environment removed


\subsubsection{High-Dimensional Data}

In previous sections (Sections \ref{sec:comparison} and \ref{sec:shap_simulated}), we analyzed the validity of our ranking method in multiple settings, including with moderately high-dimensional data ($p=50$), and showed that our method maintains simultaneous coverage. Now we demonstrate the use of our ranking method with high-dimensional data, utilizing the Nomao dataset \citep{misc_nomao_227}, which consists of 118 input features and a binary target variable. We create 60/40 train/test splits, fit an XGB classification model (with the default hyperparameters) to the training set ($accuracy=0.99$), and evaluate the performance on the test set ($accuracy=0.97$). Then, we calculate the base FI values with TreeSHAP; the distribution of the global FI values is shown on the left side of Figure \ref{fig:big_data}. Thirty-one features have importance values of zero.

We use our ranking method to rank all the features from the least important (1) to the most important (118); the full ranking is presented in Appendix \ref{app:high-dim} (Figure \ref{fig:high_dim_ranking}). As 31 features have the same importance value of zero, the CI of each feature is $[1, 31]$. In such a case, if we measure the efficiency of the ranking across all features, the long CIs of the irrelevant features will affect it. The skewness of the CI length is presented on the right of Figure \ref{fig:big_data}. A model will likely use some features, and the unused features will get a low importance value (or a value of zero); a filtering step is required to improve the ranking process by comparing only the relevant features.

% Figure environment removed