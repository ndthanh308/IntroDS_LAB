\section{Quantifying Uncertainty in Global FI Values}\label{sec:framework}

\subsection{Terminology}

Consider the supervised learning task of predicting a real-value outcome $Y \in \mathcal{Y}$ from a vector of $p$ features $X = (X_1,\ldots,X_p) \in \mathcal{X}$.
A prediction model $f: \mathcal{X} \rightarrow \mathcal{Y}$ is trained on a training set 
$\sampleset_{train} =  \{(x_i, y_i)\}_{i=1}^M$ and fits the data well according to standard metrics (e.g., MSE or accuracy on external test sets). 
Stakeholders are then interested in the extent to which a feature contributes to the model's performance or predictions -- the FI value.

\subsection{Base-to-Global Framework}

Post-hoc global FI methods describe the average behavior of the model. These methods produce an importance value for each feature, $\imp_1, \imp_2, \ldots, \imp_p \in \real$, based on a trained model $f$ and a sample $\sampleset_{explain} = \{(x_i, y_i)\}_{i=1}^N$, preferably independent of $\sampleset_{train}$. In most methods, the assumption is that a higher value of $\imp_j$ indicates greater importance. Generally, the features are ranked according to their FI values, and only the top-$k$ features are considered.

In many cases, the FI values are calculated by averaging many independent runs. For example, in SHAP \citep{lundberg2017SHAP}, the global FI value is an average of the absolute values assigned to each observation (the local SHAP values). Variability in the explanation set $\sampleset_{explain}$ introduces uncertainty into the global FI values.\footnote{This paper only considers the exact computation of SHAP values without approximation.} In PFI \citep{breiman2001random}, the global FI value is the average obtained over multiple permutations. In this case, both variability in the explanation set and the randomized permutations introduce uncertainty into the global FI values.

In considering how these examples could be addressed in a single framework, we make the following observation: there is a two-level FI hierarchy in which the observed \emph{global} FI value is an average of independent \emph{base} FI values; in the first example (SHAP), the base FI values correspond to the local SHAP values, and in the second example (PFI), the base FI values correspond to the PFI values calculated for a single permutation on the full explanation set.

We set the following notations: matrix $\basematrix_{n \times p}$ is defined as the matrix of \emph{base} FI values,
with rows  $\basevalue_1, \ldots, \basevalue_n \in \real^p$ representing the FI value for each feature.\footnote{If the base FI values are the local values, $n = N$ is the size of $\sampleset_{explain}$.}
$\basematrix_j$ are the columns of the matrix referring to the base FI values for the $j$'th feature. The observed global FI value for the $j$'th feature is $\imp_j =\frac{1}{n} \sum \basevalue_{ij}$.

\paragraph{SHAP Example} For a single observation $(x, y)$, the local SHAP value of a feature $j$ is:
\begin{align*}
    &\phi_j = \sum_{S \subseteq \features \setminus \{j\}} \frac{|S|!(p - |S|-1)!}{p!} \times \\
    &\Big(\mathbb{E}[f(X)|X_{S \cup \{j\}}=x_{S \cup \{j\}}]
    - \mathbb{E}[f(X)|X_S=x_S]\Big)
\end{align*}

where $\features$ is the set of all features, and $S$ is a subset of features.
The base FI value is:
\begin{equation}\label{eq:local_shap}
    \basevalue_j^{SHAP} = |\phi_j|,
\end{equation}
and the global FI value is: $\imp_j^{SHAP} =  \frac{1}{n}\sum_{i=1}^n \basevalue_j^{SHAP}$.

\paragraph{PFI Example} Let $L$ be a loss function; the global PFI value of a feature $j$ is:
\begin{align*}
    \imp_j^{PFI}=\frac{1}{B}\sum_{b=1}^B L(f(X_{[j]}^b), Y) - L(f(X), Y),
\end{align*}
where $X_{[j]}$ is a replication of $X$ with a permuted version of the $j$'th feature, and $B$ is the number of permutations. The base FI value can be defined either as a single permutation of the $j$'th feature: $\basevalue_j^{PFI}=L(f(X_{[j]}), Y) - L(f(X), Y)$ (here the number of base FI values is the number of permutations ($n=B$)) or as the average of permutations for an observation:
\begin{equation}\label{eq:local_pfi}
    \basevalue_j^{PFI} = \frac{1}{B}\sum_{b=1}^B
    L(f(x_{[j]}^b), y) - L(f(x), y),
\end{equation}
where $x_{[j]}$ is a replication of observation $x$ with a permuted version of the $j$'th feature. Here, the number of base FI values is the number of observations ($n=N$). A detailed analysis of the sources of uncertainty in PFI is provided in Appendix \ref{app:pfi_var}.


\subsection{Uncertainty in Feature Ranking}

Global FI values are often interpreted as a ranking used to highlight or select the most relevant features. Since different FI methods produce FI values of varying scales, the ranking of the features is often used to compare the methods' output. The observed ranks $\hat{r} = (\hat{r}_1, \ldots, \hat{r}_p)$, $\hat{r}_j \in \{1,...,p\}$ are typically derived directly from the observed global FI values, with the rank $p$ assigned to the highest global FI value, and rank $1$ assigned to the lowest.

The sampling of the base FI values introduces uncertainty into the global FI values. The global FI values are then ranked, propagating the uncertainty into the observed ranks. This process is summarized in Figure \ref{fig:framework} which presents the framework's pipeline for quantifying the uncertainty in the observed ranks.


% Figure environment removed

Note that the definitions of the base and global FI levels specify the source of the uncertainty to be reflected in the CIs for the ranks; two options for base FI value definition for PFI are presented above.