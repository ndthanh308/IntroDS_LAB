\section{Evaluation}\label{sec:experiments} 

This section will demonstrate our framework and ranking method for TreeSHAP and PFI as described in Sec. \ref{sec:unifying}. We will follow the definition of the observation level as base FI; see Eq. \ref{eq:local_shap} and \ref{eq:local_pfi}.

\subsection{Experimental setup}

\subsubsection{Synthetic data} We use synthetic data to check the validity and efficiency of our method. For validity, we need to identify the true global FI values. We used the following two approaches:
\begin{enumerate}[label=(\alph*)]
    \item \emph{Mock importance values} - we simulated the base FI values from a multivariate normal distribution with predetermined means and covariance matrices.
    \item \emph{Low-variance importance values} - we built a prediction model and calculated $\imp_1, \ldots \imp_p$ based on a sufficiently large sample as a low variance estimator of $\trueimp_1, \ldots \trueimp_p$ \cite{slack2021reliable}.
\end{enumerate}


\paragraph{Prediction model} We used the XGBoost model \citep{chen2016xgboost} in all experiments, due to its computational efficiency and performance on high-dimensional tabular data.


\paragraph{Metrics}
We use the the following metrics suggested by~\cite{al2022simultaneous}:
\begin{itemize}
    \item \emph{Simultaneous coverage:} The proportion of experiments where all true ranks are covered by their confidence intervals.
    \begin{equation*}
        1 \text{ if all } \{\trueimp_j \in [L_j, U_j]\} \text{ else } 0
    \end{equation*}
    \item \emph{Efficiency:} the average relative size of the confidence intervals:
    \begin{equation*}
        \frac{1}{p \cdot (p-1)}\sum_{j=1}^p (U_j - L_j).
    \end{equation*}
    Lower efficiency is better.
\end{itemize}

\subsubsection{Real data} 

We experimented with two tabular datasets for classification and regression tasks; the Bike sharing dataset \cite{fanaee2014event}, and the COMPAS dataset \cite{angwin2016machine}. We used XGBoost regressor \citep{chen2016xgboost} for the bike dataset and Random Forest (RF) classifier \cite{breiman2001random} for the COMPAS dataset. We created 60/40 train/test splits; a prediction model was fitted to the training data and evaluated on the test data to ensure reasonable prediction accuracy. Then, FI values were calculated by sampling from the test set. See App. \ref{app:real_data} for details about the datasets and the models' performances.


\subsection{Results}

\subsubsection{Synthetic data}

\paragraph{Mock data: Efficiency compared to ICRanks}
We compared our efficiency to the ICRanks \cite{al2022simultaneous} method, for different number of features ($p$) and base FI values ($n$). The results are in Fig. \ref{fig:mock_p}. As the level of correlation between features increases, our method is more efficient than ICRanks.


% Figure environment removed


\paragraph{Simulated FI: Efficiency comparing FI methods}
Here we study the efficiency under multiple simulation conditions, varying the FI algorithm (TreeSHAP and PFI), $p$, number of relevant features, and $n$. Fig. \ref{fig:shap_pfi_simulated} shows the results. In general, the ranking is inefficient with a small number of important features, since the noisy features may be indistinguishable inducing large true rank-sets.


% Figure environment removed


\paragraph{Simultaneous coverage}
In both the mock FI and model FI, the ranking maintains simultaneous coverage levels of almost 1.
This indicates that the method is overly conservative compared to the nominal required simultaneous coverage of $1-\alpha = 90\%$.

A detailed description of the synthetic data and additional results are at App. \ref{app:sim_imp}.


\subsubsection{Real data}


\paragraph{Number of base FI values} The global FI values are an aggregation over the base FI values ($n$). As we increase $n$, we expect the global values to be more stable; see Fig. \ref{fig:bike_freq} that support that expectation. We measure the stability of a rank as the frequency of the most common rank of the feature across repeated rankings. Moreover, our ranking visualization makes it straightforward to compare the level of uncertainty across different sizes of $n$; see Fig. \ref{fig:bike_base_size}.


% Figure environment removed


% Figure environment removed


\paragraph{Top-$k$ important features} The simultaneity of our ranking allows selecting a subset of $k$ features. We suggest that there might be more than $k$ possible top-$k$ important features; all subset of $k$ features out of the super-set of possible top-$k$ should result in a group of prediction models with the same score. For example, in Fig. \ref{fig:compas_rank}, two features are the third most important candidates. Fitting a model for all possible subsets shows similar scores. If there is more than one option to select $k$ features, one can choose features while assessing secondary considerations, such as the percentage of missing values in a feature.

% Figure environment removed