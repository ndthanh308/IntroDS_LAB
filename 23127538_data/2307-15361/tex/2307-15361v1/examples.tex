\section{Base to global FI: examples}\label{sec:unifying}

Here, we will introduce examples of FI methods in view of how post-hoc global importance values are calculated from the base values.

\subsection{Tree SHAP}

TreeSHAP is a post-hoc local FI method for tree-based ML models. It is a model-specific version of SHAP values \cite{lundberg2017SHAP}, that exactly computes the Shapley values from cooperative game theory \cite{shapley1953value}. SHAP is a method to compute Shapley values for the conditional expectation of the predictions of the model $\mathbb{E}[f(X)|X_S=x_S]$. For a tree model the conditional expectation can be estimated recursively based on the tree structure and therefore allow to compute the SHAP values in polynomial instead of exponential time. The TreeSHAP method is exact and not stochastic.

For a single observation $(x, y)$ the local TreeSHAP value of a feature $j$ is:
\begin{equation*}
\begin{split}
    \tilde{\localimp}_j^{Tree SHAP} = \sum_{S \subseteq \features} \frac{|S|!(p - |S|-1)!}{p!} \Big(\mathbb{E}[f(X)|X_S=x_S] \\
     - \mathbb{E}[f(X)|X_{S \setminus \{j\}}=x_{S \setminus \{j\}}]\Big)
\end{split}
\end{equation*}
Where $\features$ is the set of all features and $S$ is a subset of features.

The \textit{global} TreeSHAP averages over the absolute-value of all observations in a sample of size $n$:
\begin{equation*}
    \imp_j^{TreeSHAP} =  \frac{1}{n}\sum_{i=1}^n |\localimp_j^{TreeSHAP}|
\end{equation*}
Formally, the \textit{base} explanation is:
\begin{equation}\label{eq:local_shap}
    \localimp_ij^{TreeSHAP} = |\tilde{\localimp}_ij^{TreeSHAP}|.
\end{equation}

\subsection{PFI}

PFI \cite{breiman2001random, fisher2019all} is commonly used as a model-agnostic post-hoc global FI method. The FI is defined as the increase in a model loss when a single feature value is randomly permuted.

Let $L$ be a loss function. The common definition of PFI is the \textit{global} level:
\begin{equation*}
    \imp_j^{PFI}=\frac{1}{B}\sum_{b=1}^B L(f(X_{[j]}^b), Y) - L(f(X), Y)
\end{equation*}
Where $X_{[j]}$ is a replication of the data matrix $X$ with $N$ rows ($\mathcal{D}_{explain}$), and a permuted version of the $j$'th feature.

Here are two examples of definition of \textit{base} level PFI:
\begin{itemize}
    \item A single permutation of the $j$'th feature: $\localimp_j^{PFI}=L(f(X_{[j]}), Y) - L(f(X), Y)$. In this case $n$, the number of base values is $B$.

    \item A single observation:
    \begin{equation}\label{eq:local_pfi}
        \localimp_j^{PFI} = \frac{1}{B}\sum_{b=1}^B L(f(x_{[j]}^b), y) - L(f(x), y),
    \end{equation}
    where $x_{[j]}$ is a replication of the observation $x$ with a permuted version of the $j$'th feature. Here, $n=N$, the number of observations in $\mathcal{D}_{explain}$.
\end{itemize}


Note that the base and global FI levels define the source of uncertainty to be measured by the ranks. For example, by averaging multiple permutations, the primary source of uncertainty is the number of permutations ($B$), while by averaging over observations, the primary source of uncertainty is the sample size $N$.