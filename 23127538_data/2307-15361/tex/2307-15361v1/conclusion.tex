\section{Conclusions}
We propose a \emph{base-to-global} framework, and a method for constructing confidence intervals for the true ranking of FI values. Because ranks are so often used for summarizing FI methods, we think it is crucial to measure and report the stability of the rankings. For sources of uncertainty that can be explicitly modeled, such as the size of the explanation set, we can design rigorous criteria for reporting uncertainty, as seen in this paper. Other sources of instability, such as the difference between FI methods, should also be addressed, even if their effect is harder to quantify. We view this method as a step to producing new types of stability checks in explainable machine learning.

Our current algorithm is conservative, as demonstrated in simulations where the coverage level surpasses the requested 90\%. Future research aims to narrow the confidence intervals while maintaining nominal coverage. One reason for this conservativeness is the use of the Holm algorithm on the one-sided hypothesis. Holm sets bounds assuming the hypotheses are independent, whereas we have both dependence between the pairs of one-sided hypotheses and dependence in the pairwise testing. We can seek to reduce the impact of the number of features on coverage by using a filtering step. Finally, we would like to enhance practitioner confidence through the implementation of robust tests, which currently do not appear to affect coverage.
