\section{Confident simultaneous feature ranking}\label{sec:conf_rank}

In this section, we introduce our ranking method which is designed to rank FI values while taking into account the uncertainty associated with the post-hoc FI method and the sampling process. Using our \emph{base-to-global} framework, we are able to quantify the uncertainty via simultaneous confidence intervals for the ranks.

\subsection{Feature ranking}

Our method uses pairwise hypothesis tests to form lower and upper bounds of the true rank of each feature.

For each feature pair $j,k$, we form two hypothesis tests:
\begin{align*}
H_{jk}^0:& \trueimp_j \geq \trueimp_k \text{ versus }  H^1_{jk}:  \trueimp_j < \trueimp_k,\\
H_{kj}^0:& \trueimp_k \geq \trueimp_j \text{ versus }  H^1_{kj}: \trueimp_k < \trueimp_j.
\end{align*}

The paired test takes vectors $\mathbf{v}_j, \mathbf{v}_k$ and returns a p-value $p_{j,k}$. Denote as
\begin{equation*}
p_{j,k} = pairedOnesidedTest(\mathbf{v}_j, \mathbf{v}_k, <).
\end{equation*}
A partial ranking is obtained by comparing $p_{j,k}$ to a threshold $\alpha$.
In practice, when both tests use the same data and the threshold $\alpha$ is less than $1/2$,  the two null hypotheses will not be rejected together.

To see the relationship between the hypothesis tests and the ranking, consider a feature pair $j,k$ for which we can reject with high probability 
the hypothesis $H_{jk}: \trueimp_j \geq \trueimp_k$. Then, with a high probability $\trueimp_j < \trueimp_k$, a partial ranking of the global FI. In that case, we conclude w.h.p. that feature $j$ is not ranked highest ($u_j < p$) and feature $k$ is not ranked lowest ($l_k > 1$).
Our method is based on \cite{al2022simultaneous}, and we discuss the differences in Sec. \ref{subsec:alg}.


\subsection{Controlling partial ranking error}

We would like to identify intervals that are simultaneous and therefore would still be valid under selection. To achieve this, we will look to control the FWER of all partial rankings.

Define $D$ the set of partial rankings from all pairwise tests 
$D \{(j,k) : H_{jk}^0 \text{was rejected} \}$
 
%\footnote {It is more common to use ``reject the null`` rather than ``accept the alternative''. However, here, we indeed have complimentary sets, hence rejecting $H_0$ means accepting $H_1$.} 
We make an error if there is a pair $(j',k')\in D$ for which $\trueimp_{j'}\geq \trueimp_{k'}$.
A level $\alpha$ calibrated test\footnote{See definition of calibrated test in App. \ref{app:proof}} assures that the probability of this event is less than $\alpha$. However, for simultaneous confidence intervals, we need a stronger condition.

\begin{definition}(Family wise error rate)
The pairwise tests  $\{pairedOnesidedTest(\mathbf{v}_j, \mathbf{v}_k), <\}$
 control the family wise error rate (FWER) at level $\alpha$ if:
 \begin{align*}
     P(\text{Making any error}) \leq \alpha.
 \end{align*}
\end{definition}
Indeed, the probability of making at least one error increases with more tests, and thus we need more conservative tests to control the FWER.


\subsection{Confident simultaneous feature ranking}\label{subsec:alg}

Once a family of pairwise tests is available with FWER control, we derive the simultaneous confidence rank intervals:

\begin{theorem} [Al Mohamad et al, \citep{al2022simultaneous} ]\label{thm}
Let $D$ be the set of partial rankings with FWER control at level $\alpha$. For $j = 1,...,p$, define 
\begin{align*}
    L_j = 1 + \#\{k : (k,j) \in D\}, \\
    U_j = p - \#\{k : (j,k) \in D\}.
\end{align*}
Then the sets $\{[L_j, U_j]$ for $j \in \features\}$ are simultaneous $(1-\alpha)$ confidence intervals for the ranks of the true global FI values.
\end{theorem}
The construction naturally extends the definition of rank-set used in Def. \ref{def:setrank}. The idea in this proof is that a coverage failure means that the set of true (one-sided) differences was smaller than the set of (one-sided) observed differences. This means that at least one partial ranking was an error. Therefore the FWER upper bounds the probability of an error in the confidence intervals. See proof in App. \ref{app:proof}.


Alg. \ref{alg} describes the complete method. The algorithm works directly on the base FI matrix without requiring access to the trained model, FI method, or the explanation data set. The main assumption is that our paired test is calibrated for the family of possible distributions for base FI values.

\begin{algorithm}
\caption{Confidence intervals for rank-sets}\label{alg}
\begin{algorithmic}
\REQUIRE 
\STATE $\mathbf{v}$: base FI values;
\STATE $1 - \alpha>0$: level of confidence;
\STATE $pairedOnesidedTest$: suitable paired test;
\FOR {$j,k \in \features, j \neq k$} {
\STATE    $p_{j,k} \gets pairOnesidedTest(\mathbf{v}_j, \mathbf{v}_k, <)$.
}\ENDFOR
%\Comment{Adjust the p-values $\{p_{j,k}\}_{j<k}$ to control the family-wise error rate}

\STATE $\{p_{j,k}^{adj}\}_{j\neq k} \gets Holm (\{p_{j,k}\}_{j\neq k} )$
\STATE $D \gets  \{(j,k) : p_{j,k}^{adj}< \alpha\}$

\FOR {$j \in \features$} {
\STATE $L_j \gets 1 + \#\{k : (k,j) \in D\}$
\STATE $U_j \gets p - \#\{k : (j,k) \in D\}$
}\ENDFOR

\RETURN $[L_1, U_1], \ldots, [L_p, U_p]$.
\end{algorithmic}
\end{algorithm}

\subsection{Implementation details}

\paragraph{Paired test} We use the parametric paired t-test:
Set $\mathbf{d} = \mathbf{v}_j - \mathbf{v}_k$ to be the vector of differences, 
write $\bar{d}$ for the sample average and $s_d$ for the sample standard deviation. Then the one-sided $\alpha$ level test rejects the null hypothesis if  $|\bar{d}/ (s_d/\sqrt{n})| >  T_{n-1}(1-\alpha)$, 
where the r.h.s. marks the $1-\alpha$ quantile of student-t ($n-1$ df).
The paired t-test is fairly robust to departures from normality\cite{posten1979robustness}.

\paragraph{Correction to multiple comparisons} We use the sequential Holm \cite{holm1979simple} procedure to adjust (increase) the p-values. The adjusted p-values are then compared to the requested level $\alpha$; if the p-values are calibrated, the FWER for the rejected tests (after Holm) is controlled at level $\alpha$, regardless of dependence. We currently implement the Holm on the one-sided tests, though for approximately Normal data the two-sided Holm probably still works well. See Shaffer \cite{shaffer1980control, shaffer1995multiple} on using Holm for pair-wise comparisons.

\paragraph{Comparison to ICranks} Algorithm \ref{alg} is similar to the ICranks \cite{al2022simultaneous}, with some differences. In ICranks, they used the Tukey correction \cite{tukey1953problem} for the normal distribution to control all differences between ranks simultaneously. It assumes normality and independence between features and is hard to replace with non-parametric tests. In contrast, our algorithm applies tests to each feature pair and uses the Holm correction; hence, it can be used with robust or non-parametric location tests \cite{wilcox2011introduction}. 
%\begin{lemma}
%    \todo[inline]{Holm's methods controls FWER ...}
%\end{lemma}

