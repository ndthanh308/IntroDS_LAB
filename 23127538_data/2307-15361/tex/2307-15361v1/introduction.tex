\section{Introduction}\label{sec:introduction}

Complex non-linear prediction models are widely used to augment or even replace human judgment in fields such as healthcare \cite{bhardwaj2017study} and finance \cite{rundo2019machine}. Regulators, users, and developers of such models are interested in understanding the relative contribution of the different inputs - the features - toward the model's predictions \cite{preece2018stakeholders, regulation2016regulation}.
%To identify important features for model improvement, to analyze the model's behavior and detect biases, to protect privacy, or ensure fairness \citep{goodman2017european}. 
Feature importance (FI) methods such as Permutation Feature Importance (PFI, \cite{breiman2001random}) and SHapley Additive exPlanation (SHAP, \cite{lundberg2017SHAP, lundberg2019Tree}) measure feature contributions by estimating the effect of removing or perturbing the feature on the predicted value or the prediction loss. The specifics of this manipulation change across different methods and across implementations\cite{merrick2020explanation, covert2020imputation, fisher2019all}.

%Feature importance algorithms have been proposed  \cite{arrieta2020explainable, murdoch2019interpretable, guidotti2018survey}; The focus of this work are post-hoc methods such as Permutation Feature Importance \cite{breiman2001random} and SHAP \cite{lundberg2017SHAP, lundberg2019Tree}. The post-hoc methods are designed to evaluate the contribution of each feature to the predictions of a pre-trained model, either at the level of a single instance or at the data set level. 

%Interpreting the results of feature importance methods is hard, because the details in the definition and implementation of the methods greatly affect the computed values.


Recently, studies have demonstrated that post-hoc FI methods, which are employed to explain trained prediction models can be unstable. Sources for this instability include the example set used for constructing the FI, randomness in the perturbations or approximations, and the selection of hyperparameters \cite{alvarez2018robustness, lakkaraju2020robust, mishra2021survey}. 
We focus on uncertainty in sampling the explanation set, affecting the stability of the global importance values. Most methods for quantifying these types of uncertainty produce per-feature spread estimates (or confidence intervals) in the units of the FI method. These uncertainty measures are insufficient because it is often the \emph{rank of the feature importance}, rather than the value itself, which affects the decision-making and reporting on the features.
In our analysis of Nature-group journal articles from 2021 mentioning ``feature importance'', we found that all the articles either reported only the identity of the top-k ranked features \cite{heldt2021early} or referred to the relative FI values as an implicit ranking of the features \cite{jaxa2021sources}. 

The instability in importance values can lead to instability in their ranks; see Fig. \ref{fig:intro} for an example. However, the discrete nature of ranks requires different tools for estimating and reporting their uncertainty. We propose reporting confidence intervals so that with high probability, each feature's \emph{true rank-set} is covered by the appropriate interval. By \emph{true rank-set}, we mean the features' ranking obtained from an infinite sample for a trained prediction model and an FI method. We construct the intervals by testing all pairs of features for differences in means and counting the number of rejections for each feature.

% Figure environment removed


Through experimental evaluation using both synthetic and real-world datasets, we demonstrate the validity of our methodology. Our findings confirm its effectiveness and highlight its potential to detect and enhance ranking stability.