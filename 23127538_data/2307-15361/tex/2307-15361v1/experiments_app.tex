\section{Experiments Details and Results}\label{app:exp}

\subsection{Synthetic data}\label{app:sim_imp}

\subsubsection{Feature ranking - mock importance values}

\paragraph{Data description}
For this experiment, we generate multivariate-normal data with off-diagonal correlations between the features and multiple levels of correlation $\rho \in [0.1, 0.3, 0.6, 0.9]$. The variance of the features was sampled from chi-squared distribution and was scaled by different factors: 0.2, 1, 5. The structure of the means vector is: $\mu=(1, \ldots, p+1)^{\beta}$, where $\beta$ gets different values that affects the spread of the means: 0.5 (most spread), 0.25, 0.1 (most compact). In general, the ranking maintains simultaneous coverage levels of almost 1, and the efficiency worsening as the variance of the features increases.


Fig. \ref{fig:p_mu_exp} is a detailed version of Fig. \ref{fig:mock_p}, separated by the different values of the exponent.

% Figure environment removed


In addition, we analyzed the efficiency as a function of the number of observations ($n$), see summary in Fig. \ref{fig:mock_n} and the breakdown to different values of $\mu$ exponent in Fig. \ref{fig:n_mu_exp}. To evaluate the efficiency and simultaneous coverage of the ranking methods, we sampled 100 independent explanation sets for each configuration and reported the average across repetitions.

% Figure environment removed

% Figure environment removed



\subsubsection{Feature ranking - TreeSHAP and PFI importance values}\label{app:act_imp}

Here we simulate a regression task with $X$ sampled from the following synthetic data: a multivariate normal distribution, with $\mu=(1, \ldots, p+1)^{0.5}$, a block-wise pairs correlation matrix with $\rho=0.3$, and a chi-squared variance. In addition, we analyzed a different number of total features ($p=15$ and $p=50$) with varying numbers of important features.
The value of $Y$ was calculated by combining the important features linearly and then applying one of (a) adding interactions or (b) transforming the linear combination with a trigonometric function. Finally, we added noise to $Y$ and standardized it.

For each configuration of $(X, Y)$, an XGBoost model was trained and tested (70/30) on 500,000 observations. Then, the true importance values were computed with TreeSHAP and PFI using 50,000 observations for $p=50$ and 10,000 for $p=15$.

To measure our ranking method as a function of the number of base importance values ($n$), we repeatedly (100 times) sample data from the same distribution for each value of $n$, compute the local TreeSHAP and PFI values, and ranked the features with our method. Then, we measure the efficiency and coverage. The reported results are the average across repetitions.

In Fig. \ref{fig:shap_pfi_simulated} we presented the results for a linear $Y$ with interactions. In Fig. \ref{fig:shap_pfi_trigo} you can see the results for the trigonometric $Y$.

% Figure environment removed


\subsection{Real data}\label{app:real_data}

\paragraph{Bike sharing \cite{fanaee2014event}} The data contains 10886 rental bike counts between 2011 and 2012 from the Capital Bikeshare program in Washington, D.C. The regression task is forecasting demand for bike rentals based on time and environmental measures such as month and weather. We created a 60/40 train and test sets and trained an XGBoost regression model (scikit-learn API for XGBoost regression with default hyperparameters; train and test $R^2$ of 0.98 and 0.94, respectively).

\paragraph{COMPAS \cite{angwin2016machine}} The data contains 6172 records of criminal history from Broward County from 2013 and 2014. The classification task is assess a criminal defendantâ€™s likelihood to re-offend based on jail and prison time, and demographics. We created a 60/40 train and test sets and trained an RF classification model (scikit-learn API for XGBoost random forest classification with default hyperparameters; train and test mean accuracy of 0.87 and 0.85, respectively).