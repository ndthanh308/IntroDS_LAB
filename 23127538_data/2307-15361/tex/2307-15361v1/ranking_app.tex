\section{Definitions and proofs}

\subsection{Calibration}

Let $X\sim F_\theta$ be a family of distributions parameterized by $\theta \in \Theta$, and let $\Theta_0 \subset \Theta$.
\begin{definition}(A calibrated test)
The p-value $p(X)$ for the test $H_0: \theta \in \Theta_0$
is calibrated if for any $\theta \in \Theta_0$, and $\alpha < 1$ the test $p(X)< \alpha$ controls the type 1 error at level $\alpha$.
Specifically:
$$P_{\theta\in \Theta_0} \left( p_{j,k} < \alpha\right)   < \alpha,\,\, \text{for any } \alpha < 1 , \theta\in \Theta_0. $$
\end{definition}
Whether a test is calibrated depends on the assumptions of the test and the family of distributions associated with the null hypothesis ($H_0$).

\subsection{Holm procedure}
We describe the Holm procedure in terms of the the adjustment of the p-values. 

Take a set of K $p$-values $p_1,...,p_K$,
and sort them $p_{{1}}\leq,...,\leq p_{(K)}\leq 1.$

Then \\
$p^{adj}_{(1)} = K\cdot p_{{1}},$\\
$p^{adj}_{(2)} = \max\{p^{adj}_{1}, (K-1)p_{{(2)}}\},$\\
so that the $k$'th p-value \\
$p^{adj}_{(k)} = \max\{p^{adj}_{(1)},...,p^{adj}_{(k-1)} (K-k+1)p_{{(k}}\},$ \\
and\\
$p^{adj}_{(K)} = \max\{p^{adj}_{(1)},...,p^{adj}_{(K-1)}, p_{(K)}\}$.\\

Notes: 
\begin{itemize}
\item After adjustments, the p-values are compared to a chosen level $\alpha$. Note that all p-values are inflated compared to their original level, making it less likely to reject the null hypothesis. 
\item Furthermore, the p-values keep their relative order after adjustment. This is governed by the $\max$ function, which assures the order is kept. The resulting process is sequential, in that for a given level $\alpha$, after the first non-rejected value, all others would not be rejected. 
\end{itemize}

\subsection{Proof of Theorem \ref{thm}}
\label{app:proof}
Recall that $D$ is the set of partial rankings, and that we assume that the probability of any error in $D$ is less than $\alpha$.
To prove the theorem, we first show that any error in coverage of a rank confidence interval must be caused by at least one partial ranking error in $D$:

Suppose that there is a coverage error for the set of confidence intervals, and without loss of generality assume that the coverage error occurs for feature 1, 
$$ [\ell_1, u_1] \nsubseteq [L_1,U_1].$$
The coverage can break on one or both sides:

\begin{enumerate}
    \item $\ell_1 < L_1$
    \item $u_1 > U_1$
\end{enumerate}

If (1) then $L_1 > 1$, and there are $L_1-1>0$ pairs of the type $(1,k) \in D$. (Meaning, there are $L_1-1$ features that the test found to be significantly smaller in FI compared to feature 1).
However, according to Definition \ref{def:setrank}, $\ell_1-1 = \# \{k: \trueimp_1 > \trueimp_k\}$, meaning there are only $\ell_1-1$ which are truely smaller than feature 1.
Combining these two statements together, there must be at least one feature $k = 2,...,p$ for which 
$(1,k) \in D$ but $\trueimp_1 \ngtr \trueimp_k$, meaning there is a partial ranking error in $D$.

If (2) this means that the set $\{k \in 2,...,p : (k,1) \in D\}$ is 
larger than the set $\{k: \trueimp_k>\trueimp_1\}$. Again, this would mean that at least for one value of $k$ we have a partial ranking error. 


Then the event of at least one coverage error is contained in the event of getting a partial ranking error. So the probability of coverage error is bounded by the $FWER = \alpha$.