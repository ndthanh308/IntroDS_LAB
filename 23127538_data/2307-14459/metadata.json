{
  "title": "Training Quantum Boltzmann Machines with Coresets",
  "authors": [
    "Joshua Viszlai",
    "Teague Tomesh",
    "Pranav Gokhale",
    "Eric Anschuetz",
    "Frederic T. Chong"
  ],
  "submission_date": "2023-07-26T19:00:10+00:00",
  "revised_dates": [],
  "abstract": "Recent work has proposed and explored using coreset techniques for quantum algorithms that operate on classical data sets to accelerate the applicability of these algorithms on near-term quantum devices. We apply these ideas to Quantum Boltzmann Machines (QBM) where gradient-based steps which require Gibbs state sampling are the main computational bottleneck during training. By using a coreset in place of the full data set, we try to minimize the number of steps needed and accelerate the overall training time. In a regime where computational time on quantum computers is a precious resource, we propose this might lead to substantial practical savings. We evaluate this approach on 6x6 binary images from an augmented bars and stripes data set using a QBM with 36 visible units and 8 hidden units. Using an Inception score inspired metric, we compare QBM training times with and without using coresets.",
  "categories": [
    "quant-ph",
    "cs.LG"
  ],
  "primary_category": "quant-ph",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.14459",
  "pdf_url": null,
  "comment": "Appeared in IEEE International Conference on Quantum Computing and Engineering (QCE22) in September 2022",
  "num_versions": null,
  "size_before_bytes": 490019,
  "size_after_bytes": 409146
}