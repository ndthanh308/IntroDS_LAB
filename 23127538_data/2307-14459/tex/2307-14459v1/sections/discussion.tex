In this work we proposed using coreset techniques to reduce training times of Quantum Boltzmann Machines (QBMs). Coresets have already been used to reduce training times of classical machine learning models~\cite{mirzasoleiman2020coresets, sinha2020small}. In the case of QBMs, however, training is bottlenecked by Gibbs state sampling and so training time is equivalent to how often this sampling is performed. Since Gibbs state sampling is believed to be a promising use case for quantum computers~\cite{poulin2009sampling}, reducing the amount of times this sampling is needed equates to reducing the runs of the quantum hardware. In a scenario where computational time on a quantum computer is a precious resource, we propose that this might lead to substantial practical savings. Additionally, in a regime of noisy quantum computers with imperfect Gibbs state sampling algorithms, this reduction might also lead to less noisy results and better trained QBMs.

We performed initial numerical experiments exploring this direction. Although we expected to see similar results as work that used coresets for classical machine learning models~\cite{mirzasoleiman2020coresets, sinha2020small}, we find that, as shown in Figure~\ref{fig:results}, all approaches learn at the same rate. One possible explanation is that although we tried to maximize the problem size we could work with given our resources, the problem dimensionality and data set size are still too small, and the QBM trivially fits to the first couple mini-batches regardless of data set size. Another possibility is the BXS data set isn't diverse enough to have substantially differing positive phases for batches of size $k = 32$, meaning all mini-batches from the data set elicit roughly the same gradient update of the QBM parameters. Additionally we only look at two unweighted coreset constructions. It's possible they aren't able to coherently summarize the full dataset and enable more effective gradient updates. 

Despite these results, we still think coresets present a promising direction for practically training QBMs, and leave further experiments using weighted coreset constructions with larger models and data sets to future work.