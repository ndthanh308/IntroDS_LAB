Training a Boltzmann Machine requires a binary data set $\boldsymbol{X}=\{\boldsymbol{x_1},\dots,\boldsymbol{x_n}\}$ of size $n$. The task is to learn the probability distribution $P_{data}(\boldsymbol{v})$ corresponding to $\boldsymbol{X}$ where  $\boldsymbol{x}, \boldsymbol{v}$ are bitstrings of length $d$. It's typical to split $\boldsymbol{X}$ into small, constant-sized \textit{mini-batches} $\boldsymbol{B_i}=\{\boldsymbol{b_1},\dots,\boldsymbol{b_k}\}$ of size $k$ where $\boldsymbol{X} = \boldsymbol{B_1} \cup \dots \cup \boldsymbol{B_n}$~\cite{hintonPracticalGuideTraining2012}. 
The training procedure then entails iterating over $\boldsymbol{X}$ while performing a gradient-based update on our model parameters $\boldsymbol{\theta}$ for each mini-batch $\boldsymbol{B_i}$. One iteration through all our mini-batches is an \textit{epoch} and training can continue for enough epochs until $\boldsymbol{\theta}$ has sufficiently converged.

For a QBM, calculating the negative phase is the computational bottleneck since it requires Gibbs state sampling. For each mini-batch we need to calculate $|\boldsymbol{\theta}|$ negative phases. Conveniently since we only calculate $\sigma_z$ expectations, we can estimate all of these negative phases at once by just averaging the measured bitstrings, assuming we can approximately sample from the full Gibbs state. Therefore, we only need to do Gibbs state sampling once per mini-batch. For one epoch through the whole dataset $X$ this equates to $\frac{n}{k}$ instances of Gibbs state sampling. If we instead replace $X$ with a coreset $X'$ of size $m \ll n$ then we can substantially reduce the amount of times we need to perform Gibbs state sampling per iteration through the dataset. A high-level overview of this QBM training loop is shown in Figure~\ref{fig:training}. 

It's important to note that this also means we perform equally less parameter updates per epoch, and so it's necessary to choose $X'$ such that it sufficiently summarizes $X$. However, since constructing such $X'$ has been done successfully for classical machine learning models~\cite{mirzasoleiman2020coresets, sinha2020small}, we suspect it is also feasible for QBMs.

% Figure environment removed