\subsection{Coresets}
A coreset of dataset $\boldsymbol{X} = \{\boldsymbol{x_1}, ..., \boldsymbol{x_n}\}$ is a subset $\boldsymbol{X}'$ with weights $w$ such that $(\boldsymbol{X}',w)$ can be used as a proxy for $\boldsymbol{X}$ when solving some problem of interest on $\boldsymbol{X}$, (e.g. clustering). Classically, coresets have been proposed as a tool for solving optimization problems in settings where using the whole dataset is prohibitive or computationally intractable~\cite{agarwal2005geometric}. In classical machine learning, coreset techniques have been successfully used to reduce training times by creating coresets with gradients that closely approximate that of the full dataset ~\cite{mirzasoleiman2020coresets, sinha2020small}.

A variety of algorithms exist for constructing coresets~\cite{phillips2016coresets,campbellBayesianCoresetConstruction2018, campbellSparseVariationalInference2019}. Although recent work has proposed and explored using coresets for quantum algorithms where encoding the whole dataset is costly or infeasible~\cite{harrowSmallQuantumComputers2020, tomeshCoresetClusteringSmall2020}. This can be done \textit{statically}, where a classical computer generates the coreset which is then fed to the quantum algorithm and remains unchanged~\cite{tomeshCoresetClusteringSmall2020}. It's also been proposed to do coreset construction \textit{adaptively} where an iterative classical coreset algorithm queries a quantum computer to sample solutions to the problem on a coreset that is built up each iteration~\cite{harrowSmallQuantumComputers2020}.



\subsection{Boltzmann Machines}
One of the first machine learning models for the generative task of learning and sampling arbitrary probability distributions~\cite{hinton1986learning}, \textit{Boltzmann Machines} form the basis of other models such as deep belief networks~\cite{hinton2006fast}, and have been used for speech recognition~\cite{mohamed2011acoustic}, image generation~\cite{hinton2006fast}, and detecting network anomalies~\cite{fiore2013network}.

A Boltzmann Machine is defined by a graph of binary units $z_a$ with biases $b_a$ connected by weighted edges $u_{ab}$. As shown in Figure~\ref{fig:qbm}, units can either be \textit{visible}, representing the input and output data, or \textit{hidden}, representing the model's internals.

A corresponding energy function to the graph is defined: 
\begin{equation}
    E(\boldsymbol{z}) = -\sum_{a}b_a z_a - \sum_{a,b}w_{ab}z_a z_b
\end{equation}
where $\boldsymbol{z}=(z_0,..., z_a,..)$ is a specific state of all the model's units, and $z_a \in \{+1, -1\}$. For convenience we also define $\boldsymbol{z} = (\boldsymbol{v},\boldsymbol{h})$ where $\boldsymbol{v}$ are the visible units and $\boldsymbol{h}$ are the hidden units.
Then, the Boltzmann Machine's learned distribution is the Boltzmann distribution for energy $E(\boldsymbol{z})$ summed over the possible states of the hidden units:
\begin{equation}
    P(\boldsymbol{v}) = \frac{1}{Z}\sum_{\boldsymbol{h}} e^{E(\boldsymbol{v}, \boldsymbol{h})}, Z=\sum_{\boldsymbol{v}}\sum_{\boldsymbol{h}} E(\boldsymbol{v}, \boldsymbol{h})
\end{equation}

The goal of training is to adjust parameters $b_a$ and $u_{ab}$ so that $P(\boldsymbol{v})$ approximates our training data set $P_{\text{data}}(\boldsymbol{v})$. This is equivalent to minimizing the negative log-likelihood
\begin{equation}
    \mathcal{L} = -\sum_{\boldsymbol{v}} P_{\text{data}}(\boldsymbol{v}) \log P(\boldsymbol{v})
\end{equation}
This can be done using a gradient based technique where the gradient with respect to the model parameters $\theta \in \{b_a, u_{ab}\}$ is
\begin{equation}
    \partial_{\theta} \mathcal{L} = \sum_{\boldsymbol{v}}P_{\text{data}}(\boldsymbol{v})\langle \partial_{\theta}E(\boldsymbol{v}, \boldsymbol{h})\rangle_{\boldsymbol{v}} - \langle \partial_{\theta}E(\boldsymbol{v}, \boldsymbol{h})\rangle
\end{equation}
where $\langle \dots \rangle_{\boldsymbol{v}}$, often called the \textit{positive phase}, is the expectation where the visible units are clamped to be the visible state $\boldsymbol{v}$, and $\langle \dots \rangle$, often called the \textit{negative phase}, is the unclamped expectation. 

To minimize $\mathcal{L}$ the model parameters can then be updated by taking a step in the direction of the negative gradient with some step size $\eta$
\begin{equation}
    \delta \theta = -\eta \partial_{\theta} \mathcal{L}
\end{equation}
Expressed for $b_a$ and $u_{ab}$ we have
\begin{equation}
    \delta b_a = -\eta \left(\overline{\langle z_a \rangle_{\boldsymbol{v}}} - \langle z_a \rangle\right),
\end{equation}
\begin{equation}
    \delta u_{ab} = -\eta \left(\overline{\langle z_a z_b \rangle_{\boldsymbol{v}}} - \langle z_a z_b \rangle\right).
\end{equation}
where $\overline{\langle \dots \rangle_{\boldsymbol{v}}} = \sum_{\boldsymbol{v}} P_{\text{data}}(\boldsymbol{v})\langle \dots \rangle_{\boldsymbol{v}}$ is the average expectation for a multiset of data $\{\boldsymbol{v_1}, \dots, \boldsymbol{v_n}\}$.

Calculating these gradient updates for general Boltzmann Machines can be exponentially costly and so approximate sampling-based methods are often employed. Additionally, a popular technique is to restrict the model graph to be a bipartite graph on the visible units and hidden units, called a \textit{Restricted Boltzmann Machine}, which allows efficient training through approximate methods like Contrastive Divergence~\cite{hinton2006fast}.

\subsection{Quantum Boltzmann Machines}
A \textit{Quantum Boltzmann Machine} (QBM) is a Boltzmann Machine where units are replaced by qubits and the energy function $E(\boldsymbol{z})$ is now a corresponding Transverse-field Ising model Hamiltonian~\cite{aminQuantumBoltzmannMachine2018}
\begin{equation}
    H = -\sum_a \Gamma_a \sigma^x_a -\sum_a b_a \sigma^z_a -\sum_{a,b} u_{ab} \sigma^z_a\sigma^z_b
\end{equation}
where $\sigma^i_a$ is the Pauli matrix $\sigma_i$ acting on qubit $a$. 

Defining $\Lambda_{\boldsymbol{v}}$ as the projector to the subspace with visible units equal to $\boldsymbol{v}$, our learned distribution is now:
\begin{equation}
    P(\boldsymbol{v}) = \Tr[\Lambda_{\boldsymbol{v}} \rho]
\end{equation}
where $\rho$ is the Gibbs state with partition function $Z$
\begin{equation}
    \rho = \frac{e^{-H}}{Z}, Z = \Tr[e^{-H}]
\end{equation}

Like the classical Boltzmann Machine case, our goal is to find model parameters $\boldsymbol{\theta} = (\boldsymbol{\Gamma}, \boldsymbol{b}, \boldsymbol{u})$ such that $P(\boldsymbol{v})$ approximates the data distribution $P_{\text{data}}(\boldsymbol{v})$.
Often an upper bound of the negative log-likelihood is optimized on to make training more tractable, however, doing so forces $\boldsymbol{\Gamma} \rightarrow \boldsymbol{0}$ and so $\Gamma_a = \Gamma$ becomes a hyperparameter fixed for all units. Similar to the classical case, gradient updates can then be calculated as
\begin{equation} \label{bias_grad}
    \delta b_a = -\eta \left(\overline{\langle \sigma_a^z \rangle_{\boldsymbol{v}}} - \langle \sigma_a^z \rangle\right),
\end{equation}
\begin{equation} \label{weight_grad}
    \delta u_{ab} = -\eta \left(\overline{\langle \sigma_a^z \sigma_b^z \rangle_{\boldsymbol{v}}} - \langle \sigma_a^z \sigma_b^z \rangle\right).
\end{equation}

Additionally, if our model is \textit{restricted} and has no connections between hidden units, then the positive phase for hidden units can be calculated exactly as
\begin{equation} \label{pos_phase}
    \langle \sigma_a^z \rangle_{\boldsymbol{v}} = \frac{b^{\text{eff}}_a(\boldsymbol{v})}{D_a(\boldsymbol{v})}\tanh{D_a(\boldsymbol{v})},
\end{equation}
\begin{equation}
    D_a(\boldsymbol{v}) = \sqrt{\Gamma_a^2 + (b^{\text{eff}}_a(\boldsymbol{v}))^2}
\end{equation}
where $b^{\text{eff}}_a(\boldsymbol{v}) = b_a + \sum_{b}u_{ab}\boldsymbol{v}_b$ is the effective bias on hidden unit $a$ based on the clamped state of each visible unit $b$ for visible data $\boldsymbol{v}$. For more background on training QBMs, we refer the reader to~\cite{aminQuantumBoltzmannMachine2018}.

Although we can calculate the positive phase cheaply, calculating the negative phase still requires Gibbs state sampling of our model Hamiltonian, which is expensive. Prior work has explored how to do this sampling variationally on near term quantum computers~\cite{zoufalVariationalQuantumBoltzmann2021} and proposed algorithms exist for Gibbs state sampling~\cite{chowdhury2016quantum, yung2012quantum, poulin2009sampling}, but finding the best way to perform Gibbs state sampling that is also tractable on current machines is still an active area of research.


% Figure environment removed
