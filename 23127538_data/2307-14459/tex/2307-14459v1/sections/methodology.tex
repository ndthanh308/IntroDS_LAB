% Problem
\subsection{Bars $\times$ Stripes}
To apply coresets, we need a data set with: 1) a high enough dimensionality $d$ such that a perfect coreset of size $2^d$ cannot be constructed, and 2) $n$ data points where $n$ is large enough that we can create a coreset of size $m \ll n$.

% The MNIST data set of handwritten digits~\cite{lecun1998gradient} is a common choice for evaluating generative machine learning models that satisfies these criteria. However, the original data set uses $28\times28$ pixel images which would require 784 visible units in a QBM. Unfortunately, this is far too large to train with approximate classical methods or near-term quantum computers. Similarly, trying to downsample MNIST to a more usable size like $6\times6$ loses too much information. 
In the past, work on smaller generative models, including QBMs~\cite{benedetti2019generative}, has used the Bars and Stripes (BAS) data set. In the normal definition of BAS, a data point is a binary $p \times q$ image consisting of only vertical lines \textbf{or} only horizontal lines.
To use this with coresets, we can choose $p,q$ such that $d = pq$ is sufficiently large to satisfy criteria 1, however, the number of distinct BAS images is only $O(2^p + 2^q)$, and so our data set might be too small to satisify criteria 2. One option is to just choose large enough $p,q$, however, we need $d$ visible units in our QBM and getting even $O(10^3)$ distinct images would require 81 visible units. This is too large to use with current methods, and so instead we opt to define images as only vertical lines \textbf{and} horizontal lines. To avoid confusion, we call this data set Bars $\times$ Stripes (BXS) as shown in Figure~\ref{fig:bxs_dataset}. With this formulation the number of distinct BXS images is $O(2^{p+q})$ and so we can construct a data set of $\approx4000$ distinct BXS images with only $p=q=6$, needing 36 visible units. Figure~\ref{fig:qbm} depicts the QBMs we use to learn this data set.

% Figure environment removed

% Boltzmann Training
\subsection{Gibbs State Sampling}\label{gibbs}
As described in Section \ref{motivation}, training a QBM boils down to Gibbs state sampling, a task quantum computers are suspected to perform well at~\cite{poulin2009sampling}.
Unfortunately, given that we use QBMs with 44 units corresponding to 44 qubits, and efficiently performing Gibbs state sampling on current quantum computers is still an active area of research, it's difficult for us to use current quantum computers for Gibbs state sampling. Instead we opt to emulate previous work and use classical approximate methods~\cite{anschuetzNearTermQuantumClassicalAssociative2019}. 

To approximately sample from the Gibbs state we use population annealing with path integral Monte Carlo updates. Using the Trotter-Suzuki mapping, we approximate our quantum system with a corresponding classical system that has an additional imaginary time dimension discretized into $M$ slices~\cite{suzuki1976relationship}. As given in~\cite{anschuetzNearTermQuantumClassicalAssociative2019}, the corresponding probability distribution is then
\begin{equation}
    p_{\beta}(\boldsymbol{z^m}) \propto \exp{\left(-\beta [E_{\text{cl}}(\boldsymbol{z^m}) + E_{\text{qm}}(\boldsymbol{z^m};\beta)]\right)},
\end{equation}
where
\begin{equation}
    E_{\text{cl}}(\boldsymbol{z^m}) = \frac{1}{M}\sum_{m=1}^{M}E(\boldsymbol{z^m}),
\end{equation}
\begin{equation}
    E_{\text{qm}}(\boldsymbol{z^m};\beta) = \frac{1}{2\beta}\sum_{a,m}\ln\left(\tanh\left(\frac{\beta \Gamma_a}{M}\right)\right)z_a^m z_a^{m+1}
\end{equation}
In the above equations, $\boldsymbol{z^m}$ denotes the $m^\text{th}$ imaginary time slice of the system, and $z_a^m$ is the state of the $a^\text{th}$ binary unit in the $m^\text{th}$ imaginary time slice. Additionally, periodic boundary conditions are enforced so that $m=M+1$ is $m=1$.

In population annealing, we maintain $K$ replicas of an initial state of this classical system and iterate through increasing values of $\beta = \frac{1}{T}$, the inverese temperature. Each iteration the population of replicas is resampled based on their relative Boltzmann weights and replicas are updated by a finite number of Monte Carlo steps~\cite{hukushimaPopulationAnnealingIts2003}. Afterwards, we can treat the first imaginary time slice of each replica as an approximate sample from our Gibbs state.

\subsection{QBM Training}

In our numerical experiments we take $\Gamma_a = 2$ and optimize the biases and weights using the Adam method~\cite{kingma2014adam} with the gradient calculations from Eq.~\ref{bias_grad} and Eq.~\ref{weight_grad}. We split the input data set into mini-batches $\boldsymbol{B_i} = \{\boldsymbol{b_1}, \dots, \boldsymbol{b_k}\}$ of size $k=32$. Negative phases are calculated using the procedure described in Section~\ref{gibbs} with 128 replicas, 5 iterations from $\beta = 0$ to $\beta = 1$, and $M=10$ imaginary time slices. For the positive phases, since a coreset has weighted points the exact calculation becomes
\begin{equation}
    \overline{\langle \sigma_a^z\rangle_{\boldsymbol{b}}} = \frac{1}{\sum_j w_j}\sum_j w_j \langle \sigma_a^z\rangle_{\boldsymbol{b_j}}
\end{equation}
using Eq.~\ref{pos_phase} for $\langle \sigma_a^z\rangle_{\boldsymbol{b_j}}$. 

We initialize our weights and biases using the recipe described in~\cite{hintonPracticalGuideTraining2012}. Weights are sampled from a normal distribution centered at 0 with a standard deviation of 0.01, and biases are calculated to be $b_a = \log[p_a / (1 - p_a)]$, where $p_a$ is the proportion of training data where bit $a$ is on.

% Inception Evaluation
\subsection{QBM Evaluation}\label{evaluation}
Since our goal is for $P(\boldsymbol{v})$ to approximate $P_{\text{data}}(\boldsymbol{v})$ we might want to evaluate the KL divergence between the two distributions to see how accurate the model is
\begin{equation}
    KL = \sum_{\boldsymbol{v}} P_{\text{data}}(\boldsymbol{v})\log{\frac{P_{\text{data}}(\boldsymbol{v})}{P(\boldsymbol{v})}}
\end{equation}
However, for high dimensionality data this is typically impractical to calculate, since we only know empirical distributions for $P_{\text{data}}(\boldsymbol{v})$ and $P(\boldsymbol{v})$ from the data set and our Gibbs state sampling, respectively. Since the possible values of $\boldsymbol{v}$ scales exponentially with the dimension it's unlikely these empirical distributions are large enough to have meaningful overlap where we can calculate the KL divergence. Additionally, in our experiments where we know the underlying data distribution is the $\approx 4000$ BXS images, we still only get 128 samples to estimate our model distribution, and so the KL divergence will always be $\infty$.

To avoid this issue and allow for a distinction between "close" images (e.g. an image that is 1 pixel from a BXS image) and other incorrect images, we adopt a strategy similar to the inception score~\cite{salimans2016improved}. We train a classical feedforward neural network with 3 layers to differentiate BXS images from non-BXS images to $>99\%$ validation accuracy. We then have this predict our QBM samples and use the BXS classification score as a proxy for the quality of our QBM samples. 

It's important to note that in our experiments we don't have multiple classifications for BXS images, and so we can't replicate the actual inception score which intuitively also checks that generated images come from a diverse number of classes to discount model overfitting. As a result, our metric is susceptible to being fooled by a model which has overfit to only a couple of images. However, we still find it to be a useful metric to evaluate QBM training.

% Coreset Construction
\subsection{Coreset Construction}
In our preliminary experiments we create two types of coresets of size $m=128$. The first is simply a uniform sampling of $m$ images from the data distribution. The second is constructed by solving the minimax facility location problem
\begin{equation}
    \boldsymbol{X'} = \arg \min_{\boldsymbol{X'}} \left( \max_{\boldsymbol{x_i} \in \boldsymbol{X}} \min_{\boldsymbol{x'_j} \in \boldsymbol{X'}}d(\boldsymbol{x_i},\boldsymbol{x'_j})\right)
\end{equation}
similarly to~\cite{sinha2020small}. Intuitively, solving this problem entails choosing a coreset $X'$ such that we minimize the maximum distance for a point in the full dataset $X$ to its closest point in the coreset. In this formulation $d(\boldsymbol{x_i}, \boldsymbol{x'_j})$ is some distance function between data set point $\boldsymbol{x_i}$ and coreset point $\boldsymbol{x'_j}$. Solving this problem is NP-Hard~\cite{fowler1981optimal} and so we solve it greedily using Algorithm 1 from~\cite{sinha2020small}. 

Like~\cite{sinha2020small}, we don't use the Euclidean distance between two data points in the $d=pq$ dimensional space for $d(\boldsymbol{x_i}, \boldsymbol{x'_j})$, which is often not meaningful for images or useful in high-dimensional spaces. Instead, we reduce each data point to 8 dimensions using the output of the second to last layer of the classical neural network from Section~\ref{evaluation} and calculate Euclidean distances in this 8 dimensional space. We refer to this as the Inception Distance (ID), with the intuition that this projection will exploit more semantic information and be of a small enough dimensionality to get useful values for $d(\boldsymbol{x_i}, \boldsymbol{x'_j})$.

