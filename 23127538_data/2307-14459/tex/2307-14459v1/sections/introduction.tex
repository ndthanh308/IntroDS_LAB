% If we can summarize a data set with a smaller coreset, then we can iterate through the training process with fewer Gibbs state samplings per epoch. 

% We want to get the most out of each gradient update step

% Quantum RAM non-existent

% Minimizing the amount of times you need to access the quantum computer to perform Gibbs state sampling will be beneficial

% 1. introduce quantum computing and potential applications including simulation and machine learning
Recent years have seen a slowdown in the exponential improvements due to Moore's Law and Dennard scaling. This slowdown has been accompanied by a corresponding increase in attention paid to non-traditional, post-Moore's Law computer architectures including analog and neuromorphic computing. Quantum computing is another such architecture which exploits quantum mechanical properties such as entanglement and superposition to perform computations. For certain tasks, quantum computers are conjectured to outperform their classical counterparts precisely because they have access to purely quantum phenomena~\cite{feynman1982simulating}.

% want to introduce machine learning early as a potential application to motivate why we want to apply coresets to this problem
The magnitude of these speedups vary from exponential --- for example the factorization of large primes~\cite{shor1999polynomial}, simulating the physics of entangled systems~\cite{lloyd1996universal}, and solving linear systems of equations~\cite{harrow2009quantum} --- to quadratic improvements for unstructured search~\cite{grover1996fast}. Additionally, an area that has exploded with recent research~\cite{biamonte2017quantum, broughton2020tensorflow, rudolph2020generation} is quantum machine learning (QML). There are many different types of quantum neural networks, and while the exact nature of the quantum speedup (if it exists) for some QML algorithms is unknown, recent work suggests that applications targeting quantum data are a promising direction~\cite{kieferovaTomographyGenerativeTraining2017, huang2021quantum}.

% 2. introduce the current issues with quantum computing (quantum RAM non-existent)
Current quantum computers (QCs) are built from a variety of different technologies, enabling research in the early steps of realizing quantum algorithms and evaluating QCs~\cite{arute2019quantum, nam2020ground, tomesh2022supermarq}. Unfortunately there is a gap that currently exists between the capabilities of these machines and the resource requirements for many quantum algorithms. Current QCs are known as Noisy Intermediate-Scale Quantum (NISQ) devices~\cite{preskill2018quantum}. Their limited size and gate fidelities render them unable to implement the error correcting codes that are needed to implement most known quantum algorithms. Prior work has found that a co-design approach to QC system design, characterized by the breaking of abstraction layers and the sharing of information up and down the stack, can result in significantly improved performance~\cite{shi2020resource, tomesh2021quantum}.

% 3. introduce coresets for quantum computing
QML applications are especially hindered by the need to load large data sets onto small quantum devices. Access to a quantum random access memory would allow a QC to coherently load quantum states representing classical data. However, it is likely that the construction of a quantum RAM is equally or more difficult than building a fault tolerant QC~\cite{arunachalam2015robustness}. Instead, prior work has investigated the use of coresets, a succinct summarization of a larger data set \cite{mirzasoleiman2020coresets}, to apply QML models to large data sets using small quantum computers~\cite{harrowSmallQuantumComputers2020, tomeshCoresetClusteringSmall2020}.

% 4. introduce our work: applied coresets to the training of QBMs to make the process faster and cheaper
% Boltzmann machines have great potential -> classical examples, generative QML -> discriminative
% Coresets can help offset their training cost
The Quantum Boltzmann Machine (QBM) is a physically motivated quantum neural network that can be used for generative or discriminative learning~\cite{aminQuantumBoltzmannMachine2018}. Prior work has studied the application of QBMs to tasks such as image generation \cite{anschuetzNearTermQuantumClassicalAssociative2019}, and they have been shown to outperform classical Boltzmann machines for certain tasks such as quantum state generation~\cite{kieferovaTomographyGenerativeTraining2017}. 

The downside of such a powerful and versatile QML model is the overhead costs associated with the training process. The gradient updates that are needed to tune the model's parameters require samples taken from a thermal (Gibbs) state
\begin{equation}
    \rho(\beta) = \frac{e^{-\beta H}}{\text{Tr}(e^{-\beta H})}
\end{equation}
where $\beta=1/T$ is the inverse temperature and $H$ is the system Hamiltonian describing the QBM. This is an NP-Hard problem \cite{kieferovaTomographyGenerativeTraining2017, anschuetz2019realizing} and to successfully train a QBM many such states will need to be prepared and sampled from.

% How have other works proposed to make the training of QBMs more efficient? And how are we different?
To circumvent the difficulties of training a QBM many different techniques have been suggested. For example, rather than training on the exact loss function it can be simpler and more efficient to train on an upper bound of the loss or restrict the connectivity of the QBM model~\cite{aminQuantumBoltzmannMachine2018}.
Similarly, a variety of different quantum algorithms have been proposed for the specific task of thermal state preparation including quantum walks \cite{temme2011quantum}, quenches~\cite{anschuetz2019realizing}, semi-definite programming~\cite{brandao2017quantum}, and hybrid variational algorithms~\cite{verdonQuantumAlgorithmTrain2019}.

% We give a complementary approach by optimizing the training dataset
This work presents a complementary method for potentially reducing the training overhead of QBMs. We propose building coresets of the training data set to reduce the overall number of Gibbs state preparations needed during the training process. We run initial numerical simulations with QBMs needing 44 qubits and present an augmented bars and stripes data set and corresponding Inception score inspired metric to compare training with and without coresets.

% 5. List contribution bullets
Our contributions include
\begin{enumerate}
    \item A numerical study motivating the potential of coresets for QBM training.
    \item An augmented bars and stripes data set to evaluate generative machine learning models of moderate dimensionality.
    \item Numerical experiments verifying successful training of QBMs on this data set.
    \item Initial numerical experiments exploring the effectiveness of coresets for QBMs of moderate size.
\end{enumerate}

% 6. Give paper outline
