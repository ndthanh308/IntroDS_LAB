{
  "title": "Is attention all you need in medical image analysis? A review",
  "authors": [
    "Giorgos Papanastasiou",
    "Nikolaos Dikaios",
    "Jiahao Huang",
    "Chengjia Wang",
    "Guang Yang"
  ],
  "submission_date": "2023-07-24T13:24:56+00:00",
  "revised_dates": [],
  "abstract": "Medical imaging is a key component in clinical diagnosis, treatment planning and clinical trial design, accounting for almost 90% of all healthcare data. CNNs achieved performance gains in medical image analysis (MIA) over the last years. CNNs can efficiently model local pixel interactions and be trained on small-scale MI data. The main disadvantage of typical CNN models is that they ignore global pixel relationships within images, which limits their generalisation ability to understand out-of-distribution data with different 'global' information. The recent progress of Artificial Intelligence gave rise to Transformers, which can learn global relationships from data. However, full Transformer models need to be trained on large-scale data and involve tremendous computational complexity. Attention and Transformer compartments (Transf/Attention) which can well maintain properties for modelling global relationships, have been proposed as lighter alternatives of full Transformers. Recently, there is an increasing trend to co-pollinate complementary local-global properties from CNN and Transf/Attention architectures, which led to a new era of hybrid models. The past years have witnessed substantial growth in hybrid CNN-Transf/Attention models across diverse MIA problems. In this systematic review, we survey existing hybrid CNN-Transf/Attention models, review and unravel key architectural designs, analyse breakthroughs, and evaluate current and future opportunities as well as challenges. We also introduced a comprehensive analysis framework on generalisation opportunities of scientific and clinical impact, based on which new data-driven domain generalisation and adaptation methods can be stimulated.",
  "categories": [
    "cs.CV",
    "cs.AI",
    "cs.LG",
    "eess.IV"
  ],
  "primary_category": "cs.CV",
  "doi": "10.1109/JBHI.2023.3348436",
  "journal_ref": null,
  "arxiv_id": "2307.12775",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 0,
  "size_after_bytes": 0
}